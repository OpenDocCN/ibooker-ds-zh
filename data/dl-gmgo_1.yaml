- en: Part 2\. Machine learning and game AI
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分\. 机器学习和游戏人工智能
- en: In [part 2](#part02), you’ll learn the components of both classical and modern
    game AI. You’ll start with a variety of tree search algorithms, which are essential
    tools in game AI and all kinds of optimization problems. Next, you’ll learn about
    deep learning and neural networks, starting from the mathematical basics and working
    up to many practical design considerations. Finally, you’ll get an introduction
    to reinforcement learning, a framework where your game AI can improve through
    practice.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二部分](#part02)中，你将学习古典和现代游戏人工智能的组件。你将从各种树搜索算法开始，这些算法是游戏人工智能和各类优化问题的基本工具。接下来，你将学习深度学习和神经网络，从数学基础开始，逐步到许多实际设计考虑。最后，你将了解强化学习，这是一个你的游戏人工智能可以通过实践来改进的框架。
- en: Of course, none of these techniques are limited to games. Once you’ve mastered
    the components, you’ll start seeing opportunities to apply them in all sorts of
    domains.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些技术并不局限于游戏。一旦你掌握了这些组件，你将开始看到在各个领域应用它们的机会。
- en: Chapter 4\. Playing games with tree search
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章\. 使用树搜索玩游戏
- en: '*This chapter covers*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Finding the best move with the minimax algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最小-最大算法找到最佳走法
- en: Pruning minimax tree search to speed it up
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝最小-最大树搜索以加快速度
- en: Applying Monte Carlo tree search to games
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将蒙特卡洛树搜索应用于游戏
- en: 'Suppose you’re given two tasks. The first is to write a computer program that
    plays chess. The second is to write a program that plans how to efficiently pick
    orders in a warehouse. What could these programs have in common? At first glance,
    not much. But if you step back and think in abstract terms, you can see a few
    parallels:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被赋予了两个任务。第一个任务是编写一个玩棋类的计算机程序。第二个任务是编写一个计划如何在仓库中高效挑选订单的程序。这些程序有什么共同之处呢？乍一看，似乎并不多。但如果你退后一步，用抽象的思维方式思考，你会发现一些相似之处：
- en: '***You have a sequence of decisions to make.*** In chess, your decisions are
    about which piece to move. In the warehouse, your decisions are about which item
    to pick up next.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***你需要做出一系列决策。*** 在棋类游戏中，你的决策是关于移动哪个棋子。在仓库中，你的决策是关于下一个要挑选哪个物品。'
- en: '***Early decisions can affect future decisions.*** In chess, moving a pawn
    early on may expose your queen to a counterattack many turns later. In the warehouse,
    if you go for a widget on shelf 17 first, you may need to backtrack all the way
    to shelf 99 later.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***早期的决策可能会影响未来的决策。*** 在棋类游戏中，早期移动兵可能会在许多回合后使你的王后暴露在反击之下。在仓库中，如果你首先去货架17上的小工具，你可能需要在稍后需要时回溯到货架99。'
- en: '***At the end of a sequence, you can evaluate how well you achieved your goal.***
    In chess, when you reach the end of the game, you know who won. In the warehouse,
    you can add up the time it took to gather all the items.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***在序列的末尾，你可以评估你实现目标的效果。*** 在棋类游戏中，当你达到游戏结束时，你知道谁赢了。在仓库中，你可以累计收集所有物品所需的时间。'
- en: '***The number of possible sequences can get enormous.*** There are around 10^(100)
    ways a chess game can unfold. In the warehouse, if you have 20 items to pick up,
    there are 2 billion possible paths to choose from.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可能的序列数量可能非常庞大。*** 棋类游戏可能有大约 10^(100) 种展开方式。在仓库中，如果你有20个物品要挑选，有20亿种可能的路径可以选择。'
- en: Of course, this analogy only goes so far. In chess, for example, you alternate
    turns with an opponent who is actively trying to thwart your intentions. That
    doesn’t happen in any warehouse you’d want to work in.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个类比只能到此为止。例如，在棋类游戏中，你与一个积极试图阻挠你意图的对手轮流走棋。这在你想要工作的任何仓库中都不会发生。
- en: 'In computer science, *tree-search* algorithms are strategies for looping over
    many possible sequences of decisions to find the one that leads to the best outcome.
    In this chapter, we cover tree-search algorithms as they apply to games; many
    of the principles can extend to other optimization problems as well. We start
    with the *minimax* search algorithm, in which you switch perspectives between
    two opposing players on each turn. The minimax algorithm can find perfect lines
    of play but is too slow to apply to sophisticated games. Next, we take a look
    at two techniques for getting a useful result while searching only a tiny fraction
    of the tree. One of these is *pruning*: you speed up the search by eliminating
    sections of the tree. To prune effectively, you need to bring real-world knowledge
    of the problem into your code. When that’s not possible, you can sometimes apply
    *Monte Carlo tree search* (MCTS). MCTS is a randomized search algorithm that can
    find a good result without any domain-specific code.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，*树搜索*算法是遍历许多可能的决策序列以找到最佳结果的策略。在本章中，我们讨论了树搜索算法在游戏中的应用；许多原则也可以扩展到其他优化问题。我们从*最小-最大*搜索算法开始，其中你需要在每轮中切换两个对立玩家的视角。最小-最大算法可以找到完美的游戏策略，但速度太慢，无法应用于复杂的游戏。接下来，我们看看两种在搜索树的一小部分时就能得到有用结果的技术。其中之一是*剪枝*：通过消除树的部分来加速搜索。为了有效地剪枝，你需要将现实世界的知识带入你的代码中。当这不可能时，有时可以应用*蒙特卡洛树搜索*（MCTS）。MCTS是一种随机搜索算法，可以在没有任何领域特定代码的情况下找到好的结果。
- en: With these techniques in your toolkit, you can start building AIs that can play
    a variety of board and card games.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的工具箱中有了这些技术，你可以开始构建能够玩各种棋盘游戏和卡牌游戏的AI。
- en: 4.1\. Classifying games
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 游戏分类
- en: 'Tree-search algorithms are mainly relevant to games where you take turns, and
    a discrete set of options is available on each turn. Many board and card games
    fit this description. On the other hand, tree search won’t help a computer play
    basketball, charades, or World of Warcraft. We can further classify board and
    card games according to two useful properties:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 树搜索算法主要适用于轮流进行游戏，并且每轮都有离散的选项可供选择。许多棋盘游戏和卡牌游戏都符合这一描述。另一方面，树搜索对计算机打篮球、捉迷藏或魔兽世界没有帮助。我们可以根据两个有用的属性进一步对棋盘游戏和卡牌游戏进行分类：
- en: '***Deterministic vs. nondeterministic*—** In a *deterministic* game, the course
    of the game depends only on the players’ decisions. In a *nondeterministic* game,
    an element of randomness is involved, such as rolling dice or shuffling cards.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***确定性 vs. 非确定性*—** 在**确定性**游戏中，游戏的进程只取决于玩家的决策。在**非确定性**游戏中，涉及随机元素，例如掷骰子或洗牌。'
- en: '***Perfect information vs. hidden information*—** In *perfect information*
    games, both players can see the full game state at all times; the whole board
    is visible, or everyone’s cards are on the table. In *hidden information* games,
    each player can see only part of the game state. Hidden information is common
    in card games, where each player is dealt a few cards and can’t see what the other
    players are holding. Part of the appeal of hidden information games is guessing
    what the other players know based on their game decisions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***完美信息 vs. 隐藏信息*—** 在**完美信息**游戏中，两位玩家可以随时看到整个游戏状态；整个棋盘是可见的，或者每个人的牌都在桌面上。在**隐藏信息**游戏中，每位玩家只能看到游戏状态的一部分。隐藏信息在卡牌游戏中很常见，每位玩家被发几张牌，却看不到其他玩家手中的牌。隐藏信息游戏的吸引力之一是根据玩家的游戏决策猜测其他玩家知道什么。'
- en: '[Table 4.1](#ch04table01) shows how several well-known games fit into this
    taxonomy.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4.1](#ch04table01) 展示了几个知名游戏如何纳入这个分类法。'
- en: Table 4.1\. Classifying board and card games
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.1\. 游戏分类：棋盘游戏和卡牌游戏
- en: '|   | Deterministic | Nondeterministic |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|   | 确定性 | 非确定性 |'
- en: '| --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Perfect information | Go, chess | Backgammon |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 完美信息 | 国际象棋 | 象棋 |'
- en: '| Hidden information | Battleship, Stratego | Poker, Scrabble |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏信息 | 飞行艇，策略游戏 | 赌博，拼字游戏 |'
- en: In this chapter, we primarily focus on deterministic, perfect information games.
    On each turn of such games, one move is theoretically the best. There’s no luck
    and no secrets; before you choose a move, you know every move your opponent might
    choose in response, and everything you might do after that, and so on to the end
    of the game. In theory, you should have the whole game planned out on the first
    move. The minimax algorithm does exactly that in order to come up with the perfect
    play.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要关注确定性、完美信息游戏。在这样游戏的每一回合，理论上有一个最佳走法。没有运气，也没有秘密；在你选择走法之前，你知道你的对手可能会选择的所有回应，以及你之后可能采取的所有行动，以此类推到游戏结束。理论上，你应该在第一步就计划好整个游戏。最小-最大算法正是为了达到这个目的而设计的，以实现完美的游戏策略。
- en: In reality, the games that have stood the test of time, such as chess and Go,
    have an enormous number of possibilities. To humans, every game seems to take
    on a life of its own, and even computers can’t calculate them all the way to the
    end.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，经得起时间考验的游戏，如国际象棋和围棋，有无数的可能性。对人类来说，每一场游戏似乎都充满了生命力，即使是计算机也无法计算出所有可能的结局。
- en: All the examples in this chapter contain little game-specific logic, so you
    can adapt them to any deterministic, perfect information game. To do so, you can
    follow the pattern of our goboard module and implement your new game logic in
    classes such as `Player`, `Move`, and `GameState`. The essential functions for
    `GameState` are `apply_move`, `legal_moves`, `is_over`, and `winner`. We have
    done this for tic-tac-toe; you can find this in the ttt module on GitHub ([http://mng.bz/gYPe](http://mng.bz/gYPe)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有示例都包含很少的游戏特定逻辑，因此你可以将它们适应到任何确定性、完美信息游戏中。要做到这一点，你可以遵循我们井字棋模块的模式，并在`Player`、`Move`和`GameState`等类中实现你的新游戏逻辑。`GameState`的基本函数是`apply_move`、`legal_moves`、`is_over`和`winner`。我们已经为井字棋做了这件事；你可以在GitHub上的ttt模块中找到它（[http://mng.bz/gYPe](http://mng.bz/gYPe))。
- en: '|  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Games for AI experiments**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**用于AI实验的游戏**'
- en: 'Need some inspiration? Look up the rules for any of the following games:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一些灵感吗？查找以下任何游戏的规则：
- en: Chess
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际象棋
- en: Checkers
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际跳棋
- en: Reversi
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反转棋
- en: Hex
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 六边形棋
- en: Chinese checkers
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中国象棋
- en: Mancala
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曼尼拉
- en: Nine Men’s Morris
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 九子棋
- en: Gomoku
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 五子棋
- en: '|  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 4.2\. Anticipating your opponent with minimax search
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 使用最小-最大搜索预测对手
- en: 'How can you program a computer to decide what move to make next in a game?
    To start, you can think about how humans would make the same decision. Let’s start
    with the simplest deterministic, perfect information game there is: tic-tac-toe.
    The technical name for the strategy we’ll describe is *minimaxing*. This term
    is a contraction of *minimizing and maximizing*: you’re trying to maximize your
    score, while your opponent is trying to minimize your score. You can sum up the
    algorithm in one sentence: assume that your opponent is as smart as you are.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何编程让计算机在游戏中决定下一步棋？首先，你可以思考人类会如何做出同样的决定。让我们从最简单的确定性、完美信息游戏开始：井字棋。我们描述的策略的技术名称是
    *最小-最大策略*。这个术语是 *最小化和最大化* 的缩写：你试图最大化你的得分，而你的对手则试图最小化你的得分。你可以用一句话总结这个算法：假设你的对手和你一样聪明。
- en: Let’s see how minimaxing works in practice. Take a look at [figure 4.1](#ch04fig01).
    What move should X make next?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最小-最大策略在实际中是如何工作的。看看[图 4.1](#ch04fig01)。X下一步应该走哪？
- en: 'Figure 4.1\. What move should × make next? This is an easy one: playing in
    the lower-right corner wins the game.'
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1\. X下一步应该走哪？这是一个简单的选择：在右下角走棋就能赢得游戏。
- en: '![](Images/04fig01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig01.jpg)'
- en: 'There’s no trick here; taking the lower-right corner wins the game. You can
    make that into a general rule: take any move that immediately wins the game. There’s
    no way this plan can go wrong. You could implement this rule in code with something
    like the following listing.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有技巧；占据右下角就能赢得游戏。你可以将这个规则概括为：采取任何立即赢得游戏的走法。这个计划不可能出错。你可以用以下代码实现这个规则。
- en: Listing 4.1\. A function that finds a move that immediately wins the game
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.1\. 一个找到立即获胜走法的函数
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* Loops over all legal moves**'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 遍历所有合法走法**'
- en: '***2* Calculates what the board would look like if you pick this move**'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 计算如果你选择这个走法，棋盘会是什么样子**'
- en: '***3* This is a winning move! No need to continue searching.**'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 这是一个获胜的走法！无需继续搜索。**'
- en: '***4* Can’t win on this turn**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 本回合无法获胜**'
- en: '[Figure 4.2](#ch04fig02) illustrates the hypothetical board positions this
    function would examine. This structure, in which a board position points to possible
    follow-ups, is called a *game tree*.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.2](#ch04fig02)展示了这个函数将要检查的假设棋盘位置。这种结构，其中棋盘位置指向可能的后续走法，被称为*游戏树*。'
- en: Figure 4.2\. An illustration of an algorithm to find the winning move. You start
    with the position at the top. You loop over every possible move and calculate
    the game state that would result if you played that move. Then you check whether
    that hypothetical game state is a winning position for ×.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2\. 一个寻找获胜走法的算法示例。你从顶部位置开始。遍历每一个可能的走法，并计算如果你走这步棋，游戏状态会变成什么样子。然后检查这个假设的游戏状态是否是×的获胜位置。
- en: '![](Images/04fig02_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig02_alt.jpg)'
- en: 'Let’s back up a bit. How did you get in this position? Perhaps the previous
    position looked like [figure 4.3](#ch04fig03). The O player naively hoped to make
    three in a row across the bottom. But that assumes that X will cooperate with
    the plan. This gives a corollary to our previous rule: don’t choose any move that
    gives your opponent a winning move. [Listing 4.2](#ch04ex02) implements this logic.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下。你是如何进入这个位置的？也许前一个位置看起来像[图4.3](#ch04fig03)。O玩家天真地希望横跨底部完成三连珠。但这假设X会配合这个计划。这给我们之前的规则提供了一个推论：不要选择任何会给对手带来获胜走法的走法。[列表4.2](#ch04ex02)实现了这个逻辑。
- en: Figure 4.3\. What move should O make next? If O plays in the lower left, you
    must assume that × will follow up in the lower right to win the game. O must find
    the only move that prevents this.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3\. O下一步应该走什么？如果O在左下角走棋，你必须假设X将在右下角跟进以赢得比赛。O必须找到唯一一个可以阻止这种情况的走法。
- en: '![](Images/04fig03.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig03.jpg)'
- en: Listing 4.2\. A function that avoids giving the opponent a winning move
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.2\. 一个避免给对手带来获胜走法的函数
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* possible_moves will become a list of all moves worth considering.**'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* possible_moves 将成为一个值得考虑的所有走法的列表。**'
- en: '***2* Loops over all legal moves**'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历所有合法的走法**'
- en: '***3* Calculates what the board would look like if you play this move**'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 计算如果你走这步棋，棋盘会是什么样子**'
- en: '***4* Does this give your opponent a winning move? If not, this move is plausible.**'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 这个走法会给对手带来获胜机会吗？如果不给，这个走法是可行的。**'
- en: Now, you know that you must block your opponent from getting into a winning
    position. Therefore, you should assume that your opponent is going to do the same
    to you. With that in mind, how can you play to win? Take a look at the board in
    [figure 4.4](#ch04fig04).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你知道你必须阻止对手进入获胜位置。因此，你应该假设对手也会对你做同样的事情。考虑到这一点，你该如何赢得比赛？看看[图4.4](#ch04fig04)中的棋盘。
- en: 'Figure 4.4\. What move should × make? If × plays in the center, there will
    be two ways to complete three in a row: (1) top middle and (2) lower right. O
    can block only one of these options, so × is guaranteed a win.'
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4\. ×下一步应该走什么？如果×在中间走棋，将有两种方式来完成三连珠：（1）上中；（2）下右。O只能阻止其中一种选择，所以×保证能赢。
- en: '![](Images/04fig04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig04.jpg)'
- en: 'If you play in the center, you have two ways to complete three in a row: top
    middle or lower right. The opponent can’t block them both. We can describe this
    general principle as follows: look for a move that sets up a subsequent winning
    move that your opponent can’t block. Sounds complicated, but it’s easy to build
    this logic on top of the functions you’ve already written.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你走中间，你有两种方式来完成三连珠：上中或下右。对手无法同时阻止它们。我们可以将这个一般原则描述如下：寻找一个走法，这个走法可以设置一个后续的获胜走法，而对手无法阻止。听起来很复杂，但很容易在已经编写的函数之上构建这个逻辑。
- en: Listing 4.3\. A function that finds a two-move sequence that guarantees a win
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.3\. 一个找到保证获胜的两步序列的函数
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Loops over all legal moves**'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 遍历所有合法的走法**'
- en: '***2* Calculates what the board would look like if you play this move**'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 计算如果你走这步棋，棋盘会是什么样子**'
- en: '***3* Does your opponent have a good defense? If not, pick this move.**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 你的对手有好的防守吗？如果没有，选择这个走法。**'
- en: '***4* No matter what move you pick, your opponent can prevent a win.**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 无论你选择什么走法，你的对手都可以阻止你获胜。**'
- en: 'Your opponent will anticipate that you’ll try to do this, and also try to block
    such a play. You can start to see a general strategy forming:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你的对手会预料到你将尝试这样做，并试图阻止这种走法。你开始看到一种总体策略的形成：
- en: See if you can win on the next move. If so, play that move.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看你是否能在下一回合获胜。如果是，就下这个走法。
- en: If not, see if your opponent can win on the next move. If so, block that.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不是，看看你的对手是否能在下一回合获胜。如果是，就阻止他。
- en: If not, see if you can force a win in two moves. If so, play to set that up.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不是，看看你是否能在两步内迫使对手获胜。如果是这样，就进行设置。
- en: If not, see if your opponent could set up a two-move win on their next move.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不是，看看你的对手是否能在下一次移动中设置一个两步获胜的局面。
- en: Notice that all three of your functions have a similar structure. Each function
    loops over all valid moves and examines the hypothetical board position that you’d
    get after playing that move. Furthermore, each function builds on the previous
    function to simulate what your opponent would do in response. If you generalize
    this concept, you get an algorithm that can always identify the best possible
    move.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你的所有三个函数都有类似的结构。每个函数都会遍历所有有效的移动，并检查执行该移动后你将得到的假设棋盘位置。此外，每个函数都是基于前一个函数来模拟对手将如何应对。如果你将这个概念推广，你将得到一个总能识别最佳可能移动的算法。
- en: '4.3\. Solving tic-tac-toe: a minimax example'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 解决井字棋：最小-最大示例
- en: In the previous section, you examined how to anticipate your opponent’s play
    one or two moves ahead. In this section, we show how to generalize that strategy
    to pick perfect moves in tic-tac-toe. The core idea is exactly the same, but you
    need the flexibility to look an arbitrary number of moves in the future.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个章节中，你学习了如何预测对手的一步或两步棋。在本节中，我们将展示如何将这种策略推广到井字棋中，以选择完美的移动。核心思想完全相同，但你需要灵活性，以便查看任意数量的未来移动。
- en: 'First let’s define an enum that represents the three possible outcomes of a
    game: win, loss, or draw. These possibilities are defined relative to a particular
    player: a loss for one player is a win for the other.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个枚举，它代表游戏的三种可能结果：胜利、失败或平局。这些可能性是相对于特定玩家定义的：一个玩家的失败对另一个玩家来说是胜利。
- en: Listing 4.4\. An enum to represent the outcome of a game
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.4. 一个枚举，用于表示游戏的结果
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Imagine you have a function `best_result` that takes a game state and tells
    you the best outcome that a player could achieve from that state. If that player
    could guarantee a win—by any sequence, no matter how complicated—the `best_result`
    function would return `GameResult.win`. If that player could force a draw, the
    function would return `GameResult.draw`. Otherwise, it would return `GameResult.loss`.
    If you assume that function already exists, it’s easy to write a function to pick
    a move: you loop over all possible moves, call `best_result`, and pick the move
    that leads to the best result for you. Multiple moves might exist that lead to
    equal results; you can just pick randomly from them in that case. The following
    listing shows how to implement this.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个函数 `best_result`，它接受一个游戏状态并告诉你玩家从该状态中可以达到的最佳结果。如果该玩家可以通过任何序列（无论多么复杂）保证胜利，则
    `best_result` 函数将返回 `GameResult.win`。如果该玩家可以迫使平局，则函数将返回 `GameResult.draw`。否则，它将返回
    `GameResult.loss`。如果你假设该函数已经存在，编写一个选择移动的函数就很容易了：你遍历所有可能的移动，调用 `best_result`，并选择导致你最佳结果的移动。可能存在多个导致相同结果的移动；在这种情况下，你可以随机从中选择。下面的列表显示了如何实现这一点。
- en: Listing 4.5\. A game-playing agent that implements minimax search
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.5. 实现最小-最大搜索的游戏代理
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* Loops over all legal moves**'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 遍历所有合法的移动**'
- en: '***2* Calculates the game state if you select this move**'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 计算如果你选择这个移动的游戏状态**'
- en: '***3* Because your opponent plays next, figure out their best possible outcome
    from there. Your outcome is the opposite of that.**'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 因为你的对手将下下一步棋，所以从那里推断出他们可能的最佳结果。你的结果将是相反的。**'
- en: '***4* Categorizes this move according to its outcome**'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 根据其结果对这次移动进行分类**'
- en: '***5* Picks a move that leads to your best outcome**'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 选择导致你最佳结果的一步棋**'
- en: 'Now the question is how to implement `best_result`. As in the previous section,
    you can start from the end of the game and work backward. The following listing
    shows the easy case: if the game is already over, there’s only one possible result.
    You just return it.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是如何实现 `best_result`。正如前一个章节中提到的，你可以从游戏的最后一步开始，向后工作。下面的列表显示了简单的情况：如果游戏已经结束，只有一个可能的结果。你只需返回它。
- en: Listing 4.6\. First step of the minimax search algorithm
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6. 最小-最大搜索算法的第一步
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you’re somewhere in the middle of the game, you need to search ahead. By
    now, the pattern should be familiar. You start by looping over all possible moves
    and calculating the next game state. Then you must assume that your opponent will
    do their best to counter your hypothetical move. To do so, you can call `best_result`
    from this new position. That tells you the result that your *opponent* can get
    from the new position; you invert it to find out your result. Out of all the moves
    you consider, you select the one that leads to the best result for you. [Listing
    4.7](#ch04ex07) shows how to implement this logic, which makes up the second half
    of `best_result`. [Figure 4.5](#ch04fig05) illustrates the board positions this
    function will consider for a particular tic-tac-toe board.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处于游戏的中途，你需要向前搜索。到目前为止，这个模式应该是熟悉的。你首先遍历所有可能的走法，并计算下一个游戏状态。然后你必须假设你的对手会尽其所能来对抗你的假设走法。为此，你可以从这个新位置调用`best_result`。这告诉你对手可以从新位置得到的结果；你反转它以找出你的结果。在考虑的所有走法中，你选择对你最有利的走法。[列表4.7](#ch04ex07)展示了如何实现这个逻辑，这构成了`best_result`的第二部分。[图4.5](#ch04fig05)说明了这个函数将为特定的井字棋棋盘考虑的棋盘位置。
- en: Figure 4.5\. A tic-tac-toe game tree. In the top position, it’s ×’s turn. If
    × plays in the top center, O can guarantee a win. If × plays in the left center,
    × will win. If × plays in the right center, O can force a draw. Therefore, × will
    choose to play in the left center.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5\. 一个井字棋游戏树。在顶部位置，轮到×走。如果×在顶部中间走，O可以保证胜利。如果×在左中间走，×将获胜。如果×在右中间走，O可以迫使平局。因此，×会选择在左中间走。
- en: '![](Images/04fig05_alt.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig05_alt.jpg)'
- en: Listing 4.7\. Implementing minimax search
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.7\. 实现最小-最大搜索
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1* See what the board would look like if you play this move.**'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 看看如果你走这步棋，棋盘会是什么样子。**'
- en: '***2* Find out your opponent’s best move.**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 找出你的对手的最佳走法。**'
- en: '***3* Whatever your opponent wants, you want the opposite.**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 无论你的对手想要什么，你想要的是相反的。**'
- en: '***4* See if this result is better than the best you’ve seen so far.**'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 看看这个结果是否比迄今为止看到的最优结果更好。**'
- en: 'If you apply this algorithm to a simple game such as tic-tac-toe, you get an
    unbeatable opponent. You can play against it and see for yourself: try the play_ttt.py
    example on GitHub ([http://mng.bz/gYPe](http://mng.bz/gYPe)). In theory, this
    algorithm would also work for chess, Go, or any other deterministic, perfect information
    game. In reality, this algorithm is far too slow for any of those games.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这个算法应用于像井字棋这样的简单游戏，你会得到一个不可战胜的对手。你可以与之对弈，亲自看看：尝试GitHub上的play_ttt.py示例（[http://mng.bz/gYPe](http://mng.bz/gYPe)）。从理论上讲，这个算法也适用于国际象棋、围棋或其他任何确定性的、完美信息游戏。实际上，这个算法对于这些游戏来说速度太慢了。
- en: 4.4\. Reducing search space with pruning
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 通过剪枝减少搜索空间
- en: In our tic-tac-toe game, you calculated every single possible game in order
    to find the perfect strategy. There are fewer than 300,000 possible tic-tac-toe
    games, peanuts for a modern computer. Can you apply the same technique to more
    interesting games? There are around 500 billion billion (that’s a 5 followed by
    20 zeros) possible board positions in checkers, for example. It’s technically
    possible to search them all on a cluster of modern computers, but it takes years.
    In chess and Go, there are more possible board positions than there are atoms
    in the universe (as their fans are quick to point out). Searching them all is
    out of the question.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的井字棋游戏中，你计算了每一种可能的棋局，以找到完美的策略。井字棋的可能棋局少于300,000种，对于现代计算机来说微不足道。你能将同样的技术应用于更有趣的游戏吗？例如，国际象棋有大约5000亿亿（即5后面跟着20个零）种可能的棋盘位置。在技术上，可以在现代计算机集群上搜索它们，但这需要数年。在国际象棋和围棋中，可能的棋盘位置比宇宙中的原子还多（正如他们的粉丝乐于指出）。搜索所有这些位置是不可能的。
- en: To use tree search to play a sophisticated game, you need a strategy to eliminate
    parts of the tree. Identifying which parts of the tree you can skip is called
    *pruning*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用树搜索来玩复杂的游戏，你需要一种策略来消除树的一部分。确定你可以跳过的树的部分被称为*剪枝*。
- en: 'Game trees are two-dimensional: they have width and depth. The *width* is the
    number of possible moves from a given board position. The *depth* is the number
    of turns from a board position to a final game state—a possible end of the game.
    Within a game, both these quantities vary from turn to turn.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏树是二维的：它们有宽度和深度。*宽度*是从给定棋盘位置的可能走法数量。*深度*是从棋盘位置到最终游戏状态（可能的游戏结束）的回合数。在游戏中，这两个数量在每一回合都会变化。
- en: You generally estimate the size of the tree by thinking of the typical width
    and typical depth for a particular game. The number of board positions in a game
    tree is roughly given by the formula *W^d*, where *W* is the average width and
    *d* is the average depth. [Figures 4.6](#ch04fig06) and [4.7](#ch04fig07) illustrate
    the width and depth of a tic-tac-toe game tree. For example, in chess, a player
    normally has about 30 options per move, and a game lasts about 80 moves; the size
    of the tree is something like 30^(80) ≈ 10^(118) positions. Go typically has 250
    legal moves per turn, and a game might last 150 turns. This gives a game tree
    size of 250^(150) ≈ 10^(359) positions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常通过考虑特定游戏的典型宽度和典型深度来估计树的大小。游戏树中的棋盘位置数量大致由公式 *W^d* 给出，其中 *W* 是平均宽度，*d* 是平均深度。[图
    4.6](#ch04fig06) 和 [4.7](#ch04fig07) 展示了负三子棋游戏树的宽度和深度。例如，在国际象棋中，玩家通常每步有大约 30 种选择，游戏大约持续
    80 步；树的大小大约是 30^(80) ≈ 10^(118) 个位置。围棋通常每轮有 250 种合法移动，游戏可能持续 150 轮。这给出了一个游戏树大小为
    250^(150) ≈ 10^(359) 个位置。
- en: 'Figure 4.6\. The width of a tic-tac-toe game tree: the maximum width is 9,
    because you have 9 possible options on the first move. But the number of legal
    moves decreases on each turn, so the average width is 4 or 5 moves.'
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6\. 负三子棋游戏树的宽度：最大宽度为 9，因为第一步你有 9 种可能的选择。但随着每一轮的进行，合法移动的数量会减少，所以平均宽度为 4 或
    5 次移动。
- en: '![](Images/04fig06_alt.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig06_alt.jpg)'
- en: 'Figure 4.7\. The depth of a tic-tac-toe game tree: the maximum depth is 9 moves;
    after that, the board is full.'
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.7\. 负三子棋游戏树的深度：最大深度为 9 次移动；之后，棋盘就满了。
- en: '![](Images/04fig07.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig07.jpg)'
- en: 'This formula, *W^d*, is an example of exponential growth: the number of positions
    to consider grows quickly as you increase the search depth. Imagine a game with
    an average width and depth of about 10\. The full game tree would contain 10^(10),
    or 10 billion, board positions to search.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式，*W^d*，是指数增长的例子：随着搜索深度的增加，需要考虑的位置数量会迅速增长。想象一下一个平均宽度和深度约为 10 的游戏。完整的游戏树将包含
    10^(10)，即 10 亿，个棋盘位置需要搜索。
- en: Now suppose you come up with modest pruning schemes. First, you figure out how
    to quickly eliminate two moves from consideration on each turn, reducing the effective
    width to 8\. Second, you decide you can figure out the game result by looking
    just 9 moves ahead instead of 10\. This leaves you 8⁹ positions to search, or
    around 130 million. Compared to a full search, you’ve eliminated more than 98%
    of the computation! The key takeaway is that even slightly reducing the width
    or depth of your search can massively reduce the time required to select a move.
    [Figure 4.8](#ch04fig08) illustrates the impact of pruning on a small tree.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你提出了适度的剪枝方案。首先，你找出如何快速消除每一轮考虑的两个移动，将有效宽度减少到 8。其次，你决定你可以通过查看 9 次而不是 10 次来推断游戏结果。这让你有
    8⁹ 个位置需要搜索，大约为 1.3 亿。与全搜索相比，你已经消除了超过 98% 的计算！关键点是，即使稍微减少搜索的宽度或深度，也可以大幅减少选择移动所需的时间。[图
    4.8](#ch04fig08) 展示了剪枝对小型树的影响。
- en: Figure 4.8\. Pruning can quickly shrink a game tree. This tree has width 4 and
    height 3, for a total of 64 leaves to examine. Suppose you find a way to eliminate
    1 of the 4 possible options on each turn. Then you end up with just 27 leaves
    to visit.
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.8\. 剪枝可以快速缩小游戏树。这个树宽度为 4，高度为 3，总共有 64 个叶子需要检查。假设你找到了一种方法，在每一轮中消除 4 种可能选择中的
    1 种。那么你最终只需要访问 27 个叶子。
- en: '![](Images/04fig08_alt.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig08_alt.jpg)'
- en: 'In this section, we cover two pruning techniques: *position evaluation functions*
    for reducing the search depth, and *alpha-beta pruning* for reducing the search
    width. The two techniques work together to form the backbone of classical board-game
    AI.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了两种剪枝技术：用于减少搜索深度的 *位置评估函数*，以及用于减少搜索宽度的 *alpha-beta 剪枝*。这两种技术共同构成了经典棋类人工智能的骨架。
- en: 4.4.1\. Reducing search depth with position evaluation
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 使用位置评估减少搜索深度
- en: If you follow a game tree all the way to the end of the game, you can calculate
    the winner. What about earlier in the game? Human players normally have a sense
    of who is leading throughout the midgame. Even beginner Go players have an instinctive
    feel for whether they’re bossing their opponent around or scrambling to survive.
    If you can capture that sense in a computer program, you can reduce the depth
    that you need to search. A function that mimics this sense of who is leading,
    and by how much, is a *position evaluation function*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将游戏树一直追踪到游戏结束，你可以计算出胜者。那么游戏早期呢？人类玩家通常在整个中盘阶段都有一种领先的感觉。即使是围棋初学者也有一种本能的感觉，知道他们是控制对手还是挣扎求生。如果你能在计算机程序中捕捉到这种感觉，你可以减少需要搜索的深度。一个模仿这种领先感觉及其程度的函数是一个*位置评估函数*。
- en: 'For many games, the position evaluation function can be handcrafted by using
    knowledge of the game. For example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多游戏，位置评估函数可以通过使用游戏知识手工制作。例如：
- en: '***Checkers*—** Count one point for each regular piece on the board, plus two
    points for each king. Take the value of your pieces and subtract the value of
    your opponent’s.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***跳棋*—** 计算棋盘上每个普通棋子的价值，加上每个国王两分。计算你棋子的价值并减去你对手棋子的价值。'
- en: '***Chess*—** Count one point for each pawn, three points for each bishop or
    knight, five points for each rook, and nine points for the queen. Take the value
    of your pieces and subtract the value of your opponent’s.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***国际象棋*—** 每个兵算一分，每个象或马算三分，每个车算五分，而皇后算九分。计算你棋子的价值并减去你对手棋子的价值。'
- en: These evaluation functions are highly simplified; top checkers and chess engines
    use much more sophisticated heuristics. But in both cases, the AI will have the
    incentive to capture the opponent’s pieces and preserve its own. Furthermore,
    it’ll be willing to trade its weaker pieces to capture a stronger one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评估函数高度简化；顶尖的跳棋和象棋引擎使用更复杂的启发式方法。但在两种情况下，AI 都有动机捕获对手的棋子并保留自己的棋子。此外，它愿意用较弱的棋子交换以捕获更强的棋子。
- en: In Go, the equivalent heuristic is to add up the stones you’ve captured and
    then subtract the number of stones your opponent has captured. (Equivalently,
    you can count the difference in the number of stones on the board.) [Listing 4.8](#ch04ex08)
    calculates that heuristic. It turns out this isn’t an effective evaluation function.
    In Go, the *threat* of capturing stones is much more important than *actually*
    capturing them. It’s quite common for a game to last for 100 turns or more before
    any stones are captured. Crafting a board evaluation function that accurately
    captures the nuances of the game state turns out to be extremely difficult.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在围棋中，等效的启发式方法是计算你捕获的棋子总数，然后减去你对手捕获的棋子数。（等价地，你可以计算棋盘上棋子数量的差异。）[列表 4.8](#ch04ex08)
    计算了这个启发式方法。结果发现这并不是一个有效的评估函数。在围棋中，*威胁*捕获棋子的重要性远大于*实际*捕获棋子。一个游戏持续100回合或更长时间才出现任何棋子被捕获的情况是很常见的。制作一个能够准确捕捉游戏状态细微差异的棋盘评估函数证明是极其困难的。
- en: That said, you can use this too-simple heuristic for the purpose of illustrating
    pruning techniques. This isn’t going to produce a strong bot, but it’s better
    than picking moves completely at random. In [chapters 11](kindle_split_023.xhtml#ch11)
    and [12](kindle_split_024.xhtml#ch12), we cover how to use deep learning to generate
    a better evaluation function.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，你可以使用这个过于简单的启发式方法来演示剪枝技术。这不会产生一个强大的机器人，但比完全随机选择动作要好。在第 [11](kindle_split_023.xhtml#ch11)
    章和 [12](kindle_split_024.xhtml#ch12) 章中，我们介绍了如何使用深度学习来生成更好的评估函数。
- en: After you’ve chosen an evaluation function, you can implement *depth pruning*.
    Instead of searching all the way to the end of the game and seeing who won, you
    search a fixed number of moves ahead and use the evaluation function to estimate
    who is more likely to win.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在你选择了一个评估函数之后，你可以实现*深度剪枝*。你不需要搜索到游戏结束并看到谁赢了，而是搜索固定数量的步数，并使用评估函数来估计谁更有可能获胜。
- en: Listing 4.8\. A highly simplified board evaluation heuristic for Go
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.8\. 围棋的一个高度简化的棋盘评估启发式方法
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1* Calculate the difference between the number of black stones and white
    stones on the board. This will be the same as the difference in the number of
    captures, unless one player passes early.**'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 计算棋盘上黑子与白子的数量差异。除非一方提前弃权，否则这将是捕获数量差异相同。**'
- en: '***2* If it’s black’s move, return (black stones) – (white stones).**'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果是黑方的回合，返回（黑子数）-（白子数）。**'
- en: '***3* If it’s white’s move, return (white stones) – (black stones).**'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果是白方的回合，返回（白方棋子数）-（黑方棋子数）。**'
- en: '[Figure 4.9](#ch04fig09) shows a partial game tree with depth pruning. (We’ve
    left most of the branches out of the diagram to save space, but the algorithm
    would examine those too.)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.9](#ch04fig09) 展示了一个带有深度剪枝的局部棋树。（我们为了节省空间，省略了大部分分支，但算法也会检查这些分支。）'
- en: Figure 4.9\. A partial Go game tree. Here you search the tree to a depth of
    2 moves ahead. At that point, you look at the number of captures to evaluate the
    board. If black chooses the rightmost branch, white can capture a stone, yielding
    an evaluation of –1 for black. If black chooses the center branch, the black stone
    is safe (for now). That branch is evaluated at a score of 0\. Therefore, black
    chooses the center branch.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.9\. 一个部分围棋棋树。在这里，你搜索到2步棋的深度。在那个点上，你查看捕获的棋子数量来评估棋盘。如果黑方选择最右侧的分支，白方可以吃掉一个棋子，给黑方的评估结果是-1。如果黑方选择中间的分支，黑方的棋子现在安全（暂时如此）。这个分支的评估分数是0。因此，黑方选择中间的分支。
- en: '![](Images/04fig09_alt.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig09_alt.jpg)'
- en: In this tree, you’re searching to a depth of 2 moves ahead, and using the number
    of captured stones as the board evaluation function. The original position shows
    black to play; black has a stone with just one liberty. What should black do?
    If black extends straight down, as shown in the middle branch, the stone is safe
    (for now). If black plays anywhere else, white can capture the stone—the left
    branch shows one of the many ways that can happen.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个树中，你搜索到2步棋的深度，并使用捕获的棋子数量作为棋盘评估函数。原始位置显示黑方走棋；黑方有一个只有一活路的棋子。黑方应该怎么做？如果黑方像中间分支所示那样直接向下延伸，棋子现在安全（暂时如此）。如果黑方在其他地方走棋，白方可以吃掉这个棋子——左分支显示了这种情况可能发生的一种方式。
- en: After looking two moves ahead, you apply your evaluation function to the position.
    In this case, any branch where white captures the stone is scored at +1 for white
    and –1 for black. All other branches have a score of 0 (there’s no other way to
    capture a stone in two turns). In this case, black picks the only move that protects
    the stone.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在看两步棋之后，你将评估函数应用于位置。在这种情况下，任何白方捕获棋子的分支都会给白方+1分，给黑方-1分。所有其他分支的分数都是0（在两步内没有其他方式可以捕获棋子）。在这种情况下，黑方选择唯一一个保护棋子的走法。
- en: '[Listing 4.9](#ch04ex09) shows how to implement depth pruning. The code looks
    similar to the full minimax code in [listing 4.7](#ch04ex07): it may be helpful
    to compare them side by side. Note the differences:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.9](#ch04ex09) 展示了如何实现深度剪枝。代码看起来与[列表 4.7](#ch04ex07)中的完整 minimax 代码相似：可能有助于将它们并排比较。注意它们之间的差异：'
- en: 'Instead of returning a win/lose/draw enum, you return a number indicating the
    value of your board evaluation function. Our convention is that the score is from
    the perspective of the player who has the next turn: a large score means the player
    who has the next move expects to win. When you evaluate the board from your opponent’s
    perspective, you multiply the score by –1 to flip back to your perspective.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而不是返回一个赢/输/平局的枚举值，你返回一个表示你的棋盘评估函数值的数字。我们的约定是分数是从下一个回合玩家的角度出发的：高分意味着下一个回合的玩家预计会赢。当你从对手的角度评估棋盘时，你将分数乘以-1以转换回你的视角。
- en: The `max_depth` parameter controls the number of moves you want to search ahead.
    At each turn, you subtract 1 from this value.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth` 参数控制你想要搜索的步数。在每一回合，你从这个值中减去1。'
- en: When `max_depth` hits 0, you stop searching and call your board evaluation function.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 `max_depth` 达到0时，你停止搜索并调用你的棋盘评估函数。
- en: Listing 4.9\. Depth-pruned minimax search
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.9\. 深度剪枝的 minimax 搜索
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* If the game is already over, you know who the winner is.**'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 如果游戏已经结束，你知道赢家是谁。**'
- en: '***2* You’ve reached your maximum search depth. Use your heuristic to decide
    how good this sequence is.**'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 你已经达到了最大搜索深度。使用你的启发式算法来决定这个序列的好坏。**'
- en: '***3* Loop over all possible moves.**'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历所有可能的走法。**'
- en: '***4* See what the board would look like if you play this move.**'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 看看如果你走这步棋，棋盘会是什么样子。**'
- en: '***5* Find the opponent’s best result from this position.**'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 从这个位置找到对手的最佳结果。**'
- en: '***6* Whatever your opponent wants, you want the opposite.**'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 无论对手想要什么，你想要的是相反的。**'
- en: '***7* See if this is better than the best result you’ve seen so far.**'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 看看这是否比迄今为止看到的最优结果更好。**'
- en: Feel free to experiment with your own evaluation functions. It can be fun to
    see how they affect your bot’s personality, and you can certainly do a bit better
    than our simple example.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试你自己的评估函数。看看它们如何影响你的机器人性格可能会很有趣，而且你肯定可以比我们的简单例子做得更好。
- en: 4.4.2\. Reducing search width with alpha-beta pruning
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 使用 alpha-beta 剪枝减少搜索宽度
- en: Look at the diagram in [figure 4.10](#ch04fig10). It’s black’s move, and you’re
    considering playing on the point marked with a square. If you do so, white can
    then play at A to capture four stones. Clearly that’s a disaster for black! What
    if white replies at B instead? Well, who cares? White’s response at A is bad enough
    already. From black’s perspective, you don’t really care if A is the absolute
    best move white can pick. As soon as you find one powerful response, you can reject
    playing at the point marked with a square and move on to the next option. That’s
    the idea behind *alpha-beta pruning*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下 [图 4.10](#ch04fig10) 中的图。现在是黑子的回合，你正在考虑在用方框标记的点进行落子。如果你这样做，白色可以在 A 点进行回应，从而吃掉四颗棋子。显然这对黑色来说是个灾难！如果白色在
    B 点回应呢？好吧，谁在乎呢？白色在 A 点的回应已经足够糟糕了。从黑子的角度来看，你并不真的在乎 A 是否是白色能选择的绝对最佳走法。一旦你找到一个强有力的回应，你就可以拒绝在方框标记的点落子，并转向下一个选项。这就是
    *alpha-beta 剪枝* 的理念。
- en: Figure 4.10\. The black player is considering playing at the point marked with
    a square. If black plays there, white can respond at A to capture four stones.
    That result is so bad for black that you can immediately reject playing on the
    square; there’s no need for black to consider other white responses, such as B.
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.10\. 黑色玩家正在考虑在用方框标记的点进行落子。如果黑色在那里落子，白色可以在 A 点进行回应，从而吃掉四颗棋子。这个结果对黑色来说非常糟糕，以至于你可以立即拒绝在方框上落子；黑色没有必要考虑其他白色的回应，例如
    B。
- en: '![](Images/04fig10_alt.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig10_alt.jpg)'
- en: Let’s walk through how the alpha-beta algorithm would apply to that position.
    Alpha-beta pruning starts like a regular depth-pruned tree search. [Figure 4.11](#ch04fig11)
    shows the first step. You pick the first move to evaluate for black; that move
    is marked with an A in the diagram. Then you fully evaluate that move up to a
    depth of 3\. You can see that no matter how white responds, black can capture
    at least two stones. So you evaluate this branch to a score of +2 for black.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 alpha-beta 算法如何应用于这个位置。Alpha-beta 剪枝开始就像一个常规的深度剪枝树搜索。[图 4.11](#ch04fig11)
    展示了第一步。你选择第一个评估的黑子走法；这个走法在图中用 A 标记。然后你完全评估这个走法，直到深度为 3。你可以看到无论白色如何回应，黑色至少可以吃掉两颗棋子。所以你将这个分支评估为黑子
    +2 分。
- en: 'Figure 4.11\. Step 1 in alpha-beta pruning: you fully evaluate the first possible
    black move; this move is evaluated at a score of +2 for black. So far, the algorithm
    is exactly the same as the depth-pruned search covered in the previous section.'
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.11\. Alpha-beta 剪枝的第 1 步：你完全评估了第一个可能的黑子走法；这个走法被评估为黑子 +2 分。到目前为止，算法与上一节中介绍的深度剪枝搜索完全相同。
- en: '![](Images/04fig11_alt.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig11_alt.jpg)'
- en: Now you consider the next candidate move for black, marked as B in [figure 4.12](#ch04fig12).
    Just as in the depth-pruned search, you look at all possible white responses and
    evaluate them one by one. White can play in the upper-left corner to capture four
    stones; that branch gets evaluated at a score of –4 for black. Now, you already
    know that if black plays at A, black is guaranteed a score of at least +2\. If
    black plays at B, you just saw how white can hold black to a score of –4; possibly
    white can do even better. But because –4 is already worse than +2, there’s no
    need to search further. You can skip evaluating a dozen other white responses—and
    each of those positions has many more combinations after it. The amount of computation
    you save adds up quickly, and you still select the exact same move that you would’ve
    chosen with a full search to depth 3.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑黑子的下一个候选走法，标记为 B 在 [图 4.12](#ch04fig12) 中。就像在深度剪枝搜索中一样，你查看所有可能的白子回应并逐一评估它们。白色可以在左上角落子以吃掉四颗棋子；这个分支被评估为黑子
    –4 分。现在，你已经知道如果黑色在 A 点落子，黑色至少可以保证得到 +2 分。如果黑色在 B 点落子，你刚刚看到了白色如何将黑色限制在 –4 分；白色可能做得更好。但是因为
    –4 已经比 +2 差，所以没有必要进一步搜索。你可以跳过评估其他十几个白子回应——而且每个位置后面都有更多组合。你节省的计算量很快就会累积起来，而你仍然会选择与深度搜索到
    3 层时完全相同的走法。
- en: 'Figure 4.12\. Step 2 in alpha-beta pruning: you now evaluate the second possible
    black move. Here, white has a response that captures four stones. That branch
    evaluates to –4 for black. As soon as you evaluate that white response, you can
    discard this move for black entirely and skip the other possible white responses.
    It’s possible that white has an even better reply that you didn’t evaluate, but
    all you need to know is that playing at B is worse for black than playing at A.'
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.12\. alpha-beta 剪枝的第二步：现在评估第二个可能的黑方走法。在这里，白方有一个回应，可以吃掉四颗棋子。这个分支对黑方的评估结果是
    –4。一旦评估了白方的回应，你就可以完全放弃这个黑方的走法，并跳过其他可能的白方回应。可能白方有一个更好的回应你没有评估，但你需要知道的是，在 B 点下棋对黑方来说比在
    A 点下棋更糟。
- en: '![](Images/04fig12.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig12.jpg)'
- en: For the purposes of this example, we chose a specific order to evaluate the
    moves to illustrate how the pruning works. Our actual implementation evaluates
    moves in the order of their board coordinates. The time savings you get by alpha-beta
    pruning depends on how quickly you find good branches. If you happen to evaluate
    the best branches early on, you can eliminate the other branches quickly. In the
    worst case, you evaluate the best branch last, and then alpha-beta pruning is
    no faster than a full depth-pruned search.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们选择了一个特定的顺序来评估走法，以说明剪枝是如何工作的。我们的实际实现是按照棋盘坐标的顺序评估走法的。通过 alpha-beta 剪枝获得的时间节省取决于你多快找到好的分支。如果你碰巧早期就评估了最好的分支，你可以快速消除其他分支。在最坏的情况下，你最后评估最好的分支，那么
    alpha-beta 剪枝并不比全深度剪枝搜索更快。
- en: To implement the algorithm, you must track the best result so far for each player
    throughout your search. These values are traditionally called *alpha* and *beta*,
    which is where the name of the algorithm comes from. In our implementation, we
    call those values `best_black` and `best_white`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现该算法，你必须在搜索过程中跟踪每个玩家的最佳结果。这些值传统上被称为 *alpha* 和 *beta*，这也是算法名称的由来。在我们的实现中，我们称这些值为
    `best_black` 和 `best_white`。
- en: Listing 4.10\. Checking whether you can stop evaluating a branch
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.10\. 检查你是否可以停止评估一个分支
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1* Updates your benchmark for white**'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 更新白方的基准。**'
- en: '***2* You’re picking a move for white; it needs to be only strong enough to
    eliminate black’s previous move. As soon as you find something that defeats black’s
    best option, you can stop searching.**'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 你正在为白方选择走法；它只需要足够强大，能够消除黑方的上一个走法。一旦你找到了能够击败黑方最佳选择的东西，你就可以停止搜索。**'
- en: You can extend your implementation of depth pruning to include alpha-beta pruning
    as well. [Listing 4.10](#ch04ex10) shows the key new addition. This block is implemented
    from white’s perspective; you need a similar block for black.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将深度剪枝的实现扩展到包括 alpha-beta 剪枝。[列表 4.10](#ch04ex10) 显示了关键的新添加。这个块是从白方的角度实现的；你需要一个类似的块来为黑方实现。
- en: First, you check whether you need to update the `best_white` score. Next, you
    check whether you can stop evaluating moves for white. You do this by comparing
    the current score to the best score you’ve found for black in *any* branch. If
    white can hold black to a lower score, black won’t choose this branch; you don’t
    need to find the absolute best score.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查你是否需要更新 `best_white` 分数。接下来，检查你是否可以停止评估白方的走法。你通过比较当前分数与你在 *任何* 分支中找到的黑方的最佳分数来进行。如果白方可以将黑方的分数限制在一个更低的水平，黑方不会选择这个分支；你不需要找到绝对的最佳分数。
- en: The full implementation of alpha-beta pruning is shown in the following listing.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: alpha-beta 剪枝的完整实现如下所示。
- en: Listing 4.11\. Full implementation of alpha-beta pruning
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.11\. alpha-beta 剪枝的完整实现
- en: '[PRE10]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1* Check if the game is already over.**'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 检查游戏是否已经结束。**'
- en: '***2* You’ve reached your maximum search depth. Use your heuristic to decide
    how good this sequence is.**'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 你已经达到了最大搜索深度。使用你的启发式方法来决定这个序列有多好。**'
- en: '***3* Loop over all valid moves.**'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历所有有效的走法。**'
- en: '***4* See what the board would look like if you play this move.**'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 看看如果你走这步棋，棋盘会是什么样子。**'
- en: '***5* Find out your opponent’s best result from that position.**'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 从那个位置找出对手的最佳结果。**'
- en: '***6* Whatever your oppo-nent wants, you want the opposite.**'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 无论你的对手想要什么，你想要的是相反的。**'
- en: '***7* See whether this result is better than the best you’ve seen so far.**'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 看看这个结果是否比迄今为止看到的最优结果更好。**'
- en: '***8* Update your benchmark for white.**'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 更新白方的基准。**'
- en: '***9* You’re picking a move for white; it needs to be only strong enough to
    eliminate black’s previous move.**'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 你正在为白方选择走法；它只需要足够强大，能够消除黑方的上一个走法。**'
- en: '***10* Update your benchmark for black.**'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 更新黑方的基准。**'
- en: '***11* You’re picking a move for black; it needs to be only strong enough to
    eliminate white’s previous move.**'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 你正在为黑方选择一步棋；它只需要足够强大，以消除白方的上一步棋。**'
- en: 4.5\. Evaluating game states with Monte Carlo tree search
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 使用蒙特卡洛树搜索评估游戏状态
- en: 'For alpha-beta pruning, you used a position evaluation function to help reduce
    the number of positions you had to consider. But position evaluation in Go is
    very, very hard: your simple heuristic based on captures won’t fool many Go players.
    *Monte Carlo tree search* (MCTS) provides a way to evaluate a game state without
    *any* strategic knowledge about the game. Instead of a game-specific heuristic,
    the MCTS algorithm simulates random games to estimate how good a position is.
    One of these random games is called a *rollout* or a *playout*. In this book,
    we use the term *rollout*.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 alpha-beta 剪枝，你使用位置评估函数来帮助减少你必须考虑的位置数量。但在围棋中，位置评估非常、非常困难：你基于捕获的简单启发式方法不会欺骗许多围棋玩家。*蒙特卡洛树搜索*（MCTS）提供了一种评估游戏状态的方法，而无需对游戏有任何战略知识。MCTS
    算法不是使用特定于游戏的启发式方法，而是通过模拟随机游戏来估计位置的好坏。其中一种随机游戏被称为 *模拟游戏* 或 *模拟走法*。在这本书中，我们使用术语
    *模拟游戏*。
- en: Monte Carlo tree search is part of the larger family of *Monte Carlo algorithms,*
    which use randomness to analyze extremely complex situations. The element of randomness
    inspired the name, an allusion to the famous casino district in Monaco.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索是更大类别的 *蒙特卡洛算法* 的一部分，这些算法使用随机性来分析极其复杂的情况。随机性的元素启发了这个名字，是对摩纳哥著名的赌场区的暗喻。
- en: It may seem impossible that you can build a good strategy out of picking random
    moves. A game AI that chooses completely random moves is going to be extremely
    weak, of course. But when you pit two random AIs against each other, the opponent
    is equally clueless. If black consistently wins more often than white, it must
    be because black started with an advantage. Therefore, you can figure out whether
    a position gives one player an advantage by starting random games from there.
    And you don’t need any understanding of *why* the position is good to do this.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从随机移动中构建出良好策略似乎是不可能的。当然，完全随机移动的游戏 AI 会非常弱。但是当你让两个随机 AI 相互对抗时，对手同样毫无头绪。如果黑方比白方赢得更多，那一定是因为黑方一开始就占据了优势。因此，你可以通过从那里开始随机游戏来找出某个位置是否给一方提供了优势。而且你不需要对为什么这个位置是好的有任何理解就可以做到这一点。
- en: 'It’s possible to get unbalanced results by chance. If you simulate 10 random
    games and white wins seven, how confident can you be that white had an advantage?
    Not very: white has won only two more games than you’d expect by chance. If black
    and white were perfectly balanced, there’s about a 30% chance of seeing a 7-out-of-10
    result. On the other hand, if white wins 70 out of 100 random games, you can be
    virtually certain that the starting position did favor white. The key idea is
    that your estimate gets more accurate as you do more rollouts.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能由于偶然而得到不平衡的结果。如果你模拟了 10 个随机游戏，白方赢了 7 个，你有多大的信心认为白方有优势？不太大：白方只比随机预期多赢了 2 个游戏。如果黑方和白方完全平衡，看到
    7 比 10 的结果大约有 30% 的可能性。另一方面，如果白方在 100 个随机游戏中赢了 70 个，你可以几乎肯定起始位置确实对白方有利。关键思想是，随着你进行更多的模拟游戏，你的估计会变得更加准确。
- en: 'Each round of the MCTS algorithm takes three steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: MCTS 算法的每一轮包含三个步骤：
- en: Add a new board position to the MCTS tree.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个新的棋盘位置添加到 MCTS 树中。
- en: Simulate a random game from that position.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从该位置模拟一个随机游戏。
- en: Update the tree statistics with the results of that random game.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机游戏的结果更新树统计信息。
- en: You repeat this process as many times as you can in the time available. Then
    the statistics at the top of the tree tell you which move to pick.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你尽可能地重复这个过程。然后树顶部的统计数据告诉你选择哪个移动。
- en: Let’s step through a single round of the MCTS algorithm. [Figure 4.13](#ch04fig13)
    shows an MCTS tree. At this point in the algorithm, you’ve already completed a
    number of rollouts and built up a partial tree. Each node tracks the counts of
    who won the rollouts that started from any board position after that node. Every
    node’s count includes the sum of all its children. (Normally, the tree would have
    many more nodes at this point; in the diagram, we’ve omitted many of the nodes
    to save space.)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析 MCTS 算法的一轮。图 4.13 [图 4.13](#ch04fig13) 展示了一个 MCTS 树。在这个算法的当前点，你已经完成了一系列的模拟游戏并构建了一个部分树。每个节点跟踪从该节点之后任何棋盘位置开始的模拟游戏的胜负计数。每个节点的计数包括其所有子节点的总和。（通常，在这个点上树会有更多的节点；在图中，我们省略了许多节点以节省空间。）
- en: Figure 4.13\. An MCTS game tree. The top of the tree represents the current
    board position; you’re trying to find the next move for black. At this point in
    the algorithm, you’ve performed 70 random rollouts from various possible positions.
    Each node tracks statistics on all the rollouts that started from any of its children.
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.13\. MCTS 游戏树。树的顶部代表当前的棋盘位置；你正在尝试找到黑方的下一个移动。在这个算法的这一点，你已经从各种可能的位置进行了 70
    次随机 rollout。每个节点跟踪从其任何子节点开始的全部 rollout 的统计数据。
- en: '![](Images/04fig13_alt.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4.13](Images/04fig13_alt.jpg)'
- en: At each round, you add a new game position to the tree. First, you pick a node
    at the bottom of the tree (a *leaf*) where you want to add a new child. This tree
    has five leaves. To get the best results, you need to be a little careful about
    the way you pick a leaf; [section 4.5.2](#ch04lev2sec4) covers a good strategy
    for doing so. For now, just suppose that you walked all the way down the leftmost
    branches. From that point, you randomly pick the next move, calculate the new
    board position, and add that node to the tree. [Figure 4.14](#ch04fig14) shows
    what the tree looks like after that process.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一轮中，你将一个新的游戏位置添加到树中。首先，你选择树底部的节点（一个 *叶节点*），你想要在这里添加一个新的子节点。这个树有五个叶节点。为了获得最佳结果，你需要对选择叶节点的方式稍微小心一些；[第
    4.5.2 节](#ch04lev2sec4) 讨论了这样做的一个好策略。现在，假设你已经沿着最左边的分支走到底。从那个点开始，你随机选择下一个移动，计算新的棋盘位置，并将该节点添加到树中。[图
    4.14](#ch04fig14) 展示了该过程之后树的样子。
- en: Figure 4.14\. Adding a new node to an MCTS tree. Here you select the leftmost
    branch as the place to insert a new node. You then randomly select the next move
    from the position to create the new node in the tree.
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.14\. 向 MCTS 树中添加一个新节点。在这里，你选择最左边的分支作为插入新节点的地方。然后，你从这个位置随机选择下一个移动来在树中创建新节点。
- en: '![](Images/04fig14_alt.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4.14](Images/04fig14_alt.jpg)'
- en: The new node in the tree is the starting point for a random game. You simulate
    the rest of the game, literally just selecting any legal play at each turn, until
    the game is over. Then you count up the score and find the winner. In this case,
    let’s suppose the winner is white. You record the result of this rollout in the
    new node. In addition, you walk up to all the node’s ancestors and add the new
    rollout to their counts as well. [Figure 4.15](#ch04fig15) shows what the tree
    looks like after this step is complete.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 树中的新节点是随机游戏的起点。你模拟剩余的游戏，实际上就是每一步随机选择任何合法的走法，直到游戏结束。然后你计算分数并找出赢家。在这个例子中，假设赢家是白方。你将这个
    rollout 的结果记录在新节点中。此外，你向上走到所有节点的祖先节点，并将新的 rollout 添加到它们的计数中。[图 4.15](#ch04fig15)
    展示了这一步完成后树的样子。
- en: Figure 4.15\. Updating MCTS after a new rollout. In this scenario, the rollout
    is a win for white. You add that win into the new tree node and all its parent
    nodes.
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.15\. 在新的 rollout 后更新 MCTS。在这个场景中，rollout 对白方是胜利。你将这个胜利添加到新的树节点及其所有父节点中。
- en: '![](Images/04fig15_alt.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4.15](Images/04fig15_alt.jpg)'
- en: That whole process is a single round of MCTS. Every time you repeat it, the
    tree gets bigger, and the estimates at the top get more accurate. Normally, you
    stop after a fixed number of rounds or after a fixed amount of time passes. At
    that point, you select the move that has the highest winning percentage.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程是 MCTS 的一次单轮搜索。每次重复这个过程，树会变得更大，顶部的估计也会更加准确。通常，你会在固定轮数或固定时间过后停止。到那时，你选择具有最高胜率的移动。
- en: 4.5.1\. Implementing Monte Carlo tree search in Python
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1\. 在 Python 中实现蒙特卡洛树搜索
- en: Now that you’ve walked through the MCTS algorithm, let’s look at the implementation
    details. First, you’ll design a data structure to represent the MCTS tree. Next,
    you’ll write a function to execute the MCTS rollouts.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经走过了 MCTS 算法，让我们看看实现细节。首先，你将设计一个数据结构来表示 MCTS 树。接下来，你将编写一个函数来执行 MCTS rollout。
- en: 'As shown in [listing 4.12](#ch04ex12), you start by defining a new class, `MCTSNode`,
    to represent any node in your tree. Each `MCTSNode` will track the following properties:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [代码清单 4.12](#ch04ex12) 所示，你首先定义一个新的类，`MCTSNode`，来表示树中的任何节点。每个 `MCTSNode` 将跟踪以下属性：
- en: '`game_state`—The current state of the game (board position and next player)
    at this node in the tree.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`game_state`—树中此节点的当前游戏状态（棋盘位置和下一位玩家）。'
- en: '`parent`—The parent `MCTSNode` that led to this one. You can set `parent` to
    `None` to indicate the root of the tree.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parent`—导致此节点的父 `MCTSNode`。你可以将 `parent` 设置为 `None` 来表示树的根。'
- en: '`move`—The last move that directly led to this node.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`move`—直接导致此节点的最后一个移动。'
- en: '`children`—A list of all child nodes in the tree.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`children`—树中所有子节点的列表。'
- en: '`win_counts` and `num_rollouts`—Statistics about the rollouts that started
    from this node.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`win_counts` 和 `num_rollouts`—从该节点开始的滚动统计信息。'
- en: '`unvisited_moves`—A list of all legal moves from this position that aren’t
    yet part of the tree. Whenever you add a new node to the tree, you pull one move
    out of `unvisited_moves`, generate a new `MCTSNode` for it, and add it to the
    `children` list.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unvisited_moves`—一个列表，包含从当前位置出发的所有合法移动，这些移动尚未成为树的一部分。每次你向树中添加一个新节点时，你都会从`unvisited_moves`中取出一个移动，为它生成一个新的`MCTSNode`，并将其添加到`children`列表中。'
- en: Listing 4.12\. A data structure to represent an MCTS tree
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.12\. 用于表示 MCTS 树的数据结构
- en: '[PRE11]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: An `MCTSNode` can be modified in two ways. You can add a new child to the tree,
    and you can update its rollout stats. The following listing shows both functions.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`MCTSNode` 可以通过两种方式修改。你可以向树中添加一个新的子节点，并可以更新其滚动统计信息。以下列表显示了这两个函数。'
- en: Listing 4.13\. Methods to update a node in an MCTS tree
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.13\. 更新 MCTS 树中节点的方法
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, you add three convenience methods to access useful properties of your
    tree node:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你添加了三个方便的方法来访问树节点的有用属性：
- en: '`can_add_child` reports whether this position has any legal moves that haven’t
    yet been added to the tree.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`can_add_child` 报告此位置是否有任何尚未添加到树中的合法移动。'
- en: '`is_terminal` reports whether the game is over at this node; if so, you can’t
    search any further from here.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_terminal` 报告在此节点处游戏是否结束；如果是，你从这里就不能再进一步搜索了。'
- en: '`winning_frac` returns the fraction of rollouts that were won by a given player.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winning_frac` 返回给定玩家赢得的滚动比例。'
- en: These functions are implemented in the following listing.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数在以下列表中实现。
- en: Listing 4.14\. Helper methods to access useful MCTS tree properties
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.14\. 访问有用的 MCTS 树属性的辅助方法
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Having defined the data structure for the tree, you can now implement the MCTS
    algorithm. You start by creating a new tree. The root node is the current game
    state. Then you repeatedly generate rollouts. In this implementation, you loop
    for a fixed number of rounds for each turn; other implementations may run for
    a specific length of time instead.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了树的数据库结构之后，你现在可以实现 MCTS 算法。你首先创建一个新的树。根节点是当前游戏状态。然后你反复生成滚动。在这个实现中，你为每一回合循环固定次数；其他实现可能运行特定长度的而不是固定次数。
- en: Each round begins by walking down the tree until you find a node where you can
    add a child (any board position that has a legal move that isn’t yet in the tree).
    The `select_move` function hides the work of choosing the best branch to explore;
    we cover the details in the next section.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 每一轮都是从树中向下遍历，直到找到一个可以添加子节点（任何具有合法移动且尚未在树中的棋盘位置）。`select_move` 函数隐藏了选择最佳分支进行探索的工作；我们将在下一节中详细介绍。
- en: After you find a suitable node, you call `add_random_child` to pick any follow-up
    move and bring it into the tree. At this point, `node` is a newly created `MCTSNode`
    that has zero rollouts.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 找到合适的节点后，你调用`add_random_child`来选择任何后续移动并将其添加到树中。此时，`node`是一个新创建的`MCTSNode`，它没有滚动。
- en: You now start a rollout from this node by calling `simulate_random_game`. The
    implementation of `simulate_random_game` is identical to the `bot_v_bot` example
    covered in [chapter 3](kindle_split_014.xhtml#ch03).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在从这个节点开始滚动，通过调用`simulate_random_game`。`simulate_random_game`的实现与第 3 章中介绍的`bot_v_bot`示例相同。
- en: Finally, you update the win counts of the newly created node and all its ancestors.
    This whole process is implemented in the following listing.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你更新新创建节点的胜利次数以及所有祖先节点的胜利次数。整个过程在以下列表中实现。
- en: Listing 4.15\. The MCTS algorithm
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.15\. MCTS 算法
- en: '[PRE14]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1* Adds a new child node into the tree**'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在树中添加一个新的子节点**'
- en: '***2* Simulates a random game from this node**'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从此节点模拟随机游戏**'
- en: '***3* Propagates the score back up the tree**'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将分数传播回树**'
- en: After you’ve completed the allotted rounds, you need to pick a move. To do so,
    you just loop over all the top-level branches and pick the one with the best winning
    percentage. The following listing shows how to implement this.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成分配的回合后，你需要选择一个移动。为此，你只需遍历所有顶级分支，并选择具有最佳胜利百分比的分支。以下列表显示了如何实现这一点。
- en: Listing 4.16\. Selecting a move after completing your MCTS rollouts
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.16\. 完成MCTS滚动后选择移动
- en: '[PRE15]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 4.5.2\. How to select which branch to explore
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2\. 如何选择要探索的分支
- en: 'Your game AI has a limited amount of time to spend on each turn, which means
    you can perform only a fixed number of rollouts. Each rollout improves your evaluation
    of a single possible move. Think of your rollouts as a limited resource: if you
    spend an extra rollout on move A, you have to spend one fewer rollout on move
    B. You need a strategy to decide how to allocate your limited budget. The standard
    strategy is called the *upper confidence bound for trees*, or *UCT* formula. The
    UCT formula strikes a balance between two conflicting goals.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你的游戏AI在每一回合上有一定的时间限制，这意味着你只能执行固定数量的模拟。每次模拟都会提高你对单个可能移动的评估。将你的模拟视为一种有限的资源：如果你在移动A上多花费一次模拟，你就要在移动B上少花费一次模拟。你需要一种策略来决定如何分配你的有限预算。标准的策略被称为*树的上置信界*，或*UCT公式*。UCT公式在两个冲突的目标之间取得平衡。
- en: The first goal is to spend your time looking at the best moves. This goal is
    called *exploitation* (you want to exploit any advantage that you’ve discovered
    so far). You’d spend more rollouts on the moves with the highest estimated winning
    percentage. Now, some of those moves have a high winning percentage just by chance.
    But as you complete more rollouts through those branches, your estimates get more
    accurate. The false positives will drop lower down the list.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个目标是把时间花在查看最佳移动上。这个目标被称为*利用*（你想要利用你迄今为止发现的任何优势）。你会在估计获胜率最高的移动上花费更多的模拟。现在，其中一些移动的获胜率很高只是偶然。但随着你通过这些分支完成更多的模拟，你的估计会变得更加准确。错误阳性将会在列表的下方降低。
- en: On the other hand, if you’ve visited a node only a few times, your estimate
    may be way off. By sheer chance, you may have a low estimate for a move that’s
    really good. Spending a few more rollouts there may reveal its true quality. So
    your second goal is to get more accurate evaluations for the branches you’ve visited
    the least. This goal is called *exploration*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你只访问了一个节点几次，你的估计可能会大错特错。纯粹出于偶然，你可能对实际上很好的移动给出了一个较低的估计。在那些节点上多进行几次模拟可能会揭示其真正的质量。因此，你的第二个目标是为你访问最少的分支获得更准确的评估。这个目标被称为*探索*。
- en: '[Figure 4.16](#ch04fig16) compares a search tree biased toward exploitation
    against a tree biased toward exploration. The exploitation-exploration trade-off
    is a common feature of trial-and-error algorithms. It’ll come up again when we
    look at reinforcement learning later in the book.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.16](#ch04fig16)比较了偏向于利用的搜索树与偏向于探索的树。利用-探索权衡是试错算法的常见特征。当我们稍后在本章中查看强化学习时，它还会再次出现。'
- en: 'Figure 4.16\. The exploitation-exploration trade-off. In both game trees, you’ve
    visited seven board positions. On top, the search is biased more toward exploitation:
    the tree is deeper for the most promising moves. On the bottom, the search is
    biased more toward exploration: it tries more moves, but to less depth.'
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.16。利用-探索权衡。在两个游戏树中，你都访问了七个棋盘位置。在上部，搜索更偏向于利用：最有希望的移动的树更深。在下部，搜索更偏向于探索：它尝试了更多的移动，但深度较小。
- en: '![](Images/04fig16_alt.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig16_alt.jpg)'
- en: For each node you’re considering, you compute the winning percentage *w* to
    represent the exploitation goal. To represent exploration, you compute, where
    *N* is the total number of rollouts, and *n* is the number of rollouts that started
    with the node under consideration. This specific formula has a theoretical basis;
    for our purposes, just note that its value will be largest for the nodes you’ve
    visited the least.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你正在考虑的每个节点，你计算获胜率 *w* 来表示利用目标。为了表示探索，你计算，其中 *N* 是模拟的总次数，*n* 是以考虑中的节点开始的模拟次数。这个特定的公式有一个理论基础；就我们的目的而言，只需注意其值将对于你访问最少的节点最大。
- en: '![](Images/p0082_01.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0082_01.jpg)'
- en: 'You combine these two components to get the UCT formula:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你将这两个组件结合起来得到UCT公式：
- en: '![](Images/p0082_02.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0082_02.jpg)'
- en: Here, *c* is a parameter that represents your preferred balance between exploitation
    and exploration. The UCT formula gives you a score for each node, and the node
    with the highest UCT score is the start of the next rollout.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*c* 是一个参数，它代表你希望在利用和探索之间偏好的平衡。UCT公式为每个节点给出一个分数，具有最高UCT分数的节点是下一次模拟的起点。
- en: With a larger value of *c*, you’ll spend more time visiting the least-explored
    nodes. With a smaller value of *c*, you’ll spend more time gathering a better
    evaluation of the most promising node. The choice of *c* that makes the most effective
    game player is usually found via trial and error. We suggest starting somewhere
    around 1.5 and experimenting from there. The parameter *c* is sometimes called
    the *temperature*. When the temperature is “hotter,” your search will be more
    volatile, and when the temperature is “cooler,” your search will be more focused.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当*c*的值较大时，你将花费更多时间访问最少探索的节点。当*c*的值较小时，你将花费更多时间收集对最有希望节点的更好评估。使游戏玩家最有效的*c*的选择通常是通过试错来找到的。我们建议从大约1.5开始，并从那里进行实验。参数*c*有时被称为*温度*。当温度“更高”时，你的搜索将更加波动，而当温度“更低”时，你的搜索将更加专注。
- en: '[Listing 4.17](#ch04ex17) shows how to implement this policy. After you’ve
    identified the metric you want to use, selecting a child is a simple matter of
    calculating the formula for each node and choosing the node with the largest value.
    Just as in minimax search, you need to switch your perspective on each turn. You
    calculate the winning percentage from the point of view of the player who’d pick
    the next move, so that perspective alternates between black and white as you walk
    down the tree.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.17](#ch04ex17)展示了如何实现此策略。在你确定了想要使用的度量标准之后，选择一个子节点就是一个简单的问题，即计算每个节点的公式并选择具有最大值的节点。就像在minimax搜索中一样，你需要在每个回合切换你的视角。你从选择下一个移动的玩家的角度计算胜率，因此随着你沿着树向下走，这个视角在黑和白之间交替。'
- en: Listing 4.17\. Selecting a branch to explore with the UCT formula
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.17\. 使用UCT公式选择要探索的分支
- en: '[PRE16]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 4.5.3\. Applying Monte Carlo tree search to Go
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.3\. 将蒙特卡洛树搜索应用于围棋
- en: In the previous section, you implemented the general form of the MCTS algorithm.
    Straightforward MCTS implementations can reach the amateur 1 dan level for Go,
    the level of a strong amateur player. Combining MCTS with other techniques can
    produce a bot that’s a fair bit stronger than that; many of the top Go AIs today
    use both MCTS and deep learning. If you’re interested in reaching a competitive
    level with your MCTS bot, this section covers some of the practical details to
    consider.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你实现了MCTS算法的通用形式。简单的MCTS实现可以达到围棋业余1段的水平，即一个强大的业余玩家的水平。将MCTS与其他技术相结合可以产生一个比那强得多的机器人；今天许多顶尖的围棋AI都使用了MCTS和深度学习。如果你对你的MCTS机器人达到竞争水平感兴趣，本节涵盖了需要考虑的一些实际细节。
- en: Fast code makes a strong bot
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 快速的代码使机器人强大
- en: 'MCTS starts to be a viable strategy for full-size (19 × 19) Go at around 10,000
    rollouts per turn. The implementation in this chapter isn’t fast enough to do
    that: you’ll be waiting several minutes for it to choose each move. You’ll need
    to optimize your implementation a bit in order to complete that many rollouts
    in a reasonable amount of time. On small boards, on the other hand, even your
    reference implementation makes a fun opponent.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: MCTS开始成为全尺寸（19 × 19）围棋的可行策略，大约在每回合10,000次rollout时。本章中的实现还不够快，你将等待几分钟才能选择每个移动。你需要对你的实现进行一些优化，以便在合理的时间内完成那么多的rollout。另一方面，在小棋盘上，即使是你的参考实现也是一个有趣的对手。
- en: All else being equal, more rollouts means a better decision. You can always
    make your bot stronger just by speeding up the code so as to squeeze more rollouts
    in the same amount of time. It’s not just the MCTS-specific code that’s relevant.
    The code that calculates captures, for example, is called hundreds of times per
    rollout. All the basic game logic is fair game for optimization.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有其他条件相同的情况下，更多的rollout意味着更好的决策。你总是可以通过加快代码的速度来使你的机器人更强，这样你就可以在相同的时间内挤入更多的rollout。这不仅与MCTS特定的代码相关。例如，计算捕获的代码在每个rollout中被调用数百次。所有基本游戏逻辑都是优化对象。
- en: Better rollout policies make better evaluations
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更好的 rollout 策略产生更好的评估
- en: The algorithm for selecting moves during random rollouts is called the *rollout
    policy*. The more realistic your rollout policy is, the more accurate your evaluations
    will be. In [chapter 3](kindle_split_014.xhtml#ch03), you implemented a `RandomAgent`
    that plays Go; in this chapter, you used your `RandomAgent` as your rollout policy.
    But it’s not quite true that the `RandomAgent` chooses moves *completely* randomly
    with no Go knowledge at all. First, you programmed it not to pass or resign before
    the board is full. Second, you programmed it not to fill its own eyes, so it wouldn’t
    kill its own stones at the end of the game. Without this logic, the rollouts would
    be less accurate.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机走法中选择走法的算法称为*走法策略*。你的走法策略越现实，你的评估就越准确。在[第3章](kindle_split_014.xhtml#ch03)中，你实现了一个`RandomAgent`来玩围棋；在这一章中，你使用了你的`RandomAgent`作为走法策略。但`RandomAgent`并不是完全随机选择走法，没有任何围棋知识。首先，你编程让它不会在棋盘填满之前放弃或认输。其次，你编程让它不会填自己的眼，这样它就不会在游戏结束时杀死自己的棋子。没有这个逻辑，走法就会不准确。
- en: Some MCTS implementations go further and implement more Go-specific logic in
    their rollout policy. Rollouts with game-specific logic are sometimes called *heavy*
    rollouts; by contrast, rollouts that are close to purely random are sometimes
    called *light* rollouts.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一些MCTS实现进一步扩展，并在他们的走法策略中实现更多围棋特定的逻辑。带有游戏特定逻辑的走法有时被称为*重型*走法；相比之下，接近纯随机的走法有时被称为*轻型*走法。
- en: One way to implement heavy rollouts is to build up a list of basic tactical
    shapes that are common in Go, along with a known response. Anywhere you find a
    known shape on the board, you look up the known response and boost its probability
    of being selected. You don’t want to *always* pick the known response as a hard-and-fast
    rule; that will remove the vital element of randomness from the algorithm.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 实现大量走法的一种方法是在围棋中常见的、带有已知反应的基本战术形状列表。无论你在棋盘上找到哪个已知的形状，你都应该查找已知的反应并提高其被选中的概率。你不希望总是将已知的反应作为一条硬性规则来选择；这将从算法中去除必要的随机元素。
- en: One example is in [figure 4.17](#ch04fig17). This is a 3 × 3 local pattern in
    which a black stone is in danger of getting captured on white’s next turn. Black
    can save it, at least temporarily, by extending. This isn’t always the best move;
    it’s not even always a good move. But it’s more likely to be a good move than
    any random point on the board.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子可以在[图4.17](#ch04fig17)中找到。这是一个3×3的局部模式，其中黑子在下一次白棋的回合有被吃掉的危险。黑子可以通过延伸来至少暂时地救它。这并不总是最好的走法；甚至都不一定是好走法。但它比棋盘上任何随机点更有可能是一个好走法。
- en: Figure 4.17\. An example of a local tactical pattern. When you see the shape
    on the left, you should consider the response on the right. A policy that follows
    tactical patterns like this one won’t be especially strong, but will be much stronger
    than choosing moves completely at random.
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.17\. 这是一个局部战术模式的例子。当你看到左边的形状时，你应该考虑右边的反应。遵循这种战术模式的策略不会特别强大，但会比完全随机选择走法强得多。
- en: '![](Images/04fig17.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图4.17](Images/04fig17.jpg)'
- en: Building up a good list of these patterns requires some knowledge of Go tactics.
    If you’re curious about other tactical patterns that you can use in heavy rollouts,
    we suggest looking at the source code for Fuego ([http://fuego.sourceforge.net/](http://fuego.sourceforge.net/))
    or Pachi ([https://github.com/pasky/pachi](https://github.com/pasky/pachi)), two
    open source MCTS Go engines.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个良好的这些模式列表需要一些围棋战术知识。如果你对在大量走法中使用的其他战术模式感兴趣，我们建议查看Fuego([http://fuego.sourceforge.net/](http://fuego.sourceforge.net/))或Pachi([https://github.com/pasky/pachi](https://github.com/pasky/pachi))的源代码，这两个是开源的MCTS围棋引擎。
- en: Be careful when implementing heavy rollouts. If the logic in your rollout policy
    is slow to compute, you can’t execute as many rollouts. You may end up wiping
    out the gains of the more sophisticated policy.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 实施大量走法时要小心。如果你的走法策略中的逻辑计算速度慢，你将无法执行那么多走法。你可能会失去更复杂策略的收益。
- en: A polite bot knows when to resign
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一个有礼貌的机器人知道何时认输
- en: Making a game AI isn’t just an exercise in developing the best algorithm. It’s
    also about creating a fun experience for the human opponent. Part of that fun
    comes from giving the human player the satisfaction of winning. The first Go bot
    you implemented in this book, the `RandomAgent`, is maddening to play against.
    After the human player inevitably pulls ahead, the random bot insists on continuing
    until the entire board is full. Nothing is stopping the human player from walking
    away and mentally scoring the game as a win. But this somehow feels unsporting.
    It’s a much better experience if your bot can gracefully resign instead.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 制作游戏人工智能不仅仅是开发最佳算法的练习。它还涉及到为人类对手创造一个有趣的体验。其中一部分乐趣来自于给予人类玩家获胜的满足感。本书中你首先实现的围棋机器人`RandomAgent`与人类玩家对战时非常令人沮丧。在人类玩家不可避免地领先之后，随机机器人坚持继续游戏直到整个棋盘填满。没有任何东西阻止人类玩家离开并心理上判定游戏为胜利。但这种方式似乎有些不公平。如果您的机器人能够优雅地认输，那么体验会更好。
- en: You can easily add human-friendly resignation logic on top of a basic MCTS implementation.
    The MCTS algorithm computes an estimated winning percentage in the process of
    selecting a move. Within a single turn, you compare these numbers to decide what
    move to pick. But you can also compare the estimated winning percentage at different
    points in the same game. If these numbers are dropping, the game is tilting in
    the human’s favor. When the best option carries a sufficiently low winning percentage,
    say 10%, you can make your bot resign.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地在基本的MCTS实现上添加人类友好的认输逻辑。MCTS算法在选择走法的过程中计算估计的获胜百分比。在单次回合中，你比较这些数字来决定选择哪个走法。但你也可以比较同一游戏中不同点的估计获胜百分比。如果这些数字在下降，游戏正在向人类玩家倾斜。当最佳选项的获胜百分比足够低时，比如10%，你可以让你的机器人认输。
- en: 4.6\. Summary
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. 概述
- en: Tree-search algorithms evaluate many possible sequences of decisions to find
    the best one. Tree search comes up in games as well as general optimization problems.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树搜索算法评估许多可能的决策序列以找到最佳方案。树搜索不仅出现在游戏中，也出现在一般的优化问题中。
- en: The variant of tree search that applies to games is *minimax* tree search. In
    minimax search, you alternate between two players with opposing goals.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于游戏的树搜索变体是*最小-最大树搜索*。在最小-最大搜索中，你交替两个具有对立目标的玩家。
- en: Full minimax tree search is practical in only extremely simple games (for example,
    tic-tac-toe). To apply it to sophisticated games (such as chess or Go), you need
    to reduce the size of the tree that you search.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在极其简单的游戏中（例如井字棋），全最小-最大树搜索是实用的。要将它应用于复杂的游戏（如国际象棋或围棋），你需要减小搜索树的大小。
- en: A *position evaluation function* estimates which player is more likely to win
    from a given board position. With a good position evaluation function, you don’t
    have to search all the way to the end of a game in order to make a decision. This
    strategy is called *depth pruning*.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置评估函数*估计从给定棋盘位置哪个玩家更有可能获胜。有了良好的位置评估函数，你不必搜索到游戏结束才能做出决定。这种策略被称为*深度剪枝*。'
- en: 'Alpha-beta pruning reduces the number of moves you need to consider at each
    turn, making it practical for games as complex as chess. The idea of alpha-beta
    pruning is intuitive: when evaluating a possible move, if you find a single strong
    countermove from your opponent, you can immediately drop that move from consideration
    entirely.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alpha-beta剪枝减少了每个回合需要考虑的走法数量，使其适用于像国际象棋这样复杂的游戏。Alpha-beta剪枝的想法是直观的：在评估一个可能的走法时，如果你发现对手有一个强有力的反击，你可以立即将该走法从考虑中排除。
- en: When you don’t have a good position evaluation heuristic, you can sometimes
    use *Monte Carlo tree search*. This algorithm simulates random games from a particular
    position and tracks which player wins more often.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你没有良好的位置评估启发式方法时，有时可以使用*蒙特卡洛树搜索*。此算法从特定位置模拟随机游戏，并跟踪哪个玩家获胜的频率更高。
- en: Chapter 5\. Getting started with neural networks
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章\. 开始使用神经网络
- en: '*This chapter covers*'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Introducing the fundamentals of artificial neural networks
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍人工神经网络的基本原理
- en: Teaching a network to recognize handwritten digits
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教会网络识别手写数字
- en: Creating neural networks by composing layers
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过组合层创建神经网络
- en: Understanding how neural networks learn from data
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解神经网络如何从数据中学习
- en: Implementing a simple neural network from scratch
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始实现简单的神经网络
- en: This chapter introduces the core notions of artificial neural networks (ANNs),
    a class of algorithms central to modern-day *deep learning*. The history of artificial
    neural networks is a surprisingly old one, dating back to the early 1940s. It
    took many decades for its applications to become vast successes in many areas,
    but the basic ideas remain in effect.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了人工神经网络（ANNs）的核心概念，这是现代深度学习中的一个核心算法类别。人工神经网络的历史非常悠久，可以追溯到20世纪40年代初。它花了数十年时间，其应用才在许多领域取得了巨大成功，但基本思想仍然有效。
- en: At the core of ANNs is the idea to take inspiration from neuroscience and model
    a class of algorithms that works similarly to the way we hypothesize part of the
    brain functions. In particular, we use the notion of *neurons* as atomic blocks
    for our artificial networks. Neurons form groups called *layers*, and these layers
    are *connected* to each other in specific ways to span a *network*. Given input
    data, neurons can transfer information layer by layer via connections, and we
    say that they *activate* if the signal is strong enough. In this way, data is
    propagated through the network until we arrive at the last step, the output layer,
    from which we get our *predictions*. These predictions can then be compared to
    the *expected output* to compute the *error* of the prediction, which the network
    uses to learn and improve future predictions.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs的核心思想是借鉴神经科学，并构建一类算法，其工作方式与我们假设大脑部分功能的方式相似。特别是，我们使用*神经元*作为我们人工网络的原子块。神经元形成称为*层*的组，这些层以特定方式相互*连接*，形成一个*网络*。给定输入数据，神经元可以通过连接逐层传递信息，如果信号足够强，我们说它们*激活*。通过这种方式，数据在网络中传播，直到我们到达最后一步，即输出层，从那里我们得到我们的*预测*。然后，我们可以将这些预测与*预期输出*进行比较，以计算预测的*误差*，网络使用这个误差来学习和改进未来的预测。
- en: Although the brain-inspired architecture analogy is useful at times, we don’t
    want to overstress it here. We do know a lot about the visual cortex of our brain
    in particular, but the analogy can sometimes be misleading or even harmful. We
    think it’s better to think of ANNs as trying to uncover *the guiding principles
    of learning in organisms*, just as an airplane makes use of aerodynamics, but
    doesn’t try to copy a bird.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大脑启发的架构类比有时很有用，但我们不想在这里过分强调它。我们确实对大脑的视觉皮层了解很多，但类比有时可能会误导甚至有害。我们认为，将ANNs视为试图揭示*生物体学习指导原则*的方法会更好，就像飞机利用空气动力学，但并不试图模仿鸟。
- en: To make things more concrete in this chapter, we provide a basic implementation
    of a neural network to follow—starting from scratch. You’ll apply this network
    to tackle a problem from *optical character recognition* (OCR); namely, how to
    let a computer predict which digit is displayed on an image of a handwritten digit.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使本章的内容更加具体，我们提供了一个神经网络的基本实现，供大家参考——从头开始。你将应用这个网络来解决一个来自*光学字符识别*（OCR）的问题；即如何让计算机预测手写数字图像上显示的哪个数字。
- en: Each image in our OCR data set is made up of pixels laid out on a grid, and
    you must analyze spatial relationships between the pixels to figure out what digit
    it represents. Go, like many other board games, is played on a grid, and you must
    consider the spatial relationships on the board in order to pick a good move.
    You might hope that machine-learning techniques for OCR could also apply to games
    like Go. As it turns out, they do. [Chapters 6](kindle_split_018.xhtml#ch06) through
    [8](kindle_split_020.xhtml#ch08) show how to apply these methods to games.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们OCR数据集中的每一张图像都是由网格上排列的像素组成的，你必须分析像素之间的空间关系，以确定它代表的数字。围棋，像许多其他棋类游戏一样，是在网格上进行的，你必须考虑棋盘上的空间关系，以便选择一个好的走法。你可能希望OCR的机器学习技术也能应用于围棋等游戏。事实上，它们确实可以。第[6章](kindle_split_018.xhtml#ch06)到第[8章](kindle_split_020.xhtml#ch08)展示了如何将这些方法应用于游戏。
- en: We keep the mathematics relatively low in this chapter. If you aren’t familiar
    with the basics of linear algebra, calculus, and probability theory or need a
    brief and practical reminder, we suggest that you read [appendix A](kindle_split_028.xhtml#app01)
    first. Also, the more difficult parts of the learning procedure of a neural network
    can be found in [appendix B](kindle_split_029.xhtml#app02). If you know neural
    networks, but have never implemented one, we suggest that you skip to [section
    5.5](#ch05lev1sec5) right away. If you’re familiar with implementing networks
    as well, jump right into [chapter 6](kindle_split_018.xhtml#ch06), in which you’ll
    apply neural networks to predict moves from games generated in [chapter 4](kindle_split_016.xhtml#ch04).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将保持数学内容相对简单。如果你不熟悉线性代数、微积分和概率论的基础，或者需要简要的实用提醒，我们建议你首先阅读[附录A](kindle_split_028.xhtml#app01)。此外，神经网络学习过程中的难点部分可以在[附录B](kindle_split_029.xhtml#app02)中找到。如果你了解神经网络但从未实现过，我们建议你直接跳转到[第5.5节](#ch05lev1sec5)。如果你熟悉实现网络，可以直接跳转到[第6章](kindle_split_018.xhtml#ch06)，在该章中，你将应用神经网络来预测[第4章](kindle_split_016.xhtml#ch04)中生成的游戏动作。
- en: '5.1\. A simple use case: classifying handwritten digits'
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 一个简单的用例：手写数字分类
- en: Before introducing neural networks in detail, let’s start with a concrete use
    case. Throughout this chapter, you’ll build an application that can predict digits
    from handwritten image data reasonably well, with about 95% accuracy. Notably,
    you’ll do all this by exposing only the pixel values of the images to a neural
    network; the algorithm will learn to extract relevant information about the structure
    of digits on its own.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细介绍神经网络之前，让我们从一个具体的用例开始。在本章中，你将构建一个应用程序，它可以合理地预测手写图像数据中的数字，准确率约为95%。值得注意的是，你将通过仅向神经网络暴露图像的像素值来完成所有这些工作；算法将学会自行提取有关数字结构的相关信息。
- en: You’ll use the Modified National Institute of Standards and Technology (MNIST)
    data set of handwritten digits to do so, a well-studied data set among machine-learning
    practitioners and the fruit fly of deep learning.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用修改后的国家标准与技术研究院（MNIST）手写数字数据集来完成这项工作，这是机器学习从业者中一个研究广泛的数据集，也是深度学习的“果蝇”。
- en: In this chapter, you’ll use the NumPy library for handling low-level mathematical
    operations. NumPy is the industry standard for machine learning and mathematical
    computing in Python, and you’ll use it throughout the rest of the book. Before
    trying out any of the code samples in this chapter, you should install NumPy with
    your preferred package manager. If you use pip, run `pip install numpy` from a
    shell to install it. If you use Conda, run `conda install numpy`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用NumPy库来处理底层数学运算。NumPy是Python中机器学习和数学计算的行业标准库，你将在本书的其余部分使用它。在尝试本章中的任何代码示例之前，你应该使用你喜欢的包管理器安装NumPy。如果你使用pip，请在shell中运行`pip
    install numpy`来安装它。如果你使用Conda，请运行`conda install numpy`。
- en: 5.1.1\. The MNIST data set of handwritten digits
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 手写数字的MNIST数据集
- en: The MNIST data set consists of 60,000 images, 28 × 28 pixels each. A few examples
    of this data are shown in [figure 5.1](#ch05fig01). To humans, recognizing most
    of these examples is a trivial task, and you can easily read the examples in the
    first row as 7, 5, 3, 9, 3, 0, and so on. But in some cases, it’s difficult even
    for humans to understand what the picture represents. For instance, the fourth
    picture in row five in [figure 5.1](#ch05fig01) could easily be a 4 or a 9.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集包含60,000张图像，每张图像为28 × 28像素。该数据集的一些示例显示在[图5.1](#ch05fig01)中。对于人类来说，识别这些示例中的大多数是一个简单任务，你可以轻松地读取第一行的示例为7，5，3，9，3，0等等。但在某些情况下，即使是人类也很难理解图片代表什么。例如，[图5.1](#ch05fig01)中第五行的第四张图片很容易被识别为4或9。
- en: Figure 5.1\. A few samples from the MNIST data set for handwritten digits, a
    well-studied entity in the field of optical character recognition
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1\. 来自MNIST手写数字数据集的一些样本，这是光学字符识别领域的一个研究广泛的实体
- en: '![](Images/05fig01.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig01.jpg)'
- en: Each image in MNIST is annotated with a *label*, a digit from 0 to 9 representing
    the true value depicted on the image.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST中的每张图像都标注了一个*标签*，一个从0到9的数字，代表图像上所表示的真实值。
- en: Before you can look at the data, you need to load it first. In our GitHub repository
    for this book, you’ll find a file called mnist.pkl.gz located in the folder [http://mng.bz/P8mn](http://mng.bz/P8mn).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能够查看数据之前，你需要首先加载它。在我们的GitHub仓库中，你可以找到一个名为mnist.pkl.gz的文件，位于[http://mng.bz/P8mn](http://mng.bz/P8mn)文件夹中。
- en: In this folder, you’ll also find all the code you’ll write in this chapter.
    As before, we suggest that you follow the flow of this chapter and build the code
    base as you go, but you can also run the code as found in the GitHub repository.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件夹中，你还可以找到本章中将要编写的所有代码。和之前一样，我们建议你遵循本章的流程，并在编写代码的同时构建代码库，但你也可以运行 GitHub
    仓库中找到的代码。
- en: 5.1.2\. MNIST data preprocessing
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. MNIST数据预处理
- en: 'Because the labels in this data set are integers from 0 to 9, you’ll use a
    technique called *one-hot encoding* to transform the digit 1 to a vector of length
    10 with all 0s, except you place a 1 at position 1\. This representation is useful
    and widely used in the context of machine learning. Reserving the first slot in
    a vector for label 1 allows algorithms such as neural networks to distinguish
    more easily between labels. Using one-hot encoding, the digit 2, for instance,
    has the following representation: `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个数据集中的标签是 0 到 9 的整数，所以你会使用一种称为 *独热编码* 的技术，将数字 1 转换为一个长度为 10 的向量，除了在位置 1 放置一个
    1 之外，其余都是 0。这种表示在机器学习环境中非常有用且广泛使用。在向量中为标签 1 保留第一个槽位，使得像神经网络这样的算法更容易区分标签。使用独热编码，例如，数字
    2 的表示如下：`[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`。
- en: Listing 5.1\. One-hot encoding of MNIST labels
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.1\. MNIST 标签的独热编码
- en: '[PRE17]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '***1* You one-hot encode indices to vectors of length 10.**'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将索引进行独热编码为长度为 10 的向量。**'
- en: The benefit of one-hot encoding is that each digit has its own “slot,” and you
    can use neural networks to output *probabilities* for an input image, which will
    become useful later.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码的好处是每个数字都有自己的“槽位”，你可以使用神经网络为输入图像输出 *概率*，这将在以后变得很有用。
- en: 'Examining the contents of the file mnist.pkl.gz, you have access to three pools
    of data: training, validation, and test data. Recall from [chapter 1](kindle_split_012.xhtml#ch01)
    that you use training data to train, or fit, a machine-learning algorithm, and
    use test data to evaluate how well the algorithm learned. Validation data can
    be used to tweak and validate the configuration of the algorithm, but can be safely
    ignored in this chapter.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 检查文件 mnist.pkl.gz 的内容，你可以访问三个数据集：训练数据、验证数据和测试数据。回想一下[第1章](kindle_split_012.xhtml#ch01)，你使用训练数据来训练或拟合机器学习算法，并使用测试数据来评估算法学习的好坏。验证数据可以用来调整和验证算法的配置，但在这章中可以安全地忽略。
- en: Images in the MNIST data set are quadratic and have both height and width of
    28 pixels. You load image data into *feature vectors* of size 784 = 28 × 28; you
    discard the image structure altogether and look only at pixels represented as
    a vector. Each value of this vector represents a grayscale value between 0 and
    1, with 0 being white and 1 black.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集中的图像是二维的，高度和宽度都是 28 像素。你将图像数据加载到大小为 784 的 *特征向量* 中，即 28 × 28；你完全丢弃了图像结构，只看表示为向量的像素。这个向量中的每个值代表一个介于
    0 和 1 之间的灰度值，其中 0 表示白色，1 表示黑色。
- en: Listing 5.2\. Reshaping MNIST data and loading training and test data
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.2\. 调整 MNIST 数据形状并加载训练和测试数据
- en: '[PRE18]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1* Flatten the input images to feature vectors of length 784.**'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将输入图像展平为长度为 784 的特征向量。**'
- en: '***2* All labels are one-hot encoded.**'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 所有标签都进行独热编码。**'
- en: '***3* Create pairs of features and labels.**'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建特征和标签的配对。**'
- en: '***4* Unzipping and loading the MNIST data yields three data sets.**'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 解压并加载 MNIST 数据得到三个数据集。**'
- en: '***5* Discard validation data here and reshape the other two data sets.**'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 在这里丢弃验证数据，并调整其他两个数据集的形状。**'
- en: You now have a simple representation of the MNIST data set; both features and
    labels are encoded as vectors. Your task is to devise a mechanism that learns
    to map features to labels accurately. Specifically, you want to design an algorithm
    that takes in training features and labels to learn, such that it can predict
    labels for test features.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有了 MNIST 数据集的简单表示；特征和标签都编码为向量。你的任务是设计一个机制，使其能够准确地将特征映射到标签。具体来说，你想要设计一个算法，该算法接受训练特征和标签进行学习，以便它可以预测测试特征的标签。
- en: Neural networks can do this job well, as you’ll see in the next section, but
    let’s first discuss a naive approach that will show you the general problems you
    have to tackle for this application. Recognizing digits is a relatively simple
    task for humans, but it’s difficult to explain exactly how to do it and how we
    know what we know. This phenomenon of knowing more than you can explain is called
    *Polanyi’s paradox*. This makes it particularly hard to describe to a machine
    *explicitly* how to solve this problem.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以很好地完成这项工作，正如你将在下一节中看到的，但让我们首先讨论一种天真方法，这将向你展示你必须解决这个应用的一般问题。对于人类来说，识别数字是一个相对简单的工作，但很难确切地解释如何做以及我们如何知道我们所知道的东西。这种知道得比能解释得多的现象被称为**波利尼悖论**。这使得向机器**明确地**描述如何解决这个问题尤其困难。
- en: 'One aspect that plays a crucial role is *pattern recognition*—each handwritten
    digit has certain traits that derive from its prototypical, digital version. For
    instance, a 0 is roughly an oval, and in many countries a 1 is simply a vertical
    line. Given this heuristic, you can naively approach classifying handwritten digits
    by comparing them to each other: given an image of an 8, this image should be
    closer to the average image of an 8 than to any other digit. The following `average_digit`
    function does this for you.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 一个起着关键作用方面是**模式识别**——每个手写数字都有某些来自其典型、数字版本的特性。例如，0 大约是一个椭圆形，在许多国家，1 只是一个垂直线。根据这个启发式方法，你可以天真地通过比较它们来对手写数字进行分类：给定一个
    8 的图像，这个图像应该比任何其他数字更接近 8 的平均图像。下面的 `average_digit` 函数为你做这件事。
- en: Listing 5.3\. Computing the average value for images representing the same digit
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3\. 计算表示相同数字的图像的平均值
- en: '[PRE19]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1* Compute the average over all samples in your data representing a given
    digit.**'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 计算表示给定数字的所有样本的平均值。**'
- en: '***2* Use the average 8 as parameters for a simple model to detect 8s.**'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用平均 8 作为简单模型检测 8 的参数。**'
- en: What does the average 8 in your training set look like? [Figure 5.2](#ch05fig02)
    gives the answer.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 你的训练集中的平均 8 看起来是什么样子？[图 5.2](#ch05fig02) 给出了答案。
- en: Figure 5.2\. This is what an average handwritten 8 from the MNIST training set
    looks like. Averaging many hundreds of images in general will result in an unrecognizable
    blob, but this average 8 still looks very much like an 8.
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2\. 这就是 MNIST 训练集中平均手写 8 的样子。平均数百张图像通常会导致一个无法识别的块状物，但这个平均 8 仍然非常像 8。
- en: '![](Images/05fig02.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig02.jpg)'
- en: Because handwriting can differ quite a lot from individual to individual, as
    expected, the average 8 is a little fuzzy, but it’s still noticeably shaped like
    an 8\. Maybe you can use this representation to identify other 8s in your data
    set? You use the following code to compute and display [figure 5.2](#ch05fig02).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由于手写体可能因人而异，正如预期的那样，平均 8 略微模糊，但它仍然明显地呈现出 8 的形状。也许你可以使用这种表示来识别数据集中的其他 8？你使用以下代码来计算并显示
    [图 5.2](#ch05fig02)。
- en: Listing 5.4\. Computing and displaying the average 8 in your training set
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4\. 计算并显示训练集中的平均 8
- en: '[PRE20]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This average representation of an 8, `avg_eight`, in the training set of MNIST,
    should carry a lot of information about what it means for an image to have an
    8 on it. You’ll use `avg_eight` as *parameters* of a simple model to decide whether
    a given input vector `x`, representing a digit, is an 8\. In the context of neural
    networks, we often speak of *weights* when referring to parameters, and `avg_eight`
    will serve as your weight.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MNIST 训练集中，这个 8 的平均表示 `avg_eight` 应该包含大量关于图像上有一个 8 的含义的信息。你将使用 `avg_eight`
    作为简单模型的**参数**来决定给定的输入向量 `x`（表示一个数字）是否是 8。在神经网络中，当我们提到参数时，我们经常说**权重**，而 `avg_eight`
    将作为你的权重。
- en: For convenience, you’ll use transposition and define `W = np.transpose(avg_eight)`.
    You can then compute the *dot product* of `W` and `x`, which does pointwise multiplication
    of values of `W` and `x` and sums up all 784 resulting values. If your heuristic
    is right, if `x` is an 8, individual pixels should have a darker tone at roughly
    the same places `W` does, and vice versa. Conversely, if `x` isn’t an 8, there
    should be less overlap. Let’s test this hypothesis in a few examples.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，你将使用转置并定义 `W = np.transpose(avg_eight)`。然后你可以计算 `W` 和 `x` 的**点积**，这相当于对
    `W` 和 `x` 的值进行逐点乘法并求和所有 784 个结果值。如果你的启发式方法是正确的，如果 `x` 是一个 8，那么单个像素应该在大约与 `W` 相同的位置上具有较深的色调，反之亦然。相反，如果
    `x` 不是一个 8，那么重叠应该更少。让我们在几个例子中测试这个假设。
- en: Listing 5.5\. Computing how close a digit is to your weights by using the dot
    product
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.5\. 使用点积计算数字与你的权重接近程度
- en: '[PRE21]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1* Training sample at index 2 is a 4**'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 索引为2的训练样本是一个4**'
- en: '***2* Training sample at index 17 is an 8**'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 索引为17的训练样本是一个8**'
- en: '***3* This evaluates to about 20.1.**'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 这个结果大约是20.1。**'
- en: '***4* This term is much bigger, about 54.2.**'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 这个值要大得多，大约是54.2。**'
- en: You compute the dot product of your weights `W` with two MNIST samples, one
    representing a 4, and another one representing an 8\. You can see that the latter
    result of 54.2 for the 8 is much higher than the 20.1 result for the 4\. So, it
    seems you’re on to something. Now, how do you decide when a resulting value is
    high enough to predict it as an 8? In principle, the dot product of two vectors
    can spit out any real number. What you do to address this is *transform* the output
    of the dot product to the range [0, 1]. Doing so, you can, for instance, try to
    define a cutoff value at 0.5 and declare everything above this value an 8.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你计算了权重`W`与两个MNIST样本的点积，一个代表4，另一个代表8。你可以看到，对于8的后者结果54.2远高于4的20.1结果。所以，你似乎找到了一些东西。现在，你如何决定一个结果值是否足够高，可以预测它为8？原则上，两个向量的点积可以输出任何实数。你如何处理这个问题是将点积的输出*转换*到[0,
    1]的范围。这样做后，例如，你可以尝试定义一个截止值在0.5，并宣布所有高于这个值的都是8。
- en: 'One way to do this is with the *sigmoid* function. The sigmoid function is
    often denoted by s, the Greek letter *sigma*. For a real number *x*, the sigmoid
    function is defined as:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的其中一种方法是使用*sigmoid*函数。sigmoid函数通常用希腊字母*sigma*表示，记作s。对于实数*x*，sigmoid函数定义为：
- en: '![](Images/p0091_01.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0091_01.jpg)'
- en: '[Figure 5.3](#ch05fig03) shows how it looks to gain some intuition.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.3](#ch05fig03)展示了如何获得一些直观感受。'
- en: Figure 5.3\. Plot of the sigmoid function. The sigmoid maps real values to a
    range of [0, 1]. Around 0, the slope is rather steep, and the curve flattens out
    both for small and large values.
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3\. sigmoid函数的图像。sigmoid将实数值映射到[0, 1]的范围。在0附近，斜率相当陡峭，曲线在小的和大的值上都变平。
- en: '![](Images/05fig03.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig03.jpg)'
- en: Next, let’s code the sigmoid function in Python before you apply it to the output
    of the dot product.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在你将sigmoid函数应用于点积的输出之前，让我们用Python编写sigmoid函数。
- en: Listing 5.6\. Simple implementation of sigmoid function for double values and
    vectors
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.6\. 双值和向量的sigmoid函数的简单实现
- en: '[PRE22]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that you provide both `sigmoid_double`, which operates on double values,
    as well as a version that computes the sigmoid for vectors that you’ll use extensively
    in this chapter. Before you apply sigmoid to your previous computation, note that
    the sigmoid of 2 is already close to 1, so for your two previously computed samples,
    sigmoid(54.2) and sigmoid(20.1) will be practically indistinguishable. You can
    fix this issue by *shifting* the output of the dot product toward 0\. Doing this
    is called applying a *bias term*, which we often refer to as *b*. From the samples,
    you compute that a good guess for a bias term might be *b* = –45\. Using weights
    and the bias term, you can now compute *predictions* of your model as follows.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你提供了`sigmoid_double`，它操作双值，以及一个用于计算向量的sigmoid版本，你将在本章中广泛使用。在你将sigmoid应用于之前的计算之前，请注意，2的sigmoid值已经接近1，所以对于你之前计算的两个样本，sigmoid(54.2)和sigmoid(20.1)实际上是无法区分的。你可以通过*平移*点积的输出向0来解决这个问题。这样做被称为应用一个*偏差项*，我们通常称之为*b*。从样本中，你可以计算出偏差项的一个好的猜测可能是*b*
    = –45。使用权重和偏差项，你现在可以按照以下方式计算模型的*预测*。
- en: Listing 5.7\. Computing predictions from weights and bias with dot product and
    sigmoid
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.7\. 使用点积和sigmoid计算权重和偏差的预测
- en: '[PRE23]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1* A simple prediction is defined by applying sigmoid to the output of np.doc(W,
    x) + b.**'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 简单预测是通过将np.doc(W, x) + b的输出应用sigmoid函数来定义的。**'
- en: '***2* Based on the examples computed so far, you set the bias term to –45.**'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 根据到目前为止计算的示例，你将偏差项设置为–45。**'
- en: '***3* The prediction for the example with a 4 is close to 0.**'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 对于4的预测接近于0。**'
- en: '***4* The prediction for an 8 is 0.96 here. You seem to be onto something with
    your heuristic.**'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 这里对8的预测是0.96。你的启发式方法似乎很有道理。**'
- en: You get satisfying results on the two examples `x_3` and `x_18`. The prediction
    for the latter is close to 1, and for the former is almost 0\. This procedure
    of mapping an input vector *x* to s(*Wx* + b) for *W*, a vector the same size
    as *x*, is called *logistic regression*. [Figure 5.4](#ch05fig04) depicts this
    algorithm schematically for a vector of length 4.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个例子 `x_3` 和 `x_18` 上，你得到了令人满意的结果。对于后者，预测值接近 1，而对于前者则几乎为 0。这种将输入向量 *x* 映射到
    s(*Wx* + b) 的过程，其中 *W* 是一个与 *x* 大小相同的向量，被称为 *逻辑回归*。[图 5.4](#ch05fig04) 以 4 维向量为例，示意了该算法。
- en: Figure 5.4\. An example of logistic regression, mapping an input vector *x*
    of length 4 to an output value *y* between 0 and 1\. The schematic indicates how
    the output *y* depends on all four values in the input vector *x*.
  id: totrans-357
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.4\. 逻辑回归的一个例子，将长度为 4 的输入向量 *x* 映射到 0 到 1 之间的输出值 *y*。示意图表明输出 *y* 如何依赖于输入向量
    *x* 中的所有四个值。
- en: '![](Images/05fig04.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig04.jpg)'
- en: To get a better idea of how well this procedure works, let’s compute predictions
    for all training and test samples. As indicated before, you define a cutoff, or
    decision *threshold*, to decide whether a prediction is counted as an 8\. As an
    evaluation metric here, you choose *accuracy*; you compute the ratio of correct
    predictions among all predictions.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解这个过程的性能，让我们计算所有训练和测试样本的预测。如前所述，你定义一个截止点或决策 *阈值* 来决定一个预测是否被计为 8。在这里，你选择
    *准确率* 作为评估指标；你计算所有预测中正确预测的比例。
- en: Listing 5.8\. Evaluating predictions of your model with a decision threshold
  id: totrans-360
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.8\. 使用决策阈值评估模型的预测
- en: '[PRE24]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* For an evaluation metric, you choose accuracy, the ratio of correct predictions
    among all.**'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 对于评估指标，你选择了准确率，即所有正确预测的比例。**'
- en: '***2* Predicting an instance of an 8 as 8 is a correct prediction.**'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将一个 8 的实例预测为 8 是一个正确的预测。**'
- en: '***3* If the prediction is below your threshold and the sample isn’t an 8,
    you also predicted correctly.**'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果预测低于你的阈值且样本不是 8，你也做出了正确的预测。**'
- en: 'Let’s use this evaluation function to assess the quality of predictions for
    three data sets: the training set, the test set, and the set of all 8s among the
    test set. You do this with a threshold of 0.5 and weights `W` and bias term `b`
    as before.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个评估函数来评估三个数据集（训练集、测试集和测试集中所有 8 的集合）的预测质量。你使用 0.5 的阈值以及之前的权重 `W` 和偏置项 `b`
    来做这件事。
- en: Listing 5.9\. Calculating prediction accuracy for three data sets
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.9\. 计算三个数据集的预测准确率
- en: '[PRE25]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '***1* Accuracy on training data of your simple model is 78% (0.7814).**'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你的简单模型在训练数据上的准确率为 78%（0.7814）。**'
- en: '***2* Accuracy on test data is slightly lower, at 77% (0.7749).**'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 测试数据上的准确率略低，为 77%（0.7749）。**'
- en: '***3* Evaluating only on the set of 8s in the test set results in only 67%
    accuracy (0.6663).**'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 仅在测试集中的 8 的集合上评估结果只有 67% 的准确率（0.6663）。**'
- en: What you see is that accuracy on the training set is highest, at about 78%.
    This shouldn’t come as a surprise, because you *calibrated* your model on the
    training set. In particular, it doesn’t make sense to evaluate on the training
    set, because it doesn’t tell you how well your algorithm *generalizes* (how well
    it performs on previously unseen data). Performance on test data is close to that
    for training, at about 77%, but it’s the last accuracy term that’s noteworthy.
    On the set of all 8s in the test set, you achieve merely 66%, so with your simple
    model, you’re right in only two out of three unseen cases of 8s. This result might
    be acceptable as a first baseline, but is far from the best you can do. What went
    wrong, and what can you do better?
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在训练集上的准确率最高，大约为 78%。这并不令人惊讶，因为你已经在训练集上 *校准* 了你的模型。特别是，在训练集上进行评估没有意义，因为它不能告诉你你的算法
    *泛化能力*（在之前未见过的数据上的表现）如何。测试数据的表现接近训练数据，大约为 77%，但值得注意的是最后一个准确率项。在测试集中所有 8 的集合上，你只达到了
    66%，所以使用你的简单模型，在三个未见过的 8 的案例中只有两个是正确的。这个结果可能作为第一个基线是可以接受的，但离你能做到的最好水平还远。出了什么问题，你还能做什么改进？
- en: Your model is capable of distinguishing between only a specific digit (here,
    an 8) and all others. Because in both the training and test sets the number of
    images per digit is *balanced*, only about 10% are 8s. Thus, a model predicting
    0 all the time would yield about 90% accuracy. Keep in mind such *class imbalances*
    when analyzing classification problems like this one. In light of this, your 77%
    accuracy on test data doesn’t look quite as strong anymore. *You need to define
    a model that can predict all 10 digits accurately.*
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的模型只能区分特定的数字（在这里是一个8）和所有其他数字。因为在训练集和测试集中每个数字的图像数量都是*平衡的*，所以只有大约10%是8。因此，一个总是预测0的模型将产生大约90%的准确率。记住在分析此类分类问题时存在的*类别不平衡*。鉴于这一点，你在测试数据上的77%准确率看起来不再那么强劲。*你需要定义一个可以准确预测所有10个数字的模型*。
- en: The parameters of your models are fairly small. For a collection of many thousand
    diverse handwritten images, all you have is a set of weights the size of one of
    such image. It’s unrealistic to believe you can capture the variability in handwriting
    found on these images with such a small model. *You have to find a class of algorithms
    that uses many more parameters effectively to capture the variability in data.*
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的模型参数相当小。对于一个包含成千上万种不同手写图像的集合，你只有一组与其中一张图像大小相当的权重。相信你可以用如此小的模型捕捉到这些图像上发现的书写变化是不现实的。*你必须找到一类算法，它使用更多的参数有效地捕捉数据中的变化*。
- en: For a given prediction, you simply chose a cutoff value to declare a digit an
    8 or not. You didn’t use the actual prediction value to assess the quality of
    your model. For instance, a correct prediction at 0.95 certainly indicates a stronger
    result than one at 0.51\. *You have to formalize the notion of how close the prediction
    was to the actual outcome*.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的预测，你只需选择一个截止值来宣布一个数字是8还是不是。你没有使用实际的预测值来评估你模型的质量。例如，在0.95的准确率下做出正确的预测肯定比在0.51的准确率下做出正确的预测结果要强。*你必须形式化预测与实际结果接近的概念*。
- en: You handcrafted the parameters of your model, guided by intuition. Although
    this might be a good first shot, the promise of machine learning is that you don’t
    have to impose your view on the data, but rather let the algorithm learn from
    data. Whenever your model makes correct predictions, you need to reinforce this
    behavior, and whenever the output is wrong, you need to adjust your model accordingly.
    In other words, *you need to devise a mechanism that updates model parameters
    according to how well you predicted on training data.*
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你通过直觉手工调整了模型的参数。尽管这可能是一个好的起点，但机器学习的承诺是，你不必将自己的观点强加于数据，而是让算法从数据中学习。每当你的模型做出正确的预测时，你需要强化这种行为，每当输出错误时，你需要相应地调整你的模型。换句话说，*你需要设计一种机制，根据你在训练数据上的预测效果来更新模型参数*。
- en: Although the discussion of this little use case and the naive model you built
    might not seem like much, you’ve already seen many parts constituting neural networks.
    In the next section, you’ll use the intuition built around this use case to take
    your first steps with neural networks by tackling each of these four points.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对这个小用例和构建的朴素模型进行讨论可能看起来不多，但你已经看到了构成神经网络的多部分。在下一节中，你将利用围绕这个用例构建的直觉，通过解决这四个点中的每一个来迈出与神经网络的第一步。
- en: 5.2\. The basics of neural networks
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. 神经网络的基本原理
- en: How can you improve your OCR model? As we hinted in the introduction, neural
    networks can do a bang-up job on this sort of task—much better than our handcrafted
    model. But the handcrafted model does illustrate key concepts that you’ll use
    to build your neural network. This section describes the model from the previous
    section in the language of neural networks.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何改进你的OCR模型？正如我们在引言中暗示的那样，神经网络可以在这个任务上做得很好——比我们手工制作的模型要好得多。但手工制作的模型确实说明了你构建神经网络时将使用的关键概念。本节将使用神经网络的语言描述上一节中的模型。
- en: 5.2.1\. Logistic regression as simple artificial neural network
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1. 逻辑回归作为简单的人工神经网络
- en: 'In [section 5.1](#ch05lev1sec1), you saw logistic regression used for *binary
    classification*. To recap, you took a feature vector *x*, representing a data
    sample, fed it into the algorithm by first multiplying it by a weight matrix *W*
    and then adding a bias term *b*. To end up with a prediction *y* between 0 and
    1, you applied the sigmoid function to it: *y =* s*(Wx + b)*'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [5.1节](#ch05lev1sec1) 中，你看到了逻辑回归用于 *二元分类*。为了回顾，你取了一个特征向量 *x*，代表一个数据样本，通过首先将其乘以权重矩阵
    *W*，然后加上偏差项 *b*，将其输入算法。为了得到一个介于0和1之间的预测 *y*，你对其应用了sigmoid函数：*y =* s*(Wx + b)*
- en: You should notice a few things here. First of all, the feature vector *x* can
    be interpreted as a collection of neurons, sometimes called *units*, connected
    to *y* by means of *W* and *b*, which you’ve already seen in [figure 5.4](#ch05fig04).
    Next, note that the sigmoid can be seen as an activation function, in that it
    takes the outcome of *Wx* + *b* and maps it to the range [0,1]. If you interpret
    a value close to 1 as the neuron *y* activating, and in turn not activating if
    it’s close to 0, this setup can be seen as a small example of an artificial neural
    network already.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意这里的一些事情。首先，特征向量 *x* 可以被解释为一组神经元，有时被称为 *单元*，通过 *W* 和 *b* 与 *y* 连接，你已经在 [图5.4](#ch05fig04)
    中看到了。接下来，请注意，sigmoid可以被视为一个激活函数，因为它将 *Wx* + *b* 的结果映射到[0,1]的范围内。如果你将接近1的值解释为神经元
    *y* 激活，反之，如果它接近0则不激活，这个设置可以被视为一个人工神经网络的小型示例。
- en: 5.2.2\. Networks with more than one output dimension
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 具有多个输出维度的网络
- en: In the use case in [section 5.1](#ch05lev1sec1), you simplified the problem
    of recognizing handwritten digits to a binary classification problem; namely,
    distinguishing an 8 from all other digits. But you’re interested in predicting
    10 classes, one for each digit. At least formally, you can achieve this fairly
    easily by changing what you denote by *y*, *W*, and *b*; you alter the output,
    weights, and bias of your model.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [5.1节](#ch05lev1sec1) 的用例中，你将识别手写数字的问题简化为二元分类问题；即区分8和其他所有数字。但你感兴趣的是预测10个类别，每个数字一个。至少在形式上，你可以通过改变你表示的
    *y*、*W* 和 *b* 来轻松实现这一点；你改变了模型的输出、权重和偏差。
- en: 'First, you make *y* a vector of length 10; *y* will have one value representing
    the likelihood of each of the 10 digits:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将 *y* 设为一个长度为10的向量；*y* 将包含一个值，表示每个10个数字中每个数字的可能性：
- en: '![](Images/p0095_01.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/p0095_01.jpg)'
- en: 'Next, let’s adapt weights and bias accordingly. Recall that so far *W* is a
    vector of length 784\. Instead, you make *W* a matrix with dimensions (10, 784).
    This way, you can do matrix multiplication of *W* and an input vector *x*, namely
    *Wx*, the result of which will be a vector of length 10\. Continuing, if you make
    the bias term a vector of length 10, you can add it to *Wx*. Finally, note that
    you can compute the sigmoid of a vector *z* by applying it to each of its components:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，相应地调整权重和偏差。回想一下，到目前为止 *W* 是一个长度为784的向量。相反，你将 *W* 转换为一个维度为（10，784）的矩阵。这样，你可以对
    *W* 和输入向量 *x* 进行矩阵乘法，即 *Wx*，其结果将是一个长度为10的向量。继续，如果你将偏差项设为一个长度为10的向量，你可以将其加到 *Wx*
    上。最后，请注意，你可以通过将其应用于其每个分量来计算向量 *z* 的sigmoid值：
- en: '![](Images/p0095_02.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/p0095_02.jpg)'
- en: '[Figure 5.5](#ch05fig05) depicts this slightly altered setup for four input
    and two output neurons.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.5](#ch05fig05) 描述了这种稍微修改过的四个输入和两个输出神经元的设置。'
- en: Figure 5.5\. In this simple network, four input neurons are connected to two
    output neurons by means of first multiplying with a two-by-four matrix, adding
    a two-dimensional bias term, and then applying the sigmoid component-wise.
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5\. 在这个简单的网络中，四个输入神经元通过先乘以一个2x4的矩阵，加上一个二维偏差项，然后逐分量应用sigmoid函数，与两个输出神经元相连。
- en: '![](Images/05fig05.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig05.jpg)'
- en: Now, what did you gain? You can now map an input vector *x* to an output vector
    *y*, whereas before, *y* was just a single value. The benefit of this is that
    nothing stops you from doing this vector-to-vector transformation multiple times,
    thereby building what we call a *feed-forward network*.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你得到了什么？你现在可以将输入向量 *x* 映射到输出向量 *y*，而之前，*y* 只是一个单一值。这个好处是，没有任何东西阻止你多次进行这种向量到向量的转换，从而构建我们所说的
    *前馈网络*。
- en: 5.3\. Feed-forward networks
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 前馈网络
- en: 'Let’s quickly recap what you did in the preceding section. On a high level,
    you carried out the following steps:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 快速回顾一下上一节你做了什么。从高层次来看，你执行了以下步骤：
- en: You started from a vector of input neurons *x* and applied a simple transformation
    to it, namely *z* = *Wx* + *b*. In the language of linear algebra, these transformations
    are called *affine linear*. Here you use *z* as an intermediary variable to ease
    notation down the line.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你从一个输入神经元向量*x*开始，并对其应用了一个简单的转换，即*z* = *Wx* + *b*。在线性代数的语言中，这些转换被称为*仿射线性*。在这里，你使用*z*作为中间变量，以简化后续的表示。
- en: You applied an activation function, the sigmoid *y* = s(*z*), to get output
    neurons *y*. The outcome of applying s tells you how much *y* activates.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应用了一个激活函数，sigmoid *y* = s(*z*), 以获得输出神经元*y*。应用s的结果告诉你*y*激活的程度。
- en: 'At the heart of feed-forward networks is the idea that you can apply this process
    iteratively, thereby applying many times over the simple building blocks specified
    by these two steps. These building blocks form what we call a *layer*. With this
    notation, you can say that you *stack many layers* to form a *multilayer neural
    network*. Let’s modify our last example by introducing one more layer. You now
    have to run the following steps:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈网络的核心是这样一个想法：你可以迭代地应用这个过程，因此可以多次应用这两个步骤指定的简单构建块。这些构建块形成了我们所说的*层*。用这种表示法，你可以说你*堆叠了许多层*来形成一个*多层神经网络*。让我们通过引入一个额外的层来修改我们的最后一个例子。你现在必须运行以下步骤：
- en: Starting with the input *x*, compute *z*¹ = *W*¹*x* + *b*¹.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入*x*开始，计算*z*¹ = *W*¹*x* + *b*¹。
- en: From the intermediate result *z*¹, you get the output *y* by computing *y* =
    *W*²z¹ + *b*².
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从中间结果*z*¹，通过计算*y* = *W*²*z*¹ + *b*²得到输出*y*。
- en: Note that you use superscripts here to denote which layer you’re in, and subscripts
    to denote position within a vector or matrix. The prescription of working with
    two layers instead of just one is visualized in [figure 5.6](#ch05fig06).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你在这里使用上标来表示你所在的层，下标来表示向量或矩阵中的位置。使用两个层而不是一个层的操作规定在[图5.6](#ch05fig06)中得到了可视化。
- en: Figure 5.6\. An artificial neural network with two layers. The input neurons
    *x* connect to an intermediate set of units *z*, which themselves connect to the
    output neurons *y*.
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6\. 具有两个层的神经网络。输入神经元*x*连接到中间单元集合*z*，而*z*本身又连接到输出神经元*y*。
- en: '![](Images/05fig06.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig06.jpg)'
- en: At this point, it should become clear that you’re not bound to any particular
    number of layers to stack. You could use many more. Moreover, you don’t necessarily
    have to use logistic sigmoid as the activation all the time. You have a plethora
    of activation functions to choose from, and we introduce some of them in the next
    chapter. Applying these functions of all layers in a network sequentially to one
    or more data points is usually referred to as a *forward pass*. It’s referred
    to as *forward*, because data always flows only forward, from input to output
    (in the figures, left to right), and never back.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，应该已经很清楚，你不需要堆叠特定数量的层。你可以使用更多。此外，你不必始终使用逻辑sigmoid作为激活函数。你有大量的激活函数可供选择，我们将在下一章介绍其中的一些。将网络中所有层的这些函数按顺序应用于一个或多个数据点通常被称为*前向传播*。它被称为*前向*，因为数据总是只向前流动，从输入到输出（在图中，从左到右），永远不会向后流动。
- en: With this notation, depicting a regular feed-forward network with three layers
    looks like [figure 5.7](#ch05fig07).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种表示法，用三个层描述的常规前馈网络看起来像[图5.7](#ch05fig07)。
- en: Figure 5.7\. A neural network with three layers. When defining a neural network,
    you’re limited in neither the number of layers, nor the number of neurons per
    layer.
  id: totrans-404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7\. 具有三个层的神经网络。在定义神经网络时，你既不受层数的限制，也不受每层神经元数量的限制。
- en: '![](Images/05fig07.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig07.jpg)'
- en: 'To recap what you’ve learned so far, let’s put together all the notions we
    mentioned in one concise list:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾你到目前为止学到的内容，让我们将我们提到的所有概念汇总成一个简洁的列表：
- en: A *sequential neural network* is a mechanism to map features, or input neurons,
    *x*, to predictions, or output neurons, *y*. You do this by stacking layers of
    simple functions one by one in a sequential manner.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*顺序神经网络*是将特征或输入神经元*x*映射到预测或输出神经元*y*的机制。你通过按顺序逐个堆叠简单函数的层来实现这一点。'
- en: A *layer* is a prescription to map a given input to an output. Computing the
    output of a layer for a batch of data is called a *forward pass*. Likewise, to
    compute the forward pass for a sequential network is to compute the forward pass
    of each layer sequentially, starting with the input.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层*是将给定输入映射到输出的规定。对一批数据的层输出计算称为*前向传播*。同样，计算顺序网络的向前传播就是按顺序计算每一层的向前传播，从输入开始。'
- en: The sigmoid function is an activation function that takes a vector of real-valued
    neurons and *activates* them so that they’re mapped to the range [0,1]. You interpret
    values close to 1 as activating.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 函数是一种激活函数，它将实值神经元的向量 *激活*，使它们映射到范围 [0,1]。你将接近 1 的值解释为激活。
- en: Given a weight matrix *W* and a bias term *b*, applying the affine-linear transformation
    *Wx* + *b* forms a layer. This kind of layer is usually called a *dense layer*
    or *fully connected layer*. Going forward, we’ll stick to calling them *dense
    layers*.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定权重矩阵 *W* 和偏置项 *b*，应用仿射线性变换 *Wx* + *b* 形成一个层。这种类型的层通常称为 *密集层* 或 *全连接层*。向前推进，我们将坚持使用
    *密集层* 这个名称。
- en: Depending on the implementation, dense layers may or may not come with activations
    built in; you might see s(*Wx* + *b*) as the layer, not just the affine-linear
    transformation. On the other hand, it’s common to consider just the activation
    function a layer, and you’ll do so in your implementation. In the end, whether
    to add activations into your dense layers or not is just a slightly different
    view of how to split up and group parts of a function into a logical unit.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据实现方式，密集层可能内置了激活，你可能看到 s(*Wx* + *b*) 作为层，而不仅仅是仿射线性变换。另一方面，通常只考虑激活函数为一个层，你也会在实现中这样做。最终，是否将激活添加到你的密集层中，只是对如何将函数的部分拆分和分组为逻辑单元的略微不同的看法。
- en: A feed-forward neural network is a sequential network consisting of dense layers
    with activations. For historical reasons we don’t have space to dive into, this
    architecture is also often called *multilayer perceptron,* or *MLP* for short.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络是由具有激活的密集层组成的顺序网络。由于历史原因，我们没有空间深入探讨，这种架构也常被称为 *多层感知器*，或简称为 *MLP*。
- en: 'All neurons that are neither input nor output neurons are called *hidden units*.
    In contrast, input and output neurons are sometimes called *visible units*. The
    intuition behind this is that hidden units are *internal to the network*, whereas
    the visible ones are directly observable. This is somewhat of a stretch, because
    you normally have access to any part of the system, but it’s nevertheless good
    to know this nomenclature. Consequently, the layers between input and output are
    called *hidden layers*: each sequential network with at least two layers has at
    least a hidden one.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有既不是输入也不是输出神经元的神经元都称为 *隐藏单元*。相比之下，输入和输出神经元有时被称为 *可见单元*。背后的直觉是隐藏单元是 *网络内部的*，而可见单元是直接可观察的。这有点牵强，因为你通常可以访问系统的任何部分，但了解这个术语还是有用的。因此，输入和输出之间的层被称为
    *隐藏层*：每个至少有两个层的顺序网络至少有一个隐藏层。
- en: If not stated otherwise, *x* will stand for input to the network, *y* for output
    of it; sometimes with subscripts to indicate which sample you’re considering.
    Stacking many layers to build a large network with many hidden layers is called
    a *deep neural network*, hence the name *deep learning*.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有特别说明，*x* 将代表网络的输入，*y* 代表输出；有时会使用下标来表示你正在考虑的哪个样本。将多层堆叠以构建具有许多隐藏层的大型网络称为 *深度神经网络*，因此得名
    *深度学习*。
- en: '|  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Nonsequential neural networks**'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**非顺序神经网络**'
- en: At this point in the book, you’re concerned with only *sequential* neural networks,
    in which the layers form a sequence. In a sequential network, you start with the
    input, and each following (hidden) layer has precisely one predecessor and one
    successor, ending in the output layer. This is enough to cover everything you
    need in order to apply deep learning to the game of Go.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，你只关心 *顺序型* 神经网络，其中层形成序列。在顺序网络中，你从输入开始，每个后续（隐藏）层恰好有一个前驱和一个后继，最终到达输出层。这足以涵盖你应用深度学习到围棋游戏所需的一切。
- en: In general, the theory of neural networks allows for arbitrary nonsequential
    architectures as well. For instance, in some applications it makes sense to concatenate,
    or add, the output of two layers (you merge two or more previous layers). In such
    a scenario, you merge multiple inputs and emit one output.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络理论允许任意非顺序架构。例如，在某些应用中，将两个层的输出连接或相加是有意义的（你合并两个或更多之前的层）。在这种情况下，你合并多个输入并输出一个结果。
- en: In other applications, it can be useful to split an input into several outputs.
    In general, a layer can have multiple inputs and outputs. We introduce multi-input
    and multi-output networks in [chapters 11](kindle_split_023.xhtml#ch11) and [12](kindle_split_024.xhtml#ch12),
    respectively.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他应用中，将输入分成几个输出可能是有用的。一般来说，一层可以有多个输入和输出。我们在第11章和第12章分别介绍了多输入和多输出网络。
- en: '|  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The setup of a multilayer perceptron with *l* layers is fully described by
    the set of weights *W* = *W*¹, ..., *W^l*, the set of biases *b* = *b*¹, ...,
    *b^l*, and the set of activation functions chosen for each layer. But a vital
    ingredient for learning from data and updating parameters is still missing: loss
    functions and how to optimize them.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 具有l层的多层感知器的设置完全由权重集*W* = *W*¹，...，*W^l*、偏置集*b* = *b*¹，...，*b^l*以及为每一层选择的激活函数集描述。但学习数据和更新参数的一个关键成分仍然缺失：损失函数及其优化方法。
- en: 5.4\. How good are our predictions? Loss functions and optimization
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 我们的预测有多好？损失函数和优化
- en: '[Section 5.3](#ch05lev1sec3) defined how to set up a feed-forward neural network
    and pass input data through it, but you still don’t know how to assess the quality
    of your predictions. To do so, you need a measure to define how close prediction
    and actual outcome are.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5.3节](#ch05lev1sec3)定义了如何设置前馈神经网络并传递输入数据，但你仍然不知道如何评估预测的质量。为了做到这一点，你需要一个度量来定义预测和实际结果之间的接近程度。'
- en: 5.4.1\. What is a loss function?
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 什么是损失函数？
- en: 'To quantify how much you missed your target with your prediction, we introduce
    the concept of *loss functions*, often called *objective functions*. Let’s say
    you have a feed-forward network with weights *W*, biases *b*, and sigmoid activation
    functions. For a given set of input features *X*[1], ..., *Xk* and respective
    labels *ŷ*[1], ..., *ŷ[k]* (the symbol *ŷ* for a label is pronounced *y-hat*),
    using your network you can compute predictions *y*[1], ..., *y[k]*. In such a
    scenario, a loss function is defined as follows:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化你的预测与目标之间的差距，我们引入了*损失函数*的概念，通常也称为*目标函数*。假设你有一个带有权重*W*、偏置*b*和sigmoid激活函数的前馈网络。对于给定的一组输入特征*X*[1]，...，*Xk*和相应的标签*ŷ*[1]，...，*ŷ[k]*（标签的符号*ŷ*读作*y-hat*），使用你的网络你可以计算出预测值*y*[1]，...，*y[k]*。在这种情况下，损失函数的定义如下：
- en: '![](Images/p0099_01.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/p0099_01.jpg)'
- en: Here, Loss(*y[i]*,*ŷ[i]*) ≥ 0, and Loss is a *differentiable function*. A loss
    function is a smooth function that assigns a non-negative value to each (prediction,
    label) pair. The loss of a bunch of features and labels is the sum of losses of
    the samples. A loss function assesses the fit of your algorithm’s parameters,
    given the data you show it. Your training target is to *minimize loss* by finding
    good strategies to adapt your parameters.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Loss(*y[i]*,*ŷ[i]*) ≥ 0，且Loss是一个*可微函数*。损失函数是一个平滑函数，它为每个（预测，标签）对分配一个非负值。一组特征和标签的损失是样本损失的加和。损失函数评估了你的算法参数在给定数据时的拟合度。你的训练目标是*最小化损失*，通过找到适应参数的良好策略来实现。
- en: 5.4.2\. Mean squared error
  id: totrans-428
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2\. 均方误差
- en: 'One example of a widely used loss function is *mean squared error* (*MSE*).
    Although MSE isn’t ideal for our use case, it’s one of the most intuitive loss
    functions to work with. You measure how close your prediction was to the actual
    label, by measuring the squared distance and averaging over all observed examples.
    Writing *ŷ* = *ŷ*[1], ..., *ŷ[k]* for labels and *y* = *y*[1], ..., *y[k]* for
    predictions, the mean squared error is defined as follows:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的损失函数之一是*均方误差*（MSE）。尽管MSE对于我们的用例来说并不理想，但它是最直观的损失函数之一。你通过测量预测值与实际标签之间的平方距离，并对所有观察到的示例进行平均来衡量你的预测有多接近实际标签。用*ŷ*
    = *ŷ*[1]，...，*ŷ[k]*表示标签，用*y* = *y*[1]，...，*y[k]*表示预测，均方误差的定义如下：
- en: '![](Images/p0099_02.jpg)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/p0099_02.jpg)'
- en: We’ll present the benefits and drawbacks of various loss functions after you’ve
    seen an application of the theory presented here. For now, let’s implement the
    mean squared error in Python.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在你看到这里所提出的理论的应用之后，我们将介绍各种损失函数的优缺点。现在，让我们在Python中实现均方误差。
- en: Listing 5.10\. Mean squared error loss function and its derivative
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.10\. 均方误差损失函数及其导数
- en: '[PRE26]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '***1* Use the mean squared error as your loss function.**'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用均方误差作为你的损失函数。**'
- en: '***2* By defining MSE as 0.5 times the square difference between predictions
    and labels...**'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 通过将均方误差定义为预测值与标签之间平方差的0.5倍...**'
- en: '***3* ...the loss derivative is simply predictions—labels.**'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* ...损失导数仅仅是预测值减去标签。**'
- en: 'Note that you implemented not only the loss function itself, but also its derivative
    with respect to your predictions: `loss_derivative`. This derivative is a vector
    and is obtained by subtracting labels from predictions.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你不仅实现了损失函数本身，还实现了它相对于预测的导数：`loss_derivative`。这个导数是一个向量，通过从预测中减去标签来获得。
- en: Next, you’ll see how derivatives like this one for MSE play a crucial role in
    training neural networks.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将看到这样的导数在训练神经网络中扮演着至关重要的角色。
- en: 5.4.3\. Finding minima in loss functions
  id: totrans-439
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3\. 在损失函数中寻找最小值
- en: The loss function for a set of predictions and labels gives you information
    about how well tuned the parameters of your model are. The smaller the loss, the
    better your predictions, and vice versa. The loss function itself is a function
    of the parameters of your network. In your MSE implementation, you don’t directly
    supply weights, but they’re *implicitly* given through `predictions`, because
    you use weights to compute them.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组预测和标签的损失函数会告诉你你的模型参数调整得有多好。损失越小，你的预测越好，反之亦然。损失函数本身是网络参数的函数。在你的均方误差（MSE）实现中，你并不直接提供权重，但它们通过
    `predictions` *隐式地*给出，因为你使用权重来计算它们。
- en: In theory, you know from calculus that to minimize the loss, you need to compute
    its *derivative* and set it to 0\. We call the set of parameters at this point
    a *solution*. Computing the derivative of a function and evaluating it at a specific
    point is called *computing the gradient*. You’ve done the first step in computing
    the derivative in your MSE implementation, but there’s more to it. What you aim
    for is to explicitly compute gradients for all weight and bias terms in your network.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，你知道从微积分中，为了最小化损失，你需要计算它的 *导数* 并将其设置为 0。我们称这个点的参数集为一个 *解*。计算函数的导数并在特定点进行评估称为
    *计算梯度*。你在 MSE 实现中已经完成了计算导数的第一个步骤，但还有更多。你的目标是明确计算网络中所有权重和偏置项的梯度。
- en: If you need a refresher on the basics of calculus, make sure to check out [appendix
    A](kindle_split_028.xhtml#app01). [Figure 5.8](#ch05fig08) shows a surface in
    three-dimensional space. This surface can be interpreted as a loss function for
    two-dimensional input. The first two axes represent your weights, and the third,
    upward-pointing axis indicates the loss value.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要复习微积分的基础知识，请务必查看[附录 A](kindle_split_028.xhtml#app01)。[图 5.8](#ch05fig08)
    展示了三维空间中的一个表面。这个表面可以解释为二维输入的损失函数。前两个轴代表你的权重，第三个向上指的轴表示损失值。
- en: Figure 5.8\. An example of a loss function for two-dimensional input (a loss
    surface). This surface has a minimum around the dark area in the lower right that
    can be computed by solving the derivative of the loss function.
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.8\. 二维输入的损失函数示例（一个损失表面）。这个表面在右下角的暗区有一个最小值，可以通过求解损失函数的导数来计算。
- en: '![](Images/05fig08_alt.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig08_alt.jpg)'
- en: 5.4.4\. Gradient descent to find minima
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.4\. 使用梯度下降寻找最小值
- en: 'Intuitively speaking, when you compute the gradient of a function for a given
    point, that gradient points in the direction of steepest ascent. Starting with
    a loss function, Loss, and a set of parameters *W*, the *gradient descent* algorithm
    to find a minimum for this function goes like this:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，当你对一个给定点的函数计算梯度时，这个梯度指向最陡上升的方向。从一个损失函数 Loss 和一组参数 *W* 开始，梯度下降算法寻找该函数的最小值如下：
- en: Compute the gradient Δ of Loss for the current set of parameters *W* (compute
    the derivative of Loss with respect to each weight *W*).
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算当前参数集 *W* 的 Loss 的梯度 Δ（计算 Loss 对每个权重 *W* 的导数）。
- en: Update *W* by subtracting Δ from it. We call this step *following the gradient*.
    Because Δ points in the direction of steepest ascent, subtracting it leads you
    in the direction of steepest descent.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从 *W* 中减去 Δ 来更新 *W*。我们称这一步为 *跟随梯度*。因为 Δ 指向最陡上升的方向，减去它会使你朝着最陡下降的方向前进。
- en: Repeat until Δ is 0.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到 Δ 为 0。
- en: Because your loss function is non-negative, you know in particular that it has
    a minimum. It could have many, even infinitely many, minima. For instance, if
    you think about a flat surface, *every* point on it is a minimum.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你的损失函数是非负的，你知道它有一个最小值。它可能有多个，甚至无限多个最小值。例如，如果你考虑一个平坦的表面，*表面上的每个点*都是一个最小值。
- en: '|  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Local and global minima**'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部和全局最小值**'
- en: A point with zero gradient reached by gradient descent is by definition a minimum.
    The precise mathematical definition of a minimum for differentiable functions
    of many variables is relatively involved and uses information about the *curvature*
    of the function.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降达到的零梯度点是定义上的最小值。对于多变量可微函数的最小值的精确数学定义相对复杂，并使用了关于函数**曲率**的信息。
- en: 'With gradient descent, you’ll eventually find a minimum; you can follow the
    gradient of the function until you find a point of zero gradient. There’s only
    one caveat: you don’t know whether this minimum is a *local* or a *global* minimum.
    You might be stuck in a plateau that locally is the smallest point the function
    can take, but other points might have a smaller absolute value. The marked point
    in [figure 5.8](#ch05fig08) is a local minimum, but clearly smaller values exist
    on this surface.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降，你最终会找到一个最小值；你可以跟随函数的梯度，直到找到一个零梯度点。只有一个注意事项：你不知道这个最小值是**局部**还是**全局**最小值。你可能被困在一个局部最小值上，这个局部最小值是函数可以取到的最小点，但其他点可能有更小的绝对值。图5.8中的标记点是一个局部最小值，但显然在这个表面上存在更小的值。
- en: 'What we do to resolve this problem might strike you as strange: we ignore it.
    In practice, gradient descent often leads to satisfying results, so in the context
    of loss functions for neural networks, we tend to ignore the question of whether
    a minimum is local or global. We usually won’t even run the algorithm until convergence,
    but rather stop after a predefined number of steps.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为了解决这个问题所采取的做法可能让你觉得奇怪：我们忽略它。在实践中，梯度下降通常会导致令人满意的结果，因此在神经网络损失函数的上下文中，我们倾向于忽略最小值是局部还是全局的问题。我们通常甚至不会运行算法直到收敛，而是在预定义的步骤数后停止。
- en: '|  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[Figure 5.9](#ch05fig09) shows how gradient descent works for the loss surface
    from [figure 5.8](#ch05fig08) and a choice of parameters indicated by the marked
    point in the upper right.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.9](#ch05fig09) 展示了从[图5.8](#ch05fig08)的损失表面以及右上角标记点所指示的参数选择中，梯度下降是如何工作的。'
- en: Figure 5.9\. Iteratively following the gradients of a loss function will eventually
    lead you to a minimum.
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.9\. 依次跟随损失函数的梯度最终会引导你到达最小值。
- en: '![](Images/05fig09_alt.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9的替代文本](Images/05fig09_alt.jpg)'
- en: 'In your MSE implementation, you’ve seen that the derivative of the mean squared
    error loss is easy to calculate *formally*: it’s the difference between labels
    and predictions. But to *evaluate such a derivative*, you have to compute predictions
    first. To get a view of the gradients for all parameters, you have to evaluate
    and aggregate derivatives for every sample in your training set. Given that you’re
    usually dealing with many thousands, if not millions, of data samples for your
    network, this is practically infeasible. Instead, you approximate gradient computation
    with a technique called *stochastic gradient descent*.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的均方误差(MSE)实现中，你已经看到均方误差损失的导数很容易**形式化**计算：它是标签和预测之间的差异。但为了**评估**这样的导数，你必须首先计算预测值。为了查看所有参数的梯度，你必须评估和聚合训练集中每个样本的导数。鉴于你通常要处理成千上万，甚至数百万的数据样本，这在大多数情况下实际上是不切实际的。相反，你使用一种称为**随机梯度下降**的技术来近似梯度计算。
- en: 5.4.5\. Stochastic gradient descent for loss functions
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.5\. 损失函数的随机梯度下降
- en: To compute gradients and apply gradient descent for a neural network, you’d
    have to evaluate the loss function and its derivative with respect to network
    parameters at every single point in the training set, which is too expensive in
    most cases. Instead, you use a technique called *stochastic gradient descent*
    (*SGD*). To run SGD, you first select a few samples from your training set, which
    you call a *mini-batch*. Each mini-batch is selected with a fixed length that
    we call the *mini-batch size*. For a classification problem like the handwritten
    digit problem you’re tackling, it’s good practice to choose a batch size in the
    same order of magnitude as the number of labels so as to make sure each label
    is represented in a mini-batch.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算梯度并应用神经网络中的梯度下降，你必须评估损失函数及其相对于网络参数的导数，在训练集中的每一个点上，这在大多数情况下太昂贵了。相反，你使用一种称为**随机梯度下降**（SGD）的技术。要运行SGD，你首先从你的训练集中选择一些样本，这些样本被称为**小批量**。每个小批量以一个固定的长度被选择，我们称之为**小批量大小**。对于你正在处理的分类问题，例如手写数字问题，选择一个与小批量标签数量同数量级的批量大小是一个好的实践，以确保每个标签在小批量中都有代表。
- en: For a given feed-forward neural network with *l* layers and a mini-batch of
    input data *x*[1], ..., *x[k]* of mini-batch size *k*, you can compute the forward
    pass of your neural network and compute the loss for that mini-batch. For each
    sample *x[j]* in this batch, you can then evaluate the gradient of your loss function
    with respect to any parameter in your network. The weight and bias gradients in
    layer *i* we call Δ*[j]W^i* and Δ*[j]b^i*, respectively.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个具有**l**层和输入数据迷你批次**x**[1]，...，**x[k]**的迷你批次大小**k**的前馈神经网络，你可以计算你神经网络的正向传播并计算该迷你批次的损失。对于这个批次中的每个样本**x[j]**，你可以然后评估你的损失函数相对于你网络中任何参数的梯度。我们称层**i**中的权重和偏置梯度分别为Δ*[j]W^i*和Δ*[j]b^i*。
- en: 'For each layer and each sample in the batch, you compute the respective gradients
    and use the following *update rules* for parameters:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个层和批次中的每个样本，你计算相应的梯度并使用以下**更新规则**来更新参数：
- en: '![](Images/p0103_01.jpg)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/p0103_01.jpg)'
- en: You update your parameters by subtracting the cumulative error you receive for
    that batch. Here *a* > 0 denotes the *learning rate*, an entity specified ahead
    of training the network.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过减去为该批次收到的累积误差来更新你的参数。这里*a* > 0表示**学习率**，这是一个在训练网络之前指定的实体。
- en: You’d get much more precise information about gradients if you were to sum over
    all training samples at once. Using mini-batches is a compromise in terms of gradient
    accuracy, but is much more computationally efficient. We call this method *stochastic*
    gradient descent because the mini-batch samples are randomly chosen. Although
    in gradient descent you have a theoretical guarantee of approaching a local minimum,
    in SGD that’s not the case. [Figure 5.10](#ch05fig10) displays the typical behavior
    of SGD. Some of your approximate stochastic gradients may not point toward a descending
    direction, but given enough iterations, you’ll usually get close to a (local)
    minimum.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一次性对所有训练样本求和，你会得到关于梯度的更精确信息。使用迷你批次是在梯度精度和计算效率之间的折衷。我们称这种方法为**随机**梯度下降，因为迷你批次样本是随机选择的。虽然在梯度下降中你有理论上的保证可以接近局部最小值，但在SGD中则不是这样。[图5.10](#ch05fig10)显示了SGD的典型行为。你的一些近似随机梯度可能不会指向下降方向，但给定足够的迭代次数，你通常会接近一个（局部）最小值。
- en: Figure 5.10\. Stochastic gradients are less precise, so when following them
    on a loss surface, you might take a few detours before closing in on a local minimum.
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.10\. 随机梯度不够精确，所以在沿着损失表面追踪时，你可能会在接近局部最小值之前走一些弯路。
- en: '![](Images/05fig10_alt.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig10_alt.jpg)'
- en: '|  |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Optimizers**'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器**'
- en: Computing (stochastic) gradients is defined by the fundamental principles of
    calculus. The way you use the gradients to update parameters isn’t. Techniques
    like the update rule for SGD are called *optimizers*.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 计算随机梯度是由微积分的基本原理定义的。你使用梯度来更新参数的方式则不是。像SGD的更新规则这样的技术被称为**优化器**。
- en: Many other optimizers exist, as well as more sophisticated versions of stochastic
    gradient descent. We cover some of the extensions of SGD in [chapter 7](kindle_split_019.xhtml#ch07).
    Most of these extensions revolve around adapting the learning rate over time or
    have more granular updates for individual weights.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多其他优化器，以及更复杂的随机梯度下降版本。我们在第7章（kindle_split_019.xhtml#ch07）中介绍了SGD的一些扩展。这些扩展大多围绕随着时间的推移调整学习率或对单个权重进行更细粒度的更新。
- en: '|  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.4.6\. Propagating gradients back through your network
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.6\. 通过你的网络反向传播梯度
- en: We discussed how to update parameters of a neural network by using stochastic
    gradient descent, but we didn’t explain how to arrive at the gradients. The algorithm
    used to compute these gradients is called the *backpropagation* algorithm and
    is covered in detail in [appendix B](kindle_split_029.xhtml#app02). This section
    gives you the intuition behind backpropagation and the necessary building blocks
    to implement a feed-forward network yourself.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了如何通过使用随机梯度下降来更新神经网络的参数，但没有解释如何得到梯度。用于计算这些梯度的算法被称为**反向传播**算法，并在附录B（kindle_split_029.xhtml#app02）中详细说明。本节为你提供了反向传播的直觉以及实现前馈网络所需的必要构建块。
- en: 'Recall that in a feed-forward network, you run the forward pass on data by
    computing one simple building block after another. From the output of the last
    layer, the network’s predictions, and the labels, you can compute the loss. The
    loss function itself is the *composition* of simpler functions. To compute the
    derivative of the loss function, you can use a fundamental property from calculus:
    the chain rule. This rule roughly says that the derivative of composed functions
    is the composition of the derivatives of these functions. Therefore, just as you
    passed input data forward layer by layer, you can *pass derivatives back layer
    by layer*. You propagate derivatives back through your network, hence the name
    *backpropagation*. In [figure 5.11](#ch05fig11), you can see the backpropagation
    in action for a feed-forward network with two dense layers and sigmoid activations.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在一个前馈网络中，你通过计算一个简单的构建块之后另一个构建块来运行前向传播。从最后一层的输出，即网络的预测和标签，你可以计算损失。损失函数本身是更简单函数的*组合*。为了计算损失函数的导数，你可以使用微积分的一个基本性质：链式法则。这条规则大致说明，复合函数的导数是这些函数导数的组合。因此，正如你逐层传递输入数据一样，你也可以逐层传递导数。你通过你的网络传播导数，因此得名*反向传播*。在[图5.11](#ch05fig11)中，你可以看到对于具有两个密集层和sigmoid激活函数的前馈网络的反向传播的实际应用。
- en: Figure 5.11\. Forward and backward passes in a two-layer feed-forward neural
    network with sigmoid activations and MSE loss function
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.11\. 具有sigmoid激活函数和MSE损失函数的两个层前馈神经网络的正向和反向传播
- en: '![](Images/05fig11_alt.jpg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig11_alt.jpg)'
- en: 'To guide you through [figure 5.11](#ch05fig11), let’s take it step-by-step:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引导你通过[图5.11](#ch05fig11)，让我们一步一步来：
- en: '***Forward pass on training data.*** In this step, you take an input data sample
    *x* and pass it through the network to arrive at a prediction, as follows:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***在训练数据上的前向传播。*** 在这一步，你取一个输入数据样本 *x* 并将其通过网络以得到一个预测，如下所示：'
- en: 'You compute the affine-linear part: *Wx* + *b*.'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你计算仿射线性部分：*Wx* + *b*。
- en: You apply the sigmoid function σ(*x*) to the result. Note that we’re abusing
    notation slightly in that *x* in a computation step means the output of the previous
    result.
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将sigmoid函数 σ(*x*) 应用到结果上。注意，我们在计算步骤中稍微滥用符号，这里的 *x* 指的是前一个结果的输出。
- en: You repeat these two steps until you arrive at the output layer. We chose two
    layers in this example, but the number of layers doesn’t matter.
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你重复这些两个步骤，直到你到达输出层。在这个例子中，我们选择了两个层，但层数并不重要。
- en: '***Loss function evaluation.*** In this step, you take your labels *ŷ* for
    the sample *x* and compare them to your predictions *y* by computing a loss value.
    In this example, you choose mean squared error as your loss function.'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***损失函数评估。*** 在这一步，你取样本 *x* 的标签 *ŷ* 并通过计算损失值将其与你的预测 *y* 进行比较。在这个例子中，你选择均方误差作为你的损失函数。'
- en: '***Propagating error terms back.*** In this step, you take your loss value
    and pass it back through the network. You do so by computing derivatives layer-wise,
    which is possible because of the chain rule. While the forward pass feeds input
    data through the network in one direction, the backward pass feeds error terms
    back in the opposite direction.'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***反向传播错误项。*** 在这一步，你取你的损失值并将其通过网络反向传播。你是通过逐层计算导数来做到这一点的，这是由于链式法则。当前向传播将输入数据通过网络单向传递时，反向传播将错误项反向传递。'
- en: You propagate error terms, or deltas, denoted by Δ, in the inverse order of
    the forward pass.
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你以与前向传播相反的顺序传播错误项，或称为 Δ。
- en: To begin with, you compute the derivative of the loss function, which will be
    your initial Δ. Again, as in the forward pass, we abuse notation and call the
    propagated error term Δ at every step in the process.
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你计算损失函数的导数，这将作为你的初始 Δ。同样，正如在前向传播中一样，我们滥用符号，在过程中的每一步都称传播的错误项为 Δ。
- en: 'You compute the derivative of the sigmoid with respect to its input, which
    is simply σ·(1 – σ). To pass Δ to the next layer, you can do component-wise multiplication:
    σ(1 – σ) · Δ.'
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你计算sigmoid关于其输入的导数，这简单地是 σ·(1 – σ)。为了将 Δ 传递到下一层，你可以进行逐元素乘法：σ(1 – σ) · Δ。
- en: The derivative of your affine-linear transformation *Wx* + *b* with respect
    to *x* is simply *W*. To pass on Δ, you compute *W^t* · Δ.
  id: totrans-490
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你对仿射线性变换 *Wx* + *b* 关于 *x* 的导数简单地是 *W*。为了传递 Δ，你计算 *W^t* · Δ。
- en: These two steps are repeated until you reach the first layer of the network.
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两个步骤会重复进行，直到你达到网络的第1层。
- en: '***Update weights with gradient information.*** In the final step, you use
    the deltas you computed along the way to update your network parameters (weights
    and bias terms).'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***使用梯度信息更新权重。*** 在最后一步，你使用沿途计算出的 delta 来更新你的网络参数（权重和偏置项）。'
- en: The sigmoid function doesn’t have any parameters, so there’s nothing for you
    to do.
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sigmoid 函数没有参数，所以你没有什么可做的。
- en: The update Δ*b* that the bias term in each layer receives is simply Δ.
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个层中的偏置项接收的更新 Δ*b* 简单地是 Δ。
- en: The update Δ*W* for the weights in a layer is given by Δ · *x*^T (you need to
    transpose *x* before multiplying it with delta).
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层中权重的更新 Δ*W* 由 Δ · *x*^T 给出（你需要在乘以 delta 之前将 *x* 转置）。
- en: Note that we started out by saying *x* is a single sample. Everything we discussed
    carries over to mini-batches, however. If *x* denotes a mini-batch of samples
    (*x* is a matrix in which every column is an input vector), the computations of
    forward and backward passes look exactly the same.
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我们最初说 *x* 是一个单独的样本。然而，我们讨论的所有内容都适用于小批量。如果 *x* 表示样本的小批量（*x* 是一个矩阵，其中每一列是一个输入向量），则前向和反向传递的计算看起来完全相同。
- en: Now that you have all the necessary mathematics covered to build and run a feed-forward
    network, let’s apply what you’ve learned on a theoretical level by building a
    neural network implementation from scratch.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经涵盖了构建和运行前馈网络所需的所有必要数学知识，让我们通过从头开始构建神经网络实现来应用你在理论层面学到的知识。
- en: 5.5\. Training a neural network step-by-step in Python
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. 训练神经网络步骤详解
- en: 'The preceding section covered a lot of theoretical ground, but on a conceptual
    level, you got away with working through only a few basic concepts. For our implementation,
    you need to worry about only three things: a `Layer` class, a `SequentialNetwork`
    class that’s built by adding several `Layer` objects one by one, and a `Loss`
    class that the network needs for backpropagation. These three classes are covered
    next, after which you’ll load and inspect handwritten digit data and apply your
    network implementation to it. [Figure 5.12](#ch05fig12) shows how those Python
    classes fit together to implement the forward and backward passes described in
    the previous section.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节涵盖了大量的理论内容，但在概念层面上，你只需通过几个基本概念就能完成任务。对于我们的实现，你只需要关注三件事：一个 `Layer` 类，一个通过逐个添加
    `Layer` 对象构建的 `SequentialNetwork` 类，以及网络需要用于反向传播的 `Loss` 类。这三个类将在下面介绍，之后你将加载和检查手写数字数据，并将你的网络实现应用于它。[图
    5.12](#ch05fig12) 展示了这些 Python 类如何组合在一起以实现前一节中描述的前向和反向传递。
- en: Figure 5.12\. Class diagram for your Python implementation of a feed-forward
    network. A `SequentialNetwork` contains several `Layer` instances. Each `Layer`
    implements a mathematical function and its derivative. The forward and backward
    methods implement the forward and backward pass, respectively. A `Loss` instance
    calculates your loss function, the error between your prediction and your training
    data.
  id: totrans-500
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.12. Python 实现的前馈网络的类图。一个 `SequentialNetwork` 包含多个 `Layer` 实例。每个 `Layer`
    实现一个数学函数及其导数。前向和反向方法分别实现前向和反向传递。一个 `Loss` 实例计算你的损失函数，即预测与训练数据之间的误差。
- en: '![](Images/05fig12_alt.jpg)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig12_alt.jpg)'
- en: 5.5.1\. Neural network layers in Python
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1. Python 中的神经网络层
- en: To start with a general `Layer` class, note that layers, as we discussed them
    before, come with not only a prescription to deal with input data (the forward
    pass), but also a mechanism to *propagate back* error terms. In order not to recompute
    activation values on the backward pass, it’s practical to maintain the *state*
    of data coming into and out of the layer for both passes. Having said that, the
    following initialization of `Layer` should be straightforward. You’ll begin creating
    a layers module; later in this chapter you’ll use the components in this module
    to build up a neural network.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从通用的 `Layer` 类开始，请注意，正如我们之前讨论的那样，层不仅包含处理输入数据（前向传递）的规范，而且还包含一个机制来 *反向传播* 错误项。为了不在反向传递中重新计算激活值，对于两个传递，维护数据进入和离开层的
    *状态* 是实用的。话虽如此，以下对 `Layer` 的初始化应该是直截了当的。你将开始创建一个层模块；在本章的后面，你将使用该模块中的组件来构建神经网络。
- en: Listing 5.11\. Base layer implementation
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.11. 基础层实现
- en: '[PRE27]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '***1* Layers are stacked to build a sequential neural network.**'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 层被堆叠以构建一个顺序神经网络。**'
- en: '***2* A layer knows its predecessor (previous)...**'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 一层知道其前驱（上一个）...**'
- en: '***3* ...and its successor (next).**'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* ...及其后续者（下一个）。**'
- en: '***4* Each layer can persist data flowing into and out of it in the forward
    pass.**'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 每个层可以在前向传递中持久化流入和流出它的数据。**'
- en: '***5* Analogously, a layer holds input and output data for the backward pass.**'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 类似地，层在反向传播中持有输入和输出数据。**'
- en: A layer has a list of parameters and stores both its current input and output
    data, as well as the respective input and output deltas for the backward pass.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 一层有一个参数列表，并存储其当前输入和输出数据，以及反向传播的相应输入和输出 delta。
- en: Also, because you’re concerned with sequential neural networks, it makes sense
    to give each layer a successor and a predecessor. Continuing with the definition,
    you add the following.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为你关注的是顺序神经网络，所以给每个层一个后续者和前驱是有意义的。继续定义，你添加以下内容。
- en: Listing 5.12\. Connecting layers through successors and predecessors
  id: totrans-513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.12\. 通过后续者和前驱连接层
- en: '[PRE28]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '***1* This method connects a layer to its direct neighbors in the sequential
    network.**'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 此方法将一层连接到顺序网络中的直接邻居。**'
- en: Next, you provide stubs for forward and backward passes in an abstract `Layer`
    class, which subclasses have to implement.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你为抽象的 `Layer` 类提供前向和反向传播的存根，子类必须实现。
- en: Listing 5.13\. Forward and backward passes in a layer of a sequential neural
    network
  id: totrans-517
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.13\. 顺序神经网络层的前向和反向传播
- en: '[PRE29]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '***1* Each layer implementation has to provide a function to feed input data
    forward.**'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 每个层实现必须提供一个函数来提供前向输入数据。**'
- en: '***2* input_data is reserved for the first layer; all others get their input
    from the previous output.**'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 输入数据保留用于第一层；所有其他层都从前一个输出获取输入。**'
- en: '***3* Layers have to implement backpropagation of error terms—a way to feed
    input errors backward through the network.**'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 层必须实现误差项的反向传播——一种将输入误差反向传递通过网络的方式。**'
- en: '***4* Input delta is reserved for the last layer; all other layers get their
    error terms from their successor.**'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 输入 delta 保留用于最后一层；所有其他层都从其后续层获取误差项。**'
- en: '***5* You compute and accumulate deltas per mini-batch, after which you need
    to reset these deltas.**'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 你计算并累积每个 mini-batch 的 delta，之后你需要重置这些 delta。**'
- en: '***6* Update layer parameters according to current deltas, using the specified
    learning_rate.**'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 根据当前 delta 更新层参数，使用指定的学习率。**'
- en: '***7* Layer implementations can print their properties.**'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 层实现可以打印它们的属性。**'
- en: As helper functions, you provide `get_forward_input` and `get_backward_input`,
    which just retrieve input for the respective pass, but take special care of input
    and output neurons. On top of this, you implement a `clear_deltas` method to reset
    your deltas periodically, after accumulating deltas over mini-batches, as well
    as `update_params`, which takes care of updating parameters for this layer, after
    the network using this layer tells it to.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 作为辅助函数，你提供了 `get_forward_input` 和 `get_backward_input`，这些函数只是检索相应传递的输入，但特别关注输入和输出神经元。此外，你实现了
    `clear_deltas` 方法，定期重置你的 delta，在累积 mini-batch 上的 delta 之后，以及 `update_params`，它负责在网络的这个层使用它之后更新该层的参数。
- en: Note that as a last piece of functionality, you add a method for a layer to
    print a description of itself, which you add for convenience to get an easier
    overview of what your network looks like.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，作为最后的功能部分，你添加了一个方法，让层打印其自身的描述，这为了方便获取网络更容易的概览而添加。
- en: 5.5.2\. Activation layers in neural networks
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2\. 神经网络中的激活层
- en: Next, you’ll provide your first layer, an `ActivationLayer`. You’ll work with
    the sigmoid function, which you’ve implemented already. To do backpropagation,
    you also need its derivative, which can be implemented easily.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要提供第一层，一个 `ActivationLayer`。你将使用 sigmoid 函数，该函数你已经实现过了。为了进行反向传播，你还需要它的导数，这可以很容易地实现。
- en: Listing 5.14\. Implementation of the derivative of the sigmoid function
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.14\. Sigmoid 函数的导数实现
- en: '[PRE30]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that as for the sigmoid itself, you provide both scalar and vector versions
    for the derivative. Now, to define an `ActivationLayer` with the sigmoid function
    as activation built in, you note that the sigmoid function doesn’t have any parameters,
    so you don’t need to worry about updating any parameters just yet.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于 sigmoid 本身，你提供了标量和向量版本的导数。现在，为了定义一个内置 sigmoid 函数作为激活的 `ActivationLayer`，你注意到
    sigmoid 函数没有参数，所以你目前不需要担心更新任何参数。
- en: Listing 5.15\. Sigmoid activation layer
  id: totrans-533
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.15\. Sigmoid 激活层
- en: '[PRE31]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '***1* This activation layer uses the sigmoid function to activate neurons.**'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 此激活层使用 sigmoid 函数来激活神经元。**'
- en: '***2* The forward pass is simply applying the sigmoid to the input data.**'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 正向传播仅仅是将 sigmoid 应用于输入数据。**'
- en: '***3* The backward pass is element-wise multipli-cation of the error term with
    the sigmoid derivative evaluated at the input to this layer.**'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 反向传播是误差项与在此层输入处评估的 sigmoid 导数的逐元素乘积。**'
- en: 'Carefully inspect the gradient implementation to see how it fits the picture
    described in [figure 5.11](#ch05fig11). For this layer, the backward pass is just
    element-wise multiplication of the layer’s current delta with the sigmoid derivative
    evaluated at the input of this layer: σ(*x*) · (1 – σ(*x*)) · Δ.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细检查梯度实现，看看它如何与 [图 5.11](#ch05fig11) 中描述的图景相符。对于这一层，反向传播仅仅是层当前 delta 与在此层输入处评估的
    sigmoid 导数的逐元素乘积：σ(*x*) · (1 – σ(*x*)) · Δ。
- en: 5.5.3\. Dense layers in Python as building blocks for feed-forward networks
  id: totrans-539
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3\. Python 中的密集层作为前馈网络的构建块
- en: To continue with your implementation, you next turn to `DenseLayer`, which is
    the more complicated layer to implement, but also the last one you’ll tackle in
    this chapter. Initializing this layer takes a few more variables, because this
    time you also have to take care of the weight matrix, bias term, and their respective
    gradients.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续你的实现，你接下来转向 `DenseLayer`，这是更复杂的层来实现，但也是本章你要解决的最后一个问题。初始化这个层需要更多的变量，因为这次你还要注意权重矩阵、偏置项及其各自的梯度。
- en: Listing 5.16\. Dense layer weight initialization
  id: totrans-541
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.16\. 密集层权重初始化
- en: '[PRE32]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '***1* Dense layers have input and output dimensions.**'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 密集层具有输入和输出维度。**'
- en: '***2* Randomly initialize weight matrix and bias vector.**'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 随机初始化权重矩阵和偏置向量。**'
- en: '***3* The layer parameters consist of weights and bias terms.**'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 层参数包括权重和偏置项。**'
- en: '***4* Deltas for weights and biases are set to 0.**'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 权重和偏置的 delta 被设置为 0。**'
- en: Note that you initialize *W* and *b* randomly. There are many ways to initialize
    the weights of a neural network. Random initialization is an acceptable baseline,
    but there are many more sophisticated ways to initialize parameters so that they
    more accurately reflect the structure of your input data.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你应随机初始化 *W* 和 *b*。初始化神经网络权重的方法有很多。随机初始化是一个可接受的基线，但还有许多更复杂的方法来初始化参数，以便它们更准确地反映你的输入数据结构。
- en: '|  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Parameter initialization as a starting point for optimization**'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数初始化作为优化的起点**'
- en: Initializing parameters is an interesting topic, and we’ll discuss a few other
    initialization techniques in [chapter 6](kindle_split_018.xhtml#ch06).
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 参数初始化是一个有趣的话题，我们将在 [第 6 章](kindle_split_018.xhtml#ch06) 中讨论一些其他初始化技术。
- en: For now, just keep in mind that initialization will influence your learning
    behavior. If you think about the loss surface in [figure 5.10](#ch05fig10), initialization
    of parameters means *choosing a starting point* for optimization; you can easily
    imagine that different starting points for SGD on the loss surface of [figure
    5.10](#ch05fig10) may lead to different results. That makes initialization an
    important topic in the study of neural networks.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，只需记住初始化会影响你的学习行为。如果你考虑 [图 5.10](#ch05fig10) 中的损失表面，参数的初始化意味着 *选择一个优化起点*；你可以很容易地想象，[图
    5.10](#ch05fig10) 中损失表面的不同起点可能导致不同的结果。这使得初始化成为神经网络研究中的一个重要主题。
- en: '|  |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Now, the forward pass for a dense layer is straightforward.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，密集层的正向传播是直接的。
- en: Listing 5.17\. Dense layer forward pass
  id: totrans-554
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.17\. 密集层正向传播
- en: '[PRE33]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '***1* The forward pass of the dense layer is the affine-linear transformation
    on input data defined by weights and biases.**'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 密集层的正向传播是权重和偏置定义的输入数据的仿射线性变换。**'
- en: 'As for the backward pass, recall that to compute the delta for this layer,
    you just need to transpose *W* and multiply it by the incoming delta: *W*^tΔ.
    The gradients for *W* and *b* are also easily computed: Δ*W* = Δ*yt* and Δ*b*
    = Δ, where *y* denotes the input to this layer (evaluated with the data you’re
    currently using).'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向传播，回想一下，为了计算这一层的 delta，你只需要转置 *W* 并将其与输入的 delta 相乘：*W*^tΔ。*W* 和 *b* 的梯度也容易计算：Δ*W*
    = Δ*yt* 和 Δ*b* = Δ，其中 *y* 表示这一层的输入（使用你当前使用的数据进行评估）。
- en: Listing 5.18\. Dense layer backward pass
  id: totrans-558
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.18\. 密集层反向传播
- en: '[PRE34]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '***1* For the backward pass, you first get input data and delta.**'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 对于反向传播，你首先获取输入数据和 delta。**'
- en: '***2* The current delta is added to the bias delta.**'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将当前的 delta 添加到偏置 delta。**'
- en: '***3* Then you add this term to the weight delta.**'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 然后将此项添加到权重 delta。**'
- en: '***4* The backward pass is completed by passing an output delta to the previous
    layer.**'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 通过将输出delta传递给前一层来完成反向传播。**'
- en: The update rule for this layer is given by accumulating the deltas, according
    to the learning rate you specify for your network.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层的更新规则是通过累积delta来给出的，根据你为网络指定的学习率。
- en: Listing 5.19\. Dense layer weight update mechanism
  id: totrans-565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.19\. 密集层权重更新机制
- en: '[PRE35]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '***1* Using weight and bias deltas, you can update model parameters with gradient
    descent.**'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用权重和偏差delta，你可以使用梯度下降更新模型参数。**'
- en: '***2* After updating parameters, you should reset all deltas.**'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 更新参数后，你应该重置所有delta。**'
- en: '***3* A dense layer can be described by its input and output dimensions.**'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 一个密集层可以通过其输入和输出维度来描述。**'
- en: 5.5.4\. Sequential neural networks with Python
  id: totrans-570
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.4\. 使用Python的顺序神经网络
- en: Having taken care of layers as building blocks for a network, let’s turn to
    the networks themselves. You initialize a sequential neural network by equipping
    it with an empty list of layers and let it use MSE as the loss function, unless
    provided otherwise.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了层作为网络构建块之后，让我们转向网络本身。你通过装备一个空的层列表并让它使用MSE作为损失函数（除非提供其他方式）来初始化顺序神经网络。
- en: Listing 5.20\. Sequential neural network initialization
  id: totrans-572
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.20\. 顺序神经网络初始化
- en: '[PRE36]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '***1* In a sequential neural network, you stack layers sequentially.**'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在顺序神经网络中，你按顺序堆叠层。**'
- en: '***2* If no loss function is provided, MSE is used.**'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果没有提供损失函数，则使用MSE。**'
- en: Next, you add functionality to add layers one by one.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你添加了逐个添加层的功能。
- en: Listing 5.21\. Adding layers sequentially
  id: totrans-577
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.21\. 顺序添加层
- en: '[PRE37]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '***1* Whenever you add a layer, you connect it to its predecessor and let it
    describe itself.**'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 每当你添加一个层时，你将其连接到其前驱并让它描述自己。**'
- en: 'At the core of your network implementation is the *train* method. You use mini-batches
    as input: you shuffle training data and split it into batches of size `mini_batch_size`.
    To train your network, you feed it one mini-batch after another. To improve learning,
    you’ll feed the network your training data in batches multiple times. We say that
    we train it for multiple *epochs*. For each mini-batch, you call the `train_batch`
    method. If `test_data` is provided, you evaluate network performance after each
    epoch.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的网络实现的核心是*train*方法。你使用小批量作为输入：你打乱训练数据并将其分成大小为`mini_batch_size`的批次。为了训练你的网络，你一次又一次地以批量的形式提供你的训练数据。我们说我们进行了多次*epochs*的训练。对于每个小批量，你调用`train_batch`方法。如果提供了`test_data`，你将在每个epoch后评估网络性能。
- en: Listing 5.22\. Train method on a sequential network
  id: totrans-581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.22\. 在顺序网络上的训练方法
- en: '[PRE38]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1* To train your network, you pass over data for as many times as there
    are epochs.**'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 要训练你的网络，你需要遍历数据，次数与epoch数相同。**'
- en: '***2* Shuffle training data and create mini-batches.**'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 打乱训练数据并创建小批量。**'
- en: '***3* For each mini-batch, you train your network.**'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 对于每个小批量，你训练你的网络。**'
- en: '***4* If you provided test data, you evaluate your network on it after each
    epoch.**'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 如果你提供了测试数据，你将在每个epoch后评估你的网络。**'
- en: Now, your `train_batch` computes forward and backward passes on this mini-batch
    and updates parameters afterward.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的`train_batch`计算这个小批量上的前向和反向传播，并在之后更新参数。
- en: Listing 5.23\. Training a sequential neural network on a batch of data
  id: totrans-588
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.23\. 在数据批次上训练顺序神经网络
- en: '[PRE39]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '***1* To train the network on a mini-batch, you compute feed-forward and backward
    pass...**'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 要在小批量上训练网络，你计算前向和反向传播...**'
- en: '***2* ...and then update model parameters accordingly.**'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* ...然后相应地更新模型参数。**'
- en: The two steps, `update` and `forward_backward`, are computed as follows.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤，`update`和`forward_backward`，计算如下。
- en: Listing 5.24\. Updating rule and feed-forward and backward passes for your network
  id: totrans-593
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.24\. 网络的更新规则和前向和反向传播
- en: '[PRE40]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '***1* A common technique is to normalize the learning rate by the mini-batch
    size.**'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 一种常见的技巧是将学习率通过小批量大小进行归一化。**'
- en: '***2* Update parameters for all layers.**'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 更新所有层的参数。**'
- en: '***3* Clear all deltas in each layer.**'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 清除每个层中的所有delta。**'
- en: '***4* For each sample in the mini batch, feed the features forward layer by
    layer.**'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 对于小批量中的每个样本，逐层前向传播特征。**'
- en: '***5* Compute the loss derivative for the output data.**'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 计算输出数据的损失导数。**'
- en: '***6* Do layer-by-layer backpropagation of error terms.**'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 对误差项进行逐层反向传播。**'
- en: The implementation is straightforward, but there are a few noteworthy points
    to observe. First, you normalize the learning rate by your mini-batch size in
    order to keep updates small. Second, before computing the full backward pass by
    traversing layers in reversed order, you compute the loss derivative of the network
    output, which serves as the first input delta for the backward pass.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 实现很简单，但有几个值得注意的点。首先，您通过您的mini-batch大小来归一化学习率，以保持更新较小。其次，在通过反向顺序遍历层来计算完整的反向传递之前，您计算网络输出的损失导数，这作为反向传递的第一个输入delta。
- en: The remaining part of your `SequentialNetwork` implementation concerns model
    performance and evaluation. To evaluate your network on test data, you need to
    feed this data forward through your network, and this is precisely what `single_forward`
    does. The evaluation takes place in `evaluate`, and you return the number of correctly
    predicted results to assess accuracy.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 您的`SequentialNetwork`实现剩余部分涉及模型性能和评估。为了在测试数据上评估您的网络，您需要将数据前向传递通过您的网络，这正是`single_forward`所做的事情。评估发生在`evaluate`中，您返回正确预测的结果数量以评估准确性。
- en: Listing 5.25\. Evaluation
  id: totrans-603
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.25\. 评估
- en: '[PRE41]'
  id: totrans-604
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '***1* Pass a single sample forward and return the result.**'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 前向传递单个样本并返回结果。**'
- en: '***2* Compute accuracy on test data.**'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在测试数据上计算准确率。**'
- en: 5.5.5\. Applying your network handwritten digit classification
  id: totrans-607
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.5\. 将您的网络应用于手写数字分类
- en: Having implemented a feed-forward network, let’s return to our initial use case
    of predicting handwritten digits for the MNIST data set. After importing the necessary
    classes that you just built, you load MNIST data, initialize a network, add layers
    to it, and then train and evaluate the network with your data.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了前向网络之后，让我们回到我们的初始用例，即预测MNIST数据集的手写数字。在导入您刚刚构建的必要类之后，您加载MNIST数据，初始化一个网络，向其中添加层，然后使用您的数据训练和评估网络。
- en: To build a network, keep in mind that your input dimension is 784 and your output
    dimension is 10, the number of digits. You choose three dense layers with output
    dimensions 392, 196, and 10, respectively, and add sigmoid activations after each
    of them. With each new dense layer, you are effectively dividing layer capacity
    in half. The layer sizes and the number of layers are *hyperparameters* for this
    network. You’ve chosen these values to set up a network architecture. We encourage
    you to experiment with other layer sizes to gain intuition about the learning
    process of a network in relation to its architecture.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建网络时，请记住您的输入维度是784，输出维度是10，即数字的数量。您选择三个输出维度分别为392、196和10的密集层，并在每个密集层之后添加sigmoid激活函数。通过每个新的密集层，您实际上是在将层容量减半。层的大小和层数是该网络的*超参数*。您选择了这些值来设置网络架构。我们鼓励您尝试其他层大小，以获得关于网络学习过程与其架构之间关系的直觉。
- en: Listing 5.26\. Instantiating a neural network
  id: totrans-610
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.26\. 实例化一个神经网络
- en: '[PRE42]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '***1* Load training and test data.**'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 加载训练和测试数据。**'
- en: '***2* Initialize a sequential neural network.**'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 初始化一个序列神经网络。**'
- en: '***3* You can then add dense and activation layers one by one.**'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 您可以逐个添加密集和激活层。**'
- en: '***4* The final layer has size 10, the number of classes to predict.**'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 最终层的大小为10，即预测的类别数量。**'
- en: You train the network on data by calling `train` with all required parameters.
    You run training for 10 epochs and set the learning rate to 3.0\. As mini-batch
    size, you choose 10, the number of classes. If you were to shuffle training data
    near perfectly, in most batches each class would be represented, leading to good
    stochastic gradients.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用`train`并传入所有必需参数来在数据上训练网络。您进行10个epoch的训练，并将学习率设置为3.0。对于mini-batch大小，您选择10，即类别数量。如果您几乎完美地打乱训练数据，那么在大多数批次中每个类别都会被表示，从而产生良好的随机梯度。
- en: Listing 5.27\. Running a neural network instance on training data
  id: totrans-617
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.27\. 在训练数据上运行神经网络实例
- en: '[PRE43]'
  id: totrans-618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '***1* You can now easily train the model by specifying train and test data,
    the number of epochs, the mini-batch size, and the learning rate.**'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 您现在可以通过指定训练和测试数据、epoch数量、mini-batch大小和学习率来轻松训练模型。**'
- en: 'Now, run this on the command line:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在命令行上运行此操作：
- en: '[PRE44]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'That yields the following prompt:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下提示：
- en: '[PRE45]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The numbers you get for each epoch don’t matter here, apart from the fact that
    the result is highly dependent on the initialization of the weights. But it’s
    noteworthy to observe that you often end up with more than 95% accuracy in less
    than 10 epochs. This is quite an achievement already, especially given that you
    did this completely from scratch. In particular, this model performs vastly better
    than your naive model from the beginning of this chapter. Still, you can do much
    better.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个epoch得到的数字在这里并不重要，除了结果高度依赖于权重的初始化。但值得注意的是，你通常在不到10个epoch内就能达到超过95%的准确率。这已经是一项相当大的成就了，尤其是考虑到你是从头开始的。特别是，这个模型从一开始就比你的朴素模型表现要好得多。尽管如此，你还能做得更好。
- en: Note that for the use case you studied, you completely disregard the spatial
    structure of your input images and treat them as vectors. But it should be clear
    that the neighborhood of a given pixel is important information that should be
    used. Ultimately, you want to come back to the game of Go, and you’ve seen throughout
    [chapters 2](kindle_split_013.xhtml#ch02) and [3](kindle_split_014.xhtml#ch03)
    just how important the neighborhood of (a string of) stones is.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于你所研究的用例，你完全忽略了输入图像的空间结构，将它们视为向量。但应该清楚的是，给定像素的邻域是重要的信息，应该被利用。最终，你希望回到围棋游戏，你在[第2章](kindle_split_013.xhtml#ch02)和[第3章](kindle_split_014.xhtml#ch03)中已经看到，(一串)棋子的邻域是多么重要。
- en: In the next chapter, you’ll see how to build a particular kind of neural network
    that’s more suitable for detecting patterns in spatial data, such as images or
    Go boards. This will bring you much closer to developing a Go bot in [chapter
    7](kindle_split_019.xhtml#ch07).
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将看到如何构建一种更适合检测空间数据模式（如图像或围棋盘）的特定类型的神经网络。这将使你更接近于在[第7章](kindle_split_019.xhtml#ch07)中开发围棋机器人。
- en: 5.6\. Summary
  id: totrans-627
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6\. 概述
- en: A *sequential neural network* is a simple artificial neural network built from
    a linear stack of layers. You can apply neural networks to a wide variety of machine-learning
    problems, including image recognition.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*顺序神经网络*是由线性堆叠的层组成的简单人工神经网络。你可以将神经网络应用于广泛的机器学习问题，包括图像识别。'
- en: A *feed-forward network* is a sequential network consisting of dense layers
    with an activation function.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前馈网络*是由具有激活函数的密集层组成的顺序网络。'
- en: '*Loss functions* assess the quality of our predictions. *Mean squared error*
    is one of the most common loss functions used in practice. A loss function gives
    you a rigorous way to quantify the accuracy of your model.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数*评估我们预测的质量。*均方误差*是实践中最常用的损失函数之一。损失函数为你提供了一个严格的方式来量化模型的准确性。'
- en: '*Gradient descent* is an algorithm for minimizing a function. Gradient descent
    involves following the steepest slope of a function. In machine learning, you
    use gradient descent to find model weights that give you the smallest loss.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度下降*是一种最小化函数的算法。梯度下降涉及跟随函数的最陡斜率。在机器学习中，你使用梯度下降来找到最小的损失。'
- en: '*Stochastic gradient descent* is a variation on the gradient descent algorithm.
    In stochastic gradient descent, you compute the gradient on a small subset of
    your training set called a *mini-batch*, and then update the network weights based
    on each mini-batch. Stochastic gradient descent is normally much faster than regular
    gradient descent on large training sets.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机梯度下降*是梯度下降算法的一种变体。在随机梯度下降中，你在训练集的一个小子集上计算梯度，这个小子集被称为*批处理*，然后根据每个批处理更新网络权重。随机梯度下降通常比常规梯度下降在大训练集上快得多。'
- en: With a sequential neural network, you can use the *backpropagation algorithm*
    to calculate the gradient efficiently. The combination of backpropagation and
    mini-batches makes training fast enough to be practical on huge data sets.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用顺序神经网络，你可以使用**反向传播算法**来高效地计算梯度。反向传播和批处理的结合使得训练足够快，可以在大型数据集上实际应用。
- en: Chapter 6\. Designing a neural network for Go data
  id: totrans-634
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章\. 为围棋数据设计神经网络
- en: '*This chapter covers*'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Building a deep-learning application that can predict the next Go move from
    data
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个可以从数据预测下一个围棋走法的深度学习应用
- en: Introducing the Keras deep-learning framework
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Keras深度学习框架
- en: Understanding convolutional neural networks
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积神经网络
- en: Building neural networks to analyze spatial Go data
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建神经网络以分析空间围棋数据
- en: In the preceding chapter, you saw the fundamental principles of neural networks
    in action and implemented feed-forward networks from scratch. In this chapter,
    you’ll turn your attention back to the game of Go and tackle the problem of how
    to use deep-learning techniques to predict the next move for any given board situation
    of a Go game. In particular, you’ll generate Go game data with tree-search techniques
    developed in [chapter 4](kindle_split_016.xhtml#ch04) that you can then use to
    train a neural network. [Figure 6.1](#ch06fig01) gives an overview of the application
    you’re going to build in this chapter.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你看到了神经网络的基本原理在实际中的应用，并从头开始实现了前馈网络。在本章中，你将把注意力转回围棋游戏，并解决如何使用深度学习技术预测围棋游戏任何给定棋盘情况下的下一步问题。特别是，你将使用在第4章（kindle_split_016.xhtml#ch04）中开发的树搜索技术生成围棋游戏数据，然后你可以使用这些数据来训练一个神经网络。[图6.1](#ch06fig01)概述了本章将要构建的应用。
- en: Figure 6.1\. How to predict the next move in a game of Go by using deep learning
  id: totrans-641
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1\. 如何使用深度学习预测围棋游戏中的下一步
- en: '![](Images/06fig01_alt.jpg)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig01_alt.jpg)'
- en: 'As [figure 6.1](#ch06fig01) illustrates, to connect your working knowledge
    of neural networks from the preceding chapter, you have to address a few critical
    steps first:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.1](#ch06fig01)所示，为了将前一章中关于神经网络的工作知识联系起来，你首先需要解决几个关键步骤：
- en: In [chapter 3](kindle_split_014.xhtml#ch03), you focused on teaching a machine
    the rules of Go by implementing game play on a Go board. [Chapter 4](kindle_split_016.xhtml#ch04)
    used these structures for tree search. But in [chapter 5](kindle_split_017.xhtml#ch05),
    you saw that neural networks need *numerical input*; for the feed-forward architecture
    you implemented, *vectors* are required.
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第3章（kindle_split_014.xhtml#ch03）中，你专注于通过在围棋棋盘上实现游戏玩法来教机器围棋的规则。[第4章](kindle_split_016.xhtml#ch04)使用了这些结构进行树搜索。但在第5章（kindle_split_017.xhtml#ch05）中，你看到神经网络需要*数值输入*；对于你实现的前馈架构，需要*向量*。
- en: To transform a Go board position into an input vector to be fed into a neural
    network, you have to create an *encoder* to do the job. In [figure 6.1](#ch06fig01),
    we sketched a simple encoder that you’ll implement in [section 6.1](#ch06lev1sec1);
    the board is encoded as a matrix of board size, white stones are represented as
    –1, black stones as 1, and empty points as 0\. This matrix can be flattened to
    a vector, just as you did with MNIST data in the preceding chapter. Although this
    representation is a little too simple to provide excellent results for move prediction,
    it’s a first step in the right direction. In [chapter 7](kindle_split_019.xhtml#ch07),
    you’ll see more-sophisticated and useful ways to encode the board.
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将围棋棋盘位置转换为一个可以输入神经网络的输入向量，你必须创建一个*编码器*来完成这项工作。[图6.1](#ch06fig01)中，我们草拟了一个简单的编码器，你将在第6.1节（#ch06lev1sec1）中实现它；棋盘被编码为一个棋盘大小的矩阵，白子表示为-1，黑子表示为1，空点表示为0。这个矩阵可以被展平为一个向量，就像你在前一章中处理MNIST数据时做的那样。尽管这种表示方法对于移动预测来说过于简单，无法提供优秀的结果，但它是在正确方向上的第一步。在第7章（kindle_split_019.xhtml#ch07）中，你将看到更复杂、更有用的编码棋盘的方法。
- en: To train a neural network to predict moves, you first have to get your hands
    on data to feed into it. In [section 6.2](#ch06lev1sec2), you’ll pick up the techniques
    from [chapter 4](kindle_split_016.xhtml#ch04) to generate game records. You’ll
    encode each board position as just discussed, which will serve as your features,
    and store the next move for each position as labels.
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了训练一个神经网络来预测移动，你首先需要获取可以输入到其中的数据。在第6.2节（#ch06lev1sec2）中，你将学习第4章（kindle_split_016.xhtml#ch04）中的技术来生成游戏记录。你将按照前面讨论的方式将每个棋盘位置编码，这将成为你的特征，并将每个位置的下一步作为标签存储。
- en: Although it’s useful to have implemented a neural network as you did in [chapter
    5](kindle_split_017.xhtml#ch05), it’s now equally important to gain more speed
    and reliability by introducing a more mature deep-learning library. To this end,
    [section 6.3](#ch06lev1sec3) introduces *Keras*, a popular deep-learning library
    written in Python. You’ll use Keras to model a network for move prediction.
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然像第5章（kindle_split_017.xhtml#ch05）中那样实现了一个神经网络是有用的，但现在通过引入一个更成熟的深度学习库来获得更多速度和可靠性同样重要。为此，第6.3节（#ch06lev1sec3）介绍了*Keras*，这是一个用Python编写的流行的深度学习库。你将使用Keras来为移动预测建模网络。
- en: At this point, you might be wondering why you completely discard the spatial
    structure of the Go board by flattening the encoded board to a vector. In [section
    6.4](#ch06lev1sec4), you’ll learn about a new layer type called a *convolutional
    layer* that’s much better suited for your use case. You’ll use these layers to
    build a new architecture called a *convolutional neural network*.
  id: totrans-648
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道为什么你通过将编码的棋盘展平为向量而完全丢弃了围棋棋盘的空间结构。在[第6.4节](#ch06lev1sec4)中，你将了解一种新的层类型，称为*卷积层*，它非常适合你的使用场景。你将使用这些层来构建一个新的架构，称为*卷积神经网络*。
- en: Toward the end of the chapter, you’ll get to know more key concepts of modern
    deep learning that will further increase move-prediction accuracy, such as efficiently
    predicting probabilities with *softmax* in [section 6.5](#ch06lev1sec5) or building
    deeper neural networks in [section 6.6](#ch06lev1sec6) with an interesting activation
    function called a *rectified linear unit (ReLU)*.
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章的结尾部分，你将了解更多现代深度学习的关键概念，这些概念将进一步提高移动预测的准确性，例如在[第6.5节](#ch06lev1sec5)中高效地使用*softmax*预测概率，或者在[第6.6节](#ch06lev1sec6)中使用一个有趣的激活函数*ReLU*（修正线性单元）构建更深的神经网络。
- en: 6.1\. Encoding a Go game position for neural networks
  id: totrans-650
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 为神经网络编码围棋游戏位置
- en: 'In [chapter 3](kindle_split_014.xhtml#ch03), you built a library of Python
    classes that represented all the entities in a game of Go: `Player`, `Board`,
    `GameState`, and so on. Now you want to apply machine learning to problems in
    Go. But mathematical models like neural networks can’t operate on high-level objects
    like our `GameState` class; they can deal with only mathematical objects, such
    as vectors and matrices. In this section, you’ll create an `Encoder` class that
    translates your native game objects to a mathematical form. Throughout the rest
    of the chapter, you can feed that mathematical representation to your machine-learning
    tools.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](kindle_split_014.xhtml#ch03)中，你构建了一个Python类库，代表了围棋游戏中的所有实体：`Player`、`Board`、`GameState`等。现在你想要将机器学习应用于围棋问题。但是，像神经网络这样的数学模型不能在像我们的`GameState`类这样的高级对象上操作；它们只能处理数学对象，例如向量和矩阵。在本节中，你将创建一个`Encoder`类，将你的本地游戏对象转换为数学形式。在整个本章的其余部分，你可以将这种数学表示形式输入到你的机器学习工具中。
- en: The first step toward building a deep-learning model for Go move prediction
    is to load data that can be fed into a neural network. You do this by defining
    a simple *encoder* for the Go board, introduced in [figure 6.1](#ch06fig01). An
    encoder is a way to transform the Go board you implemented in [chapter 3](kindle_split_014.xhtml#ch03)
    in a suitable way. The neural networks you’ve learned about to this point, multilayer
    perceptrons, take vectors as inputs, but in [section 6.4](#ch06lev1sec4) you’ll
    see another network architecture that operates on higher-dimensional data. [Figure
    6.2](#ch06fig02) gives you an idea how such an encoder could be defined.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 构建围棋移动预测的深度学习模型的第一个步骤是加载可以输入到神经网络中的数据。你通过定义一个简单的*编码器*来实现这一点，该编码器在[图6.1](#ch06fig01)中介绍。编码器是将你在[第3章](kindle_split_014.xhtml#ch03)中实现的围棋棋盘以适当方式转换的一种方法。你到目前为止所学习的神经网络，多层感知器，以向量作为输入，但在[第6.4节](#ch06lev1sec4)中你将看到另一种网络架构，它操作于更高维度的数据。[图6.2](#ch06fig02)为你提供了一个关于如何定义这样一个编码器的主意。
- en: Figure 6.2\. An illustration of the `Encoder` class. It takes your `GameState`
    class and translates it into a mathematical form—a NumPy array.
  id: totrans-653
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2\. `Encoder`类的示意图。它将你的`GameState`类转换为数学形式——NumPy数组。
- en: '![](Images/06fig02.jpg)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2](Images/06fig02.jpg)'
- en: 'At its core, an encoder has to know how to encode a full Go game state. In
    particular, it should define how to encode a single point on the board. Sometimes
    the inverse is also interesting: if you’ve predicted the next move with a network,
    that move will be encoded, and you need to translate it back to an actual move
    on the board. This operation, called *decoding*, is integral to applying predicted
    moves.'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，编码器必须知道如何编码完整的围棋游戏状态。特别是，它应该定义如何编码棋盘上的一个点。有时逆操作也很有趣：如果你使用网络预测了下一步，那么这一步将被编码，你需要将其转换回棋盘上的实际移动。这个操作，称为*解码*，对于应用预测的移动至关重要。
- en: With this in mind, you can now define your `Encoder` class, an interface for
    the encoders that you’ll create in this and the next chapter. You’ll define a
    new module in dlgo called *encoders*, which you’ll initialize with an empty __init__.py,
    and put the file base.py in it. Then you’ll put the following definition in that
    file.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，你现在可以定义你的 `Encoder` 类，这是你将在本章和下一章中创建的编码器的接口。你将在 dlgo 中定义一个新的模块，名为 *encoders*，并用一个空的
    __init__.py 初始化它，然后将文件 base.py 放入其中。然后你将在该文件中放入以下定义。
- en: Listing 6.1\. Abstract `Encoder` class to encode Go game state
  id: totrans-657
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.1\. 抽象 `Encoder` 类用于编码围棋游戏状态
- en: '[PRE46]'
  id: totrans-658
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '***1* Lets you support logging or saving the name of the encoder your model
    is using**'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 允许你支持记录或保存模型使用的编码器的名称**'
- en: '***2* Turns a Go board into numeric data**'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将围棋棋盘转换为数值数据**'
- en: '***3* Turns a Go board point into an integer index**'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将围棋棋盘点转换为整数索引**'
- en: '***4* Turns an integer index back into a Go board point**'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将整数索引转换回围棋棋盘点**'
- en: '***5* Number of points on the board—board width times board height**'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 棋盘上的点数——棋盘宽度乘以棋盘高度**'
- en: '***6* Shape of the encoded board structure**'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 编码棋盘结构的形状**'
- en: 'The definition of encoders is straightforward, but we want to add one more
    convenience feature into base.py: a function to create an encoder by its name,
    a string, instead of creating an object explicitly. You do this with the `get_encoder_by_name`
    function that you append to the definition of encoders.'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的定义很简单，但我们想在 base.py 中添加一个额外的便利功能：一个通过名称（一个字符串）创建编码器的函数，而不是显式创建对象。你通过将 `get_encoder_by_name`
    函数附加到编码器的定义来实现这一点。
- en: Listing 6.2\. Referencing Go board encoders by name
  id: totrans-666
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2\. 通过名称引用围棋棋盘编码器
- en: '[PRE47]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '***1* You can create encoder instances by referencing their name.**'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你可以通过引用它们的名称来创建编码器实例。**'
- en: '***2* If board_size is one integer, you create a square board from it.**'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果 board_size 是一个整数，你将用它创建一个正方形棋盘。**'
- en: '***3* Each encoder implementation will have to provide a “create” function
    that provides an instance.**'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 每个编码器实现都必须提供一个“创建”函数，该函数提供一个实例。**'
- en: 'Now that you know what an encoder is and how to build one, let’s implement
    the idea from [figure 6.2](#ch06fig02) as your first encoder: one color is represented
    as 1, the other as –1, and empty points as 0\. To make accurate predictions, the
    model also needs to know whose turn it is. So instead of using 1 for black and
    –1 for white, you’ll use 1 for whoever has the next turn, and –1 for the opponent.
    You’ll call this `OnePlaneEncoder`, because you encode the Go board into a single
    matrix or plane of the same size as the board. In [chapter 7](kindle_split_019.xhtml#ch07),
    you’ll see encoders with more *feature planes*; for instance, you’ll implement
    an encoder that has one plane each for black and white stones, and one plane to
    capture ko. Right now, you’ll stick with our simple one-plane encoding idea that
    you implement in oneplane.py in the encoders module. The following listing shows
    the first part.'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了编码器是什么以及如何构建一个，让我们将 [图 6.2](#ch06fig02) 中的想法实现为你的第一个编码器：一种颜色表示为 1，另一种颜色表示为
    –1，空点表示为 0。为了做出准确的预测，模型还需要知道是谁的回合。所以，你不会用 1 表示黑子，用 –1 表示白子，而是用 1 表示下一个回合的人，用 –1
    表示对手。你会称这个为 `OnePlaneEncoder`，因为你会将围棋棋盘编码成一个与棋盘大小相同的单矩阵或平面。在 [第 7 章](kindle_split_019.xhtml#ch07)
    中，你会看到具有更多 *特征平面* 的编码器；例如，你会实现一个具有黑白棋子各一个平面和一个平面来捕捉“活”的编码器。现在，你将坚持使用我们在 encoders
    模块中的 oneplane.py 中实现的简单单平面编码想法。以下列表显示了第一部分。
- en: Listing 6.3\. Encoding game state with a simple one-plane Go board encoder
  id: totrans-672
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3\. 使用简单的单平面围棋棋盘编码器编码游戏状态
- en: '[PRE48]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '***1* You can reference this encoder by the name oneplane.**'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你可以通过名称 oneplane 来引用这个编码器。**'
- en: '***2* To encode, you fill a matrix with 1 if the point contains one of the
    current player’s stones, –1 if the point contains the opponent’s stones, and 0
    if the point is empty.**'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 编码时，你用一个矩阵填充 1，如果该点包含当前玩家的棋子，用 –1 如果该点包含对手的棋子，如果该点是空的，则用 0。**'
- en: In the second part of the definition, you’ll take care of encoding and decoding
    single points of the board. The encoding is done by mapping a point on the board
    to a vector that has a length of board width times board height; the decoding
    recovers point coordinates from such a vector.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义的第二部分中，你将处理编码和解码棋盘的单个点。编码是通过将棋盘上的一个点映射到一个长度为棋盘宽度乘以棋盘高度的向量来完成的；解码从这样的向量中恢复点坐标。
- en: Listing 6.4\. Encoding and decoding points with your one-plane Go board encoder
  id: totrans-677
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.4\. 使用您的单平面围棋棋盘编码器进行编码和解码点
- en: '[PRE49]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '***1* Turns a board point into an integer index**'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将棋盘点转换为整数索引**'
- en: '***2* Turns an integer index into a board point**'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将整数索引转换为棋盘点**'
- en: This concludes our section on Go board encoders. You can now move on to create
    data that you can encode and feed into a neural network.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对围棋棋盘编码器的讨论。你现在可以继续创建可以编码并输入到神经网络中的数据。
- en: 6.2\. Generating tree-search games as network training data
  id: totrans-682
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 生成树搜索游戏作为网络训练数据
- en: Before you can apply machine learning to Go games, you need a set of training
    data. Fortunately, strong players are playing on public Go servers all the time.
    [Chapter 7](kindle_split_019.xhtml#ch07) covers how to find and process those
    game records to create training data. For now, you can generate your own game
    records. This section shows how to use the tree-search bots you created in [chapter
    4](kindle_split_016.xhtml#ch04) to generate game records. In the rest of the chapter,
    you can use those bot game records as training data to experiment with deep learning.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能够将机器学习应用于围棋游戏之前，你需要一组训练数据。幸运的是，强大的玩家一直在公共围棋服务器上玩游戏。[第7章](kindle_split_019.xhtml#ch07)介绍了如何找到和处理这些游戏记录以创建训练数据。目前，你可以生成自己的游戏记录。本节展示了如何使用你在[第4章](kindle_split_016.xhtml#ch04)中创建的树搜索机器人生成游戏记录。在本章的其余部分，你可以使用这些机器人游戏记录作为训练数据来尝试深度学习。
- en: Does it seem silly to use machine learning to imitate a classical algorithm?
    Not if the traditional algorithm is slow! Here you hope to use machine learning
    to get a fast approximation to a slow tree search. This concept is a key part
    of AlphaGo Zero, the strongest version of AlphaGo. [Chapter 14](kindle_split_027.xhtml#ch14)
    covers how AlphaGo Zero works.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习来模仿经典算法看起来很愚蠢吗？不是如果传统算法很慢的话！在这里，你希望使用机器学习来获得对慢速树搜索的快速近似。这个概念是AlphaGo Zero（AlphaGo的最强版本）的关键部分。[第14章](kindle_split_027.xhtml#ch14)介绍了AlphaGo
    Zero的工作原理。
- en: Go ahead and create a file called generate_mcts_games.py outside the dlgo module.
    As the filename suggests, you’ll write code that generates games with MCTS. Each
    move in each of these games will then be encoded with your `OnePlaneEncoder` from
    [section 6.1](#ch06lev1sec1) and stored in `numpy` arrays for future use. To begin
    with, put the following `import` statements at the top of generate_mcts_games.py.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为generate_mcts_games.py的文件，位于dlgo模块外部。正如文件名所暗示的，你将编写生成带有MCTS的游戏的代码。然后，这些游戏中的每一步都将使用你的`OnePlaneEncoder`（来自[第6.1节](#ch06lev1sec1)）进行编码，并存储在`numpy`数组中以供将来使用。首先，在generate_mcts_games.py的顶部放置以下`import`语句。
- en: Listing 6.5\. Imports for generating encoded Monte Carlo tree-search game data
  id: totrans-686
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5\. 生成编码蒙特卡洛树搜索游戏数据的导入
- en: '[PRE50]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'From these imports, you can already see which tools you’ll use for the job:
    the mcts module, your goboard implementation from [chapter 3](kindle_split_014.xhtml#ch03),
    and the encoders module you just defined. Let’s move on to creating the function
    that’ll generate the game data for you. In `generate_game`, you let an instance
    of an `MCTSAgent` from [chapter 4](kindle_split_016.xhtml#ch04) play games against
    itself (recall from [chapter 4](kindle_split_016.xhtml#ch04) that the *temperature*
    of an MCTS agent regulates the volatility of your tree search). For each move,
    you encode the board state before the move has been played, encode the move as
    a one-hot vector, and then apply the move to the board.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些导入中，你可以看到你将使用哪些工具来完成这项工作：mcts模块、你在[第3章](kindle_split_014.xhtml#ch03)中实现的goboard，以及你刚刚定义的encoders模块。让我们继续创建一个函数，该函数将为你生成游戏数据。在`generate_game`中，你让一个来自[第4章](kindle_split_016.xhtml#ch04)的`MCTSAgent`实例与自己玩游戏（回想一下[第4章](kindle_split_016.xhtml#ch04)中，MCTS代理的温度调节了你的树搜索的波动性）。对于每一步，你在移动之前编码棋盘状态，将移动编码为一个独热向量，然后将移动应用到棋盘上。
- en: Listing 6.6\. Generating MCTS games for this chapter
  id: totrans-689
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6\. 为本章生成MCTS游戏
- en: '[PRE51]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '***1* In boards you store encoded board state; moves is for encoded moves.**'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在棋盘上存储编码的棋盘状态；动作列表用于编码动作。**'
- en: '***2* Initialize a OnePlaneEncoder by name with given board size.**'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用名称和给定的棋盘大小初始化OnePlaneEncoder。**'
- en: '***3* A new game of size board_size is instantiated.**'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 实例化一个大小为board_size的新游戏。**'
- en: '***4* A Monte Carlo tree-search agent with specified number of rounds and temperature
    will serve as your bot.**'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 具有指定轮数和温度的蒙特卡洛树搜索代理将作为你的机器人。**'
- en: '***5* The next move is selected by the bot.**'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 机器人选择下一步。**'
- en: '***6* The encoded board situation is appended to boards.**'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将编码的棋盘情况附加到棋盘列表中。**'
- en: '***7* The one-hot-encoded next move is appended to moves.**'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 将独热编码的下一步动作附加到动作列表中。**'
- en: '***8* Afterward, the bot move is applied to the board.**'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 之后，将机器人移动应用到棋盘上。**'
- en: '***9* You continue with the next move, unless the maximum number of moves has
    been reached.**'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 您将继续进行下一步，除非达到最大步数。**'
- en: Now that you have the means to create and encode game data with Monte Carlo
    tree search, you can define a `main` method to run a few games and persist them
    afterward, which you can also put into generate_mcts_games.py.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了使用蒙特卡洛树搜索创建和编码游戏数据的方法，您可以定义一个`main`方法来运行几场比赛，并在之后持久化它们，您也可以将它们放入generate_mcts_games.py中。
- en: Listing 6.7\. Main application for generating MCTS games for this chapter
  id: totrans-701
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7\. 为本章生成MCTS游戏的主体应用
- en: '[PRE52]'
  id: totrans-702
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '***1* This application allows customization via command-line arguments.**'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 此应用程序允许通过命令行参数进行自定义。**'
- en: '***2* For the specified number of games, you generate game data.**'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对于指定的游戏数量，您将生成游戏数据。**'
- en: '***3* After all games have been generated, you concatenate features and labels,
    respectively.**'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在所有游戏生成完毕后，您分别连接特征和标签。**'
- en: '***4* You store feature and label data to separate files, as specified by the
    command-line options.**'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 您将特征和标签数据存储到由命令行选项指定的单独文件中。**'
- en: 'Using this tool, you can now generate game data easily. Let’s say you want
    to create data for twenty 9 × 9 Go games and store features in features.npy, and
    labels in labels.npy. The following command will do it:'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个工具，您现在可以轻松地生成游戏数据。假设您想为20场9×9围棋游戏创建数据，并将特征存储在features.npy中，将标签存储在labels.npy中。以下命令将完成这项工作：
- en: '[PRE53]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note that generating games like this can be fairly slow, so generating a lot
    of games will take a while. You could always decrease the number of rounds for
    MCTS, but this also decreases the bot’s level of play. Therefore, we generated
    game data for you already that you can find in the GitHub repo under generated_games.
    You can find the output in features-40k.npy and labels-40k.npy; it contains about
    40,000 moves over several hundred games. We generated these with 5,000 MCTS rounds
    per move. At that setting, the MCTS engine mostly plays sensible moves, so we
    can reasonably hope that a neural network can learn to imitate it.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，生成此类游戏可能相当慢，因此生成大量游戏需要一段时间。您始终可以减少MCTS的回合数，但这也会降低机器人的游戏水平。因此，我们已经为您生成了游戏数据，您可以在GitHub仓库中的generated_games文件夹下找到。您可以在features-40k.npy和labels-40k.npy中找到输出；它包含了几百场比赛中的大约40,000步。我们为每一步生成了5,000个MCTS回合。在这个设置下，MCTS引擎主要玩合理的棋步，所以我们有理由希望神经网络可以学会模仿它。
- en: At this point, you’ve done all the preprocessing you need in order to apply
    a neural network to your generated data. You could do this with your network implementation
    from [chapter 5](kindle_split_017.xhtml#ch05) in a straightforward manner—and
    it’s a good exercise to do so—but going forward, you need a more powerful tool
    to satisfy your needs to work with increasingly complex deep neural networks.
    To this end, we introduce Keras next.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经完成了将神经网络应用于生成数据所需的所有预处理工作。您可以用第5章中的网络实现以直接的方式完成这项工作——这样做也是一个很好的练习——但是向前看，您需要一个更强大的工具来满足您处理越来越复杂的深度神经网络的需求。为此，我们接下来介绍Keras。
- en: 6.3\. Using the Keras deep-learning library
  id: totrans-711
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 使用Keras深度学习库
- en: Computing gradients and the backward pass of a neural network is becoming more
    and more of a lost art form because of the emergence of many powerful deep-learning
    libraries that hide lower-level abstractions. It’s good to have implemented neural
    networks from scratch in the previous chapter, but now it’s time to move on to
    more mature and feature-rich software.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多强大的深度学习库的出现，神经网络梯度和反向传播的计算变得越来越像一门失传的艺术。在前一章中从头开始实现神经网络是件好事，但现在我们该转向更成熟且功能丰富的软件了。
- en: The Keras deep-learning library is a particularly elegant and popular deep-learning
    tool written in Python. The open source project was created in 2015 and quickly
    accumulated a strong user base. The code is hosted at [https://github.com/keras-team/keras](https://github.com/keras-team/keras)
    and has excellent documentation that can be found at [https://keras.io](https://keras.io).
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: Keras深度学习库是一个特别优雅且流行的Python编写的深度学习工具。这个开源项目创建于2015年，并迅速积累了强大的用户基础。代码托管在[https://github.com/keras-team/keras](https://github.com/keras-team/keras)，并且有优秀的文档，可以在[https://keras.io](https://keras.io)找到。
- en: 6.3.1\. Understanding Keras design principles
  id: totrans-714
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1\. 理解Keras设计原则
- en: One of the strong suits of Keras is that it’s an intuitive and easy-to-pick-up
    API that allows for quick prototyping and a fast experimentation cycle. This makes
    Keras a popular pick in many data science challenges, such as on [https://kaggle.com](https://kaggle.com).
    Keras is built from modular building blocks and was originally inspired by other
    deep-learning tools such as Torch. Another big plus for Keras is its extensibility.
    Adding new custom layers or augmenting existing functionality is relatively straightforward.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的一个优点是它具有直观且易于学习的 API，允许快速原型设计和快速实验周期。这使得 Keras 在许多数据科学挑战中很受欢迎，例如在 [https://kaggle.com](https://kaggle.com)
    上。Keras 由模块化构建块组成，最初受到其他深度学习工具（如 Torch）的启发。Keras 的另一个优点是其可扩展性。添加新的自定义层或增强现有功能相对简单。
- en: Another aspect that makes Keras easy to get started with is that it comes with
    batteries included. For instance, many popular data sets, like MNIST, can be loaded
    directly with Keras, and you can find a lot of good examples in the GitHub repository.
    On top of that, there’s a whole community-built ecosystem of Keras extensions
    and independent projects at [https://github.com/fchollet/keras-resources](https://github.com/fchollet/keras-resources).
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 易于入门的另一个方面是它自带了所有必要的组件。例如，许多流行的数据集，如 MNIST，可以直接使用 Keras 加载，您可以在 GitHub
    仓库中找到很多好的示例。除此之外，还有一个由社区构建的 Keras 扩展和独立项目的生态系统，位于 [https://github.com/fchollet/keras-resources](https://github.com/fchollet/keras-resources)。
- en: 'A distinctive feature of Keras is the concept of *backends*: it runs with powerful
    engines that can be swapped on demand. One way to think of Keras is as a deep-learning
    *frontend*, a library that provides a convenient set of high-level abstractions
    and functionality to run your models, but is backed by a choice of backend that
    does the heavy lifting in the background. As of the writing of this book, three
    official backends are available for Keras: TensorFlow, Theano, and the Microsoft
    Cognitive Toolkit. In this book, you’ll work with Google’s TensorFlow library
    exclusively, which is also the default backend used by Keras. But if you prefer
    another backend, you shouldn’t need much effort to switch; Keras handles most
    of the differences for you.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的一个显著特点是 *后端* 的概念：它使用强大的引擎，可以根据需要更换。一种思考 Keras 的方式是将其视为深度学习的 *前端*，这是一个提供方便的、高级抽象和功能的库，以运行您的模型，但背后有一个选择的后端，在后台完成繁重的工作。截至本书编写时，Keras
    有三个官方后端可供选择：TensorFlow、Theano 和微软认知工具包。在本书中，您将专门使用 Google 的 TensorFlow 库，这也是 Keras
    的默认后端。但如果您更喜欢另一个后端，您不需要花费太多精力就可以切换；Keras 会为您处理大部分差异。
- en: In this section, you’ll first install Keras. Then you’ll learn about its API
    by running the handwritten digit classification example from [chapter 5](kindle_split_017.xhtml#ch05)
    with it, and then move on to the task of Go move prediction.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您首先将安装 Keras。然后您将通过运行来自 [第 5 章](kindle_split_017.xhtml#ch05) 的手写数字分类示例来了解其
    API，然后继续进行围棋移动预测的任务。
- en: 6.3.2\. Installing the Keras deep-learning library
  id: totrans-719
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 安装 Keras 深度学习库
- en: 'To get started with Keras, you need to install a backend first. You can start
    with TensorFlow, which is easiest installed through pip by running the following:'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Keras，您需要首先安装一个后端。您可以从 TensorFlow 开始，它可以通过运行以下命令通过 pip 最容易地安装：
- en: '[PRE54]'
  id: totrans-721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'If your machine has an NVIDIA GPU and current CUDA drivers installed, you can
    try installing the GPU-accelerated version of TensorFlow instead:'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的机器安装了 NVIDIA GPU 和当前的 CUDA 驱动程序，您可以尝试安装 TensorFlow 的 GPU 加速版本：
- en: '[PRE55]'
  id: totrans-723
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: If `tensorflow-gpu` is compatible with your hardware and drivers, that’ll give
    you a huge speed improvement.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `tensorflow-gpu` 与您的硬件和驱动程序兼容，这将为您带来巨大的速度提升。
- en: 'A few optional dependencies that are helpful for model serialization and visualization
    can be installed for Keras, but you’ll skip them for now and directly proceed
    to installing the library itself:'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有助于模型序列化和可视化的可选依赖项可以用于 Keras，但您现在可以跳过它们，直接安装库本身：
- en: '[PRE56]'
  id: totrans-726
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 6.3.3\. Running a familiar first example with Keras
  id: totrans-727
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3\. 使用 Keras 运行熟悉的第一示例
- en: 'In this section, you’ll see that defining and running Keras models follows
    a four-step workflow:'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将看到定义和运行 Keras 模型遵循以下四个步骤的工作流程：
- en: '***Data preprocessing*—** Load and prepare a data set to be fed into a neural
    network.'
  id: totrans-729
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***数据预处理*—** 加载并准备一个数据集以供神经网络输入。'
- en: '***Model definition*—** Instantiate a model and add layers to it as needed.'
  id: totrans-730
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***模型定义*—** 实例化一个模型并根据需要添加层。'
- en: '***Model compilation*—** Compile your previously defined model with an optimizer,
    a loss function, and an optional list of evaluation metrics.'
  id: totrans-731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***模型编译*—** 使用优化器、损失函数和可选的评估指标列表编译你之前定义的模型。'
- en: '***Model training and evaluation*—** Fit your deep-learning model to data and
    evaluate it.'
  id: totrans-732
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***模型训练和评估*—** 将你的深度学习模型拟合到数据并对其进行评估。'
- en: 'To get started with Keras, we walk you through an example use case that you
    encountered in the preceding chapter: predicting handwritten digits with the MNIST
    data set. As you’ll see, our simple model from [chapter 5](kindle_split_017.xhtml#ch05)
    is remarkably close to the Keras syntax already, so using Keras should come even
    easier.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Keras，我们将通过一个例子来引导你，这个例子是你之前章节中遇到的使用案例：使用 MNIST 数据集预测手写数字。正如你将看到的，我们第
    5 章（kindle_split_017.xhtml#ch05）中的简单模型与 Keras 语法已经非常接近，所以使用 Keras 应该会更加容易。
- en: 'With Keras, you can define two types of models: sequential and more general
    nonsequential models. You’ll use only sequential models here. Both model types
    can be found in keras.models. To define a sequential model, you have to add layers
    to it, just as you did in [chapter 5](kindle_split_017.xhtml#ch05) in your own
    implementation. Keras layers are available through the keras.layers module. Loading
    MNIST with Keras is simple; the data set can be found in the keras.datasets module.
    Let’s import everything you need to tackle this application first.'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras，你可以定义两种类型的模型：顺序模型和更通用的非顺序模型。在这里，你将只使用顺序模型。这两种模型类型都可以在 keras.models
    中找到。要定义一个顺序模型，你必须向其中添加层，就像你在第 5 章（kindle_split_017.xhtml#ch05）中的实现中做的那样。Keras
    层可以通过 keras.layers 模块获得。使用 Keras 加载 MNIST 数据非常简单；数据集可以在 keras.datasets 模块中找到。让我们首先导入处理这个应用所需的所有内容。
- en: Listing 6.8\. Importing models, layers, and data sets from Keras
  id: totrans-735
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.8\. 从 Keras 导入模型、层和数据集
- en: '[PRE57]'
  id: totrans-736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Next, you load and preprocess MNIST data, which is achieved in just a few lines.
    After loading, you flatten the 60,000 training samples and 10,000 test samples,
    convert them to the `float` type, and then normalize input data by dividing by
    255\. This is done because the pixel values of the data set vary from 0 to 255,
    and you normalize these values to a range of [0, 1], as this will lead to better
    training of your network. Also, the labels have to be one-hot encoded, just as
    you did in [chapter 5](kindle_split_017.xhtml#ch05). The following listing shows
    how to do what we just described with Keras.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你加载并预处理 MNIST 数据，这只需几行代码即可完成。加载后，你将 60,000 个训练样本和 10,000 个测试样本展平，将它们转换为
    `float` 类型，然后通过除以 255 来归一化输入数据。这样做是因为数据集的像素值范围从 0 到 255，你需要将这些值归一化到 [0, 1] 的范围内，因为这将有助于你的网络更好地训练。此外，标签也需要进行
    one-hot 编码，就像你在第 5 章（kindle_split_017.xhtml#ch05）中做的那样。下面的列表展示了如何使用 Keras 实现我们刚才描述的内容。
- en: Listing 6.9\. Loading and preprocessing MNIST data with Keras
  id: totrans-738
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.9\. 使用 Keras 加载和预处理 MNIST 数据
- en: '[PRE58]'
  id: totrans-739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: With data ready to go, you can now proceed to define a neural network to run.
    In Keras, you initialize a `Sequential` model and then add layers one by one.
    In the first layer, you have to provide the input data *shape*, provided through
    `input_shape`. In our case, input data is a vector of length 784, so you have
    to provide `input_shape=(784,)` as shape information. `Dense` layers in Keras
    can be created with an `activation` keyword to provide the layer with an activation
    function. You’ll choose `sigmoid`, because it’s the only activation function you
    know so far. Keras has many more activation functions, some of which we’ll discuss
    in more detail.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪后，你现在可以继续定义一个神经网络来运行。在 Keras 中，你初始化一个 `Sequential` 模型，然后逐个添加层。在第一层中，你必须提供通过
    `input_shape` 提供的输入数据 *形状*。在我们的例子中，输入数据是一个长度为 784 的向量，所以你必须提供 `input_shape=(784,)`
    作为形状信息。Keras 中的 `Dense` 层可以通过 `activation` 关键字创建，以向层提供激活函数。你会选择 `sigmoid`，因为这是你迄今为止知道的唯一激活函数。Keras
    有许多其他的激活函数，其中一些我们将在更详细的讨论中介绍。
- en: Listing 6.10\. Building a simple sequential model with Keras
  id: totrans-741
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.10\. 使用 Keras 构建一个简单的顺序模型
- en: '[PRE59]'
  id: totrans-742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The next step in creating a Keras model is to *compile* the model with a loss
    function and an optimizer. You can do this by specifying strings, and you’ll choose
    `sgd` (stochastic gradient descent) as the optimizer and `mean_squared_error`
    as the loss function. Again, Keras has many more losses and optimizers, but to
    get started, you’ll use the ones you already encountered in [chapter 5](kindle_split_017.xhtml#ch05).
    Another argument that you can feed into the compilation step of Keras models is
    a list of evaluation metrics. For your first application, you’ll use `accuracy`
    as the only metric. The `accuracy` metric indicates how often the model’s highest-scoring
    prediction matches the true label.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Keras模型的下一步是使用损失函数和优化器来*编译*模型。你可以通过指定字符串来完成此操作，你将选择`sgd`（随机梯度下降）作为优化器，并将`mean_squared_error`作为损失函数。同样，Keras有更多损失函数和优化器，但为了入门，你将使用你在[第5章](kindle_split_017.xhtml#ch05)中已经遇到的那些。你还可以将一个评估指标列表作为参数传递给Keras模型的编译步骤。对于你的第一个应用，你将使用`accuracy`作为唯一的指标。`accuracy`指标表示模型最高得分的预测与真实标签匹配的频率。
- en: Listing 6.11\. Compiling a Keras deep-learning model
  id: totrans-744
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.11\. 编译Keras深度学习模型
- en: '[PRE60]'
  id: totrans-745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The final step for this application is to carry out the training step of the
    network and then evaluate it on test data. This is done by calling `fit` on your
    `model` by providing not only training data, but also the mini-batch size to work
    with and the number of epochs to run.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个应用，最后一步是执行网络的训练步骤，然后在测试数据上评估它。这是通过在`model`上调用`fit`来完成的，不仅要提供训练数据，还要提供要处理的迷你批大小和要运行的epoch数量。
- en: Listing 6.12\. Training and evaluating a Keras model
  id: totrans-747
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.12\. 训练和评估Keras模型
- en: '[PRE61]'
  id: totrans-748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'To recap, building and running a Keras model proceeds in four steps: data preprocessing,
    model definition, model compilation, and model training plus evaluation. One of
    the core strengths of Keras is that this four-step cycle can be done quickly,
    which leads to a fast experimentation cycle. This is of great importance, because
    often your initial model definition can be improved a lot by tweaking parameters.'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，构建和运行Keras模型分为四个步骤：数据预处理、模型定义、模型编译以及模型训练和评估。Keras的一个核心优势是这四个步骤可以快速完成，这导致了一个快速的实验周期。这非常重要，因为通常你的初始模型定义可以通过调整参数得到很大的改进。
- en: 6.3.4\. Go move prediction with feed-forward neural networks in Keras
  id: totrans-750
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4\. 在Keras中使用前馈神经网络进行走棋预测
- en: Now that you know what the Keras API for sequential neural networks looks like,
    let’s turn back to our Go move-prediction use case. [Figure 6.3](#ch06fig03) illustrates
    this step of the process. You’ll first load the generated Go data from [section
    6.2](#ch06lev1sec2), as shown in [listing 6.13](#ch06ex13). Note that, as with
    MNIST before, you need to flatten Go board data into vectors.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了Keras序列神经网络的API看起来是什么样子，让我们回到我们的Go走棋预测用例。[图6.3](#ch06fig03)说明了这个过程的一个步骤。你首先从[第6.2节](#ch06lev1sec2)加载生成的Go数据，如[列表6.13](#ch06ex13)所示。请注意，与MNIST之前一样，你需要将Go棋盘数据展平成向量。
- en: Figure 6.3\. A neural network can predict game moves. Having already encoded
    the game state as a matrix, you can feed that matrix to the move-prediction model.
    The model outputs a vector representing the probability of each possible move.
  id: totrans-752
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3\. 神经网络可以预测游戏走法。已经将游戏状态编码为矩阵后，你可以将这个矩阵输入到走棋预测模型中。模型输出一个向量，表示每个可能走法的概率。
- en: '![](Images/06fig03_alt.jpg)'
  id: totrans-753
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig03_alt.jpg)'
- en: Listing 6.13\. Loading and preprocessing previously stored Go game data
  id: totrans-754
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.13\. 加载和预处理之前存储的Go棋局数据
- en: '[PRE62]'
  id: totrans-755
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '***1* By setting a random seed, you make sure this script is exactly reproducible.**'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 通过设置随机种子，你可以确保这个脚本是完全可复制的。**'
- en: '***2* Load the sample data into NumPy arrays.**'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将样本数据加载到NumPy数组中。**'
- en: '***3* Transform the input into vectors of size 81, instead of 9 × 9 matrices.**'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将输入转换为81维的向量，而不是9×9的矩阵。**'
- en: '***4* Hold back 10% of the data for a test set; train on the other 90%.**'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 保留10%的数据作为测试集；其余90%用于训练。**'
- en: Next, let’s define and run a model to predict Go moves for the features *X*
    and labels *Y* you just defined. For a 9 × 9 board, there are 81 possible moves,
    so you need to predict 81 classes with your network. As a baseline, pretend you
    just closed your eyes and pointed at a spot on the board at random. There’s a
    1 in 81 chance you’d find the next play by pure luck, or 1.2%. So you’d like to
    see your model significantly exceed 1.2% accuracy.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义并运行一个模型来预测你刚刚定义的特征*X*和标签*Y*的围棋移动。对于一个9×9的棋盘，有81种可能的移动，所以你需要用你的网络预测81个类别。作为一个基线，假设你只是闭上眼睛，随机在棋盘上指一个点。你有1/81的机会通过纯粹的运气找到下一个移动，或者1.2%。所以你希望看到你的模型显著超过1.2%的准确率。
- en: You define a simple Keras MLP with three `Dense` layers, each with `sigmoid`
    activation functions, that you compile with mean squared error loss and a stochastic
    gradient descent optimizer. You then let this network train for 15 epochs and
    evaluate it on test data.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 你定义了一个简单的Keras MLP，包含三个`Dense`层，每个层都有`sigmoid`激活函数，你用均方误差损失和随机梯度下降优化器来编译它。然后你让这个网络训练15个周期，并在测试数据上评估它。
- en: Listing 6.14\. Running a Keras multilayer perceptron on generated Go data
  id: totrans-762
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.14\. 在生成的围棋数据上运行Keras多层感知器
- en: '[PRE63]'
  id: totrans-763
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Running this code, you should see the model summary and evaluation metrics
    printed to the console:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码，你应该会在控制台看到模型摘要和评估指标打印出来：
- en: '[PRE64]'
  id: totrans-765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Note the line `Trainable params: 623,081` in the output; this means the training
    process is updating the value of over 600,000 individual weights. This is a rough
    indicator of the computational intensity of the model. It also gives you a rough
    sense of the *capacity* of your model: its ability to learn complex relationships.
    As you compare different network architectures, the total number of parameters
    provides a way to approximately compare the total size of the models.'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '注意输出中的行`Trainable params: 623,081`；这意味着训练过程正在更新超过600,000个单独权重的值。这是模型计算强度的粗略指标。它也给你一个关于模型*容量*的粗略感觉：其学习复杂关系的能力。当你比较不同的网络架构时，参数的总数提供了一个方法来大致比较模型的总大小。'
- en: As you can see, the prediction accuracy of your experiment is at only around
    2.3%, which isn’t satisfying at first sight. But recall that your baseline of
    randomly guessing moves is about 1.2%. This tells you that although the performance
    isn’t great, the model is learning and can predict moves better than random.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，你实验的预测准确率只有大约2.3%，乍一看并不令人满意。但回想一下，你随机猜测移动的基线大约是1.2%。这告诉你，尽管性能并不出色，但模型正在学习，并且能够比随机猜测更好地预测移动。
- en: You can get some insight into the model by feeding it sample board positions.
    [Figure 6.4](#ch06fig04) shows a board that we contrived to make the right play
    obvious. Whoever plays next can capture two opponent stones by playing at either
    A or B. This position doesn’t appear in our training set.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过向模型提供样本棋盘位置来了解模型的一些信息。[图6.4](#ch06fig04)展示了我们设计的一个棋盘，使得正确的移动显而易见。下一个回合的玩家可以在A或B处移动，从而捕获两个对手的棋子。这个位置并不出现在我们的训练集中。
- en: Figure 6.4\. An example game position for testing our model. In this position,
    black can capture two stones by playing at A, or white can capture two stones
    by playing at B. Whoever plays first in that area has a huge advantage in the
    game.
  id: totrans-769
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4\. 测试我们模型的示例游戏位置。在这个位置，黑方可以通过在A处移动来捕获两个棋子，或者白方可以通过在B处移动来捕获两个棋子。在那个区域首先移动的一方在游戏中拥有巨大的优势。
- en: '![](Images/06fig04.jpg)'
  id: totrans-770
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig04.jpg)'
- en: Now you can feed that board position into the trained model and print out its
    predictions.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将这个棋盘位置输入到训练好的模型中，并打印出它的预测结果。
- en: Listing 6.15\. Evaluating the model on a known board position
  id: totrans-772
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.15\. 在已知棋盘位置上评估模型
- en: '[PRE65]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output looks something like this:'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像这样：
- en: '[PRE66]'
  id: totrans-775
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This matrix maps to the original 9 × 9 board: each number represents the model’s
    confidence that it should play on that point next. This result isn’t too impressive;
    it hasn’t even learned not to play on a spot where there’s already a stone. But
    notice that the scores for the edge of the board are consistently lower than scores
    closer to the center. The conventional wisdom in Go is that you should avoid playing
    on the very edge of the board, except at the end of the game and other special
    situations. So the model has learned a legitimate concept about the game: not
    by understanding strategy or efficiency, but just by copying what our MCTS bot
    does. This model isn’t likely to predict many great moves, but it has learned
    to avoid a whole class of poor moves.'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵映射到原始的9×9棋盘：每个数字代表模型在下一个点下棋的信心。这个结果并不太令人印象深刻；它甚至还没有学会不在已经有棋子的地方下棋。但请注意，棋盘边缘的分数始终低于靠近中心的分数。围棋的常规智慧是，你应该避免在棋盘的边缘下棋，除非是在游戏结束时或其他特殊情况下。所以模型已经学习了一个关于游戏的合法概念：不是通过理解策略或效率，而是仅仅通过复制我们的MCTS机器人所做的。这个模型不太可能预测出很多出色的走法，但它已经学会了避免一类糟糕的走法。
- en: 'This is real progress, but you can do better. The rest of this chapter addresses
    shortcomings of your first experiment and improves Go move-prediction accuracy
    along the way. You’ll take care of the following points:'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个进步，但你还能做得更好。本章的其余部分将解决你第一次实验的不足，并在提高围棋走法预测准确性的过程中进行改进。你需要注意以下要点：
- en: The data you’re using for this prediction task has been *generated by using
    tree search*, which has a strong element of randomness. Sometimes MCTS engines
    generate strange moves, especially when they’re either far ahead or far behind
    in the game. In [chapter 7](kindle_split_019.xhtml#ch07), you’ll create a deep-learning
    model from human game play. Of course, humans are also unpredictable, but they’re
    less likely to play nonsense moves.
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你用于这个预测任务的数据是通过使用树搜索生成的，它具有强烈的随机性元素。有时MCTS引擎会生成奇怪的走法，尤其是在游戏领先或落后很多的情况下。在第7章（[chapter
    7](kindle_split_019.xhtml#ch07)）中，你将创建一个从人类游戏玩法中学习深度学习模型。当然，人类也是不可预测的，但他们不太可能玩出无意义的走法。
- en: The neural network architecture you used can be vastly improved. Multilayer
    perceptrons aren’t well suited to capture Go board data. You have to flatten the
    two-dimensional board data to a flat vector, thereby losing all spatial information
    about the board. In [section 6.4](#ch06lev1sec4), you’ll learn about a new type
    of network that’s much better at capturing the Go board structure.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你使用的神经网络架构可以大幅改进。多层感知器不适合捕捉围棋盘数据。你必须将二维的棋盘数据展平成一个扁平向量，从而丢失了关于棋盘的所有空间信息。在第6.4节（[section
    6.4](#ch06lev1sec4)）中，你将了解一种新的网络类型，它非常适合捕捉围棋盘结构。
- en: Throughout all networks so far, you used only the `sigmoid` activation function.
    In [sections 6.5](#ch06lev1sec5) and [6.6](#ch06lev1sec6), you’ll learn about
    two new activation functions that often lead to better results.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在迄今为止的所有网络中，你只使用了`sigmoid`激活函数。在第6.5节（[sections 6.5](#ch06lev1sec5)）和6.6节（[6.6](#ch06lev1sec6)）中，你将了解两种新的激活函数，它们通常会导致更好的结果。
- en: Up to this point, you’ve used MSE only as a loss function, which is intuitive,
    but not well suited for your use case. In [section 6.5](#ch06lev1sec5), you’ll
    use a loss function that’s tailored to classification tasks like ours.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，你只将MSE用作损失函数，这是直观的，但并不适合你的用例。在第6.5节（[section 6.5](#ch06lev1sec5)）中，你将使用一个针对我们这种分类任务定制的损失函数。
- en: Having addressed most of these points, at the end of this chapter you’ll be
    able to build a neural network that can predict moves better than your first shot.
    You’ll learn key techniques to build a significantly stronger bot in [chapter
    7](kindle_split_019.xhtml#ch07).
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决这些问题的大部分之后，在本章结束时，你将能够构建一个比你的第一次尝试预测更好的神经网络。你将在第7章（[chapter 7](kindle_split_019.xhtml#ch07)）中学习构建一个显著更强的机器人的关键技巧。
- en: Keep in mind that, ultimately, you’re not interested in predicting moves as
    accurately as possible, but in creating a bot that can play as well as possible.
    Even if your deep neural networks may never become extraordinarily good at predicting
    the next move from historical data, the power of deep learning is that they’ll
    still implicitly pick up the *structure of the game* and play reasonable or even
    very good moves.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，最终，你感兴趣的并不是尽可能准确地预测走法，而是创建一个尽可能会玩的机器人。即使你的深度神经网络可能永远不会从历史数据中预测出非常出色的下一步，深度学习的力量在于它们仍然会隐式地学习游戏的*结构*，并玩出合理甚至非常好的走法。
- en: 6.4\. Analyzing space with convolutional networks
  id: totrans-784
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 使用卷积网络分析空间
- en: 'In Go, you often see particular local patterns of stones over and over again.
    Human players learn to recognize dozens of these shapes, and often give them evocative
    names (like *tiger’s mouth*, *bamboo joint*, or my personal favorite, the *rabbitty
    six*). To make decisions like a human, our Go AI will also have to recognize many
    local spatial arrangements. A particular type of neural network called a *convolutional
    network* is specially designed for detecting spatial relationships like this.
    Convolutional neural networks, or CNNs, have many applications beyond games: you’ll
    find them applied to images, audio, and even text. This section shows how to build
    CNNs and apply them to Go game data. First, we introduce the concept of convolution.
    Next, we show how to build CNNs in Keras. Finally, we show useful ways to process
    the output of a convolutional layer.'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 在围棋中，你经常会看到石头排列的特定局部模式反复出现。人类玩家学会识别这些形状中的几十种，并且经常给它们起一些富有启发性的名字（比如*老虎嘴*、*竹节*，或者我个人最喜欢的，*毛毛六*）。为了像人类一样做出决策，我们的围棋AI也必须识别许多局部的空间排列。一种称为*卷积网络*的特定类型的神经网络专门设计用于检测这种空间关系。卷积神经网络，或CNN，在游戏之外有许多应用：你会在图像、音频甚至文本中找到它们。本节展示了如何构建CNN并将它们应用于围棋游戏数据。首先，我们介绍卷积的概念。接下来，我们展示如何在Keras中构建CNN。最后，我们展示了处理卷积层输出的有用方法。
- en: 6.4.1\. What convolutions do intuitively
  id: totrans-786
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 什么是直观上的卷积
- en: 'Convolutional layers and the networks we build from them get their name from
    a traditional operation from computer vision: *convolutions*. Convolutions are
    a straightforward way to transform an image or apply a filter, if you will. For
    two matrices of the same size, a simple convolution is computed by doing the following:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层以及我们从它们构建的网络得名于计算机视觉中的一个传统操作：*卷积*。卷积是一种直接将图像或应用过滤器的方式。对于两个相同大小的矩阵，简单的卷积是通过以下步骤计算的：
- en: Multiplying these two matrices element by element
  id: totrans-788
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个矩阵逐元素相乘
- en: Summing up all the values of the resulting matrix
  id: totrans-789
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求和结果矩阵的所有值
- en: The output of such a simple convolution is a scalar value. [Figure 6.5](#ch06fig05)
    shows an example of such an operation, convolving two 3 × 3 matrices to compute
    a scalar.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 这样简单卷积的输出是一个标量值。[图6.5](#ch06fig05)展示了这种操作的例子，通过卷积两个3×3矩阵来计算一个标量。
- en: Figure 6.5\. In a simple convolution, you multiply two matrices of the same
    size element by element and then sum up all the values.
  id: totrans-791
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5\. 在简单的卷积中，你将两个相同大小的矩阵逐元素相乘，然后求和所有值。
- en: '![](Images/06fig05_alt.jpg)'
  id: totrans-792
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig05_alt.jpg)'
- en: These simple convolutions alone don’t help you right away, but they can be used
    to compute more-complex convolutions that prove useful for your use case. Instead
    of starting with two matrices of the same size, let’s fix the size of the second
    matrix and increase the size of the first one arbitrarily. In this scenario, you
    call the first matrix the *input image* and the second one the *convolutional
    kernel,* or simply *kernel* (sometimes you also see *filter* used). Because the
    kernel is smaller than the input image, you can compute simple convolutions on
    many *patches* of the input image. In [figure 6.6](#ch06fig06), you see such a
    convolution operation of a 10 × 10 input image with a 3 × 3 kernel in action.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的卷积本身并不能立即帮助你，但它们可以用来计算更复杂的卷积，这些卷积对于你的用例非常有用。不是从两个相同大小的矩阵开始，让我们固定第二个矩阵的大小，并任意增加第一个矩阵的大小。在这种情况下，你称第一个矩阵为*输入图像*，第二个矩阵为*卷积核*，或者简单地称为*核*（有时你也看到使用*过滤器*）。因为核比输入图像小，你可以在输入图像的许多*块*上计算简单的卷积。在[图6.6](#ch06fig06)中，你可以看到这种卷积操作的实例，一个10×10的输入图像与一个3×3的核进行卷积操作。
- en: Figure 6.6\. By passing a convolutional kernel over patches of an input image,
    you can compute a convolution of the image with the kernel. The kernel chosen
    in this example is a vertical edge detector.
  id: totrans-794
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. 通过在输入图像的块上传递卷积核，你可以计算图像与核的卷积。在这个例子中选择的核是一个垂直边缘检测器。
- en: '![](Images/06fig06_alt.jpg)'
  id: totrans-795
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig06_alt.jpg)'
- en: 'The example in [figure 6.6](#ch06fig06) might give you a first hint at why
    convolutions are interesting for us. The input image is a 10 × 10 matrix consisting
    of a center 4 × 8 block of 1s surrounded by 0s. The kernel is chosen so that the
    first column of the matrix (–1, –2, –1) is the negative of the third column (–1,
    –2, –1), and the middle column is all 0s. Therefore, the following points are
    true:'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.6](#ch06fig06)中的例子可能会让你对为什么卷积对我们来说很有趣有一个初步的了解。输入图像是一个由1s组成的10 × 10矩阵，中心是一个4
    × 8的块，周围是0。核的选择使得矩阵的第一列（-1, -2, -1）是第三列（-1, -2, -1）的相反数，中间列全是0。因此，以下各点是正确的：'
- en: Whenever you apply this kernel to a 3 × 3 patch of the input image in which
    all pixel values are the same, the output of the convolution will be 0.
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论何时你将这个核应用于输入图像的3 × 3块，其中所有像素值都相同，卷积的输出将是0。
- en: When you apply this convolutional kernel to an image patch in which the left
    column has higher values than the right, the convolution will be negative.
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你将这个卷积核应用于一个左列值高于右列的图像块时，卷积结果将是负数。
- en: When you apply this convolutional kernel to an image patch in which the right
    column has higher values than the left, the convolution will be positive.
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你将这个卷积核应用于一个右列值高于左列的图像块时，卷积结果将是正数。
- en: The convolutional kernel is chosen to detect *vertical edges in the input image*.
    Edges on the left of an object will have positive values; edges on the right will
    have negative ones. This is exactly what you can see in the result of the convolution
    in [figure 6.6](#ch06fig06).
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核被选择来检测输入图像中的*垂直边缘*。物体左侧的边缘将具有正值；右侧的边缘将具有负值。这正是你在[图6.6](#ch06fig06)中卷积的结果中可以看到的。
- en: The kernel in [figure 6.6](#ch06fig06) is a classical kernel used in many applications
    and is called a *Sobel kernel*. If you flip this kernel by 90 degrees, you end
    up with a horizontal edge detector. In the same way, you can define convolutional
    kernels that blur or sharpen an image, detect corners, and many other things.
    Many of these kernels can be found in standard image-processing libraries.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.6](#ch06fig06)中的核是许多应用中使用的经典核，称为*索贝尔核*。如果你将这个核旋转90度，你将得到一个水平边缘检测器。同样，你可以定义卷积核来模糊或锐化图像，检测角点，以及许多其他功能。许多这些核可以在标准的图像处理库中找到。'
- en: What’s interesting is to see that convolutions can be used to extract valuable
    information from image data, which is exactly what you intend to do for your use
    case of predicting the next move from Go data. Although in the preceding example
    we chose a particular convolutional kernel, the way convolutions are used in neural
    networks is that these kernels are learned from data by backpropagation.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，可以看到卷积可以用来从图像数据中提取有价值的信息，这正是你从围棋数据中预测下一步的目的。尽管在先前的例子中我们选择了一个特定的卷积核，但在神经网络中使用卷积的方式是，这些核通过反向传播从数据中学习。
- en: So far, we’ve discussed how to apply one convolutional kernel to one input image
    only. In general, it’s useful to apply many kernels to many images to produce
    many output images. How can you do this? Let’s say you have four input images
    and define four kernels. Then you can sum up the convolutions for each input and
    arrive at one output image. In what follows, you’ll call the output images of
    such convolutions *feature maps*. Now, if you want to have five resulting feature
    maps instead of one, you define five kernels per input image instead of one. Mapping
    *n* input images to *m* feature maps, by using *n* × *m* convolutional kernels,
    is called a *convolutional layer*. [Figure 6.7](#ch06fig07) illustrates this situation.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了如何将一个卷积核应用于一个输入图像。一般来说，将多个核应用于多个图像以产生多个输出图像是有用的。你该如何做呢？假设你有四个输入图像并定义了四个核。然后你可以对每个输入的卷积进行求和，得到一个输出图像。在接下来的内容中，你将称这种卷积的输出图像为*特征图*。现在，如果你想得到五个结果特征图而不是一个，你可以在每个输入图像上定义五个核而不是一个。通过使用*n*
    × *m*个卷积核将*n*个输入图像映射到*m*个特征图，称为*卷积层*。[图6.7](#ch06fig07)说明了这种情况。
- en: Figure 6.7\. In a convolutional layer, a number of input images is operated
    on by convolutional kernels to produce a specified number of feature maps.
  id: totrans-804
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7。在一个卷积层中，多个输入图像通过卷积核进行处理，以产生指定数量的特征图。
- en: '![](Images/06fig07_alt.jpg)'
  id: totrans-805
  prefs: []
  type: TYPE_IMG
  zh: '![图像6.7](Images/06fig07_alt.jpg)'
- en: Seen this way, a convolution layer is a way to transform a number of input images
    to output images, thereby extracting relevant spatial information of the input.
    In particular, as you might have anticipated, convolutional layers can be *chained*,
    thereby forming a neural network of convolutional layers. Usually, a network that
    consists of convolutional and dense layers only is referred to as *convolutional
    neural network*, or simply a *convolutional network*.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，卷积层是将多个输入图像转换为输出图像的一种方式，从而提取输入的相关空间信息。特别是，正如你可能预料的那样，卷积层可以**串联**，从而形成一个由卷积层组成的神经网络。通常，只由卷积层和密集层组成的网络被称为**卷积神经网络**，或简称**卷积网络**。
- en: '|  |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Tensors in deep learning**'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习中的张量**'
- en: We stated that the output of a convolutional layer is a bunch of images. Although
    it can certainly be helpful to think of it that way, there’s also a bit more to
    it. Just as vectors (1D) consist of individual entries, they’re not just a bunch
    of numbers. In the same way, matrices (2D) consist of column vectors, but have
    an inherent two-dimensional structure that’s used in matrix multiplications and
    other operations (such as convolutions). The output of a convolutional layer has
    a three-dimensional structure. The filters in a convolutional layer have even
    one dimension more and possess a 4D structure (a 2D filter for each combination
    of input and output image). And it doesn’t stop there—it’s common for advanced
    deep-learning techniques to deal with even higher-dimensional data structures.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指出卷积层的输出是一系列图像。虽然这样想确实有助于理解，但其中还有更多内容。正如向量（一维）由单个条目组成，它们不仅仅是数字的集合。同样，矩阵（二维）由列向量组成，但具有矩阵乘法和其他操作（如卷积）中使用的固有二维结构。卷积层的输出具有三维结构。卷积层中的过滤器甚至还有一个维度，具有四维结构（每个输入和输出图像组合都有一个二维过滤器）。而且这还没有结束——高级深度学习技术通常处理更高维度的数据结构。
- en: In linear algebra, the higher-dimensional equivalent of vectors and matrices
    is *tensors*. [Appendix A](kindle_split_028.xhtml#app01) goes into a little more
    detail, but we can’t go into the definition of tensors here. For the rest of this
    book, you don’t need any formal definition of tensors. But apart from having heard
    about the concept, tensors give us convenient terminology that we use in later
    chapters. For instance, the collection of images coming out of a convolutional
    layer can be referred to as 3-Tensor. The 4D filters in a convolutional layer
    form a 4-Tensor. So you could say that a convolution is an operation in which
    a 4-Tensor (the convolutional filters) operates on a 3-Tensor (the input images)
    to transform it into another 3-Tensor.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，向量和矩阵的高维等价物是**张量**。[附录A](kindle_split_028.xhtml#app01)中会详细介绍，但在这里我们无法给出张量的定义。在这本书的其余部分，你不需要任何正式的张量定义。但除了听说过这个概念外，张量为我们提供了在后续章节中使用的方便术语。例如，从卷积层输出的图像集合可以被称为3-张量。卷积层中的4维过滤器形成了一个4-张量。因此，可以说卷积是一个操作，其中4-张量（卷积过滤器）作用于3-张量（输入图像），将其转换为另一个3-张量。
- en: More generally, you can say that a sequential neural network is a mechanism
    that transforms tensors of varying dimension step-by-step. This idea of input
    data “flowing” through a network by using tensors is what led to the name TensorFlow,
    Google’s popular machine-learning library that you’ll use to run your Keras models.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，可以说一个序列神经网络是一种逐步转换不同维度张量的机制。这种通过使用张量使输入数据“流动”通过网络的想法导致了TensorFlow这个名称，这是谷歌流行的机器学习库，你将使用它来运行你的Keras模型。
- en: '|  |'
  id: totrans-812
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note that in all of this discussion, we’ve talked only about how to feed data
    through a convolutional layer, but not how backpropagation would work. We leave
    this part out on purpose, because it would mathematically go beyond the scope
    of this book, but more importantly, Keras takes care of the backward pass for
    us.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在所有这些讨论中，我们只讨论了如何将数据通过卷积层，但没有讨论反向传播的工作方式。我们故意省略了这部分内容，因为它在数学上超出了本书的范围，但更重要的是，Keras会为我们处理反向传播。
- en: Generally, a convolutional layer has a lot fewer parameters than a comparable
    dense layer. If you were to define a convolutional layer with kernel size (3,
    3) on a 28 × 28 input image, leading to an output of size 26 × 26, the convolutional
    layer would have 3 × 3 = 9 parameters. In a convolutional layer, you’ll usually
    have a *bias term* as well that’s added to the output of each convolution, resulting
    in a total of 10 parameters. If you compare this to a dense layer connecting an
    input vector of length 28 × 28 to an output vector of length 26 × 26, such a dense
    layer would have 28 × 28 × 26 × 26 = 529,984 parameters, excluding biases. At
    the same time, convolution operations are computationally more costly than regular
    matrix multiplications used in dense layers.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，卷积层的参数数量比相应的密集层要少得多。如果你在 28 × 28 输入图像上定义一个大小为 (3, 3) 的卷积层，导致输出大小为 26 × 26，那么卷积层将有
    3 × 3 = 9 个参数。在卷积层中，你通常还会有一个添加到每个卷积输出的 *偏置项*，从而总共达到 10 个参数。如果你将此与连接 28 × 28 长度输入向量到
    26 × 26 长度输出向量的密集层进行比较，这样的密集层将有 28 × 28 × 26 × 26 = 529,984 个参数，不包括偏置项。同时，卷积操作在计算上比密集层中使用的常规矩阵乘法更昂贵。
- en: 6.4.2\. Building convolutional neural networks with Keras
  id: totrans-815
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. 使用 Keras 构建卷积神经网络
- en: To build and run convolutional neural networks with Keras, you need to work
    with a new layer type called `Conv2D` that carries out convolutions on two-dimensional
    data, such as Go board data. You’ll also get to know another layer called `Flatten`
    that flattens the output of a convolutional layer into vectors, which can then
    be fed into a dense layer.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Keras 构建和运行卷积神经网络，你需要与一种新的层类型 `Conv2D` 一起工作，该层对二维数据进行卷积，例如围棋棋盘数据。你还将了解另一个名为
    `Flatten` 的层，它将卷积层的输出展平为向量，然后可以将其输入到密集层。
- en: To start, the preprocessing step for your input data now looks a little different
    than before. Instead of flattening the Go board, you keep its two-dimensional
    structure intact.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你的输入数据的预处理步骤现在看起来与之前略有不同。你不再展平围棋棋盘，而是保持其二维结构完整。
- en: Listing 6.16\. Loading and preprocessing Go data for convolutional neural networks
  id: totrans-818
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.16\. 加载和预处理用于卷积神经网络的围棋数据
- en: '[PRE67]'
  id: totrans-819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '***1* Import two new layers, a 2D convolutional layer, and one that flattens
    its input to vectors.**'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 导入两个新的层，一个 2D 卷积层和一个将输入展平为向量的层。**'
- en: '***2* The input data shape is three-dimensional; you use one plane of a 9 ×
    9 board representation.**'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 输入数据形状是三维的；你使用 9 × 9 棋盘表示的一个平面。**'
- en: '***3* Then reshape your input data accordingly.**'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 然后相应地调整你的输入数据。**'
- en: Now you can use the Keras `Conv2D` object to build the network. You use two
    convolutional layers, and then *flatten* the output of the second and follow up
    with two dense layers to arrive at an output of size 9 × 9, as before.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用 Keras 的 `Conv2D` 对象来构建网络。你使用两个卷积层，然后将第二个卷积层的输出 *展平*，接着添加两个密集层，以得到之前的大小
    9 × 9 的输出。
- en: Listing 6.17\. Building a simple convolutional neural network for Go data with
    Keras
  id: totrans-824
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.17\. 使用 Keras 为围棋数据构建简单的卷积神经网络
- en: '[PRE68]'
  id: totrans-825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '***1* The first layer in your network is a Conv2D layer with 48 output filters.**'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你的网络中的第一层是一个具有 48 个输出滤波器的 Conv2D 层。**'
- en: '***2* For this layer, you choose a 3 × 3 convolutional kernel.**'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对于这个层，你选择一个 3 × 3 的卷积核。**'
- en: '***3* Normally, the output of a convolution is smaller than the input. By adding
    padding=’same’, you ask Keras to pad your matrix with 0s around the edges, so
    the output has the same dimension as the input.**'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 通常，卷积的输出小于输入。通过添加 padding=''same''，你要求 Keras 在矩阵边缘填充 0，以便输出与输入具有相同的维度。**'
- en: '***4* The second layer is another convolution. You leave out the filters and
    kernel_size arguments for brevity.**'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 第二层是另一个卷积。为了简洁起见，你省略了 filters 和 kernel_size 参数。**'
- en: '***5* You then flatten the 3D output of the previous convolutional layer...**'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 然后展平之前卷积层的 3D 输出...**'
- en: '***6* ...and follow up with two more dense layers, as you did in the MLP example.**'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* ...然后像在 MLP 示例中那样添加两个更多的密集层。**'
- en: The compiling, running, and evaluating of this model can stay exactly the same
    as in the MLP example. The only things you changed are the input data shape and
    the specification of the model itself.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的编译、运行和评估可以与 MLP 示例中的完全相同。你唯一改变的是输入数据形状和模型本身的指定。
- en: 'If you run the preceding model, you’ll see that the test accuracy has barely
    budged: it should land somewhere around 2.3% again. That’s completely fine—you
    have a few more tricks to unlock the full power of your convolutional model. For
    the rest of this chapter, you’ll introduce more-advanced deep-learning techniques
    to improve your move-prediction accuracy.'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的模型，你会看到测试准确率几乎没有变化：它应该再次落在2.3%左右。这完全没问题——你还有一些技巧可以解锁卷积模型的全威力。在本章的剩余部分，你将介绍更多高级的深度学习技术来提高你的移动预测准确率。
- en: 6.4.3\. Reducing space with pooling layers
  id: totrans-834
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3. 使用池化层减少空间
- en: One common technique that you’ll find in most deep-learning applications featuring
    convolutional layers is that of *pooling*. You use pooling to downsize images,
    to reduce the number of neurons a previous layer has.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在大多数具有卷积层的深度学习应用中找到的一种常见技术是*池化*。你使用池化来缩小图像，以减少前一层中的神经元数量。
- en: 'The concept of pooling is easily explained: you down-sample images by grouping
    or pooling patches of the image into a single value. The example in [figure 6.8](#ch06fig08)
    demonstrates how to cut an image by a factor of 4 by keeping only the maximum
    value in each disjoint 2 × 2 patch of the image.'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 池化概念很容易解释：通过将图像的图像块分组或池化成一个单一值来下采样图像。图6.8的例子展示了如何通过只保留图像中每个不重叠的2×2块中的最大值，将图像按4倍因子切割。
- en: Figure 6.8\. Reducing an 8 × 8 image to an image of size (4, 4) by applying
    a 2 × 2 max pooling kernel
  id: totrans-837
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8. 通过应用2×2最大池化核将8×8图像减少到大小为(4, 4)的图像
- en: '![](Images/06fig08_alt.jpg)'
  id: totrans-838
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig08_alt.jpg)'
- en: This technique is called *max pooling*, and the size of the disjoint patches
    used for pooling is referred to as *pool size*. You can define other types of
    pooling as well; for instance, computing the average of the values in a patch.
    This version is called *average pooling*.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术称为*最大池化*，用于池化的不重叠块的大小被称为*池大小*。你也可以定义其他类型的池化；例如，计算块中值的平均值。这个版本被称为*平均池化*。
- en: You can define a neural network layer, usually preceding or following a convolutional
    layer, as follows.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以定义一个神经网络层，通常位于卷积层之前或之后，如下所示。
- en: Listing 6.18\. Adding a max pooling layer of pool size (2, 2) to a Keras model
  id: totrans-841
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.18. 在Keras模型中添加大小为(2, 2)的最大池化层
- en: '[PRE69]'
  id: totrans-842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: You can also experiment with replacing `MaxPooling2D` by `AveragePooling2D`
    in [listing 6.4](#ch06ex04). In cases such as image recognition, pooling is in
    practice often indispensable to reduce the output size of convolutional layers.
    Although the operation loses a little information by down-sampling images, it’ll
    usually retain enough of it to make accurate predictions, but at the same time
    reducing the amount of computation needed quite drastically.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在[列表6.4](#ch06ex04)中尝试用`AveragePooling2D`替换`MaxPooling2D`。在图像识别等情况下，池化在实践中通常不可或缺，用于减少卷积层的输出大小。尽管通过下采样图像操作会丢失一些信息，但它通常会保留足够的信息以进行准确的预测，同时大幅减少所需的计算量。
- en: Before you see pooling layers in action, let’s discuss a few other tools that
    will make your Go move predictions much more accurate.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 在你看到池化层实际应用之前，让我们讨论一些其他工具，这些工具将使你的围棋移动预测更加准确。
- en: 6.5\. Predicting Go move probabilities
  id: totrans-845
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5. 预测围棋移动概率
- en: 'Since we first introduced neural networks in [chapter 5](kindle_split_017.xhtml#ch05),
    you’ve used only a single activation function: the logistic sigmoid function.
    Also, you’ve been using mean squared error as a loss function throughout. Both
    choices are good first guesses and certainly have their place in your deep-learning
    toolbox, but aren’t particularly well suited for our use case.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们在第5章首次介绍神经网络以来，你一直只使用了一个激活函数：逻辑sigmoid函数。而且，你一直使用均方误差作为损失函数。这两个选择都是好的起点，并且在你的深度学习工具箱中当然有它们的位置，但它们并不特别适合我们的用例。
- en: 'In the end, when predicting Go moves, what you’re really after is this question:
    for each possible move on the board, how *likely* is it that this move is the
    next move? At each point in time, many good moves are usually available on the
    board. You set up your deep-learning experiments to find *the* next move from
    the data you feed into the algorithm, but ultimately the promise of representation
    learning, and deep learning in particular, is that you can learn enough about
    the structure of the game to predict the likelihood of a move. You want to predict
    a *probability distribution* of all possible moves. This can’t be guaranteed with
    sigmoid activation functions. Instead, you introduce the softmax activation function,
    which is used to predict probabilities in the last layer.'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当你预测围棋走法时，你真正追求的是这个问题：对于棋盘上每个可能的走法，这个走法成为下一步的概率有多大？在每一个时间点，棋盘上通常有很多好的走法。你设置你的深度学习实验，从算法中输入的数据中找到
    *下一个* 走法，但最终，表示学习和深度学习，特别是深度学习的承诺是，你可以了解足够多的游戏结构来预测走法的可能性。你想要预测所有可能走法的 *概率分布*。这不能通过
    sigmoid 激活函数来保证。相反，你引入了 softmax 激活函数，它在最后一层用于预测概率。
- en: 6.5.1\. Using the softmax activation function in the last layer
  id: totrans-848
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1\. 在最后一层使用 softmax 激活函数
- en: 'The softmax activation function is a straightforward generalization of the
    logistic sigmoid σ. To compute the softmax function for a vector *x* = (*x*[1],
    ..., *x[l]*), you first apply the exponential function to each component; you
    compute *ex[i]*. Then you *normalize* each of these values by the sum of all values:'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 激活函数是对逻辑 sigmoid σ 的直接推广。为了计算向量 *x* = (*x*[1], ..., *x[l]*) 的 softmax
    函数，你首先对每个分量应用指数函数；你计算 *ex[i]*。然后你将这些值中的每一个通过所有值的总和进行归一化：
- en: '![](Images/p0139_01.jpg)'
  id: totrans-850
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0139_01.jpg)'
- en: By definition, the components of the softmax function are non-negative and add
    up to 1, meaning the softmax spits out probabilities. Let’s compute an example
    to see how it works.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，softmax 函数的分量都是非负的，并且加起来等于 1，这意味着 softmax 输出的是概率。让我们计算一个例子来看看它是如何工作的。
- en: Listing 6.19\. Defining the softmax activation function in Python
  id: totrans-852
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.19\. 在 Python 中定义 softmax 激活函数
- en: '[PRE70]'
  id: totrans-853
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'After defining softmax in Python, you compute it on a vector of length 2; namely,
    *x* = (100, 100). If you compute the sigmoid of *x*, the outcome will be close
    to (1, 1). But computing the softmax for this example yields (0.5, 0.5). This
    is what you should’ve expected: because the values of the softmax function sum
    up to 1, and both entries are the same, softmax assigns both components equal
    probability.'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中定义 softmax 之后，你在一个长度为 2 的向量上计算它；即，*x* = (100, 100)。如果你计算 *x* 的 sigmoid，结果将接近
    (1, 1)。但是计算这个示例的 softmax 得到的是 (0.5, 0.5)。这正是你所期望的：因为 softmax 函数的值加起来等于 1，而且两个条目都是相同的，softmax
    将两个分量分配了相同的概率。
- en: Most often, you see the softmax activation function applied as the activation
    function of the last layer in a neural network, so that you get a guarantee on
    predicting output probabilities.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，你会在神经网络的最外层看到 softmax 激活函数被用作激活函数，这样你可以确保预测输出概率。
- en: Listing 6.20\. Adding a max pooling layer of pool size (2, 2) to a Keras model
  id: totrans-856
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.20\. 向 Keras 模型添加大小为 (2, 2) 的最大池化层
- en: '[PRE71]'
  id: totrans-857
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 6.5.2\. Cross-entropy loss for classification problems
  id: totrans-858
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.2\. 用于分类问题的交叉熵损失
- en: In the preceding chapter, you started out with mean squared error as your loss
    function and we remarked that it’s not the best choice for your use case. To follow
    up on this, let’s have a closer look at what might go wrong and propose a viable
    alternative.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你开始使用均方误差作为你的损失函数，我们指出这并不是你用例的最佳选择。为了继续这个话题，让我们更仔细地看看可能出错的地方，并提出一个可行的替代方案。
- en: Recall that you formulated your move-prediction use case as a *classification
    problem*, in which you have 9 × 9 possible classes, only one of which is correct.
    The correct class is labeled as 1, and all others are labeled as 0\. Your predictions
    for each class will always be a value between 0 and 1\. This is a strong assumption
    on the way your prediction data looks, and the loss function you’re using should
    reflect that. If you look at what MSE does, taking the square of the difference
    between prediction and label, it makes no use of the fact that you’re constrained
    to a range of 0 to 1\. In fact, MSE works best for *regression problems*, in which
    the output is a continuous range. Think of predicting the height of a person.
    In such scenarios, MSE will penalize *large* differences. In your scenario, the
    absolute largest difference between prediction and actual outcome is 1.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，你将你的动作预测用例表述为一个*分类问题*，其中你有9 × 9个可能的类别，只有一个类别是正确的。正确的类别被标记为1，所有其他类别都被标记为0。你对每个类别的预测将始终是一个介于0和1之间的值。这是对你预测数据外观的强烈假设，你使用的损失函数应该反映这一点。如果你看看MSE做了什么，它取预测和标签之间差异的平方，它没有使用你被限制在0到1范围内的这一事实。实际上，MSE最适合*回归问题*，其中输出是一个连续的范围。想想预测一个人的身高。在这种情况下，MSE会惩罚*大的*差异。在你的场景中，预测和实际结果之间最大的绝对差异是1。
- en: 'Another problem with MSE is that it penalizes all 81 prediction values the
    same way. In the end, you’re concerned only with predicting the one true class,
    labeled 1\. Let’s say you have a model that predicts the correct move with a value
    of 0.6 and all others 0, except for one, which the model assigns to 0.4\. In this
    situation, the mean squared error is (1 – 0.6)² + (0 – 0.4)² = 2 × 0.4², or about
    0.32\. Your prediction is correct, but you assign the same loss value to both
    nonzero predictions: about 0.16\. Is it really worth putting the same emphasis
    on the smaller value? If you compare this to the situation in which the correct
    move gets 0.6 again, but two other moves receive a prediction of 0.2, then the
    MSE is (0.4)² + 2 × 0.2², or roughly 0.24, a significantly lower value than in
    the preceding scenario. But what if the value 0.4 really is more accurate, in
    that it’s just a strong move that *may also be a candidate for the next move*?
    Should you really penalize this with your loss function?'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: MSE的另一个问题是它以相同的方式惩罚所有81个预测值。最终，你只关心预测正确的类别，即标记为1的类别。假设你有一个模型，它用0.6的值预测正确的动作，其他所有动作都是0，除了一个，模型将其分配为0.4。在这种情况下，均方误差是(1
    – 0.6)² + (0 – 0.4)² = 2 × 0.4²，大约是0.32。你的预测是正确的，但你将相同的损失值分配给所有非零预测：大约是0.16。这真的值得对较小的值给予同样的重视吗？如果你将这种情况与正确的动作再次得到0.6，但两个其他动作被预测为0.2的情况进行比较，那么MSE是(0.4)²
    + 2 × 0.2²，大约是0.24，比前一种情况低得多。但如果是0.4的值确实更准确，即它只是一个*也可能成为下一个动作候选者*的强动作，你应该真的用你的损失函数来惩罚它吗？
- en: 'To take care of these issues, we introduce the *categorical cross-entropy loss
    function*, or *cross-entropy loss* for short. For labels *ŷ* and predictions *y*
    of a model, this loss function is defined as follows:'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们引入了*分类交叉熵损失函数*，或简称为*交叉熵损失*。对于模型的标签*ŷ*和预测*y*，这个损失函数的定义如下：
- en: '![](Images/p0140_01.jpg)'
  id: totrans-863
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0140_01.jpg)'
- en: 'Note that although this might look like a sum consisting of many terms, involving
    a lot of computation, for our use case this formula boils down to just a single
    term: the one for which *ŷ[i]* is 1\. The cross-entropy error is simply –log(*y[i]*)
    for the index *i* for which *ŷ[i]* = 1\. Simple enough, but what do you gain from
    this?'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管这看起来像是由许多项组成的和，涉及大量的计算，但对于我们的用例来说，这个公式简化为只有一个项：即*ŷ[i]*为1的那个项。对于*ŷ[i]* =
    1的索引*i*，交叉熵误差简单地是 –log(*y[i]*)。这很简单，但你从中得到了什么？
- en: Because cross-entropy loss penalizes only the term for which the label is 1,
    the distribution of all other values isn’t directly affected by it. In particular,
    in the scenario in which you predict the correct next move with a probability
    of 0.6, there’s no difference between attributing one other move a likelihood
    of 0.4 or two with 0.2\. The cross-entropy loss is –log(0.6) = 0.51 in both cases.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为交叉熵损失只惩罚标签为1的项，所以其他所有值的分布不会直接受到影响。特别是，在你预测下一个动作的概率为0.6的情况下，将一个其他动作赋予0.4的概率或两个动作各赋予0.2的概率之间没有区别。在两种情况下，交叉熵损失都是
    –log(0.6) = 0.51。
- en: Cross-entropy loss is tailored to a range of [0,1]. If your model predicts a
    probability of 0 for the move that actually happened, that’s as wrong as it can
    get. You know that log(1) = 0 and that –log(*x*) for *x* between 0 and 1 approaches
    infinity as *x* approaches 0, meaning that –log(*x*) becomes arbitrarily large
    (and doesn’t just grow quadratically, as MSE).
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵损失针对的范围是 [0,1]。如果你的模型预测了一个实际发生的动作的概率为 0，那么这是最错误的情况。你知道 log(1) = 0，并且当 *x*
    在 0 和 1 之间时，-log(*x*) 随着 *x* 接近 0 而趋近于无穷大，这意味着 -log(*x*) 变得任意大（而不仅仅是平方增长，如 MSE）。
- en: On top of that, MSE falls off more quickly as *x* approaches 1, meaning that
    you get a much smaller loss for less-confident predictions. [Figure 6.9](#ch06fig09)
    gives a visual comparison of MSE and cross-entropy loss.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，当 *x* 接近 1 时，均方误差下降得更快，这意味着对于不太确定的预测，你会得到更小的损失。[图 6.9](#ch06fig09) 给出了 MSE
    和交叉熵损失的视觉比较。
- en: Figure 6.9\. Plot of MSE vs. cross-entropy loss for the class labeled as 1\.
    Cross-entropy loss attributes a higher loss for each value in the range [0,1].
  id: totrans-868
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.9. 标记为 1 的类的 MSE 与交叉熵损失的对比图。交叉熵损失将 [0,1] 范围内的每个值分配更高的损失。
- en: '![](Images/06fig09.jpg)'
  id: totrans-869
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9](Images/06fig09.jpg)'
- en: Another crucial point that distinguishes cross-entropy loss from MSE is its
    behavior during *learning* with stochastic gradient descent (SGD). As a matter
    of fact, the gradient updates for MSE get smaller and smaller as you approach
    higher prediction values (*y* getting closer to 1); learning typically slows down.
    Compared to this, cross-entropy loss doesn’t show this slowdown in SGD, and the
    parameter updates are proportional to the difference between prediction and true
    value. We can’t go into the details here, but this represents a tremendous benefit
    for our move-prediction use case.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区分交叉熵损失和均方误差的关键点是它在使用随机梯度下降（SGD）进行*学习*时的行为。事实上，随着你接近更高的预测值（*y* 越来越接近 1），均方误差的梯度更新会越来越小；学习通常会减慢。相比之下，交叉熵损失在
    SGD 中没有显示出这种减速，参数更新与预测值和真实值之间的差异成正比。我们这里不深入细节，但这对我们移动预测用例来说是一个巨大的好处。
- en: Compiling a Keras model with categorical cross-entropy loss, instead of MSE,
    is again simply achieved.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类交叉熵损失而不是 MSE 编译 Keras 模型，同样简单易行。
- en: Listing 6.21\. Compiling a Keras model with categorical cross-entropy
  id: totrans-872
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.21. 使用分类交叉熵编译 Keras 模型
- en: '[PRE72]'
  id: totrans-873
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: With cross-entropy loss and softmax activations in your tool belt, you’re now
    much better equipped to deal with categorical labels and predicting probabilities
    with a neural network. To finish off this chapter, let’s add two techniques that
    will allow you to build deeper networks—networks with more layers.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的工具箱中有了交叉熵损失和softmax激活，你现在处理分类标签和用神经网络预测概率的能力大大增强。为了完成这一章，让我们添加两种技术，这将使你能够构建更深的网络——具有更多层的网络。
- en: 6.6\. Building deeper networks with dropout and rectified linear units
  id: totrans-875
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6. 使用 dropout 和修正线性单元构建更深的网络
- en: 'So far, you haven’t built a neural network with more than two to four layers.
    It might be tempting to just add more of the same in the hope that results will
    improve. It’d be great if it were that simple, but in practice you have a few
    aspects to consider. Although continually building deeper and deeper neural networks
    increases the number of parameters a model has and thereby its capacity to adapt
    to data you feed into it, you may also run into trouble doing so. Among the prime
    reasons that this might fail is *overfitting*: your model gets better and better
    at predicting *training* data, but performs suboptimal on *test* data. Put to
    an extreme, there’s no use for a model that can near perfectly predict, or even
    memorize, what it has seen before, but doesn’t know what to do when confronted
    with data that’s a little different. You need to be able to generalize. This is
    particularly true for predicting the next move in a game as complex as Go. No
    matter how much time you spend on collecting training data, situations will always
    arise in game play that your model hasn’t encountered before. In any case, it’s
    important to find a strong next move.'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你还没有构建超过两到四层的神经网络。可能会让人想只是添加更多相同类型的层，希望结果会改善。如果这样简单就好了，但在实践中，你需要考虑几个方面。虽然不断构建更深层次的神经网络会增加模型拥有的参数数量，从而提高其适应输入数据的适应能力，但你可能会遇到一些问题。可能导致失败的主要原因之一是
    *过拟合*：你的模型在预测 *训练* 数据方面越来越好，但在 *测试* 数据上的表现不佳。如果到了极端，一个能够几乎完美预测，甚至记住之前所见，但面对稍微不同的数据却不知道如何应对的模型是没有用的。你需要能够进行泛化。这在预测像围棋这样复杂的游戏中的下一步尤其重要。无论你花多少时间收集训练数据，在游戏过程中总会出现模型之前未曾遇到的情况。无论如何，找到强有力的下一步是非常重要的。
- en: 6.6.1\. Dropping neurons for regularization
  id: totrans-877
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.1\. 通过丢弃神经元进行正则化
- en: Preventing overfitting is a common challenge in machine learning in general.
    You can find a lot of literature about *regularization techniques* that are designed
    to address the issue of overfitting. For deep neural networks, you can apply a
    surprisingly simple, yet effective, technique called *dropout*. With dropout applied
    to a layer in a network, for each training step you pick a number of neurons *at
    random* and set them to 0; you *drop* these neurons entirely from the training
    procedure. At each training step, you randomly select new neurons to drop. This
    is usually done by specifying a *dropout rate*, the percentage of neurons to drop
    for the layer at hand. [Figure 6.10](#ch06fig10) shows an example dropout layer
    in which probabilistically half of the neurons are dropped for each mini-batch
    (forward and backward pass).
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 防止过拟合是机器学习中一个常见的挑战。你可以找到很多关于旨在解决过拟合问题的 *正则化技术* 的文献。对于深度神经网络，你可以应用一个简单而有效的技术，称为
    *dropout*。在网络的层上应用 dropout，对于每个训练步骤，你随机选择一定数量的神经元并将其设置为 0；你将这些神经元完全从训练过程中 *丢弃*。在每个训练步骤中，你随机选择新的神经元进行丢弃。这通常通过指定一个
    *dropout rate*，即当前层的神经元丢弃百分比来完成。![图 6.10](#ch06fig10) 展示了一个示例 dropout 层，其中每个小批量（正向和反向传播）以概率丢弃一半的神经元。
- en: Figure 6.10\. A dropout layer with a rate of 50% will randomly drop half of
    the neurons from the computation for each mini-batch of data fed into the network.
  id: totrans-879
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.10\. 一个具有 50% 丢弃率的 `dropout` 层会在网络中为每个输入到网络的数据的小批量随机丢弃一半的神经元。
- en: '![](Images/06fig10_alt.jpg)'
  id: totrans-880
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig10_alt.jpg)'
- en: The rationale behind this process is that by dropping neurons randomly, you
    prevent individual layers, and thereby the whole network, from specializing too
    much to the given data. Layers have to be flexible enough not to rely too much
    on individual neurons. By doing so, you can keep your neural network from overfitting.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的原理是通过随机丢弃神经元，防止单个层，以及整个网络对给定数据过度专门化。层必须足够灵活，不要过度依赖单个神经元。通过这样做，你可以防止你的神经网络过拟合。
- en: In Keras, you can define a `Dropout` layer with a dropout `rate` as follows.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，你可以通过以下方式定义具有 `dropout` `rate` 的 `Dropout` 层。
- en: Listing 6.22\. Importing and adding a `Dropout` layer to a Keras model
  id: totrans-883
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.22\. 将 `Dropout` 层导入并添加到 Keras 模型中
- en: '[PRE73]'
  id: totrans-884
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: You can add dropout layers like this in a sequential network before or after
    every other layer available. Especially in deeper architectures, adding dropout
    layers is often indispensable.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在顺序网络中，在每一层之前或之后添加 `dropout` 层。特别是在更深的架构中，添加 `dropout` 层通常是必不可少的。
- en: 6.6.2\. The rectified linear unit activation function
  id: totrans-886
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.2\. 矩形线性单元激活函数
- en: As a last building block for this chapter, you’ll get to know the *rectified
    linear unit* (*ReLU*) activation function, which turns out to often yield better
    results for deep networks than sigmoid and other activation functions. [Figure
    6.11](#ch06fig11) shows what ReLU looks like.
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的最后一个构建块，你将了解*修正线性单元*（ReLU）激活函数，它最终往往比sigmoid和其他激活函数对深度网络产生更好的结果。[图6.11](#ch06fig11)显示了ReLU的样子。
- en: Figure 6.11\. The ReLU activation function sets negative inputs to 0 and leaves
    positive inputs as is.
  id: totrans-888
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11. ReLU激活函数将负输入设置为0，而将正输入保持不变。
- en: '![](Images/06fig11.jpg)'
  id: totrans-889
  prefs: []
  type: TYPE_IMG
  zh: '![图像6.11](Images/06fig11.jpg)'
- en: ReLU ignores negative inputs by setting them to 0 and returns positive inputs
    unchanged. The stronger the positive signal, the stronger the activation with
    ReLUs. Given this interpretation, rectified linear unit activation functions are
    pretty close to a simple model of neurons in the brain, in which weaker signals
    are ignored, but stronger ones lead to the neuron firing. Beyond this basic analogy,
    we’re not going to argue for or against any theoretical benefits of ReLUs, but
    just note that using them often leads to satisfactory results. To use ReLUs in
    Keras, you replace `sigmoid` with `relu` in the `activation` argument of your
    layers.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU通过将它们设置为0来忽略负输入，并返回未更改的正输入。正信号越强，ReLU的激活作用就越强。根据这种解释，修正线性单元激活函数与大脑中神经元的一个简单模型非常接近，其中弱信号被忽略，但强信号会导致神经元放电。除了这个基本类比之外，我们不会为或反对ReLU的任何理论优势进行辩论，但只是指出，使用它们通常会导致令人满意的结果。要在Keras中使用ReLU，你只需在层的`activation`参数中将`sigmoid`替换为`relu`。
- en: Listing 6.23\. Adding a rectified linear activation to a `Dense` layer
  id: totrans-891
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.23. 向`Dense`层添加修正线性激活
- en: '[PRE74]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 6.7\. Putting it all together for a stronger Go move-prediction network
  id: totrans-893
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7. 将所有内容组合起来以增强围棋走法预测网络
- en: The preceding sections covered a lot of ground and introduced not only convolutional
    networks with max pooling layers, but also cross-entropy loss, the softmax activation
    for last layers, dropout for regularization, and ReLU activations to improve performance
    of your networks. To conclude this chapter, let’s put every new ingredient you
    learned about together into a neural network for your Go move-prediction use case
    and see how well you do now.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节涵盖了大量的内容，不仅介绍了带有最大池化层的卷积网络，还介绍了交叉熵损失、最后一层的softmax激活、用于正则化的dropout以及ReLU激活来提高网络的性能。为了结束本章，让我们将你所学到的每一个新元素组合到一个神经网络中，用于你的围棋走法预测用例，并看看你现在做得怎么样。
- en: To start, let’s recall how to load Go data, encoded with your simple one-plane
    encoder, and reshape it for a convolutional network.
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下如何加载使用简单单平面编码器编码的围棋数据，并将其重塑为卷积网络。
- en: Listing 6.24\. Loading and preprocessing Go data for convolutional neural networks
  id: totrans-896
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.24. 加载和预处理用于卷积神经网络的围棋数据
- en: '[PRE75]'
  id: totrans-897
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Next, let’s enhance your previous convolutional network from [listing 6.3](#ch06ex03)
    as follows:'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们按照以下方式增强你之前的卷积网络[列表6.3](#ch06ex03)：
- en: Keep the basic architecture intact, starting with two convolutional layers,
    then a max pooling layer, and two dense layers to finish off.
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持基本架构不变，从两个卷积层开始，然后是一个最大池化层，最后是两个密集层来完成。
- en: 'Add three dropout layers for regularization: one after each convolutional layer
    and one after the first dense layer. Use a dropout rate of 50%.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加三个dropout层进行正则化：在每个卷积层之后以及第一个密集层之后各一个。使用50%的dropout率。
- en: Change the output layer to a softmax activation, and the inner layers to ReLU
    activations.
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出层更改为softmax激活，并将内部层更改为ReLU激活。
- en: Change the loss function to cross-entropy loss instead of mean squared error.
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将损失函数改为交叉熵损失，而不是均方误差。
- en: Let’s see what this model looks like in Keras.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个模型在Keras中的样子。
- en: Listing 6.25\. Building a convolutional network for Go data with dropout and
    ReLUs
  id: totrans-904
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.25. 使用dropout和ReLU构建用于围棋数据的卷积网络
- en: '[PRE76]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Finally, to evaluate this model, you can run the following code.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了评估这个模型，你可以运行以下代码。
- en: Listing 6.26\. Evaluating your enhanced convolutional network
  id: totrans-907
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.26. 评估增强的卷积网络
- en: '[PRE77]'
  id: totrans-908
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Note that this example increases the number of epochs to 100, whereas you used
    15 before. The output looks something like this:'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个例子将epoch的数量增加到100，而你之前使用的是15。输出看起来大致如下：
- en: '[PRE78]'
  id: totrans-910
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'With this model, your test accuracy goes up to over 8%, which is a solid improvement
    over your baseline model. Also, note the `Trainable params: 456,545` in the output.
    Recall that your baseline model had over 600,000 trainable parameters. While increasing
    the accuracy by a factor of three, you also cut the number of weights. This means
    the credit for the improvement must go to the *structure* of your new model, not
    just its size.'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '使用这个模型，你的测试准确度上升到超过8%，这比你的基线模型有显著的提升。此外，注意输出中的`Trainable params: 456,545`。回想一下，你的基线模型有超过600,000个可训练参数。在将准确度提高三倍的同时，你也减少了权重的数量。这意味着改进的功劳必须归功于你新模型的*结构*，而不仅仅是其大小。'
- en: On the negative side, the training took a lot longer, in large part because
    you increased the number of epochs. This model is learning more-complicated concepts,
    and it needs more training passes. If you have the patience to set `epochs` even
    higher, you can pick up a few more percentage points of accuracy with this model.
    [Chapter 7](kindle_split_019.xhtml#ch07) introduces advanced optimizers that can
    speed up this process.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 在负面方面，训练时间大大延长，这在很大程度上是因为你增加了epoch的数量。这个模型正在学习更复杂的概念，需要更多的训练遍历。如果你有耐心将`epochs`设置得更高，你可以用这个模型提高几个百分点的准确度。[第7章](kindle_split_019.xhtml#ch07)介绍了可以加快这一过程的先进优化器。
- en: 'Next, let’s feed the example board to the model and see what moves it recommends:'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将示例棋盘输入到模型中，看看它推荐了哪些走法：
- en: '[PRE79]'
  id: totrans-914
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The highest-rated move on the board has a score of 0.052—and maps to point
    A in [figure 6.4](#ch06fig04), where black captures the two white stones. Your
    model may not be a master tactician yet, but it has definitely learned something
    about capturing stones! Of course, the results are far from perfect: it still
    gives high scores to many points that already have stones on them.'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 棋盘上评分最高的走法得分为0.052——对应于[图6.4](#ch06fig04)中的点A，那里黑方吃掉了两个白子。你的模型可能还不是一位大师级的战术家，但它确实学到了一些关于吃子的事情！当然，结果远非完美：它仍然会给很多已经有子的情况赋予高分数。
- en: 'At this point, we encourage you to experiment with the model and see what happens.
    Here are a few ideas to get you started:'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们鼓励你尝试这个模型并看看会发生什么。以下是一些帮助你开始的想法：
- en: 'What’s most effective on this problem: max pooling, average pooling, or no
    pooling? (Remember that removing the pooling layer increases the number of trainable
    parameters in the model; so if you see any extra accuracy, keep in mind that you’re
    paying for it with extra computation.)'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个问题上最有效的是最大池化、平均池化，还是不池化？（记住，移除池化层会增加模型中的可训练参数数量；所以如果你看到任何额外的准确度，请记住你是在用额外的计算来支付这笔费用。）
- en: Is it more effective to add a third convolutional layer, or to increase the
    number of filters on the two layers that are already there?
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加第三个卷积层，还是增加现有两个层的滤波器数量更有效？
- en: How small can you make the second-to-last `Dense` layer and still get a good
    result?
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将倒数第二层的`Dense`层缩小到多小，同时还能得到良好的结果？
- en: Can you improve the result by changing the dropout rate?
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能否通过改变dropout率来提高结果？
- en: How accurate can you make the model without using convolutional layers? How
    does the size and training time of that model compare to your best results with
    a CNN?
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用卷积层，你能将模型的准确度提升到多高？该模型的大小和训练时间与你的CNN最佳结果相比如何？
- en: In the next chapter, you’ll apply all the techniques you learned here to build
    a deep-learning Go bot that’s trained on *actual game data*, not just simulated
    games. You’ll also see new ways to encode the inputs, which will improve model
    performance. With these techniques combined, you can build a bot that makes reasonable
    moves and can at least beat beginner Go players.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将应用在这里学到的所有技术来构建一个在真实游戏数据上训练的深度学习围棋机器人，而不仅仅是模拟游戏。你还将看到编码输入的新方法，这将提高模型性能。结合这些技术，你可以构建一个能够做出合理走法并且至少能击败初学者的机器人。
- en: 6.8\. Summary
  id: totrans-923
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8. 摘要
- en: With encoders, you can transform Go board states into inputs for neural networks,
    which is an important first step toward applying deep learning to Go.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编码器，你可以将围棋棋盘状态转换为神经网络的输入，这是将深度学习应用于围棋的重要第一步。
- en: Generating Go data with tree search gives you a first Go data set to apply neural
    networks to.
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过树搜索生成围棋数据，为你提供了一个应用神经网络的第一个围棋数据集。
- en: Keras is a powerful deep-learning library with which you can create many relevant
    deep-learning architectures.
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras是一个强大的深度学习库，你可以用它创建许多相关的深度学习架构。
- en: Using convolutional neural networks, you can leverage the spatial structure
    of input data to extract relevant features.
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络，您可以利用输入数据的空间结构来提取相关特征。
- en: With pooling layers, you can reduce image sizes to reduce computational complexity.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过池化层，您可以减小图像大小以降低计算复杂度。
- en: Using softmax activations in the last layer of your network, you can predict
    output probabilities.
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络的最后一层使用softmax激活函数，您可以预测输出概率。
- en: Working with categorical cross-entropy as a loss function is a more natural
    choice for Go move-prediction networks than mean squared error. Mean squared error
    is more useful when you’re trying to predict numbers in a continuous range.
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将类别交叉熵作为损失函数用于围棋走法预测网络比均方误差更自然。当您试图预测连续范围内的数字时，均方误差更有用。
- en: With dropout layers, you have a simple tool to avoid overfitting for deep network
    architectures.
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用dropout层，您有一个简单的工具来避免深度网络架构的过拟合。
- en: Using rectified linear units instead of sigmoid activation functions can bring
    a significant performance boost.
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ReLU激活函数而不是sigmoid激活函数可以带来显著的性能提升。
- en: 'Chapter 7\. Learning from data: a deep-learning bot'
  id: totrans-933
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章. 从数据中学习：深度学习机器人
- en: '*This chapter covers*'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Downloading and processing actual Go game records
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载和处理实际的围棋游戏记录
- en: Understanding the standard format for storing Go games
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解存储围棋的标准格式
- en: Training a deep-learning model for move prediction with such data
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用此类数据训练用于走法预测的深度学习模型
- en: Using sophisticated Go board encoders to create strong bots
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用复杂的围棋棋盘编码器来创建强大的机器人
- en: Running your own experiments and evaluating them
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行自己的实验并评估它们
- en: In the preceding chapter, you saw many essential ingredients for building a
    deep-learning application, and you built a few neural networks to test the tools
    you learned about. One of the key things you’re still missing is good data to
    learn from. A supervised deep neural network is only as good as the data you feed
    it—and so far, you’ve had only self-generated data at your disposal.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您看到了构建深度学习应用的基本要素，并且构建了一些神经网络来测试您所学的工具。您仍然缺少的是好的学习数据。监督式深度神经网络的好坏取决于您输入的数据——到目前为止，您只有自生成的数据可供使用。
- en: In this chapter, you’ll learn about the most common data format for Go data,
    the Smart Game Format (SGF). You can obtain historical game records in SGF from
    practically every popular Go server. To power a deep neural network for Go move
    prediction, in this chapter you’ll download many SGF files from a Go server, encode
    them in a smart way, and train a neural network with this data. The resulting
    trained network will be much stronger than any previous models in earlier chapters.
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解围棋数据最常见的数据格式，即智能游戏格式（SGF）。您可以从几乎每个流行的围棋服务器获取SGF格式的历史游戏记录。为了为围棋走法预测的深度神经网络提供动力，在本章中，您将从围棋服务器下载许多SGF文件，以智能的方式对它们进行编码，并使用这些数据训练一个神经网络。所得到的训练网络将比早期章节中的任何先前模型都要强大。
- en: '[Figure 7.1](#ch07fig01) illustrates what you can build at the end of this
    chapter.'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.1](#ch07fig01)展示了本章结束时您可以构建的内容。'
- en: Figure 7.1\. Building a deep-learning Go bot, using real-world Go data for training.
    You can find game records from public Go servers to use for training a bot. In
    this chapter, you’ll learn how to find those records, transform them into a training
    set, and train a Keras model to imitate the human players’ decisions.
  id: totrans-943
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1. 使用真实世界的围棋数据进行深度学习围棋机器人构建。您可以从公共围棋服务器找到用于训练机器人的游戏记录。在本章中，您将学习如何找到这些记录，将它们转换为训练集，并训练一个Keras模型来模仿人类玩家的决策。
- en: '![](Images/07fig01_alt.jpg)'
  id: totrans-944
  prefs: []
  type: TYPE_IMG
  zh: '![图片 7.1](Images/07fig01_alt.jpg)'
- en: At the end of this chapter, you can run your own experiments with complex neural
    networks to build a strong bot completely on your own. To get started, you need
    access to real-world Go data.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您可以使用复杂的神经网络进行自己的实验，完全独立地构建一个强大的机器人。要开始，您需要访问真实世界的围棋数据。
- en: 7.1\. Importing Go game records
  id: totrans-946
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1. 导入围棋游戏记录
- en: All the Go data you used up to this point has been generated by yourself. In
    the preceding chapter, you trained a deep neural network to predict moves for
    generated data. The best you could hope for was that your network could perfectly
    predict these moves, in which case the network would play *as well as your tree-search
    algorithm that generated the data*. In a way, the data you feed into the network
    provides an upper bound to a deep-learning bot trained from it. The bot can’t
    outperform the strength of the players generating the data.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你使用的所有 Go 数据都是由你自己生成的。在前一章中，你训练了一个深度神经网络来预测生成数据的走法。你所能期望的最好结果是，你的网络能够完美地预测这些走法，在这种情况下，网络的表现将和生成数据的树搜索算法一样好。从某种意义上说，你输入网络的数据为从它训练的深度学习机器人提供了一个上限。机器人无法超越生成数据的玩家的实力。
- en: By using records of games played by strong human opponents as input to your
    deep neural networks, you can considerably improve the strength of your bots.
    You’ll use game data from the KGS Go Server (formerly known as Kiseido Go Server),
    one of the most popular Go platforms in the world. Before explaining how to download
    and process data from KGS, we’ll first introduce you to the data format your Go
    data comes in.
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将强大人类对手对弈的记录作为输入提供给你的深度神经网络，你可以显著提高你机器人的实力。你将使用来自 KGS 围棋服务器（以前称为 Kiseido 围棋服务器）的游戏数据，这是世界上最受欢迎的围棋平台之一。在解释如何从
    KGS 下载和处理数据之前，我们首先向你介绍你的围棋数据所采用的数据格式。
- en: 7.1.1\. The SGF file format
  id: totrans-949
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1. SGF 文件格式
- en: The Smart Game Format (SGF), initially called Smart Go Format, has been developed
    since the late 80s. Its current, fourth major release (denoted `FF[4]`) was released
    in the late 90s. SGF is a straightforward, text-based format that can be used
    to express games of Go, variations of Go games (for instance, extended game commentaries
    by professional players), and other board games. For the rest of this chapter,
    you’ll assume that the SGF files you’re dealing with consist of Go games without
    any variations. In this section, we teach you a few basics of this rich game format,
    but if you want to learn more about it, start with [https://senseis.xmp.net/?SmartGameFormat](https://senseis.xmp.net/?SmartGameFormat)
    at Sensei’s Library.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 智能游戏格式（SGF），最初被称为智能围棋格式，自 80 年代末以来一直在开发。其当前第四个主要版本（表示为 `FF[4]`）在 90 年代末发布。SGF
    是一种简单、基于文本的格式，可以用来表达围棋游戏、围棋游戏的变体（例如，专业玩家的扩展游戏评论）以及其他棋类游戏。在本章的其余部分，你将假设你处理的 SGF
    文件仅包含没有变体的围棋游戏。在本节中，我们教你一些关于这个丰富游戏格式的基础知识，但如果你想了解更多，可以从 Sensei’s Library 的 [https://senseis.xmp.net/?SmartGameFormat](https://senseis.xmp.net/?SmartGameFormat)
    开始学习。
- en: 'At its core, SGF consists of metadata about the game and the moves played.
    You can specify metadata by two capital letters, encoding a property, and a respective
    value in square brackets. For instance, a Go game played on a board of size (`SZ`)
    9 × 9 would be encoded as `SZ[9]` in SGF. Go moves are encoded as follows: a white
    move on the third row and third column of the board is `W[cc]` in SGF, whereas
    a black move on the seventh row and third column is represented as `B[gc]`; the
    letters `B` and `W` stand for stone color, and coordinates for rows and columns
    are indexed alphabetically. To represent a pass, you use the empty moves `B[]`
    and `W[]`.'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: SGF 的核心是关于游戏和走法的元数据。你可以通过两个大写字母来指定元数据，编码一个属性及其对应的值，放在方括号中。例如，在一个 9 × 9 大小的棋盘上进行的围棋游戏在
    SGF 中会被编码为 `SZ[9]`。围棋的走法编码如下：棋盘第三行第三列的白色走法在 SGF 中表示为 `W[cc]`，而第七行第三列的黑色走法表示为 `B[gc]`；字母
    `B` 和 `W` 代表棋子的颜色，行和列的坐标按字母顺序索引。要表示弃权，你使用空走法 `B[]` 和 `W[]`。
- en: 'The following example of an SGF file is taken from the complete 9 × 9 example
    at the end of [chapter 2](kindle_split_013.xhtml#ch02). It shows a game of Go
    (Go has game number 1, or `GM[1]`, in SGF) in the current SGF version (`FF[4]`),
    played on a 9 × 9 board with zero handicap (`HA[0]`), and 6.5 points komi for
    white as compensation for black getting the first move (`KM[6.5]`). The game is
    played under Japanese rule settings (`RU[Japanese]`) and results (`RE`) in a 9.5-point
    win by white (`RE[W+9.5]`):'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 SGF 文件的示例取自 [第 2 章](kindle_split_013.xhtml#ch02) 结尾的完整 9 × 9 示例。它展示了一局围棋游戏（在
    SGF 中，围棋的游戏编号为 1，或 `GM[1]`），在当前 SGF 版本（`FF[4]`）下进行，棋盘大小为零（`HA[0]`），白方有 6.5 点的
    komi 以补偿黑方先走一步（`KM[6.5]`）。游戏按照日本规则设置（`RU[Japanese]`）进行，最终白方以 9.5 点的胜利结束（`RE[W+9.5]`）：
- en: '[PRE80]'
  id: totrans-953
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'An SGF file is organized as a list of *nodes*, which are separated by semicolons.
    The first node contains metadata about the game: board size, rule set, game result,
    and other background information. Each following node represents a move in the
    game. Whitespace is completely unimportant; you could collapse the whole example
    string into a single line, and it would still be valid SGF. At the end, you also
    see the points that belong to white’s territory, listed under `TW`, and the ones
    that belong to black, under `TB`. Note that the territory indicators are part
    of the same node as white’s last move (`W[]`, indicating a pass): you can consider
    them as a sort of comment on that position in the game.'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: SGF文件组织为一系列 *节点*，它们由分号分隔。第一个节点包含关于游戏的数据：棋盘大小、规则集、游戏结果以及其他背景信息。每个后续节点代表游戏中的一步棋。空白字符完全不重要；你可以将整个示例字符串压缩成一行，它仍然是一个有效的SGF。最后，你还会看到属于白方的领地得分，列在
    `TW` 下，以及属于黑方的得分，列在 `TB` 下。请注意，领地指示符是白方最后一步棋（`W[]`，表示弃权）的同一节点的一部分：你可以把它们看作是对游戏中该位置的某种注释。
- en: This example illustrates some of the core properties of SGF files, and shows
    everything you’ll need for replaying game records in order to generate training
    data. The SGF format supports many more features, but those are mainly useful
    for adding commentary and annotations to game records, so you won’t need them
    for this book.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了SGF文件的一些核心属性，并展示了为了生成训练数据而回放游戏记录所需的一切。SGF格式支持更多功能，但那些主要用于为游戏记录添加注释和说明，所以在这本书中你不需要它们。
- en: 7.1.2\. Downloading and replaying Go game records from KGS
  id: totrans-956
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 从KGS下载并回放围棋游戏记录
- en: If you go to the page [https://u-go.net/gamerecords/](https://u-go.net/gamerecords/),
    you’ll see a table with game records available for download in various formats
    (zip, tar.gz). This game data has been collected from the KGS Go Server since
    2001 and contains only games played in which at least one of the players was 7
    dan or above, or both players were 6 dan. Recall from [chapter 2](kindle_split_013.xhtml#ch02)
    that dan ranks are master ranks, ranging from 1 dan to 9 dan, so these are games
    played by strong players. Also note that all of these games were played on a 19
    × 19 board, whereas in [chapter 6](kindle_split_018.xhtml#ch06) we used only data
    generated for the much less complex situation of a 9 × 9 board.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你访问页面 [https://u-go.net/gamerecords/](https://u-go.net/gamerecords/)，你会看到一个表格，其中列出了以各种格式（zip、tar.gz）可下载的游戏记录。这些游戏数据自2001年以来从KGS围棋服务器收集，只包含至少一名玩家为7段或以上，或者两名玩家都是6段的比赛。回想一下[第2章](kindle_split_013.xhtml#ch02)，段位是大师级段位，从1段到9段不等，所以这些是高手之间的比赛。此外，请注意，所有这些比赛都是在19×19的棋盘上进行的，而在[第6章](kindle_split_018.xhtml#ch06)中，我们只使用了为9×9棋盘这种远没有那么复杂的情况生成的数据。
- en: This is an incredibly powerful data set for Go move prediction, which you’ll
    use in this chapter to power a strong deep-learning bot. You want to download
    this data in an automated fashion by fetching the HTML containing the links to
    individual files, unpacking the files, and then processing the SGF game records
    contained in them.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于围棋走法预测的极其强大的数据集，你将在本章中使用它来为一个强大的深度学习机器人提供动力。你希望以自动化的方式下载这些数据，通过获取包含指向单个文件链接的HTML，解压文件，然后处理其中包含的SGF游戏记录。
- en: As a first step toward using this data as input to deep-learning models, you
    create a new submodule called *data* within your main dlgo module, that you provide
    with an empty __init__.py, as usual. This submodule will contain everything related
    to Go data processing needed for this book.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用这些数据作为深度学习模型输入的第一步，你在主dlgo模块中创建一个新的子模块叫做 *data*，并像往常一样提供一个空的 __init__.py
    文件。这个子模块将包含这本书需要的所有与围棋数据处理相关的信息。
- en: 'Next, to download game data, you create a class called `KGSIndex` in the new
    file index_processor.py within the data submodule. Because this step is entirely
    technical and contributes to neither your Go nor your machine-learning knowledge,
    we omit the implementation here. If you’re interested in the details, the code
    can be found in our GitHub repository. The `KGSIndex` implementation found there
    has precisely one method that you’ll use later: `download_files`. This method
    will mirror the page [https://u-go.net/gamerecords/](https://u-go.net/gamerecords/)
    locally, find all relevant download links, and then download the respective tar.gz
    files in a separate folder called data. Here’s how you can call it.'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了下载比赛数据，你需要在数据子模块中的新文件index_processor.py中创建一个名为`KGSIndex`的类。因为这个步骤完全是技术性的，既不增加你的围棋知识，也不增加你的机器学习知识，所以我们在这里省略了实现细节。如果你对细节感兴趣，代码可以在我们的GitHub仓库中找到。那里找到的`KGSIndex`实现有一个你稍后会用到的确切方法：`download_files`。这个方法将镜像页面[https://u-go.net/gamerecords/](https://u-go.net/gamerecords/)，找到所有相关的下载链接，然后在名为data的单独文件夹中下载相应的tar.gz文件。以下是你可以如何调用它的方法。
- en: Listing 7.1\. Creating an index of zip files containing Go data from KGS
  id: totrans-961
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1\. 从KGS创建包含围棋数据的zip文件的索引
- en: '[PRE81]'
  id: totrans-962
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Running this should result in a command-line output that looks as follows:'
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令应该会产生如下所示的命令行输出：
- en: '[PRE82]'
  id: totrans-964
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Now that you have this data stored locally, let’s move on to processing it for
    use in a neural network.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经将数据存储在本地，让我们继续处理它，以便在神经网络中使用。
- en: 7.2\. Preparing Go data for deep learning
  id: totrans-966
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 准备围棋数据以进行深度学习
- en: In [chapter 6](kindle_split_018.xhtml#ch06), you saw a simple *encoder* for
    Go data that was already presented in terms of the `Board` and `GameState` classes
    introduced in [chapter 3](kindle_split_014.xhtml#ch03). When working with SGF
    files, you first have to extract their content (the *unpacking* we referred to
    earlier) and replay a game from them, so that you can create the necessary game
    state information for your Go playing framework.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_018.xhtml#ch06)中，你看到了一个用于围棋数据的简单*编码器*，它已经在[第3章](kindle_split_014.xhtml#ch03)中引入的`Board`和`GameState`类的基础上进行了介绍。当与SGF文件一起工作时，你必须首先提取它们的内容（我们之前提到的*解包*），然后从它们中重放游戏，以便为你的围棋游戏框架创建必要的游戏状态信息。
- en: 7.2.1\. Replaying a Go game from an SGF record
  id: totrans-968
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 从SGF记录中重放围棋比赛
- en: Reading out an SGF file for Go game state information means understanding and
    implementing the format specifications. Although this isn’t particularly hard
    to do (in the end, it’s just imposing a fixed set of rules on a string of text),
    it’s also not the most exciting aspect of building a Go bot and takes a lot of
    effort and time to do flawlessly. For these reasons, we’ll introduce another submodule
    within dlgo called gosgf that’s responsible for handling all the logic of processing
    SGF files. We treat this submodule as a black box within this chapter and refer
    you to our GitHub repository for more information on how to read and interpret
    SGF with Python.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 读取SGF文件以获取围棋比赛状态信息意味着理解和实现格式规范。尽管这并不特别困难（最终，这只是对文本字符串施加一组固定的规则），但它也不是构建围棋机器人最令人兴奋的方面，而且要完美地完成它需要大量的努力和时间。出于这些原因，我们将在dlgo中引入另一个子模块，称为gosgf，该模块负责处理所有SGF文件的处理逻辑。在本章中，我们将这个子模块视为一个黑盒，并建议你参考我们的GitHub仓库以获取有关如何使用Python读取和解释SGF的更多信息。
- en: '|  |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-971
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The gosgf module is adapted from the Gomill Python library, available at [https://mjw.woodcraft.me.uk/gomill/](https://mjw.woodcraft.me.uk/gomill/).
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: gosgf模块是从Gomill Python库改编的，可在[https://mjw.woodcraft.me.uk/gomill/](https://mjw.woodcraft.me.uk/gomill/)找到。
- en: '|  |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'You’ll need precisely one entity from gosgf that’s sufficient to process everything
    you need: `Sgf_game`. Let’s see how you can use `Sgf_game` to load a sample SGF
    game, read out game information move by move, and apply the moves to a `GameState`
    object. [Figure 7.2](#ch07fig02) shows the beginning of a Go game in terms of
    SGF commands.'
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要从gosgf中获取一个实体，这个实体足以处理你需要的一切：`Sgf_game`。让我们看看如何使用`Sgf_game`来加载一个示例SGF游戏，逐个读取游戏信息，并将这些移动应用到`GameState`对象上。[图7.2](#ch07fig02)展示了从SGF命令的角度来看的围棋比赛的开始。
- en: Figure 7.2\. Replaying a game record from an SGF file. The original SGF file
    encodes game moves with strings such as `B[ee]`. The `Sgf_game` class decodes
    those strings and returns them as Python tuples. You can then apply these moves
    to your `GameState` object to reconstruct the game, as shown in the following
    listing.
  id: totrans-975
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2\. 从SGF文件中重放游戏记录。原始SGF文件使用如`B[ee]`之类的字符串编码游戏移动。`Sgf_game`类解码这些字符串，并将它们作为Python元组返回。然后你可以将这些移动应用到你的`GameState`对象上，以重建游戏，如下所示。
- en: '![](Images/07fig02.jpg)'
  id: totrans-976
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig02.jpg)'
- en: Listing 7.2\. Replaying moves from an SGF file with your Go framework
  id: totrans-977
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.2\. 使用你的围棋框架重放 SGF 文件中的走棋
- en: '[PRE83]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '***1* Import the Sgf_game class from the new gosgf module first.**'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 首先从新的 gosgf 模块导入 Sgf_game 类。**'
- en: '***2* Define a sample SGF string. This content will come from downloaded data
    later.**'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 定义一个样本 SGF 字符串。此内容将来自稍后下载的数据。**'
- en: '***3* With the from_string method, you can create an Sgf_game.**'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用 from_string 方法，你可以创建一个 Sgf_game。**'
- en: '***4* Iterate over the game’s main sequence; you ignore variations and commentaries.**'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 遍历游戏的主序列；你忽略变体和注释。**'
- en: '***5* Items in this main sequence come as (color, move) pairs, where “move”
    is a pair of board coordinates.**'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 主序列中的项以 (颜色，走法) 对的形式出现，其中“走法”是一对棋盘坐标。**'
- en: '***6* The read-out move can then be applied to your current game state.**'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 然后将读取出的走棋应用到当前游戏状态。**'
- en: 'In essence, after you have a valid SGF string, you create a game from it, whose
    main sequence you can iterate over and process however you want. [Listing 7.2](#ch07ex02)
    is central to this chapter and gives you a rough outline for how you’ll proceed
    to process Go data for deep learning:'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，在你有一个有效的 SGF 字符串之后，你可以从中创建一个游戏，你可以遍历其主序列并按需处理。 [列表 7.2](#ch07ex02) 是本章的核心，为你提供了一个如何处理围棋数据以进行深度学习的粗略概述：
- en: Download and unpack the compressed Go game files.
  id: totrans-986
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并解压压缩的围棋游戏文件。
- en: Iterate over each SGF file contained in these files, read them as Python strings,
    and create an `Sgf_game` from these strings.
  id: totrans-987
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历这些文件中包含的每个 SGF 文件，将它们作为 Python 字符串读取，并从这些字符串创建一个 `Sgf_game`。
- en: Read out the main sequence of the Go game for each SGF string, make sure to
    take care of important details such as placing handicap stones, and feed the resulting
    move data into a `GameState` object.
  id: totrans-988
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取每个 SGF 字符串的围棋游戏主序列，确保注意重要细节，例如放置让子石，并将生成的走棋数据输入到 `GameState` 对象中。
- en: For each move, encode the current board information as features with an `Encoder`
    and store the move itself as a label, before placing it on the board. This way,
    you’ll create move prediction data for deep learning on the fly.
  id: totrans-989
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步棋，使用 `Encoder` 将当前棋盘信息编码为特征，并将该步棋本身作为标签存储，然后再将其放置在棋盘上。这样，你就可以实时创建用于深度学习的走棋预测数据。
- en: Store the resulting features and labels in a suitable format so you can pick
    it up later and feed it into a deep neural network.
  id: totrans-990
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的特征和标签存储在合适的格式中，以便你可以稍后将其拾取并输入到深度神经网络中。
- en: Throughout the next few sections, you’ll tackle these five tasks in great detail.
    After processing data like this, you can go back to your move-prediction application
    and see how this data affects move-prediction accuracy.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将详细处理这五个任务。处理完这些数据后，你可以回到你的走棋预测应用程序，看看这些数据如何影响走棋预测的准确性。
- en: 7.2.2\. Building a Go data processor
  id: totrans-992
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 构建 Go 数据处理器
- en: In this section, you’ll build a Go *data processor* that can transform raw SGF
    data into features and labels for a machine-learning algorithm. This is going
    to be a relatively long implementation, so we split it into several parts. When
    you’re finished, you’ll have everything ready to run a deep-learning model on
    real data.
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将构建一个围棋 *数据处理器*，可以将原始 SGF 数据转换为机器学习算法的特征和标签。这将是一个相对较长的实现，所以我们将其分为几个部分。完成之后，你将准备好在真实数据上运行深度学习模型。
- en: To get started, create a new file called processor.py within your new data submodule.
    As before, it’s also completely fine to just follow the implementation here on
    a copy of processor.py from the GitHub repository. Let’s import a few core Python
    libraries that you’ll work with in processor.py. Apart from NumPy for data, you’ll
    need quite a few packages for processing files.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，在你的新数据子模块中创建一个名为 processor.py 的新文件。与之前一样，你也可以完全按照 GitHub 仓库中的 processor.py
    的实现来操作。让我们导入一些你将在 processor.py 中使用的核心 Python 库。除了 NumPy 用于数据外，你还需要许多用于处理文件的包。
- en: Listing 7.3\. Python libraries needed for data and file processing
  id: totrans-995
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.3\. 用于数据和处理文件所需的 Python 库
- en: '[PRE84]'
  id: totrans-996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: As for functionality needed from dlgo itself, you need to import many of the
    core abstractions you’ve built so far.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 dlgo 本身所需的功能，你需要导入你迄今为止构建的许多核心抽象。
- en: Listing 7.4\. Imports for data processing from the `dlgo` module.
  id: totrans-998
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.4\. 从 `dlgo` 模块导入的数据处理导入。
- en: '[PRE85]'
  id: totrans-999
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '***1* Sampler will be used to sample training and test data from files**'
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 样本器将被用来从文件中采样训练和测试数据**'
- en: We haven’t yet discussed the two last imports in the listing (`Sampler` and
    `DataGenerator`) but will introduce them as we build our Go data processor. Continuing
    with processor.py, a `GoDataProcessor` is initialized by providing an `Encoder`
    as string and a `data_directory` to store SGF data in.
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有讨论列表中的最后两个导入（`Sampler`和`DataGenerator`），但将在构建我们的Go数据处理器时介绍它们。继续使用processor.py，通过提供一个`Encoder`字符串和一个用于存储SGF数据的`data_directory`来初始化`GoDataProcessor`。
- en: Listing 7.5\. Initializing a Go data processor with an encoder and a local data
    directory
  id: totrans-1002
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**列表7.5\. 使用编码器和本地数据目录初始化Go数据处理器**'
- en: '[PRE86]'
  id: totrans-1003
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Next, you’ll implement the main data processing method, called `load_go_data`.
    In this method, you can specify the number of games you’d like to process, as
    well as the type of data to load, meaning either *training* or *test* data. `load_go_data`
    will download online Go records form KGS, sample the specified number of games,
    process them by creating features and labels, and then persist the result locally
    as NumPy arrays.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将实现主要的数据处理方法，称为`load_go_data`。在这个方法中，您可以指定您想要处理的比赛数量，以及要加载的数据类型，这意味着可以是*训练*或*测试*数据。`load_go_data`将从KGS下载在线围棋记录，采样指定数量的比赛，通过创建特征和标签来处理它们，然后将结果以NumPy数组的形式本地持久化。
- en: Listing 7.6\. `load_go_data` loads, processes, and stores data
  id: totrans-1005
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**列表7.6\. `load_go_data`加载数据、处理数据并存储数据**'
- en: '[PRE87]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '***1* For data_type, you can choose either train or test.**'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 对于数据类型，您可以选择train或test。**'
- en: '***2* num_samples refers to the number of games to load data from.**'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* num_samples指的是从游戏中加载数据的数量。**'
- en: '***3* Download all games from KGS to your local data directory. If data is
    available, it won’t be downloaded again.**'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将所有游戏从KGS下载到您的本地数据目录。如果数据可用，则不会再次下载。**'
- en: '***4* The Sampler instance selects the specified number of games for a data
    type.**'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* Sampler实例选择指定数量的游戏用于数据类型。**'
- en: '***5* Collect all zip file names contained in the data in a list.**'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将数据中包含的所有zip文件名收集到一个列表中。**'
- en: '***6* Group all SGF file indices by zip file name.**'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将所有SGF文件索引按zip文件名分组。**'
- en: '***7* The zip files are then processed individually.**'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 然后逐个处理zip文件。**'
- en: '***8* Features and labels from each zip are then aggregated and returned.**'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 然后将每个zip的特征和标签汇总并返回。**'
- en: Note that after downloading data, you split it by using a `Sampler` instance.
    All this sampler does is make sure it randomly picks the specified number of games,
    but more importantly, that *training and test data don’t overlap in any way*.
    `Sampler` does that by splitting training and test data on a file level, by simply
    declaring games played prior to 2014 as test data and newer games as training
    data. Doing so, you make absolutely sure that no game information available in
    test data is also (partly) included in training data, which may lead to overfitting
    of your models.
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在下载数据后，您可以使用`Sampler`实例来分割它。这个采样器所做的只是确保它随机选择指定数量的比赛，但更重要的是，确保*训练和测试数据在任何方面都不重叠*。`Sampler`通过在文件级别上分割训练和测试数据来实现这一点，简单地将2014年之前玩的游戏声明为测试数据，而较新的游戏作为训练数据。这样做可以确保测试数据中可用的任何游戏信息（部分）都不会包含在训练数据中，这可能导致模型过拟合。
- en: '|  |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Splitting training and test data**'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: '**分割训练和测试数据**'
- en: The reason you split data into training and test data is to obtain reliable
    performance metrics. You train a model on training data and evaluate it on test
    data to see how well the model adapts to *previously unseen situations*, how well
    it extrapolates from what it learned in the training phase to the real world.
    Proper data collection and split is crucially important to trust the results you
    get from a model.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 您将数据分割成训练和测试数据的原因是为了获得可靠的性能指标。您在训练数据上训练一个模型，并在测试数据上评估它，以查看模型如何适应*之前未见过的情景*，如何从训练阶段学到的内容外推到现实世界。适当的数据收集和分割对于信任从模型中获得的结果至关重要。
- en: It can be tempting to just load all the data you have, shuffle it, and randomly
    split it into training and test data. Depending on the problem at hand, this naive
    approach may or may not be a good idea. If you think of Go game records, the moves
    within a single game depend on each other. Training a model on a set of moves
    that are also included in the test set can lead to the illusion of having found
    a strong model. But it may turn out that your bot won’t be as strong in practice.
    Make sure to spend time analyzing your data and find a split that makes sense.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 直接加载所有数据，打乱顺序，并将其随机分成训练数据和测试数据可能会很有吸引力。根据问题的性质，这种天真方法可能或可能不是好主意。如果您考虑围棋游戏记录，单个游戏中的动作相互依赖。在包含在测试集中的动作集上训练模型可能会导致找到强大模型的错觉。但实际中您的机器人可能并不那么强大。确保花时间分析您的数据，并找到一个有意义的分割。
- en: '|  |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'After downloading and sampling data, `load_go_data` relies essentially on helpers
    to process data: `process_zip` to read out individual zip files, and `consolidate_games`
    to group the results from each zip into one set of features and labels. Let’s
    have a look at `process_zip` next, which carries out the following steps for you:'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载和采样数据后，`load_go_data` 主要依赖于辅助程序来处理数据：`process_zip` 读取单个 zip 文件，以及 `consolidate_games`
    将每个 zip 文件的结果组合成一组特征和标签。接下来，让我们看看 `process_zip`，它为您执行以下步骤：
- en: Unzip the current file by using `unzip_data`.
  id: totrans-1022
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `unzip_data` 解压当前文件。
- en: Initialize an `Encoder` instance to encode SGF records.
  id: totrans-1023
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个 `Encoder` 实例来编码 SGF 记录。
- en: Initialize feature and label NumPy arrays of the right shape.
  id: totrans-1024
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化具有正确形状的特征和标签 NumPy 数组。
- en: Iterate through the game list and process games one by one.
  id: totrans-1025
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历游戏列表并逐个处理游戏。
- en: For each game, first apply all handicap stones.
  id: totrans-1026
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个游戏，首先应用所有让子。
- en: Then read out each move as found in the SGF record.
  id: totrans-1027
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后读取 SGF 记录中找到的每个动作。
- en: For each next move, encode the move as `label`.
  id: totrans-1028
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个下一个动作，将其编码为 `label`。
- en: Encode the current board state as `feature`.
  id: totrans-1029
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前棋盘状态编码为 `feature`。
- en: Apply the next move to the board and proceed.
  id: totrans-1030
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将下一个动作应用于棋盘并继续。
- en: Store small chunks of features and labels in the local filesystem.
  id: totrans-1031
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地文件系统中存储特征和标签的小块。
- en: Here’s how you implement the first nine of these steps in `process_zip`. Note
    that the technical utility method `unzip_data` has been omitted for brevity, but
    can be found in our GitHub repository. In [figure 7.3](#ch07fig03), you see how
    processing zipped SGF files into an encoded game state works.
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何在 `process_zip` 中实现前九个步骤的示例。请注意，为了简洁起见，省略了技术实用方法 `unzip_data`，但可以在我们的 GitHub
    仓库中找到。在 [图 7.3](#ch07fig03) 中，您可以看到将压缩 SGF 文件处理成编码棋局状态的工作方式。
- en: Figure 7.3\. The `process_zip` function. You iterate over a zip file that contains
    many SGF files. Each SGF file contains a sequence of game moves; you use those
    to reconstruct `GameState` objects. Then you use an `Encoder` object to convert
    each game state to a NumPy array.
  id: totrans-1033
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.3\. `process_zip` 函数。您遍历包含许多 SGF 文件的 zip 文件。每个 SGF 文件包含一系列游戏动作；您使用这些动作来重建
    `GameState` 对象。然后您使用 `Encoder` 对象将每个游戏状态转换为 NumPy 数组。
- en: '![](Images/07fig03_alt.jpg)'
  id: totrans-1034
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig03_alt.jpg)'
- en: Next, you can define `process_zip`.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以定义 `process_zip`。
- en: Listing 7.7\. Processing Go records stored in zip files into encoded features
    and labels
  id: totrans-1036
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.7\. 将存储在 zip 文件中的 Go 记录处理成编码特征和标签
- en: '[PRE88]'
  id: totrans-1037
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '***1* Determines the total number of moves in all games in this zip file**'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 确定该 zip 文件中所有游戏中动作的总数**'
- en: '***2* Infers the shape of features and labels from the encoder you use**'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从您使用的编码器推断特征和标签的形状**'
- en: '***3* Reads the SGF content as string, after extracting the zip file**'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 读取解压 zip 文件后的 SGF 内容作为字符串**'
- en: '***4* Infers the initial game state by applying all handicap stones**'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 通过应用所有让子来推断初始棋局状态**'
- en: '***5* Iterates over all moves in the SGF file**'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 遍历 SGF 文件中的所有动作**'
- en: '***6* Reads the coordinates of the stone to be played...**'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 读取要放置的棋子的坐标...**'
- en: '***7* ...or passes, if there is none**'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* ...如果没有，则跳过**'
- en: '***8* Encodes the current game state as features...**'
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 将当前棋局状态编码为特征...**'
- en: '***9* ...and the next move as label for the features**'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* ...并且下一个动作作为特征的标签**'
- en: '***10* Afterward, the move is applied to the board, and you proceed with the
    next one.**'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 之后，将动作应用于棋盘，并继续下一个动作。**'
- en: Note how closely the `for` loop resembles the process you sketched in [listing
    7.2](#ch07ex02), so this code should feel familiar to you. `process_zip` uses
    two helper methods that you’ll implement next. The first one is `num_total_examples`,
    which precomputes the number of moves available per zip file so that you can efficiently
    determine the size of feature and label arrays.
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`for`循环与你在[列表7.2](#ch07ex02)中概述的过程是多么相似，因此这段代码应该对你来说很熟悉。`process_zip`使用两个你将在下一个步骤中实现的辅助方法。第一个是`num_total_examples`，它预先计算每个zip文件中可用的移动次数，这样你可以有效地确定特征和标签数组的大小。
- en: Listing 7.8\. Calculating the total number of moves available in the current
    zip file
  id: totrans-1049
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8\. 计算当前zip文件中可用的总移动次数
- en: '[PRE89]'
  id: totrans-1050
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: You use the second helper method to figure out the number of handicap stones
    the current game has and apply these moves to an empty board.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用第二个辅助方法来确定当前游戏有多少让子石，并将这些移动应用于空棋盘。
- en: Listing 7.9\. Retrieving handicap stones and applying them to an empty Go board
  id: totrans-1052
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.9\. 获取让子石并将其应用于空围棋棋盘
- en: '[PRE90]'
  id: totrans-1053
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: To finish the implementation of `process_zip`, you store chunks of features
    and labels in separate files.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成`process_zip`的实现，你将特征和标签的块存储在单独的文件中。
- en: Listing 7.10\. Persisting features and labels locally in small chunks
  id: totrans-1055
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.10\. 以小块形式在本地持久化特征和标签
- en: '[PRE91]'
  id: totrans-1056
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '***1* You process features and labels in chunks of size 1024.**'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你以1024大小的块处理特征和标签。**'
- en: '***2* The current chunk is cut off from features and labels...**'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 当前块从特征和标签中切分出来...**'
- en: '***3* ... and then stored in a separate file.**'
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* ... 然后存储在单独的文件中。**'
- en: The reason you store small chunks is that NumPy arrays can become large quickly,
    and storing data in smaller files enables more flexibility later. For instance,
    you could either consolidate data for all chunks or load chunks into memory as
    needed. You’ll work with both approaches. Although the latter—dynamically loading
    batches of data as you go—is a little more intricate, consolidating data is straightforward.
    As a side note, in our implementation you potentially lose the last fraction of
    a chunk in the `while` loop, but this is insubstantial because you have more than
    enough data at your disposal.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 你存储小块的原因是NumPy数组可以迅速变得很大，将数据存储在较小的文件中可以在以后提供更多的灵活性。例如，你可以合并所有块的数据，或者根据需要加载块到内存中。你将使用这两种方法。虽然后者——在前进过程中动态加载数据批次——稍微复杂一些，但合并数据是直接的。作为旁注，在我们的实现中，你可能在`while`循环中丢失最后一块的最后一部分，但这微不足道，因为你手头有足够多的数据。
- en: Continuing with processor.py and our definition of `GoDataProcessor`, you simply
    *concatenate* all arrays into one.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用`processor.py`和我们的`GoDataProcessor`定义，你只需将所有数组连接成一个。
- en: Listing 7.11\. Consolidating individual NumPy arrays of features and labels
    into one set
  id: totrans-1062
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.11\. 将特征和标签的单独NumPy数组合并成一个集合
- en: '[PRE92]'
  id: totrans-1063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: You can test this implementation by loading features and labels for 100 games
    as follows.
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式加载100个游戏的特征和标签来测试这个实现。
- en: Listing 7.12\. Loading training data from 100 game records
  id: totrans-1065
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.12\. 从100个游戏记录中加载训练数据
- en: '[PRE93]'
  id: totrans-1066
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: These features and labels have been encoded with your `oneplane` encoder from
    [chapter 6](kindle_split_018.xhtml#ch06), meaning they have exactly the same structure.
    In particular, you can go ahead and train any of the networks you created in [chapter
    6](kindle_split_018.xhtml#ch06) with the data you just created. Don’t expect too
    much in terms of evaluation performance if you do so. Although this real-world
    game data is much better than the games generated in [chapter 6](kindle_split_018.xhtml#ch06),
    you’re now working with 19 × 19 Go data, which is much more complex than games
    played on 9 × 9 boards.
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征和标签已经使用你的`oneplane`编码器从[第6章](kindle_split_018.xhtml#ch06)中编码，这意味着它们具有完全相同的结构。特别是，你可以使用你刚刚创建的数据来训练你在[第6章](kindle_split_018.xhtml#ch06)中创建的任何网络。如果你这样做，不要对评估性能期望过高。尽管这个现实世界的游戏数据比[第6章](kindle_split_018.xhtml#ch06)中生成的游戏数据要好得多，但你现在正在处理19
    × 19的围棋数据，这比在9 × 9棋盘上玩的游戏要复杂得多。
- en: The procedure of loading a lot of smaller files into memory for consolidation
    can potentially lead to out-of-memory exceptions when loading large amounts of
    data. You’ll address this issue in the next section by using a *data generator*
    to provide just the next mini-batch of data needed for model training.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 将大量较小的文件加载到内存中进行合并的过程可能会导致在加载大量数据时出现内存不足异常。你将在下一节中通过使用*数据生成器*来提供仅用于模型训练所需的下一个迷你批次数据来解决此问题。
- en: 7.2.3\. Building a Go data generator to load data efficiently
  id: totrans-1069
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3\. 构建围棋数据生成器以有效地加载数据
- en: The KGS index you downloaded from [https://u-go.net/gamerecords/](https://u-go.net/gamerecords/)
    contains well over 170,000 games, translating into many millions of Go moves to
    predict. Loading all of these data points into a single pair of NumPy arrays will
    become increasingly difficult as you load more and more game records. Your approach
    to consolidate games is doomed to break down at some point.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: 你从[https://u-go.net/gamerecords/](https://u-go.net/gamerecords/)下载的KGS索引包含超过170,000场比赛，这转化为数百万个需要预测的围棋走法。随着你加载越来越多的游戏记录，将这些数据点全部加载到单个NumPy数组中将会变得越来越困难。你合并游戏的方法注定会在某个时刻崩溃。
- en: Instead, we suggest a smart replacement for `consolidate_games` in your `GoDataProcessor`.
    Note that in the end, all a neural network needs for training is that you feed
    it mini-batches of features and labels *one by one*. There’s no need to keep data
    in memory at all times. So, what you’re going to build next is a *generator* for
    Go data. If you know the concept of generators from Python, you’ll immediately
    recognize the pattern of what you’re building. If not, think of a generator as
    a function that efficiently provides you with just the next batch of data you
    need, when you need it.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们建议在`GoDataProcessor`中使用`consolidate_games`的智能替代方案。请注意，最终，神经网络训练所需的只是你一个接一个地给它提供特征和标签的小批量数据。没有必要始终将数据保留在内存中。因此，你接下来要构建的是Go数据的*生成器*。如果你了解Python中的生成器概念，你会立即认出你正在构建的模式。如果不了解，可以把生成器想象成一个函数，它高效地在你需要时只提供你需要的下一个数据批次。
- en: To start, let’s initialize a `DataGenerator`. Put this code into generator.py
    inside the data module. You initialize such a generator by providing a local data_directory
    and samples as provided by your `Sampler` in `GoDataProcessor`.
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们初始化一个`DataGenerator`。将此代码放入`data`模块内的`generator.py`文件中。你可以通过提供一个本地数据目录和由`GoDataProcessor`中的`Sampler`提供的样本来初始化这样的生成器。
- en: Listing 7.13\. The signature of a Go data generator
  id: totrans-1073
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.13。Go数据生成器的签名
- en: '[PRE94]'
  id: totrans-1074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '***1* Your generator has access to a set of files that you sampled earlier.**'
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你的生成器可以访问你之前采样的文件集。**'
- en: '***2* Depending on the application, you may need to know how many examples
    you have.**'
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 根据应用的不同，你可能需要知道你有多少个示例。**'
- en: Next, you’ll implement a private `_generate` method that creates and returns
    batches of data. This method follows a similar overall logic as `consolidate_games`,
    with one important difference. Whereas previously you created a big NumPy array
    for both features and labels, you now only return, or `yield`, the next batch
    of data.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将实现一个私有`_generate`方法，该方法创建并返回数据批次。此方法遵循与`consolidate_games`类似的总体逻辑，但有一个重要区别。以前你为特征和标签创建了一个大的NumPy数组，而现在你只返回或`yield`下一个数据批次。
- en: Listing 7.14\. Private method to generate and yield the next batch of Go data
  id: totrans-1078
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.14。生成并产生下一个Go数据批次的私有方法
- en: '[PRE95]'
  id: totrans-1079
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '***1* You return, or yield, batches of data as you go.**'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你在过程中返回或`yield`数据批次。**'
- en: All that’s missing from your generator is a method to return a generator. Having
    a generator, you can explicitly call `next()` on it to generate batches of data
    for your use case. This is done as follows.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 你所缺少的只是生成器的一个返回生成器的方法。拥有一个生成器后，你可以显式地调用`next()`来为你的用例生成数据批次。这可以通过以下方式完成。
- en: Listing 7.15\. Calling the `generate` method to obtain a generator for model
    training
  id: totrans-1082
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.15。调用`generate`方法以获得用于模型训练的生成器
- en: '[PRE96]'
  id: totrans-1083
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Before we can show you how to use such a generator to train a neural network,
    we have to explain how to incorporate this concept into your `GoDataProcessor`.
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们向你展示如何使用这样的生成器来训练神经网络之前，我们必须解释如何将这个概念融入你的`GoDataProcessor`。
- en: 7.2.4\. Parallel Go data processing and generators
  id: totrans-1085
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.4。并行Go数据处理和生成器
- en: You may have noticed that loading just 100 game records in [listing 7.3](#ch07ex03)
    feels a little slower than you may have expected. Although naturally you need
    to download the data first, it’s the processing itself that’s relatively slow.
    Recall from your implementation that you process zip files *sequentially*. After
    you finish a file, you proceed to the next. But if you look closely, the processing
    of Go data as we presented it is what you’d call *embarrassingly parallel*. It
    takes just a little effort to process the zip files in parallel by distributing
    workload across all CPUs in your computer; for instance, using Python’s multiprocessing
    library.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，在[列表7.3](#ch07ex03)中仅加载100场比赛记录比您预期的要慢一些。虽然您自然需要首先下载数据，但处理本身相对较慢。回想一下您的实现，您是*顺序地*处理zip文件的。在您完成一个文件后，您继续处理下一个。但如果您仔细观察，我们展示的围棋数据处理方式可以称为*令人尴尬的并行*。只需一点努力，就可以通过在您的计算机的所有CPU之间分配工作负载来并行处理zip文件；例如，使用Python的multiprocessing库。
- en: In our GitHub repository, you’ll find a parallel implementation of `GoDataProcessor`
    in the data module in parallel_processor.py. If you’re interested in how this
    works in detail, we encourage you to go through the implementation provided there.
    The reason we omit the details here is that although the speedup of parallelization
    is of immediate benefit to you, the implementation details make the code quite
    a bit harder to read.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的GitHub仓库中，您将在`parallel_processor.py`的数据模块中找到一个`GoDataProcessor`的并行实现。如果您对如何详细工作感兴趣，我们鼓励您查看那里提供的实现。我们在这里省略细节的原因是，尽管并行化的加速对您来说有即时的好处，但实现细节使得代码难以阅读。
- en: Another benefit that you get from using the parallel version of `GoDataProcessor`
    is that you can optionally use your `DataGenerator` with it, to return a generator
    instead of data.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`GoDataProcessor`的并行版本带来的另一个好处是，您可以选择使用它来与您的`DataGenerator`一起使用，以返回一个生成器而不是数据。
- en: Listing 7.16\. The parallel version of `load_go_data` can optionally return
    a generator
  id: totrans-1089
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.16\. `load_go_data`的并行版本可以可选地返回一个生成器
- en: '[PRE97]'
  id: totrans-1090
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '***1* Map workload to CPUs.**'
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将工作负载映射到CPU。**'
- en: '***2* Either returns a Go data generator...**'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 要么返回一个围棋数据生成器...**'
- en: '***3* ...or returns consolidated data as before**'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* ...或者像以前一样返回合并后的数据**'
- en: With the exception of the `use_generator` flag in the parallel extension, both
    `GoDataProcessor` versions share the same interface. Through `GoDataProcessor`
    from dlgo .data.parallel_processor, you can now use a generator to provide Go
    data as follows.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 除了并行扩展中的`use_generator`标志外，`GoDataProcessor`的两个版本共享相同的接口。通过`GoDataProcessor`从`dlgo.data.parallel_processor`，您现在可以使用生成器提供围棋数据，如下所示。
- en: Listing 7.17\. Loading training data from 100 game records
  id: totrans-1095
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.17\. 从100场比赛记录中加载数据
- en: '[PRE98]'
  id: totrans-1096
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Initially loading the data still takes time, although it should speed up proportionally
    to the number of processors you have in your machine. After the generator has
    been created, calling `next()` returns batches instantly. Also, this way, you
    don’t run into trouble with exceeding memory.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 初始加载数据仍然需要时间，尽管它应该与您机器中处理器的数量成比例地加速。一旦创建生成器，调用`next()`会立即返回批次。此外，这种方式，您也不会遇到超出内存的问题。
- en: 7.3\. Training a deep-learning model on human game-play data
  id: totrans-1098
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 在人类游戏数据上训练深度学习模型
- en: Now that you have access to high-dan Go data and processed it to fit a move-prediction
    model, let’s connect the dots and build a deep neural network for this data. In
    our GitHub repository, you’ll find a module called *networks* within our dlgo
    package that you’ll use to provide example architectures of neural networks that
    you can use as baselines to build strong move-prediction models. For instance,
    you’ll find three convolutional neural networks of varying complexity in the networks
    module, called small.py, medium.py, and large.py. Each of these files contains
    a `layers` function that returns a list of layers that you can add to a sequential
    Keras model.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经可以访问高段位的围棋数据，并且已经将其处理以适应移动预测模型，让我们连接这些点，并为此数据构建一个深度神经网络。在我们的GitHub仓库中，您将在我们的`dlgo`包中找到一个名为`*networks*`的模块，您将使用它来提供神经网络示例架构，您可以使用这些架构作为构建强大移动预测模型的基线。例如，您将在`networks`模块中找到三个不同复杂性的卷积神经网络，分别称为`small.py`、`medium.py`和`large.py`。这些文件中的每个文件都包含一个`layers`函数，该函数返回一个可以添加到顺序Keras模型的层列表。
- en: You’ll build a convolutional neural network consisting of four convolutional
    layers, followed by a final dense layer, all with ReLU activations. On top of
    that, you’ll use a new utility layer right before each convolutional layer—a `ZeroPadding2D`
    layer. Zero padding is an operation in which the input features are *padded* with
    0s. Let’s say you use your one-plane encoder from [chapter 6](kindle_split_018.xhtml#ch06)
    to encode the board as a 19 × 19 matrix. If you specify a padding of 2, that means
    you add two columns of 0s to the left and right, as well as two rows of 0s to
    the top and bottom of that matrix, resulting in an enlarged 23 × 23 matrix. You
    use zero padding in this situation to artificially increase the input of a convolutional
    layer, so that the convolution operation doesn’t shrink the image by too much.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: '你将构建一个由四个卷积层组成、后面跟着一个最终的全连接层、所有层都使用ReLU激活的卷积神经网络。在此之上，你将在每个卷积层之前使用一个新的实用层——一个`ZeroPadding2D`层。零填充是一种操作，其中输入特征被*填充*为0。假设你使用[第6章](kindle_split_018.xhtml#ch06)中的单平面编码器将棋盘编码为一个19
    × 19的矩阵。如果你指定填充为2，这意味着你在矩阵的左侧和右侧各添加两列0，以及在矩阵的顶部和底部各添加两行0，从而得到一个扩大的23 × 23矩阵。在这种情况下，你使用零填充来人为地增加卷积层的输入，这样卷积操作就不会过度缩小图像。 '
- en: 'Before we show you the code, we have to discuss a small technicality. Recall
    that both input and output of convolutional layers are four-dimensional: we provide
    a mini-batch of a number of filters that are two-dimensional each (namely, they
    have width and height). The *order* in which these four dimensions (mini-batch
    size, number of filters, width, and height) are represented is a matter of convention,
    and you mainly find two such orderings in practice. Note that filters are also
    often referred to as channels (`C`), and the mini-batch size is also called number
    (`N`) of examples. Moreover, you can use shorthand for width (`W`) and height
    (`H`). With this notation, the two predominant orderings are NWHC and NCWH. In
    Keras, this ordering is called `data_format`, and NWHC is called `channels_last`,
    and NCWH `channels_first`, for somewhat obvious reasons. Now, the way you built
    your first Go board encoder, the one-plane encoder, is in *channels first* convention
    (an encoded board has shape 1,19,19, meaning the single encoded plane comes *first*).
    That means you have to provide `data_format=channels_first` as an argument to
    all convolutional layers. Let’s have a look at what the model from small.py looks
    like.'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们向您展示代码之前，我们必须讨论一个小技术问题。回想一下，卷积层的输入和输出都是四维的：我们提供一个包含多个二维滤波器的小批量（即，它们有宽度和高度）。这四个维度（小批量大小、滤波器数量、宽度和高度）的表示顺序是一个约定问题，在实践中你主要会找到两种这样的顺序。请注意，滤波器也经常被称为通道（`C`），小批量大小也称为示例数量（`N`）。此外，你可以使用缩写表示宽度（`W`）和高度（`H`）。用这种表示法，两种主要的顺序是NWHC和NCWH。在Keras中，这种顺序被称为`data_format`，NWHC被称为`channels_last`，而NCWH被称为`channels_first`，这是出于某些明显的原因。现在，你构建的第一个围棋棋盘编码器，即单平面编码器，遵循*channels
    first*约定（一个编码的棋盘形状为1,19,19，这意味着单个编码平面是*首先*）。这意味着你必须将`data_format=channels_first`作为参数提供给所有卷积层。让我们看看small.py中的模型看起来像什么。
- en: Listing 7.18\. Specifying layers for a small convolutional network for Go move
    prediction
  id: totrans-1102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.18\. 为围棋走法预测构建的小卷积网络指定层
- en: '[PRE99]'
  id: totrans-1103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '***1* Use zero padding layers to enlarge input images.**'
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用零填充层来扩大输入图像。**'
- en: '***2* By using channels_first, you specify that the input plane dimension for
    your features comes first.**'
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 通过使用channels_first，你指定了你的特征输入平面的维度是第一个。**'
- en: The `layers` function returns a list of Keras layers that you can add one by
    one to a `Sequential` model. Using these layers, you can now build an application
    that carries out the first five steps from the overview in [figure 7.1](#ch07fig01)—an
    application that downloads, extracts, and encodes Go data and uses it to train
    a neural network. For the training part, you’ll use the *data generator* you built.
    But first, let’s import some of the essential components of your growing Go machine-learning
    library. You need a Go data processor, an encoder, and a neural network architecture
    to build this application.
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: '`layers`函数返回一个Keras层的列表，你可以逐个将这些层添加到一个`Sequential`模型中。使用这些层，你现在可以构建一个应用程序，执行概述中的前五步[图7.1](#ch07fig01)——一个下载、提取和编码围棋数据并使用它来训练神经网络的程序。在训练部分，你将使用你构建的*数据生成器*。但首先，让我们导入你不断增长的围棋机器学习库的一些基本组件。你需要一个围棋数据处理器、一个编码器和构建此应用程序所需的神经网络架构。'
- en: Listing 7.19\. Core imports for building a neural network for Go data
  id: totrans-1107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.19\. 构建围棋数据神经网络的核心导入
- en: '[PRE100]'
  id: totrans-1108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '***1* With model checkpoints, you can store progress for time-consuming experiments.**'
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用模型检查点，你可以存储耗时实验的进度。**'
- en: 'The last of these imports provides a handy Keras tool called `ModelCheckpoint`.
    Because you have access to a large amount of data for training, completing a full
    run of training a model for some epochs can take a few hours or even days. If
    such an experiment fails for some reason, you better have a backup in place. And
    that’s precisely what model checkpoints do for you: they persist a snapshot of
    your model after each epoch of training. Even if something fails, you can resume
    training from the last checkpoint.'
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个导入提供了一个方便的Keras工具，称为`ModelCheckpoint`。由于你有大量的数据用于训练，完成某些epoch的完整模型训练可能需要几个小时甚至几天。如果这样的实验由于某种原因失败，你最好有一个备份。这正是模型检查点为你做的事情：在每个epoch的训练后持久化模型的快照。即使出现问题，你也可以从最后一个检查点恢复训练。
- en: Next, let’s define training and test data. To do so, you first initialize a
    `OnePlaneEncoder` that you use to create a `GoDataProcessor`. With this processor,
    you can instantiate a training and a testing data generator that you’ll use with
    a Keras model.
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义训练和测试数据。为此，你首先初始化一个`OnePlaneEncoder`，你用它来创建一个`GoDataProcessor`。使用这个处理器，你可以实例化一个训练和测试数据生成器，你将使用这个生成器与Keras模型一起使用。
- en: Listing 7.20\. Creating training and test generators
  id: totrans-1112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.20\. 创建训练和测试生成器
- en: '[PRE101]'
  id: totrans-1113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '***1* First you create an encoder of board size.**'
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 首先创建一个棋盘大小的编码器。**'
- en: '***2* Then you initialize a Go data processor with it.**'
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 然后使用它初始化一个Go数据处理器。**'
- en: '***3* From the processor, you create two data generators, for training and
    testing.**'
  id: totrans-1116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 从处理器中，你创建了两个数据生成器，用于训练和测试。**'
- en: As a next step, you define a neural network with Keras by using the `layers`
    function from dlgo.networks.small. You add the layers of this small network one
    by one to a new sequential network, and then finish off by adding a final `Dense`
    layer with softmax activation. You then compile this model with categorical cross-entropy
    loss and train it with SGD.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，你通过使用dlgo.networks.small中的`layers`函数，用Keras定义一个神经网络。你逐个将这个小网络中的层添加到一个新的序列网络中，然后通过添加一个具有softmax激活的最终`Dense`层来完成。然后你使用分类交叉熵损失编译这个模型，并用SGD进行训练。
- en: Listing 7.21\. Defining a Keras model from your small layer architecture
  id: totrans-1118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.21\. 从你的小层架构定义Keras模型
- en: '[PRE102]'
  id: totrans-1119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: To train a Keras model with generators works a little bit differently from training
    with data sets. Instead of calling `fit` on your model, you now need to call `fit_generator`,
    and you also replace `evaluate` with `evaluate_generator`. Moreover, the signatures
    of these methods are slightly different from what you’ve seen before. Using `fit_generator`
    works by specifying a `generator`, the number of `epochs`, and the number of training
    steps per epoch, which you provide with `steps_per_epoch`. These three arguments
    provide the bare minimum to train a model. You also want to validate your training
    process on test data. For this, you provide `validation_data` with your test data
    generator and specify the number of validation steps per epoch as `validation_steps`.
    Lastly, you add a `callback` to your model. Callbacks allow you to track and return
    additional information during the training process. You use callbacks here to
    hook in the `ModelCheckpoint` utility to store the Keras model after each epoch.
    As an example, you train a model for five epochs on a batch size of 128.
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成器训练Keras模型与使用数据集训练略有不同。现在你不需要在模型上调用`fit`，而是需要调用`fit_generator`，并且用`evaluate_generator`替换`evaluate`。此外，这些方法的签名与之前看到的不同。使用`fit_generator`是通过指定一个`generator`、`epochs`的数量以及每个epoch的训练步数来实现的，这些步数通过`steps_per_epoch`提供。这三个参数提供了训练模型所需的最小信息。你还需要在测试数据上验证你的训练过程。为此，你提供`validation_data`作为你的测试数据生成器，并指定每个epoch的验证步数作为`validation_steps`。最后，你向模型添加一个`callback`。回调允许你在训练过程中跟踪和返回额外的信息。在这里，你使用回调将`ModelCheckpoint`实用工具钩入，以便在每个epoch后存储Keras模型。作为一个例子，你在一个批量为128的情况下对一个模型进行了五个epoch的训练。
- en: Listing 7.22\. Fitting and evaluating Keras models with generators
  id: totrans-1121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.22\. 使用生成器拟合和评估Keras模型
- en: '[PRE103]'
  id: totrans-1122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '***1* You specify a training data generator for your batch size...**'
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你指定一个训练数据生成器用于你的批量大小...**'
- en: '***2* ...and the number of training steps per epoch you execute.**'
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* ...以及每个epoch执行的训练步数。**'
- en: '***3* An additional generator is used for validation...**'
  id: totrans-1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用额外的生成器进行验证...**'
- en: '***4* ...which also needs a number of steps.**'
  id: totrans-1126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* ...这同样需要一个步数。**'
- en: '***5* After each epoch, you persist a checkpoint of the model.**'
  id: totrans-1127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 每个epoch结束后，你将保存模型的检查点。**'
- en: '***6* For evaluation, you also specify a generator and the number of steps.**'
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 对于评估，你还需要指定生成器和步数。**'
- en: Note that if you run this code yourself, you should be aware of the time it
    may take to complete this experiment. If you run this on a CPU, training an epoch
    might take a few hours. As it happens, the math used in machine learning has a
    lot in common with the math used in computer graphics. So in some cases, you can
    move your neural network computation onto your GPU and get a big speedup. Using
    a GPU for computation will massively speed up computation, usually by one or two
    orders of magnitude for convolutional neural networks. TensorFlow has extensive
    support for moving computation onto certain GPUs, if your machine has suitable
    drivers available.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你亲自运行这段代码，你应该意识到完成这个实验可能需要的时间。如果你在CPU上运行，训练一个epoch可能需要几个小时。实际上，机器学习中使用的数学与计算机图形学中使用的数学有很多相似之处。因此，在某些情况下，你可以将你的神经网络计算移动到GPU上，从而获得很大的速度提升。使用GPU进行计算将极大地加快计算速度，通常对于卷积神经网络来说，速度提升可达一个或两个数量级。如果你的机器上有合适的驱动程序可用，TensorFlow提供了广泛的对于将计算移动到某些GPU的支持。
- en: '|  |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to use a GPU for machine learning, an NVIDIA chip with a Windows
    or Linux OS is the best-supported combination. Other combinations are possible,
    but you may spend a lot of time fiddling with drivers.
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要为机器学习使用GPU，配备Windows或Linux操作系统的NVIDIA芯片是最好的组合。其他组合也是可能的，但你可能需要花费大量时间调整驱动程序。
- en: '|  |'
  id: totrans-1133
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'In case you don’t want to try this yourself, or just don’t want to do this
    right now, we’ve precomputed this model for you. Have a look at our GitHub repository
    to see the five checkpoint models stored in `checkpoints`, one for each completed
    epoch. Here’s the output of that training run (computed on an old CPU on a laptop,
    to encourage you to get a fast GPU right away):'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不打算亲自尝试，或者只是现在不想做，我们已经为你预先计算了这个模型。查看我们的GitHub仓库，看看存储在`checkpoints`中的五个检查点模型，每个完成的epoch一个。以下是那次训练运行的输出（在一个旧CPU上计算，以鼓励你立即购买一个快速的GPU）：
- en: '[PRE104]'
  id: totrans-1135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'As you can see, after three epochs, you’ve reached 98% accuracy on training
    and 84% on test data. This is a massive improvement over the models you computed
    in [chapter 6](kindle_split_018.xhtml#ch06)! It seems that training a larger network
    on real data paid off: your network learned to predict moves from 100 games almost
    perfectly, but generalizes reasonably well. You can be more than happy with the
    84% validation accuracy. On the other hand, 100 games’ worth of moves is still
    a tiny data set, and you don’t know yet how well you’d do on a much larger corpus
    of games. After all, your goal is to build a strong Go bot that can compete with
    strong opponents, not to crush a toy data set.'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，经过三个epoch后，你在训练数据上达到了98%的准确率，在测试数据上达到了84%。这比你在[第6章](kindle_split_018.xhtml#ch06)中计算出的模型有了巨大的改进！似乎在真实数据上训练更大的网络是值得的：你的网络学会了几乎完美地预测100场比赛的走法，但泛化能力相当好。你可以对84%的验证准确率感到非常满意。另一方面，100场比赛的走法仍然是一个很小的数据集，你还不清楚在一个更大的游戏数据集上你会做得如何。毕竟，你的目标是构建一个能够与强大对手竞争的强大围棋机器人，而不是仅仅压倒一个玩具数据集。
- en: To build a really strong opponent, you need to work with better Go data encoders
    next. Your one-plane encoder from [chapter 6](kindle_split_018.xhtml#ch06) is
    a good first guess, but it doesn’t capture the complexity that you’re dealing
    with. In [section 7.4](#ch07lev1sec4), you’ll learn about two more-sophisticated
    encoders that will boost your training performance.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个真正强大的对手，你需要与更好的围棋数据编码器合作。你从[第6章](kindle_split_018.xhtml#ch06)中的单平面编码器是一个好的起点，但它无法捕捉到你正在处理的复杂性。在[第7.4节](#ch07lev1sec4)中，你将了解两种更复杂的编码器，这些编码器将提高你的训练性能。
- en: 7.4\. Building more-realistic Go data encoders
  id: totrans-1138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4. 构建更真实的围棋数据编码器
- en: '[Chapters 2](kindle_split_013.xhtml#ch02) and [3](kindle_split_014.xhtml#ch03)
    covered the ko rule in Go quite a bit. Recall that this rule exists to prevent
    infinite loops in games: you can’t play a stone that leads to a situation previously
    on the board. If we give you a random Go board position and you have to decide
    whether there’s a ko going on, you’d have to guess. There’s no way of knowing
    without having seen the sequence leading up to that position. In particular, your
    one-plane encoder, which encoded black stones as –1, white ones as 1, and empty
    positions as 0, can’t possibly learn anything about ko. This is just one example,
    but it goes to show that the `OnePlaneEncoder` you built in [chapter 6](kindle_split_018.xhtml#ch06)
    is a little too simplistic to capture everything you need to build a strong Go
    bot.'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 2 章](kindle_split_013.xhtml#ch02) 和 [第 3 章](kindle_split_014.xhtml#ch03)
    在围棋中相当详细地介绍了劫的规则。回想一下，这个规则存在是为了防止游戏中的无限循环：你不能下出一个会导致棋盘上之前情况的棋子。如果我们给你一个随机的围棋棋盘位置，你必须决定是否正在进行劫，你就必须猜测。没有看到导致该位置的前一个序列，你无法知道。特别是，你的单平面编码器，它将黑棋编码为
    -1，白棋编码为 1，空位置编码为 0，不可能学习到关于劫的任何东西。这只是例子之一，但它表明你在[第 6 章](kindle_split_018.xhtml#ch06)中构建的
    `OnePlaneEncoder` 稍显简单，无法捕捉到构建强大的围棋机器人所需的所有内容。'
- en: 'In this section, we’ll provide you with two more elaborate encoders that led
    to relatively strong move-prediction performance in the literature. The first
    one we call `SevenPlaneEncoder`, which consists of the following seven feature
    planes. Each plane is a 19 × 19 matrix and describes a different set of features:'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为您提供两个更详细的编码器，这些编码器在文献中导致了相对较强的移动预测性能。第一个我们称之为 `SevenPlaneEncoder`，它由以下七个特征平面组成。每个平面是一个
    19 × 19 矩阵，描述了一组不同的特征：
- en: The first plane has a 1 for every *white* stone that has precisely *one* liberty,
    and 0s otherwise.
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个平面对于每个恰好有一个**自由**的**白色**棋子有一个 1，否则为 0。
- en: The second and third feature planes have a 1 for white stones with two or at
    least three liberties, respectively.
  id: totrans-1142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二和第三个特征平面分别对白色棋子有两个或至少三个自由度有一个 1。
- en: The fourth to sixth planes do the same for black stones; they encode black stones
    with one, two, or at least three liberties.
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四到第六个平面对黑棋做同样的处理；它们用一、二或至少三个自由度来编码黑棋。
- en: The last feature plane marks points that can’t be played because of ko with
    a 1.
  id: totrans-1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个特征平面用 1 标记由于劫而不能落子的点。
- en: Apart from explicitly encoding the concepts of ko, with this set of features
    you also model liberties and distinguish between black and white stones. Stones
    with just one liberty have extra tactical significance, because they’re at risk
    of getting captured on the next turn. (Go players say that a stone with just one
    liberty is in *atari*.) Because the model can “see” this property directly, it’s
    easier for it to pick up on how that affects game play. By creating planes for
    concepts such as ko and the number of liberties, you give a hint to the model
    that these concepts are important, without having to explain how or why they’re
    important.
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 除了明确编码**劫**的概念外，这组特征还模拟了自由度，并区分了黑白棋子。只有一个自由度的棋子具有额外的战术意义，因为它们在下一次回合中可能会被捕获。（围棋玩家说，只有一个自由度的棋子处于**劫**中。）因为模型可以直接“看到”这个属性，所以它更容易捕捉到这对游戏的影响。通过为诸如劫和自由度数量等概念创建平面，您向模型暗示这些概念很重要，而无需解释它们为什么或如何重要。
- en: Let’s see how you can implement this by extending the base `Encoder` from the
    encoders module. Save the following code in sevenplane.py.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过扩展编码器模块中的基本 `Encoder` 来实现这一点。将以下代码保存到 sevenplane.py 中。
- en: Listing 7.23\. Initializing a simple seven-plane encoder
  id: totrans-1147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.23\. 初始化一个简单的七平面编码器
- en: '[PRE105]'
  id: totrans-1148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: The interesting part is the encoding of the board position, which is done as
    follows.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的部分是编码棋盘位置，它如下进行。
- en: Listing 7.24\. Encoding game state with a `SevenPlaneEncoder`
  id: totrans-1150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.24\. 使用 `SevenPlaneEncoder` 编码游戏状态
- en: '[PRE106]'
  id: totrans-1151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '***1* Encoding moves prohibited by the ko rule**'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 编码由劫规则禁止的移动**'
- en: '***2* Encoding black and white stones with 1, 2, or more liberties**'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* 使用 1、2 或更多自由来编码黑白棋子**'
- en: To finish this definition, you also need to implement a few convenience methods,
    to suffice the `Encoder` interface.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个定义，您还需要实现一些便利方法，以满足 `Encoder` 接口。
- en: Listing 7.25\. Implementing all other `Encoder` methods for your seven-plane
    encoder
  id: totrans-1155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.25\. 为你的七平面编码器实现所有其他 `Encoder` 方法
- en: '[PRE107]'
  id: totrans-1156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Another encoder that we’ll discuss here, and point you to the code in GitHub,
    is an encoder with 11 feature planes that’s similar to `SevenPlaneEncoder`. In
    this encoder, called `SimpleEncoder,` which you can find under simple.py in the
    encoders module in GitHub, you use the following feature planes:'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的另一个编码器，并将指向GitHub中的代码，是一个具有11个特征平面的编码器，类似于`SevenPlaneEncoder`。在这个被称为`SimpleEncoder`的编码器中，你可以在GitHub的encoders模块下的simple.py中找到，你使用以下特征平面：
- en: The first four feature planes describe black stones with one, two, three, or
    four liberties.
  id: totrans-1158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前四个特征平面描述了带有一个、两个、三个或四个眼位的黑子。
- en: The second four planes describe white stones with one, two, three, or four liberties.
  id: totrans-1159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个四个平面描述了带有一个、两个、三个或四个眼位的白子。
- en: The ninth plane is set to 1 if it’s black’s turn, and the tenth if it’s white’s.
  id: totrans-1160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第九个平面在黑子轮到时设置为1，第十个平面在白子轮到时设置为1。
- en: The last feature plane is again reserved for indicating ko.
  id: totrans-1161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个特征平面再次保留用于指示劫争。
- en: This encoder with 11 planes is close to the last one, but is more explicit about
    whose turn it is and more specific about the number of liberties a stone has.
    Both are great encoders that will lead to notable improvements in model performance.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具有11个平面的编码器接近上一个编码器，但更明确地指出了轮到谁走棋，以及一个子有多少眼位。这两个都是优秀的编码器，将导致模型性能的显著提升。
- en: 'Throughout [chapters 5](kindle_split_017.xhtml#ch05) and [6](kindle_split_018.xhtml#ch06),
    you learned about many techniques that improve your deep-learning models, but
    one ingredient remained the same for all experiments: you used stochastic gradient
    descent as the optimizer. Although SGD provides a great baseline, in the next
    section we’ll teach you about *Adagrad* and *Adadelta*, two optimizers that your
    training process will greatly benefit from.'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](kindle_split_017.xhtml#ch05)和[第6章](kindle_split_018.xhtml#ch06)中，你学习了许多提高你的深度学习模型的技术，但所有实验中都有一个共同的成分：你使用了随机梯度下降作为优化器。尽管SGD提供了一个很好的基线，但在下一节中，我们将教你关于*Adagrad*和*Adadelta*，这两个优化器将极大地提高你的训练过程。
- en: 7.5\. Training efficiently with adaptive gradients
  id: totrans-1164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5. 使用自适应梯度高效训练
- en: To further improve performance of your Go move-prediction models, we’ll introduce
    one last set of tools in this chapter—optimizers other than stochastic gradient
    descent. Recall from [chapter 5](kindle_split_017.xhtml#ch05) that SGD has a fairly
    simplistic update rule. If for a parameter *W* you receive a backpropagation error
    of Δ*W* and you have a learning rate of α specified, then updating this parameter
    with SGD simply means computing *W* – αΔ*W*.
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高你的围棋走法预测模型的性能，我们将在本章介绍最后一组工具——除了随机梯度下降之外的优化器。回想一下[第5章](kindle_split_017.xhtml#ch05)，SGD有一个相当简单的更新规则。对于一个参数*W*，如果你收到一个反向传播误差Δ*W*，并且你有一个学习率α被指定，那么使用SGD更新这个参数简单地说就是计算*W*
    – αΔ*W*。
- en: In many cases, this update rule can lead to good results, but a few drawbacks
    exist as well. To address them, you can use many excellent extensions to plain
    SGD.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，这个更新规则可以带来良好的结果，但也存在一些缺点。为了解决这些问题，你可以使用许多优秀的扩展来改进普通的SGD。
- en: 7.5.1\. Decay and momentum in SGD
  id: totrans-1167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.1. SGD中的衰减和动量
- en: For instance, a widely used idea is to let the learning rate *decay* over time;
    with every update step you take, the learning rate becomes smaller. This technique
    usually works well, because in the beginning your network hasn’t learned anything
    yet, and big update steps might make sense to get closer to a minimum of the loss
    function. But after the training process has reached a certain level, you should
    make your updates smaller and make only appropriate refinements to the learning
    process that don’t spoil progress. Usually, you specify learning rate decay by
    a *decay rate*, a percentage by which you’ll decrease the next step.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个广泛使用的方法是让学习率*衰减*随时间变化；随着你采取的每个更新步骤，学习率会变得更小。这种技术通常效果很好，因为一开始你的网络还没有学到任何东西，大的更新步骤可能有助于接近损失函数的最小值。但是，在训练过程达到一定水平后，你应该减小更新，并对学习过程进行适当的细化，以避免破坏进度。通常，你通过一个*衰减率*来指定学习率衰减，这是一个百分比，表示你将减少下一步的值。
- en: 'Another popular technique is that of *momentum*, in which a fraction of the
    last update step is added to the current one. For instance, if *W* is a parameter
    vector that you want to update, *]W* is the current gradient computed for *W*,
    and if the last update you used was *U,* then the next update step will be as
    follows:'
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的技术是**动量**，其中将上一步更新的一部分加到当前步骤中。例如，如果*W*是你想要更新的参数向量，*]W*是针对*W*计算出的当前梯度，并且如果你上一次使用的更新是*U*，那么下一次更新步骤将如下所示：
- en: '| *W* ← *W* – α(γ*U* + (1 + γ)∂*W*) |'
  id: totrans-1170
  prefs: []
  type: TYPE_TB
  zh: '| *W* ← *W* – α(γ*U* + (1 + γ)∂*W*) |'
- en: This fraction *g* you keep from the last update is called the *momentum term*.
    If both gradient terms point in roughly the same direction, your next update step
    gets reinforced (receives momentum). If the gradients point in opposite directions,
    they cancel each other out and the gradient gets dampened. The technique is called
    *momentum* because of the similarity of the physical concept by the same name.
    Think about your loss function as a surface and the parameters lying on that surface
    as a ball. Then a parameter update describes movement of the ball. Because you’re
    doing gradient descent, you can even think of this as a ball rolling down the
    surface, by receiving movements one by one. If the last few (gradient) steps all
    point in the same general direction, the ball will pick up speed and reach its
    destination, the minimum of the surface, quicker. The momentum technique exploits
    this analogy.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个从上次更新中保留的分数 *g* 被称为 *动量项*。如果两个梯度项大致指向同一方向，你的下一个更新步骤会得到加强（获得动量）。如果梯度项指向相反方向，它们会相互抵消，梯度会减弱。这种技术被称为
    *动量*，因为它与同名物理概念的相似性。将你的损失函数视为一个表面，而该表面上的参数作为一个球。那么参数更新描述了球体的运动。因为你在做梯度下降，你可以甚至将这视为球体沿着表面滚动，逐个接收运动。如果最后几个（梯度）步骤都指向大致相同的方向，球体会加速并更快地到达目的地，即表面的最小值。动量技术利用了这种类比。
- en: If you want to use decay, momentum, or both in SGD with Keras, it’s as simple
    as providing the respective rates to an `SGD` instance. Let’s say you want SGD
    with a learning rate of 0.1, a 1% decay rate, and 90% momentum; you’d do the following.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在 Keras 中的 SGD 中使用衰减、动量或两者，只需向 `SGD` 实例提供相应的速率即可。假设你想要一个学习率为 0.1、衰减率为 1%、动量为
    90% 的 SGD；你会这样做。
- en: Listing 7.26\. Initializing SGD in Keras with momentum and learning rate decay
  id: totrans-1173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.26\. 在 Keras 中使用动量和学习率衰减初始化 SGD
- en: '[PRE108]'
  id: totrans-1174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 7.5.2\. Optimizing neural networks with Adagrad
  id: totrans-1175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.2\. 使用 Adagrad 优化神经网络
- en: Both learning rate decay and momentum do a good job at refining plain SGD, but
    a few weaknesses still remain. For instance, if you think about the Go board,
    professionals will almost exclusively play their first few moves on the third
    to fifth lines of the board, but, without exception, never on the first or second.
    In the endgame, the situation is somewhat reversed, in that many of the last moves
    happen at the border of the board. In all deep-learning models you worked with
    so far, the last layer was a dense layer of board size (here 19 × 19). Each neuron
    of this layer corresponds to a position on the board. If you use SGD, with or
    without momentum or decay, *the same learning rate is used for each of these neurons.*
    This can be dangerous. Maybe you did a poor job at shuffling the training data,
    and the learning rate has decayed so much that endgame moves on the first and
    second line don’t get any significant updates anymore—meaning, no learning. In
    general, you want to make sure that infrequently observed patterns still get large-enough
    updates, while frequent patterns receive smaller and smaller updates.
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率衰减和动量在改进普通的 SGD 方面都做得很好，但仍然存在一些弱点。例如，如果你考虑围棋盘，专业人士几乎只在第三到第五行上玩他们的前几步棋，但无一例外，决不在第一或第二行上。在残局中，情况有所逆转，因为在棋盘边缘发生了很多最后一步棋。在迄今为止你使用的所有深度学习模型中，最后一层是一个大小为
    (这里 19 × 19) 的密集层。这个层的每个神经元对应于棋盘上的一个位置。如果你使用 SGD，无论是否有动量或衰减，*每个这些神经元都使用相同的学习率*。这可能很危险。也许你在洗牌训练数据时做得不好，学习率已经衰减得太多，以至于第一和第二行的残局移动不再得到任何显著更新——这意味着，没有学习。一般来说，你想要确保不常观察到的模式仍然得到足够大的更新，而频繁的模式则接收越来越小的更新。
- en: 'To address the problem caused by setting global learning rates, you can use
    techniques using *adaptive* gradient methods. We’ll show you two of these methods:
    *Adagrad* and *Adadelta*.'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决设置全局学习率引起的问题，你可以使用使用 *自适应* 梯度方法的技巧。我们将向你展示这两种方法：*Adagrad* 和 *Adadelta*。
- en: 'In Adagrad, there’s no global learning rate. You *adapt the learning rate per
    parameter*. Adagrad works pretty well when you have a lot of data and patterns
    in the data can be found only rarely. Both of these criteria apply to our situation:
    you have a lot of data, and professional Go game play is so complex that certain
    move combinations occur infrequently in your data set, although they’re considered
    standard play by professionals.'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Adagrad 中，没有全局学习率。你 *按参数调整学习率*。当数据量很大且数据中的模式很少被发现时，Adagrad 工作得相当好。这两个标准都适用于我们的情况：你有很多数据，而且职业围棋游戏非常复杂，尽管这些移动组合在专业玩家的数据集中出现频率不高，但它们被认为是标准的游戏方式。
- en: 'Let’s say you have a weight vector *W* of length *l* (it’s easier to think
    of vectors here, but this technique applies more generally to tensors as well)
    with individual entries *W[i]*. For a given gradient *]W* for these parameters,
    in plain SGD with a learning rate of *a*, the update rule for each *W[i]* is as
    follows:'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个长度为 *l* 的权重向量 *W*（在这里更容易想象向量，但这项技术更普遍地适用于张量）以及单个条目 *W[i]*。对于这些参数的给定梯度
    *]W*，在具有学习率 *a* 的简单 SGD 中，每个 *W[i]* 的更新规则如下：
- en: '| *W[i]* ← *W[i]* – α∂*W[i]* |'
  id: totrans-1180
  prefs: []
  type: TYPE_TB
  zh: '| *W[i]* ← *W[i]* – α∂*W[i]* |'
- en: 'In Adagrad, you replace α with a term that adapts dynamically for each index
    *i* by looking at how much you’ve updated *W[i]* in the past. In fact, in Adagrad
    the individual learning rate will be inversely proportional to the previous updates.
    To be more precise, in Adagrad, you update parameters as follows:'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Adagrad 中，你用一个动态调整每个索引 *i* 的项替换 α，通过查看你过去更新 *W[i]* 的程度。事实上，在 Adagrad 中，个别学习率将与之前的更新成反比。更准确地说，在
    Adagrad 中，你按以下方式更新参数：
- en: '![](Images/p0174_01.jpg)'
  id: totrans-1182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0174_01.jpg)'
- en: 'In this equation, e is a small positive value to ensure you’re not dividing
    by 0, and *Gi,i* is the sum of squared gradients *W[i]* received until this point.
    We write this as *Gi,i* because you can view this term as part of a square matrix
    *G* of length *l* in which all diagonal entries *Gj,j* have the form we just described
    and all off-diagonal terms are 0\. A matrix of this form is called a *diagonal
    matrix*. You update *G* after each parameter update, by adding the latest gradient
    contributions to the diagonal elements. That’s all there is to defining Adagrad,
    but if you want to write this update rule in a concise form independent of the
    index i, this is the way to do it:'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，e 是一个很小的正值，以确保你不会除以 0，而 *Gi,i* 是直到这个点收到的平方梯度的总和。我们将其写作 *Gi,i*，因为你可以将这个项视为一个长度为
    *l* 的平方矩阵 *G* 的一部分，其中所有对角线元素 *Gj,j* 都具有我们刚刚描述的形式，而所有非对角线项都是 0。这种形式的矩阵被称为 *对角矩阵*。你会在每次参数更新后更新
    *G*，通过将最新的梯度贡献添加到对角元素中。这就是定义 Adagrad 的全部内容，但如果你想要以独立于索引 i 的简洁形式写出这个更新规则，这就是你该这样做的方式：
- en: '![](Images/p0174_02.jpg)'
  id: totrans-1184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0174_02.jpg)'
- en: Note that because *G* is a matrix, you need to add e to each entry *Gi,j* and
    divide α by each such entry. Moreover, by *G*·*]W* you mean matrix multiplication
    of *G* with *]W*. To use Adagrad with Keras, compiling a model with this optimizer
    works as follows.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，因为 *G* 是一个矩阵，所以你需要将 e 添加到每个条目 *Gi,j* 中，并将 α 除以每个这样的条目。此外，通过 *G*·*]W* 你意味着
    *G* 与 *]W* 的矩阵乘法。要使用 Adagrad 与 Keras，使用此优化器编译模型的工作方式如下。
- en: Listing 7.27\. Using the Adagrad optimizer for Keras models
  id: totrans-1186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.27\. 使用 Adagrad 优化器为 Keras 模型
- en: '[PRE109]'
  id: totrans-1187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: A key benefit of Adagrad over other SGD techniques is that you don’t have to
    manually set the learning rate—one thing less to worry about. It’s hard enough
    already to find a good network architecture and tune all the parameters for the
    model. In fact, you could alter the initial learning rate in Keras by using `Adagrad(lr=0.02)`,
    but it’s not recommended to do so.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad 相比于其他 SGD 技术的一个关键优点是，你不必手动设置学习率——又少了一件要担心的事情。找到好的网络架构并调整模型的所有参数已经很困难了。实际上，你可以通过使用
    `Adagrad(lr=0.02)` 在 Keras 中改变初始学习率，但这样做并不推荐。
- en: 7.5.3\. Refining adaptive gradients with Adadelta
  id: totrans-1189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.3\. 使用 Adadelta 精炼自适应梯度
- en: 'An optimizer that’s similar to Adagrad and is an extension of it is *Adadelta*.
    In this optimizer, instead of accumulating all past (squares of) gradients in
    *G*, you use the same idea we’ve shown you in the momentum technique and keep
    only a *fraction of the last update* and add the current gradient to it:'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Adagrad 类似且是其扩展的优化器是 *Adadelta*。在这个优化器中，你不会在 *G* 中累积所有过去的（平方的）梯度，而是使用我们在动量技术中向你展示的相同想法，只保留上一次更新的
    *一部分* 并将其添加到当前梯度中：
- en: '| *G* ← γ*G* + (1 – γ)∂*W* |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
  zh: '| *G* ← γ*G* + (1 – γ)∂*W* |'
- en: Although this idea is roughly what happens in Adadelta, the details that make
    this optimizer work and that lead to its precise update rule are a little too
    intricate to present here. We recommend that you look into the original paper
    for more details ([https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)).
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个想法大致上是 Adadelta 发生的事情，但使这个优化器工作并导致其精确更新规则的细节过于复杂，无法在此展示。我们建议你查阅原始论文以获取更多详细信息（[https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)）。
- en: In Keras, you use the Adadelta optimizer as follows.
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，你可以这样使用 Adadelta 优化器。
- en: Listing 7.28\. Using the Adadelta optimizer for Keras models
  id: totrans-1194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.28\. 使用 Adadelta 优化器进行 Keras 模型
- en: '[PRE110]'
  id: totrans-1195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Both Adagrad and Adadelta are hugely beneficial to training deep neural networks
    on Go data, as compared to stochastic gradient descent. In later chapters, you’ll
    often use one or the other as an optimizer in more-advanced models.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机梯度下降相比，Adagrad 和 Adadelta 对于在围棋数据上训练深度神经网络非常有用。在后面的章节中，你经常会在更高级的模型中使用其中一个作为优化器。
- en: 7.6\. Running your own experiments and evaluating performance
  id: totrans-1197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6\. 运行自己的实验和评估性能
- en: Throughout [chapters 5](kindle_split_017.xhtml#ch05), [6](kindle_split_018.xhtml#ch06),
    and this one, we’ve shown you many deep-learning techniques. We gave you some
    hints and sample architectures that made sense as a baseline, but now it’s time
    to train your own models. In machine-learning experiments, it’s crucial to try
    various combinations of *hyperparameters*, such as the number of layers, which
    layers to choose, how many epochs to train for, and so on. In particular, with
    deep neural networks, the number of choices you face can be overwhelming. It’s
    not always as clear how tweaking a specific knob impacts model performance. Deep-learning
    researchers can rely on a large corpus of experimental results and further theoretical
    arguments from decades of research to back their intuition. We can’t provide you
    with that deep a level of knowledge here, but we can help get you started building
    intuition of your own.
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](kindle_split_017.xhtml#ch05)、[第 6 章](kindle_split_018.xhtml#ch06)和这一章中，我们向您展示了许多深度学习技术。我们提供了一些作为基线有意义的提示和示例架构，但现在是你训练自己的模型的时候了。在机器学习实验中，尝试各种*超参数*的组合至关重要，例如层数的数量、选择哪些层、训练多少个周期等。特别是，对于深度神经网络，你面临的选择可能令人不知所措。调整特定旋钮如何影响模型性能并不总是那么清晰。深度学习研究人员可以依靠数十年的研究的大量实验结果和进一步的理论论证来支持他们的直觉。我们无法提供如此深入的知识水平，但我们可以帮助你开始建立自己的直觉。
- en: A crucial factor in achieving strong results in experimental setups such as
    ours—namely, training a neural network to predict Go moves as well as possible—is
    a *fast experimentation cycle*. The time it takes you to build a model architecture,
    start model training, observe and evaluate performance metrics, and then go back
    to adjust your model and start the process anew has to be short. When you look
    at data science challenges such as those hosted on kaggle.com, it’s often the
    teams *who tried the most* that win. Luckily for you, Keras was built with fast
    experimentation in mind. It’s also one of the prime reasons we chose it as deep-learning
    framework for this book. We hope you agree that you can build neural networks
    with Keras quickly and that changing your experimental setup comes naturally.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这样的实验设置中取得良好结果的关键因素——即训练一个神经网络尽可能准确地预测围棋走法——是一个*快速实验周期*。你构建模型架构、开始模型训练、观察和评估性能指标，然后返回调整你的模型并重新开始这个过程所需的时间必须很短。当你查看像
    kaggle.com 上举办的数据科学挑战时，通常获胜的是那些*尝试最多*的团队。幸运的是，Keras 是以快速实验为前提构建的。这也是我们选择它作为本书深度学习框架的主要原因之一。我们希望你能同意，你可以用
    Keras 快速构建神经网络，而且改变你的实验设置是自然而然的。
- en: 7.6.1\. A guideline to testing architectures and hyperparameters
  id: totrans-1200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.1\. 测试架构和超参数的指南
- en: 'Let’s have a look at a few practical considerations when building a move-prediction
    network:'
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在构建走法预测网络时的一些实际考虑因素：
- en: Convolutional neural networks are a good candidate for Go move-prediction networks.
    Make sure to convince yourself that working with only dense layers will result
    in inferior prediction quality. Building a network that consists of several convolutional
    layers and one or two dense layers at the end is usually a must. In later chapters,
    you’ll see more-complex architectures, but for now, work with convolutional networks.
  id: totrans-1202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络是围棋走法预测网络的理想候选者。确保你确信仅使用密集层会导致预测质量较差。通常，构建由多个卷积层和末尾的一到两个密集层组成的网络是必须的。在后面的章节中，你将看到更复杂的架构，但就目前而言，使用卷积网络。
- en: In your convolutional layers, vary the kernel sizes to see how this change influences
    model performance. As a rule of thumb, kernel sizes between 2 and 7 are suitable,
    and you shouldn’t go much larger than that.
  id: totrans-1203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的卷积层中，改变核大小以观察这种变化如何影响模型性能。作为一个经验法则，核大小在2到7之间是合适的，你不应该做得更大。
- en: If you use pooling layers, make sure to experiment with both max and average
    pooling, but more important, don’t choose a too large pooling size. A practical
    upper bound might be 3 in your situation. You may also want to try building networks
    without pooling layers, which might be computationally more expensive, but can
    work pretty well.
  id: totrans-1204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用池化层，确保尝试最大池化和平均池化，但更重要的是，不要选择过大的池化尺寸。在你的情况下，一个实用的上限可能是3。你可能还想要尝试构建没有池化层的网络，这可能会在计算上更昂贵，但可以工作得相当好。
- en: Use dropout layers for regularization. In [chapter 6](kindle_split_018.xhtml#ch06),
    you saw how dropout can be used to prevent your model from overfitting. Your networks
    will generally benefit from adding in dropout layers, as long as you don’t use
    too many of them and don’t set the dropout rate too high.
  id: totrans-1205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用dropout层进行正则化。在[第6章](kindle_split_018.xhtml#ch06)中，你看到了如何使用dropout来防止你的模型过拟合。只要你不使用太多的dropout层，并且不将dropout率设置得太高，你的网络通常都会从添加dropout层中受益。
- en: Use softmax activation in your last layer for its benefit of producing probability
    distributions and use it in combination with categorical cross-entropy loss, which
    suits your situation very well.
  id: totrans-1206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的最后一层使用softmax激活函数，因为它可以产生概率分布，并且与适合你情况的分类交叉熵损失函数结合使用。
- en: Experiment with different activation functions. We’ve introduced you to ReLU,
    which should act as your default choice for now, and sigmoid activations. You
    can use plenty of other activation functions in Keras, such as elu, selu, PReLU,
    and LeakyReLU. We can’t discuss these ReLU variants here, but their usage is well
    described at [https://keras.io/activations/](https://keras.io/activations/).
  id: totrans-1207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的激活函数。我们已经向你介绍了ReLU，它现在应该作为你的默认选择，以及sigmoid激活。你可以在Keras中使用许多其他激活函数，如elu、selu、PReLU和LeakyReLU。我们在这里不能讨论这些ReLU变体，但它们的用法在[https://keras.io/activations/](https://keras.io/activations/)上有很好的描述。
- en: Varying mini-batch size has an impact on model performance. In prediction problems
    such as MNIST from [chapter 5](kindle_split_017.xhtml#ch05), it’s usually recommended
    to choose mini-batches in the same order of magnitude as the number of classes.
    For MNIST, you often see mini-batch sizes ranging from 10 to 50\. If data is perfectly
    randomized, this way, each gradient will receive information from each class,
    which makes SGD generally perform better. In our use case, some Go moves are played
    much more often than others. For instance, the four corners of the board are rarely
    played, especially compared with the star points. We call this a *class imbalance*
    in our data. In this case, you can’t expect a mini-batch to cover all the classes,
    and should work with mini-batch sizes ranging from 16 to 256 (which is what you
    find in the literature). The choice of optimizer also has a considerable impact
    on how well your network learns. SGD with or without learning rate decay, as well
    as Adagrad and Adadelta, already give you options to experiment with. Under [https://keras.io/optimizers/](https://keras.io/optimizers/)
    you’ll find other optimizers that your model training process might benefit from.
    The number of epochs used to train a model has to be chosen appropriately. If
    you use model checkpointing and track various performance metrics per epoch, you
    can effectively measure when training stops improving. In the next and final section
    of this chapter, we briefly discuss how to evaluate performance metrics. As a
    general rule of thumb, given enough compute power, set the number of epochs too
    high rather than too low. If model training stops improving or even gets worse
    through overfitting, you can still take an earlier checkpoint model for your bot.
  id: totrans-1208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变化的批大小对模型性能有影响。在预测问题，如第5章中的MNIST（[章节5](kindle_split_017.xhtml#ch05)），通常建议选择与类别数量同数量级的批大小。对于MNIST，你经常看到批大小在10到50之间。如果数据完全随机化，这样，每个梯度都会从每个类别接收信息，这使得SGD通常表现更好。在我们的用例中，一些围棋走法比其他走法更常被使用。例如，棋盘的四个角落很少被使用，尤其是与星点相比。我们称这种数据为*类别不平衡*。在这种情况下，你不能期望一个批大小涵盖所有类别，而应该使用16到256（这在文献中有所发现）的批大小。优化器的选择也对网络学习效果有相当大的影响。带有或没有学习率衰减的SGD，以及Adagrad和Adadelta，已经为你提供了实验的选项。在[https://keras.io/optimizers/](https://keras.io/optimizers/)下，你会找到其他可能对你的模型训练过程有益的优化器。用于训练模型的epoch数量必须适当选择。如果你使用模型检查点和跟踪每个epoch的各种性能指标，你可以有效地测量训练何时停止改进。在下一节和本章的最后一节中，我们简要讨论了如何评估性能指标。作为一个一般性的指导原则，如果计算能力足够，将epoch数量设置得高一些而不是低一些。如果模型训练停止改进或甚至变得更差，你可以仍然使用早期检查点的模型为你的机器人服务。
- en: '|  |'
  id: totrans-1209
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Weight initializers**'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重初始化器**'
- en: Another crucial aspect for tuning deep neural networks is how to initialize
    the weights before training starts. Because optimizing a network means finding
    a set of weights corresponding to a minimum on the loss surface, the weights you
    start with are important. In your network implementation from [chapter 5](kindle_split_017.xhtml#ch05),
    you *randomly* assigned initial weights, which is generally a bad idea.
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: 调整深度神经网络的一个另一个关键方面是在训练开始之前如何初始化权重。因为优化网络意味着在损失曲面上找到一组对应于最小值的权重，你开始时的权重很重要。在你的网络实现中，你*随机地*分配了初始权重，这通常是一个坏主意。
- en: Weight initializations are an interesting topic of research and almost deserve
    a chapter of their own. Keras has many weight initialization schemes, and each
    layer with weights can be initialized accordingly. The reason you don’t cover
    them in the main text is that the initializers Keras chooses by default are usually
    so good that it’s not worth bothering to change them. Usually, it’s other aspects
    of your network definition that require attention. But it’s good to know that
    there are differences, and advanced users might want to experiment with Keras
    initializers, found at [https://keras.io/initializers/](https://keras.io/initializers/),
    as well.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 权重初始化是一个有趣的研究主题，几乎值得一个章节。Keras有许多权重初始化方案，每个带有权重的层都可以相应地初始化。你不在正文中介绍它们的原因是，Keras默认选择的初始化器通常非常好，不值得麻烦去改变它们。通常，你的网络定义的其他方面需要关注。但了解这些差异是好的，高级用户可能想要实验Keras初始化器，这些初始化器可以在[https://keras.io/initializers/](https://keras.io/initializers/)找到。
- en: '|  |'
  id: totrans-1213
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 7.6.2\. Evaluating performance metrics for training and test data
  id: totrans-1214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.2\. 训练和测试数据性能指标的评估
- en: 'In [section 7.3](#ch07lev1sec3), we showed you results of a training run performed
    on a small data set. The network we used was a relatively small convolutional
    network, and we trained this network for five epochs. In this experiment, we tracked
    loss and accuracy on training data and used test data for validation. At the end,
    we computed accuracy on test data. That’s the general workflow you should follow,
    but how do you judge when to stop training or detect when something is off? Here
    are a few guidelines:'
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7.3节](#ch07lev1sec3)中，我们展示了在一个小数据集上进行的训练运行的结果。我们使用的网络是一个相对较小的卷积网络，我们训练了这个网络五个epoch。在这个实验中，我们跟踪了训练数据上的损失和准确率，并使用测试数据进行了验证。最后，我们在测试数据上计算了准确率。这就是你应该遵循的一般工作流程，但你怎么判断何时停止训练或检测到有问题呢？以下是一些指导原则：
- en: Your training accuracy and loss should generally improve for each epoch. In
    later epochs, these metrics will taper off and sometimes fluctuate a little. If
    you don’t see any improvement for a few epochs, you might want to stop.
  id: totrans-1216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的训练准确率和损失应该通常在每个epoch中都有所提高。在后期epoch中，这些指标会逐渐减少，有时会有轻微的波动。如果你在几个epoch内看不到任何改进，你可能想停止训练。
- en: At the same time, you should see what your validation loss and accuracy look
    like. In early epochs, validation loss will drop consistently, but in later epochs,
    what you often see is that it plateaus and often starts to increase again. This
    is a sure sign that the network starts to overfit on the training data.
  id: totrans-1217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，你应该看看你的验证损失和准确率看起来如何。在早期epoch中，验证损失会持续下降，但在后期epoch中，你经常看到的是它开始平台期，并经常再次开始增加。这是网络开始对训练数据过拟合的明确迹象。
- en: If you use model checkpointing, pick the model from the epoch with high training
    accuracy that still has a low validation error.
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用模型检查点，请选择在具有高训练准确率的同时仍具有低验证误差的epoch中的模型。
- en: If both training and validation loss are high, try to choose a deeper network
    architecture or other hyperparameters.
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练和验证损失都很高，尝试选择更深的网络架构或其他超参数。
- en: In case your training error is low, but validation error high, your model is
    overfitting. This scenario usually doesn’t occur when you have a truly large training
    data set. With more than 170,000 Go games and many million moves to learn from,
    you should be fine.
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的训练误差很低，但验证误差很高，那么你的模型可能过拟合了。这种情况在你拥有真正庞大的训练数据集时通常不会发生。有了超过170,000场围棋比赛和数百万个可供学习的走法，你应该没问题。
- en: Choose a training data size that makes sense for your hardware requirements.
    If training an epoch takes more than a few hours, it’s just not that much fun.
    Instead, try to find a well-performing model among many tries on a medium-sized
    data set and then train this model once again on the largest data set possible.
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个适合你硬件要求的训练数据大小。如果训练一个epoch需要几个小时以上，那就不是那么有趣了。相反，尝试在中等大小的数据集上尝试许多模型，并从中找到一个表现良好的模型，然后在这个可能的最大数据集上再次训练这个模型。
- en: If you don’t have a good GPU, you might want to opt for training your model
    in the cloud. In [appendix D](kindle_split_031.xhtml#app04), we’ll show you how
    to train a model on a GPU using Amazon Web Services (AWS).
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你没有好的GPU，你可能想选择在云中训练你的模型。在[附录D](kindle_split_031.xhtml#app04)中，我们将向你展示如何使用亚马逊网络服务（AWS）在GPU上训练模型。
- en: When comparing runs, don’t stop a run that looks worse than a previous run too
    early. Some learning processes are slower than others—and might eventually catch
    up or even outperform other models.
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当比较运行结果时，不要过早地停止看起来比之前运行更差的运行。一些学习过程比其他过程慢——最终可能会赶上甚至超越其他模型。
- en: 'You might ask yourself how strong a bot you can potentially build with the
    methods presented in this chapter. A theoretical upper bound is this: the network
    can never get better at playing Go than the data you feed it. In particular, using
    just supervised deep-learning techniques, as you did in the last three chapters,
    won’t surpass human game play. In practice, with enough compute power and time,
    it’s definitely possible to reach results up to about 2 dan level.'
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问自己，使用本章介绍的方法，你能够构建多强大的机器人。一个理论上的上限是：网络在玩围棋方面永远不会比它所接受的数据更好。特别是，仅仅使用你在前三章中使用的监督深度学习技术，不会超越人类的游戏水平。在实践中，只要有足够的计算能力和时间，肯定可以达到大约2段水平的成果。
- en: To reach super-human performance of game play, you need to work with *reinforcement-learning*
    techniques, introduced in [chapters 9](kindle_split_021.xhtml#ch09) to [12](kindle_split_024.xhtml#ch12).
    Afterward, you can combine tree search from [chapter 4](kindle_split_016.xhtml#ch04),
    reinforcement learning, and supervised deep learning to build even stronger bots
    in [chapters 13](kindle_split_026.xhtml#ch13) and [14](kindle_split_027.xhtml#ch14).
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到超越人类水平的游戏表现，您需要使用在[第9章](kindle_split_021.xhtml#ch09)到[第12章](kindle_split_024.xhtml#ch12)中介绍的*强化学习*技术。之后，您可以将[第4章](kindle_split_016.xhtml#ch04)中的树搜索、强化学习和监督深度学习结合起来，在[第13章](kindle_split_026.xhtml#ch13)和[第14章](kindle_split_027.xhtml#ch14)中构建更强大的机器人。
- en: But before you go deeper into the methodology of building stronger bots, in
    the next chapter we’ll show you how to *deploy* a bot and let it interact with
    its environment by playing against either human opponents or other bots.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: 但在您深入探讨构建更强大机器人方法之前，在下一章中我们将向您展示如何*部署*一个机器人，并通过与人类对手或其他机器人进行比赛来让它与环境互动。
- en: 7.7\. Summary
  id: totrans-1227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7\. 摘要
- en: The ubiquitous Smart Game Format (SGF) for Go and other game records is useful
    to build data for neural networks.
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于围棋和其他游戏记录的通用智能游戏格式（SGF）对于构建神经网络数据很有用。
- en: Go data can be processed in parallel for speed and efficiently represented as
    generators.
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 围棋数据可以并行处理以提高速度，并有效地表示为生成器。
- en: With strong amateur-to-professional game records, you can build deep-learning
    models that predict Go moves quite well.
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用强大的业余到专业游戏记录，您可以构建预测围棋走法相当好的深度学习模型。
- en: If you know certain properties of your training data that are important, you
    can explicitly encode them in *feature planes*. Then the model can quickly learn
    connections between the feature planes and the results you’re trying to predict.
    For a Go bot, you can add feature planes that represent concepts such as the number
    of liberties (adjacent empty points) a string of stones has.
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您知道训练数据中某些重要的属性，您可以在*特征平面*中明确编码它们。然后模型可以快速学习特征平面与您试图预测的结果之间的联系。对于围棋机器人，您可以添加代表诸如石块串的空点数（相邻空点）等概念的特征平面。
- en: You can train more efficiently by using adaptive gradient techniques such as
    Adagrad or Adadelta. These algorithms adjust the learning rate on the fly as training
    progresses.
  id: totrans-1232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过使用自适应梯度技术，如Adagrad或Adadelta，来更有效地训练。这些算法在训练过程中动态调整学习率。
- en: End-to-end model training can be achieved in a relatively small script that
    you can use as a template for your own experiments.
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端到端模型训练可以通过一个相对较小的脚本实现，您可以用它作为自己实验的模板。
- en: Chapter 8\. Deploying bots in the wild
  id: totrans-1234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章\. 在野外部署机器人
- en: '*This chapter covers*'
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Building an end-to-end application to train and run a Go bot
  id: totrans-1236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个端到端应用程序来训练和运行围棋机器人
- en: Running a frontend to play against your bot
  id: totrans-1237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行前端与您的机器人进行比赛
- en: Letting your bot play against other bots locally
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让您的机器人本地与其他机器人进行比赛
- en: Deploying your bot on an online Go server
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在在线围棋服务器上部署您的机器人
- en: By now, you know how to build and train a strong deep-learning model for Go
    move prediction—but how do you integrate this into an application that plays games
    against opponents? Training a neural network is just one part of building an end-to-end
    application, whether you’re playing yourself or letting your bot compete against
    other bots. The trained model has to be integrated into an engine that can be
    played against.
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，您已经知道如何构建和训练一个强大的深度学习模型用于围棋走法预测——但是如何将其集成到与对手玩游戏的应用程序中呢？训练神经网络只是构建端到端应用程序的一部分，无论您是在自己玩游戏还是让您的机器人与其他机器人竞争。训练好的模型必须集成到一个可以与之对战的引擎中。
- en: 'In this chapter, you’ll build a simple Go model server and two frontends. First,
    we provide you with an HTTP frontend that you can use to play against your bot.
    Then, we introduce you to the Go Text Protocol (GTP), a widely used protocol that
    Go bots use to exchange information, so your bot can play against other bots like
    *GNU Go* or *Pachi*, two freely available Go programs based on GTP. Finally, we
    show you how to deploy your Go bot on Amazon Web Services (AWS) and connect it
    against the Online Go Server (OGS). Doing so will allow your bots to play ranked
    games in a real environment, compete against other bots and human players worldwide,
    and even enter tournaments. To do all this, we’ll show you how to tackle the following
    tasks:'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将构建一个简单的Go模型服务器和两个前端。首先，我们为你提供了一个HTTP前端，你可以用它来与你的机器人对战。然后，我们向你介绍Go文本协议（GTP），这是一个广泛使用的协议，Go机器人使用它来交换信息，因此你的机器人可以与其他机器人如*GNU
    Go*或*Pachi*对战，这两个基于GTP的免费Go程序。最后，我们向你展示如何将你的Go机器人部署在亚马逊网络服务（AWS）上，并将其连接到在线围棋服务器（OGS）。这样做将允许你的机器人在真实环境中进行排名对战，与世界各地的其他机器人和人类玩家竞争，甚至参加锦标赛。为了完成所有这些，我们将向你展示如何完成以下任务：
- en: '***Building a move-prediction agent*—** The neural networks you trained in
    [chapters 6](kindle_split_018.xhtml#ch06) and [7](kindle_split_019.xhtml#ch07)
    need to be integrated into a framework that allows you to use them in game play.
    In [section 8.1](#ch08lev1sec1), we’ll pick up the idea of *agents* from [chapter
    3](kindle_split_014.xhtml#ch03) (in which you created a randomly playing agent)
    as a basis to serve a deep-learning bot.'
  id: totrans-1242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建移动预测代理**——**你在[第6章](kindle_split_018.xhtml#ch06)和[第7章](kindle_split_019.xhtml#ch07)中训练的神经网络需要集成到一个框架中，以便你在游戏中使用它们。在[第8.1节](#ch08lev1sec1)中，我们将从[第3章](kindle_split_014.xhtml#ch03)（其中你创建了一个随机播放的代理）中提取*代理*的概念，作为构建深度学习机器人的基础。'
- en: '***Providing a graphical interface*—** As humans, to conveniently play against
    a Go bot, we need some sort of (graphical) interface. Although so far we’ve been
    happy with command-line interfaces, in [section 8.2](#ch08lev1sec2) we’ll equip
    you with a fun-to-play frontend for your bot.'
  id: totrans-1243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供图形界面**——**作为人类，为了方便地与Go机器人对战，我们需要某种（图形）界面。虽然到目前为止我们一直对命令行界面感到满意，但在[第8.2节](#ch08lev1sec2)中，我们将为你提供一款有趣的机器人前端。'
- en: '***Deploying a bot in the cloud*—** If you don’t have a powerful GPU in your
    computer, you won’t get far training strong Go bots. Luckily, most big cloud providers
    offer GPU instances on demand. But even if you have a strong-enough GPU for training,
    you still may want to host your previously trained model on a server. In [section
    8.3](#ch08lev1sec3), we’ll show you how this can be done and refer you to [appendix
    D](kindle_split_031.xhtml#app04) for more details on how to set everything up
    in AWS.'
  id: totrans-1244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在云端部署机器人**——**如果你的电脑中没有强大的GPU，你将无法训练出强大的Go机器人。幸运的是，大多数大型云服务提供商都提供按需的GPU实例。即使你有足够的GPU来训练，你仍然可能希望将之前训练好的模型托管在服务器上。在[第8.3节](#ch08lev1sec3)中，我们将向你展示如何做到这一点，并参考[附录D](kindle_split_031.xhtml#app04)以获取更多关于如何在AWS中设置一切的详细信息。'
- en: '***Talking to other bots*—** Humans use graphical and other interfaces to interact
    with each other. For bots, it’s customary to communicate through a standardized
    protocol. In [section 8.4](#ch08lev1sec4), we’ll introduce you to the common Go
    Text Protocol (GTP). This is an essential component for the following two points:'
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他机器人交流**——**人类使用图形和其他界面进行交互。对于机器人来说，通过标准化协议进行通信是惯例。在[第8.4节](#ch08lev1sec4)中，我们将向你介绍常见的Go文本协议（GTP）。这是以下两个要点的基础：'
- en: '***Playing against other bots*—** You’ll then build a GTP frontend for your
    bot to let it play against other programs in [section 8.5](#ch08lev1sec5). We’ll
    show you how to let your bot play against two other Go programs locally, to see
    how well your creation does.'
  id: totrans-1246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他机器人对战**——**你将构建一个GTP前端，让你的机器人可以在[第8.5节](#ch08lev1sec5)中与其他程序对战。我们将向你展示如何让你的机器人本地与两个其他Go程序对战，以了解你的创作表现如何。'
- en: '***Deploying a bot on an online Go server*—** In [section 8.6](#ch08lev1sec6),
    we’ll finally show you how to deploy a bot on an online Go platform so that registered
    users and other bots can compete against your bot. This way, your bot can even
    enter ranked games and enter tournaments, all of which we’ll show you in this
    last section. Because most of this material is technical, you’ll find most of
    the details in [appendix E](kindle_split_032.xhtml#app05).'
  id: totrans-1247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***在在线围棋服务器上部署机器人*—** 在 [第 8.6 节](#ch08lev1sec6) 中，我们将最终向你展示如何在在线围棋平台上部署机器人，以便注册用户和其他机器人可以与你的机器人竞争。这样，你的机器人甚至可以进入排名游戏和参加锦标赛，所有这些我们将在本节的最后部分展示。由于大部分内容都是技术性的，你将在
    [附录 E](kindle_split_032.xhtml#app05) 中找到大部分细节）。'
- en: 8.1\. Creating a move-prediction agent from a deep neural network
  id: totrans-1248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 从深度神经网络创建移动预测代理
- en: Now that you have all the building blocks in place to build a strong neural
    network for Go data, let’s integrate such networks into an *agent* that will serve
    them. Recall from [chapter 3](kindle_split_014.xhtml#ch03) the concept of `Agent`.
    We defined it as a class that can select the next move for the current game state,
    by implementing a `select_move` method. Let’s write a `DeepLearningAgent` by using
    Keras models and our Go board `Encoder` concept (put this code into predict.py
    in the agent module in dlgo).
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有了构建强大的围棋数据神经网络的所有构建块，让我们将这些网络集成到一个 *代理* 中，以便为其提供服务。回顾一下 [第 3 章](kindle_split_014.xhtml#ch03)
    中的 `Agent` 概念。我们将其定义为一种可以实施 `select_move` 方法来选择当前游戏状态的下一步移动的类。让我们通过使用 Keras 模型和我们的围棋盘
    `Encoder` 概念来编写一个 `DeepLearningAgent`（将此代码放入 dlgo 模块中的 predict.py 文件中）。
- en: Listing 8.1\. Initializing an agent with a Keras model and a Go board encoder
  id: totrans-1250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1\. 使用 Keras 模型和围棋盘编码器初始化代理
- en: '[PRE111]'
  id: totrans-1251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: You’ll use the encoder to transform the board state into features, and you’ll
    use the model to predict the next move. In fact, you’ll use the model to compute
    a whole probability distribution of possible moves that you’ll later sample from.
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用编码器将棋盘状态转换为特征，并使用模型来预测下一步移动。实际上，你将使用模型来计算可能的整个移动概率分布，然后从其中采样。
- en: Listing 8.2\. Encoding board state and predicting move probabilities with a
    model
  id: totrans-1253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2\. 使用模型编码棋盘状态并预测移动概率
- en: '[PRE112]'
  id: totrans-1254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Next, you alter the probability distribution stored in `move_probs` a little.
    First, you compute the cube of all values to drastically increase the distance
    between more-likely and less-likely moves. You want the best possible moves to
    be picked much more often. Then you use a trick called *clipping* that prevents
    move probabilities from being too close to either 0 or 1\. This is done by defining
    a small positive value, ϵ = 0.000001, and setting values smaller than ϵ to ϵ,
    and values larger than 1 – ϵ to 1 – ϵ. Afterward, you normalize the resulting
    values to end up with a probability distribution once again.
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你稍微改变存储在 `move_probs` 中的概率分布。首先，计算所有值的立方，以极大地增加更可能和不太可能的移动之间的距离。你希望最佳可能的移动被选择得更加频繁。然后，你使用一种称为
    *裁剪* 的技巧，防止移动概率过于接近 0 或 1。这是通过定义一个小的正数 ϵ = 0.000001，并将小于 ϵ 的值设置为 ϵ，将大于 1 – ϵ 的值设置为
    1 – ϵ 来实现的。之后，你将得到的值归一化，以再次得到一个概率分布。
- en: Listing 8.3\. Scaling, clipping, and renormalizing your move probability distribution
  id: totrans-1256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3\. 缩放、裁剪和重新归一化你的移动概率分布
- en: '[PRE113]'
  id: totrans-1257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '***1* Increases the distance between the more likely and least likely moves**'
  id: totrans-1258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 增加更可能和最不可能移动之间的距离**'
- en: '***2* Prevents move probabilities from getting stuck at 0 or 1**'
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 防止移动概率卡在 0 或 1**'
- en: '***3* Renormalizes to get another probability distribution**'
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 重新归一化以获得另一个概率分布**'
- en: You do this transformation because you want to sample moves from this distribution,
    according to their probabilities. Instead of sampling moves, another viable strategy
    would be to always take the most likely move (taking the maximum over the distribution).
    The benefit of the way you’re doing it is that sometimes other moves get chosen,
    which might be especially useful when there isn’t one single move that sticks
    out from the rest.
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 你进行这种转换是因为你想要根据它们的概率从这个分布中采样移动。除了采样移动之外，另一种可行的策略是始终选择最可能的移动（在分布上取最大值）。你正在采取的方法的好处是，有时其他移动会被选择，这可能在没有单一移动突出时特别有用。
- en: Listing 8.4\. Trying to apply moves from a ranked candidate list
  id: totrans-1262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.4\. 尝试应用排名候选列表中的移动
- en: '[PRE114]'
  id: totrans-1263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '***1* Turns the probabilities into a ranked list of moves**'
  id: totrans-1264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将概率转换为移动的排名列表**'
- en: '***2* Samples potential candidates**'
  id: totrans-1265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 样本潜在候选人**'
- en: '***3* Starting from the top, finds a valid move that doesn’t reduce eye-space**'
  id: totrans-1266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 从顶部开始，找到一个有效且不会减少势的走法**'
- en: '***4* If no legal and non-self-destructive moves are left, passes**'
  id: totrans-1267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 如果没有合法且非自杀性的走法，则跳过**'
- en: 'For convenience, you also want to persist a `DeepLearningAgent`, so you can
    pick it up at a later point. The prototypical situation in practice is this: you
    train a deep-learning model and create an agent, which you then persist. At a
    later point, this agent gets deserialized and served, so human players or other
    bots can play against it. To do the serialization step, you hijack the serialization
    format of Keras. When you persist a Keras model, it gets stored in HDF5, an efficient
    serialization format. HDF5 files contain flexible *groups* that are used to store
    *meta-information* and *data*. For any Keras model, you can call `model.save("model_path.h5")`
    to persist the full model, meaning the neural network architecture and all weights,
    to the local file model_path.h5\. The only thing you need to do before persisting
    a Keras model like this is to install the Python library h5py; for instance, with
    `pip install h5py`.'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，你还需要持久化一个`DeepLearningAgent`，这样你可以在以后的时间点重新使用它。在实践中，典型的场景是这样的：你训练一个深度学习模型并创建一个代理，然后将其持久化。在以后的时间点，这个代理被反序列化并托管，以便人类玩家或其他机器人可以与之对弈。为了执行序列化步骤，你篡改了Keras的序列化格式。当你持久化一个Keras模型时，它会被存储在HDF5中，这是一种高效的序列化格式。HDF5文件包含灵活的*组*，用于存储*元信息*和*数据*。对于任何Keras模型，你可以调用`model.save("model_path.h5")`来持久化整个模型，这意味着神经网络架构和所有权重都会被保存到本地的`model_path.h5`文件中。在持久化这样的Keras模型之前，你需要安装Python库h5py；例如，使用`pip
    install h5py`。
- en: To store a complete agent, you can add an additional group for information about
    your Go board encoder.
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: 要存储一个完整的代理，你可以为你的围棋棋盘编码器添加一个额外的信息组。
- en: Listing 8.5\. Serializing a deep-learning agent
  id: totrans-1270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5\. 序列化深度学习代理
- en: '[PRE115]'
  id: totrans-1271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Finally, after you serialize a model, you also need to know how to load it from
    an HDF5 file.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在你序列化了一个模型之后，你还需要知道如何从HDF5文件中加载它。
- en: Listing 8.6\. Deserializing a `DeepLearningAgent` from an HDF5 file
  id: totrans-1273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6\. 从HDF5文件反序列化`DeepLearningAgent`
- en: '[PRE116]'
  id: totrans-1274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: This completes our definition of a deep-learning agent. As a next step, you
    have to make sure this agent connects and interacts with an environment. You do
    this by embedding `DeepLearningAgent` into a web application that human players
    can play against in their browser.
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对深度学习代理的定义。作为下一步，你必须确保这个代理能够连接并与环境交互。你通过将`DeepLearningAgent`嵌入到一个人类玩家可以在浏览器中与之对弈的Web应用程序中来实现这一点。
- en: 8.2\. Serving your Go bot to a web frontend
  id: totrans-1276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 将你的围棋机器人托管到Web前端
- en: In [chapters 6](kindle_split_018.xhtml#ch06) and [7](kindle_split_019.xhtml#ch07),
    you designed and trained a neural network that predicts what move a human would
    play in a Go game. In [section 8.1](#ch08lev1sec1), you turned that model for
    move *prediction* into a `DeepLearningAgent` that does move *selection*. The next
    step is to play your bot! Back in [chapter 3](kindle_split_014.xhtml#ch03), you
    built a bare-bones interface in which you could type in moves on your keyboard,
    and your benighted `RandomBot` would print its reply to the console. Now that
    you’ve built a more sophisticated bot, it deserves a nicer frontend to communicate
    moves with a human player.
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_018.xhtml#ch06)和[第7章](kindle_split_019.xhtml#ch07)中，你设计和训练了一个神经网络，用于预测人类在围棋游戏中的下一步棋。在[第8.1节](#ch08lev1sec1)中，你将这个用于*预测*的模型转换成了一个`DeepLearningAgent`，用于*选择*走棋。下一步就是让你的机器人下棋！回到[第3章](kindle_split_014.xhtml#ch03)，你构建了一个基本的界面，你可以通过键盘输入走棋，而你那无知的`RandomBot`会将它的回复打印到控制台。现在你已经构建了一个更复杂的机器人，它值得一个更好的前端来与人类玩家进行交互。
- en: In this section, you’ll connect the `DeepLearningAgent` to a Python web application,
    so you can play against it in your web browser. You’ll use the lightweight Flask
    library to serve such an agent via HTTP. On the browser side, you’ll use a JavaScript
    library called jgoboard to render a Go board that humans can use. The code can
    be found in our repository on GitHub, in the httpfrontend module in dlgo. We don’t
    explicitly discuss this code here, because we don’t want to distract from the
    main topic, building a Go AI, by digressing into web development techniques in
    other languages (such as HTML or JavaScript). Instead, we’ll give you an overview
    of what the application does and how to use it in an end-to-end example. [Figure
    8.1](#ch08fig01) provides an overview of the application you’re going to build
    in this chapter.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将连接`DeepLearningAgent`到一个Python网络应用，这样你就可以在浏览器中与之对弈。你将使用轻量级的Flask库通过HTTP提供这样的代理。在浏览器端，你将使用一个名为jgoboard的JavaScript库来渲染人类可以使用的围棋盘。代码可以在我们GitHub上的仓库中找到，在dlgo的httpfrontend模块中。我们在这里没有明确讨论这段代码，因为我们不想通过涉及其他语言的Web开发技术（如HTML或JavaScript）来分散对构建围棋AI这一主要主题的注意力。相反，我们将提供一个关于应用程序做什么以及如何在端到端示例中使用它的概述。[图8.1](#ch08fig01)提供了你将在本章中构建的应用程序概述。
- en: Figure 8.1\. Building a web frontend for your Go bot. The httpfrontend module
    starts a Flask web server that decodes HTTP requests and passes them to one or
    more Go-playing agents. In the browser, a client based on the jgoboard library
    communicates with the server over HTTP.
  id: totrans-1279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1\. 为你的围棋机器人构建一个网络前端。httpfrontend模块启动一个Flask网络服务器，它解码HTTP请求并将它们传递给一个或多个围棋机器人代理。在浏览器中，基于jgoboard库的客户端通过HTTP与服务器通信。
- en: '![](Images/08fig01_alt.jpg)'
  id: totrans-1280
  prefs: []
  type: TYPE_IMG
  zh: '![图片8.1](Images/08fig01_alt.jpg)'
- en: If you look into the structure of httpfrontend, you find a file called server.py
    that has a single, well-documented method, `get_web_app`, that you can use to
    return a web application to run. Here’s an example of how to use `get_web_app`
    to load a random bot and serve it.
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看httpfrontend的结构，你会找到一个名为server.py的文件，它有一个单一、文档良好的方法`get_web_app`，你可以使用它来返回一个要运行的网络应用。以下是如何使用`get_web_app`加载一个随机机器人并为其提供服务的示例。
- en: Listing 8.7\. Registering a random agent and starting a web application with
    it
  id: totrans-1282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7\. 注册一个随机代理并使用它启动网络应用
- en: '[PRE117]'
  id: totrans-1283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'When you run this example, a web application will start on localhost (127.0.0.1),
    listening on port 5000, which is the default port used in Flask applications.
    The `RandomBot` you just registered as `random` corresponds to an HTML file in
    the static folder in httpfrontend: play_random_99.html. In this file, a Go board
    is rendered, and it’s also the place in which the rules of human-bot game play
    are defined. The human opponent starts with the black stones; the bot takes white.
    Whenever a human move has been played, the route/select-move/random is triggered
    to receive the next move from the bot. After the bot move has been received, it’s
    applied to the board, and it’s the human’s move once again. To play against this
    bot, navigate to http://127.0.0.1:5000/static/play_random_99.html in your browser.
    You should see a playable demo, as shown in [figure 8.2](#ch08fig02).'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个示例时，一个网络应用将在本地主机（127.0.0.1）上启动，监听端口5000，这是Flask应用中使用的默认端口。你刚刚注册的`RandomBot`作为`random`对应于httpfrontend中的静态文件夹中的一个HTML文件：play_random_99.html。在这个文件中，一个围棋盘被渲染出来，这也是定义人机游戏规则的地方。人类对手开始时使用黑子；机器人使用白子。每当人类移动后，就会触发route/select-move/random来从机器人那里接收下一步移动。收到机器人的移动后，它将被应用到棋盘上，然后轮到人类移动。要与此机器人对弈，请在浏览器中导航到http://127.0.0.1:5000/static/play_random_99.html。你应该会看到一个可玩演示，如图8.2所示。
- en: Figure 8.2\. Running a Python web application to play against a Go bot in your
    browser
  id: totrans-1285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2\. 在浏览器中运行Python网络应用以与围棋机器人对弈
- en: '![](Images/08fig02.jpg)'
  id: totrans-1286
  prefs: []
  type: TYPE_IMG
  zh: '![图片8.2](Images/08fig02.jpg)'
- en: 'You’ll add more and more bots in the next chapters, but for now note that another
    frontend is available under play_predict_19.html. This web frontend talks to a
    bot called predict and can be used to play 19 × 19 games. Therefore, if you train
    a Keras neural network model on Go data and use a Go board `encoder`, you can
    first create an instance `agent = DeepLearningAgent(model, encoder)` and then
    register it in a web application `web_app = get_web_app({''predict'': agent})`
    that you can then start with `web_app.run()`.'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的章节中，你将添加越来越多的机器人，但到目前为止请注意，在play_predict_19.html下还有一个可用的前端。这个网络前端与一个名为predict的机器人通信，可以用来玩19×19的游戏。因此，如果你在围棋数据上训练一个Keras神经网络模型并使用一个围棋盘`encoder`，你可以首先创建一个实例`agent
    = DeepLearningAgent(model, encoder)`，然后将其注册到一个网络应用`web_app = get_web_app({''predict'':
    agent})`中，然后你可以通过`web_app.run()`启动它。'
- en: 8.2.1\. An end-to-end Go bot example
  id: totrans-1288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. 一个端到端的围棋机器人示例
- en: '[Figure 8.3](#ch08fig03) shows an end-to-end example covering the whole process
    (the same flow we introduced in the beginning of [chapter 7](kindle_split_019.xhtml#ch07)).
    You start with the imports you need and load Go data into features *X* and labels
    *y* by using an encoder and a Go data processor, as shown in [listing 8.8](#ch08ex08).'
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.3](#ch08fig03)展示了涵盖整个过程的端到端示例（与我们在[第7章](kindle_split_019.xhtml#ch07)开头介绍的过程相同）。您从所需的导入开始，并使用编码器和围棋数据处理器将围棋数据加载到特征*X*和标签*y*中，如[列表8.8](#ch08ex08)所示。'
- en: Listing 8.8\. Loading features and labels from Go data with a processor
  id: totrans-1290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.8\. 使用处理器从围棋数据中加载特征和标签
- en: '[PRE118]'
  id: totrans-1291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Figure 8.3\. The training process for a deep-learning Go bot
  id: totrans-1292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3\. 深度学习围棋机器人的训练过程
- en: '![](Images/08fig03_alt.jpg)'
  id: totrans-1293
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig03_alt.jpg)'
- en: Equipped with features and labels, you can build a deep convolutional neural
    network and train it on this data. This time, you choose the large network from
    dlgo.networks and use Adadelta as the optimizer.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了特征和标签，您可以使用深度卷积神经网络并使用Adadelta作为优化器来训练它。这次，您从dlgo.networks中选择大型网络。
- en: Listing 8.9\. Building and running a large Go move-predicting model with Adadelta
  id: totrans-1295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.9\. 使用Adadelta构建和运行大型围棋移动预测模型
- en: '[PRE119]'
  id: totrans-1296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: After the model has finished training, you can create a Go bot from it and save
    this bot in HDF5 format.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，您可以从它创建一个围棋机器人并将其保存为HDF5格式。
- en: Listing 8.10\. Creating and persisting a `DeepLearningAgent`
  id: totrans-1298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.10\. 创建并持久化一个`DeepLearningAgent`
- en: '[PRE120]'
  id: totrans-1299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Finally, you can load the bot from file and serve it in a web application.
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以从文件中加载机器人并将其托管在Web应用程序中。
- en: Listing 8.11\. Loading a bot back into memory and serving it in a web application
  id: totrans-1301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.11\. 将机器人加载回内存并在Web应用程序中托管它
- en: '[PRE121]'
  id: totrans-1302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Of course, if you’ve already trained a strong bot, you can skip all but the
    last part. For instance, you could load one of the models stored in checkpoints
    in [chapter 7](kindle_split_019.xhtml#ch07) and see how they perform as opponents
    in action by changing the model_file accordingly.
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您已经训练了一个强大的机器人，您可以跳过除了最后部分之外的所有内容。例如，您可以加载存储在[第7章](kindle_split_019.xhtml#ch07)检查点中的一个模型，并通过相应地更改model_file来查看它们作为对手的表现。
- en: 8.3\. Training and deploying a Go bot in the cloud
  id: totrans-1304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 在云端训练和部署围棋机器人
- en: Until this point, all development took place on your local machine at home.
    If you’re in the good position to have a modern GPU available on your computer,
    training the deep neural networks we developed in [chapters 5](kindle_split_017.xhtml#ch05)–[7](kindle_split_019.xhtml#ch07)
    isn’t of concern for you. If you don’t have a powerful GPU or can’t spare any
    compute time on it, it’s usually a good option to *rent compute time on a GPU
    in the cloud*.
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有开发都是在您家里的本地机器上进行的。如果您幸运地能在电脑上使用现代GPU，那么训练我们在[第5章](kindle_split_017.xhtml#ch05)–[第7章](kindle_split_019.xhtml#ch07)中开发的深度神经网络对您来说不是问题。如果您没有强大的GPU或无法在它上面节省任何计算时间，那么在云端租用GPU的计算时间通常是一个不错的选择。
- en: If you disregard training for now and assume you have a strong bot already,
    serving this bot is another situation in which cloud providers can come in handy.
    In [section 8.2](#ch08lev1sec2), you ran a bot via a web application hosted from
    localhost. If you want to share your bot with friends or make it public, that’s
    not exactly ideal. You neither want to ensure that your computer runs night and
    day, nor give the public access to your machine. By hosting your bot in the cloud,
    you separate development from deployment and can simply share a URL with anyone
    who’s interested in playing your bot.
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在不考虑训练，并假设您已经有一个强大的机器人，那么托管这个机器人是云服务提供商可以派上用场的情况之一。在[第8.2节](#ch08lev1sec2)中，您通过本地主机托管的一个Web应用程序运行了一个机器人。如果您想与朋友分享您的机器人或使其公开，这并不完全理想。您既不想确保您的电脑日夜运行，也不想让公众访问您的机器。通过在云端托管您的机器人，您可以分离开发和部署，并可以简单地与任何对玩您的机器人感兴趣的人分享一个URL。
- en: 'Because this topic is important, but somewhat special and only indirectly related
    to machine learning, we entirely outsourced it to [appendix D](kindle_split_031.xhtml#app04).
    Reading and applying the techniques from this appendix is entirely optional, but
    recommended. In [appendix D](kindle_split_031.xhtml#app04), you’ll learn how to
    get started with one particular cloud provider, Amazon Web Services (AWS). You’ll
    learn the following skills in the appendix:'
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个话题很重要，但有些特殊，并且仅与机器学习间接相关，所以我们完全将其外包给了 [附录 D](kindle_split_031.xhtml#app04)。阅读并应用附录中的技术是可选的，但推荐。在
    [附录 D](kindle_split_031.xhtml#app04) 中，你将学习如何开始使用一个特定的云服务提供商，亚马逊网络服务（AWS）。附录中你将学习以下技能：
- en: Creating an account with AWS
  id: totrans-1308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS 上创建账户
- en: Flexibly setting up, running, and terminating virtual server instances
  id: totrans-1309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活地设置、运行和终止虚拟服务器实例
- en: Creating an AWS instance suitable for deep-learning model training on a cloud
    GPU at reasonable cost
  id: totrans-1310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云 GPU 上以合理的成本创建适合深度学习模型训练的 AWS 实例
- en: Deploying your Go bot served over HTTP on an (almost) free server
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个（几乎）免费的服务器上部署通过 HTTP 服务的 Go 机器人
- en: On top of learning these useful skills, [appendix D](kindle_split_031.xhtml#app04)
    is also a prerequisite for deploying a full-blown Go bot that connects to an online
    Go server, a topic we cover later in [section 8.6](#ch08lev1sec6).
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习这些有用的技能的基础上，[附录 D](kindle_split_031.xhtml#app04) 也是部署一个连接到在线围棋服务器的完整 Go 机器人的先决条件，我们将在第
    8.6 节中介绍这个话题。
- en: '8.4\. Talking to other bots: the Go Text Protocol'
  id: totrans-1313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 与其他机器人交谈：Go 文本协议
- en: In [section 8.2](#ch08lev1sec2), you saw how to integrate your bot framework
    into a web frontend. For this to work, you handled communication between the bot
    and human player with the Hypertext Transfer Protocol (HTTP), one of the core
    protocols running the web. To avoid distraction, we purposefully left out all
    the details, but having a *standardized protocol* in place is necessary to pull
    this off. Humans and bots don’t share a common language to exchange Go moves,
    but a protocol can act as a bridge.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 8.2 节](#ch08lev1sec2) 中，你看到了如何将你的机器人框架集成到网页前端。为了实现这一点，你处理了机器人和人类玩家之间的通信，使用了超文本传输协议（HTTP），这是运行网络的核心理协议之一。为了避免分心，我们故意省略了所有细节，但有一个
    *标准化协议* 在位是必要的。人类和机器人没有共享的语言来交换围棋走法，但协议可以作为一座桥梁。
- en: The Go Text Protocol (GTP) is the de facto standard used by Go servers around
    the world to connect humans and bots on their platforms. Many offline Go programs
    are based on GTP as well. This section introduces you to GTP by example; you’ll
    implement part of the protocol in Python and use this implementation to let your
    bots play against other Go programs.
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: Go 文本协议（GTP）是全球围棋服务器使用的默认标准，用于连接其平台上的人类和机器人。许多离线围棋程序也基于 GTP。本节通过示例介绍 GTP；你将在
    Python 中实现协议的一部分，并使用这个实现让你的机器人与其他围棋程序对弈。
- en: 'In [appendix C](kindle_split_030.xhtml#app03), we explain how to install GNU
    Go and Pachi, two common Go programs available for practically all operating systems.
    We recommend installing both, so please make sure to have both programs on your
    system. You don’t need any frontends, just the plain command-line tools. If you
    have GNU Go installed, you can start it in GTP mode by running the following:'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [附录 C](kindle_split_030.xhtml#app03) 中，我们解释了如何安装 GNU Go 和 Pachi，这两种常见的围棋程序几乎适用于所有操作系统。我们建议安装这两个程序，所以请确保你的系统上安装了这两个程序。你不需要任何前端，只需要纯命令行工具。如果你已经安装了
    GNU Go，你可以通过运行以下命令以 GTP 模式启动它：
- en: '[PRE122]'
  id: totrans-1317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Using this mode, you can now explore how GTP works. GTP is a text-based protocol,
    so you can type commands into your terminal and hit Enter. For instance, to set
    up a 9 × 9 board, you can type `boardsize 9`. This will trigger GNU Go to return
    a response and acknowledge that the command has been executed correctly. Every
    successful GTP command triggers a response starting with the symbol `=`, whereas
    failed commands lead to a `?`. To check the current board state, you can issue
    the command `showboard`, which will print out an empty 9 × 9 board, as expected.
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此模式，你现在可以探索 GTP 的工作原理。GTP 是一种基于文本的协议，因此你可以在你的终端中输入命令并按 Enter 键。例如，要设置一个 9
    × 9 的棋盘，你可以输入 `boardsize 9`。这将触发 GNU Go 返回响应并确认命令已正确执行。每个成功的 GTP 命令都会触发一个以符号 `=`
    开头的响应，而失败的命令则会导致一个 `?`。要检查当前棋盘状态，你可以发出 `showboard` 命令，这将打印出一个空白的 9 × 9 棋盘，正如预期的那样。
- en: 'In actual game play, two commands are the most important: `genmove` and `play`.
    The first command, `genmove`, is used to ask a GTP bot to generate the next move.
    The GTP bot will usually also apply this move to its game state internally. All
    this command needs as arguments is the player color, either black or white. For
    instance, to generate a white move and place it on GNU Go’s board, type `genmove
    white`. This will lead to a response such as `= C4`, meaning GNU Go accepts this
    command (`=`) and places a white stone at C4\. As you can see, GTP accepts standard
    coordinates as introduced in [chapters 2](kindle_split_013.xhtml#ch02) and [3](kindle_split_014.xhtml#ch03).'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际游戏中，两个命令是最重要的：`genmove`和`play`。第一个命令`genmove`用于请求GTP机器人生成下一步棋。GTP机器人通常会将其应用于其游戏状态。这个命令需要的参数只有玩家颜色，即黑或白。例如，要生成白棋并将其放置在GNU
    Go的棋盘上，请输入`genmove white`。这将导致一个响应，如`= C4`，这意味着GNU Go接受这个命令（`=`）并在C4放置一个白棋。正如你所看到的，GTP接受标准坐标，如第2章和第3章中介绍的那样。
- en: The other game-play relevant move for us is `play`. This command is used to
    let a GTP bot know it has to play a move on the board. For instance, you could
    tell GNU Go that you want it to play a black move on D4 by issuing `play black
    D4`, which will return an `=` to acknowledge this command. When two bots play
    against each other, they’ll take turns asking each other to `genmove` the next
    move, and then `play` the move from the response on their own board. This is all
    pretty straightforward—but we left out many details. A complete GTP client has
    a lot more commands to handle, ranging from handling handicap stones to managing
    time settings and counting rules. If you’re interested in the details of GTP,
    see [http://mng.bz/MWNQ](http://mng.bz/MWNQ). Having said that, at a basic level
    `genmove` and `play` will be enough to let your deep-learning bots play against
    GNU Go and Pachi.
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，与游戏玩法相关的另一个操作是`play`。这个命令用于让一个GTP机器人知道它需要在棋盘上走一步棋。例如，你可以通过发出`play black
    D4`来告诉GNU Go你想让它走D4的黑棋，这将返回一个`=`来确认这个命令。当两个机器人相互对战时，它们会轮流要求对方生成下一步棋，然后在它们自己的棋盘上`play`这一步棋。这一切都很直接——但我们省略了很多细节。一个完整的GTP客户端有很多更多的命令来处理，从处理让子棋到管理时间设置和计分规则。如果你对GTP的细节感兴趣，请参阅[http://mng.bz/MWNQ](http://mng.bz/MWNQ)。话虽如此，在基本层面上，`genmove`和`play`将足以让你的深度学习机器人与GNU
    Go和Pachi对战。
- en: To handle GTP and wrap your `Agent` concept so it can exchange Go moves by using
    this protocol, you create a new dlgo module called gtp. You can still try to follow
    the implementation alongside this main text, but from this chapter on, we suggest
    directly following our implementation on GitHub at [http://mng.bz/a4Wj](http://mng.bz/a4Wj).
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理GTP并包装你的`Agent`概念，以便可以通过使用此协议交换围棋走法，你创建了一个新的dlgo模块，名为gtp。你仍然可以尝试跟随本正文中的实现，但从本章开始，我们建议直接跟随我们在GitHub上的实现[http://mng.bz/a4Wj](http://mng.bz/a4Wj)。
- en: To start, let’s formalize what a GTP command is. To do so, we have to note that
    on many Go servers, commands get a sequence number to make sure that we can match
    commands and responses. These sequence numbers are optional and can be `None`.
    For us, a GTP command consists of a sequence number, a command, and potentially
    multiple arguments to that command. You place this definition in command.py in
    the gtp module.
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们正式化GTP命令的定义。为此，我们必须注意，在许多围棋服务器上，命令会得到一个序列号以确保我们可以匹配命令和响应。这些序列号是可选的，可以是`None`。对于我们来说，一个GTP命令由一个序列号、一个命令以及可能对该命令的多个参数组成。你将这个定义放在`command.py`中的`gtp`模块。
- en: Listing 8.12\. Python implementation of a GTP command
  id: totrans-1323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.12\. Python实现GTP命令
- en: '[PRE123]'
  id: totrans-1324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: Next, you want to parse text input from the command line into `Command`. For
    instance, parsing “999 play white D4” should result in `Command(999, 'play', ('white',
    'D4'))`. The `parse` function used for this goes into command.py as well.
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你想要将命令行中的文本输入解析为`Command`。例如，解析“999 play white D4”应该得到`Command(999, 'play',
    ('white', 'D4'))`。用于此的`parse`函数也包含在`command.py`中。
- en: Listing 8.13\. Parsing a GTP `Command` from plain text
  id: totrans-1326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.13\. 从纯文本解析GTP `Command`
- en: '[PRE124]'
  id: totrans-1327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '***1* GTP commands may start with an optional sequence number.**'
  id: totrans-1328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* GTP命令可以有一个可选的序列号。**'
- en: '***2* If the first piece isn’t numeric, there’s no sequence number.**'
  id: totrans-1329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* 如果第一个棋子不是数字，则没有序列号。**'
- en: We’ve just argued that GTP coordinates come in standard notation, so parsing
    GTP coordinates into `Board` positions and vice versa is simple. You define two
    helper functions to convert between coordinates and positions in board.py within
    gtp.
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚论证了GTP坐标以标准记号的形式出现，因此将GTP坐标解析为`Board`位置以及相反的操作是简单的。你定义了两个辅助函数，在gtp中的`board.py`内将坐标和棋盘位置之间进行转换。
- en: Listing 8.14\. Converting between GTP coordinates and your internal `Point`
    type
  id: totrans-1331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.14\. 在GTP坐标和你的内部`Point`类型之间进行转换
- en: '[PRE125]'
  id: totrans-1332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 8.5\. Competing against other bots locally
  id: totrans-1333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5\. 在本地与其他机器人竞争
- en: Now that you understand the basics of GTP, let’s dive right into an application
    and build a program that loads one of your bots and lets it compete against either
    GNU Go or Pachi. Before we present this program, we have just one technicality
    left to resolve—when our bot should resign a game or pass.
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了GTP的基础知识，让我们直接进入一个应用，并构建一个程序，该程序加载你的一个机器人，并让它与GNU Go或Pachi进行竞争。在我们介绍这个程序之前，我们还有一个技术问题需要解决——当我们的机器人应该辞职或通过时。
- en: 8.5.1\. When a bot should pass or resign
  id: totrans-1335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.1\. 机器人何时应该通过或辞职
- en: 'At the current development status, your deep-learning bots have no means of
    knowing when to stop playing. The way you designed them so far, your bot will
    always pick the best move to play. This can be detrimental toward the end of the
    game, when it might be better to pass or even resign when the situation looks
    a little too bad. For this reason, you’ll impose *termination strategies*: you’ll
    explicitly tell the bot when to stop. In [chapters 13](kindle_split_026.xhtml#ch13)
    and [14](kindle_split_027.xhtml#ch14), you’ll learn powerful techniques that’ll
    render this entirely useless (your bot will learn to judge the current board situation
    and thereby learn that sometimes it’s best to stop). But for now, this concept
    is useful and will help you on the way to deploy a bot against other opponents.'
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的开发状态下，你的深度学习机器人没有知道何时停止游戏的方法。到目前为止，你设计它们的方式是，你的机器人将始终选择最佳的行动。这可能在游戏的后期阶段是有害的，当情况看起来有点糟糕时，最好是通过或甚至辞职。因此，你会实施*终止策略*：你将明确告诉机器人何时停止。在第[13](kindle_split_026.xhtml#ch13)章和[14](kindle_split_027.xhtml#ch14)章中，你将学习到强大的技术，这将使这个概念完全无用（你的机器人将学会判断当前的棋盘情况，并因此学会有时最好停止）。但现在，这个概念是有用的，并将帮助你部署一个机器人对抗其他对手。
- en: You build the following `TerminationStrategy` in a file called termination.py
    in the agent module of dlgo. All it does is decide when you should pass or resign—and
    by default, you never pass or resign.
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: 你在dlgo的agent模块中的`termination.py`文件中构建以下`TerminationStrategy`。它所做的只是决定你何时应该通过或辞职——默认情况下，你永远不会通过或辞职。
- en: Listing 8.15\. A termination strategy tells your bot when to end a game
  id: totrans-1338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.15\. 终止策略告诉你的机器人何时结束游戏
- en: '[PRE126]'
  id: totrans-1339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: A simple heuristic for stopping game play is to pass when your opponent passes.
    You have to rely on the fact that your opponent knows when to pass, but it’s a
    start, and it works well against GNU Go and Pachi.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的停止游戏的经验法则是当对手通过时你也通过。你必须依赖对手知道何时通过的事实，但这是一个开始，并且它对GNU Go和Pachi非常有效。
- en: Listing 8.16\. Passing whenever an opponent passes
  id: totrans-1341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.16\. 当对手通过时通过
- en: '[PRE127]'
  id: totrans-1342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: In termination.py, you also find another strategy called `ResignLargeMargin`
    that resigns whenever the estimated score of the game goes too much in favor of
    the opponent. You can cook up many other such strategies, but keep in mind that
    ultimately you can get rid of this crutch with machine learning.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 在`termination.py`文件中，你还可以找到一个名为`ResignLargeMargin`的策略，该策略会在游戏评分估计值过于有利于对手时辞职。你可以想出许多其他这样的策略，但请记住，最终你可以通过机器学习摆脱这个拐杖。
- en: The last thing you need in order to let bots play against each other is to equip
    an `Agent` with a `TerminationStrategy` so as to pass and resign when appropriate.
    This `TerminationAgent` class goes into termination.py as well.
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让机器人相互对战，你需要给一个`Agent`配备一个`TerminationStrategy`，以便在适当的时候通过或辞职。这个`TerminationAgent`类也包含在`termination.py`中。
- en: Listing 8.17\. Wrapping an agent with a termination strategy
  id: totrans-1345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.17\. 使用终止策略包装代理
- en: '[PRE128]'
  id: totrans-1346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 8.5.2\. Let your bot play against other Go programs
  id: totrans-1347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.2\. 让你的机器人与其他围棋程序对战
- en: Having discussed termination strategies, you can now turn to pairing your Go
    bots with other programs. Under play_local.py in the gtp module, find a script
    that sets up a game between one of your bots and either GNU Go or Pachi. Go through
    this script step-by-step, starting with the necessary imports.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了终止策略之后，你现在可以转向将围棋机器人与其他程序配对。在gtp模块的`play_local.py`中，找到一个设置你的一个机器人与GNU Go或Pachi之间游戏的脚本。逐步执行这个脚本，从必要的导入开始。
- en: Listing 8.18\. Imports for your local bot runner
  id: totrans-1349
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.18\. 你本地机器人运行器的导入
- en: '[PRE129]'
  id: totrans-1350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: You should recognize most of the imports, with the exception of `SGFWriter`.
    This is a little utility class from dlgo.gtp.utils that keeps track of the game
    and writes an SGF file at the end.
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能识别大部分导入，除了`SGFWriter`。这是一个来自dlgo.gtp.utils的小工具类，它跟踪游戏并在结束时写入一个SGF文件。
- en: To initialize your game runner `LocalGtpBot`, you need to provide a deep-learning
    agent and optionally a termination strategy. Also, you can specify how many handicap
    stones should be used and which bot opponent should be played against. For the
    latter, you can choose between `gnugo` and `pachi`. `LocalGtpBot` will initialize
    either one of these programs as subprocesses, and both your bot and its opponent
    will communicate over GTP.
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化你的游戏运行器`LocalGtpBot`，你需要提供一个深度学习代理和可选的终止策略。你也可以指定应该使用多少贴子和应该与哪个机器人对手对战。对于后者，你可以选择`gnugo`和`pachi`。`LocalGtpBot`将初始化这些程序之一作为子进程，你的机器人和它的对手将通过GTP进行通信。
- en: Listing 8.19\. Initializing a runner to clash two bot opponents
  id: totrans-1353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.19\. 初始化一个运行器以对抗两个机器人对手
- en: '[PRE130]'
  id: totrans-1354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '***1* You initialize a bot from an agent and a termination strategy.**'
  id: totrans-1355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你从一个代理和一个终止策略初始化一个机器人。**'
- en: '***2* You play until the game is stopped by one of the players.**'
  id: totrans-1356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 你会一直玩，直到其中一个玩家停止游戏。**'
- en: '***3* At the end, you write the game to the provided file in SGF format.**'
  id: totrans-1357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 最后，你将游戏写入提供的文件，格式为SGF。**'
- en: '***4* Your opponent will either be GNU Go or Pachi.**'
  id: totrans-1358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 你的对手将是GNU Go或Pachi。**'
- en: '***5* You read and write GTP commands from the command line.**'
  id: totrans-1359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 你从命令行读取和写入GTP命令。**'
- en: One of the main methods used in the tool we’re demonstrating here is `command_and_response`,
    which sends out a GTP command and reads back the response for this command.
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示的工具中使用的最主要的方法之一是`command_and_response`，它发送一个GTP命令并读取该命令的响应。
- en: Listing 8.20\. Sending a GTP command and receiving a response
  id: totrans-1361
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.20\. 发送GTP命令并接收响应
- en: '[PRE131]'
  id: totrans-1362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Playing a game works as follows:'
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: 玩游戏的工作方式如下：
- en: Set up the board with the GTP `boardsize` command. You allow only 19 × 19 boards
    here, because your deep-learning bots are tailored to that.
  id: totrans-1364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GTP的`boardsize`命令设置棋盘。在这里，你只允许19×19的棋盘，因为你的深度学习机器人是为这个定制的。
- en: Set the right handicap in the `set_handicap` method.
  id: totrans-1365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`set_handicap`方法中设置正确的贴数。
- en: Play the game itself, which you’ll cover in the `play` method.
  id: totrans-1366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩游戏本身，你将在`play`方法中介绍。
- en: Persist the game record as an SGF file.
  id: totrans-1367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将游戏记录持久化为SGF文件。
- en: Listing 8.21\. Set up the board, let the opponents play the game, and persist
    it
  id: totrans-1368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.21\. 设置棋盘，让对手玩游戏，并持久化它
- en: '[PRE132]'
  id: totrans-1369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'The game-play logic for your bot clash is simple: while none of the opponents
    stop, take turns and continue to play moves. The bots do that in methods called
    `play_our_move` and `play_their_move`, respectively. You also clear the screen,
    and print out the current board situation and a crude estimate of the outcome.'
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器人对抗游戏玩法逻辑很简单：在对手没有停止的情况下，轮流继续走棋。机器人分别在`play_our_move`和`play_their_move`方法中这样做。你也会清除屏幕，并打印出当前的棋盘情况和粗略的结局估计。
- en: Listing 8.22\. A game ends when an opponent signals to stop it
  id: totrans-1371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.22\. 当对手发出停止信号时游戏结束
- en: '[PRE133]'
  id: totrans-1372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: Playing moves for your bot means asking it to generate a move with `select_move`,
    applying it to your board, and then translating the move and sending it over GTP.
    This needs special treatment for passing and resigning.
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的机器人走棋意味着要求它使用`select_move`生成一步棋，将其应用到你的棋盘上，然后将这步棋翻译并发送到GTP。这需要对认输和放弃进行特殊处理。
- en: Listing 8.23\. Asking your bot to generate and play a move that’s translated
    into GTP
  id: totrans-1374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.23\. 请求你的机器人生成并玩一个翻译成GTP的棋步
- en: '[PRE134]'
  id: totrans-1375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Letting your opponent play a move is structurally similar to your move. You
    ask GNU Go or Pachi to `genmove` a move, and you have to take care of converting
    the GTP response into a move that your bot understands. The only other thing you
    have to do is stop the game when your opponent resigns or both players pass.
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 让你的对手走棋的结构与你的走棋相似。你要求GNU Go或Pachi使用`genmove`走一步棋，你必须负责将GTP响应转换成你的机器人能理解的棋步。唯一其他你需要做的是，当对手认输或双方都认输时停止游戏。
- en: Listing 8.24\. Your opponent plays moves by responding to `genmove`
  id: totrans-1377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.24\. 你的对手通过响应`genmove`来走棋
- en: '[PRE135]'
  id: totrans-1378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: That concludes your play_local.py implementation, and you can now test it as
    follows.
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了你的play_local.py实现，你现在可以按照以下方式测试它。
- en: Listing 8.25\. Letting one of your bots loose on Pachi
  id: totrans-1380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.25\. 让你的一个机器人自由地在Pachi上玩
- en: '[PRE136]'
  id: totrans-1381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: You should see the way the game between the bots unfolds, as shown in [figure
    8.4](#ch08fig04).
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到机器人之间的游戏展开方式，如图 8.4 所示。
- en: Figure 8.4\. A snapshot of how Pachi and your bot see and evaluate a game between
    them
  id: totrans-1383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4\. Pachi 和你的机器人如何观察和评估他们之间的游戏的一个快照
- en: '![](Images/08fig04_alt.jpg)'
  id: totrans-1384
  prefs: []
  type: TYPE_IMG
  zh: '![图片 8.4 的替代文本](Images/08fig04_alt.jpg)'
- en: In the top part of the figure, you see the board printed by you, followed by
    your current estimate. In the lower half, you see Pachi’s game state (which is
    identical to yours) on the left, and on the right Pachi gives you an estimation
    of its current assessment of the game in terms of which part of the board it thinks
    belongs to which player.
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的上半部分，你看到你打印的棋盘，然后是你的当前估计。在下半部分，你看到 Pachi 的游戏状态（与你的相同）在左侧，右侧 Pachi 给你一个关于他认为棋盘哪一部分属于哪个玩家的游戏评估。
- en: This is a hopefully convincing and exciting demo of what your bot can do by
    now, but it’s not the end of the story. In the next section, we go one step further
    and show you how to connect your bot to a real-life Go server.
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个希望令人信服且令人兴奋的演示，展示了你的机器人现在可以做什么，但这并不是故事的结束。在下一节中，我们将更进一步，展示如何将你的机器人连接到真实的围棋服务器。
- en: 8.6\. Deploying a Go bot to an online Go server
  id: totrans-1387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6\. 将围棋机器人部署到在线围棋服务器
- en: Note that play_local.py is really a tiny Go server for two bot opponents to
    play against each other. It accepts and sends GTP commands and knows when to start
    and finish a game. This produces overhead, because the program takes the role
    of a referee that controls how the opponents interact.
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，play_local.py 实际上是一个为两个机器人对手相互对战而设计的微型的 Go 服务器。它接受并发送 GTP 命令，并知道何时开始和结束游戏。这会产生开销，因为程序扮演了裁判的角色，控制对手如何互动。
- en: If you want to connect a bot to an actual Go server, this server will take care
    of all the game-play logic, and you can focus entirely on sending and receiving
    GTP commands. On the one hand, your fate becomes easier because you have less
    to worry about. On the other hand, connecting to a proper Go server means that
    you have to make sure to support the full range of GTP commands supported by that
    server, because otherwise your bot may crash.
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要将一个机器人连接到一个实际的围棋服务器，这个服务器将处理所有的游戏逻辑，你可以完全专注于发送和接收 GTP 命令。一方面，你的命运变得更容易，因为你需要担心的事情更少。另一方面，连接到一个合适的围棋服务器意味着你必须确保支持该服务器支持的完整范围的
    GTP 命令，否则你的机器人可能会崩溃。
- en: To ensure that this doesn’t happen, let’s formalize the processing of GTP commands
    a little more. First, you implement a proper GTP response class for successful
    and failed commands.
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保这种情况不会发生，让我们更正式地处理 GTP 命令。首先，你实现一个适当的 GTP 响应类，用于成功和失败的命令。
- en: Listing 8.26\. Encoding and serializing a GTP response
  id: totrans-1391
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.26\. 编码和序列化 GTP 响应
- en: '[PRE137]'
  id: totrans-1392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '***1* Making a successful GTP response with response body**'
  id: totrans-1393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 创建一个带有响应体的成功 GTP 响应**'
- en: '***2* Making an error GTP response**'
  id: totrans-1394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* 创建一个错误的 GTP 响应**'
- en: '***3* Converting a Python Boolean into GTP**'
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3* 将 Python 布尔值转换为 GTP**'
- en: '***4* Serializing a GTP response as a string**'
  id: totrans-1396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4* 将 GTP 响应序列化为字符串**'
- en: This leaves you with implementing the main class for this section, `GTPFrontend`.
    You put this class into frontend.py in the gtp module. You need the following
    imports, including `command` and `response` from your gtp module.
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 这让你需要实现本节的主要类 `GTPFrontend`。你将这个类放入 gtp 模块中的 frontend.py 文件。你需要以下导入，包括来自你的 gtp
    模块的 `command` 和 `response`。
- en: Listing 8.27\. Python imports for your GTP frontend
  id: totrans-1398
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.27\. 你的 GTP 前端所需的 Python 导入
- en: '[PRE138]'
  id: totrans-1399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: To initialize a GTP frontend, you need to specify an `Agent` instance and an
    optional termination strategy. `GTPFrontend` will then instantiate a dictionary
    of GTP events that you process. Each of these events, which includes common commands
    like `play` and others, will have to be implemented by you.
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化一个 GTP 前端，你需要指定一个 `Agent` 实例和一个可选的终止策略。`GTPFrontend` 然后实例化一个 GTP 事件字典，你将处理这些事件。这些事件包括
    `play` 等常见命令，你将需要实现它们。
- en: Listing 8.28\. Initializing a `GTPFrontend`, which defines GTP event handlers
  id: totrans-1401
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.28\. 初始化 `GTPFrontend`，它定义了 GTP 事件处理器
- en: '[PRE139]'
  id: totrans-1402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: After you start a game with the following `run` method, you continually read
    GTP commands that are forwarded to the respective event handler, which is done
    by the `process` method.
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 在你使用以下 `run` 方法开始游戏后，你将不断地读取转发到相应事件处理器的 GTP 命令，这是通过 `process` 方法完成的。
- en: Listing 8.29\. The frontend parses from the input stream until the game ends
  id: totrans-1404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.29\. 前端解析输入流直到游戏结束
- en: '[PRE140]'
  id: totrans-1405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: What’s left to complete this `GTPFrontend` is the implementation of the individual
    GTP commands. The following listing shows the three most important ones; we refer
    you to the GitHub repository for the rest.
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: 完成`GTPFrontend`所剩无几的是实现单个GTP命令。以下列表显示了最重要的三个；其他部分请参考GitHub仓库。
- en: Listing 8.30\. A few of the most important event responses for your GTP frontend
  id: totrans-1407
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.30\. 你GTP前端最重要的几个事件响应
- en: '[PRE141]'
  id: totrans-1408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: You can now use this GTP frontend in a little script to start it from the command
    line.
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用这个GTP前端在一个小脚本中启动它，从命令行启动。
- en: Listing 8.31\. Starting your GTP interface from the command line
  id: totrans-1410
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.31\. 从命令行启动你的GTP接口
- en: '[PRE142]'
  id: totrans-1411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'After this program runs, you can use it in exactly the same way you tested
    GNU Go in [section 8.4](#ch08lev1sec4): you can throw GTP commands at it, and
    it’ll process them properly. Go ahead and test it by generating a move with `genmove`
    or printing out the board state with `showboard`. Any command covered in your
    event handler in `GTPFrontend` is feasible.'
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个程序运行后，你可以像在[第8.4节](#ch08lev1sec4)测试GNU Go一样使用它：你可以向它发送GTP命令，它将正确处理它们。你可以通过使用`genmove`生成走法或使用`showboard`打印出棋盘状态来测试它。在`GTPFrontend`的事件处理程序中覆盖的任何命令都是可行的。
- en: 8.6.1\. Registering a bot at the Online Go Server
  id: totrans-1413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.1\. 在在线围棋服务器上注册一个机器人
- en: Now that your GTP frontend is complete and works in the same way as GNU Go and
    Pachi locally, you can register your bots at an online platform that uses GTP
    for communication. You’ll find that most popular Go servers are based on GTP,
    and [appendix C](kindle_split_030.xhtml#app03) covers three of them explicitly.
    One of the most popular servers in Europe and North America is the Online Go Server
    (OGS). We’ve chosen OGS as the platform to show you how to run a bot, but you
    could do the same thing with most other platforms as well.
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的GTP前端已经完成并且与本地GNU Go和Pachi的工作方式相同，你可以在使用GTP进行通信的在线平台上注册你的机器人。你会发现大多数流行的围棋服务器都是基于GTP的，[附录C](kindle_split_030.xhtml#app03)明确介绍了其中三个。欧洲和北美最受欢迎的服务器之一是在线围棋服务器（OGS）。我们选择OGS作为平台来展示如何运行机器人，但你也可以用大多数其他平台做同样的事情。
- en: 'Because the registration process for your bot at OGS is somewhat involved and
    the piece of software that connects your bot to OGS is a tool written in JavaScript,
    we’ve put this part into [appendix E](kindle_split_032.xhtml#app05). You can either
    read this appendix now and come back here, or skip it if you’re not interested
    in running your own bot online. When you complete [appendix E](kindle_split_032.xhtml#app05),
    you’ll have learned the following skills:'
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你在OGS上注册机器人的过程有些复杂，而且连接你的机器人到OGS的软件是一个用JavaScript编写的工具，我们将这部分内容放在了[附录E](kindle_split_032.xhtml#app05)中。你可以现在阅读这个附录然后回来，或者如果你不感兴趣运行自己的机器人在线，也可以跳过它。当你完成[附录E](kindle_split_032.xhtml#app05)后，你将学会以下技能：
- en: Creating two accounts at OGS, one for your bot and one for you to administer
    your bot account
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在OGS上创建两个账户，一个用于你的机器人，另一个用于你管理你的机器人账户
- en: Connecting your bot to OGS from your local computer for testing purposes
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的机器人连接到本地计算机以进行测试
- en: Deploying your bot on an AWS instance to connect to OGS for as long as you wish
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的机器人部署在AWS实例上，以连接到OGS，持续的时间由你决定
- en: This will allow you to enter a (ranked) game against your own creation online.
    Also, everyone with an OGS account can play your bot at this point, which can
    be motivating to see. On top of that, your bot could even enter tournaments hosted
    on OGS!
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 这将允许你在线上与自己的创作进行（排名）游戏。此外，任何拥有OGS账户的人都可以在这个时候玩你的机器人，看到这一点可能会很有动力。更重要的是，你的机器人甚至可以参加在OGS上举办的锦标赛！
- en: 8.7\. Summary
  id: totrans-1420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7\. 摘要
- en: By building a deep network into your agent framework, you can make it so your
    models can interact with their environment.
  id: totrans-1421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在你的代理框架中构建深度网络，你可以让你的模型与环境进行交互。
- en: Registering an agent in a web application, by building an HTTP frontend, you
    can play against your own bots through a graphical interface.
  id: totrans-1422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过构建一个HTTP前端在Web应用程序中注册一个代理，你可以通过图形界面与自己的机器人进行对战。
- en: Using a cloud provider like AWS, you can rent compute power on a GPU to efficiently
    run your deep-learning experiments.
  id: totrans-1423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用像AWS这样的云服务提供商，你可以租用GPU上的计算能力来高效地运行你的深度学习实验。
- en: Deploying your web application on AWS, you can easily share your bot and let
    it play with others.
  id: totrans-1424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS上部署你的Web应用程序，你可以轻松地分享你的机器人，并让它与其他人进行对战。
- en: By letting your bot emit and receive Go Text Protocol (GTP) commands, it can
    play against other Go programs locally in a standardized way.
  id: totrans-1425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过让你的机器人发出和接收围棋文本协议（GTP）命令，它可以以标准化的方式在本地与其他围棋程序进行对战。
- en: Building a GTP frontend for your bot is the most important stepping stone to
    registering it at an online Go platform.
  id: totrans-1426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的机器人构建一个GTP前端是将其注册到在线围棋平台上的最重要步骤。
- en: Deploying a bot in the cloud, you can let it enter regular games and tournaments
    at the Online Go Server (OGS), and play against it yourself at any time.
  id: totrans-1427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云中部署机器人，您可以让它进入在线围棋服务器（OGS）的常规游戏和锦标赛，并且可以随时与之对弈。
- en: 'Chapter 9\. Learning by practice: reinforcement learning'
  id: totrans-1428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章：通过练习学习：强化学习
- en: '*This chapter covers*'
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Defining a task for reinforcement learning
  id: totrans-1430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义强化学习任务
- en: Building a learning agent for games
  id: totrans-1431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为游戏构建学习代理
- en: Collecting self-play experiences for training
  id: totrans-1432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集自我对弈经验用于训练
- en: I’ve probably read a dozen books on Go, all written by strong pros from China,
    Korea, and Japan. And yet I’m just an intermediate amateur player. Why haven’t
    I reached the level of these legendary players? Have I forgotten their lessons?
    I don’t think that’s it; I can practically recite Toshiro Kageyama’s *Lessons
    in the Fundamentals of Go* (Ishi Press, 1978) by heart. Maybe I just need to read
    more books....
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能读过关于围棋的十几本书，都是由来自中国、韩国和日本的强大职业选手所写。然而，我只是一个中级业余选手。为什么我没有达到这些传奇选手的水平？我是不是忘记了他们的教诲？我不认为是这样；我可以几乎背诵Toshiro
    Kageyama的《围棋基础教程》（Ishi Press，1978）的内容。也许我只是需要阅读更多的书籍....。
- en: 'I don’t know the full recipe for becoming a top Go star, but I know at least
    one difference between me and Go professionals: practice. A Go player probably
    clocks in five or ten thousand games before qualifying as a professional. Practice
    creates knowledge, and sometimes that’s knowledge that you can’t directly communicate.
    You can *summarize* that knowledge—that’s what makes it into Go books. But the
    subtleties get lost in the translation. If I expect to master the lessons I’ve
    read, I need to put in a similar level of practice.'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道成为顶级围棋明星的完整秘方，但我知道我和围棋职业选手至少有一点不同：练习。一个围棋选手可能在成为职业选手之前要下五到一万场比赛。练习创造知识，有时这是你无法直接传达的知识。你可以**总结**这些知识——这就是它成为围棋书籍的原因。但是，细微之处在翻译中丢失了。如果我期望掌握我所阅读的教训，我需要投入类似的练习水平。
- en: 'If practice is so valuable for humans, what about computers? Can a computer
    program learn by practicing? That’s the promise of *reinforcement learning*. In
    reinforcement learning (RL), you improve a program by having it repeatedly attempt
    a task. When it has good outcomes, you modify the program to repeat its decisions.
    When it has bad outcomes, you modify the program to avoid those decisions. This
    doesn’t mean you write new code after each trial: RL algorithms provide automated
    methods for making those modifications.'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: 如果练习对人类来说如此有价值，那么对于计算机呢？一个计算机程序能否通过练习来学习？这就是**强化学习**的承诺。在强化学习（RL）中，你通过让程序反复尝试一个任务来改进它。当它有好的结果时，你修改程序以重复其决策。当它有坏的结果时，你修改程序以避免那些决策。这并不意味着你在每次试验后都编写新的代码：RL算法提供了自动化的方法来执行这些修改。
- en: 'Reinforcement learning isn’t a free lunch. For one thing, it’s slow: your bot
    will need to play thousands of games in order to make a measurable improvement.
    In addition, the training process is fiddly and hard to debug. But if you put
    in the effort to make these techniques work for you, the payoff is huge. You can
    build software that applies sophisticated strategies to tackle a variety of tasks,
    even if you can’t describe those strategies yourself.'
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不是免费的午餐。一方面，它很慢：你的机器人需要玩数千场比赛才能实现可测量的改进。此外，训练过程很繁琐，难以调试。但如果你投入努力使这些技术为你所用，回报是巨大的。你可以构建应用复杂策略来解决各种任务的软件，即使你自己无法描述这些策略。
- en: This chapter starts with a birds-eye view of the reinforcement-learning cycle.
    Next, you’ll see how to set up a Go bot to play against itself in a way that fits
    into the reinforcement-learning process. [Chapter 10](kindle_split_022.xhtml#ch10)
    shows how to use the self-play data to improve your bot’s performance.
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从强化学习周期的鸟瞰图开始。接下来，你将看到如何设置一个围棋机器人，使其以适合强化学习过程的方式与自己对弈。[第10章](kindle_split_022.xhtml#ch10)展示了如何使用自我对弈数据来提高你的机器人性能。
- en: 9.1\. The reinforcement-learning cycle
  id: totrans-1438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1. 强化学习周期
- en: Many algorithms implement the mechanics of reinforcement learning, but they
    all work within a standard framework. This section describes the reinforcement-learning
    cycle, in which a computer program improves by repeatedly attempting a task. [Figure
    9.1](#ch09fig01) illustrates the cycle.
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法实现了强化学习的机制，但它们都在一个标准框架内工作。本节描述了强化学习周期，其中计算机程序通过反复尝试一个任务来提高。![图9.1](#ch09fig01)说明了这个周期。
- en: Figure 9.1\. The reinforcement-learning cycle. You can implement reinforcement
    learning in many ways, but the overall process has a common structure. First,
    a computer program attempts a task repeatedly. The records of these attempts are
    called *experience data*. Next, you modify the behavior to imitate the more successful
    attempts; this process is *training*. You then periodically evaluate the performance
    to confirm that the program is improving. Normally, you need to repeat this process
    for many cycles.
  id: totrans-1440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1. 强化学习周期。你可以用许多方式实现强化学习，但整体过程有一个共同的结构。首先，一个计算机程序反复尝试一个任务。这些尝试的记录被称为*经验数据*。接下来，你修改行为以模仿更成功的尝试；这个过程是*训练*。然后你定期评估性能以确认程序正在改进。通常，你需要重复这个过程许多周期。
- en: '![](Images/09fig01_alt.jpg)'
  id: totrans-1441
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig01_alt.jpg)'
- en: 'In the language of reinforcement learning, your Go bot is an *agent*: a program
    that makes decisions in order to accomplish a task. Earlier in the book, you implemented
    several versions of an `Agent` class that could choose Go moves. In those cases,
    you provided the agent with a situation—a `GameState` object—and it responded
    with a decision—a move to play. Although you weren’t using reinforcement learning
    at that time, the concept of an agent is the same.'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习的语言中，你的围棋机器人是一个*智能体*：一个为了完成任务而做出决策的程序。在本书的前面部分，你实现了几个版本的`Agent`类，它可以选择围棋走法。在那些情况下，你为智能体提供了一个情况——一个`GameState`对象——然后它响应一个决策——一个要下的走法。尽管当时你没有使用强化学习，但智能体的概念是相同的。
- en: The goal of reinforcement learning is to make the agent as effective as possible.
    In this case, you want your agent to win at Go.
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是使智能体尽可能有效。在这种情况下，你希望你的智能体在围棋中获胜。
- en: First, you have your Go bot play a batch of games against itself; during each
    game, it should record every turn and the final outcome. These game records are
    called its *experience*.
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让你的围棋机器人与自身玩一批游戏；在每场比赛中，它应该记录每一回合和最终结果。这些游戏记录被称为它的*经验*。
- en: 'Next, you *train* your bot by updating its behavior in response to what happened
    in its self-play games. This process is similar to training the neural networks
    covered in [chapters 6](kindle_split_018.xhtml#ch06) and [7](kindle_split_019.xhtml#ch07).
    The core idea is that you want the bot to repeat the decisions it made in games
    it won, and stop making the decisions it made in games it lost. The training algorithm
    comes as a package deal with the structure of your agent: you need to be able
    to systematically modify the behavior of your agent in order to train. There are
    many algorithms for doing this; we cover three in this book. In this chapter and
    the next, we start with the *policy gradient* algorithm. In [chapter 11](kindle_split_023.xhtml#ch11),
    we cover the *Q-learning* algorithm. [Chapter 12](kindle_split_024.xhtml#ch12)
    introduces the *actor-critic* algorithm.'
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你通过更新其在自我对弈游戏中发生的行为来*训练*你的机器人。这个过程类似于[第6章](kindle_split_018.xhtml#ch06)和[第7章](kindle_split_019.xhtml#ch07)中介绍的神经网络训练。核心思想是，你希望机器人重复它在赢得的游戏中做出的决策，并停止在它输掉的游戏中做出的决策。训练算法与你的智能体结构捆绑在一起：你需要能够系统地修改智能体的行为以便进行训练。有许多算法可以做到这一点；本书中我们介绍了三种。在本章和下一章中，我们首先介绍*策略梯度*算法。在第11章[第11章](kindle_split_023.xhtml#ch11)中，我们介绍了*Q学习*算法。[第12章](kindle_split_024.xhtml#ch12)介绍了*演员-评论家*算法。
- en: After training, you expect your bot to be a bit stronger. But there are many
    ways for the training process to go wrong, so it’s a good idea to evaluate the
    bot’s progress to confirm its strength. To evaluate a game-playing agent, have
    it play more games. You can pit your agent against earlier versions of itself
    to measure its progress. As a sanity check, you can also periodically compare
    your bot to other AIs or play against it yourself.
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，你期望你的机器人变得更强大一些。但是，训练过程有很多出错的方式，所以评估机器人的进度以确认其强度是个好主意。为了评估一个游戏智能体，让它玩更多的游戏。你可以让你的智能体与它的早期版本对弈来衡量其进步。作为一个理智的检查，你还可以定期将你的机器人与其他AI或自己对其玩来进行比较。
- en: 'Then you can repeat this entire cycle indefinitely:'
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以无限重复这个整个周期：
- en: Collect experience
  id: totrans-1448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集经验
- en: Train
  id: totrans-1449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练
- en: Evaluate
  id: totrans-1450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估
- en: We’ll break this cycle into multiple scripts. In this chapter, you’ll implement
    a `self_play` script that will simulate the self-play games and save the experience
    data to disk. In the next chapter, you’ll make a `train` script that takes the
    experience data as input, updates the agent accordingly, and saves the new agent.
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个循环分解成多个脚本。在本章中，你将实现一个`self_play`脚本，该脚本将模拟自我对弈游戏并将经验数据保存到磁盘。在下一章中，你将制作一个`train`脚本，该脚本将经验数据作为输入，相应地更新智能体，并保存新的智能体。
- en: 9.2\. What goes into experience?
  id: totrans-1452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 经验包含什么？
- en: 'In [chapter 3](kindle_split_014.xhtml#ch03), you designed a set of data structures
    for representing Go games. You can imagine how you could store an entire game
    record by using classes such as `Move`, `GoBoard`, and `GameState`. But reinforcement-learning
    algorithms are generic: they deal with a highly abstract representation of a problem,
    so that the same algorithms can apply to as many problem domains as possible.
    This section shows how to describe game records in the language of reinforcement
    learning.'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](kindle_split_014.xhtml#ch03)中，你设计了一套数据结构来表示围棋游戏。你可以想象如何使用`Move`、`GoBoard`和`GameState`等类来存储整个游戏记录。但是强化学习算法是通用的：它们处理问题的非常抽象的表示，这样相同的算法可以应用于尽可能多的问题域。本节将展示如何用强化学习的语言描述游戏记录。
- en: In the case of game playing, you can divide your experience into individual
    games, or *episodes*. An episode has a clear end, and decisions made during one
    episode have no bearing on what happens in the next. In other domains, you may
    not have any obvious way to divide the experience into episodes; for example,
    a robot that’s designed to operate continuously makes an endless sequence of decisions.
    You can still apply reinforcement learning to such problems, but the episode boundaries
    here make it a little simpler.
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏玩法的案例中，你可以将你的经验分成单个游戏，或称之为一*场景*。一个场景有一个明确的结束，一个场景中做出的决策不会影响下一个场景发生的事情。在其他领域，你可能没有明显的方法将经验分成场景；例如，一个设计用于连续操作的机器人会做出无限序列的决策。你仍然可以将强化学习应用于此类问题，但这里的场景边界使问题稍微简单一些。
- en: Within an episode, an agent is faced with a *state* of its environment. Based
    on the current state, the agent must select an *action*. After choosing an action,
    the agent sees a new state; the next state depends on both the chosen action and
    whatever else is going on in the environment. In the case of Go, your AI will
    see a board position (the state), and then select a legal move (an action). After
    that, the AI will see a new board position on its next turn (the next state).
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个场景中，智能体面临其环境的*状态*。基于当前状态，智能体必须选择一个*动作*。在选择动作后，智能体将看到一个新的状态；下一个状态取决于所选动作和环境中的其他任何活动。在围棋的情况下，你的AI将看到棋盘位置（状态），然后选择一个合法的移动（动作）。之后，AI将在其下一回合看到一个新的棋盘位置（下一个状态）。
- en: 'Note that after the agent chooses an action, the next state also includes the
    opponent’s move. You can’t determine the next state from the current state and
    the action you choose: you must also wait for the opponent’s move. The opponent’s
    behavior is part of the *environment* that your agent must learn to navigate.'
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在智能体选择一个动作后，下一个状态也包括对手的动作。你不能仅从当前状态和你选择的动作中确定下一个状态：你还必须等待对手的动作。对手的行为是智能体必须学会导航的*环境*的一部分。
- en: In order to improve, your agent needs feedback about whether it’s achieving
    its objective. You provide that feedback by calculating its *reward*, a numerical
    score for meeting a goal. For your Go AI, the goal is to win a game, so you’ll
    communicate a reward of 1 each time it wins and –1 each time it loses. Reinforcement-learning
    algorithms will modify the agent’s behavior so as to increase the amount of reward
    it accumulates. [Figure 9.2](#ch09fig02) illustrates how a game of Go can be described
    with states, actions, and rewards.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高，你的智能体需要关于它是否达到目标的反馈。你通过计算其*奖励*来提供这种反馈，这是一个用于达成目标的数值分数。对于你的围棋AI，目标是赢得游戏，所以每次它获胜时，你将传达一个奖励1，每次它失败时，你将传达一个奖励-1。强化学习算法将修改智能体的行为，以便增加它积累的奖励量。[图9.2](#ch09fig02)说明了如何用状态、动作和奖励来描述围棋游戏。
- en: Figure 9.2\. A game of 5 × 5 Go translated into the language of reinforcement
    learning. The agent that you want to train is the black player. It sees a sequence
    of states (board positions) and chooses actions (legal moves). At the end of an
    episode (a complete game), it gets a reward to indicate whether it achieved its
    goal. In this case, black wins the game, so the agent sees a reward of +1.
  id: totrans-1458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.2\. 将 5 × 5 Go 游戏翻译成强化学习语言。你想要训练的智能体是黑方玩家。它看到一系列状态（棋盘位置）并选择动作（合法棋步）。在游戏结束（完整游戏）时，它会获得一个奖励以指示它是否实现了目标。在这种情况下，黑方赢得了游戏，所以智能体看到一个
    +1 的奖励。
- en: '![](Images/09fig02.jpg)'
  id: totrans-1459
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09fig02.jpg)'
- en: 'Go and similar games are special cases: the reward comes all at once, at the
    end of the game. And there are only two possible rewards: you win or you lose,
    and you don’t care about what else happens in the game. In other domains, the
    reward may be spread out. Imagine making an AI to play Scrabble. On each turn,
    the AI will place a word and score points, and then its opponent will do the same.
    In that case, you can compute a positive reward for the AI’s points, and a negative
    reward for the opponent’s points. Then the AI doesn’t have to wait all the way
    to the end of an episode for its reward; it gets little pieces of its reward after
    every action it takes.'
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: Go 和类似的游戏是特殊情况：奖励在游戏结束时一次性到来。而且只有两种可能的奖励：你赢了或你输了，你不在乎游戏中发生的其他事情。在其他领域，奖励可能分散。想象一下制作一个
    AI 来玩 Scrabble。在每一轮，AI 都会放置一个单词并得分，然后对手也会这样做。在这种情况下，你可以为 AI 的得分计算一个正奖励，为对手的得分计算一个负奖励。然后
    AI 不必等到游戏结束才能获得奖励；它在每次采取行动后都会获得其奖励的一部分。
- en: A key idea in reinforcement learning is that an action may be responsible for
    a reward that comes much later. Imagine you make an especially clever play on
    move 35 of a game, and continue on to win after 200 moves. Your good move early
    on deserves at least some of the credit for the win. You must somehow split up
    the credit for the reward over all the moves in the game. The future reward that
    your agent sees after an action is called the *return* on that action. To compute
    the return on an action, you add up all the rewards the agent saw after that action,
    all the way to the end of the episode, as shown in [listing 9.1](#ch09ex01). This
    is a way of saying that you don’t know, in advance, which moves are responsible
    for winning or losing. The onus is on the learning algorithm to split up the credit
    or blame among individual moves.
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的一个关键思想是，一个动作可能对很久以后才到来的奖励负责。想象一下，你在游戏的第 35 棋步上做出了一步特别聪明的棋，并在接下来的 200 棋步中继续获胜。你早期的良好棋步至少应该得到一些胜利的功劳。你必须以某种方式将奖励的功劳分配到游戏中所有棋步上。你的智能体在动作之后看到的未来奖励被称为该动作的*回报*。为了计算动作的回报，你需要将智能体在该动作之后看到的所有奖励加起来，直到游戏结束，如[列表
    9.1](#ch09ex01)所示。这表示你事先不知道哪些棋步导致了胜利或失败。责任在于学习算法在各个单独的棋步之间分配功劳或责任。
- en: Listing 9.1\. Calculating return on an action
  id: totrans-1462
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1\. 计算动作的回报
- en: '[PRE143]'
  id: totrans-1463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '***1* reward[i] is the reward the agent saw immediately after action i.**'
  id: totrans-1464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* reward[i] 是智能体在动作 i 后立即看到的奖励。**'
- en: '***2* Loops over all future rewards and adds them into the return**'
  id: totrans-1465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历所有未来奖励并将它们加到回报中**'
- en: That assumption doesn’t make sense for every problem. Consider our Scrabble
    example again. The decisions you make on your first turn could plausibly affect
    your score on your third turn—maybe you held a high-scoring X in reserve until
    you could combine it with a bonus square. But it’s hard to see how decisions on
    your third turn could affect your twentieth. To represent this concept in your
    return calculation, you can compute a weighted sum of the future rewards from
    each action. The weights should get smaller as you go further from the action,
    so that far-future rewards have less influence than immediate rewards.
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
  zh: 这种假设并不适用于每个问题。再次考虑我们的 Scrabble 例子。你在第一轮做出的决定可能合理地影响你第三轮的得分——也许你保留了一个高分 X，直到你可以与一个加分方格结合。但很难想象你在第三轮做出的决定如何影响你的第二十轮。为了在回报计算中表达这个概念，你可以计算每个动作的未来奖励的加权总和。随着你离动作越来越远，权重应该越来越小，这样远期奖励的影响力就比即时奖励小。
- en: This technique is called *discounting* the reward. [Listing 9.2](#ch09ex02)
    shows how to calculate discounted returns. In that example, each action gets full
    credit for the reward that comes immediately after. But the reward from the next
    step counts for only 75% as much; the reward two steps out counts 75% × 75% =
    56% as much; and so on. The choice of 75% is just an example; the correct discount
    rate will depend on your particular domain, and you may need to experiment a bit
    to find the most effective number.
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术被称为**折现**奖励。[代码清单9.2](#ch09ex02)展示了如何计算折现回报。在那个例子中，每个动作都获得了紧随其后的奖励的全部信用。但下一个步骤的奖励只占75%的重要性；两步之外的奖励占75%
    × 75% = 56%的重要性；以此类推。75%的选择只是一个例子；正确的折现率将取决于你的特定领域，你可能需要做一些实验来找到最有效的数字。
- en: Listing 9.2\. Calculating discounted returns
  id: totrans-1468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单9.2\. 计算折现回报
- en: '[PRE144]'
  id: totrans-1469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '***1* The discount _amount gets smaller and smaller as you get further from
    the original action.**'
  id: totrans-1470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 折现**金额随着你离原始动作越来越远而越来越小**。'
- en: In the case of building a Go AI, the only possible reward is a win or loss.
    This lets you take a shortcut in the return calculation. When your agent wins,
    every action in the game has a return of 1\. When your agent loses, every action
    has a return of –1.
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建围棋AI的情况下，唯一的可能奖励是胜利或失败。这让你在回报计算中可以走捷径。当你的智能体获胜时，游戏中每个动作的回报都是1。当你的智能体失败时，每个动作的回报都是-1。
- en: 9.3\. Building an agent that can learn
  id: totrans-1472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 构建一个能够学习的智能体
- en: Reinforcement learning can’t create a Go AI, or any other kind of agent, out
    of thin air. It can only *improve* a bot that already works within the parameters
    of the game. To get started, you need an agent that can at least complete a game.
    This section shows how to create a Go bot that selects moves by using a neural
    network. If you start with an untrained network, the bot will play as badly as
    your original `RandomAgent` from [chapter 3](kindle_split_014.xhtml#ch03). Later,
    you can improve this neural network through reinforcement learning.
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不能凭空创造出围棋AI或任何其他类型的智能体。它只能**改进**已经在游戏参数内运行的机器人。要开始，你需要一个至少能够完成游戏的智能体。本节将展示如何创建一个通过使用神经网络选择走法的围棋机器人。如果你从一个未训练的神经网络开始，机器人将表现得和你的原始`RandomAgent`（来自第3章）一样糟糕。之后，你可以通过强化学习来改进这个神经网络。
- en: 'A *policy* is a function that selects an action from a given state. In earlier
    chapters, you saw several implementations of the `Agent` class that have a `select_move`
    function. Each of those `select_move` functions is a policy: a game state comes
    in, and a move comes out. All the policies you’ve implemented so far are valid,
    in the sense that they produce legal moves. But they’re not equally good: the
    `MCTSAgent` from [chapter 4](kindle_split_016.xhtml#ch04) will defeat the `RandomAgent`
    from [chapter 3](kindle_split_014.xhtml#ch03) more often than not. If you want
    to improve one of these agents, you need to think of an improvement to the algorithm,
    write new code, and test it—the standard software development process.'
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略**是一个从给定状态中选择动作的函数。在早期章节中，你看到了几个`Agent`类的实现，它们都有一个`select_move`函数。每个`select_move`函数都是一个策略：一个游戏状态进来，一个走法出来。你迄今为止实现的所有策略都是有效的，从它们产生合法走法的意义上说。但它们并不同等有效：第4章中的`MCTSAgent`将比第3章中的`RandomAgent`更频繁地击败它。如果你想改进这些智能体之一，你需要考虑算法的改进，编写新的代码，并对其进行测试——这是标准的软件开发过程。'
- en: 'To use reinforcement learning, you need a policy that you can update automatically,
    using another computer program. In [chapter 6](kindle_split_018.xhtml#ch06), you
    studied a class of functions that lets you do exactly that: convolutional neural
    networks. A deep neural network can compute sophisticated logic, and you can modify
    its behavior by using the gradient descent algorithm.'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用强化学习，你需要一个可以自动更新的策略，使用另一个计算机程序。在第6章中，你学习了一类函数，它允许你做到这一点：卷积神经网络。深度神经网络可以计算复杂的逻辑，你可以通过使用梯度下降算法来修改其行为。
- en: The move-prediction neural network you designed in [chapters 6](kindle_split_018.xhtml#ch06)
    and [7](kindle_split_019.xhtml#ch07) outputs a vector with a value for each point
    on the board; the value represents the network’s confidence that point would be
    the next play. How can you form a policy from such an output? One way is to simply
    select the move with the highest value. This will produce good results if your
    network has already been trained to select good moves. But it’ll always select
    the same move for any given board position. This creates a problem for reinforcement
    learning. To improve through reinforcement learning, you need to select a variety
    of moves. Some will be better, and some will be worse; you can detect the good
    moves by looking at the outcomes they produce. But you need the variety in order
    to improve.
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第 6 章和第 7 章中设计的移动预测神经网络为棋盘上的每个点输出一个值向量；该值表示网络对该点将是下一个游戏的信心。你如何从这样的输出中形成一个策略？一种方法就是简单地选择具有最高值的移动。如果你的网络已经被训练来选择好的移动，这将产生良好的结果。但它会为任何给定的棋盘位置选择相同的移动。这为强化学习带来了问题。为了通过强化学习进行改进，你需要选择各种移动。一些会更好，一些会更差；你可以通过查看它们产生的结果来检测好的移动。但你需要这种多样性才能进行改进。
- en: Instead of always selecting the highest-rated move, you want a stochastic policy.
    Here, *stochastic* means that if you input the exact same board position twice,
    your agent may select different moves. This involves randomness, but not in the
    same way as your `RandomAgent` from [chapter 3](kindle_split_014.xhtml#ch03).
    The `RandomAgent` chose moves with no regard to what was happening in the game.
    A stochastic policy means that your move selection will depend on the state of
    the board, but it won’t be 100% predictable.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: 你不希望总是选择评分最高的移动，你想要一个随机策略。在这里，*随机*意味着如果你两次输入完全相同的棋盘位置，你的智能体可能会选择不同的移动。这涉及到随机性，但与你的第
    3 章中的 `RandomAgent` 不同。`RandomAgent` 选择移动时，不考虑游戏中的情况。随机策略意味着你的移动选择将取决于棋盘的状态，但不会是
    100% 可预测的。
- en: 9.3.1\. Sampling from a probability distribution
  id: totrans-1478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1\. 从概率分布中进行采样
- en: For any board position, your neural network will give you a vector with one
    element for each board position. To create a policy from this, you can treat each
    element of the vector as indicating the probability that you select a particular
    move. This section shows how to select moves according to those probabilities.
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何棋盘位置，你的神经网络将为你提供一个向量，其中每个棋盘位置都有一个元素。要从这个向量中创建一个策略，你可以将向量的每个元素视为选择特定移动的概率。本节将展示如何根据这些概率选择移动。
- en: 'For example, if you’re playing rock-paper-scissors, you could follow a policy
    of choosing rock 50% of the time, paper 30% of the time, and scissors 20% of the
    time. The 50%-30%-20% split is a *probability distribution* over the three choices.
    Note that probabilities sum to exactly 100%: this is because your policy must
    always choose exactly one item from the list. This is a necessary property of
    a probability distribution; a 50%-30%-10% policy would leave you with no decision
    10% of the time.'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你在玩剪刀石头布，你可以遵循以下策略：50% 的时间选择石头，30% 的时间选择纸，20% 的时间选择剪刀。50%-30%-20% 的分配是三种选择上的
    *概率分布*。请注意，概率总和正好是 100%：这是因为你的策略必须始终从列表中选择一个项目。这是概率分布的必要属性；50%-30%-10% 的策略会在 10%
    的时间里让你无法做出决定。
- en: The process of randomly selecting one of those items in those proportions is
    called *sampling* from that probability distribution. The following listing shows
    a Python function that will choose one of those options according to that policy.
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种比例随机选择这些项目之一的过程称为从该概率分布中进行 *采样*。以下列表展示了一个 Python 函数，该函数将根据该策略选择这些选项之一。
- en: Listing 9.3\. An example of sampling from a probability distribution
  id: totrans-1482
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.3\. 从概率分布中进行采样的示例
- en: '[PRE145]'
  id: totrans-1483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: Try this snippet out a few times and see how it behaves. You’ll see `rock` more
    than `paper`, and `paper` more than `scissors`. But all three will appear regularly.
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行这个片段几次，看看它的表现。你会发现 `rock` 比 `paper` 出现得多，而 `paper` 比 `scissors` 出现得多。但所有三种都会定期出现。
- en: This logic for sampling from a probability distribution is built into NumPy
    as the `np.random.choice` function. The following listing shows the exact same
    behavior implemented with NumPy.
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率分布中进行采样的逻辑已经内置到 NumPy 中的 `np.random.choice` 函数。以下列表展示了使用 NumPy 实现的相同行为。
- en: Listing 9.4\. Sampling from a probability distribution with NumPy
  id: totrans-1486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.4\. 使用 NumPy 从概率分布中进行采样
- en: '[PRE146]'
  id: totrans-1487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: In addition, `np.random.choice` will handle *repeated* sampling from the same
    distribution. It’ll sample from your distribution once, remove that item from
    the list, and sample again from the remaining items. In this way, you get a semirandom
    ordered list. The high-probability items are likely to appear near the front of
    the list, but some variety remains. The following listing shows how to get repeated
    sampling with `np.random.choice`. You pass `size=3` to indicate that you want
    three different items, and `replace=False` to indicate that you don’t want any
    results repeated.
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`np.random.choice` 将处理从同一分布的**重复**采样。它将从你的分布中采样一次，从列表中移除该项目，然后再次从剩余的项目中采样。这样，你得到一个半随机排序的列表。高概率的项目可能出现在列表的前面，但仍然有一些多样性。以下列表显示了如何使用
    `np.random.choice` 进行重复采样。你传递 `size=3` 来表示你想要三个不同的项目，并且传递 `replace=False` 来表示你不想有任何重复的结果。
- en: Listing 9.5\. Repeatedly sampling from a probability distribution with NumPy
  id: totrans-1489
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.5\. 使用 NumPy 从概率分布中重复采样
- en: '[PRE147]'
  id: totrans-1490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: The repeated sampling will be useful in case your Go policy recommends an invalid
    move. In that case, you’ll want to select another one. You can call `np.random.choice`
    once and then just work your way down the list it generates.
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: 重复采样在围棋策略建议无效走法时将很有用。在这种情况下，你可能想要选择另一个走法。你可以调用一次 `np.random.choice`，然后只需按照它生成的列表向下工作。
- en: 9.3.2\. Clipping a probability distribution
  id: totrans-1492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2\. 剪切概率分布
- en: 'The reinforcement-learning process can be fairly unstable, especially early
    on. The agent may overreact to a few chance wins and temporarily assign a high
    probability to moves that really aren’t that good. (In that respect, it’s not
    unlike human beginners!) It’s possible for the probability for a particular move
    to go all the way to 1\. This creates a subtle problem: because your agent will
    always select the same move, it has no opportunity to unlearn it.'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习过程可能相当不稳定，尤其是在早期。代理可能会对几次偶然的胜利反应过度，并暂时将很高的概率分配给实际上并不那么好的走法。（在这方面，它与人类初学者没有太大区别！）某个特定走法的概率可能会达到
    1。这会创建一个微妙的问题：因为你的代理将始终选择相同的走法，它没有机会去学习它。
- en: To prevent this, you’ll *clip* the probability distribution to make sure no
    probabilities get pushed all the way to 0 or 1\. You did the same with the `DeepLearningAgent`
    from [chapter 8](kindle_split_020.xhtml#ch08). The `np.clip` function from NumPy
    handles most of the work here.
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况，你需要**剪切**概率分布，以确保没有概率被推到 0 或 1。你已经在第 8 章（[chapter 8](kindle_split_020.xhtml#ch08)）中的
    `DeepLearningAgent` 做过同样的操作。NumPy 的 `np.clip` 函数在这里处理了大部分工作。
- en: Listing 9.6\. Clipping a probability distribution
  id: totrans-1495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.6\. 剪切概率分布
- en: '[PRE148]'
  id: totrans-1496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '***1* Ensure that the result is still a valid probability distribution.**'
  id: totrans-1497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 确保结果仍然是一个有效的概率分布。**'
- en: 9.3.3\. Initializing an agent
  id: totrans-1498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.3\. 初始化代理
- en: Let’s start building out a new type of agent, a `PolicyAgent`, that selects
    moves according to a stochastic policy and can learn from experience data. This
    model can be identical to the move-prediction model from [chapters 6](kindle_split_018.xhtml#ch06)
    and [7](kindle_split_019.xhtml#ch07); the only difference is in how you train
    it. You’ll add this to your dlgo library in the dlgo/agent/pg.py module.
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建一种新的代理类型，即 `PolicyAgent`，它根据随机策略选择走法，并可以从经验数据中学习。这个模型可以与第 6 章（[chapters
    6](kindle_split_018.xhtml#ch06)）和第 7 章（[chapters 7](kindle_split_019.xhtml#ch07)）中的走法预测模型相同；唯一的区别在于你如何训练它。你将在
    dlgo/agent/pg.py 模块中将这个添加到你的 dlgo 库中。
- en: Recall from the previous chapters that your model needs a matching board-encoding
    scheme. The `PolicyAgent` class can accept the model and board encoder in the
    constructor. This creates a nice separation of concerns. The `PolicyAgent` class
    is responsible for selecting moves according to the model and changing its behavior
    in response to its experience. But it can ignore the details of the model structure
    and the board-encoding scheme.
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾前几章，你的模型需要一个匹配的棋盘编码方案。`PolicyAgent` 类可以在构造函数中接受模型和棋盘编码器。这创建了一个很好的关注点分离。`PolicyAgent`
    类负责根据模型选择走法，并根据其经验改变其行为。但它可以忽略模型结构和棋盘编码方案的细节。
- en: Listing 9.7\. The constructor for the `PolicyAgent` class
  id: totrans-1501
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.7\. `PolicyAgent` 类的构造函数
- en: '[PRE149]'
  id: totrans-1502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '***1* A Keras Sequential model instance**'
  id: totrans-1503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 一个 Keras Sequential 模型实例**'
- en: '***2* Implements the Encoder interface**'
  id: totrans-1504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 实现编码器接口**'
- en: To start the reinforcement-learning process, you first construct a board encoder,
    then a model, and finally the agent. The following listing shows this process.
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始强化学习过程，你首先构建一个棋盘编码器，然后是一个模型，最后是代理。以下列表显示了此过程。
- en: Listing 9.8\. Constructing a new learning agent
  id: totrans-1506
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.8\. 构建一个新的学习代理
- en: '[PRE150]'
  id: totrans-1507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '***1* Builds a Sequential model out of the layers described in dlgo.networks.large
    (covered in [chapter 6](kindle_split_018.xhtml#ch06))**'
  id: totrans-1508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用dlgo.networks.large中描述的层构建一个Sequential模型（在第6章中介绍）**'
- en: '***2* Adds an output layer that will return a probability distribution over
    points on the board**'
  id: totrans-1509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 添加一个输出层，该层将返回棋盘上各点的概率分布**'
- en: 'When you construct an agent like this, using a newly created model, Keras initializes
    the model weights to small, random values. At this point, the agent’s policy will
    be close to *uniform random*: it’ll choose any valid move with roughly equal probability.
    Later, training the model will add structure to its decisions.'
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用新创建的模型构建这样的代理时，Keras将初始化模型权重为小的随机值。在这个时候，代理的策略将接近*均匀随机*：它将以大致相等的概率选择任何有效的移动。后来，训练模型将为它的决策添加结构。
- en: 9.3.4\. Loading and saving your agent from disk
  id: totrans-1511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.4\. 从磁盘加载和保存你的代理
- en: The reinforcement-learning process can continue indefinitely; you may spend
    days or even weeks training your bot. You’ll want to periodically persist your
    bot to disk so you can start and stop the training process, and compare its performance
    at different points in the training cycle.
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习过程可以无限期进行；你可能需要花费几天甚至几周的时间来训练你的机器人。你可能会定期将你的机器人持久化到磁盘，以便你可以开始和停止训练过程，并在训练周期的不同点比较其性能。
- en: You can use the HDF5 file format, which we introduced in [chapter 8](kindle_split_020.xhtml#ch08),
    to store your agent. The HDF5 format is a convenient way to store numerical arrays,
    and it integrates nicely with NumPy and Keras.
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用我们在[第8章](kindle_split_020.xhtml#ch08)中介绍的HDF5文件格式来存储你的代理。HDF5格式是存储数值数组的一种方便方式，并且与NumPy和Keras集成良好。
- en: A `serialize` method on your `PolicyAgent` class can persist its encoder and
    model to disk, which is enough to re-create the agent.
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的`PolicyAgent`类上实现一个`serialize`方法可以将它的编码器和模型持久化到磁盘，这足以重新创建代理。
- en: Listing 9.9\. Serializing a `PolicyAgent` to disk
  id: totrans-1515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.9\. 将`PolicyAgent`序列化到磁盘
- en: '[PRE151]'
  id: totrans-1516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '***1* Stores enough information to reconstruct the board encoder**'
  id: totrans-1517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 存储足够的信息来重建棋盘编码器**'
- en: '***2* Uses built-in Keras features to persist the model and its weights**'
  id: totrans-1518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用内置的Keras功能来持久化模型及其权重**'
- en: The `h5file` argument could be an `h5py.File` object, or it could be a group
    inside an `h5py.File`. This allows you to bundle other data with the agent in
    a single HDF5 file.
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: '`h5file`参数可以是`h5py.File`对象，也可以是`h5py.File`内部的组。这允许你将其他数据与代理捆绑在一个HDF5文件中。'
- en: To use this `serialize` method, you first create a new HDF5 file, and then pass
    in the file handle.
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个`serialize`方法，你首先创建一个新的HDF5文件，然后传递文件句柄。
- en: Listing 9.10\. An example of using the `serialize` function
  id: totrans-1521
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.10\. 使用`serialize`函数的示例
- en: '[PRE152]'
  id: totrans-1522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: Then a corresponding `load_policy_agent` function reverses the procedure.
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: 然后相应的`load_policy_agent`函数将执行相反的操作。
- en: Listing 9.11\. Loading a policy agent from a file
  id: totrans-1524
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.11\. 从文件中加载策略代理
- en: '[PRE153]'
  id: totrans-1525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '***1* Uses built-in Keras functions to load the model structure and weights**'
  id: totrans-1526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用内置的Keras函数来加载模型结构和权重**'
- en: '***2* Recovers the board encoder**'
  id: totrans-1527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 恢复棋盘编码器**'
- en: '***3* Reconstructs the agent**'
  id: totrans-1528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 重建代理**'
- en: 9.3.5\. Implementing move selection
  id: totrans-1529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.5\. 实现移动选择
- en: 'The `PolicyAgent` needs one more function before you can begin self-play: the
    `select_move` implementation. This function will look similar to the `select_move`
    function you added to the `DeepLearningAgent` from [chapter 8](kindle_split_020.xhtml#ch08).
    The first step is to encode the board as a tensor (a stack of matrices; see [appendix
    A](kindle_split_028.xhtml#app01)) suitable for feeding into the model. Next, you
    feed the board tensor to the model and get back a probability distribution of
    the moves. You then clip the distribution to make sure no probability goes all
    the way to 1 or 0\. [Figure 9.3](#ch09fig03) illustrates the flow of this process.
    [Listing 9.12](#ch09ex12) shows how to implement these steps.'
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
  zh: 在你可以开始自我对弈之前，`PolicyAgent`需要另一个函数：`select_move`实现。这个函数将类似于你在[第8章](kindle_split_020.xhtml#ch08)中添加到`DeepLearningAgent`的`select_move`函数。第一步是将棋盘编码为适合输入模型的张量（矩阵堆栈；参见[附录A](kindle_split_028.xhtml#app01)）。接下来，你将棋盘张量输入到模型中，并得到移动的概率分布。然后你剪辑分布，以确保没有概率完全达到1或0。图9.3([#ch09fig03](#ch09fig03))说明了这个过程。列表9.12([#ch09ex12](#ch09ex12))展示了如何实现这些步骤。
- en: Listing 9.12\. Selecting a move with a neural network
  id: totrans-1531
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.12\. 使用神经网络选择移动
- en: '[PRE154]'
  id: totrans-1532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '***1* The Keras predict call makes batch predictions, so you wrap your single
    board in an array and pull out the first item from the resulting array.**'
  id: totrans-1533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* Keras的predict调用进行批量预测，所以你需要将单个板包裹在一个数组中，并从结果数组中提取第一个元素。**'
- en: '***2* Creates an array containing the index of every point on the board**'
  id: totrans-1534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个包含棋盘上每个点索引的数组**'
- en: '***3* Samples from the points on the board according to the policy, creates
    a ranked list of points to try**'
  id: totrans-1535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 根据策略从板上的点中抽取3个样本，创建一个要尝试的点的排名列表**'
- en: '***4* Loops over each point, checks if it’s a valid move, and picks the first
    valid on**'
  id: totrans-1536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 遍历每个点，检查它是否是有效的走法，并选择第一个有效的走法**'
- en: '***5* If you fall through here, there are no reasonable moves left.**'
  id: totrans-1537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 如果你从这里掉下来，就没有合理的走法了。**'
- en: Figure 9.3\. The move-selection process. First you encode a game state as a
    numerical tensor; then you can pass that tensor to your model to get move probabilities.
    You sample from all points on the board according to the move probabilities to
    get an order in which to try the moves.
  id: totrans-1538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3\. 移动选择过程。首先，将游戏状态编码为一个数值张量；然后，你可以将这个张量传递给你的模型以获取移动概率。你根据移动概率从棋盘上的所有点中进行采样，以获得尝试移动的顺序。
- en: '![](Images/09fig03_alt.jpg)'
  id: totrans-1539
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09fig03_alt.jpg)'
- en: '9.4\. Self-play: how a computer program practices'
  id: totrans-1540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. 自对弈：计算机程序如何练习
- en: Now that you have a learning agent capable of completing a game, you can begin
    collecting experience data. For a Go AI, this means playing thousands of games.
    This section shows how to implement this process. First, we describe some data
    structures to make handling experience data more convenient. Next, we show how
    to implement the self-play driver program.
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有一个能够完成游戏的智能体，你可以开始收集经验数据。对于一个围棋AI来说，这意味着要玩成千上万场比赛。本节展示了如何实现这个过程。首先，我们描述了一些数据结构，以便更方便地处理经验数据。接下来，我们展示了如何实现自对弈驱动程序。
- en: 9.4.1\. Representing experience data
  id: totrans-1542
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.4.1\. 表示经验数据
- en: 'Experience data contains three parts: states, actions, and rewards. To help
    keep these organized, you can create a single data structure that holds all three
    of these together.'
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
  zh: 经验数据包含三个部分：状态、动作和奖励。为了帮助保持这些内容的组织，你可以创建一个单一的数据结构来保存这三个部分。
- en: 'The `ExperienceBuffer` class is a minimal container for an experience data
    set. It has three attributes: `states`, `actions`, and `rewards`. All of these
    are represented as NumPy arrays; your agent will be responsible for encoding its
    states and actions as numerical structures. The `ExperienceBuffer` is nothing
    more than a container for passing the data set around. Nothing in this implementation
    is specific to policy gradient learning; you can reuse this class with other RL
    algorithms in later chapters. So you’ll add this class to the dlgo/rl/experience.py
    module.'
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExperienceBuffer`类是一个经验数据集的最小容器。它有三个属性：`states`、`actions`和`rewards`。所有这些都以NumPy数组的形式表示；你的智能体将负责将其状态和动作编码为数值结构。`ExperienceBuffer`只是一个用于传递数据集的容器。在这个实现中，没有任何内容是特定于策略梯度学习的；你可以在后续章节中使用这个类与其他RL算法。因此，你需要将这个类添加到dlgo/rl/experience.py模块中。'
- en: Listing 9.13\. Constructor for an experience buffer
  id: totrans-1545
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.13\. 经验缓冲区的构造函数
- en: '[PRE155]'
  id: totrans-1546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: After you’ve collected a large experience buffer, you’ll want a way to persist
    it to disk. The HDF5 file format is a perfect fit once again. You can add a `serialize`
    method to the `ExperienceBuffer` class.
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了大量经验缓冲区之后，你将需要一个方法将其持久化到磁盘。HDF5文件格式再次成为完美的选择。你可以在`ExperienceBuffer`类中添加一个`serialize`方法。
- en: Listing 9.14\. Saving an experience buffer to disk
  id: totrans-1548
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.14\. 将经验缓冲区保存到磁盘
- en: '[PRE156]'
  id: totrans-1549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'You’ll also need a corresponding function, `load_experience`, to read the experience
    buffer back out of the file. Note that you cast each data set to `np.array` when
    reading it: that’ll read the entire dataset into memory.'
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要一个相应的函数，`load_experience`，来从文件中读取经验缓冲区。注意，在读取时，你需要将每个数据集转换为`np.array`：这将把整个数据集读入内存。
- en: Listing 9.15\. Restoring an `ExperienceBuffer` from an HDF5 file
  id: totrans-1551
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.15\. 从HDF5文件中恢复`ExperienceBuffer`
- en: '[PRE157]'
  id: totrans-1552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: Now you have a simple container for passing around experience data. You still
    need a way to fill it with your agent’s decisions. The complication is that the
    agent makes decisions one at a time, but it doesn’t get a reward until the game
    is over and you know who won. To resolve this, you need to keep track of all the
    decisions from the current episode until it’s complete. One option is to put this
    logic directly in the agent, but this will clutter up the implementation of `PolicyAgent`.
    Alternately, you can separate this out into a discrete `ExperienceCollector` object
    whose sole responsibility is episode-by-episode bookkeeping.
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有一个简单的容器来传递经验数据。您仍然需要一种方式来填充它，以包含智能体的决策。复杂性在于智能体一次做出一个决策，但直到游戏结束并且您知道谁赢了，它才获得奖励。为了解决这个问题，您需要跟踪当前回合中所有决策，直到它完成。一个选项是将此逻辑直接放入智能体中，但这将使
    `PolicyAgent` 的实现变得杂乱。或者，您可以将其分离成一个离散的 `ExperienceCollector` 对象，其唯一责任是逐回合记账。
- en: 'The `ExperienceCollector` implements four methods:'
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExperienceCollector` 实现了四个方法：'
- en: '`begin_episode` and `complete_episode`, which are called by the self-play driver
    to indicate the start and end of a single game.'
  id: totrans-1555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`begin_episode` 和 `complete_episode`，由自玩驱动程序调用以指示单个游戏的开始和结束。'
- en: '`record_decision`, which is called by the agent to indicate a single action
    it chose.'
  id: totrans-1556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`record_decision`，由智能体调用以指示它选择的一个动作。'
- en: '`to_buffer`, which packages up everything the `ExperienceCollector` has recorded
    and returns an `ExperienceBuffer`. The self-play driver will call this at the
    end of a self-play session.'
  id: totrans-1557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to_buffer`，它将 `ExperienceCollector` 记录的所有内容打包并返回一个 `ExperienceBuffer`。自玩驱动程序将在自玩会话结束时调用此方法。'
- en: The full implementation appears in the following listing.
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的实现细节如下所示。
- en: Listing 9.16\. An object to track decisions within a single episode
  id: totrans-1559
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.16\. 跟踪单个回合内决策的对象
- en: '[PRE158]'
  id: totrans-1560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '***1* Saves a single decision in the current episode; the agent is responsible
    for encoding the state and action.**'
  id: totrans-1561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在当前回合中保存单个决策；智能体负责编码状态和动作。**'
- en: '***2* Spreads the final reward across every action in the game**'
  id: totrans-1562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将最终奖励分配给游戏中的每个动作**'
- en: '***3* The ExperienceCollector accumulates Python lists; this converts them
    to NumPy arrays.**'
  id: totrans-1563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* ExperienceCollector 累积 Python 列表；这会将它们转换为 NumPy 数组。**'
- en: To integrate the `ExperienceCollector` with your agent, you can add a `set_collector`
    method that tells the agent where to send its experiences. Then inside `select_move`,
    the agent will notify the collector every time it makes a decision.
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 `ExperienceCollector` 集成到您的智能体中，您可以添加一个 `set_collector` 方法，告诉智能体将经验发送到何处。然后，在
    `select_move` 内部，智能体将每次做出决策时通知收集器。
- en: Listing 9.17\. Integrating an `ExperienceCollector` with a `PolicyAgent`
  id: totrans-1565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.17\. 将 `ExperienceCollector` 集成到 `PolicyAgent`
- en: '[PRE159]'
  id: totrans-1566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '***1* Allows the self-play driver program to attach a collector to the agent**'
  id: totrans-1567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 允许自玩驱动程序将收集器附加到智能体**'
- en: '***2* At the time it chooses a move, notifies the collector of the decision**'
  id: totrans-1568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在它选择移动时，通知收集器决策**'
- en: 9.4.2\. Simulating games
  id: totrans-1569
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.4.2\. 模拟游戏
- en: 'The next step is playing the games. You’ve done this twice before in the book:
    in the `bot_v_bot` demo in [chapter 3](kindle_split_014.xhtml#ch03), and as part
    of the Monte Carlo tree-search implementation in [chapter 4](kindle_split_016.xhtml#ch04).
    You can use the same `simulate_game` implementation here.'
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是玩游戏。您在书中已经做过两次：在第 3 章的 `bot_v_bot` 演示中，以及在第 4 章的蒙特卡洛树搜索实现中。您可以使用相同的 `simulate_game`
    实现在这里。
- en: Listing 9.18\. Simulating a game between two agents
  id: totrans-1571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.18\. 模拟两个智能体之间的游戏
- en: '[PRE160]'
  id: totrans-1572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: In this function, `black_player` and `white_player` could be any instance of
    your `Agent` class. You can match up the `PolicyAgent` that you’re training against
    any opponent you like. Theoretically, the opponent could be a human player, although
    it’d take ages to collect enough experience data that way. Or your learner could
    play against a third-party Go bot, perhaps using the GTP framework from [chapter
    8](kindle_split_020.xhtml#ch08) to handle the communications.
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: 在此函数中，`black_player` 和 `white_player` 可以是您的 `Agent` 类的任何实例。您可以将您正在训练的 `PolicyAgent`
    与任何对手相匹配。理论上，对手可以是人类玩家，尽管通过这种方式收集足够多的经验数据需要很长时间。或者，您的学习者可以与第三方围棋机器人对战，也许可以使用第
    8 章中提到的 GTP 框架来处理通信。
- en: You can also just match up your learning agent with a copy of itself. Besides
    the simplicity of this solution, there are two specific advantages.
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以将您的学习智能体与自身的副本相匹配。除了这种解决方案的简单性之外，还有两个具体优势。
- en: First, reinforcement learning needs plenty of both successes and failures to
    learn from. Imagine playing your first-ever game of chess or Go against a grandmaster.
    As a novice, you’d be so far behind it would be impossible to tell where you went
    wrong, and the experienced player could probably make a few mistakes and still
    win comfortably. As a result, neither player would learn much from the game. Instead,
    beginners usually start against other beginners and work their way up slowly.
    The same principle applies in reinforcement learning. When your bot plays itself,
    it’ll always have an equal-strength opponent.
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，强化学习需要大量的成功和失败来学习。想象一下，你第一次与一位围棋或国际象棋大师对弈。作为一个新手，你会落后太多，以至于无法判断自己哪里出错，而经验丰富的玩家可能犯几个错误仍然可以轻松获胜。因此，双方都不会从游戏中学到很多。相反，初学者通常先与其他初学者对弈，然后慢慢提升。同样的原则也适用于强化学习。当你的机器人与自己对弈时，它将始终面对一个实力相当的对手。
- en: Second, by playing your agent against itself, you get two games for the price
    of one. Because the same decision-making process went into both sides of the game,
    you can learn from both the winning side and the losing side. You’ll need huge
    volumes of games for reinforcement learning, so generating them twice as fast
    is a nice bonus.
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，通过让你的代理与自己对弈，你可以以一当二。因为相同的决策过程被用于游戏的双方，你可以从胜利方和失败方都学到东西。强化学习需要大量的游戏，因此将游戏生成速度提高一倍是一个很好的额外奖励。
- en: To start the self-play process, you construct two copies of your agent and assign
    them each an `ExperienceCollector`. Each agent needs its own collector because
    the two agents will see different rewards at the end of a game. [Listing 9.19](#ch09ex19)
    shows this initialization step.
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始自我对弈过程，你需要构建你代理的两个副本，并为每个副本分配一个 `ExperienceCollector`。每个代理都需要自己的收集器，因为两个代理在游戏结束时将看到不同的奖励。[列表
    9.19](#ch09ex19) 展示了这个初始化步骤。
- en: '|  |'
  id: totrans-1578
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Reinforcement learning beyond games**'
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习超越游戏**'
- en: Self-play is a great technique for collecting experience data for board games.
    In other domains, you’ll need to separately build a simulated environment to run
    your agent. For example, if you want to use reinforcement learning to build a
    control system for a robot, you’d need a detailed simulation of the physical environment
    the robot will operate in.
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对弈是收集棋类游戏经验数据的一个很好的技术。在其他领域，你需要单独构建一个模拟环境来运行你的代理。例如，如果你想使用强化学习为机器人构建控制系统，你需要对机器人将运行的物理环境进行详细的模拟。
- en: If you want to experiment further with reinforcement learning, the OpenAI Gym
    ([https://github.com/openai/gym](https://github.com/openai/gym)) is a useful resource.
    It provides environments for a variety of board games, video games, and physical
    simulations.
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要进一步实验强化学习，OpenAI Gym ([https://github.com/openai/gym](https://github.com/openai/gym))
    是一个有用的资源。它为各种棋类游戏、视频游戏和物理模拟提供了环境。
- en: '|  |'
  id: totrans-1582
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 9.19\. Initialization for generating a batch of experience
  id: totrans-1583
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.19\. 生成一批经验数据的初始化
- en: '[PRE161]'
  id: totrans-1584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: Now you’re ready to implement the main loop that simulates the self-play games.
    In this loop, `agent1` will always play as black, while `agent2` will always play
    as white. This is fine, so long as `agent1` and `agent2` are identical and you
    intend to combine their experiences for training. If your learning agent is playing
    against another reference agent, you’ll want it to alternate between black and
    white. In Go, black and white have slightly different personalities due to black
    playing first, so a learning agent needs to practice from both sides.
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好实现模拟自我对弈游戏的主循环了。在这个循环中，`agent1` 将始终扮演黑方，而 `agent2` 将始终扮演白方。这是可以的，只要
    `agent1` 和 `agent2` 是相同的，并且你打算将它们的经验合并用于训练。如果你的学习代理在与另一个参考代理对弈，你希望它交替扮演黑方和白方。在围棋中，由于黑方先走，黑和白有略微不同的性格，因此学习代理需要从双方进行练习。
- en: Listing 9.20\. Playing a batch of games
  id: totrans-1586
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.20\. 玩一批游戏
- en: '[PRE162]'
  id: totrans-1587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '***1* agent1 won the game, so it gets a positive reward.**'
  id: totrans-1588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* agent1 赢得了游戏，因此获得正奖励。**'
- en: '***2* agent2 won the game.**'
  id: totrans-1589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* agent2 赢得了游戏。**'
- en: When the self-play is complete, the last step is to combine all the collected
    experience and save it in a file. That file provides the input for the training
    script, which we cover in the next chapter.
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
  zh: 当自我对弈完成时，最后一步是将所有收集到的经验合并并保存到文件中。该文件为训练脚本提供输入，我们将在下一章中介绍。
- en: Listing 9.21\. Saving a batch of experience data
  id: totrans-1591
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.21\. 保存一批经验数据
- en: '[PRE163]'
  id: totrans-1592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '***1* Merges both agents’ experience into a single buffer**'
  id: totrans-1593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将两个代理的经验合并到一个缓冲区中**'
- en: '***2* Saves into an HDF5 file**'
  id: totrans-1594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 保存到 HDF5 文件中**'
- en: At this point, you’re ready to generate self-play games. The next chapter shows
    you how to start improving your bot from the self-play data.
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经准备好生成自我对弈游戏了。下一章将展示如何从自我对弈数据中开始改进你的机器人。
- en: 9.5\. Summary
  id: totrans-1596
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5. 概述
- en: An *agent* is a computer program that’s supposed to accomplish a certain task.
    For example, our Go-playing AI is an agent with the goal of winning games of Go.
  id: totrans-1597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*智能体*是一个旨在完成特定任务的计算机程序。例如，我们的围棋人工智能是一个以赢得围棋比赛为目标的智能体。'
- en: The reinforcement-learning cycle involves collecting experience data, training
    the agent from the experience data, and evaluating the updated agent. At the end
    of a cycle, you expect a small improvement in your agent’s performance. Ideally,
    you can repeat this cycle many times to continually improve your agent.
  id: totrans-1598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习周期包括收集经验数据、从经验数据中训练智能体以及评估更新的智能体。在一个周期结束时，你期望智能体的性能有轻微的改进。理想情况下，你可以重复这个周期多次，以持续改进你的智能体。
- en: To apply reinforcement learning to a problem, you must describe the problem
    in terms of *states*, *actions*, and *rewards*.
  id: totrans-1599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将强化学习应用于一个问题，你必须用*状态*、*动作*和*奖励*来描述这个问题。
- en: Rewards are the way you control the behavior of your reinforcement-learning
    agent. You can provide positive rewards for outcomes you want your agent to achieve,
    and negative rewards for outcomes you want your agent to avoid.
  id: totrans-1600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是你控制强化学习智能体行为的方式。你可以为智能体想要实现的结果提供正奖励，为智能体想要避免的结果提供负奖励。
- en: A *policy* is a rule for making decisions from a given state. In a Go AI, the
    algorithm that selects a move from a board position is its policy.
  id: totrans-1601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略*是从给定状态做出决策的规则。在围棋人工智能中，从棋盘位置选择走法的算法是其策略。'
- en: You can make a policy out of a neural network by treating the output vector
    as a *probability distribution* over possible actions, and then *sampling* from
    the probability distribution.
  id: totrans-1602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过将输出向量视为可能动作的*概率分布*，然后从概率分布中进行*采样*，从神经网络中制作出一个策略。
- en: 'When applying reinforcement learning to games, you can collect experience data
    through *self-play*: your agent plays games against a copy of itself.'
  id: totrans-1603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当将强化学习应用于游戏时，你可以通过*自我对弈*收集经验数据：你的智能体与自身的副本进行游戏。
- en: Chapter 10\. Reinforcement learning with policy gradients
  id: totrans-1604
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章. 基于策略梯度的强化学习
- en: '*This chapter covers*'
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Improving game play with policy gradient learning
  id: totrans-1606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用策略梯度学习提高游戏表现
- en: Implementing policy gradient learning in Keras
  id: totrans-1607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中实现策略梯度学习
- en: Tuning optimizers for policy gradient learning
  id: totrans-1608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整策略梯度学习的优化器
- en: '[Chapter 9](kindle_split_021.xhtml#ch09) showed you how to make a Go-playing
    program play against itself and save the results in experience data. That’s the
    first half of reinforcement learning; the next step is to use experience data
    to improve the agent so that it wins more often. The agent from the previous chapter
    used a neural network to select which move to play. As a thought experiment, imagine
    you shift every weight in the network by a random amount. Then the agent will
    select different moves. Just by luck, some of those new moves will be better than
    the old ones; others will be worse. On balance, the updated agent might be slightly
    stronger or weaker than the previous version. Which way it goes is up to chance.'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](kindle_split_021.xhtml#ch09)向你展示了如何制作一个围棋程序与自身对弈并将结果保存为经验数据。这是强化学习的一半；下一步是使用经验数据来改进智能体，使其赢得比赛的机会更多。上一章中的智能体使用神经网络来选择要走的棋。作为一个思想实验，想象你将网络中的每个权重随机改变一个量。然后智能体将选择不同的走法。仅凭运气，其中一些新走法可能比旧的好；而另一些则更差。总的来说，更新后的智能体可能比之前的版本稍微强或稍微弱。它将走向何方取决于运气。'
- en: Can you improve on that? This chapter covers a form of *policy gradient learning*.
    Policy gradient methods provide a scheme for estimating which direction to shift
    the weights in order to make the agent better at its task. Instead of randomly
    shifting each weight, you can analyze the experience data to guess whether it’s
    better to increase or decrease a particular weight. Randomness still plays a role,
    but policy gradient learning improves your odds.
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: 你能做得更好吗？本章涵盖了一种*策略梯度学习*的形式。策略梯度方法提供了一种估计如何移动权重以使智能体在其任务上表现更好的方案。你不必随机移动每个权重，你可以分析经验数据来猜测是否应该增加或减少特定的权重。随机性仍然发挥作用，但策略梯度学习提高了你的机会。
- en: 'Recall from [chapter 9](kindle_split_021.xhtml#ch09) that you’re making decisions
    with a stochastic policy—a function that specifies a probability for each possible
    move the agent can make. The policy-learning method we cover in this chapter works
    like this:'
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第9章](kindle_split_021.xhtml#ch09)，你正在使用随机策略进行决策——一个为智能体可以做出的每个可能的移动指定概率的函数。本章中我们介绍的策略学习方法是这样的：
- en: When the agent wins, increase the probability of each move it picked.
  id: totrans-1612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当智能体获胜时，增加它选择的每个移动的概率。
- en: When the agent loses, decrease the probability of each move it picked.
  id: totrans-1613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当智能体失败时，减少它选择的每个移动的概率。
- en: First, you’ll work through a simplified example to show how improving a policy
    by this technique can lead to winning more games. Next, you’ll see how to use
    gradient descent to make the change you want—increasing or decreasing the probability
    of a specific move—in a neural network. We wrap up with some practical tips for
    managing the training process.
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将通过一个简化的例子来展示如何通过这种方法改进策略，从而赢得更多游戏。接下来，你将看到如何使用梯度下降来在神经网络中实现你想要的改变——增加或减少特定移动的概率。我们最后总结了一些关于管理训练过程的实际技巧。
- en: 10.1\. How random games can identify good decisions
  id: totrans-1615
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1. 随机游戏如何识别好的决策
- en: 'To introduce policy learning, we’ll start with a game that’s much simpler than
    Go. Let’s call this game Add It Up. Here are the rules:'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍策略学习，我们将从一个比围棋更简单的游戏开始。让我们称这个游戏为“加起来”。以下是规则：
- en: On each turn, each player picks a number between 1 and 5.
  id: totrans-1617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一轮中，每位玩家选择一个介于1到5之间的数字。
- en: After 100 turns, each player adds up all the numbers they chose.
  id: totrans-1618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过100轮后，每位玩家将他们选择的数字加起来。
- en: The player with the higher total wins.
  id: totrans-1619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总数较高的玩家获胜。
- en: Yes, this means the optimal strategy is to just pick 5 on each turn. No, this
    isn’t a good game. We’ll use this game to illustrate *policy learning*, where
    you gradually improve a stochastic policy based on game results. Because you know
    the correct strategy for this game, you can see how policy learning leads toward
    perfect play.
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这意味着最佳策略是在每一轮都选择5。不，这不是一个好游戏。我们将使用这个游戏来说明*策略学习*，即根据游戏结果逐渐改进随机策略。因为你知道这个游戏的正确策略，你可以看到策略学习如何引导玩家走向完美游戏。
- en: Add It Up is a shallow game, but we can use it as a metaphor for a more serious
    game like Go. Just as in Go, a game of Add It Up is long, and players have many
    opportunities to make good plays or blunders within the same game. To update a
    policy from game results, you need to identify which moves deserve credit or blame
    for winning or losing a particular game. This is called the *credit assignment*
    problem, and it’s one of the core problems in reinforcement learning. This section
    demonstrates how you can average many game results to assign credit to individual
    decisions. In [chapter 12](kindle_split_024.xhtml#ch12), you’ll build on this
    technique to create a more sophisticated and robust credit assignment algorithm.
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: “加起来”是一个简单的游戏，但我们可以用它来比喻像围棋这样的更严肃的游戏。就像围棋一样，“加起来”这个游戏很长，玩家在同一个游戏中有很多机会做出好的或糟糕的决策。要从游戏结果中更新策略，你需要确定哪些移动值得赞扬或责备，以赢得或输掉特定的游戏。这被称为*信用分配*问题，它是强化学习中的核心问题之一。本节展示了如何通过平均许多游戏结果来为个别决策分配信用。在[第12章](kindle_split_024.xhtml#ch12)中，你将在此基础上构建一个更复杂和鲁棒的信用分配算法。
- en: Let’s start with a purely random policy, where you select any of the five options
    with equal probability. (Such a policy is called *uniform random*.) Over the course
    of a full game, you’d expect that policy to select 1 about 20 times, 2 about 20
    times, 3 about 20 times, and so on. But you won’t expect 1 to appear *exactly*
    20 times; it’ll vary from game to game. The following listing shows a Python function
    that simulates all the choices such an agent will make over a game. [Figure 10.1](#ch10fig01)
    shows the results of a few sample runs; you can try the snippet yourself a few
    times and see.
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一种纯粹随机的策略开始，其中你以相等的概率选择五个选项中的任何一个。（这样的策略被称为*均匀随机*。）在整个游戏过程中，你预计该策略会选择1大约20次，2大约20次，3大约20次，依此类推。但你不会期望1恰好出现20次；它会在每一轮游戏中有所不同。以下列表显示了一个Python函数，该函数模拟了这样一个智能体在游戏中将做出的所有选择。[图10.1](#ch10fig01)显示了几个样本运行的结果；你可以自己尝试几次并看看。
- en: Listing 10.1\. Randomly drawing a number from 1 to 5
  id: totrans-1623
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1. 从1到5随机抽取一个数字
- en: '[PRE164]'
  id: totrans-1624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: Figure 10.1\. This graph shows four sample games played by a random agent. The
    bars indicate how often the agent chose each of the five possible moves in each
    game. Although the agent used the same policy in all the games, the exact counts
    vary quite a bit from game to game.
  id: totrans-1625
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1\. 此图显示了随机代理玩过的四个样本游戏。条形表示代理在每场比赛中选择五种可能走法的频率。尽管代理在所有游戏中都使用了相同的策略，但每场比赛的确切计数差异很大。
- en: '![](Images/10fig01_alt.jpg)'
  id: totrans-1626
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig01_alt.jpg)'
- en: Even though the agent follows the exact same policy every game, the stochastic
    nature of the policy causes variance from game to game. You can exploit that variance
    to improve the policy.
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代理在每场比赛中都遵循完全相同的策略，但策略的随机性质导致每场比赛之间的差异。你可以利用这种差异来改进策略。
- en: The following listing shows a function that simulates a complete game of Add
    It Up, tracks the decisions each player makes, and calculates the winner.
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了一个模拟完整游戏“加起来”的函数，跟踪每个玩家所做的决策，并计算赢家。
- en: Listing 10.2\. Simulating a game of Add It Up
  id: totrans-1629
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2\. 模拟“加起来”游戏
- en: '[PRE165]'
  id: totrans-1630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: Run a few games and look at the results; [listing 10.3](#ch10ex03) shows some
    example runs. Usually, the winner picks 1 less often, but not always. Sometimes
    the winner picks 5 more often, but that’s not guaranteed either.
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
  zh: 进行几场比赛并查看结果；[列表10.3](#ch10ex03)显示了几个示例运行。通常，赢家选择1的次数较少，但并不总是如此。有时赢家选择5的次数更多，但这也不是保证的。
- en: Listing 10.3\. Sample outputs of [listing 10.2](#ch10ex02)
  id: totrans-1632
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3\. [列表10.2](#ch10ex02)的示例输出
- en: '[PRE166]'
  id: totrans-1633
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '***1* Winner’s choices**'
  id: totrans-1634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 赢家的选择**'
- en: '***2* Loser’s choices**'
  id: totrans-1635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 输家的选择**'
- en: If you average over the four example games in [listing 10.3](#ch10ex03), you
    see that the winners picked 1 an average of 18.75 times per game, whereas the
    losers picked 1 an average of 22.5 times. This makes sense, because 1 is a bad
    move. Even though all these games were sampled from the same policy, the distributions
    differ between the winners and losers, because picking 1 more often caused the
    agent to lose.
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你平均来看[列表10.3](#ch10ex03)中的四个示例游戏，你会发现赢家平均每场比赛选择1的次数为18.75次，而输家平均每场比赛选择1的次数为22.5次。这很有道理，因为1是一个糟糕的走法。尽管所有这些游戏都是从同一策略中抽取的样本，但赢家和输家之间的分布是不同的，因为更频繁地选择1会导致代理输掉游戏。
- en: The difference between the moves the agent picks in wins and the moves the agent
    picks in losses tells you which moves are better. To improve the policy, you can
    update the probabilities according to those differences. In this case, you can
    add a small fixed amount for each time a move appears in a win, and subtract a
    small fixed amount for each time a move appears in a loss. Then the probability
    distribution will slowly shift toward the moves that appear more often in wins—which
    you assume are the good moves. For Add It Up, this algorithm works fine. To learn
    a complicated game like Go, you’ll need a more sophisticated scheme for updating
    the probabilities; we cover that in [section 10.2](#ch10lev1sec2).
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在胜利和失败中选择的走法之间的差异告诉你哪些走法更好。为了改进策略，你可以根据这些差异更新概率。在这种情况下，你可以为每次走法在胜利中出现添加一个小固定量，并为每次走法在失败中出现减去一个小固定量。然后概率分布将逐渐向在胜利中出现次数更多的走法（你假设是好的走法）移动。对于“加起来”游戏，这个算法效果很好。要学习像围棋这样复杂的游戏，你需要一个更复杂的方案来更新概率；我们将在[第10.2节](#ch10lev1sec2)中介绍。
- en: Listing 10.4\. A policy learning implementation for the simple game Add It Up
  id: totrans-1638
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.4\. 简单游戏“加起来”的策略学习实现
- en: '[PRE167]'
  id: totrans-1639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '***1* Ensures that the policy is a valid probability distribution, by making
    sure it sums to 1**'
  id: totrans-1640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 确保策略是一个有效的概率分布，通过确保其总和为1**'
- en: '***2* A setting controlling how fast you update the policy**'
  id: totrans-1641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 控制你更新策略速度的设置**'
- en: '***3* net_wins will be positive if choice appears more often in wins than in
    losses; it’ll be negative if choice appears more often in losses than in wins.**'
  id: totrans-1642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果选择在胜利中出现的次数多于在失败中出现的次数，net_wins将是正数；如果选择在失败中出现的次数多于在胜利中出现的次数，它将是负数。**'
- en: '[Figure 10.2](#ch10fig02) shows how the policy evolves throughout this demo.
    After about a thousand games, the algorithm learns to stop choosing the worst
    move. In another thousand games or so, it has more or less arrived at the perfect
    strategy: selecting 5 on each turn. The curves aren’t perfectly smooth. Sometimes
    the agent will choose a lot of 1s in a game and win anyway; then the policy will
    (incorrectly) shift toward 1\. You rely on these mistakes getting smoothed out
    over the course of many, many games.'
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.2](#ch10fig02) 展示了在此演示过程中策略是如何演变的。大约经过一千场比赛后，算法学会了停止选择最差的走法。在另一千场左右的比赛中，它或多或少已经达到了完美的策略：每轮选择5。曲线并不完全平滑。有时代理会在一场比赛中选择很多1，但仍然获胜；然后策略将（错误地）偏向1。你依赖于这些错误在许多许多场比赛中得到平滑。'
- en: Figure 10.2\. This graph shows how a policy evolves under your simplified policy-learning
    scheme. Over the course of hundreds of games, the agent gradually becomes less
    likely to choose the worst move (playing a 1). Likewise, the agent gradually becomes
    more likely to choose the best move (playing a 5). Both curves are wobbly, because
    the policy sometimes takes a small step in the wrong direction.
  id: totrans-1644
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2. 此图显示了在你的简化策略学习方案下策略是如何演变的。在数百场比赛的过程中，代理逐渐变得不太可能选择最差的走法（玩1）。同样，代理逐渐变得更有可能选择最好的走法（玩5）。两条曲线都是摇摆不定的，因为策略有时会朝着错误的方向迈出小步。
- en: '![](Images/10fig02.jpg)'
  id: totrans-1645
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2](Images/10fig02.jpg)'
- en: 10.2\. Modifying neural network policies with gradient descent
  id: totrans-1646
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2. 使用梯度下降修改神经网络策略
- en: There’s one glaring difference between learning to play Add It Up and learning
    to play Go. The policy you used in the Add It Up example doesn’t depend on the
    game state in any way. Picking 5 is always a good move, and picking 1 is always
    a bad move. In Go, when we say we want to increase the probability of a particular
    move, we really want to increase the probability of that move *in similar situations*.
    But the definition of *similar situations* is impossibly vague. Here, we rely
    on the power of neural networks to tease apart what *similar situations* really
    means.
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习玩“加起来”和学会玩围棋之间有一个明显的区别。你在“加起来”示例中使用的策略在没有任何方式上依赖于游戏状态。选择5始终是一个好策略，而选择1始终是一个坏策略。在围棋中，当我们说我们想要增加特定走法的概率时，我们实际上想要增加该走法在类似情况下的概率。但“类似情况”的定义是难以言表的模糊。在这里，我们依靠神经网络的力量来揭示“类似情况”真正意味着什么。
- en: When you created a neural network policy in [chapter 9](kindle_split_021.xhtml#ch09),
    you built a function that took a board position as input and produced a probability
    distribution over moves as output. For every board position in your experience
    data, you want to either increase the probability of the chosen move (if it led
    to a win) or decrease the probability of the chosen move (if it led to a loss).
    But you can’t forcibly modify the probabilities in the policy as you did in [section
    9.1](kindle_split_021.xhtml#ch09lev1sec1). Instead, you have to modify the weights
    of the neural network to make your desired outcome happen. Gradient descent is
    the tool that makes this possible. Modifying a policy with gradient descent is
    called *policy gradient learning*. There are a few variations on this idea; the
    particular learning algorithm we describe in this chapter is sometimes called
    *Monte Carlo policy gradient,* or the *REINFORCE* method. [Figure 10.3](#ch10fig03)
    shows the high-level flow of how this process applies to games.
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在[第9章](kindle_split_021.xhtml#ch09)中创建神经网络策略时，你构建了一个函数，该函数以棋盘位置作为输入，并产生一个关于走法的概率分布作为输出。对于你的经验数据中的每一个棋盘位置，你都想增加所选走法的概率（如果它导致了胜利），或者减少所选走法的概率（如果它导致了失败）。但你不能像在[第9.1节](kindle_split_021.xhtml#ch09lev1sec1)中那样强制修改策略中的概率。相反，你必须修改神经网络的权重，以实现你期望的结果。梯度下降是使这成为可能的工具。使用梯度下降修改策略被称为*策略梯度学习*。这个想法有一些变体；我们在本章中描述的特定学习算法有时被称为*蒙特卡洛策略梯度*，或*REINFORCE*方法。[图10.3](#ch10fig03)展示了这个过程如何应用于游戏的高级流程。
- en: Figure 10.3\. A flowchart for policy gradient learning. You start with a collection
    of game records and their outcomes. For each move the agent chose, you want to
    either increase the probability of that move (if the agent won the game) or decrease
    the probability (if the agent lost the game). Gradient descent handles the mechanics
    of updating the policy weights. After a pass of gradient descent, the probabilities
    will be shifted in the desired direction.
  id: totrans-1649
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3。策略梯度学习的流程图。你从一个游戏记录集合及其结果开始。对于智能体选择的每个移动，你想要要么增加该移动的概率（如果智能体赢得了游戏），要么降低该移动的概率（如果智能体输掉了游戏）。梯度下降处理策略权重的更新机制。经过一次梯度下降后，概率将按照期望的方向移动。
- en: '![](Images/10fig03_alt.jpg)'
  id: totrans-1650
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3](Images/10fig03_alt.jpg)'
- en: '|  |'
  id: totrans-1651
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: The *REINFORCE* method stands for *REward Increment = Nonnegative Factor times
    Offset Reinforcement times Characteristic Eligibility*, which spells out the formula
    for the gradient update.
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: '*REINFORCE*方法代表*REward Increment = Nonnegative Factor times Offset Reinforcement
    times Characteristic Eligibility*，它阐明了梯度更新的公式。'
- en: '|  |'
  id: totrans-1654
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s recap how supervised learning with gradient descent works, as covered
    in [chapter 5](kindle_split_017.xhtml#ch05). You chose a loss function that represents
    how far away your function is from the training data, and calculated its gradient.
    The goal was to make the value of the loss function smaller, meaning the learned
    function would match the training data better. Gradient descent—gradually updating
    the weights in the direction of the gradient of the loss function—provided the
    mechanism for decreasing the loss function. The gradient told you the direction
    to shift each weight in order to decrease the loss function.
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下带有梯度下降的有监督学习是如何工作的，如第5章所述。[第5章](kindle_split_017.xhtml#ch05)。你选择了一个损失函数，它表示你的函数与训练数据之间的距离，并计算了它的梯度。目标是使损失函数的值更小，这意味着学习到的函数将更好地匹配训练数据。梯度下降——逐渐更新损失函数梯度的方向上的权重——提供了减少损失函数的机制。梯度告诉你每个权重偏移的方向，以减少损失函数。
- en: For policy learning, you want to find the direction to shift each weight in
    order to bias the policy toward (or away from) a specific move. You can craft
    a loss function so that its gradient has this property. When you have that, you
    can take advantage of the fast and flexible infrastructure that Keras provides
    to modify the weights of your policy network.
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
  zh: 对于策略学习，你需要找到每个权重偏移的方向，以便将策略偏向（或远离）特定的移动。你可以设计一个损失函数，使其梯度具有这种特性。当你有了这个，你就可以利用Keras提供的快速灵活的基础设施来修改你的策略网络权重。
- en: Recall the supervised learning you did in [chapter 7](kindle_split_019.xhtml#ch07).
    For each game state, you also knew the human move that happened in the game. You
    created a target vector that contained a 0 for each board position, with a 1 to
    indicate the human move. The loss function measured the gap between the predicted
    probability distribution, and its gradient indicated the direction to follow to
    shrink the gap. After completing a batch of gradient descent, the predicted probability
    of the human move increased slightly.
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下你在第7章中进行的监督学习。[第7章](kindle_split_019.xhtml#ch07)。对于每个游戏状态，你也知道游戏中发生的人类移动。你创建了一个目标向量，其中每个棋盘位置都有一个0，用1表示人类移动。损失函数测量预测概率分布与目标分布之间的差距，其梯度指示了缩小差距的方向。完成一批梯度下降后，预测的人类移动概率略有增加。
- en: 'That’s exactly the effect that you want to achieve: increasing the probability
    of a particular move. For games that your agent won, you can construct the exact
    same target vector for the agent’s move as if it were a human move from a real
    game record. Then the Keras `fit` function updates the policy in the right direction.'
  id: totrans-1658
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是你想要达到的效果：增加特定移动的概率。对于你的智能体赢得的游戏，你可以为智能体的移动构建与真实游戏记录中的人类移动完全相同的目标向量。然后，Keras的`fit`函数将更新策略到正确的方向。
- en: What about the case of a lost game? In that case, you want to decrease the probability
    of the chosen move, but you don’t know the actual best move. Ideally, the update
    should have the exact opposite effect as in a won game.
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: 那么输掉游戏的情况呢？在这种情况下，你想要降低所选移动的概率，但你不知道实际的最佳移动。理想情况下，更新应该与赢得游戏时的效果正好相反。
- en: As it turns out, if you train with the cross-entropy loss function, you can
    just insert a –1 into the target vector instead of a 1\. This will reverse the
    sign of the gradient of the loss function, meaning your weights will move in the
    exact opposite direction, thereby decreasing the probability.
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，如果你使用交叉熵损失函数进行训练，你只需在目标向量中插入一个-1而不是1。这将反转损失函数梯度的符号，这意味着你的权重将向完全相反的方向移动，从而降低概率。
- en: 'To do this trick, you must use cross-entropy loss; other loss functions, like
    mean squared error, won’t work the same way. In [chapter 7](kindle_split_019.xhtml#ch07),
    you chose cross-entropy loss because it’s the most efficient way to train a network
    to choose one of a fixed number of options. Here you choose it for a different
    property: swapping a –1 for a 1 in the target vector will reverse the direction
    of the gradient.'
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这个技巧，你必须使用交叉熵损失；其他损失函数，如均方误差，不会以相同的方式工作。在[第7章](kindle_split_019.xhtml#ch07)中，你选择了交叉熵损失，因为它是训练网络选择固定数量选项中最有效的方法。在这里，你选择它是因为一个不同的特性：在目标向量中将-1替换为1将反转梯度的方向。
- en: 'Recall that your experience data comprises three parallel arrays:'
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，你的经验数据由三个并行数组组成：
- en: '`states[i]` represents a particular board position your agent faced during
    self-play.'
  id: totrans-1663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`states[i]`代表你的代理在自我对弈中遇到的一个特定棋盘位置。'
- en: '`actions[i]` represents the move your agent chose, given that position.'
  id: totrans-1664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`actions[i]`代表你的代理在给定位置时选择的移动。'
- en: '`rewards[i]` contains a 1 if the agent won the game, and a –1 otherwise.'
  id: totrans-1665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards[i]`包含一个1，如果代理赢得了游戏，否则为-1。'
- en: The following listing implements a `prepare_experience_data` function that packs
    an experience buffer into a target array suitable for Keras `fit`.
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表实现了一个`prepare_experience_data`函数，该函数将经验缓冲区打包成适合Keras `fit`的目标数组。
- en: Listing 10.5\. Encoding experience data as a target vector
  id: totrans-1667
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.5：将经验数据编码为目标向量
- en: '[PRE168]'
  id: totrans-1668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: The following listing shows how to implement a `train` function on your `PolicyAgent`
    class.
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了如何在你的`PolicyAgent`类上实现一个`train`函数。
- en: Listing 10.6\. Training an agent from experience data with policy gradient learning
  id: totrans-1670
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6：使用策略梯度学习从经验数据训练代理
- en: '[PRE169]'
  id: totrans-1671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '***1* lr (learning rate), clipnorm, and batch_size let you fine-tune the training
    process; we cover these in detail in the following text.**'
  id: totrans-1672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 学习率（lr）、clipnorm和batch_size允许你微调训练过程；我们将在以下文本中详细介绍这些内容。**'
- en: '***2* The compile method assigns an optimizer to the model; in this case, the
    stochastic gradient descent (SGD) optimizer.**'
  id: totrans-1673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 编译方法将优化器分配给模型；在这种情况下，是随机梯度下降（SGD）优化器。**'
- en: 'In addition to an experience buffer, this `train` function takes three parameters
    that modify the behavior of the optimizer:'
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: 除了经验缓冲区外，这个`train`函数还接受三个参数，这些参数会修改优化器的行为：
- en: '`lr` is the *learning rate*, which controls how far to move the weights at
    each step.'
  id: totrans-1675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`是*学习率*，它控制每个步骤中权重移动的距离。'
- en: '`clipnorm` provides a hard maximum on how far to move the weights on any individual
    step.'
  id: totrans-1676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clipnorm`为任何单个步骤中权重移动的距离提供了一个硬上限。'
- en: '`batch_size` controls how many moves from the experience data get combined
    into a single weight update.'
  id: totrans-1677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`控制从经验数据中组合到单个权重更新中的移动数量。'
- en: You may need to fine-tune these parameters to get a good result with policy
    gradient learning. In [section 10.3](#ch10lev1sec3), we provide tips to help you
    find the right settings.
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要调整这些参数才能使用策略梯度学习获得良好的结果。在[第10.3节](#ch10lev1sec3)中，我们提供了帮助你找到正确设置的提示。
- en: In [chapter 7](kindle_split_019.xhtml#ch07), you used the Adadelta and Adagrad
    optimizers, which automatically adapt the learning rate throughout the training
    process. Unfortunately, they both make assumptions that don’t always apply to
    policy gradient learning. Instead, you should use the basic stochastic gradient
    descent optimizer and set the learning rate manually. We want to emphasize that
    95% of the time, an adaptive optimizer like Adadelta or Adagrad is the best choice;
    you’ll see faster training with fewer headaches. But in rare cases, you need to
    fall back to plain SGD, and it’s good to have some understanding of how to set
    the learning rate by hand.
  id: totrans-1679
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](kindle_split_019.xhtml#ch07)中，你使用了Adadelta和Adagrad优化器，它们会在整个训练过程中自动调整学习率。不幸的是，它们都做出了不一定适用于策略梯度学习的假设。相反，你应该使用基本的随机梯度下降优化器并手动设置学习率。我们想强调的是，95%的时间，自适应优化器如Adadelta或Adagrad是最好的选择；你将看到更快的训练速度和更少的麻烦。但在罕见的情况下，你需要回退到普通的SGD，并且了解如何手动设置学习率是很好的。
- en: 'Notice also that you run only a single epoch of training on the experience
    buffer. This is different from [chapter 7](kindle_split_019.xhtml#ch07), where
    you ran several epochs on the same training set. The key difference is that the
    training data in [chapter 7](kindle_split_019.xhtml#ch07) was known to be good.
    Each game move in that data set was a move that a skilled human player chose in
    a real game. In your self-play data, the game outcomes are partially randomized,
    and you don’t know which moves deserve credit for the wins. You’re counting on
    a huge number of games to smooth out the errors. So you don’t want to reuse any
    single game record: that’ll double down on whatever misleading data it contains.'
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你只在经验缓冲区上运行单个时期的训练。这与 [第 7 章](kindle_split_019.xhtml#ch07) 不同，在那里你在同一个训练集上运行了多个时期。关键的区别在于
    [第 7 章](kindle_split_019.xhtml#ch07) 中的训练数据是已知的良好数据。该数据集中的每一步都是一名熟练的人类玩家在真实游戏中选择的步骤。在你的自我对弈数据中，游戏结果部分是随机化的，你不知道哪些步骤应该得到胜利的认可。你依赖大量的游戏来平滑错误。因此，你不想重复使用任何单个游戏记录：这将放大其中包含的任何误导性数据。
- en: Fortunately, with reinforcement learning, you have an unlimited supply of training
    data. Instead of running multiple epochs on the same training set, you should
    run another batch of self-play and generate a new training set.
  id: totrans-1681
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过强化学习，你有无限的训练数据。你不需要在同一个训练集上运行多个时期，你应该运行另一批自我对弈并生成一个新的训练集。
- en: Now that you have a `train` function ready, the following listing shows a script
    to do the training. You can find the full script in train_pg.py on GitHub. It
    consumes experience data files, generated by the self_play script from [chapter
    9](kindle_split_021.xhtml#ch09).
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好了一个 `train` 函数，以下列表显示了进行训练的脚本。你可以在 GitHub 上的 train_pg.py 中找到完整的脚本。它消耗由自我对弈脚本从
    [第 9 章](kindle_split_021.xhtml#ch09) 生成的经验数据文件。
- en: Listing 10.7\. Training on previously saved experience data
  id: totrans-1683
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.7\. 在之前保存的经验数据上训练
- en: '[PRE170]'
  id: totrans-1684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '***1* You may have more training data than can fit in memory at once; this
    implementation will read it in from multiple files, one chunk at a time.**'
  id: totrans-1685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你可能拥有的训练数据比一次能放入内存的要多；这个实现将一次读取一个数据块，从多个文件中读取。**'
- en: 10.3\. Tips for training with self-play
  id: totrans-1686
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3\. 使用自我对弈训练的技巧
- en: Tuning the training process can be difficult. And training a large network is
    slow, which means you may have to wait a long time to check your results. You
    should be prepared for some trial and error and a few false starts. This section
    provides tips for managing a long training process. First, we provide details
    on how to test and verify your bot’s progress. Then we dig into some of the tuning
    parameters that affect the training process.
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: 调整训练过程可能很困难。训练大型网络很慢，这意味着你可能需要等待很长时间才能检查你的结果。你应该准备好进行一些尝试和错误以及几次失败。本节提供了管理长时间训练过程的一些技巧。首先，我们提供如何测试和验证你的机器人进度的详细信息。然后我们深入探讨一些影响训练过程的调整参数。
- en: 'Reinforcement learning is slow: if you’re training a Go AI, you may need 10,000
    self-play games or more before you see a visible improvement. We suggest starting
    your experiments on a smaller board, such as 9 × 9 or even 5 × 5\. On small boards,
    the game is shorter, so you can generate the self-play games faster; and the game
    is less complicated, so you need less training data to make progress. That way,
    you can test your code and tune the training process faster. Once you’re confident
    in your code, you can move up to a larger board size.'
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习很慢：如果你在训练围棋 AI，你可能需要 10,000 次自我对弈或更多才能看到明显的改进。我们建议从较小的棋盘开始你的实验，例如 9×9 或甚至
    5×5。在小棋盘上，游戏更短，因此你可以更快地生成自我对弈游戏；游戏也更简单，因此你需要更少的训练数据来取得进展。这样，你可以更快地测试你的代码和调整训练过程。一旦你对你的代码有信心，你就可以升级到更大的棋盘大小。
- en: 10.3.1\. Evaluating your progress
  id: totrans-1689
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1\. 评估你的进度
- en: In a game as complex as Go, reinforcement learning can take a long time—especially
    if you don’t have access to specialized hardware. Nothing is more frustrating
    than spending days running a training process, only to find that something went
    wrong many hours ago. We recommend regularly checking the progress of your learning
    agent. You do this by simulating more games. The eval_pg_bot.py script pits two
    versions of your bot against each other; the following listing shows how it works.
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: 在像围棋这样复杂的游戏中，强化学习可能需要很长时间——尤其是如果你没有访问专用硬件的话。没有什么比花费几天时间运行训练过程，最后发现几个小时前就出了问题更令人沮丧的了。我们建议定期检查你的学习代理的进度。你可以通过模拟更多游戏来实现这一点。eval_pg_bot.py
    脚本将你的机器人的两个版本相互对抗；以下列表显示了它是如何工作的。
- en: Listing 10.8\. Script for comparing the strength of two agents
  id: totrans-1691
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.8. 比较两个代理实力的脚本
- en: '[PRE171]'
  id: totrans-1692
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '***1* This script tracks wins and losses from the point of view of agent1.**'
  id: totrans-1693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 此脚本从agent1的角度跟踪胜负。**'
- en: '***2* color1 is the color that agent1 plays with; agent2 gets the opposite
    color.**'
  id: totrans-1694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* color1是agent1使用的颜色；agent2得到相反的颜色。**'
- en: '***3* Swap colors after each game, in case either agent plays better with a
    particular color.**'
  id: totrans-1695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在每场比赛后交换颜色，以防任何一个代理对某种颜色有更好的表现。**'
- en: After each batch of training, you can pit the updated agent against the original
    agent and confirm that the updated agent is improving, or at least not getting
    worse.
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
  zh: 在每批训练之后，你可以让更新的代理与原始代理对抗，以确认更新的代理正在改进，或者至少没有变得更差。
- en: 10.3.2\. Measuring small differences in strength
  id: totrans-1697
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.2. 衡量实力的小差异
- en: After training on thousands of self-play games, your bot may improve by only
    a few percentage points compared to its predecessor. Measuring a difference that
    small is fairly difficult. Let’s say you’ve completed a round of training. For
    evaluation, you run your updated bot against the previous version for 100 games.
    The updated bot wins 53\. Is the new bot truly 3 percentage points stronger? Or
    did it just get lucky? You need a way to decide whether you have enough data to
    accurately evaluate your bot’s strength.
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练了数千场自我对弈游戏之后，你的机器人可能只比它的前辈提高了几个百分点。衡量这么小的差异相当困难。假设你已经完成了一轮训练。为了评估，你让你的更新后的机器人与之前的版本进行100场比赛。更新后的机器人赢得了53场。新的机器人真的强了3个百分点吗？或者它只是运气好？你需要一种方法来判断你是否拥有足够的数据来准确评估你的机器人实力。
- en: 'Imagine that your training did nothing at all, and your updated bot is identical
    to the previous version. What are the chances that the identical bot wins at least
    53 games? Statisticians use a formula called a *binomial test* to calculate that
    chance. The Python package scipy provides a convenient implementation of the binomial
    test:'
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的训练没有任何效果，更新的机器人与之前的版本完全相同。那么，相同的机器人赢得至少53场比赛的概率是多少？统计学家使用一个称为*二项式检验*的公式来计算这个概率。Python包scipy提供了一个方便的二项式检验实现：
- en: '[PRE172]'
  id: totrans-1700
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'In that snippet:'
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个片段中：
- en: 53 represents the number of wins you observed.
  id: totrans-1702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 53代表你观察到的胜利次数。
- en: 100 represents the number of games you simulated.
  id: totrans-1703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100代表你模拟的游戏数量。
- en: 0.5 represents the probability of your bot winning a single game if it’s identical
    to its opponent.
  id: totrans-1704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0.5代表如果你的机器人与对手完全相同，它赢得单场比赛的概率。
- en: The binomial test gave a value of 61.7%. If your bot is truly identical to its
    opponent, it still has a 61.7% chance of winning 53 or more games. This probability
    is sometime called a *p-value*. This does *not* mean there’s a 61.7% chance that
    your bot has learned nothing—it just means you don’t have enough evidence to judge
    that. If you want to be confident that your bot has improved, you need to run
    more trials.
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
  zh: 二项式检验给出了61.7%的值。如果你的机器人确实与对手完全相同，它仍然有61.7%的几率赢得53场或更多的游戏。这个概率有时被称为*p-value*。这*并不*意味着你的机器人有61.7%的几率什么都没学到——它只是意味着你没有足够的证据来判断这一点。如果你想有信心认为你的机器人已经改进，你需要进行更多的试验。
- en: As it turns out, you need quite a few trials to reliably measure a difference
    in strength this small. If you run 1,000 games and get 530 wins, the binomial
    test gives a p-value of about 6%. A common guideline is to look for a p-value
    under 5% before making a decision. But there’s nothing magical about that 5% threshold.
    Instead, think of the p-value as a spectrum indicating how skeptical you should
    be of your bot’s winning record, and use your judgment.
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你需要进行相当多的试验才能可靠地衡量这么小的实力差异。如果你运行了1000场比赛并赢得了530场，二项式检验给出的p-value大约是6%。一个常见的指导原则是在做出决定之前寻找低于5%的p-value。但那个5%的阈值并没有什么神奇之处。相反，将p-value视为一个范围，表示你应该对机器人获胜记录有多大的怀疑，并使用你的判断力。
- en: 10.3.3\. Tuning a stochastic gradient descent (SGD) optimizer
  id: totrans-1707
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.3. 调整随机梯度下降（SGD）优化器
- en: The SGD optimizer has a few parameters that can affect its performance. Generally,
    they have a trade-off between speed and accuracy. Policy gradient learning is
    typically more sensitive to accuracy than supervised learning, so you’ll need
    to set the parameters appropriately.
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
  zh: SGD优化器有几个参数可以影响其性能。通常，它们在速度和准确性之间有一个权衡。策略梯度学习通常比监督学习对准确性更敏感，因此你需要适当地设置参数。
- en: The first parameter you must set is the learning rate. To properly set the learning
    rate, you should understand the problems that an incorrectly set learning rate
    can cause. Throughout this section, you’ll refer to [figure 10.4](#ch10fig04).
    This shows an imaginary objective function that you’re trying to minimize. Conceptually,
    this diagram is showing the same thing as the diagrams you studied in [section
    5.4](kindle_split_017.xhtml#ch05lev1sec4); but here we restrict it to one dimension
    to illustrate a few specific points. In reality, you’re normally optimizing a
    function over thousands of dimensions.
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须设置的第一个参数是学习率。为了正确设置学习率，你应该了解设置错误的学习率可能引起的问题。在本节中，你将参考[图10.4](#ch10fig04)。这显示了一个你试图最小化的假设目标函数。从概念上讲，这个图与你在[第5.4节](kindle_split_017.xhtml#ch05lev1sec4)中学习的图示是相同的；但在这里我们将其限制为一维以说明几个具体点。实际上，你通常是在数千维上优化一个函数。
- en: Figure 10.4\. This graph shows how a hypothetical objective function may vary
    with a learnable weight. You want to move the value of *u* (Greek letter theta)
    from its current location to the minimum. You can think of gradient descent as
    causing the weight to roll downhill.
  id: totrans-1710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4。此图显示了假设目标函数如何随可学习权重变化。你希望将*u*（希腊字母theta）的值从当前位置移动到最小值。你可以将梯度下降想象成使权重滚下山谷。
- en: '![](Images/10fig04_alt.jpg)'
  id: totrans-1711
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/10fig04_alt.jpg)'
- en: In [chapter 5](kindle_split_017.xhtml#ch05), you were trying to optimize a loss
    function that measured the error between your predictions and known correct examples.
    In this case, the objective is your bot’s winning rate. (Technically, in the case
    of winning percentage, you’d want to maximize the objective instead. The same
    principles apply in either case, just flipped upside-down.) Unlike the loss function,
    you can’t calculate the winning rate directly, but you can estimate its gradient
    from the self-play data. In [figure 10.4](#ch10fig04), the x-axis represents some
    weight in your network, and the y-axis shows how the value of the objective varies
    with that weight. The marked point indicates the current state of the network.
    In the ideal case, you can imagine that gradient descent makes the marked point
    roll downhill and settle in the valley.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](kindle_split_017.xhtml#ch05)中，你试图优化一个损失函数，该函数衡量你的预测与已知正确示例之间的误差。在这种情况下，目标是你的机器人胜率。（技术上，在胜率的情况下，你希望最大化目标。这两种情况下的原则相同，只是上下颠倒。）与损失函数不同，你不能直接计算胜率，但你可以从自我对弈数据中估计其梯度。在[图10.4](#ch10fig04)中，x轴代表你网络中的某个权重，y轴显示目标值如何随该权重变化。标记的点表示网络的当前状态。在理想情况下，你可以想象梯度下降会使标记的点滚下山谷并稳定在山谷中。
- en: If the learning rate is too small, as in [figure 10.5](#ch10fig05), the optimizer
    will move the weight in the right direction, but it will take many, many rounds
    of training to reach the minimum. For the sake of efficiency, you want the learning
    rate to be as large as possible without causing problems.
  id: totrans-1713
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率太小，如[图10.5](#ch10fig05)所示，优化器会将权重移动到正确的方向，但需要经过许多轮次的训练才能达到最小值。为了提高效率，你希望学习率尽可能大，同时不引起问题。
- en: Figure 10.5\. In this example, the learning rate is too small, and you need
    many updates before the weight reaches the minimum.
  id: totrans-1714
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5。在这个例子中，学习率太小，需要经过多次更新才能使权重达到最小值。
- en: '![](Images/10fig05_alt.jpg)'
  id: totrans-1715
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/10fig05_alt.jpg)'
- en: If you overshoot a little, the objective may not improve as much as it could.
    But the next gradient will point in the right direction, so it could bounce back
    and forth for a while, as shown in [figure 10.6](#ch10fig06).
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微超出一点，目标可能不会像理论上那样提高得那么多。但下一个梯度将指向正确的方向，所以它可能会像[图10.6](#ch10fig06)所示的那样来回弹跳一段时间。
- en: Figure 10.6\. Here, the learning rate is too large. The weight overshoots its
    target. On the next round of learning, the gradient will point in the opposite
    direction, but it’s likely to overshoot again on the next turn. This may cause
    the weight to bounce back and forth around the true minimum.
  id: totrans-1717
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6。在这里，学习率太大。权重超出了目标。在下一轮学习中，梯度将指向相反方向，但很可能会在下一轮再次超出。这可能导致权重在真实最小值周围来回弹跳。
- en: '![](Images/10fig06_alt.jpg)'
  id: totrans-1718
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/10fig06_alt.jpg)'
- en: 'In the example objective, if you overshoot a lot, the weight will end up in
    the flat region on the right. [Figure 10.7](#ch10fig07) shows how that can happen.
    The gradient is close to zero in that region, which means gradient descent no
    longer gives you a clue about which direction to move the weight. In that case,
    the objective can get stuck permanently. This isn’t just a theoretical problem:
    these flat regions are common in networks with rectified linear units, which we
    introduced in [chapter 6](kindle_split_018.xhtml#ch06). Deep-learning engineers
    sometimes refer to this problem as *dead ReLUs*: they’re considered “dead” because
    they get stuck returning a value of 0, and stop contributing to the overall learning
    process.'
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例目标函数中，如果你超出了很多，权重最终会落在右边的平坦区域。[图10.7](#ch10fig07)展示了这种情况是如何发生的。在那个区域，梯度接近于零，这意味着梯度下降不再给你提供关于权重移动方向的线索。在这种情况下，目标函数可能会永久地卡住。这不仅仅是一个理论问题：这些平坦区域在具有整流线性单元的网络中很常见，我们在[第6章](kindle_split_018.xhtml#ch06)中介绍了这种单元。深度学习工程师有时将这个问题称为*死ReLUs*：它们被认为是“死”的，因为它们卡在返回0的值，并停止对整体学习过程做出贡献。
- en: Figure 10.7\. In this case, the learning rate is so large that the weight jumped
    all the way to the flat region on the right. The gradient is 0 in that region,
    so the optimizer no longer has a clue which direction to go. The weight may be
    stuck there permanently. This is a common problem in neural networks that use
    rectified linear units.
  id: totrans-1720
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7。在这种情况下，学习率太大，权重跳到了右边的平坦区域。在那个区域，梯度为0，所以优化器不再知道该朝哪个方向移动。权重可能会永久地卡在那里。这是使用整流线性单元的神经网络中常见的问题。
- en: '![](Images/10fig07_alt.jpg)'
  id: totrans-1721
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/10fig07_alt.jpg)'
- en: Those are the problems that can happen when you overshoot in the correct direction.
    In policy gradient learning, the problem is even hairier, because you don’t know
    the true gradient you’re trying to follow. Somewhere in the universe, there’s
    a theoretical function that relates your agent’s playing strength to the weights
    of its policy network. But you have no way of writing down that function; the
    best you can do is estimate the gradient from the training data. That estimate
    is noisy and can sometimes point in the wrong direction. (Recall [figure 10.2](#ch10fig02)
    from [section 10.1](#ch10lev1sec1); the probability of choosing the best move
    frequently made a small step in the wrong direction. Self-play data for Go or
    a similarly complex game will be even noisier than that.)
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在正确方向上超出时可能发生的问题。在策略梯度学习中，问题更加复杂，因为你不知道你试图跟随的真实梯度。在宇宙的某个地方，有一个理论函数将你的智能体的游戏强度与其策略网络的权重联系起来。但你无法写下这个函数；你能做的最好的事情是从训练数据中估计梯度。这个估计是有噪声的，有时会指向错误的方向。（回想一下[图10.2](#ch10fig02)来自[第10.1节](#ch10lev1sec1)；选择最佳移动的概率经常在错误的方向上迈出小步。围棋或类似复杂游戏的自我对弈数据将比这更嘈杂。）
- en: 'If you move too far in the wrong direction, the weight may settle in the other
    valley on the left. [Figure 10.8](#ch10fig08) shows how this case can happen.
    This is called *forgetting*: the network learned to handle a certain property
    of the data set, and then suddenly that’s undone.'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你向错误的方向移动得太远，权重可能会落在左边的另一个山谷中。[图10.8](#ch10fig08)展示了这种情况是如何发生的。这被称为*遗忘*：网络学会了处理数据集的某些属性，然后突然这种属性被撤销了。
- en: Figure 10.8\. In policy gradient learning, you are trying to estimate the true
    gradient from a very noisy signal. Sometimes a single estimate will point in the
    wrong direction. If you move the weight too far in the wrong direction, it can
    jump all the way from the true minimum in the middle to the local minimum on the
    left, and it may get stuck there a while.
  id: totrans-1724
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8。在策略梯度学习中，你试图从一个非常嘈杂的信号中估计真实的梯度。有时一个单一的估计会指向错误的方向。如果你将权重向错误的方向移动得太远，它可能会从中间的真实最小值跳到左边的局部最小值，并且可能会在那里停留一段时间。
- en: '![](Images/10fig08_alt.jpg)'
  id: totrans-1725
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/10fig08_alt.jpg)'
- en: 'You can also take steps to improve the gradient estimates. Recall that stochastic
    gradient descent works on mini-batches: the optimizer takes a small subset of
    the training set, calculates the gradient from just those points, and then updates
    all the weights. Larger batch sizes tend to smooth out errors. The default batch
    size in Keras is 32, which is good for many supervised learning problems. For
    policy learning, we suggest making it a lot larger: try 1,024 or even 2,048 to
    start.'
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以采取步骤来改进梯度估计。回想一下，随机梯度下降在迷你批次上工作：优化器从训练集的小子集中取出一部分，仅从这些点计算梯度，然后更新所有权重。较大的批次大小往往可以平滑错误。Keras中的默认批次大小是32，这对于许多监督学习问题来说是个不错的选择。对于策略学习，我们建议将其做得更大：尝试1,024或甚至2,048开始。
- en: Finally, policy gradient learning is susceptible to getting stuck in *local
    maxima*—a case where any incremental change to the policy makes the bot weaker.
    Sometimes you can escape a local maximum by introducing a little extra randomness
    to the self-play. Some small fraction of the time (say 1% or 0.5% of turns), the
    agent can go off-policy and select a completely random move.
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，策略梯度学习容易陷入**局部最大值**——这是任何策略的增量变化都会使机器人变弱的情况。有时，通过在自我对弈中引入一点额外的随机性，你可以逃离局部最大值。有时（比如说，1%或0.5%的回合），代理可以偏离策略并选择一个完全随机的动作。
- en: 'Practically speaking, the policy gradient training process goes as follows:'
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，策略梯度训练过程如下：
- en: Generate a large batch of self-play games (as many as you can fit in memory).
  id: totrans-1729
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成大量自我对弈游戏（尽可能多，以适应内存）。
- en: Train.
  id: totrans-1730
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练。
- en: Test the updated bot against the previous version.
  id: totrans-1731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对更新的机器人进行与之前版本的测试。
- en: If the bot is measurably stronger, switch to this new version.
  id: totrans-1732
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果机器人明显更强，切换到这个新版本。
- en: If the bot is about the same strength, generate more games and train again.
  id: totrans-1733
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果机器人的实力大致相同，生成更多游戏并重新训练。
- en: If the bot gets significantly weaker, adjust the optimizer settings and retrain.
  id: totrans-1734
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果机器人明显变弱，调整优化器设置并重新训练。
- en: Tuning the optimizer may feel like threading a needle, but with a little practice
    and experimentation, you can develop a feel for it. [Table 10.1](#ch10table01)
    summarizes the tips we’ve covered in this section.
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: 调整优化器可能感觉像是在针尖上跳舞，但通过一点练习和实验，你可以培养出对它的感觉。[表10.1](#ch10table01)总结了本节中我们涵盖的技巧。
- en: Table 10.1\. Policy-learning troubleshooting
  id: totrans-1736
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.1\. 策略学习故障排除
- en: '| Symptom | Possible causes | Remedies |'
  id: totrans-1737
  prefs: []
  type: TYPE_TB
  zh: '| 症状 | 可能的原因 | 解决方案 |'
- en: '| --- | --- | --- |'
  id: totrans-1738
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Win rate is stuck at 50%. | Learning rate is too small. | Increase learning
    rate. |'
  id: totrans-1739
  prefs: []
  type: TYPE_TB
  zh: '| 胜率卡在50%。 | 学习率太小。 | 增加学习率。 |'
- en: '|   | Policy is at a local maximum. | Add more randomness to self-play. |'
  id: totrans-1740
  prefs: []
  type: TYPE_TB
  zh: '|   | 策略处于局部最大值。 | 在自我对弈中添加更多随机性。 |'
- en: '| Win rate drops significantly. | Overshoot | Decrease learning rate. |'
  id: totrans-1741
  prefs: []
  type: TYPE_TB
  zh: '| 胜率显著下降。 | 超出预期 | 降低学习率。 |'
- en: '|   | Bad gradient estimates | Increase batch size. |'
  id: totrans-1742
  prefs: []
  type: TYPE_TB
  zh: '|   | 梯度估计不良 | 增加批次大小。 |'
- en: '|   |   | Collect more self-play games. |'
  id: totrans-1743
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 收集更多自我对弈游戏。 |'
- en: 10.4\. Summary
  id: totrans-1744
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4\. 概述
- en: Policy learning is a reinforcement-learning technique that updates a policy
    from experience data. In the case of game playing, this means updating a bot to
    choose better moves based on the agent’s game results.
  id: totrans-1745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略学习是一种强化学习技术，它通过经验数据更新策略。在游戏玩法的案例中，这意味着根据代理的游戏结果更新机器人以选择更好的动作。
- en: One form of policy learning is to increase the probability of every move that
    happened in a win, and decrease the probability of every move that happened in
    a loss. Over thousands of games, this algorithm will slowly update the policy
    so that it wins more often. This algorithm is *policy gradient learning*.
  id: totrans-1746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种策略学习的形式是增加在胜利中发生的每个动作的概率，并减少在失败中发生的每个动作的概率。经过数千场比赛，这个算法将逐渐更新策略，使其赢得比赛的机会更多。这个算法是**策略梯度学习**。
- en: '*Cross-entropy loss* is a loss function designed for situations where you want
    to choose one out of a fixed set of options. In [chapter 7](kindle_split_019.xhtml#ch07),
    you used cross-entropy loss when predicting which move a human would choose in
    a given game situation. You can also adapt cross-entropy loss for policy gradient
    learning.'
  id: totrans-1747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉熵损失**是一种为选择固定选项集中的一个而设计的损失函数。在[第7章](kindle_split_019.xhtml#ch07)中，你使用交叉熵损失来预测在给定游戏情况下人类会选择哪个动作。你还可以将交叉熵损失应用于策略梯度学习。'
- en: You can implement policy gradient learning efficiently within the Keras framework
    by encoding the experience correctly, and then training by using cross-entropy
    loss.
  id: totrans-1748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过在Keras框架中正确编码经验，然后使用交叉熵损失进行训练，有效地实现策略梯度学习。
- en: Policy gradient training may require hand-tuning the optimizer settings. For
    policy gradient learning, you may need a smaller learning rate and a larger batch
    size than you’d use for supervised learning.
  id: totrans-1749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策梯度训练可能需要手动调整优化器的设置。对于政策梯度学习，你可能需要一个比监督学习更小的学习率和一个更大的批量大小。
- en: Chapter 11\. Reinforcement learning with value methods
  id: totrans-1750
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章. 使用价值方法的强化学习
- en: '*This chapter covers*'
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Making a self-improving game AI with the Q-learning algorithm
  id: totrans-1752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Q学习算法制作自我改进的游戏AI
- en: Defining and training multi-input neural networks in Keras
  id: totrans-1753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中定义和训练多输入神经网络
- en: Building and training a Q-learning agent by using Keras
  id: totrans-1754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras构建和训练Q学习代理
- en: Have you ever read an expert commentary on a high-level chess or Go tournament
    game? You’ll often see comments like, “Black is far behind at this point” or “The
    result up to here is slightly better for white.” What does it mean to be “ahead”
    or “behind” in the middle of such a strategy game? This isn’t basketball, with
    a running score to refer to. Instead, the commentator means that the board position
    is favorable to one player or the other. If you want to be precise, you could
    define it with a thought experiment. Find a hundred evenly matched pairs of players.
    Give each pair the board position from the middle of the game, and tell them to
    start playing from there. If the player taking black wins a small majority of
    the games—say, 55 out of 100—you can say the position was slightly good for black.
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经阅读过关于高级象棋或围棋锦标赛比赛的专家评论？你经常会看到这样的评论，“黑方此时落后很多”或者“到目前为止，白方的结果略好。”在这样一场策略游戏中，“领先”或“落后”是什么意思？这并不是篮球，有实时比分可以参考。相反，评论员的意思是棋盘位置对某一方有利。如果你想精确地定义它，你可以用一个思想实验来定义。找出一百对实力相当的对局。给每一对对局提供游戏中间的棋盘位置，并告诉他们从这里开始对局。如果执黑方的玩家赢得大多数游戏——比如说，100场中的55场——你可以说这个位置对黑方略有利。
- en: 'Of course, the commentators are doing no such thing. Instead, they’re relying
    on their own intuition, built up over thousands of games, to make a judgment on
    what might happen. In this chapter, we show how to train a computer game player
    to make similar judgments. And the computer will learn to do it in much the same
    way a human will: by playing many, many games.'
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，评论员并没有这样做。相反，他们依赖自己在数千场比赛中积累的直觉来判断可能发生的事情。在本章中，我们将展示如何训练计算机游戏玩家做出类似的判断。计算机将以与人类相似的方式学习这样做：通过玩很多很多游戏。
- en: This chapter introduces the *Q-learning* algorithm. Q-learning is a method for
    training a reinforcement-learning agent to anticipate how much reward it can expect
    in the future. (In the context of games, *reward* means *winning games*.) First,
    we describe how a Q-learning agent makes decisions and improves over time. After
    that, we show how to implement Q-learning within the Keras framework. Then you’ll
    be ready to train another self-improving game AI, with a different personality
    from the policy learning agent from [chapter 10](kindle_split_022.xhtml#ch10).
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了*Q学习*算法。Q学习是一种训练强化学习代理以预测未来可以期待多少奖励的方法。（在游戏的背景下，“奖励”意味着“赢得游戏”）。首先，我们描述了Q学习代理如何做出决策并在时间上改进。之后，我们展示了如何在Keras框架内实现Q学习。然后你将准备好训练另一个自我改进的游戏AI，它具有与第10章中政策学习代理不同的个性。[第10章](kindle_split_022.xhtml#ch10)。
- en: 11.1\. Playing games with Q-learning
  id: totrans-1758
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1. 使用Q学习玩游戏
- en: 'Suppose you have a function that tells you your chances of winning after playing
    a particular move. That’s called an *action-value function*—it tells you how valuable
    a particular action is. Then playing the game would be easy: you’d just pick the
    highest-value move on each turn. The question, then, is how you come up with an
    action-value function.'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个函数，告诉你执行特定移动后赢得比赛的机会。这被称为*动作值函数*——它告诉你特定动作的价值。那么玩游戏就会变得简单：你只需在每个回合选择价值最高的移动。那么问题来了，你是如何提出动作值函数的。
- en: 'This section describes *Q-learning*, a technique for training an action-value
    function through reinforcement learning. Of course, you can never learn the true
    action-value function for moves in Go: that would require reading out the entire
    game tree, with all its trillions of trillions of trillions of possibilities.
    But you can iteratively improve an *estimate* of the action-value function through
    self-play. As the estimate gets more accurate, a bot that relies on the estimate
    will get stronger.'
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了*Q学习*，这是一种通过强化学习训练动作值函数的技术。当然，你永远无法学习围棋中移动的真实动作值函数：那将需要读取整个游戏树，包括其数万亿的数万亿的可能性。但你可以通过自我对弈迭代地改进动作值函数的*估计*。随着估计越来越准确，依赖于这个估计的机器人将变得更强大。
- en: 'The name *Q-learning* comes from standard mathematical notation. Traditionally,
    *Q*(s,*a*) is used to represent the action-value function. This is a function
    of two variables: *s* represents the state the agent is faced with (for example,
    a board position); *a* represents an action the agent is considering (a possible
    move to play next). [Figure 11.1](#ch11fig01) illustrates the inputs to an action-value
    function. This chapter focuses on *deep Q-learning*, in which you use a neural
    network to estimate the Q function. But most of the principles also apply to classical
    Q-learning, where you approximate the Q function with a simple table that has
    a row for each possible state and a column for each possible action.'
  id: totrans-1761
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q学习*这个名字来源于标准的数学符号。传统上，*Q*(s,*a*)用来表示动作值函数。这是一个两个变量的函数：*s*代表代理所面临的状态（例如，棋盘位置）；*a*代表代理正在考虑的动作（下一个可能的移动）。[图11.1](#ch11fig01)说明了动作值函数的输入。本章重点介绍*深度Q学习*，其中你使用神经网络来估计Q函数。但大多数原则也适用于经典Q学习，其中你用一个简单的表格来近似Q函数，该表格为每个可能的状态有一行，为每个可能的动作有一列。'
- en: 'Figure 11.1\. An *action-value function* takes two inputs: a state (board position)
    and an action (proposed move). It returns an estimate of the expected return (chance
    of winning the game) if the agent chooses this action. The action-value function
    is traditionally called *Q* in mathematical notation.'
  id: totrans-1762
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1\. *动作值函数*接受两个输入：一个状态（棋盘位置）和一个动作（提议的移动）。如果代理选择这个动作，它返回预期的回报（赢得游戏的概率）的估计。在数学符号中，动作值函数传统上被称为*Q*。
- en: '![](Images/11fig01.jpg)'
  id: totrans-1763
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig01.jpg)'
- en: In the previous section, you studied reinforcement learning by directly learning
    a policy—a rule for selecting moves. The structure of Q-learning will seem familiar.
    First you make an agent play against itself, recording all the decisions and game
    results; the game results tell you something about whether the decisions were
    good; then you update the agent’s behavior accordingly. Q-learning differs from
    policy learning in the way the agent makes decisions in a game, and the way it
    updates its behavior based on the results.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你通过直接学习策略——选择移动的规则——来学习强化学习。Q学习的结构看起来很熟悉。首先，你让代理与自己对弈，记录所有决策和游戏结果；游戏结果告诉你关于决策是否良好的信息；然后你根据这些结果相应地更新代理的行为。Q学习与策略学习不同，它在于代理在游戏中的决策方式以及它根据结果更新行为的方式。
- en: To build a game-playing agent out of a Q function, you need to turn the Q function
    into a policy. One option is to plug in every possible move to the Q function
    and pick the move with the highest expected return, as shown in [figure 11.2](#ch11fig02).
    This policy is called a *greedy* policy.
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Q函数构建一个游戏代理，你需要将Q函数转换为策略。一个选项是将每个可能的移动插入到Q函数中，并选择具有最高预期回报的移动，如图11.2所示。这种策略被称为*贪婪*策略。
- en: Figure 11.2\. In a *greedy* action-value policy, you loop over every possible
    move and estimate the action value. Then you choose the action with the highest
    estimated value. (Many legal moves have been omitted to save space.)
  id: totrans-1766
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2\. 在*贪婪*的动作值策略中，你遍历每个可能的移动并估计动作值。然后你选择估计值最高的动作。（为了节省空间，省略了许多合法的移动。）
- en: '![](Images/11fig02_alt.jpg)'
  id: totrans-1767
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig02_alt.jpg)'
- en: 'If you have confidence in your estimates of the action values, a greedy policy
    is the best choice. But in order to *improve* your estimates, you need your bot
    to occasionally explore the unknown. This is called an ϵ*-greedy* policy: some
    fraction ϵ of the time, the policy chooses completely randomly; the rest of the
    time, it’s a normal greedy policy. [Figure 11.3](#ch11fig03) shows this procedure
    as a flowchart.'
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对自己的动作值估计有信心，那么贪婪策略是最好的选择。但为了*改进*你的估计，你需要你的机器人偶尔探索未知领域。这被称为ϵ*-贪婪*策略：在一段时间内，策略会随机选择；其余时间，它是一个正常的贪婪策略。[图11.3](#ch11fig03)展示了这个过程的流程图。
- en: Figure 11.3\. Flowchart for the ϵ-*greedy* action-value policy. This policy
    tries to balance playing the best move with exploring unknown moves. The value
    of ϵ controls that balance.
  id: totrans-1769
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3\. ϵ-*贪婪*动作价值策略的流程图。这个策略试图在玩最佳移动和探索未知移动之间取得平衡。ϵ的值控制着这种平衡。
- en: '![](Images/11fig03.jpg)'
  id: totrans-1770
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig03.jpg)'
- en: '|  |'
  id: totrans-1771
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1772
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: ϵ is the Greek letter epsilon, often used to represent a small fraction.
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
  zh: ϵ是希腊字母epsilon，通常用来表示一个小分数。
- en: '|  |'
  id: totrans-1774
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 11.1\. Pseudocode for an ϵ-greedy policy
  id: totrans-1775
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.1\. ϵ-贪婪策略的伪代码
- en: '[PRE173]'
  id: totrans-1776
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '***1* Random exploration case**'
  id: totrans-1777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 随机探索案例**'
- en: '***2* Picking the best known move**'
  id: totrans-1778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 选择最佳已知移动**'
- en: The choice of ϵ represents a trade-off. When it’s close to 0, the agent chooses
    whatever the best moves are, according to its current estimate of the action-value;
    but the agent won’t have any opportunity to try new moves and thereby improve
    its estimates. When ϵ is higher, the agent will lose more games, but in exchange
    it’ll learn about many unknown moves.
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: ϵ的选择代表了一种权衡。当它接近0时，智能体将根据其当前的动作值估计选择最佳移动；但智能体将没有机会尝试新的移动，从而改善其估计。当ϵ较高时，智能体会输掉更多游戏，但作为交换，它会了解许多未知移动。
- en: You can make an analogy with the way humans learn a skill, whether it’s playing
    Go or playing the piano. It’s common for human learners to hit a plateau—a point
    where they’re comfortable with a certain range of skills but have stopped improving.
    To get over the hump, you need to force yourself out of your comfort zone and
    experiment with new things. Maybe that’s new fingerings and rhythms on a piano,
    or new openings and tactics in Go. Your performance may get worse while you find
    yourself in unfamiliar situations, but after you learn how the new techniques
    work, you come out stronger than before.
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将人类学习技能的方式与之类比，无论是下围棋还是弹钢琴。学习者达到平台期是很常见的——在这个阶段，他们对一定范围内的技能感到舒适，但已经停止了进步。为了克服障碍，你需要强迫自己走出舒适区，尝试新事物。也许是在钢琴上尝试新的指法和节奏，或者在围棋中尝试新的开局和策略。在你发现自己处于不熟悉的情况时，你的表现可能会变差，但当你学会了新技术的运作方式后，你将比以前更强大。
- en: 'In Q-learning, you generally start with a fairly high value of ϵ, perhaps 0.5\.
    As the agent improves, you gradually decrease ϵ. Note that if ϵ drops all the
    way to 0, your agent will stop learning: it’ll just play out the same game over
    and over again.'
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中，你通常从一个相对较高的值开始，比如0.5。随着智能体能力的提升，你逐渐减小ϵ。请注意，如果ϵ降至0，你的智能体将停止学习：它将反复玩同样的游戏。
- en: After you’ve generated a large set of games, the training process for Q-learning
    is similar to supervised learning. The actions that the agent took provide your
    training set, and you can treat the game outcomes as known good labels for the
    data. Of course, there’ll be some lucky wins in the training set; but over the
    course of thousands of games, you can rely on an equal number of losses to cancel
    them out.
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成大量游戏后，Q学习的训练过程类似于监督学习。智能体采取的行动提供你的训练集，你可以将游戏结果视为已知的好标签。当然，训练集中会有一些幸运的胜利；但在数千场游戏的进程中，你可以依靠同样数量的失败来抵消它们。
- en: In the same way that the move-prediction model from [chapter 7](kindle_split_019.xhtml#ch07)
    learned to predict human moves from games it had never seen before, the action-value
    model can learn to predict the value of moves it has never played before. You
    can use the game outcomes as the target for the training process, as shown in
    [figure 11.4](#ch11fig04). To get it to generalize in this way, you need an appropriate
    neural network design and plenty of training data.
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
  zh: 就像第7章中提到的移动预测模型从它从未见过的游戏中学习预测人类移动一样，动作价值模型可以学习预测它从未玩过的移动的价值。你可以将游戏结果作为训练过程的目标，如图11.4所示。为了使它以这种方式泛化，你需要适当的神经网络设计和大量的训练数据。
- en: 'Figure 11.4\. Setting up training data for deep Q-learning. At the top, we
    show how we created training data for the move-prediction networks you used in
    [chapters 6](kindle_split_018.xhtml#ch06) and [7](kindle_split_019.xhtml#ch07).
    The board position was the input, and the actual move was the output. The bottom
    shows the structure for training data for Q-learning. Both the board position
    and chosen move are inputs; the output is the game outcome: a 1 for a win and
    a –1 for a loss.'
  id: totrans-1784
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.4\. 设置深度 Q-learning 的训练数据。在顶部，我们展示了如何为你在 [第 6 章](kindle_split_018.xhtml#ch06)
    和 [第 7 章](kindle_split_019.xhtml#ch07) 中使用的移动预测网络创建训练数据。棋盘位置是输入，实际移动是输出。底部显示了
    Q-learning 训练数据的结构。棋盘位置和选择的移动都是输入；输出是游戏结果：胜利为 1，失败为 -1。
- en: '![](Images/11fig04_alt.jpg)'
  id: totrans-1785
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 的替代文本](Images/11fig04_alt.jpg)'
- en: 11.2\. Q-learning with Keras
  id: totrans-1786
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 使用 Keras 的 Q-learning
- en: This section shows how to implement the Q-learning algorithm in the Keras framework.
    So far, you’ve used Keras to train functions that have one input and one output.
    Because an action-value function has two inputs, you’ll need to use new Keras
    features to design an appropriate network. After introducing two-input networks
    in Keras, we show how to evaluate moves, assemble training data, and train your
    agent.
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何在 Keras 框架中实现 Q-learning 算法。到目前为止，你已使用 Keras 训练具有一个输入和一个输出的函数。因为动作值函数有两个输入，你需要使用新的
    Keras 功能来设计合适的网络。在介绍了 Keras 中的双输入网络后，我们展示了如何评估移动、组装训练数据以及训练你的智能体。
- en: 11.2.1\. Building two-input networks in Keras
  id: totrans-1788
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1\. 在 Keras 中构建双输入网络
- en: In previous chapters, you used the Keras `Sequential` model to define your neural
    networks. The following listing shows an example model defined with the sequential
    API.
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你使用了 Keras 的 `Sequential` 模型来定义你的神经网络。以下列表展示了使用顺序 API 定义的示例模型。
- en: Listing 11.2\. Defining a model with the Keras sequential API
  id: totrans-1790
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.2\. 使用 Keras 顺序 API 定义模型
- en: '[PRE174]'
  id: totrans-1791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'Keras provides a second API for defining a neural network: the *functional*
    API. The functional API provides a superset of the functionality of the sequential
    API. You can rewrite any sequential network in the functional style, and you can
    also create complex networks that can’t be described in the sequential style.'
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了定义神经网络的第二个 API：*功能性* API。功能性 API 提供了顺序 API 功能的超集。你可以用功能性风格重写任何顺序网络，也可以创建无法用顺序风格描述的复杂网络。
- en: The main difference is in the way you specify the connections between layers.
    To connect layers in a sequential model, you repeatedly call `add` on the model
    object; that automatically connects the last layer’s output to the input of the
    new layer. To connect layers in a functional model, you pass the input layer to
    the next layer with syntax that looks like a function call. Because you’re explicitly
    creating each connection, you can describe more-complex networks. The following
    listing shows how to create an identical network to [listing 11.2](#ch11ex02)
    by using the functional style.
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于指定层之间连接的方式。要在顺序模型中连接层，你需要在模型对象上反复调用 `add`；这会自动将最后一层的输出连接到新层的输入。要在功能性模型中连接层，你需要使用类似于函数调用的语法将输入层传递给下一层。因为你正在显式地创建每个连接，所以你可以描述更复杂的网络。以下列表展示了如何使用功能性风格创建与
    [列表 11.2](#ch11ex02) 相同的网络。
- en: Listing 11.3\. Defining an identical model with the Keras functional API
  id: totrans-1794
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.3\. 使用 Keras 功能性 API 定义相同模型
- en: '[PRE175]'
  id: totrans-1795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '***1* Connects model_input to the input of a Dense layer, and names that layer
    hidden_layer**'
  id: totrans-1796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将模型输入连接到 Dense 层的输入，并将该层命名为 hidden_layer**'
- en: '***2* Connects hidden_layer to the input of a new Dense layer, and names that
    layer output_layer**'
  id: totrans-1797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将 hidden_layer 连接到新 Dense 层的输入，并将该层命名为 output_layer**'
- en: These two models are identical. The sequential API is a convenient way to describe
    the most common neural networks, and the functional API provides the flexibility
    to specify multiple inputs and outputs, or complex connections.
  id: totrans-1798
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型是相同的。顺序 API 是描述最常见神经网络的便捷方式，而功能性 API 提供了指定多个输入和输出或复杂连接的灵活性。
- en: Because your action-value network has two inputs and one output, at some point
    you need to merge the two input chains together. The Keras `Concatenate` layer
    lets you accomplish this. A `Concatenate` layer doesn’t do any computation; it
    just glues together two vectors or tensors into one, as shown in [figure 11.5](#ch11fig05).
    It takes an optional `axis` argument that specifies which dimension to concatenate;
    it defaults to the last dimension, which is what you want in this case. All other
    dimensions must be the same size.
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您的动作值网络有两个输入和一个输出，所以您需要在某个时候将两个输入链合并在一起。Keras的`Concatenate`层让您能够完成这个任务。一个`Concatenate`层不做任何计算；它只是将两个向量或张量粘合在一起，如[图11.5](#ch11fig05)所示。它接受一个可选的`axis`参数，指定要连接的维度；默认为最后一个维度，这正是您想要的。所有其他维度的大小必须相同。
- en: Figure 11.5\. The Keras `Concatenate` layer appends two tensors into one.
  id: totrans-1800
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.5\. Keras的`Concatenate`层将两个张量连接到一起。
- en: '![](Images/11fig05.jpg)'
  id: totrans-1801
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5](Images/11fig05.jpg)'
- en: 'Now you can design a network for learning an action-value function. Recall
    the convolutional networks you used for move prediction in [chapters 6](kindle_split_018.xhtml#ch06)
    and [7](kindle_split_019.xhtml#ch07). You can conceptually chunk the network into
    two steps: first the convolutional layers identify the important shapes of stones
    on the board; then a dense layer makes a decision based on those shapes. [Figure
    11.6](#ch11fig06) shows how the layers in the move-prediction network perform
    two separate roles.'
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以设计一个网络来学习动作值函数。回想一下您在第6章和第7章中用于移动预测的卷积网络。您可以从概念上将网络分成两个步骤：首先，卷积层识别棋盘上重要的棋子形状；然后一个密集层基于这些形状做出决策。[图11.6](#ch11fig06)显示了移动预测网络中的层如何执行两个不同的角色。
- en: Figure 11.6\. A move-prediction network, as covered in [chapters 6](kindle_split_018.xhtml#ch06)
    and [7](kindle_split_019.xhtml#ch07). Although there are many layers, you can
    conceptually think of it as two steps. The convolutional layers process the raw
    stones and organize them into logical groups and tactical shapes. From that representation,
    the dense layers can choose an action.
  id: totrans-1803
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.6\. 一种移动预测网络，如第6章和第7章所述。尽管有很多层，但你可以从概念上将其视为两个步骤。卷积层处理原始棋子并将它们组织成逻辑组和战术形状。从这个表示中，密集层可以选择一个动作。
- en: '![](Images/11fig06_alt.jpg)'
  id: totrans-1804
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6](Images/11fig06_alt.jpg)'
- en: For the action-value network, you still want to process the board into important
    shapes and groups of stones. Any shape that’s relevant for move prediction is
    likely to be relevant for estimating the action-value as well, so this part of
    the network can borrow the same structure. The difference comes in the decision-making
    step. Instead of making a decision based only on the identified groups of stones,
    you want to estimate a value based on the processed board *and* the proposed action.
    So you can bring in the proposed move vector after the convolutional layers. [Figure
    11.8](#ch11fig08) illustrates such a network.
  id: totrans-1805
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动作值网络，您仍然希望将棋盘处理成重要的形状和棋子组。任何与移动预测相关的形状都可能对估计动作值也相关，因此这部分网络可以借用相同的结构。区别在于决策步骤。您不仅基于识别的棋子组做出决策，还想根据处理过的棋盘和提出的动作估计一个值。因此，您可以在卷积层之后引入提出的移动向量。[图11.8](#ch11fig08)说明了这样的网络。
- en: Because you use –1 to represent a loss, and 1 to represent a win, the action-value
    should be a single value in the range of –1 to 1\. To achieve this, you add a
    `Dense` layer of size 1 with a `tanh` activation. You may know tanh as the hyperbolic
    tangent function from trigonometry. In deep learning, you don’t care about the
    trigonometric properties of tanh at all. Instead, you use it because it’s a smooth
    function that’s bounded below by –1 and above by 1\. No matter what the early
    layers of the network compute, the output will be in the desired range. [Figure
    11.7](#ch11fig07) shows a plot of the tanh function.
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您用-1来表示损失，用1来表示胜利，所以动作值应该在-1到1的范围内。为了实现这一点，您添加了一个大小为1的`Dense`层，并使用`tanh`激活函数。您可能知道tanh是三角学中的双曲正切函数。在深度学习中，您根本不关心tanh的三角学属性。相反，您使用它是因为它是一个平滑的函数，其值在-1以下且在1以上有界。无论网络的早期层计算什么，输出都将处于所需的范围内。[图11.7](#ch11fig07)显示了tanh函数的图像。
- en: Figure 11.7\. The tanh (hyperbolic tangent) function, which clamps its value
    between –1 and 1
  id: totrans-1807
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.7\. 双曲正切（hyperbolic tangent）函数，其值被限制在-1和1之间
- en: '![](Images/11fig07_alt.jpg)'
  id: totrans-1808
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7](Images/11fig07_alt.jpg)'
- en: The full specification for your action-value network looks like the following
    listing.
  id: totrans-1809
  prefs: []
  type: TYPE_NORMAL
  zh: 您的动作值网络的完整规范如下所示。
- en: Listing 11.4\. A two-input action-value network
  id: totrans-1810
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.4. 一个双输入动作值网络
- en: '[PRE176]'
  id: totrans-1811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '***1* Add as many convolutional layers as you like. Anything that worked well
    for move prediction ought to work well here.**'
  id: totrans-1812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 添加尽可能多的卷积层。任何对移动预测有效的东西在这里也应该有效。**'
- en: '***2* You may want to experiment with the size of this hidden layer.**'
  id: totrans-1813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 您可能想尝试调整这个隐藏层的大小。**'
- en: '***3* The tanh activation layer clamps the output between –1 and 1.**'
  id: totrans-1814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* tanh激活层将输出限制在-1和1之间。**'
- en: Figure 11.8\. The two-input neural network described in [listing 11.4](#ch11ex04).
    The game board goes through several convolutional layers, just like the move-prediction
    network from [chapter 7](kindle_split_019.xhtml#ch07). The proposed move goes
    into a separate input. The proposed move is combined with the output of the convolutional
    layers and passed through another dense layer.
  id: totrans-1815
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.8. 第11.4列表中描述的两个输入神经网络。游戏棋盘经过几个卷积层，就像第7章中移动预测网络一样。提议的移动进入一个单独的输入。提议的移动与卷积层的输出结合，并通过另一个密集层传递。
- en: '![](Images/11fig08.jpg)'
  id: totrans-1816
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/11fig08.jpg)'
- en: 11.2.2\. Implementing the ϵ-greedy policy with Keras
  id: totrans-1817
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2. 使用Keras实现ε-greedy策略
- en: 'Let’s start building a `QAgent` that can learn via Q-learning; this code will
    live in the dlgo/rl/q.py module. [Listing 11.5](#ch11ex05) shows the constructor:
    just as in our policy learning agent, it takes a model and a board encoder. You
    also define two utility methods. The `set_temperature` method lets you change
    the value of ϵ, which you’ll want to vary across the training process. Just as
    in [chapter 9](kindle_split_021.xhtml#ch09), the `set_collector` method lets you
    attach an `ExperienceCollector` object to store the experience data for later
    training.'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建一个可以通过Q学习学习的`QAgent`；此代码将位于dlgo/rl/q.py模块中。[列表11.5](#ch11ex05)显示了构造函数：就像我们的策略学习智能体一样，它接受一个模型和一个棋盘编码器。您还定义了两个实用方法。`set_temperature`方法允许您更改ϵ的值，您将在训练过程中想要改变这个值。正如第9章中所述，`set_collector`方法允许您附加一个`ExperienceCollector`对象以存储用于后续训练的经验数据。
- en: Listing 11.5\. Constructor and utility methods for a Q-learning agent
  id: totrans-1819
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.5. Q学习智能体的构造函数和实用方法
- en: '[PRE177]'
  id: totrans-1820
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '***1* temperature is the ϵ value that controls how randomized the policy is.**'
  id: totrans-1821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 温度是控制策略随机化的ϵ值。**'
- en: '***2* See [chapter 9](kindle_split_021.xhtml#ch09) for more information about
    using a collector object to record the agent’s experiences.**'
  id: totrans-1822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 有关使用收集器对象记录智能体经验的更多信息，请参见第9章。**'
- en: Next, you implement the ϵ-greedy policy. Instead of just picking the top-rated
    move, you sort all the moves and try them in order. As in [chapter 9](kindle_split_021.xhtml#ch09),
    this prevents the agent from self-destructing at the end of a won game.
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您实现ε-greedy策略。不是只选择评分最高的移动，而是对所有移动进行排序并按顺序尝试。正如第9章中所述，这可以防止智能体在赢得游戏结束时自我毁灭。
- en: Listing 11.6\. Selecting moves for a Q-learning agent
  id: totrans-1824
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.6. 为Q学习智能体选择移动
- en: '[PRE178]'
  id: totrans-1825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '***1* Generates a list of all valid moves**'
  id: totrans-1826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 生成所有有效移动的列表**'
- en: '***2* If there are no valid moves left, the agent can just pass.**'
  id: totrans-1827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果没有有效的移动，智能体可以简单地跳过。**'
- en: '***3* One-hot encodes all the valid moves (see [chapter 5](kindle_split_017.xhtml#ch05)
    for more on one-hot encoding).**'
  id: totrans-1828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 对所有有效移动进行one-hot编码（有关one-hot编码的更多信息，请参见第5章）。**'
- en: '***4* This is the two-input form of predict: you pass the two inputs as a list.**'
  id: totrans-1829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 这是预测的两个输入形式：您将两个输入作为列表传递。**'
- en: '***5* Values will be an *N* × 1 matrix, where *N* is the number of legal moves;
    the reshape call converts to a vector of size *N*.**'
  id: totrans-1830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 值将是一个*N* × 1的矩阵，其中*N*是合法移动的数量；reshape调用将其转换为大小为*N*的向量。**'
- en: '***6* Ranks the moves according to the ϵ-greedy policy**'
  id: totrans-1831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 根据ε-greedy策略对移动进行排序**'
- en: '***7* Picks the first non-self-destructive move in your list, similar to the
    self-play agents from [chapter 9](kindle_split_021.xhtml#ch09)**'
  id: totrans-1832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 选择列表中的第一个非自我毁灭性移动，类似于第9章中自我博弈智能体**'
- en: '***8* Records the decision in an experience buffer; see [chapter 9](kindle_split_021.xhtml#ch09)**'
  id: totrans-1833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 在经验缓冲区中记录决策；参见第9章**'
- en: '***9* You’ll fall through here if all the valid moves are determined to be
    self-destructive.**'
  id: totrans-1834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 如果所有有效移动都被判定为自我毁灭性，您将在这里掉落。**'
- en: '|  |'
  id: totrans-1835
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Q-learning and tree search**'
  id: totrans-1836
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q学习与树搜索**'
- en: 'The structure of the `select_move` implementation is similar to some of the
    tree-search algorithms we covered in [chapter 4](kindle_split_016.xhtml#ch04).
    For example, alpha-beta search relies on a board evaluation function: a function
    that takes a board position and estimates which player is ahead and by how much.
    This is similar, but not identical, to the action-value function we’ve covered
    in this chapter. Suppose the agent is playing as black and evaluating some move
    *X*. It gets an estimated action-value of 0.65 for *X*. Now, you know exactly
    what the board will look like after playing move *X*, and you know that a win
    for black is a loss for white. So you can say that the next board position has
    a value of –0.65 for white.'
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
  zh: '`select_move`实现的架构与我们在第4章中介绍的一些树搜索算法类似。例如，alpha-beta搜索依赖于一个棋盘评估函数：一个接受棋盘位置并估计哪个玩家领先以及领先多少的函数。这与我们在本章中介绍的动作值函数类似，但并不相同。假设代理正在扮演黑方并评估某个移动*X*。它为*X*得到一个估计的动作值0.65。现在，你知道在玩移动*X*后棋盘将是什么样子，你也知道黑方的胜利意味着白方的失败。因此，你可以认为下一个棋盘位置对白方的价值为-0.65。'
- en: 'Mathematically, we describe the relationship as follows:'
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上描述这种关系如下：
- en: '*Q*(*s,a*) = –*V*(*s*′)'
  id: totrans-1839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q*(*s,a*) = –*V*(*s*′)'
- en: where *s*’ is the state that white sees after black chooses move *a*.
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*s*’是黑方选择移动*a*后白方看到的那个状态。
- en: Although Q-learning in general can apply to any environment, this equivalence
    between the action-value of one state and value of the next state is true only
    in deterministic games.
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Q学习在一般情况下可以应用于任何环境，但这种一个状态的动作值与下一个状态的价值之间的等价性只在确定性游戏中成立。
- en: '[Chapter 12](kindle_split_024.xhtml#ch12) covers a third reinforcement-learning
    technique that includes learning a value function directly, instead of an action-value
    function. [Chapters 13](kindle_split_026.xhtml#ch13) and [14](kindle_split_027.xhtml#ch14)
    show methods for integrating such a value function with a tree-search algorithm.'
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章](kindle_split_024.xhtml#ch12)介绍了一种第三种强化学习技术，它包括直接学习价值函数，而不是动作值函数。[第13章](kindle_split_026.xhtml#ch13)和[第14章](kindle_split_027.xhtml#ch14)展示了将此类价值函数与树搜索算法集成的方
    法。'
- en: '|  |'
  id: totrans-1843
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'All that remains is the code to sort the moves in order from most valuable
    to least valuable. The complication is that you have two parallel arrays: `values`
    and `moves`. The NumPy `argsort` function provides a convenient way to handle
    this. Instead of sorting an array in place, `argsort` returns a list of indices.
    Then you can read off the elements of the parallel array according to those indices.
    [Figure 11.9](#ch11fig09) illustrates how `argsort` works. [Listing 11.7](#ch11ex07)
    shows how to rank the moves by using `argsort`.'
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: 所有剩下的就是按照从最有价值到最无价值排序移动的代码。复杂性在于你有两个并行数组：`values`和`moves`。NumPy的`argsort`函数提供了一个方便的方式来处理这个问题。`argsort`不是在原地排序数组，而是返回一个索引列表。然后你可以根据这些索引读取并行数组的元素。[图11.9](#ch11fig09)说明了`argsort`是如何工作的。[列表11.7](#ch11ex07)展示了如何使用`argsort`来对移动进行排序。
- en: Figure 11.9\. An illustration of the `argsort` function from the NumPy library.
    `argsort` takes a vector of values that you want to sort. Instead of sorting the
    values directly, it returns a vector of indices that will give you the values
    in sorted order. So the first value of the output vector is the index of the smallest
    value in the input, and the last value of the output vector is the index of the
    largest value in the input.
  id: totrans-1845
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.9\. NumPy库中`argsort`函数的说明。`argsort`接受一个要排序的值向量。它不是直接排序这些值，而是返回一个索引向量，这些索引将给出排序后的值。因此，输出向量的第一个值是输入中最小值的索引，输出向量的最后一个值是输入中最大值的索引。
- en: '![](Images/11fig09.jpg)'
  id: totrans-1846
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/11fig09.jpg)'
- en: Listing 11.7\. Selecting moves for a Q-learning agent
  id: totrans-1847
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.7\. 为Q学习代理选择移动
- en: '[PRE179]'
  id: totrans-1848
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '***1* In the exploration case, rank the moves by random numbers instead of
    the real values.**'
  id: totrans-1849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在探索的情况下，通过随机数而不是真实值来对移动进行排序。**'
- en: '***2* Gets the indices of the moves in order from least value to highest value**'
  id: totrans-1850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 获取从最低值到最高值的移动索引**'
- en: '***3* The [::-1] syntax is the most efficient way to reverse a vector in NumPy.
    This returns the moves in order from highest value to least value.**'
  id: totrans-1851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* NumPy中[::-1]语法是反转向量的最有效方式。这将以从最高值到最低值的顺序返回移动。**'
- en: With this in place, you’re ready to generate self-play games with your Q-learning
    agent. Next, we cover how to train the action-value network.
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，你就可以使用你的Q学习代理生成自我对弈游戏了。接下来，我们将介绍如何训练动作值网络。
- en: 11.2.3\. Training an action-value function
  id: totrans-1853
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.3\. 训练动作值函数
- en: After you have a batch of experience data, you’re ready to update the agent’s
    network. With policy gradient learning, you knew the approximate gradient that
    you wanted, but you had to come up with a complicated scheme to apply that gradient
    update inside the Keras framework. In contrast, training with Q-learning is a
    straightforward application of the Keras `fit` function. You can directly put
    the game results into the target vector.
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: 在你有一批经验数据后，你就可以更新代理的网络了。使用策略梯度学习，你知道你想要的近似梯度，但你必须想出一个复杂的方案来在Keras框架内应用该梯度更新。相比之下，使用Q-learning进行训练是Keras
    `fit`函数的直接应用。你可以直接将游戏结果放入目标向量中。
- en: '[Chapter 6](kindle_split_018.xhtml#ch06) covered two loss functions: mean squared
    error and cross-entropy loss. You used cross-entropy loss when you wanted to match
    one out of a discrete set of items: in that case, you were trying to match one
    of the points on a Go board. The Q function, on the other hand, has a continuous
    value that can be anywhere in the range –1 to 1\. For this problem, we prefer
    mean squared error.'
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](kindle_split_018.xhtml#ch06)介绍了两种损失函数：均方误差和交叉熵损失。当你想要匹配一组离散项目中的一个时，你使用了交叉熵损失：在这种情况下，你试图匹配围棋棋盘上的一个点。另一方面，Q函数有一个连续值，可以在-1到1的范围内任何地方。对于这个问题，我们更喜欢均方误差。'
- en: The following listing shows the implementation of a `train` function for the
    `QAgent` class.
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了`QAgent`类的`train`函数的实现。
- en: Listing 11.8\. Training the Q-learning agent from its experience
  id: totrans-1857
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.8\. 从其经验中训练Q-learning代理
- en: '[PRE180]'
  id: totrans-1858
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '***1* lr and batch_size are options to fine-tune the training process. See
    [chapter 10](kindle_split_022.xhtml#ch10) for more discussion.**'
  id: totrans-1859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* lr和batch_size是用于微调训练过程的选项。有关更多讨论，请参阅[第10章](kindle_split_022.xhtml#ch10)。**'
- en: '***2* mse is mean squared error. You use mse instead of categorical_crossentropy
    because you’re trying to learn a continuous value.**'
  id: totrans-1860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* mse是均方误差。你使用mse而不是categorical_crossentropy，因为你试图学习一个连续值。**'
- en: '***3* Passes the two different inputs as a list**'
  id: totrans-1861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将两个不同的输入作为列表传递**'
- en: 11.3\. Summary
  id: totrans-1862
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3\. 概述
- en: An *action-value function* estimates how much reward an agent can expect after
    taking a specific action. In the case of games, this means the expected chance
    of winning.
  id: totrans-1863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动作值函数*估计代理在采取特定动作后可以期望获得多少奖励。在游戏的情况下，这意味着预期的获胜概率。'
- en: '*Q-learning* is the technique of reinforcement learning by estimating an action-value
    function (traditionally notated as Q).'
  id: totrans-1864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q-learning* 是一种通过估计动作值函数（传统上表示为Q）的强化学习技术。'
- en: While training a Q-learning agent, you normally use an ϵ*-greedy policy*. Under
    this policy, the agent will select the highest-valued move a fraction of the time,
    and a random move the rest of the time. The parameter ϵ controls how much the
    agent will explore unknown moves in order to learn more about them.
  id: totrans-1865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练Q-learning代理时，你通常使用一个ϵ*-greedy策略*。在这个策略下，代理将选择最高价值的移动一部分时间，其余时间随机移动。参数ϵ控制代理将探索未知移动以了解更多信息。
- en: The Keras functional API lets you design neural networks with multiple inputs,
    multiple outputs, or complex internal connections. For Q-learning, you can use
    the functional API to build a network with separate inputs for the game state
    and the proposed move.
  id: totrans-1866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras功能API允许你设计具有多个输入、多个输出或复杂内部连接的神经网络。对于Q-learning，你可以使用功能API构建一个具有游戏状态和提议移动的单独输入的网络。
- en: Chapter 12\. Reinforcement learning with actor-critic methods
  id: totrans-1867
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12章\. 使用actor-critic方法的强化学习
- en: '*This chapter covers*'
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using advantage to make reinforcement learning more efficient
  id: totrans-1869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用优势提高强化学习的效率
- en: Making a self-improving game AI with the actor-critic method
  id: totrans-1870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用actor-critic方法制作自我改进的游戏AI
- en: Designing and training multi-output neural networks in Keras
  id: totrans-1871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中设计和训练多输出神经网络
- en: If you’re learning to play Go, one of the best ways to improve is to get a stronger
    player to review your games. Sometimes the most useful feedback just points out
    where you won or lost the game. The reviewer might give comments like, “You were
    already far behind by move 30” or “At move 110, you had a winning position, but
    your opponent turned it around by move 130.”
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在学习玩围棋，提高水平的一种最好的方法是让一个更强的玩家来回顾你的比赛。有时最有用的反馈只是指出你在哪里赢了或输了比赛。评论者可能会给出这样的评论：“你在第30步时就已经落后很多”或“在第110步时，你处于胜利的位置，但你的对手在第130步时扭转了局势。”
- en: Why is this feedback helpful? You may not have time to scrutinize all 300 moves
    in a game, but you can focus your full attention on a 10- or 20-move sequence.
    The reviewer lets you know which parts of the game are important.
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这种反馈有帮助？你可能没有时间仔细审查游戏中所有的300步，但你可以将全部注意力集中在10步或20步的序列上。评论者会让你知道游戏的哪些部分是重要的。
- en: 'Reinforcement-learning researchers apply this principle in *actor-critic learning*,
    which is a combination of policy learning (as covered in [chapter 10](kindle_split_022.xhtml#ch10))
    and value learning (as covered in [chapter 11](kindle_split_023.xhtml#ch11)).
    The policy function plays the role of the *actor*: it picks what moves to play.
    The value function is the *critic*: it tracks whether the agent is ahead or behind
    in the course of the game. That feedback guides the training process, in the same
    way that a game review can guide your own study.'
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习研究人员将这一原则应用于*演员-评论家学习*，这是一种策略学习（如第10章所述）和价值学习（如第11章所述）的结合。策略函数扮演着*演员*的角色：它选择要采取的步骤。价值函数是*评论家*：它跟踪代理在游戏过程中的领先或落后情况。这种反馈引导训练过程，就像游戏评论可以指导你的学习一样。
- en: 'This chapter describes how to make a self-improving game AI with actor-critic
    learning. The key concept that makes it all work is *advantage*, the difference
    between the actual game outcome and the expected outcome. We start by illustrating
    how advantage can improve the training process. After that, we’re ready to build
    an actor-critic game agent. First we show how to implement move selection; then
    we implement the new training process. In both functions, we borrow heavily from
    the code examples in [chapters 10](kindle_split_022.xhtml#ch10) and [11](kindle_split_023.xhtml#ch11).
    The end result is the best of both worlds: it combines the benefits of policy
    learning and Q-learning into one agent.'
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了如何使用演员-评论家学习制作一个自我改进的游戏人工智能。使这一切都发挥作用的关键概念是"优势"，即实际游戏结果与预期结果之间的差异。我们首先说明优势如何改善训练过程。之后，我们就准备好构建一个演员-评论家游戏代理。首先，我们展示如何实现移动选择；然后，我们实现新的训练过程。在这两个函数中，我们大量借鉴了[第10章](kindle_split_022.xhtml#ch10)和[第11章](kindle_split_023.xhtml#ch11)中的代码示例。最终结果是两者的最佳结合：它将策略学习和Q学习的优点结合到一个代理中。
- en: 12.1\. Advantage tells you which decisions are important
  id: totrans-1876
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1\. 优势告诉你哪些决策是重要的
- en: In [chapter 10](kindle_split_022.xhtml#ch10), we briefly mentioned the credit-assignment
    problem. Suppose your learning agent played a game with 200 moves and ultimately
    won the game. Because it won, you can assume it chose at least a few good moves,
    but it probably chose a couple of bad moves as well. *Credit assignment* is the
    problem of separating the good moves, which you want to reinforce, from the bad
    moves, which you should ignore. This section introduces the concept of *advantage*,
    a formula for estimating how much a particular decision contributed to the final
    result. First we describe how advantage helps with credit assignment; then we
    provide code samples showing how to calculate it.
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](kindle_split_022.xhtml#ch10)中，我们简要提到了信用分配问题。假设你的学习代理玩了一款200步的游戏并最终赢得了比赛。因为它赢了，你可以假设它至少选择了一些好的步骤，但可能也选择了一些不好的步骤。"信用分配"是将你想要强化的好步骤与应该忽略的坏步骤分开的问题。本节介绍了"优势"的概念，这是一个估计特定决策对最终结果贡献大小的公式。首先，我们描述优势如何帮助信用分配；然后，我们提供代码示例来展示如何计算它。
- en: 12.1.1\. What is advantage?
  id: totrans-1878
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.1\. 什么是优势？
- en: Imagine you’re watching a basketball game; while the fourth quarter ticks down,
    your favorite player nails a three-pointer. How excited do you get? It depends
    on the game state. If the score is 80 to 78, you’re probably jumping out of your
    seat. If the score is 110 to 80, you’re indifferent. What’s the difference? In
    a close game, a three-point swing creates a huge change in the expected outcome
    of the game. On the other hand, if the game is a blowout, a single play won’t
    affect the result. The most important plays happen while the outcome is still
    in doubt. In reinforcement learning, advantage is a formula that quantifies this
    concept.
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在观看一场篮球比赛；当第四节时间即将结束时，你最喜欢的球员投中了一个三分球。你会有多兴奋？这取决于比赛的状态。如果比分是80比78，你可能会从座位上跳起来。如果比分是110比80，你可能就无所谓了。有什么区别呢？在一场接近的比赛里，三分球的得分会极大地改变比赛的预期结果。另一方面，如果比赛是一场一边倒的比赛，单次进攻并不会影响结果。最重要的比赛发生在结果仍然不确定的时候。在强化学习中，优势是一个量化这一概念的公式。
- en: To calculate advantage, you first need an estimate of the value of a state,
    which we denote as *V*(*s*). This is the expected return the agent will see, given
    that it has already arrived at a particular state *s*. In games, you can think
    of *V*(*s*) as indicating whether the board position is good for black or white.
    If *V*(*s*) is close to 1, your agent is in a favorable position; if *V*(*s*)
    is close to –1, your agent is losing.
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算收益，你首先需要一个状态的值估计，我们用*V*(*s*)表示。这是代理在到达特定状态*s*时预期的回报。在游戏中，你可以将*V*(*s*)视为指示棋盘位置对黑方或白方是否有利。如果*V*(*s*)接近1，你的代理处于有利位置；如果*V*(*s*)接近-1，你的代理处于劣势。
- en: If you recall the action-value function *Q*(*s*,*a*) from the previous chapter,
    the concept is similar. The difference is that *V*(*s*) represents how favorable
    the board is *before* you choose a move; *Q*(*s*,*a*) represents how favorable
    the board is *after* you choose a move.
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回忆起上一章中的动作值函数*Q*(*s*,*a*)，概念是相似的。区别在于*V*(*s*)代表在你选择走法之前棋盘的优劣；*Q*(*s*,*a*)代表在你选择走法之后棋盘的优劣。
- en: 'The definition of advantage is usually specified as follows:'
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
  zh: 收益的定义通常如下所述：
- en: '| *A* = *Q*(*s*, *a*) – *V*(*s*) |'
  id: totrans-1883
  prefs: []
  type: TYPE_TB
  zh: '| *A* = *Q*(*s*, *a*) – *V*(*s*) |'
- en: 'One way to think of this is that if you’re in a good state (that is, *V*(*s*)
    is high), but you make a terrible move (*Q*(*s*,*a*) is low), you give away your
    advantage: hence the calculation is negative. One problem with this formula, however,
    is that you don’t know how to calculate *Q*(*s*,*a*). But you can consider the
    reward you get at the end of the game as an unbiased estimate of the true *Q*.
    So you can wait until you get your reward *R*, and then estimate the advantage
    as follows:'
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样理解：如果你处于一个好的状态（即*V*(*s*)较高），但你却做出了一步糟糕的走法（*Q*(*s*,*a*)较低），你就失去了你的优势：因此计算结果是负数。然而，这个公式的缺点是，你不知道如何计算*Q*(*s*,*a*)。但你可以考虑游戏结束时的奖励作为对真实*Q*的无偏估计。因此，你可以等待获得奖励*R*，然后按照以下方式估计收益：
- en: '| *A* = *R* – *V*(*s*) |'
  id: totrans-1885
  prefs: []
  type: TYPE_TB
  zh: '| *A* = *R* – *V*(*s*) |'
- en: That’s the calculation you’ll use to estimate advantage throughout this chapter.
    Let’s see how this value is useful.
  id: totrans-1886
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本章中用来估计收益的计算方法。让我们看看这个值是如何有用的。
- en: For the purposes of illustration, you’ll pretend that you already have an accurate
    way to estimate *V*(*s*). In reality, your agent learns its value-estimating function
    and its policy function simultaneously. The next section covers how that works.
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，你将假装你已经找到了一种准确估计*V*(*s*)的方法。实际上，你的代理同时学习其价值估计函数和策略函数。下一节将介绍这是如何工作的。
- en: 'Let’s work through a few examples:'
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几个例子来分析：
- en: 'At the beginning of a game, *V*(*s*) = 0: both players have a roughly equal
    chance. Suppose your agent wins the game; then its reward will be 1, so the advantage
    of its first move is 1 – 0 = 1.'
  id: totrans-1889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在游戏开始时，*V*(*s*) = 0：两位玩家有大致相等的胜算。假设你的代理赢得了游戏；那么它的奖励将是1，因此它的第一步的收益是1 – 0 = 1。
- en: Imagine that the game is almost over and your agent has practically locked the
    game up, so V(s) = 0.95\. If your agent does indeed win the game, the advantage
    from that state is 1 – 0.95 = 0.05.
  id: totrans-1890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设游戏即将结束，你的代理实际上已经锁定了游戏，因此V(s) = 0.95。如果代理确实赢得了游戏，那么从该状态获得的收益是1 – 0.95 = 0.05。
- en: Now imagine your agent has another winning position, where once again *V*(*s*)
    = 0.95\. But in this game, your bot somehow blunders away the end game and loses,
    giving it a reward of –1\. Its advantage from that state is –1 – 0.95 = –1.95.
  id: totrans-1891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在想象你的代理又有一个获胜的位置，其中*V*(*s*)再次等于0.95。但在这次游戏中，你的机器人不知何故在游戏结束时犯了大错，输掉了比赛，得到了-1的奖励。从该状态获得的收益是-1
    – 0.95 = -1.95。
- en: '[Figures 12.1](#ch12fig01) and [12.2](#ch12fig02) illustrate the advantage
    calculation for a hypothetical game. In this game, your learning agent slowly
    pulled ahead over the first few moves; then it made some big mistakes and fell
    all the way to a lost position. Somewhere before move 150, it suddenly managed
    to reverse the game and finally cruised to a win. Under the policy gradient technique
    from [chapter 10](kindle_split_022.xhtml#ch10), you’d weight each move equally
    in this game. With actor-critic learning, you want to find the most important
    moves and give them greater weight. The advantage calculation shows you how.'
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.1](#ch12fig01) 和 [12.2](#ch12fig02) 展示了一个假设游戏的收益计算。在这个游戏中，你的学习代理在最初的几步中逐渐领先；然后它犯了一些大错误，一路跌落到失败的位置。在150步之前，它突然设法扭转了局势，并最终顺利获胜。在[第10章](kindle_split_022.xhtml#ch10)中提到的策略梯度技术下，你会在游戏中对每一步给予相同的权重。在演员-评论家学习（actor-critic
    learning）中，你希望找到最重要的步骤并给予它们更大的权重。收益计算就展示了这一点。'
- en: Figure 12.1\. Estimated values over the course of a hypothetical game. This
    game lasted 200 moves. In the beginning, the learning agent pulled slightly ahead;
    then it fell far behind; then it suddenly reversed the game and came out with
    a win.
  id: totrans-1893
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1\. 在一个假设游戏过程中的估计值。这个游戏持续了200步。一开始，学习智能体略微领先；然后它落后很远；然后它突然逆转游戏并取得胜利。
- en: '![](Images/12fig01.jpg)'
  id: totrans-1894
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/12fig01.jpg)'
- en: Because the learning agent won, the advantage is given by *A*(s) = 1 – *V*(s).
    In [figure 12.2](#ch12fig02), you can see that the advantage curve has the same
    shape as the estimated value curve, but flipped upside down. The largest advantage
    comes while the agent was far behind. Because most players would lose in such
    a bad situation, the agent must have made a great move somewhere.
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
  zh: 因为学习智能体获胜，所以优势由 *A*(s) = 1 – *V*(s) 给出。在[图12.2](#ch12fig02)中，你可以看到优势曲线与估计值曲线形状相同，但上下颠倒。最大的优势出现在智能体落后很远的时候。因为大多数玩家在这种糟糕的情况下会输，智能体一定在某个地方做出了伟大的决策。
- en: Figure 12.2\. The advantages for each move in a hypothetical game. The learning
    agent won the game, so its final reward was 1\. The moves that led to the comeback
    have an advantage close to 2, so they’ll be strongly reinforced during training.
    The moves near the end of the game, when the outcome was already decided, have
    an advantage close to 0, so they’ll be nearly ignored during training.
  id: totrans-1896
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2\. 假设游戏中每一步的优势。学习智能体赢得了游戏，所以它的最终奖励是1。导致逆转的步骤具有接近2的优势，因此在训练中会得到强烈的强化。游戏接近结束时，结果已经确定，这些步骤的优势接近于0，因此在训练中几乎会被忽略。
- en: '![](Images/12fig02_alt.jpg)'
  id: totrans-1897
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/12fig02_alt.jpg)'
- en: 'After the agent had already pulled back ahead, around move 160 or so, its decisions
    are no longer interesting: the game had already wrapped up. The advantage in that
    section is close to 0.'
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体已经领先并拉开距离，大约在第160步左右之后，其决策就不再有趣了：游戏已经结束。那一部分的领先优势接近于0。
- en: Later in this chapter, we show how to adjust the training process based on the
    advantages. Before that, you need to calculate and store advantage through your
    self-play process.
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将展示如何根据优势调整训练过程。在那之前，你需要通过自我对弈过程计算并存储优势。
- en: 12.1.2\. Calculating advantage during self-play
  id: totrans-1900
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.2\. 在自我对弈中计算优势
- en: 'To calculate advantage, you’ll update your `ExperienceCollector` that you defined
    in [chapter 9](kindle_split_021.xhtml#ch09). Originally, an experience buffer
    tracked three parallel arrays: states, actions, and rewards. You can add a fourth
    parallel array to track advantages. To fill this array, you need both the estimated
    value for each state and the final game outcome. You won’t have the latter until
    the end; so in the middle of the episode, you can accumulate estimated values,
    and when the game is complete, you can translate those into advantages.'
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算优势，你需要更新你在[第9章](kindle_split_021.xhtml#ch09)中定义的 `ExperienceCollector`。最初，经验缓冲区跟踪三个并行数组：状态、动作和奖励。你可以添加一个第四个并行数组来跟踪优势。要填充这个数组，你需要每个状态的估计值和最终的游戏结果。你不会在游戏结束前得到后者；所以在游戏中间，你可以累积估计值，当游戏结束时，你可以将这些转换成优势。
- en: Listing 12.1\. Updating `ExperienceCollector` to track advantages
  id: totrans-1902
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.1\. 更新 `ExperienceCollector` 以跟踪优势
- en: '[PRE181]'
  id: totrans-1903
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '***1* These can span many episodes.**'
  id: totrans-1904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 这些可以跨越多个游戏片段。**'
- en: '***2* These are reset at the end of every episode.**'
  id: totrans-1905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 这些在每一个游戏片段结束时都会重置。**'
- en: Similarly, you need to update the `record_decision` method to accept an estimated
    value along with a state and an action.
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你需要更新 `record_decision` 方法以接受一个估计值以及状态和动作。
- en: Listing 12.2\. Updating `ExperienceCollector` to store estimated values
  id: totrans-1907
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.2\. 更新 `ExperienceCollector` 以存储估计值
- en: '[PRE182]'
  id: totrans-1908
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: Then, in the `complete_episode` method, you can calculate the advantage of each
    decision the agent made.
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 `complete_episode` 方法中，你可以计算智能体做出的每个决策的优势。
- en: Listing 12.3\. Calculating advantage at the end of an episode
  id: totrans-1910
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.3\. 在一个游戏片段结束时计算优势
- en: '[PRE183]'
  id: totrans-1911
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '***1* Calculates the advantage of each decision**'
  id: totrans-1912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 计算每个决策的优势**'
- en: '***2* Reset the per-episode buffers.**'
  id: totrans-1913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 重置每个游戏片段的缓冲区。**'
- en: You also need to update the `ExperienceBuffer` class and `combine_experience`
    helper to handle the advantages.
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要更新 `ExperienceBuffer` 类和 `combine_experience` 辅助函数以处理优势。
- en: Listing 12.4\. Adding advantage to the `ExperienceBuffer` structure
  id: totrans-1915
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.4\. 将优势添加到 `ExperienceBuffer` 结构中
- en: '[PRE184]'
  id: totrans-1916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Your experience classes are now ready to track advantage. You can still use
    these classes with techniques that don’t rely on advantage; just ignore the contents
    of the `advantages` buffer while training.
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: 你的经验类别现在可以跟踪优势。你仍然可以使用这些类别与不依赖于优势的技术；只是在训练时忽略 `advantages` 缓冲区的内容。
- en: 12.2\. Designing a neural network for actor-critic learning
  id: totrans-1918
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2\. 设计用于演员-评论家学习的神经网络
- en: '[Chapter 11](kindle_split_023.xhtml#ch11) showed how to define a neural network
    with two inputs in Keras. The Q-learning network had one input for the board and
    one input for the proposed move. For actor-critic learning, you want a network
    with one input and two outputs. The input is a representation of the board state.
    One output is a probability distribution over moves—the actor. The other output
    represents the expected return from the current position—the critic.'
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 11 章](kindle_split_023.xhtml#ch11) 介绍了如何在 Keras 中定义具有两个输入的神经网络。Q-learning
    网络有一个用于棋盘的输入和一个用于提议移动的输入。对于演员-评论家学习，你希望有一个具有一个输入和两个输出的网络。输入是棋盘状态的表示。一个输出是移动的概率分布——演员。另一个输出表示从当前位置的预期回报——评论家。'
- en: 'Building a network with two outputs brings a surprising bonus: each output
    serves as a sort of regularizer on the other. (Recall from [chapter 6](kindle_split_018.xhtml#ch06)
    that *regularization* is any technique to prevent your model from *overfitting*
    to the exact data set it was trained on.) Imagine that a group of stones on the
    board is in danger of getting captured. This fact is relevant for the value output,
    because the player with the weak stones is probably behind. It’s also relevant
    to the action output, because you probably want to either attack or defend the
    weak stones. If your network learns a “weak stone” detector in the early layers,
    that’s relevant to both outputs. Training on both outputs forces the network to
    learn a representation that’s useful for both goals. This can often improve generalization
    and sometimes even speed up training.'
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: 构建具有两个输出的网络带来一个意想不到的额外好处：每个输出都充当另一种意义上的正则化。 (回想一下 [第 6 章](kindle_split_018.xhtml#ch06)
    中的 *正则化* 是任何防止你的模型对训练数据集过度拟合的技术。) 假设棋盘上的一组石头有被捕获的危险。这个事实对值输出是相关的，因为拥有弱石头的玩家可能落后。这也与动作输出相关，因为你可能想要攻击或防御弱石头。如果你的网络在早期层学习到“弱石头”检测器，这对两个输出都是相关的。在两个输出上训练迫使网络学习一个对两个目标都有用的表示。这通常可以改善泛化，有时甚至可以加快训练速度。
- en: '[Chapter 11](kindle_split_023.xhtml#ch11) introduced the Keras functional API,
    which gives you full freedom to connect layers in your network however you like.
    You’ll use it again here to build the network described in [figure 12.3](#ch12fig03);
    this code goes in the `init_ac_agent.py` script.'
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 11 章](kindle_split_023.xhtml#ch11) 介绍了 Keras 功能 API，它让你有完全的自由来以你喜欢的任何方式连接网络中的层。你将再次使用它来构建
    [图 12.3](#ch12fig03) 中描述的网络；这段代码放在 `init_ac_agent.py` 脚本中。'
- en: Listing 12.5\. A two-output network with a policy output and a value output
  id: totrans-1922
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.5\. 具有策略输出和值输出的双输出网络
- en: '[PRE185]'
  id: totrans-1923
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '***1* Add as many convolutional layers as you like.**'
  id: totrans-1924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 添加尽可能多的卷积层。**'
- en: '***2* This example uses hidden layers of size 512\. Experiment to find the
    best size. The three hidden layers don’t need to be the same size.**'
  id: totrans-1925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 此示例使用大小为 512 的隐藏层。实验以找到最佳大小。三个隐藏层不需要相同的大小。**'
- en: '***3* This output yields the policy function.**'
  id: totrans-1926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 此输出产生策略函数。**'
- en: '***4* This output yields the value function.**'
  id: totrans-1927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 此输出产生值函数。**'
- en: Figure 12.3\. A neural network suitable for actor-critic learning for Go. This
    network has a single input, which takes a representation of the current board
    position. The network produces two outputs. One output indicates which moves it
    should play—this is the policy output, or the actor. The other output indicates
    which player is ahead in the game—this is the value output, or the critic. The
    critic isn’t used in playing a game but helps the training process.
  id: totrans-1928
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.3\. 适用于围棋的演员-评论家学习的神经网络。此网络有一个单一输入，它接受当前棋盘位置的表示。网络产生两个输出。一个输出指示它应该玩哪些移动——这是策略输出或演员。另一个输出指示游戏中哪位玩家领先——这是值输出或评论家。评论家在玩游戏时没有被使用，但有助于训练过程。
- en: '![](Images/12fig03_alt.jpg)'
  id: totrans-1929
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/12fig03_alt.jpg)'
- en: This network has three convolutional layers with 64 filters each. That’s on
    the smaller side for a Go-playing network, but it has the advantage of faster
    training. As always, we encourage you to experiment with different network structures
    here.
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络有三个卷积层，每个层有64个过滤器。对于围棋网络来说，这属于较小的规模，但它有训练速度更快的优势。一如既往，我们鼓励你在这里尝试不同的网络结构。
- en: The policy output represents a probability distribution over possible moves.
    The dimension is equal to the number of points on the board, and you use the softmax
    activation to ensure that the policy sums to 1.
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: 策略输出表示了可能动作的概率分布。维度等于棋盘上的点数，你使用softmax激活函数来确保策略的总和为1。
- en: The value output is a single number in the range of –1 to 1\. This output has
    dimension 1, and you use a tanh activation to clamp the value.
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
  zh: 值输出是一个介于-1到1之间的单个数字。这个输出的维度是1，你使用tanh激活函数来限制这个值。
- en: 12.3\. Playing games with an actor-critic agent
  id: totrans-1933
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3\. 与演员-评论家智能体玩游戏
- en: Selecting moves is almost exactly the same as in the policy agent from [chapter
    10](kindle_split_022.xhtml#ch10). You make two changes. First, because the model
    now produces two outputs, you need a little extra code to unpack the results.
    Second, you need to pass the estimated value to the experience collector, along
    with the state and action. The process of picking a move from the probability
    distribution is identical. The following listing shows the updated `select_move`
    implementation. We’ve called out places where it differs from implementation of
    [chapter 10](kindle_split_022.xhtml#ch10)’s policy agent.
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
  zh: 选择动作几乎与第10章中的策略智能体完全相同。你做了两个修改。首先，因为模型现在产生两个输出，你需要一点额外的代码来解包结果。其次，你需要将估计的值传递给经验收集器，同时传递状态和动作。从概率分布中挑选动作的过程是相同的。下面的列表显示了更新的`select_move`实现。我们指出了它与第10章策略智能体实现的不同之处。
- en: Listing 12.6\. Selecting a move for an actor-critic agent
  id: totrans-1935
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.6\. 为演员-评论家智能体选择动作
- en: '[PRE186]'
  id: totrans-1936
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '***1* Because this is a two-output model, predict returns a tuple containing
    two NumPy arrays.**'
  id: totrans-1937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 因为这是一个双输出模型，预测返回一个包含两个NumPy数组的元组。**'
- en: '***2* predict is a batch call that can process several boards at once, so you
    must select the first element of the array to get the probability distribution
    you want.**'
  id: totrans-1938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* predict是一个批处理调用，可以同时处理多个棋盘，所以你必须选择数组中的第一个元素来获取你想要的概率分布。**'
- en: '***3* The values are represented as a one-dimensional vector, so you must pull
    out the first element to get the value as a plain float.**'
  id: totrans-1939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 值被表示为一个一维向量，因此你必须取出第一个元素来获取作为普通浮点数的值。**'
- en: '***4* Include the estimated value in the experience buffer.**'
  id: totrans-1940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将估计的值包含在经验缓冲区中。**'
- en: 12.4\. Training an actor-critic agent from experience data
  id: totrans-1941
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4\. 从经验数据训练演员-评论家智能体
- en: Training your actor-critic network looks like a combination of training the
    policy network in [chapter 10](kindle_split_022.xhtml#ch10) and the action-value
    network in [chapter 11](kindle_split_023.xhtml#ch11). To train a two-output network,
    you construct a separate training target for each output, and choose a separate
    loss function for each output. This section describes how to convert the experience
    data to training targets, and how to use the Keras `fit` function with multiple
    outputs.
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: 训练你的演员-评论家网络看起来像是第10章中的策略网络和第11章中的动作-价值网络的训练组合。为了训练一个双输出网络，你为每个输出构建一个单独的训练目标，并为每个输出选择一个单独的损失函数。本节描述了如何将经验数据转换为训练目标，以及如何使用Keras的`fit`函数处理多个输出。
- en: Recall how you encoded training data for policy gradient learning. For any game
    position, the training target was a vector the same size as the board, with a
    1 or –1 in the slot corresponding to the chosen move; the 1 indicated a win, and
    the –1 indicated a loss. In your actor-critic learning, you use the same encoding
    scheme for the training data, but you replace the 1 or –1 with the advantage of
    the move. The advantage will have the same sign as the final reward, so the probability
    of the game decision will move in the same direction as in simple policy learning.
    But it’ll move further for actions that were deemed important, and move just a
    little for actions with an advantage that’s close to zero.
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下你是如何为策略梯度学习编码训练数据的。对于任何游戏位置，训练目标是与棋盘大小相同的向量，在对应所选移动的槽位中有一个1或-1；1表示胜利，-1表示失败。在你的演员-评论家学习中，你使用相同的编码方案来处理训练数据，但将1或-1替换为移动的优势。优势将与最终奖励的符号相同，因此游戏决策的概率将与简单策略学习中的方向相同。但对于被认为重要的动作，它会移动得更远，而对于优势接近零的动作，它只会移动一小点。
- en: For the value output, the training target is the total reward. This looks exactly
    like the training target for Q-learning. [Figure 12.4](#ch12fig04) illustrates
    the training setup.
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
  zh: 对于价值输出，训练目标是总奖励。这看起来与Q学习的训练目标完全相同。[图12.4](#ch12fig04)说明了训练设置。
- en: 'Figure 12.4\. Training setup for actor-critic learning. The neural network
    has two outputs: one for the policy and one for the value. Each gets its own training
    target. The policy output is trained against a vector the same size as the board.
    The cell in the vector corresponding to the chosen move is filled in with the
    advantage calculated for that move; the rest are zero. The value output is trained
    against the final outcome of the game.'
  id: totrans-1945
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.4\. 演员评论家学习的训练设置。神经网络有两个输出：一个用于策略，一个用于价值。每个都有自己的训练目标。策略输出针对与棋盘大小相同的向量进行训练。向量中对应所选移动的单元格填充了为该移动计算的优势；其余为0。价值输出针对游戏的最终结果进行训练。
- en: '![](Images/12fig04_alt.jpg)'
  id: totrans-1946
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/12fig04_alt.jpg)'
- en: When you have multiple outputs in a network, you can pick a different loss function
    for each output. You’ll use categorical cross-entropy for the policy output, and
    mean squared error for the value output. (Refer to [chapters 10](kindle_split_022.xhtml#ch10)
    and [11](kindle_split_023.xhtml#ch11) for an explanation of why those loss functions
    make sense for those purposes.)
  id: totrans-1947
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在网络中有多个输出时，你可以为每个输出选择不同的损失函数。你将使用交叉熵来处理策略输出，使用均方误差来处理价值输出。（参考[第10章](kindle_split_022.xhtml#ch10)和[第11章](kindle_split_023.xhtml#ch11)了解为什么这些损失函数适用于这些目的。）
- en: One new Keras feature you’ll use is *loss weights*. By default, Keras will sum
    the loss function for each output to get the overall loss function. If you specify
    loss weights, Keras will scale each individual loss function before summing. This
    allows you to adjust the relative importance of each output. In our experiments,
    we found the value loss was large compared to the policy loss, so we scaled down
    the value loss by half. Depending on your exact network and training data, you
    may need to adjust the loss weights somewhat.
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用的一个新的Keras特性是*损失权重*。默认情况下，Keras将每个输出的损失函数相加以获得整体损失函数。如果你指定了损失权重，Keras将在求和之前对每个单独的损失函数进行缩放。这允许你调整每个输出的相对重要性。在我们的实验中，我们发现价值损失相对于策略损失较大，所以我们把价值损失缩小了一半。根据你确切的网络和训练数据，你可能需要适当地调整损失权重。
- en: '|  |'
  id: totrans-1949
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1950
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Keras will print out the computed loss values every time you call `fit`. For
    a two-output network, it’ll print out the two losses separately. You can check
    there to see whether the magnitudes are comparable. If one loss is far larger
    than the other, consider adjusting the weights. Don’t worry about getting too
    precise.
  id: totrans-1951
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你调用`fit`时，Keras都会打印出计算出的损失值。对于有两个输出的网络，它会分别打印出两个损失。你可以检查那里，看看幅度是否相当。如果一个损失远大于另一个，考虑调整权重。不用担心过于精确。
- en: '|  |'
  id: totrans-1952
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The following listing shows how to encode the experience data as training data,
    and then call `fit` on the training targets. The structure is similar to the `train`
    implementations from [chapters 10](kindle_split_022.xhtml#ch10) and [11](kindle_split_023.xhtml#ch11).
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了如何将经验数据编码为训练数据，然后对训练目标调用`fit`。结构与[第10章](kindle_split_022.xhtml#ch10)和[第11章](kindle_split_023.xhtml#ch11)中的`train`实现类似。
- en: Listing 12.7\. Selecting a move for an actor-critic agent
  id: totrans-1954
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.7\. 为演员-评论家代理选择移动
- en: '[PRE187]'
  id: totrans-1955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '***1* lr (learning rate) and batch_size are tuning parameters for the optimizer;
    refer to [chapter 10](kindle_split_022.xhtml#ch10) for more discussion.**'
  id: totrans-1956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* lr（学习率）和batch_size是优化器的调整参数；有关更多讨论，请参阅[第10章](kindle_split_022.xhtml#ch10)。'
- en: '***2* categorical_crossentropy is for the policy output, just as in [chapter
    10](kindle_split_022.xhtml#ch10). mse (mean squared error) is for the value output,
    just as in [chapter 11](kindle_split_023.xhtml#ch11). The order here matches the
    order in the Model constructor in [listing 12.5](#ch12ex05).**'
  id: totrans-1957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* categorical_crossentropy用于策略输出，就像在第10章中一样。[第10章](kindle_split_022.xhtml#ch10)。mse（均方误差）用于价值输出，就像在第11章中一样。这里的顺序与[清单12.5](#ch12ex05)中模型构造函数中的顺序相同。'
- en: '***3* The weight 1.0 applies to the policy output; the weight 0.5 applies to
    the value output.**'
  id: totrans-1958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 权重1.0应用于策略输出；权重0.5应用于价值输出。'
- en: '***4* This is the same as the encoding scheme used in [chapter 10](kindle_split_022.xhtml#ch10),
    but weighted by the advantage.**'
  id: totrans-1959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 这与第10章中使用的编码方案相同，但按优势加权。'
- en: '***5* This is the same as the encoding scheme used in [chapter 11](kindle_split_023.xhtml#ch11).**'
  id: totrans-1960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 这与第11章中使用的编码方案相同。[第11章](kindle_split_023.xhtml#ch11)。'
- en: 'Now that you have all the pieces, let’s try actor-critic learning end-to-end.
    You’ll start with a 9 × 9 bot so you can see results faster. The cycle will go
    like this:'
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所有部件，让我们尝试端到端演员-评论家学习。你将从一个9×9的机器人开始，这样你可以更快地看到结果。循环将是这样进行的：
- en: Generate self-play games in chunks of 5,000.
  id: totrans-1962
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以5,000个为单位生成自玩游戏。
- en: After each chunk, train the agent and compare it to the previous version of
    your bot.
  id: totrans-1963
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个块之后，训练代理并与你机器人的上一个版本进行比较。
- en: If the new bot can beat the previous bot 60 out of 100 games, you’ve successfully
    improved your agent! Start the process over with the new bot.
  id: totrans-1964
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果新机器人能在100场比赛中击败前机器人60场，那么你已经成功改进了你的代理！用新机器人重新开始这个过程。
- en: If the updated bot wins fewer than 60 out of 100 games, generate another chunk
    of self-play games and retrain. Continue training until the new bot is strong
    enough.
  id: totrans-1965
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果更新后的机器人赢得的100场比赛中少于60场，则生成另一个自玩游戏块并重新训练。继续训练，直到新机器人足够强大。
- en: The benchmark of 60 wins out of 100 is somewhat arbitrary; it’s a nice round
    number that gives you reasonable confidence that your bot is truly stronger, and
    not just lucky.
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
  zh: 60胜100场的基准是有些任意的；这是一个很好的圆数，让你有合理的信心认为你的机器人确实更强，而不仅仅是运气好。
- en: 'Start by initializing a bot with the init_ac_agent script (as shown in [listing
    12.5](#ch12ex05)):'
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用init_ac_agent脚本初始化一个机器人（如[清单12.5](#ch12ex05)所示）：
- en: '[PRE188]'
  id: totrans-1968
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'After this, you should have a new file, ac_v1.hdf5, that contains the weights
    for your new bot. At this point, both the bot’s play and its value estimates are
    essentially random. You can now start generating self-play games:'
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你应该有一个包含新机器人权重的ac_v1.hdf5新文件。此时，机器人的游戏和其价值估计基本上是随机的。你现在可以开始生成自玩游戏：
- en: '[PRE189]'
  id: totrans-1970
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: 'If you’re not fortunate enough to have access to a fast GPU, this is a good
    time to go out for a coffee or take the dog for a walk. When the self_play script
    is done, the output will look something like this:'
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有足够幸运能够访问到快速的GPU，这是一个出去喝咖啡或遛狗的好时机。当self_play脚本完成后，输出将看起来像这样：
- en: '[PRE190]'
  id: totrans-1972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: 'After this, you should have an exp_0001.hdf5 file containing a big chunk of
    game records. The next step is to train:'
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你应该有一个包含大量游戏记录的exp_0001.hdf5文件。下一步是训练：
- en: '[PRE191]'
  id: totrans-1974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'This will take the neural network currently stored in ac_v1.hdf1, run a single
    epoch of training against the data in exp_0001.hdf, and save the updated agent
    to ac_v2.hdf5\. The optimizer will use a learning rate of 0.01 and a batch size
    of 1,024\. You should see output something like this:'
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用当前存储在ac_v1.hdf1中的神经网络，对exp_0001.hdf中的数据进行单次训练周期，并将更新的代理保存到ac_v2.hdf5。优化器将使用学习率为0.01，批大小为1,024。你应该看到类似这样的输出：
- en: '[PRE192]'
  id: totrans-1976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: 'Notice that the loss is now broken into two values: `dense_3_loss` and `dense_5_loss`,
    corresponding to the policy output and the value output, respectively.'
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在损失被分解为两个值：`dense_3_loss`和`dense_5_loss`，分别对应策略输出和价值输出。
- en: 'After this, you can compare the updated bot against its predecessor with the
    eval_ac_bot.py script:'
  id: totrans-1978
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你可以使用eval_ac_bot.py脚本来比较更新后的机器人与其前辈：
- en: '[PRE193]'
  id: totrans-1979
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: 'The output should look something like this:'
  id: totrans-1980
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该看起来像这样：
- en: '[PRE194]'
  id: totrans-1981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: 'In this case, the output shows you exactly hit the threshold of 60 wins out
    of 100: you can have reasonable confidence that your bot has learned something
    useful. (This is just example output, of course; your actual results will look
    a little different, and that’s fine.) Because the ac_v2 bot is measurably stronger
    than ac_v1, you can switch to generating games with ac_v2:'
  id: totrans-1982
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输出显示你正好达到了60次胜利的阈值：你可以有理由相信你的机器人已经学到了一些有用的东西。（这只是一个示例输出，当然；你的实际结果可能会有所不同，这是正常的。）因为ac_v2机器人比ac_v1机器人明显更强，你可以切换到生成ac_v2的游戏：
- en: '[PRE195]'
  id: totrans-1983
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: 'When that’s done, you can train and evaluate again:'
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你可以再次进行训练和评估：
- en: '[PRE196]'
  id: totrans-1985
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: 'This case wasn’t quite as successful as the last time:'
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: 这次的情况并不像上次那么成功：
- en: '[PRE197]'
  id: totrans-1987
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: 'The ac_v3 bot beat the ac_v2 bot only 51 times out of 100\. With those results,
    it’s hard to say whether ac_v3 is a tiny bit stronger or not; the safest conclusion
    is that it’s basically the same strength as ac_v2\. But don’t despair. You can
    generate more training data and try again:'
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: 在100次对决中，ac_v3机器人只打败了ac_v2机器人51次。根据这些结果，很难说ac_v3是否稍微强一些；最安全的结论是，它的实力基本上与ac_v2相当。但不要灰心。你可以生成更多的训练数据并再次尝试：
- en: '[PRE198]'
  id: totrans-1989
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'The train_ac script will accept multiple training data files on the command
    line:'
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
  zh: train_ac脚本将在命令行接受多个训练数据文件：
- en: '[PRE199]'
  id: totrans-1991
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: 'After each additional batch of games, you can compare against ac_v2 again.
    In our experiments, we needed three batches of 5,000 games—a total of 15,000 games—before
    we got a satisfactory result:'
  id: totrans-1992
  prefs: []
  type: TYPE_NORMAL
  zh: 在每批额外的游戏之后，你可以再次与ac_v2进行比较。在我们的实验中，我们需要三批5,000场游戏——总共15,000场游戏——才能得到令人满意的结果：
- en: '[PRE200]'
  id: totrans-1993
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: Success! With 62 wins against ac_v2, you can feel confident that ac_v3 is stronger
    than ac_v2\. Now you can switch over to generating self-play games with ac_v3,
    and repeat the cycle again.
  id: totrans-1994
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！在对ac_v2取得62次胜利后，你可以有信心认为ac_v3比ac_v2更强。现在你可以切换到使用ac_v3生成自我对弈游戏，并再次重复这个循环。
- en: It’s unclear exactly how strong a Go bot can get with this actor-critic implementation
    alone. We’ve shown that you can train a bot to learn basic tactics, but its strength
    is bound to top out at some point. By deeply integrating reinforcement learning
    with a kind of tree search, you *can* train a bot that’s stronger than any human
    player; [chapter 14](kindle_split_027.xhtml#ch14) covers that technique.
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: 单独使用这个演员-评论家实现，很难说围棋机器人能达到多强的水平。我们已经展示了你可以训练一个机器人学习基本策略，但它的实力最终会达到一个顶点。通过将强化学习与一种树搜索深度融合，你可以训练出一个比任何人类玩家都强的机器人；[第14章](kindle_split_027.xhtml#ch14)介绍了这项技术。
- en: 12.5\. Summary
  id: totrans-1996
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5. 摘要
- en: '*Actor-critic learning* is a reinforcement-learning technique in which you
    simultaneously learn a policy function and a value function. The policy function
    tells you how to make decisions, and the value function helps improve the training
    process for the value function. You can apply actor-critic learning to the same
    kinds of problems where you’d apply policy gradient learning, but actor-critic
    learning is often more stable.'
  id: totrans-1997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*演员-评论家学习*是一种强化学习技术，其中你同时学习策略函数和值函数。策略函数告诉你如何做出决策，而值函数有助于改进值函数的训练过程。你可以将演员-评论家学习应用于与策略梯度学习相同类型的问题，但演员-评论家学习通常更稳定。'
- en: '*Advantage* is the difference between the actual reward an agent sees and the
    expected reward at some point in the episode. For games, this is the difference
    between the actual game result (win or loss) and the expected value (as estimated
    by the agent’s value model).'
  id: totrans-1998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优势*是代理看到的实际奖励与在某个时刻的期望奖励之间的差异。对于游戏来说，这是实际游戏结果（胜利或失败）与期望值（由代理的价值模型估计）之间的差异。'
- en: Advantage helps identify the important decisions in a game. If a learning agent
    wins a game, the advantage will be largest for moves it made from an even or losing
    position. The advantage will be close to zero for moves it made after the game
    was already decided.
  id: totrans-1999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优势有助于识别游戏中的重要决策。如果一个学习代理赢得了游戏，那么它在从均势或劣势位置做出的移动的优势将最大。它在游戏已经决定后的移动的优势将接近零。
- en: A Keras sequential network can have multiple outputs. In actor-critic learning,
    this lets you create a single network to model both the policy function and the
    value function.
  id: totrans-2000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras顺序网络可以有多个输出。在演员-评论家学习中，这让你可以创建一个单一的网络来模拟策略函数和值函数。
