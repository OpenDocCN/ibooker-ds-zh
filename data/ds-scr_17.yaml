- en: Chapter 16\. Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章 logistic回归
- en: A lot of people say there’s a fine line between genius and insanity. I don’t
    think there’s a fine line, I actually think there’s a yawning gulf.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 很多人说天才和疯狂之间只有一条细微的界限。我认为这不是细微的界限，实际上是一个巨大的鸿沟。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bill Bailey
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Bill Bailey
- en: In [Chapter 1](ch01.html#introduction), we briefly looked at the problem of
    trying to predict which DataSciencester users paid for premium accounts. Here
    we’ll revisit that problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html#introduction)中，我们简要讨论了试图预测哪些DataSciencester用户购买高级帐户的问题。在这里，我们将重新讨论这个问题。
- en: The Problem
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: We have an anonymized dataset of about 200 users, containing each user’s salary,
    her years of experience as a data scientist, and whether she paid for a premium
    account ([Figure 16-1](#logit_image)). As is typical with categorical variables,
    we represent the dependent variable as either 0 (no premium account) or 1 (premium
    account).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个约200个用户的匿名数据集，包含每个用户的工资、作为数据科学家的经验年数，以及她是否为高级帐户支付费用（[图 16-1](#logit_image)）。像典型的分类变量一样，我们将依赖变量表示为0（没有高级帐户）或1（高级帐户）。
- en: 'As usual, our data is a list of rows `[experience, salary, paid_account]`.
    Let’s turn it into the format we need:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们的数据是一列行 `[experience, salary, paid_account]`。让我们把它转换成我们需要的格式：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'An obvious first attempt is to use linear regression and find the best model:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的第一次尝试是使用线性回归找到最佳模型：
- en: <math alttext="paid account equals beta 0 plus beta 1 experience plus beta 2
    salary plus epsilon" display="block"><mrow><mtext>paid</mtext> <mtext>account</mtext>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <mtext>experience</mtext> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mtext>salary</mtext>
    <mo>+</mo> <mi>ε</mi></mrow></math>![Paid and Unpaid Users.](assets/dsf2_1601.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="paid account equals beta 0 plus beta 1 experience plus beta 2
    salary plus epsilon" display="block"><mrow><mtext>paid</mtext> <mtext>account</mtext>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <mtext>experience</mtext> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mtext>salary</mtext>
    <mo>+</mo> <mi>ε</mi></mrow></math>![付费和非付费用户。](assets/dsf2_1601.png)
- en: Figure 16-1\. Paid and unpaid users
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-1 付费和非付费用户
- en: 'And certainly, there’s nothing preventing us from modeling the problem this
    way. The results are shown in [Figure 16-2](#linear_regression_for_probabilities):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们完全可以用这种方式对问题建模。结果显示在[图 16-2](#linear_regression_for_probabilities)中：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Using Linear Regression to Predict Premium Accounts.](assets/dsf2_1602.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![使用线性回归预测高级账户。](assets/dsf2_1602.png)'
- en: Figure 16-2\. Using linear regression to predict premium accounts
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-2 使用线性回归预测高级账户
- en: 'But this approach leads to a couple of immediate problems:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这种方法会导致一些即时问题：
- en: We’d like for our predicted outputs to be 0 or 1, to indicate class membership.
    It’s fine if they’re between 0 and 1, since we can interpret these as probabilities—an
    output of 0.25 could mean 25% chance of being a paid member. But the outputs of
    the linear model can be huge positive numbers or even negative numbers, which
    it’s not clear how to interpret. Indeed, here a lot of our predictions were negative.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望我们预测的输出是0或1，以表示类别成员资格。如果它们在0和1之间，我们可以将其解释为概率，例如0.25的输出可能表示成为付费会员的25%的机会。但是线性模型的输出可以是非常大的正数甚至是负数，这样不清楚如何解释。实际上，这里很多我们的预测是负数。
- en: The linear regression model assumed that the errors were uncorrelated with the
    columns of *x*. But here, the regression coefficient for `experience` is 0.43,
    indicating that more experience leads to a greater likelihood of a premium account.
    This means that our model outputs very large values for people with lots of experience.
    But we know that the actual values must be at most 1, which means that necessarily
    very large outputs (and therefore very large values of `experience`) correspond
    to very large negative values of the error term. Because this is the case, our
    estimate of `beta` is biased.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归模型假设误差与*x*的列无关。但是在这里，`experience`的回归系数为0.43，表明经验越多，付费会员的可能性越大。这意味着我们的模型对于经验丰富的人输出非常大的值。但是我们知道实际值最多只能是1，这意味着非常大的输出（因此非常大的`experience`值）对应于误差项非常大的负值。由于这种情况，我们对`beta`的估计是有偏的。
- en: What we’d like instead is for large positive values of `dot(x_i, beta)` to correspond
    to probabilities close to 1, and for large negative values to correspond to probabilities
    close to 0\. We can accomplish this by applying another function to the result.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望`dot(x_i, beta)`的大正值对应接近1的概率，大负值对应接近0的概率。我们可以通过对结果应用另一个函数来实现这一点。
- en: The Logistic Function
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑函数
- en: 'In the case of logistic regression, we use the *logistic function*, pictured
    in [Figure 16-3](#graph_of_logistic_function):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，我们使用*逻辑函数*，如[图 16-3](#graph_of_logistic_function)所示：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Logistic function.](assets/dsf2_1603.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑函数。](assets/dsf2_1603.png)'
- en: Figure 16-3\. The logistic function
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-3. 逻辑函数
- en: 'As its input gets large and positive, it gets closer and closer to 1\. As its
    input gets large and negative, it gets closer and closer to 0\. Additionally,
    it has the convenient property that its derivative is given by:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当其输入变得很大且为正时，它逐渐接近1。当其输入变得很大且为负时，它逐渐接近0。此外，它具有一个方便的性质，即其导数为：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'which we’ll make use of in a bit. We’ll use this to fit a model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将利用这一点。我们将使用这个来拟合一个模型：
- en: <math alttext="y Subscript i Baseline equals f left-parenthesis x Subscript
    i Baseline beta right-parenthesis plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mi>β</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript i Baseline equals f left-parenthesis x Subscript
    i Baseline beta right-parenthesis plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mi>β</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
- en: where *f* is the `logistic` function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*f*是`logistic`函数。
- en: Recall that for linear regression we fit the model by minimizing the sum of
    squared errors, which ended up choosing the *β* that maximized the likelihood
    of the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，对于线性回归，我们通过最小化平方误差来拟合模型，这最终选择了最大化数据的似然的*β*。
- en: Here the two aren’t equivalent, so we’ll use gradient descent to maximize the
    likelihood directly. This means we need to calculate the likelihood function and
    its gradient.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里两者并不等价，因此我们将使用梯度下降直接最大化似然。这意味着我们需要计算似然函数及其梯度。
- en: Given some *β*, our model says that each <math><msub><mi>y</mi> <mi>i</mi></msub></math>
    should equal 1 with probability <math><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow></math> and 0 with probability <math><mrow><mn>1</mn>
    <mo>-</mo> <mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi>
    <mo>)</mo></mrow></math> .
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一些*β*，我们的模型表明每个<math><msub><mi>y</mi> <mi>i</mi></msub></math>应该等于1的概率为<math><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow></math>，等于0的概率为<math><mrow><mn>1</mn>
    <mo>-</mo> <mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi>
    <mo>)</mo></mrow></math>。
- en: 'In particular, the PDF for <math><msub><mi>y</mi> <mi>i</mi></msub></math>
    can be written as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，<math><msub><mi>y</mi> <mi>i</mi></msub></math>的概率密度函数可以写成：
- en: <math alttext="p left-parenthesis y Subscript i Baseline vertical-bar x Subscript
    i Baseline comma beta right-parenthesis equals f left-parenthesis x Subscript
    i Baseline beta right-parenthesis Superscript y Super Subscript i Baseline left-parenthesis
    1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis
    Superscript 1 minus y Super Subscript i" display="block"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <mi>β</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>f</mi> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi><mo>)</mo></mrow> <msub><mi>y</mi> <mi>i</mi></msub></msup>
    <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mrow><mn>1</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup></mrow></math>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p left-parenthesis y Subscript i Baseline vertical-bar x Subscript
    i Baseline comma beta right-parenthesis equals f left-parenthesis x Subscript
    i Baseline beta right-parenthesis Superscript y Super Subscript i Baseline left-parenthesis
    1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis
    Superscript 1 minus y Super Subscript i" display="block"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <mi>β</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>f</mi> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi><mo>)</mo></mrow> <msub><mi>y</mi> <mi>i</mi></msub></msup>
    <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mrow><mn>1</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup></mrow></math>
- en: 'since if <math><msub><mi>y</mi> <mi>i</mi></msub></math> is 0, this equals:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果<math><msub><mi>y</mi> <mi>i</mi></msub></math>为0，则此等于：
- en: <math alttext="1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis"
    display="block"><mrow><mn>1</mn> <mo>-</mo> <mi>f</mi> <mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow></math>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis"
    display="block"><mrow><mn>1</mn> <mo>-</mo> <mi>f</mi> <mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow></math>
- en: 'and if <math><msub><mi>y</mi> <mi>i</mi></msub></math> is 1, it equals:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果<math><msub><mi>y</mi> <mi>i</mi></msub></math>为1，则它等于：
- en: <math alttext="f left-parenthesis x Subscript i Baseline beta right-parenthesis"
    display="block"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mi>β</mi> <mo>)</mo></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f left-parenthesis x Subscript i Baseline beta right-parenthesis"
    display="block"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mi>β</mi> <mo>)</mo></mrow></math>
- en: 'It turns out that it’s actually simpler to maximize the *log likelihood*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，最大化*对数似然*实际上更简单：
- en: <math alttext="log upper L left-parenthesis beta vertical-bar x Subscript i
    Baseline comma y Subscript i Baseline right-parenthesis equals y Subscript i Baseline
    log f left-parenthesis x Subscript i Baseline beta right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mi>L</mi> <mrow><mo>(</mo> <mi>β</mi>
    <mo>|</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo form="prefix">log</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="log upper L left-parenthesis beta vertical-bar x Subscript i
    Baseline comma y Subscript i Baseline right-parenthesis equals y Subscript i Baseline
    log f left-parenthesis x Subscript i Baseline beta right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mi>L</mi> <mrow><mo>(</mo> <mi>β</mi>
    <mo>|</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo form="prefix">log</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
- en: 'Because log is a strictly increasing function, any `beta` that maximizes the
    log likelihood also maximizes the likelihood, and vice versa. Because gradient
    descent minimizes things, we’ll actually work with the *negative* log likelihood,
    since maximizing the likelihood is the same as minimizing its negative:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因为对数是一个严格递增的函数，任何使对数似然最大化的`beta`也将最大化似然，反之亦然。因为梯度下降是最小化的，所以我们实际上会处理*负*对数似然，因为最大化似然等同于最小化其负值：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If we assume different data points are independent from one another, the overall
    likelihood is just the product of the individual likelihoods. That means the overall
    log likelihood is the sum of the individual log likelihoods:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设不同数据点彼此独立，则总体似然仅为各个似然的乘积。这意味着总体对数似然是各个对数似然的和：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A little bit of calculus gives us the gradient:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一点微积分给了我们梯度：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: at which point we have all the pieces we need.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点我们已经拥有所有需要的部分。
- en: Applying the Model
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用模型
- en: 'We’ll want to split our data into a training set and a test set:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要将数据分成训练集和测试集：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'after which we find that `beta` is approximately:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 之后我们发现`beta`大约是：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'These are coefficients for the `rescale`d data, but we can transform them back
    to the original data as well:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是`rescale`d数据的系数，但我们也可以将它们转换回原始数据：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Unfortunately, these are not as easy to interpret as linear regression coefficients.
    All else being equal, an extra year of experience adds 1.6 to the input of `logistic`.
    All else being equal, an extra $10,000 of salary subtracts 2.88 from the input
    of `logistic`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这些并不像线性回归系数那样容易解释。其他条件相同，一年额外的经验将在`logistic`的输入上增加1.6。其他条件相同，额外的10,000美元薪水将在`logistic`的输入上减少2.88。
- en: The impact on the output, however, depends on the other inputs as well. If `dot(beta,
    x_i)` is already large (corresponding to a probability close to 1), increasing
    it even by a lot cannot affect the probability very much. If it’s close to 0,
    increasing it just a little might increase the probability quite a bit.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，输出的影响也取决于其他输入。如果`dot(beta, x_i)`已经很大（对应概率接近1），即使大幅增加它也不能太大影响概率。如果接近0，稍微增加可能会大幅增加概率。
- en: What we can say is that—all else being equal—people with more experience are
    more likely to pay for accounts. And that—all else being equal—people with higher
    salaries are less likely to pay for accounts. (This was also somewhat apparent
    when we plotted the data.)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说的是，其他条件相同的情况下，经验丰富的人更有可能支付账户。而其他条件相同的情况下，收入较高的人支付账户的可能性较低。（这在我们绘制数据时也有些明显。）
- en: Goodness of Fit
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合度
- en: 'We haven’t yet used the test data that we held out. Let’s see what happens
    if we predict *paid account* whenever the probability exceeds 0.5:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有使用我们留出的测试数据。让我们看看如果我们在概率超过0.5时预测*付费账户*会发生什么：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This gives a precision of 75% (“when we predict *paid account* we’re right 75%
    of the time”) and a recall of 80% (“when a user has a paid account we predict
    *paid account* 80% of the time”), which is not terrible considering how little
    data we have.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了75%的精度（“当我们预测*付费账户*时，我们有75%的准确率”）和80%的召回率（“当用户有付费账户时，我们80%的时间预测为*付费账户*”），考虑到我们拥有的数据很少，这并不算糟糕。
- en: 'We can also plot the predictions versus the actuals ([Figure 16-4](#logistic_prediction_vs_actual)),
    which also shows that the model performs well:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制预测与实际情况对比图（见图16-4，#logistic_prediction_vs_actual），这也显示出模型表现良好：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Logistic Regression Predicted vs Actual.](assets/dsf2_1604.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归预测 vs 实际。](assets/dsf2_1604.png)'
- en: Figure 16-4\. Logistic regression predicted versus actual
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-4。逻辑回归预测与实际
- en: Support Vector Machines
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: The set of points where `dot(beta, x_i)` equals 0 is the boundary between our
    classes. We can plot this to see exactly what our model is doing ([Figure 16-5](#logit_image_part_two)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`dot(beta, x_i)`等于0的点集是我们类之间的边界。我们可以绘制这个图来精确了解我们的模型在做什么（见图16-5，#logit_image_part_two）。'
- en: '![Paid and Unpaid Users With Decision Boundary.](assets/dsf2_1605.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![付费和未付费用户与决策边界。](assets/dsf2_1605.png)'
- en: Figure 16-5\. Paid and unpaid users with decision boundary
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-5。付费和未付费用户与决策边界
- en: This boundary is a *hyperplane* that splits the parameter space into two half-spaces
    corresponding to *predict paid* and *predict unpaid*. We found it as a side effect
    of finding the most likely logistic model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个边界是一个*超平面*，将参数空间分成两个半空间，对应*预测付费*和*预测未付费*。我们发现它是在找到最可能的逻辑模型的副作用中发现的。
- en: An alternative approach to classification is to just look for the hyperplane
    that “best” separates the classes in the training data. This is the idea behind
    the *support vector machine*, which finds the hyperplane that maximizes the distance
    to the nearest point in each class ([Figure 16-6](#separating_hyperplane)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的另一种方法是只需寻找在训练数据中“最佳”分离类别的超平面。这是支持向量机背后的思想，它找到最大化每个类别最近点到超平面距离的超平面（见图16-6，#separating_hyperplane）。
- en: '![A Separating Hyperplane](assets/dsf2_1606.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![一个分离超平面。](assets/dsf2_1606.png)'
- en: Figure 16-6\. A separating hyperplane
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-6。一个分离超平面
- en: Finding such a hyperplane is an optimization problem that involves techniques
    that are too advanced for us. A different problem is that a separating hyperplane
    might not exist at all. In our “who pays?” dataset there simply is no line that
    perfectly separates the paid users from the unpaid users.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找这样的超平面是一个涉及我们过于高级的技术的优化问题。另一个问题是，一个分离超平面可能根本不存在。在我们的“谁付费？”数据集中，简单地没有一条线完全分离付费用户和未付费用户。
- en: We can sometimes get around this by transforming the data into a higher-dimensional
    space. For example, consider the simple one-dimensional dataset shown in [Figure 16-7](#svm_non_separable).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时可以通过将数据转换为更高维空间来绕过这个问题。例如，考虑到简单的一维数据集，如[图16-7](#svm_non_separable)所示。
- en: '![A Non-Separable One-Dimensional Data set](assets/dsf2_1607.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![一个不可分离的一维数据集](assets/dsf2_1607.png)'
- en: Figure 16-7\. A nonseparable one-dimensional dataset
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-7。一个不可分离的一维数据集
- en: It’s clear that there’s no hyperplane that separates the positive examples from
    the negative ones. However, look at what happens when we map this dataset to two
    dimensions by sending the point `x` to `(x, x**2)`. Suddenly it’s possible to
    find a hyperplane that splits the data ([Figure 16-8](#svm_separable)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从明显的来看，没有一个超平面可以将正例和负例分开。然而，当我们通过将点`x`映射到`(x, x**2)`的方式将这个数据集映射到二维空间时，看看会发生什么。突然间，可以找到一个可以分割数据的超平面（见[图 16-8](#svm_separable)）。
- en: '![Becomes Separable In Higher Dimensions](assets/dsf2_1608.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![在更高维度中变得可分离](assets/dsf2_1608.png)'
- en: Figure 16-8\. Dataset becomes separable in higher dimensions
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-8\. 数据集在更高维度中变得可分离
- en: This is usually called the *kernel trick* because rather than actually mapping
    the points into the higher-dimensional space (which could be expensive if there
    are a lot of points and the mapping is complicated), we can use a “kernel” function
    to compute dot products in the higher-dimensional space and use those to find
    a hyperplane.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为*核技巧*，因为我们不是实际将点映射到更高维度的空间（如果有很多点并且映射很复杂的话，这可能会很昂贵），而是可以使用一个“核”函数来计算在更高维度空间中的点积，并使用这些来找到超平面。
- en: It’s hard (and probably not a good idea) to *use* support vector machines without
    relying on specialized optimization software written by people with the appropriate
    expertise, so we’ll have to leave our treatment here.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用支持向量机而不依赖于由具有适当专业知识的人编写的专业优化软件是困难的（也可能不是一个好主意），因此我们将不得不在这里结束我们的讨论。
- en: For Further Investigation
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步调查
- en: scikit-learn has modules for both [logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
    and [support vector machines](https://scikit-learn.org/stable/modules/svm.html).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn既有[逻辑回归](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)模块，也有[支持向量机](https://scikit-learn.org/stable/modules/svm.html)模块。
- en: '[LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) is the support vector
    machine implementation that scikit-learn is using behind the scenes. Its website
    has a variety of useful documentation about support vector machines.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)是scikit-learn背后使用的支持向量机实现。其网站上有大量关于支持向量机的有用文档。'
