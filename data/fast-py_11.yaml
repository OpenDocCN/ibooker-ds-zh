- en: 8 Storing big data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 存储大数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Getting to know fsspec, an abstraction library over filesystems
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解fsspec，一个在文件系统之上的抽象库
- en: Storing heterogeneous columnar data efficiently with Parquet
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Parquet高效存储异构列式数据
- en: Processing data files with in-memory libraries like pandas or Parquet
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas或Parquet等内存库处理数据文件
- en: Processing homogeneous multi-dimensional array data with Zarr
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Zarr处理同构的多维数组数据
- en: When dealing with big data, persistence is of paramount importance. We want
    to be able to access—to read and write—data as fast as possible, preferably from
    many parallel processes. We also want persistent representations that are compact
    because storing large amounts of data can be expensive.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大数据时，持久性至关重要。我们希望尽可能快地访问——读取和写入——数据，最好是来自多个并行进程。我们还希望持久表示紧凑，因为存储大量数据可能很昂贵。
- en: In this chapter, we will consider several approaches to make persistent storage
    of data more efficient. We will start with a short discussion of fsspec, a library
    that abstracts access to file systems, both local and remote. While fsspec isn’t
    directly involved in performance problems, it is a modern library used by many
    applica-tions to deal with storage systems, and its use is recurrent in efficient
    storage implementations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨几种使数据持久化存储更有效的方法。我们将从对fsspec的简要讨论开始，fsspec是一个抽象访问本地和远程文件系统的库。虽然fsspec并不直接涉及性能问题，但它是一个现代库，被许多应用程序用于处理存储系统，并且在高效的存储实现中经常被使用。
- en: We will then consider Parquet, a file format to persist heterogeneous columnar
    datasets. Parquet is supported in Python via the Apache Arrow project, which was
    introduced in the previous chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将考虑Parquet，这是一种用于持久化异构列式数据集的文件格式。Parquet通过Apache Arrow项目在Python中得到支持，该项目在前一章中已介绍。
- en: Next, we will discuss chunked reading of very large datasets, sometimes called
    an *out-of-core approach*. Often, we have stored datasets that cannot be processed
    in-memory all at the same time. Chunked reading allows you to process data in
    parts with software libraries that you already know, which is a simple but very
    efficient strategy. Our example will take a large pandas data frame and convert
    it to a Parquet file. Finally, we will look at Zarr, a modern format and library
    to store multidimensional homogeneous arrays (i.e., NumPy arrays) in persistent
    memory.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论对非常大的数据集进行分块读取，有时称为*离核方法*。通常，我们存储的数据集无法一次性全部在内存中处理。分块读取允许您使用您已经熟悉的软件库分部分批处理数据，这是一种简单但非常有效的策略。我们的示例将从一个大的pandas数据框转换成Parquet文件。最后，我们将探讨Zarr，这是一种用于在持久内存中存储多维同构数组（即NumPy数组）的现代格式和库。
- en: For this chapter, you will need to install fsspec, Zarr, and Arrow, which provides
    the Parquet interface. To install conda, you can use `conda install fsspec zarr
    pyarrow`. The Docker image `tiagoantao/python-performance-dask` includes all the
    necessary libraries. Let’s start with a small overview of the fsspec library,
    which allows us to deal with different types of filesystems, both local and remote,
    using the same API.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要安装fsspec、Zarr和Arrow，后者提供了Parquet接口。要安装conda，您可以使用`conda install fsspec
    zarr pyarrow`。`tiagoantao/python-performance-dask` Docker镜像包含了所有必要的库。让我们先对fsspec库进行简要概述，它允许我们使用相同的API处理不同类型的本地和远程文件系统。
- en: '8.1 A unified interface for file access: fsspec'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 文件访问的统一接口：fsspec
- en: 'There are many systems for file storage, from the venerable local filesystem,
    to cloud storage like Amazon S3, to protocols like SFTP and SMB (Windows file
    shares). The list is large, especially if we consider that there are many other
    filesystem-like objects: for example, a zip file is a file and directory container,
    an HTTP server has a traversable tree, and so on.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 存储文件系统有很多系统，从古老的本地文件系统，到云存储如Amazon S3，再到SFTP和SMB（Windows文件共享）等协议。列表很长，特别是如果我们考虑到还有许多其他类似文件系统的对象：例如，zip文件是一个文件和目录容器，HTTP服务器有一个可遍历的树，等等。
- en: 'Dealing with every type of filesystem means learning a different programming
    API for each one—a laborious, even painful, prospect. Enter fsspec, a library
    that abstracts away many filesystem types behind a unified API. With fsspec, you
    only need to learn a single API to interact with many filesystem types. There
    are a few quirks: for example, you cannot expect the behavior of a local filesystem
    to be the same as a remote one, but the library simplifies access to file systems
    substantially with minimal overhead.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 处理每种类型的文件系统意味着需要学习每种类型的不同编程 API——这是一个费时甚至痛苦的过程。fsspec 就是这样一种库，它通过统一的 API 抽象出许多文件系统类型。使用
    fsspec，你只需要学习一个 API 就可以与许多文件系统类型交互。有几个小问题：例如，你不能期望本地文件系统的行为与远程文件系统相同，但该库通过最小开销大大简化了对文件系统的访问。
- en: 8.1.1 Using fsspec to search for files in a GitHub repo
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 使用 fsspec 在 GitHub 仓库中搜索文件
- en: To illustrate how fsspec works, we will use it to traverse a GitHub repository
    in search of zip files and then determine whether or not those zip files contain
    CSV files. In this exercise, we are treating a GitHub repository as if it is a
    filesystem. This is not so far-fetched as it may sound. When you think about it,
    a GitHub repo is essentially a directory tree with versioned content.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 fsspec 的工作原理，我们将使用它遍历 GitHub 仓库以查找 zip 文件，然后确定这些 zip 文件是否包含 CSV 文件。在这个练习中，我们将
    GitHub 仓库视为文件系统。这并不像听起来那么牵强。当你这么想的时候，GitHub 仓库本质上是一个带有版本化内容的目录树。
- en: For a sample repository, we will use the one from this book. In `08-persistence/
    01-fspec`, you will find a zip file named `dummy.zip`, which contains two dummy
    CSV files. Our code will traverse the repository, find all zip files—in our case,
    only `dummy.zip` exists—open them, and use pandas’ `describe` command to summarize
    all CSVs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个示例仓库，我们将使用本书中的仓库。在 `08-persistence/ 01-fspec` 中，你可以找到一个名为 `dummy.zip` 的
    zip 文件，其中包含两个虚拟 CSV 文件。我们的代码将遍历仓库，找到所有 zip 文件——在我们的情况下，只有 `dummy.zip` 存在——打开它们，并使用
    pandas 的 `describe` 命令来总结所有 CSV 文件。
- en: 'Let’s start by accessing the repository with fsspec and listing the root directory:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先使用 fsspec 访问仓库并列出根目录：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We import the class `GithubFileSystem`, pass the user and repository name, and
    list the top-level directory. Note that the root directory is represented by the
    empty string, not by the typical `/`. fsspec provides many other classes to access
    storage, like the local filesystem, compressed files, Amazon S3, Arrow, HTTP,
    SFTP, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入 `GithubFileSystem` 类，传递用户和仓库名，并列出顶级目录。请注意，根目录由空字符串表示，而不是典型的 `/`。fsspec
    提供了许多其他类来访问存储，如本地文件系统、压缩文件、Amazon S3、Arrow、HTTP、SFTP 等。
- en: 'The `fs` object has several methods common with Python’s filesystem interfaces.
    For example, to traverse the filesystem, which we need to do to find all zip files,
    a `walk` method exists that is very similar to the `walk` method of the `os` module:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`fs` 对象具有与 Python 文件系统接口的几个常用方法。例如，为了遍历文件系统，我们需要这样做以找到所有 zip 文件，存在一个 `walk`
    方法，它与 `os` 模块的 `walk` 方法非常相似：'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`get_zip_list` is a generator that yields all complete paths to existing zip
    files. Note that the code is exactly what you would use with `os.walk` if the
    `root_path` was `/`.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_zip_list` 是一个生成器，它产生所有现有 zip 文件的完整路径。请注意，如果 `root_path` 是 `/`，则代码与 `os.walk`
    完全相同。'
- en: fsspec interface limitations
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: fsspec 接口限制
- en: 'While fsspec provides a unified and simple interface for filesystems, it cannot
    hide all semantic differences. Indeed, in some cases, we do not want it to hide
    all the differences. Using `GitHubFileSystem` as an example, here are two possible
    situations where differences can be seen:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 fsspec 为文件系统提供了一个统一且简单的接口，但它不能隐藏所有的语义差异。实际上，在某些情况下，我们并不希望它隐藏所有的差异。以 `GitHubFileSystem`
    为例，这里有两个可能看到差异的情况：
- en: '*Extra functionality*—You can navigate the repository at any point in time,
    not just at the current time point of the master branch. You can specify a branch
    or a tag, and fsspec will allow you to inspect the repository at that precise
    point.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*额外功能*——你可以在任何时间点导航仓库，而不仅仅是当前主分支的时间点。你可以指定一个分支或标签，fsspec 将允许你在那个精确点检查仓库。'
- en: '*Limitations*—Not only you will have the typical problems with remote filesystems
    (e.g., if you are not connected to the internet, the code won’t work), but also
    if you query the server many times, it will rate-limit you.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*限制*——你不仅会遇到远程文件系统的典型问题（例如，如果你没有连接到互联网，代码将无法工作），而且如果你多次查询服务器，它将对你进行速率限制。'
- en: 'Now that we have a list of zips in the repository, as a first, naïve solution,
    we will copy the zip files from the repository to the local filesystem. The idea
    here is that we will open them locally to see whether they have CSV files:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了仓库中 zip 文件列表，作为一个初步的、天真的解决方案，我们将从仓库复制 zip 文件到本地文件系统。这里的想法是我们将本地打开它们，看看它们是否有
    CSV 文件：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we can inspect the file inside. For this, we can, again naively, use Python’s
    built-in `zipfile` module:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查文件内部的内容。为此，我们再次天真地使用 Python 内置的 `zipfile` 模块：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① We open the file using the zipfile module here.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们在这里使用 zipfile 模块打开文件。
- en: ② Note that the infolist method is specific to the zipfile module, something
    that needs to be learned.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ② 注意 infolist 方法是 zipfile 模块的特有方法，这是需要学习的。
- en: Notice the new API that we need to learn for `zipfile`. We started with the
    constructor and then used the `infolist` method, but we may need to re-open the
    zip mid-listing due to `zipfile` semantics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们需要学习的新 API，用于 `zipfile`。我们从构造函数开始，然后使用了 `infolist` 方法，但由于 `zipfile` 的语义，我们可能需要在列表中间重新打开
    zip 文件。
- en: 8.1.2 Using fsspec to inspect zip files
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 使用 fsspec 检查 zip 文件
- en: 'That previous code listing is just a simple illustration of the *mess* that
    fsspec is saving us from. fsspec provides an interface to zip files, and thus
    we can rewrite that code like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那段之前的代码列表只是简单说明了 fsspec 帮我们避免的 *混乱*。fsspec 提供了访问 zip 文件的接口，因此我们可以像这样重写代码：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① The find method, along with all others, exists for all kinds of filesystems,
    not just zip.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ① find 方法，以及所有其他方法，对所有类型的文件系统都存在，而不仅仅是 zip。
- en: ② As with the find method, open is also available for all types of filesystems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ② 与 find 方法一样，open 也适用于所有类型的文件系统。
- en: Other than creating the `ZipFileSystem` object, the interface is exactly the
    same as the one for GitHub and very close to common Python file interfaces. There
    is no need to learn the `zipfile` interface.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建 `ZipFileSystem` 对象外，接口与 GitHub 和常见的 Python 文件接口完全相同。不需要学习 `zipfile` 接口。
- en: 8.1.3 Accessing files using fsspec
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 使用 fsspec 访问文件
- en: 'You can also use fsspec to open files directly, although the semantics are
    a bit different from the standard `open`. For example, to open a zip file using
    fsspec `open`, we use the following code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 fsspec 直接打开文件，尽管其语义与标准的 `open` 函数略有不同。例如，要使用 fsspec 的 `open` 打开 zip 文件，我们使用以下代码：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① To open the file, we need to use the with statement.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ① 打开文件时，我们需要使用 with 语句。
- en: ② We are using Python’s zipfile module again to parse the file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们再次使用 Python 的 zipfile 模块来解析文件。
- en: 'The output is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice the need to use the `with` dialect after `open` to get a proper file
    descriptor, which is different from the typical approach of just using the `open`
    function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 `open` 之后需要使用 `with` 语句来获取合适的文件描述符，这与仅使用 `open` 函数的典型方法不同。
- en: 8.1.4 Using URL chaining to traverse different filesystems transparently
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 使用 URL 链接透明地遍历不同的文件系统
- en: 'Let’s go back to our zip file inside the GitHub repository. Note that because
    we can interpret the zip file as a container for files, this zip file is like
    having a filesystem inside another filesystem. fsspec has a declarative way of
    allowing us to get to our data quite easily: URL chaining. You can sometimes take
    a stream and reinterpret it as a filesystem. An example will make this clear;
    let’s print the content of `dummy1.csv`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到 GitHub 仓库内的 zip 文件。注意，因为我们可以将 zip 文件解释为文件的容器，所以这个 zip 文件就像在另一个文件系统中有一个文件系统。fsspec
    有一种声明式的方法，允许我们轻松地访问我们的数据：URL 链接。有时你可以取一个流并将其重新解释为文件系统。一个例子将使这一点更清晰；让我们打印 `dummy1.csv`
    的内容：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Notice URL chaining in action: we take `dummy1.csv` from `/tmp/dl.zip`. You
    did not need to explicitly open the zip file; fsspec took care of that for you.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 URL 链接的实际应用：我们从 `/tmp/dl.zip` 中获取 `dummy1.csv`。你不需要显式打开 zip 文件；fsspec 为你处理了这一点。
- en: 'Remember that we referred to our implementation of `get_zips` as naive? It
    is naive because we do not need to explicitly download the file, courtesy of URL
    chaining:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们称我们的 `get_zips` 实现为“天真”吗？这是因为我们不需要显式下载文件，多亏了 URL 链接：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We are hardcoding the complete chained URL to make clear an explicit example
    of usage.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们硬编码完整的链式 URL 以清楚地说明一个明确的用法示例。
- en: 8.1.5 Replacing filesystem backends
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.5 替换文件系统后端
- en: 'Now, because fsspec abstracts away filesystem interfacing, it is very easy
    to replace a filesystem implementation. For example, let’s replace GitHub with
    the local filesystem. It is as simple as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为 fsspec 抽象了文件系统接口，所以很容易替换文件系统实现。例如，让我们将 GitHub 替换为本地文件系统。这很简单：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This assumes that you are running the script from the directory `08-persistence/
    sec1-fsspec`; as such, `../..` will be the root of the book repository.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设您正在从目录 `08-persistence/sec1-fsspec` 运行脚本；因此，`../..` 将是本书库的根目录。
- en: We use `LocalFileSystem` instead of `GitHubFileSystem`, and that is *almost*
    it. Because we are running this code two levels deep from the top of the repository,
    we need to move up the tree—hence, the `chdir`. Now how the code works is on top
    of the local filesystem, not GitHub. For example, run `describe_all_csvs_in_zips(fs)`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `LocalFileSystem` 而不是 `GitHubFileSystem`，这就是全部。因为我们在这个代码中从仓库顶部向下运行了两个层级，我们需要向上移动到树的一级——因此，需要使用
    `chdir`。现在代码的工作方式是在本地文件系统之上，而不是 GitHub。例如，运行 `describe_all_csvs_in_zips(fs)`。
- en: 8.1.6 Interfacing with PyArrow
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.6 与 PyArrow 交互
- en: 'Finally, it is worth noting that PyArrow, which we discussed in the previous
    chapter, can interface directly with fsspec:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得注意的是，我们在上一章中讨论的 PyArrow 可以直接与 fsspec 交互：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The important part here is that Arrow has the concept of the filesystem, which
    allows it to naturally integrate with fsspec. The Arrow filesystem can bridge
    with fsspec via `pyarrow.fs.FSSpecHandler`. After a fsspec filesystem is mapped
    this way, Arrow filesystem primitives can be used on top of it transparently.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是，Arrow 有文件系统的概念，这使得它可以自然地与 fsspec 集成。Arrow 文件系统可以通过 `pyarrow.fs.FSSpecHandler`
    与 fsspec 互连。一旦以这种方式映射了 fsspec 文件系统，就可以在它之上透明地使用 Arrow 文件系统原语。
- en: Tip fsspec supports the ability to partially download data from remote servers,
    which can be important in big data situations where we might only need a fraction
    of a big file. This can only be done if the server type that we are trying to
    use supports partial file downloading. For example, GitHub doesn’t support it;
    conversely, S3 does. You can enable this feature by activating the cache when
    you call `open` by using the parameter `cache_type` with a value of `readahead`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 fsspec 支持从远程服务器部分下载数据的能力，这在可能只需要大文件的一部分的大数据场景中可能很重要。只有当我们尝试使用的服务器类型支持部分文件下载时，才能这样做。例如，GitHub
    不支持它；相反，S3 支持。您可以通过在调用 `open` 时激活缓存来启用此功能，使用参数 `cache_type` 并将其值设置为 `readahead`。
- en: That was a bit of a sidetrack, as fsspec is not directly related to performance,
    although it is used in many performance-related libraries like Dask, Zarr, and
    Arrow. Now, let’s go back to our normally scheduled programming, looking at approaches
    to efficiently store heterogenous columnar data, aka data frames.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点跑题了，因为 fsspec 并不直接与性能相关，尽管它被用于许多与性能相关的库中，如 Dask、Zarr 和 Arrow。现在，让我们回到我们的正常编程计划，探讨高效存储异构列式数据（即数据框）的方法。
- en: '8.2 Parquet: An efficient format to store columnar data'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 Parquet：一种高效的列式数据存储格式
- en: Storing data in CSVs is fraught with problems. First, because they can’t accommodate
    typing of each column, it’s not uncommon to have unexpected values in columns.
    In addition, the format itself is inefficient. For example, you can represent
    numbers much more compactly in binary form than in text. In addition, you cannot
    jump to a specific row or column in constant time, as it is not possible to compute
    the location of its position because each line in a CSV can vary in size.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据存储在 CSV 中充满了问题。首先，因为它们无法容纳每列的类型，所以在列中意外值并不少见。此外，该格式本身效率低下。例如，您可以用二进制形式比文本形式更紧凑地表示数字。此外，您不能在常数时间内跳转到特定的行或列，因为无法计算其位置，因为
    CSV 中的每一行大小都可能不同。
- en: Apache Parquet is becoming the most common format to efficiently store heterogeneous
    columnar data. This means you can access just the columns you need and also use
    data-compression and column-encoding formats to increase performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet 正在成为最常用的格式，用于高效存储异构列式数据。这意味着您可以访问所需的列，并且还可以使用数据压缩和列编码格式来提高性能。
- en: In this section, we will learn how to use Parquet to store data frames, drawing
    on the New York City taxi data from the previous chapter. As we walk through this
    task, I will also present a tour of many Parquet features.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 Parquet 存储数据框，借鉴上一章中纽约市出租车数据。在完成这个任务的过程中，我还会介绍许多 Parquet 功能的概览。
- en: Warning Parquet is a file format that started in the Java world, specifically
    in the Hadoop ecosystem. While the available Python implementations are perfectly
    fit for production purposes, they do not implement the specification completely.
    For example, we cannot specify in full detail how we want to encode columns; neither
    we can inspect how a column is stored—something I will show here. But for the
    vast majority of use cases, the necessary functionality is present and will only
    increase over time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：Parquet 是一个起源于 Java 世界、特别是在 Hadoop 生态系统中的文件格式。虽然可用的 Python 实现非常适合生产目的，但它们并没有完全实现规范。例如，我们无法详细指定我们想要如何编码列；我们也不能检查列是如何存储的——我将在下面展示这一点。但对于绝大多数用例，必要的功能是存在的，并且随着时间的推移只会增加。
- en: 'Just as a reminder, the taxi dataset has information about all taxi runs in
    New York City for a period of time. Information includes, among other things,
    start and end time of the run, start and end location, cost, taxes, and tips.
    We will begin by using the same file as in the previous chapter, which includes
    taxi runs for January 2020\. The first thing that we will do is convert the CSV
    file into Parquet. For this, we will use Apache Arrow, introduced in the previous
    chapter. The code can be found in `08-persistence/ sec2-parquet/start.py`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 就作为一个提醒，出租车数据集包含了关于一段时间内纽约市所有出租车行程的信息。信息包括但不限于行程的开始和结束时间、开始和结束位置、费用、税费和小费。我们将从使用与上一章相同的文件开始，该文件包括
    2020 年 1 月的出租车行程。我们将首先将 CSV 文件转换为 Parquet。为此，我们将使用上一章中介绍的 Apache Arrow。代码可以在 `08-persistence/
    sec2-parquet/start.py` 中找到：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We simply use `write_table` from the Parquet module of PyArrow. We end up with
    a 111 MB binary file. The compressed CSV is 105 MB, and the original uncompressed
    version, 567 MB. Because Parquet is a structured binary format, we should expect
    some differences in size for the same content. The point here is not to fixate
    on the details but to have an insight into the size relationships.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地使用 PyArrow 的 Parquet 模块中的 `write_table`。最终我们得到一个 111 MB 的二进制文件。压缩后的 CSV
    文件是 105 MB，原始未压缩版本是 567 MB。因为 Parquet 是一种结构化二进制格式，所以我们应预期相同内容的大小会有所不同。这里的重点不是纠结于细节，而是要了解大小关系。
- en: 8.2.1 Inspecting Parquet metadata
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 检查 Parquet 元数据
- en: 'Let’s discover some of Parquet’s features by inspecting the file:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过检查文件来发现一些 Parquet 的特性：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The abridged output is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的输出是：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We start by printing the metadata for the file. Here we simply get some summary
    information such as having 18 columns and 6,405,008 rows. Paquet also tells us
    that there is a single row group in the file. A row group is a partition of the
    total of rows: in larger files, there may be more than a single row group. A row
    group will have all the column data for the rows in the group. Remember, information
    in Parquet is organized by columns. This will become clear shortly.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先打印文件的元数据。这里我们只是获取一些摘要信息，例如有 18 列和 6,405,008 行。Parquet 还告诉我们文件中只有一个行组。行组是总行数的分区：在较大的文件中，可能有多个行组。行组将包含组中所有行的列数据。记住，Parquet
    中的信息是按列组织的。这很快就会变得清晰。
- en: 'We then print the schema of the file. An abridged version is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们打印文件的架构。简化的版本是：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Here you have the definition of VendorID, which has a bit width of 8 and is
    not signed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这里是 VendorID 的定义，它有 8 位宽，且未签名。
- en: This code lists all the columns for our data. For example `VendorID` is an `int32`,
    but note that the bit width is 8 and that it is unsigned. `VendorID` had only
    two possible values plus a null, so it makes sense to reduce its implementation
    to just 8 unsigned bits. This could even be reduced to fewer bits as, in theory,
    Parquet supports such a reduction.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码列出了我们数据中的所有列。例如 `VendorID` 是一个 `int32`，但请注意位宽是 8 位，且是无符号的。`VendorID` 只有两个可能的值加上一个空值，所以将其实现减少到仅
    8 位无符号位是有意义的。理论上，Parquet 支持这种减少，甚至可以减少到更少的位。
- en: 'Then we have `tpep_pickup_datetime`, which is a time stamp. From a storage
    perspective, the time unit is the most important variable, as more precision will
    require more space. pandas defaults to nanosecond precision. Also, note `store_and_fwd_
    flag`: text is stored as general binary data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是 `tpep_pickup_datetime`，这是一个时间戳。从存储的角度来看，时间单位是最重要的变量，因为更高的精度需要更多的空间。pandas
    默认为纳秒精度。注意 `store_and_fwd_ flag`：文本以通用二进制数据存储。
- en: 8.2.2 Column encoding with Parquet
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 Parquet 的列编码
- en: 'Let’s now look at the existing metadata for several columns:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看几个列的现有元数据：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The abridged output is:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的输出是：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Statistical information about the column starts here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ① 列的统计信息从这里开始。
- en: ② The compression algorithm used in the column
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ② 列中使用的压缩算法
- en: The metadata starts with the physical type, the number of values, and the column
    name. The statistical information (there seems to be a negative tip—probably an
    entry mistake) shows $-91 as the minimum and $1,000 as the maximum tip. Now things
    start to get *really* interesting.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据从物理类型、值的数量和列名开始。统计信息（似乎有一个负的小费——可能是输入错误）显示$-91为最小值，$1,000为最大小费。现在事情开始变得*真正*有趣。
- en: With regards to the storage of data proper, Parquet can compress columns, which
    saves disk space. Compressing columns can also provide potential computational
    gains related to cache management problems discussed in the previous chapter.
    Different columns can have different compression types or no compression at all.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据的存储，Parquet可以压缩列，这可以节省磁盘空间。压缩列还可以提供与上一章讨论的缓存管理问题相关的潜在计算收益。不同的列可以有不同的压缩类型或根本不进行压缩。
- en: In our example, the Snappy algorithm is used for compression. Snappy trades
    more compression for speed, compared to, say, gzip, which is also an option. Make
    sure to check what compression algorithms Arrow implements at the time of usage.
    Facebook has some benchmarking information available at [https://facebook.github.io/zstd/#benchmarks](https://facebook.github.io/zstd/#benchmarks)
    to help you decide.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，使用的是Snappy压缩算法。与gzip（也是一个选项）相比，Snappy在压缩和速度之间进行了权衡。确保在使用时检查Arrow实现了哪些压缩算法。Facebook在[https://facebook.github.io/zstd/#benchmarks](https://facebook.github.io/zstd/#benchmarks)有一些基准测试信息，可以帮助你做出决定。
- en: 'For example, you can use ZSTD:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用ZSTD：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We are using ZSTD on all columns in this example. In this case, you go down
    from 110 MB with Snappy to 82 MB.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用ZSTD对所有列进行压缩。在这种情况下，从Snappy的110 MB降至82 MB。
- en: 'Parquet can encode columns not only with direct values but also by using dictionaries,
    where a long value is converted to an indirect reference, potentially saving a
    lot of disk space. To understand how this can help, consider that tips are represented
    by a double requiring 64 bits, whereas there are only 3626 different values for
    tips:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet不仅可以直接使用值编码列，还可以使用字典，其中长值被转换为间接引用，这可能会节省大量磁盘空间。为了了解这如何有所帮助，考虑小费是以双精度表示的，需要64位，而小费只有3626个不同的值：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: A dictionary can reduce encoding from 64 bits to 12 bits per value, which is
    enough to encode up to 4096 values. We also need to store the dictionary, which
    is residual for 3626 values. However, because we have many distinct values, it
    might not make sense to use a dictionary. You can control whether a column is
    stored with a dictionary or not with `write_table`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 字典可以将编码从每值64位减少到12位，这足以编码高达4096个值。我们还需要存储字典，这对于3626个值来说是多余的。然而，因为我们有很多不同的值，使用字典可能没有意义。你可以使用`write_table`来控制是否使用字典存储列。
- en: 'Last but not least, note that the encoding also has RLE, which stands for Run
    Length Encoding. Let’s look at the advantage of RLE with a somewhat silly example.
    Let’s create a data frame with a column with `VendorID` followed by another column
    *also* with `VendorID`, but ordered:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，请注意编码也使用了RLE，即运行长度编码。让我们用一个有点愚蠢的例子来看看RLE的优势。让我们创建一个包含`VendorID`列的数据帧，后面跟着另一个也带有`VendorID`列，但有序：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'So it’s the same data, with ordered and unordered versions. Let’s now see how
    much space each column occupies on a Parquet file:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是相同的数据，有序和无序版本。现在让我们看看每个列在Parquet文件中占用多少空间：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The unordered file takes 953,295 bytes, and the ordered file takes 141 bytes!
    The way RLE works is by storing the value and the number of repetitions. With
    an ordered `VendorID` column, we have an extreme case: we have only three values
    (`1`, `2`, and `null`), which are ordered. So in theory, RLE can store: 1.0 2094439
    / 2.0 4245128 / null 65441.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 无序文件占用953,295字节，而有序文件仅占用141字节！RLE的工作方式是存储值和重复次数。对于有序的`VendorID`列，我们有一个极端案例：我们只有三个值（`1`、`2`和`null`），它们是有序的。所以理论上，RLE可以存储：1.0
    2094439 / 2.0 4245128 / null 65441。
- en: RLE can compress data quite substantially. While our case is extreme in terms
    of efficiency, RLE typically works well for ordered fields or fields with few
    values. However, if you deviate from these assumptions, make sure you evaluate
    the compression benefit that you are getting.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: RLE可以相当大幅度地压缩数据。虽然就效率而言我们的案例是极端的，但RLE通常适用于有序字段或值较少的字段。然而，如果你偏离了这些假设，确保你评估你获得的压缩效益。
- en: Smaller files help with storage *and* processing time. Remember from chapter
    6 that if you can have data in faster types of memory, you can sometimes have
    orders of magnitude gains in performance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的文件有助于存储 *和* 处理时间。记住，在第 6 章中提到，如果您可以将数据存储在更快的内存类型中，您有时可以在性能上获得数量级的提升。
- en: The format is extensible, so you can expect a new way to efficiently store data
    to be developed over time. The format also allows for data partitioning, which
    has several advantages from an efficiency perspective. Let’s make it clear with
    an example.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该格式是可扩展的，因此您可以期待随着时间的推移开发出一种新的高效存储数据的方式。该格式还允许数据分区，从效率角度来看具有几个优点。让我们用一个例子来说明。
- en: 8.2.3 Partitioning with datasets
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 使用数据集进行分区
- en: 'To clarify what partitioning means and what the process entails, let’s partition
    our dataset using `VendorID` and `passenger_count`. As partitions cannot be based
    on null values, we will remove those from our dataset. We only do this for this
    exercise; in general, you cannot remove null value rows just out of convenience:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明分区意味着什么以及涉及的过程，让我们使用 `VendorID` 和 `passenger_count` 对我们的数据集进行分区。由于分区不能基于空值，我们将从数据集中删除这些值。我们只为这次练习这样做；通常情况下，您不能仅仅为了方便就删除空值行：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Note again that the syntax to do the computation with Arrow is very different
    from pandas.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ① 再次注意，使用 Arrow 进行计算时的语法与 pandas 非常不同。
- en: The equivalent to the first filter line in pandas would be `table = table[~table
    ["VendorID"].isna()]`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 中第一行过滤语句的等价操作将是 `table = table[~table ["VendorID"].isna()]`。
- en: 'If you look at `all.parquet`, you will find a few surprises: The biggest surprise
    is that it is not a file anymore but a directory! The abridged contents will be
    something like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看 `all.parquet`，您会发现一些惊喜：最大的惊喜是它不再是文件，而是一个目录！简化的内容可能如下所示：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The directory structure reflects our partitioning strategy. The first level
    of directories has an entry per `VendorID`, and the second level, one per `passenger_count`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 目录结构反映了我们的分区策略。第一级目录中每个 `VendorID` 都有一个条目，第二级目录中每个 `passenger_count` 都有一个条目。
- en: 'You now have two options. The easiest one—and arguably less interesting—is
    to load everything as a table:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在有两个选择。最简单的一个——也许不那么有趣——是将所有内容都加载为一个表格：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here you will have all the data as a normal table. You can, alternatively,
    do the following to the same effect:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您将拥有所有数据作为一个正常的表格。您也可以采取以下措施达到相同的效果：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'But, as an alternative option, you can also load each parquet *file* separately.
    For example, let’s load the file for the partition of vendor ID 1 with three passengers:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，作为另一种选择，您也可以单独加载每个 parquet *文件*。例如，让我们加载包含三个乘客的供应商 ID 1 的分区文件：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① The name of the parquet file is not assured, so we get the first file in the
    directory.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ① parquet 文件的名字并不保证，所以我们获取目录中的第一个文件。
- en: If you look at the output, you will notice that the columns `VendorID` and `passenger_
    count` are missing, as they can be inferred from the directory.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看输出，您会注意到列 `VendorID` 和 `passenger_count` 缺失，因为它们可以从目录中推断出来。
- en: Warning What is inside each directory can vary. In our case, with PyArrow, it
    is a single Parquet file. For example, you can tell Parquet to further split each
    partition into a file by row group. So, make sure you investigate how the data
    is actually written to the disk and adapt the code accordingly.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：每个目录中的内容可能不同。在我们的案例中，使用 PyArrow，它是一个单独的 Parquet 文件。例如，您可以让 Parquet 将每个分区进一步拆分为一个文件，按行组拆分。因此，请确保您调查数据实际上是如何写入磁盘的，并相应地调整代码。
- en: What is the point of partitioning from a performance perspective? We can now
    load each Parquet file separately and process each one accordingly. For example,
    we can improve performance by using multiple processes on the same machine, each
    doing analysis on each file. We can even process different files on different
    machines. Implicitly, the filesystem can be more efficient as concurrent loads
    are done in different parts of the disk. There can also be a memory gain because
    we don’t load the partition columns. Finally, partitioning opens the avenue for
    concurrent writes, which provides performance gains from parallelism. We will
    discuss concurrent writes in more detail in section 8.4.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能角度来看，分区的目的是什么？我们现在可以单独加载每个 Parquet 文件并相应地处理每个文件。例如，我们可以通过在同一台机器上使用多个进程来提高性能，每个进程分析每个文件。我们甚至可以在不同的机器上处理不同的文件。隐含地，文件系统可以更高效，因为并发加载在不同的磁盘部分进行。我们还可以通过不加载分区列来获得内存上的收益。最后，分区开辟了并发写入的途径，这从并行性中获得了性能提升。我们将在第
    8.4 节中更详细地讨论并发写入。
- en: The way data is partitioned matters from a performance perspective. For example,
    Vendor 1 has half the data of Vendor 2, which means that the cost of processing
    Vendor 2 will probably be double that of Vendor 1\. This doubling may cause you
    to wait for the slowest of all the partitions because you want to be as uniform
    as possible. `VendorID` might be a good choice when compared with `passenger_count`.
    Parquet has many more features, but from a performance perspective, we now have
    a good overview of how we can benefit from the format.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的分区方式从性能角度来看很重要。例如，供应商1的数据是供应商2的一半，这意味着处理供应商2的成本可能大约是供应商1的两倍。这种加倍可能会导致您等待所有分区中最慢的一个，因为您希望尽可能均匀。与`passenger_count`相比，`VendorID`可能是一个不错的选择。Parquet有许多更多功能，但从性能角度来看，我们现在已经很好地概述了我们可以从该格式中受益的方式。
- en: 8.3 Dealing with larger-than-memory datasets the old-fashioned way
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 以传统方式处理大于内存的数据集
- en: 'In this section, we will work with Parquet and CSV files to go over two simple
    techniques to deal with data that is bigger than memory: memory mapping and chunking.
    There are more sophisticated ways of doing both tasks, and we will discuss them
    in section 8.4, as well as in the next chapter. But chunking and memory mapping
    are important concepts that underpin more sophisticated libraries. Therefore,
    understanding them is not only valid in itself but also fundamental to understanding
    more advanced techniques.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Parquet和CSV文件来介绍两种处理大于内存的数据的简单技术：内存映射和分块。处理这两个任务还有更复杂的方法，我们将在第8.4节以及下一章中讨论它们。但是，分块和内存映射是支撑更复杂库的重要概念。因此，理解它们不仅本身有效，而且对于理解更高级的技术也是基本的。
- en: 8.3.1 Memory mapping files with NumPy
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 使用NumPy进行内存映射文件
- en: Memory mapping occurs when a part of the memory is directly associated with
    a part of the filesystem. In the specific case of NumPy, an array that is persisted
    to storage can be assessed with the normal NumPy API, and NumPy will take care
    of bringing to RAM whatever parts we need from the array. In most cases, this
    is done transparently by the operating system kernel for NumPy. Conversely, it
    will change the persistent representation when we write. Because you are assessing
    memory, this can speed up your code by orders of magnitude. Figure 8.1 depicts
    memory mapping.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当内存的一部分直接与文件系统的一部分关联时，就会发生内存映射。在NumPy的具体情况下，持久化到存储的数组可以使用正常的NumPy API进行评估，NumPy将负责将我们从数组中需要的任何部分带到RAM中。在大多数情况下，这是由操作系统内核为NumPy透明完成的。相反，当我们写入时，它将更改持久表示。因为您正在评估内存，这可以以数量级的方式加快您的代码。图8.1描述了内存映射。
- en: '![](../Images/CH08_F01_Antao.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F01_Antao.png)'
- en: Figure 8.1 Memory mapping mirrors a part of a file into memory.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 内存映射将文件的一部分映射到内存中。
- en: 'In this case, we will use a simple abstract example of creating a big array
    and accessing it. You can decide the size of the array. For this exercise, I recommend
    a size that is bigger than your memory but for which you have enough disk space.
    Allocation is quite simple:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将使用一个简单的抽象示例来创建一个大数组并访问它。您可以决定数组的大小。为此练习，我建议一个比您的内存更大的大小，但您有足够的磁盘空间。分配相当简单：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① Changes the size to a value appropriate for your machine, as previously specified
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将大小更改为适合您机器的值，如之前所述
- en: 'The `np.memmap` call is quite straightforward: you pass it a file name, an
    open mode, and the type and shape of the array. If you list the files on your
    disk, you will find a file that is 10 GB in size.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.memmap`调用非常简单：您传递给它一个文件名、一个打开模式以及数组的类型和形状。如果您列出磁盘上的文件，您将找到一个大小为10GB的文件。'
- en: 'The array will be initialized with all zeroes; hence, the print will show an
    array with 10 zeroes. Let’s now add 2 to all elements in the array:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数组将以所有零初始化；因此，打印将显示一个包含10个零的数组。现在让我们向数组中的所有元素添加2：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The interface is exactly the same as for an in-memory NumPy array. But you will
    notice that this operation will take a few seconds. The time increases because
    the large file is being changed all across it; it’s not a fast in-memory operation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接口与内存中的NumPy数组完全相同。但您会注意到这个操作将花费几秒钟。时间增加是因为整个大文件正在被改变；这不是一个快速的内存操作。
- en: 'Let’s now open the file and print the last value:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打开文件并打印最后一个值：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The important point here is that the shape of the array is not saved with it,
    so if you map it without specifying the shape, you get a linear array. Therefore,
    you need to make sure that you recover the desired shape. We then print the last
    10 elements of that array and we get ten 2s.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要点是数组的形状并没有与其保存，所以如果你不指定形状进行映射，你会得到一个线性数组。因此，你需要确保你恢复所需的形状。然后我们打印该数组的最后
    10 个元素，我们得到十个 2。
- en: NumPy copy-on-write
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 写时复制
- en: NumPy memory mapping allows you to use a technique called *copy-on-writ*e. This
    permits you to have several copies of a disk array loaded into memory and pay
    a substantially lower price in terms of memory usage. This technique is prone
    to bugs in many circumstances, mostly because Python is not the best language
    to deal with shared data structures and because memory mapping semantics become
    unclear when you change the underlying file. I don’t think the benefits justify
    the risks unless you are *sure* you will be performing only read operations. If
    you want to research this technique, I recommend the excellent article from Itamar
    Turner-Trauring, available at [https://pythonspeed.com/articles/reduce-memory-array-copies/](https://pythonspeed.com/articles/reduce-memory-array-copies/).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 内存映射允许你使用一种称为 *写时复制* 的技术。这允许你将多个磁盘数组副本加载到内存中，并在内存使用方面支付显著较低的价格。这种技术在许多情况下都容易出bug，主要是因为
    Python 不是处理共享数据结构最好的语言，而且当更改底层文件时，内存映射语义变得不明确。我认为除非你 *确定* 你只会执行读操作，否则这种技术的优势不足以证明其风险。如果你想研究这种技术，我推荐
    Itamar Turner-Trauring 的优秀文章，可在 [https://pythonspeed.com/articles/reduce-memory-array-copies/](https://pythonspeed.com/articles/reduce-memory-array-copies/)
    找到。
- en: I would generally steer away from explicit memory-mapping techniques that perform
    concurrency writes and sharing unless you are absolutely sure that every process
    is only reading. Also, if you are a developer of a low-level library, you might
    use memory mapping with writing, but you probably will not be using Python to
    implement the most efficient parts anyway, so the problem will be addressed in
    other languages.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常会避免使用执行并发写入和共享的显式内存映射技术，除非你绝对确定每个进程只进行读取。此外，如果你是低级库的开发者，你可能会使用带有写入的内存映射，但你可能不会使用
    Python 来实现最有效的部分，所以这个问题将在其他语言中得到解决。
- en: 'Remember that even if you are not using memory mapping directly, many frameworks
    that you use will do that implicitly, so understanding it is useful. Let’s now
    discuss another technique to deal with large files: chunking.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，即使你没有直接使用内存映射，你使用的许多框架也会隐式地这样做，所以理解它是很有用的。现在让我们讨论另一种处理大文件的技术：分块。
- en: 8.3.2 Chunk reading and writing of data frames
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 数据帧的块读取和写入
- en: Chunking, as the name suggests, means processing a file in, well, *chunks*.
    You read (or write) the file in parts. You will definitely deal with chunking
    if you use Zarr (see section 8.4) or Dask (see chapter 10).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 分块，正如其名所示，意味着以，嗯，*块*的形式处理文件。你按部分读取（或写入）文件。如果你使用 Zarr（见第 8.4 节）或 Dask（见第 10 章），你肯定会处理分块。
- en: Here we will return to our trusty taxi example. We will convert the file from
    CSV to Parquet but in chunks. Although the file is small enough that we could
    do this in-memory in most computers, let’s assume we are in a memory-constrained
    machine and that loading the full file in-memory is not possible.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将回到我们信任的出租车示例。我们将以块的形式将文件从 CSV 转换为 Parquet，尽管文件足够小，在大多数计算机上我们可以在内存中完成这个操作，但让我们假设我们在一个内存受限的机器上，并且无法在内存中加载整个文件。
- en: 'We will use pandas to read the CSV file and Arrow to write the Parquet version.
    We could do everything with Arrow, which would be more efficient, but we want
    to demonstrate the pandas chunking interface:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 pandas 读取 CSV 文件，并使用 Arrow 写入 Parquet 版本。我们可以用 Arrow 完成所有操作，这将更高效，但我们想展示
    pandas 分块接口：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ① The type will be pandas.io.parsers.TextFileReader.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ① 类型将是 pandas.io.parsers.TextFileReader。
- en: ② Each chunk will be a data frame.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ② 每个块将是一个数据帧。
- en: We only need to add the parameter `chunksize` to `read_csv`. You will not have
    a data frame from `read_csv`, but a generator of chunks. Each chunk will then
    be a data frame with a maximum size of 1 million rows.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要将 `chunksize` 参数添加到 `read_csv` 中。你将不会从 `read_csv` 获得一个数据帧，而是一个块生成器。然后每个块将是一个数据帧，最大行数为
    100 万行。
- en: 'We will now do the conversion proper. The first thing that we need to do is
    to reopen the file. We have iterated through all the chunks once, so we need to
    go back to the beginning:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将进行适当的转换。首先，我们需要重新打开文件。我们已经遍历了所有块一次，所以我们需要回到开始：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We also need to specify some column data types; the type of some columns will
    change from chunk to chunk. This is mostly the case with integer columns that
    have null values. When there are null values, the type will be promoted to `float`
    as there is no way to represent a null value in pandas with integers.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要指定一些列的数据类型；某些列的类型可能会从一块数据变化到另一块数据。这种情况在整数列中尤为常见，尤其是那些包含空值的列。当存在空值时，类型会被提升为`float`，因为在pandas中无法用整数来表示空值。
- en: 'Now, we will traverse the chunks and create the Parquet file:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将遍历块并创建Parquet文件：
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① We convert the pandas frame into an Arrow table.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们将pandas框架转换为Arrow表。
- en: ② We create a writer object. We need to specify the schema at initialization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们创建一个写入对象。在初始化时我们需要指定模式。
- en: The `ParquetWriter` interface allows us to write table after table in the same
    file. Each table will be written in a separate Parquet row group. It will be,
    in a sense, a chunk.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`ParquetWriter`接口允许我们在同一文件中写入多个表。每个表将写入一个单独的Parquet行组。在某种程度上，它将是一个块。'
- en: 'We can read the Parquet data in several ways:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式读取Parquet数据：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ① We can read each row group separately.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们可以单独读取每一行组。
- en: The metadata of the Parquet file will indicate that there are seven row groups.
    Parquet allows us to read row group by row group. If you have enough memory, there
    are two interfaces—in `ParquetFile` with `read` or in the `parquet` module with
    `read_table`—that take care of reading all read groups and creating a table in-memory.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet文件的元数据将指示存在七个行组。Parquet允许我们按行组读取。如果您有足够的内存，有两个接口可以在`ParquetFile`的`read`方法或`parquet`模块的`read_table`方法中使用，它们负责读取所有行组并在内存中创建一个表。
- en: Armed with the notion of chunking, which allows us to load and process the data
    in parts, we are now going to have a look at Zarr. Zarr is a library that allows
    us to manipulate very large homogeneous N-dimension arrays (i.e., NumPy objects).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 借助分块的概念，这使我们能够分部分加载数据并进行处理，我们现在将来看看Zarr。Zarr是一个库，它允许我们操作非常大的同构N维数组（即NumPy对象）。
- en: 8.4 Zarr for large-array persistence
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 Zarr用于大数组持久化
- en: Some of the biggest datasets in existence are not heterogeneous table data frames
    but multidimensional homogeneous arrays. Therefore, it is important to store these
    larger arrays efficiently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现实中存在的一些最大的数据集并不是异构的表格数据帧，而是多维同构数组。因此，高效地存储这些较大的数组非常重要。
- en: Zarr allows us to efficiently store homogeneous multidimensional arrays with
    different backends and different encoding formats. Functionality like concurrent
    writing can be extremely useful to generate data efficiently.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr 允许我们使用不同的后端和不同的编码格式高效地存储同构的多维数组。像并发写入这样的功能可以非常有效地生成数据。
- en: There are a couple of very mature standards to represent array data (e.g., NetCDF
    and HDF5), but in our case, we will use the nascent format Zarr. Zarr is substantially
    more optimizable than any other format for efficient processing. For example,
    it allows for concurrent writes and different organization of the file structure,
    both of which can have massive implications on performance. Concurrent writes
    allow for many parallel processes to work simultaneously on the same structure.
    Different file structures allow us to take advantage of the performance properties
    of the filesystem.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着一些非常成熟的表示数组数据的标准（例如NetCDF和HDF5），但在这个案例中，我们将使用新兴的格式Zarr。Zarr在优化方面比其他任何格式都要好，对于高效处理非常有用。例如，它允许并发写入和不同的文件结构组织，这两者都可以对性能产生巨大影响。并发写入允许许多并行进程同时在同一结构上工作。不同的文件结构使我们能够利用文件系统的性能特性。
- en: 'While Zarr is a file format, it started in the Python space implemented in
    a library called Zarr. So you can be sure the Python version implements all the
    major features of the format. If you plan to use Zarr files from other programming
    languages, you should check first to see whether the libraries for those languages
    support such features. Zarr is, in a sense, the opposite of Parquet: Parquet came
    from the Java ecosystem to Python, so Python for Parquet still doesn’t cover all
    the features. With Zarr, the Python implementation is the gold standard.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Zarr 是一种文件格式，但它始于 Python 空间，由一个名为 Zarr 的库实现。因此，您可以确信 Python 版本实现了该格式的所有主要功能。如果您计划使用其他编程语言的
    Zarr 文件，您应该首先检查这些语言的库是否支持这些功能。在某种程度上，Zarr 与 Parquet 相反：Parquet 是从 Java 生态系统迁移到
    Python 的，因此 Python 对 Parquet 的支持仍然不全面。对于 Zarr，Python 实现是金标准。
- en: Zarr was started in the bioinformatics space, and we will be using a bioinformatics
    example. We will use data from an old genomics project called the HapMap ([https://www.genome.gov/10001688/international-hapmap-project](https://www.genome.gov/10001688/international-hapmap-project)).
    This project has genomics variants (variation in DNA letters) for many individuals
    accross human populations. You don’t need to know any of the scientific details
    for this exercise. We will introduce the minimal concepts needed as we go along.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr起源于生物信息学领域，我们将使用一个生物信息学示例。我们将使用来自一个名为HapMap的旧基因组项目的数据（[https://www.genome.gov/10001688/international-hapmap-project](https://www.genome.gov/10001688/international-hapmap-project)）。该项目为人类群体中的许多个体的基因组变异（DNA字母的变化）提供了信息。你不需要了解这个练习中的任何科学细节。我们将随着我们的进展介绍所需的最小概念。
- en: For our example, we will start with a pre-prepared Zarr database that I generated
    from HapMap data in Plink format ([https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)).
    You don’t need to worry about the original format, but if you are interested and
    for completion, you can find the code that generates the Zarr database that you
    should use in the repository in `08-persistence/sec4-zarr/hapmap` . The pre-prepared
    Zarr file can be found at [https://tiago.org/db.zarr.tar.gz](https://tiago.org/db.zarr.tar.gz).
    It includes genetic information of 210 individuals across several human populations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将从一个预先准备好的Zarr数据库开始，该数据库是我从Plink格式的HapMap数据生成的（[https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/)）。你不需要担心原始格式，但如果你对它感兴趣并且为了完整性，你可以在`08-persistence/sec4-zarr/hapmap`
    仓库中找到生成你应使用的Zarr数据库的代码。预先准备好的Zarr文件可以在[https://tiago.org/db.zarr.tar.gz](https://tiago.org/db.zarr.tar.gz)找到。它包括跨越几个人类群体的210个个体的遗传信息。
- en: One of our objectives will be to generate another Zarr database that can be
    used to perform principal components analysis (PCA)—a unsupervised machine learning
    technique commom in genomics—which will require reformatting the data that we
    have from the original database. We will not run a PCA here but just prepare the
    file for that.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的一个目标将是生成另一个Zarr数据库，可以用于执行主成分分析（PCA）——在基因组学中常见的无监督机器学习技术——这将需要重新格式化我们从原始数据库中拥有的数据。我们不会在这里运行PCA，但只是为该操作准备文件。
- en: 8.4.1 Understanding Zarr’s internal structure
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 理解Zarr的内部结构
- en: 'Let’s start by seeing what is inside the database. While we traverse the database,
    we will remind ourselves of the necessary genomic concepts involved:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看数据库中有什么。在我们遍历数据库的同时，我们将提醒自己涉及的必要基因组概念：
- en: '[PRE34]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Prints the tree structure of the file contents
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ① 打印文件内容的树形结构
- en: 'Zarr is a tree container for arrays, so we have a directory structure where
    the leaf nodes are arrays. An abridged version of our file is:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr 是一个用于数组的树形容器，因此我们有一个目录结构，其中叶子节点是数组。我们文件的简化版本如下：
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Data is split in chromosomes, and there is a hierarchy per chromosome. Each
    chromosome has a list of positions genotyped (for which we get the DNA letters)
    in `positions`. The possible alleles (i.e., the DNA letters) per position are
    in the `alleles` array. The main matrix is in `calls`, where for the 210 individuals,
    we have the alleles for each marker. So, as there are 318,558 markers in chromosome
    1, the `calls` matrix will be 318,558 × 210\. For every individual and marker,
    there are two calls, which will be encoded in a single number.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数据按染色体分割，每个染色体都有一个层次结构。每个染色体都有一个基因型位置列表（对于这些位置，我们得到DNA字母），这些位置在 `positions`
    中。每个位置的可能等位基因（即DNA字母）在 `alleles` 数组中。主要矩阵在 `calls` 中，其中对于210个个体，我们有每个标记的等位基因。因此，由于第1个染色体中有318,558个标记，`calls`
    矩阵将是318,558 × 210。对于每个个体和标记，有两个调用，这些调用将用一个单独的数字编码。
- en: Our objective is to create a concatenated matrix of all calls to submit to a
    PCA implementation. Do not worry about the genetics; what matters from our perspective
    is that we have a two-dimensional `calls` matrix with 0/1/2 values coded as unsigned
    integers with 8 bits, and two one-dimensional arrays, one with 64-bit integers
    (`positions`) and the other with strings of a size of up to two characters (`alleles`).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是创建一个所有调用拼接的矩阵，以提交给PCA实现。不要担心遗传学；从我们的角度来看，重要的是我们有一个二维的 `calls` 矩阵，其中的0/1/2值以8位无符号整数编码，以及两个一维数组，一个包含64位整数（`positions`），另一个包含最多两个字符的字符串（`alleles`）。
- en: 'Before we delve into performance-related problems, let’s briefly discuss how
    to traverse Zarr data. We can traverse the whole structure like this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入性能相关的问题之前，让我们简要讨论如何遍历Zarr数据。我们可以这样遍历整个结构：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ① Gets all groups inside a group
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取组内所有组
- en: ② Gets all arrays inside a group
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取组内所有数组
- en: When Zarr reads a file, it returns a `Group` object. The `groups` method will
    return a generator with all the subgroups inside, so we can rely on that to traverse
    a Zarr repository.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当Zarr读取文件时，它返回一个 `Group` 对象。`groups` 方法将返回一个生成器，包含所有子组，因此我们可以依赖它来遍历Zarr存储库。
- en: 'You can also use a simple directory-like nomenclature to access content, which
    depends on your subjective preference. For example:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用类似目录的简单命名法来访问内容，这取决于你的主观偏好。例如：
- en: '[PRE37]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`in_chr_2` will have a `Group` for the key `pos_chr_2`, `calls_chr_2` and `alleles_chr_2`
    will have respective the arrays `chromosome-2/positions`, `chromosome-2/calls`
    and `chromosome-2/alleles` ready for use.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`in_chr_2` 将有一个名为 `pos_chr_2` 的 `Group`，`calls_chr_2` 和 `alleles_chr_2` 分别有相应的数组
    `chromosome-2/positions`、`chromosome-2/calls` 和 `chromosome-2/alleles`，准备使用。'
- en: 'Let’s get some information from our data structures:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的数据结构中获取一些信息：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: What we have is a `Group`, containing three members, all of which happen to
    be arrays; there could also be subgroups inside the name.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个 `Group`，包含三个成员，它们恰好都是数组；名称内部也可能有子组。
- en: 'Zarr supports many types of stores: in our case, we are using `zarr.storage
    .DirectoryStore`, but you can find classes for in-memory, zip files, DBM files,
    SQL, fsspec, Mongo, and so on.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr支持许多类型的存储：在我们的例子中，我们使用 `zarr.storage .DirectoryStore`，但你也可以找到内存、zip文件、DBM文件、SQL、fsspec、Mongo等类的实现。
- en: 'As we will see shortly, `DirectoryStore` is very helpful in supporting advanced
    parallel features, but for now, let’s look at the directory structure that it
    uses. In case you haven’t noticed, `db.zarr` is not a file but a directory. The
    following code snippet is an abridged version of the directory structure:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们很快就会看到的，`DirectoryStore`在支持高级并行功能方面非常有帮助，但就目前而言，让我们看看它使用的目录结构。如果你还没有注意到，`db.zarr`不是一个文件，而是一个目录。以下代码片段是目录结构的简化版本：
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The directory structure mimics the Zarr group structure, which makes it easy
    for development.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 目录结构模仿了Zarr组结构，这使得开发变得容易。
- en: 8.4.2 Storage of arrays in Zarr
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 Zarr中数组的存储
- en: 'Let’s now discuss how arrays are stored, a substantially more complex and interesting
    subject:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论数组是如何存储的，这是一个更加复杂和有趣的主题：
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we will reorder the output and split it into several parts. Let’s start
    with some basic information:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重新排序输出并将其分成几个部分。让我们从一些基本信息开始：
- en: '[PRE42]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You should be able to interpret this information with what we studied in previous
    chapters: the object is a `zarr.core.Array`, the data type is a 64-bit integer
    with 333,056 numbers, and the array is C-ordered and can be written on.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够用我们在前几章学到的知识来解释这条信息：这个对象是一个 `zarr.core.Array`，数据类型是64位整数，包含333,056个数字，数组是C顺序的，并且可以写入。
- en: 'Let’s now look at chunk shape:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看块形状：
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Remember that chunking is a way to partition a big array into smaller equal
    parts (chunks) that can be more easily manipulated (figure 8.2).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，分块是将一个大数组分割成更小的相等部分（块）的一种方式，这样就可以更容易地操作（图8.2）。
- en: '![](../Images/CH08_F02_Antao.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F02_Antao.png)'
- en: Figure 8.2 A large array file can be partitioned into equally sized chunks to
    be processed separately.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 一个大数组文件可以被分割成等大小的块以分别处理。
- en: Zarr is telling us that each chunk is 41,632 elements in size; hence, we end
    up with eight chunks to accommodate 333,056 elements. When we created the array
    in the support script to create the pre-prepared version, we were a bit naive
    and did not specify the chunk size, and as such, Zarr tried to guess a reasonable
    value. Chunk size can—and should—be specified on creation. We will see why later
    in the section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Zarr告诉我们每个块的大小是41,632个元素；因此，我们最终得到八个块来容纳333,056个元素。当我们创建支持脚本来创建预准备版本中的数组时，我们有点天真，没有指定块大小，因此Zarr尝试猜测一个合理的值。块大小可以在创建时指定。我们将在后面的部分中看到原因。
- en: 'Note that all chunks are also initialized: however, under some circumstances,
    not all chunks need to be initialized (e.g., an empty array). Uninitialized chunks
    can potentially save a lot of disk space. Again, we will see that when we create
    arrays later.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有块都已初始化：然而，在某些情况下，并非所有块都需要初始化（例如，空数组）。未初始化的块可以潜在地节省大量磁盘空间。我们将在创建数组时看到这一点。
- en: If you go into the directory `db.zarr/chromosome-2/positions`, you will find
    eight files, named from `0` to `7`; this is a file per chunk. This separation
    will make concurrent writes—a sophisticated feature not found in many array storage
    systems—easier with Zarr.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进入`db.zarr/chromosome-2/positions`目录，你会找到八个文件，分别命名为`0`到`7`；这是一个块对应的文件。这种分离将使得在Zarr中实现并发写入——这是一个在许多数组存储系统中找不到的复杂特性——变得更加容易。
- en: 'Finally, Zarr arrays can be compressed, thus saving a lot of disk space and
    potentially processing time, as discussed earlier in the chapter. Here is part
    of the output describing this:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Zarr数组可以被压缩，从而节省大量磁盘空间和潜在的处理时间，正如本章前面所讨论的。以下是描述这一部分的输出：
- en: '[PRE44]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In our case, data was stored using Blosc with the LZ4 algorithm. The original
    size is 2,664,448 bytes—333,056 elements multiplied by 8 bytes for 64-bit integers,
    for a final storage of 687,723 bytes, thus a compression of 3.9 times. Given that
    the arrays are of homogeneous type, we should expect, on average, that compression
    will outperform the overall compression of heterogeneous data frames. Of course,
    this expectation is for an average case; for example, a random array is very difficult
    to compress.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，数据使用Blosc和LZ4算法存储。原始大小是2,664,448字节——333,056个元素乘以8字节用于64位整数，最终存储为687,723字节，因此压缩了3.9倍。鉴于数组是同质的，我们应该期望，平均而言，压缩将优于异构数据帧的整体压缩。当然，这个期望是针对平均情况；例如，随机数组非常难以压缩。
- en: 'For the `calls` array, we have a similar output but adapted to two dimensions.
    Here is an abridged version of `print(calls_chr_2.info)`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`calls`数组，我们有类似的输出，但适应了二维。以下是`print(calls_chr_2.info)`的简化版本：
- en: '[PRE45]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In this case, we have a matrix of dimension 333,056 × 210 and two-dimensional
    chunks.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有一个333,056 × 210维度的矩阵和二维块。
- en: Tip You can chunk an N-dimensional array in fewer dimensions than N. For example,
    our two-dimensional array can be chunked only over one dimension. This choice
    may make sense if you need to process all information over one dimension at the
    same time. As with all chunking decisions, it depends on your use case.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：你可以将N维数组分成少于N维的维度。例如，我们的二维数组可以只在一个维度上分块。如果你需要同时处理一个维度上的所有信息，这个选择可能是有意义的。与所有分块决策一样，这取决于你的用例。
- en: Each dimension is split into eight intervals for a total of 64 chunks. If you
    list the contents of `db.zarr/chromosome-2/calls`, you will find 64 files conveniently
    named X.Y, where X and Y vary from 0 to 7, which refers to the chunk number on
    each dimension.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 每个维度被分成八个区间，总共64个块。如果你列出`db.zarr/chromosome-2/calls`的内容，你会找到64个方便命名的文件X.Y，其中X和Y从0到7变化，这指的是每个维度上的块编号。
- en: 'Finally, we have an array for all alleles, which is a string with two characters
    (e.g., AT, CG, TC, etc.). The abridged output from `print(alleles_chr_2.info)`
    is:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一个包含所有等位基因的数组，它是一个由两个字符组成的字符串（例如，AT、CG、TC等）。`print(alleles_chr_2.info)`的简化输出如下：
- en: '[PRE46]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This output is a Unicode string of fixed two-byte size. Remember from chapter
    2 that Python string representation is sophisticated—or cumbersome, depending
    on the point of view—and assessing the size in bytes of a Python string is far
    from trivial.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出是一个固定两字节大小的Unicode字符串。记得在第二章中提到，Python字符串表示是复杂的——或者根据观点的不同，可能是繁琐的——评估Python字符串的字节数远非易事。
- en: 'For efficient access, it helps if we have strings of a fixed size and a representation
    that is predictable in size. Zarr provides two built-in representations for strings:
    if you only have ASCII characters, you can use a byte array, and if you have more
    than ASCII characters, Zarr provides a fixed-size Unicode representation, as opposed
    to the variable-size Python string implementation. If you need variable length
    strings and different encodings, Zarr provides encoders for those as well, but
    be careful about the performance implications of that much flexibility; if possible
    and reasonable in terms of storage, allocate a fixed long string.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效访问，如果我们有固定大小的字符串和可预测大小的表示，那就很有帮助。Zarr提供了两种内置的字符串表示：如果你只有ASCII字符，你可以使用字节数组；如果你有超过ASCII字符的情况，Zarr提供了一个固定大小的Unicode表示，这与可变大小的Python字符串实现相反。如果你需要可变长度的字符串和不同的编码，Zarr也提供了相应的编码器，但要注意这种灵活性的性能影响；如果可能且在存储方面合理，分配一个固定长度的长字符串。
- en: Now that we have an overview of how Zarr data is organized, let’s create an
    array that is the concatenation of all positions across all chromosomes. We need
    a single matrix from all chromosomes, as PCA requires a single matrix as input.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Zarr数据的组织方式，让我们创建一个数组，它是所有染色体上所有位置的连接。我们需要一个来自所有染色体的单个矩阵，因为PCA需要一个单独的矩阵作为输入。
- en: 8.4.3 Creating a new array
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 创建新数组
- en: We are now going to create a new array that can be used for unsupervised learning
    algorithms like PCA. This is simply a concatenation of all the call arrays (i.e.,
    the calls for all chromosomes).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个新的数组，它可以用于像PCA这样的无监督学习算法。这仅仅是所有调用数组（即所有染色体的调用）的连接。
- en: 'Before we can start, we must know the size of the array we need to allocate.
    To learn this, we traverse the existing Zarr file to extract the number of markers
    per chromosome, which will differ depending on your concrete problem:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们必须知道我们需要分配的数组的大小。为了了解这一点，我们遍历现有的Zarr文件以提取每个染色体上的标记数量，这取决于你的具体问题：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This code simply checks the first dimension of all the one-dimensional arrays
    for positions. With this information, we can calculate the size of the all-encompassing
    Zarr array.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码只是检查所有一维数组的第一维以确定位置。有了这些信息，我们可以计算包含所有数据的Zarr数组的大小。
- en: 'With the total size in hand, we can allocate the array:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了总大小之后，我们可以分配数组：
- en: '[PRE48]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ① 210 is the number of individuals in our dataset.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ① 210是我们数据集中个体的数量。
- en: The most important parameter in terms of performance is the chunk size. We chose
    a value that will put us above 1 MB per chunk, although you will have to calibrate
    chunk size for your specific case. The total of 20,000 × 210 is around 4 MB, but
    we are counting with some compression. We are assuming that all individuals will
    be read at once, so we chunk only on a single dimension. Feel free to vary the
    chunk size, you will see clear performance differences.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面最重要的参数是块大小。我们选择了一个值，使得每个块的大小超过1 MB，尽管你可能需要根据你的具体情况调整块大小。20,000乘以210的总数大约是4
    MB，但我们预计会有一些压缩。我们假设所有个体将一次性读取，所以我们只在单个维度上进行分块。你可以自由地调整块大小，你会看到明显的性能差异。
- en: General ideas to decide on chunk size
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 决定块大小的通用想法
- en: 'It is very difficult to come up with general rules for the chunk size. You
    will need to have a look at your algorithms and use cases. That being said, here
    are some basic rules:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于块大小，很难制定一般性的规则。你需要查看你的算法和用例。话虽如此，这里有一些基本的规则：
- en: You don’t want chunks that are too small; they should typically be at least
    1 MB or bigger.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不希望块太小；它们通常至少应该是1 MB或更大。
- en: Your chunks should comfortably fit in-memory.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的块应该能够轻松地适应内存。
- en: Try different values across different dimensions. These can have important effects
    on performance and on your ability to do all work in-memory.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同维度上尝试不同的值。这些值可能会对性能和你在内存中完成所有工作的能力产生重要影响。
- en: Chunk size and type of storage are not orthogonal. For example, `DirectoryStore`
    will not scale well with thousand of chunks because of filesystem performance
    problems with too many files in the same directory. In this case, Zarr offers
    `NestedDirectoryStore` to spread chunks over subdirectories. But the important
    point is, it’s better if you understand the limitations of different stores and
    parameterize chunking accordingly.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块大小和存储类型不是正交的。例如，`DirectoryStore`由于同一目录中文件过多导致的文件系统性能问题，在处理成千上万的块时扩展性不好。在这种情况下，Zarr提供了`NestedDirectoryStore`来将块分散到子目录中。但重要的是，如果你理解不同存储的限制，并根据这些限制来参数化分块，那就更好了。
- en: 'Let’s get the info for `all_calls`. The abridged version is:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取`all_calls`的信息。简而言之是：
- en: '[PRE49]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The most important issue to note is the number of bytes stored and, relatedly,
    the number of chunks initialized. While the total expected size is 796.4 MB, only
    345 bytes (!) are in use because no data has been saved (i.e., no chunk is initialized).
    By default, Zarr assumes all values in the array are 0 if not initialized. If,
    at this stage, you list the `all_ calls.zarr` directory, you will find that it
    is empty and that it occupies no space at all.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的最重要的问题是存储的字节数，以及与之相关的初始化的块数量。虽然预期的总大小是796.4 MB，但实际使用中只有345字节（！）因为还没有保存数据（即没有初始化任何块）。默认情况下，Zarr假设如果未初始化，数组中的所有值都是0。如果在当前阶段列出`all_calls.zarr`目录，你会发现它是空的，并且它根本不占用任何空间。
- en: Actually, a hidden file, called `.zarray`, has some metadata. If you open the
    file, you will find a file with a JSON version of the parameters that we passed
    to the Zarr array creation along with other defaults.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，一个名为`.zarray`的隐藏文件包含一些元数据。如果您打开该文件，您将找到一个包含我们传递给Zarr数组创建的参数的JSON版本以及其他默认值的文件。
- en: 8.4.4 Parallel reading and writing of Zarr arrays
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.4 Zarr数组的并行读写
- en: Now let’s create the single concatenated array. We require a single array with
    all data for PCA analysis.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个单一的连接数组。我们需要一个包含所有PCA分析数据的单个数组。
- en: 'We will discuss two versions: the first is a sequential version and the second,
    a parallel one. Here is the first:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论两个版本：第一个是顺序版本，第二个是并行版本。以下是第一个：
- en: '[PRE50]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This code simply copies all chromosome calls in sequence to the `all_calls`
    array. Note that all the storage management is completely abstracted on top of
    a typical NumPy interface.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码只是将所有染色体调用按顺序复制到`all_calls`数组中。请注意，所有存储管理都是在典型的NumPy接口之上完全抽象的。
- en: 'After you run the code, if you print the info for `all_calls`, few changes
    occur:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在您运行代码后，如果您打印`all_calls`的信息，几乎没有变化：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now that all chunks are initialized, the store occupies 283.3 MB—a storage
    ratio of 2.8 compared to 796.4 MB for the total number of bytes. If you list the
    `all_calls.zarr` directory, you will find 199 files: one for each chunk.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有块都已初始化，存储占用283.3 MB——与总字节数796.4 MB相比，存储比为2.8。如果您列出`all_calls.zarr`目录，您将找到199个文件：每个块一个。
- en: The previous code takes a few seconds to run. While I won’t ask you to run an
    example with many terabytes of data that will take hours, it is easy to see that,
    for more data, the time to do such a conversion could become prohibitively long.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码运行需要几秒钟。虽然我不会要求您运行一个包含数以TB计的数据的示例，这将花费数小时，但很容易看出，对于更多数据，进行此类转换所需的时间可能会变得过长。
- en: So, as a second version, we will create a parallel version that will read from
    the chromosome arrays and write to the `all_calls` array. Both reads *and* writes
    will be parallel.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为第二个版本，我们将创建一个并行版本，该版本将从染色体数组中读取并将写入`all_calls`数组。读取和写入都将并行进行。
- en: Not many libraries support parallel writing, but Zarr does. By putting each
    chunk in a separate file, the directory store makes it easy for Zarr to implement
    parallel writes. In this case, a simple design based on filesystem performance
    properties opens the possibility for a very important feature.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 并非许多库支持并行写入，但Zarr支持。通过将每个块放入单独的文件中，目录存储使得Zarr实现并行写入变得容易。在这种情况下，基于文件系统性能属性的一个简单设计为一个非常重要的特性打开了可能性。
- en: In theory, it is possible to write in whatever size you prefer, but doing this
    chunk by chunk will be the most efficient, as Zarr will not have to deal with
    concurrent writes on the same file. The fundamental point is that your chunk size
    should align with your use cases and you should try to process data on a chunk-by-chunk
    basis if possible.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '理论上，您可以使用您喜欢的任何大小进行写入，但按块进行写入将是最有效的，因为Zarr将不必处理同一文件上的并发写入。基本点是您的块大小应与您的用例相匹配，如果可能的话，您应该尝试按块处理数据。 '
- en: 'In our case, we cannot go simply chromosome by chromosome; we have to write
    chunk by chunk. Here is the function to write a chunk in general:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们不能简单地按染色体进行；我们必须按块写入。以下是写入块的一般函数：
- en: '[PRE52]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: ① The first write position is the chunk number times the chunk size.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ① 首次写入位置是块号乘以块大小。
- en: ② We traverse all chromosome sizes until we find where to start.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们遍历所有染色体大小，直到找到开始的位置。
- en: ③ A chunk might require more than one chromosome.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一个块可能需要多个染色体。
- en: 'Do not stress if you do not understand the previous code in its entirety: the
    code is specific to the domain. What matters is the general approach. We are trying
    to work in a chunk-based way, which is appropriate for large files that do not
    fit memory.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不完全理解之前的代码，请不要担心：该代码特定于领域。重要的是一般方法。我们正在尝试以块为基础的方式工作，这对于不适合内存的大文件是合适的。
- en: 'We can now use a simple multiprocessing pool with a map call to process each
    chunk:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用一个简单的多进程池和一个映射调用来处理每个块：
- en: '[PRE53]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We do a partial function application by defining `partial_process_chunk` so
    that the `Pool.map` call is easier. We then use a multiprocessing pool to process
    our map; for more details, see chapter 3\.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义`partial_process_chunk`进行部分函数应用，以便`Pool.map`调用更容易。然后我们使用多进程池来处理我们的映射；更多细节，请参阅第3章。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: fsspec works as a unified interface for file storage, allowing for the same
    API to be used across many different backends.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fsspec 作为文件存储的统一接口，允许使用相同的 API 在许多不同的后端之间进行操作。
- en: Because there is a unified API with fsspec, it is substantially easier to replace
    backends.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于与 fsspec 有统一的 API，替换后端的工作变得大大简化。
- en: While fsspec is not directly related to performance, several advanced libraries
    make use of it, including Arrow and Zarr.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 fsspec 与性能没有直接关系，但几个高级库都使用了它，包括 Arrow 和 Zarr。
- en: 'Parquet is a columnar data format that allows for more efficient storage of
    data: data is typed, potentially compressed, and organized by column.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 是一种列式数据格式，它允许更有效地存储数据：数据被类型化，可能被压缩，并且按列组织。
- en: Parquet uses sophisticated strategies for data encoding, like dictionaries or
    run-length encoding, allowing for very compact representations, especially of
    data with clear patterns and repetitions. Furthermore, the format is extensible,
    and there may be even more performance enhancements in the future.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 使用复杂的数据编码策略，如字典或运行长度编码，允许非常紧凑的表示，特别是对于具有明显模式和重复的数据。此外，该格式是可扩展的，未来可能会有更多的性能提升。
- en: Parquet allows for data partitioning, which gives programmers the ability to
    process data in parallel.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 允许数据分区，这为程序员提供了并行处理数据的能力。
- en: The most common technique for dealing with larger-than-memory files is chunking.
    Chunking is supported in many libraries, including pandas, Parquet, and Zarr.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理大于内存的文件最常见的技术是分块。pandas、Parquet 和 Zarr 等许多库都支持分块。
- en: Zarr is a modern library to process homogeneous multidimensional arrays. It
    originated in the Python world and provides NumPy-based interfaces.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zarr 是一个用于处理同构多维数组的现代库。它起源于 Python 世界，并提供了基于 NumPy 的接口。
- en: Zarr supports parallelism out of the box. Support for a concurrent writing process
    is of note as it is an uncommon feature in other libraries.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zarr 默认支持并行性。支持并发写入过程是一个值得注意的特点，因为在其他库中这是不常见的功能。
