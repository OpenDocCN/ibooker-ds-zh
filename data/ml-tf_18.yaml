- en: '15 Building a real-world CNN: VGG -Face and VGG -Face Lite'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 构建真实世界的 CNN：VGG-Face 和 VGG-Face Lite
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Augmenting data for training a convolution neural network (CNN)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练卷积神经网络（CNN）增强数据
- en: Tuning a CNN by using dropout and batch normalization and evaluating performance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用 dropout 和批量归一化调整 CNN 并评估性能
- en: Building an accurate CNN for object recognition with CIFAR-10 and facial identification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CIFAR-10 和面部识别构建准确的 CNN
- en: Convolutional neural network (CNN) architectures are useful tools for analyzing
    images and for differentiating their features. Lines or curves may indicate your
    favorite automobile, or the indicator might be a particular higher-level feature,
    such as the green coloring present in most frog pictures. More complex indicators
    might be a freckle near your left nostril or the curvature of your chin passed
    down through generations of your family.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）架构是分析图像和区分其特征的有用工具。线条或曲线可能表明你最喜欢的汽车，或者指示可能是一个特定的更高阶特征，例如大多数青蛙图片中存在的绿色。更复杂的指示可能是在你左鼻孔附近的一个雀斑或你家族几代人传下来的下巴曲线。
- en: Humans have become adept through the years at picking out these identifying
    features, and it’s fine to wonder why. Humans have grown accustomed to looking
    at billions of example images shown to them since birth and then receiving feedback
    about what they are seeing in those images. Remember your mom repeating the word
    ball while showing you a ball? There’s a good chance that you remember some time
    she said it. What about the time you saw another ball of a slightly different
    shape or color and said, “Ball”?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在多年中已经熟练地挑选出这些识别特征，并且好奇为什么是如此。人类已经习惯于从出生以来就观察数亿个展示给他们的示例图像，并接收关于他们在那些图像中看到内容的反馈。记得你妈妈在给你展示球的时候重复“球”这个词吗？你很可能记得她说过这个词的某个时刻。那么，当你看到另一个形状或颜色略有不同的球时说“球”又是怎么回事呢？
- en: Perhaps the toddler version of yourself, upon being handed a new action figure
    of a caped superhero, asked whether that figure was Superman. It wasn’t, but it
    was another hero that looked similar. Why did you think it was Superman? The cape,
    the dark hair, and a possible triangle near the chest with some sort of symbol
    in it were image features that looked familiar. Your biological neural network
    fired with the input image, retrieving the label that was reinforced over time
    when your parents provided it to you verbally. When you were handed a new toy,
    you spit out the label, and your parents corrected you. (“No, honey, it’s Shazam.
    It looks like Superman, though; I can totally understand why you thought that!”)
    Boom—a label was added based on the slightly different features, and you moved
    on to learning more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是你小时候，当你被 handed 一个新的带有披风的超级英雄动作人偶时，问这个人物是不是超人。它不是，但它是另一个看起来相似的英雄。你为什么认为它是超人呢？披风、深色的头发，以及胸部附近可能带有某种符号的三角形是看起来熟悉的图像特征。你的生物神经网络在输入图像时被激活，检索出当你的父母口头提供时经过时间强化的标签。当你被
    handed 一个新的玩具时，你脱口而出标签，你的父母纠正了你。（“不，宝贝，它是沙赞。它看起来像超人，但我完全理解你为什么那么想！”）砰——基于略有不同的特征添加了一个标签，然后你继续学习。
- en: This process is similar to the way you train CNNs, which allow a computer program
    to capture higher- and lower-order features automatically and use them to differentiate
    among images. As you learned in chapter 14, those features are represented by
    convolutional filters, which are the learned parameters of the network.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程类似于你训练 CNN 的方式，它允许计算机程序自动捕捉更高和更低阶的特征，并使用这些特征来区分图像。正如你在第 14 章所学，这些特征由卷积滤波器表示，它们是网络的学到的参数。
- en: 'Training CNNs to learn those parameters is no easy task due to a variety of
    factors:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各种因素，训练 CNN 来学习这些参数并非易事：
- en: '*Accessibility of training data* —To be realistic and accurate, a CNN needs
    boatloads of training data with lots of feature variations so that the filters
    can capture them. Likewise, lots of filters are required to represent those features.
    As a human, you’ve probably seen billions of images in your lifetime, with lots
    of repetition of classes, colors, objects, and people. Your label assignments
    get better over time, and so does the CNN.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练数据的可访问性* ——为了真实和准确，CNN 需要大量的训练数据，具有许多特征变化，以便过滤器可以捕捉它们。同样，需要大量的过滤器来表示这些特征。作为一个人类，你可能在一生中看到了数十亿张图片，其中有很多类、颜色、物体和人的重复。你的标签分配会随着时间的推移而变得更好，CNN
    也是如此。'
- en: '*Deeper neural architectures* *and more feature delineation* —These architectures
    help CNNs differentiate among images that have great parity. If your CNN has learned
    what a human is versus a bird, how does it separate different kinds of humans,
    such as those with wavy or curly hair, or with fair or tan skin? Deeper architectures
    require your network to learn more parameters, but it has more capacity to represent
    the feature variations and, as such, will take much longer to train.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更深的神经网络架构* *和更多特征细化* ——这些架构帮助CNN区分具有高度相似性的图像。如果你的CNN已经学会了人类与鸟类的区别，那么它是如何区分不同种类的人类，比如那些有波浪卷发或卷发，或者有白皙或棕褐色皮肤的人呢？更深的架构需要你的网络学习更多的参数，但它有更多的能力来表示特征变化，因此训练时间会更长。'
- en: '*Preventing memorization* *and learning more resilient representations* —Training
    a robust CNN means preventing the network from memorizing features for objects
    or people from the training data. This process also ensures that your network
    is open to new interpretations of objects and people in those images that have
    slightly different representations of those features that it may see in the wild,
    breaking its simply trained understanding.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*防止记忆化* *和学习更具弹性的表示* ——训练一个鲁棒的CNN意味着防止网络从训练数据中记住物体或人的特征。这个过程也确保了你的网络对新解释图像中物体和人的开放性，这些图像具有与它在野外可能看到的那些特征略有不同的表示，打破了它简单训练的理解。'
- en: All these issues are required to make a CNN useful tools for solving problems
    in the real world, as you see in figure 15.1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题都是使卷积神经网络（CNN）成为解决现实世界问题有用工具所必需的，正如你在图15.1中看到的。
- en: '![CH15_F01_Mattmann2](../Images/CH15_F01_Mattmann2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F01_Mattmann2](../Images/CH15_F01_Mattmann2.png)'
- en: Figure 15.1 CNN architectures help a machine-learning algorithm label an image.
    Whether you are trying to label an object or someone’s face, the same CNN architectures
    can perform the task.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 CNN架构帮助机器学习算法对图像进行标记。无论你是在尝试标记一个物体还是某人的脸，相同的CNN架构都可以完成这项任务。
- en: 'In this chapter, I’ll show you how to build resilient CNN architectures by
    using TensorFlow, beginning with something you’re familiar with: the Canadian
    Institute for Advanced Research (CIFAR-10) dataset of automobiles, planes, ships,
    birds, and so on. This dataset is representative enough of the real world that
    to make them accurate, you need to make the optimizations in the basic CNN architectures
    that I showed you in chapter 14.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将向你展示如何使用TensorFlow构建鲁棒的CNN架构，从你熟悉的内容开始：加拿大高级研究研究所（CIFAR-10）的数据集，包括汽车、飞机、船只、鸟类等。这个数据集足以代表现实世界，为了使它们准确，你需要对我在第14章中展示的基本CNN架构进行优化。
- en: Additionally, you will build a facial-detection CNN system by using the Visual
    Geometry Group (VGG) Face model. That CNN, when given one of 2,622 possible celebrity
    faces, will identify with high accuracy which celebrity the face belongs to. The
    CNN can even handle different poses, lighting, makeup (or no makeup), glasses
    (or no glasses), hats (or no hats), and loads of other properties in the images.
    Let’s get started building real-world CNNs!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将使用视觉几何组（VGG）Face模型构建一个面部检测CNN系统。当给定2,622个可能的名人面孔之一时，该CNN将能够以高精度识别该面孔属于哪位名人。该CNN甚至可以处理不同的姿势、光照、化妆（或无化妆）、眼镜（或无眼镜）、帽子（或无帽子）以及图像中的大量其他属性。让我们开始构建现实世界的CNN吧！
- en: 15.1 Making a real-world CNN architecture for CIFAR-10
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 为CIFAR-10构建现实世界的CNN架构
- en: The CIFAR-10 dataset should be familiar because you used it in chapters 11 and
    12\. It contains 50,000 images for training, representing 5,000 images per each
    of 10 classes of object—`airplane`, `automobile`, `bird`, `cat`, `deer`, `dog`,
    `frog`, `horse`, `ship`, and `truck`—and 10,000 test images (1,000 per class).
    In chapter 14, I showed you how to construct a shallow network CNN with a few
    layers to classify CIFAR-10 images into one of the 10 classes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10数据集你应该很熟悉，因为你已经在第11章和第12章中使用过它。它包含50,000张用于训练的图像，代表10个类别中的每个类别有5,000张图像——`飞机`、`汽车`、`鸟`、`猫`、`鹿`、`狗`、`青蛙`、`马`、`船`和`卡车`——以及10,000张测试图像（每个类别1,000张）。在第14章中，我向你展示了如何构建一个具有几层的浅层网络CNN，以将CIFAR-10图像分类到10个类别之一。
- en: The CIFAR-10 dataset was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey
    Hinton. Krizhevsky is the author of a seminal paper on CNNs, “ImageNet Classification
    with Deep Convolutional Neural Networks” ([http://www.cs.toronto.edu/~hinton/
    absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)). The
    paper, cited more than 40,000 times, proposes what became known as AlexNet, which
    is Krizhevsky’s famous CNN architecture for image processing named after him.
    In addition to being used for CIFAR-10, AlexNet was used to win the ImageNet 2012
    challenge. (ImageNet is a corpus of millions of images labeled with the WordNet
    taxonomy with 1,000 classes of objects; CIFAR-10 is the subset of that with 10
    object classes.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10数据集是由Alex Krizhevsky、Vinod Nair和Geoffrey Hinton收集的。Krizhevsky是关于CNNs的开创性论文《使用深度卷积神经网络进行ImageNet分类》（[http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)）的作者。这篇被引用超过40,000次的论文提出了后来被称为AlexNet的概念，这是Krizhevsky以其名字命名的用于图像处理的著名CNN架构。除了用于CIFAR-10之外，AlexNet还被用于赢得2012年ImageNet挑战赛。（ImageNet是一个包含数百万张图像的语料库，这些图像被WordNet分类法标记，有1,000个物体类别；CIFAR-10是其中包含10个物体类别的子集。）
- en: 'AlexNet employs several important optimizations beyond the CNNs you constructed
    in chapter 14\. In particular, it proposes a deeper architecture with many more
    convolutional filters to capture higher- and lower-order features (figure 15.2).
    Those optimizations include the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet在CNNs之外采用了几个重要的优化。特别是，它提出了一种更深层的架构，具有更多的卷积滤波器来捕获更高和更低阶的特征（图15.2）。这些优化包括以下内容：
- en: Deeper architectures and more convolutional layers with associated filters and
    capacity
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更深的架构和更多具有相关滤波器和容量的卷积层
- en: The use of data augmentation (rotating an image, flipping it left and right,
    or randomly cropping it)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据增强（旋转图像、左右翻转或随机裁剪图像）
- en: The application of a technique called dropout, which randomly turns off neurons
    in a particular layer so that the architecture learns a more resilient representation
    of the input
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术称为dropout的应用，该技术随机关闭特定层的神经元，从而使架构学习到对输入的更鲁棒的表达
- en: '![CH15_F02_Mattmann2](../Images/CH15_F02_Mattmann2.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F02_Mattmann2](../Images/CH15_F02_Mattmann2.png)'
- en: Figure 15.2 The famous AlexNet CNN architecture for object classification on
    the CIFAR data
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 在CIFAR数据上用于物体分类的著名AlexNet CNN架构
- en: You can implement these optimizations starting from the shallow CIFAR-10 CNN
    you created in chapter 14\. To begin, review the read data functions that handle
    loading the CIFAR-10 50,000 training images; then convert them to grayscale to
    reduce the number of learned parameters. Don’t worry; you’ll deal with color images
    a bit later in the chapter when you build a facial recognition CNN.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从第14章中创建的浅层CIFAR-10 CNN开始实现这些优化。首先，回顾处理加载CIFAR-10 50,000个训练图像的读取数据函数；然后将它们转换为灰度以减少学习参数的数量。不用担心；你将在本章稍后构建面部识别CNN时处理彩色图像。
- en: 15.1.1 Loading and preparing the CIFAR-10 image data
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 加载和准备CIFAR-10图像数据
- en: 'CIFAR-10 data is in Python Pickle format, so you’ll need functions to read
    that pickle format back into Python dictionaries—one for the 32 × 32 × 50000 image
    data and another for the 1 × 50000 labels. Additionally, you’ll need to prepare
    and clean the data up a bit, a task that I want to emphasize strongly. You’ll
    do the following things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10数据是Python Pickle格式，因此你需要函数将那种pickle格式读回到Python字典中——一个用于32 × 32 × 50000个图像数据，另一个用于1
    × 50000个标签。此外，你还需要准备和清理数据，这是一个我想强烈强调的任务。你将执行以下操作：
- en: Create a function to clean the data and normalize the image variance by dividing
    by the mean image values.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来清理数据并通过除以平均图像值来归一化图像方差。
- en: Crop the images to the centers, and reduce background noise.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像裁剪到中心，并减少背景噪声。
- en: Convert the images to grayscale to reduce the dimensionality and increase learning
    efficiency in the network.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度以减少网络中的维度并提高学习效率。
- en: You’ll copy those functions forward in listing 15.1 and get your images and
    labels ready for training. Note that these optimizations ease learning and training.
    You could omit doing some of them, but your training might take forever to converge
    or perhaps would never converge quickly to optimal learned parameters. You perform
    this cleaning to aid the learning process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在列表15.1中复制这些函数，并准备好你的图像和标签以进行训练。请注意，这些优化简化了学习和训练过程。你可以省略其中的一些，但你的训练可能需要无限期地收敛，或者可能永远不会快速收敛到最优的学习参数。你进行这种清理是为了帮助学习过程。
- en: Listing 15.1 Loading and preparing CIFAR-10 training images and labels
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.1 加载和准备CIFAR-10训练图像和标签
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Loads the pickled dictionary
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载序列化的字典
- en: ❷ The grayscale image is the mean of the R,G,B axes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 灰度图像是R、G、B轴的平均值
- en: ❸ Crops the images
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 裁剪图像
- en: ❹ Subtracts the image means and divides by the standard deviation so that the
    images are not too sensitive to high variation, and eases learning
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过减去图像均值并除以标准差，使得图像对高变异性不敏感，从而简化学习过程
- en: The data-reading function saves the existing image data and labels if you’ve
    loaded them before by using NumPy’s `npy` compact binary format to store NumPy
    arrays. This way, after you’ve processed, cleaned, and loaded the images and their
    labels, you don’t have to bother waiting for those functions to complete again
    before training, because they can take quite a while to complete.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据读取函数使用NumPy的`npy`紧凑二进制格式存储NumPy数组，如果你之前已经加载了图像数据和标签，它会保存现有的图像数据和标签。这样，在你处理、清理和加载图像及其标签之后，你不必再等待这些函数完成，因为它们可能需要相当长的时间才能完成。
- en: But I’ve got a GPU!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但我有一个GPU！
- en: Graphical processing units (GPUs) have changed the game in terms of neural networks
    and training them for deep learning tasks such as prediction and classification.
    Optimized over the years for instructions related to graphical processing that
    were originally in video games, the operations that GPUs support—matrix multiplications—have
    found a friend in matrix-hungry machine-learning algorithms. GPUs do not really
    help you in traditional CPU-oriented operations, however, or in operations involving
    disk input and output (I/O) such as loading cached data from disk. The good news?
    Machine-learning frameworks including TensorFlow know which operations to optimize
    for the GPU and which ones are fine for the CPU, and they allocate those operations
    accordingly. Don’t get too excited about your GPU for the data preparation part(s),
    though, because you are still I/O-bound.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理单元（GPU）在神经网络及其深度学习任务（如预测和分类）的训练方面改变了游戏规则。经过多年的优化，GPU支持与图形处理相关的指令，这些指令最初用于视频游戏，GPU支持的运算——矩阵乘法——在矩阵需求量大的机器学习算法中找到了盟友。然而，GPU并不真正帮助你在传统的以CPU为中心的操作中，或者在涉及磁盘输入输出（I/O）的操作中，例如从磁盘加载缓存数据。好消息是，包括TensorFlow在内的机器学习框架知道哪些操作需要为GPU优化，哪些操作适合CPU，并且相应地分配这些操作。不过，对于数据准备部分，不要过于兴奋你的GPU，因为你仍然受限于I/O。
- en: Listing 15.2 loads the image data and labels for CIFAR-10\. Note that it also
    includes the `augment` function, which I’ll discuss in section 15.1.2.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.2加载了CIFAR-10的图像数据和标签。请注意，它还包括`augment`函数，我将在第15.1.2节中讨论。
- en: Listing 15.2 Loading CIFAR-10 image data and labels
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.2 加载CIFAR-10图像数据和标签
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Caches the NumPy arrays for image data and labels using these filenames or
    loads them if the files do not exist
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用这些文件名缓存图像数据和标签的NumPy数组，如果文件不存在则加载它们
- en: ❷ Performs data augmentation as shown in section 15.2
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如第15.2节所示执行数据增强
- en: ❸ Cleans the image data by normalizing the image variance and converting to
    grayscale
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过归一化图像方差并将其转换为灰度来清理图像数据
- en: ❹ Saves the resultant loaded data in NPY cache files so that you don’t have
    to recompute them
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将加载的结果数据保存到NPY缓存文件中，这样你就不需要重新计算它们
- en: With the functions built to get the data loading pipeline going, next I’ll explain
    how to handle data augmentation for your CIFAR-10 CNN.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用构建的数据加载管道函数，接下来我将解释如何处理CIFAR-10卷积神经网络（CNN）的数据增强。
- en: 15.1.2 Performing data augmentation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.2 执行数据增强
- en: CIFAR-10 is a static dataset. The dataset was collected with great effort and
    is used all over the world. No one is collecting and adding images to the dataset
    at the moment; instead, everyone uses it as-is. CIFAR-10 has 10 classes of objects,
    such as `birds`, and for those objects, it captures many of the variations you
    would expect in real life, such as a bird taking off in flight, a bird on the
    ground, or perhaps a bird pecking or making some traditional eating movement.
    The dataset doesn’t include all variations of all birds and what they may be doing,
    however. Consider a bird taking flight and rising to the top-left portion of the
    image. An analogous image you could imagine would be of a bird making the same
    ascension movement to the top right of the image. Or the image could be flipped
    from left to right. You could imagine a bird in flight in the top portion of the
    image, but not the bottom, and flip the image from top to bottom or rotate it.
    Consider seeing a bird far away versus close-up, or when it’s sunny or a bit darker
    before dusk. The possibilities are endless!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10是一个静态数据集。该数据集经过极大的努力收集，并被全世界使用。目前没有人收集并向数据集中添加图像；相反，每个人都使用它原样。CIFAR-10有10个物体类别，例如`鸟类`，对于这些物体，它捕捉了你在现实生活中预期到的许多变化，例如一只鸟在飞行中起飞，一只鸟在地上，或者可能是一只鸟啄食或做出一些传统进食动作。然而，数据集并没有包括所有鸟类及其可能做的所有变化。考虑一只鸟在飞行中上升到图像的左上角。你可以想象的一个类似图像是一只鸟在图像的右上角做同样的上升动作。或者图像可以从左到右翻转。你可以想象一只鸟在图像顶部飞行，但在底部不飞，翻转图像从上到下或旋转它。考虑看到一只远处的鸟与近距离的鸟，或者它在晴天或黄昏前有点暗。可能性是无限的！
- en: As you’ve learned so far, the CNN’s job is to use filters to represent higher-
    and lower-order image features in the network. Those features are heavily influenced
    by foreground, background, object position, rotation, and so on. To account for
    the variational features in images that you’ll find in real life, you use data
    augmentation. *Data augmentation* takes a static dataset and represents the variation
    in images by applying these transformations randomly to images in the dataset
    during training, augmenting your static data with new images. Given enough training
    epochs and based on batch size (a hyperparameter), you can use data augmentation
    to significantly increase the variation and learnability of your dataset. Take
    that bird image ascending left, and flip it to the right during some of the training
    epochs. Take the bird landing at dusk, and change the image contrast to make it
    brighter during other epochs. These changes allow the network to account for further
    variation in images in its learned parameters, making it more resilient to unseen
    images in real life that will have these variations—all without having to collect
    new images, which could be costly or perhaps impossible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所学，卷积神经网络（CNN）的任务是使用过滤器在网络上表示更高和更低阶的图像特征。这些特征受到前景、背景、物体位置、旋转等因素的严重影响。为了应对现实生活中图像中的可变特征，你使用数据增强。*数据增强*通过在训练过程中随机将这些变换应用于数据集中的图像，以表示图像中的变化，通过添加新的图像来增强你的静态数据。在足够的训练轮次和基于批大小（一个超参数）的情况下，你可以使用数据增强来显著增加数据集的变异性和可学习性。例如，将那只鸟的图像向上左方翻转，在部分训练轮次中将其翻转至右侧。将鸟在黄昏时分着陆的图像，在其他轮次中改变图像对比度使其更亮。这些变化使得网络能够在其学习参数中考虑图像的进一步变化，使其对现实生活中具有这些变化的不见图像更具鲁棒性——而无需收集新的图像，这可能成本高昂或可能不可能。
- en: You’ll implement a few of these augmentations in listings 15.3 and 15.4\. In
    particular, you’ll implement the image flip left/right; then you’ll add some random
    noise to the image called contrast. *Contrast* is also referred to as *salt-and-pepper*
    or *black-and-white* randomly masked pixels, as shown in the `sp_noise` function
    in listing 15.3.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在列表15.3和15.4中实现这些增强中的一些。特别是，你将实现图像左右翻转；然后你将在图像中添加一些随机噪声，称为对比度。*对比度*也被称为*盐和胡椒*或*黑白*随机掩码像素，如列表15.3中的`sp_noise`函数所示。
- en: Listing 15.3 Simple salt-and-pepper noise for the image
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.3 简单的盐和胡椒噪声图像
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Adds salt (0 value, or white) and pepper (255 value, or black) noise to image.
    prob is the probability of the noise.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向图像添加盐（0值，或白色）和胡椒（255值，或黑色）噪声。prob 是噪声的概率。
- en: ❷ Decides the threshold for setting the pixel to black (255) or white (0)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 决定设置像素为黑色（255）或白色（0）的阈值
- en: ❸ Returns the salted image
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回加盐的图像
- en: You can use NumPy’s `random` function and then set hyperparameters for the probability
    of flipping the image and for salt and pepper, set to 5% for both parameters in
    listing 15.4\. You can play around with these values (they are hyperparameters)
    because they control how much extra training data you will generate. More data
    is always better, but it can grow your dataset significantly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用NumPy的`random`函数，然后为翻转图像的概率和盐和胡椒设置超参数，在列表15.4中将这两个参数都设置为5%。您可以玩弄这些值（它们是超参数），因为它们控制您将生成多少额外的训练数据。更多的数据总是更好的，但它可以显著增加您的数据集。
- en: Listing 15.4 Achieving data augmentation for CIFAR-10 images
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.4 实现CIFAR-10图像的数据增强
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ 5% chance of flipping the image each time
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每次翻转图像的概率为5%
- en: ❷ 5% chance of applying the salt/pepper to the image
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将盐和胡椒应用于图像的概率为5%
- en: ❸ Amount of noise or salt/pepper (15% in the image)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 图像中噪声或盐和胡椒的量（图像中的15%）
- en: ❹ Random flip UD (which will in effect do the left/right flip)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 随机翻转UD（这将实际上执行左右翻转）
- en: ❺ Adds the flipped image and label
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加翻转图像和标签
- en: ❻ Random salt and pepper for the image
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 为图像添加随机盐和胡椒噪声
- en: ❼ Returns the augmented image data
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 返回增强的图像数据
- en: You can double-check whether your image dataset is properly augmented by plotting
    a random image from training beyond image 50,000\. I chose 52,002, but you can
    inspect others. The following code uses Matplotlib to print a 24 × 24 grayscale
    augmented image from CIFAR-10, as shown in figure 15.3\. Note that the count of
    images begins at index 0; hence, the index for image 52,002 is 52,001.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过绘制训练数据中超过50,000张图像的随机图像来双重检查您的图像数据集是否已正确增强。我选择了52,002，但您也可以检查其他图像。以下代码使用Matplotlib打印出CIFAR-10的24
    × 24灰度增强图像，如图15.3所示。请注意，图像的计数从索引0开始；因此，图像52,002的索引是52,001。
- en: '![CH15_F03_Mattmann](../Images/CH15_F03_Mattmann.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F03_Mattmann](../Images/CH15_F03_Mattmann.png)'
- en: Figure 15.3 A flipped (to the right) image of an automobile, likely with salted
    contrast
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 一张翻转（向右）的汽车图像，可能带有盐和胡椒增强的对比度
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, with your augmented dataset, you’re ready to construct a deeper CIFAR-10
    CNN model to capture those image features.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了您的增强数据集，您就可以构建一个更深的CIFAR-10 CNN模型来捕捉这些图像特征。
- en: 15.2 Building a deeper CNN architecture for CIFAR-10
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 为CIFAR-10构建更深的CNN架构
- en: The CNN architecture you constructed in chapter 14 was a shallow architecture
    with only two convolutional layers and one fully connected layer leading to the
    output layer for image class prediction. The CNN worked, but if you evaluated
    it against the CIFAR-10 test labels by generating a receiver operating characteristic
    (ROC) curve, as I’ve shown you in earlier chapters, it would not sufficiently
    differentiate among classes of CIFAR-10 test images and would do even worse evaluating
    unseen data randomly collected from the internet. How poorly the shallow architecture
    would perform, I’ll leave as an exercise for you. Instead, in this section I’ll
    focus on how you can build a better architecture and evaluate its performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14章中构建的CNN架构是一个浅层架构，仅包含两个卷积层和一个全连接层，最终连接到输出层以进行图像类别预测。CNN是有效的，但如果您像我在前面的章节中展示的那样，通过生成接收者操作特征（ROC）曲线对其进行评估，它将不足以区分CIFAR-10测试图像的类别，并且在使用从互联网随机收集的未见数据评估时表现会更差。浅层架构将如何表现，我将留作您的练习。相反，在本节中，我将重点介绍如何构建更好的架构并评估其性能。
- en: One big reason for the poor performance has to do with what machine-learning
    theorists call model capacity. In shallow architectures, the network lacks the
    necessary capacity to capture variance between higher- and lower-order image features
    that properly distinguish among image classes. Without the necessary amount of
    weights for your machine-learning model to learn, the model can’t separate the
    variance in the images; therefore, it can’t properly discern large and small differences
    in input images of different classes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 性能不佳的一个主要原因是与机器学习理论家所说的模型容量有关。在浅层架构中，网络缺乏捕获高阶和低阶图像特征之间差异的必要容量，这些特征可以正确区分图像类别。如果没有足够的权重让您的机器学习模型进行学习，模型就无法分离图像中的差异；因此，它无法正确区分不同类别的输入图像中的大小差异。
- en: 'Take the example of a frog versus an automobile. Frogs and cars have a similar
    feature: four shapes toward the bottom of the image (figure 15.4). Those shapes
    in frogs—feet—are flatter and, depending on sitting orientation, can be spaced
    in different ways. In a car with four tires, these shapes are circular and more
    stationary, and they can’t reorient themselves as the frog’s feet do. Without
    the capacity to learn these parameters, a CNN with shallow architecture can’t
    tell the difference between these features, and the input image is close enough
    to a car that the output neurons may fire on the `automobile` class and not the
    `frog` class. With extra weights and neurons, the network may figure out that
    the top-left outward shape is more indicative of animals and frogs than it is
    of cars—a key feature that would update the network’s understanding and cause
    it to flag the image as a frog instead of an automobile.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以青蛙与汽车为例。青蛙和汽车有一个相似的特征：图像底部有四个形状（图15.4）。青蛙中的这些形状——脚——更平，并且根据坐姿，可以以不同的方式排列。在四轮汽车的四个轮胎上，这些形状是圆形的，并且更固定，不能像青蛙的脚那样重新定位。如果没有学习这些参数的能力，浅层架构的CNN无法区分这些特征，输入图像与汽车非常接近，输出神经元可能会在`automobile`类别上触发，而不是在`frog`类别上。通过额外的权重和神经元，网络可能会发现左上角的向外形状比汽车更能表明动物和青蛙，这是一个关键特征，将更新网络的理解，并导致它将图像标记为青蛙而不是汽车。
- en: '![CH15_F04_Mattmann2](../Images/CH15_F04_Mattmann2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F04_Mattmann2](../Images/CH15_F04_Mattmann2.png)'
- en: Figure 15.4 An input CIFAR-10 training image—a frog—run first through a shallow
    network and then through a dense network. The shallow network thinks that the
    image is a car; the dense network learns that the image is a frog.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 一个输入的CIFAR-10训练图像——青蛙——首先通过浅层网络，然后通过密集网络。浅层网络认为图像是汽车；密集网络学习到图像是青蛙。
- en: If your neural architecture lacks the capacity to learn these variations, in
    the form of weights to train and update based on training data and labels, your
    model won’t be able to learn these simple differences. You can think of the model
    filters as being tunable parameters that distinguish between the big-deal and
    smaller-deal differences in the images.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的神经网络架构缺乏学习这些变化的能力，即以权重形式根据训练数据和标签进行训练和更新，那么你的模型将无法学习这些简单的差异。你可以将模型过滤器视为可调节的参数，它们区分图像中的重大差异和较小差异。
- en: Even humans may need some reinforcement
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是人类也可能需要一些强化
- en: A simple experiment that I conducted while writing this book yielded some interesting
    results. I showed a picture of the frog in figure 15.4 to a few members of my
    family and asked them to identify the image in a context-free manner, without
    telling them the labels.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书的过程中，我进行了一个简单的实验，得到了一些有趣的结果。我向我的家庭成员展示图15.4中的青蛙图片，并要求他们以无上下文的方式识别图像，没有告诉他们标签。
- en: Was it a dinosaur? The Loch Ness Monster? A boomerang?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那是恐龙吗？尼斯湖水怪？回旋镖？
- en: Providing a choice of two labels—automobile or frog—produced better results.
    The point is that even humans, with our dense neural architectures, may need label
    retraining or possibly more data augmentation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 提供两个标签的选择——汽车或青蛙——产生了更好的结果。重点是，即使是拥有密集神经网络的人类，可能也需要标签重新训练或可能更多的数据增强。
- en: The AlexNet model gives you a road map showing how many features are necessary
    to delineate all the features in the CIFAR-10 dataset and improve its accuracy
    significantly. In fact, AlexNet goes well beyond CIFAR-10; it contains enough
    capacity that if it’s given enough input data and training, it performs well on
    the much larger ImageNet dataset, which has 1,000 classes of input images and
    millions of training samples. AlexNet’s architecture will suffice as a model for
    a denser architecture with the capacity needed to improve the model accuracy for
    CIFAR-10.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet模型为你提供了一个路线图，显示了需要多少特征来界定CIFAR-10数据集中的所有特征并显著提高其准确性。事实上，AlexNet远远超出了CIFAR-10；它具有足够的容量，如果给它足够的输入数据和训练，它可以在更大的ImageNet数据集上表现良好，该数据集有1,000个输入图像类别和数百万个训练样本。AlexNet的架构足以作为具有所需容量的密集架构的模型，以改善CIFAR-10的模型准确性。
- en: Listing 15.5 isn’t exactly AlexNet; I omitted a couple of filters that will
    reduce the amount of training time and memory needed for your computer without
    sacrificing noticeable accuracy. You’ll make these types of choices while training
    machine-learning models. Some of these choices will vary based on your access
    to high-capacity or cloud computing, or whether you are working only on your laptop.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.5 并非完全等同于 AlexNet；我省略了一些滤波器，这将在不牺牲明显准确性的情况下减少训练时间和计算机内存的需求。在训练机器学习模型时，你将做出这类选择。其中一些选择将取决于你是否有高容量或云计算资源，或者你是否仅在笔记本电脑上工作。
- en: 'The function `model``()` takes as input the CIFAR-10 image, a hyperparameter
    called `keep_prob`, and a function called `tf.nn.dropout` that I’ll explain in
    section 15.2.1\. The model construction uses TensorFlow’s 2D convolutional filter
    function, which takes as input four convolutional filters, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `model()` 以 CIFAR-10 图像、一个名为 `keep_prob` 的超参数以及一个名为 `tf.nn.dropout` 的函数作为输入，这些内容将在第
    15.2.1 节中解释。模型构建使用了 TensorFlow 的二维卷积滤波器函数，它以四个卷积滤波器作为输入，如下所示：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Recall from chapter 14 that the filters are of the shape `[x, y, c, n]`, where
    `[x, y]` is the window size for the convolutional patch, `c` is the number of
    image channels (1 for grayscale), and `n` is the number of filters. So your dense
    model uses four filters, with a 3 × 3 window patch size for the first two layers,
    5 × 5 for the last two layers, 1 channel for grayscale input, 64 filters in the
    first convolutional layer, 128 filters in the second, 256 in the third, and 512
    in the fourth layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾第 14 章的内容，滤波器的形状为 `[x, y, c, n]`，其中 `[x, y]` 是卷积块的大小，`c` 是图像通道数（灰度图为 1），`n`
    是滤波器的数量。因此，你的密集模型使用了四个滤波器，前两层使用 3 × 3 的窗口块大小，后两层使用 5 × 5，灰度输入使用 1 个通道，第一层卷积使用
    64 个滤波器，第二层使用 128 个，第三层使用 256 个，第四层使用 512 个。
- en: Note The choices of these architectural properties of the network are elaborated
    in the Krizhevsky paper. But I’ll note that it’s an active area of research and
    something that in itself is best left to the latest papers in machine learning
    for constructing neural architectures. In this chapter, we’re going to follow
    the prescribed models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些网络架构属性的选择在 Krizhevsky 的论文中有详细阐述。但我要指出，这是一个活跃的研究领域，并且最好留给最新的机器学习论文来构建神经网络架构。在本章中，我们将遵循规定的模型。
- en: The latter portion of the model construction uses some higher-level TensorFlow
    functions to create neural network layers. The function `tf.contrib.layers.fully_
    connected` creates fully connected layers to interpret the learned feature parameters
    and output the CIFAR-10 image labels.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建的后半部分使用了一些高级 TensorFlow 函数来创建神经网络层。函数 `tf.contrib.layers.fully_connected`
    创建全连接层来解释学习到的特征参数并输出 CIFAR-10 图像标签。
- en: To begin, reshape the input image from CIFAR’s 32 × 32 × 1 (1024 pixels) to
    [24 × 24 × 1] input, mainly to reduce the number of parameters and lessen the
    burden on your local computing resources. If you’ve got a machine more powerful
    than a laptop, you can experiment with this parameter, so feel free to tweak the
    size upward. To each convolutional filter, you’ll apply a 1-pixel stride and the
    same padding, and convert the output to neurons via the rectifying linear unit
    (ReLU) activation function. Each layer includes max pooling to further reduce
    the learned parameter space through averaging, and you’ll use batch normalization
    to make the statistics in each layer easier to learn. The convolutional layers
    are flattened into a 1D layer of parameters representing the fully connected layers;
    then they are mapped into the final 10-class softmax output predictions to round
    out the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将输入图像从 CIFAR 的 32 × 32 × 1（1024 像素）重塑为 [24 × 24 × 1] 输入，主要是为了减少参数数量并减轻本地计算资源的负担。如果你有一台比笔记本电脑更强大的机器，你可以尝试调整这个参数，所以请随意调整大小。对每个卷积滤波器，你将应用
    1 像素步长和相同的填充，并通过 ReLU 激活函数将输出转换为神经元。每一层都包括最大池化，通过平均进一步减少学习到的参数空间，并且你将使用批量归一化来使每一层的统计信息更容易学习。卷积层被展平成一个代表全连接层的
    1D 参数层；然后它们被映射到最终的 10 类 softmax 输出预测，以完善模型。
- en: Listing 15.5 A denser CNN architecture for CIFAR-10 modeled on AlexNet
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.5：基于 AlexNet 的 CIFAR-10 密集 CNN 架构
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Reshapes into [24 × 24 × 1] input
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重塑为 [24 × 24 × 1] 输入
- en: ❷ Applies the filters
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 应用滤波器
- en: ❸ Uses the ReLU activation function for the neurons
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为神经元使用 ReLU 激活函数
- en: ❹ Applies max pooling
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 应用最大池化
- en: ❺ Uses batch normalization to make the statistics in each layer easier to learn
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用批量归一化使每层的统计信息更容易学习
- en: ❻ Flattens the neurons into a 1D layer
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将神经元展平为1D层
- en: ❼ Maps the hidden-layer neurons to the final 10-class softmax output predictions
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将隐藏层神经元映射到最终的10类softmax输出预测
- en: Now that you’ve got the model built, I’ll show you how to make it more resilient
    to variations in input during training.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经构建了模型，我将向你展示如何使其在训练过程中对输入的变化更具抵抗力。
- en: 15.2.1 CNN optimizations for increasing learned parameter resilience
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 增强学习参数弹性的CNN优化
- en: 'The model function also uses two other optimizations: batch normalization and
    dropout. There are plenty of more-detailed mathematical explanations for the utility
    and purpose of batch normalization, which I’ll leave to you to research. The simple
    way to explain batch normalization is as a mathematical function that ensures
    learned parameters in each layer are easier to train and not overfit to the input
    images, which themselves are already normalized (converted to grayscale, divided
    by the image mean, and so on). In short, batch normalization eases training and
    accelerates model convergence to optimal parameters. The good news is that TensorFlow
    hides all the mathematical complexity and gives you an easy utility function to
    apply this technique.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模型函数还使用了两种其他优化：批量归一化和dropout。关于批量归一化的效用和目的有大量的更详细的数学解释，我将留给你们去研究。简单来说，批量归一化是一个数学函数，确保每个层的所学参数更容易训练，并且不会过度拟合输入图像，这些图像本身已经归一化（转换为灰度，除以图像均值等）。简而言之，批量归一化简化了训练并加速了模型收敛到最佳参数。好消息是，TensorFlow隐藏了所有的数学复杂性，并为你提供了一个易于使用的实用函数来应用这项技术。
- en: 'TIP If you want to read more about batch normalization, consider this informative
    article: [http://mng.bz/yrxB](https://shortener.manning.com/yrxB).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你想了解更多关于批量归一化的信息，请考虑这篇信息丰富的文章：[http://mng.bz/yrxB](https://shortener.manning.com/yrxB)。
- en: For dropout, the key intuition is similar to data augmentation in the sense
    that you want to make the CNN learned parameters more resilient (less sensitive)
    to imperfections in the input, as in real life. Even if you squint, distort the
    image you are seeing, you can still make out the objects in it, up to a point.
    This phenomenon means that you can distort an image and still reinforce the learned
    labels. Dropout goes a step further by forcing the network to randomly forget
    or mask the internal neurons for their learned values while reinforcing the output
    labels during training. Dropout randomly turns off neurons during training with
    probability (`1-keep_prob)`, where `keep_prob` is the second parameter input to
    the `model()` function from listing 15.5.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于dropout，关键直觉与数据增强类似，即你希望使CNN学习到的参数对输入中的不完美（更不敏感）具有更强的抵抗力，就像在现实生活中一样。即使你眯着眼睛，扭曲你看到的图像，你仍然可以辨认出其中的物体，直到一定程度。这种现象意味着你可以扭曲图像，同时仍然加强学习到的标签。Dropout通过在训练过程中强制网络随机忘记或掩盖内部神经元的学习值，同时在训练过程中加强输出标签，更进一步。Dropout在训练过程中以概率(`1-keep_prob`)随机关闭神经元，其中`keep_prob`是列表15.5中输入到`model()`函数的第二个参数。
- en: Dropout causes the network to learn hardened weights and parameters that are
    resilient to imperfections or variations in the training process. So, similar
    to noising an image as you saw in chapter 12 and still learning more robust weights
    in spite of the distortion, dropout operates similarly and masks the *internal*
    world of the network. In other words, dropout randomly disables its own hidden
    neurons during training so that what it learns ends up being more resilient independent
    of the input. The technique has proved to be effective in the construction of
    neural networks because with enough epochs and training time, the network learns
    to deal with internal failure and still recall the correct labels and tuning parameters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout会导致网络学习到更加坚固的权重和参数，这些权重和参数对训练过程中的不完美或变化具有抵抗力。因此，类似于在第12章中看到的对图像进行噪声处理，尽管存在扭曲，但仍然学习到更鲁棒的权重，dropout操作类似，并掩盖了网络的*内部*世界。换句话说，dropout在训练过程中随机禁用其自身的隐藏神经元，因此它所学习到的最终结果对输入更加有弹性。这项技术在神经网络构建中已被证明是有效的，因为随着足够多的时代和训练时间的增加，网络学会处理内部故障，并仍然能够回忆正确的标签和调整参数。
- en: Ever forget something that you remembered later? Dropout helped!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经忘记了一些后来又记得的事情？Dropout帮了忙！
- en: 'This happens all the time, at least to me. I was trying to remember the label
    for an input image, stroller, the other day after discussing something that happened
    on my daily walk with my wife and kids. I kept referring to the stroller as the
    “pushing cart.” Eventually I focused, and there it came: stroller. I had no reinforcement
    from my wife, who was internally laughing uncontrollably, I’m sure. Over the years,
    I’m sure that my internal network layers are set up with dropout to allow recollection,
    even if not immediately. Aren’t biologically-inspired computer models grand?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我来说经常发生。那天，我在和妻子和孩子讨论了我的日常散步中发生的事情后，试图回忆一个输入图像的标签，婴儿车。我不断地把婴儿车称为“推车”。最终我集中了注意力，然后就想起来了：婴儿车。我肯定妻子没有给我任何强化，她肯定在内心狂笑。多年来，我确信我的内部网络层已经设置了dropout以允许回忆，即使不是立即的。生物启发的计算机模型不是非常棒吗？
- en: Now it’s time to train your optimized CNN and see whether you can do better
    than the initial shallow network you trained in chapter 14.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候训练你的优化后的卷积神经网络（CNN）了，看看你是否能比第14章训练的初始浅层网络做得更好。
- en: 15.3 Training and applying a better CIFAR-10 CNN
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 训练和应用更好的CIFAR-10 CNN
- en: With the well-curated CIFAR-10 input dataset, data augmentation, cleaning, and
    normalization complete, and given your resilient biologically inspired CNN model,
    you’re ready for some training with TensorFlow. Consider the optimization steps
    for building a CNN, highlighted in figure 15.5\. Thus far, I’ve covered the ways
    you can have optimal data for training and some steps for model representation
    to ensure capacity, memory, resilience, and convergence for training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在精心整理的CIFAR-10输入数据集、数据增强、清理和归一化完成后，以及有了你具有生物启发性的CNN模型，你就可以使用TensorFlow进行一些训练了。考虑图15.5中突出显示的构建CNN的优化步骤。到目前为止，我已经介绍了如何获得最佳的训练数据以及确保训练容量、内存、弹性和收敛的一些模型表示步骤。
- en: '![CH15_F05_Mattmann2](../Images/CH15_F05_Mattmann2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F05_Mattmann2](../Images/CH15_F05_Mattmann2.png)'
- en: Figure 15.5 Optimization steps for building a CNN and what you can do in each
    stage from data to learned model
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5展示构建CNN的优化步骤以及在每个阶段从数据到学习模型你可以做什么
- en: 'The training step also has optimizations that you can apply, as covered in
    previous chapters. You may recall that regularization is a technique in training
    that influences better learned parameters by penalizing exploration of nonoptimal
    parameter values during training. Regularization applies to input images as well
    as to numerical and text inputs. That’s the beauty of machine learning and of
    TensorFlow: everything is an input tensor or matrix of numbers.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 训练步骤也有你可以在之前章节中找到的优化方法。你可能还记得，正则化是一种在训练中通过惩罚非最优参数值探索来影响更好学习参数的技术。正则化适用于输入图像以及数值和文本输入。这正是机器学习和TensorFlow的美丽之处：一切都是输入张量或数字矩阵。
- en: 'Listing 15.6 sets up the training process by rescaling the CIFAR 32 × 32 RGB
    images to 24 × 24 grayscale. Training is set up as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.6通过将CIFAR 32 × 32 RGB图像重新缩放为24 × 24灰度图来设置训练过程。训练设置如下：
- en: Using 1,000 epochs with a 0.001 learning rate
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用1000个周期和0.001的学习率
- en: Dropping out neurons with 30% probability, using L2 regularization
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以30%的概率丢弃神经元，使用L2正则化
- en: Applying the AdamOptimizer technique
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用AdamOptimizer技术
- en: 'The rationales for selecting these tunable training parameters could be chapters
    in their own right and are covered in detail elsewhere. As I mention throughout
    the book, you should experiment by changing these values to see how they affect
    the overall experience during training:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这些可调训练参数的理由可以成为它们自己的章节，并且已经在其他地方详细介绍了。正如我在书中提到的，你应该通过改变这些值来实验，看看它们如何影响训练过程中的整体体验：
- en: Length of training time
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时间的长度
- en: Amount of GPU and memory used while processing
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理时使用的GPU和内存量
- en: Ability to converge to an optimal result
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收敛到最优结果的能力
- en: 'For now, these parameters will allow you to complete training, even on a laptop
    with a CPU and no GPU. I should warn you, though: training a network this dense
    can take a day or more.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这些参数将允许你在没有GPU的笔记本电脑上完成训练。不过，我要提醒你：训练这样一个密集的网络可能需要一天或更长时间。
- en: Listing 15.6 Setting up the training process for your CIFAR-10 dense CNN
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.6为你的CIFAR-10密集CNN设置训练过程
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Removes previous weights, bias, and inputs
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 移除之前的权重、偏差和输入
- en: ❷ Input of size (50000,576), or 50,000 24 × 24 images
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 大小为（50000,576）的输入，或50,000个24 × 24的图像
- en: ❸ Input of size (50000,10), or 50,000 class labels for the 10 classes
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 大小为（50000,10）的输入，或10个类别的50,000个类别标签
- en: ❹ Defines the hyperparameter for dropout. Neurons in layers where this is applied
    will be set to 0 with probability 1-keep_prob.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义dropout的超参数。应用此操作层的神经元将以1-keep_prob的概率被设置为0。
- en: ❺ Trains for 1,000 epochs with learning rate 0.001
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用学习率0.001进行1,000个epoch的训练
- en: ❻ Defines the model and enables you to look it up with the name ‘logits’ from
    disk after training
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义模型并允许您在训练后从磁盘通过名称‘logits’查找它
- en: ❼ Applies regularization by adding the weights together, applying a hyperparameter
    beta to them, and adding to the cost for L2 regularization
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过将权重相加，应用超参数beta，并将其添加到L2正则化的成本中来实现正则化
- en: ❽ Uses the AdamOptimizer
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用AdamOptimizer
- en: ❾ Measures accuracy and the number of correct predictions
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 测量准确率和正确预测的数量
- en: Now that the training process is defined, you can train your model (listing
    15.7). I’ve included a few options that take advantage of a GPU if one is available
    in your system. Note if you have a GPU, the training can complete in a few hours
    rather than days. The listing code also saves your trained model so you can load
    it for predictions and generate the ROC curve for evaluation later.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练过程已经定义，您可以训练您的模型（列表15.7）。我包括了一些选项，如果您的系统中有GPU，可以利用GPU。注意，如果您有GPU，训练可以在几小时内完成，而不是几天。列表代码还保存了您的训练模型，以便您可以加载它进行预测并生成ROC曲线进行评估。
- en: Listing 15.7 Executing the deep CNN training for CIFAR-10
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.7 执行CIFAR-10的深度CNN训练
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Allows for GPU to be used and for growth in GPU memory
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 允许使用GPU并允许GPU内存增长
- en: ❷ Creates a saver for your TensorFlow model to store it to disk
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为您的TensorFlow模型创建一个保存器以将其存储到磁盘
- en: ❸ Converts the 10 CIFAR-10 label names to one-hot labels
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将10个CIFAR-10标签名称转换为one-hot标签
- en: ❹ Divides your training into batches (may be greater than 200 due to data augmentation)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将训练数据分成批次（由于数据增强，批次大小可能超过200）
- en: ❺ Trains and visually shows progress using the TQDM library
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用TQDM库进行训练并可视化进度
- en: ❻ Saves the model
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保存模型
- en: 'Go ahead and get a cup of coffee, as they say in the science community. This
    code is going to take a while to run as your network does the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 按照科学界的说法，去拿一杯咖啡吧。这段代码运行起来会花费一些时间，因为您的网络正在执行以下操作：
- en: Generates automatic additional examples of the input data and images
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动生成输入数据和图像的附加示例
- en: Learns a more resilient representation, both externally through augmentation
    and internally by randomly turning off neurons 30% of the time
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过外部增强和内部随机关闭30%的神经元，学习更健壮的表示
- en: Captures more variations using its four convolutional filters, and distinguishes
    between the CIFAR-10 image classes
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其四个卷积滤波器捕捉更多变化，并区分CIFAR-10图像类别
- en: Trains faster and with higher probability of finding optimal weights via regularization
    and batch normalization
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过正则化和批量归一化训练更快，并且有更高的概率找到最优权重
- en: 15.4 Testing and evaluating your CNN for CIFAR-10
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 测试和评估您的CNN对CIFAR-10
- en: OK, it’s hours or maybe days later, and you’re back. I know, I know—why did
    I subject you to this torture? Yes, I’m sorry that running this code on your laptop
    crashed all your other programs. At least you’ve got your trained model now, and
    it’s time to try it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，几个小时或可能几天后，您回来了。我知道，我知道——为什么我要让你受这样的折磨？是的，我为在您的笔记本电脑上运行这段代码导致所有其他程序崩溃而道歉。至少你现在有了训练好的模型，是时候尝试它了。
- en: 'To do that, you’ll need a prediction function. Using your learned model to
    make predictions involves some steps similar to training. First, you need to make
    sure that your input is a 24 × 24 single-channel grayscale image. If so, you can
    load your trained model from disk and obtain a few key pieces of information from
    running your input image through it:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，您需要一个预测函数。使用您学习到的模型进行预测涉及一些与训练类似的步骤。首先，您需要确保您的输入是一个24 × 24的单通道灰度图像。如果是这样，您可以从磁盘加载您的训练模型，并通过运行输入图像通过它来获取一些关键信息：
- en: The output logits, which you’ll run a `tf.nn.softmax` over to get predictions
    for all 10 image classes from CIFAR-10.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出logits，您将运行`tf.nn.softmax`以从CIFAR-10的10个图像类别中获得预测。
- en: The dimension with the highest softmax value is the output predicted class.
    You can get this dimension with an `np.argmax` call that returns the highest-value
    column index in a row. The associated confidence for this prediction is the output
    of its softmax value. The `np.argmax` call obtains and returns the selected highest
    confidence class (`class_num`), its name (`bird`, `automobile`, and so on), the
    confidence or softmax value, and the full set of confidences for all classes.
    Listing 15.8 creates the `predict` function to allow you to call the classifier.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有最高softmax值的维度是预测的输出类别。您可以通过`np.argmax`调用获取此维度，它返回行中最高值列的索引。此预测的关联置信度是softmax值的输出。`np.argmax`调用获取并返回所选最高置信度类别（`class_num`）、其名称（`bird`、`automobile`等）、置信度或softmax值以及所有类别的完整置信度集合。列表15.8创建了`predict`函数，允许您调用分类器。
- en: Listing 15.8 Making predictions of CIFAR-10 classes from input images
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.8：从输入图像预测CIFAR-10类别
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Gets a pointer to the default TensorFlow graph
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取默认TensorFlow图的指针
- en: ❷ Loads model into the graph
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将模型加载到图中
- en: ❸ Gets tensors from loaded model
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从加载的模型获取张量
- en: ❹ Runs the model using its learned weights; gets the output logits and runs
    the softmax function over them
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用其学习到的权重运行模型；获取输出logits并在其上运行softmax函数
- en: ❺ Returns highest confidence class number, name, and predictions
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回最高置信度类别编号、名称和预测结果
- en: 'You can test your prediction function by applying it to the third training
    image from CIFAR-10: a deer (figure 15.6).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将预测函数应用于CIFAR-10的第三张训练图像（鹿，图15.6）来测试您的预测函数。
- en: '![CH15_F06_Mattmann](../Images/CH15_F06_Mattmann.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F06_Mattmann](../Images/CH15_F06_Mattmann.png)'
- en: Figure 15.6 An image of a deer from CIFAR-10
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6 来自CIFAR-10的鹿的图像
- en: 'The following code loads the model and obtains the class number, its name,
    the confidence in the prediction, and the full set of predictions for this image:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载模型并获取类别编号、名称、预测置信度和该图像的完整预测集合：
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Because it’s a softmax, you get a confidence score in the model’s prediction
    for all 10 CIFAR-10 classes. So your model is 93% confident (`0.9301368` value)
    this image is a picture of a deer. The next-highest beliefs are class 6 (`frog`)
    with confidence ~3%—a virtual tie between classes 2 (`bird`) and 5 (`dog`) at
    ~1%, respectively. The drop-off in confidence between `deer` and the latter three
    classes is statistically significant (90%+ points):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因为是softmax，所以您会得到模型对CIFAR-10所有10个类别的预测置信度。所以您的模型有93%的置信度（`0.9301368`值）认为这张图像是鹿的图片。下一个最高置信度是类别6（`frog`）置信度约为3%——类别2（`bird`）和类别5（`dog`）在约1%的置信度上形成虚拟平局。`deer`与后三个类别之间的置信度下降在统计上具有显著性（90%+点）：
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: That result is a nice confidence-booster for a single image from the training
    data, but how well does your deep CNN with optimizations perform on the unseen
    test data from CIFAR-10? You can build a simple evaluation function that will
    run your new model across all the test data and output prediction accuracy. You
    can load the model the same way. This time, give the model the full set of 10,000
    test images and 10,000 test labels (1,000 per class); count the number of times
    that the model predicted correctly; and store `1` for each correct entry and `0`
    otherwise. Overall accuracy, then, is the mean of that prediction array for all
    the images. Listing 15.9 has the full evaluation of the deep CNN on the CIFAR-10
    test data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该结果是对训练数据中单个图像的很好的置信度提升，但您的经过优化的深度CNN在CIFAR-10未见测试数据上的表现如何？您可以构建一个简单的评估函数，该函数将在所有测试数据上运行您的新模型并输出预测准确率。您可以以相同的方式加载模型。这次，给模型提供完整的10,000个测试图像和10,000个测试标签（每类1,000个）；计算模型预测正确的次数；每次正确预测存储`1`，否则存储`0`。因此，整体准确率是所有图像预测数组的平均值。列表15.9包含对CIFAR-10测试数据深度CNN的完整评估。
- en: Listing 15.9 Running your deep CNN across CIFAR-10’s test data
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.9：在CIFAR-10测试数据上运行您的深度CNN
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Loads the model
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载模型
- en: ❷ Gets tensors from loaded model
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从加载的模型获取张量
- en: ❸ Applies the model given the input test images and test labels
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据输入测试图像和测试标签应用模型
- en: ❹ Counts how many times the model agreed with the one-hot test labels, storing
    1 for each correct prediction and 0 otherwise
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算模型与one-hot测试标签一致次数，每次正确预测存储1，否则存储0
- en: ❺ Measures average accuracy by computing the mean
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过计算平均值来衡量平均准确率
- en: ❻ Returns the predictions, the correct prediction counts, and the one-hot test
    labels
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回预测结果、正确预测次数和one-hot测试标签
- en: 'Run listing 15.9 with the following simple invocation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下简单调用运行列表15.9：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The command produces the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 命令产生以下输出：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, I’ll discuss how you can evaluate the accuracy of your CNN. A familiar
    technique will re-appear: ROC curves.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将讨论如何评估你的 CNN 的准确性。一个熟悉的技术将再次出现：ROC 曲线。
- en: 15.4.1 CIFAR-10 accuracy results and ROC curves
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 CIFAR-10 准确率结果和 ROC 曲线
- en: Normally, producing a test accuracy overall of ~65% on 10,000 images wouldn’t
    feel great. Did you improve your model by training a deep CNN with optimizations?
    In fact, you have to dig a little deeper to find out, because your model’s accuracy
    is more than right or wrong for a particular image class; each time, you are predicting
    across 10 classes, and because it’s a softmax, getting a label wrong may not have
    been that wrong.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在 10,000 张图像上产生大约 65% 的测试准确率不会感觉很好。你是否通过训练一个具有优化的深度 CNN 来改进了你的模型？实际上，你必须深入挖掘才能找到答案，因为你的模型准确率不仅仅是针对特定图像类别的对或错；每次，你都在预测
    10 个类别，因为它是 softmax，得到一个错误的标签可能并不那么错误。
- en: What if the label was `bird`, and your model predicted the highest confidence
    class label as `deer` at 93%, but its next-highest confidence, 91%, was `bird`?
    Sure, you got the test answer wrong, but you weren’t far off. If your model uniformly
    was less confident in all the remaining classes, you could say that it is performing
    well overall because one of the top two predictions was correct. Scaling this
    result out, if it always happens, you will have poor overall accuracy. But considering
    the top-k (k is a hyperparameter) predictions, your model performs quite well
    and is sensitive to the right image class. Perhaps the model is missing the capacity
    to distinguish between a deer and a bird, or perhaps you do not have enough training
    examples or data augmentation for the model to distinguish between them.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 假设标签是 `bird`，而你的模型预测的最高置信度类别标签为 `deer`，置信度为 93%，但其次高置信度，91%，为 `bird`？当然，你的测试答案错了，但你并没有错得太离谱。如果你的模型在所有剩余类别上都表现出较低的不确定性，你可以说它在整体上表现良好，因为前两个预测中有一个是正确的。将这个结果扩展出去，如果它总是发生，你将会有很差的总体准确率。但考虑到
    top-k（k 是一个超参数）预测，你的模型表现相当好，并且对正确的图像类别敏感。也许模型缺少区分鹿和鸟的能力，或者也许你没有足够的训练示例或数据增强来让模型区分它们。
- en: You apply a ROC curve to evaluate the true-positive rate against the false-positive
    rate of your predictions and look at the micro-average across all the classes
    to evaluate. The ROC curve shows you how well your model is performing based on
    distinguishing between classes in the full CIFAR-10 test data, which is a more
    appropriate measure of model performance on multiclass classification problems.
    You can use your friendly neighborhood Matplotlib and SK-learn libraries, as I’ve
    shown you throughout the book.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你应用 ROC 曲线来评估预测的真阳性率与假阳性率，并查看所有类别的微观平均值以进行评估。ROC 曲线显示了你的模型在区分 CIFAR-10 测试数据中的类别时的表现，这是多类分类问题模型性能的一个更合适的衡量标准。你可以使用我在这本书中展示的友好的
    Matplotlib 和 SK-learn 库。
- en: The SK-learn libraries provide functionality to compute the ROC curve based
    on the false-positive rate (fpr) and true-positive rate (tpr), and to compute
    the area under the curve (AUC). Then Matplotlib provides plotting and graphical
    functionality to display the results. Note the `np.ravel()` function in listing
    15.10, which provides ROC curve generation; plotting code is used to return a
    continuous flattened NumPy array. The output of running the ROC curve-generation
    code in listing 15.10 is shown in figure 15.7.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: SK-learn 库提供了基于假阳性率（fpr）和真阳性率（tpr）计算 ROC 曲线以及计算曲线下面积（AUC）的功能。然后 Matplotlib 提供了绘图和图形功能来显示结果。注意列表
    15.10 中的 `np.ravel()` 函数，它提供 ROC 曲线生成；绘图代码用于返回一个连续的扁平化 NumPy 数组。运行列表 15.10 中 ROC
    曲线生成代码的输出如图 15.7 所示。
- en: '![CH15_F07_Mattmann2](../Images/CH15_F07_Mattmann2.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F07_Mattmann2](../Images/CH15_F07_Mattmann2.png)'
- en: Figure 15.7 The ROC curve for your CIFAR-10 deep CNN model. Overall, it performed
    quite well across all classes except `cat`, `bird`, and `deer`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7 你的 CIFAR-10 深度 CNN 模型的 ROC 曲线。总体而言，它在所有类别上表现相当好，除了 `cat`、`bird` 和 `deer`。
- en: The ROC curve shows that the model performed excellently for most classes and
    better than average for a few of them (`cat`, `bird`, and `deer`).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线显示，该模型在大多数类别上表现优秀，在少数类别上（`cat`、`bird` 和 `deer`）表现优于平均水平。
- en: Listing 15.10 CIFAR-10 ROC curve
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.10 CIFAR-10 ROC 曲线
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Uses SK-learn’s label_binarize function to create one-hot prediction values
    and test labels to compare
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 SK-learn 的 label_binarize 函数创建单热预测值和测试标签以进行比较
- en: ❷ Computes ROC curve and ROC area for each class
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算每个类别的ROC曲线和ROC面积
- en: ❸ Computes micro-average ROC curve and ROC area
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算微观平均ROC曲线和ROC面积
- en: ❹ Plot of a ROC curve for a specific class
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 特定类别的ROC曲线图
- en: ❺ Plots ROC curve
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 绘制ROC曲线
- en: ❻ Displays the micro-average ROC curve across all classes
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 显示所有类别的微观平均ROC曲线
- en: So although your model’s test accuracy left you feeling a little bummed (~65%),
    the model is performing quite well, with an 80% overall micro-average ROC across
    all classes. Also, the model has exceptional performance on all classes except
    `cat`, `bird`, and `deer`. The model does a good job of distinguishing among different
    image classes and likely has some confident responses for the top two or three
    predictions, even if it didn’t always get the top one right. I explore that topic
    more in section 15.4.2.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管您的模型测试准确率让您有些失望（约65%），但该模型在所有类别上的整体微观平均ROC达到了80%。此外，该模型在所有类别上除了`猫`、`鸟`和`鹿`之外都表现出色。该模型在区分不同的图像类别方面做得很好，并且对于前两个或三个预测，即使它并不总是正确地选择第一个，也可能会对某些预测有信心。我将在15.4.2节中更深入地探讨这个话题。
- en: 15.4.2 Evaluating the softmax predictions per class
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 评估每个类别的softmax预测
- en: You can go even further by looking at the softmax predictions per class to see
    your model trying to decide between classes. Instead of an ROC curve, imagine
    for each image prediction a horizontal bar chart, with a softmax bar value of
    `0` to `1` for each possible predicted class. The softmax-based bar chart is like
    watching your model try to figure out what class it believes is most correct.
    The softmax process also shows what classes the model was deciding among for the
    top spot prediction versus what classes it was throwing out or wasn’t confident
    in.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看每个类别的softmax预测来进一步了解模型在尝试决定类别。想象一下，对于每张图像预测，都有一个水平条形图，其中每个可能的预测类别的softmax条值在`0`到`1`之间。基于softmax的条形图就像观察您的模型试图确定哪个类别最正确。softmax过程还显示了模型在顶级预测中决定哪些类别，以及它丢弃或对哪些类别不自信的类别。
- en: With some effort, you can create a function that will show you this chart. You
    wouldn’t want to run the function on tens of thousands of images, but you can
    test the model on a few images outside your train-and-test set. I curated a list
    of nine random image URLs across six of the CIFAR-10 classes for this purpose.
    These images are of a frog, three ships, two trucks, a cat, a horse, and a car.
    The simple URLs (listing 15.11) are followed by calls to the `predict` function
    to run your model on each image.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一些努力，您可以创建一个函数来显示此图表。您可能不想在成千上万张图像上运行该函数，但您可以在训练和测试集之外的一些图像上测试模型。为此，我整理了一个包含九个随机图像URL的列表，这些图像来自CIFAR-10的六个类别。这些图像包括一只青蛙、三艘船、两辆卡车、一只猫、一匹马和一辆车。简单的URL（列表15.11）后面跟着对`predict`函数的调用，以在每个图像上运行您的模型。
- en: Listing 15.11 Unseen evaluation URLs from the internet for your CNN
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.11 来自互联网的未见过CNN评估URL
- en: '[PRE16]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the `predict` function has been altered slightly in listing 15.12
    to prepare the online image for your network (converting it to 24 × 24 grayscale
    via the OpenCV library) and to read the image by using the SK-learn library and
    its `imread` function.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，列表15.12中的`predict`函数已经略有改动，以准备在线图像供您的网络使用（通过OpenCV库将其转换为24 × 24灰度）以及使用SK-learn库及其`imread`函数读取图像。
- en: Listing 15.12 A `predict` function for images in the wild
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.12 用于野外图像的`predict`函数
- en: '[PRE17]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Reads the image at the specified URL and converts it to grayscale
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从指定的URL读取图像并将其转换为灰度
- en: ❷ Rescales the image to 24,24, using intercubic interpolation and OpenCV
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用双三次插值和OpenCV将图像重缩放为24,24
- en: ❸ Runs the prepared image against your network and returns the prediction
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将准备好的图像与您的网络运行并返回预测
- en: 'The following snippet runs the function against all the random images:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段在所有随机图像上运行该函数：
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With the predictions made and softmax values returned, you can make an `evaluate_
    model` function that does the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出预测并返回softmax值后，您可以创建一个`evaluate_model`函数，该函数执行以下操作：
- en: Grabs the image data from the internet and rescales to 24,24 grayscale to display
    each image
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从互联网抓取图像数据并重缩放为24,24灰度以显示每张图像
- en: Shows the output softmax predictions right next to the rescaled grayscale images
    to show you how much confidence the network had in each class
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出softmax预测直接显示在重缩放后的灰度图像旁边，以显示网络对每个类别的信心程度
- en: Listing 15.13 shows the `evaluate_model` function, and figure 15.8 shows a partial
    screenshot of the output for a few of the images. The model appears to be sensitive
    to whether it is classifying animals, objects, or vehicles, but on the third ship,
    it has some trouble deciding between the second and third guesses for each class.
    This test can be useful for evaluating the sensitivities of your model to specific
    image features. Evaluating your CNN model by using a per-class confidence figure
    like the one shown in figure 15.8 provides a guide for tuning the CNN by expanding
    capacity to capture missing features.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.13展示了`evaluate_model`函数，图15.8显示了几个图像的部分输出截图。模型似乎对它是否在分类动物、物体或车辆方面很敏感，但在第三艘船的情况下，它在每个类别的第二和第三次猜测之间有些难以决定。这项测试可以用来评估您的模型对特定图像特征的敏感性。使用如图15.8所示的每个类别的置信度图来评估您的CNN模型，可以为通过扩展容量以捕获缺失特征来调整CNN提供指导。
- en: '![CH15_F08_Mattmann](../Images/CH15_F08_Mattmann.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F08_Mattmann](../Images/CH15_F08_Mattmann.png)'
- en: Figure 15.8 Output of the `evaluate_model` function, showing the model deciding
    among four of the image URLs and their associated class labels
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8展示了`evaluate_model`函数的输出，显示了模型在四个图像URL及其相关类别标签之间进行决策
- en: The test could also suggest that you need to change your data augmentation approach
    or to tweak the hyperparameters for dropout or regularization. Functions such
    as `evaluate_model` are a necessity for improving your deep CNN and providing
    a road map to debugging and investigating it.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 测试也可能表明您需要更改数据增强方法或调整dropout或正则化的超参数。像`evaluate_model`这样的函数对于改进您的深度CNN以及提供调试和调查它的路线图是必不可少的。
- en: The code in listing 15.13 produces figure 15.8\. First, it prepares the images,
    using the OpenCV and SK-learn libraries to convert them to grayscale; next, it
    resizes the images to 24 × 24\. Then the `evaluate_model` function stacks the
    images vertically as a matrix. For each prediction, the function shows the image
    to be classified; to the right is a horizontal bar chart of the softmax predictions
    that the model derived for each class it knows about.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.13中的代码生成了图15.8。首先，它使用OpenCV和SK-learn库将图像转换为灰度；接下来，将图像调整大小为24 × 24。然后，`evaluate_model`函数将图像垂直堆叠成一个矩阵。对于每个预测，该函数显示要分类的图像；右侧是模型为它所知的每个类别推导出的softmax预测的横向条形图。
- en: Listing 15.13 The `evaluate_model` function for CIFAR-10
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.13 CIFAR-10的`evaluate_model`函数
- en: '[PRE19]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Prepares the images
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备图像
- en: ❷ Shows the image and its predicted class name
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 显示图像及其预测的类别名称
- en: ❸ Shows the bar chart next to the image
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在图像旁边显示条形图
- en: 'Now that you’ve created a model with 80% micro-average ROC accuracy on CIFAR-10,
    you’ve significantly improved on the shallow CNN you built in chapter 14\. The
    good news? Those improvements can also translate to a similar problem: facial
    recognition, which is another image classification problem with a similar construction.
    Instead of providing images containing 10 classes of objects sized 32 × 32 as
    input, you will provide images sized 244 × 244, containing images of 2,622 celebrity
    faces as input, and try to classify the label as one of those 2,622 celebrities.
    Everything you’ve learned thus far in the chapter can help you create the VGG
    -Face model.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经创建了一个在CIFAR-10上具有80%微平均ROC精度的模型，您在第14章构建的浅层CNN上取得了显著改进。好消息是，这些改进也可以转化为类似的问题：人脸识别，这是一个具有类似结构的另一个图像分类问题。您将提供尺寸为244
    × 244的图像作为输入，其中包含2,622位名人的图像，并尝试将这些标签分类为这2,622位名人之一。您在本章中学到的所有内容都可以帮助您创建VGG-Face模型。
- en: 15.5 Building VGG -Face for facial recognition
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 构建用于人脸识别的VGG-Face
- en: The problem of facial recognition has been studied for decades and in recent
    years has become newsworthy for various reasons. In 2015, Oxford’s Visual Geometry
    Group (VGG), fresh from creating deep CNN networks for use in the ImageNet challenge,
    attempted to reapply its CNN network, called VGG, to the problem of celebrity
    facial recognition. The members of the VGG group wrote a seminal paper called
    “Deep Face Recognition” and published the results of their work to identify celebrity
    faces through a deep CNN. The paper is available at [http://mng.bz/MomW](https://shortener.manning.com/MomW).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别问题已经研究了数十年，近年来由于各种原因而成为新闻焦点。2015 年，牛津大学的视觉几何组（VGG），在为 ImageNet 挑战赛创建深度 CNN
    网络后，试图将它的 CNN 网络重新应用于名人面部识别问题。VGG 小组成员撰写了一篇开创性的论文，名为“深度面部识别”，并发布了他们通过深度 CNN 识别名人脸部的成果。该论文可在
    [http://mng.bz/MomW](https://shortener.manning.com/MomW) 获取。
- en: The authors—Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman—built a dataset
    of 2,622 celebrity faces (some of which are shown in figure 15.9) in various poses
    with varying backgrounds. The dataset after initial collection was further culled,
    using human curators to sort through and curate 1,000 URLs for each of the celebrities.
    In the end, the authors created a dataset of 2,622,000 images for use in a deep
    CNN that detected celebrity faces. The network used 13 convolutional filters and
    comprised 37 layers, the last layer being a fully connected layer emitting a softmax
    probability value corresponding to which of the 2,622 celebrities the input image
    corresponds to.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 作者——Omkar M. Parkhi、Andrea Vedaldi 和 Andrew Zisserman——构建了一个包含 2,622 张名人脸部图像的数据集（其中一些如图
    15.9 所示），这些图像具有不同的姿态和背景。在初步收集后，数据集进一步筛选，使用人工整理员对每位名人的 1,000 个 URL 进行排序和整理。最终，作者创建了一个包含
    2,622,000 张图像的数据集，用于在深度 CNN 中检测名人脸部。该网络使用了 13 个卷积滤波器，由 37 层组成，最后一层是一个全连接层，输出一个
    softmax 概率值，对应于输入图像对应于 2,622 位名人中的哪一位。
- en: '![CH15_F09_Mattmann2](../Images/CH15_F09_Mattmann2.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F09_Mattmann2](../Images/CH15_F09_Mattmann2.png)'
- en: Figure 15.9 A few of the faces and poses like those you would see in the VGG
    -Face dataset.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9 一些你在 VGG-Face 数据集中会看到的脸部和姿态。
- en: 'In my attempts to re-create this network for this book, I found several challenges
    that I will summarize for you:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在我尝试为这本书重新创建这个网络的过程中，我发现了一些挑战，我将为您总结：
- en: More than 50% of the data, largely from 2015, doesn’t exist anymore. The VGG
    group published the URLs to the data it used, but the internet moved on, so the
    URLs pointed to images that no longer existed.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过 50% 的数据，主要来自 2015 年，已经不再存在。VGG 小组发布了他们使用的数据的 URL，但互联网已经发展，所以这些 URL 指向的图像已经不存在了。
- en: Collecting the remaining data that existed—around 1,250,000 images—required
    a sophisticated crawling technique, URL validation, and the use of a supercomputer
    over a few weeks, employing both trial and error and manual curation.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集剩余存在的数据——大约 1,250,000 张图像——需要复杂的爬取技术、URL 验证，并在几周内使用超级计算机，结合试错和人工整理。
- en: The resulting data had a mean image sample average of ~477 images per class—much
    less than the original 1,000 images per class that made data augmentation more
    necessary but also reduced its effectiveness.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果数据每个类别的平均图像样本数量约为 ~477 张图像——远少于原始的每个类别 1,000 张图像，这使得数据增强变得更加必要，但也降低了其有效性。
- en: Even this updated VGG -Face Lite dataset that I collected was ~90 GB, which
    is huge, hard to run on a laptop, and doesn’t fit into memory. Also, the size
    of the dataset severely limited the batch-size parameter because laptops, GPUs,
    and even supercomputers don’t have infinite memory.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使我收集的这个更新的 VGG-Face Lite 数据集也有 ~90 GB，这非常庞大，难以在笔记本电脑上运行，而且无法放入内存。此外，数据集的大小严重限制了批处理大小参数，因为笔记本电脑、GPU，甚至超级计算机都没有无限的内存。
- en: Dealing with images of size 244 × 244 and full-color RGB channels requires the
    deep network and its 13 filters to capture the higher- and lower-order features
    needed to distinguish among so many output classes (2,622).
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理大小为 244 × 244 且包含全彩 RGB 通道的图像需要深度网络及其 13 个滤波器来捕捉区分众多输出类别（2,622 个）所需的高阶和低阶特征。
- en: I could list many other issues in collecting an updated version of this dataset
    and in testing and building a deep CNN for it based on the VGG -Face paper, but
    I won’t. Summarizing the data collection issues here wouldn’t add much color other
    than the points I’ve already made about the importance of data cleaning, augmentation,
    and preparation to machine learning.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以列出许多其他问题，包括收集此数据集的更新版本、基于VGG-Face论文测试和构建深度CNN的问题，但我不打算这么做。在这里总结数据收集问题不会增加太多色彩，除了我已经提到的关于数据清洗、增强和准备对机器学习重要性的观点。
- en: Why don’t machine-learning researchers provide their data?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么机器学习研究人员不提供他们的数据？
- en: The short answer is that it’s complicated. It would have been nice to download
    the original 2015 VGG -Face dataset of 2 million images and start training instead
    of having to re-collect only the remaining subset. But there are likely legal
    and other issues related to open data collection and how that data is used. A
    lot of image datasets provide only the image URLs, or if you do get the images,
    they’re a small subset with a lot of legalese to read. This situation makes it
    difficult to reproduce machine-learning models and is a problem that plagues the
    community even today. The only solution is to prepare a well-curated dataset and
    to provide a recipe for re-collecting it.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是，这很复杂。本可以下载原始的2015年VGG-Face数据集（包含200万张图像）并开始训练，而不是不得不重新收集剩余的子集。但可能存在与开放数据收集及其使用相关的法律和其他问题。许多图像数据集只提供图像URL，或者如果你得到了图像，它们是包含大量法律条款的小子集。这种情况使得难以复制机器学习模型，并且至今仍是困扰社区的难题。唯一的解决方案是准备一个精心整理的数据集，并提供重新收集它的配方。
- en: The good news is that I’ve got a dataset that you can use to perform facial
    identification and to build your own version of VGG -Face, which I’ll call VGG
    -Face Lite, a subset of four celebrities that will run on your computer and illustrate
    the architecture. Then I’ll show you how to use the full model to make predictions
    with TensorFlow’s Estimator API.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我已经有一个你可以用来执行面部识别和构建你自己的VGG-Face版本的数据集，我将称之为VGG-Face Lite，这是一个包含四位名人的子集，可以在你的电脑上运行并展示架构。然后我会向你展示如何使用完整的模型通过TensorFlow的Estimator
    API进行预测。
- en: 15.5.1 Picking a subset of VGG -Face for training VGG -Face Lite
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.1 选择VGG-Face的子集进行训练VGG-Face Lite
- en: I randomly selected a set of four celebrities from the updated VGG -Face dataset
    based on average number of samples, trying to find a representative subset of
    features, backgrounds, and learnability for the model. There are other sets of
    four that I could have picked, but for the purposes of getting the model trained,
    this one works fine. I used four random celebrities to train the model. You can
    grab a small subset of the VGG -Face dataset, containing 244 × 244 images, at
    [http://mng.bz/awy7](http://mng.bz/awy7).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我根据平均样本数量从更新的VGG-Face数据集中随机选择了一组四位名人，试图找到模型特征、背景和学习性的代表性子集。我还可以选择其他四组，但为了训练模型，这一组效果很好。我使用了四位随机名人来训练模型。你可以在[http://mng.bz/awy7](http://mng.bz/awy7)获取包含244
    × 244图像的小型VGG-Face数据集子集。
- en: 'The subset has 1,903 total images arranged in directories that contain the
    celebrities’ first and last names concatenated with the underscore character:
    `Firstname_ Lastname`. Unzip the images into a top-level folder called vgg_face.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 该子集包含1,903张总图像，这些图像被组织在包含名人名字首字母和姓氏首字母连接下划线的目录中：`Firstname_Lastname`。将图像解压缩到名为vgg_face的顶级文件夹中。
- en: Earlier in the chapter, you developed functionality for image dataset augmentation
    on your own, using lower-level NumPy functions for introducing salt-and-pepper
    noise and for flipping images left and right. This time, I’ll show you how to
    use TensorFlow’s robust capabilities to do the same thing. I’ll also introduce
    you to TensorFlow’s powerful Dataset API, which provides native capabilities for
    batching, repeating data over epochs, and combining data and labels into a powerful
    construct for learning. Hand-in-hand with the Dataset API is TensorFlow’s support
    for data augmentation through its graph structure, which I’ll show you in section
    15.5.2.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，你开发了用于图像数据集增强的功能，使用较低级别的NumPy函数引入盐和胡椒噪声以及左右翻转图像。这次，我将向你展示如何使用TensorFlow强大的功能来完成同样的事情。我还会介绍TensorFlow的强大Dataset
    API，它提供了批处理、在epoch中重复数据以及将数据和标签组合成强大的学习结构的原生功能。与Dataset API相辅相成的是TensorFlow通过其图结构支持数据增强，我将在15.5.2节中向你展示。
- en: 15.5.2 TensorFlow’s Dataset API and data augmentation
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.2 TensorFlow的Dataset API和数据增强
- en: Whether or not you use TensorFlow’s native constructs to iterate over datasets
    or prepare them for machine learning, or whether you combine functionality from
    libraries such as SK-learn and NumPy, TensorFlow works well with the results for
    training and prediction tasks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是否使用 TensorFlow 的原生结构来迭代数据集或为机器学习准备它们，或者您是否结合了 SK-learn 和 NumPy 等库的功能，TensorFlow
    都能与训练和预测任务的结果很好地协同工作。
- en: It’s worthwhile to explore TensorFlow’s capabilities in this area. TensorFlow
    provides a powerful Dataset API that uses its great properties for lazy evaluation
    and graph-based operations in dataset preparation and processing. These features
    come in handy for batching, data augmentation, epoch management, and other preparation
    and training tasks, which you can perform by making a few calls to the Dataset
    API.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 探索 TensorFlow 在这个领域的功能是值得的。TensorFlow 提供了一个强大的 Dataset API，它利用其优秀的属性进行懒加载评估和基于图的操作，用于数据集的准备和处理。这些功能在批处理、数据增强、周期管理以及其他准备和训练任务中非常有用，您可以通过对
    Dataset API 进行几次调用来完成这些任务。
- en: To begin, you’ll need a TensorFlow dataset. You’ll start preparing this dataset
    by gathering the initial image paths for the 1,903 images and 4 celebrities (listing
    15.14). The images are in the form index_244x244.png and stored in BGR (blue,
    green, red) format.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要一个 TensorFlow 数据集。您将开始准备这个数据集，通过收集 1,903 张图像和 4 位名人的初始图像路径（列表 15.14）。这些图像以
    index_244x244.png 的形式存在，并存储在 BGR（蓝色、绿色、红色）格式中。
- en: Listing 15.14 Gathering image paths for the VGG -Face Lite TensorFlow dataset
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.14 收集 VGG-Face Lite TensorFlow 数据集的图像路径
- en: '[PRE20]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ The four celebrities you will train on
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 您将要训练的四个名人
- en: ❷ Selects all the images in each directory
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择每个目录中的所有图像
- en: ❸ Ignores hidden files and appends images
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 忽略隐藏文件并附加图像
- en: ❹ Shuffles image paths so that the network doesn’t remember the actual order
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打乱图像路径，以便网络不会记住实际顺序
- en: ❺ Counts the images (1,903)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算图像数量（1,903）
- en: With the set of image paths defined and associated labels for the four celebrities,
    you can begin to construct your TensorFlow dataset by using its Dataset API. In
    section 15.1, you wrote a lot of your own data augmentation by using lower-level
    NumPy constructs. This data augmentation code operated on image matrices. Now
    I’ll show you how to create new code to perform the same functionality with TensorFlow’s
    API.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了图像路径集合并关联了四位名人的标签后，您可以使用其 Dataset API 开始构建您的 TensorFlow 数据集。在 15.1 节中，您使用低级
    NumPy 结构编写了大量的数据增强代码。该数据增强代码在图像矩阵上操作。现在，我将向您展示如何使用 TensorFlow API 创建新的代码以执行相同的功能。
- en: TensorFlow provides elegant support for data augmentation. The functions are
    provided by the `tf.image` package. This package includes `tf.image.random_ flip_left_right`
    to flip the images at random; `tf.image_random_brightness` and `tf.image.random_contrast`
    change background hues and perform augmentation similar to salt and pepper, which
    you implemented by hand earlier in the chapter. What’s more, rather than directly
    altering the images and generating new training images that expand your dataset,
    data augmentation provided by TensorFlow’s API is a lazy-evaluated graph construct
    that creates the augmented image only when it’s invoked.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了对数据增强的优雅支持。这些函数由 `tf.image` 包提供。此包包括 `tf.image.random_flip_left_right`
    以随机翻转图像；`tf.image_random_brightness` 和 `tf.image_random_contrast` 改变背景色调，并执行类似于您在章节中手动实现的盐和胡椒增强。更重要的是，TensorFlow
    API 提供的数据增强不是直接修改图像并生成新的训练图像来扩展数据集，而是一个懒加载的图结构，仅在调用时创建增强图像。
- en: You can use TensorFlow’s Dataset API, which includes full support for shuffling,
    batching, and repetition for epochs to arbitrarily provide data augmentation at
    random without creating physical new data to store in memory or on disk. Also,
    the augmentation happens only at run time during training and is deallocated when
    your Python code completes running.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 TensorFlow 的 Dataset API，它包括对打乱、批处理和周期重复的全面支持，以任意方式在随机位置提供数据增强，而无需创建物理新数据存储在内存或磁盘上。此外，增强仅在训练运行时发生，并在您的
    Python 代码运行完成后释放。
- en: To begin using the augmentation capabilities, write a `preprocess_image` function
    that takes in a VGG -Face Lite image sized 244 × 244 in BGR format and returns
    an image tensor to be modified only during execution. You can think of the tensor
    as being the same as the image, but it’s much more powerful. The result is a tensor
    representing a graph of operations that will execute at runtime during training.
    You can also pipeline augmentation techniques together and have TensorFlow run
    them randomly as you iterate through batches and epochs during training.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用增强功能，编写一个 `preprocess_image` 函数，该函数接受一个大小为 244 × 244 的 VGG-Face Lite 图像，以
    BGR 格式返回，并且仅在执行期间修改图像张量。您可以将张量视为与图像相同，但它要强大得多。结果是表示在训练期间运行时执行的操作图的张量。您还可以将增强技术管道化，并在训练过程中迭代批次和周期时由
    TensorFlow 随机运行它们。
- en: 'One other thing that TensorFlow can do is image standardization or cleaning
    to divide by the mean value to ease training. TensorFlow’s `tf.image.per_image_
    standardization` function returns a tensor after invocation. Because tensor operations
    are graphs, you can combine these operations on the original input image. Your
    `preprocess_image` function pipelines the following operations, shown in listing
    15.15:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事是 TensorFlow 可以做的图像标准化或清理，通过除以平均值来简化训练。TensorFlow 的 `tf.image.per_image_standardization`
    函数在调用后返回一个张量。因为张量操作是图，您可以在原始输入图像上组合这些操作。您的 `preprocess_image` 函数将以下操作管道化，如列表 15.15
    所示：
- en: Convert the image to RGB format instead of BGR.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像转换为 RGB 格式而不是 BGR。
- en: Randomly flip the image from left to right.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机翻转图像从左到右。
- en: Randomly apply brightness to the image.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机调整图像的亮度。
- en: Randomly create image contrast (similar to salt and pepper).
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机创建图像对比度（类似于盐和胡椒）。
- en: Randomly rotate the image by 90 degrees.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机旋转图像 90 度。
- en: Apply fixed image standardization and divide by the mean variance of pixels.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用固定的图像标准化并除以像素的平均方差。
- en: Listing 15.15 Image dataset augmentation with TensorFlow
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.15 使用 TensorFlow 进行图像数据集增强
- en: '[PRE21]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Images are stored in files with BGR to convert to RGB.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 图像存储在文件中，以 BGR 格式存储，需要转换为 RGB。
- en: ❷ Resizes the image to 244 × 244
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将图像调整大小到 244 × 244
- en: ❸ Applies random flip (L-R), brightness, contrast, and rotation to the image,
    using tensor graph
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用张量图对图像应用随机翻转（左右）、亮度、对比度和旋转。
- en: ❹ Fixed standardization for the image and subtract off the mean and divide by
    the variance of the pixels
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对图像进行固定的标准化，减去平均值并除以像素的方差
- en: 'With your image data augmentation `function preprocess_image` returning a tensor
    graph of operations to apply during training, you’re nearly ready to create your
    Tensor-Flow dataset. First, you need to split the input data into training and
    testing sets, using a 70/30 split:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的图像数据增强 `function preprocess_image` 返回一个在训练期间应用的操作张量图时，您几乎准备好创建您的 Tensor-Flow
    数据集了。首先，您需要将输入数据分割成训练集和测试集，使用 70/30 的比例：
- en: '[PRE22]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can use `get_training_and_testing_sets` to split your image path list into
    a 70/30 split, with 70% of the images to be used for training and the other 30%
    for testing. You’ll also need to prepare your labels and the image paths to construct
    the overall dataset. One easy way to do that is to iterate the folders that correspond
    to the celebrity names and then assign each celebrity name an index from `0` to
    `4`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `get_training_and_testing_sets` 将您的图像路径列表分割成 70/30 的比例，其中 70% 的图像用于训练，其余
    30% 用于测试。您还需要准备您的标签和图像路径来构建整个数据集。一个简单的方法是遍历对应名人名称的文件夹，然后为每个名人名称分配一个从 `0` 到 `4`
    的索引：
- en: '[PRE23]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, you can generate your image paths and your labels for training and
    testing by calling the `get_training_and_testing_sets` function to split them:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以通过调用 `get_training_and_testing_sets` 函数来分割它们，从而生成您的图像路径和训练和测试标签：
- en: '[PRE24]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now you are ready to create your TensorFlow dataset.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经准备好创建您的 TensorFlow 数据集了。
- en: 15.5.3 Creating a TensorFlow dataset
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.3 创建 TensorFlow 数据集
- en: 'Datasets in TensorFlow are also lazily-executed graphs of operations that can
    be constructed in various ways. One easy way is to provide a set of existing data
    slices that can be operated over to yield a new dataset tensor. If you provide
    `train_paths` as input to TensorFlow’s `tf.data.Dataset.from_tensor_slices` function,
    for example, the function will yield a TensorFlow `Dataset` object: a graph of
    operations that will execute at runtime and provide the wrapped paths to images.
    Then, if you pass that `Dataset` object to the `tf.data.Dataset.map` function,
    you can further construct your TensorFlow `Dataset` graph as follows.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的数据集也是可以以各种方式构建的延迟执行的运算图。一种简单的方法是提供一组现有的数据切片，可以操作以生成新的数据集张量。例如，如果你将
    `train_paths` 作为输入传递给 TensorFlow 的 `tf.data.Dataset.from_tensor_slices` 函数，该函数将生成一个
    TensorFlow `Dataset` 对象：一个在运行时执行的运算图，并提供包装的图像路径。然后，如果你将那个 `Dataset` 对象传递给 `tf.data.Dataset.map`
    函数，你可以进一步构建你的 TensorFlow `Dataset` 图如下。
- en: The `tf.data.Dataset.map` function takes as input a function to run in parallel
    across each iterable item in the dataset, so you can use the `preprocess_image`
    function from listing 15.15\. That function returns another `Dataset` object that
    corresponds to the image paths that have been run through the data augmentation
    operations.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Dataset.map` 函数接受一个函数作为输入，该函数将在数据集的每个可迭代项上并行运行，因此你可以使用列表 15.15 中的
    `preprocess_image` 函数。该函数返回另一个 `Dataset` 对象，对应于经过数据增强操作运行过的图像路径。'
- en: Remember operations such as image flipping, random brightness, contrast, and
    random rotation? Giving `tf.data.Dataset.map` a copy of the `preprocess_image`
    function creates a graph of operations to be applied to each image path in the
    `Dataset`. Finally, the TensorFlow `Dataset` API provides a `zip` method that
    combines two datasets, with each entry being an enumeration of each item pair
    from the two datasets.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 记得图像翻转、随机亮度、对比度和随机旋转等操作吗？将 `preprocess_image` 函数的副本提供给 `tf.data.Dataset.map`
    会创建一个操作图，该图将应用于 `Dataset` 中的每个图像路径。最后，TensorFlow `Dataset` API 提供了一个 `zip` 方法，该方法结合两个数据集，每个条目都是来自两个数据集的每个项目对的枚举。
- en: Again, all these operations are lazily executed, so you are building up a graph
    of operations to execute only when you iterate or perform some other operation
    on the dataset in a TensorFlow session. Figure 15.10 shows the resulting Dataset
    pipeline that combines data augmentation on the input images from the VGG -Face
    paths and their labels, which are the directory names that contain the images.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，所有这些操作都是延迟执行的，因此你正在构建一个操作图，仅在迭代或在对数据集执行 TensorFlow 会话中的某些其他操作时才会执行。图 15.10
    显示了结果数据集管道，该管道结合了来自 VGG-Face 路径的输入图像的数据增强以及它们的标签（包含图像的目录名称）。
- en: '![CH15_F10_Mattmann2](../Images/CH15_F10_Mattmann2.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F10_Mattmann2](../Images/CH15_F10_Mattmann2.png)'
- en: Figure 15.10 The TensorFlow `Dataset` API pipeline to combine the VGG -Face
    image paths with data augmentation and the image labels (the directories that
    contain each image from the path)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10 TensorFlow `Dataset` API 管道，将 VGG-Face 图像路径与数据增强和图像标签（包含路径中每个图像的目录）结合
- en: The code in listing 15.16 implements the process shown in figure 15.10, creating
    a train and test TensorFlow dataset for VGG -Face images and labels named `train_
    image_label_ds` and `val_image_label_ds``,` respectively. You will use these datasets
    during the training process, which I’ll explain in section 15.5.4\. The Dataset
    API will come in handy for training too, because operations that you had to implement
    by hand before—such as batching, prefetching, and repetition during epochs—are
    provided natively by TensorFlow.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.16 中的代码实现了图 15.10 中所示的过程，创建了名为 `train_image_label_ds` 和 `val_image_label_ds`
    的 TensorFlow 训练和测试数据集，用于 VGG-Face 图像和标签。你将在训练过程中使用这些数据集，我将在第 15.5.4 节中解释。数据集 API
    在训练过程中也非常有用，因为之前必须手动实现的操作，如批处理、预取和在纪元中的重复，都由 TensorFlow 本地提供。
- en: Listing 15.16 Creating the VGG -Face val and train datasets
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.16 创建 VGG-Face 验证和训练数据集
- en: '[PRE25]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Divides into 70/30 train/test split for the input image paths and input labels
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将输入图像路径和输入标签分为 70/30 的训练/测试分割
- en: ❷ Creates initial Dataset from the mage paths for train and test
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从图像路径创建初始数据集用于训练和测试
- en: ❸ Executes the map function to create a new Dataset that applies the data augmentation
    steps to train and test images
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行 `map` 函数以创建一个新的 `Dataset`，该 `Dataset` 将数据增强步骤应用于训练和测试图像
- en: ❹ Creates a dataset by casting the train and test labels to int64 values
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过将训练和测试标签转换为 int64 值创建数据集
- en: ❺ Zips the augmented image data and the labels into datasets for train and val/test
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将增强后的图像数据和标签压缩成训练和验证/测试数据集
- en: If you were to inspect the results of applying the data augmentation, you might
    see a randomly flipped image of celebrity A, a high-contrast black-and-white photo
    of celebrity B, or perhaps a slightly rotated picture of celebrity C. Some of
    these augmentations are shown in figure 15.11.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您检查数据增强的结果，可能会看到名人 A 的随机翻转图像、名人 B 的高对比度黑白照片，或者名人 C 的轻微旋转图片。其中一些增强在图 15.11
    中展示。
- en: '![CH15_F11_Mattmann2](../Images/CH15_F11_Mattmann2.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F11_Mattmann2](../Images/CH15_F11_Mattmann2.png)'
- en: Figure 15.11 Results of data augmentation with the TensorFlow Dataset API
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11 使用 TensorFlow Dataset API 进行数据增强的结果
- en: Now that you have your TensorFlow `Dataset` graph prepared, it’s time to configure
    your dataset with typical training hyperparameters, such as batch size and shufflings.
    The cool part about the `Dataset` API is that you do this all beforehand by setting
    properties on the `Dataset` object to execute during training.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好了 TensorFlow `Dataset` 图，是时候配置您的数据集，使用典型的训练超参数，例如批量大小和洗牌。关于 `Dataset`
    API 的酷之处在于，您可以在训练之前通过在 `Dataset` 对象上设置属性来执行这些操作。
- en: 15.5.4 Training using TensorFlow datasets
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.4 使用 TensorFlow 数据集进行训练
- en: With the TensorFlow dataset graph you created for VGG -Face, you’ve got a combined
    lazily-executable graph representing your data augmentation operations that will
    execute only when you iterate and realize each entry in the dataset with a TensorFlow
    session. The power of the dataset API shows itself during the training and setup
    process.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您为 VGG-Face 创建的 TensorFlow 数据集图，您得到了一个表示数据增强操作的组合惰性可执行图，它只会在您使用 TensorFlow
    会话迭代并实现数据集中每个条目时执行。数据集 API 的强大之处在于它在训练和设置过程中显现出来。
- en: Because you’ve got a dataset, you can perform explicit operations on it, such
    as defining ahead of time what batch size you want in each iteration when you
    perform training. You can also define ahead of time that you would like to shuffle
    the dataset so that you are guaranteed to get the dataset in a different order
    in each epoch. You do this so that the network doesn’t memorize the order of the
    dataset, which can happen as it tries to optimize the weights. Seeing the same
    image in the same order may never allow the train operation to achieve a particular
    optimization step during backpropagation and the weight updates it needs to achieve
    an optimal result. So you can turn on shuffling in your dataset ahead of time
    (listing 15.17). You can also tell your dataset to repeat a certain number of
    times, obviating the need to use a loop for epochs. The power of the TensorFlow
    Dataset API is front and center in listing 15.17.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您有一个数据集，您可以对它执行显式操作，例如在执行训练之前定义每个迭代中想要的批量大小。您还可以提前定义您希望洗牌数据集，以确保在每个纪元中都能以不同的顺序获得数据集。这样做是为了确保网络不会记住数据集的顺序，这可能会在它尝试优化权重时发生。在反向传播中，以相同的顺序看到相同的图像可能永远无法使训练操作在反向传播期间达到特定的优化步骤，以及它需要更新的权重以实现最佳结果。因此，您可以在数据集之前打开洗牌（列表
    15.17）。您还可以告诉数据集重复一定次数，从而无需使用循环进行纪元。TensorFlow Dataset API 的强大之处在列表 15.17 中得到了充分体现。
- en: The listing also sets up the Dataset API to use a batch size of 128\. The more
    images you have per batch, the more memory your CPU and GPU (if you have one)
    will use, so you’ll have to play with this number. Additionally, the more images
    you have per batch, the less randomness and the fewer chances the training operation
    will have to update the weights for the learned parameters in each epoch. You’ll
    shuffle the dataset by using a buffer size, which is the length of the input,
    ensuring that the whole dataset is shuffled only once in each epoch. Finally,
    you prefetch the data on your dataset, which allows it to gather data during the
    training operation when the graph is finally executing, optimizing, reducing I/O
    waiting, and taking advantage of TensorFlow’s parallelism. All these features
    are possible because of the `Dataset` API.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 列表还设置了数据集 API 使用 128 的批量大小。每个批量中图像越多，您的 CPU 和 GPU（如果您有的话）将使用的内存就越多，所以您需要调整这个数字。此外，每个批量中图像越多，随机性越少，训练操作更新每个纪元中学习参数的权重的机会就越少。您将通过使用缓冲区大小来洗牌数据集，这是输入的长度，确保整个数据集在每个纪元中只洗牌一次。最后，您在数据集上预取数据，这允许它在图最终执行、优化、减少
    I/O 等待并利用 TensorFlow 的并行性时收集数据。所有这些功能都是由于 `Dataset` API 的存在。
- en: With the datasets created for training and validation, it’s time to build the
    model for VGG -Face Lite.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在为训练和验证创建数据集后，是时候为VGG-Face Lite构建模型了。
- en: Listing 15.17 Preparing the VGG -Face TensorFlow dataset for training
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.17 准备VGG-Face TensorFlow数据集进行训练
- en: '[PRE26]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Shuffles the entire dataset of training images and validation images for testing
    during each epoch
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在每个epoch期间对训练图像和验证图像的整个数据集进行洗牌
- en: ❷ Uses the batch size of 128 images/labels during training, ensuring ~11 epochs
    because there are 1,903 images and 70% are used for training
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在训练期间使用128个图像/标签的批量大小，确保大约11个epoch，因为有1,903个图像，其中70%用于训练
- en: ❸ Uses the remaining 30% of images for validation and batches the whole set
    for validation
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用剩余的30%图像进行验证，并将整个集合分批进行验证
- en: ❹ Prefetch lets the dataset fetch batches in the background while the model
    is training.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Prefetch允许数据集在模型训练的同时在后台获取批次。
- en: Now that you’ve parameterized your TensorFlow `Dataset` graph for training,
    you’ll get to the actual running of the training process. Follow me to section
    15.5.5!
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经为训练参数化了TensorFlow `Dataset`图，你将进入实际的训练过程。跟随我到15.5.5节！
- en: 15.5.5 VGG -Face Lite model and training
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.5 VGG-Face Lite模型和训练
- en: The full VGG -Face model includes 37 layers and is a deep network model taking
    gigabytes of memory to load the model graph after training for predictions. If
    I were sure that you had access to supercomputing and cloud resources, we’d reimplement
    the model. But I’m not sure, so instead, we’ll chop off a few of the filters and
    layers for something that you’ll be able to train in about a day on your laptop.
    Even without a GPU, the model will perform with exceptional accuracy for the four
    celebrity faces.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的VGG-Face模型包括37层，是一个深度网络模型，在训练后需要数GB的内存来加载模型图以进行预测。如果我相信你有超级计算和云资源，我们会重新实现这个模型。但我不确定，所以我们将在你的笔记本电脑上大约一天内可以训练的模型中删除一些滤波器和层。即使没有GPU，该模型在四个名人脸上的表现也将非常准确。
- en: VGG -Face Lite uses five convolutional filters and is a 10-layer deep network
    that takes advantages of the some optimizations discussed in this chapter, such
    as batch normalization. One other thing you can do to speed training and learning
    is to rescale your image sizes to 64 × 64\. This rescaling reduces the amount
    of learning that your model must do by decreasing the input pixels by a factor
    of ~4\. You can bet that if the computer program can learn the differentiation
    in smaller-scale images, you can start to scale it up to larger ones. The output
    of the CNN model is which of the four celebrity-face classes the input image corresponds
    to.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-Face Lite 使用五个卷积滤波器，是一个10层的深度网络，它利用了本章讨论的一些优化，例如批量归一化。为了加速训练和学习，你可以将图像大小重新缩放为64
    × 64。这种缩放通过将输入像素减少约4倍来减少模型必须学习的数量。你可以确信，如果计算机程序可以在小规模图像中学习微分，你就可以开始将其扩展到更大的图像。CNN模型的输出是输入图像对应于四个名人脸类的哪一个。
- en: The model architecture is shown in listing 15.18\. The first portion defines
    the convolutional filters in full RGB three-channel space and then uses 64 convolutional
    filters, followed by 64, 128, 128, and 256 filters for learning. These filters
    correspond to the 4D parameter in convolution filters `conv1_2` through `conv3_1`
    in listing 15.18\. The output fully connected layer is 128 neurons, which are
    mapped to the four output classes via a softmax in the final output. In the first
    input filter, the third parameter is the RGB 3 channel because you will use color
    images.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构在列表15.18中展示。第一部分在完整的RGB三通道空间中定义卷积滤波器，然后使用64个卷积滤波器，接着是64、128、128和256个滤波器用于学习。这些滤波器对应于列表15.18中卷积滤波器的4D参数`conv1_2`到`conv3_1`。输出全连接层有128个神经元，通过最终的softmax映射到四个输出类别。在第一个输入滤波器中，第三个参数是RGB
    3通道，因为你会使用彩色图像。
- en: Listing 15.18 The VGG -Face Lite model
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.18 VGG-Face Lite模型
- en: '[PRE27]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Defines convolutional filters (five of them)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义卷积滤波器（五个）
- en: ❷ Defines the model function for VGG -Face Lite
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义VGG-Face Lite的模型函数
- en: ❸ Uses dropout only in the last layer
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 只在最后一层使用dropout
- en: ❹ Returns the logits of the model
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回模型的logits
- en: With the model defined, you can move on to set up the hyperparameters for training.
    You can use hyperparameters similar to those in your CIFAR-10 object recognition
    model. In practice, you would experiment with these hyperparameters to obtain
    the optimal values. But for the purposes of this example, these parameters should
    allow you to complete training in about a day.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义完成后，你可以继续设置训练的超参数。你可以使用与你的CIFAR-10物体识别模型类似的超参数。在实践中，你会对这些超参数进行实验以获得最优值。但为了本例的目的，这些参数应该允许你在大约一天内完成训练。
- en: One new hyperparameter to try is exponential weight decay, which uses the overall
    global training epoch step as an influence factor in decreasing the learning weight.
    Over time, your network will make smaller learning steps and try to hone in on
    an optimal value. Combined with the `ADAMOptimizer`, weight decay has been shown
    to help CNNs converge on optimal learning parameters. TensorFlow provides easy-to-use
    optimizers that you can experiment with. Associated techniques such as weight
    decay are fairly simple to test with the framework, as shown in listing 15.19.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以尝试的新超参数是指数权重衰减，它使用整体全局训练epoch步长作为降低学习权重的因素。随着时间的推移，你的网络将做出更小的学习步长，并试图聚焦于一个最优值。结合`ADAMOptimizer`，权重衰减已被证明有助于CNN收敛到最优学习参数。TensorFlow提供了易于使用的优化器，你可以进行实验。与权重衰减相关的技术，如本例中15.19列表所示，在框架中测试起来相当简单。
- en: Listing 15.19 Setting up the hyperparameters for VGG -Face Lite model training
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.19 设置VGG-Face Lite模型训练的超参数
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ N three-channel RGB images of input N × 64 × 64 × 3
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入N张三通道RGB图像 N × 64 × 64 × 3
- en: ❷ Output of length N images N × 4 classes
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 长度为N的图像输出 N × 4个类别
- en: ❸ Uses 0.5 dropout per the deep facial recognition paper
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据深度面部识别论文使用0.5的dropout
- en: ❹ Names logits tensor so that can be loaded from disk after training
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将logits张量命名为，以便在训练后从磁盘加载
- en: ❺ Implements L2 regularization
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 实现L2正则化
- en: ❻ Uses exponential weight decay to set learning rate
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用指数权重衰减来设置学习率
- en: The model definition and hyperparameters for implementing facial recognition
    are similar to those used for CIFAR-10 object detection. Whether you are trying
    to build CNN architectures to learn facial features or object features, the same
    techniques apply. You create deeper networks by adding filters and layers, experimenting
    with rescaling and sizing the images. You can use dropout to enable more-resilient
    architectures and data augmentation to take static datasets and create new data.
    Augmentation is per-formed with TensorFlow’s powerful `Dataset` API.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 实现面部识别的模型定义和超参数与用于CIFAR-10物体检测的类似。无论你是在尝试构建CNN架构来学习面部特征还是物体特征，相同的技巧都适用。你通过添加滤波器和层来创建更深的网络，实验图像的缩放和尺寸。你可以使用dropout来启用更健壮的架构，并使用数据增强将静态数据集转换为新的数据。增强是通过TensorFlow强大的`Dataset`
    API实现的。
- en: In section 15.5.6, you’ll train the network and learn something new, watching
    the training for early stopping by performing validation accuracy checks on unseen
    data for training every few epochs. This technique will give you better understanding
    during network training and help you understand the influence of validation accuracy
    and loss.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在15.5.6节中，你将训练网络并学习一些新知识，通过在每几个epoch对训练中的未见数据进行验证准确度检查来实现早期停止。这种技术将在网络训练过程中给你带来更好的理解，并帮助你理解验证准确度和损失的影响。
- en: 15.5.6 Training and evaluating VGG -Face Lite
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.6 训练和评估VGG-Face Lite
- en: During training, one optimization you can do is using validation loss instead
    of training accuracy to measure how well your model is converging. The theory
    is simple. If you separate your train and validation data—perhaps using a 70/30
    split, as you are for VGG -Face Lite—your validation loss should decrease while
    your train accuracy increases. Modeling train and validation loss are the common
    convex intersecting curves that you’ve probably seen when people try to explain
    deep learning. The top-right descending curve is the validation loss, and the
    bottom-left curve ascending in a polynomial or exponential path is the training
    accuracy.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，你可以进行的一种优化是使用验证损失而不是训练准确度来衡量你的模型收敛得有多好。理论很简单。如果你将训练数据和验证数据分开——比如使用70/30的分割，就像你在VGG-Face
    Lite中所做的那样——你的验证损失应该下降，而你的训练准确度应该上升。建模训练和验证损失是当你看到人们试图解释深度学习时可能看到的常见凸交曲线。右上角下降的曲线是验证损失，而左下角曲线以多项式或指数路径上升的是训练准确度。
- en: 'You can measure your validation loss by testing it and printing it every so
    often during training epochs. The code in listing 15.20 prints the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在训练周期中测试它并偶尔打印它来测量您的验证损失。列表15.20中的代码打印以下内容：
- en: Validation loss and accuracy every five epochs
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每5个周期验证损失和准确度
- en: Training accuracy every epoch (to give you a feel for how your model is performing)
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个周期的训练准确度（以了解您的模型性能）
- en: Note that training the model will likely take up to 36 hours on a CPU laptop
    and a few hours on a machine with a GPU.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在CPU笔记本电脑上训练模型可能需要长达36小时，而在具有GPU的机器上只需几小时。
- en: Another important point in the listing is the use of the TensorFlow `Dataset`
    API. You create an iterator with the `make_one_shot_iterator()` function, which
    uses the preset parameters for batch size and a prefetch buffer to consume a batch
    of data during each iteration. The other thing to notice is that you loop `while
    True` for your training; during each epoch, the iterator will consume the entire
    batch set and then throw a `tf.errors.OutOfRangeError` that you catch to break
    the `while True` loop and move on to the next epoch.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中另一个重要点是使用TensorFlow `Dataset` API。您使用`make_one_shot_iterator()`函数创建一个迭代器，该函数使用预设的批次大小和预取缓冲区，在每次迭代中消耗一批数据。要注意的另一件事是，您在训练中使用`while
    True`循环；在每个周期中，迭代器将消耗整个批次集，然后抛出一个`tf.errors.OutOfRangeError`，您捕获它以跳出`while True`循环并进入下一个周期。
- en: The validation batch size is the full size of the set during training. During
    each training epoch, you get a batch size of 128 images, which you configured
    in listing 15.17\. The code will also handle validation every five epochs and
    save the model checkpoint during that time by getting a list of all the file paths
    and iterating over that list, removing the previous checkpoint files before saving
    the new model checkpoint.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 验证批次大小在训练期间为集合的全尺寸。在每次训练周期中，您会得到128个图像的批次大小，这是您在列表15.17中配置的。代码还会在每五个周期进行验证，并在那时通过获取所有文件路径的列表并遍历该列表来保存模型检查点，在保存新模型检查点之前删除之前的检查点文件。
- en: Listing 15.20 Training VGG -Face Lite
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.20 训练VGG-Face Lite
- en: '[PRE29]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Creates a saver for your model
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为您的模型创建一个保存器
- en: ❷ Loops over 1,000 epochs
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 循环1000个周期
- en: ❸ Makes one_shot_iterators for the val and train datasets
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为验证和训练数据集创建one_shot_iterators
- en: ❹ Training batch size 128 images per batch
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练批次大小为每批次128个图像
- en: ❺ Gets one-hot labels for train and validation
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取训练和验证的一热标签
- en: ❻ Every five steps, measure validation loss and accuracy.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 每5步测量验证损失和准确度。
- en: ❼ Save the new model checkpoint.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存新的模型检查点。
- en: Next, I’ll show you how to make predictions with the model and how to evaluate
    it.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将向您展示如何使用模型进行预测以及如何评估它。
- en: 15.5.7 Evaluating and predicting with VGG -Face Lite
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.7 使用VGG-Face Lite进行评估和预测
- en: You can take the trained output model and construct a `predict` function for
    input facial images (listing 15.21), reusing code from listing 15.8\. You load
    the graph and its logits and then ensure that the input image is sized to `IMAGE_SIZE`
    (64 × 64 × 3) for three-channel RGB. The output class name and class number of
    the most confident of the four celebrities are emitted by the function, along
    with the softmax confidence for all predictions and their values.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将训练好的输出模型用于构建一个用于输入面部图像的`predict`函数（列表15.21），重用列表15.8中的代码。您加载图和其logits，并确保输入图像的大小为`IMAGE_SIZE`（64
    × 64 × 3），用于三通道RGB。函数会输出四个名人中最有信心的人的类别名称和类别编号，以及所有预测的softmax置信度和它们的值。
- en: Listing 15.21 Predicting with VGG -Face Lite
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.21 使用VGG-Face Lite进行预测
- en: '[PRE30]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Loads the graph
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载图
- en: ❷ Reshapes the input image to 1 × 64 × 64 × 3
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将输入图像重塑为1 × 64 × 64 × 3
- en: ❸ Applies the logits to the input image and gets the softmax
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将logits应用于输入图像并获取softmax
- en: ❹ Returns the highest predicted class number, name, confidence, and all the
    logits
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回最高的预测类别编号、名称、置信度和所有logits
- en: As with CIFAR-10, you can run your `predict` function over the entire validation
    dataset to evaluate loss and accuracy during training. You can build a `get_test_accuracy`
    for VGG -Face that is also a duplicate of listing 15.8, except for the different
    model name used during loading. Using that function for VGG -Face shows a test
    accuracy of 97.37%, which is amazing across the four classes of celebrity faces.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 与CIFAR-10一样，您可以在整个验证数据集上运行您的`predict`函数以在训练期间评估损失和准确度。您可以为VGG-Face构建一个`get_test_accuracy`函数，它也是列表15.8的副本，除了在加载时使用的不同模型名称。使用该函数为VGG-Face显示的测试准确度为97.37%，这在四个名人面部类别中是非常惊人的。
- en: You can use `predict` and `get_test_accuracy` to generate the ROC curve across
    all four celebrity classes and evaluate your model performance, using the code
    in listing 15.22\. This listing is similar to listing 15.10 except that VGG -Face
    Lite has four output classes instead of ten. The output shown in figure 15.12
    indicates a 98% micro-average ROC and exceptional performance for your first deep
    CNN for facial recognition.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`predict`和`get_test_accuracy`生成所有四个名人类别的ROC曲线，并使用列表15.22中的代码评估您的模型性能。此列表与列表15.10类似，但VGG-Face
    Lite有四个输出类别而不是十个。图15.12中显示的输出表明，对于您第一个用于面部识别的深度CNN，微平均ROC达到了98%，表现卓越。
- en: '![CH15_F12_Mattmann2](../Images/CH15_F12_Mattmann2.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![CH15_F12_Mattmann2](../Images/CH15_F12_Mattmann2.png)'
- en: Figure 15.12 ROC curve for VGG -Face Lite.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.12 VGG-Face Lite的ROC曲线。
- en: Listing 15.22 Generating the VGG -Face Lite ROC curve
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.22 生成VGG-Face Lite ROC曲线
- en: '[PRE31]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Computes ROC curve and ROC area for each class
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算每个类的ROC曲线和ROC面积
- en: ❷ Computes micro-average ROC curve and ROC area
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算微平均ROC曲线和ROC面积
- en: ❸ Plots ROC curve
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制ROC曲线
- en: One final function you can borrow from earlier in the chapter is the `evaluate_
    model()` function. For VGG -Face, the function is slightly different because you
    won’t use data from the internet; you can use your validation dataset. But this
    function is valuable in that you can see how confident your model is in each class
    prediction. The output is generated by the function shown in listing 15.23.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从本章早期借用的一个最终函数是`evaluate_model()`函数。对于VGG-Face，该函数略有不同，因为您不会使用互联网上的数据；您可以使用您的验证数据集。但这个函数很有价值，因为它可以显示您的模型在每个类别预测中的信心程度。输出由列表15.23中显示的函数生成。
- en: Listing 15.23 Evaluating VGG -Face Lite with validation images
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.23 使用验证图像评估VGG-Face Lite
- en: '[PRE32]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Number of output classes
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输出类别数量
- en: ❷ Iterates over the predictions and shows the image on the left and the softmax
    predictions on the right
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历预测并显示左侧的图像和右侧的softmax预测
- en: Phew! This chapter has been a lot of work. Now that you’ve applied CNNs to object
    recognition, facial recognition, and facial detection, I’m sure that you can think
    of other similar problems to try them on. You don’t need faces or objects; there
    are plenty of other things to train and predict. You’ve got all the tools necessary!
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 呼吁！这一章做了很多工作。现在您已经将CNN应用于物体识别、面部识别和面部检测，我相信您可以想到其他类似的问题来尝试。您不需要面孔或物体；还有很多其他东西可以训练和预测。您已经拥有了所有必要的工具！
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: CNNs can be used for general image-matching problems and building facial identification
    systems, but unless you apply optimizations in the real world, they will not perform
    well.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN可以用于通用图像匹配问题和构建面部识别系统，但除非您在现实世界中应用优化，否则它们的表现不会很好。
- en: Training a CNN without applying optimizations such as dropout, deeper architectures,
    and image augmentation leads to overfitting, and models won’t perform well on
    unseen data.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不应用诸如dropout、更深层次架构和图像增强等优化措施的情况下训练CNN会导致过拟合，并且模型在未见过的数据上表现不佳。
- en: TensorFlow provides functions for augmenting image data and for preventing memorization
    in CNN architectures using the dropout technique, as well as APIs for scanning
    datasets and preparing them for training to make creating real-world CNNs simple.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow提供了增强图像数据的功能，以及使用dropout技术防止CNN架构中记忆化的API，还有用于扫描数据集并准备训练数据的API，使得创建现实世界的CNN变得简单。
