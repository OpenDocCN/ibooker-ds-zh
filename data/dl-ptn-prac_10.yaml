- en: 8 Mobile convolutional neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8个移动卷积神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Understanding the design principles and unique requirements for mobile convolutional
    networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解移动卷积网络的设计原则和独特要求
- en: Examining the design patterns for MobileNet v1 and v2, SqueezeNet, and ShuffleNet
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查MobileNet v1和v2、SqueezeNet和ShuffleNet的设计模式
- en: Coding examples of these models by using the procedural design pattern
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用过程设计模式对这些模型的编码示例
- en: Making models more compact by quantizing models and then executing them using
    TensorFlow Lite (TF Lite)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过量化模型并在TensorFlow Lite（TF Lite）中执行它们来使模型更加紧凑
- en: You have now learned several key design patterns for large models without memory
    constraints. Now let’s turn to design patterns such as the popular FaceApp from
    Facebook that are optimized for memory-constrained devices, such as mobile phones
    and IoT devices.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经学习了几个大型模型的无内存约束关键设计模式。现在让我们转向设计模式，例如来自Facebook的流行应用FaceApp，这些模式针对内存受限设备，如手机和物联网设备进行了优化。
- en: 'In contrast to their PC or cloud equivalents, compact models have a special
    challenge: they need to operate in substantially less memory, and therefore cannot
    benefit from the use of overcapacity to achieve high accuracy. To fit into these
    constrained memory sizes, models need to have substantially fewer parameters for
    inference or prediction. The architecture for compact models relies on a tradeoff
    between accuracy and latency. The more of the device’s memory the model occupies,
    the higher the accuracy, but the longer the response time latency.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与它们的PC或云等价物相比，紧凑模型面临一个特殊挑战：它们需要在显著更少的内存中运行，因此不能从使用过容量以实现高精度中受益。为了适应这些受限的内存大小，模型在推理或预测时需要显著减少参数。紧凑模型的架构依赖于精度和延迟之间的权衡。模型占用的设备内存越多，精度越高，但响应时间延迟越长。
- en: In early SOTA mobile convolutional models, researchers found ways to address
    this tradeoff with methods that substantially reduced parameters and computational
    complexity while maintaining minimal loss of accuracy. Those methods relied on
    further refactoring of convolutions, such as the depthwise separable convolution
    (MobileNet) and pointwise group convolutions (ShuffleNet). These refactoring techniques
    provided the means to increase capacity for accuracy, which would otherwise be
    lost by the more extreme refactoring methods.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的SOTA移动卷积模型中，研究人员找到了通过方法来解决这种权衡，这些方法在大幅减少参数和计算复杂性的同时，保持了最小程度的精度损失。这些方法依赖于对卷积的进一步重构，例如深度可分离卷积（MobileNet）和点卷积组卷积（ShuffleNet）。这些重构技术提供了增加容量以提高精度的手段，否则这些更极端的重构方法会导致精度损失。
- en: 'This chapter presents two of those refactoring approaches, used in two different
    models: MobileNet and SqueezeNet. We’ll also look at another novel approach for
    memory-constrained devices in a third model, ShuffleNet. We’ll finish the chapter
    by exploring other strategies to further reduce the memory footprint, such as
    parameter compression and quantization to make models more compact.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了两种重构方法，这些方法用于两种不同的模型：MobileNet和SqueezeNet。我们还将探讨第三种模型ShuffleNet中针对内存受限设备的另一种新颖方法。我们将在本章结束时探讨其他策略，以进一步减少内存占用，例如参数压缩和量化，以使模型更加紧凑。
- en: Before we start looking at the particulars of the three models, let me briefly
    compare the methods they use to deal with limited memory. MobileNet’s researchers
    explored strategies of thinning the model to adjust to various memory sizes and
    latency requirements, and the effects that thinning had on accuracy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始研究这三个模型的细节之前，让我简要比较一下它们处理有限内存的方法。MobileNet的研究人员探索了通过调整模型以适应各种内存大小和延迟要求，以及这种调整对精度的影响。
- en: SqueezeNet researchers proposed a block pattern, known as a *fire module*, that
    would sustain accuracy after the model size is reduced by up to 90%. The fire
    module uses deep compression. This method for compressing the size of a neural
    network was introduced in “Deep Compression,” by Song Han et al. ([https://arxiv.org/abs/
    1510.00149](https://arxiv.org/abs/1510.00149)) presented at the 2015 International
    Conference on Learning Representations (ICLR).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SqueezeNet研究人员提出了一种称为“fire模块”的块模式，该模式在模型大小减少高达90%后仍能保持精度。fire模块使用深度压缩。这种压缩神经网络大小的方法在Song
    Han等人撰写的《深度压缩》一文中被介绍，该文于2015年国际学习表示会议（ICLR）上提出（[https://arxiv.org/abs/1510.00149](https://arxiv.org/abs/1510.00149)）。
- en: 'Meanwhile, ShuffleNet researchers focused on increasing representational power
    for models deployed on extremely low-power computational devices (for example,
    10 to 150 MFLOPs). They proposed a twofold approach: a channel shuffle within
    a highly factorized group, along with pointwise convolution.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，ShuffleNet研究人员专注于为部署在极低功耗计算设备上的模型（例如，10到150 MFLOPs）增加表示能力。他们提出了两种方法：在一个高度分解的组内进行通道洗牌，以及点卷积。
- en: Now we can get into the details of each.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以深入了解每个细节。
- en: 8.1 MobileNet v1
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 MobileNet v1
- en: '*MobileNet v1* is an architecture introduced by Google in 2017 for producing
    smaller networks that can fit on mobile and IoT devices, while maintaining accuracy
    close to their larger network counterparts. The MobileNet v1 architecture, explained
    in “MobileNets” by Andrew G. Howard et al. ([https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)),
    replaces normal convolutions with depthwise separable convolutions to further
    reduce computational complexity. (As you remember, we covered the theory behind
    the refactoring of normal convolutions into depthwise separable convolutions when
    we looked at the Xception model in chapter 7.) Let’s see how MobileNet put this
    approach to work on a compact model.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*MobileNet v1*是谷歌在2017年推出的一种架构，用于生成更小的网络，可以适应移动和物联网设备，同时保持与较大网络近似的准确性。在Andrew
    G. Howard等人撰写的“MobileNets”一文中（[https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)），MobileNet
    v1架构用深度可分离卷积替换了常规卷积，以进一步降低计算复杂度。（如您所记得，我们在第7章讨论Xception模型时，已经介绍了将常规卷积重构为深度可分离卷积的理论。）让我们看看MobileNet如何将这种方法应用于紧凑的模型。'
- en: 8.1.1 Architecture
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 架构
- en: 'The MobileNet v1 architecture incorporated several design principles for constrained
    memory devices:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet v1架构结合了几个针对受限内存设备的设计原则：
- en: The stem convolutional group introduced an additional parameter, known as a
    *resolution multiplier**,* for a more aggressive reduction in the *size* of the
    feature maps feeding into the learner component. (This is labeled *A* in figure
    8.1.)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 茎卷积组引入了一个额外的参数，称为**分辨率乘数**，用于更激进地减少输入到学习组件的特征图**大小**。（这在图8.1中标记为**A**。）
- en: Similarly, the learner component added a *width multiplier* parameter for a
    more aggressive reduction in the *number* of feature maps within the learner component
    (B).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，学习组件为学习组件内部的特征图**数量**增加了一个**宽度乘数**参数，以实现更激进的减少。
- en: The model uses depthwise convolutionals (as in Xception) to reduce computational
    complexity while maintaining representational equivalence (C).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型使用深度卷积（如Xception中所示）来降低计算复杂度，同时保持表示等价性（C）。
- en: The classifier component uses a convolutional layer in place of a dense layer
    for final classification (D).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类组件使用卷积层代替密集层进行最终分类（D）。
- en: You can see these innovations implemented in the macro-architecture in figure
    8.1, with letters A, B, C, and D marking the corresponding feature.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图8.1的宏架构中看到这些创新的应用，字母A、B、C和D标记了相应的特征。
- en: '![](Images/CH08_F01_Ferlitsch.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F01_Ferlitsch.png)'
- en: Figure 8.1 The MobileNet v1 macro-architecture uses metaparameters in the stem
    and learner (A and B), along with depthwise convolutions in the learner (C), and
    a convolutional layer instead of a dense layer in the classifier (D).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 MobileNet v1宏架构在茎和学习者（A和B）中使用了元参数，在学习者中使用了深度卷积（C），在分类器中使用了卷积层而不是密集层（D）。
- en: Unlike the models we’ve covered so far, MobileNets are categorized not by their
    number of layers, but by the input resolution. For example, a MobileNet-224 has
    input (224, 224, 3). The convolutional groups follow the convention of doubling
    the number of filters from the previous group.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论的模型不同，MobileNets是根据其输入分辨率进行分类的。例如，MobileNet-224的输入为（224, 224, 3）。卷积组遵循从上一个组加倍滤波器数量的惯例。
- en: Let’s first look at the two new hyperparameters, the width multiplier and resolution
    multiplier, to see how and where they help thin the network. Then we’ll go through
    the stem, learner, and classifier components step-by-step.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看两个新的超参数，宽度乘数和分辨率乘数，看看它们如何以及在哪里帮助细化网络。然后我们将逐步介绍茎、学习者和分类组件。
- en: 8.1.2 Width multiplier
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 宽度乘数
- en: The first hyperparameter introduced was the *width multiplier* *α* (alpha),
    which thinned a network uniformly at each layer. Let’s take a quick look at the
    pros and cons of thinning a network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 引入的第一个超参数是**宽度乘数**α（alpha），它在每一层均匀地细化网络。让我们快速看一下细化网络的优势和劣势。
- en: We know that by thinning it, we are reducing the number of parameters between
    layers, and exponentially reducing the number of matmul operations. Using dense
    layers, for example, if, prior to thinning, the output of one dense layer and
    the corresponding input is 100 parameters each, we would have 10,000 matrix multiply
    (matmul) operations. In other words, two 100-node dense layers fully connected
    to one another would have 100 x 100 matmul operations per 1D vector that passes
    through the two layers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，通过减薄，我们正在减少层之间的参数数量，并且指数级减少矩阵乘法操作的次数。例如，使用密集层，如果在减薄之前，一个密集层的输出和相应的输入各有
    100 个参数，那么我们将有 10,000 次矩阵乘法（matmul）操作。换句话说，两个完全连接的 100 节点密集层，每通过一个 1D 向量就会进行 100
    x 100 次矩阵乘法操作。
- en: Now let’s thin it by one-half. That is 50 output parameters and 50 input parameters.
    We’ve now reduced the number of matmul operations to 2500\. The result would be
    a 50% reduction in memory size and 75% reduction in computation (latency). The
    downside is we are further removing overcapacity for accuracy, and will need to
    explore other strategies to compensate for it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将其减薄一半。即 50 个输出参数和 50 个输入参数。现在我们已经将矩阵乘法操作的次数减少到 2500 次。结果是内存大小减少 50%，计算（延迟）减少
    75%。缺点是我们进一步减少了过容量以提高准确性，并将需要探索其他策略来补偿这一点。
- en: At each layer, the number of input channels is *αM*, and the number of output
    channels is *αN*, where *M* and *N* are the number of channels (feature maps)
    of a nonthinned MobileNet. Let’s now look at how to calculate the reduction in
    parameters by thinning the network layers. The value of *α* (alpha) is from 0
    to 1, and will reduce the computational complexity of a MobileNet by *α*² (the
    number of parameters). A value of *α* < 1 is referred to as a *reduced MobileNet*.
    Typically, values are 0.25 (6% of nonthinned), 0.50 (25%), and 0.75 (56%). Let’s
    go ahead and do the computation. If the *α* factor is 0.25, the resulting complexity
    is 0.25 × 0.25, which computes to 0.0625.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层，输入通道的数量是 *αM*，输出通道的数量是 *αN*，其中 *M* 和 *N* 是未减薄的 MobileNet 的通道数（特征图）。现在让我们看看如何通过减薄网络层来计算参数的减少。*α*（alpha）的值从
    0 到 1，并且通过 *α*²（参数数量）减少 MobileNet 的计算复杂度。*α* < 1 的值被称为*减薄 MobileNet*。通常，这些值是 0.25（6%
    的未减薄）、0.50（25%）和 0.75（56%）。让我们继续进行计算。如果 *α* 因子是 0.25，那么得到的复杂度是 0.25 × 0.25，计算结果是
    0.0625。
- en: In tests results reported in the paper, a nonthinned MobileNet-224 had a 70.6%
    accuracy on ImageNet with 4.2 million parameters and 569 million matrix multiply-add
    operations, while a 0.25 (width multiplier) MobileNet-224 had 50.6% accuracy with
    0.5 million parameters and 41 million matrix multiply-add operations. These results
    show that the loss of overcapacity by aggressive thinning was not being effectively
    offset by the design of the model. So the researchers looked to reducing the resolution,
    which turned out to be more effective in maintaining accuracy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中报告的测试结果中，未减薄的 MobileNet-224 在 ImageNet 上有 70.6% 的准确性，参数数量为 420 万，矩阵乘加操作为
    5.69 亿，而 0.25（宽度乘数）MobileNet-224 有 50.6% 的准确性，参数数量为 50 万，矩阵乘加操作为 4100 万。这些结果表明，通过激进减薄导致的过容量损失并没有被模型设计有效地抵消。因此，研究人员转向减少分辨率，结果证明这更有利于保持准确性。
- en: 8.1.3 Resolution multiplier
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 分辨率乘数
- en: The second hyperparameter introduced was the *resolution multiplier* *ρ* (rho),
    which thins the input shape and consequently the feature map sizes at each layer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个引入的超参数是*分辨率乘数* *ρ*（rho），它减少了输入形状以及每个层的特征图大小。
- en: When we reduce the input resolution without altering the stem component, the
    size of the feature maps entering the learner component is correspondingly reduced.
    For example, if the height and width of an input image is reduced by one-half,
    the number of input pixels is reduced by 75%. If we maintain the same coarse-level
    filters and number of filters, the outputted feature maps would be reduced by
    75%. Since the feature maps are reduced, this will have a downstream effect of
    reducing the number of parameters per convolution (model size) and number of matmul
    operations (latency). Note this is in contrast to the width thinning that would
    reduce the number of feature maps while maintaining their size.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在不改变主干组件的情况下降低输入分辨率时，进入学习组件的特征图大小相应减少。例如，如果输入图像的高度和宽度减少一半，输入像素的数量将减少75%。如果我们保持相同的粗略级滤波器和滤波器数量，输出的特征图将减少75%。由于特征图减少，这将导致每个卷积（模型大小）和矩阵乘法操作（延迟）的数量减少。请注意，这与宽度细化不同，宽度细化会减少特征图的数量，同时保持其大小。
- en: The downside is that if we reduce too aggressively, the size of the feature
    maps by the time we get to the bottleneck may be 1 × 1 pixels and in essence lose
    the spatial relationships. We could offset this by reducing the number of intermediate
    layers so the feature maps are bigger than 1 × 1, but then we are removing more
    overcapacity for accuracy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是，如果我们过于激进地减少，当我们到达瓶颈时特征图的大小可能变为1 × 1像素，本质上失去了空间关系。我们可以通过减少中间层的数量来补偿这一点，使特征图大于1
    × 1，但这样我们会为了精度而移除更多的冗余。
- en: In tests results reported in the paper, a 0.25 (resolution multiplier) MobileNet-224
    had 64.4% accuracy with 4.2 million parameters and 186 million matrix multiply-add
    operations. Let’s go ahead and do the computation, given the value of *ρ* (rho)
    is from 0 to 1, and will reduce computational complexity of a MobileNet by *ρ*².
    If the *ρ* factor is 0.25, the resulting complexity is 0.25 × 0.25, which computes
    to 0.0625.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中报告的测试结果中，一个0.25（分辨率乘数）的MobileNet-224在4.2百万个参数和1.86亿次矩阵乘加操作下达到了64.4%的准确率。鉴于*ρ*（rho）的值在0到1之间，并且将MobileNet的计算复杂度降低到*ρ*²。如果*ρ*因子为0.25，则结果复杂度为0.25
    × 0.25，计算结果为0.0625。
- en: 'The following is a skeleton template for a MobileNet-224\. Note the use of
    parameters `alpha` and `rho` for the width and resolution multiplier:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个MobileNet-224的骨架模板。请注意，使用参数`alpha`和`rho`作为宽度和分辨率乘数：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Width multiplier used on all layers in the model
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模型中所有层使用的宽度乘数
- en: ❷ Code removed for brevity
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为了简洁性移除的代码
- en: ❸ Resolution multiplier used only on input tensor
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 仅在输入张量上使用的分辨率乘数
- en: 8.1.4 Stem
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 主干
- en: The stem component consists of a strided 3 × 3 convolution, for feature pooling,
    followed by a single depthwise separable block of 64 filters. The number of filters
    in both the strided convolution and the depthwise block are further reduced by
    the hyperparameter *α* (alpha). The reduction of the input size by the hyperparameter
    *ρ* (rho) is not done in the model, but upstream in the input preprocessing function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 主干组件由一个步进的3 × 3卷积（用于特征池化）和一个64个滤波器的单个深度可分离块组成。步进卷积和深度可分离块中的滤波器数量进一步通过超参数*α*（alpha）减少。通过超参数*ρ*（rho）减少输入大小不是在模型中完成，而是在输入预处理函数的上游完成。
- en: Let’s discuss how this differed from a conventional stem at the time for a large
    model. Typically, the first convolutional layer would start with a coarse 7 ×
    7, or 5 × 5, or refactored stack of two 3 × 3 convolutions with 64 filters. The
    coarse convolution would be strided for reduction in size of feature maps and
    then followed by a max pooling layer for another reduction in feature map size.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下这与当时大型模型的传统主干有何不同。通常，第一个卷积层会从粗略的7 × 7、5 × 5或重构的两个3 × 3卷积层（64个滤波器）开始。粗略卷积会进行步进以减少特征图的大小，然后跟随一个最大池化层以进一步减少特征图的大小。
- en: 'In the MobileNet v1 stem, the convention of using 64 filters and a stack of
    two 3 × 3 convolutions is continued, but with three significant changes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在MobileNet v1主干中，继续使用64个滤波器和两个3 × 3卷积层的传统，但有三个显著的变化：
- en: The first convolution outputs one-half (32) the number of feature maps as the
    second one. This acts as a bottleneck, reducing computational complexity in the
    dual 3 × 3 stack.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个卷积输出的特征图数量是第二个卷积的一半（32）。这起到瓶颈的作用，在双3 × 3堆叠中减少计算复杂度。
- en: The second convolutional is replaced by a depthwise separable convolution, further
    reducing computational complexity in the stem.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个卷积被替换为深度可分离卷积，进一步降低了茎中的计算复杂度。
- en: Without max pooling, there is only one feature map size reduction with the first
    strided convolution.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有最大池化，只有第一个步长卷积导致一个特征图大小的减少。
- en: The tradeoff here is that the sizes of the feature maps are kept larger—double
    the *H* × *W*. This offsets the representational loss from aggressive reduction
    in computational complexity in the first coarse-level feature extraction.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的权衡是保持特征图的大小更大——是*H* × *W*的两倍。这抵消了在第一级粗略特征提取中激进减少计算复杂度所带来的表示损失。
- en: Figure 8.2 illustrates the stem component, which consists of a stack of two
    3 × 3 convolutions. The first is a normal convolution that does feature pooling
    (strided). The second is a depthwise convolution, which maintains the size of
    the feature maps (non-strided). The strided 3 × 3 convolution did not use padding.
    To maintain feature map reduction of 75% (0.5*H* × 0.5*W*), a zero padding is
    added to the input prior to the convolution. Note the use of the metaparameter
    *ρ* on the input size for resolution reduction and *α* on the dual stack of 3
    × 3 convolutions for network thinning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2展示了茎组件，它由两个3 × 3卷积的堆叠组成。第一个是一个正常的卷积，它执行特征池化（步长）。第二个是一个深度卷积，它保持特征图的大小（非步长）。步长3
    × 3卷积没有使用填充。为了保持特征图减少75%（0.5*H* × 0.5*W*），在卷积之前对输入添加了零填充。注意在输入大小上使用元参数*ρ*进行分辨率降低，以及在3
    × 3卷积的双重堆叠上使用*α*进行网络细化。
- en: '![](Images/CH08_F02_Ferlitsch.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F02_Ferlitsch.png)'
- en: Figure 8.2 MobileNet stem group thins the network in the stack of 3 × 3 convolutions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2展示了MobileNet茎组在3 × 3卷积堆叠中细化网络。
- en: 'The following is an example implementation of the stem component. As you can
    see, a post-activation batch normalization (Conv-BN-RE) is used for convolutional
    layers, so the model did not have the benefit of using a pre-activation batch
    normalization, which was found to increase accuracy from 0.5 to 2%:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个茎组件的实现示例。正如您所看到的，卷积层使用了后激活批量归一化（Conv-BN-RE），因此模型没有使用预激活批量归一化的好处，这被发现可以将准确率从0.5提升到2%：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Convolutional block with zero padding of input feature maps
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入特征图零填充的卷积块
- en: ❷ Depthwise separable convolution block
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 深度可分离卷积块
- en: Note that `ReLU` in this example takes an optional parameter with the value
    of 6.0\. This is the `max_value` argument to `ReLU`, which defaults to `None`.
    Its purpose is to clip any value above `max_value`. Thus in the preceding examples,
    all the outputs will be in the range of 0 to 6.0\. It is common practice to clip
    the output from `ReLU` in mobile networks, if the weights are later quantized.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个例子中，`ReLU`有一个可选参数，其值为6.0。这是`ReLU`的`max_value`参数，默认为`None`。它的目的是剪辑任何高于`max_value`的值。因此，在前面的例子中，所有输出都将位于0到6.0的范围内。在移动网络中，如果权重后来被量化，通常会将`ReLU`的输出进行剪辑。
- en: '*Quantizing* in this context is computing using a lower-bit representation;
    I’ll explain the details of this process in section 8.5.1\. Quantized models have
    been found to maintain better accuracy when the output from `ReLU` has a constrained
    range. The general practice is to set it to 6.0.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，“量化”是指使用较低位表示的计算；我将在第8.5.1节中解释这个过程的细节。研究发现，当`ReLU`的输出有一个约束范围时，量化模型可以保持更好的准确率。一般做法是将其设置为6.0。
- en: Let’s briefly discuss the reasoning behind the choice of the value of 6\. The
    concept was introduced in Alex Krizhevsky’s 2010 paper, “Convolutional Deep Belief
    Networks on CIFAR-10” ([www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf](https://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf)).
    Krizhevsky proposed it as a solution to the problem of exploding gradients in
    deeper layers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要讨论一下选择6这个值的理由。这个概念是在Alex Krizhevsky 2010年的论文“CIFAR-10上的卷积深度信念网络”中提出的（[www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf](https://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf)）。Krizhevsky将其作为解决深层网络梯度爆炸问题的解决方案。
- en: When the output of an activation got very large, it could dominate the outputs
    of surrounding activations. As a result, that area of the network would exhibit
    symmetry, meaning that it would reduce down, as if there were only a single node.
    Through experimentation, Krizhevsky found the value 6 to be the best.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当激活的输出变得非常大时，它可能会主导周围激活的输出。结果，该网络区域会表现出对称性，这意味着它会减少，就像只有一个节点一样。通过实验，Krizhevsky发现6这个值是最好的。
- en: Remember, this was before we were aware of the benefit of batch normalization,
    which would not be introduced until 2015\. With batch normalization, the activations
    would be squashed at each successive depth, so the need for clipping went away.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这在我们意识到批归一化的好处之前。批归一化会在每个连续的深度处压缩激活，因此不再需要截断。
- en: The idea of clipping the ReLU returned when quantization was introduced. In
    brief, when weights are quantized, we are decreasing the number of bits that represent
    a value. If we mapped the weights to, say, an 8-bit integer range, we have to
    “bucketize” the entire output range into 256 bins, based on the actual distribution
    of output values. The longer the range, the more stretched thin the floating-point-value
    mappings to the buckets, making each bucket less distinctive.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化引入时截断ReLU返回值的概念。简而言之，当权重被量化时，我们正在减少表示值的位数。如果我们将权重映射到，比如说，一个8位整数范围，我们必须根据实际输出值的分布将整个输出范围“分桶”到256个桶中。范围越长，浮点值映射到桶中的拉伸就越薄，使得每个桶的特征就越不显著。
- en: The theory here is that values that would be 98, 99, and 99.5% confidence are
    essentially the same, and lower values are more distinctive—that is, output is
    70% confident. But with clipping, we are treating everything above 6 as essentially
    100% and bucketizing only the distribution between 0 and 6, and those values are
    more meaningful for inference.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的理论是，那些98%、99%和99.5%置信度的值本质上是一样的，而较低值则更加独特——也就是说，输出有70%的置信度。但是，通过截断，我们将所有高于6的值视为本质上100%，并且仅对0到6之间的分布进行分桶，这些值对于推理更有意义。
- en: 8.1.5 Learner
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.5 学习者
- en: The learner component in the MobileNet-224 consists of four groups, and each
    group has two or more convolutional blocks. Each group will double the number
    of filters from the preceding group, and the first block in each group uses a
    strided convolution (feature pooling) to reduce the feature map sizes by 75%.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet-224中的学习组件由四个组组成，每个组包含两个或更多的卷积块。每个组将前一个组的过滤器数量翻倍，并且每个组中的第一个块使用步长卷积（特征池化）将特征图大小减少75%。
- en: 'The construction of a MobileNet group follows the same principles as its large
    convolutional network groups. Both typically have the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 构建MobileNet组遵循与大型卷积网络组相同的原理。两者通常具有以下特点：
- en: A progression in the number of filters per group, such as doubling the number
    of filters
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每组过滤器数量的进展，例如将过滤器数量翻倍
- en: A reduction in the outputted feature map sizes either by using a strided convolution
    or deferred max pooling
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用步长卷积或延迟最大池化来减少输出的特征图大小
- en: You can see in figure 8.3 that the MobileNet group uses a strided convolution
    for the first block, to reduce the feature map (principle 2). Though not shown
    in the diagram, each group in the learner doubles the number of filters (principle
    1) starting at 128.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图8.3中看到，MobileNet组在第一个块中使用步长卷积来减少特征图（原则2）。尽管图中没有显示，但学习器中的每个组从128开始将过滤器数量翻倍（原则1）。
- en: '![](Images/CH08_F03_Ferlitsch.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F03_Ferlitsch.png)'
- en: Figure 8.3 In a MobileNet v1 learner component, each group is a sequence of
    depthwise convolutional blocks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 在MobileNet v1的学习组件中，每个组是一系列深度卷积块。
- en: Figure 8.4 zooms in on a depthwise convolutional block in the learner group.
    In v1, the model’s authors used a convolutional block design instead of a residual
    block design; there is no identity link. Each block is essentially a single depthwise
    separable convolution constructed as two separate convolutional layers. The first
    layer is a 3 × 3 depthwise convolution followed by the 1 × 1 pointwise convolution.
    When combined, these form the depthwise separable convolution. The number of filters,
    which corresponds to the number of feature maps, can be further reduced for network
    thinning by the metaparameter *α*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4放大了学习组中的深度卷积块。在v1中，模型的作者使用了卷积块设计而不是残差块设计；没有恒等连接。每个块本质上是一个单深度可分离卷积，由两个独立的卷积层构建。第一层是一个3×3的深度卷积，后面跟着一个1×1的点卷积。当结合时，这些形成深度可分离卷积。过滤器的数量，即对应于特征图的数量，可以通过元参数*α*进一步减少以进行网络细化。
- en: '![](Images/CH08_F04_Ferlitsch.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F04_Ferlitsch.png)'
- en: Figure 8.4 MobileNet v1 convolutional block
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 MobileNet v1卷积块
- en: Next is an example implementation of a depthwise separable convolutional block.
    The first step is to calculate the number of reduced `filters` for network thinning,
    after applying the width multiplier `alpha`. For the first block in the group,
    the feature map sizes are reduced (feature pooling) using a strided convolution
    (`strides=(2, 2)`). This corresponds to the convolutional group design principle
    2 mentioned earlier, where the first block in a group typically does a dimensionality
    reduction on the size of the input feature maps.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个深度可分离卷积块的示例实现。第一步是计算在应用宽度乘数 `alpha` 后网络变薄的 `filters` 数量。对于组中的第一个块，使用步长卷积
    (`strides=(2, 2)`) 对特征图大小进行减少（特征池化）。这对应于之前提到的卷积组设计原则2，其中组中的第一个块通常对输入特征图的大小进行维度降低。
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Applies the width filter to the number of feature maps
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将宽度滤波器应用于特征图数量
- en: ❷ Adds zero padding when a strided convolution, for matching the number of filters
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在进行步长卷积时添加零填充，以匹配滤波器的数量
- en: ❸ The depthwise convolution
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 深度卷积
- en: ❹ The pointwise convolution
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 点卷积
- en: 8.1.6 Classifier
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.6 分类器
- en: The classifier component differed from the conventional classifier for large
    models in that it used a convolutional layer in place of a dense layer for the
    classification step. Like other classifiers of the time, to prevent memorization,
    it added a dropout layer prior to the classification for regularization.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 分类组件与大型模型的传统分类器不同，它在分类步骤中使用卷积层代替密集层。像其他当时的分类器一样，为了防止记忆化，它在分类之前添加了一个dropout层进行正则化。
- en: You can see in figure 8.5 that the classifier component contains a `GlobalAveragePooling2D`
    layer to flatten the feature maps and reduce the high-dimensional encoding to
    a lower-dimensional encoding (1 pixel per feature map). Then a `Reshape` layer
    reshapes the 1D vector for a 2D convolution, using a softmax activation, in which
    the number of filters is the number of classes. Then comes another `Reshape` to
    reshape the output back to a 1D vector (one element per class). Prior to the 2D
    convolution is the `Dropout` layer for regularization.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图8.5中看到，分类组件包含一个 `GlobalAveragePooling2D` 层，用于展平特征图并将高维编码降低到低维编码（每个特征图1个像素）。然后使用softmax激活函数，其中滤波器的数量是类别的数量，通过一个
    `Reshape` 层将1D向量重塑为2D向量。然后是另一个 `Reshape` 层，将输出重塑回1D向量（每个类别一个元素）。在2D卷积之前是用于正则化的
    `Dropout` 层。
- en: '![](Images/CH08_F05_Ferlitsch.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F05_Ferlitsch.png)'
- en: Figure 8.5 MobileNet v1 classifier group using a convolutional layer for classification
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 MobileNet v1分类组使用卷积层进行分类
- en: 'The following is an example implementation of the classifier component. The
    first `Reshape` layer reshapes the 1D vector from `GlobalAveragePooling2D` to
    a 2D vector of size 1 × 1\. The second `Reshape` layer reshapes the 2D 1 × 1 output
    from `Conv2D` into a 1D vector for the softmax probability distribution (classification):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例实现是分类组件。第一个 `Reshape` 层将 `GlobalAveragePooling2D` 的1D向量重塑为大小为1 × 1的2D向量。第二个
    `Reshape` 层将 `Conv2D` 的2D 1 × 1输出重塑为用于softmax概率分布（分类）的1D向量：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Flattens the feature maps into 1D feature maps (*α*, N)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将特征图展平为1D特征图 (*α*, N)
- en: ❷ Reshapes the flattened feature maps to (*α*, 1, 1, 1024)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将展平的特征图重塑为 (*α*, 1, 1, 1024)
- en: ❸ Performs dropout for preventing overfitting
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行dropout以防止过拟合
- en: ❹ Uses convolution for classifying (emulates a fully connected layer)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用卷积进行分类（模拟全连接层）
- en: ❺ Reshapes the resulting output to 1D vector of number of classes
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将结果输出重塑为包含类别数量的1D向量
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for MobileNet v1 is located on GitHub ([http://mng.bz/Q2rG](https://shortener.manning.com/Q2rG)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic procedure reuse设计模式对MobileNet v1进行完整代码实现的示例位于GitHub上 ([http://mng.bz/Q2rG](https://shortener.manning.com/Q2rG)).
- en: 8.2 MobileNet v2
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 MobileNet v2
- en: 'After improving version 1, Google introduced *MobileNet v2* in “MobileNetV2:
    Inverted Residuals and Linear Bottlenecks” by Mark Sandler et al. in 2018 ([https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)).
    The new architecture replaces convolutional blocks with inverted residual blocks
    to improve performance. The paper summarizes the benefits of the inverted residual
    blocks:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '在改进版本1之后，谷歌在2018年Mark Sandler等人撰写的“MobileNetV2: Inverted Residuals and Linear
    Bottlenecks”一文中引入了 *MobileNet v2* ([https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381))。新的架构用倒残差块替换了卷积块以提高性能。该论文总结了倒残差块的好处：'
- en: Significantly decreasing the number of operations while retaining the same accuracy
    as convolutional block
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著减少操作数量，同时保持与卷积块相同的准确度
- en: Significantly reducing the memory footprint needed for inference
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著减少推理所需的内存占用
- en: 8.2.1 Architecture
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 架构
- en: 'The MobileNet v2 architecture incorporated several design principles for constrained
    memory devices:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet v2架构结合了几个针对受限内存设备的设计原则：
- en: It continued using the hyperparameter (alpha) as a width multiplier, as in v1,
    for network thinning in the stem and learner components.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它继续使用超参数（alpha）作为宽度乘数，如v1中所述，在根部和学习者组件中进行网络稀疏化。
- en: Continued using depthwise separable convolutions in place of normal convolutions,
    as in v1, for substantial reduction in computational complexity (latency), while
    maintaining nearly comparable representational power.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续使用深度可分离卷积代替常规卷积，如v1中所述，以显著降低计算复杂度（延迟），同时保持几乎相当的表现力。
- en: Replaced using convolutional blocks with residual blocks, allowing deeper layers
    for more accuracy.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用残差块替换卷积块，允许更深的层以获得更高的准确度。
- en: Introduced a new design for residual blocks, which the authors called *inverted
    residual blocks*.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入了一种新的残差块设计，作者们称之为*倒残差块*。
- en: Replaced using 1 × 1 nonlinear convolutions with 1 × 1 linear convolutions.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用1 × 1非线性卷积替换1 × 1线性卷积。
- en: 'The reason for the last modification, using 1 × 1 linear convolutions, according
    to the authors: “Additionally, we find that it is important to remove nonlinearities
    in the narrow layers in order to maintain representational power.” In their ablation
    study, they compared using a 1 × 1 nonlinear convolution (with `ReLU`) to a 1
    × 1 linear convolution (without `ReLU`) and got a 1% top-1 accuracy improvement
    on ImageNet by removing the ReLU.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 根据作者的说法，最后修改的原因，使用1 × 1线性卷积： “此外，我们发现，为了保持表现力，移除狭窄层中的非线性是很重要的。” 在他们的消融研究中，他们比较了使用1
    × 1非线性卷积（带有`ReLU`）和使用1 × 1线性卷积（不带`ReLU`），通过移除ReLU在ImageNet上获得了1%的top-1准确度提升。
- en: 'The authors describe their main contribution as a novel layer module: the inverted
    residual with linear bottleneck. I describe the inverted residual block in detail
    in section 8.2.3.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们将他们的主要贡献描述为一种新颖的层模块：线性瓶颈的倒残差。我在第8.2.3节中详细描述了倒残差块。
- en: Figure 8.6 depicts the MobileNet v2 architecture. In the macro-architecture,
    the learner component consists of four inverted residual groups, followed by a
    final 1 × 1 linear convolution, which means that the activation function is linear.
    Each inverted residual group increases the number of filters from the previous
    group. The number of filters per group is thinned by the metaparameter width multiplier
    *α* (alpha). The final 1 × 1 convolution does a linear projection, increasing
    the final number of feature maps four times, to 2048.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6展示了MobileNet v2架构。在宏观架构中，学习者组件由四个倒残差组组成，随后是一个最终的1 × 1线性卷积，这意味着激活函数是线性的。每个倒残差组将前一个组的滤波器数量增加。每个组的滤波器数量通过元参数宽度乘数*α*（alpha）进行稀疏化。最终的1
    × 1卷积进行线性投影，将最终的特征图数量增加到四倍，达到2048。
- en: '![](Images/CH08_F06_Ferlitsch.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F06_Ferlitsch.png)'
- en: Figure 8.6 MobileNet v2 macro-architecture
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 MobileNet v2宏观架构
- en: 8.2.2 Stem
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 根部
- en: The stem component is similar to v1, except after the initial 3 × 3 convolutional
    layer, it is not followed by a depthwise convolutional block as in v1 (figure
    8.7). As such, the coarse-level feature extraction would have less representational
    power than the dual stack of 3 × 3 in v1\. The authors don’t state why the reduction
    in representational power did not impact the model, which outperformed v1 in accuracy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 根部组件与v1相似，除了在初始3 × 3卷积层之后，它不跟随v1中的深度卷积块（图8.7）。因此，粗粒度特征提取的表现力将低于v1中的3 × 3双栈。作者们没有说明为什么表现力的降低没有影响模型，该模型在准确度上优于v1。
- en: '![](Images/CH08_F07_Ferlitsch.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F07_Ferlitsch.png)'
- en: Figure 8.7 MobileNet v2 stem group
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 MobileNet v2根部组
- en: 8.2.3 Learner
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 学习者
- en: The learner component consists of seven inverted residual groups, followed by
    a 1 × 1 linear convolution. Each inverted residual group consists of two or more
    inverted residual blocks. Each group progressively increases the number of filters,
    also known as *output channels*. Each group starts with a strided convolutional,
    reducing the size of the feature maps (channels) as each group progressively increases
    the number of feature maps.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 学习组件由七个倒置残差组组成，随后是一个1 × 1的线性卷积。每个倒置残差组包含两个或更多的倒置残差块。每个组逐渐增加滤波器的数量，也称为*输出通道*。每个组从步长卷积开始，随着每个组逐渐增加特征图（通道）的数量，减少特征图的大小（通道）。
- en: Figure 8.8 depicts a MobileNet v2 group, in which the first inverted residual
    block is strided for reducing the size of the feature maps to offset the progressive
    increase in the number of feature maps per group. As noted in the diagram, only
    groups 2, 3, 4, and 6, start with a strided inverted residual block. In other
    words, groups 1, 5, and 7 start with a nonstrided residual block. Additionally,
    each nonstrided block has an identity link, and the strided blocks do not have
    an identity link.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8描述了一个MobileNet v2组，其中第一个倒置残差块进行了步长操作以减少特征图的大小，以抵消每个组中特征图数量逐渐增加的趋势。如图表所示，只有第2、3、4和6组以步长倒置残差块开始。换句话说，第1、5和7组以非步长残差块开始。此外，每个非步长块都有一个恒等连接，而步长块没有恒等连接。
- en: '![](Images/CH08_F08_Ferlitsch.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F08_Ferlitsch.png)'
- en: Figure 8.8 MobileNet v2 group micro-architecture
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 MobileNet v2组微架构
- en: The following is an example implementation of a MobileNet v2 group. The group
    follows the convention whereby the first block does a dimensionality reduction
    to reduce the size of feature maps. In this case, the first inverted block is
    strided (feature pooling), and the remaining blocks are not strided (no feature
    pooling).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个MobileNet v2组的示例实现。该组遵循以下惯例：第一个块执行降维以减少特征图的大小。在这种情况下，第一个倒置块是步长的（特征池化），其余块不是步长的（无特征池化）。
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The first inverted residual block in the group may be strided.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 组中的第一个倒置残差块可能是步长的。
- en: ❷ Constructs the remaining blocks
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建剩余的块
- en: The block is referred to as an *inverted residual block* because it reverses
    (inverts) the relationship of the dimensionality reduction and expansion surrounding
    the middle convolution layer from a conventional residual block, such as in a
    ResNet50\. Instead of starting with a 1 × 1 bottleneck convolution for dimensionality
    reduction and ending with a 1 × 1 linear projection convolution for dimensionality
    restoration, the order is reversed. An inverted block starts with a 1 × 1 projection
    convolution for dimensionality expansion, and ends with a 1 × 1 bottleneck convolution
    for dimensionality restoration (figure 8.9).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 该块被称为*倒置残差块*，因为它反转（倒置）了围绕中间卷积层的降维和扩展关系，这与传统的残差块不同，例如在ResNet50中。它不是从1 × 1的瓶颈卷积开始进行降维，以1
    × 1的线性投影卷积结束以恢复维度，而是顺序相反。一个倒置块从1 × 1的投影卷积开始进行维度扩展，并以1 × 1的瓶颈卷积结束以恢复维度（图8.9）。
- en: '![](Images/CH08_F09_Ferlitsch.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F09_Ferlitsch.png)'
- en: Figure 8.9 Conceptual difference between residual bottleneck block and inverted
    residual block
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 残差瓶颈块与倒置残差块之间的概念差异
- en: In their ablation study comparing the bottleneck residual block design in MobileNet
    v1 to the inverted residual block design in v2, the authors achieved a 1.4% improvement
    in top-1 accuracy on ImageNet. The inverted residual block design is also more
    efficient, reducing the total number of parameters from 4.2 million to 3.4 million,
    and the number of matmul operations from 575 million to 300 million.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们比较MobileNet v1中的瓶颈残差块设计与v2中的倒置残差块设计的消融研究中，作者在ImageNet上实现了1.4%的top-1准确率提升。倒置残差块设计也更加高效，将总参数数量从420万减少到340万，并将matmul操作的数量从5.75亿减少到3亿。
- en: Next, we will dive deeper into the mechanics behind the inversion. MobileNet
    v2 introduced a new metaparameter expansion for the initial 1 × 1 projection convolution.
    The 1 × 1 projection convolution performs the dimensionality expansion, and the
    metaparameter specifies the amount to expand the number of filters. In other words,
    the 1 × 1 projection convolution expands the number of feature maps to a high-dimensional
    space.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更深入地探讨反演背后的机制。MobileNet v2 引入了一种新的元参数扩展，用于初始的 1 × 1 投影卷积。1 × 1 投影卷积执行维度扩展，而元参数指定了扩展滤波器数量的量。换句话说，1
    × 1 投影卷积将特征图数量扩展到高维空间。
- en: The middle convolution is a 3 × 3 depthwise convolution. This is followed by
    a linear pointwise convolution that reduces the feature maps (also called *channels*),
    restoring them to the original number. Note that restoration convolutions use
    a linear activation instead of a nonlinear (ReLU). The authors found it important
    to remove nonlinearities in the narrow layers in order to maintain representational
    power.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 中间卷积是一个 3 × 3 深度卷积。这随后是一个线性点卷积，它减少了特征图（也称为 *通道*），将它们恢复到原始数量。请注意，恢复卷积使用线性激活而不是非线性（ReLU）。作者发现，为了保持表示能力，移除狭窄层中的非线性很重要。
- en: The authors also found that a ReLU activation loses information in low-dimensional
    space, but makes up for it when there are lots of filters. The assumption here
    is that the input to the block is in a lower-dimensional space but expands the
    number of filters, thus the reason to maintain using the ReLU activation in the
    first 1 × 1 convolution.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还发现，ReLU 激活在低维空间中会丢失信息，但当有大量滤波器时可以弥补这一点。这里的假设是，块的输入处于低维空间，但扩展了滤波器的数量，因此保持使用
    ReLU 激活在第一个 1 × 1 卷积中的原因。
- en: The MobileNet v2 researchers referred to the amount of expansion as the *expressiveness*
    of the block. In their main experiments, they tried expansion factors between
    5 and 10 and observed little difference in accuracy. Since an increase in expansion
    results in an increase in the number of parameters, while observing little gain
    in accuracy, the authors used a ratio of 6 for expansion in their ablation study.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet v2 研究人员将扩展量称为块的 *表达能力*。在他们主要的实验中，他们尝试了 5 到 10 之间的扩展因子，并观察到准确率几乎没有差异。由于扩展的增加会导致参数数量的增加，而准确率的提升却很小，因此作者们在消融研究中使用了
    6 的扩展比率。
- en: Figure 8.10 shows the inverted residual block. You can see how its design changes
    took another step forward in reducing memory footprint while maintaining accuracy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 展示了反演残差块。你可以看到其设计在减少内存占用同时保持准确性的基础上又迈出了新的一步。
- en: '![](Images/CH08_F10_Ferlitsch.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F10_Ferlitsch.png)'
- en: Figure 8.10 Inverted residual block with identity shortcut inverts the relationships
    of the 1 × 1 convolutions from v1.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 带有恒等快捷连接的反演残差块反转了 v1 中 1 × 1 卷积的关系。
- en: The following is an example implementation of an inverted residual block. For
    context, remember that the input to the inverted residual block is the output
    from a prior block, or stem group, in a low-dimensional space. The input is then
    projected into a higher-dimensional space by the 1 × 1 projection convolution,
    where the 3 × 3 depthwise convolution is performed. The pointwise 1 × 1 linear
    convolution then restores the output to the lower dimensionality of the input.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例实现了一个反演残差块。为了理解上下文，请记住，反演残差块的输入是来自先前块或低维空间中的主干组的输出。然后，通过 1 × 1 投影卷积将输入投影到更高维的空间，其中执行
    3 × 3 深度卷积。然后，点卷积的 1 × 1 线性卷积将输出恢复到输入的较低维度。
- en: 'Here are some notable steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些显著的步骤：
- en: 'The width factor is applied to the number of output filters for the block:
    `filters = int(n_filters` `*` `alpha)`.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽度因子应用于块的输出滤波器数量：`filters = int(n_filters` `*` `alpha)`。
- en: The number of input channels (feature maps) is determined by `n_channels` `=
    int(x.shape[-1])`.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入通道（特征图）的数量由 `n_channels` `= int(x.shape[-1])` 确定。
- en: The 1 × 1 linear projection is applied when the `expansion` factor is greater
    than 1.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 `expansion` 因子大于 1 时，应用 1 × 1 线性投影。
- en: 'The `Add()` operation is done on every block, except the first block in the
    first group: `if` `n_channels` `==` `filters` `and` `strides` `==` `(1,` `1)`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一个组的第一块之外，每个块都执行 `Add()` 操作：`if` `n_channels` `==` `filters` `and` `strides`
    `==` `(1,` `1)`。
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Applies the width multiplier to the number of feature maps for the pointwise
    convolution
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将宽度乘数应用于点卷积的特征图数量
- en: ❷ Does dimensionality expansion when not the first block in a group
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当不是组中的第一个块时，进行维度扩展（dimensionality expansion）
- en: ❸ Adds zero padding to feature map when strided convolution (feature pooling)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在步进卷积（特征池化）时向特征图添加零填充（zero padding）
- en: ❹ 3 × 3 depthwise convolution
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 3 × 3 深度卷积
- en: ❺ 1 × 1 linear pointwise convolution
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 1 × 1 线性点卷积
- en: ❻ Adds the identity link to output when the number of input filters matches
    the number of output filters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 当输入滤波器数量与输出滤波器数量相匹配时，向输出添加身份链接（identity link）
- en: 8.2.4 Classifier
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.4 分类器
- en: In v2, the researchers used the conventional approach of a `GlobalAveragePooling2D`
    layer followed by a `Dense` layer, which we covered in section 5.4 in chapter
    5 on large SOTA models. Early ConvNets, such as AlexNet, ZFNet, and VGG flatten
    the bottleneck layer (final feature maps), which was then followed by one or more
    hidden dense layers, before the final dense layer for classification. For example,
    VGG used two layers of 4096 nodes before the final dense layer for classification.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在 v2 版本中，研究人员采用了传统的`GlobalAveragePooling2D`层后跟`Dense`层的方法，这在第 5 章第 5.4 节中已经介绍过。早期的卷积神经网络，如
    AlexNet、ZFNet 和 VGG，会将瓶颈层（最终特征图）进行扁平化，然后接一个或多个隐藏密集层，最后是用于分类的最终密集层。例如，VGG 在最终密集层之前使用了两个包含
    4096 个节点的层。
- en: As representational learning improved, starting with ResNet and Inception, the
    need for hidden layers in the classifier became unnecessary, as did the need for
    a flattening layer without a reduction into a lower-dimensionality bottleneck
    layer. MobileNet v2 followed the practice that when the latent space had strong
    enough representational information, we could further reduce it to a lower-dimensionality
    space—the bottleneck layer. With the high representational information, the model
    can then pass the lower dimensionality, also known as the *embedding* or *feature
    vector*, straight to the classifier’s dense layer without the need for intermediate
    hidden dense layers. Figure 8.11 illustrates the classifier component.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 随着表示学习（representational learning）的改进，从 ResNet 和 Inception 开始，分类器中隐藏层的需求变得不再必要，同样，不需要将数据降维到瓶颈层（bottleneck
    layer）的扁平化层。MobileNet v2 沿袭了这一做法，当潜在空间（latent space）具有足够的表示信息时，我们可以进一步将其降低到低维空间——瓶颈层。在高表示信息下，模型可以将低维度的数据，也称为*嵌入*或*特征向量*，直接传递到分类器的密集层，而不需要中间的隐藏密集层。图
    8.11 展示了分类器组件。
- en: '![](Images/CH08_F11_Ferlitsch.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F11_Ferlitsch.png)'
- en: Figure 8.11 MobileNet v2 classifier group
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 MobileNet v2 分类器组
- en: In the authors’ ablation study, they compare MobileNet v1 to v2 with the ImageNet
    classification task. MobileNet v2 achieved 72% top-1 accuracy compared to v1,
    which achieved 70.6%. A complete code rendition using the Idiomatic procedure
    reuse design pattern for MobileNet v2 is on GitHub ([http://mng.bz/Q2rG](http://mng.bz/Q2rG)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在作者的消融研究中，他们比较了 MobileNet v1 和 v2 在 ImageNet 分类任务上的表现。MobileNet v2 实现了 72% 的
    top-1 准确率，而 v1 实现了 70.6%。使用 Idiomatic procedure reuse 设计模式对 MobileNet v2 的完整代码实现可在
    GitHub 上找到 ([http://mng.bz/Q2rG](http://mng.bz/Q2rG))。
- en: Next, we will cover SqueezeNet, which introduced the fire module and the terminology
    of macro- and micro-architecture and metaparameters for configuring the micro-architecture
    attributes. While other researchers of the time explored this concept, the SqueezeNet
    authors coined the terms for this innovative stepping-stone to later advances
    in macro-architecture search, machine design, and model amalgamation. For myself,
    when I first read their paper and these concepts, it was like a light bulb went
    off.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍 SqueezeNet，它引入了 fire 模块以及用于配置微架构属性的宏观架构和元参数的术语。虽然当时其他研究人员也在探索这个概念，但
    SqueezeNet 的作者为这个创新性的里程碑式进展创造了术语，为后来宏观架构搜索、机器设计和模型融合的进步奠定了基础。对我个人而言，当我第一次阅读他们的论文和这些概念时，感觉就像一个灯泡突然亮了起来。
- en: 8.3 SqueezeNet
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 SqueezeNet
- en: 'SqueezeNet is an architecture introduced by joint research of DeepScale, the
    University of California at Berkeley, and Stanford University in 2016\. In the
    corresponding “SqueezeNet” paper (“SqueezeNet: AlexNet-Level Accuracy with 50x
    Fewer Parameters and <0.5MB Model Size”; [https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360)),
    Forrest N. Iandola et al. introduced a new type of module, the *fire module*,
    as well as terminology for micro-architecture, macro-architecture, and metaparameters.
    The authors’ goal was to find a CNN architecture with fewer parameters but equivalent
    accuracy compared to the well-known AlexNet model.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'SqueezeNet是由DeepScale、加州大学伯克利分校和斯坦福大学于2016年共同研究引入的架构。在相应的"SqueezeNet"论文（"SqueezeNet:
    AlexNet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size"; [https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360)）中，Forrest
    N. Iandola等人介绍了一种新型模块，即"fire模块"，以及微架构、宏架构和超参数的术语。作者们的目标是找到一个参数更少但与知名AlexNet模型具有相当准确性的CNN架构。'
- en: The fire module design was based on their research on the micro-architecture
    to achieve this goal. The *micro-architecture* is the design of modules, or groups,
    and the *macro-architecture* is how the modules, or groups, are connected. The
    introduction of the term *metaparameters* aided in better distinguishing what
    a hyperparameter is (discussed in detail in chapter 10).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 火模块的设计基于他们对微架构的研究以实现这一目标。"微架构"是模块或组的设计，而"宏架构"则是模块或组之间的连接方式。引入"超参数"这一术语有助于更好地区分什么是超参数（在第10章中详细讨论）。
- en: Generally, the weights and biases that are learned during training are the model
    parameters. The term *hyperparameters* can be confusing. Some researchers/ practitioners
    used the term to refer to the tunable parameters for training the model, while
    others used the term to also include the model architecture (for example, layers
    and width). In the SqueezeNet paper, the authors used *metaparameters* to refer
    to the structure of the model architecture that is configurable—such as the number
    of blocks per group, and the number of filters per convolutional layer in a block,
    and the amount of dimensionality reduction at the end of a group.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练过程中学习的权重和偏差是模型参数。"超参数"这个术语可能会令人困惑。一些研究人员/实践者使用这个术语来指代用于训练模型的可调参数，而其他人则使用这个术语来包括模型架构（例如，层和宽度）。在SqueezeNet论文中，作者们使用"元参数"来指代可配置的模型架构结构——例如，每组中的块数，每个块中卷积层的滤波器数量，以及组末端的维度缩减量。
- en: The authors tackled several issues in their paper. First, they wanted to demonstrate
    a CNN architecture design that would fit on a mobile device and still retain accuracy
    comparable to that of AlexNet on the ImageNet 2012 dataset. On this point, the
    authors achieved the same results empirically as AlexNet with a 50 times reduction
    in parameters.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们在他们的论文中解决了几个问题。首先，他们想要展示一种CNN架构设计，这种设计可以在移动设备上运行，同时仍然保持与ImageNet 2012数据集上AlexNet相当的准确性。在这方面，作者们在参数数量减少了50倍的情况下，通过经验实证达到了与AlexNet相同的结果。
- en: Second, they wanted to demonstrate a small CNN architecture that would maintain
    accuracy when compressed. Here the authors achieved the same results without compression
    after compressing with the Deep Compression algorithm, which reduced the size
    of the model from 4.8 MB to 0.47 MB. Getting the model size down to under 0.5
    MB while maintaining the accuracy of AlexNet demonstrated the practicality of
    placing models on extremely memory-constrained IoT devices such as microcontrollers.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，他们想要展示一种小型CNN架构，在压缩后仍能保持准确性。在这里，作者们在使用深度压缩算法压缩后，没有压缩的情况下达到了相同的结果，将模型的大小从4.8
    MB减少到0.47 MB。将模型大小降低到0.5 MB以下，同时保持AlexNet的准确性，证明了在极端内存受限的物联网设备（如微控制器）上放置模型的实用性。
- en: 'In their SqueezeNet paper, the authors refer to their design principles for
    achieving their objectives as strategies 1, 2, and 3:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的SqueezeNet论文中，作者们将实现目标的设计原则称为策略1、2和3：
- en: '*Strategy 1*—Use mostly 1 × 1 filters, which give a 9 times reduction in the
    number of parameters, instead of the more common convention of 3 × 3 filters.
    The v1.0 version of SqueezeNet used a 2:1 ratio of 1 × 1 to 3 × 3 filters.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略1*——主要使用1 × 1滤波器，这比更常见的3 × 3滤波器减少了9倍的参数数量。SqueezeNet的v1.0版本使用了1 × 1到3 ×
    3滤波器的2:1比例。'
- en: '*Strategy 2*—Reduce the number of input filters to the 3 × 3 layers to further
    reduce the number of parameters. They refer to this component of the fire module
    as the *squeeze layer*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略 2*—减少 3 × 3 层的输入滤波器数量以进一步减少参数数量。他们将火模块的这个部分称为 *挤压层*。'
- en: '*Strategy 3*—Delay downsampling of feature maps to as late as possible in the
    network. This contrasts the convention of downsampling early to preserve accuracy.
    The authors used a stride of 1 on the early convolution layers and delayed using
    a stride of 2.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略 3*—尽可能晚地延迟特征图的下采样。这与早期下采样以保持精度的传统做法相反。作者在早期卷积层使用了步长为 1，而延迟使用步长为 2。'
- en: 'The authors stated this justification for their strategies:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 作者陈述了他们策略的以下理由：
- en: '*Strategies 1 and 2 are about judiciously decreasing the quantity of parameters
    in a CNN while attempting to preserve accuracy. Strategy 3 is about maximizing
    accuracy on a limited budget of parameters*.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略 1 和 2 是关于在尝试保持精度的同时，巧妙地减少 CNN 中的参数数量。策略 3 是关于在有限的参数预算内最大化精度*。'
- en: The authors named their architecture after their design of the fire block, which
    uses a squeeze operation followed by an expand operation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将他们的架构命名为其 fire 块的设计，该设计使用了一个挤压操作后跟一个扩展操作。
- en: 8.3.1 Architecture
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 架构
- en: The SqueezeNet architecture consists of a stem group, three fire groups comprising
    a total of eight fire blocks (referred to as *modules* in the paper), and a classifier
    group. The authors did not explicitly state why they chose three fire groups and
    eight fire blocks, but described a macro-architecture exploration that demonstrated
    a cost-effective method of designing a model for a specific memory footprint and
    accuracy range merely by training different combinations of numbers of blocks
    per group and the input-to-output filter sizes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: SqueezeNet 架构由一个主干组、三个包含总共八个 fire 块（在论文中称为 *模块*）的 fire 组和一个分类器组组成。作者没有明确说明他们为什么选择三个
    fire 组和八个 fire 块，但描述了一种宏观架构探索，该探索展示了一种成本效益高的方法，即通过训练每组中块的数量和输入到输出的滤波器大小的不同组合来设计针对特定内存足迹和精度范围的模型。
- en: Figure 8.12 shows the architecture. In the macro-architecture view, you see
    the three fire groups. The feature learning is done in the stem group and first
    two fire groups. The last fire group overlaps feature learning and classification
    learning with the classification group.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 展示了架构。在宏观架构视图中，你可以看到三个 fire 组。特征学习在主干组和前两个 fire 组中进行。最后一个 fire 组与分类组重叠，进行特征学习和分类学习。
- en: '![](Images/CH08_F12_Ferlitsch.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F12_Ferlitsch.png)'
- en: Figure 8.12 SqueezeNet macro-architecture
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 SqueezeNet 宏观架构
- en: The first two fire groups double the number of feature maps from the input to
    the output, starting at 16, doubling to 32, and then doubling again to 64\. Both
    the first and second fire groups delay dimensionality reduction to the end of
    the group. The last fire group does not do any doubling of the feature maps or
    dimensionality reduction, but does add a dropout for regularization at the end
    of the group. This last step differed from the convention of the time, whereby
    the dropout layer would otherwise be placed in the classifier group after the
    bottleneck layer (after the feature maps were reduced and flattened to a 1D vector).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个 fire 组将输入到输出的特征图数量翻倍，从 16 开始，翻倍到 32，然后再次翻倍到 64。第一和第二 fire 组都延迟了维度减少到组的末尾。最后一个
    fire 组不进行特征图数量的加倍或维度减少，但在组的末尾添加了一个 dropout 以进行正则化。这一步骤与当时的传统做法不同，当时 dropout 层本应放置在瓶颈层（特征图减少并展平为
    1D 向量）之后的分类器组中。
- en: 8.3.2 Stem
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 主干
- en: The stem component uses a coarse-level 7 × 7 convolutional layer, which was
    the convention of the time, in contrast to the current convention of using a 5
    × 5 or refactored stack of two 3 × 3 convolutional layers. The stem performs an
    aggressive feature map reduction, which continues to be the present convention.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 主干组件使用了一个粗略级别的 7 × 7 卷积层，这与当时使用 5 × 5 或重构的两个 3 × 3 卷积层的传统做法相反。主干执行了激进的特性图减少，这继续是现在的传统做法。
- en: The coarse 7 × 7 convolution is strided (feature pooling) for a 75% reduction,
    followed by a max pooling layer for a further 75% reduction, resulting in feature
    maps that are 6% the size of the input channels. Figure 8.13 depicts the stem
    component.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略的 7 × 7 卷积进行了步长（特征池化）以实现 75% 的减少，随后是一个最大池化层以进一步减少 75%，结果得到特征图的大小仅为输入通道的 6%。图
    8.13 描述了主干组件。
- en: '![](Images/CH08_F13_Ferlitsch.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F13_Ferlitsch.png)'
- en: Figure 8.13 SqueezeNet stem group
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 SqueezeNet主干组
- en: 8.3.3 Learner
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 学习者
- en: The learner consists of three fire groups. The first fire group has an input
    of 16 filters (channels) and an output of 32 filters (channels). Recall that the
    stem outputs 96 channels, so the first fire group does a dimensionality reduction
    on the input by reducing to 16 filters. The second fire group doubles that with
    input of 32 filters (channels) and output of 64 filters (channels).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者由三个火组组成。第一个火组的输入为16个滤波器（通道），输出为32个滤波器（通道）。回想一下，主干输出96个通道，因此第一个火组通过减少到16个滤波器对输入进行降维。第二个火组将这个数量加倍，输入为32个滤波器（通道），输出为64个滤波器（通道）。
- en: Both the first and second fire groups consist of multiple fire blocks. All but
    the last fire block use the same number of input filters. The last fire block
    doubles the number of filters for the output. Both fire groups delay downsampling
    of the feature maps to the end of the group with a `MaxPooling2D` layer.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个和第二个火组都由多个火块组成。除了最后一个火块外，所有火块使用相同数量的输入滤波器。最后一个火块将输出滤波器的数量加倍。两个火组都使用`MaxPooling2D`层将特征图的下采样延迟到组的末尾。
- en: The third fire group consists of a single fire block of 64 filters followed
    by a dropout layer for regularization, prior to the classifier group. This is
    slightly different from the convention of the time, as SqueezeNet’s dropout layer
    appears prior to the bottleneck layer in the classifier, versus appearing after
    the bottleneck layer. Figure 8.14 depicts a fire group.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个火组由一个64个滤波器的单个火块组成，随后是一个用于正则化的dropout层，在分类组之前。这与当时的惯例略有不同，因为SqueezeNet的dropout层出现在分类器的瓶颈层之前，而不是之后。图8.14描述了一个火组。
- en: '![](Images/CH08_F14_Ferlitsch.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F14_Ferlitsch.png)'
- en: Figure 8.14 In the SqueezeNet group micro-architecture, the last fire group
    uses dropout instead of max pooling.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 在SqueezeNet组微架构中，最后一个火组使用dropout而不是max pooling。
- en: The following is an example implementation for the first and second fire group.
    Note that the parameter `filters` is a list, in which each element corresponds
    to a fire block and the value is the number of filters for that block. For example,
    consider the first fire group, which consists of three fire blocks; the input
    is 16 filters, and the output is 32 filters. The parameter `filters` would be
    the list [16, 16, 32].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对第一个和第二个火组的示例实现。请注意，参数`filters`是一个列表，其中每个元素对应一个火块，其值是该块的滤波器数量。例如，考虑第一个火组，它由三个火块组成；输入为16个滤波器，输出为32个滤波器。参数`filters`将是列表[16,
    16, 32]。
- en: 'After all the fire blocks have been added for the group, a `MaxPooling2D` layer
    is added for the delayed downsampling:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在为组添加所有火块之后，添加一个`MaxPooling2D`层以进行延迟下采样：
- en: '[PRE6]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Adds the fire blocks (modules) for the group
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为组添加火块（模块）
- en: ❷ Adds the delayed downsampling at the end of the group
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在组的末尾添加延迟下采样
- en: Figure 8.15 illustrates the fire block, which consists of two convolutional
    layers. The first layer is the squeeze layer, and the second layer is the expand
    layer. The *squeeze layer* reduces, or squeezes, the number of input channels
    to a lower dimensionality by using a 1 × 1 bottleneck convolution, while maintaining
    sufficient information for the subsequent convolution in the expand layer. The
    squeeze operation substantially lowers the number of parameters and corresponding
    matmul operations. In other words, the 1 × 1 bottleneck convolution learns the
    best way to maximize squeezing the number of feature maps into fewer feature maps,
    while still being able to perform feature extraction in the subsequent expand
    *layer*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15说明了火块，它由两个卷积层组成。第一层是挤压层，第二层是扩展层。*挤压层*通过使用1 × 1瓶颈卷积将输入通道的数量减少到更低的维度，同时保持足够的信息供扩展层中的后续卷积使用。挤压操作显著减少了参数数量和相应的矩阵乘法操作。换句话说，1
    × 1瓶颈卷积学习最大化挤压特征图数量到更少的特征图的最佳方式，同时仍然能够在后续的扩展*层*中进行特征提取。
- en: '![](Images/CH08_F15_Ferlitsch.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F15_Ferlitsch.png)'
- en: Figure 8.15 SqueezeNet fire block
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 SqueezeNet火块
- en: 'The *expand layer* is a branch of two convolutions: a 1 × 1 linear projection
    convolution and a 3 × 3 convolution, where the feature extraction occurs. The
    outputs (feature maps) from the convolutions are then concatenated. The expand
    layer expands the number of feature maps by a factor of 8.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*扩展层*是两个卷积的分支：一个1 × 1的线性投影卷积和一个3 × 3卷积，特征提取发生在其中。卷积的输出（feature maps）随后被连接。扩展层通过8倍因子扩展了feature
    maps的数量。'
- en: Let’s take an example. The input to the first fire block from the stem is 96
    feature maps (channels), and the squeeze layer reduces it to 16 feature maps.
    The expand layer then expands by a factor of 8, so the output is 96 feature maps
    again. The next (second) fire block once again squeezes it to 16 feature maps,
    and so forth.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子。来自主干的第一个fire块的输入是96个特征图（通道），squeeze层将其减少到16个特征图。然后扩展层将其扩展8倍，因此输出再次是96个特征图。下一个（第二个）fire块再次将其压缩到16个特征图，以此类推。
- en: The following is an example implementation of a fire block. The block starts
    with a 1 × 1 bottleneck convolution for the squeeze layer. The output `squeeze`
    from the squeeze layer is branched to the two parallel expansion convolutions
    `expand1x1` and `expand3x3`. Finally, the output from the two expansion convolutions
    is concatenated together.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例是一个fire块的实现。该块从squeeze层的1 × 1瓶颈卷积开始。squeeze层的输出`squeeze`分支到两个并行扩展卷积`expand1x1`和`expand3x3`。最后，两个扩展卷积的输出被连接在一起。
- en: '[PRE7]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Squeeze layer with 1 × 1 bottleneck convolution
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 带有1 × 1瓶颈卷积的squeeze层
- en: ❷ Expansion layer branches into 1 × 1 and 3 × 3 convolutions and doubles the
    number of filters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 扩展层分支为1 × 1和3 × 3卷积，并加倍了滤波器的数量。
- en: ❸ The branched output from the excitation layer is concatenated together.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从激励层输出的分支被连接在一起。
- en: 8.3.4 Classifier
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 分类器
- en: The classifier does not follow the conventional practice of a `GlobalAveragingPooling2D`
    layer followed by a `Dense` layer in which the number of output nodes equals the
    number of classes. Instead, it uses a convolutional layer, and the number of filters
    equals the number of classes, followed by a `GlobalAveragingPooling2D` layer.
    This arrangement reduces each prior filter (class) to a single value. The outputs
    from the `GlobalAveragingPooling2D` layer are then passed through a softmax activation
    to get a probability distribution across all the classes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器不遵循传统的`GlobalAveragingPooling2D`层后跟一个`Dense`层（输出节点的数量等于类别的数量）的做法。相反，它使用一个卷积层，滤波器的数量等于类别的数量，然后跟一个`GlobalAveragingPooling2D`层。这种安排将每个先前的滤波器（类别）减少到单个值。然后，`GlobalAveragingPooling2D`层的输出通过softmax激活，得到所有类别的概率分布。
- en: Let’s revisit a conventional classifier. In a conventional classifier, the final
    feature maps are reduced and flattened to a lower dimensionality at the bottleneck
    layer, generally with `GlobalAveragingPooling2D`. We would now have 1 pixel per
    feature map as a 1D vector (embedding). This 1D vector is then passed to a dense
    layer, where the number of nodes is equal to the number of output classes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一个传统的分类器。在传统的分类器中，最终的feature maps在瓶颈层被减少并展平到更低的维度，通常使用`GlobalAveragingPooling2D`。现在，每个feature
    map将有一个像素作为1D向量（嵌入）。这个1D向量随后被传递到一个密集层，其中节点的数量等于输出类别的数量。
- en: Figure 8.16 shows the classifier component. In SqueezeNet, the final feature
    maps are passed through a 1 × 1 linear projection, which learns to project the
    final feature maps to a new set that is exactly equal to the number of output
    classes. Now, these projected feature maps, each corresponding to a class, are
    reduced to a single pixel per feature map and flattened, becoming a 1D vector
    whose length is exactly equal to the number of output classes. The 1D vector is
    then passed through a softmax for prediction.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16显示了分类器组件。在SqueezeNet中，最终的feature maps通过一个1 × 1的线性投影，该投影学习将最终的feature maps投影到一个新的集合，该集合正好等于输出类别的数量。现在，这些投影的feature
    maps，每个对应一个类别，被减少到每个feature map的单个像素，并展平，成为一个长度正好等于输出类别数量的1D向量。这个1D向量随后通过softmax进行预测。
- en: '![](Images/CH08_F16_Ferlitsch.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F16_Ferlitsch.png)'
- en: Figure 8.16 SqueezeNet classifier group using a convolution instead of a dense
    layer for classification
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 使用卷积而不是密集层进行分类的SqueezeNet分类器组
- en: What’s the fundamental difference? In a conventional classifier, the dense layer
    learns the classification. In this mobile version, the 1 × 1 linear projection
    learns the classification.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 基本区别是什么？在传统的分类器中，密集层学习分类。在这个移动版本中，1 × 1线性投影学习分类。
- en: 'The following is an example implementation of the classifier. In this example,
    the inputs, which are the final feature maps, are passed through a `Conv2D` layer
    that does the 1 × 1 linear projection into the number of output classes. The subsequent
    feature maps are then reduced to a single pixel 1D vector with `GlobalAveragePooling2D`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对分类器的一个示例实现。在这个例子中，输入是最终的特征图，通过一个`Conv2D`层进行1 × 1线性投影到输出类别数量。随后，特征图通过`GlobalAveragePooling2D`被减少到一个单像素的1D向量：
- en: '[PRE8]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Sets the number of filters equal to the number of classes
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将过滤器数量设置为类别数量
- en: ❷ Reduces each filter (class) to a single value for classification
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将每个过滤器（类别）简化为单个值用于分类
- en: Next, let’s look deeper into the classifier design by constructing it with the
    conventional approach of large SOTA models. Figure 8.17 depicts the conventional
    approach. The final feature maps are globally pooled into a 1 × 1 matrix (a single
    value) each. The matrices are then flattened into a 1D vector that’s the length
    of the number of feature maps (such as 2048 in ResNet). The 1D vector is then
    passed through a dense layer with a softmax activation that outputs a probability
    for each class.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过构建传统的大规模SOTA模型的传统方法来更深入地了解分类器设计。图8.17展示了传统方法。最终的特征图被全局池化成一个1 × 1矩阵（一个值）。然后矩阵被展平成一个长度等于特征图数量的1D向量（例如ResNet中的2048）。然后1D向量通过一个具有softmax激活的密集层，输出每个类别的概率。
- en: '![](Images/CH08_F17_Ferlitsch.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F17_Ferlitsch.png)'
- en: Figure 8.17 Feature map processing in a conventional, large SOTA classifier
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 传统的大规模SOTA分类器中的特征图处理
- en: Figure 8.18 depicts the approach in SqueezeNet. The feature maps are processed
    by a 1 × 1 bottleneck convolution, which reduces the number of feature maps to
    the number of classes. In essence, this is the class prediction step—except we
    don’t have a single value, but an *N* × *N* matrix. The *N* × *N* matrix predictions
    are then globally pooled into 1 × 1 matrices, which are then flattened into a
    1D vector, in which each element is the probability for the corresponding class.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18展示了SqueezeNet中的方法。特征图通过一个1 × 1瓶颈卷积进行处理，将特征图的数量减少到类别数量。本质上，这是类别预测步骤——除了我们没有单个值，而是一个*N*
    × *N*矩阵。然后*N* × *N*矩阵的预测被全局池化成1 × 1矩阵，这些矩阵随后被展平成一个1D向量，其中每个元素是对应类别的概率。
- en: '![](Images/CH08_F18_Ferlitsch.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F18_Ferlitsch.png)'
- en: Figure 8.18 Using a convolution instead of dense layer for classification
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 使用卷积而不是密集层进行分类
- en: 8.3.5 Bypass connections
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.5 绕过连接
- en: In their ablation study, the authors experimented with micro-architecture search
    of a block using an identity link, introduced in ResNet, which they referred to
    as *bypass connections*. SqueezeNet, they said in their paper, resided “in a broad
    and largely unexplored design space of CNN architectures.” Part of their exploration
    included what they called the *micro-architecture design space*. They indicated
    they were inspired by the ResNet authors’ A/B comparison on a ResNet34 with and
    without bypass connections and obtained a 2% performance improvement with a bypass
    connection.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的消融研究中，作者使用ResNet中引入的恒等连接对块进行微架构搜索，他们将这种连接称为*绕过连接*。他们在论文中说，SqueezeNet位于“CNN架构的广泛且大部分未探索的设计空间中。”他们探索的一部分包括他们所说的*微架构设计空间*。他们指出，他们受到了ResNet作者在ResNet34上带有和不带有绕过连接的A/B比较的启发，并通过绕过连接获得了2%的性能提升。
- en: The authors experimented with what they called a simple bypass and a complex
    bypass. In the *simple bypass*, they gained a 2.9% increase on top-1, and 2.2%
    on top-5, accuracy for ImageNet without increasing computational complexity. Thus
    their improvements were comparable to those observed by the ResNet authors.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 作者尝试了他们所说的简单绕过和复杂绕过。在*简单绕过*中，他们在ImageNet上获得了2.9%的top-1准确率提升和2.2%的top-5准确率提升，而没有增加计算复杂度。因此，他们的改进与ResNet作者观察到的改进相当。
- en: In the *complex bypass*, they observed a lesser improvement, with a gain of
    only 1.3% accuracy with an increase in model size from 4.8 MB to 7.7 MB. In the
    simple bypass, there was no increase in model size. The authors concluded the
    simple bypass was sufficient.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在*复杂旁路*中，他们观察到较小的改进，准确率仅提高了1.3%，模型大小从4.8 MB增加到7.7 MB。在简单旁路中，模型大小没有增加。作者得出结论，简单旁路是足够的。
- en: Simple bypass
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 简单旁路
- en: In the simple bypass, the identity link occurs only in the first fire block
    (entry into the group) and the fire block prior to the doubling of filters. Figure
    8.19 illustrates a fire group with simple bypass connection. The first fire block
    in the group has the bypass connection (identity link), and then the fire block
    preceding the fire block doubles the number of output channels (feature maps).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单旁路中，恒等链接仅在第一个fire块（组入口）和过滤器加倍之前的fire块中出现。图8.19说明了具有简单旁路连接的fire组。组中的第一个fire块具有旁路连接（恒等链接），然后是fire块，它将输出通道数（特征图）加倍。
- en: '![](Images/CH08_F19_Ferlitsch.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F19_Ferlitsch.png)'
- en: Figure 8.19 SqueezeNet group with simple bypass blocks
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 SqueezeNet组带有简单旁路块
- en: Now let’s take a close-up look at a fire block with a simple bypass (identity
    link) connection. This is depicted in figure 8.20\. Note that the input to the
    block is added to the output of the concatenation operation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来近距离观察一个具有简单旁路（恒等链接）连接的fire块。这如图8.20所示。请注意，块输入被添加到连接操作的输出中。
- en: '![](Images/CH08_F20_Ferlitsch.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F20_Ferlitsch.png)'
- en: Figure 8.20 SqueezeNet fire block with identity link
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 SqueezeNet fire块带有恒等链接
- en: Let’s walk through this. First, we know that with a matrix add operation, the
    number of feature maps on the input must match the number of outputs from the
    concatenation operation. For a multitude of fire blocks, this is true. For example,
    from the stem group, we have 96 feature maps as input, which are reduced to 16
    in the squeeze layer, and then expanded by 8 times (back to 96) through the expand
    layer. Since the number of feature maps on the input equals the output, we can
    add an identity link. But this isn’t the case for all the fire blocks, and thus
    why only a subset have the bypass connection.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来看。首先，我们知道，使用矩阵加法操作，输入上的特征图数量必须与连接操作的输出数量相匹配。对于许多fire块来说，这是正确的。例如，从stem组中，我们有96个特征图作为输入，在squeeze层中减少到16，然后通过expand层扩展8倍（回到96）。由于输入上的特征图数量等于输出，我们可以添加一个恒等链接。但并非所有fire块都是这样，这就是为什么只有一部分具有旁路连接。
- en: 'The following is an example implementation of a fire block with a simple bypass
    connection (identity link). In this implementation, we pass the additional parameter
    `bypass`. If it is true, we add a final layer at the end of the block that does
    a matrix add (`Add()`) to the output from the concatenation:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个具有简单旁路连接（恒等链接）的fire块的示例实现。在这个实现中，我们传递额外的参数`bypass`。如果它是真的，我们在块的末尾添加一个最终层，该层对来自连接操作的输出执行矩阵加法（`Add()`）：
- en: '[PRE9]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ When bypass is True, the input (shortcut) is matrix-added to the output of
    the fire block.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当bypass为True时，输入（快捷方式）会矩阵加到fire块的输出上。
- en: Complex bypass
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂旁路
- en: In the authors’ next micro-architecture search, they explored adding a linear
    projection to the remaining fire blocks without an identity link (simple bypass).
    The linear projection would project the number of input features to be equal to
    the number of output feature maps after the concatenation operation. They referred
    to this as the *complex bypass*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在作者接下来的微架构搜索中，他们探索了在不使用恒等链接（简单旁路）的情况下向剩余的fire块添加线性投影。线性投影会将输入特征的数量投影到连接操作后等于输出特征图数量的数量。他们将这称为*复杂旁路*。
- en: The intent was to see if this would further increase top-1/top-5 accuracy, though
    at the expense of increasing model size. As I noted earlier, their experiments
    showed that using the complex bypass was detrimental to the objective. Figure
    8.21 depicts a fire group in which the remaining fire blocks without a simple
    bypass (identity link) have a complex bypass (linear projection link).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 目的是看看这是否会进一步提高top-1/top-5准确率，尽管会增加模型大小。正如我之前提到的，他们的实验表明使用复杂旁路对目标是有害的。图8.21描绘了一个fire组，其中剩余的没有简单旁路（恒等链接）的fire块具有复杂旁路（线性投影链接）。
- en: '![](Images/CH08_F21_Ferlitsch.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F21_Ferlitsch.png)'
- en: Figure 8.21 SqueezeNet group with projection shortcut fire blocks (complex bypass)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21 SqueezeNet组带有投影快捷fire块（复杂旁路）
- en: Now for a closer look at a fire block with a complex bypass, shown in figure
    8.22\. Note that the 1 × 1 linear projection on the identity link increases the
    number of filters (channels) by 8\. This is to match the output size of the concatenation
    of outputs from the branched 1 × 1 and 3 × 3, both of which increased the output
    size by 4 (4 + 4 = 8). Using a 1 × 1 linear projection on the identity link is
    what distinguished the complex from the simple bypass.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地看看图8.22中所示的带复杂旁路的fire块。请注意，在身份链接上的1 × 1线性投影将滤波器（通道）的数量增加了8。这是为了匹配分支1
    × 1和3 × 3输出连接的大小，两者都增加了输出大小4（4 + 4 = 8）。在身份链接上使用1 × 1线性投影是将复杂旁路与简单旁路区分开来的关键。
- en: '![](Images/CH08_F22_Ferlitsch.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F22_Ferlitsch.png)'
- en: Figure 8.22 SqueezeNet fire block with projection shortcut (complex bypass)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22 SqueezeNet带投影快捷方式的fire块（复杂旁路）
- en: In the ablation study, the use of a simple bypass increased accuracy from the
    vanilla SqueezeNet on ImageNet from 57.5% to 60.4%. For the complex bypass, the
    accuracy increased to only 58.8%. The authors made no conclusion about why, other
    than to say it was interesting. A complete code rendition using the Idiomatic
    procedure reuse design pattern for SqueezeNet is on GitHub ([http://mng.bz/XYmv](https://shortener.manning.com/XYmv)).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在消融研究中，使用简单的旁路将ImageNet上vanilla SqueezeNet的准确率从57.5%提高到60.4%。对于复杂旁路，准确率仅提高到58.8%。作者没有对为什么会出现这种情况做出结论，除了说这很有趣。使用Idiomatic
    procedure reuse设计模式对SqueezeNet的完整代码实现可在GitHub上找到（[http://mng.bz/XYmv](https://shortener.manning.com/XYmv)）。
- en: Next, we will cover ShuffleNet, which introduced pointwise group convolutions
    and channel shuffle (transpose) operations to increase the number of feature maps
    without increasing computational complexity and size.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍ShuffleNet，它引入了点卷积和通道洗牌（转置）操作，在不增加计算复杂性和尺寸的情况下增加特征图的数量。
- en: 8.4 ShuffleNet v1
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 ShuffleNet v1
- en: One of the challenges of large networks is that they need many feature maps,
    typically in the thousands, which means they have a high computational cost. So
    in 2017, Xiangyu Zhang et al. at Face++ introduced a way to have a large number
    of feature maps at a substantial reduction in compute costs. This new architecture,
    called *ShuffleNet v1* ([https://arxiv.org/abs/1707.01083](https://arxiv.org/abs/1707.01083)),
    was designed specifically for low-compute devices typically found on mobile phones,
    drones, and robots.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 大型网络的一个挑战是它们需要许多特征图，通常有数千个，这意味着它们有很高的计算成本。因此，在2017年，Face++的Xiangyu Zhang等人提出了一种在大幅降低计算成本的同时拥有大量特征图的方法。这种新的架构称为*ShuffleNet
    v1* ([https://arxiv.org/abs/1707.01083](https://arxiv.org/abs/1707.01083))，专门为通常在手机、无人机和机器人上发现的低计算设备设计。
- en: 'The architecture introduced new layer operations: groupwise point convolutions
    and channel shuffle. When compared to MobileNet, the authors found that ShuffleNet
    achieves superior performance by a significant margin: absolute 7.8% lower ImageNet
    top-1 error at a level of 40 MFLOPs. While the authors reported gains in accuracy
    over the MobileNet counterparts, MobileNets continued to be favored for production,
    though they’re now being replaced by EfficientNets.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构引入了新的层操作：分组点卷积和通道洗牌。与MobileNet相比，作者发现ShuffleNet通过显著的优势实现了更好的性能：在40 MFLOPs的水平上，ImageNet
    top-1错误率绝对降低了7.8%。虽然作者报告了在MobileNet对应版本上的准确率提升，但MobileNets仍然在生产中受到青睐，尽管它们现在正被EfficientNets所取代。
- en: 8.4.1 Architecture
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 架构
- en: The ShuffleNet architecture consists of three shuffle groups, which the paper
    refers to as *stages*. The architecture followed the conventional practice, with
    each group doubling the number of output channels or feature maps from the previous
    group. Figure 8.23 depicts the ShuffleNet architecture.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ShuffleNet架构由三个洗牌组组成，论文中将其称为*阶段*。该架构遵循传统做法，每个组将前一个组的输出通道或特征图数量翻倍。图8.23展示了ShuffleNet架构。
- en: '![](Images/CH08_F23_Ferlitsch.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F23_Ferlitsch.png)'
- en: Figure 8.23 In the ShuffleNet v1 macro-architecture, each group doubles the
    number of output feature maps.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23 在ShuffleNet v1宏架构中，每个组将输出特征图的数量翻倍。
- en: 8.4.2 Stem
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 茎
- en: The stem component uses a less coarse 3 × 3 convolutional layer when compared
    to other mobile SOTA models at the time, which typically used a 7 × 7 or stack
    of two 3 × 3 convolutional layers. The stem, depicted in figure 8.24, performs
    an aggressive feature map reduction, which continues to be the present convention.
    The 3 × 3 convolution is strided (feature pooling) for a 75% reduction, followed
    by a max pooling layer for a further 75% reduction, resulting in feature maps
    that are 6% the size of the input channels. Reducing the channel size from the
    input to 6% continues to be the conventional practice.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 与当时其他移动端SOTA模型相比，主干组件使用了更精细的3 × 3卷积层，而其他模型通常使用7 × 7或两个3 × 3卷积层的堆叠。主干组件，如图8.24所示，执行了激进的特征图降维，这至今仍是一种惯例。3
    × 3卷积层采用步长（特征池化）以实现75%的降维，随后通过一个最大池化层进一步降维75%，结果得到的特征图大小仅为输入通道的6%。从输入到6%的通道尺寸降低一直是一种传统做法。
- en: '![](Images/CH08_F24_Ferlitsch.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH08_F24_Ferlitsch.png)'
- en: Figure 8.24 The ShuffleNet stem group combines feature and max pooling for an
    output feature map reduction to 6% of the size of the input.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24 ShuffleNet主干组件通过结合特征和最大池化来降低输出特征图的大小，降至输入大小的6%。
- en: 8.4.3 Learner
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 学习组件
- en: Each group in the learner component consists of a strided shuffle block (referred
    to as a *unit* in the paper), followed by one or more shuffle blocks. The strided
    shuffle block doubles the number of output channels while reducing the size of
    each channel by 75%. The progressive doubling of the number of filters, and hence
    output feature maps, per feature was the convention of the time and continues
    today. It has also been a convention that when a group doubles the number of output
    feature maps, their sizes are reduced to prevent an explosion in parameter growth
    as you go deeper into layers.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 学习组件中的每个组由一个步长洗牌块（在论文中称为“单元”）组成，后面跟一个或多个洗牌块。步长洗牌块将输出通道数加倍，同时将每个通道的大小减少75%。每个特征中过滤器数量和输出特征图的逐步加倍，当时是一种惯例，并且至今仍保持。同时，当一组将输出特征图的数量加倍时，它们的尺寸也会减少，以防止在层中深入时参数增长爆炸。
- en: Group
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 组
- en: Like MobileNet v1/v2, the ShuffleNet group does the feature map reduction at
    the beginning of the group, with the strided shuffle block. This is in contrast
    to SqueezeNet and large SOTA models that delay the feature map reduction to the
    end of the group. By reducing the size at the beginning of the group, the number
    of parameters and matmul operations are substantially reduced, but at the cost
    of less representational power.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 与MobileNet v1/v2类似，ShuffleNet组在组的开始处进行特征图降维，使用步长洗牌块。这与SqueezeNet和大型SOTA模型将特征图降维推迟到组末的做法形成对比。通过在组开始处减小尺寸，参数数量和矩阵乘法操作的数量显著减少，但代价是减少了表示能力。
- en: Figure 8.25 illustrates a shuffle group. The group starts with the strided shuffle
    block, which does the feature map size reduction at the beginning of the group,
    and then one or more shuffle blocks follows. The strided and subsequent shuffle
    blocks double the number of filters from the previous group. For example, if the
    previous group had 144 filters, the current group would double to 288.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25说明了洗牌组。组从步长洗牌块开始，它在组的开始处进行特征图尺寸的降低，然后跟一个或多个洗牌块。步长和随后的洗牌块将前一个组的过滤器数量加倍。例如，如果前一个组有144个过滤器，当前组将加倍到288个。
- en: '![](Images/CH08_F25_Ferlitsch.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH08_F25_Ferlitsch.png)'
- en: Figure 8.25 ShuffleNet group micro-architecture
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25 ShuffleNet组微观架构
- en: 'The following is an example implementation of a shuffle group. The parameter
    `n_blocks` is the number of blocks in the group, and `n_filters` is the number
    of filters for each block. The parameter `reduction` is a metaparameter for dimensionality
    reduction in the shuffle block (discussed subsequently), and the parameter `n_partitions`
    is the metaparameter used for partitioning the feature maps for the channel shuffle
    (discussed subsequently). The first block is a strided shuffle block, and the
    remaining blocks are not strided: `for _ in range(n_blocks-1)`.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例实现了一个洗牌组。参数`n_blocks`是组中的块数，`n_filters`是每个块的过滤器数量。参数`reduction`是洗牌块中维度降低的元参数（随后讨论），参数`n_partitions`是用于通道洗牌的分区元参数（随后讨论）。第一个块是一个步长洗牌块，其余块不是步长的：`for
    _ in range(n_blocks-1)`。
- en: '[PRE10]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ First block in a group is a strided shuffle block.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 组中的第一个块是一个步长洗牌块。
- en: ❷ Adds the remaining nonstrided shuffle blocks
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加剩余的非步长洗牌块
- en: Block
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 块
- en: The shuffle block is based on a B(1, 3, 1) residual block, where the 3 × 3 convolution
    is a depthwise convolution (as in MobileNet). The authors noted that architectures
    such as Xception and ResNeXt become less efficient in extremely small networks
    because of the costly dense 1 × 1 convolutions. To address this, they replaced
    the 1 × 1 pointwise convolutions with pointwise group convolutions to reduce computational
    complexity. Figure 8.26 shows the difference in design.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffle块基于B(1, 3, 1)残差块，其中3 × 3卷积是深度卷积（如MobileNet）。作者指出，由于昂贵的密集1 × 1卷积，Xception和ResNeXt等架构在极小的网络中效率降低。为了解决这个问题，他们用逐点组卷积替换了1
    × 1逐点卷积，以降低计算复杂度。图8.26显示了设计上的差异。
- en: '![](Images/CH08_F26_Ferlitsch.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F26_Ferlitsch.png)'
- en: Figure 8.26 Comparing ResNet and ShuffleNet B(1,3,1) designs
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.26比较ResNet和ShuffleNet B(1,3,1)设计
- en: The first pointwise group convolution also performs a dimensionality reduction
    on the number of input filters to the block, when the parameter `reduction` is
    < 1 `(reduction` `*` `n_filters)`, and then it is restored in the output channels
    with the second pointwise group convolution, for matching the input with the residual
    for the matrix add operation.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 当参数`reduction`小于1时，第一个逐点组卷积也会对块输入的滤波器数量进行降维（`reduction` `*` `n_filters`），然后在第二个逐点组卷积的输出通道中恢复，以匹配矩阵加法操作的输入残差。
- en: They also deviated from the practice in Xception of using a ReLU after the depthwise
    convolution, to use a linear activation. Their reasoning for this change is not
    clear, nor is the advantage of using a linear activation. The paper merely states,
    “The usage of batch normalization (BN) and nonlinearity is similar to [ResNet,
    ResNeXt], except that we do not use ReLU after depthwise convolution as suggested
    by [Xception].” Between the first pointwise group convolution and the depthwise
    convolution is the channel shuffle operation, both of which will be discussed
    subsequently.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还偏离了Xception中在深度卷积后使用ReLU的惯例，转而使用线性激活。他们对此变化的原因并不明确，使用线性激活的优势也不清楚。论文仅陈述：“批归一化（BN）和非线性的使用与[ResNet,
    ResNeXt]类似，只是我们没有像[Xception]建议的那样在深度卷积后使用ReLU。”在第一个逐点组卷积和深度卷积之间是通道洗牌操作，这两个操作将在后面讨论。
- en: Figure 8.27 shows a shuffle block. You can see how the channel shuffle has been
    inserted in the B(1,3,1) residual block design before the 3 × 3 depthwise convolution,
    where the feature extraction occurs. The B(1, 3, 1) residual block is a bottleneck
    design comparable to MobileNet v1, in which the first 1 × 1 convolution does a
    dimensionality reduction, and the second 1 × 1 convolution does a dimensionality
    expansion. The block continued with the convention in MobileNet of pairing a 3
    × 3 depthwise convolution with a 1 × 1 pointwise group convolution to form a depthwise
    separable convolution. It differs from MobileNet v1, though, by changing the first
    1 × 1 bottleneck convolution into a 1 × 1 bottleneck pointwise group convolution.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.27展示了shuffle块。你可以看到通道洗牌是如何在3 × 3深度卷积之前插入到B(1,3,1)残差块设计中的，特征提取就在这里发生。B(1,
    3, 1)残差块是一个与MobileNet v1相当的瓶颈设计，其中第一个1 × 1卷积进行降维，第二个1 × 1卷积进行升维。该块继续遵循MobileNet中的惯例，将3
    × 3深度卷积与1 × 1逐点组卷积配对，形成一个深度可分离卷积。尽管如此，它与MobileNet v1的不同之处在于，将第一个1 × 1瓶颈卷积改为1 ×
    1瓶颈逐点组卷积。
- en: '![](Images/CH08_F27_Ferlitsch.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F27_Ferlitsch.png)'
- en: Figure 8.27 ShuffleNet block using Idiomatic design
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.27 ShuffleNet块使用惯用设计
- en: The following is an example implementation of a shuffle block. The block starts
    with a pointwise 1 × 1 group convolution defined in the function `pw_group_conv``()`,
    where the parameter value `int(reduction * n_filters)` specifies the dimensionality
    reduction. Next is the channel shuffle defined in the function `channel_shuffle()`,
    followed by the depthwise convolution (`DepthwiseConv2D`). Next is the final pointwise
    group 1 × 1 convolution, which restores the dimensionality. Finally, the input
    to the block is matrix-added (`Add()`) to the output from the pointwise group
    convolution.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个shuffle块的示例实现。该块从函数`pw_group_conv``()`中定义的逐点1 × 1组卷积开始，其中参数值`int(reduction
    * n_filters)`指定了降维。接下来是函数`channel_shuffle()`中定义的通道洗牌，然后是深度卷积（`DepthwiseConv2D`）。接下来是最终的逐点组1
    × 1卷积，它恢复了维度。最后，将块的输入通过矩阵加法（`Add()`）与逐点组卷积的输出相加。
- en: '[PRE11]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The first pointwise group convolution does a dimensionality reduction.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个逐点组卷积操作进行了一次降维。
- en: ❷ The channel shuffle
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通道洗牌
- en: ❸ 3 × 3 depthwise convolution
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 3 × 3 深度卷积
- en: ❹ The second group convolution does a dimensionality restoration.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第二组卷积进行维度恢复。
- en: ❺ Adds the input (shortcut) to the output of the block
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将输入（快捷连接）添加到块的输出中
- en: Pointwise group convolution
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 点卷积组
- en: The following is an example implementation of a pointwise group convolution.
    The function starts by determining the number of input channels `(in_filters =
    x.shape[-1]`). Next, the number of channels per group is determined by dividing
    the number of input channels by the number of groups (`n_partitions`). The feature
    maps are then proportionally split across the groups (`lambda`), and each group
    is passed through a separate 1 × 1 pointwise convolution. Finally, the outputs
    from the group convolutions are concatenated together and passed through a batch
    normalization layer.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个点卷积组卷积的示例实现。函数首先确定输入通道的数量 `(in_filters = x.shape[-1])`。接下来，通过将输入通道数除以组数
    (`n_partitions`) 来确定每个组的通道数。然后，特征图按比例分配到各个组中 (`lambda`)，每个组通过一个单独的 1 × 1 点卷积。最后，将组卷积的输出连接在一起并通过批量归一化层。
- en: '[PRE12]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Calculates the number of input feature maps (channels)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算输入特征图（通道）的数量
- en: ❷ Calculates the number of input and output filters (channels) per group. Note
    the rounding up.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算每个组的输入和输出滤波器（通道）数量。注意向上取整。
- en: ❸ Performs 1 × 1 linear pointwise convolution across each channel group
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在每个通道组上执行 1 × 1 线性点卷积
- en: ❹ Slices the feature maps across the channel group
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 沿通道组切片特征图
- en: ❺ Maintains the group pointwise group convolutions in a list
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在列表中保持组点卷积。
- en: ❻ Concatenates the outputs of the group pointwise convolutions together
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将组点卷积的输出连接在一起
- en: ❼ Does batch normalization of the concatenated group outputs (feature maps)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对连接的组输出（特征图）进行批量归一化
- en: Strided shuffle block
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 步长洗牌块
- en: 'The strided shuffle block differs as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 步长洗牌块与以下不同：
- en: The dimensionality of the shortcut link (input to the block) is reduced by a
    3 × 3 average pooling operation.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短路连接（块输入）的维度通过 3 × 3 平均池化操作减少。
- en: The residual and shortcut feature maps are concatenated instead of using a matrix
    add in the nonstrided shuffle block.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非步长洗牌块中，使用矩阵加法而不是连接残差和快捷特征图。
- en: As for the use of concatenation, the authors reasoned to “replace the element-wise
    addition with channel concatenation, which makes it easy to enlarge channel dimension
    with little extra computation cost.”
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用连接，作者推理出“用通道连接替换逐元素加法，这样可以在很少的计算成本下轻松扩大通道维度。”
- en: Figure 8.28 depicts a strided shuffle block. You can see the two differences
    from the nonstrided shuffle block. On the shortcut link, an average pooling has
    been added that does a dimensionality reduction by reducing the feature maps to
    0.5*H* × 0.5*W*. This is done to match the size of the feature pooling that is
    done by the strided 3 × 3 depthwise convolution, so they can be concatenated together—instead
    of a matrix addition in the nonstrided shuffle block.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28 描述了一个步长洗牌块。你可以看到与非步长洗牌块的两个不同之处。在快捷连接上添加了一个平均池化，通过将特征图减少到 0.5*H* × 0.5*W*
    来进行维度减少。这是为了匹配步长 3 × 3 深度卷积所做的特征池化大小，这样它们就可以连接在一起——而不是在非步长洗牌块中的矩阵加法。
- en: '![](Images/CH08_F28_Ferlitsch.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F28_Ferlitsch.png)'
- en: Figure 8.28 Strided shuffle block
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28 步长洗牌块
- en: The following is an example implementation of a strided shuffle block. The parameter
    `n_filters` is the number of filters for the convolution layers in the block.
    The parameter `reduction` is the metaparameter for further thinning of the network,
    and the parameter `n_partitions` specifies the number of groups to partition the
    feature maps into for the pointwise group convolution.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个步长洗牌块的示例实现。参数 `n_filters` 是块中卷积层的滤波器数量。参数 `reduction` 是用于进一步细化网络的元参数，参数
    `n_partitions` 指定了将特征图划分成多少组进行点卷积组。
- en: The function starts by creating the projection shortcut. The input is passed
    through a strided `AveragePooling2D` layer, which reduces the size of the feature
    maps in the projection shortcut to 0.5*H* × 0.5*W*.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先创建投影快捷连接。输入通过一个步长的 `AveragePooling2D` 层，将投影快捷连接中的特征图大小减少到 0.5*H* × 0.5*W*。
- en: The input is then passed through the 1 × 1 pointwise group convolution (`pw_
    group_conv()`). Note that the network thinning occurs in the first pointwise group
    convolution (`int(reduction` `*` `n_filters)`). The input is channel-shuffled
    (`channel_ shuffle()`), and then passed through the 3 × 3 strided depthwise convolution
    that does the feature extraction and feature pooling; note there is no ReLU activation.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 输入随后通过1 × 1点卷积组卷积(`pw_ group_conv()`)。请注意，网络细化发生在第一个点卷积组卷积(`int(reduction` `*`
    `n_filters)`)。输入经过通道混洗(`channel_ shuffle()`),然后通过3 × 3步长深度卷积，进行特征提取和特征池化；注意这里没有ReLU激活。
- en: The output from `DepthwiseConv2D()` is then passed through the second 1 × 1
    pointwise group convolution, whose output is then concatenated with the projection
    shortcut.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`DepthwiseConv2D()`的输出随后通过第二个1 × 1点卷积组卷积，其输出随后与投影快捷连接。'
- en: '[PRE13]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Uses average pooling for a bottleneck shortcut
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用平均池化进行瓶颈快捷连接
- en: ❷ On the first block, the number of output filters of the entry pointwise group
    convolution is adjusted to match the exit pointwise group convolution.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在第一个块中，入口点卷积组卷积的输出滤波器数量调整为与出口点卷积组卷积匹配。
- en: ❸ Concatenates the projection shortcut to the output of the block
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将投影快捷连接到块的输出
- en: Channel shuffle
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 通道混洗
- en: 'The *channel shuffle* was designed to overcome side effects with the group
    convolutions, thus helping information to flow across the output channels. The
    group convolution significantly reduces computation cost by ensuring that each
    convolution operates only on the corresponding input channel group. As the authors
    point out, if multiple group convolutions are stacked together, one side effect
    results: outputs from a certain channel are derived from only a small fraction
    of input channels. In other words, each group convolution is limited to learning
    the next feature extraction level for its filter, based only on a single feature
    map (channel) instead of from all, or portion of, all the input feature maps.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '*通道混洗*被设计用来克服组卷积的副作用，从而帮助信息在输出通道中流动。组卷积通过确保每个卷积只操作对应的输入通道组，显著降低了计算成本。正如作者所指出的，如果将多个组卷积堆叠在一起，会产生一个副作用：某些通道的输出仅来自输入通道的一小部分。换句话说，每个组卷积仅限于根据单个特征图（通道）学习其滤波器的下一个特征提取级别，而不是所有或部分输入特征图。'
- en: Figure 8.29 depicts splitting channels into groups and then shuffling the channels.
    In essence, a shuffle consists of constructing new channels, as each shuffled
    channel has a portion from every other channel—thus increasing information flow
    across the output channels.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.29展示了将通道分成组并随后混洗通道的过程。本质上，混洗是通过构建新的通道来实现的，因为每个混洗通道都包含来自其他每个通道的部分——从而增加了输出通道之间的信息流。
- en: '![](Images/CH08_F29_Ferlitsch.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F29_Ferlitsch.png)'
- en: Figure 8.29 Channel shuffle
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.29 通道混洗
- en: Let’s take a closer look at this process. We start with a group of input channels,
    which I gray-shaded in the diagram to denote that they are different channels
    (not copies). Next, based on the partition setting, the channels are split into
    equal-size partitions, which we call *groups*. In our depiction, each group has
    three separate channels. We construct three shuffled versions of the three channels.
    Through the gray shading, we denote that each shuffled channel is formed from
    a portion of each unshuffled channel, and the portion differs for each shuffled
    channel.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这个过程。我们从一个输入通道组开始，我在图中用灰色阴影表示，以表明它们是不同的通道（不是副本）。接下来，根据分区设置，通道被分成相等大小的分区，我们称之为*组*。在我们的表示中，每个组有三个独立的通道。我们构建了三个通道的混洗版本。通过灰色阴影，我们表示每个混洗通道是由每个未混洗通道的部分组成的，并且每个混洗通道的部分是不同的。
- en: For example, the first shuffled channel is constructed from the first one-third
    of the feature maps of the three unshuffled channels. The second shuffled channel
    is constructed from the first one-third of the feature maps of the three unshuffled
    channels, and so forth.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第一个混洗通道是由三个未混洗通道的特征图的前三分之一构建的。第二个混洗通道是由三个未混洗通道的特征图的前三分之一构建的，以此类推。
- en: The following is an example implementation of a channel shuffle. The parameter
    `n_partitions` specifies the number of groups to partition the input feature map,
    parameter `x`, into. We use the shape of the input to determine *B* × *H* × *W*
    × *C* (where *C* is channels), and then calculate the number of channels per group
    (`grp_in_channels`).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个通道洗牌的示例实现。参数 `n_partitions` 指定了将输入特征图 `x` 分成多少组。我们使用输入的形状来确定 *B* × *H*
    × *W* × *C*（其中 *C* 是通道），然后计算每个组的通道数（`grp_in_channels`）。
- en: 'The next three Lambda operations do the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的三个Lambda操作执行以下操作：
- en: Reshapes the input from *B* × *H* × *W* × *C* to *B* × *W* × *W* × *G* × *Cg*.
    A fifth dimension, *G* (groups), is added, and *C* is reshaped into *G* × *Cg*,
    where *Cg* is the subset of channels per group.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入从 *B* × *H* × *W* × *C* 重塑为 *B* × *W* × *W* × *G* × *Cg*。添加了一个第五维 *G*（组），并将
    *C* 重塑为 *G* × *Cg*，其中 *Cg* 是每个组中通道的子集。
- en: The `k.permute_dimesions()` performs the channel shuffle depicted in figure
    5.27.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`k.permute_dimensions()` 执行了图5.27中展示的通道洗牌操作。'
- en: The second reshape reconstructs the shuffled channels back into the shape *B*
    × *H* × *W* × *C*.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二次重塑将洗牌后的通道重新构建为形状 *B* × *H* × *W* × *C*。
- en: '[PRE14]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Gets dimensions of the input tensor
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取输入张量的维度
- en: ❷ Derives the number of input filters (channels) per group
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 推导每个组中输入滤波器（通道）的数量
- en: ❸ Separates out the channel groups
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 分离通道组
- en: ❹ Transposes the order (shuffle) of the channel groups (i.e., 3, 4 => 4, 3)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 交换通道组的顺序（洗牌）（即，3, 4 => 4, 3）
- en: ❺ Restores the output shape
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 恢复输出形状
- en: In their ablation study, the authors found that the best tradeoff in complexity
    versus accuracy was for a reduction factor of 1 (no reduction), and the number
    of group partitions set to 8\. A complete code rendition using the Idiomatic procedure
    reuse design pattern for ShuffleNet is located on GitHub ([http://mng.bz/oGop](http://mng.bz/oGop)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的消融研究中，作者发现，在复杂性和准确性之间的最佳权衡是在减少因子为1（无减少）的情况下，并将组分区数量设置为8。使用Idiomatic过程重用设计模式为ShuffleNet编写的完整代码可以在GitHub上找到（[http://mng.bz/oGop](http://mng.bz/oGop)）。
- en: Next, we will cover shrinking the size of the model for a memory-constrained
    device with quantization, and converting/predicting using the TensorFlow Lite
    Python package, for the deployment of a mobile model.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍如何使用量化缩小内存受限设备的模型大小，并使用TensorFlow Lite Python包进行转换/预测，以部署移动模型。
- en: 8.5 Deployment
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 部署
- en: We will wrap up this chapter by covering the basics for deploying a mobile convolutional
    model. We will look first at quantization, which reduces the parameter size and
    hence the memory footprint. Quantization happens prior to deploying the model.
    Next, we will see how to use TF Lite to execute the model on a memory-constrained
    device. In our examples, we use a Python environment as a proxy. We won’t dive
    into specifics related to Android or iOS.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过介绍部署移动卷积模型的基本知识来结束本章。我们首先将探讨量化，它减少了参数大小，从而降低了内存占用。量化在部署模型之前进行。接下来，我们将了解如何使用TF
    Lite在内存受限的设备上执行模型。在我们的示例中，我们使用Python环境作为代理。我们不会深入探讨与Android或iOS相关的具体细节。
- en: 8.5.1 Quantization
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 量化
- en: '*Quantization* is a process for reducing the number of bits that represent
    a number. For memory-constrained devices, we want to store the weights at a lower
    bit representation without significant loss of accuracy.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*量化* 是一个减少表示数字的位数的过程。对于内存受限的设备，我们希望在不会显著损失准确性的情况下，以较低的位表示存储权重。'
- en: Because neural networks are fairly resilient to small errors in calculations,
    they do not need as high of a precision for inference as for training. This provides
    the opportunity to reduce the precision of the weights in a mobile neural network.
    The conventional reduction is to replace the 32-bit floating-point weight values
    with a discrete approximation as 8-bit integer values. The primary advantage is
    that the reduction from 32 to 8 bits requires only one-quarter of the memory space
    for the model.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络对计算中的小错误具有一定的鲁棒性，因此在推理时不需要像训练时那样高的精度。这为在移动神经网络中降低权重的精度提供了机会。传统的减少方法是将32位浮点权重值替换为8位整数的离散近似。主要优势是，从32位到8位的减少只需要模型四分之一的内存空间。
- en: During inference (prediction), the weights are scaled back to their approximate
    32-bit floating-point values for the matrix operations, which are then passed
    through the activation function. Modern hardware accelerators have been designed
    to optimize this rescaling operation so that there is nominal compute overhead.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理（预测）过程中，权重被缩放回其大约32位浮点值，以便进行矩阵运算，然后通过激活函数。现代硬件加速器已被设计用于优化此缩放操作，以实现名义上的计算开销。
- en: In the conventional reduction, the 32-bit floating-point weights are divided
    into buckets (bins) across the integer range. For an 8-bit value, this would be
    256 buckets, as depicted in figure 8.30.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的缩减中，32位浮点权重被分成整数范围内的桶（桶）。对于8位值，这将会有256个桶，如图8.30所示。
- en: '![](Images/CH08_F30_Ferlitsch.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F30_Ferlitsch.png)'
- en: Figure 8.30 Quantization categorizes a floating-point range into a fixed set
    of bins represented by an integer type.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.30 量化将浮点数范围分类为一系列由整数类型表示的固定桶。
- en: To perform the quantization in this example, the floating-point range of the
    weights is first determined, which we refer to as `[rmin, rmax]`, for the minimum
    and maximum value. The range is then linearly divided by the number of buckets
    (256 in the case of 8-bit integers).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中执行量化时，首先确定权重的浮点数范围，我们将其称为`[rmin, rmax]`，即最小值和最大值。然后，该范围按桶的数量（在8位整数的情况下为256）进行线性划分。
- en: Depending on the hardware accelerator, we may additionally see an execution
    speedup from two to three times on a CPU (and TPU). Integer operations are not
    supported on a GPU.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 根据硬件加速器，我们可能还会在CPU（和TPU）上看到从两次到三次的执行速度提升。GPU不支持整数运算。
- en: For a GPU that natively supports float16 (half precision), quantization is done
    by converting the float32 values to float16\. This reduces the model’s memory
    footprint in half and typically speeds up execution by four times.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 对于原生支持float16（半精度）的GPU，量化是通过将float32值转换为float16来完成的。这将模型的内存占用减半，并且通常将执行速度提高四倍。
- en: Additionally, quantization works best when the floating-point range of the weights
    is constrained (shrunk). The current convention for mobile models is to use a
    `max_ value` of 6.0 for the ReLU for this purpose.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当权重的浮点数范围受到限制（缩小）时，量化效果最佳。对于移动模型，当前惯例是使用ReLU的`max_value`为6.0。
- en: We should be careful of quantizing very small models. Large models benefit from
    the redundancy of weights and are immune to loss of accuracy when quantized to
    8-bit integers. The SOTA mobile models have been designed to limit the amount
    of accuracy loss when quantized. If we design smaller models and quantize them,
    they may degrade significantly in accuracy.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该小心量化非常小的模型。大型模型受益于权重的冗余，并且在量化为8位整数时对精度损失具有免疫力。最先进的移动模型已被设计为在量化时限制精度损失的数量。如果我们设计较小的模型并对其进行量化，它们在精度上可能会显著下降。
- en: Next, we will cover TF Lite for executing models in memory constrained devices.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍TF Lite在内存受限设备上执行模型。
- en: 8.5.2 TF Lite conversion and prediction
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.2 TF Lite转换和预测
- en: TF Lite is an execution environment for TensorFlow models in memory-constrained
    devices. Unlike the native TensorFlow runtime environment, the TF Lite runtime
    environment is much smaller and easier to fit into memory-constrained devices.
    While optimized for this purpose, it does come with some tradeoffs. For instance,
    some TF graph ops are not supported, and some operations require additional steps.
    We won’t cover the unsupported graph ops, but we will go over the required additional
    steps.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: TF Lite是内存受限设备上TensorFlow模型的执行环境。与原生的TensorFlow运行时环境不同，TF Lite运行时环境要小得多，更容易适应内存受限设备。虽然针对此目的进行了优化，但它也带来了一些权衡。例如，一些TF图操作不受支持，一些操作需要额外的步骤。我们不会涵盖不受支持的图操作，但我们会介绍所需的额外步骤。
- en: 'The following code demonstrates using TensorFlow Lite to quantize an existing
    model, where the model is a trained TF.Keras model. The first step is to convert
    the model in SavedModel format into a TF Lite model format. This is done by instantiating
    a `TFLiteConverter` and passing it an in-memory or on-disk model in SavedModel
    format, and then invoking the `convert()` method:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了使用TensorFlow Lite对现有模型进行量化，其中模型是训练好的TF.Keras模型。第一步是将SavedModel格式的模型转换为TF
    Lite模型格式。这是通过实例化一个`TFLiteConverter`并将内存中或磁盘上的SavedModel格式模型传递给它来完成的，然后调用`convert()`方法：
- en: '[PRE15]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Creates an instance of the converter for TF.Keras (SavedModel format) model
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为TF.Keras（SavedModel格式）模型创建转换器实例
- en: ❷ Converts the model to the TF Lite format
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将模型转换为 TF Lite 格式
- en: 'The TF Lite version of the model is not a TensorFlow SavedModel format. You
    cannot directly use methods like `predict``()`. Instead, we use the TF Lite interpreter.
    You must first set up the interpreter for the TF Lite model as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的 TF Lite 版本不是 TensorFlow SavedModel 格式。您不能直接使用 `predict()` 等方法。相反，我们使用 TF
    Lite 解释器。您必须首先按照以下方式设置 TF Lite 模型的解释器：
- en: Instantiate a TF Lite interpreter for the TF Lite model.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 TF Lite 模型实例化一个 TF Lite 解释器。
- en: Instruct the interpreter to allocate input and output tensors for the model.
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示解释器为模型分配输入和输出张量。
- en: Get detail information about the model’s input and output tensors that will
    need to be known for prediction.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取模型输入和输出张量的详细信息，这些信息将需要在预测时了解。
- en: 'The following code demonstrates these steps:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了这些步骤：
- en: '[PRE16]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Instantiates an interpreter for the TF Lite model
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化 TF Lite 模型的解释器
- en: ❷ Allocates the input and output tensors for the model
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为模型分配输入和输出张量
- en: ❸ Gets input and output tensor details needed for prediction
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取预测所需的输入和输出张量详情
- en: The `input_details` and `output_details` are returned as a list; the number
    of elements corresponds to the number of input and output tensors, respectively.
    For example, a model with a single input (for example, image) and a single output
    (multiclass classifier) would have one element for both the input and output tensors.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_details` 和 `output_details` 作为列表返回；元素的数量分别对应输入和输出张量的数量。例如，具有单个输入（例如图像）和单个输出（多类分类器）的模型将分别为输入和输出张量有一个元素。'
- en: Each element contains a dictionary with corresponding details. In the case of
    an input tensor, the key `shape` returns a tuple that is the shape of the input.
    For example, if the model took as input (32, 32, 3) images (for example, CIFAR-10),
    the key would return (32, 32, 3).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元素都包含一个包含相应详细信息的字典。在输入张量的情况下，键 `shape` 返回一个元组，表示输入的形状。例如，如果模型以 (32, 32, 3)
    的图像（例如 CIFAR-10）作为输入，则键将返回 (32, 32, 3)。
- en: 'To make a single prediction, we do the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行单个预测，我们执行以下操作：
- en: Prepare the input to be a batch of size 1\. For our CIFAR-10 example, that would
    be (1, 32, 32, 3).
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备输入以形成一个大小为 1 的批次。对于我们的 CIFAR-10 示例，这将是一个 (1, 32, 32, 3)。
- en: Assign the batch to the input tensor.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将批次分配给输入张量。
- en: Invoke the interpreter to perform the prediction.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用解释器以执行预测。
- en: Get the output tensor from the model (for example, softmax outputs in a multiclass
    model).
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型获取输出张量（例如，多类模型中的 softmax 输出）。
- en: 'The following code demonstrates these steps:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了这些步骤：
- en: '[PRE17]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Converts the single input to a batch of size 1
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将单个输入转换为大小为 1 的批次
- en: ❷ Assigns the batch to the input tensor
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将批次分配给输入张量
- en: ❸ Executes (invoke) the interpreter to perform the prediction
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行（调用）解释器以执行预测
- en: ❹ Gets the output from the model
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从模型获取输出
- en: ❺ Multiclass example, determine the label predicted from the softmax output
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 多类示例，确定从 softmax 输出预测的标签
- en: 'For batch prediction, we need to modify (resize) the interpreter’s input and
    output tensors for the batch size. The following code resizes the batch size for
    the interpreter to 128 for a (32, 32, 3) input (CIFAR-10), prior to allocating
    the tensors:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批量预测，我们需要修改（调整大小）解释器的输入和输出张量以适应批次大小。以下代码在分配张量之前将解释器的批次大小调整为 128，对于 (32, 32,
    3) 的输入（CIFAR-10）：
- en: '[PRE18]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Instantiates an interpreter for the TF Lite model
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化 TF Lite 模型的解释器
- en: ❷ Resizes the input and output tensors for a batch of 128
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对 128 批次的输入和输出张量进行调整大小
- en: ❸ Allocates the input and output tensors for the model
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为模型分配输入和输出张量
- en: Summary
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Refactoring using depthwise convolutions and network thinning in MobileNet v1
    demonstrated the ability to run models on memory-constrained devices with AlexNet
    accuracy.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度卷积和网络细化在 MobileNet v1 中的重构展示了在内存受限设备上运行模型并达到 AlexNet 准确率的能力。
- en: Redesigning the residual block in MobileNet v2 to an inverted residual block
    further reduced the memory footprint and increased accuracy.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 MobileNet v2 中的残差块重新设计为倒残差块进一步减少了内存占用并提高了准确率。
- en: SqueezeNet introduced the concept of computationally efficient macro-architecture
    search using metaparameters to configure group and block attributes.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SqueezeNet 引入了使用元参数配置组和块属性的计算效率宏架构搜索的概念。
- en: Refactoring and channel shuffle in ShuffleNet v1 demonstrated the ability to
    run models on extremely constrained memory devices, such as microcontrollers.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ShuffleNet v1 中的重构和通道洗牌展示了在极端内存受限的设备上运行模型的能力，例如微控制器。
- en: Quantization techniques provided a means to reduce memory footprint by 75% with
    little or no loss of accuracy for inference.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化技术提供了一种方法，通过减少内存占用，可以将内存占用降低 75%，同时几乎不会损失推理精度。
- en: Use TF Lite to convert from SavedModel format to quantized TF Lite format and
    do predictions for deployment to a memory-constrained device.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TF Lite 将 SavedModel 格式转换为量化 TF Lite 格式，并在内存受限的设备上进行预测部署。
