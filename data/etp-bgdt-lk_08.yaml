- en: Chapter 8\. Cataloging the Data Lake
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第八章：数据湖的分类
- en: Data lakes tend to suffer from a number of traits that make them difficult,
    if not impossible, to navigate. They contain a massive number of data sets. Field
    names are often cryptic, and some types of data sets—such as delimited files and
    unstructured data collected from online comments—may lack header lines altogether.
    Even well-labeled data sets may have inconsistent names and different naming conventions.
    It is virtually impossible to guess what particular attributes may be called in
    different files, and thus impossible to find all instances of those attributes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖往往具有一些特征，使得它们难以，如果不是不可能的话，进行导航。它们包含大量的数据集。字段名称通常是神秘的，并且某些类型的数据集（例如从在线评论收集的分隔文件和非结构化数据）可能根本没有标题行。即使是标记良好的数据集，其名称也可能不一致，并且具有不同的命名约定。几乎不可能猜测不同文件中可能称为何种属性的名称，因此也就不可能找到这些属性的所有实例。
- en: As a result, data needs either to be documented as new data sets are ingested
    or created in the lake or to go through extensive manual examination, neither
    alternative being scalable or manageable for the typical size and variety found
    in big data systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据需要在湖中摄入或创建新数据集时进行记录，或者经过广泛的手动检查，这两种选择对于大数据系统中典型的规模和多样性都不可扩展或可管理。
- en: Data catalogs solve the problem by tagging fields and data sets with consistent
    business terms and providing a shopping-type interface that allows the users to
    find data sets by describing what they are looking for using the business terms
    that they are used to, and to understand the data in those data sets through tags
    and descriptions that use business terms. In this chapter we’ll explore some of
    the many uses of data catalogs, and take a quick look at some of the data cataloging
    products on the market today.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据目录通过使用一致的业务术语对字段和数据集进行标记，并提供一种类似购物的界面来解决这个问题，允许用户通过描述他们所寻找的内容（使用他们习惯的业务术语）来查找数据集，并通过使用业务术语的标签和描述来理解这些数据集中的数据。在本章中，我们将探讨数据目录的许多用途，并快速查看今天市场上的一些数据目录产品。
- en: Organizing the Data
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据组织
- en: 'While the directory structure and naming conventions described in [Chapter 7](ch07.xhtml#architecting_the_data_lake)
    can help analysts navigate a big data cluster, they are not sufficient. Here’s
    what they lack:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 [第 7 章](ch07.xhtml#architecting_the_data_lake) 中描述的目录结构和命名约定可以帮助分析师导航大数据集群，但这还不够。以下是它们的不足之处：
- en: There is no search capability. Analysts have to browse to the right directory,
    which works when they know what they want but is not practical when they’re just
    exploring the potentially thousands of sources/folders.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有搜索功能。分析师必须浏览到正确的目录，这在他们知道自己想要什么时候是有效的，但当他们仅仅在探索可能的成千上万的来源/文件夹时，这是不切实际的。
- en: Useful Hadoop utilities like Hue allow users to peek at an initial small segment
    of a file, but this may not be enough to understand what’s inside large files.
    To decide whether a file is appropriate for their project, the analyst may need
    a better idea of what it contains. For example, is there any New York data? How
    many tweets are there? What are the order amounts? If the analyst is looking for
    customer demographics such as age, just previewing a few lines of a file and seeing
    some age data doesn’t tell them whether this data is available for a substantial
    enough number of customers.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 实用工具像 Hue 允许用户查看文件的初始小片段，但这可能不足以理解大文件内部的内容。为了决定一个文件是否适合他们的项目，分析师可能需要更清楚地了解其包含的内容。例如，是否有纽约的数据？有多少条推文？订单金额是多少？如果分析师正在寻找诸如年龄之类的客户人口统计数据，仅仅预览文件的几行并看到一些年龄数据，并不能告诉他们这些数据是否适用于足够数量的客户。
- en: The analyst also needs to be able to tell where a file came from. Not all data
    can be trusted. Some of it may come from failed data science experiments and some
    from systems that are notoriously inconsistent, whereas other data is from well-curated
    and trusted sources. Is the file in the landing zone, gold zone, or work zone?
    Depending on the analyst’s needs, either raw or cleansed data may suffice. If
    the data is in someone’s work folder, the analyst may have to carefully study
    the file description or talk to the file or project owner. And it’s just as critical
    to understand what has been done to the data. Some attributes may already have
    been curated and treated in the way the analyst needs, whereas other attributes
    may need different treatment and need to be obtained from the raw files.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析师还需要能够确定文件的来源。并非所有数据都可以信任。有些可能来自于失败的数据科学实验，有些可能来自于系统存在显著不一致性的数据，而另一些数据则来自于经过精心策划和信任的来源。文件位于着陆区、黄金区还是工作区？根据分析师的需求，原始数据或经过清洗的数据可能都可以使用。如果数据存储在某人的工作文件夹中，分析师可能需要仔细研究文件描述或与文件或项目所有者交流。同样重要的是了解数据的处理过程。有些属性可能已经经过策划并按照分析师的需求进行了处理，而其他属性可能需要不同的处理方式，并且需要从原始文件中获取。
- en: To address these shortcomings, just as every library is organized and cataloged,
    enterprises need to organize and catalog their data sets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些缺点的方法与每个图书馆的组织和目录化方式一样，企业需要组织和目录化它们的数据集。
- en: Finding the appropriate input data for analytics has been an unsolved problem
    for as long as I have been working in this space (more than 30 years). In the
    remainder of this section we’ll look at different ways that data can be annotated
    and described with metadata to enhance findability, and we’ll see how glossaries,
    taxonomies, and ontologies can be used to describe, organize, and search data
    sets.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，为分析而寻找合适的输入数据一直是一个未解决的问题（我在这个领域已经工作了超过30年）。在本节的其余部分，我们将探讨使用元数据注释和描述数据的不同方法，以增强可查性，并看看词汇表、分类法和本体论如何用于描述、组织和搜索数据集。
- en: Then, in the next section, we’ll consider how automation can help. Given the
    sheer number of files in a data lake, and the fact that often there will be many
    years of history of people ignoring or circumventing processes and procedures
    designed to document and track data, the cataloging process has to be automated
    as much as possible, and whatever tool is used has to make it extremely easy for
    analysts to take notes and tag fields and data sets with meaningful business terms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在下一节中，我们将考虑自动化如何帮助。考虑到数据湖中文件的数量庞大，并且通常会存在许多年来人们无视或绕过旨在记录和跟踪数据的流程和程序的情况，目录化过程必须尽可能自动化，并且无论使用何种工具，都必须使分析师能够轻松地记录笔记并使用有意义的业务术语标记字段和数据集。
- en: Technical Metadata
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术元数据
- en: To help describe data sets, we turn to *metadata*, or data about data. For example,
    in a relational database, a table definition specifies the metadata, including
    the table name, column names, descriptions, data types, lengths, and so forth.
    The actual values in the table rows, then, are considered data. Unfortunately,
    the line between data and metadata is blurry. Consider the following example ([Table 8-1](#the_sales_table)),
    a table called `Sales` that contains quarterly and monthly sales data (in millions)
    for different years, for products identified by ID.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助描述数据集，我们转向*元数据*或关于数据的数据。例如，在关系数据库中，表定义指定了元数据，包括表名、列名、描述、数据类型、长度等等。然后，表行中的实际值被视为数据。不幸的是，数据与元数据之间的界限很模糊。考虑以下示例（[表 8-1](#the_sales_table)），一个名为`Sales`的表，其中包含不同年份产品ID的季度和月度销售数据（以百万计）。
- en: Table 8-1\. *The Sales table*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-1\. *销售表*
- en: '| **ProdID** | **Year** | **Q1** | **Q2** | **Q3** | **Q4** | **Jan** | **Feb**
    | **Mar** | **…** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **ProdID** | **Year** | **Q1** | **Q2** | **Q3** | **Q4** | **Jan** | **Feb**
    | **Mar** | **…** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| X11899 | 2010 | 5 | 4.5 | 6 | 9 | 1.1 | 1.9 | 2.2 | … |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | 5 | 4.5 | 6 | 9 | 1.1 | 1.9 | 2.2 | … |'
- en: '| F22122 | 2010 | 1.2 | 3.5 | 11 | 1.3 | .2 | .3 | .6 | … |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| F22122 | 2010 | 1.2 | 3.5 | 11 | 1.3 | .2 | .3 | .6 | … |'
- en: '| X11899 | 2011 | 6 | 6 | 6.5 | 7 | 4.5 | 2 | .5 | … |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2011 | 6 | 6 | 6.5 | 7 | 4.5 | 2 | .5 | … |'
- en: '| … | … | … | … | … | … | … | … | … | … |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … | … | … | … | … | … |'
- en: In this example, the field names `ProdID`, `Year`, `Q1`, `Q2`, `Q3`, `Q4`, `Jan`,
    `Feb`, and so on are all metadata, while the actual product IDs (`X11899`, `F22122`),
    years (`2010`, `2011`), and sales amounts are data. If an analyst is looking for
    quarterly sales for products, by looking at the metadata, they can tell that they
    should be able to find it in this table. Similarly, if they are looking for monthly
    sales, they know they will be able to find that information in the table as well.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，字段名 `ProdID`、`Year`、`Q1`、`Q2`、`Q3`、`Q4`、`Jan`、`Feb` 等都是元数据，而实际的产品ID（`X11899`、`F22122`）、年份（`2010`、`2011`）和销售金额是数据。如果分析师正在寻找产品的季度销售数据，通过查看元数据，他们可以知道应该能在这个表中找到它。同样，如果他们正在寻找月度销售数据，他们也知道可以在表中找到相关信息。
- en: However, we could instead design the same table as shown next ([Table 8-2](#sales_table_with_more_obscure_metadata)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们可以设计与下文显示的相同的表格（[表 8-2](#sales_table_with_more_obscure_metadata)）。
- en: Table 8-2\. *Sales table with more obscure metadata*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-2\. *具有更复杂元数据的销售表*
- en: '| **ProductID** | **Year** | **Period** | **SalesAmount** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **ProductID** | **Year** | **Period** | **SalesAmount** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| X11899 | 2010 | Q1 | 5 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | Q1 | 5 |'
- en: '| X11899 | 2010 | Q2 | 4.5 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | Q2 | 4.5 |'
- en: '| F22122 | 2010 | Q3 | 11 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| F22122 | 2010 | Q3 | 11 |'
- en: '| X11899 | 2011 | Q1 | 6 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2011 | Q1 | 6 |'
- en: '| X11899 | 2010 | Jan | 1.1 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | 一月 | 1.1 |'
- en: '| X11899 | 2010 | Feb | 1.9 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | 二月 | 1.9 |'
- en: '| X11899 | 2010 | Mar | 2.2 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | 三月 | 2.2 |'
- en: '| … | … | … | … |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … |'
- en: Instead of having a separate column for each period, we now have a row for each
    period. The `Period` column specifies what period we are looking at—either a quarter
    (`Q1..Q4`) or a month (`Jan..Dec`). Even though the two tables contain exactly
    the same information and can be used interchangeably, in this example quarter
    number and month are data, not metadata.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是为每个期间单独列出一个列，我们现在为每个期间列出一行。`Period` 列指定我们正在查看的期间是季度（`Q1..Q4`）还是月份（`Jan..Dec`）。尽管两个表包含完全相同的信息且可以互换使用，但在这个例子中，季度编号和月份是数据，而不是元数据。
- en: Of course, real life is much messier than this simplified example. Just as field
    names can be cryptic or misleading, so can data. For example, in real life, the
    second table is more likely to use codes and might look something like [Table 8-3](#sales_table_with_more_obscure_data).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，实际生活比这个简化的例子要复杂得多。就像字段名称可以晦涩或误导一样，数据也可以如此。例如，在实际生活中，第二个表更可能使用代码，并且可能看起来像[表 8-3](#sales_table_with_more_obscure_data)。
- en: Table 8-3\. *Sales table with more obscure data*
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-3\. *具有更复杂数据的销售表*
- en: '| **ProductID** | **Year** | **Period_Type** | **Period** | **SalesAmount**
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **ProductID** | **Year** | **Period_Type** | **Period** | **SalesAmount**
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| X11899 | 2010 | Q | 1 | 5 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | Q | 1 | 5 |'
- en: '| X11899 | 2010 | Q | 2 | 4.5 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | Q | 2 | 4.5 |'
- en: '| F22122 | 2010 | Q | 3 | 11 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| F22122 | 2010 | Q | 3 | 11 |'
- en: '| X11899 | 2011 | Q | 1 | 6 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2011 | Q | 1 | 6 |'
- en: '| X11899 | 2010 | M | 1 | 1.1 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | M | 1 | 1.1 |'
- en: '| X11899 | 2010 | M | 2 | 1.9 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | M | 2 | 1.9 |'
- en: '| X11899 | 2010 | M | 3 | 2.2 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| X11899 | 2010 | M | 3 | 2.2 |'
- en: '| … | … | … | … | … |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … |'
- en: Here, a `Period_Type` of `M` means “month” and an associated `Period` value
    of `2` means February, but a `Period_Type` of `Q` designates “quarter” and an
    associated `Period` value of `2` means the second quarter of the year. Profiling
    this table would not yield month names, although a clever analyst might be able
    to infer from the metadata (seeing that `Period_Type` is either `M` or `Q` and
    that there are three times more `M`s than `Q`s) that they are looking at monthly
    and quarterly sales data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`Period_Type`为 `M` 表示“月”，相关的 `Period` 值为 `2` 表示二月，而 `Period_Type`为 `Q` 表示“季度”，相关的
    `Period` 值为 `2` 表示年度的第二季度。对该表进行分析不会显示月份名称，尽管一位聪明的分析师可能能从元数据中推断（看到 `Period_Type`
    是 `M` 或 `Q`，以及 `M` 的数量是 `Q` 的三倍）他们正在查看的是月度和季度销售数据。
- en: As this example illustrates, there is no hard boundary between data and metadata,
    and depending on the schema design, the same information can be captured as either.
    Relying solely on metadata provides no way to tell what kinds of periods the latter
    table contains, and analysts looking for monthly sales data won’t know whether
    this table will help them until they look at the data and see that periods include
    months.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所说明的，数据和元数据之间没有明确的界限，根据架构设计，相同的信息可以作为数据或元数据进行捕捉。仅依赖元数据无法告知后者表包含哪些种类的期间，寻找月度销售数据的分析师不会知道这个表是否会对他们有帮助，直到查看数据并看到期间包含月份为止。
- en: Data profiling
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据分析
- en: 'Since studying each table to understand what it contains significantly slows
    down the process of finding the right table to use, *profiling* is often used
    to bridge the gap between data and metadata. For example, if the analyst knew
    without having to look at the data that the `Period` field contained the values
    `Q1..Q4` and `Jan..Dec`, they would immediately recognize that they would be able
    to find quarterly and monthly sales data in that table. Profiling analyzes the
    data in each column to help round out our understanding of the data as well as
    its quality, as we covered in [Chapter 6](ch06.xhtml#optimizing_for_self-service).
    Among other things (such as most frequent values and their counts), it calculates:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于研究每个表格以理解其包含的内容显著减慢了找到正确表格的过程，*分析*常被用来弥合数据与元数据之间的差距。例如，如果分析人员无需查看数据就能知道`Period`字段包含`Q1..Q4`和`Jan..Dec`的值，他们会立即意识到该表格中可能包含季度和月度销售数据。分析会分析每一列的数据，帮助我们更全面地理解数据及其质量，正如我们在[第六章](ch06.xhtml#optimizing_for_self-service)中讨论的那样。除了最频繁的值及其计数外，它还计算：
- en: Cardinality
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基数
- en: How many unique values are in each field. For example, if two tables are equivalent,
    the cardinality of the `ProductID` and `Year` columns should be the same in both.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个字段中有多少个唯一值。例如，如果两个表格是等价的，则`ProductID`和`Year`列的基数应该在两个表格中相同。
- en: Selectivity
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性
- en: How unique the values in each field are. This is calculated by dividing the
    cardinality of a field by the number of rows. Selectivity of 1 or 100% means that
    each value in the column is unique.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个字段值的唯一性。这是通过将字段的基数除以行数来计算的。选择性为1或100%意味着该列中的每个值都是唯一的。
- en: Density
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 密度
- en: How many `NULL`s (or missing values) are in each column. Density of 1 or 100%
    means that there are no `NULL`s, whereas density of 0% means that the field only
    contains `NULL`s (i.e., is empty).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每列中有多少个`NULL`（或缺失值）。密度为1或100%意味着没有`NULL`，而0%的密度意味着该字段只包含`NULL`（即为空）。
- en: Range, mean, and standard deviation
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 范围、均值和标准差
- en: For numeric fields, the smallest and largest values are calculated as well as
    the mean and sigma or standard deviation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值字段，计算最小值和最大值，以及均值和标准差。
- en: Format frequencies
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 格式频率
- en: Some data has very distinctive formats—for example, US zip codes are either
    five digits, nine digits, or five digits followed by a dash and four more digits.
    Formats can be very helpful in identifying the type of data contained in a field.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据具有非常独特的格式——例如，美国邮政编码要么是五位数字，要么是九位数字，或者是五位数字后跟一个短划线和四位数字。格式可以帮助识别字段中所包含数据的类型。
- en: The statistical information captured by profiling, together with other metadata
    such as the names of fields, tables, and files, is called *technical metadata*.
    While this helps us to understand the nature of the data, it does not address
    the problem of findability. In fact, to make matters worse, technical metadata
    is often abbreviated or obscure—for example, just as the columns in [Table 8-1](#the_sales_table)
    were called `Q1` and `Q2` instead of `First_Quarter` and `Second_Quarter`, the
    `Year` column might just have been called `Y`; analysts would then have a hard
    time searching for it because `Y` might stand for `Yield`, `Yes`, `Year`, or any
    number of other things.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析获取的统计信息，以及其他元数据（如字段、表格和文件的名称），被称为*技术元数据*。虽然这有助于我们理解数据的性质，但并不能解决可发现性的问题。事实上，更加糟糕的是，技术元数据通常是简称或者晦涩难懂——例如，就像[表格 8-1](#the_sales_table)中的列被称为`Q1`和`Q2`，而不是`First_Quarter`和`Second_Quarter`，`Year`列可能仅仅被称为`Y`；分析人员因此很难搜索到它，因为`Y`可能代表`Yield`、`Yes`、`Year`或者其他任何东西。
- en: Profiling hierarchical data
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析层次数据
- en: Profiling information is very intuitive for tabular data—the stats are for each
    column, aggregated across all the rows—but it gets trickier for hierarchical data
    such as JSON or XML files.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据时，表格数据的分析结果非常直观——统计数据针对每一列进行汇总，涵盖所有行，但是对于像JSON或XML文件这样的层次数据则更加复杂。
- en: While the format may be different, conceptually JSON files represent the same
    data as tabular files. For example, an order can be represented as a set of tables
    in a relational database or a set of tabular files related to each other by what
    are called *primary key–foreign key relationships*. In [Figure 8-1](#entity_relationship_diagram),
    four tables are used to store information about orders, customers, and products.
    Primary key–foreign key relationships are captured with lines between primary
    key fields and foreign key fields with one-to-many relationships illustrated with
    1:*N*, meaning that, for example, for each `CustomerID` in the `Customers` table,
    there may be many orders with the same `CustomerID` in the `Orders` table.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管格式可能不同，但在概念上，JSON 文件与表格文件表示相同的数据。例如，订单可以被表示为关系数据库中一组表，或者一组通过所谓的*主键-外键关系*相互关联的表格文件。在[图 8-1](#entity_relationship_diagram)中，有四个表用于存储订单、客户和产品的信息。主键-外键关系用线条连接主键字段和外键字段表示，一对多的关系用
    1:*N* 表示，这意味着例如在`Customers`表中的每个`CustomerID`可能在`Orders`表中有多个相同`CustomerID`的订单。
- en: '![Entity relationship diagram](Images/ebdl_0801.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![实体关系图](Images/ebdl_0801.png)'
- en: Figure 8-1\. Entity relationship diagram
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 实体关系图
- en: 'The same information may be presented in JSON format, with the hierarchy representing
    the relationships expressed by primary and foreign keys in relational systems.
    Instead of four different tables, a single JSON file will capture all the attributes
    and relationships. The following snippet captures all the information for an order.
    Note that there are no `CustomerID`s and `ProductID`s required to relate orders,
    customers, and products. Customer information is embedded in the `Order` records,
    product information is embedded in `OrderLine`, and so forth:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的信息可能以 JSON 格式呈现，其中层次结构表示关系数据库中由主键和外键表达的关系。与四个不同的表不同，单个 JSON 文件将捕获所有属性和关系。以下片段捕获了订单的所有信息。请注意，不需要`CustomerID`和`ProductID`来关联订单、客户和产品。客户信息嵌入在`Order`记录中，产品信息嵌入在`OrderLine`中，依此类推：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since the information in both cases is really the same, a simple process called
    *shredding* is often used to extract fields from hierarchical files. For example,
    a customer name may be extracted using its full hierarchical name, `Order.Customer.Name`.
    This name is an XPATH expression (XPATH is a query language to access specific
    parts of an XML document); shredding basically creates a unique field for each
    unique XPATH expression in a hierarchical file.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两种情况中的信息实际上是相同的，通常会使用一种称为*剪切*的简单过程来从分层文件中提取字段。例如，可以使用其完整分层名称`Order.Customer.Name`来提取客户名称。此名称是一个
    XPATH 表达式（XPATH 是用于访问 XML 文档特定部分的查询语言）；剪切基本上为分层文件中的每个唯一 XPATH 表达式创建一个唯一字段。
- en: One problem with shredding is that it is what’s called “lossy”—it loses information
    when transforming the data into tabular format. For example, if there happen to
    be two customers for one order and three line items, there is no simple way of
    shredding this data to preserve such information. Do we attribute all three line
    items to the first customer, or to each customer? Do we assign some to one customer
    and some to the other? There are no simple or universally correct answers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 剪切的一个问题是所谓的“有损性”——在将数据转换为表格格式时会丢失信息。例如，如果一个订单有两个客户和三个订单行项目，没有简单的方法可以保留这些信息。我们是将三个订单行项目归属于第一个客户，还是每个客户都归属于各自的订单行项目？我们是将一些归属于一个客户，一些归属于另一个客户吗？这里没有简单或通用的正确答案。
- en: Most profiling tools, such as the ones from Informatica and IBM, require hierarchical
    files to be explicitly shredded or converted to a tabular format before they can
    be profiled, while more modern tools like Trifacta, Paxata, or Waterline Data
    that were developed for non-relational big data environments either profile hierarchical
    data natively in a non-lossy way or at least shred hierarchical data automatically.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分析工具，如Informatica和IBM的工具，要求分层文件在进行分析之前必须明确地被剪切或转换为表格格式，而像Trifacta、Paxata或Waterline
    Data这样针对非关系型大数据环境开发的现代工具可以原生地非损失地分析分层数据，或者至少会自动地对分层数据进行剪切。
- en: Business Metadata
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 业务元数据
- en: To help analysts find data, we turn to *business metadata*, or business-level
    descriptions of the data. These can come in many forms.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助分析师找到数据，我们转向*业务元数据*，即关于数据的业务级描述。这些可以采用多种形式。
- en: Glossaries, taxonomies, and ontologies
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 术语表、分类学和本体论
- en: Business metadata is often captured in glossaries, taxonomies, and ontologies.
    A business glossary is a highly formalized (usually hierarchical) list of business
    terms and their definitions. Some business glossaries are taxonomies, some are
    ontologies, and some are just grouping constructs without much semantic rigor.
    There are many spirited debates about what differentiates a taxonomy and an ontology.
    My goal here is to give you a flavor of the two, not to argue one side or another.
    With that in mind, you can think of a taxonomy as a hierarchy of objects where
    a child is a subclass of the parent. This is also known as an *is-a* (pronounced
    “izah”) relationship. [Figure 8-2](#a_biological_taxonomy) illustrates the biological
    taxonomy that most of us studied in biology class.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 业务元数据通常包含在词汇表、分类法和本体论中。业务词汇表是业务术语及其定义的高度形式化（通常是分层次）列表。一些业务词汇表是分类法，一些是本体论，还有一些只是没有太多语义严谨的分组结构。关于分类法和本体论的区别，有许多激烈的讨论。我的目标在于给你介绍这两者的风格，而不是支持任何一方。有了这个理念，你可以将分类法视为对象的层次结构，其中子类是父类的子类。这也被称为*是-一个*（读作“izah”）关系。[图 8-2](#a_biological_taxonomy)
    展示了我们大多数人在生物课上学习的生物分类法。
- en: '![A biological taxonomy](Images/ebdl_0802.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![生物分类法](Images/ebdl_0802.png)'
- en: Figure 8-2\. A biological taxonomy
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 生物分类学
- en: An ontology is generally more elaborate than a taxonomy and supports arbitrary
    relationships between objects. For example, in addition to is-a relationships,
    it includes *has-a* relationships between objects and attributes (as in, an automobile
    has an engine). As another example of a relationship, a driver *drives* an automobile.
    [Figure 8-3](#part_of_a_vehicle_ontology) illustrates a snippet of a possible
    automobile-related ontology, where an automobile has wheels and an engine and
    is a subclass of a vehicle.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本体论通常比分类法更为复杂，并支持对象之间的任意关系。例如，除了*是-一个*关系之外，还包括对象和属性之间的*有-一个*关系（例如，汽车有引擎）。作为另一个关系的例子，驾驶员*驾驶*汽车。[图 8-3](#part_of_a_vehicle_ontology)
    展示了可能与汽车相关的本体论片段，其中汽车有轮子和引擎，并且是车辆的子类。
- en: '![Part of a vehicle ontology](Images/ebdl_0803.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![车辆本体论的一部分](Images/ebdl_0803.png)'
- en: Figure 8-3\. Part of a vehicle ontology
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 车辆本体论的一部分
- en: Industry ontologies
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 行业本体论
- en: A number of standard ontologies have been developed for different industries.
    [ACORD](https://www.acord.org/) is an insurance industry organization with over
    8,000 organizations as members that helps insurance companies exchange data in
    a standard way. To describe the data being exchanged, ACORD developed a business
    glossary that describes each element in its forms, which cover many aspects of
    the insurance business. Another example is the [Financial Industry Business Ontology
    (FIBO)](http://bit.ly/2UJ9nF5), developed by two well-respected industry organizations,
    the Object Management Group (OMG) and Enterprise Data Council (EDM).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 已为不同行业开发了许多标准本体论。[ACORD](https://www.acord.org/) 是一个保险行业组织，拥有超过 8,000 个成员组织，帮助保险公司以标准方式交换数据。为了描述正在交换的数据，ACORD
    开发了一个业务词汇表，描述了其表单中的每个元素，涵盖了保险业务的许多方面。另一个例子是由两个受尊敬的行业组织——对象管理组织（OMG）和企业数据委员会（EDM）开发的
    [金融业务本体论（FIBO）](http://bit.ly/2UJ9nF5)。
- en: Companies can also develop their own standards. When I was working at IBM, we
    developed a number of industry models that encapsulated both an industry-specific
    ontology and the corresponding analytical data model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 公司也可以制定自己的标准。我在 IBM 工作时，我们开发了许多行业模型，其中包括特定行业本体论和相应的分析数据模型。
- en: Folksonomies
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 民间分类法
- en: The challenge with industry- or even company-standard ontologies is that, while
    they are great for contextual search, they are often very complex, with tens of
    thousands of terms, and keeping track of all the elements is difficult. Furthermore,
    adopting a standard ontology requires heavy training for the business teams on
    using a standard terminology.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 行业标准本体论的挑战在于，虽然它们非常适合上下文搜索，但通常非常复杂，有成千上万个术语，难以跟踪所有元素。此外，采用标准本体论需要对业务团队进行大量的培训，以使用标准术语。
- en: '*Folksonomies* are much less rigorous constructs that attempt to represent
    how employees think of their data. Instead of training analysts to use strict
    definitions, folksonomies collect current terms, organize them into coherent hierarchies,
    and use them as business metadata.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*民间分类法* 是一种非常不严格的结构，试图代表员工对其数据的看法。民间分类法不会培训分析师使用严格的定义，而是收集当前术语，将它们组织成连贯的层次结构，并将其用作业务元数据。'
- en: Another challenge is that different groups may legitimately have different names
    for the same thing. For example, marketing might refer to a field as a *prospect
    name*, sales as an *opportunity name*, and support as a *customer name*. Some
    systems may represent these as three different data sets dedicated to prospects,
    opportunities, and customers, respectively, while others may combine them in a
    single data set with a flag indicating the kind of contact this is.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是不同的群体可能对相同的事物有不同的名称。例如，市场营销可能将一个字段称为*prospect name*，销售可能称之为*opportunity
    name*，而支持可能称之为*customer name*。一些系统可能将这些分别表示为专门用于前景、机会和客户的三个不同数据集，而其他系统则可能将它们合并在一个单一的数据集中，并通过标志指示这是何种类型的联系。
- en: To avoid confusion, different groups may use different folksonomies to search
    for the same data using the terms they are accustomed to. At Waterline Data, we
    chose to support this by creating different domains of tags or terms, dedicating
    some domains to certain groups and sharing other domains among multiple groups.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免混淆，不同的群体可以使用不同的民间分类法来使用他们习惯的术语搜索相同的数据。在Waterline Data，我们选择通过创建不同领域的标签或术语来支持这一点，将一些领域专门用于特定群体，并在多个群体之间共享其他领域。
- en: Tagging
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记
- en: Once we have a glossary, taxonomy, folksonomy, or ontology, in order to use
    it to find data sets, we have to assign the appropriate terms and concepts to
    those data sets. This process, known as *tagging*, consists of associating business
    terms with the fields or data sets that contain the data represented by those
    terms. For example, the `Period` field from our earlier example on technical metadata
    might be tagged with business terms `Number_of_Quarter` and `Month` that reflect
    its contents. Analysts could then find it by searching for “month,” “quarter,”
    or “quarter number.” This tagging process is critical to building a catalog.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了词汇表、分类法、民间分类法或本体论，为了使用它们来查找数据集，我们必须为这些数据集分配适当的术语和概念。这个过程被称为*标记*，它包括将业务术语与包含这些术语表示的字段或数据集相关联。例如，我们早期在技术元数据的示例中的`Period`字段可能会被标记为`Number_of_Quarter`和`Month`这两个反映其内容的业务术语。分析师们可以通过搜索“month”、“quarter”或“quarter
    number”来找到它。这个标记过程对于构建目录至关重要。
- en: To be able to tag a data set, however, the analyst or data steward must understand
    it. Since there isn’t a single person or even a team in any large enterprise that
    knows and understands every data set, this job must be crowdsourced among the
    enterprise’s many data stewards, data analysts, and other subject matter experts.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要能够标记数据集，分析师或数据监护人必须了解它。由于在任何大型企业中没有一个人甚至一个团队知道并理解每一个数据集，这项工作必须在企业众多的数据监护人、数据分析师和其他主题专家之间进行众包。
- en: Many companies, such as Google, Facebook, and LinkedIn, have catalogs where
    data stewards and analysts can manually tag data sets. There are also products
    from companies such as Alation, Informatica, and Waterline Data that support this
    kind of tagging and also allow the users of data to rate data sets, add comments,
    and more.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司，如Google、Facebook和LinkedIn，都有目录，数据监护人和分析师可以手动标记数据集。还有来自公司如Alation、Informatica和Waterline
    Data的产品支持这种标记，并允许数据的使用者评分数据集、添加评论等。
- en: We explored the idea of crowdsourcing tribal knowledge in [Chapter 6](ch06.xhtml#optimizing_for_self-service),
    and we’ll look at some of the products that are available to help with cataloging
    later in this chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](ch06.xhtml#optimizing_for_self-service)中探讨了通过众包部落知识的想法，并将在本章后面看一些可用于编目的产品。
- en: Automated Cataloging
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动编目
- en: Although manual tagging and crowdsourcing are necessary, usually these processes
    are not sufficient and are much too time-consuming. Enterprises might have millions
    of data sets with hundreds of millions of fields, and even if each field could
    be tagged in a matter of minutes (in reality, sometimes this takes hours of investigation
    and discussion), we are still talking several hundred million minutes, or thousands
    of person years, of work! Clearly, this is not practical. From my discussions
    with teams at Google, LinkedIn, and other organizations, I have learned that when
    manual processes are relied upon only the most popular data sets end up being
    tagged, leaving a huge number “dark.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管手动打标签和众包是必要的，但通常这些过程并不足够，并且耗时太长。企业可能拥有数百万个数据集，其中包含数亿个字段，即使每个字段在几分钟内就能打标签（实际上，有时这需要几小时的调查和讨论），我们仍然需要数亿分钟，或数千人年的工作！显然，这是不切实际的。通过与Google、LinkedIn和其他组织的团队讨论，我了解到，仅依赖手动流程最终只会给最受欢迎的数据集打标签，导致大量数据集“黑暗”。
- en: As mentioned in [Chapter 6](ch06.xhtml#optimizing_for_self-service), the answer
    to this problem is automation. New tools leverage AI and machine learning to enable
    identification and automatic tagging and annotation of elements in dark data sets
    (based on tags provided elsewhere by SMEs and analysts), so that analysts can
    find and use these data sets. Waterline Data’s Smart Data Catalog and Alation
    are probably the best examples of this approach. Alation tries to infer the meaning
    of fields from field names and to automatically interpret various field name abbreviations,
    while Waterline Data automatically tags fields based on field names (if available),
    field content, and field context, so it attempts to tag even the files that lack
    headers (field names).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第6章](ch06.xhtml#optimizing_for_self-service)所述，这个问题的解决方案是自动化。新工具利用AI和机器学习来识别和自动标记和注释暗数据集中的元素（基于SME和分析师提供的标签），以便分析师可以找到并使用这些数据集。Waterline
    Data的智能数据目录和Alation可能是这种方法的最佳例子。Alation试图从字段名称中推断字段的含义，并自动解释各种字段名缩写，而Waterline
    Data则根据字段名称（如果有的话）、字段内容和字段上下文自动为字段打标签，因此它尝试为甚至缺少标题（字段名称）的文件打标签。
- en: We will use Waterline Data as an example that illustrates automated cataloging.
    The tool crawls through Hadoop clusters and relational databases and fingerprints
    every field (a fingerprint is a collection of the field’s properties, including
    its name, content, and profile). It then lets analysts tag the fields while they
    are working with different files and tables. You can think of this like creating
    “wanted” posters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以Waterline Data为例，说明自动目录编制。该工具可以在Hadoop集群和关系型数据库中进行爬取，并对每个字段进行指纹识别（指纹是字段属性的集合，包括名称、内容和特征）。然后在分析人员处理不同文件和表格时，允许他们为字段打标签。你可以将其想象成创建“通缉”海报。
- en: Waterline Data’s AI-driven classification engine, called Aristotle, then uses
    these fingerprints as well as field context to automatically assign tags to the
    untagged fields. Context is determined by the other tags in the same data set.
    For example, a numeric field with three-digit numbers ranging from 000 to 999
    is very likely to be a credit card verification code if it is found next to a
    credit card number, but a field with exactly the same data is very unlikely to
    be a credit card verification code if found in a table where all the other tags
    refer to medical procedure properties.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Waterline Data的AI驱动分类引擎称为亚里士多德，然后利用这些指纹以及字段上下文自动为未打标签的字段分配标签。上下文是由同一数据集中的其他标签决定的。例如，如果一个数字字段包含从000到999的三位数，如果它旁边有一个信用卡号码，它很可能是信用卡验证代码；但如果它位于一个所有其他标签都涉及医疗程序属性的表中，则具有完全相同数据的字段很不可能是信用卡验证代码。
- en: Finally, the analysts can accept or reject these inferred tags, as illustrated
    in [Figure 8-4](#automated_taggingcomma_approved_by_human), thereby training Waterline
    Data’s AI engine.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分析人员可以接受或拒绝这些推断的标签，如[图 8-4](#automated_taggingcomma_approved_by_human)所示，从而训练Waterline
    Data的AI引擎。
- en: '![Automated tagging, approved by human analyst](Images/ebdl_0804.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![自动标记，由人类分析师批准](Images/ebdl_0804.png)'
- en: Figure 8-4\. Automated tagging, approved by human analyst
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. 自动标记，由人类分析师批准
- en: This process greatly reduces the need to manually tag data sets, so new data
    sets become findable as soon as they are cataloged.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程极大地减少了手动打标签数据集的需求，因此一旦数据集被编目，新数据集就能被找到。
- en: Logical Data Management
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑数据管理
- en: While tags are a great way for analysts to find data using familiar business
    terms, they also provide a consistent “logical” view of the enterprise data. Data
    stewards and analysts can now create consistent policies for all the data assets
    without having to keep track of what different fields are called in different
    data sets and systems. From data protection to data quality, modern data management
    tools are embracing tag-based policies as a way to automate what has traditionally
    been manual, error-prone, labor-intensive, and fragile technology that slowed
    down data projects and impeded self-service.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然标签是分析师使用熟悉的业务术语查找数据的好方法，但它们也提供了企业数据的一致“逻辑”视图。数据监护人员和分析师现在可以创建一致的政策，适用于所有数据资产，而无需跟踪不同数据集和系统中不同字段的称呼。从数据保护到数据质量，现代数据管理工具正在采用基于标签的政策作为自动化的方式，这些政策在传统上是手动、易错、劳动密集且脆弱的技术，导致数据项目进展缓慢并阻碍了自助服务。
- en: Sensitive Data Management and Access Control
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 敏感数据管理和访问控制
- en: One of the great worries of data governance teams is how to manage sensitive
    data. There are numerous industry-specific and country-specific regulations that
    govern usage and protection of personal or sensitive information, such as the
    GDPR in Europe, HIPAA in the US, and PCI internationally. In addition, companies
    often maintain their own lists of internal “secret” information that must be protected.
    We refer to any data that is subject to regulatory compliance and access restrictions
    as *sensitive*. To manage sensitive data, enterprises have to first catalog it
    (i.e., find out where it is stored) and then protect it, through either restricting
    access or masking the data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理团队的一个重要关注点是如何管理敏感数据。有许多行业特定和国家特定的法规来管理和保护个人或敏感信息，比如欧洲的GDPR、美国的HIPAA以及国际上的PCI。此外，公司通常也会维护自己的内部“机密”信息清单，这些信息必须受到保护。我们将受监管合规性和访问限制的任何数据称为*敏感数据*。为了管理敏感数据，企业必须首先对其进行目录化（即找出存储位置），然后通过限制访问或对数据进行掩码来保护它。
- en: Traditionally, security administrators had to manually protect each field. For
    example, if a database had a table with a column that contained Social Security
    numbers (SSNs), the administrator had to figure that out and manually create a
    rule that allowed access to that field only to authorized users. If for some reason
    users started putting SSNs in a different field (say, the `Notes` field), that
    field would stay unprotected until someone noticed it and created a new rule to
    protect it. Instead, modern security systems such as Apache Ranger and Cloudera
    Sentry rely on what’s called *tag-based security*. Rather than defining data access
    and data masking policies for specific data sets and fields, these systems define
    policies for specific tags and then apply these policies to any data sets or fields
    with those tags. (For a detailed discussion of managing access, please refer to
    [Chapter 9](ch09.xhtml#governing_data_access).)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，安全管理员必须手动保护每个字段。例如，如果数据库中有一个表，其中包含社会安全号码（SSN）的列，管理员必须找出这一点，并手动创建一个规则，仅允许授权用户访问该字段。如果由于某种原因用户开始将SSN放入不同的字段（比如`Notes`字段），直到有人注意到并创建新的规则来保护它，那个字段就会保持不受保护。相反，像Apache
    Ranger和Cloudera Sentry这样的现代安全系统依赖于所谓的*基于标签的安全*。这些系统不是为特定数据集和字段定义数据访问和数据掩码策略，而是为特定标签定义策略，然后将这些策略应用于具有这些标签的任何数据集或字段。（有关管理访问的详细讨论，请参阅[第9章](ch09.xhtml#governing_data_access)。）
- en: Automated and manual vetting
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化和手动审核
- en: Without an automated approach to sensitive data management, new data sets ingested
    into the data lake cannot be released for use until a human has reviewed them
    and figured out whether they contain anything sensitive.  To drive this process,
    some companies have tried creating a “quarantine zone” where all new data sets
    go and stay until they’ve been manually reviewed and blessed for general use.
    Although the quarantine zone approach makes sense, these companies report significant
    backups in working their way through quarantined data sets. This is because the
    process is time-consuming and error prone—a problem that’s often exacerbated by
    a lack of budget for doing this type of work, because most of the data sets are
    not immediately being used for any projects. This neglect, unfortunately, leads
    to a vicious circle. Since the files in the quarantine zone are not accessible
    to anyone, they are not findable and cannot be used by the analysts, nor can the
    analysts influence the order of curation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有自动化的敏感数据管理方法，新收集到数据湖的数据集无法在经过人工审查并确定其是否包含敏感信息之前释放供使用。为推动此过程，一些公司尝试创建“隔离区”，新数据集全部进入并留在那里，直到经过人工审查并获得一般使用的认可。尽管隔离区方法合乎逻辑，但这些公司报告称在处理隔离数据集时出现了显著的积压。这是因为该过程耗时且容易出错——这种问题通常由于没有预算进行这类工作而恶化，因为大多数数据集并未立即用于任何项目。不幸的是，这种忽视导致了恶性循环。由于隔离区中的文件对任何人都不可访问，因此它们不可查找，分析师也无法影响策划的顺序。
- en: A much more elegant solution can be achieved by using automated sensitive data
    detection. Data sets in the quarantine zone can be automatically scanned by the
    cataloging software and automatically tagged with the type of sensitive data that
    they contain. Tag-based security can then be applied to automatically restrict
    access to those files or deidentify sensitive data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动敏感数据检测可以实现更加优雅的解决方案。隔离区域的数据集可以通过目录软件自动扫描，并自动标记其包含的敏感数据类型。基于标签的安全性可以应用于自动限制对这些文件的访问或者去标识化敏感数据。
- en: As an additional precaution, instead of making the data sets automatically available,
    manual vetting can be done on demand. Such a system applies automated tagging
    and adds the metadata for the data sets to the catalog to make the data sets findable.
    Once an analyst finds a data set that they would like to use, this data set is
    manually curated to verify the correctness of the tags, and any sensitive data
    can be deidentified. This way, while all the data sets are findable and available,
    the limited resources of the data steward group doing curation are spent on useful
    files and funded projects.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的预防措施，可以根据需求手动审查数据集，而不是自动提供数据集。这种系统应用自动标记，并将数据集的元数据添加到目录中以便找到数据集。一旦分析师找到想要使用的数据集，该数据集就会经过手动策划以验证标签的正确性，并对任何敏感数据进行去标识化。通过这种方式，虽然所有数据集都是可查找和可用的，但数据监护团队有限的资源会花费在有用的文件和资助项目上。
- en: Finally, as data sets are provisioned, data sovereignty laws and other regulations
    can be respected. For example, if an analyst in the UK asks for access to German
    data, instead of shipping the data over to the UK, they may be granted permission
    to access the local German data lake.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，随着数据集的提供，可以遵守数据主权法律和其他法规。例如，如果英国的分析师要求访问德国数据，而不是将数据传输到英国，他们可能会被允许访问当地的德国数据湖。
- en: Data Quality
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量
- en: Data quality is a broad topic covered elsewhere in this book. In this chapter,
    we will focus on using the catalog to capture and communicate information about
    the quality of data. A couple of important innovations that a catalog brings to
    the table in this regard are the ability to apply tag-based data quality rules
    and to measure the annotation quality and curation quality of a data set.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量是本书的其他章节中涵盖的广泛主题。在本章中，我们将重点讨论使用目录捕获和传达有关数据质量的信息。目录在这方面带来的一些重要创新包括能够应用基于标签的数据质量规则，并测量数据集的注释质量和策划质量。
- en: Tag-based data quality rules
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于标签的数据质量规则
- en: Less sophisticated cataloging techniques hardcode the rules for data quality
    and sensitivity for each physical field in each physical table or file. More modern
    catalogs—especially catalogs with automated tagging—allow data quality specialists
    and data stewards to define and apply data quality rules for a specific tag. The
    idea is to define the rules and then apply these rules to any field tagged with
    that tag. For example, if we create data quality rules for `Age` that specify
    that it should be a number between 0 and 125, we can then apply it to any field
    tagged with `Age` and count the number of rows that do not contain a number between
    0 and 125\. The quality score can then be captured as a percentage of rows that
    do not conform to the quality rule. In [Figure 8-5](#data_quality_level), out
    of the five rows, only three conform to the data quality rule; therefore, the
    quality level is 60%.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 较简单的编目技术会为每个物理字段在每个物理表或文件中硬编码数据质量和敏感度规则。更现代化的编目系统，特别是具有自动标记功能的编目系统，允许数据质量专家和数据监护人员定义和应用特定标签的数据质量规则。其核心思想是定义规则，然后将这些规则应用到任何带有该标签的字段上。例如，如果我们为`Age`创建数据质量规则，规定其应该是0到125之间的数字，我们可以将其应用于任何带有`Age`标签的字段，并计算不符合0到125之间数字的行数。然后，可以将质量得分捕获为不符合质量规则的行数的百分比。在
    [图8-5](#data_quality_level) 中，五行中只有三行符合数据质量规则，因此质量水平为60%。
- en: '![Data quality level](Images/ebdl_0805.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![数据质量水平](Images/ebdl_0805.png)'
- en: Figure 8-5\. Data quality level
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5\. 数据质量水平
- en: The following sections cover some other ways data quality can be measured.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的部分涵盖了衡量数据质量的其他方式。
- en: Annotation quality
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释质量
- en: Annotation quality refers to how much of a data set is annotated. For example,
    if each field has a tag the annotation quality is 100%, if only half of the fields
    have a tag it’s 50%, and so on. Note that annotation quality can take into account
    both manual and automatically suggested tags. In addition to the tags, it can
    include such information as whether a data set has a description, whether it has
    lineage (imported, manually specified, or automatically suggested), and whether
    required properties (such as sovereignty) are populated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注释质量是指数据集的注释比例。例如，如果每个字段都有一个标签，则注释质量为100%；如果只有一半的字段有标签，则为50%，依此类推。注意，注释质量可以同时考虑手动和自动建议的标签。除了标签，还可以包括数据集是否有描述、是否有血统（导入的、手动指定的或自动建议的）、以及是否填充了必需的属性（如主权）。
- en: Curation quality
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修订质量
- en: Curation quality refers to how many of the tags have been approved or curated
    by humans. The ultimate concern is trustworthiness, and a manually curated data
    set is usually more trustworthy than an automatically annotated one.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 修订质量是指有多少标签已被人工批准或修订。最终关注的是可信度，手动修订的数据集通常比自动注释的数据集更可信。
- en: Unlike annotation quality, which simply checks whether tags are present, curation
    quality does not count an automatically suggested tag as “valid” unless a data
    steward or other authorized user has approved it. Curation quality can also reflect
    whether the data set has a description and lineage (imported, manually specified,
    or automatically suggested), and, if the lineage was automatically suggested,
    whether a curator has approved it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与注释质量不同的是，修订质量不会将自动建议的标签视为“有效”，除非数据监护人员或其他授权用户已经批准。修订质量还可以反映数据集是否有描述和血统（导入的、手动指定的或自动建议的），以及如果血统是自动建议的，则是否已由管理员批准。
- en: Data set quality
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集质量
- en: 'Data set quality can summarize the other three types of quality: tag-based
    data quality, annotation quality, and curation quality. Again, the central issue
    is trustworthiness; the trick is to blend all the measurements into a single one
    with a meaningful value. There is no best practice or formula for this. The approaches
    range from only considering the quality of curated tags to trying to reflect every
    aspect of data quality.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集质量可以总结另外三种质量类型：基于标签的数据质量、注释质量和修订质量。再次强调的是可信度问题；关键在于将所有测量结果融合为一个具有实际意义的单一值。这方面没有最佳实践或公式。方法范围从仅考虑修订标签的质量到试图反映数据质量的每个方面。
- en: For example, if we consider only the quality of curated tags, if every field
    has a curated tag with a quality level of 100%, the data set’s quality level can
    be said to be 100%. If only half the fields have curated tags and the average
    quality of those fields is 80%, the data set quality level is 40%, and so on.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们仅考虑策划标签的质量，如果每个字段都有一个质量水平为100%的策划标签，那么数据集的质量水平可以说是100%。如果只有一半的字段有策划标签，并且这些字段的平均质量为80%，那么数据集的质量水平为40%，依此类推。
- en: 'However, even if a data set has perfectly formed fields with corresponding
    tags, can we really trust it if we do not know where it came from? If the answer
    is no, how do we normalize quality of data with quality of lineage in a single
    measurement? This is a difficult problem, and most companies punt (take the easy
    way out) and simply use different properties to reflect the different aspects
    of data quality or trustworthiness. In fact, this is the solution I recommend:
    aggregate the tag-based quality of all the fields to produce one value at the
    data set level, and keep annotation and curation quality separate. Don’t try to
    come up with a formula that can reflect all three.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使数据集具有完美形式的字段和相应的标签，如果我们不知道它来自哪里，我们真的可以信任它吗？如果答案是否定的，那么我们如何在单一的衡量中归一化数据的质量和血统的质量？这是一个困难的问题，大多数公司选择（采取了简单的方法）简单地使用不同的属性来反映数据质量或可信度的不同方面。事实上，这是我推荐的解决方案：聚合所有字段的基于标签的质量，以生成数据集级别的一个值，并保持注释和策划质量分开。不要试图提出一个可以反映所有三个因素的公式。
- en: Relating Disparate Data
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关联不同的数据
- en: 'One of the challenges of data science work is that it often requires data scientists
    and data engineers to bring together data that has never been combined before.
    The challenge of finding data then becomes not only to find the data sets that
    contain the data you need, but to be able to tell if these data sets can be combined.
    This has two aspects:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学工作的一个挑战是，它通常需要数据科学家和数据工程师将以前从未组合过的数据组合起来。然后找到数据的挑战不仅是找到包含所需数据的数据集，还要能够判断这些数据集是否可以组合。这有两个方面：
- en: Can these data sets be joined? In other words, is there a way to correctly relate
    data in one data set to the data in another data set? For example, suppose a data
    scientist finds a great data set containing personal demographics with people’s
    names, but wants to correlate it to people’s incomes. A search for income data
    may return a number of data sets, but can any of these be correlated to the demographic
    data? If any of the income data sets contain name and address information, that
    would be a great start. If none of them have that data but some contain SSNs,
    the data scientist may then be able to search for a data set containing both SSNs
    and names and addresses and use that data set to join the income and demographic
    data sets together.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些数据集能够连接吗？换句话说，有没有一种方法可以正确地将一个数据集中的数据与另一个数据集中的数据相关联？例如，假设一位数据科学家找到了一个包含个人人口统计信息（包括姓名）的好数据集，但是希望将其与人们的收入相关联。搜索收入数据可能会返回多个数据集，但其中哪些可以与人口统计数据相关联？如果任何一个收入数据集包含姓名和地址信息，那将是一个很好的开始。如果没有一个数据集包含该数据，但其中一些包含社会安全号码（SSNs），则数据科学家随后可以搜索一个既包含SSNs又包含姓名和地址的数据集，并使用该数据集将收入和人口统计数据集连接在一起。
- en: Will joins produce meaningful results? Even if the data scientist in the previous
    example finds some data sets that can be joined, what if these data sets contain
    nonoverlapping data? For example, what if they hold demographic information for
    US customers and income information for EU customers? Even if they both contain
    names and addresses, there will be very little overlap.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接能产生有意义的结果吗？即使前面的数据科学家找到了一些可以连接的数据集，如果这些数据集包含非重叠的数据怎么办？例如，如果它们分别包含美国客户的人口统计信息和欧盟客户的收入信息，即使它们都包含姓名和地址，重叠部分将非常少。
- en: 'In order to be useful, catalogs should assist users in finding related data
    and estimating the usefulness of combining it. There are several ways that can
    be achieved:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有用，目录应该帮助用户找到相关的数据并估计组合它的有用性。有几种方法可以实现这一点：
- en: Field names
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 字段名称
- en: In many well-designed systems, fields with the same name are assumed to contain
    the same data, so analysts often look for columns with the same names in tables
    that need to be joined. This, unfortunately, is often not the case in larger systems,
    and relying on field names may lead to incorrect results across different systems
    with different naming conventions. It also does not help ascertain the usefulness
    of a join without actually running it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多设计良好的系统中，假定具有相同名称的字段包含相同的数据，因此分析人员经常在需要连接的表中寻找具有相同名称的列。然而，在较大的系统中，这通常并非如此，并且依赖字段名称可能会导致在具有不同命名约定的不同系统之间产生错误结果。此外，如果没有实际运行连接，也无法确定连接的有用性。
- en: Primary and foreign keys
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 主键和外键
- en: In relational databases, tables are related to each other using keys. These
    are called primary key–foreign key (PKFK) or referential integrity relationships.
    To use the example from earlier in this chapter, suppose you have a table containing
    customer information. This table will have a primary key to uniquely identify
    each customer, and all other tables will refer to that customer using this key.
    The columns in other tables that reference the primary key in the customer table
    are called foreign keys. PKFK relationships are often captured in entity relationship
    (ER) diagrams created using data modeling tools such as Erwin Data Modeler from
    ERwin or ER/Studio from Idera. PKFK relationships are also sometimes declared
    as referential integrity constraints in relational databases, although most production
    systems avoid these constraints because of the overhead they introduce. PKFK relationships
    are a great way to guarantee that joins will return good results. Unfortunately,
    these relationships are typically only available inside a single system and do
    not help with relating data across systems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系型数据库中，表通过键相关联。这些称为主键-外键（PKFK）或引用完整性关系。举例来说，假设你有一个包含客户信息的表。这个表将有一个主键来唯一标识每个客户，而所有其他表将使用这个键引用该客户。其他表中引用客户表主键的列称为外键。PKFK关系通常在实体关系（ER）图中捕获，使用如Erwin
    Data Modeler（ERwin）或IDERA的ER/Studio等数据建模工具创建。尽管大多数生产系统避免这些约束因为它们引入的开销，但PKFK关系通常也被声明为关系型数据库中的引用完整性约束。PKFK关系是确保连接返回良好结果的一种好方法。不幸的是，这些关系通常仅在单个系统内可用，并且不能帮助关联跨系统的数据。
- en: Usage
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用情况
- en: Useful joins can be gleaned from data usage—for example, by looking at existing
    artifacts that join the data, such as database views, ETL jobs, and reports. They
    can also be inferred by scanning database SQL logs to see what queries are being
    used to join the data. While the artifacts typically provide some context for
    the joins through names and descriptions and usually guarantee that joins will
    produce useful results, SQL queries may not provide information on why the data
    was joined or whether the join was successful.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从数据使用中获取有用的连接，例如通过查看现有的连接数据的工件，如数据库视图、ETL作业和报表。它们还可以通过扫描数据库SQL日志来推断哪些查询用于连接数据。尽管这些工件通常通过名称和描述提供某些连接的上下文，并且通常保证连接将生成有用的结果，但SQL查询可能不会提供有关为何连接数据以及连接是否成功的信息。
- en: Tags
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 标签
- en: The most difficult joins are the ones that have never been done before—especially
    joins across disparate systems and different data formats. Catalogs can greatly
    help with this effort by helping users identify related data by finding data sets
    with the same tags. An estimate of a join’s usefulness can sometimes be deduced
    from the technical metadata obtained during profiling, or it may be necessary
    to actually execute the join and profile the result.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最困难的连接通常是从未进行过的连接，特别是跨不同系统和不同数据格式的连接。目录可以通过帮助用户识别具有相同标签的数据集来极大地帮助这一努力。有时可以通过分析获取的技术元数据来估计连接的有用性，或者可能需要实际执行连接并分析结果。
- en: Establishing Lineage
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立血统
- en: One of the critical questions a catalog needs to answer is whether the analyst
    can trust the data, and knowing where the data came from is a big part of that.
    This is called the data’s *lineage* or *provenance* (a detailed discussion of
    lineage can be found in [Chapter 6](ch06.xhtml#optimizing_for_self-service)).
    One of the jobs of a catalog is to show lineage for the data assets and to fill
    in the gaps where the lineage is missing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 目录需要回答的一个关键问题是分析员是否可以信任数据，而知道数据来源是其中的一个重要部分。这被称为数据的*血统*或*来源*（有关血统的详细讨论可见[第6章](ch06.xhtml#optimizing_for_self-service)）。目录的一个工作是显示数据资产的血统，并填补血统缺失的地方。
- en: Most BI tools, like Tableau and Qlik, capture lineage information indicating
    how visualizations and reports are created. Similarly, most ETL tools, like Informatica,
    IBM InfoSphere, and Talend, capture lineage information automatically as they
    move and transform data. However, a lot of advanced analytics are done using R
    and Python scripts, and a lot of the data transformation and movement is done
    using FTP, scripts written in Pig or Python, and open source Hadoop tools such
    as Sqoop and Flume. These tools do not capture or expose the lineage of data.
    Since lineage is useful only if you can trace it all the way back to the source,
    it’s critical to fill these gaps. Some tools try to fill in missing lineage information
    by scraping system logs (Cloudera Navigator), instrumenting open source systems
    to report lineage (Apache Atlas), requiring users to manually provide lineage
    information for every job they write (Apache Falcon), or trying to deduce it by
    examining the content of the files (IBM InfoSphere Discovery and Waterline Data)
    or SQL logs (Manta and Alation).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数BI工具（如Tableau和Qlik）都会捕获指示可视化和报告创建方式的血统信息。同样，大多数ETL工具（如Informatica、IBM InfoSphere和Talend）在移动和转换数据时会自动捕获血统信息。然而，许多高级分析是通过R和Python脚本完成的，许多数据转换和移动是通过FTP、Pig或Python编写的脚本以及开源Hadoop工具（如Sqoop和Flume）完成的。这些工具不会捕获或暴露数据的血统信息。由于只有追溯到源头时血统信息才有用，因此填补这些空白至关重要。一些工具尝试通过抓取系统日志（Cloudera
    Navigator）、对开源系统进行仪器化以报告血统（Apache Atlas）、要求用户为他们编写的每个作业手动提供血统信息（Apache Falcon），或者通过检查文件内容（IBM
    InfoSphere Discovery和Waterline Data）或SQL日志（Manta和Alation）来推断血统信息来填补缺失的血统信息。
- en: As you can see, there really isn’t one single way to get all the lineage, but
    the catalog is responsible for importing this information from wherever possible
    and stitching it all together. “Stitching” refers to the process of connecting
    different lineage segments.  For example, a table may have been ingested from
    an Oracle data warehouse using Informatica’s ETL tool into a data lake file in
    Parquet format, then joined with a JSON file generated from a Twitter feed using
    a Python script to create a new Hive table, which then was used by a data prep
    tool to create a CSV file that was loaded into a table in a data mart, which in
    turn was used by a BI tool to generate a report. To get a complete picture of
    where the data in the report came from, all these steps need to be connected or
    stitched together. If any of the steps is missing, there will be no way for the
    analyst to tie the report back to the original sources—the Oracle data warehouse
    and Twitter feed.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，确实没有一种单一的方式可以获得所有的血统信息，但目录负责从可能的地方导入这些信息并将其全部串联起来。 "串联" 指的是连接不同的血统段的过程。例如，一张表可能已经通过Informatica的ETL工具从Oracle数据仓库中摄取到Parquet格式的数据湖文件中，然后与通过Python脚本从Twitter源生成的JSON文件进行连接，以创建一个新的Hive表，然后被数据准备工具用来创建一个CSV文件，最终被加载到数据集市中的表中，再由BI工具生成报告。为了全面了解报告中数据的来源，所有这些步骤都需要连接或串联起来。如果任何一步缺失，分析员将无法将报告追溯到原始来源——Oracle数据仓库和Twitter源。
- en: As this hypothetical example illustrates, data often undergoes many changes
    by many tools. Another issue is that even if lineage is available, it is often
    expressed in the language of the tool that generated the data—and not all analysts
    are well versed in all tools. To be able to interpret what’s been done to the
    data, the analyst needs the steps to be documented in business terms. This is
    called *business lineage*. Unfortunately, most enterprises today do not have a
    place to capture and track business lineage. Each job and step may be documented,
    but this is often done inside the tool (for example, as comments in a script),
    or in the developers’ notebooks, Excel files, or wikis. Catalogs present an attractive
    place to gather such lineage documentation together and make it available to the
    users of the data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如这个假设性例子所示，数据经常会经过多种工具的多次变更。另一个问题是，即使有源流信息，它通常是用生成数据的工具的语言表达的——而不是所有分析师都精通所有工具。为了能够解释数据经历了什么，分析师需要将步骤以业务术语记录下来。这被称为*业务源流*。不幸的是，今天大多数企业并没有一个地方来捕捉和跟踪业务源流。每个作业和步骤可能会有记录，但通常是在工具内部完成（例如，作为脚本的注释），或者在开发者的笔记本、Excel
    文件或维基中完成。目录提供了一个吸引人的场所来集合这样的源流文档，并且使其对数据的使用者可用。
- en: Data Provisioning
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据供应
- en: Once the right data set or sets are identified, the user will want to use those
    in other tools. To support this, catalogs often provide data provisioning options.
    Data provisioning may be as simple as opening the data set with a specific tool—for
    example, if an analyst finds a data set containing sales information, they may
    wish to open it in their favorite BI tool to visualize and analyze the data. Similarly,
    if a data scientist or a data engineer has found an interesting raw data set,
    they may want to open it in their favorite data prep tool. This is similar to
    using the Mac Finder or Microsoft Explorer to find a file, right-clicking on it,
    and getting an “Open with” menu that lists all the programs that the file can
    be used in. Since there is a great variety of tools that can be used with data,
    you should make sure that the provisioning capability is extensible and can be
    configured for any tools that users may wish to use.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确认了正确的数据集或数据集合，用户就希望在其他工具中使用它们。为了支持这一点，目录通常提供数据供应选项。数据供应可能就像使用特定工具打开数据集那样简单——例如，如果分析师找到包含销售信息的数据集，他们可能希望在他们喜爱的
    BI 工具中打开它以可视化和分析数据。同样地，如果数据科学家或数据工程师发现了一个有趣的原始数据集，他们可能希望在他们喜爱的数据准备工具中打开它。这类似于使用
    Mac Finder 或 Microsoft Explorer 查找文件，右键单击文件，然后弹出“打开方式”菜单，列出可以用于该文件的所有程序。由于有多种工具可以用于数据，你应该确保供应能力是可扩展的，并且可以配置为用户可能希望使用的任何工具。
- en: Another provisioning operation involves getting access to the data. One of the
    great advantages of using a data catalog is that it makes data findable without
    having to give users access to it. That means that when users find the data they
    need, they must request access before they can use it. An access request may be
    as simple as sending an email to the data owner with a request to add the user
    to the access control group so they can access the data in place, or as complex
    as creating a ticket and starting a lengthy approval workflow that will bring
    the data into the data lake. Access management and ingestion are covered in more
    detail in [Chapter 9](ch09.xhtml#governing_data_access).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个供应操作涉及获取数据访问权。使用数据目录的一个巨大优势是，它使得数据可以被发现，而无需用户直接访问。这意味着当用户找到他们需要的数据时，他们必须在使用之前请求访问权限。访问请求可能只需发送电子邮件给数据所有者，请求将用户添加到访问控制组，以便他们可以直接访问数据；或者可能会复杂到创建一个工单，并启动一个长期的批准工作流程，将数据导入数据湖中。访问管理和摄取将在[第9章](ch09.xhtml#governing_data_access)中详细讨论。
- en: Tools for Building a Catalog
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建目录的工具
- en: 'Several vendors provide data catalog tools, including Waterline Smart Data
    Catalog, Informatica Enterprise Data Catalog, Alation, IBM Watson Knowledge Catalog,
    AWS Glue, and Apache Atlas (developed by Hortonworks and its partners). When choosing
    a vendor, you should consider several important capabilities:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 几家供应商提供数据目录工具，包括Waterline Smart Data Catalog、Informatica Enterprise Data Catalog、Alation、IBM
    Watson Knowledge Catalog、AWS Glue和Apache Atlas（由Hortonworks及其合作伙伴开发）。在选择供应商时，应考虑几个重要的能力：
- en: Native big data processing for performance and scalability
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地大数据处理以实现性能和可伸缩性
- en: Automated data discovery and classification
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化数据发现和分类
- en: Integration with other enterprise metadata repositories and single-platform
    catalogs
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他企业元数据存储库和单平台目录的集成
- en: User-friendliness
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户友好性
- en: The first consideration is support for native big data processing tools such
    as Hadoop or Spark. A data lake is the largest data system in the enterprise,
    and the combined power of a large cluster of nodes is required to process and
    catalog the data it contains. The whole point of Hadoop is not just to provide
    a cost-effective place to store data—we’ve had that for years—but also a cost-effective
    place to *process* data. Trying to catalog Hadoop-scale data without leveraging
    Hadoop’s processing capabilities simply won’t work. While some of the available
    tools were designed natively for Hadoop, others, like Alation and Collibra, were
    designed to work with relational databases and either require data to be loaded
    into an RDBMS or have proprietary engines that run outside Hadoop and won’t be
    able to scale to handle a data lake.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个考虑因素是是否支持本地大数据处理工具，如Hadoop或Spark。数据湖是企业中最大的数据系统，需要大型节点集群的组合能力来处理和目录其包含的数据。Hadoop的整个关键不仅在于提供一个成本效益的数据存储场所（多年来我们一直有这样的场所），还在于提供一个成本效益的数据*处理*场所。试图在不利用Hadoop处理能力的情况下目录Hadoop规模的数据是行不通的。虽然一些可用工具是专为Hadoop设计的，比如Alation和Collibra等，但另一些工具如Cloudera
    Navigator或AWS Glue则是为关系数据库设计的，要求数据加载到RDBMS中或者有专有引擎在Hadoop之外运行，不能扩展以处理数据湖规模。
- en: Another issue is that the sheer scope and complexity of data in the data lake
    makes it impossible for humans to manually classify or tag all the data with business
    metadata. Therefore, an automated approach is required to complete this classification.
    While all the tools mentioned here allow analysts to tag data in some fashion,
    some, like Waterline Data, provide an automated discovery engine that learns from
    analyst tagging and automatically classifies other data sets.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是数据湖中数据的范围和复杂性使得人类无法手动对所有数据进行分类或标记业务元数据。因此，需要采用自动化方法来完成此分类。虽然所有提到的工具都允许分析师以某种方式标记数据，但像Waterline
    Data这样的一些工具提供了一个自动发现引擎，从分析师标记中学习，并自动分类其他数据集。
- en: Of course, a Hadoop data lake is only part of the enterprise data ecosystem,
    and as such has to integrate with the rest of the governance infrastructure. Many
    enterprises will have extensive metadata investments already that need to be incorporated
    into the new data lake. There are several tools that provide enterprise data repositories.
    While Hadoop may be the most scalable and cost-effective platform to process both
    data inside Hadoop and data from other sources, single-platform solutions like
    Cloudera Navigator or AWS Glue are limiting and will not address enterprise requirements.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Hadoop数据湖只是企业数据生态系统的一部分，因此必须与其余的治理基础设施集成。许多企业可能已经在大量元数据投资，需要将其整合到新的数据湖中。有几种工具提供企业数据存储库。虽然Hadoop可能是处理Hadoop内部数据和其他来源数据的最可扩展和成本效益最高的平台，但像Cloudera
    Navigator或AWS Glue这样的单平台解决方案具有限制性，并且不能满足企业需求。
- en: Finally, a lot of metadata solutions are designed for IT and governance specialists.
    To be widely adopted, the inventory solution must be intuitive and usable by non-technical
    analysts, usually without much, if any, training. A business analyst–focused UI
    helps by providing an uncluttered view with business terms and descriptions instead
    of cramming in a lot of technical details. Technical details are certainly necessary
    for some of the users, and should be easily accessible, but should not be forced
    onto the business users. Some catalogs achieve this by having different role-based
    views, while others present a business view and provide a way for technical users
    to drill in to see technical details.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多元数据解决方案是为IT和治理专家设计的。为了被广泛采用，库存解决方案必须直观且可供非技术分析师使用，通常无需太多培训，如果有的话。面向业务分析师的用户界面有助于提供一个清晰的视图，显示业务术语和描述，而不是填入大量技术细节。对于某些用户来说，技术细节当然是必要的，并且应该易于访问，但不应该强加给业务用户。一些目录通过具有不同基于角色的视图来实现此目标，而其他则提供业务视图，并提供了一种让技术用户深入查看技术细节的方式。
- en: Tool Comparison
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具比较
- en: '[Table 8-4](#catalog_tool_comparison) summarizes the capabilities of a selection
    of data cataloging products. There are basically three groups of tools:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-4](#catalog_tool_comparison) 总结了一些数据目录产品的功能。基本上有三组工具：'
- en: Enterprise cataloging tools that try to catalog all the data in the enterprise.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业目录工具试图对企业中所有数据进行目录化。
- en: Single-platform cataloging tools that focus on a particular platform.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单平台目录工具专注于特定平台。
- en: Legacy/relational cataloging tools that do not provide native big data support.
    These tools do not run natively in Hadoop or other big data environments and require
    a relational interface, such as Hive, to be able to catalog big data meaningfully.
    They are sometimes put in place as part of a data pond (a data warehouse built
    on a big data platform, as described in [Chapter 5](ch05.xhtml#from_data_pondssolidusbig_data_warehouse),
    where the only access to data is through Hive), but will not be able to support
    a data lake with lots of raw data in native Hadoop file formats.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统/关系型的编目工具不提供本地的大数据支持。这些工具不能在Hadoop或其他大数据环境中本地运行，并且需要关系接口（如Hive）来有意义地编目大数据。它们有时被放置在数据池中（一个构建在大数据平台上的数据仓库，如[第5章](ch05.xhtml#from_data_pondssolidusbig_data_warehouse)中所述），但不能支持以本地Hadoop文件格式存储大量原始数据的数据湖。
- en: Table 8-4\. *Catalog tool comparison*
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Table 8-4\. *目录工具比较*
- en: '|   | **Big data support** | **Tagging** | **Enterprise** | **Business analyst–focused
    UI** |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|   | **大数据支持** | **标记** | **企业** | **面向业务分析师的用户界面** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Enterprise** |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| **企业** |'
- en: '| Waterline Data | Native | Automated | Y | Y |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Waterline Data | 本地 | 自动化 | Y | Y |'
- en: '| Informatica Enterprise Data Catalog | Native | Manual | Y | Y |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Informatica Enterprise Data Catalog | 本地 | 手动 | Y | Y |'
- en: '| **Single platform** |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **单一平台** |'
- en: '| Cloudera Navigator | Native | Manual |   |   |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Cloudera Navigator | 本地 | 手动 |   |   |'
- en: '| Apache Atlas | Native | Manual |   |   |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Apache Atlas | 本地 | 手动 |   |   |'
- en: '| AWS Glue | Native | Manual |   |   |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| AWS Glue | 本地 | 手动 |   |   |'
- en: '| IBM Watson Catalog | Native | Automated |   | Y |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| IBM Watson Catalog | 本地 | 自动化 |   | Y |'
- en: '| **Legacy/relational** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **传统/关系型** |'
- en: '| Alation | Hive only | Manual | Y | Y |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Alation | 仅Hive | 手动 | Y | Y |'
- en: '| Collibra | Hive only | Manual | Y | Y |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Collibra | 仅Hive | 手动 | Y | Y |'
- en: The Data Ocean
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据海洋
- en: If catalogs provide location transparency, the question arises of whether we
    even need a data lake. Why not catalog *all* the data and make it available, in
    a so-called “data ocean”? Some enterprises are embarking on such ambitious projects,
    but the scope and complexity of these undertakings is quite staggering and will
    likely require years of dedicated effort. Nevertheless, it is such an attractive
    alternative to shuffling and copying data around that some early adapters are
    willing to embark on this journey. In addition, the current regulatory climate
    that requires data transparency, mandates data privacy, and specifies appropriate
    uses of data is a strong driver forcing enterprises to work on creating a single
    point of visibility, governance, and audit centered on data catalogs. The regulatory
    compliance and data ocean efforts are synergistic and often work hand in hand.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目录提供位置透明度，那么我们是否甚至需要数据湖呢？为什么不将*所有*数据编目并提供在所谓的“数据海洋”中？一些企业正在着手进行这样雄心勃勃的项目，但这些项目的范围和复杂性非常惊人，可能需要数年的专注努力。然而，这种替代方案非常吸引人，可以避免频繁移动和复制数据，因此一些早期采用者愿意踏上这个旅程。此外，当前的监管环境要求数据透明性，规定数据隐私，并规定数据的适当使用，这是强制企业在数据编目中创建单一的可见性、治理和审计点的强大动力。监管合规和数据海洋的努力是协同的，并且经常手牵手工作。
- en: Conclusion
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Data catalogs are an integral part of data lakes and the enterprise data ecosystem.
    As data grows exponentially and data use pervades all aspects of business, having
    an automated way of cataloging the data and enabling users to find, understand,
    and trust it is a necessary first step on the road to data-driven decision making.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数据目录是数据湖和企业数据生态系统的一个组成部分。随着数据呈指数级增长并且数据使用渗透到业务的各个方面，有一种自动化编目数据的方式并使用户能够找到、理解和信任数据，这是通向数据驱动决策的必要第一步。
