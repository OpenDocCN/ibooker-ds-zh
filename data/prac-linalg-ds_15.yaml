- en: Chapter 15\. Eigendecomposition and SVD Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 15 ç« ã€‚ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£åº”ç”¨
- en: Eigendecomposition and SVD are gems that linear algebra has bestowed upon modern
    human civilization. Their importance in modern applied mathematics cannot be understated,
    and their applications are uncountable and spread across myriad disciplines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£æ˜¯çº¿æ€§ä»£æ•°èµ‹äºˆç°ä»£äººç±»æ–‡æ˜çš„å®è—ã€‚å®ƒä»¬åœ¨ç°ä»£åº”ç”¨æ•°å­¦ä¸­çš„é‡è¦æ€§ä¸å¯ä½ä¼°ï¼Œå…¶åº”ç”¨éå¸ƒå„ç§å­¦ç§‘ã€‚
- en: In this chapter, I will highlight three applications that you are likely to
    come across in data science and related fields. My main goal is to show you that
    seemingly complicated data science and machine learning techniques are actually
    quite sensible and easily understood, once youâ€™ve learned the linear algebra topics
    in this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘å°†é‡ç‚¹ä»‹ç»æ‚¨åœ¨æ•°æ®ç§‘å­¦åŠç›¸å…³é¢†åŸŸå¯èƒ½é‡åˆ°çš„ä¸‰ä¸ªåº”ç”¨ç¨‹åºã€‚æˆ‘çš„ä¸»è¦ç›®æ ‡æ˜¯å‘æ‚¨å±•ç¤ºï¼Œä¸€æ—¦æ‚¨å­¦ä¹ äº†æœ¬ä¹¦ä¸­çš„çº¿æ€§ä»£æ•°ä¸»é¢˜ï¼Œè¡¨é¢ä¸Šå¤æ‚çš„æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯å®é™…ä¸Šæ˜¯ç›¸å½“åˆç†ä¸”å®¹æ˜“ç†è§£çš„ã€‚
- en: PCA Using Eigendecomposition and SVD
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£è¿›è¡Œ PCA
- en: The purpose of PCA is to find a set of basis vectors for a dataset that point
    in the direction that maximizes covariation across the variables.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: PCA çš„ç›®çš„æ˜¯ä¸ºæ•°æ®é›†æ‰¾åˆ°ä¸€ç»„åŸºå‘é‡ï¼Œè¿™äº›å‘é‡æŒ‡å‘æœ€å¤§åŒ–å˜é‡ä¹‹é—´åå˜æ€§çš„æ–¹å‘ã€‚
- en: Imagine that an N-D dataset exists in an N-D space, with each data point being
    a coordinate in that space. This is sensible when you think about storing the
    data in a matrix with *N* observations (each row is an observation) of *M* features
    (each column is a feature, also called variable or measurement); the data live
    in <math alttext="double-struck upper R Superscript upper M"><msup><mi>â„</mi>
    <mi>M</mi></msup></math> and comprise *N* vectors or coordinates.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ª N ç»´æ•°æ®é›†å­˜åœ¨äº N ç»´ç©ºé—´ä¸­ï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®ç‚¹æ˜¯è¯¥ç©ºé—´ä¸­çš„ä¸€ä¸ªåæ ‡ã€‚å½“æ‚¨è€ƒè™‘å°†æ•°æ®å­˜å‚¨åœ¨ä¸€ä¸ªçŸ©é˜µä¸­æ—¶ï¼ŒçŸ©é˜µå…·æœ‰ *N* ä¸ªè§‚æµ‹ï¼ˆæ¯è¡Œæ˜¯ä¸€ä¸ªè§‚æµ‹ï¼‰å’Œ
    *M* ä¸ªç‰¹å¾ï¼ˆæ¯åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œä¹Ÿç§°ä¸ºå˜é‡æˆ–æµ‹é‡ï¼‰ï¼›æ•°æ®å­˜åœ¨äº<math alttext="double-struck upper R Superscript
    upper M"><msup><mi>â„</mi> <mi>M</mi></msup></math>ï¼Œå¹¶ä¸”åŒ…å« *N* ä¸ªå‘é‡æˆ–åæ ‡ã€‚
- en: An example in 2D is shown in [FigureÂ 15-1](#fig_15_1). The left-side panel shows
    the data in its original data space, in which each variable provides a basis vector
    for the data. Clearly the two variables (the x- and y-axes) are related to each
    other, and clearly there is a direction in the data that captures that relation
    better than either of the feature basis vectors.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äºŒç»´ç¤ºä¾‹ä¸­ï¼Œå¦‚[å›¾Â 15-1](#fig_15_1)æ‰€ç¤ºã€‚å·¦ä¾§é¢æ¿æ˜¾ç¤ºäº†æ•°æ®åœ¨å…¶åŸå§‹æ•°æ®ç©ºé—´ä¸­çš„æƒ…å†µï¼Œå…¶ä¸­æ¯ä¸ªå˜é‡ä¸ºæ•°æ®æä¾›äº†ä¸€ä¸ªåŸºå‘é‡ã€‚æ˜¾ç„¶ï¼Œä¸¤ä¸ªå˜é‡ï¼ˆx
    å’Œ y è½´ï¼‰å½¼æ­¤ç›¸å…³ï¼Œå¹¶ä¸”æ•°æ®ä¸­æœ‰ä¸€ä¸ªæ–¹å‘èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åˆ°è¿™ç§å…³ç³»ï¼Œè¶…è¿‡äº†ä»»ä¸€ç‰¹å¾åŸºå‘é‡ã€‚
- en: '![PCA](assets/plad_1501.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![PCA](assets/plad_1501.png)'
- en: Figure 15-1\. Graphical overview of PCA in 2D
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 15-1ã€‚PCA åœ¨äºŒç»´ä¸­çš„å›¾å½¢æ¦‚è¿°
- en: The goal of PCA is to find a new set of basis vectors such that the linear relationships
    across the variables are maximally aligned with the basis vectorsâ€”thatâ€™s what
    the right-side panel of [FigureÂ 15-1](#fig_15_1) shows. Importantly, PCA has the
    constraint that the new basis vectors are orthogonal rotations of the original
    basis vectors. In the exercises, you will see the implications of this constraint.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PCA çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æ–°çš„åŸºå‘é‡ï¼Œä½¿å¾—å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ä¸åŸºå‘é‡æœ€å¤§åœ°å¯¹é½â€”â€”è¿™æ­£æ˜¯[å›¾Â 15-1](#fig_15_1)å³ä¾§é¢æ¿æ‰€å±•ç¤ºçš„å†…å®¹ã€‚é‡è¦çš„æ˜¯ï¼ŒPCA
    æœ‰ä¸€ä¸ªçº¦æŸæ¡ä»¶ï¼Œå³æ–°çš„åŸºå‘é‡å¿…é¡»æ˜¯åŸå§‹åŸºå‘é‡çš„æ­£äº¤æ—‹è½¬ã€‚åœ¨ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°è¿™ä¸ªçº¦æŸæ¡ä»¶çš„å½±å“ã€‚
- en: In the next section, I will introduce the math and procedures for computing
    PCA; in the exercises, you will have the opportunity to implement PCA using eigendecomposition
    and SVD, and compare your results against Pythonâ€™s implementation of PCA.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘å°†ä»‹ç»è®¡ç®— PCA çš„æ•°å­¦å’Œè¿‡ç¨‹ï¼›åœ¨ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†æœ‰æœºä¼šä½¿ç”¨ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£å®ç° PCAï¼Œå¹¶å°†ç»“æœä¸ Python å®ç°çš„ PCA
    è¿›è¡Œæ¯”è¾ƒã€‚
- en: The Math of PCA
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA çš„æ•°å­¦åŸºç¡€
- en: PCA combines the statistical concept of variance with the linear algebra concept
    of linear weighted combination. Variance, as you know, is a measure of the dispersion
    of a dataset around its average value. PCA makes the assumption that variance
    is good, and directions in the data space that have more variance are more important
    (a.k.a. â€œvariance = relevanceâ€).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PCA ç»“åˆäº†æ–¹å·®çš„ç»Ÿè®¡æ¦‚å¿µå’Œçº¿æ€§ä»£æ•°ä¸­çš„çº¿æ€§åŠ æƒç»„åˆæ¦‚å¿µã€‚æ–¹å·®æ˜¯æ•°æ®é›†å›´ç»•å…¶å¹³å‡å€¼çš„ç¦»æ•£ç¨‹åº¦çš„åº¦é‡ã€‚PCA å‡è®¾æ–¹å·®æ˜¯æœ‰ç›Šçš„ï¼Œå¹¶ä¸”åœ¨æ•°æ®ç©ºé—´ä¸­å…·æœ‰æ›´å¤šæ–¹å·®çš„æ–¹å‘æ›´é‡è¦ï¼ˆä¹Ÿå°±æ˜¯â€œæ–¹å·®=ç›¸å…³æ€§â€ï¼‰ã€‚
- en: But in PCA, weâ€™re not just interested in the variance *within* one variable;
    instead, we want to find the linear weighted combination *across* all variables
    that maximizes variance of that component (a *component* is a linear weighted
    combination of variables).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨PCAä¸­ï¼Œæˆ‘ä»¬ä¸ä»…ä»…å…³æ³¨ä¸€ä¸ªå˜é‡å†…çš„æ–¹å·®ï¼›ç›¸åï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°è·¨æ‰€æœ‰å˜é‡çš„çº¿æ€§åŠ æƒç»„åˆï¼Œä½¿å¾—è¯¥æˆåˆ†çš„æ–¹å·®æœ€å¤§åŒ–ï¼ˆä¸€ä¸ª*æˆåˆ†*æ˜¯å˜é‡çš„çº¿æ€§åŠ æƒç»„åˆï¼‰ã€‚
- en: 'Letâ€™s write this down in math. Matrix <math alttext="bold upper X"><mi>ğ—</mi></math>
    is our data matrix (a tall full column-rank matrix of observations by features),
    and <math alttext="bold w"><mi>ğ°</mi></math> is the vector of weights. Our goal
    in PCA is to find the set of weights in <math alttext="bold w"><mi>ğ°</mi></math>
    such that <math alttext="bold upper X bold w"><mrow><mi>ğ—</mi> <mi>ğ°</mi></mrow></math>
    has maximal variance. Variance is a scalar, so we can write that down as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨æ•°å­¦æ–¹å¼è¡¨è¾¾è¿™ä¸€ç‚¹ã€‚çŸ©é˜µ <math alttext="bold upper X"><mi>ğ—</mi></math> æ˜¯æˆ‘ä»¬çš„æ•°æ®çŸ©é˜µï¼ˆä¸€ä¸ªé«˜çš„ã€å…¨åˆ—ç§©çš„è§‚æµ‹ç‰¹å¾çŸ©é˜µï¼‰ï¼Œ<math
    alttext="bold w"><mi>ğ°</mi></math> æ˜¯æƒé‡å‘é‡ã€‚æˆ‘ä»¬åœ¨PCAä¸­çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æƒé‡ <math alttext="bold
    w"><mi>ğ°</mi></math> ï¼Œä½¿å¾— <math alttext="bold upper X bold w"><mrow><mi>ğ—</mi>
    <mi>ğ°</mi></mrow></math> çš„æ–¹å·®æœ€å¤§åŒ–ã€‚æ–¹å·®æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å†™ä¸‹å¦‚ä¸‹æ–¹ç¨‹ï¼š
- en: <math alttext="lamda equals parallel-to bold upper X bold w parallel-to" display="block"><mrow><mi>Î»</mi>
    <mo>=</mo> <msup><mrow><mo>âˆ¥</mo><mi>ğ—</mi><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda equals parallel-to bold upper X bold w parallel-to" display="block"><mrow><mi>Î»</mi>
    <mo>=</mo> <msup><mrow><mo>âˆ¥</mo><mi>ğ—</mi><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: The squared vector norm is actually the same thing as variance when the data
    is mean-centered (that is, each data variable has a mean of zero);^([1](ch15.xhtml#idm45733290809152))
    Iâ€™ve omitted a scaling factor of 1/(N âˆ’ 1), because it does not affect the solution
    to our optimization goal.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ•°æ®å‡å€¼ä¸ºé›¶ï¼ˆå³ï¼Œæ¯ä¸ªæ•°æ®å˜é‡çš„å‡å€¼ä¸ºé›¶ï¼‰æ—¶ï¼Œå¹³æ–¹å‘é‡èŒƒæ•°å®é™…ä¸Šä¸æ–¹å·®ç›¸åŒï¼›^([1](ch15.xhtml#idm45733290809152))
    æˆ‘ä»¬çœç•¥äº†ç¼©æ”¾å› å­ 1/(N âˆ’ 1)ï¼Œå› ä¸ºå®ƒä¸ä¼šå½±å“æˆ‘ä»¬ä¼˜åŒ–ç›®æ ‡çš„è§£å†³æ–¹æ¡ˆã€‚
- en: 'The problem with that equation is you can simply set <math alttext="bold w"><mi>ğ°</mi></math>
    to be HUGE numbers; the bigger the weights, the larger the variance. The solution
    is to scale the norm of the weighted combination of data variables by the norm
    of the weights:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹ç¨‹çš„é—®é¢˜åœ¨äºä½ å¯ä»¥ç®€å•åœ°å°† <math alttext="bold w"><mi>ğ°</mi></math> è®¾ç½®ä¸ºå·¨å¤§çš„æ•°ï¼›æƒé‡è¶Šå¤§ï¼Œæ–¹å·®è¶Šå¤§ã€‚è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡æ•°æ®å˜é‡çš„åŠ æƒç»„åˆçš„èŒƒæ•°æ¥ç¼©æ”¾æƒé‡çš„èŒƒæ•°ï¼š
- en: <math alttext="lamda equals StartFraction parallel-to bold upper X bold w parallel-to
    Over parallel-to bold w parallel-to EndFraction" display="block"><mrow><mi>Î»</mi>
    <mo>=</mo> <mfrac><msup><mrow><mo>âˆ¥</mo><mi>ğ—</mi><mi>ğ°</mi><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup> <msup><mrow><mo>âˆ¥</mo><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mfrac></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda equals StartFraction parallel-to bold upper X bold w parallel-to
    Over parallel-to bold w parallel-to EndFraction" display="block"><mrow><mi>Î»</mi>
    <mo>=</mo> <mfrac><msup><mrow><mo>âˆ¥</mo><mi>ğ—</mi><mi>ğ°</mi><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup> <msup><mrow><mo>âˆ¥</mo><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mfrac></mrow></math>
- en: 'Now we have a ratio of two vector norms. We can expand those norms into dot
    products to gain some insight into the equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰ä¸¤ä¸ªå‘é‡èŒƒæ•°çš„æ¯”ç‡ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›èŒƒæ•°æ‰©å±•ä¸ºç‚¹ç§¯ï¼Œä»¥è·å¾—å¯¹æ–¹ç¨‹çš„ä¸€äº›è§è§£ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper X Superscript upper T Baseline
    bold upper X bold w Over bold w Superscript upper T Baseline bold w EndFraction
    2nd Row 1st Column bold upper C 2nd Column equals bold upper X Superscript upper
    T Baseline bold upper X 3rd Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper C bold w Over bold w Superscript
    upper T Baseline bold w EndFraction EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>Î»</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <msup><mi>ğ—</mi> <mtext>T</mtext></msup>
    <mi>ğ—</mi><mi>ğ°</mi></mrow> <mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ°</mi></mrow></mfrac></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi>ğ‚</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>ğ—</mi> <mtext>T</mtext></msup> <mi>ğ—</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mi>Î»</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ‚</mi><mi>ğ°</mi></mrow>
    <mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ°</mi></mrow></mfrac></mrow></mtd></mtr></mtable></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper X Superscript upper T Baseline
    bold upper X bold w Over bold w Superscript upper T Baseline bold w EndFraction
    2nd Row 1st Column bold upper C 2nd Column equals bold upper X Superscript upper
    T Baseline bold upper X 3rd Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper C bold w Over bold w Superscript
    upper T Baseline bold w EndFraction EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>Î»</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <msup><mi>ğ—</mi> <mtext>T</mtext></msup>
    <mi>ğ—</mi><mi>ğ°</mi></mrow> <mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ°</mi></mrow></mfrac></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi>ğ‚</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>ğ—</mi> <mtext>T</mtext></msup> <mi>ğ—</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mi>Î»</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ‚</mi><mi>ğ°</mi></mrow>
    <mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ°</mi></mrow></mfrac></mrow></mtd></mtr></mtable></math>
- en: Weâ€™ve now discovered that the solution to PCA is the same as the solution to
    finding the directional vector that maximizes the *normalized* quadratic form
    (the vector norm is the normalization term) of the data covariance matrix.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‘ç°PCAçš„è§£æ³•ä¸æ‰¾åˆ°æœ€å¤§åŒ–æ•°æ®åæ–¹å·®çŸ©é˜µçš„*æ ‡å‡†åŒ–*äºŒæ¬¡å½¢å¼çš„æ–¹å‘å‘é‡ç›¸åŒã€‚
- en: Thatâ€™s all fine, but how do we actually find the elements in vector <math alttext="bold
    w"><mi>ğ°</mi></math> that maximize <math alttext="lamda"><mi>Î»</mi></math> ?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€åˆ‡éƒ½å¾ˆå¥½ï¼Œä½†æˆ‘ä»¬å¦‚ä½•å®é™…æ‰¾åˆ°å‘é‡ <math alttext="bold w"><mi>ğ°</mi></math> ä¸­çš„å…ƒç´ æ¥æœ€å¤§åŒ– <math alttext="lamda"><mi>Î»</mi></math>
    ï¼Ÿ
- en: 'The linear algebra approach here is to consider not just a single vector solution
    but an entire set of solutions. Thus, we rewrite the equation using matrix <math
    alttext="bold upper W"><mi>ğ–</mi></math> instead of vector <math alttext="bold
    w"><mi>ğ°</mi></math> . That would give a matrix in the denominator, which is not
    a valid operation in linear algebra; therefore, we multiply by the inverse:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§ä»£æ•°çš„æ–¹æ³•æ˜¯è€ƒè™‘ä¸åªæ˜¯å•ä¸€å‘é‡è§£ï¼Œè€Œæ˜¯æ•´ä¸ªè§£é›†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ©é˜µ <math alttext="bold upper W"><mi>ğ–</mi></math>
    è€Œä¸æ˜¯å‘é‡ <math alttext="bold w"><mi>ğ°</mi></math> é‡å†™æ–¹ç¨‹ã€‚è¿™å°†ç»™å‡ºä¸€ä¸ªåœ¨çº¿æ€§ä»£æ•°ä¸­ä¸åˆæ³•çš„çŸ©é˜µä½œä¸ºåˆ†æ¯ï¼›å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡å…¶é€†çŸ©é˜µä¹˜ä»¥ï¼š
- en: <math alttext="bold upper Lamda equals left-parenthesis bold upper W Superscript
    upper T Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C bold upper W" display="block"><mrow><mi
    mathvariant="bold">Î›</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Lamda equals left-parenthesis bold upper W Superscript
    upper T Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C bold upper W" display="block"><mrow><mi
    mathvariant="bold">Î›</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></math>
- en: 'From here, we apply some algebra and see what happens:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œï¼Œæˆ‘ä»¬åº”ç”¨ä¸€äº›ä»£æ•°ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper W right-parenthesis
    Superscript negative 1 Baseline bold upper W Superscript upper T Baseline bold
    upper C bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold
    upper W Superscript negative 1 Baseline bold upper W Superscript minus upper T
    Baseline bold upper W Superscript upper T Baseline bold upper C bold upper W 3rd
    Row 1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C bold upper W 4th Row 1st Column bold upper W bold upper
    Lamda 2nd Column equals bold upper C bold upper W EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi mathvariant="bold">Î›</mi></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mrow><mo>-</mo><mtext>T</mtext></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi
    mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Î›</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper W right-parenthesis
    Superscript negative 1 Baseline bold upper W Superscript upper T Baseline bold
    upper C bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold
    upper W Superscript negative 1 Baseline bold upper W Superscript minus upper T
    Baseline bold upper W Superscript upper T Baseline bold upper C bold upper W 3rd
    Row 1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C bold upper W 4th Row 1st Column bold upper W bold upper
    Lamda 2nd Column equals bold upper C bold upper W EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi mathvariant="bold">Î›</mi></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mrow><mo>-</mo><mtext>T</mtext></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi
    mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Î›</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
- en: Remarkably, weâ€™ve discovered that the solution to PCA is to perform an eigendecomposition
    on the data covariance matrix. The eigenvectors are the weights for the data variables,
    and their corresponding eigenvalues are the variances of the data along each direction
    (each column of <math alttext="bold upper W"><mi>ğ–</mi></math> ).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°PCAçš„è§£æ³•æ˜¯å¯¹æ•°æ®åæ–¹å·®çŸ©é˜µæ‰§è¡Œç‰¹å¾åˆ†è§£ã€‚ç‰¹å¾å‘é‡æ˜¯æ•°æ®å˜é‡çš„æƒé‡ï¼Œå®ƒä»¬å¯¹åº”çš„ç‰¹å¾å€¼æ˜¯æ•°æ®æ²¿æ¯ä¸ªæ–¹å‘çš„æ–¹å·®ï¼ˆ<math alttext="bold
    upper W"><mi>ğ–</mi></math> çš„æ¯ä¸€åˆ—ï¼‰ã€‚
- en: Because covariance matrices are symmetric, their eigenvectorsâ€”and therefore
    principal componentsâ€”are orthogonal. This has important implications for the appropriateness
    of PCA for data analysis, which you will discover in the exercises.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºåæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œå®ƒä»¬çš„ç‰¹å¾å‘é‡â€”â€”å› æ­¤ä¸»æˆåˆ†â€”â€”æ˜¯æ­£äº¤çš„ã€‚è¿™å¯¹PCAåœ¨æ•°æ®åˆ†æä¸­çš„é€‚ç”¨æ€§æœ‰é‡è¦çš„å½±å“ï¼Œä½ å°†åœ¨ç»ƒä¹ ä¸­å‘ç°ã€‚
- en: The Steps to Perform a PCA
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰§è¡ŒPCAçš„æ­¥éª¤
- en: With the math out of the way, here are the steps to implement a PCA:^([2](ch15.xhtml#idm45733290657888))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°å­¦å·²ç»æ¸…æ¥šäº†ï¼Œä¸‹é¢æ˜¯å®æ–½PCAçš„æ­¥éª¤ï¼š
- en: Compute the covariance matrix of the data. The resulting covariance matrix will
    be features-by-features. Each feature in the data must be mean-centered prior
    to computing covariance.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ•°æ®çš„åæ–¹å·®çŸ©é˜µã€‚å¾—åˆ°çš„åæ–¹å·®çŸ©é˜µå°†æŒ‰ç‰¹å¾-ç‰¹å¾æ’åˆ—ã€‚åœ¨è®¡ç®—åæ–¹å·®ä¹‹å‰ï¼Œæ•°æ®ä¸­çš„æ¯ä¸ªç‰¹å¾å¿…é¡»è¿›è¡Œå‡å€¼ä¸­å¿ƒåŒ–ã€‚
- en: Take the eigendecomposition of that covariance matrix.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹è¯¥åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾å€¼åˆ†è§£ã€‚
- en: Sort the eigenvalues descending by magnitude, and sort the eigenvectors accordingly.
    Eigenvalues of the PCA are sometimes called *latent factor scores*.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰ç…§å¤§å°é™åºæ’åºç‰¹å¾å€¼ï¼Œå¹¶ç›¸åº”åœ°æ’åºç‰¹å¾å‘é‡ã€‚PCAçš„ç‰¹å¾å€¼æœ‰æ—¶è¢«ç§°ä¸º*æ½œåœ¨å› å­å¾—åˆ†*ã€‚
- en: Compute the â€œcomponent scoresâ€ as the weighted combination of all data features,
    where the eigenvector provides the weights. The eigenvector associated with the
    largest eigenvalue is the â€œmost importantâ€ component, meaning the one with the
    largest variance.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—â€œæˆåˆ†å¾—åˆ†â€ï¼Œä½œä¸ºæ‰€æœ‰æ•°æ®ç‰¹å¾çš„åŠ æƒç»„åˆï¼Œå…¶ä¸­ç‰¹å¾å‘é‡æä¾›æƒé‡ã€‚ä¸æœ€å¤§ç‰¹å¾å€¼ç›¸å…³è”çš„ç‰¹å¾å‘é‡æ˜¯â€œæœ€é‡è¦â€çš„æˆåˆ†ï¼Œæ„å‘³ç€å®ƒå…·æœ‰æœ€å¤§çš„æ–¹å·®ã€‚
- en: Convert the eigenvalues to percent variance explained to facilitate interpretation.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç‰¹å¾å€¼è½¬æ¢ä¸ºç™¾åˆ†æ¯”æ–¹å·®è§£é‡Šï¼Œä»¥ä¾¿äºè§£é‡Šã€‚
- en: PCA via SVD
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡SVDè¿›è¡ŒPCA
- en: 'PCA can equivalently be performed via eigendecomposition as previously described
    or via SVD. There are two ways to perform a PCA using SVD:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PCAå¯ä»¥é€šè¿‡å‰è¿°çš„ç‰¹å¾å€¼åˆ†è§£æˆ–é€šè¿‡SVDç­‰æ•ˆåœ°è¿›è¡Œã€‚ä½¿ç”¨SVDæ‰§è¡ŒPCAæœ‰ä¸¤ç§æ–¹æ³•ï¼š
- en: Take the SVD of the covariance matrix. The procedure is identical to that previously
    described, because SVD and eigendecomposition are the same decomposition for covariance
    matrices.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡ŒSVDã€‚è¯¥è¿‡ç¨‹ä¸å…ˆå‰æè¿°çš„ç›¸åŒï¼Œå› ä¸ºSVDå’Œç‰¹å¾å€¼åˆ†è§£æ˜¯åæ–¹å·®çŸ©é˜µçš„ç›¸åŒåˆ†è§£æ–¹æ³•ã€‚
- en: Take the SVD of the data matrix directly. In this case, the right singular vectors
    (matrix <math alttext="bold upper V"><mi>ğ•</mi></math> ) are equivalent to the
    eigenvectors of the covariance matrix (it would be the left singular vectors if
    the data matrix is stored as features-by-observations). The data must be mean-centered
    before computing the SVD. The square root of the singular values is equivalent
    to the eigenvalues of the covariance matrix.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›´æ¥å¯¹æ•°æ®çŸ©é˜µè¿›è¡ŒSVDã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå³å¥‡å¼‚å‘é‡ï¼ˆçŸ©é˜µ <math alttext="bold upper V"><mi>ğ•</mi></math>
    ï¼‰ç­‰ä»·äºåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼ˆå¦‚æœæ•°æ®çŸ©é˜µæŒ‰ç‰¹å¾-è§‚å¯Ÿå­˜å‚¨ï¼Œåˆ™å·¦å¥‡å¼‚å‘é‡ï¼‰ã€‚åœ¨è®¡ç®—SVDä¹‹å‰ï¼Œæ•°æ®å¿…é¡»è¿›è¡Œå‡å€¼ä¸­å¿ƒåŒ–ã€‚å¥‡å¼‚å€¼çš„å¹³æ–¹æ ¹ç­‰ä»·äºåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ã€‚
- en: Should you use eigendecomposition or SVD to perform a PCA? You might think that
    SVD is easier because it does not require the covariance matrix. Thatâ€™s true for
    relatively small and clean datasets. But larger or more complicated datasets may
    require data selection or may be too memory intensive to take the SVD of the entire
    data matrix. In these cases, computing the covariance matrix first can increase
    analysis flexibility. But the choice of eigendecomposition versus SVD is often
    a matter of personal preference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡ŒPCAæ—¶ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ç‰¹å¾å€¼åˆ†è§£è¿˜æ˜¯SVDï¼Ÿæ‚¨å¯èƒ½è®¤ä¸ºSVDæ›´å®¹æ˜“ï¼Œå› ä¸ºå®ƒä¸éœ€è¦åæ–¹å·®çŸ©é˜µã€‚å¯¹äºç›¸å¯¹è¾ƒå°ä¸”å¹²å‡€çš„æ•°æ®é›†ï¼Œè¿™æ˜¯æ­£ç¡®çš„ã€‚ä½†å¯¹äºæ›´å¤§æˆ–æ›´å¤æ‚çš„æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦æ•°æ®é€‰æ‹©ï¼Œæˆ–è€…å¯èƒ½å› ä¸ºå†…å­˜éœ€æ±‚è¿‡é«˜è€Œæ— æ³•ç›´æ¥å¯¹æ•´ä¸ªæ•°æ®çŸ©é˜µè¿›è¡ŒSVDã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå…ˆè®¡ç®—åæ–¹å·®çŸ©é˜µå¯ä»¥å¢åŠ åˆ†æçš„çµæ´»æ€§ã€‚ä½†æ˜¯é€‰æ‹©ç‰¹å¾å€¼åˆ†è§£è¿˜æ˜¯SVDé€šå¸¸æ˜¯ä¸ªäººå–œå¥½çš„é—®é¢˜ã€‚
- en: Linear Discriminant Analysis
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çº¿æ€§åˆ¤åˆ«åˆ†æ
- en: Linear discriminant analysis (LDA) is a multivariate classification technique
    that is often used in machine learning and statistics. It was initially developed
    by Ronald Fisher,^([3](ch15.xhtml#idm45733290633904)) who is often considered
    the â€œgrandfatherâ€ of statistics for his numerous and important contributions to
    the mathematical foundations of statistics.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰æ˜¯ä¸€ç§å¸¸ç”¨äºæœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å­¦ä¸­çš„å¤šå˜é‡åˆ†ç±»æŠ€æœ¯ã€‚æœ€åˆç”±ç½—çº³å¾·Â·è´¹èˆå°”å¼€å‘ï¼Œ^([3](ch15.xhtml#idm45733290633904))ä»–å› å…¶å¯¹ç»Ÿè®¡å­¦æ•°å­¦åŸºç¡€çš„ä¼—å¤šé‡è¦è´¡çŒ®è€Œå¸¸è¢«ç§°ä¸ºç»Ÿè®¡å­¦çš„â€œç¥–çˆ¶â€ã€‚
- en: The goal of LDA is to find a direction in the data space that maximally separates
    categories of data. An example problem dataset is shown in graph A in [FigureÂ 15-2](#fig_15_2).
    It is visually obvious that the two categories are separable, but they are not
    separable on either of the data axes aloneâ€”that is clear from visual inspection
    of the marginal distributions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LDAçš„ç›®æ ‡æ˜¯åœ¨æ•°æ®ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªæ–¹å‘ï¼Œæœ€å¤§åŒ–åœ°åˆ†ç¦»æ•°æ®çš„ç±»åˆ«ã€‚å›¾Aä¸­å±•ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹é—®é¢˜æ•°æ®é›†ï¼Œ[å›¾Â 15-2](#fig_15_2)ã€‚ä»è§†è§‰ä¸Šçœ‹ï¼Œä¸¤ä¸ªç±»åˆ«æ˜¯å¯åˆ†ç¦»çš„ï¼Œä½†åœ¨ä»»ä½•å•ä¸ªæ•°æ®è½´ä¸Šå®ƒä»¬éƒ½ä¸å¯åˆ†ç¦»â€”è¿™ä»è¾¹é™…åˆ†å¸ƒçš„è§†è§‰æ£€æŸ¥ä¸­æ˜¾è€Œæ˜“è§ã€‚
- en: Enter LDA. LDA will find basis vectors in the data space that maximally separate
    the two categories. Graph B in [FigureÂ 15-2](#fig_15_2) shows the same data but
    in the LDA space. Now the classification is simpleâ€”observations with negative
    values on axis-1 are labeled category â€œ0â€ and any observations with positive values
    on axis 1 are labeled category â€œ1.â€ The data is completely inseparable on axis
    2, indicating that one dimension is sufficient for accurate categorization in
    this dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›å…¥LDAã€‚LDAå°†åœ¨æ•°æ®ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ç»„åŸºå‘é‡ï¼Œä½¿å¾—ä¸¤ä¸ªç±»åˆ«èƒ½å¤Ÿæœ€å¤§åŒ–åˆ†ç¦»ã€‚å›¾Båœ¨[LDAç©ºé—´çš„å›¾ç¤º](#fig_15_2)ä¸­å±•ç¤ºäº†ç›¸åŒçš„æ•°æ®ã€‚ç°åœ¨åˆ†ç±»å¾ˆç®€å•â€”â€”åœ¨è½´-1ä¸Šå…·æœ‰è´Ÿå€¼çš„è§‚æµ‹æ ‡è®°ä¸ºç±»åˆ«â€œ0â€ï¼Œè€Œåœ¨è½´1ä¸Šå…·æœ‰æ­£å€¼çš„è§‚æµ‹æ ‡è®°ä¸ºç±»åˆ«â€œ1â€ã€‚åœ¨è½´2ä¸Šï¼Œæ•°æ®å®Œå…¨æ— æ³•åˆ†ç¦»ï¼Œè¿™è¡¨æ˜åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œä¸€ä¸ªç»´åº¦è¶³ä»¥å®ç°å‡†ç¡®çš„åˆ†ç±»ã€‚
- en: '![LDA](assets/plad_1502.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![LDA](assets/plad_1502.png)'
- en: Figure 15-2\. Example 2D problem for LDA
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾15-2\. LDAçš„äºŒç»´é—®é¢˜ç¤ºä¾‹
- en: Sounds great, right? But how does such a marvel of mathematics work? Itâ€™s actually
    fairly straightforward and based on generalized eigendecomposition, which you
    learned about toward the end of [ChapterÂ 13](ch13.xhtml#Chapter_13).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥ä¸é”™ï¼Œå¯¹å§ï¼Ÿä½†è¿™æ ·ä¸€ç§æ•°å­¦å¥‡è¿¹æ˜¯å¦‚ä½•è¿ä½œçš„å‘¢ï¼Ÿäº‹å®ä¸Šï¼Œå®ƒéå¸¸ç›´æ¥ï¼ŒåŸºäºå¹¿ä¹‰ç‰¹å¾åˆ†è§£ï¼Œä½ åœ¨[ç¬¬13ç« ](ch13.xhtml#Chapter_13)æœ«å°¾å­¦ä¹ è¿‡è¿™ä¸ªæ–¹æ³•ã€‚
- en: 'Let me begin with the objective function: our goal is to find a set of weights
    such that the weighted combination of variables maximally separates the categories.
    That objective function can be written similarly as with the PCA objective function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»ç›®æ ‡å‡½æ•°å¼€å§‹ï¼šæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æƒé‡ï¼Œä½¿å¾—å˜é‡çš„åŠ æƒç»„åˆèƒ½å¤Ÿæœ€å¤§åŒ–åœ°å°†ç±»åˆ«åˆ†å¼€ã€‚è¯¥ç›®æ ‡å‡½æ•°å¯ä»¥ç±»ä¼¼äºPCAçš„ç›®æ ‡å‡½æ•°è¿›è¡Œè¡¨è¾¾ï¼š
- en: <math alttext="lamda equals StartFraction parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to Over parallel-to bold upper X Subscript upper
    W Baseline bold w parallel-to EndFraction" display="block"><mrow><mi>Î»</mi> <mo>=</mo>
    <mfrac><mrow><mrow><mo>âˆ¥</mo></mrow><msub><mi>ğ—</mi> <mi>B</mi></msub> <msup><mrow><mi>ğ°</mi><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><mrow><mo>âˆ¥</mo></mrow><msub><mi>ğ—</mi> <mi>W</mi></msub>
    <msup><mrow><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mfrac></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda equals StartFraction parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to Over parallel-to bold upper X Subscript upper
    W Baseline bold w parallel-to EndFraction" display="block"><mrow><mi>Î»</mi> <mo>=</mo>
    <mfrac><mrow><mrow><mo>âˆ¥</mo></mrow><msub><mi>ğ—</mi> <mi>B</mi></msub> <msup><mrow><mi>ğ°</mi><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><mrow><mo>âˆ¥</mo></mrow><msub><mi>ğ—</mi> <mi>W</mi></msub>
    <msup><mrow><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mfrac></mrow></math>
- en: 'In English: we want to find a set of feature weights <math alttext="bold w"><mi>ğ°</mi></math>
    that maximizes the *ratio* of the variance of data feature <math alttext="bold
    upper X Subscript upper B"><msub><mi>ğ—</mi> <mi>B</mi></msub></math> , to the
    variance of data feature <math alttext="bold upper X Subscript upper W"><msub><mi>ğ—</mi>
    <mi>W</mi></msub></math> . Notice that the same weights are applied to all data
    observations. (Iâ€™ll write more about data features *B* and *W* after discussing
    the math.)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨ç®€å•çš„è‹±è¯­è¯´ï¼Œæˆ‘ä»¬æƒ³æ‰¾åˆ°ä¸€ç»„ç‰¹å¾æƒé‡<math alttext="bold w"><mi>ğ°</mi></math>ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®ç‰¹å¾<math alttext="bold
    upper X Subscript upper B"><msub><mi>ğ—</mi> <mi>B</mi></msub></math>çš„æ–¹å·®ä¸æ•°æ®ç‰¹å¾<math
    alttext="bold upper X Subscript upper W"><msub><mi>ğ—</mi> <mi>W</mi></msub></math>çš„æ–¹å·®çš„*æ¯”ç‡*ã€‚æ³¨æ„ï¼Œè¿™äº›æƒé‡é€‚ç”¨äºæ‰€æœ‰æ•°æ®è§‚æµ‹ã€‚ï¼ˆåœ¨è®¨è®ºå®Œæ•°å­¦åï¼Œæˆ‘ä¼šè¯¦ç»†è®¨è®ºæ•°æ®ç‰¹å¾*B*å’Œ*W*ã€‚ï¼‰
- en: 'The linear algebra solution comes from following a similar argument as described
    in the PCA section. First, expand <math alttext="parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to"><mrow><mrow><mo>âˆ¥</mo></mrow> <msub><mi>ğ—</mi>
    <mi>B</mi></msub> <msup><mrow><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></math>
    to <math alttext="bold w Superscript upper T Baseline bold upper X Subscript upper
    B Superscript upper T Baseline bold upper X Subscript upper B Baseline bold w"><mrow><msup><mi>ğ°</mi>
    <mtext>T</mtext></msup> <msubsup><mi>ğ—</mi> <mi>B</mi> <mtext>T</mtext></msubsup>
    <msub><mi>ğ—</mi> <mi>B</mi></msub> <mi>ğ°</mi></mrow></math> and express this as
    <math alttext="bold w Superscript upper T Baseline bold upper C Subscript upper
    B Baseline bold w"><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <msub><mi>ğ‚</mi>
    <mi>B</mi></msub> <mi>ğ°</mi></mrow></math> ; second, consider a set of solutions
    instead of one solution; third, replace the division with multiplication of the
    inverse; and finally, do some algebra and see what happens:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§ä»£æ•°çš„è§£å†³æ–¹æ¡ˆæºäºä¸PCAéƒ¨åˆ†æè¿°ç±»ä¼¼çš„è®ºè¯ã€‚é¦–å…ˆï¼Œå°†<math alttext="parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to"><mrow><mrow><mo>âˆ¥</mo></mrow> <msub><mi>ğ—</mi>
    <mi>B</mi></msub> <msup><mrow><mi>ğ°</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></math>æ‰©å±•ä¸º<math
    alttext="bold w Superscript upper T Baseline bold upper X Subscript upper B Superscript
    upper T Baseline bold upper X Subscript upper B Baseline bold w"><mrow><msup><mi>ğ°</mi>
    <mtext>T</mtext></msup> <msubsup><mi>ğ—</mi> <mi>B</mi> <mtext>T</mtext></msubsup>
    <msub><mi>ğ—</mi> <mi>B</mi></msub> <mi>ğ°</mi></mrow></math>ï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸º<math alttext="bold
    w Superscript upper T Baseline bold upper C Subscript upper B Baseline bold w"><mrow><msup><mi>ğ°</mi>
    <mtext>T</mtext></msup> <msub><mi>ğ‚</mi> <mi>B</mi></msub> <mi>ğ°</mi></mrow></math>ï¼›å…¶æ¬¡ï¼Œè€ƒè™‘ä¸€ç»„è§£è€Œä¸æ˜¯ä¸€ä¸ªè§£ï¼›ç¬¬ä¸‰ï¼Œå°†é™¤æ³•æ›¿æ¢ä¸ºé€†çš„ä¹˜æ³•ï¼›æœ€åï¼Œè¿›è¡Œä¸€äº›ä»£æ•°è¿ç®—å¹¶è§‚å¯Ÿç»“æœï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper C Subscript
    upper W Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C Subscript upper B Baseline
    bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold upper
    W Superscript negative 1 Baseline bold upper C Subscript upper W Superscript negative
    1 Baseline bold upper W Superscript minus upper T Baseline bold upper W Superscript
    upper T Baseline bold upper C Subscript upper B Baseline bold upper W 3rd Row
    1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C Subscript upper W Superscript negative 1 Baseline bold
    upper C Subscript upper B Baseline bold upper W 4th Row 1st Column bold upper
    W bold upper Lamda 2nd Column equals bold upper C Subscript upper W Superscript
    negative 1 Baseline bold upper C Subscript upper B Baseline bold upper W 5th Row
    1st Column bold upper C Subscript upper W Baseline bold upper W bold upper Lamda
    2nd Column equals bold upper C Subscript upper B Baseline bold upper W EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>W</mi></msub> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mtext>T</mtext></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Î›</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi mathvariant="bold">C</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi
    mathvariant="bold">C</mi> <mi>W</mi></msub> <mi mathvariant="bold">W</mi> <mi
    mathvariant="bold">Î›</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper C Subscript
    upper W Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C Subscript upper B Baseline
    bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold upper
    W Superscript negative 1 Baseline bold upper C Subscript upper W Superscript negative
    1 Baseline bold upper W Superscript minus upper T Baseline bold upper W Superscript
    upper T Baseline bold upper C Subscript upper B Baseline bold upper W 3rd Row
    1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C Subscript upper W Superscript negative 1 Baseline bold
    upper C Subscript upper B Baseline bold upper W 4th Row 1st Column bold upper
    W bold upper Lamda 2nd Column equals bold upper C Subscript upper W Superscript
    negative 1 Baseline bold upper C Subscript upper B Baseline bold upper W 5th Row
    1st Column bold upper C Subscript upper W Baseline bold upper W bold upper Lamda
    2nd Column equals bold upper C Subscript upper B Baseline bold upper W EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>W</mi></msub> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mtext>T</mtext></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Î›</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi mathvariant="bold">C</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi
    mathvariant="bold">C</mi> <mi>W</mi></msub> <mi mathvariant="bold">W</mi> <mi
    mathvariant="bold">Î›</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
- en: In other words, the solution to LDA comes from a generalized eigendecomposition
    on two covariance matrices. The eigenvectors are the weights, and the generalized
    eigenvalues are the variance ratios of each component.^([4](ch15.xhtml#idm45733290531744))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼ŒLDAçš„è§£å†³æ–¹æ¡ˆæ¥è‡ªäºå¯¹ä¸¤ä¸ªåæ–¹å·®çŸ©é˜µè¿›è¡Œå¹¿ä¹‰ç‰¹å¾åˆ†è§£ã€‚ç‰¹å¾å‘é‡æ˜¯æƒé‡ï¼Œå¹¿ä¹‰ç‰¹å¾å€¼æ˜¯æ¯ä¸ªåˆ†é‡çš„æ–¹å·®æ¯”ç‡ã€‚^([4](ch15.xhtml#idm45733290531744))
- en: With the math out of the way, which data features are used to construct <math
    alttext="bold upper X Subscript upper B"><msub><mi>ğ—</mi> <mi>B</mi></msub></math>
    and <math alttext="bold upper X Subscript upper W"><msub><mi>ğ—</mi> <mi>W</mi></msub></math>
    ? Well, there are different ways of implementing that formula, depending on the
    nature of the problem and the specific goal of the analysis. But in a typical
    LDA model, the <math alttext="bold upper X Subscript upper B"><msub><mi>ğ—</mi>
    <mi>B</mi></msub></math> comes from the between-category covariance while the
    <math alttext="bold upper X Subscript upper W"><msub><mi>ğ—</mi> <mi>W</mi></msub></math>
    comes from the within-category covariance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ•°å­¦éƒ¨åˆ†å·²ç»æå®šï¼Œç”¨æ¥æ„å»º<math alttext="bold upper X Subscript upper B"><msub><mi>ğ—</mi>
    <mi>B</mi></msub></math>å’Œ<math alttext="bold upper X Subscript upper W"><msub><mi>ğ—</mi>
    <mi>W</mi></msub></math>çš„æ•°æ®ç‰¹å¾æ˜¯ä»€ä¹ˆï¼Ÿå—¯ï¼Œæ ¹æ®é—®é¢˜çš„æ€§è´¨å’Œåˆ†æçš„å…·ä½“ç›®æ ‡ï¼Œå®æ–½è¯¥å…¬å¼æœ‰ä¸åŒçš„æ–¹å¼ã€‚ä½†åœ¨å…¸å‹çš„LDAæ¨¡å‹ä¸­ï¼Œ<math
    alttext="bold upper X Subscript upper B"><msub><mi>ğ—</mi> <mi>B</mi></msub></math>æ¥è‡ªäºç±»é—´åæ–¹å·®ï¼Œè€Œ<math
    alttext="bold upper X Subscript upper W"><msub><mi>ğ—</mi> <mi>W</mi></msub></math>æ¥è‡ªäºç±»å†…åæ–¹å·®ã€‚
- en: The within-category covariance is simply the average of the covariances of the
    data samples within each class. The between-category covariance comes from creating
    a new data matrix comprising the feature averages within each class. I will walk
    you through the procedure in the exercises. If you are familiar with statistics,
    then youâ€™ll recognize this formulation as analogous to the ratio of between-group
    to within-group sum of squared errors in ANOVA models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç±»å†…åæ–¹å·®ç®€å•æ¥è¯´æ˜¯æ•°æ®æ ·æœ¬åœ¨æ¯ä¸ªç±»åˆ«å†…åæ–¹å·®çš„å¹³å‡å€¼ã€‚ç±»é—´åæ–¹å·®æ¥è‡ªäºåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®çŸ©é˜µï¼Œå…¶ä¸­åŒ…æ‹¬æ¯ä¸ªç±»åˆ«å†…çš„ç‰¹å¾å¹³å‡å€¼ã€‚æˆ‘å°†åœ¨ç»ƒä¹ ä¸­ä¸ºæ‚¨è®²è§£è¿™ä¸ªè¿‡ç¨‹ã€‚å¦‚æœæ‚¨ç†Ÿæ‚‰ç»Ÿè®¡å­¦ï¼Œé‚£ä¹ˆæ‚¨ä¼šè®¤è¯†åˆ°è¿™ç§è¡¨è¿°ç±»ä¼¼äºANOVAæ¨¡å‹ä¸­ç»„é—´å¹³æ–¹è¯¯å·®ä¸ç»„å†…å¹³æ–¹è¯¯å·®æ¯”ç‡çš„å½¢å¼ã€‚
- en: 'Two final comments: The eigenvectors of a generalized eigendecomposition are
    not constrained to be orthogonal. Thatâ€™s because <math alttext="bold upper C Subscript
    upper W Superscript negative 1 Baseline bold upper C Subscript upper B"><mrow><msubsup><mi>ğ‚</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi>ğ‚</mi> <mi>B</mi></msub></mrow></math>
    is generally not a symmetric matrix even though the two covariance matrices are
    themselves symmetric. Nonsymmetric matrices do not have the orthogonal-eigenvector
    constraint. Youâ€™ll see this in the exercises.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸¤ç‚¹è¯´æ˜ï¼šå¹¿ä¹‰ç‰¹å¾åˆ†è§£çš„ç‰¹å¾å‘é‡ä¸å—å¼ºåˆ¶è¦æ±‚æ˜¯æ­£äº¤çš„ã€‚è¿™æ˜¯å› ä¸º<math alttext="bold upper C Subscript upper
    W Superscript negative 1 Baseline bold upper C Subscript upper B"><mrow><msubsup><mi>ğ‚</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi>ğ‚</mi> <mi>B</mi></msub></mrow></math>é€šå¸¸ä¸æ˜¯å¯¹ç§°çŸ©é˜µï¼Œå°½ç®¡ä¸¤ä¸ªåæ–¹å·®çŸ©é˜µæœ¬èº«æ˜¯å¯¹ç§°çš„ã€‚éå¯¹ç§°çŸ©é˜µæ²¡æœ‰æ­£äº¤ç‰¹å¾å‘é‡çš„çº¦æŸã€‚æ‚¨å°†åœ¨ç»ƒä¹ ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
- en: Finally, LDA will always find a *linear* solution (duh, thatâ€™s right in the
    name *L*DA), even if the data is *not* linearly separable. Nonlinear separation
    would require a transformation of the data or the use of a nonlinear categorization
    method like artificial neural networks. LDA will still work in the sense of producing
    a result; itâ€™s up to you as the data scientist to determine whether that result
    is appropriate and interpretable for a given problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒLDAï¼ˆçº¿æ€§åˆ¤åˆ«åˆ†æï¼‰å°†å§‹ç»ˆæ‰¾åˆ°ä¸€ä¸ª*çº¿æ€§*è§£å†³æ–¹æ¡ˆï¼ˆå½“ç„¶ï¼Œè¿™åœ¨*L*DAçš„åç§°ä¸­å°±å·²ç»æ˜ç¡®äº†ï¼‰ï¼Œå³ä½¿æ•°æ®*ä¸*æ˜¯çº¿æ€§å¯åˆ†çš„ã€‚éçº¿æ€§åˆ†ç¦»å°†éœ€è¦å¯¹æ•°æ®è¿›è¡Œå˜æ¢æˆ–ä½¿ç”¨åƒäººå·¥ç¥ç»ç½‘ç»œè¿™æ ·çš„éçº¿æ€§åˆ†ç±»æ–¹æ³•ã€‚LDAåœ¨äº§ç”Ÿç»“æœæ–¹é¢ä»ç„¶æœ‰æ•ˆï¼›ä½œä¸ºæ•°æ®ç§‘å­¦å®¶ï¼Œæ‚¨éœ€è¦ç¡®å®šè¯¥ç»“æœæ˜¯å¦é€‚åˆå’Œå¯è§£é‡Šäºç»™å®šé—®é¢˜ã€‚
- en: Low-Rank Approximations via SVD
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡SVDè¿›è¡Œä½ç§©é€¼è¿‘
- en: I explained the concept of low-rank approximations in the previous chapter (e.g.,
    [Exercise 14-5](ch14.xhtml#exercise_14_5)). The idea is to take the SVD of a data
    matrix or image, and then reconstruct that data matrix using some subset of SVD
    components.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ä¸Šä¸€ç« è§£é‡Šäº†ä½ç§©é€¼è¿‘çš„æ¦‚å¿µï¼ˆä¾‹å¦‚ï¼Œ[ç»ƒä¹  14-5](ch14.xhtml#exercise_14_5)ï¼‰ã€‚å…¶æ€æƒ³æ˜¯å¯¹æ•°æ®çŸ©é˜µæˆ–å›¾åƒè¿›è¡ŒSVDåˆ†è§£ï¼Œç„¶åä½¿ç”¨SVDåˆ†é‡çš„æŸä¸ªå­é›†é‡æ„è¯¥æ•°æ®çŸ©é˜µã€‚
- en: You can achieve this by setting selected <math alttext="sigma"><mi>Ïƒ</mi></math>
    s to equal zero or by creating new SVD matrices that are rectangular, with the
    to-be-rejected vectors and singular values removed. This second approach is preferred
    because it reduces the sizes of the data to be stored, as you will see in the
    exercises. In this way, the SVD can be used to compress data down to a smaller
    size.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡å°†é€‰å®šçš„<math alttext="sigma"><mi>Ïƒ</mi></math>è®¾ç½®ä¸ºé›¶ï¼Œæˆ–åˆ›å»ºæ–°çš„SVDçŸ©é˜µï¼ˆè¿™äº›çŸ©é˜µæ˜¯çŸ©å½¢çš„ï¼Œç§»é™¤è¦è¢«æ‹’ç»çš„å‘é‡å’Œå¥‡å¼‚å€¼ï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç¬¬äºŒç§æ–¹æ³•æ›´ä¸ºæ¨èï¼Œå› ä¸ºå®ƒå‡å°‘äº†è¦å­˜å‚¨çš„æ•°æ®å¤§å°ï¼Œæ‚¨å°†åœ¨ç»ƒä¹ ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSVDå¯ä»¥ç”¨äºå°†æ•°æ®å‹ç¼©åˆ°è¾ƒå°çš„å¤§å°ã€‚
- en: SVD for Denoising
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVDç”¨äºå»å™ª
- en: Denoising via SVD is simply an application of low-rank approximation. The only
    difference is that SVD components are selected for exclusion based on them representing
    noise as opposed to making small contributions to the data matrix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡SVDè¿›è¡Œå»å™ªä»…ä»…æ˜¯ä½ç§©é€¼è¿‘çš„ä¸€ä¸ªåº”ç”¨ã€‚å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼ŒSVDçš„ç»„æˆéƒ¨åˆ†è¢«é€‰æ‹©æ’é™¤ï¼Œå› ä¸ºå®ƒä»¬ä»£è¡¨å™ªå£°ï¼Œè€Œä¸æ˜¯å¯¹æ•°æ®çŸ©é˜µä½œå‡ºå°è´¡çŒ®ã€‚
- en: The to-be-removed components might be layers associated with the smallest singular
    valuesâ€”that would be the case for low-amplitude noise associated with small equipment
    imperfections. But larger sources of noise that have a stronger impact on the
    data might have larger singular values. These noise components can be identified
    by an algorithm based on their characteristics or by visual inspection. In the
    exercises, you will see an example of using SVD to separate a source of noise
    that was added to an image.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¾…ç§»é™¤çš„ç»„ä»¶å¯èƒ½æ˜¯ä¸æœ€å°å¥‡å¼‚å€¼ç›¸å…³è”çš„å±‚â€”è¿™åœ¨ä¸å°å‹è®¾å¤‡ä¸å®Œç¾ç›¸å…³çš„ä½æŒ¯å¹…å™ªå£°çš„æƒ…å†µä¸‹æ˜¯è¿™æ ·ã€‚ä½†å¯¹æ•°æ®å½±å“æ›´å¤§çš„è¾ƒå¤§å™ªå£°æºå¯èƒ½å…·æœ‰è¾ƒå¤§çš„å¥‡å¼‚å€¼ã€‚è¿™äº›å™ªå£°ç»„ä»¶å¯ä»¥é€šè¿‡åŸºäºå®ƒä»¬ç‰¹å¾çš„ç®—æ³•æˆ–è§†è§‰æ£€æŸ¥æ¥è¯†åˆ«ã€‚åœ¨ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°ä½¿ç”¨SVDåˆ†ç¦»æ·»åŠ åˆ°å›¾åƒä¸­çš„å™ªå£°æºçš„ç¤ºä¾‹ã€‚
- en: Summary
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: Youâ€™ve made it to the end of the book (except for the exercises below)! Congrats!
    Take a moment to be proud of yourself and your commitment to learning and investing
    in your brain (it is, after all, your most precious resource). I am proud of you,
    and if we would meet in person, Iâ€™d give you a high five, fist bump, elbow tap,
    or whatever is socially/medically appropriate at the time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ï¼ä½ å·²ç»è¯»å®Œäº†æœ¬ä¹¦çš„å†…å®¹ï¼ˆé™¤äº†ä¸‹é¢çš„ç»ƒä¹ ï¼‰ï¼è¯·èŠ±ç‚¹æ—¶é—´ä¸ºè‡ªå·±å’Œä½ å¯¹å­¦ä¹ å’ŒæŠ•èµ„å¤§è„‘çš„æ‰¿è¯ºæ„Ÿåˆ°è‡ªè±ªï¼ˆæ¯•ç«Ÿï¼Œè¿™æ˜¯ä½ æœ€å®è´µçš„èµ„æºï¼‰ã€‚æˆ‘ä¸ºä½ æ„Ÿåˆ°éª„å‚²ï¼Œå¦‚æœæˆ‘ä»¬èƒ½è§é¢ï¼Œæˆ‘ä¼šå’Œä½ å‡»æŒã€æ‹³å¤´ç¢°ã€è‚˜éƒ¨ç¢°æˆ–è€…åœ¨å½“æ—¶ç¤¾ä¼š/åŒ»å­¦ä¸Šåˆé€‚çš„æ–¹å¼è¡¨ç¤ºç¥è´ºã€‚
- en: 'I hope you feel that this chapter helped you see the incredible importance
    of eigendecomposition and singular value decomposition to applications in statistics
    and machine learning. Hereâ€™s a summary of the key points that I am contractually
    obligated to include:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ è§‰å¾—æœ¬ç« å¸®åŠ©ä½ çœ‹åˆ°ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£åœ¨ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚è¿™é‡Œæ˜¯æˆ‘å¿…é¡»åŒ…æ‹¬çš„å…³é”®ç‚¹æ€»ç»“ï¼š
- en: The goal of PCA is to find a set of weights such that the linear weighted combination
    of data features has maximal variance. That goal reflects the assumption underlying
    PCA, which is that â€œvariance equals relevance.â€
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCAçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æƒé‡ï¼Œä½¿å¾—æ•°æ®ç‰¹å¾çš„çº¿æ€§åŠ æƒç»„åˆå…·æœ‰æœ€å¤§æ–¹å·®ã€‚è¿™ä¸ªç›®æ ‡åæ˜ äº†PCAçš„åŸºæœ¬å‡è®¾ï¼Œâ€œæ–¹å·®ç­‰äºç›¸å…³æ€§â€ã€‚
- en: PCA is implemented as the eigendecomposition of a data covariance matrix. The
    eigenvectors are the feature weightings, and the eigenvalues can be scaled to
    encode percent variance accounted for by each component (a *component* is the
    linear weighted combination).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCAä½œä¸ºæ•°æ®åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾åˆ†è§£å®ç°ã€‚ç‰¹å¾å‘é‡æ˜¯ç‰¹å¾æƒé‡ï¼Œè€Œç‰¹å¾å€¼å¯ä»¥ç¼©æ”¾ä»¥ç¼–ç æ¯ä¸ªç»„ä»¶æ‰€å çš„ç™¾åˆ†æ¯”æ–¹å·®ï¼ˆä¸€ä¸ª*ç»„ä»¶*æ˜¯çº¿æ€§åŠ æƒç»„åˆï¼‰ã€‚
- en: PCA can be equivalently implemented using the SVD of the covariance matrix or
    the data matrix.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCAå¯ä»¥ç­‰æ•ˆåœ°ä½¿ç”¨åæ–¹å·®çŸ©é˜µæˆ–æ•°æ®çŸ©é˜µçš„SVDå®ç°ã€‚
- en: 'Linear discriminant analysis (LDA) is used for linear categorization of multivariable
    data. It can be seen as an extension of PCA: whereas PCA maximizes variance, LDA
    maximizes the ratio of variances between two data features.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ç”¨äºå¤šå˜é‡æ•°æ®çš„çº¿æ€§åˆ†ç±»ã€‚å¯ä»¥çœ‹ä½œæ˜¯PCAçš„æ‰©å±•ï¼šPCAæœ€å¤§åŒ–æ–¹å·®ï¼Œè€ŒLDAæœ€å¤§åŒ–ä¸¤ä¸ªæ•°æ®ç‰¹å¾ä¹‹é—´çš„æ–¹å·®æ¯”ã€‚
- en: LDA is implemented as a generalized eigendecomposition on two covariance matrices
    that are formed from two different data features. The two data features are often
    the between-class covariance (to maximize) and the within-class covariance (to
    minimize).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDAä½œä¸ºä¸¤ä¸ªåæ–¹å·®çŸ©é˜µçš„å¹¿ä¹‰ç‰¹å¾åˆ†è§£å®ç°ï¼Œè¿™äº›çŸ©é˜µç”±ä¸¤ä¸ªä¸åŒçš„æ•°æ®ç‰¹å¾å½¢æˆã€‚è¿™ä¸¤ä¸ªæ•°æ®ç‰¹å¾é€šå¸¸æ˜¯ç±»é—´åæ–¹å·®ï¼ˆè¦æœ€å¤§åŒ–ï¼‰å’Œç±»å†…åæ–¹å·®ï¼ˆè¦æœ€å°åŒ–ï¼‰ã€‚
- en: Low-rank approximations involve reproducing a matrix from a subset of singular
    vectors/values and are used for data compression and denoising.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ç§©é€¼è¿‘æ¶‰åŠä»å¥‡å¼‚å‘é‡/å€¼çš„å­é›†ä¸­å¤åˆ¶çŸ©é˜µï¼Œå¹¶ç”¨äºæ•°æ®å‹ç¼©å’Œå»å™ªã€‚
- en: For data compression, the components associated with the smallest singular values
    are removed; for data denoising, components that capture noise or artifacts are
    removed (their corresponding singular values could be small or large).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®å‹ç¼©ï¼Œä¸æœ€å°å¥‡å¼‚å€¼ç›¸å…³è”çš„ç»„ä»¶è¢«ç§»é™¤ï¼›å¯¹äºæ•°æ®å»å™ªï¼Œæ•æ‰å™ªå£°æˆ–ä¼ªå½±çš„ç»„ä»¶è¢«ç§»é™¤ï¼ˆå®ƒä»¬å¯¹åº”çš„å¥‡å¼‚å€¼å¯èƒ½å¾ˆå°æˆ–å¾ˆå¤§ï¼‰ã€‚
- en: Exercises
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 
- en: PCA
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: I love Turkish coffee. Itâ€™s made with very finely ground beans and no filter.
    The whole ritual of making it and drinking it is wonderful. And if you drink it
    with a Turk, perhaps you can have your fortune read.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å–œæ¬¢åœŸè€³å…¶å’–å•¡ã€‚å®ƒç”¨éå¸¸ç»†ç£¨çš„è±†å­åˆ¶æˆï¼Œæ²¡æœ‰è¿‡æ»¤å™¨ã€‚æ•´ä¸ªåˆ¶ä½œå’Œäº«ç”¨è¿‡ç¨‹éƒ½å¾ˆç¾å¦™ã€‚å¦‚æœä½ å’ŒåœŸè€³å…¶äººä¸€èµ·å–ï¼Œä¹Ÿè®¸ä½ å¯ä»¥ç®—ç®—ä½ çš„å‘½è¿ã€‚
- en: This exercise is not about Turkish coffee, but it is about doing a PCA on a
    dataset^([5](ch15.xhtml#idm45733290481824)) that contains time series data from
    the Istanbul stock exchange, along with stock exchange data from several other
    stock indices in different countries. We could use this dataset to ask, for example,
    whether the international stock exchanges are driven by one common factor, or
    whether different countries have independent financial markets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»ƒä¹ ä¸æ˜¯å…³äºåœŸè€³å…¶å’–å•¡ï¼Œè€Œæ˜¯å…³äºå¯¹åŒ…å«æ¥è‡ªä¼Šæ–¯å¦å¸ƒå°”è¯åˆ¸äº¤æ˜“æ‰€çš„æ—¶é—´åºåˆ—æ•°æ®ä»¥åŠæ¥è‡ªå…¶ä»–å‡ ä¸ªå›½å®¶ä¸åŒè‚¡ç¥¨æŒ‡æ•°çš„è‚¡ç¥¨äº¤æ˜“æ•°æ®çš„æ•°æ®é›†è¿›è¡ŒPCA^([5](ch15.xhtml#idm45733290481824))ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†æ¥è¯¢é—®ï¼Œä¾‹å¦‚å›½é™…è‚¡ç¥¨äº¤æ˜“æ˜¯å¦ç”±å…¨çƒç»æµçš„ä¸€ä¸ªå…±åŒå› ç´ é©±åŠ¨ï¼Œæˆ–è€…ä¸åŒå›½å®¶æ˜¯å¦æ‹¥æœ‰ç‹¬ç«‹çš„é‡‘èå¸‚åœºã€‚
- en: Exercise 15-1\.
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-1\.
- en: Before performing a PCA, import and inspect the data. I made several plots of
    the data shown in [FigureÂ 15-3](#fig_15_3); you are welcome to reproduce these
    plots and/or use different methods to explore the data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡ŒPCAä¹‹å‰ï¼Œè¯·å¯¼å…¥å¹¶æ£€æŸ¥æ•°æ®ã€‚æˆ‘å¯¹æ•°æ®è¿›è¡Œäº†å¤šä¸ªç»˜å›¾ï¼Œæ˜¾ç¤ºåœ¨[å›¾Â 15-3](#fig_15_3)ä¸­ï¼›æ¬¢è¿æ‚¨é‡ç°è¿™äº›å›¾è¡¨å’Œ/æˆ–ä½¿ç”¨ä¸åŒæ–¹æ³•æ¢ç´¢æ•°æ®ã€‚
- en: '![exercise 15-1](assets/plad_1503.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![exercise 15-1](assets/plad_1503.png)'
- en: Figure 15-3\. Some investigations of the international stock exchange dataset
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 15-3\. å¯¹å›½é™…è‚¡ç¥¨äº¤æ˜“æ‰€æ•°æ®é›†çš„ä¸€äº›è°ƒæŸ¥
- en: 'Now for the PCA. Implement the PCA using the five steps outlined earlier in
    this chapter. Visualize the results as in [FigureÂ 15-4](#fig_15_4). Use code to
    demonstrate several features of PCA:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿›è¡ŒPCAã€‚æŒ‰ç…§æœ¬ç« å‰é¢æåˆ°çš„äº”ä¸ªæ­¥éª¤å®æ–½PCAã€‚åƒ[å›¾Â 15-4](#fig_15_4)ä¸€æ ·å¯è§†åŒ–ç»“æœã€‚ä½¿ç”¨ä»£ç å±•ç¤ºPCAçš„å‡ ä¸ªç‰¹æ€§ï¼š
- en: 'The variance of the component time series (using `np.var`) equals the eigenvalue
    associated with that component. You can see the results for the first two components
    here:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆåˆ†æ—¶é—´åºåˆ—çš„æ–¹å·®ï¼ˆä½¿ç”¨ `np.var`ï¼‰ç­‰äºä¸è¯¥æˆåˆ†ç›¸å…³è”çš„ç‰¹å¾å€¼ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹å‰ä¸¤ä¸ªæˆåˆ†çš„ç»“æœï¼š
- en: '[PRE0]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The correlation between principal components (that is, the weighted combinations
    of the stock exchanges) 1 and 2 is zero, i.e., orthogonal.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸»æˆåˆ†ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆå³è‚¡ç¥¨äº¤æ˜“æ‰€çš„åŠ æƒç»„åˆï¼‰1å’Œ2ä¸ºé›¶ï¼Œå³æ­£äº¤ã€‚
- en: Visualize the eigenvector weights for the first two components. The weights
    show how much each variable contributes to the component.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–å‰ä¸¤ä¸ªæˆåˆ†çš„ç‰¹å¾å‘é‡æƒé‡ã€‚æƒé‡æ˜¾ç¤ºæ¯ä¸ªå˜é‡å¯¹æˆåˆ†çš„è´¡çŒ®ç¨‹åº¦ã€‚
- en: '![ex0.](assets/plad_1504.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![ex0.](assets/plad_1504.png)'
- en: Figure 15-4\. Results of PCA on the Instanbul Stock Exchange dataset
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 15-4\. ä¼Šæ–¯å¦å¸ƒå°”è¯åˆ¸äº¤æ˜“æ‰€æ•°æ®é›†çš„PCAç»“æœ
- en: '**Discussion:** The scree plot strongly suggests that the international stock
    exchanges are driven by a common factor of the global economy: there is one large
    component that accounts for around 64% of the variance in the data, while the
    other components each account for less than 15% of the variance (in a purely random
    dataset we would expect each component to account for 100/9 = 11% of the variance,
    plus/minus noise).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®ºï¼š** å±é£å›¾å¼ºçƒˆè¡¨æ˜å›½é™…è‚¡ç¥¨äº¤æ˜“æ‰€å—å…¨çƒç»æµçš„ä¸€ä¸ªå…±åŒå› ç´ é©±åŠ¨ï¼šæœ‰ä¸€ä¸ªå¤§æˆåˆ†è§£é‡Šäº†æ•°æ®ä¸­çº¦64%çš„æ–¹å·®ï¼Œè€Œå…¶ä»–æˆåˆ†æ¯ä¸ªéƒ½è§£é‡Šäº†ä¸åˆ°15%çš„æ–¹å·®ï¼ˆåœ¨çº¯éšæœºæ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬é¢„è®¡æ¯ä¸ªæˆåˆ†è§£é‡Š100/9
    = 11%çš„æ–¹å·®ï¼ŒåŠ å‡å™ªå£°ï¼‰ã€‚'
- en: A rigorous evaluation of the statistical significance of these components is
    outside the scope of this book, but based on visual inspection of the scree plot,
    we are not really justified to interpret the components after the first one; it
    appears that most of the variance in this dataset fits neatly into one dimension.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹è¿™äº›æˆåˆ†çš„ç»Ÿè®¡æ˜¾è‘—æ€§è¿›è¡Œä¸¥æ ¼è¯„ä¼°è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ï¼Œä½†åŸºäºå¯¹å±é£å›¾çš„è§†è§‰æ£€æŸ¥ï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½å®Œå…¨æœ‰ç†ç”±è§£é‡Šç¬¬ä¸€ä¸ªæˆåˆ†ä¹‹åçš„æˆåˆ†ï¼›ä¼¼ä¹è¿™ä¸ªæ•°æ®é›†çš„å¤§éƒ¨åˆ†æ–¹å·®éƒ½æ•´é½åœ°é€‚åˆä¸€ä¸ªç»´åº¦ã€‚
- en: From the perspective of dimensionality reduction, we could reduce the entire
    dataset to the component associated with the largest eigenvalue (this is often
    called the *top component*), thereby representing this 9D dataset using a 1D vector.
    Of course, we lose informationâ€”36% of the information in the dataset is removed
    if we focus only on the top componentâ€”but hopefully, the important features of
    the signal are in the top component while the less important features, including
    random noise, are ignored.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é™ç»´çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ•´ä¸ªæ•°æ®é›†å‡å°‘åˆ°ä¸æœ€å¤§ç‰¹å¾å€¼ç›¸å…³è”çš„åˆ†é‡ï¼ˆé€šå¸¸ç§°ä¸º*é¡¶éƒ¨åˆ†é‡*ï¼‰ï¼Œä»è€Œä½¿ç”¨1Då‘é‡è¡¨ç¤ºè¿™ä¸ª9Dæ•°æ®é›†ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä¼šå¤±å»ä¿¡æ¯â€”â€”å¦‚æœæˆ‘ä»¬åªå…³æ³¨é¡¶éƒ¨åˆ†é‡ï¼Œæ•°æ®é›†ä¸­36%çš„ä¿¡æ¯ä¼šè¢«ç§»é™¤â€”â€”ä½†å¸Œæœ›ä¿¡å·çš„é‡è¦ç‰¹å¾ä½äºé¡¶éƒ¨åˆ†é‡ä¸­ï¼Œè€Œä¸é‡è¦çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬éšæœºå™ªå£°ï¼Œåˆ™è¢«å¿½ç•¥äº†ã€‚
- en: Exercise 15-2\.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 15-2\.
- en: Reproduce the results using (1) the SVD of the data covariance matrix and (2)
    the SVD of the data matrix itself. Remember that the eigenvalues of <math alttext="bold
    upper X Superscript upper T Baseline bold upper X"><mrow><msup><mi>ğ—</mi> <mtext>T</mtext></msup>
    <mi>ğ—</mi></mrow></math> are the squared singular values of <math alttext="bold
    upper X"><mi>ğ—</mi></math> ; furthermore, the scaling factor on the covariance
    matrix must be applied to the singular values to find equivaluence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ï¼ˆ1ï¼‰æ•°æ®åæ–¹å·®çŸ©é˜µçš„SVDå’Œï¼ˆ2ï¼‰æ•°æ®çŸ©é˜µæœ¬èº«çš„SVDæ¥é‡ç°ç»“æœã€‚è¯·è®°ä½<math alttext="bold upper X Superscript
    upper T Baseline bold upper X"><mrow><msup><mi>ğ—</mi> <mtext>T</mtext></msup>
    <mi>ğ—</mi></mrow></math>çš„ç‰¹å¾å€¼æ˜¯<math alttext="bold upper X"><mi>ğ—</mi></math>çš„å¹³æ–¹å¥‡å¼‚å€¼ï¼›æ­¤å¤–ï¼Œå¿…é¡»å°†åæ–¹å·®çŸ©é˜µä¸Šçš„ç¼©æ”¾å› å­åº”ç”¨äºå¥‡å¼‚å€¼ä»¥æ‰¾åˆ°ç­‰ä»·æ€§ã€‚
- en: Exercise 15-3\.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 15-3\.
- en: 'Compare your â€œmanualâ€ PCA with the output of Pythonâ€™s PCA routine. Youâ€™ll have
    to do some online research to figure out how to run a PCA in Python (this is one
    of the most important skills in Python programming!), but Iâ€™ll give you a hint:
    itâ€™s in the sklearn.decomposition library.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä½ çš„â€œæ‰‹åŠ¨â€PCAä¸Pythonçš„PCAä¾‹ç¨‹çš„è¾“å‡ºè¿›è¡Œæ¯”è¾ƒã€‚ä½ éœ€è¦è¿›è¡Œä¸€äº›åœ¨çº¿ç ”ç©¶ï¼Œä»¥æ‰¾å‡ºå¦‚ä½•åœ¨Pythonä¸­è¿è¡ŒPCAï¼ˆè¿™æ˜¯Pythonç¼–ç¨‹ä¸­æœ€é‡è¦çš„æŠ€èƒ½ä¹‹ä¸€ï¼ï¼‰ï¼Œä½†æˆ‘ä¼šç»™ä½ ä¸€ä¸ªæç¤ºï¼šå®ƒåœ¨sklearn.decompositionåº“ä¸­ã€‚
- en: sklearn or Manual Implementation of PCA?
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: sklearnè¿˜æ˜¯æ‰‹åŠ¨å®ç°PCAï¼Ÿ
- en: Should you compute PCA by writing code to compute and eigendecompose the covariance
    matrix or use sklearnâ€™s implementation? There is always a trade-off between using
    your own code to maximize customization versus using prepackaged code to maximize
    ease. One of the myriad and amazing benefits of understanding the math behind
    data science analyses is that you can custom tailor analyses to suit your needs.
    In my own research, I find that implementing PCA on my own gives me more freedom
    and flexibility.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥é€šè¿‡ç¼–å†™ä»£ç æ¥è®¡ç®—å’Œç‰¹å¾åˆ†è§£åæ–¹å·®çŸ©é˜µæ¥è®¡ç®—PCAï¼Œè¿˜æ˜¯ä½¿ç”¨sklearnçš„å®ç°ï¼Ÿå§‹ç»ˆå­˜åœ¨ä½¿ç”¨è‡ªå·±çš„ä»£ç ä»¥æœ€å¤§ç¨‹åº¦å®šåˆ¶ä¸ä½¿ç”¨é¢„åŒ…è£…ä»£ç ä»¥æœ€å¤§ç¨‹åº¦ä¾¿æ·ä¹‹é—´çš„æƒè¡¡ã€‚ç†è§£æ•°æ®ç§‘å­¦åˆ†æèƒŒåçš„æ•°å­¦çš„æ— æ•°å’Œä»¤äººæƒŠå¥‡çš„å¥½å¤„ä¹‹ä¸€æ˜¯ï¼Œä½ å¯ä»¥å®šåˆ¶åˆ†æä»¥æ»¡è¶³ä½ çš„éœ€æ±‚ã€‚åœ¨æˆ‘çš„ç ”ç©¶ä¸­ï¼Œæˆ‘å‘ç°è‡ªå·±å®ç°PCAèƒ½ç»™æˆ‘æ›´å¤šçš„è‡ªç”±å’Œçµæ´»æ€§ã€‚
- en: Exercise 15-4\.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 15-4\.
- en: Now you will perform a PCA on simulated data, which will highlight one of the
    potential limitations of PCA. The goal is to create a dataset comprising two â€œstreamsâ€
    of data and plot the principal components on top, like in [FigureÂ 15-5](#fig_15_5).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å°†åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šæ‰§è¡ŒPCAï¼Œè¿™å°†çªæ˜¾PCAçš„æ½œåœ¨é™åˆ¶ä¹‹ä¸€ã€‚ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªâ€œæµâ€æ•°æ®çš„æ•°æ®é›†ï¼Œå¹¶åœ¨é¡¶éƒ¨ç»˜åˆ¶ä¸»æˆåˆ†ï¼Œå°±åƒ[å›¾15-5](#fig_15_5)ä¸­ä¸€æ ·ã€‚
- en: '![exercise 15-4](assets/plad_1505.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹ 15-4](assets/plad_1505.png)'
- en: Figure 15-5\. Results from Exercise 15-4
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾15-5\. ç»ƒä¹ 15-4çš„ç»“æœ
- en: 'Hereâ€™s how to create the data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åˆ›å»ºæ•°æ®çš„æ–¹æ³•ï¼š
- en: Create a 1,000 Ã— 2 matrix of random numbers drawn from a normal (Gaussian) distribution
    in which the second column is scaled down by .05.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå¤§å°ä¸º1,000 Ã— 2çš„çŸ©é˜µï¼Œå…¶ä¸­çš„éšæœºæ•°æ˜¯ä»æ­£æ€ï¼ˆé«˜æ–¯ï¼‰åˆ†å¸ƒä¸­æŠ½å–çš„ï¼Œå…¶ä¸­ç¬¬äºŒåˆ—çš„æ•°å€¼ç¼©å°äº†0.05å€ã€‚
- en: Create a 2 Ã— 2 pure rotation matrix (see [ChapterÂ 7](ch07.xhtml#Chapter_7)).
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ª2 Ã— 2çš„çº¯æ—‹è½¬çŸ©é˜µï¼ˆå‚è§[ç¬¬7ç« ](ch07.xhtml#Chapter_7)ï¼‰ã€‚
- en: 'Stack two copies of the data vertically: once with the data rotated by <math
    alttext="theta"><mi>Î¸</mi></math> = âˆ’*Ï€*/6, and once with the data rotated by
    <math alttext="theta"><mi>Î¸</mi></math> = âˆ’*Ï€*/3\. The resulting data matrix will
    be of size 2,000 Ã— 2.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‚ç›´å †å ä¸¤ä»½æ•°æ®å‰¯æœ¬ï¼šä¸€ä»½æ˜¯æ•°æ®æŒ‰è§’åº¦<math alttext="theta"><mi>Î¸</mi></math> = âˆ’*Ï€*/6æ—‹è½¬ï¼Œå¦ä¸€ä»½æ˜¯æŒ‰è§’åº¦<math
    alttext="theta"><mi>Î¸</mi></math> = âˆ’*Ï€*/3æ—‹è½¬ã€‚å¾—åˆ°çš„æ•°æ®çŸ©é˜µå¤§å°ä¸º2,000 Ã— 2ã€‚
- en: Use SVD to implement the PCA. I scaled the singular vectors by a factor of 2
    for visual inspection.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨SVDå®ç°PCAã€‚æˆ‘å°†å¥‡å¼‚å‘é‡çš„å°ºåº¦æ”¾å¤§äº†2å€ä»¥è¿›è¡Œè§†è§‰æ£€æŸ¥ã€‚
- en: '**Discussion:** PCA is excellent for reducing dimensionality of a high-dimensional
    dataset. This can facilitate data compression, data cleaning, and numerical stability
    issues (e.g., imagine that a 200-dimensional dataset with a condition number of
    10^(10) is reduced to the largest 100 dimensions with a condition number of 10âµ).
    But the dimensions themselves may be poor choices for feature extraction, because
    of the orthogonality constraint. Indeed, the principal directions of variance
    in [FigureÂ 15-5](#fig_15_5) are correct in a mathematical sense, but Iâ€™m sure
    you have the feeling that those are not the best basis vectors to capture the
    features of the data.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®ºï¼š** ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰éå¸¸é€‚åˆé™ä½é«˜ç»´æ•°æ®é›†çš„ç»´åº¦ã€‚è¿™å¯ä»¥ä¿ƒè¿›æ•°æ®å‹ç¼©ã€æ•°æ®æ¸…ç†å’Œæ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œæƒ³è±¡ä¸€ä¸ªå…·æœ‰æ¡ä»¶æ•° 10^(10)
    çš„ 200 ç»´æ•°æ®é›†ï¼Œå°†å…¶é™ä½åˆ°å…·æœ‰æ¡ä»¶æ•° 10âµ çš„æœ€å¤§ 100 ç»´ï¼‰ã€‚ä½†æ˜¯ï¼Œç”±äºæ­£äº¤æ€§çº¦æŸï¼Œç»´åº¦æœ¬èº«å¯èƒ½ä¸æ˜¯æå–ç‰¹å¾çš„æœ€ä½³é€‰æ‹©ã€‚ç¡®å®ï¼Œåœ¨ [å›¾Â 15-5](#fig_15_5)
    ä¸­æ–¹å·®çš„ä¸»è¦æ–¹å‘åœ¨æ•°å­¦ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œä½†æˆ‘ç›¸ä¿¡æ‚¨ä¼šæ„Ÿè§‰åˆ°è¿™äº›ä¸æ˜¯æ•è·æ•°æ®ç‰¹å¾çš„æœ€ä½³åŸºå‘é‡ã€‚'
- en: Linear Discriminant Analyses
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çº¿æ€§åˆ¤åˆ«åˆ†æã€‚
- en: Exercise 15-5\.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-5ã€‚
- en: You are going to perform an LDA on simulated 2D data. Simulated data is advantageous
    because you can manipulate the effects sizes, amount and nature of noise, number
    of categories, and so on.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†åœ¨æ¨¡æ‹Ÿçš„ 2D æ•°æ®ä¸Šæ‰§è¡Œ LDAã€‚æ¨¡æ‹Ÿæ•°æ®å…·æœ‰ä¼˜åŠ¿ï¼Œå› ä¸ºæ‚¨å¯ä»¥æ“çºµæ•ˆåº”å¤§å°ã€å™ªå£°çš„æ•°é‡å’Œæ€§è´¨ã€ç±»åˆ«æ•°é‡ç­‰ã€‚
- en: The data you will create was shown in [FigureÂ 15-2](#fig_15_2). Create two sets
    of normally distributed random numbers, each of size <math alttext="200 times
    2"><mrow><mn>200</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math> , with the second dimension
    being added onto the first (this imposes a correlation between the variables).
    Then add an xy offset of [2 âˆ’1] to the first set of numbers. It will be convenient
    to create a <math alttext="400 times 2"><mrow><mn>400</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math>
    matrix that contains both data classes, as well as a 400-element vector of class
    labels (I used 0s for the first class and 1s for the second class).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†åˆ›å»ºçš„æ•°æ®åœ¨ [å›¾Â 15-2](#fig_15_2) ä¸­æ˜¾ç¤ºã€‚åˆ›å»ºä¸¤ç»„æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼Œæ¯ç»„å¤§å°ä¸º <math alttext="200 times
    2"><mrow><mn>200</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math>ï¼Œç¬¬äºŒç»´åº¦æ·»åŠ åˆ°ç¬¬ä¸€ç»´åº¦ï¼ˆè¿™ä¼šåœ¨å˜é‡ä¹‹é—´å¼•å…¥ç›¸å…³æ€§ï¼‰ã€‚ç„¶åç»™ç¬¬ä¸€ç»„æ•°å­—æ·»åŠ ä¸€ä¸ª
    xy åç§»é‡ [2 âˆ’1]ã€‚æ–¹ä¾¿èµ·è§ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ•°æ®ç±»åˆ«çš„ <math alttext="400 times 2"><mrow><mn>400</mn>
    <mo>Ã—</mo> <mn>2</mn></mrow></math> çŸ©é˜µï¼Œä»¥åŠä¸€ä¸ªåŒ…å« 400 ä¸ªå…ƒç´ çš„ç±»åˆ«æ ‡ç­¾å‘é‡ï¼ˆæˆ‘ç”¨ 0 è¡¨ç¤ºç¬¬ä¸€ç±»ï¼Œ1 è¡¨ç¤ºç¬¬äºŒç±»ï¼‰ã€‚
- en: Use `sns.jointplot` and `plot_joint` to reproduce graph A in [FigureÂ 15-2](#fig_15_2).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `sns.jointplot` å’Œ `plot_joint` é‡æ–°ç”Ÿæˆå›¾ A åœ¨ [å›¾Â 15-2](#fig_15_2) ä¸­çš„å›¾å½¢ã€‚
- en: Exercise 15-6\.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-6ã€‚
- en: Now for the LDA. Write code in NumPy and/or SciPy instead of using a built-in
    library such as sklearn (weâ€™ll get to that later).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿›è¡Œ LDAã€‚ä½¿ç”¨ NumPy å’Œ/æˆ– SciPy ç¼–å†™ä»£ç ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ sklearn ç­‰å†…ç½®åº“ï¼ˆæˆ‘ä»¬ç¨åä¼šè®²åˆ°ï¼‰ã€‚
- en: The within-class covariance matrix <math alttext="bold upper C Subscript upper
    W"><msub><mi>ğ‚</mi> <mi>W</mi></msub></math> is created by computing the covariance
    of each class separately and then averaging those covariance matrices. The between-class
    covariance matrix <math alttext="bold upper C Subscript upper B"><msub><mi>ğ‚</mi>
    <mi>B</mi></msub></math> is created by computing the means of each data feature
    (in this case, the xy-coordinates) within each class, concatenating those feature-mean
    vectors for all classes (that will create a <math alttext="2 times 2"><mrow><mn>2</mn>
    <mo>Ã—</mo> <mn>2</mn></mrow></math> matrix for two features and two classes),
    and then computing the covariance matrix of that concatenated matrix.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å†…åæ–¹å·®çŸ©é˜µ <math alttext="bold upper C Subscript upper W"><msub><mi>ğ‚</mi> <mi>W</mi></msub></math>
    æ˜¯é€šè¿‡åˆ†åˆ«è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åæ–¹å·®ç„¶åå¯¹è¿™äº›åæ–¹å·®çŸ©é˜µå–å¹³å‡å€¼æ¥åˆ›å»ºçš„ã€‚ç±»é—´åæ–¹å·®çŸ©é˜µ <math alttext="bold upper C Subscript
    upper B"><msub><mi>ğ‚</mi> <mi>B</mi></msub></math> æ˜¯é€šè¿‡è®¡ç®—æ¯ä¸ªæ•°æ®ç‰¹å¾ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º xy åæ ‡ï¼‰åœ¨æ¯ä¸ªç±»åˆ«å†…çš„å‡å€¼ï¼Œå°†æ‰€æœ‰ç±»åˆ«çš„ç‰¹å¾å‡å€¼å‘é‡ä¸²è”èµ·æ¥ï¼ˆè¿™å°†åˆ›å»ºä¸€ä¸ª
    <math alttext="2 times 2"><mrow><mn>2</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math>
    çš„çŸ©é˜µï¼Œå¯¹äºä¸¤ä¸ªç‰¹å¾å’Œä¸¤ä¸ªç±»åˆ«ï¼‰ï¼Œç„¶åè®¡ç®—è¯¥ä¸²è”çŸ©é˜µçš„åæ–¹å·®çŸ©é˜µã€‚
- en: Remember from [ChapterÂ 13](ch13.xhtml#Chapter_13) that generalized eigendecomposition
    is implemented using SciPyâ€™s `eigh` function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä» [ç¬¬ 13 ç« ](ch13.xhtml#Chapter_13) ä¸­è®°ä½ï¼Œå¹¿ä¹‰ç‰¹å¾å€¼åˆ†è§£æ˜¯ä½¿ç”¨ SciPy çš„ `eigh` å‡½æ•°å®ç°çš„ã€‚
- en: The data projected into the LDA space is computed as <math alttext="bold upper
    X overTilde bold upper V"><mrow><mover accent="true"><mi>ğ—</mi> <mo>Ëœ</mo></mover>
    <mi>ğ•</mi></mrow></math> , where <math alttext="bold upper X overTilde"><mover
    accent="true"><mi>ğ—</mi> <mo>Ëœ</mo></mover></math> contains the concatenated data
    from all classes, mean-centered per feature, and <math alttext="bold upper V"><mi>ğ•</mi></math>
    is the matrix of eigenvectors.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ•å½±åˆ° LDA ç©ºé—´çš„æ•°æ®è®¡ç®—å¦‚ <math alttext="bold upper X overTilde bold upper V"><mrow><mover
    accent="true"><mi>ğ—</mi> <mo>Ëœ</mo></mover> <mi>ğ•</mi></mrow></math> ï¼Œå…¶ä¸­ <math
    alttext="bold upper X overTilde"><mover accent="true"><mi>ğ—</mi> <mo>Ëœ</mo></mover></math>
    åŒ…å«æ‰€æœ‰ç±»åˆ«çš„æ‹¼æ¥æ•°æ®ï¼Œæ¯ä¸ªç‰¹å¾è¿›è¡Œäº†å‡å€¼ä¸­å¿ƒåŒ–ï¼Œ<math alttext="bold upper V"><mi>ğ•</mi></math> æ˜¯ç‰¹å¾å‘é‡çŸ©é˜µã€‚
- en: Compute the classification accuracy, which is simply whether each data sample
    has a negative (â€œclass 0â€) or positive (â€œclass 1â€) projection onto the first LDA
    component. Graph C in [FigureÂ 15-6](#fig_15_6) shows the predicted class label
    for each data sample.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ï¼Œç®€å•æ¥è¯´å°±æ˜¯æ¯ä¸ªæ•°æ®æ ·æœ¬åœ¨ç¬¬ä¸€ä¸ªLDAæˆåˆ†ä¸Šçš„æŠ•å½±æ˜¯è´Ÿæ•°ï¼ˆâ€œç±» 0â€ï¼‰è¿˜æ˜¯æ­£æ•°ï¼ˆâ€œç±» 1â€ï¼‰ã€‚å›¾ C åœ¨ [å›¾Â 15-6](#fig_15_6)
    ä¸­å±•ç¤ºäº†æ¯ä¸ªæ•°æ®æ ·æœ¬çš„é¢„æµ‹ç±»åˆ«æ ‡ç­¾ã€‚
- en: Finally, show the results as shown in [FigureÂ 15-6](#fig_15_6).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå±•ç¤ºå¦‚ [å›¾Â 15-6](#fig_15_6) ä¸­æ‰€ç¤ºçš„ç»“æœã€‚
- en: '![exercise 15-6](assets/plad_1506.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  15-6](assets/plad_1506.png)'
- en: Figure 15-6\. Results from Exercise 15-6
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 15-6\. ç»ƒä¹  15-6 çš„ç»“æœ
- en: Exercise 15-7\.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-7\.
- en: I claimed in [ChapterÂ 13](ch13.xhtml#Chapter_13) that for a generalized eigendecomposition,
    the matrix of eigenvectors <math alttext="bold upper V"><mi>ğ•</mi></math> is not
    orthogonal, but it is orthogonal in the space of the â€œdenominatorâ€ matrix. Your
    goal here is to demonstrate that empirically.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ [ç¬¬ 13 ç« ](ch13.xhtml#Chapter_13) ä¸­æåˆ°ï¼Œå¯¹äºå¹¿ä¹‰ç‰¹å¾åˆ†è§£ï¼Œç‰¹å¾å‘é‡çŸ©é˜µ <math alttext="bold upper
    V"><mi>ğ•</mi></math> ä¸æ˜¯æ­£äº¤çš„ï¼Œä½†åœ¨â€œåˆ†æ¯â€çŸ©é˜µç©ºé—´ä¸­æ˜¯æ­£äº¤çš„ã€‚ä½ çš„ç›®æ ‡æ˜¯åœ¨è¿™é‡Œé€šè¿‡å®éªŒè¯æ˜è¿™ä¸€ç‚¹ã€‚
- en: Compute and inspect the results of <math alttext="bold upper V Superscript upper
    T Baseline bold upper V"><mrow><msup><mi>ğ•</mi> <mtext>T</mtext></msup> <mi>ğ•</mi></mrow></math>
    and <math alttext="bold upper V Superscript upper T Baseline bold upper C Subscript
    upper W Baseline bold upper V"><mrow><msup><mi>ğ•</mi> <mtext>T</mtext></msup>
    <msub><mi>ğ‚</mi> <mi>W</mi></msub> <mi>ğ•</mi></mrow></math> . Ignoring tiny precision
    errors, which one produces the identity matrix?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å¹¶æ£€æŸ¥ <math alttext="bold upper V Superscript upper T Baseline bold upper V"><mrow><msup><mi>ğ•</mi>
    <mtext>T</mtext></msup> <mi>ğ•</mi></mrow></math> å’Œ <math alttext="bold upper V
    Superscript upper T Baseline bold upper C Subscript upper W Baseline bold upper
    V"><mrow><msup><mi>ğ•</mi> <mtext>T</mtext></msup> <msub><mi>ğ‚</mi> <mi>W</mi></msub>
    <mi>ğ•</mi></mrow></math> çš„ç»“æœã€‚å¿½ç•¥å¾®å°çš„ç²¾åº¦è¯¯å·®ï¼Œå“ªä¸€ä¸ªäº§ç”Ÿäº†å•ä½çŸ©é˜µï¼Ÿ
- en: Exercise 15-8\.
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-8\.
- en: Now to reproduce our results using Pythonâ€™s sklearn library. Use the `LinearDiscriminantAnalysis`
    function in `sklearn.discriminant_analysis`. Produce a plot like [FigureÂ 15-7](#fig_15_7)
    and confirm that the overall prediction accuracy matches results from your â€œmanualâ€
    LDA analysis in the previous exercise. This function allows for several different
    solvers; use the `eigen` solver to match the previous exercise, and also to complete
    the following exercise.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨ Python çš„ sklearn åº“é‡ç°æˆ‘ä»¬çš„ç»“æœã€‚ä½¿ç”¨ `sklearn.discriminant_analysis` ä¸­çš„ `LinearDiscriminantAnalysis`
    å‡½æ•°ã€‚ç”Ÿæˆç±»ä¼¼äº [å›¾Â 15-7](#fig_15_7) çš„å›¾ï¼Œå¹¶ç¡®è®¤æ€»ä½“é¢„æµ‹å‡†ç¡®ç‡ä¸å‰ä¸€ç»ƒä¹ ä¸­â€œæ‰‹åŠ¨â€LDAåˆ†æçš„ç»“æœä¸€è‡´ã€‚æ­¤å‡½æ•°æ”¯æŒå¤šç§ä¸åŒçš„æ±‚è§£å™¨ï¼›ä½¿ç”¨
    `eigen` æ±‚è§£å™¨ä»¥åŒ¹é…å‰ä¸€ç»ƒä¹ ï¼Œå¹¶ç»§ç»­å®Œæˆä»¥ä¸‹ç»ƒä¹ ã€‚
- en: Plot the predicted labels from your â€œmanualâ€ LDA on top; you should find that
    the predicted labels are the same from both approaches.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ çš„â€œæ‰‹åŠ¨â€LDAä¸Šç»˜åˆ¶é¢„æµ‹æ ‡ç­¾ï¼›ä½ åº”è¯¥ä¼šå‘ç°ä¸¤ç§æ–¹æ³•çš„é¢„æµ‹æ ‡ç­¾æ˜¯ç›¸åŒçš„ã€‚
- en: '![exercise 15-8](assets/plad_1507.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  15-8](assets/plad_1507.png)'
- en: Figure 15-7\. Results from Exercise 15-8
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 15-7\. ç»ƒä¹  15-8 çš„ç»“æœ
- en: Exercise 15-9\.
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-9\.
- en: Letâ€™s use sklearn to explore the effects of shrinkage regularization. As I wrote
    in Chapters [12](ch12.xhtml#Chapter_12) and [13](ch13.xhtml#Chapter_13), it is
    trivial that shrinkage will reduce performance on training data; the important
    question is whether the regularization improves prediction accuracy on unseen
    data (sometimes called a *validation set* or *test set*). Therefore, you should
    write code to implement train/test splits. I did this by randomly permuting sample
    indices between 0 and 399, training on the first 350, and then testing on the
    final 50\. Because this is a small number of samples to average, I repeated this
    random selection 50 times and took the average accuracy to be the accuracy per
    shrinkage amount in [FigureÂ 15-8](#fig_15_8).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ sklearn æ¢ç´¢æ”¶ç¼©æ­£åˆ™åŒ–çš„æ•ˆæœã€‚æ­£å¦‚æˆ‘åœ¨ç¬¬ [12](ch12.xhtml#Chapter_12) å’Œ [13](ch13.xhtml#Chapter_13)
    ç« ä¸­å†™çš„é‚£æ ·ï¼Œæ˜¾ç„¶ï¼Œæ”¶ç¼©ä¼šé™ä½è®­ç»ƒæ•°æ®çš„æ€§èƒ½ï¼›é‡è¦çš„é—®é¢˜æ˜¯æ­£åˆ™åŒ–æ˜¯å¦æé«˜äº†åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„é¢„æµ‹å‡†ç¡®æ€§ï¼ˆæœ‰æ—¶ç§°ä¸º*éªŒè¯é›†*æˆ–*æµ‹è¯•é›†*ï¼‰ã€‚å› æ­¤ï¼Œæ‚¨åº”è¯¥ç¼–å†™ä»£ç æ¥å®ç°è®­ç»ƒ/æµ‹è¯•æ‹†åˆ†ã€‚æˆ‘é€šè¿‡éšæœºæ’åˆ—æ ·æœ¬ç´¢å¼•åœ¨
    0 åˆ° 399 ä¹‹é—´ï¼Œé¦–å…ˆåœ¨å‰ 350 ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨æœ€å 50 ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç”±äºæ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œæˆ‘é‡å¤äº†è¿™ä¸ªéšæœºé€‰æ‹© 50 æ¬¡ï¼Œå¹¶å°†å¹³å‡å‡†ç¡®ç‡ä½œä¸ºæ¯ä¸ªæ”¶ç¼©é‡åœ¨[å›¾
    15-8](#fig_15_8)ä¸­çš„å‡†ç¡®ç‡ã€‚
- en: '![exercise 15-9](assets/plad_1508.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  15-9](assets/plad_1508.png)'
- en: Figure 15-8\. Results from Exercise 15-9
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 15-8\. ç»ƒä¹  15-9 çš„ç»“æœ
- en: '**Discussion:** Shrinkage generally had a negative impact on validation performance.
    Although it looks like the performance improved with some shrinkage, repeating
    the code multiple times showed that these were just some random fluctuations.
    A deeper dive into regularization is more appropriate for a dedicated machine
    learning book, but I wanted to highlight here that many â€œtricksâ€ that have been
    developed in machine learning are not necessarily advantageous in all cases.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®ºï¼š** æ”¶ç¼©é€šå¸¸å¯¹éªŒè¯æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚è™½ç„¶çœ‹èµ·æ¥é€šè¿‡ä¸€äº›æ”¶ç¼©å¯ä»¥æ”¹å–„æ€§èƒ½ï¼Œä½†å¤šæ¬¡é‡å¤ä»£ç è¡¨æ˜è¿™äº›åªæ˜¯ä¸€äº›éšæœºæ³¢åŠ¨ã€‚æ·±å…¥æ¢è®¨æ­£åˆ™åŒ–æ›´é€‚åˆä¸“é—¨çš„æœºå™¨å­¦ä¹ ä¹¦ç±ï¼Œä½†æˆ‘æƒ³åœ¨è¿™é‡Œå¼ºè°ƒçš„æ˜¯ï¼Œè®¸å¤šåœ¨æœºå™¨å­¦ä¹ ä¸­å¼€å‘çš„â€œæŠ€å·§â€å¹¶ä¸ä¸€å®šåœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æœ‰åˆ©ã€‚'
- en: SVD for Low-Rank Approximations
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ç§©è¿‘ä¼¼çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰
- en: Exercise 15-10\.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-10\.
- en: 'Igor Stravinsky was one of the greatest music composers of all time (IMHO)â€”certainly
    one of the most influential of the 20th century. He also made many thought-provoking
    statements on the nature of art, media, and criticism, including one of my favorite
    quotes: â€œThe more art is limited, the more it is free.â€ There is a famous and
    captivating drawing of Stravinsky by none other than the great Pablo Picasso.
    An image of this drawing is [available on Wikipedia](https://oreil.ly/BtSZv),
    and we are going to work with this picture in the next several exercises. Like
    with other images weâ€™ve worked with in this book, it is natively a 3D matrix (
    <math alttext="640 times 430 times 3"><mrow><mn>640</mn> <mo>Ã—</mo> <mn>430</mn>
    <mo>Ã—</mo> <mn>3</mn></mrow></math> ), but we will convert it to grayscale (2D)
    for convenience.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼Šæˆˆå°”Â·æ–¯ç‰¹æ‹‰æ–‡æ–¯åŸºï¼ˆIMHO æ˜¯æ‰€æœ‰æ—¶ä»£æœ€ä¼Ÿå¤§çš„éŸ³ä¹ä½œæ›²å®¶ä¹‹ä¸€ï¼‰ï¼Œæ— ç–‘æ˜¯ 20 ä¸–çºªæœ€å…·å½±å“åŠ›çš„éŸ³ä¹å®¶ä¹‹ä¸€ã€‚ä»–è¿˜å¯¹è‰ºæœ¯ã€åª’ä½“å’Œæ‰¹è¯„çš„æ€§è´¨å‘è¡¨äº†è®¸å¤šå‘äººæ·±çœçš„è¨€è®ºï¼ŒåŒ…æ‹¬æˆ‘æœ€å–œæ¬¢çš„ä¸€å¥è¯ï¼šâ€œè‰ºæœ¯è¶Šå—é™åˆ¶ï¼Œå®ƒå°±è¶Šè‡ªç”±ã€‚â€æœ‰ä¸€å¹…ç”±ä¼Ÿå¤§çš„å·´å‹ƒç½—Â·æ¯•åŠ ç´¢åˆ›ä½œçš„è‘—åè€Œå¼•äººå…¥èƒœçš„æ–¯ç‰¹æ‹‰æ–‡æ–¯åŸºçš„ç”»åƒã€‚è¿™å¹…ç”»åƒåœ¨[ç»´åŸºç™¾ç§‘ä¸Šæœ‰](https://oreil.ly/BtSZv)ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªç»ƒä¹ ä¸­ä½¿ç”¨è¿™å¹…å›¾ç‰‡ã€‚åƒæˆ‘ä»¬åœ¨æœ¬ä¹¦ä¸­ä½¿ç”¨è¿‡çš„å…¶ä»–å›¾ç‰‡ä¸€æ ·ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª
    3D çŸ©é˜µï¼ˆ <math alttext="640 times 430 times 3"><mrow><mn>640</mn> <mo>Ã—</mo> <mn>430</mn>
    <mo>Ã—</mo> <mn>3</mn></mrow></math> ï¼‰ï¼Œä½†æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºç°åº¦å›¾åƒï¼ˆ2Dï¼‰ä»¥æ–¹ä¾¿å¤„ç†ã€‚
- en: 'The purpose of this exercise is to repeat [Exercise 14-5](ch14.xhtml#exercise_14_5),
    in which you re-created a close approximation to a smooth-noise image based on
    four â€œlayersâ€ from the SVD (please look back at that exercise to refresh your
    memory). Produce a figure like [FigureÂ 15-9](#fig_15_9) using the Stravinsky image.
    Here is the main question: does reconstructing the image using the first four
    components give a good result like it did in the previous chapter?'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç»ƒä¹ çš„ç›®çš„æ˜¯é‡å¤[ç»ƒä¹  14-5](ch14.xhtml#exercise_14_5)ï¼Œåœ¨å…¶ä¸­æ ¹æ®å¥‡å¼‚å€¼åˆ†è§£çš„å››ä¸ªâ€œå±‚æ¬¡â€é‡æ–°åˆ›å»ºäº†ä¸€ä¸ªæ¥è¿‘å…‰æ»‘å™ªå£°å›¾åƒçš„è¿‘ä¼¼å›¾åƒï¼ˆè¯·å›é¡¾é‚£ä¸ªç»ƒä¹ ä»¥åˆ·æ–°æ‚¨çš„è®°å¿†ï¼‰ã€‚ä½¿ç”¨æ–¯ç‰¹æ‹‰æ–‡æ–¯åŸºçš„å›¾åƒåˆ¶ä½œåƒ[å›¾
    15-9](#fig_15_9)é‚£æ ·çš„å›¾è¡¨ã€‚è¿™é‡Œçš„ä¸»è¦é—®é¢˜æ˜¯ï¼šä½¿ç”¨å‰å››ä¸ªåˆ†é‡é‡å»ºå›¾åƒæ˜¯å¦åƒä¸Šä¸€ç« é‚£æ ·å–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Ÿ
- en: '![exercise 15-10](assets/plad_1509.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  15-10](assets/plad_1509.png)'
- en: Figure 15-9\. Results from Exercise 15-10
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 15-9\. ç»ƒä¹  15-10 çš„ç»“æœ
- en: Exercise 15-11\.
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-11\.
- en: Well, the answer to the question at the end of the previous exercise is a resounding
    â€œNo!â€ The rank-4 approximation is terrible! It looks nothing like the original
    image. The goal of this exercise is to reconstruct the image using more layers
    so that the low-rank approximation is reasonably accurateâ€”and then compute the
    amount of compression obtained.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œåœ¨ä¸Šä¸€ä¸ªç»ƒä¹ çš„æœ€åä¸€ä¸ªé—®é¢˜çš„ç­”æ¡ˆæ˜¯ä¸€ä¸ªå“äº®çš„â€œä¸è¡Œï¼â€ç§©ä¸º 4 çš„è¿‘ä¼¼å¤ªç³Ÿç³•äº†ï¼çœ‹èµ·æ¥å’ŒåŸå§‹å›¾åƒå®Œå…¨ä¸ä¸€æ ·ã€‚è¿™ä¸ªç»ƒä¹ çš„ç›®æ ‡æ˜¯ä½¿ç”¨æ›´å¤šå±‚æ¥é‡å»ºå›¾åƒï¼Œä»¥ä¾¿ä½ç§©è¿‘ä¼¼æ˜¯åˆç†å‡†ç¡®çš„â€”â€”ç„¶åè®¡ç®—æ‰€è·å¾—çš„å‹ç¼©é‡ã€‚
- en: Start by producing [FigureÂ 15-10](#fig_15_10), which shows the original image,
    the reconstructed image, and the error map, which is the squared difference between
    the original and the approximation. For this figure, I chose *k* = 80 components,
    but I encourage you to explore different values (that is, different rank approximations).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç”Ÿæˆ [FigureÂ 15-10](#fig_15_10) å¼€å§‹ï¼Œæ˜¾ç¤ºåŸå§‹å›¾åƒã€é‡å»ºå›¾åƒå’Œè¯¯å·®å›¾ï¼Œå³åŸå§‹å›¾åƒä¸è¿‘ä¼¼å›¾åƒçš„å¹³æ–¹å·®ã€‚å¯¹äºè¿™ä¸ªå›¾ï¼Œæˆ‘é€‰æ‹©äº†
    *k* = 80 ä¸ªç»„ä»¶ï¼Œä½†é¼“åŠ±ä½ æ¢ç´¢ä¸åŒæ•°å€¼ï¼ˆå³ä¸åŒç§©çš„è¿‘ä¼¼ï¼‰ã€‚
- en: '![exercise 15-11](assets/plad_1510.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  15-11](assets/plad_1510.png)'
- en: Figure 15-10\. Results from Exercise 15-11
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 15-10\. ç»ƒä¹  15-11 çš„ç»“æœ
- en: Next, compute the compression ratio, which is the percentage of the number of
    bytes used by the low-rank approximation versus the number of bytes used by the
    original image. My results for *k* = 80 are shown here.^([6](ch15.xhtml#idm45733290310320))
    Keep in mind that with low-rank approximations, you donâ€™t need to store the full
    image or the full SVD matrices!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®¡ç®—å‹ç¼©æ¯”ç‡ï¼Œå³ä½ç§©è¿‘ä¼¼ä½¿ç”¨çš„å­—èŠ‚ä¸åŸå§‹å›¾åƒä½¿ç”¨çš„å­—èŠ‚çš„ç™¾åˆ†æ¯”ã€‚æˆ‘çš„ *k* = 80 çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºã€‚^([6](ch15.xhtml#idm45733290310320))
    è¯·è®°ä½ï¼Œä½¿ç”¨ä½ç§©è¿‘ä¼¼æ—¶ï¼Œä½ ä¸éœ€è¦å­˜å‚¨å®Œæ•´å›¾åƒæˆ–å®Œæ•´çš„å¥‡å¼‚å€¼åˆ†è§£çŸ©é˜µï¼
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Exercise 15-12\.
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-12\.
- en: Why did I choose *k* = 80 and not, e.g., 70 or 103? It was quite arbitrary,
    to be honest. The goal of this exercise is to see whether itâ€™s possible to use
    the error map to determine an appropriate rank parameter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘é€‰æ‹©äº† *k* = 80 è€Œä¸æ˜¯ï¼Œä¾‹å¦‚ï¼Œ70 æˆ– 103ï¼Ÿè¯´å®è¯ï¼Œè¿™ç›¸å½“éšæ„ã€‚è¿™ä¸ªç»ƒä¹ çš„ç›®æ ‡æ˜¯çœ‹çœ‹æ˜¯å¦å¯ä»¥ä½¿ç”¨è¯¯å·®å›¾æ¥ç¡®å®šé€‚å½“çš„ç§©å‚æ•°ã€‚
- en: In a `for` loop over reconstruction ranks between 1 and the number of singular
    values, create the low-rank approximation and compute the Frobenius distance between
    the original and *k*-rank approximation. Then make a plot of the error as a function
    of rank, as in [FigureÂ 15-11](#fig_15_11). The error certainly decreases with
    increasing rank, but there is no clear rank that seems best. Sometimes in optimization
    algorithms, the derivative of the error function is more informative; give that
    a try!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä» 1 åˆ°å¥‡å¼‚å€¼æ•°é‡çš„é‡å»ºç§©çš„ `for` å¾ªç¯ä¸­ï¼Œåˆ›å»ºä½ç§©è¿‘ä¼¼å¹¶è®¡ç®—åŸå§‹å›¾åƒä¸ *k* ç§©è¿‘ä¼¼ä¹‹é—´çš„Frobeniusè·ç¦»ã€‚ç„¶åï¼Œåƒ [FigureÂ 15-11](#fig_15_11)
    ä¸­é‚£æ ·ç»˜åˆ¶éšç§©å˜åŒ–çš„è¯¯å·®å›¾ã€‚è¯¯å·®éšç§©å¢åŠ è€Œå‡å°‘ï¼Œä½†æ²¡æœ‰æ˜æ˜¾çš„æœ€ä½³ç§©ã€‚æœ‰æ—¶åœ¨ä¼˜åŒ–ç®—æ³•ä¸­ï¼Œè¯¯å·®å‡½æ•°çš„å¯¼æ•°æ›´å…·ä¿¡æ¯æ€§ï¼›è¯•è¯•çœ‹ï¼
- en: '![exercise 15-12](assets/plad_1511.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  15-12](assets/plad_1511.png)'
- en: Figure 15-11\. Results from Exercise 15-12
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 15-11\. ç»ƒä¹  15-12 çš„ç»“æœ
- en: 'Final thought for this exercise: the reconstruction error for *k* = 430 (i.e.,
    the full SVD) should be exactly 0\. Is it? Obviously the answer is no; otherwise,
    I wouldnâ€™t have written the question. But you should confirm this yourself. This
    is yet another demonstration of precision errors in applied linear algebra.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»ƒä¹ çš„æœ€åæƒ³æ³•æ˜¯ï¼š*k* = 430 çš„é‡å»ºè¯¯å·®ï¼ˆå³å®Œå…¨å¥‡å¼‚å€¼åˆ†è§£ï¼‰åº”è¯¥æ˜¯å®Œå…¨ä¸º 0ã€‚æ˜¯å—ï¼Ÿæ˜¾ç„¶ä¸æ˜¯ï¼Œå¦åˆ™æˆ‘å°±ä¸ä¼šå†™è¿™ä¸ªé—®é¢˜äº†ã€‚ä½†ä½ åº”è¯¥è‡ªå·±ç¡®è®¤ä¸€ä¸‹ã€‚è¿™åˆæ˜¯åº”ç”¨çº¿æ€§ä»£æ•°ä¸­ç²¾åº¦è¯¯å·®çš„åˆä¸€æ¬¡æ¼”ç¤ºã€‚
- en: SVD for Image Denoising
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒå»å™ªçš„å¥‡å¼‚å€¼åˆ†è§£
- en: Exercise 15-13\.
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  15-13\.
- en: Letâ€™s see if we can extend the concept of low-rank approximation to denoise
    the Stravinsky picture. The goal of this exercise is to add noise and inspect
    the SVD results, and then the following exercise will involve â€œprojecting outâ€
    the corruption.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦èƒ½å°†ä½ç§©è¿‘ä¼¼çš„æ¦‚å¿µæ‰©å±•åˆ°å»å™ªæ–¯ç‰¹æ‹‰æ–‡æ–¯åŸºçš„å›¾ç‰‡ã€‚è¿™ä¸ªç»ƒä¹ çš„ç›®æ ‡æ˜¯æ·»åŠ å™ªå£°å¹¶æ£€æŸ¥å¥‡å¼‚å€¼åˆ†è§£çš„ç»“æœï¼Œæ¥ä¸‹æ¥çš„ç»ƒä¹ å°†æ¶‰åŠâ€œæŠ•å½±å‡ºâ€æŸåçš„éƒ¨åˆ†ã€‚
- en: The noise here will be a spatial sine wave. You can see the noise and corrupted
    image in [FigureÂ 15-12](#fig_15_12).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å™ªå£°å°†æ˜¯ç©ºé—´æ­£å¼¦æ³¢ã€‚ä½ å¯ä»¥åœ¨ [FigureÂ 15-12](#fig_15_12) ä¸­çœ‹åˆ°å™ªå£°å’ŒæŸåçš„å›¾åƒã€‚
- en: '![exercise 15-13](assets/plad_1512.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  15-13](assets/plad_1512.png)'
- en: Figure 15-12\. Preparation for Exercise 15-13
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 15-12\. ç»ƒä¹  15-13 çš„å‡†å¤‡å·¥ä½œ
- en: 'I will now describe how to create a 2D sine wave (also called a *sine grating*).
    This is a good opportunity to practice your math-to-code translation skills. The
    formula for a 2D sine grating is:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘å°†æè¿°å¦‚ä½•åˆ›å»ºäºŒç»´æ­£å¼¦æ³¢ï¼ˆä¹Ÿç§°ä¸º *æ­£å¼¦å…‰æ …*ï¼‰ã€‚è¿™æ˜¯ç»ƒä¹ ä½ çš„æ•°å­¦åˆ°ä»£ç è½¬æ¢æŠ€èƒ½çš„å¥½æœºä¼šã€‚äºŒç»´æ­£å¼¦æ³¢çš„å…¬å¼æ˜¯ï¼š
- en: <math alttext="bold upper Z equals sine left-parenthesis 2 pi f left-parenthesis
    bold upper X cosine left-parenthesis theta right-parenthesis plus bold upper Y
    sine left-parenthesis theta right-parenthesis right-parenthesis right-parenthesis"
    display="block"><mrow><mi>ğ™</mi> <mo>=</mo> <mo form="prefix">sin</mo> <mo>(</mo>
    <mn>2</mn> <mi>Ï€</mi> <mi>f</mi> <mo>(</mo> <mi>ğ—</mi> <mo form="prefix">cos</mo>
    <mo>(</mo> <mi>Î¸</mi> <mo>)</mo> <mo>+</mo> <mi>ğ˜</mi> <mo form="prefix">sin</mo>
    <mo>(</mo> <mi>Î¸</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Z equals sine left-parenthesis 2 pi f left-parenthesis
    bold upper X cosine left-parenthesis theta right-parenthesis plus bold upper Y
    sine left-parenthesis theta right-parenthesis right-parenthesis right-parenthesis"
    display="block"><mrow><mi>ğ™</mi> <mo>=</mo> <mo form="prefix">sin</mo> <mo>(</mo>
    <mn>2</mn> <mi>Ï€</mi> <mi>f</mi> <mo>(</mo> <mi>ğ—</mi> <mo form="prefix">cos</mo>
    <mo>(</mo> <mi>Î¸</mi> <mo>)</mo> <mo>+</mo> <mi>ğ˜</mi> <mo form="prefix">sin</mo>
    <mo>(</mo> <mi>Î¸</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math>
- en: In this formula, *f* is the frequency of the sine wave, <math alttext="theta"><mi>Î¸</mi></math>
    is a rotation parameter, and <math alttext="pi"><mi>Ï€</mi></math> is the constant
    3.14â€¦. <math alttext="bold upper X"><mi>ğ—</mi></math> and <math alttext="bold
    upper Y"><mi>ğ˜</mi></math> are grid locations on which the function is evaluated,
    which I set to be integers from âˆ’100 to 100 with the number of steps set to match
    the size of the Stravinsky picture. I set <math alttext="f"><mi>f</mi></math>
    = .02 and <math alttext="theta"><mi>Î¸</mi></math> = *Ï€*/6.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œ*f*æ˜¯æ­£å¼¦æ³¢çš„é¢‘ç‡ï¼Œ<math alttext="theta"><mi>Î¸</mi></math>æ˜¯æ—‹è½¬å‚æ•°ï¼Œ<math alttext="pi"><mi>Ï€</mi></math>æ˜¯å¸¸æ•°3.14â€¦ã€‚<math
    alttext="bold upper X"><mi>ğ—</mi></math>å’Œ<math alttext="bold upper Y"><mi>ğ˜</mi></math>æ˜¯å‡½æ•°è¯„ä¼°çš„ç½‘æ ¼ä½ç½®ï¼Œæˆ‘è®¾ç½®ä¸ºä»âˆ’100åˆ°100çš„æ•´æ•°ï¼Œæ­¥æ•°è®¾ç½®ä¸ºä¸æ–¯ç‰¹æ‹‰æ–‡æ–¯åŸºå›¾ç‰‡å¤§å°ç›¸åŒ¹é…ã€‚æˆ‘è®¾ç½®<math
    alttext="f"><mi>f</mi></math> = .02å’Œ<math alttext="theta"><mi>Î¸</mi></math> =
    *Ï€*/6ã€‚
- en: Before moving on to the rest of the exercise, I encourage you to spend some
    time with the sine grating code by exploring the effects of changing the parameters
    on the resulting image. However, please use the parameters I wrote previously
    to make sure you can reproduce my results that follow.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»§ç»­è¿›è¡Œå…¶ä»–ç»ƒä¹ ä¹‹å‰ï¼Œæˆ‘å»ºè®®æ‚¨èŠ±ä¸€äº›æ—¶é—´ä½¿ç”¨æ­£å¼¦å…‰æ …ä»£ç ï¼Œæ¢ç´¢æ›´æ”¹å‚æ•°å¯¹ç”Ÿæˆçš„å›¾åƒçš„å½±å“ã€‚ä½†è¯·ä½¿ç”¨æˆ‘ä¹‹å‰å†™çš„å‚æ•°ï¼Œä»¥ç¡®ä¿æ‚¨å¯ä»¥å¤ç°æˆ‘éšåçš„ç»“æœã€‚
- en: 'Next, corrupt the Stravinsky picture by adding the noise to the image. You
    should first scale the noise to a range of 0 to 1, then add the noise and the
    original picture together, and then rescale. Scaling an image between 0 and 1
    is achieved by applying the following formula:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œé€šè¿‡å°†å™ªå£°æ·»åŠ åˆ°å›¾åƒä¸­æ¥æŸåæ–¯ç‰¹æ‹‰æ–‡æ–¯åŸºå›¾ç‰‡ã€‚æ‚¨åº”è¯¥é¦–å…ˆå°†å™ªå£°ç¼©æ”¾åˆ°0åˆ°1çš„èŒƒå›´å†…ï¼Œç„¶åå°†å™ªå£°å’ŒåŸå§‹å›¾ç‰‡ç›¸åŠ ï¼Œç„¶åé‡æ–°ç¼©æ”¾ã€‚å°†å›¾åƒåœ¨0åˆ°1ä¹‹é—´ç¼©æ”¾çš„æ–¹æ³•æ˜¯åº”ç”¨ä»¥ä¸‹å…¬å¼ï¼š
- en: <math alttext="bold upper R overTilde equals StartFraction bold upper R minus
    min left-parenthesis bold upper R right-parenthesis Over max left-parenthesis
    bold upper R right-parenthesis minus min left-parenthesis bold upper R right-parenthesis
    EndFraction" display="block"><mrow><mover accent="true"><mi>ğ‘</mi> <mo>Ëœ</mo></mover>
    <mo>=</mo> <mfrac><mrow><mi>ğ‘</mi><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>ğ‘</mi><mo>)</mo></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>ğ‘</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>ğ‘</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper R overTilde equals StartFraction bold upper R minus
    min left-parenthesis bold upper R right-parenthesis Over max left-parenthesis
    bold upper R right-parenthesis minus min left-parenthesis bold upper R right-parenthesis
    EndFraction" display="block"><mrow><mover accent="true"><mi>ğ‘</mi> <mo>Ëœ</mo></mover>
    <mo>=</mo> <mfrac><mrow><mi>ğ‘</mi><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>ğ‘</mi><mo>)</mo></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>ğ‘</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>ğ‘</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: OK, now you have your noise-corrupted image. Reproduce [FigureÂ 15-13](#fig_15_13),
    which is the same as [FigureÂ 15-6](#fig_15_6) but using the noisy image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨æœ‰äº†å¸¦å™ªå£°çš„å›¾åƒã€‚é‡ç°[å›¾15-13](#fig_15_13)ï¼Œè¿™ä¸[å›¾15-6](#fig_15_6)ç›¸åŒï¼Œä½†ä½¿ç”¨äº†æœ‰å™ªå£°çš„å›¾åƒã€‚
- en: '![ex12.](assets/plad_1513.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![ex12.](assets/plad_1513.png)'
- en: Figure 15-13\. Results from Exercise 15-13
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾15-13\. ç»ƒä¹ 15-13çš„ç»“æœ
- en: '**Discussion:** Itâ€™s interesting to compare [FigureÂ 15-13](#fig_15_13) with
    [FigureÂ 15-9](#fig_15_9). Although we created the noise based on one feature (the
    sine wave grating), the SVD separated the grating into two components of equal
    importance (roughly equal singular values).^([7](ch15.xhtml#idm45733290197184))
    Those two components are not sine gratings but instead are vertically oriented
    patches. Their sum, however, produces the diagonal bands of the grating.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®ºï¼š** æ¯”è¾ƒ[å›¾15-13](#fig_15_13)ä¸[å›¾15-9](#fig_15_9)æ˜¯å¾ˆæœ‰æ„æ€çš„ã€‚å°½ç®¡æˆ‘ä»¬åŸºäºä¸€ä¸ªç‰¹å¾ï¼ˆæ­£å¼¦æ³¢å…‰æ …ï¼‰åˆ›å»ºäº†å™ªå£°ï¼Œä½†SVDå°†è¯¥å…‰æ …åˆ†ä¸ºä¸¤ä¸ªé‡è¦æ€§ç›¸ç­‰çš„åˆ†é‡ï¼ˆå¤§è‡´ç›¸ç­‰çš„å¥‡å¼‚å€¼ï¼‰ã€‚è¿™ä¸¤ä¸ªåˆ†é‡ä¸æ˜¯æ­£å¼¦æ³¢å…‰æ …ï¼Œè€Œæ˜¯å‚ç›´æ–¹å‘çš„è¡¥ä¸ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€»å’Œäº§ç”Ÿäº†å…‰æ …çš„å¯¹è§’çº¿æ¡å¸¦ã€‚'
- en: Exercise 15-14\.
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'ç»ƒä¹ 15-14\. '
- en: Now for the denoising. It appears that the noise is contained in the second
    and third components, so your goal now is to reconstruct the image using all components
    except for those two. Produce a figure like [FigureÂ 15-14](#fig_15_14).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿›è¡Œå»å™ªã€‚çœ‹èµ·æ¥å™ªéŸ³åŒ…å«åœ¨ç¬¬äºŒå’Œç¬¬ä¸‰ä¸ªåˆ†é‡ä¸­ï¼Œæ‰€ä»¥ä½ ç°åœ¨çš„ç›®æ ‡æ˜¯ä½¿ç”¨é™¤äº†è¿™ä¸¤ä¸ªåˆ†é‡ä¹‹å¤–çš„æ‰€æœ‰åˆ†é‡æ¥é‡å»ºå›¾åƒã€‚åˆ¶ä½œä¸€ä¸ªç±»ä¼¼äº[å›¾15-14](#fig_15_14)çš„å›¾ã€‚
- en: '![exercise 14](assets/plad_1514.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹ 14](assets/plad_1514.png)'
- en: Figure 15-14\. Results from Exercise 15-14
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾15-14\. ç»ƒä¹ 15-14çš„ç»“æœ
- en: '**Discussion:** The denoising is decent but certainly not perfect. One of the
    reasons for the imperfection is that the noise is not entirely contained in two
    dimensions (notice that the middle panel of [FigureÂ 15-14](#fig_15_14) does not
    perfectly match the noise image). Furthermore, the noise projection (the image
    made from components 1 and 2) has negative values and is distributed around zero,
    even though the sine grating had no negative values. (You can confirm this by
    plotting a histogram of the noise image, which I show in the online code.) The
    rest of the image needs to have fluctuations in values to account for this so
    that the full reconstruction has only positive values.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¨è®ºï¼š** å»å™ªæ•ˆæœè¿˜å¯ä»¥ï¼Œä½†è‚¯å®šä¸å®Œç¾ã€‚ä¸å®Œç¾çš„åŸå› ä¹‹ä¸€æ˜¯å™ªéŸ³å¹¶éå®Œå…¨åŒ…å«åœ¨ä¸¤ä¸ªç»´åº¦ä¸­ï¼ˆæ³¨æ„[å›¾15-14](#fig_15_14)çš„ä¸­é—´é¢æ¿å¹¶ä¸å®Œå…¨åŒ¹é…å™ªéŸ³å›¾åƒï¼‰ã€‚æ­¤å¤–ï¼Œå™ªå£°æŠ•å½±ï¼ˆç”±ç¬¬1å’Œç¬¬2ä¸ªåˆ†é‡åˆ¶æˆçš„å›¾åƒï¼‰å…·æœ‰è´Ÿå€¼ï¼Œå¹¶åˆ†å¸ƒåœ¨é›¶é™„è¿‘ï¼Œå°½ç®¡æ­£å¼¦å…‰æ …æ²¡æœ‰è´Ÿå€¼ã€‚ï¼ˆæ‚¨å¯ä»¥é€šè¿‡ç»˜åˆ¶å™ªéŸ³å›¾åƒçš„ç›´æ–¹å›¾æ¥ç¡®è®¤è¿™ä¸€ç‚¹ï¼Œæˆ‘åœ¨åœ¨çº¿ä»£ç ä¸­å±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚ï¼‰å›¾åƒçš„å…¶ä½™éƒ¨åˆ†éœ€è¦æœ‰å€¼çš„æ³¢åŠ¨æ¥è§£é‡Šè¿™ä¸€ç‚¹ï¼Œä»¥ä¾¿å…¨é¢é‡å»ºåªæœ‰æ­£å€¼ã€‚'
- en: ^([1](ch15.xhtml#idm45733290809152-marker)) The online code demonstrates this
    point.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch15.xhtml#idm45733290809152-marker)) åœ¨çº¿ä»£ç æ¼”ç¤ºäº†è¿™ä¸€ç‚¹ã€‚
- en: ^([2](ch15.xhtml#idm45733290657888-marker)) In [Exercise 15-3](#exercise_15_3),
    you will also learn how to implement PCA using the Python scikit-learn library.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch15.xhtml#idm45733290657888-marker)) åœ¨[ç»ƒä¹  15-3](#exercise_15_3)ä¸­ï¼Œæ‚¨è¿˜å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨Pythonçš„scikit-learnåº“å®ç°ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€‚
- en: ^([3](ch15.xhtml#idm45733290633904-marker)) Indeed, linear discriminant analysis
    is also called Fisherâ€™s discriminant analysis.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch15.xhtml#idm45733290633904-marker)) å®é™…ä¸Šï¼Œçº¿æ€§åˆ¤åˆ«åˆ†æä¹Ÿç§°ä¸ºè´¹èˆå°”åˆ¤åˆ«åˆ†æã€‚
- en: ^([4](ch15.xhtml#idm45733290531744-marker)) I wonâ€™t go through the calculus-laden
    proof, but itâ€™s just a minor variant of the proof given in the PCA section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch15.xhtml#idm45733290531744-marker)) æˆ‘ä¸ä¼šè¯¦ç»†è®²è¿°è¿™ä¸ªå……æ»¡å¾®ç§¯åˆ†çš„è¯æ˜ï¼Œä½†å®ƒåªæ˜¯ä¸»æˆåˆ†åˆ†æéƒ¨åˆ†ç»™å‡ºçš„è¯æ˜çš„ä¸€ä¸ªå°å˜ä½“ã€‚
- en: '^([5](ch15.xhtml#idm45733290481824-marker)) Data citation: Akbilgic, Oguz.
    (2013). Istanbul Stock Exchange. UCI Machine Learning Repository. Data source
    website: [*https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange*](https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch15.xhtml#idm45733290481824-marker)) æ•°æ®å¼•ç”¨ï¼šAkbilgic, Oguz. (2013). ä¼Šæ–¯å¦å¸ƒå°”è¯åˆ¸äº¤æ˜“æ‰€ã€‚UCIæœºå™¨å­¦ä¹ åº“ã€‚æ•°æ®æ¥æºç½‘ç«™ï¼š[*https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange*](https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange)ã€‚
- en: ^([6](ch15.xhtml#idm45733290310320-marker)) There is some ambiguity about whether
    one megabyte is 1,000Â² or 1,024Â² bytes; I used the latter, but it doesnâ€™t affect
    the compression ratio.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch15.xhtml#idm45733290310320-marker)) æœ‰äº›ä¸ç¡®å®šæ˜¯å¦ä¸€å…†å­—èŠ‚æ˜¯1000Â²è¿˜æ˜¯1024Â²å­—èŠ‚ï¼›æˆ‘ä½¿ç”¨äº†åè€…ï¼Œä½†è¿™ä¸å½±å“å‹ç¼©æ¯”ã€‚
- en: ^([7](ch15.xhtml#idm45733290197184-marker)) The likely explanation is that this
    is a 2D singular *plane*, not a pair of singular *vectors*; any two linearly independent
    vectors on this plane could be basis vectors, and Python selected an orthogonal
    pair.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch15.xhtml#idm45733290197184-marker)) å¯èƒ½çš„è§£é‡Šæ˜¯è¿™æ˜¯ä¸€ä¸ªäºŒç»´çš„å¥‡å¼‚*å¹³é¢*ï¼Œè€Œä¸æ˜¯ä¸€å¯¹å¥‡å¼‚*å‘é‡*ï¼›åœ¨è¿™ä¸ªå¹³é¢ä¸Šï¼Œä»»æ„ä¸¤ä¸ªçº¿æ€§ç‹¬ç«‹çš„å‘é‡éƒ½å¯ä»¥æ˜¯åŸºå‘é‡ï¼ŒPythoné€‰æ‹©äº†ä¸€å¯¹æ­£äº¤å‘é‡ã€‚
