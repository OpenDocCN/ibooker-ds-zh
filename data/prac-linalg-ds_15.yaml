- en: Chapter 15\. Eigendecomposition and SVD Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 15 章。特征分解和奇异值分解应用
- en: Eigendecomposition and SVD are gems that linear algebra has bestowed upon modern
    human civilization. Their importance in modern applied mathematics cannot be understated,
    and their applications are uncountable and spread across myriad disciplines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 特征分解和奇异值分解是线性代数赋予现代人类文明的宝藏。它们在现代应用数学中的重要性不可低估，其应用遍布各种学科。
- en: In this chapter, I will highlight three applications that you are likely to
    come across in data science and related fields. My main goal is to show you that
    seemingly complicated data science and machine learning techniques are actually
    quite sensible and easily understood, once you’ve learned the linear algebra topics
    in this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将重点介绍您在数据科学及相关领域可能遇到的三个应用程序。我的主要目标是向您展示，一旦您学习了本书中的线性代数主题，表面上复杂的数据科学和机器学习技术实际上是相当合理且容易理解的。
- en: PCA Using Eigendecomposition and SVD
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征分解和奇异值分解进行 PCA
- en: The purpose of PCA is to find a set of basis vectors for a dataset that point
    in the direction that maximizes covariation across the variables.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的目的是为数据集找到一组基向量，这些向量指向最大化变量之间协变性的方向。
- en: Imagine that an N-D dataset exists in an N-D space, with each data point being
    a coordinate in that space. This is sensible when you think about storing the
    data in a matrix with *N* observations (each row is an observation) of *M* features
    (each column is a feature, also called variable or measurement); the data live
    in <math alttext="double-struck upper R Superscript upper M"><msup><mi>ℝ</mi>
    <mi>M</mi></msup></math> and comprise *N* vectors or coordinates.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个 N 维数据集存在于 N 维空间中，其中每个数据点是该空间中的一个坐标。当您考虑将数据存储在一个矩阵中时，矩阵具有 *N* 个观测（每行是一个观测）和
    *M* 个特征（每列是一个特征，也称为变量或测量）；数据存在于<math alttext="double-struck upper R Superscript
    upper M"><msup><mi>ℝ</mi> <mi>M</mi></msup></math>，并且包含 *N* 个向量或坐标。
- en: An example in 2D is shown in [Figure 15-1](#fig_15_1). The left-side panel shows
    the data in its original data space, in which each variable provides a basis vector
    for the data. Clearly the two variables (the x- and y-axes) are related to each
    other, and clearly there is a direction in the data that captures that relation
    better than either of the feature basis vectors.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维示例中，如[图 15-1](#fig_15_1)所示。左侧面板显示了数据在其原始数据空间中的情况，其中每个变量为数据提供了一个基向量。显然，两个变量（x
    和 y 轴）彼此相关，并且数据中有一个方向能够更好地捕捉到这种关系，超过了任一特征基向量。
- en: '![PCA](assets/plad_1501.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![PCA](assets/plad_1501.png)'
- en: Figure 15-1\. Graphical overview of PCA in 2D
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-1。PCA 在二维中的图形概述
- en: The goal of PCA is to find a new set of basis vectors such that the linear relationships
    across the variables are maximally aligned with the basis vectors—that’s what
    the right-side panel of [Figure 15-1](#fig_15_1) shows. Importantly, PCA has the
    constraint that the new basis vectors are orthogonal rotations of the original
    basis vectors. In the exercises, you will see the implications of this constraint.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的目标是找到一组新的基向量，使得变量之间的线性关系与基向量最大地对齐——这正是[图 15-1](#fig_15_1)右侧面板所展示的内容。重要的是，PCA
    有一个约束条件，即新的基向量必须是原始基向量的正交旋转。在练习中，您将看到这个约束条件的影响。
- en: In the next section, I will introduce the math and procedures for computing
    PCA; in the exercises, you will have the opportunity to implement PCA using eigendecomposition
    and SVD, and compare your results against Python’s implementation of PCA.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我将介绍计算 PCA 的数学和过程；在练习中，您将有机会使用特征分解和奇异值分解实现 PCA，并将结果与 Python 实现的 PCA
    进行比较。
- en: The Math of PCA
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 的数学基础
- en: PCA combines the statistical concept of variance with the linear algebra concept
    of linear weighted combination. Variance, as you know, is a measure of the dispersion
    of a dataset around its average value. PCA makes the assumption that variance
    is good, and directions in the data space that have more variance are more important
    (a.k.a. “variance = relevance”).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 结合了方差的统计概念和线性代数中的线性加权组合概念。方差是数据集围绕其平均值的离散程度的度量。PCA 假设方差是有益的，并且在数据空间中具有更多方差的方向更重要（也就是“方差=相关性”）。
- en: But in PCA, we’re not just interested in the variance *within* one variable;
    instead, we want to find the linear weighted combination *across* all variables
    that maximizes variance of that component (a *component* is a linear weighted
    combination of variables).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但在PCA中，我们不仅仅关注一个变量内的方差；相反，我们希望找到跨所有变量的线性加权组合，使得该成分的方差最大化（一个*成分*是变量的线性加权组合）。
- en: 'Let’s write this down in math. Matrix <math alttext="bold upper X"><mi>𝐗</mi></math>
    is our data matrix (a tall full column-rank matrix of observations by features),
    and <math alttext="bold w"><mi>𝐰</mi></math> is the vector of weights. Our goal
    in PCA is to find the set of weights in <math alttext="bold w"><mi>𝐰</mi></math>
    such that <math alttext="bold upper X bold w"><mrow><mi>𝐗</mi> <mi>𝐰</mi></mrow></math>
    has maximal variance. Variance is a scalar, so we can write that down as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用数学方式表达这一点。矩阵 <math alttext="bold upper X"><mi>𝐗</mi></math> 是我们的数据矩阵（一个高的、全列秩的观测特征矩阵），<math
    alttext="bold w"><mi>𝐰</mi></math> 是权重向量。我们在PCA中的目标是找到一组权重 <math alttext="bold
    w"><mi>𝐰</mi></math> ，使得 <math alttext="bold upper X bold w"><mrow><mi>𝐗</mi>
    <mi>𝐰</mi></mrow></math> 的方差最大化。方差是一个标量，因此我们可以写下如下方程：
- en: <math alttext="lamda equals parallel-to bold upper X bold w parallel-to" display="block"><mrow><mi>λ</mi>
    <mo>=</mo> <msup><mrow><mo>∥</mo><mi>𝐗</mi><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda equals parallel-to bold upper X bold w parallel-to" display="block"><mrow><mi>λ</mi>
    <mo>=</mo> <msup><mrow><mo>∥</mo><mi>𝐗</mi><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: The squared vector norm is actually the same thing as variance when the data
    is mean-centered (that is, each data variable has a mean of zero);^([1](ch15.xhtml#idm45733290809152))
    I’ve omitted a scaling factor of 1/(N − 1), because it does not affect the solution
    to our optimization goal.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据均值为零（即，每个数据变量的均值为零）时，平方向量范数实际上与方差相同；^([1](ch15.xhtml#idm45733290809152))
    我们省略了缩放因子 1/(N − 1)，因为它不会影响我们优化目标的解决方案。
- en: 'The problem with that equation is you can simply set <math alttext="bold w"><mi>𝐰</mi></math>
    to be HUGE numbers; the bigger the weights, the larger the variance. The solution
    is to scale the norm of the weighted combination of data variables by the norm
    of the weights:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程的问题在于你可以简单地将 <math alttext="bold w"><mi>𝐰</mi></math> 设置为巨大的数；权重越大，方差越大。解决方案是通过数据变量的加权组合的范数来缩放权重的范数：
- en: <math alttext="lamda equals StartFraction parallel-to bold upper X bold w parallel-to
    Over parallel-to bold w parallel-to EndFraction" display="block"><mrow><mi>λ</mi>
    <mo>=</mo> <mfrac><msup><mrow><mo>∥</mo><mi>𝐗</mi><mi>𝐰</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup> <msup><mrow><mo>∥</mo><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mfrac></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda equals StartFraction parallel-to bold upper X bold w parallel-to
    Over parallel-to bold w parallel-to EndFraction" display="block"><mrow><mi>λ</mi>
    <mo>=</mo> <mfrac><msup><mrow><mo>∥</mo><mi>𝐗</mi><mi>𝐰</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup> <msup><mrow><mo>∥</mo><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mfrac></mrow></math>
- en: 'Now we have a ratio of two vector norms. We can expand those norms into dot
    products to gain some insight into the equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个向量范数的比率。我们可以将这些范数扩展为点积，以获得对方程的一些见解：
- en: <math alttext="StartLayout 1st Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper X Superscript upper T Baseline
    bold upper X bold w Over bold w Superscript upper T Baseline bold w EndFraction
    2nd Row 1st Column bold upper C 2nd Column equals bold upper X Superscript upper
    T Baseline bold upper X 3rd Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper C bold w Over bold w Superscript
    upper T Baseline bold w EndFraction EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <msup><mi>𝐗</mi> <mtext>T</mtext></msup>
    <mi>𝐗</mi><mi>𝐰</mi></mrow> <mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐰</mi></mrow></mfrac></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi>𝐂</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>𝐗</mi> <mtext>T</mtext></msup> <mi>𝐗</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mi>λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐂</mi><mi>𝐰</mi></mrow>
    <mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐰</mi></mrow></mfrac></mrow></mtd></mtr></mtable></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper X Superscript upper T Baseline
    bold upper X bold w Over bold w Superscript upper T Baseline bold w EndFraction
    2nd Row 1st Column bold upper C 2nd Column equals bold upper X Superscript upper
    T Baseline bold upper X 3rd Row 1st Column lamda 2nd Column equals StartFraction
    bold w Superscript upper T Baseline bold upper C bold w Over bold w Superscript
    upper T Baseline bold w EndFraction EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <msup><mi>𝐗</mi> <mtext>T</mtext></msup>
    <mi>𝐗</mi><mi>𝐰</mi></mrow> <mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐰</mi></mrow></mfrac></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi>𝐂</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>𝐗</mi> <mtext>T</mtext></msup> <mi>𝐗</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mi>λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐂</mi><mi>𝐰</mi></mrow>
    <mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐰</mi></mrow></mfrac></mrow></mtd></mtr></mtable></math>
- en: We’ve now discovered that the solution to PCA is the same as the solution to
    finding the directional vector that maximizes the *normalized* quadratic form
    (the vector norm is the normalization term) of the data covariance matrix.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们发现PCA的解法与找到最大化数据协方差矩阵的*标准化*二次形式的方向向量相同。
- en: That’s all fine, but how do we actually find the elements in vector <math alttext="bold
    w"><mi>𝐰</mi></math> that maximize <math alttext="lamda"><mi>λ</mi></math> ?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，但我们如何实际找到向量 <math alttext="bold w"><mi>𝐰</mi></math> 中的元素来最大化 <math alttext="lamda"><mi>λ</mi></math>
    ？
- en: 'The linear algebra approach here is to consider not just a single vector solution
    but an entire set of solutions. Thus, we rewrite the equation using matrix <math
    alttext="bold upper W"><mi>𝐖</mi></math> instead of vector <math alttext="bold
    w"><mi>𝐰</mi></math> . That would give a matrix in the denominator, which is not
    a valid operation in linear algebra; therefore, we multiply by the inverse:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数的方法是考虑不只是单一向量解，而是整个解集。因此，我们使用矩阵 <math alttext="bold upper W"><mi>𝐖</mi></math>
    而不是向量 <math alttext="bold w"><mi>𝐰</mi></math> 重写方程。这将给出一个在线性代数中不合法的矩阵作为分母；因此，我们通过其逆矩阵乘以：
- en: <math alttext="bold upper Lamda equals left-parenthesis bold upper W Superscript
    upper T Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C bold upper W" display="block"><mrow><mi
    mathvariant="bold">Λ</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Lamda equals left-parenthesis bold upper W Superscript
    upper T Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C bold upper W" display="block"><mrow><mi
    mathvariant="bold">Λ</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></math>
- en: 'From here, we apply some algebra and see what happens:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们应用一些代数，看看会发生什么：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper W right-parenthesis
    Superscript negative 1 Baseline bold upper W Superscript upper T Baseline bold
    upper C bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold
    upper W Superscript negative 1 Baseline bold upper W Superscript minus upper T
    Baseline bold upper W Superscript upper T Baseline bold upper C bold upper W 3rd
    Row 1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C bold upper W 4th Row 1st Column bold upper W bold upper
    Lamda 2nd Column equals bold upper C bold upper W EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi mathvariant="bold">Λ</mi></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mrow><mo>-</mo><mtext>T</mtext></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi
    mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Λ</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper W right-parenthesis
    Superscript negative 1 Baseline bold upper W Superscript upper T Baseline bold
    upper C bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold
    upper W Superscript negative 1 Baseline bold upper W Superscript minus upper T
    Baseline bold upper W Superscript upper T Baseline bold upper C bold upper W 3rd
    Row 1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C bold upper W 4th Row 1st Column bold upper W bold upper
    Lamda 2nd Column equals bold upper C bold upper W EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi mathvariant="bold">Λ</mi></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi>
    <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mrow><mo>-</mo><mtext>T</mtext></mrow></msup> <msup><mi mathvariant="bold">W</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi
    mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Λ</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi mathvariant="bold">C</mi> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
- en: Remarkably, we’ve discovered that the solution to PCA is to perform an eigendecomposition
    on the data covariance matrix. The eigenvectors are the weights for the data variables,
    and their corresponding eigenvalues are the variances of the data along each direction
    (each column of <math alttext="bold upper W"><mi>𝐖</mi></math> ).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们发现PCA的解法是对数据协方差矩阵执行特征分解。特征向量是数据变量的权重，它们对应的特征值是数据沿每个方向的方差（<math alttext="bold
    upper W"><mi>𝐖</mi></math> 的每一列）。
- en: Because covariance matrices are symmetric, their eigenvectors—and therefore
    principal components—are orthogonal. This has important implications for the appropriateness
    of PCA for data analysis, which you will discover in the exercises.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因为协方差矩阵是对称的，它们的特征向量——因此主成分——是正交的。这对PCA在数据分析中的适用性有重要的影响，你将在练习中发现。
- en: The Steps to Perform a PCA
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行PCA的步骤
- en: With the math out of the way, here are the steps to implement a PCA:^([2](ch15.xhtml#idm45733290657888))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数学已经清楚了，下面是实施PCA的步骤：
- en: Compute the covariance matrix of the data. The resulting covariance matrix will
    be features-by-features. Each feature in the data must be mean-centered prior
    to computing covariance.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据的协方差矩阵。得到的协方差矩阵将按特征-特征排列。在计算协方差之前，数据中的每个特征必须进行均值中心化。
- en: Take the eigendecomposition of that covariance matrix.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对该协方差矩阵进行特征值分解。
- en: Sort the eigenvalues descending by magnitude, and sort the eigenvectors accordingly.
    Eigenvalues of the PCA are sometimes called *latent factor scores*.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照大小降序排序特征值，并相应地排序特征向量。PCA的特征值有时被称为*潜在因子得分*。
- en: Compute the “component scores” as the weighted combination of all data features,
    where the eigenvector provides the weights. The eigenvector associated with the
    largest eigenvalue is the “most important” component, meaning the one with the
    largest variance.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算“成分得分”，作为所有数据特征的加权组合，其中特征向量提供权重。与最大特征值相关联的特征向量是“最重要”的成分，意味着它具有最大的方差。
- en: Convert the eigenvalues to percent variance explained to facilitate interpretation.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征值转换为百分比方差解释，以便于解释。
- en: PCA via SVD
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过SVD进行PCA
- en: 'PCA can equivalently be performed via eigendecomposition as previously described
    or via SVD. There are two ways to perform a PCA using SVD:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PCA可以通过前述的特征值分解或通过SVD等效地进行。使用SVD执行PCA有两种方法：
- en: Take the SVD of the covariance matrix. The procedure is identical to that previously
    described, because SVD and eigendecomposition are the same decomposition for covariance
    matrices.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对协方差矩阵进行SVD。该过程与先前描述的相同，因为SVD和特征值分解是协方差矩阵的相同分解方法。
- en: Take the SVD of the data matrix directly. In this case, the right singular vectors
    (matrix <math alttext="bold upper V"><mi>𝐕</mi></math> ) are equivalent to the
    eigenvectors of the covariance matrix (it would be the left singular vectors if
    the data matrix is stored as features-by-observations). The data must be mean-centered
    before computing the SVD. The square root of the singular values is equivalent
    to the eigenvalues of the covariance matrix.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接对数据矩阵进行SVD。在这种情况下，右奇异向量（矩阵 <math alttext="bold upper V"><mi>𝐕</mi></math>
    ）等价于协方差矩阵的特征向量（如果数据矩阵按特征-观察存储，则左奇异向量）。在计算SVD之前，数据必须进行均值中心化。奇异值的平方根等价于协方差矩阵的特征值。
- en: Should you use eigendecomposition or SVD to perform a PCA? You might think that
    SVD is easier because it does not require the covariance matrix. That’s true for
    relatively small and clean datasets. But larger or more complicated datasets may
    require data selection or may be too memory intensive to take the SVD of the entire
    data matrix. In these cases, computing the covariance matrix first can increase
    analysis flexibility. But the choice of eigendecomposition versus SVD is often
    a matter of personal preference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行PCA时，您应该使用特征值分解还是SVD？您可能认为SVD更容易，因为它不需要协方差矩阵。对于相对较小且干净的数据集，这是正确的。但对于更大或更复杂的数据集，可能需要数据选择，或者可能因为内存需求过高而无法直接对整个数据矩阵进行SVD。在这些情况下，先计算协方差矩阵可以增加分析的灵活性。但是选择特征值分解还是SVD通常是个人喜好的问题。
- en: Linear Discriminant Analysis
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: Linear discriminant analysis (LDA) is a multivariate classification technique
    that is often used in machine learning and statistics. It was initially developed
    by Ronald Fisher,^([3](ch15.xhtml#idm45733290633904)) who is often considered
    the “grandfather” of statistics for his numerous and important contributions to
    the mathematical foundations of statistics.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）是一种常用于机器学习和统计学中的多变量分类技术。最初由罗纳德·费舍尔开发，^([3](ch15.xhtml#idm45733290633904))他因其对统计学数学基础的众多重要贡献而常被称为统计学的“祖父”。
- en: The goal of LDA is to find a direction in the data space that maximally separates
    categories of data. An example problem dataset is shown in graph A in [Figure 15-2](#fig_15_2).
    It is visually obvious that the two categories are separable, but they are not
    separable on either of the data axes alone—that is clear from visual inspection
    of the marginal distributions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的目标是在数据空间中找到一个方向，最大化地分离数据的类别。图A中展示了一个示例问题数据集，[图 15-2](#fig_15_2)。从视觉上看，两个类别是可分离的，但在任何单个数据轴上它们都不可分离—这从边际分布的视觉检查中显而易见。
- en: Enter LDA. LDA will find basis vectors in the data space that maximally separate
    the two categories. Graph B in [Figure 15-2](#fig_15_2) shows the same data but
    in the LDA space. Now the classification is simple—observations with negative
    values on axis-1 are labeled category “0” and any observations with positive values
    on axis 1 are labeled category “1.” The data is completely inseparable on axis
    2, indicating that one dimension is sufficient for accurate categorization in
    this dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 进入LDA。LDA将在数据空间中找到一组基向量，使得两个类别能够最大化分离。图B在[LDA空间的图示](#fig_15_2)中展示了相同的数据。现在分类很简单——在轴-1上具有负值的观测标记为类别“0”，而在轴1上具有正值的观测标记为类别“1”。在轴2上，数据完全无法分离，这表明在这个数据集中，一个维度足以实现准确的分类。
- en: '![LDA](assets/plad_1502.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![LDA](assets/plad_1502.png)'
- en: Figure 15-2\. Example 2D problem for LDA
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-2\. LDA的二维问题示例
- en: Sounds great, right? But how does such a marvel of mathematics work? It’s actually
    fairly straightforward and based on generalized eigendecomposition, which you
    learned about toward the end of [Chapter 13](ch13.xhtml#Chapter_13).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来不错，对吧？但这样一种数学奇迹是如何运作的呢？事实上，它非常直接，基于广义特征分解，你在[第13章](ch13.xhtml#Chapter_13)末尾学习过这个方法。
- en: 'Let me begin with the objective function: our goal is to find a set of weights
    such that the weighted combination of variables maximally separates the categories.
    That objective function can be written similarly as with the PCA objective function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我从目标函数开始：我们的目标是找到一组权重，使得变量的加权组合能够最大化地将类别分开。该目标函数可以类似于PCA的目标函数进行表达：
- en: <math alttext="lamda equals StartFraction parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to Over parallel-to bold upper X Subscript upper
    W Baseline bold w parallel-to EndFraction" display="block"><mrow><mi>λ</mi> <mo>=</mo>
    <mfrac><mrow><mrow><mo>∥</mo></mrow><msub><mi>𝐗</mi> <mi>B</mi></msub> <msup><mrow><mi>𝐰</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><mrow><mo>∥</mo></mrow><msub><mi>𝐗</mi> <mi>W</mi></msub>
    <msup><mrow><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mfrac></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda equals StartFraction parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to Over parallel-to bold upper X Subscript upper
    W Baseline bold w parallel-to EndFraction" display="block"><mrow><mi>λ</mi> <mo>=</mo>
    <mfrac><mrow><mrow><mo>∥</mo></mrow><msub><mi>𝐗</mi> <mi>B</mi></msub> <msup><mrow><mi>𝐰</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><mrow><mo>∥</mo></mrow><msub><mi>𝐗</mi> <mi>W</mi></msub>
    <msup><mrow><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mfrac></mrow></math>
- en: 'In English: we want to find a set of feature weights <math alttext="bold w"><mi>𝐰</mi></math>
    that maximizes the *ratio* of the variance of data feature <math alttext="bold
    upper X Subscript upper B"><msub><mi>𝐗</mi> <mi>B</mi></msub></math> , to the
    variance of data feature <math alttext="bold upper X Subscript upper W"><msub><mi>𝐗</mi>
    <mi>W</mi></msub></math> . Notice that the same weights are applied to all data
    observations. (I’ll write more about data features *B* and *W* after discussing
    the math.)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的英语说，我们想找到一组特征权重<math alttext="bold w"><mi>𝐰</mi></math>，以最大化数据特征<math alttext="bold
    upper X Subscript upper B"><msub><mi>𝐗</mi> <mi>B</mi></msub></math>的方差与数据特征<math
    alttext="bold upper X Subscript upper W"><msub><mi>𝐗</mi> <mi>W</mi></msub></math>的方差的*比率*。注意，这些权重适用于所有数据观测。（在讨论完数学后，我会详细讨论数据特征*B*和*W*。）
- en: 'The linear algebra solution comes from following a similar argument as described
    in the PCA section. First, expand <math alttext="parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to"><mrow><mrow><mo>∥</mo></mrow> <msub><mi>𝐗</mi>
    <mi>B</mi></msub> <msup><mrow><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></math>
    to <math alttext="bold w Superscript upper T Baseline bold upper X Subscript upper
    B Superscript upper T Baseline bold upper X Subscript upper B Baseline bold w"><mrow><msup><mi>𝐰</mi>
    <mtext>T</mtext></msup> <msubsup><mi>𝐗</mi> <mi>B</mi> <mtext>T</mtext></msubsup>
    <msub><mi>𝐗</mi> <mi>B</mi></msub> <mi>𝐰</mi></mrow></math> and express this as
    <math alttext="bold w Superscript upper T Baseline bold upper C Subscript upper
    B Baseline bold w"><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <msub><mi>𝐂</mi>
    <mi>B</mi></msub> <mi>𝐰</mi></mrow></math> ; second, consider a set of solutions
    instead of one solution; third, replace the division with multiplication of the
    inverse; and finally, do some algebra and see what happens:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数的解决方案源于与PCA部分描述类似的论证。首先，将<math alttext="parallel-to bold upper X Subscript
    upper B Baseline bold w parallel-to"><mrow><mrow><mo>∥</mo></mrow> <msub><mi>𝐗</mi>
    <mi>B</mi></msub> <msup><mrow><mi>𝐰</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></math>扩展为<math
    alttext="bold w Superscript upper T Baseline bold upper X Subscript upper B Superscript
    upper T Baseline bold upper X Subscript upper B Baseline bold w"><mrow><msup><mi>𝐰</mi>
    <mtext>T</mtext></msup> <msubsup><mi>𝐗</mi> <mi>B</mi> <mtext>T</mtext></msubsup>
    <msub><mi>𝐗</mi> <mi>B</mi></msub> <mi>𝐰</mi></mrow></math>，并将其表示为<math alttext="bold
    w Superscript upper T Baseline bold upper C Subscript upper B Baseline bold w"><mrow><msup><mi>𝐰</mi>
    <mtext>T</mtext></msup> <msub><mi>𝐂</mi> <mi>B</mi></msub> <mi>𝐰</mi></mrow></math>；其次，考虑一组解而不是一个解；第三，将除法替换为逆的乘法；最后，进行一些代数运算并观察结果：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper C Subscript
    upper W Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C Subscript upper B Baseline
    bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold upper
    W Superscript negative 1 Baseline bold upper C Subscript upper W Superscript negative
    1 Baseline bold upper W Superscript minus upper T Baseline bold upper W Superscript
    upper T Baseline bold upper C Subscript upper B Baseline bold upper W 3rd Row
    1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C Subscript upper W Superscript negative 1 Baseline bold
    upper C Subscript upper B Baseline bold upper W 4th Row 1st Column bold upper
    W bold upper Lamda 2nd Column equals bold upper C Subscript upper W Superscript
    negative 1 Baseline bold upper C Subscript upper B Baseline bold upper W 5th Row
    1st Column bold upper C Subscript upper W Baseline bold upper W bold upper Lamda
    2nd Column equals bold upper C Subscript upper B Baseline bold upper W EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>W</mi></msub> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mtext>T</mtext></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Λ</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi mathvariant="bold">C</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi
    mathvariant="bold">C</mi> <mi>W</mi></msub> <mi mathvariant="bold">W</mi> <mi
    mathvariant="bold">Λ</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper Lamda 2nd Column equals
    left-parenthesis bold upper W Superscript upper T Baseline bold upper C Subscript
    upper W Baseline bold upper W right-parenthesis Superscript negative 1 Baseline
    bold upper W Superscript upper T Baseline bold upper C Subscript upper B Baseline
    bold upper W 2nd Row 1st Column bold upper Lamda 2nd Column equals bold upper
    W Superscript negative 1 Baseline bold upper C Subscript upper W Superscript negative
    1 Baseline bold upper W Superscript minus upper T Baseline bold upper W Superscript
    upper T Baseline bold upper C Subscript upper B Baseline bold upper W 3rd Row
    1st Column bold upper Lamda 2nd Column equals bold upper W Superscript negative
    1 Baseline bold upper C Subscript upper W Superscript negative 1 Baseline bold
    upper C Subscript upper B Baseline bold upper W 4th Row 1st Column bold upper
    W bold upper Lamda 2nd Column equals bold upper C Subscript upper W Superscript
    negative 1 Baseline bold upper C Subscript upper B Baseline bold upper W 5th Row
    1st Column bold upper C Subscript upper W Baseline bold upper W bold upper Lamda
    2nd Column equals bold upper C Subscript upper B Baseline bold upper W EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>W</mi></msub> <mi mathvariant="bold">W</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msup><mi mathvariant="bold">W</mi> <mrow><mo>-</mo><mtext>T</mtext></mrow></msup>
    <msup><mi mathvariant="bold">W</mi> <mtext>T</mtext></msup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msubsup><mi
    mathvariant="bold">C</mi> <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">W</mi> <mi mathvariant="bold">Λ</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi mathvariant="bold">C</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="bold">C</mi>
    <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi
    mathvariant="bold">C</mi> <mi>W</mi></msub> <mi mathvariant="bold">W</mi> <mi
    mathvariant="bold">Λ</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi mathvariant="bold">C</mi> <mi>B</mi></msub> <mi mathvariant="bold">W</mi></mrow></mtd></mtr></mtable></math>
- en: In other words, the solution to LDA comes from a generalized eigendecomposition
    on two covariance matrices. The eigenvectors are the weights, and the generalized
    eigenvalues are the variance ratios of each component.^([4](ch15.xhtml#idm45733290531744))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，LDA的解决方案来自于对两个协方差矩阵进行广义特征分解。特征向量是权重，广义特征值是每个分量的方差比率。^([4](ch15.xhtml#idm45733290531744))
- en: With the math out of the way, which data features are used to construct <math
    alttext="bold upper X Subscript upper B"><msub><mi>𝐗</mi> <mi>B</mi></msub></math>
    and <math alttext="bold upper X Subscript upper W"><msub><mi>𝐗</mi> <mi>W</mi></msub></math>
    ? Well, there are different ways of implementing that formula, depending on the
    nature of the problem and the specific goal of the analysis. But in a typical
    LDA model, the <math alttext="bold upper X Subscript upper B"><msub><mi>𝐗</mi>
    <mi>B</mi></msub></math> comes from the between-category covariance while the
    <math alttext="bold upper X Subscript upper W"><msub><mi>𝐗</mi> <mi>W</mi></msub></math>
    comes from the within-category covariance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数学部分已经搞定，用来构建<math alttext="bold upper X Subscript upper B"><msub><mi>𝐗</mi>
    <mi>B</mi></msub></math>和<math alttext="bold upper X Subscript upper W"><msub><mi>𝐗</mi>
    <mi>W</mi></msub></math>的数据特征是什么？嗯，根据问题的性质和分析的具体目标，实施该公式有不同的方式。但在典型的LDA模型中，<math
    alttext="bold upper X Subscript upper B"><msub><mi>𝐗</mi> <mi>B</mi></msub></math>来自于类间协方差，而<math
    alttext="bold upper X Subscript upper W"><msub><mi>𝐗</mi> <mi>W</mi></msub></math>来自于类内协方差。
- en: The within-category covariance is simply the average of the covariances of the
    data samples within each class. The between-category covariance comes from creating
    a new data matrix comprising the feature averages within each class. I will walk
    you through the procedure in the exercises. If you are familiar with statistics,
    then you’ll recognize this formulation as analogous to the ratio of between-group
    to within-group sum of squared errors in ANOVA models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在类内协方差简单来说是数据样本在每个类别内协方差的平均值。类间协方差来自于创建一个新的数据矩阵，其中包括每个类别内的特征平均值。我将在练习中为您讲解这个过程。如果您熟悉统计学，那么您会认识到这种表述类似于ANOVA模型中组间平方误差与组内平方误差比率的形式。
- en: 'Two final comments: The eigenvectors of a generalized eigendecomposition are
    not constrained to be orthogonal. That’s because <math alttext="bold upper C Subscript
    upper W Superscript negative 1 Baseline bold upper C Subscript upper B"><mrow><msubsup><mi>𝐂</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi>𝐂</mi> <mi>B</mi></msub></mrow></math>
    is generally not a symmetric matrix even though the two covariance matrices are
    themselves symmetric. Nonsymmetric matrices do not have the orthogonal-eigenvector
    constraint. You’ll see this in the exercises.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两点说明：广义特征分解的特征向量不受强制要求是正交的。这是因为<math alttext="bold upper C Subscript upper
    W Superscript negative 1 Baseline bold upper C Subscript upper B"><mrow><msubsup><mi>𝐂</mi>
    <mi>W</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi>𝐂</mi> <mi>B</mi></msub></mrow></math>通常不是对称矩阵，尽管两个协方差矩阵本身是对称的。非对称矩阵没有正交特征向量的约束。您将在练习中看到这一点。
- en: Finally, LDA will always find a *linear* solution (duh, that’s right in the
    name *L*DA), even if the data is *not* linearly separable. Nonlinear separation
    would require a transformation of the data or the use of a nonlinear categorization
    method like artificial neural networks. LDA will still work in the sense of producing
    a result; it’s up to you as the data scientist to determine whether that result
    is appropriate and interpretable for a given problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LDA（线性判别分析）将始终找到一个*线性*解决方案（当然，这在*L*DA的名称中就已经明确了），即使数据*不*是线性可分的。非线性分离将需要对数据进行变换或使用像人工神经网络这样的非线性分类方法。LDA在产生结果方面仍然有效；作为数据科学家，您需要确定该结果是否适合和可解释于给定问题。
- en: Low-Rank Approximations via SVD
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过SVD进行低秩逼近
- en: I explained the concept of low-rank approximations in the previous chapter (e.g.,
    [Exercise 14-5](ch14.xhtml#exercise_14_5)). The idea is to take the SVD of a data
    matrix or image, and then reconstruct that data matrix using some subset of SVD
    components.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一章解释了低秩逼近的概念（例如，[练习 14-5](ch14.xhtml#exercise_14_5)）。其思想是对数据矩阵或图像进行SVD分解，然后使用SVD分量的某个子集重构该数据矩阵。
- en: You can achieve this by setting selected <math alttext="sigma"><mi>σ</mi></math>
    s to equal zero or by creating new SVD matrices that are rectangular, with the
    to-be-rejected vectors and singular values removed. This second approach is preferred
    because it reduces the sizes of the data to be stored, as you will see in the
    exercises. In this way, the SVD can be used to compress data down to a smaller
    size.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将选定的<math alttext="sigma"><mi>σ</mi></math>设置为零，或创建新的SVD矩阵（这些矩阵是矩形的，移除要被拒绝的向量和奇异值）来实现这一点。第二种方法更为推荐，因为它减少了要存储的数据大小，您将在练习中看到这一点。通过这种方式，SVD可以用于将数据压缩到较小的大小。
- en: SVD for Denoising
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVD用于去噪
- en: Denoising via SVD is simply an application of low-rank approximation. The only
    difference is that SVD components are selected for exclusion based on them representing
    noise as opposed to making small contributions to the data matrix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过SVD进行去噪仅仅是低秩逼近的一个应用。唯一的区别在于，SVD的组成部分被选择排除，因为它们代表噪声，而不是对数据矩阵作出小贡献。
- en: The to-be-removed components might be layers associated with the smallest singular
    values—that would be the case for low-amplitude noise associated with small equipment
    imperfections. But larger sources of noise that have a stronger impact on the
    data might have larger singular values. These noise components can be identified
    by an algorithm based on their characteristics or by visual inspection. In the
    exercises, you will see an example of using SVD to separate a source of noise
    that was added to an image.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 待移除的组件可能是与最小奇异值相关联的层—这在与小型设备不完美相关的低振幅噪声的情况下是这样。但对数据影响更大的较大噪声源可能具有较大的奇异值。这些噪声组件可以通过基于它们特征的算法或视觉检查来识别。在练习中，您将看到使用SVD分离添加到图像中的噪声源的示例。
- en: Summary
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You’ve made it to the end of the book (except for the exercises below)! Congrats!
    Take a moment to be proud of yourself and your commitment to learning and investing
    in your brain (it is, after all, your most precious resource). I am proud of you,
    and if we would meet in person, I’d give you a high five, fist bump, elbow tap,
    or whatever is socially/medically appropriate at the time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经读完了本书的内容（除了下面的练习）！请花点时间为自己和你对学习和投资大脑的承诺感到自豪（毕竟，这是你最宝贵的资源）。我为你感到骄傲，如果我们能见面，我会和你击掌、拳头碰、肘部碰或者在当时社会/医学上合适的方式表示祝贺。
- en: 'I hope you feel that this chapter helped you see the incredible importance
    of eigendecomposition and singular value decomposition to applications in statistics
    and machine learning. Here’s a summary of the key points that I am contractually
    obligated to include:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你觉得本章帮助你看到特征分解和奇异值分解在统计学和机器学习应用中的重要性。这里是我必须包括的关键点总结：
- en: The goal of PCA is to find a set of weights such that the linear weighted combination
    of data features has maximal variance. That goal reflects the assumption underlying
    PCA, which is that “variance equals relevance.”
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA的目标是找到一组权重，使得数据特征的线性加权组合具有最大方差。这个目标反映了PCA的基本假设，“方差等于相关性”。
- en: PCA is implemented as the eigendecomposition of a data covariance matrix. The
    eigenvectors are the feature weightings, and the eigenvalues can be scaled to
    encode percent variance accounted for by each component (a *component* is the
    linear weighted combination).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA作为数据协方差矩阵的特征分解实现。特征向量是特征权重，而特征值可以缩放以编码每个组件所占的百分比方差（一个*组件*是线性加权组合）。
- en: PCA can be equivalently implemented using the SVD of the covariance matrix or
    the data matrix.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA可以等效地使用协方差矩阵或数据矩阵的SVD实现。
- en: 'Linear discriminant analysis (LDA) is used for linear categorization of multivariable
    data. It can be seen as an extension of PCA: whereas PCA maximizes variance, LDA
    maximizes the ratio of variances between two data features.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）用于多变量数据的线性分类。可以看作是PCA的扩展：PCA最大化方差，而LDA最大化两个数据特征之间的方差比。
- en: LDA is implemented as a generalized eigendecomposition on two covariance matrices
    that are formed from two different data features. The two data features are often
    the between-class covariance (to maximize) and the within-class covariance (to
    minimize).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA作为两个协方差矩阵的广义特征分解实现，这些矩阵由两个不同的数据特征形成。这两个数据特征通常是类间协方差（要最大化）和类内协方差（要最小化）。
- en: Low-rank approximations involve reproducing a matrix from a subset of singular
    vectors/values and are used for data compression and denoising.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低秩逼近涉及从奇异向量/值的子集中复制矩阵，并用于数据压缩和去噪。
- en: For data compression, the components associated with the smallest singular values
    are removed; for data denoising, components that capture noise or artifacts are
    removed (their corresponding singular values could be small or large).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数据压缩，与最小奇异值相关联的组件被移除；对于数据去噪，捕捉噪声或伪影的组件被移除（它们对应的奇异值可能很小或很大）。
- en: Exercises
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: PCA
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: I love Turkish coffee. It’s made with very finely ground beans and no filter.
    The whole ritual of making it and drinking it is wonderful. And if you drink it
    with a Turk, perhaps you can have your fortune read.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢土耳其咖啡。它用非常细磨的豆子制成，没有过滤器。整个制作和享用过程都很美妙。如果你和土耳其人一起喝，也许你可以算算你的命运。
- en: This exercise is not about Turkish coffee, but it is about doing a PCA on a
    dataset^([5](ch15.xhtml#idm45733290481824)) that contains time series data from
    the Istanbul stock exchange, along with stock exchange data from several other
    stock indices in different countries. We could use this dataset to ask, for example,
    whether the international stock exchanges are driven by one common factor, or
    whether different countries have independent financial markets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习不是关于土耳其咖啡，而是关于对包含来自伊斯坦布尔证券交易所的时间序列数据以及来自其他几个国家不同股票指数的股票交易数据的数据集进行PCA^([5](ch15.xhtml#idm45733290481824))。我们可以使用这个数据集来询问，例如国际股票交易是否由全球经济的一个共同因素驱动，或者不同国家是否拥有独立的金融市场。
- en: Exercise 15-1\.
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-1\.
- en: Before performing a PCA, import and inspect the data. I made several plots of
    the data shown in [Figure 15-3](#fig_15_3); you are welcome to reproduce these
    plots and/or use different methods to explore the data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行PCA之前，请导入并检查数据。我对数据进行了多个绘图，显示在[图 15-3](#fig_15_3)中；欢迎您重现这些图表和/或使用不同方法探索数据。
- en: '![exercise 15-1](assets/plad_1503.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![exercise 15-1](assets/plad_1503.png)'
- en: Figure 15-3\. Some investigations of the international stock exchange dataset
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-3\. 对国际股票交易所数据集的一些调查
- en: 'Now for the PCA. Implement the PCA using the five steps outlined earlier in
    this chapter. Visualize the results as in [Figure 15-4](#fig_15_4). Use code to
    demonstrate several features of PCA:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行PCA。按照本章前面提到的五个步骤实施PCA。像[图 15-4](#fig_15_4)一样可视化结果。使用代码展示PCA的几个特性：
- en: 'The variance of the component time series (using `np.var`) equals the eigenvalue
    associated with that component. You can see the results for the first two components
    here:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成分时间序列的方差（使用 `np.var`）等于与该成分相关联的特征值。您可以在这里查看前两个成分的结果：
- en: '[PRE0]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The correlation between principal components (that is, the weighted combinations
    of the stock exchanges) 1 and 2 is zero, i.e., orthogonal.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主成分之间的相关性（即股票交易所的加权组合）1和2为零，即正交。
- en: Visualize the eigenvector weights for the first two components. The weights
    show how much each variable contributes to the component.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化前两个成分的特征向量权重。权重显示每个变量对成分的贡献程度。
- en: '![ex0.](assets/plad_1504.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![ex0.](assets/plad_1504.png)'
- en: Figure 15-4\. Results of PCA on the Instanbul Stock Exchange dataset
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-4\. 伊斯坦布尔证券交易所数据集的PCA结果
- en: '**Discussion:** The scree plot strongly suggests that the international stock
    exchanges are driven by a common factor of the global economy: there is one large
    component that accounts for around 64% of the variance in the data, while the
    other components each account for less than 15% of the variance (in a purely random
    dataset we would expect each component to account for 100/9 = 11% of the variance,
    plus/minus noise).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论：** 屏风图强烈表明国际股票交易所受全球经济的一个共同因素驱动：有一个大成分解释了数据中约64%的方差，而其他成分每个都解释了不到15%的方差（在纯随机数据集中，我们预计每个成分解释100/9
    = 11%的方差，加减噪声）。'
- en: A rigorous evaluation of the statistical significance of these components is
    outside the scope of this book, but based on visual inspection of the scree plot,
    we are not really justified to interpret the components after the first one; it
    appears that most of the variance in this dataset fits neatly into one dimension.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些成分的统计显著性进行严格评估超出了本书的范围，但基于对屏风图的视觉检查，我们并不能完全有理由解释第一个成分之后的成分；似乎这个数据集的大部分方差都整齐地适合一个维度。
- en: From the perspective of dimensionality reduction, we could reduce the entire
    dataset to the component associated with the largest eigenvalue (this is often
    called the *top component*), thereby representing this 9D dataset using a 1D vector.
    Of course, we lose information—36% of the information in the dataset is removed
    if we focus only on the top component—but hopefully, the important features of
    the signal are in the top component while the less important features, including
    random noise, are ignored.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从降维的角度来看，我们可以将整个数据集减少到与最大特征值相关联的分量（通常称为*顶部分量*），从而使用1D向量表示这个9D数据集。当然，我们会失去信息——如果我们只关注顶部分量，数据集中36%的信息会被移除——但希望信号的重要特征位于顶部分量中，而不重要的特征，包括随机噪声，则被忽略了。
- en: Exercise 15-2\.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习15-2\.
- en: Reproduce the results using (1) the SVD of the data covariance matrix and (2)
    the SVD of the data matrix itself. Remember that the eigenvalues of <math alttext="bold
    upper X Superscript upper T Baseline bold upper X"><mrow><msup><mi>𝐗</mi> <mtext>T</mtext></msup>
    <mi>𝐗</mi></mrow></math> are the squared singular values of <math alttext="bold
    upper X"><mi>𝐗</mi></math> ; furthermore, the scaling factor on the covariance
    matrix must be applied to the singular values to find equivaluence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过（1）数据协方差矩阵的SVD和（2）数据矩阵本身的SVD来重现结果。请记住<math alttext="bold upper X Superscript
    upper T Baseline bold upper X"><mrow><msup><mi>𝐗</mi> <mtext>T</mtext></msup>
    <mi>𝐗</mi></mrow></math>的特征值是<math alttext="bold upper X"><mi>𝐗</mi></math>的平方奇异值；此外，必须将协方差矩阵上的缩放因子应用于奇异值以找到等价性。
- en: Exercise 15-3\.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习15-3\.
- en: 'Compare your “manual” PCA with the output of Python’s PCA routine. You’ll have
    to do some online research to figure out how to run a PCA in Python (this is one
    of the most important skills in Python programming!), but I’ll give you a hint:
    it’s in the sklearn.decomposition library.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的“手动”PCA与Python的PCA例程的输出进行比较。你需要进行一些在线研究，以找出如何在Python中运行PCA（这是Python编程中最重要的技能之一！），但我会给你一个提示：它在sklearn.decomposition库中。
- en: sklearn or Manual Implementation of PCA?
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: sklearn还是手动实现PCA？
- en: Should you compute PCA by writing code to compute and eigendecompose the covariance
    matrix or use sklearn’s implementation? There is always a trade-off between using
    your own code to maximize customization versus using prepackaged code to maximize
    ease. One of the myriad and amazing benefits of understanding the math behind
    data science analyses is that you can custom tailor analyses to suit your needs.
    In my own research, I find that implementing PCA on my own gives me more freedom
    and flexibility.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 应该通过编写代码来计算和特征分解协方差矩阵来计算PCA，还是使用sklearn的实现？始终存在使用自己的代码以最大程度定制与使用预包装代码以最大程度便捷之间的权衡。理解数据科学分析背后的数学的无数和令人惊奇的好处之一是，你可以定制分析以满足你的需求。在我的研究中，我发现自己实现PCA能给我更多的自由和灵活性。
- en: Exercise 15-4\.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习15-4\.
- en: Now you will perform a PCA on simulated data, which will highlight one of the
    potential limitations of PCA. The goal is to create a dataset comprising two “streams”
    of data and plot the principal components on top, like in [Figure 15-5](#fig_15_5).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你将在模拟数据上执行PCA，这将突显PCA的潜在限制之一。目标是创建一个包含两个“流”数据的数据集，并在顶部绘制主成分，就像[图15-5](#fig_15_5)中一样。
- en: '![exercise 15-4](assets/plad_1505.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![练习15-4](assets/plad_1505.png)'
- en: Figure 15-5\. Results from Exercise 15-4
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-5\. 练习15-4的结果
- en: 'Here’s how to create the data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是创建数据的方法：
- en: Create a 1,000 × 2 matrix of random numbers drawn from a normal (Gaussian) distribution
    in which the second column is scaled down by .05.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个大小为1,000 × 2的矩阵，其中的随机数是从正态（高斯）分布中抽取的，其中第二列的数值缩小了0.05倍。
- en: Create a 2 × 2 pure rotation matrix (see [Chapter 7](ch07.xhtml#Chapter_7)).
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个2 × 2的纯旋转矩阵（参见[第7章](ch07.xhtml#Chapter_7)）。
- en: 'Stack two copies of the data vertically: once with the data rotated by <math
    alttext="theta"><mi>θ</mi></math> = −*π*/6, and once with the data rotated by
    <math alttext="theta"><mi>θ</mi></math> = −*π*/3\. The resulting data matrix will
    be of size 2,000 × 2.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 垂直堆叠两份数据副本：一份是数据按角度<math alttext="theta"><mi>θ</mi></math> = −*π*/6旋转，另一份是按角度<math
    alttext="theta"><mi>θ</mi></math> = −*π*/3旋转。得到的数据矩阵大小为2,000 × 2。
- en: Use SVD to implement the PCA. I scaled the singular vectors by a factor of 2
    for visual inspection.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SVD实现PCA。我将奇异向量的尺度放大了2倍以进行视觉检查。
- en: '**Discussion:** PCA is excellent for reducing dimensionality of a high-dimensional
    dataset. This can facilitate data compression, data cleaning, and numerical stability
    issues (e.g., imagine that a 200-dimensional dataset with a condition number of
    10^(10) is reduced to the largest 100 dimensions with a condition number of 10⁵).
    But the dimensions themselves may be poor choices for feature extraction, because
    of the orthogonality constraint. Indeed, the principal directions of variance
    in [Figure 15-5](#fig_15_5) are correct in a mathematical sense, but I’m sure
    you have the feeling that those are not the best basis vectors to capture the
    features of the data.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论：** 主成分分析（PCA）非常适合降低高维数据集的维度。这可以促进数据压缩、数据清理和数值稳定性问题（例如，想象一个具有条件数 10^(10)
    的 200 维数据集，将其降低到具有条件数 10⁵ 的最大 100 维）。但是，由于正交性约束，维度本身可能不是提取特征的最佳选择。确实，在 [图 15-5](#fig_15_5)
    中方差的主要方向在数学上是正确的，但我相信您会感觉到这些不是捕获数据特征的最佳基向量。'
- en: Linear Discriminant Analyses
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性判别分析。
- en: Exercise 15-5\.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-5。
- en: You are going to perform an LDA on simulated 2D data. Simulated data is advantageous
    because you can manipulate the effects sizes, amount and nature of noise, number
    of categories, and so on.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在模拟的 2D 数据上执行 LDA。模拟数据具有优势，因为您可以操纵效应大小、噪声的数量和性质、类别数量等。
- en: The data you will create was shown in [Figure 15-2](#fig_15_2). Create two sets
    of normally distributed random numbers, each of size <math alttext="200 times
    2"><mrow><mn>200</mn> <mo>×</mo> <mn>2</mn></mrow></math> , with the second dimension
    being added onto the first (this imposes a correlation between the variables).
    Then add an xy offset of [2 −1] to the first set of numbers. It will be convenient
    to create a <math alttext="400 times 2"><mrow><mn>400</mn> <mo>×</mo> <mn>2</mn></mrow></math>
    matrix that contains both data classes, as well as a 400-element vector of class
    labels (I used 0s for the first class and 1s for the second class).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您将创建的数据在 [图 15-2](#fig_15_2) 中显示。创建两组正态分布随机数，每组大小为 <math alttext="200 times
    2"><mrow><mn>200</mn> <mo>×</mo> <mn>2</mn></mrow></math>，第二维度添加到第一维度（这会在变量之间引入相关性）。然后给第一组数字添加一个
    xy 偏移量 [2 −1]。方便起见，创建一个包含两个数据类别的 <math alttext="400 times 2"><mrow><mn>400</mn>
    <mo>×</mo> <mn>2</mn></mrow></math> 矩阵，以及一个包含 400 个元素的类别标签向量（我用 0 表示第一类，1 表示第二类）。
- en: Use `sns.jointplot` and `plot_joint` to reproduce graph A in [Figure 15-2](#fig_15_2).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `sns.jointplot` 和 `plot_joint` 重新生成图 A 在 [图 15-2](#fig_15_2) 中的图形。
- en: Exercise 15-6\.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-6。
- en: Now for the LDA. Write code in NumPy and/or SciPy instead of using a built-in
    library such as sklearn (we’ll get to that later).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行 LDA。使用 NumPy 和/或 SciPy 编写代码，而不是使用 sklearn 等内置库（我们稍后会讲到）。
- en: The within-class covariance matrix <math alttext="bold upper C Subscript upper
    W"><msub><mi>𝐂</mi> <mi>W</mi></msub></math> is created by computing the covariance
    of each class separately and then averaging those covariance matrices. The between-class
    covariance matrix <math alttext="bold upper C Subscript upper B"><msub><mi>𝐂</mi>
    <mi>B</mi></msub></math> is created by computing the means of each data feature
    (in this case, the xy-coordinates) within each class, concatenating those feature-mean
    vectors for all classes (that will create a <math alttext="2 times 2"><mrow><mn>2</mn>
    <mo>×</mo> <mn>2</mn></mrow></math> matrix for two features and two classes),
    and then computing the covariance matrix of that concatenated matrix.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 类内协方差矩阵 <math alttext="bold upper C Subscript upper W"><msub><mi>𝐂</mi> <mi>W</mi></msub></math>
    是通过分别计算每个类别的协方差然后对这些协方差矩阵取平均值来创建的。类间协方差矩阵 <math alttext="bold upper C Subscript
    upper B"><msub><mi>𝐂</mi> <mi>B</mi></msub></math> 是通过计算每个数据特征（在本例中为 xy 坐标）在每个类别内的均值，将所有类别的特征均值向量串联起来（这将创建一个
    <math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn></mrow></math>
    的矩阵，对于两个特征和两个类别），然后计算该串联矩阵的协方差矩阵。
- en: Remember from [Chapter 13](ch13.xhtml#Chapter_13) that generalized eigendecomposition
    is implemented using SciPy’s `eigh` function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [第 13 章](ch13.xhtml#Chapter_13) 中记住，广义特征值分解是使用 SciPy 的 `eigh` 函数实现的。
- en: The data projected into the LDA space is computed as <math alttext="bold upper
    X overTilde bold upper V"><mrow><mover accent="true"><mi>𝐗</mi> <mo>˜</mo></mover>
    <mi>𝐕</mi></mrow></math> , where <math alttext="bold upper X overTilde"><mover
    accent="true"><mi>𝐗</mi> <mo>˜</mo></mover></math> contains the concatenated data
    from all classes, mean-centered per feature, and <math alttext="bold upper V"><mi>𝐕</mi></math>
    is the matrix of eigenvectors.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 投影到 LDA 空间的数据计算如 <math alttext="bold upper X overTilde bold upper V"><mrow><mover
    accent="true"><mi>𝐗</mi> <mo>˜</mo></mover> <mi>𝐕</mi></mrow></math> ，其中 <math
    alttext="bold upper X overTilde"><mover accent="true"><mi>𝐗</mi> <mo>˜</mo></mover></math>
    包含所有类别的拼接数据，每个特征进行了均值中心化，<math alttext="bold upper V"><mi>𝐕</mi></math> 是特征向量矩阵。
- en: Compute the classification accuracy, which is simply whether each data sample
    has a negative (“class 0”) or positive (“class 1”) projection onto the first LDA
    component. Graph C in [Figure 15-6](#fig_15_6) shows the predicted class label
    for each data sample.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 计算分类准确率，简单来说就是每个数据样本在第一个LDA成分上的投影是负数（“类 0”）还是正数（“类 1”）。图 C 在 [图 15-6](#fig_15_6)
    中展示了每个数据样本的预测类别标签。
- en: Finally, show the results as shown in [Figure 15-6](#fig_15_6).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，展示如 [图 15-6](#fig_15_6) 中所示的结果。
- en: '![exercise 15-6](assets/plad_1506.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![练习 15-6](assets/plad_1506.png)'
- en: Figure 15-6\. Results from Exercise 15-6
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-6\. 练习 15-6 的结果
- en: Exercise 15-7\.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-7\.
- en: I claimed in [Chapter 13](ch13.xhtml#Chapter_13) that for a generalized eigendecomposition,
    the matrix of eigenvectors <math alttext="bold upper V"><mi>𝐕</mi></math> is not
    orthogonal, but it is orthogonal in the space of the “denominator” matrix. Your
    goal here is to demonstrate that empirically.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 [第 13 章](ch13.xhtml#Chapter_13) 中提到，对于广义特征分解，特征向量矩阵 <math alttext="bold upper
    V"><mi>𝐕</mi></math> 不是正交的，但在“分母”矩阵空间中是正交的。你的目标是在这里通过实验证明这一点。
- en: Compute and inspect the results of <math alttext="bold upper V Superscript upper
    T Baseline bold upper V"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup> <mi>𝐕</mi></mrow></math>
    and <math alttext="bold upper V Superscript upper T Baseline bold upper C Subscript
    upper W Baseline bold upper V"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup>
    <msub><mi>𝐂</mi> <mi>W</mi></msub> <mi>𝐕</mi></mrow></math> . Ignoring tiny precision
    errors, which one produces the identity matrix?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 计算并检查 <math alttext="bold upper V Superscript upper T Baseline bold upper V"><mrow><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup> <mi>𝐕</mi></mrow></math> 和 <math alttext="bold upper V
    Superscript upper T Baseline bold upper C Subscript upper W Baseline bold upper
    V"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup> <msub><mi>𝐂</mi> <mi>W</mi></msub>
    <mi>𝐕</mi></mrow></math> 的结果。忽略微小的精度误差，哪一个产生了单位矩阵？
- en: Exercise 15-8\.
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-8\.
- en: Now to reproduce our results using Python’s sklearn library. Use the `LinearDiscriminantAnalysis`
    function in `sklearn.discriminant_analysis`. Produce a plot like [Figure 15-7](#fig_15_7)
    and confirm that the overall prediction accuracy matches results from your “manual”
    LDA analysis in the previous exercise. This function allows for several different
    solvers; use the `eigen` solver to match the previous exercise, and also to complete
    the following exercise.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用 Python 的 sklearn 库重现我们的结果。使用 `sklearn.discriminant_analysis` 中的 `LinearDiscriminantAnalysis`
    函数。生成类似于 [图 15-7](#fig_15_7) 的图，并确认总体预测准确率与前一练习中“手动”LDA分析的结果一致。此函数支持多种不同的求解器；使用
    `eigen` 求解器以匹配前一练习，并继续完成以下练习。
- en: Plot the predicted labels from your “manual” LDA on top; you should find that
    the predicted labels are the same from both approaches.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的“手动”LDA上绘制预测标签；你应该会发现两种方法的预测标签是相同的。
- en: '![exercise 15-8](assets/plad_1507.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![练习 15-8](assets/plad_1507.png)'
- en: Figure 15-7\. Results from Exercise 15-8
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-7\. 练习 15-8 的结果
- en: Exercise 15-9\.
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-9\.
- en: Let’s use sklearn to explore the effects of shrinkage regularization. As I wrote
    in Chapters [12](ch12.xhtml#Chapter_12) and [13](ch13.xhtml#Chapter_13), it is
    trivial that shrinkage will reduce performance on training data; the important
    question is whether the regularization improves prediction accuracy on unseen
    data (sometimes called a *validation set* or *test set*). Therefore, you should
    write code to implement train/test splits. I did this by randomly permuting sample
    indices between 0 and 399, training on the first 350, and then testing on the
    final 50\. Because this is a small number of samples to average, I repeated this
    random selection 50 times and took the average accuracy to be the accuracy per
    shrinkage amount in [Figure 15-8](#fig_15_8).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 sklearn 探索收缩正则化的效果。正如我在第 [12](ch12.xhtml#Chapter_12) 和 [13](ch13.xhtml#Chapter_13)
    章中写的那样，显然，收缩会降低训练数据的性能；重要的问题是正则化是否提高了在未见过的数据上的预测准确性（有时称为*验证集*或*测试集*）。因此，您应该编写代码来实现训练/测试拆分。我通过随机排列样本索引在
    0 到 399 之间，首先在前 350 个样本上进行训练，然后在最后 50 个样本上进行测试来实现这一点。由于样本数量较少，我重复了这个随机选择 50 次，并将平均准确率作为每个收缩量在[图
    15-8](#fig_15_8)中的准确率。
- en: '![exercise 15-9](assets/plad_1508.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![练习 15-9](assets/plad_1508.png)'
- en: Figure 15-8\. Results from Exercise 15-9
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-8\. 练习 15-9 的结果
- en: '**Discussion:** Shrinkage generally had a negative impact on validation performance.
    Although it looks like the performance improved with some shrinkage, repeating
    the code multiple times showed that these were just some random fluctuations.
    A deeper dive into regularization is more appropriate for a dedicated machine
    learning book, but I wanted to highlight here that many “tricks” that have been
    developed in machine learning are not necessarily advantageous in all cases.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论：** 收缩通常对验证性能产生负面影响。虽然看起来通过一些收缩可以改善性能，但多次重复代码表明这些只是一些随机波动。深入探讨正则化更适合专门的机器学习书籍，但我想在这里强调的是，许多在机器学习中开发的“技巧”并不一定在所有情况下都有利。'
- en: SVD for Low-Rank Approximations
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩近似的奇异值分解（SVD）
- en: Exercise 15-10\.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-10\.
- en: 'Igor Stravinsky was one of the greatest music composers of all time (IMHO)—certainly
    one of the most influential of the 20th century. He also made many thought-provoking
    statements on the nature of art, media, and criticism, including one of my favorite
    quotes: “The more art is limited, the more it is free.” There is a famous and
    captivating drawing of Stravinsky by none other than the great Pablo Picasso.
    An image of this drawing is [available on Wikipedia](https://oreil.ly/BtSZv),
    and we are going to work with this picture in the next several exercises. Like
    with other images we’ve worked with in this book, it is natively a 3D matrix (
    <math alttext="640 times 430 times 3"><mrow><mn>640</mn> <mo>×</mo> <mn>430</mn>
    <mo>×</mo> <mn>3</mn></mrow></math> ), but we will convert it to grayscale (2D)
    for convenience.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 伊戈尔·斯特拉文斯基（IMHO 是所有时代最伟大的音乐作曲家之一），无疑是 20 世纪最具影响力的音乐家之一。他还对艺术、媒体和批评的性质发表了许多发人深省的言论，包括我最喜欢的一句话：“艺术越受限制，它就越自由。”有一幅由伟大的巴勃罗·毕加索创作的著名而引人入胜的斯特拉文斯基的画像。这幅画像在[维基百科上有](https://oreil.ly/BtSZv)，我们将在接下来的几个练习中使用这幅图片。像我们在本书中使用过的其他图片一样，它本质上是一个
    3D 矩阵（ <math alttext="640 times 430 times 3"><mrow><mn>640</mn> <mo>×</mo> <mn>430</mn>
    <mo>×</mo> <mn>3</mn></mrow></math> ），但我们将其转换为灰度图像（2D）以方便处理。
- en: 'The purpose of this exercise is to repeat [Exercise 14-5](ch14.xhtml#exercise_14_5),
    in which you re-created a close approximation to a smooth-noise image based on
    four “layers” from the SVD (please look back at that exercise to refresh your
    memory). Produce a figure like [Figure 15-9](#fig_15_9) using the Stravinsky image.
    Here is the main question: does reconstructing the image using the first four
    components give a good result like it did in the previous chapter?'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是重复[练习 14-5](ch14.xhtml#exercise_14_5)，在其中根据奇异值分解的四个“层次”重新创建了一个接近光滑噪声图像的近似图像（请回顾那个练习以刷新您的记忆）。使用斯特拉文斯基的图像制作像[图
    15-9](#fig_15_9)那样的图表。这里的主要问题是：使用前四个分量重建图像是否像上一章那样取得了良好的结果？
- en: '![exercise 15-10](assets/plad_1509.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![练习 15-10](assets/plad_1509.png)'
- en: Figure 15-9\. Results from Exercise 15-10
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-9\. 练习 15-10 的结果
- en: Exercise 15-11\.
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-11\.
- en: Well, the answer to the question at the end of the previous exercise is a resounding
    “No!” The rank-4 approximation is terrible! It looks nothing like the original
    image. The goal of this exercise is to reconstruct the image using more layers
    so that the low-rank approximation is reasonably accurate—and then compute the
    amount of compression obtained.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在上一个练习的最后一个问题的答案是一个响亮的“不行！”秩为 4 的近似太糟糕了！看起来和原始图像完全不一样。这个练习的目标是使用更多层来重建图像，以便低秩近似是合理准确的——然后计算所获得的压缩量。
- en: Start by producing [Figure 15-10](#fig_15_10), which shows the original image,
    the reconstructed image, and the error map, which is the squared difference between
    the original and the approximation. For this figure, I chose *k* = 80 components,
    but I encourage you to explore different values (that is, different rank approximations).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成 [Figure 15-10](#fig_15_10) 开始，显示原始图像、重建图像和误差图，即原始图像与近似图像的平方差。对于这个图，我选择了
    *k* = 80 个组件，但鼓励你探索不同数值（即不同秩的近似）。
- en: '![exercise 15-11](assets/plad_1510.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![练习 15-11](assets/plad_1510.png)'
- en: Figure 15-10\. Results from Exercise 15-11
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 15-10\. 练习 15-11 的结果
- en: Next, compute the compression ratio, which is the percentage of the number of
    bytes used by the low-rank approximation versus the number of bytes used by the
    original image. My results for *k* = 80 are shown here.^([6](ch15.xhtml#idm45733290310320))
    Keep in mind that with low-rank approximations, you don’t need to store the full
    image or the full SVD matrices!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，计算压缩比率，即低秩近似使用的字节与原始图像使用的字节的百分比。我的 *k* = 80 的结果如下所示。^([6](ch15.xhtml#idm45733290310320))
    请记住，使用低秩近似时，你不需要存储完整图像或完整的奇异值分解矩阵！
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Exercise 15-12\.
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-12\.
- en: Why did I choose *k* = 80 and not, e.g., 70 or 103? It was quite arbitrary,
    to be honest. The goal of this exercise is to see whether it’s possible to use
    the error map to determine an appropriate rank parameter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我选择了 *k* = 80 而不是，例如，70 或 103？说实话，这相当随意。这个练习的目标是看看是否可以使用误差图来确定适当的秩参数。
- en: In a `for` loop over reconstruction ranks between 1 and the number of singular
    values, create the low-rank approximation and compute the Frobenius distance between
    the original and *k*-rank approximation. Then make a plot of the error as a function
    of rank, as in [Figure 15-11](#fig_15_11). The error certainly decreases with
    increasing rank, but there is no clear rank that seems best. Sometimes in optimization
    algorithms, the derivative of the error function is more informative; give that
    a try!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在从 1 到奇异值数量的重建秩的 `for` 循环中，创建低秩近似并计算原始图像与 *k* 秩近似之间的Frobenius距离。然后，像 [Figure 15-11](#fig_15_11)
    中那样绘制随秩变化的误差图。误差随秩增加而减少，但没有明显的最佳秩。有时在优化算法中，误差函数的导数更具信息性；试试看！
- en: '![exercise 15-12](assets/plad_1511.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![练习 15-12](assets/plad_1511.png)'
- en: Figure 15-11\. Results from Exercise 15-12
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 15-11\. 练习 15-12 的结果
- en: 'Final thought for this exercise: the reconstruction error for *k* = 430 (i.e.,
    the full SVD) should be exactly 0\. Is it? Obviously the answer is no; otherwise,
    I wouldn’t have written the question. But you should confirm this yourself. This
    is yet another demonstration of precision errors in applied linear algebra.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的最后想法是：*k* = 430 的重建误差（即完全奇异值分解）应该是完全为 0。是吗？显然不是，否则我就不会写这个问题了。但你应该自己确认一下。这又是应用线性代数中精度误差的又一次演示。
- en: SVD for Image Denoising
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像去噪的奇异值分解
- en: Exercise 15-13\.
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 15-13\.
- en: Let’s see if we can extend the concept of low-rank approximation to denoise
    the Stravinsky picture. The goal of this exercise is to add noise and inspect
    the SVD results, and then the following exercise will involve “projecting out”
    the corruption.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否能将低秩近似的概念扩展到去噪斯特拉文斯基的图片。这个练习的目标是添加噪声并检查奇异值分解的结果，接下来的练习将涉及“投影出”损坏的部分。
- en: The noise here will be a spatial sine wave. You can see the noise and corrupted
    image in [Figure 15-12](#fig_15_12).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声将是空间正弦波。你可以在 [Figure 15-12](#fig_15_12) 中看到噪声和损坏的图像。
- en: '![exercise 15-13](assets/plad_1512.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![练习 15-13](assets/plad_1512.png)'
- en: Figure 15-12\. Preparation for Exercise 15-13
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 15-12\. 练习 15-13 的准备工作
- en: 'I will now describe how to create a 2D sine wave (also called a *sine grating*).
    This is a good opportunity to practice your math-to-code translation skills. The
    formula for a 2D sine grating is:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将描述如何创建二维正弦波（也称为 *正弦光栅*）。这是练习你的数学到代码转换技能的好机会。二维正弦波的公式是：
- en: <math alttext="bold upper Z equals sine left-parenthesis 2 pi f left-parenthesis
    bold upper X cosine left-parenthesis theta right-parenthesis plus bold upper Y
    sine left-parenthesis theta right-parenthesis right-parenthesis right-parenthesis"
    display="block"><mrow><mi>𝐙</mi> <mo>=</mo> <mo form="prefix">sin</mo> <mo>(</mo>
    <mn>2</mn> <mi>π</mi> <mi>f</mi> <mo>(</mo> <mi>𝐗</mi> <mo form="prefix">cos</mo>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo> <mo>+</mo> <mi>𝐘</mi> <mo form="prefix">sin</mo>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Z equals sine left-parenthesis 2 pi f left-parenthesis
    bold upper X cosine left-parenthesis theta right-parenthesis plus bold upper Y
    sine left-parenthesis theta right-parenthesis right-parenthesis right-parenthesis"
    display="block"><mrow><mi>𝐙</mi> <mo>=</mo> <mo form="prefix">sin</mo> <mo>(</mo>
    <mn>2</mn> <mi>π</mi> <mi>f</mi> <mo>(</mo> <mi>𝐗</mi> <mo form="prefix">cos</mo>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo> <mo>+</mo> <mi>𝐘</mi> <mo form="prefix">sin</mo>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math>
- en: In this formula, *f* is the frequency of the sine wave, <math alttext="theta"><mi>θ</mi></math>
    is a rotation parameter, and <math alttext="pi"><mi>π</mi></math> is the constant
    3.14…. <math alttext="bold upper X"><mi>𝐗</mi></math> and <math alttext="bold
    upper Y"><mi>𝐘</mi></math> are grid locations on which the function is evaluated,
    which I set to be integers from −100 to 100 with the number of steps set to match
    the size of the Stravinsky picture. I set <math alttext="f"><mi>f</mi></math>
    = .02 and <math alttext="theta"><mi>θ</mi></math> = *π*/6.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*f*是正弦波的频率，<math alttext="theta"><mi>θ</mi></math>是旋转参数，<math alttext="pi"><mi>π</mi></math>是常数3.14…。<math
    alttext="bold upper X"><mi>𝐗</mi></math>和<math alttext="bold upper Y"><mi>𝐘</mi></math>是函数评估的网格位置，我设置为从−100到100的整数，步数设置为与斯特拉文斯基图片大小相匹配。我设置<math
    alttext="f"><mi>f</mi></math> = .02和<math alttext="theta"><mi>θ</mi></math> =
    *π*/6。
- en: Before moving on to the rest of the exercise, I encourage you to spend some
    time with the sine grating code by exploring the effects of changing the parameters
    on the resulting image. However, please use the parameters I wrote previously
    to make sure you can reproduce my results that follow.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行其他练习之前，我建议您花一些时间使用正弦光栅代码，探索更改参数对生成的图像的影响。但请使用我之前写的参数，以确保您可以复现我随后的结果。
- en: 'Next, corrupt the Stravinsky picture by adding the noise to the image. You
    should first scale the noise to a range of 0 to 1, then add the noise and the
    original picture together, and then rescale. Scaling an image between 0 and 1
    is achieved by applying the following formula:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过将噪声添加到图像中来损坏斯特拉文斯基图片。您应该首先将噪声缩放到0到1的范围内，然后将噪声和原始图片相加，然后重新缩放。将图像在0到1之间缩放的方法是应用以下公式：
- en: <math alttext="bold upper R overTilde equals StartFraction bold upper R minus
    min left-parenthesis bold upper R right-parenthesis Over max left-parenthesis
    bold upper R right-parenthesis minus min left-parenthesis bold upper R right-parenthesis
    EndFraction" display="block"><mrow><mover accent="true"><mi>𝐑</mi> <mo>˜</mo></mover>
    <mo>=</mo> <mfrac><mrow><mi>𝐑</mi><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>𝐑</mi><mo>)</mo></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>𝐑</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>𝐑</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper R overTilde equals StartFraction bold upper R minus
    min left-parenthesis bold upper R right-parenthesis Over max left-parenthesis
    bold upper R right-parenthesis minus min left-parenthesis bold upper R right-parenthesis
    EndFraction" display="block"><mrow><mover accent="true"><mi>𝐑</mi> <mo>˜</mo></mover>
    <mo>=</mo> <mfrac><mrow><mi>𝐑</mi><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>𝐑</mi><mo>)</mo></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>𝐑</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>𝐑</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: OK, now you have your noise-corrupted image. Reproduce [Figure 15-13](#fig_15_13),
    which is the same as [Figure 15-6](#fig_15_6) but using the noisy image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了带噪声的图像。重现[图15-13](#fig_15_13)，这与[图15-6](#fig_15_6)相同，但使用了有噪声的图像。
- en: '![ex12.](assets/plad_1513.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![ex12.](assets/plad_1513.png)'
- en: Figure 15-13\. Results from Exercise 15-13
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-13\. 练习15-13的结果
- en: '**Discussion:** It’s interesting to compare [Figure 15-13](#fig_15_13) with
    [Figure 15-9](#fig_15_9). Although we created the noise based on one feature (the
    sine wave grating), the SVD separated the grating into two components of equal
    importance (roughly equal singular values).^([7](ch15.xhtml#idm45733290197184))
    Those two components are not sine gratings but instead are vertically oriented
    patches. Their sum, however, produces the diagonal bands of the grating.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论：** 比较[图15-13](#fig_15_13)与[图15-9](#fig_15_9)是很有意思的。尽管我们基于一个特征（正弦波光栅）创建了噪声，但SVD将该光栅分为两个重要性相等的分量（大致相等的奇异值）。这两个分量不是正弦波光栅，而是垂直方向的补丁。然而，它们的总和产生了光栅的对角线条带。'
- en: Exercise 15-14\.
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '练习15-14\. '
- en: Now for the denoising. It appears that the noise is contained in the second
    and third components, so your goal now is to reconstruct the image using all components
    except for those two. Produce a figure like [Figure 15-14](#fig_15_14).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行去噪。看起来噪音包含在第二和第三个分量中，所以你现在的目标是使用除了这两个分量之外的所有分量来重建图像。制作一个类似于[图15-14](#fig_15_14)的图。
- en: '![exercise 14](assets/plad_1514.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![练习14](assets/plad_1514.png)'
- en: Figure 15-14\. Results from Exercise 15-14
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-14\. 练习15-14的结果
- en: '**Discussion:** The denoising is decent but certainly not perfect. One of the
    reasons for the imperfection is that the noise is not entirely contained in two
    dimensions (notice that the middle panel of [Figure 15-14](#fig_15_14) does not
    perfectly match the noise image). Furthermore, the noise projection (the image
    made from components 1 and 2) has negative values and is distributed around zero,
    even though the sine grating had no negative values. (You can confirm this by
    plotting a histogram of the noise image, which I show in the online code.) The
    rest of the image needs to have fluctuations in values to account for this so
    that the full reconstruction has only positive values.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论：** 去噪效果还可以，但肯定不完美。不完美的原因之一是噪音并非完全包含在两个维度中（注意[图15-14](#fig_15_14)的中间面板并不完全匹配噪音图像）。此外，噪声投影（由第1和第2个分量制成的图像）具有负值，并分布在零附近，尽管正弦光栅没有负值。（您可以通过绘制噪音图像的直方图来确认这一点，我在在线代码中展示了这一点。）图像的其余部分需要有值的波动来解释这一点，以便全面重建只有正值。'
- en: ^([1](ch15.xhtml#idm45733290809152-marker)) The online code demonstrates this
    point.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch15.xhtml#idm45733290809152-marker)) 在线代码演示了这一点。
- en: ^([2](ch15.xhtml#idm45733290657888-marker)) In [Exercise 15-3](#exercise_15_3),
    you will also learn how to implement PCA using the Python scikit-learn library.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch15.xhtml#idm45733290657888-marker)) 在[练习 15-3](#exercise_15_3)中，您还将学习如何使用Python的scikit-learn库实现主成分分析（PCA）。
- en: ^([3](ch15.xhtml#idm45733290633904-marker)) Indeed, linear discriminant analysis
    is also called Fisher’s discriminant analysis.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch15.xhtml#idm45733290633904-marker)) 实际上，线性判别分析也称为费舍尔判别分析。
- en: ^([4](ch15.xhtml#idm45733290531744-marker)) I won’t go through the calculus-laden
    proof, but it’s just a minor variant of the proof given in the PCA section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch15.xhtml#idm45733290531744-marker)) 我不会详细讲述这个充满微积分的证明，但它只是主成分分析部分给出的证明的一个小变体。
- en: '^([5](ch15.xhtml#idm45733290481824-marker)) Data citation: Akbilgic, Oguz.
    (2013). Istanbul Stock Exchange. UCI Machine Learning Repository. Data source
    website: [*https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange*](https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch15.xhtml#idm45733290481824-marker)) 数据引用：Akbilgic, Oguz. (2013). 伊斯坦布尔证券交易所。UCI机器学习库。数据来源网站：[*https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange*](https://archive-beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange)。
- en: ^([6](ch15.xhtml#idm45733290310320-marker)) There is some ambiguity about whether
    one megabyte is 1,000² or 1,024² bytes; I used the latter, but it doesn’t affect
    the compression ratio.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch15.xhtml#idm45733290310320-marker)) 有些不确定是否一兆字节是1000²还是1024²字节；我使用了后者，但这不影响压缩比。
- en: ^([7](ch15.xhtml#idm45733290197184-marker)) The likely explanation is that this
    is a 2D singular *plane*, not a pair of singular *vectors*; any two linearly independent
    vectors on this plane could be basis vectors, and Python selected an orthogonal
    pair.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch15.xhtml#idm45733290197184-marker)) 可能的解释是这是一个二维的奇异*平面*，而不是一对奇异*向量*；在这个平面上，任意两个线性独立的向量都可以是基向量，Python选择了一对正交向量。
