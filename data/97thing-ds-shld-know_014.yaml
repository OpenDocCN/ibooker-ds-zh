- en: 'Chapter 12\. Unbiased ≠ Fair: For Data Science, It Cannot Be Just About the
    Math'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。不偏不等于公平：对于数据科学而言，不能只关注数学问题。
- en: Doug Hague
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Doug Hague
- en: '![](Images/Doug_Hague.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Doug_Hague.png)'
- en: Executive Director, School of Data Science at UNC Charlotte
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 北卡罗来纳大学夏洛特分校数据科学学院执行主任
- en: 'As I have been thinking through the ethical implications in data science, one
    thing has become glaringly obvious to me: data scientists like math! Nothing very
    surprising there. But as we go about our work building models and making great
    predictions, we tend to reduce the conversation about ethics to mathematical terms.
    Is my prediction for Caucasian Americans the same as for African Americans? Are
    female predictions equivalent to male ones? We develop confusion matrices and
    measure the accuracy of our predictions. Or maybe the sensitivity (true positive
    rate) or the specificity (true negative rate) is important, so we balance that
    for various subgroups. Unfortunately, mathematicians have shown that while we
    may be able to balance the accuracy, specificity, or other measures of bias for
    real datasets, we cannot balance them all and make perfectly unbiased models.
    So we do the best we can within the framework we are provided and declare that
    our model is fair.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我深思数据科学中的伦理影响时，有一件事对我变得非常明显：数据科学家喜欢数学！这并不奇怪。但是当我们建立模型并做出伟大的预测时，我们往往将关于伦理的讨论简化为数学术语。我的对白人美国人的预测和对非洲裔美国人的预测相同吗？女性的预测是否等同于男性的预测？我们制定混淆矩阵并测量预测的准确性。或者也许灵敏度（真正例率）或特异性（真负例率）很重要，所以我们为各个子群体进行平衡。不幸的是，数学家们已经表明，虽然我们可以在真实数据集上平衡准确性、特异性或其他偏见度量，但我们不能平衡所有这些并创建完全无偏见的模型。因此，我们在所提供的框架内尽力而为，并宣称我们的模型是公平的。
- en: After studying the issues and applications, I assert that models that balance
    bias are not fair. Fairness really does not pay attention to mathematics. It pays
    attention to individual viewpoints, societal and cultural norms, and morals. In
    other words, fairness is defined by social systems and philosophy.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究了相关问题和应用之后，我断言平衡偏见的模型并不公平。公平真正关注的不是数学，而是个体观点、社会文化规范和道德。换句话说，公平由社会系统和哲学定义。
- en: For example, in criminal justice, recidivism models predict whether a person
    arrested will commit another crime if released on bond. As an indicted individual,
    you believe that the false positive rate should be as low as possible so you are
    not kept in jail when you should not be. The average citizen, however, wants the
    false negative rate as low as possible to minimize the number of people who are
    let out and go on to commit a new crime. Balancing these two is a trade-off that
    both sides will say is not fair. And we have not even started to discuss the bias
    in the data and in the system that has resulted in disproportionately higher numbers
    of African Americans being incarcerated.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在刑事司法中，累犯模型预测被捕人员在获得保释后是否会再次犯罪。作为被起诉的个人，您认为误判率应尽可能低，这样您就不会在不应该被关押时被关押。然而，普通市民希望尽可能降低误放人员再犯的可能性，因此希望将假阴性率尽可能降低。平衡这两者是一个权衡，双方都会说这不公平。而且我们甚至还没有开始讨论数据和系统中导致非裔美国人被不成比例地关押的偏见问题。
- en: As one considers the ethical implications of data science, one quickly gets
    to debating the cultural and moral norms of the society that the model is being
    deployed into. As a data science team deploys a model, those cultural norms must
    be considered. Philosophies of utilitarianism and its derivatives are prevalent
    within Western society; here, the role of overall good is debated, and the balance
    between individual good and common good is discussed. Different philosophical
    constructs are favored in other cultures and geographies. Understanding which
    cultures a model will touch and how and where it will touch them is important
    to reaching for fairness for the deployed model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们考虑数据科学的伦理影响时，很快就会讨论模型被部署到的社会的文化和道德规范。当数据科学团队部署模型时，必须考虑这些文化规范。功利主义及其衍生物在西方社会中很常见；在这里，整体利益的角色被辩论，个体利益和共同利益之间的平衡被讨论。在其他文化和地理区域，不同的哲学构想得到青睐。了解模型将触及哪些文化以及它将如何触及它们，对于实现部署模型的公平性至关重要。
- en: Understanding the system within which a model is deployed is just as important.
    As models are deployed, they enter an operational system. Depending on the specifics
    of the situation, there frequently are decisions that get made after the model
    prediction. Often data scientists develop and measure model accuracy based on
    what mathematics predicts. However, measurement of the entire system and decisions
    that occur *after* the model prediction are just as important. Additionally, human-in-the-loop
    models are often held up as being even more accurate; however, are they also less
    biased and fairer? With a human in the loop, bias may creep back into decisions.
    Also, if there is more than one decision maker, different people will bring different
    levels of information as well as cultural differences. Each of these differences
    can easily result in system bias and fairness issues even if the model was tuned
    and prepared to be as fair as possible. Framing the operations and measuring performance
    should occur for both the model outcome as well as the system outcome. I believe
    many lawsuits over fairness and discrimination occur because both sides frame
    the situation differently. Each side is “right” within their framing, but which
    frame will a jury conclude to be fair?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理解模型部署所处的系统同样重要。随着模型的部署，它们进入一个运行系统。根据具体情况，经常在模型预测之后做出决策。通常数据科学家根据数学预测开发和衡量模型的准确性。然而，测量整个系统和模型预测之后发生的决策同样重要。此外，人在回路模型通常被认为更准确；然而，它们是否也更少偏见和更公平？在人为因素参与的情况下，偏见可能重新蔓延到决策中。此外，如果有多个决策者，不同的人会带来不同水平的信息以及文化差异。这些差异每一个都可能导致系统偏见和公平问题，即使模型经过调优和准备，力求尽可能公平。应该为模型结果以及系统结果制定运作框架并进行性能衡量。我认为许多关于公平和歧视的诉讼之所以发生，是因为双方对情况的框架不同。在各自的框架内，每一方都是“正确”的，但陪审团会认定哪个框架更公平？
- en: As responsible data scientists, we should expand our ethical considerations
    beyond the mathematical bias of our model to include cultural and societal definitions
    of fairness, and our model deployment should consider framing the outcomes of
    the system and not just the model predictions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为负责任的数据科学家，我们应该将我们的道德考虑扩展到模型的数学偏差之外，包括文化和社会对公平的定义，而我们的模型部署应考虑到系统的结果框架，而不仅仅是模型预测。
