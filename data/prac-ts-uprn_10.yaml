- en: Chapter 10\. Deep Learning for Time Series
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。时间序列的深度学习
- en: Deep learning for time series is a relatively new endeavor, but it’s a promising
    one. Because deep learning is a highly flexible technique, it can be advantageous
    for time series analysis. Most promisingly, it offers the possibility of modeling
    highly complex and nonlinear temporal behavior without having to guess at functional
    forms—which could potentially be a game changer for nonstatistical forecasting
    techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的深度学习是一个相对新的尝试，但是它是一个有希望的尝试。由于深度学习是一种高度灵活的技术，它对时间序列分析具有优势。最有希望的是，它提供了模拟高度复杂和非线性时间行为的可能性，而无需猜测功能形式——这可能对非统计预测技术是一个改变游戏规则的因素。
- en: 'If you aren’t familiar with deep learning, here’s a one-paragraph summary (we’ll
    go into more details later). Deep learning describes a branch of machine learning
    in which a “graph” is built that connects input nodes to a complicated structure
    of nodes and edges. In passing from one node to another via an edge, a value is
    multiplied by that edge’s weight and then, usually, passed through some kind of
    nonlinear activation function. It is this nonlinear activation function that makes
    deep learning so interesting: it enables us to fit highly complex, nonlinear data,
    something that had not been very successfully done previously.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对深度学习不熟悉，这里有一个段落摘要（稍后我们会详细讨论）。深度学习描述了机器学习的一个分支，其中构建了一个“图”，将输入节点连接到复杂的节点和边的结构中。通过边从一个节点传递到另一个节点时，值会乘以该边的权重，然后通常通过某种非线性激活函数传递。正是这种非线性激活函数使得深度学习如此有趣：它使我们能够拟合高度复杂、非线性的数据，这是之前没有成功做到的。
- en: Deep learning has come into its own primarily within the past 10 years, as improvements
    in commercially available hardware have been coupled with massive amounts of data
    to enable this kind of heavy-duty model fitting. Deep learning models can have
    millions of parameters, so one way of understanding them is to dream up just about
    any graph you can think of, with all sorts of matrix multiplications and nonlinear
    transforms, and then imagine setting loose a smart optimizer that optimizes your
    model one small group of data at a time, continuously adjusting the weights of
    this large model so they give increasingly good outputs. This is deep learning
    in a nutshell.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习主要在过去10年内发展起来，随着商用硬件的改进和海量数据的提供，使得这种重型模型拟合成为可能。深度学习模型可以拥有数百万个参数，因此理解它们的一种方式是想象出你能想到的任何图，其中包括各种矩阵乘法和非线性变换，然后想象释放一个智能优化器，逐步调整这个大模型的权重，以便逐渐提供越来越好的输出。这就是深度学习的核心。
- en: Deep learning has not yet delivered the amazing results for forecasting that
    it has for other areas, such as image processing and natural language processing.
    However, there is good reason to be optimistic that deep learning will eventually
    improve the art of forecasting while also lessening the brittle and highly uniform
    nature of assumptions and technical requirements common for traditional forecasting
    models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在预测方面尚未像在图像处理和自然语言处理等其他领域那样取得惊人的结果。然而，有充分的理由乐观地认为，深度学习最终将改进预测的技术，同时减少传统预测模型中常见的脆弱和高度统一的假设与技术要求。
- en: 'Many of the headaches of preprocessing data to fit a model’s assumptions are
    gone when deep learning models are used:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用深度学习模型时，许多预处理数据以适应模型假设的头痛问题都不复存在：
- en: There is no requirement of stationarity.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有稳态的要求。
- en: There is no need to develop the art and skill of picking parameters, such as
    assessing seasonality and order of a seasonal ARIMA model.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需开发选择参数的艺术和技能，例如评估季节性和季节性ARIMA模型的顺序。
- en: There is no need to develop a hypothesis about the underlying dynamics of a
    system, as is helpful with state space modeling.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需对系统的底层动态做假设，这对状态空间建模非常有帮助。
- en: 'These advantages should sound familiar after [Chapter 9](ch09.html#ml_for_time_series_chapter),
    where we discussed many of the same advantages with respect to applying machine
    learning to time series. Deep learning is even more flexible for a number of reasons:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章讨论了这些优势后，这些优势应该听起来很熟悉，即将机器学习应用于时间序列。深度学习因多种原因而更加灵活：
- en: Many machine learning algorithms tend to be fairly brittle in terms of the dimensionality
    and kinds of input data required for a training algorithm to work. In contrast,
    deep learning is highly flexible as to the model and the nature of the inputs.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多机器学习算法在需要训练算法的维度和输入数据类型方面往往比较脆弱。相比之下，深度学习在模型和输入数据的性质上具有高度的灵活性。
- en: Heterogenous data can be challenging with many commonly applied machine learning
    techniques, whereas it is quite commonly used for deep learning models.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异构数据对于许多常见的机器学习技术来说是具有挑战性的，而对于深度学习模型来说却很常见。
- en: Machine learning models rarely are developed for time series problems, whereas
    deep learning offers a great deal of flexibility to develop architectures specific
    to temporal data.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型很少被开发用于时间序列问题，而深度学习则提供了灵活性，可以开发特定于时间数据的架构。
- en: However, deep learning is not a magic bullet. Although there is no requirement
    of stationarity for deep learning applied to time series, in practice, deep learning
    does not do a good job of fitting data with a trend unless standard architectures
    are modified to fit the trend. So we still need to preprocess our data or our
    technique.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深度学习并非万能药。尽管对于应用于时间序列的深度学习没有平稳性的要求，但实际上，除非标准架构被修改以适应趋势，深度学习在拟合带有趋势的数据方面表现不佳。因此，我们仍然需要预处理我们的数据或者我们的技术。
- en: Also, deep learning does best with numerical inputs in different channels all
    scaled to similar values between –1 and 1\. This means that you will need to preprocess
    your data even though it’s not theoretically required. Also, you need to preprocess
    it in a way that avoids lookahead, which is not something the deep learning community
    as a whole has spent a lot of time perfecting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习最适合于不同通道中数值输入，所有数值都缩放到-1到1之间的相似值。这意味着即使在理论上并不需要，你仍需要预处理你的数据。而且，你需要以避免前瞻的方式进行预处理，这不是整个深度学习社区花费大量时间完善的事项。
- en: Finally, deep learning optimization techniques and modeling for time-oriented
    neural networks (the largest class of which is recurrent neural networks, or RNNs)
    are not as well developed as those for image processing (the largest class of
    which is convolutional neural networks, or CNNs). This means you will find less
    guidance on best practices and rules of thumb for selecting and training an architecture
    than you would for nontemporal tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，针对面向时间的神经网络（其中最大的类别是循环神经网络或RNN）的深度学习优化技术和建模并没有像图像处理（其中最大的类别是卷积神经网络或CNN）那样发展得好。这意味着你在选择和训练架构方面会比非时间任务获得更少的最佳实践和经验法则指导。
- en: In addition to these difficulties of applying deep learning to time series,
    you will find the rewards for doing so are a mixed bag. For starters, deep learning’s
    performance for time series does not consistently outperform more traditional
    methods for time series forecasting and classification. Indeed, forecasting is
    an area ripe for improvements from deep learning, but as of this writing these
    improvements have not yet materialized.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将深度学习应用于时间序列的这些困难外，你会发现这样做的回报是一把双刃剑。首先，深度学习在时间序列的性能并不总是优于传统方法用于时间序列预测和分类。确实，预测是一个从深度学习中可以得到改进的领域，但至今这些改进还没有实质性的体现。
- en: Nonetheless, there is reason to expect both immediate and long-term benefits
    to adding deep learning to your time series analysis toolkit. First, large tech
    companies have begun launching deep learning for time series services with custom
    architectures they have developed in-house, often with industry-specific modeling
    tasks in mind. You can use these services or combine them with your own analyses
    to get good performance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，有理由期待将深度学习添加到你的时间序列分析工具包中会带来即时和长期的好处。首先，大型科技公司已经开始推出专门针对时间序列的深度学习服务，使用他们在内部开发的定制架构，通常考虑了行业特定的建模任务。你可以使用这些服务或将它们与你自己的分析结合，以获得良好的性能。
- en: Second, you may have a data set that does amazingly well with deep learning
    for time series. In general, the stronger your signal-to-noise ratio is, the better
    you will do. I have had more than one fairly new programmer tell me that they
    succeeded to a surprising degree with some simple deep learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，你可能有一个数据集，在时间序列的深度学习中表现得非常出色。总的来说，你的信噪比越强，你的表现就会越好。我曾经有过不止一位新手程序员告诉我，他们用一些简单的深度学习取得了惊人的成功。
- en: For example, a college found that a simple LSTM (more on this later) did as
    well as overworked guidance counselors at predicting which students were likely
    to fail or drop out soon, so that the college could reach out to those students
    and provide more resources. While the best outcome would be to have more guidance
    counselors, it is heartwarming to know that a simple LSTM applied to the heterogeneous
    time series data of a student’s grades and attendance record could help change
    vulnerable students by flagging them for outreach and enhanced support. In the
    future, I believe we can expect more of such innovative and assistive uses of
    deep learning for time series.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一所大学发现，一个简单的LSTM（稍后详细介绍）在预测哪些学生可能很快失败或辍学方面，与超负荷的指导顾问的表现一样好，以便学校可以联系这些学生并提供更多资源。虽然最好的结果是拥有更多的指导顾问，但令人欣慰的是，简单的LSTM应用于学生成绩和出勤记录的异质时间序列数据，可以通过标记他们进行联系和增强支持，来帮助改变脆弱的学生。未来，我相信我们可以期待更多这样的创新和支持性使用深度学习的时间序列。
- en: Warning
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Remeber that every model has assumptions. Machine learning models, including
    neural networks, invariably have assumptions built in, both in the architecture
    and in training methodologies. Even the fact that most neural networks work best
    with inputs scaled to [–1, 1] suggests there are strong assumptions built into
    the model, even if these have not been well identified yet.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，每个模型都有假设。机器学习模型，包括神经网络，在架构和训练方法上都不可避免地有内建的假设。即使是大多数神经网络在输入缩放到[–1, 1]时表现最佳，这也暗示着模型中存在强烈的假设，即使这些假设尚未被充分确认。
- en: It may even be that neural network forecasts haven’t reached their optimum performance
    yet, and that better understanding the theoretical underpinnings of and requirements
    for forecasting neural networks will lead to performance enhancements.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可能是神经网络预测尚未达到最佳性能，更好地理解神经网络预测的理论基础和要求将导致性能提升。
- en: If you are new to deep learning, this chapter will not provide you with all
    the conceptual and programming equipment you need to get a competent start. However,
    it will give you a point of departure, and from here, there are many good tutorials,
    books, and even online courses that you can use to learn more in depth. The good
    news about deep learning is that there need be no mathematics involved to get
    a general flavor of how it works and how it is programmed. Additionally, APIs
    are available at various levels of specificity. This means that a beginner can
    get by using fairly high-level APIs to try out some introductory techniques, and
    frankly even experts will use these high-level APIs to save time. Later, when
    some specific architectural innovation is necessary, and as your understanding
    develops, you can use lower-level APIs where more is left for you to decide and
    specify concretely.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是深度学习的新手，本章节不会为您提供开始所需的所有概念和编程工具。然而，它将为您提供一个出发点，从这里，有许多好的教程、书籍，甚至在线课程可以帮助您深入学习。关于深度学习的好消息是，您不需要涉及数学就可以对其工作原理有一个大致了解，并知道如何编程。此外，各种级别的API都可用。这意味着初学者可以使用相当高级的API来尝试一些入门技术，而即使是专家也会使用这些高级API来节省时间。后来，当需要某些特定的架构创新时，随着您的理解加深，您可以使用更低级别的API，在这些API中，更多的决策和具体规定由您自行决定。
- en: This chapter will provide a brief review of the concepts that inspired and supported
    deep learning as a mathematical and computer science pursuit, as well as concrete
    examples of the code you can use to apply a deep learning model to data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将简要回顾激发和支持深度学习作为数学和计算机科学追求的概念，并提供您可以用来将深度学习模型应用于数据的代码的具体示例。
- en: Deep Learning Concepts
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习概念
- en: Deep learning has roots in a number of areas. There was a biological inspiration,
    with computer scientists and quantitative analysts wondering if the way to build
    intelligent machines was, ultimately, to mimic brains, with their networks of
    neurons firing off in response to certain triggers. There was mathematical inspiration
    in the form of the universal approximation theorem being proven for various activations,
    with many of these proofs originating in the late 1980s and early 1990s. Finally,
    there was the growth of computing capabilities and availability coupled with a
    burgeoning field of machine learning, whose success showed that, with enough data
    and parameters, complicated systems could be modeled and predicted. Deep learning
    took all these ideas even further by creating networks described by millions of
    parameters, trained on large data sets, and with a theoretical grounding that
    a neural network should be able to represent an arbitrary and nonlinear function
    to a large degree of accuracy. [Figure 10-1](#fig-1001) shows a simple neural
    network, a multilevel *perceptron* (or fully connected network).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多领域有其根源。生物学启发了人们，计算机科学家和量化分析师在思考是否建立智能机器的方法是模仿大脑，其神经元网络根据特定触发器发射。数学上的启发来自于万能逼近定理在各种激活函数上的证明，其中许多证明起源于
    1980 年代末和 1990 年代初。最后，计算能力和可用性的增长，加上机器学习领域的蓬勃发展，显示出只要有足够的数据和参数，就可以建模和预测复杂系统。深度学习通过创建由数百万参数描述的网络，这些网络在大数据集上训练，并具有理论基础表明神经网络应能以高精度表示任意的非线性函数，进一步发展了这些想法。[图
    10-1](#fig-1001) 展示了一个简单的神经网络，即多层感知机（或全连接网络）。
- en: In [Figure 10-1](#fig-1001), we can see how multichannel inputs are served to
    the model in the form of a vector of dimension d. The nodes represent the input
    values, while the edges represent the multipliers. All the edges entering into
    a node represent a previous value multiplied by the value of the edge it traversed.
    These values from all the inputs at a single node are summed and then, usually,
    passed through a nonlinear activation function, creating a nonlinearity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 10-1](#fig-1001) 中，我们可以看到多通道输入是如何以维度 d 的向量形式提供给模型的。节点表示输入值，边表示乘数。进入节点的所有边表示先前值乘以其所经过的边的值。单个节点中来自所有输入的这些值被求和，通常通过非线性激活函数传递，从而创建非线性。
- en: We can see that the input consists of three channels, or a vector of length
    3\. There are four hidden units. We multiply each of the three inputs by a different
    weight for each of the four hidden units for which it is destined, meaning that
    we need 3 × 4 = 12 weights to fully describe the problem. Also, since we will
    then sum the results of these various multiplications, matrix multiplication is
    not just *analogous* to what we are doing but *exactly* what we are doing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到输入由三个通道组成，即长度为 3 的向量。有四个隐藏单元。我们将每个输入与其分配给的四个隐藏单元的不同权重相乘，这意味着我们需要 3 × 4
    = 12 个权重来完整描述问题。此外，由于我们随后将对这些不同乘法的结果求和，矩阵乘法不仅仅类似于我们所做的，而是确切地是我们所做的。
- en: '![](assets/ptsa_1001.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1001.png)'
- en: Figure 10-1\. A simple feed forward network.
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 一个简单的前馈网络。
- en: 'If we wanted to write out the steps for what we are doing, they would go something
    like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要详细描述我们正在做的步骤，可以大致如下：
- en: 'Input vector X has three elements. Layer 1 weights are designated by W[1],
    a 4 × 3 matrix, such that we compute the hidden layer values as W[1] × X[1]. This
    results in a 4 × 1 matrix, but this is not actually the output of the hidden layer:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入向量 X 共有三个元素。第一层的权重由 W[1] 表示，是一个 4 × 3 的矩阵，我们通过 W[1] × X[1] 计算隐藏层的值。这会得到一个
    4 × 1 的矩阵，但实际上这并不是隐藏层的输出：
- en: W[1] × X[1]
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: W[1] × X[1]
- en: 'We need to apply a nonlinearity, which we can do with various “activation functions,”
    such as hyperbolic tan (tanh) or the sigmoid function (*σ*). We will usually also
    apply a bias, B1, inside the activation function, such that the output of the
    hidden layer is really:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要应用一个非线性函数，可以使用各种“激活函数”，如双曲正切（tanh）或 sigmoid 函数（*σ*）。通常我们还会在激活函数内部应用偏置 B1，这样隐藏层的输出实际上是：
- en: H = a(W[1] × X[1] + B[1])
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: H = a(W[1] × X[1] + B[1])
- en: 'In the neural network depicted in [Figure 10-1](#fig-1001), we have two outputs
    to predict. For this reason we need to convert the four-dimensional hidden state
    output to two outputs. Traditionally, the last layer does not include a nonlinear
    activation function, unless we count applying a softmax in the case of a categorization
    problem. Let’s assume we are just trying to predict two numbers, not two probabilities
    or categories, so we simply apply a last “dense layer” to combine the four outputs
    of the hidden layer per ultimate output. This dense layer will combine four inputs
    into two outputs, so we need a 2 × 4 matrix, W[2]:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 [图 10-1](#fig-1001) 中描述的神经网络中，我们有两个输出要预测。因此，我们需要将四维隐藏状态输出转换为两个输出。传统上，最后一层不包括非线性激活函数，除非我们考虑在分类问题中应用
    softmax。假设我们只是尝试预测两个数字，而不是两个概率或类别，因此我们简单地应用一个“密集层”将隐藏层的四个输出组合成最终的输出。这个密集层将四个输入组合成两个输出，因此我们需要一个
    2 × 4 的矩阵 W[2]：
- en: Y = W[2] × H
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Y = W[2] × H
- en: Hopefully this gave you some sense of how these models work. The overall idea
    is to have lots of parameters and opportunities for nonlinearities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些内容能让您对这些模型的工作方式有所了解。总体思想是具有大量参数和非线性的机会。
- en: It’s something of an art form to choose the right number and form of parameters,
    the right training hyperparameters, and, of course, a reasonably accessible problem.
    The magic comes in learning how to train these parameters, initialize them in
    a smart way from the start, and make sure the model goes in the right direction
    toward a reasonably good solution. This is a nonconvex class of models, and the
    intention is not about finding the global optimum. Instead, the thinking goes,
    so long as you find clever ways to regularize the model, you will find a “good
    enough” local optimum to suit your needs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确数量和形式的参数、正确的训练超参数以及一个相对可访问的问题，这些都是一种艺术形式。魔法在于学习如何从一开始就以聪明的方式初始化这些参数，并确保模型朝着正确的方向朝着一个相对良好的解决方案前进。这是一类非凸模型，意图并非在于找到全局最优解。相反，思路是，只要找到聪明的方法来正则化模型，你就会找到一个“足够好”的局部最优解来满足你的需求。
- en: Programming a Neural Network
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程神经网络
- en: Understanding how neural networks work in principle can feel quite a bit easier
    than understanding how the associated programming frameworks applied to this problem
    actually operate. As we’ll discuss here, however, there are a few broad themes
    that these frameworks tend to have in common.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上理解神经网络如何工作可能比理解应用于此问题的相关编程框架要容易得多。然而，正如我们将在这里讨论的那样，这些框架通常具有几个共同的广泛主题。
- en: Data, Symbols, Operations, Layers, and Graphs
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据、符号、操作、层和图
- en: 'Deep learning frameworks often focus on some notion of a graph and building
    that graph. The idea is that any architecture should be describable in terms of
    its individual components and their relationship to one another. Additionally,
    the idea of variables as divorced from actual values is also quite important.
    So you may have a symbol, A, and a symbol, B, and then a third symbol, C, which
    is the result of matrix multiplying A and B:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架通常侧重于某种图形的概念及其构建。其核心思想是任何架构都可以描述为其各个组件及其相互关系。此外，将变量与实际值分离的概念也非常重要。因此，您可能会有一个符号
    A 和一个符号 B，然后第三个符号 C，它是将 A 和 B 进行矩阵乘法得到的结果：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The distinction between symbols and data is essential to every framework because
    of the relationship between the symbol and the data. The symbol is used to learn
    a more general relationship; the data may have noise. There is only one symbol
    A, even as we may have millions or even billions of values to feed in for A, each
    paired with a respective B and C.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于符号与数据之间的关系，这种区分对于每个框架都至关重要。符号用于学习更一般的关系；数据可能存在噪声。即使我们可能有数百万甚至数十亿个值要为 A 提供输入，并且每个值都与相应的
    B 和 C 成对出现，但只有一个符号 A。
- en: As we take a step back and think about what we do with data, these are the operations.
    We might add or multiply symbols together, and we can think of these as operations.
    We can also perform univariate operations, such as changing the shape of a symbol
    (perhaps converting symbol A from a 2 × 4 matrix to an 8 × 1 matrix) or passing
    values through an activation function, such as computing tanh(A).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们退一步思考我们如何处理数据时，这些就是操作。我们可以将符号相加或相乘，我们可以将这些视为操作。我们还可以执行单变量操作，例如改变符号的形状（也许将符号
    A 从 2 × 4 矩阵转换为 8 × 1 矩阵），或通过激活函数传递值，例如计算 tanh(A)。
- en: 'Taking a step even further back, we can think of layers as those traditional
    units of processing we associate with common architectures, such as a fully connected
    layer, which we explored in the previous section. Taking into account the activation
    function and bias as well as the core matrix operation, we can say:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步地，我们可以将层视为我们与常见架构相关联的传统处理单元，例如全连接层，在前一节中我们探讨过。考虑到激活函数和偏置以及核心矩阵操作，我们可以说：
- en: layer L = tanh(A × B + bias)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 层 L = tanh(A × B + 偏置)
- en: In many frameworks this layer may be expressible as one or two layers rather
    than several operations, depending on whether a combination of operations is popular
    enough to warrant its own designation as a unit of a layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多框架中，这一层可能表达为一个或两个层，而不是几个操作，这取决于是否有足够流行的操作组合来保证其作为层单元的独立指定。
- en: 'Finally, we can relate multiple layers to one another, with one passing to
    the next:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将多个层与彼此关联，一个传递到下一个：
- en: layer L1 = tanh(A × B + bias1)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 层 L1 = tanh(A × B + 偏置1)
- en: layer L2 = tanh(L1 × D + bias2)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 层 L2 = tanh(L1 × D + 偏置2)
- en: The entire conglomerate of these symbols, operations, and layers results in
    a graph. A graph need not be fully connected—that is, it will not necessarily
    be the case that all symbols have dependencies on one another. A graph is used
    precisely to sort out what symbols depend on what other symbols and how. This
    is essential so that when we are computing gradients for the purposes of gradient
    descent, we can tweak our weights with each iteration for a better solution. The
    nice thing is that most modern deep learning packages do all this for us. We don’t
    need to specify what depends on what and how the gradient changes with each layer
    added—it’s all baked in.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些符号、操作和层的整合形成一个图。图不一定是完全连接的——也就是说，并不一定所有的符号都彼此依赖。图的作用在于准确地整理哪些符号依赖于哪些其他符号以及依赖关系如何。这是至关重要的，因为当我们计算梯度以进行梯度下降时，我们可以在每次迭代中微调权重，以获得更好的解决方案。好处是，大多数现代深度学习包已经为我们处理了所有这些。我们不需要指定什么依赖于什么以及随着每个添加层如何改变梯度——这一切都已经内置。
- en: 'What’s more, as I mentioned before, we can use high-level APIs such that we
    do not need to spell out matrix multiplications as I did in the preceding example.
    Rather, if we want to use a fully connected layer, which is the equivalent of
    a matrix multiplication, followed by a matrix addition, and then an element-wise
    activation function of some kind, we don’t need to write out all the math. For
    example, in `mxnet`, we can accomplish this with the following simple code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，正如我之前提到的，我们可以使用高级API，这样我们就不需要像我在前面的示例中那样详细说明矩阵乘法。相反，如果我们想使用全连接层，这相当于矩阵乘法，然后是矩阵加法，再然后是某种元素级的激活函数，我们不需要编写所有的数学公式。例如，在`mxnet`中，我们可以通过以下简单的代码实现这一点：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will give us a fully connected layer that converts inputs to an output
    dimension of 120\. Furthermore, this one line represents all that I just mentioned:
    a matrix multiplication, followed by a matrix addition, followed by an element-wise
    activation function.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个完全连接的层，将输入转换为120的输出维度。此外，这一行代表了我刚才提到的所有内容：矩阵乘法，接着是矩阵加法，再接着是逐元素的激活函数。
- en: You can verify this in the [documentation](https://perma.cc/8PQW-4NKY), or by
    trying sample outputs and inputs when you know the weights. It’s really quite
    impressive how much the API doesn’t require you to do. You are not required to
    specify the input shape (although this must be a constant once you have built
    your graph so that it can be deduced). You are not required to specify a data
    type—this will default to the very sensible and most commonly used value of `float32`
    (`float64` is overkill for deep learning). If your data happens to come in a non-1D
    shape per example, this layer will also automatically “flatten” your data so that
    it is in the appropriate shape to be input into a fully connected/dense layer.
    This is helpful for a beginner, but once you develop a minimal level of competence
    it’s good to revisit the documentation to understand just how much is being decided
    and taken care of for you even in a simple deep learning model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[文档](https://perma.cc/8PQW-4NKY)中验证这一点，或者在您知道权重时尝试样本输出和输入。令人印象深刻的是 API 不需要您做太多的事情。您不需要指定输入形状（虽然一旦构建了图形，这必须是一个常量，以便可以推断出来）。您不需要指定数据类型——这将默认为非常明智和最常用的`float32`（对于深度学习来说，`float64`
    是过度的）。如果您的数据恰好以非 1D 形状每个示例传入，该层还将自动“展平”您的数据，以便以适当的形状输入到完全连接/密集层中。这对于初学者很有帮助，但是一旦您发展出一定的能力水平，重温文档以理解在一个简单的深度学习模型中有多少决策和细节被自动处理是很有益的。
- en: 'This simple code, of course, does not require everything you need even for
    a short deep learning task. You will need to describe your inputs, your targets,
    and how you want to measure loss. You also need to situate your layer inside a
    model. We do this in the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的代码，当然，即使是用于短期深度学习任务，也不需要你所需要的一切。你需要描述你的输入、目标以及如何测量损失。你还需要将你的层置于模型中。我们在下面的代码中实现了这一点：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, assuming we have also set up our data as needed, we can run through
    an epoch of training (that is, one pass through all the data) as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设我们已经按需设置好我们的数据，我们可以通过一个训练周期（即，所有数据的一次遍历）来运行如下：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It is also easy in most packages, as it is with `mxnet`, to save your model
    and accompanying parameters for later production use or additional training:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数软件包中，如同在`mxnet`中一样，保存模型及其相关参数以供后续生产使用或额外训练也是很容易的：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the examples that follow, we will use `mxnet`’s `Module` API to give you
    another flavor of creating graphs and training models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的例子中，我们将使用`mxnet`的`Module` API，以便展示创建图形和训练模型的另一种方式。
- en: As a deep learning practitioner, you will eventually want to become familiar
    with all the major packages and their various APIs (usually packages have, at
    the least, a very high-level API and a low-level API) so that you are easily able
    to read sample code. Working knowledge of all major deep learning modules is necessary
    to keep up with the latest industry practices and academic research, because this
    information is most easily learned through open source code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为深度学习实践者，你最终会希望熟悉所有主要的软件包及其各种 API（通常软件包至少具有一个非常高级别的 API 和一个低级别的 API），这样你就能轻松阅读示例代码。掌握所有主要的深度学习模块知识对于跟上最新的行业实践和学术研究是必要的，因为这些信息最容易通过开源代码学习。
- en: Building a Training Pipeline
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建训练流水线
- en: 'Throughout this section we will model the same data set: a measurement of hourly
    electric use in a variety of locations for several years. We’ll preprocess this
    data so that we look at the change in electric use from hour to hour, a more difficult
    task than predicting the overall values because we are looking at the most unpredictable
    part of the time series.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将模拟相同的数据集：多年来多个地点的每小时电使用测量。我们将预处理这些数据，以便查看每小时电使用量的变化，这比预测总体值更加困难，因为我们正在观察时间序列中最不可预测的部分。
- en: Inspecting Our Data Set
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查我们的数据集
- en: 'We use an open data repository of hourly electric measurements provided in
    a code base demonstrating a new neural network architecture (which we will discuss
    later in this post).^([1](ch10.html#idm45576029280392)) To get a sense of what
    this data looks like, we read it in R and make some fast plots:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个小时电测量的开放数据仓库，在演示一个新的神经网络架构的代码库中提供（我们稍后在本文中会讨论）^([1](ch10.html#idm45576029280392))。为了了解这些数据的样子，我们在
    R 中读取并快速绘制一些图：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Already from a quick inspection, we can see how many rows and columns of data
    we have, as well as that there are no timestamps. We have been independently told
    these timestamps are hourly, but we don’t know when exactly these measurements
    took place:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从快速检查中，我们可以看出我们有多少行和列的数据，以及没有时间戳。我们已经独立地被告知这些时间戳是每小时的，但我们不知道这些测量究竟是在什么时候进行的：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also plot some random samples of the data to get a sense of what it
    looks like (see [Figure 10-2](#fig-1002)). Since this is hourly data, we know
    if we plot 24 data points we will have a full day’s worth of data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制一些数据的随机样本，以了解其外观（见[图 10-2](#fig-1002)）。由于这是小时数据，我们知道如果绘制24个数据点，我们将得到一整天的数据：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We also make a weekly plot (see [Figure 10-3](#fig-1003)):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还制作了一周的图表（见[图 10-3](#fig-1003)）：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Remember that we don’t know what the local hour is for any particular index,
    although we know the relation between them. However, consistent with our discussions
    back in [Chapter 2](ch02.html#finding_and_wrangling_time_series_data), we could
    probably guess at what parts of the day represent standard human schedules based
    on the patterns of electric use. We could likely also identify the weekend. We
    don’t do that here, but it could be a good idea to include models relating to
    time of day and day of the week for a more in-depth analysis and modeling of this
    data set.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们不知道任何特定索引的本地时间是什么，尽管我们知道它们之间的关系。然而，与我们在[第2章](ch02.html#finding_and_wrangling_time_series_data)中的讨论一致，根据电力使用的模式，我们可能可以猜测哪些时间段代表标准人类日程的一部分。我们可能还可以识别周末。我们在这里没有这样做，但包含关于一天的时间和一周的时间模型可能是一个更深入分析和建模这一数据集的好主意。
- en: '![](assets/ptsa_1002.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1002.png)'
- en: Figure 10-2\. 24 hours sampled from the data set for 3 different locations out
    of the 321 locations available in the data set. While we do not have a sense of
    which local hours these indices correspond to, we do see a coherent daily pattern.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 从数据集中抽取的三个不同位置的24小时样本，数据集中共有321个位置。虽然我们不知道这些索引对应的本地小时是哪些，但我们看到了一个连贯的日常模式。
- en: '![](assets/ptsa_1003.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1003.png)'
- en: Figure 10-3\. A full seven-day cycle of the sampled data for the same three
    locations as pictured in the daily plot. Our sense of a daily pattern is confirmed
    with this broader look at the data, indicating that one large peak per day along
    with some smaller features seems consistent in the behavior.
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 同一三个位置的数据的完整七天周期样本，与日常图中显示的相同。通过这种更广泛的数据查看，我们确认了我们对日常模式的感觉，表明每天有一个大峰值以及一些较小的特征在行为上似乎是一致的。
- en: While we could predict the absolute values of the data as our forecasting technique,
    this has already been done in both academic papers and blog posts. Instead, we
    are going to predict the difference of the data. Predicting differences rather
    than total values of a time series tends to be more challenging because the data
    is noisier, and we can see this even in analogous plots of what we have done earlier
    (see [Figure 10-4](#fig-1004)):^([2](ch10.html#idm45576028867064))
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以预测数据的绝对值作为我们的预测技术，但这在学术论文和博客文章中已经完成。相反，我们将预测数据的差异。预测时间序列的差异而不是总值往往更具挑战性，因为数据更加嘈杂，我们可以在我们之前做过的类似图中看到这一点（见[图 10-4](#fig-1004)）:^([2](ch10.html#idm45576028867064))
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](assets/ptsa_1004.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1004.png)'
- en: Figure 10-4\. A week-long sample of the same three sites’ differenced power
    time series, representing the change from hour to hour of electric use. While
    this series still exhibits a pattern, as did the original series, the unpredictable
    components of the series are made more apparent as they represent a larger portion
    of the value of the differenced series than they do of the original series.
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 同一三个站点的电力时间序列的一个星期样本，表示电力使用的逐小时变化。尽管这个序列仍然表现出模式，就像原始序列一样，但序列的不可预测组成部分变得更加明显，因为它们在差异系列的值中所占比例比在原始序列中所占比例要大。
- en: If we were going to run a traditional statistical model, a state space model,
    or even a machine learning model, we would need to do a fair amount of analysis
    at this point to look at correlations between different electricity-consuming
    sites in the data set. We would want to see whether there was drift over time
    in the data and assess stationarity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要运行传统的统计模型，状态空间模型，甚至是机器学习模型，我们需要在此时进行大量分析，以查看数据集中不同用电站点之间的相关性。我们需要看看数据中是否随时间漂移，并评估其平稳性。
- en: You should do these things too for deep learning so you can assess appropriate
    models for a data set and also establish expectations for how well you think your
    model can perform. However, the beauty of deep learning is that we can move forward
    even with somewhat messier data and even without passing any specific statistical
    tests for the quality of our data. In a production environment, we would spend
    more time on data exploration, but for the purposes of this chapter we will move
    on to our modeling options.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您也应该为深度学习做这些事情，这样您可以评估数据集的适当模型，并为您认为您的模型可以执行多好建立期望。然而，深度学习的美妙之处在于，即使我们的数据有些凌乱，甚至没有通过任何特定的统计测试来评估我们数据的质量，我们也可以继续前进。在生产环境中，我们将花更多时间进行数据探索，但出于本章的目的，我们将继续进行建模选项。
- en: Steps of a Training Pipeline
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练管道的步骤
- en: In general when we are modeling with a neural network, there are a few common
    steps our scripts will always have. These scripts can often be more challenging
    to write than when we are fitting statistical or traditional machine learning
    models because our data sets tend to be larger. Also we fit these deep models
    in batches so that we are using iterators rather than arrays of the full data
    set.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当我们使用神经网络建模时，我们的脚本总是会有一些共同的步骤。这些脚本通常比我们拟合统计或传统机器学习模型时更难撰写，因为我们的数据集往往更大。此外，我们以批次方式拟合这些深层模型，因此我们使用迭代器而不是整个数据集的数组。
- en: 'A data pipeline will include the following steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道将包括以下步骤：
- en: Make our code easily configurable by importing a preset list of parameters for
    default training values, which is particularly convenient since there are otherwise
    so many values to set.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过导入默认训练值的预设参数列表，使我们的代码易于配置，这特别方便，因为否则有太多的值需要设置。
- en: Pull the data into memory and preprocess it.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据加载到内存中并进行预处理。
- en: Shape the data into the appropriately expected format.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据塑造成适当预期的格式。
- en: Build iterators appropriate to the deep learning module you are using.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建适合您使用的深度学习模块的迭代器。
- en: Build a graph that uses these iterators to know what shape of data to expect;
    this includes building out the entire model.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个图表，使用这些迭代器来了解期望的数据形状；这包括构建整个模型。
- en: Set training parameters, such as your optimizer, your learning rate, and how
    many epochs you will train for.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置训练参数，如优化器、学习率以及训练的时期数。
- en: Establish some recordkeeping system both for your weights and for your results
    from epoch to epoch.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一些记录系统，用于记录您的权重和每个时期的结果。
- en: Making our code easily configurable
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使我们的代码易于配置。
- en: 'The following code shows how we have accomplished these tasks. We start with
    a list of standard imports:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了我们如何完成这些任务。我们从标准导入列表开始：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we use a mix of hardcoded variables and adjustable parameters. To some
    extent these are a matter of experience and your priorities when training. Don’t
    expect the variables to make much sense yet, as many of these parameters apply
    to neural network components we will discuss later in the chapter. The main thing
    is to notice the adjustability of the parameters:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用一些硬编码变量和可调参数。在一定程度上，这些是基于经验和您在训练时的优先考虑因素。不要指望这些变量现在能够很有意义，因为其中许多参数适用于我们稍后在本章讨论的神经网络组件。主要的事情是注意参数的可调性：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s crucial to have many adjustable parameters because training a deep learning
    model always involves hyperparameter searches to improve your model from your
    baseline. Commonly tuned hyperparameters affect all aspects of training, from
    data preparation (how far back in time to look) to model specification (how complex
    and what kind of a model to build) and training (how long to train, and how large
    a learning rate to use).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 具有许多可调参数至关重要，因为训练深度学习模型始终涉及超参数搜索，以改进从基线到模型的效果。通常调整的超参数会影响训练的各个方面，从数据准备（查看多久以前的时间）到模型规范（建立多复杂和什么样的模型）以及训练（训练多长时间，使用多大的学习率）。
- en: There are distinct categories of parameters in the preceding code. First, the
    data shaping relates to how we want to take our raw input, in this case a CSV
    full of parallel differenced time series of electricity use at 321 stations. To
    shape our data, we need two parameters. The `window` variable is how far back
    in time we will allow our models to look when attempting to make predictions.
    The `horizon` variable is how far forward we want to predict. Notice that these
    aren’t specific to units, such as “5 minutes,” but rather to time steps, consistent
    with our practices in earlier chapters. Like other statistical and machine learning
    models, neural networks care about our computational representation and don’t
    have any sense of 5 minutes versus 5 eons when looking at data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中有不同类别的参数。首先，数据形状化涉及我们如何处理我们的原始输入，本例中是一个CSV文件，其中包含321个站点的并行差分时间序列电力使用情况。为了塑造我们的数据，我们需要两个参数。`window`变量是我们允许我们的模型在试图进行预测时向后查看的时间范围。`horizon`变量是我们希望向前预测的时间跨度。请注意，这些并不是特定的单位，比如“5分钟”，而是时间步长，与我们在早期章节的做法一致。像其他统计和机器学习模型一样，神经网络关心我们的计算表示，并不在意数据是5分钟还是5个纪元。
- en: The penultimate section, the training details, will often be the most important
    for hyperparameter optimization and the most commonly adjusted. It’s crucial at
    the outset to experiment with learning rate and make sure you have not picked
    something wildly off. A good rule of thumb is to start with 0.001 and then adjust
    up and down by orders of magnitude. It’s not so important to have just the right
    learning rate, but it is important to have the right order of magnitude.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 倒数第二部分，培训细节，通常是超参数优化中最重要的部分，也是最常调整的部分。在开始时，调整学习率并确保你没有选择一个远远偏离的数值是至关重要的。一个好的经验法则是从0.001开始，然后按数量级上下调整。拥有恰当的学习率并不是那么重要，但拥有正确的数量级却是很重要的。
- en: The model specifications allow us to specify a variety of models (such as an
    RNN versus a CNN) and architectural details about those models. In general, we
    will want to tune hyperparameters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 模型规范允许我们指定各种模型（如RNN与CNN之间的比较）以及关于这些模型的架构细节。一般来说，我们将希望调整超参数。
- en: 'For the current examples, we use the following hyperparameters fed to our script
    at the command line:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前的示例，我们在命令行中使用以下超参数提供给我们的脚本：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Prepping our input data
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备我们的输入数据
- en: Once we have configurable parameters, there is a way to provide information
    to our script such as where to find a file, how far ahead we want to predict,
    and how far back in time we want to include in our inputs for a given horizon.
    These configurable parameters are important even at the preliminary step of reading
    in data and shaping it properly. We also need to arrange infrastructure to work
    through the data, because neural networks are trained via variations of stochastic
    gradient descent, which means that training takes place on small batches of data
    at a time, with one epoch meaning that all data has been used for training (although
    not all at the same time).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了可配置的参数，就有一种方法可以向我们的脚本提供信息，例如文件的位置、我们想要预测的提前量以及我们想要在给定时间段内包含的时间回溯量。即使在初步读取和正确形状化数据的阶段，这些可配置的参数也非常重要。我们还需要安排基础设施来处理数据，因为神经网络是通过各种随机梯度下降的变体进行训练的，这意味着训练是逐批次进行的，一个周期意味着所有数据都已用于训练（尽管不是同时）。
- en: Next we discuss both the high-level process of providing data for training via
    iterators and the low-level details of shaping the data that goes into the iterators.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们讨论通过迭代器为训练提供数据的高级过程以及塑造输入到迭代器的数据的细节。
- en: Shaping the input data
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 形状化输入数据
- en: In the preceding section, we saw how to form iterators from NumPy arrays, and
    we took the provision of those NumPy arrays for granted. Now we will discuss how
    the data is shaped, first conceptually, and then with a code example. We will
    discuss two data formats, NC and NTC.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们看到了如何从NumPy数组中形成迭代器，并且我们认为这些NumPy数组是已经准备好的。现在我们将讨论数据的形状，首先是概念上，然后是通过代码示例。我们将讨论两种数据格式，NC和NTC。
- en: We begin our discussion of different input data formats with a worked example
    that has nothing to do with coding. Let’s imagine we have multivariate time series
    data, with columns A, B, and C.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个与编码无关的实际示例开始讨论不同的输入数据格式。让我们想象我们有多变量时间序列数据，具有列A、B和C。
- en: '| Time | A | B | C |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | A | B | C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *t* – 3 | 0 | –1 | –2 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| *t* – 3 | 0 | –1 | –2 |'
- en: '| *t* – 2 | 3 | –2 | –3 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| *t* – 2 | 3 | –2 | –3 |'
- en: '| *t* – 1 | 4 | –2 | –4 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| *t* – 1 | 4 | –2 | –4 |'
- en: '| *t* | 8 | –3 | –9 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| *t* | 8 | –3 | –9 |'
- en: We want to build a model that predicts one step ahead, and we want to use the
    data from the previous two points in time to predict. We want to use the data
    from A, B, and C to predict A, B, and C. We will call our inputs X and our outputs
    Y.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要建立一个模型，预测一步，我们想要使用前两个时间点的数据来预测。我们希望使用A、B和C的数据来预测A、B和C。我们将称我们的输入为X，输出为Y。
- en: 'We seek to predict Y at time *t*. At time *t*, these are the actual values
    of Y = [A, B, C]:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图预测时间*t*的Y。在时间*t*，这些是Y的实际值 = [A, B, C]：
- en: '| *t* | 8 | –3 | –9 |'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_TB
  zh: '| *t* | 8 | –3 | –9 |'
- en: 'We specified that we would have the previous two time points available for
    all variables to make the prediction. That amounts to the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定，我们将为所有变量提供前两个时间点以进行预测。这总计如下：
- en: '| A, *t* – 1 | A, *t* – 2 | B, *t* – 1 | B, *t* – 2 | C, *t* – 1 | C, *t* –
    2 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| A, *t* – 1 | A, *t* – 2 | B, *t* – 1 | B, *t* – 2 | C, *t* – 1 | C, *t* –
    2 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 4 | 3 | –2 | –2 | –4 | –3 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | –2 | –2 | –4 | –3 |'
- en: 'Likewise, to predict Y at time *t* – 1, we have the following data as our target:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，为了预测时间*t* – 1时的Y，我们有以下数据作为我们的目标：
- en: '| *t* – 1 | 4 | –2 | –4 |'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_TB
  zh: '| *t* – 1 | 4 | –2 | –4 |'
- en: 'and we expect to have the following values available to make the prediction:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们期望能够使用以下值进行预测：
- en: '| A, *t* – 2 | A, *t* – 3 | B, *t* – 2 | B, *t* – 3 | C, *t* – 2 | C, *t* –
    3 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| A, *t* – 2 | A, *t* – 3 | B, *t* – 2 | B, *t* – 3 | C, *t* – 2 | C, *t* –
    3 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 3 | 0 | –2 | –1 | –3 | –2 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | –2 | –1 | –3 | –2 |'
- en: 'If we wanted to store the inputs for both these data points in one data format,
    it would look like this:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将这两个数据点的输入存储在一个数据格式中，它看起来会像这样：
- en: '| Time | A, time – 1 | A, time – 2 | B, time – 1 | B, time – 2 | C, time –
    1 | C, time – 2 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | A, 时间 – 1 | A, 时间 – 2 | B, 时间 – 1 | B, 时间 – 2 | C, 时间 – 1 | C, 时间 –
    2 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| *t* – 1 | 3 | 0 | –2 | –1 | –3 | –2 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| *t* – 1 | 3 | 0 | –2 | –1 | –3 | –2 |'
- en: '| *t* | 4 | 3 | –2 | –2 | –4 | –3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| *t* | 4 | 3 | –2 | –2 | –4 | –3 |'
- en: This is called the NC data format, where *N* indicates individual samples and
    *C* indicates channels, which is another way of describing multivariate information.
    We will use this data format for training a fully connected neural network, and
    it is the first option in the method we will eventually discuss that takes input
    data in CSV format and converts it to NumPy arrays of the proper shape and dimensionality.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为NC数据格式，其中*N*表示个体样本，*C*表示通道，这是描述多变量信息的另一种方式。我们将使用这种数据格式来训练全连接神经网络，并且它是我们将讨论的方法中的第一选项，该方法将输入数据以CSV格式接收并转换为正确形状和维度的NumPy数组。
- en: On the other hand, we can shape the data differently, in a way that creates
    a specific axis for time. This is commonly done by putting the data in NTC format,
    which designates the number of samples × time × channels. In this case, the sample
    is each row of the original data—that is, each slice in time for which we want
    to make a prediction (and for which we have available data to do so). The time
    dimension is how far back in time we will look, which in this example is two time
    steps (and which is specified by `--win` in our sample script for this chapter).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以以不同的方式塑造数据，以创建一个特定的时间轴。这通常通过将数据放置在NTC格式中来完成，该格式指定了样本数量 × 时间 × 通道数。在这种情况下，样本是原始数据的每一行，即我们想要进行预测的每个时间切片（并且我们有可用数据来这样做）。时间维度是我们将查看多远以前的时间，例如在此示例中为两个时间步长（并在我们本章的示例脚本中通过`--win`指定）。
- en: 'In the NTC format, the input data we formatted before would look something
    like this for predicting the *t* – 1 horizon:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在NTC格式中，我们之前格式化的输入数据看起来会像这样，以预测*t* – 1的水平：
- en: '| Time | A | B | C |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | A | B | C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *t* – 1 | 0, 3 | –1, –2 | –2, –3 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| *t* – 1 | 0, 3 | –1, –2 | –2, –3 |'
- en: 'Or if we wanted to represent the input data for both of the samples we produced
    earlier, we could do so compactly as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们想要紧凑地表示我们先前生成的两个样本的输入数据，我们可以这样做：
- en: '| Time | A | B | C |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | A | B | C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *t* – 1 | 0, 3 | –1, –2 | –2, –3 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| *t* – 1 | 0, 3 | –1, –2 | –2, –3 |'
- en: '| *t* | 3, 4 | –2, –2 | –3, –4 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| *t* | 3, 4 | –2, –2 | –3, –4 |'
- en: 'This could be joined to the labels we produced for Y like so:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以与我们为Y生成的标签连接在一起：
- en: '| Time | A | B | C |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | A | B | C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *t* – 1 | 4 | –2 | –4 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| *t* – 1 | 4 | –2 | –4 |'
- en: '| *t* | 8 | –3 | –9 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| *t* | 8 | –3 | –9 |'
- en: Neither of these representations is more accurate than the other, but one convenient
    aspect of the NTC representation is that time has meaning with an explicit temporal
    axis.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种表示法都没有比另一种更准确，但 NTC 表示法的一个便利之处在于时间具有显式的时间轴含义。
- en: The reason we form two shapes of inputs is that some models prefer one format
    and some the other. We will use the NTC format for input into convolutional and
    recurrent neural networks, which we discuss later in this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们形成两种输入形状的原因是一些模型更喜欢一种格式，而另一些模型喜欢另一种。我们将使用 NTC 格式将输入提供给卷积和循环神经网络，这些我们将在本章后面讨论。
- en: Building iterators
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建迭代器
- en: 'Broadly, to provide data to a training routine, we need to provide iterators.
    Iterators are not unique to deep learning or to Python but rather reflect the
    general idea of an object that works its way through some kind of collection,
    keeping track of where it is and indicating when it has worked through an entire
    collection. Forming an iterator is simple in the case of training data coming
    from a NumPy array. If `X` and `Y` are NumPy arrays, we see that forming iterators
    is simple:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上讲，为训练程序提供数据，我们需要提供迭代器。迭代器不仅适用于深度学习或 Python，而是反映了对象沿着某种集合的通用概念，跟踪其位置并指示何时完成整个集合的遍历。在训练数据来自
    NumPy 数组的情况下，形成迭代器非常简单。如果 `X` 和 `Y` 是 NumPy 数组，我们可以看到形成迭代器非常简单：
- en: '[PRE13]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here we have a method to prepare the iterators for the data set, and these iterators
    wrap `numpy` arrays that are received by a method called `prepared_data()` (more
    on this in a moment). Once the arrays are available, they are broken down into
    training, validation, and testing data sources, with the training data being the
    earliest, the validation being used as a way to tune hyperparameters with out-of-sample
    feedback, and testing data held until the end for true testing.^([3](ch10.html#idm45576028110856))
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一种准备数据集迭代器的方法，这些迭代器包装了由名为 `prepared_data()` 的方法接收的 `numpy` 数组（稍后详细说明）。一旦数组可用，它们将被分解为训练、验证和测试数据源，其中训练数据是最早的，验证数据用作调整超参数的一种方式，并具有外样本反馈，测试数据则保留到最后进行真正的测试。^([3](ch10.html#idm45576028110856))
- en: Notice that the initializer for an iterator takes the input (`data`), the target
    value (`label`), and `batch_size` parameter, which reflects how many examples
    will be used per iteration to compute gradients and update the model weights.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，迭代器的初始化程序接受输入（`data`）、目标值（`label`）和 `batch_size` 参数，该参数反映了每次迭代中将使用多少个示例来计算梯度并更新模型权重。
- en: Shaping the data in code
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在代码中形成数据的形状
- en: 'Now that we know the two shapes of data we want to create, we can look at the
    code that shapes it:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道要创建的两种数据形状，我们可以查看形成它的代码：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After reading our data in from a text file, we standardize each column. Notice
    that each column is standardized on its own rather than across the whole data
    set homogeneously. This is because we saw even in our brief data exploration that
    different electric sites had very different values (see the plots in Figures [10-2](#fig-1002)
    and [10-4](#fig-1004)):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本文件中读取数据后，我们对每一列进行标准化。请注意，每一列都是单独标准化的，而不是整个数据集统一标准化。这是因为即使在我们简短的数据探索中，我们也看到不同的电力站具有非常不同的值（请参见图
    [10-2](#fig-1002) 和 [10-4](#fig-1004) 中的图表）：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: NC data format
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NC 数据格式
- en: 'Producing the NC data format is fairly straightforward:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 生成 NC 数据格式非常简单：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To generate the *X* inputs representing time *t – h*, to make a prediction at
    *t*, we take *x* and remove the last *h* rows (since that input data would require
    label values later in time than the latest data we have). Then we shift this data
    back along the time axis to produce further lagged values, and we need to make
    sure the NumPy arrays representing different lags have the same shape so they
    can be stacked together. This is what leads to the preceding formulation. It’s
    worth working through this on your own computer and proving that it works out.
    You might also think about how to generalize the expression for an arbitrarily
    long lookback.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 生成代表时间 *t – h* 的 *X* 输入，以便在 *t* 时进行预测时，我们取 *x* 并移除最后 *h* 行（因为该输入数据需要比我们拥有的最新数据稍后的标签值）。然后，我们沿时间轴将这些数据向后移动，以产生进一步滞后的值，并且我们需要确保代表不同滞后的
    NumPy 数组具有相同的形状，以便它们可以堆叠在一起。这就是导致前述公式的原因。值得在自己的计算机上通过这个过程并证明其有效性。你还可以考虑如何将表达式推广为任意长的回顾。
- en: 'We check our work by setting a `Pdb` breakpoint and verifying that the values
    in `X` and `Y` match their expected counterparts in `x`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 `Pdb` 断点来检查我们的工作，并验证 `X` 和 `Y` 中的值是否与它们在 `x` 中的预期对应：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first half of the columns in `X` represent the last points in time we make
    available for prediction, and the label/target for the prediction is three steps
    ahead of this. That’s why `X[0, 1:10`] should match `x[1, 1:10]`, and `Y[0, 1:10]`
    should match `x[4, 1:10]`, because it should be three time steps ahead (our input
    set the horizon to `3`).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`中前半部分的列代表我们用于预测的最后时间点，而预测的标签/目标则比这个时间点晚三个步骤。这就是为什么`X[0, 1:10]`应该匹配`x[1,
    1:10]`，而`Y[0, 1:10]`应该匹配`x[4, 1:10]`，因为它应该是向前三个时间步（我们的输入集将视野设定为`3`）。'
- en: It can be confusing that time and samples (data point indices) often have the
    same label, but they are separate things. There is the time ahead we are forecasting,
    there is the time at which we are taking the snapshot of inputs to make a forecast,
    and there is the time back we look at to collect data to make the forecast. These
    values are necessarily interrelated, but it’s a good idea to keep the concepts
    separate.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会令人困惑的是，时间和样本（数据点索引）经常具有相同的标签，但它们是不同的概念。有我们预测的时间提前，有我们拍摄输入快照的时间，以便进行预测，还有我们查看以收集数据以进行预测的时间。这些值必然是相互关联的，但将这些概念分开是个好主意。
- en: NTC data format
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NTC数据格式
- en: 'Producing the NTC format is also not too bad:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 生成NTC格式也不算太难：
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: For any given example (the *N* dimension, i.e., the first dimension), we took
    the last `win` rows of the input data across all columns. That is how we create
    three dimensions. The first dimension is essentially the data point index, and
    the values provided in that data point amount to 2D data, namely time × channels
    (here, electricity sites).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何给定的示例（即*N*维度，即第一维度），我们获取了输入数据的最后`win`行，跨所有列。这就是我们创建三维数据的方式。第一维度实际上是数据点索引，而在该数据点中提供的值总计为2D数据，即时间
    × 通道（在这里是电力站）。
- en: 'As before, we set a `Pdb` breakpoint to test this code. Note also that we confirm
    our own understanding of what we have done to test the code. Often the code to
    test a data format is more illuminating than the actual code because we use concrete
    numbers to do spot checks:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们设置了`Pdb`断点来测试这段代码。同时注意，我们确认了我们对代码测试的理解。通常情况下，测试数据格式的代码比实际代码更具启发性，因为我们使用具体数字进行抽查测试：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We see that the first data point we have prepared in `X` and `Y` (that is, the
    first row) corresponds to rows 0:96 (because we set our configurable window lookback
    to be 96 time steps in our parser inputs), and the forward horizon 3 time steps
    ahead corresponds to row 98 (because `x` ends at 95; remember, indexing on a slice
    excludes the last number in the slice, so `x` represents all the rows from 0 to
    95 inclusive or 0 to 96 exclusive).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们准备在`X`和`Y`中的第一个数据点（即第一行）对应于行0到96（因为我们将可配置的窗口回溯设置为96个时间步长），而向前预测3个时间步对应于第98行（因为`x`结束于95；请记住，对切片的索引排除切片中的最后一个数字，因此`x`表示从0到95（包括0到96）的所有行）。
- en: Data processing code is bug-prone, messy, and slow. However, you will find that
    the more times you write it and work through it, the more sense it makes. Nonetheless,
    it’s good to test your data processing code thoroughly and then keep it somewhere
    safe so you can avoid having to work through the sample problems every time you
    need to shape your data. It’s also wise to keep this code in a versioning system
    and have some way to track which version of your code was used in training a particular
    model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理代码容易出错，混乱且速度慢。然而，您会发现，每次编写和处理它时，它都会变得更加清晰。尽管如此，彻底测试您的数据处理代码并将其安全地保存在某处，以避免每次需要整理数据时都需要处理示例问题，这是明智的做法。此外，将此代码保存在版本控制系统中，并有一些方式跟踪使用哪个版本的代码来训练特定模型，也是明智的做法。
- en: Setting training parameters and establishing a recordkeeping system
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置训练参数并建立记录系统
- en: We will discuss the details of various models in the coming sections, so for
    now we bypass the portion of the code that relates to graph building and go directly
    to training and recordkeeping logistics.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节讨论各种模型的细节，因此暂时跳过与图构建相关的代码部分，直接进入训练和记录保留的逻辑。
- en: 'This is how we implement training in the simple examples we will focus on:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在接下来要重点讨论的简单示例中如何实现训练的方法：
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding code accomplishes a diverse set of tasks, all routine. First,
    we set up values to track the history of the validation accuracy scores as a way
    to make sure training is seeing improvements. If a model is not training sufficiently
    quickly, we don’t want to keep spinning our GPUs, wasting both time and electricity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码完成了多样的任务，都是例行公事。首先，我们设置了值来跟踪验证准确性分数历史，以确保训练看到改进。如果模型训练速度不够快，我们不希望继续旋转我们的GPU，浪费时间和电力。
- en: 'The MXNet boilerplate uses the `Module` API (as opposed to the `Gluon` API
    we saw earlier in this chapter):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet的样板使用`Module` API（而不是我们在本章前面看到的`Gluon` API）：
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'These four lines of code accomplish the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这四行代码实现了以下功能：
- en: Set up the raw component of a neural network as a computational graph.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将神经网络的原始组件设置为计算图。
- en: Set up the data shapes so the network knows what to expect and can optimize.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置数据形状，以便网络知道要期望什么并进行优化。
- en: Initialize all weights in the graph to random values (this is an art, not purely
    a random set of numbers drawn from infinite possibilities).
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化图中所有权重为随机值（这是一门艺术，不仅仅是从无限可能性中随机选择的一组数字）。
- en: Initialize an optimizer, which can come in a variety of flavors and for which
    we explicitly set the initial learning rate depending on our input parameters.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化优化器，可以有各种风味，我们根据输入参数显式设置初始学习率。
- en: 'Next we use our training data iterator to increment our way through the data
    as we train:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用我们的训练数据迭代器逐步递增我们在训练过程中的数据：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then measure the predicted results both for the training and validation
    sets (in the same outer `for` loop as before):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们测量训练集和验证集的预测结果（与之前相同的外部`for`循环）：
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The loop concludes with some logic for early stopping:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 循环结束时有一些早停逻辑：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This clunky code does some recordkeeping and simple logic, recording each successive
    correlation value between predicted and actual values. If the correlation from
    epoch to epoch has failed to improve sufficiently for enough epochs (or has even
    gotten worse), training will stop.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笨重的代码做一些记录和简单逻辑，记录预测值与实际值之间每个连续相关值。如果从时代到时代的相关性没有充分改善足够的时代（或者甚至变得更糟），训练将停止。
- en: Evaluation metrics
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: 'Our function, `evaluate_and_write`, both records the correlations per epoch
    and the raw value for both the target value and estimated value. We do the same
    for the testing at the end of all training:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的函数`evaluate_and_write`，同时记录每个时代的相关性和目标值与估计值的原始值。我们在所有训练结束时对测试也是如此：
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This in turn makes use of a correlation function we define as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这反过来又利用了我们定义的相关函数，如下所示：
- en: '[PRE26]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Occasionally in this data set there are cases of zero variance, which can create
    a `NAN` in a column, so we opt to use `np.nanmean()` rather than `np.mean()`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中偶尔会出现零方差的情况，这可能会在一列中创建一个`NAN`，所以我们选择使用`np.nanmean()`而不是`np.mean()`。
- en: Notice that one basic functionality we don’t include here is to save the weights
    of the model, checkpointing throughout the training process. If we were training
    for production and needed to be able to reload the model and deploy it, we would
    want to use `Module.save_checkpoint` (to save the weights) and `Module.load` (to
    load a model back into memory, from which point you can continue training or deploy
    your model to production). There is a lot to learn to get started with a proper
    deep learning pipeline, but here we are keeping it basic.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里不包括的一个基本功能是保存模型权重，通过训练过程进行检查点。如果我们正在为生产进行训练并且需要能够重新加载模型并部署它，我们将使用`Module.save_checkpoint`（保存权重）和`Module.load`（从此加载模型回到内存，从此你可以继续训练或将模型部署到生产中）。有很多东西可以学习，开始一个合适的深度学习管道，但在这里我们保持基本。
- en: Putting it together
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汇总
- en: 'We put our pipeline components together in the body of our `__main__` scope:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们的`__main__`范围内把我们的管道组件放在一起：
- en: '[PRE27]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here we make use of the infrastructure we have just arranged. First, we parse
    our command-line arguments. Then we create iterators with the configurable inputs,
    including our prediction horizon, our lookback window, our batch size, and the
    name of the model we want to build. We create MXNet symbols and also record the
    input shape, and these are passed along as we create the model. Finally, we pass
    information about the model, along with our iterators and our save directory,
    to the training function, which does the interesting part: train a model and output
    its performance metrics.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们利用我们刚刚安排好的基础设施。首先，我们解析命令行参数。然后，我们创建可配置输入的迭代器，包括我们的预测周期、回溯窗口、批处理大小以及我们想要构建的模型的名称。我们创建MXNet符号，并记录输入形状，这些都传递给我们创建模型的过程中。最后，我们将关于模型的信息、以及我们的迭代器和保存目录传递给训练函数，这部分很有趣：训练模型并输出其性能指标。
- en: So in this case, we see a minimal but fully functional training pipeline, featuring
    data ingestion and reshaping, model construction, model training, and logging
    of important values for model assessments.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，我们看到一个最小但完全功能的训练流水线，包括数据摄取和重塑、模型构建、模型训练以及重要数值记录用于模型评估。
- en: 'By the way, our `print_to_file()` method is just a handy wrapper for `print()`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，我们的`print_to_file()`方法只是`print()`的一个便利包装器：
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You will want to create a record of your model as it trains. It’s possible that
    your preferred model weights will not be the ones where you completed your training
    run but sometime earlier. Having a record of how training progressed will help
    you tune hyperparameters related to both model structure and training, ranging
    from how many parameters to have (to avoid underfitting or overfitting) to what
    learning rate to train at (to avoid making overly large or small adjustments during
    training).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练模型时，你会想要创建一个记录。可能你最终选择的模型权重并不是你完成训练时的权重，而是之前的某个时间点。记录训练进展将帮助你调整与模型结构和训练相关的超参数，从参数数量的选择（以避免欠拟合或过拟合）到训练时的学习率（以避免在训练过程中进行过大或过小的调整）。
- en: We now have a full minimal viable pipeline, except that we are missing our models.
    Now we will look at some basic kinds of models that can be applied to time series
    data, and we will train each to see its relative performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个完整的最小可行流水线，只是缺少我们的模型。现在我们将看一些可以应用于时间序列数据的基本模型，并训练每个模型以查看其相对性能。
- en: Feed Forward Networks
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈网络
- en: 'This book is quite unusual in presenting feed forward networks in the context
    of time series analysis. Most modern time series analysis problems are undertaken
    with recurrent network structures, or, less commonly, convolutional network structures.
    We, however, begin with the feed forward neural network, as it’s the simplest
    and most historic network structure. There are several reasons this is a good
    place to begin:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书在时间序列分析的背景下呈现前馈网络相当不寻常。大多数现代时间序列分析问题使用递归网络结构或者较少见的卷积网络结构。然而，我们从前馈神经网络开始，因为它是最简单和最具历史意义的网络结构。有几个原因使得它是一个好的起点：
- en: Feed forward networks are highly parallelizable, which means they are quite
    performant. If you can find a reasonably good feed forward model for your purposes,
    you can compute it very quickly.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络（Feed forward networks）具有高度可并行化的特点，这意味着它们的性能相当出色。如果你能找到一个合适的前馈模型，你可以非常快速地计算它。
- en: Feed forward networks are good tests for whether there really are complex time-axis
    dynamics in your sequence. Not all time series really are time series in the sense
    of having time-axis dynamics, where earlier values have a specific relationship
    to later values. It can be good to fit a feed forward neural network as one baseline,
    apart from a simpler linear model.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络是检验您的序列是否真的具有复杂时间轴动态的良好测试。并非所有时间序列都真的是时间序列，即早期值与后续值具有特定关系的意义。将前馈神经网络作为一个基线进行拟合可能是一个好方法，与更简单的线性模型分开。
- en: Feed forward network components are often integrated into larger and more complex
    time series deep learning architectures. Therefore, you need to know how these
    work even if they will not form an entire model in your work.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络组件通常集成到更大更复杂的时间序列深度学习架构中。因此，即使它们在您的工作中不会形成整个模型，您也需要了解它们的工作原理。
- en: A Simple Example
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单示例
- en: A feed forward network is the simplest kind of neural network, so we begin by
    applying a feed forward architecture to a time series example. There is nothing
    in the structure of a standard feed forward neural network that indicates a temporal
    relationship, but the idea is that the algorithm may nonetheless learn something
    of how past inputs predict future inputs. We saw an example of a feed forward
    network in [Figure 10-1](#fig-1001). A feed forward network is a series of fully
    connected layers, meaning the input to each layer connects to every node in the
    graph.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络是最简单的神经网络类型，因此我们首先将前馈架构应用于一个时间序列示例。标准前馈神经网络的结构中并没有暗示时间关系，但算法可能仍然能够学习过去输入如何预测未来输入。我们在
    [图 10-1](#fig-1001) 中看到了一个前馈神经网络的示例。前馈神经网络是一系列全连接层，即每一层的输入连接到图中的每个节点。
- en: 'We start with this simple example, which uses the first kind of data formatting
    we saw earlier. That is, our input, `X`, will simply be two-dimensional data of
    N × C (i.e., samples × channels), where the time components has been flattened
    into channels. That corresponds to this data formatting:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这个简单的示例开始，它使用了我们之前看到的第一种数据格式化方式。也就是说，我们的输入 `X` 简单地是 N × C 的二维数据（即样本 × 通道），其中时间成分已被展平成通道。这对应于以下数据格式化方式：
- en: '| A, *t* – 2 | A, *t* – 3 | B, *t* – 2 | B, *t* – 3 | C, *t* – 2 | C, *t* –
    3 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| A, *t* – 2 | A, *t* – 3 | B, *t* – 2 | B, *t* – 3 | C, *t* – 2 | C, *t* –
    3 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 3 | 0 | –2 | –1 | –3 | –2 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | –2 | –1 | –3 | –2 |'
- en: 'As a reminder, in code that represents this branch of the data:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，在代表数据这一分支的代码中：
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then build a fully connected model with MXNet’s `Module` API:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用 MXNet 的 `Module` API 构建一个全连接模型：
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here we build a three-layer, fully connected network. The first layer has 20
    hidden units, and the second layer has 10 hidden units. After the first two layers,
    there is an “activation” layer. This is what makes the model nonlinear, and without
    this it would simply be a series of matrix multiplications that would come down
    to a linear model. A fully connected layer effectively is:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们构建了一个三层全连接网络。第一层有 20 个隐藏单元，第二层有 10 个隐藏单元。在前两层之后，有一个“激活”层。这是使模型非线性的关键所在，如果没有这一层，模型将简单地成为一系列矩阵乘法，最终变成一个线性模型。全连接层有效地是：
- en: <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Y</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></math>
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Y</mi><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></math>
- en: '*W* is a set of weights corresponding to those of the correction dimensions
    for a matrix multiplication, with the output resulting in a vector of dimensions
    equal to the hidden units. This is then added with a bias. The weights in *W*
    and *b* are both trainable. However, training one such set of weights, or even
    a series of such weights, would not be very interesting—it would be a roundabout
    way of running a linear regression. However, it is the activation applied after
    this set of matrix operations that creates the interest. The activation layer
    applies various nonlinear functions, such as the tanh and ReLU, which are pictured
    in Figures [10-5](#fig-1005) and [10-6](#fig-1006), respectively.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*W* 是一组权重，对应于矩阵乘法的修正维度，其输出结果是一个维度等于隐藏单元数的向量。然后加上一个偏置项 *b*。 *W* 和 *b* 中的权重都是可训练的。然而，训练一个这样的权重集，甚至一系列这样的权重集，将会非常无聊——它将是一个迂回的线性回归方式。然而，是在这组矩阵运算后应用的激活函数，如
    tanh 和 ReLU，造成了兴趣，这些函数在图 [10-5](#fig-1005) 和 [10-6](#fig-1006) 中有所展示。'
- en: '![](assets/ptsa_1005.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1005.png)'
- en: Figure 10-5\. The tanh function exhibits nonlinear behavior at small values
    before becoming functionally constant, with a zero derivative, at higher values.
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. tanh 函数在小值处表现出非线性行为，随后在更高的值处变为函数常数，其导数为零。
- en: '![](assets/ptsa_1006.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1006.png)'
- en: Figure 10-6\. The ReLU function is easy to compute while also introducing a
    nonlinearity.
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. ReLU 函数易于计算，同时引入了非线性。
- en: In the preceding code, you will notice we use the ReLU function as our activation
    function. It often makes sense to test a variety of activation functions (ReLU,
    tanh, sigmoid) in your early stages of model construction and hyperparameter tuning.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，您会注意到我们将 ReLU 函数作为激活函数。在模型构建和超参数调整的早期阶段，测试各种激活函数（ReLU、tanh、sigmoid）通常是有意义的。
- en: 'Now let’s train with this model and see how it performs:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用这个模型进行训练，并看看它的表现：
- en: '[PRE31]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Is this good performance? It’s hard to know in a void. When you progress to
    a deep learning model, you should put it into production only if it can outperform
    the substantially simpler models we have looked at in previous chapters. A deep
    learning model will take longer to compute a prediction and generally has higher
    overhead, so it should be used only when it can justify the additional costs.
    For this reason, when you are approaching a time series problem with deep learning
    tools, you usually want to have previous models you have fit to serve as an objective
    standard to beat.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个良好的表现吗？在一个孤立的情况下很难说。当你进入深度学习模型时，只有当它能超越我们在前几章中查看过的显著简化模型时，才应该将其投入生产。深度学习模型预测所需的时间较长，通常具有更高的开销，因此只有当它能证明额外成本是合理的时候才应该使用。因此，当你用深度学习工具解决时间序列问题时，通常希望有先前拟合的模型作为一个客观的标准来超越。
- en: In any case, we see that even a model without any temporal awareness can train
    and produce predictions that are reasonably correlated with measured values.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们看到即使一个没有任何时间意识的模型也能训练并产生与测量值合理相关的预测。
- en: Using an Attention Mechanism to Make Feed Forward Networks More Time-Aware
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用注意力机制使前馈网络更具时间感知性
- en: Although feed forward networks have not made great strides in time series problems,
    there is still research being done to imagine architectural variations that could
    enhance their performance on sequential data. One idea is *attention*, which describes
    mechanisms added to a neural network that allow it to learn what part of a sequence
    to focus on and what part of an incoming sequence might relate to desirable outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前馈网络在时间序列问题上并未取得重大进展，但仍在进行研究，以设想可能增强其在序列数据上性能的架构变体。其中一种想法是*注意力*，它描述了添加到神经网络中的机制，允许它学习集中精力处理序列的哪一部分以及传入序列的哪一部分可能与期望的输出相关。
- en: The idea of attention is that a neural network architecture should provide a
    mechanism for a model to learn which information is important when. This is done
    through *attention weights*, which are fit for each time step so that the model
    learns how to combine information from different time steps. These attention weights
    multiply what would otherwise be either the output or hidden state of a model,
    thereby converting that hidden state to a *context vector*, so-called because
    the hidden state is now better contextualized to account for and relate to all
    the information contained in the time series over time, hopefully including the
    temporal patterns.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的概念是，神经网络架构应该提供一种机制，让模型学会在什么时候学习哪些信息是重要的。这通过*注意力权重*来实现，每个时间步都要调整这些权重，使得模型学会如何结合不同时间步的信息。这些注意力权重乘以模型的输出或隐藏状态，从而将隐藏状态转换为*上下文向量*，因为隐藏状态现在更好地融入和关联时间序列中的所有信息，希望包括时间模式。
- en: Attention first came into use within recurrent neural networks (more on these
    later in this chapter), but its use and implementation is actually easier to understand
    in a variant proposed for a feed forward architecture in a [research paper](https://arxiv.org/pdf/1512.08756.pdf).
    The paper proposes a way to apply feed forward neural networks to sequential data
    such that the network’s reaction to each step of the sequence can be used as an
    input to the ultimate output of that network. This approach also allows variable-length
    sequences, just as variable-length time series are common in real-world problems.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力首先在循环神经网络中开始使用（本章后面将更详细介绍），但其使用和实现实际上更容易理解，在一篇[研究论文](https://arxiv.org/pdf/1512.08756.pdf)中提出了一种应用于前馈架构的变体。该论文提出了一种方法，可以将前馈神经网络应用于序列数据，使得网络对序列的每个步骤的反应可以用作该网络最终输出的输入。这种方法还允许变长序列，就像真实世界问题中常见的变长时间序列一样。
- en: '[Figure 10-7](#fig-1007) shows an example architecture of how a feed forward
    neural network can be used for a task that involves processing sequential data.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-7](#fig-1007)展示了一个示例架构，说明了前馈神经网络如何用于处理序列数据的任务。'
- en: '![](assets/ptsa_1007.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1007.png)'
- en: Figure 10-7\. A “feed forward attention mechanism.”
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. “前馈注意力机制。”
- en: A feed forward network is applied to the inputs at each time step individually,
    generating a “hidden state” for each time step *h[1]*...*h[T]*. This effectively
    creates a series of hidden states. A second neural network, developed to learn
    <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi> <mo>(</mo> <msub><mi>h</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></math> , is depicted in the corner, and this
    is the attention mechanism. The attention mechanism allows the network to learn
    how much to weight each input, where each input represents a state coming from
    a different point in time. This allows the mechanism to determine which time steps
    to weight more or less heavily in the final summation of inputs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步骤，前馈网络逐个应用于输入，为每个时间步骤生成一个“隐藏状态” *h[1]*...*h[T]*。这有效地创建了一系列隐藏状态。第二个神经网络专门用于学习
    <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi> <mo>(</mo> <msub><mi>h</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></math> ，显示在角落里，这就是注意机制。注意机制允许网络学习如何加权每个输入，其中每个输入表示来自不同时间点的状态。这使得机制能够确定在最终输入的总和中对哪些时间步骤进行更重或更轻的加权。
- en: Then the hidden states of different times are combined with their attention
    coefficients before final processing to produce whatever target/label is sought.
    The researchers who designed this network found that it performed respectably
    on a variety of tasks usually thought to necessitate a recurrent neural network
    to meet the requirements of remembering earlier inputs and combining them with
    later ones.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在最终处理之前，不同时间的隐藏状态与它们的注意系数结合起来，以生成所寻找的目标/标签。设计这个网络的研究人员发现，在通常被认为需要递归神经网络来满足记住先前输入并将其与后来的输入结合起来的要求的各种任务上，它表现得相当不错。
- en: This is a great example of how there are no simple architectures in deep learning
    models because there are so many possibilities for adapting even a basic model
    to a complex time series question.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的例子，说明在深度学习模型中没有简单的架构，因为甚至基本模型也有很多适应复杂时间序列问题的可能性。
- en: As noted, feed forward neural networks are not the leading networks for time
    series problems. Nonetheless, they are a useful starting point to establish the
    performance of a relatively simple model. Interestingly, if we do not include
    activation functions, we can use these to code up AR and VAR models using the
    MXNet framework. This can sometimes come in handy in its own right. More importantly,
    there are architectural variations of fully connected models that can be quite
    accurate for certain time series data sets.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 正如注意到的那样，前向神经网络并不是处理时间序列问题的领先网络。然而，它们是建立相对简单模型性能的一个有用起点。有趣的是，如果我们不包括激活函数，我们可以使用这些来使用MXNet框架编码AR和VAR模型。这在某些情况下可能非常有用。更重要的是，有全连接模型的架构变体可以对某些时间序列数据集非常精确。
- en: CNNs
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNNs
- en: If you already have exposure to deep learning, you are likely quite familiar
    with convolutional neural networks (CNNs). Most of the jaw-dropping, record-breaking,
    human-like functionality attributable to computers in the last years has been
    thanks to extremely sophisticated and complex convolutional architectures. All
    that notwithstanding, the idea of a convolution is fairly intuitive and long predates
    deep learning. Convolutions have long been used in more transparent human-driven
    forms of image processing and image recognition studies, starting with something
    as simple as a Gaussian blur. If you are not familiar with the concepts of image
    processing kernels and don’t have an idea of how you might code one up, [Stack
    Overflow](https://perma.cc/8U8Y-RBYW) provides a good explanation of how this
    can be done both with a high-level API or manually via NumPy.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经接触过深度学习，您很可能对卷积神经网络（CNNs）非常熟悉。过去几年，计算机实现了极其复杂和复杂的卷积架构，以至于产生了令人瞠目结舌的、创纪录的、人类化的功能。尽管如此，卷积的概念相当直观，并且早在深度学习之前就已经存在。卷积长期以来在更透明的人类驱动的图像处理和图像识别研究中被广泛使用，从简单的高斯模糊开始。如果您不熟悉图像处理内核的概念，也不知道如何编写一个，[Stack
    Overflow](https://perma.cc/8U8Y-RBYW) 提供了一个很好的解释，说明了如何使用高级API或通过NumPy手动实现。
- en: '*Convolution* means applying a kernel (a matrix) to a larger matrix by sliding
    it across the larger matrix, forming a new one. Each element of the new matrix
    is the sum of element-wise multiplication of the kernel and a subsection of the
    larger matrix. This kernel is applied repeatedly as it slides across a matrix/image.
    This is done with a prespecified number of kernels so that different features
    can emerge. A schematic of how this works on many layers is shown in [Figure 10-8](#fig-1008).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*意味着将一个核（一个矩阵）应用到一个较大的矩阵上，通过在较大矩阵上滑动核来形成一个新的矩阵。新矩阵的每个元素是核和较大矩阵的一个子部分的逐元素乘法的总和。这个核被重复应用，因此不同的特征可以出现。如何在多层上运作的示意图显示在[图 10-8](#fig-1008)中。'
- en: '![](assets/ptsa_1008.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1008.png)'
- en: Figure 10-8\. A convolutional network. Many two-dimensional windows of specified
    kernel size slide across the original image, producing many feature maps out of
    the trainable weights applied to the image. Often these are pooled and otherwise
    post-processed with an activation function. The process is repeated several times
    over several layers to collapse many features down into a smaller range of values,
    ultimately leading to, for example, classification ratings.
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 一个卷积网络。许多指定核大小的二维窗口滑过原始图像，通过应用于图像的可训练权重产生许多特征映射。通常，这些特征映射会被汇聚和用激活函数进行后处理。这个过程在几层中重复进行，将许多特征折叠到一个较小的值范围内，最终导致例如分类评分。
- en: For many reasons, traditional convolution is a poor match to time series. A
    main feature of convolutions is that all spaces are treated equally. This makes
    sense for images but not for time series, where some points in time are necessarily
    closer than others. Convolutional networks are usually structured to be scale
    invariant so that, say, a horse can be identified in an image whether it’s a larger
    or smaller portion of the image. However, in time series, again, we likely want
    to preserve scale and scaled features. A yearly seasonal oscillation should not
    be interpreted in the same way or “triggered” by the same feature selector as
    a daily oscillation, although in some contexts this might be helpful.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多种原因，传统卷积不太适合时间序列。卷积的一个主要特点是所有空间被等同对待。这对图像来说是合理的，但对于时间序列来说却不是，因为某些时间点比其他时间点更接近。卷积网络通常被设计为尺度不变，以便在图像中识别马的时候，无论它占据图像的较大部分还是较小部分都能够被识别出来。然而，在时间序列中，我们可能希望保留尺度和缩放特征。例如，年度季节性振荡不应以与日常振荡相同的方式或相同的特征选择器进行解释，尽管在某些情况下这可能有所帮助。
- en: It is this double-edge nature of convolutions—that their strengths are their
    weaknesses from a time series perspective—that leads to them being used most often
    as a component of a network for time series analysis rather than the entire network.
    Also, it has led to their being studied more often for purposes of classification
    than forecasting, though, both uses are found in the field.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 正是卷积的这种双刃剑特性——它们的优势从时间序列的角度来看也是它们的弱点——这导致它们通常被用作时间序列分析网络的组成部分，而不是整个网络的主体。此外，这也导致它们更多地被研究用于分类而不是预测，尽管这两种用途在该领域中都有发现。
- en: 'Some uses of convolutional networks for time series applications include:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络在时间序列应用中的一些用途包括：
- en: Establishing a “fingerprint” for an internet user’s browsing history, which
    helps detect anomalous browsing activity
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个互联网用户浏览历史的“指纹”，有助于检测异常浏览活动
- en: Identifying anomalous heartbeat patterns from EKG data
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从心电图数据中识别异常心跳模式
- en: Generating traffic predictions based on past recordings from multiple locations
    in a large city
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于大城市多个位置的过去记录生成交通预测
- en: Convolutions are not all that interesting per se to apply to a univariate time
    series. Multichannel time series can be more interesting because then we can develop
    a 2D (or even 3D) image where time is only one axis.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积本身并不适用于单变量时间序列，因此并不是所有情况下都十分有趣。多通道时间序列可能更有趣，因为我们可以开发一个二维（甚至三维）图像，其中时间仅是一个轴。
- en: There are other architectural innovations worth mentioning, as we will discuss
    two examples of convolutions for time series in the following section.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他值得一提的架构创新，我们将在下一节讨论两个时间序列的卷积示例。
- en: A Simple Convolutional Model
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的卷积模型
- en: 'We can fit a convolutional model to our data rather than a fully connected
    one by swapping in a different model. In this case (and for the rest of the examples),
    we take the data in NTC format, which, as a reminder, looks like this:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将卷积模型拟合到我们的数据中，而不是完全连接的模型，通过替换不同的模型。在这种情况下（以及其余的示例中），我们采用 NTC 格式的数据，作为提醒，它看起来像这样：
- en: '| Time | A | B | C |'
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_TB
  zh: '| 时间 | A | B | C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *t* – 1 | 0, 3 | –1, –2 | –2, –3 |'
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_TB
  zh: '| *t* – 1 | 0, 3 | –1, –2 | –2, –3 |'
- en: '| *t* | 3, 4 | –2, –2 | –3, –4 |'
  id: totrans-274
  prefs:
  - PREF_BQ
  type: TYPE_TB
  zh: '| *t* | 3, 4 | –2, –2 | –3, –4 |'
- en: 'This is N × T × C. However, the data expected by a convolutional layer is `batch_size`,
    `channel`, `height` × `width`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 N × T × C。然而，卷积层期望的数据是 `batch_size`, `channel`, `height` × `width`：
- en: '[PRE32]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice that, again, this does not include any explicit temporal awareness. What
    is different is that now time is laid out along a single axis, giving it some
    ordering.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，这并不包括任何明确的时间意识。不同的是，现在时间沿着单一轴排列，因此具有某种顺序。
- en: 'Does this temporal awareness improve performance? Possibly not:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这种时间意识能提高性能吗？可能并不是：
- en: '[PRE33]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It can be difficult to determine why one model doesn’t improve on another. In
    fact, often even a model’s creator can be wrong about why the model works so well.
    There are few analytic proofs, and given how much can depend on the contours of
    a data set, this can also add confusion. Does the CNN not doing any better reflect
    the fact that most of the important information is in the nearest points in time?
    Or does it reflect a difference in the number of parameters? Or perhaps it reflects
    a failure to choose good hyperparameters. It would be important in a real use
    case to understand whether the performance seemed reasonable given the structure
    of the model, the structure of the data, and the overall number of parameters
    available.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 确定为何一个模型没有超越另一个模型可能是困难的。事实上，甚至模型的创建者对模型为何如此有效可能也会错误。这里几乎没有分析证明，考虑到数据集的轮廓有多重要，这也可能导致混淆。CNN
    没有表现更好是否反映了大部分重要信息在最近的时间点上？还是反映了参数数量的不同？或者可能反映了未能选择好的超参数。在实际应用中，重要的是要理解性能是否合理，考虑到模型的结构、数据的结构以及可用的总参数数量。
- en: 'We also see in the code a fault with our early stopping logic. It appears that
    it was too lenient. In this case, I revisited the issue and noticed that changes
    in correlation could look like the following over a series of epochs:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在代码中看到我们早停逻辑的一个缺陷。看起来它过于宽松。在这种情况下，我重新审视了这个问题，并注意到相关性的变化可能如下所示，经过一系列的 epochs：
- en: '[PRE34]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This means that the change in correlation could be terrible—even negative—many
    times, so long as there was improvement, even a small one, every once in a while.
    This leniency turns out to be a bad decision, so it would be a good idea to backtrack
    to our pipeline and have a more stringent early stopping condition. This is an
    example of the give-and-take you will find as you fit deep learning models. It
    helps to do many trial runs as you build your pipeline to get an idea of what
    parameters work for your data set and models of interest—bearing in mind that
    these will vary substantially from one data set to the next.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着相关性的变化可能非常糟糕，甚至负面——只要偶尔有改善，即使是微小的改善，也会如此。这种宽松度证明是一个糟糕的决定，因此最好回溯到我们的流程管道，并设置更严格的早停条件。这是你在拟合深度学习模型时会遇到的一种让步和取舍。在建立管道时进行多次试运行有助于了解哪些参数适用于你的数据集和感兴趣的模型，要牢记，这些参数在不同的数据集中会有很大的差异。
- en: Alternative Convolutional Models
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代卷积模型
- en: The simple convolutional model we just saw did surprisingly well even though
    it included no modifications to be time aware. Now we discuss two approaches from
    research and industry to use convolutional architectures in time series problems.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单的卷积模型没有进行任何时间感知性的修改，但我们刚刚看到它表现出乎意料地好。现在我们讨论研究和工业中使用卷积架构解决时间序列问题的两种方法。
- en: Why is this attractive? There are a number of reasons. First, convolutional
    architectures are tried-and-true methods, with a number of best practices that
    are well known by practitioners. This makes convolutional models appealing because
    they are a known quantity. Additionally, convolutional models have few parameters
    since the same filters are repeated over and over, meaning there are not too many
    weights to train. And, finally, large portions of convolutional models can be
    computed in parallel, meaning these can be very fast for purposes of inference.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这么吸引人呢？有几个原因。首先，卷积架构是经过验证的方法，实践者都熟知其中的最佳实践。这使得卷积模型很有吸引力，因为它们是已知的量。此外，卷积模型的参数很少，因为相同的滤波器一遍又一遍地重复使用，意味着没有太多需要训练的权重。最后，卷积模型的大部分计算可以并行进行，这意味着它们在推理过程中非常快速。
- en: Causal convolutions
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果卷积
- en: 'Causal convolutions are best understood with an image, as this can intuitively
    express how convolutions are modified to produce causality and a sense of time.
    [Figure 10-9](#fig-1009) shows an example of a *dilated causal convolution*. The
    causality part refers to the fact that only points earlier in time go into any
    given convolutional filter. That is why the image is not symmetric: earlier points
    flow into the convolutions used at later times, but not vice versa.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 因果卷积最好通过图像来理解，因为这可以直观地表达卷积如何被修改以产生因果性和时间感。[图 10-9](#fig-1009) 展示了一个*扩张因果卷积*的示例。因果性部分指的是只有早于当前时间的点可以输入到任何给定的卷积滤波器中。这就是为什么图像不对称的原因：早期的点流入到稍后时间使用的卷积中，但反之则不成立。
- en: The dilation part refers to the fact that points are skipped in the arrangement
    of the convolutional filters, such that any given point goes only into one convolutional
    filter in each level of layering. This promotes model sparsity and reduces redundant
    or overlapping convolutions, allowing the model to look further back in time while
    keeping overall computations reasonably contained.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 扩张部分指的是在卷积滤波器的排列中跳过点，使得任何给定点仅进入每个层次中的一个卷积滤波器。这促进了模型的稀疏性，并减少了冗余或重叠的卷积，使模型可以在保持总体计算合理的情况下更深入地观察过去的时间。
- en: '![](assets/ptsa_1009.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1009.png)'
- en: Figure 10-9\. This graph depicts an important component of [WaveNet](https://perma.cc/Z4KZ-ZXBQ)’s
    architecture. Here we see how convolutional neural networks have architectural
    modifications appropriate to time series.
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 此图描述了[WaveNet](https://perma.cc/Z4KZ-ZXBQ)架构的一个重要组成部分。在这里，我们看到卷积神经网络如何针对时间序列进行了架构修改。
- en: This example of dilated causal convolution introduces the notion of temporal
    causality by permitting only data from prior time points. That is, the convolutions
    in this image are not equal opportunity; they allow data to flow only from the
    past to the future and not the other way around. Every data point in the original
    input does have an impact on the final point. Note what dilation means here, which
    is that increasingly “deep” layers in the convolution skip between increasing
    numbers of data points in previous layers. The way the dilation here was set up,
    every data point in the original input is included in the final input, but only
    once. This is not required for dilation but just what was used in this case. Dilation
    could also be used to skip time points altogether.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这个扩张因果卷积的示例引入了时间因果性的概念，通过允许仅来自之前时间点的数据。也就是说，在这个图像中的卷积并非机会均等；它们只允许数据从过去流向未来，而不是反过来。原始输入中的每个数据点确实对最终的点产生影响。请注意这里的扩张是什么意思，即卷积的“深度”层之间跳过越来越多的数据点。在这里设置的扩张方式是，原始输入中的每个数据点都包含在最终的输入中，但只出现一次。这并不是扩张的必需条件，只是在这种情况下使用的方式。扩张也可以用于跳过时间点。
- en: While causal convolutions sound complicated and theoretical, they are surprisingly
    easy to perform. Simply add padding onto the left side of the matrix—that is,
    add earlier time step stand-ins that are zero, so that they won’t contribute to
    the value—and then set padding to “valid” so that the convolution will only run
    against the actual boundaries of the matrix without including imaginary empty
    cells. Given that convolutions work as the sum of element-wise products, adding
    the zeros to the left side of [Figure 10-9](#fig-1009) would mean that we could
    run a standard convolution and that the extra zero cells would not change the
    final outcome.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然因果卷积听起来复杂和理论化，但实际上执行起来非常简单。只需在矩阵的左侧添加填充——即添加早期时间步骤的零占位符，这样它们就不会对值产生贡献——然后设置填充为“valid”，这样卷积将只针对矩阵的实际边界运行，而不包括虚构的空单元。鉴于卷积的工作方式是元素逐元素乘积的总和，将零添加到[图 10-9](#fig-1009)的左侧意味着我们可以运行标准卷积，而额外的零单元格不会改变最终结果。
- en: Causal convolutions have had great success in the wild, most particularly as
    part of the model used for Google’s text-to-speech and speech-recognition technologies.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 因果卷积在实际应用中取得了巨大成功，尤其是作为谷歌文本转语音和语音识别技术模型的一部分。
- en: Converting a time series into pictures
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将时间序列转换为图片
- en: 'Convolutional models are known to work very well on image analysis, so a good
    idea in trying to make them relevant to time series is to find a way to turn a
    time series into a picture. There are many ways to do this, one of which is interesting
    because it can turn even a univariate time series into a picture: constructing
    a recurrence plot (see [Figure 10-10](#fig-1010)).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积模型在图像分析上表现非常好，因此在试图使它们与时间序列相关时的一个好主意是找到一种将时间序列转换为图片的方法。有许多方法可以做到这一点，其中一个方法很有趣，因为它可以将单变量时间序列甚至转换为图片：构建循环图（参见[图 10-10](#fig-1010)）。
- en: '![](assets/ptsa_1010.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1010.png)'
- en: 'Figure 10-10\. A beautiful and insightful visualization of four kinds of time
    series, from left to right: (1) white noise, (2) harmonic/seasonal series with
    two frequencies, (3) chaotic data with a trend, and (4) an autoregressive process.
    Source: [Wikipedia](https://perma.cc/4BV2-57T4), provided by Norbert Marwan in
    2006.'
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 从左到右展示了四种类型时间序列的美观而深刻的可视化：（1）白噪声，（2）具有两个频率的谐波/季节性系列，（3）带有趋势的混沌数据，以及（4）自回归过程。来源：[维基百科](https://perma.cc/4BV2-57T4)，由诺伯特·马尔万（Norbert
    Marwan）于2006年提供。
- en: A recurrence plot is a way of describing, in phase-state space, when a time
    series revisits approximately the same phase and state it was in at an earlier
    moment in time. This is defined via the binary recurrence function. Recurrence
    is defined as *R*(*i*, *j*) = 1 if *f*(*i*) – *f*(*j*) is sufficiently small;
    0 otherwise. This results in a binary black-and-white image, such as the ones
    we see depicted in [Figure 10-10](#fig-1010). Notice that *i* and *j* refer to
    the time values, and the time axis is not bounded or in any way restricted.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 循环图是一种在相位状态空间中描述时间序列何时大致重访先前时间点相同相位和状态的方法。这是通过二进制循环函数定义的。如果*f*(*i*) - *f*(*j*)足够小，则循环被定义为*R*(*i*,
    *j*) = 1；否则为0。这导致了一个二进制的黑白图像，就像我们在[图 10-10](#fig-1010)中看到的那些。请注意，*i*和*j*是时间值，时间轴没有被限制或以任何方式限制。
- en: While it is relatively easy to code up your own recurrence plot, recurrence
    plot functionality is also available in packages such as [`pyts`](https://perma.cc/4K5X-VYQR),
    and the [source code](https://perma.cc/VS2Z-EJ8J) for the plotting is easy to
    find and understand.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管编写自己的循环图相对容易，但循环图功能也包含在诸如[`pyts`](https://perma.cc/4K5X-VYQR)之类的软件包中，并且绘图的[源代码](https://perma.cc/VS2Z-EJ8J)易于查找和理解。
- en: We can imagine extending this idea beyond a univariate time series by treating
    different variables of a multivariate time series as different “channels” of an
    image. This is just one example of how deep learning architectures and techniques
    are quite versatile and open to variation.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象，通过将多变量时间序列的不同变量视为图像的不同“通道”，将这个想法推广到单变量时间序列之外。这只是深度学习架构和技术非常灵活和开放变化的一个例子。
- en: RNNs
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNNs
- en: 'Recurrent neural networks (RNNs) are a broad class of networks wherein the
    same parameters are applied again and again even as the inputs change as time
    passes. This sounds much like the feed forward neural network we covered previously,
    and yet RNNs make up—in whole or in part—many of the most successful models in
    academia and industry for sequence-based tasks, language, forecasting, and time
    series classification. The important differences between an RNN and a feed forward
    network are:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（RNNs）是一类广泛的网络，在输入随时间变化的同时应用相同的参数。这听起来很像我们之前介绍的前馈神经网络，然而 RNNs 构成学术界和工业界在基于序列的任务、语言、预测和时间序列分类中最成功的模型，全面或部分如此。RNN
    和前馈网络之间的重要区别是：
- en: An RNN sees time steps one at a time, in order.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 逐个时间步地看到时间步骤。
- en: An RNN has state that it preserves from one time step to another, and it is
    this state as well as its static parameters that determine its response updates
    to each new piece of information at each step.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 保留了从一个时间步到另一个时间步的状态，正是这种状态以及其静态参数决定了它对每个步骤的每个新信息的响应更新。
- en: An RNN thus has parameters that help it “update” its state, including a hidden
    state, from time step to time step.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，RNN 具有帮助它“更新”其状态（包括隐藏状态）的参数，从一个时间步到另一个时间步。
- en: Often when RNNs are introduced, they are presented via the paradigm of “unrolling”
    because a recurrent architecture is cell-based. This describes the way the same
    parameters are used again and again, so that the number of parameters is fairly
    small even for very long time sequences (see [Figure 10-11](#fig-1011)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍递归神经网络时，通常会通过“展开”范式来呈现，因为递归架构是基于单元的。这描述了相同参数一次又一次地使用，以便即使是非常长的时间序列，参数数量也相对较少（见[图 10-11](#fig-1011)）。
- en: '![](assets/ptsa_1011.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1011.png)'
- en: Figure 10-11\. How a recurrent neural network architecture is unrolled once
    for each time step when it is applied to data.
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. 当递归神经网络架构应用于数据时，如何每个时间步展开一次。
- en: 'The easiest way to understand how an RNN works, however, may just be to see
    an example. My particular favorite RNN cell is the Gated Recurrent Unit (GRU).
    Sometimes it can be more intimidating to see the expressions in mathematical form
    than in code, so here I provide an implementation of a GRU in Python with the
    help of NumPy. As you can see, there are two activation functions used: sigmoid
    and tanh. Other than this, all we do is perform matrix multiplication and addition
    as well as element-wise matrix multiplication (the *Hadamard product*):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，理解 RNN 如何工作的最简单方法可能只是看一个例子。我特别喜欢的 RNN 单元是门控循环单元（GRU）。有时，看到数学形式的表达式可能比看到代码更具威胁性，因此在此我提供了一个在
    Python 中使用 NumPy 实现 GRU 的示例。如你所见，这里使用了两个激活函数：sigmoid 和 tanh。除此之外，我们所做的就是进行矩阵乘法和加法，以及逐元素矩阵乘法（*Hadamard
    乘积*）。
- en: '[PRE35]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A GRU is currently one of the most widely used RNN cells. It is a simpler version
    of the Long Short-Term Memory (LSTM) cell, which operates in a similar way. The
    differences between a GRU and an LSTM are as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 目前是最广泛使用的 RNN 单元之一。它是长短期记忆（LSTM）单元的简化版本，其操作方式类似。GRU 和 LSTM 之间的区别如下：
- en: A GRU has two “gates,” while an LSTM has three. These gates are used to determine
    how much new information is allowed in, how much old information is preserved,
    and so on. Because an LSTM has more gates, it has more parameters.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU 有两个“门”，而 LSTM 有三个。这些门用于确定允许多少新信息进入，保留多少旧信息等。因为 LSTM 有更多的门，所以它有更多的参数。
- en: An LSTM tends have better performance, but a GRU is faster to train (due to
    the number of parameters). However, there are published results where a GRU outperforms
    an LSTM. A GRU is particularly likely to outperform an LSTM for nonlanguage tasks.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 往往具有更好的性能，但 GRU 训练速度更快（由于参数数量）。然而，有出版的结果显示，GRU 优于 LSTM。GRU 尤其可能在非语言任务中优于
    LSTM。
- en: As you can see, the difference is more a matter of the degree of complexity
    appropriate for your training resources and of what you are trying to understand
    and predict.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，区别更多地取决于适合你的培训资源的复杂程度以及你试图理解和预测的内容。
- en: It is important to be familiar with the matrix implementation of the GRU and
    LSTM so that you get a sense of how they work. Once you do, you may also develop
    an intuition when they don’t train on a particular data set as to why they don’t.
    There may be something about the dynamics that is not easily recognized by the
    format shown here.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉GRU和LSTM的矩阵实现非常重要，这样你就能了解它们的工作原理。一旦你了解了这一点，当它们在特定数据集上无法训练时，你也可以发展出一些直觉，知道为什么会这样。这里显示的格式可能难以识别动态特性的某些内容。
- en: Note that both GRUs and LSTMs help solve the problem that was first encountered
    when RNNs were used, namely exploding and vanishing gradients. Because of the
    recurrent application of the same parameters, it was often the case that gradients
    would quickly go to zero (not helpful) or to infinity (also not helpful), meaning
    that backpropagation was difficult or even impossible as the recurrent network
    was unrolled. This problem was addressed with GRU and LSTM because these tend
    to keep inputs and outputs from the cell in tractable value ranges. This is due
    both to the form of the activation function they use and to the way that the update
    gate can learn to pass information through or not, leading to reasonable gradient
    values being much more likely than in a vanilla RNN cell, which has no notion
    of a gate.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，GRU和LSTM都有助于解决当使用RNN时首次遇到的问题，即梯度爆炸和消失。由于同一参数的递归应用，经常出现梯度迅速降至零（无帮助）或无穷大（同样无帮助）的情况，这意味着反向传播难以进行，甚至不可能进行，因为递归网络已展开。这个问题通过GRU和LSTM得到了解决，因为它们倾向于保持单元中的输入和输出在可控的值范围内。这既是由于它们使用的激活函数的形式，也是由于更新门可以学习如何传递或不传递信息，从而使合理的梯度值比在没有门控概念的香草RNN单元中更有可能。
- en: While both GRUs and LSTMs are reasonably easy to DIY, as just demonstrated,
    in reality you would not want to do this. The most important reason is that many
    of the matrix multiplication operations can be fused. The most efficient and accessible
    implementation for taking care of this and exploiting the hardware is NVIDIA’s
    cuDNN, which fuses the matrix multiplication operations needed for both GRU and
    LSTM cells. Using the cuDNN interface rather than another implementation is substantially
    faster, and has even been cited by some Kaggle contest winners as the difference
    between winning and not even coming close because it helps so much in speeding
    up training. All the main deep learning frameworks offer access to this implementation,
    although in some cases (such as TensorFlow’s `tf.contrib.cudnn_rnn`), you need
    to use the specially designated interface. In other cases, such as MXNet, you
    will default to using cuDNN so long as you don’t do anything fancy with custom
    unrolled cells.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GRU和LSTM都相对容易自行实现，正如刚刚展示的那样，在现实中你不会希望这样做。最重要的原因是，许多矩阵乘法操作可以被融合。用于处理这些操作并利用硬件的最有效和可访问的实现是NVIDIA的cuDNN，它融合了GRU和LSTM单元所需的矩阵乘法操作。使用cuDNN接口而不是另一种实现显著提高了速度，甚至被一些Kaggle比赛的获胜者引用为赢与输的区别，因为它在加速训练过程中非常有帮助。所有主要的深度学习框架都提供对此实现的访问，尽管在某些情况下（例如TensorFlow的`tf.contrib.cudnn_rnn`），你需要使用特别指定的接口。在其他情况下，如MXNet，你会默认使用cuDNN，只要不对自定义展开的单元做任何花哨的处理。
- en: Continuing Our Electric Example
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续我们的电力示例
- en: 'We can work through the electric forecasting example with an RNN as well. Again,
    we begin with the TNC data format as an input. This is the expected format for
    an RNN, so we don’t even need to change it:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用RNN来处理电力预测示例。同样地，我们从TNC数据格式作为输入开始。这是RNN的预期格式，因此我们甚至不需要对其进行更改：
- en: '[PRE36]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The performance of this model is rather disappointing given that it is designed
    to handle temporal data:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于设计用于处理时间数据，这个模型的性能相当令人失望：
- en: '[PRE37]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: It would be much more investigation to see why the performance is underwhelming.
    Did we not provide enough parameters to the RNN? Would it make sense to add some
    related model architectures that are commonly used, such as attention? (This applies
    to RNNs as well as feed forward networks, as was discussed earlier.)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 研究性能为何不佳需要更多调查。我们是否没有向RNN提供足够的参数？增加一些常用的相关模型架构，比如注意力，是否有意义？（这同样适用于RNN和前馈网络，正如之前讨论的那样。）
- en: The model reached its peak performance quite a bit earlier in the training process
    than did the feed forward or convolutional models. This is a lack of sufficient
    parameters to describe the data set, or that an RNN is specially tailored to this
    data or something else. You would need to do additional experiments and tuning
    to get a sense of the reason.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在训练过程中比前馈或卷积模型早达到了其性能峰值。这是描述数据集的参数不足，或者RNN专门为这些数据设计，或者其他原因。你需要进行额外的实验和调整，以了解原因。
- en: The Autoencoder Innovation
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器的创新
- en: Occasionally you may happen upon a data set where a very simple model already
    does spectacular work for the mission you hope to accomplish.^([4](ch10.html#idm45576022793272))
    But sometimes thinking outside the box can prove fruitful as well. For example,
    early on it was discovered that a simple innovation with an RNN could also substantially
    improve performance in sequence-like modeling. While this model was originally
    developed for language learning and machine translation, it has often proven quite
    successful at more numeric tasks, such as predicting electric load forecasting
    or stock prices. Known as the *autoencoder*, or alternately as the *seq2seq* model,
    this is a very commonly used model and one you should make a go-to part of your
    time series deep learning toolkit (see [Figure 10-12](#fig-1012)). We will deploy
    it in several of the example chapters later in the book, where we analyze real-world
    time series data.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔你可能会遇到一个数据集，一个非常简单的模型已经为你希望实现的任务做了惊人的工作。^([4](ch10.html#idm45576022793272))
    但有时候打破常规也可能会带来丰硕的成果。例如，早期发现，简单的RNN创新在类似序列建模中也能显著提高性能。虽然这个模型最初是为语言学习和机器翻译开发的，但在更多数值任务，如预测电力负荷预测或股票价格方面，它通常也被证明非常成功。被称为*自编码器*，或者*seq2seq*模型，这是一个非常常用的模型，你应该将其作为时间序列深度学习工具包的核心部分（见[Figure 10-12](#fig-1012)）。我们将在本书的几个示例章节中部署它，分析真实世界的时间序列数据。
- en: The autoencoder is two recurrent layers, but not in the traditional sense where
    each layer processes each input successively. Rather, the first layer runs to
    completion. Its hidden state is then passed onto the second layer, which takes
    this hidden state and its own outputs as new inputs for the next step. This model
    was particularly formulated with the idea of machine translation, whereby outputting
    a prediction at each time step was not meaningful given that in different languages,
    word and concept ordering can be drastically different and say the same thing.
    The idea was that once the first layer had fully processed the time series, its
    hidden state could come to have a kind of summarizing functionality. This summary
    would then be injected into the new model, which would gradually unroll the meaning
    into the new language by combining this summary with its own output at each time
    step so that it would know what it had said.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是两个递归层，但不是传统意义上的每个层依次处理每个输入。相反，第一层运行完成后。它的隐藏状态然后传递到第二层，第二层将这个隐藏状态和自己的输出作为新的输入传递到下一个步骤。这个模型特别是以机器翻译的概念来构建的，其中在每个时间步输出预测并不重要，因为在不同的语言中，单词和概念的排序可能截然不同，但说的是同样的事情。这个想法是一旦第一层完全处理了时间序列，它的隐藏状态就能具有某种总结功能。然后这个摘要将被注入到新模型中，新模型将通过将这个摘要与自己的每个时间步的输出结合起来逐渐展开到新的语言中，以便它知道它说了什么。
- en: As mentioned, despite the natural language processing origins of this model,
    it is also useful for more traditional time series tasks, such as predicting univariate
    or multivariate time series. For example, in a Kaggle competition to predict web
    traffic in Wikipedia posts, the first-place winner, after trying many hyperparameters
    and architectural components, eventually settled on an autoencoder model. It seems
    that the winner’s advantage was likely in smart training and hyperparameter search
    as well as a good study of useful feature generation and selection.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，尽管这个模型起源于自然语言处理，但它也对传统的时间序列任务非常有用，比如预测单变量或多变量时间序列。例如，在一个Kaggle竞赛中，预测维基百科文章中的网络流量，第一名获得者在尝试了许多超参数和架构组件后，最终选择了自编码器模型。看来，获胜者的优势可能主要在于智能训练和超参数搜索，以及对有用特征生成和选择的深入研究。
- en: '![](assets/ptsa_1012.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1012.png)'
- en: Figure 10-12\. An autoencoder, also often referred to as a seq2seq model, is
    very popular for language processing and modeling but has also shown significant
    success in time series analysis.
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12\. 自编码器，通常称为seq2seq模型，在语言处理和建模中非常流行，但在时间序列分析中也显示出显著的成功。
- en: Combination Architectures
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合架构
- en: Very often, successful industry and contest applications of deep learning for
    time series forecasting will use some kind of novel architecture, be it a new
    spin on applying traditional LSTM cells or combining different components. One
    example of this is a 2018 neural network architecture proposed by researchers
    at Carnegie Mellon University. The researchers looked to exploit the strengths
    of both convolutional and recurrent architectures, but they added other innovations
    as well.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列预测的成功工业和竞赛应用中，很多时候会使用一些新颖的架构，无论是在应用传统LSTM单元上的新颖方法，还是结合不同的组件。卡内基梅隆大学研究人员提出的2018年神经网络架构就是一个例子。研究人员试图利用卷积和递归架构的优势，同时还添加了其他创新。
- en: They developed a “skip recurrent” layer whereby the recurrent model could be
    tuned to pay attention to periodicities (such as yearly, weekly, daily, depending
    on the nature of the data set) present in the data (and this periodicity could
    itself be explored as a hyperparameter).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 他们开发了一个“跳跃递归”层，使得递归模型可以调整以关注数据集中存在的周期性（如年度、每周、每日，具体取决于数据集的性质），这种周期性本身可以作为超参数进行探索。
- en: They recognized that many time series have trends that are not well modeled
    by nonlinear deep learning models. This meant that a deep learning model alone
    would not show the substantial scale changes over time that some time series data
    sets demonstrate. The researchers adapted by using a traditional linear model,
    namely the autoregressive model we discussed in [Chapter 6](ch06.html#statistical_model_for_time_series).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 他们认识到许多时间序列的趋势不适合非线性深度学习模型建模。这意味着仅仅使用深度学习模型无法展示某些时间序列数据集在时间上显著的规模变化。研究人员通过使用传统的线性模型进行了调整，即我们在[第6章](ch06.html#statistical_model_for_time_series)中讨论的自回归模型。
- en: The final model, the *modified LSTNet*, was a sum of the outputs from the AR
    model and a model built with a traditional recurrent layer and a skip recurrent
    layer in parallel. The inputs fed into each recurrent layer were the outputs of
    convolutional layers that convolved both along the time and channel axis (see
    [Figure 10-13](#fig-1013)).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的模型，*修改后的LSTNet*，是来自AR模型和一个使用传统递归层和平行跳跃递归层构建的模型输出之和。输入到每个递归层的是卷积层的输出，这些卷积层沿着时间和通道轴进行卷积（参见[图10-13](#fig-1013)）。
- en: The researchers found that in three out of the four data sets they tried, they
    outperformed state-of-the-art published results spanning a broad range of topic
    areas. They failed only in the case of foreign exchange rates, a domain of finance
    that is notoriously difficult to predict, with high noise-to-signal ratios and
    a highly efficient market whereby any signal quickly dissipates as the market
    becomes efficient and investors try to find an edge.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，在他们尝试的四个数据集中，有三个数据集的表现优于涵盖广泛主题领域的最新发表结果。仅在外汇汇率这一领域失败，这是一个以高噪声信号比和高效市场著称的金融领域，在市场变得高效并且投资者试图寻找优势时，任何信号都很快消失。
- en: '![](assets/ptsa_1013.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1013.png)'
- en: Figure 10-13\. In the modified LSTNet architecture, we can see there is an autoregressive
    component (bottom of image) in parallel to a neural network architecture. The
    neural network architecture puts a convolutional element and a recurrent element
    in consecutive order, operating on the same inputs one after the other.
  id: totrans-339
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13\. 在修改后的LSTNet架构中，我们可以看到图像底部存在一个自回归组件，与神经网络架构并行。神经网络架构将卷积元素和递归元素按顺序放置在同一个输入上，一个接一个地操作。
- en: The inspiration for the next elaboration comes from the researchers’ paper,
    and we will be working with [code](https://perma.cc/3W4Y-E8E2)^([5](ch10.html#idm45576022763176))
    modified from the MXNet package catalog of examples, described in great detail
    by the example’s author, Oliver Pringle, in a [blog post](https://perma.cc/9KM2-RNPK).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个详细阐述的灵感来自研究人员的论文，我们将使用从MXNet包示例目录中修改的[代码](https://perma.cc/3W4Y-E8E2)^([5](ch10.html#idm45576022763176))，由示例的作者Oliver
    Pringle在一篇[博客文章](https://perma.cc/9KM2-RNPK)中进行了详细描述。
- en: 'Here, as noted, we apply code based on modifications to the MXNet repository,
    simplifying it by removing the seasonal/skip connections and also by using only
    one convolutional filter size. We apply a convolutional layer, just as we did
    in the `cnn_model` example earlier:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前文所述，我们应用基于对MXNet存储库的修改的代码，通过删除季节性/跳跃连接，并仅使用一个卷积滤波器大小来简化它。我们应用一个卷积层，就像我们在`cnn_model`示例中做的那样：
- en: '[PRE38]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We then apply a RNN not to the original inputs but to the convolutional component,
    as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将RNN应用于卷积组件而不是原始输入，如下所示：
- en: '[PRE39]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, in parallel to this CNN/RNN combination, we train an AR model, with
    one AR model per electric station location (so 321 distinct AR models, one per
    column/variable/electric site), as shown here. This uses every time point for
    every station, with the model being specified one station at a time:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与CNN/RNN结合并行，我们训练了一个AR模型，每个电站位置一个AR模型（因此有321个不同的AR模型，每个对应一个列/变量/电站），如下所示。这使用了每个站点的每个时间点，模型逐个站点指定：
- en: '[PRE40]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The full code is as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码如下：
- en: '[PRE41]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Notice that the performance is strikingly better in this model than any other:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个模型的性能明显比其他任何模型都要好：
- en: '[PRE42]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This model obviously does quite a bit better than the other models, so there
    is something special and helpful about this architecture that uses convolutional
    images as the sequential data on which the recurrent layer trains. The traditional
    statistical tool of an AR model also adds quite a bit of functionality.^([6](ch10.html#idm45576022241128))
    This model is far and away the best one, and this is a good lesson for the small
    amount of data exploration and training we did. It’s worth trying a variety of
    models, and you don’t even need to spend a lot of time training all models to
    be able to discover a clear leader.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型显然比其他模型做得更好，所以这个架构在使用卷积图像作为循环层训练的序列数据上有一些特别和有用的东西。传统的AR模型作为一个统计工具也增加了相当多的功能。^([6](ch10.html#idm45576022241128))
    这个模型远远是最好的，这是我们进行少量数据探索和培训的一个很好的教训。值得尝试各种模型，你甚至不需要花费大量时间来训练所有的模型以发现一个明确的领导者。
- en: Summing Up
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'One interesting aspect of our training in this chapter is that the models’
    performances did not turn out the way we might have expected. The simple feed
    forward network substantially outperformed some models that seem more conceptually
    complicated. However, this doesn’t necessarily settle the question of which models
    are better or worse for our data set for a number of reasons:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章节训练中一个有趣的方面是，一些看似概念上更复杂的模型表现不如简单的前馈网络。然而，这并不一定能解决对于我们数据集来说哪些模型更好或更差的问题，原因有很多：
- en: We did not check the number of parameters used in each model. It’s possible
    that different genres of model would have drastically different performance with
    different numbers of parameters. We could play with model complexity, such as
    number of convolutional/recurrent layers or number of filters/hidden units.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有检查每个模型使用的参数数量。不同类型的模型可能会因参数数量不同而表现出截然不同的性能。我们可以玩转模型复杂性，例如卷积/递归层数或过滤器/隐藏单元的数量。
- en: We did not tune hyperparameters. Sometimes getting the right hyperparameters
    can make an enormous difference in the performance of a model.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有调整超参数。有时候，获取正确的超参数可以极大地提升模型的性能。
- en: We did not explore our data enough to have a prior idea of what model we would
    expect to do better or worse given the correlations over time and between different
    columns/electric sites.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有对数据进行足够的探索，以便有一个预先的想法，即在时间和不同列/电站之间的相关性中，我们期望哪个模型表现更好或更差。
- en: More Resources
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多资源
- en: 'Historical documents:'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史文献：
- en: Sepp Hochreiter and Jürgen Schmidhuber, [“Long Short-Term Memory,”](https://perma.cc/AHR3-FU5H)
    *Neural Computation* 9, no. 8 (1997):1735–80, https://perma.cc/AHR3-FU5H.
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sepp Hochreiter和Jürgen Schmidhuber，《长短期记忆》，《神经计算》9，第8卷（1997年）：1735–80，https://perma.cc/AHR3-FU5H。
- en: This 1997 seminal work introduced the Long Short-Term Memory cell (LSTM) and
    also proposed several experimental benchmarks that remain in use today for studying
    neural network performance on sequence analysis.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这部1997年的开创性作品介绍了长短期记忆单元（LSTM），并提出了几个至今用于研究神经网络在序列分析上性能的实验基准。
- en: 'Peter G. Zhang, Eddy Patuwo, and Michael Hu, [“Forecasting with Artificial
    Neural Networks: The State of the Art,”](https://perma.cc/Z32G-4ZQ3) *International
    Journal of Forecasting* 14, no. 1 (1998): 35–62, https://perma.cc/Z32G-4ZQ3.'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Peter G. Zhang, Eddy Patuwo, 和 Michael Hu, [“用人工神经网络进行预测: 现状综述,”](https://perma.cc/Z32G-4ZQ3)
    *International Journal of Forecasting* 14, no. 1 (1998): 35–62，https://perma.cc/Z32G-4ZQ3。'
- en: This article gives an overview of the state of the art for time series and deep
    learning in 1998.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文概述了1998年时间序列和深度学习的现状。
- en: 'On RNNs:'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于RNNs：
- en: 'Aurélien Geron, “Processing Sequences Using RNNs and CNNs,” in *Hands-On Machine
    Learning with Scikit-Learn, Keras, and TensorFlow*, 2nd Edition (Sebastopol: O’Reilly
    Media, Inc., 2019).'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Aurélien Geron, “使用RNNs和CNNs处理序列”，在*Hands-On Machine Learning with Scikit-Learn,
    Keras, and TensorFlow*，第二版（Sebastopol: O’Reilly Media, Inc., 2019）。'
- en: Aurélien Geron offers an exceptionally extensive set of examples on how to apply
    deep learning to time series data in this popular book. If you are comfortable
    with these tools, [this Jupyter notebook](https://perma.cc/D3UG-59SX) offers an
    excellent example of many different kinds of models applied to sequence data,
    including some exercises with solutions.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Aurélien Geron 在这本流行书中提供了大量关于如何将深度学习应用于时间序列数据的示例。如果你对这些工具很熟悉，[这个Jupyter笔记本](https://perma.cc/D3UG-59SX)
    展示了许多不同种类的模型在序列数据上的应用实例，包括一些带解决方案的练习。
- en: 'Valentin Flunkert, David Salinas and Jan Gasthaus, [“DeepAR: Probabilistic
    Forecasting with Autoregressive Recurrent Networks,”](https://perma.cc/MT7N-A2L6)
    unpublished paper, 2017, https://perma.cc/MT7N-A2L6.'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Valentin Flunkert, David Salinas 和 Jan Gasthaus, [“DeepAR: 自回归递归网络的概率预测,”](https://perma.cc/MT7N-A2L6)
    2017，https://perma.cc/MT7N-A2L6。'
- en: This groundbreaking paper illustrated Amazon’s model that was developed to fit
    time series for its retail data that occurs at a large variety of scales, and
    with trends. One particular innovation of the work was the ability to make probabilistic
    forecasts rather than the usual point estimates that tend to be the result of
    deep learning analysis.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本篇开创性的论文展示了亚马逊开发的适应其零售数据时间序列的模型，该数据出现在多种规模和趋势中。工作的一个特别创新之处在于能够进行概率预测，而不是通常由深度学习分析产生的点估计结果。
- en: Lingxue Zhu and Nikolay Laptev, [“Deep and Confident Prediction for Time Series
    at Uber,”](https://perma.cc/PV8R-PHV4) paper presented at the 2017 IEEE International
    Conference on Data Mining Workshops (ICDMW), New Orleans, LA, https://perma.cc/PV8R-PHV4.
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Lingxue Zhu 和 Nikolay Laptev, [“Uber时间序列的深度和自信预测,”](https://perma.cc/PV8R-PHV4)
    论文发表于2017年IEEE国际数据挖掘工作坊（ICDMW），新奥尔良，路易斯安那州，https://perma.cc/PV8R-PHV4。
- en: This paper illustrates another example of probability and statistically inspired
    modifications to typical RNNs. In this case, Uber proposed a novel Bayesian deep
    model that provided both a point estimate and an uncertainty estimation that could
    be deployed in production with reasonably fast performance.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文展示了另一个关于典型RNN进行概率和统计启发修改的例子。在这种情况下，Uber提出了一种新颖的贝叶斯深度模型，提供了点估计和不确定性估计，可以在生产环境中实现相对快速的性能。
- en: Zhengping Che et al., [“Recurrent Neural Networks for Multivariate Time Series
    with Missing Values,”](https://perma.cc/4YM4-SFNX) *Scientific Reports* 8, no.
    6085 (2018), https://perma.cc/4YM4-SFNX.
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Zhengping Che 等人，[“多变量时间序列中的递归神经网络和缺失值,”](https://perma.cc/4YM4-SFNX) *Scientific
    Reports* 8, no. 6085 (2018)，https://perma.cc/4YM4-SFNX。
- en: This paper provides a strong example of state-of-the-art work on medical time
    series. It demonstrates the use of a GRU, combined with novel architectures to
    account for missing data and make missingness into an informative attribute. The
    authors showcase neural networks that beat all existing clinical metrics currently
    deployed to make predictions about patient health and hospital stay statistics.
    This is a great example of how an intuitive and easy-to-understand modification
    to a simple and widely used RNN structure (the GRU) can lead to groundbreaking
    results on a good data set.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文展示了一项关于医疗时间序列的最新工作，展示了使用GRU结合新颖架构处理缺失数据并将缺失信息转化为信息属性的方法。作者展示了能够击败目前所有用于预测患者健康和住院统计的临床指标的神经网络。这是一个关于如何通过对简单且广泛使用的RNN结构（GRU）进行直观易懂的修改来取得突破性成果的绝佳例子。
- en: 'On CNNs:'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于CNNs：
- en: 'Aäron van den Oord and Sander Dieleman, [“WaveNet: A Generative Model for Raw
    Audio DeepMind,”](https://perma.cc/G37Y-WFCM) DeepMind blog, September 8, 2016,
    https://perma.cc/G37Y-WFCM.'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Aäron van den Oord 和 Sander Dieleman, [“WaveNet: 一种原始音频生成模型 DeepMind 博客,”](https://perma.cc/G37Y-WFCM)
    2016年9月8日，https://perma.cc/G37Y-WFCM。'
- en: This blog provides an extremely well done and accessible description of a groundbreaking
    CNN architecture that was used to enhance both text-to-speech and speech-to-text
    technologies in a variety of languages and with different speakers. The new architecture
    led to a significant boost in performance and was subsequently deployed on other
    sequence-related AI tasks, particularly with respect to time series forecasting.
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这篇博客提供了一篇极为详尽且易于理解的描述，介绍了一种突破性的CNN架构，该架构用于增强文本到语音和语音到文本技术，在多种语言和不同发言者中得到了应用。新架构显著提升了性能，并随后应用于其他与序列相关的AI任务，特别是时间序列预测。
- en: 'On the application of deep learning:'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习的应用：
- en: Vera Rimmer et al., [“Automated Website Fingerprinting Through Deep Learning,”](https://perma.cc/YR2G-UJUW)
    paper presented at NDSS 2018, San Diego, CA, https://perma.cc/YR2G-UJUW.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Vera Rimmer 等人，《通过深度学习实现自动网站指纹识别》，2018年NDSS会议论文，圣迭戈，CA，https://perma.cc/YR2G-UJUW。
- en: This paper illustrates a way in which deep learning can be used to uncover private
    information about a user’s internet browsing content throught Website Fingerprinting.
    In particular the authors highlighted a way in which various neural network architectures
    could be used to formulate Website Fingerprinting attacks to pierce user privacy
    protections.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文阐述了一种利用深度学习揭示用户互联网浏览内容隐私信息的方法，即通过网站指纹识别。特别是作者强调了各种神经网络架构可以用来制定网站指纹攻击，以突破用户隐私保护。
- en: CPMP, [“Second Place Solution to the Kaggle Web Traffic Forecasting Competition,”](https://perma.cc/UUR4-VNEU)
    Kaggle blog, 2017, https://perma.cc/UUR4-VNEU.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CPMP，《Kaggle网页流量预测竞赛第二名解决方案》，Kaggle博客，2017年，https://perma.cc/UUR4-VNEU。
- en: This blog, written before the conclusion of the competition, describes the second-place
    winner’s thinking in designing a mixed machine learning/deep learning solution
    to forecasting. There is also some retrospective commentary available in a [related
    blog post](https://perma.cc/73M3-D7DW). This is a great example of a mix of modern
    packages and a relevant can-do coding style. The GitHub repository for the first-place
    solution is also [available](https://perma.cc/K6RW-KA9E) alongside [a discussion
    of its neural network based architecture](https://perma.cc/G9DW-T8LE).
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这篇博客在比赛结束前写成，描述了第二名胜者在设计混合机器学习/深度学习解决方案时的思考。在[相关博客文章](https://perma.cc/73M3-D7DW)中还有一些回顾性评论。这是一个现代化包和相关编程风格的典范。第一名解决方案的GitHub存储库也可以在[此处](https://perma.cc/K6RW-KA9E)找到，还有一个关于其基于神经网络的架构的讨论。
- en: ^([1](ch10.html#idm45576029280392-marker)) The [file download](https://github.com/laiguokun/multivariate-time-series-data/raw/master/electricity/electricity.txt.gz)
    comes from data providers at GitHub.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#idm45576029280392-marker)) 文件下载来自GitHub的数据提供者。
- en: ^([2](ch10.html#idm45576028867064-marker)) I do not include the differencing
    code, as we have covered this in [Chapter 11](ch11.html#measuring_error_chapter)
    and it is a matter of a few lines.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#idm45576028867064-marker)) 我们没有包含差分代码，因为这在第11章已经覆盖，并且只涉及几行代码。
- en: ^([3](ch10.html#idm45576028110856-marker)) As we discussed earlier in the book,
    the gold standard is to train and roll models forward across many segments of
    time, but we avoid this complication for this code base. In a production code
    base you would want to have rolling validation and testing to optimize your model
    and get a better sense of real performance; the technique here introduces an additional
    lag between testing and training data and also means the testing data used to
    judge the model ultimately reflects only one time period of the whole history.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#idm45576028110856-marker)) 正如我们在本书中之前讨论的，金标准是在多个时间段内训练和推进模型，但我们在这个代码库中避免了这种复杂性。在生产代码库中，您会希望进行滚动验证和测试，以优化模型并更好地了解实际性能；这种技术引入了测试数据和训练数据之间的额外滞后，并且意味着用于评估模型的测试数据最终只反映了整个历史的一个时间段。
- en: ^([4](ch10.html#idm45576022793272-marker)) Rumor has it that at some point Google
    Translate was powered by a fairly vanilla seven-layer LSTM. However, the largeness
    of their data set undoubtedly helped, as would clever and careful training. Not
    all vanilla models are equal!
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.html#idm45576022793272-marker)) 有传言称谷歌翻译曾由一个相当普通的七层LSTM模型驱动。然而，他们庞大的数据集无疑有所帮助，还有巧妙和谨慎的训练。并非所有普通模型都一样！
- en: ^([5](ch10.html#idm45576022763176-marker)) Code is also available in Oliver
    Pringle’s [personal GitHub repository](https://oreil.ly/L-7ri).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.html#idm45576022763176-marker)) 代码也可以在Oliver Pringle的[个人GitHub仓库](https://oreil.ly/L-7ri)找到。
- en: ^([6](ch10.html#idm45576022241128-marker)) Try training without the AR component
    if you don’t believe me—it’s not difficult to modify the code to remove this component.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.html#idm45576022241128-marker)) 如果你不相信我，可以尝试在没有增强现实（AR）组件的情况下进行训练——修改代码移除这个组件并不困难。
