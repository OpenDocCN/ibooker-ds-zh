- en: 15 Installing applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 安装应用程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Reviewing Kubernetes application management
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查 Kubernetes 应用程序管理
- en: Installing the prototypical Guestbook application
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装典型的 Guestbook 应用程序
- en: Building a production-friendly version of the Guestbook app
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个适合生产的 Guestbook 应用程序版本
- en: Managing applications in Kubernetes is generally a lot easier then managing
    applications deployed on bare servers because all the configuration for applications
    can be done through a unified command-line interface. That said, as you move tens
    or hundreds of containers into a Kubernetes environment, the volume of configuration
    management that needs to be automated can be difficult to approach from a unified
    perspective. ConfigMaps, Secrets, API server credentials, and customization of
    volume types are just a few of the day-to-day paper cuts that can make Kubernetes
    administration tedious over time.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中管理应用程序通常比在裸服务器上部署应用程序要容易得多，因为所有应用程序的配置都可以通过统一的命令行界面完成。尽管如此，当您将成百上千个容器移动到
    Kubernetes 环境中时，需要自动化的配置管理量可能难以从统一的角度来处理。ConfigMaps、Secrets、API 服务器凭证以及卷类型的定制只是日常中可能使
    Kubernetes 管理变得繁琐的几个方面。
- en: In this chapter, we’ll (finally) take a step back from the internal details
    of a Kubernetes implementation and spend a little bit of time looking at the higher-level
    aspects of application configuration and administration. We’ll start by thinking
    about what an application is and how we can install applications on Kubernetes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们（终于）将暂时从 Kubernetes 实现的内部细节中退一步，花一点时间来关注应用程序配置和管理的高级方面。我们将从思考应用程序是什么以及我们如何在
    Kubernetes 上安装应用程序开始。
- en: 15.1 Thinking about apps in Kubernetes
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 在 Kubernetes 中思考应用程序
- en: For our purposes, we’ll refer to a Kubernetes application as a collection of
    API resources that need to be deployed for a service. The canonical example of
    this might be the Guestbook application, defined at [http://mng.bz/y4NE](http://mng.bz/y4NE).
    This application includes
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们将把 Kubernetes 应用程序称为一组需要部署以提供服务的 API 资源。这个典范的例子可能是 Guestbook 应用程序，定义在
    [http://mng.bz/y4NE](http://mng.bz/y4NE)。此应用程序包括
- en: A Redis master database Pod
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Redis 主数据库 Pod
- en: A Redis slave database Pod
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Redis 从属数据库 Pod
- en: A frontend Pod that talks to our Redis master
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个与我们的 Redis 主数据库通信的前端 Pod
- en: A service for all three of these Pods
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为这三个 Pod 提供的服务
- en: 'Application delivery involves upgrading, downgrading, parameterizing, and customizing
    many different Kubernetes resources. Because this is a hotly debated subject with
    many different and competing technical solutions, we won’t attempt to solve this
    entire puzzle for you here. Instead, we’ve included a hearty dose of application
    tooling in this chapter because a large part of deciding how you deploy your Pods
    is related to how you configure them and how you install your application to begin
    with. We can run our Guestbook application on any Kubernetes cluster like so:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 应用交付涉及升级、降级、参数化和定制许多不同的 Kubernetes 资源。由于这是一个备受争议的主题，存在许多不同且相互竞争的技术解决方案，我们在此不会尝试为您解决这个整个谜题。相反，我们在本章中包含了大量应用工具，因为您如何部署您的
    Pods 的很大一部分与您如何配置它们以及您最初如何安装应用程序有关。我们可以这样在任何 Kubernetes 集群上运行我们的 Guestbook 应用程序：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Curling from the internet
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从互联网上curl
- en: 'We’ve noted this before, but we’ll do so again: curling YAML files from the
    internet can be a dangerous business. In our case, we `curl` our YAML files directly
    from github.com/kubernetes, which is a trusted repository maintained by thousands
    of well-known and vetted CNCF (Cloud Native Computing Foundation) organization
    members. By the end of this chapter, we’ll have a much more enterprise-grade and
    realistic way to install the same Guestbook app, so just hang tight.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经提到过这一点，但我们会再次强调：从互联网上curl YAML 文件可能是一件危险的事情。在我们的案例中，我们直接从 github.com/kubernetes
    `curl` 我们的 YAML 文件，这是一个由数千名知名且经过审查的 CNCF（云原生计算基金会）组织成员维护的可信仓库。到本章结束时，我们将拥有一个更符合企业级和现实的方式安装相同的
    Guestbook 应用程序，所以请耐心等待。
- en: 'After issuing the previous command, we’ll soon see all of our Pods up and running
    with multiple replicas of our three frontend and Redis slave Pods. At its most
    basic level, this is what installing a Kubernetes application looks like:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在发出上一条命令后，我们很快就会看到所有我们的 Pods 都启动并运行，我们的三个前端和 Redis 从属 Pods 有多个副本。在最基本层面上，这就是安装
    Kubernetes 应用程序的样子：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 15.1.1 Application scope influences what tools you should use
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 应用程序范围影响您应该使用哪些工具
- en: 'The minute we install Guestbook, we have to ask ourselves a few obvious questions.
    These questions are pertinent to scaling, upgrades, customization, security, and
    modularization:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Guestbook 的那一刻起，我们必须问自己几个明显的问题。这些问题与扩展、升级、定制、安全和模块化有关：
- en: Are users going to be manually scaling the Guestbook web application Pods up
    and down? Or do we want to autoscale our deployment based on load?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否将手动调整 Guestbook 网络应用 Pods 的数量？或者我们希望根据负载自动扩展我们的部署？
- en: Do we want to upgrade our Guestbook app periodically, and if so, do we want
    to upgrade the other Pods with it in lockstep? If so, should we build a Kubernetes
    Operator?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否希望定期升级 Guestbook 应用程序，如果是这样，我们是否希望与其他 Pods 同时升级？如果是这样，我们应该构建一个 Kubernetes
    Operator 吗？
- en: Do we have a discrete number of configurations that are well-defined (for example,
    are there a few alternative Redis configurations we care about)? If so, should
    we use `ytt`, `kustomize`, or some other tool so that we don’t need to copy and
    paste large redundant chunks of YAML every time we want to save a new flavor of
    our application’s settings?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有明确的离散配置数量（例如，是否有一些我们关心的替代 Redis 配置）？如果是这样，我们应该使用 `ytt`、`kustomize` 或其他工具，这样我们就不需要在保存应用程序设置的新版本时每次都复制和粘贴大量冗余的
    YAML 文件。
- en: Is our Redis database secure? Does it need to be? If so, should we add RBAC
    credentials for updating or editing the namespace in which our app resides, or
    should we install NetworkPolicy rules alongside it? We can peruse all of the rules
    at [https://redis.io/topics/security](https://redis.io/topics/security) and implement
    these using Secrets, ConfigMaps, and so on. Additionally, we may need to rotate
    these Secrets periodically, which would introduce the need for some kind of recurring
    automation. (This would hint at the need for an Operator.)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的 Redis 数据库是否安全？它需要安全吗？如果是这样，我们应该为更新或编辑应用程序所在的命名空间添加 RBAC 凭证，还是应该安装与它并行的 NetworkPolicy
    规则？我们可以在 [https://redis.io/topics/security](https://redis.io/topics/security)
    上查看所有规则，并使用 Secrets、ConfigMaps 等实现这些规则。此外，我们可能需要定期轮换这些 Secrets，这将引入对某种定期自动化的需求。（这会暗示需要
    Operator。）
- en: Although we could deploy our application in a specific namespace, how would
    we be able to track the provenance and state of the overall application’s health
    over time and upgrade it as an atomic unit? Deploying a massive list of Kubernetes
    resources as an app from a big file is clumsy for multiple reasons. One is that
    it’s not clear what our application really is once it’s lost within our cluster.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我们可以在特定的命名空间中部署我们的应用程序，但我们如何能够跟踪应用程序的整体健康状况随时间的变化，并将其作为一个原子单元进行升级？将大量 Kubernetes
    资源作为应用程序从大文件部署出去，在多个方面都显得笨拙。其中之一是，一旦它在我们集群中丢失，我们就不清楚我们的应用程序究竟是什么。
- en: 15.2 Microservice apps typically require thousands of lines of configuration
    code
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 微服务应用程序通常需要数千行配置代码
- en: Microservices break the individual functionality of an application into separate
    services, each of which has its own unique configuration. This comes with a high
    cost when compared to large, monolithic applications, where much of the communication
    and security of containers is done through the inherent use of in-memory computing.
    Getting back to our Guestbook app, which has three microservices and 200 lines
    of code, we can see that anywhere from 10 to 50 lines of code are required for
    each API object we create.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务将应用程序的个体功能分解为单独的服务，每个服务都有自己的独特配置。与大型单体应用程序相比，这会带来高昂的成本，因为在大型单体应用程序中，容器的许多通信和安全都是通过内存计算的内禀使用来完成的。回到我们的
    Guestbook 应用程序，它有三个微服务和 200 行代码，我们可以看到，为每个我们创建的 API 对象，可能需要 10 到 50 行代码。
- en: A typical Kubernetes app for an enterprise would involve more sophisticated
    configuration, anywhere from 10 to 20 containers, and each one of these would
    typically have at least one service, one ConfigMap object, and a few Secrets associated
    with it. A back-of-the-envelope estimate for the amount of code in such an application’s
    configuration would easily be thousands of lines of YAML spread over many files.
    It’s obviously unwieldy to copy thousands of lines of code every time we deploy
    a new app. Let’s look, then, at a few different solutions for managing Kubernetes
    configuration for long-running, real-world applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的企业级Kubernetes应用程序将涉及更复杂的配置，从10到20个容器不等，每个容器通常至少有一个服务、一个ConfigMap对象以及一些与其相关的Secrets。对于这样一个应用程序配置中的代码量，一个粗略的估计是数千行YAML，分布在许多文件中。显然，每次部署新应用程序时复制数千行代码是不切实际的。那么，让我们来看看管理长期运行、现实世界应用程序的Kubernetes配置的几种不同解决方案。
- en: Don’t be afraid to rethink your application
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不要害怕重新思考您的应用程序
- en: 'Before you dive too deep into the infinitude of options for applications, we’ll
    just ask that you heed one warning: if your installation tooling is exceedingly
    complex, there is a high likelihood that you are masking broken elements in your
    underlying application. In these scenarios, it might be wise to simplify the way
    your application is managed. As a primary example, often times, developers make
    too many microservices or build more flexibility into an application than is necessary
    (usually because adequate testing isn’t in place to measure and set the right
    configuration defaults). Sometimes, the best solution to configuration management
    is to simply eliminate configurability entirely.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在您深入探索应用程序选项的无穷无尽之前，我们只想提醒您注意一个警告：如果您的安装工具过于复杂，那么您很可能正在掩盖您底层应用程序中的损坏元素。在这些情况下，简化您应用程序的管理方式可能是个明智的选择。作为一个主要例子，很多时候，开发者创建了过多的微服务，或者在一个应用程序中构建了比必要的更多灵活性（通常是因为没有足够的测试来衡量和设置正确的配置默认值）。有时，配置管理的最佳解决方案就是完全消除可配置性。
- en: 15.3 Rethinking our Guestbook app installation for the real world
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 重新思考我们的Guestbook应用在现实世界中的安装
- en: 'Now that we’ve defined our overall problem space, managing Kubernetes application
    configurations, let’s rethink our Guestbook application with these solutions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经定义了我们的整体问题空间，即管理Kubernetes应用程序配置，那么让我们带着这些解决方案重新思考我们的Guestbook应用程序：
- en: '*Yaml templating*—We’ll use `ytt` for this, but we could also use tools such
    as `kustomize` or `helm3` for this functionality.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Yaml模板化*—我们将使用`ytt`来完成这项工作，但也可以使用`kustomize`或`helm3`等工具来实现这一功能。'
- en: '*Deploying and upgrading our app*—We’ll use a Carvel `kapp-controller` project
    here, but we could also build a Kubernetes Operator to do this.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*部署和升级我们的应用*—在这里我们将使用Carvel `kapp-controller`项目，但我们也可以构建一个Kubernetes Operator来完成这项工作。'
- en: Why not `helm`?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不使用`helm`？
- en: 'We don’t explicitly want to endorse one tool over another: `helm3`, `kustomize`,
    and `ytt` can all be used variably to accomplish the same end goals. We prefer
    `ytt` because of its modular and fully programmable nature (and it is integrated
    with Starlark). But at the end of the day, pick a tool. `helm3`, `kustomize`,
    `ytt` are all great tools, but there are many other great tools out there that
    solve YAML overload. There’s no specific reason why these examples cannot be implemented
    using other technologies as well. For that matter, there’s always `sed`.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不想明确地支持某一工具而反对另一工具：`helm3`、`kustomize`和`ytt`都可以根据需要用来实现相同的目标。我们更喜欢`ytt`，因为它具有模块化和完全可编程的特性（并且它与Starlark集成）。但最终，选择一个工具。`helm3`、`kustomize`、`ytt`都是优秀的工具，但还有许多其他优秀的工具可以解决YAML过载问题。没有特定的原因说明为什么这些示例不能使用其他技术来实现。至于这一点，`sed`总是可用。
- en: The Carvel toolkit ([https://carvel.dev](https://carvel.dev) toolkit) has several
    different modular tools that can be used together or separately for managing the
    entire problem space that we’ve described thus far. It is, actually, the basis
    for much of the functionality of the VMware Tanzu distribution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Carvel工具包（[https://carvel.dev](https://carvel.dev)工具包）有几个不同的模块化工具，可以一起或单独使用来管理我们迄今为止所描述的整个问题空间。实际上，它是VMware
    Tanzu发行版许多功能的基础。
- en: 15.4 Installing the Carvel toolkit
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 安装Carvel工具包
- en: 'The first step to exploring how to ramp up our "guestbook-fu" is to install
    the Carvel toolkit. We can then execute each one of these tools from the command
    line. The following code snippet shows the command to install the toolkit. Moving
    forward, we’ll use `ytt`, `kapp`, and `kapp-controller` to incrementally improve
    and automate our Guestbook application:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 探索如何提高我们的 "guestbook-fu" 的第一步是安装 Carvel 工具包。然后我们可以从命令行执行这些工具中的每一个。以下代码片段显示了安装工具包的命令。从现在开始，我们将使用
    `ytt`、`kapp` 和 `kapp-controller` 逐步改进和自动化我们的 Guestbook 应用程序：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Do we really need all of Carvel?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真的需要所有的 Carvel 吗？
- en: Although we won’t need all of the Carvel tools for this chapter, we’ll install
    them anyway because they play well together. We suggest that you explore some
    of them (such as `imgpkg` or `vendir`) on your own as additional exercises. Each
    one of the Carvel binaries is easy to run, self-contained, and takes up negligible
    space on your system. Feel free to customize this installation to suit your own
    learning goals.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这个章节中不需要所有的 Carvel 工具，但我们仍然会安装它们，因为它们配合得很好。我们建议您自己探索其中的一些工具（例如 `imgpkg`
    或 `vendir`）作为额外的练习。Carvel 的每个二进制文件都易于运行，自包含，并且占用系统空间极小。请随意根据您的学习目标自定义此安装。
- en: '15.4.1 Part 1: Modularizing our resources into separate files'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 第一部分：将资源模块化到单独的文件中
- en: 'The first logical thing to consider when looking at our 200-line wall of YAML
    might be to break it into smaller, more understandable chunks. The reasons for
    this are pretty obvious:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到200行的 YAML 墙时，首先考虑的可能是将其分解成更小、更易于理解的块。这样做的原因非常明显：
- en: Using tools like `grep` or `sed` is a lot easier when we don’t have lots of
    duplicate strings.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们没有大量重复字符串时，使用 `grep` 或 `sed` 等工具要容易得多。
- en: Tracking who may have changed something specific to a particular function simplifies
    version control for small files.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪谁可能更改了特定于特定功能的特定内容，简化了小文件的版本控制。
- en: Adding new Kubernetes objects to our file will eventually get unwieldy, so modularizing
    it will be an eventual requirement anyway. We might as well get ahead of it now.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新的 Kubernetes 对象添加到我们的文件中最终会变得难以管理，因此模块化将是最终的需求。我们不妨现在就提前做好准备。
- en: We’ve pushed our decomposition of Guestbook into two separate directories. We
    put these in [http://mng.bz/M2wm](http://mng.bz/M2wm) for you to clone and play
    with. Feel free to decompose the files in a way that is intuitive for you.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将 Guestbook 的分解工作分成了两个独立的目录。我们将这些目录放在[http://mng.bz/M2wm](http://mng.bz/M2wm)上供您克隆和尝试。请随意以您觉得直观的方式分解文件。
- en: 'Following the exact decomposition steps in this section isn’t mandatory because
    if you ask 10 different programmers how to decompose an object, you’ll get 100
    different answers. However, make sure to make one important modification when
    decomposing it: each Kubernetes resource should have a *unique name*. The Redis
    master deployment, for example, must not be named the same as the Redis service
    object. For example, in the following, we added a *-dep* suffix to the name of
    our Redis master deployment:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 按照本节中的确切分解步骤不是强制性的，因为如果你问10个不同的程序员如何分解一个对象，你会得到100个不同的答案。然而，在分解时，请确保进行一个重要的修改：每个
    Kubernetes 资源都应该有一个*唯一名称*。例如，Redis 主部署不能与 Redis 服务对象同名。例如，在下面的代码中，我们给我们的 Redis
    主部署名称添加了 *-dep* 后缀：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We’ve also done the same thing for the frontend YAML file. The resulting directory
    structure is displayed following this code snippet showing the addition of the
    *-dep* suffix:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也以同样的方式处理了前端 YAML 文件。以下代码片段显示了添加 *-dep* 后缀后的目录结构：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Renaming your Redis master and frontend resource
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重命名 Redis 主和前端资源
- en: If you’re not going to use the files on [http://mng.bz/M2wm](http://mng.bz/M2wm)
    and you are splitting up the guestbook YAML on your own, make sure to rename the
    Redis master and frontend deployment files to redis-master-dep and frontend-dep
    in the `metadata.name` field (as shown in the previous code snippets). This will
    allow us to use `ytt` later on to easily find and substitute the values of YAML
    constructs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不打算使用[http://mng.bz/M2wm](http://mng.bz/M2wm)上的文件，并且您正在自己拆分 guestbook YAML，请确保将
    Redis 主和前端部署文件的 `metadata.name` 字段重命名为 redis-master-dep 和 frontend-dep（如前述代码片段所示）。这将使我们能够稍后使用
    `ytt` 容易地查找和替换 YAML 构造的值。
- en: 'We can now test that our decomposition is equivalent to our original app by
    running `kubectl create -f v1/`. We’ll trust you to run this command and confirm
    that three frontend and two backend Redis Pods are up and happily running. Then,
    you can set up port forwarding to locally browse the Guestbook app at port 8080\.
    For example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过运行 `kubectl create -f v1/` 来测试我们的分解是否与原始应用程序等效。我们将相信你会运行这个命令并确认三个前端和两个后端
    Redis Pod 都已启动并正常运行。然后，你可以设置端口转发，在本地通过端口 8080 浏览 Guestbook 应用程序。例如：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now you can easily enter a few values in the app’s Messages field and see those
    values stored in the backend Redis database. Notice that these are also displayed
    for you on the Guestbook landing page.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在应用程序的消息字段中轻松输入几个值，并看到这些值存储在后端的 Redis 数据库中。注意，这些值也会显示在你的 Guestbook 登录页面上。
- en: '15.4.2 Part 2: Patching our application files with ytt'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 第二部分：使用 ytt 修补我们的应用程序文件
- en: 'We’ve got a working application with a frontend and a backend. What happens
    now if we decide to start hitting it with more load? We might want to allocate
    more CPU to it, for instance. To do this, we’ll want to modify the fe-dep.yaml
    file to increase the `requests.cpu` value. This means that we’ll need to edit
    some YAML:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个具有前端和后端的工作应用程序。现在如果我们决定开始给它增加更多负载会发生什么？我们可能想要给它分配更多的 CPU。为了做到这一点，我们需要修改
    fe-dep.yaml 文件以增加 `requests.cpu` 的值。这意味着我们需要编辑一些 YAML：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ One-tenth of a core isn’t a lot of CPU for a production application.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 核心的一十分之一对于生产应用来说并不是很多 CPU。
- en: In the code example, we could easily replace the `100m` with a `1`, but then
    we would just exchange one hardcoded constant for another. It would be better
    if we could parameterize this value. Additionally, we may want to also increase
    the CPU requirements of Redis. Luckily, we have `ytt`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码示例中，我们可以轻松地将 `100m` 替换为 `1`，但那样我们只是将一个硬编码的常量替换为另一个。如果我们能对这个值进行参数化会更好。此外，我们可能还想增加
    Redis 的 CPU 要求。幸运的是，我们有 `ytt`。
- en: The YAML templating engine, `ytt` ([https://carvel.dev/ytt/](https://carvel.dev/ytt/)),
    allows different customizations to YAML files using modalities like overlaying,
    patching, and so on. It also supports advanced constructs by using the Starlark
    language for implementing logical decisions for text manipulation. Because we’ve
    already installed the Carvel toolkit, let’s just dive into how we can customize
    our application’s CPU in our first `ytt` example.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 模板引擎 `ytt` ([https://carvel.dev/ytt/](https://carvel.dev/ytt/)) 允许使用覆盖、修补等模式对
    YAML 文件进行不同的自定义。它还通过使用 Starlark 语言来实现文本操作的逻辑决策，支持高级结构。因为我们已经安装了 Carvel 工具包，所以让我们直接深入了解我们如何在第一个
    `ytt` 示例中自定义应用程序的 CPU。
- en: YAML in, YAML out
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 输入，YAML 输出
- en: '`ytt` is a YAML in, YAML out tool, and this is an important concept to keep
    in mind. Unlike other tools that have come and gone in the Kubernetes ecosystem,
    `ytt` (like other tools in the Carvel framework) focuses on doing one specific
    job, and doing that very well. It manipulates YAML! It doesn’t install files for
    us, and it isn’t in any way specific to Kubernetes.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`ytt` 是一个 YAML 输入，YAML 输出工具，这是一个需要牢记的重要概念。与其他在 Kubernetes 生态系统中来来去去的工具不同，`ytt`（就像
    Carvel 框架中的其他工具一样）专注于做一项具体的工作，并且做得非常好。它操作 YAML！它不会为我们安装文件，并且它以任何方式都不特定于 Kubernetes。'
- en: 'For our second (v2) iteration of this Guestbook application, we’ll now add
    a new file (call it ytt-cpu-overlay.yaml) in a new directory (call it v2/). Our
    goal is to match our `cpu` stanza in the php-redis frontend web application with
    the Redis master database Pod. Here’s the code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们这个 Guestbook 应用程序的第二次（v2）迭代，我们现在将在一个新目录（称为 v2/）中添加一个新文件（称为 ytt-cpu-overlay.yaml）。我们的目标是匹配
    php-redis 前端 Web 应用程序中的 `cpu` 段落与 Redis 主数据库 Pod。以下是代码：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Our ytt overlay identifies the name of a YAML snippet we want to match.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们 ytt 遮罩识别我们想要匹配的 YAML 片段的名称。
- en: ❷ Once inside of our containers, it substitutes the container with the php-redis
    name.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一旦在容器内部，它将容器替换为 php-redis 名称。
- en: ❸ The original CPU value of 100m is now doubled to 200m.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 原始的 CPU 值 100m 现在翻倍为 200m。
- en: 'Similarly, we can do this for our database Pod. We can make a new file, call
    it v2/ ytt-cpu-overlay-db.yaml, that does the same thing as our previous file:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们也可以为我们的数据库 Pod 做同样的事情。我们可以创建一个新文件，称为 v2/ytt-cpu-overlay-db.yaml，它执行与之前文件相同的功能：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Adds a new CPU value (this time, 300m to differentiate the two)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加一个新的 CPU 值（这次，300m 以区分两者）
- en: 'We can now invoke this transformation of our YAML. For example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以调用这个 YAML 的转换。例如：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Modifies the file to have higher CPU request for the master only
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅修改文件以为主节点请求更高的 CPU
- en: Great, we’ve now come full circle! Originally we had a single file, which was
    easy to package but hard to modify. Using `ytt`, we took many different files
    and added a layer of customization on top of them, so they can then be streamed
    into the `kubectl` command like a single YAML resource.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，我们现在又回到了起点！最初我们只有一个文件，它很容易打包但难以修改。使用 `ytt`，我们取了许多不同的文件，并在它们之上添加了一层定制，这样它们就可以像单个
    YAML 资源一样被流式传输到 `kubectl` 命令中。
- en: 'We might imagine that our application is now ready for production because it
    is capable of adding and substituting our developer configurations with realistic
    ones. If you peruse the documentation at [https://carvel.dev/ytt/](https://carvel.dev/ytt/),
    you’ll see that many further customizations can also be done: adding data values,
    adding entirely new YAML constructs, and so on. In our case, however, we’ll leave
    well enough alone and move up the stack to look at how our patched YAML resources
    can now be bundled into a single, executable application that has its state managed
    as a first-class citizen.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想象，我们的应用程序现在已经准备好投入生产，因为它能够添加和替换我们的开发配置，以使用现实世界的配置。如果你浏览了 [https://carvel.dev/ytt/](https://carvel.dev/ytt/)
    的文档，你会看到还可以进行许多进一步的定制：添加数据值、添加全新的 YAML 构造，等等。然而，在我们的情况下，我们将保持现状，并向上移动到查看我们的修补过的
    YAML 资源现在如何被捆绑成一个单一的、可执行的应用程序，其状态被作为一等公民来管理。
- en: '15.4.3 Part 3: Managing and deploying Guestbook as a single application'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.3 第3部分：将 Guestbook 作为单个应用程序管理和部署
- en: 'You might be annoyed by the number of times we’ve run `kubectl` `delete` in
    this book. If you’ve asked yourself why we’ve done this, it’s usually because
    we haven’t isolated our application from other applications in our cluster. One
    easy way to do this is by deploying an entire app in a namespace that can then
    be deleted or created. However, once we begin treating several resources as a
    single app, we have a new set of questions we want to answer:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经对我们在这本书中多次运行 `kubectl delete` 感到厌烦。如果你问过自己为什么这样做，通常是因为我们没有将我们的应用程序与集群中的其他应用程序隔离开来。一个简单的方法是将整个应用程序部署在一个命名空间中，然后可以删除或创建该命名空间。然而，一旦我们开始将多个资源视为单个应用程序，我们就有一系列新的问题想要回答：
- en: How many distinct applications am I running in a given namespace?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定的命名空间中，我运行了多少个不同的应用程序？
- en: Was an upgrade of all the resources in a given application successful?
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定应用中所有资源的升级是否成功？
- en: How many resources of each type did I associate with my application?
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将多少种类型的资源关联到了我的应用程序中？
- en: Each of these questions can be answered with a combination of `kubectl`, `grep`,
    and some clever Bash aggregations. However, this approach won’t scale if you have
    tens or hundreds of containers, Secrets, ConfigMaps, and other resources in your
    app. Additionally, it won’t scale across the entire spectrum of apps in your cluster,
    which can also easily range in the hundreds or thousands. This is where the `kapp`
    tool comes into focus for us.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题可以通过结合使用 `kubectl`、`grep` 和一些巧妙的 Bash 聚合来回答。然而，如果你在应用中有成百上千个容器、Secrets、ConfigMaps
    等其他资源，这种方法就无法扩展。此外，它也无法扩展到集群中所有应用的全范围，这些应用的数量也可能轻易达到数百或数千。这就是 `kapp` 工具对我们来说变得重要的地方。
- en: What about `helm`?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 `helm` 呢？
- en: '`helm` was one of the earliest and most successful application management solutions
    for Kubernetes. Originally, it combined the aspects of stateful upgrade and resource
    installation with YAML templating. The Carvel project borrowed from the lessons
    of `helm` to separate many of these functionalities into separate tools.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`helm` 是 Kubernetes 最早且最成功的应用程序管理解决方案之一。最初，它结合了有状态升级和资源安装的方面，并使用 YAML 模板。Carvel
    项目借鉴了 `helm` 的经验，将这些功能中的许多分离成单独的工具。'
- en: '`helm3` is actually a much more modular attempt at managing applications that
    can be run in a stateless manner, similar to what we will see with the `kapp`
    tool. In any case, `helm3` and the Carvel ecosystem have quite a bit of overlap,
    and both can be used for similar cases, but they are charted by different opinions,
    philosophical approaches, and communities. We encourage you to explore both, especially
    if you feel like `kapp` is not an ideal solution for your problems. Either way,
    you will learn quite a lot about managing Kubernetes applications in general by
    following the evolution of Guestbook when using `kapp`, so keep moving forward!'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`helm3` 实际上是一个更模块化的尝试，用于以无状态方式管理应用程序，这与我们将要看到的 `kapp` 工具类似。无论如何，`helm3` 和 Carvel
    生态系统有很多重叠，两者都可以用于类似的情况，但它们由不同的观点、哲学方法和社区所引导。我们鼓励你探索两者，特别是如果你觉得 `kapp` 不是一个理想的解决方案。无论如何，通过使用
    `kapp` 时跟踪 Guestbook 的演变，你将学会很多关于管理 Kubernetes 应用程序的一般知识，所以继续前进！'
- en: 'Using the `kapp` tool ([https://carvel.dev/kapp/](https://carvel.dev/kapp/))
    is quite easy, especially now that we have the ability to customize our applicaiton
    with `ytt`. To give it a shot, let’s do one last cleanup of our Guestbook application
    in case it’s still running:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kapp` 工具([https://carvel.dev/kapp/](https://carvel.dev/kapp/)) 非常简单，尤其是现在我们有了使用
    `ytt` 定制应用程序的能力。为了尝试一下，让我们最后清理一下我们的 Guestbook 应用程序，以防它还在运行：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Deletes the resources we made in the last section (in case they’re still around)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 删除我们在上一节中创建的资源（以防它们还在）
- en: 'We’ll assume you’ve already installed the `kapp` binary. Let’s now run the
    same `ytt` commands to generate our application, but install it using `kapp` instead
    of `kubectl`. Note that in this example, we’re using Antrea as our CNI provider.
    But the CNI you run at this point doesn’t really matter, as long as you have one
    (note that several columns in the output in this code snippet are omitted due
    to page restrictions):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设你已经安装了 `kapp` 二进制文件。现在，让我们运行相同的 `ytt` 命令来生成我们的应用程序，但使用 `kapp` 而不是 `kubectl`
    来安装它。注意，在这个例子中，我们使用 Antrea 作为我们的 CNI 提供者。但你在此时运行的 CNI 并不重要，只要你有一个（注意，由于页面限制，此代码片段中的几个列被省略了）：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Takes the ytt statement for generating YAML and pushes it to kapp as input
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 ytt 生成 YAML 的语句推送到 kapp 作为输入
- en: If you type `y`, you’ll see `kapp` do a lot of work, including annotating your
    resources so that it can later manage them for you. It will upgrade or delete
    the resources or give you the overall status of your app by name. In our case,
    we called our app Guestbook, but we could call it anything we want.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你输入 `y`，你会看到 `kapp` 做很多工作，包括注释你的资源，以便它可以后来为你管理它们。它将升级或删除资源，或者通过名称给出你应用程序的整体状态。在我们的例子中，我们称我们的应用程序为
    Guestbook，但我们可以称它为任何我们想要的名称。
- en: 'After entering “yes” (by typing `y`), you’ll now see more information than
    was available from `kubectl`. This is because for `kapp`, an application is really
    a first-class citizen, and it wants to make sure that all of your resources come
    up in a healthy state. You can imagine how `kapp` can thus be used in a CI/CD
    environment to fully automate upgrading and management of an application. For
    example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入“是”（通过输入 `y`）后，你现在将看到比 `kubectl` 可用的更多信息。这是因为对于 `kapp` 来说，应用程序实际上是一个一等公民，它想确保你所有的资源都以健康状态出现。你可以想象
    `kapp` 可以如何用于 CI/CD 环境来完全自动化应用程序的升级和管理。例如：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now go back and look at our application again to confirm that it’s still
    running. Use the following command to check this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以回到我们的应用程序，再次确认它仍在运行。使用以下命令进行检查：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can also use `kapp` to give us detailed information about a running application.
    For that, we use the `inspect` command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `kapp` 来获取关于运行中的应用程序的详细信息。为此，我们使用 `inspect` 命令：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It’s instructive to note that some of the objects that were created by Kubernetes
    under the hood, such as Endpoints and EndpointSlices, are included in this readout.
    EndpointSlices and their availability as load-balancing targets for Services are
    crucial for any app to be usable by an end consumer. `kapp` has captured this
    information for us, along with the success and failure state of all resources
    in our application, in a single, easy-to-read, tabular format.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到一些由 Kubernetes 在幕后创建的对象，例如 Endpoints 和 EndpointSlices，都包含在这个读取结果中。EndpointSlices
    及其作为服务负载均衡目标的可用性对于任何应用程序都能被最终用户使用至关重要。`kapp` 已经为我们捕获了这些信息，包括我们应用程序中所有资源的成功和失败状态，以单一、易于阅读的表格格式。
- en: Finally, we can now delete our app easily and in a complete, atomic fashion
    using `kapp` by running `kapp delete --app=guestbook`. This will be the inverse
    of our `kapp deploy` operation, so we won’t show the output because the result
    of this command is mostly self-explanatory.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们现在可以使用`kapp`通过运行`kapp delete --app=guestbook`轻松且完整地删除我们的应用程序。这将是我们`kapp
    deploy`操作的逆操作，因此我们不会显示输出，因为此命令的结果主要不言自明。
- en: '15.4.4 Part 4: Constructing a kapp Operator to package and manage our application'
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.4 第4部分：构建一个kapp操作符以打包和管理我们的应用程序
- en: Now that we’ve bundled our entire application as a set of atomically-managed
    resources which have well-defined names, we’ve essentially built what might be
    thought of as a CustomResourceDefinition (CRD). The `kapp-controller` project
    allows us to take any `kapp` application and wrap it up with a few automation
    niceties. This final exploration completes our transition from “one big blob of
    YAML from the internet” to a stateful, automatically managed application that
    we can run in an enterprise situation along with hundreds of other applications.
    It will also introduce you, quite gently, to the concept of how one can build
    a Kubernetes Operator.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将整个应用程序打包成一组原子管理的资源，这些资源具有明确的名称，我们实际上已经构建了可能被认为是自定义资源定义（CRD）的东西。`kapp-controller`项目允许我们使用一些自动化优点将任何`kapp`应用程序包装起来。这次最后的探索完成了我们从“来自互联网的一个大块YAML”到状态化、自动管理应用程序的过渡，我们可以在企业环境中运行这个应用程序以及数百个其他应用程序。它还将温和地向您介绍如何构建Kubernetes操作符的概念。
- en: 'The first thing we’ll do is install the `kapp-controller` tool using `kapp`.
    Here we go again, installing things from the internet, but, as always, feel free
    to inspect the YAML before installing it. For your convenience, here’s the YAML:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是使用`kapp`安装`kapp-controller`工具。我们再次从互联网上安装东西，但，就像往常一样，在安装之前请随意检查YAML。为了您的方便，以下是YAML：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Installs kapp-controller using the kapp tool
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用kapp工具安装kapp-controller
- en: ❷ Installs a simple RBAC definition
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 安装一个简单的RBAC定义
- en: You might be wondering why we needed to set up RBAC rules for `kapp-controller`.
    Installing the RBAC definition (default-ns.yml) allows the `kapp-controller` in
    the default namespace to read and write API objects as any Operator would require.
    Operators are administrative applications, and the `kapp-controller` Pod needs
    to create, edit, and update various Kubernetes resources in order to do its job
    as a generic Operator for our applications.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们需要为`kapp-controller`设置RBAC规则。安装RBAC定义（default-ns.yml）允许默认命名空间中的`kapp-controller`像任何操作符一样读取和写入API对象。操作符是管理应用程序，`kapp-controller`
    Pod需要创建、编辑和更新各种Kubernetes资源，以便作为我们应用程序的通用操作符执行其工作。
- en: Now that `kapp-controller` is running in our cluster, we can use it to automate
    the `ytt` complexity from the previous section, and we can do this in a declarative
    way, which is entirely managed inside of Kubernetes. To do this, we need to create
    a `kapp` CR (CustomResource) The specification of a `kapp` application is described
    at [http://mng.bz/PWqv](http://mng.bz/PWqv). The particular fields we care about
    are
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`kapp-controller`已经在我们的集群中运行，我们可以使用它来自动化上一节中的`ytt`复杂性，并且我们可以以声明式的方式完成，这完全在Kubernetes内部管理。为此，我们需要创建一个`kapp`
    CR（自定义资源）。`kapp`应用程序的规范描述在[http://mng.bz/PWqv](http://mng.bz/PWqv)。我们关心的特定字段是
- en: '`git`—Defines a cloneable Git repository for our applications source code'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`git`——定义了我们应用程序源代码的可克隆Git仓库'
- en: '`template`—Defines where the `ytt` templates to install our application live'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`template`——定义了安装应用程序的`ytt`模板所在的位置'
- en: 'The first thing we’ll do is to create an application specification for our
    original Guestbook app, which will run as a `kapp`-controlled application. After
    that, we’ll add our `ytt` templates back in:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是为我们的原始Guestbook应用程序创建一个应用程序规范，该应用程序将作为一个`kapp`控制的应用程序运行。之后，我们将重新添加我们的`ytt`模板：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Uses the service account created previously for this app installation
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用之前为该应用程序安装创建的服务帐户
- en: ❷ Specifies where our app is defined
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定我们的应用程序定义的位置
- en: ❸ Because the code for our app actually lives in carvel-guestbook/v1/, we need
    to specify this subpath.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 因为我们的应用程序代码实际上位于carvel-guestbook/v1/，所以我们需要指定这个子路径。
- en: 'At this point, the lightbulbs in your mind may be going off about continuous
    delivery and rightly so. This single YAML declaration lets us leave the entire
    management of our application to Kubernetes and to our online `kapp-controller`
    Operator itself. Let’s give it a shot. Run the `kubectl create -f` to create the
    Guestbook app using the YAML snippet previously shown, and then execute this command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你心中的灯泡可能已经亮了起来，关于持续交付的想法，这是完全正确的。这个单独的 YAML 声明让我们可以将我们应用程序的整个管理权交给 Kubernetes
    和我们的在线 `kapp-controller` 操作符本身。让我们试一试。运行 `kubectl create -f` 命令，使用之前显示的 YAML 片段创建
    Guestbook 应用程序，然后执行以下命令：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can see that our guestbook-ctrl application was made for us by the `kapp-controller`
    automatically. We can again use `kapp` to inspect this application:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的 guestbook-ctrl 应用程序是由 `kapp-controller` 自动为我们创建的。我们还可以再次使用 `kapp`
    来检查这个应用程序：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We actually have now integrated our application into a CI/CD system that can
    be managed entirely inside of Kubernetes. Great! One can now imagine building
    arbitrarily complicated systems for developers to submit and maintain CRDs for
    their applications, which are ultimately deployed and managed by the single `kapp-controller`
    Operator that runs in our default namespace.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将我们的应用程序集成到一个 CI/CD 系统中，该系统可以完全在 Kubernetes 内部管理。太棒了！现在可以想象构建任意复杂的系统，让开发者提交和维护他们应用程序的
    CRDs，这些 CRDs 最终由运行在我们默认命名空间中的单个 `kapp-controller` 操作符部署和管理。
- en: 'If we want, we can redeploy this same application (often referred to as an
    “App CR”) in a new namespace. To do that, we simply add or delete these by running
    `kubectl` `get` `apps`, because the `kapp-controller` Pod has installed a CRD
    for `kapp` applications in our cluster for us:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想的话，我们可以在新的命名空间中重新部署这个相同的应用程序（通常称为“App CR”）。为此，我们只需运行 `kubectl get apps`
    命令来添加或删除这些内容，因为 `kapp-controller` Pod 已经为我们集群中的 `kapp` 应用程序安装了一个 CRD：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We’ve just implemented a full-blown Operator deployment of the Guestbook application.
    Now, let’s try to add our `ytt` templates back in. In this example, we’ve pushed
    the `ytt` output from our previous example to a specific directory in the k8sprototypes
    repository (you might want to use your own GitHub repository for this exercise,
    but it’s not necessary):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚实现了 Guestbook 应用程序的完整 Operator 部署。现在，让我们尝试将我们的 `ytt` 模板重新添加进来。在这个例子中，我们将之前示例中的
    `ytt` 输出推送到 k8sprototypes 仓库中的特定目录（你可能想为这个练习使用你自己的 GitHub 仓库，但这不是必需的）：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can now create a new definition for our Guestbook app, which includes our
    `ytt` templates, by simply writing the transformed `ytt` templates to another
    directory. Another nicety of using an Operator to manage our applications is that
    we can create and delete them without any special tooling. This is because the
    `kubectl` client is aware of them as API resources. To delete the Guestbook app,
    run this command:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过简单地编写转换后的 `ytt` 模板到另一个目录来为我们的 Guestbook 应用程序创建一个新的定义，该定义包括我们的 `ytt`
    模板。使用 Operator 来管理我们的应用程序的另一个优点是，我们可以创建和删除它们而无需任何特殊工具。这是因为 `kubectl` 客户端将它们视为
    API 资源。要删除 Guestbook 应用程序，请运行此命令：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can now just use `kubectl` to declaratively delete our Guestbook application,
    and the `kapp-controller` will do the rest. We can also use commands such as `kubectl
    describe` to see the status of our application.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `kubectl` 声明性地删除我们的 Guestbook 应用程序，`kapp-controller` 将完成剩余的工作。我们还可以使用
    `kubectl describe` 等命令来查看应用程序的状态。
- en: We’ve only touched on the flexibility of the Operator model for managing and
    creating application definitions. As follow-up exercises, it’s worth exploring
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是触及了 Operator 模型在管理和创建应用程序定义方面的灵活性。作为后续练习，值得探索
- en: Using the `kapp-controller` to deploy multiple copies of the same app in many
    namespaces
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `kapp-controller` 在许多命名空间中部署相同应用程序的多个副本
- en: Using the `ytt` directive inside of `kapp-controller`
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `kapp-controller` 中使用 `ytt` 指令
- en: Using the `kapp-controller`’s ability to deploy and manage Helm charts as applications
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `kapp-controller` 的能力部署和管理 Helm 图表作为应用程序
- en: Embedding Secrets into your `kapp` applications so that you can deploy CI/CD
    workflows securely
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将机密嵌入到你的 `kapp` 应用程序中，以便你可以安全地部署 CI/CD 工作流
- en: This wraps up our iterative improvement of the Guestbook application. We’ll
    conclude this chapter by looking at our old and familiar friends, the Calico and
    Antrea CNI providers, to see how they implement full-blown Kubernetes Operators
    with fine-grained CRDs for administrators.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了我们对 Guestbook 应用程序的迭代改进。我们将通过查看我们熟悉的老朋友 Calico 和 Antrea CNI 提供者来结束这一章，看看它们如何实现具有细粒度
    CRDs 的完整 Kubernetes Operator。
- en: 15.5 Revisiting the Kubernetes Operator
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 重新审视 Kubernetes Operator
- en: The `kapp` and `kapp-controller` tools provided us with an automated, atomic
    way to deal with all the services in our Guestbook application in a stateful way.
    This, thus, introduced the concept of an Operator to us in an organic way. For
    many applications, using a batteries-included tool such as `kapp-controller` or,
    alternatively, something like `Helm` ([https://helm.sh](https://helm.sh)) can
    spare you the time and complexity of needing to build a full-blown Kubernetes
    CRD and Operator implementation. However, CRDs are everywhere in the modern Kubernetes
    ecosystem, and we would be doing you a disservice if we didn’t explore them at
    least a little bit.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`kapp` 和 `kapp-controller` 工具为我们提供了一种自动的、原子化的方式来以有状态的方式处理 Guestbook 应用程序中的所有服务。因此，这以有机的方式向我们介绍了
    Operator 的概念。对于许多应用程序，使用内置工具如 `kapp-controller` 或类似 `Helm` ([https://helm.sh](https://helm.sh))
    可以节省你构建完整的 Kubernetes CRD 和 Operator 实现的时间和复杂性。然而，CRDs 在现代 Kubernetes 生态系统中无处不在，如果我们不至少稍微探索一下它们，那将是对你的一种不公。'
- en: Operator factories
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Operator 工厂
- en: If you do decide your application is sufficiently advanced to need its own fine-grained
    Kubernetes API extensions, you’ll want to build an Operator. The process of building
    an Operator usually involves autogenerating API clients for custom Kubernetes
    CRDs and then ensuring that these clients do the “right” thing when resources
    are created, destroyed, or edited. There are many tools online, such as the [https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder)
    project, that make it easy to build full-blown Operators.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实认为你的应用程序足够先进，需要自己的细粒度 Kubernetes API 扩展，那么你将想要构建一个 Operator。构建 Operator
    的过程通常涉及为自定义 Kubernetes CRDs 自动生成 API 客户端，并确保这些客户端在资源创建、销毁或编辑时执行“正确”的操作。网上有许多工具，例如
    [https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder)
    项目，可以轻松构建完整的 Operator。
- en: 'Let’s spin up a `kind` cluster with two different CNI providers (Calico and
    Antrea) as a way to dive into how we use CRDs. In this case, because we’ll also
    want to potentially add NetworkPolicy objects to our cluster, let’s create a Calico-based
    cluster. We can use the `kind-local-up.sh` script to do this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动一个 `kind` 集群，使用两个不同的 CNI 提供者（Calico 和 Antrea），以此作为深入了解我们如何使用 CRD 的方式。在这种情况下，因为我们还可能想要向我们的集群添加
    NetworkPolicy 对象，所以让我们创建一个基于 Calico 的集群。我们可以使用 `kind-local-up.sh` 脚本来完成这项工作：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Kubernetes-native applications often make lots of CRDs for their applications.
    CRDs allow any application to use the Kubernetes API server to store configuration
    data and to enable the creation of Operators (which we will dig into later on
    in this chapter). Operators are Kubernetes controllers that watch the API server
    for changes and then run Kubernetes administrative tasks in response. For example,
    if we were to look at our newly created `kind` cluster, we could see several Calico
    CRDs, which have specific configurations for Calico as a CNI provider:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 原生应用程序通常为它们的应用程序创建大量的 CRDs。CRDs 允许任何应用程序使用 Kubernetes API 服务器来存储配置数据，并启用
    Operators 的创建（我们将在本章后面详细探讨）。Operators 是监视 API 服务器更改并随后运行 Kubernetes 管理任务的 Kubernetes
    控制器。例如，如果我们查看我们新创建的 `kind` 集群，我们可以看到几个 Calico CRDs，它们为作为 CNI 提供者的 Calico 提供了特定的配置：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Lists all CRDs in our cluster
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列出我们集群中的所有 CRDs
- en: ❷ We can disable healthChecks if we don’t feel it’s necessary.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果我们认为不需要，我们可以禁用 healthChecks。
- en: ❸ Sets the port on which our Calico kube controller serves up its Prometheus
    metrics. Here’s where we can change the port if needed.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置 Calico kube controller 提供 Prometheus 指标所使用的端口。这里是我们可以在需要时更改端口的地方。
- en: 'Interestingly, Calico stores its configuration inside of our Kubernetes cluster
    as a custom object with its own types. Actually, Antrea does a similar thing.
    We can inspect the contents of an Antrea cluster by running the `kind-local-up.sh`
    script again like so:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，Calico 将其配置存储在我们的 Kubernetes 集群中，作为一个具有自己类型的自定义对象。实际上，Antrea 也做了类似的事情。我们可以通过再次运行
    `kind-local-up.sh` 脚本来检查 Antrea 集群的内部内容，如下所示：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Deletes our previous cluster
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 删除我们之前的集群
- en: ❷ Creates a new cluster with Antrea as the CNI provider
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 Antrea 作为 CNI 提供者创建一个新的集群
- en: 'After a few moments, we can look at the various configuration objects that
    Antrea uses, just like we did for Calico. The following code snippet shows the
    command to generate this output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，我们可以查看 Antrea 使用的一些配置对象，就像我们之前对 Calico 所做的那样。以下代码片段显示了生成此输出的命令：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Custom NetworkPolicys object types: An example of why vendors really love CRDs'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义 NetworkPolicys 对象类型：这是供应商真正喜欢 CRD 的一个例子
- en: If we look at Calico and Antrea CRDs, we can see they have some commonalities,
    one being network policies. The NetworkPolicy API in Kubernetes doesn’t support
    all possible network policies when using certain CNIs. As an example, the PortRange
    policy (which was only added to Kubernetes v1.21) was a vendor-specific policy
    in both Calico and Antrea for quite some time. Because both Calico and Antrea
    have their own network policy custom resources, however, users can create the
    newer NetworkPolicy objects that are understood by these specific CNIs. CRDs provide
    a neat way to differentiate products without having to create vendor-specific
    tooling for managing these. For example, you can edit a k8s.io NetworkPolicy object
    using the `kubectl edit` directive in the same way that you can edit any CRD.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 Calico 和 Antrea CRDs，我们可以看到它们有一些共同点，其中之一就是网络策略。Kubernetes 中的 NetworkPolicy
    API 在使用某些 CNIs 时不支持所有可能的网络策略。例如，PortRange 策略（仅在 Kubernetes v1.21 中添加）在 Calico
    和 Antrea 中都是一段时间的供应商特定策略。然而，由于 Calico 和 Antrea 都有自己的网络策略自定义资源，因此用户可以创建这些特定 CNIs
    可以理解的较新的 NetworkPolicy 对象。CRDs 提供了一种优雅的方式来区分产品，而无需为管理这些产品创建供应商特定的工具。例如，您可以使用 `kubectl
    edit` 指令编辑 k8s.io NetworkPolicy 对象，就像您可以编辑任何 CRD 一样。
- en: If you are interested in learning more about specific network policies that
    extend the network security capabilities of Kubernetes, you might be interested
    in [http://mng.bz/aD9Y](http://mng.bz/aD9Y) or [http://mng.bz/g4mn](http://mng.bz/g4mn).
    Of course, if you haven’t learned about basic Kubernetes NetworkPolicy APIs, you
    may want to research those first at [http://mng.bz/enBZ](http://mng.bz/enBZ).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于扩展 Kubernetes 网络安全能力的特定网络策略，您可能会对 [http://mng.bz/aD9Y](http://mng.bz/aD9Y)
    或 [http://mng.bz/g4mn](http://mng.bz/g4mn) 感兴趣。当然，如果您还没有学习关于基本 Kubernetes NetworkPolicy
    API 的知识，您可能首先需要研究这些内容，请访问 [http://mng.bz/enBZ](http://mng.bz/enBZ)。
- en: Note that creating, editing, or deleting NetworkPolicy objects for Calico or
    Antrea results in immediate creation of firewall rules. However, editing other
    CRDs for these applications may not result in immediate changes to their configurations,
    and these changes may not be realized until you restart the corresponding Calico
    or Antrea Pods. Thus, although CRDs give you a way to extend the Kubernetes API
    server, they don’t give you any guarantee of how your new API constructs will
    be implemented.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为 Calico 或 Antrea 创建、编辑或删除 NetworkPolicy 对象会导致立即创建防火墙规则。然而，编辑这些应用程序的其他 CRDs
    可能不会立即更改它们的配置，并且这些更改可能直到您重启相应的 Calico 或 Antrea Pods 才会实现。因此，尽管 CRDs 给您提供了扩展 Kubernetes
    API 服务器的方式，但它们并不保证您的新 API 构造将如何实现。
- en: Our earlier installation of Calico as a CNI provider was deployed via a YAML
    file, which has several configuration objects associated with it. Alternatively,
    we could have deployed it with the Tigera `operator` tool ([https://github.com/tigera/operator](https://github.com/tigera/operator)),
    which handles upgrading and creation of Calico YAML manifests for us. As a real-time
    configuration option, we could have installed the `calicoctl` tool, which can
    configure certain aspects of it for us as well.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前作为 CNI 提供者安装的 Calico 是通过 YAML 文件部署的，它关联着几个配置对象。或者，我们也可以使用 Tigera 的 `operator`
    工具（[https://github.com/tigera/operator](https://github.com/tigera/operator)）来部署它，这个工具会为我们处理
    Calico YAML 清单的升级和创建。作为一个实时配置选项，我们还可以安装 `calicoctl` 工具，它也可以为我们配置其某些方面。
- en: Similarly, our installation of Antrea was done with a YAML manifest (as we discussed
    thoroughly in our previous chapters on CNI). Just like Calico, an Antrea cluster
    involves creation of several configuration components that live inside of our
    cluster (see [http://mng.bz/J1qa](http://mng.bz/J1qa)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们的 Antrea 安装也是使用 YAML 清单完成的（正如我们在之前的 CNI 章节中详细讨论的那样）。就像 Calico 一样，一个 Antrea
    集群涉及到创建几个配置组件，这些组件存在于我们的集群内部（见 [http://mng.bz/J1qa](http://mng.bz/J1qa)）。
- en: We’ve now explored many aspects of Kubernetes application management. New tools
    in this space are released on a continuous basis, so consider this a beginning
    to your exploration of how to scale and manage large fleets of applications in
    production. For many newcomers, the addition of `ytt` to a simplistic application
    deployment workflow might be good enough for them to bootstrap their Kubernetes
    application automation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经探索了 Kubernetes 应用程序管理的许多方面。这个领域的新工具持续发布，因此这可以被视为您探索如何在生产中扩展和管理大量应用程序的开始。对于许多新来者来说，将
    `ytt` 添加到简单的应用程序部署工作流程中可能已经足够他们启动 Kubernetes 应用程序自动化。
- en: '15.6 Tanzu Community Edition: An end-to-end example of the Carvel toolkit'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.6 Tanzu Community Edition：Carvel工具包的端到端示例
- en: The Tanzu Community Edition (TCE) is a good way to learn about the Cluster API
    and the Image Builder project. TCE uses Carvel heavily for incredibly complex
    cluster configuration profiles and for managing fleets of microservices that need
    to be upgraded and modified by end users. Much of its logical core is built around
    the Carvel family of utilities.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Tanzu Community Edition（TCE）是了解Cluster API和Image Builder项目的好方法。TCE大量使用Carvel来处理极其复杂的集群配置配置文件，以及管理需要由最终用户升级和修改的微服务集群。其逻辑核心的大部分都是围绕Carvel家族工具构建的。
- en: If you’re interested in seeing how `kapp`, `imgpkg`, `ytt`, and the rest of
    the tools in the Carvel stack are used in the wild, check out [https://github.com/vmware-tanzu/tce](https://github.com/vmware-tanzu/tce)
    and [https://github.com/vmware-tanzu/tanzu-framework](https://github.com/vmware-tanzu/tanzu-framework).
    These two repositories comprise the entire VMware Tanzu Kubernetes distribution’s
    open source installation toolkit. In this distribution
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解`kapp`、`imgpkg`、`ytt`以及Carvel堆栈中的其他工具如何在现实世界中使用，请查看[https://github.com/vmware-tanzu/tce](https://github.com/vmware-tanzu/tce)和[https://github.com/vmware-tanzu/tanzu-framework](https://github.com/vmware-tanzu/tanzu-framework)。这两个存储库构成了整个VMware
    Tanzu Kubernetes分布的开放源代码安装工具包。在这个分布中
- en: '`ytt` installs and defines sophisticated cluster templates using the Kubernetes
    Cluster API specification.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ytt`使用Kubernetes Cluster API规范安装和定义复杂的集群模板。'
- en: As an example, `ytt` substitutes Windows cluster specification files (which
    manually install an Antrea agent as a Windows process) for Linux cluster specifications.
    In time, this can be modified to use other Cluster API concepts, but at the time
    of this writing, you can see these examples in action at [http://mng .bz/p2Z0](http://mng.bz/p2Z0).
    `ytt` applies various files in these directories one at a time to create a single,
    massive YAML file, which defines the blueprint for an entire cluster.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，`ytt`用Linux集群规范替换了Windows集群规范文件（这些文件会手动将Antrea代理作为Windows进程安装）。随着时间的推移，这可以修改为使用其他Cluster
    API概念，但截至本文撰写时，你可以在[http://mng.bz/p2Z0](http://mng.bz/p2Z0)上看到这些示例的实际应用。`ytt`将这些目录中的各种文件逐个应用，创建一个单一的、庞大的YAML文件，该文件定义了整个集群的蓝图。
- en: '`kapp` and `kapp-controller` reconcile everything from CNI specifications to
    various add-on applications used in these distributions.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kapp`和`kapp-controller`协调从CNI规范到这些分布中使用的各种附加应用的一切。'
- en: '`imgpkg` and `vendir` (which we didn’t dive into very deeply) are also used
    for various container packaging and release management tasks.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imgpkg`和`vendir`（我们并未深入探讨）也被用于各种容器打包和发布管理任务。'
- en: 'If you are interested in learning more about the Carvel utilities, you can
    join the #carvel channel on Kubernetes Slack (slack.k8s.io). There you’ll find
    a bubbling community of committers that can help onboard you with specific and
    general questions about these utilities.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于Carvel工具的信息，你可以加入Kubernetes Slack上的#carvel频道（slack.k8s.io）。在那里，你将找到一个充满活力的提交者社区，他们可以帮助你解决关于这些工具的特定和一般性问题。
- en: The Antrea LIVE show
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Antrea LIVE节目
- en: As a passing note, a full introduction to various aspects of the Carvel toolkit,
    including how it borrowed some concepts from Antrea, is available on the Antrea
    LIVE show. The episodes for the stream are available at [antrea.io/live](https://antrea.io/live).
    Many of the topics in this book, including Prometheus metrics, CNI providers,
    and so on, have been covered in other broadcasts.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简短的说明，关于Carvel工具包各个方面的全面介绍，包括它如何借鉴了Antrea的一些概念，可以在Antrea LIVE节目中找到。直播的节目可以在[antrea.io/live](https://antrea.io/live)上查看。本书中包括Prometheus指标、CNI提供者等内容，在其他广播中已有涉及。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A simple way to manage apps on Kubernetes is with `kubectl` and a large YAML
    file, but this runs out of steam quickly. Many great tools exist to help you with
    YAML overload. `ytt` is one we covered in this chapter.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl`和大型YAML文件管理Kubernetes上的应用是一种简单的方法，但很快就会力不从心。有许多优秀的工具可以帮助你处理YAML超载。`ytt`是我们在本章中介绍的一个。
- en: The Carvel toolkit has several applications that help us orchestrate Kubernetes
    apps at a high level.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carvel工具包有几个应用，帮助我们以高级别编排Kubernetes应用。
- en: You can cleanly implement customization of YAML files with `ytt` or `kustomize`.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用`ytt`或`kustomize`干净地实现YAML文件的定制。
- en: '`ytt` can match arbitrary parts of a YAML file by adding an `overlay/match`
    clause to an overlay file, which is applied after the original files are read.
    It then builds on top of a simple, pre-existing, standard Kubernetes YAML file.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ytt` 可以通过在覆盖文件中添加一个 `overlay/match` 子句来匹配 YAML 文件的任意部分，该子句在读取原始文件之后应用。然后，它在一个简单、预先存在的标准
    Kubernetes YAML 文件之上构建。'
- en: You can treat collections of disparate YAML resources as a single application
    using tools such as `kapp` or `Helm`.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用像 `kapp` 或 `Helm` 这样的工具将不同 YAML 资源集合视为单个应用程序。
- en: If you want to package your applications in a stateful way, but don’t want to
    build an Operator, you can use a tool like `kapp-controller`. `kapp-controller`
    manages collections of application resources in a stateful manner over time. This
    is one step below building a full-blown Operator, but it can be done in a matter
    of seconds with many of the same benefits.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想以有状态的方式打包应用程序，但又不想构建 Operator，您可以使用像 `kapp-controller` 这样的工具。`kapp-controller`
    会以有状态的方式管理应用程序资源集合，随着时间的推移进行管理。这比构建一个完整的 Operator 简一步，但可以通过几秒钟的时间完成，并且具有许多相同的优势。
- en: Operators can be used to define higher-level APIs in Kubernetes. These APIs
    are aware of your application’s specific life cycle and involve, typically, vendoring
    the Kubernetes client into a container that you run in your cluster.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Operator 可以用来在 Kubernetes 中定义更高级别的 API。这些 API 了解您应用程序的特定生命周期，通常涉及将 Kubernetes
    客户端打包到您在集群中运行的容器中。
- en: Calico and Antrea both implement the Kubernetes Operator pattern for highly
    sophisticated Kubernetes API extensions. This allows you to manage their configuration
    entirely by creating and editing Kubernetes resources.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico 和 Antrea 都实现了 Kubernetes Operator 模式，用于高度复杂的 Kubernetes API 扩展。这使得您可以通过创建和编辑
    Kubernetes 资源来完全管理它们的配置。
- en: The Carvel toolkit and many other topics from this book are covered in various
    episodes of the Antrea live stream on YouTube, which can be viewed at [antrea.io/live](https://antrea.io/live).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carvel 工具包和本书中的许多其他主题在 Antrea 的 YouTube 直播中都有涉及，可以在 [antrea.io/live](https://antrea.io/live)
    上观看。
