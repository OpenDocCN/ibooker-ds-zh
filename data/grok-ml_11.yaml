- en: '11 Finding boundaries with style: Support vector machines and the kernel method'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 以风格寻找边界：支持向量机和核方法
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: what a support vector machine is
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机是什么
- en: which of the linear classifiers for a dataset has the best boundary
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个数据集，哪个线性分类器有最好的边界
- en: using the kernel method to build nonlinear classifiers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用核方法构建非线性分类器
- en: coding support vector machines and the kernel method in Scikit-Learn
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中编码支持向量机和核方法
- en: '![](../Images/11-unnumb-1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-unnumb-1.png)'
- en: Experts recommend the kernel method when attempting to separate chicken datasets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 专家建议在尝试分离鸡数据集时使用核方法。
- en: In this chapter, we discuss a powerful classification model called the support
    vector machine (SVM for short). An SVM is similar to a perceptron, in that it
    separates a dataset with two classes using a linear boundary. However, the SVM
    aims to find the linear boundary that is located as far as possible from the points
    in the dataset. We also cover the kernel method, which is useful when used in
    conjunction with an SVM, and it can help classify datasets using highly nonlinear
    boundaries.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一个强大的分类模型，称为支持向量机（简称SVM）。SVM与感知器类似，因为它使用线性边界将两个类别的数据集分开。然而，SVM的目标是找到尽可能远离数据集中点的线性边界。我们还介绍了核方法，当与SVM结合使用时非常有用，并且可以帮助使用高度非线性边界来分类数据集。
- en: 'In chapter 5, we learned about linear classifiers, or perceptrons. With two-dimensional
    data, these are defined by a line that separates a dataset consisting of points
    with two labels. However, we may have noticed that many different lines can separate
    a dataset, and this raises the following question: how do we know which is the
    best line? In figure 11.1, we can see three different linear classifiers that
    separate this dataset. Which one do you prefer, classifier 1, 2, or 3?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章，我们学习了线性分类器，或感知器。对于二维数据，这些由一条线定义，该线将具有两个标签的点组成的数据集分开。然而，我们可能已经注意到，许多不同的线可以分离一个数据集，这引发以下问题：我们如何知道哪条是最好的线？在图11.1中，我们可以看到三种不同的线性分类器，它们将这个数据集分开。你更喜欢分类器1、2还是3？
- en: '![](../Images/11-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-1.png)'
- en: Figure 11.1 Three classifiers that classify our data set correctly. Which should
    we prefer, classifier 1, 2, or 3?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 三种正确分类我们的数据集的分类器。我们应该选择分类器1、2还是3？
- en: If you said classifier 2, we agree. All three lines separate the dataset well,
    but the second line is better placed. The first and third lines are very close
    to some of the points, whereas the second line is far from all the points. If
    we were to wiggle the three lines around a little bit, the first and the third
    may go over some of the points, misclassifying some of them in the process, whereas
    the second one will still classify them all correctly. Thus, classifier 2 is more
    robust than classifiers 1 and 3.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你说是分类器2，我们同意。这三条线都很好地分离了数据集，但第二条线放置得更好。第一条和第三条线与一些点非常接近，而第二条线与所有点都保持距离。如果我们稍微调整这三条线，第一条和第三条可能会越过一些点，在这个过程中错误地分类一些点，而第二条线仍然会正确地分类所有点。因此，分类器2比分类器1和3更稳健。
- en: This is where support vector machines come into play. An SVM classifier uses
    two parallel lines instead of one line. The goal of the SVM is twofold; it tries
    to classify the data correctly and also tries to space the lines as much as possible.
    In figure 11.2, we can see the two parallel lines for the three classifiers, together
    with their middle line for reference. The two external (dotted) lines in classifier
    2 are the farthest from each other, which makes this classifier the best one.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是支持向量机发挥作用的地方。SVM分类器使用两条平行线而不是一条线。SVM的目标有两个；它试图正确地分类数据，并且试图尽可能地将线分开。在图11.2中，我们可以看到三个分类器的两条平行线，以及它们的中间线作为参考。分类器2中的两条外部（虚线）线彼此距离最远，这使得这个分类器成为最好的一个。
- en: '![](../Images/11-2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-2.png)'
- en: Figure 11.2 We draw our classifier as two parallel lines, as far apart from
    each other as possible. We can see that classifier 2 is the one where the parallel
    lines are the farthest away from each other. This means that the middle line in
    classifier 2 is the one best located between the points.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 我们将我们的分类器绘制为两条尽可能远的平行线。我们可以看到分类器2是两条平行线之间距离最远的那一个。这意味着分类器2中的中间线是位于这些点之间位置最好的那一条。
- en: We may want to visualize an SVM as the line in the middle that tries to stay
    as far as possible from the points. We can also imagine it as the two external
    parallel lines trying to stay as far away from each other as possible. In this
    chapter, we’ll use both visualizations at different times, because each of them
    is useful in certain situations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要将支持向量机（SVM）可视化为中心的那条线，它试图尽可能远离这些点。我们也可以想象成两条外部平行的线，试图尽可能远离彼此。在本章中，我们将根据不同情况在不同时间使用这两种可视化方法，因为每种方法在特定情况下都很有用。
- en: How do we build such a classifier? We can do this in a similar way as before,
    with a slightly different error function and a slightly different iterative step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何构建这样的分类器？我们可以用与之前类似的方式来做，只是使用一个稍微不同的误差函数和稍微不同的迭代步骤。
- en: 'note In this chapter, all the classifiers are discrete, namely, their output
    is 0 or 1\. Sometimes they are described by their prediction *ŷ* = *step*(*f*(*x*)),
    and other times by their boundary equation *f*(*x*) = 0, namely, the graph of
    the function that attempts to separate our data points into two classes. For example,
    the perceptron that makes the prediction *ŷ* = *step*(3*x*[1] + 4*x*[2] – 1) sometimes
    is described only by the linear equation 3*x*[1] + 4*x*[2] – 1 = 0\. For some
    classifiers in this chapter, especially those in the section “Training SVMs with
    nonlinear boundaries: The kernel method,” the boundary equation will not necessarily
    be a linear function.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，所有分类器都是离散的，即它们的输出是0或1。有时它们通过预测 *ŷ* = *step*(*f*(*x*)) 来描述，有时通过边界方程 *f*(*x*)
    = 0 来描述，即尝试将我们的数据点分为两个类别的函数的图像。例如，预测 *ŷ* = *step*(3*x*[1] + 4*x*[2] – 1) 的感知器有时仅通过线性方程
    3*x*[1] + 4*x*[2] – 1 = 0 来描述。在本章的一些分类器中，尤其是在“使用非线性边界训练SVM：核方法”这一节中，边界方程不一定是一个线性函数。
- en: In this chapter, we see this theory mostly on datasets of one and two dimensions
    (points on a line or on the plane). However, support vector machines work equally
    well in datasets of higher dimensions. The linear boundaries in one dimension
    are points and in two dimensions are lines. Likewise, the linear boundaries in
    three dimensions are planes, and in higher dimensions, they are hyperplanes of
    one dimension less than the space in which the points live. In each of these cases,
    we try to find the boundary that is the farthest from the points. In figure 11.3,
    you can see examples of boundaries for one, two, and three dimensions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要在单维和二维数据集（线上的点或平面上的点）上看到这种理论。然而，支持向量机在更高维度的数据集上同样表现良好。一维中的线性边界是点，二维中的线性边界是线。同样，三维中的线性边界是平面，而在更高维度中，它们是比点所在空间低一维的超平面。在这些所有情况下，我们都试图找到离点最远的边界。在图11.3中，你可以看到一维、二维和三维边界的示例。
- en: '![](../Images/11-3.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-3.png)'
- en: Figure 11.3 Linear boundaries for datasets in one, two, and three dimensions.
    In one dimension, the boundary is formed by two points, in two dimensions by two
    lines, and in three dimensions by two planes. In each of the cases, we try to
    separate these two as much as possible. The middle boundary (point, line, or plane)
    is illustrated for clarity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3展示了单维、二维和三维数据集的线性边界。在一维中，边界由两个点形成，在二维中由两条线形成，在三维中由两个平面形成。在每种情况下，我们都试图尽可能地将这两者分开。中间的边界（点、线或平面）为了清晰起见进行了说明。
- en: 'All the code for this chapter is in this GitHub repository: [https://github.com/luisguiserrano/manning/tree/master/Chapter_11_Support_Vector_Machines](https://github.com/luisguiserrano/manning/tree/master/Chapter_11_Support_Vector_Machines).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码都存放在这个GitHub仓库中：[https://github.com/luisguiserrano/manning/tree/master/Chapter_11_Support_Vector_Machines](https://github.com/luisguiserrano/manning/tree/master/Chapter_11_Support_Vector_Machines)。
- en: Using a new error function to build better classifiers
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用新的误差函数构建更好的分类器
- en: 'As is common in machine learning models, SVMs are defined using an error function.
    In this section, we see the error function of SVMs, which is very special, because
    it tries to maximize two things at the same time: the classification of the points
    and the distance between the lines.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如同机器学习模型中常见的那样，SVM是通过一个误差函数来定义的。在本节中，我们将看到SVM的误差函数，它非常特殊，因为它试图同时最大化两件事：点的分类和线的距离。
- en: 'To train an SVM, we need to build an error function for a classifier consisting
    of two lines, spaced as far apart as possible. When we think of building an error
    function, we should always ask ourselves: “What do we want the model to achieve?”
    The following are the two things we want to achieve:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个SVM，我们需要为两个尽可能分开的线构建一个分类器的错误函数。当我们思考构建错误函数时，我们应该始终问自己：“我们希望模型实现什么？”以下是我们想要实现的两件事：
- en: Each of the two lines should classify the points as best as possible.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条线都应该尽可能好地分类点。
- en: The two lines should be as far away from each other as possible.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两条线应该尽可能远。
- en: 'The error function should penalize any model that doesn’t achieve these things.
    Because we want two things, our SVM error function should be the sum of two error
    functions: the first one penalizes points that are misclassified, and the second
    one penalizes lines that are too close to each other. Therefore, our error function
    can look like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 错误函数应该惩罚任何未能实现这些目标的模型。因为我们想要两件事，所以我们的SVM错误函数应该是两个错误函数的和：第一个惩罚被错误分类的点，第二个惩罚彼此太近的线。因此，我们的错误函数可以看起来像这样：
- en: Error = Classification Error + Distance Error
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 错误 = 分类错误 + 距离错误
- en: In the next two sections, we develop each one of these two terms separately.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，我们将分别开发这两个术语中的每一个。
- en: 'Classification error function: Trying to classify the points correctly'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分类错误函数：尝试正确分类点
- en: In this section, we learn the classification error function. This is the part
    of the error function that pushes the classifier to correctly classify the points.
    In short, this error is calculated as follows. Because the classifier is formed
    by two lines, we think of them as two separate discrete perceptrons (chapter 5).
    We then calculate the total error of this classifier as the sum of the two perceptron
    errors (section “How to compare classifiers? The error function” in chapter 5).
    Let’s take a look at an example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习分类错误函数。这是推动分类器正确分类点的错误函数的一部分。简而言之，这个错误是这样计算的。因为分类器由两条线组成，我们把它们看作是两个独立的离散感知器（第5章）。然后我们计算这个分类器的总错误，即两个感知器错误的和（第5章“如何比较分类器？错误函数”部分）。让我们看一个例子。
- en: 'The SVM uses two parallel lines, and luckily, parallel lines have similar equations;
    they have the same weights but a different bias. Thus, in our SVM, we use the
    central line as a frame of reference L with equation *w*[1]*x*[1] + *w*[2]*x*[2]
    + *b* = 0, and construct two lines, one above it and one below it, with the respective
    equations:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SVM使用两条平行线，幸运的是，平行线有相似的方程；它们有相同的权重但不同的偏置。因此，在我们的SVM中，我们使用中心线作为参考框架L，其方程为*w*[1]*x*[1]
    + *w*[2]*x*[2] + *b* = 0，并构建两条线，一条在上面，一条在下面，相应的方程为：
- en: 'L+: *w*[1]*x*[1] + *w*[2]*x*[2] + *b* =  1, and'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L+：*w*[1]*x*[1] + *w*[2]*x*[2] + *b* = 1，并且
- en: 'L–: *w*[1]*x*[1] + *w*[2]*x*[2] + *b* = –1'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L–：*w*[1]*x*[1] + *w*[2]*x*[2] + *b* = –1
- en: 'As an example, figure 11.4 shows the three parallel lines, L, L+, and L–, with
    the following equations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图11.4显示了三条平行线L、L+和L–，它们的方程如下：
- en: 'L:   2*x*[1] + 3*x*[2] – 6 =  0'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L：2*x*[1] + 3*x*[2] – 6 = 0
- en: 'L+: 2*x*[1] + 3*x*[2] – 6 =  1'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L+：2*x*[1] + 3*x*[2] – 6 = 1
- en: 'L–: 2*x*[1] + 3*x*[2] – 6 = –1'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L–：2*x*[1] + 3*x*[2] – 6 = –1
- en: '![](../Images/11-4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-4.png)'
- en: Figure 11.4 Our main line L is the one in the middle. We build the two parallel
    equidistant lines L+ and L– by slightly changing the equation of L.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 我们的主要线L是中间的那条。我们通过稍微改变L的方程来构建两个平行且等距的线L+和L–。
- en: Our classifier now consists of the lines L+ and L–. We can think of L+ and L–
    as two independent perceptron classifiers, and each of them has the same goal
    of classifying the points correctly. Each classifier comes with its own perceptron
    error function, so the classification function is defined as the sum of these
    two error functions, as illustrated in figure 11.5.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的分类器由线L+和L–组成。我们可以把L+和L–看作是两个独立的感知器分类器，它们各自的目标都是正确分类点。每个分类器都有自己的感知器错误函数，因此分类函数定义为这两个错误函数的和，如图11.5所示。
- en: '![](../Images/11-5.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-5.png)'
- en: Figure 11.5 Now that our classifier consists of two lines, the error of a misclassified
    point is measured with respect to both lines. We then add the two errors to obtain
    the classification error. Note that the error is not the length of the perpendicular
    segment to the boundary, as illustrated, but it is proportional to it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 现在我们分类器由两条线组成，错误分类点的误差是根据这两条线来衡量的。然后我们将两个误差相加以获得分类误差。请注意，误差不是如图所示垂直线段的长度，而是与它成比例。
- en: Notice that in an SVM, *both* lines have to classify the points well. Therefore,
    a point that is between the two lines is always misclassified by one of the lines,
    so it does not count as a correctly classified point by the SVM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在支持向量机（SVM）中，**两条**线都必须很好地对点进行分类。因此，位于两条线之间的点总是被其中一条线错误分类，所以它不被SVM视为正确分类的点。
- en: 'Recall from the section “How to compare classifiers? The error function” in
    chapter 5 that the error function for the discrete perceptron with prediction
    *ŷ* = *step*(*w*[1]*x*[1] + *w*[2]*x*[2] + *b*) at the point (*p*, *q*) is given
    by the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆第5章“如何比较分类器？误差函数”部分的内容，离散感知器的误差函数，在点(*p*, *q*)处的预测 *ŷ* = *step*(*w*[1]*x*[1]
    + *w*[2]*x*[2] + *b*)，由以下给出：
- en: 0 if the point is correctly classified, and
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 如果点被正确分类，并且
- en: '|*w*[1]*x*[1] + *w*[2]*x*[2] + *b*| if the point is incorrectly classified'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|*w*[1]*x*[1] + *w*[2]*x*[2] + *b*| 如果点被错误分类'
- en: 'As an example, consider the point (4,3) with a label of 0\. This point is incorrectly
    classified by both of the perceptrons in figure 11.5\. Note that the two perceptrons
    give the following predictions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑标签为0的点(4,3)。这个点在图11.5中的两个感知器都被错误分类。请注意，两个感知器给出了以下预测：
- en: 'L+: *ŷ* = *step*(2*x*[1] + 3*x*[2] – 7)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L+: *ŷ* = *step*(2*x*[1] + 3*x*[2] – 7)'
- en: 'L–: *ŷ* = *step*(2*x*[1] + 3*x*[2] – 5)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L–: *ŷ* = *step*(2*x*[1] + 3*x*[2] – 5)'
- en: Therefore, its classification error with respect to this SVM is
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它相对于这个SVM的分类误差是
- en: '|2 · 4 + 3 · 3 – 7| + |2 · 4 + 3 · 3 – 5| = 10 + 12 = 22.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|2 · 4 + 3 · 3 – 7| + |2 · 4 + 3 · 3 – 5| = 10 + 12 = 22。'
- en: 'Distance error function: Trying to separate our two lines as far apart as possible'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 距离误差函数：试图尽可能地将我们的两条线分开
- en: Now that we have created an error function that measures classification errors,
    we need to build one that looks at the distance between the two lines and raises
    an alarm if this distance is small. In this section, we discuss a surprisingly
    simple error function that is large when the two lines are close and small when
    they are far.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个衡量分类误差的误差函数，我们需要构建一个检查两条线之间距离的误差函数，如果这个距离很小，则发出警报。在本节中，我们讨论了一个出人意料简单的误差函数，当两条线靠近时它很大，当它们远离时它很小。
- en: 'This error function is called the distance error function, and we’ve already
    seen it before; it is the regularization term we learned in the section “Modifying
    the error function to solve our problem” in chapter 4\. More specifically, if
    our lines have equations *w*[1]*x*[1] + *w*[2]*x*[2] + *b* = 1 and *w*[1]*x*[1]
    + *w*[2]*x*[2] + *b* = –1, then the error function is *w*[1]² + *w*[2]². Why?
    We’ll make use of the following fact: the perpendicular distance between the two
    lines is precisely ![](../Images/11_05_E01.png), as illustrated in figure 11.6\.
    If you’d like to work out the details of this distance calculation, please check
    exercise 11.1 at the end of this chapter.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个误差函数被称为距离误差函数，我们之前已经见过；它是我们在第4章“修改误差函数以解决问题”部分学到的正则化项。更具体地说，如果我们的线的方程是 *w*[1]*x*[1]
    + *w*[2]*x*[2] + *b* = 1 和 *w*[1]*x*[1] + *w*[2]*x*[2] + *b* = –1，那么误差函数就是 *w*[1]²
    + *w*[2]²。为什么？我们将利用以下事实：两条线之间的垂直距离恰好是 ![](../Images/11_05_E01.png)，如图11.6所示。如果你想要计算出这个距离计算的细节，请查看本章末尾的练习11.1。
- en: '![](../Images/11-6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-6.png)'
- en: Figure 11.6 The distance between the two parallel lines can be calculated based
    on the equations of the lines.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 两条平行线之间的距离可以根据线的方程计算。
- en: 'Knowing this, notice the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一点后，请注意以下内容：
- en: When *w*[1] ² + *w*[2]² is large, ![](../Images/11_06_E01.png) is small.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *w*[1]² + *w*[2]² 较大时，![](../Images/11_06_E01.png) 较小。
- en: When *w*[1]² + *w*[2]² is small, ![](../Images/11_06_E01.png) is large.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *w*[1]² + *w*[2]² 较小时，![](../Images/11_06_E01.png) 较大。
- en: Because we want the lines to be as far apart as possible, this term *w*[1]²
    + *w*[2]² is a good error function, as it gives us large values for the bad classifiers
    (those where the lines are close) and small values for the good classifiers (those
    where the lines are far).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望线尽可能远，这个项 *w*[1]² + *w*[2]² 是一个好的错误函数，因为它为不良分类器（那些线很近的分类器）提供了大值，而对于良好分类器（那些线很远的分类器）则提供小值。
- en: 'In figure 11.7, we can see two examples of SVM classifiers. Their equations
    follow:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 11.7 中，我们可以看到两个 SVM 分类器的示例。它们的方程如下：
- en: 'SVM 1:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SVM 1:'
- en: 'L+: 3*x*[1] + 4*x*[2] + 5 = 1'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L+: 3*x*[1] + 4*x*[2] + 5 = 1'
- en: 'L–: 3*x*[1] + 4*x*[2] + 5 = –1'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L–: 3*x*[1] + 4*x*[2] + 5 = –1'
- en: 'SVM 2:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SVM 2:'
- en: 'L+: 30*x*[1] + 40*x*[2] + 50 = 1'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L+: 30*x*[1] + 40*x*[2] + 50 = 1'
- en: 'L–: 30*x*[1] + 40*x*[2] + 50 = 1'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L–: 30*x*[1] + 40*x*[2] + 50 = 1'
- en: 'Their distance error functions are shown next:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的距离错误函数如下所示：
- en: 'SVM 1:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SVM 1:'
- en: Distance error function = 3² + 4² = 25
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离错误函数 = 3² + 4² = 25
- en: 'SVM 2:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SVM 2:'
- en: Distance error function = 30² + 40² = = 2500
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离错误函数 = 30² + 40² = 2500
- en: Notice also from figure 11.7 that the lines are much closer in SVM 2 than in
    SVM 1, which makes SVM 1 a much better classifier (from the distance perspective).
    The distance between the lines in SVM 1 is ![](../Images/11_06_E02.png), whereas
    in SVM 2 it is ![](../Images/11_06_E03.png).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从图 11.7 中还可以看出，SVM 2 中的线比 SVM 1 中的线要近得多，这使得 SVM 1 成为一个更好的分类器（从距离的角度来看）。SVM
    1 中线的距离为 ![](../Images/11_06_E02.png)，而在 SVM 2 中为 ![](../Images/11_06_E03.png)。
- en: '![](../Images/11-7.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-7.png)'
- en: 'Figure 11.7 Left: An SVM where the lines are at distance 0.4 apart, with an
    error of 25\. Right: An SVM where the lines are at distance 0.04 apart, with an
    error of 2500\. Notice that in this comparison, the classifier on the left is
    much better than the one on the right, because the lines are farther apart from
    each other. This results in a smaller distance error.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 左：一个 SVM，其中线之间的距离为 0.4，错误为 25。右：一个 SVM，其中线之间的距离为 0.04，错误为 2500。注意，在这个比较中，左边的分类器比右边的分类器要好得多，因为线彼此之间的距离更远。这导致距离错误更小。
- en: Adding the two error functions to obtain the error function
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个错误函数相加以获得错误函数
- en: 'Now that we’ve built a classification error function and a distance error function,
    let’s see how to combine them to build an error function that helps us make sure
    that we have achieved both goals: classify our points well and with two lines
    that are far apart from each other.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了一个分类错误函数和一个距离错误函数，让我们看看如何将它们结合起来构建一个错误函数，帮助我们确保实现了两个目标：将点很好地分类，并且两条线彼此间隔很远。
- en: 'To obtain this error function, we add the classification error function and
    the distance error function and get the following formula:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这个错误函数，我们将分类错误函数和距离错误函数相加，得到以下公式：
- en: Error = Classification Error + Distance Error
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 错误 = 分类错误 + 距离错误
- en: A good SVM that minimizes this error function must then try to make as few classification
    errors as possible, while simultaneously trying to keep the lines as far apart
    as possible.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的 SVM 必须尝试尽可能少地犯分类错误，同时尝试使线尽可能远。
- en: '![](../Images/11-8.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-8.png)'
- en: 'Figure 11.8 Left: A good SVM consisting of two well-spaced lines that classifies
    all the points correctly. Middle: A bad SVM that misclassifies two points. Right:
    A bad SVM that consists of two lines that are too close together.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 左：一个良好的 SVM，由两条间隔良好的线组成，能够正确分类所有点。中：一个不良的 SVM，错误分类了两个点。右：一个不良的 SVM，由两条太靠近的线组成。
- en: In figure 11.8, we can see three SVM classifiers for the same dataset. The one
    on the left is a good classifier, because it classifies the data well and the
    lines are far apart, reducing the likelihood of errors. The one in the middle
    makes some errors (because there is a triangle underneath the top line and a square
    over the bottom line), so it is not a good classifier. The one on the right classifies
    the points correctly, but the lines are too close together, so it is also not
    a good classifier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 11.8 中，我们可以看到同一数据集的三个 SVM 分类器。左边的分类器是一个好的分类器，因为它很好地分类了数据，并且线之间间隔很远，减少了错误的可能性。中间的那个分类器犯了一些错误（因为顶部线下方有一个三角形，底部线上方有一个正方形），所以它不是一个好的分类器。右边的分类器正确地分类了点，但线太靠近了，所以它也不是一个好的分类器。
- en: Do we want our SVM to focus more on classification or distance? The C parameter
    can help us
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的 SVM 更关注分类还是距离？C 参数可以帮助我们
- en: In this section, we learn a useful technique to tune and improve our model,
    which involves introducing the C parameter. The C parameter is used in cases where
    we want to train an SVM that pays more attention to classification than to distance
    (or the other way around).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了一种有用的技术来调整和改进我们的模型，这涉及到引入C参数。当我们要训练一个比距离（或相反）更关注分类的SVM时，使用C参数。
- en: So far it seems that all we have to do to build a good SVM classifier is to
    keep track of two things. We want to make sure the classifier makes as few errors
    as possible, while keeping the lines as far apart as possible. But what if we
    have to sacrifice one for the benefit of the other? In figure 11.9, we have two
    classifiers for the same dataset. The one on the left makes some errors, but the
    lines are far apart. The one on the right makes no errors, but the lines are too
    close together. Which one should we prefer?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，似乎我们构建一个好的SVM分类器所要做的只是跟踪两件事。我们想确保分类器尽可能少犯错误，同时保持线尽可能分开。但如果我们不得不为了对方的利益而牺牲一方怎么办？在图11.9中，我们对同一个数据集有两个分类器。左边的一个犯了一些错误，但线分得很开。右边的一个没有错误，但线太靠近了。我们应该选择哪一个？
- en: '![](../Images/11-9.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-9.png)'
- en: Figure 11.9 Both of these classifiers have one pro and one con. The one on the
    left consists of well-spaced lines (pro), but it misclassifies some points (con).
    The one on the right consists of lines that are too close together (con), but
    it classifies all the points correctly (pro).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9这两个分类器各有优点和缺点。左边的一个由间隔良好的线组成（优点），但它错误地分类了一些点（缺点）。右边的一个由太靠近的线组成（缺点），但它正确地分类了所有点（优点）。
- en: 'It turns out that the answer for this depends on the problem we are solving.
    Sometimes we want a classifier that makes as few errors as possible, even if the
    lines are too close, and sometimes we want a classifier that keeps the lines apart,
    even if it makes a few errors. How do we control this? We use a parameter which
    we call the C parameter. We slightly modify the error formula by multiplying the
    classification error by C, to get the following formula:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个答案取决于我们正在解决的问题。有时我们希望有一个尽可能少犯错误的分类器，即使线很近，有时我们希望有一个即使犯一些错误也能保持线分开的分类器。我们如何控制这一点？我们使用一个我们称之为C参数的参数。我们通过将分类错误乘以C来稍微修改错误公式，得到以下公式：
- en: Error formula = C · (Classification Error) + (Distance Error)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 错误公式 = C · (分类错误) + (距离错误)
- en: If C is large, then the error formula is dominated by the classification error,
    so our classifier focuses more on classifying the points correctly. If C is small,
    then the formula is dominated by the distance error, so our classifier focuses
    more on keeping the lines far apart.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果C很大，那么错误公式主要由分类错误决定，因此我们的分类器更关注正确分类点。如果C很小，那么公式主要由距离错误决定，因此我们的分类器更关注保持线分开。
- en: '![](../Images/11-10.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-10.png)'
- en: Figure 11.10 Different values of C toggle between a classifier with well-spaced
    lines and one that classifies points correctly. The classifier on the left has
    a small value of C (0.01), and the lines are well spaced, but it makes mistakes.
    The classifier on the right has a large value of C (100), and it classifies points
    correctly, but the lines are too close together. The classifier in the middle
    makes one mistake but finds two lines that are well spaced apart.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10中不同的C值在具有良好间隔线的分类器和正确分类点的分类器之间切换。左边的分类器C值较小（0.01），线间隔良好，但犯了错误。右边的分类器C值较大（100），正确分类了点，但线太靠近了。中间的分类器犯了一个错误，但找到了两条间隔良好的线。
- en: 'In figure 11.10, we can see three classifiers: one with a large value of C
    that classifies all points correctly, one with a small value of C that keeps the
    lines far apart, and one with C = 1, which tries to do both. In real life, C is
    a hyperparameter that we can tune using methods such as the model complexity graph
    (section “A numerical way to decide how complex our model should be” in chapter
    4) or our own knowledge of the problem we’re solving, the data, and the model.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图11.10中，我们可以看到三个分类器：一个C值大的分类器可以正确分类所有点，一个C值小的分类器可以保持线间隔良好，还有一个C = 1的分类器试图两者兼顾。在现实生活中，C是一个超参数，我们可以通过使用模型复杂度图（第4章中“决定我们的模型应该有多复杂的一种数值方法”部分）或我们对所解决问题的了解、数据和对模型的知识来调整C。
- en: Coding support vector machines in Scikit-Learn
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中编码支持向量机
- en: Now that we’ve learned what an SVM is, we are ready to code one and use it to
    model some data. In Scikit-Learn, coding an SVM is simple and that’s what we learn
    in this section. We also learn how to use the C parameter in our code.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了 SVM 是什么，我们准备编写一个 SVM 并用它来建模一些数据。在 Scikit-Learn 中，编写 SVM 简单，这就是我们在本节中学到的。我们还学习了如何在代码中使用
    C 参数。
- en: Coding a simple SVM
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 编写简单的 SVM
- en: 'We start by coding a simple SVM in a sample dataset and then we’ll add more
    parameters. The dataset is called linear.csv, and its plot is shown in figure
    11.11\. The code for this section follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在样本数据集中编写一个简单的 SVM，然后我们将添加更多参数。数据集称为 linear.csv，其图表显示在图 11.11 中。本节的代码如下：
- en: '**Noteboo****k**: SVM_graphical_example.ipynb'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记**：SVM_graphical_example.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/SVM_graphical_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/SVM_graphical_example.ipynb)'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/SVM_graphical_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/SVM_graphical_example.ipynb)'
- en: '**Dataset**: linear.csv'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：linear.csv'
- en: 'We first import from the `svm` package in Scikit-Learn and load our data as
    follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从 Scikit-Learn 的 `svm` 包中导入，并按以下方式加载数据：
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/11-111.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-111.png)'
- en: Figure 11.11 An almost linearly separable dataset, with some outliers
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 几乎线性可分的数据集，有一些异常值
- en: Then, as shown in the next code snippet, we load our data into two Pandas DataFrames
    called `features` and `labels`, and then we define our model called `svm_linear`
    and train it. The accuracy we obtain is 0.933, and the plot is shown in figure
    11.12.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如以下代码片段所示，我们将数据加载到两个 Pandas DataFrame 中，分别命名为 `features` 和 `labels`，然后定义我们的模型
    `svm_linear` 并对其进行训练。我们获得的准确率为 0.933，图表显示在图 11.12 中。
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/11-12.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-12.png)'
- en: Figure 11.12 The plot of the SVM classifier we’ve built in Scikit-Learn consists
    of a line. The accuracy of this model is 0.933.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 我们在 Scikit-Learn 中构建的 SVM 分类器的图表由一条线组成。该模型的准确率为 0.933。
- en: The C parameter
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: C 参数
- en: 'In Scikit-Learn, we can easily introduce the C parameter into the model. Here
    we train and plot two models, one with a very small value of 0.01, and another
    one with a large value of 100, which is shown in the following code and in figure
    11.13:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中，我们可以轻松地将 C 参数引入模型。这里我们训练并绘制了两个模型，一个具有非常小的值 0.01，另一个具有较大的值 100，如下代码和图
    11.13 所示：
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/11-13.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-13.png)'
- en: Figure 11.13 The classifier on the left has a small value of C, and it spaced
    the line well between the points, but it makes some mistakes. The classifier on
    the right has a large value of C, and it makes no mistakes, although the line
    passes too close to some of the points.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 左侧的分类器 C 值较小，很好地分隔了点之间的线，但犯了一些错误。右侧的分类器 C 值较大，没有犯错误，尽管线过于接近一些点。
- en: We can see that the model with a small value of C doesn’t put that much emphasis
    on classifying the points correctly, and it makes some mistakes, as is evident
    in its low accuracy (0.867). It is hard to tell in this example, but this classifier
    puts a lot of emphasis on the line being as far away from the points as possible.
    In contrast, the classifier with the large value of C tries to classify all the
    points correctly, which reflects on its higher accuracy.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，C 值较小的模型并不太强调正确分类点，它犯了一些错误，这在它的低准确率（0.867）中很明显。在这个例子中很难看出，但这个分类器非常强调线尽可能远离点。相比之下，C
    值较大的分类器试图正确分类所有点，这反映在其更高的准确率上。
- en: 'Training SVMs with nonlinear boundaries: The kernel method'
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用非线性边界的 SVM 训练：核方法
- en: As we’ve seen in other chapters of this book, not every dataset is linearly
    separable, and many times we need to build nonlinear classifiers to capture the
    complexity of the data. In this section, we study a powerful method associated
    with SVMs called the *kernel method*, which helps us build highly nonlinear classifiers.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书的其他章节中看到的，并非每个数据集都是线性可分的，很多时候我们需要构建非线性分类器来捕捉数据的复杂性。在本节中，我们研究了一种与 SVM
    相关的强大方法，称为 *核方法*，它帮助我们构建高度非线性分类器。
- en: If we have a dataset and find that we can’t separate it with a linear classifier,
    what can we do? One idea is to add more columns to this dataset and hope that
    the richer dataset is linearly separable. The kernel method consists of adding
    more columns in a clever way, building a linear classifier on this new dataset
    and later removing the columns we added while keeping track of the (now nonlinear)
    classifier.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个数据集，发现我们无法用线性分类器将其分离，我们该怎么办？一个想法是向这个数据集添加更多列，并希望更丰富的数据集是线性可分的。核方法包括以巧妙的方式添加更多列，在这个新数据集上构建一个线性分类器，然后在保持对（现在是非线性的）分类器的跟踪的同时移除我们添加的列。
- en: That was quite a mouthful, but we have a nice geometric way to see this method.
    Imagine that the dataset is in two dimensions, which means that the input has
    two columns. If we add a third column, the dataset is now three-dimensional, like
    if the points on your paper all of a sudden start flying into space at different
    heights. Maybe if we raise the points at different heights in a clever way, we
    can separate them with a plane. This is the kernel method, and it is illustrated
    in figure 11.14.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来可能有些复杂，但我们有一个很好的几何方法来观察这种方法。想象一下，数据集在二维空间中，这意味着输入有两个列。如果我们添加一个第三列，数据集现在就是三维的，就像你手中的点突然以不同的高度飞入空间一样。也许如果我们以巧妙的方式提高这些点的高度，我们可以用平面将它们分开。这就是核方法，它在图11.14中得到了说明。
- en: '![](../Images/11-14.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/11-14.png)'
- en: 'Figure 11.14 Left: The set is not separable by a line. Middle: We look at it
    in three dimensions, and proceed to raise the two triangles and lower the two
    squares. Right: Our new dataset is now separable by a plane. (Source: Image created
    with the assistance of Grapher™ from Golden Software, LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 左：这个集合不能用一条线分开。中：我们在三维空间中观察它，然后提高两个三角形并降低两个正方形。右：我们新的数据集现在可以用一个平面分开。（来源：使用Golden
    Software, LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher))。
- en: Kernels, feature maps, and operator theory
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 核、特征图和算子理论
- en: The theory behind the kernel method comes from a field in mathematics called
    *operator theory*. A kernel is a similarity function, which, in short, is a function
    that tells us if two points are similar or different (e.g., close or far). A kernel
    can give rise to a *feature map*, which is a map between the space where our dataset
    lives and a (usually) higher-dimensional space.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 核方法背后的理论来自数学中的一个领域，称为*算子理论*。核是一个相似性函数，简而言之，是一个告诉我们两个点是否相似或不同的函数（例如，接近或远离）。核可以产生一个*特征图*，这是我们的数据集所在空间与（通常是）更高维空间之间的映射。
- en: The full theory of kernels and feature maps is not needed to understand the
    classifiers. If you’d like to delve into these more, see the resources in appendix
    C. For the purpose of this chapter, we look at the kernel method as a way of adding
    columns to our dataset to make the points separable. For example, the dataset
    in figure 11.14 has two columns, *x*[1] and *x*[2], and we have added the third
    column with the value *x*[1]*x*[2]. Equivalently, it can also be seen as the function
    that sends the point (*x*[1], *x*[2]) in the plane to the point (*x*[1], *x*[2],
    *x*[1]*x*[2]) in space. Once the points belong in 3-D space, we can separate them
    using the plane seen on the right of figure 11.14\. To study this example more
    in detail, see exercise 11.2 at the end of the chapter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要完整的核和特征图理论来理解分类器。如果你想要深入了解这些内容，请参阅附录C中的资源。为了本章的目的，我们将核方法视为向我们的数据集添加列以使点可分的一种方式。例如，图11.14中的数据集有两个列，*x*[1]和*x*[2]，我们添加了第三个列，其值为*x*[1]*x*[2]。等价地，它也可以看作是将平面上的点(*x*[1],
    *x*[2])映射到空间中的点(*x*[1], *x*[2], *x*[1]*x*[2])的函数。一旦点属于3-D空间，我们就可以使用图11.14右边的平面来分离它们。要更详细地研究这个例子，请参阅本章末尾的练习11.2。
- en: The two kernels and their corresponding features maps we see in this chapter
    are the *polynomial kernel* and the *radial basis function* (RBF) *kernel*. Both
    of them consist of adding columns to our dataset in different, yet very effective,
    ways.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们看到的两个核及其相应的特征图是*多项式核*和*径向基函数*（RBF）核。它们都以不同但非常有效的方式向我们的数据集添加列。
- en: 'Using polynomial equations to our benefit: The polynomial kernel'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 利用多项式方程为我们带来好处：多项式核
- en: In this section, we discuss the polynomial kernel, a useful kernel that will
    help us model nonlinear datasets. More specifically, the kernel method helps us
    model data using polynomial equations such as circles, parabolas, and hyperbolas.
    We’ll illustrate the polynomial kernel with two examples.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论多项式核，这是一种有用的核，可以帮助我们建模非线性数据集。更具体地说，核方法帮助我们使用如圆、抛物线和双曲线等多项式方程来建模数据。我们将通过两个示例来说明多项式核。
- en: 'Example 1: A circular dataset'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1：一个圆形数据集
- en: For our first example, let’s try to classify the dataset in table 11.1.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个示例，让我们尝试对表 11.1 中的数据集进行分类。
- en: Table 11.1 A small dataset, depicted in figure 11.15
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.1 一个小数据集，如图 11.15 所示
- en: '| *x*[1] | *x*[2] | *y*   |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *y*   |'
- en: '| 0.3 | 0.3 | 0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 0.3 | 0 |'
- en: '| 0.2 | 0.8 | 0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 0.2 | 0.8 | 0 | '
- en: '| –0.6 | 0.4 | 0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| –0.6 | 0.4 | 0 |'
- en: '| 0.6 | –0.4 | 0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | –0.4 | 0 |'
- en: '| –0.4 | –0.3 | 0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| –0.4 | –0.3 | 0 |'
- en: '| 0 | –0.8 | 0 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 0 | –0.8 | 0 |'
- en: '| –0.4 | 1.2 | 1 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| –0.4 | 1.2 | 1 |'
- en: '| 0.9 | –0.7 | 1 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | –0.7 | 1 |'
- en: '| –1.1 | –0.8 | 1 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| –1.1 | –0.8 | 1 |'
- en: '| 0.7 | 0.9 | 1 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 0.7 | 0.9 | 1 |'
- en: '| –0.9 | 0.8 | 1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| –0.9 | 0.8 | 1 |'
- en: '| 0.6 | –1 | 1 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | –1 | 1 |'
- en: The plot is shown in figure 11.15, where the points with label 0 are drawn as
    squares and those with label 1 are drawn as triangles.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该图如图 11.15 所示，其中标签为 0 的点以正方形绘制，标签为 1 的点以三角形绘制。
- en: When we look at the plot in figure 11.15, it is clear that a line won’t be able
    to separate the squares from the triangles. However, a circle would (seen in figure
    11.16). Now the question is, if a support vector machine can draw only linear
    boundaries, how can we draw this circle?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看图 11.15 中的图时，很明显一条线无法将正方形和三角形分开。然而，一个圆可以（如图 11.16 所示）。现在的问题是，如果支持向量机只能绘制线性边界，我们如何绘制这个圆？
- en: '![](../Images/11-15.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-15.png)'
- en: Figure 11.15 Plot of the dataset in table 11.1\. Note that it is not separable
    by a line. Therefore, this dataset is a good candidate for the kernel method.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15 表 11.1 中数据集的图。请注意，它不能被一条线分开。因此，这个数据集是核方法的好候选。
- en: '![](../Images/11-16.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-16.png)'
- en: Figure 11.16 The kernel method gives us a classifier with a circular boundary,
    which separates these points well.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16 核方法为我们提供了一个具有圆形边界的分类器，这些点被很好地分隔开。
- en: To draw this boundary, let’s think. What is a characteristic that separates
    the squares from the triangles? From observing the plot, it seems that the triangles
    are farther from the origin than the circles. The formula that measures the distance
    to the origin is the square root of the sum of the squares of the two coordinates.
    If these coordinates are *x*[1] and *x*[2], then this distance is ![](../Images/11_16_E01.png).
    Let’s forget about the square root, and think only of *x*[1]² + *x*[2]². Now let’s
    add a column to table 11.1 with this value and see what happens. The resulting
    dataset is shown in table 11.2.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制这个边界，让我们思考。什么特征可以区分正方形和三角形？从观察图中可以看出，三角形比圆离原点更远。测量到原点距离的公式是两个坐标平方和的平方根。如果这些坐标是
    *x*[1] 和 *x*[2]，那么这个距离是 ![](../Images/11_16_E01.png)。让我们先不考虑平方根，只考虑 *x*[1]² +
    *x*[2]²。现在让我们在表 11.1 中添加一个包含这个值的列，看看会发生什么。结果数据集如图 11.2 所示。
- en: Table 11.2 We have added one more column to table 11.1\. This one consists of
    the sum of the squares of the values of the first two columns.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.2 我们在表 11.1 中添加了一个额外的列。这一列由前两列值的平方和组成。
- en: '| *x*[1] | *x*[2] | *x*[1] ² + *x*[2]² | *y*   |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *x*[1]² + *x*[2]² | *y*   |'
- en: '| 0.3 | 0.3 | 0.18 | 0 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 0.3 | 0.18 | 0 |'
- en: '| 0.2 | 0.8 | 0.68 | 0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 0.2 | 0.8 | 0.68 | 0 |'
- en: '| –0.6 | 0.4 | 0.52 | 0 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| –0.6 | 0.4 | 0.52 | 0 |'
- en: '| 0.6 | –0.4 | 0.52 | 0 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | –0.4 | 0.52 | 0 |'
- en: '| –0.4 | –0.3 | 0.25 | 0 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| –0.4 | –0.3 | 0.25 | 0 |'
- en: '| 0 | –0.8 | 0.64 | 0 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 0 | –0.8 | 0.64 | 0 |'
- en: '| –0.4 | 1.2 | 1.6 | 1 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| –0.4 | 1.2 | 1.6 | 1 |'
- en: '| 0.9 | –0.7 | 1.3 | 1 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | –0.7 | 1.3 | 1 |'
- en: '| –1.1 | –0.8 | 1.85 | 1 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| –1.1 | –0.8 | 1.85 | 1 |'
- en: '| 0.7 | 0.9 | 1.3 | 1 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 0.7 | 0.9 | 1.3 | 1 |'
- en: '| –0.9 | 0.8 | 1.45 | 1 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| –0.9 | 0.8 | 1.45 | 1 |'
- en: '| 0.6 | –1 | 1.36 | 1 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | –1 | 1.36 | 1 |'
- en: After looking at table 11.2, we can see the trend. All the points labeled 0
    satisfy that the sum of the squares of the coordinates is less than 1, and the
    points labeled 1 satisfy that this sum is greater than 1\. Therefore, the equation
    on the coordinates that separates the points is precisely *x*[1]² + *x*[2]² =
    1\. Note that this is not a linear equation, because the variables are raised
    to a power greater than one. In fact, this is precisely the equation of a circle.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看表11.2之后，我们可以看到趋势。所有标记为0的点都满足坐标平方和小于1的条件，而标记为1的点满足这个和大于1的条件。因此，分离这些点的坐标方程正是*x*[1]²
    + *x*[2]² = 1。请注意，这不是一个线性方程，因为变量被提升到大于一的幂。事实上，这正是圆的方程。
- en: The geometric way to imagine this is depicted in figure 11.17\. Our original
    set lives in the plane, and it is impossible to separate the two classes with
    a line. But if we raise each point (*x*[1], *x*[2])to the height *x*[1]² + *x*[2]²,
    this is the same as putting the points in the paraboloid with equation *z* = *x*[1]²
    + *x*[2]² (drawn in the figure). The distance we raised each point is precisely
    the square of the distance from that point to the origin. Therefore, the squares
    are raised a small amount, because they are close to the origin, and the triangles
    are raised a large amount, because they are far away from the origin. Now the
    squares and triangles are far away from each other, and therefore, we can separate
    them with the horizontal plane at height 1—in other words, the plane with equation
    *z* = 1\. As a final step, we project everything down to the plane. The intersection
    between the paraboloid and the plane becomes the circle of equation *x*[1]² +
    *x*[2]² = 1\. Notice that this equation is not linear, because it has quadratic
    terms. Finally, the prediction this classifier makes is given by *ŷ* = *step*(*x*[1]²
    + *x*[2]² – 1).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这种几何方式如图11.17所示。我们的原始集位于平面中，无法用一条线将两个类别分开。但如果我们把每个点（*x*[1]，*x*[2]）提升到高度*x*[1]²
    + *x*[2]²，这相当于将这些点放入方程*z* = *x*[1]² + *x*[2]²的抛物面中（图中所示）。我们提升每个点的距离正是该点到原点的距离的平方。因此，正方形提升的量很小，因为它们靠近原点，而三角形提升的量很大，因为它们远离原点。现在正方形和三角形相距很远，因此我们可以用高度为1的水平平面将它们分开——换句话说，就是方程*z*
    = 1的平面。作为最后一步，我们将所有东西投影到平面上。抛物面和平面的交点变成了方程*x*[1]² + *x*[2]² = 1的圆。请注意，这个方程不是线性的，因为它包含二次项。最后，这个分类器做出的预测是*ŷ*
    = *step*(*x*[1]² + *x*[2]² – 1)。
- en: '![](../Images/11-17.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-17.png)'
- en: 'Figure 11.17 The kernel method. Step 1: We start with a dataset that is not
    linearly separable. Step 2: Then we raise each point by a distance that is the
    square of its distance to the origin. This creates a paraboloid. Step 3: Now the
    triangles are high, whereas the squares are low. We proceed to separate them with
    a plane at height 1\. Step 4\. We project everything down. The intersection between
    the paraboloid and the plane creates a circle. The projection of this circle gives
    us the circular boundary of our classifier. (Source: Image created with the assistance
    of Grapher™ from Golden Software, LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17 核方法。步骤1：我们从一组非线性可分的数据集开始。步骤2：然后我们将每个点提升到其到原点的距离的平方。这创建了一个抛物面。步骤3：现在三角形很高，而正方形很低。我们通过高度为1的平面来分离它们。步骤4：我们将所有东西投影下来。抛物面和平面的交点形成一个圆。这个圆的投影给出了我们分类器的圆形边界。（来源：图像由Golden
    Software, LLC的Grapher™辅助创建；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）。
- en: 'Example 2: The modified XOR dataset'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 示例2：修改后的XOR数据集
- en: Circles are not the only figure we can draw. Let’s consider a very simple dataset,
    illustrated in table 11.3 and plotted in figure 11.18\. This dataset is similar
    to the one that corresponds to the XOR operator from exercises 5.3 and 10.2\.
    If you’d like to solve the same problem with the original XOR dataset, you can
    do it in exercise 11.2 at the end of the chapter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 圆形并不是我们唯一可以绘制的图形。让我们考虑一个非常简单的数据集，如图11.3表所示，并在图11.18中绘制。这个数据集类似于练习5.3和10.2中对应的XOR运算符。如果你想在原始XOR数据集上解决相同的问题，你可以在本章末尾的练习11.2中做到这一点。
- en: Table 11.3 The modified XOR dataset
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.3 修改后的XOR数据集
- en: '| *x*[1] | *x*[2] | *y*   |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *y*   |'
- en: '| –1 | –1 | 1 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| –1 | –1 | 1 |'
- en: '| –1 | 1 | 0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| –1 | 1 | 0 |'
- en: '| 1 | –1 | 0 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 1 | –1 | 0 |'
- en: '| 1 | 1 | 1 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 |'
- en: To see that this dataset is not linearly separable, take a look at figure 11.18\.
    The two triangles lie on opposite corners of a large square, and the two squares
    lie on the remaining two corners. It is impossible to draw a line that separates
    the triangles from the squares. However, we can use a polynomial equation to help
    us, and this time we’ll use the product of the two features. Let’s add the column
    corresponding to the product *x*[1]*x*[2] to the original dataset. The result
    is shown in table 11.4.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要看出这个数据集不是线性可分的，请看图11.18。两个三角形位于一个大正方形的对角，而两个正方形位于剩余的两个角上。不可能画一条线将三角形和正方形分开。然而，我们可以使用多项式方程来帮助我们，这次我们将使用两个特征的乘积。让我们将对应于乘积
    *x*[1]*x*[2] 的列添加到原始数据集中。结果如表11.4所示。
- en: Table 11.4 We have added a column to table 11.3, which consists of the product
    of the first two columns. Notice that there is a strong relation between the rightmost
    two columns on the table.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.4 我们在表11.3中添加了一列，该列由前两列的乘积组成。注意，表中最右边两列之间有很强的关系。
- en: '| *x*[1] | *x*[2] | *x*[1] *x*[2] | *y*   |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *x*[1] *x*[2] | *y*   |'
- en: '| –1 | –1 | 1 | 1 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| –1 | –1 | 1 | 1 |'
- en: '| –1 | 1 | –1 | 0 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| –1 | 1 | –1 | 0 |'
- en: '| 1 | –1 | –1 | 0 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 1 | –1 | –1 | 0 |'
- en: '| 1 | 1 | 1 | 1 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 1 |'
- en: 'Notice that the column corresponding to the product *x*[1]*x*[2] is very similar
    to the column of labels. We can now see that a good classifier for this data is
    the one with the following boundary equation: *x*[1]*x*[2] = 1\. The plot of this
    equation is the union of the horizontal and vertical axes, and the reason for
    this is that for the product *x*[1]*x*[2] to be 0, we need that *x*[1] = 0 or
    *x*[2] = 0. The prediction this classifier makes is given by *ŷ* = *step*(*x*[1]*x*[2]),
    and it is 1 for points in the northeast and southwest quadrants of the plane,
    and 0 elsewhere.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到对应于乘积 *x*[1]*x*[2] 的列与标签列非常相似。我们现在可以看到，对于这个数据集，一个好的分类器具有以下边界方程：*x*[1]*x*[2]
    = 1。这个方程的图是水平和垂直轴的并集，这是因为要使乘积 *x*[1]*x*[2] 为0，我们需要 *x*[1] = 0 或 *x*[2] = 0。这个分类器做出的预测由
    *ŷ* = *step*(*x*[1]*x*[2]) 给出，对于平面的东北部和西南部的点，预测为1，其他地方为0。
- en: '![](../Images/11-18.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-18.png)'
- en: Figure 11.18 The plot of the dataset in table 11.3\. The classifier that separates
    the squares from the triangles has boundary equation *x*[1]*x*[2] = 0, which corresponds
    to the union of the horizontal and vertical axes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18 表11.3中数据集的图。将正方形与三角形分开的分类器的边界方程为 *x*[1]*x*[2] = 0，这对应于水平和垂直轴的并集。
- en: 'Going beyond quadratic equations: The polynomial kernel'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 超越二次方程：多项式核
- en: In both of the previous examples, we used a polynomial expression to help us
    classify a dataset that was not linearly separable. In the first example, this
    expression was *x*[1]² + *x*[2]² , because that value is small for points near
    the origin and large for points far from the origin. In the second example, the
    expression was *x*[1]*x*[2], which helped us separate points in different quadrants
    of the plane.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个例子中，我们使用了多项式表达式来帮助我们分类一个非线性可分的数据集。在第一个例子中，这个表达式是 *x*[1]² + *x*[2]²，因为对于靠近原点的点，这个值很小，而对于远离原点的点，这个值很大。在第二个例子中，表达式是
    *x*[1]*x*[2]，这有助于我们在平面的不同象限中分离点。
- en: 'How did we find these expressions? In a more complicated dataset, we may not
    have the luxury to look at a plot and eyeball an expression that will help us
    out. We need a method or, in other words, an algorithm. What we’ll do is consider
    all the possible monomials of degree 2 (quadratic), containing *x*[1] and *x*[2].
    These are the following three monomials: *x*[1]², *x*[1]*x*[2], and *x*[2]². We
    call these new variables *x*[3], *x*[4], and *x*[5], and we treat them as if they
    had no relation with *x*[1] and *x*[2] whatsoever. Let’s apply this to the first
    example (the circle). The dataset in table 11.1 with these new columns added is
    shown in table 11.5.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何找到这些表达式的？在一个更复杂的数据集中，我们可能没有时间查看图表并直观地找到一个有助于我们的表达式。我们需要一个方法，换句话说，一个算法。我们将考虑所有可能的二次（二次）单变量，包含
    *x*[1] 和 *x*[2]。这些是以下三个单变量：*x*[1]²，*x*[1]*x*[2]，和 *x*[2]²。我们称这些新变量为 *x*[3]，*x*[4]，和
    *x*[5]，并将它们视为与 *x*[1] 和 *x*[2] 完全无关。让我们将此应用于第一个例子（圆）。添加了这些新列的表11.1数据集如表11.5所示。
- en: 'We can now build an SVM that classifies this enhanced dataset. The way to train
    an SVM is using the methods learned in the last section. I encourage you to build
    such a classifier using Scikit-Learn, Turi Create, or the package of your choice.
    By inspection, here is one equation of a classifier that works:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建一个分类器来对增强后的数据集进行分类。训练SVM的方法是使用上一节学到的技术。我鼓励你使用Scikit-Learn，Turi Create或你选择的任何包来构建这样的分类器。通过检查，这里有一个有效的分类器方程：
- en: 0*x*[1] + 0*x*[2] + 1*x*[3] + 0*x*[4] + 1*x*[5] – 1 = 0
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 0*x*[1] + 0*x*[2] + 1*x*[3] + 0*x*[4] + 1*x*[5] – 1 = 0
- en: Table 11.5 We have added three more rowcolumns to table 11.1, one corresponding
    to each of the monomials of degree 2 on the two variables *x*[1] and *x*[2]. These
    monomials are *x*[1]², *x*[1] *x*[2], and *x*[2]².
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.5 我们在表11.1中增加了三行，每一行对应于两个变量*x*[1]和*x*[2]上的每个二次单项式。这些单项式是*x*[1]²，*x*[1] *x*[2]，和*x*[2]²。
- en: '| *x*[1] | *x*[2] | *x*[3] = *x*[1]² | *x*[4] = *x*[1] *x*[2] | *x*[5] = *x*[2]²
    | *y*   |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *x*[3] = *x*[1]² | *x*[4] = *x*[1] *x*[2] | *x*[5] = *x*[2]²
    | *y*   |'
- en: '| 0.3 | 0.3 | 0.09 | 0.09 | 0.09 | 0 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 0.3 | 0.09 | 0.09 | 0.09 | 0 |'
- en: '| 0.2 | 0.8 | 0.04 | 0.16 | 0.64 | 0 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 0.2 | 0.8 | 0.04 | 0.16 | 0.64 | 0 |'
- en: '| –0.6 | 0.4 | 0.36 | –0.24 | 0.16 | 0 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| –0.6 | 0.4 | 0.36 | –0.24 | 0.16 | 0 |'
- en: '| 0.6 | –0.4 | 0.36 | –0.24 | 0.16 | 0 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | –0.4 | 0.36 | –0.24 | 0.16 | 0 |'
- en: '| –0.4 | –0.3 | 0.16 | 0.12 | 0.09 | 0 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| –0.4 | –0.3 | 0.16 | 0.12 | 0.09 | 0 |'
- en: '| 0 | –0.8 | 0 | 0 | 0.64 | 0 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 0 | –0.8 | 0 | 0 | 0.64 | 0 |'
- en: '| –0.4 | 1.2 | 0.16 | –0.48 | 1.44 | 1 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| –0.4 | 1.2 | 0.16 | –0.48 | 1.44 | 1 |'
- en: '| 0.9 | –0.7 | 0.81 | –0.63 | 0.49 | 1 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | –0.7 | 0.81 | –0.63 | 0.49 | 1 |'
- en: '| –1.1 | –0.8 | 1.21 | 0.88 | 0.64 | 1 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| –1.1 | –0.8 | 1.21 | 0.88 | 0.64 | 1 |'
- en: '| 0.7 | 0.9 | 0.49 | 0.63 | 0.81 | 1 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 0.7 | 0.9 | 0.49 | 0.63 | 0.81 | 1 |'
- en: '| –0.9 | 0.8 | 0.81 | –0.72 | 0.64 | 1 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| –0.9 | 0.8 | 0.81 | –0.72 | 0.64 | 1 |'
- en: '| 0.6 | –1 | 0.36 | –0.6 | 1 | 1 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | –1 | 0.36 | –0.6 | 1 | 1 |'
- en: 'Remembering that *x*[3] = *x*[1]² and *x*[4] = *x*[2]², we get the desired
    equation of the circle, as shown next:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 记住*x*[3] = *x*[1]²和*x*[4] = *x*[2]²，我们得到所需的圆的方程，如下所示：
- en: '*x*[1]² + *x*[2]² = 1'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[1]² + *x*[2]² = 1'
- en: If we want to visualize this process geometrically, like we’ve done with the
    previous ones, it gets a little more complicated. Our nice two-dimensional dataset
    became a five-dimensional dataset. In this one, the points labelled 0 and 1 are
    now far away, and can be separated with a four-dimensional hyperplane. When we
    project this down to two dimensions, we get the desired circle.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要像之前那样从几何角度可视化这个过程，它会变得稍微复杂一些。我们美好的二维数据集变成了五维数据集。在这个数据集中，标记为0和1的点现在相距很远，可以用四维超平面分开。当我们将其投影到二维时，我们得到所需的圆。
- en: The polynomial kernel gives rise to the map that sends the 2-D plane to the
    5-D space. The map is the one that sends the point (*x*[1], *x*[2]) to the point
    (*x*[1], *x*[2], *x*[1]², *x*[1]*x*[2], *x*[2]²). Because the maximum degree of
    each monomial is 2, we say that this is the polynomial kernel of degree 2\. For
    the polynomial kernel, we always have to specify the degree.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式核产生了一个将二维平面映射到五维空间的映射。这个映射是将点(*x*[1]，*x*[2])映射到点(*x*[1]，*x*[2]，*x*[1]²，*x*[1]*x*[2]，*x*[2]²)。因为每个单项式的最大次数是2，所以我们说这是二次多项式核。对于多项式核，我们总是必须指定次数。
- en: 'What columns do we add to the dataset if we are using a polynomial kernel of
    higher degree, say, *k*? We add one column for each monomial in the given set
    of variables, of degree less than or equal to *k*. For example, if we are using
    the degree 3 polynomial kernel on the variables *x*[1] and *x*[2], we are adding
    columns corresponding to the monomials {*x*[1], *x*[2], *x*[1]², *x*[1]*x*[2],
    *x*[2]², *x*[1]³, *x*[1]²*x*[2], *x*[1]*x*[2]², *x*[2]³}. We can also do this
    for more variables in the same way. For example, if we use the degree 2 polynomial
    kernel on the variables *x*[1], *x*[2], and *x*[3,] we are adding columns with
    the following monomials: {*x*[1], *x*[2], *x*[3], *x*[1]², *x*[1]*x*[2], *x*[1]*x*[3],
    *x*[2]², *x*[2]*x*[3], *x*[3]²}.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用的是更高次数的多项式核，比如*k*，我们应该在数据集中添加哪些列？对于给定变量集中的每个单项式，我们添加一个列，其次数小于或等于*k*。例如，如果我们使用*x*[1]和*x*[2]上的三次多项式核，我们添加的列对应于以下单项式：{*x*[1]，*x*[2]，*x*[1]²，*x*[1]*x*[2]，*x*[2]²，*x*[1]³，*x*[1]²*x*[2]，*x*[1]*x*[2]²，*x*[2]³}。我们也可以以相同的方式为更多变量做这件事。例如，如果我们使用*x*[1]，*x*[2]，和*x*[3]上的二次多项式核，我们添加的列具有以下单项式：{*x*[1]，*x*[2]，*x*[3]，*x*[1]²，*x*[1]*x*[2]，*x*[1]*x*[3]，*x*[2]²，*x*[2]*x*[3]，*x*[3]²}。
- en: 'Using bumps in higher dimensions to our benefit: The radial basis function
    (RBF) kernel'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 利用高维空间中的峰值来为我们带来好处：径向基函数（RBF）核
- en: The next kernel that we’ll see is the radial basis function kernel. This kernel
    is tremendously useful in practice, because it can help us build nonlinear boundaries
    using certain special functions centered at each of the data points. To introduce
    the RBF kernel, let’s first look at the one-dimensional example shown in figure
    11.19\. This dataset is not linearly separable—the square lies exactly between
    the two triangles.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看到的下一个核是径向基函数核。这个核在实践中非常有用，因为它可以帮助我们使用以每个数据点为中心的某些特殊函数构建非线性边界。为了介绍RBF核，让我们首先看看图11.19中显示的一维示例。这个数据集不是线性可分的——正方形正好位于两个三角形之间。
- en: '![](../Images/11-19.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-19.png)'
- en: Figure 11.19 A dataset in one dimension that can’t be classified by a linear
    classifier. Notice that a linear classifier is a point that divides the line into
    two parts, and there is no point that we can locate on the line that leaves all
    the triangles on one side and the square on the other side.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.19 一个一维数据集，线性分类器无法对其进行分类。注意，线性分类器是一个将线分为两部分的点，并且没有点可以定位在直线上，使得所有三角形都在一边，正方形在另一边。
- en: The way we will build a classifier for this dataset is to imagine building a
    mountain or a valley on each of the points. For the points labeled 1 (the triangles),
    we’ll put a mountain, and for those labeled 0 (the square), we’ll put a valley.
    These mountains and valleys are called *radial basis functions*. The resulting
    figure is shown at the top of figure 11.20\. Now, we draw a mountain range such
    that at every point, the height is the sum of all the heights of the mountains
    and valleys at that point. We can see the resulting mountain range at the bottom
    of figure 11.20\. Finally, the boundary of our classifier corresponds to the points
    at which this mountain range is at height zero, namely, the two highlighted points
    in the bottom. This classifier classifies anything in the interval between those
    two points as a square and everything outside of the interval as a triangle.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建这个数据集的分类器的方式是想象在每个点上构建一座山或一个山谷。对于标记为1的点（三角形），我们将放置一座山，对于标记为0的点（正方形），我们将放置一个山谷。这些山和山谷被称为*径向基函数*。结果图显示在图11.20的顶部。现在，我们画出一个山脉，使得在每个点上，高度是那个点上的所有山和山谷高度的总和。我们可以在图11.20的底部看到结果山脉。最后，分类器的边界对应于这个山脉高度为零的点，即底部突出显示的两个点。这个分类器将两个点之间的任何东西分类为正方形，并将区间外的任何东西分类为三角形。
- en: '![](../Images/11-20.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-20.png)'
- en: 'Figure 11.20 Using an SVM with the RBF kernel to separate a nonlinear dataset
    in one dimension. Top: We draw a mountain (radial basis function) at each point
    with label 1 and a valley at each point of label 0\. Bottom: We add the radial
    basis functions from the top figure. The resulting function intersects the axis
    twice. The two points of intersection are the boundary of our SVM classifier.
    We classify each point between them as a square (label 0) and every point outside
    as a triangle (label 1).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20 使用具有RBF核的SVM在一维数据集中分离非线性数据集。顶部：我们在每个标记为1的点处画一座山（径向基函数），在每个标记为0的点处画一个山谷。底部：我们在顶部图中的径向基函数上添加。结果函数与轴相交两次。两个交点是我们SVM分类器的边界。我们将它们之间的每个点分类为正方形（标签0），并将每个外部点分类为三角形（标签1）。
- en: This (plus some math around it which comes in the next section) is the essence
    of the RBF kernel. Now let’s use it to build a similar classifier in a two-dimensional
    dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这（以及下一节中围绕它的数学）是RBF核的精髓。现在让我们使用它在一个二维数据集中构建一个类似的分类器。
- en: To build the mountains and valleys on the plane, imagine the plane as a blanket
    (as illustrated in figure 11.21). If we pinch the blanket at that point and raise
    it, we get the mountain. If we push it down, we get the valley. These mountains
    and valleys are radial basis functions. They are called radial basis functions
    because the value of the function at a point is dependent only on the distance
    between the point and the center. We can raise the blanket at any point we like,
    and that gives us one different radial basis function for each point. The *radial
    basis function kernel* (also called RBF kernel) gives rise to a map that uses
    these radial functions to add several columns to our dataset in a way that will
    help us separate it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要在平面上构建山脉和山谷，想象平面就像一张毯子（如图11.21所示）。如果我们捏住毯子上的那个点并抬起它，我们就会得到山脉。如果我们压下它，我们就会得到山谷。这些山脉和山谷是径向基函数。它们被称为径向基函数，因为函数在一点的值只依赖于该点与中心之间的距离。我们可以在任何我们喜欢的点上抬起毯子，这样就为每个点给出一个不同的径向基函数。*径向基函数核*（也称为RBF核）产生一个映射，使用这些径向函数以帮助我们分离数据集的方式向我们的数据集添加几列。
- en: '![](../Images/11-211.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-211.png)'
- en: 'Figure 11.21 A radial basis function consists of raising the plane at a particular
    point. This is the family of functions that we’ll use to build nonlinear classifiers.
    (Source: Image created with the assistance of Grapher™ from Golden Software, LLC;
    [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21径向基函数由在特定点抬起平面组成。这是我们用来构建非线性分类器的函数族。（来源：使用Golden Software, LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）。
- en: 'How do we use this as a classifier? Imagine the following: we have the dataset
    on the left of figure 11.22, where, as usual, the triangles represent points with
    label 1, and the squares represent points with label 0\. Now, we lift the plane
    at every triangle and push it down at every square. We get the three-dimensional
    plot shown on the right of figure 11.22.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这个用作分类器呢？想象一下以下情况：我们在图11.22的左侧有数据集，其中，像往常一样，三角形代表标签为1的点，而正方形代表标签为0的点。现在，我们在每个三角形上抬起平面，在每个正方形上压下平面。我们得到了图11.22右侧所示的三维图。
- en: To create the classifier, we draw a plane at height 0 and intersect it with
    our surface. This is the same as looking at the curve formed by the points at
    height 0\. Imagine if there is a landscape with mountains and the sea. The curve
    will correspond to the coastline, namely, where the water and the land meet. This
    coastline is the curve shown on the left in figure 11.23\. We then project everything
    back to the plane and obtain our desired classifier, shown on the right in figure
    11.23.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建分类器，我们在高度0处画一个平面，并将其与我们的表面相交。这相当于观察高度为0的点的曲线。想象一下，如果有一个有山脉和大海的地形。曲线将对应于海岸线，即水和陆地相交的地方。这条海岸线是图11.23左侧所示的曲线。然后我们将所有东西投影回平面，得到我们想要的分类器，如图11.23右侧所示。
- en: That is the idea behind the RBF kernel. Of course, we have to develop the math,
    which we will do in the next few sections. But in principle, if we can imagine
    lifting and pushing down a blanket, and then building a classifier by looking
    at the boundary of the points that lie at a particular height, then we can understand
    what an RBF kernel is.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是RBF核背后的想法。当然，我们必须发展数学，我们将在接下来的几节中这样做。但原则上，如果我们能想象抬起和压下一张毯子，然后通过观察特定高度的点的边界来构建分类器，那么我们就能理解RBF核是什么。
- en: '![](../Images/11-22.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-22.png)'
- en: 'Figure 11.22 Left: A dataset in the plane that is not linearly separable. Right:
    We have used the radial basis functions to raise each of the triangles and lower
    each of the squares. Notice that now we can separate the dataset by a plane, which
    means our modified dataset is linearly separable. (Source: Image created with
    the assistance of Grapher™ from Golden Software, LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.22左：平面上一个非线性可分的数据集。右：我们已使用径向基函数抬起每个三角形并降低每个正方形。注意，现在我们可以通过一个平面来分离数据集，这意味着我们的修改后的数据集是线性可分的。（来源：使用Golden
    Software, LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）。
- en: '![](../Images/11-23.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-23.png)'
- en: 'Figure 11.23 Left: If we look at the points at height 0, they form a curve.
    If we think of the high points as land and the low points as the sea, this curve
    is the coastline. Right: When we project (flatten) the points back to the plane,
    the coastline is now our classifier that separates the triangles from the squares.
    (Source: Image created with the assistance of Grapher™ from Golden Software, LLC;
    [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.23 左：如果我们看高度为0的点，它们形成一条曲线。如果我们把高点看作陆地，低点看作海洋，这条曲线就是海岸线。右：当我们把点投影（展平）回平面时，海岸线现在是我们区分三角形和正方形的分类器。（来源：使用Golden
    Software, LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）
- en: A more in-depth look at radial basis functions
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对径向基函数的更深入探讨
- en: Radial basis functions can exist in any number of variables. At the beginning
    of this section, we saw them in one and two variables. For one variable, the simplest
    radial basis function has the formula *y* = *e*^(−x²). This looks like a bump
    over the line (figure 11.24). It looks a lot like a standard normal (Gaussian)
    distribution. The standard normal distribution is similar, but it has a slightly
    different formula, so that the area underneath it is 1.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 径向基函数可以存在于任意数量的变量中。在本节的开头，我们看到了一维和二维的例子。对于一维，最简单的径向基函数公式是 *y* = *e*^(−x²)。这看起来像一条线上的一个隆起（图11.24）。它看起来很像一个标准的正态（高斯）分布。标准的正态分布类似，但它有一个略微不同的公式，因此其下的面积是1。
- en: '![](../Images/11-24.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-24.png)'
- en: Figure 11.24 An example of a radial basis function. It looks a lot like a normal
    (Gaussian) distribution.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.24 一个径向基函数的示例。它看起来很像一个正常的（高斯）分布。
- en: Notice that this bump happens at 0\. If we wanted it to appear at any different
    point, say *p*, we can translate the formula and get *y* = *e*^(−(*x* − *p*)²).
    For example, the radial basis function centered at the point 5 is precisely *y*
    = *e*^(−(*x* − 5)²).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个隆起发生在0处。如果我们想让它出现在任何不同的点，比如说 *p*，我们可以平移公式，得到 *y* = *e*^(−(*x* − *p*)²)。例如，以点5为中心的径向基函数正好是
    *y* = *e*^(−(*x* − 5)²)。
- en: For two variables, the formula for the most basic radial basis function is *z*
    = *e*^(−(*x*² + *y*²)), and it looks like the plot shown in figure 11.25\. Again,
    you may notice that it looks a lot like a multivariate normal distribution. It
    is, again, a modified version of the multivariate normal distribution.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个变量，最基本径向基函数的公式是 *z* = *e*^(−(*x*² + *y*²))，它看起来像图11.25中所示的图表。再次，你可能注意到它看起来很像一个多元正态分布。它再次是多元正态分布的一个修改版本。
- en: '![](../Images/11-25.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-25.png)'
- en: Figure 11.25 A radial basis function on two variables. It again looks a lot
    like a normal distribution. (Source: Image created with the assistance of Grapher™
    from Golden Software, LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.25 在两个变量上的径向基函数。它再次看起来很像一个正态分布。（来源：使用Golden Software, LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）
- en: This bump happens exactly at the point (0,0). If we wanted it to appear at any
    different point, say (*p, q*), we can translate the formula, and get *y* = *e*^(−[(*x*
    − *p*)² +(*y* − *q*)²]). For example, the radial basis function centered at the
    point (2, –3) is precisely *y* = *e*^(−[(*x* − 2)² +(*y* + 3)²]).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个隆起正好发生在点(0,0)处。如果我们想让它出现在任何不同的点，比如说(*p, q*)，我们可以平移公式，得到 *y* = *e*^(−[(*x*
    − *p*)² +(*y* − *q*)²])。例如，以点(2, –3)为中心的径向基函数正好是 *y* = *e*^(−[(*x* − 2)² +(*y*
    + 3)²])。
- en: For n variables, the formula for the basic radial basis function is *y* = *e*^(−(*x*[1]²+
    ··· +*x*[n]²)). We can’t draw a plot in *n* + 1 dimensions, but if we imagine
    pinching an *n*-dimensional blanket and lifting it up with our fingers, that’s
    how it looks. However, because the algorithm that we use is purely mathematical,
    the computer has no trouble running it in as many variables as we want. As usual,
    this *n*-dimensional bump is centered at 0, but if we wanted it centered at the
    point (*p*[1], …, *p*[n]), the formula is *y* = *e*^(−[(*x*[1] - *p*[1])²+ ···
    +(*x*[n] - *p*[n])²])
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于n个变量，基本径向基函数的公式是 *y* = *e*^(−(*x*[1]²+ ··· +*x*[n]²))。我们无法在 *n* + 1 维度中绘制图表，但如果我们想象挤压一个
    *n*-维度的毯子并用手指提起它，那就是它的样子。然而，因为我们使用的算法完全是数学的，所以计算机可以轻松地在任意多的变量中运行它。像往常一样，这个 *n*-维度的隆起以0为中心，但如果我们想让它以点
    (*p*[1], …, *p*[n])为中心，公式是 *y* = *e*^(−[(*x*[1] - *p*[1])²+ ··· +(*x*[n] - *p*[n])²])
- en: 'A measure of how close points are: Similarity'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 测量点之间距离的一个度量：相似度
- en: 'To build an SVM using the RBF kernel, we need one notion: the notion of *similarity*.
    We say that two points are similar if they are close to each other, and not similar
    if they are far away (figure 11.26). In other words, the similarity between two
    points is high if they are close to each other and low if they are far away from
    each other. If the pair of points are the same point, then the similarity is 1\.
    In theory, the similarity between two points that are an infinite distance apart
    is 0.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 RBF 内核构建 SVM，我们需要一个概念：相似度的概念。我们说两个点相似，如果它们彼此靠近，如果不相似，如果它们彼此远离（图 11.26）。换句话说，两个点之间的相似度如果它们彼此靠近就很高，如果它们彼此远离就很低。如果一对点是同一个点，那么相似度为
    1。在理论上，两个无限远的点之间的相似度为 0。
- en: '![](../Images/11-26.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-26.png)'
- en: Figure 11.26 Two points that are close to each other are defined to have high
    similarity. Two points that are far away are defined to have low similarity.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.26 两个彼此靠近的点被定义为具有高相似度。两个彼此远离的点被定义为具有低相似度。
- en: 'Now we need to find a formula for similarity. As we can see, the similarity
    between two points decreases as the distance between them increases. Thus, many
    formulas for similarity would work, as long as they satisfy that condition. Because
    we are using exponential functions in this section, let’s define it as follows.
    For points *p* and *q*, the similarity between *p* and *q* is as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要找到一个相似度的公式。正如我们所看到的，两个点之间的相似度随着它们之间距离的增加而降低。因此，许多相似度的公式都会工作，只要它们满足这个条件。因为我们在这个部分使用指数函数，所以我们定义如下。对于点
    *p* 和 *q*，*p* 和 *q* 之间的相似度如下：
- en: '*similarity*(*p*,*q*)= *e*^–^(distance)^(^p^,^q^)²'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*similarity*(*p*,*q*)= *e*^–^(distance)^(^p^,^q^)²'
- en: That looks like a complicated formula for similarity, but there is a very nice
    way to look at it. If we want to find the similarity between two points, say *p*
    and *q*, this similarity is precisely the height of the radial basis function
    centered at *p* and applied at the point *q*. This is, if we pinch the blanket
    at point *p* and lift it, then the height of the blanket at point *q* is high
    if the *q* is close to *p* and low if *q* is far from *p*. In figure 11.27, we
    can see this for one variable, but imagine it in any number of variables by using
    the blanket analogy.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这个看起来是一个复杂的相似度公式，但有一个非常好的方法来看待它。如果我们想要找到两个点之间的相似度，比如说 *p* 和 *q*，这个相似度正是以 *p*
    为中心的径向基函数在点 *q* 处应用的高度。也就是说，如果我们把毯子在点 *p* 处捏起并提起，那么点 *q* 处的毯子高度就会很高，如果 *q* 靠近
    *p*，而如果 *q* 离 *p* 很远，那么高度就会很低。在图 11.27 中，我们可以看到一个变量的情况，但想象一下通过毯子类比在任何数量的变量中。
- en: '![](../Images/11-27.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-27.png)'
- en: Figure 11.27 The similarity is defined as the height of a point in the radial
    basis function, where the input is the distance. Note that the higher the distance,
    the lower the similarity, and vice versa.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.27 相似度定义为径向基函数中点的位置，其中输入是距离。请注意，距离越高，相似度越低，反之亦然。
- en: Training an SVM with the RBF kernel
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RBF 内核训练 SVM
- en: Now that we have all the tools to train an SVM using the RBF kernel, let’s see
    how to put it all together. Let’s first look at the simple dataset displayed in
    figure 11.19\. The dataset itself appears in table 11.6.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了使用 RBF 内核训练 SVM 的所有工具，让我们看看如何将这些工具组合起来。让我们首先看看图 11.19 显示的简单数据集。数据集本身在表
    11.6 中。
- en: Table 11.6 The one-dimensional dataset shown in figure 11.19\. Note that it
    isn’t linearly separable, because the point with label 0 is right between the
    two points with label 1.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.6 图 11.19 所示的一维数据集。请注意，它不是线性可分的，因为标签为 0 的点正好位于两个标签为 1 的点之间。
- en: '| Point | *x*   | y (label) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 点 | *x*   | y (标签) |'
- en: '| 1 | –1 | 1 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 1 | –1 | 1 |'
- en: '| 2 | 0 | 0 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 |'
- en: '| 3 | 1 | 1 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 1 |'
- en: As we saw, this dataset is not linearly separable. To make it linearly separable,
    we’ll add a few columns. The three columns we are adding are the similarity columns,
    and they record the similarity between the points. The similarity between two
    points with *x*-coordinates *x*[1] and *x*[2] is measured as *e*^((*x*[1] + *x*[2])²),
    as indicated in the section “Using bumps in higher dimensions to our benefit.”
    For example, the similarity between points 1 and 2 is *e*^((−1 −0)²) = 0.368\.
    In the Sim1 column, we’ll record the similarity between point 1 and the other
    three points, and so on. The extended dataset is shown in table 11.7.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，这个数据集不是线性可分的。为了使其线性可分，我们将添加几个列。我们添加的三个列是相似度列，它们记录了点之间的相似度。具有 *x* 坐标 *x*[1]
    和 *x*[2] 的两个点之间的相似度测量为 *e*^((*x*[1] + *x*[2])²)，如“在更高维中使用凸起以获得优势”部分所示。例如，点1和点2之间的相似度为
    *e*^((−1 −0)²) = 0.368。在Sim1列中，我们将记录点1与其他三个点之间的相似度，依此类推。扩展后的数据集如表11.7所示。
- en: Table 11.7 We extend the dataset in table 11.6 by adding three new columns.
    Each column corresponds to the similarity of all points with respect to each point.
    This extended dataset lives in a four-dimensional space, and it is linearly separable.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.7 我们通过添加三个新列扩展了表11.6中的数据集。每个列对应于所有点相对于每个点的相似度。这个扩展后的数据集存在于四维空间中，并且是线性可分的。
- en: '| Point | *x*   | Sim1 | Sim2 | Sim3 | *y*   |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 点 | *x*   | 模拟1 | 模拟2 | 模拟3 | *y*   |'
- en: '| 1 | –1 | 1 | 0.368 | 0.018 | 1 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 1 | –1 | 1 | 0.368 | 0.018 | 1 |'
- en: '| 2 | 0 | 0.368 | 1 | 0.368 | 0 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0.368 | 1 | 0.368 | 0 |'
- en: '| 3 | 1 | 0.018 | 0.368 | 1 | 1 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 0.018 | 0.368 | 1 | 1 |'
- en: 'This extended dataset is now linearly separable! Many classifiers will separate
    this set, but in particular, the one with the following boundary equation will:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这个扩展后的数据集现在是线性可分的！许多分类器可以分离这个集合，但特别是以下边界方程的分类器：
- en: '*ŷ* = *step*(*Sim*1 – *Sim*2 + *Sim*3)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *step*(*Sim*1 – *Sim*2 + *Sim*3)'
- en: 'Let’s verify this by predicting the label at every point as shown next:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过预测每个点的标签来验证，如下所示：
- en: '**Point 1**: *ŷ* = *step*(1 – 0.368 + 0.018) = *step*(0.65) = 1'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点1**: *ŷ* = *step*(1 – 0.368 + 0.018) = *step*(0.65) = 1'
- en: '**Point 2**: *ŷ* = *step*(0.368 – 1 + 0.368) = *step*(–0.264) = 0'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点2**: *ŷ* = *step*(0.368 – 1 + 0.368) = *step*(–0.264) = 0'
- en: '**Point 3**: *ŷ* = *step*(0.018 – 0.368 + 1) = *step*(0.65) = 1'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点3**: *ŷ* = *step*(0.018 – 0.368 + 1) = *step*(0.65) = 1'
- en: 'Furthermore, because *Sim*1=*e*^((*x* + 1)²), *Sim*2=*e*^((*x* − 0)²), and
    *Sim*3=*e*^((*x* − 1)²) then our final classifier makes the following predictions:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为 *Sim*1=*e*^((*x* + 1)²)，*Sim*2=*e*^((*x* − 0)²)，*Sim*3=*e*^((*x* − 1)²)，所以我们的最终分类器做出以下预测：
- en: '*ŷ* = *step*(*e*^((*x* + 1)²) − *e*^(*x*²) + *e*^((*x* − 1)²))'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *step*(*e*^((*x* + 1)²) − *e*^(*x*²) + *e*^((*x* − 1)²))'
- en: 'Now, let’s do this same procedure but in two dimensions. This section does
    not require code, but the calculations are large, so if you’d like to take a look
    at them, they are in the following notebook: [https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/Calculating_similarities.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/Calculating_similarities.ipynb).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在二维空间中执行相同的程序。本节不需要代码，但计算量很大，所以如果您想查看它们，可以在以下笔记本中找到：[https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/Calculating_similarities.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/Calculating_similarities.ipynb).
- en: Table 11.8 A simple dataset in two dimensions, plotted in figure 11.28\. We’ll
    use an SVM with an RBF kernel to classify this dataset.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.8 一个简单的二维数据集，如图11.28所示。我们将使用具有RBF核的支持向量机来分类这个数据集。
- en: '| Point | *x*[1] | *x*[2] | *y*   |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 点 | *x*[1] | *x*[2] | *y*   |'
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 |'
- en: '| 2 | –1 | 0 | 0 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 2 | –1 | 0 | 0 |'
- en: '| 3 | 0 | –1 | 0 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | –1 | 0 |'
- en: '| 4 | 0 | 1 | 1 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 1 | 1 |'
- en: '| 5 | 1 | 0 | 1 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 | 0 | 1 |'
- en: '| 6 | –1 | 1 | 1 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 6 | –1 | 1 | 1 |'
- en: '| 7 | 1 | –1 | 1 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 | –1 | 1 |'
- en: Consider the dataset in table 11.8, which we already classified graphically
    (figures 11.22 and 11.23). For convenience, it is plotted again in figure 11.28\.
    In this plot, the points with label 0 appear as squares and those with label 1
    as triangles.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑表11.8中的数据集，我们已将其图形化分类（图11.22和11.23）。为了方便，它再次在图11.28中绘制。在这个图中，标签为0的点以正方形表示，标签为1的点以三角形表示。
- en: 'Notice that in the first column of table 11.8 and in figure 11.28, we have
    numbered every point. This is not part of the data; we did it only for convenience.
    We will now add seven columns to this table. The columns are the similarities
    with respect to every point. For example, for point 1, we add a similarity column
    named Sim1\. The entry for every point in this column is the amount of similarity
    between that point and point 1\. Let’s calculate one of them, for example, the
    similarity with point 6\. The distance between point 1 and point 6, by the Pythagorean
    theorem follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在表11.8的第一列和图11.28中，我们给每个点都编了号。这不是数据的一部分；我们这样做只是为了方便。现在我们将向这个表添加七个列。这些列是每个点相对于其他点的相似度。例如，对于点1，我们添加一个名为Sim1的相似度列。这个列中每个点的条目是那个点与点1之间的相似度。让我们计算其中一个，例如，与点6的相似度。根据勾股定理，点1和点6之间的距离如下：
- en: '![](../Images/11_28_E01.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11_28_E01.png)'
- en: '![](../Images/11-28.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-28.png)'
- en: Figure 11.28 The plot of the dataset in table 11.8, where the points with label
    0 are squares and those with label 1 are triangles. Notice that the squares and
    triangles cannot be separated with a line. We’ll use an SVM with an RBF kernel
    to separate them with a curved boundary.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.28 表11.8中数据集的图表，其中标签为0的点为正方形，标签为1的点为三角形。注意，正方形和三角形无法用一条线分开。我们将使用具有RBF核的支持向量机（SVM）用曲线边界将它们分开。
- en: Therefore, the similarity is precisely
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相似度精确地是
- en: '*similarity*(*point* 1, *point* 6)= *e*^–^(distance)^(^q^,^p^)² = *e*^(–2)
    = 0.135.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*similarity*(*point* 1, *point* 6)= *e*^–^(distance)^(^q^,^p^)² = *e*^(–2)
    = 0.135.'
- en: This number goes in row 1 and column Sim6 (and by symmetry, also in row 6 and
    column Sim1). Fill in a few more values in this table to convince yourself that
    this is the case, or take a look at the notebook where the whole table is calculated.
    The result is shown in table 11.9.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字位于第1行和Sim6列（以及对称地，第6行和Sim1列）。在表中填写更多值以使你自己相信这是正确的，或者查看计算整个表的笔记本。结果如表11.9所示。
- en: Table 11.9 We have added seven similarity columns to the dataset in table 11.8\.
    Each one records the similarities with all the other six points.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.9 我们在表11.8的数据集中增加了七个相似度列。每一列都记录了与其他六个点的相似度。
- en: '| Point | *x*[1] | *x*[2] | Sim1 | Sim2 | Sim3 | Sim4 | Sim5 | Sim6 | Sim7
    | *y*   |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 点 | *x*[1] | *x*[2] | Sim1 | Sim2 | Sim3 | Sim4 | Sim5 | Sim6 | Sim7 | *y*  
    |'
- en: '| 1 | 0 | 0 | 1 | 0.368 | 0.368 | 0.368 | 0.368 | 0.135 | 0.135 | 0 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 1 | 0.368 | 0.368 | 0.368 | 0.368 | 0.135 | 0.135 | 0 |'
- en: '| 2 | –1 | 0 | 0.368 | 1 | 0.135 | 0.135 | 0.018 | 0.368 | 0.007 | 0 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 2 | –1 | 0 | 0.368 | 1 | 0.135 | 0.135 | 0.018 | 0.368 | 0.007 | 0 |'
- en: '| 3 | 0 | –1 | 0.368 | 0.135 | 1 | 0.018 | 0.135 | 0.007 | 0.368 | 0 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | –1 | 0.368 | 0.135 | 1 | 0.018 | 0.135 | 0.007 | 0.368 | 0 |'
- en: '| 4 | 0 | 1 | 0.368 | 0.135 | 0.018 | 1 | 0.135 | 0.368 | 0.007 | 1 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 1 | 0.368 | 0.135 | 0.018 | 1 | 0.135 | 0.368 | 0.007 | 1 |'
- en: '| 5 | 1 | 0 | 0.368 | 0.018 | 0.135 | 0.135 | 1 | 0.007 | 0.368 | 1 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 | 0 | 0.368 | 0.018 | 0.135 | 0.135 | 1 | 0.007 | 0.368 | 1 |'
- en: '| 6 | –1 | 1 | 0.135 | 0.368 | 0.007 | 0.367 | 0.007 | 1 | 0 | 1 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 6 | –1 | 1 | 0.135 | 0.368 | 0.007 | 0.367 | 0.007 | 1 | 0 | 1 |'
- en: '| 7 | 1 | –1 | 0.135 | 0.007 | 0.368 | 0.007 | 0.368 | 0 | 1 | 1 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 | –1 | 0.135 | 0.007 | 0.368 | 0.007 | 0.368 | 0 | 1 | 1 |'
- en: 'Notice the following things:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下事项：
- en: The similarity between each point and itself is always 1.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个点与自身之间的相似度总是1。
- en: For each pair of points, the similarity is high when they are close in the plot
    and low when they are far.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每对点，当它们在图中靠近时相似度较高，当它们相距较远时相似度较低。
- en: The table consisting of the columns Sim1 to Sim7 is symmetric, because the similarity
    between *p* and *q* is the same as the similarity between *q* and *p* (as it depends
    only on the distance between *p* and *q*).
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由Sim1到Sim7列组成的表是对称的，因为点p和点q之间的相似度与点q和点p之间的相似度相同（因为它只取决于点p和点q之间的距离）。
- en: The similarity between points 6 and 7 appears as 0, but in reality, it is not.
    The distance between points 6 and 7 is ![](../Images/11_28_E02.png), so their
    similarity is *e*^(–8) = 0.00033546262, which rounds to zero because we are using
    three significant figures.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点6和点7之间的相似度显示为0，但实际上并非如此。点6和点7之间的距离是![图片](../Images/11_28_E02.png)，因此它们的相似度为
    *e*^(–8) = 0.00033546262，四舍五入后为0，因为我们使用了三位有效数字。
- en: 'Now, on to building our classifier! Notice that for the data in the small table
    11.8, no linear classifier works (because the points can’t be split by a line),
    but on the much larger table 11.9, which has a lot more features (columns), we
    can fit such a classifier. We proceed to fit an SVM to this data. Many SVMs can
    classify this dataset correctly, and in the notebook, I’ve used Turi Create to
    build one. However, a simpler one works as well. This classifier has the following
    weights:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续构建我们的分类器！注意，对于小表 11.8 中的数据，没有线性分类器可以工作（因为点不能被一条线分开），但在具有更多特征（列）的更大表
    11.9 中，我们可以拟合这样的分类器。我们继续拟合 SVM 到这些数据。许多 SVM 可以正确分类这个数据集，在笔记本中，我使用了 Turi Create
    来构建一个。然而，一个更简单的分类器也可以工作。这个分类器有以下权重：
- en: The weights of *x*[1] and *x*[2] are 0.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[1] 和 *x*[2] 的权重为 0。'
- en: The weight of Sim *p* is 1, for *p* = 1, 2, and 3.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sim *p* 的权重为 1，其中 *p* = 1, 2, 和 3。
- en: The weight of Sim *p* is –1, for *p* = 4, 5, 6, and 7.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sim *p* 的权重为 -1，其中 *p* = 4, 5, 6, 和 7。
- en: The bias is *b* = 0.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差为 *b* = 0。
- en: We find the classifier was adding a label –1 to the columns corresponding to
    the points labeled 0, and a +1 to the columns corresponding to the points labeled
    1\. This is equivalent to the process of adding a mountain at any point of label
    1 and a valley at every point of label 0, like in figure 11.29\. To check mathematically
    that this works, take table 11.7, add the values of the columns Sim4, Sim5, Sim6,
    and Sim7, then subtract the values of the columns Sim1, Sim2 and Sim3\. You’ll
    notice that you get a negative number in the first three rows and a positive one
    in the last four rows. Therefore, we can use a threshold of 0, and we have a classifier
    that classifies this dataset correctly, because the points labeled 1 get a positive
    score, and the points labeled 0 get a negative score. Using a threshold of 0 is
    equivalent to using the coastline to separate the points in the plot in figure
    11.29.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现分类器将标签 -1 添加到对应于标签 0 的列，并将 +1 添加到对应于标签 1 的列。这相当于在标签 1 的任何点上添加一个山丘，在标签 0
    的每个点上添加一个山谷，就像图 11.29 中所示。为了从数学上验证这一点，取表 11.7，加上 Sim4、Sim5、Sim6 和 Sim7 列的值，然后减去
    Sim1、Sim2 和 Sim3 列的值。你会注意到前三个行得到一个负数，后四个行得到一个正数。因此，我们可以使用阈值 0，我们就有了一个可以正确分类这个数据集的分类器，因为标签
    1 的点得到一个正分数，而标签 0 的点得到一个负分数。使用阈值 0 等同于使用图 11.29 中图表的 coastline 来分离点。
- en: 'If we plug in the similarity function, the classifier we obtain is the following:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们插入相似性函数，我们获得的分类器如下：
- en: '*ŷ* = *step*(−*e*^(*x*[1]² + *x*[2] ²) −*e*^((*x*[1] + 1)² + *x*[2]²) −*e*^(*x*[1]²
    + (*x*[2] + 1)²) +*e*^(*x*[1]² + (*x*[2] − 1)²) +*e*^((*x*[1] − 1)² + *x*[2]²)
    +*e*^((*x*[1] + 1)² + (*x*[2] − 1)²) +*e*^((*x*[1] − 1)² + (*x*[2] + 1)²))'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *step*(−*e*^(*x*[1]² + *x*[2] ²) −*e*^((*x*[1] + 1)² + *x*[2]²) −*e*^(*x*[1]²
    + (*x*[2] + 1)²) +*e*^(*x*[1]² + (*x*[2] − 1)²) +*e*^((*x*[1] − 1)² + *x*[2]²)
    +*e*^((*x*[1] + 1)² + (*x*[2] − 1)²) +*e*^((*x*[1] − 1)² + (*x*[2] + 1)²))'
- en: In summary, we found a dataset that was not linearly separable. We used radial
    basis functions and similarity between points to add several columns to the dataset.
    This helped us build a linear classifier (in a much higher-dimensional space).
    We then projected the higher-dimensional linear classifier into the plane to get
    the classifier we wanted. We can see the resulting curved classifier in figure
    11.29.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们找到了一个非线性可分的数据集。我们使用径向基函数和点之间的相似性向数据集中添加了几个列。这有助于我们在更高维的空间中构建一个线性分类器。然后我们将高维线性分类器投影到平面上，以获得我们想要的分类器。我们可以在图
    11.29 中看到得到的曲线分类器。
- en: '![](../Images/11-29.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-29.png)'
- en: 'Figure 11.29 In this dataset, we raised each triangle and lowered each square.
    Then we drew a plane at height 0, which separates the squares and the triangles.
    The plane intersects the surface in a curved boundary. We then projected everything
    back down to two dimensions, and this curved boundary is the one that separates
    our triangles from our squares. The boundary is drawn at the right. (Source: Image
    created with the assistance of Grapher™ from Golden Software, LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.29 在这个数据集中，我们升高了每个三角形并降低了每个正方形。然后我们在高度 0 处绘制了一个平面，该平面将正方形和三角形分开。该平面在曲面上形成一个曲线边界。然后我们将所有内容投影回二维，这个曲线边界就是将我们的三角形和正方形分开的边界。边界在右侧绘制。（来源：由
    Golden Software, LLC 的 Grapher™ 辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）。
- en: 'Overfitting and underfitting with the RBF kernel: The gamma parameter'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RBF 内核的过拟合和欠拟合：gamma 参数
- en: At the beginning of this section, we mentioned that many different radial basis
    functions exist, namely one per point in the plane. There are actually many more.
    Some of them lift the plane at a point and form a narrow surface, and others form
    a wide surface. Some examples can be seen in figure 11.30\. In practice, the wideness
    of our radial basis functions is something we want to tune. For this, we use a
    parameter called the *gamma parameter*. When gamma is small, the surface formed
    is very wide, and when it is large, the surface is very narrow.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开始部分，我们提到存在许多不同的径向基函数，即平面上的每一个点对应一个。实际上还有更多。其中一些在平面上某一点抬起并形成一个狭窄的表面，而另一些则形成一个宽大的表面。一些例子可以在图11.30中看到。在实践中，我们希望调整径向基函数的宽度。为此，我们使用一个称为*gamma参数*的参数。当gamma值较小时，形成的表面非常宽，而当它较大时，表面则非常狭窄。
- en: '![](../Images/11-30.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-30.png)'
- en: 'Figure 11.30 The gamma parameter determines how wide the surface is. Notice
    that for small values of gamma, the surface is very wide, and for large values
    of gamma, the surface is very narrow. (Source: Image created with the assistance
    of Grapher™ from Golden Software, LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.30 gamma参数决定了表面的宽度。注意，对于小的gamma值，表面非常宽，而对于大的gamma值，表面则非常狭窄。（来源：由Golden Software,
    LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）
- en: Gamma is a hyperparameter. Recall that hyperparameters are the specifications
    that we use to train our model. The way we tune this hyperparameter is using methods
    that we’ve seen before, such as the model complexity graph (the section “A numerical
    way to decide how complex our model should be” in chapter 4). Different values
    of gamma tend to overfit and underfit. Let’s look back at the example at the beginning
    of this section, with three different values of gamma. The three models are plotted
    in figure 11.31.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Gamma是一个超参数。回想一下，超参数是我们用来训练模型的具体规格。我们调整这个超参数的方法是使用我们之前看到的方法，例如模型复杂度图（第4章中的“决定模型复杂度的数值方法”部分）。不同的gamma值往往会过拟合或欠拟合。让我们回顾一下本节开头提到的三个不同gamma值的例子。这三个模型在图11.31中进行了展示。
- en: '![](../Images/11-311.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-311.png)'
- en: 'Figure 11.31 Three SVM classifiers shown with an RBF kernel and different values
    of gamma. (Source: Image created with the assistance of Grapher™ from Golden Software,
    LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.31展示了使用RBF核和不同gamma值的三个SVM分类器。（来源：由Golden Software, LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）
- en: Notice that for a very small value of gamma, the model overfits, because the
    curve is too simple, and it doesn’t classify our data well. For a large value
    of gamma, the model vastly overfits, because it builds a tiny mountain for each
    triangle and a tiny valley for each square. This makes it classify almost everything
    as a square, except for the areas just around the triangles. A medium value of
    gamma seems to work well, because it builds a boundary that is simple enough,
    yet classifies the points correctly.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于非常小的gamma值，模型会过拟合，因为曲线过于简单，并且不能很好地对数据进行分类。对于较大的gamma值，模型会严重过拟合，因为它为每个三角形构建一个小山，为每个正方形构建一个小山谷。这使得它几乎将所有东西都分类为正方形，除了三角形周围的区域。中等值的gamma似乎效果很好，因为它构建了一个足够简单且能正确分类点的边界。
- en: 'The equation for the radial basis function doesn’t change much when we add
    the gamma parameter—all we have to do is multiply the exponent by gamma. In the
    general case, the equation of the radial basis function follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们添加gamma参数时，径向基函数的方程变化不大——我们只需将指数乘以gamma。在一般情况下，径向基函数的方程如下：
- en: '*y* = *e*^(−*γ*[(*x*[1] − *p*[1])²+ ··· +( *x[n]* + *p[n]*)²])'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *e*^(−*γ*[(*x*[1] − *p*[1])²+ ··· +( *x[n]* + *p[n]*)²])'
- en: Don’t worry very much about learning this formula—just remember that even in
    higher dimensions, the bumps we make can be wide or narrow. As usual, there is
    a way to code this and make it work, which is what we do in the next section.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过于担心学习这个公式——只需记住，即使在更高维的情况下，我们创建的峰值可以是宽的或窄的。通常，有一种方法可以编码它并使其工作，这就是我们在下一节中要做的。
- en: Coding the kernel method
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 编写核方法
- en: 'Now that we’ve learned the kernel method for SVMs, we learn code them in Scikit-Learn
    and train a model in a more complex dataset using the polynomial and RBF kernels.
    To train an SVM in Scikit-Learn with a particular kernel, all we do is add the
    kernel as a parameter when we define the SVM. The code for this section follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了 SVM 的核方法，接下来我们将在 Scikit-Learn 中编写代码，并使用多项式和 RBF 核在一个更复杂的数据集上训练模型。要在
    Scikit-Learn 中使用特定核训练 SVM，我们只需在定义 SVM 时添加核作为参数即可。本节的代码如下：
- en: '**Notebook**: SVM_graphical_example.ipynb'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**: SVM_graphical_example.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/SVM_graphical_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/SVM_graphical_example.ipynb)'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SVM 图形示例](https://github.com/luisguiserrano/manning/blob/master/Chapter_11_Support_Vector_Machines/SVM_graphical_example.ipynb)'
- en: 'Datasets:'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集：
- en: one_circle.csv
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: one_circle.csv
- en: two_circles.csv
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: two_circles.csv
- en: Coding the polynomial kernel to classify a circular dataset
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 编写多项式核以分类圆形数据集
- en: In this subsection, we see how to code the polynomial kernel in Scikit-Learn.
    For this, we use the dataset called one_circle.csv, shown in figure 11.32.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将学习如何在 Scikit-Learn 中编写多项式核的代码。为此，我们使用一个名为 one_circle.csv 的数据集，如图 11.32
    所示。
- en: '![](../Images/11-32.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-32.png)'
- en: Figure 11.32 A circular dataset, with some noise. We will use an SVM with the
    polynomial kernel to classify this dataset.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.32 一个圆形数据集，包含一些噪声。我们将使用具有多项式核的 SVM 来分类这个数据集。
- en: Notice that aside from some outliers, this dataset is mostly circular. We train
    an SVM classifier where we specify the `kernel` parameter to be `poly`, and the
    `degree` parameter to be 2, as shown in the next code snippet. The reason we want
    the degree to be 2 is because the equation of a circle is a polynomial of degree
    2\. The result is shown in figure 11.33.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到除了少数异常值外，这个数据集基本上是圆形的。我们训练了一个 SVM 分类器，其中我们指定 `kernel` 参数为 `poly`，`degree`
    参数为 2，如下一代码片段所示。我们想要 degree 为 2 的原因是圆的方程是一个二次多项式。结果如图 11.33 所示。
- en: '[PRE3]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/11-33.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-33.png)'
- en: Figure 11.33 An SVM classifier with a polynomial kernel of degree 2
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.33 具有二次多项式核的 SVM 分类器
- en: Notice that this SVM with a polynomial kernel of degree 2 manages to build a
    mostly circular region to bound the dataset, as desired.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这个具有二次多项式核的 SVM 成功构建了一个大致圆形的区域来包围数据集，正如预期的那样。
- en: Coding the RBF kernel to classify a dataset formed by two intersecting circles
    and playing with the gamma parameter
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 编写 RBF 核以分类由两个相交圆组成的数据集，并调整 gamma 参数
- en: We’ve drawn a circle, but let’s get more complicated. In this subsection, we
    learn how to code several SVMs with the RBF kernel to classify a dataset that
    has the shape of two intersecting circles. This dataset, called two_circles.csv,
    is illustrated in figure 11.34.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经画了一个圆，但让我们变得更复杂一些。在本小节中，我们将学习如何编写几个具有 RBF 核的 SVM 来分类具有两个相交圆形状的数据集。这个数据集称为
    two_circles.csv，如图 11.34 所示。
- en: '![](../Images/11-34.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-34.png)'
- en: Figure 11.34 A dataset consisting of two intersecting circles, with some outliers.
    We will use an SVM with the RBF kernel to classify this dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.34 由两个相交圆组成的数据集，包含一些异常值。我们将使用具有 RBF 核的 SVM 来分类这个数据集。
- en: 'To use the RBF kernel, we specify `kernel = ''rbf''`. We can also specify a
    value for gamma. We’ll train four different SVM classifiers, for the following
    values of gamma: 0.1, 1, 10, and 100, as shown next:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 RBF 核，我们指定 `kernel = 'rbf'`。我们还可以指定一个 gamma 的值。我们将训练四个不同的 SVM 分类器，gamma
    的值分别为 0.1、1、10 和 100，如以下所示：
- en: '[PRE4]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Gamma = 0.1
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Gamma = 0.1
- en: ❷ Gamma = 1
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Gamma = 1
- en: ❸ Gamma = 10
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Gamma = 10
- en: ❹ Gamma = 100
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Gamma = 100
- en: '![](../Images/11-35.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-35.png)'
- en: Figure 11.35 Four SVM classifiers with an RBF kernel and different values of
    gamma
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.35 具有不同 gamma 值的 RBF 核的四个 SVM 分类器
- en: The four classifiers appear in figure 11.35\. Notice that for gamma = 0.1, the
    model underfits a little, because it thinks the boundary is one oval, and it makes
    some mistakes. Gamma = 1 gives a good model that captures the data well. By the
    time we get to gamma = 10, we can see that the model starts to overfit. Notice
    how it tries to classify every point correctly, including the outliers, which
    it encircles individually. By the time we get to gamma=100, we can see some serious
    overfitting. This classifier only surrounds each triangle with a small circular
    region and classifies everything else as a square. Thus, for this model, gamma
    = 1 seems to be the best value among the ones we tried.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 四个分类器出现在图 11.35 中。注意，当 gamma = 0.1 时，模型略微欠拟合，因为它认为边界是一个椭圆形，并犯了一些错误。gamma = 1
    提供了一个很好的模型，能够很好地捕捉数据。当我们达到 gamma = 10 时，我们可以看到模型开始过拟合。注意它如何尝试正确分类每个点，包括异常值，它单独包围每个异常值。当我们达到
    gamma=100 时，我们可以看到一些严重的过拟合。这个分类器只围绕每个三角形用一个小的圆形区域，并将其他所有内容分类为正方形。因此，对于这个模型，gamma
    = 1 在我们尝试的值中似乎是最好的。
- en: Summary
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A support vector machine (SVM) is a classifier that consists of fitting two
    parallel lines (or hyperplanes), and trying to space them as far apart as possible,
    while still trying to classify the data correctly.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是一种分类器，它由拟合两条平行线（或超平面）组成，并尝试使它们尽可能远地分开，同时仍然尝试正确分类数据。
- en: 'The way to build support vector machines is with an error function that comprises
    two terms: the sum of two perceptron errors, one per parallel line, and the distance
    error, which is high when the two parallel lines are far apart and low when they
    are close together.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建支持向量机的方法是使用一个包含两个项的错误函数：两个感知器错误的和，每个平行线一个，以及距离错误，当两条平行线相距较远时距离错误较高，当它们靠得很近时距离错误较低。
- en: We use the C parameter to regulate between trying to classify the points correctly
    and trying to space out the lines. This is useful while training because it gives
    us control over our preferences, namely, if we want to build a classifier that
    classifies the data very well, or a classifier with a well-spaced boundary.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 C 参数来调节在正确分类点和尝试使线间距最大化之间的平衡。这在训练时很有用，因为它让我们对我们的偏好有了控制权，即如果我们想构建一个能够很好地分类数据的分类器，或者一个具有良好间距边界的分类器。
- en: The kernel method is a useful and very powerful tool for building nonlinear
    classifiers.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核方法是一种构建非线性分类器的有用且非常强大的工具。
- en: The kernel method consists of using functions to help us embed our dataset inside
    a higher-dimensional space, in which the points may be easier to classify with
    a linear classifier. This is equivalent to adding columns to our dataset in a
    clever way to make the enhanced dataset linearly separable.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核方法包括使用函数帮助我们将数据集嵌入到更高维的空间中，在那里点可能更容易用线性分类器进行分类。这相当于以巧妙的方式向我们的数据集添加列，使增强后的数据集线性可分。
- en: Several different kernels, such as the polynomial kernel and the RBF kernel,
    are available. The polynomial kernel allows us to build polynomial regions such
    as circles, parabolas, and hyperbolas. The RBF kernel allows us to build more
    complex curved regions.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有几种不同的核函数可用，例如多项式核和径向基函数核。多项式核允许我们构建如圆、抛物线和双曲线等多项式区域。径向基函数核允许我们构建更复杂的曲线区域。
- en: Exercises
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 11.1
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 11.1
- en: (This exercise completes the calculation needed in the section “Distance error
    function.”)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: （此练习完成了“距离错误函数”部分所需的计算。）
- en: Show that the distance between the lines with equations *w*[1]*x*[1] + *w*[2]*x*[1]
    + *b* = 1 and *w*[1]*x*[1] + *w*[2]*x*[1] + *b* = –1 is precisely ![](../Images/11_35_E01.png).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 证明具有方程 *w*[1]*x*[1] + *w*[2]*x*[1] + *b* = 1 和 *w*[1]*x*[1] + *w*[2]*x*[1] +
    *b* = –1 的两条线的距离恰好是 ![](../Images/11_35_E01.png)。
- en: '![](../Images/11-unnumb-2.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-unnumb-2.png)'
- en: Exercise 11.2
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 11.2
- en: 'As we learned in exercise 5.3, it is impossible to build a perceptron model
    that mimics the XOR gate. In other words, it is impossible to fit the following
    dataset (with 100% accuracy) with a perceptron model:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在练习 5.3 中所学，不可能构建一个模仿 XOR 门的感知器模型。换句话说，不可能用感知器模型（以 100% 的准确率）拟合以下数据集：
- en: '| *x*[1] | *x*[2] | *y*   |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *y*   |'
- en: '| 0 | 0 | 0 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: This is because the dataset is not linearly separable. An SVM has the same problem,
    because an SVM is also a linear model. However, we can use a kernel to help us
    out. What kernel should we use to turn this dataset into a linearly separable
    one? What would the resulting SVM look like?
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为数据集不是线性可分的。支持向量机（SVM）也有同样的问题，因为SVM也是一个线性模型。然而，我们可以使用核函数来帮助我们。我们应该使用什么核函数将这个数据集转换为线性可分的数据集？转换后的SVM会是什么样子？
- en: hint Look at example 2 in the section “Using polynomial equations to your benefit,”
    which solves a very similar problem.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：查看“利用多项式方程为你带来好处”部分中的示例2，它解决了一个非常类似的问题。
