- en: 8 Object detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 目标检测
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Making a prediction for every pixel
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个像素进行预测
- en: Working with image segmentation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与图像分割一起工作
- en: Enlarging images with transposed convolutions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用转置卷积放大图像
- en: Using bounding boxes for object detection with Faster R-CNN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Faster R-CNN进行目标检测的边界框
- en: Filtering results to reduce false positives
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤结果以减少误报
- en: 'Imagine this: you want to build a system that counts the different kinds of
    birds in a park. You point a camera at the sky, and for each bird in this photograph,
    you want to know its species name. But what if there are no birds in the picture?
    Or just 1? Or 12? To accommodate these situations, you need to first detect each
    bird in the image and then classify each detected bird. This two-step process
    is known as *object detection*, and it comes in many forms. Broadly, they all
    involve identifying the subcomponents of an image. So instead of generating one
    prediction per image, which is what our models have done so far, the system generates
    many predictions from a single image.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：你想要构建一个系统，用来统计公园里不同种类的鸟。你将相机对准天空，对于照片中的每只鸟，你都想知道它的物种名称。但是，如果图片中没有鸟？或者只有1只？或者12只？为了适应这些情况，你需要首先检测图像中的每只鸟，然后对检测到的每只鸟进行分类。这个两步过程被称为*目标检测*，它有多种形式。广义上，它们都涉及识别图像的子组件。因此，系统不是对每张图像生成一个预测，这是我们迄今为止模型所做的那样，而是从单张图像生成多个预测。
- en: Even when improved with data augmentation, better optimizers, and residual networks,
    the image classification models we built earlier all assume that the image is
    of a desired class. By this, we mean the image content matches the training data.
    For example, our MNIST model assumes that an image *always* contains a digit,
    and the digit is one of 0 through 9\. Our normal CNN models simply do not have
    the concept that an image could be empty or have multiple digits. To handle these
    cases, our model needs to be able to detect what is contained in a single image,
    and where in that image those things are located. Object detection is how we handle
    this problem.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 即使通过数据增强、更好的优化器和残差网络进行改进，我们之前构建的图像分类模型都假设图像是期望的类别。通过这种方式，我们的意思是图像内容与训练数据匹配。例如，我们的MNIST模型假设图像*总是*包含一个数字，而这个数字是0到9之间的一个。我们的普通CNN模型根本不具备图像可能是空的或包含多个数字的概念。为了处理这些情况，我们的模型需要能够检测单张图像中包含的内容，以及这些内容在图像中的位置。目标检测就是解决这个问题的方法。
- en: In this chapter, we learn about two approaches used for object-detection tasks.
    First we go into the details of *image segmentation*, which is an expensive but
    simpler approach. Similar to how the autoregressive models in chapter 7 made a
    prediction for *every* item in a sequence, image segmentation makes a prediction
    for *every* pixel in an image. Image segmentation is expensive because you need
    someone to label every individual pixel in an image. The expense can often be
    justified by the effectiveness of the results, ease of implementing segmentation
    models, and applications that need that level of detail. To improve image segmentation,
    we learn about a *transposed* convolution operation that allows us to undo the
    shrinking effect of pooling so that we can get the benefits of pooling and still
    make a prediction at every pixel. With this in hand, we build a foundational architecture
    called *U-Net* that has become a de facto design approach for image segmentation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习两种用于目标检测任务的方法。首先，我们深入了解*图像分割*，这是一种成本较高但较为简单的方法。类似于第7章中自回归模型对序列中的每个项目进行预测，图像分割对图像中的每个像素进行预测。图像分割成本较高，因为你需要有人为图像中的每个单独像素进行标注。这种成本往往可以通过结果的有效性、实现分割模型的可操作性以及需要这种程度细节的应用来证明其合理性。为了提高图像分割，我们学习了*转置卷积*操作，它允许我们撤销池化操作的缩小效果，这样我们就可以获得池化的好处，同时仍然可以在每个像素上进行预测。有了这个，我们构建了一个名为*U-Net*的基础架构，它已成为图像分割的事实上的设计方法。
- en: 'The second half of this chapter moves beyond per-pixel predictions to a variable
    number of predictions. This is done with an option that is less precise but also
    less expensive to label: *bounding box*-based object detection. Bounding box labels
    are boxes that are *just* large enough to capture a whole object within an image.
    This is easier to label: you only need to click and drag a box (sometimes crudely)
    around the object. But effective object-detection models are difficult to implement
    and expensive to train. Since object detection is so challenging to implement
    from scratch, we learn about the *region proposal*-based detector that is built
    into PyTorch. The region proposal approach is widely used and readily available,
    so we cover the details about *how* it works that generalize to other approaches.
    We’ll skip the minute details that make a specific implementation, but I’ll provide
    references if you want to learn more.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的后半部分超越了逐像素预测，转向了可变数量的预测。这是通过一个不太精确但标注成本也较低的选择实现的：基于**边界框**的对象检测。边界框标签是刚好足够大的框，可以捕捉到图像中整个对象。这更容易标注：你只需要点击并拖动一个框（有时可能比较粗糙）围绕对象。但是，有效的对象检测模型难以实现且训练成本高昂。由于从头开始实现对象检测非常具有挑战性，我们学习了PyTorch内置的基于**区域提议**的检测器。区域提议方法被广泛使用且易于获取，因此我们涵盖了其**如何**工作的细节，这些细节可以推广到其他方法。我们将跳过那些使特定实现变得复杂的细节，但如果你想了解更多，我会提供参考。
- en: 8.1 Image segmentation
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 图像分割
- en: Image segmentation is a simple approach to finding objects in an image. Image
    segmentation is a classification problem, but instead of classifying the whole
    image (what we have been doing with MNIST), we classify *every pixel*. So a 200
    × 200 image will have 200 × 200 = 40, 000 classifications. The classes in image
    segmentation tasks are usually different kinds of objects we could detect. For
    instance, the image in figure 8.1 has a horse, a person, and some cars as class
    objects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是寻找图像中对象的一种简单方法。图像分割是一个分类问题，但与我们对MNIST所做的方法不同，我们是对**每个像素**进行分类。因此，一个200
    × 200的图像将有200 × 200 = 40,000次分类。图像分割任务中的类别通常是我们可以检测到的不同类型的对象。例如，图8.1中的图像有一个马、一个人和一些汽车作为类别对象。
- en: '![](../Images/CH08_F01_Raff.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F01_Raff.png)'
- en: Figure 8.1 Example of an input image (on the left) with the segmented ground-truth
    labels (on the right) from the PASCAL VOC 2012 dataset. Every pixel has been given
    a class, with a default “no class” or “background” class given to pixels that
    are not part of a labeled object.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1展示了来自PASCAL VOC 2012数据集的输入图像（左侧）和分割后的真实标签（右侧）。每个像素都被赋予了一个类别，对于不属于标记对象的像素，默认为“无类别”或“背景”类别。
- en: The goal is to produce the *ground truth*, where *every pixel* is classified
    as belonging to either a person, a horse, a car, or the background. We have a
    classification problem with four unique classes; it just happens to be that a
    single input involves making many predictions. So if we have a 128 × 128 image,
    we have 128² = 16, 384 classifications to perform.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是生成**真实标签**，其中**每个像素**都被分类为属于人、马、汽车或背景。我们有一个具有四个独特类别的分类问题；碰巧单个输入涉及到做出许多预测。因此，如果我们有一个128
    × 128的图像，我们就需要进行128² = 16,384次分类。
- en: 'If we can successfully segment an image, we can then perform object detection.
    For figure 8.1, we can find the person blob (pink pixels connected together) to
    determine that a person exists in the image and where they exist within it. Segmentation
    can also be the goal in and of itself. For example, medical[¹](#fn27) doctors
    may want to identify the percentage of tumor cells in an image of a cell biopsy
    to determine how someone’s cancer is progressing (larger percentage than before
    means it’s growing and getting worse; smaller percentage means the cancer is shrinking
    and the treatment is working). Another common task in medical research is to manually
    label the number of different types of cells in an image and use the relative
    numbers of cell types to determine a patient’s overall health or other medical
    properties. In these tasks, we don’t just want to know where objects are: we also
    want to know precisely how big they are and their relative proportions within
    an image.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够成功分割图像，那么我们可以执行对象检测。对于图8.1，我们可以找到连接在一起的粉红色像素块（person blob），以确定图像中存在人，以及他们在图像中的位置。分割也可以是目的本身。例如，医学[¹](#fn27)医生可能想要识别细胞活检图像中肿瘤细胞的百分比，以确定某人的癌症是如何进展的（比之前更大的百分比意味着它在增长并变得更糟；更小的百分比意味着癌症正在缩小，治疗正在起作用）。在医学研究中，另一个常见的任务是手动标记图像中不同类型细胞的数量，并使用细胞类型的相对数量来确定患者的整体健康状况或其他医疗属性。在这些任务中，我们不仅想知道对象在哪里：我们还想知道它们在图像中的确切大小以及它们之间的相对比例。
- en: The task of image segmentation is a great opportunity to use a convolutional
    neural network. A CNN is designed to produce *local* outputs for a *local* region
    of the input. So our strategy for designing an image segmentation network will
    use convolutional layers as the output layer, too, instead of an `nn.Linear` layer
    as we did before. When we design a network that contains only convolutional layers
    (in addition to nonlinearity and normalization layers), we call that network *fully
    convolutional*.[²](#fn28)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割的任务是使用卷积神经网络的一个绝佳机会。卷积神经网络（CNN）被设计用来对输入的局部区域产生*局部*输出。因此，我们设计图像分割网络的战略将使用卷积层作为输出层，而不是像之前那样使用`nn.Linear`层。当我们设计一个只包含卷积层（以及非线性层和归一化层）的网络时，我们称这样的网络为*全卷积网络*。[²](#fn28)
- en: '8.1.1  Nuclei detection: Loading the data'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 核检测：加载数据
- en: For our first introduction to image segmentation, we will use data from the
    2018 Data Science Bowl ([https://www.kaggle.com/c/data-science-bowl-2018](https://www.kaggle.com/c/data-science-bowl-2018)).
    The goal of this competition was to detect nuclei of cells and their respective
    sizes. But we will just segment the image for now. We will download this dataset,
    set up a `Dataset` class for the problem, and then specify a fully convolutional
    network to make predictions about the entire image. When you download the data
    and extract it, all the files should be in a stage1/train folder.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们对图像分割的第一次介绍，我们将使用2018年数据科学 Bowl（[https://www.kaggle.com/c/data-science-bowl-2018](https://www.kaggle.com/c/data-science-bowl-2018)）的数据。这个竞赛的目标是检测细胞核及其相应的大小。但我们现在只是对图像进行分割。我们将下载这个数据集，为这个问题设置一个`Dataset`类，然后指定一个全卷积网络来对整个图像进行预测。当你下载数据并解压时，所有文件都应该在stage1/train文件夹中。
- en: The data is organized into a number of paths. So if we have the path/folder
    data0, an image of a cell under a microscope is found in the path data0/images/some_file_name.png.
    For every nucleus in the image, there is a file under data0/masks/name_i.png.
    The `Dataset` object loads that in so we can get started. To make things simple
    for now, we do some normalization and prep in the `Dataset` class. We remove an
    alpha channel that comes with these images (usually for transparent images), reorder
    the channels to be the first dimension as PyTorch likes, and compute the label
    for each input. The way we prepare this data is summarized in figure 8.2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据被组织成多个路径。所以如果我们有路径/文件夹data0，那么在路径data0/images/some_file_name.png中可以找到显微镜下的细胞图像。对于图像中的每一个细胞核，在data0/masks/name_i.png下都有一个文件。`Dataset`对象加载这些文件，这样我们就可以开始工作了。为了简化当前的工作，我们在`Dataset`类中进行了某些归一化和准备。我们移除了这些图像附带的一个alpha通道（通常用于透明图像），重新排列通道，使其成为PyTorch喜欢的第一个维度，并计算每个输入的标签。我们准备这些数据的方式总结在图8.2中。
- en: '![](../Images/CH08_F02_Raff.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F02_Raff.png)'
- en: 'Figure 8.2 This is how we process the Data Science Bowl data for image segmentation.
    Each folder under the root directory has several subfolders: an images folder
    with one image (I know, confusing) and a masks folder that has a binary mask for
    each nucleus. The masks are separated for object detection (we’ll get to that
    later), so we merge them all into one image that indicates where masks are (class
    1) and are not (class 0).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 这是我们处理数据科学 Bowl数据以进行图像分割的方式。根目录下的每个文件夹都有几个子文件夹：一个包含一个图像的images文件夹（我知道，有点混乱）和一个包含每个核的二进制mask的masks文件夹。mask被分开用于对象检测（我们稍后会讨论），因此我们将它们全部合并到一个图像中，该图像指示mask的位置（类别1）和不存在的位置（类别0）。
- en: All the images in a subdirectory are the exact same size, as shown in figure
    8.2, but they have different sizes *between* directories. So for the sake of simplicity,
    we’ll resize everything to 256 × 256. That way, we don’t have to worry about padding
    images to make them the same size. To build a label, we want an image that is
    the same shape, with 0 for no class or 1 for a nucleus being present. We can do
    this by converting each image to an array of binary values called a *mask*, where
    1 = `True` = nuclei present. Then we can do a logical `or` operation on the masks
    to get one final mask that has a 1 at every pixel where a nucleus was present.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 子目录中的所有图像尺寸完全相同，如图8.2所示，但它们在不同目录之间的大小不同。因此，为了简化问题，我们将所有内容调整到256 × 256。这样，我们就不必担心填充图像以使它们具有相同的大小。为了构建标签，我们希望有一个形状相同的图像，其中0表示无类别或1表示存在核。我们可以通过将每个图像转换为名为*mask*的二进制值数组来实现这一点，其中1
    = `True` = 核存在。然后我们可以在mask上执行逻辑`or`操作，以获得一个最终mask，其中每个核存在的像素都有一个1。
- en: 'The following code is the class for the 2018 Data Science Bowl dataset. Our
    `Dataset` class goes through every mask and `or`s them together so we get a single
    mask showing every pixel that contains an object. This is done in`__getitem__`,
    which returns a tuple with the input image followed by the mask we want to predict
    (i.e., all pixels that contain nuclei):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是2018年数据科学 Bowl数据集的类。我们的`Dataset`类遍历每个mask并将它们`or`在一起，以便我们得到一个显示包含所有对象的像素的单个mask。这是在`__getitem__`中完成的，它返回一个包含输入图像和我们要预测的mask（即包含核的所有像素）的元组：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ There is only one image in each images path, so we grab the first thing we
    find with [0] at the end.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个images路径中只有一个图像，所以我们通过[0]在末尾找到我们找到的第一个东西。
- en: ❷ But there are multiple mask images in each mask path.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 但每个mask路径中都有多个mask图像。
- en: ❸ The image shape is (W, H, 4). The last dimension is an unused alpha channel.
    We trim off the alpha to get (W, H, 3).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 图像形状为(W, H, 4)。最后一个维度是一个未使用的alpha通道。我们剪掉alpha以获得(W, H, 3)。
- en: ❹ We want this to be (3, W, H), the normal shape for PyTorch.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们希望它是(3, W, H)，这是PyTorch的正常形状。
- en: '❺ Last step for the image: rescale it to the range [0, 1].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 图像的最后一步：将其缩放到范围[0, 1]。
- en: ❻ Every mask image has the shape (W, H), which has a value of 1 if the pixel
    is of a nucleus and a value of 0 if the image is background.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 每个mask图像的形状为(W, H)，如果像素是核，则其值为1，如果图像是背景，则其值为0。
- en: ❼ Since we want to do simple segmentation, we create a final mask that contains
    all nuclei pixels from every mask.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 由于我们只想进行简单的分割，我们创建一个包含每个mask中所有核像素的最终mask。
- en: ❽ Not every image in the dataset is the same size. To simplify the problem,
    we resize every image to be (256, 256). First we convert them to PyTorch tensors.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 数据集中的每个图像大小并不相同。为了简化问题，我们将每个图像调整大小为(256, 256)。首先，我们将它们转换为PyTorch张量。
- en: ❾ The interpolate function can be used to resize a batch of images. We make
    each image a “batch" of 1.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 可以使用interpolate函数来调整一批图像的大小。我们将每个图像变成一个“批次”1。
- en: ❿ The shapes are (B=1, C, W, H). We need to convert them back to FloatTensors
    and grab the first item in the batch. This will return a tuple of (3, 256, 256),
    (1, 256, 256).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 形状为(B=1, C, W, H)。我们需要将它们转换回FloatTensors并获取批次中的第一个项目。这将返回一个包含(3, 256, 256)，(1,
    256, 256)的元组。
- en: 8.1.2  Representing the image segmentation problem in PyTorch
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 在PyTorch中表示图像分割问题
- en: 'Now that we can load the dataset, let’s visualize some of the data . The corpus
    has cell images from a variety of sources: some look like they are black and white,
    while others have color from the dye used. The next block of code loads the data
    and displays the original image on the left and, on the right, the mask that shows
    exactly where all the nuclei are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经可以加载数据集了，让我们可视化一些数据。语料库包含来自各种来源的细胞图像：一些看起来像是黑白图像，而另一些则使用了染料而具有颜色。下面的代码块加载数据，并在左侧显示原始图像，在右侧显示显示所有核精确位置的mask：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Creates the Dataset class object
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建Dataset类对象
- en: ❷ Plots the original image
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制原始图像
- en: ❸ Plots the mask
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制掩码
- en: '![](../Images/CH08_UN01_Raff.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_UN01_Raff.png)'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Let’s plot a second image that is in color.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 让我们绘制一个彩色的第二张图像。
- en: '![](../Images/CH08_UN02_Raff.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_UN02_Raff.png)'
- en: 'As we can see, there are many types of input image slices. Some have many nuclei,
    some have few, and the nuclei can be close to each other or far apart. Let’s quickly
    create training and testing splits to work with, using a smaller batch size of
    16 images. We’re using smaller batches because these images are larger—256 × 256
    instead of just 28 × 28—and I want to make sure the batches fit onto your GPU
    even if Colab gives you one of the smaller instances:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，存在许多类型的输入图像切片。有些有很多细胞核，有些很少，细胞核可以彼此靠近或相隔很远。让我们快速创建训练和测试分割，使用较小的16图像批次大小来工作。我们使用较小的批次，因为这些图像较大——256
    × 256 而不是仅仅 28 × 28——并且我想确保即使 Colab 给你较小的实例，批次也能适合你的 GPU：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since these are color images, we use *C* = 3 channels for our input: red, green,
    and blue. I’ve arbitrarily chosen 32 filters for our convolutional layers. The
    last setup item in the following code is to use `BCEWithLogitLoss` instead of
    `CrossEntropyLoss`. The `BCE` part of the name stands for *binary* cross-entropy.
    It’s a specialized version of `CrossEntropyLoss` that only works for two-class
    problems. Because we *know* there are only two classes (nuclei and background),
    the output of our network can be 1 neuron per pixel for a Yes/No style prediction.
    If we used `CrossEntropyLoss`, we would need two outputs per pixel, and that’s
    a little uglier for our code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些是彩色图像，我们使用 *C* = 3 个通道作为输入：红色、绿色和蓝色。我随意选择了32个过滤器用于我们的卷积层。以下代码中的最后一个设置项是使用
    `BCEWithLogitLoss` 而不是 `CrossEntropyLoss`。名称中的 `BCE` 部分代表 *二元* 交叉熵。它是 `CrossEntropyLoss`
    的一个特殊版本，仅适用于双类问题。因为我们 *知道* 只有两个类别（细胞核和背景），所以我们的网络输出可以是一个像素一个神经元，用于是/否风格的预测。如果我们使用
    `CrossEntropyLoss`，我们每个像素就需要两个输出，这对我们的代码来说有点难看：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ How many channels are in the input?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入中有多少个通道？
- en: ❷ Smallest value of filters you should usually consider. If we wanted to try
    to optimize the architecture, we could use Optuna to pick a better number of filters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通常应该考虑的最小过滤器值。如果我们想尝试优化架构，我们可以使用 Optuna 来选择更好的过滤器数量。
- en: ❸ BCE loss implicitly assumes a binary problem.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ BCE 损失隐式地假设了一个二元问题。
- en: Note When you have only two classes, binary cross-entropy using`BCEWithLogitLoss`
    and `CrossEntropyLoss` will converge to the same result. They are *mathematically
    equivalent*, so the choice is coding preference. I prefer to use `BCEWithLogitLoss`
    for two-class problems because it makes it obvious that I’m working with a binary
    output/prediction the moment I see the loss function, which tells me a little
    more about the problem. In general, it’s good to give your classes names and write
    your code in ways that tell you about what is happening in your code. At some
    point, you will have to go back and look at the old code you wrote, and these
    details will help you remember what is going on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当你只有两个类别时，使用 `BCEWithLogitLoss` 和 `CrossEntropyLoss` 的二元交叉熵会收敛到相同的结果。它们在
    *数学上等价*，所以选择是编码偏好。我更喜欢在双类问题上使用 `BCEWithLogitLoss`，因为它让我一看到损失函数就清楚我在处理二元输出/预测，这让我对问题有更多的了解。一般来说，给你的类别命名并以告诉你在代码中发生什么的方式编写代码是好的。在某个时候，你将不得不回过头来看你以前写的旧代码，这些细节将帮助你记住发生了什么。
- en: 8.1.3  Building our first image segmentation network
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3  构建我们的第一个图像分割网络
- en: Because we need to make predictions for *every* pixel, our network’s output
    *f*(⋅) must have a shape with the same height and width as our original inputs.
    So if our input is (*B*, *C*, *W*, *H*), our output needs to be (*B*, *class*, *W*, *H*).
    The *number of channels can change* based on the number of classes. In general,
    we will have one channel for every class we could predict the input as. In this
    case, we have two classes, so we can use one output channel with the binary cross-entropy
    loss. Thus we have an output of shape (*B*, 1, *W*, *H*). If we *end* our network
    with a convolutional layer that has *only one filter*, our model’s final output
    will have only one channel. So, we use a convolutional layer as the last layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们需要对每个像素进行预测，所以我们的网络输出 *f*(⋅) 必须具有与原始输入相同的高度和宽度。因此，如果我们的输入是 (*B*, *C*, *W*, *H*)，我们的输出需要是
    (*B*, *class*, *W*, *H*)。通道数可以根据类别的数量而改变。一般来说，我们将为每个可以预测输入的类别有一个通道。在这种情况下，我们有两个类别，因此我们可以使用一个输出通道与二元交叉熵损失。因此，我们有一个形状为
    (*B*, 1, *W*, *H*) 的输出。如果我们以只有一个滤波器的卷积层结束我们的网络，我们的模型最终输出将只有一个通道。因此，我们使用卷积层作为最后一层。
- en: The easiest way to keep the same W and H values is to *never* use pooling and
    to always use padding so the output is the same size as the input. Remember from
    chapter 3 that using a filter size of k means setting padding = ⌊*k*/2⌋ will ensure
    that the height and width of the output are the same as the input. We also use
    that constraint in defining our network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 保持相同的 W 和 H 值的最简单方法是永远不使用池化，并且始终使用填充，以便输出与输入大小相同。记住，从第 3 章中，使用大小为 k 的滤波器意味着设置填充
    = ⌊*k*/2⌋ 将确保输出的高度和宽度与输入相同。我们也在定义我们的网络时使用这个约束。
- en: 'The following code incorporates both of these choices into a simple neural
    network. It follows our usual pattern of repeating convolutions, normalization,
    and then a nonlinearity:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将这两个选择结合到一个简单的神经网络中。它遵循我们通常的模式，即重复卷积、归一化和非线性：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Defines our helper function that creates a hidden layer for a CNN
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义我们的辅助函数，该函数为 CNN 创建一个隐藏层
- en: ❷ We aren’t setting the leak value, to make the code shorter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们没有设置泄漏值，以使代码更短。
- en: ❸ Specifies a model for image segmentation
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定一个用于图像分割的模型
- en: ❹ The first layer changes the number of channels to the large number.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第一层将通道数更改为大数。
- en: ❺ Creates five more hidden layers
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建了五个额外的隐藏层
- en: ❻ Makes a prediction for every location. We use one channel out since we have
    a binary problem and are using BCEWithLogitsLoss as our loss function. The shape
    is now (1, W, H).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对每个位置进行预测。我们使用一个通道，因为我们有一个二元问题，并且使用 BCEWithLogitsLoss 作为我们的损失函数。现在的形状是 (1,
    W, H)。
- en: ❼ Trains the segmentation model
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练分割模型
- en: 'Now that we have trained a model, let’s visually inspect some of the results.
    The following code shows how we can grab an item from the test dataset, push it
    through the model, and get a prediction. Since we use the binary cross-entropy
    loss, we need to use the `torch.sigmoid` (σ) function to convert the raw outputs
    (also called *logits*) into the right form. Remember that the sigmoid maps everything
    into the range [0,1], so the threshold of 0.5 tells us if we should go with a
    final answer of “nuclei present” or “not present.” Then we can plot the results
    showing the raw input (left), ground truth (center), and prediction (right) for
    the image:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个模型，让我们直观地检查一些结果。以下代码显示了如何从测试数据集中抓取一个项目，将其通过模型传递，并得到一个预测。由于我们使用二元交叉熵损失，我们需要使用
    `torch.sigmoid` (σ) 函数将原始输出（也称为 *logits*）转换为正确的形式。记住，sigmoid将所有内容映射到 [0,1] 的范围内，因此阈值为
    0.5 告诉我们是否应该选择“存在核”或“不存在”的最终答案。然后我们可以绘制结果，显示图像的原始输入（左）、真实值（中）和预测（右）：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Picks a specific example from the dataset that shows a particular result.
    Change this to look at other entries from the dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从数据集中选择一个特定示例，以显示特定结果。将其更改为查看数据集的其他条目。
- en: ❷ We don’t want gradients if we aren’t training, so no gradients please!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果我们不进行训练，我们不希望有梯度，所以请不要梯度！
- en: ❸ Pushes a test datapoint through the model. Remember, the raw outputs are called
    the logits.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将测试数据点推入模型。记住，原始输出称为 logits。
- en: ❹ Applies σ to the logits to make predictions and then applies a threshold to
    get a prediction mask.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将 σ 应用到 logits 以进行预测，然后应用阈值以获得预测掩码。
- en: ❺ Plots the input, ground truth, and prediction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 绘制输入、真实值和预测。
- en: ❻ First plots the original input to the network.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 首先绘制网络的原始输入。
- en: ❼ Second is the ground truth.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二是真实值。
- en: ❽ Third is the prediction our network made.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 第三是网络做出的预测。
- en: '![](../Images/CH08_UN03_Raff.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_UN03_Raff.png)'
- en: 'Overall, the results are very good. We even get most of the literal edge cases
    (nuclei at the border of the image) correct. Objects that occur on the edges of
    images are often harder to predict correctly. But there are some errors: for the
    really big nuclei, our segmentation model put a hole that does not belong. We
    also detected some nuclei that don’t exist. I’ve annotated the output with red
    arrows to highlight these mistakes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，结果非常好。我们甚至正确地处理了大部分字面意义上的边缘情况（图像边界的核）。出现在图像边缘的对象通常更难正确预测。但也有一些错误：对于非常大的核，我们的分割模型在错误的位置放置了一个不属于那里的洞。我们还检测到了一些不存在的核。我用红色箭头标注了输出结果以突出这些错误。
- en: Part of the issue may be that our receptive field for the network is too small
    to accurately handle large nuclei. For each layer of convolution, the *maximal*
    range is increased by ⌈*k*/2⌉. Since we have six convolutional layers, that gets
    us just barely 2 ⋅ 6 = 12 pixels of width. While the simple option would be to
    add more layers or increase the width of the convolutions, those can get expensive.
    On reason it’s so expensive may be that we are never doing any pooling, so each
    time we add a layer or double the number of filters, we increase the total memory
    used by our approach.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 部分问题可能在于我们网络的感受野太小，无法准确处理大型核。对于每一层的卷积，*最大范围*增加⌈*k*/2⌉。由于我们有六个卷积层，这仅使我们获得2⋅6=12像素的宽度。虽然简单的方法是添加更多层或增加卷积的宽度，但这些可能会变得昂贵。其中一个原因可能是因为我们从未进行过任何池化，所以每次我们添加一层或加倍滤波器的数量，我们都会增加我们方法使用的总内存。
- en: 8.2 Transposed convolutions for expanding image size
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 扩展图像大小的转置卷积
- en: We prefer to somehow use pooling so that we get the benefits of a smaller output
    (less memory) and larger receptive field, and then later *expand back up* to a
    larger form. We can do this with what is called a *transposed* convolution. In
    a normal convolution, the value of *one output* is determined by *multiple inputs*.
    Because each output has multiple inputs, the output is *smaller* than the input
    so each output gets its full contribution. An easy way to think about transposed
    convolutions is to imagine that *one input* contributes to *multiple outputs*.
    This is shown in figure 8.3 applied to a small 2 × 2 image. Because the transposed
    version has one input to multiple outputs, we need to make the output *larger
    than the original input* so each output that an input contributes to is represented.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更倾向于以某种方式使用池化，以便我们获得较小输出（更少内存）和较大感受野的好处，然后稍后*扩展回*更大的形式。我们可以通过所谓的*转置卷积*来实现这一点。在正常卷积中，*一个输出*的值由*多个输入*决定。因为每个输出有多个输入，所以输出*小于*输入，因此每个输出都得到其全部贡献。关于转置卷积的一个简单思考方式是想象*一个输入*对*多个输出*做出贡献。这如图8.3所示，应用于一个小2×2图像。因为转置版本有一个输入对应多个输出，所以我们需要使输出*大于原始输入*，以便每个输入贡献的输出都得到表示。
- en: '![](../Images/CH08_F03_Raff.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F03_Raff.png)'
- en: Figure 8.3 Step-by-step calculation of a transposed convolution. The green region
    on the left shows which part of the input is being used, and the orange on the
    right shows which part of the output is being altered. At every step, the input
    is multiplied by the filter and added to the output at the given location. Because
    the input expands by the size of the filter, the size of the output is larger
    than that of the input.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 转置卷积的逐步计算。左侧的绿色区域显示了正在使用的输入部分，右侧的橙色显示了正在改变的输出部分。在每一步中，输入乘以滤波器并加到给定位置的输出上。因为输入按滤波器的大小扩展，所以输出的尺寸大于输入。
- en: 'Figure 8.4 shows an example of convolution on top and transposed convolution
    on the bottom. Both are using the same image and filter in their respective patterns.
    Like a normal convolution, the transposed version adds together all of the contributions
    at each location to reach a final value. For this reason, notice that the *inside*
    area shown with a red dashed border has an *identical* result between normal and
    transposed convolution. The difference is how we interpret border cases: regular
    convolution shrinks, and transposed convolutions expand. Every transposed convolution
    has an equivalent regular convolution that has simply changed the amount of padding
    and other parameters applied.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4展示了顶部卷积和底部转置卷积的示例。两者都在各自的模式中使用相同的图像和滤波器。就像常规卷积一样，转置版本将每个位置的贡献相加以达到最终值。因此，请注意，用红色虚线框显示的*内部*区域在常规和转置卷积之间有*相同*的结果。区别在于我们如何解释边界情况：常规卷积缩小，而转置卷积扩展。每个转置卷积都有一个等效的常规卷积，只是改变了应用的一些填充和其他参数。
- en: '![](../Images/CH08_F04_Raff.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F04_Raff.png)'
- en: Figure 8.4 Example of a regular convolution (top) and transposed convolution
    (bottom) when applied to the same input image with the same filter. The regular
    convolution shrinks the output by the size of the filter, and the transposed version
    extends the output by the size of the kernel.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4展示了在相同输入图像和相同滤波器作用下，常规卷积（顶部）和转置卷积（底部）的示例。常规卷积通过滤波器的大小缩小输出，而转置版本通过内核的大小扩展输出。
- en: The important thing to remember here is that transposed convolutions give us
    a way to expand back up in size. In particular, we can add a *stride* to cause
    a doubling effect to reverse the halving effect caused by pooling. A *stride*
    is how much we slide our filter by when applying a convolution. By default, we
    use a stride *s* = 1, meaning we slide the filter over one position at a time.
    Figure 8.5 shows what happens when we use a stride *s* = 2. A regular convolution
    takes steps of 2 on the *input*, while a transposed one takes steps of 2 on the
    *output*. So a stride 2 *convolution halves the size*, and a stride 2 *transposed
    convolution doubles the size*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要记住的重要一点是，转置卷积为我们提供了一种扩大尺寸的方法。特别是，我们可以添加一个*步长*来产生加倍效果，以抵消池化引起的减半效果。*步长*是在应用卷积时我们的滤波器滑动的距离。默认情况下，我们使用步长*s*
    = 1，这意味着我们每次滑动滤波器一个位置。图8.5展示了当我们使用步长*s* = 2时会发生什么。常规卷积在*输入*上以2的步长前进，而转置卷积在*输出*上以2的步长前进。因此，步长2的*卷积减半了尺寸*，而步长2的*转置卷积加倍了尺寸*。
- en: '![](../Images/CH08_F05_Raff.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F05_Raff.png)'
- en: Figure 8.5 Example of how a stride of *s* = 2 impacts a regular and transposed
    convolution. The shaded regions show the input/output mappings. For convolution,
    the input moves the filter by two positions, making the output smaller. Transposed
    convolution still uses every input position, but the output is moved by two positions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5展示了步长*s* = 2如何影响常规和转置卷积。阴影区域显示了输入/输出映射。对于卷积，输入移动滤波器两个位置，使输出更小。转置卷积仍然使用每个输入位置，但输出移动两个位置。
- en: The way we incorporate transposed convolution into our architecture is that
    every few layers, we would do a round of pooling. If we do pooling in a 2 × 2
    grid (the standard), we double the width of the model’s receptive field. Each
    time we perform pooling, we end up looking at a higher-level view of the image.
    Once we reach halfway through the network, we start to perform transposed convolutions
    to get back up to the correct size. The layers after transposed convolutions give
    the model a chance to refine the high-level view. Similar to how we designed our
    autoencoders from the last chapter, we make the pooling and transpose rounds symmetric.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将转置卷积纳入架构的方式是，每隔几层，我们会进行一次池化。如果我们在一个2×2的网格中进行池化（标准做法），我们将模型的感受野宽度加倍。每次我们进行池化，我们最终会看到一个更高层次的图像视图。当我们达到网络的一半时，我们开始执行转置卷积以恢复到正确的尺寸。转置卷积之后的层给模型一个机会来细化高层次视图。类似于我们如何在上一个章节中设计我们的自编码器，我们使池化和转置轮次对称。
- en: 8.2.1  Implementing a network with transposed convolutions
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 实现带有转置卷积的网络
- en: 'With a transposed convolution, we can expand a network’s output, which means
    we can use pooling and then undo the reduction in width and height later. Let’s
    try it to see if this provides any real value to our model. In the interest of
    keeping these examples small and allowing them to run quickly, we do only one
    round of pooling and transposed convolution; but if you did more, you might see
    better results. The following code redefines our network with one round of max
    pooling and later one round of transposed convolution:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用转置卷积，我们可以扩展网络的输出，这意味着我们可以使用池化，然后在后面撤销宽度和高度的减少。让我们试一试，看看这能为我们的模型提供任何实际的价值。为了保持这些示例小且运行快速，我们只进行了一轮池化和转置卷积；但如果你做得更多，你可能会看到更好的结果。以下代码重新定义了我们的网络，包含一轮最大池化和随后的一轮转置卷积：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ The first layer changes the number of channels up to the large number.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一层将通道数增加到大量。
- en: ❷ Shrinks the height and width by 2
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 缩小高度和宽度各2
- en: ❸ Doubles the height and width, countering the effect of the single MaxPool2d
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将高度和宽度加倍，抵消单个MaxPool2d的效果
- en: ❹ Back to normal convolutions
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 回到正常的卷积
- en: ❺ Prediction for every location. Shape is now (B, 1, W, H).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 每个位置的预测。形状现在是（B，1，W，H）。
- en: 'Now that we have trained this new model, let’s try it on the same data and
    see what happens:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了这个新模型，让我们用相同的数据来测试一下看看会发生什么：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Same example as before
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与之前相同的示例
- en: ❷ We don’t want gradients if we aren’t training, so no gradients please!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果我们不进行训练，我们不想有梯度，所以请不要梯度！
- en: ❸ Pushes a test datapoint through the model. Raw outputs are called logits.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将测试数据点推过模型。原始输出称为logits。
- en: ❹ Applies σ to the logits to make predictions and then applies a threshold to
    get a prediction mask
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将σ应用于logits以进行预测，然后应用阈值以获得预测掩码
- en: ❺ Plots the input, ground truth, and prediction
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 绘制输入、真实值和预测值
- en: ❻ First plots the original input to the network
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 首先绘制网络的原输入
- en: ❼ Second is ground truth
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 第二个是真实值
- en: ❽ Third is the prediction our network made
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 第三个是我们网络做出的预测
- en: '![](../Images/CH08_UN04_Raff.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_UN04_Raff.png)'
- en: 'The hole has been patched; the nuclei object detection shows nice solid regions
    of white. The network has also done a slightly better job with some of the edge
    cases. Working on a smaller representation (the rounds after pooling) helps to
    encourage softer and smoother masks in our output. But we should never look at
    just a single image to decide if we’ve made an improvement, so let’s check the
    validation loss:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 漏洞已经修补；核对象检测显示了漂亮的白色区域。网络在处理一些边缘情况时也做得稍微好一些。在较小的表示（池化后的轮次）上工作有助于鼓励输出中更柔和、更平滑的掩码。但我们绝不能只看一张图片就决定我们是否有所改进，所以让我们检查验证损失：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/CH08_UN05_Raff.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_UN05_Raff.png)'
- en: According to the validation error, we have overall done a *slightly* better
    job than before. Something that is often just as important is learning speed,
    and we can see that this approach was able to make faster progress in fewer epochs
    of training. This faster learning is a significant benefit, and its importance
    grows as we work on harder and larger problems.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 根据验证误差，我们总体上比之前做得*稍微*好一些。同样重要的是学习速度，我们可以看到这种方法能够在更少的训练轮次中更快地取得进展。这种更快的学习是一个重要的好处，随着我们处理更难、更大的问题，其重要性也在增长。
- en: '8.3 U-Net: Looking at fine and coarse details'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 U-Net：观察细粒度和粗粒度细节
- en: Right now, we have two ways to model the image segmentation problem. The first
    approach, from section 8.1, uses no pooling and performs many rounds of convolutional
    layers. This makes a model that can look at minute fine-grained details but can
    literally miss the forest for the trees.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们有两种方式来建模图像分割问题。第一种方法，来自第8.1节，没有使用池化，执行了许多轮卷积层。这使得一个模型可以观察微小的细粒度细节，但可能会真的错过森林中的树木。
- en: The second, from section 8.2, uses rounds of max pooling followed by transposed
    convolution layers at the end of the architecture. You can think of this approach
    as progressively looking at higher-level regions of the image. The more rounds
    of pooling you do, the higher the model looks to make decisions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种，来自第8.2节，使用多轮最大池化，然后在架构的末尾使用转置卷积层。你可以将这种方法视为逐步查看图像的高级区域。你进行的池化轮数越多，模型在做出决策时考虑的级别就越高。
- en: Max pooling/up-sampling works well for detecting larger objects and broad borders,
    and the fine-grained model works better for small objects and nuanced object borders.
    We want a way to get the best of both worlds, capturing both fine details and
    high-level things simultaneously.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化/上采样对于检测较大的物体和宽边界效果很好，而细粒度模型对于小物体和细微的物体边界效果更好。我们希望有一种方法来获得两者的最佳效果，同时捕捉到细部和高级事物。
- en: 'We can achieve this best-of-both-worlds by including *skip connections* (from
    chapter 6) into our approach. Doing so creates an architecture approach known
    as *U-Net*,[³](#fn29) where we create three subnetworks to process the input:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将*跳过连接*（来自第6章）包含到我们的方法中来实现这种两全其美的效果。[³](#fn29)这样做创建了一种称为*U-Net*的架构方法，其中我们创建了三个子网络来处理输入：
- en: An input subnetwork that applies hidden layers to the full-resolution (lowest
    level of features) input.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入子网络，它将隐藏层应用于全分辨率（最低级别特征）输入。
- en: A bottleneck subnetwork that is applied after max pooling, allowing it to look
    at a lower resolution (higher-level features), and then uses a transposed convolution
    to expand its results back up to the same width and height as the original input.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个瓶颈子网络，它在最大池化后应用，允许它观察较低分辨率的（高级特征），然后使用转置卷积将其结果扩展回与原始输入相同的宽度和高度。
- en: An output subnetwork that combines the results from the two proceeding networks.
    This lets it get the best of both worlds, looking at low- and high-level details
    simultaneously.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出子网络，它结合了两个先前网络的结果。这使得它能够同时观察低级和高级细节。
- en: Figure 8.6 shows a single block of the U-Net style approach.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6展示了U-Net风格方法的一个单独块。
- en: '![](../Images/CH08_F06_Raff.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F06_Raff.png)'
- en: 'Figure 8.6 Design of a U-Net block, which is divided into three subnetworks.
    Each subnetwork has multiple convolutional hidden layers. The result of the first
    subnetwork goes to two locations: the second bottleneck subnetwork (after going
    through max pooling), and the third subnetwork after being combined with the second’s
    result.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6展示了U-Net块的架构设计，它分为三个子网络。每个子网络都有多个卷积隐藏层。第一个子网络的结果被发送到两个位置：第二个瓶颈子网络（在经过最大池化后），以及与第二个子网络的结果结合后的第三个子网络。
- en: Expanding this into a larger U-Net architecture is done by repeatedly making
    the bottleneck subnetwork be *another* U-Net block. This way, you get a network
    that learns to look at multiple different resolutions at once. When you draw the
    diagram of inserting U-Net blocks into U-Net blocks, you end up with a U shape
    as shown in figure 8.7\. This diagram also shows that each time we shrink the
    resolution by a factor of 2×, we tend to increase the number of filters by a factor
    of 2×. This way, each level of the network has a roughly comparable amount of
    work and computation to perform.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将其扩展为更大的U-Net架构是通过反复将瓶颈子网络变成**另一个**U-Net块来实现的。这样，你得到一个能够同时学习观察多个不同分辨率的网络。当你绘制将U-Net块插入到U-Net块中的图时，你最终得到一个如图8.7所示的U形。此图还显示，每次我们将分辨率缩小2倍时，我们倾向于将滤波器的数量增加2倍。这样，网络的每个级别都有大致相当的工作量和计算量。
- en: '![](../Images/CH08_F07_Raff.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F07_Raff.png)'
- en: Figure 8.7 Example of a U-Net style architecture. After some rounds of convolution,
    max pooling is used to shrink the image several times. Eventually, a transposed
    convolution up-samples the results, and every up-sample includes a skip connection
    to the previous result before pooling. The results are concatenated together and
    proceed to a new output round of convolutions. The architecture makes a U-like
    shape.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7展示了U-Net风格架构的示例。经过几轮卷积后，使用最大池化将图像缩小几次。最终，转置卷积将结果上采样，并且每次上采样都包括一个在池化之前连接到先前结果的跳过连接。结果被连接在一起，并进入新的卷积输出轮。这种架构形成了一个U形。
- en: Figure 8.7 shows one group of Conv2d, BatchNorm, and ReLu activation per input/output
    pair, but you could have any number of hidden layer blocks. While U-Net refers
    to both a *specific* architecture and a *style* of architecture, I’m going to
    refer to the style overall. In the next section, we define some code for implementing
    U-Net style models.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7显示了每个输入/输出对一组Conv2d、BatchNorm和ReLu激活，但你可以有任意数量的隐藏层块。虽然U-Net既指一个**特定**的架构，也指一种架构**风格**，但我会总体上指这种风格。在下一节中，我们将定义一些实现U-Net风格模型的代码。
- en: 8.3.1  Implementing U-Net
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 实现U-Net
- en: 'To make our implementation simpler, we take in the number of `in_channels`
    and use `mid_channels` as how many filters should be used in the convolutions.
    If we want the output to have a different number of channels, we use a 1 × 1 convolution
    to change `mid_channels` to `out_channels`. Since each block can have multiple
    `layers`, we make that an argument as well. The last thing we need is the `sub_network`
    used as the bottleneck. So our constructor documentation looks like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的实现更简单，我们接受`in_channels`的数量，并使用`mid_channels`作为卷积中应使用的滤波器数量。如果我们希望输出具有不同的通道数，我们使用1
    × 1卷积将`mid_channels`转换为`out_channels`。由于每个块可以有多个`layers`，我们也将它作为一个参数。最后，我们需要的是用作瓶颈的`sub_network`。因此，我们的构造函数文档如下：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Our class extends nn.Module; all PyTorch layers must extend this.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的这个类扩展了nn.Module；所有PyTorch层都必须扩展这个。
- en: 'Now let’s walk through the constructor’s contents. The input to a block (step
    1) will always have shape (*B*,in_channels,*W*,*H*) and produce a shape(*B*,mid_channels,*W*,*H*).
    But the output portion from step 3 will have two possible shapes: either (*B*,2⋅mid_channels,*W*,*H*),
    because it combines the results from step 1 and step 2, giving it 2× the number
    of channels;or (*B*,mid_channels,*W*,*H*) if there is no bottleneck. So we need
    to check whether we have a `sub_network` and change the number of inputs to the
    output block accordingly. Once that is done, we can build the hidden layers for
    steps 1 and 3\. For step 2, we use a subnetwork `self.bottleneck` to represent
    the model that is applied on the shrunken version of the image after `nn.MaxPool2d`
    is applied. The next block of code shows all of this and organizes step 1 into
    `self.in_model`, step 2 into `self.bottleneck`, and step 3 into `out_model`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐步了解构造函数的内容。一个块（步骤1）的输入将始终具有形状 (*B*,in_channels,*W*,*H*) 并产生形状(*B*,mid_channels,*W*,*H*)。但步骤3的输出部分将有两个可能的形状：要么是
    (*B*,2⋅mid_channels,*W*,*H*)，因为它结合了步骤1和步骤2的结果，使其通道数增加2倍；要么是 (*B*,mid_channels,*W*,*H*)
    如果没有瓶颈。因此，我们需要检查是否有`sub_network`，并相应地更改输出块的输入数量。一旦完成，我们就可以为步骤1和步骤3构建隐藏层。对于步骤2，我们使用子网络`self.bottleneck`来表示在应用`nn.MaxPool2d`后对图像的缩小版本上应用的模型。下一块代码显示了所有这些，并将步骤1组织到`self.in_model`中，步骤2组织到`self.bottleneck`中，步骤3组织到`out_model`中：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Starts preparing the layers used to process the input
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 开始准备用于处理输入的层
- en: ❷ If we have a subnetwork, we double the number of inputs to the output. So
    let’s figure that out now.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果我们有子网络，我们将输出输入的数量加倍。现在让我们来解决这个问题。
- en: ❸ Prepares the layers used to make the final output, which has extra input channels
    from any subnetwork
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 准备用于制作最终输出的层，该输出包含来自任何子网络的额外输入通道
- en: ❹ Makes the additional hidden layers used for the input and output
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建用于输入和输出的额外隐藏层
- en: ❺ Uses 1 × 1 convolutions to ensure a specific output size
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用1 × 1卷积确保特定的输出大小
- en: ❻ Defines our three total subnetworks. 1) in_model performs the initial rounds
    of convolution.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义我们的三个总子网络。1) in_model执行初始的卷积轮次。
- en: ❼ 2) Our subnetwork works on the max pooled result. We add the pooling and up-scaling
    directly into the submodel.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 2) 我们的子网络作用于最大池化结果。我们将池化和上采样直接添加到子模型中。
- en: ❽ Shrinks
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 缩小
- en: ❾ Processes the smaller resolution
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 处理较小的分辨率
- en: ❿ Expands back up
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 向上扩展
- en: ⓫ 3) The output model that processes the concatenated result, or just the output
    from in_model if no subnetwork was given
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 3) 处理连接结果的输出模型，或者如果没有给出子网络，则直接从in_model获取输出
- en: 'That gets all the hard coding out of the way. The last step is to implement
    the `forward` function that uses it. By organizing all of our parts into different
    `nn.Sequential` objects, this last step is fairly painless. We get the full-scale
    result by applying `in_model` on the input `x`. Next, we check whether a `bottleneck`
    is present, and if so, we apply it and concatenate that result with the full-scale
    one. Finally, we apply the `out_model`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就消除了所有硬编码。最后一步是实现使用它的`forward`函数。通过将所有部分组织成不同的`nn.Sequential`对象，这一步相当简单。我们通过应用`in_model`于输入`x`来获取全尺寸结果。接下来，我们检查是否有`bottleneck`，如果有，我们应用它并将结果与全尺寸结果连接。最后，我们应用`out_model`：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Computes the convolutions at the current scale. (B, C, W, H)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算当前尺度的卷积。（B, C, W, H）
- en: ❷ Checks if we have a bottleneck to apply
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查是否有瓶颈需要应用
- en: ❸ (B, C, W, H) shape because bottleneck does both the pooling and expansion
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, C, W, H) 形状，因为瓶颈同时进行池化和扩展
- en: ❹ Shapes (B, 2*C, W, H)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 形状 (B, 2*C, W, H)
- en: ❺ Computes the output on the concatenated (or not!) result
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算连接（或未连接！）结果的输出
- en: 'This gives us a single U-Net block represented by the `UNetBlock2d` class.
    With this one `UNetBlock2d` module, we can implement an entire U-Net architecture
    by specifying that the `sub_network` itself be another `UNetBlock2d`. We can then
    just repeat this as many times as we like. The following code nests three `UNetBlock2d`s
    together, followed by one round of convolution to reach our desired output size:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个由`UNetBlock2d`类表示的单个U-Net块。通过这个单一的`UNetBlock2d`模块，我们可以通过指定`sub_network`本身也是一个`UNetBlock2d`来实现整个U-Net架构。然后我们可以根据需要重复这个过程。以下代码将三个`UNetBlock2d`嵌套在一起，随后进行一次卷积，以达到我们期望的输出大小：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Shape is now (B, 1, W, H)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形状现在是 (B, 1, W, H)
- en: 'Now that we have trained this model, let’s plot the results compared to our
    first two segmentation models. Notice that the U-Net approach is the best of both
    worlds, giving us both lower total loss and learning faster than either the fine-
    or coarse-grained models from earlier. U-Net converges to the same or better accuracy
    faster than the other approaches. It is also useful because we don’t have to guess
    exactly how many layers of pooling to use. We can simply pick a slightly larger
    number of rounds of pooling (U-Net blocks) than we think is necessary and let
    U-Net learn on its own if it should use the lower-resolution results. This is
    possible because U-Net maintains information from every level of resolution via
    the concatenation and output subnetworks from each block:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了这个模型，让我们将结果与我们的前两个分割模型进行比较。注意，U-Net方法是两者的最佳结合，既提供了更低的总体损失，又比之前精细或粗粒度的模型学习得更快。U-Net比其他方法更快地收敛到相同的或更好的精度。它还很有用，因为我们不必猜测确切需要使用多少层池化。我们可以简单地选择比我们认为必要的稍微多一点的池化（U-Net块）次数，然后让U-Net自己学习是否应该使用低分辨率的结果。这是可能的，因为U-Net通过每个块的拼接和输出子网络保持了每个分辨率级别的信息：
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/CH08_UN06_Raff.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_UN06_Raff.png)'
- en: 'The U-Net approach is a strong starting point for any image segmentation problem
    or any related tasks where you need to make a prediction about multiple points
    within an image. It is also a repetition of some similar concepts we have already
    learned: combining skip connections and 1 × 1 convolutions allows us to build
    a more expressive and powerful model. This also shows how we can adjust these
    concepts to apply certain kinds of priors to the data we are working on. We believed
    that we wanted small local details and coarser higher-level details to make better
    decisions, and we used skip connections and transposed convolution to embed that
    prior into the design of our architecture. Learning to recognize these opportunities
    and following through on them will make the biggest difference in your results
    compared to almost anything else you might do.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net方法对于任何图像分割问题或任何需要在对图像内的多个点进行预测的相关任务都是一个强大的起点。它也是我们之前已经学习的一些相似概念的重复：结合跳跃连接和1
    × 1卷积使我们能够构建一个更具有表现力和强大的模型。这也展示了我们如何调整这些概念，将某些先验应用到我们正在处理的数据中。我们相信我们想要小的局部细节和更粗糙的高级细节来做出更好的决策，因此我们使用了跳跃连接和转置卷积将这个先验嵌入到我们架构的设计中。学会识别这些机会并付诸实践，与几乎任何其他可能做的事情相比，将在你的结果中产生最大的差异。
- en: 8.4 Object detection with bounding boxes
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 使用边界框进行目标检测
- en: Image segmentation is conceptually simple, a single network running once to
    get predictions for every pixel, but getting every pixel labeled is expensive.
    We’ll now learn how a more involved approach, with multiple components working
    together, can perform bounding-box-based object detection. This strategy finds
    objects in one pass, and a second step determines what specific object is present
    at a location. That makes the labeling easier, but the network is more technically
    complicated.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割在概念上很简单，即单个网络运行一次，为每个像素获取预测，但为每个像素进行标记是昂贵的。现在我们将学习一个更复杂的方法，其中多个组件协同工作，以执行基于边界框的目标检测。这种策略在一次遍历中找到对象，第二步确定特定位置存在的具体对象。这使得标记更容易，但网络在技术上更复杂。
- en: In particular, we go over an algorithm called *Faster R-CNN*,[⁴](#fn30) which
    has become the de facto baseline for object detection. Most other approaches are
    variations on Faster R-CNN. Like most object detectors, Faster R-CNN uses the
    idea of *bounding boxes* for labels and prediction. Figure 8.8 shows a potential
    bounding box label and prediction for a stop sign. The bounding box approach is
    often preferred because it is cheaper and easier to label. You simply need software
    to annotate a box around an image (you can find some freely available online at
    [https://github.com/heartexlabs/awesome-data-labeling#images](https://github.com/heartexlabs/awesome-data-labeling#images)),
    which is much easier than painstakingly marking *every pixel* as required by image
    segmentation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们要介绍一个名为 *Faster R-CNN* 的算法[⁴](#fn30)，它已经成为对象检测的事实上的基准。大多数其他方法都是Faster R-CNN的变体。像大多数对象检测器一样，Faster
    R-CNN使用 *边界框* 的概念进行标签和预测。图8.8显示了交通标志的潜在边界框标签和预测。边界框方法通常更受欢迎，因为它更便宜且更容易标记。你只需要软件来标注图像周围的框（你可以在[https://github.com/heartexlabs/awesome-data-labeling#images](https://github.com/heartexlabs/awesome-data-labeling#images)上找到一些免费可用的软件），这比费力地标记每个像素所需的图像分割要容易得多。
- en: '![](../Images/CH08_F08_Raff.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F08_Raff.png)'
- en: Figure 8.8 A stop sign as the target for bounding-box-based object detection.
    The green box shows the ground truth, a box *just* large enough to contain the
    entire object. The red box shows a potential prediction, which is close but not
    quite correct. Water doesn’t obey the whimsical desires of the lowly stop sign.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 以交通标志为目标，基于边界框的对象检测。绿色框表示真实值，一个刚好足够包含整个对象的框。红色框表示一个潜在的预测，它很接近但并不完全正确。水不会遵从低级交通标志的
    whimsical desires。
- en: We want the model to detect objects by drawing a box around any object of interest.
    Since objects could be at weird sizes or angles, the goal is that the box should
    bound the object, so the box should be *just* big enough to fit the entire object
    within the box. We do not want the box to be any larger, because then we could
    cheat by marking a box the size of the image. We also don’t want the box to be
    any smaller because then it will miss part of the object, and we don’t want a
    single object to have multiple boxes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望模型通过围绕任何感兴趣的对象绘制一个框来检测对象。由于对象可能有奇怪的大小或角度，目标是框应该包围对象，因此框应该 *刚好* 足够大，可以容纳整个对象。我们不希望框更大，因为那样我们就可以通过标记与图像大小相同的框来作弊。我们也不希望框更小，因为那样它会错过对象的一部分，我们不希望单个对象有多个框。
- en: Getting a model to predict boxes around objects is tricky. How do we represent
    the box as a prediction? Does each pixel get its own box predicted? Won’t that
    lead to tons of false positives? How do we do it efficiently without writing a
    lot of horrendous `for` loops? There are many different approaches to these issues,
    but we’ll focus on Faster R-CNN. This algorithm’s high-level strategy is a good
    basis for understanding other more sophisticated object detectors and is built
    into PyTorch by default.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让模型预测围绕对象的边界框是一个难题。我们如何表示边界框作为预测？每个像素都得到自己的边界框预测吗？这不会导致大量误报吗？我们如何高效地完成这项工作，而不需要编写大量的糟糕的`for`循环？对于这些问题有众多不同的方法，但我们将重点关注Faster
    R-CNN。这个算法的高级策略是理解其他更复杂的对象检测器的好基础，并且默认情况下集成在PyTorch中。
- en: 8.4.1  Faster R-CNN
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 Faster R-CNN
- en: 'Let’s say we have a self-driving car, and we want the car to stop at stop signs
    because we don’t want to go to jail for developing a car that plows through intersections.
    We need as many images as possible that both include and do not include stop signs
    and where the stop signs have had a box drawn around them. Faster R-CNN is a complicated
    algorithm with many parts, but by this point you’ve learned enough to understand
    all the parts that make the whole. Faster R-CNN approaches this problem in three
    steps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一辆自动驾驶汽车，我们希望汽车在交通标志处停车，因为我们不希望因为开发了一辆横冲直撞的汽车而入狱。我们需要尽可能多的图像，这些图像既包含又不含交通标志，并且交通标志周围已经画了框。Faster
    R-CNN是一个复杂的算法，有很多部分，但到这一点，你已经学到了足够多的知识来理解构成整体的所有部分。Faster R-CNN通过三个步骤来解决这个问题：
- en: Process the image and extract features.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理图像并提取特征。
- en: Use those features to detect potential/proposed objects.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些特征来检测潜在/提议的对象。
- en: Take each potential object and decide what object it is, or if it isn’t an object
    at all.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个潜在对象进行判断，确定它是什么对象，或者它根本就不是对象。
- en: 'Figure 8.9 gives an overview of the Faster R-CNN algorithm; we go into the
    details in a moment. The three steps we described can be broken into three subnetworks:
    a *backbone network* to extract feature maps, a *region proposal network* (RPN)
    to find objects, and a *region of interest pooling* (RoI pooling or just RoI)
    network that predicts what type of object is being looked at. Faster R-CNN is
    an extension of what we have learned because the backbone network is a fully convolutional
    network like we used for image segmentation, except we have the final convolution
    layer output some number of *C*′ channels instead of just 1 channel.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9给出了Faster R-CNN算法的概述；我们稍后会详细介绍。我们描述的三个步骤可以分解为三个子网络：一个用于提取特征图的*主干网络*，一个用于寻找对象的*区域建议网络*（RPN），以及一个预测正在观察的对象类型的*感兴趣区域池化网络*（RoI池化或简称RoI）网络。Faster
    R-CNN是对我们所学内容的扩展，因为主干网络是一个类似于我们用于图像分割的全卷积网络，除了我们让最终的卷积层输出一些*C*′通道而不是仅仅1个通道。
- en: '![](../Images/CH08_F09_Raff.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F09_Raff.png)'
- en: Figure 8.9 Diagram of a Faster R-CNN being applied to the stop sign from earlier.
    The backbone network scans the image and is the largest network, doing the heavy
    lifting to produce good features for the rest of the algorithm. The region proposal
    network comes next and is also fully convolutional but very small. It reuses the
    work of the backbone’s feature map to make predictions or proposals for where
    objects *may* be in the image. Finally, a region of interest network takes every
    subregion of the feature map that corresponds to one of the proposals and makes
    a final determination about whether an object is present—and, if so, what object
    it is.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9展示了Faster R-CNN应用于之前提到的停车标志的示意图。主干网络扫描图像，是最大的网络，承担了产生良好特征的重任，以便算法的其他部分可以使用。接下来是区域建议网络，它也是全卷积的，但非常小。它重用主干的特征图来做出预测或提出可能存在于图像中的对象的位置。最后，一个感兴趣区域网络对特征图的每个子区域进行最终判断，确定是否存在对象——如果存在，它是什么对象。
- en: We will not implement Faster R-CNN from scratch because it has a number of important
    details and takes hundreds of lines of code to implement in total. But we’ll review
    all the critical components because so many other object detectors build on this
    approach. Faster R-CNN is also built into PyTorch, so you don’t have to do as
    much work to use it yourself. The following sections summarize how the backbone,
    region proposal, and region of interest subnetworks work, in the order used.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会从头开始实现Faster R-CNN，因为它包含许多重要的细节，并且总共需要数百行代码来实现。但我们将回顾所有关键组件，因为许多其他目标检测器都是基于这种方法构建的。Faster
    R-CNN也内置在PyTorch中，因此您在使用它时不需要做太多工作。以下几节将总结主干、区域建议和感兴趣区域子网络的工作方式，按照使用的顺序。
- en: Backbone
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 主干网络
- en: The backbone network is essentially *any* neural network that works like the
    object-segmentation network we just defined. It takes in an image with a width,
    height, and number of channels (*C*,*W*,*H*) and outputs a new feature map (*C*′,*W*′,*H*′).
    The backbone can have a different width and height as its output, as long as the
    output height and width are always a multiple of the input height and width (i.e.,
    if *W*′ = *W* ⋅ *z* then *H*′ = *H* ⋅ *z*, you have to maintain the ratio between
    width and height). These extra details are shown in figure 8.10.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 主干网络本质上是指任何像我们刚刚定义的对象分割网络那样工作的神经网络。它接收一个具有宽度、高度和通道数（*C*、*W*、*H*）的图像，并输出一个新的特征图（*C*′、*W*′、*H*′）。主干网络可以有不同的宽度和高度作为输出，只要输出的高度和宽度始终是输入高度和宽度的倍数（即，如果
    *W*′ = *W* ⋅ *z*，那么 *H*′ = *H* ⋅ *z*，你必须保持宽度和高度之间的比例）。这些额外的细节在图8.10中展示。
- en: '![](../Images/CH08_F10_Raff.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F10_Raff.png)'
- en: Figure 8.10 The backbone takes in the original image and creates a new image
    that is the feature map. It has multiple channels *C*′ (a user-defined value)
    and a new width *W*′ and height *H*′. The backbone is the only large network in
    Faster R-CNN and is meant to do all the heavy lifting so the other networks can
    be smaller and faster.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 主干网络接收原始图像并创建一个新的图像，即特征图。它具有多个通道 *C*′（用户定义的值）和新的宽度 *W*′ 和高度 *H*′。主干网络是Faster
    R-CNN中唯一的较大网络，旨在承担所有繁重的工作，以便其他网络可以更小、更快。
- en: The goal of the backbone is to *do all of the feature extraction* so the other
    subnetworks can be smaller and don’t need to be very complex. This makes it faster
    to run (only one big network to run once) and helps coordinate the two other subnetworks
    (they are working from the same representation). The backbone is an excellent
    location to use a U-Net style approach so that you can detect and distinguish
    between high-level objects (e.g., a car versus a cat doesn’t need much detail)
    and similar objects that can only be distinguished by looking at finer, low-level
    details (e.g., different breeds of dog, like the Yorkshire terrier and the Australian
    terrier).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 主干网络的目标是**提取所有特征**，这样其他子网络可以更小，且不需要非常复杂。这使得运行速度更快（只需运行一次一个大网络）并有助于协调另外两个子网络（它们从相同的表示开始工作）。主干网络是使用U-Net风格方法的一个绝佳位置，这样你可以检测并区分高级对象（例如，一辆车与一只猫不需要太多细节）以及只能通过观察更精细的低级细节来区分的相似对象（例如，不同品种的狗，如约克夏梗和澳大利亚梗）。
- en: Region proposal network (RPN)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 区域提议网络（RPN）
- en: 'Once the backbone has given us a feature-rich representation of our image as
    a (*C*′,*W*′,*H*′) tensor, the RPN determines whether an object exists at a specific
    location in the image. It does this by predicting two things:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦主干网络为我们提供了一个丰富的特征表示，即(*C*′,*W*′,*H*′)张量，RPN就会确定图像中特定位置是否存在物体。它是通过预测两件事来做到这一点的：
- en: A bounding box with four locations (top left, top right, bottom left, and bottom
    right)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有四个位置的边界框（左上角、右上角、左下角和右下角）
- en: A binary prediction to classify the box as “has object” or “does not have object”
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对箱子进行二分类预测，将其归类为“有物体”或“没有物体”
- en: In this case, it does not matter how many classes we are trying to predict or
    what specific object is present—the only goal is to determine whether an object
    exists and where the object is. Essentially, all the classes of the problem are
    merged into one superclass of “has object.”
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们试图预测多少个类别或具体存在什么物体并不重要——唯一的目标是确定是否存在物体以及物体的位置。本质上，所有的问题类别都被合并成一个“有物体”的超类。
- en: To make the model even more robust, we can make these six total predictions
    (4 box coordinates + 2 has object/no object) k times.[⁵](#fn31) The idea is to
    give the model k chances to predict the correct box shape during learning. A common
    default is to use *k* = 9 guesses. This allows the RPN to have multiple predictions
    about the size or shape of an object at a given location within the image and
    is a common optimization that is included in most implementations. The overall
    process of the RPN is shown in figure 8.11.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型更加鲁棒，我们可以将这些总共六个预测（4个框坐标+2个有物体/无物体）进行k次。[⁵](#fn31) 理念是在学习过程中给模型k次机会来预测正确的框形状。一个常见的默认值是使用*k*
    = 9次猜测。这允许RPN在图像的特定位置对物体的尺寸或形状进行多次预测，并且这是大多数实现中包含的常见优化。RPN的整体过程如图8.11所示。
- en: '![](../Images/CH08_F11_Raff.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F11_Raff.png)'
- en: 'Figure 8.11 Further details on how the region proposal network (RPN) works.
    It takes in the feature map from the backbone and makes multiple predictions at
    every location. It is predicting multiple boxes and, for each box, whether it
    contains an object. Boxes that receive an “object” prediction become the outputs
    of the RPN: a list of regions in the original input that may contain objects.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11进一步说明了区域提议网络（RPN）的工作原理。它从主干网络接收特征图，并在每个位置进行多次预测。它预测多个框，并且对于每个框，它预测该框是否包含物体。被预测为“物体”的框成为RPN的输出：原始输入中可能包含物体的区域列表。
- en: Because we are making k predictions per location, we *know* that there will
    be more false positives than necessary (if an object is at that location, only
    one of those k predictions will be the closest, and the others become false positives).
    This is part of why the predictions are called *proposals* instead of predictions.
    We expect that many more proposals will exist than actual objects, and later in
    the process, we will do more work to clean up these false positives.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在每个位置进行k次预测，我们**知道**会有比必要的更多误报（如果物体在那个位置，只有那k次预测中的一个是最接近的，其他的就变成了误报）。这也是为什么预测被称为**提议**而不是预测的部分原因。我们预计会有比实际物体多得多的提议，在后续的处理过程中，我们将进行更多的工作来清理这些误报。
- en: 'The RPN *could* be implemented with a single convolutional layer, using `nn.Conv2d(C, 6*k, 1)`,
    which would slide over every location in the image and make six predictions. This
    is a trick to use a one-by-one convolution to make local predictions. In real
    implementations, this is usually done with two layers instead, something like
    this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: RPN*可以*通过单个卷积层实现，使用`nn.Conv2d(C, 6*k, 1)`，它会在图像的每个位置滑动并做出六个预测。这是一个使用一对一卷积进行局部预测的技巧。在实际实现中，这通常通过两层来完成，类似于以下这样：
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ One layer for nonlinearities
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一层用于非线性
- en: ❷ Some code is added here to split the output into one group of four and another
    group of two. The approach depends on the implementation strategy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在这里添加了一些代码来将输出分成一组四个和另一组两个。这种方法取决于实现策略。
- en: Adding just one extra layer gives the model some nonlinear capability to make
    better predictions. We can use such a small network because the backbone network
    has already done the heavy lifting. So a small, fast, barely nonlinear RPN network
    runs on top of the backbone. The RPN’s job is to make predictions about *where*
    objects are and *what their shape is*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 只添加一个额外的层就给模型提供了一些非线性能力，使其能做出更好的预测。我们可以使用这样一个小的网络，因为骨干网络已经完成了繁重的工作。所以，一个小型、快速、几乎非线性的RPN网络在骨干网络上运行。RPN的任务是预测对象*在哪里*以及*它们的形状是什么*。
- en: Region of interest (RoI) pooling
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 区域兴趣（RoI）池化
- en: The last step is RoI pooling. The outputs of the RPN give us the locations for
    *W*′ ⋅ *H*′ ⋅ *k* total potential regions of interest. The RoI pooling takes each
    proposal from the RPN and grabs the corresponding region of the feature map (produced
    by the backbone) as its input. But each of these regions may be a different size,
    and regions may overlap. At training, we need to make predictions for all of these
    regions so that the model can learn to suppress false positives and detect false
    negatives. At test time, we only need to make predictions for the proposals that
    received larger scores from the RPN subnetwork. In both training and testing,
    we have an issue that the *size of the proposals is variable*. So we need to devise
    a network that can handle *variable-sized* inputs and still make a single prediction.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是RoI池化。RPN的输出给我们提供了*W*′ ⋅ *H*′ ⋅ *k* 个总潜在区域兴趣的位置。RoI池化从RPN中获取每个提议，并抓取由骨干网络产生的特征图对应的区域作为其输入。但是，这些区域可能大小不同，区域可能重叠。在训练时，我们需要对所有这些区域进行预测，以便模型可以学会抑制假阳性并检测假阴性。在测试时，我们只需要对RPN子网络得到较高分数的提议进行预测。在训练和测试中，我们都面临一个问题，即*提议的大小是可变的*。因此，我们需要设计一个可以处理*可变大小输入*并仍然做出单一预测的网络。
- en: To achieve this, we use *adaptive pooling*. In normal pooling, we say how much
    we want to shrink the input by (e.g., we usually shrink the image by a factor
    of 2). In adaptive pooling, we state how large we want the output to be, and adaptive
    pooling adjusts the shrinkage factor based on the input’s size. For example, if
    we want a 3 × 3 output, and the input is 6 × 6, adaptive pooling would be done
    in a 2 × 2 grid (6/2 = 3). But if the input was 12 × 12, the pooling would be
    done in a 4 × 4 grid to get 12/4 = 3. That way, we *always* get the same size
    output. The RoI pooling process is shown in more detail in figure 8.12.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们使用*自适应池化*。在正常池化中，我们说我们想通过多少来缩小输入（例如，我们通常将图像缩小2倍）。在自适应池化中，我们声明我们希望输出有多大，自适应池化根据输入的大小调整缩放因子。例如，如果我们想得到一个
    3 × 3 的输出，而输入是 6 × 6，自适应池化将在一个 2 × 2 的网格中进行（6/2 = 3）。但是，如果输入是 12 × 12，池化将在一个 4
    × 4 的网格中进行，以得到 12/4 = 3。这样，我们*总是*得到相同大小的输出。RoI池化过程在图8.12中更详细地展示。
- en: '![](../Images/CH08_F12_Raff.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F12_Raff.png)'
- en: Figure 8.12 The region of interest (RoI) network is the final step. The results
    from the RPN tell us which regions of the feature map may contain an object. Slices
    of these regions are extracted, resized to a standard small shape using `nn.AdaptiveMaxPool2d`,
    and then a small, fully connected hidden layer makes a prediction for each proposal.
    This is the final determination of what specific object is present.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 区域兴趣（RoI）网络是最后一步。RPN的结果告诉我们特征图中哪些区域可能包含一个对象。这些区域的切片被提取出来，使用`nn.AdaptiveMaxPool2d`调整大小到标准的小形状，然后一个小型全连接隐藏层对每个提议进行预测。这是确定具体存在什么对象的最终判断。
- en: 'The code for this RoI subnetwork might look something like this:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个RoI子网络的代码可能看起来像这样：
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ For an input of any W and H to have a shape of (B, C, 7, 7)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于任何 W 和 H 的输入，形状为 (B, C, 7, 7)
- en: ❷ now (B, C*7*7)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在是 (B, C*7*7)
- en: ❸ Assuming C=1 and 256 hidden neurons
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 假设 C=1 和 256 个隐藏神经元
- en: The RoI network starts with adaptive pooling to enforce a particular size for
    all predictions. This is an aggressively small 7 × 7 to make the RoI network small
    and fast to run since we have many proposals to process. It is so small that we
    follow it with just two rounds of `nn.Linear` layers instead of convolutional
    ones because it’s been shrunken so much. We can then apply this network to every
    region identified by the RPN network, regardless of the size, and get a prediction.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: RoI 网络从自适应池化开始，以强制所有预测具有特定的大小。这是一个非常小的 7 × 7，以使 RoI 网络小巧且运行速度快，因为我们有很多提议要处理。它如此之小，以至于我们只需跟随两轮
    `nn.Linear` 层而不是卷积层，因为已经缩小了很多。然后我们可以将这个网络应用于 RPN 网络识别的每个区域，无论大小，并得到预测。
- en: 8.4.2  Using Faster R-CNN in PyTorch
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 在 PyTorch 中使用 Faster R-CNN
- en: The details of implementing Faster R-CNN are not trivial, and it’s a tough algorithm
    to get completely right. (If you want the minutia, check out the article “Object
    Detection and Classification using R-CNNs” at [http://mng.bz/RqnK](http://mng.bz/RqnK).)
    Lucky for us, PyTorch has Faster R-CNN built in. Training it is expensive, though,
    so we’ll create a toy problem from MNIST to show its basics.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 Faster R-CNN 的细节并不简单，这是一个难以完全正确的算法。（如果你想要细节，请查看文章“使用 R-CNN 进行对象检测和分类”在 [http://mng.bz/RqnK](http://mng.bz/RqnK)。）幸运的是，PyTorch
    内置了 Faster R-CNN。尽管训练它很昂贵，但我们将从 MNIST 创建一个玩具问题来展示其基本原理。
- en: Our toy dataset will be a larger 100 × 100 image containing a random number
    of MNIST digits in random locations. The goal will be to detect where these images
    are and classify them correctly. Our dataset will return a tuple. The first item
    in the tuple is the 100 × 100 image on which we want to perform object detection.
    The second item in the tuple is a dictionary with two subtensors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的玩具数据集将是一个更大的 100 × 100 图像，其中包含随机数量的 MNIST 数字，位置也随机。目标是检测这些图像的位置并将它们正确分类。我们的数据集将返回一个元组。元组中的第一个项目是我们想要在上面进行对象检测的
    100 × 100 图像。元组中的第二个项目是一个包含两个子张量的字典。
- en: The first subtensor is indexed by the name `boxes`. If there are k objects in
    the image, this have a shape of (*k*,4) and store `float32` values, giving the
    four corners of the box. The second item is indexed by `labels` and is only necessary
    to perform training. This tensor looks more like the ones we have used before,
    with a shape of (*k*) `int64` values giving the class ID for each of the k objects
    to be detected.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个子张量通过名称 `boxes` 索引。如果图像中有 k 个对象，则其形状为 (*k*,4) 并存储 `float32` 值，给出框的四个角。第二个项目通过
    `labels` 索引，并且仅在进行训练时是必要的。这个张量看起来更像是之前使用过的，形状为 (*k*) `int64` 值，为要检测的每个 k 个对象提供类别
    ID。
- en: Implementing a R-CNN dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 R-CNN 数据集
- en: 'The following code block shows our `Dataset` class that implements our toy
    MNIST detector. Note the comments for computing the `offset`, as the bounding
    box corners are all absolute locations, so we need to compute two of the corners
    based on their relative distance to the starting corners:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块显示了我们的 `Dataset` 类，该类实现了我们的玩具 MNIST 检测器。注意计算 `offset` 的注释，因为边界框的角都是绝对位置，因此我们需要根据它们与起始角的相对距离来计算两个角：
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Creates a larger image that will store all the “objects" to detect
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个更大的图像，用于存储要检测的所有“对象”
- en: ❷ Samples up to self.toSample objects to place into the image. We are calling
    the PRNG, so this function is not deterministic.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从 self.toSample 中采样对象以放置到图像中。我们正在调用 PRNG，因此此函数不是确定性的。
- en: ❸ Picks an object at random from the original dataset, and its label
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从原始数据集中随机选择一个对象及其标签
- en: ❹ Gets the height and width of that image
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取该图像的高度和宽度
- en: ❺ Picks a random offset of the x- and y-axis, essentially placing the image
    at a random location
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 随机选择 x 轴和 y 轴的偏移量，实际上是将图像放置在随机位置
- en: ❻ Changes the padding at the end to make sure we come out to a specific 100,100
    shape
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 改变末尾的填充，以确保我们得到一个特定的 100,100 形状
- en: ❼ Creates the values for the “boxes." All of these are in absolute pixel locations.
    xmin is determined by the randomly selected offset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 为“boxes”创建值。所有这些都在绝对像素位置。xmin 由随机选择的偏移量确定。
- en: ❽ xmax is the offset plus the image’s width.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ xmax 是偏移量加上图像的宽度。
- en: ❾ y min/max follows the same pattern.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ y 的最小值/最大值遵循相同的模式。
- en: ❿ Adds to the box with the right label
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 向具有正确标签的框中添加
- en: Implementing a R-CNN collate function
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 R-CNN 的 collate 函数
- en: 'The Faster R-CNN implementation of PyTorch does not take input batches using
    the pattern we have used thus far (one tensor with everything padded to be the
    same size). The reason is that Faster R-CNN is designed to work with images of
    highly variable size, so we would not have the same W and H values for every item.
    Instead, Faster R-CNN wants a `list` of tensors and a `list` of dictionaries.
    We have to use a custom collate function to make that happen. The following code
    creates our training and test sets along with the needed collate function and
    loader:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的 Faster R-CNN 实现不使用我们迄今为止使用的模式（一个包含所有填充到相同大小的张量的张量）来接受输入批次。原因是 Faster
    R-CNN 是设计用来处理高度可变大小的图像的，因此我们不会为每个项目都有相同的 W 和 H 值。相反，Faster R-CNN 希望有一个张量列表和一个字典列表。我们必须使用自定义的
    collate 函数来实现这一点。以下代码创建我们的训练集和测试集，以及所需的 collate 函数和加载器：
- en: '[PRE18]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Examining the MNIST detection data
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 MNIST 检测数据
- en: 'Now we have all our data set up. Let’s look at some of the data to get a sense
    of it:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了所有数据。让我们看看一些数据，以了解其内容：
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Grabs an image with its labels
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 抓取带有标签的图像
- en: '![](../Images/CH08_UN07_Raff.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_UN07_Raff.png)'
- en: 'This image has three items in random locations: in this case, 8, 1, and 0\.
    Let’s look at the label object, `y`. It’s a Python `dict` object so we can see
    the keys and values and index the `dict` to look at the individual items it contains.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像有三个随机位置的项目：在这种情况下，8、1 和 0。让我们看看标签对象 `y`。它是一个 Python `dict` 对象，因此我们可以看到键和值，并索引
    `dict` 来查看它包含的各个项目。
- en: '[PRE20]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Prints out everything
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印出所有内容
- en: ❷ Prints a tensor showing the pixel locations of the corners for all three objects
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印出显示所有三个对象角落像素位置的张量
- en: ❸ Prints a tensor showing the labels for all three objects
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印出显示所有三个对象标签的张量
- en: All three outputs look sensible. The `boxes` component of `y` has shape (3,4),
    and `labels` has shape (3). If we compare the first row of `boxes` with the previous
    image, we can see that on the x-axis, it starts at around 30 and goes out near
    60, which corresponds to the values in the `boxes` tensor. The same holds for
    the height values (y coordinates), which start near 10 and go down near 30.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个输出看起来都很合理。`y` 的 `boxes` 部分的形状为 (3,4)，而 `labels` 的形状为 (3)。如果我们将 `boxes` 的第一行与前面的图像进行比较，我们可以看到在
    x 轴上，它大约从 30 开始，延伸到 60，这与 `boxes` 张量中的值相对应。同样适用于高度值（y 坐标），它从大约 10 开始，下降到 30。
- en: The important thing here is that the boxes and the labels occur in a consistent
    order. Label 4 could be the first label as long as the first row of `boxes` has
    the correct location of object 4.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是，框和标签以一致顺序出现。标签 4 可以是第一个标签，只要 `boxes` 的第一行具有对象 4 的正确位置。
- en: Defining a Faster R-CNN model
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个 Faster R-CNN 模型
- en: 'With this in place, let’s build a small backbone network to use. We simply
    iterate a number of “Conv, BatchNorm, ReLU” in blocks and slowly increase the
    number of filters. The last thing we need to do is add a `backbone.out_channels`
    value to the network we create. This tells the Faster R-CNN implementation the
    value of *C*′, which is the number of channels in the feature map produced by
    the backbone network. This is used to set up the RPN and RoI subnetworks for us.
    Both the RPN and RoI networks are so small that there aren’t many parameters for
    us to tune, and we don’t want to make them bigger because then training and inference
    would be very expensive. Here’s the code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，让我们构建一个小型主干网络来使用。我们简单地迭代多个“卷积、批归一化、ReLU”块，并逐渐增加过滤器的数量。我们需要做的最后一件事是将 `backbone.out_channels`
    值添加到我们创建的网络中。这告诉 Faster R-CNN 实现值 *C*′，这是主干网络产生的特征图中的通道数。这用于为我们设置 RPN 和 RoI 子网络。RPN
    和 RoI 网络都非常小，我们没有太多参数可以调整，我们也不想使它们更大，因为那样训练和推理将会非常昂贵。以下是代码：
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ How many channels are in the input?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入中有多少个通道？
- en: ❷ How many classes are there?
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 有多少个类别？
- en: ❸ How many filters in our backbone?
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们的主干网络中有多少个过滤器？
- en: ❹ Lets Faster R-CNN know exactly how many output channels to expect
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 让 Faster R-CNN 知道期望多少个输出通道
- en: Now we can define our Faster R-CNN model. We give it the backbone network and
    tell it how many classes exist and how to normalize the image (if your image values
    are in the range of [0,1], a mean of 0.5 and deviation of 0.23 are good defaults
    for most images). We also tell Faster R-CNN the minimum and maximum image sizes.
    To make this run faster, we are setting them both to the only image size of 100\.
    But for real data, this is used to try to detect objects at multiple scales; that
    way it can handle objects that are up close or far away. However, that requires
    more computation to run and even more to train.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义我们的 Faster R-CNN 模型。我们给它提供骨干网络，并告诉它存在多少个类别以及如何归一化图像（如果你的图像值在 [0,1] 范围内，对于大多数图像来说，0.5
    的平均值和 0.23 的偏差是好的默认值）。我们还告诉 Faster R-CNN 最小和最大的图像尺寸。为了使这个运行更快，我们将它们都设置为唯一的图像尺寸
    100。但对于真实数据，这是用来尝试在多个尺度上检测对象的；这样它就可以处理近距离或远距离的对象。然而，这需要更多的计算来运行，甚至更多的训练来训练。
- en: 'Note How should you set the min and max size for Faster R-CNN on real problems?
    Here’s a rule of thumb: if it’s too small for a human to do it, the network probably
    can’t do it, either. In your data, try looking for the objects you care about
    at different image resolutions. If the smallest resolution at which you can find
    your object is 256 × 256, that’s a good minimum size. If the largest resolution
    you need to make your images to spot the objects is 1024 × 1024, that is a good
    maximum size.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在现实问题中，应该如何设置 Faster R-CNN 的最小和最大尺寸？这里有一个经验法则：如果对于人类来说太小而无法完成，那么网络可能也无法完成。在你的数据中，尝试以不同的图像分辨率查找你关心的对象。如果你能找到你的对象的最小分辨率为
    256 × 256，那么这是一个好的最小尺寸。如果你需要将图像放大到 1024 × 1024 以发现对象，那么这是一个好的最大尺寸。
- en: 'As of PyTorch 1.7.0, you must also specify some information about the RPN and
    RoI networks when using your own backbone. This is done with the `AnchorGenerator`
    and `MultiScaleRoIAlign` objects, respectively. The `AnchorGenerator` controls
    the number of proposals made by the RPN, generating them at different aspect ratios
    (e.g., 1.0 is a square, 0.5 is a rectangle twice as wide as it is tall, 2 is a
    rectangle twice as tall as it is wide), and how many pixels tall those predictions
    should be (i.e., how big or small might your objects be? You might use between
    16 and 512 pixels for real-world problems). `MultiScaleRoIAlign` needs us to tell
    `FasterRCNN` which part of the backbone provides the feature maps (it supports
    multiple feature maps as a fancy feature), how large the RoI’s network will be,
    and how to handle fractional pixel locations predicted from the RPN. The following
    code has all of this put together:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 PyTorch 1.7.0，当使用自己的骨干网络时，你还必须指定一些关于 RPN 和 RoI 网络的信息。这是通过 `AnchorGenerator`
    和 `MultiScaleRoIAlign` 对象分别完成的。`AnchorGenerator` 控制RPN生成的提议数量，以不同的长宽比（例如，1.0 是一个正方形，0.5
    是一个长宽比为 2:1 的矩形，2 是一个长宽比为 1:2 的矩形）生成，以及那些预测应该有多高（即你的对象可能有多大或小？对于现实世界问题，你可能使用 16
    到 512 像素）。`MultiScaleRoIAlign` 需要我们告诉 `FasterRCNN` 骨干网络的哪个部分提供特征图（它支持多个特征图作为一项高级功能），RoI
    网络的大小，以及如何处理从 RPN 预测的分数像素位置。以下代码将这些内容组合在一起：
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ How many proposals k should be generated? Every aspect ratio will be one,
    and the process will be repeated for multiple image sizes. To make this run faster,
    we’re telling PyTorch to look only for square images that are 32 × 32 in size.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应该生成多少个提议 k？每个长宽比都会是一个，并且这个过程会针对多个图像尺寸重复进行。为了使这个运行更快，我们告诉 PyTorch 只查找大小为 32
    × 32 的正方形图像。
- en: '❷ Tells PyTorch to use the final output of the backbone as the feature map
    ([]); uses adaptive pooling down to a 7 × 7 grid (output_size=7). sampling_ratio
    is poorly named: it controls details on how the RoI grabs slices of the feature
    map when a fractional pixel location is predicted (e.g., 5.8 instead of 6). We
    are not going into those low-level details; 2 is a reasonable default for most
    work.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 告诉 PyTorch 使用骨干网络的最终输出作为特征图 ([]); 使用自适应池化到 7 × 7 网格（output_size=7）。sampling_ratio
    的命名不佳：它控制当预测分数像素位置时 RoI 如何从特征图中抓取切片的细节（例如，5.8 而不是 6）。我们不会深入这些低级细节；对于大多数工作，2 是一个合理的默认值。
- en: ❸ Creates the FasterRCNN object. We give it the backbone network, number of
    classes, min and max size at which to process images (we know all our images are
    100 pixels), a mean and standard deviation to subtract from the images, and the
    anchor generation (RPN) and RoI objects.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建 FasterRCNN 对象。我们给它提供骨干网络、类别数量、处理图像的最小和最大尺寸（我们知道所有我们的图像都是 100 像素），从图像中减去的平均值和标准差，以及锚生成（RPN）和
    RoI 对象。
- en: Implementing a Faster R-CNN training loop
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个 Faster R-CNN 训练循环
- en: Because of the unusual use of lists of tensors and lists of dictionaries, we
    can’t use our standard `train_network` function to handle this case. So, we write
    a minimal training loop to do that training for us. The main trick is to move
    every item in each list (inputs and labels) to the compute device we want to use.
    This is because the `.to(device)` method only exists for PyTorch `nn.Module` classes
    and the standard list and dictionary in Python don’t have these functions. Luckily,
    we defined a `moveTo` function early on that does this and works on lists and
    dictionaries.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对张量列表和字典列表的不寻常使用，我们无法使用我们的标准`train_network`函数来处理这种情况。因此，我们编写了一个最小的训练循环来为我们完成这项训练。主要技巧是将每个列表（输入和标签）中的每个项目移动到我们想要使用的计算设备上。这是因为`.to(device)`方法仅存在于PyTorch
    `nn.Module`类中，Python的标准列表和字典没有这些功能。幸运的是，我们早期定义了一个`moveTo`函数，它执行此操作并且适用于列表和字典。
- en: 'The second oddity is that the `FasterRCNN` object behaves differently in training
    mode and evaluation model. In training mode, it expects the labels to be passed
    in with the inputs so that it can compute the loss of every prediction. It also
    returns a list of every individual prediction loss instead of a single scalar.
    So, we need to add all of these individual losses to get a final total loss. The
    following code shows a simple loop for training one epoch of `FasterRCNN`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个奇怪之处在于，`FasterRCNN`对象在训练模式和评估模式下的行为不同。在训练模式下，它期望将标签与输入一起传递，以便它可以计算每个预测的损失。它还返回每个单独预测损失的列表，而不是单个标量。因此，我们需要将这些单个损失全部加起来以获得最终的损失总和。以下代码显示了用于训练一个`FasterRCNN`周期的简单循环：
- en: '[PRE23]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Moves the batch to the device we are using
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将批次移动到我们使用的设备
- en: ❶ RCNN wants model(inputs, labels), not just model(inputs).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ RCNN需要`model(inputs, labels)`，而不仅仅是`model(inputs)`。
- en: ❶ Computes the loss. RCNN gives us a list of losses to add up.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算损失。RCNN给我们一个损失列表来累加。
- en: ❶ Proceed as usual.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 按照常规进行。
- en: 'That’s all it takes to use the Faster R-CNN implementation provided by PyTorch.
    Now let’s see how well it did. First, set the model to `eval` mode, which changes
    how the Faster R-CNN implementation takes inputs and returns outputs:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用PyTorch提供的Faster R-CNN实现所需的所有步骤。现在让我们看看它的表现如何。首先，将模型设置为`eval`模式，这将改变Faster
    R-CNN实现处理输入和返回输出的方式：
- en: '[PRE24]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, let’s quickly grab an item from the test dataset and see what it looks
    like. In this case, we see it has three objects, 8, 0, and 4:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们快速从测试数据集中获取一个项目并看看它是什么样子。在这种情况下，我们看到有三个对象，8、0和4：
- en: '[PRE25]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ This is the ground truth we want to get back.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是我们要获取的地面实况。
- en: 'Let’s make a prediction. Since we are in `eval` mode, PyTorch wants a `list`
    of images to make predictions on. It also no longer needs a `labels` object to
    be passed in as a second argument, which is good because if we already knew where
    all the objects were, we wouldn’t be doing this:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个预测。由于我们处于`eval`模式，PyTorch希望有一个`list`图像列表来进行预测。它也不再需要将`labels`对象作为第二个参数传递，这很好，因为如果我们已经知道所有对象的位置，我们就不会这样做：
- en: '[PRE26]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Examining the results
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 检查结果
- en: 'And now we can look at our results. PyTorch’s implementation returns a `list`
    of `dict`s containing three items: `boxes` for the locations of predicted items,
    `labels` for the class predicted for each item, and `scores` for a confidence
    associated with each prediction. The following code shows the content of `pred`
    for this image:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以查看我们的结果。PyTorch的实现返回一个包含三个项目的`dict`列表：`boxes`用于预测项的位置，`labels`用于预测每个项目的类别，`scores`用于与每个预测相关的置信度。以下代码显示了此图像的`pred`内容：
- en: '[PRE27]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Each dictionary has a `boxes` tensor with (*k*′,4) shape for *k*′ predictions.
    The `labels` tensor has (*k*′) shape, giving the label it predicted for each object.
    Last, the `scores` tensor also has (*k*′) shape, and it returns a score in the
    range of [0,1] for each returned prediction. These scores are the “object” scores
    from the RPN subnetwork.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 每个字典都有一个`boxes`张量，其形状为(*k*′,4)，用于*k*′个预测。`labels`张量具有(*k*′)形状，为每个对象预测了标签。最后，`scores`张量也具有(*k*′)形状，并为每个返回的预测返回一个范围在[0,1]之间的分数。这些分数是RPN子网络的“对象”分数。
- en: 'In this case, the model is confident it found an 8 and a 4 ( ≥ 0.9 scores)
    but less confident about other classes. It’s easier to understand these results
    by printing them into a picture, so we quickly define a function to do that:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型对其找到了一个8和一个4（≥0.9分）的信心很高，但对其他类别的信心较低。通过将这些结果打印成图片，更容易理解这些结果，所以我们快速定义了一个函数来做这件事：
- en: '[PRE28]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Make a rectangle for the bounding box.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为边界框制作一个矩形。
- en: ❷ Add the label if given.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果提供了标签，则添加标签。
- en: ❸ Plot the image.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制图像。
- en: ❹ Grab the predictions.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取预测。
- en: ❺ For each prediction, plots if it has a high enough score.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对于每个预测，绘制其是否有足够高的分数。
- en: 'With this code in hand, we can plot the result of Faster R-CNN on this image.
    We can clearly see that the network did a good job on the 4 and 8 but completely
    missed the 0\. We also have spurious predictions from the network identifying
    subparts of the 4 and 8 as other digits. For example, look at the right half of
    the 4\. If you weren’t paying attention to the left half, you might say it was
    the digit 1—or you could be looking at the entire image and think it’s an incomplete
    9\. The 8 has similar issues. If you ignore the loop at the top-left, it looks
    like a 6; and if you ignore the right half of the 8, you could be forgiven for
    calling it a 9:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这段代码，我们可以在该图像上绘制Faster R-CNN的结果。我们可以清楚地看到，网络在4和8上做得很好，但完全错过了0。我们还从网络中得到了一些虚假预测，识别4和8的子部分为其他数字。例如，看看4的右半部分。如果你没有注意到左半部分，你可能会说它是数字1——或者你可能正在看整个图像，认为它是不完整的9。8也有类似的问题。如果你忽略左上角的环，它看起来像6；如果你忽略8的右半部分，你可能会被原谅为称之为9：
- en: '[PRE29]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/CH08_UN08_Raff.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_UN08_Raff.png)'
- en: Spurious overlapping objects are a common problem with object detectors. Sometimes
    these overlapping objects are predictions of the same object (e.g., identifying
    several 8s) or mislabeled predictions as we see here.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 假重叠对象是目标检测器中常见的问题。有时这些重叠对象是同一对象的预测（例如，识别几个8）或者像我们这里看到的误标记预测。
- en: 8.4.3  Suppressing overlapping boxes
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 抑制重叠盒子
- en: A simple and effective solution to this problem is to *suppress* overlapping
    boxes. How do we know which boxes to suppress? We want to make sure we pick the
    *correct* box to use, but we also don’t want to throw away boxes that are correctly
    predicting an adjacent object.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的简单有效的方法是*抑制*重叠的盒子。我们如何知道要抑制哪些盒子？我们想要确保我们选择的是*正确*的盒子来使用，但我们也不希望丢弃正确预测相邻对象的盒子。
- en: 'A simple approach called *non-maximum suppression* (NMS) can be used to do
    this. NMS uses the *intersection over union* (IoU) between two boxes to determine
    if they overlap *too* much. The IoU is a score: 1 indicates that the boxes have
    the exact same location, and 0 indicates no overlap. Figure 8.13 shows how it
    is computed.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为*非极大值抑制*（NMS）的简单方法可以用来做这个。NMS使用两个盒子之间的*交集与并集*（IoU）来确定它们是否重叠*太多*。IoU是一个分数：1表示盒子具有完全相同的位置，0表示没有重叠。图8.13显示了它是如何计算的。
- en: '![](../Images/CH08_F13_Raff.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F13_Raff.png)'
- en: Figure 8.13 The intersection over union score is calculated by dividing the
    area of overlap between two boxes by the area of the union between two boxes.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 交集与并集分数是通过将两个盒子之间的重叠面积除以两个盒子并集的面积来计算的。
- en: IoU divides the size of the intersection of two boxes by the area of the two
    boxes. This way, we get a size-sensitive measure of how similar two boxes’ positions
    are. NMS works by taking every pair of boxes that have an IoU greater than some
    specified threshold and keeping only the one with the *largest* score from the
    RPN network.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: IoU通过将两个盒子交集的大小除以两个盒子的大小来计算。这样，我们得到一个对两个盒子位置相似性的大小敏感度量。NMS通过取每对具有大于某个指定阈值的IoU的盒子，并只保留RPN网络中分数*最高*的一个来工作。
- en: 'Let’s quickly see how the NMS method works on our data. We can import it from
    PyTorch as the `nms` function:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看NMS方法在我们数据上的工作方式。我们可以从PyTorch导入它作为`nms`函数：
- en: '[PRE30]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s remind ourselves of the boxes we have and their associated scores by
    printing out these fields from the `pred` returned by our `model`:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过从`model`返回的`pred`中打印出这些字段来提醒自己我们拥有的盒子和它们相关的分数：
- en: '[PRE31]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `nms` function happily takes the tensor of boxes as the first argument
    and the tensor of scores as the second. The third and final argument is the threshold
    for calling two boxes different objects. The following code says that if the IoU
    between two boxes is 50% or more, they are the same item, and we should keep the
    box with the highest score:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`nms`函数愉快地接受盒子张量作为第一个参数，分数张量作为第二个参数。第三个和最后一个参数是调用两个盒子为不同对象的阈值。以下代码表示，如果两个盒子的IoU达到50%或更高，它们是同一项，我们应该保留分数最高的盒子：'
- en: '[PRE32]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We use a threshold of 50% overlap, and `nms` returns a tensor of an equal or
    smaller size, telling us which indices should be kept. In this case, it says to
    keep boxes 0 and 1, which had the highest scores.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用50%的重叠阈值，`nms`返回一个大小相等或更小的张量，告诉我们应该保留哪些索引。在这种情况下，它表示应该保留分数最高的盒子0和1。
- en: 'Let’s modify our prediction function to use NMS to clean up the output from
    Faster R-CNN. We also add a `min_score` flag, which we can use to suppress predictions
    that are unlikely to be meaningful:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改我们的预测函数，使用NMS清理Faster R-CNN的输出。我们还添加了一个`min_score`标志，我们可以用它来抑制不太可能是有意义的预测：
- en: '[PRE33]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we again plot this image using our improved `showPreds` function, and
    we see a better and cleaner result: just the 4 and 8 on their own. Alas, the 0
    is still undetected, and there is nothing we can do to fix that besides more data
    and more epochs of training:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们再次使用改进的`showPreds`函数绘制此图像，我们看到更好的、更干净的结果：只有4和8单独存在。唉，0仍然未被检测到，我们除了更多数据和更多训练轮次外，别无他法来修复这个问题：
- en: '[PRE34]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/CH08_UN09_Raff.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_UN09_Raff.png)'
- en: 8.5 Using the pretrained Faster R-CNN
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 使用预训练的Faster R-CNN
- en: 'PyTorch also provides a pretrained Faster R-CNN model. It’s trained on a dataset
    called COCO, and we can instantiate it and see the class names used for this model:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了一个预训练的Faster R-CNN模型。它是在一个称为COCO的数据集上训练的，我们可以实例化它并查看此模型使用的类名：
- en: '[PRE35]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ The R-CNN detector is set up for a specific set of classes. You can reuse
    it for your own problems by setting num_classes=10 and pretrained_backbone=True
    and then training it with your data as we did with MNIST.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ R-CNN检测器是为一组特定的类设置的。你可以通过设置num_classes=10和pretrained_backbone=True，然后用你的数据训练它来为你的问题重用它，就像我们用MNIST做的那样：
- en: 'We set this model in `eval` mode since it does not need training, and we also
    define the following `NAME` list that contains the class names for all the objects
    that this pretrained R-CNN knows how to detect:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此模型设置为`eval`模式，因为它不需要训练，我们还定义了以下`NAME`列表，其中包含所有预训练R-CNN知道如何检测的对象的类名：
- en: '[PRE36]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '❶ COCO_INSTANCE_CATEGORY_NAMES. These come from the PyTorch documentation:
    [pytorch.org/vision/0.8/models.html](https://pytorch.org/vision/0.8/models.html).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ COCO_INSTANCE_CATEGORY_NAMES。这些来自PyTorch文档：[pytorch.org/vision/0.8/models.html](https://pytorch.org/vision/0.8/models.html)。
- en: 'Let’s try downloading some images from the internet and see how well our model
    does. Keep in mind that random images contain lots of things this algorithm has
    *never* seen before. This will give you some ideas about how you might be able
    to use Faster R-CNN models in the future and interesting ways in which they can
    fail. The following code imports some libraries for grabbing images from a URL,
    as well as three URLs to get some images in which to try to detect objects. Feel
    free to change which URL is used or add your own URLs to try the object detector
    on something different:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试从互联网上下载一些图像，看看我们的模型表现如何。请记住，随机图像包含许多这个算法以前从未见过的东西。这将给你一些关于你未来如何可能使用Faster
    R-CNN模型以及它们可能以何种有趣方式失败的想法。以下代码导入了一些用于从URL抓取图像的库，以及三个用于尝试检测对象的URL：
- en: '[PRE37]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once we have loaded the image, we reformat it the way PyTorch’s pretrained
    model wants. This includes normalizing the pixel values to the range [0,1] and
    reordering the dimensions to be channel, width, height. That’s done in the following
    code, after which we make our prediction:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们加载了图像，我们就按照PyTorch预训练模型的要求重新格式化它。这包括将像素值归一化到[0,1]的范围内，并重新排列维度以成为通道、宽度、高度。以下代码完成了这项工作，之后我们进行预测：
- en: '[PRE38]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Passes the image to the model
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像传递到模型
- en: 'Now we can check out the results. You may find that you need a different `nms`
    threshold or `min_score` for each image to get the best results. Tuning these
    parameters can be very problem-dependent. It depends on the relative cost of false
    positives versus false negatives, the difference between the training images in
    style/content versus the testing images, and ultimately *how* you will be using
    your object detector:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查结果了。你可能发现你需要为每张图像设置不同的`nms`阈值或`min_score`才能获得最佳结果。调整这些参数可能非常依赖于问题。它取决于假阳性与假阴性的相对成本，训练图像在风格/内容上与测试图像之间的差异，以及你最终将如何使用你的目标检测器：
- en: '[PRE39]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](../Images/CH08_UN10_Raff.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_UN10_Raff.png)'
- en: Exercises
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在Manning在线平台Inside Deep Learning Exercises上分享和讨论你的解决方案([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945))。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到作者认为哪些是最好的。
- en: Now that you know how to enlarge a tensor after pooling, you can implement a
    convolutional autoencoder using only the bottleneck approach. Go back to chapter
    7 and reimplement a convolutional autoencoder by using two rounds of pooling in
    the encoder countered by two rounds of transposed convolutions in the decoder.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何在池化后扩大张量，你可以仅使用瓶颈方法实现卷积自动编码器。回到第7章，通过在编码器中使用两轮池化，在解码器中使用两轮转置卷积来重新实现卷积自动编码器。
- en: 'You may have noticed that a transposed convolution can create unevenly spaced
    *artifacts* in its output, which occur in our example diagram. These are not always
    a problem, but you can do better. Implement your own `Conv2dExpansion(n_filters_in)`
    class that takes the following approach: first, up-sample the image using `nn.Upsample`
    to expand the tensor width and height by a factor of 2\. If you are off by a pixel,
    use `nn.ReflectionPad2d` to pad the output to the desired shape. Finally, apply
    a normal `nn.Conv2d` to perform some mixing and change the number of channels.
    Compare this new approach with transposed convolution and see if you can identify
    any pros and cons.'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能已经注意到，转置卷积可以在其输出中创建不均匀间隔的*伪影*，这在我们的示例图中有所体现。这些并不总是问题，但你还可以做得更好。实现你自己的`Conv2dExpansion(n_filters_in)`类，采用以下方法：首先，使用`nn.Upsample`对图像进行上采样，以将张量的宽度和高度扩大2倍。如果你偏离了一个像素，使用`nn.ReflectionPad2d`对输出进行填充以达到所需的形状。最后，应用正常的`nn.Conv2d`以进行一些混合并改变通道数。将这种新方法与转置卷积进行比较，看看你是否能识别出任何优缺点。
- en: Compare a network that has three rounds of pooling and transposed convolution
    and a network with three rounds of pooling and `Conv2dExpansion` on the Data Science
    Bowl dataset. Do you see any difference in the results? Why do you think you do
    or do not?
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Data Science Bowl数据集上比较具有三轮池化和转置卷积的网络以及具有三轮池化和`Conv2dExpansion`的网络。你是否看到了结果中的任何差异？你认为你看到了还是没看到，为什么？
- en: Strided convolution can be used to shrink an image as an alternative to max
    pooling. Modify the U-Net architecture to create a truly fully convolutional model
    by replacing all pooling with strided convolutions. How does the performance change?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步长卷积可以用作替代最大池化的图像缩小方法。修改U-Net架构，通过用步长卷积替换所有池化操作来创建一个真正的全卷积模型。性能如何变化？
- en: Modify the residual network from chapter 6 to use `nn.AdaptiveMaxPooling` followed
    by a `nn.Flatten()`, a linear hidden layer, and then a linear layer to make predictions.
    (*Note:* The number of `nn.MaxPool2d` classes should not change.) Does this improve
    your results on Fashion-MNIST? Try comparing the original residual network and
    your adaptive pooling variant on the CIFAR-10 dataset and see if there is a greater
    difference in performance.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改第6章中的残差网络，使用`nn.AdaptiveMaxPooling`后跟一个`nn.Flatten()`，一个线性隐藏层，然后一个线性层进行预测。(*注意：*`nn.MaxPool2d`类的数量不应改变。)这能提高你在Fashion-MNIST上的结果吗？尝试比较原始残差网络和你的自适应池化变体在CIFAR-10数据集上的性能，看看是否有更大的性能差异。
- en: Modify the Faster R-CNN training loop to include a test-pass that computes a
    test loss over a test set after each epoch of training. *Hint:* You will need
    to keep the model in `train` mode since its behavior changes when in `eval` mode.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改Faster R-CNN的训练循环，包括在每个训练周期后计算测试集上的测试损失的测试传递。*提示：*你需要保持模型在`train`模式，因为当它在`eval`模式时，其行为会改变。
- en: Try implementing a network with residual connections for the backbone of the
    Faster R-CNN detector. Does it get better or worse performance?
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试实现一个具有残差连接的Faster R-CNN检测器骨干网络的网络。性能是变好了还是变差了？
- en: Our bounding boxes for `Class2Detect` are loose because we assumed the digit
    takes up the entire image. Modify this code to find a tight bounding box on the
    digits, and retrain the detector. How does this change the results you see on
    some test images?
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为`Class2Detect`的边界框比较宽松，因为我们假设数字占据了整个图像。修改此代码以在数字上找到紧密的边界框，并重新训练检测器。这如何改变你在一些测试图像上看到的结果？
- en: '**Challenging:** Use an image-search engine to download at least 20 images
    of cats and 20 images of dogs. Then find software online for labeling images with
    bounding boxes, and create your own cat/dog-detecting Faster R-CNN. You will need
    to write a `Dataset` class for whichever labeling software you use.'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挑战性任务：** 使用图像搜索引擎下载至少20张猫的图片和20张狗的图片。然后在网上找到用于标注图像边框的软件，并创建你自己的猫/狗检测Faster
    R-CNN。你需要为所使用的任何标注软件编写一个`Dataset`类。'
- en: Summary
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: With image segmentation, we make a prediction for *every* pixel in an image.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像分割中，我们对图像中的每个像素进行预测。
- en: Object detection uses a backbone similar to an image segmentation model and
    augments it with two smaller networks to *propose* the locations of objects and
    *decide what object is present*, respectively.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测使用与图像分割模型类似的骨干网络，并通过两个较小的网络来**提出**对象的位置和**决定**存在的对象。
- en: You can upscale an image with transposed convolutions, which are often used
    to reverse the impact of max pooling.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用转置卷积来上采样图像，转置卷积通常用于逆转最大池化的影响。
- en: A residual-like block for object detection called a U-Net block combines max
    pooling and transposed convolutions to create full-scale and low-resolution paths,
    allowing more accurate models with less work.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种用于目标检测的类似残差块的U-Net块结合了最大池化和转置卷积，以创建全尺寸和低分辨率的路径，允许通过更少的计算实现更精确的模型。
- en: Adaptive max pooling converts any size input image into a target of a specific
    size and is useful for designing networks that can train and predict on arbitrarily
    sized inputs.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应最大池化将任何大小的输入图像转换为特定大小的目标，这对于设计能够对任意大小输入进行训练和预测的网络很有用。
- en: Object detection’s false positives can be reduced using non-maximum suppression.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用非极大值抑制来减少目标检测的误报。
- en: '* * *'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ I use the “medical” qualifier in purely self-interested justification that
    I too am a doctor, just not a particularly helpful one. But someday a machine
    learning emergency will happen on a plane, and I’ll be ready![↩](#fnref27)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 我使用“医疗”这个词纯粹是出于自我利益的理由，因为我也是一个医生，但不是一个特别有用的医生。但总有一天，飞机上会发生机器学习紧急情况，我会准备好的![↩](#fnref27)
- en: ² Some people get upset when you call a model fully convolutional but still
    use pooling layers. I consider this nitpicking. In general, any network that uses
    only convolutions and has no `nn.Linear` layers (or some other non-convolutional
    layer like an RNN) can be called fully convolutional.[↩](#fnref28)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ² 当你称一个模型为全卷积但仍然使用池化层时，有些人会感到不满。我认为这是吹毛求疵。一般来说，任何只使用卷积且没有`nn.Linear`层（或某些其他非卷积层，如RNN）的网络都可以称为全卷积。[↩](#fnref28)
- en: '³ O. Ronneberger, P. Fischer, and T. Brox, “U-Net: convolutional networks for
    biomedical image segmentation,” in *Medical Image Computing and Computer-Assisted
    Intervention—MICCAI 2015*, N. Navab, J. Hornegger, W.M. Wells, and A.F. Frangi,
    eds., Springer International Publishing, 2015, pp. 234–241.[↩](#fnref29)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '³ O. Ronneberger, P. Fischer, 和 T. Brox，"U-Net: 用于生物医学图像分割的卷积网络"，在*N. Navab,
    J. Hornegger, W.M. Wells, 和 A.F. Frangi*编辑的*《医学图像计算与计算机辅助干预——MICCAI 2015》*中，Springer
    International Publishing，2015年，第234–241页。[↩](#fnref29)'
- en: ⁴ Yes, “Faster" is part of the name. It is good marketing![↩](#fnref30)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 是的，“更快”确实是名字的一部分。这很好，是好的营销手段![↩](#fnref30)
- en: ⁵ This could also be done as 4 boxes + 1 by training has object/no object as
    a binary. Most papers and resources online use the +2 approach, though, so I’m
    sticking with that for my description.[↩](#fnref31)
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 这也可以通过训练将对象/非对象作为二元进行实现，即4个框加1个。不过，大多数在线论文和资源都使用+2的方法，所以我将坚持使用这种方法来描述。[↩](#fnref31)
