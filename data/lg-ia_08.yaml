- en: 6 Filtering and extrapolation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 过滤和扩展
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Applying filters to control log events
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用过滤器来控制日志事件
- en: Implementing the record_transformer filter
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现record_transformer过滤器
- en: Extrapolating information from log events
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日志事件中提取信息
- en: Injecting environmental information into a log event
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将环境信息注入到日志事件中
- en: Masking elements of log events to maintain data security
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏日志事件的元素以维护数据安全
- en: In chapter 5, we touched upon using the filter directive to send log events
    to standard out. Using filters this way, while helpful, is almost a sideshow to
    the full capability of the directive. The filter directive can help us to
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们提到了使用filter指令将日志事件发送到标准输出。虽然这种方式使用过滤器是有帮助的，但它几乎只是指令全部功能的附属品。filter指令可以帮助我们
- en: Filter out specific log events, so only particular log events go to particular
    consuming systems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉特定的日志事件，以便只有特定的日志事件发送到特定的消费系统
- en: Filter out specific pieces of a log event message and allow us to record them
    as unique attributes of the log event (ultimately making it easier to apply logic
    with that data)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉日志事件消息的特定部分，并允许我们将它们记录为日志事件的独特属性（最终使应用该数据中的逻辑更容易）
- en: Enrich the log events by amending the tag and timestamp to reflect the dynamic
    content of the log event record itself (e.g., adjusting for upstream caching of
    events)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过修改标签和时间戳来丰富日志事件，以反映日志事件记录本身的动态内容（例如，调整事件的上游缓存）
- en: Further enrich log events; for example
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步丰富日志事件；例如
- en: Using plugins that can add geographical location information based on the public
    IP (known as GeoIP)
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于公共IP（称为GeoIP）的插件添加地理位置信息
- en: Attaching error guidance by identifying information in the log event(s) (e.g.,
    if an error code is generic but the path to the code that generated it matched
    something, then annotate the log with a qualification, such as “root cause is
    DB connection error”)
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过识别日志事件中的信息来附加错误指导（例如，如果错误代码是通用的，但生成该代码的路径与某些内容匹配，则对日志进行注释，例如“根本原因是数据库连接错误”）
- en: Adding contextual information, which can help you perform further analysis later
    (e.g., the Fluentd worker_id and server_id)
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加上下文信息，这有助于你以后进行进一步分析（例如，Fluentd的worker_id和server_id）
- en: Apply changes to address security considerations (e.g., anonymization, masking
    and redaction of any sensitive data that finds its way into log events)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用更改以解决安全考虑（例如，匿名化、屏蔽和删除任何敏感数据，这些数据可能已进入日志事件）
- en: Calculate or extrapolate new data from the log event and its context (e.g.,
    take two timestamps and calculate the elapsed time)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日志事件及其上下文中计算或外推新数据（例如，取两个时间戳并计算经过的时间）
- en: Filter out log events that confirm everything is running as expected
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉确认一切按预期运行的日志事件
- en: This chapter will explore why we may want to filter log events in or out and
    how the filters are configured to do this. As filters can be used to manipulate
    event logs, we’ll look at how this can be done, whether we should, and why we
    might want to do this.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨为什么我们可能想要过滤日志事件，以及如何配置过滤器来完成这项工作。由于过滤器可以用来操纵事件日志，我们将探讨如何进行操作，是否应该这样做，以及为什么我们可能想要这样做。
- en: 6.1 Application of filters
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 过滤器的应用
- en: We have just seen a brief summary of the breadth of possibilities for using
    filters; let’s dig into some of these applications to better understand why we
    might want to use them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚简要概述了使用过滤器可能性的广泛性；让我们深入研究一些这些应用，以更好地理解为什么我们可能想要使用它们。
- en: 6.1.1 All is well events do not need to be distributed
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 所有正常事件不需要分发
- en: A lot of log information will actually indicate to us that things are running
    as they should. Getting these events is important; as noted management consultant
    and author Peter Drucker said, “You can’t manage what you don’t measure.” Perhaps
    even more pertinent, Dag Hammarskjöld (economist and secretary-general of the
    United Nations) said, “Constant attention by a good nurse may be just as important
    as a major operation by a surgeon.” In other words, we need to actively observe,
    quantify, and qualify the state to know everything is well. This steady, constant
    observation will allow us to make minor adjustments to keep things well, rather
    than needing skilled but major change.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多日志信息实际上会向我们表明事情正在按预期运行。获取这些事件很重要；正如著名的管理顾问和作家彼得·德鲁克所说：“你不能管理你无法衡量的东西。”也许更加相关的是，联合国秘书长达格·哈马舍尔德（经济学家）说：“一位好护士的持续关注可能和一位外科医生的重大手术一样重要。”换句话说，我们需要积极观察、量化并评估状态，以确保一切正常。这种持续、稳定的观察将使我们能够进行小的调整，以保持一切良好，而不是需要技能但重大的改变。
- en: 'But we do not need to share with everyone every log event that confirms things
    are fine. Like a heart monitor, when things are not right, the alarms and signals
    all go off to ensure everyone is aware help is needed. If everything is within
    expected parameters, the data doesn’t go further than the monitor’s display. For
    example, Elastic Beats can generate heartbeat log events, such as `2017-12-17T19:17:42.667-0500
    INFO [metrics] log/log.go:110 Non-zero metrics in the last 30s: beat.info.uptime.ms=30004
    beat.memstats.gc_next=5046416`. This is probably a log message that doesn’t need
    to be retained, or if retained, does not need to be distributed and is instead
    logged locally for a short period.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '但我们不需要与每个人分享每个确认一切正常的日志事件。就像心脏监测器一样，当事情不正常时，所有的警报和信号都会响起，以确保每个人都意识到需要帮助。如果一切都在预期参数内，数据不会超出监控器的显示范围。例如，Elastic
    Beats可以生成心跳日志事件，如`2017-12-17T19:17:42.667-0500 INFO [metrics] log/log.go:110 Non-zero
    metrics in the last 30s: beat.info.uptime.ms=30004 beat.memstats.gc_next=5046416`。这可能是不需要保留的日志消息，或者如果保留，也不需要分发，而是本地记录一段时间。'
- en: If we are getting log events indicating all is well and unlikely to yield any
    more insight, do we need to propagate the events to downstream systems? This will
    mean it will be easier to see significant events. By filtering out mundane information,
    we are also controlling costs. Physical infrastructure has a maximum amount of
    data it can transmit before we need more hardware. We pay for public network capacity
    based on bandwidth (i.e., data volume), so consuming the bandwidth distributing
    every “heartbeat” log event can accumulate cost with little gain—more networking
    hardware, more bandwidth, and so on. Over the last couple of years, it has been
    observed that the cost of data egress from cloud platforms can influence commercial
    decisions. In other words, pushing data out of one cloud to another location costs
    money, and that cost can become significant. But we don’t want to cut off that
    one event in a hundred that is important and worth transmitting.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们收到表示一切正常且不太可能提供更多洞察力的日志事件，我们是否需要将这些事件传播到下游系统？这意味着将更容易看到重要事件。通过过滤掉日常信息，我们也在控制成本。物理基础设施在需要更多硬件之前可以传输的最大数据量是有限的。我们根据带宽（即数据量）支付公共网络容量，因此消耗带宽分发每个“心跳”日志事件可能会积累成本，而收益却很少——更多的网络硬件、更多的带宽等等。在过去的几年里，观察到从云平台出口数据成本可以影响商业决策。换句话说，将数据从一个云推送到另一个位置需要花钱，而且这种成本可能会变得相当高。但我们不希望切断那百分之一的重要事件，这些事件值得传输。
- en: 6.1.2 Spotting the needle in a haystack
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 在 haystack 中找到针
- en: Filtering can be used to isolate those innocuous-looking events that are a warning
    of more significant problems to come. These occur when someone has wrongly classified
    what should be a warning log event as informational or even debug. The ability
    to identify and flag these kinds of events is important when you can’t get the
    logging to generate more helpful events (e.g., off-the-shelf software, legacy
    solutions that no one wants to touch).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤可以用来隔离那些看似无害的事件，这些事件是即将到来的更严重问题的警告。这些情况发生在有人错误地将应该是一个警告日志事件分类为信息性或甚至调试性事件时。当你无法让日志生成更多有用的事件时（例如，现成的软件、无人愿意接触的遗留解决方案），能够识别和标记这类事件的能力很重要。
- en: 6.1.3 False urgency
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 虚假紧急情况
- en: Sooner or later, we will encounter a situation where a warning or error log
    occurs, and the issue escalates up the management chain. Lots of “shouting” starts
    about a problem that must trump all other priorities to be fixed. But ultimately,
    the consequences of the issue and its impact don’t require everything to be dropped;
    yes, an error occurred, but it isn’t the end of the world. What has been detected
    is a problem that could have been handled by routine day-to-day operations tasks.
    With filters, we can define rules that can help us separate the “world will end”
    events from the “please fix me when you log in and direct the information accordingly”
    events.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总有一天，我们会遇到一个警告或错误日志发生的情况，问题会升级到管理层。关于必须优先解决的所有其他优先级的问题开始“大声疾呼”。但最终，问题的后果及其影响并不需要放弃一切；是的，发生了错误，但这并不是世界末日。所检测到的是一个本可以通过日常运营任务处理的问题。通过过滤器，我们可以定义规则，帮助我们区分“世界末日”事件和“请在登录时修复我并相应地指导信息”的事件。
- en: Even better, if there are known operational steps to address an issue, we add
    the reference to the remediation process to the log event. So when the alerts
    are triggered, they’ve got the remediation information linked to them. Unnecessary
    escalation is avoided, actions that can compound a problem aren’t taken, and so
    on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，如果存在解决已知问题的操作步骤，我们将修复过程的引用添加到日志事件中。因此，当警报被触发时，它们已经与修复信息相关联。避免了不必要的升级，没有采取可能加剧问题的行动，等等。
- en: 6.1.4 Releveling
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 重新分级
- en: The previous application is when a log event can be generated and tagged with
    a log level higher than it should be—for example, `Error` instead of `Warning`
    or `Info`. As before, if people can’t or won’t fix the issue, then we can modify
    the log event as it gets passed on. This is done by manipulating the log event
    to change the record’s log level to a less alarming and more accurate classification.
    Alternatively, tagging the log event with additional attributes with commentary
    shows this is a known incorrect log level.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的应用场景是当日志事件可以被生成并标记为比其应有的日志级别更高的级别时——例如，使用`Error`而不是`Warning`或`Info`。和之前一样，如果人们无法或不愿意修复这个问题，那么我们可以在日志事件传递过程中修改它。这是通过操作日志事件来改变记录的日志级别到一个不那么令人警觉且更准确的分类来实现的。或者，通过给日志事件添加带有注释的额外属性来标记，这表明这是一个已知的错误日志级别。
- en: 6.1.5 Unimplemented housekeeping
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 未实现的日常维护
- en: As long as software development exists, business drivers will prioritize functional
    capabilities over nonfunctional ones such as housekeeping (archiving or deleting
    folders of processed files, etc.). When this is a characteristic of a legacy application,
    it is not unusual for people to fear changing anything to improve the system,
    such as cleaning up after itself. The typical result is that routine support processes
    are done manually, which we may then automate via scripts and just have to run
    in certain circumstances. Filtering out the indicators in logs alerting that housekeeping
    tasks need to be done (e.g., Fluentd capturing log events relating to disk space,
    for example) is a small step that triggers the execution of housekeeping tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 只要软件开发存在，业务驱动因素就会优先考虑功能性能力，而不是非功能性能力，如日常维护（例如存档或删除处理过的文件文件夹等）。当这是一个遗留应用程序的特征时，人们害怕改变任何东西以改进系统，比如自己清理，这种情况并不罕见。典型的结果是，常规支持流程是手动完成的，然后我们可以通过脚本自动化，只需在特定情况下运行。过滤掉日志中指示需要执行日常维护任务的指标（例如，Fluentd捕获与磁盘空间相关的日志事件）是触发执行日常维护任务的小步骤。
- en: 6.2 Why change log events?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 为什么更改日志事件？
- en: Some filters allow us to modify log events. Why should we consider this, and
    how can this capability help us? Some might argue that modifying log events is
    also tampering with the “original truth,” so should we even allow it?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一些过滤器允许我们修改日志事件。我们为什么应该考虑这一点，这种能力如何帮助我们？有些人可能会争论，修改日志事件也是在篡改“原始真相”，那么我们甚至应该允许它吗？
- en: 6.2.1 Easier to process meaning downstream
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 更容易处理下游的意义
- en: When we process log events, we often need to extract more meaning from the logs
    provided. The log event is unstructured, semi-structured, or even structured but
    needs to be reparsed to a suitable data structure (e.g., reading JSON text files).
    The structure can help filter, route, create new reporting metrics, and measure
    using the log event data. Once we have invested the effort to extract meaning
    from a log event, why not make that easy to reuse downstream? In other words,
    apply the principle of DRY (don’t repeat yourself). So, if you have extracted
    meaning and structure, don’t make people do it again later. Simply pass the derived
    information with the log event.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理日志事件时，我们通常需要从提供的日志中提取更多意义。日志事件是无结构的、半结构的，甚至是有结构的，但需要重新解析为合适的数据结构（例如，读取JSON文本文件）。结构可以帮助过滤、路由、创建新的报告指标，并使用日志事件数据进行度量。一旦我们投入了从日志事件中提取意义的努力，为什么不使其易于在下游重复使用呢？换句话说，应用DRY（不要重复自己）的原则。因此，如果您已经提取了意义和结构，不要让人们以后再重复做这件事。只需将派生信息与日志事件一起传递。
- en: 6.2.2 Add context
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 添加上下文
- en: To process an event correctly, we may need additional context. When trying to
    diagnose why an application is performing poorly, it isn’t unusual to look at
    what else was happening around the events—for example, did the server have a large
    number of threads running? Sometimes it is easy to link this contextual data to
    the log event. The easiest way to associate additional context is to add it to
    the log event.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确处理事件，我们可能需要额外的上下文。当试图诊断为什么应用程序表现不佳时，查看事件周围发生的事情并不罕见——例如，服务器是否运行了大量的线程？有时很容易将此类上下文数据与日志事件关联起来。将额外上下文关联起来的最简单方法是将它添加到日志事件中。
- en: 6.2.3 Record when we have reacted to a log event
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 记录我们何时对日志事件做出反应
- en: We have already referred to the possibility that we initiate some sort of action
    due to a log event. In retrospect, it can be helpful to understand which event(s)
    triggered an action. Adding information to the triggering log event can be a more
    straightforward, acceptable action rather than correlating separate log events
    later to show cause and effect.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了由于日志事件而采取某种行动的可能性。回顾起来，了解哪些事件触发了行动可能是有帮助的。向触发日志事件添加信息可能是一种更直接、更可接受的做法，而不是在之后关联单独的日志事件来显示因果关系。
- en: 6.2.4 Data redaction/masking
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 数据编辑/掩码
- en: When we are developing software, it is often helpful to log an entire data object
    being processed during the development phase. This isn’t a problem during development
    and testing, as it’s just test data. But if the data includes sensitive information,
    such as data that can be used to identify individuals (*PII*, personally identifiable
    information), as in health care or credit card use, for example, it can become
    a challenge. Any part of an IT system that handles such data becomes subject to
    a lot of legal, legislative, and contractual technical requirements. Such requirements
    come from international, national, and regional data laws such as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在开发软件时，在开发阶段记录正在处理的数据对象通常很有帮助。在开发和测试阶段这并不是问题，因为这只是测试数据。但如果数据包含敏感信息，例如可以用来识别个人的数据（*PII*，个人可识别信息），例如在医疗保健或信用卡使用中，这可能会成为一个挑战。任何处理此类数据的IT系统部分都将成为许多法律、立法和合同技术要求的对象。这些要求来自国际、国家和地区的数据法律，例如
- en: '*GDPR* (General Data Protection Register)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GDPR*（通用数据保护条例）'
- en: '*HIPAA* (Health Insurance Portability and Accountability Act) and other health
    care legislation'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HIPAA*（健康保险可携带性和问责制法案）和其他医疗保健立法'
- en: '*PCI DSS* (Payment Card Industry Data Security Standard)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PCI DSS*（支付卡行业数据安全标准）'
- en: 'You can add to this list that many companies may also wish to treat some financial
    accounting data with the same sensitivity. The obvious solution would be to fix
    the software so it doesn’t log the data or limit the impact of such logging, limiting
    the “blast radius” of needing to apply extra extremely stringent controls, security
    mechanisms, and reporting. Fluentd provides an excellent means to address this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将此列表扩展到许多公司可能也希望以相同敏感性处理一些财务会计数据。明显的解决方案是修复软件，使其不记录数据或限制此类记录的影响，从而限制需要应用额外极其严格的控制、安全机制和报告的“爆炸半径”。Fluentd提供了一个很好的方法来解决这个问题：
- en: Remove or redact/mask the data from the logs. Masking is typically done by replacing
    sensitive values with meaningless ones. Redaction is removing information from
    sight by either deleting it from the communication, or simply never making it
    visible in logs, and so on. We can see data being masked on payment card receipts
    with asterisks or hash characters replacing your card number. Any approach to
    masking can be used as long as it can’t be reversed to get back the original data.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日志中删除或红字/掩码数据。掩码通常是通过用无意义的值替换敏感值来完成的。红字是通过从通信中删除信息，或者简单地确保日志中不显示信息等方式来移除信息。我们可以看到在支付卡收据上用星号或哈希字符替换你的卡号来掩码数据。只要不能通过掩码恢复原始数据，任何掩码方法都可以使用。
- en: Co-locate Fluentd with the log source so that the amount of infrastructure subject
    to the elevated data security requirements is limited. The smaller the scope of
    elevated security, the smaller the “attack surface” (i.e., the smaller the number
    of servers and software components that may be subject to malicious attacks attempting
    to get the data, the better).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Fluentd与日志源协同定位，以限制需要满足提高数据安全要求的基础设施数量。提高安全范围的越小，“攻击面”就越小（即，可能受到恶意攻击尝试获取数据的服务器和软件组件数量就越少，越好）。
- en: Connect the main application’s logging directly to Fluentd using *RPC* (remote
    procedure call) techniques rather than log files, so the log events are transient.
    We will see more on directly connecting applications to Fluentd in chapter 11.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*RPC*（远程过程调用）技术直接将主应用程序的日志连接到Fluentd，而不是使用日志文件，这样日志事件就是瞬时的。我们将在第11章中看到更多关于直接将应用程序连接到Fluentd的内容。
- en: Security not as a cost
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 安全不是成本
- en: It would be easy to read into what has been said here, concluding that security
    is an undesirable cost, and avoiding security is good. The reality is that today,
    security should be deemed an asset, and the application of security is a positive
    selling point. SaaS solution providers like Oracle do use their security as a
    virtue. The cost impact of data loss, particularly when the level of impact is
    not limited or understood, can easily outweigh the savings perceived of not having
    invested in securing against the risks. But the smaller the potential blast radius,
    the better. These days, a breach (malicious or accidental) is a matter of *when*,
    not *if*. The adage “assume the worst, hope for the best” is very appropriate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易误解这里所说的内容，得出结论认为安全是不受欢迎的成本，避免安全是好的。但现实是，今天，安全应该被视为一种资产，安全的应用是积极的卖点。像Oracle这样的SaaS解决方案提供商确实将他们的安全作为一项优点。数据丢失的成本影响，尤其是当影响程度没有限制或理解时，很容易超过没有投资于防范风险的节省。但潜在的影响范围越小，越好。如今，安全漏洞（恶意或意外）是“何时”的问题，而不是“是否”的问题。谚语“最坏打算，最好希望”非常恰当。
- en: 6.3 Applying filters and parsers
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 应用过滤器和解析器
- en: In this section, we’ll look at the practical configuration and use of filters
    and parsers to
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨过滤器和解析器的实际配置和使用，以
- en: Manage the routing of log events
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理日志事件的路由
- en: Manipulate log events
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作日志事件
- en: To manipulate log events, we may need to impose or extract some meaning from
    them. To extract that meaning, we need to parse unstructured log event content,
    so we will need to touch upon the use of parsers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要操作日志事件，我们可能需要从中提取或赋予它们一些意义。为了提取这种意义，我们需要解析非结构化日志事件内容，因此我们将涉及到解析器的使用。
- en: 6.3.1 Filter plugins
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 过滤器插件
- en: Filter as a directive is like a `match`, in so far as the directive can include
    tags in the declaration (e.g., `<filter myApp>` or `<filter *>`). The difference
    is that if the log event complies with the filter expression, rather than the
    log event being consumed, it can pass into the next part of the configuration
    without resorting to a copy action, as illustrated with the `match` directive
    in chapter 3.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为指令的过滤器就像一个`match`，因为指令可以在声明中包含标签（例如，`<filter myApp>`或`<filter *>`）。区别在于，如果日志事件符合过滤器表达式，而不是日志事件被消费，它可以通过配置的下一部分而不需要复制操作，就像在第3章中用`match`指令所示。
- en: 'Within the Fluentd core are the following filter plugins:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fluentd核心中包含以下过滤器插件：
- en: '*record_transformer*—The most sophisticated of the built-in filters; also provides
    a diverse set of options for manipulating the log event.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*record_transformer*—这是内置过滤器中最复杂的；同时也提供了一系列选项来操作日志事件。'
- en: '*grep*—Provides the means to define rules about log event attributes to filter
    them out of the stream of events. Multiple expressions can be provided to define
    cumulative rules.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*grep*—提供了定义关于日志事件属性的规则以从事件流中过滤掉它们的手段。可以提供多个表达式来定义累积规则。'
- en: '*filter_parser*—Combines the capability of parser plugins with the filter.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*filter_parser*—结合了解析器插件的功能和过滤器。'
- en: '*stdout*—We have seen this plugin at work. Every event is allowed to pass through
    the filter but is also written to stdout.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*stdout*—我们已看到这个插件的工作。每个事件都可以通过过滤器，但也会写入 stdout。'
- en: Fluentd comes with a core set of filter plugins; in addition to this, there
    are community-provided filter plugins. Appendix C contains details of additional
    plugins that we believe can be particularly helpful.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 内置了一组核心过滤器插件；除此之外，还有社区提供的过滤器插件。附录 C 包含了我们认为可能特别有帮助的附加插件详细信息。
- en: 6.3.2 Applying grep filters
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 应用 grep 过滤器
- en: The grep parser allows us to define a search expression and apply it to a named
    attribute in the log event. For example, we could extend our routing such that
    the events with log entries explicitly refer to computers in the text. This is
    the basis of the following scenario; while a computer reference is relatively
    meaningless, we could easily replace or extend it with a reference to a cataloged
    error code. For example, a WebLogic notification starts with `BEA-000`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: grep 解析器允许我们定义一个搜索表达式并将其应用于日志事件中的命名属性。例如，我们可以扩展我们的路由，使得具有日志条目的事件明确地引用文本中的计算机。这是以下场景的基础；虽然计算机引用相对无意义，但我们可以很容易地用已编目错误代码的引用替换或扩展它。例如，WebLogic
    通知以 `BEA-000` 开头。
- en: While we are demonstrating the use of the filter, let’s use a different output
    plugin. Chapter 1 introduced EFK (Elasticsearch, Fluentd, Kibana), so we’ll bring
    Elasticsearch into the mix to show more of this stack (appendix A provides instructions
    on how to install Elasticsearch). The Fluentd configuration we’re going to use
    is shown in figure 6.1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在演示过滤器使用时，让我们使用一个不同的输出插件。第一章介绍了 EFK（Elasticsearch、Fluentd、Kibana），因此我们将把
    Elasticsearch 加入到混合中，以展示更多这个堆栈（附录 A 提供了如何安装 Elasticsearch 的说明）。我们将使用的 Fluentd
    配置如图 6.1 所示。
- en: '![](../Images/CH06_F01_Wilkins.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F01_Wilkins.png)'
- en: Figure 6.1 Application of filter and Elasticsearch as an output
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 过滤器和 Elasticsearch 作为输出应用
- en: We can apply a filter using the grep plugin, which will execute a regular expression
    whose result can be treated in a binary manner. The result will determine whether
    the log event is to be stored. This is all done by setting the directive to be
    `regexp`. We need to define the key, which is the log event’s element to examine.
    In this case, we want to look at the core log event called `msg`. Once we’ve identified
    where to look, we need to provide a pattern for the regex parser to look for.
    Bringing this together with the attribute name gives us
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 grep 插件来应用过滤器，该插件将执行一个正则表达式，其结果可以以二进制方式处理。结果将决定是否存储日志事件。所有这些操作都是通过将指令设置为
    `regexp` 来完成的。我们需要定义一个键，即要检查的日志事件的元素。在这种情况下，我们想查看名为 `msg` 的核心日志事件。一旦我们确定了查看的位置，我们需要提供一个模式供正则表达式解析器查找。将此与属性名称结合，我们得到
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With the filter defined, we need to send any matching log events to our installation
    of Elasticsearch. We do this using the `match` directive and a `@type` value of
    `elasticsearch`. The Elasticsearch plugin is incredibly configurable, with over
    30 attributes covering behaviors ranging from caching control to determining how
    the log events are populated and indexed in Elasticsearch, and so on. We’re not
    going to cover all of these, as we’d end up with a book explaining Elasticsearch,
    and for that, you’d be better off with *Elasticsearch in Action* by Radu Gheorghe,
    et al. ([www.manning.com/books/elasticsearch-in-action](http://www.manning.com/books/elasticsearch-in-action));
    however, we should touch upon the most common attributes that you’re likely to
    encounter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了过滤器后，我们需要将任何匹配的日志事件发送到我们的 Elasticsearch 安装。我们使用 `match` 指令和一个 `@type` 值为
    `elasticsearch` 来完成此操作。Elasticsearch 插件具有极高的可配置性，超过 30 个属性，涵盖了从缓存控制到确定日志事件如何在
    Elasticsearch 中填充和索引等行为。我们不会涵盖所有这些，因为这将导致一本解释 Elasticsearch 的书，而对于那本书，你最好阅读 Radu
    Gheorghe 等人所著的 *Elasticsearch in Action*（[www.manning.com/books/elasticsearch-in-action](http://www.manning.com/books/elasticsearch-in-action)）；然而，我们应该简要介绍你可能会遇到的最常见的属性。
- en: As with the MongoDB connection, details must be provided to address the server
    (attributes `host` and `port`). Access credentials are likely to be required (*user*
    and *password*). As we haven’t set up any such restrictions using the out-of-the-box
    deployment, we don’t need to provide them. The *scheme* or type of communication,
    such as *http* or *https*, will dictate whether additional details will be needed
    (e.g., where the certificates to be used can be found); end-to-end SSL/TLS is
    always good security practice.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MongoDB 连接一样，必须提供详细信息以解决服务器（`host` 和 `port` 属性）。可能需要访问凭证（`user` 和 `password`）。由于我们没有使用开箱即用的部署设置任何此类限制，因此我们不需要提供它们。通信方案或类型，如
    `http` 或 `https`，将决定是否需要更多信息（例如，可以使用证书的位置）；端到端 SSL/TLS 总是良好的安全实践。
- en: Once the means to connect to Elasticsearch have been defined, we need to declare
    where inside Elasticsearch the data should go (`index_name`) and what data to
    provide, such as whether to include the tag value in the core log event record
    (`include_tag_key, tag_key`). Remember, we also set how the data being passed
    is being represented. With the relationship between Elasticsearch and Logstash,
    it should come as no surprise that the plugin allows us to tell Fluentd to present
    the log events as Logstash would set the attribute `logstash_format` to `true`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了连接到 Elasticsearch 的方式，我们需要声明数据在 Elasticsearch 内部的位置（`index_name`）以及要提供的数据，例如是否将标签值包含在核心日志事件记录中（`include_tag_key,
    tag_key`）。记住，我们还设置了正在传递的数据的表示方式。由于 Elasticsearch 和 Logstash 之间的关系，插件允许我们告诉 Fluentd
    将日志事件呈现为 Logstash 会将属性 `logstash_format` 设置为 `true` 是不足为奇的。
- en: The Elasticsearch plugin also leverages helper caching plugins; thus, we need
    to consider how this can impact the behavior. For ease and speed, let’s use a
    memory buffer being set to flush every 5 seconds by using the attribute `flush_interval
    5s`. This configuration can be seen in the following listing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 插件还利用辅助缓存插件；因此，我们需要考虑这可能会如何影响行为。为了方便和速度，让我们使用内存缓冲区，通过使用属性 `flush_interval
    5s` 每隔 5 秒刷新一次。此配置可以在以下列表中看到。
- en: Listing 6.1 Chapter6/Fluentd/file-source-elastic-search-out.conf
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 Chapter6/Fluentd/file-source-elastic-search-out.conf
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Allows the filter to process any tag
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 允许过滤器处理任何标签
- en: ❷ Defines the type of filter
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义过滤器类型
- en: ❸ Defines the field to apply the regular expression to and then the expression,
    which needs to yield a binary result
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义应用于字段的正则表达式以及表达式，它需要产生二进制结果
- en: ❹ All log events that have passed through the filter will now be processed by
    this match configured to write to file.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 所有通过过滤器的日志事件现在将由配置为写入文件的此匹配处理。
- en: ❺ While we do not explicitly need to set the scheme, as it defaults to http
    rather than https, it is worth including it to remind ourselves of the low-security
    threshold in use. You could also include the username and password as commented
    out as well.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 虽然我们不需要明确设置方案，因为它默认为 http 而不是 https，但包括它来提醒我们正在使用的低安全阈值是值得的。您还可以包括用户名和密码，如注释所示。
- en: ❻ Defines the index to be used; if unspecified, it will default to Fluentd
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义要使用的索引；如果未指定，则默认为 Fluentd
- en: ❼ Tells Elasticsearch to add to the named index, rather than it creating new
    ones using a timestamp name, as is the case for Logstash connectivity
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 告诉 Elasticsearch 将数据添加到命名索引，而不是使用时间戳名称创建新的索引，就像 Logstash 连接那样
- en: ❽ Shows we’re telling the Elasticsearch plugin to include the log event tag
    in the data to be stored, giving it the name key, as shown in the next attribute
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 显示我们正在告诉 Elasticsearch 插件将日志事件标签包含在要存储的数据中，并给它命名为 key，如下一个属性所示
- en: 'Let’s see the result of the configuration. As this uses a file source, we need
    to run the LogSimulator as well. Assuming Elasticsearch is also running and ready,
    the following commands are needed to run the example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看配置的结果。由于这使用的是文件源，我们需要同时运行 LogSimulator。假设 Elasticsearch 也正在运行并已准备好，以下命令是运行示例所需的：
- en: '`fluentd -c ./Chapter6/Fluentd/file-source-elastic-search-out.conf`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c ./Chapter6/Fluentd/file-source-elastic-search-out.conf`'
- en: '`groovy logSimulator.groovy ./Chapter6/SimulatorConfig/log-source-1.properties`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy logSimulator.groovy ./Chapter6/SimulatorConfig/log-source-1.properties`'
- en: We can verify the records in Elasticsearch with the UI tool by reviewing the
    index contents, which we configured as `fluentd-book`. (Appendix A also covers
    the setting up of Elasticvue for this purpose.) You should find that the index
    contains the same log events that we sent to stdout.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过UI工具验证Elasticsearch中的记录，通过查看索引内容来实现，我们将其配置为`fluentd-book`。（附录A也涵盖了为该目的设置Elasticvue的过程。）你应该会发现索引包含了我们发送到stdout的相同日志事件。
- en: Should filters modify log events?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 应该过滤日志事件吗？
- en: The idea that we can change log events can be a contentious subject. If you
    change the original log event, are you modifying the original truth? To use a
    TV detective analogy, messing with the original log event is like tampering with
    a crime scene. Shouldn’t Fluentd handle log events like the chain of custody for
    evidence? Generally, I would agree that the original log event should be retained
    unmodified. However, we often need to associate additional information to a piece
    of evidence (extending our analogy, a ballistics report would be attached to the
    relevant weapon). Rather than trying to keep the details separate, careful attachment
    of the details can be more helpful.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更改日志事件的想法可能是一个有争议的话题。如果您更改原始日志事件，您是否在修改原始真相？用一个电视侦探的类比来说，篡改原始日志事件就像篡改犯罪现场。Fluentd不应该像处理证据的保管链一样处理日志事件吗？一般来说，我会同意原始日志事件应该保留未做修改。然而，我们经常需要将附加信息关联到某个证据（继续我们的类比，弹道报告将被附加到相关武器上）。与其试图保持细节分离，不如仔细地附加细节可能更有帮助。
- en: In the real world, the guidance we use is to keep a copy of the log event unaltered,
    with one exception—information security. If you need to mask or remove data, consider
    keeping an unadulterated copy somewhere safe that can be traced back to if necessary.
    Then any manipulated, extracted values can be kept along with the original. You
    might consider adopting a naming convention, so when those elements of a log event
    are manipulated, constructed, or enriched, the origin is clear.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，我们使用的指导原则是保留日志事件的副本，不做任何修改，只有一个例外——信息安全。如果您需要屏蔽或删除数据，请考虑在安全的地方保留一份未受污染的副本，以便在必要时追溯。然后，任何经过处理、提取的值都可以与原始值一起保留。您可能考虑采用命名约定，这样当日志事件的这些元素被处理、构建或丰富时，起源就清晰了。
- en: 6.3.3 Changing log events with the record_transformer plugin
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 使用record_transformer插件更改日志事件
- en: Using a filter to control which log events are processed or not based on the
    log event’s contents addresses many previously described scenarios. Modifying
    log events to add additional contextual information and derived values or to extract
    and record log event values in a more meaningful and usable manner helps with
    some of the other scenarios mentioned.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用过滤器根据日志事件的内容控制哪些日志事件被处理或不被处理，这解决了许多之前描述的场景。修改日志事件以添加额外的上下文信息、派生值，或以更有意义和可用的方式提取和记录日志事件值，有助于解决其他提到的场景。
- en: 'To illustrate how this can work, we’re going to add to our log events new fields
    in addition to the standard ones, specifically the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这如何工作，我们将在我们的日志事件中添加新的字段，除了标准字段之外，具体如下：
- en: A field called `computer` containing the name of the host running Fluentd.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为`computer`的字段，包含运行Fluentd的主机名称。
- en: Apply a prefix to the standard *message* of '`processed-`' to illustrate the
    modification of existing values.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`processed-`前缀应用于标准`*message*`，以说明现有值的修改。
- en: The example log messages contain a JSON structure that includes a `name` attribute
    comprising `firstname` and `surname`. This combination could be considered making
    the log data sensitive to PII rules, as it references an identifiable individual.
    We will tease out the `firstname` and create a new log event attribute called
    `from` and delete the surname to address this. There should be no reason for the
    new attribute `from`; it does allow us to see how to copy elements.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例日志消息包含一个包含`name`属性的JSON结构，该属性由`firstname`和`surname`组成。这种组合可能会使日志数据对PII规则敏感，因为它引用了一个可识别的个人。我们将提取`firstname`并创建一个新的日志事件属性`from`，并删除姓氏来解决这个问题。新的属性`from`可能没有理由；它确实允许我们看到如何复制元素。
- en: Our log event message is structured and, when received, will look like
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们日志事件的消息是结构化的，当接收时，将看起来像
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Record directive
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 记录指令
- en: 'The essential part of the filter definition is the `record` directive. Each
    line within the directive represents a field name and a field value. For example,
    if we wanted to add a new field called `myNewField` with a literal value of `aValue`,
    then we would configure the directive as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器定义的关键部分是`record`指令。指令中的每一行代表一个字段名称和字段值。例如，如果我们想添加一个名为`myNewField`的新字段，其字面值为`aValue`，那么我们就会按照以下方式配置指令：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Just incorporating a design-time literal value isn’t going to provide much value.
    To tell Fluentd that it needs to process a derived value, we need to wrap the
    expression within `${}`. We can reference the other fields within the log event
    by placing the name within the brackets (e.g., `${msg}`). To access the log event
    message, we use the notation `record["<field name>"]` (e.g., `record["msg"]`).
    `Record` is a reference to a function made available to use.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 仅包含设计时字面值并不能提供太多价值。为了告诉Fluentd它需要处理一个派生值，我们需要在`${}`内包装表达式。我们可以通过在括号内放置名称来引用日志事件中的其他字段（例如，`${msg}`）。要访问日志事件消息，我们使用`record["<字段名称>"]`的表示法（例如，`record["msg"]`）。`Record`是对一个函数的引用，该函数可供使用。
- en: Within the characters `${}`, we are being allowed to use a small subset of Ruby
    objects, functions, and operators, including some provided by Fluentd. To allow
    this, we can include an attribute within the filter attribute `enable_ruby`; when
    this is set to `true`*,* it will allow the full power of the Ruby language to
    be used. This is defaulted to `false`, as it creates more work for the parser,
    such as ensuring that it can resolve dependencies, and so on; to keep things efficient,
    it’s best not set to `true` unless necessary.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在`${}`字符内，我们被允许使用Ruby对象、函数和操作符的小子集，包括Fluentd提供的一些。为了允许这样做，我们可以在过滤器属性`enable_ruby`中包含一个属性；当它设置为`true`*时，*它将允许使用Ruby语言的全部功能。这默认为`false`，因为它会给解析器带来更多的工作，例如确保它可以解决依赖关系等；为了保持效率，最好除非必要，否则不要设置为`true`。
- en: Accessing nested JSON elements
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 访问嵌套的JSON元素
- en: To obtain the `firstname` element, we need to navigate the JSON structure within
    the message part of the log event. This can be done with either the standard *record*
    method—for example, `record["name"]["firstname"]`—which would traverse to the
    `firstname` as a child attribute but requires the attribute to be present. This
    can be a problem if part of the structure is optional, as any part of the path
    that is missing will trigger a run-time error. The alternative approach is to
    use a function called dig provided by the `record` operator. The syntax is remarkably
    similar; however, a nil result is provided rather than an error if the path does
    not exist. The `dig` function is `record.dig` (`"msg", "name", “firstname”`).
    This does require the *enable_ruby* to be set to work.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取`firstname`元素，我们需要在日志事件的“消息”部分中导航JSON结构。这可以通过标准*record*方法完成——例如，`record["name"]["firstname"]`——这将遍历到`firstname`作为一个子属性，但需要该属性存在。如果结构的一部分是可选的，这可能会成为一个问题，因为路径中缺失的任何部分都会触发运行时错误。另一种方法是使用由`record`操作符提供的名为`dig`的函数。语法非常相似；然而，如果路径不存在，则提供一个nil结果而不是错误。`dig`函数是`record.dig`（`"msg",
    "name", “firstname”`）。这确实需要将`*enable_ruby*`设置为工作状态。
- en: JSON element deletion
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 删除JSON元素
- en: The `record_transformer` includes several attributes that allow the control
    of the composition of the log event elements. This can be done by using optional
    attributes in the configuration for listing elements to delete (`remove_keys`)
    or defining which elements (other than mandatory ones like tag) should remain
    (`keep_keys`). This includes the notation to traverse the JSON structure (which
    also works in other parts of the plugin). The order of attributes in the configuration
    is important. In our example, the `remove_keys` attribute needs to appear after
    the `record` directive; otherwise, we will find ourselves without an element to
    copy. To delete specific elements within a structure, we use the attribute `remove_keys`
    with the path through the object, such as `$.name.surname`. In the notation, `$`
    (dollar sign) effectively represents the root of the log event. This is then followed
    by the attribute name using dot notation to traverse the structure. This does
    go back to the previous point of whether we can trust the path to exist. A single
    `remove_keys` attribute can be extended to more elements by making it a comma-separated
    list; for example, `remove_keys $.name.surname, $.name.anotherName, $.somethingElse`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`record_transformer` 包含几个属性，允许控制日志事件元素的组成。这可以通过在配置中使用可选属性来删除列表元素（`remove_keys`）或定义哪些元素（除了像标签这样的强制元素）应该保留（`keep_keys`）来实现。这包括遍历
    JSON 结构的表示法（这也在插件的其它部分中工作）。配置中属性的顺序很重要。在我们的例子中，`remove_keys` 属性需要在 `record` 指令之后出现；否则，我们可能会发现自己没有元素可以复制。要删除结构中的特定元素，我们使用具有通过对象路径的
    `remove_keys` 属性，例如 `$.name.surname`。在表示法中，`$`（美元符号）实际上代表了日志事件的根。然后，使用点表示法跟随属性名称来遍历结构。这确实回到了我们是否可以信任路径存在的先前的点。单个
    `remove_keys` 属性可以通过将其作为逗号分隔的列表来扩展到更多元素；例如，`remove_keys $.name.surname, $.name.anotherName,
    $.somethingElse`。'
- en: Value replacement
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 值替换
- en: The record operator includes a function that allows us to replace values in
    JSON elements. This is necessary for masking data and correcting values, such
    as the error message level, as described in section 6.1.4\. This is done by referencing
    the element name and then invoking the function `gsub` followed by the parameters
    containing the value to replace and its replacement. For example, in our data
    set, the `msg` contains some occurrences of `'I'`. Using the expression `${record["msg"]`
    `.gsub('I', 'We')}`, the use of `'I'` can be replaced with `'We'`. In the following
    listing, we have included this expression. Rather than replacing the `msg` with
    the substituted string, a new attribute has been added to make the comparison
    easy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 记录操作符包含一个函数，允许我们替换 JSON 元素中的值。这在掩码数据和纠正值时是必要的，例如在 6.1.4 节中描述的错误消息级别。这是通过引用元素名称然后调用函数
    `gsub` 并跟随包含要替换的值及其替换值的参数来完成的。例如，在我们的数据集中，`msg` 包含一些 `'I'` 的出现。使用表达式 `${record["msg"]`.gsub('I',
    'We')}`，可以将 `'I'` 替换为 `'We'`。在下面的列表中，我们已经包含了这个表达式。我们不是用替换后的字符串替换 `msg`，而是添加了一个新属性，以便于比较。
- en: Listing 6.2 Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Enables Ruby to support the record.dig approach of locating values
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 启用 Ruby 支持记录.dig 方法来定位值
- en: ❷ Adds an attribute using the known contextual values
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用已知的上下文值添加一个属性
- en: ❸ Creates a new value by finding a sub-element and retrieving its value
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过查找子元素并检索其值来创建一个新值
- en: ❹ Modifies the msg element by adding textual content
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过添加文本内容修改 msg 元素
- en: ❺ Performs the string substitution of 'I' with 'We'. A white space character
    is included to avoid accidentally picking up characters, in other words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行将 'I' 替换为 'We' 的字符串替换。包含一个空白字符以避免意外选中字符，换句话说。
- en: ❻ Deletes the surname element to ensure we’re not at risk with PII considerations
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 删除姓氏元素以确保我们不会因为 PII 考虑而处于风险之中
- en: ❼ The inject directive shown here allows the worker_id for the process to be
    added to the log event. The inject directive allows some different useful values
    to be added to provide additional context.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 这里显示的 inject 指令允许将进程的 worker_id 添加到日志事件中。inject 指令允许添加一些有用的值以提供额外的上下文。
- en: 'Let’s see the result of the configuration. As this makes use of a file source,
    we need to run the LogSimulator. So, to run the example, the following commands
    are needed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看配置的结果。由于这使用了文件源，我们需要运行 LogSimulator。因此，要运行示例，需要以下命令：
- en: '`fluentd -c ./Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf`'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c ./Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf`'
- en: '`groovy logSimulator.groovy ./Chapter6/SimulatorConfig/log-source-1.properties`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy logSimulator.groovy ./Chapter6/SimulatorConfig/log-source-1.properties`'
- en: Before starting the UI to review log events stored in the `fluentd-book-transformed`
    index in Elasticsearch, the changes from the `record_transformer` and the `inject`
    directive should be visible on the console because of the stdout filter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始 UI 以审查存储在 Elasticsearch 中 `fluentd-book-transformed` 索引中的日志事件之前，由于 stdout
    过滤器，`record_transformer` 和 `inject` 指令的更改应该在控制台上可见。
- en: Predefined values
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 预定义值
- en: The `record_transformer` also helps by providing some predefined values, including
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`record_transformer` 还通过提供一些预定义值来帮助，包括'
- en: '*Hostname*—The name of the computer host'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主机名*—计算机主机的名称'
- en: '*Time*—The current time'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间*—当前时间'
- en: '*Tag*—The current log event tag'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签*—当前日志事件的标签'
- en: With the Ruby flag enabled, we can extend the ability to get and set values
    to access any public class methods by using the `${}`. For example, using a method
    in a class would use `"${#`<class>`.`<method>`}"` where `<class>` is the name
    of the Ruby class and `<method>` is the corresponding public class method. `"#{Dir.getwd}"`
    will retrieve the current working directory.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 Ruby 标志后，我们可以通过使用 `${}` 扩展获取和设置值的能力，以访问任何公共类方法。例如，在类中使用方法将使用 `"${#`<class>`.`<method>`}"`，其中
    `<class>` 是 Ruby 类的名称，而 `<method>` 是相应的公共类方法。`"#{Dir.getwd}"` 将检索当前工作目录。
- en: 6.3.4 Filter parser vs. record transformer
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 过滤器解析器与记录转换器
- en: The `record_transformer` plugin provides us with the means to work with the
    log event as a JSON payload. If the log event is simply a single block of text,
    we will likely need to parse it to obtain meaningful values. In chapter 3, we
    introduced the use of parsers to extract meaning out of log events. The parsers
    we saw, like `regexp`, also work with the `filter` directive. As a result, where
    a `regexp expression` defines the parts of the string to capture as named values,
    the parser’s behavior is extended such that the named elements will be made top-level
    log event attributes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`record_transformer` 插件为我们提供了将日志事件作为 JSON 有效负载工作的手段。如果日志事件只是一个简单的文本块，我们可能需要解析它以获取有意义的值。在第
    3 章中，我们介绍了使用解析器从日志事件中提取意义的方法。我们看到的解析器，如 `regexp`，也与 `filter` 指令一起工作。因此，当 `regexp
    表达式` 定义了要捕获为命名值的字符串部分时，解析器的行为被扩展，使得命名元素将成为顶级日志事件属性。'
- en: Let’s take the same expression (included in the following listing) and put it
    into the context of the `filter` directive. The essential difference here is that
    we need to tell Fluentd which log event attribute to process before the parser
    definition. This means we can target a specific part of a log event. We can also
    use consecutive filters to break down nested structures if we wanted. In listing
    6.3, we expect the output to result in additional attributes called `time`, `level`,
    `class`, `line`, `iteration`, and `msg`. Like the `record_transformer` plugin,
    we can determine whether the log event attribute that is processed is retained
    or not using the `reserve_data` configuration element. We can make the control
    a bit more nuanced by adding `remove_key_ name_field` and setting it to `true`;
    Fluentd will remove the original attribute only if the parsing process was successful.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将相同的表达式（包含在以下列表中）放入 `filter` 指令的上下文中。这里的基本区别是我们需要在解析器定义之前告诉 Fluentd 要处理哪个日志事件属性。这意味着我们可以针对日志事件的一个特定部分。如果我们想的话，我们还可以使用连续的过滤器来分解嵌套结构。在列表
    6.3 中，我们期望输出产生额外的属性，称为 `time`、`level`、`class`、`line`、`iteration` 和 `msg`。像 `record_transformer`
    插件一样，我们可以使用 `reserve_data` 配置元素确定处理的日志事件属性是否保留。我们可以通过添加 `remove_key_name_field`
    并将其设置为 `true` 来使控制更加细致；如果解析过程成功，Fluentd 将仅删除原始属性。
- en: Listing 6.3 Chapter6/Fluentd/rotating-file-read-regex.conf—parse extract
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 Chapter6/Fluentd/rotating-file-read-regex.conf—解析提取
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Identifies the log event attribute to parse
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 识别要解析的日志事件属性
- en: ❷ Tells Fluentd to retain the existing value so if there are more attributes,
    we can retrieve them downstream
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 告诉 Fluentd 保留现有值，因此如果有更多属性，我们可以在下游检索它们
- en: ❸ Tells Fluentd what data type the extracted values should be, making further
    transformation easier
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 告诉 Fluentd 提取的值应该是什么数据类型，这使得进一步的转换更容易
- en: 6.4 Demonstrating change impact with stdout in action
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 使用 stdout 展示变更影响
- en: As the application generating logs already needs to be securely locked down
    to limit the impact of recording this information, the Fluentd installation will
    be collocated with the source. As manipulating log events is discouraged, the
    decision has been made to
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成日志的应用程序已经需要被安全锁定以限制记录此信息的影响，Fluentd 的安装将与源一起部署。由于操作日志事件是不被推荐的，因此已经决定
- en: Add to the Fluentd configuration so that stdout outputs show the unmodified
    log event, so they can be observed in a contained but transient situation
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其添加到 Fluentd 配置中，以便 stdout 输出显示未修改的日志事件，这样它们可以在一个受控但短暂的环境中观察到。
- en: Allow the modified log events that are desensitized to go into Elasticsearch
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许经过脱敏处理的修改后的日志事件进入 Elasticsearch。
- en: '`Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf` is the starting
    point for making the required changes.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf` 是进行必要更改的起点。'
- en: 6.4.1 A solution demonstrating change impact with stdout in action
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 使用 stdout 展示更改影响的解决方案
- en: You can compare your configuration modifications to our implementation of the
    solution shown in `Chapter6/ExerciseResults/file-source-transformed-elastic-search-out-Answer.conf.`
    The fundamental changes are the positioning of the filters with the type set to
    stdout.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将您的配置修改与我们在 `Chapter6/ExerciseResults/file-source-transformed-elastic-search-out-Answer.conf`
    中展示的解决方案实现进行比较。基本更改是带有类型设置为 stdout 的过滤器的位置。
- en: 6.5 Extract to set key values
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 从日志中提取设置键值
- en: Sometimes we need to be clever and set the primary attributes of the log event
    (time and tag) more dynamically. This may be because we don’t want to have a static
    value as part of the tag configuration (we typically set reflecting the source),
    but rather have it set dynamically reflecting an attribute of the log event. In
    doing so, we set ourselves up to filter more effectively with the match expressions.
    For example, we want the log event tag to reflect the name of a microservice,
    but we’re collecting the log events from Kubernetes-level stdout, so a single
    feed will reflect from multiple services. As a result, we need to extract the
    value needed as the tag from the log event record.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要聪明一些，更动态地设置日志事件的次要属性（时间和标签）。这可能是因为我们不希望在标签配置中有一个静态值（我们通常设置为反映源），而是希望它动态地反映日志事件的属性。通过这样做，我们为自己设置了一个更有效地使用匹配表达式的过滤环境。例如，我们希望日志事件的标签反映微服务的名称，但我们从
    Kubernetes 级别的 stdout 收集日志事件，因此单个源将反映多个服务。因此，我们需要从日志事件记录中提取作为标签所需的价值。
- en: When it comes to the timestamp, we may wish to adjust it to a time value in
    the log event, reflecting when the event occurred rather than when Fluentd picked
    the log event up. This may be necessary, as there is some latency between the
    event generation and Fluentd getting it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到时间戳时，我们可能希望将其调整为日志事件中的时间值，反映事件发生的时间而不是 Fluentd 捕获日志事件的时间。这可能是有必要的，因为事件生成和
    Fluentd 获取它之间存在一些延迟。
- en: The `extract` feature allows us to perform such a task. Unlike our filters and
    parsers, the `extract` directive can be incorporated into the `source`, `filter`,
    and `match` (out) plugins such as `exec`. The extract mechanism is very flexible
    in its use, but it is limited to only manipulate the tag and time log event attributes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract` 功能允许我们执行此类任务。与我们的过滤器和解析器不同，`extract` 指令可以集成到 `source`、`filter` 和
    `match`（输出）插件，如 `exec`。extract 机制在用途上非常灵活，但它仅限于仅操作标签和时间日志事件属性。'
- en: The `extract` parameters allow us to declare how to interpret the time value.
    The value to be used could be the time represented as seconds from epoch (midnight
    1 Jan 1970 [UTC]). For example, `1601495341` is Wednesday, 30 September 2020 19:49:01\.
    Another possible format can be the *ISO 8601* standard (more at [www.w3.org/TR/NOTE-datetime](http://www.w3.org/TR/NOTE-datetime)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract` 参数允许我们声明如何解释时间值。要使用的时间值可以是自纪元（1970年1月1日午夜）以来的秒数。例如，`1601495341` 代表
    2020年9月30日星期三 19:49:01。另一种可能的格式是 *ISO 8601* 标准（更多信息请参阅 [www.w3.org/TR/NOTE-datetime](http://www.w3.org/TR/NOTE-datetime)）。'
- en: Let’s consider a simple example. We should set the tag using the value obtained
    by the `exec` source plugin. As with our previous use of `exec`, we’ve chosen
    a simple command that is easy to use or adapt to different OSes. We also get a
    structured object, so there are no distractions from needing to parse the payload
    before retrieving a value. The `source` directive needs to set the `tag` attribute
    to a value that won’t come from our `exec` plugin.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子。我们应该使用 `exec` 源插件获取的值来设置标签。就像我们之前对 `exec` 的使用一样，我们选择了一个简单且易于使用或适应不同操作系统的命令。我们还得到了一个结构化对象，因此无需在检索值之前解析有效负载。`source`
    指令需要将 `tag` 属性设置为不会来自我们的 `exec` 插件的值。
- en: To ensure we can see the impact of the data changing, let’s set the `run_interval`
    to 10 seconds. This means the same file will be captured as the input but gives
    us time to save changes to the file between executions. Try changing the file
    when running the configuration. We’ve told the parser to ensure that the `exec`
    command is then treated as a JSON object.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们可以看到数据变化的影响，让我们将 `run_interval` 设置为 10 秒。这意味着相同的文件将被捕获作为输入，但给我们时间在执行之间保存对文件的更改。尝试在运行配置时更改文件。我们已经告诉解析器确保
    `exec` 命令被视为一个 JSON 对象。
- en: Finally, we have included in the extract the `tag_key` attribute; this tells
    Fluentd which log event element should be retrieved and used to set the tag. We
    copy the contents of the log event element to another tag to preserve the original
    log event record. This attribute is called `keep_tag_key`*,* and we’ve elected
    to retain the captured payload unmodified. This is demonstrated in the following
    listing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在提取中包含了 `tag_key` 属性；这告诉 Fluentd 哪个日志事件元素应该被检索并用于设置标签。我们将日志事件元素的副本复制到另一个标签中，以保留原始日志事件记录。此属性称为
    `keep_tag_key`*，* 我们选择保留捕获的有效负载未做修改。这在下述列表中演示。
- en: Listing 6.4 Chapter6/Fluentd/exec-source-extract-stdout.conf
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 Chapter6/Fluentd/exec-source-extract-stdout.conf
- en: '[PRE6]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Keeps the source capture iterating so we can modify the payload and see the
    consequences
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 保持源捕获迭代，以便我们可以修改有效负载并看到后果
- en: ❷ The extract directive
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取指令
- en: ❸ Identifies the name of the element to use as the tag going forward
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 识别将用作标签的元素名称
- en: ❹ Indicates we want to leave the retrieved log event content unmodified
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 表示我们希望保留检索到的日志事件内容未做修改
- en: To run this configuration, we need only run Fluentd with the command `fluentd
    -c ./Chapter6/Fluentd/exec-source-extract-stdout.conf`. Once Fluentd is running,
    change the value in the `TestData\exe-src.json` file and see how the change impacts
    the tag.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此配置，我们只需使用命令 `fluentd -c ./Chapter6/Fluentd/exec-source-extract-stdout.conf`
    运行 Fluentd。一旦 Fluentd 开始运行，更改 `TestData\exe-src.json` 文件中的值，并查看更改如何影响标签。
- en: 6.6 Deriving new data values with the record_transformer
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 使用 record_transformer 派生新的数据值
- en: With the ability to exclude log events from subsequent actions and extract specific
    values from log events, we can now consider the possibility of generating derived
    values and metrics. For example, we may want to understand how often errors occur,
    or which components or even which parts of the codebase are the source of most
    errors. While generating such measures using Elasticsearch or Splunk is possible,
    they are used sometime after the event analysis. If we want to be more proactive,
    we need to calculate the metrics more dynamically as the events occur.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过能够排除后续操作中的日志事件并从日志事件中提取特定值，我们现在可以考虑生成派生值和指标的可能性。例如，我们可能想了解错误发生的频率，或者哪些组件甚至哪些代码库的部分是错误的主要来源。虽然使用
    Elasticsearch 或 Splunk 生成此类度量是可能的，但它们是在事件分析之后使用的。如果我们想更加主动，我们需要在事件发生时更动态地计算这些指标。
- en: In chapter 1, we introduced the idea that monitoring covers both textual-based
    contents and numeric metrics. Both log events and metrics are often, but not always,
    used within a time-based context (log events are seen in time order, and metrics
    often measure details such as a value over a period, such as CPU usage per second).
    Fluentd’s core doesn’t have the capabilities to generate time series–based metrics.
    However, some plugins written by the community, including the contributors to
    the core of Fluentd, can provide some basic numeric and time-series measures.
    As we’ll see later, there are ways to address time-series data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们介绍了监控既包括基于文本的内容，也包括数值指标。日志事件和指标通常，但不总是，在基于时间的环境中使用（日志事件按时间顺序显示，指标通常测量诸如在一定时期内的值，如每秒的CPU使用情况等）。Fluentd的核心没有生成基于时间序列的指标的能力。然而，一些由社区编写的插件，包括Fluentd核心的贡献者，可以提供一些基本的数值和时间序列度量。正如我们稍后将要看到的，有方法可以处理时间序列数据。
- en: Time-series data points are not the only valuable numeric data that could be
    helpful. For example, how an alert is signaled may be a function of the transaction
    value (unit value × quantity) or how long or how many times a system has been
    retrying and failing with a database connection (current time – original error
    timestamp). The `record_transformer` can generate numeric metric values by taking
    data values and performing mathematical operations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据点并不是唯一有价值的数值数据，可能有助于解决问题。例如，警报是如何被触发的，可能是交易价值（单位价值×数量）的函数，或者系统尝试与数据库连接失败的时间或次数（当前时间-原始错误时间戳）。`record_transformer`可以通过取数据值并执行数学运算来生成数值指标值。
- en: 'Using our example data set, we could consider replacing the age with the birth
    year, as the expression would be the current year minus age. For example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的示例数据集，我们可以考虑用出生年份替换年龄，因为表达式将是当前年份减去年龄。例如：
- en: '[PRE7]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: While this may not be a very real-world example, it does show the art of the
    possible.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能不是一个非常贴近现实世界的例子，但它确实展示了可能性的艺术。
- en: NOTE Quoting attributes needs to be done with care. If wrongly used, you will
    experience odd behaviors, where some log event attributes are found and others
    are not. When using the `record['xxxx']` approach, you need to use single quotes.
    Double quotes are necessary when using the dig method—that is, `record.dig("xxxx")`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：引用属性需要谨慎进行。如果使用不当，你可能会遇到一些日志事件属性被找到而其他属性没有被找到的奇怪行为。当使用`record['xxxx']`方法时，需要使用单引号。使用双引号是必要的，即使用dig方法，即`record.dig("xxxx")`。
- en: 6.6.1 Putting the incorporation of calculations into a log event transformation
    into action
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 将计算结果的合并到日志事件转换中付诸实践
- en: Some people hold the view that adopting birth years is less personal than age.
    This means you’ve been asked to amend the logged data that is stored downstream.
    The case has been made that the birth year is added, and the age attribute is
    removed. The `Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf`
    configuration file has been identified as the starting point to incorporate the
    necessary changes. The same test source (`./Chapter6/SimulatorConfig/log-source
    -1.properties`) can be used to exercise the configuration. To make it easy to
    identify output from this scenario, change the `index_name` in your match configuration.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为采用出生年份比年龄更不具个人色彩。这意味着你被要求修改存储在下游的日志数据。已经提出，应添加出生年份，并删除年龄属性。`Chapter6/Fluentd/file-source-transformed-elastic-search-out.conf`配置文件已被确定为合并必要更改的起点。可以使用相同的测试源（`./Chapter6/SimulatorConfig/log-source
    -1.properties`）来练习配置。为了便于识别此场景的输出，请在匹配配置中更改`index_name`。
- en: Answer
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: Our implementation of the configuration can be seen in `Chapter6/ExerciseResults/file-source-transformed-elastic-search-out-Answer2.conf`.
    The essential changes are the inclusion of `birthYr ${Date.today.year - record
    ['age']}` in the `record` directive and after the `remove_keys $.age` directive.
    The results can be examined in the contents of Elasticsearch using the UI as previously
    described.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对配置的实现可以在`Chapter6/ExerciseResults/file-source-transformed-elastic-search-out-Answer2.conf`中看到。基本更改是在`record`指令中包含`birthYr
    ${Date.today.year - record ['age']}`，并在`remove_keys $.age`指令之后。结果可以通过使用之前描述的UI在Elasticsearch的内容中检查。
- en: 6.7 Generating simple Fluentd metrics
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 生成简单的Fluentd指标
- en: Fluentd has an excellent partner project under the control of the CNCF in *Prometheus*
    ([https://prometheus.io/](https://prometheus.io/)), whose role is to handle and
    create metrics-based data. Prometheus is typically also associated with Grafana
    for the visualization of such data. Prometheus and Grafana are associated with
    microservices. Like Fluentd, there aren’t any real constraints or reasons for
    not using such tools outside of a microservice ecosystem.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 在 CNCF 的控制下有一个优秀的合作伙伴项目，即 *Prometheus* ([https://prometheus.io/](https://prometheus.io/))，其作用是处理和创建基于指标的度量数据。Prometheus
    通常也与 Grafana 相关联，用于可视化这些数据。Prometheus 和 Grafana 与微服务相关联。像 Fluentd 一样，在微服务生态系统之外使用这些工具并没有真正的限制或原因。
- en: The partnership of Fluentd and Prometheus
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 和 Prometheus 的合作关系
- en: Given the mention of Prometheus, it is worth seeing how Fluentd can fit in with
    Prometheus’s architecture and broader metrics and monitoring ecosystem. As the
    following figure shows, Fluentd can relate to Prometheus at several points.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 既然提到了 Prometheus，那么看看 Fluentd 如何与 Prometheus 的架构以及更广泛的指标和监控生态系统相契合是很有价值的。如图所示，Fluentd
    在几个点上可以与 Prometheus 相关联。
- en: As the figure shows, Fluentd has several possible relationships with Prometheus,
    covering
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，Fluentd 与 Prometheus 有几种可能的关系，包括
- en: A data feed into the Push Gateway as a source from which Prometheus can calculate
    metrics.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据推送到 Push Gateway 的数据源，Prometheus 可以从中计算指标。
- en: A feed of Fluentd internal metrics in a Prometheus format ready to be processed
    by the server (no preparation step needed from the Push Gateway). This is achieved
    using the monitor_agent plugin.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以 Prometheus 格式提供的 Fluentd 内部指标流，供服务器处理（无需从 Push Gateway 进行准备步骤）。这是通过 monitor_agent
    插件实现的。
- en: A channel for recording alerts for metrics via the Alert Mgr.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Alert Mgr 记录指标警报的通道
- en: '![](../Images/CH06_UN01_Wilkins2.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_UN01_Wilkins2.png)'
- en: Prometheus architecture and how Fluentd can relate to it
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 架构以及 Fluentd 如何与之相关
- en: The Prometheus plugin for Fluentd (installed with `fluent-gem install fluent-plugin-prometheus`)
    provides several measure options. The Prometheus plugin allows us to create metric
    values in the `filter` and `match` directives.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 的 Prometheus 插件（通过 `fluent-gem install fluent-plugin-prometheus` 安装）提供了几个度量选项。Prometheus
    插件允许我们在 `filter` 和 `match` 指令中创建度量值。
- en: More about the Prometheus plugin can be found at [https://github.com/fluent/fluent-plugin-prometheus](https://github.com/fluent/fluent-plugin-prometheus),
    and information about Prometheus can be found at [https://prometheus.io](https://prometheus.io).
    There are also several books on the subject, such as Manning’s *Microservices
    in Action* by Morgan Bruce and Paulo A. Pereira (2018) ([www.manning.com/books/microservices-in-action](http://www.manning.com/books/microservices-in-action)),
    which can also help.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 Prometheus 插件的信息可以在 [https://github.com/fluent/fluent-plugin-prometheus](https://github.com/fluent/fluent-plugin-prometheus)
    找到，有关 Prometheus 的信息可以在 [https://prometheus.io](https://prometheus.io) 找到。还有几本关于这个主题的书，例如
    Morgan Bruce 和 Paulo A. Pereira（2018）的 Manning 出版的 *Microservices in Action* ([www.manning.com/books/microservices-in-action](http://www.manning.com/books/microservices-in-action))，这些书也可以提供帮助。
- en: Prometheus’s value lies in processing event series data and extracting and providing
    metrics data. If we can easily avoid sending every log event to Prometheus (or
    any other tool) to calculate basic metrics, there is an obvious case for not doing
    it. After all, why pass all this data around? As previously mentioned, there are
    community plugins to support time-series measures. The currently available plugins
    we think are worth considering for these kinds of requirements are
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 的价值在于处理事件序列数据，提取并提供度量数据。如果我们能够轻松避免将每个日志事件发送到 Prometheus（或任何其他工具）来计算基本指标，那么显然没有必要这样做。毕竟，为什么要传递所有这些数据呢？如前所述，有社区插件支持时间序列度量。我们认为，对于这些类型的请求，目前可用的插件值得考虑的是
- en: fluent-plugin-datacounter ([http://mng.bz/voX7](http://mng.bz/voX7))
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fluent-plugin-datacounter ([http://mng.bz/voX7](http://mng.bz/voX7))
- en: fluent-plugin-numeric-counter ([http://mng.bz/4j9w](http://mng.bz/4j9w))
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fluent-plugin-numeric-counter ([http://mng.bz/4j9w](http://mng.bz/4j9w))
- en: Both plugins work in a similar fashion. The data counter will count log events
    based on matches to regular expressions. The numeric counter is looking to apply
    numeric meaning to the values. For example, we could use the numeric counter to
    count log events if an event attribute has a value in the range of one to ten.
    Both count over a defined period and emit log events based on the occurrences.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个插件以类似的方式工作。数据计数器将根据正则表达式匹配来计数日志事件。数字计数器旨在将数值意义应用于值。例如，我们可以使用数字计数器来计数日志事件，如果事件属性有一个在1到10范围内的值。两者都在定义的周期内计数，并根据出现次数发射日志事件。
- en: For instance, in our previous illustration of filtering, we isolated log events
    that referred to the word *computer* in the `msg` attribute of the event. We could
    change this to record how many log events every minute include a reference to
    *computer*, rather than filter these log events in or out.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们之前的过滤示例中，我们隔离了在事件的`msg`属性中提到单词*computer*的日志事件。我们可以将其改为记录每分钟包含对*computer*引用的日志事件数量，而不是过滤这些日志事件。
- en: In listing 6.5, we have amended the configuration so that the log event’s element
    to examine is `msg`, as specified by the `count_key` attribute. We’ve only defined
    a single expression using `pattern1` and used `count_interval` using the standard
    Fluentd notation as to the duration over which to count—in this case, 1 minute.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.5 中，我们已修改配置，以便检查日志事件的元素是`msg`，如`count_key`属性所指定的。我们只定义了一个表达式，使用`pattern1`，并使用`count_interval`按照标准的Fluentd表示法定义了计数的时间段——在这种情况下，1分钟。
- en: Listing 6.5 Chapter6/Fluentd/file-source-counted-elastic-search-out.conf
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 Chapter6/Fluentd/file-source-counted-elastic-search-out.conf
- en: '[PRE8]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Defines the match directive to use the datacounter plugin
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义使用数据计数器插件的匹配指令
- en: ❷ Tells the plugin which element of the log event to examine
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 告诉插件要检查的日志事件元素
- en: ❸ Defines the period over which we are counting events
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义我们计数事件的周期
- en: ❹ By providing a numeric sequence of patterns, we can include the individual
    patterns.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供一系列数字模式，我们可以包括单个模式。
- en: Typically, we would not expect a `match` directive to allow any events onward
    without using a copy plugin. However, as the plugin utilizes the underlying emitter
    helper plugin, it can consume the matched log events and emit new events to be
    consumed downstream. To run this configuration, we need to install the fluent-gem
    by executing the command `fluent-gem install fluent-plugin-datacounter`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们不会期望一个`match`指令在没有使用复制插件的情况下允许任何事件继续。然而，由于该插件利用了底层的发射辅助插件，它可以消费匹配的日志事件并发射新的事件供下游消费。要运行此配置，我们需要通过执行命令`fluent-gem
    install fluent-plugin-datacounter`来安装fluent-gem。
- en: The way threads and timing are handled within the plugin means that while there
    are inbound log events, the calculated values are not written to Elasticsearch.
    As a result, depending on the timing, you might not see the metrics written immediately.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 插件内部处理线程和计时的方式意味着，当有传入的日志事件时，计算出的值不会写入Elasticsearch。因此，根据时间，你可能不会立即看到写入的度量。
- en: 'As with the previous Elasticsearch scenarios, it is easier to see what is stored
    by changing the index name; for example, `fluentd-book-counted.` Assuming Elasticsearch
    is ready and running, we can run the scenario with the following commands:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前Elasticsearch的场景一样，通过更改索引名称更容易看到存储的内容；例如，`fluentd-book-counted.` 假设Elasticsearch已准备好并正在运行，我们可以使用以下命令运行场景：
- en: '`fluentd -c ./Chapter6/Fluentd/file-source-counted-elastic-search-out.conf`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c ./Chapter6/Fluentd/file-source-counted-elastic-search-out.conf`'
- en: '`groovy logSimulator.groovy ./Chapter6/SimulatorConfig/log-source-1.properties`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy logSimulator.groovy ./Chapter6/SimulatorConfig/log-source-1.properties`'
- en: 6.7.1 Putting log event counting into action
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7.1 将日志事件计数投入实践
- en: The LogSimulator provides the means to set and change the rate at which log
    events are played through. Try changing the `count_interval` in the Fluentd configuration
    file and altering the LogSimulator configuration to send the log events through
    at different speeds (`SimulatorConfig/log-source-1.properties`). Add a pattern
    to the `datacounter` to locate occurrences of `Unix` in the message.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: LogSimulator提供了设置和更改日志事件播放速率的手段。尝试更改Fluentd配置文件中的`count_interval`，并修改LogSimulator配置以以不同的速度发送日志事件（`SimulatorConfig/log-source-1.properties`）。向`datacounter`添加一个模式以定位消息中的`Unix`出现次数。
- en: Answer
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The changing of the LogSimulator speed will result in changing numbers of log
    events being counted. Changing the period of the count is varied by modifying
    the `count_interval` attribute in the configuration file. The second pattern defined
    in the `match` directive should look like `pattern2 p2 Unix`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 改变 LogSimulator 的速度会导致计数的日志事件数量发生变化。通过修改配置文件中的 `count_interval` 属性来改变计数周期。在
    `match` 指令中定义的第二种模式应类似于 `pattern2 p2 Unix`。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Fluentd filters can isolate specific log events that need actions to be triggered,
    such as executing a housekeeping script.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 过滤器可以隔离需要触发操作的特定日志事件，例如执行维护脚本。
- en: The application of `record_transformation` in a filter creates the possibility
    of modifying events to add, remove, and mask the content, including a look at
    cases as to why it helps to modify log events.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过滤器中应用 `record_transformation` 创建了修改事件以添加、删除和屏蔽内容的可能性，包括查看为什么修改日志事件有帮助的案例。
- en: Applying Fluentd transformation plugins to remove and mask sensitive data in
    log events enables us to limit the impact of requirements to satisfy regulations
    (from additional auditing to additional work to establish a higher level of security
    configuration).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Fluentd 转换插件应用于日志事件中移除和屏蔽敏感数据，使我们能够限制满足法规要求（从额外的审计到建立更高安全配置的额外工作）的影响。
- en: Fluentd provides the means to navigate JSON data structures, such as a transformed
    log event payload. As a result, we can apply more intelligence to event handling.
    For example, if a log event’s customer attributes identify a high-value customer,
    we could also signal the CRM system in addition to signaling Ops.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 提供了导航 JSON 数据结构的方法，例如转换后的日志事件有效负载。因此，我们可以为事件处理应用更多的智能。例如，如果一个日志事件的客户属性识别出高价值客户，我们除了向
    Ops 发出信号外，还可以向 CRM 系统发出信号。
- en: The tag, time, and record values of an event can be manipulated. The `extract`
    and `inject` features can ensure they reflect meaningful values—for example, changing
    the tag to reflect the log event record so routing and filtering using tags can
    be more dynamic.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件标签、时间和记录值可以被修改。`extract` 和 `inject` 功能可以确保它们反映有意义的值——例如，将标签更改为反映日志事件记录，以便使用标签进行路由和过滤可以更加动态。
- en: There are pros and cons of manipulating log events, from impacting an accurate
    record of what happened or having meaningful log data for downstream use. Understanding
    the potential applications of the logs can help us determine the best course (e.g.,
    use in possible legal actions needs unaltered logs).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作日志事件有优点也有缺点，从影响事件发生的准确记录或为下游使用提供有意义的日志数据。了解日志的潜在应用可以帮助我们确定最佳方案（例如，在可能的法律行动中使用需要未更改的日志）。
- en: Fluentd can play several roles in a CNCF’s Prometheus deployment, from feeding
    specific events and their event attributes to Prometheus to capturing the Prometheus
    output data and assisting in monitoring Prometheus.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 在 CNCF 的 Prometheus 部署中可以扮演多个角色，从向 Prometheus 提供特定事件及其事件属性，到捕获 Prometheus
    输出数据并协助监控 Prometheus。
