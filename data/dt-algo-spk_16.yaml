- en: Chapter 12\. Feature Engineering in PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 特征工程在PySpark中
- en: This chapter covers design patterns for working with features of data—any measurable
    attributes, from car prices to gene values, hemoglobin counts, or education levels—when
    building machine learning models (also known as *feature engineering*). These
    processes (extracting, transforming, and selecting features) are essential in
    building effective machine learning models. Feature engineering is one of the
    most important topics in machine learning, because the success or failure of a
    model at predicting the future depends mainly on the features you choose.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了在构建机器学习模型时处理数据特征（任何可测量的属性，从汽车价格到基因值、血红蛋白计数或教育水平）的设计模式（也称为*特征工程*）。在构建有效的机器学习模型时，提取、转换和选择特征是必不可少的过程。特征工程是机器学习中最重要的主题之一，因为模型在预测未来时成功与否主要取决于您选择的特征。
- en: Spark provides a comprehensive machine learning API for many well-known algorithms
    including linear regression, logistic regression, and decision trees. The goal
    of this chapter is to present fundamental tools and techniques in PySpark that
    you can use to build all sorts of machine learning pipelines. The chapter introduces
    Spark’s powerful machine learning tools and utilities and provides examples using
    the PySpark API. The skills you learn here will be useful to an aspiring data
    scientist or data engineer. My goal is not to familiarize you with famous machine
    learning algorithms such as linear regression, principal component analysis, or
    support vector machines, since these are already covered in many books, but to
    equip you with tools (normalization, standardization, string indexing, etc.) that
    you can use in cleaning data and building models for a wide range of machine learning
    algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了广泛的机器学习API，涵盖了许多著名的算法，包括线性回归、逻辑回归和决策树。本章的目标是介绍PySpark中的基础工具和技术，您可以使用这些工具构建各种机器学习流水线。本章介绍了Spark强大的机器学习工具和实用程序，并提供了使用PySpark
    API的示例。在这里学到的技能对于有志成为数据科学家或数据工程师的人来说非常有用。我的目标不是让您熟悉著名的机器学习算法，如线性回归、主成分分析或支持向量机，因为这些内容在许多书籍中已经涵盖，而是为您提供一些工具（如归一化、标准化、字符串索引等），您可以用这些工具来清洗数据并为各种机器学习算法构建模型。
- en: No matter which algorithm you use, feature engineering is important. Machine
    learning enables us to find patterns in data—we find the patterns by building
    models, then we use the built models to make predictions about new data points
    (i.e., query data). To get those predictions right, we must construct the dataset
    and transform the data correctly. This chapter covers these two key steps.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您使用哪种算法，特征工程都是重要的。机器学习使我们能够在数据中找到模式——我们通过构建模型找到这些模式，然后使用构建的模型对新数据点（即查询数据）进行预测。为了正确预测，我们必须正确构建数据集并转换数据。本章涵盖了这两个关键步骤。
- en: 'Topics we will discuss include:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的主题包括：
- en: Adding a new derived feature
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加新的派生特征
- en: Creating and applying UDFs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和应用用户定义函数（UDF）
- en: Creating pipelines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建流水线
- en: Binarizing data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据二值化
- en: Data imputation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据插补**'
- en: Tokenization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tokenization**'
- en: Standardization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**'
- en: Normalization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**'
- en: String indexing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**String indexing**'
- en: Vector assembly
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vector assembly**'
- en: Bucketing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分桶**'
- en: Logarithm transformation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数变换
- en: One-hot encoding
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独热编码
- en: TF-IDF
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Feature hashing
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征哈希**'
- en: Applying SQL transformations
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用SQL转换
- en: First, though, let’s take a more in-depth look at feature engineering.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，首先让我们更深入地研究特征工程。
- en: Introduction to Feature Engineering
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**特征工程介绍**'
- en: In his excellent [blog post](https://oreil.ly/IO6T6) on mastering feature engineering,
    Jason Brownlee defines it as “the process of transforming raw data into features
    that better represent the underlying problem to the predictive models, resulting
    in improved model accuracy on unseen data.” In this chapter, my goal is to present
    generic feature engineering techniques available in PySpark that you can use to
    build better predictive models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在他出色的[博客文章](https://oreil.ly/IO6T6)中，Jason Brownlee将特征工程定义为“将原始数据转换为更好地代表预测模型底层问题的特征的过程，从而在未见数据上提高模型准确性”。在本章中，我的目标是介绍PySpark中通用的特征工程技术，您可以使用这些技术来构建更好的预测模型。
- en: Let’s say that your data is represented in a matrix of rows and columns. In
    machine learning, columns are called features (such as age, gender, education,
    heart rate, or blood pressure), and each row represents an instance of the dataset
    (i.e., a record). The features in your data will directly influence the predictive
    models you build and use and the results you can achieve. Data scientists spend
    around half their time on data preparation, and feature engineering is an important
    part of this.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据表示为行和列的矩阵。在机器学习中，列被称为特征（如年龄、性别、教育、心率或血压），每行代表数据集的一个实例（即记录）。数据中的特征将直接影响您构建和使用的预测模型以及您可以实现的结果。数据科学家大约一半的时间都花在数据准备上，特征工程是其中重要的一部分。
- en: 'Where does feature engineering fit in with building machine learning models?
    When do you apply these techniques to your data? Let’s take a look at the key
    steps in building and using a machine learning model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程在构建机器学习模型中的位置是什么？何时将这些技术应用于您的数据？让我们来看看构建和使用机器学习模型的关键步骤：
- en: Gather requirements for machine learning data and define the problem.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集机器学习数据的要求并定义问题。
- en: Select data (collect and integrate the data, then denormalize it into a dataset).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择数据（收集和整合数据，然后将其反规范化为数据集）。
- en: Preprocess data (format, clean, and sample the data so you can work with it).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据（格式化、清理和采样数据，以便您可以处理它）。
- en: Transform data (perform feature engineering).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换数据（执行特征工程）。
- en: Model data (split the data into training and test sets, use the training data
    to create models, then use the test data to evaluate the models and tune them).
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据建模（将数据拆分为训练集和测试集，使用训练数据创建模型，然后使用测试数据评估和调整模型）。
- en: Use the built model to make predictions on query data.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建的模型对查询数据进行预测。
- en: 'Feature engineering happens right before you build a model from your data.
    After selecting and cleansing the data (for example, making sure that null values
    are replaced with proper values), you transform the data by performing feature
    engineering: this might involve converting string into numeric data, bucketizing
    the data, normalizing or standardizing the data, etc.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程发生在从数据构建模型之前。在选择和清理数据之后（例如，确保空值被替换为适当的值），通过执行特征工程来转换数据：这可能涉及将字符串转换为数值数据、对数据进行分桶、对数据进行标准化或归一化等操作。
- en: The part of the overall process that this chapter covers is illustrated in [Figure 12-1](#feature_engineering).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的整体过程的部分如图[Figure 12-1](#feature_engineering)所示。
- en: '![daws 1201](Images/daws_1201.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1201](Images/daws_1201.png)'
- en: Figure 12-1\. Feature engineering
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. 特征工程
- en: 'The Spark API provides various [algorithms](https://oreil.ly/butJ1) for working
    with features, which are roughly divided into these groups:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark API提供了多种用于处理特征的[算法](https://oreil.ly/butJ1)，大致分为以下几类：
- en: Extraction (algorithms for extracting features from “raw” data)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取（从“原始”数据中提取特征的算法）
- en: Transformation (algorithms for scaling, converting, or modifying features)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换（用于缩放、转换或修改特征的算法）
- en: Selection (algorithms for selecting a subset from a larger set of features)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择（从更大的特征集中选择子集的算法）
- en: Locality-sensitive hashing (LSH); algorithms for grouping similar items)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部敏感哈希（LSH）；用于对相似项进行分组的算法）
- en: 'There can be many reasons for data transformation and feature engineering,
    either mandatory or optional:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换和特征工程有许多原因，可以是强制性的，也可以是可选的：
- en: Mandatory transformations
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 强制转换
- en: 'These transformations are necessary to solve a problem (such as building a
    machine learning model) for data compatibility reasons. Examples include:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换是为了解决问题（如构建机器学习模型）而必要的，以保证数据的兼容性。例如：
- en: Converting non-numeric features into numeric features. For example, if a feature
    has non-numeric values, then average, sum, and median calculations will be impossible;
    likewise, we cannot perform matrix multiplication on a string but must convert
    it to some numeric representation first.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将非数值特征转换为数值特征。例如，如果一个特征具有非数值值，则无法进行平均、求和和中位数计算；同样，我们无法对字符串执行矩阵乘法，而必须首先将其转换为某种数值表示。
- en: Resizing inputs to a fixed size. Some linear models and feed-forward neural
    networks have a fixed number of input nodes, so your input data must always have
    the same size. For example, image models need to reshape the images in their dataset
    to a fixed size.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入调整为固定大小。一些线性模型和前馈神经网络具有固定数量的输入节点，因此你的输入数据必须始终具有相同的大小。例如，图像模型需要将其数据集中的图像重塑为固定大小。
- en: Optional transformations
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的转换
- en: 'Optional data transformations may help the machine learning model to perform
    better. These transformations might include:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的数据转换可能有助于机器学习模型表现更好。这些转换可能包括：
- en: Changing text to lowercase before applying other data transformations
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用其他数据转换之前将文本转换为小写。
- en: Tokenizing and removing nonessential words, such as “of,” “a,” “and,” “the,”
    and “so”
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词和去除非必要的词，比如“of”、“a”、“and”、“the”和“so”
- en: Normalizing numeric features
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数值特征进行归一化
- en: We’ll examine both types in the following sections. Let’s dive into our first
    topic, adding a new feature.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中检查这两种类型。让我们深入探讨我们的第一个主题，添加一个新特征。
- en: Adding New Features
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加新特征
- en: 'Sometimes you want to add a new derived feature (because you need that derived
    feature in your machine learning algorithm) to your dataset, to add a new column
    or feature to your dataset, you may use the function `DataFrame.withColumn()`.
    This concept is demonstrated below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你想要添加一个新的衍生特征（因为你的机器学习算法需要这个衍生特征）到你的数据集中，为了向你的数据集添加一个新列或特征，你可以使用`DataFrame.withColumn()`函数。这个概念在下面进行了演示：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You may use Spark’s `DataFrame.withColumn()` to add a new column/feature:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Spark的`DataFrame.withColumn()`来添加一个新列/特征：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Applying UDFs
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用UDFs
- en: If PySpark does not provide the function you need, you can define your own Python
    functions and register them as user-defined functions (UDFs) with Spark SQL’s
    DSL using `spark.udf.register()`. You can then apply these functions in your data
    transformations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果PySpark没有提供你需要的函数，你可以定义自己的Python函数，并使用`spark.udf.register()`将它们注册为Spark SQL的DSL中的用户定义函数（UDFs）。然后，你可以在你的数据转换中应用这些函数。
- en: 'To make your Python functions compatible with Spark’s DataFrames, you need
    to convert them to PySpark UDFs by passing them to the `pyspark.sql.functions.udf()`
    function. Alternatively, you can create your UDF in a single step using annotations,
    as shown here. Add `udf@` as a “decorator” of your Python function, and specify
    its return type as the argument:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要使你的Python函数与Spark的DataFrame兼容，你需要通过将它们传递给`pyspark.sql.functions.udf()`函数来将它们转换为PySpark
    UDFs。或者，你可以使用注解一步创建你的UDF，如下所示。将`udf@`作为你的Python函数的“装饰器”，并将其返回类型作为参数指定：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_feature_engineering_in_pyspark_CO1-1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_feature_engineering_in_pyspark_CO1-1)'
- en: The function `tripled()` is a UDF and its return type is `integer`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`tripled()`是一个UDF，其返回类型是`integer`。
- en: '[![2](Images/2.png)](#co_feature_engineering_in_pyspark_CO1-2)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_feature_engineering_in_pyspark_CO1-2)'
- en: '`tripled_col` is a derived feature.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`tripled_col`是一个衍生特征。'
- en: Note that if your features are represented as an RDD (where each RDD element
    represents an instance of your features), you may use the `RDD.map()` function
    to add a new feature to your feature set.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你的特征表示为RDD（其中每个RDD元素表示特征的一个实例），你可以使用`RDD.map()`函数向你的特征集添加一个新特征。
- en: Creating Pipelines
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建流水线
- en: In machine learning algorithms, you can glue several stages together and run
    them in order. Consider three stages, called `{Stage-1, Stage-2, Stage-3}`, where
    the output of `Stage-1` is used as an input to `Stage-2` and the output of `Stage-2`
    is used as an input to `Stage-3`. These three stages form a simple pipeline. Suppose
    we have to transform the data in the order shown in [Table 12-1](#pipeline_stages).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习算法中，你可以将几个阶段连接在一起并按顺序运行它们。考虑三个阶段，称为`{Stage-1, Stage-2, Stage-3}`，其中`Stage-1`的输出被用作`Stage-2`的输入，`Stage-2`的输出被用作`Stage-3`的输入。这三个阶段形成一个简单的流水线。假设我们必须按照显示在[Table 12-1](#pipeline_stages)中的顺序转换数据。
- en: Table 12-1\. Pipeline stages
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-1\. 流水线阶段
- en: '| Stage | Description |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Stage | 描述 |'
- en: '| --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Stage-1` | Label encode or string index the column `dept` * (create `dept_index`
    column). |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `Stage-1` | 对`dept`列进行标签编码或字符串索引（创建`dept_index`列）。'
- en: '| `Stage-2` | Label encode or string index the column `education` (create `education_index`
    column). |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `Stage-2` | 对`education`列进行标签编码或字符串索引（创建`education_index`列）。'
- en: '| `Stage-3` | One-hot encode the indexed column `education_index` (create `education_OHE`
    column). |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `Stage-3` | 对索引列`education_index`进行独热编码（创建`education_OHE`列）。'
- en: 'Spark provides a pipeline API, defined as `pyspark.ml.Pipeline(*, stages=None)`,
    which acts as an estimator (an abstraction of a learning algorithm that fits a
    model on a dataset). According to the Spark documentation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一个pipeline API，定义为`pyspark.ml.Pipeline(*, stages=None)`，它作为一个estimator（在数据集上拟合模型的学习算法的抽象）。根据Spark的文档：
- en: A `Pipeline` consists of a sequence of stages, each of which is either an `Estimator`
    or a `Transformer`. When `Pipeline.fit()` is called, the stages are executed in
    order. If a stage is an `Estimator`, its `Estimator.fit()` method will be called
    on the input dataset to fit a model. Then the model, which is a transformer, will
    be used to transform the dataset as the input to the next stage. If a stage is
    a `Transformer`, its `Transformer.transform()` method will be called to produce
    the dataset for the next stage. The fitted model from a `Pipeline` is a `PipelineModel`,
    which consists of fitted models and transformers, corresponding to the pipeline
    stages. If stages is an empty list, the pipeline acts as an identity transformer.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个`Pipeline`由一系列阶段组成，每个阶段都是一个`Estimator`或`Transformer`。当调用`Pipeline.fit()`时，阶段将按顺序执行。如果阶段是一个`Estimator`，则会在输入数据集上调用其`Estimator.fit()`方法来拟合模型。然后这个模型，即一个transformer，将被用来转换数据集作为下一个阶段的输入。如果阶段是一个`Transformer`，则会调用其`Transformer.transform()`方法来生成下一个阶段的数据集。从`Pipeline`中拟合的模型是一个`PipelineModel`，它包含了拟合的模型和transformers，对应于pipeline的各个阶段。如果阶段是一个空列表，则pipeline充当一个identity
    transformer。
- en: 'To illustrate the concept of pipelines, first we’ll create a sample DataFrame
    with three columns to use as input data, as shown here, then we’ll create a simple
    pipeline using `pyspark.ml.Pipeline()`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明pipeline的概念，首先我们将创建一个包含三列用作输入数据的示例DataFrame，如下所示，然后我们将使用`pyspark.ml.Pipeline()`创建一个简单的pipeline：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can view our sample data with `df.show()`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`df.show()`来查看我们的样本数据：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have created the DataFrame, suppose we want to transform the data
    through three defined stages, `{stage_1, stage_2, stage_3}`. In each stage, we
    will pass the input and output column names, and we’ll set up the pipeline by
    passing the defined stages to the `Pipeline` object as a list.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了DataFrame，假设我们想通过三个定义的阶段 `{stage_1, stage_2, stage_3}` 转换数据。在每个阶段中，我们将传递输入和输出列名称，并通过将定义的阶段作为列表传递给`Pipeline`对象来设置pipeline。
- en: Spark’s pipeline model then performs specific steps one by one in a sequence
    and gives us the final desired result. [Figure 12-2](#pipeline_with_three_stages)
    shows the pipeline we will define.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的pipeline模型然后按顺序执行特定步骤，并给出最终的期望结果。[图 12-2](#pipeline_with_three_stages)
    显示了我们将定义的pipeline。
- en: '![daws 1202](Images/daws_1202.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1202](Images/daws_1202.png)'
- en: Figure 12-2\. A sample pipeline with three stages
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2\. 一个包含三个阶段的示例pipeline
- en: 'The three stages are implemented as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个阶段的实现如下：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we’ll define our pipeline with these three stages:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义包含这三个阶段的pipeline：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Binarizing Data
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据二值化
- en: '*Binarizing* data means setting the feature values to 0 or 1 according to some
    threshold. Values greater than the threshold map to 1, while values less than
    or equal to the threshold map to 0\. With the default threshold of 0, only positive
    values map to 1\. Binarization is thus the process of thresholding numerical features
    to binary `{0, 1}` features.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*二值化*数据意味着根据某个阈值将特征值设置为0或1。大于阈值的值映射为1，小于或等于阈值的值映射为0。使用默认阈值0，只有正值映射为1。因此，二值化是将数值特征阈值化为二进制
    `{0, 1}` 特征的过程。'
- en: Spark’s `Binarizer` takes the parameters `inputCol` and `outputCol`, as well
    as the `threshold` for binarization. Feature values greater than the threshold
    are binarized to `1.0`; values equal to or less than the threshold are binarized
    to `0.0`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`Binarizer`接受参数`inputCol`和`outputCol`，以及二值化的`threshold`。大于阈值的特征值被二值化为`1.0`；小于或等于阈值的特征值被二值化为`0.0`。
- en: 'First, let’s create a DataFrame with a single feature:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个只有单个特征的DataFrame：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we’ll create a `Binarizer` with `threshold=0.5`, so any value less than
    or equal to `0.5` will map into `0.0` and any value greater than `0.5` will map
    into `1.0`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个`Binarizer`，设置`threshold=0.5`，因此任何小于或等于`0.5`的值将映射为`0.0`，任何大于`0.5`的值将映射为`1.0`：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we apply the defined `Binarizer` to a feature column:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将定义的`Binarizer`应用于特征列：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Imputation
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 补全
- en: 'Spark’s `Imputer` is an imputation transformer for filling in missing values.
    Real-world datasets commonly contain missing values, often encoded as nulls, blanks,
    `NaN`s, or other placeholders. There are many methods to handle these values,
    including the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的 `Imputer` 是用于填充缺失值的填充转换器。现实世界的数据集通常包含缺失值，通常编码为null、空白、`NaN`或其他占位符。有许多方法可以处理这些值，包括以下方法：
- en: Delete instances if there is any missing feature (this might not be such a good
    idea since important information from other features will be lost).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在任何缺失的特征，则删除实例（这可能不是一个好主意，因为会丢失其他特征的重要信息）。
- en: For a missing feature, find the average value of that feature and fill in that
    value.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于缺失的特征，找到该特征的平均值并填充该值。
- en: Impute the missing values, (i.e., to infer them from the known part of the data).
    This is often the best strategy.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对缺失值进行填充（即从数据的已知部分推断出缺失值）。这通常是最佳策略。
- en: 'Spark’s `Imputer` has the following signature:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的 `Imputer` 具有以下签名：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It uses either the mean or the median of the columns in which the missing values
    are located. The input columns should be of numeric type; currently `Imputer`
    does not support categorical features and may create incorrect values for a categorical
    feature.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用所在列的均值或中位数来填补缺失值。输入列应为数值型；目前 `Imputer` 不支持分类特征，可能会为分类特征创建不正确的值。
- en: Note that the mean/median/mode value is computed after filtering out missing
    values. All `null` values in the input columns are treated as missing, and so
    are also imputed. For computing the median, `pyspark.sql.DataFrame.approxQuantile()`
    is used with a relative error of 0.001.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在过滤掉缺失值后计算均值/中位数/模式值。输入列中的所有 `null` 值都视为缺失值，因此也会被填充。对于计算中位数，使用 `pyspark.sql.DataFrame.approxQuantile()`
    并设定相对误差为 0.001。
- en: You can instruct the imputer to impute custom values other than `NaN` by using
    `.setMissingValue(*custom_value*)`. For example, `.setMissingValue(0)` tells it
    to impute all occurrences of `0` (again, `null` values in the input columns will
    be treated as missing and also imputed).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指示imputer通过 `.setMissingValue(*custom_value*)` 来填充除 `NaN` 以外的自定义值。例如，`.setMissingValue(0)`
    告诉它填充所有 `0` 的出现次数（再次强调，输入列中的 `null` 值将被视为缺失并进行填充）。
- en: 'The following example shows how an imputer can be used. Suppose that we have
    a DataFrame with three columns, `id`, `col1`, and `col2`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子展示了如何使用imputer。假设我们有一个包含三列 `id`、`col1` 和 `col2` 的DataFrame：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, let’s create an imputer and apply it to our created data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个imputer并将其应用于我们创建的数据：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'How did we get the numbers to use for the missing values (`8.0` for `col1`
    and `9.0` for `col2`)? It’s easy; since the default strategy is “mean,” we simply
    compute the averages for each column and use those for the missing values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何得到用于缺失值的这些数字（`col1` 的 `8.0` 和 `col2` 的 `9.0`）？很简单；因为默认策略是“均值”，我们只需计算每列的平均值并将其用于缺失值：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Based on your data requirements, you may want to use a different strategy to
    fill in the missing values. You can instruct the imputer to use the median of
    available feature values instead as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的数据需求，您可能希望使用不同的策略来填充缺失值。您可以指示imputer使用可用特征值的中位数代替缺失值，如下所示：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To get these values (`7.0` for `col1` and `10.0` for `col2`), we just compute
    the median value for each column:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取这些值（`col1` 的 `7.0` 和 `col2` 的 `10.0`），我们只需计算每列的中位数值：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tokenization
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenization algorithms are used to split a phrase, sentence, paragraph, or
    entire text document into smaller units, such as individual words, bigrams, or
    terms. These smaller units are called *tokens*. For example, the lexical analyzer
    (an algorithm used in compiler writing) breaks programming code into a series
    of tokens by removing any whitespace or comments. Therefore, you can think of
    tokenization more generally as the process of splitting a string into any kind
    of meaningful tokens.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 分词算法用于将短语、句子、段落或整个文本文档分割成较小的单元，如单词、二元组或术语。这些较小的单元称为 *tokens*。例如，词法分析器（编译器编写中使用的算法）通过移除任何空格或注释，将编程代码分割成一系列tokens。因此，您可以将分词更普遍地理解为将字符串拆分为任何有意义的tokens的过程。
- en: In Spark, you can use the `Tokenizer` and `RegexTokenizer` (which allows you
    to define custom tokenization strategies through regular expressions) to tokenize
    strings.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，您可以使用 `Tokenizer` 和 `RegexTokenizer`（通过正则表达式定义自定义分词策略）来对字符串进行分词。
- en: Tokenizer
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器
- en: 'Spark’s `Tokenizer` is a tokenizer that converts the input string to lowercase
    and then splits it by whitespace. To show how this works, let’s create some sample
    data:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`Tokenizer`是一个将输入字符串转换为小写并按空格分割的分词器。为了展示它的工作原理，让我们创建一些示例数据：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then apply the `Tokenizer`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后应用`Tokenizer`：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: RegexTokenizer
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式分词器（RegexTokenizer）
- en: 'Spark’s `RegexTokenizer` is a regular expression–based tokenizer that extracts
    tokens either by using the provided regex pattern to split the text (the default)
    or repeatedly matching the regex (if the optional `gaps` parameter, which is `True`
    by default, is `False`). Here’s an example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`RegexTokenizer`是基于正则表达式的分词器，它通过使用提供的正则表达式模式拆分文本（默认情况）或反复匹配正则表达式（如果可选的`gaps`参数，默认值为`True`，为`False`）来提取令牌。下面是一个示例：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Tokenization with a Pipeline
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道中的分词
- en: 'We can also perform tokenization as part of a pipeline. Here, we create a DataFrame
    with two columns:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以作为管道的一部分执行分词。这里，我们创建一个包含两列的DataFrame：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we apply the `RegexTokenizer()` function to this DataFrame:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对这个DataFrame应用`RegexTokenizer()`函数：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Standardization
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化
- en: One of the most popular techniques for scaling numerical data prior to building
    a model is standardization. Standardizing a dataset involves rescaling the distribution
    of values so that the mean of observed values (as a feature) is `0.00` and the
    standard deviation is `1.00`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型之前，最流行的数值数据缩放技术之一是标准化。标准化数据集涉及重新缩放值的分布，使观察值的均值（作为特征）为`0.00`，标准差为`1.00`。
- en: Many machine learning algorithms perform better when numerical input variables
    (features) are scaled to a standard range. For example, algorithms such as linear
    regression that use a weighted sum of the input and algorithms like k-nearest
    neighbors that use distance measures require standardized values, as otherwise
    the built models might underfit or overfit the training data and underperform.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法在数值输入变量（特征）缩放到标准范围时表现更好。例如，使用输入加权和的线性回归算法和使用距离度量的k最近邻算法需要标准化值，否则构建的模型可能会欠拟合或过拟合训练数据，并且性能较差。
- en: 'A value is standardized as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值的标准化公式如下：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Where the mean is calculated as:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 均值的计   均值的计算公式为：
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: <math alttext="x overbar equals StartFraction 1 Over upper N EndFraction sigma-summation
    Underscript i equals 1 Overscript upper N Endscripts x Subscript i" display="block"><mrow><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover> <mo>=</mo> <mfrac><mn>1</mn> <mi>N</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>x</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x overbar equals StartFraction 1 Over upper N EndFraction sigma-summation
    Underscript i equals 1 Overscript upper N Endscripts x Subscript i" display="block"><mrow><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover> <mo>=</mo> <mfrac><mn>1</mn> <mi>N</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>x</mi> <mi>i</mi></msub></mrow></math>
- en: 'And the standard deviation is calculated as:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差的计算公式为：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: <math alttext="s d equals StartRoot StartFraction 1 Over upper N EndFraction
    sigma-summation Underscript i equals 1 Overscript upper N Endscripts left-parenthesis
    x Subscript i Baseline minus x overbar right-parenthesis squared EndRoot" display="block"><mrow><mi>s</mi>
    <mi>d</mi> <mo>=</mo> <msqrt><mrow><mfrac><mn>1</mn> <mi>N</mi></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s d equals StartRoot StartFraction 1 Over upper N EndFraction
    sigma-summation Underscript i equals 1 Overscript upper N Endscripts left-parenthesis
    x Subscript i Baseline minus x overbar right-parenthesis squared EndRoot" display="block"><mrow><mi>s</mi>
    <mi>d</mi> <mo>=</mo> <msqrt><mrow><mfrac><mn>1</mn> <mi>N</mi></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: 'For example, if `X = (1, 3, 6, 10)`, the mean/average is calculated as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，若`X = (1, 3, 6, 10)`，则均值/平均值计算为：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'and the standard deviation is calculated as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差的计算公式为：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'So, the new standardized values will be :'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，新标准化的值将是：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'where:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, the mean of the standardized values `(y)` is `0.00` and the
    standard deviation is `1.00`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，标准化值`(y)`的均值为`0.00`，标准差为`1.00`。
- en: 'Let’s go over how to perform standardization in PySpark. Let’s say that we
    are trying to standardize (`mean = 0.00`, `stddev = 1.00`) one column in a DataFrame.
    First we’ll create a sample DataFrame, then I’ll show you two ways to standardize
    the `age` column:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何在PySpark中执行标准化。假设我们试图将DataFrame中的一列进行标准化（`均值 = 0.00`，`标准差 = 1.00`）。首先，我们将创建一个示例DataFrame，然后我将向你展示两种标准化`age`列的方法：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Method 1 is to use DataFrame functions:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 方法1是使用DataFrame函数：
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'or alternatively, we may write this as:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以这样写：
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Method 2 is to use functions from PySpark’s `ml` package. Here, we use `pyspark.ml.feature.VectorAssembler()`
    to transform the `age` column into a vector, then standardize the values with
    Spark’s `StandardScaler`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 方法2是使用PySpark的`ml`包中的函数。这里，我们使用`pyspark.ml.feature.VectorAssembler()`将`age`列转换为向量，然后使用Spark的`StandardScaler`对值进行标准化：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Unlike normalization, which we’ll look at next, standardization can be helpful
    in cases where the data follows a Gaussian distribution. It also does not have
    a bounding range, so if you have outliers in your data they will not be impacted
    by standardization.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们接下来要讨论的归一化不同，标准化在数据服从高斯分布的情况下非常有帮助。它也没有边界范围，因此如果数据中存在离群值，标准化不会受到影响。
- en: Normalization
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化
- en: Normalization is a scaling technique often applied as part of data preparation
    for machine learning. The goal of normalization is to change the values of numeric
    columns in the dataset to use a common scale, without distorting differences in
    the ranges of values or losing information. Normalization scales each numeric
    input variable separately to the range `[0,1]`, which is the range for floating-point
    values, where we have the most precision. In other words, the feature values are
    shifted and rescaled so that they end up ranging between `0.00` and `1.00`. This
    technique is also known as *min-max scaling*, and Spark provides a transformer
    for this purpose called `MinMaxScaler`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化是机器学习中常用的数据准备技术之一。归一化的目标是将数据集中数值列的值更改为一个公共的尺度，而不失真值的差异或丢失信息。归一化将每个数值输入变量分别缩放到范围`[0,1]`内，这是浮点值的范围，具有最高的精度。换句话说，特征值被移动和重新缩放，以便它们最终在`0.00`到`1.00`之间。这种技术也称为*min-max
    scaling*，Spark提供了一个名为`MinMaxScaler`的转换器用于此目的。
- en: 'Here’s the formula for normalization:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是归一化的公式：
- en: <math alttext="upper X overTilde Subscript i Baseline equals StartFraction upper
    X Subscript i Baseline minus upper X Subscript m i n Baseline Over upper X Subscript
    m a x Baseline minus upper X Subscript m i n Baseline EndFraction" display="block"><mrow><msub><mover
    accent="true"><mi>X</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mi>X</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>X</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow>
    <mrow><msub><mi>X</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <mo>-</mo><msub><mi>X</mi>
    <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfrac></mrow></math>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper X overTilde Subscript i Baseline equals StartFraction upper
    X Subscript i Baseline minus upper X Subscript m i n Baseline Over upper X Subscript
    m a x Baseline minus upper X Subscript m i n Baseline EndFraction" display="block"><mrow><msub><mover
    accent="true"><mi>X</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mi>X</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>X</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow>
    <mrow><msub><mi>X</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <mo>-</mo><msub><mi>X</mi>
    <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfrac></mrow></math>
- en: Note that *X[max]* and *X[min]* are the maximum and minimum values of the given
    feature, *X[i]*, respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*X[max]*和*X[min]*是给定特征*X[i]*的最大值和最小值，分别。
- en: 'To illustrate the normalization process, let’s create a DataFrame with three
    features:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明归一化过程，让我们创建一个具有三个特征的DataFrame：
- en: '[PRE33]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we’ll apply the `MinMaxScaler` to our features:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`MinMaxScaler`应用于我们的特征：
- en: '[PRE34]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And examine the scaled values:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 并查看缩放后的值：
- en: '[PRE35]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Normalization is a good technique to use when you know that your data does not
    follow a Gaussian distribution. This can be useful in algorithms that do not assume
    any distribution of the data, like linear regression, *k*-nearest neighbors, and
    neural networks. In the following sections, we’ll walk through a few more examples.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当你知道你的数据不遵循高斯分布时，归一化是一个好的技术。这在不假设数据分布的算法中非常有用，例如线性回归、*k*-最近邻和神经网络。在接下来的几节中，我们将通过几个更多的例子来演示。
- en: Scaling a Column Using a Pipeline
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在管道中使用MinMaxScaler来缩放列
- en: 'As with tokenization, we can apply normalization in a pipeline. First, let’s
    define a set of features:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与标记化一样，我们可以在管道中应用归一化。首先，让我们定义一组特征：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now apply `MinMaxScaler` in a pipeline as follows to normalize the values
    of feature (column) `x`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按照以下方式在管道中应用`MinMaxScaler`来归一化特征（列）`x`的值：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Using MinMaxScaler on Multiple Columns
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多列上使用MinMaxScaler
- en: 'We can also apply a scaler (such as `MinMaxScaler`) on multiple columns:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在多列上应用缩放器（如`MinMaxScaler`）：
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can do some postprocessing to recover the original column names:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以进行一些后处理来恢复原始列名：
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output will be:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Normalization Using Normalizer
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Normalizer进行归一化
- en: 'Spark’s `Normalizer` transforms a dataset of `Vector` rows, normalizing each
    `Vector` to have unit norm (i.e., a length of 1). It takes a parameter `p` from
    the user, which represents the p-norm. For example, you can set `p=1` to use the
    Manhattan norm (or Manhattan distance) or `p=2` to use the Euclidean norm:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`Normalizer`将一个`Vector`行数据集转换为单位范数（即长度为1）的`Vector`。它接受来自用户的参数`p`，表示p-范数。例如，您可以设置`p=1`来使用曼哈顿范数（或曼哈顿距离），或者`p=2`来使用欧几里得范数：
- en: '[PRE41]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: String Indexing
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串索引
- en: Most machine learning algorithms require the conversion of categorical features
    (such as strings) into numerical ones. String indexing is the process of converting
    strings to numerical values.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法要求将分类特征（例如字符串）转换为数值特征。字符串索引是将字符串转换为数值的过程。
- en: Spark’s `StringIndexer` is a label indexer that maps a string column of labels
    to a column of label indices. If the input column is numeric, we cast it to string
    and index the string values. The indices are in the range `[0, numLabels)`. By
    default, they are ordered by label frequency in descending order, so the most
    frequent label gets the index `0`. The ordering behavior is controlled by setting
    the `stringOrderType` option.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`StringIndexer`是一个标签索引器，它将标签的字符串列映射到标签索引的列。如果输入列是数值型的，我们将其转换为字符串并索引字符串值。索引的范围为`[0,
    numLabels)`。默认情况下，它们按照标签频率降序排序，因此最频繁的标签得到索引`0`。排序行为由设置`stringOrderType`选项来控制。
- en: Applying StringIndexer to a Single Column
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将StringIndexer应用于单列
- en: 'Suppose we have the following PySpark DataFrame:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下PySpark DataFrame：
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If we want to transform it to use with `pyspark.ml`, we can use Spark’s `StringIndexer`
    to convert the `name` column to a numeric column, as shown here:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将其转换为适用于`pyspark.ml`，我们可以使用Spark的`StringIndexer`将`name`列转换为数值列，如下所示：
- en: '[PRE44]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Applying StringIndexer to Several Columns
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将StringIndexer应用于多个列
- en: 'What if we want to apply `StringIndexer` to several columns at once? The simple
    way to do this is to combine several `StringIndex`es in a `list()` function and
    use a `Pipeline` to execute them all:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要一次将`StringIndexer`应用于多个列，简单的方法是将多个`StringIndex`组合在`list()`函数中，并使用`Pipeline`来执行它们所有：
- en: '[PRE45]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Next, I’ll dig a little deeper into the `VectorAssembler`, introduced in [“Standardization”](#standardization_section).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将更深入地探讨`VectorAssembler`，介绍在[“标准化”](#standardization_section)中引入的内容。
- en: Vector Assembly
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量组装
- en: 'The main function of the `VectorAssembler` is to concatenate a set of features
    into a single vector which can be passed to the estimator or machine learning
    algorithm. In other words, it’s a feature transformer that merges multiple columns
    into a single vector column. Suppose we have the following DataFrame:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorAssembler`的主要功能是将一组特征连接成一个单独的向量，可以传递给估算器或机器学习算法。换句话说，它是一个特征转换器，将多个列合并为一个单独的向量列。假设我们有以下DataFrame：'
- en: '[PRE46]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can apply the `VectorAssembler` to these three features (`col1`, `col2`,
    and `col3`) and merge them into a vector column named `features`, as shown here:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`VectorAssembler`应用于这三个特征（`col1`、`col2`和`col3`），并将它们合并为一个名为`features`的向量列，如下所示：
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If you want to skip rows that have `NaN` or `null` values, you can do this
    by using `VectorAssembler.setParams(handleInvalid="skip")`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要跳过具有`NaN`或`null`值的行，您可以使用`VectorAssembler.setParams(handleInvalid="skip")`来实现：
- en: '[PRE48]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Bucketing
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分桶
- en: Data binning—also called discrete binning or bucketing—is a data preprocessing
    technique used to reduce the effects of minor observation errors. With this technique,
    the original data values that fall into a given small interval (a bin) are replaced
    by a value representative of that interval, often the central value. For example,
    if you have data on car prices where the values are widely scattered, you may
    prefer to use bucketing instead of the actual individual car prices.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分桶，也称为离散分桶或分箱，是一种用于减少小观察误差影响的数据预处理技术。通过这种技术，原始数据值落入给定小间隔（桶）中的数据被替换为该间隔的代表值，通常是中心值。例如，如果您有汽车价格数据，其中值广泛分布，您可能更喜欢使用分桶而不是实际的个别汽车价格。
- en: Spark’s Bucketizer transforms a column of continuous features to a column of
    feature buckets, where the buckets are specified by the user.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的Bucketizer将连续特征列转换为特征桶列，其中桶由用户指定。
- en: 'Consider this example: there’s no linear relationship between latitude and
    housing values, but you may suspect that individual latitudes and housing values
    are related. To explore this, you might bucketize the latitudes, creating buckets
    like:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个例子：纬度和住房价值之间没有线性关系，但您可能会怀疑单独的纬度和住房价值之间存在关系。为了探索这一点，您可以将纬度进行分桶处理，创建如下的桶：
- en: '[PRE49]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The binning technique can be applied on both categorical and numerical data.
    [Table 12-2](#table_1202) shows a numerical binning example, and [Table 12-3](#table_1203)
    shows a categorical binning example.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 分桶技术可以应用于分类和数值数据。[表12-2](#table_1202)展示了一个数值分桶示例，[表12-3](#table_1203)展示了一个分类分桶示例。
- en: Table 12-2\. Numerical binning example
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-2\. 数值分桶示例
- en: '| Value | Bin |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 分箱 |'
- en: '| --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0-10 | Very low |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 0-10 | 非常低 |'
- en: '| 11-30 | Low |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 11-30 | 低 |'
- en: '| 31-70 | Mid |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 31-70 | 中等 |'
- en: '| 71-90 | High |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 71-90 | 高 |'
- en: '| 91-100 | Very high |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 91-100 | 非常高 |'
- en: Table 12-3\. Categorical binning example
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-3\. 分类分桶示例
- en: '| Value | Bin |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 分箱 |'
- en: '| --- | --- |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| India | Asia |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 亚洲 |'
- en: '| China | Asia |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 中国 | 亚洲 |'
- en: '| Japan | Asia |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 日本 | 亚洲 |'
- en: '| Spain | Europe |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙 | 欧洲 |'
- en: '| Italy | Europe |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 意大利 | 欧洲 |'
- en: '| Chile | South America |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 智利 | 南美洲 |'
- en: '| Brazil | South America |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 巴西 | 南美洲 |'
- en: 'Binning is used with genomics data as well: we bucketize human genome chromosomes
    (1, 2, 3, …, 22, X, Y, MT). For instance, chromosome 1 has 250 million positions,
    which we may bucketize into 101 buckets as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 分桶也用于基因组数据：我们将人类基因组染色体（1、2、3、...、22、X、Y、MT）进行分桶处理。例如，染色体1有2.5亿个位置，我们可以将其分成101个桶，如下所示：
- en: '[PRE50]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Bucketizer
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bucketizer
- en: Bucketing is the most straightforward approach for converting continuous variables
    into categorical variables. To illustrate, let’s look at an example. In PySpark,
    the task of bucketing can be easily accomplished using the `Bucketizer` class.
    The first step is to define the bucket borders; then we create an object of the
    `Bucketizer` class and apply the `transform()` method to our DataFrame.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 桶装是将连续变量转换为分类变量的最直接方法。为了说明，让我们看一个例子。在PySpark中，通过`Bucketizer`类可以轻松实现分桶任务。第一步是定义桶边界；然后，我们创建`Bucketizer`类的对象，并应用`transform()`方法到我们的DataFrame上。
- en: 'First, let’s create a sample DataFrame for demo purposes:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为演示目的创建一个示例DataFrame：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we define our bucket borders and apply the `Bucketizer` to create buckets:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的桶边界，并应用`Bucketizer`来创建桶：
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: QuantileDiscretizer
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分位数分箱器
- en: Spark’s `QuantileDiscretizer` takes a column with continuous features and outputs
    a column with binned categorical features. The number of bins is set by the `numBuckets`
    parameter, and the bucket splits are determined based on the data. It is possible
    that the number of buckets used will be smaller than the specified value, for
    example if there are too few distinct values in the input to create enough distinct
    quantiles (i.e., segments of the dataset).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`QuantileDiscretizer`接受一个带有连续特征的列，并输出一个带有分箱分类特征的列。分箱数由`numBuckets`参数设置，桶分割基于数据确定。如果输入数据中的唯一值太少，无法创建足够的分位数（即数据集的段），则使用的桶数量可能小于指定值。
- en: 'You can use the `Bucketizer` and `QuantileDiscretizer` together, like this:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像这样同时使用`Bucketizer`和`QuantileDiscretizer`：
- en: '[PRE53]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Logarithm Transformation
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数变换
- en: 'In a nutshell, logarithm (commonly denoted by `log`) transformation compresses
    the range of large numbers and expands the range of small numbers. In mathematics,
    the logarithm is the inverse function to exponentiation and is defined as (where
    *b* is called the base number):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，对数（通常表示为`log`）变换压缩了大数的范围并扩展了小数的范围。在数学中，对数是幂运算的反函数，定义为（其中*b*称为基数）：
- en: <math><mrow><mi>l</mi> <mi>o</mi> <msub><mi>g</mi> <mi>b</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>y</mi> <mo>→</mo> <msup><mi>b</mi>
    <mi>y</mi></msup> <mo>=</mo> <mi>x</mi></mrow></math>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>l</mi> <mi>o</mi> <msub><mi>g</mi> <mi>b</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>y</mi> <mo>→</mo> <msup><mi>b</mi>
    <mi>y</mi></msup> <mo>=</mo> <mi>x</mi></mrow></math>
- en: In feature engineering, log transformation is one of the most commonly used
    mathematical transformations. It helps us to handle skewed data by forcing outlier
    values closer to the mean, making the data distribution more approximate to normal
    (for example, the natural/base e logarithm of the number 4,000 is 8.2940496401).
    This normalization reduces the effect of the outliers, helping make machine learning
    models more robust.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程中，对数变换是最常用的数学变换之一。它帮助我们通过将异常值强制拉近到均值附近来处理偏斜数据，使数据分布更接近正态（例如，数值4,000的自然对数或以e为底的对数是8.2940496401）。这种归一化减少了异常值的影响，有助于使机器学习模型更加健壮。
- en: The logarithm is only defined for positive values other than 1 (0, 1, and negative
    values cannot reliably be the base of a power function). A common technique for
    handling negative and zero values is to add a constant to the data before applying
    the log transformation (e.g., log(x+1)).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对数只对除了1之外的正数定义（0、1和负数不能可靠地作为幂函数的底数）。处理负数和零值的常见技术是在应用对数变换之前向数据添加一个常数（例如，log(x+1)）。
- en: 'Spark provides the logarithm function in any base, defined as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了以任意基数定义的对数函数，定义如下：
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Its use is illustrated in the following example. First, we create a DataFrame:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 其用法在以下示例中说明。首先，我们创建一个DataFrame：
- en: '[PRE56]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Then we apply the logarithm transformation on a feature labeled `value`:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在标记为`value`的特征上应用对数变换：
- en: '[PRE57]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: One-Hot Encoding
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码
- en: Machine learning models require that all input features and output predictions
    be numeric. This implies that if your data contains categorical features—such
    as education degree `{BS, MBA, MS, MD, PHD}`—you must encode it numerically before
    you can build and evaluate a model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型要求所有的输入特征和输出预测都是数值型的。这意味着，如果您的数据包含分类特征（例如教育程度`{学士，MBA，硕士，医学博士，博士}`），您必须在构建和评估模型之前对其进行数值编码。
- en: '[Figure 12-3](#one_hot_encoding_example) illustrates the concept of one-hot
    encoding, an encoding scheme in which each categorical value is converted to a
    binary vector.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-3](#one_hot_encoding_example)说明了一种称为独热编码的概念，这是一种编码方案，其中每个分类值都转换为一个二进制向量。'
- en: '![daws 1203](Images/daws_1203.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1203](Images/daws_1203.png)'
- en: Figure 12-3\. One-hot encoding example
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-3\. 独热编码示例
- en: A one-hot encoder maps the label indices to a binary vector representation with
    at most a single 1 value indicating the presence of a specific feature value from
    the set of all possible feature values. This method is useful when you need to
    use categorical features but the algorithm expects continuous features. To understand
    this encoding method, consider a feature called `safety_level` that has five categorical
    values (represented in [Table 12-4](#table_1204)). The first column shows the
    feature values and the rest of the columns show one-hot encoded binary vector
    representations of those values.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一种独热编码器将标签索引映射到二进制向量表示形式，其中最多有一个1值表示来自所有可能特征值集合的特定特征值的存在。当您需要使用分类特征但算法期望连续特征时，此方法非常有用。要理解这种编码方法，考虑一个名为
    `safety_level` 的特征，它具有五个分类值（在 [表 12-4](#table_1204) 中表示）。第一列显示特征值，其余列显示这些值的独热编码二进制向量表示。
- en: Table 12-4\. Representing categorical values as binary vectors
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12-4\. 将分类值表示为二进制向量
- en: '| safety_level (text) | Very-Low | Low | Medium | High | Very-High |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| safety_level (文本) | 非常低 | 低 | 中等 | 高 | 非常高 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| `Very-Low` | 1 | 0 | 0 | 0 | 0 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| `非常低` | 1 | 0 | 0 | 0 | 0 |'
- en: '| `Low` | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| `低` | 0 | 1 | 0 | 0 | 0 |'
- en: '| `Medium` | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| `中等` | 0 | 0 | 1 | 0 | 0 |'
- en: '| `High` | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| `高` | 0 | 0 | 0 | 1 | 0 |'
- en: '| `Very-High` | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| `非常高` | 0 | 0 | 0 | 0 | 1 |'
- en: 'For string type input data, it is common to encode categorical features using
    `StringIndexer` first. Spark’s `OneHotEncoder` then takes the string-indexed label
    and encodes it into a sparse vector. Let’s walk through an example to see how
    this works. First we’ll create a DataFrame with two categorical features:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字符串类型的输入数据，通常首先使用 `StringIndexer` 对分类特征进行编码。然后，Spark的 `OneHotEncoder` 将字符串索引标签编码为稀疏向量。让我们通过一个示例来看看这是如何工作的。首先，我们将创建一个包含两个分类特征的DataFrame：
- en: '[PRE58]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Next, we’ll apply the `OneHotEncoder` transformation to the `safety_level` and
    `engine_type` features. In Spark, we cannot apply `OneHotEncoder` to string columns
    directly; we need to first convert them to numeric values, which we can do with
    Spark’s `StringIndexer`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对 `safety_level` 和 `engine_type` 特征应用 `OneHotEncoder` 转换。在Spark中，我们不能直接对字符串列应用
    `OneHotEncoder`，我们需要先将它们转换为数值，这可以通过Spark的 `StringIndexer` 来完成。
- en: 'First, we apply `StringIndexer` to the `safety_level` feature:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对 `safety_level` 特征应用 `StringIndexer`：
- en: '[PRE59]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, we apply `StringIndexer` to the `engine_type` feature:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对 `engine_type` 特征应用 `StringIndexer`：
- en: '[PRE60]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can now apply `OneHotEncoder` to the `safety_level_index` and `engine_type_index`
    columns:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将 `OneHotEncoder` 应用到 `safety_level_index` 和 `engine_type_index` 列：
- en: '[PRE61]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can also apply this encoding to multiple columns at the same time:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以同时对多列应用此编码：
- en: '[PRE63]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'There is another way to do all of the data transformations: we can use a pipeline
    to simplify the process. First, we create the required stages:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种数据转换方式：我们可以使用管道来简化这个过程。首先，我们创建所需的阶段：
- en: '[PRE65]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Then we create a pipeline and pass all the defined stages to it:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个管道，并将所有定义的阶段传递给它：
- en: '[PRE66]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: TF-IDF
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Term frequency–inverse document frequency (TF-IDF) is a measure of the originality
    of a word (a.k.a. term) based on the number of times it appears in a document
    and the number of documents in a collection that it appears in. In other words,
    it’s a feature vectorization method used in text mining to reflect the importance
    of a term to a document in a corpus (set of documents). The TF-IDF technique is
    commonly used in document analysis, search engines, recommender systems, and other
    natural language processing (NLP) applications.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 词频-逆文档频率（TF-IDF）是一种基于词在文档中出现的次数及其出现在整个语料库中文档数量的原创性度量。换句话说，它是文本挖掘中用于反映术语对文档在语料库中的重要性的特征向量化方法。TF-IDF
    技术通常用于文档分析、搜索引擎、推荐系统和其他自然语言处理（NLP）应用中。
- en: Term frequency `TF(t,d)` is the number of times that term `t` appears in document
    `d`, while document frequency `DF(t, D)` is the number of documents that contain
    term `t`. If a term appears very often across the corpus, it means it does not
    carry special information about a particular document—usually these kinds of words
    (such as “of,” “the,” and “as”) may be dropped from the text analysis. Before
    we go deeper into the TF-IDF transformation, let’s define the terms used in the
    following equations ([Table 12-5](#tf_idf_notations)).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 词项频率 `TF(t,d)` 是术语 `t` 在文档 `d` 中出现的次数，而文档频率 `DF(t, D)` 是包含术语 `t` 的文档数量。如果一个术语在整个语料库中经常出现，意味着它并不提供有关特定文档的特殊信息，通常这类词汇（如“of”、“the”和“as”）可能会在文本分析中被排除。在深入探讨
    TF-IDF 转换之前，让我们先定义以下方程中使用的术语（[Table 12-5](#tf_idf_notations)）。
- en: Table 12-5\. TF-IDF notation
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Table 12-5\. TF-IDF 符号
- en: '| Notation | Description |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 描述 |'
- en: '| --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `t` | Term |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| `t` | 术语 |'
- en: '| `d` | Document |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| `d` | 文档 |'
- en: '| `D` | Corpus (set of finite documents) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| `D` | 语料库（有限文档集合） |'
- en: '| `&#124;D&#124;` | The number of documents in the corpus |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| `&#124;D&#124;` | 语料库中的文档数量 |'
- en: '| `TF(t, d)` | Term Frequency: the number of times that term `t` appears in
    document `d` |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| `TF(t, d)` | 术语频率：术语 `t` 在文档 `d` 中出现的次数 |'
- en: '| `DF(t, D)` | Document Frequency: the number of documents that contain term
    `t` |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| `DF(t, D)` | 文档频率：包含术语 `t` 的文档数量 |'
- en: '| `IDF(t, D)` | Inverse Document Frequency: a numerical measure of how much
    information a term provides |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| `IDF(t, D)` | 逆文档频率：一个术语提供信息量的数值度量 |'
- en: 'Inverse document frequency (IDF) is defined as:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 逆文档频率（IDF）定义如下：
- en: <math alttext="upper I upper D upper F left-parenthesis t comma upper D right-parenthesis
    equals l o g left-parenthesis StartFraction StartAbsoluteValue upper D EndAbsoluteValue
    plus 1 Over upper D upper F left-parenthesis t comma upper D right-parenthesis
    plus 1 EndFraction right-parenthesis" display="block"><mrow><mi>I</mi> <mi>D</mi>
    <mi>F</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><mrow><mo>|</mo><mi>D</mi><mo>|</mo><mo>+</mo><mn>1</mn></mrow>
    <mrow><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo>)</mo><mo>+</mo><mn>1</mn></mrow></mfrac>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper I upper D upper F left-parenthesis t comma upper D right-parenthesis
    equals l o g left-parenthesis StartFraction StartAbsoluteValue upper D EndAbsoluteValue
    plus 1 Over upper D upper F left-parenthesis t comma upper D right-parenthesis
    plus 1 EndFraction right-parenthesis" display="block"><mrow><mi>I</mi> <mi>D</mi>
    <mi>F</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><mrow><mo>|</mo><mi>D</mi><mo>|</mo><mo>+</mo><mn>1</mn></mrow>
    <mrow><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo>)</mo><mo>+</mo><mn>1</mn></mrow></mfrac>
    <mo>)</mo></mrow></mrow></math>
- en: 'Let’s say `N` is the number of documents in a corpus. Since the logarithm is
    used, if a term appears in all documents, its `IDF` value becomes `0`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 `N` 是语料库中的文档数量。由于使用了对数，如果一个词在所有文档中出现，其 `IDF` 值就会变为 `0`：
- en: <math alttext="upper I upper D upper F left-parenthesis t comma upper D right-parenthesis
    equals l o g left-parenthesis StartFraction upper N plus 1 Over upper N plus 1
    EndFraction right-parenthesis equals l o g left-parenthesis 1 right-parenthesis
    equals 0" display="block"><mrow><mi>I</mi> <mi>D</mi> <mi>F</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>l</mi> <mi>o</mi>
    <mi>g</mi> <mrow><mo>(</mo> <mfrac><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow>
    <mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></mfrac> <mo>)</mo></mrow> <mo>=</mo>
    <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper I upper D upper F left-parenthesis t comma upper D right-parenthesis
    equals l o g left-parenthesis StartFraction upper N plus 1 Over upper N plus 1
    EndFraction right-parenthesis equals l o g left-parenthesis 1 right-parenthesis
    equals 0" display="block"><mrow><mi>I</mi> <mi>D</mi> <mi>F</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>l</mi> <mi>o</mi>
    <mi>g</mi> <mrow><mo>(</mo> <mfrac><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow>
    <mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></mfrac> <mo>)</mo></mrow> <mo>=</mo>
    <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>=</mo> <mn>0</mn></mrow></math>
- en: 'Note that a smoothing term (`+1`) is applied to avoid dividing by zero for
    terms that do not appear in the corpus. The `TF-IDF` measure is simply the product
    of `TF` and `IDF`:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了避免在语料库中未出现的术语分母为零，我们应用了平滑项 (`+1`)。`TF-IDF` 测量值简单地是 `TF` 和 `IDF` 的乘积：
- en: <math alttext="upper T upper F minus upper I upper D upper F left-parenthesis
    t comma d comma upper D right-parenthesis equals upper T upper F left-parenthesis
    t comma d right-parenthesis times upper I upper D upper F left-parenthesis t comma
    upper D right-parenthesis" display="block"><mrow><mi>T</mi> <mi>F</mi> <mo>-</mo>
    <mi>I</mi> <mi>D</mi> <mi>F</mi> <mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>,</mo>
    <mi>D</mi> <mo>)</mo> <mo>=</mo> <mi>T</mi> <mi>F</mi> <mo>(</mo> <mi>t</mi> <mo>,</mo>
    <mi>d</mi> <mo>)</mo> <mo>×</mo> <mi>I</mi> <mi>D</mi> <mi>F</mi> <mo>(</mo> <mi>t</mi>
    <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></math>
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper T upper F minus upper I upper D upper F left-parenthesis
    t comma d comma upper D right-parenthesis equals upper T upper F left-parenthesis
    t comma d right-parenthesis times upper I upper D upper F left-parenthesis t comma
    upper D right-parenthesis" display="block"><mrow><mi>T</mi> <mi>F</mi> <mo>-</mo>
    <mi>I</mi> <mi>D</mi> <mi>F</mi> <mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>,</mo>
    <mi>D</mi> <mo>)</mo> <mo>=</mo> <mi>T</mi> <mi>F</mi> <mo>(</mo> <mi>t</mi> <mo>,</mo>
    <mi>d</mi> <mo>)</mo> <mo>×</mo> <mi>I</mi> <mi>D</mi> <mi>F</mi> <mo>(</mo> <mi>t</mi>
    <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></math>
- en: 'where:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '`t` denotes the term(s)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t` 表示术语（术语）'
- en: '`d` denotes a document'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d` 表示一个文档'
- en: '`D` denotes the corpus'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`D` 表示语料库'
- en: '`TF(t,d)` denotes the number of times that term `t` appears in document `d`'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TF(t,d)` 表示术语 `t` 在文档 `d` 中出现的次数'
- en: 'We can express TF as:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以表达 TF 如下：
- en: <math><mrow><mi>T</mi> <msub><mi>F</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <mfrac><msub><mi>n</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mrow><msub><mo>∑</mo> <mi>k</mi></msub> <msub><mi>n</mi> <mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mfrac>
    <mi>I</mi> <mi>D</mi> <msub><mi>F</mi> <mi>i</mi></msub> <mo>=</mo> <mtext>log</mtext>
    <mfrac><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow> <mrow><mrow><mo>|</mo></mrow><mrow><mi>d</mi><mo>:</mo><msub><mi>t</mi>
    <mi>i</mi></msub> <mo>∈</mo><mi>d</mi></mrow><mrow><mo>|</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>T</mi> <msub><mi>F</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <mfrac><msub><mi>n</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mrow><msub><mo>∑</mo> <mi>k</mi></msub> <msub><mi>n</mi> <mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mfrac>
    <mi>I</mi> <mi>D</mi> <msub><mi>F</mi> <mi>i</mi></msub> <mo>=</mo> <mtext>log</mtext>
    <mfrac><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow> <mrow><mrow><mo>|</mo></mrow><mrow><mi>d</mi><mo>:</mo><msub><mi>t</mi>
    <mi>i</mi></msub> <mo>∈</mo><mi>d</mi></mrow><mrow><mo>|</mo></mrow></mrow></mfrac></mrow></math>
- en: 'Before, I show you how Spark implements TF-IDF, let’s walk through a simple
    example with two documents (corpus size is 2 and `D = {doc1, doc2}`). We start
    by calculating the term frequency and document frequency:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在我展示 Spark 如何实现 TF-IDF 之前，让我们通过一个简单的例子来了解，其中包含两个文档（语料库大小为 2，`D = {doc1, doc2}`）。我们首先计算术语频率和文档频率：
- en: '[PRE67]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Then we calculate the IDF and TF-IDF (note that the logarithm base is *e* for
    all calculations):'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算 IDF 和 TF-IDF（请注意，所有计算的对数基数为 *e*）：
- en: '[PRE68]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'In Spark, `HashingTF` and `CountVectorizer` are the two algorithms used to
    generate term frequency vectors. The following example shows how to perform the
    required transformations. First, we create our sample DataFrame:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，`HashingTF` 和 `CountVectorizer` 是用于生成词项频率向量的两种算法。下面的示例展示了如何执行所需的转换。首先，我们创建我们的样本
    DataFrame：
- en: '[PRE69]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next we, create raw features:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建原始特征：
- en: '[PRE70]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then we apply the `IDF()` transformation:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应用 `IDF()` 转换：
- en: '[PRE71]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The next example shows how to do TF-IDF using `CountVectorizer`, which extracts
    a vocabulary from a document collection and generates a `CountVectorizerModel`.
    In this example, each row of the DataFrame represents a document:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`CountVectorizer`展示了如何进行TF-IDF计算，它从文档集合中提取词汇，并生成`CountVectorizerModel`。在本例中，DataFrame的每一行代表一个文档：
- en: '[PRE72]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'In the `features` column, taking the example of the second row:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在`features`列中，以第二行为例：
- en: '`3` is the vector length.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3`是向量的长度。'
- en: '`[0, 1, 2]` are the vector indices (`index(a)=0, index(b)=1, index(c)=2`).'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[0, 1, 2]`是向量的索引（`index(a)=0, index(b)=1, index(c)=2`）。'
- en: '`[2.0,2.0,1.0]` are the vector values.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[2.0, 2.0, 1.0]`是向量的值。'
- en: '`HashingTF()` converts documents to vectors of fixed size:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashingTF()`将文档转换为固定大小的向量：'
- en: '[PRE74]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Note that the size of the vector generated through `CountVectorizer` depends
    on the training corpus and the document, whereas the one generated through `HashingTF`
    has a fixed size (we set it to 128). This means that when using `CountVectorizer`,
    each raw feature is mapped to an index, but `HashingTF` might suffer from hash
    collisions, where two or more terms are mapped to the same index. To avoid this,
    we can increase the target feature dimension.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通过`CountVectorizer`生成的向量大小取决于训练语料库和文档，而通过`HashingTF`生成的向量具有固定大小（我们设置为128）。这意味着当使用`CountVectorizer`时，每个原始特征映射到一个索引，但`HashingTF`可能会遇到哈希冲突，其中两个或更多术语映射到同一个索引。为了避免这种情况，我们可以增加目标特征维度。
- en: FeatureHasher
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FeatureHasher
- en: Feature hashing projects a set of categorical or numerical features into a feature
    vector of specified dimension (typically substantially smaller than that of the
    original feature space). A [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing)
    is used to map features to indices in the feature vector.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希将一组分类或数值特征投影到指定维度的特征向量中（通常远小于原始特征空间的维度）。使用[哈希技巧](https://en.wikipedia.org/wiki/Feature_hashing)将特征映射到特征向量中的索引。
- en: 'Spark’s `FeatureHasher` operates on multiple columns, which may contain either
    numeric or categorical features. For numeric features, the hash of the column
    name is used to map the feature value to its index in the feature vector. For
    categorical and Boolean features, the hash of the string `"column_name=value"`
    is used, with an indicator value of `1.0`. Here’s an example:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`FeatureHasher`可以处理多列数据，这些数据可以是数值型或分类型特征。对于数值型特征，使用列名的哈希来映射特征值到特征向量中的索引。对于分类和布尔型特征，使用字符串`"column_name=value"`的哈希，其指示值为`1.0`。以下是一个例子：
- en: '[PRE75]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: SQLTransformer
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQLTransformer
- en: Spark’s `SQLTransformer` implements the transformations that are defined by
    a SQL statement. Rather than registering your DataFrame as a table and then querying
    the table, you can directly apply the SQL transformations to your data represented
    as a DataFrame. Currently, `SQLTransformer` has limited functionality and can
    be applied to a single DataFrame as `__THIS__`, which represents the underlying
    table of the input dataset.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`SQLTransformer`实现了由SQL语句定义的转换操作。与将DataFrame注册为表然后查询表不同，你可以直接将SQL转换应用于作为DataFrame表示的数据。目前，`SQLTransformer`的功能有限，可以应用于单个DataFrame作为`__THIS__`，它表示输入数据集的基础表。
- en: '`SQLTransformer` supports statements like:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`SQLTransformer`支持类似以下的语句：'
- en: '[PRE76]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The following example shows how to use `SQLTransformer`:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用`SQLTransformer`：
- en: '[PRE77]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Summary
- en: The goal of machine learning algorithms is to use input data to create usable
    models that can help us to answer questions. The input data comprises features
    (such as education level, car price, glucose level, etc.) which are in the form
    of structured columns. In most cases the algorithms require features with some
    specific characteristics to work properly, which raises the need for feature engineering.
    Spark’s machine learning library, MLlib (included in PySpark), has a set of high-level
    APIs that make feature engineering possible. Proper feature engineering helps
    to build semantically proper and correct machine learning models.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的目标是利用输入数据创建可用模型，帮助我们回答问题。输入数据包括结构化列形式的特征（如教育水平、汽车价格、血糖水平等）。在大多数情况下，算法要求具有特定特性的特征才能正常工作，这就需要特征工程。Spark的机器学习库MLlib（包含在PySpark中）提供了一组高级API，使特征工程成为可能。正确的特征工程有助于构建语义正确的机器学习模型。
- en: 'The following is a list of accessible resources that provide additional information
    on feature engineering and other topics covered in this book:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是提供有关特征工程和本书其他涵盖主题的可访问资源列表：
- en: '[“Getting Started with Feature Engineering”](https://oreil.ly/GAlRk), a blog
    post by Pravar Jain'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“开始特征工程”](https://oreil.ly/GAlRk)，一篇由Pravar Jain撰写的博客文章。'
- en: '[“Data Manipulation: Features”](https://oreil.ly/Qc3sP), by Wenqiang Feng'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“数据操作：特征”](https://oreil.ly/Qc3sP)，作者是冯文强。'
- en: '[“Representation: Feature Engineering”](https://oreil.ly/eDeDr) from Google’s
    *Machine Learning Crash Course with TensorFlow APIs*'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“表征：特征工程”](https://oreil.ly/eDeDr)，来自Google的*TensorFlow API机器学习速成课程*。'
- en: '[“Want to Build Machine Learning Pipelines? A Quick Introduction Using PySpark”](https://oreil.ly/kHLFL),
    a blog post by Lakshay Arora'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“想要构建机器学习流水线？使用 PySpark 的快速介绍”](https://oreil.ly/kHLFL)，一篇由Lakshay Arora撰写的博客文章。'
- en: '[TF-IDF, Term Frequency-Inverse Document Frequency](https://oreil.ly/2WlgI)—documentation
    by Ethen Liu'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TF-IDF，词频-逆文档频率](https://oreil.ly/2WlgI)，Ethen Liu的文档。'
- en: 'This concludes our journey through data algorithms with Spark! I hope you feel
    prepared to tackle any data problem, big or small. Remember my motto: keep it
    simple and use parameters so that your solution can be reused by other developers.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Spark数据算法之旅到此结束！希望你感觉准备好解决任何大小的数据问题了。记住我的座右铭：保持简单，并使用参数，以便其他开发者可以重复使用你的解决方案。
