- en: 12 Securing Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 保护Kubernetes
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Keeping your cluster up to date and patched
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持集群更新和打补丁
- en: Managing disruptions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理中断
- en: Using DaemonSets to deploy node agents to every node
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DaemonSets将节点代理部署到每个节点
- en: Running containers as the non-root user
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以非root用户运行容器
- en: Using admission controllers to validate and modify Kubernetes objects
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用准入控制器验证和修改Kubernetes对象
- en: Enforcing Pod Security Standards
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行Pod安全标准
- en: Controlling namespace access with RBAC
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RBAC控制命名空间访问
- en: So far, this book has focused on deploying different types of software into
    Kubernetes clusters. In this last chapter, I’ll cover some key topics when it
    comes to keeping everything secure. Security is a huge area in general, and Kubernetes
    is no exception. If you deploy code to a Kubernetes cluster managed by another
    team, then lucky you—you may not need to worry about some of these topics. For
    developers who are also responsible for operations or are cluster operators themselves,
    securing and updating the cluster is a key responsibility.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这本书主要关注将不同类型的软件部署到Kubernetes集群中。在本章的最后，我将介绍一些保持一切安全的关键主题。安全是一个巨大的领域，Kubernetes也不例外。如果你将代码部署到由其他团队管理的Kubernetes集群，那么你很幸运——你可能不需要担心这些主题中的某些。对于既负责运维又是集群操作员的开发者来说，保护集群和更新集群是一个关键责任。
- en: In addition to keeping your cluster up to date, handling disruption, deploying
    node agents, and building non-root containers, this chapter takes you through
    the process of creating a dedicated namespace for a team of developers and how
    access can be granted specifically to that namespace. This is a pretty common
    pattern I’ve observed in companies where several teams share clusters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了保持集群更新、处理中断、部署节点代理和构建非root容器之外，本章还介绍了为开发团队创建专用命名空间的过程以及如何具体授权访问该命名空间。这是我在公司观察到的相当常见的模式，其中几个团队共享集群。
- en: 12.1 Staying up to date
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 保持更新
- en: Kubernetes has a large surface area. There’s the Linux kernel and the Kubernetes
    software running on the control plane and user nodes. Then, there are your own
    containers and all their dependencies, including the base image. All this means
    there’s a lot to keep up to date and protected against vulnerabilities.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的表面面积很大。这里有运行在控制平面和用户节点上的Linux内核和Kubernetes软件。然后，还有你自己的容器以及所有依赖项，包括基础镜像。这意味着有很多东西需要保持更新并保护免受漏洞的侵害。
- en: 12.1.1 Cluster and node updates
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 集群和节点更新
- en: One critical task for a Kubernetes operator is to ensure that your cluster and
    nodes are up to date. This helps mitigate known vulnerabilities in Kubernetes,
    and the operating system that runs on your nodes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kubernetes操作员来说，一个关键任务是确保你的集群和节点保持更新。这有助于缓解Kubernetes以及运行在节点上的操作系统的已知漏洞。
- en: Unlike most of the topics discussed in this book so far, the updating of clusters
    and nodes is actually not part of the Kubernetes API. It sits at the platform
    level, so you’ll need to consult the docs for your Kubernetes platform. Fortunately,
    if you’re using a managed platform, this should be straightforward. If you’re
    running Kubernetes the hard way via a manual installation on VMs (which I don’t
    recommend), these updates will be a significant burden, as you are now the one
    offering the Kubernetes platform.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书迄今为止讨论的大多数主题不同，集群和节点的更新实际上不是Kubernetes API的一部分。它位于平台级别，因此你需要查阅你的Kubernetes平台文档。幸运的是，如果你使用的是托管平台，这应该很简单。如果你是通过在VM上手动安装（我不推荐这样做）来艰难地运行Kubernetes，那么这些更新将是一个重大的负担，因为你现在是你自己的Kubernetes平台提供者。
- en: Updating Google Kubernetes Engine
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 更新Google Kubernetes Engine
- en: 'In the case of GKE, staying up to date is easy. Simply enroll in one of the
    three release channels: Stable, Regular, or Rapid. Security patches are rolled
    out to all channels quickly. What differs is how soon you get other new features
    of both Kubernetes and the GKE platform.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在GKE的情况下，保持更新很容易。只需注册三个发布渠道之一：稳定版、常规版或快速版。安全补丁会迅速推广到所有渠道。不同的是，你将何时获得Kubernetes和GKE平台的其他新功能。
- en: When enrolled in a release channel, both the cluster version and nodes are automatically
    kept up to date. The older static version option is not recommended, as you need
    to keep on top of the updates manually.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当注册到发布渠道时，集群版本和节点都会自动保持更新。不推荐使用较老的静态版本选项，因为你需要手动跟踪更新。
- en: 12.1.2 Updating containers
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 更新容器
- en: Keeping the Kubernetes cluster up to date isn’t the only updating you’ll need
    to do. Security vulnerabilities are often found in the components of base images
    like Ubuntu. As your containerized application is built on these base images,
    it can inherit vulnerabilities that exist in them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 保持 Kubernetes 集群更新并不是你需要做的唯一更新。安全漏洞通常存在于基础镜像（如 Ubuntu）的组件中。由于你的容器化应用程序是基于这些基础镜像构建的，它可能会继承其中存在的漏洞。
- en: The solution is to rebuild and update your containers regularly, especially
    if any vulnerabilities are found in the base images you use. Many developers and
    enterprises employ vulnerability scanners (often known as *CVE scanners* after
    the Common Vulnerabilities and Exposures system where known vulnerabilities are
    documented) to look through built containers to see whether any reported vulnerabilities
    exist in them to prioritize rebuilds and rollouts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是定期重建和更新你的容器，特别是如果你发现使用的基镜像中存在任何漏洞。许多开发者和企业使用漏洞扫描器（通常在已知漏洞在公共漏洞和暴露系统（CVE）中记录后被称为
    *CVE 扫描器*）来检查构建的容器，以确定是否存在任何报告的漏洞，从而优先重建和部署。
- en: When updating your containers, be sure to specify the base image that contains
    the latest fixes. Typically, this can be achieved by only specifying the minor
    version of the base image you’re using rather than the specific patch version.
    You can use the `latest` tag to achieve this, but then you might get some unwanted
    feature changes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新你的容器时，确保指定包含最新修复的基镜像。通常，这可以通过仅指定你使用的基镜像的次要版本而不是特定补丁版本来实现。你可以使用 `latest` 标签来达到这个目的，但这样可能会引入一些不希望的功能变更。
- en: 'For example, take the Python base image.[¹](#pgfId-1113826) For any given version
    of Python (say, v3.10.2), you have a bunch of different options: `3.10.2-bullseye`,
    `3.10-bullseye`, `3-bullseye`, and `bullseye` (`bullseye` refers to the version
    of Debian it uses). You can also use `latest`. For images that follow semantic
    versioning (semver) principles, I would typically recommend going with the `major.minor`
    version—in this example, `3.10-bullseye`. This allows you to get patches to the
    v3.10 automatically while avoiding breaking changes. The downside is that you
    need to pay attention to when the support drops for 3.10 and migrate. Going with
    the major version instead (i.e., `3-bullseye` in this example) would give you
    longer support but with slightly more risk of breakages. In theory, with semver,
    you should be safe to use the major version as changes should be backward-compatible,
    but in practice, I find it safer to go with the minor version. Using `latest`,
    while great from a security perspective, is typically not recommended due to the
    extremely high risk of breakage from backward-incompatible changes.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以 Python 基础镜像为例。[¹](#pgfId-1113826) 对于任何给定的 Python 版本（比如，v3.10.2），你有一系列不同的选择：`3.10.2-bullseye`、`3.10-bullseye`、`3-bullseye`
    和 `bullseye`（`bullseye` 指的是它使用的 Debian 版本）。你也可以使用 `latest`。对于遵循语义版本控制（semver）原则的镜像，我通常会推荐使用
    `major.minor` 版本——在这个例子中，`3.10-bullseye`。这允许你自动获取 v3.10 的补丁，同时避免破坏性变更。缺点是，你需要注意
    3.10 的支持何时终止并进行迁移。选择主要版本（即，在这个例子中的 `3-bullseye`）将提供更长的支持，但会有稍微更高的破坏风险。从理论上看，使用
    semver，你应该安全地使用主要版本，因为变更应该是向后兼容的，但在实践中，我发现使用次要版本更安全。使用 `latest`，虽然从安全角度来看很好，但由于向后不兼容的变更风险极高，通常不推荐这样做。
- en: Whichever way you configure your Docker file, the key principles are to rebuild
    often, to reference base images that are up to date, roll out updates to your
    workloads frequently, and employ CVE scanning to look for containers that are
    out of date. A further mitigation to reduce potential vulnerabilities in application
    containers is to build extremely lightweight containers that include only the
    absolute minimum needed to run your application and its dependencies. Using a
    typical base image like Ubuntu includes a package manager and various software
    packages, which make life easy but also increase the vulnerability surface area.
    The less code there is in your container from other sources, the less you’ll need
    to update it due to vulnerabilities found in that code and the fewer bugs you
    can potentially be exposed to.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你如何配置 Dockerfile，关键原则是经常重建，引用最新的基础镜像，频繁推出工作负载的更新，并使用 CVE 扫描来查找过时的容器。为了减少应用程序容器中的潜在漏洞，进一步缓解措施是构建极其轻量级的容器，只包含运行你的应用程序及其依赖项所需的绝对最小内容。使用典型的基镜像，如
    Ubuntu，包括包管理器和各种软件包，这使得生活变得容易，但也增加了漏洞表面积。你的容器中来自其他来源的代码越少，由于在该代码中发现漏洞而需要更新的次数就越少，你可能会暴露的缺陷也就越少。
- en: The Dockerfile in section 2.1.8 on multistage builds employed this principle
    by using one container to build your code and another to run the code. To reduce
    the potential attack surface, the key is to pick the slimmest possible runtime
    base image for the second stage of the container build. Google has an open source
    project distroless[²](#pgfId-1113834) to assist with providing super-lightweight
    runtime containers. The following listing provides the distroless project’s example
    of a building Java container, referencing the Google-provided distroless image
    in the second step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2.1.8 节的多阶段构建中使用的 Dockerfile 通过使用一个容器来构建你的代码，另一个容器来运行代码，应用了这一原则。为了减少潜在的攻击面，关键是选择尽可能瘦的运行时基础镜像作为容器构建的第二阶段。Google
    有一个开源项目 distroless[²](#pgfId-1113834)，用于帮助提供超轻量级的运行时容器。以下列表提供了 distroless 项目的构建
    Java 容器示例，在第二步引用了 Google 提供的无分发镜像。
- en: Listing 12.1 https://github.com/GoogleContainerTools/distroless/tree/main/examples/java/Dockerfile
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.1 https://github.com/GoogleContainerTools/distroless/tree/main/examples/java/Dockerfile
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The regular OpenJDK image is used to build the code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用常规的 OpenJDK 镜像来构建代码。
- en: ❷ The distroless java image is used to run the code.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用无分发的 Java 镜像来运行代码。
- en: 12.1.3 Handling disruptions
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.3 处理中断
- en: With all this updating, you might be wondering what happens to your running
    workloads. It’s inevitable that as you update, Pods will be deleted and re-created.
    This can obviously be very disruptive to the workloads running in those Pods,
    but fortunately, Kubernetes has a number of ways to reduce this disruption and
    potentially eliminate any ill effects.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些更新之后，你可能想知道你的运行工作负载会发生什么。当你更新时，Pods 被删除和重新创建是不可避免的。这显然会非常干扰那些 Pod 中运行的工作负载，但幸运的是，Kubernetes
    有多种方法来减少这种干扰，并可能消除任何不良影响。
- en: Readiness checks
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 准备性检查
- en: First, if you’ve not set up readiness checks (as we did in chapter 4), now is
    the time to go back and do that, as it’s absolutely critical. Kubernetes relies
    on your container reporting when it’s ready, and if you don’t do that, it will
    assume it’s ready the moment the process starts running, which is likely *before*
    your application has finished initializing and is actually ready to serve production
    traffic. The more your Pods are moved around, such as during updates, the more
    requests will error out by hitting Pods that are not ready unless you implement
    proper readiness checks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你还没有设置准备性检查（就像我们在第 4 章中做的那样），现在是时候回去做了，因为这绝对是至关重要的。Kubernetes 依赖于你的容器报告其何时准备好，如果你不这样做，它将假设在进程开始运行的那一刻它就准备好了，这很可能是
    *在* 你的应用程序完成初始化并实际上准备好服务生产流量之前。你的 Pods 被移动得越多，比如在更新期间，如果没有实施适当的准备性检查，错误请求就会更多，因为它们会击中尚未准备好的
    Pods。
- en: Signal handling and graceful termination
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 信号处理和优雅终止
- en: Just as readiness checks are used to determine when your application is ready
    to start, graceful termination is used by Kubernetes to know when your application
    is ready to stop. In the case of a Job, which may have a process that takes a
    while to complete, you may not want to simply terminate that process if it can
    be avoided. Even web applications with short-lived requests can suffer from abrupt
    termination that causes requests to fail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如就绪性检查用于确定应用程序何时准备好启动一样，优雅终止被Kubernetes用来知道应用程序何时准备好停止。在作业的情况下，它可能有一个需要一段时间才能完成的过程，你可能不希望简单地终止该过程，如果可以避免的话。即使是具有短暂请求的生命周期Web应用程序也可能遭受突然终止，导致请求失败。
- en: To prevent these problems, it’s important to handle SIGTERM events in your application
    code to start the shutdown process, and set a graceful termination window (configured
    with `terminationGracePeriodSeconds`) long enough to complete the termination.
    Web applications should handle SIGTERM to shut down the server once all current
    requests are completed, and batch jobs would ideally wrap up any work they are
    doing and not start any new tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这些问题，处理应用程序代码中的SIGTERM事件以启动关闭过程，并设置一个足够长的优雅终止窗口（通过`terminationGracePeriodSeconds`配置）来完成终止是很重要的。Web应用程序应该处理SIGTERM，在所有当前请求完成后关闭服务器，而批处理作业理想情况下应该完成他们正在执行的工作，并且不启动任何新任务。
- en: In some cases, you may have a Job performing a long-running task that, if interrupted,
    would lose its progress. In these cases, you might set a very long graceful termination
    window whereby the application accepts the SIGTERM but simply continues on as
    before to attempt to finish the current task. Managed platforms may have a limit
    on how long the graceful termination window can be for system-originated disruption.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你可能有一个正在执行长时间运行的任务的作业，如果被中断，就会丢失其进度。在这些情况下，你可能会设置一个非常长的优雅终止窗口，使得应用程序接受SIGTERM信号，但仍然像以前一样继续尝试完成当前任务。托管平台可能对系统起源的干扰的优雅终止窗口的长度有限制。
- en: Section 10.1.2 has examples of SIGTERM handling and `terminationGracePeriodSeconds`
    configuration in the context of Jobs. The same principles apply to other workload
    types.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第10.1.2节提供了在作业上下文中处理SIGTERM和`terminationGracePeriodSeconds`配置的示例。相同的原理适用于其他工作负载类型。
- en: Rolling updates
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新
- en: When you update the containers in a Deployment or a StatefulSet (e.g., to update
    the base image), the rollout is governed by your rollout strategy. Rolling update,
    covered in chapter 4, is the recommended strategy to minimize disruption when
    updating workloads by updating Pods in batches while keeping the application available.
    For Deployments, be sure to configure the `maxSurge` parameters of the Deployment,
    which will do a rollout by temporarily increasing the Pod replica count, which
    is safer for availability than reducing it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当你更新部署或有状态集（例如，更新基础镜像）中的容器时，滚动更新由你的滚动更新策略控制。在第4章中介绍的滚动更新是推荐的策略，通过批量更新Pod以最小化更新工作负载时的中断。对于部署，请确保配置部署的`maxSurge`参数，这将通过临时增加Pod副本数来进行滚动更新，这比减少副本数对可用性更安全。
- en: Pod Disruption Budgets
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Pod中断预算
- en: When nodes are updated, this process does *not* go through the same rollout
    process as updates to Deployments. Here’s how it works. First, the node is cordoned
    to prevent new Pods from being deployed on it. Then the node is drained, whereby
    Pods are deleted from this node and re-created on another node. By default, Kubernetes
    will delete all Pods at once from the node and (in the case of Pods managed by
    a workload resource such as Deployment) schedule them to be created elsewhere.
    Note that it does *not* first schedule them to be created elsewhere and then delete
    them. If multiple replicas of a single Deployment are running on the same node,
    this can cause unavailability when they are evicted at the same time, as shown
    in figure 12.1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点更新时，这个过程不会像部署的更新那样经过相同的滚动更新过程。以下是它是如何工作的。首先，节点被隔离以防止新的Pod部署在其上。然后节点被排空，Pod从这个节点删除并在另一个节点上重新创建。默认情况下，Kubernetes将一次性从节点删除所有Pod，并且（在Pod由部署等工作负载资源管理的情况下）将它们调度到其他地方。请注意，它并不是首先将它们调度到其他地方然后再删除。如果单个部署的多个副本在同一节点上运行，当它们同时被驱逐时，这可能导致不可用，如图12.1所示。
- en: '![12-01](../../OEBPS/Images/12-01.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![12-01](../../OEBPS/Images/12-01.png)'
- en: Figure 12.1 Node deletion without Pod disruption budgets. All the Pods on the
    node will become unavailable at once.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 无Pod中断预算的节点删除。节点上的所有Pod将同时变得不可用。
- en: To solve the problem where draining a node that contains multiple Pods from
    the same Deployment may reduce the availability of your Deployments (meaning too
    few running replicas), Kubernetes has a feature called Pod Disruption Budgets
    (PDBs). PDBs allow you to inform Kubernetes how many or what percentage of your
    Pods you are willing to have unavailable for your workload to still function as
    you designed it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决从包含多个Pods的同一Deployment中排空节点可能会降低你的Deployment可用性的问题（意味着运行副本太少），Kubernetes有一个名为Pod
    Disruption Budgets（PDBs）的功能。PDBs允许你通知Kubernetes你愿意让你的Pods中有多少或多少百分比不可用，以便你的工作负载仍然按你设计的方式运行。
- en: Listing 12.2 Chapter12/12.1_PDB/pdb.yaml
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.2 第12章/12.1_PDB/pdb.yaml
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Declares the maximum number of Pods that can be unavailable during disruptions
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 声明在故障期间可以不可用的Pods的最大数量
- en: ❷ Selects the Pods by their labels
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过标签选择Pods
- en: Deploying this PDB into your cluster will ensure that at no time during disruptions
    will more than one of your Pods be unavailable, as illustrated in figure 12.2\.
    An alternative configuration uses `minAvailable` to set how many replicas you
    need. I prefer `maxUnavailable`, as it works better with scaling. If you use `minAvailable`,
    you may need to scale that value along with your replica count to retain the desired
    minimum availability, which is just extra work.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将此PDB部署到你的集群将确保在故障期间不会有多于一个的你的Pod不可用，如图12.2所示。另一种配置使用`minAvailable`来设置你需要多少个副本。我更喜欢`maxUnavailable`，因为它与扩展配合得更好。如果你使用`minAvailable`，你可能需要随着副本数量的增加来调整该值，以保持所需的最低可用性，这只会增加额外的工作。
- en: '![12-02](../../OEBPS/Images/12-02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![12-02](../../OEBPS/Images/12-02.png)'
- en: Figure 12.2 With a PDB, Kubernetes will wait for the required number of Pods
    in a Deployment to be available before deleting others, reducing the disruption.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 使用PDB，Kubernetes将在删除其他Pod之前等待部署中所需数量的Pod可用，从而减少中断。
- en: Note The PDB protects against voluntary evictions such as during node upgrades,
    but not every possible case of disruption, such as if a node were to fail abruptly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：PDB可以防止自愿驱逐，例如在节点升级期间，但不能防止所有可能的故障情况，例如如果节点突然失败。
- en: The process of handling disruptions with a PDB is somewhat similar to how a
    rolling update avoids taking out too many Pods at the same time. To ensure your
    application stays available during updates that you initiate and disruptions are
    initiated by cluster updates, you’ll need to have both the rolling update and
    the PDB configured.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PDB处理中断的过程与滚动更新避免同时删除太多Pods的方式有些相似。为了确保你的应用程序在由你发起的更新期间保持可用，并且由集群更新引发的中断，你需要同时配置滚动更新和PDB。
- en: 12.2 Deploying node agents with DaemonSet
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 使用DaemonSet部署节点代理
- en: This book has covered a bunch of high-order workload constructs that encapsulate
    Pods with particular objectives, like Deployment for application deployments,
    StatefulSet for database deployments, and CronJob for period tasks. DaemonSet
    is another workload type that allows you to run a Pod on every node.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书涵盖了多种高级工作负载结构，这些结构封装了具有特定目标的Pods，例如用于应用部署的Deployment、用于数据库部署的StatefulSet以及用于周期性任务的CronJob。DaemonSet是另一种工作负载类型，它允许你在每个节点上运行Pod。
- en: When would you need that? It’s almost entirely for cluster operational reasons,
    like logging, monitoring, and security. As an application developer, DaemonSet
    is generally not your go-to workload construct. Due to the ability to expose services
    internally on a cluster IP, any Pod in your cluster can talk to any service you
    create, so you don’t need to run services on every node just to make them available
    within the cluster. And if you need to be able to connect to a service on localhost,
    you can do that virtually with a Service of type `NodePort`. DaemonSets are generally
    for when you need to perform operations at a node level, like reading load logs
    or observing performance, putting them squarely in the system administration domain.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你什么时候需要这些？这几乎完全是出于集群操作的原因，比如日志记录、监控和安全。作为一个应用开发者，DaemonSet通常不是你的首选工作负载结构。由于可以在集群IP上内部暴露服务，你集群中的任何Pod都可以与任何你创建的服务通信，因此你不需要在每个节点上运行服务，只是为了在集群内部使其可用。如果你需要能够连接到localhost上的服务，你可以通过类型为`NodePort`的服务在虚拟上做到这一点。DaemonSets通常用于当你需要在节点级别执行操作时，比如读取负载日志或观察性能，这完全属于系统管理领域。
- en: DaemonSets are typically how logging, monitoring, and security vendors deploy
    their software. This software performs actions like reading logs off the node
    and uploading it to a central logging solution, querying the kubelet API for performance
    metrics (like how many Pods are running, their boot times, and so forth), and
    for security, such as monitoring container and host behaviors. These are all examples
    of Pods that need to be on every node to gather the data they need for the product
    to function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 通常是如何部署日志记录、监控和安全供应商的软件。该软件执行诸如从节点读取日志并将其上传到中央日志解决方案、查询 kubelet API
    以获取性能指标（如运行中的 Pod 数量、它们的启动时间等）以及安全监控容器和主机行为等操作。这些都是需要在每个节点上运行的 Pod 的示例，以便收集产品运行所需的数据。
- en: 'The typical cluster will have a few DaemonSets running in `kube-system`, such
    as the following abridged list from a GKE cluster, which provides functionality
    like logging, monitoring, and cluster DNS:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的集群将运行一些在 `kube-system` 中的 DaemonSet，如下所示，这是一个来自 GKE 集群的简略列表，它提供了日志记录、监控和集群
    DNS 等功能：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Typically, application developers will not be creating DaemonSets directly but
    rather will be using off-the-shelf ones from vendors. By way of example, though,
    the following listing is a simple DaemonSet that reads logs from the node into
    standard output (stdout).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，应用程序开发者不会直接创建 DaemonSet，而是会使用供应商提供的现成产品。尽管如此，以下列表是一个简单的 DaemonSet，它从节点读取日志到标准输出（stdout）。
- en: Listing 12.3 Chapter12/12.2_DaemonSet/logreader.yaml
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.3 第12章/12.2_DaemonSet/logreader.yaml
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Read and output kube-system container logs from the node
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从节点读取并输出 kube-system 容器的日志
- en: ❷ DaemonSets typically use low resource requests.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ DaemonSet 通常使用低资源请求。
- en: ❸ Mount the volume “logpath” to /var/log.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将“logpath”卷挂载到 /var/log。
- en: ❹ Define the volume “logpath” from /var/log on the host.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从主机上的 /var/log 定义“logpath”卷。
- en: To create the DaemonSet, use
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 DaemonSet，请使用
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the Pods are ready, we can stream the logs:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Pods 准备就绪，我们可以流式传输日志：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In practice, you will likely encounter DaemonSets when deploying logging, monitoring,
    and security solutions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你可能会在部署日志记录、监控和安全解决方案时遇到 DaemonSet。
- en: 12.3 Pod security context
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 Pod 安全上下文
- en: 'The PodSpec has a `securityContext` property where the security attributes
    of the Pod and its containers are defined. If your Pod needs to perform some kind
    of administrative function (e.g., perhaps it’s part of a DaemonSet that is doing
    a node-level operation), it’s here where you would define the various privileges
    it needs. For example, the following is a Pod in a DaemonSet that requests privilege
    on the node:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PodSpec 有一个 `securityContext` 属性，其中定义了 Pod 及其容器的安全属性。如果你的 Pod 需要执行某种管理功能（例如，它可能是执行节点级操作的
    DaemonSet 的一部分），你将在这里定义它需要的各种权限。例如，以下是一个请求节点权限的 DaemonSet 中的 Pod：
- en: Listing 12.4 Chapter12/12.3_PodSecurityContext/admin-ds.yaml
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.4 第12章/12.3_PodSecurityContext/admin-ds.yaml
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With this access, the Pod effectively has root access, and can, for example,
    mount the host filesystem of the node into the container, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种访问权限，Pod 实际上具有 root 权限，例如，可以将节点的宿主文件系统挂载到容器中，如下所示：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you attempt the same on a container without privilege, the mount will fail.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在没有权限的容器上尝试相同的操作，挂载将会失败。
- en: As a developer of a regular application that run on Kubernetes, you will more
    likely be using the `securityContext` properties to *limit* what functions your
    Pod can use to reduce risk. Contrasting the previous example, the following is
    the PodSpec for a Pod with locked-down privileges that runs as the non-root user
    and cannot elevate privileges.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为在 Kubernetes 上运行的应用程序的开发者，你更有可能使用 `securityContext` 属性来 *限制* 你的 Pod 可以使用的功能，以降低风险。与前面的示例形成对比，以下是一个
    PodSpec，它具有受限权限，以非 root 用户身份运行且无法提升权限。
- en: Listing 12.5 Chapter12/12.3_PodSecurityContext/pod.yaml
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.5 第12章/12.3_PodSecurityContext/pod.yaml
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By default, any Pod is free to request whatever capabilities it wants, even
    root access (unless your Kubernetes platform restricts this, as some nodeless
    platforms do). As the cluster operator, this may be something you want to restrict
    as it basically means that anyone with kubectl access to the cluster has root
    privileges. Furthermore, there are some other recommended principles for hardening
    clusters, like not running containers as the root user (which is distinct from
    having root on the node), something that is enforced by the `runAsNonRoot:` `true`
    configuration in the prior example.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，任何Pod都可以自由请求它想要的任何能力，甚至root访问权限（除非您的Kubernetes平台对此进行了限制，就像一些无节点平台所做的那样）。作为集群操作员，您可能希望限制这一点，因为这基本上意味着任何拥有kubectl访问集群的人都有root权限。此外，还有一些其他推荐的原则用于强化集群，例如不作为root用户运行容器（这与在节点上拥有root权限是不同的），这在先前的示例中的`runAsNonRoot:`
    `true`配置中得到强制执行。
- en: The following sections cover these topics, starting with how to build containers
    so they don’t need to run as the root user, and how, as a cluster administrator,
    you can force users of the cluster to adopt this and other desired security settings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节涵盖了这些主题，从如何构建容器使其不需要以root用户身份运行开始，以及作为集群管理员，如何强制集群用户采用此和其他期望的安全设置。
- en: 12.4 Non-root containers
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 非root容器
- en: One common security recommendation when deploying containers is to run them
    as a non-root user. The reason for this is that despite all the fancy packaging,
    Linux containers are basically just processes that run on the host with sandboxing
    technology applied (like Linux cgroups and namespaces). If your container is built
    to run using the root user, which is the default, it actually runs as root on
    the node, just sandboxed. Container sandboxing means that the process doesn’t
    have the power of root access, but it’s still running under the root user. The
    problem with this is that while the sandboxing prevents the process from having
    root access, if there is ever a “container escape” vulnerability due to bugs in
    the underlying Linux containerization technology, the sandboxed container process
    can gain the same privileges as the user it’s running as. That means if the container
    is running as root, a container escape would give full root access on the node—not
    so good.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 部署容器时，一个常见的安全建议是以非root用户身份运行它们。这样做的原因是，尽管有所有这些花哨的包装，Linux容器基本上只是应用了沙箱技术的宿主上运行的进程（如Linux
    cgroups和namespaces）。如果您的容器被构建为使用root用户运行，这是默认设置，它实际上在节点上作为root运行，只是沙箱化。容器沙箱意味着进程没有root访问权限，但它仍在root用户下运行。问题是，尽管沙箱防止进程获得root访问权限，但如果由于底层Linux容器化技术的错误存在“容器逃逸”漏洞，沙箱化的容器进程可以获得与它运行的用户的相同权限。这意味着如果容器以root身份运行，容器逃逸将给节点带来完整的root访问权限——这并不理想。
- en: Since Docker runs all processes as root by default, this means that any container
    escape vulnerabilities can present a problem. While such vulnerabilities are fairly
    rare, they do occur, and for the security principle known as *defense in depth*,
    it’s best to protect against it. Defense in depth means that even though container
    isolation offers protection of the host in the event your application is breached,
    ideally, you would have further layers of defense in case that protection is breached.
    In this case, defense in depth means running your containers as the non-root user,
    so in the event an attacker can breach your container and take advantage of a
    container escape vulnerability in Linux, they still wouldn’t end up with elevated
    privileges on the node. They would need to string together yet another vulnerability
    to elevate their privileges, making for three layers of defense (your application,
    Linux containerization, and Linux user privileges).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Docker默认将所有进程作为root运行，这意味着任何容器逃逸漏洞都可能带来问题。虽然此类漏洞相对较少见，但它们确实存在，并且根据被称为*深度防御*的安全原则，最好对其进行防护。深度防御意味着尽管容器隔离可以在您的应用程序被入侵时保护主机，但理想情况下，您应该有进一步的防御层以防该保护被突破。在这种情况下，深度防御意味着以非root用户身份运行容器，因此即使攻击者能够突破您的容器并利用Linux中的容器逃逸漏洞，他们也不会在节点上获得提升的权限。他们需要串联另一个漏洞来提升他们的权限，从而形成三层防御（您的应用程序、Linux容器化和Linux用户权限）。
- en: NOTE You may be wondering if it’s the best practice not to run container processes
    as root, why then does Docker default to the root user when building containers?
    The answer is developer convenience. It’s convenient to act as the root user in
    a container, as you can use privileged ports (those with numbers below 1024, like
    the default HTTP port 80), and you don’t have to deal with any folder permission
    problems. As you’ll see later in this section, building and running containers
    with the non-root user can introduce some errors that need to be worked through.
    If you adopt this principle from the start, however, you may not find it so difficult
    to fix these problems as they arise, and the payoff is adding one more layer of
    defense into your system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可能想知道如果不以root用户运行容器进程是最佳实践，那么为什么Docker在构建容器时默认使用root用户？答案是开发者便利性。在容器中以root用户身份操作很方便，因为您可以使用特权端口（如默认的HTTP端口80等低于1024的端口），并且您不必处理任何文件夹权限问题。正如您将在本节后面看到的那样，以非root用户构建和运行容器可能会引入一些需要解决的错误。如果您从一开始就采用这个原则，那么在问题出现时，您可能不会觉得修复这些问题那么困难，而且回报是向您的系统添加一层额外的防御。
- en: Preventing containers from running as the root user is simple in Kubernetes,
    although the problem (as we’ll see shortly) is that not all containers are designed
    to run this way and may fail. You can annotate your Pods in Kubernetes to prevent
    them from running as a root user. So, to achieve the goal of not running as root,
    the first step is to simply add this annotation! If you’re configuring a Kubernetes
    cluster for a wider team or you’re a member of that team using such a configured
    cluster, a Kubernetes admission controller can be used to automatically add this
    annotation to every Pod (see section 12.5.1). The end result is the same, so for
    this demo, we’ll just add it manually. The following Deployment enforces the best
    practice to prevent containers from running as root.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中防止容器以root用户运行很简单，尽管（我们很快就会看到）问题在于并非所有容器都设计为以这种方式运行，可能会失败。您可以在Kubernetes中注释您的Pod以防止它们以root用户运行。因此，为了达到不作为root用户运行的目标，第一步就是简单地添加这个注释！如果您正在为更广泛的团队配置Kubernetes集群，或者您是该团队的一员，正在使用这样的配置集群，可以使用Kubernetes准入控制器自动将此注释添加到每个Pod（见第12.5.1节）。最终结果是一样的，所以在这个演示中，我们只需手动添加。以下部署强制执行防止容器以root用户运行的最佳实践。
- en: Listing 12.6 Chapter12/12.4_NonRootContainers/1_permission_error/deploy.yaml
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.6 第12章/12.4_非root容器/1_permission_error/deploy.yaml
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Prevent running this container as the root user.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 防止以root用户运行此容器。
- en: Unfortunately, we’re not done because the container itself doesn’t configure
    a non-root user to run as. If you try to create this Deployment, Kubernetes will
    enforce the `securityContext` and won’t let the container run as root. The following
    is the truncated output you’ll see if you try and create this Deployment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们还没有完成，因为容器本身没有配置非root用户来运行。如果您尝试创建此部署，Kubernetes将强制执行`securityContext`，并阻止容器以root用户运行。以下是在尝试创建此部署时您将看到的截断输出。
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To resolve this problem, you need to configure the user that the Pod will be
    run under. Root is always user 0, so we just need to set any other user number;
    I’m going to pick user 1001\. This can either be declared in the Dockerfile with
    `USER` `1001` or in the Kubernetes configuration with `runAsUser:` `1001`. When
    both are present, the Kubernetes configuration takes priority, similar to how
    the `command` parameter in a Kubernetes PodSpec overrides `CMD` if present in
    the Dockerfile. Here’s the Dockerfile option:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，您需要配置Pod将运行的用户。root用户总是用户0，所以我们只需要设置任何其他用户编号；我将选择用户1001。这可以在Dockerfile中使用`USER`
    `1001`声明，或者在Kubernetes配置中使用`runAsUser:` `1001`。当两者都存在时，Kubernetes配置具有优先级，类似于Kubernetes
    PodSpec中的`command`参数覆盖Dockerfile中存在的`CMD`。以下是Dockerfile选项：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Or, you can specify it in the PodSpec by adding an additional field to the
    security context section:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在PodSpec中通过在安全上下文部分添加一个额外的字段来指定它：
- en: Listing 12.7 Chapter12/12.4_NonRootContainers/1_permission_error/deploy-runas.yaml
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.7 第12章/12.4_非root容器/1_permission_error/deploy-runas.yaml
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Both approaches work, but what I recommend is to configure it on the Kubernetes
    side as this is better for keeping your development and production environments
    separate. If you specify the run-as user in the Dockerfile and want to run your
    container locally outside of Kubernetes and try to mount a volume, you’ll hit
    a snag, like Docker issue #2259,[³](#pgfId-1114104) which prevents you from mounting
    a volume as a user other than root, a 7+-year-old problem. Since the original
    security concern is not to run containers as root is only related to production,
    why not relegate this whole “run as non-root” concern to production as well? Fortunately,
    it’s easy to let your container run as root in Docker locally for maximum convenience
    and as non-root in production in Kubernetes for better defense in depth.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都有效，但我推荐你在Kubernetes侧进行配置，因为这样可以更好地将你的开发和生产环境分开。如果你在Dockerfile中指定了运行用户，并想在Kubernetes之外本地运行容器并尝试挂载卷，你可能会遇到像Docker问题#2259[³](#pgfId-1114104)这样的问题，这阻止你以非root用户身份挂载卷，这是一个7年以上的问题。由于原始的安全担忧仅与生产环境有关，为什么不让整个“以非root用户身份运行”的问题也归咎于生产环境呢？幸运的是，在Docker本地以最大便利性让容器以root用户身份运行，在生产环境中在Kubernetes中以非root用户身份运行，以更好地进行深度防御。
- en: Specifying `runAsUser:` `1001` is enough to run our container as non-root. Provided
    that the container is capable of running as non-root, your job is done. Most public,
    well-known containers should be designed to run as non-root, but this likely isn’t
    the case for your own containers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`runAsUser:` `1001`就足以以非root用户身份运行我们的容器。只要容器能够以非root用户身份运行，你的任务就完成了。大多数公共、知名的容器应该被设计成以非root用户身份运行，但你的容器可能不是这种情况。
- en: In the case of our example container, it wasn’t designed to run as non-root
    and will need to be fixed. Two major differences when running the container as
    non-root are that you can’t listen on privileged ports (i.e., those between 1
    and 1023), and you don’t have write access by default to the container’s writable
    layer (meaning, by default, you can’t write any files!). This is a problem for
    version 6 of the Timeserver sample app (Chapter12/timeserver6/server.py), which
    listens on port 80 and writes a log file to `/app/logs`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例容器的情况下，它没有被设计成以非root用户身份运行，需要修复。以非root用户身份运行容器时的两个主要区别是，你不能监听特权端口（即1到1023之间的端口），并且默认情况下你没有对容器可写层的写入访问权限（这意味着默认情况下，你不能写入任何文件！）。这对于版本6的Timeserver示例应用（第12章/timeserver6/server.py）来说是个问题，它监听端口80并将日志文件写入`/app/logs`。
- en: Updating containers to run as non-root
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 更新容器以以非root用户身份运行
- en: If you deploy the revised Deployment from listing 12.7 with `runAsUser` specified,
    you will see that there is no `CreateContainerConfigError` error when deployed,
    but the container itself is crashing. When your container starts crashing after
    you change the user it runs as to non-root, it’s probably a permission error related
    to that change. Before you start debugging the non-root user errors, be sure your
    container runs fine as root; otherwise, the problem could be something completely
    unrelated.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用列表12.7中指定的`runAsUser`部署修订版的Deployment，你将看到在部署时没有`CreateContainerConfigError`错误，但容器本身正在崩溃。当你将容器运行的用户更改为非root用户后，容器开始崩溃，这很可能是与该更改相关的权限错误。在你开始调试非root用户错误之前，确保你的容器以root用户身份运行良好；否则，问题可能是完全无关的。
- en: 'The steps to debug permission problems for containers running as non-root will
    vary, but let’s walk through how to find and fix these two common errors with
    our example app. The following are the output and truncated logs that I see for
    this crashing container:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以非root用户身份运行的容器，调试权限问题的步骤可能会有所不同，但让我们通过我们的示例应用来了解一下如何找到并修复这两个常见的错误。以下是我看到的这个崩溃容器的输出和截断日志：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Fortunately, the port problem in Kubernetes is an easy fix without any end-user
    effect. We can change the port that the container uses while keeping the standard
    port 80 for the load balancer. First, let’s update the port used by the container.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes中的端口问题是一个简单的修复，不会对最终用户产生影响。我们可以更改容器使用的端口，同时保持负载均衡器使用的标准端口80。首先，让我们更新容器使用的端口。
- en: Listing 12.8 Chapter12/timeserver7/server.py
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.8 第12章/timeserver7/server.py
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If we’re changing ports in the application, we’ll need to update our Kubernetes
    Service configuration to match the new port by updating the `targetPort`. Fortunately,
    we don’t need to change the external port of the Service, as the Service networking
    glue is provided by Kubernetes and doesn’t run as a particular user, so it can
    use ports below 1024.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在更改应用程序的端口，我们需要更新我们的 Kubernetes 服务配置，通过更新 `targetPort` 来匹配新的端口。幸运的是，我们不需要更改服务的端口号，因为服务网络粘合剂由
    Kubernetes 提供，并且不以特定用户身份运行，因此它可以使用低于 1024 的端口。
- en: Listing 12.9 Chapter12/12.4_NonRootContainers/2_fixed/service.yaml
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.9 第 12 章第 12.4 节/非 root 容器/2_fixed/service.yaml
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Targets the new container port
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定位新的容器端口
- en: Once the socket problem is fixed and we rerun the application, another error
    will be encountered when the app attempts to write to the log file on disk. This
    error didn’t stop the app from starting but is encountered when a request is made.
    Looking at those logs, I see
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解决了套接字问题，并重新运行应用程序，当应用程序尝试将日志写入磁盘上的日志文件时，将遇到另一个错误。这个错误并没有阻止应用程序启动，但在请求时遇到了。查看这些日志，我看到
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you see a permission denied error when running as non-root when writing a
    file, it’s a clear sign that your folder permissions have not been set up correctly
    for non-root users.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在以非 root 身份运行并写入文件时遇到权限拒绝错误，这是您的文件夹权限没有正确设置给非 root 用户的明确迹象。
- en: The simplest way to solve this is to set the group permissions on the folder
    in question. I like using the group permissions, as we can use the same group
    (i.e., group 0) for running locally using Docker and deploying in production to
    Kubernetes without environment-specific changes in the Dockerfile. Let’s update
    the Dockerfile to give write access to group 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的最简单方法是将相关文件夹的组权限设置好。我喜欢使用组权限，因为我们可以使用相同的组（即，组 0）在本地使用 Docker 运行，并在生产中部署到
    Kubernetes，而无需在 Dockerfile 中进行针对特定环境的更改。让我们更新 Dockerfile 以给予组 0 写入权限。
- en: Listing 12.10 Chapter12/timeserver7/Dockerfile
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.10 第 12 章timeserver7/Dockerfile
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Updates the permissions on the logs folder
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 更新日志文件夹的权限
- en: 'If you want to run the container in Docker locally using a non-root user to
    test it before deploying to Kubernetes, you can set the user at runtime: `docker`
    `run` `--user` `1001:0` `$CONTAINER_NAME`.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在本地使用 Docker 以非 root 用户运行容器进行测试，然后再部署到 Kubernetes，您可以在运行时设置用户：`docker` `run`
    `--user` `1001:0` `$CONTAINER_NAME`。
- en: 'So there we have it—our revised container (published as version 7) now runs
    happily as the non-root user. Deploy the configuration in Chapter12/12.4_NonRootContainers/
    2_fixed to see it running. If you want to see all the changes made to enable the
    container and configuration to operate as non-root, diff the before and after:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们——经过修订的容器（发布为版本 7）现在作为非 root 用户愉快地运行。将第 12 章第 12.4 节/非 root 容器/2_fixed
    中的配置部署以查看其运行。如果您想查看为使容器和配置以非 root 身份运行所做的所有更改，请比较前后差异：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 12.5 Admission controllers
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 准入控制器
- en: In the previous section, we added `runAsNonRoot` to our Pod to prevent it from
    ever running as root, but we did it manually. If this is a setting we want for
    all Pods, ideally, we’d be able to configure the cluster to reject any Pod without
    this configuration or even just add it automatically.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将 `runAsNonRoot` 添加到我们的 Pod 中，以防止它以 root 身份运行，但我们手动完成了这项操作。如果我们希望所有
    Pods 都有这个设置，理想情况下，我们能够配置集群拒绝任何没有此配置的 Pod，或者甚至自动添加它。
- en: 'This is where admission controllers come in. Admission controllers are bits
    of code that are executed via webhooks when you create an object, like with `kubectl`
    `create` (figure 12.3). There are two types: validating and mutating. Validating
    admission webhooks can accept or reject the Kubernetes object—for example, rejecting
    Pods without `runAsNonRoot`. Mutating admission webhooks can change the object
    as it comes in—for example, setting `runAsNonRoot` to `true`.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是准入控制器发挥作用的地方。准入控制器是一段代码，当您创建一个对象时，例如使用 `kubectl` 的 `create` 命令（如图 12.3），将通过
    webhook 执行。它们有两种类型：验证型和突变型。验证型准入 webhook 可以接受或拒绝 Kubernetes 对象——例如，拒绝没有 `runAsNonRoot`
    的 Pods。突变型准入 webhook 可以在对象到来时更改对象——例如，将 `runAsNonRoot` 设置为 `true`。
- en: '![12-03](../../OEBPS/Images/12-03.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![12-03](../../OEBPS/Images/12-03.png)'
- en: Figure 12.3 The admission process of a Pod that gets scheduled
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 被调度 Pod 的准入过程
- en: You can write your own admission controllers to implement the behavior you desire,
    but depending on what you’re hoping to achieve, you may not need to. Kubernetes
    ships with an admission controller out of the box, and others may be available
    as commercial or open source deployments.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以编写自己的准入控制器来实现所需的操作，但根据您希望实现的目标，您可能不需要这样做。Kubernetes自带了一个准入控制器，并且可能还有其他商业或开源部署可用。
- en: 12.5.1 Pod Security admission
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.1 Pod安全准入
- en: Writing admission controllers is no walk in the park. You need to configure
    certificates, build an application that can be set up as webhook that conforms
    to the request/response API of Kubernetes and have a development process to keep
    it up to date as Kubernetes changes, which it does fairly frequently. The good
    news is that most developers don’t need to write their own admission controllers.
    You’ll typically use those from third-parties or included in Kubernetes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 编写准入控制器并非易事。您需要配置证书，构建一个可以作为webhook设置的应用程序，该webhook符合Kubernetes的请求/响应API，并有一个开发流程来保持其与Kubernetes的更新，Kubernetes更新相当频繁。好消息是，大多数开发者不需要编写自己的准入控制器。您通常会使用第三方提供的或包含在Kubernetes中的那些。
- en: Kubernetes includes admission controllers that can enforce security policies
    like requiring `runAsNonRoot`. Prior to Kubernetes v1.25, PodSecurityPolicy served
    this purpose but never left beta and was removed. Since Kubernetes v1.25, Pod
    Security admission is the recommended way to enforce security policies via an
    admission controller. You can even deploy it manually into clusters running an
    older version of Kubernetes or where the feature wasn’t enabled by the platform
    operator.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes包括可以强制执行安全策略的准入控制器，例如要求`runAsNonRoot`。在Kubernetes v1.25之前，PodSecurityPolicy曾承担这一职责，但从未离开测试版，并且已被移除。自Kubernetes
    v1.25以来，Pod安全准入是推荐通过准入控制器强制执行安全策略的方式。您甚至可以将它手动部署到运行较旧版本Kubernetes或该功能未由平台运营商启用的集群中。
- en: Pod Security Standards
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Pod安全标准
- en: 'The Pod Security Standards define[⁴](#pgfId-1114253) three security policy
    levels that apply at a namespace level:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Pod安全标准定义了三个安全策略级别，这些级别适用于命名空间级别：
- en: '*Privileged*—Pods have unrestricted administrative access and can gain root
    access to nodes.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特权*—Pod拥有无限制的管理访问权限，并且可以获取节点的root访问权限。'
- en: '*Baseline*—Pods cannot elevate privileges to gain administrative access.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基准*—Pod不能提升权限以获得管理访问权限。'
- en: '*Restricted*—Enforces current best practices for hardening (i.e., defense in
    depth), adding additional layers of protection over the baseline profile, including
    restricting running as the root user.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*受限*—强制执行加固（即深度防御）的最佳实践，在基准配置文件之上添加额外的保护层，包括限制以root用户身份运行。'
- en: Basically, `privileged` should only be granted for system workloads; `baseline`
    offers a good balance of security and compatibility; and `restricted` offers additional
    defense in depth at a cost of some compatibility, such as needing to ensure all
    containers can run as non-root, per section 12.4.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，`privileged`权限应仅授予系统工作负载；`baseline`提供了安全和兼容性之间的良好平衡；而`restricted`则在一定程度上牺牲了兼容性，以提供更深入的安全防御，例如需要确保所有容器都能以非root用户身份运行，如第12.4节所述。
- en: Creating a namespace with Pod Security
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 创建具有Pod安全的命名空间
- en: In keeping with the running example of this chapter and to implement the most
    secure profile, let’s create a namespace with the `restricted` policy. This will
    require Pods to run as a user other than root and will enforce several other security
    best practices as well.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与本章的运行示例保持一致并实现最安全的配置文件，让我们创建一个应用`restricted`策略的命名空间。这将要求Pod以非root用户身份运行，并强制执行其他几个安全最佳实践。
- en: To start, create a new namespace with the `restricted` policy. We’ll call this
    namespace `team1`, as it can be the place for a hypothetical `team1` to deploy
    their code to.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个新的命名空间并应用`restricted`策略。我们可以将其命名为`team1`，因为这个命名空间可以成为假设的`team1`部署代码的地方。
- en: Listing 12.11 Chapter12/12.5_PodSecurityAdmission/namespace.yaml
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.11 第12章/12.5_PodSecurityAdmission/namespace.yaml
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These two labels set the policy we want to enforce and the version of the policy
    that will be enforced. The `enforce-version` label exists, as the definition of
    what the policy actually enforces may evolve as new security risks are uncovered.
    Instead of pinning a particular version, for example, `v1.28`, you can specify
    `latest` to apply the most recent policy. However, there is a high risk that policy
    changes between Kubernetes versions will break existing workloads, so it’s advisable
    to always pick a specific version. Ideally, you would test the newer policy versions
    in a staging namespace or cluster to validate them first, before updating the
    `enforce-version` in your production environment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个标签设置了我们要实施的策略以及将要实施的策略版本。`enforce-version` 标签存在，因为策略实际执行的定义可能会随着新安全风险的发现而演变。例如，你不必将特定版本固定为
    `v1.28`，你可以指定 `latest` 以应用最新的策略。然而，政策在 Kubernetes 版本之间的变化可能会破坏现有的工作负载，因此建议始终选择一个特定版本。理想情况下，你会在生产环境更新
    `enforce-version` 之前，在一个暂存命名空间或集群中测试较新的策略版本以验证它们。
- en: 'Let’s create this namespace:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建这个命名空间：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, if we try to deploy a Pod from chapter 3 that doesn’t set `runAsNonRoot`,
    the Pods will be rejected:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们尝试部署第 3 章中未设置 `runAsNonRoot` 的 Pod，Pod 将会被拒绝：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If we add the appropriate `securityContext` (listing 12.12) to satisfy the Pod
    Security admission policy, our Pod will be admitted. It’s also important to use
    the updated container that is designed to run as root from the previous section
    so that it runs correctly under these new conditions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们添加适当的 `securityContext`（见12.12），以满足 Pod 安全性接受策略，我们的 Pod 将会被接受。同时，使用上一节中设计为以
    root 用户运行的新容器也非常重要，以确保它在这些新条件下能够正确运行。
- en: Listing 12.12 Chapter12/12.5_PodSecurityAdmission/nonroot_pod.yaml
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.12 Chapter12/12.5_PodSecurityAdmission/nonroot_pod.yaml
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Security context required by the restricted profile
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 受限配置文件所需的安全上下文
- en: 'Creating this non-root Pod should now succeed:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这个非 root Pod 应该现在可以成功：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Debugging Pod admission rejections for Deployments
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 调试 Deployments 的 Pod 接受拒绝问题
- en: The two examples in this section used stand-alone Pods, rather than Deployments.
    The reason I did that is it’s easier to debug when the Pod’s admission is rejected.
    Once you confirm it’s working as expected as a standalone Pod, you can always
    embed the PodSpec in the Deployment of your choice.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的两个示例使用了独立的 Pods，而不是 Deployments。我这样做的原因是当 Pod 的接受被拒绝时更容易调试。一旦你确认它作为一个独立的
    Pod 运作正常，你就可以将其 PodSpec 嵌入你选择的任何 Deployments 中。
- en: If you create a Deployment that violates the security constraints, you won’t
    see an error printed on the console, like in my example, when I tried to create
    the Pod directly. This is an unfortunate fact of Kubernetes’s implementation of
    Deployment. Creating the Deployment object itself succeeds, so you don’t see an
    error on the console. However, when the Deployment then goes to create its Pods,
    they will fail. Also, since the Deployment actually creates an object called a
    ReplicaSet under the hood to manage Pods of a particular version of the deployment,
    you won’t even find this error if you describe the Deployment object but rather
    need to inspect its ReplicaSet.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你创建了一个违反安全约束的 Deployment，你不会在控制台上看到错误打印，就像我在尝试直接创建 Pod 时的例子一样。这是 Kubernetes
    实现 Deployment 的一个不幸事实。创建 Deployment 对象本身是成功的，所以你不会在控制台上看到错误。然而，当 Deployment 然后尝试创建其
    Pods 时，它们将会失败。此外，由于 Deployment 实际上在底层创建了一个名为 ReplicaSet 的对象来管理特定版本的 Pods，所以如果你描述
    Deployment 对象而不是检查其 ReplicaSet，你甚至找不到这个错误。
- en: I’ve not mentioned ReplicaSet yet in the book as it’s essentially implementation
    detail. Basically, a ReplicaSet is a workload construct that manages a set of
    Pods. Deployment uses them by creating a new ReplicaSet for each version you deploy.
    So when you’re doing a rolling update, the Deployment will actually have two ReplicaSets,
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我在书中还没有提到 ReplicaSet，因为它基本上是实现细节。基本上，ReplicaSet 是一个管理一组 Pods 的工作负载结构。Deployment
    通过为每个部署的版本创建一个新的 ReplicaSet 来使用它们。所以当你进行滚动更新时，Deployment 实际上会有两个 ReplicaSets，
- en: one for the old version and one for the new; these are scaled gradually to achieve
    the rolling update. Normally, this implementation detail doesn’t matter, which
    is why I didn’t spend any time on it in the book so far, but here is+ one of the
    few times it does since the ReplicaSet is where this particular error is hidden.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于旧版本，一个用于新版本；这些版本会逐步扩展以实现滚动更新。通常，这个实现细节并不重要，这就是为什么我在书中至今没有花时间讨论它，但这里是一个例外，因为
    ReplicaSet 是这个特定错误隐藏的地方。
- en: It’s not exactly simple, but here’s how to debug this type of problem. Normally,
    when you create a Deployment, it will create Pods. If you run *kubectl get pods*,
    you should see a bunch of Pods. Now, those Pods may not always be `Ready`—there
    are a bunch of reasons why they might be `Pending` (and, in some cases, may get
    stuck in the `Pending` state forever, as covered in section 3.2.3), but these
    Pod objects will normally at least exist with some status. If when you call `kubectl
    get pods` and don’t see any Pod objects at all for your Deployment, it could mean
    that those Pods were rejected during admission, which is why there are no Pod
    objects.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不完全简单，但以下是如何调试这类问题的方法。通常，当你创建一个Deployment时，它将创建Pod。如果你运行`kubectl get pods`，你应该看到很多Pod。现在，这些Pod可能并不总是`Ready`——它们可能`Pending`的原因有很多（在某些情况下，它们可能永远卡在`Pending`状态，如第3.2.3节所述），但这些Pod对象通常至少会存在一些状态。如果你调用`kubectl
    get pods`却看不到你的Deployment的任何Pod对象，这可能意味着这些Pod在准入过程中被拒绝，这就是为什么没有Pod对象的原因。
- en: 'Since it’s the ReplicaSet owned by the Deployment that actually creates the
    Pods, you need to describe the ReplicaSet to see the error with `kubectl` `describe`
    `replicaset` (`kubectl` `describe` `rs` for short). The following is an example
    with the output truncated to show the error message of interest:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于是Deployment拥有的ReplicaSet实际创建了Pod，你需要使用`kubectl describe replicaset`（简称`kubectl
    describe rs`）来描述ReplicaSet以查看错误。以下是一个示例，输出被截断以显示感兴趣的错误消息：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When you’re done, you can delete this namespace and all resources as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成时，你可以按照以下方式删除此命名空间及其所有资源：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 12.5.2 Balancing security with compatibility
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.2 在安全性与兼容性之间取得平衡
- en: In the prior section we used the example of the `restricted` Pod security profile
    and configured our container to be able to run as a non-root user. Hopefully,
    this has given you the confidence to be able to run containers in a highly secure
    manner. While this is the best practice and may be required in situations like
    regulated industries, there is a clear tradeoff with ease of development, and
    it may not always be practical. Ultimately, it’s up to you, your security team,
    and maybe your regulators to determine what security profile you’re happy with.
    I’m not necessarily recommending every single Kubernetes workload should be put
    into a namespace with the `restricted` profile. I do suggest that you use `baseline`
    for every nonadministrative workload you deploy in your cluster, as it helps protect
    your cluster in the event that one of your containers is compromised and shouldn’t
    cause any incompatibility with the average app. Administrative workloads that
    need the `privileged` profile should be run in their own namespaces, separate
    from common workloads.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了`受限`Pod安全配置文件的例子，并配置了我们的容器以非root用户身份运行。希望这已经给了你信心，能够以高度安全的方式运行容器。虽然这是最佳实践，并且在像受监管行业这样的情况下可能需要，但它与开发便利性之间存在明显的权衡，并且可能并不总是实际可行。最终，这取决于你、你的安全团队，也许还有你的监管机构，来决定你满意的哪个安全配置文件。我并不一定建议将每个Kubernetes工作负载都放入具有`受限`配置文件的命名空间中。我确实建议你为你在集群中部署的每个非管理性工作负载使用`基准`配置，因为它有助于保护你的集群，以防万一你的某个容器被入侵，而且不应与普通应用程序造成任何不兼容性。需要`特权`配置文件的管理性工作负载应在自己的命名空间中运行，与普通工作负载分开。
- en: 12.6 Role-based access control
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.6 基于角色的访问控制
- en: Let’s say that you have a requirement for Pods to run as non-root (section 12.4)
    and set up an admission controller to enforce this requirement using Pod Security
    admission (section 12.5). This sounds great, provided you trust all the users
    of your cluster not to mess anything up and remove those restrictions, whether
    accidentally or on purpose. To actually enforce the requirements of your admission
    controller and create a tiered user permission setup with roles like platform
    operator, who can configure namespaces and controller, and developer, who can
    deploy to namespaces, but not remove admission controllers, you can use role-based
    access control (RBAC).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要Pod以非root身份运行（第12.4节）并设置一个准入控制器，使用Pod安全准入（第12.5节）来强制执行此要求。这听起来很棒，前提是你信任你的集群的所有用户不会搞砸任何事情并移除这些限制，无论是意外还是故意。为了实际强制执行准入控制器的要求并创建一个分层用户权限设置，包括平台操作员角色（可以配置命名空间和控制器）和开发者角色（可以部署到命名空间，但不能移除准入控制器），你可以使用基于角色的访问控制（RBAC）。
- en: RBAC is a way to control what access users of the cluster have. One common setup
    is to give developers in a team access to a particular namespace in the cluster,
    with all the desired Pod Security policies configured. This gives them the freedom
    to deploy whatever they like within the namespace, provided it conforms to the
    security requirements that’s been set. This way it’s still following DevOps principles,
    as developers are the ones doing the deployments, just with some guardrails in
    place.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC 是一种控制集群用户访问权限的方式。一种常见的设置是为团队中的开发者提供对集群中特定命名空间的访问权限，并配置所有所需的 Pod 安全策略。这给了他们在命名空间内部署他们喜欢的内容的自由，前提是它符合已设定的安全要求。这种方式仍然遵循
    DevOps 原则，因为开发者是进行部署的人，只是有一些安全措施。
- en: 'RBAC is configured through two Kubernetes object types at a namespace level:
    Role and RoleBinding. Role is where you define a particular role for a namespace,
    like the developer role. RoleBinding is where you assign this role to subjects
    in your cluster (i.e., your developer identities). There are also cluster-level
    versions, ClusterRole and ClusterRoleBinding, which behave identically to their
    namespace-level counterparts, except that they grant access at a cluster level.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC 通过两个 Kubernetes 对象类型在命名空间级别进行配置：Role 和 RoleBinding。Role 是你为命名空间定义特定角色的地方，如开发者角色。RoleBinding
    是你将此角色分配给集群中的主体（即你的开发者身份）的地方。还有集群级别的版本，ClusterRole 和 ClusterRoleBinding，它们的行为与其命名空间级别的对应物相同，只是它们在集群级别授予访问权限。
- en: Namespace Role
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间角色
- en: In the Role, you specify the API group(s), the resource(s) within that group,
    and the verb(s) that you are granting access to. Access is additive (there is
    no subtractive option), so everything you define grants access. Since our goal
    is to create a Role that gives the developer access to do pretty much everything
    within their namespace *except* modify the namespace itself and remove the Pod
    Security annotation, the following listing is a Role that can achieve that.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Role 中，你指定 API 组（s）、该组内的资源（s）以及你授予访问权限的动词（s）。访问是累加的（没有减法选项），所以你定义的所有内容都授予访问权限。由于我们的目标是创建一个
    Role，让开发者能够访问他们命名空间内的几乎所有内容 *除了* 修改命名空间本身和删除 Pod 安全注释，以下列表是一个可以达成此目标的 Role。
- en: Listing 12.13 Chapter12/12.6_RBAC/role.yaml
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.13 第 12 章/12.6_RBAC/role.yaml
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ The empty string here indicates the core API group.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这里的空字符串表示核心 API 组。
- en: ❷ Allows developers to view the namespace resource but not edit it
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 允许开发者查看命名空间资源但不能编辑它
- en: ❸ Grants developers full access to core workload types
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 授予开发者对核心工作负载类型的完全访问权限
- en: ❹ apps includes resources like Deployment.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ apps 包括像 Deployment 这样的资源。
- en: ❺ autoscaling includes resources like the HorizontalPodAutoscaler.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ autoscaling 包括像 HorizontalPodAutoscaler 这样的资源。
- en: ❻ batch includes the Job workloads.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ batch 包括 Job 工作负载。
- en: ❼ networking.k8s.io is needed so developers can configure Ingress.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ networking.k8s.io 是必需的，以便开发者可以配置 Ingress。
- en: ❽ policy is required for configuring PodDisruptionBudgets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ policy 是配置 PodDisruptionBudgets 所必需的。
- en: This Role grants access to the `team1` namespace and allows the user to modify
    Pods, Services, Secrets, and ConfigMaps within the core API grouping and all resources
    in the apps, autoscaling, batch, networking.k8s.io, and policy groupings. This
    particular set of permissions will let the developer deploy nearly every YAML
    file in this book, including Deployment, StatefulSet, Service, Ingress, Horizontal
    Pod Autoscaler, and Job objects. Importantly, the namespaces resource is not listed
    in the core API group (which is the group listed with the empty string `""`),
    so the user won’t be able to modify the namespace.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此角色授予对 `team1` 命名空间的访问权限，并允许用户在核心 API 组中修改 Pods、Services、Secrets 和 ConfigMaps，以及在
    apps、autoscaling、batch、networking.k8s.io 和 policy 组中的所有资源。这个特定的权限集将允许开发者部署本书中的几乎所有
    YAML 文件，包括 Deployment、StatefulSet、Service、Ingress、Horizontal Pod Autoscaler 和
    Job 对象。重要的是，namespaces 资源未列在核心 API 组中（这是与空字符串 `""` 列出的组），因此用户将无法修改命名空间。
- en: Once the Role exists, to grant this Role to our developer, we can use a RoleBinding
    where the subject is our user.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Role 存在，为了将此 Role 授予我们的开发者，我们可以使用 RoleBinding，其中主体是我们的用户。
- en: Listing 12.14 Chapter12/12.6_RBAC/rolebinding.yaml
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.14 第 12 章/12.6_RBAC/rolebinding.yaml
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ References the Role from listing 12.13
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 引用列表 12.13 中的 Role
- en: ❷ Sets this to be the identity of your developer. For GKE, this is a Google
    user who has the Kubernetes Engine Cluster Viewer IAM role access to the project.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将此设置为开发者的身份标识。对于 GKE，这是一个具有 Kubernetes Engine Cluster Viewer IAM 角色访问权限的 Google
    用户。
- en: Note that the acceptable values within the `User` subject are governed by your
    Kubernetes platform and any identity systems you have configured. With Google
    Cloud, the name here can be any Google user, referenced by their email address.
    RBAC authorizes the user to the actions specified in the Role. However, the user
    also needs to be able to authenticate to the cluster. In the case of Google Cloud,
    that is achieved by assigning a role such as Kubernetes Engine Cluster Viewer
    to the user. This role includes the `container.clusters.get` permission, which
    allows the user to authenticate to the cluster without actually being given any
    permissions inside the cluster (allowing you to configure fine-tuned permissions
    with RBAC). The exact steps here will vary depending on your platform provider.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`User`主题中可接受的价值受您的 Kubernetes 平台和您配置的任何身份系统管理。使用 Google Cloud，这里的名称可以是任何
    Google 用户，通过他们的电子邮件地址引用。RBAC 授权用户执行角色中指定的操作。然而，用户还需要能够对集群进行认证。在 Google Cloud 的例子中，这是通过将
    Kubernetes Engine 集群查看器等角色分配给用户来实现的。此角色包括`container.clusters.get`权限，允许用户在不实际在集群内部获得任何权限的情况下对集群进行认证（允许您使用
    RBAC 配置精细的权限）。这里的具体步骤将根据您的平台提供商而有所不同。
- en: Authentication vs. authorization
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 认证与授权的区别
- en: Authentication (AuthN) is the means by which the user presents their identity
    credentials to the system. In this case, being able to authenticate to the cluster
    means that the user can retrieve credentials to access the cluster via `kubectl`.
    Authorization (AuthZ) is the process of granting users access within the cluster.
    Depending on your platform’s IAM system, it should be possible to allow users
    to authenticate to the cluster (e.g., get credentials to use `kubectl`) but not
    actually be able to perform any action (no authorization). You can then use RBAC
    to grant the precise authorization you want. In the case of GKE, granting users
    the Kubernetes Engine Cluster Viewer role in the IAM permissions (outside of Kubernetes)
    will allow them to authenticate, after which you can authorize them to access
    specific resources using RBAC and the examples shown here. Again, depending on
    your particular Kubernetes platform, it’s possible, as is the case with GKE, that
    some IAM roles will also grant the user authorization to some resources in addition
    to whatever RBAC rules you have here. The project-wide Viewer role is one such
    example in GKE that will allow users to view most of the resources in the cluster
    without needing specific RBAC rules to do so.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 认证（AuthN）是用户向系统展示其身份凭证的手段。在这种情况下，能够对集群进行认证意味着用户可以通过`kubectl`检索到访问集群的凭证。授权（AuthZ）是在集群内授予用户访问权限的过程。根据您的平台
    IAM 系统的不同，应该可以允许用户对集群进行认证（例如，获取使用`kubectl`的凭证），但实际上无法执行任何操作（没有授权）。然后您可以使用 RBAC
    授予您想要的精确授权。在 GKE 的例子中，在 IAM 权限（在 Kubernetes 之外）中授予用户 Kubernetes Engine 集群查看器角色，将允许他们进行认证，之后您可以使用
    RBAC 和这里展示的示例授权他们访问特定的资源。再次强调，根据您的特定 Kubernetes 平台，就像 GKE 的情况一样，某些 IAM 角色可能会授予用户除了您在这里设置的
    RBAC 规则之外的某些资源的授权。在 GKE 中，项目级查看器角色就是这样一个例子，它将允许用户无需特定的 RBAC 规则即可查看集群中的大多数资源。
- en: 'As the cluster administrator, create the namespace and these two objects:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 作为集群管理员，创建命名空间和这两个对象：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With this role and binding deployed in the cluster, our developer user should
    be able to deploy most of the code in this book in the `team1` namespace but specifically
    not be able to change any other namespaces or edit the `team1` namespace itself.
    For a meaningful experiment, you’ll need to set an actual user as the User subject
    in the RoleBinding—for example, a test developer account).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中部署了此角色和绑定后，我们的开发者用户应该能够在`team1`命名空间中部署本书中的大多数代码，但具体不能更改任何其他命名空间或编辑`team1`命名空间本身。为了进行有意义的实验，您需要在
    RoleBinding 中将实际用户设置为`User`主题——例如，一个测试开发者账户）。
- en: 'To verify the RBAC is configured correctly, switch to the test developer account
    by authenticating to the cluster as the user specified in the `subjects` field.
    Once authenticated as our developer user, try to deploy something into the default
    namespace, and it should fail, as no RBAC permissions were granted:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证 RBAC 是否配置正确，通过作为`subjects`字段中指定的用户对集群进行认证来切换到测试开发者账户。一旦以我们的开发者用户身份认证，尝试在默认命名空间中部署某些内容，它应该会失败，因为没有授予任何
    RBAC 权限：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Switching the context to the `team1` namespace, for which we configured this
    test user with the previous Role, we should now be able to create the Deployment:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到 `team1` 命名空间，我们之前已经为这个测试用户配置了该角色，现在我们应该能够创建 Deployment：
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'While this developer can now deploy things in the namespace, if they try to
    edit the namespace to gain the privileged Pod Security level, they will be restricted
    by the lack of edit permission on the namespace resource:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个开发者现在可以在命名空间中部署东西，但如果他们尝试编辑命名空间以获得特权 Pod 安全级别，他们将受到对命名空间资源编辑权限不足的限制：
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Cluster role
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 集群角色
- en: So far, we’ve set up a Role and RoleBinding to give a developer access to a
    particular namespace. With this Role, they can deploy most of the configuration
    in this book. There are, however, a couple of things they won’t be able to do,
    and that is create a *PriorityClass* (chapter 6), create a *StorageClass* (chapter
    9), or list the *PersistentVolumes* in the cluster (chapter 9). Those resources
    are considered cluster-wide objects, so we can’t amend the namespace-specific
    Role we created earlier to grant that permission. Instead, we’ll need a separate
    ClusterRole and ClusterRole binding to grant this additional access.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经设置了一个 Role 和 RoleBinding，以授予开发者访问特定命名空间的权限。有了这个 Role，他们可以部署本书中的大部分配置。然而，他们无法做到的是创建
    *PriorityClass*（第 6 章）、创建 *StorageClass*（第 9 章）或列出集群中的 *PersistentVolumes*（第 9
    章）。这些资源被认为是集群级别的对象，因此我们不能修改之前创建的命名空间特定的 Role 来授予该权限。相反，我们需要一个单独的 ClusterRole 和
    ClusterRole binding 来授予额外的访问权限。
- en: Figuring out what permissions to grant
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 确定要授予的权限
- en: 'I’ve done the work here to provide a Role definition that covers all the needed
    permissions to deploy the code in the book, but there may be other missing permissions
    that you need to grant developers in the context of your own workload deployments.
    To figure out which groups, resources, and verbs you need to grant, you can consult
    the API docs. When debugging permission errors—say a developer is complaining
    that they don’t have the access they need—you can simply inspect the error message.
    Consider the following example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里做了工作，提供了一个涵盖所有部署书中代码所需权限的 Role 定义，但可能还有其他缺失的权限，您需要在您自己的工作负载部署的上下文中授予开发者。为了确定您需要授予哪些组、资源和动词，您可以查阅
    API 文档。当调试权限错误时——比如说开发者抱怨他们没有所需的访问权限——您可以简单地检查错误信息。考虑以下示例：
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: To add this permission to the Role, we can see that the group is `scheduling.k8s
    .io`, the resource is `priorityClasses`, the verb is `create`, and the RBAC scope
    is `clusterrole`. Thus, add a rule with these values to a ClusterRole definition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此权限添加到 Role 中，我们可以看到组是 `scheduling.k8s.io`，资源是 `priorityClasses`，动词是 `create`，RBAC
    范围是 `clusterrole`。因此，在 ClusterRole 定义中添加一个具有这些值的规则。
- en: The following listing shows a ClusterRole to provide the additional permissions
    needed to create StorageClass and PriorityClass objects.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了一个 ClusterRole，以提供创建 StorageClass 和 PriorityClass 对象所需的其他权限。
- en: Listing 12.15 Chapter12/12.6_RBAC/clusterrole.yaml
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.15 第 12 章/12.6_RBAC/clusterrole.yaml
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Grants the developer access to modify all PriorityClasses in the cluster
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 授予开发者修改集群中所有 PriorityClasses 的访问权限
- en: ❷ Grants the developer access to modify all StorageClasses in the cluster
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 授予开发者修改集群中所有 StorageClasses 的访问权限
- en: ❸ Grants the developer read-only access to view and list PersistentVolumes and
    Namespaces
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 授予开发者只读访问权限以查看和列出 PersistentVolumes 和 Namespaces
- en: The next listing shows the ClusterRoleBinding to bind this to our test user,
    which looks very similar to the RoleBinding used earlier.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了 ClusterRoleBinding，将其绑定到我们的测试用户，这与之前使用的 RoleBinding 非常相似。
- en: Listing 12.16 Chapter12/12.6_RBAC/clusterrolebinding.yaml
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.16 第 12 章/12.6_RBAC/clusterrolebinding.yaml
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ References the ClusterRole from listing 12.15
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 引用了列表 12.15 中的 ClusterRole
- en: ❷ Sets this as the identity of your developer. For GKE, this is a Google User
    with Kubernetes Engine Cluster Viewer IAM role access to the project.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将此设置为开发者的身份。对于 GKE，这是一个具有 Kubernetes Engine 集群查看 IAM 角色访问权限的 Google 用户。
- en: With these additional cluster roles and bindings, our developer should be able
    to perform every action in this book.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些额外的集群角色和绑定，我们的开发者应该能够执行本书中的每一个操作。
- en: Identity federation
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 身份联合
- en: For RBAC to be able to reference your developer identities as Users and Groups,
    your cluster needs to understand how to authenticate your developer’s identities.
    In the case of GKE, it natively understands Google users in the User field as
    well as Google groups when the Google Groups for RBAC feature is enabled. Depending
    on your platform and your corporate identity provider, you may have similar access
    already, or you may need to set it up. This setup is outside the scope of this
    book, but you may consider configuring the OpenID Connect (OIDC) integration so
    that RBAC can reference identities provided by your identity system.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使RBAC能够将你的开发人员身份作为用户和组进行引用，你的集群需要了解如何验证你的开发人员的身份。在GKE的情况下，当启用RBAC的Google Groups功能时，它能够原生地理解用户字段中的Google用户以及Google组。根据你的平台和你的企业身份提供者，你可能已经拥有了类似的访问权限，或者你可能需要设置它。这种设置超出了本书的范围，但你可能考虑配置OpenID
    Connect（OIDC）集成，以便RBAC可以引用你的身份系统提供的身份。
- en: Furthermore, when using an identity system plugin that offers Groups support,
    instead of needing to list every User as a subject of our role bindings, you can
    specify a single Group instead.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当使用提供组支持的标识系统插件时，你不需要列出每个用户作为我们的角色绑定主体，而可以指定一个单一的组。
- en: Applying the Pod Security profile
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 应用Pod安全配置文件
- en: The namespace in this section was created without using Pod Security. If we
    go back and configure the namespace with the Pod Security labels from section
    12.5, it would lock down this namespace to the Restricted Pod Security profile,
    and thanks to RBAC, our developer would not be able to modify that restriction.
    Mission accomplished.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的命名空间是在没有使用Pod安全的情况下创建的。如果我们回到配置命名空间，使用12.5节中的Pod安全标签，它将锁定此命名空间到受限的Pod安全配置文件，并且由于RBAC的存在，我们的开发人员将无法修改这个限制。任务完成。
- en: RBAC for ServiceAccounts
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ServiceAccounts的RBAC
- en: In the examples in this section, we used RBAC with the User subject because
    our developers are actual human users of our cluster. Another common use case
    for RBAC is to grant access to services—that is, code running in the cluster.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中的示例中，我们使用了基于用户的RBAC（基于角色的访问控制），因为我们的开发人员是集群的实际人类用户。RBAC的另一个常见用例是授予对服务的访问权限——即在集群中运行的代码。
- en: Let’s say you have a Pod that belongs to a Deployment in the cluster that needs
    to access the Kubernetes API—say, it’s monitoring the Pod status of another Deployment.
    To give this machine user access, you can create a Kubernetes ServiceAccount and
    then reference that in the subject of your RBAC binding instead of a user.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个Pod属于集群中的Deployment，该Deployment需要访问Kubernetes API——比如说，它在监控另一个Deployment的Pod状态。为了给这个机器用户授予访问权限，你可以创建一个Kubernetes
    ServiceAccount，然后在你的RBAC绑定主体中引用它，而不是用户。
- en: You may see some documentation that sets up ServiceAccounts for human users,
    where the user then downloads the certs of the service account to interact with
    Kubernetes. While this is one way to configure your developers and bypasses the
    need to set up identity federation, it is not recommended as it sits outside of
    your identity system. For example, if the developer quit and their account was
    suspended in the identity system, the tokens they downloaded for the ServiceAccount
    would continue to be valid. It’s better to properly configure identity federation
    and only use User subjects for human users so that if the user is suspended from
    the identity system, their Kubernetes access will also be revoked. Once again,
    managed platforms like Google Cloud make this integration easy; for other platforms,
    you may need to do a bit of setup to get it working.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会看到一些为人类用户设置ServiceAccounts的文档，其中用户随后下载服务账户的证书以与Kubernetes交互。虽然这是一种配置开发人员并绕过设置身份联合的需要的办法，但它不建议这样做，因为它位于你的身份系统之外。例如，如果开发人员离职并且他们的账户在身份系统中被暂停，他们为ServiceAccount下载的令牌仍然有效。更好的做法是正确配置身份联合，并且只为人类用户使用用户主体，这样如果用户在身份系统中被暂停，他们的Kubernetes访问也将被撤销。再次强调，像Google
    Cloud这样的托管平台使这种集成变得简单；对于其他平台，你可能需要做一些设置才能使其工作。
- en: Kubernetes ServiceAccounts are intended for when you have, for example, a Pod
    inside the cluster that needs its own access to the Kubernetes API. Say you want
    to create a Pod to monitor another Deployment. You can create a ServiceAccount
    to use as the subject of the RoleBinding and assign that service account to the
    Pod. The Pod can then utilize that credential when making API calls, including
    with kubectl.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes ServiceAccounts旨在当你有一个例如集群内的Pod需要对其自己的Kubernetes API进行访问时使用。比如说，你想创建一个Pod来监控另一个Deployment。你可以创建一个ServiceAccount作为RoleBinding的主题，并将该服务账户分配给Pod。然后，Pod可以在进行API调用时利用这些凭据，包括使用kubectl。
- en: 12.7 Next steps
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.7 下一步
- en: Pod Security admission can be used to control what permissions the Pod has on
    the Node, and RBAC governs what resources users can manage in the cluster. This
    is a good start; however, if you need further isolation at a network and container
    level there is more you can do.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Pod安全准入可以用来控制Pod在节点上的权限，而RBAC管理用户可以在集群中管理哪些资源。这是一个好的开始；然而，如果你需要在网络和容器级别进行进一步隔离，你还可以做更多。
- en: 12.7.1 Network policies
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.7.1 网络策略
- en: By default, every Pod can talk to every other Pod in the cluster. This is useful,
    as it allows teams operating in different namespaces to share services, but it
    means that Pods, including a potentially compromised Pod, can access other internal
    services. To control traffic to and from the network and other Pods, including
    the ability to restrict Pods in a namespace from accessing Pods in other namespaces,
    you can configure network policies.[⁵](#pgfId-1123407)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，每个Pod都可以与集群中的其他所有Pod通信。这很有用，因为它允许在不同命名空间中工作的团队共享服务，但这也意味着Pod，包括可能被破坏的Pod，可以访问其他内部服务。为了控制网络和其他Pod的流量，包括限制命名空间中的Pod访问其他命名空间的Pod的能力，你可以配置网络策略。[⁵](#pgfId-1123407)
- en: The way network policies work is that if no NetworkPolicy applies to a Pod (by
    selecting it), then all traffic is allowed (and there are no network policies
    by default, thus all traffic is allowed). However, once a NetworkPolicy selects
    the Pod for either ingress or egress traffic, then all traffic in the chosen direction
    is denied other than what you explicitly allow. This means to deny egress traffic
    to a particular destination, you need to build an exhaustive list of what is allowed
    by understanding the requirements of your Pods.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略的工作方式是，如果没有网络策略应用于Pod（通过选择它），则允许所有流量（默认情况下没有网络策略，因此允许所有流量）。然而，一旦网络策略选择了Pod进行入站或出站流量，则除了你明确允许的流量之外，所有选择方向的流量都将被拒绝。这意味着要拒绝特定目的地的出站流量，你需要构建一个详尽的允许列表，这需要理解Pod的要求。
- en: For example, to restrict traffic to Pods in other namespaces you might create
    a rule to allow traffic within the namespace and to the public internet. Since
    such a ruleset omits Pods in other namespaces, that traffic will be denied, thus
    achieving the objective.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了限制对其他命名空间中Pod的流量，你可能创建一个规则来允许命名空间内的流量和对公共互联网的流量。由于这样的规则集省略了其他命名空间的Pod，因此该流量将被拒绝，从而实现目标。
- en: This nature of network policies to deny all traffic other than what you explicitly
    allow means you need to carefully study what access is required (including potentially
    some specific requirements of your platform), and it might take some trial and
    error to get right. I’ve published a series of posts on this topic at [https://wdenniss.com/networkpolicy](https://wdenniss.com/networkpolicy),
    which can help get you started.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略的这一特性，即拒绝除你明确允许的所有流量之外的所有流量，意味着你需要仔细研究所需的访问权限（包括可能的一些特定平台要求），并且可能需要一些尝试和错误才能正确设置。我在[https://wdenniss.com/networkpolicy](https://wdenniss.com/networkpolicy)上发布了一系列关于这个主题的文章，可以帮助你开始。
- en: 12.7.2 Container isolation
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.7.2 容器隔离
- en: Containerization offers some isolation of the process from the node, and Pod
    Security admission allows you to limit the access that containers have, but from
    time to time there are so-called container escape vulnerabilities that can result
    in the process gaining node-level access. It is possible to add an additional
    isolation layer between the container and the host for an added layer of defense
    in depth, beyond what is afforded by containerization alone. This isolation typically
    comes with a performance penalty, which is why you don’t usually see it configured
    by default. If you are running untrusted code in the cluster, for example, for
    a multi-tenant system where users are providing their own containers, then you
    almost certainly want an additional layer of isolation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 容器化提供了一定程度的进程与节点的隔离，Pod安全准入允许您限制容器拥有的访问权限，但有时会出现所谓的容器逃逸漏洞，这可能导致进程获得节点级访问权限。可以在容器和主机之间添加额外的隔离层，以提供比仅容器化更多的深度防御。这种隔离通常伴随着性能损失，这也是为什么您通常看不到默认配置的原因。如果您在集群中运行不受信任的代码，例如，在多租户系统中，用户提供自己的容器，那么您几乎肯定需要一个额外的隔离层。
- en: You can configure your Pods for additional isolation by defining a secure runtime
    with RuntimeClass.[⁶](#pgfId-1123382) A popular choice, developed and open sourced
    by Google, is gVisor[⁷](#pgfId-1123375), which implements the Linux kernel API
    and intercepts system calls between the container and the system kernel to provide
    an isolated sandbox.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过定义一个安全的RuntimeClass来配置Pod以实现额外的隔离。[⁶](#pgfId-1123382)一个流行的选择是由Google开发和开源的gVisor[⁷](#pgfId-1123375)，它实现了Linux内核API并拦截容器和系统内核之间的系统调用，以提供一个隔离的沙盒。
- en: 12.7.3 Cluster hardening
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.7.3 集群加固
- en: I hope this chapter has provided some practical security considerations as you
    develop and deploy your applications to Kubernetes, and potentially find yourself
    operating in clusters with RBAC permissions and restricted admission rules such
    as running non-root containers. For cluster operators, the broader topic of hardening
    your cluster and its operating environment (such as the network, nodes, and cloud
    resources) is a lengthy one, and many of the considerations are specific to the
    precise platform that you choose.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这一章在您开发和部署应用程序到Kubernetes时提供了一些实际的安全考虑，并且可能发现自己正在操作具有RBAC权限和受限准入规则（如运行非root容器）的集群。对于集群管理员来说，加固集群及其操作环境（如网络、节点和云资源）的更广泛主题是一个漫长的过程，其中许多考虑因素都特定于您选择的精确平台。
- en: I recommend reading up-to-date hardening information with a search for “Kubernetes
    Hardening Guide.” Since so much depends on your specific operating environment,
    a good starting point is to read the hardening guide for your specific platform,
    such as *Harden your cluster’s security*[⁸](#pgfId-1123328) from GKE. The security
    space is constantly evolving, so be sure to stay up to date with the latest best
    practices from authoritative sources.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议阅读最新的加固信息，并搜索“Kubernetes加固指南”。由于许多内容取决于您的特定操作系统环境，一个好的起点是阅读您特定平台的加固指南，例如来自GKE的*加固您的集群安全*[⁸](#pgfId-1123328)。安全领域不断演变，因此请确保通过权威来源了解最新的最佳实践。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: It’s important to keep your cluster and its nodes up to date to mitigate against
    security vulnerabilities.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持您的集群及其节点更新对于减轻安全漏洞至关重要。
- en: Docker base images also introduce their own attack surface area, requiring monitoring
    and updating of deployed containers, which a CI/CD system can help with.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker基本镜像也引入了他们自己的攻击面，需要监控和更新已部署的容器，CI/CD系统可以帮助解决这个问题。
- en: Using the smallest possible base image can help to reduce this surface area,
    decreasing the frequency of application updates to mitigate security vulnerabilities.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用尽可能小的基本镜像可以帮助减少攻击面，降低应用程序更新的频率，以减轻安全漏洞。
- en: DaemonSets can be used to run a Pod on every node and are commonly used to configure
    logging, monitoring, and security software in the cluster.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSets可用于在每个节点上运行Pod，通常用于在集群中配置日志记录、监控和安全软件。
- en: The Pod security context is how Pods are configured to have elevated or restricted
    permissions.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod安全上下文是配置Pod以具有提升或受限权限的方式。
- en: Admission controllers can be used to make changes to Kubernetes objects as they
    are created and enforce requirements, including around the Pod security context.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准入控制器可用于在创建Kubernetes对象时进行更改并强制执行要求，包括Pod安全上下文。
- en: Kubernetes ships with an admission controller named Pod Security admission to
    enable you to enforce security profiles, like Baseline, for mitigating most known
    attacks, and Restricted, for enforcing security best practices on Pods.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes附带了一个名为Pod Security admission的准入控制器，允许您强制执行安全配置文件，如基线，以减轻大多数已知攻击，以及限制，以在Pod上强制执行安全最佳实践。
- en: RBAC is a role-based permission system that allows users with the cluster administrator
    role to grant fine-grained access to developers in the system, like restricting
    a team to a particular namespace.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBAC是一种基于角色的权限系统，允许具有集群管理员角色的用户向系统中的开发者授予细粒度的访问权限，例如限制一个团队访问特定的命名空间。
- en: By default, Pods can communicate with all Pods in the cluster. Network Policies
    can be used to control network access to Pods.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，Pod可以与集群中的所有Pod进行通信。可以使用网络策略来控制Pod的网络访问。
- en: To offer another layer of isolation, especially if you are running untrusted
    code in the cluster, apply a RuntimeClass like gVisor.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了提供另一层隔离，尤其是如果您在集群中运行不受信任的代码，请应用一个如gVisor的RuntimeClass。
- en: Review your platform’s Kubernetes hardening guide for comprehensive and platform-specific
    security considerations.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请查阅您平台上的Kubernetes加固指南，以获取全面和针对特定平台的安全考虑。
- en: '* * *'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) [https://hub.docker.com/_/python](https://hub.docker.com/_/python)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ^（1.）[https://hub.docker.com/_/python](https://hub.docker.com/_/python)
- en: ^(2.) [https://github.com/GoogleContainerTools/distroless](https://github.com/GoogleContainerTools/distroless)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ^（2.）[https://github.com/GoogleContainerTools/distroless](https://github.com/GoogleContainerTools/distroless)
- en: ^(3.) [https://github.com/moby/moby/issues/2259](https://github.com/moby/moby/issues/2259)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^（3.）[https://github.com/moby/moby/issues/2259](https://github.com/moby/moby/issues/2259)
- en: ^(4.) [https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ^（4.）[https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/)
- en: ^(5.) [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ^（5.）[https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- en: ^(6.) [https://kubernetes.io/docs/concepts/containers/runtime-class/](https://kubernetes.io/docs/concepts/containers/runtime-class/)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^（6.）[https://kubernetes.io/docs/concepts/containers/runtime-class/](https://kubernetes.io/docs/concepts/containers/runtime-class/)
- en: ^(7.) [https://gvisor.dev/](https://gvisor.dev/)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^（7.）[https://gvisor.dev/](https://gvisor.dev/)
- en: ^(8.) [https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster](https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^（8.）[https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster](https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster)
