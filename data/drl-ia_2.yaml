- en: Appendix. Mathematics, deep learning, PyTorch
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录。数学、深度学习、PyTorch
- en: This appendix offers a rapid review of deep learning, the relevant mathematics
    we use in this book, and how to implement deep learning models in PyTorch. We’ll
    cover these topics by demonstrating how to implement a deep learning model in
    PyTorch to classify images of handwritten digits from the famous MNIST dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录提供了对深度学习、本书中使用的相关数学以及如何在PyTorch中实现深度学习模型的快速回顾。我们将通过演示如何在PyTorch中实现一个深度学习模型来对著名的手写数字MNIST数据集的图像进行分类来涵盖这些主题。
- en: '*Deep learning algorithms*, which are also called *artificial neural networks*,
    are relatively simple mathematical functions and mostly just require an understanding
    of vectors and matrices. Training a neural network, however, requires an understanding
    of the basics of calculus, namely the derivative. The fundamentals of applied
    deep learning therefore require only knowing how to multiply vectors and matrices
    and take the derivative of multivariable functions, which we’ll review here. *Theoretical
    machine learning* refers to the field that rigorously studies the properties and
    behavior of machine learning algorithms and yields new approaches and algorithms.
    Theoretical machine learning involves advanced graduate-level mathematics that
    covers a wide variety of mathematical disciplines that are outside the scope of
    this book. In this book we only utilize informal mathematics in order to achieve
    our practical aims, not rigorous proof-based mathematics.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习算法*，也称为*人工神经网络*，是相对简单的数学函数，主要只需要理解向量和矩阵。然而，训练一个神经网络却需要理解微积分的基础，即导数。因此，应用深度学习的基础知识只需要知道如何乘以向量和矩阵，以及如何求多元函数的导数，这些内容我们将在下面进行回顾。*理论机器学习*是指严格研究机器学习算法的性质和行为，并产生新的方法和算法的领域。理论机器学习涉及高级研究生水平的数学，涵盖了广泛的各种数学学科，这些学科超出了本书的范围。在这本书中，我们只利用非正式的数学来达到我们的实际目标，而不是基于严格证明的数学。'
- en: A.1\. Linear algebra
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.1\. 线性代数
- en: '*Linear algebra* is the study of linear transformations. A *linear transformation*
    is a transformation (e.g., a function) in which the sum of the transformation
    of two inputs separately, such as *T*(*a*) and *T*(*b*), is the same as summing
    the two inputs and transforming them together, i.e., *T*(*a + b*) = *T*(*a*) +
    *T*(*b*). A linear transformation also has the property that *T*(*a* × *b*) =
    *a* × *T*(*b*). Linear transformations are said to preserve the operations of
    addition and multiplication since you can apply these operations either before
    or after the linear transformation and the result is the same.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性代数*是线性变换的研究。一个*线性变换*是一种变换（例如，一个函数），其中两个输入分别变换的和，例如 *T*(*a*) 和 *T*(*b*)，与将两个输入相加后再一起变换的和相同，即
    *T*(*a + b*) = *T*(*a*) + *T*(*b*)。线性变换还具有这样的性质：*T*(*a* × *b*) = *a* × *T*(*b*)。由于你可以在线性变换之前或之后应用这些操作，并且结果相同，因此线性变换被称为保持加法和乘法运算。'
- en: One informal way to think of this is that linear transformations do not have
    “economies of scale.” For example, think of a linear transformation as converting
    money as the input into some other resource, like gold, so that *T*($100) = 1
    *unit of gold*. The unit price of gold will be constant no matter how much money
    you put in. In contrast, nonlinear transformations might give you a “bulk discount,”
    so that if you buy 1,000 units of gold or more, the price would be less on a per
    unit basis than if you bought less than 1,000 units.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非正式的想法是，线性变换没有“规模经济”。例如，将线性变换想象成将货币作为输入转换为其他资源，比如黄金，这样 *T*($100) = 1 *单位黄金*。无论你投入多少货币，黄金的单位价格都将保持不变。相比之下，非线性变换可能会给你“批量折扣”，即如果你购买1000单位或更多的黄金，每单位的价格将低于购买少于1000单位的情况。
- en: Another way to think of linear transformations is to make a connection to calculus
    (which we’ll review in more detail shortly). A function or transformation takes
    some input value, *x*, and maps it to some output value, *y*. A particular output
    *y* may be a larger or smaller value than the input *x*, or more generally a *neighborhood*
    around an input *x* will be mapped to a larger or smaller neighborhood around
    the output *y*. Here a *neighborhood* refers to the set of points arbitrarily
    close to *x* or *y.* For a single-variable function like *f*(*x*) = 2*x* + 1,
    a neighborhood is actually an interval. For example, the neighborhood around an
    input point *x* = 2 would be all the points arbitrarily close to 2, such as 2.000001
    and 1.99999999.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考线性变换的方法是将其与微积分联系起来（我们将在稍后更详细地回顾）。一个函数或变换接受一些输入值*x*，并将其映射到某个输出值*y*。特定的输出*y*可能比输入*x*大或小，或者更一般地说，输入*x*周围的*邻域*将被映射到输出*y*周围的更大或更小的*邻域*。在这里，*邻域*指的是任意接近*x*或*y*的点的集合。对于一个单变量函数如*f*(*x*)
    = 2*x* + 1，邻域实际上是一个区间。例如，输入点*x* = 2周围的邻域将包括所有任意接近2的点，如2.000001和1.99999999。
- en: One way to think of the derivative of a function at a point is as the ratio
    of the size of the output interval around that point to the size of the input
    interval around the input point. Linear transformations will always have some
    constant ratio of output to input intervals for all points, whereas nonlinear
    transformations will have a varying ratio.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个点上函数的导数可以看作是围绕该点的输出区间的长度与围绕输入点的输入区间长度的比率。线性变换对于所有点都将始终有一个输出到输入区间的恒定比率，而非线性变换将有一个变化的比率。
- en: Linear transformations are often represented as *matrices*, which are rectangular
    grids of numbers. Matrices encode the coefficients for multivariable linear functions,
    such as
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换通常表示为*矩阵*，即数字的矩形网格。矩阵编码了多元线性函数的系数，例如
- en: '| *f^x*(*x*,*y*) = *Ax* + *By* *f^y*(*x*,*y*) = *Cx* + *Dy* |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| *f^x*(*x*,*y*) = *Ax* + *By* *f^y*(*x*,*y*) = *Cx* + *Dy* |'
- en: 'While this appears to be two functions, this is really a single function that
    maps a 2-dimensional point (*x*,*y*) to a new 2-dimensional point (*x*′,*y*′)
    using the coefficients *A*,*B*,*C*,*D*. To find *x*, you use the *f^x* function,
    and to find *y*′ you use the *f^y* function. We could have written this as a single
    line:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来像是两个函数，但实际上这是一个将二维点(*x*,*y*)映射到新的二维点(*x*′,*y*′)的单个函数，使用系数*A*，*B*，*C*，*D*。要找到*x*，你使用*f^x*函数，要找到*y*′，你使用*f^y*函数。我们也可以将其写为单行：
- en: '| *f*(*x*,*y*) = (*Ax* + *By*, *Cx* + *Dy*) |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| *f*(*x*,*y*) = (*Ax* + *By*, *Cx* + *Dy*) |'
- en: This makes it more clear that the output is a 2-tuple or 2-dimensional vector.
    In any case, it is useful to think of this function in two separate pieces since
    the computations for the *x* and *y* components are independent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得输出是一个2元组或二维向量的概念更加清晰。无论如何，将这个函数视为两个独立的部分是有用的，因为*x*和*y*分量的计算是独立的。
- en: While the mathematical notion of a vector is very general and abstract, for
    machine learning a vector is just a 1-dimensional array of numbers. This linear
    transformation takes a 2-vector (one that has 2 elements) and turns it into another
    2-vector, and to do this it requires four separate pieces of data, the four coefficients.
    There is a difference between a linear transformation like *Ax* + *By* and something
    like *Ax* + *By + C* which adds a constant; the latter is called an *affine* transformation.
    In practice, we use affine transformations in machine learning, but for this discussion
    we will stick with just linear transformations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然向量的数学概念非常通用和抽象，但在机器学习中，向量只是一个数字的一维数组。这个线性变换将一个2向量（具有2个元素）转换成另一个2向量，为此它需要四条独立的数据，即四个系数。*Ax*
    + *By*这样的线性变换与*Ax* + *By + C*这样的东西不同，后者添加了一个常数；后者称为*仿射变换*。在实践中，我们在机器学习中使用仿射变换，但在这个讨论中我们将坚持只讨论线性变换。
- en: 'Matrices are a convenient way to store these coefficients. We can package the
    data into a 2 by 2 matrix:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵是存储这些系数的便捷方式。我们可以将这些数据打包成一个2x2的矩阵：
- en: '![](pg338-1.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](pg338-1.jpg)'
- en: The linear transformation is now represented completely by this matrix, assuming
    you understand how to use it, which we shall cover. We can apply this linear transformation
    by juxtaposing the matrix with a vector, e.g., *Fx*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换现在完全由这个矩阵表示，假设你理解如何使用它，我们将讨论这一点。我们可以通过将矩阵与向量并列来应用这个线性变换，例如，*Fx*。
- en: '![](pg338-2.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](pg338-2.jpg)'
- en: We compute the result of this transformation by multiplying each row in *F*
    with each column (only one here) of *x*. If you do this, you get the same result
    as the explicit function definition above. Matrices do not need to be square,
    they can be any rectangular shape.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将矩阵 *F* 的每一行与矩阵 *x* 的每一列（这里只有一列）相乘来计算这个变换的结果。如果你这样做，你会得到上面显式函数定义的相同结果。矩阵不必是方阵，可以是任何矩形形状。
- en: 'We can graphically represent matrices as boxes with two strings coming out
    on each end with labeled indices:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将矩阵图形化表示为两端各伸出两条带有标记索引的字符串的盒子：
- en: '![](pg338-3.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg338-3.jpg)'
- en: We call this a string diagram. The *n* represents the dimensionality of the
    input vector and the *m* is the dimensionality of the output vector. You can imagine
    a vector flowing into the linear transformation from the left, and a new vector
    is produced on the right side. For the practical deep learning we use in this
    book, you only need to understand this much linear algebra, i.e., the principles
    of multiplying vectors by matrices. Any additional math will be introduced in
    the respective chapters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这为弦图。*n* 代表输入向量的维度，而 *m* 是输出向量的维度。你可以想象一个向量从左侧流入线性变换，并在右侧产生一个新的向量。对于本书中使用的实际深度学习，你只需要理解这么多线性代数，即通过矩阵乘以向量的原则。任何额外的数学将在相应的章节中介绍。
- en: A.2\. Calculus
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.2\. 微积分
- en: '*Calculus* is essentially the study of differentiation and integration. In
    deep learning, we only really need to use differentiation. *Differentiation* is
    the process of getting a derivative of a function.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*微积分* 实质上是微分和积分的研究。在深度学习中，我们实际上只需要使用微分。*微分* 是获取函数导数的过程。'
- en: 'We already introduced one notion of derivative: the ratio of an output interval
    to the input interval. It tells you how much the output space is stretched or
    squished. Importantly, these intervals are oriented intervals so they can be negative
    or positive, and thus the ratio can be negative or positive.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了一个导数的概念：输出区间与输入区间的比率。它告诉你输出空间被拉伸或压缩了多少。重要的是，这些区间是有方向的区间，因此可以是负数或正数，因此比率可以是负数或正数。
- en: For example, consider the function *f*(*x*) = *x*². Take a point *x* and its
    neighborhood (*x –* ε*,x +* ε), where ε is some arbitrarily small value, and we
    get an interval around *x*. To be concrete, let *x* = 3, *ε* = 0.1; the interval
    around *x* = 3 is (2.9,3.1). The size (and orientation) of this interval is 3.1
    – 2.9 = +0.2, and this interval gets mapped to *f*(2.9) = 8.41 and *f*(3.1) =
    9.61\. This output interval is (8.41,9.61) and its size is 9.61 – 8.41 = 1.2\.
    As you can see, the output interval is still positive, so the ratio ![](pg339-0.jpg),
    which is the derivative of the function *f* at *x* = 3.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑函数 *f*(*x*) = *x*²。取一个点 *x* 和其邻域 (*x –* ε*,x +* ε)，其中 ε 是某个任意小的值，我们得到一个围绕
    *x* 的区间。具体来说，让 *x* = 3，*ε* = 0.1；围绕 *x* = 3 的区间是 (2.9,3.1)。这个区间的长度（和方向）是 3.1 –
    2.9 = +0.2，这个区间被映射到 *f*(2.9) = 8.41 和 *f*(3.1) = 9.61。这个输出区间是 (8.41,9.61) 并且其长度是
    9.61 – 8.41 = 1.2。正如你所看到的，输出区间仍然是正的，所以比率 ![](pg339-0.jpg)，这是函数 *f* 在 *x* = 3 处的导数。
- en: We denote the derivative of a function, *f*, with respect to an input variable,
    *x*, as *df*/*dx*, but this is not to be thought of as a literal fraction; it’s
    just a notation. We don’t need to take an interval on both sides of the point;
    an interval on one side will do as long as it’s small, i.e., we can define an
    interval as (*x*,*x* + ε) and the size of the interval is just ε, whereas the
    size of the output interval is *f*(*x* + ε) – *f*(*x*).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 *df*/*dx* 表示函数 *f* 关于输入变量 *x* 的导数，但这并不是要将其视为一个实际的分数；它只是一个符号。我们不需要在点的两侧都取一个区间；只要区间足够小，一侧的区间就可以，即我们可以定义一个区间为
    (*x*,*x* + ε)，区间的长度只是 ε，而输出区间的长度是 *f*(*x* + ε) – *f*(*x*)。
- en: Using concrete values like we did only yields approximations in general; to
    get absolutes we’d need to use infinitely small intervals. We can do this symbolically
    by imagining that ε is an infinitely small number such that it is bigger than
    0 but smaller than any other number in our number system. Now differentiation
    becomes an algebra problem.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 只使用像我们之前那样具体的值通常只能得到近似值；要得到绝对值，我们需要使用无限小的区间。我们可以通过想象 ε 是一个无限小的数，它大于 0 但小于我们数系中的任何其他数来符号化地做到这一点。现在微分变成了一个代数问题。
- en: '![](pg339-1_alt.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg339-1_alt.jpg)'
- en: '![](pg339-2.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg339-2.jpg)'
- en: '![](pg339-3.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg339-3.jpg)'
- en: '![](pg339-4.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg339-4.jpg)'
- en: '![](pg339-5.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg339-5.jpg)'
- en: '![](pg339-6.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg339-6.jpg)'
- en: '![](pg339-7.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg339-7.jpg)'
- en: Here we simply take the ratio of the output interval to the input interval,
    both of which are infinitely small because ε is an infinitesimal number. We can
    algebraically reduce the expression to 2*x* + ε, and since ε is infinitesimal,
    2*x* + ε is infinitely close to 2*x*, which we take as the true derivative of
    the original function *f*(*x*) = *x*². Remember, we’re taking ratios of oriented
    intervals that can be positive or negative. We not only want to know how much
    a function stretches (or squeezes) the input, but whether it changes the direction
    of the interval. There is a lot of advanced mathematics justifying all of this
    (see nonstandard analysis or smooth infinitesimal analysis) but this process works
    just fine for practical purposes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简单地取输出区间与输入区间的比率，这两个区间都是无穷小，因为ε是一个无穷小的数。我们可以将表达式代数地简化为2*x* + ε，由于ε是无穷小的，2*x*
    + ε无限接近于2*x*，我们将其视为原始函数*f*(*x*) = *x*²的真实导数。记住，我们正在取有方向的区间的比率，这些区间可以是正的或负的。我们不仅想知道函数如何拉伸（或压缩）输入，而且还想知道它是否改变了区间的方向。有许多高级数学理论来证明所有这些（参见非标准分析或光滑无穷小分析），但这个过程对于实际目的来说完全适用。
- en: Why is differentiation a useful concept in deep learning? Well, in machine learning
    we are trying to *optimize* a function, which means finding the input points to
    the function such that the output of the function is a maximum or minimum over
    all possible inputs. That is, given some function, *f*(*x*), we want to find an
    *x* such that *f*(*x*) is smaller than any other choice of *x*; we generally denote
    this as *argmin*(*f*(*x*)). Usually we have a loss function (or cost or error
    function) that takes some input vector, a target vector, and a parameter vector
    and returns the degree of error between the predicted output and the true output,
    and our goal is to find the set of parameters that minimizes this error function.
    There are many possible ways to minimize this function, not all of which depend
    on using derivatives, but in most cases the most effective and efficient way to
    optimize loss functions in machine learning is to use derivative information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么微分在深度学习中是一个有用的概念？嗯，在机器学习中，我们试图**优化**一个函数，这意味着找到函数的输入点，使得函数的输出在所有可能的输入中达到最大值或最小值。也就是说，给定某个函数，*f*(*x*)，我们希望找到一个*x*，使得*f*(*x*)小于任何其他*x*的选择；我们通常用*argmin*(*f*(*x*))来表示这个。通常我们有一个损失函数（或成本或误差函数），它接受一些输入向量、目标向量和参数向量，并返回预测输出和真实输出之间的误差程度，我们的目标是找到使这个误差函数最小化的参数集。有许多可能的方法可以最小化这个函数，但并非所有方法都依赖于使用导数，但在大多数情况下，在机器学习中优化损失函数最有效和最有效的方法是使用导数信息。
- en: Since deep learning models are nonlinear (i.e., they do not preserve addition
    and scalar multiplication), the derivatives are not constant like in linear transformations.
    The amount and direction of squishing or stretching that happens from input to
    output points varies from point to point. In another sense, it tells us which
    direction the function is curving, so we can follow the curve downward to the
    lowest point. Multivariable functions like deep learning models don’t just have
    a single derivative but a set of partial derivatives that describe the curvature
    of the function with respect to each individual input component. This way we can
    figure out which sets of parameters for a deep neural network lead to the smallest
    error.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习模型是非线性的（即，它们不保持加法和标量乘法），导数不像在线性变换中那样是常数。从输入到输出点的压缩或拉伸的量和方向各不相同。从另一个角度来看，它告诉我们函数的哪个方向是弯曲的，因此我们可以沿着曲线向下到最低点。深度学习模型这样的多元函数不仅仅有一个导数，而是一组偏导数，这些偏导数描述了函数相对于每个单独输入组件的曲率。这样我们就可以找出哪些深度神经网络的参数集会导致最小的误差。
- en: 'The simplest example of using derivative information to minimize a function
    is to see how it works for a simple compositional function. The function we will
    try to find the minimum of is:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用导数信息最小化函数的最简单例子是看看它对一个简单的组合函数是如何工作的。我们将尝试找到最小值的函数是：
- en: '| *f*(*x*) = log(*x*⁴ + *x*³ + 2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| *f*(*x*) = log(*x*⁴ + *x*³ + 2) |'
- en: The graph is shown in [figure A.1](#app01fig01). You can see that the minimum
    of this function appears to be around –1\. This is a compositional function because
    it contains a polynomial expression “wrapped” in a logarithm, so we need to use
    the chain rule from calculus to compute the derivative. We want the derivative
    of this function with respect to *x*. This function only has one “valley,” so
    it will only have one minimum; however, deep learning models are high-dimensional
    and highly compositional and tend to have many minima. Ideally, we’d like to find
    the global minimum that is the lowest point in the function. Global or local minima
    are points on the function where the slope (i.e., the derivative) at those points
    is 0\. For some functions, like this simple example, we can compute the minimum
    analytically, using algebra. Deep learning models are generally too complex for
    algebraic calculations, and we must use iterative techniques.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图形显示在[图A.1](#app01fig01)。你可以看到这个函数的极小值似乎在-1左右。这是一个组合函数，因为它包含一个多项式表达式“包裹”在对数中，因此我们需要使用微积分中的链式法则来计算导数。我们想要这个函数相对于
    *x* 的导数。这个函数只有一个“山谷”，所以它只有一个极小值；然而，深度学习模型是高维和高度组合的，往往有多个极小值。理想情况下，我们希望找到全局最小值，即函数中的最低点。全局或局部极小值是函数上的点，在这些点上，斜率（即导数）为0。对于某些函数，如这个简单例子，我们可以通过代数方法计算极小值。深度学习模型通常过于复杂，无法进行代数计算，我们必须使用迭代技术。
- en: Figure A.1\. The output of a simple compositional function, *f(x)* = log(*x*⁴
    + *x*³ + 2)
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.1\. 简单组合函数的输出，*f(x)* = log(*x*⁴ + *x*³ + 2)
- en: '![](afig01_alt.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](afig01_alt.jpg)'
- en: 'The chain rule in calculus gives us a way of computing derivatives of compositional
    functions by decomposing them into pieces. If you’ve heard of *backpropagation*,
    it’s basically just the chain rule applied to neural networks with some tricks
    to make it more efficient. For our example case, let’s rewrite the previous function
    as two functions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分中的链式法则为我们提供了一种通过分解它们来计算组合函数导数的方法。如果你听说过*反向传播*，它基本上就是链式法则应用于神经网络，并添加了一些技巧以提高其效率。对于我们的示例情况，让我们将之前的函数重写为两个函数：
- en: '| *h*(*x*) = *x*⁴ + *x*³ + 2 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| *h*(*x*) = *x*⁴ + *x*³ + 2 |'
- en: '| *f*(*x*) = log(*h*(*x*)) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| *f*(*x*) = log(*h*(*x*)) |'
- en: We first compute the derivative of the “outer” function, which is *f*(*x*) =
    log(*h*(*x*)), but this just gives us *df*/*dh* and what we really want is *df*/*dx*.
    You may have learned that the derivative of natural-log is
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算“外层”函数的导数，即 *f*(*x*) = log(*h*(*x*))，但这只给我们 *df*/*dh*，而我们真正想要的是 *df*/*dx*。你可能已经学过自然对数的导数
- en: '![](pg341-2.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](pg341-2.jpg)'
- en: And the derivative of the inner function *h*(*x*) is
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 内层函数 *h*(*x*) 的导数是
- en: '![](pg341-3.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](pg341-3.jpg)'
- en: To get the full derivative of the compositional function, we notice that
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到组合函数的完整导数，我们注意到
- en: '![](pg341-4.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](pg341-4.jpg)'
- en: That is, the derivative we want, *df*/*dx*, is obtained by multiplying the derivative
    of the outer function with respect to its input and the inner function (the polynomial)
    with respect to *x*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们想要的导数 *df*/*dx* 是通过将外层函数相对于其输入的导数与内层函数（多项式）相对于 *x* 的导数相乘得到的。
- en: '![](pg342-1.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](pg342-1.jpg)'
- en: 'You can set this derivative to 0 to calculate the minima algebraically: 4*x*²
    + 3*x* = 0\. This function has two minima at *x* = 0 and *x* = –3/4 = –0.75\.
    But only *x* = –0.75 is the global minimum since *f*(–0.75) = 0.638971 whereas
    *f*(0) = 0.693147, which is slightly larger.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个导数设为0，通过代数方法计算极小值：4*x*² + 3*x* = 0。这个函数在 *x* = 0 和 *x* = –3/4 = –0.75
    处有两个极小值。但只有 *x* = –0.75 是全局最小值，因为 *f*(–0.75) = 0.638971，而 *f*(0) = 0.693147，后者略大。
- en: Let’s see how we can solve this using *gradient descent*, which is an iterative
    algorithm to find the minima of a function. The idea is we start with a random
    *x* as a starting point. We then compute the derivative of the function at this
    point, which tells us the magnitude and direction of curvature at this point.
    We then choose a new *x* point based on the old *x* point, its derivative, and
    a step-size parameter to control how fast we move. That is,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用*梯度下降*来解决这个问题，这是一种迭代算法，用于寻找函数的极小值。其思想是，我们从一个随机的 *x* 作为起始点开始。然后我们计算这个点的函数导数，这告诉我们这个点的曲率的大小和方向。然后我们根据旧的
    *x* 点、其导数和一个步长参数来选择一个新的 *x* 点，以控制我们移动的速度。也就是说，
- en: '![](pg342-2.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](pg342-2.jpg)'
- en: Let’s see how to do this in code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在代码中实现这一点。
- en: Listing A.1\. Gradient descent
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.1\. 梯度下降
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1*** The original function'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 原始函数'
- en: '***2*** The derivative function'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 导数函数'
- en: '***3*** Random starting point'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 随机起始点'
- en: '***4*** Learning rate (step size)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 学习率（步长）'
- en: '***5*** Number of iterations to optimize over'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 优化迭代的次数'
- en: '***6*** Calculates derivative of current point'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 计算当前点的导数'
- en: '***7*** Updates current point'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 更新当前点'
- en: If you run this gradient descent algorithm, you should get *x* = –0.750000000882165,
    which is (if rounded) exactly what you get when calculated algebraically. This
    simple process is the same one we use when training deep neural networks, except
    that deep neural networks are multivariable compositional functions, so we use
    partial derivatives. A partial derivative is no more complex than a normal derivative.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个梯度下降算法，你应该得到 *x* = –0.750000000882165，这（如果四舍五入）正是你通过代数计算得到的结果。这个简单的过程就是我们训练深度神经网络时所使用的方法，只不过深度神经网络是多变量组合函数，所以我们使用偏导数。偏导数并不比普通导数复杂。
- en: Consider the multivariable function *f*(*x*,*y*) = *x*⁴ + *y*². There is no
    longer a single derivative of this function since it has two input variables.
    We can take the derivative with respect to *x* or *y* or both. When we take the
    derivative of a multivariable function with respect to all of its inputs and package
    this into a vector, we call it the *gradient*, which is denoted by the nabla symbol
    ∇, i.e., ∇*f*(*x*) = [*df*/*dx*,*df*/*dy*]. To compute the partial derivative
    of *f* with respect to *x*, i.e., *df*/*dx*, we simply set the other variable
    *y* to be a constant and differentiate as usual. In this case, *df*/*dx =* 4*x*³
    and *df*/*dy =* 2*y*. So the gradient ∇*f*(*x*) = [4*x*³,2*y*], which is the vector
    of partial derivatives. Then we can run gradient descent as usual, except now
    we find the vector associated with the lowest point in an error function of the
    deep neural network.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑多变量函数 *f*(*x*,*y*) = *x*⁴ + *y*²。由于它有两个输入变量，这个函数不再只有一个导数。我们可以对 *x* 或 *y* 或两者同时求导。当我们对多变量函数的所有输入求导并将这些导数打包成一个向量时，我们称之为梯度，用
    nabla 符号 ∇ 表示，即 ∇*f*(*x*) = [*df*/*dx*,*df*/*dy*]。要计算 *f* 对 *x* 的偏导数，即 *df*/*dx*，我们只需将另一个变量
    *y* 设为常数，然后像往常一样求导。在这种情况下，*df*/*dx =* 4*x*³ 和 *df*/*dy =* 2*y*。所以梯度 ∇*f*(*x*)
    = [4*x*³,2*y*]，这是偏导数的向量。然后我们可以像往常一样运行梯度下降，但现在我们找到与深度神经网络错误函数最低点相关的向量。
- en: A.3\. Deep learning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.3\. 深度学习
- en: A deep neural network is simply a composition of multiple layers of simpler
    functions called *layers*. Each layer function consists of a matrix multiplication
    followed by a nonlinear *activation function*. The most common activation function
    is *f*(*x*) = max(0,*x*), which returns 0 if *x* is negative or returns *x* otherwise.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络简单地说就是由多个称为 *layers* 的简单函数组成的组合。每个层函数由矩阵乘法后跟一个非线性 *激活函数* 组成。最常用的激活函数是
    *f*(*x*) = max(0,*x*)，如果 *x* 为负则返回0，否则返回 *x*。
- en: A simple neural network might be
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的神经网络可能如下所示
- en: '![](pg343-1.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](pg343-1.jpg)'
- en: Read this diagram from left to right as if data flows in from the left into
    the L1 function then the L2 function and becomes the output on the right. The
    symbols *k*, *m*, and *n* refer to the dimensionality of the vectors. A *k*-length
    vector is input to function L1, which produces an *m*-length vector that then
    gets passed to L2, which finally produces an *n*-dimensional vector.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右阅读这个图，就像数据从左边流入L1函数然后是L2函数，最后在右边输出。符号 *k*，*m*，和 *n* 指的是向量的维度。一个 *k*-长度的向量输入到函数L1，它产生一个
    *m*-长度的向量，然后传递给L2，最终产生一个 *n*-维度的向量。
- en: Now let’s look at what each of these L functions are doing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看每个L函数都在做什么。
- en: '![](pg343-2.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](pg343-2.jpg)'
- en: 'A neural network layer, generically, consists of two parts: a matrix multiplication
    and an activation function. An *n*-length vector comes in from the left and gets
    multiplied by a matrix (often called a parameter or weight matrix), which may
    change the dimensionality of the resulting output vector. The output vector, now
    of length *m*, gets passed through a nonlinear activation function, which does
    not change the dimensionality of the vector.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络的层，一般由两部分组成：矩阵乘法和激活函数。一个 *n*-长度的向量从左边进入，并与一个矩阵（通常称为参数或权重矩阵）相乘，这可能会改变输出向量的维度。输出向量，现在长度为
    *m*，通过一个非线性激活函数，这个函数不会改变向量的维度。
- en: A deep neural network just stacks these layers together, and we train it by
    applying gradient descent on the weight matrices, which are the parameters of
    the neural network. Here’s a simple 2-layer neural network in Numpy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络只是将这些层堆叠在一起，我们通过在权重矩阵上应用梯度下降来训练它，这些权重矩阵是神经网络的参数。以下是一个简单的2层神经网络，使用Numpy实现。
- en: Listing A.2\. A simple neural network
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.2\. 一个简单的神经网络
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1*** Matrix multiplication'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 矩阵乘法'
- en: '***2*** Nonlinear activation function'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 非线性激活函数'
- en: '***3*** Weight (parameter) matrix, initialized randomly'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 权重（参数）矩阵，随机初始化'
- en: '***4*** Random input vector'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 随机输入向量'
- en: In the next section you’ll learn how to use the PyTorch library to automatically
    compute gradients to easily train neural networks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将学习如何使用 PyTorch 库自动计算梯度以轻松训练神经网络。
- en: A.4\. PyTorch
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.4\. PyTorch
- en: In the previous sections you learned how to use gradient descent to find the
    minimum of a function, but to do that we needed the gradient. For our simple example,
    we could compute the gradient with paper and pencil. For deep learning models,
    that is impractical, so we rely on libraries like PyTorch that provide *automatic
    differentiation* capabilities that make it much easier.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学习了如何使用梯度下降来找到函数的最小值，但要做到这一点，我们需要梯度。在我们的简单例子中，我们可以用纸和笔来计算梯度。对于深度学习模型来说，这是不切实际的，所以我们依赖于像
    PyTorch 这样的库，它提供了 *自动微分* 功能，这使得计算梯度变得容易得多。
- en: The basic idea is that in PyTorch we create a *computational graph,* similar
    to the diagrams we used in the previous section, where relations between inputs,
    outputs, and connections between different functions are made explicit and kept
    track of so we can easily apply the chain rule automatically to compute gradients.
    Fortunately, switching from numpy to PyTorch is simple, and most of the time we
    can just replace `numpy` with `torch`. Let’s translate our neural network from
    above into PyTorch.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是，在 PyTorch 中，我们创建一个 *计算图*，类似于我们在前面章节中使用的图表，其中输入、输出以及不同函数之间的连接关系被明确表示并跟踪，这样我们就可以轻松地自动应用链式法则来计算梯度。幸运的是，从
    numpy 切换到 PyTorch 很简单，大多数时候我们只需将 `numpy` 替换为 `torch`。让我们将上面的神经网络翻译成 PyTorch。
- en: Listing A.3\. PyTorch neural network
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.3\. PyTorch 神经网络
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** Matrix multiplication'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 矩阵乘法'
- en: '***2*** Nonlinear activation function'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 非线性激活函数'
- en: '***3*** Weight (parameter) matrix, with gradients tracked'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 权重（参数）矩阵，跟踪梯度'
- en: This looks almost identical to the numpy version except that we use `torch.relu`
    instead of `np.maximum`, but they are the same function. We also added a `requires_grad=True`
    parameter to the weight matrix setup. This tells PyTorch that these are trainable
    parameters that we want to track gradients for, whereas `x` is an input, not a
    trainable parameter. We also got rid of the last activation function for reasons
    that will become clear. For this example, we will use the famous MNIST data set
    that contains images of handwritten digits from 0 to 9, such as the one in [figure
    A.2](#app01fig02).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎与 numpy 版本相同，除了我们使用 `torch.relu` 而不是 `np.maximum`，但它们是同一个函数。我们还向权重矩阵设置中添加了
    `requires_grad=True` 参数。这告诉 PyTorch 这些是我们想要跟踪梯度的可训练参数，而 `x` 是一个输入，不是一个可训练参数。我们还去掉了最后一个激活函数，原因将在后面变得清晰。在这个例子中，我们将使用著名的
    MNIST 数据集，它包含从 0 到 9 的手写数字图像，例如 [图 A.2](#app01fig02) 中的那个。
- en: Figure A.2\. An example image from the MNIST dataset of hand-drawn digits.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 A.2\. MNIST 数据集中手绘数字的一个示例图像。
- en: '![](afig02.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![afig02.jpg](afig02.jpg)'
- en: We want to train our neural network to recognize these images and classify them
    as digits 0 through 9\. PyTorch has a related library that lets us easily download
    this data set.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望训练我们的神经网络来识别这些图像并将它们分类为 0 到 9 的数字。PyTorch 有一个相关的库，允许我们轻松下载这个数据集。
- en: Listing A.4\. Classifying MNIST using a neural network
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 A.4\. 使用神经网络对 MNIST 进行分类
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1*** Downloads and loads the MNIST dataset'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 下载并加载 MNIST 数据集'
- en: '***2*** Sets up a loss function'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 设置损失函数'
- en: '***3*** Gets a set of random index values'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 获取一组随机索引值'
- en: '***4*** Subsets the data and flattens the 28 x 28 images into 784 vectors'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 将数据子集并展平 28 x 28 图像为 784 向量'
- en: '***5*** Normalizes the vector to be between 0 and 1'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将向量归一化到 0 到 1 之间'
- en: '***6*** Makes a prediction using the neural network'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 使用神经网络进行预测'
- en: '***7*** Gets the ground-truth image labels'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 获取真实图像标签'
- en: '***8*** Computes the loss'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 计算损失'
- en: '***9*** Backpropagates'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 反向传播'
- en: '***10*** Does not compute gradients in this block'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10*** 在此块中不计算梯度'
- en: '***11*** Gradient descent over the parameter matrices'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11*** 在参数矩阵上进行梯度下降'
- en: You can tell that the neural network is successfully training by observing the
    loss function fairly steadily decreasing over training time ([figure A.3](#app01fig03)).
    This short code snippet trains a complete neural network to successfully classify
    MNIST digits at around 70% accuracy. We just implemented gradient descent exactly
    the same way we did with our simple logarithmic function *f*(*x*) = log(*x*⁴ +
    *x*³ + 2), but PyTorch handled the gradients for us. Since the gradient of the
    neural network’s parameters depends on the input data, each time we run the neural
    network “forward” with a new random sample of images, the gradients will be different.
    So we run the neural network forward with a random sample of data, PyTorch keeps
    track of the computations that occur, and when we’re done, we call the `backward()`
    method on the last output; in this case it is generally the loss. The `backward()`
    method uses automatic differentiation to compute all gradients for all PyTorch
    variables that have `requires_grad=True` set. Then we can update the model parameters
    using gradient descent. We wrap the actual gradient descent part in the `torch.no_grad()`
    context because we don’t want it to keep track of these computations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过观察损失函数在训练时间内的稳步下降来判断神经网络是否成功训练（[图A.3](#app01fig03)）。这段简短的代码片段训练了一个完整的神经网络，以大约70%的准确率成功分类MNIST数字。我们只是以与我们的简单对数函数
    *f*(*x*) = log(*x*⁴ + *x*³ + 2)相同的方式实现了梯度下降，但PyTorch为我们处理了梯度。由于神经网络参数的梯度依赖于输入数据，每次我们用新的随机图像样本运行神经网络“正向”时，梯度都会不同。因此，我们用数据的一个随机样本运行神经网络正向，PyTorch跟踪发生的计算，当我们完成时，我们在最后一个输出上调用`backward()`方法；在这种情况下通常是损失。`backward()`方法使用自动微分来计算所有`requires_grad=True`设置的PyTorch变量的所有梯度。然后我们可以使用梯度下降更新模型参数。我们将实际的梯度下降部分包装在`torch.no_grad()`上下文中，因为我们不希望它跟踪这些计算。
- en: Figure A.3\. The loss function for our neural network trained on the MNIST dataset.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.3\. 在MNIST数据集上训练的神经网络的损失函数。
- en: '![](afig03_alt.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](afig03_alt.jpg)'
- en: We can easily achieve greater than 95% accuracy by improving the training algorithm
    with a more sophisticated version of gradient descent. In listing A.4 we implemented
    our own version of *stochastic gradient descent*, the *stochastic* part because
    we are randomly taking subsets from the dataset and computing gradients based
    on that, which gives us noisy estimates of the true gradient given the full set
    of data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用梯度下降的更复杂版本改进训练算法，我们可以轻松实现超过95%的准确率。在列表A.4中，我们实现了自己的随机梯度下降版本，*随机*部分是因为我们从数据集中随机抽取子集并基于此计算梯度，这给出了给定完整数据集的真实梯度的噪声估计。
- en: PyTorch includes built-in optimizers, of which stochastic gradient descent (SGD)
    is one. The most popular alternative is called Adam, which is a more sophisticated
    version of SGD. We just need to instantiate the optimizer with the model parameters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch包括内置的优化器，其中之一是随机梯度下降（SGD）。最受欢迎的替代方案称为Adam，它是SGD的一个更复杂的版本。我们只需要用模型参数实例化优化器。
- en: Listing A.5\. Using the Adam optimizer
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表A.5\. 使用Adam优化器
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1*** Sets up the loss function'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 设置损失函数'
- en: '***2*** Sets up the ADAM optimizer'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 设置ADAM优化器'
- en: '***3*** Backpropagates'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 反向传播'
- en: '***4*** Updates the parameters'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 更新参数'
- en: '***5*** Resets the gradients'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 重置梯度'
- en: You can see that the loss function in [figure A.4](#app01fig04) is much smoother
    now with the Adam optimizer, and it dramatically increases the accuracy of our
    neural network classifier.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，使用Adam优化器后，[图A.4](#app01fig04)中的损失函数现在要平滑得多，并且它显著提高了我们神经网络分类器的准确率。
- en: Figure A.4\. The loss plot of our neural network trained on MNIST with the built-in
    PyTorch optimizer Adam.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图A.4\. 使用内置的PyTorch优化器Adam在MNIST上训练的神经网络的损失图。
- en: '![](afig04_alt.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](afig04_alt.jpg)'
