- en: 8 Using and operating models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用和操作模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using machine-learning models to produce predictions that benefit real-world
    applications
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习模型生成对现实应用有益的预测
- en: Producing predictions as a batch workflow
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为批量工作流程生成预测
- en: Producing predictions as a real-time application
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为实时应用生成预测
- en: Why do businesses invest in data science applications? “To produce models” isn’t
    an adequate answer, because models are just bundles of data and code with no intrinsic
    value. To produce tangible value, applications must have a positive impact on
    the surrounding world. For instance, a recommendation model is useless in isolation,
    but when connected to a user interface, it can lower customer churn and increase
    long-term revenue. Or a model predicting credit risk becomes valuable when connected
    to a decision-support dashboard used by human decision-makers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么企业会投资于数据科学应用？“为了生成模型”并不是一个充分的答案，因为模型只是数据代码的集合，没有内在价值。为了产生有形价值，应用必须对周围世界产生积极影响。例如，一个推荐模型在孤立状态下是无用的，但连接到用户界面后，它可以降低客户流失并增加长期收入。或者，一个预测信用风险的模型在连接到供人类决策者使用的决策支持仪表板时变得有价值。
- en: In this chapter, we bridge the gap between data science and business applications.
    Although this is the second-to-last chapter of the book, in real-life projects,
    you should start thinking about the connection early on. Figure 8.1 illustrates
    the idea using the spiral diagram introduced in chapter 3.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们弥合了数据科学和商业应用之间的差距。尽管这是本书的第二章倒数第二章，但在实际项目中，你应该尽早开始考虑这种联系。图8.1使用第3章中引入的螺旋图来阐述这一想法。
- en: '![CH08_F01_Tuulos](../../OEBPS/Images/CH08_F01_Tuulos.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F01_Tuulos](../../OEBPS/Images/CH08_F01_Tuulos.png)'
- en: Figure 8.1 Connecting outputs to surrounding systems
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 将输出连接到周围系统
- en: In general, it is a good idea to begin by thoroughly learning about the business
    problem that needs to be solved. After this, you can identify and evaluate data
    assets that you can use to solve the problem. Before writing a line of modeling
    code, you can choose an architectural pattern that allows the results to be connected
    to a value-generating business application, which is the main topic of this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，首先彻底了解需要解决的商业问题是明智的。之后，你可以识别和评估可用于解决问题的数据资产。在编写任何建模代码之前，你可以选择一个架构模式，允许结果连接到一个价值生成业务应用，这是本章的主要内容。
- en: 'Frequently, thinking about inputs and outputs reveals issues, such as the lack
    of suitable data or technical or organizational difficulties in using the results,
    which can be addressed before any models have been built. Once it is clear how
    the application can be deployed in its environment, you can begin the actual modeling
    work. If the project is successful, such as personalized video recommendations
    at Netflix, the modeling work never ends: data scientists keep improving the models
    year after year.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，思考输入和输出会揭示问题，例如缺乏合适的数据或在使用结果时遇到的技术或组织困难，这些问题可以在构建任何模型之前得到解决。一旦明确了应用在其环境中的部署方式，你就可以开始实际建模工作。如果项目成功，例如Netflix的个性化视频推荐，建模工作永远不会结束：数据科学家会年复一年地不断改进模型。
- en: Models can be used in production in countless ways. Companies are increasingly
    eager to apply data science to all aspects of business, resulting in a diverse
    spectrum of requirements. Theoretically, there’s a well-defined way to produce
    predictions given a model. Technically, however, it is quite different from a
    system that produces real-time predictions in a high-speed trading system, say,
    populating a handful of predictions for an internal dashboard.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以用无数种方式在生产中使用。公司越来越渴望将数据科学应用于商业的各个方面，从而产生多样化的需求。理论上，给定一个模型，有一种明确的方式来生成预测。然而，技术上，这与一个在高速交易系统中产生实时预测的系统大不相同，比如，为内部仪表板填充少量预测。
- en: Note that we use the term *prediction* to refer to any output of a data science
    workflow. Strictly speaking, not all data science applications produce predictions—they
    can produce classifications, categorizations, inferences, and other insights as
    well. For brevity, we use predictions as an umbrella term to refer to any such
    outputs. Another commonly used term for activities like this is *model serving*—we
    want to make the model available, that is, serve the model, to other systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用术语*预测*来指代数据科学工作流程的任何输出。严格来说，并非所有数据科学应用都会产生预测——它们还可以产生分类、分类、推理和其他见解。为了简洁起见，我们使用预测作为一个总称，来指代任何此类输出。对于此类活动，另一个常用的术语是*模型服务*——我们希望使模型可供其他系统使用，即提供模型服务。
- en: 'We begin the chapter by describing a number of common architectural patterns
    for using models to produce predictions. You can then choose and apply the most
    appropriate patterns to your use cases. We focus on two common patterns: precomputing
    results in advance, aka *batch predictions*, and doing the same in real-time through
    a web service, aka *real-time predictions*. These techniques are complementary
    to all the lessons we have learned in previous chapters.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的开头，描述了多种常见的架构模式，用于使用模型进行预测。然后，你可以根据你的用例选择并应用最合适的模式。我们重点关注两种常见的模式：预先计算结果，即所谓的*批预测*，以及通过网络服务实时进行相同的操作，即所谓的*实时预测*。这些技术与我们在前几章中学到的所有课程都是互补的。
- en: The latter part of the chapter focuses on *model operations*, namely, how to
    ensure that the model keeps producing correct results over time. We learn about
    patterns for retraining and testing models continuously, as well as monitoring
    the performance of models. Similar to inference, there isn’t a single right approach
    or a single right tool for implementing model operations, but learning about general
    patterns helps you to choose the right tool for each use case.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的后半部分专注于*模型操作*，即如何确保模型在一段时间内持续产生正确的结果。我们学习了持续重新训练和测试模型的模式，以及监控模型性能的模式。与推理类似，没有一种正确的方法或工具来实现模型操作，但了解通用模式有助于你为每个用例选择正确的工具。
- en: Figure 8.2 summarizes these topics on our infrastructure stack.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2总结了我们在基础设施堆栈上的这些主题。
- en: '![CH08_F02_Tuulos](../../OEBPS/Images/CH08_F02_Tuulos.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F02_Tuulos](../../OEBPS/Images/CH08_F02_Tuulos.png)'
- en: Figure 8.2 The infrastructure stack
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 基础设施堆栈
- en: By placing the model operations and architecture high on the stack, we signal
    that these concerns can’t be abstracted away from the data scientists—much less
    so than, say, the compute layer covered in chapter 4\. The architecture of the
    overall data science application, and how it is operated in production, should
    follow the business problem being solved. Understanding how the application works
    end to end, from raw data to business results, allows the data scientist to use
    their domain knowledge and modeling expertise to the fullest. Naturally effective
    data science infrastructure should make it easy enough to apply various patterns
    without having to become a DevOps specialist. You can find all code listings for
    this chapter at [http://mng.bz/8MyB](http://mng.bz/8MyB).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将模型操作和架构放在堆栈的较高位置，我们表明这些关注点不能从数据科学家那里抽象出来——更不用说与第4章中涵盖的计算层相比。整体数据科学应用的架构以及它在生产中的操作，应遵循要解决的问题的业务问题。理解应用程序从原始数据到业务结果的端到端工作方式，使数据科学家能够充分利用他们的领域知识和建模专业知识。自然有效的数据科学基础设施应该足够简单，以至于无需成为DevOps专家就可以应用各种模式。你可以在这个章节中找到所有代码列表，请访问[http://mng.bz/8MyB](http://mng.bz/8MyB)。
- en: 8.1 Producing predictions
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 生成预测
- en: '*Encouraged by the efficiency gains produced by Alex’s models, Caveman Cupcakes
    made a major investment in an industry-first, fully-automated cupcake factory.
    Alex and Bowie have been planning for months how to connect the outputs of Alex’s
    machine learning models to the various control systems that power the facility.
    Some of the systems require real-time predictions, whereas others are optimized
    nightly. Designing a robust and versatile interface between the models and industrial
    automation systems wasn’t trivial, but the effort was worth it! The well-designed
    setup allows Alex to develop and test better versions of the models as before,
    Bowie to observe the health of the production systems with full visibility, and
    Harper to scale the business dramatically.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*受到Alex模型效率提升的鼓舞，Caveman Cupcakes在行业内首次投资了一个全自动蛋糕工厂。Alex和Bowie已经计划了几个月如何将Alex的机器学习模型的输出连接到为设施提供动力的各种控制系统。一些系统需要实时预测，而其他系统则每晚优化。在模型和工业自动化系统之间设计一个强大且通用的接口并不简单，但这项努力是值得的！精心设计的设置允许Alex像以前一样开发和测试模型的更好版本，Bowie可以全面观察生产系统的健康状况，Harper可以大幅扩展业务。*'
- en: '![CH08_F02_UN01_Tuulos](../../OEBPS/Images/CH08_F02_UN01_Tuulos.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F02_UN01_Tuulos](../../OEBPS/Images/CH08_F02_UN01_Tuulos.png)'
- en: Theoretically speaking, producing predictions should be quite straightforward.
    For instance, predicting with a logistic regression model is just a matter of
    computing a dot product and passing the result through a sigmoid function. Besides
    the formula being simple mathematically, the operations are not particularly costly
    computationally.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，生成预测应该是相当直接的。例如，使用逻辑回归模型进行预测只是计算点积并通过sigmoid函数传递结果的问题。除了公式在数学上简单之外，这些操作在计算上并不特别昂贵。
- en: 'Challenges related to using models in real-life are practical, not theoretical.
    You need to consider how to manage the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中使用模型的相关挑战是实际的，而不是理论上的。你需要考虑如何管理以下方面：
- en: '*Scale*—Although it might not take a long time to produce a single prediction,
    producing millions in a short enough time requires more thinking.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*规模*—尽管生成单个预测可能不需要很长时间，但在短时间内生成数百万个预测则需要更多的思考。'
- en: '*Change*—Data changes over time, and models need to be retrained and redeployed.
    Moreover, the application code will likely evolve over time, too.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变化*—数据会随时间变化，模型需要重新训练和重新部署。此外，应用程序代码也可能随时间演变。'
- en: '*Integrations*—To make a real-world impact, predictions need to be used by
    another system that is subject to challenges of scale and change on its own.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成*—要产生实际影响，预测需要被另一个系统使用，而这个系统本身也面临着规模和变化的挑战。'
- en: '*Failures*—The previous three issues are a rich source of failures: systems
    fail under load, change causes predictions to become incorrect over time, and
    integrations are hard to keep stable. Whenever something fails, you need to understand
    what failed, why, and how to fix it quickly.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*失败*—前三个问题都是失败案例的丰富来源：系统在负载下会失败，变化会导致预测随时间变得不准确，集成难以保持稳定。每当有东西失败时，你需要了解它是什么失败了，为什么失败了，以及如何快速修复它。'
- en: 'Addressing any of these four points in isolation is a nontrivial engineering
    challenge by itself. Addressing all of them simultaneously requires significant
    engineering effort. Luckily, only a few data science projects require a perfect
    solution for all these concerns. Consider the following prototypical examples:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 单独解决这四个问题中的任何一个都是一个非平凡的工程挑战。同时解决所有这些问题需要大量的工程努力。幸运的是，只有少数数据科学项目需要为所有这些担忧提供完美的解决方案。考虑以下典型的例子：
- en: A model to balance marketing budgets may require special integrations to marketing
    platforms, but the scale of data and the speed of change may be modest. Failures
    can also be handled manually without causing a major disruption.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于平衡营销预算的模型可能需要特殊的营销平台集成，但数据规模和变化速度可能适中。失败也可以手动处理，而不会造成重大破坏。
- en: A recommendations model may need to handle millions of items for millions of
    users, but if the system fails sporadically, users won’t notice slightly stale
    recommendations.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个推荐模型可能需要处理数百万个用户和数百万个物品，但如果系统偶尔失败，用户可能不会注意到推荐稍微有些过时。
- en: A high-frequency trading system must produce a prediction in less than a millisecond,
    handling hundreds of thousands of predictions every second. Because the model
    is directly responsible for tens of millions of dollars of profit, it is cost
    effective to massively overprovision the infrastructure that hosts the models.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个高频交易系统必须在不到一毫秒的时间内生成预测，每秒处理数十万次预测。由于模型直接负责数千万美元的利润，因此为托管模型的底层基础设施大规模超额配置是划算的。
- en: A credit score model used by human decision-makers to underwrite loans has a
    modest scale and isn’t subject to rapid change, but it is required to be highly
    transparent so humans can guard against any bias or other subtle failures.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类决策者使用的信用评分模型规模适中，且不会迅速变化，但它需要高度透明，以便人类可以防范任何偏见或其他细微的失败。
- en: A language model trained to recognize place names isn’t subject to rapid change.
    The model can be shared as a static file that is updated only sporadically.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练以识别地名的语言模型不会迅速变化。该模型可以作为一个静态文件共享，仅偶尔更新。
- en: A single cookie-cutter approach wouldn’t solve all of these use cases. Effective
    data science infrastructure can provide a few different patterns and tools to
    gradually harden deployments, as discussed in chapter 6, making sure that each
    use case can be solved in the simplest possible manner. Due to the inherent complexity
    of using models to power real-life applications, it is critical to avoid introducing
    additional accidental complexity in the system. For instance, it isn’t necessary
    or beneficial to deploy a scalable, low-latency model-serving system just to produce
    a modest number of predictions daily.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 单一的模式无法解决所有这些用例。有效的数据科学基础设施可以提供几种不同的模式和工具，以逐步加固部署，如第6章所述，确保每个用例都可以以最简单的方式解决。由于使用模型为现实生活应用提供动力的固有复杂性，避免在系统中引入额外的意外复杂性至关重要。例如，没有必要或有益于仅为了每天生成少量预测而部署一个可扩展、低延迟的模型托管系统。
- en: 'Your toolbox can also include off-the-shelf tools and products, many of which
    have emerged to address the previously described challenges over the past few
    years. Confusingly, no established nomenclature and categorization exist for these
    new tools, making it occasionally hard to understand how they should be used most
    effectively. Categories of tools, which are often grouped under the term *MLOps*
    (machine learning operations), include the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您的工具箱还可以包括现成的工具和产品，其中许多在过去几年中涌现出来，以解决之前描述的挑战。令人困惑的是，这些新工具没有既定的命名法和分类，这使得有时难以理解它们应该如何最有效地使用。通常被归类为
    *MLOps*（机器学习运维）的工具类别包括以下内容：
- en: '*Model monitoring* tools that help address change, for example, by monitoring
    how the distribution of input data and predictions change over time, and failures
    by alerting when predictions drift outside an expected range.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型监控* 工具有助于应对变化，例如，通过监控输入数据和预测随时间的变化分布，以及当预测超出预期范围时发出警报。'
- en: '*Model hosting and serving* tools that help address scale and integrations
    by deploying models as a fleet of microservices that can be queried by outside
    systems, providing a solution for real-time predictions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型托管和部署* 工具通过将模型作为微服务集群部署，以便外部系统查询，从而提供实时预测的解决方案。'
- en: '*Feature stores* address change by providing a consistent way of dealing with
    input data, making sure that data is used consistently both during training time
    as well as for predictions.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征存储* 通过提供处理输入数据的一致方式来应对变化，确保数据在训练时间和预测过程中都得到一致使用。'
- en: Besides tools that are specific to machine learning and data science, it is
    often possible to leverage general-purpose infrastructure, such as a microservice
    platform to host models or a dashboarding tool to monitor models. This is a useful
    approach, especially when such tools are already installed in your environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特定于机器学习和数据科学的工具之外，通常可以利用通用基础设施，例如微服务平台来托管模型或仪表板工具来监控模型。这是一个有用的方法，尤其是在这些工具已经安装在你的环境中时。
- en: In the following sections, we will introduce a mental framework that helps you
    to choose the best pattern for your use case. After this, we will walk through
    a hands-on example that shows the patterns in action.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将介绍一个思维框架，帮助您为您的用例选择最佳模式。之后，我们将通过一个实际操作示例来展示这些模式的应用。
- en: 8.1.1 Batch, streaming, and real-time predictions
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 批量、流式和实时预测
- en: 'When considering how to use a model effectively, you can start by asking the
    following central question: how quickly do we need a prediction after we know
    the input data? Note that the question is not how quickly the model can produce
    a prediction but rather what’s the maximum amount of time we can wait until the
    prediction is used for something. Figure 8.3 illustrates this question of *input-response
    gap*.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑如何有效地使用模型时，你可以从以下核心问题开始：在知道输入数据后，我们需要多快就能得到预测？请注意，这个问题不是模型可以多快产生预测，而是我们最多可以等待多长时间直到预测被用于某事。图8.3说明了这个*输入-响应差距*问题。
- en: '![CH08_F03_Tuulos](../../OEBPS/Images/CH08_F03_Tuulos.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F03_Tuulos](../../OEBPS/Images/CH08_F03_Tuulos.png)'
- en: Figure 8.3 The input-response gap
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 输入-响应差距
- en: We can decide to produce predictions, depicted by the big arrow in figure 8.3,
    any time after the input data is known (obviously not before) and before the predictions
    are needed by outside systems. The longer the input-response gap, the more leeway
    we have to choose when and how to produce the predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在输入数据已知（显然不是在之前）并且在外部系统需要预测之前，任何时候决定生成预测，如图8.3中的大箭头所示。输入-响应差距越长，我们选择何时以及如何生成预测的自由度就越大。
- en: Why is this question important? Intuitively, it is clear that producing answers
    quickly is harder than producing them slowly. The narrower the gap, the harder
    the challenges of scale, change, integrations, and failures become. Although technically
    a system that can support a low latency can handle high latency as well, and,
    hence, it might be tempting to use a single low-latency system to handle all use
    cases, avoiding overengineering can make life much easier.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个问题很重要？直观上，很明显，快速产生答案比慢速产生答案更困难。差距越窄，规模、变化、集成和故障的挑战就越严峻。虽然技术上支持低延迟的系统也可以处理高延迟，因此可能会诱人使用单个低延迟系统来处理所有用例，但避免过度设计可以使生活更加轻松。
- en: 'Depending on the answer, we can choose an appropriate infrastructure and software
    architecture for the use case. Figure 8.4 presents three common classes of model-serving
    systems based on the size of the gap: *batch*, where the gap is typically measured
    in tens of minutes and up; *streaming,* which can support gaps in the minute scale;
    and *real-time*, when responses are needed in seconds or milliseconds.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据答案，我们可以为用例选择合适的基础设施和软件架构。图8.4展示了基于差距大小的三种常见的模型服务系统类别：*批量*，其中差距通常以分钟计，甚至更长；*流*，可以支持分钟级别的差距；以及*实时*，当需要秒或毫秒级的响应时。
- en: '![CH08_F04_Tuulos](../../OEBPS/Images/CH08_F04_Tuulos.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F04_Tuulos](../../OEBPS/Images/CH08_F04_Tuulos.png)'
- en: Figure 8.4 Classes of model-serving systems
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 模型服务系统类别
- en: The batch prediction approach is the easiest to implement—all systems have plenty
    of headroom to operate—but this approach is suitable only if you can afford waiting
    15 minutes or more before using the predictions. In this case, you can aggregate
    all new input data in a big batch and schedule a workflow to run, say, hourly
    or daily, to produce predictions for the batch. The results can be persisted in
    a database or a cache where they can be accessed quickly by outside systems, providing
    a very low latency for accessing precomputed predictions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 批量预测方法最容易实现——所有系统都有足够的余量来运行——但这种方法仅适用于你能够承受在15分钟或更长时间后使用预测的情况。在这种情况下，你可以将所有新的输入数据汇总到一个大批次中，并安排一个工作流每小时或每天运行一次，以生成批次的预测。结果可以持久化存储在数据库或缓存中，以便外部系统可以快速访问，从而提供非常低的访问预计算预测的延迟。
- en: It is not very practical to use a workflow scheduler to produce predictions
    much more frequently because a typical workflow scheduler is not optimized for
    minimal latency. If you need results faster, between 30 seconds and 15 minutes
    after receiving an input data point, you can use *a streaming platform* such as
    Apache Kafka, maybe with a streaming application platform like Apache Flink. These
    systems can receive data at scale and funnel it to data consumers in a few seconds.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作流调度器来频繁地生成预测并不太实用，因为典型的工作流调度器并没有针对最小延迟进行优化。如果你需要更快的结果，在接收到输入数据点后的30秒到15分钟内，你可以使用*流平台*，例如Apache
    Kafka，也许还可以配合Apache Flink这样的流应用程序平台。这些系统可以大规模接收数据，并在几秒钟内将其传递给数据消费者。
- en: An example of a streaming model might be a “watch next” recommendation in a
    video service. Let’s say we want to show a personalized recommendation for a new
    video immediately when the previous one finishes. We can compute the recommendation
    while the previous show is playing, so we can tolerate a latency of a few minutes.
    This allows us to show a recommendation even if the user stops the show prematurely.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 流式模型的例子可能是一个视频服务中的“观看下一个”推荐。假设我们希望在之前的视频结束时立即显示个性化的新视频推荐。我们可以在播放前一个节目时计算推荐，因此我们可以容忍几分钟的延迟。这允许我们在用户提前停止节目时仍然显示推荐。
- en: Finally, if you need results in less than a few seconds, you need a solution
    for *real-time model serving**.* This is similar to any web service that needs
    to produce a response immediately, in tens of milliseconds, after the user clicks
    a button. For instance, companies powering internet advertising use systems like
    this to predict the most effective personalized banner ad for the user on the
    fly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你需要在几秒钟内得到结果，你需要一个用于**实时模型服务**的解决方案。这类似于任何需要在使用者点击按钮后立即（在几十毫秒内）产生响应的Web服务。例如，支持互联网广告的公司使用此类系统来即时预测用户最有效的个性化横幅广告。
- en: 'Another important consideration is how the predictions are going to be consumed
    by outside systems: is the workflow going to push results to a datastore, or are
    they going to be pulled from a service by a consumer? Figure 8.5 outlines these
    patterns—pay attention to the direction of the arrows.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是外部系统如何消费预测：工作流程是否会将结果推送到数据存储，还是消费者会从服务中拉取它们？图8.5概述了这些模式——请注意箭头的方向。
- en: '![CH08_F05_Tuulos](../../OEBPS/Images/CH08_F05_Tuulos.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F05_Tuulos](../../OEBPS/Images/CH08_F05_Tuulos.png)'
- en: Figure 8.5 Patterns for sharing outputs
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 分享输出的模式
- en: 'A workflow producing batch predictions can send the predictions to an external
    API or a database. It is then easy for other business applications, including
    dashboards, to access the predictions from the database. The same pattern applies
    to streaming predictions, with the crucial difference that predictions are refreshed
    more frequently. Note the direction of the arrows: batch and streaming push predictions
    to outside systems. Outside systems can’t call them directly.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生成批量预测的工作流程可以将预测发送到外部API或数据库。然后，其他业务应用，包括仪表板，很容易从数据库中访问预测。相同的模式适用于流式预测，关键的区别在于预测刷新得更频繁。注意箭头的方向：批量和流式将预测推送到外部系统。外部系统不能直接调用它们。
- en: 'In contrast, a real-time model-serving system reverses the pattern: external
    systems pull predictions from the model. This approach is required when the input
    data becomes available just before the prediction is needed. For instance, consider
    a model generating targeted advertisements based on a list of websites the user
    has just visited. In this scenario, it is not feasible to precompute all possible
    combinations of websites in advance. The prediction must be computed in real-time.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，实时模型服务系统反转了模式：外部系统从模型中拉取预测。当输入数据在预测之前变得可用时，需要这种方法。例如，考虑一个基于用户刚刚访问的网站列表生成定向广告的模型。在这种情况下，预先计算所有可能的网站组合是不切实际的。预测必须在实时进行。
- en: 'The choice of the pattern has far-reaching implications, as described next:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 模式的选择具有深远的影响，如下所述：
- en: It is easy to develop and test batch predictions locally. You can use the same
    workflow system for batch predictions that is used to train the model, so no additional
    systems are needed. In contrast, developing and testing streaming and real-time
    predictions require additional infrastructure.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地开发和测试批量预测很容易。你可以使用与训练模型相同的流程系统进行批量预测，因此不需要额外的系统。相比之下，开发和测试流式和实时预测需要额外的基础设施。
- en: Data processing for batch predictions, and sometimes in streaming, can follow
    the patterns outlined in chapter 7\. It is relatively easy to make sure that features
    stay consistent across training and predictions because the same data processing
    code can be used on both sides. In contrast, real-time predictions need a datastore
    that supports low-latency queries as well as low-latency feature encoders.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量预测的数据处理，有时在流式处理中，可以遵循第7章中概述的模式。确保特征在训练和预测中保持一致相对容易，因为可以在双方使用相同的数据处理代码。相比之下，实时预测需要一个支持低延迟查询以及低延迟特征编码器的数据存储。
- en: Scaling batch predictions is as easy as scaling training, using the compute
    layer outlined in chapter 4\. Autoscaling streaming and real-time systems requires
    more sophisticated infrastructure.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理预测的扩展与训练扩展一样简单，使用第4章中概述的计算层。自动扩展流和实时系统需要更复杂的架构。
- en: Monitoring models accurately and logging and managing failures are easier in
    batch systems. In general, ensuring that a batch system stays highly available
    incurs less operational overhead in contrast to a streaming or real-time system.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在批处理系统中，准确监控模型、记录和管理故障更容易。一般来说，确保批处理系统保持高可用性相比流或实时系统，产生的运营开销更小。
- en: All in all, it is beneficial to start by considering whether the application,
    or parts of it, can be powered by batch predictions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，首先考虑该应用程序或其部分是否可以由批处理预测提供动力是有益的。
- en: Note Batch processing doesn’t mean that predictions couldn’t be accessed quickly
    when needed. In fact, precomputing predictions as a batch process and pushing
    the results to a high-performance datastore, like an in-memory cache, provides
    the fastest possible access to predictions. However, batch processing requires
    that inputs are known well in advance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意批处理并不意味着在需要时无法快速访问预测。事实上，作为批处理过程预先计算预测并将其推送到高性能数据存储，如内存缓存，可以提供最快的预测访问。然而，批处理要求输入提前很好地了解。
- en: 'In the next sections, we will demonstrate the three patterns through a practical
    use case: a movie-recommendation system. We will begin with a short introduction
    on recommendation systems. We will then show how to use the recommendation model
    for batch predictions and, after this, how to use it to power real-time predictions.
    Finally, we will outline a high-level architecture for streaming predictions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将通过一个实际用例来展示三种模式：一个电影推荐系统。我们将从对推荐系统的一个简要介绍开始。然后，我们将展示如何使用推荐模型进行批处理预测，然后是如何使用它来提供实时预测。最后，我们将概述流预测的高级架构。
- en: '8.1.2 Example: Recommendation system'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 示例：推荐系统
- en: Let’s build a simple movie-recommendation system to see how the patterns work
    in practice. Imagine that you work at a startup that streams videos online. The
    startup has no existing machine learning systems in production. You were hired
    as a data scientist to build a recommendations model for the company. However,
    you don’t want to stop at the model. To make the model (and yourself) valuable,
    you would like to integrate the recommendations model you created to the company’s
    live product, so it would power a new “recommended for you” feature in the UI.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个简单的电影推荐系统，以了解模式在实际应用中的工作原理。想象一下，你在一家人工智能流媒体视频的初创公司工作。这家初创公司目前没有在生产中部署任何机器学习系统。你被雇佣为数据科学家，为公司构建一个推荐模型。然而，你不想仅仅停留在模型上。为了使模型（以及你自己）变得有价值，你希望将你创建的推荐模型集成到公司的实时产品中，以便在用户界面中提供新的“为您推荐”功能。
- en: Building such a system from scratch takes some effort. We lay out a project
    plan, shown in figure 8.6, which outlines the topics we will focus on in this
    section. We follow the spiral approach, so the first step would be to understand
    the business context of the feature in detail. For this exercise, we can skip
    the step (in real life you shouldn’t!). We start by getting familiar with the
    data available and outlining a rudimentary modeling approach, which we can improve
    later if the project shows promise.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始构建这样一个系统需要一些努力。我们在图8.6中概述了一个项目计划，该计划概述了本节我们将关注的主题。我们遵循螺旋式方法，所以第一步将是详细了解该功能的业务背景。对于这个练习，我们可以跳过这一步（在现实生活中你不应该这样做！）。我们首先熟悉可用的数据，并概述一个基本的建模方法，如果项目显示出希望，我们可以在以后改进它。
- en: '![CH08_F06_Tuulos](../../OEBPS/Images/CH08_F06_Tuulos.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F06_Tuulos](../../OEBPS/Images/CH08_F06_Tuulos.png)'
- en: 'Figure 8.6 Recommendation system project: Focusing on inputs'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 推荐系统项目：关注输入
- en: 'We will develop the first version of the model using a well-known technique
    called *collaborative filtering*. The idea is straightforward: we know what movies
    existing users have watched in the past. By knowing a few movies that a new user
    has watched, we can find existing users similar to the new one and recommend movies
    that they had enjoyed. The key question for a model like this is to define what
    “similar” means exactly and how to calculate similarities between users quickly.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一种称为**协同过滤**的知名技术来开发模型的第一版。这个想法很简单：我们知道现有用户过去观看过哪些电影。通过知道新用户观看的一些电影，我们可以找到与该新用户相似的用户，并推荐他们喜欢的电影。对于这种类型的模型，关键问题是确切地定义“相似”是什么，以及如何快速计算用户之间的相似度。
- en: To train the model, we will use the publicly available MovieLens dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)).
    Download the full dataset with 27 million ratings from [http://mng.bz/EWnj](http://mng.bz/EWnj)
    and unpack the archive.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们将使用公开可用的MovieLens数据集([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/))。从[http://mng.bz/EWnj](http://mng.bz/EWnj)下载包含2700万条评分的完整数据集，并解压存档。
- en: 'Conveniently, the dataset includes a rich characterization of each movie, called
    the tag genome. Figure 8.7 illustrates the idea. In the figure, each movie is
    characterized by two dimensions: drama versus action and serious versus funny.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 便利的是，数据集包括对每部电影丰富的描述，称为标签基因组。图8.7说明了这个概念。在图中，每部电影由两个维度来表征：戏剧与动作，以及严肃与幽默。
- en: '![CH08_F07_Tuulos](../../OEBPS/Images/CH08_F07_Tuulos.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F07_Tuulos](../../OEBPS/Images/CH08_F07_Tuulos.png)'
- en: Figure 8.7 Movie tag genome
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 电影标签基因组
- en: Our model represents movies in a similar vector space, but instead of two dimensions,
    the actual tag genome includes 1,128 dimensions, described in genome-tags.csv.
    The coordinates of each movie in this 1,128-dimensional space are listed in genome-scores.csv.
    To figure out the mapping between movie IDs and movie names, see movies.csv.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在类似的向量空间中表征电影，但实际标签基因组包括1,128个维度，在genome-tags.csv中描述。在这个1,128维空间中每部电影的坐标列在genome-scores.csv中。要了解电影ID与电影名称之间的映射，请参阅movies.csv。
- en: We know what movies each user has watched and the star rating they assigned
    for the movies. This information is contained in ratings.csv. We want to recommend
    only movies that users enjoyed, so we include only movies that the user has rated
    as four or five stars. Now we characterize each user based on the kinds of movies
    they like, in other words, a user (vector) is a sum of movie vectors they liked.
    Figure 8.8 illustrates the idea.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道每个用户观看的电影以及他们为电影分配的星级评分。这些信息包含在ratings.csv文件中。我们只想推荐用户喜欢的电影，因此我们只包括用户评分达到四星或五星的电影。现在，我们根据用户喜欢的电影类型来描述每个用户，换句话说，一个用户（向量）是他们喜欢的电影向量的总和。图8.8说明了这个概念。
- en: '![CH08_F08_Tuulos](../../OEBPS/Images/CH08_F08_Tuulos.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F08_Tuulos](../../OEBPS/Images/CH08_F08_Tuulos.png)'
- en: Figure 8.8 The three key matrices
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 三个关键矩阵
- en: To represent each user as a vector in the tag genome space, we take the *users
    matrix**,* which tells us which movies the users liked, and use it to sum vectors
    from the *movies matrix*, which represents each movie. We can use the same method
    to represent any new user as a vector, if we know a few movies they have enjoyed.
    We normalize each user vector to unit length, because we don’t want the number
    of movies watched to make any difference—only the nature of the movies should
    count.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在标签基因组空间中将每个用户表示为一个向量，我们采用**用户矩阵**，它告诉我们用户喜欢哪些电影，并使用它来对**电影矩阵**中的向量进行求和，该矩阵代表每部电影。如果我们知道一些用户喜欢的电影，我们可以用同样的方法将任何新用户表示为一个向量。我们将每个用户向量归一化到单位长度，因为我们不希望观看电影的数量产生任何影响——只有电影的性质应该被计算在内。
- en: The operations depicted in figure 8.8 are implemented in listing 8.1, which
    we use as a utility module. Most of the code in the listing is used for loading
    and transforming data from CSV files. The movies matrix is loaded in load_model_movies_mtx
    from a CSV that contains a genome_dim (1,128) number of rows for each movie. We
    chunk the file in fixed-sized vectors, stored in a dictionary keyed by movie ID.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8中所示的操作在列表8.1中实现，我们将其用作实用模块。列表中的大部分代码用于从CSV文件加载数据并进行转换。电影矩阵通过包含每个电影基因组_dim（1,128）行数的CSV文件从load_model_movies_mtx加载。我们将文件分成固定大小的向量，并存储在以电影ID为键的字典中。
- en: The users matrix is a bit trickier to load because each user has watched a variable
    number of movies. Based on our learnings from chapter 5, which highlighted the
    performance of NumPy arrays, and chapter 7, which showed the power of Apache Arrow,
    we will use these two projects to handle the large dataset efficiently. We use
    Arrow’s filter method to include only rows with a high rating. We use NumPy’s
    unique method to count the number of movies watched by a user, chunk the rows
    accordingly, and store the resulting list of movie IDs watched by a user in a
    dictionary keyed by user ID.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 加载用户矩阵稍微复杂一些，因为每个用户观看的电影数量是可变的。根据我们在第 5 章中学到的知识，该章节强调了 NumPy 数组的性能，以及第 7 章中展示的
    Apache Arrow 的强大功能，我们将使用这两个项目来高效地处理大数据集。我们使用 Arrow 的过滤方法来仅包括评分高的行。我们使用 NumPy 的唯一方法来计算用户观看的电影数量，相应地分块行，并将用户观看的电影
    ID 列表存储在以用户 ID 为键的字典中。
- en: Listing 8.1 Loading movie data
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 加载电影数据
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Loads the movies matrix
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载电影矩阵
- en: ❷ Parses the dimensionality of the tag genome
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 解析标签基因组的维度
- en: ❸ Parses the movie genome file
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 解析电影基因组文件
- en: ❹ Extracts the two columns we need as NumPy arrays
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 提取所需的两个列作为 NumPy 数组
- en: ❺ Extracts individual movie vectors from the long array
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从长数组中提取单个电影向量
- en: ❻ Loads the users matrix
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 加载用户矩阵
- en: ❼ Includes only watched movies that received four or five stars
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 仅包括获得四星或五星的观看电影
- en: ❽ Determines how many movies each user watched
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 确定每个用户观看的电影数量
- en: ❾ Extracts individual user vectors from the long array
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 从长数组中提取单个用户向量
- en: ❿ Loads the movie ID—movie name mapping
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 加载电影 ID-电影名称映射
- en: Save the code to a utility module, movie_data.py.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到实用模块，movie_data.py。
- en: Finally, we create the user vectors in the next listing. The make_user_vectors
    function works by combining information from the user matrix and movie matrix.
    As a small optimization, we avoid creating a separate vector for each user, because
    we don’t need to store the user vectors explicitly—more about that later. Instead,
    we reuse the same vector for each user sequentially.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在下一个列表中创建用户向量。make_user_vectors 函数通过结合用户矩阵和电影矩阵中的信息来工作。作为一个小的优化，我们避免为每个用户创建单独的向量，因为我们不需要显式地存储用户向量——关于这一点稍后会有更多介绍。相反，我们按顺序重用相同的向量。
- en: Listing 8.2 Making user vectors
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 制作用户向量
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Extracts the first movie vector as a template
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提取第一个电影向量作为模板
- en: ❷ Clears the user vector
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 清除用户向量
- en: ❸ Iterates through movies watched by the user
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历用户观看的电影
- en: ❹ Creates the user vector by summing movie vectors
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过求和电影向量来创建用户向量
- en: ❺ Normalizes the user vector to unit length
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将用户向量归一化到单位长度
- en: Save the code to a utility module, movie_uservec.py. We will use this soon in
    a training workflow.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到实用模块，movie_uservec.py。我们将在训练流程中使用它。
- en: Training a rudimentary recommendations model
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个基本的推荐模型
- en: Figure 8.9 shows how our project is progressing. We are done with the input
    data part. Next, we sketch a rudimentary recommendation model as a placeholder
    for future work.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 展示了我们的项目进展情况。我们已经完成了输入数据部分。接下来，我们将草拟一个基本的推荐模型作为未来工作的占位符。
- en: '![CH08_F09_Tuulos](../../OEBPS/Images/CH08_F09_Tuulos.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F09_Tuulos](../../OEBPS/Images/CH08_F09_Tuulos.png)'
- en: 'Figure 8.9 Recommendation system project: First iteration of the training flow'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 推荐系统项目：训练流程的第一迭代
- en: To produce recommendations for a new user, we will find similar users by measuring
    the vector distance between the new user and all existing users and by picking
    the nearest neighbors. This would require hundreds of thousands of distance measurements
    just for a single set of recommendations, which is computationally very expensive.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为新用户提供推荐，我们将通过测量新用户与所有现有用户之间的向量距离，并选择最近的邻居来找到相似的用户。这仅为了生成一组推荐就需要进行数十万次距离测量，这在计算上非常昂贵。
- en: Fortunately, highly optimized libraries exist to speed up the nearest neighbor
    search. We will use one such library, *Annoy*, that Spotify created for its music-recommendation
    system. Annoy will create an index, a model, of all user vectors, which we can
    save and use later to produce recommendations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，存在高度优化的库来加速最近邻搜索。我们将使用 Spotify 为其音乐推荐系统创建的一个这样的库，*Annoy*。Annoy 将创建所有用户向量的索引和模型，我们可以将其保存并稍后用于生成推荐。
- en: The code in listing 8.3 shows a workflow that trains a recommendation model.
    It uses the functions from movie_data to load the data, store them as artifacts,
    and produce user vectors, which it feeds to Annoy. Annoy will then produce an
    efficient representation of the user vector space.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 中的代码展示了训练推荐模型的流程。它使用 movie_data 中的函数来加载数据，将其存储为工件，并生成用户向量，然后将这些向量输入到
    Annoy 中。Annoy 将然后生成用户向量空间的效率表示。
- en: Listing 8.3 Recommendation model training flow
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 推荐模型训练流程
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Increases this parameter to improve the accuracy of the Annoy index
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 增加此参数以提高 Annoy 索引的准确性
- en: ❷ The start step stores movie data as artifacts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 开始步骤将电影数据存储为工件。
- en: ❸ Iterator that produces user vectors for all existing users
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为所有现有用户生成用户向量的迭代器
- en: ❹ Initializes an Annoy index
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化 Annoy 索引
- en: ❺ Feeds user vectors to the index
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将用户向量输入到索引中
- en: ❻ Finalizes the index
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 最终化索引
- en: ❼ Stores the index as an artifact
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将索引存储为工件
- en: 'Save the code to a file, movie_train_flow.py. To run the flow, make sure you
    have the MovieLens CSVs in the current working directory. Run the flow as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到文件中，名为 movie_train_flow.py。要运行流程，请确保你当前工作目录中有 MovieLens CSV 文件。按照以下方式运行流程：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Building an index takes about 10-15 minutes. You can speed up the process at
    the cost of lower accuracy by lowering the ANN_ACCURACY constant. If you are curious
    to know how the constant affects the Annoy index, see the documentation of the
    build method at [https://github.com/spotify/annoy](https://github.com/spotify/annoy).
    Alternatively, you can make things faster by running the code in the cloud as
    run --with batch.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 构建索引大约需要 10-15 分钟。你可以通过降低 ANN_ACCURACY 常量来加快这个过程，但这样做会降低准确性。如果你对常量如何影响 Annoy
    索引感兴趣，请参阅构建方法的文档，见 [https://github.com/spotify/annoy](https://github.com/spotify/annoy)。或者，你可以通过在云端运行代码来加快速度，使用
    run --with batch。
- en: After the index has been built, it is stored as an artifact. We will use this
    artifact in the next sections to produce recommendations, first as a batch workflow,
    then in real time.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引构建完成后，它被存储为一个工件。我们将在下一节中使用这个工件来生成推荐，首先作为批量工作流程，然后是实时推荐。
- en: 8.1.3 Batch predictions
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 批量预测
- en: 'Now that we have a model, we can focus on the core of this chapter: using a
    model to produce predictions. How should we do it in practice? Let’s consider
    how the discussion could play out in a business setting.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型，我们可以专注于本章的核心：使用模型生成预测。在实际操作中我们应该怎么做？让我们考虑一下在商业环境中讨论可能会如何进行。
- en: 'It is likely that the startup’s systems are already organized as microservices,
    individual containers that expose well-defined APIs. Following the same pattern,
    it would feel natural to create the recommendation systems as another microservice
    (which we will do in the next section). However, during discussions with the engineering
    team, the following potential issues are identified with the microservice approach:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，初创公司的系统已经组织为微服务，即暴露了良好定义的 API 的单个容器。遵循相同的模式，将推荐系统作为另一个微服务（我们将在下一节中这样做）似乎是自然而然的。然而，在与工程团队讨论时，以下潜在问题被识别为微服务方法的问题：
- en: The model takes much more memory and CPU power than existing lightweight web
    services. The company’s container orchestration system would need to be changed
    to accommodate the new service.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型比现有的轻量级网络服务需要更多的内存和 CPU 功率。公司的容器编排系统需要改变以适应新的服务。
- en: How should we scale the service? What if we get a surge of new users? The amount
    of compute power to produce thousands of recommendations per second is nontrivial.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何扩展服务？如果我们突然迎来大量新用户怎么办？每秒生成数千个推荐所需的计算能力非同小可。
- en: Isn’t it wasteful to keep requesting recommendations every time the user refreshes
    the page? The recommendations don’t change until the user has completed watching
    a movie. Maybe we should cache the recommendations somehow?
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次用户刷新页面时都请求推荐，这不是很浪费吗？直到用户完成观看电影，推荐才会改变。我们可能需要以某种方式缓存推荐？
- en: Are you, the data scientist, going to be responsible for operating the new microservice?
    Engineers are responsible for their microservices, but operating them requires
    a sophisticated toolchain, which the data science team hasn’t used before.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你，作为数据科学家，将要负责运营这个新的微服务吗？工程师负责他们的微服务，但运营它们需要复杂的工具链，而数据科学团队之前没有使用过这些工具链。
- en: 'Also, you recognize *the cold start problem*: when a new user signs up, there’s
    no data to produce recommendations for them. A product manager suggests that we
    could ask the user for a few movies they like as a part of the signup process
    to address the issue.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你也认识到了冷启动问题：当新用户注册时，没有数据为他们生成推荐。产品经理建议，我们可以在注册过程中询问用户他们喜欢的几部电影，以解决这个问题。
- en: 'All of these points are valid concerns. After pondering the situation for a
    while, you come up with a radically different approach: what if, instead of a
    microservice, you produce recommendations for all the existing users as a big
    batch operation, structured as a workflow? For existing users, we can produce
    a long list of recommendations, so even after they have watched a movie, the list
    won’t change drastically. You can refresh the list, say, nightly.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些观点都是有效的关注点。经过一段时间思考，你提出了一个截然不同的方法：如果，不是微服务，而是作为一个大批量操作，结构化为工作流程来为所有现有用户生成推荐，会怎样？对于现有用户，我们可以生成一个长长的推荐列表，所以即使他们看过了电影，列表也不会有太大变化。你可以每晚刷新列表。
- en: To solve the cold start problem, when a new user signs up, we can ask them to
    choose two movies they have enjoyed in the past, among, say, the top 1,000 most
    popular movies. Based on their initial choice, we can recommend other movies they
    might enjoy. A key observation is that we can precompute recommendations for all
    possible pairs of movies.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决冷启动问题，当新用户注册时，我们可以要求他们选择过去喜欢的两部电影，比如在最受欢迎的前1000部电影中。根据他们的初始选择，我们可以推荐他们可能喜欢的其他电影。一个关键的观察结果是，我们可以预先计算所有可能电影对的推荐。
- en: Figure 8.10 visualizes the situation. Conceptually, the user chooses a row corresponding
    to a movie and then a column corresponding to another movie. We exclude the diagonal—we
    don’t allow the user to choose the same movie twice. Because the choice of (movie
    A, movie B) is equal to (movie B, movie A)—the order doesn’t matter—we can also
    exclude one-half of the matrix. Hence, we need to precompute recommendations only
    for the upper triangle, the dark gray area in the matrix. You may remember the
    formula for determining the number of dark cells in matrix, that is, the number
    of 2-combinations ![CH08_EQ01](../../OEBPS/Images/CH08_EQ01.png), which equals
    499,500.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 可视化了这种情况。从概念上讲，用户选择与一部电影对应的行，然后选择与另一部电影对应的列。我们排除了对角线——我们不允许用户两次选择同一部电影。因为（电影A，电影B）的选择等于（电影B，电影A）——顺序不重要——我们也可以排除矩阵的一半。因此，我们只需要预先计算矩阵的上三角部分，即矩阵中的深灰色区域。你可能还记得确定矩阵中深色单元格数量的公式，即2-组合的数量![CH08_EQ01](../../OEBPS/Images/CH08_EQ01.png)，它等于499,500。
- en: '![CH08_F10_Tuulos](../../OEBPS/Images/CH08_F10_Tuulos.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F10_Tuulos](../../OEBPS/Images/CH08_F10_Tuulos.png)'
- en: Figure 8.10 Producing recommendations for all pairs of movies (the dark triangle)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 为所有电影对生成推荐（深色三角形）
- en: 'In other words, we can handle any choice made by the user by precomputing about
    half a million recommendations, which is quite doable! We gain many benefits by
    doing this: we can write the recommendations to a database that the engineering
    team manages with no performance concerns, no scalability concerns, no need to
    cache anything, and no new operational overhead. Batch predictions seem like a
    great approach for this use case.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以通过预先计算大约五十万条推荐来处理用户做出的任何选择，这是完全可行的！通过这样做，我们可以获得许多好处：我们可以将推荐写入由工程团队管理的数据库，无需担心性能问题、可扩展性问题、无需缓存任何内容，也没有新的运营开销。批量预测似乎是这个用例的一个很好的方法。
- en: Producing recommendations
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 生成推荐
- en: Per the spiral approach, we should consider how the results will be consumed
    by outside systems before we spend too much time in producing them. We will start
    by thinking through how we can use the model to produce recommendations to better
    understand the shape of the results. We will focus on sharing the results and
    then come back to finalize the batch prediction flow. Figure 8.11 shows our progress.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 根据螺旋方法，在花费太多时间生成结果之前，我们应该考虑外部系统将如何消费这些结果。我们将首先思考如何使用模型生成推荐，以更好地理解结果的形式。我们将专注于共享结果，然后回来最终确定批量预测流程。图8.11显示了我们的进度。
- en: '![CH08_F11_Tuulos](../../OEBPS/Images/CH08_F11_Tuulos.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F11_Tuulos](../../OEBPS/Images/CH08_F11_Tuulos.png)'
- en: 'Figure 8.11 Recommendation system project: Focusing on predictions'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 推荐系统项目：专注于预测
- en: The Annoy index produced by MovieTrainFlow allows us to find nearest neighbors
    for a new user vector quickly. How should we go from similar users to actual movie
    recommendations? A simple policy is to consider what other movies the neighbors
    have enjoyed and recommend them.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: MovieTrainFlow生成的Annoy索引使我们能够快速为新用户向量找到最近邻。我们如何从相似用户到实际电影推荐呢？一个简单的策略是考虑邻居们喜欢了哪些其他电影，然后推荐给他们。
- en: Figure 8.12 illustrates the idea. Imagine we have a vector for a new user, depicted
    by the large gray circle. Using the Annoy index, we can find its neighbors, bounded
    by the oval. Based on the users matrix, we know what movies the neighbors have
    enjoyed, so we can simply count the frequencies of movies in the neighborhood,
    exclude the ones that the user has watched already, and return the remaining highest
    frequency movies as recommendations. In the figure, all the neighbors have watched
    the movie *Alien*, so it has the highest frequency and, hence, becomes our recommendation.
    If the user had already watched all the movies in the neighborhood, we can increase
    the size of the neighborhood until we find valid recommendations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12说明了这个想法。想象我们有一个新用户的向量，用大灰色圆圈表示。使用Annoy索引，我们可以找到它的邻居，由椭圆形界定。根据用户矩阵，我们知道邻居们喜欢了哪些电影，因此我们可以简单地计算邻域中电影的频率，排除用户已经看过的电影，并将剩余的最高频率电影作为推荐返回。在图中，所有邻居都看过电影*异形*，因此它具有最高的频率，因此成为我们的推荐。如果用户已经看过邻域中的所有电影，我们可以增加邻域的大小，直到找到有效的推荐。
- en: '![CH08_F12_Tuulos](../../OEBPS/Images/CH08_F12_Tuulos.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F12_Tuulos](../../OEBPS/Images/CH08_F12_Tuulos.png)'
- en: Figure 8.12 Neighborhood of similar users (the dashed circle)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12相似用户的邻域（虚线圆圈）
- en: Listing 8.4 shows a utility module that implements the logic. The load_model
    function is used to load the Annoy index from an artifact. Annoy wants to read
    the model from a file, so we have to write the model first to a temporary file.
    The recommend function produces recommendations for a set of users, represented
    by sets of movie IDs they have watched (movie_sets). It increases the size of
    the neighborhood until new movies are found. The find_common_movies returns the
    top_n most commonly watched movies in the neighborhood, excluding movies that
    the user has already seen.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4展示了实现逻辑的实用模块。load_model函数用于从工件中加载Annoy索引。Annoy想要从文件中读取模型，因此我们必须先将模型写入临时文件。recommend函数为一系列用户生成推荐，这些用户通过他们观看的电影ID集合（movie_sets）表示。它增加邻域的大小，直到找到新电影。find_common_movies返回邻域中观看次数最多的top_n部电影，排除用户已经看过的电影。
- en: Listing 8.4 Producing recommendations
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 生成推荐
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Uses the same function to create vectors as training
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用与训练相同的函数创建向量
- en: ❷ Lower this value for faster, less accurate results.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 降低此值以获得更快、更不精确的结果。
- en: ❸ Loads an Annoy index from an artifact
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从工件中加载Annoy索引
- en: ❹ Allows Annoy to read the index from a file
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 允许Annoy从文件中读取索引
- en: ❺ Returns recommendations for the given users
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为给定用户返回推荐
- en: ❻ Produces user vectors
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 生成用户向量
- en: ❼ Increases the neighborhood size until recommendations are found
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 增加邻域大小，直到找到推荐
- en: ❽ Finds the nearest neighbors using the Annoy index
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用Annoy索引找到最近邻
- en: ❾ Finds the most frequently liked movies in the neighborhood
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 在邻域中找到最受欢迎的电影
- en: ❿ Uses the users matrix to collect statistics about movies watched
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 使用用户矩阵收集关于观看电影的统计数据
- en: ⓫ Excludes movies that the new user has already seen
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 排除新用户已经看过的电影
- en: ⓬ Returns the top-N most frequently liked movies
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 返回最受欢迎的前N部电影
- en: Save the code to a file movie_model.py. We will use the module soon.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到文件movie_model.py中。我们很快就会使用这个模块。
- en: Note that we use the same make_user_vectors function both to train the model
    (build an index) as well as produce predictions. This is very important because
    models must use the same feature space for training and predictions. Remember
    how in chapter 7 we discussed how feature encoders turn facts into features. Although
    we can’t guarantee that facts stay stable over time—facts changing is called *data
    drift*—at least we can guarantee that the feature encoders, here make_user_vectors,
    are used consistently.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用相同的make_user_vectors函数来训练模型（构建索引）以及生成预测。这非常重要，因为模型必须在训练和预测时使用相同的特征空间。记得在第七章我们讨论了特征编码器如何将事实转换为特征。虽然我们无法保证事实在时间上保持稳定——事实的变化称为*数据漂移*——但至少我们可以保证特征编码器，这里make_user_vectors，是一致使用的。
- en: Important Place data processing and feature encoding code in a separate module
    that you can share between training and prediction flows, ensuring that features
    are produced consistently.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 重要 将位置数据处理和特征编码代码放在一个单独的模块中，您可以在训练和预测流程之间共享，确保特征是一致生成的。
- en: Now that we have a good idea of how to produce recommendations, let’s consider
    how we can share them with outside systems effectively.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地了解了如何生成推荐，让我们考虑如何有效地与外部系统共享它们。
- en: Sharing results robustly
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 坚固地共享结果
- en: Let’s build an integration with a database that is used by a web application
    showing recommendations. Figure 8.13 shows our progress. In a real-life project,
    you would most likely use a library specific to your data warehouse or database
    to store the results. Here, to demonstrate the idea, we write the results to a
    SQLite database, which is conveniently built into Python.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个与用于显示推荐的Web应用程序数据库集成的示例。图8.13显示了我们的进度。在实际项目中，您最有可能使用特定于您数据仓库或数据库的库来存储结果。在这里，为了演示这个想法，我们将结果写入SQLite数据库，它方便地内置在Python中。
- en: '![CH08_F13_Tuulos](../../OEBPS/Images/CH08_F13_Tuulos.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F13_Tuulos](../../OEBPS/Images/CH08_F13_Tuulos.png)'
- en: 'Figure 8.13 Recommendation system project: Focusing on outputs'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 推荐系统项目：关注输出
- en: 'Listing 8.5 shows a function, save, which creates a database (stored in a file)
    with two tables: movies, which stores information about movies (movie ID, name,
    and whether they should be shown during the signup process), and recs which stores
    a list of recommended movies for each pair of movies chosen during signup.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5显示了一个名为save的函数，它创建一个数据库（存储在文件中），包含两个表：movies，它存储有关电影的信息（电影ID、名称以及是否应在注册过程中显示），和recs，它存储在注册过程中选择的每对电影推荐的列表。
- en: Listing 8.5 Storing recommendations
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 存储推荐
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Makes a canonical key out of two movie IDs. The order doesn’t matter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从两个电影ID中生成一个规范键。顺序无关紧要。
- en: ❷ Returns a versioned database name
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回一个带版本的数据库名称
- en: ❸ SQL to create a movies table
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建电影表的SQL语句
- en: ❹ SQL to insert movies in the table
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在表中插入电影的SQL语句
- en: ❺ SQL to create a recommendations table
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建推荐表的SQL语句
- en: ❻ SQL to insert recommendations in the table
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在表中插入推荐的SQL语句
- en: ❼ SQL to create an index to speed up querying
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 创建索引以加快查询速度的SQL语句
- en: ❽ Makes recommendations compatible with our SQL statement
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使推荐与我们的SQL语句兼容
- en: ❾ Creates and populates a database with recommendations
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 创建并填充带有推荐的数据库
- en: ❿ Returns the versioned database name
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 返回带版本的数据库名称
- en: 'Save the code to a file movie_db.py. We will use the module soon. Note an important
    detail about the save function: it versions the database and the tables with a
    Metaflow run ID. Versioning the outputs is crucial, as it allows you to do the
    following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到文件movie_db.py中。我们很快就会使用这个模块。注意save函数的一个重要细节：它使用Metaflow运行ID对数据库和表进行版本控制。版本化输出至关重要，因为它允许您执行以下操作：
- en: '*Operate multiple parallel versions safely*. You can have multiple versions
    of recommendations in production, for example, using the @project decorator we
    discussed in chapter 6\. For instance, this is required if you want to A/B test
    recommendation variants with live traffic. Thanks to versioning, the variants
    can never interfere with each other.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全地操作多个并行版本*。在生产环境中，您可以拥有多个版本的推荐，例如，使用我们在第6章中讨论的@project装饰器。例如，如果您想对带有实时流量的推荐变体进行A/B测试，这是必需的。多亏了版本控制，这些变体永远不会相互干扰。'
- en: '*Separate publishing of results, validation, and promotion to production*.
    Using this pattern, you can safely run a batch prediction workflow, but its results
    don’t take effect until you point the consumers at the new table, making it possible
    to validate the results first.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分离结果的发布、验证和推广到生产环境*。使用这种模式，您可以安全地运行批量预测工作流程，但它的结果不会生效，直到您将消费者指向新表，这使得您可以首先验证结果。'
- en: '*Write all results as an atomic operation*. Imagine the workflow fails while
    writing results. It would be very confusing if half of the results were new and
    half old. Many databases support transactions, but not all, especially if results
    span multiple tables or even multiple databases. The versioning approach works
    with all systems.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将所有结果作为一个原子操作写入*。想象一下，当写入结果时工作流程失败了。如果一半的结果是新的，一半是旧的，那会非常混乱。许多数据库支持事务，但并非所有，尤其是如果结果跨越多个表甚至多个数据库。版本化方法适用于所有系统。'
- en: '*Experiment safely*. Even if someone runs the workflow on their laptop during
    prototyping, there won’t be an adverse effect on production systems automatically.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全实验*。即使有人在原型设计期间在自己的笔记本电脑上运行工作流程，也不会对生产系统产生自动的负面影响。'
- en: '*Aid debugging and auditing*. Imagine a user reports unexpected or incorrect
    predictions. How do you know what they were seeing exactly? Versioning makes sure
    that you can backtrack a full lineage of predictions from the UI back to model
    training.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*帮助调试和审计*。想象一下，一个用户报告了意外的或不正确的预测。你如何知道他们确切看到了什么？版本控制确保你可以从UI回溯到模型训练的完整预测谱系。'
- en: '*Clean up old results efficiently*. In particular, if you version the whole
    tables of results, you can quickly clean old results with a single DROP TABLE
    statement.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高效清理旧结果*。特别是，如果你对整个结果表进行版本控制，你可以使用单个DROP TABLE语句快速清理旧结果。'
- en: We will discuss these topics in more detail in section 8.2 about model operations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第8.2节关于模型操作的详细讨论中讨论这些主题。
- en: Recommendation Always include a version identifier in all results you write
    to external systems.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐始终在写入外部系统的所有结果中包含版本标识符。
- en: Now that we have the two key ingredients, a module to produce recommendations
    and a module to share them, we can develop a flow that precomputes all recommendations
    needed during the signup.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个关键组成部分，一个用于生成推荐的模块和一个用于共享推荐的模块，我们可以开发一个预先计算注册过程中所需的所有推荐的工作流程。
- en: Producing a batch of recommendations
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一批推荐
- en: 'Next, we will implement the last piece: producing hypothetical user profiles
    and generating recommendations for them. We will do this in a flow that will use
    the latest model produced by MovieTrainFlow. Figure 8.14 shows our progress.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现最后一部分：生成假设的用户资料并为它们生成推荐。我们将使用由MovieTrainFlow生成的最新模型来完成这项工作。图8.14显示了我们的进展。
- en: '![CH08_F14_Tuulos](../../OEBPS/Images/CH08_F14_Tuulos.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F14_Tuulos](../../OEBPS/Images/CH08_F14_Tuulos.png)'
- en: 'Figure 8.14 Recommendation system project: Finalizing predictions'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 推荐系统项目：最终确定预测
- en: Having a separate model to train a model and batch-produce predictions is useful
    because it allows you to schedule the two flows independently. For instance, you
    could retrain the model nightly and refresh recommendations hourly.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个单独的模型来训练模型和批量生成预测是有用的，因为它允许你独立地安排这两个流程。例如，你可以每晚重新训练模型，每小时刷新推荐。
- en: 'A key challenge with batch predictions is scale and performance. Producing
    hundreds of thousands of recommendations takes many compute cycles, but it shouldn’t
    take hours or days to complete a run. Luckily, as shown in the following listings,
    we can use familiar tools and techniques to address the challenge: horizontally
    scaling workflows from chapter 3, scalable compute layers from chapter 4, performance
    tips from chapter 5, and patterns for large-scale data processing from chapter
    7\. Once finished, we can deploy the workflow to production using the lessons
    from chapter 6.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 批量预测的一个主要挑战是规模和性能。生成数十万条推荐需要许多计算周期，但完成一次运行不应该需要数小时或数天。幸运的是，如以下列表所示，我们可以使用熟悉的工具和技术来应对这一挑战：从第3章中扩展工作流程的水平缩放，从第4章中可扩展的计算层，第5章中的性能提示，以及第7章中的大规模数据处理模式。一旦完成，我们可以使用第6章中的经验将工作流程部署到生产环境中。
- en: Figure 8.15 shows a general architecture of batch prediction workflows. We start
    by fetching data that we want to predict with. We split the data to multiple batches
    that can be processed in parallel. Finally, we send the results to an outside
    system.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15显示了批量预测工作流程的一般架构。我们首先获取我们想要预测的数据。我们将数据分成多个批次，这些批次可以并行处理。最后，我们将结果发送到外部系统。
- en: '![CH08_F15_Tuulos](../../OEBPS/Images/CH08_F15_Tuulos.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F15_Tuulos](../../OEBPS/Images/CH08_F15_Tuulos.png)'
- en: Figure 8.15 Typical structure of a batch prediction workflow
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 批量预测工作流程的典型结构
- en: In this example, we produce recommendations only for hypothetical new users
    who pick two of their favorite movies during the signup process. In a real product,
    we would update recommendations for existing users as well, fetching their user
    profiles every time the flow runs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们只为在注册过程中选择了两部他们最喜欢的电影的理论新用户生成推荐。在一个真实的产品中，我们还会更新现有用户的推荐，每次流程运行时都会获取他们的用户资料。
- en: As discussed in the beginning of this section, we limit the number of movies
    new users can choose to limit the number of combinations we have to precompute.
    To increase the likelihood that a new user is able to find their favorites, we
    pick the top K most popular movies. The next listing shows a function, top_movies,
    that finds the subset of popular movies.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节开头所述，我们限制新用户可以选择的电影数量，以限制我们必须预先计算的组合数量。为了增加新用户找到他们喜欢的电影的可能性，我们选择最受欢迎的前 K
    部电影。下一个列表显示了一个函数 top_movies，它找到热门电影的子集。
- en: We will produce all combinations of popular movies, resulting in about 500,000
    pairs for the top-1,000 movies. We can produce recommendations for these 500,000
    hypothetical user profiles in parallel. We split the list of pairs to chunks of
    100,000 profiles using a utility function, make_batches.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成所有热门电影的组合，对于前 1,000 部电影，这将产生大约 500,000 对。我们可以并行地为这 500,000 个假设的用户配置文件生成推荐。我们使用
    make_batches 工具函数将配对列表分成 100,000 个配置文件的块。
- en: Listing 8.6 Utilities for batch recommendations
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.6 批量推荐工具
- en: '[PRE6]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Splits a list to fixed-size chunks and returns a list of chunks
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将列表分割成固定大小的块并返回块列表
- en: ❷ Counts all movie IDs in the user matrix and returns the top K most popular
    ones
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 统计用户矩阵中的所有电影 ID 并返回最受欢迎的前 K 个
- en: Save the code to a file, movie_recs_util.py. It will be used in the following
    flow. Listing 8.7 puts all the pieces together. It generates all movie pairs in
    the start step, produces recommendations for batches of user profiles in parallel
    in the batch_recommend step, and aggregates and stores the results in the join
    step, following the pattern depicted in figure 8.15 above.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到文件 movie_recs_util.py 中。它将在以下流程中使用。列表 8.7 将所有部分组合在一起。它生成所有电影对，在开始步骤中，在批量推荐步骤中并行地为用户配置文件批量生成推荐，并在连接步骤中汇总并存储结果，遵循上述图
    8.15 中描述的模式。
- en: Listing 8.7 Batch recommendations flow
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.7 批量推荐流程
- en: '[PRE7]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Fetches the latest model
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取最新模型
- en: ❷ Produces a list of the most popular movies
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成最热门电影列表
- en: '❸ Produces hypothetical user profiles: pairs of popular movies'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成假设用户配置文件：热门电影的配对
- en: ❹ These steps are run in parallel.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这些步骤是并行运行的。
- en: ❺ Loads the Annoy index
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 加载 Annoy 索引
- en: ❻ Produces recommendations for this batch
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 为此批次的用户生成推荐
- en: ❼ Aggregates recommendations across all batches
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 汇总所有批次的推荐
- en: ❽ Saves recommendations to the database
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将推荐保存到数据库
- en: 'Save the code to a file, movie_recs_flow.py. Run the flow as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到文件 movie_recs_flow.py 中。按照以下方式运行流程：
- en: '[PRE8]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By default, only the top 100 movies are considered. You can add the option --num_top=1000
    (or higher) to precompute recommendations for more movies. After the run has finished,
    you should find a SQLite database file, prefixed with movie_recs_, in the current
    working directory. The database containing recommendations for 500,000 users (hypothetical
    user profiles) is about 56 MB—there is clearly room for growth! If you want to
    run this flow in the cloud, you can change the @resources decorators to @batch
    to run just the start and batch_recommend steps remotely. You need to run the
    join step locally because we will need the SQLite database stored in a local file
    soon.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，只考虑前 100 部电影。您可以通过添加选项 --num_top=1000（或更高）来预先计算更多电影的推荐。运行完成后，您应该在当前工作目录中找到一个以
    movie_recs_ 为前缀的 SQLite 数据库文件。包含 500,000 个用户（假设用户配置文件）的推荐数据库大约是 56 MB——显然还有增长空间！如果您想在云中运行此流程，可以将
    @resources 装饰器更改为 @batch，以远程运行仅开始和批量推荐步骤。您需要本地运行连接步骤，因为我们很快就需要存储在本地文件中的 SQLite
    数据库。
- en: Note how we use the expression Flow('MovieTrainFlow').latest_successful_ run
    to access the model produced by MovieTrainFlow. This call operates in the same
    namespace as MovieRecsFlow, meaning each user can experiment with flows freely
    without interfering with each other’s work. As discussed in chapter 6, namespacing
    works with the @project decorator, too, so you can safely deploy various variants
    of the flows in production concurrently.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何使用表达式 Flow('MovieTrainFlow').latest_successful_run 来访问 MovieTrainFlow
    生成的模型。这个调用在 MovieRecsFlow 的相同命名空间中操作，这意味着每个用户可以自由地实验流程，而不会相互干扰。正如第 6 章中讨论的，命名空间与
    @project 装饰器一起工作，因此您可以在生产环境中安全地并发部署各种流程变体。
- en: You can use the sqlite3 command-line tool to open the database and query it.
    However, this chapter is all about producing actual business value using models,
    so we can take a step further and view the results on a web UI!
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 sqlite3 命令行工具打开数据库并查询它。然而，本章全部关于使用模型产生实际业务价值，因此我们可以更进一步，并在 Web UI 上查看结果！
- en: Using recommendations in a web app
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Web 应用程序中使用推荐
- en: In a real-life business setting, your responsibility as a data scientist would
    probably end at writing the results to a database. Another engineering team can
    easily read the recommendations from the database and build an application around
    them. Figure 8.16 shows the final step.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中的商业环境中，作为数据科学家，你的职责可能仅限于将结果写入数据库。另一个工程团队能够轻松地从数据库中读取建议并围绕它们构建应用程序。图 8.16
    展示了最终步骤。
- en: '![CH08_F16_Tuulos](../../OEBPS/Images/CH08_F16_Tuulos.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F16_Tuulos](../../OEBPS/Images/CH08_F16_Tuulos.png)'
- en: 'Figure 8.16 Recommendation system project: Focusing on the web UI'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 推荐系统项目：关注 Web UI
- en: Although usually you can rely on other engineering teams when it comes to web
    application development, it is occasionally useful to be able to build a quick
    application prototype to see the results in action. Fortunately, powerful open
    source frameworks exist that make it easy to build simple dashboards in Python.
    Next, we will use one such framework, Plotly Dash ([https://plotly.com/dash/](https://plotly.com/dash/)),
    to build a simple UI to simulate the signup flow with recommendations.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常你可以依赖其他工程团队进行 Web 应用程序开发，但有时能够快速构建一个应用程序原型来查看实际结果是有用的。幸运的是，存在一些强大的开源框架，使得在
    Python 中构建简单的仪表板变得容易。接下来，我们将使用其中一个这样的框架，即 Plotly Dash ([https://plotly.com/dash/](https://plotly.com/dash/))，来构建一个简单的
    UI 来模拟带有推荐的注册流程。
- en: 'First, let’s create a simple client library to fetch recommendations from the
    SQLite database. We need only two functions: get_recs, which returns precomputed
    recommendations given two movies, and get_top_movies, which returns a list of
    top movies that we have recommendations for. The next listing shows the client.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个简单的客户端库来从 SQLite 数据库中获取推荐。我们只需要两个函数：get_recs，它返回两个电影给出的预计算推荐，以及 get_top_movies，它返回我们对其有推荐的顶级电影列表。下一个列表显示了客户端。
- en: Listing 8.8 Accessing recommendations
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.8 访问推荐
- en: '[PRE9]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Opens a versioned database given a run ID
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据运行 ID 打开一个版本化的数据库
- en: ❷ Fetches recommendations for a hypothetical user profile consisting of two
    movies
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取由两个电影组成的假设用户配置文件的推荐
- en: ❸ Returns a list of top movies
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回顶级电影的列表
- en: 'Save the code to a file, movie_db_client.py. Next, install Plotly Dash by running
    the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到文件 movie_db_client.py 中。接下来，通过运行以下命令安装 Plotly Dash：
- en: '[PRE10]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Plotly Dash allows you to build a whole web application, both the user interface
    running in the browser and the web server backend in a single Python module. You
    can refer to its documentation and tutorials to learn the tool in detail. Next,
    we use it to put together a small, self-explanatory prototype UI, shown in figure
    8.17.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Plotly Dash 允许你在一个 Python 模块中构建整个网络应用程序，包括在浏览器中运行的用户界面和作为后端的网络服务器。你可以参考其文档和教程来详细了解这个工具。接下来，我们将使用它来构建一个简单、自解释的原型用户界面，如图
    8.17 所示。
- en: '![CH08_F17_Tuulos](../../OEBPS/Images/CH08_F17_Tuulos.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F17_Tuulos](../../OEBPS/Images/CH08_F17_Tuulos.png)'
- en: Figure 8.17 Web UI for our example recommendation system
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 我们示例推荐系统的 Web UI
- en: 'Listing 8.9 shows the Dash app that produces the web UI from figure 8.17\.
    The app has two main parts: the layout of the app is defined in app .layout, including
    a list of movies in dropdowns that we fetch from the database. The function update_output
    is called by Dash whenever the user clicks the button. If the user has chosen
    two movies, we can fetch the corresponding recommendations from the database.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 展示了生成图 8.17 中 Web UI 的 Dash 应用程序。应用程序有两个主要部分：应用程序的布局定义在 app.layout 中，包括我们从数据库中获取的电影下拉列表。当用户点击按钮时，Dash
    会调用 update_output 函数。如果用户选择了两部电影，我们可以从数据库中获取相应的推荐。
- en: Listing 8.9 Recommendations web app
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 推荐网络应用程序
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Specifies the version of the database as a command-line argument
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据库版本作为命令行参数指定
- en: ❷ Fetches a list of top movies from the database
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从数据库中获取顶级电影的列表
- en: ❸ Defines the UI components
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义 UI 组件
- en: ❹ Function that is called when the button is clicked
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 当按钮被点击时调用的函数
- en: ❺ Fetches recommendations from the database
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从数据库中获取推荐
- en: ❻ Starts the web server
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 启动网络服务器
- en: 'Save the code to a file, movie_dash.py. To start the server, you need a database
    produced by MovieRecsFlow in the current working directory. Once you have a database,
    you can point the server at it by specifying a run ID as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到文件 movie_dash.py 中。要启动服务器，你需要在当前工作目录中有一个由 MovieRecsFlow 生成的数据库。一旦你有了数据库，你可以通过指定运行
    ID 来指向服务器，如下所示：
- en: '[PRE12]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The server should output a line like
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器应该输出类似以下内容的行
- en: '[PRE13]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can copy and paste the URL into your browser, which should then open the
    web app. Now you can choose any pair of movies and get recommendations personalized
    to your taste! Figure 8.18 shows two example results.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将URL复制并粘贴到浏览器中，然后应该会打开web应用。现在你可以选择任何一对电影，并获得符合你口味的个性化推荐！图8.18显示了两个示例结果。
- en: '![CH08_F18_Tuulos](../../OEBPS/Images/CH08_F18_Tuulos.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F18_Tuulos](../../OEBPS/Images/CH08_F18_Tuulos.png)'
- en: Figure 8.18 Examples of recommendations
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 推荐示例
- en: 'Notice how the UI is perfectly snappy. There is no noticeable delay in producing
    the recommendations, resulting in a pleasant user experience. This is a major
    benefit of batch predictions: there isn’t a faster way of producing predictions
    than not computing anything at all on the fly.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意UI的响应速度非常快。在生成推荐时没有明显的延迟，从而提供了愉悦的用户体验。这是批量预测的主要优势：没有比完全不进行即时计算更快的预测方式。
- en: Congratulations for developing a fully functional recommendations system, from
    raw data all the way to a functional web UI! In the next section, we will see
    how the same model could be used to produce recommendations on the fly, which
    is useful for situations where we don’t know the input data in advance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你开发了一个功能齐全的推荐系统，从原始数据到功能性的web UI！在下一节中，我们将看到如何使用相同的模型即时生成推荐，这对于我们事先不知道输入数据的情况非常有用。
- en: 8.1.4 Real-time predictions
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 实时预测
- en: 'Remember figure 8.4 from the beginning of the chapter? As shown again in figure
    8.19, if you have at least 15 minutes from the moment you know the input data
    to the time when predictions are actually needed, you can consider batch predictions.
    The previous section demonstrated an extreme case of this: we could pregenerate
    all input data, pairs of top movies, long before predictions were needed.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 记得章节开头提到的图8.4吗？如图8.19再次所示，如果你从知道输入数据的那一刻起至少有15分钟的时间到实际需要预测的时间，你可以考虑批量预测。上一节演示了这种情况的一个极端例子：我们可以在预测之前很久就预先生成所有输入数据，包括顶级电影的配对。
- en: '![CH08_F19_Tuulos](../../OEBPS/Images/CH08_F19_Tuulos.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F19_Tuulos](../../OEBPS/Images/CH08_F19_Tuulos.png)'
- en: Figure 8.19 Focusing on streaming and real-time predictions
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 关注流式和实时预测
- en: Clearly, we don’t always have the luxury of time. For instance, we may want
    to recommend movies or other products a few minutes or seconds after the user
    has viewed them. Although ideally, it would be convenient to have a system that
    can work the same way regardless of the time scale, in practice, one needs to
    make tradeoffs to produce predictions quickly. For example, if you need answers
    in seconds, there’s not enough time to spin instances up and down on the compute
    layer on the fly. Or, there’s not enough time to download a large dataset from
    S3 as a whole.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们并不总是有足够的时间。例如，我们可能希望在用户观看电影或产品几分钟或几秒钟后推荐电影或其他产品。虽然理想情况下，有一个可以在任何时间尺度上以相同方式工作的系统会非常方便，但在实践中，为了快速生成预测，需要做出权衡。例如，如果你需要在几秒钟内得到答案，就没有足够的时间在计算层上动态启动和关闭实例。或者，没有足够的时间从S3下载整个大型数据集。
- en: 'Hence, systems that need to produce answers quickly need to be built differently.
    In the context of machine learning, such systems are often called *model-serving*
    or *model-hosting* systems. Their operating principle at the high-level is simple:
    first, you need a model (file), which is typically produced by a batch workflow,
    such as by Metaflow. Often, the model is accompanied by functions for preprocessing
    incoming data, for example, to convert incoming facts to features, and functions
    for postprocessing results in the desired format. The bundle of a model and supporting
    code can be packaged as a container, which is deployed on a microservice platform
    that takes care of running the containers and routing requests to them.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要快速生成答案的系统需要以不同的方式构建。在机器学习的背景下，这类系统通常被称为*模型服务*或*模型托管*系统。它们在高级别的运行原理很简单：首先，你需要一个模型（文件），这通常由批量工作流程生成，例如通过Metaflow。通常，模型会附带预处理输入数据的函数，例如将输入事实转换为特征，以及将结果后处理为所需格式的函数。模型和支持代码的集合可以打包成一个容器，部署在微服务平台上，该平台负责运行容器并将请求路由到它们。
- en: '![CH08_F20_Tuulos](../../OEBPS/Images/CH08_F20_Tuulos.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F20_Tuulos](../../OEBPS/Images/CH08_F20_Tuulos.png)'
- en: Figure 8.20 Architecture of a typical model-hosting service
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 典型模型托管服务的架构
- en: 'Figure 8.20 illustrates the high-level architecture of a model hosting service:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20展示了模型托管服务的高级架构：
- en: A real-time prediction is produced by sending a request, such as over HTTP,
    to *a hosting endpoint*, which is hosted at an address like http://hosting-service/predict.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过发送请求，例如通过HTTP，到 *一个托管端点*，该端点托管在类似http://hosting-service/predict的地址，来生成实时预测。
- en: The endpoint decodes and validates the request and forwards it to a preprocessing
    function.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端点解码并验证请求，并将其转发到预处理函数。
- en: A preprocessing function is responsible for converting facts included in the
    request to features used by the deployed model. Often, the request itself doesn’t
    contain all the required data, but additional data is looked up from a database.
    For instance, the request may contain just a user ID, and the latest data related
    to the user is fetched from the DB.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理函数负责将请求中包含的事实转换为部署的模型使用的特征。通常，请求本身不包含所有所需数据，但需要从数据库中查找附加数据。例如，请求可能只包含一个用户ID，然后从数据库中获取与用户相关的最新数据。
- en: A feature vector/tensor is fed to the model, which produces a prediction. Sometimes,
    a model may be an ensemble of multiple models or even a complex graph of models.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个特征向量/张量被输入到模型中，该模型生成一个预测。有时，一个模型可能是由多个模型组成的集成，甚至是一个复杂的模型图。
- en: The prediction is processed by a postprocessing function to convert it to a
    suitable response format.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测通过后处理函数进行处理，以转换为合适的响应格式。
- en: A response is returned.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一个响应。
- en: Note that the architecture of a model-hosting service is not that different
    from a typical microservice that is not related to machine learning. You just
    need a container with a service that can receive requests, process some code on
    the fly, and return a response. For some data science use cases, it is quite feasible
    to use an existing microservice platform, such as Fargate on AWS or even Google
    App Engine for model serving.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型托管服务的架构与典型的非机器学习相关的微服务并没有太大区别。你只需要一个可以接收请求、即时处理一些代码并返回响应的容器。对于某些数据科学用例，使用现有的微服务平台，如AWS上的Fargate或Google
    App Engine进行模型托管是完全可行的。
- en: 'However, the following additional concerns may make it necessary to use a specialized
    model-serving platform or layer additional services on top of a general-purpose
    platform:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，以下额外的考虑可能使得使用专门的模型托管平台或在通用平台上添加额外服务成为必要：
- en: The models may be computationally demanding and require a lot of memory and
    specialized hardware like GPUs. Traditional microservice platforms may have a
    hard time accommodating such heavyweight services.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可能计算量较大，需要大量内存和专门的硬件，如GPU。传统的微服务平台可能难以容纳这样的重型服务。
- en: Models and their inputs and outputs need to be monitored in real time. Many
    *model-monitoring* solutions exist to help with this use case.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型和它们的输入输出需要实时监控。许多 *模型监控* 解决方案可以帮助处理这种情况。
- en: You want to use *a feature store* to take care of converting facts to features
    in a consistent manner. In figure 8.20, the feature store would replace or integrate
    to the preprocessing function and the database.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想使用 *一个特征存储库* 来以一致的方式处理将事实转换为特征。在图8.20中，特征存储库将替换或集成到预处理函数和数据库中。
- en: If you need a specialized platform, all major cloud providers provide model-serving
    solutions like Amazon Sagemaker Hosting or Google AI Platform. Or you can leverage
    open source libraries like Ray Serve ([ray.io](https://www.ray.io/)) or Seldon
    ([seldon.io](https://www.seldon.io/)).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个专门的平台，所有主要云提供商都提供模型托管解决方案，如Amazon Sagemaker Hosting或Google AI Platform。或者，你可以利用开源库，如Ray
    Serve ([ray.io](https://www.ray.io/)) 或 Seldon ([seldon.io](https://www.seldon.io/))。
- en: 'Example: Real-time movie recommendations'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：实时电影推荐
- en: Let’s practice real-time predictions using a minimal model-hosting service.
    This example expands our previous movie recommendation system by allowing recommendations
    to be produced based on a list of movies that can be updated in real time. For
    instance, you could use this system to produce real-time recommendations based
    on all movies that the user has been browsing recently.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用最小化模型托管服务来练习实时预测。此示例通过允许基于可以实时更新的电影列表生成推荐来扩展我们之前的电影推荐系统。例如，你可以使用这个系统根据用户最近浏览的所有电影生成实时推荐。
- en: 'The example is not meant to be production ready as such. Instead, it will demonstrate
    the following key concepts of a model-serving system, as outlined in figure 8.20:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例并非旨在用于生产环境，而是将展示图8.20中概述的模型托管系统的以下关键概念：
- en: A service endpoint that is accessible over HTTP
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可通过HTTP访问的服务端点
- en: A preprocessing function that uses the same featurization module as the training
    code
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用与训练代码相同的特征化模块的预处理函数
- en: Loading and using a model trained with a batch workflow
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和使用使用批量工作流程训练的模型
- en: Postprocessing the predictions in a decided output format
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以决定输出格式对预测进行后处理
- en: We will use a popular Python web framework, Flask ([flask.palletsprojects.com](https://flask.palletsprojects.com/en/2.1.x/)),
    to wrap the logic as a web service. The example can be easily adapted to any other
    web framework as well. The following code sample lists a fully functional model-hosting
    service.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用流行的Python Web框架Flask ([flask.palletsprojects.com](https://flask.palletsprojects.com/en/2.1.x/))，将逻辑封装为Web服务。此示例可以轻松地适应任何其他Web框架。以下代码示例列出了一个功能齐全的模型托管服务。
- en: Listing 8.10 Model-hosting service
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.10模型托管服务
- en: '[PRE14]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Uses helper modules from the previous section
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用上一节中的辅助模块
- en: ❷ The model helper class
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 模型辅助类
- en: ❸ Fetches the latest model ID
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 获取最新的模型ID
- en: ❹ Loads the model
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 加载模型
- en: ❺ Generates recommendations
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成推荐
- en: ❻ Produces recommendations for one set of movies
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 为一组电影生成推荐
- en: ❼ A helper function to map IDs to movie names
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个将ID映射到电影名称的辅助函数
- en: ❽ Returns the model version
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回模型版本
- en: ❾ Loads the model (this may take a few minutes)
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ⓱ 加载模型（这可能需要几分钟）
- en: ❿ Preprocess function to parse information from the request
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ⓱ 预处理函数以解析请求中的信息
- en: ⓫ Parses integer IDs from a comma-separated string
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 解析逗号分隔字符串中的整数ID
- en: ⓬ Outputs a version identifier of the model
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 输出模型的版本标识符
- en: ⓭ Postprocess function to output response
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 后处理函数以输出响应
- en: ⓮ Flask endpoint specification
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Flask端点规范
- en: ⓯ Processes input from the request
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ⓯ 处理请求的输入
- en: ⓰ Produces recommendations
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ⓰ 生成推荐
- en: ⓱ Finalizes and outputs response
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ⓰ 最终输出响应
- en: 'Save the code to movie_recs_server.py. To run the server, you need an execution
    environment that includes the libraries needed by the model. Because this is not
    a Metaflow workflow but a Flask app, we can’t use @conda as in earlier examples.
    Instead, you can create a suitable Conda environment manually by executing the
    following:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到movie_recs_server.py。要运行服务器，您需要一个包含模型所需库的执行环境。因为这不是Metaflow工作流程，而是Flask应用程序，所以我们不能像早期示例中那样使用@conda。相反，您可以通过执行以下命令手动创建一个合适的Conda环境：
- en: '[PRE15]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the environment has been activated, you can execute the service locally
    as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦环境被激活，您就可以按照以下方式在本地执行服务：
- en: '[PRE16]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It will take a minute or more to load the model and start the server. Once
    the server is up and running, you will see next output:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型并启动服务器可能需要一分钟或更长时间。一旦服务器启动并运行，您将看到以下输出：
- en: '[PRE17]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After this, you can start querying the server. To send an HTTP request to the
    server, open another terminal window where you can send requests to the server
    using the command-line client curl. You can browse movies.csv for interesting
    movie IDs and then query recommendations as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，您就可以开始查询服务器了。要向服务器发送HTTP请求，请打开另一个终端窗口，您可以在其中使用命令行客户端curl向服务器发送请求。您可以在movies.csv中浏览有趣的电影ID，然后按如下方式查询推荐：
- en: '[PRE18]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It takes about 50-100 ms to produce a response that looks like this:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 生成类似以下响应需要大约50-100毫秒：
- en: '[PRE19]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can use the num parameter to produce more recommendations:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用num参数来生成更多推荐：
- en: '[PRE20]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Congratulations! You have created a web service that produces recommendations
    in real time. Although the service works, you should consider a number of improvements
    to make the service fully production ready. First, on the infrastructure side
    consider the following:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经创建了一个能够实时生成推荐的Web服务。尽管服务运行正常，但您应该考虑一些改进，以便使服务完全准备好投入生产。首先，在基础设施方面，考虑以下方面：
- en: The service should be packaged in a Docker container so it can be deployed to
    a microservice platform.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务应打包在Docker容器中，以便可以部署到微服务平台。
- en: This service is able to handle only a single request at a time. You should consult
    the Flask documentation to learn how to deploy the app so that multiple requests
    can be handled in parallel.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此服务一次只能处理单个请求。您应该查阅Flask文档，了解如何部署应用程序以便并行处理多个请求。
- en: If even more scale is needed, you can run multiple containers in parallel. This
    requires a load balancer to route traffic to individual containers.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要更大的规模，您可以在并行运行多个容器。这需要一个负载均衡器将流量路由到各个容器。
- en: It is a good idea to capture logs and basic metrics about requests volumes,
    for which many off-the-shelf tools are available.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录请求量日志和基本指标是一个好主意，为此有许多现成的工具可用。
- en: 'Second, on the modeling side, consider adding the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在建模方面，考虑添加以下内容：
- en: A model monitoring solution to track model metrics in real time
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于实时跟踪模型指标的模型监控解决方案
- en: A solution to track data quality in requests to detect changes in the input
    data distribution
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于跟踪请求中的数据质量以检测输入数据分布变化的解决方案
- en: A service to manage A/B experiments
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于管理A/B测试的服务
- en: Most ML-specific tooling required by model deployments relate to debuggability
    and the quality of results. Imagine that a prediction returned by the service
    looked odd. The first question is, what model produced the predictions? To answer
    the question, we included the model version in each prediction response. Without
    a model version identifier, it would be impossible to know where the prediction
    originated, especially in a complex environment that may have multiple model versions
    deployed concurrently.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署所需的几乎所有ML特定工具都与调试性和结果质量相关。想象一下，服务返回的预测看起来很奇怪。第一个问题是，哪个模型产生了预测？为了回答这个问题，我们在每个预测响应中包含了模型版本。如果没有模型版本标识符，将无法知道预测的来源，尤其是在可能同时部署多个模型版本复杂环境中。
- en: Figure 8.21 illustrates the idea of model lineage.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21说明了模型血缘的概念。
- en: '![CH08_F21_Tuulos](../../OEBPS/Images/CH08_F21_Tuulos.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F21_Tuulos](../../OEBPS/Images/CH08_F21_Tuulos.png)'
- en: Figure 8.21 Backtracking model lineage from predictions to raw data
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21 从预测回溯到原始数据的模型血缘
- en: 'By using an architecture as shown in figure 8.21, we can track the lineage
    of predictions all the way back to the source data:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用如图8.21所示的架构，我们可以跟踪预测的血缘，一直回溯到源数据：
- en: Each prediction response should contain an ID denoting what deployment produced
    the response.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个预测响应都应该包含一个ID，表示哪个部署产生了响应。
- en: Each deployment, for example, a container running a certain version of the model,
    should get a unique ID.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，每个部署（例如，运行模型特定版本的容器）都应该获得一个唯一的ID。
- en: The container should know the ID of the model and the run that produced the
    model.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器应该知道模型的ID和生成模型的运行ID。
- en: Knowing the run ID, we can trace back to the data that was used to train the
    model.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道运行ID，我们可以回溯到用于训练模型的原始数据。
- en: You are now well equipped to make informed choices between real-time or precomputed
    batch prediction, and the frameworks to support them. When in doubt, steer toward
    the simplest possible approach that works.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经具备了在实时或预计算批量预测之间做出明智选择的能力，以及支持它们的框架。当不确定时，选择最简单可行的方法。
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: To produce value, machine learning models must be connected to other surrounding
    systems.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了产生价值，机器学习模型必须与其他周围系统连接。
- en: 'There isn’t a single way to deploy a data science application and produce predictions:
    the right approach depends on the use case.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署数据科学应用并生成预测没有唯一的方法：正确的方法取决于用例。
- en: Choose the right infrastructure for predictions, depending on the time window
    between when the input data becomes known and when predictions are needed.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据输入数据变得已知和需要预测之间的时间窗口，选择合适的预测基础设施。
- en: Another key consideration is whether surrounding systems need to request predictions
    from the model, or whether the model can push predictions to the surrounding systems.
    In the latter case, batch or streaming predictions are a good approach.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个关键考虑因素是周围系统是否需要从模型请求预测，或者模型是否可以将预测推送到周围系统。在后一种情况下，批量或流预测是一个好方法。
- en: If the input data is known at least 15-30 minutes before predictions are needed,
    it is often possible to produce predictions as a batch workflow, which is the
    most straightforward approach technically.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在需要预测之前至少15-30分钟知道输入数据，通常可以以批量工作流的形式生成预测，这是技术上最直接的方法。
- en: It is important to attach a version identifier in all model outputs, both in
    batch and real-time use cases.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在批量和实时用例中，所有模型输出都应附上版本标识符非常重要。
- en: Real-time predictions can be produced either using a general-purpose microservice
    framework or a solution that is tailored to data science applications. The latter
    may be the best approach if your models are computationally demanding.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时预测可以通过通用微服务框架或针对数据科学应用定制的解决方案来生成。如果您的模型计算需求高，后者可能是最佳方法。
- en: Make sure your deployments are debuggable by investing in monitoring tools and
    lineage. It should be possible to track every prediction all the way to the model
    and a workflow that produced it.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您的部署可以通过投资监控工具和溯源功能进行调试。应该能够追踪每个预测，直至模型及其生成的流程。
