- en: 'Appendix B. Three examples: Classification of image, text, and tabular data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B. 三个示例：图像、文本和表格数据的分类
- en: In chapter 2, we studied how to build an end-to-end ML pipeline to solve a regression
    problem with tabular (structured) data. This appendix provides three more examples
    aimed at getting you more familiar with the ML pipeline. The examples illustrate
    solving classification problems involving image, text, and tabular data with various
    classic ML models. If you’re not experienced with ML, you’ll learn how to prepare
    these different types of data for the models using tailored data-preprocessing
    methods. The problems presented here are also solved in the second part of the
    book with AutoML techniques. We’ll begin with an image classification problem.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们学习了如何构建一个端到端的机器学习管道来解决表格（结构化）数据的回归问题。本附录提供了三个额外的示例，旨在使你更熟悉机器学习管道。这些示例展示了使用各种经典机器学习模型解决涉及图像、文本和表格数据的分类问题。如果你不熟悉机器学习，你将学习如何使用定制的数据预处理方法为模型准备这些不同类型的数据。这里提出的问题也在书的第二部分使用自动化机器学习技术解决。我们将从一个图像分类问题开始。
- en: 'B.1 Image classification: Recognizing handwritten digits'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 图像分类：识别手写数字
- en: Our first problem is to recognize handwritten digits in images. Following the
    workflow of building an ML pipeline introduced in chapter 2, we begin by framing
    the problem and assembling the dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个问题是识别图像中的手写数字。遵循第二章中介绍的构建机器学习管道的工作流程，我们首先界定问题并组装数据集。
- en: B.1.1 Problem framing and data assembly
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.1 问题界定和数据组装
- en: This is a classification problem, because we assume the digit in each image
    can be only an integer in the range of 0 to 9\. Recognizing the digits is, therefore,
    equivalent to classifying the images into the right digit classes. We can further
    specify the problem as a *multiclass* *classification problem* because we have
    more than two different types of digits. If there are only two target classes,
    we call it a *binary* *classification problem*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个分类问题，因为我们假设每个图像中的数字只能是0到9范围内的整数。因此，识别数字等价于将图像分类到正确的数字类别中。由于我们有超过两种不同的数字类型，我们可以进一步将问题指定为*多类*分类问题。如果有两个目标类别，我们称之为*二元*分类问题。
- en: The data we will work with is the digits dataset that comes with the scikit-learn
    library, consisting of 1,797 images of handwritten digits with size 8×8\. We can
    load it with the load_digits() function, which we import from scikit-learn, as
    shown in the following listing. This returns the images and their corresponding
    labels (digits), which we store separately.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的数据是scikit-learn库附带的手写数字数据集，包含1,797个大小为8×8的手写数字图像。我们可以使用load_digits()函数加载它，该函数来自scikit-learn，如下所示。这将返回图像及其相应的标签（数字），我们将它们分别存储。
- en: Listing B.1 Loading the digits dataset
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.1 加载数字数据集
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Loads the digits dataset
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数字数据集
- en: ❷ Separately stores the images and corresponding targets
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分别存储图像和相应的目标
- en: 'The loaded data contains 1,797 images grouped into a three-dimensional array
    of shape 1797×8×8\. Each element in the array is an integer value between 0 and
    16, corresponding to a pixel in an image. The second and third dimensions are
    the height and width of the images, respectively, as shown next:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 加载的数据包含1,797个图像，分为一个形状为1797×8×8的三维数组。数组中的每个元素是一个介于0到16之间的整数，对应于图像中的一个像素。第二和第三维度分别是图像的高度和宽度，如下所示：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In practice, the images you’re dealing with may have different sizes and resolutions,
    requiring crop and resize operations to align them. The dataset in this example
    is already in good shape, so we can move on to some exploratory data analysis
    (EDA) to prepare it for the ML model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你处理的图像可能具有不同的尺寸和分辨率，需要裁剪和调整大小操作以对齐它们。本例中的数据集已经处于良好状态，因此我们可以继续进行一些探索性数据分析（EDA），为机器学习模型做准备。
- en: B.1.2 Exploring and preparing the data
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.2 探索和准备数据
- en: We can visualize the first 20 samples using the code in listing B.2 to get a
    glimpse of what the data looks like. The first 20 images are shown in figure B.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用B.2列表中的代码可视化前20个样本，以了解数据的外观。前20个图像显示在图B.1中。
- en: Listing B.2 Visualizing the first 20 digit images and their labels
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.2 可视化前20个数字图像及其标签
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Creates a figure and 20 subplots with layout 2×10
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个图和20个子图，布局为2×10
- en: ❷ Automatically adjusts the subplot parameters to give the specified padding
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 自动调整子图参数以提供指定的填充
- en: ❸ Plots the first 20 digit images
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制前20个数字图像
- en: '![B-01](../Images/B-01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![B-01](../Images/B-01.png)'
- en: Figure B.1 Visualization of the first 20 digit images and their labels
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.1 前20位数字图像及其标签的可视化
- en: 'Many classification algorithms cannot be directly applied to two-dimensional
    images, so next we reshape each image into a vector, as shown in the following
    code snippet. The reshaped data becomes a two-dimensional array of shape 1797×64\.
    Each row vector in the array represents an image:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分类算法不能直接应用于二维图像，因此接下来我们将每个图像重塑为一个向量，如下面的代码片段所示。重塑后的数据变成了一个形状为1797×64的二维数组。数组中的每一行向量代表一个图像：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The reshaped data has a similar format to the tabular data we worked with in
    chapter 2, but each image has 64 features (compared to 8 in the California housing
    example). More features usually translates to more computational resources and
    more time spent on model training. It may also make it harder to learn the ML
    model and extract useful classification patterns from the data. We have many feature
    engineering methods to deal with this issue, one of which is introduced in the
    next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重新塑形的数据格式与我们在第二章中处理过的表格数据类似，但每个图像有64个特征（与加利福尼亚住房示例中的8个特征相比）。更多的特征通常意味着需要更多的计算资源和更多的时间用于模型训练。这也可能使得学习机器学习模型和从数据中提取有用的分类模式变得更加困难。我们有许多特征工程方法来处理这个问题，其中之一将在下一节中介绍。
- en: B.1.3 Using principal component analysis to condense the features
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.3 使用主成分分析来压缩特征
- en: 'Before performing the feature engineering, let’s first hold out 20% of the
    data to use as the final testing set, as shown next. This will help prevent overfitting
    to the training data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行特征工程之前，让我们首先保留20%的数据作为最终的测试集，如下所示。这将有助于防止过度拟合训练数据：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A natural solution to reduce the number of features in the data is to select
    a percentage of them. However, image features (pixels) often do not have practical
    meaning, which makes it hard to formulate assumptions about which ones to select.
    Naively selecting some of the features may spoil some images and affect the performance
    of the classification algorithm. For example, suppose the digits can appear either
    on the left or the right in any given image. Their labels will be the same, no
    matter which side they appear on. But if you remove the left half of all the images,
    some of the images will no longer contain a digit, leading to a loss of information
    that is useful for classification.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 减少数据中特征数量的一个自然解决方案是选择其中的一部分。然而，图像特征（像素）通常没有实际意义，这使得很难制定关于选择哪些特征的选择假设。天真地选择一些特征可能会破坏一些图像并影响分类算法的性能。例如，假设数字可以出现在任何给定图像的左侧或右侧。无论它们出现在哪一侧，它们的标签都将相同。但如果你移除所有图像的左侧一半，一些图像将不再包含数字，导致丢失对分类有用的信息。
- en: One classic method for reducing the number of features in images while retaining
    as much information as possible is *principal component analysis* (PCA). It tries
    to linearly transform the original features into fewer features by fitting an
    ellipsoid to the data (see figure B.2) and creating a low-dimensional space based
    on the axes of the ellipsoid. The axes are pointed out by a set of orthogonal
    vectors called *principal components*. Based on these components, we can redefine
    the coordinate space and represent each data point with the new axes. The coordinate
    values of each data point are its new features. We carry out the dimensionality
    reduction by selecting some of the principal components, using the axes they point
    out to form a subspace, and then projecting the data onto the new space. The selection
    of the subspace is based on the variances of the new features within the whole
    dataset. The variance is visualized by the lengths of the ellipsoid axes in the
    figure. To preserve more information in the dataset, we select the principal components
    corresponding to the largest variances. We can choose the number of principal
    components empirically, based on how much of the whole feature variance they can
    preserve.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一种经典的方法是在尽可能保留信息的同时，减少图像中的特征数量，这种方法是*主成分分析*（PCA）。它试图通过拟合一个椭球到数据上（见图 B.2），并基于椭球的轴创建一个低维空间，将原始特征线性变换成更少的特征。这些轴由称为*主成分*的一组正交向量指出。基于这些成分，我们可以重新定义坐标空间，并用新的轴来表示每个数据点。每个数据点的坐标值是其新的特征。我们通过选择一些主成分来执行降维，使用它们指向的轴形成一个子空间，然后将数据投影到新的空间上。子空间的选择基于整个数据集中新特征的方差。图中的椭球轴长度表示了方差。为了在数据集中保留更多信息，我们选择对应于最大方差的主成分。我们可以根据它们可以保留多少整体特征方差来经验性地选择主成分的数量。
- en: In figure B.2, we can see that if the number of components we select is the
    same as the original number of features, doing the PCA transformation is the same
    as rotating the coordinates and mapping the data onto the new coordinate system
    spanned by the two principal components. If we choose to use one component, it
    will project the points onto the longer axis of the ellipse (the first PCA dimension
    in figure B.2).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 B.2 中，我们可以看到，如果我们选择的主成分数量与原始特征数量相同，进行 PCA 变换就等同于旋转坐标并将数据映射到由两个主成分构成的新坐标系上。如果我们选择使用一个成分，它将点投影到椭圆的长轴上（图
    B.2 中的第一个 PCA 维度）。
- en: '![B-02](../Images/B-02.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![B-02](../Images/B-02.png)'
- en: 'Figure B.2 Illustration of PCA: The two principal components of a cloud of
    data points with two features'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.2 PCA 示例：具有两个特征的点云的两个主成分
- en: For the digit images, we select 10 features here as an example, and implement
    PCA with scikit-learn as follows.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数字图像，我们在这里选择 10 个特征作为示例，并使用 scikit-learn 实现PCA，如下所示。
- en: Listing B.3 Applying PCA on training data
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.3 在训练数据上应用 PCA
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Imports the PCA model
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 PCA 模型
- en: ❷ Fits the model with the training data
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用训练数据拟合模型
- en: ❸ Transforms the training data to the lower-dimensional space
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将训练数据转换到低维空间
- en: 'For our training data, the shapes of the original feature matrix and the transformed
    feature matrix are (1437, 64) and (1437, 10), respectively, as shown here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的训练数据，原始特征矩阵和转换后的特征矩阵的形状分别是（1437, 64）和（1437, 10），如图所示：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To fit the PCA model, we need to input only X_train without providing the target
    labels. This is different from the supervised learning paradigm, where we need
    to provide some targets to train the ML models. This learning paradigm is called
    *unsupervised learning*. It aims to find undetected patterns or learn hidden transformations
    directly from the features without human supervision, such as labels. Unsupervised
    learning models such as PCA are very helpful for EDA, enabling us to uncover patterns
    in the data. For example, getting back to our problem, we can project the data
    into the two-dimensional space spanned by the first two principal components and
    color them with corresponding labels to visualize patterns in the training data
    like so (and as depicted in figure B.3):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要拟合 PCA 模型，我们只需要输入 X_train，而不需要提供目标标签。这与监督学习范式不同，在监督学习中，我们需要提供一些目标来训练机器学习模型。这种学习范式被称为
    *无监督学习*。它的目的是直接从特征中找到未检测到的模式或学习隐藏的转换，而不需要人类监督，例如标签。PCA 等无监督学习模型对于 EDA 非常有帮助，使我们能够揭示数据中的模式。例如，回到我们的问题，我们可以将数据投影到由前两个主成分构成的两个维空间中，并用相应的标签进行着色，以可视化训练数据中的模式，如下所示（如图
    B.3 所示）：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![B-03](../Images/B-03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![B-03](../Images/B-03.png)'
- en: Figure B.3 Visualization of training data in 2-D space after the PCA transformation
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.3 展示了 PCA 变换后 2 维空间中的训练数据可视化
- en: The visualization shows a clustering pattern among the training images. That
    is, the images within the same digit class tend to be closer to each other in
    this projected two-dimensional space. Now that we’ve condensed the features, it’s
    time to select an ML model to build up the classification algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化显示了训练图像之间的聚类模式。也就是说，同一数字类别的图像在这个投影的两个维空间中往往彼此更接近。现在我们已经压缩了特征，是时候选择一个机器学习模型来构建分类算法了。
- en: B.1.4 Classification with a support vector machine
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.4 使用支持向量机进行分类
- en: This section introduces one of the most popular classification models, the *support
    vector machine* (SVM), and two hyperparameters that we can tune to improve its
    performance. The simplest version of SVM is a *linear* SVM for two-class classification.
    We’ll use binary classification as an example and assume the data has two features.
    The main idea of a linear SVM model is to find a line to separate the two classes
    of points and maximize the margin between them. The instances located on the boundaries
    of the margin (e.g., instances A and B in figure B.4(a)) are called *support vectors*
    because they “support” the two boundary lines of the margin. Suppose the two classes
    are directly separable, which means we can find a line that ensures all the training
    instances within the same class are located on the same side. In that case, we
    call the margin a *hard margin*. Otherwise, we can achieve only a *soft margin*,
    which is tolerant of some violations. A hyperparameter C can be used to decide
    the strength of this tolerance. Adjusting this hyperparameter is useful for a
    linear SVM, especially when the two classes are nearly linearly separable. For
    example, we can reduce C to increase the tolerance and achieve a larger margin,
    as shown in figure B.4(c).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了最流行的分类模型之一，即 *支持向量机*（SVM），以及我们可以调整以改进其性能的两个超参数。SVM 的最简单版本是用于二分类的 *线性* SVM。我们将以二分类为例，并假设数据有两个特征。线性
    SVM 模型的主要思想是找到一个线来分隔两个类别的点，并最大化它们之间的间隔。位于间隔边界上的实例（例如，图 B.4(a) 中的实例 A 和 B）被称为 *支持向量*，因为它们“支撑”间隔的两个边界线。假设两个类别是可以直接分隔的，这意味着我们可以找到一个线确保同一类别的所有训练实例都位于同一侧。在这种情况下，我们称这个间隔为
    *硬间隔*。否则，我们只能达到 *软间隔*，它可以容忍一些违规。超参数 C 可以用来决定这种容忍的强度。调整这个超参数对于线性 SVM 很有用，尤其是在两个类别几乎线性可分时。例如，我们可以减小
    C 的值来增加容忍度，从而实现更大的间隔，如图 B.4(c) 所示。
- en: '![B-04](../Images/B-04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![B-04](../Images/B-04.png)'
- en: Figure B.4 An illustration of SVM for hard margin classification (a) and soft
    margin classification with different values for the hyperparameter C (b and c).
    The size of C here is only for illustration purposes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.4 展示了 SVM 对硬间隔分类（a）和具有不同超参数 C 值的软间隔分类（b 和 c）。这里的 C 值仅用于说明。
- en: Sometimes the dataset is not even close to being linearly separable (as shown
    in figure B.5(a)). In this case, we can use a *nonlinear* SVM. The main idea of
    a nonlinear SVM is to augment the number of features by mapping the original features
    into a higher-dimensional feature space so that the instances can become more
    linearly separable (see figure B.5(b)). We can make this transformation implicitly
    with the help of a mathematical technique called the *kernel trick*. It applies
    a function called a *kernel function* to directly compute the similarity between
    the instances in the new feature space without explicitly creating the new features,
    thus greatly improving the efficiency of the SVM algorithm.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据集甚至接近线性不可分（如图B.5(a)所示）。在这种情况下，我们可以使用*非线性*支持向量机。非线性支持向量机的主要思想是通过将原始特征映射到更高维的特征空间来增加特征的数量，从而使实例可以变得更加线性可分（见图B.5(b)）。我们可以通过一种称为*核技巧*的数学技术隐式地进行这种转换。它应用一个称为*核函数*的函数，直接计算新特征空间中实例之间的相似性，而不需要显式创建新特征，从而大大提高了支持向量机算法的效率。
- en: '![B-05](../Images/B-05.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![B-05](../Images/B-05.png)'
- en: Figure B.5 Making linearly inseparable 2-D data linearly separable by transforming
    it into a 3-D space
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.5 通过将其转换为3-D空间使线性不可分的2-D数据线性可分
- en: SVMs were originally designed for binary, or two-class, classification. To generalize
    this approach for multiclass classification, we can use the common approach of
    a *one-versus-one* (OvO) scheme. The scheme selects two classes from the full
    set of classes and builds a binary SVM classifier on this pair. Repeating this
    process for each pair of classes results in ![B-05-EQ01](../Images/B-05-EQ01.png) classifiers,
    where *c* is the number of classes. During the testing phase, all the binary SVM
    classifiers are tested. Each one will classify the current example into one of
    the two classes with which it’s trained, meaning every example will receive ![B-05-EQ01](../Images/B-05-EQ01.png) votes
    across all the classes. The final class that a sample belongs to is the class
    receiving the most votes. The scikit-learn implementation of a multiclass linear
    SVM classifier is shown in the following example. The kernel parameter specifies
    the type of kernel to use.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机最初是为二元或双类分类设计的。为了将这种方法推广到多类分类，我们可以使用常见的*一对多*（OvO）方案。该方案从所有类中选择两个类，并在此对上构建一个二元支持向量机分类器。对每一对类重复此过程，将得到![B-05-EQ01](../Images/B-05-EQ01.png)个分类器，其中*c*是类的数量。在测试阶段，所有二元支持向量机分类器都会被测试。每个分类器将当前示例分类为它所训练的两个类中的一个，这意味着每个示例将在所有类中收到![B-05-EQ01](../Images/B-05-EQ01.png)票。样本最终所属的类是获得最多票的类。以下示例展示了scikit-learn实现的多类线性支持向量机分类器。核参数指定了要使用的核类型。
- en: Listing B.4 Building and training an SVM classifier
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.4 构建和训练SVM分类器
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Imports the SVM classifier module
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入SVM分类器模块
- en: ❷ Creates a support vector classifier with a linear kernel
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用线性核创建支持向量分类器
- en: 'We use the accuracy score to evaluate our model. The prediction accuracy is
    defined as the number of correctly classified samples divided by the total number
    of test samples, as shown here:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用准确度分数来评估我们的模型。预测精度定义为正确分类的样本数除以测试样本总数，如下所示：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The classification accuracy can be more comprehensively visualized with a *confusion
    matrix*, which displays the number of correct and incorrect predictions in each
    class like so:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用*混淆矩阵*更全面地可视化分类精度，该矩阵显示每个类中正确和错误预测的数量，如下所示：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Each row in the confusion matrix corresponds to a true label, and each column
    is a predicted label. For example, the first row in figure B.6 corresponds to
    the true label 0, and the first column is the predicted label 0\. The elements
    in a row indicate how many instances with that label were predicted to have each
    of the possible labels. The summation of each row equals the total number of instances
    in the test set with that label. The diagonal of the confusion matrix indicates
    the number of correct labels predicted for each class. In figure B.6, you can
    see that the created classifier performs worst when classifying test images with
    the label 3: it misclassifies six images with the true label 3 as 8s.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵中的每一行对应一个真实标签，每一列对应一个预测标签。例如，图B.6中的第一行对应真实标签0，第一列是预测标签0。行中的元素表示具有该标签的实例被预测为每个可能标签的数量。每一行的总和等于测试集中具有该标签的实例总数。混淆矩阵的对角线表示每个类别正确预测的标签数量。在图B.6中，你可以看到创建的分类器在分类带有标签3的测试图像时表现最差：它将六个真实标签为3的图像错误地分类为8。
- en: '![B-06](../Images/B-06.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![B-06](../Images/B-06.png)'
- en: Figure B.6 Confusion matrix of linear SVM Classifier
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.6 线性SVM分类器的混淆矩阵
- en: For more details about the commonly used metrics for model evaluation, like
    F1-score, precision, and recall, you may refer to the book of *Deep Learning with
    Python* by François Chollet.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型评估中常用的指标，如F1分数、精确度和召回率，你可以参考弗朗索瓦·肖莱特的《Python深度学习》一书中的更多细节。
- en: For greater convenience in data processing and hyperparameter tuning, we can
    combine the steps of applying PCA for feature engineering and an SVM for classification
    into an integrated pipeline.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在数据处理和超参数调整方面更加方便，我们可以将应用PCA进行特征工程和SVM进行分类的步骤组合成一个集成管道。
- en: B.1.5 Building a data-processing pipeline with PCA and SVMPCA (principal component
    analysis)
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.5 使用PCA和SVM构建数据预处理管道（主成分分析）
- en: scikit-learn provides a simple pipeline module that we can use to assemble multiple
    data-processing components sequentially. The code for building up a sequential
    pipeline with two components (PCA followed by SVM) is shown in the following listing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了一个简单的管道模块，我们可以用它来按顺序组装多个数据处理组件。以下列表显示了使用两个组件（PCA后跟SVM）构建顺序管道的代码。
- en: Listing B.5 Building a scikit-learn pipeline with PCA and SVM
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.5 使用PCA和SVM构建scikit-learn管道
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Builds the image classification pipeline and assigns a name for each component
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建图像分类管道并为每个组件分配名称
- en: ❷ Trains the pipeline
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练管道
- en: ❸ Tests the pipeline
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 测试管道
- en: 'If we check the test accuracy, as shown next, we see that it’s lower than when
    we applied the SVM without PCA:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查测试准确率，如下所示，我们会看到它低于我们应用SVM而不使用PCA时的准确率：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we saw earlier, although PCA reduces the number of features, it may also
    remove some discriminating information that is useful for classification. That’s
    likely what has happened here. We shouldn’t deny the usefulness of PCA in condensing
    the features in our data, but we should always consider the tradeoff between accuracy
    and simplicity when designing a pipeline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所看到的，尽管PCA减少了特征数量，但它也可能移除一些对分类有用的区分信息。这很可能是这里发生的情况。我们不应该否认PCA在压缩我们数据中的特征方面的有用性，但在设计管道时，我们应始终考虑准确性和简单性之间的权衡。
- en: Now, to improve the classification accuracy, let’s try to tune both the PCA
    and SVM models in the pipeline.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了提高分类准确率，让我们尝试调整管道中的PCA和SVM模型。
- en: B.1.6 Jointly tuning multiple components in the pipeline
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.6 联合调整管道中的多个组件
- en: In this example we have a scikit-learn pipeline with two different components,
    each of which may have multiple hyperparameters to tune. For example, we can tune
    both the C hyperparameter and the kernel type for the SVM classifier. Tuning a
    pipeline with scikit-learn is almost the same as tuning a single model. The only
    difference is how the names of hyperparameters in the search space are defined.
    To differentiate the hyperparameters in different pipeline components, we add
    a prefix to each one’s name to indicate which component the hyperparameter belongs
    to (as in ComponentName_HyperparameterName). Then we can perform a grid search
    on all the hyperparameters by feeding in the whole pipeline.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们有一个包含两个不同组件的scikit-learn管道，每个组件可能有多个超参数需要调整。例如，我们可以调整SVM分类器的C超参数和核类型。使用scikit-learn调整管道几乎与调整单个模型相同。唯一的区别是搜索空间中超参数名称的定义方式。为了区分不同管道组件中的超参数，我们给每个超参数的名称添加一个前缀，以指示该超参数属于哪个组件（如ComponentName_HyperparameterName）。然后我们可以通过输入整个管道来对所有超参数进行网格搜索。
- en: Listing B.6 Jointly tuning three hyperparameters
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.6 联合调整三个超参数
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Creates a dictionary as the hyperparameter search space
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个字典作为超参数搜索空间
- en: ❷ Constructs a scoring function for performance estimation
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建一个评分函数以进行性能估计
- en: ❸ Creates the grid search object for the whole pipeline with three-fold CV
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用三折交叉验证创建整个管道的网格搜索对象
- en: ❹ Fits the grid search object to the training data to search for the optimal
    model
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将网格搜索对象拟合到训练数据以搜索最佳模型
- en: 'We can then print the best combination of hyperparameters and retrieve the
    corresponding pipeline for final testing. The final testing result is much better
    than we achieved with the previous pipeline, in line with our expectations, as
    shown here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以打印最佳超参数组合并检索相应的管道进行最终测试。最终测试结果比我们使用先前管道获得的结果要好得多，符合我们的预期，如下所示：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this example, we explored a classic image-classification problem and learned
    how to stack together and jointly tune different components (models) in an ML
    pipeline. In the next section, we’ll turn to another important data type in ML
    applications—text data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们探索了一个经典图像分类问题，并学习了如何在ML管道中堆叠和联合调整不同的组件（模型）。在下一节中，我们将转向ML应用中的另一种重要数据类型——文本数据。
- en: 'B.2 Text classification: Classifying topics of newsgroup posts'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 文本分类：对新闻组帖子进行主题分类
- en: In this section, we focus on a classification example with text data. In contrast
    with image data and tabular data, with text data we have consider semantic meaning
    and stronger dependencies between the features (words). We will use the 20 newsgroups
    dataset for our exploration here, which can be fetched with the scikit-learn library.
    It contains 18,846 newsgroup posts on 20 topics ([http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)).
    You’ll learn how to preprocess text data for use with *probabilistic classification
    models*, which use statistical principles for classification.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们专注于一个文本数据的分类示例。与图像数据和表格数据相比，在文本数据中，我们需要考虑特征（单词）之间的语义意义和更强的依赖关系。我们将使用20个新闻组数据集进行探索，该数据集可以通过scikit-learn库获取。它包含关于20个主题的18,846个新闻组帖子([http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/))。你将学习如何对文本数据进行预处理，以便与使用统计原理进行分类的*概率分类模型*一起使用。
- en: B.2.1 Problem framing and data assembly
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.1 问题定义和数据组装
- en: As usual, we start by framing the problem and assembling the dataset. As shown
    in the next listing, this is a multiclass classification problem, with 20 classes
    representing different newsgroup topics. Our goal is to predict the topic to which
    a previously unseen post belongs. The data is downloaded with the built-in data
    loader fetch_ 20newsgroups in scikit-learn. It has already been split into training
    and test sets for ease of use.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们首先定义问题并组装数据集。如下所示，这是一个多类分类问题，有20个类别代表不同的新闻组主题。我们的目标是预测一个先前未见过的帖子所属的主题。数据是通过scikit-learn库内置的数据加载器fetch_20newsgroups下载的。它已经被分为训练集和测试集，以便于使用。
- en: Listing B.7 Loading the 20 newsgroup dataset via the scikit-learn library
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.7 通过scikit-learn库加载20个新闻组数据集
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Loads the training and test data separately and shuffles the data in each
    of them
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 分别加载训练数据和测试数据，并对每个中的数据进行洗牌
- en: ❷ Separately stores the text documents and corresponding targets
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分别存储文本文档和相应的目标
- en: 'A quick inspection shows that doc_train and doc_test are lists of 11,314 and
    7,532 documents, respectively, as shown here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 快速检查显示，doc_train和doc_test分别是11,314和7,532个文档的列表，如下所示：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s print a sample document to get a sense of what the raw text features
    look like. Each document is a string containing letters, numbers, punctuation,
    and some special characters, as shown in the following code sample:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印一个样本文档，以了解原始文本特征的样子。每个文档都是一个包含字母、数字、标点和一些特殊字符的字符串，如下面的代码示例所示：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The raw text is not something we can input to an ML model directly, so let’s
    do some data preparation to make it neater.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文本不是我们可以直接输入到机器学习模型中的东西，所以让我们做一些数据准备，使其更整洁。
- en: B.2.2 Data preprocessing and feature engineering
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.2 数据预处理和特征工程
- en: The current text documents are in a string format, so we’ll start by converting
    the strings into numeric vectors that we can feed to our ML algorithms.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的文本文档是以字符串格式存在的，所以我们将首先将这些字符串转换为数值向量，这样我们就可以将其输入到我们的机器学习算法中。
- en: In general, all the words, special characters, and punctuation can be features
    of a text document. We can group multiple words and/or characters as one feature
    (called a “word” or “term”) and deal with them jointly. An intuitive way to numerically
    encode the features of the dataset is to collect all the unique words appearing
    in all the documents of the corpus and convert each document into a numeric vector,
    where the length of the vector equals the number of unique words it contains.
    Each element in the vector denotes the occurrence of the corresponding word in
    this document. This transformation method is called the *bag of words* (BoW) approach
    (shown in figure B.7).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，所有单词、特殊字符和标点都可以作为文本文档的特征。我们可以将多个单词和/或字符组合成一个特征（称为“词”或“术语”）并共同处理。对数据集的特征进行数值编码的一种直观方法是收集语料库中所有文档中出现的所有唯一单词，并将每个文档转换为包含其包含的唯一单词数量的数值向量。向量中的每个元素表示该文档中相应单词的出现次数。这种转换方法称为“词袋”（BoW）方法（如图B.7所示）。
- en: '![B-07](../Images/B-07.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![B-07](../Images/B-07.png)'
- en: Figure B.7 Transforming a document into a BoW representation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.7 将文档转换为BoW表示
- en: To implement the approach, we need to first partition each document into a set
    of words (*tokens*). This process is called *tokenization*. The tokenization and
    BoW transformation can be done together with a single call of the built-in class
    CountVectorizer in scikit-learn, as follows.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这种方法，我们首先需要将每个文档划分为一组单词（*标记*）。这个过程称为*标记化*。标记化和BoW转换可以通过scikit-learn内置类CountVectorizer的单次调用一起完成，如下所示。
- en: Listing B.8 Transforming the training documents to a BoW representation
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.8 将训练文档转换为BoW表示
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Constructs a CountVectorizer object
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建一个CountVectorizer对象
- en: ❷ Converts the training text documents to a matrix of token counts
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将训练文本文档转换为标记计数的矩阵
- en: 'By printing the shape of the transformed training documents, as shown next,
    we can see that the transformed matrix has 130,107 columns representing the BoW
    extracted from the training documents:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印转换后的训练文档的形状，如下所示，我们可以看到转换后的矩阵有130,107列，代表从训练文档中提取的BoW：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The BoW approach helps us convert our collection of string documents into a
    matrix of token counts that can be fed into an ML algorithm. However, the number
    of the occurrence of words in the documents may not be good features to be used
    directly, due to the following two problems:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: BoW方法帮助我们将我们的字符串文档集合转换为标记计数的矩阵，可以输入到机器学习算法中。然而，由于以下两个问题，文档中单词的出现次数可能不是直接使用的良好特征：
- en: Documents are of different lengths. A word that occurs the same number of times
    in two documents may not have the same importance in both of them.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档的长度不同。在两个文档中发生相同次数的词可能在这两个文档中的重要性并不相同。
- en: Some words may have a very high rate of occurrence in the whole text corpus,
    such as “the” and “a.” They often carry little meaningful information for classification
    compared with some low-frequency words.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些词在整个文本语料库中可能具有非常高的出现频率，例如“the”和“a”。与一些低频词相比，它们在分类中携带的意义信息很少。
- en: To address these problems, we often use a feature transformation technique named
    *Term Frequency-Inverse Document Frequency* (TF-IDF). The core idea is to evaluate
    how important a word is in a document by calculating its frequency (usually just
    a raw count) and dividing it by the number of documents in the corpus that contain
    this word. Terms that occur very frequently in a document but less frequently
    across the whole corpus are considered to be more important for that document
    than words that occur frequently, not only in that document but also in a large
    number of other documents.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们通常使用一种称为 *词频-逆文档频率* (TF-IDF) 的特征转换技术。其核心思想是通过计算一个词在文档中的频率（通常只是一个原始计数）并将其除以包含该词的语料库中的文档数量来评估该词在文档中的重要性。在文档中频繁出现但在整个语料库中较少出现的术语被认为比在文档中频繁出现且在大量其他文档中也频繁出现的词对该文档更重要。
- en: 'The transformation can be implemented with the TfidfTransformer class in scikit-learn
    as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换可以通过 scikit-learn 中的 TfidfTransformer 类实现，如下所示：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Despite the advantages of TF-IDF, it also suffers from some problems, such as
    high computational complexity and limited power to capture the semantic similarities
    between words. Further exploration is left as an exercise for the reader. Another
    approach, using *word embeddings*, is introduced in chapter 3.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TF-IDF具有优势，但它也存在一些问题，例如计算复杂度高以及有限的能力来捕捉词语之间的语义相似性。进一步的探索留给读者作为练习。另一种方法，使用
    *词嵌入*，在第3章中介绍。
- en: Our next step is to build a text classifier and train it with the prepared dataset.
    The following two sections introduce two probabilistic classifiers that can be
    used to address this problem. Both of them are fundamental ML models that have
    been widely used in the areas of text classification and beyond.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的步骤是构建一个文本分类器，并用准备好的数据集对其进行训练。接下来的两节介绍了两种可以用来解决这个问题概率分类器。这两个都是基本的机器学习模型，在文本分类以及其他领域得到了广泛的应用。
- en: B.2.3 Building a text classifier with the logistic regression model
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.3 使用逻辑回归模型构建文本分类器
- en: The first model we will explore is the *logistic regression* *model*. From the
    name, you may think it is a regression model similar to the linear regression
    model introduced in chapter 2, but, in fact, it is designed for classification
    problems. The difference between the linear regression model and the logistic
    regression model is in their outputs. Whereas the linear regression model directly
    outputs a predicted target value based on a sample’s features, the logistic regression
    model outputs the *logits* of the sample. A logit is defined as ![B-07-EQ03](../Images/B-07-EQ03.png),
    where *p* is the probability of a sample belonging to a specific class. We can
    estimate this probability by counting the frequency of labels of each class in
    the training set. During the testing phase, given an unseen example, we can calculate
    the probability of it belonging to each of the classes based on its features and
    assign it to the class with the highest probability.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的第一个模型是 *逻辑回归* 模型。从名称上看，你可能认为它是一个类似于第2章中介绍的线性回归模型，但实际上，它是为分类问题设计的。线性回归模型和逻辑回归模型之间的区别在于它们的输出。线性回归模型直接根据样本的特征输出预测的目标值，而逻辑回归模型输出样本的
    *logits*。logit 定义为 ![B-07-EQ03](../Images/B-07-EQ03.png)，其中 *p* 是样本属于特定类别的概率。我们可以通过计算训练集中每个类别的标签频率来估计这个概率。在测试阶段，给定一个未见过的例子，我们可以根据其特征计算它属于每个类别的概率，并将其分配给概率最高的类别。
- en: For multiclass classification, we can use the OvO scheme (similar to the multiclass
    SVM model), or we can use another scheme called *one-versus-rest* (OvR). Unlike
    the OvO scheme, which needs to build a classifier for each pair of classes, the
    OvR scheme requires building *n* classifiers, where *n* is the number of classes.
    Each classifier corresponds to one of the classes. It then tries to discriminate
    the examples in each class from the remaining *n*-1 classes. In the testing phase,
    we apply all *n* classifiers to an unseen example to get the sample’s probabilities
    for every class. The example is assigned to the class with the highest probability.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类分类，我们可以使用 OvO 方案（类似于多类 SVM 模型），或者我们可以使用另一种称为 *一对余* (OvR) 的方案。与需要为每一对类别构建分类器的
    OvO 方案不同，OvR 方案需要构建 *n* 个分类器，其中 *n* 是类别的数量。每个分类器对应一个类别。然后它试图将每个类别的例子与剩余的 *n*-1
    个类别区分开来。在测试阶段，我们将所有 *n* 个分类器应用于一个未见过的例子，以获取样本每个类别的概率。该例子被分配给概率最高的类别。
- en: 'We can construct a multiclass logistic regression classifier and perform training
    with the preprocessed documents using the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码构建一个多类逻辑回归分类器，并使用预处理后的文档进行训练：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then apply the same transformers on the test data and evaluate the learned
    logistic regression classifier as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将相同的转换器应用于测试数据，并如下评估学习到的逻辑回归分类器：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The test accuracy is 82.78%. You can also plot the confusion matrix if you’re
    interested in more detailed classification results across different labels.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率为82.78%。如果你对跨不同标签的更详细的分类结果感兴趣，也可以绘制混淆矩阵。
- en: B.2.4 Building a text classifier with the naive Bayes model
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.4 使用朴素贝叶斯模型构建文本分类器
- en: Another well-known probabilistic model that is often used for text classification
    is the *naive Bayes model*. It applies Bayes’ theorem to calculate the probability
    that a given data point belongs to each class. The “naive” here means we assume
    all the features are independent of each other. For example, in this news topic
    classification problem, we assume the probability of each term occurring in a
    specific topic class is independent of other terms. That assumption might sound
    unreasonable to you, especially for text data, but it often works quite well in
    practice.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用于文本分类的著名概率模型是*朴素贝叶斯模型*。它将贝叶斯定理应用于计算给定数据点属于每个类的概率。“朴素”在这里意味着我们假设所有特征都是相互独立的。例如，在这个新闻主题分类问题中，我们假设每个术语出现在特定主题类中的概率与其他术语独立。这个假设可能对你来说听起来不太合理，尤其是对于文本数据，但在实践中它通常工作得相当好。
- en: 'When applying the naive Bayes model, we need to perform the following two operations
    during the training phase:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用朴素贝叶斯模型时，我们需要在训练阶段执行以下两个操作：
- en: Count the occurrences of each term that appears in each class in the training
    set, and divide it by the total number of terms in this class. This serves as
    the estimation of the probability ![B-07-EQ4](../Images/B-07-EQ4.png).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算训练集中每个类中出现的每个术语的出现次数，并将其除以该类中术语的总数。这作为概率![B-07-EQ4](../Images/B-07-EQ4.png)的估计。
- en: Count the frequency of each class that appears in the training set, which is
    the estimation of ![B-07-EQ5](../Images/B-07-EQ5.png).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算训练集中每个类出现的频率，这是![B-07-EQ5](../Images/B-07-EQ5.png)的估计。
- en: 'The naive Bayes model can also handle multiclass classification by assuming
    the probability distribution of ![B-07-EQ6](../Images/B-07-EQ6.png) to be a multinomial
    distribution. We will not get into the mathematical details but focus only on
    the scikit-learn implementation. Here’s the code to train a naive Bayes model
    with the preprocessed text data:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型还可以通过假设![B-07-EQ6](../Images/B-07-EQ6.png)的概率分布为多项分布来处理多类分类。我们不会深入数学细节，只关注scikit-learn的实现。以下是使用预处理文本数据训练朴素贝叶斯模型的代码：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'During the testing phase, we can apply Bayes’ theorem to calculate the probability
    of a document belonging to each class based on the features it contains. The final
    label of the document is predicted as the class with the largest probability.
    We apply the same transformers fit on the training data to the test data and evaluate
    the learned naive Bayes classifier as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，我们可以根据文档包含的特征应用贝叶斯定理来计算文档属于每个类的概率。文档的最终标签被预测为概率最大的类。我们将相同的转换器fit应用于训练数据，并如下评估学习到的朴素贝叶斯分类器：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The final test accuracy of the multinomial naive Bayes model with default hyperparameters
    in scikit-learn is 77.39%—a bit worse than 82.78% achieved by the previous logistic
    regression model. Let’s try to improve it by tuning some of the key hyperparameters.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn中默认超参数的多项式朴素贝叶斯模型的最终测试准确率为77.39%——略低于之前逻辑回归模型实现的82.78%。让我们尝试通过调整一些关键超参数来提高它。
- en: B.2.5 Tuning the text-classification pipeline with grid search
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.5 使用网格搜索调整文本分类流程
- en: 'Up to now, we have introduced the following three data processing components
    into the text classification pipeline:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将以下三个数据处理组件引入到文本分类流程中：
- en: BoW transformer
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BoW 转换器
- en: TF-IDF transformer
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF 转换器
- en: Classifier (logistic regression model/naive Bayes model)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器（逻辑回归模型/朴素贝叶斯模型）
- en: 'We can use the same process introduced in section B.1.6 to construct a sequential
    pipeline combining all three components for joint hyperparameter tuning. We select
    the multinomial naive Bayes classifier here as an example to build up the scikit-learn
    pipeline, and choose the folllowing three hyperparameters to tune:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在 B.1.6 节中介绍的相同过程来构建一个结合所有三个组件的顺序管道，以进行联合超参数调整。我们在这里选择多项式朴素贝叶斯分类器作为示例来构建
    scikit-learn 管道，并选择以下三个超参数进行调整：
- en: ngram_range *in the* CountVectorizer *operation*—CountVectorizer counts the
    occurrences of each term in the document to perform the BoW transformation. It
    can also count the occurrences of *n* consecutively appearing terms, which is
    called a *ngram*. For example, in the sentence “I love machine learning,” the
    1-grams (unigrams) are “I,” “love,” “machine,” and “learning”; the 2-grams (bigrams)
    are “I love,” “love machine,” and “machine learning.”
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ngram_range *在* CountVectorizer *操作中*—CountVectorizer 计算文档中每个术语的出现次数以执行 BoW
    转换。它还可以计算连续出现的 *n* 个术语的出现次数，这被称为 *ngram*。例如，在句子“我爱机器学习”中，1-grams（单语元）是“我”、“爱”、“机器”和“学习”；2-grams（双语元）是“我爱”、“爱机器”和“机器学习”。
- en: use_idf—Determines whether we want to use the TF-IDF transformation or the TF
    transformation.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: use_idf—确定我们是否想要使用 TF-IDF 转换或 TF 转换。
- en: alpha—The smoothing hyperparameter in the multinomial naive Bayes classifier.
    One problem with the naive Bayes model is that if a term never occurs in any documents
    in a given topic class in the training set (![B-07-EQ7](../Images/B-07-EQ7.png)),
    we will never assign a document with this term to that class during testing. To
    mitigate this problem, we usually introduce a smoothing hyperparameter to assign
    a small probability to these terms in these classes so that each term has a chance
    to occur in each class.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: alpha—多项式朴素贝叶斯分类器中的平滑超参数。朴素贝叶斯模型的一个问题是，如果在训练集中给定主题类中的任何文档中一个术语从未出现（![B-07-EQ7](../Images/B-07-EQ7.png)），则在测试期间我们永远不会将包含此术语的文档分配给该类。为了减轻这个问题，我们通常引入一个平滑超参数，将这些术语分配给这些类的小概率，这样每个术语都有机会出现在每个类中。
- en: We can set up the whole pipeline and customize the search spaces of the three
    hyperparameters with the following code.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码设置整个管道并自定义三个超参数的搜索空间。
- en: Listing B.9 Creating a pipeline for text classification
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.9 创建用于文本分类的管道
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Defines the pipeline
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义管道
- en: ❷ Declares the hyperparameters to be searched and defines their search spaces
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 声明要搜索的超参数并定义它们的搜索空间
- en: 'We apply grid search and use three-fold CV for model selection as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用网格搜索并使用三折交叉验证进行模型选择，如下所示：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then retrieve the search results of each pipeline represented by the three
    hyperparameters from grid_search.cv_results_. The best pipeline can be obtained
    as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从 grid_search.cv_results_ 中检索每个管道（由三个超参数表示）的搜索结果。最佳管道可以如下获得：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If we do not specify the refit hyperparameter in the GridSearchCV object, the
    best pipeline will be automatically trained on the whole training dataset after
    CV. Thus, we can directly evaluate the final results of the best pipeline on the
    test set. As shown next, the final accuracy is 83.44%, which is much better than
    the 77.39% achieved by the initial one:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 GridSearchCV 对象中未指定 refit 超参数，则在 CV 后将自动在整个训练数据集上训练最佳管道。因此，我们可以直接在测试集上评估最佳管道的最终结果。如下所示，最终准确率为
    83.44%，这比初始的 77.39% 有很大提升：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now that you’ve seen how to approach classification tasks on image data and
    text data, let’s come back to tabular data to work on a comparably more complex
    example.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经看到了如何处理图像数据和文本数据的分类任务，让我们回到表格数据，来处理一个相对更复杂的例子。
- en: 'B.3 Tabular classification: Identifying Titanic survivors'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 表格分类：识别泰坦尼克号幸存者
- en: Our last example will use the famous Titanic Kaggle competition dataset, edited
    by Michael A. Findlay. It contains the personal information of 1,309 passengers
    on the *Titanic*, such as name and gender (891 in the training set and 418 in
    the test set). Our goal is to identify the survivors. Instead of having a well-prepared
    numeric feature matrix for the tabular data, you’ll learn how to preprocess a
    more coarse dataset with mixed data types and missing values. The techniques described
    here are commonly seen in many Kaggle competitions that deal with tabular datasets
    and are also widely used in practical applications, even today with the prevalence
    of deep learning.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的例子将使用著名的泰坦尼克号Kaggle竞赛数据集，由Michael A. Findlay编辑。它包含了1,309名乘客的个人资料，例如姓名和性别（训练集中有891名，测试集中有418名）。我们的目标是识别幸存者。对于表格数据，你将学习如何预处理一个包含混合数据类型和缺失值的更粗糙的数据集，而不是有一个准备好的数值特征矩阵。这里描述的技术在许多处理表格数据集的Kaggle竞赛中很常见，并且至今仍广泛应用于实际应用中，即使在深度学习普遍的今天。
- en: B.3.1 Problem framing and data assembly
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.1 问题界定和数据组装
- en: The problem here is a binary classification problem, where the target label
    indicates whether the passenger survived (1) or not (0). We collect the data from
    the OpenML platform.[¹](#pgfId-1016744) scikit-learn provides a built-in API to
    fetch the dataset. We can format the fetched data into a DataFrame by setting
    the as_frame option to True, as shown in the following listing.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是一个二元分类问题，其中目标标签表示乘客是否幸存（1）或未幸存（0）。我们从OpenML平台收集数据。[¹](#pgfId-1016744)
    scikit-learn提供了一个内置API来获取数据集。我们可以通过将as_frame选项设置为True来将获取的数据格式化为DataFrame，如下面的列表所示。
- en: Listing B.10 Fetching the Titanic dataset
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.10 获取泰坦尼克号数据集
- en: '[PRE29]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Fetches the first version of the Titanic dataset from OpenML as a DataFrame
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从OpenML获取泰坦尼克号数据集的第一版本作为DataFrame
- en: ❷ Deep copies the data to avoid in-place data-processing operations later
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 深度复制数据以避免后续的就地数据处理操作
- en: ❸ Views the features of the first five passengers
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 查看前五个乘客的特征
- en: The first five examples in the raw feature matrix are shown in figure B.8.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 原始特征矩阵的前五个示例如图B.8所示。
- en: '![B-08](../Images/B-08.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![B-08](../Images/B-08.png)'
- en: Figure B.8 Raw features of the first five samples of the Titanic data
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.8 泰坦尼克号数据的第一个五个样本的原始特征
- en: By viewing the first five samples, we can observe that the passengers have 13
    features. The features are of different formats and data types. For example, the
    name feature is of type string, the sex feature is a categorical feature, and
    the age feature is a numerical feature. Let’s do some further exploration of the
    dataset and prepare it for the ML algorithms.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看前五个样本，我们可以观察到乘客有13个特征。这些特征具有不同的格式和数据类型。例如，姓名特征是字符串类型，性别特征是分类特征，年龄特征是数值特征。让我们进一步探索数据集，并为机器学习算法做准备。
- en: B.3.2 Data preprocessing and feature engineering
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.2 数据预处理和特征工程
- en: 'We can consider the following three typical procedures to prepare the tabular
    data:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以考虑以下三个典型程序来准备表格数据：
- en: Recover missing data.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恢复缺失数据。
- en: Extract features based on prior knowledge, such as extracting the gender feature
    based on the titles in passenger names (Mr., Ms., and so on).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据先验知识提取特征，例如根据乘客姓名中的头衔（先生、女士等）提取性别特征。
- en: Encode categorical features to numerical ones.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分类特征编码为数值类型。
- en: 'First let’s check for missing values. By counting the number of missing values
    in each feature, we can see that there are seven features with missing values,
    as shown here: age, fare, cabin, port of embarkation (embarked), boat, body, and
    passenger’s home/destination (home.dest):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查缺失值。通过计算每个特征的缺失值数量，我们可以看到有七个特征存在缺失值，如下所示：年龄、船票、船舱、登船港口（embarked）、船只、身体和乘客的家园/目的地（home.dest）：
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Considering that we only have 1,309 data points, the numbers of missing values
    in the cabin, boat, body, and home.dest features are quite large. Imputing these
    data points improperly could hurt the model’s performance, so we’ll drop these
    four features and consider imputing missing values only for the remaining three,
    as shown here:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们只有1,309个数据点，船舱、船只、身体和home.dest特征中的缺失值数量相当大。不恰当地填充这些数据点可能会损害模型的性能，因此我们将删除这四个特征，并仅考虑填充剩余三个特征的缺失值，如下所示：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can use many techniques to impute the missing data, of which the next three
    are quite common:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多技术来填充缺失数据，其中以下三种相当常见：
- en: '*Manually extrapolate the features based on sensible feature correlations*—This
    often requires domain expertise and human effort but can be more accurate than
    the other options. For example, we can extract the two passengers whose embarked
    feature is missing a value and check their ticket fares to guess their embarkation
    ports.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*根据合理的特征相关性手动外推特征*——这通常需要领域专业知识和人力，但可能比其他选项更准确。例如，我们可以提取两个缺失登船特征的乘客，并检查他们的票价来猜测他们的登船港。'
- en: '*Use statistical information*—For example, we can use the mean or median value
    to impute the missing fare value. This is often a convenient and efficient approach
    but may lose some discriminative information for classification.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用统计信息*——例如，我们可以使用平均值或中位数来估算缺失的票价。这种方法通常方便且高效，但可能会丢失一些用于分类的判别信息。'
- en: '*Use ML models to estimate the values based on other features*—For example,
    we can treat the age feature as a target and use regression to estimate the missing
    values based on other features. This method can be powerful but may overfit the
    data by overusing the feature correlation. It also highly relies on and can be
    biased by the model used to impute the missing data.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用机器学习模型根据其他特征估计值*——例如，我们可以将年龄特征视为目标，并使用回归根据其他特征估算缺失值。这种方法可能非常强大，但可能会通过过度使用特征相关性而过度拟合数据。它还高度依赖于用于估算缺失数据的模型，并且可能受到模型偏差的影响。'
- en: Because the fare and embarked features are missing only a few values, let’s
    first impute them with some sensible feature correlation. We begin by plotting
    a set of box plots of the passengers’ fares, grouping them by their embarked port
    (embarked) and the class they were in (pclass), as shown in the next code sample.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于票价和登船特征仅缺失少数值，让我们首先使用一些合理的特征相关性来估算它们。我们首先绘制一组乘客票价的箱线图，按他们的登船港（embarked）和舱位（pclass）分组，如下所示。
- en: Listing B.11 Box plots of fares grouped by embarked and pclass
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表B.11 按登船和pclass分组的票价箱线图
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Imports the package for drawing the box plots
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入绘制箱线图的包
- en: ❷ Draws the box plots
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制箱线图
- en: ❸ Sets the $80 fare horizontal line
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置$80票价水平线
- en: ❹ Adds a title
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加标题
- en: ❺ Adds a plot legend
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加图表图例
- en: From figure B.9, we can observe that passengers with the same port of embarkation
    have quite different distributions on the fare feature, highly related to the
    pclass feature.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 从图B.9中，我们可以观察到具有相同登船港的乘客在票价特征上的分布相当不同，这与pclass特征高度相关。
- en: '![B-09](../Images/B-09.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![B-09](../Images/B-09.png)'
- en: Figure B.9 Box plots of passenger fares grouped by the embarked and pclass features
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.9 按登船港和pclass特征分组的乘客票价箱线图
- en: 'By checking the passengers who are missing the embarked feature, we can see
    that both of the passengers are in first class with an $80 fare paid for their
    tickets (see figure B.10), as shown here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查缺失登船特征的乘客，我们可以看到这两位乘客都是头等舱，并且为他们的票支付了$80（见图B.10），如下所示：
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![B-10](../Images/B-10.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![B-10](../Images/B-10.png)'
- en: Figure B.10 Passengers missing the embarked feature
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.10 缺失登船特征的乘客
- en: 'The $80 reference line on the plot shows that passengers in first class who
    paid a fare of $80 are highly likely to have embarked at port C. Thus, we’ll fill
    in the missing embarked feature for these two passengers with port C as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图表上的$80参考线表明，支付了$80票价的头等舱乘客很可能是在C港登船的。因此，我们将这两位乘客缺失的登船特征填充为C港，如下所示：
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Similarly, we can check the passenger with the missing fare (see figure B.11)
    like so:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以检查缺失票价的乘客（见图B.11），如下所示：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![B-11](../Images/B-11.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![B-11](../Images/B-11.png)'
- en: Figure B.11 Passengers missing the fare feature
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.11 缺失票价特征的乘客
- en: 'This passenger traveled in third class and embarked at port S. We can combine
    this with statistical information and impute the missing fare by supplying the
    median fare value of the passengers in the same class who also embarked at port
    S, as shown next:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这位乘客乘坐三等舱并在S港登船。我们可以结合统计信息，通过提供同一舱位且在S港登船的乘客的中位票价来估算缺失的票价，如下所示：
- en: '[PRE36]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'For the last missing feature (age), we directly use the statistical median
    to impute the missing values as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一个缺失的特征（年龄），我们直接使用统计中位数来估算缺失值，如下所示：
- en: '[PRE37]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After dealing with all the missing data, the next step is to do some feature
    extraction based on common sense. We use the name feature as an example here.
    This feature seems to be quite chaotic and useless at first glance, but on examination
    we can see that the names include titles (Mr., Mrs., Master, and so on) that may
    reflect the passengers’ marital status and potential social status. We first extract
    the titles and explore the frequency of each as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完所有缺失数据后，下一步是基于常识进行一些特征提取。这里我们以名称特征为例。这个特征乍一看似乎非常混乱且无用，但在仔细检查后我们可以看到，名字中包含头衔（先生、夫人、少爷等），这些可能反映了乘客的婚姻状况和潜在的社会地位。我们首先提取头衔，并按以下方式探索每个头衔的频率：
- en: '[PRE38]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As some of the titles are similar from a machine learning perspective, such
    as “Ms” and “Miss,” we can unify them first. And because several of the titles
    appear only rarely in the dataset, a common practice we can use here is to combine
    the rare titles into one category to aggregate the information. In the next listing,
    we consider titles that appear fewer than eight times as rare titles.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些标题在机器学习角度来看是相似的，例如“Ms”和“Miss”，我们可以首先统一它们。并且因为数据集中有几个标题出现频率很低，我们可以使用的一种常见做法是将这些罕见标题合并为一个类别以聚合信息。在下一个列表中，我们考虑出现次数少于八次的标题作为罕见标题。
- en: Listing B.12 Unifying synonymous titles and combining rare titles
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.12 统一同义词标题和合并罕见标题
- en: '[PRE39]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Aggregates titles with same meaning by reassigning
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过重新分配将具有相同意义的标题进行聚合
- en: ❷ Combines the remaining rare titles into one
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将剩余的罕见标题合并为一个
- en: ❸ Drops the original name column
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 删除了原始名称列
- en: 'After doing the title extraction, we display the current data again by viewing
    the first five examples (see figure B.12) as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行标题提取后，我们再次通过查看前五个示例（见图 B.12）来显示当前数据，如下所示：
- en: '[PRE40]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![B-12](../Images/B-12.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![B-12](../Images/B-12.png)'
- en: Figure B.12 The first five samples of the Titanic dataset after missing data
    imputation and name title extraction
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.12 在缺失数据插补和名称标题提取后，泰坦尼克号数据集的前五个样本
- en: The last step is to convert the categorical features, including sex, embarked,
    and title, into numerical ones. Although some ML libraries can directly accept
    string-valued categorical features, we’ll do this preprocessing for the purposes
    of illustration. An intuitive way of transforming these features is to encode
    them as integers. This method can work well for categorical features with a low
    number of levels (in other words, few categories), such as sex, but it can affect
    the model’s performance when the number of levels is high. This is because we
    introduce ordinal relationships in the transformed numerical values that do not
    necessarily exist originally. Another popular option is *one-hot encoding*, which
    does not have this problem but may harm efficiency by introducing too many new
    features. One-hot encoding is done by converting a categorical feature into *N*
    binary categorical features, where the *N* new features represent the *N* levels
    of the original categorical feature. For each instance, we will set only one of
    the *N* new features (the category it belongs to) to 1; the others will be set
    to 0\. For example, the sex feature of a female and a male passenger will be transformed
    to [1, 0] and [0, 1], respectively, where the first element in the vector indicates
    that the passenger is female, and the second indicates that the passenger is male.
    We can do the transformation for all the categorical features in the same way
    as follows.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将分类特征，包括性别、登船地点和头衔，转换为数值型。尽管一些机器学习库可以直接接受字符串值的分类特征，但我们将为此进行预处理以供说明。转换这些特征的一种直观方法是将其编码为整数。这种方法对于层级数量较少的分类特征（换句话说，类别较少）可以很好地工作，例如性别，但当层级数量较多时可能会影响模型的性能。这是因为我们在转换后的数值中引入了原始数据中可能不存在的序数关系。另一个流行的选项是*独热编码*，它没有这个问题，但可能会因为引入过多的新特征而损害效率。独热编码是通过将一个分类特征转换为*N*个二进制分类特征来完成的，其中*N*个新特征代表原始分类特征的*N*个层级。对于每个实例，我们将只设置*N*个新特征中的一个（它所属的类别）为1；其他将被设置为0。例如，女性和男性乘客的性别特征将被转换为[1,
    0]和[0, 1]，其中向量中的第一个元素表示乘客是女性，第二个元素表示乘客是男性。我们可以像以下这样对所有的分类特征进行转换。
- en: Listing B.13 One-hot encoding of categorical data
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.13 分类数据的独热编码
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Performs the one-hot encoding
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 执行独热编码
- en: ❷ Removes the original feature
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 移除了原始特征
- en: ❸ Drops the ticket feature because it contains too many levels
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 删除了票务功能，因为它包含太多层级
- en: 'Note that although the ticket feature is also a categorical feature, we directly
    drop it without encoding it. This is because it contains too many unique categories
    compared with the total number of examples, as shown in the next code snippet:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管票务特征也是一个分类特征，但我们直接将其删除，而没有对其进行编码。这是因为与总样本数相比，它包含太多的唯一类别，如下一代码片段所示：
- en: '[PRE42]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The final data contains 15 features, as shown in figure B.13.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最终数据包含15个特征，如图 B.13 所示。
- en: '![B-13](../Images/B-13.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![B-13](../Images/B-13.png)'
- en: Figure B.13 Final features of the first five samples of the Titanic dataset
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.13 泰坦尼克号数据集前五个样本的最终特征
- en: 'Finally, we split the data into training and test sets, as shown next. The
    split number 891 is set based on the original split of the dataset in the Kaggle
    competition:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据分为训练集和测试集，如下所示。分割编号891是基于Kaggle竞赛中数据集原始分割设置的：
- en: '[PRE43]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now that we’ve finished our preprocessing, we are ready to apply ML algorithms
    on the Titanic dataset.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了预处理，我们就可以在泰坦尼克号数据集上应用机器学习算法了。
- en: B.3.3 Building tree-based classifiers
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.3 构建基于树的分类器
- en: Until the rise of deep learning, tree-based models were often considered the
    most powerful models for tabular data classification. This section introduces
    three types of tree-based classification models. The first one is a regular decision
    tree model, similar to the one we used in chapter 2\. The others are *ensemble
    models*, which aim to leverage the collective power of multiple decision trees
    for better classification accuracy.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习兴起之前，基于树的模型通常被认为是表格数据分类中最强大的模型。本节介绍了三种基于树的分类模型。第一个是一个常规决策树模型，类似于我们在第二章中使用的模型。其他的是“集成模型”，旨在利用多个决策树的集体力量以获得更好的分类精度。
- en: 'In the California housing price-prediction example in chapter 2, we created
    a decision tree model to perform the regression task. We can also build a decision
    tree model for a classification problem with a few minor modifications—we need
    to change the tree-split criterion from a regression performance measurement to
    a classification performance measurement during training, and we need to alter
    the way we make predictions as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章的加利福尼亚房价预测示例中，我们创建了一个决策树模型来执行回归任务。我们也可以通过一些小的修改为分类问题构建一个决策树模型——在训练过程中，我们需要将树分裂标准从回归性能度量改为分类性能度量，并且我们需要改变预测的方式如下：
- en: In the regression example, we used MSE as the split criterion to measure the
    quality of each node split. Here we can use a criterion called *entropy*, which
    measures the useful information gain for classification achieved by a specific
    node split. The mathematical definition of entropy is beyond the scope of this
    book.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回归示例中，我们使用均方误差（MSE）作为分裂标准来衡量每个节点分裂的质量。这里我们可以使用一个称为“熵”的标准，它衡量特定节点分裂通过分类获得的实用信息增益。熵的数学定义超出了本书的范围。
- en: In the regression example, we did the prediction using the mean value of the
    training samples in the same leaf node as the test sample. Here we use the mode
    value of the target labels of those training samples instead.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回归示例中，我们使用与测试样本相同的叶子节点中训练样本的均值进行预测。这里我们使用那些训练样本的目标标签的众数。
- en: Implementing a decision tree classifier is straightforward with the help of
    scikit-learn, as shown in the next listing.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn，实现决策树分类器非常简单，如下一列表所示。
- en: Listing B.14 Creating a tree a decision tree classifier on the Titanic dataset
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.14 在泰坦尼克号数据集上创建一个决策树分类器
- en: '[PRE44]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Creates a decision tree classifier with entropy as the split criterion
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用熵作为分裂标准创建一个决策树分类器
- en: 'The test accuracy is calculated as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确度计算如下：
- en: '[PRE45]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now let’s create two more advanced decision tree-based models leveraging the
    technique of *ensemble learning*. As the proverb goes, two heads are better than
    one, and the idea of the wisdom of the crowd holds not only for humans but also
    for ML models in many cases. Ensemble learning is a process of aggregating multiple
    ML models to achieve better predictions. We’ll look at the following two representative
    tree-based algorithms using ensemble learning, which are probably the most popular
    models in the Kaggle competitions:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们利用集成学习技术创建两个更高级的基于决策树的模型。正如谚语所说，三个臭皮匠顶个诸葛亮，群众的智慧在许多情况下不仅适用于人类，也适用于机器学习模型。集成学习是一个将多个机器学习模型聚合起来以实现更好预测的过程。我们将查看以下两个代表性的基于集成学习的树算法，这些算法可能是Kaggle竞赛中最受欢迎的模型：
- en: '*Random forest*—This approach builds up multiple decision trees simultaneously
    and jointly considers their predictions by voting. We can select the number of
    trees to be assembled and control the hyperparameters for individual trees, such
    as the split criterion and the trees’ maximum depth.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机森林*—这种方法同时构建多个决策树，并通过投票共同考虑它们的预测。我们可以选择要组装的树的数量，并控制单个树的超参数，例如分割标准和树的深度最大值。'
- en: '*Gradient-boosted decision tree* (GBDT)—This approach builds up multiple trees
    sequentially and orients the newly constructed tree to address the erroneous classifications
    or weak predictions of the previous tree ensemble. We can control the number of
    trees to be assembled, the relative contribution of each tree to the final ensemble
    model (the learning_rate hyperparameter), and the hyperparameters for individual
    trees.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度提升决策树* (GBDT)—这种方法按顺序构建多个树，并将新构建的树定位在解决先前树集成中的错误分类或弱预测。我们可以控制要组装的树的数量，每个树对最终集成模型相对贡献（学习率超参数），以及单个树的超参数。'
- en: You can import both of the classifiers from the sklearn.ensemble module. The
    following listing shows how to apply them to the Titanic dataset.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 sklearn.ensemble 模块导入这两个分类器。以下列表显示了如何将它们应用于泰坦尼克号数据集。
- en: Listing B.15 Applying random forest and GBDT algorithms to the Titanic dataset
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.15 将随机森林和 GBDT 算法应用于泰坦尼克号数据集
- en: '[PRE46]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ Trains and tests the random forest algorithm
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练和测试随机森林算法
- en: ❷ Trains and tests the GBDT algorithm
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练和测试 GBDT 算法
- en: 'The final performance shows that both models perform better than our previous
    decision tree model, which achieved an accuracy of 71.05%. The obtained GBDT model
    performs slightly better than the random forest model, as shown next:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的性能显示，这两个模型都比我们之前的决策树模型表现更好，后者实现了 71.05% 的准确率。获得的 GBDT 模型比随机森林模型略好，如下所示：
- en: '[PRE47]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We can also tune the hyperparameters for each of the three algorithms using
    grid search, similar to the previous examples. That exercise is left to the reader
    as a self-test.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用网格搜索调整每个算法的超参数，类似于之前的示例。这项练习留给读者作为自我测试。
- en: '* * *'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) The OpenML platform ([https://openml.org](https://openml.org)), founded
    by Joaquin Vanschoren, is an online platform for sharing data, code, models, and
    experiments to make ML and data analysis simple, open, accessible, and reproducible.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) 由 Joaquin Vanschoren 创立的 OpenML 平台 ([https://openml.org](https://openml.org))
    是一个在线平台，用于共享数据、代码、模型和实验，以使机器学习和数据分析变得简单、开放、易于访问和可重复。
