- en: 6 Transfer learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 迁移学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the transfer learning technique
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解迁移学习技术
- en: Using a pretrained network to solve your problem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练网络解决你的问题
- en: Understanding network fine-tuning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解网络微调
- en: Exploring open source image datasets for training a model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索开源图像数据集以训练模型
- en: Building two end-to-end transfer learning projects
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建两个端到端迁移学习项目
- en: Transfer learning is one of the most important techniques of deep learning.
    When building a vision system to solve a specific problem, you usually need to
    collect and label a huge amount of data to train your network. You can build convnets,
    as you learned in chapter 3, and start the training from scratch; that is an acceptable
    approach. But what if you could download an existing neural network that someone
    else has tuned and trained, and use it as a starting point for your new task?
    Transfer learning allows you to do just that. You can download an open source
    model that someone else has already trained and tuned and use their optimized
    parameters (weights) as a starting point to train your model on a smaller dataset
    for a given task. This way, you can train your network a lot faster and achieve
    higher results.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是深度学习中最重要的技术之一。当构建一个用于解决特定问题的视觉系统时，你通常需要收集和标记大量数据来训练你的网络。你可以构建卷积神经网络（convnets），正如你在第3章中学到的，并从头开始训练；这是一个可接受的方法。但如果你能下载一个其他人已经调整和训练好的现有神经网络，并将其作为你新任务的起点呢？迁移学习允许你做到这一点。你可以下载一个其他人已经训练和调整好的开源模型，并使用他们的优化参数（权重）作为起点，在给定任务的小数据集上训练你的模型。这样，你可以更快地训练你的网络并取得更高的结果。
- en: 'DL researchers and practitioners have posted many research papers and open
    source projects of trained algorithms that they have worked on for weeks and months
    and trained on GPUs to get state-of-the-art results on an array of problems. Often,
    the fact that someone else has done this work and gone through the painful high-performance
    research process means you can download an open source architecture and weights
    and use them as a good start for your own neural network. This is transfer learning
    : the transfer of knowledge from a pretrained network in one domain to your own
    problem in a different domain.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: DL研究人员和从业者已经发布了他们花费数周甚至数月时间在GPU上训练的算法的研究论文和开源项目，这些算法在各种问题上取得了最先进的结果。通常，其他人已经完成这项工作并经历了痛苦的高性能研究过程的事实意味着你可以下载一个开源架构和权重，并将它们作为你自己的神经网络的良好起点。这就是迁移学习：将一个领域预训练网络的知识迁移到不同领域的自己的问题上。
- en: In this chapter, I will explain transfer learning and outline reasons why using
    it is important. I will also detail different transfer learning scenarios and
    how to use them. Finally, we will see examples of using transfer learning to solve
    real-world problems. Ready? Let’s get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将解释迁移学习并概述使用它的原因。我还会详细说明不同的迁移学习场景以及如何使用它们。最后，我们将看到使用迁移学习解决现实世界问题的例子。准备好了吗？让我们开始吧！
- en: 6.1 What problems does transfer learning solve?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 迁移学习解决了哪些问题？
- en: As the name implies, transfer learning means transferring what a neural network
    has learned from being trained on a specific dataset to another related problem
    (figure 6.1). Transfer learning is currently very popular in the field of DL because
    it enables you to train deep neural networks with comparatively little data in
    a short training time. The importance of transfer learning comes from the fact
    that in most real-world problems, we typically do not have millions of labeled
    images to train such complex models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，迁移学习意味着将神经网络从特定数据集的训练中学到的知识迁移到另一个相关问题上（图6.1）。迁移学习目前在深度学习领域非常流行，因为它允许你在较短的时间内用相对较少的数据训练深度神经网络。迁移学习的重要性在于，在大多数现实世界的问题中，我们通常没有数百万个标记的图像来训练如此复杂的模型。
- en: '![](../Images/6-1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图6-1](../Images/6-1.png)'
- en: Figure 6.1 Transfer learning is the transfer of the knowledge that the network
    has acquired from one task to a new task. In the context of neural networks, the
    acquired knowledge is the extracted features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 迁移学习是将网络从一项任务中获取的知识迁移到新任务的过程。在神经网络的情况下，获取的知识是提取的特征。
- en: The idea is pretty straightforward. First we train a deep neural network on
    a very large amount of data. During the training process, the network extracts
    a large number of useful features that can be used to detect objects in this dataset.
    We then transfer these extracted features (feature maps) to a new network and
    train this new network on our new dataset to solve a different problem. Transfer
    learning is a great way to shortcut the process of collecting and training huge
    amounts of data simply by reusing the model weights from pretrained models that
    were developed for standard CV benchmark datasets, such as the ImageNet image-recognition
    tasks. Top-performing models can be downloaded and used directly, or integrated
    into a new model for your own CV problems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常直接。首先，我们在大量数据上训练一个深度神经网络。在训练过程中，网络提取了大量有用的特征，这些特征可以用来检测这个数据集中的对象。然后，我们将这些提取的特征（特征图）转移到新的网络上，并在我们的新数据集上训练这个新的网络来解决不同的问题。迁移学习是一种通过重用为标准CV基准数据集（如ImageNet图像识别任务）开发的预训练模型的模型权重来简化收集和训练大量数据过程的好方法。表现最好的模型可以直接下载并使用，或者集成到新的模型中，用于解决你自己的CV问题。
- en: The question is, why would we want to use transfer learning? Why don’t we just
    train a neural network directly on our new dataset to solve our problem? To answer
    this question, we first need to know the main problems that transfer learning
    solves. We’ll discuss those now; then I’ll go into the details of how transfer
    learning works and the different approaches to apply it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，我们为什么要使用迁移学习？为什么我们不可以直接在我们的新数据集上训练一个神经网络来解决问题呢？为了回答这个问题，我们首先需要了解迁移学习解决的主要问题。我们现在就来讨论这些问题；然后我会详细介绍迁移学习的工作原理以及应用它的不同方法。
- en: 'Deep neural networks are immensely data-hungry and rely on huge amounts of
    labeled data to achieve high performance. In practice, very few people train an
    entire convolutional network from scratch. This is due to two main problems:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络对数据的需求极大，需要大量的标记数据才能实现高性能。在实践中，很少有人从头开始训练整个卷积网络。这主要是由于两个主要问题：
- en: 'Data problem --Training a network from scratch requires a lot of data in order
    to get decent results, which is not feasible in most cases. It is relatively rare
    to have a dataset of sufficient size to solve your problem. It is also very expensive
    to acquire and label data: this is mostly a manual process done by humans capturing
    images and labeling them one by one, which makes it a nontrivial task.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据问题——从头开始训练一个网络需要大量的数据才能得到令人满意的结果，这在大多数情况下是不切实际的。拥有足够大以解决你问题的数据集相对较少。获取和标记数据也非常昂贵：这主要是一个由人类完成的繁琐过程，他们捕获图像并逐个标记它们，这使得它成为一个非平凡的任务。
- en: Computation problem --Even if you are able to acquire hundreds of thousands
    of images for your problem, it is computationally very expensive to train a deep
    neural network on millions of images because doing so usually requires weeks of
    training on multiple GPUs. Also keep in mind that training a neural network is
    an iterative process. So, even if you happen to have the computing power required
    to train a complex neural network, spending weeks experimenting with different
    hyperparameters in each training iteration until you finally reach satisfactory
    results will make the project very costly.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算问题——即使你能为你的问题获取数十万张图像，在数百万张图像上训练深度神经网络在计算上也非常昂贵，因为这通常需要数周在多个GPU上的训练。此外，请记住，训练神经网络是一个迭代过程。所以，即使你碰巧有训练复杂神经网络所需的计算能力，花费数周时间在每次训练迭代中尝试不同的超参数，直到最终达到令人满意的结果，也会使项目成本高昂。
- en: Additionally, an important benefit of using transfer learning is that it helps
    the model generalize its learnings and avoid overfitting. When you apply a DL
    model in the wild, it is faced with countless conditions it may never have seen
    before and does not know how to deal with; each client has its own preferences
    and generates data that is different from the data used for training. The model
    is asked to perform well on many tasks that are related to but not exactly similar
    to the task it was trained for.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用迁移学习的一个重要好处是它有助于模型泛化其学习并避免过拟合。当你在实际环境中应用深度学习模型时，它面临着无数它可能从未见过的条件，并且不知道如何处理；每个客户端都有自己的偏好，并生成与用于训练的数据不同的数据。模型被要求在许多与训练任务相关但不完全相似的任务上表现良好。
- en: For example, when you deploy a car classifier model to production, people usually
    have different camera types, each with its own image quality and resolution. Also,
    images can be taken during different weather conditions. These image nuances vary
    from one user to another. To train the model on all these different cases, you
    either have to account for every case and acquire a lot of images to train the
    network on, or try to build a more robust model that is better at generalizing
    to new use cases. This is what transfer learning does. Since it is not realistic
    to account for all the cases the model may face in the wild, transfer learning
    can help us deal with novel scenarios. It is necessary for production-scale use
    of DL that goes beyond tasks and domains where labeled data is plentiful. Transferring
    features extracted from another network that has seen millions of images will
    make our model less prone to overfit and help it generalize better when faced
    with novel scenarios. You will be able to fully grasp this concept when we explain
    how transfer learning works in the following sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当您将汽车分类器模型部署到生产环境中时，人们通常使用不同类型的摄像头，每种摄像头都有其自身的图像质量和分辨率。此外，图像可能在不同天气条件下拍摄。这些图像细微差别因用户而异。为了训练模型以涵盖所有这些不同情况，您要么必须考虑每一个案例并获取大量图像来训练网络，要么尝试构建一个更鲁棒的模型，使其更好地泛化到新的用例中。这就是迁移学习所做的事情。由于在野外考虑模型可能遇到的所有情况并不现实，迁移学习可以帮助我们应对新场景。对于超出大量标记数据任务和领域的深度学习生产规模使用，这是必要的。从已经看过数百万图像的另一个网络中迁移提取的特征将使我们的模型更不容易过拟合，并在面对新场景时更好地泛化。当我们解释以下章节中迁移学习是如何工作时，您将能够完全理解这个概念。
- en: 6.2 What is transfer learning?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 什么是迁移学习？
- en: Armed with the understanding of the problems that transfer learning solves,
    let’s look at its formal definition. Transfer learning is the transfer of the
    knowledge (feature maps) that the network has acquired from one task, where we
    have a large amount of data, to a new task where data is not abundantly available.
    It is generally used where a neural network model is first trained on a problem
    similar to the problem that is being solved. One or more layers from the trained
    model are then used in a new model trained on the problem of interest.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了迁移学习解决的问题之后，让我们来看看它的正式定义。迁移学习是将网络从一个任务中获取的知识（特征图）转移到另一个任务中，在第一个任务中我们有大量数据，而在第二个任务中数据并不丰富。它通常用于在神经网络模型首先在一个与正在解决的问题相似的问题上训练的情况下。然后，从训练模型中使用的层在一个新的模型上训练感兴趣的问题。
- en: As we discussed earlier, to train an image classifier that will achieve image
    classification accuracy near to or above the human level, we’ll need massive amounts
    of data, large compute power, and lots of time on our hands. I’m sure most of
    us don’t have all these things. Knowing that this would be a problem for people
    with little-to-no resources, researchers built state-of-the-art models that were
    trained on large image datasets like ImageNet, MS COCO, Open Images, and so on,
    and then shared their models with the general public for reuse. This means you
    should never have to train an image classifier from scratch again, unless you
    have an exceptionally large dataset and a very large computation budget to train
    everything from scratch by yourself. Even if that is the case, you might be better
    off using transfer learning to fine-tune the pretrained network on your large
    dataset. Later in this chapter, we will discuss the different transfer learning
    approaches, and you will understand what fine-tuning means and why it is better
    to use transfer learning even when you have a large dataset. We will also talk
    briefly about some of the popular datasets mentioned here.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，为了训练一个图像分类器，使其图像分类精度接近或超过人类水平，我们需要大量的数据、强大的计算能力和大量的时间。我相信我们大多数人都没有这些。知道这将是资源有限的人的问题，研究人员构建了在大型图像数据集（如ImageNet、MS
    COCO、Open Images等）上训练的最先进模型，并将他们的模型与公众共享以供重用。这意味着您永远不必从头开始训练图像分类器，除非您有一个异常大的数据集和非常大的计算预算来从头开始自己训练一切。即使是这样，您可能还是最好使用迁移学习来微调预训练网络以适应您的大数据集。在本章的后面部分，我们将讨论不同的迁移学习方法，您将了解微调的含义以及为什么即使您有大量数据集，使用迁移学习也是更好的选择。我们还将简要讨论这里提到的某些流行数据集。
- en: NOTE When we talk about training a model from scratch, we mean that the model
    starts with zero knowledge of the world, and the model’s structure and parameters
    begin as random guesses. Practically speaking, this means the weights of the model
    are randomly initialized, and they need to go through a training process to be
    optimized.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当我们谈论从头开始训练一个模型时，我们的意思是模型从对世界的零知识开始，模型的结构和参数开始时是随机猜测。从实际的角度来说，这意味着模型的权重是随机初始化的，并且它们需要通过训练过程来优化。
- en: The intuition behind transfer learning is that if a model is trained on a large
    and general enough dataset, this model will effectively serve as a generic representation
    of the visual world. We can then use the feature maps it has learned, without
    having to train on a large dataset, by transferring what it learned to our model
    and using that as a base starting model for our own task.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习的直觉在于，如果一个模型在大而足够通用的数据集上进行了训练，那么这个模型将有效地作为视觉世界的通用表示。然后我们可以使用它所学习的特征图，而无需在大型数据集上训练，通过将所学知识转移到我们的模型中，并以此作为我们自己的任务的基础起始模型。
- en: In transfer learning, we first train a base network on a base dataset and task,
    and then we repurpose the learned features, or transfer them to a second target
    network to be trained on a target dataset and task. This process will tend to
    work if the features are general, meaning suitable to both base and target tasks,
    instead of specific to the base task.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在转移学习中，我们首先在一个基础数据集和任务上训练一个基础网络，然后重新利用学到的特征，或者将它们转移到第二个目标网络，以在目标数据集和任务上进行训练。如果特征是通用的，即适合基础和目标任务，而不是仅针对基础任务，那么这个过程通常会有效。
- en: --Jason Yosinski et al.[1](#pgfId-1164387)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: --Jason Yosinski等人[1](#pgfId-1164387)
- en: 'Let’s jump directly to an example to get a better intuition for how to use
    transfer learning. Suppose we want to train a model that classifies dog and cat
    images, and we have only two classes in our problem: dog and cat. We need to collect
    hundreds of thousands of images for each class, label them, and train our network
    from scratch. Another option is to use transfer knowledge from another pretrained
    network.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接跳到一个例子，以更好地理解如何使用转移学习。假设我们想要训练一个能够对狗和猫的图像进行分类的模型，而我们的问题中只有两个类别：狗和猫。我们需要为每个类别收集数十万张图像，对它们进行标记，并从头开始训练我们的网络。另一个选择是使用另一个预训练网络的知识。
- en: First, we need to find a dataset that has similar features to our problem at
    hand. This involves spending some time exploring different open source datasets
    to find the one closest to our problem. For the sake of this example, let’s use
    ImageNet, since we are already familiar with it from the previous chapter and
    it has a lot of dog and cat images. So the pretrained network is familiar with
    dog and cat features and will require minimum training. (Later in this chapter,
    we will explore other datasets.) Next, we need to choose a network that has been
    trained on ImageNet and achieved good results. In chapter 5, we learned about
    state-of-the-art architectures like VGGNet, GoogLeNet, and ResNet. Any of them
    would work fine. For this example, we will go with a VGG16 network that has been
    trained on ImageNet datasets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要找到一个具有与我们当前问题相似特征的数据库。这涉及到花一些时间去探索不同的开源数据库，以找到与我们问题最接近的一个。为了这个例子，让我们使用ImageNet，因为我们已经在前一章中熟悉它，并且它有很多狗和猫的图像。因此，预训练网络熟悉狗和猫的特征，并且将需要最少的训练。（在本章的后面，我们将探索其他数据库。）接下来，我们需要选择一个在ImageNet上训练并取得良好结果的网络。在第5章中，我们学习了像VGGNet、GoogLeNet和ResNet这样的最先进架构。它们中的任何一个都可以工作。对于这个例子，我们将选择一个在ImageNet数据集上训练的VGG16网络。
- en: To adapt the VGG16 network to our problem, we are going to download it with
    the pretrained weights, remove the classifier part, add our own classifier, and
    then retrain the new network (figure 6.2). This is called using a pretrained network
    as a feature extractor. We will discuss the different types of transfer learning
    later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将VGG16网络适应我们的问题，我们将下载它并带有预训练的权重，移除分类器部分，添加我们自己的分类器，然后重新训练新的网络（图6.2）。这被称为使用预训练网络作为特征提取器。我们将在本章后面讨论不同类型的转移学习。
- en: DEFINITION A pretrained model is a network that has been previously trained
    on a large dataset, typically on a large-scale image classification task. We can
    either use the pretrained model directly as is to run our predictions, or use
    the pretrained feature extraction part of the network and add our own classifier.
    The classifier here could be one or more dense layers or even traditional ML algorithms
    like support vector machines (SVMs).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：预训练模型是一个在大型数据集上预先训练过的网络，通常是在大规模图像分类任务上。我们可以直接使用预训练模型来运行我们的预测，或者使用网络的预训练特征提取部分并添加我们自己的分类器。这里的分类器可以是一个或多个密集层，甚至是传统的机器学习算法，如支持向量机（SVMs）。
- en: '![](../Images/6-2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-2.png)'
- en: Figure 6.2 Example of applying transfer learning to a VGG16 network. We freeze
    the feature extraction part of the network and remove the classifier part. Then
    we add our new classifier softmax layer with two hidden units.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 应用迁移学习到VGG16网络的示例。我们冻结了网络的特征提取部分并移除了分类器部分。然后我们添加了新的分类器softmax层，包含两个隐藏单元。
- en: 'To fully understand how to use transfer learning, let’s implement this example
    in Keras. (Luckily, Keras has a set of pretrained networks that are ready for
    us to download and use: the complete list of models is at [https://keras.io/api/applications](https://keras.io/api/applications/).)
    Here are the steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全理解如何使用迁移学习，让我们在Keras中实现这个示例。（幸运的是，Keras有一系列预训练网络，我们可以直接下载和使用：模型完整列表见[https://keras.io/api/applications](https://keras.io/api/applications/)。）以下是步骤：
- en: 'Download the open source code of the VGG16 network and its weights to create
    our base model, and remove the classification layers from the VGG network (`FC_4096`
    > `FC_4096` > `Softmax_1000`):'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载VGG16网络及其权重的开源代码来创建我们的基础模型，并从VGG网络中移除分类层（`FC_4096` > `FC_4096` > `Softmax_1000`）：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports the VGG16 model from Keras
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 从Keras导入VGG16模型
- en: ❷ Downloads the model’s pretrained weights and saves them in the variable base_model.
    We specify that Keras should download the ImageNet weights. include_top is false
    to ignore the fully connected classifier part on top of the model.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 下载模型的预训练权重，并将它们保存在变量`base_model`中。我们指定Keras下载ImageNet权重。`include_top`设置为`False`以忽略模型顶部的全连接分类器部分。
- en: 'When you print a summary of the base model, you will notice that we downloaded
    the exact VGG16 architecture that we implemented in chapter 5\. This is a fast
    approach to download popular networks that are supported by the DL library you
    are using. Alternatively, you can build the network yourself, as we did in chapter
    5, and download the weights separately. I’ll show you how in the project at the
    end of this chapter. But for now, let’s look at the `base_model` summary that
    we just downloaded:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你打印基础模型的摘要时，你会注意到我们下载了与第5章中实现的完全相同的VGG16架构。这是下载受你使用的深度学习库支持的流行网络的一种快速方法。或者，你也可以像我们在第5章中做的那样自己构建网络，并单独下载权重。我将在本章末尾的项目中向你展示如何操作。但就目前而言，让我们看看我们刚刚下载的`base_model`摘要：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that this downloaded architecture does not contain the classifier part
    (three fully connected layers) at the top of the network because we set the `include_top`
    argument to `False`. More importantly, notice the number of trainable and non-trainable
    parameters in the summary. The downloaded network as it is makes all the network
    parameters trainable. As you can see, our `base_` `model` has more than 14 million
    trainable parameters. Next, we want to freeze all the downloaded layers and add
    our own classifier.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，这个下载的架构不包含网络顶部的分类器部分（三个全连接层），因为我们设置了`include_top`参数为`False`。更重要的是，注意摘要中可训练和非可训练参数的数量。下载的网络本身使所有网络参数可训练。正如你所见，我们的`base_model`有超过1400万个可训练参数。接下来，我们想要冻结所有下载的层并添加我们自己的分类器。
- en: 'Freeze the feature extraction layers that have been trained on the ImageNet
    dataset. Freezing layers means freezing their trained weights to prevent them
    from being retrained when we run our training:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结在ImageNet数据集上训练的特征提取层。冻结层意味着冻结它们的训练权重，以防止我们在运行训练时重新训练：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Iterates through layers and locks them to make them non-trainable with this
    code
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 通过此代码遍历层并将它们锁定，使它们不可训练
- en: 'The model summary is omitted in this case for brevity, as it is similar to
    the previous one. The difference is that all the weights have been frozen, the
    trainable parameters are now equal to zero, and all the parameters of the frozen
    layers are non-trainable:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此情况下省略模型摘要以节省空间，因为它与之前的一个类似。不同之处在于所有权重都已冻结，可训练参数现在等于零，所有冻结层的参数都是不可训练的：
- en: '[PRE3]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add our own classification dense layer. Here, we will add a softmax layer with
    two units because we have only two classes in our problem (see figure 6.3):'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加我们自己的分类密集层。在这里，我们将添加一个具有两个单元的softmax层，因为我们的问题中只有两个类别（见图6.3）：
- en: '[PRE4]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Imports Keras modules
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 导入Keras模块
- en: ❷ Uses the get_layer method to save the last layer of the network
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 使用get_layer方法保存网络的最后一层
- en: ❸ Saves the output of the last layer to be the input of the next layer
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 将最后一层的输出保存为下一层的输入
- en: ❹ Flattens the classifier input, which is the output of the last layer of the
    VGG16 model
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 将分类器输入（VGG16模型的最后一层输出）展平
- en: ❺ Adds our new softmax layer with two hidden units
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❺ 添加我们的新softmax层，包含两个隐藏单元
- en: '![](../Images/6-3.png)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/6-3.png)'
- en: Figure 6.3 Remove the classifier part of the network, and add a softmax layer
    with two hidden nodes.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.3 移除网络的分类器部分，并添加一个具有两个隐藏节点的softmax层。
- en: 'Build a `new_model` that takes the input of the base model as its input and
    the output of the last softmax layer as an output. The new model is composed of
    all the feature extraction layers in VGGNet with the pretrained weights, plus
    our new, untrained, softmax layer. In other words, when we train the model, we
    are only going to train the softmax layer in this example to detect the specific
    features of our new problem (German Shepherd, Beagle, Neither):'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个`new_model`，它以基础模型的输入作为其输入，以最后一个softmax层的输出作为输出。新的模型由VGGNet中的所有特征提取层组成，并带有预训练的权重，再加上我们新的、未训练的softmax层。换句话说，当我们训练模型时，我们只会在本例中训练softmax层以检测我们新问题的特定特征（德国牧羊犬、比格犬、两者都不是）：
- en: '[PRE5]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Instantiates a new_model using Keras’s Model class
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 使用Keras的Model类实例化一个new_model
- en: ❷ Prints the new_model summary
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 打印new_model摘要
- en: Training the new model is a lot faster than training the network from scratch.
    To verify that, look at the number of trainable params in this model (~50,000)
    compared to the number of non-trainable params in the network (~14 million). These
    “non-trainable” parameters are already trained on a large dataset, and we froze
    them to use the extracted features in our problem. With this new model, we don’t
    have to train the entire VGGNet from scratch because we only have to deal with
    the newly added softmax layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 训练新的模型比从头开始训练网络快得多。为了验证这一点，看看这个模型中的可训练参数数量（约50,000）与网络中不可训练参数数量（约1400万）的对比。这些“不可训练”的参数已经在大型数据集上进行了训练，并且我们冻结了它们以使用我们问题中提取的特征。使用这个新模型，我们不必从头开始训练整个VGGNet，因为我们只需要处理新添加的softmax层。
- en: Additionally, we get much better performance with transfer learning because
    the new model has been trained on millions of images (ImageNet dataset + our small
    dataset). This allows the network to understand the finer details of object nuances,
    which in turn makes it generalize better on new, previously unseen images.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于新模型在数百万张图像（ImageNet数据集+我们的小型数据集）上进行了训练，我们通过迁移学习获得了更好的性能。这允许网络理解物体细微差别的更详细信息，从而使其在新、以前未见过的图像上更好地泛化。
- en: Note that in this example, we only explored the part where we build the model,
    to show how transfer learning is used. At the end of this chapter, I’ll walk you
    through two end-to-end projects to demonstrate how to train the new network on
    your small dataset. But now, let’s see how transfer learning works.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在本例中，我们只探讨了构建模型的部分，以展示迁移学习是如何被使用的。在本章的结尾，我将带您通过两个端到端项目来展示如何在您的数据集上训练新的网络。但现在，让我们看看迁移学习是如何工作的。
- en: 6.3 How transfer learning works
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 迁移学习是如何工作的
- en: So far, we learned what the transfer learning technique is and the main problems
    it solves. We also saw an example of how to take a pretrained network that was
    trained on ImageNet and transfer its learnings to our specific task. Now, let’s
    see why transfer learning works, what is really being transferred from one problem
    to another, and how a network that is trained on one dataset can perform well
    on a different, possibly unrelated, dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们学习了迁移学习技术是什么以及它解决的主要问题。我们还看到了一个例子，说明如何将训练在ImageNet上的预训练网络的学习迁移到我们的特定任务。现在，让我们看看为什么迁移学习有效，真正从一个问题转移到另一个问题的内容是什么，以及一个在某个数据集上训练的网络如何在不同的、可能无关的数据集上表现良好。
- en: 'The following quick questions are reminders from previous chapters to get us
    to the core of what is happening in transfer learning:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下快速问题是来自前几章的提醒，以帮助我们了解迁移学习中的核心内容：
- en: 'What is really being learned by the network during training? The short answer
    is: feature maps.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络在训练过程中真正学习的是什么？简短的答案是：特征图。
- en: How are these features learned? During the backpropagation process, the weights
    are updated until we get to the optimized weights that minimize the error function.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些特征是如何学习的？在反向传播过程中，权重被更新，直到我们得到最小化误差函数的优化权重。
- en: What is the relationship between features and weights? A feature map is the
    result of passing the weights filter on the input image during the convolution
    process (figure 6.4).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征和权重之间的关系是什么？特征图是在卷积过程中将权重滤波器应用于输入图像的结果（图6.4）。
- en: '![](../Images/6-4.png)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/6-4.png)'
- en: Figure 6.4 Example of generating a feature map by applying a convolutional kernel
    to the input image
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.4 通过将卷积核应用于输入图像生成特征图的示例
- en: What is really being transferred from one network to another? To transfer features,
    we download the optimized weights of the pretrained network. These weights are
    then reused as the starting point for the training process and retrained to adapt
    to the new problem.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个网络到另一个网络真正传递的是什么？为了传递特征，我们下载预训练网络的优化权重。然后，这些权重被用作训练过程的起点，并重新训练以适应新的问题。
- en: 'Okay, let’s dive into the details to understand what we mean when we say pretrained
    network. When we’re training a convolutional neural network, the network extracts
    features from an image in the form of feature maps: outputs of each layer in a
    neural network after applying the weights filter. They are representations of
    the features that exist in the training set. They are called feature maps because
    they map where a certain kind of feature is found in the image. CNNs look for
    features such as straight lines, edges, and even objects. Whenever they spot these
    features, they report them to the feature map. Each weight filter is looking for
    something different that is reflected in the feature maps: one filter could be
    looking for straight lines, another for curves, and so on (figure 6.5).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们深入了解，了解当我们说预训练网络时我们指的是什么。当我们训练卷积神经网络时，网络以特征图的形式从图像中提取特征：神经网络中每个层在应用权重滤波器后的输出。它们是训练集中存在的特征的表示。它们被称为特征图，因为它们映射了图像中某种特征的位置。CNNs寻找直线、边缘甚至物体等特征。每当它们发现这些特征时，它们就会将它们报告给特征图。每个权重滤波器都在寻找不同的东西，这在特征图中得到反映：一个滤波器可能正在寻找直线，另一个可能寻找曲线，等等（图6.5）。
- en: '![](../Images/6-5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-5.png)'
- en: Figure 6.5 The network extracts features from an image in the form of feature
    maps. They are representations of the features that exist in the training set
    after applying the weight filters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 网络以特征图的形式从图像中提取特征。它们是在应用权重滤波器后，训练集中存在的特征的表示。
- en: 'Now, recall that neural networks iteratively update their weights during the
    training cycle of feedforward and backpropagation. We say the network has been
    trained when we go through a series of training iterations and hyperparameter
    tuning until the network yields satisfactory results. When training is complete,
    we output two main items: the network architecture and the trained weights. So,
    when we say that we are going to use a pretrained network, we mean that we will
    download the network architecture together with the weights.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回想一下，神经网络在正向传播和反向传播的训练周期中会迭代更新它们的权重。当我们经历一系列的训练迭代和超参数调整，直到网络产生令人满意的结果时，我们说网络已经被训练了。训练完成后，我们输出两个主要项目：网络架构和训练好的权重。因此，当我们说我们要使用一个预训练网络时，我们的意思是我们将下载网络架构以及权重。
- en: During training, the model learns only the features that exist in this training
    dataset. But when we download large models (like Inception) that have been trained
    on huge numbers of datasets (like ImageNet), all the features that have already
    been extracted from these large datasets are now available for us to use. I find
    that really exciting because these pretrained models have spotted other features
    that weren’t in our dataset and will help us build better convolutional networks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型只学习存在于这个训练数据集中的特征。但是，当我们下载在大量数据集（如ImageNet）上训练的大型模型（如Inception）时，从这些大型数据集中已经提取的所有特征现在都可供我们使用。我发现这真的很令人兴奋，因为这些预训练模型已经发现了我们数据集中没有的其他特征，这将帮助我们构建更好的卷积网络。
- en: In vision problems, there’s a huge amount of stuff for neural networks to learn
    about the training dataset. There are low-level features like edges, corners,
    round shapes, curvy shapes, and blobs; and then there are mid- and higher-level
    features like eyes, circles, squares, and wheels. There are many details in the
    images that CNNs can pick up on--but if we have only 1,000 images or even 25,000
    images in our training dataset, this may not be enough data for the model to learn
    all those things. By using a pretrained network, we can basically download all
    this knowledge into our neural network to give it a huge and much faster start
    with even higher performance levels.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉问题中，神经网络需要学习大量的关于训练数据集的知识。有低级特征，如边缘、角、圆形形状、曲线形状和块状物；还有中级和高级特征，如眼睛、圆形、方形和轮子。图像中有许多CNN可以捕捉到的细节——但如果我们训练数据集中只有1,000张图像，甚至25,000张图像，这可能不足以让模型学习所有这些内容。通过使用预训练网络，我们基本上可以将所有这些知识下载到我们的神经网络中，给它一个巨大且更快的起点，并实现更高的性能水平。
- en: 6.3.1 How do neural networks learn features?
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 神经网络是如何学习特征的？
- en: A neural network learns the features in a dataset step by step in increasing
    levels of complexity, one layer after another. These are called feature maps.
    The deeper you go through the network layers, the more image-specific features
    are learned. In figure 6.6, the first layer detects low-level features such as
    edges and curves. The output of the first layer becomes input to the second layer,
    which produces higher-level features like semicircles and squares. The next layer
    assembles the output of the previous layer into parts of familiar objects, and
    a subsequent layer detects the objects. As we go through more layers, the network
    yields an activation map that represents more complex features. As we go deeper
    into the network, the filters begin to be more responsive to a larger region of
    the pixel space. Higher-level layers amplify aspects of the received inputs that
    are important for discrimination and suppress irrelevant variations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过逐步增加复杂度，一层层地在数据集中学习特征。这些被称为特征图。你越深入网络层，学习的图像特定特征就越多。在图6.6中，第一层检测低级特征，如边缘和曲线。第一层的输出成为第二层的输入，产生更高层次的半圆和方形等特征。下一层将前一层输出组装成熟悉物体的部分，随后的一层检测物体。随着我们通过更多层，网络产生一个表示更复杂特征的激活图。随着我们深入网络，过滤器开始对像素空间更大区域做出更敏感的反应。高级层放大对区分重要方面有重要作用的输入，并抑制无关的变异。
- en: '![](../Images/6-6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6](../Images/6-6.png)'
- en: Figure 6.6 An example of how CNNs detect low-level generic features at the early
    layers of the network. The deeper you go through the network layers, the more
    image-specific features are learned.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 CNN在网络的早期层检测低级通用特征的示例。你越深入网络层，学习的图像特定特征就越多。
- en: 'Consider the example in figure 6.6\. Suppose we are building a model that detects
    human faces. We notice that the network learns low-level features like lines,
    edges, and blobs in the first layer. These low-level features appear not to be
    specific to a particular dataset or task; they are general features that are applicable
    to many datasets and tasks. The mid-level layers assemble those lines to be able
    to recognize shapes, corners, and circles. Notice that the extracted features
    start to get a little more specific to our task (human faces): mid-level features
    contain combinations of shapes that form objects in the human face like eyes and
    noses. As we go deeper through the network, we notice that features eventually
    transition from general to specific and, by the last layer of the network, form
    high-level features that are very specific to our task. We start seeing parts
    of human faces that distinguish one person from another.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图6.6中的示例。假设我们正在构建一个检测人脸的模型。我们注意到网络在第一层学习了低级特征，如线条、边缘和块状物。这些低级特征似乎并不特定于某个数据集或任务；它们是通用的特征，适用于许多数据集和任务。中级层将这些线条组装起来，以便能够识别形状、角和圆。请注意，提取的特征开始变得更加具体于我们的任务（人脸）：中级特征包含形成人脸中眼睛和鼻子等物体的形状组合。随着我们深入网络，我们注意到特征最终从通用过渡到具体，并且到网络的最后一层，形成了非常具体于我们任务的顶级特征。我们开始看到区分不同人的面部特征。
- en: Now, let’s take this example and compare the feature maps extracted from four
    models that are trained to classify faces, cars, elephants, and chairs (see figure
    6.7). Notice that the earlier layers’ features are very similar for all the models.
    They represent low-level features like edges, lines, and blobs. This means models
    that are trained on one task capture similar relations in the data types in the
    earlier layers of the network and can easily be reused for different problems
    in other domains. The deeper we go into the network, the more specific the features,
    until the network overfits its training data and it becomes harder to generalize
    to different tasks. The lower-level features are almost always transferable from
    one task to another because they contain generic information like the structure
    and nature of how images look. Transferring information like lines, dots, curves,
    and small parts of objects is very valuable for the network to learn faster and
    with less data on the new task.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们以这个例子为例，比较从四个模型中提取的特征图，这些模型被训练来分类人脸、汽车、大象和椅子（见图6.7）。注意，早期层的特征在所有模型中都非常相似。它们代表低级特征，如边缘、线条和块。这意味着在单一任务上训练的模型在网络的早期层中捕获了相似的数据类型关系，并且可以很容易地用于其他域的不同问题。我们越深入网络，特征就越具体，直到网络过度拟合其训练数据，使其更难泛化到不同的任务。较低级的特征几乎总是可以从一个任务迁移到另一个任务，因为它们包含通用的信息，如图像的结构和性质。将线条、点、曲线和物体的小部分信息迁移对于网络更快地学习以及在新任务上使用更少的数据是非常有价值的。
- en: '![](../Images/6-7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-7.png)'
- en: Figure 6.7 Feature maps extracted from four models that are trained to classify
    faces, cars, elephants, and chairs
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 从四个模型中提取的特征图，这些模型被训练来分类人脸、汽车、大象和椅子
- en: 6.3.2 Transferability of features extracted at later layers
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 后层提取特征的可迁移性
- en: 'The transferability of features that are extracted at later layers depends
    on the similarity of the original and new datasets. The idea is that all images
    must have shapes and edges, so the early layers are usually transferable between
    different domains. We can only identify differences between objects when we start
    extracting higher-level features: say, the nose on a face or the tires on a car.
    Only then can we say, “Okay, this is a person, because it has a nose. And this
    is a car, because it has tires.” Based on the similarity of the source and target
    domains, we can decide whether to transfer only the low-level features from the
    source domain, or the high-level features, or somewhere in between. This is motivated
    by the observation that the later layers of the network become progressively more
    specific to the details of the classes contained in the original dataset, as we
    are going to discuss in the next section.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在较深层提取的特征的可迁移性取决于原始数据集和新数据集的相似性。其理念是所有图像都必须有形状和边缘，因此早期层通常可以在不同域之间迁移。只有当我们开始提取更高级的特征时，我们才能识别物体之间的差异：比如，脸上的鼻子或汽车上的轮胎。只有在这种情况下，我们才能说，“好吧，这是一个人物，因为它有鼻子。这是汽车，因为它有轮胎。”基于源域和目标域的相似性，我们可以决定是否只从源域迁移低级特征，或者迁移高级特征，或者介于两者之间。这源于观察，随着我们讨论的下一段，网络的深层变得越来越具体于原始数据集中包含的类别的细节。
- en: DEFINITIONS The source domain is the original dataset that the pretrained network
    is trained on. The target domain is the new dataset that we want to train the
    network on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：源域是预训练网络所训练的原始数据集。目标域是我们希望训练网络的新数据集。
- en: 6.4 Transfer learning approaches
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 迁移学习方法
- en: 'There are three major transfer learning approaches: pretrained network as a
    classifier, pretrained network as a feature extractor, and fine-tuning. Each approach
    can be effective and save significant time in developing and training a deep CNN
    model. It may not be clear which use of a pretrained model may yield the best
    results on your new CV task, so some experimentation may be required. In this
    section, we will explain these three scenarios and give examples of how to implement
    them.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的迁移学习方法有三种：将预训练网络作为分类器、将预训练网络作为特征提取器以及微调。每种方法都可能有效，并且在开发和训练深度CNN模型时可以节省大量时间。可能不清楚哪种预训练模型的使用能在你的新计算机视觉任务上产生最佳结果，因此可能需要进行一些实验。在本节中，我们将解释这三种场景，并给出如何实现它们的示例。
- en: 6.4.1 Using a pretrained network as a classifier
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 使用预训练网络作为分类器
- en: Using a pretrained network as a classifier doesn’t involve freezing any layers
    or doing extra model training. Instead, we just take a network that was trained
    on a similar problem and deploy it directly to our task. The pretrained model
    is used directly to classify new images with no changes applied to it and no extra
    training. All we do is download the network architecture and its pretrained weights
    and then run the predictions directly on our new data. In this case, we are saying
    that the domain of our new problem is very similar to the one that the pretrained
    network was trained on, and it is ready to be deployed.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练网络作为分类器不需要冻结任何层或进行额外的模型训练。相反，我们只需取一个在类似问题上训练过的网络，并将其直接部署到我们的任务中。预训练模型直接用于对新图像进行分类，没有对其进行更改，也没有进行额外训练。我们所做的只是下载网络架构及其预训练权重，然后直接在我们的新数据上运行预测。在这种情况下，我们说我们新问题的领域与预训练网络训练的领域非常相似，并且它已经准备好部署。
- en: In the dog breed example, we could have used a VGG16 network that was trained
    on an ImageNet dataset directly to run predictions. ImageNet already contains
    a lot of dog images, so a significant portion of the representational power of
    the pretrained network may be devoted to features that are specific to differentiating
    between dog breeds.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在狗品种示例中，我们可以直接使用在ImageNet数据集上训练的VGG16网络来运行预测。ImageNet已经包含了很多狗的图片，因此预训练网络的大部分表示能力可能被用于区分不同狗品种的特征。
- en: Let’s see how to use a pretrained network as a classifier. In this example,
    we will use a VGG16 network that was pretrained on the ImageNet dataset to classify
    the image of the German Shepherd dog in figure 6.8.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用预训练网络作为分类器。在这个例子中，我们将使用在ImageNet数据集上预训练的VGG16网络来对图6.8中的德国牧羊犬图像进行分类。
- en: '![](../Images/6-8.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-8.png)'
- en: Figure 6.8 A sample image of a German Shepherd that we will use to run predictions
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 我们将用于运行预测的德国牧羊犬样本图像
- en: 'The steps are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Import the necessary libraries:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE6]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Download the pretrained model of VGG16 and its ImageNet weights. We set `include_top`
    to `True` because we want to use the entire network as a classifier:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载VGG16的预训练模型及其ImageNet权重。我们将`include_top`设置为`True`，因为我们想使用整个网络作为分类器：
- en: '[PRE7]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load and preprocess the input image:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并预处理输入图像：
- en: '[PRE8]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Loads an image from a file
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 从文件中加载图像
- en: ❷ Converts the image pixels to a NumPy array
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 将图像像素转换为NumPy数组
- en: ❸ Reshapes the data for the model
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 重新塑形数据以适应模型
- en: ❹ Prepares the image for the VGG model
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 准备图像以供VGG模型使用
- en: 'Now our input image is ready for us to run predictions:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在输入图像已经准备好供我们运行预测：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Predicts the probability across all output classes
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 预测所有输出类别的概率
- en: ❷ Converts the probabilities to class labels
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 将概率转换为类别标签
- en: ❸ Retrieves the most likely result with the highest probability
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 获取概率最高的最可能结果
- en: ❹ Prints the classification
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 打印分类
- en: 'When you run this code, you will get the following output:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行此代码时，你将得到以下输出：
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can see that the model was already trained to predict the correct dog breed
    with a high confidence score (99.72%). This is because the ImageNet dataset has
    more than 20,000 labeled dog images classified into 120 classes. Go to the book’s
    website to play with the code yourself with your own images: [www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    or [www.computervisionbook.com](http://www.computervisionbook.com). Feel free
    to explore the classes available in ImageNet and run this experiment on your own
    images.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，该模型已经被训练来以高置信度分数（99.72%）预测正确的狗品种。这是因为ImageNet数据集包含超过20,000个标记的狗图片，分为120个类别。前往本书的网站，用你自己的图片亲自尝试代码：[www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    或 [www.computervisionbook.com](http://www.computervisionbook.com)。请随意探索ImageNet中可用的类别，并在你自己的图片上运行此实验。
- en: 6.4.2 Using a pretrained network as a feature extractor
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 使用预训练网络作为特征提取器
- en: 'This approach is similar to the dog breed example that we implemented earlier
    in this chapter: we take a pretrained CNN on ImageNet, freeze its feature extraction
    part, remove the classifier part, and add our own new, dense classifier layers.
    In figure 6.9, we use a pretrained VGG16 network, freeze the weights in all 13
    convolutional layers, and replace the old classifier with a new one to be trained
    from scratch.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与我们本章早期实现的狗品种示例类似：我们从一个在ImageNet上预训练的CNN中提取，冻结其特征提取部分，移除分类器部分，并添加我们自己的新、密集分类器层。在图6.9中，我们使用预训练的VGG16网络，冻结所有13个卷积层的权重，并用一个新的分类器替换旧的分类器，以便从头开始训练。
- en: We usually go with this scenario when our new task is similar to the original
    dataset that the pretrained network was trained on. Since the ImageNet dataset
    has a lot of dog and cat examples, the feature maps that the network has learned
    contain a lot of dog and cat features that are very applicable to our new task.
    This means we can use the high-level features that were extracted from the ImageNet
    dataset in this new task.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的新任务与预训练网络训练的原数据集相似时，我们通常采用这种场景。由于ImageNet数据集包含大量的狗和猫的示例，网络学习到的特征图包含许多适用于我们新任务的狗和猫特征。这意味着我们可以使用从ImageNet数据集中提取的高级特征来完成这项新任务。
- en: To do that, we freeze all the layers from the pretrained network and only train
    the classifier part that we just added on the new dataset. This approach is called
    using a pretrained network as a feature extractor because we freeze the feature
    extractor part to transfer all the learned feature maps to our new problem. We
    only add a new classifier, which will be trained from scratch, on top of the pretrained
    model so that we can repurpose the previously learned feature maps for our dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们冻结预训练网络的所有层，并在新数据集上仅训练我们刚刚添加的分类器部分。这种方法被称为使用预训练网络作为特征提取器，因为我们冻结了特征提取器部分，以便将所有学习到的特征图转移到我们的新问题上。我们仅在预训练模型之上添加一个新的分类器，该分类器将从零开始训练，这样我们就可以重新利用之前学习到的特征图来处理我们的数据集。
- en: We remove the classification part of the pretrained network because it is often
    very specific to the original classification task, and subsequently it is specific
    to the set of classes on which the model was trained. For example, ImageNet has
    1,000 classes. The classifier part has been trained to overfit the training data
    to classify them into 1,000 classes. But in our new problem, let’s say cats versus
    dogs, we have only two classes. So, it is a lot more effective to train a new
    classifier from scratch to overfit these two classes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了预训练网络的分类部分，因为它通常非常特定于原始分类任务，并且随后它对模型训练的类别集合也是特定的。例如，ImageNet有1,000个类别。分类器部分已经被训练来过度拟合训练数据，将它们分类到1,000个类别中。但在我们的新问题中，比如猫与狗的区别，我们只有两个类别。因此，从头开始训练一个新的分类器来过度拟合这两个类别要有效得多。
- en: '![](../Images/6-9.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-9.png)'
- en: Figure 6.9 Load a pretrained VGG16 network, remove the classifier, and add your
    own classifier.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 加载预训练的VGG16网络，移除分类器，并添加自己的分类器。
- en: 6.4.3 Fine-tuning
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 微调
- en: 'So far, we’ve seen two basic approaches of using a pretrained network in transfer
    learning: using a pretrained network as a classifier or as a feature extractor.
    We generally use these approaches when the target domain is somewhat similar to
    the source domain. But what if the target domain is different from the source
    domain? What if it is very different? Can we still use transfer learning? Yes.
    Transfer learning works great even when the domains are very different. We just
    need to extract the correct feature maps from the source domain and fine-tune
    them to fit the target domain.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了两种使用预训练网络进行迁移学习的基本方法：将预训练网络用作分类器或特征提取器。我们通常在目标域与源域相似时使用这些方法。但如果目标域与源域不同呢？如果它非常不同呢？我们还能使用迁移学习吗？是的。即使域非常不同，迁移学习仍然效果很好。我们只需要从源域提取正确的特征图，并微调它们以适应目标域。
- en: In figure 6.10, we show the different approaches of transferring knowledge from
    a pretrained network. If you are downloading the entire network with no changes
    and just running predictions, then you are using the network as a classifier.
    If you are freezing the convolutional layers only, then you are using the pretrained
    network as a feature extractor and transferring all of its high-level feature
    maps to your domain. The formal definition of fine-tuning is freezing a few of
    the network layers that are used for feature extraction, and jointly training
    both the non-frozen layers and the newly added classifier layers of the pretrained
    model. It is called fine-tuning because when we retrain the feature extraction
    layers, we fine-tune the higher-order feature representations to make them more
    relevant for the new task dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.10中，我们展示了从预训练网络中转移知识的不同方法。如果你下载整个网络且没有进行任何更改，只是运行预测，那么你是在使用该网络作为分类器。如果你只冻结卷积层，那么你是在使用预训练网络作为特征提取器，并将所有高级特征图转移到你的领域。微调的正式定义是冻结用于特征提取的一些网络层，并联合训练非冻结层和预训练模型中新添加的分类器层。它被称为微调，因为我们重新训练特征提取层时，我们微调高阶特征表示，使其对新任务数据集更加相关。
- en: In more practical terms, if we freeze features maps 1 and 2 in figure 6.10,
    the new network will take feature maps 2 as its input and will start learning
    from this point to adapt the features of the later layers to the new dataset.
    This saves the network the time that it would have spent learning feature maps
    1 and 2.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在更实际的层面上，如果我们冻结图6.10中的特征图1和2，新的网络将使用特征图2作为其输入，并从这一点开始学习以适应后续层的特征到新的数据集。这节省了网络学习特征图1和2所需的时间。
- en: '![](../Images/6-10.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-10.png)'
- en: Figure 6.10 The network learns features through its layers. In transfer learning,
    we make a decision to freeze specific layers of a pretrained network to preserve
    the learned features. For example, if we freeze the network at feature maps of
    layer 3, we preserve what it has learned in layers 1, 2, and 3.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 网络通过其层学习特征。在迁移学习中，我们决定冻结预训练网络中的特定层以保留学习到的特征。例如，如果我们冻结网络在层3的特征图上，我们保留了它在层1、2和3中学到的内容。
- en: 'As we discussed earlier, feature maps that are extracted early in the network
    are generic. The feature maps get progressively more specific as we go deeper
    in the network. This means feature maps 4 in figure 6.10 are very specific to
    the source domain. Based on the similarity of the two domains, we can decide to
    freeze the network at the appropriate level of feature maps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，网络早期提取的特征图是通用的。随着我们深入网络，特征图变得越来越具体。这意味着图6.10中的特征图4对源领域非常具体。基于两个领域的相似性，我们可以决定在适当的特征图级别冻结网络：
- en: If the domains are similar, we might want to freeze the network up to the last
    feature map level (feature maps 4, in the example).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果领域相似，我们可能希望冻结网络直到最后一个特征图级别（例如，特征图4）。
- en: If the domains are very different, we might decide to freeze the pretrained
    network after feature maps 1 and retrain all the remaining layers.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果领域非常不同，我们可能决定在特征图1之后冻结预训练网络，并重新训练所有剩余的层。
- en: 'Between these two possibilities are a range of fine-tuning options that we
    can apply. We can retrain the entire network, or freeze the pretrained network
    at any level of feature maps 1, 2, 3, or 4 and retrain the remainder of the network.
    We typically decide the appropriate level of fine-tuning by trial and error. But
    there are guidelines that we can follow to intuitively decide on the fine-tuning
    level for the pretrained network. The decision is a function of two factors: the
    amount of data we have and the level of similarity between the source and target
    domains. We will explain these factors and the four possible scenarios to choose
    the appropriate level of fine-tuning in section 6.5.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种可能性之间，有一系列我们可以应用的微调选项。我们可以重新训练整个网络，或者冻结预训练网络在特征图1、2、3或4的任何级别，并重新训练剩余的网络。我们通常通过试错来决定适当的微调级别。但有一些指导原则，我们可以遵循以直观地决定预训练网络的微调级别。这个决定是两个因素的结果：我们拥有的数据量以及源领域和目标领域之间的相似程度。我们将在第6.5节中解释这些因素和四种可能的场景，以选择适当的微调级别。
- en: Why is fine-tuning better than training from scratch?
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么微调比从头开始训练更好？
- en: When we train a network from scratch, we usually randomly initialize the weights
    and apply a gradient descent optimizer to find the best set of weights that optimizes
    our error function (as discussed in chapter 2). Since these weights start with
    random values, there is no guarantee that they will begin with values that are
    close to the desired optimal values. And if the initialized value is far from
    the optimal value, the optimizer will take a long time to converge. This is when
    fine-tuning can be very useful. The pretrained network’s weights have been already
    optimized to learn from its dataset. Thus, when we use this network in our problem,
    we start with the weight values that it ended with. So, the network converges
    much faster than if it had to randomly initialize the weights. We are basically
    fine-tuning the already-optimized weights to fit our new problem instead of training
    the entire network from scratch with random weights. Even if we decide to retrain
    the entire pretrained network, starting with the trained weights will converge
    faster than having to train the network from scratch with randomly initialized
    weights.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从零开始训练一个网络时，我们通常随机初始化权重并应用梯度下降优化器来找到最佳权重集，以优化我们的误差函数（如第2章所述）。由于这些权重从随机值开始，没有保证它们会以接近期望的最优值开始。如果初始化值远离最优值，优化器将需要很长时间才能收敛。这时微调可以非常有用。预训练网络的权重已经优化以从其数据集中学习。因此，当我们使用这个网络来解决问题时，我们以它结束时的权重值开始。所以，网络收敛得比如果它必须从随机初始化的权重从头开始训练要快得多。我们基本上是在微调已经优化的权重以适应我们的新问题，而不是用随机权重从头开始训练整个网络。即使我们决定重新训练整个预训练网络，从训练好的权重开始也会比从头开始用随机初始化的权重训练网络收敛得更快。
- en: Using a smaller learning rate when fine-tuning
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调时使用较小的学习率
- en: It’s common to use a smaller learning rate for ConvNet weights that are being
    fine-tuned, in comparison to the (randomly initialized) weights for the new linear
    classifier that computes the class scores of a new dataset. This is because we
    expect that the ConvNet weights are relatively good, so we don’t want to distort
    them too quickly and too much (especially while the new classifier above them
    is being trained from random initialization).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与新数据集的类分数计算的新线性分类器（随机初始化）的权重相比，通常在微调时使用较小的学习率。这是因为我们预计卷积网络的权重相对较好，所以我们不希望太快太多地扭曲它们（尤其是在上面的新分类器从随机初始化中进行训练时）。
- en: 6.5 Choosing the appropriate level of transfer learning
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 选择适当的迁移学习级别
- en: Recall that early convolutional layers extract generic features and become more
    specific to the training data the deeper we go through the network. With that
    said, we can choose the level of detail for feature extraction from an existing
    pretrained model. For example, if a new task is quite different from the source
    domain of the pretrained network (for example, different from ImageNet), then
    perhaps the output of the pretrained model after the first few layers would be
    appropriate. If a new task is similar to the source domain, then perhaps the output
    from layers much deeper in the model can be used, or even the output of the fully
    connected layer prior to the softmax layer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，早期的卷积层提取的是通用特征，并且随着我们深入网络，它们对训练数据的特定性会越来越高。换句话说，我们可以从现有的预训练模型中选择特征提取的详细程度。例如，如果新的任务与预训练网络的源域（例如，不同于ImageNet）相当不同，那么预训练模型在第一层之后的输出可能就合适了。如果新的任务与源域相似，那么可能可以使用模型中更深层的输出，甚至可以使用在softmax层之前的全连接层的输出。
- en: 'As mentioned earlier, choosing the appropriate level for transfer learning
    is a function of two important factors:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，选择适当的迁移学习级别是两个重要因素的函数：
- en: Size of the target dataset (small or large) --When we have a small dataset,
    the network probably won’t learn much from training more layers, so it will tend
    to overfit the new data. In this case, we most likely want to do less fine-tuning
    and rely more on the source dataset.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标数据集的大小（小或大）--当我们有一个小数据集时，网络可能不会从训练更多层中学习到很多，因此它可能会过度拟合新数据。在这种情况下，我们可能希望进行较少的微调，并更多地依赖于源数据集。
- en: Domain similarity of the source and target datasets --How similar is our new
    problem to the domain of the original dataset? For example, if your problem is
    to classify cars and boats, ImageNet could be a good option because it contains
    a lot of images of similar features. On the other hand, if your problem is to
    classify lung cancer on X-ray images, this is a completely different domain that
    will likely require a lot of fine-tuning.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据集和目标数据集的领域相似性——我们的新问题与原始数据集的领域相似到什么程度？例如，如果你的问题是分类汽车和船只，ImageNet可能是一个不错的选择，因为它包含许多具有相似特征的图像。另一方面，如果你的问题是根据X射线图像对肺癌进行分类，这是一个完全不同的领域，可能需要大量的微调。
- en: 'These two factors lead to the four major scenarios:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个因素导致了四种主要场景：
- en: The target dataset is small and similar to the source dataset.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标数据集很小，并且与源数据集相似。
- en: The target dataset is large and similar to the source dataset.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标数据集很大且与源数据集相似。
- en: The target dataset is small and very different from the source dataset.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标数据集很小，并且与源数据集非常不同。
- en: The target dataset is large and very different from the source dataset.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标数据集很大，并且与源数据集非常不同。
- en: Let’s discuss these scenarios one by one to learn the common rules of thumb
    for navigating our options.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一讨论这些场景，以了解导航我们选项的常见经验法则。
- en: '6.5.1 Scenario 1: Target dataset is small and similar to the source dataset'
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 场景1：目标数据集很小且与源数据集相似
- en: Since the original dataset is similar to our new dataset, we can expect that
    the higher-level features in the pretrained ConvNet are relevant to our dataset
    as well. Then it might be best to freeze the feature extraction part of the network
    and only retrain the classifier.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原始数据集与我们的新数据集相似，我们可以预期预训练的卷积神经网络中的高级特征也与我们的数据集相关。因此，最好冻结网络的特征提取部分，只重新训练分类器。
- en: Another reason it might not be a good idea to fine-tune the network is that
    our new dataset is small. If we fine-tune the feature extraction layers on a small
    dataset, that will force the network to overfit to our data. This is not good
    because, by definition, a small dataset doesn’t have enough information to cover
    all possible features of its objects, which makes it fail to generalize to new,
    previously unseen, data. So in this case, the more fine-tuning we do, the more
    the network is prone to overfit the new data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能不是很好对网络进行微调的原因是我们的新数据集很小。如果我们在一个小数据集上微调特征提取层，这将迫使网络对我们的数据进行过度拟合。这并不好，因为根据定义，小数据集没有足够的信息来覆盖其对象的所有可能特征，这使得它无法泛化到新的、之前未见过的数据。因此，在这种情况下，我们进行的微调越多，网络就越容易过度拟合新数据。
- en: 'For example, suppose all the images in our new dataset contain dogs in a specific
    weather environment--snow, for example. If we fine-tuned on this dataset, we would
    force the new network to pick up features like snow and a white background as
    dog-specific features and make it fail to classify dogs in other weather conditions.
    Thus the general rule of thumb is: if you have a small amount of data, be careful
    of overfitting when you fine-tune your pretrained network.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们新数据集中的所有图像都包含在特定天气环境下的狗——例如雪。如果我们在这个数据集上微调，我们将迫使新的网络选择像雪和白色背景这样的特征作为狗的特定特征，并使其无法在其他天气条件下对狗进行分类。因此，一般经验法则是：如果你有少量数据，在微调预训练网络时要小心过度拟合。
- en: '6.5.2 Scenario 2: Target dataset is large and similar to the source dataset'
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 场景2：目标数据集很大且与源数据集相似
- en: Since both domains are similar, we can freeze the feature extraction part and
    retrain the classifier, similar to what we did in scenario 1\. But since we have
    more data in the new domain, we can get a performance boost from fine-tuning through
    all or part of the pretrained network with more confidence that we won’t overfit.
    Fine-tuning through the entire network is not really needed because the higher-level
    features are related (since the datasets are similar). So a good start is to freeze
    approximately 60-80% of the pretrained network and retrain the rest on the new
    data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个领域相似，我们可以冻结特征提取部分并重新训练分类器，类似于我们在场景1中做的。但由于我们新领域中的数据更多，我们可以通过微调整个预训练网络或其部分来获得性能提升，并且更有信心不会过度拟合。由于高级特征相关（因为数据集相似），因此不需要通过整个网络进行微调。所以一个好的开始是冻结大约60-80%的预训练网络，并在新数据上重新训练剩余的部分。
- en: '6.5.3 Scenario 3: Target dataset is small and different from the source dataset'
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 场景3：目标数据集很小且与源数据集不同
- en: Since the dataset is different, it might not be best to freeze the higher-level
    features of the pretrained network, because they contain more dataset-specific
    features. Instead, it would work better to retrain layers from somewhere earlier
    in the network--or to not freeze any layers and fine-tune the entire network.
    However, since you have a small dataset, fine-tuning the entire network on the
    dataset might not be a good idea, because doing so will make it prone to overfitting.
    A midway solution will work better in this case. A good start is to freeze approximately
    the first third or half of the pretrained network. After all, the early layers
    contain very generic feature maps that will be useful for your dataset even if
    it is very different.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集不同，可能最好不冻结预训练网络的更高层特征，因为它们包含更多数据集特定的特征。相反，从网络中较早的部分重新训练层会更好——或者不冻结任何层，对整个网络进行微调。然而，由于你的数据集较小，在整个数据集上微调整个网络可能不是一个好主意，因为这样做会使它容易过拟合。在这种情况下，一个折中的方案会更好。一个好的开始是冻结大约预训练网络的前三分之一或一半。毕竟，早期层包含非常通用的特征图，即使数据集非常不同，这些特征图对你的数据集也将是有用的。
- en: '6.5.4 Scenario 4: Target dataset is large and different from the source dataset'
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.4 场景 4：目标数据集很大且与源数据集不同
- en: Since the new dataset is large, you might be tempted to just train the entire
    network from scratch and not use transfer learning at all. However, in practice,
    it is often still very beneficial to initialize weights from a pretrained model,
    as we discussed earlier. Doing so makes the model converge faster. In this case,
    we have a large dataset that provides us with the confidence to fine-tune through
    the entire network without having to worry about overfitting.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新数据集很大，你可能想从头开始训练整个网络，根本不使用迁移学习。然而，在实践中，正如我们之前讨论的那样，从预训练模型初始化权重通常仍然非常有益。这样做可以使模型更快收敛。在这种情况下，我们有一个大型的数据集，这使我们能够有信心在整个网络上进行微调，而不必担心过拟合。
- en: 6.5.5 Recap of the transfer learning scenarios
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.5 迁移学习场景回顾
- en: We’ve explored the two main factors that help us define which transfer learning
    approach to use (size of our data and similarity between the source and target
    datasets). These two factors give us the four major scenarios defined in table
    6.1\. Figure 6.11 summarizes the guidelines for the appropriate fine-tuning level
    to use in each of the scenarios.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了帮助我们定义使用哪种迁移学习方法的两个主要因素（我们数据的大小和源数据集与目标数据集之间的相似性）。这两个因素为我们提供了表6.1中定义的四个主要场景。图6.11总结了在每个场景中应使用适当微调级别的指南。
- en: '![](../Images/6-11.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-11.png)'
- en: Figure 6.11 Guidelines for the appropriate fine-tuning level to use in each
    of the four scenarios
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 在四种场景中适当微调级别的指南
- en: Table 6.1 Transfer learning scenarios
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 迁移学习场景
- en: '| Scenario | Size of the target data | Similarity of the original and new datasets
    | Approach |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 目标数据集的大小 | 原始数据集和新数据集的相似性 | 方法 |'
- en: '| 1 | Small | Similar | Pretrained network as a feature extractor |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 小 | 相似 | 使用预训练网络作为特征提取器 |'
- en: '| 2 | Large | Similar | Fine-tune through the full network |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 大 | 相似 | 在整个网络中进行微调 |'
- en: '| 3 | Small | Very different | Fine-tune from activations earlier in the network
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 小 | 非常不同 | 从网络早期激活中进行微调 |'
- en: '| 4 | Large | Very different | Fine-tune through the entire network |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 大 | 非常不同 | 在整个网络中进行微调 |'
- en: 6.6 Open source datasets
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 开源数据集
- en: The CV research community has been pretty good about posting datasets on the
    internet. So, when you hear names like ImageNet, MS COCO, Open Images, MNIST,
    CIFAR, and many others, these are datasets that people have posted online and
    that a lot of computer researchers have used as benchmarks to train their algorithms
    and get state-of-the-art results.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: CV研究社区在互联网上发布数据集方面做得相当不错。所以，当你听到像ImageNet、MS COCO、Open Images、MNIST、CIFAR等名字时，这些是人们已经发布到网上，并且许多计算机研究人员已经将它们用作基准来训练他们的算法并获得最先进结果的数据集。
- en: In this section, we will review some of the popular open source datasets to
    help guide you in your search to find the most suitable dataset for your problem.
    Keep in mind that the ones listed in this chapter are the most popular datasets
    used in the CV research community at the time of writing; we do not intend to
    provide a comprehensive list of all the open source datasets out there. A great
    many image datasets are available, and the number is growing every day. Before
    starting your project, I encourage you to do your own research to explore the
    available datasets.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一些流行的开源数据集，以帮助您在寻找最适合您问题的数据集时提供指导。请注意，本章中列出的数据集是撰写时CV研究社区中最流行的数据集；我们并不打算提供所有开源数据集的完整列表。许多图像数据集可供使用，而且数量每天都在增长。在开始您的项目之前，我鼓励您进行自己的研究，以探索可用的数据集。
- en: 6.6.1 MNIST
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 MNIST
- en: MNIST ([http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist))
    stands for Modified National Institute of Standards and Technology. It contains
    labeled handwritten images of digits from 0 to 9\. The goal of this dataset is
    to classify handwritten digits. MNIST has been popular with the research community
    for benchmarking classification algorithms. In fact, it is considered the “hello,
    world!” of image datasets. But nowadays, the MNIST dataset is comparatively pretty
    simple, and a basic CNN can achieve more than 99% accuracy, so MNIST is no longer
    considered a benchmark for CNN performance. We implemented a CNN classification
    project using MNIST dataset in chapter 3; feel free to go back and review it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST ([http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist))
    代表修改后的国家标准与技术研究院。它包含从0到9的手写数字的标记图像。该数据集的目标是对手写数字进行分类。MNIST在研究社区中因其作为分类算法的基准而被广泛使用。实际上，它被认为是图像数据集的“hello,
    world!”。但如今，MNIST数据集相对比较简单，一个基本的卷积神经网络就能达到超过99%的准确率，因此MNIST不再被视为CNN性能的基准。我们在第3章中实现了使用MNIST数据集的CNN分类项目；请随意回顾。
- en: '![](../Images/6-12.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-12.png)'
- en: Figure 6.12 Samples from the MNIST dataset
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 MNIST数据集的样本
- en: MNIST consists of 60,000 training images and 10,000 test images. All are grayscale
    (one-channel), and each image is 28 pixels high and 28 pixels wide. Figure 6.12
    shows some sample images from the MNIST dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST包含60,000个训练图像和10,000个测试图像。所有图像都是灰度图（单通道），每个图像高28像素，宽28像素。图6.12展示了MNIST数据集的一些样本图像。
- en: 6.6.2 Fashion-MNIST
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.2 Fashion-MNIST
- en: 'Fashion-MNIST was created with the intention of replacing the original MNIST
    dataset, which has become too simple for modern convolutional networks. The data
    is stored in the same format as MNIST, but instead of handwritten digits, it contains
    60,000 training images and 10,000 test images of 10 fashion clothing classes:
    t-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and
    ankle boot. Visit [https:// github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)
    to explore and download the dataset. Figure 6.13 shows a sample of the represented
    classes.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion-MNIST是为了取代原始的MNIST数据集而创建的，因为对于现代卷积神经网络来说，它已经变得过于简单。数据存储的格式与MNIST相同，但不是手写数字，而是包含10个时尚服装类别的60,000个训练图像和10,000个测试图像：T恤/上衣、裤子、套头衫、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴。访问[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)以探索和下载数据集。图6.13展示了所代表类别的样本。
- en: '![](../Images/6-13.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-13.png)'
- en: Figure 6.13 Sample images from the Fashion-MNIST dataset
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 Fashion-MNIST数据集的样本图像
- en: 6.6.3 CIFAR
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.3 CIFAR
- en: CIFAR-10 ([www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html))
    is considered another benchmark dataset for image classification in the CV and
    ML literature. CIFAR images are more complex than those in MNIST in the sense
    that MNIST images are all grayscale with perfectly centered objects, whereas CIFAR
    images are color (three channels) with dramatic variation in how the objects appear.
    The CIFAR-10 dataset consists of 32×32 color images in 10 classes, with 6,000
    images per class. There are 50,000 training images and 10,000 test images. Figure
    6.14 shows the classes in the dataset.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10 ([www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html))
    被认为是CV和ML文献中图像分类的另一个基准数据集。与MNIST中的图像相比，CIFAR图像更为复杂，因为MNIST图像都是灰度图，且物体居中，而CIFAR图像是彩色（三个通道）的，物体外观变化很大。CIFAR-10数据集包含10个类别的32×32彩色图像，每个类别有6,000个图像。共有50,000个训练图像和10,000个测试图像。图6.14展示了数据集中的类别。
- en: '![](../Images/6-14.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-14.png)'
- en: Figure 6.14 Sample images from the CIFAR-10 dataset
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 CIFAR-10数据集的样本图像
- en: 'CIFAR-100 is the bigger brother of CIFAR-10: it contains 100 classes with 600
    images each. These 100 classes are grouped into 20 superclasses. Each image comes
    with a fine label (the class to which it belongs) and a coarse label (the superclass
    to which it belongs).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-100是CIFAR-10的“大哥”：它包含100个类别，每个类别有600个图像。这100个类别被分为20个超级类别。每个图像都附有精细标签（它所属的类别）和粗略标签（它所属的超级类别）。
- en: 6.6.4 ImageNet
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.4 ImageNet
- en: We’ve discussed the ImageNet dataset several times in the previous chapters
    and used it extensively in chapter 5 and this chapter. But for completeness of
    this list, we are discussing it here as well. At the time of writing, ImageNet
    is considered the current benchmark and is widely used by CV researchers to evaluate
    their classification algorithms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中多次讨论了ImageNet数据集，并在第5章和本章中广泛使用了它。但为了完整性，我们在这里也进行讨论。在撰写本文时，ImageNet被认为是当前的基准，并被CV研究人员广泛用于评估他们的分类算法。
- en: 'ImageNet is a large visual database designed for use in visual object recognition
    software research. It is aimed at labeling and categorizing images into almost
    22,000 categories based on a defined set of words and phrases. The images were
    collected from the web and labeled by humans via Amazon’s Mechanical Turk crowdsourcing
    tool. At the time of this writing, there are over 14 million images in the ImageNet
    project. To organize such a massive amount of data, the creators of ImageNet followed
    the WordNet hierarchy: each meaningful word/phrase in WordNet is called a synonym
    set (synset for short). Within the ImageNet project, images are organized according
    to these synsets, with the goal being to have 1,000+ images per synset. Figure
    6.15 shows a collage of ImageNet examples put together by Stanford University.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet是一个大型视觉数据库，旨在用于视觉对象识别软件研究。它旨在根据一组定义的单词和短语将图像标记和分类到近22,000个类别中。这些图像是从网络收集的，并由人类通过亚马逊的Mechanical
    Turk众包工具进行标记。在撰写本文时，ImageNet项目中已有超过1400万张图像。为了组织如此大量的数据，ImageNet的创造者遵循了WordNet层次结构：WordNet中的每个有意义的单词/短语被称为同义词集（简称synset）。在ImageNet项目中，图像根据这些synset组织，目标是每个synset有1,000+张图像。图6.15显示了由斯坦福大学汇编的ImageNet示例拼贴。
- en: '![](../Images/6-15.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-15.png)'
- en: Figure 6.15 A collage of ImageNet examples compiled by Stanford University
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 斯坦福大学汇编的ImageNet示例拼贴
- en: The CV community usually refers to the ImageNet Large Scale Visual Recognition
    Challenge (ILSVRC) when talking about ImageNet. In this challenge, software programs
    compete to correctly classify and detect objects and scenes. We will be using
    the ILSVRC challenge as a benchmark to compare the different networks’ performance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当CV社区谈论ImageNet时，通常指的是ImageNet大规模视觉识别挑战（ILSVRC）。在这个挑战中，软件程序竞争正确分类和检测对象和场景。我们将使用ILSVRC挑战作为基准来比较不同网络的性能。
- en: 6.6.5 MS COCO
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.5 MS COCO
- en: MS COCO ([http://cocodataset.org](http://cocodataset.org)) is short for Microsoft
    Common Objects in Context. It is an open source database that aims to enable future
    research for object detection, instance segmentation, image captioning, and localizing
    person keypoints. It contains 328,000 images. More than 200,000 of them are labeled,
    and they include 1.5 million object instances and 80 object categories that would
    be easily recognizable by a 4-year-old. The original research paper by the creators
    of the dataset describes the motivation for and content of this dataset.[2](#pgfId-1164918)
    Figure 6.16 shows a sample of the dataset provided on the MS COCO website.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: MS COCO ([http://cocodataset.org](http://cocodataset.org))代表Microsoft Common
    Objects in Context。它是一个开源数据库，旨在使未来的研究能够进行对象检测、实例分割、图像标题和定位人体关键点。它包含328,000张图像。其中超过200,000张被标记，包括1.5百万个对象实例和80个对象类别，这些类别对于一个4岁的孩子来说很容易识别。数据集创造者的原始研究论文描述了该数据集的动机和内容。[2](#pgfId-1164918)
    图6.16显示了MS COCO网站上提供的数据集样本。
- en: '![](../Images/6-16.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-16.png)'
- en: Figure 6.16 A sample of the MS COCO dataset (Image copyright © 2015, COCO Consortium,
    used by permission under Creative Commons Attribution 4.0 License.)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 MS COCO数据集的样本（图片版权©2015，COCO联盟，经Creative Commons Attribution 4.0许可使用。）
- en: 6.6.6 Google Open Images
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.6 Google Open Images
- en: Open Images ([https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html))
    is an open source image database created by Google. It contains more than 9 million
    images as of this writing. What makes it stand out is that these images are mostly
    of complex scenes that span thousands of classes of objects. Additionally, more
    than 2 million of these images are hand-annotated with bounding boxes, making
    Open Images by far the largest existing dataset with object-location annotations
    (see figure 6.17). In this subset of images, there are ~15.4 million bounding
    boxes of 600 classes of objects. Similar to ImageNet and ILSVRC, Open Images has
    a challenge called the Open Images Challenge ([http://mng.bz/aRQz](http://mng.bz/aRQz)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Open Images ([https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html))
    是由谷歌创建的一个开源图像数据库。截至本文撰写时，它包含超过 900 万张图像。使其脱颖而出的原因是这些图像大多是复杂场景，跨越了成千上万的物体类别。此外，其中超过
    200 万张图像被人工标注了边界框，使得 Open Images 成为迄今为止最大的具有物体位置标注的数据集（见图 6.17）。在这个图像子集中，有大约 1540
    万个 600 个类别物体的边界框。类似于 ImageNet 和 ILSVRC，Open Images 有一个名为 Open Images Challenge
    ([http://mng.bz/aRQz](http://mng.bz/aRQz)) 的挑战。
- en: 6.6.7 Kaggle
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.7 Kaggle
- en: In addition to the datasets listed in this section, Kaggle ([www.kaggle.com](http://www.kaggle.com))
    is another great source for datasets. Kaggle is a website that hosts ML and DL
    challenges where people from all around the world can participate and submit algorithms
    for evaluations.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本节中列出的数据集之外，Kaggle ([www.kaggle.com](http://www.kaggle.com)) 也是数据集的另一个优秀来源。Kaggle
    是一个网站，它主办了机器学习和深度学习挑战，来自世界各地的人们可以参与并提交算法以供评估。
- en: You are strongly encouraged to explore these datasets and search for the many
    other open source datasets that come up every day, to gain a better understanding
    of the classes and use cases they support. We mostly use ImageNet in this chapter’s
    projects; and throughout the book, we will be using MS COCO, especially in chapter
    7.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议你探索这些数据集，并寻找每天出现的许多其他开源数据集，以更好地理解它们支持的类别和使用案例。在本章的项目中，我们主要使用 ImageNet；在整个书中，我们将使用
    MS COCO，尤其是在第 7 章。
- en: '![](../Images/6-17.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6-17](../Images/6-17.png)'
- en: Figure 6.17 Annotated images from the Open Images dataset, taken from the Google
    AI Blog (Vittorio Ferrari, “An Update to Open Images--Now with Bounding-Boxes,”
    July 2017, [http://mng.bz/yyVG](http://mng.bz/yyVG)).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 来自 Open Images 数据集的标注图像，摘自谷歌 AI 博客（Vittorio Ferrari，“Open Images 更新——现在包含边界框”，2017
    年 7 月，[http://mng.bz/yyVG](http://mng.bz/yyVG))。
- en: '6.7 Project 1: A pretrained network as a feature extractor'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 项目 1：作为特征提取器的预训练网络
- en: In this project, we use a very small amount of data to train a classifier that
    detects images of dogs and cats. This is a pretty simple project, but the goal
    of the exercise is to see how to implement transfer learning when you have a very
    small amount of data and the target domain is similar to the source domain (scenario
    1). As explained in this chapter, in this case, we will use the pretrained convolutional
    network as a feature extractor. This means we are going to freeze the feature
    extractor part of the network, add our own classifier, and then retrain the network
    on our new small dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们使用非常少量的数据来训练一个检测狗和猫图像的分类器。这是一个相当简单的项目，但这个练习的目标是了解如何在数据非常少且目标域与源域相似（场景
    1）的情况下实现迁移学习。正如本章所解释的，在这种情况下，我们将使用预训练的卷积网络作为特征提取器。这意味着我们将冻结网络的特征提取部分，添加我们自己的分类器，然后在我们的新小型数据集上重新训练网络。
- en: 'One other important takeaway from this project is learning how to preprocess
    custom data and make it ready to train your neural network. In previous projects,
    we used the CIFAR and MNIST datasets: they are preprocessed by Keras, so all we
    had to do was download them from the Keras library and use them directly to train
    the network. This project provides a tutorial of how to structure your data repository
    and use the Keras library to get your data ready.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个项目中，另一个重要的收获是学习如何预处理自定义数据并将其准备好以训练你的神经网络。在以前的项目中，我们使用了 CIFAR 和 MNIST 数据集：它们已经被
    Keras 预处理，所以我们只需要从 Keras 库中下载它们，并直接用于训练网络。本项目提供了一个教程，说明如何构建你的数据存储库并使用 Keras 库来准备你的数据。
- en: Visit the book’s website at [www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    or [www.computervisionbook.com](http://www.computervisionbook.com) to download
    the code notebook and the dataset used for this project. Since we are using transfer
    learning, the training does not require high computation power, so you can run
    this notebook on your personal computer; you don’t need a GPU.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 访问书籍网站[www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)或[www.computervisionbook.com](http://www.computervisionbook.com)下载用于此项目的代码笔记本和数据集。由于我们使用迁移学习，训练不需要高计算能力，因此你可以在个人电脑上运行这个笔记本；你不需要GPU。
- en: For this implementation, we’ll be using the VGG16\. Although it didn’t record
    the lowest error in the ILSVRC, I found that it worked well for the task and was
    quicker to train than other models. I got an accuracy of about 96%, but you can
    feel free to use GoogLeNet or ResNet to experiment and compare results.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实现，我们将使用VGG16。尽管它没有在ILSVRC中记录最低的错误率，但我发现它对这项任务效果很好，并且比其他模型训练得更快。我得到了大约96%的准确率，但你完全可以自由地使用GoogLeNet或ResNet进行实验并比较结果。
- en: 'The process to use a pretrained model as a feature extractor is well established:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型作为特征提取器的过程已经确立：
- en: Import the necessary libraries.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Preprocess the data to make it ready for the neural network.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据，使其准备好用于神经网络。
- en: Load pretrained weights from the VGG16 network trained on a large dataset.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从在大数据集上训练的VGG16网络中加载预训练的权重。
- en: Freeze all the weights in the convolutional layers (feature extraction part).
    Remember, the layers to freeze are adjusted depending on the similarity of the
    new task to the original dataset. In our case, we observed that ImageNet has a
    lot of dog and cat images, so the network has already been trained to extract
    the detailed features of our target object.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结卷积层（特征提取部分）中的所有权重。记住，要冻结的层会根据新任务与原始数据集的相似性进行调整。在我们的案例中，我们观察到ImageNet有很多狗和猫的图片，因此网络已经训练好了提取我们目标对象的详细特征。
- en: Replace the fully connected layers of the network with a custom classifier.
    You can add as many fully connected layers as you see fit, and each can have as
    many hidden units as you want. For simple problems like this, we will just add
    one hidden layer with 64 units. You can observe the results and tune up if the
    model is underfitting or down if the model is overfitting. For the softmax layer,
    the number of units must be set equal to the number of classes (two units, in
    our case).
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用自定义分类器替换网络的完全连接层。你可以添加你认为合适的完全连接层，每个层可以有任意数量的隐藏单元。对于像这样的简单问题，我们将只添加一个包含64个单元的隐藏层。你可以观察结果，如果模型欠拟合就调整，如果模型过拟合就下调。对于softmax层，单元的数量必须设置为类别数（在我们的案例中是两个单元）。
- en: Compile the network, and run the training process on the new data of cats and
    dogs to optimize the model for the smaller dataset.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译网络，并在新的猫狗数据上运行训练过程以优化模型，使其适用于较小的数据集。
- en: Evaluate the model.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型。
- en: 'Now, let’s go through these steps and implement this project:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐步进行这些步骤并实现这个项目：
- en: 'Import the necessary libraries:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE11]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Preprocess the data to make it ready for the neural network. Keras has an `ImageDataGenerator`
    class that allows us to easily perform image augmentation on the fly; you can
    read about it at [https://keras.io/api/preprocessing/image](https://keras.io/api/preprocessing/image).
    In this example, we use `ImageDataGenerator` to generate our image tensors, but
    for simplicity, we will not implement image augmentation.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据，使其准备好用于神经网络。Keras有一个`ImageDataGenerator`类，允许我们轻松地即时执行图像增强；你可以在[https://keras.io/api/preprocessing/image](https://keras.io/api/preprocessing/image)上了解更多信息。在这个例子中，我们使用`ImageDataGenerator`生成我们的图像张量，但为了简单起见，我们不会实现图像增强。
- en: The `ImageDataGenerator` class has a method called `flow_from_directory``()`
    that is used to read images from folders containing images. This method expects
    your data directory to be structured as in figure 6.18.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`ImageDataGenerator`类有一个名为`flow_from_directory()`的方法，用于从包含图像的文件夹中读取图像。此方法期望你的数据目录结构如图6.18所示。'
- en: '![](../Images/6-18.png)'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/6-18.png)'
- en: Figure 6.18 The required directory structure for your dataset to use the `.flow_from_directory()`
    method from Keras
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.18 使用Keras的`.flow_from_directory()`方法所需的目录结构
- en: 'I have the data structured in the book’s code so it’s ready for you to use
    `flow_` `from_directory()`. Now, load the data into `train_path`, `valid_path`,
    and `test` `_path` variables, and then generate the train, valid, and test batches:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我已经将数据结构在书的代码中，所以它已经准备好供你使用`flow_from_directory()`。现在，将数据加载到`train_path`、`valid_path`和`test_path`变量中，然后生成训练、验证和测试批次：
- en: '[PRE12]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ ImageDataGenerator generates batches of tensor image data with real-time data
    augmentation. The data will be looped over (in batches). In this example, we won’t
    be doing any image augmentation.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ ImageDataGenerator生成具有实时数据增强的批处理张量图像数据。数据将循环（以批处理形式）。在这个例子中，我们不会进行任何图像增强。
- en: 'Load in pretrained weights from the VGG16 network trained on a large dataset.
    Similar to the examples in this chapter, we download the VGG16 network from Keras
    and download its weights after they are pretrained on the ImageNet dataset. Remember
    that we want to remove the classifier part from this network, so we set the parameter
    `include_top=False`:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从在大数据集上训练的VGG16网络中加载预训练的权重。类似于本章中的示例，我们从Keras下载VGG16网络，并在ImageNet数据集上预训练后下载其权重。请记住，我们想要从这个网络中移除分类器部分，因此我们将参数`include_top=False`设置为：
- en: '[PRE13]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Freeze all the weights in the convolutional layers (feature extraction part).
    We freeze the convolutional layers from the `base_model` created in the previous
    step and use that as a feature extractor, and then add a classifier on top of
    it in the next step:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结卷积层（特征提取部分）中的所有权重。我们冻结了之前步骤中创建的`base_model`中的卷积层，并将其用作特征提取器，然后在下一步中在其顶部添加分类器：
- en: '[PRE14]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Iterates through layers and locks them to make them non-trainable with this
    code
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 通过此代码遍历层并将它们锁定，以使它们不可训练
- en: 'Add the new classifier, and build the new model. We add a few layers on top
    of the base model. In this example, we add one fully connected layer with 64 hidden
    units and a softmax with 2 hidden units. We also add batch norm and dropout layers
    to avoid overfitting:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新的分类器，并构建新的模型。我们在基础模型之上添加了一些层。在这个例子中，我们添加了一个具有64个隐藏单元的全连接层和一个具有2个隐藏单元的softmax层。我们还添加了批归一化和dropout层以避免过拟合：
- en: '[PRE15]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Uses the get_layer method to save the last layer of the network. Then saves
    the output of the last layer to be the input of the next layer.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 使用get_layer方法保存网络的最后一层。然后将最后一层的输出保存为下一层的输入。
- en: ❷ Flattens the classifier input, which is output of the last layer of the VGG16
    model
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 将分类器输入（VGG16模型的最后一层的输出）展平
- en: ❸ Adds one fully connected layer that has 64 units and batchnorm, dropout, and
    softmax layers
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 添加一个具有64个单元和批归一化、dropout和softmax层的全连接层
- en: ❹ Instantiates a new_model using Keras’s Model class
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 使用Keras的Model类实例化一个新模型
- en: 'Compile the model and run the training process:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型并运行训练过程：
- en: '[PRE16]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When you run the previous code snippet, the verbose training is printed after
    each epoch as follows:'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你运行前面的代码片段时，每个epoch之后都会打印出详细的训练信息，如下所示：
- en: '[PRE17]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that the model was trained very quickly using regular CPU computing power.
    Each epoch took approximately 25 to 29 seconds, which means the model took less
    than 10 minutes to train for 20 epochs.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，该模型使用常规CPU计算能力训练得非常快。每个epoch大约需要25到29秒，这意味着模型在20个epoch的训练中花费不到10分钟。
- en: 'Evaluate the model. First, let’s define the `load_dataset()` method that we
    will use to convert our dataset into tensors:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型。首先，让我们定义`load_dataset()`方法，我们将使用它将我们的数据集转换为张量：
- en: '[PRE18]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we create test_tensors to evaluate the model on them:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们创建test_tensors以在它们上评估模型：
- en: '[PRE19]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Loads an RGB image as PIL.Image.Image type
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 以PIL.Image.Image类型加载RGB图像
- en: ❷ Converts the PIL.Image.Image type to a 3D tensor with shape (224, 224, 3)
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 将PIL.Image.Image类型转换为形状为(224, 224, 3)的3D张量
- en: ❸ Converts the 3D tensor to a 4D tensor with shape (1, 224, 224, 3) and returns
    the 4D tensor
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 将3D张量转换为形状为(1, 224, 224, 3)的4D张量，并返回4D张量
- en: 'Now we can run Keras’s `evaluate()` method to calculate the model accuracy:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们可以运行Keras的`evaluate()`方法来计算模型准确率：
- en: '[PRE20]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The model has achieved an accuracy of 95.79% in less than 10 minutes of training.
    This is very good, given our very small dataset.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在不到10分钟的训练时间内达到了95.79%的准确率。考虑到我们的数据集非常小，这是一个非常好的结果。
- en: '6.8 Project 2: Fine-tuning'
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.8 项目2：微调
- en: 'In this project, we are going to explore scenario 3, discussed earlier in this
    chapter, where the target dataset is small and very different from the source
    dataset. The goal of this project is to build a sign language classifier that
    distinguishes 10 classes: the sign language digits from 0 to 9\. Figure 6.19 shows
    a sample of our dataset.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将探讨本章前面讨论过的场景3，其中目标数据集很小，并且与源数据集非常不同。这个项目的目标是构建一个能够区分10个类别的手势语言分类器：从0到9的手势语言数字。图6.19展示了我们数据集的一个样本。
- en: 'Following are the details of our dataset:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的数据集详细信息：
- en: Number of classes = 10 (digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9)
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别数量 = 10（数字0，1，2，3，4，5，6，7，8和9）
- en: Image size = 100 × 100
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像大小 = 100 × 100
- en: Color space = RGB
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 色彩空间 = RGB
- en: 1,712 images in the training set
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集包含1,712个图像
- en: 300 images in the validation set
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集包含300个图像
- en: 50 images in the test set
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集包含50个图像
- en: '![](../Images/6-19.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-19.png)'
- en: Figure 6.19 A sample from the sign language dataset
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 标准手势数据集的一个样本
- en: It is very noticeable how small our dataset is. If you try to train a network
    from scratch on this very small dataset, you will not achieve good results. On
    the other hand, we were able to achieve an accuracy higher than 98% by using transfer
    learning, even though the source and target domains were very different.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集非常小这一点非常明显。如果你尝试在这个非常小的数据集上从头开始训练网络，你将不会取得好的结果。另一方面，尽管源域和目标域非常不同，我们仍然能够通过使用迁移学习达到超过98%的准确率。
- en: NOTE Please take this evaluation with a grain of salt, because the network hasn't
    been thoroughly tested with a lot of data. We only have 50 test images in this
    dataset. Transfer learning is expected to achieve good results anyway, but I wanted
    to highlight this fact.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请带着批判的眼光看待这次评估，因为网络还没有用大量数据彻底测试。在这个数据集中我们只有50个测试图像。尽管如此，预期迁移学习仍然能够取得良好的结果，但我还是想强调这一点。
- en: Visit the book’s website at [www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    or [www.computervisionbook.com](http://www.computervisionbook.com) to download
    the source code notebook and the dataset used for this project. Similar to project
    1, the training does not require high computation power, so you can run this notebook
    on your personal computer; you don’t need a GPU.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 访问本书的网站[www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)或[www.computervisionbook.com](http://www.computervisionbook.com)以下载用于此项目的源代码笔记本和数据集。与项目1类似，训练不需要高计算能力，因此你可以在个人电脑上运行这个笔记本；你不需要GPU。
- en: 'For ease of comparison with the previous project, we will use the VGG16 network
    trained on the ImageNet dataset. The process to fine-tune a pretrained network
    is as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便与前面的项目进行比较，我们将使用在ImageNet数据集上训练的VGG16网络。微调预训练网络的步骤如下：
- en: Import the necessary libraries.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Preprocess the data to make it ready for the neural network.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据以使其准备好供神经网络使用。
- en: Load in pretrained weights from the VGG16 network trained on a large dataset
    (ImageNet).
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从在大数据集（ImageNet）上训练的VGG16网络中加载预训练的权重。
- en: Freeze part of the feature extractor part.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结特征提取器部分的一部分。
- en: Add the new classifier layers.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新的分类器层。
- en: Compile the network, and run the training process to optimize the model for
    the smaller dataset.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译网络，并运行训练过程以优化模型以适应较小的数据集。
- en: Evaluate the model.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型。
- en: 'Now let’s implement this project:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现这个项目：
- en: 'Import the necessary libraries:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE21]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Preprocess the data to make it ready for the neural network. Similar to project
    1, we use the `ImageDataGenerator` class from Keras and the `flow_from_ directory()`
    method to preprocess our data. The data is already structured for you to directly
    create your tensors:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据以使其准备好供神经网络使用。与项目1类似，我们使用Keras中的`ImageDataGenerator`类和`flow_from_directory()`方法来预处理我们的数据。数据已经为你结构化，可以直接创建张量：
- en: '[PRE22]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ ImageDataGenerator generates batches of tensor image data with real-time data
    augmentation. The data will be looped over (in batches). In this example, we won’t
    be doing any image augmentation.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ `ImageDataGenerator` 生成具有实时数据增强的批处理张量图像数据。数据将会循环（以批处理形式）。在这个例子中，我们不会进行任何图像增强。
- en: 'Load in pretrained weights from the VGG16 network trained on a large dataset
    (ImageNet). We download the VGG16 architecture from the Keras library with ImageNet
    weights. Note that we use the parameter `pooling=''avg''` here: this basically
    means global average pooling will be applied to the output of the last convolutional
    layer, and thus the output of the model will be a 2D tensor. We use this as an
    alternative to the `Flatten` layer before adding the fully connected layers:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从在大型数据集（ImageNet）上训练的VGG16网络中加载预训练的权重。我们从Keras库中下载带有ImageNet权重的VGG16架构。注意，我们在这里使用参数`pooling='avg'`：这基本上意味着将对最后一个卷积层的输出应用全局平均池化，因此模型的输出将是一个2D张量。我们将其用作在添加全连接层之前`Flatten`层的替代方案：
- en: '[PRE23]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Freeze some of the feature extractor part, and fine-tune the rest on our new
    training data. The level of fine-tuning is usually determined by trial and error.
    VGG16 has 13 convolutional layers: you can freeze them all or freeze a few of
    them, depending on how similar your data is to the source data. In the sign language
    case, the new domain is very different from our domain, so we will start with
    fine-tuning only the last five layers; if we don’t get satisfying results, we
    can fine-tune more. It turns out that after we trained the new model, we got 98%
    accuracy, so this was a good level of fine-tuning. But in other cases, if you
    find that your network doesn’t converge, try fine-tuning more layers.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结特征提取器部分的一些层，并在我们的新训练数据上微调其余部分。微调的程度通常是通过试错来确定的。VGG16有13个卷积层：你可以冻结所有层，也可以根据你的数据与源数据的相似程度冻结其中一些层。在手语案例中，新领域与我们的领域非常不同，因此我们将从仅微调最后五层开始；如果我们没有得到令人满意的结果，我们可以进一步微调。结果是我们训练了新模型后，达到了98%的准确率，所以这是一个很好的微调水平。但在其他情况下，如果你发现你的网络没有收敛，尝试微调更多层。
- en: '[PRE24]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Iterates through layers and locks them, except for the last five layers
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 遍历层并锁定它们，除了最后五层
- en: 'Add the new classifier layers, and build the new model:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新的分类器层，并构建新的模型：
- en: '[PRE25]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Saves the output of base_model to be the input of the next layer
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 将base_model的输出保存为下一层的输入
- en: ❷ Adds our new softmax layer with 10 hidden units
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 添加我们的新softmax层，包含10个隐藏单元
- en: ❸ Instantiates a new_model using Keras’s Model class
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 使用Keras的Model类实例化一个新模型
- en: ❹ Prints the new_model summary
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❹ 打印新模型的摘要
- en: 'Compile the network, and run the training process to optimize the model for
    the smaller dataset:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译网络，并运行训练过程以优化模型以适应较小的数据集：
- en: '[PRE26]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice the training time of each epoch from the verbose output. The model was
    trained very quickly using regular CPU computing power. Each epoch took approximately
    40 seconds, which means it took the model less than 15 minutes to train for 20
    epochs.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意从详细输出中查看每个epoch的训练时间。该模型使用常规CPU计算能力训练得非常快。每个epoch大约需要40秒，这意味着模型在15分钟内训练了20个epoch。
- en: 'Evaluate the accuracy of the model. Similar to the previous project, we create
    a `load_dataset()` method to create `test_targets` and `test_tensors` and then
    use the `evaluate()` method from Keras to run inferences on the test images and
    get the model accuracy:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型的准确率。与之前的项目类似，我们创建了一个`load_dataset()`方法来创建`test_targets`和`test_tensors`，然后使用Keras的`evaluate()`方法在测试图像上运行推理并获取模型准确率：
- en: '[PRE27]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A deeper level of evaluating your model involves creating a confusion matrix.
    We explained the confusion matrix in chapter 4: it is a table that is often used
    to describe the performance of a classification model, to provide a deeper understanding
    of how the model performed on the test dataset. See chapter 4 for details on the
    different model evaluation metrics. Now, let’s build the confusion matrix for
    our model (see figure 6.20):'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估模型更深层次的方法是创建一个混淆矩阵。我们在第4章中解释了混淆矩阵：它是一个常用来描述分类模型性能的表格，以提供对模型在测试数据集上表现更深入的理解。有关不同模型评估指标的具体细节，请参阅第4章。现在，让我们为我们的模型构建一个混淆矩阵（见图6.20）：
- en: '[PRE28]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To read this confusion matrix, look at the number on the Predicted Label axis
    and check whether it was correctly classified on the True Label axis. For example,
    look at number 0 on the Predicted Label axis: all five images were classified
    as 0, and no images were mistakenly classified as any other number. Similarly,
    go through the rest of the numbers on the Predicted Label axis. You will notice
    that the model successfully made the correct predictions for all the test images
    except the image with true label = 8\. In that case, the model mistakenly classified
    an image of number 8 as number = 7.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要阅读这个混淆矩阵，请查看预测标签轴上的数字，并检查它是否在真实标签轴上正确分类。例如，查看预测标签轴上的数字0：所有五张图片都被分类为0，没有图片被错误地分类为其他任何数字。同样，查看预测标签轴上的其余数字。你会注意到，模型成功地对所有测试图像进行了正确预测，除了真实标签为8的图像。在这种情况下，模型错误地将数字8的图像分类为数字7。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Transfer learning is usually the go-to approach when starting a classification
    and object detection project, especially when you don’t have a lot of training
    data.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习通常是在开始分类和目标检测项目时首选的方法，尤其是在你没有大量训练数据时。
- en: Transfer learning migrates the knowledge learned from the source dataset to
    the target dataset, to save training time and computational cost.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习将源数据集学习到的知识迁移到目标数据集，以节省训练时间和计算成本。
- en: The neural network learns the features in your dataset step by step in increasing
    levels of complexity. The deeper you go through the network layers, the more image-specific
    the features that are learned.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络逐步以递增的复杂度层次学习数据集中的特征。你越深入网络层，学习的特征就越具有图像特异性。
- en: Early layers in the network learn low-level features like lines, blobs, and
    edges. The output of the first layer becomes input to the second layer, which
    produces higher-level features. The next layer assembles the output of the previous
    layer into parts of familiar objects, and a subsequent layer detects the objects.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络的早期层学习低级特征，如线条、块和边缘。第一层的输出成为第二层的输入，产生更高级的特征。下一层将前一层输出组装成熟悉物体的部分，后续层检测物体。
- en: The three main transfer learning approaches are using a pretrained network as
    a classifier, using a pretrained network as a feature extractor, and fine-tuning.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习的主要方法有：使用预训练网络作为分类器、使用预训练网络作为特征提取器以及微调。
- en: Using a pretrained network as a classifier means using the network directly
    to classify new images without freezing layers or applying model training.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练网络作为分类器意味着直接使用网络对新图像进行分类，而不冻结层或应用模型训练。
- en: Using a pretrained network as a feature extractor means freezing the classifier
    part of the network and retraining the new classifier.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练网络作为特征提取器意味着冻结网络的分类器部分，并重新训练新的分类器。
- en: Fine-tuning means freezing a few of the network layers that are used for feature
    extraction, and jointly training both the non-frozen layers and the newly added
    classifier layers of the pretrained model.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调意味着冻结用于特征提取的一些网络层，并联合训练非冻结层和预训练模型中新添加的分类器层。
- en: '![](../Images/6-20.png)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/6-20.png)'
- en: Figure 6.20 Confusion matrix for the sign language classifier
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.20 手语分类器的混淆矩阵
- en: The transferability of features from one network to another is a function of
    the size of the target data and the domain similarity between the source and target
    data.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络到另一个网络的特征可迁移性是目标数据大小和源数据与目标数据域相似性的函数。
- en: Generally, fine-tuning parameters use a smaller learning rate, while training
    the output layer from scratch can use a larger learning rate.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用较大的学习率。
- en: '* * *'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '1.Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson, “How Transferable
    Are Features in Deep Neural Networks?” Advances in Neural Information Processing
    Systems 27 (Dec. 2014): 3320-3328, [https://arxiv.org/ abs/1411.1792](https://arxiv.org/abs/1411.1792).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '1.Jason Yosinski, Jeff Clune, Yoshua Bengio, 和 Hod Lipson, “How Transferable
    Are Features in Deep Neural Networks?” Advances in Neural Information Processing
    Systems 27 (Dec. 2014): 3320-3328, [https://arxiv.org/abs/1411.1792](https://arxiv.org/abs/1411.1792).'
- en: '2. Tsung-Yi Lin, Michael Maire, Serge Belongie, et al., “Microsoft COCO: Common
    Objects in Context” (February 2015), [https://arxiv.org/pdf/1405.0312.pdf](https://arxiv.org/pdf/1405.0312.pdf).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '2. Tsung-Yi Lin, Michael Maire, Serge Belongie, 等，“Microsoft COCO: Common Objects
    in Context” (February 2015), [https://arxiv.org/pdf/1405.0312.pdf](https://arxiv.org/pdf/1405.0312.pdf).'
