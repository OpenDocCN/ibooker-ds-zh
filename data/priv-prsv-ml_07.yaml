- en: 5 Advanced LDP mechanisms for machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 适用于机器学习的高级LDP机制
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Advanced LDP mechanisms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级LDP机制
- en: Working with naive Bayes for ML classification
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行机器学习分类
- en: Using LDP naive Bayes for discrete features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LDP朴素贝叶斯处理离散特征
- en: Using LDP naive Bayes for continuous features and multidimensional data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LDP朴素贝叶斯处理连续特征和多维数据
- en: Designing and analyzing an LDP ML algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和分析LDP机器学习算法
- en: In the previous chapter we looked into the basic concepts and definition of
    local differential privacy (LDP), along with its underlying mechanisms and some
    examples. However, most of those mechanisms are explicitly designed for one-dimensional
    data and frequency estimation techniques, with direct encoding, histogram encoding,
    unary encoding, and so on. In this chapter we will extend our discussion further
    and look at how we can work with multidimensional data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了局部微分隐私（LDP）的基本概念和定义，以及其底层机制和一些示例。然而，大多数这些机制都是专门为处理一维数据和频率估计技术而设计的，包括直接编码、直方图编码、一元编码等。在本章中，我们将进一步扩展我们的讨论，并探讨如何处理多维数据。
- en: First, we’ll introduce an example machine learning (ML) use case with naive
    Bayes classification. Then, we’ll look at a case study implementation of LDP naive
    Bayes by designing and analyzing an LDP ML algorithm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍一个使用朴素贝叶斯分类的机器学习（ML）用例示例。然后，我们将通过设计和分析一个LDP ML算法来探讨LDP朴素贝叶斯案例研究的实现。
- en: '![CH05_00_UN01_Zhuang](../../OEBPS/Images/CH05_00_UN01_Zhuang.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_00_UN01_Zhuang](../../OEBPS/Images/CH05_00_UN01_Zhuang.png)'
- en: 5.1 A quick recap of local differential privacy
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 局部微分隐私快速回顾
- en: As we discussed in the previous chapter, LDP is a way of measuring individual
    privacy when the data collector is not trusted. LDP aims to guarantee that when
    an individual provides a particular value, it should be difficult to identify
    the individual, thus providing privacy protection. Many LDP mechanisms also aim
    to estimate the distribution of the population as accurately as possible, based
    on an aggregation of perturbed data collected from multiple individuals.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章所讨论的，当数据收集者不可信时，LDP是一种衡量个人隐私的方式。LDP的目标是确保当个人提供特定值时，难以识别该个人，从而提供隐私保护。许多LDP机制还旨在尽可能准确地估计人群的分布，基于从多个个体收集的扰动数据的聚合。
- en: Figure 5.1 summarizes the steps of applying LDP in different application scenarios.
    For more about how LDP works, see chapter 4.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1总结了在不同应用场景中应用LDP的步骤。有关LDP如何工作的更多信息，请参阅第四章。
- en: '![CH05_F01_Zhuang](../../OEBPS/Images/CH05_F01_Zhuang.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_Zhuang](../../OEBPS/Images/CH05_F01_Zhuang.png)'
- en: 'Figure 5.1 How LDP works: each data owner perturbs their data locally and submits
    it to the aggregator.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 LDP的工作原理：每个数据所有者对其数据进行局部扰动，并将其提交给聚合器。
- en: Now that we’ve reviewed the basics of how LDP works, let’s look at some more
    advanced LDP mechanisms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了LDP的工作原理，让我们来看看一些更高级的LDP机制。
- en: 5.2 Advanced LDP mechanisms
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 高级LDP机制
- en: In chapter 4 we looked at the direct encoding, histogram encoding, and unary
    encoding LDP mechanisms. Those algorithms all work for one-dimensional categorical
    data or discrete numerical data. For example, the answer to the survey question
    “What is your occupation?” will be one occupation category, which is one-dimensional
    categorical data. The answer to the survey question “What is your age?” will be
    a one-dimensional discrete numerical value. However, many other datasets, especially
    when working with ML tasks, are more complex and contain multidimensional continuous
    numerical data. For instance, pixel-based images are usually high-dimensional,
    and mobile-device sensor data (such as gyroscope or accelerometer sensors) is
    multidimensional, usually continuous, numerical data. Therefore, learning about
    the LDP mechanisms that deal with such scenarios in ML tasks is essential.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四章中，我们探讨了直接编码、直方图编码和一元编码LDP机制。这些算法都适用于一维分类数据或离散数值数据。例如，调查问题“你的职业是什么？”的答案将是一个职业类别，这是一维分类数据。调查问题“你的年龄是多少？”的答案将是一个一维离散数值。然而，许多其他数据集，尤其是在处理机器学习任务时，更为复杂，包含多维连续数值数据。例如，基于像素的图像通常是高维的，而移动设备传感器数据（如陀螺仪或加速度计传感器）是多维的，通常是连续的数值数据。因此，了解在机器学习任务中处理此类场景的LDP机制至关重要。
- en: 'In this section we’ll focus on three different mechanisms that are designed
    for multidimensional continuous numerical data: the Laplace mechanism, Duchi’s
    mechanism, and the Piecewise mechanism.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将关注三种为多维连续数值数据设计的不同机制：Laplace机制、Duchi机制和分段机制。
- en: 5.2.1 The Laplace mechanism for LDP
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 LDP的Laplace机制
- en: In chapter 2 we introduced the Laplace mechanism for centralized differential
    privacy. We can similarly implement the Laplace mechanism for LDP. First, though,
    let’s review some basics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们介绍了用于集中式差分隐私的Laplace机制。我们可以类似地实现用于LDP的Laplace机制。不过，首先，让我们回顾一些基础知识。
- en: For simplicity, let’s assume each participant in the LDP mechanism is *u*[i]
    and that each *u*[i]’s data record is *t*[i]. This *t*[i] is a usually one-dimensional
    numerical value in the range of -1 to 1 (as discussed in the previous chapter),
    and we can mathematically represent it as *t*[i] ∈ [-1,1]^d. However, in this
    section we are going to discuss multidimensional data, so *t*[i] can be defined
    as a *d*-dimensional numerical vector in the range of -1 to 1 as *t*[i] ∈ [-1,
    1]^d. In addition, we are going to perturb *t*[i] with the Laplace mechanism,
    so we’ll let *t*[i]^* denote the perturbed data record of *t*[i] after applying
    LDP.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们假设LDP机制中的每个参与者是 *u*[i]，并且每个 *u*[i] 的数据记录是 *t*[i]。这个 *t*[i] 通常是一个在-1到1范围内的单维数值（如前一章所述），我们可以用数学表示为
    *t*[i] ∈ [-1,1]^d。然而，在本节中，我们将讨论多维数据，因此 *t*[i] 可以定义为在-1到1范围内的 *d*-维数值向量，即 *t*[i]
    ∈ [-1, 1]^d。此外，我们将使用Laplace机制扰动 *t*[i]，因此我们将让 *t*[i]^* 表示应用LDP后 *t*[i] 的扰动数据记录。
- en: With those basics covered, we can now dig into the Laplace mechanism for LDP.
    Let’s say we have a participant *u*[i], and their data record is *t*[i] ∈ [-1,1]^d.
    Remember, this data record is now multidimensional. In order to satisfy LDP, we
    need to perturb this data record, and in this case we are going to use noise generated
    from a Laplace distribution. We’ll define a randomized function *t*[i]^* = *t*[i]
    + *Lap*(2 * *d*/ϵ) to generate a perturbed value*t*[i]^*, *where Lap(λ) is* a
    random variable from a Laplace distribution of scale *λ*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了这些基础知识之后，我们现在可以深入探讨LDP的Laplace机制。假设我们有一个参与者 *u*[i]，他们的数据记录是 *t*[i] ∈ [-1,1]^d。记住，这个数据记录现在是多维的。为了满足LDP，我们需要扰动这个数据记录，在这种情况下，我们将使用来自Laplace分布的噪声。我们将定义一个随机函数
    *t*[i]^* = *t*[i] + *Lap*(2 * *d*/ϵ) 来生成一个扰动值 *t*[i]^*，其中 *Lap(λ)* 是来自尺度为 *λ*
    的Laplace分布的随机变量。
- en: Tip Are you wondering why we are always looking at the Laplace mechanism when
    it comes to differential privacy? The reason is that Gaussian perturbations are
    not always satisfactory, since they cannot be used to achieve pure DP (ϵ-DP),
    which requires heavier-tailed distributions. Thus, the most popular distribution
    is the Laplace mechanism, whose tails are “just right” for achieving pure DP.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：当你提到差分隐私时，为什么我们总是关注Laplace机制？原因在于高斯扰动并不总是令人满意，因为它们不能用来实现纯DP（ϵ-DP），这需要重尾分布。因此，最流行的分布是Laplace机制，其尾部“恰到好处”，可以用来实现纯DP。
- en: The following listing shows the Python code that implements the Laplace mechanism
    for LDP.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了实现LDP的Laplace机制的Python代码。
- en: Listing 5.1 The Laplace mechanism for LDP
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1 LDP的Laplace机制
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Laplace mechanism provides the basic functionality to perturb multidimensional
    continuous numerical data for LDP. However, as studied by Wang et al. [1], when
    using a smaller privacy budget ϵ (i.e., ϵ < 2.3), the Laplace mechanism tends
    to result in more variance in the perturbed data, thus providing bad utility performance.
    In the next section we’ll look into the Duchi’s mechanism, which performs better
    when using a smaller privacy budget.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Laplace机制为LDP扰动多维连续数值数据提供了基本功能。然而，如Wang等人[1]所研究，当使用较小的隐私预算 ϵ（即，ϵ < 2.3）时，Laplace机制往往会导致扰动数据中更多的方差，从而提供较差的效用性能。在下一节中，我们将探讨Duchi机制，它在使用较小的隐私预算时表现更好。
- en: 5.2.2 Duchi’s mechanism for LDP
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 LDP的Duchi机制
- en: While the Laplace mechanism is one way to generate noise for the perturbation
    that is used with LDP, Duchi et al. [2] introduced another approach, known as
    Duchi’s mechanism, to perturb multidimensional continuous numerical data for LDP.
    The concept is similar to the Laplace mechanism, but the data perturbation is
    handled differently.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Laplace机制是生成用于LDP扰动的噪声的一种方法，但Duchi等人[2]介绍了一种另一种方法，称为Duchi机制，用于扰动多维连续数值数据以实现LDP。这个概念与Laplace机制类似，但数据处理方式不同。
- en: 'Let’s first look at how Duchi’s mechanism handles perturbation for one-dimensional
    numerical data. Given a participant *u*[i], their one-dimensional data record
    *t*[i] ∈ [-1,1], and a privacy budget ϵ, Duchi’s mechanism performs as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看Duchi机制是如何处理一维数值数据的扰动的。给定一个参与者 *u*[i]，他们的一个维数据记录 *t*[i] ∈ [-1,1]，以及隐私预算
    ϵ，Duchi机制执行如下：
- en: A Bernoulli variable *u* is sampled such that ![05_EQ_01](../../OEBPS/Images/05_EQ_01.png).
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 ![05_EQ_01](../../OEBPS/Images/05_EQ_01.png) 中采样一个伯努利变量 *u*。
- en: Then, if *u* = 1, the perturbed data will be ![05_EQ_02](../../OEBPS/Images/05_EQ_02.png);
    otherwise, it is ![05_EQ_02](../../OEBPS/Images/05_EQ_02.png).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，如果 *u* = 1，扰动的数据将是 ![05_EQ_02](../../OEBPS/Images/05_EQ_02.png)；否则，它是 ![05_EQ_02](../../OEBPS/Images/05_EQ_02.png)。
- en: The perturbed data ![05_EQ_03](../../OEBPS/Images/05_EQ_03.png)will be the output
    of Duchi’s mechanism.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扰动的数据 ![05_EQ_03](../../OEBPS/Images/05_EQ_03.png)将是Duchi机制的输出。
- en: The Python implementation of this algorithm is shown in the following listing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的Python实现如下所示。
- en: Listing 5.2 Duchi’s mechanism for one-dimensional data
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.2 Duchi的一维数据机制
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What is the Bernoulli distribution?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 伯努利分布是什么？
- en: In probability theory, the Bernoulli distribution is one of the simplest distributions
    to understand, and it is often used as a building block for more complex distributions.
    At a high level, the Bernoulli distribution is a discrete probability distribution
    with only two possible values for the random variable, where it takes the value
    1 with probability p and the value 0 with probability *q* = 1 - *p*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，伯努利分布是理解起来最简单的分布之一，它经常被用作更复杂分布的构建块。从高层次来看，伯努利分布是一个只有随机变量两个可能值的离散概率分布，其中它以概率
    p 取值为 1，以概率 *q* = 1 - *p* 取值为 0。
- en: In simpler terms, if an experiment has only two possible outcomes, “success”
    and “failure,” and if p is the probability of success, then
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，如果一个实验只有两种可能的结果，“成功”和“失败”，并且如果 p 是成功的概率，那么
- en: '![CH05_F01_zhuang-ch5-eqs-8x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-8x.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-8x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-8x.png)'
- en: In this context, we usually consider “success” to be the outcome that we want
    to keep track of.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们通常将“成功”视为我们想要跟踪的结果。
- en: 'Now that you know how Duchi’s mechanism works for one-dimensional data, let’s
    extend it for multidimensional data. Given a *d*-dimensional data record *t*[i]
    ∈ [-1,1]^d and the privacy budget ϵ, Duchi’s mechanism for multidimensional data
    performs as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了Duchi机制在一维数据上的工作原理，让我们将其扩展到多维数据。给定一个 *d*-维数据记录 *t*[i] ∈ [-1,1]^d 和隐私预算
    ϵ，Duchi机制的多维数据执行如下：
- en: Generate a random *d*-dimensional data record *v* ∈ [-1,1]^d by sampling each
    *v*[*A*[j]] independently from the following distribution,
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过独立地从以下分布中采样每个 *v*[*A*[j]]，生成一个随机的 *d*-维数据记录 *v* ∈ [-1,1]^d。
- en: '![CH05_F01_zhuang-ch5-eqs-9x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-9x.png)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-9x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-9x.png)'
- en: where *v*[*A*[j]] is the *j*th value of *v*.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*v*[*A*[j]] 表示 *v* 的第 *j* 个值。'
- en: Define *T*^+ (resp. *T*^-) as the set of all data records *t* ^* ∈ {-*B*, *B*}^d
    such that *t*^* ⋅ *v* ≥ 0 (resp. *t*^* ⋅ *v* ≤ 0), where we have
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 *T*^+（相应地，*T*^-）为所有数据记录 *t* ^* ∈ {-*B*, *B*}^d 的集合，使得 *t*^* ⋅ *v* ≥ 0（相应地，*t*^*
    ⋅ *v* ≤ 0），其中我们得到
- en: '![CH05_F01_zhuang-ch5-eqs-10x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-10x.png)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-10x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-10x.png)'
- en: '![CH05_F01_zhuang-ch5-eqs-11x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-11x.png)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-11x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-11x.png)'
- en: A Bernoulli variable *u* is sampled such that ![CH05_F01_zhuang-ch5-eqs-12x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-12x.png).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 ![CH05_F01_zhuang-ch5-eqs-12x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-12x.png)
    中采样一个伯努利变量 *u*。
- en: Finally, if *u* = 1, output a data record uniformly selected from *T*^+; otherwise,
    output a data record uniformly selected from *T*^-.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，如果 *u* = 1，输出从 *T*^+ 中均匀选择的数据记录；否则，输出从 *T*^- 中均匀选择的数据记录。
- en: The following listing shows the Python implementation of this algorithm. If
    you carefully follow the steps we just discussed, you will quickly understand
    what we are doing in the code.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了该算法的Python实现。如果你仔细遵循我们刚才讨论的步骤，你会很快理解代码中的操作。
- en: Listing 5.3 Duchi’s mechanism for multidimensional data
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3 Duchi的多维数据机制
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Duchi’s mechanism performs well with a smaller privacy budget (i.e., ϵ < 2.3).
    However, it performs worse than the Laplace mechanism in terms of the utility
    when you are using a larger privacy budget. Could there be a more general algorithm
    that works well regardless of whether the privacy budget is small or large? We’ll
    look at the Piecewise mechanism in the next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用较小的隐私预算（即，ϵ < 2.3）时，Duchi机制表现良好。然而，当使用较大的隐私预算时，它在效用方面不如拉普拉斯机制。是否可能存在一个更通用的算法，无论隐私预算是较小还是较大，都能表现良好？我们将在下一节中查看分段机制。
- en: 5.2.3 The Piecewise mechanism for LDP
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 LDP的分段机制
- en: So far in this chapter you’ve learned about mechanisms that can be used for
    LDP. A third mechanism, called the Piecewise mechanism [1], had been proposed
    to deal with multidimensional continuous numerical data when using LDP, and it
    can overcome the disadvantages of the Laplace and Duchi’s mechanisms. The idea
    is to perturb multidimensional numerical values with an asymptotic optimal error
    bound. Hence, it requires only one bit for each individual to be reported to the
    data aggregator.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中你已经学习了可用于LDP的机制。第三种机制，称为分段机制[1]，已被提出用于处理在使用LDP时多维连续数值数据，并且它可以克服拉普拉斯和Duchi机制的缺点。其思路是对多维数值进行扰动，并具有渐近最优误差界限。因此，每个个体只需报告一个比特给数据聚合器。
- en: 'First, let’s look at the Piecewise mechanism for one-dimensional data. Given
    the participant *u*[i]’s one-dimensional data record *t*[i] ∈ [-1,1] and privacy
    budget ϵ, the Piecewise mechanism performs as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看一维数据的分段机制。给定参与者 *u*[i] 的一维数据记录 *t*[i] ∈ [-1,1] 和隐私预算 ϵ，分段机制执行如下：
- en: A value *x* is selected in the range of 0 to 1, uniformly at random.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 0 到 1 的范围内选择一个值 *x*，均匀随机。
- en: If ![CH05_F01_zhuang-ch5-eqs-13x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-13x.png),
    we sample t[i]^* uniformly at random from [l(t[i]), r(t[i])]; otherwise, we sample
    t[i]^* uniformly at random from [-C, l(t[i]) **∪** r(t[i]), C], where
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ![CH05_F01_zhuang-ch5-eqs-13x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-13x.png)，我们从
    [l(t[i]), r(t[i])] 中均匀随机抽取 t[i]^*；否则，我们从 [-C, l(t[i]) **∪** r(t[i]), C] 中均匀随机抽取
    t[i]^*，其中
- en: '![CH05_F01_zhuang-ch5-eqs-14x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-14x.png)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-14x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-14x.png)'
- en: '![CH05_F01_zhuang-ch5-eqs-15x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-15x.png)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-15x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-15x.png)'
- en: and
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 和
- en: '![CH05_F01_zhuang-ch5-eqs-16x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-16x.png)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-16x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-16x.png)'
- en: Perturbed data *t*[i]^* ∈{-*C*,*C*} will be the output of the Piecewise mechanism.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扰动数据 *t*[i]^* ∈{-*C*,*C*} 将是分段机制的输出。
- en: 'The Piecewise mechanism consists of three pieces: the center piece, the right
    piece *r*(), and the left piece *l*(). The centerpiece is calculated as *t*[i]*
    ∈ [*l*(*t*[i]), *r*(*t*[i])], the rightmost piece as *t*[i]* ∈ [*r*(*t*[i]),*C*],
    and the leftmost piece as *t*[i]* ∈ [-*C*, *l*(*t*[i])]. You can see a Python
    implementation of this algorithm in the following listing.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分段机制由三部分组成：中心部分、右侧部分 *r*() 和左侧部分 *l*()。中心部分计算为 *t*[i]* ∈ [*l*(*t*[i]), *r*(*t*[i])]，最右侧部分为
    *t*[i]* ∈ [*r*(*t*[i]),*C*]，最左侧部分为 *t*[i]* ∈ [-*C*, *l*(*t*[i])]。你可以在下面的列表中看到一个该算法的Python实现。
- en: Listing 5.4 Piecewise mechanism for one-dimensional data
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 一维数据的分段机制
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Providing the size parameter in uniform() would result in an ndarray.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 uniform() 中提供大小参数将产生一个 ndarray。
- en: 'How can we deal with multidimensional data? We can simply extend the Piecewise
    mechanism from its one-dimensional version to work with multidimensional data.
    Given a *d*-dimensional data record t[i] ∈ [-1,1]^d and a privacy budget ϵ, the
    Piecewise mechanism for multidimensional data performs as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何处理多维数据？我们可以简单地将分段机制从其一维版本扩展到处理多维数据。给定一个 *d*-维数据记录 t[i] ∈ [-1,1]^d 和隐私预算
    ϵ，多维数据的分段机制执行如下：
- en: Sample *k* values uniformly without replacement from {1, 2, ..., *d*}, where
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 {1, 2, ..., *d*} 中均匀无放回地抽取 *k* 个值，其中
- en: '![CH05_F01_zhuang-ch5-eqs-17x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-17x.png)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-17x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-17x.png)'
- en: For each sampled value *j*, feed *t*[i][*A*[j]] and ![CH05_F01_zhuang-ch5-eqs-18x-missing](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-18x-missing.png) as
    the input to the one-dimensional version of the Piecewise mechanism and obtain
    a noisy value *x*[i,j].
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个采样的值 *j*，将 *t*[i][*A*[j]] 和 ![CH05_F01_zhuang-ch5-eqs-18x-missing](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-18x-missing.png)
    作为分段机制的一维版本的输入，并得到一个噪声值 *x*[i,j]。
- en: Output t[i]^*, where ![CH05_F01_zhuang-ch5-eqs-18x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-18x.png).
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出t[i]^*，其中![CH05_F01_zhuang-ch5-eqs-18x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-18x.png)。
- en: The following listing shows a Python implementation of this algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了该算法的Python实现。
- en: Listing 5.5 Piecewise mechanism for multidimensional data
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.5 多维数据的分段机制
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have studied three advanced LDP mechanisms for multidimensional
    numerical data, let’s look at a case study showing how LDP can be implemented
    with real-world datasets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了多维数值数据的三个高级LDP机制，让我们来看一个案例研究，展示如何使用真实世界的数据集实现LDP。
- en: 5.3 A case study implementing LDP naive Bayes classification
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 实现LDP朴素贝叶斯分类的案例研究
- en: In the previous chapter we introduced a set of mechanisms that can be used to
    implement LDP protocols. In this section we’ll use the LDP naive Bayes classification
    design as a case study to walk through the process of designing an LDP ML algorithm.
    The content in this section has been partially published in one of our research
    papers [3]. The implementation and the complete code for this case study can be
    found at [https://github.com/nogrady/PPML/tree/main/Ch5](https://github.com/nogrady/PPML/tree/main/Ch5).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了一系列可以用来实现LDP协议的机制。在本节中，我们将以LDP朴素贝叶斯分类设计作为一个案例研究，来讲解设计LDP机器学习算法的过程。本节内容部分已发表在我们的研究论文[3]中。本案例研究的实现和完整代码可以在[https://github.com/nogrady/PPML/tree/main/Ch5](https://github.com/nogrady/PPML/tree/main/Ch5)找到。
- en: NOTE This section will walk you through the mathematical formulations and empirical
    evaluations for this case study so that you can learn how to develop an LDP application
    from scratch. If you do not need to know these implementation details right now,
    you can skip ahead to the next chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节将向您介绍本案例研究的数学公式和实证评估，以便您可以从零开始学习如何开发LDP应用。如果您目前不需要了解这些实现细节，您可以跳到下一章。
- en: 5.3.1 Using naive Bayes with ML classification
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 使用朴素贝叶斯进行机器学习分类
- en: In section 3.2.1 you learned how differentially private naive Bayes classification
    works and its mathematical formulations. In this section we will broaden the discussion
    and explore how we can use naive Bayes with LDP. As discussed in the previous
    chapter, LDP involves individuals sending their data to the data aggregator after
    privatizing the data by perturbation. These techniques provide plausible deniability
    for individuals. The data aggregator then collects all the perturbed values and
    estimates statistics such as the frequency of each value in the population.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在3.2.1节中，您学习了差分隐私朴素贝叶斯分类的工作原理及其数学公式。在本节中，我们将扩展讨论，探讨如何使用LDP结合朴素贝叶斯。正如前一章所述，LDP涉及个人在通过扰动对数据进行隐私化后，将数据发送给数据聚合器。这些技术为个人提供了合理的否认可能性。数据聚合器随后收集所有扰动值并估计诸如人口中每个值的频率等统计数据。
- en: To guarantee the privacy of the individuals who provide training data in a classification
    task, LDP techniques can be used at the data collection stage. In this chapter
    we’ll apply LDP techniques to naive Bayes classifiers, which are a set of simple
    probabilistic classifiers based on Bayes’ theorem. To quickly recap, naive Bayes
    classifiers assume independence between every pair of features. Most importantly,
    these classifiers are highly scalable and particularly suitable when the number
    of features is high or when the training data is small. Naive Bayes can often
    perform better than or close to more sophisticated classification methods, despite
    its simplicity.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证在分类任务中提供训练数据的个人的隐私，可以在数据收集阶段使用LDP技术。在本章中，我们将应用LDP技术到朴素贝叶斯分类器上，这些分类器是基于贝叶斯定理的一组简单概率分类器。为了快速回顾，朴素贝叶斯分类器假设每对特征之间相互独立。最重要的是，这些分类器具有高度的可扩展性，特别适合特征数量多或训练数据量小的情况。尽管朴素贝叶斯分类器很简单，但它通常可以比更复杂的分类方法表现得更好或接近。
- en: Let’s get into the details now. Given a new instance (a known class value),
    naive Bayes computes the conditional probability of each class label and then
    assigns the class label with maximum likelihood to the given instance. The idea
    is that, using Bayes’ theorem and the assumption of independence of features,
    each conditional probability can be decomposed as the multiplication of several
    probabilities. We need to compute each of these probabilities using training data
    to achieve naive Bayes classification. Since the training data must be collected
    from individuals by preserving privacy, we can utilize LDP frequency and statistical
    estimation methods to collect perturbed data from individuals and then estimate
    conditional probabilities with the naive Bayes classification.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来详细探讨一下。给定一个新实例（一个已知的类别值），朴素贝叶斯计算每个类别标签的条件概率，然后将最大似然类标签分配给给定的实例。其想法是，使用贝叶斯定理和特征独立性的假设，每个条件概率可以分解为几个概率的乘积。我们需要使用训练数据计算这些概率，以实现朴素贝叶斯分类。由于训练数据必须通过保护隐私从个人那里收集，我们可以利用LDP频率和统计估计方法从个人那里收集扰动数据，然后使用朴素贝叶斯分类估计条件概率。
- en: In this case study, we’ll first look into how we can work with discrete features
    using an LDP naive Bayes classifier by preserving the relationships between the
    class labels and the features. Second, we’ll walk through the case of continuous
    features. We’ll discuss how we can discretize data and apply Gaussian naive Bayes
    after adding Laplace noise to the data to satisfy LDP. We will also show you how
    to work with continuous data perturbation methods. Finally, we’ll explore and
    implement these techniques with a set of experimental scenarios and real datasets,
    showing you how LDP guarantees are satisfied while maintaining the accuracy of
    the classifier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们首先将探讨如何使用LDP朴素贝叶斯分类器处理离散特征，同时保持类别标签和特征之间的关系。其次，我们将讨论连续特征的情况。我们将讨论如何对数据进行离散化，并在向数据添加拉普拉斯噪声以满足LDP后应用高斯朴素贝叶斯。我们还将向您展示如何处理连续数据的扰动方法。最后，我们将通过一系列实验场景和真实数据集来探索和实现这些技术，展示如何在保持分类器准确性的同时，LDP保证是如何得到满足的。
- en: Discrete versus continuous features
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 离散特征与连续特征
- en: '*Discrete variables* are numeric variables with a countable number of values
    between any two values. A discrete variable is always numeric. For example, the
    number of defective parts or the number of missed payments can be treated as discrete
    values.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*离散变量*是两个值之间具有可数个值的数值变量。离散变量始终是数值的。例如，缺陷部件的数量或错过付款的数量可以被视为离散值。'
- en: In contrast, *continuous variables* are numeric variables with infinite values
    between any two values. A continuous variable can be numeric or it can be a date/time,
    such as the length of a part or the date/time a payment is received.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*连续变量*是两个值之间具有无限个值的数值变量。连续变量可以是数值的，也可以是日期/时间，例如部件的长度或收到付款的日期/时间。
- en: Nevertheless, we sometimes treat discrete data as continuous and continuous
    data as discrete, depending on the application.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们有时会根据应用的不同，将离散数据视为连续数据，将连续数据视为离散数据。
- en: 5.3.2 Using LDP naive Bayes with discrete features
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 使用具有离散特征的LDP朴素贝叶斯
- en: Before digging into the theory, let’s first look at an example. An independent
    analyst wants to train an ML classifier to predict “how likely a person is to
    miss a mortgage payment.” The idea is to use data from different mortgage and
    financial companies, train a model, and use this model to predict the behavior
    of future customers. The problem is that none of the financial companies want
    to participate because they do not want to share their customers’ private or sensitive
    information. The best option for them is to share a perturbed version of their
    data so that their customers’ privacy is preserved. But how can we perturb the
    data so that it can be used to train a naive Bayes classifier while protecting
    privacy? That’s what we are going to find out.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入理论之前，让我们先来看一个例子。一个独立分析师想要训练一个机器学习分类器来预测“一个人错过抵押贷款支付的可能性”。想法是使用来自不同抵押贷款和金融公司的数据，训练一个模型，并使用这个模型来预测未来客户的行为了。问题是，没有任何一家金融公司愿意参与，因为他们不想分享他们客户的私人或敏感信息。对他们来说，最好的选择是分享他们数据的扰动版本，这样就能保护客户的隐私。但如何扰动数据，使其可以用于训练朴素贝叶斯分类器同时保护隐私呢？这正是我们要弄清楚的问题。
- en: We discussed how naive Bayes classification works in section 3.2.1, so we’ll
    only recap the essential points here. Refer back to chapter 3 for more details.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第3.2.1节中讨论了朴素贝叶斯分类的工作原理，所以这里我们只回顾一下关键点。更多细节请参阅第3章。
- en: 'In probability theory, Bayes’ theorem describes the probability of an event,
    based on prior knowledge of conditions that might be related to the event. It
    is stated as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，贝叶斯定理描述了基于与事件可能相关的前置知识的事件概率。它表述如下：
- en: '![CH05_F01_zhuang-ch5-eqs-19x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-19x.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-19x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-19x.png)'
- en: The naive Bayes classification technique uses Bayes’ theorem and the assumption
    of independence between every pair of features. Suppose the instance to be classified
    is the *n*-dimensional vector *X* = {*x*[1], *x*[2], ..., *x*[n]}, the names of
    the features are *F*[1], *F*[2], ..., *F*[n], and the possible classes that can
    be assigned to the instance are *C* = {*C*[1], *C*[2], ..., *C*[k]}. A naive Bayes
    classifier assigns the instance *X* to the class *C*[s] if and only if *P*(*C*[s]|*X*)
    > *P*(*C*[j]|*X*) for 1 ≤ *j* ≤ *k* and *j* ≠ *s*. Hence, the classifier needs
    to compute *P*(*C*[j]|*X*) for all classes and compare these probabilities. Using
    Bayes’ theorem, the probability *P*(*C*[j]|*X*) can be calculated as
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯定理和每对特征之间独立性的假设，朴素贝叶斯分类技术使用贝叶斯定理。假设要分类的实例是 *n*-维向量 *X* = {*x*[1], *x*[2],
    ..., *x*[n]}，特征的名称是 *F*[1], *F*[2], ..., *F*[n]，可以分配给实例的可能类别是 *C* = {*C*[1], *C*[2],
    ..., *C*[k]}。朴素贝叶斯分类器将实例 *X* 分配到类别 *C*[s]，当且仅当 *P*(*C*[s]|*X*) > *P*(*C*[j]|*X*)
    对于 1 ≤ *j* ≤ *k* 且 *j* ≠ *s*。因此，分类器需要计算所有类别的 *P*(*C*[j]|*X*) 并比较这些概率。使用贝叶斯定理，概率
    *P*(*C*[j]|*X*) 可以计算如下
- en: '![CH05_F01_zhuang-ch5-eqs-20x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-20x.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-20x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-20x.png)'
- en: Since *P*(*X*) is same for all classes, it is sufficient to find the class with
    the maximum *P*(*X*|*C*[j]) ∙ *P*(*C*[j]).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *P*(*X*) 对于所有类别都是相同的，因此找到具有最大 *P*(*X*|*C*[j]) ∙ *P*(*C*[j]) 的类别就足够了。
- en: Let’s first consider the case where all the features are numerical and discrete.
    Suppose there are *m* different records or individuals (among these financial
    companies) that can be used to train this classifier. Table 5.1 shows an extract
    from a mortgage payment dataset that we discussed in chapter 3.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑所有特征都是数值和离散的情况。假设有 *m* 个不同的记录或个人（在这些金融公司中）可以用来训练这个分类器。表5.1显示了我们在第3章中讨论的抵押贷款支付数据集的一个摘录。
- en: Table 5.1 An extract from a dataset of mortgage payments. Age, income, and gender
    are the independent variables, whereas missed payment represents the dependent
    variable for the prediction task.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 来自抵押贷款支付数据集的一个摘录。年龄、收入和性别是独立变量，而错过付款代表预测任务中的因变量。
- en: '| Number | Age | Income | Gender | Missed payment (yes or no) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 编号 | 年龄 | 收入 | 性别 | 是否错过付款（是或否） |'
- en: '| 1 | Young | Low | Male | Yes |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 年轻 | 低 | 男 | 是 |'
- en: '| 2 | Young | High | Female | Yes |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 年轻 | 高 | 女 | 是 |'
- en: '| 3 | Medium | High | Male | No |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 中等 | 高 | 男 | 否 |'
- en: '| 4 | Old | Medium | Male | No |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 老年 | 中等 | 男 | 否 |'
- en: '| 5 | Old | High | Male | No |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 老年 | 高 | 男 | 否 |'
- en: '| 6 | Old | Low | Female | Yes |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 老年 | 低 | 女 | 是 |'
- en: '| 7 | Medium | Low | Female | No |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 中等 | 低 | 女 | 否 |'
- en: '| 8 | Medium | Medium | Male | Yes |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 中等 | 中等 | 男 | 是 |'
- en: '| 9 | Young | Low | Male | No |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 年轻 | 低 | 男 | 否 |'
- en: '| 10 | Old | High | Female | No |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 老年 | 高 | 女 | 否 |'
- en: Our objective is to use this data to train a classifier that can be used to
    predict future customers and to determine whether a particular customer is likely
    to miss a mortgage payment or not. Thus, in this case, the classification task
    is the prediction of the customer’s behavior (whether they will miss a mortgage
    payment or not), which makes it a binary classification—we only have two possible
    classes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是使用这些数据来训练一个分类器，该分类器可以用来预测未来的客户，并确定特定客户是否有可能错过抵押贷款付款。因此，在这种情况下，分类任务是预测客户的行为（他们是否会错过抵押贷款付款），这使得它成为二元分类——我们只有两个可能的类别。
- en: 'Much as we did in chapter 3, let’s define these two classes as *C*[1] and *C*[2]
    where *C*[1] represents missing a previous payment and *C*[2] represents otherwise.
    Based on the data reported in table 5.1, the probabilities of these classes can
    be computed as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在第3章中所做的那样，让我们将这两个类别定义为 *C*[1] 和 *C*[2]，其中 *C*[1] 表示错过前一次付款，而 *C*[2] 表示其他情况。根据表5.1中报告的数据，这些类别的概率可以按以下方式计算：
- en: '![05_EQ_04](../../OEBPS/Images/05_EQ_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![05_EQ_04](../../OEBPS/Images/05_EQ_04.png)'
- en: Similarly, we can compute the conditional probabilities. Table 5.2 summarizes
    the conditional probabilities we already calculated in chapter 3 for the age feature.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算条件概率。表5.2总结了我们在第三章中计算的年龄特征的已计算条件概率。
- en: Table 5.2 A summary of conditional probabilities computed for the age feature
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 计算的年龄特征条件概率总结
- en: '| Conditional probability | Result |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 条件概率 | 结果 |'
- en: '| P(Age = Young &#124; C[1])P(Age = Young &#124; C[2]) | 2/41/6 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| P(Age = Young | C[1])P(Age = Young | C[2]) | 2/41/6 |'
- en: '| P(Age = Medium &#124; C[1])P(Age = Medium &#124; C[2]) | 1/42/6 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| P(Age = Medium | C[1])P(Age = Medium | C[2]) | 1/42/6 |'
- en: '| P(Age = Old &#124; C[1])P(Age = Old &#124; C[2]) | 1/43/6 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| P(Age = Old | C[1])P(Age = Old | C[2]) | 1/43/6 |'
- en: Once we have all the conditional probabilities, we can predict whether, for
    example, a young female with medium income will miss a payment or not. To do that,
    first we need to set *X* as *X* = (*Age* = *Young*, *Income* = *Medium*, *Gender*
    = *Female*). Chapter 3 presents the remaining steps and computations for using
    the naive Bayes classifier, and you can refer back if necessary.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦我们有了所有条件概率，我们就可以预测例如一个收入中等的年轻女性是否会错过付款。为此，首先我们需要将 *X* 设置为 *X* = (*Age* =
    *Young*, *Income* = *Medium*, *Gender* = *Female*)。第三章介绍了使用朴素贝叶斯分类器的剩余步骤和计算，如果需要可以参考。 '
- en: Based on the results of these computations, the naive Bayes classifier will
    assign *C*[2] for the instance *X*. In other words, a young female with medium
    income will not miss a payment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些计算的结果，朴素贝叶斯分类器将为实例 *X* 分配 *C*[2]。换句话说，一个收入中等的年轻女性不会错过付款。
- en: '![CH05_F01_zhuang-ch5-eqs-23x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-23x.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-23x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-23x.png)'
- en: and
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![CH05_F01_zhuang-ch5-eqs-24x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-24x.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-24x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-24x.png)'
- en: With this basic understanding of how the naive Bayes classifier works, let’s
    see how we can use the LDP frequency estimation methods discussed earlier to compute
    the necessary probabilities for a naive Bayes classifier. Remember, in LDP, the
    data aggregator is the one that computes the class probabilities *P*(*C*[j]) for
    all classes in *C* = {*C*[1], *C*[2], ..., *C*[k]} and conditional probabilities
    *P*(*F*[i] = *x*[i]|*C*[j]) for all possible *x*[i] values.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了朴素贝叶斯分类器如何工作的基本原理之后，让我们看看如何使用之前讨论过的LDP频率估计方法来计算朴素贝叶斯分类器所需的概率。记住，在LDP中，数据聚合器负责计算类别概率
    *P*(*C*[j]) 对于类别 *C* = {*C*[1], *C*[2], ..., *C*[k]} 中的所有类别，以及对于所有可能的 *x*[i] 值的条件概率
    *P*(*F*[i] = *x*[i]|*C*[j])。
- en: Suppose an individual’s data, Alice’s data, is (*a*[1], *a*[2], ..., *a*[n])
    and her class label is *C*[v]. In order to satisfy LDP, she needs to prepare her
    input and perturb it. Let’s look at the details of how Alice’s data can be prepared
    and perturbed and how the class probabilities and the conditional probabilities
    can be estimated by the data aggregator.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某个个体的数据，爱丽丝的数据，是 (*a*[1], *a*[2], ..., *a*[n])，并且她的类别标签是 *C*[v]。为了满足LDP，她需要准备她的输入并对其进行扰动。让我们看看爱丽丝的数据如何准备和扰动，以及数据聚合器如何估计类别概率和条件概率。
- en: Computing class probabilities
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 计算类别概率
- en: For the computation of class probabilities, Alice’s input becomes *v* ∈ {1,
    2, ..., *k*} since her class label is *C*[v]. Then Alice encodes and perturbs
    her value *v* and reports to the data aggregator. Any LDP frequency estimation
    method we discussed earlier can be used. Similarly, other individuals report their
    perturbed class labels to the data aggregator.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算类别概率，由于爱丽丝的类别标签是 *C*[v]，她的输入变为 *v* ∈ {1, 2, ..., *k*}。然后爱丽丝对她的值 *v* 进行编码和扰动，并向数据聚合器报告。我们可以使用之前讨论过的任何LDP频率估计方法。同样，其他个人也会向数据聚合器报告他们扰动的类别标签。
- en: The data aggregator collects all the perturbed data and estimates the frequency
    of each value *j* ∈ {1, 2, ..., *k*} as *E*[j]. Now the probability *P*(*C*[j])
    is estimated as
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据聚合器收集所有扰动的数据，并估计每个值 *j* ∈ {1, 2, ..., *k*} 的频率为 *E*[j]。现在，概率 *P*(*C*[j]) 被估计为
- en: '![CH05_F01_zhuang-ch5-eqs-25x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-25x.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_zhuang-ch5-eqs-25x](../../OEBPS/Images/CH05_F01_zhuang-ch5-eqs-25x.png)'
- en: 'Let’s consider an example to make it clearer. There are only two options for
    the class label in the example dataset in table 5.1: missing a payment or not.
    Let’s say Alice has a missing payment. Then Alice’s input *v* becomes 1 and she
    reports it to the data aggregator. Similarly, if she does not have a missing payment,
    she would report 2 as her input to the data aggregator. Figure 5.2 shows three
    people submitting their perturbed values to the data aggregator. The data aggregator
    then estimates the frequency of each value and calculates the class probabilities.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来使问题更清晰。在表5.1的示例数据集中，类别标签只有两种选择：缺失付款或未缺失付款。假设Alice有缺失付款。那么Alice的输入*v*变为1，并报告给数据聚合器。同样，如果她没有缺失付款，她将报告2作为她的输入给数据聚合器。图5.2显示了三个人向数据聚合器提交他们的扰动值。然后数据聚合器估计每个值的频率并计算类别概率。
- en: '![CH05_F02_Zhuang](../../OEBPS/Images/CH05_F02_Zhuang.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_Zhuang](../../OEBPS/Images/CH05_F02_Zhuang.png)'
- en: Figure 5.2 An example showing how to compute class probabilities
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 展示如何计算类别概率的示例
- en: Computing conditional probabilities
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 计算条件概率
- en: In order to estimate the conditional probabilities *P*(*F*[i] = *x*[i]|*C*[j]),
    it is not sufficient to report the feature values directly. To compute these probabilities,
    the relationship between class labels and features must be preserved, which means
    individuals need to prepare their inputs using a combination of feature values
    and class labels.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计条件概率*P*(*F*[i] = *x*[i]|*C*[j])，直接报告特征值是不够的。为了计算这些概率，必须保留类别标签和特征之间的关系，这意味着个体需要使用特征值和类别标签的组合来准备他们的输入。
- en: We’ll let the total number of possible values for *F*[i] be *n*[i]. If Alice’s
    value in *i*th dimension is *a*[i] ∈ {1, 2, ..., *n*[i]} and her class label value
    is *v* ∈ {1, 2, ..., *k*}, then Alice’s input for feature *F*[i] becomes *v*[i]
    = (*a*[i] - 1)∙*k* + *v*. Therefore, each individual calculates her input for
    the *i*th feature in the range of [1, *k*∙*n*[i]].
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*F*[i]的可能值的总数设为*n*[i]。如果Alice在第*i*个维度的值是*a*[i] ∈ {1, 2, ..., *n*[i]}，并且她的类别标签值是*v*
    ∈ {1, 2, ..., *k*}，那么Alice对特征*F*[i]的输入变为*v*[i] = (*a*[i] - 1)∙*k* + *v*。因此，每个个体都会在她的第*i*个特征的[1,
    *k*∙*n*[i]]范围内计算她的输入。
- en: It’s a bit hard to follow, isn’t it? But don’t worry. Let’s look at this through
    an example. For instance, suppose the age values in table 5.1 are enumerated as
    (Young = 1), (Medium = 2), (Old = 3). For this age feature, an individual’s input
    can be a value from 1 to 6, as shown in table 5.3, where 1 represents that the
    age is young and there is a missing payment, and 6 represents that the age is
    old and there is no missing payment.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点难以理解，不是吗？但别担心。让我们通过一个例子来看看。例如，假设表5.1中的年龄值被枚举为（年轻 = 1），（中等 = 2），（老年 = 3）。对于这个年龄特征，个人的输入可以是1到6之间的任何值，如表5.3所示，其中1表示年龄年轻且存在缺失付款，而6表示年龄老年且没有缺失付款。
- en: Table 5.3 Preparing input as a combination of feature values and class labels
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.3 准备输入作为特征值和类别标签的组合
- en: '| Relationship between class label and feature | Enumerated value |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 类别标签与特征之间的关系 | 枚举值 |'
- en: '| (Age = Young &#124; C[1]) | 1 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| (年龄 = 年轻 &#124; C[1]) | 1 |'
- en: '| (Age = Young &#124; C[2]) | 2 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| (年龄 = 年轻 &#124; C[2]) | 2 |'
- en: '| (Age = Medium &#124; C[1]) | 3 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| (年龄 = 中等 &#124; C[1]) | 3 |'
- en: '| (Age = Medium &#124; C[2]) | 4 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| (年龄 = 中等 &#124; C[2]) | 4 |'
- en: '| (Age = Old &#124; C[1]) | 5 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| (年龄 = 老年 &#124; C[1]) | 5 |'
- en: '| (Age = Old &#124; C[2]) | 6 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| (年龄 = 老年 &#124; C[2]) | 6 |'
- en: You may have noticed that there is one input value for each line in table 5.2\.
    Similarly, the number of possible inputs for income is 6, and the number of possible
    inputs for gender is 4\. After determining her input in the *i*th feature, Alice
    encodes and perturbs her value *v*[i], and reports the perturbed value to the
    data aggregator. To estimate the conditional probabilities for *F*[i] , the data
    aggregator estimates the frequency of individuals having the value *y* ∈ {1, 2,
    ..., *n*[i]} and class label *z* ∈ {1, 2, ..., *k*} as *E*[y,z] by estimating
    the frequency of input (*y* - 1)∙ *k* + *z*. Hence, the conditional probability
    *P*(*F*[i] = *x*[i]|*C*[j]) is estimated as
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在表5.2中每一行都有一个输入值。同样，收入的可能输入值有6个，性别的可能输入值有4个。在确定她的第*i*个特征的输入后，Alice会对她的值*v*[i]进行编码和扰动，并将扰动后的值报告给数据聚合器。为了估计*F*[i]的条件概率，数据聚合器通过估计具有值*y*
    ∈ {1, 2, ..., *n*[i]}和类别标签*z* ∈ {1, 2, ..., *k*}的个体的频率*E*[y,z]，来估计输入(*y* - 1)∙
    *k* + *z*的频率。因此，条件概率*P*(*F*[i] = *x*[i]|*C*[j])被估计为
- en: '![CH05_F02_zhuang-ch5-eqs-26x](../../OEBPS/Images/CH05_F02_zhuang-ch5-eqs-26x.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_zhuang-ch5-eqs-26x](../../OEBPS/Images/CH05_F02_zhuang-ch5-eqs-26x.png)'
- en: For the example in table 5.3, to estimate the probability *P*(*Age* = *Medium*|*C*[2]),
    the data aggregator estimates the frequency of 2, 4, and 6 as *E*[1,2], *E*[2,2],
    and *E*[3,2], respectively. Then *P*(*Age* = *Medium*|*C*[2]) can be estimated
    as
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于表 5.3 中的示例，为了估计概率 *P*(*Age* = *Medium*|*C*[2])，数据聚合器估计了 2、4 和 6 的频率为 *E*[1,2]、*E*[2,2]
    和 *E*[3,2]。然后，*P*(*Age* = *Medium*|*C*[2]) 可以估计为
- en: '![CH05_F02_zhuang-ch5-eqs-27x](../../OEBPS/Images/CH05_F02_zhuang-ch5-eqs-27x.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_zhuang-ch5-eqs-27x](../../OEBPS/Images/CH05_F02_zhuang-ch5-eqs-27x.png)'
- en: It is noteworthy that in order to contribute to the computation of class probabilities
    and conditional probabilities, each individual can prepare *n* + 1 inputs (i.e.,
    {*v*, *v*[1], *v*[2], ...., *v*[n]} for Alice) that can be reported after perturbation.
    However, reporting multiple values that are dependent on each other usually decreases
    the privacy level. Hence, each individual reports only one input value.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，为了有助于计算类概率和条件概率，每个个体可以准备 *n* + 1 个输入（即，Alice 的 {*v*, *v*[1], *v*[2],
    ..., *v*[n]}），这些输入可以在扰动后报告。然而，报告相互依赖的多个值通常会降低隐私级别。因此，每个个体只报告一个输入值。
- en: Finally, when the data aggregator estimates a value such as *E*[j] or *E*[(y,z)],
    the estimate may be negative. We can set all negative estimates to 1 to obtain
    a valid and reasonable probability.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当数据聚合器估计一个值，例如 *E*[j] 或 *E*[(y,z)] 时，估计值可能是负数。我们可以将所有负估计值设为 1，以获得一个有效且合理的概率。
- en: Using LDP with multidimensional data
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LDP 与多维数据
- en: The aforementioned frequency and mean estimation methods only work for one-dimensional
    data. But what if we have higher-dimensional data? If the data owned by individuals
    is multidimensional, reporting each value with these methods may cause privacy
    leaks due to the dependence on features.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述频率和均值估计方法仅适用于一维数据。但如果我们有更高维度的数据呢？如果个体拥有的数据是多维的，使用这些方法报告每个值可能会因为特征之间的依赖性而导致隐私泄露。
- en: 'To that end, three common approaches can be used with *n*-dimensional data:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，可以使用三种常见的方法与 *n*-维数据一起使用：
- en: '*Approach 1*—The Laplace mechanism (discussed in chapter 3) could be used with
    LDP if the noise is scaled with the number of dimensions *n*. Hence, if an individual’s
    input is *V* = (*v*[1], ..., *v*[n]) such that *v*[i] ∈ [-1,1] for all *i* ∈ {1,
    ..., *n*}, individuals can report each v[i] after adding *Lap*(2*n*/ε). However,
    this approach is not suitable if the number of dimensions *n* is high, because
    a large amount of noise reduces the accuracy.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 1*—如果噪声与维度 *n* 成比例缩放，则可以使用 LDP 与拉普拉斯机制（在第 3 章中讨论）一起使用。因此，如果个体的输入是 *V* =
    (*v*[1], ..., *v*[n])，且对所有 *i* ∈ {1, ..., *n*}，*v*[i] ∈ [-1,1]，则个体可以在添加 *Lap*(2*n*/ε)
    后报告每个 v[i]。然而，如果维度 *n* 较高，这种方法不适用，因为大量的噪声会降低准确性。'
- en: '*Approach 2*—We could utilize the Piecewise mechanism that we described in
    section 5.2.3\. The Piecewise mechanism can be used to perturb multidimensional
    numerical values with LDP protocols.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 2*—我们可以利用第 5.2.3 节中描述的 Piecewise 机制。Piecewise 机制可以用 LDP 协议扰动多维数值。'
- en: '*Approach 3*—The data aggregator can request only one perturbed input from
    each individual to satisfy ϵ-LDP. Each individual can select the input to be reported
    uniformly at random, or the data aggregator can divide the individuals into *n*
    groups and request different input values from each group. As a result, each feature
    is approximately reported by *m*/*n* individuals. This approach is suitable when
    the number of individuals *m* is high, relative to the number of features *n*.
    Otherwise, the accuracy decreases, since the number of reported values is low
    for each feature.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 3*—数据聚合器可以从每个个体请求一个扰动输入以满足 ϵ-LDP。每个个体可以随机均匀选择要报告的输入，或者数据聚合器可以将个体分成 *n*
    组，并从每组请求不同的输入值。结果，每个特征大约由 *m*/*n* 个个体报告。当个体数量 *m* 相对于特征数量 *n* 较高时，这种方法是合适的。否则，由于每个特征的报告值数量较低，准确性会降低。'
- en: Now that we’ve looked at how multidimensional data works with LDP, let’s look
    at the details of LDP in naive Bayes classification for continuous data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了多维数据如何与 LDP 一起工作，让我们看看 LDP 在连续数据朴素贝叶斯分类中的细节。
- en: 5.3.3 Using LDP naive Bayes with continuous features
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 使用具有连续特征的 LDP 朴素贝叶斯
- en: 'So far, we’ve seen how to apply LDP for discrete features. Now we’re going
    to explore how we can use the same concept for continuous features. LDP in naive
    Bayes classification for continuous data can be approached in two different ways:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何应用LDP（拉普拉斯离散化）来处理离散特征。现在我们将探讨如何将相同的概念应用于连续特征。在朴素贝叶斯分类中，对于连续数据，LDP可以采用两种不同的方法：
- en: We can discretize the continuous data and apply the discrete naive Bayes solution
    outlined in the previous section. In that case, continuous numerical data is divided
    into buckets to make it finite and discrete. Each individual perturbs their input
    after discretization.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将连续数据离散化，并应用上一节中概述的离散朴素贝叶斯解决方案。在这种情况下，连续数值数据被分为桶，使其有限且离散。每个个体在离散化后都会扰动其输入。
- en: The data aggregator can use Gaussian naive Bayes to estimate the probabilities.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据聚合器可以使用高斯朴素贝叶斯来估计概率。
- en: Let’s start with the first approach, discrete naive Bayes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一种方法，离散朴素贝叶斯开始。
- en: Discrete naive Bayes
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 离散朴素贝叶斯
- en: For discrete naive Bayes, we need to discretize the continuous data and use
    LDP frequency estimation techniques to estimate the frequency. Based on known
    features within a continuous domain, the data aggregator determines the intervals
    for the buckets in order to discretize the domain—Equal-Width Discretization (EWD)
    or Equal-Width Binning (EWB) can be used to equally partition the domain. EWD
    computes the width of each bin as (*max* - *min*)/*n*[b] where *max* and *min*
    are the maximum and minimum feature values and *n*[b] is the number of desired
    bins. In section 5.3.4, we’ll use the EWD method in some experiments for discretization.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散朴素贝叶斯，我们需要将连续数据离散化，并使用LDP频率估计技术来估计频率。基于连续域内的已知特征，数据聚合器确定桶的区间以对域进行离散化——可以使用等宽离散化（EWD）或等宽分箱（EWB）来等分域。EWD计算每个桶的宽度为
    (*max* - *min*)/*n*[b]，其中 *max* 和 *min* 是最大和最小特征值，*n*[b] 是期望的桶数。在第5.3.4节中，我们将使用EWD方法在一些实验中进行离散化。
- en: What is equal-width binning?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是等宽分箱？
- en: In general, binning is a data preprocessing method used to minimize the effects
    of small observation errors by dividing the original data values into small intervals
    known as *bins*. The original values are then replaced by a general value calculated
    for that bin. Basically, binning methods transform numerical variables into categorical
    counterparts but do not use the target (or class) information. This has a better
    chance of reducing overfitting in the case of smaller datasets.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分箱是一种数据预处理方法，通过将原始数据值分为称为“桶”的小区间来最小化小观察误差的影响。然后，原始值被替换为该桶计算出的一个一般值。基本上，分箱方法将数值变量转换为分类对应物，但不使用目标（或类别）信息。在较小数据集的情况下，这有更好的机会减少过拟合。
- en: 'There are two basic methods of dividing data into bins:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为桶有两种基本方法：
- en: '*Equal Frequency Binning* (*EFB*) **—**In this case, all the bins have equal
    frequency.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*等频分箱* (*EFB*) **—**在这种情况下，所有桶具有相同的频率。'
- en: 'Example input data: [0,4,12,16,16,18,24,26,28]'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例输入数据：[0,4,12,16,16,18,24,26,28]
- en: 'Bin 1: [0,4,12]'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱1：[0,4,12]
- en: 'Bin 2: [16,16,18]'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱2：[16,16,18]
- en: 'Bin 3: [24,26,28]'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱3：[24,26,28]
- en: '![CH05_F03_UN02_Zhuang](../../OEBPS/Images/CH05_F03_UN02_Zhuang.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03_UN02_Zhuang](../../OEBPS/Images/CH05_F03_UN02_Zhuang.png)'
- en: '*Equal Width Binning* (*EWB*) **—**In this case, the data is divided into intervals
    of equal size where the interval (or width) is defined as w = (max - min)/(number
    of bins).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*等宽分箱* (*EWB*) **—**在这种情况下，数据被分为大小相等的区间，其中区间（或宽度）定义为 w = (max - min)/(number
    of bins)。'
- en: 'Example input data: [0,4,12,16,16,18,24,26,28]'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例输入数据：[0,4,12,16,16,18,24,26,28]
- en: 'Bin 1: [0,4]'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱1：[0,4]
- en: 'Bin 2: [12,16,16,18]'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱2：[12,16,16,18]
- en: 'Bin 3: [24,26,28]'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱3：[24,26,28]
- en: '![CH05_F03_UN03_Zhuang](../../OEBPS/Images/CH05_F03_UN03_Zhuang.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03_UN03_Zhuang](../../OEBPS/Images/CH05_F03_UN03_Zhuang.png)'
- en: When the data aggregator shares the intervals with individuals, each individual
    discretizes their continuous feature values and applies a procedure similar to
    what we discussed for LDP naive Bayes with discrete features. The data aggregator
    also estimates the probabilities using the same procedure as for LDP naive Bayes
    for discrete data. Each individual should report just one perturbed value to guarantee
    ϵ-LDP. As you can see, discretization through binning is a kind of data preprocessing
    approach. The actual privacy protection is achieved through discrete naive Bayes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据聚合器与个体共享区间时，每个个体将他们的连续特征值离散化，并应用类似于我们讨论的LDP朴素贝叶斯离散特征的程序。数据聚合器也会使用与LDP朴素贝叶斯离散数据相同的程序来估计概率。每个个体应只报告一个扰动值以确保
    ϵ-LDP。如您所见，通过分箱进行离散化是一种数据预处理方法。实际的隐私保护是通过离散朴素贝叶斯实现的。
- en: Gaussian naive Bayes
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: The second approach for continuous data is using Gaussian naive Bayes. In this
    case, the most common practice is to assume the data is normally distributed.
    For LDP Gaussian naive Bayes, computing class probabilities is the same as for
    discrete features. To compute conditional probabilities, the data aggregator needs
    to have the mean and the variance of the training values for each feature given
    a class label. In other words, to compute *P*(*F*[i] = *x*[i]|*C*[j]), the data
    aggregator needs to estimate the mean *μ*[(i,j)] and the variance σ²[i,j] using
    the *F*[i] values of individuals with class label *C*[j]. That means the association
    between features and class labels has to be maintained (similar to the discrete
    naive Bayes classifier).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续数据的第二种方法是使用高斯朴素贝叶斯。在这种情况下，最常见的方法是假设数据呈正态分布。对于LDP高斯朴素贝叶斯，计算类别概率与离散特征相同。为了计算条件概率，数据聚合器需要具有每个特征在给定类别标签下的训练值的均值和方差。换句话说，为了计算
    *P*(*F*[i] = *x*[i]|*C*[j])，数据聚合器需要使用具有类别标签 *C*[j] 的个体的 *F*[i] 值来估计均值 *μ*[(i,j)]
    和方差 σ²[i,j]。这意味着特征与类别标签之间的关联必须保持（类似于离散朴素贝叶斯分类器）。
- en: We already discussed the mean estimation process, but to compute the mean *μ*[(i,j)]
    and the variance σ²[i,j] together, the data aggregator can divide the individuals
    into two groups. One group contributes to the estimation of the mean (i.e., *μ*[(i,j)])
    by perturbing their inputs and sharing them with the data aggregator. The other
    group contributes to estimating the mean of squares (i.e., μ^S[i,j]) by perturbing
    the squares of their inputs and sharing them with the data aggregator.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了均值估计过程，但要同时计算均值 *μ*[(i,j)] 和方差 σ²[i,j]，数据聚合器可以将个体分为两组。一组通过扰动他们的输入并与数据聚合器共享来为均值估计（即
    *μ*[(i,j)]) 贡献。另一组通过扰动他们输入的平方并与之共享来为平方均值（即 μ^S[i,j]）的估计做出贡献。
- en: Let’s consider another example. Suppose Bob has class label *C*[j] and a feature
    *F*[i] with a value *b*[i]. Let’s also assume that the domain of each feature
    is normalized to have a value between [-1,1]. If Bob is in the first group, he
    would add the Laplace noise to his value *b*[i] and obtain the perturbed feature
    value *b'*[i]. When the data aggregator collects all perturbed feature values
    from individuals in the first group having class label *C*[j], it computes the
    mean of the perturbed feature values, which gives an estimate of the mean *μ*[(i,j)]
    because the mean of the noise added by individuals is 0\. A similar approach could
    be followed by the second group. If Bob is in the second group, he would add noise
    to his squared value *b*²[i] to obtain *b^(2')*[i] and would share it with the
    data aggregator. Again, the data aggregator computes the estimate of the mean
    of squares (μ^S[i,j]). Finally, the variance σ²[i,j] can be computed as μ^S[i,j]
    - (μ[i,j])². Once again, each individual reports only one of their values or squares
    of their values after perturbation because they are dependent.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个例子。假设鲍勃有一个类别标签 *C*[j] 和一个特征 *F*[i]，其值为 *b*[i]。我们还可以假设每个特征的域被归一化，使其值介于
    [-1,1] 之间。如果鲍勃属于第一组，他会向他的值 *b*[i] 添加拉普拉斯噪声，从而获得扰动后的特征值 *b'*[i]。当数据聚合器收集到第一组具有类别标签
    *C*[j] 的个体扰动后的特征值时，它会计算扰动特征值的平均值，这给出了均值 *μ*[(i,j)] 的估计，因为个体添加的噪声平均值是 0。第二组可以采用类似的方法。如果鲍勃属于第二组，他会向他的平方值
    *b*²[i] 添加噪声，以获得 *b^(2')*[i]，并将其与数据聚合器共享。同样，数据聚合器会计算平方均值（μ^S[i,j]）的估计。最后，方差 σ²[i,j]
    可以通过 μ^S[i,j] - (μ[i,j])² 来计算。再次强调，每个个体在扰动后只报告他们的一个值或值的平方，因为它们是相关的。
- en: Thus far, the calculation of probabilities is clear and straightforward. But
    did you notice that when we calculate the mean and the variance, the class labels
    of individuals are not hidden from the data aggregator? How can we hide the original
    class labels?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，概率的计算是清晰且直接的。但你注意到当我们计算平均值和方差时，个体的类别标签并没有从数据聚合器那里隐藏吗？我们如何隐藏原始的类别标签？
- en: 'To overcome this problem and hide the class labels, we can adopt the following
    approach: let’s say Bob is reporting a feature value *F*[i] = *b*[i] associated
    with class *C*[j] where *j* ∈ {1,2, ..., *k*}. First he constructs a vector of
    length *k* where *k* is the number of class labels. The vector is initialized
    to zeros except for the *j*th element corresponding to the *j*th class label,
    which is set to the feature value *b*[i]. After that, each element of the vector
    is perturbed as usual (i.e., by adding the Laplace noise) and contributed to the
    data aggregator. Since noise is added even to the zero elements of the vector,
    the data aggregator will not be able to deduce the actual class label, or the
    actual values.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题并隐藏类别标签，我们可以采用以下方法：假设鲍勃正在报告一个与类别 *C*[j] 相关的特征值 *F*[i] = *b*[i]，其中 *j*
    ∈ {1,2, ..., *k*}。首先，他构建一个长度为 *k* 的向量，其中 *k* 是类别标签的数量。该向量初始化为零，除了对应于第 *j* 个类别标签的第
    *j* 个元素，它被设置为特征值 *b*[i]。之后，向量的每个元素都像往常一样被扰动（即，通过添加拉普拉斯噪声）并贡献给数据聚合器。由于甚至在向量的零元素上也添加了噪声，数据聚合器将无法推断出实际的类别标签或实际值。
- en: As for estimating the actual mean value (and the mean of the squared values)
    for each class, the data aggregator only needs to compute the mean of the perturbed
    values as usual and then divide by the probability of that class. To understand
    why we need to do that, assume that a class *j* has probability *P*(*C*[j]). Thus,
    for a feature *F*[i], only *P*(*C*[j]) of the individuals have their actual values
    in the *j*th element of the input vector, while the remaining proportion (1 -
    *P*(*C*[j])) have zeros. Hence, after the noise clustered around the actual mean
    cancels each other, and the noise clustered around zero cancels each other, we
    will have *P*(*C*[j]) × *μ*[(i,j)] = *observed* (*shifted*) *mean*. Thereafter,
    we can divide the observed mean by *P*(*C*[j]) to obtain the estimated mean. The
    same applies to the mean of the squared values and hence for computing the variance.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于估计每个类别的实际平均值（以及平方值的平均值），数据聚合器只需要像往常一样计算扰动值的平均值，然后除以该类的概率。为了理解为什么我们需要这样做，假设一个类别
    *j* 的概率为 *P*(*C*[j])。因此，对于特征 *F*[i]，只有 *P*(*C*[j]) 的个体在其输入向量的第 *j* 个元素中有实际值，而其余比例（1
    - *P*(*C*[j])) 有零。因此，在围绕实际平均值的噪声相互抵消后，围绕零的噪声也相互抵消，我们将得到 *P*(*C*[j]) × *μ*[(i,j)]
    = *观测* (*偏移*) *平均值*。之后，我们可以通过 *P*(*C*[j]) 来除以观测平均值以获得估计的平均值。同样的方法适用于平方值的平均值，因此也适用于计算方差。
- en: 5.3.4 Evaluating the performance of different LDP protocols
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.4 评估不同 LDP 协议的性能
- en: Now that we have gone through the theory, it’s time to discuss the implementation
    strategies and the results of the experimental evaluation of different LDP protocols.
    These experiments are based on datasets obtained from the UCI Machine Learning
    Repository [4]. Table 5.4 summarizes the datasets used in the experiments.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了理论，是时候讨论不同 LDP 协议的实现策略和实验评估结果了。这些实验基于从 UCI 机器学习仓库 [4] 获得的数据集。表 5.4
    总结了实验中使用的数据集。
- en: Table 5.4 A summary of datasets used in the experiments
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.4 实验中使用的数据集摘要
- en: '| Name of the dataset | Number of instances | Number of features | Number of
    class labels |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | 实例数量 | 特征数量 | 类别标签数量 |'
- en: '| Car evaluation | 1,728 | 6 | 4 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 车辆评估 | 1,728 | 6 | 4 |'
- en: '| Chess | 3,196 | 36 | 2 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 国际象棋 | 3,196 | 36 | 2 |'
- en: '| Mushroom | 8,124 | 22 | 2 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 蘑菇 | 8,124 | 22 | 2 |'
- en: '| Connect-4 | 67,557 | 42 | 3 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 四子棋 | 67,557 | 42 | 3 |'
- en: '| Australian credit approval | 690 | 14 | 2 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚信用批准 | 690 | 14 | 2 |'
- en: '| Diabetes | 768 | 8 | 2 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 糖尿病 | 768 | 8 | 2 |'
- en: To evaluate the accuracy of naive Bayes classification under LDP, we implemented
    the methods discussed in the previous sections in Python utilizing the pandas
    and NumPy libraries. We implemented five different LDP protocols for frequency
    estimation—direct encoding (DE), summation with histogram encoding (SHE), thresholding
    with histogram encoding (THE), symmetric unary encoding (SUE), and optimal unary
    encoding (OUE)—and the experiments were performed with different θ values in THE.
    With these experiments, we found that the best accuracy can be achieved whenever
    θ = 0.25, so we’ll give the experimental results of SHE for θ = 0.25\. In a nutshell,
    we will compare the results of these different implementations to show you which
    algorithm works best for each of these datasets.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估在LDP下朴素贝叶斯分类的准确性，我们使用Python和pandas以及NumPy库实现了前几节讨论的方法。我们实现了五种不同的LDP协议用于频率估计——直接编码（DE）、使用直方图编码的求和（SHE）、使用直方图编码的阈值（THE）、对称一元编码（SUE）和最优一元编码（OUE），并且在不同θ值下进行了THE的实验。通过这些实验，我们发现当θ
    = 0.25时可以达到最佳准确性，因此我们将给出SHE在θ = 0.25时的实验结果。总的来说，我们将比较这些不同实现的成果，以展示哪个算法最适合这些数据集。
- en: Evaluating LDP naive Bayes with discrete features
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用离散特征评估LDP朴素贝叶斯
- en: To evaluate the accuracy of using LDP naive Bayes for classifying data with
    discrete features, four different datasets from the UCI ML repository (Car evaluation,
    Chess, Mushroom, and Connect-4) were used. Initially, naive Bayes classification
    without LDP was performed as the baseline for comparing the accuracy of different
    encoding mechanisms under LDP.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估使用LDP朴素贝叶斯对具有离散特征的数据进行分类的准确性，我们使用了来自UCI ML存储库的四个不同数据集（Car evaluation、Chess、Mushroom和Connect-4）。最初，执行了没有LDP的朴素贝叶斯分类作为比较不同编码机制在LDP下准确性的基准。
- en: Let’s look at the experimental results for varying ϵ values up to 5, as shown
    in figure 5.3\. The dotted lines show accuracy without privacy. As expected, when
    the number of instances in the training set increases, the accuracy is better
    for smaller ϵ values. For instance, in the Connect-4 dataset, all protocols except
    SHE provide more than 65% accuracy even for very small ϵ values. Since the accuracy
    without privacy is approximately 75%, the accuracy of all of these protocols for
    ϵ values smaller than 1 is noticeable. The results are also similar for the Mushroom
    dataset. When it comes to ϵ = 0.5, all protocols except SHE provide nearly 90%
    classification accuracy.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看图5.3所示的实验结果，其中显示了直到5的ε值的变化。虚线显示了没有隐私的准确性。正如预期的那样，当训练集中的实例数量增加时，较小的ε值准确性更好。例如，在Connect-4数据集中，除了SHE以外的所有协议即使在非常小的ε值下也提供了超过65%的准确性。由于没有隐私的准确性大约为75%，因此所有这些协议在ε值小于1时的准确性是显著的。Mushroom数据集的结果也类似。当ε
    = 0.5时，除了SHE以外的所有协议提供了近90%的分类准确性。
- en: '![CH05_F03_Zhuang](../../OEBPS/Images/CH05_F03_Zhuang.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03_Zhuang](../../OEBPS/Images/CH05_F03_Zhuang.png)'
- en: Figure 5.3 LDP naive Bayes classification accuracy for datasets with discrete
    features
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3具有离散特征的LDP朴素贝叶斯分类准确性
- en: In all of the datasets, you can see that the protocol with the worst accuracy
    is SHE. Since this protocol simply sums the noisy values, its variance is higher
    than the other protocols. In addition, DE achieves the best accuracy for small
    ϵ values in the Car Evaluation and Chess datasets because the input domains are
    small. On the other hand, the variance of DE is proportional to the size of the
    input domain. Therefore, its accuracy is better when the input domain is small.
    We can also see that SUE and OUE provide similar accuracy in all of the experiments.
    They perform better than DE when the size of the input domain is large. Although
    OUE is proposed to decrease the variance, a considerable utility difference between
    SUE and OUE was not observed in this set of experiments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有数据集中，你可以看到准确性最差的协议是SHE。因为这个协议只是简单地对噪声值求和，其方差高于其他协议。此外，DE在Car Evaluation和Chess数据集中对于小的ε值实现了最佳准确性，因为输入域较小。另一方面，DE的方差与输入域的大小成正比。因此，当输入域较小时，其准确性更好。我们还可以看到，在所有实验中，SUE和OUE提供了相似的准确性。当输入域较大时，它们的表现优于DE。尽管OUE被提出以减少方差，但在这一组实验中，SUE和OUE之间没有观察到显著的效用差异。
- en: Evaluating LDP naive Bayes with continuous features
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用连续特征评估LDP朴素贝叶斯
- en: 'Now let’s discuss the results of LDP naive Bayes with continuous data. In this
    case, the experiments were conducted on two different datasets: Australian credit
    approval and Diabetes. The Australian dataset has 14 original features, and the
    Diabetes dataset has 8 features.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论使用连续数据的 LDP Naive Bayes 的结果。在这种情况下，实验是在两个不同的数据集上进行的：澳大利亚信用批准和糖尿病。澳大利亚数据集有
    14 个原始特征，而糖尿病数据集有 8 个特征。
- en: Initially, the discretization method was applied, and then two dimensionality
    reduction techniques (PCA and DCA) were implemented to observe their effect on
    accuracy. The results for the two datasets for different values of ϵ are given
    in figure 5.4\. We also present the results for two LDP schemes, direct encoding
    and optimized unary encoding, which provide the best accuracy for different domain
    sizes. The input domain is divided into *d* = 2 buckets for the Australian dataset
    and *d* = 4 buckets for the Diabetes dataset.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，应用了离散化方法，然后实现了两种降维技术（PCA 和 DCA）以观察它们对准确率的影响。图 5.4 给出了两个数据集在不同 ϵ 值下的结果。我们还展示了两种
    LDP 方案的结果，直接编码和优化一元编码，它们为不同领域大小提供了最佳准确率。对于澳大利亚数据集，输入域被划分为 *d* = 2 个桶，而对于糖尿病数据集，输入域被划分为
    *d* = 4 个桶。
- en: '![CH05_F04_Zhuang](../../OEBPS/Images/CH05_F04_Zhuang.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F04_Zhuang](../../OEBPS/Images/CH05_F04_Zhuang.png)'
- en: Figure 5.4 LDP naive Bayes classification accuracy for datasets with continuous
    features using discretization
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 使用离散化方法对具有连续特征的数据集进行 LDP Naive Bayes 分类准确率
- en: As you can see, for the Australian dataset, the best results for principal component
    analysis (PCA) and discriminant component analysis (DCA) are obtained when the
    number of features is reduced to 1\. On the other hand, for the Diabetes dataset,
    the best accuracy is achieved when PCA reduces the number of features to 6 and
    when DCA reduces the number of features to 1\. As is evident in figure 5.4, DCA
    provides the best classification accuracy, which shows the advantage of using
    dimensionality reduction before discretization. You can also see that DCA’s accuracy
    is better than PCA’s, since DCA is mainly designed for classification.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，对于澳大利亚数据集，当特征数量减少到 1 时，主成分分析（PCA）和判别成分分析（DCA）获得了最佳结果。另一方面，对于糖尿病数据集，当 PCA
    将特征数量减少到 6，而 DCA 将特征数量减少到 1 时，达到了最佳准确率。如图 5.4 所示，DCA 提供了最佳的分类准确率，这显示了在离散化之前使用降维的优势。您还可以看到，DCA
    的准确率优于 PCA，因为 DCA 主要是为分类设计的。
- en: What are PCA and DCA?
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 和 DCA 是什么？
- en: Principal component analysis (PCA) and discriminant component analysis (DCA)
    are two well-known dimensionality reduction methods that are often used to reduce
    the dimensionality of large datasets. Both PCA and DCA work by projecting data
    to a lower-dimensional hyperplane. However, the critical difference between them
    is that PCA assumes a linear relationship to the gradients, while DCA assumes
    a unimodal relationship. We will discuss different dimensionality reduction approaches
    in more detail in chapter 9.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）和判别成分分析（DCA）是两种常用的降维方法，常用于降低大型数据集的维度。PCA 和 DCA 都通过将数据投影到低维超平面来工作。然而，它们之间的关键区别在于
    PCA 假设与梯度之间存在线性关系，而 DCA 假设存在单峰关系。我们将在第 9 章中更详细地讨论不同的降维方法。
- en: In addition, we applied LDP Gaussian naive Bayes (LDP-GNB) on the same two datasets.
    All three perturbation approaches that we discussed for multidimensional data
    (in the subsection titled “Using LDP with multidimensional data”) were implemented.
    Figure 5.5 shows the results of performing LDP-GNB on these two datasets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还对相同的两个数据集应用了 LDP 高斯 Naive Bayes（LDP-GNB）。我们讨论的多维数据（在标题为“使用 LDP 处理多维数据”的子节中）的三个扰动方法都得到了实现。图
    5.5 展示了在这两个数据集上执行 LDP-GNB 的结果。
- en: '![CH05_F05_Zhuang](../../OEBPS/Images/CH05_F05_Zhuang.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Zhuang](../../OEBPS/Images/CH05_F05_Zhuang.png)'
- en: Figure 5.5 LDP Gaussian naive Bayes classification accuracy for datasets with
    continuous features
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 使用连续特征的 LDP 高斯 Naive Bayes 分类准确率
- en: As you can see, the first of the three approaches (using the Laplace mechanism)
    results in the lowest utility, since individuals report all the features by adding
    more noise, proportional to the number of dimensions. In each figure, three curves
    are shown, corresponding to using the original data (with 14 or 8 features for
    the Australian and Diabetes datasets, respectively) or projecting the data using
    PCA or DCA before applying the LDP noise. All the graphs show the positive effect
    of reducing the dimensions. In both datasets, and for PCA and DCA, the number
    of reduced dimensions was 1\. DCA or PCA always performs better than the original
    data and for all perturbation approaches.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，三种方法中的第一种（使用拉普拉斯机制）导致最低的效用，因为个体通过添加更多与维度数量成比例的噪声来报告所有特征。在每个图中，都显示了三条曲线，对应于使用原始数据（澳大利亚和糖尿病数据集分别有14个或8个特征）或在使用
    LDP 噪声之前使用 PCA 或 DCA 投影数据。所有图表都显示了降低维度的积极影响。在两个数据集中，对于 PCA 和 DCA，降低的维度数都是 1。DCA
    或 PCA 总是比原始数据表现更好，并且对于所有扰动方法。
- en: Finally, when you compare the results of discretization and Gaussian naive Bayes
    for continuous data, you’ll see that discretization provides better accuracy.
    Especially for smaller ϵ values, the superiority of discretization is apparent.
    Although it is impossible to compare the amount of noise for randomized response
    and the Laplace mechanism, discretization possibly causes less noise due to the
    smaller input domain.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当您比较连续数据的离散化和高斯 Naive Bayes 的结果时，您会发现离散化提供了更好的准确性。特别是对于较小的 ϵ 值，离散化的优越性很明显。尽管无法比较随机响应和拉普拉斯机制产生的噪声量，但离散化可能由于输入域较小而引起更少的噪声。
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: There are different advanced LDP mechanisms that work for both one-dimensional
    and multidimensional data.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有不同的高级 LDP 机制适用于一维和多维数据。
- en: As we did with the centralized setting of DP, we can implement the Laplace mechanism
    for LDP as well.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我们在 DP 的集中设置中所做的那样，我们也可以为 LDP 实现拉普拉斯机制。
- en: While the Laplace mechanism is one way to generate noise for the perturbation,
    Duchi’s mechanism can also be used to perturb multidimensional continuous numerical
    data for LDP.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然拉普拉斯机制是生成扰动噪声的一种方法，但 Duchi 机制也可以用来扰动 LDP 的多维连续数值数据。
- en: The Piecewise mechanism can be used to deal with multidimensional continuous
    numerical data for LDP while overcoming the disadvantages of the Laplace and Duchi’s
    mechanisms.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 断续机制可以用来处理 LDP 的多维连续数值数据，同时克服拉普拉斯和 Duchi 机制的不利之处。
- en: Naive Bayes is a simple yet powerful ML classifier that can be used with LDP
    frequency estimation techniques.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naive Bayes 是一种简单而强大的机器学习分类器，它可以与 LDP 频率估计技术一起使用。
- en: LDP naive Bayes can be used with both discrete and continuous features.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDP Naive Bayes 可以与离散和连续特征一起使用。
- en: When it comes to LDP naive Bayes with continuous features, there are two main
    approaches, namely, discrete naive Bayes and Gaussian naive Bayes.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当涉及到具有连续特征的 LDP Naive Bayes 时，有两种主要方法，即离散 Naive Bayes 和高斯 Naive Bayes。
