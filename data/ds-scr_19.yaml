- en: Chapter 18\. Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章\. 神经网络
- en: I like nonsense; it wakes up the brain cells.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我喜欢胡说八道；它唤醒了大脑细胞。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dr. Seuss
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 博士苏斯
- en: An *artificial neural network* (or neural network for short) is a predictive
    model motivated by the way the brain operates. Think of the brain as a collection
    of neurons wired together. Each neuron looks at the outputs of the other neurons
    that feed into it, does a calculation, and then either fires (if the calculation
    exceeds some threshold) or doesn’t (if it doesn’t).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工神经网络*（或简称神经网络）是一种受大脑运作方式启发的预测模型。将大脑视为一组互相连接的神经元。每个神经元查看输入到它的其他神经元的输出，进行计算，然后根据计算结果是否超过某个阈值来“激活”或不激活。'
- en: Accordingly, artificial neural networks consist of artificial neurons, which
    perform similar calculations over their inputs. Neural networks can solve a wide
    variety of problems like handwriting recognition and face detection, and they
    are used heavily in deep learning, one of the trendiest subfields of data science.
    However, most neural networks are “black boxes”—inspecting their details doesn’t
    give you much understanding of *how* they’re solving a problem. And large neural
    networks can be difficult to train. For most problems you’ll encounter as a budding
    data scientist, they’re probably not the right choice. Someday, when you’re trying
    to build an artificial intelligence to bring about the Singularity, they very
    well might be.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，人工神经网络由人工神经元组成，它们对其输入执行类似的计算。神经网络可以解决各种问题，如手写识别和面部检测，在深度学习中广泛使用，这是数据科学中最流行的子领域之一。然而，大多数神经网络是“黑箱”—检查它们的细节并不能让你理解它们如何解决问题。而且，大型神经网络可能难以训练。对于大多数初涉数据科学的问题，它们可能不是正确的选择。当你试图构建一个引发“奇点”的人工智能时，它们可能非常合适。
- en: Perceptrons
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器
- en: 'Pretty much the simplest neural network is the *perceptron*, which approximates
    a single neuron with *n* binary inputs. It computes a weighted sum of its inputs
    and “fires” if that weighted sum is 0 or greater:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的神经网络就是*感知器*，它模拟了一个具有*n*个二进制输入的单个神经元。它计算其输入的加权和，如果该加权和大于等于0，则“激活”：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The perceptron is simply distinguishing between the half-spaces separated by
    the hyperplane of points `x` for which:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器只是区分了由点`x`组成的超平面分隔的半空间：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With properly chosen weights, perceptrons can solve a number of simple problems
    ([Figure 18-1](#a_perceptron)). For example, we can create an *AND gate* (which
    returns 1 if both its inputs are 1 but returns 0 if one of its inputs is 0) with:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当选择的权重，感知器可以解决许多简单的问题（[图18-1](#a_perceptron)）。例如，我们可以创建一个*AND门*（如果其输入都是1则返回1，但如果其中一个输入是0则返回0）：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If both inputs are 1, the `calculation` equals 2 + 2 – 3 = 1, and the output
    is 1\. If only one of the inputs is 1, the `calculation` equals 2 + 0 – 3 = –1,
    and the output is 0\. And if both of the inputs are 0, the `calculation` equals
    –3, and the output is 0.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个输入都是1，`计算`等于2 + 2 – 3 = 1，并且输出为1。如果只有一个输入是1，`计算`等于2 + 0 – 3 = –1，并且输出为0。如果两个输入都是0，`计算`等于–3，并且输出为0。
- en: 'Using similar reasoning, we could build an *OR gate* with:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的推理，我们可以用以下方式构建一个*OR门*：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Perceptron.](assets/dsf2_1801.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![感知器。](assets/dsf2_1801.png)'
- en: Figure 18-1\. Decision space for a two-input perceptron
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-1\. 二输入感知器的决策空间
- en: 'We could also build a *NOT gate* (which has one input and converts 1 to 0 and
    0 to 1) with:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用以下方式构建一个*NOT门*（其只有一个输入，并将1转换为0，将0转换为1）：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, there are some problems that simply can’t be solved by a single perceptron.
    For example, no matter how hard you try, you cannot use a perceptron to build
    an *XOR gate* that outputs 1 if exactly one of its inputs is 1 and 0 otherwise.
    This is where we start needing more complicated neural networks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一些问题简单地无法通过单个感知器解决。例如，无论你多么努力，你都不能使用一个感知器来构建一个*异或门*，即当其输入中只有一个是1时输出为1，否则为0。这时我们需要更复杂的神经网络。
- en: 'Of course, you don’t need to approximate a neuron in order to build a logic
    gate:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你不需要模拟神经元来构建逻辑门：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Like real neurons, artificial neurons start getting more interesting when you
    start connecting them together.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 像真实的神经元一样，人工神经元开始在连接起来时变得更加有趣。
- en: Feed-Forward Neural Networks
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: The topology of the brain is enormously complicated, so it’s common to approximate
    it with an idealized *feed-forward* neural network that consists of discrete *layers*
    of neurons, each connected to the next. This typically entails an input layer
    (which receives inputs and feeds them forward unchanged), one or more “hidden
    layers” (each of which consists of neurons that take the outputs of the previous
    layer, performs some calculation, and passes the result to the next layer), and
    an output layer (which produces the final outputs).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑的拓扑结构极其复杂，因此常常用由离散的*层*组成的理想化*前馈*神经网络来近似它，每个层都与下一层相连接。通常包括一个输入层（接收并不改变输入并向前传递），一个或多个“隐藏层”（每个都由神经元组成，接收前一层的输出，执行某些计算，并将结果传递到下一层），和一个输出层（生成最终的输出）。
- en: Just like in the perceptron, each (noninput) neuron has a weight corresponding
    to each of its inputs and a bias. To make our representation simpler, we’ll add
    the bias to the end of our weights vector and give each neuron a *bias input*
    that always equals 1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在感知机中一样，每个（非输入）神经元都有与其每个输入对应的权重和偏置。为了简化我们的表示，我们将偏置添加到权重向量的末尾，并给每个神经元一个*偏置输入*，其值始终为1。
- en: 'As with the perceptron, for each neuron we’ll sum up the products of its inputs
    and its weights. But here, rather than outputting the `step_function` applied
    to that product, we’ll output a smooth approximation of it. Here we’ll use the
    `sigmoid` function ([Figure 18-2](#sigmoid_function)):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与感知机类似，对于每个神经元，我们将计算其输入和权重的乘积之和。但在这里，我们不会输出应用于该乘积的`step_function`，而是输出其平滑的近似。这里我们将使用`sigmoid`函数（[图18-2](#sigmoid_function)）：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Sigmoid.](assets/dsf2_1802.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Sigmoid.](assets/dsf2_1802.png)'
- en: Figure 18-2\. The sigmoid function
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-2\. sigmoid函数
- en: Why use `sigmoid` instead of the simpler `step_function`? In order to train
    a neural network, we need to use calculus, and in order to use calculus, we need
    *smooth* functions. `step_function` isn’t even continuous, and `sigmoid` is a
    good smooth approximation of it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用`sigmoid`而不是更简单的`step_function`？为了训练神经网络，我们需要使用微积分，而要使用微积分，我们需要*平滑*的函数。`step_function`甚至不连续，而`sigmoid`是它的一个很好的平滑近似。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may remember `sigmoid` from [Chapter 16](ch16.html#logistic_regression),
    where it was called `logistic`. Technically “sigmoid” refers to the *shape* of
    the function and “logistic” to this particular function, although people often
    use the terms interchangeably.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得`sigmoid`函数，它在[第16章](ch16.html#logistic_regression)中称为`logistic`。技术上，“sigmoid”指的是函数的形状，“logistic”指的是特定的函数，尽管人们经常将这些术语互换使用。
- en: 'We then calculate the output as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算输出为：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Given this function, we can represent a neuron simply as a vector of weights
    whose length is one more than the number of inputs to that neuron (because of
    the bias weight). Then we can represent a neural network as a list of (noninput)
    *layers*, where each layer is just a list of the neurons in that layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个函数，我们可以简单地将神经元表示为一个权重向量，其长度比该神经元的输入数量多一个（因为有偏置权重）。然后我们可以将神经网络表示为（非输入）*层*的列表，其中每一层只是该层中神经元的列表。
- en: That is, we’ll represent a neural network as a list (layers) of lists (neurons)
    of vectors (weights).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是，我们将神经网络表示为向量的列表（层），其中每个向量（神经元）是向量（权重）的列表。
- en: 'Given such a representation, using the neural network is quite simple:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这样的表示法，使用神经网络非常简单：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now it’s easy to build the XOR gate that we couldn’t build with a single perceptron.
    We just need to scale the weights up so that the `neuron_output`s are either really
    close to 0 or really close to 1:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很容易构建我们无法用单个感知机构建的XOR门。我们只需扩展权重，使得`neuron_output`要么非常接近0，要么非常接近1：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For a given input (which is a two-dimensional vector), the hidden layer produces
    a two-dimensional vector consisting of the “and” of the two input values and the
    “or” of the two input values.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的输入（一个二维向量），隐藏层生成一个二维向量，包含两个输入值的“与”和两个输入值的“或”。
- en: And the output layer takes a two-dimensional vector and computes “second element
    but not first element.” The result is a network that performs “or, but not and,”
    which is precisely XOR ([Figure 18-3](#xor_neural_network)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层接收一个二维向量，并计算“第二个元素但不是第一个元素”。结果是一个执行“或但不是与”的网络，这正是XOR（[图18-3](#xor_neural_network)）。
- en: '![Neural Network.](assets/dsf2_1803.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络.](assets/dsf2_1803.png)'
- en: Figure 18-3\. A neural network for XOR
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-3\. 一个用于XOR的神经网络
- en: One suggestive way of thinking about this is that the hidden layer is computing
    *features* of the input data (in this case “and” and “or”) and the output layer
    is combining those features in a way that generates the desired output.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有启发性的思考方式是，隐藏层正在计算输入数据的*特征*（在本例中是“and”和“or”），输出层将这些特征组合起来以生成所需的输出。
- en: Backpropagation
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Usually we don’t build neural networks by hand. This is in part because we use
    them to solve much bigger problems—an image recognition problem might involve
    hundreds or thousands of neurons. And it’s in part because we usually won’t be
    able to “reason out” what the neurons should be.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们不会手工构建神经网络。部分原因是因为我们使用它们来解决更大的问题——例如图像识别问题可能涉及数百或数千个神经元。另一部分原因是通常我们无法“推理出”神经元应该是什么。
- en: Instead (as usual) we use data to *train* neural networks. The typical approach
    is an algorithm called *backpropagation*, which uses gradient descent or one of
    its variants.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相反（通常情况下），我们使用数据来*训练*神经网络。典型的方法是一种称为*反向传播*的算法，它使用梯度下降或其变种之一。
- en: 'Imagine we have a training set that consists of input vectors and corresponding
    target output vectors. For example, in our previous `xor_network` example, the
    input vector `[1, 0]` corresponded to the target output `[1]`. Imagine that our
    network has some set of weights. We then adjust the weights using the following
    algorithm:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个训练集，由输入向量和相应的目标输出向量组成。例如，在我们之前的`xor_network`示例中，输入向量`[1, 0]`对应于目标输出`[1]`。假设我们的网络有一些权重。然后，我们使用以下算法调整权重：
- en: Run `feed_forward` on an input vector to produce the outputs of all the neurons
    in the network.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入向量运行`feed_forward`以产生网络中所有神经元的输出。
- en: We know the target output, so we can compute a *loss* that’s the sum of the
    squared errors.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们知道目标输出，所以我们可以计算一个*损失*，即平方误差的总和。
- en: Compute the gradient of this loss as a function of the output neuron’s weights.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这种损失作为输出神经元权重的函数的梯度。
- en: “Propagate” the gradients and errors backward to compute the gradients with
    respect to the hidden neurons’ weights.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “传播”梯度和误差向后计算与隐藏神经元权重相关的梯度。
- en: Take a gradient descent step.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行梯度下降步骤。
- en: Typically we run this algorithm many times for our entire training set until
    the network converges.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会对整个训练集运行这个算法多次，直到网络收敛。
- en: 'To start with, let’s write the function to compute the gradients:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编写计算梯度的函数：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The math behind the preceding calculations is not terribly difficult, but it
    involves some tedious calculus and careful attention to detail, so I’ll leave
    it as an exercise for you.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 前述计算背后的数学并不是非常困难，但涉及一些繁琐的微积分和仔细的注意细节，所以我会留给你作为练习。
- en: Armed with the ability to compute gradients, we can now train neural networks.
    Let’s try to learn the XOR network we previously designed by hand.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有了计算梯度的能力，我们现在可以训练神经网络。让我们试着通过手工设计的XOR网络来学习它。
- en: 'We’ll start by generating the training data and initializing our neural network
    with random weights:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从生成训练数据开始，并使用随机权重初始化我们的神经网络：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As usual, we can train it using gradient descent. One difference from our previous
    examples is that here we have several parameter vectors, each with its own gradient,
    which means we’ll have to call `gradient_step` for each of them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们可以使用梯度下降来训练它。与我们之前的例子不同之处在于，这里我们有几个参数向量，每个向量都有自己的梯度，这意味着我们需要为每个向量调用`gradient_step`。
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For me the resulting network has weights that look like:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，得到的网络具有如下权重：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: which is conceptually pretty similar to our previous bespoke network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这与我们之前的定制网络非常相似。
- en: 'Example: Fizz Buzz'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：Fizz Buzz
- en: 'The VP of Engineering wants to interview technical candidates by making them
    solve “Fizz Buzz,” the following well-trod programming challenge:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 工程副总裁希望通过让技术候选人解决“Fizz Buzz”，来面试技术候选人，这是一个广为人知的编程挑战：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: He thinks the ability to solve this demonstrates extreme programming skill.
    You think that this problem is so simple that a neural network could solve it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 他认为解决这个问题表明了极限编程技能。你认为这个问题如此简单，以至于一个神经网络可以解决它。
- en: Neural networks take vectors as inputs and produce vectors as outputs. As stated,
    the programming problem is to turn an integer into a string. So the first challenge
    is to come up with a way to recast it as a vector problem.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络将向量作为输入，并产生向量作为输出。正如所述，编程问题是将整数转换为字符串。因此，第一个挑战是想出一种将其重新定义为向量问题的方法。
- en: 'For the outputs it’s not tough: there are basically four classes of outputs,
    so we can encode the output as a vector of four 0s and 1s:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出来说并不困难：基本上有四类输出，所以我们可以将输出编码为一个包含四个 0 和 1 的向量：
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We’ll use this to generate our target vectors. The input vectors are less obvious.
    You don’t want to just use a one-dimensional vector containing the input number,
    for a couple of reasons. A single input captures an “intensity,” but the fact
    that 2 is twice as much as 1, and that 4 is twice as much again, doesn’t feel
    relevant to this problem. Additionally, with just one input the hidden layer wouldn’t
    be able to compute very interesting features, which means it probably wouldn’t
    be able to solve the problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用这个来生成我们的目标向量。输入向量则不那么明显。你不应该只使用一个包含输入数字的一维向量，因为有几个原因。一个单一的输入捕捉到了“强度”，但是
    2 是 1 的两倍，4 又是两倍，对这个问题并不感兴趣。此外，只有一个输入，隐藏层将无法计算非常有趣的特征，这意味着它可能无法解决问题。
- en: It turns out that one thing that works reasonably well is to convert each number
    to its *binary* representation of 1s and 0s. (Don’t worry, this isn’t obvious—at
    least it wasn’t to me.)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 原来，一个相对有效的方法是将每个数字转换为其*二进制*表示，由 1 和 0 组成。（别担心，这不明显——至少对我来说不是。）
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As the goal is to construct the outputs for the numbers 1 to 100, it would
    be cheating to train on those numbers. Therefore, we’ll train on the numbers 101
    to 1,023 (which is the largest number we can represent with 10 binary digits):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因为目标是构建 1 到 100 的输出，所以在这些数字上进行训练是作弊的。因此，我们将在数字 101 到 1,023 上进行训练（这是我们可以用 10
    位二进制表示的最大数字）：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, let’s create a neural network with random initial weights. It will have
    10 input neurons (since we’re representing our inputs as 10-dimensional vectors)
    and 4 output neurons (since we’re representing our targets as 4-dimensional vectors).
    We’ll give it 25 hidden units, but we’ll use a variable for that so it’s easy
    to change:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个具有随机初始权重的神经网络。它将有 10 个输入神经元（因为我们将我们的输入表示为 10 维向量）和 4 个输出神经元（因为我们将我们的目标表示为
    4 维向量）。我们将给它 25 个隐藏单元，但我们将使用一个变量，所以这样很容易改变：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'That’s it. Now we’re ready to train. Because this is a more involved problem
    (and there are a lot more things to mess up), we’d like to closely monitor the
    training process. In particular, for each epoch we’ll track the sum of squared
    errors and print them out. We want to make sure they decrease:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是了。现在我们准备好训练了。因为这是一个更复杂的问题（而且有很多事情可能会出错），我们希望密切监控训练过程。特别是，对于每个时期，我们将跟踪平方误差的总和并将其打印出来。我们希望确保它们会减少：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This will take a while to train, but eventually the loss should start to bottom
    out.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要一段时间来训练，但最终损失应该开始稳定下来。
- en: 'At last we’re ready to solve our original problem. We have one remaining issue.
    Our network will produce a four-dimensional vector of numbers, but we want a single
    prediction. We’ll do that by taking the `argmax`, which is the index of the largest
    value:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备解决我们最初的问题。我们还有一个问题。我们的网络将生成一个四维向量的数字，但我们想要一个单一的预测。我们将通过取`argmax`来做到这一点，这是最大值的索引：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we can finally solve “FizzBuzz”:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以解决“FizzBuzz”了：
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: For me the trained network gets 96/100 correct, which is well above the VP of
    Engineering’s hiring threshold. Faced with the evidence, he relents and changes
    the interview challenge to “Invert a Binary Tree.”
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，训练后的网络获得了 96/100 的正确率，远高于工程副总裁的招聘门槛。面对这些证据，他屈服了并将面试挑战改为“反转二叉树”。
- en: For Further Exploration
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: 'Keep reading: [Chapter 19](ch19.html#deep_learning) will explore these topics
    in much more detail.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续阅读：[第 19 章](ch19.html#deep_learning)将更详细地探讨这些主题。
- en: My blog post on [“Fizz Buzz in Tensorflow”](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/)
    is pretty good.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我关于[“Tensorflow 中的 Fizz Buzz”](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/)的博客帖子相当不错。
