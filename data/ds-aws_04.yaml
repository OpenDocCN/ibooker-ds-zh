- en: Chapter 4\. Ingest Data into the Cloud
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章\. 将数据引入云中
- en: In this chapter, we will show how to ingest data into the cloud. For that purpose,
    we will look at a typical scenario in which an application writes files into an
    Amazon S3 data lake, which in turn needs to be accessed by the ML engineering/data
    science team as well as the business intelligence/data analyst team, as shown
    in [Figure 4-1](#an_application_writes_data_into_our_sth).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示如何将数据引入云中。为此，我们将查看一个典型的场景，即应用程序将文件写入亚马逊 S3 数据湖，然后由 ML 工程师/数据科学团队以及商业智能/数据分析团队访问，如[图
    4-1](#an_application_writes_data_into_our_sth)所示。
- en: '![](assets/dsaw_0401.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0401.png)'
- en: Figure 4-1\. An application writes data into our S3 data lake for the data science,
    machine learning engineering, and business intelligence teams.
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 一个应用程序将数据写入我们的 S3 数据湖，供数据科学、机器学习工程和商业智能团队使用。
- en: Amazon Simple Storage Service (Amazon S3) is fully managed object storage that
    offers extreme durability, high availability, and infinite data scalability at
    a very low cost. Hence, it is the perfect foundation for data lakes, training
    datasets, and models. We will learn more about the advantages of building data
    lakes on Amazon S3 in the next section.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊简单存储服务（Amazon S3）是一种完全托管的对象存储，具有极高的耐久性、高可用性和无限的数据可伸缩性，成本非常低廉。因此，它是构建数据湖、训练数据集和模型的理想基础。在接下来的章节中，我们将更多地了解在亚马逊
    S3 上构建数据湖的优势。
- en: Let’s assume our application continually captures data (i.e., customer interactions
    on our website, product review messages) and writes the data to S3 in the tab-separated
    values (TSV) file format.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们的应用程序持续捕获数据（例如网站上的客户互动、产品评论消息），并将数据以制表符分隔值（TSV）文件格式写入 S3。
- en: As a data scientist or machine learning engineer, we want to quickly explore
    raw datasets. We will introduce Amazon Athena and show how to leverage Athena
    as an interactive query service to analyze data in S3 using standard SQL, without
    moving the data. In the first step, we will register the TSV data in our S3 bucket
    with Athena and then run some ad hoc queries on the dataset. We will also show
    how to easily convert the TSV data into the more query-optimized, columnar file
    format Apache Parquet.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家或机器学习工程师，我们希望能够快速探索原始数据集。我们将介绍亚马逊 Athena，并展示如何利用 Athena 作为交互式查询服务来分析
    S3 中的数据，使用标准 SQL 而无需移动数据。在第一步中，我们将在 Athena 中注册我们 S3 存储桶中的 TSV 数据，然后对数据集运行一些即席查询。我们还将展示如何轻松将
    TSV 数据转换为更适合查询的列式文件格式 Apache Parquet。
- en: Our business intelligence team might also want to have a subset of the data
    in a data warehouse, which they can then transform and query with standard SQL
    clients to create reports and visualize trends. We will introduce Amazon Redshift,
    a fully managed data warehouse service, and show how to insert TSV data into Amazon
    Redshift, as well as combine the data warehouse queries with the less frequently
    accessed data that’s still in our S3 data lake via Amazon Redshift Spectrum. Our
    business intelligence team can also use Amazon Redshift’s data lake export functionality
    to unload (transformed, enriched) data back into our S3 data lake in Parquet file
    format.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的商业智能团队可能还希望在数据仓库中有一部分数据子集，然后可以使用标准 SQL 客户端进行转换和查询，以创建报告并可视化趋势。我们将介绍亚马逊 Redshift，一个完全托管的数据仓库服务，并展示如何将
    TSV 数据插入亚马逊 Redshift，以及如何通过亚马逊 Redshift Spectrum将数据仓库查询与仍在我们 S3 数据湖中的不经常访问的数据结合起来。我们的商业智能团队还可以使用亚马逊
    Redshift 的数据湖导出功能，将（转换、丰富的）数据以 Parquet 文件格式再次卸载到我们的 S3 数据湖中。
- en: We will conclude this chapter with some tips and tricks for increasing performance
    using compression algorithms and reducing cost by leveraging S3 Intelligent-Tiering.
    In [Chapter 12](ch12.html#secure_data_science_on_aws), we will dive deep into
    securing datasets, tracking data access, encrypting data at rest, and encrypting
    data in transit.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一些关于使用压缩算法增加性能和利用 S3 智能分层降低成本的技巧和窍门来结束本章。在[第 12 章](ch12.html#secure_data_science_on_aws)中，我们将深入探讨保护数据集、跟踪数据访问、数据静态加密和数据传输加密。
- en: Data Lakes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖
- en: In [Chapter 3](ch03.html#automated_machine_learnin), we discussed the democratization
    of artificial intelligence and data science over the last few years, the explosion
    of data, and how cloud services provide the infrastructure agility to store and
    process data of any amount.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 3 章](ch03.html#automated_machine_learnin)中，我们讨论了人工智能和数据科学在过去几年中的民主化、数据的爆炸增长，以及云服务如何提供基础设施的灵活性来存储和处理任意量的数据。
- en: Yet, in order to use all this data efficiently, companies are tasked to break
    down existing data silos and find ways to analyze very diverse datasets, dealing
    with both structured and unstructured data while ensuring the highest standards
    of data governance, data security, and compliance with privacy regulations. These
    (big) data challenges set the stage for data lakes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了有效利用所有这些数据，公司需要打破现有的数据孤岛，并找到分析非常多样化数据集的方法，处理结构化和非结构化数据，同时确保符合数据治理、数据安全和隐私法规的最高标准。这些（大）数据挑战为数据湖奠定了基础。
- en: One of the biggest advantages of data lakes is that we don’t need to predefine
    any schemas. We can store our raw data at scale and then decide later in which
    ways we need to process and analyze it. Data lakes may contain structured, semistructured,
    and unstructured data. [Figure 4-2](#a_data_lake_is_a_centralized_and_secure)
    shows the centralized and secure data lake repository that enables us to store,
    govern, discover, and share data at any scale—even in real time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖最大的优势之一是我们无需预定义任何模式。我们可以按规模存储原始数据，然后稍后决定需要以哪些方式处理和分析它。数据湖可以包含结构化、半结构化和非结构化数据。[图 4-2](#a_data_lake_is_a_centralized_and_secure)显示了集中且安全的数据湖仓库，使我们能够以任意规模（甚至实时）存储、治理、发现和共享数据。
- en: '![](assets/dsaw_0402.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0402.png)'
- en: Figure 4-2\. A data lake is a centralized and secure repository that enables
    us to store, govern, discover, and share data at any scale.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 数据湖是一个集中且安全的仓库，使我们能够以任意规模存储、治理、发现和共享数据。
- en: 'Data lakes provide a perfect base for data science and machine learning, as
    they give us access to large and diverse datasets to train and deploy more accurate
    models. Building a data lake typically consists of the following (high-level)
    steps, as shown in [Figure 4-3](#building_a_data_lake_involves_many_step):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖为数据科学和机器学习提供了完美的基础，因为它们使我们能够访问大量丰富多样的数据集，从而训练和部署更加精确的模型。构建数据湖通常包括以下（高级别）步骤，如[图 4-3](#building_a_data_lake_involves_many_step)所示：
- en: Set up storage.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置存储。
- en: Move data.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动数据。
- en: Cleanse, prepare, and catalog data.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清洗、准备和编目数据。
- en: Configure and enforce security and compliance policies.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置和执行安全和合规策略。
- en: Make data available for analytics.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使数据可用于分析。
- en: Each of those steps involves a range of tools and technologies. While we can
    build a data lake manually from the ground up, there are cloud services available
    to help us streamline this process, i.e., AWS Lake Formation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤涉及一系列工具和技术。虽然我们可以从零开始手动构建数据湖，但云服务可帮助我们简化此过程，例如AWS Lake Formation。
- en: '![](assets/dsaw_0403.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0403.png)'
- en: Figure 4-3\. Building a data lake involves many steps.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 构建数据湖涉及许多步骤。
- en: '[Lake Formation](https://oreil.ly/5HBtg) collects and catalogs data from databases
    and object storage, moves data into an S3-based data lake, secures access to sensitive
    data, and deduplicates data using machine learning.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lake Formation](https://oreil.ly/5HBtg)从数据库和对象存储中收集和编目数据，将数据移入基于S3的数据湖，保障对敏感数据的访问，并利用机器学习去重数据。'
- en: Additional capabilities of Lake Formation include row-level security, column-level
    security, and “governed” tables that support atomic, consistent, isolated, and
    durable transactions. With row-level and column-level permissions, users only
    see the data to which they have access. With Lake Formation transactions, users
    can concurrently and reliably insert, delete, and modify rows across the governed
    tables. Lake Formation also improves query performance by automatically compacting
    data storage and optimizing the data layout of governed tables.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Lake Formation的附加功能包括行级安全性、列级安全性和支持原子、一致、隔离和持久事务的“治理”表。通过行级和列级权限，用户只能看到他们有权限访问的数据。使用Lake
    Formation事务，用户可以并发和可靠地在治理表中插入、删除和修改行。Lake Formation还通过自动压缩数据存储和优化治理表的数据布局来提高查询性能。
- en: S3 has become a popular choice for data lakes, as it offers many ways to ingest
    our data while enabling cost optimization with intelligent tiering of data, including
    cold storage and archiving capabilities. S3 also exposes many object-level controls
    for security and compliance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: S3已成为数据湖的热门选择，因为它提供了多种方式来摄取我们的数据，并通过智能数据分层（包括冷存储和存档能力）实现成本优化。S3还为安全和合规性提供了许多对象级别的控制。
- en: On top of the S3 data lake, AWS implements the Lake House Architecture. The
    Lake House Architecture integrates our S3 data lake with our Amazon Redshift data
    warehouse for a unified governance model. We will see an example of this architecture
    in this chapter when we run a query joining data across our Amazon Redshift data
    warehouse with our S3 data lake.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在S3数据湖之上，AWS实施了Lake House架构。Lake House架构将我们的S3数据湖与Amazon Redshift数据仓库集成，形成统一的治理模型。在本章中，当我们运行跨Amazon
    Redshift数据仓库和S3数据湖的数据联接查询时，我们将看到这种架构的一个示例。
- en: From a data analysis perspective, another key benefit of storing our data in
    Amazon S3 is that it shortens the “time to insight” dramatically as we can run
    ad hoc queries directly on the data in S3\. We don’t have to go through complex
    transformation processes and data pipelines to get our data into traditional enterprise
    data warehouses, as we will see in the upcoming sections of this chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据分析的角度来看，将数据存储在亚马逊S3中的另一个关键好处是显著缩短了“洞察时间”，因为我们可以直接在S3中对数据运行特定查询。我们不需要通过复杂的转换流程和数据管道将数据导入传统的企业数据仓库，正如我们将在本章后面的部分中看到的那样。
- en: Import Data into the S3 Data Lake
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据导入S3数据湖
- en: We are now ready to import our data into S3\. We have chosen the [Amazon Customer
    Reviews Dataset](https://oreil.ly/jvgLz) as the primary dataset for this book.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好将数据导入S3了。我们选择了[亚马逊客户评论数据集](https://oreil.ly/jvgLz)作为本书的主要数据集。
- en: The Amazon Customer Reviews Dataset consists of more than 150+ million customer
    reviews of products across 43 different product categories on the Amazon.com website
    from 1995 until 2015\. It is a great resource for demonstrating machine learning
    concepts such as natural language processing (NLP), as we demonstrate throughout
    this book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊客户评论数据集包含了从1995年到2015年在亚马逊网站上涵盖43个不同产品类别的1.5亿多条客户评论。它是展示诸如自然语言处理（NLP）等机器学习概念的重要资源，正如本书中我们所展示的那样。
- en: Many of us have seen these customer reviews on Amazon.com when contemplating
    whether to purchase products via the Amazon.com marketplace. [Figure 4-4](#the_product_reviews_section_on_the_amaz)
    shows the product reviews section on Amazon.com for an Amazon Echo Dot device.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人在考虑是否通过亚马逊市场购买产品时，都看过亚马逊网站上的客户评论。[图 4-4](#the_product_reviews_section_on_the_amaz)显示了亚马逊Echo
    Dot设备的产品评论部分。
- en: '![](assets/dsaw_0404.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0404.png)'
- en: 'Figure 4-4\. Reviews for an Amazon Echo Dot device. Source: Amazon.com.'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4。亚马逊Echo Dot设备的评论。来源：Amazon.com。
- en: Describe the Dataset
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述数据集
- en: 'Customer reviews are one of Amazon’s most valuable tools for customers looking
    to make informed purchase decisions. In Amazon’s annual shareholder letters, Jeff
    Bezos (founder of Amazon) regularly elaborates on the importance of “word of mouth”
    as a customer acquisition tool. Jeff loves “customers’ constant discontent,” as
    he calls it:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 客户评论是亚马逊对于希望做出明智购买决策的客户而言最宝贵的工具之一。在亚马逊的年度股东信中，亚马逊创始人杰夫·贝索斯经常详细阐述了“口碑”的重要性作为客户获取工具。“杰夫热爱‘客户的持续不满’，正如他所称：
- en: “We now offer customers…vastly more reviews, content, browsing options, and
    recommendation features…Word of mouth remains the most powerful customer acquisition
    tool we have, and we are grateful for the trust our customers have placed in us.
    Repeat purchases and word of mouth have combined to make Amazon.com the market
    leader in online bookselling.”
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “现在我们为客户提供…更多的评论、内容、浏览选项和推荐功能…口碑仍然是我们拥有的最强大的客户获取工具，我们感谢客户对我们的信任。重复购买和口碑的结合使得亚马逊成为在线图书销售的市场领导者。”
- en: ''
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: –Jeff Bezos, [1997 Shareholder (“Share Owner”) Letter](https://oreil.ly/mj8M0)
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —杰夫·贝索斯，[1997年股东（“股东”）信](https://oreil.ly/mj8M0)
- en: 'Here is the schema for the dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集的架构：
- en: marketplace
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: marketplace
- en: Two-letter country code (in this case all “US”).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 两位字母国家代码（在本例中全部为“US”）。
- en: customer_id
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: customer_id
- en: Random identifier that can be used to aggregate reviews written by a single
    author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于汇总由单个作者撰写的评论的随机标识符。
- en: review_id
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: review_id
- en: A unique ID for the review.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 评论的唯一ID。
- en: product_id
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: product_id
- en: The Amazon Standard Identification Number (ASIN).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊标准识别号（ASIN）。
- en: product_parent
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: product_parent
- en: The parent of that ASIN. Multiple ASINs (color or format variations of the same
    product) can roll up into a single product parent.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该ASIN的父级。多个ASIN（同一产品的颜色或格式变体）可以汇总为一个产品父级。
- en: product_title
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: product_title
- en: Title description of the product.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 产品的标题描述。
- en: product_category
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: product_category
- en: Broad product category that can be used to group reviews.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用来分组评论的广泛产品类别。
- en: star_rating
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 星级评分
- en: The review’s rating of 1 to 5 stars, where 1 is the worst and 5 is the best.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对评论的评分为 1 到 5 星，其中 1 是最差，5 是最好。
- en: helpful_votes
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有用票数
- en: Number of helpful votes for the review.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 评论有用票数。
- en: total_votes
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总票数
- en: Number of total votes the review received.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 评论接收到的总票数。
- en: vine
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Vine
- en: Was the review written as part of the Vine program?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 评论是否作为 Vine 计划的一部分撰写？
- en: verified_purchase
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 经过验证的购买
- en: Was the review from a verified purchase?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 评论是否来自经过验证的购买？
- en: review_headline
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 评论标题
- en: The title of the review itself.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 评论本身的标题。
- en: review_body
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 评论正文
- en: The text of the review.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 评论的标题。
- en: review_date
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 评论日期
- en: The date the review was written.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 评论撰写日期。
- en: 'The dataset is shared in a public Amazon S3 bucket and is available in two
    file formats:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在公共亚马逊 S3 存储桶中共享，并提供两种文件格式：
- en: 'TSV, a text format: *s3://amazon-reviews-pds/tsv*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TSV，文本格式：*s3://amazon-reviews-pds/tsv*
- en: 'Parquet, an optimized columnar binary format: *s3://amazon-reviews-pds/parquet*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet，一种优化的列式二进制格式：*s3://amazon-reviews-pds/parquet*
- en: The Parquet dataset is partitioned (divided into subfolders) by the column `product_category`
    to further improve query performance. With this, we can use a `WHERE` clause on
    `product_category` in our SQL queries to only read data specific to that category.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 数据集按 `product_category` 列进行分区（分成子文件夹），以进一步提高查询性能。通过这种方式，我们可以在 SQL 查询中使用
    `WHERE` 子句来只读取特定类别的数据。
- en: 'We can use the AWS Command Line Interface (AWS CLI) to list the S3 bucket content
    using the following CLI commands:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 AWS 命令行界面（AWS CLI）来列出 S3 存储桶内容，具体命令如下：
- en: '`aws s3 ls s3://amazon-reviews-pds/tsv`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aws s3 ls s3://amazon-reviews-pds/tsv`'
- en: '`aws s3 ls s3://amazon-reviews-pds/parquet`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aws s3 ls s3://amazon-reviews-pds/parquet`'
- en: Note
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The AWS CLI tool provides a unified command line interface to Amazon Web Services.
    We can find more information on [how to install and configure the tool](https://aws.amazon.com/cli).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: AWS CLI 工具提供了统一的命令行界面，用于 Amazon Web Services。我们可以在 [如何安装和配置该工具](https://aws.amazon.com/cli)
    找到更多信息。
- en: The following listings show us the available dataset files in TSV format and
    the Parquet partitioning folder structure.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了 TSV 格式和 Parquet 分区文件夹结构中的可用数据集文件。
- en: 'Dataset files in TSV format:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: TSV 格式的数据集文件：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Dataset files in Parquet format:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 格式的数据集文件：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that `PRE` stands for “prefix.” For now, we can think of prefixes as folders
    in S3.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`PRE` 代表“前缀”。目前，我们可以将前缀视为 S3 中的文件夹。
- en: It is sometimes useful to use `EXPLAIN` in our queries to make sure the S3 partitions
    are being utilized. Spark, for example, will highlight which partitions are being
    used in Spark SQL. If our query patterns change over time, we may want to revisit
    updating the existing partitions—or even adding new partitions to match our business
    needs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询中有时使用 `EXPLAIN` 是很有用的，以确保 S3 分区被充分利用。例如，Spark 将突出显示在 Spark SQL 中使用的分区。如果我们的查询模式随时间改变，可能需要重新审视更新现有分区——甚至添加新分区以满足业务需求。
- en: So which data format should we choose? The Parquet columnar file format is definitely
    preferred when running analytics queries since many analytics queries perform
    summary statistics (`AVG`, `SUM`, `STDDEV`, etc.) on columns of data. On the other
    hand, many applications write out data in simple CSV or TSV files, e.g., application
    log files. So let’s actually assume we don’t have the Parquet files ready to use
    as this allows us to show us how we can easily get there from CSV or TSV files.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们应该选择哪种数据格式呢？Parquet 列式文件格式在运行分析查询时绝对是首选，因为许多分析查询在数据列上执行汇总统计（`AVG`、`SUM`、`STDDEV`
    等）。另一方面，许多应用程序将数据写入简单的 CSV 或 TSV 文件中，例如应用程序日志文件。因此，我们假设暂时没有准备好使用 Parquet 文件，这样可以展示如何从
    CSV 或 TSV 文件轻松转换到 Parquet 文件。
- en: In a first step, let’s copy the TSV data from Amazon’s public S3 bucket into
    a privately hosted S3 bucket to simulate that process, as shown in [Figure 4-5](#we_copy_the_dataset_from_the_public_sth).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将来自亚马逊公共 S3 存储桶的 TSV 数据复制到私人托管的 S3 存储桶中，以模拟该过程，如 [图 4-5](#we_copy_the_dataset_from_the_public_sth)
    所示。
- en: '![](assets/dsaw_0405.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0405.png)'
- en: Figure 4-5\. We copy the dataset from the public S3 bucket to a private S3 bucket.
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 我们将数据集从公共 S3 存储桶复制到私有 S3 存储桶。
- en: We can use the AWS CLI tool again to perform the following steps.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用 AWS CLI 工具执行以下步骤。
- en: 'Create a new private S3 bucket:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建新的私有 S3 存储桶：
- en: '`aws s3 mb s3://data-science-on-aws`'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`aws s3 mb s3://data-science-on-aws`'
- en: 'Copy the content of the public S3 bucket to our newly created private S3 bucket
    as follows (only include the files starting with `amazon_reviews_us_`, i.e., skipping
    any index, multilingual, and sample data files in that directory):'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将公共 S3 存储桶的内容复制到我们新创建的私有 S3 存储桶，操作如下（仅包括以`amazon_reviews_us_`开头的文件，即跳过该目录中的任何索引、多语言和样本数据文件）：
- en: '[PRE2]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are now ready to use Amazon Athena to register and query the data and transform
    the TSV files into Parquet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 Amazon Athena 注册和查询数据，并将 TSV 文件转换为 Parquet。
- en: Query the Amazon S3 Data Lake with Amazon Athena
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon Athena 查询 Amazon S3 数据湖
- en: Amazon Athena is an interactive query service that makes it easy to analyze
    data in Amazon S3 using standard SQL. With Athena, we can query raw data—including
    encrypted data—directly from our S3-based data lake. Athena separates compute
    from storage and lowers the overall time to insight for our business. When we
    register an Athena table with our S3 data, Athena stores the table-to-S3 mapping.
    Athena uses the AWS Glue Data Catalog, a Hive Metastore–compatible service, to
    store the table-to-S3 mapping. We can think of the AWS Glue Data Catalog as a
    persistent metadata store in our AWS account. Other AWS services, such as Athena
    and Amazon Redshift Spectrum, can use the data catalog to locate and query data.
    Apache Spark reads from the AWS Glue Data Catalog, as well.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Athena 是一个交互式查询服务，可以使用标准 SQL 轻松分析 Amazon S3 中的数据。通过 Athena，我们可以直接从基于
    S3 的数据湖中查询原始数据，包括加密数据。Athena 将计算与存储分离，并降低我们业务的整体洞察时间。当我们使用 Athena 将 Athena 表注册到我们的
    S3 数据时，Athena 存储表与 S3 映射。Athena 使用 AWS Glue 数据目录作为 Hive Metastore 兼容的服务来存储表与 S3
    的映射。我们可以将 AWS Glue 数据目录视为 AWS 帐户中的持久性元数据存储。其他 AWS 服务，如 Athena 和 Amazon Redshift
    Spectrum，可以使用数据目录定位和查询数据。Apache Spark 也可以从 AWS Glue 数据目录中读取数据。
- en: Besides the data catalog, AWS Glue also provides tools to build ETL (extract-transform-load)
    workflows. ETL workflows could include the automatic discovery and extraction
    of data from different sources. We can leverage Glue Studio to visually compose
    and run ETL workflows without writing code. Glue Studio also provides the single
    pane of glass to monitor all ETL jobs. AWS Glue executes the workflows on an Apache
    Spark–based serverless ETL engine.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据目录外，AWS Glue 还提供了构建 ETL（抽取-转换-加载）工作流的工具。ETL 工作流可能包括从不同来源自动发现和提取数据。我们可以利用
    Glue Studio 在可视化环境中组合和运行 ETL 工作流，而无需编写代码。Glue Studio 还提供了监视所有 ETL 作业的单一窗口。AWS
    Glue 在基于 Apache Spark 的无服务器 ETL 引擎上执行工作流。
- en: Athena queries run in parallel inside a dynamically scaled, serverless query
    engine. Athena will automatically scale the cluster depending on the query and
    dataset involved. This makes Athena extremely fast on large datasets and frees
    the user from worrying about infrastructure details.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Athena 查询在一个动态缩放、无服务器查询引擎内并行运行。Athena 会根据所涉及的查询和数据集自动扩展集群。这使得 Athena 在处理大数据集时非常快速，并且使用户不必担心基础设施的细节。
- en: In addition, Athena supports the Parquet columnar file format with tens of millions
    of partitions (i.e., by `product_category`, `year`, or `marketplace`) to improve
    the performance of our queries. For example, if we plan to run frequent queries
    that group the results by `product_category`, then we should create a partition
    in Athena for `product_category`. Upon creation, Athena will update the AWS Glue
    Data Catalog accordingly so that future queries will inherit the performance benefits
    of this new partition.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Athena 支持 Parquet 列式文件格式，并支持数千万个分区（例如按`product_category`、`year`或`marketplace`）。以提高查询性能。例如，如果我们计划频繁查询并按`product_category`分组结果，则应在
    Athena 中为`product_category`创建一个分区。创建后，Athena 将相应更新 AWS Glue 数据目录，以便将来的查询可以继承这个新分区的性能优势。
- en: Athena is based on Presto, an open source, distributed SQL query engine designed
    for fast, ad hoc data analytics on large datasets. Similar to Apache Spark, [Presto](https://oreil.ly/WKcDS)
    uses high-RAM clusters to perform its queries. However, Presto does not require
    a large amount of disk as it is designed for ad hoc queries (versus automated,
    repeatable queries) and therefore does not perform the checkpointing required
    for fault tolerance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Athena 基于 Presto，这是一个开源的分布式 SQL 查询引擎，专为在大数据集上进行快速、即席数据分析而设计。类似于 Apache Spark，[Presto](https://oreil.ly/WKcDS)
    使用高 RAM 集群执行查询。但 Presto 不需要大量磁盘，因为它设计用于即席查询（而不是自动化、可重复的查询），因此不执行需要容错的检查点操作。
- en: For longer-running Athena jobs, we can listen for query-completion events using
    Amazon CloudWatch Events. When the query completes, all listeners are notified
    with the event details, including query success status, total execution time,
    and total bytes scanned.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于运行时间较长的 Athena 作业，我们可以使用 Amazon CloudWatch Events 监听查询完成事件。查询完成后，所有监听器都会收到事件详细信息，包括查询成功状态、总执行时间和扫描的总字节数。
- en: With a functionality called *Athena Federated Query*, we can also run SQL queries
    across data stored in relational databases, such as Amazon RDS and Aurora, nonrelational
    databases such as DynamoDB, object storage such as Amazon S3, and custom data
    sources. This gives us a unified analytics view across data stored in our data
    warehouse, data lake, and operational databases without the need to actually move
    the data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用名为 *Athena Federated Query* 的功能，我们还可以在存储在关系数据库（例如 Amazon RDS 和 Aurora）、非关系数据库（例如
    DynamoDB）、对象存储（例如 Amazon S3）以及自定义数据源中的数据上运行 SQL 查询。这使我们可以在不实际移动数据的情况下，通过统一的分析视图访问我们的数据仓库、数据湖和操作数据库中存储的数据。
- en: We can access Athena via the AWS Management Console, an API, or an Open Database
    Connectivity (ODBC) or Java Database Connectivity (JDBC) driver for programmatic
    access. Let’s take a look at how to use Amazon Athena via the AWS Management Console.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 AWS 管理控制台、API，或者通过 Open Database Connectivity (ODBC) 或 Java Database
    Connectivity (JDBC) 驱动程序进行编程访问 Athena。让我们看看如何通过 AWS 管理控制台使用 Amazon Athena。
- en: Access Athena from the AWS Console
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 AWS 控制台访问 Athena
- en: To use Amazon Athena, we first need to quickly set up the service. First, click
    on Amazon Athena in the AWS Management Console. If we are asked to set up a “query
    result” location for Athena in S3, specify an S3 location for the query results
    (e.g., *s3://<BUCKET>/data-science-on-aws/athena/query-results*.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Amazon Athena，我们首先需要快速设置该服务。首先，在 AWS 管理控制台中点击 Amazon Athena。如果我们被要求为 Athena
    在 S3 中的“查询结果”位置设置位置，请指定一个 S3 位置作为查询结果的位置（例如 *s3://<BUCKET>/data-science-on-aws/athena/query-results*）。
- en: 'In the next step, we create a database. In the Athena Query Editor, we see
    a query pane with an example query. We can start typing our query anywhere in
    the query pane. To create our database, enter the following `CREATE DATABASE`
    statement, run the query, and confirm that `dsoaws` appears in the DATABASE list
    in the Catalog dashboard:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们创建一个数据库。在 Athena 查询编辑器中，我们看到一个带有示例查询的查询面板。我们可以在查询面板的任何位置开始输入我们的查询。要创建我们的数据库，请输入以下`CREATE
    DATABASE`语句，运行查询，并确认`dsoaws`出现在目录仪表板的数据库列表中：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When we run `CREATE DATABASE` and `CREATE TABLE` queries in Athena with the
    AWS Glue Data Catalog as our source, we automatically see the database and table
    metadata entries being created in the AWS Glue Data Catalog.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 Athena 中使用 AWS Glue 数据目录作为源运行 `CREATE DATABASE` 和 `CREATE TABLE` 查询时，我们会自动看到在
    AWS Glue 数据目录中创建的数据库和表元数据条目。
- en: Register S3 Data as an Athena Table
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注册 S3 数据作为 Athena 表
- en: Now that we have a database, we are ready to create a table based on the Amazon
    Customer Reviews Dataset. We define the columns that map to the data, specify
    how the data is delimited, and provide the Amazon S3 path to the data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个数据库，可以基于亚马逊客户评论数据集创建表。我们定义映射到数据的列，指定数据的分隔方式，并提供数据的 Amazon S3 路径。
- en: 'Let’s define a “schema-on-read” to avoid the need to predefine a rigid schema
    when data is written and ingested. In the Athena Console, make sure that `dsoaws`
    is selected for DATABASE and then choose New Query. Run the following SQL statement
    to read the compressed (`compression=gzip`) files and skip the CSV header (`skip.header.line.count=1`)
    at the top of each file. After running the SQL statement, verify that the newly
    created table, `amazon_reviews_tsv`, appears on the left under Tables:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个“基于读取的模式”，避免在数据写入和摄入时需要预定义严格的模式。在 Athena 控制台中，确保选择了`dsoaws`作为数据库，然后选择新查询。运行以下
    SQL 语句来读取压缩（`compression=gzip`）文件，并跳过每个文件顶部的 CSV 标头（`skip.header.line.count=1`）。运行
    SQL 语句后，请验证新创建的表`amazon_reviews_tsv`在左侧的表下是否出现：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s run a sample query like this to check if everything works correctly.
    This query will produce the results shown in the following table:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个示例查询，以检查一切是否正常工作。此查询将生成如下表格中显示的结果：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '| marketplace | customer_id | review_id | product_id | product_title | product_category
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| marketplace | customer_id | review_id | product_id | product_title | product_category
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| US | 12190288 | R3FBDHSJD | BOOAYB23D | Enlightened | Digital_Video_Download
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| US | 12190288 | R3FBDHSJD | BOOAYB23D | Enlightened | Digital_Video_Download
    |'
- en: '| ... | ... | ... | ... | ... | ... |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... |'
- en: Update Athena Tables as New Data Arrives with AWS Glue Crawler
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS Glue Crawler 更新 Athena 表以处理新数据到达
- en: 'The following code crawls S3 every night at 23:59 UTC and updates the Athena
    table as new data arrives. If we add another *.tar.gz* file to S3, for example,
    we will see the new data in our Athena queries after the crawler completes its
    scheduled run:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码每天在 UTC 时间 23:59 自动爬取 S3，并在新数据到达时更新 Athena 表。例如，如果我们向 S3 添加另一个 *.tar.gz*
    文件，我们将在爬虫完成预定运行后在 Athena 查询中看到新数据：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create a Parquet-Based Table in Athena
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Athena 中创建基于 Parquet 的表
- en: In a next step, we will show how we can easily convert that data into the Apache
    Parquet columnar file format to improve query performance. Parquet is optimized
    for columnar-based queries such as counts, sums, averages, and other aggregation-based
    summary statistics that focus on the column values versus row information.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示如何轻松地将数据转换为 Apache Parquet 列式文件格式以提高查询性能。Parquet 针对基于列的查询进行了优化，如计数、求和、平均值和其他基于列的汇总统计，重点放在列值而不是行信息上。
- en: By storing our data in columnar format, Parquet performs sequential reads for
    columnar summary statistics. This results in much more efficient data access and
    “mechanical sympathy” versus having the disk controller jump from row to row and
    having to reseek to retrieve the column data. If we are doing any type of large-scale
    data analytics, we should be using a columnar file format like Parquet. We discuss
    the benefits of Parquet in the performance section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以列式格式存储我们的数据，Parquet 可以执行针对列的顺序读取以进行列式汇总统计。这使得数据访问更加高效，并且在性能上表现更好，与磁盘控制器从行到行跳转并重新查找以检索列数据相比，有着“机械同情心”。如果我们进行任何类型的大规模数据分析，我们应该使用像
    Parquet 这样的列式文件格式。我们在性能部分讨论了 Parquet 的好处。
- en: Note
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While we already have the data in Parquet format from the public dataset, we
    feel that creating a Parquet table is an important-enough topic to demonstrate
    in this book.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经从公共数据集中以 Parquet 格式获取了数据，但我们认为创建一个 Parquet 表是一个足够重要的主题，可以在本书中进行演示。
- en: 'Again, make sure that `dsoaws` is selected for DATABASE and then choose New
    Query and run the following CREATE TABLE AS (CTAS) SQL statement:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 再次确保选择 DATABASE 中的`dsoaws`，然后选择 New Query 并运行以下 CREATE TABLE AS (CTAS) SQL 语句：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we can see from the query, we’re also adding a new `year` column to our
    dataset by converting the `review_date` string to a date format and then casting
    the year out of the date. Let’s store the year value as an integer. After running
    the CTAS query, we should now see the newly created table `amazon_reviews_parquet`
    appear as well on the left under Tables. As a last step, we need to load the Parquet
    partitions. To do so, just issue the following SQL command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如查询所示，我们还通过将`review_date`字符串转换为日期格式，然后将年份从日期中提取，向我们的数据集添加了一个新的`year`列。让我们将年份值存储为整数。运行完
    CTAS 查询后，我们现在应该可以在左侧的表下看到新创建的`amazon_reviews_parquet`表。作为最后一步，我们需要加载 Parquet 分区。为此，只需执行以下
    SQL 命令：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We can automate the `MSCK REPAIR TABLE` command to load the partitions after
    data ingest from any workflow manager (or use an Lambda function that runs when
    new data is uploaded to S3).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以自动化执行`MSCK REPAIR TABLE`命令，以在从任何工作流管理器中获取数据后加载分区（或使用 Lambda 函数，当新数据上传到 S3
    时运行）。
- en: 'We can run our sample query again to check if everything works correctly:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次运行我们的示例查询来检查一切是否正常：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Both tables also have metadata entries in the Hive Metastore–compatible AWS
    Glue Data Catalog. This metadata defines the schema used by many query and data-processing
    engines, such as Amazon EMR, Athena, Redshift, Kinesis, SageMaker, and Apache
    Spark.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 两个表还在 Hive Metastore 兼容的 AWS Glue 数据目录中有元数据条目。这些元数据定义了许多查询和数据处理引擎（如 Amazon EMR、Athena、Redshift、Kinesis、SageMaker
    和 Apache Spark）使用的模式。
- en: In just a few steps, we have set up Amazon Athena to transform the TSV dataset
    files into the Apache Parquet file format. The query on the Parquet files finished
    in a fraction of the time compared to the query on the TSV files. We accelerated
    our query response time by leveraging the columnar Parquet file format and `product_category`
    partition scheme.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几个步骤，我们就设置了 Amazon Athena，将 TSV 数据集文件转换为 Apache Parquet 文件格式。与对 TSV 文件的查询相比，Parquet
    文件的查询时间大大缩短。通过利用列式 Parquet 文件格式和`product_category`分区方案，我们加快了查询响应时间。
- en: Continuously Ingest New Data with AWS Glue Crawler
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS Glue Crawler 持续摄取新数据
- en: New data is always arriving from applications, and we need a way to register
    this new data into our system for analytics and model-training purposes. AWS Glue
    provides sophisticated data-cleansing and machine-learning transformations, including
    “fuzzy” record deduplication. One way to register the new data from S3 into our
    AWS Glue Data Catalog is with a Glue Crawler, as shown in [Figure 4-6](#ingest_and_register_data_from_various_d).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序始终在传送新数据，我们需要一种方法将这些新数据注册到我们的系统中进行分析和模型训练。AWS Glue 提供复杂的数据清洗和机器学习转换，包括“模糊”记录去重。将新数据从
    S3 注册到我们的 AWS Glue 数据目录的一种方式是使用 Glue 爬虫，如 [图 4-6](#ingest_and_register_data_from_various_d)
    所示。
- en: '![](assets/dsaw_0406.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0406.png)'
- en: Figure 4-6\. Ingest and register data from various data sources with AWS Glue
    Crawler.
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 使用 AWS Glue 爬虫从各种数据源摄取和注册数据。
- en: 'We can trigger the crawler either periodically on a schedule or with, for example,
    an S3 trigger. The following code creates the crawler and schedules new S3 folders
    (prefixes) to be ingested every night at 23:59 UTC:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按计划定期触发爬虫，也可以使用例如 S3 触发器。以下代码创建爬虫，并安排每天晚上 23:59 UTC 注入新的 S3 文件夹（前缀）：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This assumes that we are storing new data in new folders. Typically, we use
    an S3 prefix that includes the year, month, day, hour, quarter hour, etc. For
    example, we can store application logs in hourly S3 folders with the following
    naming convention for the S3 prefix: *s3://<S3_BUCKET>/<YEAR>/<MONTH>/<DAY>/<HOUR>/*.
    If we want to crawl all of the data, we can use `CRAWL_EVERYTHING` for our `RecrawlBehavior`.
    We can change the schedule using a different `cron()` trigger. We can also add
    a second trigger to start an ETL job to transform and load new data when the scheduled
    Glue Crawler reaches the `SUCCEEDED` state.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在将新数据存储在新文件夹中。通常，我们使用包括年、月、日、小时、刻钟等信息的 S3 前缀。例如，我们可以使用以下 S3 前缀命名约定将应用程序日志存储在每小时的
    S3 文件夹中：*s3://<S3_BUCKET>/<YEAR>/<MONTH>/<DAY>/<HOUR>/*。如果我们希望爬行所有数据，可以使用 `CRAWL_EVERYTHING`
    作为我们的 `RecrawlBehavior`。我们可以使用不同的 `cron()` 触发器更改计划。我们还可以添加第二个触发器，在调度的 Glue 爬虫达到
    `SUCCEEDED` 状态时启动 ETL 作业以转换和加载新数据。
- en: Build a Lake House with Amazon Redshift Spectrum
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用亚马逊 Redshift Spectrum 构建数据湖
- en: One of the fundamental differences between data lakes and data warehouses is
    that while we ingest and store huge amounts of raw, unprocessed data in our data
    lake, we normally only load some fraction of our recent data into the data warehouse.
    Depending on our business and analytics use case, this might be data from the
    past couple of months, a year, or maybe the past two years. Let’s assume we want
    to have the past two years of our Amazon Customer Reviews Dataset in a data warehouse
    to analyze year-over-year customer behavior and review trends. We will use Amazon
    Redshift as our data warehouse for this.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖和数据仓库之间的一个基本区别在于，虽然我们在数据湖中摄取和存储大量原始未加工的数据，但通常只将最近数据的一部分加载到数据仓库中。根据我们的业务和分析用例，这可能是过去几个月、一年或最近两年的数据。假设我们想在数据仓库中分析亚马逊客户评论数据集的过去两年数据以分析年度客户行为和评论趋势，我们将使用亚马逊
    Redshift 作为我们的数据仓库。
- en: Amazon Redshift is a fully managed data warehouse that allows us to run complex
    analytic queries against petabytes of structured data, semistructured, and JSON
    data. Our queries are distributed and parallelized across multiple nodes. In contrast
    to relational databases, which are optimized to store data in rows and mostly
    serve transactional applications, Amazon Redshift implements columnar data storage,
    which is optimized for analytical applications where we are mostly interested
    in the data within the individual columns.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 Redshift 是一个完全托管的数据仓库，允许我们针对千兆字节的结构化数据、半结构化数据和 JSON 数据运行复杂的分析查询。我们的查询分布和并行化在多个节点上执行。与关系数据库相比，后者优化了按行存储数据，并主要用于事务应用程序；亚马逊
    Redshift 实施了列存储，这对于分析应用程序而言更为优化，因为我们主要关注的是单个列内的数据。
- en: Amazon Redshift also includes Amazon Redshift Spectrum, which allows us to directly
    execute SQL queries from Amazon Redshift against exabytes of unstructured data
    in our Amazon S3 data lake without the need to physically move the data. Amazon
    Redshift Spectrum is part of the Lake House Architecture that unifies our S3 data
    lake and Amazon Redshift data warehouse—including shared security and row-and-column-based
    access control. Amazon Redshift Spectrum supports various open source storage
    frameworks, including Apache Hudi and Delta Lake.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift还包括Amazon Redshift Spectrum，允许我们直接从Amazon Redshift针对S3数据湖中的数百亿字节的非结构化数据执行SQL查询，而无需实际移动数据。Amazon
    Redshift Spectrum是Lake House Architecture的一部分，统一了我们的S3数据湖和Amazon Redshift数据仓库，包括共享安全性和基于行和列的访问控制。Amazon
    Redshift Spectrum支持各种开源存储框架，包括Apache Hudi和Delta Lake。
- en: Since Amazon Redshift Spectrum automatically scales the compute resources needed
    based on how much data is being retrieved, queries against Amazon S3 run fast,
    regardless of the size of our data. Amazon Redshift Spectrum will use pushdown
    filters, bloom filters, and materialized views to reduce seek time and increase
    query performance on external data stores like S3\. We discuss more performance
    tips later in [“Reduce Cost and Increase Performance”](#reduce_cost_and_increase_performanc).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Amazon Redshift Spectrum根据检索的数据量自动扩展所需的计算资源，因此针对Amazon S3的查询速度很快，而不管数据大小如何。Amazon
    Redshift Spectrum将使用推送过滤器、布隆过滤器和物化视图来减少搜索时间，并提高对S3等外部数据存储的查询性能。我们稍后在[“降低成本并提高性能”](#reduce_cost_and_increase_performanc)中讨论更多性能提示。
- en: Amazon Redshift Spectrum converts traditional ETL into extract-load-transform
    (ELT) by transforming and cleaning data after it is loaded into Amazon Redshift.
    We will use Amazon Redshift Spectrum to access our data in S3 and then show how
    to combine data that is stored in Amazon Redshift with data that is still in S3.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift Spectrum通过在加载到Amazon Redshift后转换和清理数据，将传统的ETL转换为提取-加载-转换（ELT）。我们将使用Amazon
    Redshift Spectrum访问我们在S3中的数据，然后展示如何将存储在Amazon Redshift中的数据与仍在S3中的数据结合起来。
- en: This might sound similar to the approach we showed earlier with Amazon Athena,
    but note that in this case we show how our business intelligence team can enrich
    their queries with data that is not stored in the data warehouse itself. Once
    we have our Redshift cluster set up and configured, we can navigate to the AWS
    Console and Amazon Redshift and then click on Query Editor to execute commands.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来可能与我们早些时候展示的Amazon Athena的方法相似，但请注意，在这种情况下，我们展示了我们的业务智能团队如何在其查询中使用未存储在数据仓库本身中的数据进行丰富。一旦我们设置和配置了Redshift集群，我们可以导航到AWS控制台和Amazon
    Redshift，然后点击查询编辑器来执行命令。
- en: We can leverage our previously created table in Amazon Athena with its metadata
    and schema information stored in the AWS Glue Data Catalog to access our data
    in S3 through Amazon Redshift Spectrum. All we need to do is create an external
    schema in Amazon Redshift, point it to our AWS Glue Data Catalog, and point Amazon
    Redshift to the database we’ve created.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用在AWS Glue Data Catalog中存储的Amazon Athena的表格及其元数据和模式信息来通过Amazon Redshift
    Spectrum访问我们在S3中的数据。我们所需做的就是在Amazon Redshift中创建一个外部模式，将其指向AWS Glue Data Catalog，并指向我们创建的数据库。
- en: 'In the Amazon Redshift Query Editor (or via any other ODBC/JDBC SQL client
    that we might prefer to use), execute the following command:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon Redshift查询编辑器（或通过我们可能更喜欢使用的任何其他ODBC/JDBC SQL客户端）中执行以下命令：
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With this command, we are creating a new schema in Amazon Redshift called `athena`
    to highlight the data access we set up through our tables in Amazon Athena:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此命令，我们在Amazon Redshift中创建一个名为`athena`的新模式，以突出我们通过Amazon Athena设置的表格中的数据访问：
- en: '`FROM DATA CATALOG` indicates that the external database is defined in the
    AWS Glue Data Catalog.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FROM DATA CATALOG`表示外部数据库在AWS Glue Data Catalog中定义。'
- en: '`DATABASE` refers to our previously created database in the AWS Glue Data Catalog.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DATABASE`指的是在AWS Glue Data Catalog中之前创建的数据库。'
- en: '`IAM_ROLE` needs to point to an Amazon Resource Name (ARN) for an IAM role
    that our cluster uses for authentication and authorization.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IAM_ROLE`需要指向用于集群身份验证和授权的IAM角色的Amazon资源名称（ARN）。'
- en: IAM is the AWS Identity and Access Management service, which enables us to manage
    and control access to AWS services and resources in our account. With an IAM role,
    we can specify the permissions a user or service is granted. In this example,
    the IAM role must have at a minimum permission to perform a `LIST` operation on
    the Amazon S3 bucket to be accessed and a `GET` operation on the Amazon S3 objects
    the bucket contains. If the external database is defined in an Amazon Athena data
    catalog, the IAM role must have permission to access Athena unless `CATALOG_ROLE`
    is specified. We will go into more details on IAM in a later section of this chapter
    when we discuss how we can secure our data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: IAM是AWS身份和访问管理服务，它使我们能够管理和控制访问我们帐户中的AWS服务和资源。通过IAM角色，我们可以指定用户或服务被授予的权限。在本例中，IAM角色必须至少具有执行Amazon
    S3存储桶上的`LIST`操作和Amazon S3对象上的`GET`操作的权限。如果外部数据库在Amazon Athena数据目录中定义，IAM角色必须具有访问Athena的权限，除非指定了`CATALOG_ROLE`。在本章后面的部分中，当我们讨论如何保护我们的数据时，我们将详细介绍IAM。
- en: 'If we now select `athena` in the Schema dropdown menu in the Amazon Redshift
    Query Editor, we can see that our two tables, `amazon_reviews_tsv` and `amazon_reviews_parquet`,
    appear, which we created with Amazon Athena. Let’s run a sample query again to
    make sure everything works. In the Query Editor, run the following command:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在在Amazon Redshift查询编辑器的模式下拉菜单中选择`athena`，我们可以看到我们使用Amazon Athena创建的两个表，`amazon_reviews_tsv`和`amazon_reviews_parquet`。让我们再次运行一个示例查询来确保一切正常。在查询编辑器中，运行以下命令：
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We should see results similar to the following table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似下表的结果：
- en: '| product_category | count_star_rating |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| product_category | count_star_rating |'
- en: '| --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Books | 19531329 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Books | 19531329 |'
- en: '| Digital_Ebook_Purchase | 17622415 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Digital_Ebook_Purchase | 17622415 |'
- en: '| Wireless | 9002021 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Wireless | 9002021 |'
- en: '| ... | ... |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... |'
- en: So with just one command, we now have access and can query our S3 data lake
    from Amazon Redshift without moving any data into our data warehouse. This is
    the power of Amazon Redshift Spectrum.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，只需一条命令，我们现在可以访问并从Amazon Redshift中的S3数据湖查询数据，而无需将任何数据移动到我们的数据仓库中。这就是Amazon
    Redshift Spectrum的威力。
- en: But now, let’s actually copy some data from S3 into Amazon Redshift. Let’s pull
    in customer reviews data from the year 2015.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实际从S3复制一些数据到Amazon Redshift。让我们拉取2015年的客户评论数据。
- en: 'First, we create another Amazon Redshift schema called `redshift` with the
    following SQL command:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用以下SQL命令创建另一个Amazon Redshift模式称为`redshift`：
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will create a new table that represents our customer reviews data.
    We will also add a new column and add `year` to our table:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个新的表来表示我们的客户评论数据。我们还将添加一个新列，并将`year`添加到我们的表中：
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the performance section, we will dive deep into the `SORTKEY`, `DISTKEY`,
    and `ENCODE` attributes. For now, let’s copy the data from S3 into our new Amazon
    Redshift table and run some sample queries.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能部分，我们将深入探讨`SORTKEY`、`DISTKEY`和`ENCODE`属性。现在，让我们从S3复制数据到我们的新Amazon Redshift表，并运行一些示例查询。
- en: For such bulk inserts, we can either use a `COPY` command or an `INSERT INTO`
    command. In general, the `COPY` command is preferred, as it loads data in parallel
    and more efficiently from Amazon S3, or other supported data sources.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的批量插入，我们可以使用`COPY`命令或`INSERT INTO`命令。一般来说，`COPY`命令更受青睐，因为它可以并行加载数据，效率更高，可以从Amazon
    S3或其他支持的数据源加载数据。
- en: 'If we are loading data or a subset of data from one table into another, we
    can use the `INSERT INTO` command with a `SELECT` clause for high-performance
    data insertion. As we’re loading our data from the `athena.amazon_reviews_tsv`
    table, let’s choose this option:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从一个表加载数据或数据子集到另一个表中，我们可以使用`INSERT INTO`命令和带有`SELECT`子句的高性能数据插入。因为我们正在从`athena.amazon_reviews_tsv`表加载数据，所以让我们选择这个选项：
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We use a date conversion to parse the year out of our `review_date` column and
    store it in a separate `year` column, which we then use to filter records from
    2015\. This is an example of how we can simplify ETL tasks, as we put our data
    transformation logic directly in a `SELECT` query and ingest the result into Amazon
    Redshift.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用日期转换来从我们的`review_date`列中解析年份，并将其存储在一个单独的`year`列中，然后我们使用它来过滤2015年的记录。这是一个简化ETL任务的示例，因为我们将数据转换逻辑直接放在`SELECT`查询中，并将结果摄入到Amazon
    Redshift中。
- en: Another way to optimize our tables would be to create them as a sequence of
    time-series tables, especially when our data has a fixed retention period. Let’s
    say we want to store data of the last two years (24 months) in our data warehouse
    and update with new data once a month.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种优化表格的方法是将它们创建为一系列时间序列表，特别是当我们的数据具有固定的保留期限时。假设我们希望将最近两年（24个月）的数据存储在我们的数据仓库中，并每月更新新数据。
- en: If we create one table per month, we can easily remove old data by running a
    `DROP TABLE` command on the corresponding table. This approach is much faster
    than running a large-scale `DELETE` process and also saves us from having to run
    a subsequent `VACUUM` process to reclaim space and resort the rows.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们每月创建一个表格，我们可以通过在相应的表格上运行`DROP TABLE`命令轻松删除旧数据。这种方法比运行大规模的`DELETE`流程要快得多，并且还能避免运行后续的`VACUUM`过程以回收空间和重新排序行。
- en: To combine query results across tables, we can use a `UNION ALL` view. Similarly,
    when we need to delete old data, we remove the dropped table from the `UNION ALL`
    view.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要跨表合并查询结果，我们可以使用`UNION ALL`视图。类似地，当我们需要删除旧数据时，我们从`UNION ALL`视图中删除已丢弃的表。
- en: 'Here is an example of a `UNION ALL` view across two tables with customer reviews
    from years 2014 and 2015—assuming we have one table each for 2014 and 2015 data.
    The following table shows the results of the query:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个跨两个表格进行`UNION ALL`视图的示例，涉及来自 2014 年和 2015 年的客户评论数据。以下表格显示了查询结果：
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '| product_category | count_star_rating | year |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 产品类别 | 评分计数 | 年份 |'
- en: '| --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Digital_Ebook_Purchase | 6615914 | 2014 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 数字电子书购买 | 6615914 | 2014 |'
- en: '| Digital_Ebook_Purchase | 4533519 | 2015 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 数字电子书购买 | 4533519 | 2015 |'
- en: '| Books | 3472631 | 2014 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 图书 | 3472631 | 2014 |'
- en: '| Wireless | 2998518 | 2015 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 无线 | 2998518 | 2015 |'
- en: '| Wireless | 2830482 | 2014 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 无线 | 2830482 | 2014 |'
- en: '| Books | 2808751 | 2015 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 图书 | 2808751 | 2015 |'
- en: '| Apparel | 2369754 | 2015 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 服装 | 2369754 | 2015 |'
- en: '| Home | 2172297 | 2015 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 家居 | 2172297 | 2015 |'
- en: '| Apparel | 2122455 | 2014 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 服装 | 2122455 | 2014 |'
- en: '| Home | 1999452 | 2014 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 家居 | 1999452 | 2014 |'
- en: 'Now, let’s actually run a query and combine data from Amazon Redshift with
    data that is still in S3\. Let’s take the data from the previous query for the
    years 2015 and 2014 and query Athena/S3 for the years 2013–1995 by running this
    command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实际运行一个查询，并将 Amazon Redshift 的数据与仍然存储在 S3 中的数据进行合并。让我们使用以下命令从之前查询的数据获取
    2015 年和 2014 年的数据，并查询 Athena/S3 中的 2013 年至 1995 年的数据：
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '| year | product_category | count_star_rating |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 产品类别 | 评分计数 |'
- en: '| --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2015 | Apparel | 4739508 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 服装 | 4739508 |'
- en: '| 2014 | Apparel | 4244910 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 服装 | 4244910 |'
- en: '| 2013 | Apparel | 854813 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 服装 | 854813 |'
- en: '| 2012 | Apparel | 273694 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | 服装 | 273694 |'
- en: '| 2011 | Apparel | 109323 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 2011 | 服装 | 109323 |'
- en: '| 2010 | Apparel | 57332 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | 服装 | 57332 |'
- en: '| 2009 | Apparel | 42967 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 2009 | 服装 | 42967 |'
- en: '| 2008 | Apparel | 33761 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2008 | 服装 | 33761 |'
- en: '| 2007 | Apparel | 25986 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 2007 | 服装 | 25986 |'
- en: '| 2006 | Apparel | 7293 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 2006 | 服装 | 7293 |'
- en: '| 2005 | Apparel | 3533 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 2005 | 服装 | 3533 |'
- en: '| 2004 | Apparel | 2357 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 2004 | 服装 | 2357 |'
- en: '| 2003 | Apparel | 2147 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 2003 | 服装 | 2147 |'
- en: '| 2002 | Apparel | 907 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 2002 | 服装 | 907 |'
- en: '| 2001 | Apparel | 5 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 2001 | 服装 | 5 |'
- en: '| 2000 | Apparel | 6 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 服装 | 6 |'
- en: '| 2015 | Automotive | 2609750 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 汽车 | 2609750 |'
- en: '| 2014 | Automotive | 2350246 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 汽车 | 2350246 |'
- en: Export Amazon Redshift Data to S3 Data Lake as Parquet
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Amazon Redshift 数据导出到 S3 数据湖作为 Parquet
- en: Amazon Redshift Data Lake Export gives us the ability to unload the result of
    an Amazon Redshift query to our S3 data lake in the optimized Apache Parquet columnar
    file format. This enables us to share any data transformation and enrichment we
    have done in Amazon Redshift back into our S3 data lake in an open format. Unloaded
    data is automatically registered in the AWS Glue Data Catalog to be used by any
    Hive Metastore–compatible query engines, including Amazon Athena, EMR, Kinesis,
    SageMaker, and Apache Spark.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 数据湖导出使我们能够将 Amazon Redshift 查询的结果卸载到我们的 S3 数据湖中，以优化的 Apache
    Parquet 列式文件格式。这使我们能够将在 Amazon Redshift 中进行的任何数据转换和增强重新共享到我们的 S3 数据湖中，以开放格式存储。卸载的数据会自动在
    AWS Glue 数据目录中注册，供任何 Hive Metastore 兼容的查询引擎使用，包括 Amazon Athena、EMR、Kinesis、SageMaker
    和 Apache Spark。
- en: We can specify one or more partition columns so that unloaded data is automatically
    partitioned into folders in our Amazon S3 bucket. For example, we can choose to
    unload our customer reviews data and partition it by `product_category`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以指定一个或多个分区列，使得卸载的数据自动分区到 Amazon S3 存储桶中的文件夹中。例如，我们可以选择卸载我们的客户评论数据，并按`product_category`进行分区。
- en: 'We can simply run the following SQL command to unload our 2015 customer reviews
    data in Parquet file format into S3, partitioned by `product_category`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需运行以下 SQL 命令，将我们的 2015 年客户评论数据以 Parquet 文件格式卸载到 S3 中，并按`product_category`分区：
- en: '[PRE18]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can use the AWS CLI tool again to list the S3 folder and see our unloaded
    data from 2015 in Parquet format:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用 AWS CLI 工具列出 S3 文件夹，并查看我们以 Parquet 格式卸载的 2015 年数据：
- en: '[PRE19]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Share Data Between Amazon Redshift Clusters
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在亚马逊 Redshift 集群之间共享数据
- en: Amazon Redshift also implements a data sharing capability that allows us to
    securely share live data across Amazon Redshift clusters without the need to move
    data. Instead, we create a “data share” object that specifies the data to share
    and the list of Amazon Redshift clusters that are allowed to access the data.
    On the consuming Amazon Redshift cluster, we create a new database from the data
    share object and assign permissions to the relevant IAM users and groups to manage
    access to the database. The data sharing capability is useful if we need to share
    data among multiple business units, or if we want to share data from a central
    data warehouse cluster with additional BI and analytics clusters.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 Redshift 还实现了数据共享功能，允许我们在不移动数据的情况下安全地在亚马逊 Redshift 集群之间共享实时数据。相反，我们创建一个“数据共享”对象，指定要共享的数据以及被允许访问数据的亚马逊
    Redshift 集群列表。在消费的亚马逊 Redshift 集群上，我们从数据共享对象创建一个新的数据库，并分配权限给相关的 IAM 用户和组，以管理对数据库的访问。数据共享功能在需要在多个业务单元之间共享数据或者希望从中央数据仓库集群向其他
    BI 和分析集群共享数据时非常有用。
- en: Choose Between Amazon Athena and Amazon Redshift
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在亚马逊 Athena 和亚马逊 Redshift 之间做出选择
- en: Amazon Athena is the preferred choice when running ad hoc SQL queries on data
    that is stored in Amazon S3\. It doesn’t require us to set up or manage any infrastructure
    resources—we don’t need to move any data. It supports structured, unstructured,
    and semistructured data. With Athena, we are defining a “schema on read”—we basically
    just log in, create a table, and start running queries.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当在存储在亚马逊 S3 中的数据上运行即席 SQL 查询时，亚马逊 Athena 是首选。它不需要我们设置或管理任何基础设施资源——我们不需要移动任何数据。它支持结构化、非结构化和半结构化数据。使用
    Athena，我们在“读时模式”下定义模式——基本上只需登录、创建表格并开始运行查询。
- en: Amazon Redshift is targeted for modern data analytics on petabytes of structured
    data. Here, we need to have a predefined “schema on write.” Unlike serverless
    Athena, Amazon Redshift requires us to create a cluster (compute and storage resources),
    ingest the data, and build tables before we can start to query but caters to performance
    and scale. So for highly relational data with a transactional nature (data gets
    updated), workloads that involve complex joins, or subsecond latency requirements,
    Amazon Redshift is the right choice.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 Redshift 针对 PB 级结构化数据的现代数据分析进行了优化。在这里，我们需要预定义的“写时模式”。与无服务器的 Athena 不同，亚马逊
    Redshift 要求我们创建一个集群（计算和存储资源）、摄入数据并构建表格，然后才能开始查询，但能够满足性能和规模需求。因此，对于具有事务性特性的高度关系型数据（数据更新）、涉及复杂连接或需要次秒级响应时间的工作负载，亚马逊
    Redshift 是正确的选择。
- en: Athena and Amazon Redshift are optimized for read-heavy analytics workloads;
    they are not replacements for write-heavy, relational databases such as Amazon
    Relational Database Service (RDS) and Aurora. At a high level, use Athena for
    exploratory analytics and operational debugging; use Amazon Redshift for business-critical
    reports and dashboards.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Athena 和亚马逊 Redshift 都专为读重型分析工作负载进行了优化；它们不适用于写重型的关系数据库，比如亚马逊关系数据库服务（RDS）和 Aurora。从高层次来看，使用
    Athena 进行探索性分析和操作调试；使用亚马逊 Redshift 进行业务关键报告和仪表板。
- en: Reduce Cost and Increase Performance
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少成本并提高性能
- en: In this section, we want to provide some tips and tricks to reduce cost and
    increase performance during data ingestion, including file formats, partitions,
    compression, and sort/distribution keys. We will also demonstrate how to use Amazon
    S3 Intelligent-Tiering to lower our storage bill.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们想提供一些有关在数据摄入期间减少成本和提高性能的技巧，包括文件格式、分区、压缩以及排序/分布键。我们还将演示如何使用亚马逊 S3 智能分层来降低存储成本。
- en: S3 Intelligent-Tiering
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: S3 智能分层
- en: We introduced Amazon S3 in this chapter as a scalable, durable storage service
    for building shared datasets, such as data lakes in the cloud. And while we keep
    the S3 usage fairly simple in this book, the service actually offers us a variety
    of options to optimize our storage cost as our data grows.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了亚马逊 S3 作为一种可扩展的、持久的存储服务，用于构建云中的共享数据集，比如数据湖。虽然在本书中我们将 S3 的使用保持相对简单，但实际上该服务为我们提供了多种选项来优化随着数据增长而增长的存储成本。
- en: Depending on our data’s access frequency patterns and service-level agreement
    (SLA) needs, we can choose from various Amazon S3 storage classes. [Table 4-1](#comparison_of_amazon_sthree_storage_cla)
    compares the Amazon S3 storage classes in terms of data access frequency and data
    retrieval time.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们数据的访问频率模式和服务级别协议（SLA）需求，我们可以选择不同的Amazon S3存储类别。 [表4-1](#comparison_of_amazon_sthree_storage_cla)
    比较了Amazon S3存储类别在数据访问频率和数据检索时间方面的差异。
- en: Table 4-1\. Comparison of Amazon S3 storage classes
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. Amazon S3存储类别比较
- en: '| From frequent access | To infrequent access |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 从频繁访问 | 到不经常访问 |'
- en: '| --- | --- |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| S3 Standard (default storage class) | S3 Intelligent-Tiering | S3 Standard-IA
    | S3 One Zone-IA | Amazon S3 Glacier | Amazon S3 Glacier Deep Archive |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| S3标准（默认存储类） | S3智能分层 | S3标准-IA | S3单区域-IA | Amazon S3冰川 | Amazon S3冰川深度存档
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| General-purpose storage Active, frequently'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '| 通用存储活跃，频繁'
- en: accessed data
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 访问数据
- en: Access in milliseconds | Data with unknown or changing access patterns
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 毫秒级访问 | 数据具有未知或变化的访问模式
- en: Access in milliseconds
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 毫秒级访问
- en: Opt in for automatic
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 自动选择
- en: archiving | Infrequently accessed (IA) data
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 存档 | 不经常访问的（IA）数据
- en: Access in milliseconds | Lower durability (one Zvailability zone) Re-creatable
    data
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 毫秒级访问 | 更低的耐久性（单可用区）可重建数据
- en: Access in milliseconds | Archive data Access in minutes or hours | Long-term
    archive data Access in hours |
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 毫秒级访问 | 存档数据分钟或小时级访问 | 长期存档数据小时级访问 |
- en: But how do we know which objects to move? Imagine our S3 data lake has grown
    over time and we possibly have billions of objects across several S3 buckets in
    the S3 Standard storage class. Some of those objects are extremely important,
    while we haven’t accessed others maybe in months or even years. This is where
    S3 Intelligent-Tiering comes into play.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何知道要移动哪些对象？想象一下我们的S3数据湖随着时间的推移不断增长，我们可能在多个S3存储桶中拥有数十亿个对象，其中一些对象非常重要，而其他对象可能数月甚至数年未访问。这就是S3智能分层发挥作用的地方。
- en: Amazon S3 Intelligent-Tiering automatically optimizes our storage cost for data
    with changing access patterns by moving objects between the frequent-access tier
    optimized for frequent use of data and the lower-cost infrequent-access tier optimized
    for less-accessed data. Intelligent-Tiering monitors our access patterns and auto-tiers
    on a granular object level without performance impact or any operational overhead.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon S3智能分层通过在频繁访问数据优化层和低成本不经常访问数据优化层之间移动对象，自动优化我们的存储成本，适应变化的访问模式。智能分层监控我们的访问模式，并在粒度对象级别自动分层，没有性能影响或任何操作开销。
- en: Parquet Partitions and Compression
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet分区和压缩
- en: 'Athena supports the Parquet columnar format for large-scale analytics workloads.
    Parquet enables the following performance optimizations for our queries:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Athena支持Parquet列式格式用于大规模分析工作负载。Parquet为我们的查询提供以下性能优化：
- en: Partitions and pushdowns
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 分区和下推
- en: Partitions are physical groupings of data on disk to match our query patterns
    (i.e., `SELECT * FROM reviews WHERE product_category='Books'`). Modern query engines
    like Athena, Amazon Redshift, and Apache Spark will “pushdown” the `WHERE` into
    the physical storage system to allow the disk controller to seek once and read
    all relevant data in one scan without randomly skipping to different areas of
    the disk. This improves query performance even with solid state drives (SSDs),
    which have a lower seek time than traditional, media-based disks.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 分区是在磁盘上物理数据的分组，以匹配我们的查询模式（例如`SELECT * FROM reviews WHERE product_category='Books'`）。现代查询引擎如Athena、Amazon
    Redshift和Apache Spark会将`WHERE`条件“下推”到物理存储系统，允许磁盘控制器仅需一次寻址即可扫描所有相关数据，而无需随机跳转到磁盘的不同区域。这提高了查询性能，即使使用固态硬盘（SSD），其寻址时间比传统的基于介质的硬盘低。
- en: Dictionary encoding/compression
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 字典编码/压缩
- en: When a small number of categorical values are stored together on disk (i.e.,
    `product_category`, which has 43 total values in our dataset), the values can
    be compressed into a small number of bits to represent each value (i.e., `Books`,
    `Lawn_and_Garden`, `Software`, etc.) versus storing the entire string.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 当少量分类值一起存储在磁盘上（例如我们数据集中共有43个`product_category`值），这些值可以被压缩成少量比特来表示每个值（例如`Books`、`Lawn_and_Garden`、`Software`等），而不是存储整个字符串。
- en: Type compression
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 类型压缩
- en: 'When values of a similar type (i.e., String, Date, Integer) are stored together
    on disk, the values can be compressed together: (String, String), (Date, Date),
    (Integer, Integer). This compression is more efficient than if the values were
    stored separately on disk in a row-wise manner: (String, Date, Integer), (String,
    Date, Integer)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 类似类型的值（例如 String、Date、Integer）在磁盘上存储在一起时，这些值可以一起压缩：(String, String), (Date,
    Date), (Integer, Integer)。这种压缩比将这些值以行方式分开存储在磁盘上更为高效：(String, Date, Integer), (String,
    Date, Integer)
- en: Vectorized aggregations
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 矢量化聚合
- en: Because column values are stored together on disk, the disk controller needs
    to only perform one disk seek to find the beginning of the data. From that point,
    it will scan the data to perform the aggregation. Additionally, modern chips/processors
    offer high-performance vectorization instructions to perform calculations on large
    amounts of data versus flushing data in and out of the various data caches (L1,
    L2) or main memory.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 因为列值在磁盘上存储在一起，磁盘控制器只需执行一个磁盘搜索来找到数据的起始位置。从那时起，它将扫描数据执行聚合操作。此外，现代芯片/处理器提供高性能的矢量化指令，以在大量数据上执行计算，而不是将数据刷新到各种数据缓存（L1、L2）或主内存中。
- en: See an example of row versus columnar data format in [Figure 4-7](#using_a_columnar_data_format_such_as_pa).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4-7](#using_a_columnar_data_format_such_as_pa) 中看到行与列式数据格式的示例。
- en: '![](assets/dsaw_0407.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0407.png)'
- en: Figure 4-7\. Using a columnar data format such as Parquet, we can apply various
    performance optimizations for query execution and data encoding.
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 使用 Parquet 等列式数据格式，我们可以对查询执行和数据编码应用各种性能优化。
- en: Amazon Redshift Table Design and Compression
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Redshift 表设计和压缩
- en: 'Here is the `CREATE TABLE` statement that we used to create the Amazon Redshift
    tables:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们用来创建 Amazon Redshift 表的 `CREATE TABLE` 语句：
- en: '[PRE20]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When we create a table, we can specify one or more columns as the `SORTKEY`.
    Amazon Redshift stores the data on disk in sorted order according to the `SORTKEY`.
    Hence, we can optimize our table by choosing a `SORTKEY` that reflects our most
    frequently used query types. If we query a lot of recent data, we can specify
    a timestamp column as the `SORTKEY`. If we frequently query based on range or
    equality filtering on one column, we should choose that column as the `SORTKEY`.
    As we are going to run a lot of queries in the next chapter filtering on `product_category`,
    let’s choose that one as our `SORTKEY`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建表时，可以指定一个或多个列作为 `SORTKEY`。Amazon Redshift 按照 `SORTKEY` 的顺序将数据存储在磁盘上，因此我们可以通过选择反映我们最常用查询类型的
    `SORTKEY` 来优化表。如果我们频繁查询最近的数据，可以将时间戳列指定为 `SORTKEY`。如果我们经常基于一列上的范围或等值过滤进行查询，则应选择该列作为
    `SORTKEY`。在下一章节我们将运行大量基于 `product_category` 进行过滤的查询，让我们将其选择为我们的 `SORTKEY`。
- en: Tip
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Amazon Redshift Advisor continuously recommends `SORTKEY`s for frequently queried
    tables. Advisor will generate an `ALTER TABLE` command that we run without having
    to re-create the tables—without impacting concurrent read and write queries. Note
    that Advisor does not provide recommendations if it doesn’t see enough data (queries)
    or if the benefits are relatively small.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift Advisor 持续推荐频繁查询的表使用 `SORTKEY`。Advisor 将生成一个 `ALTER TABLE` 命令，我们可以直接运行，而无需重新创建表——不影响并发读写查询。请注意，如果
    Advisor 没有看到足够的数据（查询），或者收益相对较小，它将不会提供推荐。
- en: 'We can also define a distribution style for every table. When we load data
    into a table, Amazon Redshift distributes the rows of the table among our cluster
    nodes according to the table’s distribution style. When we perform a query, the
    query optimizer redistributes the rows to the cluster nodes as needed to perform
    any joins and aggregations. So our goal should be to optimize the rows distribution
    to minimize data movements. There are three distribution styles from which we
    can choose:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以为每个表定义一个分布样式。当我们将数据加载到表中时，Amazon Redshift 根据表的分布样式将表的行分布到我们的集群节点中。当执行查询时，查询优化器根据需要将行重新分布到集群节点，以执行任何连接和聚合操作。因此，我们的目标应该是优化行分布，以最小化数据移动。我们可以选择三种分布样式中的一种：
- en: '`KEY` distribution'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`KEY` 分布'
- en: Distribute the rows according to the values in one column.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一列中的值分发行。
- en: '`ALL` distribution'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`ALL` 分布'
- en: Distribute a copy of the entire table to every node.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 将整个表的副本分发到每个节点。
- en: '`EVEN` distribution'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`EVEN` 分布'
- en: The rows are distributed across all nodes in a round-robin fashion, which is
    the default distribution style.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 行以循环轮换方式在所有节点之间分布，这是默认的分布样式。
- en: For our table, we’ve chosen `KEY` distribution based on `product_id`, as this
    column has a high cardinality, shows an even distribution, and can be used to
    join with other tables.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的表，我们基于`product_id`选择了`KEY`分布，因为这一列具有高基数，显示均匀分布，并且可以用于与其他表进行连接。
- en: At any time, we can use `EXPLAIN` on our Amazon Redshift queries to make sure
    the `DISTKEY` and `SORTKEY` are being utilized. If our query patterns change over
    time, we may want to revisit changing these keys.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时候，我们都可以在我们的Amazon Redshift查询中使用`EXPLAIN`来确保`DISTKEY`和`SORTKEY`得到了利用。如果我们的查询模式随时间变化，我们可能需要重新审视更改这些键。
- en: In addition, we are using compression for most columns to reduce the overall
    storage footprint and reduce our cost. [Table 4-2](#compression_used_for_each_redshift_colu)
    analyzes the compression used for each Amazon Redshift column in our schema.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们为大多数列使用了压缩，以减少总体存储占用和降低成本。[Table 4-2](#compression_used_for_each_redshift_colu)
    分析了我们模式中每个Amazon Redshift列使用的压缩方式。
- en: Table 4-2\. Compression types used in our Amazon Redshift table
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-2。我们Amazon Redshift表中使用的压缩类型
- en: '| Column | Data type | Encoding | Explanation |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Column | Data type | Encoding | 解释 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| marketplace | varchar(2) | zstd | Low cardinality, too small for higher compression
    overhead |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| marketplace | varchar(2) | zstd | 基数较低，对于更高的压缩开销来说太小了 |'
- en: '| customer_id | varchar(8) | zstd | High cardinality, relatively few repeat
    values |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| customer_id | varchar(8) | zstd | 高基数，重复值相对较少 |'
- en: '| review_id | varchar(14) | zstd | Unique, unbounded cardinality, no repeat
    values |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| review_id | varchar(14) | zstd | 唯一，无限的基数，无重复值 |'
- en: '| product_id | varchar(10) | zstd | Unbounded cardinality, relatively low number
    of repeat values |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| product_id | varchar(10) | zstd | 无限的基数，重复词汇数量相对较低 |'
- en: '| product_parent | varchar(10) | zstd | Unbounded cardinality, relatively low
    number of repeat words |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| product_parent | varchar(10) | zstd | 无限的基数，重复词汇数量相对较低 |'
- en: '| product_title | varchar(400) | zstd | Unbounded cardinality, relatively low
    number of repeat words |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| product_title | varchar(400) | zstd | 无限的基数，重复词汇数量相对较低 |'
- en: '| product_category | varchar(24) | raw | Low cardinality, many repeat values,
    but first SORT key is raw |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| product_category | varchar(24) | raw | 基数较低，许多重复值，但第一个SORT键是raw |'
- en: '| star_rating | int | az64 | Low cardinality, many repeat values |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| star_rating | int | az64 | 基数较低，许多重复值 |'
- en: '| helpful_votes | int | zstd | Relatively high cardinality |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| helpful_votes | int | zstd | 基数较高 |'
- en: '| total_votes | int | zstd | Relatively high cardinality |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| total_votes | int | zstd | 基数相对较高 |'
- en: '| vine | varchar(1) | zstd | Low cardinality, too small to incur higher compression
    overhead |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| vine | varchar(1) | zstd | 基数较低，对于更高的压缩开销来说太小了 |'
- en: '| verified_purchase | varchar(1) | zstd | Low cardinality, too small to incur
    higher compression overhead |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| verified_purchase | varchar(1) | zstd | 基数较低，对于更高的压缩开销来说太小了 |'
- en: '| review_headline | varchar(128) | zstd | Varying length text, high cardinality,
    low repeat words |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| review_headline | varchar(128) | zstd | 变长文本，高基数，重复词汇数量低 |'
- en: '| review_body | varchar(65535) | zstd | Varying length text, high cardinality,
    low repeat words |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| review_body | varchar(65535) | zstd | 变长文本，高基数，重复词汇数量低 |'
- en: '| review_date | varchar(10) | bytedict | Fixed length, relatively low cardinality,
    many repeat values |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| review_date | varchar(10) | bytedict | 固定长度，基数相对较低，许多重复值 |'
- en: '| year | int | az64 | Low cardinality, many repeat values |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| year | int | az64 | 基数较低，许多重复值 |'
- en: Note
  id: totrans-301
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While AWS CEO Andy Jassy maintains “there is no compression algorithm for experience,”
    there is a compression algorithm for data. Compression is a powerful tool for
    the ever-growing world of big data. All modern big data processing tools are compression-friendly,
    including Amazon Athena, Redshift, Parquet, pandas, and Apache Spark. Using compression
    on small values such as `varchar(1)` may not improve performance. However, due
    to native hardware support, there are almost no drawbacks to using compression.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AWS CEO Andy Jassy坚持认为“没有经验的压缩算法”，但对于数据来说，却有压缩算法。压缩是处理不断增长的大数据世界的强大工具。所有现代大数据处理工具都支持压缩，包括Amazon
    Athena、Redshift、Parquet、pandas和Apache Spark。对于诸如`varchar(1)`这样的小值使用压缩可能不会提高性能。然而，由于本地硬件支持，使用压缩几乎没有任何缺点。
- en: '`zstd` is a generic compression algorithm that works across many different
    data types and column sizes. The `star_rating` and `year` fields are set to the
    default `az64` encoding applied to most numeric and date fields. For most columns,
    we gain a quick win by using the default `az64` encoding for integers and overriding
    the default `lzo` encoding in favor of the flexible `zstd` encoding for everything
    else, including text.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`zstd` 是一种通用的压缩算法，适用于许多不同的数据类型和列大小。`star_rating` 和 `year` 字段设置为默认的 `az64` 编码，适用于大多数数值和日期字段。对于大多数列，通过为整数使用默认的
    `az64` 编码和为文本等其他内容使用灵活的 `zstd` 编码，我们获得了快速的优势。'
- en: We are using `bytedict` for `review_date` to perform dictionary encoding on
    the string-based dates `(YYYY-MM-DD)`. While it seemingly has a large number of
    unique values, `review_date` actually contains a small number of unique values
    because there are only ~7,300 (365 days per year × 20 years) days in a 20-year
    span. This cardinality is low enough to capture all of the possible dates in just
    a few bits versus using a full `varchar(10)` for each date.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `bytedict` 对 `review_date` 进行字典编码，以处理基于字符串的日期 `(YYYY-MM-DD)`。尽管它看似有大量唯一的值，实际上
    `review_date` 只包含少量唯一值，因为在 20 年的时间跨度内只有大约 7300 天（每年 365 天 × 20 年）。这个基数很低，可以用几个比特捕获所有可能的日期，而不是对每个日期使用完整的
    `varchar(10)`。
- en: While `product_category` is a great candidate for `bytedict` dictionary encoding,
    it is our first (and only, in this case) `SORTKEY`. As a performance best practice,
    the first `SORTKEY` should not be compressed.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `product_category` 是 `bytedict` 字典编码的绝佳候选项，但它是我们的第一个（也是唯一一个，在这种情况下）`SORTKEY`。作为性能最佳实践，第一个
    `SORTKEY` 不应该被压缩。
- en: While `marketplace`, `product_category`, `vine`, and `verified_purchase` seem
    to be good candidates for `bytedict`, they are too small to benefit from the extra
    overhead. For now, we leave them as `zstd`.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `marketplace`、`product_category`、`vine` 和 `verified_purchase` 看起来是 `bytedict`
    的良好候选项，但它们太小，无法从额外的开销中受益。目前我们将它们保留为 `zstd`。
- en: 'If we have an existing Amazon Redshift table to optimize, we can run the `ANALYZE
    COMPRESSION` command in Amazon Redshift to generate a report of suggested compression
    encodings as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个现有的 Amazon Redshift 表需要优化，可以在 Amazon Redshift 中运行 `ANALYZE COMPRESSION`
    命令，生成推荐的压缩编码报告如下：
- en: '[PRE21]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The result will be a table like the following showing the % improvement in
    compression if we switch to another encoding:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下表所示，展示如果切换到另一种编码方式，压缩改善的百分比：
- en: '| Column | Encoding | Estimated reduction (%) |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 编码方式 | 估计的压缩率 (%) |'
- en: '| --- | --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `marketplace` | `zstd` | 90.84 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| `marketplace` | `zstd` | 90.84 |'
- en: '| `customer_id` | `zstd` | 38.88 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| `customer_id` | `zstd` | 38.88 |'
- en: '| `review_id` | `zstd` | 36.56 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| `review_id` | `zstd` | 36.56 |'
- en: '| `product_id` | `zstd` | 44.15 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| `product_id` | `zstd` | 44.15 |'
- en: '| `product_parent` | `zstd` | 44.03 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| `product_parent` | `zstd` | 44.03 |'
- en: '| `product_title` | `zstd` | 30.72 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| `product_title` | `zstd` | 30.72 |'
- en: '| `product_category` | `zstd` | 99.95 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| `product_category` | `zstd` | 99.95 |'
- en: '| `star_rating` | `az64` | 0 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| `star_rating` | `az64` | 0 |'
- en: '| `helpful_votes` | `zstd` | 47.58 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| `helpful_votes` | `zstd` | 47.58 |'
- en: '| `total_votes` | `zstd` | 39.75 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| `total_votes` | `zstd` | 39.75 |'
- en: '| `vine` | `zstd` | 85.03 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| `vine` | `zstd` | 85.03 |'
- en: '| `verified_purchase` | `zstd` | 73.09 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| `verified_purchase` | `zstd` | 73.09 |'
- en: '| `review_headline` | `zstd` | 30.55 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| `review_headline` | `zstd` | 30.55 |'
- en: '| `review_body` | `zstd` | 32.19 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| `review_body` | `zstd` | 32.19 |'
- en: '| `review_date` | `bytedict` | 64.1 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| `review_date` | `bytedict` | 64.1 |'
- en: '| `year` | `az64` | 0 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| `year` | `az64` | 0 |'
- en: We performed this analysis on a version of the `CREATE TABLE` that did not specify
    any `ENCODE` attributes. By default, Amazon Redshift will use `az64` for numerics/dates
    and `lzo` for everything else (hence the 0% gain for the `az64` suggestions).
    We can also use the `ALTER TABLE` statement to change the compression used for
    each column.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对一个没有指定任何 `ENCODE` 属性的 `CREATE TABLE` 版本进行了此分析。默认情况下，Amazon Redshift 将对数值/日期使用
    `az64`，对其他所有内容使用 `lzo`（因此对于 `az64` 建议的增益为 0%）。我们还可以使用 `ALTER TABLE` 语句更改每列使用的压缩方式。
- en: Keep in mind that these are just suggestions and not always appropriate for
    our specific environment. We should try different encodings for our dataset and
    query the `STV_BLOCKLIST` table to compare the % reduction in physical number
    of blocks. For example, the analyzer recommends using `zstd` for our `SORTKEY`,
    `product_category`, but our experience shows that query performance suffers when
    we compress the `SORTKEY`. We are using the extra disk space to improve our query
    performance.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些仅仅是建议，不一定适用于我们特定的环境。我们应该尝试不同的数据集编码，并查询 `STV_BLOCKLIST` 表，比较物理块数目的减少百分比。例如，分析器建议在我们的
    `SORTKEY` 中使用 `zstd`，但我们的经验表明，当我们压缩 `SORTKEY` 时，查询性能会受到影响。我们正在利用额外的磁盘空间来提高查询性能。
- en: Amazon Redshift supports automatic table optimization and other self-tuning
    capabilities that leverage machine learning to optimize peak performance and adapt
    to shifting workloads. The performance optimizations include automatic vacuum
    deletes, intelligent workload management, automatic table sorts, and automatic
    selection of distribution and sort keys.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 支持自动表优化和其他自调整功能，利用机器学习优化峰值性能并适应变化的工作负载。性能优化包括自动清理删除、智能工作负载管理、自动表排序以及自动选择分发和排序键。
- en: Use Bloom Filters to Improve Query Performance
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Bloom 过滤器提高查询性能
- en: Amazon Redshift is a distributed query engine and S3 is a distributed object
    store. Distributed systems consist of many cluster instances. To improve performance
    of distributed queries, we need to minimize the number of instances that are scanned
    and the amount of data transferred between the instances.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 是一个分布式查询引擎，而 S3 是一个分布式对象存储。分布式系统由许多集群实例组成。为了提高分布式查询的性能，我们需要尽量减少扫描的实例数量以及实例之间的数据传输量。
- en: Bloom filters, probabilistic and memory-efficient data structures, help answer
    the question, “Does this specific cluster instance contain data that might be
    included in the query results?” Bloom filters answer with either a definite NO
    or a MAYBE. If the bloom filter answered with a NO, the engine will completely
    skip that cluster instance and scan the remaining instances where the bloom filter
    answered with a MAYBE.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Bloom 过滤器是一种概率和内存效率高的数据结构，用于回答“特定的集群实例是否包含可能包含在查询结果中的数据？”的问题。Bloom 过滤器的答案可以是肯定的“NO”或者“MAYBE”。如果
    Bloom 过滤器的答案是“NO”，引擎将完全跳过该集群实例，并扫描其余 Bloom 过滤器答案为“MAYBE”的实例。
- en: By filtering out rows of data that do not match the given query, bloom filters
    result in huge performance gains for join queries. And since bloom filtering happens
    close to the data source, data transfer is minimized between the nodes in the
    distributed cluster during join queries. This ultimately increases query performance
    for data stores such as S3.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 通过过滤掉不匹配给定查询的数据行，Bloom 过滤器显著提高连接查询的性能。并且由于 Bloom 过滤器在数据源附近执行，减少了在连接查询期间分布式集群节点之间的数据传输。这最终提高了对诸如
    S3 这样的数据存储的查询性能。
- en: Amazon Redshift Spectrum actually automatically creates and manages bloom filters
    on external data such as S3, but we should be aware of their importance in improving
    query performance on distributed data stores. Bloom filters are a pattern used
    throughout all of distributed computing, including distributed query engines.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift Spectrum 实际上会自动在诸如 S3 等外部数据上创建和管理 Bloom 过滤器，但我们应该意识到在改善分布式数据存储的查询性能中，它们的重要性。Bloom
    过滤器是分布式计算中广泛使用的一种模式，包括分布式查询引擎。
- en: Materialized Views in Amazon Redshift Spectrum
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Amazon Redshift Spectrum 中的物化视图
- en: Materialized views provide repeatable and predictable query performance on external
    data sources such as S3\. They pretransform and prejoin data before SQL queries
    are executed. Materialized views can be updated either manually or on a predefined
    schedule using Amazon Redshift Spectrum.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Materialized views provide repeatable and predictable query performance on external
    data sources such as S3\. They pretransform and prejoin data before SQL queries
    are executed. Materialized views can be updated either manually or on a predefined
    schedule using Amazon Redshift Spectrum.
- en: Summary
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we provided an overview on how we can load our data into Amazon
    S3, discussed the value of an S3 data lake, and showed how we can leverage services
    like Amazon Athena to run ad hoc SQL queries across the data in S3 without the
    need to physically move the data. We showed how to continuously ingest new application
    data using AWS Glue Crawler. We also introduced our dataset, the Amazon Customer
    Reviews Dataset, which we will be using through the rest of this book.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了如何将数据加载到Amazon S3中，讨论了S3数据湖的价值，并展示了如何利用Amazon Athena在S3中运行自由查询SQL而无需实际移动数据。我们展示了如何使用AWS
    Glue Crawler持续摄取新的应用程序数据。我们还介绍了我们的数据集，即亚马逊客户评论数据集，这将是本书后续章节的使用对象。
- en: As different use cases require data in different formats, we elaborated on how
    we can use Athena to convert tab-separated data into query-optimized, columnar
    Parquet data.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于不同的使用情况需要不同格式的数据，我们详细阐述了如何使用Athena将制表符分隔的数据转换为查询优化的列式Parquet数据。
- en: Data in our S3 data lake often needs to be accessed not only by the data science
    and machine learning teams but also by business intelligence teams. We introduced
    the Lake House Architecture based on Amazon Redshift, AWS’s petabyte-scale cloud
    data warehouse. We showed how to use Amazon Redshift Spectrum to combine queries
    across data stores, including Amazon Redshift and S3.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的S3数据湖中，数据通常不仅需要由数据科学和机器学习团队访问，还需要由商业智能团队访问。我们介绍了基于亚马逊Redshift的Lake House架构，这是AWS的PB级云数据仓库。我们展示了如何使用Amazon
    Redshift Spectrum跨数据存储库（包括Amazon Redshift和S3）合并查询。
- en: To conclude this chapter, we discussed the various data compression formats
    and S3 tiering options, showing how they can reduce cost and improve query performance.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 结束本章时，我们讨论了各种数据压缩格式和S3分层选项，展示它们如何降低成本并改善查询性能。
- en: In [Chapter 5](ch05.html#explore_the_dataset) we will explore the dataset in
    more detail. We will run queries to understand and visualize our datasets. We
    will also show how to detect data anomalies with Apache Spark and Amazon SageMaker
    Processing Jobs.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#explore_the_dataset)中，我们将更详细地探索数据集。我们将运行查询以理解和可视化我们的数据集。我们还将展示如何使用Apache
    Spark和Amazon SageMaker处理作业检测数据异常。
