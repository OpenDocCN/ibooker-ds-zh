- en: Part 1\. Background and fundamentals
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分. 背景 和 基础
- en: '[Part 1](#part01) of this book consists of [chapters 1](kindle_split_010.html#ch01)
    and [2](kindle_split_011.html#ch02), which cover the important Hadoop fundamentals.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一部分](#part01) 本书的由 [第一章](kindle_split_010.html#ch01) 和 [第二章](kindle_split_011.html#ch02)
    组成，涵盖了重要的 Hadoop 基础知识。'
- en: '[Chapter 1](kindle_split_010.html#ch01) covers Hadoop’s components and its
    ecosystem and provides instructions for installing a pseudo-distributed Hadoop
    setup on a single host, along with a system that will enable you to run all of
    the examples in the book. [Chapter 1](kindle_split_010.html#ch01) also covers
    the basics of Hadoop configuration, and walks you through how to write and run
    a MapReduce job on your new setup.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章](kindle_split_010.html#ch01) 介绍了 Hadoop 的组件及其生态系统，并提供了在单个主机上安装伪分布式 Hadoop
    设置的说明，以及一个能够让您运行本书中所有示例的系统。[第一章](kindle_split_010.html#ch01) 还涵盖了 Hadoop 配置的基础知识，并指导您如何在新配置上编写和运行
    MapReduce 作业。'
- en: '[Chapter 2](kindle_split_011.html#ch02) introduces YARN, which is a new and
    exciting development in Hadoop version 2, transitioning Hadoop from being a MapReduce-only
    system to one that can support many execution engines. Given that YARN is new
    to the community, the goal of this chapter is to look at some basics such as its
    components, how configuration works, and also how MapReduce works as a YARN application.
    [Chapter 2](kindle_split_011.html#ch02) also provides an overview of some applications
    that YARN has enabled to execute on Hadoop, such as Spark and Storm.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章](kindle_split_011.html#ch02) 介绍了 YARN，这是 Hadoop 2 版本中的一个新且令人兴奋的发展，将 Hadoop
    从仅支持 MapReduce 的系统转变为支持多个执行引擎的系统。鉴于 YARN 对社区来说是新的，本章的目标是探讨一些基础知识，例如其组件、配置如何工作，以及
    MapReduce 作为 YARN 应用程序的工作方式。[第二章](kindle_split_011.html#ch02) 还概述了一些 YARN 在 Hadoop
    上启用执行的应用程序，例如 Spark 和 Storm。'
- en: Chapter 1\. Hadoop in a heartbeat
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一章. Hadoop 的心跳
- en: '*This chapter covers*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Examining how the core Hadoop system works
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查核心 Hadoop 系统的工作原理
- en: Understanding the Hadoop ecosystem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Hadoop 生态系统
- en: Running a MapReduce job
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 MapReduce 作业
- en: 'We live in the age of big data, where the data volumes we need to work with
    on a day-to-day basis have outgrown the storage and processing capabilities of
    a single host. Big data brings with it two fundamental challenges: how to store
    and work with voluminous data sizes, and more important, how to understand data
    and turn it into a competitive advantage.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在大数据时代，我们每天需要处理的数据量已经超过了单个主机的存储和处理能力。大数据带来了两个基本挑战：如何存储和处理大量数据，更重要的是，如何理解数据并将其转化为竞争优势。
- en: Hadoop fills a gap in the market by effectively storing and providing computational
    capabilities for substantial amounts of data. It’s a distributed system made up
    of a distributed filesystem, and it offers a way to parallelize and execute programs
    on a cluster of machines (see [figure 1.1](#ch01fig01)). You’ve most likely come
    across Hadoop because it’s been adopted by technology giants like Yahoo!, Facebook,
    and Twitter to address their big data needs, and it’s making inroads across all
    industrial sectors.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 通过有效地存储和提供大量数据的计算能力，在市场上填补了一个空白。它由一个分布式文件系统组成，并提供了一种在机器集群上并行执行程序的方法（参见[图
    1.1](#ch01fig01)）。您可能已经接触过 Hadoop，因为它已被雅虎、Facebook 和 Twitter 等技术巨头采用，以解决它们的大数据需求，并且它在所有工业领域都取得了进展。
- en: Figure 1.1\. The Hadoop environment is a distributed system that runs on commodity
    hardware.
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.1. Hadoop 环境是一个运行在通用硬件上的分布式系统。
- en: '![](01fig01_alt.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig01_alt.jpg)'
- en: Because you’ve come to this book to get some practical experience with Hadoop
    and Java,^([[1](#ch01fn01)]) I’ll start with a brief overview and then show you
    how to install Hadoop and run a MapReduce job. By the end of this chapter, you’ll
    have had a basic refresher on the nuts and bolts of Hadoop, which will allow you
    to move on to the more challenging aspects of working with it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您来到这本书是为了获得一些 Hadoop 和 Java 的实践经验，^([[1](#ch01fn01)]) 我将从简要概述开始，然后向您展示如何安装
    Hadoop 并运行 MapReduce 作业。到本章结束时，您将对 Hadoop 的基本原理有一个基本的复习，这将使您能够继续学习与之相关的更具挑战性的方面。
- en: ¹ To benefit from this book, you should have some practical experience with
    Hadoop and understand the basic concepts of MapReduce and HDFS (covered in Manning’s
    *Hadoop in Action* by Chuck Lam, 2010). Further, you should have an intermediate-level
    knowledge of Java—*Effective Java*, 2nd Edition by Joshua Bloch (Addison-Wesley,
    2008) is an excellent resource on this topic.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 为了从本书中受益，你应该有一些Hadoop的实际经验，并理解MapReduce和HDFS的基本概念（在Chuck Lam的2010年出版的Manning的*Hadoop
    in Action*中介绍）。此外，你应该具备Java的中级知识—Joshua Bloch的*Effective Java*，第2版（Addison-Wesley，2008年）是这方面的优秀资源。
- en: Let’s get started with a detailed overview.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从详细概述开始。
- en: 1.1\. What is Hadoop?
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 什么是Hadoop？
- en: Hadoop is a platform that provides both distributed storage and computational
    capabilities. Hadoop was first conceived to fix a scalability issue that existed
    in Nutch,^([[2](#ch01fn02)]) an open source crawler and search engine. At the
    time, Google had published papers that described its novel distributed filesystem,
    the Google File System (GFS), and MapReduce, a computational framework for parallel
    processing. The successful implementation of these papers’ concepts in Nutch resulted
    in it being split into two separate projects, the second of which became Hadoop,
    a first-class Apache project.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是一个提供分布式存储和计算能力的平台。Hadoop最初是为了解决Nutch中存在的可扩展性问题而构思的，Nutch是一个开源的爬虫和搜索引擎。当时，谷歌发表了描述其新颖的分布式文件系统Google文件系统（GFS）和用于并行处理的计算框架MapReduce的论文。这些论文概念在Nutch中的成功实现导致它被分割成两个独立的项目，其中第二个项目成为Hadoop，一个一流的Apache项目。
- en: ² The Nutch project, and by extension Hadoop, was led by Doug Cutting and Mike
    Cafarella.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² Nutch项目，以及由此扩展的Hadoop，由Doug Cutting和Mike Cafarella领导。
- en: In this section we’ll look at Hadoop from an architectural perspective, examine
    how industry uses it, and consider some of its weaknesses. Once we’ve covered
    this background, we’ll look at how to install Hadoop and run a MapReduce job.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从架构的角度来看Hadoop，考察行业如何使用它，并考虑一些其弱点。一旦我们了解了这些背景知识，我们将探讨如何安装Hadoop并运行一个MapReduce作业。
- en: 'Hadoop proper, as shown in [figure 1.2](#ch01fig02), is a distributed master-slave
    architecture^([[3](#ch01fn03)]) that consists of the following primary components:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图1.2](#ch01fig02)所示，Hadoop本身是一个分布式主从架构^([[3](#ch01fn03)))，它由以下主要组件组成：
- en: ³ A model of communication where one process, called the *master*, has control
    over one or more other processes, called *slaves*.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ 一种通信模型，其中一个进程，称为*主进程*，控制一个或多个其他进程，称为*从进程*。
- en: Figure 1.2\. High-level Hadoop 2 master-slave architecture
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2\. 高级Hadoop 2主从架构
- en: '![](01fig02_alt.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig02_alt.jpg)'
- en: Hadoop Distributed File System (HDFS) for data storage.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统（HDFS）用于数据存储。
- en: Yet Another Resource Negotiator (YARN), introduced in Hadoop 2, a general-purpose
    scheduler and resource manager. Any YARN application can run on a Hadoop cluster.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yet Another Resource Negotiator (YARN)，在Hadoop 2中引入，是一个通用调度器和资源管理器。任何YARN应用程序都可以在Hadoop集群上运行。
- en: MapReduce, a batch-based computational engine. In Hadoop 2, MapReduce is implemented
    as a YARN application.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce，一种基于批处理的计算引擎。在Hadoop 2中，MapReduce被实现为一个YARN应用程序。
- en: Traits intrinsic to Hadoop are data partitioning and parallel computation of
    large datasets. Its storage and computational capabilities scale with the addition
    of hosts to a Hadoop cluster; clusters with hundreds of hosts can easily reach
    data volumes in the petabytes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop固有的特性是数据分区和大数据集的并行计算。其存储和计算能力随着Hadoop集群中主机数量的增加而扩展；拥有数百个主机的集群可以轻松达到PB级的数据量。
- en: In the first step in this section, we’ll examine the HDFS, YARN, and MapReduce
    architectures.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的第一个步骤中，我们将检查HDFS、YARN和MapReduce架构。
- en: 1.1.1\. Core Hadoop components
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.1\. 核心Hadoop组件
- en: To understand Hadoop’s architecture we’ll start by looking at the basics of
    HDFS.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解Hadoop的架构，我们将首先查看HDFS的基本知识。
- en: HDFS
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: HDFS
- en: HDFS is the storage component of Hadoop. It’s a distributed filesystem that’s
    modeled after the Google File System (GFS) paper.^([[4](#ch01fn04)]) HDFS is optimized
    for high throughput and works best when reading and writing large files (gigabytes
    and larger). To support this throughput, HDFS uses unusually large (for a filesystem)
    block sizes and data locality optimizations to reduce network input/output (I/O).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是Hadoop的存储组件。它是一个基于Google文件系统（GFS）论文的分布式文件系统^([[4](#ch01fn04)))。HDFS针对高吞吐量进行了优化，在读取和写入大文件（千兆字节及以上）时表现最佳。为了支持这种吞吐量，HDFS使用异常大的（对于文件系统而言）块大小和数据局部性优化来减少网络输入/输出（I/O）。
- en: ⁴ See “The Google File System,” [http://research.google.com/archive/gfs.html](http://research.google.com/archive/gfs.html).
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ 请参阅“谷歌文件系统”，[http://research.google.com/archive/gfs.html](http://research.google.com/archive/gfs.html)。
- en: Scalability and availability are also key traits of HDFS, achieved in part due
    to data replication and fault tolerance. HDFS replicates files for a configured
    number of times, is tolerant of both software and hardware failure, and automatically
    re-replicates data blocks on nodes that have failed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和可用性也是 HDFS 的关键特性，部分得益于数据复制和容错。HDFS 会根据配置的次数复制文件，对软件和硬件故障都具有容错性，并在节点失败时自动重新复制数据块。
- en: '[Figure 1.3](#ch01fig03) shows a logical representation of the components in
    HDFS: the Name-Node and the DataNode. It also shows an application that’s using
    the Hadoop filesystem library to access HDFS.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1.3](#ch01fig03) 展示了 HDFS 组件的逻辑表示：Name-Node 和 DataNode。它还显示了一个使用 Hadoop
    文件系统库来访问 HDFS 的应用程序。'
- en: Figure 1.3\. An HDFS client communicating with the master NameNode and slave
    DataNodes
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.3\. 一个与主 NameNode 和从属 DataNode 通信的 HDFS 客户端
- en: '![](01fig03_alt.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig03_alt.jpg)'
- en: 'Hadoop 2 introduced two significant new features for HDFS—Federation and High
    Availability (HA):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2 为 HDFS 引入了两个重要的新特性——联邦和可用性（HA）：
- en: Federation allows HDFS metadata to be shared across multiple NameNode hosts,
    which aides with HDFS scalability and also provides data isolation, allowing different
    applications or teams to run their own NameNodes without fear of impacting other
    NameNodes on the same cluster.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联邦允许 HDFS 元数据在多个 NameNode 主机之间共享，这有助于 HDFS 的可扩展性，同时也提供了数据隔离，允许不同的应用程序或团队运行自己的
    NameNode，而不用担心会影响到同一集群中的其他 NameNode。
- en: High Availability in HDFS removes the single point of failure that existed in
    Hadoop 1, wherein a NameNode disaster would result in a cluster outage. HDFS HA
    also offers the ability for failover (the process by which a standby Name-Node
    takes over work from a failed primary NameNode) to be automated.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS 的高可用性消除了 Hadoop 1 中存在的单点故障，即 NameNode 灾难会导致集群故障。HDFS HA 还提供了故障转移（备用 Name-Node
    从失败的 NameNode 接管工作的过程）自动化的能力。
- en: Now that you have a bit of HDFS knowledge, it’s time to look at YARN, Hadoop’s
    scheduler.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 HDFS 有了一些了解，是时候看看 YARN，Hadoop 的调度器了。
- en: YARN
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: YARN
- en: 'YARN is Hadoop’s distributed resource scheduler. YARN is new to Hadoop version
    2 and was created to address challenges with the Hadoop 1 architecture:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 是 Hadoop 的分布式资源调度器。YARN 是 Hadoop 版本 2 中的新功能，旨在解决 Hadoop 1 架构的挑战：
- en: Deployments larger than 4,000 nodes encountered scalability issues, and adding
    additional nodes didn’t yield the expected linear scalability improvements.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大于 4,000 节点的部署遇到了可扩展性问题，增加额外的节点并没有带来预期的线性可扩展性改进。
- en: Only MapReduce workloads were supported, which meant it wasn’t suited to run
    execution models such as machine learning algorithms that often require iterative
    computations.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只支持 MapReduce 工作负载，这意味着它不适合运行需要迭代计算的学习算法等执行模型。
- en: For Hadoop 2 these problems were solved by extracting the scheduling function
    from MapReduce and reworking it into a generic application scheduler, called YARN.
    With this change, Hadoop clusters are no longer limited to running MapReduce workloads;
    YARN enables a new set of workloads to be natively supported on Hadoop, and it
    allows alternative processing models, such as graph processing and stream processing,
    to coexist with MapReduce. [Chapters 2](kindle_split_011.html#ch02) and [10](kindle_split_023.html#ch10)
    cover YARN and how to write YARN applications.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Hadoop 2，这些问题通过从 MapReduce 中提取调度功能并将其重构为一个通用的应用程序调度器（称为 YARN）来解决。这一变化使得 Hadoop
    集群不再仅限于运行 MapReduce 工作负载；YARN 允许在 Hadoop 上原生支持一系列新的工作负载，并允许不同的处理模型，如图处理和流处理，与
    MapReduce 共存。[第 2 章](kindle_split_011.html#ch02) 和 [第 10 章](kindle_split_023.html#ch10)
    讲述了 YARN 以及如何编写 YARN 应用程序。
- en: 'YARN’s architecture is simple because its primary role is to schedule and manage
    resources in a Hadoop cluster. [Figure 1.4](#ch01fig04) shows a logical representation
    of the core components in YARN: the ResourceManager and the NodeManager. Also
    shown are the components specific to YARN applications, namely, the YARN application
    client, the ApplicationMaster, and the container.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 的架构很简单，因为它的主要角色是在 Hadoop 集群中调度和管理资源。[图 1.4](#ch01fig04) 展示了 YARN 核心组件的逻辑表示：ResourceManager
    和 NodeManager。还展示了特定于 YARN 应用程序的组件，即 YARN 应用程序客户端、ApplicationMaster 和容器。
- en: Figure 1.4\. The logical YARN architecture showing typical communication between
    the core YARN components and YARN application components
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.4\. 显示核心 YARN 组件和 YARN 应用程序组件之间典型通信的逻辑 YARN 架构
- en: '![](01fig04_alt.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片01fig04_alt](01fig04_alt.jpg)'
- en: To fully realize the dream of a generalized distributed platform, Hadoop 2 introduced
    another change—the ability to allocate containers in various configurations. Hadoop
    1 had the notion of “slots,” which were a fixed number of map and reduce processes
    that were allowed to run on a single node. This was wasteful in terms of cluster
    utilization and resulted in underutilized resources during MapReduce operations,
    and it also imposed memory limits for map and reduce tasks. With YARN, each container
    requested by an ApplicationMaster can have disparate memory and CPU traits, and
    this gives YARN applications full control over the resources they need to fulfill
    their work.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全实现通用分布式平台的梦想，Hadoop 2 引入了另一个变化——能够在各种配置中分配容器。Hadoop 1 有“槽位”的概念，这是在单个节点上允许运行的固定数量的
    map 和 reduce 进程。这在集群利用率方面是浪费的，并在 MapReduce 操作期间导致资源未充分利用，并且它还为 map 和 reduce 任务设定了内存限制。使用
    YARN，ApplicationMaster 请求的每个容器都可以具有不同的内存和 CPU 特性，这使得 YARN 应用程序对其所需资源有完全的控制权。
- en: You’ll work with YARN in more detail in [chapters 2](kindle_split_011.html#ch02)
    and [10](kindle_split_023.html#ch10), where you’ll learn how YARN works and how
    to write a YARN application. Next up is an examination of MapReduce, Hadoop’s
    computation engine.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在第 [2](kindle_split_011.html#ch02) 章和 [10](kindle_split_023.html#ch10) 章中更详细地了解
    YARN，那里你将学习 YARN 的工作原理以及如何编写 YARN 应用程序。接下来是对 MapReduce 的考察，这是 Hadoop 的计算引擎。
- en: MapReduce
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce
- en: MapReduce is a batch-based, distributed computing framework modeled after Google’s
    paper on MapReduce.^([[5](#ch01fn05)]) It allows you to parallelize work over
    a large amount of raw data, such as combining web logs with relational data from
    an OLTP database to model how users interact with your website. This type of work,
    which could take days or longer using conventional serial programming techniques,
    can be reduced to minutes using MapReduce on a Hadoop cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 是一个基于批处理的、分布式计算框架，其模式是模仿 Google 的 MapReduce 论文。[^([5](#ch01fn05))]
    它允许你在大量原始数据上并行化工作，例如将 Web 日志与来自 OLTP 数据库的关系数据相结合，以模拟用户如何与你的网站互动。这种类型的工作，如果使用传统的串行编程技术，可能需要几天或更长时间，但使用
    Hadoop 集群上的 MapReduce 可以将其缩短到几分钟。
- en: '⁵ See “MapReduce: Simplified Data Processing on Large Clusters,” [http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html).'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ 请参阅“MapReduce：在大型集群上简化的数据处理”，[http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html)。
- en: The MapReduce model simplifies parallel processing by abstracting away the complexities
    involved in working with distributed systems, such as computational parallelization,
    work distribution, and dealing with unreliable hardware and software. With this
    abstraction, MapReduce allows the programmer to focus on addressing business needs
    rather than getting tangled up in distributed system complications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 模型通过抽象掉与分布式系统工作相关的复杂性（如计算并行化、工作分配和处理不可靠的硬件和软件）来简化并行处理。通过这种抽象，MapReduce
    允许程序员专注于解决业务需求，而不是陷入分布式系统复杂性的泥潭。
- en: MapReduce decomposes work submitted by a client into small parallelized map
    and reduce tasks, as shown in [figure 1.5](#ch01fig05). The map and reduce constructs
    used in MapReduce are borrowed from those found in the Lisp functional programming
    language, and they use a shared-nothing model to remove any parallel execution
    interdependencies that could add unwanted synchronization points or state sharing.^([[6](#ch01fn06)])
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 将客户端提交的工作分解成小的并行化 map 和 reduce 任务，如图 1.5 所示。[6](#ch01fn06)] MapReduce
    中使用的 map 和 reduce 构造是从 Lisp 函数式编程语言中借用的，并且它们使用无共享模型来消除任何可能添加不必要同步点或状态共享的并行执行依赖关系。
- en: ⁶ A shared-nothing architecture is a distributed computing concept that represents
    the notion that each node is independent and self-sufficient.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ 无共享架构是一种分布式计算概念，它代表了每个节点都是独立和自给自足的观点。
- en: Figure 1.5\. A client submitting a job to MapReduce, breaking the work into
    small map and reduce tasks
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.5\. 客户端向 MapReduce 提交作业，将工作分解成小的 map 和 reduce 任务
- en: '![](01fig05.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片01fig05](01fig05.jpg)'
- en: The role of the programmer is to define map and reduce functions where the map
    function outputs key/value tuples, which are processed by reduce functions to
    produce the final output. [Figure 1.6](#ch01fig06) shows a pseudocode definition
    of a map function with regard to its input and output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员的角色是定义 map 和 reduce 函数，其中 map 函数输出键/值元组，这些元组由 reduce 函数处理以生成最终输出。[图 1.6](#ch01fig06)显示了关于其输入和输出的
    map 函数的伪代码定义。
- en: Figure 1.6\. A logical view of the map function that takes a key/value pair
    as input
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.6\. 以键/值对作为输入的 map 函数的逻辑视图
- en: '![](01fig06_alt.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig06_alt.jpg)'
- en: The power of MapReduce occurs between the map output and the reduce input in
    the shuffle and sort phases, as shown in [figure 1.7](#ch01fig07).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 的强大之处在于洗牌和排序阶段中映射输出和减少输入之间，如图[图 1.7](#ch01fig07)所示。
- en: Figure 1.7\. MapReduce’s shuffle and sort phases
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.7\. MapReduce 的洗牌和排序阶段
- en: '![](01fig07_alt.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig07_alt.jpg)'
- en: '[Figure 1.8](#ch01fig08) shows a pseudocode definition of a reduce function.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1.8](#ch01fig08)显示了 reduce 函数的伪代码定义。'
- en: Figure 1.8\. A logical view of the reduce function that produces output for
    flat files, NoSQL rows, or any data sink
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.8\. 生成输出文件、NoSQL 行或任何数据目的地的 reduce 函数的逻辑视图
- en: '![](01fig08.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig08.jpg)'
- en: With the advent of YARN in Hadoop 2, MapReduce has been rewritten as a YARN
    application and is now referred to as MapReduce 2 (or MRv2). From a developer’s
    perspective, MapReduce in Hadoop 2 works in much the same way it did in Hadoop
    1, and code written for Hadoop 1 will execute without code changes on version
    2.^([[7](#ch01fn07)]) There are changes to the physical architecture and internal
    plumbing in MRv2 that are examined in more detail in [chapter 2](kindle_split_011.html#ch02).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Hadoop 2 中 YARN 的出现，MapReduce 已被重写为 YARN 应用程序，现在被称为 MapReduce 2（或 MRv2）。从开发者的角度来看，Hadoop
    2 中的 MapReduce 与 Hadoop 1 中的工作方式几乎相同，为 Hadoop 1 编写的代码在版本 2 上无需代码更改即可执行.^([[7](#ch01fn07)])
    MRv2 中的物理架构和内部管道有所变化，这些变化在[第 2 章](kindle_split_011.html#ch02)中进行了更详细的探讨。
- en: ⁷ Some code may require recompilation against Hadoop 2 binaries to work with
    MRv2; see [chapter 2](kindle_split_011.html#ch02) for more details.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ 一些代码可能需要针对 Hadoop 2 二进制文件重新编译才能与 MRv2 一起工作；有关更多详细信息，请参阅[第 2 章](kindle_split_011.html#ch02)。
- en: With some Hadoop basics under your belt, it’s time to take a look at the Hadoop
    ecosystem and the projects that are covered in this book.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了一些 Hadoop 基础知识之后，是时候看看 Hadoop 生态系统以及本书涵盖的项目了。
- en: 1.1.2\. The Hadoop ecosystem
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.2\. Hadoop 生态系统
- en: The Hadoop ecosystem is diverse and grows by the day. It’s impossible to keep
    track of all of the various projects that interact with Hadoop in some form. In
    this book the focus is on the tools that are currently receiving the greatest
    adoption by users, as shown in [figure 1.9](#ch01fig09).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 生态系统多样且每天都在增长。跟踪所有与 Hadoop 以某种形式交互的各种项目是不可能的。本书的重点是用户目前采用率最高的工具，如图[图
    1.9](#ch01fig09)所示。
- en: Figure 1.9\. Hadoop and related technologies that are covered in this book
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.9\. 本书涵盖的 Hadoop 和相关技术
- en: '![](01fig09_alt.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig09_alt.jpg)'
- en: MapReduce and YARN are not for the faint of heart, which means the goal for
    many of these Hadoop-related projects is to increase the accessibility of Hadoop
    to programmers and nonprogrammers. I’ll cover many of the technologies listed
    in [figure 1.9](#ch01fig09) in this book and describe them in detail within their
    respective chapters. In addition, the appendix includes descriptions and installation
    instructions for technologies that are covered in this book.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 和 YARN 并非易事，这意味着许多这些与 Hadoop 相关项目的目标是为程序员和非程序员提高 Hadoop 的可访问性。本书将涵盖[图
    1.9](#ch01fig09)中列出的许多技术，并在各自的章节中详细描述它们。此外，附录还包括了本书涵盖的技术描述和安装说明。
- en: '|  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Coverage of the Hadoop ecosystem in this book
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 本书对 Hadoop 生态系统的覆盖范围
- en: The Hadoop ecosystem grows by the day, and there are often multiple tools with
    overlapping features and benefits. The goal of this book is to provide practical
    techniques that cover the core Hadoop technologies, as well as select ecosystem
    technologies that are ubiquitous and essential to Hadoop.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 生态系统每天都在增长，通常有多种具有重叠功能和优势的工具。本书的目标是提供涵盖核心 Hadoop 技术的实用技术，以及一些普遍且对 Hadoop
    至关重要的生态系统技术。
- en: '|  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s look at the hardware requirements for your cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看您集群的硬件要求。
- en: 1.1.3\. Hardware requirements
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.3\. 硬件要求
- en: The term *commodity hardware* is often used to describe Hadoop hardware requirements.
    It’s true that Hadoop can run on any old servers you can dig up, but you’ll still
    want your cluster to perform well, and you don’t want to swamp your operations
    department with diagnosing and fixing hardware issues. Therefore, *commodity*
    refers to mid-level rack servers with dual sockets, as much error-correcting RAM
    as is affordable, and SATA drives optimized for RAID storage. Using RAID on the
    DataNode filesystems used to store HDFS content is strongly discouraged because
    HDFS already has replication and error-checking built in; on the NameNode, RAID
    is strongly recommended for additional security.^([[8](#ch01fn08)])
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*通用硬件*常用来描述Hadoop的硬件要求。确实，Hadoop可以在你能够找到的任何旧服务器上运行，但你仍然希望你的集群性能良好，并且你不想让你的运维部门忙于诊断和修复硬件问题。因此，*通用*指的是具有双插槽的中端机架服务器，尽可能多的纠错RAM，以及针对RAID存储优化的SATA驱动器。由于HDFS已经内置了复制和错误检查，因此强烈不建议在用于存储HDFS内容的DataNode文件系统上使用RAID；在NameNode上，强烈建议使用RAID以提供额外的安全性.^([[8](#ch01fn08)])
- en: ⁸ HDFS uses disks to durably store metadata about the filesystem.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ HDFS使用磁盘来持久存储关于文件系统的元数据。
- en: From a network topology perspective with regard to switches and firewalls, all
    of the master and slave nodes must be able to open connections to each other.
    For small clusters, all the hosts would run 1 GB network cards connected to a
    single, good-quality switch. For larger clusters, look at 10 GB top-of-rack switches
    that have at least multiple 1 GB uplinks to dual-central switches. Client nodes
    also need to be able to talk to all of the master and slave nodes, but if necessary,
    that access can be from behind a firewall that permits connection establishment
    only from the client side.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络拓扑的角度来看，关于交换机和防火墙，所有主节点和从节点都必须能够相互打开连接。对于小型集群，所有主机都会运行1 GB网络卡，连接到单个高质量交换机。对于大型集群，请考虑具有至少多个1
    GB上行链路的10 GB机架交换机，这些交换机连接到双中央交换机。客户端节点也需要能够与所有主节点和从节点通信，但如果需要，这种访问可以从防火墙后面进行，该防火墙仅允许从客户端建立连接。
- en: After reviewing Hadoop from a software and hardware perspective, you’ve likely
    developed a good idea of who might benefit from using it. Once you start working
    with Hadoop, you’ll need to pick a distribution to use, which is the next topic.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件和硬件的角度回顾了Hadoop之后，你可能已经对谁可能从使用它中受益有了很好的了解。一旦你开始使用Hadoop，你将需要选择一个发行版来使用，这是下一个话题。
- en: 1.1.4\. Hadoop distributions
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.4\. Hadoop发行版
- en: Hadoop is an Apache open source project, and regular releases of the software
    are available for download directly from the Apache project’s website ([http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download)).
    You can either download and install Hadoop from the website or use a quickstart
    virtual machine from a commercial distribution, which is usually a great starting
    point if you’re new to Hadoop and want to quickly get it up and running.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是一个Apache开源项目，软件的常规版本可以直接从Apache项目的网站([http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download))下载。你可以从网站上下载并安装Hadoop，或者使用来自商业发行版的快速启动虚拟机，这对于你是Hadoop新手且希望快速启动和运行来说通常是一个很好的起点。
- en: After you’ve whet your appetite with Hadoop and have committed to using it in
    production, the next question that you’ll need to answer is which distribution
    to use. You can continue to use the vanilla Hadoop distribution, but you’ll have
    to build the in-house expertise to manage your clusters. This is not a trivial
    task and is usually only successful in organizations that are comfortable with
    having dedicated Hadoop DevOps engineers running and managing their clusters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在你用Hadoop开胃之后并决定在生产中使用它之后，你需要回答的下一个问题是使用哪个发行版。你可以继续使用纯Hadoop发行版，但你将需要建立内部专业知识来管理你的集群。这不是一个微不足道的工作，通常只有在那些对拥有专门负责运行和管理其集群的Hadoop
    DevOps工程师感到舒适的组织中才能成功。
- en: Alternatively, you can turn to a commercial distribution of Hadoop, which will
    give you the added benefits of enterprise administration software, a support team
    to consult when planning your clusters or to help you out when things go bump
    in the night, and the possibility of a rapid fix for software issues that you
    encounter. Of course, none of this comes for free (or for cheap!), but if you’re
    running mission-critical services on Hadoop and don’t have a dedicated team to
    support your infrastructure and services, then going with a commercial Hadoop
    distribution is prudent.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以转向Hadoop的商业发行版，这将为您提供企业级管理软件的额外好处，当您规划集群或遇到夜间问题需要帮助时，可以咨询的支持团队，以及快速修复您遇到的软件问题的可能性。当然，这一切都不是免费的（或便宜的！），但如果您在Hadoop上运行关键任务服务，并且没有专门的团队来支持您的基础设施和服务，那么选择商业Hadoop发行版是明智的。
- en: '|  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Picking the distribution that’s right for you
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择适合您的分布
- en: It’s highly recommended that you engage with the major vendors to gain an understanding
    of which distribution suits your needs from a feature, support, and cost perspective.
    Remember that each vendor will highlight their advantages and at the same time
    expose the disadvantages of their competitors, so talking to two or more vendors
    will give you a more realistic sense of what the distributions offer. Make sure
    you download and test the distributions and validate that they integrate and work
    within your existing software and hardware stacks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 非常推荐您与主要供应商合作，从功能、支持和成本的角度了解哪种分布适合您的需求。请记住，每个供应商都会强调他们的优势，同时也会暴露其竞争对手的劣势，因此与两个或更多供应商交谈将使您对分布提供的内容有更现实的了解。确保您下载并测试这些分布，并验证它们是否可以与您现有的软件和硬件堆栈集成和运行。
- en: '|  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: There are a number of distributions to choose from, and in this section I’ll
    briefly summarize each distribution and highlight some of its advantages.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多分布可供选择，在本节中，我将简要总结每种分布并突出其一些优点。
- en: Apache
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Apache
- en: Apache is the organization that maintains the core Hadoop code and distribution,
    and because all the code is open source, you can crack open your favorite IDE
    and browse the source code to understand how things work under the hood. Historically
    the challenge with the Apache distributions has been that support is limited to
    the goodwill of the open source community, and there’s no guarantee that your
    issue will be investigated and fixed. Having said that, the Hadoop community is
    a very supportive one, and responses to problems are usually rapid, even if the
    actual fixes will likely take longer than you may be able to afford.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Apache是维护Hadoop核心代码和分布的组织，由于所有代码都是开源的，您可以使用您最喜欢的IDE打开源代码，了解底层的工作原理。从历史上看，Apache分布的挑战在于支持仅限于开源社区的良好意愿，并且无法保证您的問題会被调查和修复。话虽如此，Hadoop社区是一个非常支持性的社区，对问题的响应通常是快速的，即使实际的修复可能需要更长的时间，您可能无法承担。
- en: The Apache Hadoop distribution has become more compelling now that administration
    has been simplified with the advent of Apache Ambari, which provides a GUI to
    help with provisioning and managing your cluster. As useful as Ambari is, though,
    it’s worth comparing it against offerings from the commercial vendors, as the
    commercial tooling is typically more sophisticated.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Apache Ambari的出现，管理得到了简化，Apache Hadoop分布现在更具吸引力，它提供了一个GUI来帮助配置和管理您的集群。尽管Ambari非常有用，但将其与商业供应商的产品进行比较是值得的，因为商业工具通常更为复杂。
- en: Cloudera
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Cloudera
- en: Cloudera is the most tenured Hadoop distribution, and it employs a large number
    of Hadoop (and Hadoop ecosystem) committers. Doug Cutting, who along with Mike
    Caferella originally created Hadoop, is the chief architect at Cloudera. In aggregate,
    this means that bug fixes and feature requests have a better chance of being addressed
    in Cloudera compared to Hadoop distributions with fewer committers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera是最资深的Hadoop分布，它雇佣了大量的Hadoop（以及Hadoop生态系统）提交者。与Mike Cafarella共同最初创建Hadoop的Doug
    Cutting现在是Cloudera的首席架构师。总的来说，这意味着与提交者较少的Hadoop分布相比，在Cloudera中解决错误修复和功能请求的机会更大。
- en: 'Beyond maintaining and supporting Hadoop, Cloudera has been innovating in the
    Hadoop space by developing projects that address areas where Hadoop has been weak.
    A prime example of this is Impala, which offers a SQL-on-Hadoop system, similar
    to Hive but focusing on a near-real-time user experience, as opposed to Hive,
    which has traditionally been a high-latency system. There are numerous other projects
    that Cloudera has been working on: highlights include Flume, a log collection
    and distribution system; Sqoop, for moving relational data in and out of Hadoop;
    and Cloudera Search, which offers near-real-time search indexing.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了维护和支持Hadoop之外，Cloudera通过开发解决Hadoop弱点领域的项目，在Hadoop空间进行了创新。一个典型的例子是Impala，它提供了一个基于Hadoop的SQL系统，类似于Hive，但专注于近似实时用户体验，而Hive传统上是一个高延迟系统。Cloudera还在进行许多其他项目：亮点包括Flume，一个日志收集和分发系统；Sqoop，用于在Hadoop中移动关系型数据；以及Cloudera
    Search，它提供近似实时的搜索索引。
- en: Hortonworks
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hortonworks
- en: Hortonworks is also made up of a large number of Hadoop committers, and it offers
    the same advantages as Cloudera in terms of the ability to quickly address problems
    and feature requests in core Hadoop and its ecosystem projects.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Hortonworks由大量Hadoop提交者组成，它在快速解决核心Hadoop及其生态系统项目的问题和功能请求方面提供了与Cloudera相同的优势。
- en: 'From an innovation perspective, Hortonworks has taken a slightly different
    approach than Cloudera. An example is Hive: Cloudera’s approach was to develop
    a whole new SQL-on-Hadoop system, but Hortonworks has instead looked at innovating
    inside of Hive to remove its high-latency shackles and add new capabilities such
    as support for ACID. Hortonworks is also the main driver behind the next-generation
    YARN platform, which is a key strategic piece keeping Hadoop relevant. Similarly,
    Horton-works has used Apache Ambari for its administration tooling rather than
    developing an in-house proprietary administration tool, which is the path taken
    by the other distributions. Hortonworks’ focus on developing and expanding the
    Apache ecosystem tooling has a direct benefit to the community, as it makes its
    tools available to all users without the need for support contracts.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从创新的角度来看，Hortonworks采取了与Cloudera略有不同的方法。一个例子是Hive：Cloudera的方法是开发一个全新的基于Hadoop的SQL系统，但Hortonworks则着眼于在Hive内部进行创新，以去除其高延迟的束缚，并添加新的功能，如对ACID的支持。Hortonworks也是下一代YARN平台的主要推动者，这是保持Hadoop相关性的关键战略部分。同样，Hortonworks使用Apache
    Ambari作为其管理工具，而不是开发内部专有管理工具，这是其他发行版所采取的道路。Hortonworks专注于开发和扩展Apache生态系统工具，这对社区有直接的好处，因为它使其工具对所有用户可用，无需支持合同。
- en: MapR
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapR
- en: MapR has fewer Hadoop committers on its team than the other distributions discussed
    here, so its ability to fix and shape Hadoop’s future is potentially more bounded
    than its peers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MapR在其团队中的Hadoop提交者比这里讨论的其他发行版要少，因此其修复和塑造Hadoop未来的能力可能比其同行更有限。
- en: From an innovation perspective, MapR has taken a decidedly different approach
    to Hadoop support compared to its peers. From the start it decided that HDFS wasn’t
    an enterprise-ready filesystem, and instead developed its own proprietary filesystem,
    which offers compelling features such as POSIX compliance (offering random-write
    support and atomic operations), High Availability, NFS mounting, data mirroring,
    and snapshots. Some of these features have been introduced into Hadoop 2, but
    MapR has offered them from the start, and, as a result, one can expect that these
    features are robust.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从创新的角度来看，MapR在Hadoop支持方面采取了与同行截然不同的方法。从一开始，它就决定HDFS不是一个企业级文件系统，因此开发了它自己的专有文件系统，该系统提供了引人注目的功能，如POSIX兼容性（提供随机写入支持和原子操作）、高可用性、NFS挂载、数据镜像和快照。其中一些功能已引入到Hadoop
    2中，但MapR从一开始就提供了这些功能，因此可以预期这些功能是稳健的。
- en: As part of the evaluation criteria, it should be noted that parts of the MapR
    stack, such as its filesystem and its HBase offering, are closed source and proprietary.
    This affects the ability of your engineers to browse, fix, and contribute patches
    back to the community. In contrast, most of Cloudera’s and Hortonworks’ stacks
    are open source, especially Hortonworks’, which is unique in that the entire stack,
    including the management platform, is open source.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为评估标准的一部分，需要注意的是，MapR堆栈的部分，如其文件系统和其HBase提供的产品，是封闭源代码和专有性质。这影响了您的工程师浏览、修复并向社区贡献补丁的能力。相比之下，Cloudera和Hortonworks的堆栈大多是开源的，尤其是Hortonworks，它在整个堆栈，包括管理平台，都是开源的。
- en: MapR’s notable highlights include being made available in Amazon’s cloud as
    an alternative to Amazon’s own Elastic MapReduce and being integrated with Google’s
    Compute Cloud.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: MapR 的显著亮点包括作为 Amazon 自家的 Elastic MapReduce 的替代方案在 Amazon 的云平台上提供，以及与 Google
    的 Compute Cloud 集成。
- en: I’ve just scratched the surface of the advantages that the various Hadoop distributions
    offer; your next steps will likely be to contact the vendors and start playing
    with the distributions yourself.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是刚刚触及了各种 Hadoop 发行版提供的优势的表面；你的下一步可能将是联系供应商，并开始自己尝试这些发行版。
- en: Next, let’s take a look at companies currently using Hadoop, and in what capacity
    they’re using it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看目前使用 Hadoop 的公司，以及它们使用 Hadoop 的方式。
- en: 1.1.5\. Who’s using Hadoop?
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.5\. 谁在使用 Hadoop？
- en: Hadoop has a high level of penetration in high-tech companies, and it’s starting
    to make inroads in a broad range of sectors, including the enterprise (Booz Allen
    Hamilton, J.P. Morgan), government (NSA), and health care.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 在高科技公司中的渗透率很高，它开始进入包括企业（Booz Allen Hamilton、J.P. Morgan）、政府（NSA）和医疗保健在内的广泛领域。
- en: Facebook uses Hadoop, Hive, and HBase for data warehousing and real-time application
    serving.^([[9](#ch01fn09)]) Facebook’s data warehousing clusters are petabytes
    in size with thousands of nodes, and they use separate HBase-driven, real-time
    clusters for messaging and real-time analytics.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook 使用 Hadoop、Hive 和 HBase 进行数据仓库和实时应用程序服务。[9](#ch01fn09) Facebook 的数据仓库集群规模达到千兆字节，拥有数千个节点，并且它们使用基于
    HBase 的独立实时集群进行消息传递和实时分析。
- en: '⁹ See Dhruba Borthakur, “Looking at the code behind our three uses of Apache
    Hadoop” on Facebook at [http://mng.bz/4cMc](http://mng.bz/4cMc). Facebook has
    also developed its own SQL-on-Hadoop tool called Presto and is migrating away
    from Hive (see Martin Traverso, “Presto: Interacting with petabytes of data at
    Facebook,” [http://mng.bz/p0Xz](http://mng.bz/p0Xz)).'
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([9](#ch01fn09)) 请参阅 Dhruba Borthakur 在 Facebook 上发表的“查看我们使用 Apache Hadoop
    的代码背后的情况”文章，“[http://mng.bz/4cMc](http://mng.bz/4cMc)”。Facebook 还开发了其自己的 SQL-on-Hadoop
    工具 Presto，并正在从 Hive 迁移（参见 Martin Traverso 的“Presto：在 Facebook 上交互 PB 级数据”，[http://mng.bz/p0Xz](http://mng.bz/p0Xz)）。
- en: Yahoo! uses Hadoop for data analytics, machine learning, search ranking, email
    antispam, ad optimization, ETL,^([[10](#ch01fn10)]) and more. Combined, it has
    over 40,000 servers running Hadoop with 170 PB of storage. Yahoo! is also running
    the first large-scale YARN deployments with clusters of up to 4,000 nodes.^([[11](#ch01fn11)])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Yahoo! 使用 Hadoop 进行数据分析、机器学习、搜索排名、电子邮件反垃圾邮件、广告优化、ETL 等。总计，它拥有超过 40,000 台运行 Hadoop
    的服务器，存储容量达到 170 PB。Yahoo! 还在运行第一个大规模的 YARN 部署，集群节点数高达 4,000 个。[11](#ch01fn11)
- en: ^(10) Extract, transform, and load (ETL) is the process by which data is extracted
    from outside sources, transformed to fit the project’s needs, and loaded into
    the target data sink. ETL is a common process in data warehousing.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([10](#ch01fn10)) 提取、转换和加载（ETL）是从外部源提取数据，将其转换为满足项目需求，并将其加载到目标数据存储的过程。ETL 是数据仓库中的常见过程。
- en: '^(11) There are more details on YARN and its use at Yahoo! in “Apache Hadoop
    YARN: Yet Another Resource Negotiator” by Vinod Kumar Vavilapalli et al., [www.cs.cmu.edu/~garth/15719/papers/yarn.pdf](http://www.cs.cmu.edu/~garth/15719/papers/yarn.pdf).'
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([11](#ch01fn11)) 关于 YARN 及其在 Yahoo! 的使用的更多细节，请参阅 Vinod Kumar Vavilapalli 等人撰写的《Apache
    Hadoop YARN：另一个资源协调器》，“[www.cs.cmu.edu/~garth/15719/papers/yarn.pdf](http://www.cs.cmu.edu/~garth/15719/papers/yarn.pdf)”。
- en: Twitter is a major big data innovator, and it has made notable contributions
    to Hadoop with projects such as Scalding, a Scala API for Cascading; Summingbird,
    a component that can be used to implement parts of Nathan Marz’s lambda architecture;
    and various other gems such as Bijection, Algebird, and Elephant Bird.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 是一个主要的大数据创新者，它通过 Scalding（Cascading 的 Scala API）、Summingbird（可用于实现 Nathan
    Marz 的 lambda 架构的部分组件）以及其他各种宝石（如 Bijection、Algebird 和 Elephant Bird）等项目对 Hadoop
    做出了显著的贡献。
- en: eBay, Samsung, Rackspace, J.P. Morgan, Groupon, LinkedIn, AOL, Spotify, and
    StumbleUpon are some other organizations that are also heavily invested in Hadoop.
    Microsoft has collaborated with Hortonworks to ensure that Hadoop works on its
    platform.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: eBay、Samsung、Rackspace、J.P. Morgan、Groupon、LinkedIn、AOL、Spotify 和 StumbleUpon
    等其他组织也在 Hadoop 上进行了大量投资。微软与 Hortonworks 合作，以确保 Hadoop 在其平台上运行。
- en: Google, in its MapReduce paper, indicated that it uses Caffeine,^([[12](#ch01fn12)])
    its version of MapReduce, to create its web index from crawl data. Google also
    highlights applications of MapReduce to include activities such as a distributed
    grep, URL access frequency (from log data), and a term-vector algorithm, which
    determines popular keywords for a host.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在其 MapReduce 论文中指出，它使用自己的 MapReduce 版本 Caffeine^([12](#ch01fn12)) 从爬取数据创建其网页索引。谷歌还强调了
    MapReduce 的应用，包括分布式 grep、URL 访问频率（来自日志数据）和术语向量算法，该算法确定主机的热门关键词。
- en: '^(12) In 2010 Google moved to a real-time indexing system called Caffeine;
    see “Our new search index: Caffeine” on the Google blog (June 8, 2010), [http://googleblog.blogspot.com/2010/06/our-new-search-index-caffeine.html](http://googleblog.blogspot.com/2010/06/our-new-search-index-caffeine.html).'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([12](#ch01fn12)) 2010 年，谷歌转向了一个名为 Caffeine 的实时索引系统；请参阅谷歌博客上的“我们的新搜索索引：Caffeine”（2010
    年 6 月 8 日），[http://googleblog.blogspot.com/2010/06/our-new-search-index-caffeine.html](http://googleblog.blogspot.com/2010/06/our-new-search-index-caffeine.html)。
- en: The number of organizations that use Hadoop grows by the day, and if you work
    at a Fortune 500 company you almost certainly use a Hadoop cluster in some capacity.
    It’s clear that as Hadoop continues to mature, its adoption will continue to grow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 每天使用 Hadoop 的组织数量都在增长，如果你在一家财富 500 强公司工作，你几乎肯定会在某种程度上使用 Hadoop 集群。很明显，随着 Hadoop
    的不断成熟，其采用率将继续增长。
- en: As with all technologies, a key part to being able to work effectively with
    Hadoop is to understand its shortcomings and design and architect your solutions
    to mitigate these as much as possible.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有技术一样，能够有效地使用 Hadoop 的关键部分是了解其不足之处，并设计和架构你的解决方案以尽可能多地减轻这些不足。
- en: 1.1.6\. Hadoop limitations
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.6\. Hadoop 的局限性
- en: High availability and security often rank among the top concerns cited with
    Hadoop. Many of these concerns have been addressed in Hadoop 2; let’s take a closer
    look at some of its weaknesses as of release 2.2.0.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性和安全性通常是人们提到 Hadoop 时最关心的几个问题之一。许多这些问题在 Hadoop 2 中都得到了解决；让我们更详细地看看截至 2.2.0
    版本的一些弱点。
- en: Enterprise organizations using Hadoop 1 and earlier had concerns with the lack
    of high availability and security. In Hadoop 1, all of the master processes are
    single points of failure, which means that a failure in the master process causes
    an outage. In Hadoop 2, HDFS now has high availability support, and the re-architecture
    of Map-Reduce with YARN has removed the single point of failure. Security is another
    area that has had its wrinkles, and it’s receiving focus.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hadoop 1 及更早版本的企业组织对高可用性和安全性缺乏感到担忧。在 Hadoop 1 中，所有主进程都是单点故障，这意味着主进程的故障会导致系统停机。在
    Hadoop 2 中，HDFS 现在有了高可用性支持，并且 Map-Reduce 与 YARN 的重新架构消除了单点故障。安全性是另一个存在问题的领域，并且正在受到关注。
- en: High availability
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高可用性
- en: High availability is often mandated in enterprise organizations that have high
    uptime SLA requirements to ensure that systems are always on, even in the event
    of a node going down due to planned or unplanned circumstances. Prior to Hadoop
    2, the master HDFS process could only run on a single node, resulting in single
    points of failure.^([[13](#ch01fn13)]) Hadoop 2 brings NameNode High Availability
    (HA) support, which means that multiple NameNodes for the same Hadoop cluster
    can be running. With the current design, one of the NameNodes is active and the
    other NameNode is designated as a standby process. In the event that the active
    NameNode experiences a planned or unplanned outage, the standby NameNode will
    take over as the active NameNode, which is a process called *failover*. This failover
    can be configured to be automatic, negating the need for human intervention. The
    fact that a NameNode failover occurred is transparent to Hadoop clients.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性通常在企业组织中强制执行，这些组织有高正常运行时间 SLA 要求，以确保系统始终处于运行状态，即使在节点因计划内或计划外的情况而宕机的情况下也是如此。在
    Hadoop 2 之前，主 HDFS 进程只能在单个节点上运行，这导致了单点故障^([13](#ch01fn13))。Hadoop 2 带来了 NameNode
    高可用性（HA）支持，这意味着同一个 Hadoop 集群可以有多个 NameNode 运行。按照当前的设计，一个 NameNode 是活动的，另一个 NameNode
    被指定为备用进程。如果活动 NameNode 发生计划内或计划外的停机，备用 NameNode 将接管作为活动 NameNode，这个过程称为 *故障转移*。这个故障转移可以被配置为自动进行，从而无需人工干预。NameNode
    故障转移的发生对 Hadoop 客户端是透明的。
- en: ^(13) In reality, the HDFS single point of failure may not be terribly significant;
    see “NameNode HA” by Suresh Srinivas and Aaron T. Myers, [http://goo.gl/1iSab](http://goo.gl/1iSab).
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([13](#ch01fn13)) 实际上，HDFS 的单点故障可能并不特别严重；请参阅 Suresh Srinivas 和 Aaron T. Myers
    的“NameNode HA”，[http://goo.gl/1iSab](http://goo.gl/1iSab)。
- en: The MapReduce master process (the JobTracker) doesn’t have HA support in Hadoop
    2, but now that each MapReduce job has its own JobTracker process (a separate
    YARN ApplicationMaster), HA support is arguably less important.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce主进程（JobTracker）在Hadoop 2中没有高可用性支持，但现在每个MapReduce作业都有自己的JobTracker进程（一个独立的YARN
    ApplicationMaster），因此高可用性支持的重要性可能有所降低。
- en: HA support in the YARN master process (the ResourceManager) is important, however,
    and development is currently underway to add this feature to Hadoop.^([[14](#ch01fn14)])
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: YARN主进程（ResourceManager）中的高可用性支持很重要，目前正在进行开发，以将此功能添加到Hadoop中.^([[14](#ch01fn14)])
- en: ^(14) For additional details on YARN HA support, see the JIRA ticket titled
    “ResourceManager (RM) High-Availability (HA),” [https://issues.apache.org/jira/browse/YARN-149](https://issues.apache.org/jira/browse/YARN-149).
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（14）有关YARN高可用性支持的详细信息，请参阅标题为“ResourceManager (RM) 高可用性 (HA)”的JIRA工单[https://issues.apache.org/jira/browse/YARN-149](https://issues.apache.org/jira/browse/YARN-149)。
- en: Multiple datacenters
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多数据中心
- en: Multiple datacenter support is another key feature that’s increasingly expected
    in enterprise software, as it offers strong data protection and locality properties
    due to data being replicated across multiple datacenters. Apache Hadoop, and most
    of its commercial distributions, has never had support for multiple datacenters,
    which poses challenges for organizations that have software running in multiple
    datacenters. WANdisco is currently the only solution available for Hadoop multidatacenter
    support.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 多数据中心支持是企业软件中越来越被期待的关键特性之一，因为它通过在多个数据中心复制数据提供了强大的数据保护和地域属性。Apache Hadoop及其大多数商业发行版从未支持过多数据中心，这对在多个数据中心运行软件的组织构成了挑战。WANdisco是目前唯一可用的Hadoop多数据中心支持解决方案。
- en: Security
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安全性
- en: Hadoop does offer a security model, but by default it’s disabled. With the security
    model disabled, the only security feature that exists in Hadoop is HDFS file-
    and directory-level ownership and permissions. But it’s easy for malicious users
    to subvert and assume other users’ identities. By default, all other Hadoop services
    are wide open, allowing any user to perform any kind of operation, such as killing
    another user’s MapReduce jobs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop提供了一个安全模型，但默认情况下是禁用的。安全模型禁用时，Hadoop中存在的唯一安全特性是HDFS文件和目录级别的所有权和权限。但恶意用户很容易绕过并假设其他用户的身份。默认情况下，所有其他Hadoop服务都是开放的，允许任何用户执行任何类型的操作，例如终止另一个用户的MapReduce作业。
- en: Hadoop can be configured to run with Kerberos, a network authentication protocol,
    which requires Hadoop daemons to authenticate clients, both users and other Hadoop
    components. Kerberos can be integrated with an organization’s existing Active
    Directory and therefore offers a single-sign-on experience for users. Care needs
    to be taken when enabling Kerberos, as any Hadoop tool that wishes to interact
    with your cluster will need to support Kerberos.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop可以配置为使用Kerberos，这是一种网络认证协议，它要求Hadoop守护进程对客户端进行认证，包括用户和其他Hadoop组件。Kerberos可以与组织的现有Active
    Directory集成，因此为用户提供单一登录体验。启用Kerberos时需要小心，因为任何希望与您的集群交互的Hadoop工具都需要支持Kerberos。
- en: Wire-level encryption can be configured in Hadoop 2 and allows data crossing
    the network (both HDFS transport^([[15](#ch01fn15)]) and MapReduce shuffle data^([[16](#ch01fn16)]))
    to be encrypted. Encryption of data at rest (data stored by HDFS on disk) is currently
    missing in Hadoop.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 2中可以配置网络级别的加密，允许跨网络传输的数据（包括HDFS传输^[[15](#ch01fn15)]和MapReduce shuffle数据^[[16](#ch01fn16)]）被加密。目前Hadoop中缺少对静态数据（HDFS存储在磁盘上的数据）的加密。
- en: ^(15) See the JIRA ticket titled “Add support for encrypting the DataTransferProtocol”
    at [https://issues.apache.org/jira/browse/HDFS-3637](https://issues.apache.org/jira/browse/HDFS-3637).
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（15）有关添加对加密数据传输协议的支持的详细信息，请参阅标题为“添加对加密数据传输协议的支持”的JIRA工单[https://issues.apache.org/jira/browse/HDFS-3637](https://issues.apache.org/jira/browse/HDFS-3637)。
- en: ^(16) See the JIRA ticket titled “Add support for encrypted shuffle” at [https://issues.apache.org/jira/browse/MAPREDUCE-4417](https://issues.apache.org/jira/browse/MAPREDUCE-4417).
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（16）有关YARN高可用性支持的详细信息，请参阅标题为“添加对加密shuffle的支持”的JIRA工单[https://issues.apache.org/jira/browse/MAPREDUCE-4417](https://issues.apache.org/jira/browse/MAPREDUCE-4417)。
- en: Let’s examine the limitations of some of the individual systems.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些个别系统的局限性。
- en: HDFS
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: HDFS
- en: The weakness of HDFS is mainly its lack of high availability (in Hadoop 1.x
    and earlier), its inefficient handling of small files,^([[17](#ch01fn17)]) and
    its lack of transparent compression. HDFS doesn’t support random writes into files
    (only appends are supported), and it’s generally designed to support high-throughput
    sequential reads and writes over large files.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 的弱点主要是其缺乏高可用性（在 Hadoop 1.x 及更早版本中），对小文件处理效率低下，^([[17](#ch01fn17)]) 以及缺乏透明的压缩。HDFS
    不支持对文件的随机写入（仅支持追加），并且通常设计为支持对大文件的顺序读写，具有高吞吐量。
- en: ^(17) Although HDFS Federation in Hadoop 2 has introduced a way for multiple
    NameNodes to share file metadata, the fact remains that metadata is stored in
    memory.
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([17](#ch01fn17)) 尽管 Hadoop 2 中的 HDFS 联邦引入了多个 NameNode 共享文件元数据的方法，但事实仍然是元数据存储在内存中。
- en: MapReduce
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce
- en: MapReduce is a batch-based architecture, which means it doesn’t lend itself
    to use cases that need real-time data access. Tasks that require global synchronization
    or sharing of mutable data aren’t a good fit for MapReduce, because it’s a shared-nothing
    architecture, which can pose challenges for some algorithms.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 是一种基于批处理的架构，这意味着它不适合需要实时数据访问的使用场景。需要全局同步或共享可变数据的任务不适合 MapReduce，因为它是一个无共享架构，这可能会对某些算法造成挑战。
- en: Version incompatibilities
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 版本不兼容性
- en: The Hadoop 2 release brought with it some headaches with regard to MapReduce
    API runtime compatibility, especially in the `org.hadoop.mapreduce` package. These
    problems often result in runtime issues with code that’s compiled against Hadoop
    1 (and earlier). The solution is usually to recompile against Hadoop 2, or to
    consider a technique outlined in [chapter 2](kindle_split_011.html#ch02) that
    introduces a compatibility library to target both Hadoop versions without the
    need to recompile code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2 版本的发布带来了与 MapReduce API 运行时兼容性的一些问题，尤其是在 `org.hadoop.mapreduce` 包中。这些问题通常会导致针对
    Hadoop 1（及更早版本）编译的代码在运行时出现问题。解决方案通常是重新编译以针对 Hadoop 2，或者考虑在 [第 2 章](kindle_split_011.html#ch02)
    中概述的技术，该技术介绍了一个兼容性库，以便在不重新编译代码的情况下针对两个 Hadoop 版本。
- en: Other challenges with Hive and Hadoop also exist, where Hive may need to be
    recompiled to work with versions of Hadoop other than the one it was built against.
    Pig has had compatibility issues, too. For example, the Pig 0.8 release didn’t
    work with Hadoop 0.20.203, and manual intervention was required to work around
    this problem. This is one of the advantages of using a Hadoop distribution other
    than Apache, as these compatibility problems have been fixed. If using the vanilla
    Apache distributions is desired, it’s worth taking a look at Bigtop ([http://bigtop.apache.org/](http://bigtop.apache.org/)),
    an Apache open source automated build and compliance system. It includes all of
    the major Hadoop ecosystem components and runs a number of integration tests to
    ensure they all work in conjunction with each other.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 和 Hadoop 之间也存在其他挑战，其中 Hive 可能需要重新编译才能与构建时使用的 Hadoop 版本以外的版本兼容。Pig 也存在兼容性问题。例如，Pig
    0.8 版本与 Hadoop 0.20.203 不兼容，需要手动干预才能解决这个问题。使用 Apache 之外的 Hadoop 发行版的一个优点是，这些兼容性问题已经得到解决。如果希望使用纯
    Apache 发行版，那么查看 Bigtop ([http://bigtop.apache.org/](http://bigtop.apache.org/))
    是值得的，这是一个 Apache 开源自动化构建和合规性系统。它包括所有主要的 Hadoop 生态系统组件，并运行一系列集成测试，以确保它们可以协同工作。
- en: After tackling Hadoop’s architecture and its weaknesses, you’re probably ready
    to roll up your sleeves and get hands-on with Hadoop, so let’s look at running
    the first example in this book.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决 Hadoop 架构及其弱点之后，您可能已经准备好卷起袖子，亲自动手使用 Hadoop，因此让我们看看如何运行本书中的第一个示例。
- en: 1.2\. Getting your hands dirty with MapReduce
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 深入了解 MapReduce
- en: This section shows you how to run a MapReduce job on your host.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本节向您展示如何在您的主机上运行 MapReduce 作业。
- en: '|  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Installing Hadoop and building the examples
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安装 Hadoop 和构建示例
- en: To run the code example in this section, you’ll need to follow the instructions
    in the appendix, which explain how to install Hadoop and download and run the
    examples bundled with this book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本节中的代码示例，您需要遵循附录中的说明，这些说明解释了如何安装 Hadoop 以及下载和运行本书附带示例。
- en: '|  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s say you want to build an inverted index. MapReduce would be a good choice
    for this task because it can create indexes in parallel (a common MapReduce use
    case). Your input is a number of text files, and your output is a list of tuples,
    where each tuple is a word and a list of files that contain the word. Using standard
    processing techniques, this would require you to find a mechanism to join all
    the words together. A naive approach would be to perform this join in memory,
    but you might run out of memory if you have large numbers of unique keys. You
    could use an intermediary datastore, such as a database, but that would be inefficient.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想构建一个倒排索引。MapReduce对于这个任务是一个不错的选择，因为它可以并行创建索引（这是MapReduce的常见用途）。您的输入是一系列文本文件，您的输出是一个元组列表，其中每个元组是一个单词和包含该单词的文件列表。使用标准处理技术，这需要您找到一种机制来连接所有单词。一个简单的方法是在内存中执行这个连接，但您可能会因为具有大量唯一键而耗尽内存。您可以使用中间数据存储，例如数据库，但这将是不高效的。
- en: A better approach would be to tokenize each line and produce an intermediary
    file containing a word per line. Each of these intermediary files could then be
    sorted. The final step would be to open all the sorted intermediary files and
    call a function for each unique word. This is what MapReduce does, albeit in a
    distributed fashion.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的方法是逐行分词，并生成一个包含每行一个单词的中间文件。然后，可以对这些中间文件进行排序。最后一步是打开所有排序后的中间文件，并对每个唯一单词调用一个函数。这正是MapReduce所做的事情，尽管是以分布式的方式。
- en: '[Figure 1.10](#ch01fig10) walks you through an example of a simple inverted
    index in MapReduce. Let’s start by defining your mapper. Your reducers need to
    be able to generate a line for each word in your input, so your map output key
    should be each word in the input files so that MapReduce can join them all together.
    The value for each key will be the containing filename, which is your document
    ID.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.10](#ch01fig10) 带您了解MapReduce中简单倒排索引的示例。首先，定义您的mapper。您的reducers需要能够为输入中的每个单词生成一行，因此您的map输出键应该是输入文件中的每个单词，以便MapReduce可以将它们全部连接起来。每个键的值将是包含的文件名，即您的文档ID。'
- en: Figure 1.10\. An example of an inverted index being created in MapReduce
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.10. MapReduce中创建倒排索引的示例
- en: '![](01fig10_alt.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig10_alt.jpg)'
- en: 'This is the mapper code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是mapper代码：
- en: '![](019fig01_alt.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](019fig01_alt.jpg)'
- en: 'The goal of this reducer is to create an output line for each word and a list
    of the document IDs in which the word appears. The MapReduce framework will take
    care of calling the reducer once per unique key outputted by the mappers, along
    with a list of document IDs. All you need to do in the reducer is combine all
    the document IDs together and output them once in the reducer, as you can see
    in the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个reducer的目标是为每个单词和出现该单词的文档ID列表创建一个输出行。MapReduce框架将负责为mappers输出的每个唯一键调用reducer一次，以及一个文档ID列表。在reducer中，您需要做的就是将所有文档ID合并在一起，并在reducer中一次性输出，如下面的代码所示：
- en: '![](019fig02_alt.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](019fig02_alt.jpg)'
- en: The last step is to write the driver code that will set all the necessary properties
    to configure the MapReduce job to run. You need to let the framework know what
    classes should be used for the map and reduce functions, and also let it know
    where the input and output data is located. By default, MapReduce assumes you’re
    working with text; if you’re working with more complex text structures, or altogether
    different datastorage technologies, you’ll need to tell MapReduce how it should
    read and write from these data sources and sinks. The following source shows the
    full driver code:^([[18](#ch01fn18)])
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是编写驱动代码，该代码将设置所有必要的属性以配置MapReduce作业的运行。您需要让框架知道应该使用哪些类来处理map和reduce函数，并且还需要让它知道输入和输出数据的位置。默认情况下，MapReduce假设您正在处理文本；如果您正在处理更复杂的文本结构或完全不同的数据存储技术，您需要告诉MapReduce它应该如何从这些数据源和接收器中读取和写入。以下示例显示了完整的驱动代码^([[18](#ch01fn18)]).
- en: '^(18) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch1/InvertedIndexJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch1/InvertedIndexJob.java).'
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([18]) GitHub源：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch1/InvertedIndexJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch1/InvertedIndexJob.java).
- en: '![](020fig01_alt.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](020fig01_alt.jpg)'
- en: 'Let’s see how this code works. First, you need to create two simple input files
    in HDFS:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这段代码是如何工作的。首先，您需要在HDFS中创建两个简单的输入文件：
- en: '![](020fig02_alt.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](020fig02_alt.jpg)'
- en: 'Next, run the MapReduce code. You’ll use a shell script to run it, supplying
    the two input files as arguments, along with the job output directory:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行MapReduce代码。你将使用shell脚本来运行它，将两个输入文件作为参数传递，以及作业输出目录：
- en: '[PRE0]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Executing code examples in the book
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 执行书中的代码示例
- en: The appendix contains instructions for downloading and installing the binaries
    and code that accompany this book. Most of the examples are launched via the `hip`
    script, which is located inside the bin directory. For convenience, it’s recommended
    that you add the book’s bin directory to your path so that you can copy-paste
    all the example commands as is. The appendix has instructions on how to set up
    your environment.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 附录包含了下载和安装本书附带二进制文件和代码的说明。大多数示例都是通过位于bin目录中的`hip`脚本启动的。为了方便，建议将书的bin目录添加到你的路径中，这样你就可以直接复制粘贴所有示例命令。附录有如何设置你的环境的说明。
- en: '|  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'When your job completes, you can examine HDFS for the job output files and
    view their contents:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的作业完成后，你可以检查HDFS中的作业输出文件并查看其内容：
- en: '[PRE1]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This completes your whirlwind tour of how to run Hadoop.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了你对如何运行Hadoop的快速浏览。
- en: 1.3\. Chapter summary
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 章节总结
- en: Hadoop is a distributed system designed to process, generate, and store large
    datasets. Its MapReduce implementation provides you with a fault-tolerant mechanism
    for large-scale data analysis of heterogeneous structured and unstructured data
    sources, and YARN now supports multi-tenant disparate applications on the same
    Hadoop cluster.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是一个分布式系统，旨在处理、生成和存储大数据集。其MapReduce实现为你提供了一个容错机制，用于大规模分析异构的半结构化和非结构化数据源，而YARN现在支持在同一Hadoop集群上运行多租户不同应用。
- en: In this chapter, we examined Hadoop from both functional and physical architectural
    standpoints. You also installed Hadoop and ran a MapReduce job.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从功能和物理架构的角度分析了Hadoop。你还安装了Hadoop并运行了一个MapReduce作业。
- en: The remainder of this book is dedicated to presenting real-world techniques
    for solving common problems you’ll encounter when working with Hadoop. You’ll
    be introduced to a broad spectrum of subject areas, starting with YARN, HDFS and
    MapReduce, and Hive. You’ll also look at data-analysis techniques and explore
    technologies such as Mahout and Rhipe.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本书剩余部分致力于介绍解决你在使用Hadoop时遇到的一些常见问题的实际技术。你将接触到广泛的主题领域，从YARN、HDFS和MapReduce开始，再到Hive。你还将了解数据分析技术，并探索如Mahout和Rhipe等技术。
- en: In [chapter 2](kindle_split_011.html#ch02), the first stop on your journey,
    you’ll discover YARN, which heralds a new era for Hadoop, one that transforms
    Hadoop into a distributed processing kernel. Without further ado, let’s get started.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](kindle_split_011.html#ch02)中，你旅程的第一站，你将发现YARN，它预示着Hadoop新时代的到来，将Hadoop转变为分布式处理内核。无需多言，让我们开始吧。
- en: Chapter 2\. Introduction to YARN
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章\. YARN简介
- en: '*This chapter covers*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding how YARN works
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解YARN的工作原理
- en: How MapReduce works as a YARN application
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce作为YARN应用的工作原理
- en: A look at other YARN applications
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他YARN应用的概述
- en: 'Imagine buying your first car, which upon delivery has a steering wheel that
    doesn’t function and brakes that don’t work. Oh, and it only drives in first gear.
    No speeding on winding back roads for you! That empty, sad feeling is familiar
    to those of us who want to run some cool new tech such as graph or real-time data
    processing with Hadoop 1,^([[1](#ch02fn01)]) only to be reminded that our powerful
    Hadoop clusters were good for one thing, and one thing only: MapReduce.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你买了一辆第一辆车，交付时方向盘不工作，刹车也不工作。哦，而且它只能用一档行驶。在蜿蜒的山路上不能超速！对于那些想使用Hadoop 1运行一些酷炫的新技术，如图或实时数据处理的人来说，这种空虚、悲伤的感觉是熟悉的，只会提醒我们，我们强大的Hadoop集群只擅长一件事，那就是MapReduce。
- en: ¹ While you can do graph processing in Hadoop 1, it’s not a native fit, which
    means you’re either incurring the inefficiencies of multiple disk barriers between
    each iteration on your graph, or hacking around in MapReduce to avoid such barriers.
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 虽然你可以在Hadoop 1中进行图处理，但这并不是原生的，这意味着你可能在图的每次迭代之间都要承受多个磁盘屏障的低效，或者通过MapReduce进行修改以避免这些屏障。
- en: Luckily for us the Hadoop committers took these and other constraints to heart
    and dreamt up a vision that would metamorphose Hadoop above and beyond MapReduce.
    YARN is the realization of this dream, and it’s an exciting new development that
    transitions Hadoop into a distributed computing kernel that can support any type
    of workload.^([[2](#ch02fn02)]) This opens up the types of applications that can
    be run on Hadoop to efficiently support computing models for machine learning,
    graph processing, and other generalized computing projects (such as Tez), which
    are discussed later in this chapter
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Hadoop 的贡献者将这些以及其他限制因素铭记在心，并构想出一个将 Hadoop 转变为超越 MapReduce 的愿景。YARN 就是这个梦想的实现，它是一个令人兴奋的新发展，将
    Hadoop 转变为可以支持任何类型工作负载的分布式计算内核。[^([2](#ch02fn02))] 这使得可以在 Hadoop 上运行的应用程序类型得以扩展，以有效地支持机器学习、图处理和其他通用计算项目（如
    Tez）的计算模型，这些内容将在本章后面讨论。
- en: ² Prior to YARN, Hadoop only supported MapReduce for computational work.
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 在 YARN 之前，Hadoop 只支持 MapReduce 进行计算工作。
- en: The upshot of all this is that you can now run MapReduce, Storm, and HBase all
    on a single Hadoop cluster. This allows for exciting new possibilities, not only
    in computational multi-tenancy, but also in the ability to efficiently share data
    between applications.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些的最终结果是，你现在可以在单个 Hadoop 集群上运行 MapReduce、Storm 和 HBase。这不仅为计算多租户提供了新的可能性，而且还有效地共享数据的能力。
- en: Because YARN is a new technology, we’ll kick off this chapter with a look at
    how YARN works, followed by a section that covers how to interact with YARN from
    the command line and the UI. Combined, these sections will give you a good grasp
    of what YARN is and how to use it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 YARN 是一项新技术，我们将从本章的开头部分开始，探讨 YARN 的工作原理，随后将介绍如何从命令行和用户界面与 YARN 交互。这两部分结合起来，将帮助你更好地理解
    YARN 是什么以及如何使用它。
- en: Once you have a good handle on how YARN works, you’ll see how MapReduce has
    been rewritten to be a YARN application (titled MapReduce 2, or MRv2), and look
    at some of the architectural and systems changes that occurred in MapReduce to
    make this happen. This will help you better understand how to work with MapReduce
    in Hadoop 2 and give you some background into why some aspects of MapReduce changed
    in version 2.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解了 YARN 的工作原理，你将看到 MapReduce 如何被重写为 YARN 应用程序（称为 MapReduce 2，或 MRv2），并查看
    MapReduce 中发生的某些架构和系统变化，以实现这一目标。这将帮助你更好地理解如何在 Hadoop 2 中使用 MapReduce，并为你提供一些关于为什么
    MapReduce 在版本 2 中某些方面发生变化的原因背景。
- en: '|  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: YARN development
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: YARN 开发
- en: If you’re looking for details on how to write YARN applications, feel free to
    skip to [chapter 10](kindle_split_023.html#ch10). But if you’re new to YARN, I
    recommend you read this chapter before you move on to [chapter 10](kindle_split_023.html#ch10).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找如何编写 YARN 应用的详细信息，可以自由地跳转到第 10 章。[chapter 10](kindle_split_023.html#ch10)。但如果你是
    YARN 的初学者，我建议你在继续阅读第 10 章[chapter 10](kindle_split_023.html#ch10)之前先阅读本章。
- en: '|  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the final section of this chapter, you’ll examine several YARN applications
    and their practical uses.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，你将检查几个 YARN 应用程序及其实际用途。
- en: Let’s get things started with an overview of YARN.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 YARN 的概述开始。
- en: 2.1\. YARN overview
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. YARN 概述
- en: With Hadoop 1 and older versions, you were limited to only running MapReduce
    jobs. This was great if the type of work you were performing fit well into the
    MapReduce processing model, but it was restrictive for those wanting to perform
    graph processing, iterative computing, or any other type of work.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 1 和更早的版本中，你只能运行 MapReduce 作业。如果你执行的工作类型非常适合 MapReduce 处理模型，这很好，但对于想要执行图处理、迭代计算或其他类型工作的人来说，这很受限制。
- en: In Hadoop 2 the scheduling pieces of MapReduce were externalized and reworked
    into a new component called YARN, which is short for *Yet Another Resource Negotiator*.
    YARN is agnostic to the type of work you do on Hadoop—all that it requires is
    that applications that wish to operate on Hadoop are implemented as YARN applications.
    As a result, MapReduce is now a YARN application. The old and new Hadoop stacks
    can be seen in [figure 2.1](#ch02fig01).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 2 中，MapReduce 的调度部分被外部化并重新设计为一个名为 YARN 的新组件，YARN 是 *Yet Another Resource
    Negotiator* 的缩写。YARN 对你在 Hadoop 上执行的工作类型是中立的——它只要求希望运行在 Hadoop 上的应用程序以 YARN 应用程序的形式实现。因此，MapReduce
    现在是一个 YARN 应用程序。旧的和新的 Hadoop 堆栈可以在[图 2.1](#ch02fig01)中看到。
- en: Figure 2.1\. Hadoop 1 and 2 architectures, showing YARN as a generalized scheduler
    and various YARN applications
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.1\. Hadoop 1 和 2 架构，显示 YARN 作为通用调度器和各种 YARN 应用程序
- en: '![](02fig01_alt.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig01_alt.jpg)'
- en: There are multiple benefits to this architectural change, which you’ll examine
    in the next section.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构变化带来了多方面的好处，你将在下一节中对其进行探讨。
- en: 2.1.1\. Why YARN?
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 为什么是YARN？
- en: We’ve touched on how YARN enables work other than MapReduce to be performed
    on Hadoop, but let’s expand on that and also look at additional advantages that
    YARN brings to the table.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了YARN如何使除了MapReduce之外的工作能够在Hadoop上执行，但让我们进一步探讨这一点，并看看YARN带来的其他优势。
- en: MapReduce is a powerful distributed framework and programming model that allows
    batch-based parallelized work to be performed on a cluster of multiple nodes.
    Despite being very efficient at what it does, though, MapReduce has some disadvantages;
    principally that it’s batch-based, and as a result isn’t suited to real-time or
    even near-real-time data processing. Historically this has meant that processing
    models such as graph, iterative, and real-time data processing are not a natural
    fit for MapReduce.^([[3](#ch02fn03)])
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是一个强大的分布式框架和编程模型，它允许在多个节点集群上执行基于批次的并行化工作。尽管它在所做的工作上非常高效，但MapReduce也有一些缺点；主要缺点是它是基于批次的，因此不适合实时或甚至接近实时的数据处理。从历史上看，这意味着图、迭代和实时数据处理等处理模型并不是MapReduce的自然选择.^([[3](#ch02fn03)])
- en: ³ HBase is an exception; it uses HDFS for storage but doesn’t use MapReduce
    for the processing engine.
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ HBase是一个例外；它使用HDFS进行存储，但不使用MapReduce作为处理引擎。
- en: The bottom line is that Hadoop version 1 restricts you from running exciting
    new processing frameworks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是，Hadoop版本1限制了你可以运行令人兴奋的新处理框架。
- en: 'YARN changes all of this by taking over the scheduling portions of MapReduce,
    and nothing else. At its core, YARN is a distributed scheduler and is responsible
    for two activities:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: YARN通过接管MapReduce的调度部分，改变了这一切，而仅此而已。在其核心，YARN是一个分布式调度器，负责两项活动：
- en: '***Responding to a client’s request to create a container*** —A container is
    in essence a process, with a contract governing the physical resources that it’s
    permitted to use.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***响应客户端创建容器的请求*** —容器本质上是一个进程，有一个合同规定了它被允许使用的物理资源。'
- en: '***Monitoring containers that are running, and terminating them if needed***
    —Containers can be terminated if a YARN scheduler wants to free up resources so
    that containers from other applications can run, or if a container is using more
    than its allocated resources.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***监控正在运行的容器，并在需要时终止它们*** —如果YARN调度器想要释放资源以便其他应用程序的容器可以运行，或者如果容器使用了超过其分配的资源，容器可以被终止。'
- en: '[Table 2.1](#ch02table01) compares MapReduce 1 and YARN (in Hadoop versions
    1 and 2) to show why YARN is such a revolutionary jump.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2.1](#ch02table01)比较了MapReduce 1和YARN（在Hadoop版本1和2中），以展示为什么YARN是一个如此革命性的飞跃。'
- en: Table 2.1\. Comparison of MapReduce 1 and YARN
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.1\. MapReduce 1和YARN的比较
- en: '| Capability | MapReduce 1 | YARN |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | MapReduce 1 | YARN |'
- en: '| --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Execution model | Only MapReduce is supported on Hadoop 1, limiting the types
    of activities you can perform to batch-based flows that fit within the confines
    of the MapReduce processing model. | YARN places no restrictions on the type of
    work that can be executed in Hadoop; you pick which execution engines you need
    (whether it’s real-time processing with Spark, graph processing with Giraph, or
    MapReduce batch processing), and they can all be executing in parallel on the
    same cluster. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 执行模型 | 在Hadoop 1中仅支持MapReduce，这限制了你可以执行的活动类型，只能是在MapReduce处理模型范围内的基于批次的流程。
    | YARN对在Hadoop中可以执行的工作类型没有任何限制；你可以选择你需要哪种执行引擎（无论是使用Spark进行实时处理，使用Giraph进行图处理，还是使用MapReduce进行批量处理），并且它们都可以在同一个集群上并行执行。
    |'
- en: '| Concurrent processes | MapReduce had the notion of “slots,” which were node-specific
    static configurations that determined the maximum number of map and reduce processes
    that could run concurrently on each node. Based on where in the lifecycle a MapReduce
    application was, this would often lead to underutilized clusters. | YARN allows
    for more fluid resource allocation, and the number of processes is limited only
    by the configured maximum amount of memory and CPU for each node. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 并发进程 | MapReduce有“槽位”的概念，这些是节点特定的静态配置，决定了每个节点上可以同时运行的map和reduce进程的最大数量。根据MapReduce应用程序的生命周期阶段，这通常会导致集群利用率不足。
    | YARN允许更灵活的资源分配，进程的数量仅受每个节点配置的最大内存和CPU数量的限制。 |'
- en: '| Memory limits | Slots in Hadoop 1 also had a maximum limit, so typically
    Hadoop 1 clusters were provisioned such that the number of slots multiplied by
    the maximum configured memory for each slot was less than the available RAM. This
    often resulted in smaller than desired maximum slot memory sizes, which impeded
    your ability to run memory-intensive jobs.^([[a](#ch02fn04)]) Another drawback
    of MRv1 was that it was more difficult for memory-intensive and IO-intensive jobs
    to coexist on the same cluster or machines. Either you had more slots to boost
    the I/O jobs, or fewer slots but more RAM for RAM jobs. Once again, the static
    nature of these slots made it a challenge to tune clusters for mixed workloads.
    | YARN allows applications to request resources of varying memory sizes. YARN
    has minimum and maximum memory limits, but because the number of slots is no longer
    fixed, the maximum values can be much larger to support memory-intensive workloads.
    YARN therefore provides a much more dynamic scheduling model that doesn’t limit
    the number of processes or the amount of RAM requested by a process. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 内存限制 | Hadoop 1 中的槽位也有最大限制，因此通常 Hadoop 1 集群配置得使得槽位数量乘以每个槽位配置的最大内存小于可用的 RAM。这通常会导致小于期望的最大槽位内存大小，从而阻碍了你运行内存密集型作业的能力。[^[[a](#ch02fn04)]]
    MRv1 的另一个缺点是，内存密集型和 I/O 密集型作业在同一集群或机器上共存变得更加困难。要么你有更多的槽位来提升 I/O 作业，要么有更少的槽位但更多的
    RAM 用于 RAM 作业。再次，这些槽位的静态性质使得为混合工作负载调整集群成为一项挑战。YARN 允许应用程序请求不同内存大小的资源。YARN 有最小和最大内存限制，但由于槽位数量不再固定，最大值可以大得多，以支持内存密集型工作负载。因此，YARN
    提供了一个更加动态的调度模型，不会限制进程的数量或进程请求的 RAM 量。|'
- en: '| Scalability | There were concurrency issues with the Job-Tracker, which limited
    the number of nodes in a Hadoop cluster to 3,000–4,000 nodes. | By separating
    out the scheduling parts of MapReduce into YARN and making it lightweight by delegating
    fault tolerance to YARN applications, YARN can scale to much larger numbers than
    prior versions of Hadoop.^([[b](#ch02fn05)]) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | Job-Tracker 存在并发问题，这限制了 Hadoop 集群中的节点数量在 3,000-4,000 个之间。| 通过将 MapReduce
    的调度部分分离到 YARN 中，并通过将容错委托给 YARN 应用程序来使其轻量级，YARN 可以扩展到比 Hadoop 早期版本大得多的数量。[^[[b](#ch02fn05)]]|'
- en: '| Execution | Only a single version of MapReduce could be supported on a cluster
    at a time. This was problematic in large multi-tenant environments where product
    teams that wanted to upgrade to newer versions of MapReduce had to convince all
    the other users. This typically resulted in huge coordination and integration
    efforts and made such upgrades huge infrastructure projects. | MapReduce is no
    longer at the core of Hadoop, and is now a YARN application running in user space.
    This means that you can now run different versions of MapReduce on the same cluster
    at the same time. This is a huge productivity gain in large multi-tenant environments,
    and it allows you to organizationally decouple product teams and roadmaps. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 执行 | 在集群中同一时间只能支持一个版本的 MapReduce。这在大型多租户环境中是个问题，因为希望升级到 MapReduce 新版本的产品团队必须说服所有其他用户。这通常会导致巨大的协调和集成工作，使得这样的升级成为巨大的基础设施项目。MapReduce
    已不再是 Hadoop 的核心，现在是一个运行在用户空间中的 YARN 应用程序。这意味着你现在可以在同一集群上同时运行不同版本的 MapReduce。这在大型多租户环境中是一个巨大的生产力提升，并允许你组织上解耦产品团队和路线图。|'
- en: ^a This limitation in MapReduce was especially painful for those running machine-learning
    tasks using tools such as Mahout, as they often required large amounts of RAM
    for processing—amounts often larger than the maximum configured slot size in MapReduce.
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a 对于使用 Mahout 等工具运行机器学习任务的人来说，MapReduce 中的这种限制尤其痛苦，因为这些工具通常需要大量的 RAM 进行处理——通常比
    MapReduce 中配置的最大槽位大小还要大。
- en: ^b The goal of YARN is to be able to scale to 10,000 nodes; scaling beyond that
    number could result in the ResourceManager becoming a bottleneck, as it’s a single
    process.
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^b YARN 的目标是能够扩展到 10,000 个节点；超过这个数量可能会导致 ResourceManager 成为瓶颈，因为它是一个单一进程。
- en: Now that you know about the key benefits of YARN, it’s time to look at the main
    components in YARN and examine their roles.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 YARN 的关键优势，是时候查看 YARN 的主要组件并检查它们的作用了。
- en: 2.1.2\. YARN concepts and components
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2. YARN 概念和组件
- en: YARN comprises a framework that’s responsible for resource scheduling and monitoring,
    and applications that execute application-specific logic in a cluster. Let’s examine
    YARN concepts and components in more detail, starting with the YARN framework
    components.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 包含一个负责资源调度和监控的框架，以及一些在集群中执行特定逻辑的应用程序。让我们更详细地考察 YARN 的概念和组件，从 YARN 框架组件开始。
- en: YARN framework
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: YARN 框架
- en: 'The YARN framework performs one primary function, which is to schedule resources
    (*containers* in YARN parlance) in a cluster. Applications in a cluster talk to
    the YARN framework, asking for application-specific containers to be allocated,
    and the YARN framework evaluates these requests and attempts to fulfill them.
    An important part of the YARN scheduling also includes monitoring currently executing
    containers. There are two reasons that container monitoring is important: Once
    a container has completed, the scheduler can then use freed-up capacity to schedule
    more work. Additionally, each container has a contract that specifies the system
    resources that it’s allowed to use, and in cases where containers overstep these
    bounds, the scheduler can terminate the container to avoid rogue containers impacting
    other applications.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 框架执行一个主要功能，即在集群中调度资源（在 YARN 术语中称为 *容器*）。集群中的应用程序与 YARN 框架通信，请求分配特定应用的容器，YARN
    框架评估这些请求并尝试满足它们。YARN 调度的另一个重要部分包括监控当前正在执行的容器。容器监控之所以重要，有两个原因：一旦容器完成，调度器就可以使用释放出的容量来调度更多的工作。此外，每个容器都有一个合同，指定了它允许使用的系统资源，在容器超出这些界限的情况下，调度器可以终止容器以避免恶意容器影响其他应用程序。
- en: The YARN framework was intentionally designed to be as simple as possible; as
    such, it doesn’t know or care about the type of applications that are running.
    Nor does it care about keeping any historical information about what has executed
    on the cluster. These design decisions are the primary reasons that YARN can scale
    beyond the levels of MapReduce.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 框架被有意设计得尽可能简单；因此，它不知道或关心正在运行的应用程序类型。它也不关心保留关于集群上执行的历史信息。这些设计决策是 YARN 能够超越
    MapReduce 层级扩展的主要原因。
- en: There are two primary components that comprise the YARN framework—the ResourceManager
    and the NodeManager—which are seen in [figure 2.2](#ch02fig02).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 框架由两个主要组件组成，即 ResourceManager 和 NodeManager，这在 [图 2.2](#ch02fig02) 中可以看到。
- en: Figure 2.2\. YARN framework components and their interactions. Application-specific
    components, such as the YARN client, ApplicationMaster, and containers are not
    shown.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.2\. YARN 框架组件及其交互。未显示特定应用组件，如 YARN 客户端、ApplicationMaster 和容器。
- en: '![](02fig02_alt.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig02_alt.jpg)'
- en: '***ResourceManager*** —A Hadoop cluster has a single *ResourceManager* (RM)
    for the entire cluster. The ResourceManager is the YARN master process, and its
    sole function is to arbitrate resources on a Hadoop cluster. It responds to client
    requests to create containers, and a scheduler determines when and where a container
    can be created according to scheduler-specific multi-tenancy rules that govern
    who can create containers where and when. Just like with Hadoop 1, the scheduler
    part of the ResourceManager is pluggable, which means that you can pick the scheduler
    that works best for your environment. The actual creation of containers is delegated
    to the NodeManager.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ResourceManager*** —一个 Hadoop 集群中有一个用于整个集群的单一 *ResourceManager*（RM）。ResourceManager
    是 YARN 的主进程，其唯一功能是在 Hadoop 集群中仲裁资源。它响应客户端创建容器的请求，调度器根据调度器特定的多租户规则确定何时何地可以创建容器，这些规则规定了谁可以在何时何地创建容器。就像
    Hadoop 1 一样，ResourceManager 的调度器部分是可插拔的，这意味着你可以选择最适合你环境的调度器。实际创建容器的任务委托给 NodeManager。'
- en: '***NodeManager*** —The *NodeManager* is the slave process that runs on every
    node in a cluster. Its job is to create, monitor, and kill containers. It services
    requests from the ResourceManager and ApplicationMaster to create containers,
    and it reports on the status of the containers to the ResourceManager. The ResourceManager
    uses the data contained in these status messages to make scheduling decisions
    for new container requests.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***NodeManager*** —*NodeManager* 是在每个集群节点上运行的从进程。其任务是创建、监控和终止容器。它服务 ResourceManager
    和 ApplicationMaster 的容器创建请求，并向 ResourceManager 报告容器的状态。ResourceManager 使用这些状态消息中包含的数据来为新容器请求做出调度决策。'
- en: In non-HA mode, only a single instance of the ResourceManager exists.^([[4](#ch02fn06)])
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在非HA模式下，只存在一个资源管理器的实例。[4](#ch02fn06)
- en: ⁴ As of the time of writing, YARN ResourceManager HA is still actively being
    developed, and its progress can be followed on a JIRA ticket titled “ResourceManager
    (RM) High-Availability (HA),” [https://issues.apache.org/jira/browse/YARN-149](https://issues.apache.org/jira/browse/YARN-149).
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ 在撰写本文时，YARN资源管理器的高可用性（HA）仍在积极开发中，其进展可以在名为“资源管理器（RM）高可用性（HA）”的JIRA工单上跟踪，[https://issues.apache.org/jira/browse/YARN-149](https://issues.apache.org/jira/browse/YARN-149)。
- en: The YARN framework exists to manage applications, so let’s take a look at what
    components a YARN application is composed of.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: YARN框架的存在是为了管理应用程序，因此让我们看看一个YARN应用程序由哪些组件组成。
- en: YARN applications
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: YARN应用程序
- en: A YARN application implements a specific function that runs on Hadoop. MapReduce
    is an example of a YARN application, as are projects such as Hoya, which allows
    multiple HBase instances to run on a single cluster, and storm-yarn, which allows
    Storm to run inside a Hadoop cluster. You’ll see more details on these projects
    and other YARN applications later in this chapter.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一个YARN应用程序实现了一个在Hadoop上运行的具体功能。MapReduce是YARN应用程序的一个例子，Hoya项目也是如此，它允许多个HBase实例在单个集群上运行，以及storm-yarn，它允许Storm在Hadoop集群内部运行。你将在本章后面看到这些项目和其它YARN应用程序的更多细节。
- en: A YARN application involves three components—the client, the ApplicationMaster
    (AM), and the container, which can be seen in [figure 2.3](#ch02fig03).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一个YARN应用程序涉及三个组件——客户端、应用程序主控（ApplicationMaster，简称AM）和容器，这些可以在[图2.3](#ch02fig03)中看到。
- en: Figure 2.3\. Typical interactions of a YARN application
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3\. YARN应用程序的典型交互
- en: '![](02fig03_alt.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![02fig03_alt.jpg](02fig03_alt.jpg)'
- en: Launching a new YARN application starts with a YARN client communicating with
    the ResourceManager to create a new YARN ApplicationMaster instance. Part of this
    process involves the YARN client informing the ResourceManager of the ApplicationMaster’s
    physical resource requirements.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个新的YARN应用程序从YARN客户端与资源管理器通信以创建一个新的YARN应用程序主控实例开始。这个过程的一部分涉及YARN客户端通知资源管理器应用程序主控的物理资源需求。
- en: 'The *ApplicationMaster* is the master process of a YARN application. It doesn’t
    perform any application-specific work, as these functions are delegated to the
    containers. Instead, it’s responsible for managing the application-specific containers:
    asking the ResourceManager of its intent to create containers and then liaising
    with the NodeManager to actually perform the container creation.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*应用程序主控*是YARN应用程序的主进程。它不执行任何特定于应用程序的工作，因为这些功能被委托给容器。相反，它负责管理特定于应用程序的容器：向资源管理器表明其创建容器的意图，然后与节点管理器协商以实际执行容器创建。'
- en: As part of this process, the ApplicationMaster must specify the resources that
    each container requires in terms of which host should launch the container and
    what the container’s memory and CPU requirements are.^([[5](#ch02fn07)]) The ability
    of the ResourceManager to schedule work based on exact resource requirements is
    a key to YARN’s flexibility, and it enables hosts to run a mix of containers,
    as highlighted in [figure 2.4](#ch02fig04).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个过程的一部分，应用程序主控必须指定每个容器所需的资源，包括哪个主机应该启动容器以及容器的内存和CPU需求。[5](#ch02fn07) 资源管理器根据确切资源需求调度工作的能力是YARN灵活性的关键，它使得主机能够运行容器的混合配置，如[图2.4](#ch02fig04)所示。
- en: ⁵ Future versions of Hadoop may allow network, disk, and GPU requirements to
    be specified.
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ Hadoop的未来版本可能允许指定网络、磁盘和GPU的要求。
- en: Figure 2.4\. Various container configurations running on a single YARN-managed
    Hadoop node
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4\. 在单个YARN管理的Hadoop节点上运行的多种容器配置
- en: '![](02fig04_alt.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![02fig04_alt.jpg](02fig04_alt.jpg)'
- en: The ApplicationMaster is also responsible for the specific fault-tolerance behavior
    of the application. It receives status messages from the ResourceManager when
    its containers fail, and it can decide to take action based on these events (by
    asking the ResourceManager to create a new container), or to ignore these events.^([[6](#ch02fn08)])
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序主控还负责应用程序的具体容错行为。当其容器失败时，它会从资源管理器接收状态消息，并且可以根据这些事件（通过请求资源管理器创建一个新的容器）采取行动，或者忽略这些事件。[6](#ch02fn08)
- en: ⁶ Containers can fail for a variety of reasons, including a node going down,
    a container being killed by YARN to allow another application’s container to be
    launched, or YARN killing a container when the container exceeds its configured
    physical/virtual memory.
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ 容器可能因多种原因失败，包括节点故障、YARN杀死容器以允许启动另一个应用的容器，或者当容器超出其配置的物理/虚拟内存时，YARN杀死容器。
- en: A *container* is an application-specific process that’s created by a NodeManager
    on behalf of an ApplicationMaster. The ApplicationManager itself is also a container,
    created by the ResourceManager. A container created by an ApplicationManager can
    be an arbitrary process—for example, a container process could simply be a Linux
    command such as `awk`, a Python application, or any process that can be launched
    by the operating system. This is the power of YARN—the ability to launch and manage
    any process across any node in a Hadoop cluster.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*容器*是由NodeManager代表ApplicationMaster创建的应用特定进程。ApplicationManager本身也是一个容器，由ResourceManager创建。由ApplicationManager创建的容器可以是一个任意进程——例如，容器进程可能只是一个Linux命令，如`awk`，一个Python应用程序，或任何操作系统可以启动的进程。这是YARN的力量——能够在Hadoop集群的任何节点上启动和管理任何进程。'
- en: By this point, you should have a high-level understanding of the YARN components
    and what they do. Next we’ll look at common YARN configurables.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对YARN组件及其功能有一个高层次的理解。接下来，我们将探讨常见的YARN可配置项。
- en: 2.1.3\. YARN configuration
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. YARN配置
- en: YARN brings with it a whole slew of configurations for various components, such
    as the UI, remote procedure calls (RPCs), the scheduler, and more.^([[7](#ch02fn09)])
    In this section, you’ll learn how you can quickly access your running cluster’s
    configuration.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: YARN带来了大量针对各种组件的配置，例如UI、远程过程调用（RPCs）、调度器等。[^[[7](#ch02fn09)]] 在本节中，你将了解如何快速访问运行中的集群的配置。
- en: ⁷ Details on the default YARN configurations can be seen at [http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml).
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ 默认YARN配置的详细信息可以在[http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)中查看。
- en: Technique 1 Determining the configuration of your cluster
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧1 确定集群的配置
- en: Figuring out the configuration for a running Hadoop cluster can be a nuisance—it
    often requires looking at several configuration files, including the default configuration
    files, to determine the value for the property you’re interested in. In this technique,
    you’ll see how to sidestep the hoops you normally need to jump through, and instead
    focus on how to expediently get at the configuration of a running Hadoop cluster.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 确定运行中的Hadoop集群的配置可能很麻烦——通常需要查看多个配置文件，包括默认配置文件，以确定你感兴趣的属性的值。在这个技巧中，你将看到如何绕过通常需要跳过的圈子，并专注于如何迅速获取运行中的Hadoop集群的配置。
- en: Problem
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to access the configuration of a running Hadoop cluster.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你想访问运行中的Hadoop集群的配置。
- en: Solution
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: View the configuration using the ResourceManager UI.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ResourceManager UI查看配置。
- en: Discussion
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The ResourceManager UI shows the configuration for your Hadoop cluster; [figure
    2.5](#ch02fig05) shows how you can navigate to this information.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ResourceManager UI显示了你的Hadoop集群的配置；[图2.5](#ch02fig05)显示了如何导航到这些信息。
- en: Figure 2.5\. The YARN ResourceManager UI showing the cluster’s configuration
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5. YARN ResourceManager UI显示集群的配置
- en: '![](02fig05_alt.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![02fig05_alt.jpg](02fig05_alt.jpg)'
- en: What’s useful about this feature is that the UI shows not only a property value,
    but also which file it originated from. If the value wasn’t defined in a <component>-site.xml
    file, then it’ll show the default value and the default filename.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特性的有用之处在于，UI不仅显示了属性值，还显示了它来自哪个文件。如果值没有在<component>-site.xml文件中定义，它将显示默认值和默认文件名。
- en: Another useful feature of this UI is that it’ll show you the configuration from
    multiple files, including the core, HDFS, YARN, and MapReduce files.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 此UI的另一个有用功能是它会显示来自多个文件的配置，包括核心、HDFS、YARN和MapReduce文件。
- en: The configuration for an individual Hadoop slave node can be navigated to in
    the same way from the NodeManager UI. This is most helpful when working with Hadoop
    clusters that consist of heterogeneous nodes, where you often have varying configurations
    that cater to differing hardware resources.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过NodeManager UI以相同的方式导航到单个Hadoop从节点上的配置。这在处理由异构节点组成的Hadoop集群时非常有用，您经常需要不同的配置来适应不同的硬件资源。
- en: By this point, you should have a high-level understanding of the YARN components,
    what they do, and how to configure them for your cluster. The next step is to
    actually see YARN in action by using the command line and the UI.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该对YARN组件有一个高级的了解，包括它们的功能以及如何为您的集群配置它们。下一步是实际通过命令行和UI来查看YARN的实际运行情况。
- en: 2.1.4\. Interacting with YARN
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 与YARN交互
- en: 'Out of the box, Hadoop 2 is bundled with two YARN applications—MapReduce 2
    and DistributedShell. You’ll learn more about MapReduce 2 later in this chapter,
    but for now, you can get your toes wet by taking a look at a simpler example of
    a YARN application: the DistributedShell. You’ll see how to run your first YARN
    application and where to go to examine the logs.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Hadoop 2捆绑了两个YARN应用程序——MapReduce 2和DistributedShell。您将在本章后面了解更多关于MapReduce
    2的内容，但就目前而言，您可以通过查看一个更简单的YARN应用程序示例来尝试一下：DistributedShell。您将了解如何运行第一个YARN应用程序以及在哪里检查日志。
- en: 'If you don’t know the configured values for your cluster, you have two options:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不知道集群的配置值，您有两个选择：
- en: Examine the contents of yarn-site.xml to view the property values. If an entry
    doesn’t exist, the default value will be in effect.^([[8](#ch02fn10)])
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查yarn-site.xml的内容以查看属性值。如果条目不存在，则默认值将生效.^([[8](#ch02fn10)])
- en: '⁸ Visit the following URL for YARN default values: [http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml).'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ 访问以下网址以获取YARN默认值：[http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)。
- en: Even better, use the ResourceManager UI, which gives you more detailed information
    on the running configuration, including what the default values are and if they’re
    in effect.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的是，使用ResourceManager UI，它提供了关于运行配置的更详细信息，包括默认值是什么以及它们是否生效。
- en: Let’s now take a look at how to quickly view the YARN configuration for a running
    Hadoop cluster.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何快速查看运行中的Hadoop集群的YARN配置。
- en: Technique 2 Running a command on your YARN cluster
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧2 在您的YARN集群上运行命令
- en: Running a command on your cluster is a good first step when you start working
    with a new YARN cluster. It’s the “hello world” in YARN, if you will.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始使用新的YARN集群时，在集群上运行命令是一个很好的第一步。如果您愿意，这是YARN中的“hello world”。
- en: Problem
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to run a Linux command on a node in your Hadoop cluster.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 您想在Hadoop集群中的某个节点上运行Linux命令。
- en: Solution
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the DistributedShell example application bundled with Hadoop.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop捆绑的DistributedShell示例应用程序。
- en: Discussion
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'YARN is bundled with the DistributedShell application, which serves two primary
    purposes—it’s a reference YARN application that’s also a handy utility for running
    a command in parallel across your Hadoop cluster. Start by issuing a Linux `find`
    command in a single container:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: YARN捆绑了DistributedShell应用程序，它有两个主要用途——它是一个参考YARN应用程序，也是一个方便的实用程序，可以在您的Hadoop集群上并行运行命令。首先，在一个容器中发出Linux
    `find` 命令：
- en: '![](031fig01_alt.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](031fig01_alt.jpg)'
- en: 'If all is well with your cluster, then executing the preceding command will
    result in the following log message:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群一切正常，那么执行前面的命令将导致以下日志消息：
- en: '[PRE2]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are various other logging statements that you’ll see in the command’s
    output prior to this line, but you’ll notice that none of them contain the actual
    results of your `find` command. This is because the DistributedShell ApplicationMaster
    launches the `find` command in a separate container, and the standard output (and
    standard error) of the `find` command is redirected to the log output directory
    of the container. To see the output of your command, you need to get access to
    that directory. That, as it happens, is covered in the next technique!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在此行之前，您将在命令的输出中看到各种其他的日志语句，但您会注意到它们都不包含您`find`命令的实际结果。这是因为DistributedShell ApplicationMaster在单独的容器中启动`find`命令，并且`find`命令的标准输出（和标准错误）被重定向到容器的日志输出目录。要查看命令的输出，您需要访问该目录。这正是下一个技巧要介绍的内容！
- en: Technique 3 Accessing container logs
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧3 访问容器日志
- en: Turning to the log files is the most common first step one takes when trying
    to diagnose an application that behaved in an unexpected way, or to simply understand
    more about the application. In this technique, you’ll learn how to access these
    application log files.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试诊断行为异常的应用程序或简单地了解更多关于应用程序的信息时，转向日志文件是最常见的第一步。在这个技巧中，你将学习如何访问这些应用程序日志文件。
- en: Problem
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to access container log files.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要访问容器日志文件。
- en: Solution
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use YARN’s UI and the command line to access the logs.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用YARN的UI和命令行来访问日志。
- en: Discussion
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Each container that runs in YARN has its own output directory, where the standard
    output, standard error, and any other output files are written. [Figure 2.6](#ch02fig06)
    shows the location of the output directory on a slave node, including the data
    retention details for the logs.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 每个在YARN中运行的容器都有自己的输出目录，其中标准输出、标准错误以及任何其他输出文件都会被写入。![图2.6](#ch02fig06)展示了输出目录在从节点上的位置，包括日志数据保留的详细信息。
- en: Figure 2.6\. Container log locations and retention
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6\. 容器日志位置和保留
- en: '![](02fig06_alt.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig06_alt.jpg)'
- en: Access to container logs is not as simple as it should be—let’s take a look
    at how you can use the CLI and the UIs to access logs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 访问容器日志并不像应该的那样简单——让我们看看如何使用CLI和UI来访问日志。
- en: Accessing container logs using the YARN command line
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用YARN命令行访问容器日志
- en: YARN comes with a command-line interface (CLI) for accessing YARN application
    logs. To use the CLI, you need to know the ID of your application.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: YARN提供了一个用于访问YARN应用程序日志的命令行界面（CLI）。要使用CLI，你需要知道你的应用程序ID。
- en: '|  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: How do I find the application ID?
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我该如何找到应用程序ID？
- en: 'Most YARN clients will display the application ID in their output and logs.
    For example, the DistributedShell command that you executed in the previous technique
    echoed the application ID to standard output:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数YARN客户端都会在其输出和日志中显示应用程序ID。例如，你在上一个技巧中执行的DistributedShell命令将应用程序ID回显到标准输出：
- en: '[PRE3]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Alternatively, you can use the CLI (using `yarn application -list`) or the ResourceManager
    UI to browse and find your application ID.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用CLI（使用`yarn application -list`）或ResourceManager UI来浏览并找到你的应用程序ID。
- en: '|  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'If you attempt to use the CLI when the application is still running, you’ll
    be presented with the following error message:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试在应用程序仍在运行时使用CLI，你会看到以下错误信息：
- en: '[PRE4]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The message tells it all—the CLI is only useful once an application has completed.
    You’ll need to use the UI to access the container logs when the application is
    running, which we’ll cover shortly.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 消息已经说明了一切——CLI仅在应用程序完成后才有用。当应用程序运行时，你需要使用UI来访问容器日志，我们将在稍后介绍。
- en: 'Once the application has completed, you may see the following output if you
    attempt to run the command again:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用程序完成，如果你再次运行该命令，你可能会看到以下输出：
- en: '[PRE5]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Basically, the YARN CLI only works if the application has completed and log
    aggregation is enabled. Log aggregation is covered in the next technique. If you
    enable log aggregation, the CLI will give you the logs for all the containers
    in your application, as you can see in the next example:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，YARN CLI仅在应用程序完成并且启用了日志聚合时才有效。日志聚合将在下一个技巧中介绍。如果你启用了日志聚合，CLI将为你提供应用程序中所有容器的日志，如下一个示例所示：
- en: '[PRE6]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding output shows the contents of the logs of the DistributedShell
    example that you ran in the previous technique. There are two containers in the
    output—one for the `find` command that was executed, and the other for the ApplicationMaster,
    which is also executed within a container.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示了你在上一个技巧中运行的DistributedShell示例的日志内容。输出中有两个容器——一个用于执行的`find`命令，另一个用于ApplicationMaster，它也在容器内执行。
- en: Accessing logs using the YARN UIs
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用YARN UI访问日志
- en: YARN provides access to the ApplicationMaster logs via the ResourceManager UI.
    On a pseudo-distributed setup, point your browser at http://localhost:8088/cluster.
    If you’re working with a multi-node Hadoop cluster, point your browser at [http://$yarn.resourcemanager.webapp.address/cluster](http://$yarn.resourcemanager.webapp.address/cluster).
    Click on the application you’re interested in, and then select the Logs link as
    shown in [figure 2.7](#ch02fig07).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: YARN通过ResourceManager UI提供对ApplicationMaster日志的访问。在伪分布式设置中，将您的浏览器指向http://localhost:8088/cluster。如果您在与多节点Hadoop集群一起工作，请将您的浏览器指向[http://$yarn.resourcemanager.webapp.address/cluster](http://$yarn.resourcemanager.webapp.address/cluster)。点击您感兴趣的应用程序，然后选择日志链接，如图2.7所示。
- en: Figure 2.7\. The YARN ResourceManager UI showing the ApplicationMaster container
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7\. 显示ApplicationMaster容器的YARN ResourceManager UI
- en: '![](02fig07_alt.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig07_alt.jpg)'
- en: Great, but how do you access the logs for containers other than the ApplicationMaster?
    Unfortunately, things get a little murky here. The ResourceManager doesn’t keep
    track of a YARN application’s containers, so it can’t provide you with a way to
    list and navigate to the container logs. Therefore, the onus is on individual
    YARN applications to provide their users with a way to access container logs.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，但如何访问除了ApplicationMaster之外的容器日志？不幸的是，这里的事情变得有些模糊。ResourceManager不会跟踪YARN应用程序的容器，因此它不能为您提供列出和导航到容器日志的方法。因此，责任在于个别YARN应用程序为用户提供访问容器日志的方法。
- en: '|  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Hey, ResourceManager, what are my container IDs?
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 嘿，ResourceManager，我的容器ID是什么？
- en: In order to keep the ResourceManager lightweight, it doesn’t keep track of the
    container IDs for an application. As a result, the ResourceManager UI only provides
    a way to access the ApplicationMaster logs for an application.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持ResourceManager轻量级，它不会跟踪应用程序的容器ID。因此，ResourceManager UI只为访问应用程序的ApplicationMaster日志提供了一种方式。
- en: '|  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Case in point is the DistributedShell application. It’s a simple application
    that doesn’t provide an ApplicationMaster UI or keep track of the containers that
    it’s launched. Therefore, there’s no easy way to view the container logs other
    than by using the approach presented earlier: using the CLI.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以DistributedShell应用程序为例。这是一个简单的应用程序，它不提供ApplicationMaster UI或跟踪它启动的容器。因此，除了使用前面介绍的方法（使用CLI）之外，没有简单的方法来查看容器日志。
- en: Luckily, the MapReduce YARN application provides an ApplicationMaster UI that
    you can use to access the container (the map and reduce task) logs, as well as
    a Job-History UI that can be used to access logs after a MapReduce job has completed.
    When you run a MapReduce job, the ResourceManager UI gives you a link to the MapReduce
    ApplicationMaster UI, as shown in [figure 2.8](#ch02fig08), which you can use
    to access the map and reduce logs (much like the JobTracker in MapReduce 1).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，MapReduce YARN应用程序提供了一个ApplicationMaster UI，您可以使用它来访问容器（映射和减少任务）日志，以及一个Job-History
    UI，可以在MapReduce作业完成后访问日志。当您运行MapReduce作业时，ResourceManager UI会为您提供链接到MapReduce
    ApplicationMaster UI，如图2.8所示，您可以使用它来访问映射和减少日志（类似于MapReduce 1中的JobTracker）。
- en: Figure 2.8\. Accessing the MapReduce UI for a running job
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8\. 访问正在运行作业的MapReduce UI
- en: '![](02fig08_alt.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig08_alt.jpg)'
- en: If your YARN application provides some way for you to identify container IDs
    and the hosts that they execute on, you can either access the container logs using
    the NodeManager UI or you can use a shell to `ssh` to the slave node that executed
    a container.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的YARN应用程序提供了一种方法来识别容器ID和它们执行的宿主机，您可以使用NodeManager UI访问容器日志，或者使用shell通过`ssh`到执行容器的从节点。
- en: The NodeManager URL for accessing a container’s logs is [http://<nodemanager-host>:8042/node/containerlogs/<container-id>/<username>](http://<nodemanager-host>:8042/node/containerlogs/<container-id>/<username>).
    Alternatively, you can `ssh` to the NodeManager host and access the container
    logs directory at $yarn.nodemanager.log-dirs/<application-id>/<container-id>.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 访问容器日志的NodeManager URL是[http://<nodemanager-host>:8042/node/containerlogs/<container-id>/<username>](http://<nodemanager-host>:8042/node/containerlogs/<container-id>/<username>）。或者，您可以通过`ssh`到NodeManager主机并访问$yarn.nodemanager.log-dirs/<application-id>/<container-id>目录来访问容器日志目录。
- en: Really, the best advice I can give here is that you should enable log aggregation,
    which will allow you to use the CLI, HDFS, and UIs, such as the MapReduce ApplicationMaster
    and JobHistory, to access application logs. Keep reading for details on how to
    do this.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 真的，我能给出的最好建议是您应该启用日志聚合，这将允许您使用CLI、HDFS和UI，例如MapReduce ApplicationMaster和JobHistory，来访问应用程序日志。继续阅读以获取如何操作的详细信息。
- en: Technique 4 Aggregating container log files
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术四：聚合容器日志文件
- en: Log aggregation is a feature that was missing from Hadoop 1, making it challenging
    to archive and access task logs. Luckily Hadoop 2 has this feature baked-in, and
    you have a number of ways to access aggregated log files. In this technique you’ll
    learn how to configure your cluster to archive log files for long-term storage
    and access.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 日志聚合是Hadoop 1中缺失的功能，这使得归档和访问任务日志变得具有挑战性。幸运的是，Hadoop 2内置了这个功能，并且你有多种方式可以访问聚合的日志文件。在这个技术中，你将学习如何配置你的集群以归档日志文件进行长期存储和访问。
- en: Problem
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to aggregate container log files to HDFS and manage their retention
    policies.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要将容器日志文件聚合到HDFS并管理它们的保留策略。
- en: Solution
  id: totrans-341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use YARN’s built-in log aggregation capabilities.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 使用YARN的内置日志聚合功能。
- en: Discussion
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: In Hadoop 1 your logs were stowed locally on each slave node, with the JobTracker
    and TaskTracker being the only mechanisms for getting access to these logs. This
    was cumbersome and didn’t easily support programmatic access to them. In addition,
    log files would often disappear due to aggressive log-retention policies that
    existed to prevent local disks on slave nodes from filling up.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 1中，你的日志被存储在每个从节点上，JobTracker和TaskTracker是获取这些日志的唯一机制。这很麻烦，并且不容易支持对这些日志的编程访问。此外，由于存在旨在防止从节点上的本地磁盘填满的积极的日志保留策略，日志文件通常会丢失。
- en: Log aggregation in Hadoop 2 is therefore a welcome feature, and if enabled,
    it copies container log files into a Hadoop filesystem (such as HDFS) after a
    YARN application has completed. By default, this behavior is disabled, and you
    need to set `yarn.log-aggregation-enable` to `true` to enable this feature. [Figure
    2.9](#ch02fig09) shows the data flow for container log files.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Hadoop 2中的日志聚合是一个受欢迎的功能，如果启用，它会在YARN应用程序完成后将容器日志文件复制到Hadoop文件系统（如HDFS）中。默认情况下，此行为是禁用的，你需要将`yarn.log-aggregation-enable`设置为`true`来启用此功能。[图2.9](#ch02fig09)显示了容器日志文件的数据流。
- en: Figure 2.9\. Log file aggregation from local filesystem to HDFS
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9\. 从本地文件系统到HDFS的日志文件聚合
- en: '![](02fig09_alt.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![02fig09_alt.jpg](02fig09_alt.jpg)'
- en: Now that you know how log aggregation works, let’s take a look at how you can
    access aggregated logs.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了日志聚合的工作原理，让我们看看你如何可以访问聚合日志。
- en: Accessing log files using the CLI
  id: totrans-349
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用CLI访问日志文件
- en: 'With your application ID in hand (see technique 3 for details on how to get
    it), you can use the command line to fetch all the logs and write them to the
    console:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在你手头有应用程序ID的情况下（有关如何获取它的详细信息，请参阅技术3），你可以使用命令行来获取所有日志并将它们写入控制台：
- en: '[PRE7]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Enabling log aggregation
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 启用日志聚合
- en: 'If the preceding `yarn logs` command yields the following output, then it’s
    likely that you don’t have YARN log aggregation enabled:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的`yarn logs`命令产生以下输出，那么很可能你没有启用YARN日志聚合：
- en: '[PRE8]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'This will dump out all the logs for all the containers for the YARN application.
    The output for each container is delimited with a header indicating the container
    ID, followed by details on each file in the container’s output directory. For
    example, if you ran a DistributedShell command that executed `ls -l`, then the
    output of the `yarn logs` command would yield something like the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出YARN应用程序中所有容器的所有日志。每个容器的输出由一个标题分隔，标题指示容器ID，然后是容器输出目录中每个文件的详细信息。例如，如果你运行了一个执行`ls
    -l`的DistributedShell命令，那么`yarn logs`命令的输出将类似于以下内容：
- en: '[PRE9]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The stdout file contains the directory listing of the `ls` process’s current
    directory, which is a container-specific working directory.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: stdout文件包含`ls`进程当前目录的目录列表，这是一个容器特定的工作目录。
- en: Accessing aggregated logs via the UI
  id: totrans-360
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过UI访问聚合日志
- en: Fully featured YARN applications such as MapReduce provide an ApplicationMaster
    UI that can be used to access container logs. Similarly, the JobHistory UI can
    also access aggregated logs.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 具有完整功能的YARN应用程序，如MapReduce，提供了一个ApplicationMaster UI，可以用来访问容器日志。同样，作业历史UI也可以访问聚合日志。
- en: '|  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: UI aggregated log rendering
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: UI聚合日志渲染
- en: If log aggregation is enabled, you’ll need to update yarn-site.xml and set `yarn.log.server.url`
    to point at the job history server so that the ResourceManager UI can render the
    logs.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了日志聚合，你需要更新yarn-site.xml并将`yarn.log.server.url`设置为指向作业历史服务器，以便ResourceManager
    UI可以渲染日志。
- en: '|  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Accessing log files in HDFS
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在HDFS中访问日志文件
- en: 'By default, aggregated log files go into the following directory in HDFS:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，聚合的日志文件会放入以下HDFS目录中：
- en: '[PRE10]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The directory prefix can be configured via the `yarn.nodemanager.remote-app-log-dir`
    property; similarly, the path name after the username (“logs” in the previous
    example, which is the default) can be customized via `yarn.nodemanager.remote-app-log-dir-suffix`.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 目录前缀可以通过`yarn.nodemanager.remote-app-log-dir`属性进行配置；同样，用户名之后的路径名（在之前的例子中是“logs”，默认值）可以通过`yarn.nodemanager.remote-app-log-dir-suffix`进行自定义。
- en: Differences between log files in local filesystem and HDFS
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 本地文件系统和HDFS中日志文件之间的差异
- en: 'As you saw earlier, each container results in two log files in the local filesystem:
    one for standard output and another for standard error. As part of the aggregation
    process, all the files for a given node are concatenated together into a node-specific
    log. For example, if you had five containers running across three nodes, you’d
    end up with three log files in HDFS.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前所见，每个容器在本地文件系统中会产生两个日志文件：一个用于标准输出，另一个用于标准错误。作为聚合过程的一部分，给定节点的所有文件都会被连接成一个特定于节点的日志文件。例如，如果你在三个节点上运行了五个容器，你最终会在HDFS中得到三个日志文件。
- en: Compression
  id: totrans-372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 压缩
- en: Compression of aggregated logs is disabled by default, but you can enable it
    by setting the value of `yarn.nodemanager.log-aggregation.compression-type` to
    either `lzo` or `gzip` depending on your compression requirements. As of Hadoop
    2.2, these are the only two compression codecs supported.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，聚合日志的压缩是禁用的，但你可以通过将`yarn.nodemanager.log-aggregation.compression-type`的值设置为`lzo`或`gzip`来启用它，具体取决于你的压缩需求。截至Hadoop
    2.2，这两个是唯一支持的压缩编解码器。
- en: Log retention
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 日志保留
- en: When log aggregation is turned off, the container log files on the local host
    are retained for `yarn.nodemanager.log.retain-seconds` seconds, the default being
    10,800 (3 hours).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 当关闭日志聚合时，本地主机上的容器日志文件会保留`yarn.nodemanager.log.retain-seconds`秒，默认为10,800秒（3小时）。
- en: When log aggregation is turned on, the `yarn.nodemanager.log.retain-seconds`
    configurable is ignored, and instead the local container log files are deleted
    as soon as they are copied into HDFS. But all is not lost if you want to retain
    them on the local filesystem—simply set `yarn.nodemanager.delete.debug-delay-sec`
    to a value that you want to keep the files around for. Note that this applies
    not only to the log files but also to all other metadata associated with the container
    (such as JAR files).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 当开启日志聚合时，`yarn.nodemanager.log.retain-seconds`的可配置设置会被忽略，并且一旦本地容器日志文件被复制到HDFS，它们就会被删除。但如果你想在本地文件系统中保留它们，只需将`yarn.nodemanager.delete.debug-delay-sec`设置为想要保留文件的时间即可。请注意，这不仅适用于日志文件，也适用于与容器相关联的所有其他元数据（如JAR文件）。
- en: The data retention for the files in HDFS is configured via a different setting,
    `yarn.log-aggregation.retain-seconds`.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS中文件的保留时间是通过不同的设置`yarn.log-aggregation.retain-seconds`进行配置的。
- en: NameNode considerations
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: NameNode注意事项
- en: At scale, you may want to consider an aggressive log retention setting so that
    you don’t overwhelm the NameNode with all the log file metadata. The NameNode
    keeps the metadata in memory, and on a large active cluster, the number of log
    files can quickly overwhelm the NameNode.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模部署时，你可能需要考虑一个激进的日志保留设置，以避免因所有日志文件元数据而使NameNode过载。NameNode将元数据保存在内存中，在一个大型活跃集群中，日志文件的数量可能会迅速超过NameNode的处理能力。
- en: '|  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Real-life example of NameNode impact
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: NameNode影响的真实案例
- en: Take a look at Bobby Evans’ “Our Experience Running YARN at Scale” ([http://www.slideshare.net/Hadoop_Summit/evans-june27-230pmroom210c](http://www.slideshare.net/Hadoop_Summit/evans-june27-230pmroom210c))
    for a real-life example of how Yahoo! utilized 30% of their NameNode with seven
    days’ worth of aggregated logs.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Bobby Evans的“我们在规模上运行YARN的经验”（[http://www.slideshare.net/Hadoop_Summit/evans-june27-230pmroom210c](http://www.slideshare.net/Hadoop_Summit/evans-june27-230pmroom210c)），了解雅虎如何利用30%的NameNode存储七天累积日志的实际情况。
- en: '|  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Alternative solutions
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他解决方案
- en: The solution highlighted in this technique is useful for getting your logs into
    HDFS, but if you will need to organize any log mining or visualization activities
    yourself, there are other options available such as Hunk, which supports aggregating
    logs from both Hadoop 1 and 2 and providing first-class query, visualization,
    and monitoring features, just like regular Splunk. You could also set up a query
    and visualization pipeline using tools such as Logstash, ElasticSearch, and Kibana
    if you want to own the log management process. Other tools such as Loggly are
    worth investigating.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术中突出的解决方案对于将日志放入HDFS很有用，但如果你需要自己组织任何日志挖掘或可视化活动，还有其他选项可用，例如Hunk，它支持从Hadoop
    1和2聚合日志，并提供与常规Splunk一样的一流查询、可视化和监控功能。如果你想要拥有日志管理流程，你也可以使用Logstash、ElasticSearch和Kibana等工具设置查询和可视化管道。其他工具如Loggly也值得调查。
- en: For now, this concludes our hands-on look at YARN. That’s not the end of the
    story, however. [Section 2.2](#ch02lev1sec2) looks at how MapReduce works as a
    YARN application, and later in [chapter 10](kindle_split_023.html#ch10), you’ll
    learn how to write your own YARN applications.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们关于YARN的实战考察就结束了。然而，这并不是故事的结束。[第2.2节](#ch02lev1sec2)将探讨MapReduce作为YARN应用程序的工作方式，而在[第10章](kindle_split_023.html#ch10)的后面，你将学习如何编写自己的YARN应用程序。
- en: 2.1.5\. YARN challenges
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5\. YARN挑战
- en: 'There are some gotchas to be aware of with YARN:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN中需要注意一些陷阱：
- en: '***YARN currently isn’t designed to work well with long-running processes.***
    This has created challenges for projects such as Impala and Tez that would benefit
    from such a feature. Work is currently underway to bring this feature to YARN,
    and it’s being tracked in a JIRA ticket titled “Roll up for long-lived services
    in YARN,” [https://issues.apache.org/jira/browse/YARN-896](https://issues.apache.org/jira/browse/YARN-896).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***YARN目前尚未设计为与长时间运行的过程很好地协同工作。*** 这给像Impala和Tez这样的项目带来了挑战，这些项目将受益于这样的功能。目前正在进行将此功能引入YARN的工作，并在名为“在YARN中滚动长期服务”的JIRA票据中跟踪，[https://issues.apache.org/jira/browse/YARN-896](https://issues.apache.org/jira/browse/YARN-896)。'
- en: '***Writing YARN applications is quite complex, as you’re required to implement
    container management and fault tolerance.*** This may require some complex Application-Master
    and container-state management so that upon failure the work can continue from
    some previous well-known state. There are several frameworks whose goal is to
    simplify development—refer to [chapter 10](kindle_split_023.html#ch10) for more
    details.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***编写YARN应用程序相当复杂，因为你需要实现容器管理和容错性。*** 这可能需要一些复杂的Application-Master和容器状态管理，以便在失败后可以从某个已知的状态继续工作。有几个框架的目标是简化开发——更多细节请参阅[第10章](kindle_split_023.html#ch10)。'
- en: '***Gang scheduling, which is the ability to rapidly launch a large number of
    containers in parallel, is currently not supported.*** This is another feature
    that projects such as Impala and Hamster (OpenMPI) would require for native YARN
    integration. The Hadoop committers are currently working on adding support for
    gang scheduling, which is being tracked in the JIRA ticket titled “Support gang
    scheduling in the AM RM protocol,” [https://issues.apache.org/jira/browse/YARN-624](https://issues.apache.org/jira/browse/YARN-624).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***群组调度，即能够快速并行启动大量容器的能力，目前尚不支持。*** 这是像Impala和Hamster（OpenMPI）这样的项目需要用于原生YARN集成的一个特性。Hadoop提交者目前正在努力添加对群组调度的支持，这已在名为“在AM
    RM协议中支持群组调度”的JIRA票据中跟踪，[https://issues.apache.org/jira/browse/YARN-624](https://issues.apache.org/jira/browse/YARN-624)。'
- en: So far we’ve focused on the capabilities of the core YARN system. Let’s move
    on to look at how MapReduce works as a YARN application.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注核心YARN系统的功能。接下来，让我们看看MapReduce作为YARN应用程序是如何工作的。
- en: 2.2\. YARN and MapReduce
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. YARN和MapReduce
- en: In Hadoop 1, MapReduce was the only way to process your data natively in Hadoop.
    YARN was created so that Hadoop clusters could run any type of work, and its only
    requirement was that applications adhere to the YARN specification. This meant
    Map-Reduce had to become a YARN application and required the Hadoop developers
    to rewrite key parts of MapReduce.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 1中，MapReduce是唯一在Hadoop中本地处理数据的方式。YARN的创建是为了让Hadoop集群能够运行任何类型的工作，其唯一要求是应用程序遵循YARN规范。这意味着Map-Reduce必须成为YARN应用程序，并要求Hadoop开发者重写MapReduce的关键部分。
- en: Given that MapReduce had to go through some open-heart surgery to get it working
    as a YARN application, the goal of this section is to demystify how MapReduce
    works in Hadoop 2\. You’ll see how MapReduce 2 executes in a Hadoop cluster, and
    you’ll also get to look at configuration changes and backward compatibility with
    MapReduce 1\. Toward the end of this section, you’ll learn how to run and monitor
    jobs, and you’ll see how small jobs can be quickly executed.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MapReduce需要经历一些“心脏手术”才能作为一个YARN应用程序运行，本节的目标是揭开MapReduce在Hadoop 2中的工作原理。您将看到MapReduce
    2如何在Hadoop集群中执行，同时也会了解配置更改以及与MapReduce 1的向后兼容性。在本节的最后，您将学习如何运行和监控作业，并了解小型作业是如何快速执行的。
- en: There’s a lot to go over, so let’s take MapReduce into the lab and see what’s
    going on under the covers.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多内容需要介绍，所以让我们将MapReduce带入实验室，看看其内部的工作情况。
- en: 2.2.1\. Dissecting a YARN MapReduce application
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1. 解构YARN MapReduce应用程序
- en: Architectural changes had to be made to MapReduce to port it to YARN. [Figure
    2.10](#ch02fig10) shows the processes involved in MRv2 and some of the interactions
    between them.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将MapReduce移植到YARN，必须对其进行架构上的修改。[图2.10](#ch02fig10)展示了MRv2中涉及的过程以及它们之间的一些交互。
- en: Figure 2.10\. The interactions of a MapReduce 2 YARN application
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10. MapReduce 2 YARN应用程序的交互
- en: '![](02fig10_alt.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![02fig10_alt.jpg](02fig10_alt.jpg)'
- en: Each MapReduce job is executed as a separate YARN application. When you launch
    a new MapReduce job, the client calculates the input splits and writes them along
    with other job resources into HDFS (step 1). The client then communicates with
    the ResourceManager to create the ApplicationMaster for the MapReduce job (step
    2). The ApplicationMaster is actually a container, so the ResourceManager will
    allocate the container when resources become available on the cluster and then
    communicate with a NodeManager to create the ApplicationMaster container (steps
    3–4).^([[9](#ch02fn11)])
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 每个MapReduce作业都作为一个独立的YARN应用程序执行。当您启动一个新的MapReduce作业时，客户端计算输入拆分并将它们与其他作业资源一起写入HDFS（步骤1）。然后客户端与ResourceManager通信以创建MapReduce作业的应用程序主（步骤2）。实际上，应用程序主是一个容器，因此当集群上有可用资源时，ResourceManager将分配容器，然后与NodeManager通信以创建应用程序主容器（步骤3-4).^([[9](#ch02fn11)])
- en: ⁹ If there aren’t any available resources for creating the container, the ResourceManager
    may choose to kill one or more existing containers to free up space.
  id: totrans-402
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ 如果没有可用资源来创建容器，ResourceManager可能会选择杀死一个或多个现有的容器以腾出空间。
- en: The MapReduce ApplicationMaster (MRAM) is responsible for creating map and reduce
    containers and monitoring their status. The MRAM pulls the input splits from HDFS
    (step 5) so that when it communicates with the ResourceManager (step 6) it can
    request that map containers are launched on nodes local to their input data.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce应用程序主（MRAM）负责创建map和reduce容器并监控它们的状态。MRAM从HDFS中拉取输入拆分（步骤5），这样当它与ResourceManager通信（步骤6）时，它可以请求在输入数据所在的节点上启动map容器。
- en: Container allocation requests to the ResourceManager are piggybacked on regular
    heartbeat messages that flow between the ApplicationMaster and the ResourceManager.
    The heartbeat responses may contain details on containers that are allocated for
    the application. Data locality is maintained as an important part of the architecture—when
    it requests map containers, the MapReduce ApplicationManager will use the input
    splits’ location details to request that the containers are assigned to one of
    the nodes that contains the input splits, and the ResourceManager will make a
    best attempt at container allocation on these input split nodes.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 向ResourceManager发送的容器分配请求附加在应用程序主和ResourceManager之间流动的常规心跳消息上。心跳响应可能包含有关为应用程序分配的容器的详细信息。数据局部性作为架构的一个重要部分得到维护——当它请求map容器时，MapReduce应用程序管理器将使用输入拆分的位置细节请求将这些容器分配给包含输入拆分的节点之一，并且ResourceManager将在这组输入拆分节点上尽力进行容器分配。
- en: Once the MapReduce ApplicationManager is allocated a container, it talks to
    the NodeManager to launch the map or reduce task (steps 7–8). At this point, the
    map/reduce process acts very similarly to the way it worked in MRv1.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦MapReduce ApplicationManager分配到容器，它就会与NodeManager通信以启动map或reduce任务（步骤7-8）。在这个阶段，map/reduce进程的工作方式与MRv1非常相似。
- en: The shuffle
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混洗
- en: The shuffle phase in MapReduce, which is responsible for sorting mapper outputs
    and distributing them to the reducers, didn’t fundamentally change in MapReduce
    2\. The main difference is that the map outputs are fetched via ShuffleHandlers,
    which are auxiliary YARN services that run on each slave node.^([[10](#ch02fn12)])
    Some minor memory management tweaks were made to the shuffle implementation; for
    example, `io.sort.record.percent` is no longer used.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 中的洗牌阶段，负责对映射器输出进行排序并将它们分发到减少器，在 MapReduce 2 中没有发生根本性的变化。主要区别在于映射输出是通过
    ShuffleHandlers 获取的，这些是运行在每个从节点上的辅助 YARN 服务。[10](#ch02fn12) 对洗牌实现进行了一些微小的内存管理调整；例如，`io.sort.record.percent`
    现在不再使用。
- en: ^(10) The ShuffleHandler must be configured in your yarn-site.xml; the property
    name is `yarn.nodemanager.aux-services` and the `value is mapreduce_shuffle`.
  id: totrans-408
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^((10)) ShuffleHandler 必须配置在您的 yarn-site.xml 中；属性名为 `yarn.nodemanager.aux-services`，其值为
    `mapreduce_shuffle`。
- en: Where’s the JobTracker?
  id: totrans-409
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: JobTracker 呢？
- en: You’ll note that the JobTracker no longer exists in this architecture. The scheduling
    part of the JobTracker was moved as a general-purpose resource scheduler into
    the YARN ResourceManager. The remaining part of JobTracker, which is primarily
    the metadata about running and completed jobs, was split in two. Each MapReduce
    ApplicationMaster hosts a UI that renders details on the current job, and once
    jobs are completed, their details are pushed to the JobHistoryServer, which aggregates
    and renders details on all completed jobs. Refer to [section 2.2.5](#ch02lev2sec10)
    for additional details, including how to access the MapReduce ApplicationMaster
    UI.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，在这个架构中 JobTracker 已不再存在。JobTracker 的调度部分被移动到 YARN ResourceManager 中的通用资源调度器。JobTracker
    的剩余部分，主要是关于运行中和已完成作业的元数据，被分成两部分。每个 MapReduce ApplicationMaster 都托管一个 UI，用于显示当前作业的详细信息，一旦作业完成，它们的详细信息将被推送到
    JobHistoryServer，该服务器聚合并显示所有已完成作业的详细信息。有关详细信息，包括如何访问 MapReduce ApplicationMaster
    UI，请参阅[第 2.2.5 节](#ch02lev2sec10)。
- en: Hopefully, you now have a better sense of how MapReduce 2 works. MapReduce configuration
    didn’t go untouched in the move to YARN, so let’s take a look at what’s hot and
    what’s not.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在您对 MapReduce 2 的工作方式有了更好的理解。MapReduce 配置在迁移到 YARN 时并未未受影响，因此让我们看看哪些是热门的，哪些不是。
- en: 2.2.2\. Configuration
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 配置
- en: The port of MapReduce 2 to YARN brought with it some major changes in the Map-Reduce
    properties. In this section, we’ll cover some of the frequently used properties
    that have been affected.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 2 转换到 YARN 带来了一些 Map-Reduce 属性的重大变化。在本节中，我们将介绍一些受影响的常用属性。
- en: New properties
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 新属性
- en: There are several new properties in MapReduce 2, identified in [table 2.2](#ch02table02).
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 2 中有几个新特性，在[表 2.2](#ch02table02)中进行了识别。
- en: Table 2.2\. New MapReduce 2 properties
  id: totrans-416
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.2\. 新的 MapReduce 2 属性
- en: '| Property name | Default value | Description |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 默认值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| mapreduce.framework.name | local | Determines which framework should be used
    to run MapReduce jobs. There are three possible values:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '| mapreduce.framework.name | local | 确定运行 MapReduce 作业时应使用哪个框架。有三个可能的值：'
- en: local, which means the LocalJobRunner is used (the entire MapReduce job is run
    in a single JVM).
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: local，表示使用 LocalJobRunner（整个 MapReduce 作业在单个 JVM 中运行）。
- en: classic, which means that the job will be launched on a MapReduce 1 cluster.
    In this case, the mapreduce.jobtracker.address property will be used to retrieve
    the JobTracker that the job will be submitted to.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: classic，表示作业将在 MapReduce 1 集群上启动。在这种情况下，将使用 mapreduce.jobtracker.address 属性来检索作业提交到的
    JobTracker。
- en: yarn, which runs the MapReduce job in YARN. This can either be in a pseudo-distributed
    or full-blown YARN cluster.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: yarn，表示在 YARN 中运行 MapReduce 作业。这可以是伪分布式或完整的 YARN 集群。
- en: '|'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| mapreduce.job.ubertask.enable | false | Uber jobs are small jobs that can
    be run inside the MapReduce ApplicationMaster process to avoid the overhead of
    spawning map and reduce containers. Uber jobs are covered in more detail in [section
    2.2.6](#ch02lev2sec11). |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.job.ubertask.enable | false | Uber 作业是可以在 MapReduce ApplicationMaster
    进程内部运行的较小作业，以避免启动映射和减少容器的开销。Uber 作业在[第 2.2.6 节](#ch02lev2sec11)中有更详细的介绍。 |'
- en: '| mapreduce.shuffle.max.connections | 0 | The maximum allowed connections for
    the shuffle. Set to 0 (zero) to indicate no limit on the number of connections.
    This is similar to the old (now unused) MapReduce 1 property tasktracker.http.threads,
    which defined the number of TaskTracker threads that would be used to service
    reducer requests for map outputs. |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.shuffle.max.connections | 0 | shuffle允许的最大连接数。设置为0（零）表示不对连接数进行限制。这与旧的（现在已弃用）MapReduce
    1属性tasktracker.http.threads类似，该属性定义了用于服务reducer对map输出的请求的TaskTracker线程数。|'
- en: '| yarn.resourcemanager.am.max-attempts | 2 | The maximum number of application
    attempts. It’s a global setting for all ApplicationMasters. Each application master
    can specify its individual maximum number of application attempts via the API,
    but the individual number can’t be more than the global upper bound. If it is,
    the ResourceManager will override it. The default value is 2, to allow at least
    one retry for AM. |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| yarn.resourcemanager.am.max-attempts | 2 | 最大应用尝试次数。这是所有ApplicationMasters的全局设置。每个应用程序主可以通过API指定其单独的最大应用尝试次数，但单独的数字不能超过全局上限。如果超过，资源管理器将覆盖它。默认值为2，以允许至少重试一次AM。|'
- en: '| yarn.resourcemanager.recovery.enabled | false | Enable RM to recover state
    after starting. If true, then yarn.resourcemanager.store.class must be specified.
    Hadoop 2.4.0 also brings in a ZooKeeper-based mechanism to store the RM state
    (class org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore).
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| yarn.resourcemanager.recovery.enabled | false | 启用RM在启动后恢复状态。如果为true，则必须指定yarn.resourcemanager.store.class。Hadoop
    2.4.0还引入了一种基于ZooKeeper的机制来存储RM状态（类org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore）。|'
- en: '| yarn.resourcemanager.store.class | org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystem-RMStateStore
    | Writes ResourceManager state into a filesystem for recovery purposes. |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| yarn.resourcemanager.store.class | org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystem-RMStateStore
    | 将资源管理器状态写入文件系统以用于恢复目的。|'
- en: Container properties
  id: totrans-429
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 容器属性
- en: '[Table 2.3](#ch02table03) shows the MapReduce properties that are related to
    the map and reduce processes that run the tasks.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2.3](#ch02table03)显示了与运行任务的map和reduce进程相关的MapReduce属性。'
- en: Table 2.3\. MapReduce 2 properties that impact containers (map/reduce tasks)
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.3\. 影响容器（map/reduce任务）的MapReduce 2属性
- en: '| Property name | Default value | Description |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 属性名 | 默认值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| mapreduce.map.memory.mb | 1024 | The amount of memory to be allocated to
    containers (processes) that run mappers, in megabytes. The YARN scheduler uses
    this information to determine whether there’s available capacity on nodes in a
    cluster. The old property name, mapred.job.map.memory.mb, has been deprecated.
    |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.map.memory.mb | 1024 | 分配给运行mappers的容器（进程）的内存量，以兆字节为单位。YARN调度器使用此信息来确定集群中节点上的可用容量。旧属性名mapred.job.map.memory.mb已被弃用。|'
- en: '| mapreduce.reduce.memory.mb | 1024 | The amount of memory to be allocated
    to containers (processes) that run reducers, in megabytes. The YARN scheduler
    uses this information to determine whether there’s available capacity on nodes
    in a cluster. The old property name, mapreduce.reduce.memory.mb, has been deprecated.
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.reduce.memory.mb | 1024 | 分配给运行reducer的容器（进程）的内存量，以兆字节为单位。YARN调度器使用此信息来确定集群中节点上的可用容量。旧属性名mapreduce.reduce.memory.mb已被弃用。|'
- en: '| mapreduce.map.cpu.vcores | 1 | The number of virtual cores to be allocated
    to the map processes. |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.map.cpu.vcores | 1 | 分配给map进程的虚拟核心数。|'
- en: '| mapreduce.reduce.cpu.vcores | 1 | The number of virtual cores to be allocated
    to the reduce processes. |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.reduce.cpu.vcores | 1 | 分配给reduce进程的虚拟核心数。|'
- en: '| mapred.child.java.opts | -Xmx200m | Java options for the map and reduce processes.
    The @taskid@ symbol, if present, will be replaced by the current TaskID. Any other
    occurrences of @ will go unchanged. For example, to enable verbose garbage collection
    logging to a file named for the TaskID in /tmp and to set the heap maximum to
    be a gigabyte, pass a value of -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc.
    Usage of -Djava.library.path can cause programs to no longer function if Hadoop-native
    libraries are used. These values should instead be set as part of LD_LIBRARY_PATH
    in the map/reduce JVM environment using the mapreduce.map.env and mapreduce.reduce.env
    configuration settings. |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| mapred.child.java.opts | -Xmx200m | Map和Reduce进程的Java选项。如果存在@taskid@符号，它将被当前的TaskID所替换。任何其他@的出现将保持不变。例如，为了将详细的垃圾回收日志记录到以TaskID命名的文件中（位于/tmp目录下），并将堆最大值设置为1GB，请传递值-Xmx1024m
    -verbose:gc -Xloggc:/tmp/@taskid@.gc。使用-Djava.library.path可能会导致程序无法正常工作，如果使用了Hadoop原生库。这些值应该作为LD_LIBRARY_PATH的一部分在map/reduce
    JVM环境中设置，使用mapreduce.map.env和mapreduce.reduce.env配置设置。|'
- en: '| mapred.map.child.java.opts | None | Map process–specific JVM arguments. The
    old property name, mapred.map.child.java.opts, has been deprecated. |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| mapred.map.child.java.opts | None | Map进程特定的JVM参数。旧的属性名称mapred.map.child.java.opts已被弃用。|'
- en: '| mapreduce.reduce.java.opts | None | Reduce process–specific JVM arguments.
    The old property name, mapred.reduce.child.java.opts, has been deprecated. |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.reduce.java.opts | None | Reduce进程特定的JVM参数。旧的属性名称mapred.reduce.child.java.opts已被弃用。|'
- en: Configuration no longer in effect
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 已不再有效的配置
- en: Common properties in MapReduce 1 that are no longer in effect in MapReduce 2
    are shown in [table 2.4](#ch02table04), along with explanations as to why they
    no longer exist.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 1中常见的属性在MapReduce 2中不再有效，这些属性在[表2.4](#ch02table04)中列出，同时解释了为什么它们不再存在。
- en: Table 2.4\. Old MapReduce 1 properties that are no longer in use
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.4\. 已不再使用的旧MapReduce 1属性
- en: '| Property name | Description |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 属性名称 | 描述 |'
- en: '| --- | --- |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| mapred.job.tracker mapred.job.tracker.http.address | The JobTracker no longer
    exists in YARN; it’s been replaced by the ApplicationMaster UI and the JobHistory
    UI. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| mapred.job.tracker mapred.job.tracker.http.address | 在YARN中，JobTracker不再存在；它已被ApplicationMaster
    UI和JobHistory UI所取代。|'
- en: '| mapred.task.tracker.http.address mapred.task.tracker.report.address | The
    TaskTracker also doesn’t exist in YARN—it’s been replaced by the YARN NodeManager.
    |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| mapred.task.tracker.http.address mapred.task.tracker.report.address | TaskTracker在YARN中也不再存在——它已被YARN
    NodeManager所取代。|'
- en: '| mapred.local.dir | This used to be the local directory where intermediary
    data for MapReduce jobs was stored. This has been deprecated, and the new property
    name is mapreduce.jobtracker.system.dir. Its use has also been relegated to use
    only in the LocalJob-Runner, which comes into play if you’re running a local job
    (not on a YARN cluster). |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| mapred.local.dir | 这曾经是存储MapReduce作业中间数据的本地目录。这个属性已经被弃用，新的属性名称是mapreduce.jobtracker.system.dir。它的使用也仅限于LocalJob-Runner，如果你在运行本地作业（不在YARN集群上）时，它才会发挥作用。|'
- en: '| mapred.system.dir | Much like mapred.local.dir, this is relegated to duty
    when running the LocalJobRunner. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| mapred.system.dir | 与mapred.local.dir类似，它在运行LocalJobRunner时才会被使用。|'
- en: '| mapred.tasktracker.map.tasks.maximum mapred.tasktracker.reduce.tasks.maximum
    | This was used to control the maximum number of map and reduce task processes
    that could run on a node. These were called “slots,” and they were static in Hadoop
    1\. In Hadoop 2, YARN doesn’t impose a static limit on the number of concurrent
    containers on a node, so these properties are no longer needed. |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| mapred.tasktracker.map.tasks.maximum mapred.tasktracker.reduce.tasks.maximum
    | 这用于控制一个节点上可以运行的map和reduce任务进程的最大数量。这些被称为“槽位”，在Hadoop 1中是静态的。在Hadoop 2中，YARN不对节点上并发容器的数量施加静态限制，因此这些属性不再需要。|'
- en: '| mapred.job.reuse.jvm.num.tasks | You used to be able to sequentially run
    multiple tasks in the same JVM, which was useful for tasks that were short-lived
    (and to diminish the overhead of creating a separate process per task). This is
    no longer supported in YARN. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| mapred.job.reuse.jvm.num.tasks | 你曾经能够在同一个JVM中顺序运行多个任务，这对于生命周期短的任务（以及减少为每个任务创建单独进程的开销）是有用的。这在YARN中不再被支持。|'
- en: '| tasktracker.http.threads | This is no longer used in MRv2\. Map outputs are
    now fetched from a new ShuffleHandler service, which is NIO-based and is by default
    configured with no cap in the number of open connections (configured via mapreduce.shuffle.max.connections).
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| tasktracker.http.threads | 在 MRv2 中不再使用。现在从新的 ShuffleHandler 服务中获取 Map 输出，该服务基于
    NIO，默认配置为无连接数上限（通过 mapreduce.shuffle.max.connections 配置）。|'
- en: '| io.sort.record.percent | This shuffle property used to control how much accounting
    space was used in the map-side sort buffer (io.sort.mb). MapReduce 2 is smarter
    about how to fill up io.sort.mb.^([[a](#ch02fn13)]) |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| io.sort.record.percent | 这个洗牌属性曾经用来控制 map 端排序缓冲区（io.sort.mb）中使用的会计空间量。MapReduce
    2 在如何填充 io.sort.mb 方面更智能。[^([a](#ch02fn13))] |'
- en: ^a “Map-side sort is hampered by io.sort.record.percent” and details can be
    seen at [https://issues.apache.org/jira/browse/MAPREDUCE-64](https://issues.apache.org/jira/browse/MAPREDUCE-64).
  id: totrans-454
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a “Map-side sort is hampered by io.sort.record.percent” 和详细信息可以在 [https://issues.apache.org/jira/browse/MAPREDUCE-64](https://issues.apache.org/jira/browse/MAPREDUCE-64)
    查看。
- en: Deprecated properties
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 已弃用的属性
- en: 'Most of the MapReduce 1 (and many HDFS) properties have been deprecated in
    favor of property names that are better organized.^([[11](#ch02fn14)]) Currently
    Hadoop 2 supports both the deprecated and new property names, but it would be
    prudent for you to update your properties, as there’s no guarantee that Hadoop
    3 and later will support deprecated properties. Luckily, you get a dump of all
    the deprecated configuration properties on standard output when you run a MapReduce
    job, an example of which is shown here:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 MapReduce 1（以及许多 HDFS）的属性已被弃用，以支持更好的组织结构的属性名称。[^([11](#ch02fn14))] 目前 Hadoop
    2 支持已弃用和新属性名称，但您最好更新您的属性，因为没有保证 Hadoop 3 及以后的版本将支持已弃用的属性。幸运的是，当您运行 MapReduce 作业时，您会在标准输出上获得所有已弃用的配置属性的转储，以下是一个示例：
- en: ^(11) See the web page “[Deprecated properties](#ch02lev3sec12)” at [http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/DeprecatedProperties.html](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/DeprecatedProperties.html)
    for the properties that have been deprecated and their new names.
  id: totrans-457
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (11) 请参阅网页 "[已弃用的属性](#ch02lev3sec12)"，其中列出了已弃用的属性及其新名称。[http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/DeprecatedProperties.html](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/DeprecatedProperties.html)
- en: '[PRE11]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s clear that there were quite a few changes to MapReduce properties. You
    may be curious to know how the rest of MapReduce changed and what parts managed
    to retain strong backward compatibility. Did the MapReduce APIs and binaries escape
    unscathed with the major version bump in Hadoop?^([[12](#ch02fn15)])
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，MapReduce 属性有很多变化。您可能想知道 MapReduce 的其余部分是如何变化的，以及哪些部分设法保持了强大的向后兼容性。MapReduce
    API 和二进制文件在 Hadoop 主版本号增加时是否安然无恙？[^([12](#ch02fn15))]
- en: ^(12) Semantic versioning ([http://semver.org/](http://semver.org/)) permits
    APIs to change in ways that break backward compatibility when the major version
    number is incremented.
  id: totrans-460
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (12) 语义版本控制 ([http://semver.org/](http://semver.org/)) 允许在主版本号增加时以破坏向后兼容性的方式更改
    API。
- en: 2.2.3\. Backward compatibility
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3. 向后兼容性
- en: Backward compatibility is an important consideration for systems with large,
    established user bases, as it ensures that they can rapidly move to a new version
    of a system with little or no change. This section covers various parts of the
    MapReduce system and help you determine whether you need to change your systems
    to be able to function on MapReduce 2.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 对于拥有大量、稳定用户基础的系统，向后兼容性是一个重要的考虑因素，因为它确保它们可以快速迁移到系统的全新版本，而几乎不需要或不需要进行更改。本节涵盖了
    MapReduce 系统的各个部分，并帮助您确定您是否需要更改系统以使其能够在 MapReduce 2 上运行。
- en: Script compatibility
  id: totrans-463
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 脚本兼容性
- en: The scripts that are bundled with Hadoop remain unchanged. This means that you
    can continue to use `hadoop jar ...` to launch jobs, and all other uses of the
    main `hadoop` script continue to work, as do the other scripts bundled with Hadoop.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Hadoop 一起捆绑的脚本保持不变。这意味着您可以使用 `hadoop jar ...` 来启动作业，并且所有其他对主 `hadoop` 脚本的使用以及与
    Hadoop 一起捆绑的其他脚本都将继续工作。
- en: Configuration
  id: totrans-465
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 配置
- en: With the introduction of YARN, and MapReduce becoming an application, many MapReduce
    1 property names are now deprecated in MapReduce 2, and some are no longer in
    effect. [Section 2.2.2](#ch02lev2sec7) covers changes to some of the more commonly
    used properties.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 YARN 的引入和 MapReduce 成为一个应用程序，MapReduce 1 中的许多属性名称现在在 MapReduce 2 中已被弃用，其中一些已不再有效。[第
    2.2.2 节](#ch02lev2sec7) 涵盖了一些常用属性的变化。
- en: API backward compatibility
  id: totrans-467
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: API 向后兼容性
- en: 'In porting MapReduce to YARN, the developers did their best to maintain backward
    compatibility for existing MapReduce applications. They were able to achieve code
    compatibility, but in some cases weren’t able to preserve binary compatibility:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在将MapReduce移植到YARN的过程中，开发人员尽力保持现有MapReduce应用程序的向后兼容性。他们能够实现代码兼容性，但在某些情况下无法保持二进制兼容性：
- en: '*Code compatibility* means that any MapReduce code that exists today will run
    fine on YARN as long as the code is recompiled. This is great, as it means that
    you don’t need to modify your code to get it working on YARN.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代码兼容性*意味着任何今天存在的MapReduce代码，只要重新编译，就可以在YARN上良好运行。这很好，这意味着你不需要修改代码就能使其在YARN上工作。'
- en: '*Binary compatibility* means that MapReduce bytecode will run unchanged on
    YARN. In other words, you don’t have to recompile your code—you can use the same
    classes and JARs that worked on Hadoop 1, and they’ll work just fine on YARN.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二进制兼容性*意味着MapReduce的字节码可以在YARN上不变地运行。换句话说，你不需要重新编译你的代码——你可以使用在Hadoop 1上工作过的相同的类和JAR文件，它们在YARN上也能正常工作。'
- en: Code that uses the “old” MapReduce API (`org.apache.hadoop.mapreduce package`)
    is binary compatible, so if your existing MapReduce code only uses the old API,
    you’re all set—no recompilation of your code is required.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 使用“旧”的MapReduce API（`org.apache.hadoop.mapreduce包`）的代码是二进制兼容的，所以如果你的现有MapReduce代码只使用旧API，你就没问题——不需要重新编译你的代码。
- en: 'This isn’t the case for certain uses of the “new” MapReduce API (`org.apache.hadoop.mapreduce`).
    If you use the new API, it’s possible that you are using some features of the
    API that changed; namely, some classes were changed to interfaces. A few of these
    classes are as follows:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“新”的MapReduce API（`org.apache.hadoop.mapreduce`）的某些使用情况，情况并非如此。如果你使用新的API，可能你正在使用API的一些已更改的功能；即，一些类被更改为接口。以下是一些这样的类的例子：
- en: '`JobContext`'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JobContext`'
- en: '`TaskAttemptContext`'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TaskAttemptContext`'
- en: '`Counter`'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Counter`'
- en: This begs the question of what to do if you’re using the new MapReduce API and
    have code that needs to run on both versions of Hadoop.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个问题：如果你使用新的MapReduce API并且有需要在Hadoop两个版本上运行的代码，你会怎么做。
- en: Technique 5 Writing code that works on Hadoop versions 1 and 2
  id: totrans-477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧5：编写在Hadoop版本1和2上都能运行的代码
- en: If you’re using the “new” MapReduce API and have your own Input/OutputFormat
    classes or use counters (to name a few operations that are not code-compatible
    across MapReduce versions), then you have JARs that will likely need to be recompiled
    to work with MapReduce 2\. This is a nuisance if you have to support both MapReduce
    1 and 2\. You could create two sets of JARs targeting each version of MapReduce,
    but you would likely owe your build team several beers and end up with more complicated
    build and deployment systems. Or you can use the tip in this technique and continue
    to distribute a single JAR.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用“新”的MapReduce API并且有自己的Input/OutputFormat类或使用计数器（仅举几个在MapReduce版本之间不兼容的操作），那么你可能有需要重新编译以与MapReduce
    2兼容的JAR文件。如果你必须同时支持MapReduce 1和2，这将是一个麻烦事。你可以为每个MapReduce版本创建两组JAR文件，但你可能需要向你的构建团队支付几杯啤酒，并最终拥有更复杂的构建和部署系统。或者，你可以使用这个技巧中的提示，继续分发单个JAR文件。
- en: Problem
  id: totrans-479
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You’re using MapReduce code that isn’t binary compatible with MapReduce 2, and
    you want to be able to update your code in a way that will be compatible with
    both MapReduce versions.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在使用与MapReduce 2不二进制兼容的MapReduce代码，并且你想要以能够与两个MapReduce版本兼容的方式更新你的代码。
- en: Solution
  id: totrans-481
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a Hadoop compatibility library that works around the API differences.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个处理API差异的Hadoop兼容性库。
- en: Discussion
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The Elephant Bird project includes a HadoopCompat class, which dynamically
    figures out which version of Hadoop you’re running on and uses Java reflection
    to invoke the appropriate method calls to work with your version of Hadoop. The
    following code shows an example of its usage, where inside an `InputFormat` implementation,
    the `TaskAttemptContext` changed from a class to an interface, and the `HadoopCompat`
    class is being used to extract the `Configuration` object:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: Elephant Bird项目包含一个HadoopCompat类，它可以动态地确定你正在运行哪个版本的Hadoop，并使用Java反射来调用适当的方法调用以与你的Hadoop版本一起工作。以下代码展示了其使用的一个例子，其中在一个`InputFormat`实现中，`TaskAttemptContext`从类更改为接口，并且正在使用`HadoopCompat`类来提取`Configuration`对象：
- en: '[PRE12]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Which classes changed to interfaces in Hadoop 2? Some of the notable ones are
    `TaskAttemptContext`, `JobContext`, and `MapContext`. [Table 2.5](#ch02table05)
    shows a selection of some of the methods available in the `HadoopCompat` class.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2 中哪些类被改为接口？一些值得注意的类包括 `TaskAttemptContext`、`JobContext` 和 `MapContext`。[表
    2.5](#ch02table05) 展示了 `HadoopCompat` 类中可用的一些方法。
- en: Table 2.5\. Common classes and methods that are not binary compatible across
    MapReduce versions
  id: totrans-487
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 2.5\. 在 MapReduce 版本之间不兼容的二进制类和方法
- en: '| Hadoop class and method | HadoopCompat call | Where you’d encounter the interface
    |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| Hadoop 类和方法 | HadoopCompat 调用 | 你会遇到这个接口的地方 |'
- en: '| --- | --- | --- |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| JobContext.getConfiguration | HadoopCompat.getConfiguration | This is probably
    the most commonly used class (now an interface). You’ll likely bump into this
    interface as it’s how you get to a map or reduce task’s configuration. |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| JobContext.getConfiguration | HadoopCompat.getConfiguration | 这可能是最常用的类（现在是一个接口）。你可能会遇到这个接口，因为它是你获取
    map 或 reduce 任务的配置的方式。|'
- en: '| TaskAttemptContext.setStatus | HadoopCompat.setStatus | You’ll encounter
    this interface if you have a custom InputFormat, OutputFormat, RecordReader, or
    RecordWriter. |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| TaskAttemptContext.setStatus | HadoopCompat.setStatus | 如果你有一个自定义的 InputFormat、OutputFormat、RecordReader
    或 RecordWriter，你会遇到这个接口。|'
- en: '| TaskAttemptContext.getTaskAttemptID | HadoopCompat.getTaskAttemptID | You’ll
    use this interface if you have a custom InputFormat, OutputFormat, RecordReader,
    or RecordWriter. |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| TaskAttemptContext.getTaskAttemptID | HadoopCompat.getTaskAttemptID | 如果你有一个自定义的
    InputFormat、OutputFormat、RecordReader 或 RecordWriter，你会使用这个接口。|'
- en: '| TaskAttemptContext.getCounter | HadoopCompat.getCounter | You’ll bump into
    this interface if you have a custom InputFormat, OutputFormat, RecordReader, or
    RecordWriter. |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| TaskAttemptContext.getCounter | HadoopCompat.getCounter | 如果你有一个自定义的 InputFormat、OutputFormat、RecordReader
    或 RecordWriter，你会遇到这个接口。|'
- en: '| Counter.incrementCounter | HadoopCompat .incrementCounter | If you use counters
    in your jobs, you’ll need to use the HadoopCompat call. |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| Counter.incrementCounter | HadoopCompat.incrementCounter | 如果你使用计数器在你的作业中，你需要使用
    HadoopCompat 调用。|'
- en: The `HadoopCompat` class also has a handy method called `isVersion2x`, which
    returns a Boolean if the class has determined that your runtime is running against
    version 2 of Hadoop.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '`HadoopCompat` 类还有一个方便的方法叫做 `isVersion2x`，它返回一个布尔值，如果类已经确定你的运行时正在运行 Hadoop
    的 2 版本。'
- en: 'This is just a sample of the methods on this class—for complete details, see
    the Elephant Bird project’s HadoopCompat page on GitHub: [https://github.com/kevinweil/elephant-bird/blob/master/hadoop-compat/src/main/java/com/twitter/elephantbird/util/HadoopCompat.java](https://github.com/kevinweil/elephant-bird/blob/master/hadoop-compat/src/main/java/com/twitter/elephantbird/util/HadoopCompat.java).'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是这个类上方法的一个示例——对于完整详情，请参阅 Elephant Bird 项目在 GitHub 上的 HadoopCompat 页面：[https://github.com/kevinweil/elephant-bird/blob/master/hadoop-compat/src/main/java/com/twitter/elephantbird/util/HadoopCompat.java](https://github.com/kevinweil/elephant-bird/blob/master/hadoop-compat/src/main/java/com/twitter/elephantbird/util/HadoopCompat.java)。
- en: Maven Central contains a package with this library in it, and you can take a
    look at the Maven repository’s page on “Elephant Bird Hadoop Compatibility” at
    [http://mvnrepository.com/artifact/com.twitter.elephantbird/elephant-bird-hadoop-compat](http://mvnrepository.com/artifact/com.twitter.elephantbird/elephant-bird-hadoop-compat)
    for an example entry you can add to your Maven file.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: Maven Central 包含了这个库的包，你可以在 Maven 仓库的“Elephant Bird Hadoop 兼容性”页面查看示例条目，并将其添加到你的
    Maven 文件中。[http://mvnrepository.com/artifact/com.twitter.elephantbird/elephant-bird-hadoop-compat](http://mvnrepository.com/artifact/com.twitter.elephantbird/elephant-bird-hadoop-compat)
- en: As you saw earlier, the main script in Hadoop 1, `hadoop`, continues to exist
    unchanged in Hadoop 2\. In the next section you’ll see how a newer version of
    the script should be used to run not only MapReduce jobs but also issue YARN commands.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前所见，Hadoop 1 中的主要脚本 `hadoop` 在 Hadoop 2 中继续存在且未做更改。在下一节中，你将看到如何使用较新版本的脚本不仅运行
    MapReduce 作业，还可以发出 YARN 命令。
- en: 2.2.4\. Running a job
  id: totrans-499
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 运行一个作业
- en: It’s time to run a MapReduce 2 job. Don’t worry, doing so is pretty much identical
    to how you did it in MapReduce 1.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候运行一个 MapReduce 2 作业了。别担心，这样做几乎与你在 MapReduce 1 中做的一样。
- en: Technique 6 Using the command line to run a job
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 6 使用命令行运行作业
- en: In this technique you’ll learn how to use the command line to run a MapReduce
    job.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，你将学习如何使用命令行来运行一个 MapReduce 作业。
- en: Problem
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use the YARN command line to run a MapReduce job.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用 YARN 命令行来运行一个 MapReduce 作业。
- en: Solution
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `yarn` command.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `yarn` 命令。
- en: Discussion
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: In Hadoop 1, the `hadoop` command was the one used to launch jobs. This command
    still works for backward compatibility reasons, but the YARN form of this command
    is the `yarn` script, which works much like the old `hadoop` script works. As
    an example, this is how you’d run the `pi` job bundled in the Hadoop examples
    JAR:^([[13](#ch02fn16)])
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 1中，`hadoop` 命令是用于启动作业的命令。出于向后兼容的原因，此命令仍然有效，但此命令的YARN形式是 `yarn` 脚本，它的工作方式与旧的
    `hadoop` 脚本非常相似。例如，这是如何在Hadoop示例JAR中运行捆绑的 `pi` 作业的方法：^([[13](#ch02fn16)])
- en: ^(13) This example calculates the value of pi using the quasi-Monte Carlo method.
  id: totrans-509
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(13) 此示例使用准蒙特卡洛方法计算π的值。
- en: '[PRE13]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you’re in the habit of using `hadoop` to run your jobs, give some thought
    to replacing it with the `yarn` command. It’s unclear whether there are plans
    to deprecate and remove the `hadoop` command, but you can be sure that the `yarn`
    equivalent is here to stay.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你习惯使用 `hadoop` 来运行你的作业，考虑将其替换为 `yarn` 命令。目前尚不清楚是否有计划废弃并移除 `hadoop` 命令，但可以肯定的是，`yarn`
    的对应命令将会持续存在。
- en: The ways you can launch MapReduce jobs have changed in version 2, and so has
    the mechanism by which you view the status and details of running and completed
    jobs.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在版本2中，你可以启动MapReduce作业的方式已经改变，查看运行中和已完成作业的状态和细节的机制也发生了变化。
- en: 2.2.5\. Monitoring running jobs and viewing archived jobs
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5. 监控运行中的作业和查看归档作业
- en: When running MapReduce jobs, it’s important for monitoring and debugging purposes
    to be able to view the status of a job and its tasks and to gain access to the
    task logs. In MapReduce 1 this would have all been carried out using the JobTracker
    UI, which could be used to view details on running and completed or archived jobs.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行MapReduce作业时，为了监控和调试的目的，能够查看作业及其任务的状态，并访问任务日志非常重要。在MapReduce 1中，所有这些操作都是通过JobTracker
    UI完成的，它可以用来查看运行中、已完成或归档作业的详细信息。
- en: As highlighted in [section 2.2.1](#ch02lev2sec6), the JobTracker no longer exists
    in MapReduce 2; it has been replaced with an ApplicationMaster-specific UI, and
    the JobHistoryServer for completed jobs. The ApplicationMaster UI can be seen
    in [figure 2.11](#ch02fig11). For fetching map and reduce task logs, the UI redirects
    to the NodeManager.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 如[2.2.1节](#ch02lev2sec6)中所述，JobTracker在MapReduce 2中不再存在；它已被特定于ApplicationMaster的UI和已完成作业的JobHistoryServer所取代。ApplicationMaster
    UI可以在[图2.11](#ch02fig11)中看到。对于获取映射和减少任务日志，UI将重定向到NodeManager。
- en: Figure 2.11\. The YARN ResourceManager UI, showing applications that are currently
    executing
  id: totrans-516
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11。YARN ResourceManager UI，显示当前正在执行的应用程序
- en: '![](02fig11_alt.jpg)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig11_alt.jpg)'
- en: '|  |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figuring out where your ResourceManager UI is running
  id: totrans-519
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 确定ResourceManager UI的运行位置
- en: You can retrieve the host and port of the ResourceManager by examining the value
    of `yarn.resourcemanager.webapp.address` (or `yarn.resourcemanager.webapp.https.address`
    if HTTPS access is required). In the case of a pseudo-distributed installation,
    this will be http://localhost:8088 (or port 8090 for HTTPS). Copying the host
    and port into your browser is sufficient to access the UI as a URL path isn’t
    required.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查 `yarn.resourcemanager.webapp.address` 的值（如果需要HTTPS访问，则为 `yarn.resourcemanager.webapp.https.address`）来获取ResourceManager的主机和端口。在伪分布式安装的情况下，这将是指定的
    http://localhost:8088（或HTTPS的端口8090）。将主机和端口复制到浏览器中就足以访问UI，因为不需要URL路径。
- en: '|  |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The JobHistoryServer can be seen in [figure 2.12](#ch02fig12).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: JobHistoryServer可以在[图2.12](#ch02fig12)中看到。
- en: Figure 2.12\. The JobHistory UI, showing MapReduce applications that have completed
  id: totrans-523
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12。JobHistory UI，显示已完成的MapReduce应用程序
- en: '![](02fig12_alt.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig12_alt.jpg)'
- en: MapReduce 2 has changed how jobs are executed, configured, and monitored. It
    has also introduced new features, such as uber jobs, which are up next.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 2改变了作业的执行、配置和监控方式。它还引入了新的功能，例如uber作业，接下来将介绍。
- en: 2.2.6\. Uber jobs
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6. Uber作业
- en: When running small MapReduce jobs, the time taken for resource scheduling and
    process forking is often a large percentage of the overall runtime. In MapReduce
    1 you didn’t have any choice about this overhead, but MapReduce 2 has become smarter
    and can now cater to your needs to run lightweight jobs as quickly as possible.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行小型MapReduce作业时，资源调度和进程分叉所需的时间通常是总体运行时间的一个大比例。在MapReduce 1中，你对此开销没有任何选择，但MapReduce
    2变得更加智能，现在可以满足你尽快运行轻量级作业的需求。
- en: Technique 7 Running small MapReduce jobs
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧7 运行小型MapReduce作业
- en: This technique looks at how you can run MapReduce jobs within the MapReduce
    ApplicationMaster. This is useful when you’re working with a small amount of data,
    as you remove the additional time that MapReduce normally spends spinning up and
    bringing down map and reduce processes.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术探讨了如何在MapReduce ApplicationMaster内部运行MapReduce作业。当你处理少量数据时，这很有用，因为它减少了MapReduce通常花费在启动和关闭映射和减少进程上的额外时间。
- en: Problem
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You have a MapReduce job that operates on a small dataset, and you want to avoid
    the overhead of scheduling and creating map and reduce processes.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个在小型数据集上运行的MapReduce作业，并且你想要避免调度和创建映射和减少进程的开销。
- en: Solution
  id: totrans-532
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Configure your job to enable uber jobs; this will run the mappers and reducers
    in the same process as the ApplicationMaster.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 配置你的作业以启用uber作业；这将使映射器和减少器在ApplicationMaster的同一进程中运行。
- en: Discussion
  id: totrans-534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Uber jobs are jobs that are executed within the MapReduce ApplicationMaster.
    Rather than liaise with the ResourceManager to create the map and reduce containers,
    the ApplicationMaster runs the map and reduce tasks within its own process and
    avoids the overhead of launching and communicating with remote containers.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: Uber作业是在MapReduce ApplicationMaster内部执行的作业。而不是与ResourceManager协商来创建映射和减少容器，ApplicationMaster在其自己的进程中运行映射和减少任务，从而避免了启动和与远程容器通信的开销。
- en: 'To enable uber jobs, you need to set the following property:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用uber作业，你需要设置以下属性：
- en: '[PRE14]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Table 2.6](#ch02table06) lists some additional properties that control whether
    a job qualities for uberization.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2.6](#ch02table06) 列出了一些控制作业是否适合uber化的额外属性。'
- en: Table 2.6\. Properties for customizing uber jobs
  id: totrans-539
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.6\. 个性化uber作业的属性
- en: '| Property | Default value | Description |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 默认值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| mapreduce.job.ubertask.maxmaps | 9 | The number of mappers for a job must
    be less than or equal to this value for the job to be uberized. |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.job.ubertask.maxmaps | 9 | 一个作业的映射器数量必须小于或等于此值，作业才能被uber化。 |'
- en: '| mapreduce.job.ubertask.maxreduces | 1 | The number of reducers for a job
    must be less than or equal to this value for the job to be uberized. |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.job.ubertask.maxreduces | 1 | 一个作业的减少器数量必须小于或等于此值，作业才能被uber化。 |'
- en: '| mapreduce.job.ubertask.maxbytes | Default block size | The total input size
    of a job must be less than or equal to this value for the job to be uberized.
    |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.job.ubertask.maxbytes | 默认块大小 | 作业的总输入大小必须小于或等于此值，作业才能被uber化。 |'
- en: When running uber jobs, MapReduce disables speculative execution and also sets
    the maximum attempts for tasks to `1`.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行uber作业时，MapReduce禁用了推测执行，并将任务的最大尝试次数设置为`1`。
- en: '|  |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Reducer restrictions
  id: totrans-547
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Reducer限制
- en: Currently only map-only jobs and jobs with one reducer are supported for uberization.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 目前只支持map-only作业和只有一个减少器的作业进行uber化。
- en: '|  |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Uber jobs are a handy new addition to the MapReduce capabilities, and they only
    work on YARN. This concludes our look at MapReduce on YARN. Next you’ll see examples
    of other systems running on YARN.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: Uber作业是MapReduce功能的一个方便的新增功能，并且它们只在YARN上工作。这标志着我们对YARN上的MapReduce的探讨结束。接下来，你将看到其他在YARN上运行的系统的示例。
- en: 2.3\. YARN applications
  id: totrans-551
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. YARN应用程序
- en: So far you’ve seen what YARN is, how it works, and how MapReduce 2 works as
    a YARN application. But this is only the first step of YARN’s journey; there are
    already several projects that work on YARN, and over time, you should expect to
    see rapid growth in YARN’s ecosystem.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了YARN是什么，它是如何工作的，以及MapReduce 2作为YARN应用程序是如何工作的。但这只是YARN旅程的第一步；已经有几个项目在YARN上工作，随着时间的推移，你应该期待看到YARN生态系统的快速增长。
- en: At this point, you may be asking yourself why YARN applications are compelling
    and why the Hadoop community put so much work into YARN’s architecture and the
    port of MapReduce to a YARN application. There are many reasons that we touched
    on at the start of the chapter, but the most important reason behind this revolutionary
    change in Hadoop is to open up the platform. Think about how our systems work
    today—gone are the days when we worked on monolithic systems; instead, we live
    in a world where we run multiple disparate systems in our datacenters, as shown
    in [figure 2.13](#ch02fig13).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道为什么YARN应用程序具有吸引力，为什么Hadoop社区在YARN架构和将MapReduce移植到YARN应用程序上投入了如此多的工作。我们在本章开头提到了许多原因，但Hadoop这一革命性变革背后的最重要的原因是开放平台。想想看，我们现在的系统是如何工作的——那些在单体系统中工作的日子已经过去了；相反，我们生活在一个在我们的数据中心运行多个不同系统的世界里，如图2.13所示。
- en: Figure 2.13\. Common systems we run today. They are siloed, which complicates
    data and resource sharing.
  id: totrans-554
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.13. 我们今天运行的一些常见系统。它们是孤立的，这增加了数据和资源共享的复杂性。
- en: '![](02fig13_alt.jpg)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![02fig13_alt.jpg](02fig13_alt.jpg)'
- en: 'That’s a lot of systems! And chances are that you’re already running many of
    them in production right now. If you’re an engineer, you’re probably excited about
    having all these systems in play, but systems administrators and architects get
    migraines thinking about the challenges that supporting all these systems brings:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是很多系统！而且很可能你现在已经在生产中运行了许多这样的系统。如果你是一名工程师，你可能对拥有所有这些系统感到兴奋，但系统管理员和架构师在思考支持所有这些系统带来的挑战时可能会感到头痛：
- en: They have to build the in-house knowledge to administer and keep the systems
    up and healthy. Systems fail, especially complicated distributed systems, and
    being open source, many of these systems don’t have the tooling to facilitate
    easy management.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们必须建立内部知识来管理和维护系统，使其保持健康。系统会失败，尤其是复杂的分布式系统，而且作为开源项目，许多这些系统没有工具来简化管理。
- en: Data exchange between systems is painful, primarily due to the volume of data
    and the lack of tooling for the data movement. Large, expensive projects ensue.^([[14](#ch02fn17)])
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统间的数据交换很痛苦，主要是因为数据量庞大，以及缺乏数据移动工具。随之而来的是大型、昂贵的项目。[14](#ch02fn17)。
- en: '^(14) LinkedIn addresses this with a “data plane” architecture highlighted
    in Jay Kreps’ blog post, “The Log: What every software engineer should know about
    real-time data’s unifying abstraction”—take a look at the “unified log” image
    and the surrounding text for an architectural solution to help reduce these pain
    points: [http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying).'
  id: totrans-559
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([14](#ch02fn17)) LinkedIn通过Jay Kreps在其博客文章“日志：每位软件工程师都应该了解关于实时数据统一抽象的知识”——查看“统一日志”图像及其周围的文本，了解一个有助于减少这些痛点的架构解决方案：[http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)。
- en: Each system has to solve the same distributed problems, such as fault tolerance,
    distributed storage, log handling, and resource scheduling.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个系统都必须解决相同的一些分布式问题，例如容错、分布式存储、日志处理和资源调度。
- en: 'YARN promises a single cluster that can have its resources managed in a uniform
    way, support multi-tenant applications and users, and offer elastic computation
    over shared storage. HBase coupled with Hoya gives us a sneak peek at what the
    future could look like: strong data locality properties are used for efficient
    movement of data in and out of HBase; and Hoya, with its YARN integration, provides
    elastic, on-demand computing, with the ability to run multiple HBase clusters
    on a single YARN cluster.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: YARN承诺提供一个可以统一管理资源的单一集群，支持多租户应用程序和用户，并在共享存储上提供弹性计算。HBase与Hoya的结合为我们提供了一个关于未来可能形态的预览：利用强大的数据局部性属性，高效地在HBase内外移动数据；Hoya通过其与YARN的集成，提供了弹性、按需计算，能够在单个YARN集群上运行多个HBase集群。
- en: In the following sections, you’ll be introduced to several systems across a
    broad spectrum of technologies that are built on YARN. We’ll look at one or more
    examples of these technologies that have been built with YARN compatibility.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将了解到基于YARN的多个系统，这些系统跨越了广泛的技术领域。我们将查看一些使用YARN兼容性构建的技术示例。
- en: 2.3.1\. NoSQL
  id: totrans-563
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1. NoSQL
- en: NoSQL covers a wide array of technologies, but, in short, they’re systems that
    provide real-time CRUD operations in a way that doesn’t hold ACID properties sacred.
    These systems were created to work around the shortcomings of monolithic OLAP
    systems, which impeded the ability of system architectures to scale out and provide
    responsive services.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL涵盖了广泛的技术，但简而言之，它们是提供实时CRUD操作但不神圣化ACID属性的系统。这些系统是为了克服单体OLAP系统的不足而创建的，这些系统阻碍了系统架构扩展和提供响应性服务的能力。
- en: There are many NoSQL systems out there, but none have been more integrated with
    Hadoop than HBase. Even prior to YARN, the goal of HBase was to use HDFS for its
    storage, and HBase benefited from close integration with MapReduce, allowing for
    batch-processing facilities that often eluded its competitors.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有许多NoSQL系统，但没有哪一个比HBase与Hadoop的集成更紧密。甚至在YARN出现之前，HBase的目标就是使用HDFS进行存储，并且HBase通过与MapReduce的紧密集成受益，这为批量处理提供了竞争对手通常难以实现的功能。
- en: YARN solves two challenges for HBase. HBase and MapReduce 1 coexisting on a
    cluster brought resource management challenges, as there were no easy ways to
    guarantee SLAs to both systems. YARN capitalizes on cgroups in Linux, which provide
    concurrently executing processes with guaranteed access to their required resources.
    The second opportunity that YARN gave HBase was the ability to run multiple HBase
    clusters on the same Hadoop cluster. This support is being carried out in a project
    called Hoya, short for HBase on Yarn.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: YARN为HBase解决了两个挑战。HBase和MapReduce 1在同一个集群中共存带来了资源管理挑战，因为没有简单的方法可以保证同时为这两个系统提供SLA。YARN利用Linux中的cgroups，为并发执行的过程提供保证其所需资源的访问。YARN给HBase带来的第二个机会是能够在同一个Hadoop集群上运行多个HBase集群。这个支持正在一个名为Hoya的项目中实施，Hoya是HBase
    on Yarn的简称。
- en: 2.3.2\. Interactive SQL
  id: totrans-567
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 交互式SQL
- en: Up until recently, running SQL on Hadoop has been an exercise in patience—kick
    up your Hive shell, enter your query, and wait, often minutes, until you get a
    result.^([[15](#ch02fn18)]) Data scientists and analysts would likely not find
    this to be the most conducive environment for quickly probing and experimenting
    with data.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，在Hadoop上运行SQL一直是一项需要耐心的练习——启动Hive shell，输入查询，然后等待，通常需要几分钟，才能得到结果.^([15](#ch02fn18))
    数据科学家和分析师可能不会认为这是一个快速探索和实验数据的最佳环境。
- en: ^(15) The reason Hive queries used to take a long time is that they would be
    translated to one or more MapReduce jobs, so job startup times (coupled with writing
    intermediary outputs to and from disk) resulted in long query times.
  id: totrans-569
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([15](#ch02fn18)) Hive查询之所以以前需要很长时间，是因为它们会被转换为一个或多个MapReduce作业，因此作业启动时间（加上将中间输出写入和从磁盘读取）导致了查询时间过长。
- en: There have been several initiatives to work around this issue. Cloudera’s solution
    was to create the Impala project, which bypasses MapReduce altogether and operates
    by running its own daemon on each slave node in your cluster (colocated with the
    HDFS slave daemon, the DataNode, for data locality). To help with multi-tenancy
    on YARN clusters, Cloudera has developed Llama ([http://cloudera.github.io/llama/](http://cloudera.github.io/llama/)),
    which aims to work with YARN in such a way that YARN understands the resources
    that the Impala daemons are utilizing on a cluster.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有几项举措来解决这个问题。Cloudera的解决方案是创建Impala项目，该项目完全绕过MapReduce，通过在每个从节点上运行自己的守护进程（与HDFS从守护进程、DataNode一起本地化，以实现数据局部性）来运行。为了帮助YARN集群上的多租户，Cloudera开发了Llama([http://cloudera.github.io/llama/](http://cloudera.github.io/llama/))，旨在以这种方式与YARN协同工作，使得YARN能够理解Impala守护进程在集群上使用的资源。
- en: Hortonworks has taken a different approach—they’ve focused on making improvements
    to Hive and have made significant steps toward making Hive more interactive. They’ve
    combined their improvements under a project called Stinger ([http://hortonworks.com/labs/stinger/](http://hortonworks.com/labs/stinger/)),
    and the most significant change involves bypassing Map-Reduce and using Tez, a
    YARN DAG processing framework, to execute their work.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: Hortonworks采取了一种不同的方法——他们专注于对Hive进行改进，并已迈出了使Hive更具交互性的重要步伐。他们将改进成果整合在一个名为Stinger的项目下([http://hortonworks.com/labs/stinger/](http://hortonworks.com/labs/stinger/))，其中最显著的变化是绕过Map-Reduce，使用Tez，一个YARN
    DAG处理框架来执行工作。
- en: Apache Drill is another SQL-on-Hadoop solution that promises the ability to
    work over many persistent stores, such as Cassandra and MongoDB. They have an
    open ticket to add YARN support to the project ([https://issues.apache.org/jira/browse/DRILL-142](https://issues.apache.org/jira/browse/DRILL-142)).
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Drill是另一个承诺能够在多个持久存储上工作的SQL-on-Hadoop解决方案，例如Cassandra和MongoDB。他们有一个开放的任务单，旨在向项目中添加YARN支持([https://issues.apache.org/jira/browse/DRILL-142](https://issues.apache.org/jira/browse/DRILL-142))。
- en: Facebook Presto is also in the SQL-on-Hadoop camp, but so far there’s no word
    on whether there will be YARN support.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook Presto也属于SQL-on-Hadoop阵营，但到目前为止还没有关于是否会支持YARN的消息。
- en: 2.3.3\. Graph processing
  id: totrans-574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. 图处理
- en: Modern graph-processing systems allow distributed graph algorithms to execute
    against large graphs that contain billions of nodes and trillions of edges. Graph
    operations using traditional MapReduce typically result in one job per iteration,^([[16](#ch02fn19)])
    which is slow and cumbersome, as it requires the entire graph data structure to
    be serialized to and from disk on each iteration.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 现代图处理系统允许分布式图算法在包含数十亿个节点和数万亿条边的庞大图上执行。使用传统 MapReduce 的图操作通常会导致每个迭代一个作业，^([[16](#ch02fn19)])
    这很慢且繁琐，因为它需要在每个迭代中将整个图数据结构序列化到磁盘上。
- en: ^(16) Giraph in its MapReduce 1 implementation works around this by using long-running
    map tasks that exchange state with ZooKeeper and pass messages to each other.
  id: totrans-576
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(16) Giraph 在其 MapReduce 1 实现中通过使用与 ZooKeeper 交换状态的长运行 map 任务以及相互传递消息来解决这个问题。
- en: Apache Giraph, a popular graph-processing project, has worked on Hadoop since
    version 1 and earlier, and the committers have also updated Giraph so that it
    runs as a native YARN application.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Giraph 是一个流行的图处理项目，自版本 1 及更早版本以来一直在 Hadoop 上工作，提交者还更新了 Giraph，使其能够作为一个原生
    YARN 应用程序运行。
- en: Apache Hama also has some graph-processing capabilities on YARN.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hama 也在 YARN 上具备一些图处理能力。
- en: 2.3.4\. Real-time data processing
  id: totrans-579
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4\. 实时数据处理
- en: Real-time data processing systems are computational systems that work on unbounded
    streams of data. The features of these systems are similar to those of MapReduce,
    as they allow operations such as filtering, projection, joins, and aggregations.
    A typical use of these systems is to process real-time events occurring in a system,
    perform some aggregations, and then push the results out to a NoSQL store for
    retrieval by another system.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据处理系统是处理无界数据流的工作计算系统。这些系统的特性与 MapReduce 类似，因为它们允许过滤、投影、连接和聚合等操作。这些系统的典型用途是处理系统中发生的实时事件，进行一些聚合，然后将结果推送到
    NoSQL 存储中，以便其他系统检索。
- en: 'Arguably, the real-time data processing system with most traction at the time
    of writing is Apache Storm, originally built by Nathan Marz, which is a key part
    of his Lambda Architecture.^([[17](#ch02fn20)]) To bring Storm to YARN, Yahoo
    has created a project called storm-yarn. This project offers several advantages—not
    only will this allow multiple Storm clusters to run on YARN, but it promises elasticity
    for Storm clusters: the ability to quickly provision additional resources for
    Storm. More details on the project can be seen at [https://github.com/yahoo/storm-yarn](https://github.com/yahoo/storm-yarn).'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 不可否认，在撰写本文时，最具吸引力的实时数据处理系统是 Apache Storm，它最初由 Nathan Marz 构建，是 Lambda 架构的关键部分^([[17](#ch02fn20)])。为了将
    Storm 引入 YARN，Yahoo 创建了一个名为 storm-yarn 的项目。该项目提供了几个优点——不仅允许多个 Storm 集群在 YARN 上运行，而且它还承诺为
    Storm 集群提供弹性：能够快速为 Storm 分配额外资源。有关该项目的更多详细信息，请参阅 [https://github.com/yahoo/storm-yarn](https://github.com/yahoo/storm-yarn)。
- en: ^(17) The Lambda Architecture plays to the strengths of batch and real-time.
    Read more in Nathan Marz’s book, *Big Data* (Manning, 2014).
  id: totrans-582
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(17) Lambda 架构利用了批处理和实时处理的优势。更多内容请参阅 Nathan Marz 的著作 *Big Data*（Manning，2014）。
- en: Spark Streaming is another notable real-time data processing project developed
    as an extension to the Spark API, and it supports consuming data sources such
    as HDFS, Kafka, Flume, and more. Spark is also supported on YARN. Spark Streaming
    may become a strong competitor for Storm, notably because once you master Spark,
    you also know how to do Spark Streaming, and vice versa. This means you have a
    single programming paradigm for both offline and real-time data analysis.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 是另一个值得注意的实时数据处理项目，它是作为 Spark API 的扩展开发的，并支持消费诸如 HDFS、Kafka、Flume
    等数据源。Spark 也支持在 YARN 上运行。Spark Streaming 可能会成为 Storm 的强大竞争对手，尤其是因为一旦你掌握了 Spark，你也知道如何进行
    Spark Streaming，反之亦然。这意味着你有一个单一的编程范式，既可以用于离线数据分析，也可以用于实时数据分析。
- en: Other real-time data processing systems with YARN integration are Apache S4,
    Apache Samza (which came out of LinkedIn), and DataTorrent.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 其他与 YARN 集成的实时数据处理系统包括 Apache S4、Apache Samza（源自 LinkedIn）和 DataTorrent。
- en: 2.3.5\. Bulk synchronous parallel
  id: totrans-585
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5\. 批量同步并行
- en: Bulk synchronous parallel (BSP) is a distributed processing method whereby multiple
    parallel workers independently work on a subset of an overall problem, after which
    they exchange data among themselves and then use a global synchronization mechanism
    to wait for all workers to complete before repeating the process. Google Pregel
    published how their graph processing framework is inspired by BSP, and Apache
    Giraph uses a similar BSP model for graph iteration.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 批量同步并行（BSP）是一种分布式处理方法，其中多个并行工作者独立地对整体问题的一个子集进行工作，之后他们相互交换数据，然后使用全局同步机制等待所有工作者完成，然后再重复该过程。Google
    Pregel 发布了他们的图处理框架如何受到 BSP 启发，Apache Giraph 使用类似的 BSP 模型进行图迭代。
- en: Apache Hama is a general-purpose BSP implementation that can work on YARN. It
    also has built-in graph-processing capabilities.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hama 是一个通用的 BSP（Bulk Synchronous Parallel）实现，它可以在 YARN 上运行。它还具备内置的图处理能力。
- en: 2.3.6\. MPI
  id: totrans-588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.6\. MPI
- en: MPI (Message Passing Interface) is a mechanism that allows messages to be exchanged
    on clusters of hosts. Open MPI is an open source MPI implementation. There’s currently
    an open ticket to complete work on integrating Open MPI support into Hadoop ([https://issues.apache.org/jira/browse/MAPREDUCE-2911](https://issues.apache.org/jira/browse/MAPREDUCE-2911)).
    The work that has been completed so far for this integration is in mpich2-yarn
    at [https://github.com/clarkyzl/mpich2-yarn](https://github.com/clarkyzl/mpich2-yarn).
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: MPI（消息传递接口）是一种机制，允许在主机集群之间交换消息。Open MPI 是一个开源的 MPI 实现。目前有一个开放工单来完成将 Open MPI
    支持集成到 Hadoop 的工作 ([https://issues.apache.org/jira/browse/MAPREDUCE-2911](https://issues.apache.org/jira/browse/MAPREDUCE-2911))。为此集成已完成的工作在
    mpich2-yarn 的 [https://github.com/clarkyzl/mpich2-yarn](https://github.com/clarkyzl/mpich2-yarn)。
- en: 2.3.7\. In-memory
  id: totrans-590
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.7\. 内存中
- en: In-memory computing uses the ever-increasing memory footprint in our systems
    to quickly perform computing activities such as iterative processing and interactive
    data mining.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 内存计算利用我们系统中不断增长的内存占用，快速执行迭代处理和交互式数据挖掘等计算活动。
- en: Apache Spark is a popular example that came out of Berkeley. It’s a key part
    of an overall set of solutions that also includes Shark for SQL operations and
    GraphX for graph processing. Cloudera’s CDH5 distribution includes Spark running
    on YARN.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是来自伯克利的一个流行例子。它是包括 Shark 用于 SQL 操作和 GraphX 用于图处理在内的整体解决方案的关键部分。Cloudera
    的 CDH5 发行版包括在 YARN 上运行的 Spark。
- en: For additional details on how to run Spark on YARN, see Spark’s “Launching Spark
    on YARN” page at [http://spark.apache.org/docs/0.9.0/running-on-yarn.html](http://spark.apache.org/docs/0.9.0/running-on-yarn.html).
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何在 YARN 上运行 Spark 的更多详细信息，请参阅 Spark 的“在 YARN 上启动 Spark”页面，[http://spark.apache.org/docs/0.9.0/running-on-yarn.html](http://spark.apache.org/docs/0.9.0/running-on-yarn.html)。
- en: 2.3.8\. DAG execution
  id: totrans-594
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.8\. DAG 执行
- en: Directed Acyclic Graph (DAG) execution engines allow you to model data-processing
    logic as a DAG and then execute it in parallel over a large dataset.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 有向无环图（DAG）执行引擎允许您将数据处理逻辑建模为 DAG，然后在大型数据集上并行执行。
- en: Apache Tez is an example of a DAG execution engine; it was born out of the need
    to provide a more generalized MapReduce system that would preserve the parallelism
    and throughput of MapReduce, and at the same time support additional processing
    models and optimizations beyond that which MapReduce provides. Examples of Tez’s
    abilities include not imposing a specific data model, so that both the key/value
    model of MapReduce, as well as the tuple-based models of Hive and Pig, can be
    supported.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Tez 是 DAG（Directed Acyclic Graph）执行引擎的一个例子；它诞生于提供更通用化的 MapReduce 系统的需求，该系统将保留
    MapReduce 的并行性和吞吐量，同时支持 MapReduce 提供的额外处理模型和优化。Tez 的能力示例包括不强制使用特定的数据模型，因此既支持 MapReduce
    的键/值模型，也支持 Hive 和 Pig 的基于元组的模型。
- en: Tez provides a number of advantages over MapReduce, which include eliminating
    replicated write barriers that exist in MapReduce between multiple jobs—a major
    performance bottleneck for systems like Hive and Pig. Tez can also support reduce
    operations without the sorting overhead that MapReduce requires, resulting in
    more efficient pipelines where sorting isn’t necessary for the application. Tez
    also supports sophisticated operations such as Map-Map-Reduce, or any arbitrary
    graph of operations, freeing up developers to more naturally express their data
    pipelines. Tez can also be used to make dynamic data flow choices when executing—for
    example, based on the size of intermediary data in your flow, you may decide to
    store it in memory or in HDFS or local disk.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 与MapReduce相比，Tez提供了许多优势，包括消除MapReduce中存在于多个作业之间的复制写屏障——这是像Hive和Pig这样的系统的主要性能瓶颈。Tez还可以在不需要MapReduce所需的排序开销的情况下支持reduce操作，从而在不需要排序的应用程序中实现更高效的管道。Tez还支持复杂的操作，如Map-Map-Reduce或任何任意操作图，让开发者能够更自然地表达他们的数据管道。Tez还可以在执行时进行动态数据流选择——例如，根据你流程中中间数据的大小，你可能决定将其存储在内存中或在HDFS或本地磁盘上。
- en: The upshot of all of this is that Tez can shake off the batch-only shackles
    of Map-Reduce and support interactive use cases. As an example, the original scope
    of Tez is a large step in Hortonworks’ goal of making Hive interactive—moving
    from Map-Reduce to Tez is a key part of that work.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些的结果是Tez可以摆脱Map-Reduce仅支持批处理的束缚，并支持交互式用例。以一个例子来说，Tez最初的范围是实现Hortonworks使Hive交互式目标的一大步——从Map-Reduce迁移到Tez是这项工作的关键部分。
- en: 2.4\. Chapter summary
  id: totrans-599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4. 章节总结
- en: Hadoop version 2 turns the old way work has been done in Hadoop upside down.
    No longer are you limited to running MapReduce on your clusters. This chapter
    covered the essentials that you need to get going with YARN. You looked at why
    YARN is important in Hadoop, saw a high-level overview of the architecture, and
    learned about some of the salient YARN configuration properties that you’ll need
    to use.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2版本颠覆了Hadoop中工作方式的传统。你不再局限于在集群上运行MapReduce。本章涵盖了开始使用YARN所需的基本知识。你了解了为什么YARN在Hadoop中很重要，看到了架构的高级概述，并学习了你需要使用的一些显著的YARN配置属性。
- en: The advent of YARN has also introduced significant changes in how MapReduce
    works. MapReduce has been ported into a YARN application, and in [section 2.2](#ch02lev1sec2)
    you saw how MapReduce executes on Hadoop 2, learned what configuration properties
    have changed, and also picked up some new features, such as uber jobs.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: YARN的出现也带来了MapReduce工作方式的重要变化。MapReduce已被移植到YARN应用程序中，在[第2.2节](#ch02lev1sec2)中，你看到了MapReduce在Hadoop
    2中的执行方式，了解了哪些配置属性发生了变化，还了解了一些新特性，例如uber作业。
- en: The last section of this chapter covered some exciting examples of up-and-coming
    YARN applications to give you a sense of what capabilities you should expect to
    be able to unleash on your YARN cluster. For additional YARN coverage, feel free
    to skip ahead to [chapter 10](kindle_split_023.html#ch10) and look at how to develop
    your very own YARN application!
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后部分介绍了一些新兴YARN应用程序的精彩示例，以让你对在YARN集群上可以期待释放的能力有所了解。如需了解更多关于YARN的内容，请随意跳转到[第10章](kindle_split_023.html#ch10)并查看如何开发你自己的YARN应用程序！
- en: Now that you understand the lay of the land with YARN, it’s time to move on
    to look at data storage in Hadoop. The focus of the next chapter is on working
    with common file formats such as XML and JSON, as well as picking file formats
    better suited for life in Hadoop, such as Parquet and Avro.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了YARN的布局，是时候转向查看Hadoop中的数据存储了。下一章的重点是处理常见的文件格式，如XML和JSON，以及选择更适合Hadoop生活的文件格式，如Parquet和Avro。
