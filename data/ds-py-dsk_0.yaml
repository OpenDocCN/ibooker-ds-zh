- en: Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分
- en: The building blocks of scalable computing
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展计算的基本构建块
- en: This part of the book covers some fundamental concepts in scalable computing
    to give you a good basis for understanding what makes Dask different and how it
    works “under the hood.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书这一部分涵盖了可扩展计算的一些基本概念，为你理解Dask的独特之处及其工作原理提供良好的基础。
- en: In chapter 1, you’ll learn what a *directed acyclic graph* (DAG)*is and why
    it’s useful for scaling out workloads across many different workers.*
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，你将学习什么是*有向无环图*（DAG）以及为什么它在扩展工作负载到许多不同的工作者时很有用。
- en: '*Chapter 2 explains how Dask uses DAGs as an abstraction to enable you to analyze
    huge datasets and take advantage of scalability and parallelism whether you’re
    running your code on a laptop or a cluster of thousands of machines.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二章解释了Dask如何使用DAGs作为抽象，使你能够分析大型数据集，并利用可扩展性和并行性，无论你在笔记本电脑上还是在成千上万的机器集群上运行代码。'
- en: Once you’ve completed part 1, you’ll have a basic understanding of the internals
    of Dask, and you’ll be ready to get some hands-on experience with a real dataset.*  *#
    1
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 完成第一部分后，你将基本了解Dask的内部结构，并准备好通过实际数据集获得一些实践经验。*  *# 1
- en: Why scalable computing matters
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展计算的重要性
- en: '**This chapter covers**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容涵盖**'
- en: Presenting what makes Dask a standout framework for scalable computing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示是什么使Dask成为可扩展计算领域的杰出框架
- en: Demonstrating how to read and interpret directed acyclic graphs (DAGs) using
    a pasta recipe as a tangible example
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以意大利面食谱作为具体实例，展示如何阅读和解释有向无环图（DAGs）
- en: Discussing why DAGs are useful for distributed workloads and how Dask’s task
    scheduler uses DAGs to compose, control, and monitor computations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论为什么DAGs对分布式工作负载很有用，以及Dask的任务调度器如何使用DAGs来组合、控制和监控计算
- en: Introducing the companion dataset
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍配套数据集
- en: 'Welcome to *Data Science with Python and Dask*! Since you’ve decided to pick
    up this book, no doubt you are interested in data science and machine learning—perhaps
    you’re even a practicing data scientist, analyst, or machine learning engineer.
    However, I suspect that you’re either currently facing a significant challenge,
    or you’ve faced it at some point in your career. I’m talking, of course, about
    the notorious challenges that arise when working with large datasets. The symptoms
    are easy to spot: painfully long run times—even for the simplest of calculations—unstable
    code, and unwieldy workflows. But don’t despair! These challenges have become
    commonplace as both the expense and effort to collect and store vast quantities
    of data have declined significantly. In response, the computer science community
    has put a great deal of effort into creating better, more accessible programming
    frameworks to reduce the complexity of working with massive datasets. While many
    different technologies and frameworks aim to solve these problems, few are as
    powerful and flexible as Dask. This book aims to take your data science skills
    to the next level by giving you the tools and techniques you need to analyze and
    model large datasets using Dask.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到*使用Python和Dask进行数据科学*！既然你决定拿起这本书，毫无疑问你对数据科学和机器学习感兴趣——也许你甚至是一名实践中的数据科学家、分析师或机器学习工程师。然而，我怀疑你目前可能面临一个重大的挑战，或者你在职业生涯的某个阶段遇到过。我当然是在谈论与大型数据集一起工作时出现的臭名昭著的挑战。症状很容易识别：计算时间漫长——即使是简单的计算——代码不稳定，工作流程难以管理。但不要绝望！随着收集和存储大量数据的成本和努力显著下降，这些挑战已经变得司空见惯。作为回应，计算机科学界投入了大量努力，创建更好的、更易于使用的编程框架，以降低处理大量数据集的复杂性。虽然许多不同的技术和框架旨在解决这些问题，但Dask作为其中之一，既强大又灵活。本书旨在通过提供你分析和使用Dask分析大型数据集所需的工具和技术，将你的数据科学技能提升到下一个层次。
- en: While the majority of this book is centered around hands-on examples of typical
    tasks that you as a data scientist or data engineer will encounter on most projects,
    this chapter will cover some fundamental knowledge essential for understanding
    how Dask works “under the hood.” First, we’ll examine why a tool like Dask is
    even necessary to have in your data science toolkit and what makes it unique;
    then, we’ll cover directed acyclic graphs, a concept that Dask uses extensively
    to control parallel execution of code. With that knowledge, you should have a
    better understanding of how Dask works when you ask it to crunch through a big
    dataset; this knowledge will serve you well as you continue through your Dask
    journey, and we will come back to this knowledge in later chapters when we walk
    through how to build out your own cluster in the cloud. With that in mind, we’ll
    turn our focus to what makes Dask unique, and why it’s a valuable tool for data
    science.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这本书的大部分内容都集中在数据科学家或数据工程师在大多数项目中会遇到的实际任务的手动示例，但本章将涵盖理解Dask“内部工作原理”的一些基本知识。首先，我们将探讨为什么在数据科学工具箱中拥有像Dask这样的工具是必要的，以及它独特之处在哪里；然后，我们将介绍有向无环图，这是Dask广泛使用来控制代码并行执行的概念。有了这些知识，当你要求Dask处理大数据集时，你应该对它的工作原理有更好的理解；这些知识将伴随你在Dask之旅中继续前进，我们将在后面的章节中回顾这些知识，当我们介绍如何在云中构建自己的集群时。考虑到这一点，我们将关注Dask的独特之处，以及为什么它是数据科学中的一个宝贵工具。
- en: 1.1 Why Dask?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 为什么选择Dask？
- en: 'For many modern organizations, the promise of data science’s transformative
    powers is universally alluring—and for good reason. In the right hands, effective
    data science teams can transform mere ones and zeros into real competitive advantages.
    Making better decisions, optimizing business processes, and detecting strategic
    blind spots are all touted as benefits of investing in data science capabilities.
    However, what we call “data science” today isn’t really a new idea. For the past
    several decades, organizations all over the world have been trying to find better
    ways to make strategic and tactical decisions. Using names like *decision support*,
    *business intelligence*, *analytics*, or just plain old *operations research*,
    the goals of each have been the same: keep tabs on what’s happening and make better-informed
    decisions. What has changed in recent years, however, is that the barriers to
    learning and applying data science have been significantly lowered. Data science
    is no longer relegated to operations research journals or academic-like research
    and development arms of large consulting groups. A key enabler of bringing data
    science to the masses has been the rising popularity of the Python programming
    language and its powerful collection of libraries called the Python Open Data
    Science Stack. These libraries, which include NumPy, SciPy, Pandas, and scikit-learn,
    have become industry-standard tools that boast a large community of developers
    and plentiful learning materials. Other languages that have been historically
    favored for this kind of work, such as FORTRAN, MATLAB, and Octave, are more difficult
    to learn and don’t have nearly the same amount of community support. For these
    reasons, Python and its Open Data Science Stack has become one of the most popular
    platforms both for learning data science and for everyday practitioners.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多现代组织来说，数据科学变革力量的承诺具有普遍的吸引力——而且理由充分。在合适的人手中，有效的数据科学团队能够将零散的数据转化为真正的竞争优势。做出更好的决策、优化业务流程和发现战略盲点都被视为投资数据科学能力的益处。然而，我们今天所说的“数据科学”并不是一个全新的概念。在过去几十年里，全球各地的组织都在尝试找到更好的方法来做出战略和战术决策。使用诸如“决策支持”、“商业智能”、“分析”或简单的“运筹学”等名称，每个领域的目标都是相同的：跟踪正在发生的事情并做出更明智的决策。然而，近年来发生的变化是，学习和应用数据科学的障碍已经显著降低。数据科学不再局限于运筹学期刊或大型咨询集团类似学术的研究和开发部门。将数据科学带给大众的关键推动力是Python编程语言的日益流行及其强大的Python开放数据科学堆栈库集。这些库包括NumPy、SciPy、Pandas和scikit-learn，已成为行业标准工具，拥有庞大的开发者社区和丰富的学习资料。其他历史上被用于此类工作的语言，如FORTRAN、MATLAB和Octave，学习起来更困难，且社区支持远不如Python及其开放数据科学堆栈。
- en: Alongside these developments in data science accessibility, computers have continued
    to become ever more powerful. This makes it easy to produce, collect, store, and
    process far more data than before, all at a price that continues to march downward.
    But this deluge of data now has many organizations questioning the value of collecting
    and storing all that data—and rightfully so! Raw data has no intrinsic value;
    it must be cleaned, scrutinized, and interpreted to extract actionable information
    out of it. Obviously, this is where you—the data scientist—come into play. Working
    with the Python Open Data Science Stack, data scientists often turn to tools like
    Pandas for data cleaning and exploratory data analysis, SciPy and NumPy to run
    statistical tests on the data, and scikit-learn to build predictive models. This
    all works well for relatively small-sized datasets that can comfortably fit into
    RAM. But because of the shrinking expense of data collection and storage, data
    scientists are more frequently working on problems that involve analyzing enormous
    datasets. These tools have upper limits to their feasibility when working with
    datasets beyond a certain size. Once the threshold is crossed, the problems described
    in the beginning of the chapter start to appear. But where is that threshold?
    To avoid the ill-defined and oft-overused term *big data*, we’ll use a three-tiered
    definition throughout the book to describe different-sized datasets and the challenges
    that come with each. [Table 1.1](#table1.1) describes the different criteria we’ll
    use to define the terms *small dataset**, medium dataset**,* and *large dataset*
    throughout the book.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学可访问性不断发展的同时，计算机的运算能力也在不断增强。这使得生产、收集、存储和处理比以前多得多的数据变得容易，而且价格持续下降。但如今这股数据洪流让许多组织开始质疑收集和存储所有这些数据的价值——这是理所当然的！原始数据本身没有内在价值；必须对其进行清理、审查和解释，才能从中提取可操作的信息。显然，这正是你——数据科学家——发挥作用的地方。与
    Python 开放数据科学栈一起工作，数据科学家通常会转向 Pandas 等工具进行数据清理和探索性数据分析，使用 SciPy 和 NumPy 对数据进行统计分析，以及使用
    scikit-learn 构建预测模型。这对于相对较小且能够舒适地适应 RAM 的大小数据集来说效果很好。但由于数据收集和存储成本的降低，数据科学家越来越多地处理涉及分析巨大数据集的问题。当处理超过一定大小的数据集时，这些工具的可行性存在上限。一旦超过这个阈值，章节开头描述的问题就开始出现。但这个阈值在哪里？为了避免使用定义不明确且经常被滥用的术语“大数据”，本书将使用三层定义来描述不同大小的数据集及其带来的挑战。[表
    1.1](#table1.1) 描述了本书中将使用的不同标准来定义“小型数据集”、“中型数据集”和“大型数据集”。
- en: Table 1.1 A tiered definition of data sizes
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 数据大小的分层定义
- en: '| **Dataset type** | **Size range** | **Fits in RAM?** | **Fits on local disk?**
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **数据集类型** | **大小范围** | **是否适应 RAM** | **是否适应本地磁盘** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Small dataset | Less than 2–4 GB | Yes | Yes |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 小型数据集 | 小于 2–4 GB | 是 | 是 |'
- en: '| Medium dataset | Less than 2 TB | No | Yes |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 中型数据集 | 小于 2 TB | 否 | 是 |'
- en: '| Large dataset | Greater than 2 TB | No | No |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 大型数据集 | 大于 2 TB | 否 | 否 |'
- en: Small datasets are datasets that fit comfortably in RAM, leaving memory to spare
    for manipulation and transformations. They are usually no more than 2–4 GB in
    size, and complex operations like sorting and aggregating can be done without
    paging. Paging, or spilling to disk, uses a computer’s persistent storage (such
    as a hard disk or solid-state drive) as an extra place to store intermediate results
    while processing. It can greatly slow down processing because persistent storage
    is less efficient than RAM at fast data access. These datasets are frequently
    encountered when learning data science, and tools like Pandas, NumPy, and scikit-learn
    are the best tools for the job. In fact, throwing more sophisticated tools at
    these problems is not only overkill, but can be counterproductive by adding unnecessary
    layers of complexity and management overhead that can reduce performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 小型数据集是指可以舒适地适应 RAM 的数据集，留有足够的内存进行操作和转换。它们的大小通常不超过 2–4 GB，复杂的操作如排序和聚合可以在不进行分页的情况下完成。分页，或溢出到磁盘，使用计算机的持久存储（如硬盘或固态驱动器）作为存储中间结果的额外空间，在处理过程中可能会大大减慢处理速度，因为持久存储在快速数据访问方面不如
    RAM 效率高。这些数据集在学习数据科学时经常遇到，Pandas、NumPy 和 scikit-learn 等工具是处理这些工作的最佳工具。实际上，将这些更复杂的工具应用于这些问题不仅过度，而且可能适得其反，因为它们增加了不必要的复杂性和管理开销，这可能会降低性能。
- en: Medium datasets are datasets that cannot be held entirely in RAM but can fit
    comfortably in a single computer’s persistent storage. These datasets typically
    range in size from 10 GB to 2 TB. While it’s possible to use the same toolset
    to analyze both small datasets and medium datasets, a significant performance
    penalty is imposed because these tools must use paging in order to avoid out-of-memory
    errors. These datasets are also large enough that it can make sense to introduce
    parallelism to cut down processing time. Rather than limiting execution to a single
    CPU core, dividing the work across all available CPU cores can speed up computations
    substantially. However, Python was not designed to make sharing work between processes
    on multicore systems particularly easy. As a result, it can be difficult to take
    advantage of parallelism within Pandas.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 中等数据集是指不能完全保持在RAM中，但可以舒适地存放在单个计算机的持久存储中的数据集。这些数据集的大小通常在10 GB到2 TB之间。虽然可以使用相同的工具集来分析小数据集和中等数据集，但由于这些工具必须使用分页来避免内存不足错误，因此会带来显著的性能损失。这些数据集也足够大，引入并行处理以减少处理时间是有意义的。而不是将执行限制在单个CPU核心上，将工作分配到所有可用的CPU核心可以显著加快计算速度。然而，Python并非设计为在多核系统上共享进程间的任务变得特别容易。因此，在Pandas中利用并行性可能会很困难。
- en: Large datasets are datasets that can neither fit in RAM nor fit in a single
    computer’s persistent storage. These datasets are typically above 2 TB in size,
    and depending on the problem, can reach into petabytes and beyond. Pandas, NumPy,
    and scikit-learn are not suitable at all for datasets of this size, because they
    were not inherently built to operate on distributed datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据集是指既不能完全装入RAM，也不能装入单个计算机的持久存储的数据集。这些数据集通常大小超过2 TB，根据问题的不同，可以达到PB甚至更高。Pandas、NumPy和scikit-learn完全不适用于这种规模的数据集，因为它们并非天生就是为了在分布式数据集上操作而构建的。
- en: Naturally, the boundaries between these thresholds are a bit fuzzy and depend
    on how powerful your computer is. The significance lies more in the different
    orders of magnitude rather than hard size limits. For example, on a very powerful
    computer, small data might be on the order of 10s of gigabytes, but not on the
    order of terabytes. Medium data might be on the order of 10s of terabytes, but
    not on the order of petabytes. Regardless, the most important takeaway is that
    there are advantages (and often necessities) of looking for alternative analysis
    tools when your dataset is pushing the limits of our definition of small data.
    However, choosing the right tool for the job can be equally challenging. Oftentimes,
    this can lead data scientists to get stuck with evaluating unfamiliar technologies,
    rewriting code in different languages, and generally slowing down the projects
    they are working on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，这些阈值之间的界限有些模糊，并且取决于你的计算机有多强大。其重要性更多在于不同数量级的差异，而不是硬性的大小限制。例如，在非常强大的计算机上，小数据可能只有10s
    GB的数量级，而不是TB的数量级。中等数据可能达到10s TB的数量级，而不是PB的数量级。无论如何，最重要的启示是，当你的数据集正在推动我们对小数据定义的极限时，寻找替代分析工具是有优势的（并且通常是必需的）。然而，选择合适的工具同样具有挑战性。很多时候，这会导致数据科学家陷入评估不熟悉的技术、用不同语言重写代码，以及一般性地减缓他们正在工作的项目。
- en: 'Dask was launched in late 2014 by Matthew Rocklin with aims to bring native
    scalability to the Python Open Data Science Stack and overcome its single-machine
    restrictions. Over time, the project has grown into arguably one of the best scalable
    computing frameworks available for Python developers. Dask consists of several
    different components and APIs, which can be categorized into three layers: the
    scheduler, low-level APIs, and high-level APIs. An overview of these components
    can be seen in [figure 1.1](#figure1.1).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Dask是由Matthew Rocklin在2014年底推出的，旨在为Python Open Data Science Stack带来原生可扩展性，并克服其单机限制。随着时间的推移，该项目已经发展成为Python开发者可用的最佳可扩展计算框架之一。Dask由几个不同的组件和API组成，可以分为三个层次：调度器、低级API和高级API。这些组件的概述可以在[图1.1](#figure1.1)中看到。
- en: '![c01_01.eps](Images/c01_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![c01_01.eps](Images/c01_01.png)'
- en: '[Figure 1.1](#figureanchor1.1) The components and layers than make up Dask'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.1](#figureanchor1.1) 构成Dask的组件和层次结构'
- en: 'What makes Dask so powerful is how these components and layers are built on
    top of one another. At the core is the task scheduler, which coordinates and monitors
    execution of computations across CPU cores and machines. These computations are
    represented in code as either Dask Delayed objects or Dask Futures objects (the
    key difference is the former are evaluated *lazily* —meaning they are evaluated
    just in time when the values are needed, while the latter are evaluated *eagerly*
    —meaning they are evaluated in real time regardless if the value is needed immediately
    or not). Dask’s high-level APIs offer a layer of abstraction over Delayed and
    Futures objects. Operations on these high-level objects result in many parallel
    low-level operations managed by the task schedulers, which provides a seamless
    experience for the user. Because of this design, Dask brings four key advantages
    to the table:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使 Dask 非常强大的原因是这些组件和层是如何相互构建的。核心是任务调度器，它协调并监控跨 CPU 核心和机器的计算执行。这些计算在代码中表现为 Dask
    延迟对象或 Dask 未来对象（关键区别在于前者是*惰性评估*——意味着仅在需要值时才进行评估，而后者是*即时评估*——意味着无论是否需要立即评估，都会实时进行评估）。Dask
    的高级 API 为延迟和未来对象提供了一层抽象。对这些高级对象的操作会导致许多由任务调度器管理的并行低级操作，这为用户提供了无缝的体验。正因为这种设计，Dask
    带来了四个关键优势：
- en: Dask is fully implemented in Python and natively scales NumPy, Pandas, and scikit-learn.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 完全用 Python 实现，并原生扩展了 NumPy、Pandas 和 scikit-learn。
- en: Dask can be used effectively to work with both medium datasets on a single machine
    and large datasets on a cluster.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 可以有效地用于处理单台机器上的中等数据集以及集群上的大数据集。
- en: Dask can be used as a general framework for parallelizing most Python objects.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 可以作为一个通用框架，用于并行化大多数 Python 对象。
- en: Dask has a very low configuration and maintenance overhead.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 的配置和维护开销非常低。
- en: 'The first thing that sets Dask apart from the competition is that it is written
    and implemented entirely in Python, and its collection APIs natively scale NumPy,
    Pandas, and scikit-learn. This doesn’t mean that Dask merely mirrors common operations
    and patterns that NumPy and Pandas users will find familiar; it means that the
    underlying objects used by Dask *are* corresponding objects from each respective
    library. A Dask DataFrame is made up of many smaller Pandas DataFrames, a Dask
    Array is made up of many smaller NumPy Arrays, and so forth. Each of the smaller
    underlying objects, called *chunks* or *partitions*, can be shipped from machine
    to machine within a cluster, or queued up and worked on one piece at a time locally.
    We will cover this process much more in depth later, but the approach of breaking
    up medium and large datasets into smaller pieces and managing the parallel execution
    of functions over those pieces is fundamentally how Dask is able to gracefully
    handle datasets that would be too large to work with otherwise. The practical
    result of using these objects to underpin Dask’s distributed collections is that
    many of the functions, attributes, and methods that Pandas and NumPy users will
    already be familiar with are syntactically equivalent in Dask. This design choice
    makes transitioning from working with small datasets to medium and large datasets
    very easy for experienced Pandas, NumPy, and scikit-learn users. Rather than learning
    new syntax, transitioning data scientists can focus on the most important aspect
    of learning about scalable computing: writing code that’s robust, performant,
    and optimized for parallelism. Fortunately, Dask does a lot of the heavy lifting
    for common use cases, but throughout the book we’ll examine some best practices
    and pitfalls that will enable you to use Dask to its fullest extent.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 与其他竞争者最大的不同之处在于，它是完全用 Python 编写和实现的，并且其集合 API 能够原生地扩展 NumPy、Pandas 和 scikit-learn。这并不意味着
    Dask 仅仅反映了 NumPy 和 Pandas 用户会感到熟悉的常见操作和模式；这意味着 Dask 所使用的底层对象与每个相应库中的对应对象是相匹配的。一个
    Dask DataFrame 由许多较小的 Pandas DataFrame 组成，一个 Dask Array 由许多较小的 NumPy Array 组成，依此类推。每个较小的底层对象，称为
    *chunks* 或 *partitions*，可以在集群内的机器之间传输，或者排队并逐个处理本地。我们将在稍后更深入地介绍这个过程，但将中等和大型数据集拆分成更小的部分并管理这些部分上函数的并行执行，是
    Dask 能够优雅地处理其他情况下难以处理的大型数据集的基本方法。使用这些对象作为 Dask 分布式集合的基础的实际结果是，许多 Pandas 和 NumPy
    用户已经熟悉的函数、属性和方法在 Dask 中的语法是等效的。这种设计选择使得经验丰富的 Pandas、NumPy 和 scikit-learn 用户从处理小型数据集过渡到中等和大型数据集变得非常容易。过渡数据科学家不必学习新的语法，可以专注于学习可扩展计算最重要的方面：编写健壮、性能良好且针对并行性优化的代码。幸运的是，Dask
    为常见用例做了很多繁重的工作，但在这本书中，我们将检查一些最佳实践和陷阱，这将使您能够充分利用 Dask。
- en: 'Next, Dask is just as useful for working with medium datasets on a single machine
    as it is for working with large datasets on a cluster. Scaling Dask up or down
    is not at all complicated. This makes it easy for users to prototype tasks on
    their local machines and seamlessly submit those tasks to a cluster when needed.
    This can all be done without having to refactor existing code or write additional
    code to handle cluster-specific issues like resource management, recovery, and
    data movement. It also gives users a lot of flexibility to choose the best way
    to deploy and run their code. Oftentimes, using a cluster to work with medium
    datasets is entirely unnecessary, and can occasionally be slower due to the overhead
    involved with coordinating many machines to work together. Dask is optimized to
    minimize its memory footprint, so it can gracefully handle medium datasets even
    on relatively low-powered machines. This transparent scalability is thanks to
    Dask’s well-designed built-in task schedulers. The local task scheduler can be
    used when Dask is running on a single machine, and the distributed task scheduler
    can be used for both local execution and execution across a cluster. Dask also
    supports interfacing with popular cluster resource managers such as YARN, Mesos,
    and Kubernetes, allowing you to use an existing cluster with the distributed task
    scheduler. Configuring the task scheduler and using resource managers to deploy
    Dask across any number of systems takes a minimal amount of effort. Throughout
    the book, we’ll look at running Dask in different configurations: locally with
    the local task scheduler, and clustered in the cloud using the distributed task
    scheduler with Docker and Amazon Elastic Container Service.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Dask 在处理单个机器上的中等数据集时同样有用，就像在集群上处理大型数据集一样。调整 Dask 的规模上下并不复杂。这使得用户能够在本地机器上原型化任务，并在需要时无缝地将这些任务提交到集群。所有这些都可以在不修改现有代码或编写额外的代码来处理集群特定问题（如资源管理、恢复和数据移动）的情况下完成。这也为用户提供了很大的灵活性，以选择最佳的代码部署和运行方式。通常，使用集群来处理中等数据集是完全不必要的，有时由于协调多台机器协同工作所带来的开销，可能会更慢。Dask
    优化了其内存占用，因此即使在相对低功耗的机器上也能优雅地处理中等数据集。这种透明的可扩展性归功于 Dask 设计精良的内置任务调度器。当 Dask 在单个机器上运行时，可以使用本地任务调度器；而对于本地执行和跨集群执行，可以使用分布式任务调度器。Dask
    还支持与流行的集群资源管理器（如 YARN、Mesos 和 Kubernetes）接口，允许您使用分布式任务调度器使用现有的集群。配置任务调度器和使用资源管理器将
    Dask 部署到任何数量的系统只需付出最小的努力。在整个书中，我们将探讨在不同配置下运行 Dask：使用本地任务调度器在本地运行，以及使用 Docker 和亚马逊弹性容器服务在云中集群化运行。
- en: One of the most unusual aspects of Dask is its inherent ability to scale most
    Python objects. Dask’s low-level APIs, Dask Delayed and Dask Futures, are the
    common basis for scaling NumPy arrays used in Dask Array, Pandas DataFrames used
    in Dask DataFrame, and Python lists used in Dask Bag. Rather than building distributed
    applications from scratch, Dask’s low-level APIs can be used directly to apply
    all of Dask’s scalability, fault tolerance, and remote execution capabilities
    to any problem.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 最不寻常的方面之一是其固有的扩展大多数 Python 对象的能力。Dask 的低级 API，Dask Delayed 和 Dask Futures，是扩展
    Dask Array 中使用的 NumPy 数组、Dask DataFrame 中使用的 Pandas DataFrame 以及 Dask Bag 中使用的
    Python 列的共同基础。Dask 的低级 API 可以直接使用，将 Dask 的所有可扩展性、容错性和远程执行能力应用于任何问题，而无需从头开始构建分布式应用程序。
- en: Finally, Dask is very lightweight and is easy to set up, tear down, and maintain.
    All its dependencies can be installed using the `pip` or `conda` package manager.
    It’s very easy to build and deploy cluster worker images using Docker, which we
    will do later in the book, and Dask requires very little configuration out of
    the box. Because of this, Dask not only does well for handling recurring jobs,
    but is also a great tool for building proofs of concept and performing ad hoc
    data analysis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Dask 非常轻量级，易于设置、拆除和维护。所有依赖项都可以使用 `pip` 或 `conda` 软件包管理器安装。使用 Docker 构建和部署集群工作器镜像非常容易，我们将在本书的后续部分进行操作，而
    Dask 默认配置要求很少。正因为如此，Dask 不仅在处理重复性工作方面表现良好，而且也是构建概念验证和执行临时数据分析的出色工具。
- en: A common question in the minds of data scientists discovering Dask for the first
    time is how it compares to other superficially similar technologies like Apache
    Spark. Spark has certainly become a very popular framework for analyzing large
    datasets and does quite well in this area. However, although Spark supports several
    different languages including Python, its legacy as a Java library can pose a
    few challenges to users who lack Java expertise. Spark was launched in 2010 as
    an in-memory alternative to the MapReduce processing engine for Apache Hadoop
    and is heavily reliant on the Java Virtual Machine (JVM) for its core functionality.
    Support for Python came along a few release cycles later, with an API called PySpark,
    but this API simply enables you to interact with a Spark cluster using Python.
    Any Python code that gets submitted to Spark must pass through the JVM using the
    Py4J library. This can make it quite difficult to fine-tune and debug PySpark
    code because some execution occurs outside of the Python context.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家首次发现Dask时心中常见的疑问是它与Apache Spark等其他表面上类似的技术如何比较。Spark确实已经成为分析大数据集的非常流行的框架，并在这一领域表现出色。然而，尽管Spark支持包括Python在内的几种不同的语言，但其作为Java库的遗产可能会给缺乏Java专业知识的使用者带来一些挑战。Spark于2010年作为Apache
    Hadoop的MapReduce处理引擎的内存替代品而推出，其核心功能高度依赖于Java虚拟机（JVM）。Python的支持在几个发布周期后出现，名为PySpark的API，但这个API仅仅允许您使用Python与Spark集群交互。任何提交给Spark的Python代码都必须通过Py4J库在JVM中传递。这可能会使PySpark代码的微调和调试变得相当困难，因为一些执行发生在Python上下文之外。
- en: PySpark users may eventually determine that they need to migrate their codebase
    to Scala or Java anyway to get the most out of Spark. New features and enhancements
    to Spark are added to the Java and Scala APIs first, and it typically takes a
    few release cycles for that functionality to be exposed to PySpark. Furthermore,
    PySpark’s learning curve isn’t trivial. Its DataFrame API, while conceptually
    similar to Pandas, has substantial differences in syntax and structure. This means
    that new PySpark users must relearn how to do things “the Spark way” rather than
    draw from existing experience and knowledge of working with Pandas and scikit-learn.
    Spark is highly optimized to apply computations over collection objects, such
    as adding a constant to each item in an array or calculating the sum of an array.
    But this optimization comes at the price of flexibility. Spark is not equipped
    to handle code that can’t be expressed as a map or reduce type operation over
    a collection. Therefore, you can’t use Spark to scale out custom algorithms with
    the same elegance that you can with Dask. Spark is also notorious for its difficulty
    to set up and configure, requiring many dependencies such as Apache ZooKeeper
    and Apache Ambari, which can also be difficult to install and configure in their
    own right. It’s not unusual for organizations that use Spark and Hadoop to have
    dedicated IT resources whose sole responsibility is to configure, monitor, and
    maintain the cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark用户可能会最终确定他们无论如何都需要将代码库迁移到Scala或Java，以充分利用Spark。Spark的新功能和增强首先添加到Java和Scala
    API中，通常需要几个发布周期才能将此功能暴露给PySpark。此外，PySpark的学习曲线并不简单。其DataFrame API虽然在概念上与Pandas相似，但在语法和结构上有很大的差异。这意味着新的PySpark用户必须重新学习如何“Spark方式”做事，而不是从现有的使用Pandas和scikit-learn的经验和知识中汲取。Spark高度优化以应用于集合对象上的计算，例如向数组中的每个元素添加一个常数或计算数组的总和。但这种优化是以灵活性为代价的。Spark无法处理无法表示为集合上的映射或归约类型操作的代码。因此，您不能像使用Dask那样优雅地扩展自定义算法。Spark还以其设置和配置的困难而闻名，需要许多依赖项，如Apache
    ZooKeeper和Apache Ambari，这些依赖项本身也可能难以安装和配置。对于使用Spark和Hadoop的组织来说，拥有专门的IT资源并不罕见，这些资源的唯一责任是配置、监控和维护集群。
- en: This comparison is not intended to be unfair to Spark. Spark is very good at
    what it does and is certainly a viable solution for analyzing and processing large
    datasets. However, Dask’s short learning curve, flexibility, and familiar APIs
    make Dask a more attractive solution for data scientists with a background in
    the Python Open Data Science Stack.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种比较并非有意对Spark不公平。Spark在其擅长的领域表现得非常出色，并且当然是一个分析处理大数据集的有效解决方案。然而，Dask的学习曲线短、灵活性高以及熟悉的API使其成为有Python
    Open Data Science Stack背景的数据科学家更吸引人的解决方案。
- en: I hope that by now you’re starting to see why Dask is such a powerful and versatile
    toolset. And, if my earlier suspicions were correct—that you decided to pick up
    this book because you’re currently struggling with a large dataset—I hope you
    feel both encouraged to give Dask a try and excited to learn more about using
    Dask to analyze a real-world dataset. Before we look at some Dask code, however,
    it’ll be good to review a few core concepts that will help you understand how
    Dask’s task schedulers “divide and conquer” computations. This will be especially
    helpful if you’re new to the idea of distributed computing because understanding
    the mechanics of task scheduling will give you a good idea of what happens when
    a computation is executed and where potential bottlenecks may lie.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望到现在你已经开始看到为什么Dask是一个如此强大且多功能的工具集。而且，如果我的早期怀疑是正确的——你决定拿起这本书是因为你目前正在处理大量数据集——我希望你感到既鼓励尝试Dask，又兴奋地想了解更多关于如何使用Dask分析真实世界数据集的信息。然而，在我们查看一些Dask代码之前，回顾几个核心概念将有助于你理解Dask的任务调度器如何“分而治之”计算。这对于你来说尤其有帮助，因为你对分布式计算的新手来说，理解任务调度的机制将给你一个很好的想法，当计算执行时会发生什么，以及潜在瓶颈可能在哪里。
- en: 1.2 Cooking with DAGs
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 使用DAG烹饪
- en: Dask’s task schedulers use the concept of directed acyclic graphs (or DAGs for
    short) to compose, control, and express computations. DAGs come from a larger
    body of mathematics known as *graph theory*. Unlike what you may expect from the
    name, graph theory doesn’t have anything to do with pie charts or bar graphs.
    Instead, graph theory describes a graph as a representation of a set of objects
    that have a relationship with one another. While this definition is quite vague
    and abstract, it means graphs are useful for representing a very wide variety
    of information. Directed acyclic graphs have some special properties that give
    them a slightly narrower definition. But rather than continuing to talk about
    graphs in the abstract, let’s have a look at an example of using a DAG to model
    a real process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的任务调度器使用有向无环图（或简称DAG）的概念来组合、控制和表达计算。DAG来自一个更大的数学领域，称为*图论*。与名字可能让你预期的不同，图论与饼图或条形图无关。相反，图论将图描述为具有相互关系的一组对象的表示。虽然这个定义相当模糊和抽象，但它意味着图可以用来表示非常广泛的信息。有向无环图有一些特殊的属性，使它们的定义稍微狭窄一些。但与其继续在抽象上谈论图，不如让我们看看使用DAG来模拟一个真实过程的例子。
- en: When I’m not busy writing, teaching, or analyzing data, I love cooking. To me,
    few things in this world can compare to a piping hot plate of pasta. And right
    up at the top of my all-time-favorite pasta dishes is bucatini all’Amatriciana.
    If you enjoy Italian cuisine, you’ll love the bite of thick bucatini noodles,
    the sharp saltiness of Pecorino Romano cheese, and the peppery richness of the
    tomato sauce cooked with guanciale and onion. But I digress! My intent here is
    not for you to drop the book and run to your kitchen. Rather, I want to explain
    how making a delicious plate of bucatini all’Amatriciana can be modeled using
    a directed acyclic graph. First, let’s take a quick overview of the recipe, which
    can be seen in [figure 1.2](#figure1.2).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我不忙于写作、教学或分析数据时，我喜欢烹饪。对我来说，在这个世界上很少有东西能与一碗热腾腾的意大利面相比。在我的所有时间最喜欢的意大利面菜肴中，bucatini
    all’Amatriciana位居榜首。如果你喜欢意大利菜，你会喜欢厚实的bucatini面条的口感，Pecorino Romano奶酪的鲜明咸味，以及用猪脸肉和洋葱烹制的番茄酱的辛辣丰富。但我在这里并不是要让你放下这本书就跑到厨房去。相反，我想解释如何使用有向无环图来模拟制作美味的bucatini
    all’Amatriciana。首先，让我们快速概述一下食谱，如图[1.2](#figure1.2)所示。
- en: '![c01_02.eps](Images/c01_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![c01_02.eps](Images/c01_02.png)'
- en: '[Figure 1.2](#figureanchor1.2) My favorite recipe for bucatini all’Amatriciana'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.2](#figureanchor1.2) 我最喜欢的bucatini all’Amatriciana食谱'
- en: Cooking a recipe consists of following a series of sequential steps where raw
    ingredients are transformed into intermediate states until all the ingredients
    are ultimately combined into a single complete dish. For example, when you dice
    an onion, you start with a whole onion and cut it into pieces, and then you’re
    left with some amount of diced onion. In software engineering parlance, we would
    describe the process of dicing onions as a *function*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 烹饪一道菜谱包括遵循一系列连续的步骤，其中生食材料被转化为中间状态，直到所有食材最终结合成一道完整的菜肴。例如，当你切洋葱时，你从一个完整的洋葱开始，将其切成片，然后你剩下一些切好的洋葱。在软件工程术语中，我们会将切洋葱的过程描述为一个*函数*。
- en: Dicing onions, while important, is only a very small part of the whole recipe.
    To complete the entire recipe, we must define many more steps (or functions).
    Each of these functions is called a *node* in a graph. Since most steps in a recipe
    follow a logical order (for example, you wouldn’t plate the noodles before cooking
    them), each node can take on dependencies, which means that a prior step (or steps)
    must be complete before starting the next node’s operation. Another step of the
    recipe is to sauté the diced onions in olive oil, which is represented by another
    node. Of course, it’s not possible to sauté diced onions if you haven’t diced
    any onions yet! Because sautéing the diced onion is directly dependent on and
    related to dicing the onion, these two nodes are connected by a *line*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 切洋葱虽然很重要，但只是整个食谱中很小的一部分。要完成整个食谱，我们必须定义更多步骤（或函数）。在图中，每个这些函数被称为一个*节点*。由于大多数食谱步骤遵循逻辑顺序（例如，你不会在煮面之前上菜），每个节点可以承担依赖关系，这意味着在开始下一个节点的操作之前，必须完成先前的步骤（或步骤）。食谱的另一个步骤是将切好的洋葱在橄榄油中炒熟，这由另一个节点表示。当然，如果你还没有切洋葱，是不可能炒洋葱的！因为炒洋葱直接依赖于并关联到切洋葱，这两个节点通过一条*线*连接。
- en: '![c01_03.eps](Images/c01_03.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![c01_03.eps](Images/c01_03.png)'
- en: '[Figure 1.3](#figureanchor1.3) A graph displaying nodes with dependencies'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.3](#figureanchor1.3) 显示具有依赖关系的节点图'
- en: '[Figure 1.3](#figure1.3) represents a graph of the process described so far.
    Notice that the Sauté Ingredients node has three direct dependencies: the onion
    and garlic must be diced and the guanciale must be sautéed before the three ingredients
    can be sautéed together. Conversely, the Dice Onion, Mince Garlic, and Heat Olive
    Oil nodes do not have any dependencies. The order in which you complete those
    steps does not matter, but you must complete all of them before proceeding to
    the final sauté step. Also notice that the lines connecting the nodes have arrows
    as endpoints. This implies that there is only one possible way to traverse the
    graph. It makes sense neither to sauté the onion before it’s diced, nor to attempt
    to sauté the onion without a hot, oiled pan ready. This is what’s meant by a *directed*
    acyclic graph: there’s a logical, one-way traversal through the graph from nodes
    with no dependencies to a single terminal node.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.3](#figure1.3) 表示了到目前为止描述的过程的图。注意，炒食材节点有三个直接依赖项：洋葱和大蒜必须切好，猪脸肉必须炒熟，这三样食材才能一起炒。相反，切洋葱、切碎大蒜和加热橄榄油节点没有任何依赖项。完成这些步骤的顺序并不重要，但你必须完成所有这些步骤才能进行最后的炒制步骤。此外，注意连接节点的线条的端点有箭头。这意味着只有一种可能的遍历图的方式。在洋葱切好之前炒洋葱，或者在没有热油锅的情况下尝试炒洋葱是没有意义的。这就是所谓的*有向无环图*：从没有依赖关系的节点到单个终端节点存在一种逻辑的、单向的遍历。'
- en: Another thing you may notice about the graph in [figure 1.3](#figure1.3) is
    that no lines connect later nodes back to earlier nodes. Once a node is complete,
    it is never repeated or revisited. This is what makes the graph an *acyclic* graph.
    If the graph contained a feedback loop or some kind of continuous process, it
    would instead be a *cyclic* graph. This, of course, would not be an appropriate
    representation of cooking, since recipes have a finite number of steps, have a
    finite state (finished or unfinished), and deterministically resolve to a completed
    state, barring any kitchen catastrophes. [Figure 1.4](#figure1.4) demonstrates
    what a cyclic graph might look like.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[图1.3](#figure1.3)中的图形，你可能还会注意到没有线条将后续节点连接回早期节点。一旦一个节点完成，它就永远不会重复或再次访问。这就是使图形成为*无环图*的原因。如果图形包含反馈循环或某种连续过程，它将是一个*循环图*。当然，这不适合表示烹饪，因为食谱有有限步骤，有有限状态（完成或未完成），并且可以确定地解决到完成状态，除非发生厨房灾难。[图1.4](#figure1.4)展示了循环图可能的样子。
- en: '![c01_04.eps](Images/c01_04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![c01_04.eps](Images/c01_04.png)'
- en: '[Figure 1.4](#figureanchor1.4) An example of a cyclic graph demonstrating an
    infinite feedback loop'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.4](#figureanchor1.4) 展示了一个具有无限反馈循环的循环图示例'
- en: 'From a programming perspective, this might sound like directed acyclic graphs
    would not allow looping operations. But this is not necessarily the case: a directed
    acyclic graph can be constructed from deterministic loops (such as `for` loops)
    by copying the nodes to be repeated and connecting them sequentially. In [figure
    1.3](#figure1.3), the guanciale was sautéed in two different steps—first alone,
    then together with the onions. If the ingredients needed to be sautéed a non-deterministic
    number of times, the process could not be expressed as an acyclic graph.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程的角度来看，这听起来可能像是有向无环图不允许循环操作。但这并不一定是这样：可以通过复制要重复的节点并按顺序连接它们来从确定性循环（如`for`循环）构建有向无环图。在[图1.3](#figure1.3)中，guanciale是在两个不同的步骤中煎炒的——首先单独煎炒，然后与洋葱一起煎炒。如果需要煎炒的食材数量是不确定的，那么这个过程就不能表示为无环图。
- en: The final thing to note about the graph in [figure 1.3](#figure1.3) is that
    it’s in a special form known as a *transitive reduction*. This means that any
    lines that express *transitive dependencies* are eliminated. A transitive dependency
    simply means a dependency that is met indirectly through completion of another
    node. [Figure 1.5](#figure1.5) shows [figure 1.3](#figure1.3) redrawn without
    transitive reduction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[图1.3](#figure1.3)中的图形，需要注意的最后一件事是它采用了一种称为*传递闭包*的特殊形式。这意味着任何表达*传递依赖*的线条都被消除了。传递依赖简单来说就是通过完成另一个节点间接满足的依赖。[图1.5](#figure1.5)展示了没有传递闭包的[图1.3](#figure1.3)的重绘。
- en: '![c01_05.eps](Images/c01_05.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![c01_05.eps](Images/c01_05.png)'
- en: '[Figure 1.5](#figureanchor1.5) The graph represented in [figure 1.3](#figure1.3)
    redrawn without transitive reduction'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.5](#figureanchor1.5)展示了没有传递闭包的[图1.3](#figure1.3)中的图形。'
- en: Notice that a line is drawn between the nodes containing the operation Heat
    Olive Oil and Sauté Ingredients (8 minutes). Heating the olive oil is a transitive
    dependency of sautéing the onion, garlic, and guanciale because the guanciale
    must be sautéed alone before adding the onion and garlic. In order to sauté the
    guanciale, you must heat up a pan with olive oil first, so by the time you’re
    ready to sauté all three ingredients together, you already have a hot pan with
    oil—the dependency is already met!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在包含操作“加热橄榄油”和“煎炒食材（8分钟）”的节点之间画了一条线。加热橄榄油是煎炒洋葱、大蒜和guanciale的传递依赖，因为guanciale必须在加入洋葱和大蒜之前单独煎炒。为了煎炒guanciale，你必须先加热装有橄榄油的平底锅，所以当你准备好煎炒所有三种食材时，你已经有了一个热的平底锅和油——依赖关系已经满足！
- en: '[Figure 1.6](#figure1.6) represents the full directed acyclic graph for the
    complete recipe. As you can see, the graph fully represents the process from start
    to finish. You can start at any of the red nodes (medium gray in the print version
    of this book) since they do not have dependencies, and you will eventually reach
    the terminal node labeled “Buon appetito!” While looking at this graph, it might
    be easy to spot some bottlenecks, and potentially reorder some nodes to produce
    a more optimal or time-efficient way of preparing the dish. For instance, if the
    pasta water takes 20 minutes to come to a rolling boil, perhaps you could draw
    a graph with a single starting node of putting the water on to boil. Then you
    wouldn’t have to wait for the water to heat up after already preparing the rest
    of the dish. These are great examples of optimizations that either an intelligent
    task scheduler or you, the designer of the workload, may come up with. And now
    that you have the foundational understanding of how directed acyclic graphs work,
    you should be able to read and understand any arbitrary graph—from cooking pasta
    to calculating descriptive statistics on a big data set. Next, we’ll look at why
    DAGs are so useful for scalable computing.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.6](#figure1.6)表示了完整菜谱的完整有向无环图。如图所示，该图完全代表了从开始到结束的过程。你可以从任何红色节点（本书印刷版的浅灰色）开始，因为它们没有依赖关系，最终你会到达标记为“Buon
    appetito！”的终端节点。在查看这个图时，你可能很容易发现一些瓶颈，并可能重新排列一些节点以产生更优或更节省时间的准备菜肴的方法。例如，如果意大利面的水需要20分钟才能达到翻滚的沸腾，也许你可以画一个只有一个起始节点——把水烧开——的图。这样，你就不必在准备完其他菜肴之后再等待水加热了。这些都是优化示例，无论是智能任务调度器还是你，作为工作负载的设计者，都可能提出这些优化。现在，你已经有了有向无环图如何工作的基础理解，你应该能够阅读和理解任何任意的图形——从煮意大利面到在大数据集上计算描述性统计。接下来，我们将探讨为什么DAGs对于可扩展计算如此有用。'
- en: '![c01_06.eps](Images/c01_06.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![c01_06.eps](Images/c01_06.png)'
- en: '[Figure 1.6](#figureanchor1.6) The full directed acyclic graph representation
    of the bucatini all’Amatriciana recipe'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.6](#figureanchor1.6) bucatini all’Amatriciana食谱的完整有向无环图表示'
- en: 1.3 Scaling out, concurrency, and recovery
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 向外扩展、并发性和恢复
- en: Up to this point, our example of cooking bucatini all’Amatriciana assumed that
    you were the sole cook in the kitchen. This might be fine if you’re only cooking
    dinner for your family or a small get-together with friends, but if you needed
    to cook hundreds of servings for a busy dinner service in midtown Manhattan, you
    would likely reach the limits of your abilities very quickly. It’s now time to
    search for some help!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们烹饪bucatini all’Amatriciana的例子假设你是厨房中唯一的厨师。如果你只是为家人做饭或者与朋友的小聚，这可能没问题，但如果你需要在曼哈顿中城繁忙的晚餐服务中烹饪数百份菜肴，你可能会很快达到自己能力的极限。现在，是时候寻找一些帮助了！
- en: 'First, you must decide how you will handle the resource problem: should you
    upgrade your equipment to help you be more efficient in the kitchen, or should
    you hire more cooks to help share the workload? In computing, these two approaches
    are called *scaling up* and *scaling out*, respectively. Just like in our hypothetical
    kitchen, neither approach is as simple as it may sound. In section 1.3.1, I’ll
    discuss the limitations of scale-up solutions and how scale-out solutions overcome
    those limitations. Since a key use case of Dask is scaling out complex problems,
    we’ll assume that the best course of action for our hypothetical kitchen is to
    hire more workers and scale out. Given that assumption, it’ll be important to
    understand some of the challenges that come with orchestrating complex tasks across
    many different workers. I’ll discuss how workers share resources in section 1.3.2,
    and how worker failures are handled in section 1.3.3.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你必须决定你将如何处理资源问题：你应该升级你的设备以帮助你在厨房中更有效率，还是应该雇佣更多的厨师来帮助分担工作量？在计算中，这两种方法分别被称为*向上扩展*和*向外扩展*。就像在我们假设的厨房中一样，这两种方法都不像听起来那么简单。在第1.3.1节中，我将讨论向上扩展解决方案的限制以及向外扩展解决方案如何克服这些限制。由于Dask的关键用例是向外扩展复杂问题，我们将假设我们假设的厨房的最佳行动方案是雇佣更多工人并扩展。基于这个假设，理解与在许多不同工人之间协调复杂任务相关的挑战将非常重要。我将在第1.3.2节中讨论工人如何共享资源，在第1.3.3节中讨论如何处理工人故障。
- en: 1.3.1 Scaling up vs. scaling out
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 向上扩展与向外扩展
- en: '![c01_07.eps](Images/c01_07.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![c01_07.eps](Images/c01_07.png)'
- en: '[Figure 1.7](#figureanchor1.7) Scaling up replaces existing equipment with
    larger/faster/more efficient equipment, while scaling out divides the work between
    many workers in parallel.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.7](#figureanchor1.7) 扩大规模是用更大/更快/更高效的设备替换现有设备，而扩展则是将工作分配给许多工人并行进行。'
- en: Back in our hypothetical kitchen, you’re faced with the question of what to
    do now that you’re expected to feed a horde of hungry customers at dinner rush.
    The first thing you might notice is that as the volume of pasta you need to make
    increases, the amount of time that each step takes also increases. For example,
    the original recipe makes four servings and calls for ¾ cup of diced onions. This
    amount roughly equates to a single medium-sized yellow onion. If you were to make
    400 servings of the dish, you would need to dice 100 onions. Assuming you can
    dice an onion in around two minutes, and it takes you 30 seconds to clear the
    cutting board and grab another onion, you would be chopping onions for roughly
    five hours! Forget the time it would take to prepare the rest of the recipe. By
    the time you merely finish dicing the onions, your angry customers would already
    have taken their business elsewhere. And to add insult to injury, you’d have cried
    your eyes dry from spending the last five hours cutting onions! The two potential
    solutions to this problem are to replace your existing kitchen equipment with
    faster, more efficient equipment (scale up) or to hire more workers to work in
    parallel (scale out). [Figure 1.7](#figure1.7) shows what these two methods would
    look like.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们假设的厨房里，你面临着一个问题：在晚餐高峰时段，你被期望为一大群饥饿的顾客提供食物。你首先可能注意到的是，随着你需要制作的意大利面的数量增加，每个步骤所需的时间也会增加。例如，原始食谱制作四份，需要¾杯切碎的洋葱。这个数量大约相当于一个中等大小的黄洋葱。如果你要制作400份这道菜，你需要切碎100个洋葱。假设你可以在大约两分钟内切碎一个洋葱，清理切菜板并抓取另一个洋葱需要你30秒，那么你将花费大约五小时来切洋葱！别提准备其他食谱所需的时间了。在你仅仅完成切洋葱的工作时，愤怒的顾客可能已经将他们的生意带到了别处。更糟糕的是，你可能会因为花了最后五小时切洋葱而哭干了眼泪！解决这个问题的两种潜在方法是：用更快、更高效的设备替换现有的厨房设备（扩大规模）或者雇佣更多的工人并行工作（扩展规模）。[图1.7](#figure1.7)展示了这两种方法的样子。
- en: The decision to scale up or scale out isn’t an easy one because there are advantages
    and trade-offs to both. You might want to consider scaling up, because you would
    still ultimately oversee the whole process from start to finish. You wouldn’t
    have to deal with others’ potential unreliability or variation in skills, and
    you wouldn’t have to worry about bumping into other people in the kitchen. Perhaps
    you can trade in your trusty knife and cutting board for a food processor that
    can chop onions in one-tenth the time that it takes you to do it manually. This
    will suit your needs until you start scaling again. As your business expands and
    you start serving 800, 1,600, and 3,200 plates of pasta per day, you will start
    running into the same capacity problems you had before, and you’ll eventually
    outgrow your food processor. There will come a time you will need to buy a new,
    faster machine. Taking this example to an extreme, you’ll eventually hit the limit
    of current kitchen technology and have to go to great lengths and expense to develop
    and build better and better food processors. Eventually, your simple food processor
    will become highly specialized for chopping an extraordinarily large amount of
    onions and require incredible feats of engineering to build and maintain. Even
    then, you will reach a point when further innovation is simply not tenable (at
    some point, the blades will have to rotate so quickly that the onion will just
    turn to mush!). But hold on a second, let’s not get carried away. For most chefs,
    opening a small checkered-tablecloth joint in the city doesn’t entail formulating
    a plan to become a worldwide pasta magnate and a food processor R&D powerhouse—meaning
    simply choosing to get the food processor (scaling up) is likely the best option.
    Likewise, most of the time, upgrading a cheap, low-end workstation to a high-end
    server will be easier and cheaper than buying a bunch of hardware and setting
    up a cluster. This is especially true if the size of the problem you’re facing
    sits at the high end of medium datasets or the low end of large datasets. This
    also becomes an easier choice to make if you’re working in the cloud, because
    it’s much easier to scale up a process from one instance type to another instead
    of paying to acquire hardware that might not end up meeting your needs. That said,
    scaling out can be the better option if you can take advantage of a lot of parallelism
    or if you’re working with large datasets. Let’s look at what scaling out will
    yield in the kitchen.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 决定是扩展规模还是横向扩展并不容易，因为两者都有其优势和权衡。你可能想要考虑扩展规模，因为你仍然需要从开始到结束全面监督整个过程。你不必处理他人可能的不稳定或技能上的差异，也不必担心在厨房里遇到其他人。也许你可以用你那把可靠的刀子和切菜板交换一台食品加工机，它可以在你手动切洋葱所需时间的十分之一内完成切洋葱的工作。这会满足你的需求，直到你再次开始扩展。随着你的业务扩大，你开始每天供应800、1,600和3,200份意大利面，你将开始遇到之前遇到的相同产能问题，最终你会超出食品加工机的处理能力。将会有一个时刻你需要购买一台新的、更快的机器。将这个例子推向极端，你最终会达到当前厨房技术的极限，不得不付出巨大的努力和费用来开发和制造更好、更好的食品加工机。最终，你那台简单的食品加工机将变得高度专业化，用于切割大量的洋葱，并且需要惊人的工程成就来建造和维护。即使如此，你也会达到一个点，进一步的创新将不再可行（在某个时刻，刀片将不得不旋转得如此之快，以至于洋葱将变成泥状！）。但是，等等，我们不要太过分了。对于大多数厨师来说，在市中心开设一家小格子桌布餐馆并不需要制定成为全球意大利面巨头和食品加工机研发强国的计划——这意味着简单地选择购买食品加工机（扩展规模）可能是最好的选择。同样，在大多数情况下，将一台廉价的低端工作站升级为高端服务器，比购买大量硬件并设置集群更容易、更便宜。这尤其适用于你面临的问题处于中等数据集的高端或大型数据集的低端。如果你在云端工作，这也会成为一个更容易做出的选择，因为从一种实例类型扩展到另一种实例类型要比支付购买可能最终不能满足你需求的硬件更容易。话虽如此，如果你可以利用大量的并行性，或者你正在处理大型数据集，横向扩展可能是一个更好的选择。让我们看看横向扩展在厨房中会带来什么结果。
- en: Rather than attempt to improve on your own skills and abilities, you hire nine
    additional cooks to help share the workload. If all 10 of you focused 100% of
    your time and attention to the process of chopping onions, that five hours of
    work now comes down to a mere 30 minutes, assuming you have equal skill levels.
    Of course, you would need to buy additional knives, cutting boards, and other
    tools, and you would need to provide adequate facilities and pay for your additional
    cooks, but in the long run this will be a more cost-effective solution if your
    other alternative is pouring money into development of specialized equipment.
    Not only can the additional cooks help you with reducing the time it takes to
    prepare the onions, but because they are non-specialized workers, they can also
    be trained to do all the other necessary tasks. A food processor, on the other
    hand, cannot be trained to boil pasta no matter how hard you may try! The trade-offs
    are that your other cooks can get sick, might need to miss work, or otherwise
    do things that are unexpected and hinder the process. Getting your team of cooks
    to work together toward a single goal does not come for free. At first you might
    be able to supervise if only three or four other cooks are in the kitchen, but
    eventually you might need to hire a sous chef as the kitchen grows out. Likewise,
    real costs are associated with maintaining a cluster, and these should be honestly
    evaluated when considering whether to scale up or scale out.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是试图提升你自己的技能和能力，你雇佣了九位额外的厨师来帮助分担工作量。如果你们十个人都将100%的时间和注意力集中在切洋葱这个过程中，那么原本的五小时工作现在只需30分钟，前提是你们拥有相同的技能水平。当然，你需要购买额外的刀具、切割板和其他工具，还需要提供足够的设施并支付额外厨师的费用，但长远来看，如果其他选择是不断投入资金开发专用设备，这将是一个更经济有效的解决方案。不仅额外的厨师可以帮助你减少准备洋葱所需的时间，而且由于他们是非专业工人，他们还可以被训练来完成所有其他必要的任务。另一方面，无论你多么努力，食品加工机也无法被训练来煮意大利面！权衡利弊是，其他厨师可能会生病，可能需要请假，或者做些意料之外的事情，从而阻碍进程。让你的厨师团队共同为一个单一目标工作并非不花钱。起初，如果你厨房里只有三四个其他厨师，你可能还能监督他们，但随着厨房规模的扩大，你可能最终需要雇佣一位副厨师长。同样，维护一个团队群组也有实际成本，在考虑是否扩大规模或扩展规模时，这些成本应该被诚实地评估。
- en: Pressing on with your new team of cooks, you now must figure out how to relay
    instructions to each cook and make sure the recipe comes out as intended. Directed
    acyclic graphs are a great tool for planning and orchestrating complex tasks across
    a pool of workers. Most importantly, dependencies between nodes help ensure that
    the work will follow a certain order (remember that a node cannot begin work until
    all its dependencies have completed), but there are no restrictions on how individual
    nodes are completed—whether by a single entity or many entities working in parallel.
    A node is a standalone unit of work, so it’s possible to subdivide the work and
    share it among many workers. This means that you could assign four cooks to chop
    the onions, while four other cooks sauté the guanciale, and the remaining two
    cooks mince the garlic. Dividing and supervising the work in the kitchen is the
    job of the sous chef, which represents Dask’s task scheduler. As each cook completes
    their task, the sous chef can assign them the next available task. To keep food
    moving through the kitchen in an efficient manner, the sous chef should constantly
    evaluate what work needs to be done and aim to start tasks closest to the terminal
    node as soon as possible. For example, rather than waiting for all 100 onions
    to be chopped, if enough onions, garlic, and guanciale have been prepared to begin
    making a complete batch of sauce, the sous chef should tell the next available
    cook to begin preparing a batch of sauce. This strategy allows some customers
    to be served sooner, rather than keeping all customers waiting until everyone
    can be served at the same time. It’s also more efficient to avoid having all the
    onions in a chopped state at once, because it can take up a large amount of cutting
    board space. Likewise, Dask’s task scheduler aims to cycle workers between many
    tasks in order to reduce memory load and emit finished results quickly. It distributes
    units of work to machines in an efficient manner and aims to minimize the worker
    pool’s idle time. Organizing execution of the graph between workers and assigning
    an appropriate number of workers to each task is crucial for minimizing the time
    it takes to complete the graph. [Figure 1.8](#figure1.8) depicts a possible way
    the original graph can be distributed to multiple workers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的新厨师团队中继续前进，你现在必须想出如何传达指令给每位厨师，并确保食谱按照预期制作。有向无环图是规划和管理跨工作者池的复杂任务的优秀工具。最重要的是，节点之间的依赖关系有助于确保工作将遵循一定的顺序（记住，一个节点在所有依赖项完成之前不能开始工作），但没有任何限制来规定单个节点如何完成——无论是单个实体完成还是多个实体并行工作。节点是一个独立的工作单元，因此可以将工作细分并在许多工作者之间共享。这意味着你可以指派四位厨师切洋葱，同时四位其他厨师煎香肠，剩下的两位厨师切碎蒜。在厨房中划分和监督工作是副厨的职责，它代表了
    Dask 的任务调度器。随着每位厨师完成他们的任务，副厨可以分配给他们下一个可用的任务。为了以高效的方式让食物在厨房中流动，副厨应不断评估需要完成的工作，并尽可能快地开始接近终端节点的任务。例如，而不是等待所有
    100 个洋葱都被切好，如果已经准备了足够的洋葱、大蒜和香肠来开始制作一整批酱汁，副厨应该告诉下一个可用的厨师开始准备一批酱汁。这种策略允许一些顾客更快地被服务，而不是让所有顾客都等待直到所有人都能同时被服务。同样，避免一次性将所有洋葱都切成切碎状态也更有效率，因为这可能会占用大量的切菜板空间。同样，Dask
    的任务调度器旨在在许多任务之间循环工作者，以减少内存负载并快速输出完成的结果。它以高效的方式将工作单元分配到机器上，并旨在最小化工作者池的空闲时间。在工作者之间组织图的执行并为每个任务分配适当数量的工作者对于最小化完成图所需的时间至关重要。[图
    1.8](#figureanchor1.8) 描述了原始图可以分配给多个工作者的可能方式。
- en: '![c01_08.eps](Images/c01_08.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![c01_08.eps](Images/c01_08.png)'
- en: '[Figure 1.8](#figureanchor1.8) A graph with nodes distributed to many workers
    depicting dynamic redistribution of work as tasks complete at different times'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1.8](#figureanchor1.8) 一个节点分布到多个工作者的图，展示了任务在不同时间完成时的动态工作重新分配'
- en: 1.3.2 Concurrency and resource management
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 并发与资源管理
- en: More often than not, you have to consider more constraints than just the number
    of available workers. In scalable computing, these are called issues of *concurrency*.
    For example, if you hire more cooks to dice onions, but you only have five knives
    in the kitchen, only five operations that require a knife can be carried out simultaneously.
    Some other tasks may require sharing resources, such as the step that calls for
    minced garlic. Therefore, if all five knives are in use by cooks dicing onions,
    the garlic can’t be minced until at least one knife becomes available. Even if
    the remaining five cooks have completed all other possible nodes, the garlic-mincing
    step becomes delayed due to *resource starvation*. [Figure 1.9](#figure1.9) demonstrates
    an example of resource starvation in our hypothetical kitchen.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的时候，你不得不考虑的约束不仅仅是可用的工作者数量。在可扩展计算中，这些问题被称为*并发性*问题。例如，如果你雇佣更多的厨师来切洋葱，但厨房里只有五把刀，那么同时只能进行五次需要刀子的操作。一些其他任务可能需要共享资源，比如需要切碎大蒜的步骤。因此，如果所有五把刀都被切洋葱的厨师使用，那么至少有一把刀可用之前，大蒜是无法切碎的。即使剩下的五名厨师完成了所有其他可能的节点，切碎大蒜的步骤也会因为*资源饥饿*而延迟。[图1.9](#figure1.9)展示了我们假设的厨房中资源饥饿的一个例子。
- en: '![c01_09.eps](Images/c01_09.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![c01_09.eps](Images/c01_09.png)'
- en: '[Figure 1.9](#figureanchor1.9) An example of resource starvation'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.9](#figureanchor1.9) 资源饥饿的例子'
- en: The other cooks are forced to remain idle until the onion-dicing step is complete.
    When a shared resource is in use, a *resource lock* is placed on it, meaning other
    workers can’t “steal” the resource until the worker who locked the resource is
    finished using it. It would be quite rude (and dangerous) for one of your cooks
    to wrestle the knife out of the hands of another cook. If your cooks are constantly
    fighting over who gets to use the knife next, those disagreements consume time
    that could be spent working on completing the recipe. The sous chef is responsible
    for defusing these confrontations by laying the ground rules about who can use
    certain resources and what happens when a resource becomes available. Similarly,
    the task scheduler in a scalable computing framework must decide how to deal with
    resource contention and locking. If not handled properly, resource contention
    can be very detrimental to performance. But fortunately, most frameworks (including
    Dask) are pretty good at efficient task scheduling and don’t normally need to
    be hand-tuned.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其他厨师被迫保持空闲，直到洋葱切丁步骤完成。当一个共享资源正在使用时，会对其放置一个*资源锁*，这意味着其他工作者不能在锁定资源的工作者完成使用之前“偷取”该资源。如果你们的厨师之间不断争夺谁下一个使用刀子，这些争执会消耗本可以用来完成食谱的时间。副厨师负责通过制定关于谁可以使用某些资源以及资源可用时会发生什么的规则来平息这些对抗。同样，在可扩展计算框架中的任务调度器必须决定如何处理资源竞争和锁定。如果处理不当，资源竞争可能会对性能造成很大的损害。但幸运的是，大多数框架（包括Dask）在高效任务调度方面做得相当不错，通常不需要手动调整。
- en: 1.3.3 Recovering from failures
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 从失败中恢复
- en: 'Finally, no discussion of scalable computing would be complete without mentioning
    recovery strategies. Just like it’s difficult for a sous chef to closely supervise
    all her cooks at once, it gets increasingly difficult to orchestrate distribution
    of processing tasks as the number of machines in a cluster increases. Since the
    final result consists of the aggregate of all the individual operations, it’s
    important to ensure that all the pieces find their way to where they need to go.
    But machines, like people, are imperfect and fail at times. Two types of failures
    must be accounted for: worker failure and data loss. For example, if you’ve assigned
    one of your cooks to dice the onions and going into the third hour straight of
    chopping he decided he can’t take the monotony anymore, he might put down his
    knife, take off his coat, and walk out the door. You’re now down a worker! One
    of your other cooks will need to take up his place in order to finish dicing the
    onions, but thankfully you can still use the onions that the previous cook diced
    before he left. This is worker failure without data loss. The work that the failed
    worker completed does not need to be reproduced, so the impact to performance
    is not as severe.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，任何关于可扩展计算的讨论如果没有提及恢复策略都是不完整的。就像副厨师难以同时密切监督所有厨师一样，随着集群中机器数量的增加，协调处理任务的分配变得越来越困难。由于最终结果是由所有单个操作的汇总组成，因此确保所有部分都能到达它们需要去的地方非常重要。但是，机器，就像人一样，是不完美的，有时会失败。必须考虑两种类型的故障：工作节点故障和数据丢失。例如，如果你让你的一个厨师切洋葱，经过连续三个小时的切菜，他决定无法再忍受这种单调，他可能会放下刀子，脱掉外套，走出门口。你现在失去了一个工人！你的另一位厨师需要接替他的位置来完成切洋葱的工作，但幸运的是，你仍然可以使用之前厨师离开前切的洋葱。这是没有数据丢失的工作节点故障。失败的工作节点完成的工作不需要重新生成，因此对性能的影响并不严重。
- en: When data loss occurs, a significant impact to performance is much more likely.
    For example, your kitchen staff has completed all the initial prep steps and the
    sauce is simmering away on the stove. Unfortunately, the pot is accidentally knocked
    over and spills all over the floor. Knowing that scraping the sauce off the floor
    and attempting to recover would violate all the health codes in the book, you’re
    forced to remake the sauce. This means going back to dicing more onions, sautéing
    more guanciale, and so on. The dependencies for the Simmer Sauce node are no longer
    met, meaning you have to step all the way back to the first dependency-free node
    and work your way back from there. Although this is a fairly catastrophic example,
    the important thing to remember is that at any point in the graph, the complete
    lineage of operations up to a given node can be “replayed” in the event of a failure.
    The task scheduler is ultimately responsible for stopping work and redistributing
    the work to be replayed. And because the task scheduler can dynamically redistribute
    tasks away from failed workers, the specific workers that completed the tasks
    before don’t need to be present to redo the tasks. For example, if the cook who
    decided to quit earlier had taken some diced onions with him, you would not need
    to stop the whole kitchen and redo everything from the beginning. You would just
    need to determine how many additional onions need to be diced and assign a new
    cook to do that work.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生数据丢失时，对性能的影响可能会更加显著。例如，你的厨房工作人员已经完成了所有初步准备工作，酱汁正在炉子上慢慢煮沸。不幸的是，锅子意外被撞倒，酱汁洒满了地板。知道刮掉地板上的酱汁并尝试恢复将违反所有健康法规，你被迫重新制作酱汁。这意味着要回到切更多的洋葱，炒更多的猪脸肉等等。Simmer
    Sauce节点的依赖关系不再满足，这意味着你必须退回到第一个无依赖关系的节点，并从那里开始工作。虽然这是一个相当灾难性的例子，但重要的是要记住，在任何时刻，在图中，给定节点之前的所有操作完整记录可以在发生故障时“重放”。任务调度器最终负责停止工作并将工作重新分配以进行重放。由于任务调度器可以动态地将任务重新分配到失败的工人之外，因此之前完成任务的特定工人不需要在场来重新执行任务。例如，如果决定提前辞职的厨师带了一些切好的洋葱，你就不需要停止整个厨房并从头开始重做所有事情。你只需要确定还需要切多少洋葱，并指派一个新的厨师来完成这项工作。
- en: In rare circumstances, the task scheduler might run into problems and fail.
    This is akin to your sous chef deciding to hang up her hat and walk out the door.
    This kind of failure can be recovered from, but since only the task scheduler
    knows the complete DAG and how much was finished, the only option is to start
    over at step 1 with a brand-new task graph. Admittedly, the kitchen analogy falls
    apart a bit here. In reality, your cooks would know the recipe well enough to
    finish service without micromanagement of the sous, but this isn’t the case with
    Dask. The workers simply do what they’re told, and if there’s no task scheduler
    around to tell them what to do, they can’t make decisions on their own.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在罕见情况下，任务调度器可能会遇到问题并失败。这就像你的副厨师决定挂上帽子，走出大门一样。这种失败是可以恢复的，但由于只有任务调度器知道完整的DAG以及完成了多少，唯一的选择是从步骤1重新开始，使用全新的任务图。诚然，这里的厨房类比有点破裂。在现实中，你的厨师对食谱足够了解，可以在副厨的微观管理下完成服务，但Dask的情况并非如此。工人只是按照指示行事，如果没有任务调度器告诉他们该做什么，他们就不能自己做出决定。
- en: Hopefully you now have a good understanding of the power of DAGs and how they
    relate to scalable computing frameworks. These concepts will certainly come up
    again through this book since all of Dask’s task scheduling is based on the DAG
    concepts presented here. Before we close out the chapter, we’ll take a brief look
    at the dataset that we’ll use throughout the book to learn about Dask’s operations
    and capabilities.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在已经很好地理解了DAG的力量以及它们如何与可扩展计算框架相关联。这些概念肯定会在本书中再次出现，因为Dask的所有任务调度都是基于这里提出的DAG概念。在我们结束本章之前，我们将简要地看一下我们将贯穿整本书使用的数据集，以了解Dask的操作和功能。
- en: 1.4 Introducing a companion dataset
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 介绍配套数据集
- en: Since the intent of this book is to give you hands-on experience using Dask
    for data science, it’s only natural to have a dataset you can work with alongside
    the examples in the forthcoming chapters. Rather than proceed through the book
    working on a number of purpose-built “toy” examples, it will be a lot more valuable
    to apply your newfound skills to a real, messy dataset. It’s also important for
    you to gain experience using an appropriately large dataset, because you will
    be better equipped to apply your knowledge to medium and large datasets in the
    wild. So, over the next several chapters, we’ll use a great public domain dataset
    provided by NYC OpenData (*[https://opendata.cityofnewyork.us](https://opendata.cityofnewyork.us)*)
    as a backdrop for learning how to use Dask.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书的目的是让你通过使用Dask进行数据科学获得实践经验，因此拥有一个可以与即将到来的章节中的示例一起工作的数据集是自然而然的。与其在书中通过一系列专门设计的“玩具”示例进行工作，不如将你新获得的技术应用到真实且混乱的数据集上更有价值。同时，对你来说，使用一个适当大的数据集获得经验也很重要，因为这将使你更好地将你的知识应用到现实世界中的中大型数据集上。因此，在接下来的几章中，我们将使用由纽约市开放数据（[https://opendata.cityofnewyork.us](https://opendata.cityofnewyork.us)）提供的公共领域数据集作为背景，学习如何使用Dask。
- en: Every third week of the month, the New York City Department of Finance records
    and publishes a data set of all parking citations issued throughout the fiscal
    year so far. The data collected by the city is quite rich, even including some
    interesting geographic features. To make the data more accessible, an archive
    containing four years of data from NYC OpenData has been collected and published
    on the popular machine learning website, Kaggle, by Aleksey Bilogur and Jacob
    Boysen under the City of New York account. The dataset spans from 2013 through
    June 2017 and is over 8 GB uncompressed. While this dataset may meet the definition
    of small data if you have a very powerful computer, for most readers it should
    be a well-sized medium dataset. Although larger datasets are certainly out there,
    I hope you appreciate not needing to download 2 TB of data before proceeding to
    the next chapter. You can get the data for yourself at *[www.kaggle.com/new-york-city/nyc-parking-tickets](http://www.kaggle.com/new-york-city/nyc-parking-tickets)*.
    After you’ve downloaded the data, roll up your sleeves and get ready to have a
    first look at Dask in the next chapter!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每月第三个星期，纽约市财政部门会记录并发布到目前为止整个财政年度发出的所有停车罚单数据集。城市收集的数据非常丰富，甚至包括一些有趣的地理特征。为了使数据更易于访问，由
    Aleksey Bilogur 和 Jacob Boysen 在纽约市账户下收集并发布在流行的机器学习网站上，Kaggle，包含四年纽约市 OpenData
    数据的存档。该数据集从 2013 年到 2017 年 6 月，未压缩超过 8 GB。虽然对于拥有非常强大计算机的人来说，这个数据集可能符合小数据的定义，但对于大多数读者来说，它应该是一个大小合适的中等数据集。尽管确实存在更大的数据集，但我希望你能欣赏到在进入下一章之前不需要下载
    2 TB 数据。你可以在 *[www.kaggle.com/new-york-city/nyc-parking-tickets](http://www.kaggle.com/new-york-city/nyc-parking-tickets)*
    上获取数据。下载完数据后，卷起袖子，准备好在下一章中首次了解 Dask！
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Dask can be used to scale popular data analysis libraries such as Pandas and
    NumPy, allowing you to analyze medium and large datasets with ease.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 可以用于扩展流行的数据分析库，如 Pandas 和 NumPy，让你能够轻松分析中等和大型数据集。
- en: Dask uses directed acyclic graphs (DAGs) to coordinate execution of parallelized
    code across CPU cores and machines.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 使用有向无环图（DAG）来协调跨 CPU 核心和机器的并行化代码的执行。
- en: Directed acyclic graphs are made up of nodes and have a clearly defined start
    and end, a single traversal path, and no looping.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有向无环图由节点组成，具有明确定义的开始和结束、单一路径和没有循环。
- en: Upstream nodes must be completed before work can begin on any dependent downstream
    nodes.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上游节点必须完成，然后才能开始任何依赖的下游节点的作业。
- en: Scaling out can generally improve performance of complex workloads, but it creates
    additional overhead that might substantially reduce those performance gains.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展通常可以提高复杂工作负载的性能，但它会创建额外的开销，这可能会大幅减少这些性能提升。
- en: In the event of a failure, the steps to reach a node can be repeated from the
    beginning without disturbing the rest of the process.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在发生故障的情况下，可以重复从开始到节点的步骤，而不会干扰整个过程的其余部分。
- en: '2'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Introducing Dask
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍 Dask
- en: '**This chapter covers**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Warming up with a short example of data cleaning using Dask DataFrames
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask DataFrames 的简短示例进行数据清洗
- en: Visualizing DAGs generated by Dask workloads with graphviz
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 graphviz 可视化 Dask 工作负载生成的 DAG
- en: Exploring how the Dask task scheduler applies the concept of DAGs to coordinate
    execution of code
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 Dask 任务调度器如何将 DAG 的概念应用于协调代码的执行
- en: 'Now that you have a basic understanding of how DAGs work, let’s take a look
    at how Dask uses DAGs to create robust, scalable workloads. To do this, we’ll
    use the NYC Parking Ticket data you downloaded at the end of the previous chapter.
    This will help us accomplish two things at once: you’ll get your first taste of
    using Dask’s DataFrame API to analyze a structured dataset, and you’ll start to
    get familiar with some of the quirks in the dataset that we’ll address throughout
    the next few chapters. We’ll also take a look at a few useful diagnostic tools
    and use the low-level Delayed API to create a simple custom task graph.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 DAG 的工作原理有了基本的了解，让我们来看看 Dask 是如何使用 DAG 来创建健壮、可扩展的工作负载的。为此，我们将使用你在上一章末尾下载的纽约市停车罚单数据。这将帮助我们同时完成两件事：你将第一次尝试使用
    Dask 的 DataFrame API 分析结构化数据集，并且你将开始熟悉数据集中的一些怪癖，这些怪癖将在接下来的几章中解决。我们还将查看一些有用的诊断工具，并使用低级别的
    Delayed API 创建一个简单的自定义任务图。
- en: Before we dive into the Dask code, if you haven’t already done so, check out
    the appendix for instructions on how to install Dask and all the packages you’ll
    need for the code examples in the remainder of the book. You can also find the
    complete code notebooks online at [www.manning.com/books/data-science-with-python-and-dask](http://www.manning.com/books/data-science-with-python-and-dask)
    and also at [http://bit.ly/daskbook](http://bit.ly/daskbook). For all the examples
    throughout the book (unless otherwise noted), I recommend you use a Jupyter Notebook.
    Jupyter Notebooks will help you keep your code organized and will make it easy
    to produce visualizations when necessary. The code in the examples has been tested
    in both Python 2.7 and Python 3.6 environments, so you should be able to use either
    without issue. Dask is available for both major versions of Python, but I highly
    recommend that you use Python 3 for any new projects, because the end of support
    for Python 2 is slated for 2020.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入 Dask 代码之前，如果你还没有这样做，请查看附录以获取有关如何安装 Dask 以及本书其余部分代码示例所需的所有包的说明。你还可以在 [www.manning.com/books/data-science-with-python-and-dask](http://www.manning.com/books/data-science-with-python-and-dask)
    和 [http://bit.ly/daskbook](http://bit.ly/daskbook) 上找到完整的代码笔记本。对于本书中的所有示例（除非另有说明），我建议你使用
    Jupyter Notebook。Jupyter Notebooks 将帮助你保持代码的整洁，并在必要时轻松生成可视化。示例中的代码已在 Python 2.7
    和 Python 3.6 环境中进行了测试，因此你应该能够无问题地使用其中任何一个。Dask 适用于 Python 的两个主要版本，但我强烈建议你使用 Python
    3 进行任何新的项目，因为 Python 2 的支持将在 2020 年结束。
- en: Finally, before we get going, we’ll take a moment to set the table for where
    we’ll be headed over the next few chapters. As mentioned before, the objective
    of this book is to teach you the fundamentals of Dask in a pragmatic way that
    focuses on how to use it for common data science tasks. [Figure 2.1](#figure2.1)
    represents a fairly standard way of approaching data science problems, and we’ll
    use this workflow as a backdrop to demonstrate how to apply Dask to each part
    of it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们开始之前，我们将花一点时间来设定接下来几章我们将要走向的方向。如前所述，本书的目的是以实用主义的方式教授你 Dask 的基础知识，重点关注如何将其用于常见的数据科学任务。[图
    2.1](#figure2.1) 代表了一种相当标准的处理数据科学问题的方法，我们将以此工作流程为背景来演示如何将 Dask 应用于其各个部分。
- en: '![c02_01.eps](Images/c02_01.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![c02_01.eps](Images/c02_01.png)'
- en: '[Figure 2.1](#figureanchor2.1) Our workflow, at a glance, in *Data Science
    with Python and Dask*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.1](#figureanchor2.1) 在 *使用 Python 和 Dask 进行数据科学* 中，我们的工作流程一览'
- en: In this chapter, we’ll take a look at a few snippets of Dask code that fall
    into the areas of data gathering, data cleaning, and exploratory analysis. However,
    chapters 4, 5, and 6 will cover those topics much more in depth. Instead, the
    focus here is to give you a first glimpse of what Dask syntax looks like. We’ll
    also focus on how the high-level commands we give Dask relate to the DAGs generated
    by the underlying scheduler. So, let’s get going!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将查看一些 Dask 代码片段，这些代码片段涉及数据收集、数据清理和探索性分析。然而，第 4、5 和 6 章将更深入地探讨这些主题。在这里，我们的重点是给你一个
    Dask 语法的第一印象。我们还将关注我们给予 Dask 的高级命令如何与底层调度器生成的 DAG 相关联。那么，让我们开始吧！
- en: '2.1 Hello Dask: A first look at the DataFrame API'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 欢迎使用 Dask：DataFrame API 的首次探索
- en: An essential step of any data science project is to perform exploratory analysis
    on the dataset. During exploratory analysis, you’ll want to check the data for
    missing values, outliers, and any other data quality issues. Cleaning the dataset
    ensures that the analysis you do, and any conclusions you make about the data,
    are not influenced by erroneous or anomalous data. In our first look at using
    Dask DataFrames, we’ll step through reading in a data file, scanning the data
    for missing values, and dropping columns that either are missing too much data
    or won’t be useful for analysis.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 任何数据科学项目的关键步骤之一是对数据集进行探索性分析。在探索性分析过程中，你需要检查数据中的缺失值、异常值以及任何其他数据质量问题。清理数据集确保你进行的分析和关于数据的任何结论都不会受到错误或异常数据的影响。在我们第一次使用
    Dask DataFrames 的探索中，我们将逐步演示如何读取数据文件，扫描数据中的缺失值，以及删除那些数据缺失过多或对分析无用的列。
- en: 2.1.1 Examining the metadata of Dask objects
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 检查 Dask 对象的元数据
- en: For this example, we’ll look at just the data collected from 2017\. First, you’ll
    need to import the Dask modules and read in your data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将查看从 2017 年收集的数据。首先，你需要导入 Dask 模块并读取你的数据。
- en: Listing 2.1 Importing relevant libraries and data
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 导入相关库和数据
- en: '[PRE0]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you’re an experienced Pandas user, [listing 2.1](#listing2.1) will look very
    familiar. In fact, it is syntactically equivalent! For simplicity’s sake, I’ve
    unzipped the data into the same folder as the Python notebook I’m working in.
    If you put your data elsewhere, you will either need to find the correct path
    to use or change your working directory to the folder that contains your data
    using os.chdir. Inspecting the DataFrame we just created yields the output shown
    in [figure 2.2](#figure2.2).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个经验丰富的Pandas用户，[列表2.1](#listing2.1)看起来会非常熟悉。事实上，它在语法上是等价的！为了简单起见，我已经将数据解压缩到与我正在工作的Python笔记本相同的文件夹中。如果你将数据放在其他地方，你可能需要找到正确的路径来使用，或者使用os.chdir更改工作目录到包含你的数据的文件夹。检查我们刚刚创建的DataFrame会产生[图2.2](#figure2.2)中所示的输出。
- en: '![c02_02.eps](Images/c02_02.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![c02_02.eps](Images/c02_02.png)'
- en: '[Figure 2.2](#figureanchor2.2) Inspecting the Dask DataFrame'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.2](#figureanchor2.2) 检查Dask DataFrame'
- en: The output of [listing 2.1](#listing2.1) might not be what you expected. While
    Pandas would display a sample of the data, when inspecting a Dask DataFrame, we
    are shown the metadata of the DataFrame. The column names are along the top, and
    underneath is each column’s respective datatype. Dask tries very hard to intelligently
    infer datatypes from the data, just as Pandas does. But its ability to do so accurately
    is limited by the fact that Dask was built to handle medium and large datasets
    that can’t be loaded into RAM at once. Since Pandas can perform operations entirely
    in memory, it can quickly and easily scan the entire DataFrame to find the best
    datatype for each column. Dask, on the other hand, must be able to work just as
    well with local datasets and large datasets that could be scattered across multiple
    physical machines in a distributed filesystem. Therefore, Dask DataFrames employ
    random sampling methods to profile and infer datatypes from a small sample of
    the data. This works fine if data anomalies, such as letters appearing in a numeric
    column, are widespread. However, if there’s a single anomalous row among millions
    or billions of rows, it’s very improbable that the anomalous row would be picked
    in a random sample. This will lead to Dask picking an incompatible datatype, which
    will cause errors later on when performing computations. Therefore, a best practice
    to avoid that situation would be to explicitly set datatypes rather than relying
    on Dask’s inference process. Even better, storing data in a binary file format
    that supports explicit data types, such as Parquet, will avoid the issue altogether
    and bring some additional performance gains to the table as well. We will return
    to this issue in a later chapter, but for now we will let Dask infer datatypes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表2.1](#listing2.1)的输出可能不是你所期望的。虽然Pandas会显示数据的一个样本，但在检查Dask DataFrame时，我们看到的却是DataFrame的元数据。列名位于顶部，下面是每个列的相应数据类型。Dask非常努力地尝试智能地推断数据类型，就像Pandas一样。但它的准确性受到限制，因为Dask是为了处理那些不能一次性加载到RAM中的中等和大数据集而构建的。由于Pandas可以在内存中执行所有操作，它可以快速轻松地扫描整个DataFrame以找到每个列的最佳数据类型。另一方面，Dask必须能够与本地数据集以及可能分散在分布式文件系统中的多个物理机器上的大型数据集一样好地工作。因此，Dask
    DataFrame使用随机抽样方法来从数据的小样本中分析和推断数据类型。如果数据异常，如数字列中出现字母，是普遍现象，这没问题。然而，如果在数百万或数十亿行中只有一行异常，那么在随机样本中选中异常行的可能性非常低。这将导致Dask选择一个不兼容的数据类型，这将在后续的计算中导致错误。因此，为了避免这种情况的最佳实践是显式设置数据类型，而不是依赖于Dask的推断过程。甚至更好的做法是存储支持显式数据类型的二进制文件格式，如Parquet，这可以完全避免问题，并带来一些额外的性能提升。我们将在后面的章节中回到这个问题，但现在我们将让Dask推断数据类型。'
- en: The other interesting pieces of information about the DataFrame’s metadata give
    us insight into how Dask’s scheduler is deciding to break up the work of processing
    this file. The `npartitions` value shows how many partitions the DataFrame is
    split into. Since the 2017 file is slightly over 2 GB in size, at 33 partitions,
    each partition is roughly 64 MB in size. That means that instead of loading the
    entire file into RAM all at once, each Dask worker thread will work on processing
    the file one 64 MB chunk at a time.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的元数据中其他有趣的信息让我们了解到Dask调度器是如何决定分割处理这个文件的工作的。`npartitions`值显示了DataFrame被分割成多少个分区。由于2017年的文件大小略超过2
    GB，在33个分区的情况下，每个分区的大小大约为64 MB。这意味着不是一次性将整个文件加载到RAM中，而是每个Dask工作线程一次处理一个64 MB的数据块。
- en: '![c02_03.eps](Images/c02_03.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![c02_03.eps](Images/c02_03.png)'
- en: '[Figure 2.3](#figureanchor2.3) Dask splits large data files into multiple partitions
    and works on one partition at a time.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.3](#figureanchor2.3) Dask 将大型数据文件拆分成多个分区，并一次处理一个分区。'
- en: '[Figure 2.3](#figure2.3) demonstrates this behavior. Rather than eagerly loading
    the entire DataFrame into RAM, Dask breaks the file into smaller chunks that can
    be worked on independently. These chunks are called *partitions*. In the case
    of Dask DataFrames, each partition is a relatively small Pandas DataFrame. In
    the example in [figure 2.3](#figure2.3), the DataFrame consists of two partitions.
    Therefore, the single Dask DataFrame is made up of two smaller Pandas DataFrames.
    Each partition can be loaded into memory and worked on one at a time or in parallel.
    In this case, the worker node first picks up partition 1 and processes it, and
    saves the result in a temporary holding space. Next it picks up partition 2 and
    processes it, saving the result to a temporary holding space. Finally, it combines
    the results and ships it down to our client, which displays the result. Because
    the worker node can work on smaller pieces of the data at a time, work can be
    distributed out to many machines. Or, in the case of a local cluster, work can
    proceed on very large datasets without resulting in out-of-memory errors.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.3](#figure2.3) 展示了这种行为。Dask 不是将整个 DataFrame 整个加载到 RAM 中，而是将文件拆分成更小的块，这些块可以独立工作。这些块被称为
    *分区*。在 Dask DataFrame 的情况下，每个分区都是一个相对较小的 Pandas DataFrame。在 [图 2.3](#figure2.3)
    的例子中，DataFrame 由两个分区组成。因此，单个 Dask DataFrame 由两个较小的 Pandas DataFrame 组成。每个分区可以单独加载到内存中并处理，或者并行处理。在这种情况下，工作节点首先获取分区
    1 并处理它，并将结果保存在临时存储空间中。接下来，它获取分区 2 并处理它，将结果保存到临时存储空间中。最后，它将结果合并并发送到我们的客户端，客户端显示结果。因为工作节点可以一次处理数据的小块，所以可以将工作分配到多台机器上。或者，在本地集群的情况下，可以在不产生内存不足错误的情况下处理非常大的数据集。'
- en: 'The last bit of metadata we got from our DataFrame is that it consists of 99
    tasks. That’s telling us that Dask created a DAG with 99 nodes to process the
    data. The graph consists of 99 nodes because each partition requires three operations
    to be created: reading the raw data, splitting the data into the appropriately
    sized block, and initializing the underlying DataFrame object. In total, 33 partitions
    with 3 tasks per partition results in 99 tasks. If we had 33 workers in our worker
    pool, the entire file could be worked on simultaneously. With just one worker,
    Dask will cycle through each partition one at a time. Now, let’s try to count
    the missing values in each column across the entire file.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 DataFrame 获取的最后一点元数据是它由 99 个任务组成。这告诉我们 Dask 创建了一个包含 99 个节点的 DAG 来处理数据。图由
    99 个节点组成，因为每个分区需要创建三个操作：读取原始数据、将数据分割成适当大小的块，以及初始化底层的 DataFrame 对象。总共 33 个分区，每个分区有
    3 个任务，结果产生 99 个任务。如果我们有 33 个工作节点在工作池中，整个文件可以同时处理。只有一个工作节点时，Dask 将逐个循环处理每个分区。现在，让我们尝试计算整个文件中每个列的缺失值。
- en: Listing 2.2 Counting missing values in the DataFrame
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 在 DataFrame 中计算缺失值
- en: '[PRE1]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The syntax for counting null values again looks a lot like Pandas. But as before,
    inspecting the resulting Series object doesn’t give us the output we might expect.
    Instead of getting the missing counts, Dask returns some metadata information
    about the expected result. It looks like `missing_values` is a Series of `int64`s,
    but where’s the actual data? Dask hasn’t actually done any processing yet because
    it uses *lazy computation*. This means that what Dask has actually done under
    the hood is prepare another DAG, which was then stored in the `missing_values`
    variable. The data isn’t computed until the task graph is explicitly executed.
    This behavior makes it possible to build up complex task graphs quickly without
    having to wait for each intermediate step to finish. You might notice that the
    tasks count has grown to 166 now. That’s because Dask has taken the first 99 tasks
    from the DAG used to read in the data file and create the DataFrame called `df`,
    added 66 tasks (2 per partition) to check for nulls and sum, and then added a
    final step to collect all the pieces together into a single Series object and
    return the answer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 计算空值的语法再次看起来很像 Pandas。但就像之前一样，检查结果的 Series 对象并没有给出我们可能期望的输出。Dask 并没有真正进行任何处理，因为它使用的是
    *延迟计算*。这意味着 Dask 实际上在底层所做的只是准备另一个 DAG，然后将其存储在 `missing_values` 变量中。数据实际上是在任务图明确执行之前不会计算的。这种行为使得可以快速构建复杂的任务图，而无需等待每个中间步骤完成。你可能注意到任务计数现在增长到了
    166。这是因为 Dask 从用于读取数据文件并创建名为 `df` 的 DataFrame 的 DAG 中取出了前 99 个任务，然后添加了 66 个任务（每个分区
    2 个）来检查空值和求和，最后添加了一个最终步骤，将所有部分收集到一个单一的 Series 对象中并返回答案。
- en: Listing 2.3 Calculating the percent of missing values in the DataFrame
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 计算 DataFrame 中缺失值的百分比
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Before we run the computation, we’ll ask Dask to transform these numbers into
    percentages by dividing the missing value counts (`missing_values`) by the total
    number of rows in the DataFrame (`df.index.size`), then multiplying everything
    by 100\. Notice that the number of tasks has increased again, and the datatype
    of the resulting Series changed from `int64` to `float64`! This is because the
    division operation resulted in answers that were not whole (integer) numbers.
    Therefore, Dask automatically converted the answer to floating-point (decimal)
    numbers. Just as Dask tries to infer datatypes from files, it will also try to
    infer how operations will affect the datatype of the output. Since we’ve added
    a stage to the DAG that divides two numbers, Dask infers that we’ll likely move
    from integers to floating-point numbers and changes the metadata of the result
    accordingly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行计算之前，我们将要求 Dask 将这些数字转换为百分比，方法是将缺失值计数（`missing_values`）除以 DataFrame 中的总行数（`df.index.size`），然后将所有结果乘以
    100。请注意，任务的数量再次增加，结果的 Series 数据类型从 `int64` 变为 `float64`！这是因为除法操作产生了非整数（整数）的答案。因此，Dask
    自动将答案转换为浮点数（小数）。正如 Dask 尝试从文件中推断数据类型一样，它也会尝试推断操作将如何影响输出的数据类型。由于我们在 DAG 中添加了一个除法阶段，Dask
    推断我们可能会从整数转换为浮点数，并相应地更改结果的元数据。
- en: 2.1.2 Running computations with the compute method
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 使用 compute 方法运行计算
- en: Now we’re ready to run and produce our output.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好运行并生成我们的输出。
- en: Listing 2.4 Computing the DAG
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 计算有向无环图（DAG）
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Whenever you want Dask to compute the result of your work, you need to call
    the `.compute()` method of the DataFrame. This tells Dask to go ahead and run
    the computation and display the results. You may sometimes see this referred to
    as *materializing* results, because the DAG that Dask creates to run the computation
    is a logical representation of the results, but the actual results aren’t calculated
    (that is, materialized) until you explicitly compute them. You’ll also notice
    that we wrapped the call to compute within a `ProgressBar` context. This is one
    of several diagnostic contexts that Dask provides to help you keep track of running
    tasks, and especially comes in handy when using the local task scheduler. The
    `ProgressBar` context will simply print out a text-based progress bar showing
    you the estimated percent complete and the elapsed time for the computation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你想让 Dask 计算你工作的结果时，你需要调用 DataFrame 的 `.compute()` 方法。这告诉 Dask 开始运行计算并显示结果。你有时可能会看到这被称为
    *materializing* 结果，因为 Dask 创建用于运行计算的 DAG 是结果的一个逻辑表示，但实际上结果只有在您明确计算它们时才会被计算（即materialized）。你还会注意到我们将对
    `compute` 的调用包裹在 `ProgressBar` 上下文中。这是 Dask 提供的几个诊断上下文之一，可以帮助你跟踪正在运行的任务，尤其是在使用本地任务调度器时特别有用。`ProgressBar`
    上下文将简单地打印出一个基于文本的进度条，显示计算的估计完成百分比和已过时间。
- en: 'By the output of our missing-values calculation, it looks like we can immediately
    throw out a few columns: No Standing or Stopping Violation, Hydrant Violation,
    and Double Parking Violation are completely empty, so there’s no value in keeping
    them around. We’ll drop any column that’s missing more than 60% of its values
    (note: 60% is just an arbitrary value chosen for the sake of example; the threshold
    you use to throw out columns with missing data depends on the problem you’re trying
    to solve, and usually relies on your best judgement).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的缺失值计算输出，看起来我们可以立即丢弃一些列：无站立或停止违规、消防栓违规和双停车违规都是完全空的，所以保留它们没有价值。我们将删除任何缺失值超过
    60% 的列（注意：60% 只是为了示例而选择的任意值；你用来丢弃具有缺失数据的列的阈值取决于你试图解决的问题，并且通常依赖于你的最佳判断）。
- en: Listing 2.5 Filtering sparse columns
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 过滤稀疏列
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is interesting. Since we materialized the data in [listing 2.4](#listing2.4),
    `missing_count_pct` is a Pandas Series object, but we can use it with the `drop`
    method on the Dask DataFrame. We first took the Series we created in [listing
    2.4](#listing2.4) and filtered it to get the columns that have more than 60% missing
    values. We then got the index of the filtered Series, which is a list of column
    names. We then used that index to drop columns in the Dask DataFrame with the
    same name. You can generally mix Pandas objects and Dask objects because each
    partition of a Dask DataFrame is a Pandas DataFrame. In this case, the Pandas
    Series object is made available to all threads, so they can use it in their computation.
    In the case of running on a cluster, the Pandas Series object will be serialized
    and broadcasted to all worker nodes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣。由于我们在 [清单 2.4](#listing2.4) 中materialized了数据，`missing_count_pct` 是一个 Pandas
    Series 对象，但我们可以使用它对 Dask DataFrame 应用 `drop` 方法。我们首先取在 [清单 2.4](#listing2.4) 中创建的
    Series，并过滤它以获取具有超过 60% 缺失值的列。然后我们获取过滤后的 Series 的索引，它是一个列名列表。然后我们使用该索引在具有相同名称的
    Dask DataFrame 中删除列。通常可以混合 Pandas 对象和 Dask 对象，因为 Dask DataFrame 的每个分区都是一个 Pandas
    DataFrame。在这种情况下，Pandas Series 对象对所有线程都是可用的，因此它们可以在计算中使用它。在集群上运行的情况下，Pandas Series
    对象将被序列化并广播到所有工作节点。
- en: 2.1.3 Making complex computations more efficient with persist
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 使用 persist 提高复杂计算的效率
- en: Because we’ve decided that we don’t care about the columns we just dropped,
    it would be inefficient to re-read the columns into memory every time we want
    to make an additional calculation just to drop them again. We really only care
    about analyzing the filtered subset of data we just created. Recall that as soon
    as a node in the active task graph emits results, its intermediate work is discarded
    in order to minimize memory usage. That means if we want to do something additional
    with the filtered data (for example, look at the first five rows of the DataFrame),
    we would have to go to the trouble of re-running the entire chain of transformations
    again. To avoid repeating the same calculations many times over, Dask allows us
    to store intermediate results of a computation so they can be reused. Using the
    `persist()` method of the Dask DataFrame tells Dask to try to keep as much of
    the intermediate result in memory as possible. In case Dask needs some of the
    memory being used by the persisted DataFrame, it will select a number of partitions
    to drop from memory. These dropped partitions will be recalculated on the fly
    when needed, and although it may take some time to recalculate the missing partitions,
    it is still likely to be much faster than recomputing the entire DataFrame. Using
    `persist` appropriately can be very useful for speeding up computations if you
    have a very large and complex DAG that needs to be reused many times.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经决定我们不在乎刚刚删除的列，每次我们想要进行额外的计算并再次删除它们时重新读取这些列到内存中将会非常低效。我们真正关心的是分析我们刚刚创建的过滤数据子集。回想一下，一旦活动任务图中的节点发出结果，其中间工作就会被丢弃，以最小化内存使用。这意味着如果我们想要对过滤数据进行一些额外的操作（例如，查看DataFrame的前五行），我们就必须麻烦地重新运行整个转换链。为了避免多次重复相同的计算，Dask允许我们存储计算的中间结果，以便可以重用。使用Dask
    DataFrame的`persist()`方法告诉Dask尽可能多地保留中间结果在内存中。如果Dask需要一些由持久化的DataFrame使用的内存，它将选择一些分区从内存中删除。这些被删除的分区将在需要时即时重新计算，尽管重新计算缺失分区可能需要一些时间，但它仍然可能比重新计算整个DataFrame要快得多。适当地使用`persist`可以在你有非常庞大且复杂的DAG需要多次重用时非常有用，以加快计算速度。
- en: This concludes our first look at Dask DataFrames. You saw how, in only a few
    lines of code, we were able to read in a dataset and begin preparing it for exploratory
    analysis. The beauty of this code is that it works the same regardless of whether
    you’re running Dask on one machine or thousands of machines, and regardless of
    whether you’re crunching through a couple gigabytes of data (as we did here) or
    analyzing petabytes of data. Also, because of the syntactic similarities with
    Pandas, you can easily transition workloads from Pandas to Dask with a minimum
    amount of code refactoring (which mostly amounts to adding Dask imports and `compute`
    calls). We’ll go much further into our analysis in the coming chapters, but for
    now we’ll dig a little deeper into how Dask uses DAGs to manage distribution of
    the tasks that underpin the code we just walked through.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对Dask DataFrame的第一瞥。你看到了，仅用几行代码，我们就能读取数据集并开始为探索性分析做准备。这段代码的美丽之处在于，无论你是在一台机器上运行Dask还是在数千台机器上运行，无论你是在处理几GB的数据（就像我们在这里做的那样）还是在分析PB级的数据，它的工作方式都是一样的。此外，由于与Pandas的语法相似性，你可以很容易地将工作负载从Pandas迁移到Dask，而无需进行大量的代码重构（这主要涉及到添加Dask导入和`compute`调用）。我们将在接下来的章节中深入分析，但现在我们将更深入地探讨Dask是如何使用DAGs来管理支撑我们刚刚走过的代码的任务的分布。
- en: 2.2 Visualizing DAGs
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 可视化DAGs
- en: So far, you’ve learned how directed acyclic graphs (DAGs) work, and you’ve learned
    that Dask uses DAGs to orchestrate distributed computations of DataFrames. However,
    we haven’t yet peeked “under the hood” and seen the actual DAGs that the schedulers
    create. Dask uses the graphviz library to generate visual representations of the
    DAGs created by the task scheduler. If you followed the steps in the appendix
    to install graphviz, you will be able to inspect the DAG backing any Dask Delayed
    object. You can inspect the DAGs of DataFrames, series, bags, and arrays by calling
    the `visualize()` method on the object.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了有向无环图（DAGs）的工作原理，并且了解到Dask使用DAGs来协调DataFrame的分布式计算。然而，我们还没有深入“引擎盖下”看看调度器实际创建的DAGs。Dask使用graphviz库来生成任务调度器创建的DAGs的可视化表示。如果你按照附录中的步骤安装了graphviz，你将能够检查任何Dask
    Delayed对象的DAG。你可以通过在对象上调用`visualize()`方法来检查DataFrame、series、bags和数组的DAG。
- en: 2.2.1 Visualizing a simple DAG using Dask Delayed objects
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 使用 Dask Delayed 对象可视化简单的 DAG
- en: 'For this example, we’ll take a step back from the Dask DataFrame objects seen
    in the previous example to step down a level of abstraction: the Dask Delayed
    object. The reason we’ll move to Delayed objects is because the DAGs that Dask
    creates for even simple DataFrame operations can grow quite large and be hard
    to visualize. Therefore, for convenience, we’ll use Dask Delayed objects for this
    example so we have better control over composition of the DAG.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将从之前例子中看到的 Dask DataFrame 对象退后一步，降低一个抽象级别：Dask Delayed 对象。我们将转向延迟对象的原因是，Dask
    为即使是简单的 DataFrame 操作创建的 DAG 可能会变得非常大，难以可视化。因此，为了方便起见，我们将使用 Dask Delayed 对象进行这个例子，以便我们更好地控制
    DAG 的组合。
- en: Listing 2.6 Creating some simple functions
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 创建一些简单的函数
- en: '[PRE5]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 2.6](#listing2.6) begins by importing the packages needed for this
    example: in this case, the `delayed` package and the `ProgressBar` diagnostic
    we used previously. Next, we define a couple simple Python functions. The first
    adds one to its given input, and the second adds the two given inputs. The next
    three lines introduce the `delayed` constructor. By wrapping `delayed` around
    a function, a Dask Delayed representation of the function is produced. Delayed
    objects are equivalent to a node in a DAG. The arguments of the original function
    are passed in a second set of parentheses. For example, the object `x` represents
    a Delayed evaluation of the `inc` function, passing in 1 as the value of `i`.
    Delayed objects can reference other Delayed objects as well, which can be seen
    in the definition of the object `z`. Chaining together these Delayed objects is
    what ultimately makes up a graph. For the object `z` to be evaluated, both of
    the objects `x` and `y` must first be evaluated. If `x` or `y` has other Delayed
    dependencies that must be met as part of their evaluation, those dependencies
    would need to be evaluated first and so forth. This sounds an awful lot like a
    DAG: evaluating the object `z` has a well-known chain of dependencies that must
    be evaluated in a deterministic order, and there is a well-defined start and end
    point. Indeed, this is a representation of a very simple DAG in code. We can have
    a look at what that looks like by using the `visualize` method.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 2.6](#listing2.6) 首先导入了本例所需的包：在这种情况下，是 `delayed` 包和之前使用的 `ProgressBar`
    诊断工具。接下来，我们定义了几个简单的 Python 函数。第一个函数将其输入值加一，第二个函数将两个输入值相加。接下来的三行介绍了 `delayed` 构造函数。通过将
    `delayed` 包裹在一个函数周围，可以生成该函数的 Dask Delayed 表示。延迟对象相当于 DAG 中的一个节点。原始函数的参数通过第二组括号传入。例如，对象
    `x` 代表对 `inc` 函数的延迟评估，传入 1 作为 `i` 的值。延迟对象也可以引用其他延迟对象，这在对象 `z` 的定义中可以看得到。将这些延迟对象连接起来最终构成了一个图。为了评估对象
    `z`，必须首先评估对象 `x` 和 `y`。如果 `x` 或 `y` 有其他必须满足的延迟依赖，那么这些依赖需要先被评估，依此类推。这听起来非常像 DAG：评估对象
    `z` 有一个已知的依赖链，必须以确定性的顺序进行评估，并且有一个明确的起点和终点。确实，这是代码中一个非常简单的 DAG 的表示。我们可以通过使用 `visualize`
    方法来看看它是什么样子。'
- en: '![c02_04.png](Images/c02_04.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![c02_04.png](Images/c02_04.png)'
- en: '[Figure 2.4](#figureanchor2.4) A visual representation of the DAG produced
    in [listing 2.6](#listing2.6)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.4](#figureanchor2.4) 由 [列表 2.6](#listing2.6) 产生的 DAG 的可视化表示'
- en: As you can see in [figure 2.4](#figure2.4), object `z` is represented by a DAG.
    At the bottom of the graph, we can see the two calls to the `inc` function. That
    function didn’t have any Delayed dependencies of its own, so there are no lines
    with arrows pointing into the `inc` nodes. However, the `add` node has two lines
    with arrows pointing into it. This represents the dependency on first calculating
    `x` and `y` before being able to sum the two values. Since each `inc` node is
    free of dependencies, a unique worker would be able to work on each task independently.
    Taking advantage of parallelism like this could be very advantageous if the `inc`
    function took a long time to evaluate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在 [图 2.4](#figure2.4) 中所见，对象 `z` 由一个 DAG 表示。在图的底部，我们可以看到对 `inc` 函数的两个调用。该函数没有自己的延迟依赖，因此没有指向
    `inc` 节点的箭头线。然而，`add` 节点有两个指向它的箭头线。这表示在能够对两个值求和之前，必须先计算 `x` 和 `y`。由于每个 `inc` 节点没有依赖，一个独特的工人可以独立地处理每个任务。如果
    `inc` 函数的评估需要很长时间，利用这种并行性可能会非常有优势。
- en: 2.2.2 Visualizing more complex DAGs with loops and collections
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 使用循环和集合可视化更复杂的 DAG
- en: Let’s look at a slightly more complex example.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个稍微复杂一点的例子。
- en: Listing 2.7 Performing the `add_two` operation
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.7 执行`add_two`操作
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now things are getting interesting. Let’s unpack what happened here. We started
    again by defining a few simple functions, and also defined a list of integers
    to use. This time, though, instead of creating a Delayed object from a single
    function call, the Delayed constructor is placed inside a list comprehension that
    iterates over the list of numbers. The result is that `step1` becomes a list of
    Delayed objects instead of a list of integers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在事情变得有趣了。让我们分析一下这里发生了什么。我们再次定义了一些简单的函数，并定义了一个整数列表以供使用。不过，这次不是从单个函数调用创建一个 Delayed
    对象，而是将 Delayed 构造函数放在一个列表推导式中，该推导式遍历数字列表。结果是`step1`变成了一个 Delayed 对象的列表，而不是一个整数列表。
- en: The next line of code uses the built-in `sum` function to add up all the numbers
    in the list. The `sum` function normally takes an iterable as an argument, but
    since it’s been wrapped in a Delayed constructor, it can be passed the list of
    Delayed objects. As before, this code ultimately represents a graph. Let’s take
    a look at what the graph looks like.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码使用内置的`sum`函数将列表中的所有数字加起来。`sum`函数通常需要一个可迭代对象作为参数，但由于它被包裹在一个 Delayed 构造函数中，所以可以将
    Delayed 对象的列表传递给它。和之前一样，这段代码最终代表了一个图。让我们看看这个图是什么样子。
- en: '![c02_05.eps](Images/c02_05.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![c02_05.eps](Images/c02_05.png)'
- en: '[Figure 2.5](#figureanchor2.5) The directed acyclic graph representing the
    computation in [listing 2.7](#listing2.7)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.5](#figureanchor2.5) 代表[列表 2.7](#listing2.7)中计算的定向无环图'
- en: 'Now, the variable total is a Delayed object, which means we can use the `visualize`
    method on it to draw the DAG that Dask will use if we ask Dask to compute the
    answer! [Figure 2.5](#figure2.5) shows the output of the `visualize` method. One
    thing to notice is that Dask draws DAGs from the bottom up. We started with four
    numbers in a list called `data`, which corresponds to four nodes at the bottom
    of the DAG. The circles on the Dask DAGs represent function calls. This makes
    sense: we had four numbers, and we wanted to apply the `add_two` function to each
    number so we have to call it four times. Similarly, we call the `sum` function
    only one time because we’re passing in the complete list. The squares on the DAG
    represent intermediate results. For instance, the result of iterating over the
    list of numbers and applying the `add_two` function to each of the original numbers
    is four transformed numbers that had two added to them. Just like with the DataFrame
    in the previous section, Dask doesn’t actually compute the answer until you call
    the `compute` method on the total object.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，变量 total 是一个 Delayed 对象，这意味着我们可以使用它的`visualize`方法来绘制 DAG，这样如果要求 Dask 计算答案时，Dask
    会使用这个 DAG！[图 2.5](#figure2.5)显示了`visualize`方法的输出。需要注意的是，Dask 是从下往上绘制 DAG 的。我们从列表`data`中的四个数字开始，这对应于
    DAG 底部的四个节点。Dask DAG 上的圆圈代表函数调用。这很有道理：我们本来有四个数字，我们想要将`add_two`函数应用到每个数字上，所以我们必须调用它四次。同样，我们只调用一次`sum`函数，因为我们传递的是完整的列表。DAG
    上的方块代表中间结果。例如，迭代数字列表并应用`add_two`函数到原始数字上的结果是四个转换后的数字，每个数字都增加了二。就像前一个部分中的 DataFrame
    一样，Dask 实际上不会在调用 total 对象上的`compute`方法之前计算答案。
- en: '![c02_06.eps](Images/c02_06.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![c02_06.eps](Images/c02_06.png)'
- en: '[Figure 2.6](#figureanchor2.6) The DAG from [figure 2.5](#figure2.5) with the
    values superimposed over the computation'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.6](#figureanchor2.6) 在[图 2.5](#figure2.5)上的 DAG，数值叠加到计算中'
- en: In [figure 2.6](#figure2.6), we’ve taken the four numbers from the data list
    and superimposed them over the DAG so you can see the result of each function
    call. The result, 32, is calculated by taking the original four numbers, applying
    the `addTwo` transformation to each, then summing the result.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2.6](#figure2.6)中，我们从数据列表中取出了四个数字，并将它们叠加到 DAG 上，以便您可以看到每个函数调用的结果。结果 32 是通过取原始的四个数字，对每个数字应用`addTwo`转换，然后求和得到的。
- en: We’ll now add another degree of complexity to the DAG by multiplying every number
    by four before collecting the result.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过在收集结果之前将每个数字乘以四来增加 DAG 的复杂度。
- en: Listing 2.8 Multiply each value by four
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.8 将每个值乘以四
- en: '[PRE7]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This looks an awful lot like the previous code listing, with one key difference.
    In the first line of code, we apply the `multiply_four` function to `step1`, which
    was the list of Delayed objects we produced by adding two to the original list
    of numbers. Just like you saw in the DataFrame example, it’s possible to chain
    together computations without immediately calculating the intermediate results.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常像之前的代码列表，但有一个关键的区别。在代码的第一行，我们将 `multiply_four` 函数应用于 `step1`，这是通过将原始数字列表加2得到的
    Delayed 对象列表。就像你在 DataFrame 示例中看到的那样，可以在不立即计算中间结果的情况下链式进行计算。
- en: '![c02_07.eps](Images/c02_07.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![c02_07.eps](Images/c02_07.png)'
- en: '[Figure 2.7](#figureanchor2.7) The DAG including the multiplyFour step'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.7](#figureanchor2.7) 包含乘以四步骤的 DAG'
- en: '[Figure 2.7](#figure2.7) shows the output of the computation in [listing 2.8](#listing2.8).
    If you look closely at the DAG, you’ll notice another layer has been added between
    the `addTwo` nodes and the sum node. This is because we’ve now instructed Dask
    to take each number from the list, add two to it, then add four, and *then* sum
    the results.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.7](#figure2.7) 展示了 [列表2.8](#listing2.8) 中计算的结果。如果你仔细观察 DAG，你会注意到在 `addTwo`
    节点和求和节点之间增加了一层。这是因为我们现在指示 Dask 从列表中取出每个数字，将其加2，然后加4，然后**再**求和结果。'
- en: 2.2.3 Reducing DAG complexity with persist
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 使用持久化降低 DAG 复杂度
- en: 'Let’s now take this one step further: say we want to take this sum, add it
    back to each of our original numbers, then sum all that together.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们再进一步：假设我们想要将这个总和加回到我们的原始数字上，然后将所有这些数字加在一起。
- en: Listing 2.9 Adding another layer to the DAG
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.9 在 DAG 中添加另一层
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![c02_08.eps](Images/c02_08.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![c02_08.eps](Images/c02_08.png)'
- en: '[Figure 2.8](#figureanchor2.8) The DAG generated by [listing 2.9](#listing2.9)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.8](#figureanchor2.8) 由 [列表2.9](#listing2.9) 生成的 DAG'
- en: In this example, we’ve taken the complete DAG we created in the last example,
    which is stored in the `total` variable, and used it to create a new list of Delayed
    objects.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了在上一个例子中创建的完整 DAG，它存储在 `total` 变量中，并使用它创建了一个新的 Delayed 对象列表。
- en: 'The DAG in [figure 2.8](#figure2.8) looks like the DAG from [listing 2.9](#listing2.9)
    was copied and another DAG was fused on top of it. That’s precisely what we want!
    First, Dask will calculate the sum of the first set of transformations, then add
    it to each of the original numbers, then finally compute the sum of that intermediate
    step. As you can imagine, if we repeat this cycle a few more times, the DAG will
    start to get too large to visualize. Similarly, if we had 100 numbers in the original
    list instead of 4, the DAG diagram would also be very large (try replacing the
    data list with a `range[100]` and rerun the code!) But we touched on a more important
    reason in the last section as to why a large DAG might become unwieldy: persistence.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.8](#figure2.8) 中的 DAG 看起来像是 [列表2.9](#listing2.9) 中的 DAG 被复制，并在其上方融合了另一个
    DAG。这正是我们想要的！首先，Dask 将计算第一组变换的总和，然后将其添加到每个原始数字上，最后计算这个中间步骤的总和。正如你可以想象的那样，如果我们重复这个循环几次，DAG
    将开始变得太大而无法可视化。同样，如果我们原始列表中有100个数字而不是4个，DAG 图也会非常大（尝试将数据列表替换为 `range[100]` 并重新运行代码！）但在上一节中，我们提到了一个更重要的原因，即为什么大型
    DAG 可能会变得难以管理：持久化。'
- en: As mentioned before, every time you call the `compute` method on a Delayed object,
    Dask will step through the complete DAG to generate the result. This can be okay
    for simple computations, but if you’re working on very large, distributed datasets,
    it can quickly become inefficient to repeat calculations over and over again.
    One way around that is to persist intermediate results that you want to reuse.
    But what does that do to the DAG?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每次你在 Delayed 对象上调用 `compute` 方法时，Dask 都会遍历整个 DAG 以生成结果。这对于简单的计算来说可能没问题，但如果你正在处理非常大的分布式数据集，重复计算可能会迅速变得低效。一种解决方法是持久化你想要重用的中间结果。但这会对
    DAG 有什么影响呢？
- en: Listing 2.10 Persisting calculations
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.10 持久化计算
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![c02_09.png](Images/c02_09.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![c02_09.png](Images/c02_09.png)'
- en: '[Figure 2.9](#figureanchor2.9) The DAG generated by [listing 2.10](#listing2.10)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.9](#figureanchor2.9) 由 [列表2.10](#listing2.10) 生成的 DAG'
- en: In this example, we took the DAG we created in [listing 2.9](#listing2.9) and
    persisted it. What we get instead of the full DAG is a single result, as seen
    in [figure 2.9](#figure2.9) (remember that a rectangle represents a result). This
    result represents the value that Dask would calculate when the `compute` method
    is called on the `total` object. But instead of re-computing it every time we
    need to access its value, Dask will now compute it once and save the result in
    memory. We can now chain another delayed calculation on top of this persisted
    result, and we get some interesting results.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们取了我们创建的[列表 2.9](#listing2.9)中的 DAG 并将其持久化。我们得到的结果不是完整的 DAG，而是一个单一的结果，如图
    2.9 所示（记住，一个矩形代表一个结果）。这个结果代表了当在 `total` 对象上调用 `compute` 方法时，Dask 会计算出的值。但是，我们不需要每次访问其值时都重新计算它，Dask
    现在将只计算一次并将结果保存在内存中。现在我们可以在这个持久化的结果之上链式调用另一个延迟计算，并得到一些有趣的结果。
- en: Listing 2.11 Chaining a DAG from a persisted DAG
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.11 从持久化 DAG 链接 DAG
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The resulting DAG in [figure 2.10](#figure2.10) is much smaller. In fact, it
    looks like only the top half of the DAG from [listing 2.9](#listing2.9). That’s
    because the `sum-#1` result is precomputed and persisted. So instead of calculating
    the whole DAG in [listing 2.11](#listing2.11), Dask can use the persisted data,
    thereby reducing the number of calculations needed to produce the result.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 中生成的结果 DAG 要小得多。实际上，它看起来就像 [列表 2.9](#listing2.9) 中的 DAG 的上半部分。这是因为 `sum-#1`
    的结果已经被预先计算并持久化。因此，在 [列表 2.11](#listing2.11) 中，Dask 可以使用持久化的数据，从而减少产生结果所需的计算次数。
- en: Before we move on to the next section, give [listing 2.12](#listing2.12) a try!
    Dask can generate very large DAGs. Although the diagram won’t fit on this page,
    it will hopefully give you an appreciation of the complexity that Dask can handle
    very elegantly.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一节之前，先尝试一下[列表 2.12](#listing2.12)！Dask 可以生成非常大的 DAG。尽管这个图表可能不会在这个页面上完全显示，但它可能会让你对
    Dask 能够非常优雅地处理的复杂性有所认识。
- en: '![c02_10.eps](Images/c02_10.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![c02_10.eps](Images/c02_10.png)'
- en: '[Figure 2.10](#figureanchor2.10) The DAG generated by [listing 2.11](#listing2.11)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.10](#figureanchor2.10) 由 [列表 2.11](#listing2.11) 生成的 DAG'
- en: Listing 2.12 Visualizing the last NYC data DAG
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.12 可视化最后的纽约市数据 DAG
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 2.3 Task scheduling
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 任务调度
- en: As I’ve mentioned a few times now, Dask uses the concept of *lazy computations*
    throughout its API. We’ve seen the effect of this in action—whenever we perform
    some kind of action on a Dask Delayed object, we have to call the `compute` method
    before anything actually happens. This is quite advantageous when you consider
    the time it might take to churn through petabytes of data. Since no computation
    actually happens until you request the result, you can define the complete string
    of transformations that Dask should perform on the data without having to wait
    for one computation to finish before defining the next—leaving you to do something
    else (like dice onions for that pot of bucatini all’Amatriciana I’ve convinced
    you to make) while the complete result is computing!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前多次提到的，Dask 在其 API 中使用了 *惰性计算* 的概念。我们已经看到了这种效果的实际应用——无论我们对 Dask 延迟对象执行什么操作，我们都必须在实际上发生任何事情之前调用
    `compute` 方法。当你考虑到处理 PB 级数据可能需要的时间时，这非常有优势。由于实际上只有在请求结果时才会发生计算，因此你可以在定义下一个计算之前，无需等待一个计算完成就可以定义
    Dask 应该对数据进行的所有完整转换——这样你就可以在完整结果计算的同时做其他事情（比如为那锅你被我说服要做的 bucatini all’Amatriciana
    切洋葱）！
- en: 2.3.1 Lazy computations
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 惰性计算
- en: Lazy computations also allow Dask to split work into smaller logical pieces,
    which helps avoid loading the entire data structure that it’s operating on into
    memory. As you saw with the DataFrame in section 2.1, Dask divided the 2 GB file
    into 33 64 MB chunks, and operated on 8 chunks at a time. That means the maximum
    memory consumption for the entire operation didn’t exceed 512 MB, yet we were
    still able to process the entire 2 GB file. This gets even more important as the
    size of the datasets you work on stretches into the terabyte and petabyte range.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性计算还允许 Dask 将工作拆分成更小的逻辑块，这有助于避免将整个数据结构加载到内存中。正如你在 2.1 节中看到的 DataFrame，Dask
    将 2 GB 的文件分成了 33 个 64 MB 的块，并且一次操作 8 个块。这意味着整个操作的最大内存消耗不超过 512 MB，但我们仍然能够处理整个
    2 GB 的文件。当你处理的数据集大小扩展到 TB 和 PB 范围时，这一点变得更加重要。
- en: 'But what actually happens when you request the result from Dask? The computations
    you defined are represented by a DAG, which is a step-by-step plan for computing
    the result you want. However, that step-by-step plan doesn’t define which physical
    resources should be used to perform the computations. Two important things still
    must be considered: where the computations will take place and where the results
    of each computation should be shipped to if necessary. Unlike relational database
    systems, Dask does not predetermine the precise runtime location of each task
    before the work begins. Instead, the task scheduler dynamically assesses what
    work has been completed, what work is left to do, and what resources are free
    to accept additional work in real time. This allows Dask to gracefully handle
    a host of issues that arise in distributed computing, including recovery from
    worker failure, network unreliability, and workers completing work at different
    speeds. In addition, the task scheduler can keep track of where intermediate results
    have been stored, allowing follow-on work to be shipped to the data instead of
    unnecessarily shipping the data around the network. This results in far greater
    efficiency when operating Dask on a cluster.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 但当你从Dask请求结果时，实际上会发生什么呢？你定义的计算由一个DAG表示，这是一个计算所需结果的逐步计划。然而，这个逐步计划并没有定义应该使用哪些物理资源来执行计算。仍然必须考虑两个重要的事情：计算将在哪里进行，如果需要的话，每个计算的结果应该发送到哪里。与关系数据库系统不同，Dask在开始工作之前并不预先确定每个任务的精确运行时位置。相反，任务调度器实时评估已完成的工作、剩余的工作以及可用于接受额外工作的空闲资源。这使得Dask能够优雅地处理分布式计算中出现的各种问题，包括从工作节点故障中恢复、网络不可靠以及工作节点以不同速度完成工作。此外，任务调度器可以跟踪中间结果存储的位置，允许后续工作将数据发送到数据而不是不必要地在网络上传输数据。这在使用集群上的Dask操作时，大大提高了效率。
- en: 2.3.2 Data locality
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 数据局部性
- en: Since Dask makes it easy to scale up your code from your laptop to hundreds
    or thousands of physical servers, the task scheduler must make intelligent decisions
    about which physical machine(s) will be asked to take place in a specific piece
    of a computation. Dask uses a centralized task scheduler to orchestrate all this
    work. To do this, each Dask worker node reports what data it has available and
    how much load it’s experiencing to the task scheduler. The task scheduler constantly
    evaluates the state of the cluster to come up with fair, efficient execution plans
    for computations submitted by users. For example, if we split the example from
    section 2.1 (reading the NYC parking ticket data) between two computers (server
    A and server B), the task scheduler may state that an operation on partition 26
    should be performed by server A, and the same operation should be performed on
    partition 8 by server B. For the most part, if the task scheduler divides up the
    work as evenly as possible across machines in the cluster, the computations will
    complete as quickly and efficiently as possible.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Dask使得将代码从笔记本电脑扩展到数百或数千个物理服务器变得容易，任务调度器必须就哪些物理机器将被要求执行计算的具体部分做出明智的决策。Dask使用集中式任务调度器来协调所有这些工作。为此，每个Dask工作节点向任务调度器报告它拥有的数据和它正在承受的负载。任务调度器持续评估集群的状态，为用户提交的计算提供公平、高效的执行计划。例如，如果我们把2.1节中的示例（读取纽约市停车票数据）分配到两台计算机（服务器A和服务器B）之间，任务调度器可能会声明分区26的操作应由服务器A执行，而相同的操作应由服务器B在分区8上执行。在大多数情况下，如果任务调度器尽可能均匀地将工作分配给集群中的机器，计算将尽可能快和高效地完成。
- en: 'But that rule of thumb does not always hold true in a number of scenarios:
    one server is under heavier load than the others, has less powerful hardware than
    the others, or does not have fast access to the data. If any of those conditions
    is true, the busier/weaker server will lag behind the others, and therefore should
    be given proportionately fewer tasks to avoid becoming a bottleneck. The dynamic
    nature of the task scheduler allows it to react to these situations accordingly
    if they cannot be avoided.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个经验法则并不总是适用于多种场景：一个服务器比其他服务器负载更重，硬件不如其他服务器强大，或者无法快速访问数据。如果这些条件中的任何一个成立，那么繁忙/较弱的服务器将落后于其他服务器，因此应该分配较少的任务，以避免成为瓶颈。任务调度器的动态特性允许它在无法避免这些情况时相应地做出反应。
- en: For best performance, a Dask cluster should use a distributed filesystem like
    S3 or HDFS to back its data storage. To illustrate why this is important, consider
    the following counterexample, where a file is stored on only one machine. For
    the sake of our example, the data is stored on server A. When server A is directed
    to work on partition 26, it can read the partition directly off its hard disk.
    However, this poses a problem for server B. Before server B can work on partition
    8, server A will need to send partition 8 to server B. Any additional partitions
    that server B is to work on will also need to be sent over to server B before
    the work can begin. This will cause a considerable slowdown in the computations
    because operations involving networking (even 10 Gb fiber) are slower than direct
    reads off of locally attached disks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，Dask集群应该使用分布式文件系统如S3或HDFS来支持其数据存储。为了说明这一点的重要性，考虑以下反例，其中文件仅存储在一台机器上。为了我们的例子，数据存储在服务器A上。当服务器A被指示处理分区26时，它可以直接从其硬盘上读取该分区。然而，这对服务器B来说是个问题。在服务器B开始处理分区8之前，服务器A需要将分区8发送到服务器B。服务器B将要处理的任何其他附加分区也必须在开始工作之前发送到服务器B。这将导致计算速度显著减慢，因为涉及网络的操作（即使是10
    Gb光纤）比直接从本地连接的硬盘读取要慢。
- en: '![c02_11.eps](Images/c02_11.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![c02_11.eps](Images/c02_11.png)'
- en: '[Figure 2.11](#figureanchor2.11) Reading data from local disk is much faster
    than reading data stored remotely.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.11](#figureanchor2.11)从本地硬盘读取数据比从远程存储的数据读取要快得多。'
- en: '[Figure 2.11](#figure2.11) demonstrates this issue. If Node 1 wants to work
    on Partition 1, it would be able to do so much more quickly if it had Partition
    1 available on local disk. If this wasn’t an option, it could read the data over
    the network from Node 2, but that would be much slower.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.11](#figure2.11)展示了这个问题。如果节点1想要处理分区1，如果它能在本地硬盘上找到分区1，那么它将能够更快地完成。如果这不是一个选项，它可以从节点2通过网络读取数据，但这将会慢得多。'
- en: The remedy to this problem would be to split the file up ahead of time, store
    some partitions on server A, and store some partitions on server B. This is precisely
    what a distributed file system does. Logical files are split up between physical
    machines. Aside from other obvious benefits, like redundancy in the event one
    of the servers’ hard disks fails, distributing the data across many physical machines
    allows the workload to be spread out more evenly. It’s far faster to bring the
    computations to the data than to bring the data to the computations!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是在事先将文件分割，将一些分区存储在服务器A上，并将一些分区存储在服务器B上。这正是分布式文件系统所做的事情。逻辑文件在物理机器之间分割。除了其他明显的优点，如服务器硬盘故障时的冗余之外，将数据分布到许多物理机器上可以使工作负载更加均匀地分散。将计算带到数据那里比将数据带到计算那里要快得多！
- en: Dask’s task scheduler takes *data locality*, or the physical location of data,
    into account when considering where a computation should take place. Although
    it’s sometimes not possible for Dask to completely avoid moving data from one
    worker to another, such as instances where some data must be broadcast to all
    machines in the cluster, the task scheduler tries its hardest to minimize the
    amount of data that moves between physical servers. When datasets are smaller,
    it might not make much difference, but when datasets are very large, the effects
    of moving data around the network are much more evident. Therefore, minimizing
    data movement generally leads to more performant computations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的任务调度器在考虑计算应该在何处进行时，会考虑到*数据局部性*，即数据的物理位置。尽管有时Dask无法完全避免将数据从一个工作者移动到另一个工作者，例如某些数据必须广播到集群中的所有机器的情况，但任务调度器会尽力最小化在物理服务器之间移动的数据量。当数据集较小时，这可能不会造成太大差异，但当数据集非常大时，在网络中移动数据的影响就更加明显。因此，最小化数据移动通常会导致计算性能更优。
- en: Hopefully you now have a better understanding of the important role that DAGs
    play in enabling Dask to break up huge amounts of work into more manageable pieces.
    We’ll come back to the Delayed API in later chapters, but keep in mind that every
    piece of Dask we touch in this book is backed by Delayed objects, and you can
    visualize the backing DAG at any time. In practice, you likely won’t need to troubleshoot
    computations in such explicit detail very often, but understanding the underlying
    mechanics of Dask will help you better identify potential issues and bottlenecks
    in your workloads. In the next chapter, we’ll begin our deep dive into the DataFrame
    API in earnest.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在对DAGs在使Dask能够将大量工作分解成更易管理的小块所起的重要作用有了更好的理解。我们将在后面的章节中回到延迟API，但请记住，本书中我们接触到的每一个Dask都是基于延迟对象的，并且你可以在任何时间可视化支持DAG。在实践中，你可能不需要经常以如此明确的方式调试计算，但理解Dask的底层机制将帮助你更好地识别工作中潜在的问题和瓶颈。在下一章中，我们将开始深入探讨DataFrame
    API。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Computations on Dask DataFrames are structured by the task scheduler using DAGs.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask DataFrame上的计算是通过任务调度器使用DAGs来组织的。
- en: Computations are constructed lazily, and the `compute` method is called to execute
    the computation and retrieve the results.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算是懒加载构建的，调用`compute`方法来执行计算并检索结果。
- en: You can call the `visualize` method on any Dask object to see a visual representation
    of the underlying DAG.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在任何Dask对象上调用`visualize`方法来查看底层DAG的视觉表示。
- en: Computations can be streamlined by using the `persist` method to store and reuse
    intermediate results of complex computations.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过使用`persist`方法来存储和重用复杂计算的中间结果，从而简化计算流程。
- en: Data locality brings the computation to the data in order to minimize network
    and IO latency.*
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据局部性将计算带到数据处，以最小化网络和I/O延迟.*
