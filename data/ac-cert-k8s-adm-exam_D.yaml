- en: Appendix D. Solving the exam practice exercises
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录D. 解决考试练习题
- en: This appendix will walk you through the methods to solve the exam practice exercises,
    organized by chapter, starting with chapter 1\. This will get you in the right
    frame of mind to initiate possible solutions for tasks on the CKA exam. If you’ve
    read the book, the solutions should be apparent, but this appendix will not simply
    present you with the answers right away, with the intention being to prepare you
    for exam day.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录将按章节顺序向您介绍解决考试练习题的方法，从第一章开始。这将帮助您进入正确的思维模式，以启动CKA考试任务的可能解决方案。如果您已经阅读了本书，解决方案应该是显而易见的，但本附录不会立即向您提供答案，目的是为了帮助您为考试做准备。
- en: D.1 Chapter 1 exam exercises
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.1 第一章考试练习
- en: There are fewer exam exercises in this chapter than in the other chapters, as
    this was an introductory chapter. The exercises are more exploratory, and we’ll
    review them here.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的考试练习题比其他章节少，因为这是一章介绍性章节。练习题更具探索性，我们将在这里进行回顾。
- en: D.1.1 Listing API resources
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1.1 列出API资源
- en: 'When listing API resources from within the cluster, you should first consider
    using the `kubectl` command-line utility. This is by far the easiest method. If
    you don’t know the command off hand, you can always use the help menu by simply
    typing `kubectl`, which will provide some clues. The output of the help menu should
    look similar to this (abbreviated):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当在集群内部列出API资源时，您应该首先考虑使用`kubectl`命令行工具。这无疑是 easiest 方法。如果您手头不知道命令，您总是可以通过简单地输入`kubectl`来使用帮助菜单，这将提供一些线索。帮助菜单的输出应该类似于以下内容（已缩写）：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you look in the section of the help menu that says Other Commands, you’ll
    see that the command to “Print the supported API resources on the server” is `api-resources`.
    Therefore, the command to solve this exercise is `kubectl api-resources`.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看帮助菜单中名为“其他命令”的部分，您会看到“打印服务器上支持的API资源”的命令是`api-resources`。因此，解决这个练习的命令是`kubectl
    api-resources`。
- en: D.1.2 Listing services
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1.2 列出服务
- en: 'Listing the services in your cluster on a Linux operating system was covered
    in section 1.7 and is different from the Kubernetes Services that we talk about
    in chapter 7\. Whenever you see the terms *Linux*, *daemon*, or *system service*,
    you should think about the system components on the node itself, whereas in the
    context of Kubernetes, Services are a completely different resource. To list the
    Services that are related to Kubernetes on the node, we run the command `systemctl
    list-unit-files --type service --all``,` and then we can use the grep feature
    in Linux to further search through the results of the `list-unit-files` command.
    The complete command will look similar to the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1.7节中介绍了如何在Linux操作系统上列出集群中的服务，这与我们在第7章中讨论的Kubernetes服务不同。每当您看到术语*Linux*、*守护进程*或*系统服务*时，您应该考虑节点本身上的系统组件，而在Kubernetes的上下文中，服务是完全不同的资源。要列出与Kubernetes相关的节点上的服务，我们运行命令`systemctl
    list-unit-files --type service --all`，然后我们可以使用Linux中的grep功能进一步搜索`list-unit-files`命令的结果。完整的命令将类似于以下内容：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A lot of system services exist, which is why we use the grep function to list
    only the one we need, which is kubelet. In the context of the CKA exam, kubelet
    is the only system service located on the node itself. To send the output to a
    file named `services.csv`, we can add a `> services.csv` to the existing command.
    Therefore, the complete command will be `systemctl list-unit-files --type service
    --all | grep kubelet > services.csv`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 系统服务很多，这就是我们为什么使用grep功能只列出我们需要的那个，即kubelet。在CKA考试的上下文中，kubelet是唯一位于节点本身上的系统服务。要将输出发送到名为`services.csv`的文件中，我们可以在现有命令中添加`>
    services.csv`。因此，完整的命令将是`systemctl list-unit-files --type service --all | grep
    kubelet > services.csv`。
- en: D.1.3 The status of the kubelet service
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1.3 kubelet服务的状态
- en: For this exam exercise, the same rules apply as in the previous exercise. The
    kubelet service will be the only system service that’s running on the node itself.
    When you think about doing anything with the kubelet Service, think `systemctl`.
    You saw it used in the previous exercise as well. `systemctl` is the standard
    utility tool for controlling systemd, which is the overarching system service
    that controls them all. The command to obtain the status of any systemd service
    is `systemctl status`. You can find a hint for this in the help menu, so don’t
    be concerned if you completely forget this during the exam. (Let the help menu
    be your friend with the command `systemd -h`.) So, the complete command is `systemctl
    status kubelet`. Using the same method from the last exercise, we’ll output that
    to a file named `kubelet-status.txt` with the complete command `systemctl status
    kubelet > kubelet-status.txt`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个考试练习，适用的规则与上一个练习相同。kubelet 服务将是节点本身运行的唯一系统服务。当你考虑对 kubelet 服务进行任何操作时，请考虑使用
    `systemctl`。你已经在之前的练习中看到了它的使用。`systemctl` 是控制 systemd 的标准实用工具，systemd 是控制所有系统服务的总服务。获取任何
    systemd 服务状态的命令是 `systemctl status`。你可以在帮助菜单中找到这个提示，所以如果你在考试中完全忘记了，也不要担心。（让帮助菜单成为你的朋友，使用命令
    `systemd -h`。）因此，完整的命令是 `systemctl status kubelet`。使用上一次练习中的相同方法，我们将输出到名为 `kubelet-status.txt`
    的文件中，完整的命令是 `systemctl status kubelet > kubelet-status.txt`。
- en: D.1.4 Using declarative syntax
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1.4 使用声明式语法
- en: As we learned about in section 1.8, declarative syntax helps retain a history
    of our Kubernetes configurations. As opposed to imperative, which is one command
    after another in sequence, declarative lets us define the configuration’s end
    state, and the Kubernetes controller will make it so. To create a YAML file with
    the specifications for a Pod, we can use the Vim text editor.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 1.8 节中学到的，声明式语法有助于保留我们的 Kubernetes 配置的历史记录。与命令式不同，命令式是一系列按顺序执行的命令，声明式允许我们定义配置的最终状态，Kubernetes
    控制器将使其成为现实。要创建一个包含 Pod 规范的 YAML 文件，我们可以使用 Vim 文本编辑器。
- en: 'Create and open the file in Vim with the command `vim chap1-pod.yaml`. Once
    the file is open in Vim, you can write the YAML. As an alternative, and something
    that I’ve recommended you do many times in the book to save you time on the exam,
    you can let the `kubectl` command line write the YAML for you with the command
    `k run pod --image nginx --dry-run=client -o yaml > chap1-pod.yaml`. The result
    will be the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命令 `vim chap1-pod.yaml` 创建并打开文件。一旦文件在 Vim 中打开，你就可以编写 YAML。作为替代方案，并且这是我多次在书中推荐你做的，以节省你在考试中的时间，你可以让
    `kubectl` 命令行为你编写 YAML，使用命令 `k run pod --image nginx --dry-run=client -o yaml
    > chap1-pod.yaml`。结果将是以下内容：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Whichever way you’d like to complete this exercise is fine, but just know that
    the latter is a shortcut that I recommend using on the exam.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种方式来完成这个练习都行，但要知道后者是一个我推荐在考试中使用的快捷方式。
- en: D.1.5 Listing Kubernetes services
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1.5 列出 Kubernetes 服务
- en: This is where we make the distinction between system services (as a part of
    Linux on the node) and what we call Kubernetes services. As the exercise mentioned,
    list the services created in your Kubernetes cluster, where “created in your Kubernetes
    cluster” are the keywords that lead you down the path of using the `kubectl` tool,
    as opposed to the `systemctl` tool.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们区分系统服务（作为节点上 Linux 的一部分）和我们所说的 Kubernetes 服务的地方。正如练习中提到的，列出你在 Kubernetes
    集群中创建的服务，其中“在 Kubernetes 集群中创建”是引导你使用 `kubectl` 工具而不是 `systemctl` 工具的关键词。
- en: 'To list the Services, we can use the `kubectl` help menu to find the correct
    command. As with many other operations that would list a Kubernetes resource,
    think about this like a `GET` request to the API. We’re listing what’s in the
    API, and we’re using the `kubectl` tool to do so. The way we list everything in
    our cluster is by tacking on the `--all-namespaces` or `-A` for short. So the
    complete command is `kubectl get svc -A`, or it could be another answer that is
    `kubectl get services --all-namespaces`. The output will look similar to the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出服务，我们可以使用 `kubectl` 帮助菜单来找到正确的命令。就像许多其他会列出 Kubernetes 资源的操作一样，将其视为对 API 的
    `GET` 请求。我们正在列出 API 中的内容，并且使用 `kubectl` 工具来做这件事。我们列出集群中所有内容的方法是附加 `--all-namespaces`
    或简写为 `-A`。因此，完整的命令是 `kubectl get svc -A`，或者它也可能是另一个答案，即 `kubectl get services
    --all-namespaces`。输出将类似于以下内容：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: D.2 Chapter 2 exam exercises
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.2 第 2 章考试练习
- en: These are the exam exercises located at the end of chapter 2\. There are approximately
    the same amount of exercises as in the previous chapter, so we’ll go through these
    with the same intentions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习题位于第2章的末尾。练习题的数量与上一章大致相同，因此我们将以相同的目的进行练习。
- en: D.2.1 Shortening the kubectl command
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2.1 缩短kubectl命令
- en: We talked about this briefly in chapter 1, but the alias will already be set
    for you on the exam. This is an exercise that you’ll need to practice for the
    local cluster that you’ll use (and clusters that you’ll use on the job). Setting
    the alias is fairly straightforward; however, there are two ways to do it because
    performing the command `alias k=kubectl` is correct; this will reset itself once
    you log out of your current Bash session. To make this persistent, we can add
    it to your Bash profile with the command `echo "alias k=kubectl" >> ~/.bashrc`.
    This is the more permanent command, as you can log out and log back into your
    machine, and the alias will persist.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第1章中简要地讨论了这个问题，但在考试中别名已经为您设置好了。这是一个您需要为将要使用的本地集群（以及您在工作中将要使用的集群）进行练习的练习。设置别名相当直接；然而，有两种方法可以做到这一点，因为执行命令`alias
    k=kubectl`是正确的；这将在您退出当前的Bash会话时重置。为了使其持久，我们可以使用命令`echo "alias k=kubectl" >> ~/.bashrc`将其添加到您的Bash配置文件中。这是一个更持久的命令，因为您可以注销并重新登录到您的机器，别名将保持不变。
- en: D.2.2 Listing running Pods
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2.2 列出正在运行的Pod
- en: 'We’ve reached the point where we’ll be using the `kubectl` command-line tool,
    and you should become very familiar with this tool. You will use it in almost
    every task that you attempt to solve on the exam. To list the running Pods in
    the `kube-system` namespace, you should be thinking of the keyword `get`, as it’s
    the common word that comes after `kubectl` (or alias `k`) in listing most Kubernetes
    resources. In addition, this exercise asks us to show the Pod IP addresses, so
    we’re looking for a verbose output. In showing the IP addresses of Pods and nodes,
    think “output wide.” When you put this all together, the complete command is `k
    get po -n kube-system -o wide`. The output should look similar to the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达了将要使用`kubectl`命令行工具的阶段，您应该非常熟悉这个工具。您将在尝试解决考试中的几乎所有任务时使用它。要列出`kube-system`命名空间中运行的Pod，您应该想到关键字`get`，因为它是`kubectl`（或别名`k`）在列出大多数Kubernetes资源后常见的单词。此外，这个练习要求我们显示Pod
    IP地址，因此我们正在寻找详细输出。在显示Pod和节点的IP地址时，考虑“输出宽”。当您将这些全部组合起来，完整的命令是`k get po -n kube-system
    -o wide`。输出应该看起来类似于以下内容：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that you have the output, you can save it to a file named `pod-ip-output.txt`,
    with the command `k get po -n kube-system -o wide > pod-ip-output.txt`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经得到了输出结果，您可以将它保存到名为`pod-ip-output.txt`的文件中，使用命令`k get po -n kube-system
    -o wide > pod-ip-output.txt`。
- en: D.2.3 Viewing the kubelet client certificate
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2.3 查看kubelet客户端证书
- en: We learned in this chapter that every component within Kubernetes has a certificate
    authority and either a client certificate or a server certificate. This is how
    a client–server model works, with the most common example being the World Wide
    Web.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中了解到，Kubernetes中的每个组件都有一个证书授权机构以及客户端证书或服务器证书。这就是客户端-服务器模型的工作方式，最常见的一个例子就是万维网。
- en: 'When you think of certificates, you should always think of the `/etc/kubernetes`
    directory, as it stores all the certificate files and configurations. Whether
    it’s in `/etc/kubernetes/pki` or `/etc/kubernetes/pki/etcd`, you will find the
    appropriate certificate file, as they are all properly labeled. In this case,
    from the control plane node, we can change the directory to `/etc/kubernetes/pki`
    with the command `cd /etc/Kubernetes/pki`. If we then list the contents with the
    `ls` command, we’ll get an output similar to the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想到证书时，您应该总是想到`/etc/kubernetes`目录，因为它存储了所有的证书文件和配置。无论是位于`/etc/kubernetes/pki`还是`/etc/kubernetes/pki/etcd`，您都会找到适当的证书文件，因为它们都有适当的标签。在这种情况下，从控制平面节点，我们可以使用命令`cd
    /etc/Kubernetes/pki`来更改目录。如果我们使用`ls`命令列出内容，我们会得到以下类似的输出：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result, as you can decipher, is the location of the kubelet client certificate,
    which you can output to a file named `kubelet-config.txt` with the command `echo
    “/etc/kubernetes/pki” > kubelet-config.txt`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，正如您能解读的，是kubelet客户端证书的位置，您可以使用命令`echo “/etc/kubernetes/pki” > kubelet-config.txt`将其输出到名为`kubelet-config.txt`的文件中。
- en: D.2.4 Backing up etcd
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2.4 备份etcd
- en: As we know from reading chapter 2, etcd is a datastore for all the configurations
    of the cluster. It’s what keeps track of the Pods running in the `kube-system`
    namespace, and it also retains the data about the kubelet configuration. It maintains
    its own server certificate, as the API server must authenticate to it to access
    the data within. This is an important component of Kubernetes and, therefore,
    proves why we need to back it up.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从第 2 章的阅读中了解到的，etcd 是集群所有配置的数据存储。它是跟踪在 `kube-system` 命名空间中运行的 Pods 的工具，同时也保留关于
    kubelet 配置的数据。它维护自己的服务器证书，因为 API 服务器必须对其进行身份验证才能访问其内部数据。这是 Kubernetes 的一个重要组成部分，因此证明了为什么我们需要对其进行备份。
- en: To interface with the etcd datastore, we use a tool called `etcdctl`, which
    also has a help menu—in case you find yourself blanking on the exam. Type the
    command `etcdctl -h` to get a list of possible commands we can run to back up
    the etcd datastore.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要与 etcd 数据存储进行接口交互，我们使用一个名为 `etcdctl` 的工具，它也提供了一个帮助菜单——以防你在考试中遇到困难。输入命令 `etcdctl
    -h` 以获取我们可以运行的命令列表，用于备份 etcd 数据存储。
- en: NOTE You won’t have to do this on the exam, but if you’re doing this exercise
    in your lab at home (e.g., using kind Kubernetes), run the command `apt update;
    apt install -y etcd-client; export ETCDCTL_API=3` to install the `etcdctl` command-line
    tool, and set to version 3.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在考试中你不需要做这件事，但如果你在家里的实验室里做这个练习（例如，使用友好的 Kubernetes），请运行以下命令来安装 `etcdctl`
    命令行工具并将其设置为版本 3：`apt update; apt install -y etcd-client; export ETCDCTL_API=3`。
- en: When the version is set to 3, the help menu includes the command `snapshot save`.
    Furthermore, if you view the help page of `snapshot save` with the command `etcdctl
    snapshot save -h`, you’ll see the description “stores an etcd node backend snapshot
    to a given file.” Following help pages like this is, in a lot of ways, a great
    resource when you forget the command on exam day, which is why I’m going into
    detail about it here.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当版本设置为 3 时，帮助菜单包括 `snapshot save` 命令。此外，如果你使用命令 `etcdctl snapshot save -h` 查看`snapshot
    save` 的帮助页面，你会看到描述“将 etcd 节点后端快照存储到指定的文件中。”在考试当天忘记命令时，像这样的帮助页面在许多方面都是一项宝贵的资源，这就是为什么我在这里详细说明它的原因。
- en: 'As mentioned in the previous paragraph (and in chapter 2), the etcd datastore
    has its own server certificate, meaning you must authenticate to it to access
    the data within. This means we must pass the certificate authority (CA), client
    certificate, and key along with our request to back up etcd. Luckily, we already
    know that all certificates are located in the `/etc/Kubernetes/pki/etcd` directory.
    Therefore, we can point to them in their existing locations after the global options
    `--cacert`, `--cert`, and `--key` (we’re able to see the global options from the
    help page as well). The final command is `etcdctl snapshot save etcdbackup1 --cacert
    /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key
    /etc/kubernetes/pki/etcd/server.key``.` Once backed up, we can run the command
    `etcdctl snapshot status etcdbackup1 > snapshot-status.txt` to get the status
    of the backup and redirect it to a file. The result of these commands will look
    similar to the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一段（以及第 2 章）所述，etcd 数据存储有自己的服务器证书，这意味着你必须对其进行身份验证才能访问其内部数据。这意味着我们必须将证书颁发机构（CA）、客户端证书和密钥与我们的备份请求一起传递。幸运的是，我们已经知道所有证书都位于
    `/etc/Kubernetes/pki/etcd` 目录中。因此，我们可以在全局选项 `--cacert`、`--cert` 和 `--key`（我们也可以从帮助页面中看到全局选项）的现有位置引用它们。最终的命令是
    `etcdctl snapshot save etcdbackup1 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert
    /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key`。一旦备份完成，我们可以运行命令
    `etcdctl snapshot status etcdbackup1 > snapshot-status.txt` 来获取备份的状态，并将其重定向到文件。这些命令的结果将类似于以下内容：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: D.2.5 Restoring etcd
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2.5 恢复 etcd
- en: 'If you haven’t done the previous exercise, the following exercise will not
    work, as it continues from the point at which you conducted a backup. So now that
    we have the snapshot named `etcdbackup1`, we can restore using the `etcdctl` command-line
    tool, with an option to restore instead of backing up. Again, if we can run the
    command `etcdctl snapshot -h`, we’ll see the available commands are `restore`,
    `save`, and `status`. Choose the `restore` command, and run the command `etcdctl
    snapshot restore -h` to get more information. From the output of the help menu,
    it looks like we can specify a data directory. This will allow us to restore to
    a directory that etcd has already accessed. We know that the directory for the
    current etcd data directory is `/var/lib`. We know this because of the manifest
    for etcd, which is located in the `/etc/kubernetes/manifests` directory. We can
    run the command `cat /etc/kubernetes/manifests/etcd.yaml | tail -10`. The output
    will look similar to this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有完成前面的练习，接下来的练习将无法工作，因为它会从你进行备份的点继续。所以，现在我们有了名为`etcdbackup1`的快照，我们可以使用`etcdctl`命令行工具进行恢复，有一个选项是恢复而不是备份。再次，如果我们运行命令`etcdctl
    snapshot -h`，我们会看到可用的命令是`restore`、`save`和`status`。选择`restore`命令，并运行命令`etcdctl
    snapshot restore -h`以获取更多信息。从帮助菜单的输出中，看起来我们可以指定一个数据目录。这将允许我们恢复到一个etcd已经访问过的目录。我们知道当前etcd数据目录的目录是`/var/lib`。我们知道这一点是因为etcd的清单，它位于`/etc/kubernetes/manifests`目录中。我们可以运行命令`cat
    /etc/kubernetes/manifests/etcd.yaml | tail -10`。输出将看起来像这样：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This means that we can specify a similar directory, let’s say `/var/lib/etcd-restore`.
    The complete command is `etcdctl snapshot restore snapshotdb --data-dir /var/lib/etcd-restore`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以指定一个类似的目录，比如说`/var/lib/etcd-restore`。完整的命令是`etcdctl snapshot restore
    snapshotdb --data-dir /var/lib/etcd-restore`。
- en: D.2.6 Upgrading the control plane
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2.6 升级控制平面
- en: 'Whenever you see the word `upgrade` for the exam, think kubeadm. It would be
    a good habit to practice using kubeadm to upgrade multiple times before taking
    the exam. I always like using the help menu to find my way around, so again, let’s
    run the command `kubeadm -h` to see what options we have available to us for the
    upgrade. From the output, among the available options is the command `upgrade`.
    If we drill down one more level in the help pages with the command `kubeadm upgrade
    -h`, we can surmise that `plan` will check our cluster for the available version
    to choose. Let’s try it, with the command `kubeadm upgrade plan`. The output will
    be very long, but the important part will look like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你在考试中看到“升级”这个词，就想到kubeadm。在考试前多次使用kubeadm进行升级是一个好习惯。我总是喜欢使用帮助菜单来找到我的方法，所以，让我们再次运行命令`kubeadm
    -h`来查看我们可用于升级的选项。从输出中，可用的选项中包括`upgrade`命令。如果我们使用命令`kubeadm upgrade -h`在帮助页面中再深入一级，我们可以推断出`plan`将检查我们的集群以选择可用的版本。让我们试试，使用命令`kubeadm
    upgrade plan`。输出将会非常长，但重要部分看起来像这样：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If we compare the versions, we see that we can upgrade from v1.24.7 to v1.24.10\.
    The output even gave us the exact command to run, which is very convenient! Let’s
    run the command `kubeadm upgrade apply v1.24.10.`
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较版本，我们看到我们可以从v1.24.7升级到v1.24.10。输出甚至给出了要运行的精确命令，这非常方便！让我们运行命令`kubeadm upgrade
    apply v1.24.10`。
- en: NOTE You may get the error message “Specified version to upgrade ‘v1.24.10’
    is higher than the kubeadm version ‘v1.24.7.’” You can fix this message by running
    the command `apt update; apt install -y kubeadm=1.24.10-00.`
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能会收到错误信息“指定的升级版本‘v1.24.10’高于kubeadm版本‘v1.24.7’。”你可以通过运行命令`apt update; apt
    install -y kubeadm=1.24.10-00.`来修复此信息。
- en: D.3 Chapter 3 exam exercises
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.3 第3章考试练习
- en: These exercises are intertwined, so I advise you to do all of the exam exercises
    from chapter 3 at one time. This will give you the best practice for the CKA exam.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习是相互关联的，所以我建议你一次性完成第3章的所有考试练习。这将为你准备CKA考试提供最佳实践。
- en: D.3.1 Creating a Role
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3.1 创建角色
- en: 'To create a Role in Kubernetes, we use the `kubectl` command-line utility.
    Make sure to utilize the help menu, as there are usually examples that you can
    copy and paste directly into your terminal for the exam. For example, if you run
    the command `k create role -h`, you will get several examples, as in the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Kubernetes中创建角色，我们使用`kubectl`命令行工具。确保使用帮助菜单，因为通常有一些示例可以直接复制粘贴到你的终端进行考试。例如，如果你运行命令`k
    create role -h`，你会得到几个示例，如下所示：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is not a trick; you can copy and paste these into the terminal for the
    exam, and I advise you to do so. For this exercise, as it will allow the `create`
    verb for Service Accounts, we will copy the first example from the help menu and
    change it to be `kubectl create role sa-creator --verb=create --resource=sa.`
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是个玩笑；您可以将这些内容复制粘贴到考试时的终端中，我建议您这样做。对于这个练习，因为它将允许为服务账户使用 `create` 动词，我们将从帮助菜单中复制第一个示例，并将其修改为
    `kubectl create role sa-creator --verb=create --resource=sa.`。
- en: 'This will give us a new Role that will allow us to create Service Accounts.
    We can see that this Role exists with the proper permissions with the command
    `k get role sa-creator -o yaml`. The output will look similar to this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个新的角色，允许我们创建服务账户。我们可以通过命令 `k get role sa-creator -o yaml` 看到这个角色存在并且具有适当的权限。输出将类似于以下内容：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: D.3.2 Create a role binding
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3.2 创建角色绑定
- en: 'The following exercise can only be completed if you already created the `sa-creator`
    Role. If you haven’t already, complete the first exam exercise in this chapter.
    Much like creating a Role, we can use the help menu to find the right command.
    Let’s run the command `k create rolebinding -h`, and we’ll see a helpful example
    to use and modify to our liking. The examples from the output should look like
    this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习只能在您已经创建了 `sa-creator` 角色的情况下完成。如果您还没有创建，请完成本章的第一个考试练习。与创建角色类似，我们可以使用帮助菜单来找到正确的命令。让我们运行命令
    `k create rolebinding -h`，我们将看到一个有用的示例，我们可以根据需要修改它。输出中的示例应类似于以下内容：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s copy and paste that example and modify it to be `kubectl create rolebinding
    sa-creator-binding --role=sa-creator --user=sandra`. Once the role binding is
    created, you can verify the settings with the command `k get rolebinding sa-creator-binding
    -o yaml`. The output should look similar to this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们复制并粘贴这个示例，并将其修改为 `kubectl create rolebinding sa-creator-binding --role=sa-creator
    --user=sandra`。一旦创建了角色绑定，您可以使用命令 `k get rolebinding sa-creator-binding -o yaml`
    来验证设置。输出应类似于以下内容：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Again, we’ll use the help menu to give us clues as to the correct command to
    run. The output of the command `k auth can-i -h` will give you the following examples:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用帮助菜单来为我们提供正确的命令提示。命令 `k auth can-i -h` 的输出将给出以下示例：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can deduce that the command to run is `kubectl auth can-i create sa --as
    sandra`. The result of running that command will look as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推断出要运行的命令是 `kubectl auth can-i create sa --as sandra`。运行该命令的结果将类似于以下内容：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: D.3.3 Creating a new user
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3.3 创建新用户
- en: 'From reading chapter 3, you get the idea that a user is just a construct, not
    an actual user in a user database. This means that even though we create the user
    Sandra, we are simply creating a certificate, where the common name is Sandra.
    Kubernetes does not have this concept of users but can integrate with other identity
    providers. For the exam, you’ll have to know how to generate this certificate,
    so I advise you to perform this exercise multiple times, as there are no straightforward
    answers in the Kubernetes documentation (which you can have open during the exam).
    Let’s start by using the `openssl` command-line tool, which will be available
    to you (installed) for the exam and comes with all Linux systems, including the
    one that you’re using for your practice lab. Let’s generate a private key using
    2048-bit encryption with the command `openssl genrsa -out sandra.key 2048.` The
    output will look like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读第3章，您应该明白用户只是一个构造，而不是用户数据库中的实际用户。这意味着尽管我们创建了用户Sandra，我们只是在创建一个证书，其中通用名称是Sandra。Kubernetes没有用户这一概念，但可以与其他身份提供者集成。对于考试，您需要知道如何生成这个证书，所以我建议您多次进行这个练习，因为Kubernetes文档中（您可以在考试期间打开）没有直接的答案。让我们首先使用
    `openssl` 命令行工具，这个工具将在考试中可用（已安装），并且所有Linux系统都自带，包括您用于练习实验室的系统。让我们使用命令 `openssl
    genrsa -out sandra.key 2048.` 生成一个2048位加密的私钥。输出将类似于以下内容：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now let’s make a certificate-signing request file using the private key we
    just created, which we’ll eventually give to the Kubernetes API. It’s important
    here that we specify the user Sandra in the common name of the certificate-signing
    request with the command `openssl req -new -key carol.key -subj "/CN=sandra" -out
    sandra.csr`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们刚刚创建的私钥制作一个证书签名请求文件，我们最终会将它提供给Kubernetes API。在这里，我们指定用户Sandra为证书签名请求的通用名称非常重要，使用命令
    `openssl req -new -key carol.key -subj "/CN=sandra" -out sandra.csr`：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, store the CSR file in an environment variable, as we’ll need it later.
    To do this, use the command `export REQUEST=$(cat sandra.csr | base64 -w 0)` to
    store the Base64-encoded version of the CSR file in an environment variable named
    `REQUEST`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将CSR文件存储在环境变量中，因为我们稍后会需要它。为此，使用命令`export REQUEST=$(cat sandra.csr | base64
    -w 0)`将CSR文件的Base64编码版本存储在名为`REQUEST`的环境变量中。
- en: 'Then, create the CSR resource from that request with the following multiline
    command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下多行命令从该请求创建CSR资源：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will create the resource and input the request all in one command. You
    can view the request with the command `k get csr`. The output will look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在一个命令中创建资源并输入请求。你可以使用命令`k get csr`查看请求。输出将看起来像这样：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can approve the request with the command `kubectl certificate approve Sandra,`
    and you’ll see the condition change from `pending` to `Approved,Issued`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用命令`kubectl certificate approve Sandra`批准请求，你会看到条件从`pending`变为`Approved,Issued`：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that it’s been approved, you can extract the client certificate from the
    signed certificate, Base64-decode it, and store it in a file named `sandra.crt`
    with the command `kubectl get csr sandra -o jsonpath=''{.status.certificate}''
    | base64 -d > carol.crt`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它已经被批准，你可以从已签名的证书中提取客户端证书，使用Base64解码，并使用命令`kubectl get csr sandra -o jsonpath='{.status.certificate}'
    | base64 -d > carol.crt`将其存储在名为`sandra.crt`的文件中：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: D.3.4 Adding Sandra to kubeconfig
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3.4 将Sandra添加到kubeconfig
- en: 'If you want to assume the Role of Sandra, you must add the user, along with
    the certificate (which is the important part) to the kubeconfig file (the file
    we use to run `kubectl`). To add the user Sandra to our context, we’ll run the
    command `kubectl config set-context carol --user=sandra --cluster=kind`. Once
    you run this command, you’ll notice that that the context has been added by running
    the command `kubectl config get-contexts``.` You will see an output similar to
    this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要承担Sandra的角色，你必须将用户（以及证书，这是重要部分）添加到kubeconfig文件中（我们使用该文件来运行`kubectl`）。要将用户Sandra添加到我们的上下文中，我们将运行命令`kubectl
    config set-context carol --user=sandra --cluster=kind`。一旦运行此命令，你会注意到上下文已经通过运行命令`kubectl
    config get-contexts`添加。你会看到类似以下的输出：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The star in the `current` column on the left indicates which context we are
    currently using. Therefore, to switch contexts, run the command `kubectl config
    use-context sandra`. You’ll notice the asterisk changed the current context to
    `Sandra`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧`current`列中的星号表示我们目前正在使用哪个上下文。因此，要切换上下文，请运行命令`kubectl config use-context sandra`。你会注意到星号将当前上下文更改为`Sandra`。
- en: D.3.5 Creating a new Service Account
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3.5 创建新的服务账户
- en: 'As we know from reading chapter 3, tokens of Service Accounts are mounted to
    a Pod automatically, and it takes a special configuration setting to prevent the
    mounting of that token. The first step is to view the help menu. I always start
    with the help menu because, in the heat of the moment (on the exam), you’ll sometimes
    lose your train of thought and perhaps forget what command to use or the order
    of commands and options. It happens to the best of us! Don’t worry, and remember
    that the help menu is there if you need it. It’s there for a reason. For example,
    you can run the command `k create -h` and see a list of available commands, and
    wouldn’t you know, `serviceaccount` is listed as one of the available commands.
    Let’s run the command `k create serviceaccount -h` and see what options exist
    for Service Accounts specifically. The output will give us a lot more options,
    but most notably, it will present the following example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从第3章阅读中得知，服务账户的令牌会自动挂载到Pod上，要防止该令牌的挂载需要特殊的配置设置。第一步是查看帮助菜单。我总是从帮助菜单开始，因为在紧张的时刻（考试中），你有时会失去思路，也许会忘记使用哪个命令或命令和选项的顺序。这种情况发生在我们所有人身上！别担心，记住，如果你需要，帮助菜单就在那里。它有存在的理由。例如，你可以运行命令`k
    create -h`来查看可用命令的列表，你会发现`serviceaccount`被列为可用命令之一。让我们运行命令`k create serviceaccount
    -h`来查看特定于服务账户的选项。输出将给我们提供很多选项，但最值得注意的是，它将展示以下示例：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can copy and paste this example and change it to `kubectl create serviceaccount
    secure-sa,` which will create the Service Account we need for this exercise. Then,
    to make sure the token is not exposed to the Pod, we can run the command `k get
    sa secure-sa -o yaml > secure-sa.yaml` and then the command `echo "automountServiceAccountToken:
    false" >> secure-sa.yaml` to ensure that the token doesn’t automount to a Pod.
    To apply the change, run the command `k apply -f secure-sa.yaml`, which will apply
    the configuration change to disable automounting the token for all Pods using
    this Service Account.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以复制并粘贴这个示例，并将其更改为 `kubectl create serviceaccount secure-sa,`，这将创建我们在这个练习中需要的
    Service Account。然后，为了确保令牌不会暴露给 Pod，我们可以运行命令 `k get sa secure-sa -o yaml > secure-sa.yaml`，然后运行命令
    `echo "automountServiceAccountToken: false" >> secure-sa.yaml` 来确保令牌不会自动挂载到 Pod。要应用更改，运行命令
    `k apply -f secure-sa.yaml`，这将应用配置更改以禁用为使用此 Service Account 的所有 Pod 自动挂载令牌。'
- en: D.3.6 Creating a new cluster role
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3.6 创建新的集群角色
- en: We created a Role in the first exercise of this chapter; now we’re going to
    create a cluster role in a similar way. I hope you are familiar with using the
    help menu by now to find excellent examples of `kubectl` commands for creating
    a cluster role. I’m going to take the first example from the command `k create
    clusterrole -h` and change it to be `kubectl create clusterrole acme-corp-role
    --verb=create --resource=deploy,rs,ds`. Next, we’ll create a role binding (notice
    how it didn’t say cluster role binding), which we’ll call `acme-corp-role-binding`,
    bind it to the `secure-sa` Service Account, and make sure the Service Account
    can only create Deployments, ReplicaSets, and DaemonSets in the default namespace
    (and not the `kube-system` namespace). We’ll use the example from the command
    `k create rolebinding -h` and change it to `kubectl create rolebinding acme-corp-role-binding
    --clusterrole=acme-corp-role --serviceaccount=default:secure-sa`. Then, we’ll
    check the Role with the command `kubectl -n kube-system auth can-i create deploy
    --as system:serviceaccount:default:nomount-sa`. You should get the response `no`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的第一个练习中创建了一个角色；现在我们将以类似的方式创建一个集群角色。我希望你现在已经熟悉了使用帮助菜单来查找创建集群角色的 `kubectl`
    命令的优秀示例。我将从命令 `k create clusterrole -h` 中的第一个示例开始，并将其更改为 `kubectl create clusterrole
    acme-corp-role --verb=create --resource=deploy,rs,ds`。接下来，我们将创建一个角色绑定（注意它没有提到集群角色绑定），我们将称之为
    `acme-corp-role-binding`，将其绑定到 `secure-sa` Service Account，并确保 Service Account
    只能在默认命名空间中创建 Deployments、ReplicaSets 和 DaemonSets（而不是 `kube-system` 命名空间）。我们将使用命令
    `k create rolebinding -h` 中的示例，并将其更改为 `kubectl create rolebinding acme-corp-role-binding
    --clusterrole=acme-corp-role --serviceaccount=default:secure-sa`。然后，我们将使用命令 `kubectl
    -n kube-system auth can-i create deploy --as system:serviceaccount:default:nomount-sa`
    来检查角色。你应该得到响应 `no`。
- en: D.4 Chapter 4 exam exercises
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.4 第四章考试练习
- en: Chapter 4 exam exercises revolve around scheduling Deployments or Pods in Kubernetes.
    You won’t have to deploy a StatefulSet for the exam, as it’s not listed as an
    objective for the exam criteria. When you think of scheduling, just think of creating
    a Pod—whether in a Deployment or not—and placing it on a node. Throughout this
    chapter, there are ways to control which node the Pod is placed on, and that’s
    where we’ll focus for the exercises.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章考试练习围绕在 Kubernetes 中调度 Deployments 或 Pods。你不需要为考试部署有状态集，因为它没有列为考试标准的对象。当你想到调度时，只需想到创建一个
    Pod——无论是否在 Deployment 中——并将其放置在节点上。在本章中，有方法可以控制 Pod 放置在哪个节点上，这就是我们将关注的练习内容。
- en: D.4.1 Applying a label and creating a Pod
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4.1 应用标签并创建 Pod
- en: 'This exercise simply involves applying a label to a node. If you have no idea
    where to begin, there’s a help menu for that! Let’s run the command `kubectl -h
    | grep label` to see if there’s a `label` under available commands. The output
    will look similar to this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习仅仅涉及给一个节点应用一个标签。如果你不知道从哪里开始，有一个帮助菜单可以提供帮助！让我们运行命令 `kubectl -h | grep label`
    来查看是否有 `label` 在可用的命令中。输出将类似于以下内容：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To label the node `kind-worker`, we can run the command `k label no kind-worker
    disk=ssd`. We can then show the labels on our nodes with the command `k get no
    -show-labels`. The output will look similar to the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要给节点 `kind-worker` 标签，我们可以运行命令 `k label no kind-worker disk=ssd`。然后，我们可以使用命令
    `k get no -show-labels` 来显示我们节点上的标签。输出将类似于以下内容：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can see that the label was successfully applied, and now we can create the
    YAML for a Pod with the command `k run fast --image nginx -dry-run=client -o yaml
    > fast.yaml`. Let’s open the file and change two lines that will schedule the
    Pod to that node with the label `disk=ssd`. At the very end of the file, just
    above the word `status`, in line with `restartPolicy`, we’ll add the following
    lines:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到标签已成功应用，现在我们可以使用命令`k run fast --image nginx -dry-run=client -o yaml >
    fast.yaml`创建Pod的YAML。让我们打开文件并更改两行，这将使Pod调度到带有标签`disk=ssd`的节点。在文件的最后，就在单词`status`上方，与`restartPolicy`对齐，我们将添加以下行：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Save your changes and close the file. Now that you have the YAML file, you
    can create the Pod with the command `k create -f fast.yaml`. Make sure that it
    was scheduled to the correct node with the command `k get po -o wide`. The output
    will look similar to this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 保存你的更改并关闭文件。现在你有了YAML文件，你可以使用命令`k create -f fast.yaml`创建Pod。确保它被调度到正确的节点，使用命令`k
    get po -o wide`。输出将类似于以下内容：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: D.4.2 Editing a running Pod
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4.2 编辑正在运行的Pod
- en: 'When you edit a running Pod with the `k edit po` command, you can only change
    certain fields within the YAML. This is normal behavior, and even though you can’t
    change it directly, Kubernetes will save a copy of the Pod YAML in a file in the
    `/tmp/` directory. Let’s go ahead and see what that looks like by running the
    command `k edit po fast`, which will open the Pod YAML in a Vim editor. You can
    search for `nodeSelector` by pressing the slash (/) key on the keyboard followed
    by the word `nodeSelector` (e.g., type `/nodeSelector` in Vim) and press Enter.
    The Vim text editor will first highlight the word `nodeSelector` in the annotations,
    so press the N key to go to the next result, which will be the one we’re looking
    for. You can then press the I key on the keyboard to go into insert mode. Change
    the text from `disk: ssd` to `disk: slow`. The portion of the YAML that you must
    change for this exercise will look like the following when you are done:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '当你使用`k edit po`命令编辑正在运行的Pod时，你只能更改YAML中的某些字段。这是正常行为，即使你无法直接更改，Kubernetes也会在`/tmp/`目录下的一个文件中保存Pod
    YAML的副本。让我们运行命令`k edit po fast`来查看它看起来像什么，这将打开Vim编辑器中的Pod YAML。你可以通过按键盘上的斜杠(/)键并跟随着单词`nodeSelector`（例如，在Vim中输入`/nodeSelector`）然后按Enter键来搜索`nodeSelector`。Vim文本编辑器将首先突出显示注释中的单词`nodeSelector`，因此按N键进入下一个结果，这将是我们正在寻找的结果。然后你可以按键盘上的I键进入插入模式。将文本从`disk:
    ssd`更改为`disk: slow`。当你完成这个练习时，必须更改的YAML部分将看起来像以下内容：'
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now that you’ve changed the YAML, you can save and quit the file; however,
    you’ll get a warning message indicating that the Pod updates may not change fields
    other than `spec.containers[*].image`. This is fine, as you will continue to quit
    the file. Only then will the file be saved to the `/tmp` directory. The message
    that you’ll receive when you quit the file will look similar to this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经更改了YAML文件，你可以保存并退出文件；然而，你会收到一个警告信息，指出Pod更新可能不会更改除`spec.containers[*].image`之外的字段。这没关系，因为你将继续退出文件。只有在这种情况下，文件才会被保存到`/tmp`目录。当你退出文件时，你收到的信息将类似于以下内容：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is still okay, even though we received an error. This is the part where
    we apply the YAML that was stored in the `/tmp` directory and force the Pod to
    restart. The command to do this is `k replace -f /tmp/kubectl-edit-589741394.yaml
    --force` (the name of the YAML file will be different for you). This will result
    in the currently running Pod being deleted and a new Pod (with a new name) being
    created. The output will look similar to this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 即使收到了错误，这也还是可以的。这是我们将存储在`/tmp`目录中的YAML应用的部分，并强制Pod重新启动。执行此操作的命令是`k replace -f
    /tmp/kubectl-edit-589741394.yaml --force`（YAML文件的名称将因你而异）。这将导致当前运行的Pod被删除，并创建一个新的Pod（具有新的名称）。输出将类似于以下内容：
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Once this is achieved, you can check if the Pod is running with the new config,
    using the command `k get po fast; k get po fast -o yaml | grep disk.`
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，你可以使用命令`k get po fast; k get po fast -o yaml | grep disk.`来检查Pod是否使用新的配置正在运行。
- en: '[PRE30]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: D.4.3 Using node affinity for a new Pod
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4.3 为新Pod使用节点亲和性
- en: Node affinity, as we know from reading chapter 4, is scheduling a Pod to a node
    based on certain preferences for nodes with a specific label. For example, in
    this exercise, we’re saying that the Pod should be scheduled to a node that has
    the label `disk=ssd` as its first preference. It has a backup plan, however, in
    case there are no available nodes that have the `disk=ssd` label. The backup plan
    is to schedule to nodes that have the label `Kubernetes.io/os=linux`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从第 4 章阅读中了解到的，节点亲和性是根据对具有特定标签的节点的一定偏好来调度 Pod。例如，在本练习中，我们表示 Pod 应该被调度到具有标签
    `disk=ssd` 作为首选的节点。然而，有一个备用计划，以防没有具有 `disk=ssd` 标签的可用节点。备用计划是将 Pod 调度到具有标签 `Kubernetes.io/os=linux`
    的节点。
- en: 'Let’s start by creating the YAML for a Pod with the command `k run ssd-pod
    -image nginx -dry-run=client -o yaml > ssd-pod.yaml`. Now open the file `ssd-pod.yaml`
    and insert the node affinity configuration. For the exam, here’s where I would
    utilize the Kubernetes docs, which you can have open during the exam. So, with
    the site [https://kubernetes.io/docs](https://kubernetes.io/docs) open in a web
    browser, type `node selector` in the search bar and press Enter. Select the first
    link, named Assigning Pods to Nodes, and click the link Affinity and Anti-affinity
    from the right side of the page. You will see the YAML listed on this page, which
    you can copy and paste right into your existing `ssd-pod.yaml` file. The part
    that you need to copy will be the entire affinity section below `spec:`. We’ll
    modify it slightly to look like the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用命令 `k run ssd-pod -image nginx -dry-run=client -o yaml > ssd-pod.yaml`
    创建 Pod 的 YAML 开始。现在打开文件 `ssd-pod.yaml` 并插入节点亲和性配置。对于考试，我会在 Kubernetes 文档中利用这一点，您可以在考试期间打开它。因此，在网页浏览器中打开网站
    [https://kubernetes.io/docs](https://kubernetes.io/docs)，在搜索栏中输入 `node selector`
    并按 Enter。选择第一个链接，命名为 Assigning Pods to Nodes，然后点击页面右侧的链接 Affinity and Anti-affinity。您将看到此页面上列出的
    YAML，您可以将其复制并粘贴到现有的 `ssd-pod.yaml` 文件中。您需要复制的是 `spec:` 下的整个亲和性部分。我们将对其进行轻微修改，使其看起来如下：
- en: '[PRE31]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once you have this pasted into the file `ssd-pod.yaml` you can save and quit.
    You can create the Pod with the command `k apply -f ssd-pod.yaml`. Check if the
    Pod has been successfully scheduled to the correct node with the command `k get
    po -o wide`. The output should look similar to this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将内容粘贴到文件 `ssd-pod.yaml` 中，您就可以保存并退出。您可以使用命令 `k apply -f ssd-pod.yaml` 创建 Pod。使用命令
    `k get po -o wide` 检查 Pod 是否已成功调度到正确的节点。输出应类似于以下内容：
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: D.5 Chapter 5 exam exercises
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.5 第 5 章考试练习
- en: These are different than the last set of exercises because they have more to
    do with the maintenance of currently running Deployments and Pods. These exercises
    will deal with scaling, updating images, and viewing the rollouts of a Deployment.
    This is helpful for the exam because you’ll be asked to roll out to a new version
    of an application running on Kubernetes, or you may be asked to roll back to a
    previous version.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习与上一组练习不同，因为它们更多地与当前运行的 Deployments 和 Pods 的维护有关。这些练习将涉及扩展、更新镜像和查看 Deployment
    的滚动更新。这对于考试很有帮助，因为您可能需要将应用程序部署到 Kubernetes 的新版本，或者可能需要回滚到以前的版本。
- en: D.5.1 Scaling replicas in a Deployment
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5.1 在 Deployment 中扩展副本
- en: 'Most likely for the exam, there will be a Deployment already running, but to
    simulate this in our practice lab, we’ll have to create one ourselves. The most
    important part of this exercise is the scaling operation, not so much creating
    the Deployment. Let’s start by running the command `k create deploy apache --image
    httpd:latest`. This will create the Deployment, and the Deployment will be created
    with one replica because we didn’t specify within the imperative command. You
    can verify that the Pod within the Deployment has been created with the command
    `k get deploy,po`. The output will look like this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于考试来说，很可能已经有一个 Deployment 在运行，但为了在我们的实践实验室中模拟这种情况，我们不得不自己创建一个。这个练习最重要的部分是扩展操作，而不是创建
    Deployment。让我们先运行命令 `k create deploy apache --image httpd:latest`。这将创建 Deployment，并且由于我们没有在命令中指定，Deployment
    将只有一个副本。您可以使用命令 `k get deploy,po` 验证 Deployment 内的 Pod 是否已创建。输出将类似于以下内容：
- en: '[PRE33]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that the Pod is up and running, we can scale the Deployment from one replica
    to five replicas. The command to scale the replicas is `k scale deploy apache
    -replias 5`. Once we run that command, we’ll see four more Pods have been started.
    To verify that this is happening, run the command `k get deploy,po` again. The
    output will now look like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Pod已经启动并运行，我们可以将Deployment从单个副本扩展到五个副本。扩展副本的命令是 `k scale deploy apache -replicas
    5`。一旦我们运行该命令，我们将看到另外四个Pod被启动。为了验证这是否发生，再次运行命令 `k get deploy,po`。输出现在将看起来像这样：
- en: '[PRE34]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: D.5.2 Updating the image
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5.2 更新镜像
- en: 'In the previous exercise, we created a Deployment named `apache`, and we’ll
    continue in this exercise with the same Deployment. If you haven’t yet completed
    the previous exercise, please do so before starting this one. Updating the image
    within a Deployment, as we know from reading chapter 5, causes Kubernetes to create
    a new ReplicaSet and record this action as a new rollout. We can update the image
    with the command `k set image deploy apache httpd=httpd:latest httpd=httpd:2.4.54`.
    You can verify that the Deployment contains the correct image with the command
    `k get deploy apache -o yaml | grep image`. The output will look similar to this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个练习中，我们创建了一个名为 `apache` 的Deployment，我们将在本练习中继续使用相同的Deployment。如果你还没有完成上一个练习，请在开始这个练习之前完成它。正如我们在第5章中了解到的，在Deployment中更新镜像会导致Kubernetes创建一个新的ReplicaSet，并将此操作记录为新的滚动。我们可以使用命令
    `k set image deploy apache httpd=httpd:latest httpd=httpd:2.4.54` 来更新镜像。你可以使用命令
    `k get deploy apache -o yaml | grep image` 验证Deployment是否包含正确的镜像。输出将看起来像这样：
- en: '[PRE35]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: D.5.3 Viewing ReplicaSet events
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5.3 查看ReplicaSet事件
- en: 'Every time you change the image, along with other characteristics of the Deployment,
    there’s a new ReplicaSet that is created. This is because the Pods change configuration;
    therefore, the old Pods are terminated and new Pods are created. As we know by
    reading chapter 5, this is all handled by the ReplicaSet, which is the controller
    to help with rolling out new versions of the Deployment. We can view the events
    by running the command `k describe rs apache-67984dc457`. Note that the name of
    the ReplicaSet will be different for you. The output contains a lot of information,
    but the important part is at the end, in the events section, which should look
    similar to this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你更改镜像，以及部署的其他特性，都会创建一个新的ReplicaSet。这是因为Pods的配置发生了变化；因此，旧的Pods会被终止，新的Pods会被创建。正如我们在第5章中了解到的，这一切都是由ReplicaSet处理的，它是控制器，用于帮助部署新版本的Deployment。我们可以通过运行命令
    `k describe rs apache-67984dc457` 来查看事件。请注意，ReplicaSet的名称对于你来说可能会有所不同。输出包含大量信息，但重要的是在末尾的事件部分，它应该看起来像这样：
- en: '[PRE36]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: D.5.4 Rolling back to a previous app version
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5.4 回滚到之前的应用程序版本
- en: Now that we’ve successfully rolled out to a new version (by changing the Deployment
    image), we can easily roll back to the previous version, with the previous image,
    by performing the command `k rollout undo deploy apache`. You can now see from
    running the command `k rollout status deploy apache`, followed by `k rollout history
    deploy apache,` what revision we are on.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功推出到新版本（通过更改部署镜像），我们可以通过执行命令 `k rollout undo deploy apache` 轻松回滚到上一个版本，使用之前的镜像。现在，通过运行命令
    `k rollout status deploy apache`，然后 `k rollout history deploy apache`，我们可以看到我们当前处于哪个版本。
- en: '[PRE37]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: D.5.5 Changing the rollout strategy
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5.5 更改滚动策略
- en: 'To change the rollout strategy, we have to modify the Deployment YAML. We can
    easily do this by running the command `k edit deploy apache`. This will open the
    Deployment YAML in a Vim text editor. We can go down to the line that starts with
    `strategy` and change the type to `Recreate`. The rest of the YAML under strategy
    can be deleted. The final result will look like this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改滚动策略，我们必须修改Deployment YAML。我们可以通过运行命令 `k edit deploy apache` 来轻松完成此操作。这将打开Vim文本编辑器中的Deployment
    YAML。我们可以找到以 `strategy` 开头的行，并将类型更改为 `Recreate`。策略下的其余YAML可以删除。最终结果将看起来像这样：
- en: '[PRE38]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Once you’ve changed the strategy from `RollingUpdate` to `Recreate`, you can
    save and quit the file. The Deployment will be updated automatically. No rollout
    will occur, as this will only take effect in the next rollout phase. The result
    after exiting the Deployment YAML will look like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将策略从 `RollingUpdate` 更改为 `Recreate`，你可以保存并退出文件。Deployment将自动更新。不会发生滚动，因为这只会影响下一个滚动阶段。退出Deployment
    YAML后的结果将看起来像这样：
- en: '[PRE39]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: For extra credit, you can perform the previous exercise again and see that all
    Pods are terminated before the new Pods are created, as this is the rollout strategy
    that we intended to change.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外加分，你可以再次执行之前的练习，并看到在创建新 Pods 之前，所有 Pods 都已被终止，因为这正是我们打算更改的部署策略。
- en: D.5.6 Cordoning and uncordoning a node
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5.6 隔离和解除隔离节点
- en: For this exercise, we’ll first create a three-node cluster. See appendix A on
    how to create a multinode cluster using kind Kubernetes. When the three-node cluster
    is up and running, and you’ve run the command `docker exec -it kind-control-plane`
    to gain access to the control-plane node (where `kubectl` is already installed),
    you can continue with this task.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们首先创建一个三节点集群。请参阅附录 A 了解如何使用 kind Kubernetes 创建多节点集群。当三节点集群启动并运行，并且你已经运行了命令
    `docker exec -it kind-control-plane` 以获取对控制平面节点（其中已安装 `kubectl`）的访问权限后，你可以继续这个任务。
- en: 'Cordoning a node means that we’re marking it as unschedulable. This doesn’t
    necessarily evict the Pods from the node. You’ll have to run the drain command
    to do this. To cordon a node, run the command `k cordon kind-worker`. You can
    verify that scheduling has been disabled on this node by running the command `k
    get no`. The output will look similar to the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离一个节点意味着我们将它标记为不可调度。这并不一定意味着从节点中驱逐 Pods。你必须运行 drain 命令来完成这个操作。要隔离一个节点，请运行命令
    `k cordon kind-worker`。你可以通过运行命令 `k get no` 来验证此节点上的调度已被禁用。输出将类似于以下内容：
- en: '[PRE40]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we can schedule a Pod, and it should go to the node `kind-worker2` because
    we’ve just cordoned the node `kind-worker`. To schedule a Pod, let’s run the command
    `k run nginx --image nginx`. Then, we can check that the Pod was scheduled to
    the correct node with the command `k get po -o wide`. The output should look similar
    to this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以调度一个 Pod，并且它应该被调度到节点 `kind-worker2`，因为我们刚刚隔离了节点 `kind-worker`。要调度一个 Pod，让我们运行命令
    `k run nginx --image nginx`。然后，我们可以使用命令 `k get po -o wide` 检查 Pod 是否被调度到正确的节点。输出应该类似于以下内容：
- en: '[PRE41]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we’ll uncordon the node `kind-worker` with the command `k uncordon kind-worker`.
    Now that the node is uncordoned, we can schedule Pods to it again. Go ahead and
    move the Pod from the node `kind-worker2` to the node `kind-worker` by adding
    a node selector. We’ll add the node selector in the Pod YAML using the command
    `k edit po nginx` (you may need to run the command `apt update; apt install -y
    vim` to install Vim again on your new kind cluster). This will open the YAML in
    a Vim text editor. In the file, locate the line starting with `nodeName` (line
    29). Change the line from `nodeName: kind-worker2` to `nodeName: kind-worker`.
    Save and quit the file. This will prompt you with a message indicating that the
    Pod updates may not change fields other than `spec.containers[*].image`, which
    is fine, as you will continue to quit the file. Once you quit the file, you’ll
    notice that it stored a copy of the YAML in the `/tmp` directory. You can run
    the command `k replace -f /tmp/kubectl-edit-3840075995.yaml --force` to terminate
    the old nginx Pod, which will create a new Pod and schedule it to the node `kind-worker`.
    We can verify this by running the command `k get po -o wide`.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们将使用命令 `k uncordon kind-worker` 解除节点 `kind-worker` 的隔离状态。现在节点已解除隔离，我们可以再次向其调度
    Pods。请通过添加节点选择器将 Pod 从节点 `kind-worker2` 移动到节点 `kind-worker`。我们将在 Pod YAML 中使用命令
    `k edit po nginx` 添加节点选择器（你可能需要运行命令 `apt update; apt install -y vim` 重新在你的新 kind
    集群上安装 Vim）。这将使用 Vim 文本编辑器打开 YAML 文件。在文件中找到以 `nodeName` 开头的行（第 29 行）。将此行从 `nodeName:
    kind-worker2` 更改为 `nodeName: kind-worker`。保存并退出文件。这将提示你一个消息，表明 Pod 更新可能不会更改除 `spec.containers[*].image`
    之外的字段，这是正常的，因为你将继续退出文件。一旦你退出文件，你会注意到它在 `/tmp` 目录中存储了一个 YAML 的副本。你可以运行命令 `k replace
    -f /tmp/kubectl-edit-3840075995.yaml --force` 来终止旧的 nginx Pod，这将创建一个新的 Pod 并将其调度到节点
    `kind-worker`。我们可以通过运行命令 `k get po -o wide` 来验证这一点。'
- en: '[PRE42]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We can see that, in fact, it was scheduled to the node `k``ind-worker`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，实际上它被调度到了节点 `k``ind-worker`。
- en: D.5.7 Removing a taint from a node
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5.7 从节点中移除污点
- en: The exam will most likely already have a Deployment running, but for your practice
    lab environment, you may not, so we’ll first create a Deployment to simulate this
    exam-like scenario. We can use the same cluster that we used in the previous exercise.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 考试可能已经有一个 Deployment 在运行，但为了你的实践实验室环境，你可能没有，所以我们将首先创建一个 Deployment 来模拟这种考试场景。我们可以使用之前练习中使用的相同集群。
- en: 'We’ll start by creating a Deployment named nginx, using the nginx image with
    the command `k create deploy nginx -image nginx`. Then, we’ll remove the taint
    from the control-plane node with the command `k taint no kind-control-plane node-role.kubernetes.io/control-plane:NoSchedule-`.
    Now that the taint has been removed, we can schedule Pods to it without including
    a toleration for the taint. Let’s go ahead and do this, but first, we’ll have
    to modify the Deployment YAML. We can do this with the command `k edit deploy
    nginx``,` which will open the Deployment YAML in a Vim text editor. We can add
    a node selector to the YAML by inserting the following to the Pod spec:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用命令 `k create deploy nginx -image nginx` 创建一个名为 nginx 的 Deployment，使用 nginx
    镜像。然后，我们将使用命令 `k taint no kind-control-plane node-role.kubernetes.io/control-plane:NoSchedule-`
    从控制平面节点移除污点。现在污点已经被移除，我们可以将 Pod 调度到该节点，无需包含对污点的容忍。让我们继续这样做，但首先，我们需要修改 Deployment
    YAML。我们可以使用命令 `k edit deploy nginx``,` 来打开 Vim 文本编辑器中的 Deployment YAML。我们可以在 Pod
    规范中添加一个节点选择器，通过插入以下内容：
- en: '[PRE43]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As soon as we add the node selector line, the new configuration will be applied
    automatically, as the Deployment controller will recognize the change and reschedule
    the Pod to the control plane node. We can verify that this took place by using
    the command `k get po -o wide`. The output will look similar to the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们添加了节点选择器行，新的配置将自动应用，因为 Deployment 控制器将识别到变化并重新调度 Pod 到控制平面节点。我们可以使用命令 `k
    get po -o wide` 验证这一点。输出将类似于以下内容：
- en: '[PRE44]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: D.6 Chapter 6 exam exercises
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.6 第 6 章考试练习
- en: The exam exercises in chapter 6 become slightly more complex, as we start to
    deal with DNS and communication within the cluster. These exercises go together,
    so I would advise you to start at the first and continue to the second, third,
    and so on. If you try to start in the middle of the exercises, you won’t be able
    to proceed without having done the prior exercises. We will cover all the details
    here as to how to solve these exercises, so you can be best prepared for the exam.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第 6 章的考试练习变得更加复杂，因为我们开始处理集群内部的 DNS 和通信。这些练习是相互关联的，所以我建议您从第一个开始，然后继续第二个、第三个等等。如果您试图在练习的中间开始，您将无法继续进行，除非您已经完成了前面的练习。我们将在这里详细说明如何解决这些练习，以便您为考试做好最佳准备。
- en: D.6.1 exec-ing into a Pod
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.1 将执行进入 Pod
- en: 'For the exam, you’ll have the Pod already created, but to practice in your
    own personal lab environment (e.g., kind Kubernetes), go ahead and create the
    Pod as a prerequisite. If you are continuing from the chapter 5 exercises, you
    can use the Pod named `nginx` that we created. If you are starting from scratch,
    you can run the command `k run nginx -image nginx` to create a Pod. Once you created
    the Pod, you can check that it’s up and running using a command inside the container.
    To obtain the IP address of the DNS server that the Pod uses to resolve domain
    names (that’s injected into each Pod at run time), we can run the command `k exec
    -it nginx --cat /etc/resolv.conf`. The output of the command will look like this:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于考试，您已经有了已经创建的 Pod，但为了在您自己的个人实验室环境中（例如，kind Kubernetes）进行练习，请继续创建 Pod 作为先决条件。如果您是从第
    5 章的练习继续，您可以使用我们创建的名为 `nginx` 的 Pod。如果您是从零开始，您可以使用命令 `k run nginx -image nginx`
    创建一个 Pod。一旦创建了 Pod，您可以使用容器内的命令检查它是否正在运行。为了获取 Pod 用于解析域名（在运行时注入到每个 Pod 中）的 DNS
    服务器的 IP 地址，我们可以运行命令 `k exec -it nginx --cat /etc/resolv.conf`。命令的输出将类似于以下内容：
- en: '[PRE45]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: D.6.2 Changing the DNS service
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.2 更改 DNS 服务
- en: 'To change the DNS service IP address, first change the CIDR range in the API
    server YAML configuration. We can do this by changing the YAML file located in
    `/etc/kubernetes/manifests`, which is named `kube-apiserver.yaml`. Let’s open
    it and modify the `service-cluster-ip` value in the YAML. We’ll run the command
    `vim /etc/kubernetes/manifests/kube-apiserver.yaml`, which will open the file
    in a Vim text editor, then we can change the CIDR from `10.96.0.0/6` to `100.96.0.0/6`.
    Once we’ve made that change, we can save and quit, and the changes will apply
    automatically. You may have to wait up to 5 minutes for the API server Pod to
    be recreated in the kube-system namespace. Now, locate the DNS service, which
    will always reside in the kube-system namespace. Let’s run the command `k -n kube-system
    get svc` to see the service. The output of the command will look like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改 DNS 服务 IP 地址，首先更改 API 服务器 YAML 配置中的 CIDR 范围。我们可以通过更改位于 `/etc/kubernetes/manifests`
    的 YAML 文件来完成，该文件名为 `kube-apiserver.yaml`。让我们打开它并修改 YAML 中的 `service-cluster-ip`
    值。我们将运行命令 `vim /etc/kubernetes/manifests/kube-apiserver.yaml`，这将使用 Vim 文本编辑器打开文件，然后我们可以将
    CIDR 从 `10.96.0.0/6` 更改为 `100.96.0.0/6`。一旦我们做出更改，我们可以保存并退出，更改将自动生效。你可能需要等待最多 5
    分钟，以便在 kube-system 命名空间中重新创建 API 服务器 Pod。现在，找到 DNS 服务，它始终位于 kube-system 命名空间中。让我们运行命令
    `k -n kube-system get svc` 来查看服务。命令的输出将类似于以下内容：
- en: '[PRE46]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The cluster IP is 10.96.0.10, and for this exercise, we’re going to change
    it to 100.96.0.10\. We can change the Service IP address with the command `k -n
    kube-system edit svc kube-dns`, which will open the YAML for the Service in a
    Vim text editor. Once the YAML is open, we can change the two values just below
    the spec. We’ll change `10.96.0.10` to `100.96.0.10` for the two instances of
    that IP address. Once we’ve made the changes, we can save and quit. We should
    expect to see the warning message that you may not change this value. You can
    continue to quit the file (!q), and a copy of the YAML file will be stored in
    the `/tmp` directory. The output will look similar to this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 集群 IP 是 10.96.0.10，在这个练习中，我们将将其更改为 100.96.0.10。我们可以使用命令 `k -n kube-system edit
    svc kube-dns` 来更改服务 IP 地址，这将在一个 Vim 文本编辑器中打开服务的 YAML。一旦 YAML 打开，我们可以更改位于 spec
    下方两个值。我们将把该 IP 地址的两个实例从 `10.96.0.10` 更改为 `100.96.0.10`。一旦我们做出更改，我们可以保存并退出。我们可能会看到警告信息，表明你无法更改此值。你可以继续退出文件
    (!q)，并且 YAML 文件的副本将被存储在 `/tmp` 目录中。输出将类似于以下内容：
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Let’s run a replace of this YAML, which will terminate the Service and create
    a new Service with our new IP address for DNS in its place. To do this, we’ll
    run the command `k replace -f /tmp/kubectl-edit-2356510614.yaml --force` to replace
    the instance of the Service.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个 YAML 的替换，这将终止服务并创建一个新的服务，其中包含我们为 DNS 的新 IP 地址。为此，我们将运行命令 `k replace
    -f /tmp/kubectl-edit-2356510614.yaml --force` 来替换服务的实例。
- en: D.6.3 Changing the kubelet configuration
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.3 修改 kubelet 配置
- en: 'Now that we’ve modified the kube-dns Service, we need to modify the kubelet
    configuration for the Pods to get the new DNS IP information. This can be done
    by modifying the file named `config.yaml` in the `/var/lib/kubelet/` directory.
    Let’s open the file in Vim with the command `vim /var/lib/kubelet/config.yaml`.
    Once the file is open in Vim, we can change the value for `clusterDNS` from `10.96.0.10`
    to `100.96.0.10`. Once you’ve done this, you can save and quit the file. Now that
    we’ve changed the Service configuration, we’ll have to reload the kubelet daemon
    with the commands `systemctl daemon-reload` and `systemctl restart kubelet` to
    restart the kubelet service on the node. Finally, we’ll verify that the service
    is active and running with the command `systemctl status kubelet`. The output
    will contain a lot of information, but the important part is that the service
    is active and running, which can be located at the beginning of the output here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经修改了 kube-dns 服务，我们需要修改 Pod 的 kubelet 配置以获取新的 DNS IP 信息。这可以通过修改 `/var/lib/kubelet/`
    目录下名为 `config.yaml` 的文件来完成。让我们使用命令 `vim /var/lib/kubelet/config.yaml` 打开该文件。一旦文件在
    Vim 中打开，我们可以将 `clusterDNS` 的值从 `10.96.0.10` 更改为 `100.96.0.10`。完成此操作后，你可以保存并退出文件。现在，我们已经更改了服务配置，我们需要使用命令
    `systemctl daemon-reload` 和 `systemctl restart kubelet` 重新加载 kubelet 守护进程，以在节点上重启
    kubelet 服务。最后，我们将使用命令 `systemctl status kubelet` 验证服务是否处于活动状态并正在运行。输出将包含大量信息，但重要的是服务处于活动状态并正在运行，这可以在输出的开头找到：
- en: '[PRE48]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: D.6.4 Editing the kubelet ConfigMap
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.4 编辑 kubelet ConfigMap
- en: 'To locate the ConfigMap, which, similar to the Service, is located in the kube-system
    namespace, we can run the command `k -n kube-system get cm`. The output will look
    like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要定位 ConfigMap，它类似于 Service，位于 kube-system 命名空间中，我们可以运行命令 `k -n kube-system get
    cm`。输出将类似于以下内容：
- en: '[PRE49]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The ConfigMap that we’re looking at specifically for this exercise is named
    `kubelet-config`. We can edit the ConfigMap with the command `k -n kube-system
    edit cm kubelet-config`. This will open the YAML for the ConfigMap in a Vim text
    editor. Go down to the line that starts with `clusterDNS` and change the value
    underneath from `10.96.0.10` to `100.96.0.10`. We can save and quit the file to
    have the changes applied automatically. The output will look like this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个练习中特别关注的 ConfigMap 被命名为 `kubelet-config`。我们可以使用命令 `k -n kube-system edit
    cm kubelet-config` 来编辑 ConfigMap。这将在一个 Vim 文本编辑器中打开 ConfigMap 的 YAML 文件。向下滚动到以
    `clusterDNS` 开头的行，并将下面的值从 `10.96.0.10` 更改为 `100.96.0.10`。我们可以保存并退出文件以自动应用更改。输出将类似于以下内容：
- en: '[PRE50]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now that we’ve upgraded the node, because the kubelet is a daemon that’s currently
    running on the node, we must update the node of this configuration, as well as
    reload the daemon and restart the kubelet service on the node. First, to update
    the kubelet configuration on the node, perform the command `kubeadm upgrade node
    phase kubelet-config`. The output will look like this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经升级了节点，因为 kubelet 是当前在节点上运行的守护进程，我们必须更新此配置的节点，以及重新加载守护进程并在节点上重新启动 kubelet
    服务。首先，为了更新节点上的 kubelet 配置，执行命令 `kubeadm upgrade node phase kubelet-config`。输出将类似于以下内容：
- en: '[PRE51]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Now that we’ve upgraded the kubelet configuration for the node, we can reload
    the daemon with the command `systemctl daemon-reload` and restart the service
    with the command `systemctl restart kubelet`. You will not receive an output;
    you will just return to the command prompt, so as long as there are no error messages,
    you have successfully restarted the kubelet service.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经升级了节点的 kubelet 配置，我们可以使用命令 `systemctl daemon-reload` 重新加载守护进程，并使用命令 `systemctl
    restart kubelet` 重新启动服务。您将不会收到输出；您将直接返回到命令提示符，所以只要没有错误消息，您已成功重新启动 kubelet 服务。
- en: D.6.5 Scaling the CoreDNS Deployment
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.5 缩放 CoreDNS 部署
- en: 'The DNS service in Kubernetes is running as a Deployment. To scale this Deployment,
    we’ll run the same command that we run to scale any other Deployment, which is
    `k -n kube-system scale deploy Coredns -replicas 3`. You can then check if the
    replicas scaled with the command `k -n kube-system` are deployed. The output will
    look similar to the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的 DNS 服务作为 Deployment 运行。为了缩放此 Deployment，我们将运行用于缩放任何其他 Deployment
    的相同命令，即 `k -n kube-system scale deploy Coredns -replicas 3`。然后，您可以使用命令 `k -n kube-system`
    检查副本是否已缩放。输出应类似于以下内容：
- en: '[PRE52]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: D.6.6 Verifying DNS changes from a Pod
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.6 从 Pod 验证 DNS 更改
- en: 'Now that we’ve made these changes to DNS, we’ll verify that new Pods are also
    receiving these changes as they are created. The command to create a Pod named
    `netshoot` with the netshoot image is `kubectl run netshoot --image=nicolaka/netshoot
    --command sleep --command "3600"`. We run the two commands `sleep` and `3600`
    so that the Pod will remain in a running state (for 3,600 seconds, or 60 minutes).
    We can check if the Pod is in a running state with the command `k get po`. The
    output should look similar to this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 DNS 进行了这些更改，我们将验证新创建的 Pod 是否也接收到了这些更改。创建名为 `netshoot` 的 Pod 并使用 netshoot
    镜像的命令是 `kubectl run netshoot --image=nicolaka/netshoot --command sleep --command
    "3600"`。我们运行 `sleep` 和 `3600` 这两个命令，以便 Pod 保持运行状态（3,600 秒，或 60 分钟）。我们可以使用命令 `k
    get po` 检查 Pod 是否处于运行状态。输出应类似于以下内容：
- en: '[PRE53]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The Pod is running, so now you can get a Bash shell to the container. To do
    this, perform the command `k exec -it netshoot -bash`. You will notice that your
    prompt changes, which means you have successfully entered the container within
    the Pod. The output should look like this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 正在运行，因此现在您可以通过容器获取 Bash shell。为此，执行命令 `k exec -it netshoot -bash`。您会注意到您的提示符已更改，这意味着您已成功进入
    Pod 内的容器。输出应类似于以下内容：
- en: '[PRE54]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now that you have a Bash shell open in the container, you can run the command
    `cat /etc/resolv.conf` to check that the correct DNS IP address is listed. The
    output should be similar to this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经在容器中打开了 Bash shell，您可以使用命令 `cat /etc/resolv.conf` 来检查是否列出了正确的 DNS IP 地址。输出应类似于以下内容：
- en: '[PRE55]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This means that the DNS is correctly configured; therefore, Pods are able to
    resolve DNS names using CoreDNS in the cluster. You can check that this Pod is
    able to resolve a DNS query to example.com with the command `nslookup example.com`.
    Nslookup is a DNS utility that allows you to query name servers. The output should
    look like this:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 DNS 已正确配置；因此，Pod 能够在集群中使用 CoreDNS 解析 DNS 名称。你可以使用命令 `nslookup example.com`
    检查这个 Pod 是否能够解析到 example.com 的 DNS 查询。Nslookup 是一个 DNS 工具，允许你查询名称服务器。输出应如下所示：
- en: '[PRE56]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: D.6.7 Creating a Deployment and Service
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.7 创建 Deployment 和 Service
- en: 'To create a Deployment named `hello` using the image `nginxdemos/hello:plain-text`,
    you can run the command `k create deploy hello --image nginxdemos/hello:plain-text`.
    We can then expose the Deployment with the command `k expose deploy hello --name
    hello-svc --port 80`. To verify that we’ve created the Service correctly, we’ll
    run the command `k get svc`. The output of the previous commands will look like
    the following:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用镜像 `nginxdemos/hello:plain-text` 创建名为 `hello` 的 Deployment，可以运行命令 `k create
    deploy hello --image nginxdemos/hello:plain-text`。然后，我们可以使用命令 `k expose deploy
    hello --name hello-svc --port 80` 来暴露 Deployment。为了验证我们是否正确创建了服务，我们将运行命令 `k get
    svc`。前述命令的输出将如下所示：
- en: '[PRE57]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: D.6.8 Changing the ClusterIP Service to NodePort
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.8 将 ClusterIP 服务改为 NodePort
- en: 'To change the Service type, we can run the command `k edit svc hello-svc`,
    which will open the YAML in a Vim text editor. Under the spec, we can change the
    line that starts with `type` (line 32) from `type: ClusterIP` to `type: NodePort`.
    Then, we can add `nodePort: 3000` under the list of ports (directly under `port:
    80`). The final YAML after the changes will look like the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '要更改服务类型，我们可以运行命令 `k edit svc hello-svc`，这将打开 YAML 文件在 Vim 文本编辑器中。在 spec 下，我们可以将起始行包含
    `type` 的行（第 32 行）从 `type: ClusterIP` 改为 `type: NodePort`。然后，我们可以在端口列表下添加 `nodePort:
    3000`（直接位于 `port: 80` 之下）。更改后的最终 YAML 将如下所示：'
- en: '[PRE58]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'When you save and quit the file, the changes will be applied automatically.
    We can view the Service again with the command `k get svc`. The output will look
    like the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 保存并退出文件后，更改将自动应用。我们可以使用命令 `k get svc` 再次查看服务。输出将如下所示：
- en: '[PRE59]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To access the application through the Service over the node port, we have to
    retrieve both the location of the Pod (which node it’s on) and the IP address
    of the node. We can view this with the command `k get po -o wide; k get no -o
    wide`. The output will look like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过节点端口通过服务访问应用程序，我们必须获取 Pod 的位置（它在哪个节点上）和节点的 IP 地址。我们可以使用命令 `k get po -o wide;
    k get no -o wide` 来查看这些信息。输出将如下所示：
- en: '[PRE60]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can see that the Pod resides on `kind-worker2` and the IP address is 172.18.0.2\.
    Let’s use curl now to access the application with the command `curl 172.18.0.2:30000`.
    The output will look like the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 Pod 位于 `kind-worker2` 上，IP 地址为 172.18.0.2。现在让我们使用命令 `curl 172.18.0.2:30000`
    来访问应用程序。输出将如下所示：
- en: '[PRE61]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: D.6.9 Installing Ingress controller and Ingress resource
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.9 安装 Ingress 控制器和 Ingress 资源
- en: 'Let’s install the Ingress controller, according to the instructions for this
    exercise, with the command `k apply -f https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/nginx-ingress-controller.yaml`.
    Once the Ingress controller has been installed, we can change the `hello-svc`
    Service back to a ClusterIP Service with the command `k edit svc hello-svc`. This
    will open the Service in a Vim text editor, where we can change the type back
    to `ClusterIP` from `NodePort` and then make sure to remove the line that has
    the node port number (nodeport: 30000). The final YAML will look like the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '根据本练习的说明，使用命令 `k apply -f https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/nginx-ingress-controller.yaml`
    安装 Ingress 控制器。一旦 Ingress 控制器安装完成，我们可以使用命令 `k edit svc hello-svc` 将 `hello-svc`
    服务改回 ClusterIP 服务。这将打开服务在 Vim 文本编辑器中，我们可以将类型从 `NodePort` 改回 `ClusterIP`，并确保删除包含节点端口号的行（nodeport:
    30000）。最终的 YAML 将如下所示：'
- en: '[PRE62]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now we can create an Ingress resource, which is where I would take advantage
    of having the Kubernetes documentation open during the exam. From the site [https://kubernetes.io/docs](https://kubernetes.io/docs),
    open a browser, type `ingress` in the search bar, and press Enter. Click the first
    link, which is named Ingress, and on the right side of the page, click The Ingress
    Resource. You can copy and paste directly from the page into your terminal. Let’s
    create a file named `ingress.yaml` with the command `vim ingress.yaml`, change
    the YAML slightly from what’s in the documentation, and paste it as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个Ingress资源，这是我在考试期间会利用打开Kubernetes文档的地方。从网站[https://kubernetes.io/docs](https://kubernetes.io/docs)打开浏览器，在搜索栏中输入`ingress`并按Enter键。点击第一个链接，名为Ingress，然后在页面右侧点击Ingress
    Resource。你可以直接从页面复制粘贴到你的终端。让我们使用命令`vim ingress.yaml`创建一个名为`ingress.yaml`的文件，将YAML文件稍微修改一下，并粘贴如下：
- en: '[PRE63]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once you have the YAML set, you can apply the YAML with the command `k apply
    -f ingress.yaml`. Then run the command `k get ing` to list the hello Ingress resource.
    The output will be similar to the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了YAML文件，你可以使用命令`k apply -f ingress.yaml`应用YAML。然后运行命令`k get ing`列出hello
    Ingress资源。输出将类似于以下内容：
- en: '[PRE64]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: D.6.10 Installing a container network interface (CNI)
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6.10 安装容器网络接口（CNI）
- en: To install a kind Kubernetes cluster without a CNI, see appendix C. Once you’ve
    created the cluster and run the Docker exec `-it kind-control-plane` command to
    access the control plane node (which has `kubectl` already installed), let’s continue
    with this exercise.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装一个不带CNI的kind Kubernetes集群，请参阅附录C。一旦你创建了集群并运行了Docker exec `-it kind-control-plane`命令来访问控制平面节点（该节点已安装`kubectl`），让我们继续这个练习。
- en: 'To install a bridge CNI, let’s first make sure we have wget installed. Let’s
    install it on both the control plane and the worker node with the command `apt
    update; apt install wget`. Once wget is installed, we’ll run the command `wget
    https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz`
    to download the CNI plugins. We’ll untar the file with the command `tar -xvf cni-plugins-linux-amd64-v1.1.1.tgz`
    and then run the command `mv bridge /opt/cni/bin``/` to move the file into the
    bin directory. Now we can install the Calico CNI with the command `kubectl apply
    -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml``.`
    When you run the command `k get no`, you will see the status of the nodes goes
    from `Not Ready` to `Ready`. The output will look similar to this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装一个桥接CNI，我们首先确保已经安装了wget。让我们使用命令`apt update; apt install wget`在控制平面和工作节点上安装它。一旦wget安装完成，我们将运行命令`wget
    https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz`来下载CNI插件。我们将使用命令`tar
    -xvf cni-plugins-linux-amd64-v1.1.1.tgz`解压文件，然后运行命令`mv bridge /opt/cni/bin`将文件移动到bin目录。现在我们可以使用命令`kubectl
    apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml`安装Calico
    CNI。当你运行命令`k get no`时，你会看到节点的状态从`Not Ready`变为`Ready`。输出将类似于以下内容：
- en: '[PRE65]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: D.7 Chapter 7 exam exercises
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.7 第7章考试练习
- en: The following exam exercises are based on storage and much like the previous
    chapters will need to be done sequentially. You will not be able to complete the
    second exercise before the first. Having a good understanding of persistent volumes,
    persistent volume claims, and storage classes are crucial for the exam.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习基于存储，并且与前面的章节类似，需要按顺序完成。在完成第一个练习之前，你将无法完成第二个练习。对持久卷、持久卷声明和存储类有良好的理解对于考试至关重要。
- en: D.7.1 Creating a persistent volume
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7.1 创建持久卷
- en: To create a persistent volume claim (PVC), we can refer to the Kubernetes documentation,
    which we can keep open during the exam. This will allow us to copy and paste directly
    from the page to our terminal, which will best utilize the limited time you have
    for the exam. Go to [https://kubernetes.io/docs](https://kubernetes.io/docs) and
    use the search bar on the left side of the screen to search for the term *use
    a persistent volume*. Click on the link that says Configure a Pod to Use a PersistentVolume
    for Storage | Kubernetes and scroll down to the section Create a Persistent Volume.
    The full URL of the page is [https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
    Copy and paste the YAML from this page into a new file named `pv.yaml`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个持久卷声明（PVC），我们可以参考 Kubernetes 文档，考试期间我们可以将其保持打开状态。这将允许我们直接从页面复制粘贴到我们的终端，这样可以最大限度地利用你在考试中有限的时间。访问
    [https://kubernetes.io/docs](https://kubernetes.io/docs) 并使用屏幕左侧的搜索栏搜索术语 *使用持久卷*。点击名为“配置
    Pod 使用持久卷进行存储 | Kubernetes”的链接，并滚动到“创建持久卷”部分。该页面的完整 URL 是 [https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume)。将此页面的
    YAML 复制粘贴到一个名为 `pv.yaml` 的新文件中。
- en: 'We’ll change the name of the persistent volume from `task-pv-volume` to `volstore308`
    and change the storage from `10Gi` to `22Mi`. Save the file `pv.yaml` and create
    the persistent volume with the command `k create -f pv.yaml`. If you perform the
    command `k get pv`, you will see an output similar to the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将持久卷的名称从 `task-pv-volume` 更改为 `volstore308`，并将存储从 `10Gi` 更改为 `22Mi`。保存文件 `pv.yaml`
    并使用命令 `k create -f pv.yaml` 创建持久卷。如果你执行 `k get pv` 命令，你将看到类似于以下内容的输出：
- en: '[PRE66]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: D.7.2 Creating a persistent volume claim
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7.2 创建持久卷声明
- en: 'To create a PVC, we’ll reference the same page in the documentation mentioned
    previously, so hopefully you still have it open. If not, here’s the link: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
    Copy and paste the YAML into a file named `pvc.yaml` (use the command `vim pvc.yaml`).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 PVC，我们将参考之前提到的文档中的同一页，希望你仍然保持它打开。如果没有，这里是有链接：[https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim)。将
    YAML 复制粘贴到一个名为 `pvc.yaml` 的文件中（使用命令 `vim pvc.yaml`）。
- en: 'We’ll change the name of the PVC from `task-pv-claim` to `pv-claim-vol` and
    change storage from `3Gi` to `90Mi`. Create the PVC with the command `k apply
    -f pvc.yaml`. Once the resource is created, you can view the PVC with the command
    `k get pvc`, and you can also view the persistent volume (PV) with the command
    `k get pv`. The output of the command will look like this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把 PVC 的名称从 `task-pv-claim` 更改为 `pv-claim-vol`，并将存储从 `3Gi` 更改为 `90Mi`。使用命令
    `k apply -f pvc.yaml` 创建 PVC。一旦资源创建成功，你可以使用命令 `k get pvc` 查看PVC，也可以使用命令 `k get
    pv` 查看持久卷（PV）。命令的输出将如下所示：
- en: '[PRE67]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: D.7.3 Creating a Pod to use the claim
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7.3 创建使用声明的 Pod
- en: 'To create the YAML for a Pod named `pod-access` with the `centos:7` image,
    we’ll run the command `k run pod-access --image centos:7 --dry-run -o yaml > pod-access.yaml`.
    Once the file has been saved, we can open it in the Vim text editor with the command
    `vim pod-access.yaml`. Once the file is open, we can add the section that will
    attach the volume via the claim we just created in the previous exercise. Under
    `containers:`, add `volumeMounts:` inline with the container name and image. Under
    `volumeMounts:`, add the name as `- name: vol` and `mountPath: /tmp/persistence`
    just below. To keep the container alive, we can add the sleep command just below
    with `- command:` followed by `- sleep` and `- "3600"`. The result will look like
    this:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '要创建一个名为 `pod-access` 的 Pod 的 YAML，并使用 `centos:7` 镜像，我们将运行命令 `k run pod-access
    --image centos:7 --dry-run -o yaml > pod-access.yaml`。一旦文件保存，我们可以使用命令 `vim pod-access.yaml`
    在 Vim 文本编辑器中打开它。一旦文件打开，我们可以在 `containers:` 下添加将卷通过之前练习中创建的声明附加的章节。在 `volumeMounts:`
    下，添加 `- name: vol` 和 `mountPath: /tmp/persistence`。为了保持容器活跃，我们可以在 `- command:`
    后面添加 sleep 命令，后面跟着 `- sleep` 和 `- "3600"`。结果将如下所示：'
- en: '[PRE68]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: You can create the Pod with the command `k apply -f pod-access.yaml`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用命令 `k apply -f pod-access.yaml` 创建 Pod。
- en: D.7.4 Creating a storage class
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7.4 创建存储类
- en: 'To create a storage class named `node-local`, we can refer again to the Kubernetes
    documentation. From [https://kubernetes.io/docs](https://kubernetes.io/docs),
    let’s search storage class in the search bar. Click on the first result, which
    is named Storage Classes, and click Local from the right side of the page. Copy
    and paste the YAML from the page right into your terminal. We’ll paste it exactly
    as it is and change the name to `node-local` into a new file named `sc.yaml`.
    The final result will be the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建名为 `node-local` 的存储类，我们可以再次参考 Kubernetes 文档。从 [https://kubernetes.io/docs](https://kubernetes.io/docs)，让我们在搜索栏中搜索存储类。点击页面右侧的“Local”链接的第一个结果，名为
    Storage Classes。将页面上的 YAML 直接复制粘贴到您的终端中。我们将完全按照原样粘贴，并将名称更改为 `node-local` 到一个名为
    `sc.yaml` 的新文件中。最终结果如下所示：
- en: '[PRE69]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The storage class can be created with the command `k apply -f sc.yaml`. We can
    view the storage class with the command `k get sc`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类可以使用命令 `k apply -f sc.yaml` 创建。我们可以使用命令 `k get sc` 查看存储类。
- en: D.7.5 Creating a persistent volume claim for a storage class
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7.5 为存储类创建持久卷声明
- en: 'To create a PVC for a storage class named `claim-sc`, we’ll use the previously
    created PVC and modify it. Let’s copy that file with the command `cp pvc.yaml
    pvc-sc.yaml`. We’ll change the name of the PVC to `claim-sc`, change the claim
    to `39Mi`, and change the access mode to `ReadWriteOnce`. The final YAML for this
    PVC should look like the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要为名为 `claim-sc` 的存储类创建 PVC，我们将使用之前创建的 PVC 并对其进行修改。让我们使用命令 `cp pvc.yaml pvc-sc.yaml`
    复制该文件。我们将 PVC 的名称更改为 `claim-sc`，将 claim 更改为 `39Mi`，并将访问模式更改为 `ReadWriteOnce`。此
    PVC 的最终 YAML 应如下所示：
- en: '[PRE70]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We can create the PVC with the command `k apply -f pvc-sc.yaml`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用命令 `k apply -f pvc-sc.yaml` 创建 PVC。
- en: D.7.6 Creating a Pod from a storage class
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7.6 从存储类创建 Pod
- en: 'To create a Pod named `pod-sc` with the nginx image, we can run the command
    `k run pod-sc --image nginx --dry-run=client -o yaml > pod-sc.yaml``.` Once the
    file has been saved, we can open it in the Vim text editor with the command `vim
    pod-sc.yaml`. Once the file is open, we can add the section that will attach the
    volume via the claim we just created in the previous exercise. Under `containers:`,
    add the `volumeMounts:` inline with the container name and image. Under `volumeMounts:`,
    add the name as `- name: vol` and `mountPath: /tmp/persistence` just below that.
    To keep the container alive, we can add the sleep command just below with `command:`
    followed by `- sleep` and `- "3600"`. The result will look like this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '要使用 nginx 镜像创建名为 `pod-sc` 的 Pod，我们可以运行命令 `k run pod-sc --image nginx --dry-run=client
    -o yaml > pod-sc.yaml`。一旦文件已保存，我们可以使用命令 `vim pod-sc.yaml` 在 Vim 文本编辑器中打开它。一旦文件打开，我们可以在
    `containers:` 下添加将通过之前练习中创建的 claim 挂载的卷的部分。在 `volumeMounts:` 下，添加与容器名称和镜像一致的 `volumeMounts:`。在
    `volumeMounts:` 下，添加名称 `- name: vol` 和 `mountPath: /tmp/persistence` 在其下方。为了保持容器活跃，我们可以在
    `command:` 下方添加 sleep 命令，后跟 `- sleep` 和 `- "3600"`。结果将如下所示：'
- en: '[PRE71]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: You can create the Pod with the command `k apply -f pod-sc.yaml`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用命令 `k apply -f pod-sc.yaml` 创建 Pod。
- en: D.8 Chapter 8 exam exercises
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.8 第 8 章考试练习
- en: Chapter 8 is all about troubleshooting, so a lot of these exercises will require
    you to simulate something being “broken” in the cluster. Just know that for the
    exam, these will probably be staged in a way that the component is already broken,
    whereas here in our practice lab environment, we’ll need to set up the scenario,
    which may take a few extra steps.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 第 8 章全部关于故障排除，因此这些练习中的许多都需要您在集群中模拟某些“损坏”的情况。只需知道，对于考试，这些可能已经以组件已经损坏的方式安排，而在这里我们的实践实验室环境中，我们需要设置场景，这可能需要额外的几个步骤。
- en: D.8.1 Fixing the Pod YAML
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.8.1 修复 Pod YAML
- en: 'Run the command in the exercise, which is `k run testbox --image busybox --command
    ''sleep 3600''`. When you do this, you’ll see that the status is `RunContainerError`.
    The first step in troubleshooting is to check the logs with the command `k logs
    testbox`. But the result will not return any logs, so we must go through the decision
    tree and describe the Pod. To do this, we can run the command `k describe po testbox`.
    We can see the events from the `describe` command, which look like this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 运行练习中的命令，即 `k run testbox --image busybox --command 'sleep 3600'`。当你这样做时，你会看到状态是
    `RunContainerError`。故障排除的第一步是使用命令 `k logs testbox` 检查日志。但结果不会返回任何日志，因此我们必须通过决策树并描述
    Pod。为此，我们可以运行命令 `k describe po testbox`。我们可以看到 `describe` 命令的事件，看起来如下所示：
- en: '[PRE72]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The error is “failed to create containerd task,” which means that we need to
    modify the YAML before we start the container again. We can edit the Pod with
    the command `k edit po testbox` and change the YAML at the line that starts with
    the word `command`. Instead of the term `sleep 3600`, put the `3600` on the next
    line and surround it with quotes and another dash. The final result of the YAML
    should be like this:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 错误是“无法创建 containerd 任务”，这意味着在再次启动容器之前，我们需要修改 YAML。我们可以通过命令 `k edit po testbox`
    来编辑 Pod，并更改以 `command` 开头的行。将 `sleep 3600` 替换为下一行上的 `3600`，并用引号和另一个破折号包围。YAML
    的最终结果应该是这样的：
- en: '[PRE73]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: When you try to save and quit, you’ll get an error message, but you can simply
    quit and ignore the message. You’ll see that a copy of the file has been saved
    in the `/tmp` directory. You can replace the Pod YAML with the command `k replace
    -f /tmp/kubectl-edit-8848373.yaml`. Note that the name of the YAML file will be
    different from the one here. This will create a new Pod, and you will see now
    that the Pod is running.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试保存并退出时，你会得到一个错误消息，但你只需简单地退出并忽略该消息。你会看到文件已经在 `/tmp` 目录中保存了一个副本。你可以用命令 `k
    replace -f /tmp/kubectl-edit-8848373.yaml` 来替换 Pod YAML。请注意，YAML 文件的名字将和这里的不同。这将创建一个新的
    Pod，你现在会看到 Pod 正在运行。
- en: '[PRE74]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: D.8.2 Fixing the Pod image
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.8.2 修复 Pod 镜像
- en: 'To create a new container named `busybox2` that uses the `busybox:1.35.0` image,
    run the command `k run busybox2 -image busybox:1.35.0`. You will see the following
    result by running the command `k get po`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个名为 `busybox2` 的新容器，使用 `busybox:1.35.0` 镜像，运行命令 `k run busybox2 -image busybox:1.35.0`。通过运行命令
    `k get po`，你会看到以下结果：
- en: '[PRE75]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We can follow the decision tree by first trying to obtain the logs, but there
    are no logs for this container. We’ll then describe the Pod with the command `k
    describe po busybox2`. The events in the Pod description don’t tell us much, so
    it seems to be operating correctly. We can perform a trick in one simple command
    to prevent the container from completing. Let’s run the command `k run busybox2
    --image busybox:1.35.0 -it -sh`, which will open a shell to the container. When
    we exit the shell, we can run `k get po` and see that the container is running.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过首先尝试获取日志来遵循决策树，但这个容器没有日志。然后我们可以通过命令 `k describe po busybox2` 来描述 Pod。Pod
    描述中的事件并没有告诉我们太多，所以它似乎正在正常工作。我们可以通过一个简单的命令来防止容器完成。让我们运行命令 `k run busybox2 --image
    busybox:1.35.0 -it -sh`，这将打开容器的 shell。当我们退出 shell 时，我们可以运行 `k get po` 并看到容器正在运行。
- en: D.8.3 Fixing a completed Pod
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.8.3 修复已完成的 Pod
- en: To create a new container named `curlpod2` with the `nicolaka/netshoot` image,
    we’ll run the command `k run curlpod --image nicolaka/netshoot -it --sh`. This
    will open a shell to the container within. Let’s run the command `nslookup kubernetes`
    and exit out of the shell. We see again that the Pod has completed. This is because
    it doesn’t have the commands within the container image to keep it running. We
    can add this with the command `kubectl run curlpod --image=nicolaka/netshoot --command
    sleep` `--command "3600"`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个名为 `curlpod2` 的新容器，使用 `nicolaka/netshoot` 镜像，我们将运行命令 `k run curlpod --image
    nicolaka/netshoot -it --sh`。这将打开容器内的 shell。让我们运行命令 `nslookup kubernetes` 并退出 shell。我们再次看到
    Pod 已经完成。这是因为容器镜像中没有命令来保持其运行。我们可以通过命令 `kubectl run curlpod --image=nicolaka/netshoot
    --command sleep` `--command "3600"` 来添加这个命令。
- en: D.8.4 Fixing the Kubernetes scheduler
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.8.4 修复 Kubernetes 调度器
- en: Move the file `kube-scheduler.yaml` to back it up. This is always a good idea,
    especially for the exam when you can’t simply rebuild the cluster. Let’s run the
    command `cp /etc/Kubernetes/manifests/kube-scheduler.yaml /tmp/kube-scheduler.yaml`.
    We can then modify the existing file by running the command `vim /etc/Kubernetes/manifests/kube-scheduler.yaml`.
    Once we have that open in our Vim text editor, add an extra `r` to the end of
    the word `kube-scheduler` so that it becomes `kube-schedulerr`. Save and quit
    the file.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件 `kube-scheduler.yaml` 移动以备份。这总是一个好主意，尤其是在考试时，你不能简单地重建集群。让我们运行命令 `cp /etc/Kubernetes/manifests/kube-scheduler.yaml
    /tmp/kube-scheduler.yaml`。然后我们可以通过运行命令 `vim /etc/Kubernetes/manifests/kube-scheduler.yaml`
    来修改现有的文件。一旦我们在 Vim 文本编辑器中打开它，将 `kube-scheduler` 末尾的额外 `r` 添加到单词末尾，使其变为 `kube-schedulerr`。保存并退出文件。
- en: 'Now that the scheduler configuration has been modified, let’s see if a Pod
    can be scheduled with the command `k run nginx -image nginx` and check the status
    of the Pod by running the command `k get po`. The output will look like the following:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在调度器配置已经被修改，让我们通过命令 `k run nginx -image nginx` 来看看是否可以调度一个 Pod，并通过运行命令 `k get
    po` 来检查 Pod 的状态。输出将如下所示：
- en: '[PRE76]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'This is one of those cases where we know the problem, but let’s pretend we
    don’t. Look at the events with the command `k -n kube-system describe po kube-scheduler-kind-control-plane`.
    The events will look like this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们知道问题所在，但让我们假装我们不知道的情况之一。使用命令 `k -n kube-system describe po kube-scheduler-kind-control-plane`
    查看事件。事件将如下所示：
- en: '[PRE77]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: You’ll also notice that the Pod is in a `crashloopbackoff` state. You can see
    the error “unable to start container process.”
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到 Pod 处于 `crashloopbackoff` 状态。你可以看到错误信息“无法启动容器进程。”
- en: D.8.5 Fixing the kubelet
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.8.5 修复 kubelet
- en: 'Let’s run the command `curl https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_08/10-kubeadm.conf
    --silent --output /etc/systemd/system/kubelet.service.d/10-kubeadm.conf; systemctl
    daemon-reload; systemctl restart kubelet.` This will break the cluster in a way
    that affects the kubelet. Let’s check the status of the kubelet service with the
    command `systemctl status kubelet`. We notice that the service is inactive and
    see that the process has an error. Let’s check the file in `/etc/systemd/system/kubelet.service.d`,
    which is named `10-kubeadm.conf`. We’ll notice that the file has an incorrect
    `bin` directory, which is `/usr/local/bin` as opposed to `usr/bin`. Let’s change
    the file to `/usr/bin` and see if that fixes it. Open the file with the command
    `vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf`. Once we have the
    file open, we can change the contents as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行命令 `curl https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_08/10-kubeadm.conf
    --silent --output /etc/systemd/system/kubelet.service.d/10-kubeadm.conf; systemctl
    daemon-reload; systemctl restart kubelet.` 这将以一种影响 kubelet 的方式破坏集群。让我们使用命令 `systemctl
    status kubelet` 检查 kubelet 服务的状态。我们注意到服务处于非活动状态，并看到进程存在错误。让我们检查 `/etc/systemd/system/kubelet.service.d`
    目录下的文件，该文件名为 `10-kubeadm.conf`。我们会注意到文件中有一个不正确的 `bin` 目录，它应该是 `usr/bin` 而不是 `/usr/local/bin`。让我们将文件更改为
    `/usr/bin` 并看看是否可以修复问题。使用命令 `vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf`
    打开文件。一旦文件打开，我们可以按照以下方式更改内容：
- en: '[PRE78]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Once your file looks like this, save and quit the file. Then reload the daemon
    with the command `systemctl daemon-reload`, followed by restarting the kubelet
    service with the command `systemctl restart kubelet`. Check the status of the
    kubelet service with the command `systemctl status kubelet`, and the service should
    be active and running now.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的文件看起来像这样时，保存并退出文件。然后使用命令 `systemctl daemon-reload` 重新加载守护进程，随后使用命令 `systemctl
    restart kubelet` 重新启动 kubelet 服务。使用命令 `systemctl status kubelet` 检查 kubelet 服务的状态，现在服务应该处于活动状态并正在运行。
