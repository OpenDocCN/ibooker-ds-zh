- en: 5 Running applications in Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 在Kubernetes中运行应用程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Scaling applications for high availability
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为高可用性扩展应用程序
- en: Performing rolling updates and rollbacks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行滚动更新和回滚
- en: Exposing Deployments to create Services
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过暴露部署来创建服务
- en: Performing maintenance tasks on a Kubernetes cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes集群上执行维护任务
- en: This chapter covers the operations side of Kubernetes as we walk through how
    to maintain applications that are already running in Kubernetes. The previous
    chapter covered how to create Deployments (including templating), scheduling attributes,
    ConfigMaps, and Secrets. This chapter covers common approaches you will take on
    the exam to complete tasks such as providing additional resources for an application,
    scaling applications, providing a consistent Endpoint for applications, and rolling
    out new versions of an application. In continuation of the workloads and scheduling
    exam curriculum, this chapter, combined with the previous chapter, will cover
    15% of the exam and is a part of accomplishing high availability and self-healing
    for applications running in Kubernetes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了Kubernetes的操作方面，我们通过介绍如何维护已在Kubernetes中运行的应用程序来介绍。上一章介绍了如何创建Deployment（包括模板化）、调度属性、ConfigMaps和Secrets。本章涵盖了你在考试中完成诸如为应用程序提供额外资源、扩展应用程序、为应用程序提供一致的端点以及推出应用程序新版本等任务时将采用的一些常见方法。在继续工作负载和调度考试课程的基础上，本章与上一章一起将涵盖考试的15%，并且是完成在Kubernetes中运行的应用程序的高可用性和自我修复的一部分。
- en: The workloads and scheduling domain
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载和调度领域
- en: This chapter covers part of the workloads and scheduling domain of the CKA curriculum.
    This domain addresses how we run applications on Kubernetes. It encompasses the
    following competencies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了CKA课程的工作负载和调度领域的一部分。该领域涉及我们在Kubernetes上运行应用程序的方式。它包括以下能力。
- en: '| Competency | Chapter section |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 能力 | 章节部分 |'
- en: '| --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Know how to scale applications. | 5.1 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 了解如何扩展应用程序。 | 5.1 |'
- en: '| Understand Deployments and how to perform rolling updates and rollbacks.
    | 5.1 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 理解Deployment以及如何执行滚动更新和回滚。 | 5.1 |'
- en: '| Understand the primitives used to create robust, self-healing application
    Deployments. | 5.2 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 理解用于创建健壮、自我修复的应用程序Deployment的原语。 | 5.2 |'
- en: 5.1 Orchestrating applications
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 应用程序编排
- en: A Deployment is a core object in Kubernetes by which you achieve high availability,
    robustness, and self-healing. Stateless applications—as the name implies—do not
    contain state (data), so the Pods (and containers within) can be replaced and
    respawned without affecting the overall health of the application. Stateless applications
    in Kubernetes are commonly managed via Deployments. In contrast, the StatefulSet
    object in Kubernetes cannot be as easily replaced or respawned, as the data is
    required for the stateful application to run. Think MySQL where data is being
    written to a primary database table and replicated to additional read-only database
    instances. You will not be tested on StatefulSets on the exam, so we will focus
    solely on Deployments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment是Kubernetes中的一个核心对象，通过它你可以实现高可用性、健壮性和自我修复。无状态应用程序——正如其名所示——不包含状态（数据），因此Pod（及其内部的容器）可以被替换和重新启动，而不会影响应用程序的整体健康。在Kubernetes中，无状态应用程序通常通过Deployment进行管理。相比之下，Kubernetes中的StatefulSet对象不能像那样轻松地替换或重新启动，因为状态化应用程序的运行需要数据。例如MySQL，数据被写入主数据库表并复制到额外的只读数据库实例。考试中不会测试StatefulSets，所以我们只关注Deployment。
- en: 5.1.1 Modifying running applications
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 修改运行中的应用程序
- en: So far in this book, we’ve paid special attention to Pods, which is great, but
    a lot of times just one Pod for your application is not sufficient. In Kubernetes,
    there’s a way to create redundancy and fault tolerance with the creation of a
    Deployment. The CKA exam will test you on modifying existing Deployments. This
    includes changing the image and changing the number of Pod replicas. A task on
    the CKA exam may look something like the following.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们特别关注Pod，这很好，但很多时候，仅有一个Pod对于你的应用程序是不够的。在Kubernetes中，有一种通过创建Deployment来创建冗余和容错的方法。CKA考试将测试你修改现有Deployment的能力。这包括更改镜像和更改Pod副本的数量。CKA考试中的一个任务可能看起来像以下这样。
- en: '| Exam Task Create a Deployment named `apache` that uses the image `httpd:2.4.54`
    and contains three Pod replicas. After the Deployment has been created, scale
    the Deployment to five replicas and change the image to `httpd:alpine`. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 创建一个名为 `apache` 的部署，使用镜像 `httpd:2.4.54` 并包含三个 Pod 副本。部署创建后，将部署扩展到五个副本，并将镜像更改为
    `httpd:alpine`。|'
- en: If you don’t already have access to an existing Kubernetes cluster, creating
    a Kubernetes cluster with kind is explained in appendix A. As soon as your kind
    cluster is built, use the `kubectl` tool preinstalled on the control plane node.
    You can get a Bash shell to the control plane node by typing the command `docker
    exec -it kind-control-plane bash` and following along.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有访问现有的 Kubernetes 集群，附录 A 中解释了如何使用 kind 创建 Kubernetes 集群。一旦你的 kind 集群构建完成，请使用预安装在控制平面节点上的
    `kubectl` 工具。你可以通过输入命令 `docker exec -it kind-control-plane bash` 并按照说明来获取控制平面节点的
    Bash shell。
- en: 'In a stateless app in Kubernetes, creating replicas of the same application
    is not a problem. This is because all microservices data is decoupled from the
    container itself; therefore, the requests get the same response no matter if they
    are going to the first replica of the application or the fifteenth. Let’s go ahead
    and create a Deployment named `apache` with the command `k create deploy apache
    --image httpd:2.4.54` `--replicas 3`. Now that we’ve created the Deployment, we
    can check on the status with the command `k get deploy`, and we’ll see that we
    have three replicas. Because we want to increase the number of replicas to five,
    you would “scale the Deployment” by running the command `k scale deploy apache
    --replicas 5`, which will scale the Deployment from one Pod to two Pods:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 的无状态应用中，创建相同应用的副本不是问题。这是因为所有微服务数据都与容器本身解耦；因此，无论请求是前往应用的第一个副本还是第十五个副本，都会得到相同的响应。让我们继续创建一个名为
    `apache` 的部署，使用命令 `k create deploy apache --image httpd:2.4.54` `--replicas 3`。现在我们已经创建了部署，我们可以使用命令
    `k get deploy` 检查状态，我们会看到有三个副本。因为我们想将副本数量增加到五个，所以你需要运行命令 `k scale deploy apache
    --replicas 5` 来“扩展部署”，这将把部署从一个 Pod 扩展到两个 Pod：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will notice that the Pods are prefixed with the same name as the Deployment,
    followed by a suffix that is a hash value, the last five being unique because
    there cannot be duplicate Pod names in a cluster.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到 Pod 前缀与部署的名称相同，后面跟着一个哈希值后缀，最后五个是唯一的，因为在集群中不允许有重复的 Pod 名称。
- en: 'Now that we have a new Deployment running, let’s change the image for the Deployment
    from `httpd:2.4.54` to `httpd:alpine`. We can update the image by running the
    command `k set image deploy apache httpd=httpd:alpine`; and by performing the
    command `k get po`, you’ll see that a whole new set of Pods have been created
    for the Deployment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个新的部署正在运行，让我们将部署的镜像从 `httpd:2.4.54` 更改为 `httpd:alpine`。我们可以通过运行命令 `k set
    image deploy apache httpd=httpd:alpine` 来更新镜像；通过执行命令 `k get po`，你会看到为部署创建了一整套全新的
    Pod：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Modifying the Deployment in this way means that there’s no downtime for the
    application while the new version of the Deployment (with the new image) is rolled
    out. It also creates flexibility and increased agility to service your end users,
    accounting for spikes in load and being able to incrementally change the application
    and add features with little downtime. A Deployment resource is the most common
    in Kubernetes, as it takes full advantage of the native ecosystem, and we’ll focus
    on it more during the coming sections of this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式修改部署意味着在新的部署版本（带有新镜像）推出时，应用没有停机时间。这也为服务最终用户提供了灵活性和更高的敏捷性，能够应对负载峰值，并能够以最小停机时间逐步更改应用和添加功能。Deployment
    资源是 Kubernetes 中最常用的，因为它充分利用了原生生态系统，我们将在本章接下来的部分中更多地关注它。
- en: 5.1.2 Application maintenance
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 应用维护
- en: 'Deployments are designed to run multiple instances of the same application,
    replicated to provide increased availability for the end users. An abstraction
    of a Deployment that provides additional capabilities is a ReplicaSet. As the
    name implies, a ReplicaSet’s purpose is to maintain a stable set of replica Pods
    running at any given time. This guarantees the availability of a specified number
    of identical Pods. List the ReplicaSets in your cluster with the command `k get
    rs`. You should get an output similar to the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment 是设计用来运行同一应用程序的多个实例，通过复制提供对最终用户的更高可用性。提供额外功能的 Deployment 抽象是 ReplicaSet。正如其名所示，ReplicaSet
    的目的是在任何给定时间维护一组稳定的副本 Pod。这保证了指定数量的相同 Pod 的可用性。使用命令 `k get rs` 列出你的集群中的 ReplicaSets。你应该得到以下类似的输出：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In figure 5.1, you will see the Deployment and ReplicaSet depicted with three
    replicated Pods running NGINX, as a part of both the Deployment and ReplicaSet
    combination of Kubernetes objects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 5.1 中，你会看到 Deployment 和 ReplicaSet 的表示，它们由运行 NGINX 的三个复制 Pod 组成，作为 Kubernetes
    对象组合的一部分。
- en: '![](../../OEBPS/Images/05-01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1](../../OEBPS/Images/05-01.png)'
- en: Figure 5.1 A ReplicaSet inside of a Deployment with three replicas, indicating
    the naming scheme for Kubernetes objects
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 部署内的一个副本集，包含三个副本，显示了 Kubernetes 对象的命名方案
- en: Speaking of the Deployment configuration YAML, let’s take a look at how the
    Pod YAML is different from a Deployment YAML. It’s actually easier than you’d
    think. You basically take the Pod YAML and put it in the section under `template:`,
    which is under `spec:` in the Deployment YAML. To prove this, let’s perform two
    different commands—one to create a YAML file for a Pod and one to create a YAML
    file for a Deployment. The command to create a Pod YAML file is `k run nginx --image
    nginx --dry-run=client -o yaml > pod.yaml`, and the command to create a Deployment
    is `k create deploy nginx --image nginx --dry-run=client -o yaml > deploy.yaml`.
    You can see the similarities between the `pod.yaml` file and the `deploy.yaml`
    file in figure 5.2.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 说到 Deployment 配置 YAML，让我们看看 Pod YAML 与 Deployment YAML 的区别。实际上比你想象的要简单。你基本上是将
    Pod YAML 放在 Deployment YAML 中 `spec:` 下 `template:` 部分的区域。为了证明这一点，让我们执行两个不同的命令——一个用于创建
    Pod YAML 文件，另一个用于创建 Deployment YAML 文件。创建 Pod YAML 文件的命令是 `k run nginx --image
    nginx --dry-run=client -o yaml > pod.yaml`，创建 Deployment 的命令是 `k create deploy
    nginx --image nginx --dry-run=client -o yaml > deploy.yaml`。你可以在图 5.2 中看到 `pod.yaml`
    文件和 `deploy.yaml` 文件之间的相似性。
- en: '![](../../OEBPS/Images/05-02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2](../../OEBPS/Images/05-02.png)'
- en: Figure 5.2 A comparison between the YAML for a Pod and the YAML for a Deployment
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 Pod 的 YAML 与 Deployment 的 YAML 的比较
- en: The Deployment spec, created via YAML, is exactly like the Pod spec, with the
    additional Deployment spec as a parent resource above it. So there’s a spec for
    the Deployment, and within the spec for the Deployment, there’s the spec for the
    Pod. Pretty easy, right?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 YAML 创建的 Deployment 规范与 Pod 规范完全相同，只是在它上面还有一个额外的 Deployment 规范作为父资源。因此，有一个
    Deployment 规范，而在 Deployment 规范内部，还有 Pod 规范。这很简单，对吧？
- en: 'We can use the Deployment that we created in the exercise at the beginning
    of this chapter, or we can generate the Deployment from the file we just created
    with the command `k create -f deploy.yaml`. We can view the Deployment status
    with the command `k get deploy` and the number of replicas in the `READY` column.
    We will see five replicas for the Deployment named `apache`, and because we didn’t
    specify any replicas in the `kubectl create` command, we only have one for the
    Deployment named `nginx`. You can list the ReplicaSets again with the command
    `k get rs`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用本章开头练习中创建的 Deployment，或者使用命令 `k create -f deploy.yaml` 从我们刚刚创建的文件生成 Deployment。我们可以使用命令
    `k get deploy` 查看Deployment 状态，并在 `READY` 列中查看副本数量。我们将看到名为 `apache` 的 Deployment
    有五个副本，因为我们没有在 `kubectl create` 命令中指定任何副本，所以名为 `nginx` 的 Deployment 只有一个副本。你可以再次使用命令
    `k get rs` 列出 ReplicaSets：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You will notice from the output that two ReplicaSets start with `apache` because
    earlier when we changed the image, the Deployment controller created a new ReplicaSet,
    as it contains a different configuration. By default, all of the Deployment’s
    rollout history is kept in the system so that you can roll back any time you want.
    We will review rollouts and rollbacks later in this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到输出中有两个 ReplicaSets 以 `apache` 开头，因为我们之前更改了镜像，Deployment 控制器创建了一个新的 ReplicaSet，因为它包含不同的配置。默认情况下，所有
    Deployment 的滚动历史都会保留在系统中，这样你就可以随时回滚。我们将在本章后面回顾滚动和回滚。
- en: 'You can scale this Deployment as we did at the beginning of this chapter by
    modifying the `deploy.yaml` file, or you can type the command `k scale deploy
    nginx --replicas 2`. When modifying the YAML, you can open the file in your text
    editor and change the line that says `replicas: 1` to `replicas: 2`, as you see
    in a snapshot of the YAML file in figure 5.3\. After you save the file, you can
    apply the same YAML file with the command `k apply -f deploy.yaml``.` You may
    get a warning that the resource is missing a “last-applied configuration,” but
    you can safely ignore the warning, as the Deployment is still scaled to two replicas.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以通过修改 `deploy.yaml` 文件，或输入命令 `k scale deploy nginx --replicas 2` 来像本章开头那样缩放此
    Deployment。在修改 YAML 时，您可以在文本编辑器中打开文件，将显示为 `replicas: 1` 的行更改为 `replicas: 2`，如图
    5.3 的 YAML 文件快照所示。保存文件后，您可以使用命令 `k apply -f deploy.yaml` 应用相同的 YAML 文件。您可能会收到资源缺少“last-applied
    configuration”的警告，但您可以安全地忽略此警告，因为 Deployment 仍然缩放到两个副本。'
- en: The last-applied configuration is added as an annotation to an object upon creation.
    The annotation contains the contents of the object configuration file that was
    used to create the object. The warning indicates that this annotation is missing;
    therefore, the new configuration file will be merged with the current one.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后应用的配置在创建对象时作为注释添加到对象中。注释包含用于创建对象的配置文件的内容。警告表明此注释缺失；因此，新配置文件将与当前配置合并。
- en: We use `apply` here, as opposed to `create`, because a Deployment already exists.
    The `apply` command overwrites the existing object, or, if the resource doesn’t
    exist, it will create it. The `create` command will only create the object if
    one doesn’t already exist but will return an error if the object exists.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用 `apply` 而不是 `create`，因为 Deployment 已经存在。`apply` 命令会覆盖现有对象，或者如果资源不存在，则会创建它。`create`
    命令只有在对象不存在时才会创建对象，如果对象已存在，则会返回错误。
- en: '![](../../OEBPS/Images/05-03.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/05-03.png)'
- en: Figure 5.3 Scale the Deployment by changing the replica count in the Deployment
    YAML.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 通过更改 Deployment YAML 中的副本数量来缩放 Deployment。
- en: The Deployment was scaled from one Pod to two Pods, and you can view the replica
    change with the command `k get deploy nginx -o yaml`. Behind the scenes, the ReplicaSet
    that’s associated with that Deployment was also modified. We can see this in action
    by looking at the events inside of the replica set by running the command `k describe
    rs`; you’ll see an output similar to figure 5.4.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment 从一个 Pod 缩放到两个 Pod，您可以使用命令 `k get deploy nginx -o yaml` 查看副本的变化。在幕后，与该
    Deployment 关联的 ReplicaSet 也被修改了。我们可以通过运行命令 `k describe rs` 来查看副本集中的事件；您会看到一个类似于图
    5.4 的输出。
- en: '![](../../OEBPS/Images/05-04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/05-04.png)'
- en: Figure 5.4 Describe the ReplicaSet to get more information about the changes
    in its configuration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 描述 ReplicaSet 以获取有关其配置更改的更多信息。
- en: 'The ReplicaSet is created along with the Deployment. If we delete the Deployment,
    we can see that the ReplicaSet is deleted as well. Delete the Deployment with
    the command `k delete deploy nginx`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 与 Deployment 同时创建。如果我们删除 Deployment，我们会看到 ReplicaSet 也会被删除。使用命令 `k
    delete deploy nginx` 删除 Deployment：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: EXAM TIP If you want to describe everything (i.e., all ReplicaSets) all at once,
    instead of listing the names one by one in the command line, you can just type
    the name of the resource (e.g., `k describe rs`), and the output will include
    all resources that currently exist.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 考试提示：如果您想一次性描述所有内容（即所有 ReplicaSet），而不是在命令行中逐个列出名称，您只需输入资源的名称（例如，`k describe
    rs`），输出将包括当前存在的所有资源。
- en: So, we’ve seen what happens to a ReplicaSet if we modify the Deployment replica
    count. If we modify the Deployment image, it creates a brand-new ReplicaSet, as
    we saw earlier. The ReplicaSet contained a new set of Pods, with new names and
    a new configuration, as depicted in figure 5.5.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经看到了如果我们修改 Deployment 副本数量时 ReplicaSet 会发生什么。如果我们修改 Deployment 图像，它将创建一个新的
    ReplicaSet，就像我们之前看到的那样。ReplicaSet 包含了一组新的 Pod，具有新的名称和新的配置，如图 5.5 所示。
- en: '![](../../OEBPS/Images/05-05.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/05-05.png)'
- en: Figure 5.5 When the Deployment image is changed, a new ReplicaSet is created
    with a new set of Pod replicas.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 当 Deployment 图像更改时，会创建一个新的 ReplicaSet，并带有新的 Pod 副本集。
- en: If you’re fast enough, you can see the old Pods being terminated and the new
    Pods being created. You can run the command `k get po` no later than 3 seconds
    after running the `set image` command. After you do so, you’ll see an output similar
    to figure 5.6\. You can also run the command `k get po -w` to watch the Pods change
    status in real time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你足够快，你可以看到旧 Pod 正在终止，新 Pod 正在创建。你可以在运行 `set image` 命令后不超过 3 秒内运行 `k get po`
    命令。这样做后，你会看到一个类似于图 5.6 的输出。你也可以运行命令 `k get po -w` 来实时观察 Pod 状态的变化。
- en: '![](../../OEBPS/Images/05-06.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/05-06.png)'
- en: Figure 5.6 Changing the image for Pods in the Deployment results in a new ReplicaSet.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 更改 Deployment 中的 Pod 的镜像会导致新的 ReplicaSet。
- en: You’ll find that there’s a lot of interesting information in the output of the
    `kubectl get po` command, including that the name of the Pod actually comes from
    the name of the ReplicaSet. Also, the Pods from the old ReplicaSet won’t terminate
    until the new ReplicaSet Pods are all up and running. This is called a *rollout
    strategy*, which ensures that the application can still operate, even in an upgrade
    scenario. This prevents downtime of the app and ensures that no connections are
    severed if someone is trying to access the application from outside of the cluster,
    as shown in figure 5.7.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现 `kubectl get po` 命令的输出中有很多有趣的信息，包括 Pod 的名称实际上来自 ReplicaSet 的名称。此外，旧 ReplicaSet
    的 Pod 不会终止，直到新 ReplicaSet 的所有 Pod 都启动并运行。这被称为**滚动更新策略**，确保应用程序即使在升级场景中也能继续运行。这防止了应用程序的停机，并确保如果有人试图从集群外部访问应用程序，不会断开任何连接，如图
    5.7 所示。
- en: '![](../../OEBPS/Images/05-07.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/05-07.png)'
- en: Figure 5.7 Rollout strategy in Kubernetes that states only a certain number
    of replicated Pods can be unavailable when rolling out a new version
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 Kubernetes 中的滚动更新策略，表示在部署新版本时，只有一定数量的副本 Pod 不可用。
- en: This is the benefit of stateless apps that contain multiple replicas of the
    same application. A rollout strategy is defined in the Deployment spec, which
    you can locate by running the command `k edit deploy apache`. You’ll see an output
    similar to figure 5.8\. If no strategy is defined when the Deployment is created,
    the default will be applied, which is called a *rolling update*. A rolling update
    is what we just described, where the Pods from an old ReplicaSet don’t terminate
    until the new ReplicaSet Pods are ready. The other type of strategy is called
    a *recreate* strategy. A recreate strategy will terminate the old ReplicaSet Pods
    before the new ReplicaSet Pods are running. This strategy requires a period of
    downtime for the application.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是包含相同应用程序多个副本的无状态应用程序的好处。滚动更新策略在 Deployment 规范中定义，你可以通过运行命令 `k edit deploy
    apache` 来定位它。你会看到一个类似于图 5.8 的输出。如果在创建 Deployment 时没有定义策略，将应用默认策略，这被称为**滚动更新**。滚动更新就是我们刚才描述的，即旧
    ReplicaSet 的 Pod 不会终止，直到新 ReplicaSet 的 Pod 准备就绪。另一种策略类型称为**重新创建**策略。重新创建策略会在新
    ReplicaSet 的 Pod 运行之前终止旧 ReplicaSet 的 Pod。这种策略需要应用程序有一段时间的停机。
- en: '![](../../OEBPS/Images/05-08.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/05-08.png)'
- en: Figure 5.8 The rollout strategy is set to `rollingUpdate` with a max surge of
    25% and a max unavailable of 25%.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 滚动更新策略设置为 `rollingUpdate`，最大激增为 25%，最大不可用为 25%。
- en: If you wanted to change the strategy to recreate, you would change the type
    from `rollingUpdate` to `Recreate` and delete the lines that start with `rollingUpdate`,
    `maxSurge`, and `maxUnavailable`. A recreate strategy is good for many Deployments
    because it offers the fastest rollout strategy, but as we discussed, there will
    be downtime.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将策略更改为重新创建，你需要将类型从 `rollingUpdate` 改为 `Recreate`，并删除以 `rollingUpdate`、`maxSurge`
    和 `maxUnavailable` 开头的行。重新创建策略对许多 Deployment 来说是个好选择，因为它提供了最快的滚动更新策略，但正如我们讨论的那样，会有停机时间。
- en: EXAM TIP If you find yourself wondering what values go where in a YAML file
    during the exam, you can use `k explain`. For example, the command `k explain
    deploy.spec.strategy` will list values that are available for input in the `spec`
    field. Try it out!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**考试技巧**：如果在考试中发现自己不知道 YAML 文件中的值应该放在哪里，可以使用 `k explain` 命令。例如，命令 `k explain
    deploy.spec.strategy` 会列出在 `spec` 字段中可输入的值。试试看吧！'
- en: In a rolling update rollout strategy, the max surge and max unavailable fields
    are optional and can be a percentage or a whole number. The max surge specifies
    the maximum number of Pods that can be created over the amount set by the replicas
    in the Deployment. For example, in figure 5.8, because the replica count is three,
    and the `maxSurge` is 25%, the Pods cannot surge above four Pods (percentages
    round up for a surge). The max unavailable is just as it sounds—the number of
    Pods that can be unavailable for the Deployment. In our example, since `maxUnavailable`
    is set to 25%, no more than one Pod can ever be unavailable at once.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在滚动更新发布策略中，`max surge`和`max unavailable`字段是可选的，可以是百分比或整数。`max surge`指定可以创建的Pod数量，超过部署中副本的数量。例如，在图5.8中，因为副本数量是三个，`maxSurge`是25%，Pod的数量不能超过四个（百分比向上取整）。`max
    unavailable`正如其名——可以不可用的Pod数量。在我们的例子中，由于`maxUnavailable`设置为25%，一次最多只能有一个Pod不可用。
- en: Wait, there’s more! In addition to a ReplicaSet, there’s a tracking capability
    for Deployments called *rollouts*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，还有更多！除了ReplicaSet之外，还有针对Deployments的跟踪能力，称为*滚动发布*。
- en: 5.1.3 Application rollouts
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 应用程序滚动发布
- en: Application rollouts are a way for a Kubernetes administrator or developer to
    roll back to a previous version, or record the number of revisions that the Deployment
    has, as depicted in figure 5.9.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序滚动发布是Kubernetes管理员或开发者回滚到之前版本或记录部署的修订次数的一种方式，如图5.9所示。
- en: '![](../../OEBPS/Images/05-09.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/05-09.png)'
- en: Figure 5.9 Two rollout revisions remain, in case you need to roll back to a
    previous version of the application.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 显示了两个滚动发布修订版，以防你需要回滚到应用程序的早期版本。
- en: 'Every time Pods are changed within a Deployment, a new revision is created.
    We can see the rollout history by typing the command `k rollout history deploy
    apache`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每当在部署内部更改Pod时，都会创建一个新的修订版。我们可以通过输入命令`k rollout history deploy apache`来查看滚动发布的历史：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this case, there are two revisions, one for the initial Deployment (when
    we created the Deployment with the `k create deploy` command), and a second when
    we changed the image tag from `alpine` to `2`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有两个修订版，一个是在我们使用`k create deploy`命令创建部署时的初始部署，另一个是在我们将镜像标签从`alpine`更改为`2`时。
- en: 'If you take a look at the `CHANGE-CAUSE` column, you can see it says `none`.
    The `change-cause` column is an opportunity to record a note along with your Deployment
    change. So, let’s say you wanted to add a note in the `change-cause` column that
    says “updated image tag from `alpine` to 2.” You can apply this to revision 2
    by typing the command `k annotate deploy apache kubernetes.io/change-cause="updated
    image tag from alpine to 2"`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看`CHANGE-CAUSE`列，你会看到它写着`none`。`change-cause`列是一个记录与你的部署更改相关的笔记的机会。所以，假设你想要在`change-cause`列中添加一条笔记，内容为“更新镜像标签从`alpine`到2。”你可以通过输入命令`k
    annotate deploy apache kubernetes.io/change-cause="updated image tag from alpine
    to 2"`来应用这个更改到修订版2：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the `change-cause` column for revision 2 has been changed. Now
    you can tell why the Deployment was revised and what changes occurred from revision
    1 to revision 2.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，修订版2的`change-cause`列已经被更改。现在你可以知道为什么部署被修订，以及从修订版1到修订版2发生了哪些变化。
- en: 'Having a revision history like this allows you to revert to a previous revision,
    which is called a *rollback*. Let’s say when you deployed the new version of your
    application and changed the image tag for your Deployment, you introduced a bug
    in your application. You must quickly revert to prevent users of your application
    from experiencing that bug. You can roll back to the previous revision with the
    command `k rollout undo deploy apache`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有这样的修订历史记录允许你回滚到之前的修订版，这被称为*回滚*。假设当你部署了应用程序的新版本并更改了部署的镜像标签时，你在应用程序中引入了一个错误。你必须迅速回滚以防止应用程序的用户遇到这个错误。你可以通过命令`k
    rollout undo deploy apache`回滚到之前的修订版：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You’ve reverted to the previous revision, which in turn creates a new revision.
    The revision number will never go backward; it will always go forward. You can
    now enter a message into the `change-cause` column for revision 3 with the command
    `k annotate deploy apache kubernetes.io/change-cause="reverted back to the original"`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经回滚到了上一个修订版，这反过来又创建了一个新的修订版。修订版号永远不会倒退；它总是向前推进。你现在可以通过命令`k annotate deploy
    apache kubernetes.io/change-cause="reverted back to the original"`在修订版3的`change-cause`列中输入一条消息：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Both of the revisions are recorded with notes. You can’t see revision 1 any
    longer because reverting to revision 1 would be the same as reverting to revision
    3, so to save duplication, revision 1 is removed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 两个修订版都记录了备注。你再也看不到修订版1了，因为回滚到修订版1将与回滚到修订版3相同，所以为了节省重复，修订版1被删除。
- en: 'Let’s say that you wanted to check the status of the rollout. Maybe the rollout
    didn’t go well. This can happen if the image tag is not available. Maybe you typed
    it incorrectly, or the image tag is no longer available from your image registry.
    You can check the status of your rollout based on the revision with the command
    `k rollout status deploy apache --revision 3`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想检查发布的状态。也许发布并不顺利。这可能发生在镜像标签不可用的情况下。也许你输入错误，或者镜像标签已不再从你的镜像仓库中可用。你可以根据修订版使用命令
    `k rollout status deploy apache --revision 3` 来检查你的发布状态：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our Deployment was successfully rolled out when we reverted to revision 1, so
    that’s good. No misspellings or unavailable images for us. Phew!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们回滚到修订版1时，我们的部署成功发布，这是好的。我们没有拼写错误或不可用的镜像。太好了！
- en: 'Lastly, I wanted to show you how to pause the rollout, so that only some of
    the Pods in the Deployment are on revision 3, and half of the Pods are on revision
    4\. We can see this in action with the commands `k set image deploy apache httpd=httpd:2.4`
    and `k rollout pause deploy apache` (note: you have to perform these two commands
    within one second of each other). When you get the status of the rollout, you’ll
    see a message that says “Waiting for Deployment ‘apache’ rollout to finish: 2
    out of 3 new replicas have been updated . . . ,” which means that the rollout
    is paused.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想展示如何暂停发布，以便只有部署中的一些Pod在修订版3上，而一半的Pod在修订版4上。我们可以通过命令 `k set image deploy
    apache httpd=httpd:2.4` 和 `k rollout pause deploy apache`（注意：你必须在这两个命令之间的一秒内执行）来看到这个动作。当你获取发布状态时，你会看到一个消息说“等待部署‘apache’发布完成：3个新副本中有2个已被更新……”，这意味着发布已暂停。
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are now two out of three new replicas running as a part of the new revision.
    Let’s go ahead and resume (unpause) the Deployment, so that the revision can finish,
    with the command `k rollout resume deploy apache`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有三分之二的新副本正在作为新修订版的一部分运行。让我们继续使用命令 `k rollout resume deploy apache` 恢复（取消暂停）部署，以便修订版可以完成：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we have successfully rolled out to revision 4, where the new image for the
    Pods in the Deployment is set to `httpd:2.4`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已成功发布到修订版4，其中部署中Pod的新镜像设置为`httpd:2.4`。
- en: EXAM TIP If you’re unsure of the order of the command or need examples of common
    ones, utilize the help menu in the terminal with the command `k rollout --help`
    and `k rollout status -help`, and you can easily obtain the correct command syntax.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考试提示：如果你不确定命令的顺序或需要常见命令的示例，请使用终端中的帮助菜单，使用命令 `k rollout --help` 和 `k rollout
    status --help`，你可以轻松地获得正确的命令语法。
- en: 5.1.4 Exposing Deployments
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 暴露部署
- en: Exposing a Deployment means that the Deployment can be accessed from the end
    user, potentially from outside the cluster (depending on the type of Service you
    create). We are going to go into depth about Services in chapter 6, but I wanted
    to mention that there’s an easy way to create a Service that is connected to a
    Deployment. Instead of creating a Service from scratch, you can use the command
    `k expose deploy nginx --name nginx-svc --port 80 --type ClusterIP --dry-run=client
    -o yaml > nginx-svc.yaml`. You’ll see a snapshot of the YAML that this command
    creates in figure 5.10\. Of course, because we like to do things declaratively
    in Kubernetes, we’ll do a dry run and create a file named `nginx-svc.yaml`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 暴露部署意味着部署可以被终端用户访问，可能是在集群外部（取决于你创建的服务类型）。我们将在第6章深入探讨服务，但我想提到有一个简单的方法来创建一个与部署相连的服务。你不必从头创建服务，可以使用命令
    `k expose deploy nginx --name nginx-svc --port 80 --type ClusterIP --dry-run=client
    -o yaml > nginx-svc.yaml`。你将在图5.10中看到该命令创建的YAML快照。当然，因为我们喜欢在Kubernetes中以声明式的方式做事，所以我们将进行dry
    run并创建一个名为`nginx-svc.yaml`的文件。
- en: '![](../../OEBPS/Images/05-10.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10](../../OEBPS/Images/05-10.png)'
- en: Figure 5.10 Selecting a Deployment via a Service in Kubernetes
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 通过服务在Kubernetes中选择部署
- en: The important part to notice about the Deployment is the selector. If you remember,
    we talked about node selectors and label selectors in the last chapter, and here
    we’re talking about *Service selectors* to associate a Service to the Pods within
    a Deployment. This particular Service will associate itself with any Deployment
    that has the label `app=nginx`. This is useful information to know as we proceed
    through this book and will come in handy as foundational knowledge for the next
    chapter, where we talk about Services in depth.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Deployment 的一个重要部分是选择器。如果你还记得，我们在上一章中讨论了节点选择器和标签选择器，而在这里我们讨论的是 *服务选择器*，用于将服务关联到
    Deployment 内的 Pods。这个特定的服务将与任何具有标签 `app=nginx` 的 Deployment 关联。这是我们继续阅读本书时需要了解的有用信息，并且它将作为下一章深入讨论服务的知识基础。
- en: Exam exercises
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 考试练习
- en: Using `kubectl`, create a Deployment named `apache` using the image `httpd:latest`
    with one replica. After the Deployment is running, scale the replicas up to five.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl`，创建一个名为 `apache` 的 Deployment，使用镜像 `httpd:latest` 并设置一个副本。当 Deployment
    运行后，将副本数扩展到五个。
- en: Update the image for the Deployment `apache` from `httpd:latest` to `httpd:2.4.54`.
    Do not create a new YAML file or edit the existing resource (only use `kubectl`).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Deployment `apache` 的镜像从 `httpd:latest` 更新为 `httpd:2.4.54`。不要创建新的 YAML 文件或编辑现有资源（仅使用
    `kubectl`）。
- en: Using `kubectl`, view the events of the ReplicaSet that was created as a result
    of the image change from the previous exercise.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl`，查看由于上一练习中的镜像更改而创建的 ReplicaSet 的事件。
- en: Using `kubectl`, roll back to the previous version of the Deployment named `apache`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl`，回滚到名为 `apache` 的 Deployment 的上一个版本。
- en: For the existing Deployment named `apache`, change the rollout strategy for
    a Deployment to `Recreate`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于现有的名为 `apache` 的 Deployment，将 Deployment 的回滚策略更改为 `Recreate`。
- en: 5.2 Application maintenance
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 应用程序维护
- en: In the life cycle of your application running in Kubernetes, there may come
    a time when the underlying nodes require maintenance, or the application requires
    additional resources to optimize performance. This highlights the robustness of
    Deployments, as they can move Pods around the cluster from node to node, facing
    no downtime. With redundancy and high availability built into the Kubernetes ecosystem,
    we’ll learn how to safely perform maintenance to create robust application Deployments.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的应用程序在 Kubernetes 中运行的生命周期中，可能会有这样的时刻：底层节点需要维护，或者应用程序需要额外的资源来优化性能。这突显了 Deployment
    的健壮性，因为它们可以在集群中从节点到节点移动 Pods，而无需停机。由于冗余和高可用性内置在 Kubernetes 生态系统中，我们将学习如何安全地进行维护以创建健壮的应用程序
    Deployment。
- en: For the exam, you will need to know how to move a Pod to a different node. An
    exam task will look something like the following.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于考试，你需要知道如何将 Pod 移动到不同的节点。考试任务可能如下所示。
- en: '| Exam Task The node named `kind-worker` on the cluster `b8s` is experiencing
    problems with leaking memory. You must take down the node for maintenance by first
    disabling the scheduling of new Pods to the node `kind-worker`. Then, evict all
    Pods that are currently running on `kind-worker`. Finally, once you’ve verified
    that no Pods are running on `kind-worker`, enable scheduling once again. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 考试任务 群集 `b8s` 中的节点 `kind-worker` 正在经历内存泄漏问题。你必须首先禁用对节点 `kind-worker` 的新 Pods
    调度，然后关闭节点进行维护。然后，驱逐当前在 `kind-worker` 上运行的所有 Pods。最后，一旦你确认 `kind-worker` 上没有正在运行的
    Pods，再次启用调度。|'
- en: For this exam task, you’ll have to use a two-node kind cluster, and you’ll need
    to offload a Pod to a secondary node, so you’ll need at least one running Pod
    in the cluster as well. If you do not have access to a two-node cluster, I recommend
    you create one as explained in appendix A. Once you’ve opened a shell to the control
    plane node using the command `docker exec -it kind-control-plane bash`, you can
    run the command `k create deploy nginx -image nginx` to get a Pod running in the
    cluster. Now you can begin the exam task by disabling scheduling for the `kind-worker`
    node.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个考试任务，你将需要使用一个两节点 kind 集群，并且你需要将一个 Pod 转移到辅助节点，因此你至少需要在集群中有一个正在运行的 Pod。如果你无法访问两节点集群，我建议你按照附录
    A 中的说明创建一个。一旦你使用命令 `docker exec -it kind-control-plane bash` 打开控制平面节点的 shell，你可以运行命令
    `k create deploy nginx -image nginx` 以在集群中运行一个 Pod。现在你可以开始考试任务，通过禁用 `kind-worker`
    节点的调度来开始。
- en: 'To disable scheduling to a node, you can use the command `k cordon no kind-worker`.
    You will get output similar to this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用对节点的调度，你可以使用命令 `k cordon no kind-worker`。你将得到类似以下的输出：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you run the command `k get no` right after, you’ll notice that the status
    changes from `Ready` to `Ready,SchedulingDisabled`. So if you try to create a
    second Pod, the Pod will remain pending, because the scheduler has marked the
    node as unavailable. We can see this in action with the command `k create deploy
    nginx2 -image nginx`, followed by the `k get po` command to see the status. The
    output should look like this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你紧接着运行命令`k get no`，你会注意到状态从`Ready`变为`Ready,SchedulingDisabled`。所以如果你尝试创建第二个Pod，Pod将保持挂起状态，因为调度器已将该节点标记为不可用。我们可以通过命令`k
    create deploy nginx2 -image nginx`来观察这一行为，然后通过`k get po`命令查看状态。输出应该类似于以下内容：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We need to move all running Pods on that node to take it down for maintenance.
    This process is called *draining the node*, as in evicting the Pods and moving
    them to another node. To do this in our two-node kind cluster, we must first remove
    the taint applied to the control plane node that prevents Pods from being scheduled
    to it, for the Pods to move from the worker node (when we drain it) to the control
    plane node. We talked about taints in chapter 2, so this should be a familiar
    concept. Perform the command `k taint no kind-control-plane node-role.kubernetes.io/control-plane:NoSchedule-`
    to untaint the control plane node. The output will look like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将该节点上所有正在运行的Pods移动，以便进行维护。这个过程被称为*节点驱逐*，即在驱逐Pods并将它们移动到另一个节点。要在我们的双节点集群中完成此操作，我们首先必须移除应用于控制平面节点的污点，以防止Pods被调度到该节点。Pods从工作节点（在我们驱逐它时）移动到控制平面节点。我们在第2章中讨论了污点，所以这应该是一个熟悉的概念。执行命令`k
    taint no kind-control-plane node-role.kubernetes.io/control-plane:NoSchedule-`以清除控制平面节点的污点。输出将类似于以下内容：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that the control plane node is untainted, we can drain the node `kind-worker`
    with the command `k drain kind-worker --ignore-daemonsets`. The output will look
    similar to this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在控制平面节点已清除污点，我们可以使用命令`k drain kind-worker --ignore-daemonsets`来驱逐节点`kind-worker`。输出将类似于以下内容：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You’ll see that by running the command `k get po -o wide` again, the Pods are
    running on the control plane node. To complete our exam task, let’s enable scheduling
    for the node once again by running the command `k uncordon kind-worker`. The output
    will look like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，再次运行命令`k get po -o wide`后，Pods正在控制平面节点上运行。为了完成我们的考试任务，让我们再次通过运行命令`k uncordon
    kind-worker`来启用节点的调度。输出将类似于以下内容：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This completes the exam task. You should now be familiar with how to move Pods
    from one node to another using the cordon, drain, and uncordon options with `kubectl`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了考试任务。你现在应该熟悉如何使用`kubectl`的隔离、驱逐和解除隔离选项将Pods从一个节点移动到另一个节点。
- en: 5.2.1 Cordoning and draining nodes
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 隔离和驱逐节点
- en: '*Cordon* is an interesting word, isn’t it? The official meaning is to form
    a barrier around, meaning a protective barrier. In the context of Kubernetes,
    the act of cordoning takes place on a node and is marking the node unschedulable.
    If you cordon a node, this puts the node out of service and routinely ready for
    maintenance, as shown in figure 5.11\. You might be wondering what kind of maintenance.
    Well, you might need to upgrade the RAM on a node, patch a security vulnerability,
    or replace it entirely.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**隔离**是一个有趣的词，不是吗？其官方含义是围绕形成障碍，意味着一个保护性障碍。在Kubernetes的上下文中，隔离的行为发生在节点上，并标记节点为不可调度。如果你隔离一个节点，这将使节点停机并准备进行常规维护，如图5.11所示。你可能想知道是什么类型的维护。嗯，你可能需要升级节点的RAM，修补安全漏洞，或者完全更换它。'
- en: '![](../../OEBPS/Images/05-11.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-11.png)'
- en: Figure 5.11 Cordon a node to disable scheduling.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.11 隔离节点以禁用调度**。'
- en: As you see, when you perform the command `k cordon kind-worker`, you disable
    scheduling to the node; therefore, any Pods that would normally tolerate this
    node for scheduling can’t until the cordon is released.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，当你执行命令`k cordon kind-worker`时，你禁用了对该节点的调度；因此，任何通常可以容忍此节点进行调度的Pods在隔离解除之前都不能这样做。
- en: EXAM TIP After you cordon a node, make sure you uncordon it. If the node has
    scheduling disabled, you may get points deducted from your final score.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**考试提示**：在你隔离一个节点后，确保你解除隔离。如果节点已禁用调度，你可能会从最终分数中扣除一些分数。'
- en: So, that begs the question, what about existing Pods running on that node? You
    have marked the node as unschedulable, but that only applies to scheduling from
    that point onward. It does not take into account Pods that are currently running
    on that node, as shown in figure 5.12\. You may have guessed from the title of
    this section that the act of moving the Pods off of the node and scheduling them
    elsewhere is called *draining*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现有的运行在该节点上的Pods怎么办呢？您已经将该节点标记为不可调度，但这只适用于从那时起开始的调度。它不考虑如图5.12所示当前在该节点上运行的Pods。您可能已经从本节的标题中猜到，将Pod从节点移除并在其他地方调度它们的行为被称为*清空*。
- en: '![](../../OEBPS/Images/05-12.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12](../../OEBPS/Images/05-12.png)'
- en: Figure 5.12 Cordoning the node doesn’t mean it will evict a Pod from a node.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 隔离节点并不意味着它会从节点中驱逐Pod。
- en: The act of draining is just how it sounds—draining the node of all Pods that
    are currently running on it. If the Pod(s) are managed by a ReplicaSet, the same
    rules of scheduling still apply, and for that matter, the Pod doesn’t move. It
    actually is deleted and recreated on another node; it just happens in a way that
    ensures the old Pod is not removed before the new Pod is running, much like a
    rollout. If you don’t already have a Deployment running, you can start one with
    the command `k create deploy nginx --image nginx`, and you too can follow along
    in the fun.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 清空的行为正如其名——清空节点上所有当前运行的Pods。如果Pods被ReplicaSet管理，调度规则仍然适用，并且实际上Pod并没有移动。它实际上是在另一个节点上被删除并重新创建的；只是确保在新的Pod运行之前，旧的Pod不会被移除，就像滚动更新一样。如果您还没有运行Deployment，可以使用命令`k
    create deploy nginx --image nginx`来启动一个，您也可以跟随这个有趣的步骤。
- en: In the spirit of fun, let’s see what happens to all the Pods that are currently
    running on that node when it’s drained. To drain the node and ignore the kube-proxy
    and kubenet DaemonSets that are running on every node in the cluster, perform
    the command `k drain kind-worker --ignore-daemonsets --force`. Force is necessary
    in this case, because one of the Pods is not managed by a ReplicaSet. You will
    see an output similar to figure 5.13.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在娱乐的精神下，让我们看看当节点被清空时，当前在该节点上运行的所有Pods会发生什么。为了清空节点并忽略在集群中每个节点上运行的kube-proxy和kubenet
    DaemonSets，执行命令`k drain kind-worker --ignore-daemonsets --force`。在这种情况下，强制执行是必要的，因为其中一个Pod没有被ReplicaSet管理。您将看到类似于图5.13的输出。
- en: '![](../../OEBPS/Images/05-13.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13](../../OEBPS/Images/05-13.png)'
- en: Figure 5.13 Draining the node removes all Pods running on the specified node,
    where some are deleted permanently.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 清空节点移除了在指定节点上运行的所有Pods，其中一些被永久删除。
- en: As you can see, a couple of actions occur. First, the node is drained, and the
    Pods are moved off, but one of the Pods is deleted and does not return. This is
    because it wasn’t managed by a ReplicaSet, so there’s no mechanism to reschedule
    that Pod. Second, the Pod that was a part of a ReplicaSet is in a pending state.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，发生了几个动作。首先，节点被清空，Pods被移除，但其中一个Pod被删除并且没有返回。这是因为它没有被ReplicaSet管理，所以没有机制来重新调度该Pod。其次，属于ReplicaSet的Pod处于挂起状态。
- en: 'As we learned in the previous chapter, the control plane nodes have a taint
    applied to them that requires a toleration to successfully schedule Pods to them.
    In this case, because the Pod doesn’t have a toleration for that taint, it will
    remain in the pending state until either a taint is applied, a toleration is added
    to the Pod, or a new node without a taint is added. For simplicity’s sake, let’s
    go ahead and remove the taint with the command `k taint no kind-control-plane
    node-role.kubernetes.io/master-`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章所学，控制平面节点被应用了一个污点，这要求必须有一个容忍才能成功调度Pod到它们。在这种情况下，因为Pod没有对该污点的容忍，它将保持挂起状态，直到应用污点、为Pod添加容忍或者添加一个没有污点的新节点。为了简化起见，让我们使用命令`k
    taint no kind-control-plane node-role.kubernetes.io/master-`来移除污点：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By running the command `k get po -o wide`, the container is now running on the
    `kind-control-plane` node. Later in this chapter we’ll try joining a node to a
    cluster, where we can simulate another scenario in which the Pod may move out
    of a pending state without potentially putting the control plane node at risk
    (by limited resources). For now, you can uncordon the `kind-worker` node with
    the command `k uncordon kind-worker`. Additionally, you could also reapply the
    taint to the control plane node with the command `k taint no kind-control-plane
    node-role.kubernetes.io/master :NoSchedule.`
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行命令`k get po -o wide`，容器现在正在`kind-control-plane`节点上运行。在本章的后面部分，我们将尝试将节点加入集群，我们可以模拟另一个场景，其中Pod可能在没有将控制平面节点置于风险（通过限制资源）的情况下从挂起状态移动出来。现在，你可以使用命令`k
    uncordon kind-worker`取消对`kind-worker`节点的隔离。此外，你也可以使用命令`k taint no kind-control-plane
    node-role.kubernetes.io/master :NoSchedule`重新应用污点到控制平面节点。
- en: 5.2.2 Adding application resources (nodes)
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 添加应用程序资源（节点）
- en: Cases may occur within the lifetime of your Kubernetes cluster in which you
    need to insert additional nodes—whether you need more resources for your application,
    or you’ve lost a node due to failure or planned downtime. No matter the reason,
    adding a node to an existing kubeadm cluster is simplified by enabling bootstrap
    token authentication in the Kubernetes API. An exam question might read something
    like the following.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的Kubernetes集群的生命周期中可能会发生需要插入额外节点的情况——无论是你需要为你的应用程序提供更多资源，还是由于故障或计划内的停机而丢失了一个节点。无论原因如何，通过在Kubernetes
    API中启用引导令牌身份验证，将节点添加到现有的kubeadm集群都变得简单。一个考试问题可能如下所示。
- en: '| Exam Task There’s a third node in the cluster named `ik8s`, but it is not
    appearing when you perform the `kubectl get nodes` command. The name of the node
    is `node02`. Allow `node02` to join the cluster by recreating the `join` command
    and ensuring that the node is in a `Ready` state when you list all the nodes in
    the cluster. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 考试任务 | 集群中有一个名为`ik8s`的第三个节点，但在你执行`kubectl get nodes`命令时，该节点没有出现。节点的名称是`node02`。通过重新创建`join`命令并确保在列出集群中的所有节点时节点处于`Ready`状态，允许`node02`加入集群。
    |'
- en: When you first create a Kubernetes cluster, kubeadm creates an initial bootstrap
    token with a 24-hour TTL, but you can create additional tokens on demand. You
    can see the bootstrap token mechanism enabled in our kind Kubernetes cluster by
    opening the file `/etc/kubernetes/manifests/kube-apiserver.yaml` and looking under
    `command`. You’ll see an ouput similar to figure 5.14.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当你首次创建Kubernetes集群时，kubeadm会创建一个具有24小时TTL的初始引导令牌，但你可以在需要时创建额外的令牌。你可以通过打开文件`/etc/kubernetes/manifests/kube-apiserver.yaml`并查看`command`部分来查看我们的kind
    Kubernetes集群中启用的引导令牌机制。你将看到类似于图5.14的输出。
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../../OEBPS/Images/05-14.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-14.png)'
- en: Figure 5.14 Enable bootstrap token authentication in the Kubernetes API.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 在Kubernetes API中启用引导令牌身份验证。
- en: '*Bootstrap tokens* are essentially bearer tokens used when creating new clusters
    or joining new nodes to an existing cluster. These tokens were primarily built
    for kubeadm but can also be used in other scenarios without kubeadm, such as with
    third-party applications. A bootstrap token works much like a Service Account
    token in that the token allows the third-party app to authenticate with the Kubernetes
    API and communicate with objects inside of the cluster.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*引导令牌*本质上是用于创建新集群或将新节点加入现有集群时使用的持有令牌。这些令牌最初是为kubeadm构建的，但也可以在不使用kubeadm的其他场景中使用，例如与第三方应用程序一起使用。引导令牌的工作方式与Service
    Account令牌非常相似，即令牌允许第三方应用程序通过Kubernetes API进行身份验证并与集群内的对象进行通信。'
- en: 'In the context of joining a node to the cluster, bootstrap tokens establish
    bidirectional trust between the node that’s joining the cluster and a control
    plane node. We can generate a new token in our kind Kubernetes cluster with the
    command `kubeadm token create --print-join-command`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在将节点加入集群的上下文中，引导令牌在加入集群的节点和控制平面节点之间建立了双向信任。我们可以在kind Kubernetes集群中使用以下命令生成一个新的令牌：`kubeadm
    token create --print-join-command`：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: EXAM TIP On exam day, don’t be afraid to use the help menu (`kubeadm -help`).
    The help menu will contain examples, in some cases, that you can copy and paste
    right into the command line.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 考试技巧：在考试当天，不要害怕使用帮助菜单（`kubeadm -help`）。帮助菜单将包含示例，在某些情况下，你可以直接复制粘贴到命令行中。
- en: Let’s proceed with adding a new node to our kind Kubernetes cluster. To create
    a new node, we must first create a secondary cluster, and then orphan the node
    from the new cluster, adding it to our original cluster. We do this because the
    orphaned node will have all the necessary prerequisites (containerd, kubelet,
    and kubeadm) to run this node as a Kubernetes node. On the CKA exam, you may come
    across a node that needs to be added to a cluster after being orphaned, but you
    most likely will not have to install the prerequisites. We can create the new
    kind cluster with the command `kind create cluster --config config2.yaml --name
    cka` using the `config.yaml` file that we used in chapter 2, but modified slightly.
    You will get an output similar to figure 5.15.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续向我们的 kind Kubernetes 集群添加一个新节点。为了创建一个新节点，我们首先必须创建一个辅助集群，然后从新集群中孤儿化节点，将其添加到我们的原始集群中。我们这样做是因为孤儿化节点将拥有运行此节点作为
    Kubernetes 节点所需的所有必要先决条件（containerd、kubelet 和 kubeadm）。在 CKA 考试中，您可能会遇到需要添加到集群中的孤儿节点，但您很可能不需要安装先决条件。我们可以使用
    `config.yaml` 文件创建新的 kind 集群，该文件我们在第 2 章中使用过，但略有修改。您将得到类似于图 5.15 的输出。
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../../OEBPS/Images/05-15.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-15.png)'
- en: Figure 5.15 Create a new cluster named `cka` with a two-node configuration.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 使用两个节点配置创建一个名为 `cka` 的新集群。
- en: Now that we have created a new cluster named `cka`, our context has already
    been switched to the new cluster. We can simply run the command `k delete no cka-worker`
    to remove the node from that cluster, as we see in figure 5.16.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个名为 `cka` 的新集群，我们的上下文已经切换到新集群。我们可以简单地运行命令 `k delete no cka-worker`
    来从该集群中移除节点，如图 5.16 所示。
- en: '![](../../OEBPS/Images/05-16.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-16.png)'
- en: Figure 5.16 Deleting the node removes it from context, but it still lives as
    a Docker container.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 删除节点将其从上下文中移除，但它仍然作为一个 Docker 容器存在。
- en: 'Even though we deleted `cka-worker`, the node is still running and can be accessed
    from outside of Kubernetes. You can get a shell to it with the command `docker
    exec -it cka-worker bash`. Now let’s run the command `kubeadm reset` to revert
    to a fresh state for this node and clear its affiliation with the `cka` cluster:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们已经删除了 `cka-worker`，该节点仍然在运行，并且可以从 Kubernetes 外部访问。您可以使用命令 `docker exec -it
    cka-worker bash` 获取到它的 shell。现在让我们运行命令 `kubeadm reset` 来将此节点恢复到新鲜状态并清除它与 `cka`
    集群的关联：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can go ahead and apply the `join` command that we generated earlier. This
    command will use kubeadm to join the node to our cluster named `kind,` pass the
    token for authentication, and also pass `--discovery-token-ca-cert-hash` to validate
    the public key of the root certificate authority(CA) presented by the control
    plane. You’ll see an output similar to figure 5.17.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续应用我们之前生成的 `join` 命令。这个命令将使用 kubeadm 将节点加入我们名为 `kind` 的集群，传递用于身份验证的令牌，并且传递
    `--discovery-token-ca-cert-hash` 以验证由控制平面提供的根证书颁发机构（CA）的公钥。您将看到类似于图 5.17 的输出。
- en: '![](../../OEBPS/Images/05-17.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/05-17.png)'
- en: Figure 5.17 Use the `join` command to join a node to an existing cluster.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 使用 `join` 命令将节点加入现有集群。
- en: 'We can go back over to the kind cluster and look at our nodes from that context:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以回到 kind 集群并从那个上下文查看我们的节点：
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Congratulations! You’ve successfully added a node to an existing cluster, making
    it a three-node cluster. Now you can continue to schedule Pods to it and use it
    just as you would any other worker node in the Kubernetes cluster.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功将一个节点添加到现有集群中，使其成为三节点集群。现在您可以将 Pod 调度到它上面，就像在 Kubernetes 集群中的任何其他工作节点一样使用它。
- en: Exam exercises
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 练习考试
- en: From a three-node cluster, cordon one of the worker nodes. Schedule a Pod without
    a `nodeSelector`. Uncordon the worker node and edit the Pod, applying a new node
    name to the YAML (set it to the node that was just uncordoned). After replacing
    the YAML, see if the Pod is scheduled to the recently uncordoned node.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个三节点集群中，隔离一个工作节点。不指定 `nodeSelector` 调度一个 Pod。解除隔离工作节点并编辑 Pod，将新的节点名称应用到 YAML
    文件中（将其设置为刚刚解除隔离的节点）。替换 YAML 文件后，看看 Pod 是否被调度到最近解除隔离的节点。
- en: Start a basic nginx Deployment; remove the taint from the control plane node
    so that Pods don’t need a toleration to be scheduled to it. Add a `nodeSelector`
    to the Pod spec within the Deployment, and see if the Pod is now running on the
    control plane node.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个基本的 nginx 部署；从控制平面节点移除污点，这样 Pod 就不需要容忍就可以调度到它上面。在部署中添加一个 `nodeSelector`
    到 Pod 规范，看看 Pod 是否现在正在控制平面节点上运行。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Deployments are a common resource in Kubernetes, and the ReplicaSets within
    are important for keeping the desired number of replicas running.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署是 Kubernetes 中的常见资源，其中的 ReplicaSet 对于保持所需副本的数量运行至关重要。
- en: Rollouts are versioned, and you can optionally leave notes for other Kubernetes
    administrators or developers.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动操作是分版本的，你可以选择为其他 Kubernetes 管理员或开发者留下备注。
- en: You can expose a Deployment, which creates a Service for users to access the
    application from outside the cluster.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以公开一个 Deployment，它为用户创建一个 Service，以便从集群外部访问应用程序。
- en: Maintenance is inevitable and will mean that at some point, you must know how
    to cordon and drain to perform OS upgrades or add resources to a node.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护是不可避免的，这意味着在某个时候，你必须知道如何进行隔离和排空以执行操作系统升级或向节点添加资源。
- en: Use kubeadm to upgrade control plane components so that the cluster can stay
    updated with the latest patches to prevent CVEs.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 kubeadm 升级控制平面组件，以便集群可以保持最新补丁，防止 CVE。
- en: You can easily add a node to an existing cluster to provide additional resources
    to applications running on Kubernetes.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以轻松地将节点添加到现有的集群中，为在 Kubernetes 上运行的应用程序提供额外的资源。
