- en: Part 5\. Clustering
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5部分\. 聚类
- en: Our next stop in unsupervised learning is clustering. Clustering covers a range
    of techniques used to identify clusters of cases in a dataset. A *cluster* is
    a set of cases that are more similar to each other than they are to cases in other
    clusters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在无监督学习中的下一个停留点是聚类。聚类涵盖了用于在数据集中识别案例簇的一系列技术。*簇*是一组案例，它们彼此之间比与其他簇中的案例更相似。
- en: Conceptually, clustering can be considered similar to classification, in that
    we are trying to assign a discrete value to each case. The difference is that
    while classification uses labeled cases to learn patterns in the data that separate
    the classes, we use clustering when we don’t have any prior knowledge about class
    membership or whether there are distinct classes in the data. Clustering therefore
    describes a set of algorithms that try to identify a grouping structure within
    a dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，聚类可以看作与分类相似，因为我们试图为每个案例分配一个离散值。区别在于，虽然分类使用标记的案例来学习数据中的模式，以区分类别，但我们使用聚类是因为我们没有关于类别成员资格或数据中是否存在不同类别的先验知识。因此，聚类描述了一组试图在数据集中识别分组结构的算法。
- en: In [chapters 16](kindle_split_029.html#ch16) through [19](kindle_split_032.html#ch19),
    I’ll arm you with different clustering techniques that can handle a range of clustering
    problems. Validating the performance of a clustering algorithm can be a challenge,
    and there may not always be an obvious or even a “correct” answer, but I’ll teach
    you skills to help maximize the information you get from these approaches.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[16章](kindle_split_029.html#ch16)到第[19章](kindle_split_032.html#ch19)中，我将为你提供不同的聚类技术，这些技术可以处理各种聚类问题。验证聚类算法的性能可能是一个挑战，并且可能并不总是有一个明显或甚至“正确”的答案，但我将教你技能，帮助你最大化从这些方法中获得的信息。
- en: Chapter 16\. Clustering by finding centers with k-means
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第16章\. 使用k-means寻找中心进行聚类
- en: '*This chapter covers*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding the need for clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解聚类需求
- en: Understanding over- and underfitting for clustering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解聚类过拟合和欠拟合
- en: Validating the performance of a clustering algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证聚类算法的性能
- en: 'Our first stop in clustering brings us to a very commonly used technique: *k-means
    clustering*. I’ve used the word *technique* here rather than *algorithm* because
    k-means describes a particular *approach* to clustering that multiple algorithms
    follow. I’ll talk about these individual algorithms later in the chapter.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在聚类中的第一个停留点带我们到一个非常常用的技术：*k-means聚类*。我在这里使用的是*技术*这个词，而不是*算法*，因为k-means描述了聚类的一个特定*方法*，多个算法都遵循这种方法。我将在本章的后面讨论这些个别算法。
- en: '|  |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Don’t confuse k-means with k-nearest neighbors! K-means is for unsupervised
    learning, whereas k-nearest neighbors is a supervised algorithm for classification.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆k-means和k-最近邻！k-means用于无监督学习，而k-最近邻是一个用于分类的监督算法。
- en: '|  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: K-means clustering attempts to learn a grouping structure in a dataset. The
    k-means approach starts with us defining how many clusters we believe there are
    in the dataset. This is what the *k* stands for; if we set *k* to 3, we will identify
    three clusters (whether these represent a real grouping structure or not). Arguably,
    this is a weakness for k-means, because we may not have any prior knowledge as
    to how many clusters to search for, but I’ll show you ways to select a sensible
    value of *k*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类试图在数据集中学习分组结构。k-means方法首先由我们定义我们认为数据集中有多少个簇。这就是*k*的含义；如果我们把*k*设为3，我们将识别出三个簇（无论这些是否代表真实的分组结构）。这可以说是k-means的一个弱点，因为我们可能没有关于要搜索多少个簇的先验知识，但我将向你展示如何选择一个合理的*k*值。
- en: Once we have defined how many clusters, *k*, we want to search for, k-means
    will initialize (usually randomly) *k* centers or *centroids* in the dataset.
    Each centroid may not be an actual case from the data but has a random value for
    every variable in the data. Each of these centroids represents a cluster, and
    cases are assigned to the cluster of the centroid closest to them. Iteratively,
    the centroids move around the feature space in a way that attempts to minimize
    the variance of the data within each cluster but maximizes the separation of different
    clusters. At each iteration, cases are assigned to the cluster of the centroid
    that is closest to them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了要搜索的集群数量*k*，k-means将在数据集中初始化（通常随机）*k*个中心或质心。每个质心可能不是数据中的实际案例，但每个变量都有一个随机值。这些质心中的每一个代表一个集群，案例被分配到与它们最近的质心的集群。迭代地，质心在特征空间中移动，试图最小化每个集群内的数据方差，但最大化不同集群之间的分离。在每次迭代中，案例被分配到与它们最近的质心的集群。
- en: By the end of this chapter, I hope you’ll understand a general approach to clustering
    and what over- and underfitting look like for clustering tasks. I’ll show you
    how to apply k-means clustering to a dataset and ways of evaluating clustering
    performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望你能理解聚类的一般方法以及聚类任务中的过拟合和欠拟合是什么样的。我将向你展示如何将k-means聚类应用于数据集以及评估聚类性能的方法。
- en: 16.1\. What is k-means clustering?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1. 什么是k-means聚类？
- en: In this section, I’ll show you how the general procedure for k-means clustering
    works and then explain the various algorithms that implement it and how they differ.
    K-means algorithms partition cases in a dataset into *k* clusters, where *k* is
    an integer defined by us. The clusters returned by k-means algorithms tend to
    be *n*-dimensionally spherical (where *n* is the number of dimensions of the feature
    space). This means the clusters tend to form a circle in two dimensions, a sphere
    in three dimensions, and a hypersphere in more than three dimensions. K-means
    clusters also tend to have a similar diameter. These are traits that may not be
    true of the underlying structure in the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示k-means聚类的通用过程，然后解释实现它的各种算法以及它们的区别。K-means算法将数据集中的案例划分为*k*个集群，其中*k*是我们定义的一个整数。k-means算法返回的集群倾向于是*n*-维度的球形（其中*n*是特征空间的维度数）。这意味着集群在二维空间中倾向于形成一个圆圈，在三维空间中形成一个球体，在超过三维的空间中形成一个超球体。K-means集群也倾向于具有相似的大径。这些可能是数据基础结构中不真实的特征。
- en: 'There are a number of k-means algorithms, but some commonly used ones are as
    follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多k-means算法，但一些常用的算法如下：
- en: Lloyd algorithm (also called Lloyd-Forgy algorithm)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lloyd算法（也称为Lloyd-Forgy算法）
- en: MacQueen algorithm
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacQueen算法
- en: Hartigan-Wong algorithm
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hartigan-Wong算法
- en: The Lloyd, MacQueen, and Hartigan-Wong algorithms are conceptually quite similar
    but have some differences that affect both their computational cost and their
    performance on a particular problem. Let’s go through each algorithm to explain
    how it works.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Lloyd、MacQueen和Hartigan-Wong算法在概念上非常相似，但它们之间的一些差异会影响它们的计算成本以及在特定问题上的性能。让我们逐一介绍每个算法，解释它们是如何工作的。
- en: 16.1.1\. Lloyd’s algorithm
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.1. Lloyd算法
- en: 'In this section, I’ll show you the easiest of these three algorithms to understand:
    Lloyd’s algorithm. Imagine that you’re a sports scientist, interested in the biophysical
    differences among runners. You measure the resting heart rate and maximum oxygen
    consumption of a cohort of runners, and you want to use k-means to identify clusters
    of runners that might benefit from different training regimens.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示这三个算法中最容易理解的一个：Lloyd算法。想象一下，你是一名体育科学家，对跑者的生物物理差异感兴趣。你测量了一组跑者的静息心率和最大摄氧量，并希望使用k-means算法来识别可能从不同训练方案中受益的跑者集群。
- en: Let’s say you have prior reason to believe there may be three distinct clusters
    of athletes in the dataset. The first step in Lloyd’s algorithm is to randomly
    initialize *k* (three in this case) centroids in the data (see [figure 16.1](#ch16fig01)).
    Next, the distance between each case and each centroid is calculated. This distance
    is commonly the Euclidean distance (straight-line distance) but can be other distance
    metrics, such as the Manhattan distance (taxi cab distance).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你事先有理由相信数据集中可能存在三个不同的运动员集群。Lloyd算法的第一步是在数据中随机初始化*k*（在这个例子中是三个）质心（见[图16.1](#ch16fig01)）。接下来，计算每个案例与每个质心之间的距离。这个距离通常是欧几里得距离（直线距离），但也可以是其他距离度量，例如曼哈顿距离（出租车距离）。
- en: Figure 16.1\. Five iterations of k-means clustering. In the top-left plot, three
    initial centers are randomly generated in the feature space (crosses). Cases are
    assigned to the cluster of their nearest center. At each iteration, each center
    moves to the mean of the cases in its cluster (indicated by arrows). The feature
    space can be partitioned up into Voronoi cells (I'll discuss these shortly), indicated
    by the shaded regions, that show regions of the feature space closest to a particular
    centroid.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图16.1\. k-means聚类的五次迭代。在左上角的图中，在特征空间中随机生成了三个初始中心（交叉点）。案例被分配给其最近中心的簇。在每次迭代中，每个中心移动到其簇中案例的平均值处（由箭头指示）。特征空间可以被划分为Voronoi单元（我将在稍后讨论这些），由阴影区域表示，显示特征空间中距离特定质心最近的区域。
- en: '![](fig16-1_alt.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](fig16-1_alt.jpg)'
- en: '|  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because k-means relies on a distance metric, it’s important to scale variables
    if they are measured on different scales; otherwise, variables on larger scales
    will disproportionately influence the result.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因为k-means依赖于距离度量，如果变量在不同的尺度上测量，那么缩放变量是很重要的；否则，较大尺度的变量将不成比例地影响结果。
- en: '|  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Each case is assigned to the cluster represented by its nearest centroid. In
    this way, each centroid serves as a *prototype* case for its cluster. Next, the
    centroids are moved, such that they are placed at the mean of the cases that were
    assigned to their cluster in the previous step (this is why the approach is called
    *k-means*).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个案例都被分配给由其最近的质心表示的簇。这样，每个质心都作为其簇的*原型*案例。接下来，将质心移动，使它们位于上一步分配给其簇的案例的平均值处（这就是为什么这种方法被称为*k-means*）。
- en: 'The process now repeats itself: the distance between each case and each centroid
    is calculated, and cases are assigned to the cluster of the nearest centroid.
    Can you see that, because the centroids update and move around the feature space,
    the centroid nearest to a particular case may change over time? This process continues
    until no cases change clusters from one iteration to the next, or until a maximum
    number of iterations is reached. Notice that between iterations 4 and 5 in [figure
    16.1](#ch16fig01), no cases change clusters, so the algorithm stops.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在过程会重复进行：计算每个案例与每个质心的距离，并将案例分配给最近的质心所在的簇。你能看到，因为质心会更新并在特征空间中移动，所以一个特定案例最近的质心可能会随时间改变吗？这个过程会一直持续到没有案例从一个迭代到下一个迭代改变簇，或者达到最大迭代次数。注意，在[图16.1](#ch16fig01)的迭代4和5之间，没有案例改变簇，所以算法停止。
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because the initial centers are usually randomly selected, it’s important that
    we repeat the procedure several times, with new random initial centers each time.
    We can then use the centers that start with the lowest within-cluster sum of squared
    error.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因为初始中心通常随机选择，所以我们重复执行此过程几次，每次都使用新的随机初始中心是很重要的。然后我们可以使用开始时具有最低簇内平方误差和的中心。
- en: '|  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Let’s summarize the steps of Lloyd’s algorithm:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下Lloyd算法的步骤：
- en: Select *k*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*k*。
- en: Randomly initialize *k* centers in the feature space.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特征空间中随机初始化*k*个中心。
- en: 'For each case:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个案例：
- en: Calculate the distance between the case and each center.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算案例与每个中心之间的距离。
- en: Assign the case to the cluster of the nearest centroid.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将案例分配给最近的质心所在的簇。
- en: Place each center at the mean of the cases assigned to its cluster.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个中心放置在其簇分配的案例的平均值处。
- en: Repeat steps 3 and 4 until no cases change clusters or a maximum number of iterations
    is reached.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤3和4，直到没有案例改变簇或达到最大迭代次数。
- en: In [figure 16.1](#ch16fig01), can you see how at each iteration, the positions
    of the centroids are updated (the arrows) such that they move toward the center
    of genuine clusters? At each iteration, we can partition the feature space into
    polygonal (or polytopal, in more than two dimensions) regions around each centroid
    that show us the regions that “belong” to a particular cluster. These regions
    are called *Voronoi cells*; and if a case falls inside one of them, this means
    the case is closest to that cell’s centroid and will be assigned to its cluster.
    Visualizing Voronoi cells on a plot (sometimes called a *Voronoi map*) is a useful
    way of visualizing how a clustering algorithm has partitioned the feature space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 16.1](#ch16fig01) 中，你能看到在每次迭代中，质心的位置是如何更新的（箭头），以便它们向真实聚类的中心移动？在每次迭代中，我们可以将特征空间划分为围绕每个质心的多边形（或多面体，在两个以上维度中）区域，这些区域显示了“属于”特定聚类的区域。这些区域被称为
    *Voronoi 单元*；如果一个案例落在其中，这意味着该案例最接近该单元的质心，并将被分配到其聚类。在图上可视化 Voronoi 单元（有时称为 *Voronoi
    地图*）是可视化聚类算法如何划分特征空间的有用方法。
- en: 16.1.2\. MacQueen’s algorithm
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.2. MacQueen 算法
- en: MacQueen’s algorithm is extremely similar to Lloyd’s algorithm, varying just
    subtly in when the centroids get updated. Lloyd’s algorithm is called a *batch*
    or *offline* algorithm, meaning it updates the centroids together at the end of
    an iteration. MacQueen’s algorithm, on the other hand, updates the centroids each
    time a case changes clusters and once the algorithm has passed through all the
    cases in the data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: MacQueen 算法与 Lloyd 算法极为相似，只是在质心更新时间上略有不同。Lloyd 算法被称为 *批量* 或 *离线* 算法，意味着它在迭代结束时一起更新质心。另一方面，MacQueen
    算法每次案例改变聚类并且算法已经遍历了数据中的所有案例时，都会更新质心。
- en: '|  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: Whereas Lloyd’s algorithm is said to be a batch or offline algorithm, MacQueen’s
    is said to be an *incremental* or *online* algorithm, because it updates the centroids
    each time a case moves clusters, rather than after a pass through all the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '与 Lloyd 算法被称为批量或离线算法相比，MacQueen 算法被称为 *增量* 或 *在线* 算法，因为它每次案例移动聚类时都会更新质心，而不是在遍历所有数据后更新。 '
- en: '|  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Just like with Lloyd’s algorithm, MacQueen’s algorithm initializes *k* centers,
    assigns each case to the cluster of the nearest centroid, and updates the position
    of the centroid to match the mean of its nearest cases. Then the algorithm considers
    each case in turn and calculates its distance to each centroid. If the case changes
    clusters (because it’s now closer to a different centroid), both the new and old
    centroid positions are updated. The algorithm continues through the dataset, considering
    each case in turn. Once all cases have been considered, the centroid positions
    are updated again. If no cases change clusters, the algorithm stops; otherwise,
    it will perform another pass.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Lloyd 算法一样，MacQueen 算法初始化 *k* 个中心，将每个案例分配到最近的质心的聚类，并将质心的位置更新为与其最近的案例的平均值。然后算法逐个考虑每个案例，并计算其到每个质心的距离。如果案例改变了聚类（因为它现在更接近不同的质心），则更新新旧质心的位置。算法继续通过数据集，逐个考虑每个案例。一旦所有案例都已考虑，再次更新质心的位置。如果没有案例改变聚类，则算法停止；否则，它将进行另一轮遍历。
- en: A benefit of MacQueen’s algorithm over Lloyd’s algorithm is that it tends to
    converge more quickly to an optimal solution. However, it may be slightly more
    computationally expensive for very large datasets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Lloyd 算法相比，MacQueen 算法的优点是它往往更快地收敛到最优解。然而，对于非常大的数据集，它可能稍微计算成本更高。
- en: 'Let’s summarize the steps of MacQueen’s algorithm:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下 MacQueen 算法的步骤：
- en: Select *k*.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 *k*。
- en: Randomly initialize *k* centers in the feature space.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特征空间中随机初始化 *k* 个中心。
- en: Assign each case to the cluster of its nearest center.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个案例分配到其最近中心的聚类。
- en: Place each center at the mean of the cases assigned to its cluster.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个中心放置在其分配给其聚类的案例的平均值处。
- en: 'For each case:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个案例：
- en: Calculate the distance between the case and each centroid.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算案例与每个质心之间的距离。
- en: Assign the case to the cluster of the nearest centroid.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将案例分配到最近的质心的聚类。
- en: If the case changed clusters, update the position of the new and old centroids.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果案例改变了聚类，则更新新旧质心的位置。
- en: Once all cases have been considered, update all centroids.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有案例都已考虑，更新所有质心。
- en: If no cases change clusters, stop; otherwise, repeat step 5.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有案例改变聚类，则停止；否则，重复步骤 5。
- en: 16.1.3\. Hartigan-Wong algorithm
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.3. Hartigan-Wong 算法
- en: 'The third k-means algorithm is a little different from the Lloyd and MacQueen
    algorithms. The Hartigan-Wong algorithm starts by initializing *k* random centers
    and assigning each case to the cluster of its nearest center, just as we saw in
    the other two algorithms. Here’s the different bit: for each case in the dataset,
    the algorithm calculates the sum of squared error of that case’s current cluster
    *if that case was removed*, and the sum of squared error of each of the other
    clusters *if that case was included in those clusters*. Recall from previous chapters
    that the sum of squared error (or simply the sum of squares) is the difference
    between each case’s values and its predicted values (in this context, its centroid),
    squared and summed across all the cases. If you prefer this in mathematical notation,
    have a look at [equation 16.1](#ch16equ01).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个 k-means 算法与 Lloyd 和 MacQueen 算法略有不同。Hartigan-Wong 算法首先初始化 *k* 个随机中心，并将每个案例分配给其最近中心的聚类，就像我们在其他两个算法中看到的那样。这里的不同之处在于：对于数据集中的每个案例，算法计算如果该案例被移除，则其当前聚类的平方误差之和，以及如果该案例包含在那些聚类中，则每个其他聚类的平方误差之和。回想一下，从前几章中我们知道，平方误差之和（或简称为平方和）是每个案例的值与其预测值（在此上下文中，其质心）之间的差异，平方后对所有案例求和。如果您更喜欢数学符号，请查看[方程
    16.1](#ch16equ01)。
- en: equation 16.1\.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 16.1。
- en: '![](eq16-1.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![eq16-1](eq16-1.jpg)'
- en: where *i* ∈ *k* is the *i*th case belonging to cluster *k*, and *c[k]* is the
    centroid of cluster *k*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *i* ∈ *k* 是属于聚类 *k* 的第 *i* 个案例，而 *c[k]* 是聚类 *k* 的质心。
- en: The cluster with the smallest sum of squared error (when including the case
    currently under consideration) is assigned as the cluster for that case. If a
    case changed clusters, then the centroids of the old and new clusters are updated
    to the mean of the cases in their cluster. The algorithm continues until no cases
    change clusters. As a result, a case could be assigned to a particular cluster
    (because it reduces the sum of squared error) even though it is closer to the
    centroid of another cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当包括当前正在考虑的案例时，具有最小平方误差之和的聚类被分配为该案例的聚类。如果一个案例改变了聚类，那么旧聚类和新聚类的质心将被更新为它们聚类中案例的平均值。算法继续进行，直到没有案例改变聚类。因此，一个案例可能被分配给特定的聚类（因为它减少了平方误差之和），即使它更接近另一个聚类的质心。
- en: 'Let’s summarize the steps of the Hartigan-Wong algorithm:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下 Hartigan-Wong 算法的步骤：
- en: Select *k*.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 *k*。
- en: Randomly initialize *k* centers in the feature space.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特征空间中随机初始化 *k* 个中心。
- en: Assign each case to the cluster of its nearest center.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个案例分配给其最近中心的聚类。
- en: Place each center at the mean of the cases assigned to its cluster.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个中心放置在其分配给其聚类的案例的平均值处。
- en: 'For each case:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个案例：
- en: Calculate the sum of squared error for its cluster, omitting the case under
    consideration.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算其聚类的平方误差之和，忽略正在考虑的案例。
- en: Calculate the sum of squared error for the other clusters, as if that case were
    included.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算其他聚类的平方误差之和，就像那个案例被包含在内一样。
- en: Assign the case to the cluster with the smallest sum of squared error.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将案例分配给具有最小平方误差之和的聚类。
- en: If the case changed clusters, update the position of the new and old centroids.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果案例改变了聚类，则更新新聚类和旧聚类的位置。
- en: If no cases change clusters, stop; otherwise, repeat step 5.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有案例改变聚类，则停止；否则，重复步骤 5。
- en: The Hartigan-Wong algorithm *tends* to find a better clustering structure than
    either the Lloyd or MacQueen algorithms, although we are always subject to the
    “no free lunch” theorem. Hartigan-Wong is also more computationally expensive
    than the other two algorithms, so it will be considerably slower for large datasets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Hartigan-Wong 算法 *倾向于*比 Lloyd 或 MacQueen 算法找到更好的聚类结构，尽管我们总是受到“没有免费午餐”定理的限制。Hartigan-Wong
    算法也比其他两个算法更耗费计算资源，因此对于大型数据集来说会慢得多。
- en: Which algorithm do we choose? Well, the choice is a discrete hyperparameter,
    so we can use hyperparameter tuning to help us choose the best-performing method
    and make sure we don’t make the Wong choice!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择哪种算法呢？嗯，选择是一个离散的超参数，因此我们可以使用超参数调整来帮助我们选择表现最好的方法，并确保我们不会做出错误的抉择！
- en: 16.2\. Building your first k-means model
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2\. 构建您的第一个 k-means 模型
- en: In this section, I’ll show you how to build a k-means model in R, using the
    mlr package. I’ll cover creating a cluster task and learner, and some methods
    we can use to evaluate the performance of a clustering algorithm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用 R 中的 mlr 包构建 k-means 模型。我将介绍创建聚类任务和学习者，以及我们可以用来评估聚类算法性能的一些方法。
- en: 'Imagine that you’re looking for clusters of white blood cells from patients
    with graft versus host disease (GvHD). GvHD is an unpleasant disease where residual
    white blood cells in transplanted tissue attack the body of the patient receiving
    the transplant. You take a biopsy from each patient and measure different proteins
    on the surface of each cell. You hope to create a clustering model that will help
    you identify different cell types from the biopsy, to help you better understand
    the disease. Let’s start by loading the mlr and tidyverse packages:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在寻找来自移植物抗宿主病（GvHD）患者的白细胞簇。GvHD是一种令人不快的疾病，其中移植组织中残留的白细胞攻击接受移植的患者。你从每位患者那里取活检，并测量每个细胞表面的不同蛋白质。你希望创建一个聚类模型，帮助你从活检中识别不同的细胞类型，以帮助你更好地理解这种疾病。让我们先加载mlr和tidyverse包：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 16.2.1\. Loading and exploring the GvHD dataset
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.1\. 加载和探索GvHD数据集
- en: Now let’s load the data, which is built into the mclust package, convert it
    into a tibble (with `as_tibble()`), and explore it a little. We have a tibble
    containing 6,809 cases and 4 variables, each of which is a different protein measured
    on the surface of each cell.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载数据，这些数据内置在mclust包中，将其转换为tibble（使用`as_tibble()`），并对其进行一些探索。我们有一个包含6,809个案例和4个变量的tibble，每个变量都是测量在每个细胞表面的不同蛋白质。
- en: Listing 16.1\. Loading and exploring the GvHD dataset
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表16.1\. 加载和探索GvHD数据集
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: 'Calling `data(GvHD, package = "mclust")` actually loads two datasets: GvHD.control
    and GvHD.pos. We’re going to work with the GvHD.control dataset, but at the end
    of this section, I’ll get you to build a clustering model on the GvHD.pos dataset
    too.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`data(GvHD, package = "mclust")`实际上加载了两个数据集：GvHD.control和GvHD.pos。我们将使用GvHD.control数据集，但在本节的最后，我还会指导你使用GvHD.pos数据集构建一个聚类模型。
- en: '|  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Because k-means algorithms use a distance metric to assign cases to clusters,
    it’s important that our variables are scaled so variables on different scales
    are given equal weight. All of our variables are continuous, so we can simply
    pipe our entire tibble into the `scale()` function. Remember that this will center
    and scale each variable by subtracting the mean and dividing by the standard deviation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于k-means算法使用距离度量来将案例分配到簇中，因此我们的变量必须是缩放的，以便不同尺度的变量得到相同的权重。我们的所有变量都是连续的，因此我们可以简单地通过将整个tibble管道到`scale()`函数中来进行缩放。记住，这将通过减去平均值并除以标准差来对每个变量进行中心化和缩放。
- en: Listing 16.2\. Scaling the GvHD dataset
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表16.2\. 缩放GvHD数据集
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, let’s plot the data using our good friend `ggpairs()` from the GGally
    package. This time, we modify the way `ggpairs()` draws the facets. We use the
    `upper`, `lower`, and `diag` arguments to specify what kind of plots should be
    drawn above, below, and on the diagonal, respectively. Each of these arguments
    takes a list where each list element can be used to specify a different type of
    plot for continuous variables, discrete variables, and combinations of the two.
    Here, I’ve chosen to draw 2D density plots on the upper plots, scatterplots on
    the lower plots, and density plots on the diagonal.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用GGally包中的好朋友`ggpairs()`来绘制数据。这次，我们修改了`ggpairs()`绘制分面图的方式。我们使用`upper`、`lower`和`diag`参数来分别指定应该在上部、下部和对角线绘制哪种类型的图表。每个参数都接受一个列表，其中每个列表元素可以用来指定连续变量、离散变量以及两者的组合的不同类型图表。在这里，我选择在上部图表上绘制二维密度图，在下部图表上绘制散点图，在对角线上绘制密度图。
- en: To prevent overcrowding, we want to reduce the size of the points on the lower
    plots. To change any of the graphical options of the plots (such as size and color
    of the geoms), we just need to wrap the name of the plot type (literally) inside
    the `wrap()` function, along with the options we’re changing. Here, we use `wrap("points",
    size = 0.5)` to draw scatterplots on the lower panels, with a smaller point size
    than the default.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止拥挤，我们希望减小下部图表上点的尺寸。要更改图表的任何图形选项（如geoms的大小和颜色），我们只需将图表类型的名称（字面上）包裹在`wrap()`函数中，以及我们正在更改的选项。在这里，我们使用`wrap("points",
    size = 0.5)`在底部面板上绘制散点图，点的尺寸比默认值小。
- en: '|  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: Remember that *geom* stands for *geometric object*, referring to graphical elements
    like lines, dots, and bars on a plot.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，*geom*代表*几何对象*，指的是图表上的线条、点和条形等图形元素。
- en: '|  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 16.3\. Creating pairwise plots with `ggpairs()`
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表16.3\. 使用`ggpairs()`创建成对图
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: The default diagonal plot for continuous variables is a density plot. I explicitly
    defined it as such here anyway so you can see how you can control the upper, lower,
    and diagonal plots independently.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续变量，默认的对角线图是密度图。我仍然明确地将其定义为这样的，这样您就可以看到如何独立控制上、下和对角线图。
- en: '|  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The resulting plot is shown in [figure 16.2](#ch16fig02). Can you see different
    clusters of cases in the data? The human brain is pretty good at identifying clusters
    in two or even three dimensions, and it looks as though there are at least four
    clusters in the dataset. The density plots are useful to help us see dense regions
    of cases, which simply appear black in the scatterplots.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图16.2](#ch16fig02)。您能在数据中看到不同的案例簇吗？人脑在识别二维甚至三维中的簇方面相当出色，看起来数据集中至少有四个簇。密度图有助于我们查看案例的密集区域，这些区域在散点图中简单地显示为黑色。
- en: Figure 16.2\. `ggpairs()` plot of each variable against every other variable
    in our GvHD dataset. Scatterplots are shown below the diagonal, 2D density plots
    are shown above the diagonal, and 1D density plots are drawn on the diagonal.
    It appears as if there are multiple clusters in the data.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图16.2\. GvHD数据集中每个变量与每个其他变量的`ggpairs()`图。散点图显示在对角线以下，二维密度图显示在对角线以上，一维密度图绘制在对角线上。看起来数据中存在多个簇。
- en: '![](fig16-2_alt.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![fig16-2_alt.jpg](fig16-2_alt.jpg)'
- en: 16.2.2\. Defining our task and learner
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何定义聚类任务和聚类学习器。在mlr中，我们通过使用`makeClusterTask()`函数来创建聚类任务（这里没有惊喜）。我们将我们的缩放数据（转换为数据框）作为`data`参数提供。
- en: In this section, I’ll show you how to define a clustering task and clustering
    learner. In mlr, we create a clustering task by using the `makeClusterTask()`
    function (no surprises there). We supply our scaled data (converted into a data
    frame) as the `data` argument.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Important
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: Notice that, unlike creating a supervised learning task (for classification
    or regression), we no longer need to supply the `target` argument. This is because
    in an unsupervised learning task, there is no labels variable to use as a target.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与创建监督学习任务（用于分类或回归）不同，我们不再需要提供`target`参数。这是因为在不监督学习任务中，没有标签变量可以用作目标。
- en: '|  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Let’s use the `listLearners()` function that you learned about all the way
    back in [chapter 3](kindle_split_013.html#ch03) to see what algorithms have been
    implemented by the mlr package so far. At the time of writing, only nine clustering
    algorithms are available to us. Admittedly, this is far fewer than the number
    of algorithms available for classification and regression, but mlr still provides
    some useful tools for clustering. If you want to use an algorithm that mlr doesn’t
    currently wrap, you can always implement it yourself (visit the mlr website to
    see how: [http://mng.bz/E1Pj](http://mng.bz/E1Pj)).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 16.2.2\. 定义我们的任务和学习器
- en: 'Now let’s define our k-means learner. We do this using the familiar `makeLearner()`
    function, this time supplying `"cluster.kmeans"` as the name of the learner. We
    use the `par.vals` argument to supply two arguments to the learner: `iter.max`
    and `nstart`.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义我们的k-means学习器。我们使用熟悉的`makeLearner()`函数来完成这个任务，这次我们将`"cluster.kmeans"`作为学习器的名称。我们使用`par.vals`参数向学习器提供两个参数：`iter.max`和`nstart`。
- en: '|  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Just as the prefixes for classification and regression learners were `classif.`
    and `regr.`, respectively, the prefix for clustering learners is `cluster`..
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就像分类和回归学习者的前缀分别是`classif.`和`regr.`一样，聚类学习者的前缀是`cluster`。
- en: '|  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 16.4\. Creating a cluster task and learner with mlr
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 让我们使用您在[第3章](kindle_split_013.html#ch03)中了解到的`listLearners()`函数，看看mlr包目前实现了哪些算法。在撰写本文时，我们目前只有九种聚类算法可用。诚然，这比分类和回归中可用的算法数量要少得多，但mlr仍然为聚类提供了一些有用的工具。如果您想使用mlr目前没有包装的算法，您始终可以自己实现它（访问mlr网站了解如何：[http://mng.bz/E1Pj](http://mng.bz/E1Pj))。
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `iter.max` argument sets an upper limit for the number of times the algorithm
    will cycle through the data (the default is 10). The k-means algorithms will all
    stop once cases stop moving clusters, but setting a maximum can be useful for
    large datasets that take a long time to converge. Later in this section, I’ll
    show you how to tell if your clustering model has converged before reaching this
    limit.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`iter.max`参数为算法遍历数据的次数设置一个上限（默认为10）。k-means算法会在案例停止移动簇时停止，但为大数据集设置最大值可能很有用，因为这些数据集收敛需要很长时间。在本节稍后，我将向您展示如何判断聚类模型是否在达到此限制之前已经收敛。'
- en: 'The `nstart` argument controls how many times the function will randomly initialize
    the centers. Recall that the initial centers are usually randomly initialized
    somewhere in the feature space: this can have an impact on the final centroid
    positions and, therefore, the final cluster memberships. Setting the `nstart`
    argument higher than the default value of 1 will randomly initialize this number
    of centers. For each set of initial centers, the cases are assigned to the cluster
    of their nearest center in each set, and the set with the smallest within-cluster
    sum of squared error is then used for the rest of the clustering algorithm. In
    this way, the algorithm selects the set of centers that is already most similar
    to the real cluster centroids in the data. Increasing `nstart` is arguably more
    important than increasing the number of iterations.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`nstart` 参数控制函数将随机初始化中心的次数。回想一下，初始中心通常在特征空间中随机初始化：这可能会影响最终的质心位置，因此也会影响最终的簇成员资格。将
    `nstart` 参数设置高于默认值 1 将随机初始化这么多中心。对于每一组初始中心，案例被分配到每个组中最近中心所在的簇，然后使用具有最小簇内平方误差和的组作为聚类算法的其余部分。这样，算法会选择与数据中的真实簇质心最相似的中心集。增加
    `nstart` 可能比增加迭代次数更重要。'
- en: '|  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you have a dataset with very clearly separable clusters, setting `nstart`
    higher than 1 might be a waste of computational resources. However, unless your
    dataset is very large, it’s usually a good idea to set `nstart` > 1; in [listing
    16.4](#ch16ex04), I set mine to 10.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集具有非常明显可分离的簇，将 `nstart` 设置高于 1 可能是计算资源的浪费。然而，除非你的数据集非常大，否则通常将 `nstart`
    设置为大于 1 是一个好主意；在 [列表 16.4](#ch16ex04) 中，我将它设置为 10。
- en: '|  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 16.2.3\. Choosing the number of clusters
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.3. 选择簇的数量
- en: 'In this section, I’ll show you how we can sensibly choose the value of *k*,
    which defines the number of centers, and therefore clusters, that our model will
    identify. The need to choose *k* is often cited as a weakness of k-means clustering.
    This is because choosing *k* can be subjective. If you have prior domain knowledge
    as to how many clusters should theoretically be present in a dataset, then you
    should use this knowledge to guide your selection. If you’re using clustering
    as a preprocessing step before a supervised learning algorithm (classification,
    for example), then the choice is quite easy: tune *k* as a hyperparameter of the
    whole model-building process, and compare the predictions of the final model against
    the original labels.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示我们如何合理地选择 *k* 的值，它定义了我们的模型将识别的中心数量和簇数量。选择 *k* 的必要性常被引用为 k-means 聚类的弱点。这是因为选择
    *k* 可能是主观的。如果你有先验领域知识，知道数据集中理论上应该有多少簇，那么你应该使用这些知识来指导你的选择。如果你在使用聚类作为监督学习算法（例如分类）之前的预处理步骤，那么选择相当简单：将
    *k* 调整为整个模型构建过程的超参数，并将最终模型的预测与原始标签进行比较。
- en: But what if we have no prior knowledge and no labeled data to compare against?
    And what happens if we get our selection wrong? Well, just like for classification
    and regression, clustering is subject to the bias-variance trade-off. If we want
    to generalize a clustering model to the wider population, it’s important we neither
    overfit nor underfit the training data. [Figure 16.3](#ch16fig03) illustrates
    what under- and overfitting might look like for a clustering problem. When we
    underfit, we fail to identify and separate real clusters in the data; but when
    we overfit, we split real clusters into smaller, nonsensical clusters that simply
    don’t exist in the wider population.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们没有先验知识，也没有可比较的标记数据怎么办？如果我们选择错误会发生什么？嗯，就像分类和回归一样，聚类也受到偏差-方差权衡的影响。如果我们想将聚类模型推广到更广泛的群体，那么我们既不能过度拟合也不能欠拟合训练数据。[图
    16.3](#ch16fig03) 展示了聚类问题中欠拟合和过拟合可能看起来像什么。当我们欠拟合时，我们未能识别和分离数据中的真实簇；但当我们过拟合时，我们将真实簇分割成更小、无意义的簇，这些簇在更广泛的群体中根本不存在。
- en: Figure 16.3\. What under- and overfitting looks like for clustering tasks. In
    the left-side plot, the clusters are underfit (fewer clusters have been identified
    than actually exist). In the right-side plot, the clusters are overfit (real clusters
    are broken up into smaller clusters). In the center plot, an optimal clustering
    model has been found that faithfully represents the structure in the data.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图16.3。聚类任务中欠拟合和过拟合的外观。在左侧图表中，簇欠拟合（识别的簇比实际存在的簇少）。在右侧图表中，簇过拟合（真实簇被分解成更小的簇）。在中间图表中，找到了一个最优的聚类模型，它忠实地代表了数据中的结构。
- en: '![](fig16-3_alt.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](fig16-3_alt.jpg)'
- en: Avoiding over- and underfitting clustering problems is not straightforward.
    People have proposed many different methods for avoiding over- and underfitting,
    and they won’t all agree with one another for a particular problem. Many of these
    methods rely on the calculation of *internal cluster metrics*, which are statistics
    that aim to quantify the “quality” of a clustering result.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过拟合和欠拟合的聚类问题并不简单。人们提出了许多不同的方法来避免过拟合和欠拟合，并且它们对于特定问题不会完全一致。许多这些方法依赖于内部聚类指标的计算，这些指标是旨在量化聚类结果“质量”的统计数据。
- en: '|  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: What constitutes “good-quality” clusters is poorly defined and somewhat subjective,
    but people typically mean that each cluster is as compact as possible, while the
    distances between clusters are as large as possible.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: “高质量”簇的构成定义不明确，有些主观，但人们通常意味着每个簇尽可能紧凑，而簇之间的距离尽可能大。
- en: '|  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'These metrics are “internal” because they are calculated from the clustered
    data itself rather than by comparing the result to any external label or ground
    truth. A common approach to selecting the number of clusters is to train multiple
    clustering models over a range of cluster numbers and compare the cluster metrics
    for each model to help choose the best-fitting one. Three commonly used internal
    cluster metrics are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标是“内部”的，因为它们是从聚类数据本身计算出来的，而不是通过与任何外部标签或真实情况进行比较。选择聚类数量的常见方法是在一系列聚类数量范围内训练多个聚类模型，并比较每个模型的聚类指标，以帮助选择最佳拟合模型。以下三种常用的内部聚类指标如下：
- en: Davies-Bouldin index
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 戴维斯-博尔丁指数
- en: Dunn index
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dunn指数
- en: Pseudo F statistic
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伪F统计量
- en: Using the Davies-Bouldin index to evaluate clustering performance
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用戴维斯-博尔丁指数评估聚类性能
- en: The Davies-Bouldin index (named after its creators, David Davies and Donald
    Bouldin) quantifies the average separability of each cluster from its nearest
    counterpart. It does this by calculating the ratio of the within-cluster variance
    (also called the *scatter*) to the separation between cluster centroids (see [figure
    16.4](#ch16fig04)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 戴维斯-博尔丁指数（以其创造者David Davies和Donald Bouldin命名）量化了每个簇与其最近邻簇的平均分离度。它是通过计算簇内方差（也称为*散度*）与簇质心之间的分离度之比来实现的（参见[图16.4](#ch16fig04)）。
- en: Figure 16.4\. The Davies-Bouldin index calculates the intracluster (within-cluster)
    variance (left-side plot) and the distance between the centroids of each cluster
    (right-side plot). For each cluster, its nearest neighboring cluster is identified,
    and the sum of their intracluster variances is divided by the difference between
    their centroids. This value is calculated for each cluster, and the Davies-Bouldin
    index is the mean of these values.
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图16.4。戴维斯-博尔丁指数计算了簇内（聚类内）方差（左侧图表）和每个簇质心的距离（右侧图表）。对于每个簇，确定其最近邻簇，并将它们的簇内方差之和除以它们质心之间的差异。这个值对每个簇进行计算，戴维斯-博尔丁指数是这些值的平均值。
- en: '![](fig16-4.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](fig16-4.jpg)'
- en: If we fix the distance between clusters but make the cases within each cluster
    more spread out, the Davies-Bouldin index will get larger. Conversely, if we fix
    the within-cluster variance but move the clusters farther apart from each other,
    the index will get smaller. In theory, the smaller the value (which is bounded
    between zero and infinity), the better the separation between clusters. Boiled
    down into plain English, the Davies-Bouldin index quantifies the mean separability
    between each cluster and its most similar counterpart.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们固定聚类之间的距离，但使每个聚类内部的案例更加分散，戴维斯-博尔丁指数将变大。相反，如果我们固定聚类内的方差，但使聚类彼此之间距离更远，则指数将变小。理论上，值越小（介于零和无穷大之间），聚类之间的分离度越好。用简单的话来说，戴维斯-博尔丁指数量化了每个聚类与其最相似对应物之间的平均分离度。
- en: '|  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Calculating the Davies-Bouldin index**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算戴维斯-博尔丁指数**'
- en: It’s not necessary for you to memorize the formula for the Davies-Bouldin index
    (in fact, it’s reasonably complex). If you are interested, we can define the scatter
    within clusters as
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要记住 Davies-Bouldin 指数的公式（实际上，它相当复杂）。如果你感兴趣，我们可以将簇内的散布定义为
- en: '![](pg388-1.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](pg388-1.jpg)'
- en: where scatter*[k]* is a measure of the scatter within cluster *k*, *n[k]* is
    the number of cases in cluster *k*, *x[i]* is the *i*th case in cluster *k*, and
    *c[k]* is the centroid of cluster *k*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 scatter*[k]* 是簇 *k* 内部散布的度量，*n[k]* 是簇 *k* 中的案例数量，*x[i]* 是簇 *k* 中的第 *i* 个案例，而
    *c[k]* 是簇 *k* 的质心。
- en: The separation between clusters can be defined as
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 簇之间的分离可以定义为
- en: '![](pg388-2.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](pg388-2.jpg)'
- en: where separation*[j]*[,*k*] is a measure of the separation between clusters
    *j* and *k*, *c[j]* and *c[k]* are their respective centroids, and *N* is the
    total number of clusters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 separation*[j]*[,*k*] 是簇 *j* 和 *k* 之间分离的度量，*c[j]* 和 *c[k]* 是它们各自的质心，而 *N*
    是簇的总数。
- en: The ratio between the within-cluster scatter and the separation between two
    clusters is then calculated as
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算簇内散布与两个簇之间的分离比
- en: '![](pg388-3.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](pg388-3.jpg)'
- en: 'This ratio is calculated for all pairs of clusters, and for each cluster, the
    largest ratio between it and the other clusters is defined to be *R[k]*. The Davies-Bouldin
    index is then simply the mean of these largest ratios:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比率是针对所有簇对计算的，对于每个簇，将其与其他簇之间的最大比率定义为 *R[k]*。然后，Davies-Bouldin 指数就是这些最大比率的平均值：
- en: '![](pg388-4.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](pg388-4.jpg)'
- en: '|  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Using the Dunn index to evaluate clustering performance
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 Dunn 指数来评估聚类性能
- en: The Dunn index is another internal cluster metric that quantifies the ratio
    between the smallest distance between points in different clusters, and the largest
    distance within any of the clusters, referred to as the cluster’s *diameter* (see
    [figure 16.5](#ch16fig05)). These can be any distance metric but are commonly
    the Euclidean distance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Dunn 指数是另一个内部簇度量，它量化了不同簇中点之间最小距离与任何簇内最大距离（称为簇的 *直径*）之间的比率（参见 [图 16.5](#ch16fig05)）。这些可以是任何距离度量，但通常是欧几里得距离。
- en: Figure 16.5\. The Dunn index quantifies the ratio between the smallest distance
    between cases in different clusters (left-side plot) and the largest distance
    within a cluster (right-side plot).
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 16.5\. Dunn 指数量化了不同簇中案例之间最小距离（左侧图表）与簇内最大距离（右侧图表）之间的比率。
- en: '![](fig16-5.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](fig16-5.jpg)'
- en: The intuition here is that if we maintain the same diameter of our clusters
    but move the closest pair apart, the Dunn index will get larger. Conversely, if
    we maintain the same distance between cluster centroids but shrink the diameter
    of the clusters (by making the clusters denser), the Dunn index will also increase.
    As such, the number of clusters resulting in the largest Dunn index is the one
    that results in the largest minimum distance between clusters and the smallest
    maximum distance between cases within a cluster.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的直觉是，如果我们保持簇的直径不变但将最近的成对案例分开，Dunn 指数将会变大。相反，如果我们保持簇中心之间的距离不变但缩小簇的直径（通过使簇更密集），Dunn
    指数也会增加。因此，产生最大 Dunn 指数的簇数量是产生簇之间最大最小距离和簇内案例之间最小最大距离的簇。
- en: '|  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Calculating the Dunn index**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算 Dunn 指数**'
- en: It’s not necessary for you to memorize the formula for the Dunn index. If you
    are interested, we can define the Dunn index as
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要记住 Dunn 指数的公式。如果你感兴趣，我们可以将 Dunn 指数定义为
- en: '![](pg389.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](pg389.jpg)'
- en: where δ(*c[i]*,*c[j]*) represents all pairwise differences between cases in
    clusters *i* and *j*, and δ(*c[i]*) represents all pairwise differences between
    cases in cluster *k*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 δ(*c[i]*,*c[j]*) 表示簇 *i* 和 *j* 中案例之间的所有成对差异，而 δ(*c[i]*) 表示簇 *k* 中案例之间的所有成对差异。
- en: '|  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Using the pseudo F statistic to evaluate clustering performance
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用伪 F 统计量来评估聚类性能
- en: The pseudo F statistic is a ratio of the *between-cluster sum of squares* to
    the *within-cluster sum of squares* (see [figure 16.6](#ch16fig06)). The between-cluster
    sum of squares is the squared difference between each cluster centroid and the
    *grand centroid* (the centroid of the data as if it was all in one big cluster),
    weighted by the number of cases in that cluster, added up across each cluster.
    This is another way of measuring how separated the clusters are from each other
    (the farther the cluster centroids are from each other, the smaller the between
    sum of squares will be). The within-cluster sum of squares is the squared difference
    between each case and its cluster’s centroid, added up across each cluster. This
    is another way of measuring the variance or dispersion within each cluster (the
    denser each cluster is, the smaller the within-cluster sum of squares will be).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 伪F统计量是簇间平方和与簇内平方和的比率（见[图16.6](#ch16fig06)）。簇间平方和是每个簇质心与总体质心（数据如果全部在一个大簇中，其质心）之间的平方差，按该簇中的案例数量加权，并在每个簇中累加。这是衡量簇之间分离程度的一种方法（簇质心彼此越远，簇间平方和越小）。簇内平方和是每个案例与其簇质心之间的平方差，在每个簇中累加。这是衡量每个簇内方差或分散度的一种方法（簇越密集，簇内平方和越小）。
- en: Figure 16.6\. The pseudo F statistic is the ratio of the between-cluster sum
    of squares (right-side plot) to the within-cluster sum of squares (left-side plot).
    The grand centroid is shown as a square in the right-side plot.
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图16.6。伪F统计量是簇间平方和（右侧图表）与簇内平方和（左侧图表）的比率。总体质心在右侧图表中以正方形表示。
- en: '![](fig16-6.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](fig16-6.jpg)'
- en: Because the pseudo F statistic is also a ratio, if we maintain the same cluster
    variance but move the clusters farther apart, the pseudo F statistic will increase.
    Conversely, if we maintain the same separation between the cluster centroids but
    make the clusters more spread out, the pseudo F statistic will decrease. As such,
    the number of clusters that results in the largest pseudo F statistic is, in theory,
    the one that maximizes the separation of the clusters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因为伪F统计量也是一个比率，如果我们保持相同的簇方差但使簇彼此更远，伪F统计量将增加。相反，如果我们保持簇质心之间的相同分离度但使簇更加分散，伪F统计量将减少。因此，导致最大伪F统计量的簇数量，在理论上，是最大化簇之间分离度的那个。
- en: '|  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Calculating the pseudo F statistic**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算伪F统计量**'
- en: It’s not necessary for you to memorize the formula for the pseudo F statistic.
    If you are interested, we can define the pseudo F statistic as
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要记住伪F统计量的公式。如果您感兴趣，我们可以将伪F统计量定义为
- en: '![](pg390-1.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](pg390-1.jpg)'
- en: where SS*[between]* and SS*[within]* are calculated as
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: SS*[between]* 和 SS*[within]* 的计算方法如下
- en: '![](pg390-2.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](pg390-2.jpg)'
- en: where there are *N* clusters, *n[k]* is the number of cases in cluster *k*,
    *c[k]* is the centroid of cluster *k*, and *c[g]* is the grand centroid of all
    the cases.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，有 *N* 个簇，*n[k]* 是簇 *k* 中的案例数量，*c[k]* 是簇 *k* 的质心，*c[g]* 是所有案例的总体质心。
- en: '|  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: These are just three among the many commonly used internal cluster metrics,
    and at this point you might be wondering why there isn’t just one metric that
    tells us how well separated our clusters are. The reason is that these metrics
    will tend to agree with each other when we have very clear, well-defined clusters,
    but will start to disagree with each other as the solution becomes more ambiguous,
    with some of the metrics performing better than others in certain circumstances.
    For example, internal cluster metrics that rely on calculating sums of squares
    may prefer to select a number of clusters that results in clusters of equal diameter.
    This may not be an optimal number of clusters if the real clusters have very unequal
    diameters. As such, it’s often a good idea to consider multiple internal cluster
    metrics as evidence when choosing the number of clusters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是众多常用内部簇度量指标中的三个，此时你可能想知道为什么没有只有一个指标能告诉我们簇之间的分离程度。原因是，当我们有非常清晰、定义良好的簇时，这些指标往往会相互一致，但当解决方案变得更加模糊时，它们将开始相互不一致，某些指标在特定情况下表现优于其他指标。例如，依赖于计算平方和的内部簇度量指标可能更喜欢选择直径相等的簇的数量。如果真实簇的直径非常不均匀，这可能不是最佳簇数量。因此，在确定簇数量时，考虑多个内部簇度量指标作为证据通常是一个好主意。
- en: So internal cluster metrics like these can help us find the optimal number of
    clusters. But there is still always a danger that we might overfit the training
    data by *overclustering*. One approach to avoid overclustering is to take multiple
    bootstrap samples (sampling cases with replacement) of the data, apply the clustering
    algorithm to each sample, and compare how well the cluster memberships agree between
    samples. If there is high stability (in other words, the clustering result is
    *stable* between samples), then we have more confidence that we are not fitting
    the noise in the data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像这样的内部聚类指标可以帮助我们找到最佳聚类数量。但仍然始终存在一种风险，我们可能会通过过度聚类来过度拟合训练数据。避免过度聚类的一种方法是从数据中抽取多个自助样本（有放回地抽取案例），对每个样本应用聚类算法，并比较样本之间聚类成员资格的一致性。如果稳定性很高（换句话说，聚类结果在样本之间是*稳定的*），那么我们更有信心我们不是在拟合数据中的噪声。
- en: For clustering algorithms that are able to predict the clusters of new data,
    like the k-means algorithms, another approach is to use a cross-validation-like
    procedure. This involves splitting the data into training and test sets (using
    k-fold, for example), training the clustering algorithm on the training set, predicting
    the cluster membership of the cases in the test set, and calculating internal
    cluster metrics for the predicted clusters. This approach has the benefit of allowing
    us both to test cluster stability and to calculate the metric on data the algorithm
    never saw. This is the approach we’ll use to select the optimal number of clusters
    using k-means in this chapter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于能够预测新数据聚类的聚类算法，如k-means算法，另一种方法是使用类似于交叉验证的程序。这涉及到将数据分为训练集和测试集（例如使用k折），在训练集上训练聚类算法，预测测试集中案例的聚类成员资格，并计算预测聚类的内部聚类指标。这种方法的好处是，它既允许我们测试聚类稳定性，又可以在算法从未见过的数据上计算指标。这是我们将在本章中使用k-means选择最佳聚类数量的方法。
- en: '|  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: With k-means clustering, new data can be projected onto an existing clustering
    model by simply assigning the new cases to the cluster of the nearest centroid.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means聚类中，可以通过简单地将新案例分配到最近的质心所在的聚类来将新数据投影到现有的聚类模型上。
- en: '|  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 16.2.4\. Tuning k and the algorithm choice for our k-means model
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.4. 调整k和k-means模型的选择算法
- en: 'In this section, I’ll show you how we can tune *k* (the number of clusters)
    and our choice of k-means algorithm, using a cross-validation-like approach with
    internal cluster metrics applied to the predicted clusters. Let’s start by defining
    our hyperparameter search space using the `makeParamSet()` function. We define
    two discrete hyperparameters over which to search for values: `centers`, which
    is the number of clusters the algorithm will search for (*k*), and `algorithm`,
    which specifies which of the three algorithms we will use to fit the model.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示我们可以如何使用类似于交叉验证的方法，通过将内部聚类指标应用于预测聚类来调整*k*（聚类数量）和我们的k-means算法选择。让我们首先使用`makeParamSet()`函数定义我们的超参数搜索空间。我们定义了两个离散超参数，我们将搜索它们的值：`centers`，这是算法将搜索的聚类数量（*k*），以及`algorithm`，它指定我们将使用哪个算法来拟合模型。
- en: '|  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Just as we’ve seen before, we can use `getParamSet(kMeans)` to find all the
    hyperparameters available to us.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，我们可以使用`getParamSet(kMeans)`来找到我们可用的所有超参数。
- en: '|  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We then define our search method as a grid search (to try every combination
    of hyperparameters) and define our cross-validation approach as 10-fold.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将搜索方法定义为网格搜索（尝试每个超参数组合），并将交叉验证方法定义为10折。
- en: Listing 16.5\. Defining how the hyperparameters will be tuned
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表16.5. 定义超参数的调整方式
- en: '[PRE5]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we’ve defined our search space, let’s perform the tuning. To use the
    Davies-Bouldin index and the pseudo F statistic performance measures, you’ll first
    need to install the clusterSim package.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了搜索空间，让我们进行调优。要使用Davies-Bouldin指数和伪F统计性能度量，您首先需要安装clusterSim包。
- en: '|  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Two other internal cluster metrics are implemented by mlr: silhouette and G2
    (use `listMeasures("cluster")` to list the available metrics). Both metrics are
    more computationally expensive to compute, so we won’t use them here, but they
    are additional metrics to help us decide on an appropriate number of clusters.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: mlr实现了另外两个内部聚类指标：轮廓和G2（使用`listMeasures("cluster")`列出可用的指标）。这两个指标的计算成本更高，所以我们在这里不会使用它们，但它们是额外的指标，帮助我们决定合适的聚类数量。
- en: '|  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'To perform tuning, we use the `tuneParams()` function. Because we didn’t use
    this function during the dimension-reduction part of the book, let’s refresh our
    memory of the arguments:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行调整，我们使用 `tuneParams()` 函数。因为我们没有在本书的降维部分使用此函数，让我们刷新一下对这个函数参数的记忆：
- en: The first argument is the name of the learner.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数是学习器的名称。
- en: The `task` argument is the name of our clustering task.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` 参数是我们聚类任务的名称。'
- en: The `resampling` argument is the name of our cross-validation strategy.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resampling` 参数是我们交叉验证策略的名称。'
- en: The `par.set` argument is our hyperparameter search space.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`par.set` 参数是我们的超参数搜索空间。'
- en: The `control` argument is our search method.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`control` 参数是我们的搜索方法。'
- en: The `measures` argument allows us to define which performance measures we want
    to calculate for each iteration of the search. Here, we ask for the Davies-Bouldin
    index (`db`), Dunn index (`dunn`), and pseudo F statistic (`G1`), in that order.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`measures` 参数允许我们定义我们想要为搜索的每一迭代计算哪些性能度量。在这里，我们请求 Davies-Bouldin 指数（`db`）、Dunn
    指数（`dunn`）和伪 F 统计量（`G1`），按此顺序。'
- en: '|  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: We can supply a list of as many performance metrics as we want. All of them
    will be calculated for each iteration of the search, but the combination of hyperparameters
    that optimizes the value of the *first* metric in the list will always be returned
    from tuning. The mlr package also “knows” which metrics should be maximized and
    which ones should be minimized for best performance.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提供我们想要的任何性能度量标准列表。所有这些度量标准都将为搜索的每一迭代计算，但优化列表中第一个度量标准值的超参数组合将始终从调整中返回。mlr
    包还“知道”哪些度量标准应该最大化，哪些应该最小化以获得最佳性能。
- en: '|  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Just to reiterate: when we perform the tuning, for each combination of hyperparameters,
    the data will be split into 10 folds, and the k-means algorithm will be trained
    on the training set of each fold. The cases in each test set will be assigned
    to their nearest cluster centroid, and the internal cluster metric will be calculated
    on these test set clusters. Calling the result of the tuning shows us that Lloyd’s
    algorithm with four clusters gave the lowest (most optimal) Davies-Bouldin index.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 只是为了重申：当我们执行调整时，对于每个超参数组合，数据将被分成 10 个折叠，k-means 算法将在每个折叠的训练集上训练。测试集中的案例将被分配到最近的簇中心，内部簇度量标准将在这些测试集簇上计算。调用调整的结果显示，具有四个簇的
    Lloyd 算法给出了最低（最优化）的 Davies-Bouldin 指数。
- en: Listing 16.6\. Performing the tuning experiment
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 16.6\. 执行调整实验
- en: '[PRE6]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: At the end of the tuning process, did you get the warning `did not converge
    in 100 iterations`? This is how to tell whether you’ve set the `iter.max` argument
    too low in your learner definition. Your choices are to either choose to accept
    the result as is, which may or may not be a near-optimal solution, or, if you
    have the computational budget, increase `iter.max`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整过程的最后，你是否收到了“在 100 次迭代后没有收敛”的警告？这是判断你在学习器定义中是否将 `iter.max` 参数设置得太低的方法。你的选择是接受结果，这可能是也可能不是接近最优的解决方案，或者如果你有计算预算，增加
    `iter.max`。
- en: '|  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: Change our `kmeans` definition (created in [listing 16.4](#ch16ex04)) such that
    the value of `iter.max` is 200\. Rerun the tuning procedure in [listing 16.6](#ch16ex06).
    Does the error about not converging disappear?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 修改我们的 `kmeans` 定义（在[代码列表 16.4](#ch16ex04) 中创建），将 `iter.max` 的值设置为 200。重新运行[代码列表
    16.6](#ch16ex06)中的调整程序。关于无法收敛的错误消失了吗？
- en: '|  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: To get a better understanding of how our three internal metrics vary with both
    cluster number and algorithm choice, let’s plot the tuning process. Recall that
    to do this, we first need to extract the tuning data from our tuning result using
    the `generateHyperParsEffectData()` function. Call the `$data` component from
    the `kMeansTuningData` object so you can see how it’s structured (I won’t print
    it here, for the sake of space).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们的三个内部度量标准如何随着簇数量和算法选择的变化而变化，让我们绘制调整过程。回想一下，为了做到这一点，我们首先需要使用 `generateHyperParsEffectData()`
    函数从调整结果中提取调整数据。从 `kMeansTuningData` 对象中调用 `$data` 组件，以便您可以查看其结构（这里我不会打印它，为了节省空间）。
- en: '|  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'Notice that we have a metric we didn’t ask for: `exec.time`, which records
    how long it took to train a model with each combination of hyperparameters, in
    seconds.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有一个我们没有请求的度量标准：`exec.time`，它记录了使用每个超参数组合训练模型所需的时间，以秒为单位。
- en: '|  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s plot this data such that we have a different facet per performance metric
    and a different line per algorithm. To do this, we first need to gather the data
    such that the name of each performance metric is in one column and the value of
    the metric is in another column. We do this using the `gather()` function, naming
    the key column `"Metric"` and the value column `"Value"`. Because we only want
    *these* columns gathered, we supply a vector of columns we don’t want to gather.
    Print the new, gathered dataset to make sure you understand what we did. Having
    the data in this format allows us to facet by algorithm and plot separate lines
    for each metric.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这些数据，以便每个性能指标有一个不同的面，每个算法有一条不同的线。为此，我们首先需要收集数据，使得每个性能指标的名字在一列，而指标值在另一列。我们使用`gather()`函数，将键列命名为`"Metric"`，将值列命名为`"Value"`。因为我们只想收集*这些*列，所以我们提供了一个我们不希望收集的列的向量。打印新的收集数据集以确保你理解我们所做的。以这种格式拥有数据允许我们按算法进行分面，并为每个指标绘制单独的线条。
- en: To plot the data, we use the `ggplot()` function, mapping `centers` (the number
    of clusters) and `Value` to the x and y aesthetics, respectively. By mapping `algorithm`
    to the `col` aesthetic, separate `geom_line()` and `geom_point()` layers will
    be drawn for each algorithm (with different colors). We use the `facet_wrap()`
    function to draw a separate subplot for each performance metric, setting the `scales
    = "free_y"` argument to allow different y-axes for each facet (as they have different
    scales). Finally, we add the `geom_line()` and `geom_point()` layers and a theme.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制数据，我们使用`ggplot()`函数，将`centers`（聚类数量）映射到x美学，将`Value`映射到y美学。通过将`algorithm`映射到`col`美学，将为每个算法（不同颜色）绘制不同的`geom_line()`和`geom_point()`层。我们使用`facet_wrap()`函数为每个性能指标绘制一个单独的子图，设置`scales
    = "free_y"`参数以允许每个面有不同的y轴（因为它们的刻度不同）。最后，我们添加`geom_line()`和`geom_point()`层以及一个主题。
- en: Listing 16.7\. Plotting the tuning experiment
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表16.7\. 绘制调整实验
- en: '[PRE7]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The resulting plot is shown in [figure 16.7](#ch16fig07). Each facet shows a
    different performance metric, and each separate line shows one of the three algorithms.
    Notice that the clustering models with four clusters (centers), the Davies-Bouldin
    index is minimized, and the Dunn index and pseudo F statistic (G1) are maximized.
    Because lower values of the Davies-Bouldin index and higher values of the Dunn
    index and pseudo F statistic indicate (in theory) better-separated clusters, all
    three of the internal metrics agree with each other that four is the optimal number
    of clusters for this dataset. There is also very little disagreement between the
    different algorithms, particularly at the optimal value of four clusters.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表显示在[图16.7](#ch16fig07)中。每个面展示了一个不同的性能指标，而每条单独的线表示三种算法之一。请注意，具有四个聚类（中心）的聚类模型中，Davies-Bouldin指数最小化，而Dunn指数和伪F统计量（G1）最大化。因为Davies-Bouldin指数的较低值和Dunn指数以及伪F统计量的较高值（理论上）表示聚类分离得更好，所以这三个内部指标都一致认为四个是此数据集的最佳聚类数量。不同算法之间也存在很少的分歧，尤其是在四个聚类的最优值时。
- en: Figure 16.7\. Plotting our tuning process. Each subplot shows the values of
    a different internal cluster metric. The different lines indicate the performance
    of each of the three different algorithms.
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图16.7\. 绘制我们的调整过程。每个子图显示了不同的内部聚类指标。不同的线条表示三种不同算法的性能。
- en: '![](fig16-7_alt.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](fig16-7_alt.jpg)'
- en: The greatest difference between the algorithms is their training time. Notice
    that MacQueen’s algorithm is consistently faster than either of the others. This
    is due to the algorithm updating its centroids more frequently than Lloyd’s and
    having to recompute distances less often than Hartigan-Wong. The Hartigan-Wong
    algorithm seems to be the most computationally intense at low cluster numbers
    but overtakes Lloyd’s algorithm as the number of clusters increases beyond seven.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 算法之间最大的差异是它们的训练时间。请注意，MacQueen算法始终比其他任何一种都要快。这是因为该算法比Lloyd算法更频繁地更新其质心，并且比Hartigan-Wong算法更少地重新计算距离。Hartigan-Wong算法在低聚类数量时似乎计算量最大，但随着聚类数量超过七个，它超过了Lloyd算法。
- en: '|  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The tuning process selected Lloyd’s algorithm because its Davis-Bouldin index
    was slightly smaller than for the other algorithms. For very large datasets, computation
    speed may be more important to you than a performance increase this small, in
    which case you might prefer to select MacQueen’s algorithm due to its shorter
    training time.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 调整过程选择了Lloyd算法，因为它的Davis-Bouldin指数略小于其他算法。对于非常大的数据集，计算速度可能对你来说比这种小的性能提升更重要，在这种情况下，你可能会更喜欢选择MacQueen算法，因为它有更短的训练时间。
- en: '|  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 16.2.5\. Training the final, tuned k-means model
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.5\. 训练最终的调整后的k-means模型
- en: In this section, we’ll use our tuned hyperparameters to train our final clustering
    model. You’ll notice that we’re *not* going to use nested cross-validation to
    cross-validate the whole model-building process. While the *k* means algorithm
    is able to predict cluster membership for new data, it isn’t typically used as
    a predictive technique. Instead, we might use k-means to help us better define
    classes in our dataset, which we can later use to build classification models.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用调整后的超参数来训练我们的最终聚类模型。你会注意到我们**不会**使用嵌套交叉验证来交叉验证整个模型构建过程。虽然k均值算法能够预测新数据的聚类成员资格，但它通常不作为预测技术使用。相反，我们可能会使用k-means来帮助我们更好地定义数据集中的类别，我们可以在以后使用这些类别来构建分类模型。
- en: Let’s start by creating a k-means learner that uses our tuned hyperparameter
    values, using the `setHyperPars()` function. We then train this tuned model on
    our `gvhdTask` using the `train()` function and use the `getLearnerModel()` function
    to extract the model data so we can plot the clusters. Print the model data by
    calling `kMeansModel-Data`, and examine the output; it contains a lot of useful
    information. By extracting the `$iter` component of the object, we can see that
    it took only three iterations for the algorithm to converge (far fewer than `iter.max`).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个使用调整后的超参数值的k-means学习器，使用`setHyperPars()`函数。然后我们使用`train()`函数在`gvhdTask`上训练这个调整后的模型，并使用`getLearnerModel()`函数提取模型数据以便绘制聚类。通过调用`kMeansModel-Data`打印模型数据，并检查输出；它包含大量有用的信息。通过提取对象的`$iter`组件，我们可以看到算法只用了三次迭代就收敛了（远少于`iter.max`）。
- en: Listing 16.8\. Training a model with the tuned hyperparameters
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表16.8\. 使用调整后的超参数训练模型
- en: '[PRE8]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finding the optimal number of clusters is not a well-defined problem; so, although
    internal metrics give evidence as to the correct number of clusters, you should
    still always try to validate your cluster model visually, to understand whether
    the result you’re getting is sensible (at the very least). This may seem subjective,
    and it is, but it’s much better for you to use your expert judgment than to rely
    solely on internal metrics. We can do this by plotting the data (as in [figure
    16.2](#ch16fig02)) but coloring each case by its cluster membership.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳聚类数量不是一个定义明确的问题；因此，尽管内部指标提供了关于正确聚类数量的证据，但你仍然应该始终尝试通过视觉验证你的聚类模型，以了解你得到的结果是否合理（至少）。这可能会显得主观，确实是这样的，但使用你的专业判断比完全依赖内部指标要好得多。我们可以通过绘制数据（如[图16.2](#ch16fig02)所示）并按其聚类成员资格着色来实现这一点。
- en: '|  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If the correct number of clusters is difficult for you to determine, it could
    be there simply aren’t well-defined clusters in the data, or you may need to do
    further exploration, including generating more data. It may be worth trying a
    different clustering method: for example, one that doesn’t find spherical clusters
    like k-means does, or one which can exclude outliers (like DBSCAN, which you’ll
    meet in [chapter 18](kindle_split_031.html#ch18)).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果确定正确的聚类数量对你来说很困难，可能是因为数据中根本不存在定义良好的聚类，或者你可能需要进行进一步的探索，包括生成更多数据。尝试不同的聚类方法可能值得考虑：例如，不寻找像k-means那样球形聚类的算法，或者可以排除异常值（如你将在第18章中遇到的DBSCAN）。
- en: '|  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: To do this, we first add the cluster membership of each case as a new column
    in our `gvhdTib` tibble, using the `mutate()` function. We extract the vector
    of cluster memberships from the `$cluster` component of the model data and turn
    this into a factor using the `as.factor()` function, to ensure that a discrete
    color scheme is applied during plotting.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先使用`mutate()`函数将每个案例的聚类成员资格添加为`gvhdTib` tibble的新列。我们从模型数据的`$cluster`组件中提取聚类成员资格的向量，并使用`as.factor()`函数将其转换为因子，以确保在绘图时应用离散的颜色方案。
- en: We then use `ggpairs()` to plot all variables against each other, mapping `kMeansCluster`
    to the color aesthetic. We use the `upper` argument to plot density plots on plots
    above the diagonal and apply the black-and-white theme.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用 `ggpairs()` 函数来绘制所有变量之间的对比图，将 `kMeansCluster` 映射到颜色美学。我们使用 `upper` 参数在主对角线以上的图表上绘制密度图，并应用黑白主题。
- en: Listing 16.9\. Plotting the clusters using `ggpairs()`
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 16.9\. 使用 `ggpairs()` 绘制簇
- en: '[PRE9]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Figure 16.8\. `ggpairs()` plot with the k-means cluster membership mapped to
    the color aesthetic. Box plots and histograms show how the values of the continuous
    variables vary between clusters.
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 16.8\. 将 k-means 簇成员资格映射到颜色美学的 `ggpairs()` 图。箱线图和直方图显示了连续变量值在簇之间的变化情况。
- en: '![](fig16-8_alt.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](fig16-8_alt.jpg)'
- en: 'The resulting plot is shown in [figure 16.8](#ch16fig08). To the eye, it looks
    like our k-means model does a pretty good job of capturing the structure in the
    data overall. But look at the plot of CD8 versus CD4: cluster three appears to
    be split. This suggests that either we have *underclustered* our data, or these
    cases have been assigned to the wrong cluster; or perhaps they are simply outlying
    cases, the importance of which is overstated by the density plot.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在 [图 16.8](#ch16fig08)。从直观上看，我们的 k-means 模型在整体上很好地捕捉了数据中的结构。但看看 CD8 与
    CD4 的对比图：第三个簇似乎被分割了。这表明我们可能对数据进行得 **过聚类**，或者这些案例被分配到了错误的簇中；或者也许它们仅仅是异常值，其重要性被密度图高估了。
- en: 16.2.6\. Using our model to predict clusters of new data
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.6\. 使用我们的模型预测新数据的聚类
- en: In this section, I’ll show you how we can use an existing k-means model to predict
    cluster membership of new data. As I mentioned already, clustering techniques
    are not intended to be used for predicting classes of data—we have classification
    algorithms that excel at that. But the k-means algorithm *can* take new data and
    output the clusters to which the new cases are closest. This can be useful when
    you are still exploring and trying to understand the structure in your data, so
    let me demonstrate how.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用现有的 k-means 模型来预测新数据的簇成员资格。正如我之前提到的，聚类技术并不打算用于预测数据类别——我们有一些在分类方面表现卓越的算法。但
    k-means 算法 **可以**处理新数据并将新案例最接近的簇输出出来。这在您仍在探索并试图理解数据中的结构时非常有用，所以让我来演示一下。
- en: Let’s start by creating a tibble containing the data for a new case, including
    a value for each variable in the dataset on which we trained the model. Because
    we scaled the training data, we need to scale the values for this new case. Remember
    that it’s important to scale new data we pass through a model according to the
    mean and standard deviation of the data used to train the model. The easiest way
    to do this is to use the `attr()` function to extract the `center` and `scale`
    attributes from the scaled data. Because the `scale()` function returns an object
    of class `matrix` (and the `predict()` function will throw an error if we give
    it a matrix), we need to pipe the scaled data into the `as_tibble()` function
    to turn it back into a tibble.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个包含新案例数据的 tibble 开始，包括模型训练数据集中每个变量的值。因为我们已经对训练数据进行了缩放，所以我们需要对新案例的值进行缩放。记住，根据用于训练模型的数据的均值和标准差缩放通过模型传递的新数据非常重要。最简单的方法是使用
    `attr()` 函数从缩放数据中提取 `center` 和 `scale` 属性。因为 `scale()` 函数返回一个类为 `matrix` 的对象（如果给它一个矩阵，`predict()`
    函数将抛出错误），我们需要将缩放数据通过 `as_tibble()` 函数管道，将其转换回 tibble。
- en: To predict which cluster the new case belongs to, we simply call the `predict()`
    function, supplying the model as the first argument and the new case as the `newdata`
    argument. We can see from the output that this new case is closest to the centroid
    of cluster 2.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要预测新案例属于哪个簇，我们只需调用 `predict()` 函数，将模型作为第一个参数，将新案例作为 `newdata` 参数。我们可以从输出中看到，这个新案例最接近簇
    2 的质心。
- en: Listing 16.10\. Predicting cluster membership of new data
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 16.10\. 预测新数据的簇成员资格
- en: '[PRE10]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You’ve now learned how to apply k-means clustering to your data. In the next
    chapter, I’ll introduce *hierarchical clustering*, a set of clustering methods
    that help reveal a hierarchy in our data. I suggest that you save your .R file,
    because we’re going to continue using the same dataset in the next chapter. This
    is so we can compare the performance of k-means and hierarchical clustering on
    the same dataset.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经学会了如何将 k-means 聚类应用于你的数据。在下一章中，我将介绍 *层次聚类*，这是一组有助于揭示数据中层次结构的聚类方法。我建议你保存你的
    .R 文件，因为我们将在下一章继续使用相同的数据集。这样我们可以比较 k-means 和层次聚类在相同数据集上的性能。
- en: 16.3\. Strengths and weaknesses of k-means clustering
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3\. k-means 聚类的优势和劣势
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    k-means clustering will perform well for you.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于给定的任务很难判断哪种算法会表现良好，但以下是一些优势和劣势，这将帮助你决定 k-means 聚类是否适合你。
- en: 'The strengths of k-means clustering are as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类的优势如下：
- en: Cases can move between clusters at each iteration until a stable result is found.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例可以在每次迭代中在聚类之间移动，直到找到稳定的结果。
- en: It may be faster to compute than other algorithms when there are many variables.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有多个变量时，它可能比其他算法计算更快。
- en: It is quite simple to implement.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的实现相当简单。
- en: 'The weaknesses of k-means clustering are these:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类的劣势如下：
- en: It cannot natively handle categorical variables. This is because calculating
    the Euclidean distance on a categorical feature space isn’t meaningful.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法原生地处理分类变量。这是因为计算分类特征空间上的欧几里得距离没有意义。
- en: It cannot select the optimal number of clusters.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不能选择最佳数量的聚类。
- en: It is sensitive to data on different scales.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对不同尺度的数据敏感。
- en: Due to the randomness of the initial centroids, clusters may vary slightly between
    runs.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于初始质心的随机性，聚类可能在运行之间略有不同。
- en: It is sensitive to outliers.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对异常值敏感。
- en: It preferentially finds spherical clusters of equal diameter, even if the underlying
    data doesn’t fit this description.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它优先找到等直径的球形聚类，即使基础数据不符合这种描述。
- en: '|  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: Cluster the GvHD.pos dataset in the same way we did with the GvHD.control dataset.
    Is the choice of cluster number as straightforward? You may need to manually supply
    a value for the `centers` argument, rather than rely on the output of the tuning
    procedure.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以与我们处理 GvHD.control 数据集相同的方式对 GvHD.pos 数据集进行聚类。聚类数量的选择是否同样简单？你可能需要手动提供 `centers`
    参数的值，而不是依赖于调整过程的输出。
- en: '|  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: Clustering is an unsupervised machine learning technique concerned with finding
    sets of cases in a dataset that are more similar to each other than to cases in
    other sets.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类是一种无监督的机器学习技术，它关注于在数据集中找到彼此之间比其他集合中的案例更相似的案例集。
- en: K-means clustering involves the creation of randomly placed centroids that iteratively
    move toward the center of clusters in a dataset.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 聚类涉及创建随机放置的质心，这些质心会迭代地移动到数据集中聚类的中心。
- en: The three most commonly used k-means algorithms are Lloyd’s, Mac-Queen’s, and
    Hartigan-Wong.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常用的三种 k-means 算法是 Lloyd 的、Mac-Queen 的和 Hartigan-Wong 的。
- en: The number of clusters for k-means needs to be selected by the user. This can
    be done graphically, and by combining internal cluster metrics with cross-validation
    and/or bootstrapping.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 的聚类数量需要由用户选择。这可以通过图形化方式完成，并且可以通过结合内部聚类指标与交叉验证和/或自助法来完成。
- en: Solutions to exercises
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题的解答
- en: 'Increase the `iter.max` of our k-means learner to 200:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的 k-means 学习者的 `iter.max` 增加到 200：
- en: '[PRE11]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Use k-means to cluster the GvHD.pos dataset:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 k-means 对 GvHD.pos 数据集进行聚类：
- en: '[PRE12]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Chapter 17\. Hierarchical clustering
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 17 章\. 层次聚类
- en: '*This chapter covers*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding hierarchical clustering
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解层次聚类
- en: Using linkage methods
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用链接方法
- en: Measuring the stability of a clustering result
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量聚类结果的不稳定性
- en: In the previous chapter, we saw how k-means clustering finds *k* centroids in
    the feature space and iteratively updates them to find a set of clusters. Hierarchical
    clustering takes a different approach and, as its name suggests, can learn a hierarchy
    of clusters in a dataset. Instead of getting a “flat” output of clusters, hierarchical
    clustering gives us a tree of clusters within clusters. As a result, hierarchical
    clustering provides more insight into complex grouping structures than flat clustering
    methods like k-means.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了k-means聚类如何在特征空间中找到*k*个质心，并迭代更新它们以找到一组集群。层次聚类采用不同的方法，正如其名称所暗示的，可以在数据集中学习集群的层次结构。与提供“平坦”的集群输出不同，层次聚类给我们提供了一个集群内部的集群树。因此，层次聚类比k-means这样的平坦聚类方法提供了对复杂分组结构的更多洞察。
- en: The tree of clusters is built iteratively by calculating the distance between
    each case or cluster, and every other case or cluster in the dataset at each step.
    Either the case/cluster pair that are most similar to each other are merged into
    a single cluster, or sets of cases/clusters that are most dissimilar from each
    other are split into separate clusters, depending on the algorithm. I’ll introduce
    both approaches to you later in the chapter.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代计算每个案例或集群与数据集中每个其他案例或集群之间的距离来构建集群树。根据算法，要么将彼此最相似的案例/集群对合并成一个集群，要么将彼此最不相似的案例/集群集分割成单独的集群。我将在本章后面向您介绍这两种方法。
- en: By the end of this chapter, I hope you’ll understand how hierarchical clustering
    works. We’ll apply this method to the GvHD data from the last chapter to help
    you understand how hierarchical clustering differs from k-means. If you no longer
    have the `gvhdScaled` object defined in your global environment, just rerun [listings
    16.1](kindle_split_029.html#ch16ex01) and [16.2](kindle_split_029.html#ch16ex02).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望您能理解层次聚类的工作原理。我们将应用这种方法来处理上一章的GvHD数据，以帮助您了解层次聚类与k-means的不同之处。如果您在全局环境中不再定义`gvhdScaled`对象，只需重新运行[列表16.1](kindle_split_029.html#ch16ex01)和[16.2](kindle_split_029.html#ch16ex02)。
- en: 17.1\. What is hierarchical clustering?
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.1. 什么是层次聚类？
- en: In this section, I’ll give you a deeper understanding of what hierarchical clustering
    is and how it differs from k-means. I’ll show you the two different approaches
    we can take to perform hierarchical clustering, how to interpret a graphical representation
    of the learned hierarchy, and how to choose the number of clusters to retain.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将为您更深入地了解层次聚类是什么以及它与k-means的不同之处。我将向您展示我们可以采取的两种不同的方法来执行层次聚类，如何解释学习到的层次结构的图形表示，以及如何选择要保留的集群数量。
- en: 'When we looked at k-means clustering in the last chapter, we only considered
    a single level of clustering. But sometimes, hierarchies exist in our dataset
    that clustering at a single, flat level is unable to highlight. For example, imagine
    that we were looking at clusters of instruments in an orchestra. At the highest
    level, we could place each instrument into one of four different clusters:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在上一章查看k-means聚类时，我们只考虑了单层聚类。但有时，我们的数据集中存在层次结构，单层平坦的聚类无法突出显示。例如，想象一下我们正在查看管弦乐队的乐器集群。在最高层，我们可以将每个乐器放入四个不同的集群之一：
- en: Percussion
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打击乐器
- en: Brass
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铜管乐器
- en: Woodwinds
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 木管乐器
- en: Strings
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弦乐器
- en: 'But we could then further split each of these clusters into sub-clusters based
    on the way they are played:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以根据它们演奏的方式进一步将这些集群细分为子集群：
- en: Percussion
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打击乐器
- en: Played with a mallet
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用木槌演奏
- en: Played by hand
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手吹
- en: Brass
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铜管乐器
- en: Valve
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阀门
- en: Slide
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑音
- en: Woodwinds
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 木管乐器
- en: Reeded
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哨片的
- en: Non-reeded
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无哨片的
- en: Strings
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弦乐器
- en: Plucked
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹拨
- en: Bowed
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弓弦乐器
- en: 'Next, we could further split this level of clusters into sub-clusters based
    on the sounds they make:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以根据它们发出的声音进一步将这一层次的集群细分为子集群：
- en: Percussion
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打击乐器
- en: Played with a mallet
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用木槌演奏
- en: Timpani
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定音鼓
- en: Gong
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锣
- en: Played by hand
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手吹
- en: Hand cymbals
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手镲
- en: Tambourine
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铃鼓
- en: Brass
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 铜管乐器
- en: Valve
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阀门
- en: Trumpet
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小号
- en: French horn
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法式号
- en: Slide
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑音
- en: Trombone
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长号
- en: Woodwinds
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 木管乐器
- en: Reeded
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哨片的
- en: Clarinet
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单簧管
- en: Bassoon
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴松管
- en: Non-reeded
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无哨片的
- en: Flute
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长笛
- en: Piccolo
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短笛
- en: Strings
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弦乐器
- en: Plucked
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹拨
- en: Harp
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竖琴
- en: Bowed
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弓弦乐器
- en: Violin
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小提琴
- en: Cello
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大提琴
- en: Notice that we have formed a hierarchy where there are clusters of instruments
    within other clusters, going all the way from a very high-level clustering down
    to each individual instrument. A common way to visualize hierarchies like this
    is with a graphical representation called a *dendrogram*. A possible dendrogram
    for our orchestra hierarchy is shown in [figure 17.1](#ch17fig01).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们形成了一个层次结构，其中包含其他簇中的乐器簇，从非常高级的聚类一直到底层的每个单独的乐器。可视化此类层次结构的一种常见方法是使用称为*树状图*的图形表示。我们管弦乐队层次结构的一个可能的树状图如图17.1所示[图17.1](#ch17fig01)。
- en: Figure 17.1\. Dendrogram showing an imaginary clustering of instruments in an
    orchestra. Horizontal lines indicate the merging of separate clusters. The height
    of a merge indicates the similarity between the clusters (lower merge, higher
    similarity).
  id: totrans-359
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图17.1\. 显示管弦乐队中乐器假想聚类的树状图。水平线表示簇的合并。合并的高度表示簇之间的相似性（合并高度越低，相似性越高）。
- en: '![](fig17-1.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图17.1](fig17-1.jpg)'
- en: Notice that at the bottom of the dendrogram, each instrument is represented
    by its own vertical line, and at this level, each instrument is considered to
    be *in a cluster of its own*. As we move up the hierarchy, instruments in the
    same cluster are connected by a horizontal line. The height at which clusters
    merge like this is inversely proportional to how similar the clusters are to each
    other. For example, I have (subjectively) drawn this dendrogram to suggest that
    the piccolo and flute are more similar to each other than how similar the bassoon
    and clarinet are to each other.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在树状图的底部，每种乐器都由其自己的垂直线表示，在这个层面上，每种乐器都被认为是*位于自己的簇中*。随着我们向上移动层次结构，同一簇中的乐器通过水平线连接。簇合并的这种高度与簇之间的相似性成反比。例如，我（主观上）绘制了这个树状图，以表明短笛和小号之间的相似性比双簧管和长笛之间的相似性更大。
- en: Typically, when finding a hierarchy in data like this, one end of the dendrogram
    displays every case in its own cluster; these clusters merge upward until eventually,
    all the cases are placed into a single cluster. As such, I’ve indicated the position
    of our strings, woodwinds, brass, and percussion clusters, but I have continued
    clustering these clusters until there is only one cluster containing all the cases.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们寻找此类数据中的层次结构时，树状图的一端显示每个案例都位于自己的簇中；这些簇向上合并，最终所有案例都被放置在单个簇中。因此，我已经指出了我们的字符串、木管乐器、铜管乐器和打击乐器簇的位置，但我已经继续对这些簇进行聚类，直到只有一个包含所有案例的簇。
- en: The purpose of hierarchical clustering algorithms, therefore, is to learn this
    hierarchy of clusters in a dataset. The main benefit of hierarchical clustering
    over k-means is that we get a much finer-grained understanding of the structure
    of our data, and this approach is often able to reconstruct real hierarchies in
    nature. For example, imagine that we sequence the genomes (all the DNA) of all
    breeds of dog. We can safely assume that the genome of a breed will be more similar
    to the genome of the breed(s) it was derived from than it is to the genomes of
    breeds it was not derived from. If we apply hierarchical clustering to this data,
    the hierarchy, which can be visualized as a dendrogram, can be directly interpreted
    as showing which breeds were derived from other breeds.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，层次聚类算法的目的是在数据集中学习这种聚类层次结构。与k-means聚类相比，层次聚类的优点在于我们能够获得对数据结构更细致的理解，并且这种方法通常能够重建自然界中的真实层次结构。例如，假设我们测序了所有犬种（所有DNA）的基因组。我们可以安全地假设一个犬种的基因组将比它所没有衍生的犬种的基因组更相似于它所衍生的犬种的基因组。如果我们对这份数据应用层次聚类，这个层次结构，可以将其可视化为树状图，可以直接解释为显示哪些犬种是从其他犬种衍生的。
- en: The hierarchy is very useful, but how do we partition the dendrogram into a
    finite set of clusters? Well, at any height on the dendrogram, we can *cut* the
    tree horizontally and take the number of clusters at that level. Another way of
    imagining it is that if we were to cut a slice through the dendrogram, however
    many individual branches would fall off is the number of clusters. Look back at
    [figure 17.1](#ch17fig01). If we cut the tree where I’ve labeled the strings,
    woodwinds, brass, and percussion, we would get four individual clusters, and cases
    would be assigned to whichever of these four clusters they fell within. I’ll show
    you how we can select a cut point later in this section.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 层次结构非常有用，但我们如何将树状图划分为有限簇集？嗯，在树状图的任何高度上，我们都可以水平地*切割*树，并取该层的簇数。另一种想象方式是，如果我们切割树状图的一个切片，那么掉落的分支数量就是簇的数量。回顾一下
    [图 17.1](#ch17fig01)。如果我们切割我在标签处标记的弦乐器、木管乐器、铜管乐器和打击乐器，我们会得到四个单独的簇，案例将被分配到它们所在的这四个簇中。我将在本节的后面部分向您展示如何选择切割点。
- en: '|  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If we cut the tree closer to the top, we get fewer clusters. If we cut the tree
    closer to the bottom, we get more clusters.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更靠近树顶切割，我们会得到更少的簇。如果我们更靠近树底切割，我们会得到更多的簇。
- en: '|  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Okay, we have an understanding of *what* hierarchical clustering algorithms
    try to achieve. Now let’s talk about *how* they achieve it. There are two approaches
    we can take while trying to learn hierarchies in data:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经了解了层次聚类算法试图实现的目标。现在让我们谈谈它们是如何实现这一目标的。在尝试学习数据中的层次结构时，我们可以采取两种方法：
- en: Agglomerative
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Divisive
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分
- en: 'Agglomerative hierarchical clustering is where we start with every case isolated
    (and lonely) in its own cluster, and sequentially merge clusters until all the
    data resides within a single cluster. Divisive hierarchical clustering does the
    opposite: it starts with all the cases in a single cluster and recursively splits
    them into clusters until each case resides in its own cluster.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类层次聚类是从每个案例都孤立（且孤独）地位于其自己的簇中开始的，然后按顺序合并簇，直到所有数据都位于单个簇中。划分层次聚类则相反：它从所有案例都位于单个簇开始，并递归地将它们划分为簇，直到每个案例都位于其自己的簇中。
- en: 17.1.1\. Agglomerative hierarchical clustering
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 17.1.1\. 聚类层次聚类
- en: 'In this section, I’ll show you how agglomerative hierarchical clustering learns
    the structure in the data. The steps of the algorithm are quite simple:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示聚类层次聚类是如何学习数据中的结构的。算法的步骤相当简单：
- en: Calculate some distance metric (defined by us) between each cluster and all
    other clusters.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个簇与其他所有簇之间的某些距离度量（由我们定义）。
- en: Merge the most similar clusters together into a single cluster.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最相似的簇合并成一个簇。
- en: Repeat steps 1 and 2 until all cases reside in a single cluster.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1 和 2，直到所有案例都位于单个簇中。
- en: An example of how this might look is shown in [figure 17.2](#ch17fig02). We
    start with nine cases (and therefore nine clusters). The algorithm calculates
    a distance metric (more about this soon) between each of the clusters and merges
    the clusters that are most similar to each other. This continues until all the
    cases are gobbled up by the final supercluster.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17.2](#ch17fig02) 展示了这种情况的一个例子。我们开始时有九个案例（因此有九个簇）。算法计算每个簇之间的距离度量（稍后将有更多关于此的信息），并将最相似的簇合并。这个过程一直持续到所有案例都被最终的超簇吞噬。'
- en: Figure 17.2\. Agglomerative hierarchical clustering merges clusters that are
    closest to each other at each iteration. Ellipses indicate the formation of clusters
    at each iteration, going from top left to bottom right.
  id: totrans-379
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 17.2\. 聚类层次聚类在每次迭代中将彼此最近的簇合并。椭圆表示每次迭代中簇的形成，从左上角到右下角。
- en: '![](fig17-2_alt.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图片 17-2](fig17-2_alt.jpg)'
- en: 'So how do we calculate the distance between clusters? The first choice we need
    to make is what kind of distance we want to compute. As usual, the Euclidean and
    Manhattan distances are the most popular choices. The second choice is how to
    calculate this distance metric between clusters. Calculating the distance between
    two cases (two vectors) is reasonably obvious, but a *cluster* contains multiple
    cases; how do we calculate, say, Euclidean distance between two clusters? Well,
    we have a few options available to us, called *linkage methods*:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何计算簇之间的距离呢？我们需要做的第一个选择是我们要计算哪种类型的距离。像往常一样，欧几里得距离和曼哈顿距离是最受欢迎的选择。第二个选择是如何计算簇之间的这种距离度量。计算两个案例（两个向量）之间的距离是相当明显的，但一个簇包含多个案例；我们如何计算，比如说，两个簇之间的欧几里得距离？嗯，我们有几种可供选择的方法，称为*链接方法*：
- en: Centroid linkage
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心链接
- en: Single linkage
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单链接
- en: Complete linkage
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全链接
- en: Average linkage
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均链接
- en: Ward’s method
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沃德方法
- en: Each of these linkage methods is illustrated in [figure 17.3](#ch17fig03). Centroid
    linkage calculates the distance (Euclidean or Manhattan, for example) between
    each cluster’s centroid to every other cluster’s centroid. Single linkage takes
    the distance between the *nearest* cases of two clusters, as the distance between
    those clusters. Complete linkage takes the distance between the *furthest* cases
    of two clusters, as the distance between those clusters. Average linkage takes
    the average distance between all the cases of two clusters, as the distance between
    those clusters.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这些链接方法在[图 17.3](#ch17fig03)中均有说明。质心链接计算每个簇质心与每个其他簇质心之间的距离（例如，欧几里得距离或曼哈顿距离）。单链接取两个簇中最近案例之间的距离作为这些簇之间的距离。完全链接取两个簇中最远案例之间的距离作为这些簇之间的距离。平均链接取两个簇中所有案例之间的平均距离作为这些簇之间的距离。
- en: Figure 17.3\. Different linkage methods to define the distance between clusters.
    Centroid linkage calculates the distance between cluster centroids. Single linkage
    calculates the smallest distance between clusters. Complete linkage calculates
    the largest distance between clusters. Average linkage calculates all pairwise
    distances between cases in two clusters and finds the mean. Ward’s method calculates
    the within-cluster sum of squares for each candidate merge and chooses the one
    with the smallest value.
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 17.3. 定义簇之间距离的不同链接方法。质心链接计算簇质心之间的距离。单链接计算簇之间的最小距离。完全链接计算簇之间的最大距离。平均链接计算两个簇中案例之间的所有成对距离并找到平均值。沃德方法计算每个候选合并的簇内平方和，并选择具有最小值的那个。
- en: '![](fig17-3_alt.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig17-3_alt.jpg)'
- en: Ward’s method is a little more complex. For every possible combination of clusters,
    Ward’s method (sometimes called Ward’s minimum variance method) calculates the
    within-cluster sum of squares. Take a look at the examples for Ward’s method in
    [figure 17.3](#ch17fig03). The algorithm has three clusters to consider merging.
    For each candidate merge, the algorithm calculates the sum of squared differences
    between each case and its cluster’s centroid, and then adds these sums of squares
    together. The candidate merge that results in the smallest sum of squared differences
    is chosen at each step.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 沃德方法稍微复杂一些。对于每个可能的簇组合，沃德方法（有时称为沃德最小方差方法）计算簇内的平方和。请查看[图 17.3](#ch17fig03)中沃德方法的示例。算法有三个簇需要考虑合并。对于每个候选合并，算法计算每个案例与其簇质心之间的平方差之和，然后将这些平方和相加。在每一步选择导致最小平方差之和的候选合并。
- en: 17.1.2\. Divisive hierarchical clustering
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 17.1.2. 分层聚类
- en: In this section, I’ll show you how divisive hierarchical clustering works. Unlike
    agglomerative clustering, divisive clustering starts with all cases in a single
    cluster and recursively divides this into smaller and smaller clusters, until
    each case resides in its own cluster. Finding the optimal split at each stage
    of clustering is a difficult task, so divisive clustering uses a heuristic approach.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示分层聚类是如何工作的。与聚合聚类不同，分层聚类从单个簇中的所有案例开始，并递归地将这些案例划分为越来越小的簇，直到每个案例都位于自己的簇中。在聚类的每个阶段找到最佳分割是一个困难的任务，因此分层聚类使用启发式方法。
- en: At each stage of clustering, the cluster with the largest *diameter* is chosen.
    Recall from [figure 16.5](kindle_split_029.html#ch16fig05) that a cluster’s diameter
    is the largest distance between any two cases within the cluster. The algorithm
    then finds the case in this cluster that has the largest average distance to all
    the other cases in the cluster. This most-dissimilar case starts its own *splinter
    group* (like a rebel without a cause). The algorithm then iterates through every
    case in the cluster and assigns cases to either the splinter group or the original
    cluster, depending on which they are most similar to. In essence, divisive clustering
    applies k-means clustering (with *k* = 2) at each level of the hierarchy, in order
    to split each cluster. This process repeats until all cases reside in their own
    cluster.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类的每个阶段，选择具有最大 *直径* 的集群。回想一下[图 16.5](kindle_split_029.html#ch16fig05)，集群的直径是集群内任何两个案例之间的最大距离。然后算法找到这个集群中与集群内所有其他案例的平均距离最大的案例。这个最不相似的案例开始其自己的
    *分裂小组*（就像一个没有原因的叛逆者）。然后算法遍历集群中的每个案例，根据它们与哪个集群最相似将案例分配到分裂小组或原始集群。本质上，分裂聚类在每个层次级别应用
    k-means 聚类（其中 *k* = 2），以分割每个集群。这个过程重复进行，直到所有案例都位于自己的集群中。
- en: 'There is only one implementation of divisive clustering: the DIANA (DIvisive
    ANAlysis) algorithm. Agglomerative clustering is more commonly used and is less
    computationally expensive than the DIANA algorithm. However, mistakes made early
    in hierarchical clustering cannot be fixed further down the tree; so whereas agglomerative
    clustering may do better at finding small clusters, DIANA may do better at finding
    large clusters. In the rest of the chapter, I’ll walk you through how to perform
    agglomerative clustering in R, but one of the exercises is to repeat the clustering
    using DIANA and compare the results.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂聚类只有一个实现：DIANA（DIvisive ANAlysis）算法。凝聚聚类比 DIANA 算法更常用，并且计算成本更低。然而，在层次聚类早期犯下的错误无法在树的下层得到修正；因此，虽然凝聚聚类可能在发现小集群方面做得更好，但
    DIANA 在发现大集群方面可能做得更好。在本章的其余部分，我将向您介绍如何在 R 中执行凝聚聚类，但其中一项练习是重复使用 DIANA 进行聚类，并比较结果。
- en: 17.2\. Building your first agglomerative hierarchical clustering model
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.2. 构建您的第一个凝聚层次聚类模型
- en: In this section, I’ll show you how to build an agglomerative hierarchical clustering
    model in R. Sadly, there isn’t an implementation of hierarchical clustering wrapped
    by the mlr package, so we’re going to use the `hclust()` function from the built-in
    stats package.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何在 R 中构建一个凝聚层次聚类模型。遗憾的是，mlr 包并没有对层次聚类进行封装的实现，因此我们将使用内置的 stats 包中的
    `hclust()` 函数。
- en: The `hclust()` function that we’ll use to perform agglomerative hierarchical
    clustering expects a *distance matrix* as input, rather than the raw data. A distance
    matrix contains the pairwise distances between each combination of elements. This
    distance can be any distance metric we specify, and in this situation, we’ll use
    the Euclidean distance. Because computing the distances between cases is the first
    step of hierarchical clustering, you might expect `hclust()` to do this for us.
    But this two-step process of creating our own distance metric and then supplying
    it to `hclust()` does allow us the flexibility of using a variety of distance
    metrics.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的 `hclust()` 函数执行凝聚层次聚类时，期望输入的是一个 *距离矩阵*，而不是原始数据。距离矩阵包含了每个元素组合之间的成对距离。这种距离可以是任何我们指定的距离度量，在这种情况下，我们将使用欧几里得距离。由于计算案例之间的距离是层次聚类的第一步，您可能会期望
    `hclust()` 为我们完成这项工作。但这个创建我们自己的距离度量并将其提供给 `hclust()` 的两步过程确实给了我们使用各种距离度量的灵活性。
- en: 'We create a distance matrix in R using the `dist()` function, supplying the
    data we want to compute distances for as the first argument and the type of distance
    we want to use. Notice that we’re using our scaled dataset, because hierarchical
    clustering is also sensitive to differences in scale between variables (as is
    any algorithm that relies on distance between continuous variables):'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 R 中使用 `dist()` 函数创建距离矩阵，将我们想要计算距离的数据作为第一个参数提供，以及我们想要使用的距离类型。请注意，我们正在使用我们的缩放数据集，因为层次聚类对变量之间的尺度差异也很敏感（任何依赖于连续变量之间距离的算法也是如此）：
- en: '[PRE13]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-401
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you want a more visual example of what a distance matrix looks like, run
    `dist(c(4, 7, 11, 30, 16))`. *Don’t* try to print the distance matrix we create
    in this section—it contains more than 2.3 × 10⁷ elements!
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更直观地了解距离矩阵的样子，运行`dist(c(4, 7, 11, 30, 16))`。*不要*尝试打印本节中创建的距离矩阵——它包含超过2.3
    × 10⁷个元素！
- en: '|  |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Now that we have our distance matrix, we can run the algorithm to learn the
    hierarchy in our data. The first argument to the `hclust()` function is the distance
    matrix, and the `method` argument allows us to specify the linkage method we wish
    to use to define the distance between clusters. The options available are `"ward.D"`,
    `"ward.D2"`, `"single"`, `"complete"`, `"average"`, `"centroid"`, and a few less
    commonly used ones that I haven’t defined (see `?hclust` if you’re interested
    in these). Notice that there seem to be two options for Ward’s method: the option
    `"ward.D2"` is the correct implementation of Ward’s method, as I described earlier.
    In this example, we’re going to start by using Ward’s method (`"ward.D2"`), but
    I’ll get you to compare the result of this to other methods as part of this chapter’s
    exercises:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了距离矩阵，我们可以运行算法来学习数据中的层次结构。`hclust()`函数的第一个参数是距离矩阵，`method`参数允许我们指定我们希望用来定义聚类之间距离的链接方法。可用的选项有`"ward.D"`、`"ward.D2"`、`"single"`、`"complete"`、`"average"`、`"centroid"`以及一些不太常用的选项，我没有定义（如果你对这些选项感兴趣，请参阅`?hclust`）。注意，对于Ward方法似乎有两个选项：选项`"ward.D2"`是Ward方法的正确实现，如我之前所述。在这个例子中，我们将首先使用Ward方法（`"ward.D2"`），但我会在本章的练习中让你比较这个结果与其他方法：
- en: '[PRE14]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that `hclust()` has learned the hierarchical clustering structure of the
    data, let’s represent this as a dendrogram. We can do this by simply calling `plot()`
    on our clustering model object, but the tree is a little clearer if we first convert
    our model into a dendrogram object and plot that. We can convert our clustering
    model into a dendrogram object using the `as.dendrogram()` function. To plot the
    dendrogram, we pass it to the `plot()` function. By default, the plot will draw
    a label for each case in the original data. Because we have such a large dataset,
    let’s suppress these labels using the argument `leaflab = "none"`.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`hclust()`已经学习了数据的层次聚类结构，让我们用树状图来表示这一点。我们可以通过简单地调用聚类模型对象的`plot()`方法来实现这一点，但如果我们首先将我们的模型转换为树状图对象并绘制它，树会显得更清晰。我们可以使用`as.dendrogram()`函数将我们的聚类模型转换为树状图对象。要绘制树状图，我们将它传递给`plot()`函数。默认情况下，图表将为原始数据中的每个案例绘制一个标签。由于我们的数据集很大，让我们使用`leaflab
    = "none"`参数来抑制这些标签。
- en: Listing 17.1\. Plotting the dendrogram
  id: totrans-407
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表17.1. 绘制树状图
- en: '[PRE15]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The resulting plot is shown in [figure 17.4](#ch17fig04). The y-axis here represents
    the distance between clusters, based on whatever linkage method (and distance
    metric) we used. Because we used Ward’s method, the values of this axis are the
    within-cluster sum of squares. When two clusters are merged together, they are
    connected by a horizontal line, the position of which along the y-axis corresponds
    to the distance between those clusters. Therefore, clusters of cases that merge
    lower down the tree (which is earlier in agglomerative clustering) are more similar
    to each other than clusters that merge further up the tree. The ordering of cases
    along the x-axis is optimized such that similar cases are drawn near each other
    to aid interpretation (otherwise, the branches would cross). As we can see, the
    dendrogram recursively joins clusters, from each case being in its own cluster
    to all the cases belonging to a supercluster.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示在[图17.4](#ch17fig04)中。这里的y轴表示基于我们使用的链接方法（和距离度量）的聚类之间的距离。因为我们使用了Ward方法，这个轴的值是聚类内的平方和。当两个聚类合并在一起时，它们通过一条水平线连接，这条线在y轴上的位置对应于这些聚类之间的距离。因此，在树的下部合并的案例聚类（在聚合聚类中较早）比在树上部合并的聚类更相似。x轴上案例的顺序被优化，以便相似的案例被绘制在一起，以帮助解释（否则，分支会交叉）。正如我们所看到的，树状图递归地合并聚类，从每个案例都在自己的聚类到所有案例都属于一个超聚类。
- en: Figure 17.4\. The resulting dendrogram representing our hierarchical clustering
    model. The y-axis represents the distances between cases. Horizontal lines indicate
    the positions at which cases/clusters merge with each other. The higher the merge,
    the less similar the clusters are to each other.
  id: totrans-410
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图17.4. 表示我们层次聚类模型的树状图。y轴表示案例之间的距离。水平线表示案例/聚类合并的位置。合并得越高，聚类之间的相似度越低。
- en: '![](fig17-4_alt.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](fig17-4_alt.jpg)'
- en: '|  |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 1**'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Repeat the clustering process, but this time specify `method = "manhattan"`
    when creating the distance matrix (don’t overwrite any existing objects). Plot
    a dendrogram of the cluster hierarchy, and compare it to the dendrogram we got
    by using Euclidean distance.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 重复聚类过程，但这次在创建距离矩阵时指定`method = "manhattan"`（不要覆盖任何现有对象）。绘制簇层次结构的树状图，并将其与我们使用欧几里得距离得到的树状图进行比较。
- en: '|  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'The hierarchical clustering algorithm has done its job: it’s learned the hierarchy,
    and what we do with it is up to us. We may want to directly interpret the structure
    of the tree to make some inference about a hierarchy that might exist in nature,
    though in our (large) dataset, that could be quite challenging.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法已经完成了它的任务：它已经学会了层次结构，而我们如何使用它取决于我们自己。我们可能想直接解释树的结构，从而对自然界中可能存在的层次结构做出一些推断，尽管在我们（大型）数据集中，这可能相当具有挑战性。
- en: Another common use of hierarchical clustering is to order the rows and columns
    of heatmaps, for example, for gene expression data. Ordering the rows and columns
    of a heatmap using hierarchical clustering helps researchers identify clusters
    of genes and clusters of patients simultaneously.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的另一种常见用途是对热图的行和列进行排序，例如，用于基因表达数据。使用层次聚类对热图的行和列进行排序有助于研究人员同时识别基因簇和患者簇。
- en: Finally, our primary motivation may be to identify a finite number of clusters
    within our dataset that are most interesting to us. This is what we will do with
    our clustering result.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的主要动机可能是识别数据集中对我们最有兴趣的有限数量的簇。这就是我们将如何处理我们的聚类结果。
- en: 17.2.1\. Choosing the number of clusters
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 17.2.1\. 选择簇的数量
- en: In this section, I’ll show you ways of deciding how many clusters to extract
    from a hierarchy. Another way of thinking about this is that we’re deciding what
    level of the hierarchy to use for clustering.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何从层次结构中提取多少个簇的方法。另一种思考方式是我们正在决定使用层次结构的哪个级别进行聚类。
- en: To define a finite number of clusters following hierarchical clustering, we
    need to define a cut point on our dendrogram. If we cut the tree near the top,
    we’ll get fewer clusters; and if we cut the tree near the bottom, we’ll get more
    clusters. So how do we choose a cut point? Well, our friends the Davies-Bouldin
    index, the Dunn index, and the pseudo F statistic can help us here. For k-means
    clustering, we performed a cross-validation-like procedure for estimating the
    performance of different numbers of clusters. Sadly, we can’t use this approach
    for hierarchical clustering because, unlike k-means, hierarchical clustering *cannot
    predict cluster membership of new cases*.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在层次聚类后定义有限数量的簇，我们需要在我们的树状图上定义一个切割点。如果我们接近树顶进行切割，我们将得到较少的簇；如果我们接近树底进行切割，我们将得到更多的簇。那么我们如何选择一个切割点呢？嗯，我们的朋友戴维斯-鲍尔丁指数、邓恩指数和伪F统计量可以在这里帮助我们。对于k-means聚类，我们执行了一个类似于交叉验证的程序来估计不同簇数量的性能。遗憾的是，我们无法使用这种方法进行层次聚类，因为与k-means不同，层次聚类*不能预测新案例的簇成员资格*。
- en: '|  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-423
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The hierarchical clustering algorithms themselves can’t predict the cluster
    membership of new cases, but you *could* do something like assigning new data
    to the cluster with the nearest centroid. You could use this approach to create
    separate training and test sets to evaluate internal cluster metrics on.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法本身不能预测新案例的簇成员资格，但你可以*尝试*将新数据分配到最近的质心所在的簇。你可以使用这种方法来创建单独的训练集和测试集，以评估内部簇度量。
- en: '|  |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Instead, we can make use of bootstrapping. Recall from [chapter 8](kindle_split_018.html#ch08)
    that bootstrapping is the process of taking bootstrap samples, applying some computation
    to each sample, and returning a statistic(s). The mean of our bootstrapped statistic(s)
    tells us the most likely value, and the distribution gives us an indication as
    to the stability of the statistic(s).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以利用自助法。回想一下[第8章](kindle_split_018.html#ch08)，自助法是从自助样本中抽取样本，对每个样本应用一些计算，并返回一个统计量（s）。我们自助统计量的平均值告诉我们最可能的价值，而分布则给我们提供了关于统计量（s）稳定性的指示。
- en: '|  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that to get a bootstrap sample, we randomly select cases from a dataset,
    with replacement, to create a new sample the same size as the old. Sampling *with
    replacement* simply means that once we sample a particular case, we put it back,
    such that there is a possibility it will be drawn again.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，为了获得自助样本，我们随机从数据集中选择案例，有放回地，以创建一个与旧样本相同大小的新的样本。有放回地采样简单地说就是，一旦我们采样了一个特定的案例，我们就将其放回，这样它就有可能再次被抽取。
- en: '|  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the context of hierarchical clustering, we can use bootstrapping to generate
    multiple samples from our data and generate a separate hierarchy for each sample.
    We can then select a range of cluster numbers from each hierarchy and calculate
    the internal cluster metrics for each. The advantage of using bootstrapping is
    that calculating the internal cluster metrics on the full dataset doesn’t give
    us an indication of the stability of the estimate, whereas the bootstrap sample
    does. The bootstrap sample of cluster metrics will have some variation around
    its mean, so we can choose the number of clusters with the most optimal and stable
    metrics.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类的背景下，我们可以使用自助法从我们的数据生成多个样本，并为每个样本生成一个单独的层次结构。然后，我们可以从每个层次结构中选择一系列聚类数量，并计算每个的内部聚类度量。使用自助法的优点是，在完整数据集上计算内部聚类度量并不给我们提供估计稳定性的指示，而自助样本则可以。聚类度量的自助样本将围绕其平均值有一些变化，因此我们可以选择具有最优化和最稳定度量的聚类数量。
- en: 'Let’s start by defining our own function that takes our data and a vector of
    cluster memberships and returns our three familiar internal cluster metrics for
    the data: the Davies-Bouldin index, the Dunn index, and the pseudo F statistic.
    Because the function we’ll use to calculate the Dunn index expects a distance
    matrix, we’ll include an additional argument in our function to which we’ll supply
    a precomputed distance matrix.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个自己的函数，该函数接受我们的数据和包含聚类成员资格向量的向量，并返回数据的三种熟悉的内部聚类度量：戴维斯-博尔丁指数、邓恩指数和伪F统计量。因为我们使用的函数来计算邓恩指数期望一个距离矩阵，所以我们在函数中包含一个额外的参数，我们将提供一个预先计算的距离矩阵。
- en: Listing 17.2\. Defining the `cluster_metrics` function
  id: totrans-433
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表17.2\. 定义`cluster_metrics`函数
- en: '[PRE16]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Follow the function body with me so what we’re doing makes sense. We use the
    `function()` argument to define a function, assigning it to the name `cluster_metrics`
    (this will allow us to call the function using `cluster_metrics()`). We define
    three mandatory arguments for the function:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随我一起查看函数的主体，这样我们就能理解我们在做什么。我们使用`function()`参数来定义一个函数，将其分配给名称`cluster_metrics`（这将允许我们使用`cluster_metrics()`调用该函数）。我们为函数定义了三个强制参数：
- en: '`data`, to which we will pass the data we’re clustering'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data`，我们将传递我们正在聚类的数据'
- en: '`clusters`, a vector containing the cluster membership of every case in `data`'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters`，一个包含`data`中每个案例聚类成员资格的向量'
- en: '`dist_matrix`, to which we will pass the precomputed distance matrix for `data`'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dist_matrix`，我们将传递`data`的预计算距离矩阵'
- en: 'The *body* of the function (the instructions that tell the function what to
    do) is defined inside curly brackets (`{}`). Our function will return a list with
    four elements: the Davies-Bouldin index (`db`), the pseudo F statistic (`G1`),
    the Dunn index (`dunn`), and the number of clusters. Rather than define them from
    scratch, we’re using predefined functions from other packages to compute the internal
    cluster metrics. The Davies-Bouldin index is computed using the `index.DB()` function
    from the clusterSim package, which takes the `data` and `clusters` arguments (the
    statistic itself is contained in the `$DB` component). The pseudo F statistic
    is computed using the `index.G1()` function, also from the clusterSim package,
    and takes the same arguments as `index.DB()`. The Dunn index is computed using
    the `dunn()` function from the clValid package, which takes the `dist_matrix`
    and `clusters` arguments.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的主体（告诉函数做什么的指令）定义在花括号`{}`内。我们的函数将返回一个包含四个元素的列表：戴维斯-博尔丁指数（`db`）、伪F统计量（`G1`）、邓恩指数（`dunn`）和聚类数量。我们不是从头开始定义它们，而是使用来自其他包的预定义函数来计算内部聚类度量。戴维斯-博尔丁指数是通过clusterSim包中的`index.DB()`函数计算的，它接受`data`和`clusters`参数（统计量本身包含在`$DB`组件中）。伪F统计量是通过clusterSim包中的`index.G1()`函数计算的，它接受与`index.DB()`相同的参数。邓恩指数是通过clValid包中的`dunn()`函数计算的，它接受`dist_matrix`和`clusters`参数。
- en: Our motivation for defining this function is that we’re going to take bootstrap
    samples from our dataset, learn the hierarchy in each, select a range of cluster
    numbers from each, and use our function to calculate these three metrics for each
    number of clusters within each bootstrap sample. So now, let’s create our bootstrap
    samples. We’ll create 10 bootstrap samples from our gvhdScaled dataset. We’re
    using the `map()` function to repeat the sampling process 10 times, to return
    a list where each element is a different bootstrap sample.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义这个函数的动机是，我们将从我们的数据集中抽取自助样本，学习每个样本中的层次结构，从每个样本中选择一系列聚类数量，并使用我们的函数来计算每个自助样本中每个聚类数量的这三个指标。所以现在，让我们创建我们的自助样本。我们将从我们的
    `gvhdScaled` 数据集中创建 10 个自助样本。我们使用 `map()` 函数重复抽样过程 10 次，以返回一个列表，其中每个元素都是不同的自助样本。
- en: Listing 17.3\. Creating bootstrap samples
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 17.3\. 创建自助样本
- en: '[PRE17]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|  |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-444
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: Remember that `~` is just shorthand for `function()`.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`~` 只是 `function()` 的简称。
- en: '|  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We’re using the `sample_n()` function from the dplyr package to create the samples.
    This function randomly samples rows from a dataset. Because this function cannot
    handle matrices, we first need to pipe our gvhdScaled data into the `as_tibble()`
    function. By setting the argument `size = nrow(.)`, we’re asking `sample_n()`
    to randomly draw a number of cases equal to the number of rows in the original
    dataset (the `.` is shorthand for “the dataset that was piped in”). By setting
    the `replace` argument equal to `TRUE`, we’re telling the function to sample with
    replacement. Creating simple bootstrap samples really is as easy as this!
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 dplyr 包中的 `sample_n()` 函数来创建样本。这个函数可以从数据集中随机抽取行。因为这个函数无法处理矩阵，我们首先需要将我们的
    `gvhdScaled` 数据通过 `as_tibble()` 函数进行管道处理。通过设置参数 `size = nrow(.)`，我们要求 `sample_n()`
    随机抽取与原始数据集行数相等的案例数量（`.` 是“被管道传输的数据集”的简称）。通过将 `replace` 参数设置为 `TRUE`，我们告诉函数进行有放回的抽样。创建简单的自助样本实际上就像这样简单！
- en: Now let’s use our `cluster_metrics()` function to calculate those three internal
    metrics for a range of cluster numbers, for each bootstrap sample we just generated.
    Take a look at the following listing, and don’t go cross-eyed! I’ll take you through
    the code step by step.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们的 `cluster_metrics()` 函数来计算我们刚刚生成的每个自助样本的三个内部指标，对于一系列的聚类数量。看看下面的列表，不要眯着眼睛！我会一步一步地带你通过这段代码。
- en: Listing 17.4\. Calculating performance metrics of our clustering model
  id: totrans-449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 17.4\. 计算聚类模型的性能指标
- en: '[PRE18]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-452
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `map_df()` function is just like `map()`, but instead of returning a list,
    it combines each element row-wise to return a data frame.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_df()` 函数就像 `map()`，但它不是返回一个列表，而是将每个元素按行组合起来返回一个数据框。'
- en: '|  |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We start by calling the `map_df()` function so that we can apply a function
    to every element of our list of bootstrap samples. We define an anonymous function
    that takes `boot` (the current element being considered) as its only argument.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调用 `map_df()` 函数，这样我们就可以将一个函数应用到我们的自助样本列表中的每个元素上。我们定义一个匿名函数，它只接受 `boot`（当前正在考虑的元素）作为其唯一参数。
- en: For each element in `gvhdBoot`, the anonymous function computes its Euclidean
    distance matrix, stores it as the object `d`, and performs hierarchical clustering
    using that matrix and Ward’s method. Once we have the hierarchy for each bootstrap
    sample, we use another `map_df()` function call to select between three and eight
    clusters to partition the data into, and then calculate the three internal clustering
    methods on each result. We’re going to use this process to see which number of
    clusters, between three and eight, gives us the best internal cluster metrics
    values.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `gvhdBoot` 中的每个元素，匿名函数计算其欧几里得距离矩阵，将其存储为对象 `d`，并使用该矩阵和 Ward 方法进行层次聚类。一旦我们有了每个自助样本的层次结构，我们就使用另一个
    `map_df()` 函数调用来在三个和八个聚类之间选择，然后将数据分割成这些聚类，并在每个结果上计算三个内部聚类方法。我们将使用这个过程来查看在三个和八个之间哪个数量的聚类给出了最佳的内部聚类指标值。
- en: Selecting the number of clusters to retain from a hierarchical clustering model
    is done using the `cutree()` function. We use this function to cut our dendrogram
    at a place that returns a number of clusters. We can do this either by specifying
    a height at which to cut, using the `h` argument, or by specifying a specific
    number of clusters to retain, using the `k` argument (as done here). The first
    argument is the result of calling the `hclust()` function. The output of the `cutree()`
    function is a vector indicating the cluster number assigned to each case in the
    dataset. Once we have this vector, we can call our `cluster_metrics()` function,
    supplying the bootstrap data, the vector of cluster membership, and the distance
    matrix.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cutree()`函数从层次聚类模型中选择要保留的簇数量。我们使用这个函数在返回簇数量的地方切割我们的树状图。我们可以通过指定一个切割的高度，使用`h`参数，或者通过指定要保留的特定簇数量，使用`k`参数（如这里所示）来完成这个操作。第一个参数是调用`hclust()`函数的结果。`cutree()`函数的输出是一个向量，表示分配给数据集中每个案例的簇编号。一旦我们有了这个向量，我们就可以调用我们的`cluster_metrics()`函数，提供bootstrap数据，簇成员资格的向量，以及距离矩阵。
- en: '|  |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Warning
  id: totrans-459
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This took nearly 3 minutes to run on my machine!
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的机器上运行了近3分钟！
- en: '|  |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: If what we just did is a little unclear to you, print the `metricsTib` tibble
    to see what the output looks like. We have a tibble with one column for each of
    the internal cluster metrics, and a column indicating the number of clusters for
    which the metrics were calculated.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对我们刚才所做的不太清楚，请打印`metricsTib` tibble以查看输出。我们有一个tibble，其中每列对应一个内部簇指标，还有一个列表示计算这些指标的簇数量。
- en: Let’s plot the result of our bootstrapping experiment. We’re going to create
    a separate subplot for each internal cluster metric (using faceting). Each subplot
    will show the number of clusters on the x-axis, the value of the internal cluster
    metric on the y-axis, a separate line for each individual bootstrap sample, and
    a line that connects the mean value across all bootstraps.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制我们的bootstrap实验的结果。我们将为每个内部簇指标创建一个单独的子图（使用分面）。每个子图将显示x轴上的簇数量，y轴上的内部簇指标值，每个单独的bootstrap样本的线条，以及连接所有bootstrap均值值的线条。
- en: Listing 17.5\. Transforming the data, ready for plotting
  id: totrans-464
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表17.5\. 转换数据，准备绘图
- en: '[PRE19]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We first need to mutate a new column, indicating the bootstrap sample each case
    belongs to. Because there are 10 bootstrap samples, evaluated for 6 different
    numbers of clusters each (3 to 8), we create this variable by using the `rep()`
    function to repeat each number from 1 to 10, six times. We wrap this inside the
    `factor()` function to ensure it isn’t treated as a continuous variable when plotting.
    Next, we gather the data so that the choice of internal metric is contained within
    a single column and the value of that metric is held in another column. We specify
    `-clusters` and `-bootstrap` to tell the function *not* to gather these variables.
    Print this new tibble, and be sure you understand how we got there.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要创建一个新列，表示每个案例所属的bootstrap样本。由于有10个bootstrap样本，每个样本针对6个不同的簇数量进行评估（3到8），我们通过使用`rep()`函数重复1到10的每个数字六次来创建这个变量。我们将这个变量包裹在`factor()`函数中，以确保在绘图时它不被视为连续变量。接下来，我们收集数据，使得内部指标的选择包含在单个列中，而该指标的价值保持在另一个列中。我们指定`-clusters`和`-bootstrap`来告诉函数不要收集这些变量。打印这个新的tibble，并确保你理解我们是如何到达这里的。
- en: Now that our data is in this format, we can create the plot.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们数据已经以这种格式，我们可以创建图表。
- en: Listing 17.6\. Calculating metrics
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表17.6\. 计算指标
- en: '[PRE20]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We map the number of clusters (as a factor) to the x aesthetic and the value
    of the internal cluster metric to the y aesthetic. We add a `facet_wrap()` layer
    to facet by internal cluster metric, setting the `scales = "free_y"` argument
    because the metrics are on different scales. Next, we add a `geom_line()` layer,
    using the `size` argument to make these lines less prominent, and map the bootstrap
    sample number to the group aesthetic. This layer will therefore draw a separate,
    thin line for each bootstrap sample.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将簇数量（作为一个因子）映射到x美学，将内部簇指标的值映射到y美学。我们添加一个`facet_wrap()`层，通过内部簇指标进行分面，设置`scales
    = "free_y"`参数，因为指标处于不同的尺度。接下来，我们添加一个`geom_line()`层，使用`size`参数使这些线条不那么突出，并将bootstrap样本编号映射到group美学。因此，这个层将为每个bootstrap样本绘制一条单独的细线。
- en: '|  |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-472
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Notice that when you specify an aesthetic mapping inside the `ggplot()` function
    layer, the mapping is inherited by all additional layers that use that aesthetic.
    However, you can specify aesthetic mappings using the `aes()` function inside
    each geom function, and the mapping will apply to that layer only.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你指定 `ggplot()` 函数层内的美学映射时，该映射会被所有使用该美学的附加层继承。然而，你可以在每个 geom 函数内部使用 `aes()`
    函数指定美学映射，并且映射只会应用于该层。
- en: '|  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We then add another `geom_line()` layer that will connect the mean across all
    bootstrap samples. By default, the `geom_line()` function likes to connect individual
    values. If we want the function to connect a summary statistic (like a mean),
    we need to specify the `stat = "summary"` argument and then use the `fun.y` argument
    to tell the function what summary statistic we want to plot. Here we’ve used `"mean"`,
    but you can supply the name of any function that returns a single value of `y`
    for its input.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着添加另一个 `geom_line()` 层，该层将连接所有自助样本的均值。默认情况下，`geom_line()` 函数喜欢连接单个值。如果我们想让函数连接一个汇总统计量（如均值），我们需要指定
    `stat = "summary"` 参数，然后使用 `fun.y` 参数告诉函数我们想要绘制哪个汇总统计量。在这里，我们使用了 `"mean"`，但你也可以提供任何返回输入
    `y` 的单个值的函数名称。
- en: Finally, it would be nice to visualize the 95% confidence interval for the bootstrap
    sample. The 95% confidence intervals tell us that, if we were to repeat this experiment
    100 times, 95 of the constructed confidence intervals would be expected to contain
    the true value of the metric. The more the estimates agree with each other between
    bootstrap samples, the smaller the confidence interval will be. We want to visualize
    the confidence intervals using the flexible `stat_summary()` function. This function
    can be used to visualize multiple summary statistics in many different ways. To
    draw the mean ± 95% confidence intervals, we use the `fun.data` argument to specify
    that we want `"mean_cl_boot"`. This will draw bootstrap confidence intervals (95%
    by default).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可视化自助样本的95%置信区间会很好。95%置信区间告诉我们，如果我们重复进行这个实验100次，预期有95个构建的置信区间将包含该指标的真正值。自助样本之间的估计越一致，置信区间就越小。我们想要使用灵活的
    `stat_summary()` 函数来可视化置信区间。这个函数可以以许多不同的方式可视化多个汇总统计量。为了绘制均值 ± 95% 置信区间，我们使用 `fun.data`
    参数指定我们想要 `"mean_cl_boot"`。这将绘制自助置信区间（默认为95%）。
- en: '|  |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The other option would be to use `"mean_cl_normal"` to construct the confidence
    intervals, but this assumes the data is normally distributed, and this may not
    be true.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是使用 `"mean_cl_normal"` 来构建置信区间，但这假设数据是正态分布的，这可能并不正确。
- en: '|  |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Now that we’ve defined our summary statistics, let’s specify the geom that we’re
    going to use to represent them, using the `geom` argument. The geom `"crossbar"`
    draws what looks like the box part of a box and whiskers plot, where a solid line
    is drawn through the measure of central tendency that we specified (the mean,
    in this case) and the upper and lower limits of the box extend to the range of
    the measure of dispersion we asked for (95% confidence limits, in this case).
    Then, according to my preference, we set the width of the crossbars to 0.5 and
    the fill color to white.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的汇总统计量，让我们指定我们将使用 `geom` 参数来表示它们的几何形状。`"crossbar"` 几何形状绘制看起来像箱线图的箱体部分，其中通过我们指定的中心趋势度量（在这种情况下是均值）绘制一条实线，箱体的上下限延伸到我们请求的分散度度量范围（在这种情况下是95%置信限）。然后，根据我的偏好，我们将交叉条的宽度设置为0.5，填充颜色设置为白色。
- en: The resulting plot is shown in [figure 17.5](#ch17fig05). Take a moment to appreciate
    how beautiful the result is after all the hard work we just put in. Look back
    at [listing 17.6](#ch17ex06) to make sure you understand how we created this plot
    (`stat_summary()` is probably the most confusing bit). It seems that the number
    of clusters resulting in the smallest mean Davies-Bouldin index and the largest
    mean Dunn index and mean pseudo F statistic is four. Take a look at the thin lines
    representing each individual bootstrap. Can you see that some of them might have
    led us to conclude that a different number of clusters was optimal? This is why
    bootstrapping these metrics is better than calculating each metric only once using
    a single dataset.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图示显示在 [图 17.5](#ch17fig05) 中。花点时间欣赏一下我们刚刚投入大量努力后的结果是多么美丽。回顾 [列表 17.6](#ch17ex06)
    确保你理解了我们是如何创建这个图的（`stat_summary()` 可能是最令人困惑的部分）。看起来导致最小的平均戴维斯-博尔丁指数和最大的平均邓恩指数以及平均伪
    F 统计量的聚类数量是四个。看看代表每个单独自助聚类的细线，你能看出其中一些可能使我们得出不同数量的聚类是最佳选择的结论吗？这就是为什么对这些度量进行自助比仅使用单个数据集计算每个度量一次更好。
- en: Figure 17.5\. Plotting the result of our bootstrap experiment. Each subplot
    shows the result of a different internal cluster metric. The x-axis shows the
    cluster number, and the y-axis shows the value of each metric. Faint lines connect
    the results of each individual bootstrap sample, while the bold line connects
    the mean. The top and bottom of each crossbar indicate the 95% confidence interval
    for that particular value, and the horizontal line represents the mean.
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 17.5\. 绘制我们的自助实验的结果。每个子图显示了不同内部聚类度量的结果。x 轴显示聚类编号，y 轴显示每个度量的值。淡线连接每个单独的自助样本的结果，而粗线连接平均值。每个十字线的顶部和底部表示该特定值的
    95% 置信区间，水平线表示平均值。
- en: '![](fig17-5_alt.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](fig17-5_alt.jpg)'
- en: '|  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: 'Let’s experiment with another way we could visualize these results. Start with
    the following operations using dplyr (piping each step into the next):'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一种可视化这些结果的方法。从以下使用 dplyr 的操作开始（将每个步骤管道输入到下一个）：
- en: Group the `metricsTib` object by `Metric`.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按 `Metric` 对 `metricsTib` 对象进行分组。
- en: Use `mutate()` to replace the `Value` variable with `scale(Value)`.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mutate()` 将 `Value` 变量替换为 `scale(Value)`。
- en: Group by both `Metric` *and* `clusters`.
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按 `Metric` 和 `clusters` 进行分组。
- en: Mutate a new column, `Stdev`, equal to `sd(Value)`.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新列 `Stdev`，等于 `sd(Value)`。
- en: 'Then pipe this tibble into a `ggplot()` call with the following aesthetic mappings:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将这个 tibble 管道输入到 `ggplot()` 调用中，并使用以下美学映射：
- en: '`x = clusters`'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x = clusters`'
- en: '`y = Metric`'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y = Metric`'
- en: '`fill = Value`'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill = Value`'
- en: '`height = Stdev`'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height = Stdev`'
- en: Finally, add a `geom_tile()` layer. Look back at your code and make sure you
    understand how you created this plot and how to interpret it.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，添加一个 `geom_tile()` 层。回顾一下你的代码，确保你理解了如何创建这个图以及如何解释它。
- en: '|  |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 17.2.2\. Cutting the tree to select a flat set of clusters
  id: totrans-499
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 17.2.2\. 剪切树以选择一组平坦的聚类
- en: In this section, I’ll show you how we can finally cut the dendrogram to return
    the cluster labels for our desired number of clusters. Our bootstrapping experiment
    has led us to conclude that four is the optimal number of clusters with which
    to represent the structure in our GvHD dataset. To extract a vector of cluster
    memberships representing these four clusters, we use the `cutree()` function,
    supplying our clustering model and `k` (the number of clusters we want to return).
    We can visualize how our dendrogram is cut to generate these four clusters by
    plotting the dendrogram as before and calling the `rect.hclust()` function with
    the same arguments we gave to `cutree()`.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示我们如何最终剪切树状图以返回我们所需数量的聚类标签。我们的自助实验使我们得出结论，四个是我们用 GvHD 数据集表示结构的最佳聚类数量。为了提取表示这四个聚类的聚类成员资格向量，我们使用
    `cutree()` 函数，提供我们的聚类模型和 `k`（我们想要返回的聚类数量）。我们可以通过像以前一样绘制树状图并调用 `rect.hclust()`
    函数来使用与 `cutree()` 相同的参数来可视化我们的树状图是如何被切割以生成这四个聚类的。
- en: Listing 17.7\. Cutting the tree
  id: totrans-501
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 17.7\. 剪切树
- en: '[PRE21]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This function draws rectangles on an existing dendrogram plot to show which
    branches are cut to result in the number of clusters we specified. The resulting
    plot is shown in [figure 17.6](#ch17fig06).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数在现有的树状图上绘制矩形，以显示哪些分支被剪切以产生我们指定的聚类数量。生成的图示显示在 [图 17.6](#ch17fig06) 中。
- en: Figure 17.6\. The same plot as in [figure 17.4](#ch17fig04), but this time with
    rectangles indicating the clusters resulting from cutting the tree
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图17.6\. 与[图17.4](#ch17fig04)相同的图表，但这次用矩形表示通过切割树得到的聚类
- en: '![](fig17-6_alt.jpg)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](fig17-6_alt.jpg)'
- en: Next, let’s plot the clusters using `ggpairs()` like we did for our k-means
    model in [chapter 16](kindle_split_029.html#ch16).
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 `ggpairs()` 来绘制聚类，就像我们在第16章中为我们的 k-means 模型所做的那样，[第16章](kindle_split_029.html#ch16)。
- en: Listing 17.8\. Plotting the clusters
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表17.8\. 绘制聚类
- en: '[PRE22]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Figure 17.7\. `ggpairs()` plot showing the result of our hierarchical clustering
    model. Compare these clusters to the ones obtained by k-means in [figure 16.8](kindle_split_029.html#ch16fig08).
  id: totrans-509
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图17.7\. `ggpairs()` 图显示我们的层次聚类模型的结果。将这些聚类与[图16.8](kindle_split_029.html#ch16fig08)中通过k-means得到的聚类进行比较。
- en: '![](fig17-7_alt.jpg)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![](fig17-7_alt.jpg)'
- en: The resulting figure is shown in [figure 17.7](#ch17fig07). Compare these clusters
    with the ones returned by our k-means model in [figure 16.8](kindle_split_029.html#ch16fig08).
    Both methods result in similar cluster membership, and the clusters from our hierarchical
    clustering also seem to undercluster cluster 3.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图17.7](#ch17fig07)中。将这些聚类与我们在[图16.8](kindle_split_029.html#ch16fig08)中得到的k-means模型返回的聚类进行比较。两种方法都产生了相似的聚类成员资格，而且我们的层次聚类得到的聚类似乎也低估了聚类3。
- en: 17.3\. How stable are our clusters?
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3\. 我们的聚类有多稳定？
- en: In this section, I’ll show you one more tool to evaluate the performance of
    our clustering model. In addition to calculating internal cluster metrics on each
    bootstrap sample in a bootstrapping experiment, we can also quantify how well
    the cluster memberships agree with each other between bootstrap samples. This
    agreement is called the cluster *stability*. A common way to quantify cluster
    stability is with a similarity metric called the *Jaccard index* (named after
    the botany professor who published it).
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示一个评估我们的聚类模型性能的更多工具。除了在bootstrap实验中计算每个bootstrap样本的内部聚类度量外，我们还可以量化聚类成员资格在bootstrap样本之间的一致性。这种一致性被称为聚类*稳定性*。量化聚类稳定性的常见方法是通过一个称为*Jaccard指数*的相似性度量（以发表它的植物学教授命名）。
- en: The Jaccard index quantifies the similarity between two sets of discrete variables.
    Its value can be interpreted as the percentage of the total values that are present
    in both sets, and it ranges from 0% (no common values) to 100% (all values common
    to both sets). The Jaccard index is defined in [equation 17.1](#ch17equ01).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard指数量化了两个离散变量集合之间的相似性。它的值可以解释为两个集合中存在的总值的百分比，其范围从0%（没有共同值）到100%（两个集合都有的所有值）。Jaccard指数在[方程17.1](#ch17equ01)中定义。
- en: equation 17.1\.
  id: totrans-515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程17.1\.
- en: '![](eq17-1.jpg)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![](eq17-1.jpg)'
- en: For example, if we have two sets
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有两个集合
- en: a = {3, 3, 5, 2, 8}
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: a = {3, 3, 5, 2, 8}
- en: b = {1, 3, 5, 6}
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: b = {1, 3, 5, 6}
- en: then the Jaccard index is
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Jaccard指数是
- en: '![](pg417.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](pg417.jpg)'
- en: If we cluster on multiple bootstrap samples, we can calculate the Jaccard index
    between the “original” clusters (the clusters on all the data) and each of the
    bootstrap samples, and take the mean. If the mean Jaccard index is low, then cluster
    membership is changing considerably between bootstrap samples, indicating our
    clustering result is *unstable* and may not generalize well. If the mean Jaccard
    index is high, then cluster membership is changing very little, indicating a stable
    clustering result.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在多个bootstrap样本上进行聚类，我们可以计算“原始”聚类（所有数据的聚类）与每个bootstrap样本之间的Jaccard指数，并取平均值。如果平均Jaccard指数低，那么聚类成员资格在bootstrap样本之间变化很大，这表明我们的聚类结果是*不稳定的*，可能无法很好地推广。如果平均Jaccard指数高，那么聚类成员资格变化很小，这表明聚类结果是稳定的。
- en: Luckily for us, the `clusterboot()` function from the fpc package has been written
    to do just this! Let’s first load the fpc package into our R session. Because
    `clusterboot()` produces a series of base R plots as a side effect, let’s split
    the plotting device into three rows and four columns to accommodate the output,
    using `par(mfrow = c(3, 4))`.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，fpc包中的 `clusterboot()` 函数已经被编写来执行这项操作！让我们首先将fpc包加载到我们的R会话中。因为 `clusterboot()`
    作为副作用产生了一系列基础R图表，让我们将绘图设备分成三行四列，以便容纳输出，使用 `par(mfrow = c(3, 4))`。
- en: Listing 17.9\. Using `clusterboot()` to calculate the Jaccard index
  id: totrans-524
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表17.9\. 使用 `clusterboot()` 计算Jaccard指数
- en: '[PRE23]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first argument to the `clusterboot()` function is the data. This argument
    will accept either the raw data or a distance matrix of class `dist` (it will
    handle either appropriately). The argument `B` is the number of bootstrap samples
    we wish to calculate, which I’ve set to 10 for the sake of reducing running time.
    The `clustermethod` argument is where we specify which type of clustering model
    we wish to build (see `?clusterboot` for a list of available methods; many common
    methods are included). For hierarchical clustering, we set this argument equal
    to `disthclustCBI`. The `k` argument specifies the number of clusters we want
    to return, `method` lets us specify the distance metric to use for clustering,
    and `showplots` gives us the opportunity to suppress the printing of the plots
    if we wish. The function may take a couple of minutes to run.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '`clusterboot()`函数的第一个参数是数据。该参数将接受原始数据或类`dist`的距离矩阵（它将适当地处理任一）。`B`参数是我们希望计算的自助样本数量，我将其设置为10以减少运行时间。`clustermethod`参数是我们指定希望构建哪种聚类模型的地方（有关可用方法的列表，请参阅`?clusterboot`；包括了许多常见方法）。对于层次聚类，我们将此参数设置为`disthclustCBI`。`k`参数指定我们想要返回的聚类数量，`method`允许我们指定用于聚类的距离度量，而`showplots`给我们提供了抑制打印图的机会。该函数可能需要几分钟才能运行。'
- en: 'I’ve truncated the output from printing the result of `clusterboot()` to show
    the most important information: the clusterwise Jaccard bootstrap means. These
    four values are the mean Jaccard indices for each cluster, between the original
    clusters and each bootstrap sample. We can see that all four clusters have good
    agreement (> 83%) across different bootstrap samples, suggesting high stability
    of the clusters.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经截断了`clusterboot()`函数输出的结果，以显示最重要的信息：聚类层面的Jaccard自助样本均值。这四个值是每个聚类，原始聚类和每个自助样本之间的平均Jaccard指数。我们可以看到，所有四个聚类在不同自助样本中都有良好的一致性（>
    83%），这表明聚类具有较高的稳定性。
- en: The resulting plot is shown in [figure 17.8](#ch17fig08). The first (top-left)
    and last (bottom-right) plots show the clustering on the original, full dataset.
    Each plot between these shows the clustering on a different bootstrap sample.
    This plot is a useful way of graphically evaluating the stability of the clusters.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图示在[图17.8](#ch17fig08)中展示。第一个（左上角）和最后一个（右下角）图展示了原始、完整数据集上的聚类。在这两个图之间的每个图展示了不同自助样本上的聚类。这种图示是图形化评估聚类稳定性的有用方式。
- en: Figure 17.8\. The graphical output of the `clusterboot()` function. The first
    and last plots show the full, original clusters of data, while the plots in between
    show the clusters on the bootstrap samples. The cluster membership of each case
    is indicated by a number. Notice the relatively high stability of the clusters.
  id: totrans-529
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图17.8. `clusterboot()`函数的图形输出。第一和最后一个图展示了完整、原始的数据聚类，而中间的图展示了自助样本上的聚类。每个案例的聚类成员资格由一个数字表示。注意聚类的相对高稳定性。
- en: '![](fig17-8_alt.jpg)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![](fig17-8_alt.jpg)'
- en: 17.4\. Strengths and weaknesses of hierarchical clustering
  id: totrans-531
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.4. 层次聚类的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    hierarchical clustering will perform well for you.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪种算法会对特定任务表现良好，但以下是一些优势和劣势，这将帮助您决定层次聚类是否适合您。
- en: 'The strengths of hierarchical clustering are as follows:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的优势如下：
- en: It learns a hierarchy that may in and of itself be interesting and interpretable.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它学习到的层次结构本身可能很有趣且可解释。
- en: It is quite simple to implement.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的实现相当简单。
- en: 'The weaknesses of hierarchical clustering are these:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的劣势如下：
- en: It cannot natively handle categorical variables. This is because calculating
    the Euclidean distance on a categorical feature space isn’t meaningful.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法原生地处理分类变量。这是因为在一个分类特征空间上计算欧几里得距离是没有意义的。
- en: It cannot select the optimal number of “flat” clusters.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法选择“平坦”聚类的最佳数量。
- en: It is sensitive to data on different scales.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对不同尺度的数据敏感。
- en: It cannot predict cluster membership of new data.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法预测新数据的聚类成员资格。
- en: Once cases have been assigned to a cluster, they cannot be moved.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦案例被分配到某个聚类，它们就不能再移动。
- en: It can become computationally expensive with large datasets.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据集，它可能会变得计算成本高昂。
- en: It is sensitive to outliers.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对异常值敏感。
- en: '|  |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习3**'
- en: 'Use the `clusterboot()` function to bootstrap the Jaccard index for k-means
    clustering (with four clusters), just like we did for hierarchical. This time,
    the `clustermethod` should be equal to `kmeansCBI` (to use k-means), and you should
    replace the `method` argument with `algorithm = "Lloyd"`. Which method results
    in more stable clusters: k-means or hierarchical clustering?'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `clusterboot()` 函数对 k-means 聚类（四个簇）的 Jaccard 指数进行自助法，就像我们对层次聚类所做的那样。这次，`clustermethod`
    应该等于 `kmeansCBI`（使用 k-means），并且你应该将 `method` 参数替换为 `algorithm = "Lloyd"`。哪种方法会导致更稳定的簇：k-means
    还是层次聚类？
- en: '|  |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 4**'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4**'
- en: 'Use the `diana()` function from the cluster package to perform divisive hierarchical
    clustering on the GvHD data. Save the output as an object, and plot the dendrogram
    by passing it into `as.dendrogram()` %>% `plot()`. Compare this to the dendrogram
    we got from agglomerative hierarchical clustering. Warning: this took nearly 15
    minutes on my machine!'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 使用聚类包中的 `diana()` 函数对 GvHD 数据进行分裂层次聚类。将输出保存为对象，并通过传递给 `as.dendrogram()` %>%
    `plot()` 来绘制树状图。将其与聚合层次聚类的树状图进行比较。警告：在我的机器上这几乎花了 15 分钟！
- en: '|  |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 5**'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 5**'
- en: Repeat our bootstrapping experiment with agglomerative hierarchical clustering,
    but this time fix the number of clusters to four and compare the different linkage
    methods on each bootstrap. Which linkage method performs the best?
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 使用聚合层次聚类重复我们的自助法实验，但这次将簇的数量固定为四个，并在每个自助法上比较不同的链接方法。哪种链接方法表现最好？
- en: '|  |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 6**'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 6**'
- en: Recluster the data using `hclust()`, using the linkage method indicated as the
    best from [exercise 5](#ch17sb05). Plot these clusters using `ggpairs()`, and
    compare them to those we generated using Ward’s method. Does this new linkage
    method do a good job of finding clusters?
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `hclust()` 重新聚类数据，使用 [练习 5](#ch17sb05) 中指示的最佳链接方法。使用 `ggpairs()` 绘制这些簇，并将它们与我们使用
    Ward 方法生成的簇进行比较。这种新的链接方法在发现簇方面做得好吗？
- en: '|  |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-560
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Hierarchical clustering uses the distances between cases to learn a hierarchy
    of clusters.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类使用案例之间的距离来学习簇的层次结构。
- en: How these distances are calculated is controlled by our choice of linkage method.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些距离是如何计算的由我们选择的链接方法控制。
- en: Hierarchical clustering can be bottom-up (agglomerative) or top-down (divisive).
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类可以是自下而上（聚合）或自上而下（分裂）。
- en: A flat set of clusters can be returned from a hierarchical clustering model
    by “cutting” the dendrogram at a particular height.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在特定高度“切割”树状图，可以从层次聚类模型中返回一个平面的簇集。
- en: Cluster stability can be measured by clustering on bootstrap samples and using
    the Jaccard index to quantify the agreement of cluster membership between samples.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在自助样本上进行聚类并使用 Jaccard 指数量化样本之间簇成员资格的一致性来衡量聚类稳定性。
- en: Solutions to exercises
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题的解答
- en: 'Create a hierarchical clustering model using the Manhattan distance, plot the
    dendrogram, and compare it:'
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用曼哈顿距离创建层次聚类模型，绘制树状图，并进行比较：
- en: '[PRE24]'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Plot the bootstrap experiment in an alternate way:'
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以另一种方式绘制自助法实验：
- en: '[PRE25]'
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Use `clusterboot()` to evaluate the stability of our k-means model:'
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `clusterboot()` 评估 k-means 模型的稳定性：
- en: '[PRE26]'
  id: totrans-572
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Cluster the data using the `diana()` function:'
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `diana()` 函数对数据进行聚类：
- en: '[PRE27]'
  id: totrans-574
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Repeat the bootstrap experiment, comparing different linkage methods:'
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复自助法实验，比较不同的链接方法：
- en: '[PRE28]'
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Cluster the data using the winning linkage method from [exercise 5](#ch17sb05):'
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 [练习 5](#ch17sb05) 中的获胜链接方法对数据进行聚类：
- en: '[PRE29]'
  id: totrans-578
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Chapter 18\. Clustering based on density: DBSCAN and OPTICS'
  id: totrans-579
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 18 章\. 基于密度的聚类：DBSCAN 和 OPTICS
- en: '*This chapter covers*'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding density-based clustering
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于密度的聚类
- en: Using the DBSCAN and OPTICS algorithms
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DBSCAN 和 OPTICS 算法
- en: 'Our penultimate stop in unsupervised learning techniques brings us to density-based
    clustering. Density-based clustering algorithms aim to achieve the same thing
    as k-means and hierarchical clustering: partitioning a dataset into a finite set
    of clusters that reveals a grouping structure in our data.'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术的最后一站带我们来到了基于密度的聚类。基于密度的聚类算法旨在实现与 k-means 和层次聚类相同的目标：将数据集划分为有限个簇，揭示数据中的分组结构。
- en: 'In the last two chapters, we saw how k-means and hierarchical clustering identify
    clusters using distance: distance between cases, and distance between cases and
    their centroids. Density-based clustering comprises a set of algorithms that,
    as the name suggests, uses the *density* of cases to assign cluster membership.
    There are multiple ways of measuring density, but we can define it as the number
    of cases per unit volume of our feature space. Areas of the feature space containing
    many cases packed closely together can be said to have high density, whereas areas
    of the feature space that contain few or no cases can be said to have low density.
    Our intuition here states that distinct clusters in a dataset will be represented
    by regions of high density, separated by regions of low density. Density-based
    clustering algorithms attempt to learn these distinct regions of high density
    and partition them into clusters. Density-based clustering algorithms have several
    nice properties that circumvent some of the limitations of k-means and hierarchical
    clustering.'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两章中，我们看到了k-means和层次聚类是如何使用距离来识别聚类的：案例之间的距离，以及案例与其质心之间的距离。基于密度的聚类包括一系列算法，正如其名称所示，它使用案例的*密度*来分配聚类成员资格。有多种测量密度的方法，但我们可以将其定义为特征空间单位体积中的案例数量。特征空间中包含许多紧密排列的案例的区域可以被认为是高密度区域，而特征空间中包含少量或没有案例的区域可以被认为是低密度区域。我们的直觉表明，数据集中的不同聚类将由高密度区域表示，这些区域由低密度区域分隔。基于密度的聚类算法试图学习这些独特的高密度区域并将它们划分为聚类。基于密度的聚类算法具有一些很好的特性，可以克服k-means和层次聚类的某些局限性。
- en: 'By the end of this chapter, I hope you’ll have a firm understanding of how
    two of the most commonly used density-based clustering algorithms work: DBSCAN
    and OPTICS. We’ll also apply some of the skills you learned in the previous chapters
    to help us evaluate and compare the performance of different cluster models.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我希望你能对两种最常用的基于密度的聚类算法的工作原理有一个牢固的理解：DBSCAN和OPTICS。我们还将应用你在前几章中学到的技能，帮助我们评估和比较不同聚类模型的表现。
- en: 18.1\. What is density-based clustering?
  id: totrans-586
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.1. 什么是基于密度的聚类？
- en: 'In this section, I’ll show you how two of the most commonly used density-based
    clustering algorithms work:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示两种最常用的基于密度的聚类算法是如何工作的：
- en: Density-based spatial clustering of applications with noise (DBSCAN)
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于噪声的密度聚类（DBSCAN）
- en: Ordering points to identify the clustering structure (OPTICS)
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用点排序来识别聚类结构（OPTICS）
- en: 'Aside from having names that were seemingly contrived to form interesting acronyms,
    DBSCAN and OPTICS both learn regions of high density, separated by regions of
    low density in a dataset. They achieve this in similar but slightly different
    ways, but both have a few advantages over k-means and hierarchical clustering:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 除了名字看起来像是精心设计以形成有趣的缩写外，DBSCAN和OPTICS都在数据集中学习高密度区域，这些区域由低密度区域分隔。它们以相似但略有不同的方式实现这一点，但两者都比k-means和层次聚类有一些优势：
- en: They are not biased to finding spherical clusters and can in fact find clusters
    of varying and complex shapes.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不倾向于寻找球形聚类，实际上可以找到形状各异且复杂的聚类。
- en: They are not biased to finding clusters of equal diameter and can identify both
    very wide and very tight clusters in the same dataset.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不倾向于寻找等直径的聚类，并且可以在同一数据集中识别非常宽和非常紧密的聚类。
- en: They are seemingly unique among clustering algorithms in that cases that do
    not fall within regions of high enough density are put into a separate “noise”
    cluster. This is often a desirable property, because it helps to prevent overfitting
    the data and allows us to focus on cases for which the evidence of cluster membership
    is stronger.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在聚类算法中似乎是独一无二的，因为那些不足以形成高密度区域的情况被放入一个单独的“噪声”聚类中。这通常是一个理想的属性，因为它有助于防止数据过拟合，并允许我们专注于证据更强的聚类案例。
- en: '|  |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the separation of cases into a noise cluster isn’t desirable for your application
    (but using DBSCAN or OPTICS is), you can use a heuristic method like classifying
    noise points based on their nearest cluster centroid, or adding them to the cluster
    of their k-nearest neighbors.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将案例分为噪声聚类对于你的应用来说不是理想的（但使用DBSCAN或OPTICS是），你可以使用一种启发式方法，例如根据它们最近的聚类质心对噪声点进行分类，或者将它们添加到它们的k个最近邻的聚类中。
- en: '|  |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: All three of these advantages can be seen in [figure 18.1](#ch18fig01). The
    three subplots each show the same data, clustered using either DBSCAN, k-means
    (Hartigan-Wong algorithm), or hierarchical clustering (complete linkage). This
    dataset is certainly strange, and you might think you’re unlikely to encounter
    real-world data like it, but it illustrates the advantages of density-based clustering
    over k-means and hierarchical clustering. The clusters in the data have very different
    shapes (that certainly aren’t spherical) and diameters. While k-means and hierarchical
    clustering learn clusters that bisect and merge these real clusters, DBSCAN is
    able to faithfully find each shape as a distinct cluster. Additionally, notice
    that k-means and hierarchical clustering place every single case into a cluster.
    DBSCAN creates the cluster “0” into which it places any cases it considers to
    be noise. In this case, all cases outside of those geometrically shaped clusters
    are placed into the noise cluster. If you look carefully, though, you may notice
    a sine wave in the data that all three fail to identify as a cluster.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个优点都可以在[图18.1](#ch18fig01)中看到。三个子图分别展示了相同的数据，使用DBSCAN、k-means（Hartigan-Wong算法）或层次聚类（完全连接）进行聚类。这个数据集确实很奇怪，你可能认为不太可能遇到类似的真实世界数据，但它说明了基于密度的聚类相对于k-means和层次聚类的优势。数据中的聚类形状和直径非常不同（当然不是球形的）。虽然k-means和层次聚类学习到的聚类将真实聚类分割和合并，但DBSCAN能够忠实地找到每个形状作为独立的聚类。此外，请注意，k-means和层次聚类将每个案例都放入一个聚类中。DBSCAN创建了一个名为“0”的聚类，将任何它认为是噪声的案例放入其中。在这种情况下，所有那些几何形状的聚类之外的案例都被放入噪声聚类中。然而，如果你仔细观察，你可能会注意到数据中有一个正弦波，而这三个算法都没有将其识别为聚类。
- en: Figure 18.1\. A challenging clustering problem. The dataset shown in each facet
    contains clusters of varying shapes and diameters, with cases that could be considered
    noise. The three subplots show the data clustered using DBSCAN, hierarchical clustering
    (complete linkage), and k-means (Hartigan-Wong). Of the three algorithms used,
    only DBSCAN is able to faithfully represent these shapes as distinct clusters.
  id: totrans-599
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.1\. 一个具有挑战性的聚类问题。每个面展示的数据集包含形状和直径各不相同、可能被认为是噪声的案例的聚类。三个子图展示了使用DBSCAN、层次聚类（完全连接）和k-means（Hartigan-Wong）进行聚类的数据。在这三个算法中，只有DBSCAN能够忠实地将这些形状表示为独立的聚类。
- en: '![](fig18-1_alt.jpg)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-1_alt.jpg)'
- en: So how do density-based clustering algorithms work? Well the DBSCAN algorithm
    is a little easier to understand, so we’ll start with it and build on it to understand
    OPTICS.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 那么基于密度的聚类算法是如何工作的呢？DBSCAN算法相对容易理解，所以我们将从它开始，并在此基础上理解OPTICS。
- en: 18.1.1\. How does the DBSCAN algorithm learn?
  id: totrans-602
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 18.1.1\. DBSCAN算法是如何学习的？
- en: 'In this section, I’ll show you how the DBSCAN algorithm learns regions of high
    density in the data to identify clusters. In order to understand the DBSCAN algorithm,
    you first need to understand its two hyperparameters:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示DBSCAN算法是如何在数据中学习高密度区域以识别聚类的。为了理解DBSCAN算法，您首先需要了解它的两个超参数：
- en: '*epsilon* (ϵ)'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*epsilon*（ϵ）'
- en: '*minPts*'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*minPts*'
- en: The algorithm starts by selecting a case in the data and searching for other
    cases within a search radius. This radius is the *epsilon* hyperparameter. So
    *epsilon* is simply how far away from each case (in an *n*-dimensional sphere)
    the algorithm will search for other cases around a point. Epsilon is expressed
    in units of the feature space and will be the Euclidean distance by default. Larger
    values mean the algorithm will search further away from each case.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 算法首先在数据中选择一个案例，并在搜索半径内寻找其他案例。这个半径是*epsilon*超参数。所以*epsilon*就是算法在点周围搜索其他案例的距离（在一个*n*-维球体中）。Epsilon以特征空间的单位表示，默认情况下是欧几里得距离。更大的值意味着算法将搜索距离每个案例更远的地方。
- en: The *minPts* hyperparameter specifies the minimum number of points (cases) that
    a cluster must have in order for it to be a cluster. The *minPts* hyperparameter
    is therefore an integer. If a particular case has at least *minPts* cases inside
    its *epsilon* radius (including itself), that case is considered a *core point*.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '*minPts*超参数指定了一个聚类必须拥有的最小点数（案例），以便它被视为一个聚类。因此，*minPts*超参数是一个整数。如果一个特定的案例在其*epsilon*半径内（包括自身）至少有*minPts*个案例，那么这个案例被认为是*核心点*。'
- en: Let’s walk through the DBSCAN algorithm together by taking a look at [figure
    18.2](#ch18fig02). The first step of the algorithm is to select a case at random
    from the dataset. The algorithm searches for other cases in an *n*-dimensional
    sphere (where *n* is the number of features in the dataset) with radius equal
    to *epsilon*. If this case contains at least *minPts* cases inside its search
    radius, it is marked as a core point. If the case does *not* contain *minPts*
    cases inside its search space, it is not a core point, and the algorithm moves
    on to another case.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起通过查看 [图 18.2](#ch18fig02) 来了解 DBSCAN 算法。算法的第一步是从数据集中随机选择一个案例。算法在一个 *n*-维球体（其中
    *n* 是数据集中特征的数量）内搜索其他案例，半径等于 *epsilon*。如果这个案例在其搜索半径内包含至少 *minPts* 个案例，它被标记为核心点。如果案例在其搜索空间内不包含
    *minPts* 个案例，它不是核心点，算法继续到另一个案例。
- en: Figure 18.2\. The DBSCAN algorithm. A case is selected at random, and if its
    *epsilon* radius (ϵ) contains *minPts* cases, it is considered a core point. Reachable
    cases of this core point are evaluated the same way until there are no more reachable
    cases. This network of density-connected cases is considered a cluster. Cases
    that are reachable from core points but are not themselves core points are border
    points. The algorithm moves on to the next unvisited case. Cases that are neither
    core nor border points are labeled as noise.
  id: totrans-609
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 18.2\. DBSCAN 算法。随机选择一个案例，如果其 *epsilon* 半径（ϵ）包含至少 *minPts* 个案例，它被认为是核心点。这个核心点的可达案例以相同的方式进行评估，直到没有更多的可达案例。这个密度连接案例的网络被认为是簇。可以从核心点到达但自身不是核心点的案例是边界点。算法继续到下一个未访问的案例。既不是核心点也不是边界点的案例被标记为噪声。
- en: '![](fig18-2_alt.jpg)'
  id: totrans-610
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-2_alt.jpg)'
- en: 'Let’s assume the algorithm picks a case and finds that it is a core point.
    The algorithm then visits each of the cases within *epsilon* of the core point
    and repeats the same task: looks to see if this case has *minPts* cases inside
    its own search radius. Two cases within each other’s search radius are said to
    be *directly density connected* and *reachable* from each other. The search continues
    recursively, following all direct density connections from core points. If the
    algorithm finds a case that is reachable to a core point but does not itself have
    minPts-reachable cases, this case is considered a *border point*. The algorithm
    *only* searches the search space of core points, not border points.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 假设算法选择了一个案例，并发现它是一个核心点。然后算法访问核心点周围 *epsilon* 范围内的每个案例，并重复相同的任务：查看这个案例是否在其自身的搜索半径内有
    *minPts* 个案例。两个位于彼此搜索半径内的案例被称为 *直接密度连接* 并可以从彼此那里 *可达*。搜索递归地进行，跟随从核心点出发的所有直接密度连接。如果算法找到一个可以到达核心点但自身没有
    minPts-可达案例的案例，这个案例被认为是 *边界点*。算法 *只* 搜索核心点的搜索空间，而不是边界点的搜索空间。
- en: Two cases are said to be *density connected* if they are not necessarily directly
    density connected but are connected to each other via a chain or series of directly
    density-connected cases. Once the search has been exhausted, and none of the visited
    cases have any more direct density connections left to explore, all cases that
    are density connected to each other are placed into the same cluster (including
    border points).
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个案例不是直接密度连接，但通过一系列直接密度连接的案例链或系列连接在一起，则称这两个案例为 *密度连接*。一旦搜索完成，并且访问过的案例没有更多的直接密度连接可以探索，所有相互密度连接的案例都被放入同一个簇中（包括边界点）。
- en: The algorithm now selects a different case in the dataset—one that it hasn’t
    visited before—and the same process begins again. Once every case in the dataset
    has been visited, any lonesome cases that were neither core points nor border
    points are added to the noise cluster and are considered too far from regions
    of high density to confidently be clustered with the rest of the cases. So DBSCAN
    finds clusters by finding chains of cases in high-density regions of the feature
    space and throws out cases occupying sparse regions of the feature space.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 现在算法在数据集中选择一个不同的案例——它之前没有访问过的案例——然后相同的流程再次开始。一旦数据集中的每个案例都被访问过，那些既不是核心点也不是边界点的孤立案例将被添加到噪声簇中，并被认为是离高密度区域太远，无法自信地将它们与其它案例聚类。因此，DBSCAN
    通过在特征空间的高密度区域找到案例链来找到簇，并将占据特征空间稀疏区域的案例排除在外。
- en: '|  |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-615
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Not searching outward from border points helps prevent the inclusion of noise
    events into clusters.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 从边界点向外搜索有助于防止噪声事件被包含到簇中。
- en: '|  |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'That was quite a lot of new terminology I just introduced! Let’s have a quick
    recap to make these terms stick in your mind, because they’re also important for
    the OPTICS algorithm:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚刚介绍了很多新的术语！让我们快速回顾一下，以便这些术语能留在你的脑海中，因为它们对OPTICS算法也很重要：
- en: '***Epsilon—*** The radius of an *n*-dimensional sphere around a case, within
    which the algorithm searches for other cases'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Epsilon—*** 案例周围的*n*-维球体的半径，算法在其中搜索其他案例'
- en: '***minPts—*** The minimum number of cases allowed in a cluster, and the number
    of cases that must be within *epsilon* of a case for it to be a core point'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***minPts—*** 簇中允许的最小案例数，以及必须在一个案例的*epsilon*范围内才能成为核心点的案例数'
- en: '***Core point—*** A case that has at least *minPts* reachable cases'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***核心点—*** 至少有*minPts*个可到达案例的案例'
- en: '***Reachable/directly density connected—*** When two cases are within *epsilon*
    of each other'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可到达/直接密度连接—*** 当两个案例彼此之间在*epsilon*范围内时'
- en: '***Density connected—*** When two cases are connected by a chain of directly
    density-connected cases but may not be directly density connected themselves'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***密度连接—*** 当两个案例通过一系列直接密度连接的案例连接，但它们本身可能不是直接密度连接时'
- en: '***Border point—*** A case that is reachable from a core point but is not itself
    a core point'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***边界点—*** 可以从核心点到达但本身不是核心点的案例'
- en: '***Noise point—*** A case that is neither a core point nor reachable from one'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***噪声点—*** 既不是核心点也从未被核心点到达的案例'
- en: 18.1.2\. How does the OPTICS algorithm learn?
  id: totrans-626
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 18.1.2\. OPTICS算法是如何学习的？
- en: In this section, I’ll show you how the OPTICS algorithm learns regions of high
    density in a dataset, how it’s similar to DBSCAN, and how it differs. Technically
    speaking, OPTICS isn’t actually a clustering algorithm. Instead, it creates an
    ordering of the cases in the data in such a way that we can extract clusters from
    it. That sounds a little abstract, so let’s work through how OPTICS works.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示OPTICS算法如何学习数据集中的高密度区域，它与DBSCAN的相似之处以及不同之处。从技术上来说，OPTICS实际上不是一个聚类算法。相反，它以某种方式对数据中的案例进行排序，以便我们可以从中提取簇。这听起来有点抽象，所以让我们来看看OPTICS是如何工作的。
- en: 'The DBSCAN algorithm has one important drawback: it struggles to identify clusters
    that have different densities than each other. The OPTICS algorithm is an attempt
    to alleviate that drawback and identify clusters with varying densities. It does
    this by allowing the search radius around each case to expand dynamically instead
    of being fixed at a predetermined value.'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN算法有一个重要的缺点：它难以识别具有不同密度的簇。OPTICS算法试图缓解这一缺点，并识别具有不同密度的簇。它是通过允许每个案例周围的搜索半径动态扩展，而不是固定在预定的值来实现的。
- en: 'In order to understand how OPTICS works, I need to introduce two new terms:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解OPTICS是如何工作的，我需要介绍两个新术语：
- en: Core distance
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心距离
- en: Reachability distance
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可达性距离
- en: 'In OPTICS, the search radius around a case isn’t fixed but expands until there
    are at least *minPts* cases within it. This means cases in dense regions of the
    feature space will have a small search radius, and cases in sparse regions will
    have a large search radius. The smallest distance away from a case that includes
    *minPts* other cases is called the *core distance*, sometimes abbreviated to ϵ′.
    In fact, the OPTICS algorithm only has one mandatory hyperparameter: *minPts*.'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 在OPTICS中，一个案例周围的搜索半径不是固定的，而是扩展到至少包含*minPts*个案例。这意味着特征空间中密集区域的案例将具有较小的搜索半径，而稀疏区域的案例将具有较大的搜索半径。包含至少*minPts*个其他案例的案例与该案例的最小距离称为*核心距离*，有时简称为ϵ′。实际上，OPTICS算法只有一个强制性的超参数：*minPts*。
- en: '|  |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-634
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: We can still supply *epsilon*, but it is mostly used to speed up the algorithm
    by acting as a maximum core distance. In other words, if the core distance reaches
    *epsilon*, just take *epsilon* as the core distance to prevent all cases in the
    dataset from being considered.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以提供*epsilon*，但它主要用于通过充当最大核心距离来加速算法。换句话说，如果核心距离达到*epsilon*，只需将*epsilon*作为核心距离，以防止数据集中的所有案例都被考虑。
- en: '|  |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The *reachability distance* is the distance between a core point and another
    core point within its *epsilon*, but it cannot be less than the core distance.
    Put another way, if a case has a core point *inside* its core distance, the reachability
    distance between these cases *is* the core distance. If a case has a core point
    *outside* its core distance, then the reachability distance between these cases
    is simply the Euclidean distance between them.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '**可达距离**是指一个核心点与其epsilon范围内的另一个核心点之间的距离，但不能小于核心距离。换句话说，如果一个案例的核心点在其核心距离**内部**，那么这些案例之间的可达距离**就是**核心距离。如果一个案例的核心点在其核心距离**外部**，那么这些案例之间的可达距离就是它们之间的欧几里得距离。'
- en: '|  |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-639
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In OPTICS, a case is a core point if there are *minPts* inside *epsilon*. If
    we don’t specify *epsilon*, then all cases will be core points. The reachability
    distance between a case and a non-core point is undefined.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在OPTICS中，如果一个案例在其epsilon范围内有*minPts*个案例，则该案例是核心点。如果我们不指定epsilon，则所有案例都将被视为核心点。一个案例与非核心点之间的可达距离是未定义的。
- en: '|  |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Take a look at the example in [figure 18.3](#ch18fig03). You can see two circles
    centered around the darkly shaded case. The circle with the larger radius is *epsilon*,
    and the one with the smaller radius is the core distance (ϵ'). This example is
    showing the core distance for a *minPts* value of 4, because the core distance
    has expanded to include four cases (including the case in question). The arrows
    indicate the reachability distance between the core point and the other cases
    within its *epsilon*.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下图18.3中的示例。你可以看到围绕深色阴影案例的两个圆圈。半径较大的圆圈是*epsilon*，半径较小的圆圈是核心距离（ϵ'）。这个例子展示了值为4的*minPts*的核心距离，因为核心距离已经扩展以包含四个案例（包括所讨论的案例）。箭头指示核心点与其epsilon范围内的其他案例之间的可达距离。
- en: Figure 18.3\. Defining the core distance and reachability distance. In OPTICS,
    *epsilon* (ϵ) is the maximum search distance. The core distance (ϵ′) is the minimum
    search distance needed to include *minPts* cases (including the case in question).
    The reachability distance for a case is the larger of the core distance and the
    distance between the case in question, and another case inside its *epsilon* (maximum
    search distance).
  id: totrans-643
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.3\. 定义核心距离和可达距离。在OPTICS中，*epsilon*（ϵ）是最大搜索距离。核心距离（ϵ′）是需要包含*minPts*个案例（包括所讨论的案例）的最小搜索距离。一个案例的可达距离是该案例的核心距离和与另一个在其epsilon范围内的案例之间的距离中的较大者。
- en: '![](fig18-3.jpg)'
  id: totrans-644
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-3.jpg)'
- en: Because the reachability distance is the distance between one core point and
    another core point within its *epsilon*, OPTICS needs to know which cases are
    core points. So the algorithm starts by visiting every case in the data and determining
    whether its core distance is less than *epsilon*. This is illustrated in [figure
    18.4](#ch18fig04). If a case’s core distance is less than or equal to *epsilon*,
    the case is a core point. If a case’s core distance is greater than *epsilon*
    (we need to expand out further than *epsilon* to find *minPts* cases), the case
    is *not* a core point. Examples of both are shown in [figure 18.4](#ch18fig04).
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 因为可达距离是一个核心点与其epsilon范围内的另一个核心点之间的距离，所以OPTICS需要知道哪些案例是核心点。因此，算法首先遍历数据中的每个案例，并确定其核心距离是否小于epsilon。这如图18.4所示。如果一个案例的核心距离小于或等于epsilon，则该案例是核心点。如果一个案例的核心距离大于epsilon（我们需要扩展到epsilon之外以找到*minPts*个案例），则该案例**不是**核心点。两种情况都在图18.4中展示。
- en: Figure 18.4\. Defining core points in OPTICS. Cases for which the core distance
    (ϵ′) is less than or equal to the maximum search distance (ϵ) are considered core
    points. Cases for which the core distance is greater than the maximum search distance
    are not considered core points.
  id: totrans-646
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.4\. 在OPTICS中定义核心点。核心距离（ϵ′）小于或等于最大搜索距离（ϵ）的案例被认为是核心点。核心距离大于最大搜索距离的案例不被认为是核心点。
- en: '![](fig18-4.jpg)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-4.jpg)'
- en: Now that you understand the concepts of core distance and reachability distance,
    let’s see how the OPTICS algorithm works. The first step is to visit each case
    in the data and mark it as a core point or not. The rest of the algorithm is illustrated
    in [figure 18.5](#ch18fig05), so let’s assume this has been done. OPTICS selects
    a case and calculates its core distance and its reachability distance to all cases
    inside its *epsilon* (the maximum search distance).
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了核心距离和可达距离的概念，让我们看看OPTICS算法是如何工作的。第一步是访问数据集中的每个情况，并标记它是否为核心点。算法的其余部分在图18.5中展示，所以假设这部分已经完成。OPTICS选择一个情况，并计算它与所有在其*epsilon*（最大搜索距离）内的情况的可达距离。
- en: 'The algorithm does two things before moving on to the next case:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动到下一个情况之前，算法做两件事：
- en: Records the *reachability score* of the case
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录情况的*可达分数*
- en: Updates the processing order of cases
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新情况的处理顺序
- en: 'The reachability score of a case is different from a reachability distance
    (the terminology is unfortunately confusing). A case’s reachability score is defined
    as the larger of its core distance or its smallest reachability distance. Let’s
    rephrase: if a case doesn’t have *minPts* cases inside *epsilon* (it isn’t a core
    point), then its reachability score will be the reachability distance to its closest
    core point. If a case *does* have *minPts* cases inside *epsilon*, then its smallest
    reachability distance will be less than or equal to its core distance, so we just
    take the core distance as the case’s reachability score.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 情况的可达分数与可达距离不同（术语很不幸地令人困惑）。一个情况的可达分数定义为它的核心距离或其最小可达距离中的较大者。让我们重新表述：如果一个情况在其*epsilon*内没有*minPts*个情况（它不是核心点），那么它的可达分数将是到其最近核心点的可达距离。如果一个情况在其*epsilon*内确实有*minPts*个情况，那么它的最小可达距离将小于或等于其核心距离，所以我们只需将核心距离作为该情况的可达分数。
- en: Figure 18.5\. The OPTICS algorithm. A case is selected, and its core distance
    (ϵ′) is measured. The reachability distance is calculated between this case and
    all the cases inside this case’s maximum search distance (ϵ). The processing order
    of the dataset is updated such that the nearest case is visited next. The reachability
    score and the processing order are recorded for this case, and the algorithm moves
    on to the next one.
  id: totrans-653
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.5\. OPTICS算法。选择一个情况，并测量其核心距离（ϵ′）。计算该情况与其最大搜索距离（ϵ）内的所有情况的可达距离。更新数据集的处理顺序，以便访问最近的下一个情况。记录该情况的可达分数和处理顺序，然后算法继续处理下一个情况。
- en: '![](fig18-5_alt.jpg)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-5_alt.jpg)'
- en: '|  |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-656
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Therefore, the reachability score of a case will never be less than its core
    distance, unless the core distance is greater than the maximum, *epsilon*, in
    which case *epsilon* will be the reachability score.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，情况的可达分数永远不会小于其核心距离，除非核心距离大于最大值*epsilon*，在这种情况下，*epsilon*将是可达分数。
- en: '|  |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Once the reachability has been recorded for a particular case, the algorithm
    then updates the sequence of cases it’s going to visit next (the processing order).
    It updates the processing order such that it will next visit the core point with
    the smallest reachability distance to the current case, then the one that is next-farthest
    away, and so on. This is illustrated in step 2 of [figure 18.5](#ch18fig05).
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦记录了特定情况的可达性，算法随后更新它将要访问的下一个情况的序列（处理顺序）。它更新处理顺序，以便接下来访问与当前情况具有最小可达距离的核心点，然后是下一个最远的点，依此类推。这可以在图18.5的第2步中看到。
- en: The algorithm then visits the next case in the updated processing order and
    repeats the same process, likely changing the processing order once again. When
    there are no more reachable cases in the current chain, the algorithm moves on
    to the next unvisited core point in the dataset and repeats the process.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 然后算法按照更新后的处理顺序访问下一个情况，并重复相同的过程，可能会再次更改处理顺序。当当前链中没有更多可达情况时，算法将移动到数据集中下一个未访问的核心点，并重复该过程。
- en: Once all cases have been visited, the algorithm returns both the processing
    order (the order in which each case was visited) and the reachability score of
    each case. If we plot processing order against reachability score, we get something
    like the top plot in [figure 18.6](#ch18fig06). To generate this plot, I applied
    the OPTICS algorithm to a simulated dataset with four clusters (you can find the
    code to reproduce this at [www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr)).
    Notice that when we plot the processing order against the reachability score,
    we get four shallow troughs, each separated by spikes of high reachability. Each
    trough in the plot corresponds to a region of high density, while each spike indicates
    a separation of these regions by a region of low density.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦访问了所有案例，算法将返回处理顺序（每个案例被访问的顺序）和每个案例的可达性分数。如果我们绘制处理顺序与可达性分数的关系图，我们得到类似于[图18.6](#ch18fig06)顶部的图。为了生成这个图，我使用具有四个簇的模拟数据集应用了OPTICS算法（你可以在这个[www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr](http://www.manning.com/books/machine-learning-with-r-the-tidyverse-and-mlr)找到重现此图的代码）。注意，当我们绘制处理顺序与可达性分数的关系图时，我们得到四个浅谷，每个谷之间都由高可达性的尖峰隔开。图中的每个谷对应于高密度区域，而每个尖峰表示这些区域通过低密度区域进行分离。
- en: Figure 18.6\. Reachability plot of a simulated dataset. The top plot shows the
    reachability plot learned by the OPTICS algorithm from the data shown in the bottom
    plot. The plots are shaded to indicate where each cluster in the feature space
    maps onto the reachability plot.
  id: totrans-662
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.6\. 模拟数据集的可达性图。上面的图显示了OPTICS算法从下面图中所示的数据中学习到的可达性图。图被着色以表示特征空间中的每个簇映射到可达性图的位置。
- en: '![](fig18-6_alt.jpg)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
  zh: '![图18.6的替代图](fig18-6_alt.jpg)'
- en: '|  |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-665
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The deeper the trough, the higher the density.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 谷越深，密度越高。
- en: '|  |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The OPTICS algorithm actually goes no further than this. Once it produces this
    plot, its work is done, and now it’s our job to use the information contained
    in the plot to extract the cluster membership. This is why I said that OPTICS
    isn’t technically a clustering algorithm but creates an ordering of the data that
    allows us to find clusters in the data.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: OPTICS算法实际上并没有进一步做。一旦它生成了这个图，它的任务就完成了，现在是我们使用图中包含的信息来提取簇成员的工作。这就是为什么我说OPTICS在技术上不是一个聚类算法，而是创建了一个数据排序，使我们能够在数据中找到簇。
- en: So how do we extract clusters? We have a couple of options. One method would
    be to simply draw a horizontal line across the reachability plot, at some reachability
    score, and define the start and end of clusters as when the plot dips below and
    back above the line. Any cases above the line could be classified as noise, as
    shown in the top plot of [figure 18.7](#ch18fig07). This approach will result
    in clustering very similar to what the DBSCAN algorithm would produce, except
    that some border points are more likely to be put into the noise cluster.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何提取簇呢？我们有几种选择。一种方法是在可达性图上简单地画一条水平线，在某个可达性分数处，并将簇的开始和结束定义为图下降并再次上升至该线以下的位置。任何在直线以上的案例可以分类为噪声，如[图18.7](#ch18fig07)顶部的图所示。这种方法将导致与DBSCAN算法产生的聚类非常相似，但某些边界点更有可能被放入噪声簇中。
- en: Figure 18.7\. An illustration of different ways clusters can be extracted from
    a reachability plot. In the top plot, a single reachability score cut-off has
    been defined, and any troughs bordered by peaks above this cut-off are defined
    as clusters. In the bottom plot, a hierarchy of clusters is defined, based on
    the steepness of changes in reachability, allowing for clusters within clusters.
  id: totrans-670
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.7\. 从可达性图中提取不同方式簇的示意图。在上面的图中，定义了一个单一的可达性分数截止值，任何被高于此截止值的峰值包围的谷都被定义为簇。在下面的图中，根据可达性变化的陡峭程度定义了一个簇的层次结构，允许簇中有簇。
- en: '![](fig18-7_alt.jpg)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
  zh: '![图18.7的替代图](fig18-7_alt.jpg)'
- en: Another (usually more useful) method is to define a particular *steepness* in
    the reachability plot as indicative of the start and end of a cluster. We can
    define the start of a cluster as when we have a downward slope of at least this
    steepness, and its end as when we have an upward slope of at least this steepness.
    The method we’ll use later defines the steepness as 1 – ξ (xi, pronounced “zy,”
    “sigh,” or “kzee,” depending on your preference and mathematics teacher), where
    the reachability of two successive cases must change by a factor of 1 – ξ. When
    we have a downward slope that meets this steepness criterion, the start of a cluster
    is defined; and when we have an upward slope that meets this steepness, the end
    of the cluster is defined.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种（通常更有用）的方法是在可达性图中定义特定的*陡度*作为簇开始和结束的指示。我们可以将簇的开始定义为当我们有一个至少这种陡度的下坡时，其结束定义为当我们有一个至少这种陡度的上坡时。我们稍后将使用的方法将陡度定义为1
    – ξ（xi，发音为“zy”，“sigh”或“kzee”，取决于你的偏好和数学老师），其中两个连续案例的可达性必须改变一个1 – ξ的因子。当我们有一个满足这个陡度标准的下坡时，簇的开始就被定义了；当我们有一个满足这个陡度的上坡时，簇的结束就被定义了。
- en: '|  |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-674
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because ξ cannot be estimated from the data, it is a hyperparameter we must
    select/tune ourselves.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 因为ξ无法从数据中估计出来，它是一个我们必须自己选择/调整的超参数。
- en: '|  |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Using this method has two major benefits. First, it allows us to overcome DBSCAN’s
    limitation of only finding clusters of equal density. Second, it allows us to
    find clusters within clusters, to form a hierarchy. Imagine that we have a downward
    slope that starts a cluster, and then we have *another* downward slope before
    the cluster ends: we have a cluster within a cluster. This hierarchical extraction
    of clusters from a reachability plot is shown in the bottom plot in [figure 18.7](#ch18fig07).'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法有两个主要好处。首先，它使我们能够克服DBSCAN只能找到相同密度簇的限制。其次，它使我们能够在簇内找到簇，从而形成层次结构。想象一下，我们有一个开始簇的下坡，然后在簇结束前还有一个*另一个*下坡：我们有一个簇内的簇。这种从可达性图中提取簇的层次结构在图18.7的底部图中显示。
- en: '|  |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-679
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that neither of these methods works with the original data. They extract
    all the information to assign cluster memberships from the order and reachability
    scores generated by the OPTICS algorithm.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这两种方法都不能直接应用于原始数据。它们会从OPTICS算法生成的顺序和可达性分数中提取所有信息来分配簇成员资格。
- en: '|  |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 18.2\. Building your first DBSCAN model
  id: totrans-682
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.2. 构建第一个DBSCAN模型
- en: In this section, I’m going to show you how to use the DBSCAN algorithm to cluster
    a dataset. We’ll then use some of the techniques you learned in [chapter 17](kindle_split_030.html#ch17)
    to validate its performance and select the best-performing hyperparameter combination.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示如何使用DBSCAN算法对数据集进行聚类。然后，我们将使用你在第17章（kindle_split_030.html#ch17）中学到的一些技术来验证其性能并选择最佳性能的超参数组合。
- en: '|  |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-685
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The mlr package does have a learner for the DBSCAN algorithm (`cluster.dbscan`),
    but we’re not going to use it. There’s nothing wrong with it; but as you’ll see
    later, the presence of the noise cluster causes problems for our internal cluster
    metrics, so we’re going to do our own performance validation outside of mlr.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: mlr包确实有DBSCAN算法的学习器（`cluster.dbscan`），但我们不会使用它。它没有问题；但正如你稍后将会看到的，噪声簇的存在会给我们内部簇度量带来问题，所以我们将要在mlr之外进行自己的性能验证。
- en: '|  |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 18.2.1\. Loading and exploring the banknote dataset
  id: totrans-688
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 18.2.1. 加载和探索纸币数据集
- en: Let’s start by loading the tidyverse and loading in the data, which is part
    of the mclust package. We’re going to work with the Swiss banknote dataset to
    which we applied PCA, t-SNE, and UMAP in [chapters 13](kindle_split_025.html#ch13)
    and [14](kindle_split_026.html#ch14). Once we’ve loaded in the data, we convert
    it into a tibble and create a separate tibble after scaling the data (because
    DBSCAN and OPTICS are sensitive to variable scales). Because we’re going to imagine
    that we have no ground truth, we remove the `Status` variable, indicating which
    banknotes are genuine and which are counterfeit. Recall that the tibble contains
    200 Swiss banknotes, with 6 measurements of their dimensions.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载tidyverse和加载数据开始，这些数据是mclust包的一部分。我们将使用瑞士纸币数据集，我们在第13章（kindle_split_025.html#ch13）和第14章（kindle_split_026.html#ch14）中对其应用了PCA、t-SNE和UMAP。一旦我们加载数据，我们将将其转换为tibble，并在对数据进行缩放后创建一个单独的tibble（因为DBSCAN和OPTICS对变量尺度敏感）。因为我们将假设我们没有地面实况，所以我们移除了`Status`变量，该变量指示哪些纸币是真钞，哪些是假钞。回想一下，tibble包含200张瑞士纸币，有6个关于其尺寸的测量值。
- en: Listing 18.1\. Loading the tidyverse packages and dataset
  id: totrans-690
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.1\. 加载 tidyverse 包和数据集
- en: '[PRE30]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s plot the data using `ggpairs()` to remind ourselves of the structure of
    the data.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `ggpairs()` 绘制数据，以提醒自己数据的结构。
- en: Listing 18.2\. Plotting the data
  id: totrans-693
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.2\. 绘制数据
- en: '[PRE31]'
  id: totrans-694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The resulting plot is shown in [figure 18.8](#ch18fig08). It looks as though
    there are at least two regions of high density in the data, with a few scattered
    cases in lower-density regions.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在 [图 18.8](#ch18fig08)。看起来数据中至少有两个高密度区域，在低密度区域有一些散布的案例。
- en: 18.2.2\. Tuning the epsilon and minPts hyperparameters
  id: totrans-696
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 18.2.2\. 调整 epsilon 和 minPts 超参数
- en: In this section, I’ll show you how to select sensible ranges of *epsilon* and
    *minPts* for DBSCAN, and how we can tune them manually to find the best-performing
    combination. Choosing the value of the *epsilon* hyperparameter is, perhaps, not
    obvious. How far away from each case should we search? Luckily, there is a heuristic
    method we can use to at least get in the right ballpark. This consists of calculating
    the distance from each point to its *k*th-nearest neighbor and then ordering the
    points in a plot based on this distance. In data with regions of high and low
    density, this tends to produce a plot containing a “knee” or “elbow” (depending
    on your preference). The optimal value of *epsilon* is in or near that knee/elbow.
    Because a core point in DBSCAN has *minPts* cases inside its *epsilon*, choosing
    a value of *epsilon* at the knee of this plot means choosing a search distance
    that will result in cases in high-density regions being considered core points.
    We can create this plot using the `kNNdistplot()` function from the dbscan package.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何为 DBSCAN 选择合理的 *epsilon* 和 *minPts* 范围，以及我们如何手动调整它们以找到最佳性能组合。选择
    *epsilon* 超参数的值可能并不明显。我们应该搜索多远？幸运的是，我们可以使用一种启发式方法来至少得到正确的范围。这包括计算每个点到其第 *k* 个最近邻的距离，然后根据这个距离在图中对点进行排序。在密度高和密度低区域的数据中，这往往会产生一个包含“拐点”或“肘部”（取决于您的偏好）的图。*epsilon*
    的最佳值就在那个拐点/肘部附近。因为 DBSCAN 中的核心点在其 *epsilon* 内有 *minPts* 个案例，所以在该图的拐点选择 *epsilon*
    的值意味着选择一个搜索距离，这将导致高密度区域的案例被视为核心点。我们可以使用 dbscan 包中的 `kNNdistplot()` 函数创建此图。
- en: Figure 18.8\. Plotting the Swiss banknote dataset with `ggpairs()`. 2D density
    plots are shown above the diagonal.
  id: totrans-698
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 18.8\. 使用 `ggpairs()` 绘制瑞士银行钞票数据集。2D 密度图显示在对角线以上。
- en: '![](fig18-8_alt.jpg)'
  id: totrans-699
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-8_alt.jpg)'
- en: Listing 18.3\. Plotting the kNN distance plot
  id: totrans-700
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.3\. 绘制 kNN 距离图
- en: '[PRE32]'
  id: totrans-701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We need to use the `k` argument to specify the number of nearest neighbors
    we want to calculate the distance to. But we don’t yet know what our *minPts*
    argument should be, so how can we set `k`? I usually pick a sensible value that
    I believe is approximately correct (remember that *minPts* defines the minimum
    cluster size): here, I’ve selected 5\. The position of the knee in the plot is
    relatively robust to changes in `k`.'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用 `k` 参数来指定我们想要计算距离的最近邻的数量。但我们还不知道我们的 *minPts* 参数应该是多少，所以我们如何设置 `k`？我通常会选择一个我认为大约正确的合理值（记住
    *minPts* 定义了最小聚类大小）：这里，我选择了 5。图中拐点的位置对 `k` 的变化相对稳健。
- en: The `kNNdistplot()` function will create a matrix with as many rows as there
    are cases in the dataset (200) and 5 columns, one for the distance between each
    case and each of its 5 nearest neighbors. Each of these 200 × 5 = 1,000 distances
    will be drawn on the plot.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '`kNNdistplot()` 函数将创建一个矩阵，其行数与数据集中的案例数（200）相同，有 5 列，每一列代表每个案例与其 5 个最近邻之间的距离。这些
    200 × 5 = 1,000 个距离将在图中绘制。'
- en: We then use the `abline()` function to draw horizontal lines at the start and
    end of the knee, to help us identify the range of *epsilon* values we’re going
    to tune over. The resulting plot is shown in [figure 18.9](#ch18fig09). Notice
    that, reading the plot from left to right, after an initial sharp increase, the
    5-nearest-neighbor distance increases only gradually, until it rapidly increases
    again. This region where the curve inflects upward is the knee/elbow, and the
    optimal value of *epsilon* at this nearest-neighbor distance in this inflection.
    Using this method, we select 1.2 and 2.0 as the lower and upper limits over which
    to tune *epsilon*.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用 `abline()` 函数在膝部的起始和结束位置绘制水平线，以帮助我们确定将要调整的 *epsilon* 值的范围。生成的图表显示在 [图
    18.9](#ch18fig09) 中。注意，从左到右读取图表，在最初的急剧增加之后，5 个最近邻的距离仅逐渐增加，直到再次急剧增加。曲线向上弯曲的这个区域是膝部/肘部，在这个弯曲点处的
    *epsilon* 的最佳值。使用这种方法，我们选择 1.2 和 2.0 作为调整 *epsilon* 的上下限。
- en: Figure 18.9\. K-nearest neighbor distance plot with *k* = 5\. Horizontal lines
    have been drawn using `abline()` to highlight the 5-NN distances at the start
    and end of the knee/elbow in the plot.
  id: totrans-705
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 18.9\. *k* = 5 的 k-最近邻距离图。使用 `abline()` 绘制了水平线，以突出显示图表中膝部/肘部起始和结束位置的 5-NN
    距离。
- en: '![](fig18-9_alt.jpg)'
  id: totrans-706
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-9_alt.jpg)'
- en: Let’s manually define our hyperparameter search space for *epsilon* and *minPts*.
    We use the `expand.grid()` function to create a data frame containing every combination
    of the values of *epsilon* (`eps`) and *minPts* we want to search over. We’re
    going to search across values of *epsilon* between 1.2 and 2.0, in steps of 0.1;
    and we’re going to search across values of *minPts* between 1 and 9, in steps
    of 1.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动定义我们的 *epsilon* 和 *minPts* 的超参数搜索空间。我们使用 `expand.grid()` 函数创建一个数据框，包含我们想要搜索的
    *epsilon* (`eps`) 和 *minPts* 的所有值组合。我们将搜索 1.2 和 2.0 之间的 *epsilon* 值，步长为 0.1；我们将搜索
    1 和 9 之间的 *minPts* 值，步长为 1。
- en: Listing 18.4\. Defining our hyperparameter search space
  id: totrans-708
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.4\. 定义我们的超参数搜索空间
- en: '[PRE33]'
  id: totrans-709
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '|  |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: Print the `dbsParamSpace` object to give yourself a better intuition of what
    `expand .grid()` is doing.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 打印 `dbsParamSpace` 对象，以更好地理解 `expand .grid()` 正在做什么。
- en: '|  |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that we’ve defined our hyperparameter search space, let’s run the DBSCAN
    algorithm on each distinct combination of *epsilon* and *minPts*. To do this,
    we use the `pmap()` function from the purrr package to apply the `dbscan()` function
    to each row of the `dbsParamSpace` object.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的超参数搜索空间，让我们对每个不同的 *epsilon* 和 *minPts* 组合运行 DBSCAN 算法。为此，我们使用 purrr
    包中的 `pmap()` 函数将 `dbscan()` 函数应用于 `dbsParamSpace` 对象的每一行。
- en: Listing 18.5\. Running DBSCAN on each combination of hyperparameters
  id: totrans-715
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.5\. 对每个超参数组合运行 DBSCAN
- en: '[PRE34]'
  id: totrans-716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We supply our scaled dataset as the argument to `dbscan()`’s argument, `x`.
    The output from `pmap()` is a list where each element is the result of running
    DBSCAN on that particular combination of *epsilon* and *minPts*. To view the output
    for a particular permutation, we simply subset the list.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将缩放后的数据集作为 `dbscan()` 的参数 `x` 的参数。`pmap()` 的输出是一个列表，其中每个元素都是对特定 *epsilon*
    和 *minPts* 组合运行 DBSCAN 的结果。要查看特定排列的输出，我们只需对列表进行子集化。
- en: The output, when printing the result of a `dbscan()` call, tells us the number
    of objects in the data, the values of *epsilon* and *minPts*, and the number of
    identified clusters and noise points. Perhaps the most important information is
    the number of cases within each cluster. In this example, we can see there are
    189 cases in cluster 2, and just a single case in most of the other clusters.
    This is because this permutation was run with *minPts* equal to 1, which allows
    clusters to contain just a single case. This is rarely what we want and will result
    in a clustering model where no cases are identified as noise.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 当打印 `dbscan()` 调用的结果时，输出告诉我们数据中的对象数量，*epsilon* 和 *minPts* 的值，以及识别出的簇和噪声点的数量。也许最重要的信息是每个簇内的案例数量。在这个例子中，我们可以看到簇
    2 中有 189 个案例，而在大多数其他簇中只有一个案例。这是因为这个排列运行时 *minPts* 等于 1，这允许簇只包含一个案例。这很少是我们想要的，并且会导致一个聚类模型，其中没有案例被识别为噪声。
- en: Now that we have our clustering result, we should visually inspect the clustering
    to see which (if any) of the permutations give a sensible result. To do this,
    we want to extract the vector of cluster membership from each permutation as a
    column and then add these columns to our original data.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了聚类结果，我们应该通过视觉检查聚类，看看哪些（如果有的话）排列给出了合理的结果。为此，我们想要从每个排列中提取聚类成员的向量作为一列，然后将这些列添加到我们的原始数据中。
- en: 'The first step is to extract the cluster memberships as separate columns in
    a tibble. To do this, we use the `map_dfc()` function. We’ve encountered the `map_df()`
    function before: it applies a function to each element of a vector and returns
    the output as a tibble, where each output forms a different row of the tibble.
    This is actually the same as using `map_dfr()`, where the *r* means row-binding.
    If, instead, we want each output to form a different *column* of the tibble, we
    use `map_dfc()`.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是从 tibble 中提取聚类成员作为单独的列。为此，我们使用 `map_dfc()` 函数。我们之前已经遇到过 `map_df()` 函数：它将函数应用于向量的每个元素，并将输出作为
    tibble 返回，其中每个输出形成 tibble 的不同行。这实际上与使用 `map_dfr()` 相同，其中 *r* 代表行绑定。如果我们想使每个输出形成
    tibble 的不同 *列*，我们则使用 `map_dfc()`。
- en: '|  |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-722
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: I’ve truncated the output here for the sake of space.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 这里为了节省空间，已截断输出。
- en: '|  |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 18.6\. Cluster memberships from DBSCAN permutations
  id: totrans-725
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.6\. DBSCAN 排列的聚类成员关系
- en: '[PRE35]'
  id: totrans-726
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that we have our tibble of cluster memberships, let’s use the `bind_cols()`
    function to, well, bind the columns of our `swissTib` tibble and our tibble of
    cluster memberships. We call this new tibble `swissClusters`, which sounds like
    a breakfast cereal. Notice that we have our original variables, with additional
    columns containing the cluster membership output from each permutation.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了聚类成员的 tibble，让我们使用 `bind_cols()` 函数来，嗯，绑定 `swissTib` tibble 和我们的聚类成员 tibble
    的列。我们称这个新的 tibble 为 `swissClusters`，听起来像是一种早餐谷物。请注意，我们有原始变量，以及包含每个排列的聚类成员输出的额外列。
- en: '|  |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-729
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Again, I’ve truncated the output slightly to save space.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了节省空间，我稍微截断了输出。
- en: '|  |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 18.7\. Binding cluster memberships to the original data
  id: totrans-732
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.7\. 将聚类成员绑定到原始数据
- en: '[PRE36]'
  id: totrans-733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In order to plot the results, we would like to facet by permutation so we can
    draw a separate subplot for each combination of our hyperparameters. To do this,
    we need to `gather()` the data to create a new column indicating permutation number
    and another column indicating the cluster number.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制结果，我们希望按排列进行分面，这样我们就可以为超参数的每个组合绘制一个单独的子图。为此，我们需要 `gather()` 数据以创建一个新列，指示排列编号，另一个列指示聚类编号。
- en: Listing 18.8\. Gathering the data, ready for plotting
  id: totrans-735
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.8\. 收集数据，准备绘图
- en: '[PRE37]'
  id: totrans-736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Great—now our tibble is in a format ready for plotting. Looking back at [figure
    18.8](#ch18fig08), we can see that the variables that most obviously separate
    clusters in the data are `Right` and `Diagonal`. As such, we’ll plot these variables
    against each other by mapping them to the x and y aesthetics, respectively. We
    map the `Cluster` variable to the color aesthetic (wrapping it inside `as.factor()`
    so the colors aren’t drawn as a single gradient). We then facet by `Permutation`,
    add a `geom_point()` layer, and add a theme. Because some of the cluster models
    have a large number of clusters, we suppress the drawing of what would be a very
    large legend, by adding the line `theme(legend.position = "none")`.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了——现在我们的 tibble 已经处于绘图格式。回顾 [图 18.8](#ch18fig08)，我们可以看到最明显区分数据中聚类的变量是 `Right`
    和 `Diagonal`。因此，我们将这些变量分别映射到 x 和 y 美学，以相互绘制。我们将 `Cluster` 变量映射到颜色美学（将其包裹在 `as.factor()`
    中，以便颜色不会绘制成单一渐变）。然后按 `Permutation` 分面，添加 `geom_point()` 层，并添加一个主题。由于一些聚类模型有大量的聚类，我们通过添加
    `theme(legend.position = "none")` 行来抑制绘制非常大的图例。
- en: Listing 18.9\. Plotting cluster memberships of permutations
  id: totrans-738
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.9\. 绘制排列的聚类成员关系
- en: '[PRE38]'
  id: totrans-739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '|  |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-741
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `theme()` function allows you to control the appearance of your plots (such
    as changing background colors, gridlines, font sizes, and so on). To find out
    more, call `?theme`.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: '`theme()` 函数允许您控制图表的外观（例如更改背景颜色、网格线、字体大小等）。要了解更多信息，请调用 `?theme`。'
- en: '|  |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The resulting plot is shown in [figure 18.10](#ch18fig10). We can see that different
    combinations of *epsilon* and *minPts* have resulted in substantially different
    clustering models. Many of these models capture the two obvious clusters in the
    dataset, but most do not.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图18.10](#ch18fig10)中。我们可以看到，不同的**epsilon**和**minPts**组合导致了实质上不同的聚类模型。许多这些模型捕捉到了数据集中的两个明显的聚类，但大多数没有。
- en: '|  |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: 'Let’s also visualize the number and size of the clusters returned by each permutation.
    Pass our `swissClustersGathered` object to `ggplot()` with the following aesthetic
    mappings:'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还将可视化每个排列返回的聚类数量和大小。将我们的`swissClustersGathered`对象传递给`ggplot()`，并使用以下美学映射：
- en: '`x = reorder(Permutation, Cluster)`'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x = reorder(Permutation, Cluster)`'
- en: '`y = fill = as.factor(Cluster)`'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y = fill = as.factor(Cluster)`'
- en: Also add a `geom_bar()` layer. Now draw the same plot again, but this time add
    a `coord_polar()` layer. Change the x aesthetic mapping to just `Permutation`.
    Can you see what the `reorder()` function was doing?
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 还添加一个`geom_bar()`层。现在再次绘制相同的图表，但这次添加一个`coord_polar()`层。将x美学映射仅设置为`Permutation`。你能看到`reorder()`函数做了什么吗？
- en: '|  |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 18.10\. Visualizing the result of our tuning experiment. Each subplot
    shows the `Right` and `Diagonal` variables plotted against each other for a different
    permutation of *epsilon* and *minPts*. Cases are shaded by their cluster membership.
  id: totrans-752
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.10。可视化我们的调整实验的结果。每个子图显示了**epsilon**和**minPts**的不同排列下，`Right`和`Diagonal`变量相互之间的绘图。案例根据其聚类成员进行着色。
- en: '![](fig18-10_alt.jpg)'
  id: totrans-753
  prefs: []
  type: TYPE_IMG
  zh: '![图18-10](fig18-10_alt.jpg)'
- en: How are we going to choose the best-performing combination of *epsilon* and
    *minPts*? Well, as we saw in [chapter 17](kindle_split_030.html#ch17), visually
    checking to make sure the clusters are sensible is important, but we can also
    calculate internal cluster metrics to help guide our choice.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如何选择表现最佳的**epsilon**和**minPts**组合？嗯，正如我们在[第17章](kindle_split_030.html#ch17)中看到的，确保聚类是有意义的视觉检查很重要，但我们还可以计算内部聚类指标以帮助我们的选择。
- en: In [chapter 17](kindle_split_030.html#ch17), we defined our own function that
    would take the data and the cluster membership from a clustering model and calculate
    the Davies-Bouldin and Dunn indices and the pseudo F statistic. Let’s redefine
    this function to refresh your memory.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第17章](kindle_split_030.html#ch17)中，我们定义了自己的函数，该函数将接受聚类模型中的数据和聚类成员，并计算Davies-Bouldin和Dunn指数以及伪F统计量。让我们重新定义这个函数以刷新你的记忆。
- en: Listing 18.10\. Defining the `cluster_metrics()` function
  id: totrans-756
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表18.10。定义`cluster_metrics()`函数
- en: '[PRE39]'
  id: totrans-757
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To help us select which of our clustering models best captures the structure
    in the data, we’re going to take bootstrap samples from our dataset and run DBSCAN
    using all 81 combinations of *epsilon* and *minPts* on each bootstrap sample.
    We can then calculate the mean of each of our performance metrics and see how
    stable they are.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们选择哪个聚类模型最能捕捉数据中的结构，我们将从我们的数据集中抽取自举样本，并对每个自举样本使用所有81种**epsilon**和**minPts**的组合来运行DBSCAN。然后我们可以计算每个性能指标的均值，并查看它们的稳定性。
- en: '|  |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-760
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 17](kindle_split_030.html#ch17) that a bootstrap sample
    is created by sampling cases from the original data, with replacement, to create
    a new sample that’s the same size as the original.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第17章](kindle_split_030.html#ch17)回顾，自举样本是通过从原始数据中抽取案例（有放回）来创建的，以创建一个与原始样本大小相同的新样本。
- en: '|  |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s start by generating 10 bootstrap samples from our `swissScaled` dataset.
    We do this just as we did in [chapter 17](kindle_split_030.html#ch17), using the
    `sample_n()` function and setting the `replace` argument equal to `TRUE`.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的`swissScaled`数据集中生成10个自举样本开始。我们这样做就像在[第17章](kindle_split_030.html#ch17)中做的那样，使用`sample_n()`函数，并将`replace`参数设置为`TRUE`。
- en: Listing 18.11\. Creating bootstrap samples
  id: totrans-764
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE40]'
  id: totrans-765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Before we run our tuning experiment, DBSCAN presents a potential problem when
    calculating internal cluster metrics. As we saw from the discussion about them
    in [chapter 16](kindle_split_029.html#ch16), these metrics work by comparing the
    separation between clusters and the spread within clusters (however they define
    these concepts). Think for a second about the noise cluster, and how it will impact
    these metrics. Because the noise cluster isn’t a distinct cluster occupying one
    region of the feature space but is typically spread out across it, its impact
    on internal cluster metrics can make the metrics uninterpretable and difficult
    to compare. As such, once we have our clustering results, we’re going to remove
    the noise cluster so we can calculate our internal cluster metrics using only
    non-noise clusters.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行调整实验之前，DBSCAN在计算内部聚类度量时可能存在潜在问题。正如我们在[第16章](kindle_split_029.html#ch16)中讨论的那样，这些度量通过比较簇之间的分离和簇内的扩散（无论它们如何定义这些概念）来工作。思考一下噪声簇，以及它将如何影响这些度量。因为噪声簇不是一个占据特征空间一个区域的独立簇，而是通常分布在整个特征空间中，它对内部聚类度量的影响可能会使度量难以解释和比较。因此，一旦我们得到聚类结果，我们将移除噪声簇，这样我们就可以只使用非噪声簇来计算我们的内部聚类度量。
- en: '|  |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-768
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: This doesn’t mean it’s not important to consider the noise cluster when evaluating
    the performance of a DBSCAN model. Two cluster models could theoretically give
    equally good cluster metrics, but one model may place cases in the noise cluster
    that you consider to be important. You should therefore always visually evaluate
    your cluster result (including noise cases), especially when you have domain knowledge
    of your task.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着在评估DBSCAN模型性能时考虑噪声簇不重要。理论上，两个聚类模型可能给出相同的聚类度量，但一个模型可能将案例放置在噪声簇中，而你认为这些案例很重要。因此，你应该始终视觉评估你的簇结果（包括噪声案例），特别是当你对你的任务有领域知识时。
- en: '|  |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the following listing, we run the tuning experiment on our bootstrap samples.
    The code is quite long, so we’ll walk through it step by step.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们在bootstrap样本上运行调整实验。代码相当长，所以我们将一步一步地讲解。
- en: Listing 18.12\. Performing the tuning experiment
  id: totrans-772
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表18.12\. 执行调整实验
- en: '[PRE41]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: First, we use the `map_df()` function, because we want to apply an anonymous
    function to each bootstrap sample and row-bind the results into a tibble. We run
    the DBSCAN algorithm using every combination of *epsilon* and *minPts* in our
    `dbsParamSpace` using `pmap()`, just as we did in [listing 18.5](#ch18ex05).
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`map_df()`函数，因为我们希望将匿名函数应用于每个bootstrap样本，并将结果行绑定到一个tibble中。我们使用`pmap()`函数，通过`dbsParamSpace`中的每个*epsilon*和*minPts*的组合来运行DBSCAN算法，就像我们在[列表18.5](#ch18ex05)中所做的那样。
- en: Now that the cluster results have been generated, the next part of the code
    applies our `cluster_metric()` function to each permutation of *epsilon* and *minPts*.
    Again, we want this to be returned as a tibble, so we use `map_df()` to iterate
    an anonymous function over each element in `clusterResult`.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 现在簇结果已经生成，代码的下一部分将`cluster_metric()`函数应用于每个*epsilon*和*minPts*的排列。同样，我们希望它返回一个tibble，所以我们使用`map_df()`来迭代匿名函数，遍历`clusterResult`中的每个元素。
- en: We start by extracting the cluster membership from each permutation, converting
    it into a tibble (of a single column), and using the `bind_cols()` function to
    stick this column of cluster membership onto the bootstrap sample. We then pipe
    this into the `filter()` function to remove cases that belong to the noise cluster
    (cluster 0). Because the Dunn index requires a distance matrix, we next define
    the distance matrix, `d`, using the filtered data.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从每个排列中提取簇成员资格，将其转换为tibble（单列），并使用`bind_cols()`函数将簇成员资格的这一列粘接到bootstrap样本上。然后，我们将这个结果传递到`filter()`函数中，以移除属于噪声簇（簇0）的案例。因为Dunn指数需要一个距离矩阵，所以我们接下来定义距离矩阵`d`，使用过滤后的数据。
- en: At this point, for a particular permutation of *epsilon* and *minPts* for a
    particular bootstrap sample, we have a tibble containing the scaled variables
    and a column of cluster membership for cases not in the noise clusters. This tibble
    is then passed to our very own `cluster_metrics()` function (removing the `value`
    variable for the first argument and extracting it for the second argument). We
    pass the distance matrix as the `dist_matrix` argument.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，对于特定bootstrap样本中特定*epsilon*和*minPts*的排列，我们有一个包含缩放变量和簇成员资格列的tibble（单列）。然后，我们将这个tibble传递给我们的`cluster_metrics()`函数（对于第一个参数移除`value`变量，并从第二个参数提取它）。我们将距离矩阵作为`dist_matrix`参数传递。
- en: 'Phew! That took quite a bit of concentration. I strongly suggest that you read
    back through the code and make sure each line makes sense to you. Print the `metricsTib`
    tibble. We end up with a tibble of four columns: one for each of our three internal
    cluster metrics, and one containing the number of clusters. Each row contains
    the result of a single DBSCAN model, 810 total (81 permutations of *epsilon* and
    *minPts* and 10 bootstrap samples for each).'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 呼！这需要相当多的专注力。我强烈建议你重新阅读代码，确保每一行对你来说都有意义。打印出 `metricsTib` tibble。我们最终得到一个包含四个列的
    tibble：每个内部聚类指标一个，还有一个包含聚类数量的列。每一行包含单个 DBSCAN 模型的结果，总共 810 个（81 个 *epsilon* 和
    *minPts* 的排列组合，以及每个组合的 10 个 bootstrap 样本）。
- en: Now that we’ve performed our tuning experiment, the easiest way to evaluate
    the results is to plot them.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了调优实验，评估结果的最简单方法就是绘制它们。
- en: Listing 18.13\. Preparing the tuning result for plotting
  id: totrans-780
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.13\. 准备绘图调优结果
- en: '[PRE42]'
  id: totrans-781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We first need to `mutate()` columns indicating which bootstrap a particular
    case used, which *epsilon* value it used, and which *minPts* value it used. Read
    as far as the first line break in [listing 18.13](#ch18ex13) to see this.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要 `mutate()` 指示特定案例使用了哪个 bootstrap、使用了哪个 *epsilon* 值以及使用了哪个 *minPts* 值的列。阅读到
    [列表 18.13](#ch18ex13) 的第一行换行处以查看这一点。
- en: Next, we need to gather the data such that we have a column indicating which
    of our four metrics the row is indicating, so that we can facet by each metric.
    We do this using the `gather()` function before the second line break in [listing
    18.13](#ch18ex13).
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要收集数据，以便有一个列指示行表示我们四个指标中的哪一个，这样我们就可以按每个指标进行分面。我们使用 `gather()` 函数在 [列表
    18.13](#ch18ex13) 的第二次换行之前完成此操作。
- en: At this point, we have a problem. Some of the cluster models contain only a
    single cluster. To return a sensible value, each of our three internal cluster
    metrics requires a minimum of two clusters. When we apply our `cluster_metrics()`
    function to the clustering models, the function will return `NA` for the Davies-Bouldin
    index and pseudo F statistic and `INF` for the Dunn index, for any model containing
    only a single cluster.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们遇到了一个问题。一些聚类模型只包含一个聚类。为了返回一个合理的值，我们的三个内部聚类指标需要至少有两个聚类。当我们将 `cluster_metrics()`
    函数应用于聚类模型时，对于只包含一个聚类的任何模型，函数将返回 Davies-Bouldin 指数和伪 F 统计量的 `NA` 值，以及 Dunn 指数的
    `INF` 值。
- en: '|  |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-786
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Run `map_int(metricsTib, ~sum(is.na(.)))` and `map_int(metricsTib, ~sum(is.infinite(.)))`
    to confirm this for yourself.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `map_int(metricsTib, ~sum(is.na(.)))` 和 `map_int(metricsTib, ~sum(is.infinite(.)))`
    以确认这一点。
- en: '|  |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: So let’s remove `INF` and `NA` values from our tibble. We do this by first turning
    `INF` values into `NA`. We use the `mutate_if()` function to consider only the
    numeric variable (we could also have used `mutate_at(.vars = "value", ...)`),
    and we use the `na_if()` function to convert values to `NA` if they are currently
    `INF`. We then pipe this into `drop_na()` to remove all the `NA` values at once.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从我们的 tibble 中移除 `INF` 和 `NA` 值。我们首先将 `INF` 值转换为 `NA`。我们使用 `mutate_if()`
    函数仅考虑数值变量（我们也可以使用 `mutate_at(.vars = "value", ...)`），并使用 `na_if()` 函数将当前为 `INF`
    的值转换为 `NA`。然后我们将这个操作通过管道传递到 `drop_na()` 以一次性移除所有 `NA` 值。
- en: Finally, to generate mean values for each metric, for each combination of *epsilon*
    and *minPts*, we first `group_by()metric`, `eps`, and `minPts`, and `summarize()`
    both the mean and number of the `value` variable. Because the metrics are on different
    scales, we then `group_by()metric`, `scale()` the `meanValue` variable, and then
    `ungroup()`.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了为每个指标生成平均值，对于每个 *epsilon* 和 *minPts* 的组合，我们首先按 `metric`、`eps` 和 `minPts`
    进行 `group_by()`，然后对 `value` 变量的平均值和数量进行 `summarize()`。由于指标处于不同的尺度上，我们接着按 `metric`
    进行 `group_by()`，对 `meanValue` 变量进行 `scale()`，然后进行 `ungroup()`。
- en: That was some serious dplyring! Again, don’t just gloss over this code. Start
    again from the top and read all of [listing 18.13](#ch18ex13) to be sure you understand
    it. Also be comforted that I didn’t just write this all out the first time; I
    knew what I wanted to achieve, and I worked through the problem line by line.
    At each step, I looked at the output to make sure what I had done was correct
    and to work out what I needed to do next. Print out the `metricsTibSummary` so
    you understand what we end up with.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些严肃的 dplyring！再次提醒，不要只是匆匆浏览这段代码。从头开始，重新阅读 [列表 18.13](#ch18ex13) 以确保你理解它。同时，请放心，我并不是第一次就写出了所有这些；我知道我想要达到的目标，并且我逐行解决了问题。在每一步，我都会查看输出以确保我所做的是正确的，并确定下一步需要做什么。打印出
    `metricsTibSummary` 以了解我们最终得到的结果。
- en: Fantastic. Now that our tuning data is in the correct format, let’s plot it.
    We’re going to create a heatmap where *epsilon* and *minPts* are mapped to the
    x and y aesthetics, and the value of the metric is mapped to the fill of each
    tile in the heatmap. There will be a separate subplot for each metric. Also, because
    we removed rows containing `NA` and `INF` values, some combinations of *epsilon*
    and *minPts* have fewer than 10 bootstrap samples. To help guide our choice of
    hyperparameters, we’re going to map the number of samples for each combination
    to the alpha aesthetic (transparency), because we may have less confidence in
    a combination of hyperparameters that has fewer bootstrap samples. We do this
    all in the following listing.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了。现在我们的调整数据已经以正确的格式，让我们来绘制它。我们将创建一个热图，其中 *epsilon* 和 *minPts* 被映射到 x 和 y 视觉效果，指标值被映射到热图中每个瓷砖的填充。每个指标将有一个单独的子图。此外，因为我们移除了包含
    `NA` 和 `INF` 值的行，所以 *epsilon* 和 *minPts* 的某些组合的自举样本数量少于 10。为了帮助我们选择超参数，我们将每个组合的样本数量映射到
    alpha 视觉效果（透明度），因为我们可能对具有较少自举样本的超参数组合的信心较低。我们将在下面的列表中完成所有这些操作。
- en: Listing 18.14\. Plotting the results of the tuning experiment
  id: totrans-793
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 18.14\. 调整实验结果的绘图
- en: '[PRE43]'
  id: totrans-794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Aside from mapping the `num` variable to the alpha aesthetic, the only new thing
    here is `geom_tile()`, which will create rectangular tiles for each combination
    of the x and y variables. Setting `col = "black"` simply draws a black border
    around each individual tile. To prevent major gridlines being drawn, we add the
    layer `theme(panel.grid.major = element_blank())`.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将 `num` 变量映射到 alpha 视觉效果之外，这里唯一的新事物是 `geom_tile()`，它将为 x 和 y 变量的每个组合创建矩形瓷砖。将
    `col = "black"` 设置为简单地围绕每个单独的瓷砖绘制黑色边框。为了防止绘制主要网格线，我们添加了层 `theme(panel.grid.major
    = element_blank())`。
- en: 'The resulting plot is shown in [figure 18.11](#ch18fig11). We have four subplots:
    one for each of our three internal cluster metrics, and one for the number of
    clusters. A hole at the top right in each internal metric’s subplot shows where
    this area of the hyperparameter tuning space resulted in only a single cluster
    (and we removed these values). Surrounding the hole are tiles that are semitransparent,
    because some of the bootstrap samples for these combinations of *epsilon* and
    *minPts* resulted in only a single cluster and so were removed.'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在[图 18.11](#ch18fig11)。我们有四个子图：一个用于我们三个内部聚类指标中的每一个，一个用于聚类数量。每个内部指标子图的右上角有一个孔，显示了超参数调整空间这个区域只产生了一个聚类（我们移除了这些值）。围绕孔的瓷砖是半透明的，因为这些组合的
    *epsilon* 和 *minPts* 的某些自举样本只产生了一个聚类，因此被移除。
- en: Figure 18.11\. Visualizing the cluster performance experiment. Each subplot
    shows a heatmap for the number of clusters, Davies-Bouldin index (`db`), Dunn
    index (`dunn`), and pseudo F statistic (G1) returned by the cluster models. Each
    tile represents the combination of *epsilon* and *minPts*, and the depth of shading
    of the tile indicates its value for each metric. The blank region at the top right
    in the metric plots indicates a region with no data, and semitransparent tiles
    indicate fewer than 10 samples.
  id: totrans-797
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 18.11\. 可视化聚类性能实验。每个子图显示了聚类模型返回的聚类数量、Davies-Bouldin 指数 (`db`)、Dunn 指数 (`dunn`)
    和伪 F 统计量 (G1) 的热图。每个瓷砖代表 *epsilon* 和 *minPts* 的组合，瓷砖的阴影深度表示其每个指标的价值。指标图右上角的空白区域表示没有数据，半透明瓷砖表示样本数量少于
    10。
- en: '![](fig18-11_alt.jpg)'
  id: totrans-798
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig18-11_alt.jpg)'
- en: '|  |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-800
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Your plot looking a little different than mine? This is because of the random
    sampling we used to create the bootstrap samples. A similar pattern should be
    present, however.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 你的图表看起来和我有点不同？这是因为我们用来创建自举样本的随机抽样。然而，应该存在一个类似的模式。
- en: '|  |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s use this plot to guide our final choice of *epsilon* and *minPts*. It
    isn’t necessarily straightforward, because there is no single, obvious combination
    that all three internal metrics agree on. First, let’s avoid combinations in or
    around the hole in the plot—I think that’s a pretty clear starting point. Next,
    let’s remind ourselves that, in theory, the best clustering model will be the
    one with the lowest Davies-Bouldin index and the largest Dunn index and pseudo
    F statistic. So we’re looking for a combination that best satisfies those criteria.
    With this in mind, before reading on, look at the plots and try to decide which
    combination you would choose.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个图表来指导我们最终选择 *epsilon* 和 *minPts*。这并不一定简单，因为没有单一、明显的组合是所有三个内部指标都同意的。首先，让我们避免图表中的洞附近或其中的组合——我认为这是一个很明确的起点。接下来，让我们提醒自己，从理论上讲，最佳的聚类模型将是具有最低的Davies-Bouldin指数、最大的Dunn指数和伪F统计量的模型。因此，我们正在寻找一个最能满足这些标准的组合。考虑到这一点，在继续阅读之前，请查看图表并尝试决定你会选择哪个组合。
- en: 'I think I would choose an *epsilon* of 1.2 and a *minPts* of 9\. Can you see
    that with this combination of values (the top left in each subplot), the Dunn
    and pseudo F statistic are near their highest, and the Davies-Bouldin index is
    at its lowest? Let’s find out which row of our `dbsParamSpace` tibble corresponds
    to this combination of values:'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我会选择 *epsilon* 为1.2，*minPts* 为9。你能看到，在这个值组合（每个子图的左上角）中，Dunn和伪F统计量接近最高，而Davies-Bouldin指数处于最低。让我们找出
    `dbsParamSpace` tibble中哪个行对应于这个值组合：
- en: '[PRE44]'
  id: totrans-805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Next, let’s use `ggpairs()` to plot the final clustering. Because we calculated
    the internal cluster metrics, not considering the noise cluster, we’ll plot the
    result with and without noise cases. This will allow us to visually confirm whether
    the assignment of cases as noise is sensible.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 `ggpairs()` 绘制最终的聚类。因为我们计算了内部聚类度量，没有考虑噪声聚类，所以我们将结果绘制为有噪声和无噪声案例的图表。这将使我们能够直观地确认将案例分配为噪声是否合理。
- en: Listing 18.15\. Plotting the final clustering with outliers
  id: totrans-807
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表18.15\. 绘制带异常值的最终聚类
- en: '[PRE45]'
  id: totrans-808
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We first filter our `swissClustersGathered` tibble to include only rows belonging
    to permutation 73 (these are the cases clustered using our chosen combination
    of *epsilon* and *minPts*). Next, we remove the column indicating the permutation
    number and convert the column of cluster membership into a factor. We then use
    the `ggpairs()` function to create the plot, mapping cluster membership to the
    color aesthetic.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先过滤 `swissClustersGathered` tibble，只包含属于排列73的行（这些是使用我们选择的 *epsilon* 和 *minPts*
    组合进行聚类的案例）。接下来，我们删除表示排列编号的列，并将聚类成员资格的列转换为因子。然后，我们使用 `ggpairs()` 函数创建图表，将聚类成员资格映射到颜色美学。
- en: Figure 18.12\. Plotting our final DBSCAN cluster model with `ggpairs()`. This
    plot includes the noise cluster.
  id: totrans-810
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.12\. 使用 `ggpairs()` 绘制我们的最终DBSCAN聚类模型。此图包括噪声聚类。
- en: '![](fig18-12_alt.jpg)'
  id: totrans-811
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig18-12_alt.jpg)'
- en: 'The resulting plot is shown in [figure 18.12](#ch18fig12). The model appears
    to have done a pretty good job of capturing the two obvious clusters in the dataset.
    Quite a lot of cases have been classified as noise. Whether this is reasonable
    will depend on your goal and how stringent you want to be. If it’s important to
    you that fewer cases are placed in the noise cluster, you may want to choose a
    different combination of *epsilon* and *minPts*. This is why relying on metrics
    alone isn’t good enough: expert/domain knowledge should always be considered where
    it is available.'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在[图18.12](#ch18fig12)中。模型似乎很好地捕捉到了数据集中的两个明显聚类。相当多的案例被分类为噪声。这是否合理将取决于你的目标和你想有多严格。如果你认为将更少的案例放在噪声聚类中很重要，你可能想选择不同的
    *epsilon* 和 *minPts* 组合。这就是为什么仅仅依赖指标是不够好的：在可用的情况下，应始终考虑专家/领域知识。
- en: Now let’s do the same thing, but without plotting outliers. All we change here
    is to add `Cluster != 0` in the `filter()` call.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们做同样的事情，但不绘制异常值。这里我们做的唯一改变是在 `filter()` 调用中添加 `Cluster != 0`。
- en: Listing 18.16\. Plotting the final clustering without outliers
  id: totrans-814
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表18.16\. 绘制不带异常值的最终聚类
- en: '[PRE46]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The resulting plot is shown in [figure 18.13](#ch18fig13). Looking at this plot,
    we can see that the two clusters our DBSCAN model identified are quite neat and
    well separated.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在[图18.13](#ch18fig13)中。查看这个图表，我们可以看到DBSCAN模型识别的两个聚类非常整洁且分离良好。
- en: Figure 18.13\. Plotting our final DBSCAN cluster model with `ggpairs()`. This
    plot excludes the noise cluster.
  id: totrans-817
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.13\. 使用 `ggpairs()` 绘制我们的最终DBSCAN聚类模型。此图排除了噪声聚类。
- en: '![](fig18-13_alt.jpg)'
  id: totrans-818
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig18-13_alt.jpg)'
- en: '|  |'
  id: totrans-819
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-820
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Make sure you always look at your outliers. It’s possible for DBSCAN to make
    clusters look more important than they are when outliers are removed.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你总是查看你的异常值。当移除异常值时，DBSCAN可能会使聚类看起来比实际更重要。
- en: '|  |'
  id: totrans-822
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Our clustering model seems pretty reasonable, but how stable is it? The final
    thing we’re going to do to evaluate the performance of our DBSCAN model is calculate
    the Jaccard index across multiple bootstrap samples. Recall from [chapter 17](kindle_split_030.html#ch17)
    that the Jaccard index quantifies the agreement of cluster membership between
    clustering models trained on different bootstrap samples.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聚类模型看起来相当合理，但它有多稳定？为了评估我们的DBSCAN模型的表现，我们将要做的最后一件事是计算多个自助样本中的Jaccard指数。回想一下第17章（kindle_split_030.html#ch17），Jaccard指数量化了在不同自助样本上训练的聚类模型之间聚类成员资格的一致性。
- en: 'To do this, we first need to load the fpc package. Then we use the `clusterboot()`
    function the same way we did in [chapter 17](kindle_split_030.html#ch17). The
    first argument is the data we’re going to be clustering (our scaled tibble), `B`
    is the number of bootstraps (more is better, depending on your computational budget),
    and `clustermethod = dbscanCBI` tells the function to use the DBSCAN algorithm.
    We then set the desired values of *epsilon* and `MinPts` (careful: note the capital
    *M* this time), and set `showplots = FALSE` to avoid drawing 500 plots.'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先需要加载fpc包。然后我们使用`clusterboot()`函数，就像我们在第17章中做的那样。第一个参数是我们将要聚类的数据（我们的缩放tibble），`B`是自助样本的数量（越多越好，取决于您的计算预算），`clustermethod
    = dbscanCBI`告诉函数使用DBSCAN算法。然后我们设置*epsilon*和`MinPts`（注意：这次是大写的*M*），并将`showplots
    = FALSE`设置为避免绘制500个图表。
- en: '|  |'
  id: totrans-825
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-826
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: I’ve truncated the output to show the most important information.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 我已截断输出以显示最重要的信息。
- en: '|  |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 18.17\. Calculating the Jaccard index across bootstrap samples
  id: totrans-829
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表18.17. 在自助样本中计算Jaccard指数
- en: '[PRE47]'
  id: totrans-830
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can see the Jaccard indices for the three clusters (where cluster 3 is,
    confusingly, the noise cluster). Cluster 2 has quite a high stability: 80.7% of
    cases in the original cluster 2 are in agreement across the bootstrap samples.
    Clusters 1 and 3 are less stable, with ~68% agreement.'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到三个聚类（其中聚类3令人困惑地是噪声聚类）的Jaccard指数。聚类2的稳定性相当高：原始聚类2中有80.7%的案例在自助样本中达成一致。聚类1和3的稳定性较低，大约有68%的一致性。
- en: 'We’ve now appraised the performance of our DBSCAN model in three ways: using
    internal cluster metrics, examining the clusters visually, and using the Jaccard
    index to evaluate their stability. For any particular clustering problem, you
    will need to evaluate all this evidence together to make a decision as to whether
    your cluster model is appropriate for the task at hand.'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经以三种方式评估了我们的DBSCAN模型的表现：使用内部聚类指标，检查聚类的外观，以及使用Jaccard指数来评估它们的稳定性。对于任何特定的聚类问题，您需要将所有这些证据综合起来，以决定您的聚类模型是否适合当前的任务。
- en: '|  |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习3**'
- en: Use `dbscan()` to cluster our `swissScaled` dataset, keeping *epsilon* as 1.2
    but setting *minPts* to 1\. How many cases are in the noise cluster? Why? The
    `fpc` package also has a `dbscan()` function, so use `dbscan::dbscan()` to use
    the function from the dbscan package.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dbscan()`对`swissScaled`数据集进行聚类，将*epsilon*设置为1.2，但将*minPts*设置为1。噪声聚类中有多少案例？为什么？`fpc`包也有一个`dbscan()`函数，所以使用`dbscan::dbscan()`来使用dbscan包中的函数。
- en: '|  |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 18.3\. Building your first OPTICS model
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.3. 构建第一个OPTICS模型
- en: In this section, I’m going to show you how we can use the OPTICS algorithm to
    create an ordering of cases in a dataset and how we can extract clusters from
    this ordering. We will directly compare the results we get using OPTICS to those
    we generated using DBSCAN.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示我们如何使用OPTICS算法对数据集中的案例进行排序，以及我们如何从这个排序中提取聚类。我们将直接比较使用OPTICS得到的结果和使用DBSCAN得到的结果。
- en: To do this, we’re going to use the `optics()` function from the dbscan package.
    The first argument is the dataset; just like DBSCAN, OPTICS is sensitive to the
    variable scale, so we’re using our scaled tibble.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们将使用来自dbscan包的`optics()`函数。第一个参数是数据集；与DBSCAN一样，OPTICS对变量尺度很敏感，所以我们使用我们的缩放tibble。
- en: Listing 18.18\. Ordering cases with OPTICS and extracting clusters
  id: totrans-840
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表18.18. 使用OPTICS对案例进行排序并提取聚类
- en: '[PRE48]'
  id: totrans-841
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Just like the `dbscan()` function, `optics()` has the `eps` and *minPts* arguments.
    Because *epsilon* is an optional argument for the OPTICS algorithm and only serves
    to speed up computation, we’ll leave it as the default of `NULL`, which means
    there is no maximum *epsilon*. We set *minPts* equal to 9 to match what we used
    in our final DBSCAN model.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`dbscan()`函数一样，`optics()`有`eps`和*minPts*参数。因为*epsilon*是OPTICS算法的可选参数，并且仅用于加速计算，我们将它保留为默认的`NULL`，这意味着没有最大*epsilon*。我们将*minPts*设置为9，以匹配我们最终DBSCAN模型中使用的值。
- en: Once we’ve created our ordering, we can inspect the reachability plot by simply
    calling `plot()` on the output from the `optics()` function; see [figure 18.14](#ch18fig14).
    Notice that we have two obvious troughs separated by high peaks. Remember that
    this indicates regions of high density separated by regions of low density in
    the feature space.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了我们的顺序，我们可以通过在`optics()`函数的输出上调用`plot()`来检查可达性图；参见[图18.14](#ch18fig14)。注意，我们有两个明显分离的低谷，由高峰值隔开。记住，这表明在特征空间中由低密度区域分隔的高密度区域。
- en: Figure 18.14\. The reachability plot generated from applying the OPTICS algorithm
    to our data. The x-axis shows the processing order of the cases, and the y-axis
    shows the reachability distance for each case. We can see two main troughs in
    the plot, bordered by peaks of higher reachability distance.
  id: totrans-844
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图18.14。从应用OPTICS算法到我们的数据生成的可达性图。x轴显示案例的处理顺序，y轴显示每个案例的可达距离。我们可以看到图中有两个主要低谷，由更高的可达距离峰值所包围。
- en: '![](fig18-14_alt.jpg)'
  id: totrans-845
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-14_alt.jpg)'
- en: 'Now let’s extract clusters from this ordering using the steepness method. To
    do so, we use the `extractXi()` function, passing the output from the `optics()`
    function as the first argument, and specifying the argument *xi*:'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用陡度法从这个顺序中提取簇。为此，我们使用`extractXi()`函数，将`optics()`函数的输出作为第一个参数传递，并指定*xi*参数：
- en: '[PRE49]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Recall that *xi* (ξ) is a hyperparameter that determines the minimum steepness
    (1 – ξ) needed to start and end clusters in the reachability plot. How do we choose
    the value of ξ? Well, in this example, I’ve simply chosen a value of ξ that gives
    a reasonable clustering result (as you’ll see in a moment). As we know, this isn’t
    a very scientific or objective approach; for your own work, you should tune ξ
    as a hyperparameter, just as we did for *epsilon* and *minPts* for DBSCAN.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，*xi*（ξ）是一个超参数，它决定了在可达性图中开始和结束簇所需的最小陡度（1 – ξ）。我们如何选择ξ的值？嗯，在这个例子中，我简单地选择了一个能给出合理聚类结果的ξ值（你很快就会看到）。正如我们所知，这不是一个非常科学或客观的方法；对于你自己的工作，你应该像我们对*epsilon*和*minPts*进行DBSCAN调整一样，将ξ作为超参数进行调整。
- en: '|  |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-850
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The ξ hyperparameter is bounded between 0 and 1, so this gives you a fixed space
    to search within.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: ξ超参数介于0和1之间，因此这为您在搜索空间内提供了一个固定的范围。
- en: '|  |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s plot the clustering result so we can compare it with our DBSCAN model.
    We mutate a new column in our dataset, containing the clusters we extracted using
    the steepness method. We then pipe this data into the `ggpairs()` function.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制聚类结果，以便我们可以将其与我们的DBSCAN模型进行比较。我们在数据集中添加一个新列，包含我们使用陡度法提取的簇。然后我们将这些数据通过管道传递到`ggpairs()`函数。
- en: Listing 18.19\. Plotting the OPTICS clusters
  id: totrans-854
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表18.19。绘制OPTICS簇
- en: '[PRE50]'
  id: totrans-855
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '|  |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-857
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because we have only a single noise case, this causes the computation of the
    density plots to fail. Therefore, we set the upper panels to simply display `"points"`
    instead of density.
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只有一个噪声案例，这导致密度图的计算失败。因此，我们将上部分设置为简单地显示`"points"`而不是密度。
- en: '|  |'
  id: totrans-859
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The resulting plot is shown in [figure 18.15](#ch18fig15). Our OPTICS clustering
    has mostly identified the same two clusters as DBSCAN but has identified an additional
    cluster that seems to be distributed across the feature space. This additional
    cluster doesn’t look convincing to me (but we could calculate internal cluster
    metrics and cluster stability to reinforce this conclusion). To improve the clustering,
    we should tune the *minPts* and ξ hyperparameters, though we won’t do this here.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图18.15](#ch18fig15)中。我们的OPTICS聚类主要识别出与DBSCAN相同的两个簇，但还识别出一个似乎分布在特征空间中的额外簇。这个额外的簇对我来说并不令人信服（但我们可以计算簇内指标和簇稳定性来加强这个结论）。为了改进聚类，我们应该调整*minPts*和ξ超参数，尽管我们在这里不会这样做。
- en: You’ve learned how to use the DBSCAN and OPTICS algorithms to cluster your data.
    In the next chapter, I’ll introduce you to *mixture model clustering*, a clustering
    technique that fits a set of models to the data and assigns cases to the most
    probable model. I suggest that you save your .R file, because we’re going to continue
    using the same dataset in the next chapter. This is so we can compare the performance
    of our DBSCAN and OPTICS models to the output of our mixture model.
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了如何使用 DBSCAN 和 OPTICS 算法来聚类你的数据。在下一章中，我将向你介绍 *混合模型聚类*，这是一种聚类技术，它将一组模型拟合到数据中，并将案例分配给最可能的模型。我建议你保存你的
    .R 文件，因为我们将继续在下一章中使用同一个数据集。这样我们可以比较我们的 DBSCAN 和 OPTICS 模型的性能与我们的混合模型输出的结果。
- en: Figure 18.15\. Plotting our final OPTICS cluster model with `ggpairs()`
  id: totrans-862
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 18.15\. 使用 `ggpairs()` 绘制我们的最终 OPTICS 聚类模型
- en: '![](fig18-15_alt.jpg)'
  id: totrans-863
  prefs: []
  type: TYPE_IMG
  zh: '![](fig18-15_alt.jpg)'
- en: 18.4\. Strengths and weaknesses of density-based clustering
  id: totrans-864
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.4\. 基于密度的聚类的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    density-based clustering will perform well for you.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪些算法会对给定的任务表现良好，但以下是一些优势和劣势，这将帮助你决定基于密度的聚类是否适合你。
- en: 'The strengths of density-based clustering are as follows:'
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类的优点如下：
- en: It can identify non-spherical clusters of different diameters.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以识别不同直径的非球形聚类。
- en: It is able to natively identify outlying cases.
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能够原生地识别异常案例。
- en: It can identify clusters of complex, non-spherical shapes.
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以识别复杂、非球形的聚类。
- en: OPTICS is able to learn a hierarchical clustering structure and doesn’t require
    the tuning of *epsilon*.
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OPTICS 能够学习层次聚类结构，并且不需要调整 *epsilon*。
- en: OPTICS is able to find clusters of differing density.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OPTICS 能够找到不同密度的聚类。
- en: OPTICS can be sped up by setting a sensible *epsilon* value.
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置合理的 *epsilon* 值可以加快 OPTICS 的速度。
- en: 'The weaknesses of density-based clustering are these:'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类的缺点如下：
- en: It cannot natively handle categorical variables.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不能原生地处理分类变量。
- en: The algorithms cannot select the optimal number of clusters automatically.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些算法不能自动选择最佳聚类数量。
- en: It is sensitive to data on different scales.
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对数据的不同尺度敏感。
- en: DBSCAN is biased toward finding clusters of equal density.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN 倾向于寻找密度相等的聚类。
- en: '|  |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 4**'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习题 4**'
- en: Use `dbscan()` to cluster our unscaled `swissTib` dataset, keeping *epsilon*
    at 1.2 and *minPts* at 9\. Are the clusters the same? Why?
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `dbscan()` 对未缩放的 `swissTib` 数据集进行聚类，将 *epsilon* 设置为 1.2，将 *minPts* 设置为 9。这些聚类是否相同？为什么？
- en: '|  |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 5**'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习题 5**'
- en: Train extract clusters from our `swissOptics` object, using the *xi* values
    0.035, 0.05, and 0.065\. Use `plot()` to see how these different values change
    the clusters extracted from the reachability plot.
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的 `swissOptics` 对象中提取聚类，使用 *xi* 值 0.035、0.05 和 0.065。使用 `plot()` 来查看这些不同的值如何改变从可达性图中提取的聚类。
- en: '|  |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-886
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Density-based clustering algorithms like DBSCAN and OPTICS find clusters by
    searching for high-density regions separated by low-density regions of the feature
    space.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于密度的聚类算法，如 DBSCAN 和 OPTICS，通过在特征空间中搜索由低密度区域分隔的高密度区域来找到聚类。
- en: DBSCAN has two hyperparameters, *epsilon* and *minPts*, where *epsilon* is the
    search radius around each case. If the case has *minPts* cases inside its *epsilon*,
    that case is a core point.
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN 有两个超参数，*epsilon* 和 *minPts*，其中 *epsilon* 是每个案例周围的搜索半径。如果一个案例在其 *epsilon*
    范围内有 *minPts* 个案例，那么这个案例就是一个核心点。
- en: DBSCAN recursively scans *epsilon* of all cases density-connected to the starting
    case in any cluster, categorizing cases as either core points or border points.
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN 递归地扫描任何聚类中从起始案例开始的与所有案例密度连接的 *epsilon*，将案例分类为核心点或边界点。
- en: DBSCAN and OPTICS create a noise cluster for cases that lie too far away from
    high-density regions.
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN 和 OPTICS 为那些离高密度区域太远的案例创建一个噪声聚类。
- en: OPTICS creates an ordering of the cases from which clusters can be extracted.
    This ordering can be visualized as a reachability plot where troughs separated
    by peaks indicate clusters.
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OPTICS 通过创建一个案例的顺序来提取聚类，这个顺序可以可视化为可达性图，其中峰谷之间的低谷表示聚类。
- en: Solutions to exercises
  id: totrans-892
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题的解答
- en: 'Print the result of using `expand.grid()`, and inspect the result to understand
    what the function does:'
  id: totrans-893
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 `expand.grid()` 的结果，并检查结果以了解该函数的功能：
- en: '[PRE51]'
  id: totrans-894
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Plot the tuning experiment to visualize the number and size of the clusters
    from each permutation:'
  id: totrans-895
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制调整实验图，以可视化每个排列的簇的数量和大小：
- en: '[PRE52]'
  id: totrans-896
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Use `dbscan()` with an *epsilon* of 1.2 and a *minPts* of 1:'
  id: totrans-897
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dbscan()`，设置*epsilon*为1.2和*minPts*为1：
- en: '[PRE53]'
  id: totrans-898
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Use `dbscan()` to cluster our unscaled data:'
  id: totrans-899
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dbscan()`对未缩放的数据进行聚类：
- en: '[PRE54]'
  id: totrans-900
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Extract different clusters from `swissOptics` using different values of *xi*:'
  id: totrans-901
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的*xi*值从`swissOptics`中提取不同的簇：
- en: '[PRE55]'
  id: totrans-902
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Chapter 19\. Clustering based on distributions with mixture modeling
  id: totrans-903
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第19章。基于混合建模的分布聚类
- en: '*This chapter covers*'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding mixture model clustering
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解混合模型聚类
- en: Understanding the difference between hard and soft clustering
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解硬聚类和软聚类的区别
- en: 'Our final stop in unsupervised learning techniques brings us to an additional
    approach to finding clusters in data: *mixture model clustering*. Just like the
    other clustering algorithms we’ve covered, mixture model clustering aims to partition
    a dataset into a finite set of clusters.'
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习技术的最后，我们来到了寻找数据中簇的另一种额外方法：*混合模型聚类*。就像我们之前讨论的其他聚类算法一样，混合模型聚类旨在将数据集划分为有限个簇。
- en: In [chapter 18](kindle_split_031.html#ch18), I showed you the DBSCAN and OPTICS
    algorithms, and how they find clusters by learning regions of high and low density
    in the feature space. Mixture model clustering takes yet another approach to identify
    clusters. A *mixture model* is any model that describes a dataset by combining
    a mix of two or more probability distributions. In the context of clustering,
    mixture models help us to identify clusters by fitting a finite number of probability
    distributions to the data and iteratively modifying the parameters of those distributions
    until they best fit the underlying data. Cases are then assigned to the cluster
    of the distribution under which they are most likely. The most common form of
    mixture modeling is *Gaussian mixture modeling*, which fits Gaussian (or normal)
    distributions to the data.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第18章](kindle_split_031.html#ch18)中，我向你展示了DBSCAN和OPTICS算法，以及它们如何通过学习特征空间中高密度和低密度的区域来找到簇。混合模型聚类采用另一种方法来识别簇。*混合模型*是任何通过结合两个或更多概率分布来描述数据集的模型。在聚类的上下文中，混合模型通过将有限数量的概率分布拟合到数据，并迭代修改这些分布的参数，直到它们最好地拟合底层数据来帮助我们识别簇。然后，案例被分配到它们最有可能属于的分布的簇中。混合建模最常见的形式是*高斯混合建模*，它将高斯（或正态）分布拟合到数据。
- en: By the end of this chapter, I hope you’ll have a firm understanding of how mixture
    model clustering works and its differences and similarities when compared to some
    of the algorithms we’ve already covered. We’ll apply this method to our Swiss
    banknote data from [chapter 18](kindle_split_031.html#ch18) to help you understand
    how mixture model clustering differs from density-based clustering. If you no
    longer have the `swissTib` object defined in your global environment, just rerun
    [listing 18.1](kindle_split_031.html#ch18ex01).
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望你能对混合模型聚类的工作原理及其与我们已经讨论的一些算法的异同有一个牢固的理解。我们将应用这种方法来处理第18章中的瑞士纸币数据（[第18章](kindle_split_031.html#ch18)），以帮助你理解混合模型聚类与基于密度的聚类有何不同。如果你在你的全局环境中不再定义`swissTib`对象，只需重新运行[列表18.1](kindle_split_031.html#ch18ex01)。
- en: 19.1\. What is mixture model clustering?
  id: totrans-910
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 19.1. 什么是混合模型聚类？
- en: 'In this section, I’m going to show you what mixture model clustering is and
    how it uses an algorithm called *expectation-maximization* to iteratively improve
    the fit of the clustering model. The clustering algorithms we’ve met so far are
    all considered *hard* clustering methods, because each case is assigned wholly
    to one cluster and not to another. One of the strengths of mixture model clustering
    is that it is a *soft* clustering method: it fits a set of probabilistic models
    to the data and assigns each case a probability of belonging to each model. This
    allows us to quantify the probability of each case belonging to each cluster.
    Thus we can say, “This case has a 90% probability of belonging to cluster A, a
    9% probability of belonging to cluster B, and a 1% probability of belonging to
    cluster C.” This is useful because it gives us the information we need to make
    better decisions. Say, for example, a case has a 51% probability of belonging
    to one cluster and 49% probability of belonging to the other, how happy are we
    to include this case in its most probable cluster? Perhaps we’re not confident
    enough to include such cases in our final clustering model.'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示混合模型聚类是什么，以及它是如何使用一个称为*期望最大化*的算法来迭代地改进聚类模型的拟合。我们之前遇到的聚类算法都被认为是*硬聚类*方法，因为每个案例完全分配给一个簇，而不是另一个簇。混合模型聚类的优势之一是它是一种*软聚类*方法：它将一组概率模型拟合到数据中，并为每个案例分配属于每个模型的概率。这使得我们可以量化每个案例属于每个簇的概率。因此，我们可以这样说：“这个案例有90%的概率属于簇A，有9%的概率属于簇B，有1%的概率属于簇C。”这很有用，因为它为我们提供了做出更好决策所需的信息。比如说，一个案例有51%的概率属于一个簇，有49%的概率属于另一个簇，我们有多高兴将其包含在其最可能的簇中？也许我们并不足够自信将这些案例包含在我们的最终聚类模型中。
- en: '|  |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-913
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Mixture model clustering doesn’t, in and of itself, identify outlying cases
    like DBSCAN and OPTICS do, but we can manually set a cut-off of probability if
    we like. For example, we could say that any case with less than a 60% probability
    of belonging to its most probable cluster should be considered an outlier.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型聚类本身并不像DBSCAN和OPTICS那样识别异常案例，但如果我们愿意，我们可以手动设置一个概率截止值。例如，我们可以说，任何属于其最可能簇的概率低于60%的案例应被视为异常。
- en: '|  |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: So mixture model clustering fits a set of probabilistic models to the data.
    These models can be a variety of probability distributions but are most commonly
    Gaussian distributions. This clustering approach is called mixture modeling because
    we fit multiple (a mixture of) probability distributions to the data. Therefore,
    a Gaussian mixture model is simply a model that fits multiple Gaussian distributions
    to a set of data.
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，混合模型聚类将一组概率模型拟合到数据中。这些模型可以是各种概率分布，但最常见的是高斯分布。这种聚类方法被称为混合模型，因为我们拟合多个（混合的）概率分布到数据中。因此，高斯混合模型简单地说就是拟合多个高斯分布到一组数据中的模型。
- en: Each Gaussian in the mixture represents a potential cluster. Once our mixture
    of Gaussians fits the data as well as possible, we can calculate the probability
    of each case belonging to each cluster and assign cases to the most probable cluster.
    But how do we find a mixture of Gaussians that fits the underlying data well?
    We can use an algorithm called expectation-maximization (EM).
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型中的每个高斯分布代表一个潜在的簇。一旦我们的高斯混合模型尽可能好地拟合了数据，我们就可以计算每个案例属于每个簇的概率，并将案例分配给最可能的簇。但我们是怎样找到拟合底层数据的高斯混合模型呢？我们可以使用一个称为期望最大化（EM）的算法。
- en: 19.1.1\. Calculating probabilities with the EM algorithm
  id: totrans-918
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 19.1.1. 使用EM算法计算概率
- en: In this section, I’ll take you through some prerequisite knowledge you’ll need
    to know in order to understand the EM algorithm. This focuses on how the algorithm
    calculates the probability that each case comes from each Gaussian.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您介绍您需要了解的一些先验知识，以便理解EM算法。这主要关注算法如何计算每个案例来自每个高斯分布的概率。
- en: 'Imagine that we have a one-dimensional dataset: a number line with cases distributed
    across it (see the top panel of [figure 19.1](#ch19fig01)). First, we must predefine
    the number of clusters to look for in the data; this sets the number of Gaussians
    we will be fitting. In this example, let’s say we believe two clusters exist in
    the dataset.'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们有一个一维数据集：一个分布有案例的数线（参见[图19.1](#ch19fig01)的上部分）。首先，我们必须预先定义在数据中要寻找的簇的数量；这设置了我们将要拟合的高斯分布的数量。在这个例子中，让我们假设我们相信数据集中存在两个簇。
- en: Figure 19.1\. The expectation-maximization algorithm for two, one-dimensional
    Gaussians. Dots represent cases along a number line. Two Gaussians are randomly
    initialized along the line. In the expectation step, the posterior probability
    of each case for each Gaussian is calculated (indicated by shading). In the maximization
    step, the means, variances, and priors for each Gaussian are updated based on
    the calculated posteriors. The process continues until the likelihood converges.
  id: totrans-921
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图19.1.两个一维高斯分布的期望最大化算法。点代表数轴上的案例。沿线随机初始化两个高斯分布。在期望步骤中，计算每个案例对于每个高斯分布的后验概率（用阴影表示）。在最大化步骤中，根据计算的后验概率更新每个高斯分布的均值、方差和先验概率。这个过程一直持续到似然收敛。
- en: '![](fig19-1_alt.jpg)'
  id: totrans-922
  prefs: []
  type: TYPE_IMG
  zh: '![fig19-1_alt.jpg](fig19-1_alt.jpg)'
- en: '|  |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-924
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: This is one of the ways in which mixture model clustering is similar to k-means.
    I’ll show you the other way in which they are similar later in the chapter.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 这是混合模型聚类与k-means相似的一种方式。我将在本章后面展示它们相似的其他方式。
- en: '|  |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'A one-dimensional Gaussian distribution needs two parameters to define it:
    the mean and the variance. So we randomly initialize two Gaussians along the number
    line by selecting random values for their means and variances. Let’s call these
    Gaussians *j* and *k*. Then, given these two Gaussians, we calculate the probability
    that each case belongs to one cluster versus the other. To do this, we can use
    our good friend, Bayes’ rule.'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 一维高斯分布需要两个参数来定义它：均值和方差。因此，我们通过选择它们的均值和方差的随机值，在数轴上随机初始化两个高斯分布。让我们称这些高斯分布为 *j*
    和 *k*。然后，给定这两个高斯分布，我们计算每个案例属于一个簇而不是另一个簇的概率。为此，我们可以使用我们的好朋友，贝叶斯定理。
- en: Recall from [chapter 6](kindle_split_016.html#ch06) that we can use Bayes’ rule
    to calculate the posterior probability of an event (*p*(*k*|*x*)) given the likelihood
    (*p*(*x*|*k*)), prior (*p*(*k*)), and evidence (*p*(*x*)).
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第6章](kindle_split_016.html#ch06)，我们可以使用贝叶斯定理来计算给定似然(*p*(*x*|*k*))、先验(*p*(*k*))和证据(*p*(*x*))的事件的后验概率(*p*(*k*|*x*))。
- en: equation 19.1\.
  id: totrans-929
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[方程19.1](#)'
- en: '![](eq19-1.jpg)'
  id: totrans-930
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-1.jpg](eq19-1.jpg)'
- en: In this case, *p*(*k*|*x*) is the probability of case *x* belonging to Gaussian
    *k*; *p*(*x*|*k*) is the probability of observing case *x* if you were to sample
    from Gaussian *k*; *p*(*k*) is the probability of a randomly selected case belonging
    to Gaussian *k*; and *p*(*x*) is the probability of drawing case *x* if you were
    to sample from the entire mixture model as a whole. The evidence, *p*(*x*), is
    therefore the probability of drawing case *x* from either Gaussian.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*p*(*k*|*x*)是案例 *x* 属于高斯分布 *k* 的概率；*p*(*x*|*k*)是在从高斯分布 *k* 中采样时观察到案例
    *x* 的概率；*p*(*k*)是随机选择的案例属于高斯分布 *k* 的概率；*p*(*x*)是从整个混合模型中采样时抽取案例 *x* 的概率。因此，证据*p*(*x*)是从任一高斯分布中抽取案例
    *x* 的概率。
- en: When computing the probability of one event or the other occurring, we simply
    add together the probabilities of each event occurring independently. Therefore,
    the probability of drawing case *x* from Gaussian *j* or *k* is the probability
    of drawing it from Gaussian *j* plus the probability of drawing it from Gaussian
    *k*. The probability of drawing case *x* from one of the Gaussians is the likelihood
    multiplied by the prior for that Gaussian. With this in mind, we can write our
    Bayes’ rule more fully, as shown in [equation 19.2](#ch19equ02).
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算一个事件或另一个事件发生的概率时，我们只需将每个事件独立发生的概率相加。因此，从高斯分布 *j* 或 *k* 中抽取案例 *x* 的概率是从高斯分布
    *j* 中抽取的概率加上从高斯分布 *k* 中抽取的概率。从高斯分布中抽取案例 *x* 的概率是似然乘以该高斯分布的先验概率。考虑到这一点，我们可以更完整地写出贝叶斯定理，如[方程19.2](#ch19equ02)所示。
- en: equation 19.2\.
  id: totrans-933
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[方程19.2](#)'
- en: '![](eq19-2.jpg)'
  id: totrans-934
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-2.jpg](eq19-2.jpg)'
- en: Notice that the evidence has been expanded to more concretely show how the probability
    of drawing case *x[i]* from either Gaussian is the sum of the probabilities of
    drawing it from either independently. [Equation 19.2](#ch19equ02) allows us to
    calculate the posterior probability of case *x[i]* belonging to Gaussian *k*.
    [Equation 19.3](#ch19equ03) shows the same calculation, but for the posterior
    probability of case *x[i]* belonging to Gaussian *j*.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，证据已经被扩展，更具体地显示了从任一高斯分布中抽取案例 *x[i]* 的概率是独立抽取的概率之和。[方程19.2](#ch19equ02)使我们能够计算案例
    *x[i]* 属于高斯分布 *k* 的后验概率。[方程19.3](#ch19equ03)展示了相同的计算，但针对案例 *x[i]* 属于高斯分布 *j* 的后验概率。
- en: equation 19.3\.
  id: totrans-936
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[方程19.3](#)'
- en: '![](eq19-3.jpg)'
  id: totrans-937
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-3.jpg](eq19-3.jpg)'
- en: 'So far, so good. But how do we calculate the likelihood and the priors? The
    likelihood is the probability density function of the Gaussian distribution, which
    tells us the relative probability of drawing a case with a particular value from
    a Gaussian distribution with a particular combination of mean and variance. The
    probability density function for Gaussian distribution *k* is shown in [equation
    19.4](#ch19equ04), but it isn’t necessary for you to memorize it:'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。但我们如何计算似然和先验概率呢？似然是高斯分布的概率密度函数，它告诉我们从具有特定均值和方差组合的高斯分布中抽取具有特定值的案例的相对概率。高斯分布
    *k* 的概率密度函数在[方程式 19.4](#ch19equ04)中显示，但不需要你记住它：
- en: equation 19.4\.
  id: totrans-939
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 19.4。
- en: '![](eq19-4.jpg)'
  id: totrans-940
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-4.jpg](eq19-4.jpg)'
- en: where μ*[k]* and ![](pg457.jpg) are the mean and variance, respectively, for
    Gaussian *k*.
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 μ*[k]* 和 ![pg457.jpg](pg457.jpg) 分别是高斯 *k* 的均值和方差。
- en: At the start of the algorithm, the prior probabilities are generated randomly,
    just like the means and variances of the Gaussians. These priors get updated at
    each iteration to be the sum of the posterior probabilities for each Gaussian,
    divided by the number of cases. You can think of this as the mean posterior probability
    for a particular Gaussian across all cases.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法开始时，先验概率是随机生成的，就像高斯分布的均值和方差一样。这些先验概率在每次迭代中都会更新，成为每个高斯分布后验概率的总和除以案例数。你可以将这视为所有案例中特定高斯分布的后验概率均值。
- en: 19.1.2\. EM algorithm expectation and maximization steps
  id: totrans-943
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 19.1.2. EM算法的期望和最大化步骤
- en: 'Now that you have the necessary knowledge to understand how the posterior probabilities
    are calculated, let’s see how the EM algorithm iteratively fits the mixture model.
    The EM algorithm (as its name suggests) has two steps: expectation and maximization.
    The expectation step is where the posterior probabilities are calculated for each
    case, for each Gaussian. This is shown in the second panel from the top in [figure
    19.1](#ch19fig01).'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有了理解后验概率如何计算的知识，让我们看看EM算法如何迭代地拟合混合模型。EM算法（正如其名称所暗示的）有两个步骤：期望和最大化。期望步骤是计算每个案例、每个高斯分布的后验概率。这可以在[图19.1](#ch19fig01)从上数第二部分中看到。
- en: At this stage, the algorithm uses Bayes’ rule as we set out earlier, to calculate
    the posterior probabilities. The cases along the number line in [figure 19.1](#ch19fig01)
    are shaded to indicate their posterior probabilities.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，算法使用我们之前设定的贝叶斯定理来计算后验概率。[图19.1](#ch19fig01)中数轴上的案例被阴影覆盖，以表示它们的后验概率。
- en: Next comes the maximization step. The job of the maximization step is to update
    the parameters of the mixture model, to maximize the likelihood of the underlying
    data. This means updating the means, variances, and priors of the Gaussians.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是最大化步骤。最大化步骤的任务是更新混合模型的参数，以最大化潜在数据的似然。这意味着更新高斯分布的均值、方差和先验概率。
- en: Updating the mean of a particular Gaussian involves adding up the values of
    each case, weighted by their posterior probability for that Gaussian, and dividing
    by the sum of all the posterior probabilities. This is shown in [equation 19.5](#ch19equ05).
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 更新特定高斯分布的均值涉及将每个案例的值加起来，并乘以它们对该高斯分布的后验概率，然后除以所有后验概率的总和。这可以在[方程式 19.5](#ch19equ05)中看到。
- en: equation 19.5\.
  id: totrans-948
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 19.5。
- en: '![](eq19-5.jpg)'
  id: totrans-949
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-5.jpg](eq19-5.jpg)'
- en: Think about this for a second. Cases that are close to the mean of the distribution
    will have a high posterior probability for that distribution and so will contribute
    more to the updated mean. Cases far away from the distribution will have a small
    posterior probability and will contribute less to the updated mean. The result
    is that the Gaussian will move toward the mean of the cases that are most probable
    under this Gaussian. You can see this illustrated in the third panel of [figure
    19.1](#ch19fig01).
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 想想看。接近分布均值的案例将具有该分布的高后验概率，因此会对更新的均值贡献更多。远离分布的案例将具有较小的后验概率，对更新的均值贡献较少。结果是高斯分布会向在此高斯分布下最可能的案例均值移动。你可以在[图19.1](#ch19fig01)的第三部分中看到这一过程的说明。
- en: The variance of each Gaussian is updated in a similar way. We sum the squared
    difference between each case and the Gaussian’s mean, multiplied by the case’s
    posterior, and then divide by the sum of posteriors. This is shown in [equation
    19.6](#ch19equ06). The result is that the Gaussian will get wider or narrower,
    based on the spread of the cases that are most probable under this Gaussian. You
    can also see this illustrated in the third panel of [figure 19.1](#ch19fig01).
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 每个高斯分布的方差以类似的方式更新。我们计算每个案例与高斯均值之间的平方差，乘以该案例的后验概率，然后除以后验概率的总和。这可以在[方程19.6](#ch19equ06)中看到。结果是，高斯分布将根据在此高斯下最可能案例的分布范围变宽或变窄。你还可以在[图19.1](#ch19fig01)的第三部分中看到这一点的说明。
- en: equation 19.6\.
  id: totrans-952
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程19.6.
- en: '![](eq19-6.jpg)'
  id: totrans-953
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-6](eq19-6.jpg)'
- en: The last thing to be updated are the prior probabilities for each Gaussian.
    As mentioned already, the new priors are calculated by dividing the sum of the
    posterior probabilities for a particular Gaussian, and dividing by the number
    of cases, as shown in [equation 19.7](#ch19equ07). This means that a Gaussian
    for which many cases have a large posterior probability will have a large prior
    probability.
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要更新的是每个高斯分布的先验概率。如前所述，新的先验是通过将特定高斯的后验概率总和除以案例数量来计算的，如[方程19.7](#ch19equ07)所示。这意味着对于许多案例具有大后验概率的高斯分布，其先验概率将很大。
- en: Conversely, a Gaussian for which few cases have a large posterior probability
    will have a small prior probability. You can think of this as a soft or probabilistic
    equivalent to setting the prior equal to the proportion of cases belonging to
    each Gaussian.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，对于很少案例具有大后验概率的高斯分布，其先验概率将很小。你可以将其视为将先验设置为属于每个高斯案例比例的软或概率等价物。
- en: equation 19.7\.
  id: totrans-956
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程19.7.
- en: '![](eq19-7.jpg)'
  id: totrans-957
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-7](eq19-7.jpg)'
- en: Once the maximization step is complete, we perform another iteration of the
    expectation step, this time computing the posterior probabilities for each case
    under the new Gaussians. Once this is done, we then rerun the maximization step,
    again updating the means, variances, and priors for each Gaussian based on the
    posteriors. This cycle of expectation-maximization continues iteratively until
    either a specified number of iterations is reached or the overall likelihood of
    the data under the model changes by less than a specified amount (called *convergence*).
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦最大化步骤完成，我们进行期望步骤的另一次迭代，这次计算每个案例在新高斯下的后验概率。完成此操作后，我们再次运行最大化步骤，再次根据后验更新每个高斯分布的均值、方差和先验。这种期望-最大化循环迭代进行，直到达到指定的迭代次数或模型下数据的整体似然变化小于指定的量（称为*收敛*）。
- en: 19.1.3\. What if we have more than one variable?
  id: totrans-959
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 19.1.3. 如果我们有多于一个变量呢？
- en: 'In this section, we’ll extend what you learned about how the EM algorithm works
    in one dimension, to clustering over multiple dimensions. It is rare to come across
    a univariate (one-dimensional) clustering problem. Usually, our datasets contain
    multiple variables that we wish to use to identify clusters. I limited my explanation
    of the EM algorithm for Gaussian mixture models in the previous section, because
    a univariate Gaussian has only two parameters: its mean and variance. When we
    have a Gaussian distribution in more than one dimension (a multivariate Gaussian),
    we need to describe it using its centroid and its *covariance matrix*.'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将扩展你关于一维EM算法工作原理的知识，将其扩展到多维聚类。遇到一元（一维）聚类问题的情况很少。通常，我们的数据集包含多个变量，我们希望使用这些变量来识别聚类。我在上一节中限制了高斯混合模型EM算法的解释，因为一元高斯只有两个参数：其均值和方差。当我们有一个多维高斯分布（多元高斯）时，我们需要使用其质心和其*协方差矩阵*来描述它。
- en: 'We’ve come across centroids in previous chapters: a centroid is simply a vector
    of means, one for each dimension/variable in the dataset. A covariance matrix
    is a square matrix whose elements are the *covariance* between variables. For
    example, the value in the second row, third column of a covariance matrix indicates
    the covariance between variables 2 and 3 in the data. Covariance is an unstandardized
    measure of how much two variables change together. A positive covariance means
    that as one variable increases, so does the other. A negative covariance means
    that as one variable increases, the other decreases. A covariance of zero usually
    indicates no relationship between the variables. We can calculate the covariance
    between two variables using [equation 19.8](#ch19equ08).'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中遇到了质心：质心简单地是一个均值向量，对于数据集中的每个维度/变量都有一个。协方差矩阵是一个方阵，其元素是变量之间的*协方差*。例如，协方差矩阵的第二行第三列的值表示数据中变量2和变量3之间的协方差。协方差是衡量两个变量一起变化的非标准化度量。正协方差表示当一个变量增加时，另一个变量也增加。负协方差表示当一个变量增加时，另一个变量减少。零协方差通常表示变量之间没有关系。我们可以使用[方程19.8](#ch19equ08)计算两个变量之间的协方差。
- en: equation 19.8\.
  id: totrans-962
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程19.8。
- en: '![](eq19-8.jpg)'
  id: totrans-963
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-8.jpg](eq19-8.jpg)'
- en: '|  |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-965
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: While covariance is an unstandardized measure of the relationship between two
    variables, correlation is a standardized measure of the relationship between two
    variables. We can convert covariance into correlation by dividing it by the product
    of the variables’ standard deviations.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然协方差是两个变量之间关系的非标准化度量，但相关系数是两个变量之间关系的标准化度量。我们可以通过除以变量标准差的乘积将协方差转换为相关系数。
- en: '|  |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The covariance between one variable and itself is simply that variable’s variance.
    Therefore, the diagonal elements of a covariance matrix are the variances of each
    of the variables.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵中一个变量与自身的协方差简单地是该变量的方差。因此，协方差矩阵的对角线元素是每个变量的方差。
- en: '|  |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-970
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Covariance matrices are often called *variance-covariance* matrices for this
    reason.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，协方差矩阵通常被称为*方差-协方差*矩阵。
- en: '|  |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If the EM algorithm only estimated a variance for each Gaussian in each dimension,
    the Gaussians would be perpendicular to the axes of the feature space. Put another
    way, it would force the model to assume there were no relationships between the
    variables in the data. It’s usually more sensible to assume there will be some
    degree of relationship between the variables, and estimating the covariance matrix
    allows the Gaussians to lie diagonally across the feature space.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 如果EM算法只估计每个高斯分布在每个维度上的方差，那么高斯分布将与特征空间的轴垂直。换句话说，这会迫使模型假设数据中的变量之间没有关系。通常更有意义的是假设变量之间将存在某种程度的关系，并且估计协方差矩阵允许高斯分布在特征空间中对角线排列。
- en: '|  |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-975
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because we estimate the covariance matrix, Gaussian mixture model clustering
    is insensitive to variables on different scales. Therefore, we *don’t* need to
    scale our variables before training the model.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们估计协方差矩阵，高斯混合模型聚类对不同尺度的变量不敏感。因此，我们*不需要*在训练模型之前对变量进行缩放。
- en: '|  |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When we’re clustering over more than one dimension, the EM algorithm randomly
    initializes the centroid, covariance matrix, and prior for each Gaussian. It then
    calculates the posterior probability for each case, for each Gaussian in the expectation
    step. In the maximization step, the centroid, covariance matrix, and prior probability
    are updated for each Gaussian. The EM algorithm continues to iterate until either
    the maximum number of iterations is reached or the algorithm reaches convergence.
    The EM algorithm for a bivariate case is illustrated in [figure 19.2](#ch19fig02).
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在多个维度上进行聚类时，EM算法会随机初始化每个高斯分布的质心、协方差矩阵和先验概率。然后，在期望步骤中，它计算每个案例的每个高斯分布的后验概率。在最大化步骤中，每个高斯分布的质心、协方差矩阵和先验概率都会更新。EM算法会继续迭代，直到达到最大迭代次数或算法收敛。二元情况的EM算法在[图19.2](#ch19fig02)中展示。
- en: Figure 19.2\. The expectation-maximization algorithm for two, two-dimensional
    Gaussians. Two Gaussians are randomly initialized in the feature space. In the
    expectation step, the posterior probabilities for each case are calculated for
    each Gaussian. In the maximization step, the centroids, covariance matrices, and
    priors are updated for each Gaussian, based on the posteriors. The process continues
    until the likelihood converges.
  id: totrans-979
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 19.2. 两个二维高斯的双向期望最大化算法。在特征空间中随机初始化两个高斯。在期望步骤中，为每个高斯计算每个案例的后验概率。在最大化步骤中，根据后验，更新每个高斯的中心、协方差矩阵和先验。这个过程一直持续到似然收敛。
- en: '![](fig19-2_alt.jpg)'
  id: totrans-980
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig19-2_alt.jpg)'
- en: '|  |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The mathematics for the multivariate case**'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: '**多元情况下的数学**'
- en: The equations for updating the means and (co)variances are a little more complicated
    than those we encountered in the univariate case. If you’re interested, here they
    are.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 更新均值和（协）方差的方程比我们在单变量情况中遇到的要复杂一些。如果你感兴趣，这里它们是。
- en: The mean of Gaussian *k* for variable *a* is
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 对于变量 a 的高斯 k 的均值是
- en: '![](pg461-1.jpg)'
  id: totrans-985
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg461-1.jpg)'
- en: The centroid of the Gaussian is therefore just a vector where each element is
    the mean of a different variable.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，高斯的重心就是一个向量，其中每个元素是不同变量的均值。
- en: The covariance between variables *a* and *b* for Gaussian *k* is
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高斯 k 的变量 a 和 b 之间的协方差是
- en: '![](pg461-2.jpg)'
  id: totrans-988
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg461-2.jpg)'
- en: where σ*[k]* is the covariance matrix for Gaussian *k*.
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 σ*[k]* 是高斯 k 的协方差矩阵。
- en: Finally, in the multivariate case, the likelihood (*p*(*x[i]*|*k*)) now needs
    to take into account the covariance, and so it now becomes
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在多元情况下，现在需要考虑协方差，因此现在变为
- en: '![](pg461-3.jpg)'
  id: totrans-991
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg461-3.jpg)'
- en: '|  |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Does this process seem familiar to you—iteratively updating the position of
    the clusters based on how far cases in the data are from them? We saw a similar
    procedure for the k-means algorithms in [chapter 16](kindle_split_029.html#ch16).
    Gaussian mixture model clustering therefore extends k-means clustering to allow
    non-spherical clusters or different diameters (due to the covariance matrix) and
    soft clustering. In fact, if you were to constrain a Gaussian mixture model such
    that all clusters had the same variance, no covariance, and equal priors, you
    would get a result very similar to that provided by Lloyd’s algorithm!
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程看起来熟悉吗——基于数据中的案例与聚类位置的距离，迭代更新聚类的位置？我们在第 16 章中看到了类似的过程，即 k-means 算法。因此，高斯混合模型聚类扩展了
    k-means 聚类，允许非球形聚类或不同直径（由于协方差矩阵）以及软聚类。实际上，如果你约束高斯混合模型，使得所有聚类都有相同的方差、没有协方差和相等的先验，你会得到与
    Lloyd 算法提供的结果非常相似的结果！
- en: 19.2\. Building your first Gaussian mixture model for clustering
  id: totrans-994
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 19.2. 构建您的第一个用于聚类的高斯混合模型
- en: In this section, I’ll show you how to build a Gaussian mixture model for clustering.
    We’ll continue using the Swiss banknote dataset so we can compare the results
    to the DBSCAN and OPTICS clustering results. An immediate advantage of mixture
    model clustering over DBSCAN and OPTICS is that it is invariant to variables on
    different scales, so there’s no need to scale our data first.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何构建用于聚类的高斯混合模型。我们将继续使用瑞士银行纸币数据集，以便我们可以将结果与 DBSCAN 和 OPTICS 聚类结果进行比较。混合模型聚类相对于
    DBSCAN 和 OPTICS 的一个直接优势是它对不同尺度的变量不变，因此我们不需要先对数据进行缩放。
- en: '|  |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-997
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: For correctness, I should say that there’s no need to scale our data as long
    as we make no prior specification of the covariances of the model components.
    It’s possible to specify our prior beliefs of the means and covariances of the
    components, though we won’t do that here. If we were to do this, it would be important
    for the covariances to consider the scale of the data.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确性，我应该说明，只要我们没有对模型组件的协方差做出先前的指定，就没有必要对数据进行缩放。我们可以指定我们对组件的均值和协方差的先验信念，尽管我们在这里不会这样做。如果我们这样做，考虑数据的尺度对于协方差来说是很重要的。
- en: '|  |'
  id: totrans-999
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The mlr package doesn’t have an implementation of the mixture modeling algorithm
    we’re going to use, so instead we’ll use functions from the mclust package. Let’s
    start by loading the package:'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: mlr 包没有我们即将使用的混合建模算法的实现，因此我们将使用 mclust 包中的函数。让我们首先加载这个包：
- en: '[PRE56]'
  id: totrans-1001
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: There are a few things I particularly like about using the mclust package for
    clustering. The first is that it’s the only R package I know of that prints a
    cool logo to the console when you load it. The second is that it displays a progress
    bar to indicate how much longer your clustering will take (very important for
    judging whether there’s time for a cup of tea). And third, its function for fitting
    the model will automatically try a range of cluster numbers and try to select
    the best-fitting number. We can also manually specify the number of clusters if
    we think we know better.
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 我特别喜欢使用mclust包进行聚类的几个方面。首先，它是唯一我知道在加载时会打印出酷炫标志的R包。其次，它显示一个进度条来指示聚类还需要多长时间（对于判断是否有时间泡一杯茶非常重要）。第三，它的模型拟合函数会自动尝试一系列聚类数量，并尝试选择最佳拟合数量。如果我们认为我们知道得更好，我们也可以手动指定聚类数量。
- en: Let’s use the `Mclust()` function to perform the clustering and then call `plot()`
    on the results.
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`Mclust()`函数进行聚类，然后对结果调用`plot()`函数。
- en: Listing 19.1\. Performing and plotting mixture model clustering
  id: totrans-1004
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 19.1。执行并绘制混合模型聚类
- en: '[PRE57]'
  id: totrans-1005
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Plotting the `Mclust()` output does something a little odd (and irritating,
    as far as I’m concerned). It prompts us to enter a number from 1 to 4, corresponding
    to one of the following options:'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制`Mclust()`输出的结果有些奇怪（而且对我来说有些恼人）。它提示我们输入一个1到4的数字，对应以下选项之一：
- en: BIC
  id: totrans-1007
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BIC
- en: Classification
  id: totrans-1008
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类
- en: Uncertainty
  id: totrans-1009
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不确定性
- en: Density
  id: totrans-1010
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 密度
- en: Entering the number will draw the corresponding plot containing useful information.
    Let’s look at each of these plots in turn.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数字将绘制包含有用信息的相应图表。让我们依次查看这些图表。
- en: The first plot available to us shows the *Bayesian information criterion* (BIC)
    for the range of cluster numbers and model types the `Mclust()` function tried.
    This plot is shown in [figure 19.3](#ch19fig03). The BIC is a metric for comparing
    the fit of different models, and it penalizes us for having too many parameters
    in the model. The BIC is usually defined as in [equation 19.9](#ch19equ09).
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以获得的第一个图表显示了`Mclust()`函数尝试的聚类数量和模型类型的贝叶斯信息准则（BIC）。此图表显示在[图 19.3](#ch19fig03)。BIC是用于比较不同模型拟合度的指标，它会对模型中过多的参数进行惩罚。BIC通常定义为[方程式
    19.9](#ch19equ09)。
- en: equation 19.9\.
  id: totrans-1013
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 19.9。
- en: '![](eq19-9.jpg)'
  id: totrans-1014
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-9.jpg](eq19-9.jpg)'
- en: where *n* is the number of cases, *p* is the number of parameters in the model,
    and *L* is the overall likelihood of the model.
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是案例数量，*p* 是模型中的参数数量，*L* 是模型的总体似然值。
- en: Therefore, for a fixed likelihood, as the number of parameters increases, the
    BIC increases. Conversely, for a fixed number of parameters, as the model likelihood
    increases, the BIC decreases. Therefore, the smaller the BIC, the better and/or
    more parsimonious our model is. Imagine that we had two models, each of which
    fit the dataset equally well, but one had 3 parameters and the other had 10\.
    The model with 3 parameters would have the lower BIC.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于固定的似然值，随着参数数量的增加，BIC会增加。相反，对于固定的参数数量，随着模型似然值的增加，BIC会减少。因此，BIC越小，我们的模型越好和/或越简洁。想象一下，我们有两个模型，它们都能同样好地拟合数据集，但一个有3个参数，另一个有10个。具有3个参数的模型将具有较低的BIC。
- en: Figure 19.3\. The BIC plot from our mclust model. The x-axis shows the number
    of clusters, the y-axis shows the Bayesian information criterion (BIC), and each
    line shows a different model, with the three-letter code indicating which constraints
    are put on the covariance matrix. In this arrangement of the BIC, higher values
    indicate better-fitting and/or more parsimonious models.
  id: totrans-1017
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 19.3。我们的mclust模型的BIC图。x轴显示聚类数量，y轴显示贝叶斯信息准则（BIC），每条线表示不同的模型，三位代码表示对协方差矩阵施加了哪些约束。在这个BIC排列中，较高的值表示更好的拟合和/或更简洁的模型。
- en: '![](fig19-3_alt.jpg)'
  id: totrans-1018
  prefs: []
  type: TYPE_IMG
  zh: '![fig19-3_alt.jpg](fig19-3_alt.jpg)'
- en: The form of BIC shown in the plot is actually sort of the other way around and
    takes the form shown in [equation 19.10](#ch19equ10). After being rearranged this
    way, better fitting and/or more parsimonious models will actually have a higher
    BIC value.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中显示的BIC形式实际上是相反的，其形式如[方程式 19.10](#ch19equ10)所示。经过这样的重新排列后，更好的拟合和/或更简洁的模型实际上会有更高的BIC值。
- en: equation 19.10\.
  id: totrans-1020
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 19.10。
- en: '![](eq19-10.jpg)'
  id: totrans-1021
  prefs: []
  type: TYPE_IMG
  zh: '![eq19-10.jpg](eq19-10.jpg)'
- en: Now we know what the BIC is and how to interpret it, but what are all the lines
    in [figure 19.3](#ch19fig03)? Well, the `Mclust()` function tries a range of cluster
    numbers for us, for a range of different model types. For each combination of
    model type and cluster number, the function evaluates the BIC. This information
    is conveyed in our BIC plot. But what do I mean by *model types*? I didn’t mention
    anything about this when I showed you how Gaussian mixture models work. When we
    train a mixture model, it’s possible to put constraints on the covariance matrix
    to reduce the number of parameters needed to describe the model. This can help
    to prevent overfitting the data.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了BIC是什么以及如何解释它，但[图19.3](#ch19fig03)中的所有线条代表什么？嗯，`Mclust()`函数尝试了不同模型类型和簇数量的范围，为我们。对于模型类型和簇数量的每一种组合，该函数都会评估BIC。这些信息通过我们的BIC图传达。但我说的是*模型类型*是什么意思？在我向您展示高斯混合模型如何工作时，我没有提到这一点。当我们训练混合模型时，我们可以对协方差矩阵施加约束，以减少描述模型所需的参数数量。这有助于防止数据过拟合。
- en: 'Each of the model types is represented by a different line in [figure 19.3](#ch19fig03),
    and each has a strange three-letter code identifying it. The first letter of each
    code refers to the *volume* of each Gaussian, the second letter refers to the
    *shape*, and the third letter refers to the *orientation*. Each of these components
    can take one of the following:'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 每种模型类型在[图19.3](#ch19fig03)中由不同的线条表示，并且每种都有代表它的奇怪的三字母代码。每个代码的第一个字母指的是每个高斯分布的*体积*，第二个字母指的是*形状*，第三个字母指的是*方向*。这些组件中的每一个都可以取以下值之一：
- en: '*E* for *equal*'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*代表*相等*'
- en: '*V* for *variable*'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*代表*变量*'
- en: 'The shape and orientation components can also take a value of *I* for *identity*.
    The effects of the values on models are as follows:'
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 形状和方向分量也可以取值为*I*，代表*恒等*。这些值对模型的影响如下：
- en: 'Volume component:'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体积分量：
- en: '*E*—Gaussians with equal volume'
  id: totrans-1028
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*—等体积的高斯分布'
- en: '*V*—Gaussians with different volumes'
  id: totrans-1029
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*—不同体积的高斯分布'
- en: 'Shape component:'
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状分量：
- en: '*E*—Gaussians with equal aspect ratios'
  id: totrans-1031
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*—具有相等纵横比的高斯分布'
- en: '*V*—Gaussians with different aspect ratios'
  id: totrans-1032
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*—具有不同纵横比的高斯分布'
- en: '*I*—Clusters that are perfectly spherical'
  id: totrans-1033
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I*—完美球形的簇'
- en: 'Orientation component:'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方向分量：
- en: '*E*—Gaussians with the same orientation through the feature space'
  id: totrans-1035
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E*—通过特征空间具有相同方向的高斯分布'
- en: '*V*—Gaussians with different orientations'
  id: totrans-1036
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*—不同方向的高斯分布'
- en: '*I*—Clusters that are orthogonal to the axes of the feature space'
  id: totrans-1037
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I*—与特征空间轴正交的簇'
- en: So really, the `Mclust()` function is performing a tuning experiment for us
    and will automatically select the model with the highest BIC value. In this case,
    the best model is the one that uses the VVE covariance matrix with three Gaussians
    (use `swissMclust$modelName` and `swissMclust$G` to extract this information).
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实际上，`Mclust()`函数为我们进行了一次调整实验，并会自动选择具有最高BIC值的模型。在这种情况下，最佳模型是使用三个高斯分布的VVE协方差矩阵的模型（使用`swissMclust$modelName`和`swissMclust$G`来提取此信息）。
- en: Figure 19.4\. The classification plot from our mclust model. All variables in
    the original data are plotted against each other in a scatterplot matrix, with
    cases shaded and shaped according to their cluster. Ellipses indicate the covariances
    of each Gaussian, and stars indicate their centroids.
  id: totrans-1039
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图19.4。我们mclust模型的分类图。原始数据中的所有变量都在散点图矩阵中相互绘制，案例根据其簇进行着色和形状设计。椭圆表示每个高斯分布的协方差，星号表示它们的质心。
- en: '![](fig19-4_alt.jpg)'
  id: totrans-1040
  prefs: []
  type: TYPE_IMG
  zh: '![图19-4替代](fig19-4_alt.jpg)'
- en: That’s the first plot, which is certainly useful. Perhaps the most useful plot,
    however, is the one obtained from option 2. It shows us our final clustering result
    from the selected model; see [figure 19.4](#ch19fig04). The ellipses indicate
    the covariances of each cluster, and the star at the center of each indicates
    its centroid. The model appears to fit the data well and seems to have identified
    three reasonably convincing clusters compared to the two identified by our DBSCAN
    model (though we should use internal cluster metrics and Jaccard indices to more
    objectively compare the models).
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 那是第一个图表，当然很有用。然而，最有用的图表可能是从选项2获得的图表。它显示了所选模型的最终聚类结果；请参见[图19.4](#ch19fig04)。椭圆表示每个簇的协方差，每个簇中心的星号表示其质心。模型似乎很好地拟合了数据，并且似乎与我们的DBSCAN模型识别的两个簇相比，识别了三个相当有说服力的簇（尽管我们应该使用内部簇指标和Jaccard指数来更客观地比较模型）。
- en: The third plot is similar to the second, but it sets the size of each case based
    on its uncertainty (see [figure 19.5](#ch19fig05)). A case whose posterior probabilities
    aren’t dominated by a single Gaussian will have a high uncertainty, and this plot
    helps us identify cases that could be considered outliers.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 第三张图与第二张图类似，但它根据每个案例的不确定性设置其大小（见[图19.5](#ch19fig05)）。如果一个案例的后验概率不是由单个高斯分布主导，那么它将具有高不确定性，而这个图有助于我们识别可能被视为异常值的案例。
- en: Figure 19.5\. The uncertainty plot from our mclust model. This plot is similar
    to the classification plot, except that the size of each case corresponds to its
    uncertainty under the final model.
  id: totrans-1043
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图19.5. 我们mclust模型的不确定性图。这个图与分类图类似，但每个案例的大小对应于最终模型下的不确定性。
- en: '![](fig19-5_alt.jpg)'
  id: totrans-1044
  prefs: []
  type: TYPE_IMG
  zh: '![](fig19-5_alt.jpg)'
- en: The fourth and final plot shows the density of the final mixture model (see
    [figure 19.6](#ch19fig06)). I find this plot less useful, but it looks quite cool.
    To exit `Mclust()`’s `plot()` method, you need to enter `0` (which is why I find
    this irritating).
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 第四张和最后一张图显示了最终混合模型的密度（见[图19.6](#ch19fig06)）。我发现这个图不太有用，但它看起来相当酷。要退出`Mclust()`的`plot()`方法，您需要输入`0`（这就是为什么我觉得这很烦人的原因）。
- en: 19.3\. Strengths and weaknesses of mixture model clustering
  id: totrans-1046
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 19.3. 混合模型聚类的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    mixture model clustering will perform well for you.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪些算法会对特定任务表现良好，但以下是一些优点和缺点，这将帮助您决定混合模型聚类是否适合您。
- en: 'The strengths of mixture model clustering are as follows:'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型聚类的优点如下：
- en: It can identify non-spherical clusters of different diameters.
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以识别不同直径的非球形聚类。
- en: It estimates the probability that a case belongs to each cluster.
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它估计一个案例属于每个聚类的概率。
- en: It is insensitive to variables on different scales.
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对不同尺度的变量不敏感。
- en: Figure 19.6\. The density plot from our mclust model. This matrix of plots shows
    the 2D density of the final model for each combination of variables in the feature
    space.
  id: totrans-1052
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图19.6. 我们mclust模型的密度图。这个图的矩阵显示了特征空间中每个变量组合的最终模型的二维密度。
- en: '![](fig19-6_alt.jpg)'
  id: totrans-1053
  prefs: []
  type: TYPE_IMG
  zh: '![](fig19-6_alt.jpg)'
- en: 'The weaknesses of mixture model clustering are these:'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型聚类的缺点如下：
- en: While the clusters need not be spherical, they do need to be elliptical.
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然聚类不需要是球形的，但它们确实需要是椭圆形的。
- en: It cannot natively handle categorical variables.
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不能原生地处理分类变量。
- en: It cannot select the optimal number of clusters automatically.
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不能自动选择最佳聚类数量。
- en: Due to the randomness of the initial Gaussians, it has the potential to converge
    to a locally optimal model.
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于初始高斯分布的随机性，它有可能收敛到一个局部最优模型。
- en: It is sensitive to outliers.
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对异常值敏感。
- en: If the clusters cannot be approximated by a multivariate Gaussian, it’s unlikely
    the final model will fit well.
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果聚类不能由多元高斯近似，那么最终模型不太可能拟合良好。
- en: '|  |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Use the `Mclust()` function to train a model, setting the `G` argument to `2`
    and the `modelNames` argument to `"VVE"` to force a VVE model with two clusters.
    Plot the results, and examine the clusters.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Mclust()`函数训练模型，将`G`参数设置为`2`，将`modelNames`参数设置为`"VVE"`以强制使用两个聚类的VVE模型。绘制结果，并检查聚类。
- en: '|  |'
  id: totrans-1064
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-1065
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: 'Using the `clusterboot()` function, calculate the stability of the clusters
    generated from a two-cluster and a three-cluster VVE model. Hint: Use `noisemclustCBI`
    as the `clustermethod` argument to use mixture modeling. Is it easy to compare
    the Jaccard indices of models with different numbers of clusters?'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`clusterboot()`函数，计算从双聚类和三聚类VVE模型生成的聚类的稳定性。提示：将`clustermethod`参数设置为`noisemclustCBI`以使用混合模型。比较不同数量聚类的Jaccard指数容易吗？
- en: '|  |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-1069
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Gaussian mixture model clustering fits a set of Gaussian distributions to the
    data and estimates the probability of the data coming from each Gaussian.
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型聚类将一组高斯分布拟合到数据中，并估计数据来自每个高斯分布的概率。
- en: The expectation-maximization (EM) algorithm is used to iteratively update the
    model until the likelihood of the data converges.
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用期望最大化（EM）算法迭代更新模型，直到数据的似然收敛。
- en: Gaussian mixture modeling is a soft-clustering method that gives us a probability
    of each case belonging to each cluster.
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合建模是一种软聚类方法，它为我们提供了每个案例属于每个聚类的概率。
- en: In one dimension, the EM algorithm only needs to update the mean, variance,
    and prior probability of each Gaussian.
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一维中，EM算法只需要更新每个高斯分布的均值、方差和先验概率。
- en: In more than one dimension, the EM algorithm needs to update the centroid, covariance
    matrix, and prior probability of each Gaussian.
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在超过一维的情况下，EM算法需要更新每个高斯分布的重心、协方差矩阵和先验概率。
- en: Constraints can be placed on the covariance matrix to control the volume, shape,
    and orientation of the Gaussians.
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以对协方差矩阵施加约束，以控制高斯分布的体积、形状和方向。
- en: Solutions to exercises
  id: totrans-1076
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题的解答
- en: 'Train a VVE mixture model with two clusters:'
  id: totrans-1077
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用两个簇训练VVE混合模型：
- en: '[PRE58]'
  id: totrans-1078
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Compare the cluster stability of a two- and three-cluster mixture model:'
  id: totrans-1079
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较两个簇和三个簇混合模型的聚类稳定性：
- en: '[PRE59]'
  id: totrans-1080
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Chapter 20\. Final notes and further reading
  id: totrans-1081
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第20章。最后的笔记和进一步阅读
- en: '*This chapter covers*'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: A brief summary of what we’ve covered
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们所涵盖内容的简要总结
- en: A roadmap to further your knowledge
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步拓展知识的路线图
- en: Take a moment to look back at all the topics we’ve covered throughout this book.
    We’ve covered a huge amount of information, and now that we’re near the end of
    the book, I’d like to put it all together to give you the bigger picture. At university,
    I used to get frustrated with lecturers who would assume that because they had
    taught something to us, we would simply remember it. I know this isn’t how most
    people learn, and you may well have forgotten many of the details I tried to teach
    throughout the book. That’s okay—I hope you feel that you can pick up this book
    as a reference for future machine learning projects you might be working on. And
    in this chapter, I summarize many of the broad, important concepts we touched
    on throughout the book.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 抽空回顾一下本书中涵盖的所有主题。我们涵盖了大量的信息，现在我们接近本书的结尾，我想将所有这些内容整合起来，给你一个更全面的视角。在大学时，我常常对那些认为因为他们教了我们某些东西，我们就应该简单地记住它们的讲师感到沮丧。我知道这并不是大多数人学习的方式，你很可能已经忘记了我在本书中试图教授的许多细节。没关系——我希望你觉得你可以将这本书作为你未来可能从事的机器学习项目的参考。在本章中，我还总结了本书中涉及到的许多广泛而重要的概念。
- en: After completing this book, you have a formidable number of machine learning
    algorithms in your toolbox—enough to tackle a huge range of problems. I also hope
    that you now know a general approach to machine learning and, importantly, how
    to objectively evaluate the performance of your model-building processes. While
    I’ve provided you with both “bread and butter” algorithms as well as modern ones,
    machine learning research is fast moving. There are many more algorithms I didn’t
    cover, such as those used in deep learning, reinforcement learning, and anomaly
    detection. Therefore, in this chapter, I also provide you with several potential
    avenues for future learning. When learning something new, I get frustrated when
    I reach the end of the textbook and then have no idea where to go next; so, I’ll
    recommend additional books and resources to further your learning.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本书后，你将在工具箱中拥有大量的机器学习算法——足够解决各种各样的问题。我也希望你现在知道了一种通用的机器学习方法，并且更重要的是，如何客观地评估你的模型构建过程的表现。虽然我为你提供了“面包和黄油”算法以及现代算法，但机器学习研究进展迅速。还有许多我没有涵盖的算法，例如深度学习、强化学习和异常检测中使用的算法。因此，在本章中，我还为你提供了几个未来学习的潜在途径。当我学到新东西时，当我到达教科书结尾却不知道下一步该去哪里时，我会感到沮丧；所以，我会推荐额外的书籍和资源来进一步你的学习。
- en: 20.1\. A brief recap of machine learning concepts
  id: totrans-1087
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.1\. 机器学习概念的简要回顾
- en: 'In this section, I’ll summarize the general machine learning concepts we covered
    throughout the book, referencing the relevant sections of the book as we go. These
    concepts include the following:'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将总结本书中涵盖的通用机器学习概念，并在进行过程中引用相关的章节。这些概念包括以下内容：
- en: Types of machine learning algorithms
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法的类型
- en: The bias-variance trade-off
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: Model validation
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型验证
- en: Hyperparameter tuning
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Missing value imputation
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值插补
- en: Feature engineering and feature selection
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程和特征选择
- en: Ensemble techniques
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成技术
- en: Regularization
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化
- en: My hope is that now that you’ve completed the book, these concepts will fit
    more concretely into your bigger picture of machine learning.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望现在你已经完成了本书，这些概念将更具体地融入你对机器学习的整体理解中。
- en: 20.1.1\. Supervised, unsupervised, and semi-supervised learning
  id: totrans-1098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.1\. 监督学习、无监督学习和半监督学习
- en: 'Machine learning tasks can be divided into supervised and unsupervised tasks,
    based on whether the algorithm has access to labeled data: whether we have access
    to the ground truth when training the model. Algorithms that learn patterns in
    the data that can be used to predict the ground truth are said to be *supervised*.
    Supervised machine learning algorithms can be further distinguished, based on
    the kind of output variable they predict. Supervised learning algorithms that
    predict categorical variables (or classes) are said to be *classification algorithms*,
    while those that predict continuous variables are said to be *regression algorithms*.'
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 根据算法是否有访问标记数据的权限：在训练模型时我们是否可以访问真实值，可以将机器学习任务分为**监督学习**和**无监督学习**。那些在数据中学习可用于预测真实值的模式的算法被称为**监督学习**。根据它们预测的输出变量的类型，监督机器学习算法可以进一步区分。预测分类变量（或类别）的监督学习算法被称为**分类算法**，而预测连续变量的算法被称为**回归算法**。
- en: '|  |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Some algorithms—like k-nearest neighbors, random forest, and XGBoost—can be
    used for both classification *and* regression.
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法——如k近邻、随机森林和XGBoost——可以用于**分类**和**回归**。
- en: '|  |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '*Unsupervised* algorithms learn patterns in the data without any form of ground
    truth. We can differentiate these algorithms based on what their purpose is. Unsupervised
    learning algorithms that can compress the information in a high-dimensional dataset
    into a lower-dimensional representation are called *dimension-reduction algorithms*.
    Unsupervised learning algorithms that find groups of cases that are more similar
    to each other than cases in other groups are called *clustering algorithms*.'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督**算法在没有任何形式真实值的情况下学习数据中的模式。我们可以根据它们的目的来区分这些算法。可以将将高维数据集中的信息压缩到低维表示的无监督学习算法称为**降维算法**。将找到比其他组中的案例更相似的案例组的无监督学习算法称为**聚类算法**。'
- en: 'You first encountered these definitions in [section 1.2](kindle_split_010.html#ch01lev1sec2),
    all the way back in [chapter 1](kindle_split_010.html#ch01). I’ve reproduced [figure
    1.5](kindle_split_010.html#ch01fig05) in [figure 20.1](#ch20fig01): it summarizes
    the differences between supervised and unsupervised learning.'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 您第一次遇到这些定义是在[第1.2节](kindle_split_010.html#ch01lev1sec2)，早在[第1章](kindle_split_010.html#ch01)。我在[图20.1](#ch20fig01)中重新绘制了[图1.5](kindle_split_010.html#ch01fig05)：它总结了监督学习和无监督学习之间的差异。
- en: Figure 20.1\. Supervised vs. unsupervised machine learning. Supervised algorithms
    take data already labeled with a ground truth and build a model that can predict
    the labels of new, unlabeled data. Unsupervised algorithms take unlabeled data
    and learn patterns within it, such that new data can be mapped onto these patterns.
  id: totrans-1106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图20.1\. 监督学习与无监督学习。监督算法使用已经标记有真实值的标签数据来构建一个可以预测新、未标记数据标签的模型。无监督算法使用未标记的数据，并学习其中的模式，以便可以将新数据映射到这些模式上。
- en: '![](fig20-1_alt.jpg)'
  id: totrans-1107
  prefs: []
  type: TYPE_IMG
  zh: '![](fig20-1_alt.jpg)'
- en: '|  |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Although I didn’t mention this in [chapter 1](kindle_split_010.html#ch01), not
    all unsupervised algorithms can make predictions on new data. For example, hierarchical
    clustering and t-SNE models are unable to make predictions on new data.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在[第1章](kindle_split_010.html#ch01)中没有提到这一点，但并非所有无监督算法都能对新数据进行预测。例如，层次聚类和t-SNE模型无法对新数据进行预测。
- en: '|  |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: There is an approach partway between supervised and unsupervised machine learning
    called *semi-supervised learning*. Semi-supervised learning is an approach, rather
    than a type of algorithm, and is useful when we have access to partially labeled
    data. If we expertly label as many of the cases in a dataset as is feasibly possible,
    then we can build a supervised model using only these labeled data. We use this
    model to predict the labels of the rest of the dataset. Now we combine the data
    with the manual labels and pseudo-labels, and use this to train a new model.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 在**监督学习**和**无监督学习**之间有一个被称为**半监督学习**的方法。半监督学习是一种方法，而不是一种算法，当我们可以访问部分标记的数据时很有用。如果我们能够尽可能精确地标记数据集中的案例，那么我们可以仅使用这些标记数据构建一个监督模型。我们使用这个模型来预测数据集中其余部分的标签。现在我们将数据与手动标签和伪标签结合起来，并使用这些数据来训练一个新的模型。
- en: '[Figure 20.2](#ch20fig02) shows all the machine learning algorithms we used
    throughout this book, partitioning them into supervised and unsupervised, and
    also classification, regression, dimension reduction, and clustering. My hope
    is that you can refer to this figure when deciding which algorithms are most suitable
    for the task at hand, and that you will add to the algorithms listed here as your
    knowledge grows.'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20.2](#ch20fig02) 展示了本书中使用的所有机器学习算法，将它们分为监督学习和无监督学习，以及分类、回归、降维和聚类。我的希望是，当你在决定哪些算法最适合当前任务时，可以参考这个图，并且随着你知识的增长，你将添加更多列在这里的算法。'
- en: Figure 20.2\. Summary of the algorithms we cover in the book, whether they are
    supervised or unsupervised learners, and whether they can be used for classification,
    regression, dimension reduction, or clustering
  id: totrans-1114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图20.2\. 本书涵盖的算法总结，无论它们是监督学习器还是无监督学习器，以及它们是否可用于分类、回归、降维或聚类
- en: '![](fig20-2.jpg)'
  id: totrans-1115
  prefs: []
  type: TYPE_IMG
  zh: '![图片20-2](fig20-2.jpg)'
- en: 20.1.2\. Balancing the bias-variance trade-off for model performance
  id: totrans-1116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.2\. 平衡模型性能的偏差-方差权衡
- en: When training a predictive model, it’s important to evaluate how it will perform
    in the real world. When evaluating the performance of our models, we should *never*
    evaluate them using the data we used to train them. This is because models will
    almost always perform better when making predictions on the data used to train
    them than when making predictions on unseen data.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练预测模型时，评估它在现实世界中的表现非常重要。在评估我们模型的表现时，我们*永远*不应该使用我们用来训练模型的数据来评估它们。这是因为模型在预测用于训练它们的数据时几乎总是表现得比在预测未见数据时更好。
- en: In [chapter 3](kindle_split_013.html#ch03), you learned that an important concept
    to understand when evaluating model performance is the bias-variance trade-off.
    As the complexity of a model increases, and the more closely it fits the training
    set, the more variable its predictions will be on unseen data. Models that are
    too simple and don’t capture the relationships in the data well are biased toward
    making consistently poor predictions. As we increase the complexity of our model,
    its variance will increase, and its bias will decrease; the inverse is also true.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](kindle_split_013.html#ch03)中，你了解到在评估模型性能时，一个重要的概念是偏差-方差权衡。随着模型复杂性的增加，以及它对训练集的拟合程度越紧密，它在未见数据上的预测将变得更加多变。过于简单且无法很好地捕捉数据中关系的模型倾向于做出持续较差的预测。当我们增加模型的复杂性时，其方差会增加，其偏差会减少；反之亦然。
- en: The bias-variance trade-off therefore describes the balance between overfitting
    (training a model that fits the noise of the training set) and underfitting (training
    a model that poorly fits the training set). Somewhere between a model that overfits
    and a model that underfits is an optimally fitting model whose predictions generalize
    well to unseen data. The way to tell if we are underfitting or overfitting is
    to use cross-validation. However, even just passing the training set back through
    the model will tell you if you’re underfitting, because the model will perform
    poorly.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偏差-方差权衡描述了过拟合（训练一个拟合训练集噪声的模型）和欠拟合（训练一个拟合训练集较差的模型）之间的平衡。在过拟合模型和欠拟合模型之间，存在一个最优拟合模型，其预测可以很好地推广到未见数据。判断我们是否欠拟合或过拟合的方法是使用交叉验证。然而，即使只是将训练集再次通过模型，也会告诉你你是否欠拟合，因为模型的表现会较差。
- en: 20.1.3\. Using model validation to identify over-/underfitting
  id: totrans-1120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.3\. 使用模型验证来识别过拟合/欠拟合
- en: To evaluate how well a model will make predictions on new data, we need to pass
    new, unseen data through the model and see how closely its predictions match the
    ground truth. One way of doing this would be to train the model on the data at
    hand and then, as new data is generated, pass that data through the model to evaluate
    its predictions. This process could make the model-building process take years,
    so a more realistic approach is to split the data into training and test sets.
    In this way, the model is trained using the training set and is given the test
    set on which to make predictions. This process is called *cross-validation*, and
    you learned about it in [chapter 3](kindle_split_013.html#ch03).
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型在新数据上预测的效果，我们需要将新的、未见过的数据通过模型，并查看其预测与真实值之间的匹配程度。一种方法是在手头的数据上训练模型，然后，随着新数据的生成，将数据通过模型来评估其预测。这个过程可能会使模型构建过程持续数年，因此一个更现实的方法是将数据分为训练集和测试集。这样，模型使用训练集进行训练，并给出测试集以进行预测。这个过程被称为*交叉验证*，你已经在[第3章](kindle_split_013.html#ch03)中了解过它。
- en: 'There are multiple ways of splitting the dataset into training and test sets.
    Holdout cross-validation is the simplest, where a proportion of cases in the dataset
    are “held out” as the test set, and the model is trained on the remaining cases.
    Because the split is usually random, the outcome from holdout cross-validation
    depends heavily on the proportion of cases held out in the test set and on the
    cases that made it into the test set. As such, holdout cross-validation can give
    quite variable results when run multiple times, though it is the least computationally
    expensive method. I’ve reproduced [figure 3.12](kindle_split_013.html#ch03fig12)
    in [figure 20.3](#ch20fig03): it shows a schematic illustrating holdout cross-validation.'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和测试集有多种方法。保留法交叉验证是最简单的，其中数据集中的案例比例被“保留”为测试集，模型在剩余的案例上进行训练。由于分割通常是随机的，因此保留法交叉验证的结果高度依赖于测试集中保留的案例比例以及进入测试集的案例。因此，保留法交叉验证在多次运行时可能会给出相当可变的结果，尽管它是计算成本最低的方法。我在[图20.3](#ch20fig03)中重现了[图3.12](kindle_split_013.html#ch03fig12)：它展示了一个说明保留法交叉验证的示意图。
- en: Figure 20.3\. Holdout cross-validation. The data is randomly split into a training
    set and a test set. The training set is used to train the model, which is then
    used to make predictions on the test set. The similarity of the predictions to
    the true values of the test set is used to evaluate model performance.
  id: totrans-1123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图20.3\. 保留法交叉验证。数据被随机分为训练集和测试集。训练集用于训练模型，然后用于在测试集上做出预测。预测与测试集真实值之间的相似性用于评估模型性能。
- en: '![](fig20-3.jpg)'
  id: totrans-1124
  prefs: []
  type: TYPE_IMG
  zh: '![图20-3](fig20-3.jpg)'
- en: 'K-fold cross-validation randomly partitions the cases into *k* near-equally
    sized folds. For each fold, the cases inside the fold are used as the test set,
    while the remaining data is used as the training set. The mean performance metric
    of all the folds is then returned. The advantage of k-fold over holdout cross-validation
    is that because each case is used in the test set once, the results are less variable,
    although the results will be sensitive to our choice of the number of folds. To
    make the result even more stable, we can use repeated k-fold cross-validation,
    where the whole k-fold process is repeated multiple times, randomly shuffling
    the cases for each repetition. I’ve reproduced [figure 3.13](kindle_split_013.html#ch03fig13)
    in [figure 20.4](#ch20fig04): it illustrates k-fold cross-validation.'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证将案例随机划分为近等大小的*k*个折。对于每个折，折内的案例用作测试集，而剩余的数据用作训练集。然后返回所有折的平均性能指标。K折交叉验证相对于保留法交叉验证的优势在于，因为每个案例只被用作测试集一次，所以结果变化较小，尽管结果将对我们选择的折数敏感。为了使结果更加稳定，我们可以使用重复K折交叉验证，其中整个K折过程重复多次，每次重复随机打乱案例。我在[图20.4](#ch20fig04)中重现了[图3.13](kindle_split_013.html#ch03fig13)：它说明了K折交叉验证。
- en: Figure 20.4\. K-fold cross-validation. The data is randomly split into near
    equally sized folds. Each fold is used as the test set once, and the rest of the
    data is used as the training set. The similarity of the predictions to the true
    values of the test set is used to evaluate model performance.
  id: totrans-1126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图20.4\. K折交叉验证。数据被随机分为近等大小的折。每个折被用作测试集一次，其余数据用作训练集。预测与测试集真实值之间的相似性用于评估模型性能。
- en: '![](fig20-4.jpg)'
  id: totrans-1127
  prefs: []
  type: TYPE_IMG
  zh: '![图20-4](fig20-4.jpg)'
- en: Leave-one-out cross-validation is the extreme of k-fold cross-validation, where
    the number of folds is equal to the number of cases in the dataset. In this way,
    every case in the dataset is used as the test set once, with the model being trained
    using all of the other cases. Leave-one-out cross-validation tends to give more
    variable performance estimates than k-fold, except where the dataset is small,
    in which circumstance k-fold may give more variable estimates due to the small
    training set. I’ve reproduced [figure 3.14](kindle_split_013.html#ch03fig14) in
    [figure 20.5](#ch20fig05); it illustrates leave-one-out cross-validation.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 留一法交叉验证是k折交叉验证的极端情况，其中折数等于数据集中的案例数。这样，数据集中的每个案例都作为测试集使用一次，模型使用所有其他案例进行训练。留一法交叉验证通常比k折交叉验证给出更多变动的性能估计，除非数据集很小，在这种情况下，k折交叉验证可能由于训练集小而给出更多变动的估计。我在图20.5中重现了[图3.14](kindle_split_013.html#ch03fig14)；它说明了留一法交叉验证。
- en: One of the most common mistakes many people make when training machine learning
    models is not including their data-dependent preprocessing steps in their cross-validation
    procedure. If this preprocessing includes the tuning of any hyperparameters, it’s
    important to use nested cross-validation. Doing so ensures that the data we use
    for the final evaluation of the model hasn’t been seen by the model at all.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人在训练机器学习模型时犯的一个最常见的错误是没有将他们的数据依赖预处理步骤包含在交叉验证过程中。如果这个预处理包括任何超参数的调整，那么使用嵌套交叉验证是非常重要的。这样做可以确保我们用于模型最终评估的数据完全没有被模型看到过。
- en: Nested cross-validation starts by splitting the data into training and test
    sets (this can be done using the holdout, k-fold, or leave-one-out methods). This
    division is called the *outer loop*. The training set is used to cross-validate
    each value of our hyperparameter search space. This is called the *inner loop*.
    The hyperparameter that gives the best cross-validated performance from each inner
    loop is passed to the outer loop. A model is trained on each training set of the
    outer loop, using the best hyperparameter from its inner loop, and these models
    are used to make predictions on their test sets. The average performance metrics
    of these models across the outer loop are then reported as an estimate of how
    the model will perform on unseen data. I’ve reproduced [figure 3.16](kindle_split_013.html#ch03fig16)
    in [figure 20.6](#ch20fig06); it illustrates nested cross-validation. In this
    example, we are using 3-fold cross-validation for the outer loop, and 4-fold for
    the inner loop.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套交叉验证首先将数据分成训练集和测试集（可以使用保留法、k折法或留一法来完成）。这种划分称为*外层循环*。训练集用于交叉验证超参数搜索空间中的每个值。这称为*内层循环*。从每个内层循环中给出最佳交叉验证性能的超参数传递到外层循环。在外层循环的每个训练集上训练一个模型，使用其内层循环的最佳超参数，并使用这些模型在其测试集上进行预测。然后报告这些模型在外层循环中的平均性能指标，作为模型在未见数据上表现的一个估计。我在图20.6中重现了[图3.16](kindle_split_013.html#ch03fig16)；它说明了嵌套交叉验证。在这个例子中，我们使用3折交叉验证作为外层循环，4折作为内层循环。
- en: Figure 20.5\. Leave-one-out cross-validation is the extreme of k-fold, where
    we reserve a single case as the test set and train the model on the remaining
    data. The similarity of the predictions to the true values of the test set is
    used to evaluate model performance.
  id: totrans-1131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图20.5\. 留一法交叉验证是k折交叉验证的极端情况，其中我们保留一个案例作为测试集，并在剩余数据上训练模型。使用测试集的真实值来评估模型性能的相似性。
- en: '![](fig20-5_alt.jpg)'
  id: totrans-1132
  prefs: []
  type: TYPE_IMG
  zh: '![](fig20-5_alt.jpg)'
- en: Figure 20.6\. Nested cross-validation. The dataset is split into folds. For
    each fold, the training set is used to create sets of inner k-fold cross-validation.
    Each of these inner sets cross-validates a single hyperparameter value by splitting
    the data into training and test sets. For each fold in these inner sets, a model
    is trained using the training set and evaluated on the test set, using that set’s
    hyperparameter value. The hyperparameter from each inner cross-validation loop
    that gives the best-performing model is used to train the models on the outer
    loop.
  id: totrans-1133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图20.6\. 嵌套交叉验证。数据集被分成几个折。对于每个折，使用训练集创建内层k折交叉验证的集合。这些内层集合中的每一个通过将数据分成训练集和测试集来交叉验证单个超参数值。对于这些内层集合中的每个折，使用训练集训练一个模型，并在测试集上评估，使用该集合的超参数值。从每个内层交叉验证循环中给出最佳性能模型的超参数用于在外层循环中训练模型。
- en: '![](fig20-6.jpg)'
  id: totrans-1134
  prefs: []
  type: TYPE_IMG
  zh: '![](fig20-6.jpg)'
- en: '|  |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Training set, test set, and ... validation set?**'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练集、测试集和...验证集？**'
- en: You may see other people refer to splitting their data into a training set,
    a test set, and a *validation set*. I want to show you how this is just a special
    case of nested cross-validation. When using this approach, people train the model
    using the training set with a range of hyperparameter values and use the test
    set to evaluate the performance of these hyperparameter values. The model with
    the best-performing hyperparameter values is then given the validation set to
    make predictions on. The performance of the model on the validation set is used
    as the final indicator of the model-building process’s performance. The importance
    of this is that the validation set isn’t seen by the model during training, including
    during hyperparameter tuning, so there is no information leak for the model to
    learn patterns present in the validation set.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会看到其他人将他们的数据分为训练集、测试集和*验证集*。我想向你展示这只是一个嵌套交叉验证的特殊情况。当使用这种方法时，人们使用具有一系列超参数值的训练集来训练模型，并使用测试集来评估这些超参数值的性能。然后，具有最佳性能超参数值的模型被赋予验证集来进行预测。模型在验证集上的性能被用作模型构建过程性能的最终指标。这一点的重要性在于，验证集在训练过程中没有被模型看到，包括在超参数调整期间，因此模型不会泄露有关验证集中存在的模式的信息。
- en: Now look again at the schematic in [figure 20.6](#ch20fig06). Can you see that
    splitting the data into training, test, and validation sets is just nested cross-validation
    using holdout cross-validation for both the inner and outer loop? I explained
    this using the “nested” nomenclature because it gives us a much more flexible
    toolset for evaluating model performance than simply splitting the data into training,
    test, and validation sets. For example, it allows us to use more complex cross-validation
    strategies for our inner and outer loops, and even mix different strategies between
    them.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在再次看看[图20.6](#ch20fig06)中的示意图。你能看到将数据分为训练集、测试集和验证集只是嵌套交叉验证，其中内循环和外循环都使用保留交叉验证吗？我使用“嵌套”这个术语是因为它为我们提供了一个比仅仅将数据分为训练集、测试集和验证集更灵活的工具集来评估模型性能。例如，它允许我们在内循环和外循环中使用更复杂的交叉验证策略，甚至可以在它们之间混合不同的策略。
- en: '|  |'
  id: totrans-1139
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 20.1.4\. Maximizing model performance with hyperparameter tuning
  id: totrans-1140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.4. 使用超参数调整最大化模型性能
- en: Many machine learning algorithms have hyperparameters that control how they
    learn. A *hyperparameter* is a variable, setting, or option that cannot be estimated
    directly from the data itself. The best way to select the optimal combination
    of hyperparameters for any given algorithm and dataset is to use hyperparameter
    tuning.
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法都有控制它们如何学习的超参数。*超参数*是一个变量、设置或选项，不能直接从数据本身估计出来。为任何给定的算法和数据集选择最佳超参数组合的最佳方式是使用超参数调整。
- en: Hyperparameter *tuning* is the process of iteratively trying models with different
    combinations of hyperparameters, and selecting the combination that gives the
    best-performing model. The tuning process should accompany cross-validation, where,
    for each combination of hyperparameters, a model is trained on the training set
    and evaluated on the test set.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数*调整*是迭代尝试不同超参数组合的模型，并选择给出最佳性能模型的组合的过程。调整过程应伴随着交叉验证，其中，对于每个超参数组合，模型在训练集上训练并在测试集上评估。
- en: If the range of hyperparameter values we need to search over is small, then
    it is often beneficial to employ a grid search method. In grid search, we simply
    try every combination of hyperparameter values that we define in our search space.
    Grid search is the only search method that is guaranteed to select the best-performing
    combination of hyperparameters in our search space.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要搜索的超参数值范围较小，那么使用网格搜索方法通常是有益的。在网格搜索中，我们简单地尝试搜索空间中定义的每个超参数值的组合。网格搜索是唯一保证能够从我们的搜索空间中选择最佳超参数组合的搜索方法。
- en: But when dealing with multiple hyperparameters, or when the search space becomes
    very large, grid search can become prohibitively slow. In such situations, we
    can employ random search instead. Random search randomly samples combinations
    of hyperparameters from the search space, for as many iterations as we can afford.
    Random search is not guaranteed to find the best-performing combination of hyperparameters,
    but it can usually find a close approximation in a fraction of the time required
    by grid search.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当处理多个超参数，或者搜索空间变得非常大时，网格搜索可能会变得过于缓慢。在这种情况下，我们可以采用随机搜索。随机搜索从搜索空间中随机采样超参数的组合，进行尽可能多的迭代。随机搜索不能保证找到最佳性能的超参数组合，但它通常可以在网格搜索所需时间的一小部分内找到接近的近似值。
- en: Whichever search method we use, as a data-dependent preprocessing step, it is
    vital to include hyperparameter tuning in our cross-validation strategy, in the
    form of nested cross-validation.
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们使用哪种搜索方法，作为一个数据依赖的前处理步骤，将超参数调整纳入我们的交叉验证策略中，以嵌套交叉验证的形式，是至关重要的。
- en: 20.1.5\. Using missing value imputation to deal with missing data
  id: totrans-1146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.5\. 使用缺失值插补处理缺失数据
- en: '*Missing value imputation* is the practice of using sensible values to fill
    in missing data in a dataset, such that we can still train a model using the full
    dataset. The alternative would be to discard cases with any missing data.'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺失值插补* 是使用合理值填充数据集中缺失数据的一种实践，这样我们仍然可以使用完整的数据集来训练模型。另一种选择是丢弃任何缺失数据的案例。'
- en: A naive way to impute missing values is to simply replace missing values with
    the mean or median for a continuous variable (as we did in [chapter 4](kindle_split_014.html#ch04))
    or the mode for a categorical variable. The problem is that this approach adds
    bias to any models you train and throws away information about relationships in
    the data that may in fact have predictive value. A better approach, therefore,
    is to use another machine learning algorithm to estimate sensible values for the
    missing data, based on the values of the other variables for that case (as we
    did in [chapters 9](kindle_split_020.html#ch09) and [10](kindle_split_021.html#ch10)).
    For example, we could use the k-nearest neighbors algorithm to find what the cases
    most similar to the one in question have as their value for the missing value.
    As a data-dependent preprocessing step, missing value imputation should be included
    in the cross-validation process.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的插补缺失值的方法是将缺失值简单地替换为连续变量的均值或中位数（正如我们在 [第4章](kindle_split_014.html#ch04)
    中所做的那样）或分类变量的众数。问题是这种方法会给任何你训练的模型添加偏差，并丢弃关于数据中可能具有预测价值的关系的信息。因此，更好的方法是使用另一个机器学习算法来估计缺失数据的合理值，基于该案例其他变量的值（正如我们在
    [第9章](kindle_split_020.html#ch09) 和 [第10章](kindle_split_021.html#ch10) 中所做的那样）。例如，我们可以使用k最近邻算法找到与问题案例最相似的案例的缺失值。作为一个数据依赖的前处理步骤，缺失值插补应包含在交叉验证过程中。
- en: 20.1.6\. Feature engineering and feature selection
  id: totrans-1149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.6\. 特征工程和特征选择
- en: '*Feature engineering* is the practice of extracting useful/predictive information
    from existing variables when the data is currently in a less useful format. For
    example, this could involve extracting gender from transcribed medical notes,
    or combining various financial metrics to create an index of market stability.
    Feature engineering usually requires some domain knowledge and some thought about
    what features are likely to impact the outcome variable. We first covered feature
    engineering in [chapter 4](kindle_split_014.html#ch04), and we used it again in
    [chapter 10](kindle_split_021.html#ch10).'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征工程* 是从现有变量中提取有用/预测信息的一种实践，当数据目前处于不太有用的格式时。例如，这可能包括从转录的医疗记录中提取性别，或者将各种财务指标结合起来创建市场稳定性的指数。特征工程通常需要一些领域知识，以及对哪些特征可能影响结果变量的思考。我们首次在
    [第4章](kindle_split_014.html#ch04) 中介绍了特征工程，并在 [第10章](kindle_split_021.html#ch10)
    中再次使用它。'
- en: '*Feature selection*, on the other hand, is concerned with removing variables
    that contribute no or little predictive information to the model. In doing so,
    we can protect ourselves from both overfitting and the curse of dimensionality.
    You learned in [chapter 9](kindle_split_020.html#ch09) that feature selection
    can be done in two different ways: filter methods and wrapper methods.'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*特征选择* 关注的是移除对模型贡献很少或没有预测信息的变量。通过这样做，我们可以防止过拟合和维度诅咒。你曾在 [第9章](kindle_split_020.html#ch09)
    中了解到，特征选择可以以两种不同的方式进行：过滤方法和包装方法。
- en: Filter methods are computationally less expensive but are less likely to result
    in an optimal selection of features. They rely on calculating some metric of the
    relationship between each feature and the outcome variable. This metric could
    simply be the correlation between each feature and the outcome, for example. We
    can then skim off a specific number or proportion of features that have a weaker
    relationship with the outcome.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤方法在计算上成本较低，但不太可能得到特征选择的最佳结果。它们依赖于计算每个特征与结果变量之间关系的一些度量。这个度量可以简单地是每个特征与结果之间的相关性，例如。然后我们可以筛选掉与结果关系较弱的特定数量或比例的特征。
- en: Wrapper methods are more computationally expensive but are more likely to result
    in a better-fitting model. They consist of iteratively fitting and evaluating
    models with different permutations of the predictor variables. The combination
    of variables that gives the best-performing model is chosen.
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法在计算上成本较高，但更有可能得到拟合度更好的模型。它们包括迭代地拟合和评估不同排列的预测变量模型。选择给出最佳性能模型变量组合。
- en: Feature engineering and selection are extremely important—arguably more important
    than our choice of algorithm. We can use the most cutting-edge, high-performing
    algorithm ever developed, but if our features are not making the most of the predictive
    information they contain, or there are many irrelevant variables in the data,
    our model won’t perform as well as it should. If our feature engineering/selection
    processes are data dependent, it’s important to include them in cross-validation.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程和选择非常重要——可以说比我们选择算法更重要。我们可以使用最前沿、性能最高的算法，但如果我们的特征没有充分利用它们所包含的预测信息，或者数据中有许多无关变量，我们的模型的表现可能不会像预期的那样好。如果我们的特征工程/选择过程依赖于数据，那么在交叉验证中包含它们是很重要的。
- en: 20.1.7\. Improving model performance with ensemble techniques
  id: totrans-1155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.7\. 使用集成技术提高模型性能
- en: 'The performance of most supervised machine learning algorithms can be improved
    by combining them with an ensemble technique. *Ensembling* is where, instead of
    training a single model, we train multiple models that help us reduce overfitting
    and improve the accuracy of predictions. There are three types of ensemble techniques:'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数监督机器学习算法的性能可以通过结合集成技术来提高。*集成*是指，而不是训练单个模型，我们训练多个模型，这些模型帮助我们减少过拟合并提高预测的准确性。有三种类型的集成技术：
- en: Bagging
  id: totrans-1157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分袋法
- en: Boosting
  id: totrans-1158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升法
- en: Stacking
  id: totrans-1159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠
- en: You learned about ensemble techniques for classification and regression in [chapters
    8](kindle_split_018.html#ch08) and [12](kindle_split_023.html#ch12), respectively.
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 你在[第8章](kindle_split_018.html#ch08)和[第12章](kindle_split_023.html#ch12)中分别学习了分类和回归的集成技术。
- en: '*Bagging* (also called *bootstrap aggregating*) consists of creating multiple
    bootstrap samples from the original dataset and training a model on each sample
    in parallel. New data is then passed to each individual model, and the modal or
    mean prediction is returned (for classification and regression problems, respectively).
    Bagging helps us to avoid overfitting and so can reduce the variance of our models.
    Bagging can be used for virtually any supervised learning algorithm (and some
    clustering algorithms), but its most famous implementation is in the random forest
    algorithm, which uses classification/regression trees.'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: '*分袋法*（也称为*自助聚合*）包括从原始数据集中创建多个自助样本，并在每个样本上并行训练模型。然后，将新数据传递给每个单独的模型，并返回模型或平均预测（对于分类和回归问题分别）。分袋法帮助我们避免过拟合，因此可以减少我们模型的方差。分袋法几乎可以用于任何监督学习算法（以及一些聚类算法），但它在随机森林算法中最著名的实现，该算法使用分类/回归树。'
- en: While bagging trains models in parallel, *boosting* trains models sequentially,
    where each subsequent model seeks to improve on the mistakes of the existing chain
    of models. In adaptive boosting, cases that are incorrectly classified by the
    existing ensemble of models are weighted more heavily, such that they are more
    likely to be sampled in the next iteration. AdaBoost is the only well-known implementation
    of adaptive boosting. In gradient boosting, the residual error of the existing
    ensemble is minimized by each additional model. XGBoost is a famous implementation
    of gradient boosting that uses classification/regression trees; but just like
    bagging, boosting can be used with any supervised learning algorithm.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 当bagging并行训练模型时，*boosting*按顺序训练模型，其中每个后续模型都试图改进现有模型链中的错误。在自适应提升中，现有集成模型错误分类的案例被赋予更高的权重，这样它们在下一迭代中更有可能被采样。AdaBoost是自适应提升的唯一知名实现。在梯度提升中，每个额外模型通过最小化现有集成模型的残差误差。XGBoost是使用分类/回归树进行梯度提升的著名实现；但就像bagging一样，boosting可以与任何监督学习算法一起使用。
- en: In *stacking*, we create base models that are good at learning different patterns
    in the feature space. One model may then be good at predicting in one area of
    the feature space, but make mistakes in another area. One of the other models
    may do a good job of predicting values in an area of the feature space where the
    others do poorly. Predictions made by the base models are used as predictor variables
    (along with all the original predictors) by a final stacked model. This stacked
    model is then able to learn from the predictions made by the base models to make
    more accurate predictions of its own.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 在*堆叠*中，我们创建了一些基础模型，这些模型擅长学习特征空间中的不同模式。一个模型可能在特征空间的一个区域预测得很好，但在另一个区域犯错误。其他模型中的一个可能在做得不好的特征空间区域中预测值做得很好。基础模型做出的预测被用作预测变量（连同所有原始预测变量）由最终的堆叠模型使用。这个堆叠模型随后能够从基础模型的预测中学习，以做出更准确的预测。
- en: 20.1.8\. Preventing overfitting with regularization
  id: totrans-1164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.1.8\. 使用正则化防止过拟合
- en: '*Regularization* describes a set of techniques for limiting the magnitude of
    model parameters to prevent overfitting. Regularization is particularly important
    for guarding against overfitting due to the inclusion of predictors with little
    or no predictive value. You learned in [chapter 11](kindle_split_022.html#ch11)
    that the two most common forms are L2 and L1 regularization.'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化*描述了一组技术，用于限制模型参数的幅度，以防止过拟合。正则化对于防止由于包含预测值很小或没有预测价值的预测因子而导致的过拟合尤为重要。你在[第11章](kindle_split_022.html#ch11)中了解到，最常见的形式是L2和L1正则化。'
- en: In L2 regularization, the loss function of the model has a penalty added to
    it, which is the L2 norm of the model parameters weighted by a tunable hyperparameter,
    *lambda*. The L2 norm of the model parameters is the sum of squared parameter
    values. The effect of L2 regularization is that model parameters can be shrunk
    toward zero (but never to zero, unless the ordinary least squares [OLS] estimate
    is zero), with weaker predictors being penalized more greatly. Ridge regression
    is an example of L2 regularization being used to prevent overfitting in linear
    regression.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 在L2正则化中，模型的损失函数增加了一个惩罚项，即模型参数的L2范数，通过可调的超参数*lambda*加权。模型参数的L2范数是参数值的平方和。L2正则化的效果是模型参数可以被缩小到零（但永远不会缩小到零，除非普通最小二乘[OLS]估计为零），预测能力较弱的预测因子会受到更大的惩罚。岭回归是使用L2正则化来防止线性回归中过拟合的一个例子。
- en: In L1 regularization, we add the L1 norm to the loss function, weighted by *lambda*.
    The L1 norm is the sum of the absolute parameter values. The effect of L1 regularization
    is that model parameters can be shrunk to zero, effectively removing them from
    the model. L1 regularization is therefore a form of automatic feature selection.
    LASSO is an example of L1 regularization being used to prevent overfitting in
    linear regression.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 在L1正则化中，我们向损失函数中添加L1范数，通过*lambda*加权。L1范数是参数值的绝对值之和。L1正则化的效果是模型参数可以被缩小到零，从而有效地从模型中移除它们。因此，L1正则化是一种自动特征选择的形式。LASSO是使用L1正则化来防止线性回归中过拟合的一个例子。
- en: 20.2\. Where can you go from here?
  id: totrans-1168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.2\. 从这里你可以走向何方？
- en: You might be wondering what your next steps are in your machine learning education.
    That really is up to you and what you want to achieve, but in this section I’ll
    point you in the direction of some excellent resources you can use to further
    develop your knowledge and skills. I’m a firm believer, though, that the best
    way to solidify new knowledge is to use it—so use the techniques and algorithms
    you learned throughout this book in your work, and teach them to your colleagues!
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道你在机器学习教育中的下一步是什么。这完全取决于你想要实现的目标，但在本节中，我会为你指明一些你可以用来进一步发展你的知识和技能的优秀资源。然而，我坚信，巩固新知识最好的方式是使用它——所以，在你的工作中使用本书中学到的技术和算法，并将它们教授给你的同事！
- en: 20.2.1\. Deep learning
  id: totrans-1170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.1\. 深度学习
- en: As I mentioned near the start of the book, I omitted deep learning (machine
    learning using artificial neural networks) because I felt it deserves a book of
    its own. But no machine learning education could be considered comprehensive without
    learning about this extraordinary field. Neural networks are powerful tools for
    any machine learning task, but if your work will revolve around computer vision,
    the classification of images/video, or building models on other complex data such
    as audio files, deep learning is a vital avenue for you to explore. For R, I cannot
    recommend more highly *Deep Learning with R* by Francois Chollet and Joseph J.
    Allaire (Manning, 2018, [www.manning.com/books/deep-learning-with-r](http://www.manning.com/books/deep-learning-with-r)).
    This book is easily digestible by non-specialists and will reinforce some of the
    basic machine learning concepts we have covered here.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 如我在本书开头所提到的，我省略了深度学习（使用人工神经网络的机器学习），因为我认为它值得一本单独的书籍。但任何机器学习教育如果没有涉及这个非凡领域，都不能算是全面的。神经网络是任何机器学习任务的强大工具，但如果你的工作将围绕计算机视觉、图像/视频的分类，或构建其他复杂数据（如音频文件）的模型，那么深度学习是你必须探索的重要途径。对于R语言，我非常推荐Francois
    Chollet和Joseph J. Allaire的《用R进行深度学习》（Manning, 2018, [www.manning.com/books/deep-learning-with-r](http://www.manning.com/books/deep-learning-with-r)）。这本书对非专业人士来说很容易理解，并将加强我们在本节中介绍的一些基本机器学习概念。
- en: 20.2.2\. Reinforcement learning
  id: totrans-1172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.2\. 强化学习
- en: '*Reinforcement learning* is a cutting-edge area of machine learning research
    and application, where algorithms learn from experience by being rewarded when
    they make a good decision. Often considered alongside supervised and unsupervised
    algorithms to be the third class of machine learning algorithms, it has been used
    to create chess bots that can outwit world champion chess players. If you’re interested
    in reinforcement learning, I highly recommend *Deep Learning and the Game of Go*
    by Max Pumperla and Kevin Ferguson (Manning, 2019, [www.manning.com/books/deep-learning-and-the-game-of-go](http://www.manning.com/books/deep-learning-and-the-game-of-go)).'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是机器学习研究和应用的前沿领域，其中的算法通过在做出良好决策时获得奖励来从经验中学习。通常与监督学习和无监督学习算法并列为机器学习算法的第三类，它已被用于创建能够战胜世界冠军棋手的棋类机器人。如果你对强化学习感兴趣，我强烈推荐Max
    Pumperla和Kevin Ferguson的《深度学习与围棋》（Manning, 2019, [www.manning.com/books/deep-learning-and-the-game-of-go](http://www.manning.com/books/deep-learning-and-the-game-of-go)）。'
- en: 20.2.3\. General R data science and the tidyverse
  id: totrans-1174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.3\. 通用R数据科学和tidyverse
- en: If you want to improve your R data science skills in general, as well as become
    more proficient with tools from the tidyverse (including some we didn’t use),
    I recommend *R for Data Science* by Garrett Grolemund and Hadley Wickham (O’Reilly
    Media, 2016).
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要提高你的R数据科学技能，以及更熟练地使用tidyverse工具（包括我们未使用的一些工具），我推荐Garrett Grolemund和Hadley
    Wickham的《R数据科学》（O’Reilly Media, 2016）。
- en: If you want to become a ggplot2 master, then pick up a copy of *ggplot2* by
    Hadley Wickham (Springer International Publishing, 2016).
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要成为ggplot2的大师，那么请购买Hadley Wickham的《ggplot2》（Springer International Publishing,
    2016）。
- en: If your R skills are pretty good and you want to learn more about how the language
    works and how to do more advanced programming (such as object-oriented programming),
    you’ll enjoy *Advanced R* by Hadley Wickham (CRC Press, 2019). You may notice
    that this guy Hadley keeps popping up; if you want to keep up to date with the
    R community and developments to the tidyverse, you can do worse than to follow
    him.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经具备相当好的R技能，并想了解更多关于语言的工作原理以及如何进行更高级的编程（例如面向对象编程），你将喜欢Hadley Wickham的《高级R》（CRC
    Press, 2019）。你可能注意到这个人Hadley经常出现；如果你想跟上R社区和tidyverse的发展，跟随他是个不错的选择。
- en: 20.2.4\. mlr tutorial and creating new learners/metrics
  id: totrans-1178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.4\. mlr 教程和创建新的学习器/度量
- en: 'A few times in the book, I mentioned that a particular algorithm hadn’t yet
    been implemented in mlr. The mlr package is meant to make your machine learning
    experience more streamlined, not less flexible; so if you wish to implement an
    algorithm in another package (or your own) or a new performance metric, it really
    isn’t that hard to do so yourself. You can find a tutorial on how to do this (as
    well as other useful information and resources) on the mlr website: [http://mng.bz/5APD](http://mng.bz/5APD).'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 在书中，我提到过几次，某个特定的算法尚未在mlr中实现。mlr包旨在使你的机器学习体验更加流畅，而不是减少灵活性；所以，如果你希望在另一个包（或你自己的）或新的性能度量中实现算法，实际上自己来做并不难。你可以在mlr网站上找到如何做到这一点的教程（以及其他有用的信息和资源）：[http://mng.bz/5APD](http://mng.bz/5APD)。
- en: 20.2.5\. Generalized additive models
  id: totrans-1180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.5\. 广义加性模型
- en: 'If your work will involve modeling nonlinear relationships for regression tasks,
    I suggest you delve deeper into the inner workings of generalized additive models
    (GAMs). For R, a great place to do this is *Generalized Additive Models: An Introduction
    with R* by Simon Wood (Chapman and Hall/CRC, 2017).'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的工作将涉及回归任务中的非线性关系建模，我建议你深入了解广义加性模型（GAMs）的内部工作原理。对于R语言，一个很好的地方是西蒙·伍德的《广义加性模型：R语言导论》（Chapman
    and Hall/CRC，2017）。
- en: 20.2.6\. Ensemble methods
  id: totrans-1182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.6\. 集成方法
- en: 'Ensemble methods got you excited? We only touched the surface in this book,
    using ensembles for tree-based models. If you are convinced that ensembling can
    almost always make models better, I suggest you dip into *Ensemble Methods: Foundations
    and Algorithms* by Zhi-Hua Zhou (Chapman and Hall/CRC, 2012).'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 对集成方法感到兴奋吗？在这本书中，我们只是触及了表面，使用了基于树的模型进行集成。如果你确信集成几乎总是可以使模型变得更好，我建议你阅读周志华的《集成方法：基础与算法》（Chapman
    and Hall/CRC，2012）。
- en: 20.2.7\. Support vector machines
  id: totrans-1184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.7\. 支持向量机
- en: Excited about how support vector machines (SVMs) can contort the feature space
    to create linear boundaries? SVMs are very popular, and their theory is quite
    complex. To learn more about how you can harness their predictive power, I recommend
    *Support Vector Machines* by Andreas Christmann and Ingo Steinwart (Springer,
    2008).
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 对支持向量机（SVMs）如何扭曲特征空间以创建线性边界感到兴奋吗？SVMs非常受欢迎，其理论相当复杂。为了了解更多关于如何利用它们的预测能力，我推荐安德烈亚斯·克里斯曼和英戈·斯坦瓦特的《支持向量机》（Springer，2008）。
- en: 20.2.8\. Anomaly detection
  id: totrans-1186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.8\. 异常检测
- en: Sometimes you’re not interested in the common patterns in your data. Sometimes
    it’s the unusual, outlying cases that you’re really interested in. For example,
    you may be trying to identify fraudulent activity on a credit card, or trying
    to identify rare bursts of radiation from stars. Identifying such rare events
    in a dataset can be challenging, but an area of machine learning called *anomaly
    detection* is dedicated to solving these problems. Some of the algorithms you
    met in this book can be repurposed for anomaly detection, such as the SVM algorithm.
    If you have a penchant for the rare and unusual, take a look at *Anomaly Detection
    Principles and Algorithms* by Kishan G. Mehrotra, Chilukuri K. Mohan, and HuaMing
    Huang (Springer, 2017).
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你可能对数据中的常见模式不感兴趣。有时候，你真正感兴趣的是那些不寻常的、异常的案例。例如，你可能正在尝试识别信用卡上的欺诈活动，或者尝试识别来自恒星的罕见辐射爆发。在数据集中识别这样的罕见事件可能具有挑战性，但机器学习中的一个领域称为*异常检测*正是致力于解决这些问题。这本书中你遇到的一些算法可以重新用于异常检测，例如SVM算法。如果你对罕见和异常的事物有偏好，可以看看基山·G·梅赫罗特拉、奇卢库里·K·莫汉和华明·黄合著的《异常检测原理与算法》（Springer，2017）。
- en: 20.2.9\. Time series
  id: totrans-1188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.9\. 时间序列
- en: Something I didn’t touch on in this book is time series forecasting. This is
    the area of machine learning and statistics concerned with predicting the future
    state of a variable, based on its previous states. Common applications of time
    series forecasting are predicting fluctuations in stock market variables and forecasting
    weather patterns. If you want to get rich or stay dry, I would start with *Introductory
    Time Series with R* by Paul Cowpertwait and Andrew Metcalfe (Springer, 2009).
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 本书没有涉及的是时间序列预测。这是机器学习和统计学中关注基于变量的先前状态预测其未来状态的领域。时间序列预测的常见应用包括预测股票市场变量的波动和预测天气模式。如果你想致富或保持干燥，我会从保罗·科珀特韦特和安德鲁·梅特卡夫合著的《R语言时间序列导论》（Springer，2009）开始。
- en: 20.2.10\. Clustering
  id: totrans-1190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.10\. 聚类
- en: 'We’ve covered pretty good ground when it comes to clustering, but there is
    much more for you to get your teeth into. To learn more, I recommend *Data Clustering:
    Algorithms and Applications* by Charu Aggarwal (Chapman & Hall/CRC, 2013).'
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类方面，我们已经覆盖了相当多的内容，但还有更多内容等待您去深入研究。要了解更多信息，我推荐Charu Aggarwal的《数据聚类：算法与应用》（Chapman
    & Hall/CRC，2013年）。
- en: 20.2.11\. Generalized linear models
  id: totrans-1192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.11. 广义线性模型
- en: Impressed at how the general linear model can be extended to predict classes
    as we did in logistic regression? We can use the same principal to predict count
    data (as in Poisson regression) or percentages (as in beta regression). The extended
    form of the general linear model to handle situations where our outcome is not
    a normally distributed continuous variable is called the generalized linear model.
    It gives us extraordinary flexibility when building predictive models, while still
    allowing complete interpretability of the model parameters. To learn more, I recommend
    *Generalized Linear Models With Examples in R* by Peter K. Dunn and Gordon K.
    Smyth (Springer, 2018), though you may find it a tough read if you don’t already
    have a good mathematical grounding in linear modeling.
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 对广义线性模型能够扩展到预测类别，就像我们在逻辑回归中所做的那样感到印象深刻吗？我们可以使用同样的原理来预测计数数据（如泊松回归）或百分比（如贝塔回归）。当我们的结果不是一个正态分布的连续变量时，广义线性模型是处理这种情况的扩展形式。它在构建预测模型时提供了非凡的灵活性，同时仍然允许完全解释模型参数。要了解更多信息，我推荐Peter
    K. Dunn和Gordon K. Smyth的《广义线性模型及其在R中的应用》（Springer，2018年），尽管如果您在线性建模方面没有良好的数学基础，可能会觉得这本书很难读。
- en: 20.2.12\. Semi-supervised learning
  id: totrans-1194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.12. 半监督学习
- en: If you have the common problem of data that is time-consuming and/or costly
    to label manually, you can probably benefit from the application of semi-supervised
    learning. To learn more, I recommend *Semi-Supervised Learning* by Olivier Chapelle,
    Bernhard Scholkopf, and Alexander Zien (MIT Press, 2006).
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有手动标记数据既耗时又昂贵的问题，您可能可以从半监督学习的应用中受益。要了解更多信息，我推荐Olivier Chapelle、Bernhard Scholkopf和Alexander
    Zien的《半监督学习》（MIT Press，2006年）。
- en: 20.2.13\. Modeling spectral data
  id: totrans-1196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20.2.13. 光谱数据建模
- en: If you’re going to be working with spectral data, or data that can be represented
    by smooth functions, you’ll want a good grounding in functional data analysis
    (briefly mentioned in [chapter 10](kindle_split_021.html#ch10)). Functional data
    analysis is where we use functions as variables in our models, rather than individual
    values. To learn more, I recommend *Functional Data Analysis* by James Ramsay
    (Springer, 2005).
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将要处理光谱数据，或者可以用平滑函数表示的数据，您将需要具备功能数据分析的良好基础（在第10章中简要提及）。功能数据分析是我们将函数作为模型中的变量使用，而不是使用单个值。要了解更多信息，我推荐James
    Ramsay的《功能数据分析》（Springer，2005年）。
- en: 20.3\. The last word
  id: totrans-1198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.3. 最后一个词
- en: I really hope that the skills you’ve acquired through reading this book will
    help you gain insight into a part of nature you’re studying, help you streamline
    and improve your business practices, or just help you get more from your hobby
    data science projects. I also hope that the tidyverse skills we used throughout
    the book will help you to write easier, more readable code, and that your new
    mlr skills will continue to make your machine learning projects simpler.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 我真心希望，通过阅读这本书，您所获得的技术能够帮助您深入理解您正在研究的那部分自然，帮助您优化和改进您的商业实践，或者只是帮助您从您的数据科学项目爱好中获得更多。我还希望，书中使用的tidyverse技能能够帮助您编写更简单、更易读的代码，而您的新mlr技能将继续使您的机器学习项目变得更加简单。
- en: Thank you for reading!
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您的阅读！
