- en: Part 2 Patterns of distributed machine learning systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2部分 分布式机器学习系统模式
- en: Now that you know the basic concepts and background of distributed machine learning
    systems, you should be able to proceed to this part of the book. We will explore
    some of the challenges involved in various components of a machine learning system
    and introduce a few established patterns adopted heavily in industries to address
    those challenges.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了分布式机器学习系统的基础概念和背景，你应该能够继续阅读本书的这一部分。我们将探讨机器学习系统各个组件中涉及的一些挑战，并介绍一些在工业界广泛采用的成熟模式来解决这些挑战。
- en: Chapter 2 introduces the batching pattern, used to handle and prepare large
    datasets for model training; the sharding pattern, used to split huge datasets
    into multiple data shards that spread among multiple worker machines; and the
    caching pattern, which could greatly speed the data ingestion process when a previously
    used dataset is re-accessed for model training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章介绍了批处理模式，用于处理和准备大型数据集以进行模型训练；分片模式，用于将大型数据集分割成多个数据分片，这些分片分散在多个工作机器上；以及缓存模式，当重新访问先前使用的数据集进行模型训练时，它可以极大地加速数据摄取过程。
- en: In chapter 3, we will explore the challenges of the distributed model training
    process. We’ll cover the challenges of training large machine learning models
    that tag main themes in new YouTube videos but cannot fit on a single machine.
    The chapter also covers how to overcome the difficulty of using the parameter
    server pattern. In addition, we see how to use the collective communication pattern
    to speed distributed training for smaller models and avoid unnecessary communication
    overhead among parameter servers and workers. At the end of this chapter, we talk
    about some of the vulnerabilities of distributed machine learning systems due
    to corrupted datasets, unstable networks, and preemptive worker machines, and
    we see how we can address those issues.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们将探讨分布式模型训练过程中的挑战。我们将涵盖训练大型机器学习模型（这些模型标记了新YouTube视频中的主要主题，但无法在一个单独的机器上运行）的挑战。本章还将介绍如何克服使用参数服务器模式的困难。此外，我们将了解如何使用集体通信模式来加速较小模型的分布式训练，并避免参数服务器和工作节点之间不必要的通信开销。在本章的结尾，我们将讨论由于数据集损坏、网络不稳定和抢占式工作机器导致的分布式机器学习系统的某些漏洞，并探讨我们如何解决这些问题。
- en: Chapter 4 focuses on the model serving component, which needs to be scalable
    and reliable to handle the growing number of user requests and the growing size
    of individual requests. We will go through the tradeoffs of making design decisions
    to build a distributed model serving system. We will use the replicated services
    to handle the growing number of model serving requests. We will also learn how
    to assess model serving systems and determine whether the event-driven design
    would be beneficial in real-world scenarios.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章重点介绍模型服务组件，它需要具备可扩展性和可靠性，以处理不断增长的用户请求数量和单个请求的大小。我们将讨论在设计决策中构建分布式模型服务系统的权衡。我们将使用复制服务来处理不断增长的模型服务请求。我们还将学习如何评估模型服务系统，并确定在现实场景中事件驱动设计是否会带来益处。
- en: In chapter 5, we’ll see how to build a system that executes complex machine
    learning workflows to train multiple machine learning models and pick the most
    performant models to provide good entity tagging results in the model serving
    system, using the fan-in and fan-out patterns. We’ll also incorporate the synchronous
    and asynchronous patterns to make machine learning workflows more efficient and
    avoid delays due to the long-running model training steps that block consecutive
    steps.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们将了解如何构建一个系统，该系统执行复杂的机器学习工作流程，以训练多个机器学习模型，并使用扇入和扇出模式选择性能最佳的模型，以在模型服务系统中提供良好的实体标记结果。我们还将结合同步和异步模式，使机器学习工作流程更加高效，并避免由于长时间运行的模型训练步骤而导致的延迟。
- en: Chapter 6, the last chapter in this part of the book, covers some operational
    efforts and patterns that can greatly accelerate the end-to-end workflow, as well
    as reduce the maintenance and communication efforts that arise when engineering
    and data science teams collaborate. We’ll introduce a couple of scheduling techniques
    that prevent resource starvation and deadlocks when many team members work in
    the same cluster with limited computational resources. We will also discuss the
    benefits of the metadata pattern, which we could use to gain insights from the
    individual steps in machine learning workflows and handle failures more appropriately
    to reduce the negative effect on users.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章，本书这一部分的最后一章，涵盖了可以极大地加速端到端工作流程的一些操作努力和模式，以及当工程和数据分析团队协作时产生的维护和沟通努力。我们将介绍几种调度技术，以防止在许多团队成员在有限的计算资源相同的集群中工作时出现资源饥饿和死锁。我们还将讨论元数据模式的益处，我们可以利用它从机器学习工作流程的各个步骤中获得见解，并更恰当地处理故障，以减少对用户的不利影响。
