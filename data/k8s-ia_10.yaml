- en: 'Chapter 9\. Deployments: updating applications declaratively'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章\. 部署：声明式更新应用程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Replacing pods with newer versions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用新版本替换Pod
- en: Updating managed pods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新托管Pod
- en: Updating pods declaratively using Deployment resources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Deployment资源声明式地更新Pod
- en: Performing rolling updates
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行滚动更新
- en: Automatically blocking rollouts of bad versions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动阻止坏版本的发布
- en: Controlling the rate of the rollout
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制滚动发布的速率
- en: Reverting pods to a previous version
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Pod回滚到之前的版本
- en: You now know how to package your app components into containers, group them
    into pods, provide them with temporary or permanent storage, pass both secret
    and non-secret config data to them, and allow pods to find and talk to each other.
    You know how to run a full-fledged system composed of independently running smaller
    components—microservices, if you will. Is there anything else?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经知道如何将你的应用程序组件打包到容器中，将它们分组到Pod中，为它们提供临时或永久存储，将秘密和非秘密配置数据传递给它们，并允许Pod相互发现和通信。你知道如何运行由独立运行的小组件组成的完整系统——如果你愿意，可以称之为微服务。还有其他什么吗？
- en: Eventually, you’re going to want to update your app. This chapter covers how
    to update apps running in a Kubernetes cluster and how Kubernetes helps you move
    toward a true zero-downtime update process. Although this can be achieved using
    only ReplicationControllers or ReplicaSets, Kubernetes also provides a Deployment
    resource that sits on top of ReplicaSets and enables declarative application updates.
    If you’re not completely sure what that means, keep reading—it’s not as complicated
    as it sounds.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你将想要更新你的应用程序。本章介绍了如何在Kubernetes集群中更新应用程序以及Kubernetes如何帮助你向真正的零停机更新过程迈进。虽然这可以通过仅使用ReplicationControllers或ReplicaSets来实现，但Kubernetes还提供了一个位于ReplicaSets之上的Deployment资源，它允许声明式应用程序更新。如果你对此完全不确定，请继续阅读——它并不像听起来那么复杂。
- en: 9.1\. Updating applications running in pods
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 9.1\. 更新在Pod中运行的应用程序
- en: Let’s start off with a simple example. Imagine having a set of pod instances
    providing a service to other pods and/or external clients. After reading this
    book up to this point, you likely recognize that these pods are backed by a ReplicationController
    or a ReplicaSet. A Service also exists through which clients (apps running in
    other pods or external clients) access the pods. This is how a basic application
    looks in Kubernetes (shown in [figure 9.1](#filepos867836)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的例子开始。想象有一组Pod实例为其他Pod和/或外部客户端提供服务。在阅读这本书的这一部分之后，你可能会认识到这些Pod由ReplicationController或ReplicaSet支持。也存在一个Service，客户端（在另一个Pod中运行的或外部客户端）通过它访问Pod。这就是在Kubernetes中基本应用程序的外观（如图9.1所示[figure
    9.1](#filepos867836)）。
- en: Figure 9.1\. The basic outline of an application running in Kubernetes
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1\. 在Kubernetes中运行的应用程序的基本结构
- en: '![](images/00103.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00103.jpg)'
- en: Initially, the pods run the first version of your application—let’s suppose
    its image is tagged as `v1`. You then develop a newer version of the app and push
    it to an image repository as a new image, tagged as `v2`. You’d next like to replace
    all the pods with this new version. Because you can’t change an existing pod’s
    image after the pod is created, you need to remove the old pods and replace them
    with new ones running the new image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，Pod运行你的应用程序的第一个版本——假设其镜像被标记为`v1`。然后你开发应用程序的新版本并将其作为新镜像推送到镜像仓库，标记为`v2`。接下来，你希望用这个新版本替换所有Pod。因为Pod创建后不能更改现有Pod的镜像，所以你需要删除旧Pod并用运行新镜像的新Pod替换它们。
- en: 'You have two ways of updating all those pods. You can do one of the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你有两种方式来更新所有这些Pod。你可以执行以下操作之一：
- en: Delete all existing pods first and then start the new ones.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先删除所有现有Pod，然后启动新的。
- en: Start new ones and, once they’re up, delete the old ones. You can do this either
    by adding all the new pods and then deleting all the old ones at once, or sequentially,
    by adding new pods and removing old ones gradually.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始新的，一旦它们启动，就删除旧的。你可以通过一次性添加所有新Pod并删除所有旧Pod，或者通过逐步添加新Pod和移除旧Pod来实现。
- en: Both these strategies have their benefits and drawbacks. The first option would
    lead to a short period of time when your application is unavailable. The second
    option requires your app to handle running two versions of the app at the same
    time. If your app stores data in a data store, the new version shouldn’t modify
    the data schema or the data in such a way that breaks the previous version.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种策略都有其优点和缺点。第一种选择会导致你的应用程序在短时间内不可用。第二种选择要求你的应用程序同时运行两个版本的程序。如果你的应用程序在数据存储中存储数据，新版本不应该修改数据模式或以破坏旧版本的方式修改数据。
- en: How do you perform these two update methods in Kubernetes? First, let’s look
    at how to do this manually; then, once you know what’s involved in the process,
    you’ll learn how to have Kubernetes perform the update automatically.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何在Kubernetes中执行这两种更新方法？首先，让我们看看如何手动执行；然后，一旦你知道这个过程涉及的内容，你将学习如何让Kubernetes自动执行更新。
- en: 9.1.1\. Deleting old pods and replacing them with new ones
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 9.1.1\. 删除旧Pods并用新Pods替换
- en: You already know how to get a ReplicationController to replace all its pod instances
    with pods running a new version. You probably remember the pod template of a ReplicationController
    can be updated at any time. When the ReplicationController creates new instances,
    it uses the updated pod template to create them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道如何让ReplicationController用运行新版本的Pods替换所有Pod实例。你可能记得ReplicationController的Pod模板可以在任何时候更新。当ReplicationController创建新实例时，它使用更新的Pod模板来创建它们。
- en: If you have a ReplicationController managing a set of `v1` pods, you can easily
    replace them by modifying the pod template so it refers to version `v2` of the
    image and then deleting the old pod instances. The ReplicationController will
    notice that no pods match its label selector and it will spin up new instances.
    The whole process is shown in [figure 9.2](#filepos870673).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个管理一组`v1` Pods的ReplicationController，你可以通过修改Pod模板来轻松替换它们，使其引用图像的`v2`版本，然后删除旧的Pod实例。ReplicationController会注意到没有Pod匹配其标签选择器，然后它会启动新实例。整个过程如图9.2所示。
- en: Figure 9.2\. Updating pods by changing a ReplicationController’s pod template
    and deleting old Pods
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2\. 通过更改ReplicationController的Pod模板和删除旧Pods来更新Pods
- en: '![](images/00122.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00122.jpg)'
- en: This is the simplest way to update a set of pods, if you can accept the short
    downtime between the time the old pods are deleted and new ones are started.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你可以接受在删除旧Pods和新Pods启动之间的短暂停机时间，这是更新一组Pods的最简单方法。
- en: 9.1.2\. Spinning up new pods and then deleting the old ones
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 9.1.2\. 启动新Pods然后删除旧Pods
- en: If you don’t want to see any downtime and your app supports running multiple
    versions at once, you can turn the process around and first spin up all the new
    pods and only then delete the old ones. This will require more hardware resources,
    because you’ll have double the number of pods running at the same time for a short
    while.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想看到任何停机时间，并且你的应用程序支持同时运行多个版本，你可以反转这个过程，首先启动所有新的Pods，然后才删除旧的Pods。这将需要更多的硬件资源，因为在一小段时间内，你将有两倍数量的Pods同时运行。
- en: This is a slightly more complex method compared to the previous one, but you
    should be able to do it by combining what you’ve learned about ReplicationControllers
    and Services so far.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法相比，这是一种稍微复杂的方法，但你应该能够通过结合到目前为止你学到的关于ReplicationControllers和Services的知识来完成它。
- en: Switching from the old to the new version at once
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性从旧版本切换到新版本
- en: Pods are usually fronted by a Service. It’s possible to have the Service front
    only the initial version of the pods while you bring up the pods running the new
    version. Then, once all the new pods are up, you can change the Service’s label
    selector and have the Service switch over to the new pods, as shown in [figure
    9.3](#filepos872883). This is called a blue-green deployment. After switching
    over, and once you’re sure the new version functions correctly, you’re free to
    delete the old pods by deleting the old ReplicationController.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Pods通常由Service作为前端。在你启动运行新版本的Pods时，Service可以仅作为Pods的初始版本的代理。一旦所有新的Pods都启动，你可以更改Service的标签选择器，让Service切换到新的Pods，如图9.3所示。这被称为蓝绿部署。切换后，一旦你确定新版本功能正常，你可以自由地通过删除旧的ReplicationController来删除旧Pods。
- en: '|  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can change a Service’s pod selector with the `kubectl set selector` command.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kubectl set selector`命令更改Service的Pod选择器。
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Figure 9.3\. Switching a Service from the old pods to the new ones
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3\. 将Service从旧Pods切换到新Pods
- en: '![](images/00139.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00139.jpg)'
- en: Performing a rolling update
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 执行滚动更新
- en: Instead of bringing up all the new pods and deleting the old pods at once, you
    can also perform a rolling update, which replaces pods step by step. You do this
    by slowly scaling down the previous ReplicationController and scaling up the new
    one. In this case, you’ll want the Service’s pod selector to include both the
    old and the new pods, so it directs requests toward both sets of pods. See [figure
    9.4](#filepos873676).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了同时启动所有新 pod 并删除旧 pod 之外，你也可以执行滚动更新，逐步替换 pod。你可以通过逐渐减少上一个 ReplicationController
    的规模并增加新的 ReplicationController 的规模来实现这一点。在这种情况下，你希望服务的 pod 选择器包括旧的和新的 pod，以便将请求指向这两组
    pod。参见[图 9.4](#filepos873676)。
- en: Figure 9.4\. A rolling update of pods using two ReplicationControllers
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4\. 使用两个 ReplicationController 对 pod 执行滚动更新
- en: '![](images/00157.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00157.jpg)'
- en: Doing a rolling update manually is laborious and error-prone. Depending on the
    number of replicas, you’d need to run a dozen or more commands in the proper order
    to perform the update process. Luckily, Kubernetes allows you to perform the rolling
    update with a single command. You’ll learn how in the next section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 手动执行滚动更新既费时又容易出错。根据副本数量的不同，你可能需要按正确的顺序运行一打或更多的命令来执行更新过程。幸运的是，Kubernetes 允许你使用单个命令执行滚动更新。你将在下一节中学习如何操作。
- en: 9.2\. Performing an automatic rolling update with a ReplicationController
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 9.2\. 使用 ReplicationController 执行自动滚动更新
- en: Instead of performing rolling updates using ReplicationControllers manually,
    you can have `kubectl` perform them. Using `kubectl` to perform the update makes
    the process much easier, but, as you’ll see later, this is now an outdated way
    of updating apps. Nevertheless, we’ll walk through this option first, because
    it was historically the first way of doing an automatic rolling update, and also
    allows us to discuss the process without introducing too many additional concepts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以不使用 ReplicationController 手动执行滚动更新，而是让 `kubectl` 来执行。使用 `kubectl` 执行更新使过程变得容易得多，但正如你将在后面看到的，这现在是一种过时的更新应用的方式。尽管如此，我们首先会介绍这个选项，因为它在历史上是执行自动滚动更新的第一种方式，同时也允许我们讨论这个过程而不引入太多额外的概念。
- en: 9.2.1\. Running the initial version of the app
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 9.2.1\. 运行应用的初始版本
- en: Obviously, before you can update an app, you need to have an app deployed. You’re
    going to use a slightly modified version of the kubia NodeJS app you created in
    [chapter 2](index_split_022.html#filepos185841) as your initial version. In case
    you don’t remember what it does, it’s a simple web-app that returns the pod’s
    hostname in the HTTP response.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在更新应用之前，你需要有一个应用部署。你将使用你在[第 2 章](index_split_022.html#filepos185841)中创建的稍作修改的
    kubia NodeJS 应用作为你的初始版本。如果你不记得它做什么，它是一个简单的 web 应用，在 HTTP 响应中返回 pod 的主机名。
- en: Creating the v1 app
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 v1 应用
- en: You’ll change the app so it also returns its version number in the response,
    which will allow you to distinguish between the different versions you’re about
    to build. I’ve already built and pushed the app image to Docker Hub under `luksa/kubia:v1`.
    The following listing shows the app’s code.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你将修改应用，使其在响应中也返回其版本号，这将允许你区分你即将构建的不同版本。我已经将应用镜像构建并推送到 Docker Hub，名称为 `luksa/kubia:v1`。下面的列表显示了应用的代码。
- en: 'Listing 9.1\. The `v1` version of our app: v1/app.js'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1\. 我们应用的 `v1` 版本：v1/app.js
- en: '`const http = require(''http''); const os = require(''os'');  console.log("Kubia
    server starting..."); var handler = function(request, response) {   console.log("Received
    request from " + request.connection.remoteAddress);   response.writeHead(200);
      response.end("This is v1 running in pod " + os.hostname() + "\n"); }; var www
    = http.createServer(handler); www.listen(8080);`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`const http = require(''http''); const os = require(''os''); console.log("Kubia
    服务器启动..."); var handler = function(request, response) { console.log("收到来自 " +
    request.connection.remoteAddress + " 的请求"); response.writeHead(200); response.end("这是运行在
    pod " + os.hostname() + " 上的 v1\n"); }; var www = http.createServer(handler);
    www.listen(8080);`'
- en: Running the app and exposing it through a service using a single YAML file
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个 YAML 文件运行应用并通过服务公开
- en: To run your app, you’ll create a ReplicationController and a `LoadBalancer`
    Service to enable you to access the app externally. This time, rather than create
    these two resources separately, you’ll create a single YAML for both of them and
    post it to the Kubernetes API with a single `kubectl create` command. A YAML manifest
    can contain multiple objects delimited with a line containing three dashes, as
    shown in the following listing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行你的应用程序，你将创建一个 ReplicationController 和一个 `LoadBalancer` Service，以便你可以从外部访问应用程序。这次，你将创建一个包含这两个资源的单个
    YAML 文件，并使用单个 `kubectl create` 命令将其发布到 Kubernetes API。YAML 清单可以包含多个对象，这些对象由包含三个短横线的行分隔，如下所示。
- en: 'Listing 9.2\. A YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.2\. 包含 RC 和 Service 的 YAML：kubia-rc-and-service-v1.yaml
- en: '`apiVersion: v1 kind: ReplicationController metadata:   name: kubia-v1 spec:
      replicas: 3   template:     metadata:       name: kubia       labels:` `1` `app:
    kubia` `1` `spec:       containers:       - image: luksa/kubia:v1` `2` `name:
    nodejs ---` `3` `apiVersion: v1 kind: Service metadata:   name: kubia spec:  
    type: LoadBalancer   selector:` `1` `app: kubia` `1` `ports:   - port: 80    
    targetPort: 8080`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ReplicationController metadata:   name: kubia-v1 spec:
      replicas: 3   template:     metadata:       name: kubia       labels:` `1` `app:
    kubia` `1` `spec:       containers:       - image: luksa/kubia:v1` `2` `name:
    nodejs ---` `3` `apiVersion: v1 kind: Service metadata:   name: kubia spec:  
    type: LoadBalancer   selector:` `1` `app: kubia` `1` `ports:   - port: 80    
    targetPort: 8080`'
- en: 1 The Service fronts all pods created by the ReplicationController.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 该 Service 前端代理了由 ReplicationController 创建的所有 pods。
- en: 2 You’re creating a ReplicationController for pods running this image.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 你正在为运行此镜像的 pods 创建一个 ReplicationController。
- en: 3 YAML files can contain multiple resource definitions separated by a line with
    three dashes.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 YAML 文件可以包含多个资源定义，这些定义由包含三个短横线的行分隔。
- en: The YAML defines a ReplicationController called `kubia-v1` and a Service called
    `kubia`. Go ahead and post the YAML to Kubernetes. After a while, your three `v1`
    pods and the load balancer should all be running, so you can look up the Service’s
    external IP and start hitting the service with `curl`, as shown in the following
    listing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 定义了一个名为 `kubia-v1` 的 ReplicationController 和一个名为 `kubia` 的 Service。将 YAML
    发布到 Kubernetes。过了一会儿，你的三个 `v1` pods 和负载均衡器都应该在运行，这样你就可以查找 Service 的外部 IP 并使用 `curl`
    开始访问服务，如下所示。
- en: Listing 9.3\. Getting the Service’s external IP and hitting the service in a
    loop with `curl`
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.3\. 获取 Service 的外部 IP 并使用 `curl` 在循环中访问服务
- en: '`$ kubectl get svc kubia` `NAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)        
    AGE kubia     10.3.246.195` `130.211.109.222``80:32143/TCP    5m` `$ while true;
    do curl http://130.211.109.222; done` `This is v1 running in pod kubia-v1-qr192
    This is v1 running in pod kubia-v1-kbtsk This is v1 running in pod kubia-v1-qr192
    This is v1 running in pod kubia-v1-2321o ...`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get svc kubia` `NAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)        
    AGE kubia     10.3.246.195` `130.211.109.222``80:32143/TCP    5m` `$ while true;
    do curl http://130.211.109.222; done` `这是在 pod kubia-v1-qr192 中运行的 v1` `这是在 pod
    kubia-v1-kbtsk 中运行的 v1` `这是在 pod kubia-v1-qr192 中运行的 v1` `这是在 pod kubia-v1-2321o
    中运行的 v1` ...'
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re using Minikube or any other Kubernetes cluster where load balancer
    services aren’t supported, you can use the Service’s node port to access the app.
    This was explained in [chapter 5](index_split_046.html#filepos469093).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Minikube 或任何不支持负载均衡器服务的其他 Kubernetes 集群，你可以使用 Service 的节点端口来访问应用程序。这已在[第
    5 章](index_split_046.html#filepos469093)中解释。
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 9.2.2\. Performing a rolling update with kubectl
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 9.2.2\. 使用 kubectl 执行滚动更新
- en: 'Next you’ll create version 2 of the app. To keep things simple, all you’ll
    do is change the response to say, “This is v2”:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将创建应用程序的第二个版本。为了保持简单，你只需更改响应内容为“这是 v2”：
- en: '`  response.end("This is v2 running in pod " + os.hostname() + "\n");`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`  response.end("This is v2 running in pod " + os.hostname() + "\n");`'
- en: This new version is available in the image `luksa/kubia:v2` on Docker Hub, so
    you don’t need to build it yourself.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新版本在 Docker Hub 上的镜像 `luksa/kubia:v2` 中可用，因此你不需要自己构建它。
- en: '|  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Pushing updates to the same image tag
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 向相同的镜像标签推送更新
- en: Modifying an app and pushing the changes to the same image tag isn’t a good
    idea, but we all tend to do that during development. If you’re modifying the `latest`
    tag, that’s not a problem, but when you’re tagging an image with a different tag
    (for example, tag `v1` instead of `latest`), once the image is pulled by a worker
    node, the image will be stored on the node and not pulled again when a new pod
    using the same image is run (at least that’s the default policy for pulling images).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 修改应用程序并将其更改推送到相同的镜像标签并不是一个好主意，但我们所有人都在开发过程中倾向于这样做。如果你正在修改`latest`标签，那不是问题，但当你使用不同的标签（例如，标签`v1`而不是`latest`）标记镜像时，一旦镜像被工作节点拉取，该镜像将存储在节点上，并且当运行使用相同镜像的新Pod时不会再次拉取（至少这是拉取镜像的默认策略）。
- en: That means any changes you make to the image won’t be picked up if you push
    them to the same tag. If a new pod is scheduled to the same node, the Kubelet
    will run the old version of the image. On the other hand, nodes that haven’t run
    the old version will pull and run the new image, so you might end up with two
    different versions of the pod running. To make sure this doesn’t happen, you need
    to set the container’s `imagePullPolicy` property to `Always`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果你将更改推送到相同的标签，那么对镜像所做的任何更改都不会被选中。如果新的Pod被调度到相同的节点，Kubelet将运行旧版本的镜像。另一方面，尚未运行旧版本的节点将拉取并运行新镜像，因此你可能会同时运行两个不同版本的Pod。为了确保这种情况不会发生，你需要将容器的`imagePullPolicy`属性设置为`Always`。
- en: You need to be aware that the default `imagePullPolicy` depends on the image
    tag. If a container refers to the `latest` tag (either explicitly or by not specifying
    the tag at all), `imagePullPolicy` defaults to `Always`, but if the container
    refers to any other tag, the policy defaults to `IfNotPresent`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要意识到默认的`imagePullPolicy`取决于镜像标签。如果容器引用了`latest`标签（无论是显式引用还是根本未指定标签），则`imagePullPolicy`默认为`Always`，但如果容器引用了任何其他标签，则策略默认为`IfNotPresent`。
- en: When using a tag other than `latest`, you need to set the `imagePullPolicy`
    properly if you make changes to an image without changing the tag. Or better yet,
    make sure you always push changes to an image under a new tag.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用除`latest`之外的标签时，如果你在未更改标签的情况下更改了镜像，你需要正确设置`imagePullPolicy`。或者更好的做法是，确保你总是将更改推送到一个新标签下的镜像。
- en: '|  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Keep the `curl` loop running and open another terminal, where you’ll get the
    rolling update started. To perform the update, you’ll run the `kubectl rolling-update`
    command. All you need to do is tell it which ReplicationController you’re replacing,
    give a name for the new ReplicationController, and specify the new image you’d
    like to replace the original one with. The following listing shows the full command
    for performing the rolling update.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 保持`curl`循环运行，并打开另一个终端，在那里你可以启动滚动更新。要执行更新，你需要运行`kubectl rolling-update`命令。你需要做的就是告诉它你正在替换哪个ReplicationController，为新ReplicationController提供一个名称，并指定你想要替换原始镜像的新镜像。以下列表显示了执行滚动更新的完整命令。
- en: Listing 9.4\. Initiating a rolling-update of a ReplicationController using `kubectl`
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4\. 使用`kubectl`启动ReplicationController的滚动更新
- en: '`$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2` `Created
    kubia-v2 Scaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep
    3      pods available, don''t exceed 4 pods) ...`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2` `已创建kubia-v2
    将kubia-v2从0扩展到3，将kubia-v1从3缩减到0（保持3个可用Pod，不超过4个Pod）...`'
- en: Because you’re replacing ReplicationController `kubia-v1` with one running version
    2 of your kubia app, you’d like the new ReplicationController to be called `kubia-v2`
    and use the `luksa/kubia:v2` container image.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你正在用运行版本2的kubia应用程序替换ReplicationController `kubia-v1`，你希望新的ReplicationController被称为`kubia-v2`并使用`luksa/kubia:v2`容器镜像。
- en: When you run the command, a new ReplicationController called `kubia-v2` is created
    immediately. The state of the system at this point is shown in [figure 9.5](#filepos884529).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行命令时，会立即创建一个新的ReplicationController，名为`kubia-v2`。此时系统的状态如图9.5所示[figure 9.5](#filepos884529)。
- en: Figure 9.5\. The state of the system immediately after starting the rolling
    update
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5\. 滚动更新启动后系统的状态
- en: '![](images/00175.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00175.jpg)'
- en: The new ReplicationController’s pod template references the `luksa/kubia:v2`
    image and its initial desired replica count is set to 0, as you can see in the
    following listing.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 新的ReplicationController的Pod模板引用了`luksa/kubia:v2`镜像，并且其初始期望副本数设置为0，如下列所示。
- en: Listing 9.5\. Describing the new ReplicationController created by the rolling
    update
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.5\. 描述滚动更新创建的新ReplicationController
- en: '`$ kubectl describe rc kubia-v2` `Name:       kubia-v2 Namespace:  default
    Image(s):   luksa/kubia:v2` `1` `Selector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155
    Labels:     app=kubia Replicas:   0 current / 0 desired` `2` `...`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe rc kubia-v2` `名称：       kubia-v2 命名空间：  default 镜像(s)：  
    luksa/kubia:v2` `1` `选择器：   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155
    标签：     app=kubia 副本：   0 当前 / 0 期望` `2` `...`'
- en: 1 The new ReplicationController refers to the v2 image.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 新的ReplicationController引用v2镜像。
- en: 2 Initially, the desired number of replicas is zero.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 初始时，期望的副本数量为零。
- en: Understanding the steps performed by kubectl before the rolling update commences
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在滚动更新开始之前，理解kubectl执行的步骤
- en: '`kubectl` created this ReplicationController by copying the `kubia-v1` controller
    and changing the image in its pod template. If you look closely at the controller’s
    label selector, you’ll notice it has been modified, too. It includes not only
    a simple `app=kubia` label, but also an additional `deployment` label which the
    pods must have in order to be managed by this ReplicationController.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl`通过复制`kubia-v1`控制器并更改其Pod模板中的镜像来创建这个ReplicationController。如果你仔细查看控制器的标签选择器，你会注意到它也被修改了。它不仅包括简单的`app=kubia`标签，还包括一个额外的`deployment`标签，Pod必须具有这个标签才能由这个ReplicationController管理。'
- en: You probably know this already, but this is necessary to avoid having both the
    new and the old ReplicationControllers operating on the same set of pods. But
    even if pods created by the new controller have the additional `deployment` label
    in addition to the `app=kubia` label, doesn’t this mean they’ll be selected by
    the first ReplicationController’s selector, because it’s set to `app=kubia`?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经知道了这一点，但这是必要的，以避免新旧ReplicationControllers在相同的Pod集上运行。但即使新控制器创建的Pod除了`app=kubia`标签外还有额外的`deployment`标签，这并不意味着它们会被第一个ReplicationController的选择器选中，因为它设置为`app=kubia`吗？
- en: 'Yes, that’s exactly what would happen, but there’s a catch. The rolling-update
    process has modified the selector of the first ReplicationController, as well:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这正是会发生的事情，但有一个转折。滚动更新过程也修改了第一个ReplicationController的选择器：
- en: '`$ kubectl describe rc kubia-v1` `Name:       kubia-v1 Namespace:  default
    Image(s):   luksa/kubia:v1 Selector:   app=kubia,``deployment=3ddd307978b502a5b975ed4045ae4964-orig`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe rc kubia-v1` `名称：       kubia-v1 命名空间：  default 镜像(s)：  
    luksa/kubia:v1 选择器：   app=kubia,` deployment=3ddd307978b502a5b975ed4045ae4964-orig`'
- en: 'Okay, but doesn’t this mean the first controller now sees zero pods matching
    its selector, because the three pods previously created by it contain only the
    `app=kubia` label? No, because `kubectl` had also modified the labels of the live
    pods just before modifying the ReplicationController’s selector:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，但这不是意味着第一个控制器现在看到没有Pod匹配其选择器，因为之前由它创建的三个Pod只包含`app=kubia`标签吗？不，因为`kubectl`在修改ReplicationController的选择器之前也修改了活动Pod的标签：
- en: '`$ kubectl get po --show-labels` `NAME            READY  STATUS   RESTARTS 
    AGE  LABELS kubia-v1-m33mv  1/1    Running  0         2m   app=kubia,``deployment=3ddd...`
    `kubia-v1-nmzw9  1/1    Running  0         2m   app=kubia,``deployment=3ddd...`
    `kubia-v1-cdtey  1/1    Running  0         2m   app=kubia,``deployment=3ddd...`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po --show-labels` `名称            就绪  状态   重启次数  年龄  标签 kubia-v1-m33mv 
    1/1    运行  0         2m   app=kubia,` deployment=3ddd...` `kubia-v1-nmzw9  1/1   
    运行  0         2m   app=kubia,` deployment=3ddd...` `kubia-v1-cdtey  1/1    运行 
    0         2m   app=kubia,` deployment=3ddd...`'
- en: If this is getting too complicated, examine [figure 9.6](#filepos888640), which
    shows the pods, their labels, and the two ReplicationControllers, along with their
    pod selectors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这变得太复杂，请查看[图9.6](#filepos888640)，它显示了Pods、它们的标签以及两个ReplicationControllers，以及它们的Pod选择器。
- en: Figure 9.6\. Detailed state of the old and new ReplicationControllers and pods
    at the start of a rolling update
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6\. 滚动更新开始时旧的和新的ReplicationControllers以及Pods的详细状态
- en: '![](images/00194.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00194.jpg)'
- en: '`kubectl` had to do all this before even starting to scale anything up or down.
    Now imagine doing the rolling update manually. It’s easy to see yourself making
    a mistake here and possibly having the ReplicationController kill off all your
    pods—pods that are actively serving your production clients!'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl`在开始缩放任何东西之前必须完成所有这些。现在想象一下手动进行滚动更新。很容易想象自己在这里犯了一个错误，并且可能让ReplicationController杀死了所有的Pods——这些Pods正在积极地为你的生产客户提供服务！'
- en: Replacing old pods with new ones by scaling the two ReplicationControllers
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过缩放两个ReplicationControllers来替换旧Pods
- en: 'After setting up all this, `kubectl` starts replacing pods by first scaling
    up the new controller to 1\. The controller thus creates the first `v2` pod. `kubectl`
    then scales down the old ReplicationController by 1\. This is shown in the next
    two lines printed by `kubectl`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好所有这些之后，`kubectl`首先通过将新控制器扩展到1来替换pods。因此，控制器创建了第一个`v2` pod。然后`kubectl`通过减少1来缩减旧的ReplicationController。这显示在`kubectl`打印的下一行中：
- en: '`Scaling kubia-v2 up to 1 Scaling kubia-v1 down to 2`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`将kubia-v2扩展到1 将kubia-v1缩减到2`'
- en: 'Because the Service is targeting all pods with the `app=kubia` label, you should
    start seeing your `curl` requests redirected to the new `v2` pod every few loop
    iterations:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因为服务正在针对所有带有`app=kubia`标签的pods，你应该开始看到你的`curl`请求在每几个循环迭代中被重定向到新的`v2` pod：
- en: '`This is v2 running in pod kubia-v2-nmzw9` `1` `This is v1 running in pod kubia-v1-kbtsk
    This is v1 running in pod kubia-v1-2321o This is v2 running in pod kubia-v2-nmzw9`
    `1` `...`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`这是运行在pod kubia-v2-nmzw9中的v2` `1` `这是运行在pod kubia-v1-kbtsk中的v1` `这是运行在pod kubia-v1-2321o中的v1`
    `这是运行在pod kubia-v2-nmzw9中的v2` `1` `...`'
- en: 1 Requests hitting the pod running the new version
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 击中运行新版本的pod的请求
- en: '[Figure 9.7](#filepos890655) shows the current state of the system.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.7](#filepos890655)显示了系统的当前状态。'
- en: Figure 9.7\. The Service is redirecting requests to both the old and new pods
    during the rolling update.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7\. 在滚动更新期间，服务正在将请求重定向到旧的和新的pods。
- en: '![](images/00015.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00015.jpg)'
- en: As `kubectl` continues with the rolling update, you start seeing a progressively
    bigger percentage of requests hitting `v2` pods, as the update process deletes
    more of the `v1` pods and replaces them with those running your new image. Eventually,
    the original ReplicationController is scaled to zero, causing the last `v1` pod
    to be deleted, which means the Service will now be backed by `v2` pods only. At
    that point, `kubectl` will delete the original ReplicationController and the update
    process will be finished, as shown in the following listing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 随着`kubectl`继续进行滚动更新，你开始看到越来越多的请求击中`v2` pods，因为更新过程删除了更多的`v1` pods，并用运行你新镜像的pods替换它们。最终，原始的ReplicationController扩展到零，导致最后一个`v1`
    pod被删除，这意味着服务现在只由`v2` pods支持。在那个时刻，`kubectl`将删除原始的ReplicationController，更新过程将完成，如下所示。
- en: Listing 9.6\. The final steps performed by `kubectl rolling-update`
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6\. `kubectl rolling-update`执行的最终步骤
- en: '`... Scaling kubia-v2 up to 2 Scaling kubia-v1 down to 1 Scaling kubia-v2 up
    to 3 Scaling kubia-v1 down to 0 Update succeeded. Deleting kubia-v1 replicationcontroller
    "kubia-v1" rolling updated to "kubia-v2"`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`... 将kubia-v2扩展到2 将kubia-v1缩减到1 将kubia-v2扩展到3 将kubia-v1缩减到0 更新成功。删除kubia-v1
    replicationcontroller "kubia-v1"，滚动更新到"kubia-v2"`'
- en: You’re now left with only the `kubia-v2` ReplicationController and three `v2`
    pods. All throughout this update process, you’ve hit your service and gotten a
    response every time. You have, in fact, performed a rolling update with zero downtime.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你只剩下`kubia-v2` ReplicationController和三个`v2` pods。在整个更新过程中，你每次都击中了你的服务并得到了响应。实际上，你已经执行了一个零停机时间的滚动更新。
- en: 9.2.3\. Understanding why kubectl rolling-update is now obsolete
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 9.2.3\. 理解为什么kubectl rolling-update现在已过时
- en: At the beginning of this section, I mentioned an even better way of doing updates
    than through `kubectl rolling-update`. What’s so wrong with this process that
    a better one had to be introduced?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开头，我提到了一种比通过`kubectl rolling-update`进行更新更好的方法。这个流程有什么问题，以至于需要引入一个更好的流程？
- en: Well, for starters, I, for one, don’t like Kubernetes modifying objects I’ve
    created. Okay, it’s perfectly fine for the scheduler to assign a node to my pods
    after I create them, but Kubernetes modifying the labels of my pods and the label
    selectors of my ReplicationController`s` is something that I don’t expect and
    could cause me to go around the office yelling at my colleagues, “Who’s been messing
    with my controllers!?!?”
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，首先，至少对我来说，我不喜欢Kubernetes修改我创建的对象。好吧，调度器在我创建pods之后分配节点给我是完全正常的，但Kubernetes修改我的pods的标签和ReplicationController的标签选择器是我没有预料到的，这可能会让我在办公室里对着同事大喊，“谁在捣鼓我的控制器！？！？”
- en: But even more importantly, if you’ve paid close attention to the words I’ve
    used, you probably noticed that all this time I said explicitly that the `kubectl`
    client was the one performing all these steps of the rolling update.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 但更重要的是，如果你仔细注意我使用的词语，你可能已经注意到，我一直在明确地说`kubectl`客户端是执行滚动更新所有这些步骤的那个。
- en: 'You can see this by turning on verbose logging with the `--v` option when triggering
    the rolling update:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在触发滚动更新时使用`--v`选项来开启详细日志记录来看到这一点：
- en: '`$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6`'
- en: '|  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Using the `--v 6` option increases the logging level enough to let you see the
    requests `kubectl` is sending to the API server.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`--v 6`选项可以将日志级别提高足够，以便让你看到`kubectl`发送给API服务器的请求。
- en: '|  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Using this option, `kubectl` will print out each HTTP request it sends to the
    Kubernetes API server. You’ll see PUT requests to
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此选项，`kubectl`将打印出它发送给Kubernetes API服务器的每个HTTP请求。你会看到对PUT请求的响应。
- en: '`/api/v1/namespaces/default/replicationcontrollers/kubia-v1`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`/api/v1/namespaces/default/replicationcontrollers/kubia-v1`'
- en: which is the RESTful URL representing your `kubia-v1` ReplicationController
    resource. These requests are the ones scaling down your ReplicationController,
    which shows that the `kubectl` client is the one doing the scaling, instead of
    it being performed by the Kubernetes master.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是表示你的`kubia-v1` ReplicationController资源的RESTful URL。这些请求是缩小你的ReplicationController的请求，这表明`kubectl`客户端正在执行扩展，而不是由Kubernetes主节点执行。
- en: '|  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Use the verbose logging option when running other `kubectl` commands, to learn
    more about the communication between `kubectl` and the API server.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行其他`kubectl`命令时使用详细日志记录选项，以了解更多关于`kubectl`和API服务器之间通信的信息。
- en: '|  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: But why is it such a bad thing that the update process is being performed by
    the client instead of on the server? Well, in your case, the update went smoothly,
    but what if you lost network connectivity while `kubectl` was performing the update?
    The update process would be interrupted mid-way. Pods and ReplicationControllers
    would end up in an intermediate state.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么客户端执行更新过程而不是在服务器上执行它是一件坏事呢？好吧，在你的情况下，更新过程进行得很顺利，但如果你在`kubectl`执行更新时失去了网络连接怎么办？更新过程会在中途中断。Pod和ReplicationController最终会处于中间状态。
- en: Another reason why performing an update like this isn’t as good as it could
    be is because it’s imperative. Throughout this book, I’ve stressed how Kubernetes
    is about you telling it the desired state of the system and having Kubernetes
    achieve that state on its own, by figuring out the best way to do it. This is
    how pods are deployed and how pods are scaled up and down. You never tell Kubernetes
    to add an additional pod or remove an excess one—you change the number of desired
    replicas and that’s it.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是，像这样的更新并不像它本可以做到的那样好，因为它是一种命令式操作。在这本书的整个过程中，我强调了Kubernetes是关于你告诉它系统的期望状态，然后由Kubernetes自己通过找出最佳方式来实现这一状态。这就是Pod的部署和扩展上下文的方式。你永远不会告诉Kubernetes添加额外的Pod或移除多余的Pod——你只需更改期望副本的数量即可。
- en: Similarly, you will also want to change the desired image tag in your pod definitions
    and have Kubernetes replace the pods with new ones running the new image. This
    is exactly what drove the introduction of a new resource called a Deployment,
    which is now the preferred way of deploying applications in Kubernetes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你也会想要更改你的Pod定义中期望的镜像标签，并让Kubernetes用运行新镜像的新Pod替换它们。这正是引入名为Deployment的新资源的原因，现在它是Kubernetes中部署应用程序的首选方式。
- en: 9.3\. Using Deployments for updating apps declaratively
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3. 使用Deployment声明式更新应用
- en: A Deployment is a higher-level resource meant for deploying applications and
    updating them declaratively, instead of doing it through a ReplicationController
    or a ReplicaSet, which are both considered lower-level concepts.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment是一个高级资源，旨在部署和声明式更新应用程序，而不是通过ReplicationController或ReplicaSet进行，这两者都被认为是低级概念。
- en: When you create a Deployment, a ReplicaSet resource is created underneath (eventually
    more of them). As you may remember from [chapter 4](index_split_038.html#filepos358794),
    ReplicaSets are a new generation of ReplicationControllers, and should be used
    instead of them. Replica-Sets replicate and manage pods, as well. When using a
    Deployment, the actual pods are created and managed by the Deployment’s ReplicaSets,
    not by the Deployment directly (the relationship is shown in [figure 9.8](#filepos897337)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个Deployment时，会在其下方创建一个ReplicaSet资源（最终会有更多）。你可能还记得[第4章](index_split_038.html#filepos358794)，ReplicaSets是ReplicationControllers的新一代，应该代替它们使用。ReplicaSet也会复制和管理Pod。当使用Deployment时，实际的Pod是由Deployment的ReplicaSet创建和管理的，而不是直接由Deployment管理（关系如图9.8所示）。
- en: Figure 9.8\. A Deployment is backed by a ReplicaSet, which supervises the deployment’s
    pods.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8\. Deployment 由 ReplicaSet 支持，它监督部署的 pod。
- en: '![](images/00035.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00035.jpg)'
- en: You might wonder why you’d want to complicate things by introducing another
    object on top of a ReplicationController or ReplicaSet, when they’re what suffices
    to keep a set of pod instances running. As the rolling update example in [section
    9.2](index_split_076.html#filepos874298) demonstrates, when updating the app,
    you need to introduce an additional ReplicationController and coordinate the two
    controllers to dance around each other without stepping on each other’s toes.
    You need something coordinating this dance. A Deployment resource takes care of
    that (it’s not the Deployment resource itself, but the controller process running
    in the Kubernetes control plane that does that; but we’ll get to that in [chapter
    11](index_split_087.html#filepos1036287)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道，为什么要在 ReplicationController 或 ReplicaSet 之上引入另一个对象来使事情复杂化，因为它们已经足够保持一组
    pod 实例的运行。正如 [第 9.2 节](index_split_076.html#filepos874298) 中的滚动更新示例所证明的，在更新应用程序时，您需要引入一个额外的
    ReplicationController 并协调两个控制器，使它们在彼此之间跳舞而不会互相干扰。您需要某种协调这种舞蹈的东西。Deployment 资源负责处理此事（这并不是
    Deployment 资源本身，而是在 Kubernetes 控制平面中运行的控制器进程；但我们将在这 [第 11 章](index_split_087.html#filepos1036287)
    中讨论这一点）。
- en: Using a Deployment instead of the lower-level constructs makes updating an app
    much easier, because you’re defining the desired state through the single Deployment
    resource and letting Kubernetes take care of the rest, as you’ll see in the next
    few pages.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Deployment 而不是较低级别的结构可以使更新应用程序变得更加容易，因为您通过单个 Deployment 资源定义所需状态，并让 Kubernetes
    处理其余部分，正如您将在接下来的几页中看到的。
- en: 9.3.1\. Creating a Deployment
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3.1\. 创建 Deployment
- en: Creating a Deployment isn’t that different from creating a ReplicationController.
    A Deployment is also composed of a label selector, a desired replica count, and
    a pod template. In addition to that, it also contains a field, which specifies
    a deployment strategy that defines how an update should be performed when the
    Deployment resource is modified.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Deployment 与创建 ReplicationController 并无太大区别。Deployment 也由标签选择器、期望副本数和 pod
    模板组成。除此之外，它还包含一个字段，该字段指定了部署策略，该策略定义了在修改 Deployment 资源时如何执行更新。
- en: Creating a Deployment manifest
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Deployment 清单
- en: Let’s see how to use the `kubia-v1` ReplicationController example from earlier
    in this chapter and modify it so it describes a Deployment instead of a ReplicationController.
    As you’ll see, this requires only three trivial changes. The following listing
    shows the modified YAML.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用本章前面提到的 `kubia-v1` ReplicationController 示例，并对其进行修改，使其描述 Deployment
    而不是 ReplicationController。正如您将看到的，这只需要进行三个微小的更改。以下列表显示了修改后的 YAML。
- en: 'Listing 9.7\. A Deployment definition: kubia-deployment-v1.yaml'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.7\. 部署定义：kubia-deployment-v1.yaml
- en: '`apiVersion: apps/v1beta1` `1` `kind: Deployment` `2` `metadata:   name: kubia`
    `3` `spec:   replicas: 3   template:     metadata:       name: kubia       labels:
            app: kubia     spec:       containers:       - image: luksa/kubia:v1        
    name: nodejs`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: apps/v1beta1` `1` `kind: Deployment` `2` `metadata:` `3` `name:
    kubia` `4` `spec:` `5` `replicas: 3` `6` `template:` `7` `metadata:` `8` `name:
    kubia` `9` `labels:` `10` `app: kubia` `11` `spec:` `12` `containers:` `13` `-
    image: luksa/kubia:v1` `14` `name: nodejs`'
- en: 1 Deployments are in the apps API group, version v1beta1.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 Deployment 位于 apps API 组，版本 v1beta1。
- en: 2 You’ve changed the kind from ReplicationController to Deployment.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 你已将类型从 ReplicationController 更改为 Deployment。
- en: 3 There’s no need to include the version in the name of the Deployment.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 在 Deployment 的名称中不需要包含版本号。
- en: '|  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You’ll find an older version of the Deployment resource in `extensions/ v1beta1`,
    and a newer one in `apps/v1beta2` with different required fields and different
    defaults. Be aware that `kubectl explain` shows the older version.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 `extensions/ v1beta1` 中找到 Deployment 资源的老版本，在 `apps/v1beta2` 中找到新版本，它们有不同的必填字段和默认值。请注意，`kubectl
    explain` 显示的是老版本。
- en: '|  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Because the ReplicationController from before was managing a specific version
    of the pods, you called it `kubia-v1`. A Deployment, on the other hand, is above
    that version stuff. At a given point in time, the Deployment can have multiple
    pod versions running under its wing, so its name shouldn’t reference the app version.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于之前的 ReplicationController 管理的是特定版本的 pod，所以你将其称为 `kubia-v1`。另一方面，Deployment
    不涉及版本问题。在某个特定时间点，Deployment 可以在其下运行多个 pod 版本，因此其名称不应引用应用程序版本。
- en: Creating the Deployment resource
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Deployment 资源
- en: 'Before you create this Deployment, make sure you delete any ReplicationControllers
    and pods that are still running, but keep the `kubia` Service for now. You can
    use the `--all` switch to delete all those ReplicationControllers like this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在你创建这个 Deployment 之前，确保删除任何仍在运行的 ReplicationController 和 pod，但暂时保留 `kubia` 服务。你可以使用
    `--all` 开关来删除所有这些 ReplicationController，如下所示：
- en: '`$ kubectl delete rc --all`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete rc --all`'
- en: 'You’re now ready to create the Deployment:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以创建 Deployment 了：
- en: '`$ kubectl create -f kubia-deployment-v1.yaml --record` `deployment "kubia"
    created`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f kubia-deployment-v1.yaml --record` `deployment "kubia"
    created`'
- en: '|  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Be sure to include the `--record` command-line option when creating it. This
    records the command in the revision history, which will be useful later.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建时，务必包含 `--record` 命令行选项。这将记录命令在修订历史中，这在以后会很有用。
- en: '|  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Displaying the status of the deployment rollout
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 显示部署滚动的状态
- en: 'You can use the usual `kubectl get deployment` and the `kubectl describe deployment`
    commands to see details of the Deployment, but let me point you to an additional
    command, which is made specifically for checking a Deployment’s status:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用常规的 `kubectl get deployment` 和 `kubectl describe deployment` 命令来查看 Deployment
    的详细信息，但让我指向一个额外的命令，这个命令是专门用于检查 Deployment 状态的：
- en: '`$ kubectl rollout status deployment kubia` `deployment kubia successfully
    rolled out`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout status deployment kubia` `deployment kubia successfully
    rolled out`'
- en: 'According to this, the Deployment has been successfully rolled out, so you
    should see the three pod replicas up and running. Let’s see:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个信息，Deployment 已经成功部署，你应该看到三个 pod 副本正在运行。让我们看看：
- en: '`$ kubectl get po` `NAME                     READY     STATUS    RESTARTS  
    AGE kubia-1506449474-otnnh   1/1       Running   0          14s kubia-1506449474-vmn7s  
    1/1       Running   0          14s kubia-1506449474-xis6m   1/1       Running  
    0          14s`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME                     READY     STATUS    RESTARTS  
    AGE kubia-1506449474-otnnh   1/1       Running   0          14s kubia-1506449474-vmn7s  
    1/1       Running   0          14s kubia-1506449474-xis6m   1/1       Running  
    0          14s`'
- en: Understanding how Deployments create ReplicaSets, which then create the pods
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Deployment 如何创建 ReplicaSet，然后 ReplicaSet 创建 pod
- en: Take note of the names of these pods. Earlier, when you used a ReplicationController
    to create pods, their names were composed of the name of the controller plus a
    randomly generated string (for example, `kubia-v1-m33mv`). The three pods created
    by the Deployment include an additional numeric value in the middle of their names.
    What is that exactly?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这些 pod 的名称。早些时候，当你使用 ReplicationController 创建 pod 时，它们的名称由控制器的名称加上一个随机生成的字符串（例如，`kubia-v1-m33mv`）组成。Deployment
    创建的三个 pod 名称中包含一个额外的数字值。这究竟是什么意思？
- en: 'The number corresponds to the hashed value of the pod template in the Deployment
    and the ReplicaSet managing these pods. As we said earlier, a Deployment doesn’t
    manage pods directly. Instead, it creates ReplicaSets and leaves the managing
    to them, so let’s look at the ReplicaSet created by your Deployment:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 数字对应于 Deployment 和管理这些 pod 的 ReplicaSet 中的 pod 模板的哈希值。正如我们之前所说的，Deployment 并不直接管理
    pod。相反，它创建 ReplicaSet 并将管理任务交给它们，所以让我们看看由你的 Deployment 创建的 ReplicaSet：
- en: '`$ kubectl get replicasets` `NAME               DESIRED   CURRENT   AGE kubia-1506449474  
    3         3         10s`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get replicasets` `NAME               DESIRED   CURRENT   AGE kubia-1506449474  
    3         3         10s`'
- en: The ReplicaSet’s name also contains the hash value of its pod template. As you’ll
    see later, a Deployment creates multiple ReplicaSets—one for each version of the
    pod template. Using the hash value of the pod template like this allows the Deployment
    to always use the same (possibly existing) ReplicaSet for a given version of the
    pod template.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 的名称也包含其 pod 模板的哈希值。正如你稍后将会看到的，Deployment 会创建多个 ReplicaSet——每个 pod
    模板版本一个。使用 pod 模板的哈希值这样做允许 Deployment 总是使用相同（可能已存在）的 ReplicaSet 来处理特定版本的 pod 模板。
- en: Accessing the pods through the service
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过服务访问 pod
- en: With the three replicas created by this ReplicaSet now running, you can use
    the Service you created a while ago to access them, because you made the new pods’
    labels match the Service’s label selector.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个 ReplicaSet 创建的三个副本现在正在运行，你可以使用你之前创建的服务来访问它们，因为新 pod 的标签与服务的标签选择器相匹配。
- en: Up until this point, you probably haven’t seen a good-enough reason why you
    should use Deployments over ReplicationControllers. Luckily, creating a Deployment
    also hasn’t been any harder than creating a ReplicationController. Now, you’ll
    start doing things with this Deployment, which will make it clear why Deployments
    are superior. This will become clear in the next few moments, when you see how
    updating the app through a Deployment resource compares to updating it through
    a ReplicationController.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能还没有看到足够好的理由说明为什么你应该使用部署（Deployments）而不是副本控制器（ReplicationControllers）。幸运的是，创建部署也没有比创建副本控制器更困难。现在，你将开始使用这个部署，这将清楚地说明为什么部署更优越。这一点将在接下来的几分钟内变得清晰，当你看到如何通过部署资源更新应用程序与通过副本控制器更新应用程序进行比较时。
- en: 9.3.2\. Updating a Deployment
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3.2\. 更新部署
- en: Previously, when you ran your app using a ReplicationController, you had to
    explicitly tell Kubernetes to perform the update by running `kubectl rolling-update`.
    You even had to specify the name for the new ReplicationController that should
    replace the old one. Kubernetes replaced all the original pods with new ones and
    deleted the original ReplicationController at the end of the process. During the
    process, you basically had to stay around, keeping your terminal open and waiting
    for `kubectl` to finish the rolling update.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在以前，当你使用副本控制器运行应用程序时，你必须明确告诉Kubernetes通过运行`kubectl rolling-update`来执行更新。你甚至必须指定应该替换旧副本控制器的新的副本控制器名称。Kubernetes在过程结束时用新的Pod替换了所有原始Pod，并删除了原始副本控制器。在这个过程中，你基本上必须待在原地，保持终端开启，等待`kubectl`完成滚动更新。
- en: Now compare this to how you’re about to update a Deployment. The only thing
    you need to do is modify the pod template defined in the Deployment resource and
    Kubernetes will take all the steps necessary to get the actual system state to
    what’s defined in the resource. Similar to scaling a ReplicationController or
    ReplicaSet up or down, all you need to do is reference a new image tag in the
    Deployment’s pod template and leave it to Kubernetes to transform your system
    so it matches the new desired state.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在比较一下你即将如何更新一个部署。你需要做的唯一一件事是修改部署资源中定义的Pod模板，Kubernetes将执行所有必要的步骤，以将实际系统状态转换为资源中定义的状态。类似于扩展或缩减副本控制器或副本集，你只需要在部署的Pod模板中引用一个新的镜像标签，然后让Kubernetes转换你的系统，使其与新的期望状态相匹配。
- en: Understanding the available deployment strategies
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 理解可用的部署策略
- en: How this new state should be achieved is governed by the deployment strategy
    configured on the Deployment itself. The default strategy is to perform a rolling
    update (the strategy is called `RollingUpdate`). The alternative is the `Recreate`
    strategy, which deletes all the old pods at once and then creates new ones, similar
    to modifying a ReplicationController’s pod template and then deleting all the
    pods (we talked about this in [section 9.1.1](index_split_075.html#filepos869791)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如何实现这个新状态由部署本身上配置的部署策略所控制。默认策略是执行滚动更新（该策略称为`RollingUpdate`）。另一种策略是`Recreate`策略，它一次性删除所有旧的Pod，然后创建新的Pod，类似于修改副本控制器的Pod模板然后删除所有Pod（我们已在[第9.1.1节](index_split_075.html#filepos869791)中讨论过）。
- en: The `Recreate` strategy causes all old pods to be deleted before the new ones
    are created. Use this strategy when your application doesn’t support running multiple
    versions in parallel and requires the old version to be stopped completely before
    the new one is started. This strategy does involve a short period of time when
    your app becomes completely unavailable.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`Recreate`策略会在创建新Pod之前删除所有旧的Pod。当你的应用程序不支持并行运行多个版本，并且需要在启动新版本之前完全停止旧版本时，请使用此策略。此策略确实涉及一个短暂的时间，此时你的应用程序将完全不可用。'
- en: The `RollingUpdate` strategy, on the other hand, removes old pods one by one,
    while adding new ones at the same time, keeping the application available throughout
    the whole process, and ensuring there’s no drop in its capacity to handle requests.
    This is the default strategy. The upper and lower limits for the number of pods
    above or below the desired replica count are configurable. You should use this
    strategy only when your app can handle running both the old and new version at
    the same time.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，`RollingUpdate`策略会逐个删除旧的Pod，同时添加新的Pod，在整个过程中保持应用程序可用，并确保其处理请求的能力不会下降。这是默认策略。可配置的上限和下限是期望副本数量以上或以下的Pod数量。你应该只在你的应用程序可以同时运行旧版本和新版本时使用此策略。
- en: Slowing down the rolling update for demo purposes
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的减慢滚动更新
- en: In the next exercise, you’ll use the `RollingUpdate` strategy, but you need
    to slow down the update process a little, so you can see that the update is indeed
    performed in a rolling fashion. You can do that by setting the `minReadySeconds`
    attribute on the Deployment. We’ll explain what this attribute does by the end
    of this chapter. For now, set it to 10 seconds with the `kubectl patch` command.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，你将使用 `RollingUpdate` 策略，但你需要稍微减慢更新过程，以便可以看到更新确实是按滚动方式进行的。你可以通过设置 Deployment
    上的 `minReadySeconds` 属性来实现这一点。我们将在本章末尾解释这个属性的作用。现在，使用 `kubectl patch` 命令将其设置为
    10 秒。
- en: '`$ kubectl patch deployment kubia -p ''{"spec": {"minReadySeconds": 10}}''`
    `"kubia" patched`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl patch deployment kubia -p ''{"spec": {"minReadySeconds": 10}}''`
    `"kubia" patched`'
- en: '|  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The `kubectl patch` command is useful for modifying a single property or a limited
    number of properties of a resource without having to edit its definition in a
    text editor.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl patch` 命令对于修改资源的单个属性或有限数量的属性非常有用，而无需在文本编辑器中编辑其定义。'
- en: '|  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: You used the patch command to change the spec of the Deployment. This doesn’t
    cause any kind of update to the pods, because you didn’t change the pod template.
    Changing other Deployment properties, like the desired replica count or the deployment
    strategy, also doesn’t trigger a rollout, because it doesn’t affect the existing
    individual pods in any way.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用了 `patch` 命令来更改 Deployment 的规范。这不会引起任何类型的 pod 更新，因为你没有更改 pod 模板。更改其他 Deployment
    属性，如期望的副本数或部署策略，也不会触发滚动更新，因为它不会以任何方式影响现有的单个 pod。
- en: Triggering the rolling update
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 触发滚动更新
- en: 'If you’d like to track the update process as it progresses, first run the `curl`
    loop again in another terminal to see what’s happening with the requests (don’t
    forget to replace the IP with the actual external IP of your service):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要跟踪更新过程，首先在另一个终端中再次运行 `curl` 循环，以查看请求的情况（别忘了将 IP 替换为你的服务的实际外部 IP）：
- en: '`$ while true; do curl http://130.211.109.222; done`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ while true; do curl http://130.211.109.222; done`'
- en: 'To trigger the actual rollout, you’ll change the image used in the single pod
    container to `luksa/kubia:v2`. Instead of editing the whole YAML of the Deployment
    object or using the `patch` command to change the image, you’ll use the `kubectl
    set image` command, which allows changing the image of any resource that contains
    a container (ReplicationControllers, ReplicaSets, Deployments, and so on). You’ll
    use it to modify your Deployment like this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要触发实际的滚动更新，你需要将单个 pod 容器中使用的镜像更改为 `luksa/kubia:v2`。你不需要编辑 Deployment 对象的整个 YAML
    文件或使用 `patch` 命令来更改镜像，而是使用 `kubectl set image` 命令，该命令允许更改包含容器的任何资源的镜像（ReplicationControllers、ReplicaSets、Deployments
    等）。你将使用它来修改你的 Deployment，如下所示：
- en: '`$ kubectl set image deployment kubia nodejs=luksa/kubia:v2` `deployment "kubia"
    image updated`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl set image deployment kubia nodejs=luksa/kubia:v2` `deployment "kubia"
    image updated`'
- en: When you execute this command, you’re updating the `kubia` Deployment’s pod
    template so the image used in its `nodejs` container is changed to `luksa/kubia:v2`
    (from `:v1`). This is shown in [figure 9.9](#filepos911981).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行此命令时，你正在更新 `kubia` Deployment 的 pod 模板，使其 `nodejs` 容器中使用的镜像更改为 `luksa/kubia:v2`（从
    `:v1`）。这如图 9.9 所示。
- en: Figure 9.9\. Updating a Deployment’s pod template to point to a new image
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9\. 更新部署的 pod 模板以指向新镜像
- en: '![](images/00055.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00055.jpg)'
- en: '|  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Ways of modifying Deployments and other resources
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 修改 Deployments 和其他资源的方法
- en: Over the course of this book, you’ve learned several ways how to modify an existing
    object. Let’s list all of them together to refresh your memory.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，你已经学习了多种修改现有对象的方法。让我们一起列出它们，以刷新你的记忆。
- en: Table 9.1\. Modifying an existing resource in Kubernetes
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1\. 在 Kubernetes 中修改现有资源
- en: '| Method | What it does |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 它的作用 |'
- en: '| kubectl edit | Opens the object’s manifest in your default editor. After
    making changes, saving the file, and exiting the editor, the object is updated.
    Example: kubectl edit deployment kubia |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| kubectl edit | 在你的默认编辑器中打开对象的规范。在做出更改、保存文件并退出编辑器后，对象将被更新。例如：kubectl edit
    deployment kubia |'
- en: '| kubectl patch | Modifies individual properties of an object. Example: kubectl
    patch deployment kubia -p ''{"spec": {"template": {"spec": {"containers": [{"name":
    "nodejs", "image": "luksa/kubia:v2"}]}}}}'' |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| kubectl patch | 修改对象的单个属性。例如：kubectl patch deployment kubia -p ''{"spec":
    {"template": {"spec": {"containers": [{"name": "nodejs", "image": "luksa/kubia:v2"}]}}}}''
    |'
- en: '| kubectl apply | Modifies the object by applying property values from a full
    YAML or JSON file. If the object specified in the YAML/JSON doesn’t exist yet,
    it’s created. The file needs to contain the full definition of the resource (it
    can’t include only the fields you want to update, as is the case with kubectl
    patch). Example: kubectl apply -f kubia-deployment-v2.yaml |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| kubectl apply | 通过应用完整的YAML或JSON文件中的属性值来修改对象。如果YAML/JSON中指定的对象尚不存在，则将其创建。该文件需要包含资源的完整定义（它不能仅包含你想要更新的字段，正如kubectl
    patch的情况一样）。例如：kubectl apply -f kubia-deployment-v2.yaml |'
- en: '| kubectl replace | Replaces the object with a new one from a YAML/JSON file.
    In contrast to the apply command, this command requires the object to exist; otherwise
    it prints an error. Example: kubectl replace -f kubia-deployment-v2.yaml |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| kubectl replace | 使用YAML/JSON文件中的新对象替换对象。与apply命令相比，此命令要求对象存在；否则它将打印错误。例如：kubectl
    replace -f kubia-deployment-v2.yaml |'
- en: '| kubectl set image | Changes the container image defined in a Pod, ReplicationController’s
    template, Deployment, DaemonSet, Job, or ReplicaSet. Example: kubectl set image
    deployment kubia nodejs=luksa/kubia:v2 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| kubectl set image | 更改Pod、ReplicationController模板、Deployment、DaemonSet、Job或ReplicaSet中定义的容器镜像。例如：kubectl
    set image deployment kubia nodejs=luksa/kubia:v2 |'
- en: All these methods are equivalent as far as Deployments go. What they do is change
    the Deployment’s specification. This change then triggers the rollout process.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法在Deployments方面都是等效的。它们所做的就是更改Deployments的规范。然后，这种更改会触发滚动过程。
- en: '|  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If you’ve run the `curl` loop, you’ll see requests initially hitting only the
    `v1` pods; then more and more of them hit the v2 pod`s` until, finally, all of
    them hit only the remaining `v2` pods, after all `v1` pods are deleted. This works
    much like the rolling update performed by `kubectl`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经运行了`curl`循环，你将看到请求最初只击中`v1` pods；然后越来越多的请求击中v2 pods，直到最后，所有请求都只击中剩余的`v2`
    pods，在所有`v1` pods被删除之后。这与`kubectl`执行的滚动更新非常相似。
- en: Understanding the awesomeness of Deployments
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Deployments的神奇之处
- en: Let’s think about what has happened. By changing the pod template in your Deployment
    resource, you’ve updated your app to a newer version—by changing a single field!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下发生了什么。通过更改Deployment资源中的Pod模板，你已经将你的应用程序更新到了一个新版本——只需更改一个字段！
- en: The controllers running as part of the Kubernetes control plane then performed
    the update. The process wasn’t performed by the `kubectl` client, like it was
    when you used `kubectl rolling-update`. I don’t know about you, but I think that’s
    simpler than having to run a special command telling Kubernetes what to do and
    then waiting around for the process to be completed.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在Kubernetes控制平面部分的控制器随后执行了更新。这个过程不是由`kubectl`客户端执行的，就像你使用`kubectl rolling-update`时那样。我不知道你，但我觉得这比运行一个特殊的命令告诉Kubernetes要做什么然后等待过程完成要简单得多。
- en: '|  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Be aware that if the pod template in the Deployment references a ConfigMap (or
    a Secret), modifying the ConfigMap will not trigger an update. One way to trigger
    an update when you need to modify an app’s config is to create a new ConfigMap
    and modify the pod template so it references the new ConfigMap.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果Deployment中的Pod模板引用了ConfigMap（或Secret），则修改ConfigMap不会触发更新。当你需要修改应用程序的配置时，触发更新的方法之一是创建一个新的ConfigMap，并修改Pod模板以便它引用新的ConfigMap。
- en: '|  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The events that occurred below the Deployment’s surface during the update are
    similar to what happened during the `kubectl rolling-update`. An additional ReplicaSet
    was created and it was then scaled up slowly, while the previous ReplicaSet was
    scaled down to zero (the initial and final states are shown in [figure 9.10](#filepos916798)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 更新期间在Deployment表面下发生的事件与`kubectl rolling-update`期间发生的事件类似。创建了一个额外的ReplicaSet，然后缓慢地将其扩展，同时将之前的ReplicaSet扩展到零（初始和最终状态在[图9.10](#filepos916798)中显示）。
- en: Figure 9.10\. A Deployment at the start and end of a rolling update
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10\. 滚动更新开始和结束时Deployment的状态
- en: '![](images/00074.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00074.jpg)'
- en: 'You can still see the old ReplicaSet next to the new one if you list them:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你列出它们，你仍然可以看到新ReplicaSet旁边旧的ReplicaSet：
- en: '`$ kubectl get rs` `NAME               DESIRED   CURRENT   AGE kubia-1506449474  
    0         0         24m kubia-1581357123   3         3         23m`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get rs` `NAME               DESIRED   CURRENT   AGE kubia-1506449474  
    0         0         24m kubia-1581357123   3         3         23m`'
- en: Similar to ReplicationControllers, all your new pods are now managed by the
    new ReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the
    old Replication-Controller was deleted at the end of the rolling-update process.
    You’ll soon see what the purpose of this inactive ReplicaSet is.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ReplicationControllers 类似，你现在所有的新的 pod 都由新的 ReplicaSet 管理。与之前不同，旧的 ReplicaSet
    仍然存在，而旧的 Replication-Controller 在滚动更新过程的末尾被删除了。你很快就会看到这个不活跃的 ReplicaSet 的用途。
- en: But you shouldn’t care about ReplicaSets here, because you didn’t create them
    directly. You created and operated only on the Deployment resource; the underlying
    ReplicaSets are an implementation detail. You’ll agree that managing a single
    Deployment object is much easier compared to dealing with and keeping track of
    multiple ReplicationControllers.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你在这里不应该关心 ReplicaSets，因为你没有直接创建它们。你创建并只操作了 Deployment 资源；底层的 ReplicaSets 是一个实现细节。你会同意，管理单个
    Deployment 对象比处理和跟踪多个 ReplicationControllers 要容易得多。
- en: Although this difference may not be so apparent when everything goes well with
    a rollout, it becomes much more obvious when you hit a problem during the rollout
    process. Let’s simulate one problem right now.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然当滚动更新一切顺利时，这种差异可能并不那么明显，但在滚动更新过程中遇到问题时，它就会变得非常明显。现在让我们模拟一个问题。
- en: 9.3.3\. Rolling back a deployment
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3.3\. 回滚部署
- en: You’re currently running version `v2` of your image, so you’ll need to prepare
    version 3 first.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你目前运行的是你的镜像的 `v2` 版本，所以你需要先准备版本 3。
- en: Creating version 3 of your app
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 创建你的应用的版本 3
- en: In version 3, you’ll introduce a bug that makes your app handle only the first
    four requests properly. All requests from the fifth request onward will return
    an internal server error (HTTP status code 500). You’ll simulate this by adding
    an `if` statement at the beginning of the handler function. The following listing
    shows the new code, with all required changes shown in bold.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在版本 3 中，你将引入一个错误，使得你的应用只能正确处理前四个请求。从第五个请求开始的请求都将返回内部服务器错误（HTTP 状态码 500）。你将通过在处理函数的开始处添加一个
    `if` 语句来模拟这种情况。以下列表显示了新的代码，所有必要的更改都以粗体显示。
- en: 'Listing 9.8\. Version 3 of our app (a broken version): v3/app.js'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.8\. 我们应用的版本 3（一个有问题的版本）：v3/app.js
- en: '`const http = require(''http''); const os = require(''os'');` `var requestCount
    = 0;` `console.log("Kubia server starting...");  var handler = function(request,
    response) {   console.log("Received request from " + request.connection.remoteAddress);`
    `if (++requestCount >= 5) {``response.writeHead(500);``response.end("Some internal
    error has occurred! This is pod " + os.hostname() + "\n");``return;``}` `response.writeHead(200);
      response.end("This is v3 running in pod " + os.hostname() + "\n"); };  var www
    = http.createServer(handler); www.listen(8080);`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`const http = require(''http''); const os = require(''os'');` `var requestCount
    = 0;` `console.log("Kubia 服务器启动...");  var handler = function(request, response)
    {   console.log("收到来自 " + request.connection.remoteAddress + " 的请求");` `if (++requestCount
    >= 5) {``response.writeHead(500);``response.end("发生了一些内部错误！这是 pod " + os.hostname()
    + "\n");``return;``}` `response.writeHead(200);   response.end("这是 v3 在 pod "
    + os.hostname() + "\n"); };  var www = http.createServer(handler); www.listen(8080);`'
- en: As you can see, on the fifth and all subsequent requests, the code returns a
    500 error with the message “Some internal error has occurred...”
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在第五次及随后的请求中，代码返回了一个带有消息“发生了一些内部错误...”的 500 错误。
- en: Deploying version 3
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 部署版本 3
- en: 'I’ve made the `v3` version of the image available as `luksa/kubia:v3`. You’ll
    deploy this new version by changing the image in the Deployment specification
    again:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经将 `v3` 版本的镜像作为 `luksa/kubia:v3` 提供。你将通过再次更改 Deployment 规范中的镜像来部署这个新版本：
- en: '`$ kubectl set image deployment kubia nodejs=luksa/kubia:v3` `deployment "kubia"
    image updated`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl set image deployment kubia nodejs=luksa/kubia:v3` `deployment "kubia"
    image updated`'
- en: 'You can follow the progress of the rollout with `kubectl rollout status`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `kubectl rollout status` 跟踪部署的进度：
- en: '`$ kubectl rollout status deployment kubia` `Waiting for rollout to finish:
    1 out of 3 new replicas have been updated... Waiting for rollout to finish: 2
    out of 3 new replicas have been updated... Waiting for rollout to finish: 1 old
    replicas are pending termination... deployment "kubia" successfully rolled out`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout status deployment kubia` `等待滚动更新完成：3 个新副本中有 1 个已更新... 等待滚动更新完成：3
    个新副本中有 2 个已更新... 等待滚动更新完成：1 个旧副本正在等待终止... deployment "kubia" 滚动更新成功`'
- en: The new version is now live. As the following listing shows, after a few requests,
    your web clients start receiving errors.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本现在已上线。如下列表所示，经过几次请求后，你的网络客户端开始收到错误。
- en: Listing 9.9\. Hitting your broken version 3
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.9\. 打破你的版本 3
- en: '`$ while true; do curl http://130.211.109.222; done` `This is v3 running in
    pod kubia-1914148340-lalmx This is v3 running in pod kubia-1914148340-bz35w This
    is v3 running in pod kubia-1914148340-w0voh ... This is v3 running in pod kubia-1914148340-w0voh
    Some internal error has occurred! This is pod kubia-1914148340-bz35w This is v3
    running in pod kubia-1914148340-w0voh Some internal error has occurred! This is
    pod kubia-1914148340-lalmx This is v3 running in pod kubia-1914148340-w0voh Some
    internal error has occurred! This is pod kubia-1914148340-lalmx Some internal
    error has occurred! This is pod kubia-1914148340-bz35w Some internal error has
    occurred! This is pod kubia-1914148340-w0voh`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ while true; do curl http://130.211.109.222; done` `This is v3 running in
    pod kubia-1914148340-lalmx This is v3 running in pod kubia-1914148340-bz35w This
    is v3 running in pod kubia-1914148340-w0voh ... This is v3 running in pod kubia-1914148340-w0voh
    Some internal error has occurred! This is pod kubia-1914148340-bz35w This is v3
    running in pod kubia-1914148340-w0voh Some internal error has occurred! This is
    pod kubia-1914148340-lalmx This is v3 running in pod kubia-1914148340-w0voh Some
    internal error has occurred! This is pod kubia-1914148340-lalmx Some internal
    error has occurred! This is pod kubia-1914148340-bz35w Some internal error has
    occurred! This is pod kubia-1914148340-w0voh`'
- en: Undoing a rollout
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚部署
- en: 'You can’t have your users experiencing internal server errors, so you need
    to do something about it fast. In [section 9.3.6](#filepos937135) you’ll see how
    to block bad rollouts automatically, but for now, let’s see what you can do about
    your bad rollout manually. Luckily, Deployments make it easy to roll back to the
    previously deployed version by telling Kubernetes to undo the last rollout of
    a Deployment:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您不能让用户遇到内部服务器错误，因此您需要迅速采取措施。在[第9.3.6节](#filepos937135)中，您将看到如何自动阻止不良的回滚，但就现在而言，让我们看看您如何手动处理不良的回滚。幸运的是，Deployment通过告诉Kubernetes撤销Deployment的最后一次回滚，使得回滚到之前部署的版本变得容易：
- en: '`$ kubectl rollout undo deployment kubia` `deployment "kubia" rolled back`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout undo deployment kubia` `deployment "kubia" rolled back`'
- en: This rolls the Deployment back to the previous revision.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这将Deployment回滚到上一个修订版。
- en: '|  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `undo` command can also be used while the rollout process is still in progress
    to essentially abort the rollout. Pods already created during the rollout process
    are removed and replaced with the old ones again.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`undo`命令也可以在回滚过程仍在进行时使用，以实质上中止回滚。在回滚过程中已创建的Pod将被移除，并再次替换为旧的Pod。'
- en: '|  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Displaying a Deployment’s rollout history
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 显示Deployment的回滚历史
- en: 'Rolling back a rollout is possible because Deployments keep a revision history.
    As you’ll see later, the history is stored in the underlying ReplicaSets. When
    a rollout completes, the old ReplicaSet isn’t deleted, and this enables rolling
    back to any revision, not only the previous one. The revision history can be displayed
    with the `kubectl rollout history` command:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Deployments保留修订历史，因此回滚回滚是可能的。您将在稍后看到，历史记录存储在底层的ReplicaSets中。当回滚完成时，旧的ReplicaSet不会被删除，这使得您可以回滚到任何修订版，而不仅仅是上一个修订版。可以使用`kubectl
    rollout history`命令显示修订历史：
- en: '`$ kubectl rollout history deployment kubia` `deployments "kubia": REVISION   
    CHANGE-CAUSE 2           kubectl set image deployment kubia nodejs=luksa/kubia:v2
    3           kubectl set image deployment kubia nodejs=luksa/kubia:v3`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout history deployment kubia` `deployments "kubia": REVISION   
    CHANGE-CAUSE 2           kubectl set image deployment kubia nodejs=luksa/kubia:v2
    3           kubectl set image deployment kubia nodejs=luksa/kubia:v3`'
- en: Remember the `--record` command-line option you used when creating the Deployment?
    Without it, the `CHANGE-CAUSE` column in the revision history would be empty,
    making it much harder to figure out what’s behind each revision.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 记得您在创建Deployment时使用的`--record`命令行选项吗？没有它，修订历史中的`CHANGE-CAUSE`列将是空的，这将使得确定每个修订版背后的内容变得更加困难。
- en: Rolling back to a specific Deployment revision
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚到特定的Deployment修订版
- en: 'You can roll back to a specific revision by specifying the revision in the
    `undo` command. For example, if you want to roll back to the first version, you’d
    execute the following command:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在`undo`命令中指定修订版来回滚到特定的修订版。例如，如果您想回滚到第一个版本，您将执行以下命令：
- en: '`$ kubectl rollout undo deployment kubia --to-revision=1`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout undo deployment kubia --to-revision=1`'
- en: Remember the inactive ReplicaSet left over when you modified the Deployment
    the first time? The ReplicaSet represents the first revision of your Deployment.
    All Replica-Sets created by a Deployment represent the complete revision history,
    as shown in [figure 9.11](#filepos925811). Each ReplicaSet stores the complete
    information of the Deployment at that specific revision, so you shouldn’t delete
    it manually. If you do, you’ll lose that specific revision from the Deployment’s
    history, preventing you from rolling back to it.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 记得你在第一次修改 Deployment 时留下的那个不活动的 ReplicaSet 吗？ReplicaSet 代表了你的 Deployment 的第一个修订版本。由
    Deployment 创建的所有 ReplicaSet 都代表了完整的修订历史，如图 9.11 所示。[figure 9.11](#filepos925811)。每个
    ReplicaSet 存储了在该特定修订版本下 Deployment 的完整信息，因此你不应该手动删除它。如果你删除了它，你将失去 Deployment 历史中的那个特定修订版本，这将阻止你回滚到它。
- en: Figure 9.11\. A Deployment’s ReplicaSets also act as its revision history.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11\. Deployment 的 ReplicaSet 也充当其修订历史。
- en: '![](images/00093.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00093.jpg)'
- en: But having old ReplicaSets cluttering your ReplicaSet list is not ideal, so
    the length of the revision history is limited by the `revisionHistoryLimit` property
    on the Deployment resource. It defaults to two, so normally only the current and
    the previous revision are shown in the history (and only the current and the previous
    ReplicaSet are preserved). Older ReplicaSets are deleted automatically.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，让旧的 ReplicaSet 杂乱地充斥着你的 ReplicaSet 列表并不是理想的做法，因此修订历史的长度由 Deployment 资源上的
    `revisionHistoryLimit` 属性限制。默认值为两个，所以通常只有当前和上一个修订版本会显示在历史记录中（并且只保留当前和上一个 ReplicaSet）。旧的
    ReplicaSet 将自动被删除。
- en: '|  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `extensions/v1beta1` version of Deployments doesn’t have a default `revisionHistoryLimit`,
    whereas the default in version `apps/v1beta2` is 10\.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Deployments 的 `extensions/v1beta1` 版本没有默认的 `revisionHistoryLimit`，而 `apps/v1beta2`
    版本的默认值是 10。
- en: '|  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 9.3.4\. Controlling the rate of the rollout
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3.4\. 控制rollout的速率
- en: When you performed the rollout to `v3` and tracked its progress with the `kubectl
    rollout status` command, you saw that first a new pod was created, and when it
    became available, one of the old pods was deleted and another new pod was created.
    This continued until there were no old pods left. The way new pods are created
    and old ones are deleted is configurable through two additional properties of
    the rolling update strategy.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行到 `v3` 版本的 rollout 并使用 `kubectl rollout status` 命令跟踪其进度时，你会看到首先创建了一个新的 pod，当它变得可用时，其中一个旧的
    pod 被删除，并创建了一个新的 pod。这个过程一直持续到没有旧的 pod 为止。新 pod 的创建和旧 pod 的删除方式可以通过滚动更新策略的另外两个属性进行配置。
- en: Introducing the maxSurge and maxUnavailable properties of the rol- lling update
    strategy
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍滚动更新策略的 `maxSurge` 和 `maxUnavailable` 属性
- en: Two properties affect how many pods are replaced at once during a Deployment’s
    rolling update. They are `maxSurge` and `maxUnavailable` and can be set as part
    of the `rollingUpdate` sub-property of the Deployment’s `strategy` attribute,
    as shown in the following listing.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 两个属性会影响在 Deployment 的滚动更新过程中一次替换多少个 pod。它们是 `maxSurge` 和 `maxUnavailable`，可以作为
    Deployment 的 `strategy` 属性的 `rollingUpdate` 子属性的一部分进行设置，如下面的列表所示。
- en: Listing 9.10\. Specifying parameters for the `rollingUpdate` strategy
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10\. 指定 `rollingUpdate` 策略的参数
- en: '`spec:   strategy:     rollingUpdate:       maxSurge: 1       maxUnavailable:
    0     type: RollingUpdate`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec:   strategy:     rollingUpdate:       maxSurge: 1       maxUnavailable:
    0     type: RollingUpdate`'
- en: What these properties do is explained in [table 9.2](#filepos928491).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性的作用在 [表 9.2](#filepos928491) 中解释。
- en: Table 9.2\. Properties for configuring the rate of the rolling update
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.2\. 配置滚动更新速率的属性
- en: '| Property | What it does |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 它的作用 |'
- en: '| maxSurge | Determines how many pod instances you allow to exist above the
    desired replica count configured on the Deployment. It defaults to 25%, so there
    can be at most 25% more pod instances than the desired count. If the desired replica
    count is set to four, there will never be more than five pod instances running
    at the same time during an update. When converting a percentage to an absolute
    number, the number is rounded up. Instead of a percentage, the value can also
    be an absolute value (for example, one or two additional pods can be allowed).
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| maxSurge | 确定你允许存在的 pod 实例数量，这些实例的数量超过了在 Deployment 上配置的期望副本数。默认值为 25%，因此
    pod 实例的数量最多可以比期望的数量多 25%。如果期望的副本数设置为四个，则在更新过程中运行的所有 pod 实例的数量永远不会超过五个。将百分比转换为绝对数时，数值将向上取整。除了百分比之外，该值也可以是绝对值（例如，可以允许一个或两个额外的
    pod）。|'
- en: '| maxUnavailable | Determines how many pod instances can be unavailable relative
    to the desired replica count during the update. It also defaults to 25%, so the
    number of available pod instances must never fall below 75% of the desired replica
    count. Here, when converting a percentage to an absolute number, the number is
    rounded down. If the desired replica count is set to four and the percentage is
    25%, only one pod can be unavailable. There will always be at least three pod
    instances available to serve requests during the whole rollout. As with maxSurge,
    you can also specify an absolute value instead of a percentage. |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| maxUnavailable | 确定在更新期间相对于所需副本数量可以不可用的Pod实例数量。它也默认为25%，因此可用的Pod实例数量必须始终不低于所需副本数量的75%。在这里，将百分比转换为绝对数时，数值会向下取整。如果所需的副本数量设置为四个且百分比为25%，则只能有一个Pod不可用。在整个滚动过程中，始终至少有三个Pod实例可用于处理请求。与maxSurge一样，您也可以指定绝对值而不是百分比。|'
- en: Because the desired replica count in your case was three, and both these properties
    default to 25%, `maxSurge` allowed the number of all pods to reach four, and `maxUnavailable`
    disallowed having any unavailable pods (in other words, three pods had to be available
    at all times). This is shown in [figure 9.12](#filepos930521).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您的情况中所需的副本数量是三个，并且这两个属性默认为25%，`maxSurge`允许所有Pod的数量达到四个，而`maxUnavailable`不允许有任何不可用的Pod（换句话说，必须始终有三个Pod可用）。这如图9.12所示。[#filepos930521](#filepos930521)。
- en: Figure 9.12\. Rolling update of a Deployment with three replicas and default
    `maxSurge` and `maxUnavailable`
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12\. 具有三个副本和默认`maxSurge`和`maxUnavailable`的部署滚动更新
- en: '![](images/00110.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00110.jpg)'
- en: Understanding the maxUnavailable property
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 理解maxUnavailable属性
- en: The `extensions/v1beta1` version of Deployments uses different defaults—it sets
    both `maxSurge` and `maxUnavailable` to `1` instead of `25%`. In the case of three
    replicas, `maxSurge` is the same as before, but `maxUnavailable` is different
    (1 instead of 0). This makes the rollout process unwind a bit differently, as
    shown in [figure 9.13](#filepos931452).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Deployments的`extensions/v1beta1`版本使用不同的默认值——它将`maxSurge`和`maxUnavailable`都设置为`1`而不是`25%`。在三个副本的情况下，`maxSurge`与之前相同，但`maxUnavailable`不同（1而不是0）。这使得滚动过程有所不同，如图9.13所示。[#filepos931452](#filepos931452)。
- en: Figure 9.13\. Rolling update of a Deployment with the `maxSurge=1` and `maxUnavailable=1`
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13\. 具有`maxSurge=1`和`maxUnavailable=1`的部署滚动更新
- en: '![](images/00130.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00130.jpg)'
- en: In this case, one replica can be unavailable, so if the desired replica count
    is three, only two of them need to be available. That’s why the rollout process
    immediately deletes one pod and creates two new ones. This ensures two pods are
    available and that the maximum number of pods isn’t exceeded (the maximum is four
    in this case—three plus one from `maxSurge`). As soon as the two new pods are
    available, the two remaining old pods are deleted.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个副本可以不可用，因此如果所需的副本数量是三个，则只需要两个副本可用。这就是为什么滚动过程会立即删除一个Pod并创建两个新的Pod。这确保了有两个Pod可用，并且Pod的最大数量不会超过（在这种情况下最大数量是四个——三个加上`maxSurge`中的一个）。一旦两个新的Pod可用，剩余的两个旧Pod就会被删除。
- en: This is a bit hard to grasp, especially since the `maxUnavailable` property
    leads you to believe that that’s the maximum number of unavailable pods that are
    allowed. If you look at the previous figure closely, you’ll see two unavailable
    pods in the second column even though `maxUnavailable` is set to 1\.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点难以理解，特别是由于`maxUnavailable`属性会让你认为这是允许的最大不可用Pod数量。如果你仔细观察前面的图，你会看到第二列中有两个不可用的Pod，尽管`maxUnavailable`设置为1。
- en: It’s important to keep in mind that `maxUnavailable` is relative to the desired
    replica count. If the replica count is set to three and `maxUnavailable` is set
    to one, that means that the update process must always keep at least two (3 minus
    1) pods available, while the number of pods that aren’t available can exceed one.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，`maxUnavailable`是相对于所需副本数量的。如果副本数量设置为三个且`maxUnavailable`设置为一个，这意味着更新过程必须始终至少保持两个Pod（3减去1）可用，而不可用的Pod数量可以超过一个。
- en: 9.3.5\. Pausing the rollout process
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3.5\. 暂停滚动过程
- en: After the bad experience with version 3 of your app, imagine you’ve now fixed
    the bug and pushed version 4 of your image. You’re a little apprehensive about
    rolling it out across all your pods the way you did before. What you want is to
    run a single `v4` pod next to your existing `v2` pods and see how it behaves with
    only a fraction of all your users. Then, once you’re sure everything’s okay, you
    can replace all the old pods with new ones.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在你应用版本3的糟糕体验之后，想象你已经修复了bug并推送了镜像的版本4。你对以之前的方式将版本4滚动发布到所有Pod上有些担忧。你希望的是运行一个单独的`v4`
    Pod，与现有的`v2` Pods并排运行，并观察它只与所有用户中的一小部分用户交互的表现。然后，一旦你确认一切正常，你可以用新的Pod替换所有旧的Pod。
- en: You could achieve this by running an additional pod either directly or through
    an additional Deployment, ReplicationController, or ReplicaSet, but you do have
    another option available on the Deployment itself. A Deployment can also be paused
    during the rollout process. This allows you to verify that everything is fine
    with the new version before proceeding with the rest of the rollout.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行一个额外的Pod，无论是直接运行还是通过额外的Deployment、ReplicationController或ReplicaSet来实现，但你还有在Deployment本身上可用的另一个选项。在滚动发布过程中，Deployment也可以被暂停。这允许你在继续其他滚动发布之前验证新版本是否一切正常。
- en: Pausing the rollout
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 暂停滚动发布
- en: 'I’ve prepared the `v4` image, so go ahead and trigger the rollout by changing
    the image to `luksa/kubia:v4`, but then immediately (within a few seconds) pause
    the rollout:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经准备好了`v4`镜像，所以请继续操作，通过将镜像改为`luksa/kubia:v4`来触发滚动发布，但随后立即（在几秒钟内）暂停滚动发布：
- en: '`$ kubectl set image deployment kubia nodejs=luksa/kubia:v4` `deployment "kubia"
    image updated` `$ kubectl rollout pause deployment kubia` `deployment "kubia"
    paused`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl set image deployment kubia nodejs=luksa/kubia:v4` `deployment "kubia"
    image updated` `$ kubectl rollout pause deployment kubia` `deployment "kubia"
    paused`'
- en: A single new pod should have been created, but all original pods should also
    still be running. Once the new pod is up, a part of all requests to the service
    will be redirected to the new pod. This way, you’ve effectively run a canary release.
    A canary release is a technique for minimizing the risk of rolling out a bad version
    of an application and it affecting all your users. Instead of rolling out the
    new version to everyone, you replace only one or a small number of old pods with
    new ones. This way only a small number of users will initially hit the new version.
    You can then verify whether the new version is working fine or not and then either
    continue the rollout across all remaining pods or roll back to the previous version.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 应该已经创建了一个新的Pod，但所有原始Pod也应该仍然在运行。一旦新的Pod启动，所有请求服务的一部分将被重定向到新的Pod。这样，你实际上已经运行了一个金丝雀发布。金丝雀发布是一种最小化发布不良版本的应用程序风险的技术，它不会影响到所有用户。而不是将新版本滚动发布给所有人，你只替换一个或少数几个旧的Pod为新Pod。这样，最初只有少数用户会接触到新版本。然后你可以验证新版本是否运行正常，然后继续在所有剩余的Pod上滚动发布或回滚到上一个版本。
- en: Resuming the rollout
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复滚动发布
- en: 'In your case, by pausing the rollout process, only a small portion of client
    requests will hit your `v4` pod, while most will still hit the `v3` pods. Once
    you’re confident the new version works as it should, you can resume the deployment
    to replace all the old pods with new ones:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的情况下，通过暂停滚动发布过程，只有一小部分客户端请求会击中你的`v4` Pod，而大多数请求仍然会击中`v3` Pods。一旦你确信新版本按预期工作，你可以恢复部署以用新的Pod替换所有旧的Pod：
- en: '`$ kubectl rollout resume deployment kubia` `deployment "kubia" resumed`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout resume deployment kubia` `deployment "kubia" resumed`'
- en: Obviously, having to pause the deployment at an exact point in the rollout process
    isn’t what you want to do. In the future, a new upgrade strategy may do that automatically,
    but currently, the proper way of performing a canary release is by using two different
    Deployments and scaling them appropriately.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在滚动发布过程中在某个确切点暂停部署不是你想要的。将来，新的升级策略可能会自动执行此操作，但当前，执行金丝雀发布的正确方式是使用两个不同的Deployment并相应地扩展它们。
- en: Using the pause feature to prevent rollouts
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用暂停功能来防止滚动发布
- en: Pausing a Deployment can also be used to prevent updates to the Deployment from
    kicking off the rollout process, allowing you to make multiple changes to the
    Deployment and starting the rollout only when you’re done making all the necessary
    changes. Once you’re ready for changes to take effect, you resume the Deployment
    and the rollout process will start.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 暂停部署也可以用来防止对部署的更新启动滚动过程，这样你可以对部署进行多次更改，并在完成所有必要的更改后才开始滚动。
- en: '|  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a Deployment is paused, the `undo` command won’t undo it until you resume
    the Deployment.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个 Deployment 被暂停，`undo` 命令不会撤销它，直到你恢复 Deployment。
- en: '|  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 9.3.6\. Blocking rollouts of bad versions
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 9.3.6\. 阻止坏版本滚动
- en: Before you conclude this chapter, we need to discuss one more property of the
    Deployment resource. Remember the `minReadySeconds` property you set on the Deployment
    at the beginning of [section 9.3.2](#filepos906222)? You used it to slow down
    the rollout, so you could see it was indeed performing a rolling update and not
    replacing all the pods at once. The main function of `minReadySeconds` is to prevent
    deploying malfunctioning versions, not slowing down a deployment for fun.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在你完成这一章之前，我们需要讨论 Deployment 资源的一个更多属性。记得你在 [9.3.2 节](#filepos906222) 开始时设置的
    `minReadySeconds` 属性吗？你用它来减慢滚动速度，这样你可以看到它确实是在执行滚动更新，而不是一次性替换所有 pod。`minReadySeconds`
    的主要功能是防止部署故障版本，而不是为了好玩而减慢部署。
- en: Understanding the applicability of minReadySeconds
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 minReadySeconds 的适用性
- en: The `minReadySeconds` property specifies how long a newly created pod should
    be ready before the pod is treated as available. Until the pod is available, the
    rollout process will not continue (remember the `maxUnavailable` property?). A
    pod is ready when readiness probes of all its containers return a success. If
    a new pod isn’t functioning properly and its readiness probe starts failing before
    `minReadySeconds` have passed, the rollout of the new version will effectively
    be blocked.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`minReadySeconds` 属性指定新创建的 pod 在被视为可用之前应该就绪多长时间。在 pod 可用之前，滚动过程将不会继续（记得 `maxUnavailable`
    属性吗？）。当 pod 的所有容器的就绪检查都返回成功时，pod 就处于就绪状态。如果一个新 pod 运行不正常，并且在其就绪检查在 `minReadySeconds`
    过去之前开始失败，新版本的滚动将实际上被阻止。'
- en: You used this property to slow down your rollout process by having Kubernetes
    wait 10 seconds after a pod was ready before continuing with the rollout. Usually,
    you’d set `minReadySeconds` to something much higher to make sure pods keep reporting
    they’re ready after they’ve already started receiving actual traffic.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用这个属性通过让 Kubernetes 在 pod 就绪后等待 10 秒再继续滚动来减慢你的滚动过程。通常，你会将 `minReadySeconds`
    设置得更高，以确保 pod 在开始接收实际流量后继续报告它们处于就绪状态。
- en: Although you should obviously test your pods both in a test and in a staging
    environment before deploying them into production, using `minReadySeconds` is
    like an airbag that saves your app from making a big mess after you’ve already
    let a buggy version slip into production.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你显然应该在将 pod 部署到生产环境之前在测试和预发布环境中对其进行测试，但使用 `minReadySeconds` 就像安全气囊一样，在你已经让有缺陷的版本进入生产后，可以防止你的应用程序造成大混乱。
- en: With a properly configured readiness probe and a proper `minReadySeconds` setting,
    Kubernetes would have prevented us from deploying the buggy `v3` version earlier.
    Let me show you how.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正确配置就绪检查和适当的 `minReadySeconds` 设置，Kubernetes 本可以阻止我们更早地部署有缺陷的 `v3` 版本。让我给你展示一下。
- en: Defining a readiness probe to prevent our v3 version from being rolled out fully
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 定义就绪检查以防止我们的 v3 版本完全滚动
- en: You’re going to deploy version `v3` again, but this time, you’ll have the proper
    readiness probe defined on the pod. Your Deployment is currently at version `v4`,
    so before you start, roll back to version `v2` again so you can pretend this is
    the first time you’re upgrading to `v3`. If you wish, you can go straight from
    `v4` to `v3`, but the text that follows assumes you returned to `v2` first.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你将再次部署版本 `v3`，但这次，你将在 pod 上定义适当的就绪检查。你的 Deployment 当前处于版本 `v4`，所以在开始之前，再次回滚到版本
    `v2`，这样你可以假装这是你第一次升级到 `v3`。如果你愿意，你可以直接从 `v4` 跳到 `v3`，但接下来的文本假设你首先回到了 `v2`。
- en: Unlike before, where you only updated the image in the pod template, you’re
    now also going to introduce a readiness probe for the container at the same time.
    Up until now, because there was no explicit readiness probe defined, the container
    and the pod were always considered ready, even if the app wasn’t truly ready or
    was returning errors. There was no way for Kubernetes to know that the app was
    malfunctioning and shouldn’t be exposed to clients.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前不同，之前你只更新了pod模板中的图像，现在你还将同时为容器引入就绪性检查。到目前为止，因为没有定义明确的就绪性检查，容器和pod始终被认为是就绪的，即使应用程序实际上并没有准备好或者正在返回错误。Kubernetes无法知道应用程序正在出现故障，不应该暴露给客户端。
- en: To change the image and introduce the readiness probe at once, you’ll use the
    `kubectl apply` command. You’ll use the following YAML to update the deployment
    (you’ll store it as `kubia-deployment-v3-with-readinesscheck.yaml`), as shown
    in the following listing.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 要一次性更改图像并引入就绪性检查，你将使用 `kubectl apply` 命令。你将使用以下 YAML 来更新部署（你将把它存储为 `kubia-deployment-v3-with-readinesscheck.yaml`），如下所示。
- en: 'Listing 9.11\. Deployment with a readiness probe: kubia-deployment-v3-with-readinesscheck.yaml'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.11\. 带有就绪性检查的 Deployment：kubia-deployment-v3-with-readinesscheck.yaml
- en: '`apiVersion: apps/v1beta1 kind: Deployment metadata:   name: kubia spec:  
    replicas: 3   minReadySeconds: 10` `1` `strategy:     rollingUpdate:       maxSurge:
    1       maxUnavailable: 0` `2` `type: RollingUpdate   template:     metadata:
          name: kubia       labels:         app: kubia     spec:       containers:
          - image: luksa/kubia:v3         name: nodejs         readinessProbe:          
    periodSeconds: 1` `3` `httpGet:` `4` `path: /` `4` `port: 8080` `4`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: apps/v1beta1 kind: Deployment metadata: name: kubia spec: replicas:
    3 minReadySeconds: 10` `1` `strategy: rollingUpdate: maxSurge: 1 maxUnavailable:
    0` `2` `type: RollingUpdate template: metadata: name: kubia labels: app: kubia
    spec: containers: - image: luksa/kubia:v3 name: nodejs readinessProbe: periodSeconds:
    1` `3` `httpGet:` `4` `path: /` `4` `port: 8080` `4`'
- en: 1 You’re keeping minReadySeconds set to 10\.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 你将保持 minReadySeconds 设置为 10。
- en: 2 You’re keeping maxUnavailable set to 0 to make the deployment replace pods
    one by one
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 你将保持 maxUnavailable 设置为 0，以便部署逐个替换 pod
- en: 3 You’re defining a readiness probe that will be executed every second.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 你正在定义一个每秒执行的就绪性检查。
- en: 4 The readiness probe will perform an HTTP GET request against our container.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 就绪性检查将对我们的容器执行 HTTP GET 请求。
- en: Updating a Deployment with kubectl apply
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 kubectl apply 更新 Deployment
- en: 'To update the Deployment this time, you’ll use `kubectl apply` like this:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 要更新 Deployment，这次你将使用 `kubectl apply` 如下所示：
- en: '`$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml` `deployment
    "kubia" configured`'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml` `部署 "kubia"
    已配置`'
- en: The `apply` command updates the Deployment with everything that’s defined in
    the YAML file. It not only updates the image but also adds the readiness probe
    definition and anything else you’ve added or modified in the YAML. If the new
    YAML also contains the `replicas` field, which doesn’t match the number of replicas
    on the existing Deployment, the apply operation will also scale the Deployment,
    which isn’t usually what you want.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply` 命令更新 Deployment，使其包含 YAML 文件中定义的所有内容。它不仅更新了图像，还添加了就绪性检查定义以及你在 YAML
    中添加或修改的任何其他内容。如果新的 YAML 也包含与现有 Deployment 上副本数量不匹配的 `replicas` 字段，apply 操作也会扩展
    Deployment，这通常不是你想要的。'
- en: '|  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To keep the desired replica count unchanged when updating a Deployment with
    `kubectl apply`, don’t include the `replicas` field in the YAML.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要在更新 Deployment 时保持期望的副本数量不变，不要在 YAML 中包含 `replicas` 字段。
- en: '|  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Running the `apply` command will kick off the update process, which you can
    again follow with the `rollout status` command:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `apply` 命令将启动更新过程，你可以再次使用 `rollout status` 命令来跟踪：
- en: '`$ kubectl rollout status deployment kubia` `Waiting for rollout to finish:
    1 out of 3 new replicas have been updated...`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout status deployment kubia` `等待滚动更新完成：3 个新副本中有 1 个已更新...`'
- en: 'Because the status says one new pod has been created, your service should be
    hitting it occasionally, right? Let’s see:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因为状态显示已创建一个新 pod，你的服务应该偶尔会访问它，对吧？让我们看看：
- en: '`$ while true; do curl http://130.211.109.222; done` `This is v2 running in
    pod kubia-1765119474-jvslk This is v2 running in pod kubia-1765119474-jvslk This
    is v2 running in pod kubia-1765119474-xk5g3 This is v2 running in pod kubia-1765119474-pmb26
    This is v2 running in pod kubia-1765119474-pmb26 This is v2 running in pod kubia-1765119474-xk5g3
    ...`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ while true; do curl http://130.211.109.222; done` `This is v2 running in
    pod kubia-1765119474-jvslk This is v2 running in pod kubia-1765119474-jvslk This
    is v2 running in pod kubia-1765119474-xk5g3 This is v2 running in pod kubia-1765119474-pmb26
    This is v2 running in pod kubia-1765119474-pmb26 This is v2 running in pod kubia-1765119474-xk5g3
    ...`'
- en: 'Nope, you never hit the `v3` pod. Why not? Is it even there? List the pods:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，你从未触碰到`v3` Pod。为什么没有？它甚至存在吗？列出Pod：
- en: '`$ kubectl get po` `NAME                     READY     STATUS    RESTARTS  
    AGE kubia-1163142519-7ws0i   0/1       Running   0          30s kubia-1765119474-jvslk  
    1/1       Running   0          9m kubia-1765119474-pmb26   1/1       Running  
    0          9m kubia-1765119474-xk5g3   1/1       Running   0          8m`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME                     READY     STATUS    RESTARTS  
    AGE kubia-1163142519-7ws0i   0/1       运行中   0          30秒 kubia-1765119474-jvslk  
    1/1       运行中   0          9分钟 kubia-1765119474-pmb26   1/1       运行中   0         
    9分钟 kubia-1765119474-xk5g3   1/1       运行中   0          8分钟`'
- en: Aha! There’s your problem (or as you’ll learn soon, your blessing)! The pod
    is shown as not ready, but I guess you’ve been expecting that, right? What has
    happened?
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！这就是你的问题（或者正如你很快就会学到的那样，你的祝福）！Pod显示为未就绪，但你是不是一直都在期待这个结果，对吧？发生了什么？
- en: Understanding how a readiness probe prevents bad versions from being rolled
    out
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 理解就绪检查如何防止不良版本被部署
- en: As soon as your new pod starts, the readiness probe starts being hit every second
    (you set the probe’s interval to one second in the pod spec). On the fifth request
    the readiness probe began failing, because your app starts returning HTTP status
    code 500 from the fifth request onward.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的新Pod开始运行，就绪检查每秒就会开始被触发（你在Pod规范中将检查的间隔设置为1秒）。在第五次请求时，就绪检查开始失败，因为你的应用程序从第五次请求开始返回HTTP状态码500。
- en: As a result, the pod is removed as an endpoint from the service (see [figure
    9.14](#filepos946671)). By the time you start hitting the service in the `curl`
    loop, the pod has already been marked as not ready. This explains why you never
    hit the new pod with `curl`. And that’s exactly what you want, because you don’t
    want clients to hit a pod that’s not functioning properly.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Pod被从服务中移除作为端点（见[图9.14](#filepos946671)）。在你开始通过`curl`循环访问服务之前，Pod已经被标记为未就绪。这解释了为什么你从未用`curl`触碰到新Pod。这正是你想要的，因为你不希望客户端触碰到一个未正确运行的Pod。
- en: Figure 9.14\. Deployment blocked by a failing readiness probe in the new pod
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14\. 新Pod中因失败的就绪检查而阻止部署
- en: '![](images/00147.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00147.jpg)'
- en: But what about the rollout process? The `rollout status` command shows only
    one new replica has started. Thankfully, the rollout process will not continue,
    because the new pod will never become available. To be considered available, it
    needs to be ready for at least 10 seconds. Until it’s available, the rollout process
    will not create any new pods, and it also won’t remove any original pods because
    you’ve set the `maxUnavailable` property to 0\.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 但关于滚动部署过程呢？`rollout status`命令只显示有一个新的副本已启动。幸运的是，滚动部署过程不会继续，因为新的Pod永远不会变得可用。要被视为可用，它至少需要就绪10秒钟。在它就绪之前，滚动部署过程不会创建任何新的Pod，也不会移除任何原始Pod，因为你已经将`maxUnavailable`属性设置为0。
- en: The fact that the deployment is stuck is a good thing, because if it had continued
    replacing the old pods with the new ones, you’d end up with a completely non-working
    service, like you did when you first rolled out version 3, when you weren’t using
    the readiness probe. But now, with the readiness probe in place, there was virtually
    no negative impact on your users. A few users may have experienced the internal
    server error, but that’s not as big of a problem as if the rollout had replaced
    all pods with the faulty version 3\.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 部署停滞不前是个好事，因为如果它继续用新Pod替换旧Pod，你最终会得到一个完全无法工作的服务，就像你最初推出版本3时那样，当时你没有使用就绪检查。但现在，由于就绪检查已经到位，对用户几乎没有负面影响。可能有一些用户遇到了内部服务器错误，但这并不像如果部署替换了所有Pod为有缺陷的版本3那样成为一个大问题。
- en: '|  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you only define the readiness probe without setting `minReadySeconds` properly,
    new pods are considered available immediately when the first invocation of the
    readiness probe succeeds. If the readiness probe starts failing shortly after,
    the bad version is rolled out across all pods. Therefore, you should set `minReadySeconds`
    appropriately.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只定义了就绪探测而没有正确设置 `minReadySeconds`，则新 pod 在就绪探测第一次调用成功时立即被视为可用。如果就绪探测在之后很快开始失败，则不良版本将在所有
    pod 上滚动。因此，你应该适当地设置 `minReadySeconds`。
- en: '|  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Configuring a deadline for the rollout
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为滚动更新配置截止日期
- en: By default, after the rollout can’t make any progress in 10 minutes, it’s considered
    as failed. If you use the `kubectl describe` deployment command, you’ll see it
    display a `ProgressDeadlineExceeded` condition, as shown in the following listing.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果在 10 分钟内滚动更新无法取得任何进展，则被视为失败。如果你使用 `kubectl describe` 部署命令，你会看到它显示一个
    `ProgressDeadlineExceeded` 条件，如下面的列表所示。
- en: Listing 9.12\. Seeing the conditions of a Deployment with `kubectl describe`
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.12\. 使用 `kubectl describe` 查看部署的条件
- en: '`$ kubectl describe deploy kubia` `Name:                   kubia ... Conditions:
      Type          Status  Reason   ----          ------  ------   Available    
    True    MinimumReplicasAvailable   Progressing   False   ProgressDeadlineExceeded`
    `1`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe deploy kubia` `Name:                   kubia ... Conditions:
      Type          Status  Reason    ----          ------  ------    Available    
    True    MinimumReplicasAvailable    Progressing   False    ProgressDeadlineExceeded`
    `1`'
- en: 1 The Deployment took too long to make progress.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 部署取得进展的时间过长。
- en: The time after which the Deployment is considered failed is configurable through
    the `progressDeadlineSeconds` property in the Deployment spec.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 部署被视为失败的时间可以通过部署规范中的 `progressDeadlineSeconds` 属性进行配置。
- en: '|  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `extensions/v1beta1` version of Deployments doesn’t set a deadline.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: Deployments 的 `extensions/v1beta1` 版本没有设置截止日期。
- en: '|  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Aborting a bad rollout
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 终止不良的滚动更新
- en: 'Because the rollout will never continue, the only thing to do now is abort
    the rollout by undoing it:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 由于滚动更新永远不会继续，现在唯一要做的就是通过撤销操作来终止滚动更新：
- en: '`$ kubectl rollout undo deployment kubia` `deployment "kubia" rolled back`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl rollout undo deployment kubia` `deployment "kubia" rolled back`'
- en: '|  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In future versions, the rollout will be aborted automatically when the time
    specified in `progressDeadlineSeconds` is exceeded.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的版本中，当超过 `progressDeadlineSeconds` 中指定的时间时，滚动更新将自动终止。
- en: '|  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 9.4\. Summary
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 9.4\. 摘要
- en: This chapter has shown you how to make your life easier by using a declarative
    approach to deploying and updating applications in Kubernetes. Now that you’ve
    read this chapter, you should know how to
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了如何通过使用声明性方法在 Kubernetes 中部署和更新应用程序来使您的生活更加轻松。现在您已经阅读了本章，您应该知道如何
- en: Perform a rolling update of pods managed by a ReplicationController
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对由 ReplicationController 管理的 pod 执行滚动更新
- en: Create Deployments instead of lower-level ReplicationControllers or ReplicaSets
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建部署而不是较低级别的 ReplicationControllers 或 ReplicaSets
- en: Update your pods by editing the pod template in the Deployment specification
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过编辑部署规范中的 pod 模板来更新你的 pod
- en: Roll back a Deployment either to the previous revision or to any earlier revision
    still listed in the revision history
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将部署回滚到上一个修订版或回滚到修订历史中列出的任何较早修订版
- en: Abort a Deployment mid-way
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在中途终止部署
- en: Pause a Deployment to inspect how a single instance of the new version behaves
    in production before allowing additional pod instances to replace the old ones
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暂停部署以检查新版本的单个实例在生产中的行为，然后再允许额外的 pod 实例替换旧版本
- en: Control the rate of the rolling update through `maxSurge` and `maxUnavailable`
    properties
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `maxSurge` 和 `maxUnavailable` 属性控制滚动更新的速率
- en: Use `minReadySeconds` and readiness probes to have the rollout of a faulty version
    blocked automatically
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `minReadySeconds` 和就绪探测来自动阻止有缺陷版本的滚动
- en: In addition to these Deployment-specific tasks, you also learned how to
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些特定的部署任务外，你还学习了如何
- en: Use three dashes as a separator to define multiple resources in a single YAML
    file
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用三个短横线作为分隔符，在单个 YAML 文件中定义多个资源
- en: Turn on `kubectl`’s verbose logging to see exactly what it’s doing behind the
    curtains
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开 `kubectl` 的详细日志记录功能，以查看它幕后正在做什么
- en: You now know how to deploy and manage sets of pods created from the same pod
    template and thus share the same persistent storage. You even know how to update
    them declaratively. But what about running sets of pods, where each instance needs
    to use its own persistent storage? We haven’t looked at that yet. That’s the subject
    of our next chapter.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经知道了如何部署和管理由相同的 pod 模板创建的 pod 集合，并且它们共享相同的持久存储。你甚至知道如何声明式地更新它们。但对于需要每个实例使用自己持久存储的
    pod 集合的运行，我们还没有探讨过。这正是我们下一章的主题。
