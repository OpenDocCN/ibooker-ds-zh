- en: 'Part 2\. Get proficient: Translate your ideas into code'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分\. 精通：将你的想法转化为代码
- en: With two different kind of programs under your belt, it’s time to expand our
    horizons. Part 2 is about diversifying your set of tools so that no data set will
    have a secret for you.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握两种不同类型的程序之后，是时候拓展我们的视野了。第二部分是关于多样化你的工具集，这样就没有任何数据集会对你保密。
- en: Chapter 6 breaks the rows and columns mold to go multidimensional. Through JSON
    data, we build data frames that contain data frames themselves. This tool catapults
    the versatility of the Spark data frame to completely new horizons.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章打破了行和列的模具，走向多维。通过JSON数据，我们构建了包含数据框的数据框。这个工具将Spark数据框的通用性推进到了全新的境界。
- en: Chapter 7 introduces PySpark and SQL together. Together, they unlock a new level
    of expressiveness and succinctness in your code, allow you to scale SQL workflows
    at record speed, and provide a new way to reason about your analyses.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章介绍了PySpark和SQL。它们一起解锁了代码的新层次的表达性和简洁性，让你能够以创纪录的速度扩展SQL工作流程，并提供一种新的方式来推理你的分析。
- en: Chapters 8 and 9 cover going full Python with your PySpark code. From the resilient
    distributed data set, a flexible and scalable data structure, to two flavors of
    UDF using Python and pandas, you’ll turbocharge your capabilities with full confidence.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章和第9章涵盖了使用PySpark代码全面使用Python。从弹性分布式数据集，一种灵活可扩展的数据结构，到使用Python和pandas的两种UDF风味，你将充满信心地提升你的能力。
- en: Chapter 10 provides a new angle on your data through the introduction of window
    functions. Window functions are one of those things that make ordered data so
    much easier to work with that you’ll wonder how anyone can do without them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章通过引入窗口函数为你提供数据的新视角。窗口函数是那些使有序数据更容易处理的事情之一，你会 wonder how anyone can do without
    them。
- en: Finally, chapter 11 takes a break from all that coding to reflect on Spark’s
    execution model. You’ll check under the hood through the Spark UI and better understand
    how your instructions are being processed by the engine.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第11章从所有的编码中暂时抽身，来反思Spark的执行模型。你将通过Spark UI深入了解，更好地理解你的指令是如何被引擎处理的。
- en: At the end of part 2, you should be able to map a clear path from data to insight,
    with a full toolbox at your disposal to bend your data to your will.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分的结尾，你应该能够从数据到洞察力绘制一条清晰的路径，拥有一个完整的工具箱来按照你的意愿弯曲你的数据。
