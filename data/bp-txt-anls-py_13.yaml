- en: Chapter 13\. Using Text Analytics in Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。在生产中使用文本分析
- en: We have introduced several blueprints so far and understood their application
    to multiple use cases. Any analysis or machine learning model is most valuable
    when it can be used easily by others. In this chapter, we will provide blueprints
    that will allow you to share the text classifier from one of our earlier chapters
    and also deploy to a cloud environment allowing anybody to make use of what we’ve
    built.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了几个蓝图，并了解了它们在多个用例中的应用。任何分析或机器学习模型在其他人能够轻松使用时，才能发挥其最大价值。在本章中，我们将提供几个蓝图，让您可以共享我们早期章节中的文本分类器，并部署到云环境中，使任何人都能使用我们构建的内容。
- en: Consider that you used one of the blueprints in [Chapter 10](ch10.xhtml#ch-embeddings)
    in this book to analyze various car models using data from Reddit. If one of your
    colleagues was interested in doing the same analysis for the motorcycle industry,
    it should be simple to change the data source and reuse the code. In practice,
    this can prove much more difficult because your colleague will first have to set
    up an environment similar to the one you used by installing the same version of
    Python and all the required packages. It’s possible that they might be working
    on a different operating system where the installation steps are different. Or
    consider that the clients to whom you presented the analysis are extremely happy
    and come back three months later asking you to cover many more industries. Now
    you have to repeat the same analysis but ensure that the code and environment
    remain the same. The volume of data for this analysis could be much larger, and
    your system resources may not be sufficient enough, prompting a move to use cloud
    computing resources. You would have to go through the installation steps on a
    cloud provider, and this can quickly become time-consuming.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在本书的[第10章](ch10.xhtml#ch-embeddings)中使用了一个蓝图来分析来自 Reddit 的各种汽车型号数据。如果您的同事对于使用同样的分析方法来研究摩托车行业感兴趣，修改数据源并重复使用代码应该是简单的。实际情况可能更为复杂，因为您的同事首先必须设置一个类似于您使用的环境，安装相同版本的
    Python 和所有必需的软件包。他们可能使用的是不同的操作系统，安装步骤也会有所不同。或者考虑到您向分析的客户展示时非常满意，三个月后他们回来要求您覆盖更多行业。现在，您必须重复相同的分析，但确保代码和环境保持不变。这次分析的数据量可能更大，您的系统资源可能不足，促使您使用云计算资源。您必须在云服务提供商上执行安装步骤，这可能会迅速变得耗时。
- en: What You’ll Learn and What We’ll Build
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么，我们将构建什么
- en: Often what happens is that you are able to produce excellent results, but they
    remain unusable because other colleagues who want to use them are unable to rerun
    the code and reproduce the results. In this chapter, we will show you some techniques
    that can ensure that your analysis or algorithm can be easily repeated by anyone
    else, including yourself at a later stage. What if we are able to make it even
    easier for others to use the output of our analysis? This removes an additional
    barrier and increases the accessibility of our results. We will show you how you
    can deploy your machine learning model as a simple REST API that allows anyone
    to use the predictions from your model in their own work or applications. Finally,
    we will show you how to make use of cloud infrastructure for faster runtimes or
    to serve multiple applications and users. Since most production servers and services
    run Linux, this chapter includes a lot of executable commands and instructions
    that run best in a Linux shell or terminal. However, they should work just as
    well in Windows PowerShell.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 经常发生的情况是，您能够产生出色的结果，但由于其他希望使用它们的同事无法重新运行代码和复现结果，因此这些结果无法使用。在本章中，我们将向您展示一些技术，确保您的分析或算法可以轻松被任何人重复使用，包括您自己在以后的阶段。如果我们能够让其他人更轻松地使用我们分析的输出会怎样？这消除了一个额外的障碍，并增加了我们结果的可访问性。我们将向您展示如何将您的机器学习模型部署为简单的
    REST API，允许任何人在其自己的工作或应用程序中使用您模型的预测。最后，我们将向您展示如何利用云基础设施实现更快的运行时或为多个应用程序和用户提供服务。由于大多数生产服务器和服务运行在
    Linux 上，本章包含许多在 Linux shell 或终端中运行最佳的可执行命令和指令。但是，它们在 Windows PowerShell 中同样有效。
- en: 'Blueprint: Using Conda to Create Reproducible Python Environments'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用 Conda 创建可重现的 Python 环境
- en: The blueprints introduced in this book use Python and the ecosystem of packages
    to accomplish several text analytics tasks. As with any programming language,
    [Python](https://python.org/downloads) has frequent updates and many supported
    versions. In addition, commonly used packages like Pandas, NumPy, and SciPy also
    have regular release cycles when they upgrade to a new version. While the maintainers
    try to ensure that newer versions are backward compatible, there is a risk that
    an analysis you completed last year will no longer be able to run with the latest
    version of Python. Your blueprint might have used a method that is deprecated
    in the latest version of a library, and this would make your analysis nonreproducible
    without knowing the version of the library used.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍的蓝图使用 Python 和包生态系统来完成多个文本分析任务。与任何编程语言一样，[Python](https://python.org/downloads)
    经常更新并支持多个版本。此外，像 Pandas、NumPy 和 SciPy 这样的常用包也有定期的发布周期，当它们升级到新版本时。尽管维护者们尽力确保新版本向后兼容，但有可能您去年完成的分析在最新版本的
    Python 下无法运行。您的蓝图可能使用了最新版本库中已弃用的方法，这会导致分析无法重现，除非知道所使用库的版本。
- en: 'Let’s suppose that you share a blueprint with your colleague in the form of
    a Jupyter notebook or a Python module; one of the common errors they might face
    when trying to run is as shown here:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您将蓝图以 Jupyter 笔记本或 Python 模块的形式与同事分享；当他们尝试运行时，可能会遇到如下常见错误：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Out:`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In most cases, `ModuleNotFoundError` can be easily resolved by manually installing
    the required package using the command `**pip install <module_name>**`. But imagine
    having to do this for every nonstandard package! This command also installs the
    latest version, which might not be the one you originally used. As a result, the
    best way to ensure reproducibility is to have a standardized way of sharing the
    Python environment that was used to run the analysis. We make use of the conda
    package manager along with the Miniconda distribution of Python to solve this
    problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，`ModuleNotFoundError` 可以通过手动使用命令 `**pip install <module_name>**` 安装所需的包来轻松解决。但想象一下，对于每个非标准包都要这样做！此命令还会安装最新版本，这可能不是您最初使用的版本。因此，确保可重复性的最佳方法是使用标准化的方式共享用于运行分析的
    Python 环境。我们使用 conda 包管理器以及 Miniconda Python 发行版来解决这个问题。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are several ways to solve the problem of creating and sharing Python environments,
    and conda is just one of them. [pip](https://oreil.ly/Dut6o) is the standard Python
    package installer that is included with Python and is widely used to install Python
    packages. [venv](https://oreil.ly/k5m6A) can be used to create virtual environments
    where each environment can have its own version of Python and set of installed
    packages. Conda combines the functionality of a package installer and environment
    manager and therefore is our preferred option. It’s important to distinguish conda
    from the Anaconda/Miniconda distributions. These distributions include Python
    and conda along with essential packages required for working with data. While
    conda can be installed directly with pip, the easiest way is to install Miniconda,
    which is a small bootstrap version that contains conda, Python, and some essential
    packages they depend on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以解决创建和共享 Python 环境的问题，conda 只是其中之一。[pip](https://oreil.ly/Dut6o) 是标准的
    Python 包安装程序，随 Python 附带，并广泛用于安装 Python 包。[venv](https://oreil.ly/k5m6A) 可用于创建虚拟环境，每个环境可以拥有其自己的
    Python 版本和一组已安装的包。Conda 结合了包安装程序和环境管理器的功能，因此是我们首选的选项。重要的是要区分 conda 和 Anaconda/Miniconda
    发行版。这些发行版包括 Python 和 conda，以及处理数据所需的基本包。虽然 conda 可以直接通过 pip 安装，但最简单的方法是安装 Miniconda，这是一个包含
    conda、Python 和一些必需包的小型引导版本。
- en: 'First, we must install the Miniconda distribution with the [following steps](https://oreil.ly/GZ4b-).
    This will create a base installation containing just Python, conda, and some essential
    packages like `pip`, `zlib`, etc. We can now create separate environments for
    each project that contains only the packages we need and are isolated from other
    such environments. This is useful since any changes you make such as installing
    additional packages or upgrading to a different Python version does not impact
    any other project or application as they use their own environment. We can do
    so by using the following command:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须按照[以下步骤](https://oreil.ly/GZ4b-)安装Miniconda发行版。这将创建一个基本安装，其中只包含Python、conda和一些基本软件包如`pip`、`zlib`等。现在，我们可以为每个项目创建单独的环境，其中只包含我们需要的软件包，并且与其他环境隔离开来。这很有用，因为您所做的任何更改，如安装额外的软件包或升级到不同的Python版本，不会影响任何其他项目或应用程序，因为它们使用自己的环境。可以通过以下命令执行此操作：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Executing the previous command will create a new Python environment with the
    default version that was available when Miniconda was installed the first time.
    Let’s create our environment called `blueprints` where we explicitly specify the
    version of Python and the list of additional packages that we would like to install
    as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的命令将使用Miniconda首次安装时可用的默认版本创建一个新的Python环境。让我们创建一个名为`blueprints`的环境，其中明确指定Python版本和要安装的附加软件包列表如下：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the command has been executed, you can activate it by executing `**conda
    activate <env_name>**`, and you will notice that the command prompt is prefixed
    with the name of the environment. You can further verify that the version of Python
    is the same as you specified:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完命令后，可以通过执行`**conda activate <env_name>**`来激活它，您会注意到命令提示符前缀带有环境名称。您可以进一步验证Python的版本与您指定的版本是否相同：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see the list of all environments on your system by using the command
    `**conda env list**`, as shown next. The output will include the base environment,
    which is the default environment created with the installation of Miniconda. An
    asterisk against a particular environment indicates the currently active one,
    in our case the environment we just created. Please make sure that you continue
    to use this environment when you work on your blueprint:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命令`**conda env list**`可以查看系统中所有环境的列表，如下所示。输出将包括基本环境，这是安装Miniconda时创建的默认环境。特定环境前的星号表示当前活动环境，在我们的例子中是刚刚创建的环境。请确保在制作蓝图时继续使用此环境：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`conda` ensures that each environment can have its own versions of the same
    package, but this could come at the cost of increased storage since the same version
    of each package will be used in more than one environment. This is mitigated to
    a certain extent with the use of hard links, but it may not work in cases where
    a package uses [hard-coded paths](https://oreil.ly/bN8Dl). However, we recommend
    creating another environment when you switch projects. But it is a good practice
    to remove unused environments using the command `**conda remove --name <env_name>
    --all**`.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda`确保每个环境可以拥有其自己版本的相同软件包，但这可能会增加存储成本，因为每个环境中将使用相同版本的每个软件包。使用硬链接在一定程度上可以减少这种影响，但在软件包使用[硬编码路径](https://oreil.ly/bN8Dl)的情况下可能无效。然而，我们建议在切换项目时创建另一个环境。但是，建议使用命令`**conda
    remove --name <env_name> --all**`删除未使用的环境是一个良好的实践。'
- en: 'The advantage of this approach is that when you want to share the code with
    someone else, you can specify the environment in which it should run. You can
    export the environment as a YAML file using the command `**conda env export >
    environment.yml**`. Ensure that you are in the desired environment (by running
    `**conda activate <environment_name>**`) before running this command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点在于，当您想与他人共享代码时，可以指定应运行的环境。可以使用命令`**conda env export > environment.yml**`将环境导出为YAML文件。在运行此命令之前，请确保您处于所需的环境中（通过运行`**conda
    activate <environment_name>**`）：
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As shown in the output, the `environment.yml` file creates a listing of all
    the packages and their dependencies used in the environment. This file can be
    used by anyone to re-create the same environment by running the command `**conda
    env create -f environment.yml**`. However, this method can have cross-platform
    limitations since the dependencies listed in the YAML file are specific to the
    platform. So if you were working on a Windows system and exported the YAML file,
    it may not necessarily work on a macOS system.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，`environment.yml` 文件列出了环境中使用的所有包及其依赖关系。任何人都可以通过运行命令 `**conda env create
    -f environment.yml**` 使用此文件重新创建相同的环境。然而，此方法可能存在跨平台限制，因为 YAML 文件中列出的依赖关系特定于平台。因此，如果您在
    Windows 系统上工作并导出了 YAML 文件，则在 macOS 系统上可能无法正常工作。
- en: 'This is because some of the dependencies required by a Python package are platform-dependent.
    For instance, the [Intel MKL optimizations](https://oreil.ly/ND7_H) are specific
    to a certain architecture and can be replaced with the [OpenBLAS](http://openblas.net)
    library. To provide a generic environment file, we can use the command `**conda
    env export --from-history > environment.yml**`, which generates a listing of only
    the packages that were explicitly requested by you. You can see the following
    output of running this command, which lists only the packages we installed when
    creating the environment. Contrast this with the previous environment file that
    also listed packages like `attrs` and `backcall`, which are part of the conda
    environment but not requested by us. When such a YAML file is used to create an
    environment on a new platform, the default packages and their platform-specific
    dependencies will be identified and installed automatically by conda. In addition,
    the packages that we explicitly specified and their dependencies will be installed:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 Python 包所需的某些依赖项取决于平台。例如，[Intel MKL 优化](https://oreil.ly/ND7_H) 是特定于某种架构的，可以用
    [OpenBLAS](http://openblas.net) 库来替代。为了提供一个通用的环境文件，我们可以使用命令 `**conda env export
    --from-history > environment.yml**`，它只生成您明确请求的包的列表。运行此命令的输出如下，列出了我们在创建环境时安装的包。与先前的环境文件相比，前者还列出了像
    `attrs` 和 `backcall` 这样的包，它们是 conda 环境的一部分，但并非我们请求的。当在新平台上使用此类 YAML 文件创建环境时，conda
    将自动识别和安装默认包及其特定于平台的依赖关系。此外，将安装我们明确指定的包及其依赖项：
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The disadvantage of using the `--from-history` option is that the created environment
    is not a replica of the original environment since the base packages and dependencies
    are platform specific and hence different. If the platform where this environment
    is to be used is the same, then we do not recommend using this option.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `--from-history` 选项的缺点是创建的环境不是原始环境的复制品，因为基本包和依赖项是特定于平台的，因此不同。如果要使用此环境的平台与创建环境的平台相同，则不建议使用此选项。
- en: 'Blueprint: Using Containers to Create Reproducible Environments'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用容器创建可复制的环境
- en: While a package manager like conda helps in installing multiple packages and
    managing dependencies, there are several platform-dependent binaries that can
    still hinder reproducibility. To make things simpler, we make use of a layer of
    abstraction called *containers*. The name is derived from the shipping industry,
    where standard-sized shipping containers are used to transport all kinds of goods
    by ships, trucks, and rail. Regardless of the type of items or the mode of transport,
    the shipping container ensures that anyone adhering to that standard can transport
    those items. In a similar fashion, we use a Docker container to standardize the
    environment we work in and guarantee that an identical environment is re-created
    every time regardless of where it runs or who runs it. [Docker](https://docker.com)
    is one of the most popular tools that enables this functionality, and we will
    make use of it in this blueprint. [Figure 13-1](#fig-docker-workflow) shows a
    high-level overview of how Docker works.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像 conda 这样的包管理器帮助安装多个包并管理依赖关系，但仍有几个特定于平台的二进制文件可能会妨碍可复制性。为了简化事务，我们利用了一个称为 *容器*
    的抽象层。这个名字来源于航运业，标准尺寸的航运集装箱用于通过船舶、卡车和铁路运输各种商品。无论物品类型或运输方式如何，集装箱确保任何遵循该标准的人都可以运输这些物品。同样地，我们使用
    Docker 容器来标准化我们工作的环境，并确保每次重新创建时都能生成相同的环境，无论在哪里运行或由谁运行。[Docker](https://docker.com)
    是实现此功能的最流行工具之一，我们将在本蓝图中使用它。[图 13-1](#fig-docker-workflow) 显示了 Docker 的工作高级概述。
- en: '![](Images/btap_1301.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1301.jpg)'
- en: Figure 13-1\. Workflow of Docker.
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. Docker 的工作流程。
- en: 'We need to start by installing Docker from the [download link](https://oreil.ly/CJWKF).
    Once it has been set up, please run `sudo docker run hello-world` from the command
    line to test that everything has been set up correctly, and you should see the
    output as shown. Please note that the Docker daemon binds to a Unix socket that
    is owned by the root user, hence the need to run all commands with `sudo`. If
    you are unable to provide root access, there is an [experimental version](https://oreil.ly/X7lzt)
    of Docker that you can also try:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从[下载链接](https://oreil.ly/CJWKF)安装 Docker。一旦设置完成，请在命令行中运行 `sudo docker run
    hello-world` 来测试一切是否设置正确，您应该看到如下输出。请注意，Docker 守护程序绑定到一个 Unix 套接字，由 root 用户拥有，因此需要使用
    `sudo` 运行所有命令。如果无法提供 root 访问权限，您也可以尝试一个[实验版本](https://oreil.ly/X7lzt)的 Docker：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can draw an analogy between building a Docker container and purchasing a
    car. We start by choosing from one of the preconfigured options. These configurations
    already have some components selected, such as the type of engine (displacement,
    fuel-type), safety features, level of equipment, etc. We can customize many of
    these components, for example, upgrade to a more fuel-efficient engine or add
    additional components like a navigation system or heated seats. At the end, we
    decide on our preferred configuration and order the car. In a similar way, we
    specify the configuration of the environment we want to create in the *Dockerfile*.
    These are described in the form of a set of instructions, executed in a sequential
    manner resulting in the creation of a *Docker image*. A Docker image is like the
    preferred car configuration that can be created based on our Dockerfile. All Docker
    images are extensible, so instead of defining all steps, we could extend an existing
    Docker image and customize it by adding specific steps that we would like. The
    final step of running a Docker image results in the creation of a *Docker container*,
    which is like the car with your preferred configuration delivered to you. In this
    case, it is a full-fledged environment including an operating system and additional
    utilities and packages as specified in the Dockerfile. It runs on the hardware
    and uses the interfaces provided by the host system but is completely isolated
    from it. In effect, it is a minimal version of a server running the way you designed
    it. Each Docker container instantiated from the same image will be the same regardless
    of which host system it is running on. This is powerful as it allows you to encapsulate
    your analysis and environment and run it on your laptop, in the cloud, or on your
    organization’s server and expect the same behavior.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将构建 Docker 容器与购买汽车类比。我们从预配置选项中选择开始。这些配置已经选择了一些组件，例如引擎类型（排量、燃料类型）、安全功能、设备水平等。我们可以自定义许多这些组件，例如升级到更节能的引擎或添加额外的组件，如导航系统或加热座椅。最后，我们确定我们喜欢的配置并订购汽车。类似地，我们在
    *Dockerfile* 中指定我们想要创建的环境配置。这些配置以一组顺序执行的指令形式描述，结果是创建一个 *Docker 镜像*。Docker 镜像就像根据我们的
    Dockerfile 创建的首选汽车配置。所有 Docker 镜像都是可扩展的，因此我们可以扩展现有的 Docker 镜像并通过添加特定步骤来自定义它。运行
    Docker 镜像的最后一步会创建一个 *Docker 容器*，这就像根据您的首选配置交付给您的汽车一样。在这种情况下，它是一个包含操作系统及其它工具和软件包的完整环境，如在
    Dockerfile 中指定的。它在硬件上运行并使用主机系统提供的接口，但与主机系统完全隔离。事实上，它是根据您设计的方式运行的服务器的最小版本。从相同镜像实例化的每个
    Docker 容器无论在哪个主机系统上运行都将是相同的。这很强大，因为它允许您封装您的分析和环境，并在笔记本电脑、云端或组织的服务器上运行并期望相同的行为。
- en: 'We are going to create a Docker image with the same Python environment as used
    in our analysis so that anyone else can reproduce our analysis by pulling the
    image and instantiating it as a container. While we can start by specifying our
    Docker image from scratch, it would be preferable to start with an existing image
    and customize certain parts of it to create our version. Such an image is referred
    to as a *parent image*. A good place to search for parent images is the [Docker
    Hub registry](https://hub.docker.com), which is a public repository containing
    prebuilt Docker images. You will find officially supported images like the [Jupyter
    Data Science](https://oreil.ly/kKLXU) notebook as well as [user-created images](https://oreil.ly/K5SMy)
    like the one we have created for [Chapter 9](ch09.xhtml#ch-summarization) that
    can be accessed here. Every image in the Docker repository can also be used as
    is to run containers. You can search for images using the `**sudo docker search**`
    command and add arguments to format results, as shown here where we search for
    available Miniconda images:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个 Docker 镜像，其中包含与我们分析中使用的相同的 Python 环境，这样其他人可以通过拉取镜像并实例化为容器来复现我们的分析。虽然我们可以从头开始指定我们的
    Docker 镜像，但最好是从现有的镜像开始，并定制其中的某些部分来创建我们自己的版本。这样的镜像称为*父镜像*。一个好地方去搜索父镜像是[Docker Hub
    注册表](https://hub.docker.com)，这是一个包含预构建 Docker 镜像的公共仓库。您会找到官方支持的镜像，如[Jupyter 数据科学](https://oreil.ly/kKLXU)笔记本，以及像我们为[第9章](ch09.xhtml#ch-summarization)创建的[用户创建的镜像](https://oreil.ly/K5SMy)。Docker
    仓库中的每个镜像也可以直接用来运行容器。您可以使用`**sudo docker search**`命令搜索镜像，并添加参数以格式化结果，如下所示，在这里我们搜索可用的
    Miniconda 镜像：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We see that there is an image for Miniconda3 that would be a good starting point
    for our own Dockerfile. Note that all Dockerfiles have to start with the `FROM`
    keyword specifying which image they are deriving from. If you are specifying a
    Dockerfile from the start, then you would use the `FROM scratch` keyword. The
    details of the [Miniconda image and the Dockerfile](https://oreil.ly/ddYff) show
    how this image derives from a Debian parent image and only adds additional steps
    to install and set up the conda package manager. When using a parent Docker image,
    it’s important to check that it’s from a trusted source. Docker Hub provides additional
    criteria like “Official Images” that can be helpful in identifying an official
    source.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有一个 Miniconda3 的镜像，这将是我们自己的 Dockerfile 的一个很好的起点。请注意，所有 Dockerfile 都必须以`FROM`关键字开头，指定它们派生自哪个镜像。如果您从头开始指定一个
    Dockerfile，那么您将使用`FROM scratch`关键字。[Miniconda 镜像和 Dockerfile 的详细信息](https://oreil.ly/ddYff)显示了该镜像如何派生自一个
    Debian 父镜像，并且只添加额外的步骤来安装和设置 conda 包管理器。在使用父 Docker 镜像时，检查它来自可信的来源非常重要。Docker Hub
    提供了额外的标准，如“官方镜像”，有助于识别官方来源。
- en: 'Let’s walk through the steps defined in our Dockerfile. We start with the Miniconda3
    image and then add a step to create our custom environment. We use the `ARG` instruction
    to specify the argument for the name of our conda environment (blueprints). We
    then use `ADD` to copy the `environment.yml` file from the build context to the
    image. Finally, we create the conda environment by providing the `**conda create**`
    command as an argument to `RUN`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照我们的 Dockerfile 中定义的步骤来进行。我们从 Miniconda3 镜像开始，然后添加一个步骤来创建我们的自定义环境。我们使用`ARG`指令来指定我们
    conda 环境的名称参数（blueprints）。然后，我们使用`ADD`将`environment.yml`文件从构建上下文复制到镜像中。最后，通过将`**conda
    create**`命令作为`RUN`的参数来创建 conda 环境：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the next set of steps, we want to ensure that the environment is activated
    in the container. Therefore, we add it to the end of the `.bashrc` script, which
    will always run when the container starts. We also update the `PATH` environment
    variable using the `ENV` instruction to ensure that the conda environment is the
    version of Python used everywhere within the container:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们希望确保环境在容器中被激活。因此，我们将其添加到`.bashrc`脚本的末尾，这将在容器启动时始终运行。我们还使用`ENV`指令更新`PATH`环境变量，以确保
    conda 环境是容器内部到处使用的 Python 版本：
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the final step, we want to automatically start a Jupyter notebook that will
    allow the users of this Docker container to run the analysis in an interactive
    fashion. We use the `ENTRYPOINT` instruction, which is used to configure a container
    that will run as an executable. There can be only one such instruction in a Dockerfile
    (if there are multiple, only the last one will be valid), and it will be the last
    command to run when a container comes up and is typically used to start a server
    like the Jupyter notebook that we want to run. We specify additional arguments
    to run the server on the IP address of the container itself (`0.0.0.0`), on a
    particular port (`8888`), as the root user (`--allow-root`), and not open a browser
    by default (`--no-browser`). When the container starts, we don’t want it to open
    the Jupyter server in its browser. Instead, we will attach the host machine to
    this container using the specified port and access it via the browser there:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们希望自动启动一个Jupyter笔记本，以便此Docker容器的用户可以以交互方式运行分析。我们使用`ENTRYPOINT`指令来配置将作为可执行文件运行的容器。Dockerfile中只能有一个这样的指令（如果有多个，则只有最后一个有效），它将是容器启动时要运行的最后一个命令，通常用于启动像我们要运行的Jupyter笔记本这样的服务器。我们指定额外的参数来在容器本身的IP地址（`0.0.0.0`）上运行服务器，在特定端口（`8888`）上以root用户身份运行（`--allow-root`），并且默认不在浏览器中打开（`--no-browser`）。当容器启动时，我们不希望它在其浏览器中打开Jupyter服务器。相反，我们将使用指定的端口将主机机器连接到此容器，并通过那里的浏览器访问它：
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We use the `docker build` command to create the image from our Dockerfile. We
    specify the name of our image with the `-t` parameter and add a username followed
    by the name of the image. This is useful in identifying our image when we want
    to refer to it later. It is not mandatory to specify a username, but we will see
    later why this is useful. The Dockerfile to be used while building the image is
    specified with the `-f` parameter. If nothing is specified, then Docker will pick
    the file named *Dockerfile* in the directory specified by the argument `PATH`.
    The `PATH` argument also specifies where to find the files for the “context” of
    the build on the Docker daemon. All the files in this directory are packaged with
    `tar` and sent to the daemon during the build process. This must include all the
    files and artifacts that have to be added to the image, e.g., the `environment.yml`
    file, which will be copied to the image to create the conda environment.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`docker build`命令根据我们的Dockerfile创建镜像。我们使用`-t`参数指定镜像的名称，并添加用户名和镜像名称。这在以后要引用镜像时非常有用。虽然没有必要指定用户名，但稍后我们会看到这样做的好处。在构建镜像时使用的Dockerfile由`-f`参数指定。如果未指定任何内容，则Docker将选择指定`PATH`参数所在目录中名为*Dockerfile*的文件。`PATH`参数还指定了在Docker守护程序上构建过程的“上下文”中查找文件的位置。此目录中的所有文件都将在构建过程中被打包为`tar`文件并发送到守护程序。这些文件必须包括所有要添加到镜像中的文件和工件，例如将复制到镜像以创建conda环境的`environment.yml`文件。
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'On executing this command, the Docker daemon starts creating an image by running
    the steps specified in the Dockerfile. Typically, you would execute the command
    in the same directory that already contains all the files and the Dockerfile as
    well. We specify the `PATH` argument using `.` referring to the current directory:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此命令时，Docker守护程序开始通过运行Dockerfile中指定的步骤来创建镜像。通常，您会在已经包含所有文件和Dockerfile的同一目录中执行该命令。我们使用`.`来引用当前目录指定`PATH`参数：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After the build completes, you can check whether the image was created successfully
    by running the command `**sudo docker images**`. You will notice that `continuumio/miniconda3`
    image has been downloaded, and in addition, the image specified with your username
    and `docker_project` has also been created. Building a Docker will take longer
    the first time since the parent images have to be downloaded, but subsequent changes
    and rebuilds will be much faster:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完成后，可以通过运行命令`**sudo docker images**`来检查镜像是否成功创建。你会注意到已经下载了`continuumio/miniconda3`镜像，并且还创建了包含你的用户名和`docker_project`指定的镜像。第一次构建Docker会花费更长时间，因为需要下载父镜像，但后续的更改和重建会快得多：
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can create a running instance of this environment, also called *container*,
    by running:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令创建这个环境的运行实例，也称为*容器*：
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `-p` argument allows port forwarding, essentially sending any requests
    received on the `host_port` to the `container_port`. By default, the Jupyter server
    can only access the files and directories within the container. However, we would
    like to access the Jupyter notebooks and code files present in a local directory
    from the Jupyter server running inside the container. We can attach a local directory
    to the container as a volume by using `-v host_volume:container_volume`, which
    will create a new directory within the container pointing to a local directory.
    This ensures that any changes made to the Jupyter notebooks are not lost when
    the container shuts down. This is the recommended approach to work with files
    locally but using a Docker container for the reproducible environment. Let’s start
    our Docker container by running the following command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`-p` 参数允许端口转发，基本上将接收到的任何请求发送到 `host_port` 到 `container_port`。默认情况下，Jupyter
    服务器只能访问容器内的文件和目录。但是，我们希望从运行在容器内部的 Jupyter 服务器中访问本地目录中的 Jupyter 笔记本和代码文件。我们可以使用
    `-v host_volume:container_volume` 将本地目录附加到容器作为卷，这将在容器内创建一个新目录，指向一个本地目录。这样做可以确保当容器关闭时，对
    Jupyter 笔记本的任何更改都不会丢失。这是在本地使用 Docker 容器进行工作的推荐方法。让我们通过运行以下命令启动我们的 Docker 容器：'
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`Out:`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE18]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The logs you see now are actually the logs of the Jupyter server starting in
    port 8888 within the container. Since we have mapped the host port 5000, you can
    copy the URL and only replace the port number to 5000 to access the Jupyter server.
    You will also find here a directory called `work`, which should contain all the
    files from the local directory that was mapped. You can also check the status
    of all running containers by running the command `**sudo docker container ps**`.
    We can also specify the name for each running container by using the `--name argument`,
    and if this is not used, the Docker daemon will assign a randomly created one,
    as you see here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您看到的日志实际上是在容器内端口 8888 上启动的 Jupyter 服务器的日志。由于我们已将主机端口映射到 5000，您可以复制 URL 并仅将端口号更改为
    5000 以访问 Jupyter 服务器。您还将在这里找到一个名为 `work` 的目录，其中应包含来自映射的本地目录中的所有文件。您还可以通过运行命令 `**sudo
    docker container ps**` 检查所有运行中的容器的状态。我们还可以使用 `--name argument` 为每个运行的容器指定名称，如果不使用此选项，则
    Docker 守护程序将分配一个随机创建的名称，如您在此处看到的：
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you quit the terminal window where you ran this command, then the container
    will also be shut down. To run it in a detached mode, just add the `-d` option
    to the run command. When the container starts, it will print the container ID
    of the started container, and you can monitor the logs using `sudo docker logs
    <container-id>`. We have reproduced the complete environment used to run our analysis
    in this Docker container, and in the next blueprint, let’s see the best techniques
    to share it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您退出运行此命令的终端窗口，则容器也将关闭。要在分离模式下运行它，只需在运行命令中添加 `-d` 选项。容器启动时，将打印已启动容器的容器 ID，并且您可以使用
    `sudo docker logs <container-id>` 监视日志。我们已在此 Docker 容器中复制了用于运行分析的完整环境，在下一个蓝图中，让我们看看分享它的最佳技术。
- en: The easiest way to share this image with anyone is by pushing this to the Docker
    Hub registry. You can [sign up](https://oreil.ly/vyi-2) for a free account. Docker
    Hub is a public repository for Docker images, and each image is uniquely identified
    by the username, the name of the image, and a tag. For example, the `miniconda3`
    package that we used as our parent image is identified as `continuumio/miniconda3:latest`,
    and any images that you share will be identified with your username. Therefore,
    when we built our image earlier, the username we specified must have been the
    same as the one used to log in to Docker Hub. Once you have created your credentials,
    you can click Create a Repository and choose a name and provide a description
    for your repository. In our case we created a repository called "`ch13`" that
    will contain a Docker image for this chapter. Once done, you can log in using
    the command `**sudo docker login**` and enter your username and password. For
    added security, please follow the [instructions](https://oreil.ly/m95HO) to securely
    store your password.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将此镜像与任何人共享的最简单方法是将其推送到Docker Hub注册表。您可以[注册](https://oreil.ly/vyi-2)一个免费帐户。Docker
    Hub是Docker镜像的公共存储库，每个镜像都由用户名、镜像名称和标签唯一标识。例如，我们用作父镜像的`miniconda3`软件包被标识为`continuumio/miniconda3:latest`，您分享的任何镜像将用您的用户名标识。因此，在之前构建镜像时，指定的用户名必须与登录Docker
    Hub时使用的用户名相同。创建凭据后，您可以单击创建存储库，并选择一个名称并为存储库提供描述。在我们的情况下，我们创建了一个名为"`ch13`"的存储库，将包含本章的Docker镜像。完成后，您可以使用命令`**sudo
    docker login**`登录，并输入您的用户名和密码。为增加安全性，请按照[说明](https://oreil.ly/m95HO)安全地存储您的密码。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'By default, during the build process of a Docker image, all of the directories
    and files present in the `PATH` argument are part of the build context. In a previous
    command, we indicated the path to be the current directory using the `.` symbol.
    This is not necessary since we need to include only the selected list of files
    that are needed for the build and later the container. For instance, we need `environment.yml`
    but not the Jupyter notebook (`.ipynb`) file. It’s important to specify the list
    of excluded files in the `.dockerignore` file to ensure that unwanted files do
    not automatically get added to the container. Our `.dockerignore` file is as shown
    here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在构建Docker镜像的过程中，`PATH`参数指定的所有目录和文件都是构建上下文的一部分。在前面的命令中，我们使用`.`符号指定路径为当前目录，但这是不必要的，因为我们只需要包含构建和容器所需的选择文件列表。例如，我们需要`environment.yml`但不需要Jupyter笔记本（`.ipynb`）文件。重要的是要在`.dockerignore`文件中指定排除的文件列表，以确保不希望的文件不会自动添加到容器中。我们的`.dockerignore`文件如下所示：
- en: '[PRE20]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Another thing to ensure is that the `host_port` (specified as 5000 in the blueprint)
    is open and not used by any other application on your system. Ideally, you must
    use a port number between 1024–49151 as these are [user ports](https://oreil.ly/F-Qps),
    but you can also check this easily by running the command `**sudo ss -tulw**`,
    which will provide the list of used ports.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件需要确保的事情是`host_port`（在蓝图中指定为5000）是开放的，且系统中没有其他应用程序在使用。理想情况下，您应使用1024到49151之间的端口号作为[user
    ports](https://oreil.ly/F-Qps)，但您也可以轻松地通过运行命令`**sudo ss -tulw**`来检查这一点，该命令将提供已使用端口的列表。
- en: The next step is to tag the image that you would like to share with a `tag_name`
    to identify what it contains. In our case, we tag the image with v1 to signify
    that it is the first version for this chapter. We run the command `sudo docker
    tag 2b45bb18c071 textblueprints/ch13:v1`, where 2b45bb18c071 is the image ID.
    We can push our file now with the command `sudo docker push textblueprints/ch13`.
    Now anyone who wants to run your project can simply run the command `docker pull
    your_username/docker_project:tag_name` to create the same environment as you,
    irrespective of the system they might be personally working on. As an example,
    you can start working on blueprints in [Chapter 9](ch09.xhtml#ch-summarization)
    by simply running the command `docker pull textblueprints/ch09:v1`. You can then
    attach the volume of the directory containing the cloned repository. Docker Hub
    is a popular public registry and configured as default with Docker, but each cloud
    provider also has their own version, and many organizations set up private registries
    for use within their internal applications and teams.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是标记您希望与 `tag_name` 一起分享的图像，以识别其包含内容。在我们的情况下，我们使用 v1 标记图像，以表示这是本章的第一个版本。我们运行命令
    `sudo docker tag 2b45bb18c071 textblueprints/ch13:v1`，其中 2b45bb18c071 是图像 ID。现在我们可以使用命令
    `sudo docker push textblueprints/ch13` 推送我们的文件。现在任何想要运行您项目的人都可以简单地运行命令 `docker
    pull your_username/docker_project:tag_name` 来创建与您相同的环境，无论他们可能在个人工作中使用的系统如何。例如，您可以通过简单运行命令
    `docker pull textblueprints/ch09:v1` 开始在 [第9章](ch09.xhtml#ch-summarization) 中的蓝图工作。然后，您可以附加包含克隆存储库的目录的卷。Docker
    Hub 是一个流行的公共注册表，并且在 Docker 中默认配置，但每个云提供商也有他们自己的版本，许多组织为他们内部应用程序和团队设置了私有注册表。
- en: When working with conda environments with multiple scientific computing packages,
    Docker images can get large and therefore create a strain on bandwidth while pushing
    to Docker Hub. A much more efficient way is to include the Dockerfile in the base
    path of your repository. For example, the GitHub repo containing the code for
    this chapter contains a Dockerfile, which can be used to create the exact environment
    required to run the code. This blueprint easily allows you to move an analysis
    from your local system to a cloud machine with additional resources by re-creating
    the same working environment. This is especially useful when the size of the data
    increases or an analysis takes too long to finish.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用具有多个科学计算包的 conda 环境时，Docker 镜像可能会变得很大，因此在将其推送到 Docker Hub 时可能会对带宽造成压力。一个更有效的方法是在存储库的基本路径中包含
    Dockerfile。例如，包含本章代码的 GitHub 存储库包含一个 Dockerfile，该文件可以用于创建运行代码所需的精确环境。这个蓝图轻松地允许您将分析从本地系统移动到具有额外资源的云机器上，重新创建相同的工作环境。当数据大小增加或分析时间过长时，这尤其有用。
- en: 'Blueprint: Creating a REST API for Your Text Analytics Model'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：为您的文本分析模型创建 REST API
- en: Let’s say you used the blueprint provided in [Chapter 11](ch11.xhtml#ch-sentiment)
    to analyze the sentiment of customer support tickets in your organization. Your
    company is running a campaign to improve customer satisfaction where they would
    like to provide vouchers to unhappy customers. A colleague from the tech team
    reaches out to you for help with automating this campaign. While they can pull
    the Docker container and reproduce your analysis, they would prefer a simpler
    method where they provide the text of the support ticket and get a response of
    whether this is an unhappy customer. By encapsulating our analysis in a REST API,
    we can create a simple method that is accessible to anyone without them having
    to rerun the blueprint. They don’t even necessarily need to know Python since
    a REST API can be called from any language. In [Chapter 2](ch02.xhtml#ch-api),
    we made use of REST APIs provided by popular websites to extract data, whereas
    in this blueprint we are going to create our own.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您使用了提供在 [第11章](ch11.xhtml#ch-sentiment) 的蓝图来分析您组织中客户支持票的情绪。您的公司正在进行一项提高客户满意度的活动，他们希望向不满意的客户提供优惠券。技术团队的同事联系您寻求帮助自动化此活动。虽然他们可以拉取
    Docker 容器并重现您的分析，但他们更倾向于一种更简单的方法，即提供支持票的文本并获取是否为不满意客户的响应。通过将我们的分析封装在一个 REST API
    中，我们可以创建一个简单的方法，任何人都可以访问，而不需要重新运行蓝图。他们甚至不一定需要知道 Python，因为 REST API 可以从任何语言调用。在
    [第2章](ch02.xhtml#ch-api) 中，我们使用了流行网站提供的 REST API 来提取数据，而在这个蓝图中，我们将创建我们自己的 REST
    API。
- en: 'We will make use of the following three components to host our REST API:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用以下三个组件来托管我们的 REST API：
- en: 'FastAPI: A fast web framework for building APIs'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FastAPI：一个用于构建 API 的快速 Web 框架
- en: 'Gunicorn: A Web Service Gateway Interface server that handles all the incoming
    requests'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunicorn：处理所有传入请求的 Web 服务网关接口服务器
- en: 'Docker: Extending the Docker container that we used in the previous blueprint'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker：扩展我们在之前蓝图中使用的 Docker 容器
- en: 'Let’s create a new folder called *app* where we will place all the code that
    we require in order to serve sentiment predictions. It will follow the directory
    structure and contain files as shown next. *main.py* is where we will create the
    FastAPI app and the sentiment prediction method, and *preprocessing.py* is where
    our helper functions are included. The *models* directory contains the trained
    models we need to use to calculate our predictions, in our case the `sentiment_vectorizer`
    and `sentiment_classification`. Finally, we have the Dockerfile, *environment.yml*,
    and *start_script.sh*, which will be used to deploy our REST API:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 *app* 的新文件夹，我们将在其中放置所有我们需要用来提供情感预测的代码。它将遵循以下目录结构，并包含如下文件。*main.py*
    是我们将创建 FastAPI 应用程序和情感预测方法的地方，*preprocessing.py* 包含我们的辅助函数。*models* 目录包含我们需要用来计算预测的训练模型，即
    `sentiment_vectorizer` 和 `sentiment_classification`。最后，我们有 Dockerfile、*environment.yml*
    和 *start_script.sh*，它们将用于部署我们的 REST API：
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[FastAPI](https://oreil.ly/fastapi) is a fast Python framework used to build
    APIs. It is capable of redirecting requests from a web server to specific functions
    defined in Python. It also takes care of validating the incoming requests against
    specified schema and is useful for creating a simple REST API. We will encapsulate
    the predict function of the model we trained in [Chapter 11](ch11.xhtml#ch-sentiment)
    in this API. Let’s walk through the code in the file *main.py* step-by-step and
    explain how it works. You can install FastAPI by running `pip install fastapi`
    and Gunicorn by running `pip install gunicorn`.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[FastAPI](https://oreil.ly/fastapi) 是一个快速的 Python 框架，用于构建 API。它能够将来自 Web 服务器的请求重定向到
    Python 中定义的特定函数。它还负责根据指定的模式验证传入的请求，并且非常适合创建简单的 REST API。我们将在这个 API 中封装我们在 [第11章](ch11.xhtml#ch-sentiment)
    中训练的模型的 predict 函数。让我们逐步解释 *main.py* 文件中的代码，并说明其工作原理。您可以通过运行 `pip install fastapi`
    安装 FastAPI，并通过运行 `pip install gunicorn` 安装 Gunicorn。'
- en: 'Once FastAPI is installed, we can create an app using the following code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了 FastAPI 后，我们可以使用以下代码创建一个应用：
- en: '[PRE22]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The FastAPI library runs this app using the included web server and can route
    requests received at an endpoint to a method in the Python file. This is specified
    by adding the `@app.post` attribute at the start of the function definition. We
    specify the endpoint to be *api/v1/sentiment*, the first version of our Sentiment
    API, which accepts HTTP POST requests. An API can evolve over time with changes
    to functionality, and it’s useful to separate them into different versions to
    ensure that users of the older version are not affected:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 库使用包含的 Web 服务器运行此应用程序，并且可以将接收到的请求路由到 Python 文件中的方法。这通过在函数定义的开头添加 `@app.post`
    属性来指定。我们指定端点为 *api/v1/sentiment*，我们 Sentiment API 的第一个版本，它接受 HTTP POST 请求。API
    可以随时间演变，功能发生变化，将它们分隔成不同的版本是有用的，以确保旧版本的用户不受影响：
- en: '[PRE23]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `predict` method retrieves the text field from the input and performs the
    preprocessing and vectorization steps. It uses the model we trained earlier to
    predict the sentiment of the product review. The returned sentiment is specified
    as an `Enum` class to restrict the possible return values for the API. The input
    parameter `review` is defined as an instance of the class `Review`. The class
    is as specified next and contains the text of the review, a mandatory field along
    with `reviewerID`, `productID`, and `sentiment`. FastAPI uses [“type hints”](https://oreil.ly/eErFf)
    to guess the type of the field `(str)` and perform the necessary validation. As
    we will see, FastAPI automatically generates a web documentation for our API following
    the [OpenAPI](https://openapis.org) specification from which the API can be tested
    directly. We add the `schema_extra` as an example to act as a guide to developers
    who want to use the API:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict` 方法从输入中检索文本字段，并执行预处理和向量化步骤。它使用我们之前训练的模型来预测产品评论的情感。返回的情感被指定为 `Enum`
    类，以限制 API 可能的返回值。输入参数 `review` 被定义为 `Review` 类的一个实例。该类如下所示，包含评论文本作为必填字段，还有 `reviewerID`、`productID`
    和 `sentiment`。FastAPI 使用 [“type hints”](https://oreil.ly/eErFf) 来猜测字段的类型 `(str)`
    并执行必要的验证。正如我们将看到的，FastAPI 会根据 [OpenAPI](https://openapis.org) 规范自动生成我们 API 的 web
    文档，通过这些文档可以直接测试 API。我们添加了 `schema_extra` 作为一个示例，以充当开发人员使用该 API 的指南：'
- en: '[PRE24]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You would have noticed the use of the `Depends` keyword in the function definition.
    This allows us to load dependencies or other resources that are required before
    the function is called. This is treated as another Python function and is defined
    here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到函数定义中使用了 `Depends` 关键字。这允许我们在调用函数之前加载必需的依赖项或其他资源。这被视为另一个 Python 函数，并在此定义：
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Pickle is a Python serialization framework that is one of the common ways in
    which models can be saved/exported. Other standardized formats include [joblib](https://oreil.ly/iyl7W)
    and [ONNX](https://onnx.ai). Some deep learning frameworks use their own export
    formats. For example, TensorFlow uses `SavedModel`, while PyTorch uses pickle
    but implements its own `save()` function. It’s important that you adapt the load
    and predict functions based on the type of model save/export you have used.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Pickle 是 Python 序列化框架之一，是模型保存/导出的常见方式之一。其他标准化格式包括 [joblib](https://oreil.ly/iyl7W)
    和 [ONNX](https://onnx.ai)。一些深度学习框架使用它们自己的导出格式。例如，TensorFlow 使用 `SavedModel`，而
    PyTorch 使用 pickle，但实现了自己的 `save()` 函数。根据你使用的模型保存/导出类型，适应加载和预测函数非常重要。
- en: During development, FastAPI can be run with any web server (like [uvicorn](https://uvicorn.org)),
    but it is recommended to use a full-fledged Web Service Gateway Interface (WSGI)
    server, which is production ready and supports multiple worker threads. We choose
    to use [Gunicorn](https://gunicorn.org) as our WSGI server as it provides us with
    an HTTP server that can receive requests and redirect to the FastAPI app.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，FastAPI 可以与任何 Web 服务器一起运行（例如 [uvicorn](https://uvicorn.org)），但建议使用成熟的、支持多个工作线程的完整的
    Web 服务网关接口（WSGI）服务器。我们选择使用 [Gunicorn](https://gunicorn.org) 作为我们的 WSGI 服务器，因为它提供了一个可以接收请求并重定向到
    FastAPI 应用的 HTTP 服务器。
- en: 'Once installed, it can be run by entering:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，可以输入以下命令运行它：
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `-w` argument is used to specify the number of worker processes to run,
    three workers in this case. The `-b` parameter specifies the port that WSGI server
    listens on, and the `-t` indicates a timeout value of five seconds after which
    the server will kill and restart the app in case it’s not responsive. The `-k`
    argument specifies the instance of worker class (`uvicorn`) that must be called
    to run the app, which is specified by referring to the Python module (`main`)
    and the name (`app`).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`-w` 参数用于指定要运行的工作进程数量，本例中为三个工作者。`-b` 参数指定了 WSGI 服务器监听的端口，`-t` 表示超时值，超过五秒后服务器将杀死并重新启动应用程序，以防止其无响应。`-k`
    参数指定了要调用以运行应用程序的工作者类的实例（`uvicorn`），通过引用 Python 模块（`main`）和名称（`app`）来指定。'
- en: 'Before deploying our API, we have to revisit the *environment.yml* file. In
    the first blueprint, we described ways to generate and share the *environment.yml*
    file to ensure that your analysis is reproducible. However, it is not recommended
    to follow this method when deploying code to production. While the exported *environment.yml*
    file is a starting point, we must inspect it manually and ensure that it does
    not contain unused packages. It’s also important to specify the exact version
    number of a package to ensure that package updates do not interfere with your
    production deployment. We use a Python code analysis tool called [Vulture](https://oreil.ly/fC71i)
    that identifies unused packages as well as other dead code fragments. Let’s run
    this analysis for the *app* folder:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署 API 之前，我们必须重新检查 *environment.yml* 文件。在第一版本中，我们描述了生成和分享 *environment.yml*
    文件的方法，以确保你的分析可复现。然而，在将代码部署到生产环境时，不建议使用此方法。虽然导出的 *environment.yml* 文件是一个起点，但我们必须手动检查并确保它不包含未使用的包。还重要的是指定包的确切版本号，以确保包更新不会干扰你的生产部署。我们使用一个名为
    [Vulture](https://oreil.ly/fC71i) 的 Python 代码分析工具，它可以识别未使用的包以及其他死代码片段。让我们为 *app*
    文件夹运行此分析：
- en: '[PRE27]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE28]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Along with the list of potential issues, Vulture also provides a confidence
    score. Please use the identified issues as pointers to check the use of these
    imports. In the previous example, we know that the class variables we have defined
    are used to validate the input to the API and are definitely used. We can see
    that even though `spacy` and `display_nlp` are part of the preprocessing module,
    they are not used in our app. We can choose to remove them and the corresponding
    dependencies from the YAML file.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了潜在问题列表，Vulture 还提供了置信度分数。请使用识别出的问题作为检查这些导入的指针。在上一个示例中，我们知道我们定义的类变量用于验证 API
    的输入并且肯定被使用。我们可以看到，即使 `spacy` 和 `display_nlp` 是预处理模块的一部分，但它们并未在我们的应用程序中使用。我们可以选择将它们和相应的依赖项从
    YAML 文件中删除。
- en: 'You can also determine the version of each package used in the conda environment
    by running the `**conda list**` command and then use this information to create
    the final cleaned-up environment YAML file, as shown here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过运行 `**conda list**` 命令确定conda环境中每个软件包的版本，然后使用此信息创建最终清理后的环境 YAML 文件，如下所示：
- en: '[PRE29]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As the final step, we can Dockerize the API so that it’s easier to run the
    entire app in its own container, which is especially beneficial when we want to
    host it on a cloud provider, as we will see in the next blueprint. We make two
    changes in the Dockerfile from the previous blueprint as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们可以将 API Docker 化，以便更容易地在其自己的容器中运行整个应用程序，特别是当我们想要在云提供商上托管它时，正如我们将在下一个蓝图中看到的那样。我们对比之前的蓝图在
    Dockerfile 中进行了两个更改，如下所示：
- en: '[PRE30]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The previous instruction is used to `COPY` all of the contents of the current
    *app* folder to the Docker image, which contains all of the files needed to deploy
    and run the REST API. The current directory in the container is then changed to
    the *app* folder by using the `WORKDIR` instruction:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述指令将当前 *app* 文件夹的所有内容复制到 Docker 镜像中，其中包含部署和运行 REST API 所需的所有文件。然后使用 `WORKDIR`
    指令将容器中的当前目录更改为 *app* 文件夹：
- en: '[PRE31]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then provide the steps to run the WSGI server by first exposing port 5000
    on the container. Next, we enable permissions on the `start_script` so that the
    Docker daemon can execute it at container startup. We use a combination of `ENTRYPOINT`
    (used to start the bash shell in which the script is to be run) and `CMD` (used
    to specify the actual script as an argument to the bash shell), which activates
    the conda environment and starts the Gunicorn server. Since we are running the
    server within a Docker container, we make a small change to specify the *access-logfile*
    to be written to STDOUT (`-`) to ensure we can still view them:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提供了运行 WSGI 服务器的步骤，首先在容器上公开端口 5000。接下来，我们在 `start_script` 上启用权限，以便 Docker
    守护程序在容器启动时执行它。我们使用 `ENTRYPOINT`（用于启动要运行脚本的 bash shell）和 `CMD`（用于将实际脚本指定为传递给 bash
    shell 的参数）的组合，它激活 conda 环境并启动 Gunicorn 服务器。由于我们在 Docker 容器中运行服务器，我们做了一个小改变，指定将
    *access-logfile* 写入到 STDOUT (`-`) 以确保我们仍然可以查看它们：
- en: '[PRE32]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We build the Docker image and run it following the same steps as in the previous
    blueprint. This will result in a running Docker container where the Gunicorn WSGI
    server is running the FastAPI app. We have to make sure that we forward a port
    from the host system where the container is running:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建 Docker 镜像并按照之前蓝图中的相同步骤运行它。这将导致一个正在运行的 Docker 容器，其中 Gunicorn WSGI 服务器正在运行
    FastAPI 应用程序。我们必须确保从容器所在的主机系统转发一个端口：
- en: '[PRE33]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can make a call to the container running the API from a different program.
    In a separate terminal window or IDE, create a test method that calls the API
    and passes in a sample review to check the response. We make a call to port 5000
    with the local IP, which is forwarded to port 5000 of the container from which
    we receive the response, as shown here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从不同的程序调用正在运行 API 的容器。在单独的终端窗口或 IDE 中，创建一个测试方法，调用 API 并传入一个样本评论以检查响应。我们使用本地
    IP 以端口 5000 发起调用，该端口被转发到容器的端口 5000，从中我们接收响应，如下所示：
- en: '[PRE34]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`Out:`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE35]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can see that our API has generated the expected response. Let’s also check
    the documentation of this API, which we can find at [*http://localhost:5000/docs*](http://localhost:5000/docs).
    It should generate a page as shown in [Figure 13-2](#fig-swagger-api), and clicking
    the link for our `/api/v1/sentiment` method will provide additional details on
    how the method is to be called and also has the option to try it out. This allows
    others to provide different text inputs and view the results generated by the
    API without writing any code.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的 API 生成了预期的响应。让我们也检查一下这个 API 的文档，可以在[*http://localhost:5000/docs*](http://localhost:5000/docs)找到。它应该生成一个页面，如[图 13-2](#fig-swagger-api)所示，并且点击我们的
    `/api/v1/sentiment` 方法的链接将提供关于如何调用该方法的额外细节，并且还有尝试的选项。这使得其他人可以提供不同的文本输入并查看 API
    生成的结果，而无需编写任何代码。
- en: Docker containers are always started in unprivileged mode, meaning that even
    if there is a terminal error, it would only be restricted to the container without
    any impact to the host system. As a result, we can run the server as a root user
    safely within the container without worrying about an impact on the host system.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 容器始终以非特权模式启动，这意味着即使有终端错误，它也只会限制在容器内部，而不会对主机系统产生任何影响。因此，我们可以安全地在容器内部以根用户身份运行服务器，而不必担心对主机系统造成影响。
- en: '![](Images/btap_1302.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1302.jpg)'
- en: Figure 13-2\. API specification and testing provided by FastAPI.
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. 由 FastAPI 提供的 API 规范和测试。
- en: You can run a combination of the `sudo docker tag` and `sudo docker push` commands
    discussed earlier to share the REST API. Your colleague could easily pull this
    Docker image to run the API and use it to identify unhappy customers by providing
    their support tickets. In the next blueprint, we will run the Docker image on
    a cloud provider and make it available on the internet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以运行前面讨论的 `sudo docker tag` 和 `sudo docker push` 命令的组合来共享 REST API。您的同事可以轻松地拉取此
    Docker 镜像来运行 API，并使用它识别不满意的客户通过提供他们的支持票证。在下一个蓝图中，我们将在云服务提供商上运行 Docker 镜像，并使其在互联网上可用。
- en: 'Blueprint: Deploying and Scaling Your API Using a Cloud Provider'
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用云服务提供者部署和扩展您的 API
- en: Deploying machine learning models and monitoring their performance is a complex
    task and includes multiple tooling options. This is an area of constant innovation
    that is continuously looking to make it easier for data scientists and developers.
    There are several cloud providers and multiple ways to deploy and host your API
    using one of them. This blueprint introduces a simple way to deploy the Docker
    container we created in the previous blueprint using [Kubernetes](https://oreil.ly/C2KX2).
    Kubernetes is an open source technology that provides functionality to deploy
    and manage Docker containers to any underlying physical or virtual infrastructure.
    In this blueprint, we will be using Google Cloud Platform (GCP), but most major
    providers have support for Kubernetes. We can deploy the Docker container directly
    to a cloud service and make the REST API available to anyone. However, we choose
    to deploy this within a Kubernetes cluster since it gives us the flexibility to
    scale up and down the deployment easily.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 部署机器学习模型并监视其性能是一项复杂的任务，并包括多种工具选项。这是一个不断创新的领域，不断努力使数据科学家和开发人员更容易。有几个云服务提供商和多种部署和托管
    API 的方式。本蓝图介绍了一种简单的方式来部署我们在之前蓝图中创建的 Docker 容器，使用 [Kubernetes](https://oreil.ly/C2KX2)。Kubernetes
    是一个开源技术，提供功能来部署和管理 Docker 容器到任何底层物理或虚拟基础设施。在本蓝图中，我们将使用 Google Cloud Platform (GCP)，但大多数主要提供商都支持
    Kubernetes。我们可以直接将 Docker 容器部署到云服务，并使 REST API 对任何人都可用。但是，我们选择在 Kubernetes 集群内部部署它，因为这样可以轻松地扩展和缩小部署。
- en: You can sign up for a [free account with GCP](https://oreil.ly/H1jQS). By signing
    up with a cloud provider, you are renting computing resources from a third-party
    provider and will be asked to provide your billing details. During this blueprint,
    we will stay within the free-tier limit, but it’s important to keep a close track
    of your usage to ensure that you are not charged for some cloud resources that
    you forgot to shut down! Once you’ve completed the sign-up process, you can check
    this by visiting the Billing section from the [GCP console](https://oreil.ly/wX4wd).
    Before using this blueprint, please ensure that you have a Docker image containing
    the REST API pushed and available in Docker Hub or any other container registry.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [GCP](https://oreil.ly/H1jQS) 上注册一个免费帐户。通过与云服务提供商的注册，你可以租用第三方提供的计算资源，并需要提供你的计费详情。在此蓝图期间，我们将保持在免费层限制内，但重要的是要密切关注你的使用情况，以确保你没有因忘记关闭某些云资源而被收费！完成注册流程后，你可以通过访问
    [GCP 控制台](https://oreil.ly/wX4wd) 的计费部分进行检查。在使用此蓝图之前，请确保你有一个包含 REST API 的 Docker
    镜像，并已将其推送并可用于 Docker Hub 或任何其他容器注册表中。
- en: Let’s start by understanding how we are going to deploy the REST API, which
    is illustrated in [Figure 13-3](#fig-kubernetes-architecture). We will create
    a scalable compute cluster using GCP. This is nothing but a collection of individual
    servers that are called *nodes*. The compute cluster shown has three such nodes
    but can be scaled when needed. We will use Kubernetes to deploy the REST API to
    each node of the cluster. Assuming we start with three nodes, this will create
    three replicas of the Docker container, each running on one node. These containers
    are still not exposed to the internet, and we make use of Kubernetes to run a
    load balancer service, which provides a gateway to the internet and also redirects
    requests to each container depending on its utilization. In addition to simplifying
    our deployment process, the use of Kubernetes ensures that node failures and traffic
    spikes can be handled by automatically creating additional instances.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解如何部署 REST API 开始，如 [图 13-3](#fig-kubernetes-architecture) 所示。我们将使用 GCP
    创建一个可扩展的计算集群。这只是由称为 *节点* 的个体服务器集合。所示的计算集群有三个这样的节点，但可以根据需要进行扩展。我们将使用 Kubernetes
    将 REST API 部署到集群的每个节点上。假设我们从三个节点开始，这将创建三个 Docker 容器的副本，每个运行在一个节点上。这些容器仍未暴露给互联网，我们利用
    Kubernetes 运行负载均衡器服务，该服务提供了通往互联网的网关，并根据其利用率重定向请求到每个容器。除了简化我们的部署流程外，使用 Kubernetes
    还可以自动创建额外的实例来处理节点故障和流量波动。
- en: '![](Images/btap_1303.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1303.jpg)'
- en: Figure 13-3\. Kubernetes architecture diagram.
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-3\. Kubernetes 架构图。
- en: Let’s create a project in GCP that we will use for our deployment. Visit [Google
    Cloud](https://oreil.ly/5mCaQ), choose the Create Project option on the top right,
    and create a project with your chosen name (we choose sentiment-rest-api). Once
    the project has been created, click the navigation menu on the top left and navigate
    to the service called Kubernetes Engine, as shown in [Figure 13-4](#fig-kubernetes-engine).
    You have to click the Enable Billing link and select the payment account that
    you set up when you signed up. You can also click the Billing tab directly and
    set it up for your project as well. Assuming you are using the free trial to run
    this blueprint, you will not be charged. It will take a few minutes before it
    gets enabled for our project. Once this is complete, we are ready to proceed with
    our deployment.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 GCP 中创建一个项目，用于我们的部署。访问 [Google Cloud](https://oreil.ly/5mCaQ)，选择右上角的创建项目选项，并使用你选择的名称创建一个项目（我们选择情感
    REST API）。项目创建完成后，点击左上角的导航菜单，进入名为 Kubernetes Engine 的服务，如 [图 13-4](#fig-kubernetes-engine)
    所示。你需要点击启用计费链接，并选择在注册时设置的付款账户。你也可以直接点击计费选项卡，并为你的项目设置计费信息。假设你正在使用免费试用运行此蓝图，你将不会被收费。在此之后，需要几分钟来为我们的项目启用此功能。完成后，我们就可以继续进行部署了。
- en: '![](Images/btap_1304.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1304.jpg)'
- en: Figure 13-4\. Enable Billing in the Kubernetes Engine option in the GCP console.
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-4\. 在 GCP 控制台的 Kubernetes Engine 选项中启用计费。
- en: 'We can continue to work with Google Cloud Platform using the [web console](https://oreil.ly/-eZ-W)
    or the command-line tool. While the functionality offered remains the same, we
    choose to describe the steps in the blueprint with the help of the command-line
    interface in the interest of brevity and to enable you to copy the commands. Please
    install the Google Cloud SDK by following the [instructions](https://oreil.ly/G3_js)
    and then use the Kubernetes command-line tool by running:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续使用Google Cloud Platform的[Web控制台](https://oreil.ly/-eZ-W)或命令行工具进行工作。尽管提供的功能保持不变，但为了简洁起见并使您能够复制命令，我们选择在蓝图中使用命令行界面描述步骤。请按照[说明](https://oreil.ly/G3_js)安装Google
    Cloud SDK，然后运行以下命令使用Kubernetes命令行工具：
- en: '[PRE36]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In a new terminal window, we first authenticate our user account by running
    `**gcloud auth login**`. This will open the browser and redirect you to the Google
    authentication page. Once you have completed this, you won’t be asked for this
    again in this terminal window. We configure the project and compute zone where
    we would like to deploy our cluster. Use the project that we just created, and
    pick a location close to you from all the [available options](https://oreil.ly/SnRc8);
    we chose us-central1-a:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的终端窗口中，我们首先通过运行`**gcloud auth login**`来验证我们的用户账户。这将打开浏览器并将您重定向到Google认证页面。完成后，在此终端窗口中不会再次询问您。我们配置项目和计算区域，选择我们刚刚创建的项目，并从所有[可用选项](https://oreil.ly/SnRc8)中选择靠近您的位置；我们选择了us-central1-a：
- en: '[PRE37]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Our next step is to create a Google Kubernetes Engine compute cluster. This
    is the compute cluster that we will use to deploy our Docker containers. Let’s
    create a cluster with three nodes and request a machine of type n1-standard-1\.
    This type of machine comes with 3.75GB of RAM and 1 CPU. We can request a more
    powerful machine, but for our API this should suffice:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是创建一个Google Kubernetes Engine计算集群。这个计算集群将用于部署我们的Docker容器。让我们创建一个包含三个节点的集群，并请求一个类型为n1-standard-1的机器。这种类型的机器配备了3.75GB的RAM和1个CPU。我们可以请求更强大的机器，但对于我们的API来说，这应该足够了：
- en: '[PRE39]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Every container cluster in GCP comes with `HorizontalPodAutoscaling`, which
    takes care of monitoring the CPU utilization and adding machines if required.
    The requested machines will be provisioned and assigned to the cluster, and once
    it’s executed, you can verify by checking the running compute instances with `gcloud
    compute instances list`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: GCP中的每个容器集群都配备了`HorizontalPodAutoscaling`，它负责监控CPU利用率并在需要时添加机器。请求的机器将被配置并分配给集群，一旦执行完毕，您可以通过运行`gcloud
    compute instances list`来验证正在运行的计算实例：
- en: '[PRE40]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now that our cluster is up and running, we will deploy the Docker image we
    created in the previous blueprint to this cluster with the help of Kubernetes.
    Our Docker image is available on Docker Hub and is uniquely identified by `username/project_name:tag`.
    We give the name of our deployment as `sentiment-app` by running the following
    command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的集群已经启动并运行，我们将通过Kubernetes将我们在前一个蓝图中创建的Docker镜像部署到这个集群。我们的Docker镜像在Docker
    Hub上可用，并通过`username/project_name:tag`唯一标识。我们通过以下命令将我们的部署命名为`sentiment-app`：
- en: '[PRE41]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Once it has been started, we can confirm that it’s running with the command
    `kubectl get pods`, which will show us that we have one pod running. A pod is
    analogous here to the container; in other words, one pod is equivalent to a running
    container of the provided image. However, we have a three-node cluster, and we
    can easily deploy more instances of our Docker image. Let’s scale this to three
    replicas with the following command:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动，我们可以通过命令`kubectl get pods`确认它正在运行，这将显示我们有一个正在运行的pod。在这里，一个pod类似于一个容器；换句话说，一个pod等同于提供的镜像的一个运行中的容器。但是，我们有一个三节点集群，我们可以轻松地部署更多我们的Docker镜像实例。让我们使用以下命令将其扩展到三个副本：
- en: '[PRE42]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You can verify that the other pods have also started running now. Sometimes
    there might be a delay as the container is deployed to the nodes in the cluster,
    and you can find detailed information by using the command `kubectl describe pods`.
    By having more than one replica, we enable our REST API to be continuously available
    even in the case of failures. For instance, let’s say one of the pods goes down
    because of an error; there would be two instances still serving the API. Kubernetes
    will also automatically create another pod in case of a failure to maintain the
    desired state. This is also the case since the REST API is stateless and would
    need additional failure handling in other scenarios.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以验证其他 Pod 是否已经开始运行了。有时，由于容器部署到集群中的节点，可能会有延迟，并且您可以使用命令`kubectl describe pods`找到详细信息。通过拥有多个副本，我们使我们的
    REST API 在发生故障时仍然可以持续可用。例如，假设其中一个 Pod 因错误而停机；仍将有两个实例提供 API。Kubernetes 还将自动在发生故障时创建另一个
    Pod 以维护所需的状态。这也是因为 REST API 是无状态的，并且在其他情况下需要额外的故障处理。
- en: 'While we have deployed and scaled the REST API, we have not made it available
    to the internet. In this final step, we will add a `LoadBalancer` service called
    `sentiment-app-loadbalancer`, which acts as the HTTP server exposing the REST
    API to the internet and directing requests to the three pods based on the traffic.
    It’s important to distinguish between the parameter `port`, which is the port
    exposed by the `LoadBalancer` and the `target-port`, which is the port exposed
    by each container:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经部署并扩展了 REST API，但我们尚未将其提供给互联网。在这最后一步中，我们将添加一个名为`sentiment-app-loadbalancer`的`LoadBalancer`服务，它作为
    HTTP 服务器将 REST API 暴露给互联网，并根据流量将请求定向到三个 Pod。重要的是要区分参数`port`，这是`LoadBalancer`暴露的端口，和`target-port`，这是每个容器暴露的端口：
- en: '[PRE43]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If you run the `kubectl get service` command, it provides a listing of all
    Kubernetes services that are running, including the `sentiment-app-loadbalancer`.
    The parameter to take note of is `EXTERNAL-IP`, which can be used to access our
    API. The `sentiment-app` can be accessed using the link *http://[EXTERNAL-IP]:5000/apidocs*,
    which will provide the Swagger documentation, and a request can be made to *http://[EXTERNAL-IP]:5000/api/v1/sentiment*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果运行`kubectl get service`命令，它将提供运行的所有 Kubernetes 服务的列表，包括`sentiment-app-loadbalancer`。需要注意的参数是`EXTERNAL-IP`，它可以用于访问我们的
    API。可以使用链接*http://[EXTERNAL-IP]:5000/apidocs*访问`sentiment-app`，该链接将提供 Swagger
    文档，并且可以向*http://[EXTERNAL-IP]:5000/api/v1/sentiment*发出请求：
- en: '[PRE44]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Let’s say you retrained the model and want to make the latest version available
    via the API. We have to build a new Docker image with a new tag (`v0.2`) and then
    set the image to that tag with the command `kubectl set image`, and Kubernetes
    will automatically update pods in the cluster in a rolling fashion. This ensures
    that our REST API will always be available but also deploy the new version using
    a rolling strategy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您重新训练了模型并希望通过 API 提供最新版本。我们必须使用新标签（`v0.2`）构建新的 Docker 映像，然后使用命令`kubectl set
    image`将映像设置为该标签，Kubernetes 将以滚动方式自动更新集群中的 Pod。这确保了我们的 REST API 将始终可用，但也使用了滚动策略来部署新版本。
- en: 'When we want to shut down our deployment and cluster, we can run the following
    commands to first delete the `LoadBalancer` service and then tear down the cluster.
    This will also release all the compute instances you were using:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要关闭部署和集群时，可以运行以下命令首先删除`LoadBalancer`服务，然后拆除集群。这也将释放您正在使用的所有计算实例：
- en: '[PRE45]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This blueprint provides a simple way to deploy and scale your machine learning
    model using cloud resources and does not cover several other aspects that can
    be crucial to production deployment. It’s important to keep track of the performance
    of your model by continuously monitoring parameters such as accuracy and adding
    triggers for retraining. To ensure the quality of predictions, one must have enough
    test cases and other quality checks before returning a result from the API. In
    addition, good software design must provide for authentication, identity management,
    and security, which should be part of any publicly available API.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个蓝图提供了一种简单的方法来使用云资源部署和扩展您的机器学习模型，并且不涵盖可以对生产部署至关重要的几个其他方面。通过持续监控诸如准确性之类的参数并添加重新训练的触发器，可以跟踪模型的性能。为了确保预测的质量，必须有足够的测试用例和其他质量检查，然后才能从
    API 返回结果。此外，良好的软件设计必须提供身份验证、身份管理和安全性，这应该是任何公开可用的 API 的一部分。
- en: 'Blueprint: Automatically Versioning and Deploying Builds'
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：自动版本控制和部署构建
- en: In the previous blueprint, we created the first deployment of our REST API.
    Consider that you now have access to additional data and retrained your model
    to achieve a higher level of accuracy. We would like to update our REST API with
    this new version so that the results of our prediction improve. In this blueprint,
    we will provide an automated way to deploy updates to your API with the help of
    GitHub actions. Since the code for this book and also the [sentiment-app](https://oreil.ly/SesD8)
    is hosted on GitHub, it made sense to use GitHub actions, but depending on the
    environment, you could use other tools, such as [GitLab](https://oreil.ly/vBS8i).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的蓝图中，我们创建了我们的 REST API 的第一个部署。考虑到您现在可以访问额外的数据并重新训练模型以达到更高的准确性水平。我们希望使用 GitHub
    Actions 提供一种自动化方式来部署更新到您的 API。由于本书和 [sentiment-app](https://oreil.ly/SesD8) 的代码都托管在
    GitHub 上，因此使用 GitHub Actions 是合理的，但根据环境的不同，您也可以使用其他工具，比如 [GitLab](https://oreil.ly/vBS8i)。
- en: We assume that you have saved the model files after retraining. Let’s check
    in our new model files and make any additional changes to `main.py`. You can see
    these additions on the [Git repository](https://oreil.ly/ktwYX). Once all the
    changes are checked in, we decide that we are satisfied and ready to deploy this
    new version. We have to tag the current state as the one that we want to deploy
    by using the `git tag v0.2` command. This binds the tag name (`v0.2`) to the current
    point in the commit history. Tags should normally follow [`Semantic Versioning`](https://semver.org),
    where version numbers are assigned in the form MAJOR.MINOR.PATCH and are often
    used to identify updates to a given software module. Once a tag has been assigned,
    additional changes can be made but will not be considered to be part of the already-tagged
    state. It will always point to the original commit. We can push the created tag
    to the repository by running `git push origin tag-name`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经在重新训练后保存了模型文件。让我们检查我们的新模型文件，并对 `main.py` 进行任何额外的更改。您可以在 [Git 存储库](https://oreil.ly/ktwYX)
    上看到这些增加。一旦所有更改都已经检入，我们决定满意并准备部署这个新版本。我们必须使用 `git tag v0.2` 命令将当前状态标记为我们要部署的状态。这将绑定标签名（`v0.2`）到当前提交历史中的特定点。标签通常应遵循
    [`语义化版本`](https://semver.org)，其中版本号以 MAJOR.MINOR.PATCH 形式分配，通常用于标识给定软件模块的更新。一旦分配了标签，可以进行额外的更改，但这些更改不会被视为已标记状态的一部分。它始终指向原始提交。我们可以通过运行
    `git push origin tag-name` 将创建的标签推送到存储库。
- en: Using GitHub actions, we have created a deployment pipeline that uses the event
    of tagging a repository to trigger the start of the deployment pipeline. This
    pipeline is defined in the *main.yml* file located in the folder *.github/workflow/*
    and defines the steps to be run each time a new tag is assigned. So whenever we
    want to release a new version of our API, we can create a new tag and push this
    to the repository.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GitHub Actions，我们创建了一个部署流水线，使用标记存储库的事件来触发部署流水线的启动。此流水线定义在位于文件夹 `.github/workflow/`
    中的 *main.yml* 文件中，并定义了每次分配新标记时运行的步骤。因此，每当我们想要发布 API 的新版本时，我们可以创建一个新的标记并将其推送到存储库。
- en: 'Let’s walk through the deployment steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下部署步骤：
- en: '[PRE47]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The file starts with a name to identify the GitHub workflow, and the `on` keyword
    specifies the events that trigger the deployment. In this case, we specify that
    only Git push commands that contain a tag will start this deployment. This ensures
    that we don’t deploy with each commit and control a deployment to the API by using
    a tag. We can also choose to build only on specific tags, for example, major version
    revisions. The `jobs` specifies the series of steps that must be run and sets
    up the environment that GitHub uses to perform the actions. The `build` parameter
    defines the kind of build machine to be used (`ubuntu`) and a time-out value for
    the entire series of steps (set to 10 minutes).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 文件以一个名称开始，用于识别 GitHub 工作流程，而 `on` 关键字则指定了触发部署的事件。在这种情况下，我们指定只有包含标签的 Git 推送命令才会启动此部署。这样可以确保我们不会在每次提交时都进行部署，并且通过标签控制
    API 的部署。我们还可以选择仅构建特定的标签，例如主要版本修订。`jobs` 指定了必须运行的一系列步骤，并设置了 GitHub 用于执行操作的环境。`build`
    参数定义了要使用的构建机器类型（`ubuntu`），以及整个步骤系列的超时值（设为 10 分钟）。
- en: 'Next, we specify the first set of actions as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将第一组操作指定如下：
- en: '[PRE48]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The first step is typically always checkout, which checks out the latest code
    on the build machine. The next step is to build the Docker container using the
    latest commit from the tag and push this to the Docker Hub registry. The `docker/build-push-action@v1`
    is a GitHub action that is already available in [GitHub Marketplace](https://oreil.ly/dHiai),
    which we reuse. Notice the use of secrets to pass in the user credentials. You
    can encrypt and store the user credentials that your deployment needs by visiting
    the Settings > Secrets tab of your GitHub repository, as shown in [Figure 13-5](#fig-github-secrets).
    This allows us to maintain security and enable automatic builds without any password
    prompts. We tag the Docker image with the same tag as the one we used in the Git
    commit. We add another step to get the tag and set this as an environment variable,
    `TAG_NAME`, which will be used while updating the cluster.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步通常是检出，它在构建机器上检出最新的代码。下一步是使用来自标签的最新提交构建 Docker 容器，并将其推送到 Docker Hub 注册表。`docker/build-push-action@v1`
    是一个已经在[GitHub Marketplace](https://oreil.ly/dHiai)中可用的 GitHub 操作，我们重复使用它。注意使用密码来传递用户凭据。您可以通过访问
    GitHub 存储库的“设置”>“密码”选项卡来加密和存储您的部署所需的用户凭据，如[图 13-5](#fig-github-secrets)所示。这使我们能够保持安全性并启用无需任何密码提示的自动构建。我们使用与
    Git 提交中使用的相同标签对 Docker 镜像进行标记。我们添加另一个步骤来获取标签，并将其设置为环境变量`TAG_NAME`，这将在更新群集时使用。
- en: '![](Images/btap_1305.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1305.jpg)'
- en: Figure 13-5\. Adding credentials to your repository using secrets.
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-5\. 使用密码向存储库添加凭据。
- en: For the deployment steps, we have to connect to our running GCP cluster and
    update the image that we use for the deployment. First, we have to add `PROJECT_ID`,
    `LOCATION_NAME`, `CLUSTER_NAME`, and `GCLOUD_AUTH` to the secrets to enable this
    action. We encode these as secrets to ensure that project details of our cloud
    deployments are not stored publicly. You can get the `GCLOUD_AUTH` by using the
    provided [instructions](https://oreil.ly/EDELL) and adding the values in the downloaded
    key as the secret for this field.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于部署步骤，我们必须连接到正在运行的 GCP 集群并更新我们用于部署的映像。首先，我们必须将`PROJECT_ID`、`LOCATION_NAME`、`CLUSTER_NAME`和`GCLOUD_AUTH`添加到密码中以启用此操作。我们将这些编码为密码，以确保我们的云部署项目细节不会公开存储。您可以通过使用提供的[说明](https://oreil.ly/EDELL)来获取`GCLOUD_AUTH`，并将下载的密钥中的值添加为此字段的密码。
- en: 'The next steps for deployment include setting up the `gcloud` utility on the
    build machine and using this to get the Kubernetes configuration file:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的下一步包括在构建机器上设置`gcloud`实用程序，并使用它获取 Kubernetes 配置文件：
- en: '[PRE49]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, we update the Kubernetes deployment with the latest Docker image.
    This is where we use the `TAG_NAME` to identify the latest release that we pushed
    in the second step. Finally, we add an action to monitor the status of the rollout
    in our cluster:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用最新的 Docker 镜像更新 Kubernetes 部署。这是我们使用`TAG_NAME`来标识我们在第二步推送的最新发布的地方。最后，我们添加一个操作来监视我们集群中滚动的状态：
- en: '[PRE50]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You can follow the various stages of the build pipeline using the Actions tab
    of your repository, as shown in [Figure 13-6](#fig-github-progress). At the end
    of the deployment pipeline, an updated version of the API should be available
    at the same URL and can also be tested by visiting the API documentation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用存储库的“操作”选项卡可以跟踪构建流水线的各个阶段，如[图 13-6](#fig-github-progress)所示。在部署流水线的末尾，应该可以在相同的网址上获取更新后的
    API 版本，并且还可以通过访问 API 文档来进行测试。
- en: This technique works well when code and model files are small enough to be packaged
    into the Docker image. If we use deep learning models, this is often not the case,
    and creating large Docker containers is not recommended. In such cases, we still
    use Docker containers to package and deploy our API, but the model files reside
    on the host system and can be attached to the Kubernetes cluster. For cloud deployments,
    this makes use of a persistent storage like [Google Persistent Disk](https://oreil.ly/OZ4Ru).
    In such cases, we can perform model updates by performing a cluster update and
    changing the attached volume.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码和模型文件足够小以打包到 Docker 镜像中时，此技术效果很好。如果我们使用深度学习模型，情况通常并非如此，不建议创建大型 Docker 容器。在这种情况下，我们仍然使用
    Docker 容器来打包和部署我们的 API，但模型文件驻留在主机系统上，并且可以附加到 Kubernetes 集群上。对于云部署，这使用像[Google
    持久磁盘](https://oreil.ly/OZ4Ru)这样的持久存储。在这种情况下，我们可以通过执行集群更新并更改附加的卷来执行模型更新。
- en: '![](Images/btap_1306.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1306.jpg)'
- en: Figure 13-6\. GitHub deployment workflow initiated by pushing a Git tag.
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-6\. 通过推送 Git 标签启动的 GitHub 部署工作流程。
- en: Closing Remarks
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We introduced a number of blueprints with the aim of allowing you to share the
    analysis and projects you created using previous chapters in this book. We started
    by showing you how to create reproducible conda environments that will allow your
    teammate or a fellow learner to easily reproduce your results. With the help of
    Docker environments, we make it even easier to share your analysis by creating
    a complete environment that works regardless of the platform or infrastructure
    that your collaborators are using. If someone would like to integrate the results
    of your analysis in their product or service, we can encapsulate the machine learning
    model into a REST API that can be called from any language or platform. Finally,
    we provided a blueprint to easily create a cloud deployment of your API that can
    be scaled up or down based on usage. This cloud deployment can be updated easily
    with a new version of your model or additional functionality. While adding each
    layer of abstraction, we make the analysis accessible to a different (and broader)
    audience and reduce the amount of detail that is exposed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了多个蓝图，旨在允许您分享使用本书前几章创建的分析和项目。我们首先向您展示如何创建可复制的conda环境，这将使您的队友或其他学习者能够轻松重现您的结果。借助Docker环境的帮助，我们甚至可以更轻松地分享您的分析，创建一个完整的环境，无论合作者使用的平台或基础设施如何，都能正常工作。如果有人希望将您的分析结果集成到他们的产品或服务中，我们可以将机器学习模型封装为一个可以从任何语言或平台调用的REST
    API。最后，我们提供了一个蓝图，可以轻松创建您的API的云部署，根据使用情况进行扩展或缩减。这种云部署可以轻松更新为模型的新版本或额外功能。在增加每一层抽象的同时，我们使分析对不同（和更广泛的）受众可访问，并减少了暴露的细节量。
- en: Further Reading
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Scully, D, et al. *Hidden Technical Debt in Machine Learning Systems*. [*https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf*](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scully, D, et al. *机器学习系统中的隐藏技术债务*。[*https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf*](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)
