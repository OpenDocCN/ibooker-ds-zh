- en: Chapter 6\. Understanding Wikipedia with LDA and Spark NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章。使用 LDA 和 Spark NLP 理解维基百科
- en: With the growing amount of unstructured text data in recent years, it has become
    difficult to obtain the relevant and desired information. Language technology
    provides powerful methods that can be used to mine through text data and fetch
    the information that we are looking for. In this chapter, we will use PySpark
    and the Spark NLP (natural language processing) library to use one such technique—topic
    modeling. Specifically, we will use the latent Dirichlet algorithm (LDA) to understand
    a dataset of Wikipedia documents.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着非结构化文本数据的增长，获取相关和期望的信息变得困难。语言技术提供了强大的方法，可以用来挖掘文本数据并获取我们所寻找的信息。在本章中，我们将使用
    PySpark 和 Spark NLP（自然语言处理）库使用一种这样的技术——主题建模。具体来说，我们将使用潜在狄利克雷算法（LDA）来理解维基百科文档的数据集。
- en: '*Topic modeling*, one of the most common tasks in natural language processing,
    is a statistical approach for data modeling that helps in discovering underlying
    topics that are present in a collection of documents. Extracting topic distribution
    from millions of documents can be useful in many ways—for example, identifying
    the reasons for complaints about a particular product or all products, or identifying
    topics in news articles. The most popular algorithm for topic modeling is LDA.
    It is a generative model that assumes that documents are represented by a distribution
    of topics. Topics, in turn, are represented by a distribution of words. PySpark
    MLlib offers an optimized version of LDA that is specifically designed to work
    in a distributed environment. We will build a simple topic modeling pipeline using
    Spark NLP for preprocessing the data and Spark MLlib’s LDA to extract topics from
    the data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*主题建模*，是自然语言处理中最常见的任务之一，是一种用于数据建模的统计方法，有助于发现文档集合中存在的潜在主题。从数百万个文档中提取主题分布在许多方面都很有用，例如，识别某个产品或所有产品投诉的原因，或在新闻文章中识别主题。主题建模的最流行算法是
    LDA。它是一个生成模型，假设文档由主题分布表示。主题本身由词语分布表示。PySpark MLlib 提供了一个专为分布式环境设计的优化版本的 LDA。我们将使用
    Spark NLP 对数据进行预处理，并使用 Spark MLlib 的 LDA 构建一个简单的主题建模管道来从数据中提取主题。'
- en: In this chapter, we’ll embark upon the modest task of distilling the human knowledge
    based on latent (hidden) topics and relationships. We’ll apply LDA to a corpus
    consisting of the articles contained in Wikipedia. We will start by understanding
    the basics of LDA and go over its implementation in PySpark. Then we’ll download
    the dataset and set up our programming environment by installing Spark NLP. This
    will be followed by data preprocessing. You will witness the power of Spark NLP
    library’s out-of-the-box methods, which make NLP tasks significantly easier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将着手于根据潜在（隐藏）主题和关系来提炼人类知识的谦虚任务。我们将应用 LDA 到包含在维基百科中的文章语料库中。我们将从理解 LDA 的基础知识开始，并在
    PySpark 中实施它。然后，我们将下载数据集，并通过安装 Spark NLP 来设置我们的编程环境。这将紧随数据预处理。你将见证到 Spark NLP
    库提供的开箱即用方法的强大功能，这使得 NLP 任务变得更加容易。
- en: Then we will score the terms in our documents using the TF-IDF (term frequency-inverse
    document frequency) technique and feed the resulting output into our LDA model.
    To finish up, we’ll go through the topics assigned by our model to the input documents.
    We should be able to understand which bucket an entry belongs in without the need
    to read it. Let’s begin by going over the fundamentals of LDA.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用 TF-IDF（词频-逆文档频率）技术对我们文档中的术语进行评分，并将结果输出到我们的 LDA 模型中。最后，我们将浏览模型分配给输入文档的主题。我们应该能够理解一个条目属于哪个桶，而无需阅读它。让我们首先复习一下
    LDA 的基础知识。
- en: Latent Dirichlet Allocation
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: The idea behind latent Dirichlet allocation is that documents are generated
    based on a set of topics. In this process, we assume that each document is distributed
    over the topics, and each topic is distributed over a set of terms. Each document
    and each word are generated from sampling these distributions. The LDA learner
    works backward and tries to identify the distributions where the observed is most
    probable.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配背后的理念是，文档是基于一组主题生成的。在这个过程中，我们假设每个文档在主题上分布，每个主题在一组术语上分布。每个文档和每个词都是从这些分布中抽样生成的。LDA
    学习者向后工作，并试图识别观察到的最有可能的分布。
- en: 'It attempts to distill the corpus into a set of relevant *topics*. Each topic
    captures a thread of variation in the data and often corresponds to one of the
    ideas that the corpus discusses. A document can be a part of multiple topics.
    You can think of LDA as providing a way to *soft cluster* the documents, too.
    Without delving into the mathematics, an LDA topic model describes two primary
    attributes: a chance of selecting a topic when sampling a particular document,
    and a chance of selecting a particular term when selecting a topic. For example,
    LDA might discover a topic with strong association with the terms “Asimov” and
    “robot,” and with the documents “foundation series” and “science fiction.” By
    selecting only the most important concepts, LDA can throw away some irrelevant
    noise and merge co-occurring strands to come up with a simpler representation
    of the data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它试图将语料库提炼为一组相关的*主题*。每个主题捕捉数据中的一个变化线索，通常对应语料库讨论的一个想法。一个文档可以是多个主题的一部分。你可以把LDA看作是一种*软聚类*文档的方式。不深入数学细节，LDA主题模型描述了两个主要属性：在抽样特定文档时选择主题的机会，以及在选择主题时选择特定术语的机会。例如，LDA可能会发现一个与术语“阿西莫夫”和“机器人”强相关的主题，并与文档“基地系列”和“科幻小说”相关联。通过仅选择最重要的概念，LDA可以丢弃一些不相关的噪音并合并共现的线索，以得到数据的更简单的表现形式。
- en: We can employ this technique in a variety of tasks. For example, it can help
    us recommend similar Wikipedia entries when provided with an input entry. By encapsulating
    the patterns of variance in the corpus, it can base scores on a deeper understanding
    than simply on counting occurrences and co-occurrences of words. Up next, let’s
    have a look at PySpark’s LDA implementation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在各种任务中应用这种技术。例如，当提供输入条目时，它可以帮助我们推荐类似的维基百科条目。通过封装语料库中的变异模式，它可以基于比简单计算单词出现和共现更深的理解来打分。接下来，让我们看一下PySpark的LDA实现。
- en: LDA in PySpark
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark中的LDA
- en: 'PySpark MLlib offers an LDA implementation as one of its clustering algorithms.
    Here’s some example code:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark MLlib提供了LDA实现作为其聚类算法之一。以下是一些示例代码：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-1)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-1)'
- en: We apply LDA to our dataframe with the number of topics (*k*) set to 2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将LDA应用于数据帧，主题数(*k*)设定为2。
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-2)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-2)'
- en: Dataframe describing the probability weight associated with each term in our
    topics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 描述我们主题中每个术语关联的概率权重的数据帧。
- en: We will explore PySpark’s LDA implementation and associated parameters when
    applying it to the Wikipedia dataset. First, though, we need to download the relevant
    dataset. That’s what we will do next.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索PySpark的LDA实现及其应用于维基百科数据集时相关的参数。不过首先，我们需要下载相关的数据集。接下来我们将做这件事。
- en: Getting the Data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'Wikipedia makes dumps of all its articles available. The full dump comes in
    a single, large XML file. These can be [downloaded](https://oreil.ly/DhGlJ) and
    then placed on a cloud storage solution (such as AWS S3 or GCS, Google Cloud Storage)
    or HDFS. For example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Wikipedia提供其所有文章的dump。完整的dump以单个大XML文件的形式出现。这些可以被[下载](https://oreil.ly/DhGlJ)，然后放置在云存储解决方案（如AWS
    S3或GCS，Google Cloud Storage）或HDFS上。例如：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This can take a little while.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一些时间。
- en: Chugging through this volume of data makes the most sense with a cluster of
    a few nodes to work with. To run this chapter’s code on a local machine, a better
    option is to generate a smaller dump using [Wikipedia’s export pages](https://oreil.ly/Rrpmr).
    Try getting all the pages from multiple categories that have many pages and few
    subcategories, such as biology, health, and geometry. For the following code to
    work, download the dump into the *ch06-LDA/* directory and rename it to *wikidump.xml*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些数据量最合理的方式是使用几个节点的集群进行。要在本地机器上运行本章的代码，更好的选择是使用[Wikipedia导出页面](https://oreil.ly/Rrpmr)生成一个较小的dump。尝试获取多个页面数多且子类别少的多个类别，例如生物学、健康和几何学。为了使下面的代码起作用，请将dump下载到*ch06-LDA/*目录并将其重命名为*wikidump.xml*。
- en: We need to convert the Wikipedia XML dump into a format that we can easily work
    with in PySpark. When working on our local machine, we can use the convenient
    [WikiExtractor tool](https://oreil.ly/pfwrE) for this. It extracts and cleans
    text from Wikipedia database dumps such as what we have.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将 Wikipedia XML 转储文件转换为 PySpark 轻松处理的格式。在本地机器上工作时，我们可以使用便捷的 [WikiExtractor
    工具](https://oreil.ly/pfwrE)。它从 Wikipedia 数据库转储中提取和清理文本，例如我们所拥有的。
- en: 'Install it using pip:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pip 安装它：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then it’s as simple as running the following command in the directory containing
    the downloaded file:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需在包含下载文件的目录中运行以下命令：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is stored in a single or several files of similar size in a given
    directory named `text`. Each file will contains several documents in the following
    format:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出存储在名为`text`的给定目录中的一个或多个文件中。每个文件将以以下格式包含多个文档：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO2-1)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO2-1)'
- en: Rename text directory to wikidump
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将 text 目录重命名为 wikidump
- en: Next, let’s get familiar with the Spark NLP library before we start working
    on the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在开始处理数据之前熟悉 Spark NLP 库。
- en: Spark NLP
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark NLP
- en: The Spark NLP library was originally designed by [John Snow Labs](https://oreil.ly/E9KVt)
    in early 2017 as an annotation library native to Spark to take full advantage
    of Spark SQL and MLlib modules. The inspiration came from trying to use Spark
    to distribute other NLP libraries, which were generally not implemented with concurrency
    or distributed computing in mind.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Spark NLP 库最初由 [John Snow Labs](https://oreil.ly/E9KVt) 于 2017 年初设计，作为一种本地于
    Spark 的注释库，以充分利用 Spark SQL 和 MLlib 模块的功能。灵感来自于尝试使用 Spark 分发其他 NLP 库，这些库通常没有考虑并发性或分布式计算。
- en: Spark NLP has the same concepts as any other annotation library but differs
    in how it stores annotations. Most annotation libraries store the annotations
    in the document object, but Spark NLP creates columns for the different types
    of annotations. The annotators are implemented as transformers, estimators, and
    models. We will look at these in the next section when applying them to our dataset
    for preprocessing. Before that, let’s download and set up Spark NLP on our system.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Spark NLP 具有与任何其他注释库相同的概念，但在注释存储方式上有所不同。大多数注释库将注释存储在文档对象中，但 Spark NLP 为不同类型的注释创建列。注释器实现为转换器、估计器和模型。在下一节中，当我们将它们应用于我们的数据集进行预处理时，我们将查看它们。在此之前，让我们在系统上下载并设置
    Spark NLP。
- en: Setting Up Your Environment
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置您的环境：
- en: 'Install Spark NLP via pip:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 pip 安装 Spark NLP：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Start the PySpark shell:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 PySpark shell：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s import Spark NLP in our PySpark shell:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 PySpark shell 中导入 Spark NLP：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, you can import the relevant Spark NLP modules that we’ll use:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以导入我们将使用的相关 Spark NLP 模块：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have set up our programming environment, let’s start working on
    our dataset. We’ll start by parsing the data as a PySpark DataFrame.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了编程环境，让我们开始处理我们的数据集。我们将从将数据解析为 PySpark DataFrame 开始。
- en: Parsing the Data
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析数据
- en: 'The output from WikiExtractor can create multiple directories depending on
    the size of the input dump. We want to import all the data as a single DataFrame.
    Let’s start by specifying the input directory:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: WikiExtractor 的输出可能会根据输入转储的大小创建多个目录。我们希望将所有数据导入为单个 DataFrame。让我们从指定的输入目录开始：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We import the data using the `wholeTextFiles` method accessible through `sparkContext`.
    This method reads the data in as an RDD. We convert it into a DataFrame since
    that’s what we want:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `sparkContext` 可访问的 `wholeTextFiles` 方法导入数据。该方法将数据读取为 RDD。我们将其转换为 DataFrame，因为这是我们想要的：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting DataFrame will consist of two columns. The number of records
    will correspond to the number of files that were read. The first column consists
    of the filepath and the second contains the corresponding text content. The text
    contains multiple entries, but we want each row to correspond to a single entry.
    Based on the entry structure that we had seen earlier, we can separate entries
    using a couple of PySpark utilities: `split` and `explode`.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame 将由两列组成。记录的数量将对应于已读取的文件数量。第一列包含文件路径，第二列包含相应的文本内容。文本包含多个条目，但我们希望每行对应一个单独的条目。基于我们之前看到的条目结构，我们可以使用几个
    PySpark 工具：`split` 和 `explode` 来分隔条目。
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `split` function is used to split the DataFrame string `Column` into an
    array based on matches of a provided pattern. In the previous code, we split the
    combined document XML string into an array based on the *</doc>* string. This
    effectively gives us an array of multiple documents. Then, we use `explode` to
    create new rows for each element in the array returned by the `split` function.
    This results in rows being created corresponding to each document.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`split` 函数用于根据提供的模式将 DataFrame 字符串 `Column` 拆分为数组。在之前的代码中，我们根据 *</doc>* 字符串将组合的文档
    XML 字符串拆分为数组。这实际上为我们提供了一个包含多个文档的数组。然后，我们使用 `explode` 将根据 `split` 函数返回的数组中的每个元素创建新行。这导致为每个文档创建相应的行。'
- en: 'Go through the structure obtained in the `content` column by our previous operation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们之前操作的 `content` 列获取的结构进行遍历：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can further split our `content` column by extracting the entries’ titles:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提取条目的标题来进一步拆分我们的 `content` 列：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have our parsed dataset, let’s move on to preprocessing using Spark
    NLP.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了解析后的数据集，让我们使用 Spark NLP 进行预处理。
- en: Preparing the Data Using Spark NLP
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark NLP 准备数据
- en: We had earlier mentioned that a library based on the document-annotator model
    such as Spark NLP has the concept of “documents.” There does not exist such a
    concept natively in PySpark. Hence, one of Spark NLP’s core design tenets is strong
    interoperability with MLlib. This is done by providing DataFrame-compatible transformers
    that convert text columns into documents and convert annotations into PySpark
    data types.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，基于文档注释器模型的库（如 Spark NLP）具有“文档”的概念。在 PySpark 中本地不存在这样的概念。因此，Spark NLP
    的一个核心设计原则之一是与 MLlib 强大的互操作性。通过提供与 DataFrame 兼容的转换器，将文本列转换为文档，并将注释转换为 PySpark 数据类型。
- en: 'We start by creating our `document` column using `DocumentAssembler`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过 `DocumentAssembler` 创建我们的 `document` 列：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We could have utilized Spark NLP’s [`DocumentNormalizer`](https://oreil.ly/UL1vp)
    annotator in the parsing section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在解析部分利用 Spark NLP 的 [`DocumentNormalizer`](https://oreil.ly/UL1vp) 注释器。
- en: We can transform the input dataframe directly as we have done in the previous
    code. However, we will instead use `DocumentAssembler` and other required annotators
    as part of an ML pipeline.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前的代码中所做的那样直接转换输入的 DataFrame。但是，我们将使用 `DocumentAssembler` 和其他必需的注释器作为 ML
    流水线的一部分。
- en: 'We will use the following annotators as part of our preprocessing pipeline:
    `Tokenizer`, `Normalizer`, `StopWordsCleaner`, and `Stemmer`.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下注释器作为我们的预处理流水线的一部分：`Tokenizer`、`Normalizer`、`StopWordsCleaner` 和 `Stemmer`。
- en: 'Let’s start with the `Tokenizer`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `Tokenizer` 开始：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `Tokenizer` is a fundamental annotator. Almost all text-based data processing
    begins with some form of tokenization, which is the process of breaking raw text
    into small chunks. Tokens can be words, characters, or subwords (n-grams). Most
    classical NLP algorithms expect tokens as the basic input. Many deep learning
    algorithms are being developed that take characters as basic input. Most NLP applications
    still use tokenization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tokenizer` 是一个基础注释器。几乎所有基于文本的数据处理都以某种形式的分词开始，这是将原始文本分解成小块的过程。标记可以是单词、字符或子词（n-gram）。大多数经典的自然语言处理算法都期望标记作为基本输入。正在开发许多将字符作为基本输入的深度学习算法。大多数自然语言处理应用程序仍然使用分词。'
- en: 'Next up is the `Normalizer`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `Normalizer`：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `Normalizer` cleans out tokens from the previous step and removes all unwanted
    characters from the text.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalizer` 清理前一步骤中的标记，并从文本中删除所有不需要的字符。'
- en: 'Next up is the `StopWordsCleaner`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `StopWordsCleaner`：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This annotator removes *stop words* from text. Stop words such as “the,” “is,”
    and “at,” which are so common that they can be removed without significantly altering
    the meaning of a text. Removing stop words is useful when one wants to deal with
    only the most semantically important words in a text and ignore words that are
    rarely semantically relevant, such as articles and prepositions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个注释器从文本中删除*停用词*。停用词如“the”、“is”和“at”非常常见，可以在不显著改变文本含义的情况下删除。从文本中删除停用词在希望处理只有最重要的语义单词而忽略文章和介词等很少有语义相关性的词时非常有用。
- en: 'Last up is the `Stemmer`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是 `Stemmer`：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Stemmer` returns hard stems out of words with the objective of retrieving
    the meaningful part of the word. *Stemming* is the process of reducing a word
    to its root word stem with the objective of retrieving the meaningful part. For
    example, “picking,” “picked,” and “picks” all have “pick” as the root.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`Stemmer` 返回单词的硬干出来，目的是检索单词的有意义部分。*Stemming* 是将单词减少到其根词干的过程，目的是检索有意义的部分。例如，“picking”，“picked”
    和 “picks” 都以 “pick” 作为根。'
- en: 'We are almost done. Before we can complete our NLP pipeline, we need to add
    the `Finisher`. Spark NLP adds its own structure when we convert each row in the
    dataframe to a document. `Finisher` is critical because it helps us to bring back
    the expected structure, an array of tokens:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了。在完成我们的 NLP 管道之前，我们需要添加`Finisher`。当我们将数据帧中的每一行转换为文档时，Spark NLP 添加了自己的结构。`Finisher`
    非常关键，因为它帮助我们恢复预期的结构，即一个令牌数组：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we have all the required pieces in place. Let’s build our pipeline so that
    each phase can be executed in sequence:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好所有必需的组件。让我们构建我们的管道，以便每个阶段可以按顺序执行：
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Execute the pipeline and transform the dataframe:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 执行管道并转换数据帧：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-1)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-1)'
- en: Train the pipeline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练管道。
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-2)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-2)'
- en: Apply the pipeline to transform the dataframe.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将管道应用于转换数据帧。
- en: 'The NLP pipeline created intermediary columns that we do not need. Let’s remove
    the redundant columns:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 管道创建了我们不需要的中间列。让我们删除这些冗余列：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, we will understand the basics of TF-IDF and implement it on the preprocessed
    dataset, `token_df`, that we have obtained before building an LDA model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解 TF-IDF 的基础知识，并在预处理数据集 `token_df` 上实施它，该数据集在构建 LDA 模型之前已经获得。
- en: TF-IDF
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Before applying LDA, we need to convert our data into a numeric representation.
    We will obtain such a representation using the term frequency-inverse document
    frequency method. Loosely, TF-IDF is used to determine the importance of terms
    corresponding to given documents. Here’s a representation in Python code of the
    formula. We won’t actually end up using this code because PySpark provides its
    own implementation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用 LDA 之前，我们需要将数据转换为数字表示。我们将使用术语频率-逆文档频率方法来获得这样的表示。大致来说，TF-IDF 用于确定与给定文档对应的术语的重要性。这里是
    Python 代码中该公式的表示。我们实际上不会使用这段代码，因为 PySpark 提供了自己的实现。
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: TF-IDF captures two intuitions about the relevance of a term to a document.
    First, we would expect that the more often a term occurs in a document, the more
    important it is to that document. Second, not all terms are equal in a global
    sense. It is more meaningful to encounter a word that occurs rarely in the entire
    corpus than a word that appears in most of the documents; thus, the metric uses
    the *inverse* of the word’s appearance in documents in the full corpus.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'TF-IDF 捕捉了有关术语与文档相关性的两种直觉。首先，我们预期一个术语在文档中出现的次数越多，它对该文档的重要性就越大。其次，在全局意义上，并非所有术语都是相等的。在整个语料库中很少出现的词比大多数文档中出现的词更有意义；因此，该度量使用术语在整个语料库中出现的逆数。 '
- en: The frequency of words in a corpus tends to be distributed exponentially. A
    common word might appear ten times as often as a mildly common word, which in
    turn might appear ten or a hundred times as often as a rare word. Basing a metric
    on the raw inverse document frequency would give rare words enormous weight and
    practically ignore the impact of all other words. To capture this distribution,
    the scheme uses the *log* of the inverse document frequency. This mellows the
    differences in document frequencies by transforming the multiplicative gaps between
    them into additive gaps.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中单词的频率往往呈指数分布。一个常见的词可能出现的次数是一个稍微常见的词的十倍，而后者可能出现的次数是一个稀有词的十到一百倍。基于原始逆文档频率的度量会赋予稀有词巨大的权重，并几乎忽略所有其他词的影响。为了捕捉这种分布，该方案使用逆文档频率的*对数*。这通过将它们之间的乘法差距转换为加法差距，使文档频率之间的差异变得温和。
- en: The model relies on a few assumptions. It treats each document as a “bag of
    words,” meaning that it pays no attention to the ordering of words, sentence structure,
    or negations. By representing each term once, the model has difficulty dealing
    with *polysemy*, the use of the same word for multiple meanings. For example,
    the model can’t distinguish between the use of “band” in “Radiohead is the best
    band ever” and “I broke a rubber band.” If both sentences appear often in the
    corpus, it may come to associate “Radiohead” with “rubber.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型依赖于一些假设。它将每个文档视为“词袋”，即不关注单词的顺序、句子结构或否定。通过表示每个术语一次，该模型难以处理 *一词多义*，即同一个词具有多个含义的情况。例如，模型无法区分“Radiohead
    is the best band ever” 和 “I broke a rubber band” 中的“band” 的使用。如果这两个句子在语料库中频繁出现，它可能会将“Radiohead”
    与“rubber” 关联起来。
- en: Let’s proceed now to the implementation of TF-IDF using PySpark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续实现使用 PySpark 的 TF-IDF。
- en: Computing the TF-IDFs
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算 TF-IDFs
- en: 'First, we’ll calculate TF (term frequency; that is, the frequency of each term
    in a document) with `CountVectorizer`, which keeps track of the vocabulary that’s
    being created so we can map our topics back to their corresponding words. TF creates
    a matrix that counts how many times each word in the vocabulary appears in each
    body of text. This then gives each word a weight based on its frequency. We derive
    the vocabulary of our data while fitting and get the counts at the transform step:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `CountVectorizer` 计算 TF（术语频率；即文档中每个术语的频率），它会跟踪正在创建的词汇表，以便我们可以将我们的主题映射回相应的单词。TF
    创建一个矩阵，统计词汇表中每个词在每个文本主体中出现的次数。然后，根据其频率给每个词赋予一个权重。我们在拟合时获取数据的词汇表并在转换步骤中获取计数：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we proceed with IDF (the inverse frequency of documents where a term
    occurred), which reduces the weights of commonly appearing terms:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们进行 IDF（文档中包含某个术语的反向频率），它会减少常见术语的权重：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is what the result will look like:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With all the preprocessing and feature engineering done, we can now create our
    LDA model. That’s what we’ll do in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有预处理和特征工程完成后，我们现在可以创建我们的 LDA 模型。这将在下一节中进行。
- en: Creating Our LDA Model
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的 LDA 模型
- en: We had mentioned previously that LDA distills a corpus into a set of relevant
    topics. We will get to have a look at examples of such topics further ahead in
    this section. Before that, we need to decide on two hyperparameters that our LDA
    model requires. They are number of topics (referred to as *k*) and number of iterations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，LDA 将语料库提炼为一组相关主题。我们将在本节后面进一步查看此类主题的示例。在此之前，我们需要决定我们的 LDA 模型所需的两个超参数。它们是主题数量（称为
    *k*）和迭代次数。
- en: There are multiple ways that you can go about choosing *k*. Two popular metrics
    used for doing this are perplexity and topic coherence. The former is made available
    by PySpark’s implementation. The basic idea is to try to figure out the *k* where
    the improvements to these metrics start to become insignificant. If you are familiar
    with the *elbow method* for finding the number of clusters for K-means, this is
    similar. Depending on the size of your corpus, it can be a resource-intensive
    and time-consuming process since you will need to build the model for multiple
    values of *k*. An alternative could be to try to create a representative sample
    of the dataset in hand and use it to determine *k*. It is left as an exercise
    for you to read up on this and try it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以选择 *k*。用于此目的的两种流行指标是困惑度和主题一致性。前者由 PySpark 的实现提供。基本思想是试图找出这些指标改善开始变得不显著的
    *k*。如果您熟悉 K-means 的寻找簇数的“拐点法”，这类似。根据语料库的大小，这可能是一个资源密集和耗时的过程，因为您需要为多个 *k* 值构建模型。另一个选择可能是尝试创建数据集的代表性样本，并使用它来确定
    *k*。您可以阅读相关资料并尝试这一点。
- en: Since you may be working locally right now, we will go ahead and assign reasonable
    values (*k* as 5 and `max_iter` as 50) for now.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您现在可能正在本地工作，我们将暂时设定合理的值（*k* 为 5，`max_iter` 为 50）。
- en: 'Let’s create our LDA model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的 LDA 模型：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Perplexity is a measurement of how well a model predicts a sample. A low perplexity
    indicates the probability distribution is good at predicting the sample. When
    comparing different models, go for the one with the lower value of perplexity.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度是衡量模型对样本预测能力的指标。低困惑度表明概率分布在预测样本方面表现良好。在比较不同模型时，选择困惑度值较低的模型。
- en: Now that we have created our model, we want to output the topics as human-readable.
    We will get the vocabulary generated from our preprocessing steps, get the topics
    from the LDA model, and map both of them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了我们的模型，我们想将主题输出为人类可读的形式。我们将从我们的预处理步骤中生成的词汇表中获取词汇表，从LDA模型中获取主题，并将它们映射起来。
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-1)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-1)'
- en: Create a reference to our vocabulary.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 创建对我们词汇表的引用。
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-2)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-2)'
- en: Get topics generated by the LDA model using `describeTopics` and load them into
    a Python list.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`describeTopics`从LDA模型获取生成的主题，并将它们加载到Python列表中。
- en: '[![3](assets/3.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-3)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-3)'
- en: Get indices of the vocabulary terms from our topics.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的主题中获取词汇表术语的索引。
- en: 'Let us now generate the mappings from our topic indices to our vocabulary:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成从我们的主题索引到我们的词汇表的映射：
- en: '[PRE29]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The previous result is not perfect, but there are some patterns that can be
    noticed in the topics. Topic 1 is primarily related to health. It also contains
    references to Islam and empire. Could it be because of them being referenced in
    medicinal history and vice versa or something else? Topics 2 and 3 are related
    to mathematics with the latter inclined toward geometry. Topic 5 is a mix of computing
    and mathematics. Even if you hadn’t read any of the documents, you can already
    guess with a reasonable accuracy about their categories. This is exciting!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果并不完美，但在主题中可以注意到一些模式。主题1主要与健康有关。它还包含对伊斯兰教和帝国的引用。这可能是因为它们在医学历史中被引用，反之亦然，或者其他原因？主题2和3与数学有关，后者倾向于几何学。主题5是计算和数学的混合体。即使您没有阅读任何文档，您也可以以合理的准确度猜测它们的类别。这很令人兴奋！
- en: We can now also check which topics our input documents are most closely related
    to. A single document can have multiple topic associations that are significant.
    For now, we’ll only look at the most strongly associated topics.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们还可以检查我们的输入文档与哪些主题最相关。单个文档可以具有多个显著的主题关联。目前，我们只会查看与之最相关的主题。
- en: 'Let’s run the LDA model’s transform operation on our input dataframe:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的输入数据帧上运行LDA模型的转换操作：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, each document has a topic probability distribution associated
    with it. To get the associated topic for each document, we want to find out the
    topic index with the highest probability score. We can then map it to the topics
    that we obtained previously.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，每个文档都与其相关的主题概率分布相关联。为了获取每个文档的相关主题，我们想找出具有最高概率得分的主题索引。然后，我们可以将其映射到之前获取的主题。
- en: 'We will write a PySpark UDF to find the highest topic probability score for
    each record:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写一个PySpark UDF，以查找每条记录的最高主题概率分数：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Topic 2, if you remember, was associated with mathematics. The output is in
    line with our expectations. You can scan more of the dataset to see how it performed.
    You can select particular topics using the `where` or `filter` commands and compare
    them against the topic list generated earlier to get a better sense of the clusters
    that have been created. As promised at the beginning of the chapter, we’re able
    to cluster articles into different topics without reading them!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Topic 2，如果你还记得，与数学有关。输出结果符合我们的预期。您可以扫描更多数据集，查看其表现。您可以使用`where`或`filter`命令选择特定主题，并与之前生成的主题列表进行比较，以更好地理解已创建的聚类。正如本章开头所承诺的那样，我们能够将文章分成不同的主题，而无需阅读它们！
- en: Where to Go from Here
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来的步骤
- en: In this chapter, we performed LDA on the Wikipedia corpus. In the process, we
    also learned about text preprocessing using the amazing Spark NLP library and
    the TF-IDF technique. You can further build on this by improving the model by
    better preprocessing and hyperparameter tuning. In addition, you can even try
    to recommend similar entries based on document similarity when provided with user
    input. Such a similarity measure may be obtained by using the probability distribution
    vector obtained from LDA.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对维基百科语料库执行了LDA。在这个过程中，我们还学习了使用令人惊叹的Spark NLP库和TF-IDF技术进行文本预处理。您可以通过改进模型的预处理和超参数调整来进一步构建这一过程。此外，当用户提供输入时，您甚至可以尝试基于文档相似性推荐类似条目。这样的相似性度量可以通过使用从LDA获得的概率分布向量来获得。
- en: In addition, a variety of other methods exist for understanding large corpora
    of text. For example, a technique known as latent semantic analysis (LSA) is useful
    in similar applications and was used in the previous edition of this book on the
    same dataset. Deep learning, which is explored in [Chapter 10](ch10.xhtml#image_similarity_detection_with_deep_learning_and_pyspark_lsh),
    also offers avenues to perform topic modeling. You can explore them on your own.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还存在多种其他方法来理解大型文本语料库。例如，被称为潜在语义分析（LSA）的技术在类似的应用中非常有用，并且在本书的上一版本中也使用了相同数据集。深度学习，在[第10章](ch10.xhtml#image_similarity_detection_with_deep_learning_and_pyspark_lsh)中进行了探讨，也提供了进行主题建模的途径。您可以自行探索这些方法。
