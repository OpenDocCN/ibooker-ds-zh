- en: Chapter 9\. Best Practices for Maintaining Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章 维护管道的最佳实践
- en: Up to this point, this book has been focused on building data pipelines. This
    chapter discusses how to maintain those pipelines as you encounter increased complexity
    and deal with the inevitable changes in the systems that your pipelines rely on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这本书主要关注于构建数据管道。本章讨论如何在遇到增加复杂性并处理管道依赖系统中不可避免的变更时，保持这些管道的维护。
- en: Handling Changes in Source Systems
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理源系统的变更
- en: One of the most common maintenance challenges for data engineers is dealing
    with the fact that the systems they ingest data from are not static. Developers
    are always making changes to their software, either adding features, refactoring
    the codebase, or fixing bugs. When those changes introduce a modification to the
    schema or meaning of data to be ingested, a pipeline is at risk of failure or
    inaccuracy.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据工程师来说，最常见的维护挑战之一是处理数据来源系统不是静态的这一事实。开发人员总是在他们的软件中进行更改，无论是添加功能，重构代码库，还是修复错误。当这些更改引入要摄取的数据的模式或含义修改时，管道就有可能面临失败或不准确的风险。
- en: As discussed throughout this book, the reality of a modern data infrastructure
    is that data is ingested from a large diversity of sources. As a result, it’s
    difficult to find a one-size-fits-all solution to handling schema and business
    logic changes in source systems. Nonetheless, there a few best practices I recommend
    investing in.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书中多次讨论的那样，现代数据基础设施的现实是数据从多种来源摄入。因此，在处理源系统中的模式和业务逻辑变更时很难找到一种适合所有情况的解决方案。尽管如此，我建议投资一些最佳实践。
- en: Introduce Abstraction
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入抽象化
- en: Whenever possible, it’s best to introduce a layer of abstraction between the
    source system and the ingestion process. It’s also important for the owner of
    the source system to either maintain or be aware of the abstraction method.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，最好在源系统和摄取过程之间引入抽象层。同样重要的是，源系统的所有者要么维护抽象方法，要么了解这种抽象方法。
- en: For example, instead of ingesting data directly from a Postgres database, consider
    working with the owner of the database to build a REST API that pulls from the
    database and can be queried for data extraction. Even if the API is simply a passthrough,
    the fact that it exists in a codebase maintained by the owner of the source system
    means that the system owner is aware of what data is being extracted and doesn’t
    have to worry about changes to the internal structure of their Postgres application
    database. If they choose to modify the structure of a database table, they’ll
    need to make a modification to the API but won’t need to consider what other code
    might rely on it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，不要直接从Postgres数据库摄取数据，考虑与数据库所有者合作构建一个REST API，从数据库中拉取数据并可以查询以进行数据提取。即使API只是一个透传，但它存在于源系统所有者维护的代码库中意味着系统所有者知道正在提取哪些数据，不必担心其Postgres应用数据库的内部结构变更。如果他们选择修改数据库表的结构，他们需要对API进行修改，但不必考虑其他可能依赖它的代码。
- en: In addition, if the change to the source system results in the removal of a
    field that a supported API endpoint includes, then a conscience decision regarding
    what to do can be made. Perhaps the field is phased out over time or is supported
    with historical data but is NULL going forward. Either way, there is an awareness
    of the need to handle the change when an explicit abstraction layer exists.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果对源系统的更改导致支持的API端点中的字段被删除，则可以做出有意识的决定。也许该字段会随时间逐渐淘汰，或者在历史数据中支持但在未来为NULL。无论哪种方式，存在一个明确的抽象层时，都意识到需要处理这种变更。
- en: REST APIs are not the only option for abstraction and at times are not the best
    fit. Publishing data via a Kafka topic is an excellent way to maintain an agreed-upon
    schema while leaving the particulars of the source system that publishes an event
    and the system that subscribes to it (the ingestion) completely separate from
    each other.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: REST API并不是唯一的抽象选项，有时并不是最佳选择。通过Kafka主题发布数据是一种维持约定模式的绝佳方式，同时完全将发布事件的源系统和订阅它的系统（摄取）的细节分开。
- en: Maintain Data Contracts
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护数据契约
- en: If you must ingest data directly from a source system’s database or via some
    method that is not explicitly designed for your extraction, creating and maintaining
    a data contract is a less technical solution to managing schema and logic changes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果必须直接从源系统的数据库或通过一些未明确为您的提取而设计的方法摄取数据，则创建和维护数据合同是管理模式和逻辑变更的较少技术性解决方案。
- en: A data contract may be written in the form of a text document, but preferably
    in a standardized configuration file such as in [Example 9-1](#ex_0901). In this
    example, a data contract for an ingestion from a table in a Postgres database
    is stored in JSON form.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据合同可以以文本文档的形式编写，但最好以标准化的配置文件形式编写，例如在[示例 9-1](#ex_0901)中。在此示例中，来自Postgres数据库中表的摄入数据合同以JSON形式存储。
- en: Example 9-1\. orders_contract.json
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-1\. orders_contract.json
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once you create data contracts, here are some ways you can use them to stay
    ahead of any source system changes that risk the integrity of your pipelines:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了数据合同，以下是您可以使用它们来提前应对任何可能影响管道完整性的源系统变更的几种方式：
- en: Build a Git hook that looks for any changes (schema or logic) to a table listed
    as a `source_table` in a data contract when a PR is submitted or code is committed
    to a branch. Automatically notify the contributor that the table is used in a
    data ingestion and who to contact (the `ingestion_owner`) for coordination of
    the change.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个Git钩子，在提交PR或提交代码到分支时查找表中的任何更改（模式或逻辑），这些表在数据合同中列为`source_table`。自动通知贡献者该表用于数据摄入，以及联系方式（`ingestion_owner`）以协调变更。
- en: If the data contract itself is in a Git repo (and it should be!), add a Git
    hook to check for changes to the contract. For example, if the frequency that
    the ingestion runs is increased, not only should the data contract be updated,
    but the source system owner should be consulted to ensure there is not a negative
    impact on a production system.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据合同本身位于Git存储库中（应该是！），请添加一个Git钩子来检查合同的变更。例如，如果增加摄入运行的频率，则不仅应更新数据合同，还应与源系统所有者协商，以确保对生产系统没有负面影响。
- en: Publish in readable form of all data contracts on the company’s centralized
    documentation site and make them searchable.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在公司集中文档站点上发布所有数据合同的可读形式，并使其可搜索。
- en: Write and schedule a script to notify source system and ingestion owners of
    any data contracts that haven’t been updated in the past six months (or other
    frequency) and ask them to review and update if needed.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写并安排一个脚本，以通知源系统和摄入所有者任何在过去六个月内未更新的数据合同（或其他频率），并要求他们进行审核和更新（如果需要）。
- en: Whether automated or not, the goal is for changes to the data being ingested
    or the method of ingestion (say from incremental to full load) to be flagged and
    communicated ahead of any issues in the pipeline or source system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是自动化还是手动操作，目标都是在数据摄入或摄入方法（例如从增量到完整加载）发生变化之前，及时标记并与管道或源系统的任何问题进行通信。
- en: Limits of Schema-on-Read
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取时模式的限制
- en: One approach to dealing with changes to the schema of source data is to move
    from a *schema-on-write* design to *schema-on-read*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 处理源数据模式变更的一种方法是从*写入时*设计转向*读取时*设计。
- en: Schema-on-write is the pattern used throughout this book; in particular, in
    Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05). When data is extracted
    from a source, the structure (schema) is defined, and the data is written to the
    data lake or S3 bucket. Then, when the load step in the ingestion is run, the
    data is in a predictable form and can be loaded into a defined table structure.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中贯穿始终采用的是写入时模式；特别是在第[4](ch04.xhtml#ch04)章和第[5](ch05.xhtml#ch05)章。从源头提取数据时，定义结构（模式），然后将数据写入数据湖或S3存储桶。然后，在运行摄入中的加载步骤时，数据处于可预测的形式，并且可以加载到定义的表结构中。
- en: Schema-on-read is a pattern where data is written to a data lake, S3 bucket,
    or other storage system with no strict schema. For example, an event defining
    an order placed in a system might be defined as a JSON object, but the properties
    of that object might change over time as new ones are added or existing ones are
    removed. In this case, the schema of the data is not known until it’s *read*,
    which is why it’s called schema-on-read.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 读取时模式是一种模式，其中数据写入数据湖、S3存储桶或其他存储系统，没有严格的模式。例如，定义系统中放置订单的事件可能被定义为JSON对象，但随着时间的推移，该对象的属性可能会随着新属性的添加或现有属性的移除而发生变化。在这种情况下，直到*读取*数据时才能知道数据的模式，这就是为什么称其为读取时模式。
- en: While very efficient for writing data to storage, this pattern adds complexity
    to the load step and has some major implications in a pipeline. From a technical
    perspective, reading data stored in this way from an S3 bucket is quite easy.
    Amazon Athena and other products make querying the raw data as simple as writing
    a SQL query. However, maintaining the definition of the data is no small task.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于将数据写入存储非常有效，但这种模式会增加加载步骤的复杂性，并在管道中产生一些重大影响。从技术角度来看，从S3存储桶中读取以这种方式存储的数据非常容易。Amazon
    Athena和其他产品使得查询原始数据就像编写SQL查询一样简单。然而，维护数据的定义并不是一件小事。
- en: First, you’ll want to make use of a *data catalog* that integrates with whatever
    tool you are using to read the schema-flexible data during the load step. A data
    catalog stores metadata for the data in your data lake and warehouse. It can store
    both the structure and the definition of datasets. For schema-on-read, it’s critical
    to define and store the structure of data in a catalog for both pragmatic use
    and human reference. [AWS Glue Data Catalog](https://oreil.ly/BpXT7) and [Apache
    Atlas](https://atlas.apache.org) are popular data catalogs, but there are many
    more to choose from.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要使用与在加载步骤中读取模式灵活数据的工具集成的*数据目录*。数据目录存储数据湖和仓库中数据的元数据。它可以存储数据集的结构和定义。对于按需模式读取，定义和存储数据目录中数据的结构对于实用和人类参考都至关重要。[AWS
    Glue数据目录](https://oreil.ly/BpXT7)和[Apache Atlas](https://atlas.apache.org)是流行的数据目录，但还有许多其他选择。
- en: Second, the logic of your load step becomes more complex. You’ll need to consider
    how you’ll handle schema changes dynamically. Do you want to dynamically add new
    columns to a table in your warehouse when new fields are detected during an ingestion?
    How will you notify data analysts who are modeling the data in the transform step
    in a pipeline or changes to their source tables?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要考虑加载步骤的逻辑变得更加复杂。您需要考虑如何在检测到摄取过程中的新字段时动态添加新列到仓库中的表中。在管道的转换步骤中，您如何通知数据分析师或者更改他们源表格的人建模的数据？
- en: If you choose to take a schema-on-read approach, you’ll want to get serious
    about *data governance*, which includes not only cataloging your data, but also
    defining the standards and process around how data is used in an organization.
    Data governance is a broad topic, and an important one regardless of how you ingest
    data. However, it’s a topic that can’t be ignored at a technical level if you
    do choose a schema-on-read approach.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择按需模式读取方法，您需要认真对待*数据治理*，这不仅包括对数据进行分类，还包括定义组织中数据使用的标准和流程。数据治理是一个广泛的主题，无论您如何摄取数据，都是一个重要的主题。但是，如果选择按需模式读取方法，在技术层面上是一个不可忽视的话题。
- en: Scaling Complexity
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展复杂性
- en: Building data pipelines when source systems and downstream data models are limited
    is challenging enough. When those numbers get large, as they do in even relatively
    small organizations, there are some challenges to scaling pipelines to handle
    the increased complexity. This section includes some tips and best practices for
    doing so at various stages of a pipeline.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当源系统和下游数据模型有限时，构建数据管道本身就具有挑战性。即使在相对较小的组织中，一旦这些数字变大，扩展管道以处理增加的复杂性也会面临一些挑战。本节包括在管道各个阶段应对这些挑战的一些提示和最佳实践。
- en: Standardizing Data Ingestion
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化数据摄取
- en: 'When it comes to complexity, the number of systems you ingest from is typically
    less of an issue than the fact that each system isn’t quite the same. That fact
    often leads to two pipeline maintenance challenges:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到复杂性时，您从中摄取的系统数量通常不是问题的主要因素，而是每个系统并不完全相同。这一事实通常会导致两个管道维护挑战：
- en: Ingestion jobs must be written to handle a variety of source system types (Postgres,
    Kafka, and so on). The more source system types you need to ingest from, the larger
    your codebase and the more to maintain.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄取作业必须编写以处理各种源系统类型（例如Postgres，Kafka等）。您需要从的源系统类型越多，您的代码库就越大，需要维护的东西也就越多。
- en: Ingestion jobs for the same source system type cannot be easily standardized.
    For example, even if you only ingest from REST APIs, if those APIs do not have
    standardized ways of paging, incrementally accessing data, and other features,
    data engineers may build “one-off” ingestion jobs that don’t reuse code and share
    logic that can be centrally maintained.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法轻松地对同一源系统类型的摄取作业进行标准化。例如，即使您只从REST API中摄取数据，如果这些API没有标准化的分页方式、增量访问数据等特性，数据工程师可能会构建“一次性”摄取作业，这些作业不能重用代码并共享可以集中维护的逻辑。
- en: Depending on your organization, you may have little control over the systems
    you ingest from. Perhaps you must ingest from mostly third-party platforms or
    the internal systems are built by an engineering team under a different part of
    the organization hierarchy. Neither is a technical problem, but each should nonetheless
    be taken into account and addressed as part of a data pipeline strategy. Thankfully,
    there are also some technical approaches within your control to mitigate the impact
    on your pipelines.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的组织情况，你可能无法控制你所摄取的系统。也许你必须主要从第三方平台摄取数据，或者内部系统由组织层次结构下不同部门的工程团队构建。这不是技术问题，但每个问题仍然应被考虑并作为数据流水线战略的一部分解决。幸运的是，在你的控制范围内也有一些技术方法来减轻对你的流水线的影响。
- en: First, the nontechnical factors. If the systems you’re ingesting from are built
    internally but are not well standardized, creating awareness of the impact on
    the data organization pipelines can lead to buy-in from system owners.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是非技术因素。如果你正在摄取的系统是内部构建的，但标准化程度不高，那么提高对数据组织流水线影响的认识可能会获得系统所有者的支持。
- en: Especially in larger companies, the software engineers building each system
    may not be aware that they are building systems that are not quite the same as
    their counterparts elsewhere in the organization. Thankfully, software engineers
    typically understand the efficiency and maintainability benefits of standardization.
    Forging a partnership with the engineering organization requires patience and
    the right touch, but it’s an underrated nontechnical skill for data teams.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在较大的公司中，构建每个系统的软件工程师可能并不意识到他们构建的系统与组织内其他部门的系统并不完全相同。幸运的是，软件工程师通常能够理解标准化带来的效率和可维护性优势。与工程组织建立合作伙伴关系需要耐心和恰到好处的方法，但这是数据团队中被低估的非技术技能之一。
- en: If you find yourself needing to ingest from a large number of third-party data
    sources, then your organization is likely choosing to buy versus build in many
    instances. Build/buy decisions are complex, and organizations typically weigh
    many factors when evaluating different vendors and proposals for internally built
    solutions. One factor that’s often either left out or left to later-than-ideal
    in the process is the impact on reporting and analytics. In such cases, data teams
    are left with the challenge of ingesting data from a product that wasn’t well
    designed for the task. Do your best to be part of the evaluation process early,
    and ensure your team has a seat at the table for the final decision. Just like
    creating awareness for internal system standardization, the importance of working
    with vendors to determine analytics needs is something that is often not considered
    unless the data team makes sure their voice is heard.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己需要从大量第三方数据源摄取数据，那么你的组织很可能在许多情况下选择购买而不是自建。建立/购买决策是复杂的，组织通常在评估不同供应商和内部解决方案提案时权衡多种因素。在过程中往往忽略或推迟的一个因素是对报告和分析的影响。在这种情况下，数据团队面临的挑战是从设计不良的产品中摄取数据。尽最大努力早期参与评估过程，并确保你的团队在最终决策中有一席之地。就像提高内部系统标准化意识一样，与供应商合作确定分析需求的重要性通常不被考虑，除非数据团队确保他们的声音被听到。
- en: 'There are also some technical approaches within your control that you can take
    to reduce the complexity of your ingestion jobs:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的控制范围内，还有一些技术方法可以减少你的数据摄取作业的复杂性：
- en: Standardize whatever code you can, and reuse
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化你能够的所有代码，并且重复使用。
- en: This is a general best practice in software engineering, but is at times passed
    over in the creating of data ingestion jobs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是软件工程的一般最佳实践，但有时会在创建数据摄取作业时被忽视。
- en: Strive for config-driven data ingestions
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 力求通过配置驱动的数据摄取
- en: Are you ingesting from a number of Postgres databases and tables? Don’t write
    a different job for each ingestion, but rather a single job that iterates through
    config files (or records in a database table!) that defines the tables and schemas
    you want to ingest.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否需要从多个Postgres数据库和表摄取数据？不要为每个摄取编写不同的作业，而是编写一个单一的作业，通过配置文件（或数据库表中的记录！）迭代定义你想要摄取的表和架构。
- en: Consider your own abstractions
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑你自己的抽象层。
- en: If you can’t get source system owners to build some standardized abstractions
    between their systems and your ingestion, consider doing so yourself or partnering
    with them and taking on the bulk of the development work. For example, if you
    must ingest data from a Postgres or MySQL database, get permission from the source
    team to implement streaming CDC with Debezium (see [Chapter 4](ch04.xhtml#ch04))
    rather than writing yet another ingestion job.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法让源系统所有者在它们的系统和你的数据摄取之间建立一些标准化的抽象层，可以考虑自行完成或与它们合作，并承担大部分开发工作。例如，如果你必须摄取来自Postgres或MySQL数据库的数据，可以征得源团队的许可，实施使用Debezium进行流式CDC（参见[第四章](ch04.xhtml#ch04)），而不是再写一个摄取任务。
- en: Reuse of Data Model Logic
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据模型逻辑的重用
- en: 'Complexity can also arise further down a pipeline and in particular during
    data modeling in the transform phase of a pipeline (see [Chapter 6](ch06.xhtml#ch06)).
    As analysts build more data models, they tend to do one of two things:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性也可能在管道的后续阶段和特别是在管道转换阶段的数据建模过程中出现（参见[第六章](ch06.xhtml#ch06)）。随着分析师构建更多数据模型，他们往往会做以下两件事之一：
- en: Repeat logic in the SQL that builds each model.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建每个模型的SQL中重复逻辑。
- en: Derive models off of each other, creating numerous dependencies between models.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 派生各个模型，创建模型之间的多个依赖关系。
- en: Just as code reuse is ideal in data ingestions (and software engineering in
    general), it’s also ideal in data modeling. It ensures that a single source of
    truth exists and reduces the amount of code that needs to be changed in the case
    of a bug or business logic change. The trade-off is a more complex dependency
    graph in a pipeline.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在数据摄取中（以及一般的软件工程中）代码重用是理想的一样，在数据建模中也是理想的。它确保存在单一的真实来源，并在存在错误或业务逻辑更改的情况下减少需要更改的代码量。这种权衡是管道中更复杂的依赖图。
- en: '[Figure 9-1](#fig_0901) shows a DAG (see [Chapter 7](ch07.xhtml#ch07)) with
    a single data ingestion and four data models that are all built via scripts that
    run in parallel. They can be executed in that fashion because they have no dependencies
    on each other.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-1](#fig_0901)展示了一个DAG（参见[第七章](ch07.xhtml#ch07)），其中只有一个数据摄取和四个通过并行运行脚本构建的数据模型。它们可以这样执行，因为它们彼此之间没有依赖关系。'
- en: '![dppr 0901](Images/dppr_0901.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0901](Images/dppr_0901.png)'
- en: Figure 9-1\. Four independent data models.
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 四个独立数据模型。
- en: If they are truly unrelated data models, that is not a problem. However, if
    they all share some logic, then it’s best to refactor the models and the DAG to
    look something like [Figure 9-2](#fig_0902).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它们确实是不相关的数据模型，那没问题。但是，如果它们都共享一些逻辑，最好重构模型和DAG，使其看起来像[图9-2](#fig_0902)。
- en: '![dppr 0902](Images/dppr_0902.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0902](Images/dppr_0902.png)'
- en: Figure 9-2\. Data models with logic reuse and dependencies.
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. 具有逻辑重用和依赖关系的数据模型。
- en: '[Example 9-2](#ex_0902) shows a simple example of logic reuse that represents
    the script executed in the `build_model_1` task in [Figure 9-2](#fig_0902). The
    script generates an order count by day and stores it in a data model called `orders_by_day`.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例9-2](#ex_0902)展示了逻辑重用的简单示例，表示在[图9-2](#fig_0902)中执行的`build_model_1`任务中执行的脚本。该脚本按日生成订单计数，并将其存储在名为`orders_by_day`的数据模型中。'
- en: 'You can use the `Orders` table from [Chapter 6](ch06.xhtml#ch06), which can
    be re-created and populated with the following SQL:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用从[第六章](ch06.xhtml#ch06)的`Orders`表格，可以使用以下SQL重新创建和填充：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 9-2\. model_1.sql
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-2\. model_1.sql
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Subsequent models in the DAG can refer to this table when they need a daily
    order count rather than recalculating each time. [Example 9-3](#ex_0903) represents
    the script executed in the `build_model_2` task in [Figure 9-2](#fig_0902). Instead
    of recalculating the order count by day, it uses the `orders_by_day` model instead.
    Though getting an order count by day may sound trivial, with more complex calculations
    or queries with additional logic in the `WHERE` clause or joins, it’s even more
    important to write the logic once and reuse. Doing so ensures a single source
    of truth, ensures a single model to maintain, and, as a bonus, only requires your
    data warehouse to run any complex logic a single time and store the results for
    later reference. In some cases, that time savings is notable in a pipeline’s runtime.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 中的后续模型在需要每日订单计数时可以引用此表，而不是每次重新计算。[示例 9-3](#ex_0903) 表示在 [图 9-2](#fig_0902)
    中的 `build_model_2` 任务中执行的脚本。与每天按订单计算订单数量相比，使用 `orders_by_day` 模型更为重要，尤其是在具有额外逻辑的更复杂计算或带有
    `WHERE` 子句或连接的查询中，更应一次编写逻辑并重复使用。这样做可以确保单一真相源，确保仅需维护单个模型，并且作为奖励，仅需要您的数据仓库运行任何复杂逻辑一次并将结果存储以供以后参考。在某些情况下，这种节省时间在管道运行时的显著性是显著的。
- en: Example 9-3\. model_2.sql
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-3\. model_2.sql
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Though some savvy data analysts design their data models and the subsequent
    DAG in this way from the start, it’s more common to find opportunity to refactor
    only after problems arise in a pipeline. For example, if a bug is found in the
    logic of a model and needs to be fixed in multiple models, then there is likely
    an opportunity to apply the logic to a single model and derive other models from
    it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些精明的数据分析师从一开始就设计他们的数据模型和随后的 DAG，但更常见的是在管道中出现问题后才找到重构的机会。例如，如果在模型逻辑中发现了错误并需要修复多个模型，则很可能有机会将逻辑应用于单个模型并从中派生其他模型。
- en: Though the end result is a more complex set of dependencies, if handled properly,
    as you’ll see in the following section, you’ll find the logic in the data modeling
    portion of your pipeline to be more reliable and less likely to result in multiple
    versions of truth.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最终结果是更复杂的依赖关系集合，如果处理得当，正如您将在下一节中看到的那样，您会发现管道中数据建模部分的逻辑更可靠且不太可能导致多个真相版本的出现。
- en: Ensuring Dependency Integrity
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保依赖完整性
- en: 'As noted in the previous section, despite all of the benefits of reusing data
    model logic, there is a trade-off: the need to keep track of what models rely
    on each other and ensure that those dependencies are defined properly in a DAG
    for orchestration.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，尽管重用数据模型逻辑带来了诸多好处，但也存在一个权衡：需要跟踪每个模型依赖的内容，并确保这些依赖在 DAG 中正确定义以进行编排。
- en: In [Figure 9-2](#fig_0902) in the previous section (and queries in Examples
    [9-2](#ex_0902) and [9-3](#ex_0903)), `model_2` is dependent on `model_1`, and
    `model_3` and `model_4` both depend on `model_2`. Those dependencies are defined
    properly in the DAG, but as teams build more models, keeping track of dependencies
    becomes quite a chore and prone to error.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中的 [图 9-2](#fig_0902)（以及示例 [9-2](#ex_0902) 和 [9-3](#ex_0903) 中的查询）中，`model_2`
    依赖于 `model_1`，而 `model_3` 和 `model_4` 都依赖于 `model_2`。这些依赖关系在 DAG 中得到了正确定义，但随着团队构建更多模型，跟踪依赖关系变得相当繁琐且容易出错。
- en: As pipelines get more complex, it’s time to consider programatic approaches
    to defining and validating dependencies between data models. There are a number
    of approaches, of which I’ll discuss two.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随着管道变得越来越复杂，是时候考虑通过程序化方法定义和验证数据模型之间的依赖关系了。有多种方法，我将讨论其中两种。
- en: First, you can build some logic into your development process to identify dependencies
    in SQL scripts and ensure that any tables that a script depends on are executed
    upstream in a DAG. Doing so isn’t simple and can be done either by parsing out
    table names from a SQL script or, more commonly, requiring the data analyst writing
    the model to provide a list of dependencies manually in a config file when they
    submit a new model or modification to an existing one. In both cases, you have
    some work ahead of you and are adding some friction to your development process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您可以在开发过程中构建一些逻辑，以识别 SQL 脚本中的依赖关系，并确保任何脚本依赖的表在 DAG 中的上游被执行。这并不简单，可以通过从 SQL
    脚本中解析表名来完成，或者更常见的是，在数据分析师提交新模型或对现有模型进行修改时，在配置文件中手动提供依赖列表。在这两种情况下，您需要做一些工作，并为开发过程增加一些摩擦。
- en: Another approach is to use a data model development framework like [dbt](https://www.getdbt.com),
    which among other benefits has a mechanism for analysts to define references between
    models right in the SQL they write for the model definition.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用数据模型开发框架，比如[dbt](https://www.getdbt.com)，它除了其他好处外，还具有分析师在模型定义的SQL中定义模型之间引用的机制。
- en: For example, I’ll rewrite `model_2.sql` from [Example 9-3](#ex_0903) and use
    the `ref()` function in dbt to refer to `model_1.sql` in the join. [Example 9-4](#ex_0904)
    shows the result.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我将重写[示例 9-3](#ex_0903)中的`model_2.sql`，并在dbt中使用`ref()`函数来引用连接中的`model_1.sql`。[示例 9-4](#ex_0904)展示了结果。
- en: Example 9-4\. model_2_dbt.sql
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-4\. model_2_dbt.sql
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With the updated SQL, dbt knows that `model_2` relies on `model_1` and ensures
    execution in the proper order. In fact, dbt builds a DAG dynamically rather than
    forcing you to do so in a tool like Airflow. When the data model is compiled by
    dbt prior to executing, the reference to `model_1` is filled in with the table
    name (`orders_by_day`). If all four models from the DAG in [Figure 9-2](#fig_0902)
    are instead written in dbt, they can be compiled and executed with a single command
    on the command line:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更新的SQL，dbt知道`model_2`依赖于`model_1`，并确保按正确顺序执行。事实上，dbt动态构建DAG，而不是强迫您在Airflow等工具中这样做。当数据模型在执行之前由dbt编译时，对`model_1`的引用将用表名（`orders_by_day`）填充。如果[图 9-2](#fig_0902)中的DAG中的所有四个模型改为在dbt中编写，它们可以在命令行上用单个命令编译和执行：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When `dbt run` is executed, the SQL scripts representing each model will run
    in the proper order based on how each table is referenced from each other. As
    you learned in [Chapter 7](ch07.xhtml#ch07), running command-line tasks in Airflow
    is simple. If you’d still like to use Airflow as your orchestrator alongside dbt
    for your data model development, that’s no problem. [Figure 9-3](#fig_0903) shows
    an updated DAG where the two steps in the ingestion are run just like before.
    When they are completed, a single Airflow task executes the `dbt run` command,
    which handles executing the SQL for all four data models in the correct order.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行`dbt run`时，表示每个模型的SQL脚本将根据每个表从彼此引用的方式按正确的顺序运行。正如您在[第7章](ch07.xhtml#ch07)中学到的，通过Airflow运行命令行任务是简单的。如果您仍然希望在数据模型开发中同时使用Airflow作为编排器和dbt，那也没问题。[图 9-3](#fig_0903)显示了一个更新的DAG，其中两个摄取步骤的运行方式与以前相同。当它们完成时，一个单独的Airflow任务执行`dbt
    run`命令，该命令按正确顺序执行所有四个数据模型的SQL。
- en: '![dppr 0903](Images/dppr_0903.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0903](Images/dppr_0903.png)'
- en: Figure 9-3\. Data models executed in dbt from Airflow.
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. 从Airflow中执行的dbt数据模型。
- en: Though in this example I’m running all models in the dbt project, you can specify
    a subset of models to run by passing parameters to `dbt run` as well.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在这个例子中，我运行了dbt项目中的所有模型，但你也可以通过向`dbt run`传递参数来指定要运行的模型子集。
- en: Whether you choose to identify and validate model dependencies with custom code
    you inject into your development process, or leverage a product like dbt, handling
    dependencies at scale is key to maintaining a data pipeline. It’s best not to
    leave it to manual checks and human eyes!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择使用自定义代码识别和验证模型依赖关系，还是利用dbt等产品，在处理大规模依赖关系时都是保持数据管道的关键。最好不要依赖手动检查和人工操作！
