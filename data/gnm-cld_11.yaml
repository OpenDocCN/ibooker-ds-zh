- en: Chapter 10\. Running Single Workflows at Scale with Pipelines API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。使用管道API在规模上运行单一工作流程
- en: 'In [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work), we started
    running workflows for the first time, working on a custom virtual machine in GCP.
    However, that single-machine setup didn’t allow us to take advantage of the biggest
    strength of the cloud: the availability of seemingly endless numbers of machines
    on demand! So in this chapter, we use a service offered by GCP called *Genomics
    Pipelines API* (PAPI), which functions as a sort of job scheduler for GCP Compute
    Engine instances, to do exactly that.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.xhtml#automating_analysis_execution_with_work)中，我们首次开始运行工作流程，使用了GCP中的自定义虚拟机设置。然而，那种单一机器的设置并不能利用云的最大优势：即按需提供看似无限数量的机器！因此，在本章中，我们使用了GCP提供的名为*基因组管道API*（PAPI）的服务，它作为GCP计算引擎实例的作业调度程序，正好能做到这一点。
- en: First, we try simply changing the Cromwell configuration on our VM to submit
    job execution to PAPI instead of the local machine. Then, we try out a tool called
    `WDL_Runner` that wraps Cromwell and manages submissions to PAPI, which makes
    it easier to “launch and forget” WDL executions. Both of these options, which
    we explore in the first half of this chapter, will open the door for us to run
    full-scale GATK pipelines that we could not have run on our single-VM setup in
    [Chapter 9](ch09.xhtml#deciphering_real_genomics_workflows). Along the way, we
    also discuss important considerations such as runtime, cost, portability, and
    overall efficiency of running workflows in the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们尝试简单地更改我们虚拟机上的Cromwell配置，以将作业执行提交到PAPI而不是本地机器。接着，我们尝试一个名为`WDL_Runner`的工具，它包装了Cromwell并管理向PAPI的提交，这使得“启动并忘记”WDL执行变得更加容易。这两个选项我们在本章的前半部分进行探讨，将为我们打开门户，使我们能够在[第9章](ch09.xhtml#deciphering_real_genomics_workflows)中的单一VM设置上无法运行的全面GATK流水线。在此过程中，我们还讨论了诸如运行时、成本、可移植性以及在云中运行工作流的整体效率等重要考虑因素。
- en: Introducing the GCP Genomics Pipelines API Service
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍GCP基因组管道API服务
- en: The Genomics Pipelines API is a service operated by GCP that makes it easy to
    dispatch jobs for execution on the GCP Compute Engine without having to actually
    manage VMs directly. Despite its name, the Genomics Pipelines API is not at all
    specific to genomics, so it can be used for a lot of different workloads and use
    cases. In general, we simply refer to it as the Pipelines API, or PAPI. It is
    possible to use PAPI to execute specific analysis commands directly as described
    in the [Google Cloud documentation](https://oreil.ly/l3OBw), but in this chapter
    our goal is to use PAPI in the context of workflow execution with Cromwell, which
    is illustrated in [Figure 10-1](#overview_of_cromwell_plus_papi_operatio).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基因组管道API是由GCP运营的服务，使得可以轻松地将作业调度到GCP计算引擎上，而无需直接管理VM。尽管其名称中带有“基因组”二字，但基因组管道API并不限于基因组研究，因此可以用于许多不同的工作负载和用例。通常，我们简称为管道API或PAPI。可以按照[Google
    Cloud文档](https://oreil.ly/l3OBw)中描述的方式使用PAPI直接执行特定的分析命令，但在本章中，我们的目标是在Cromwell的工作流执行上使用PAPI，如[图10-1](#overview_of_cromwell_plus_papi_operatio)所示。
- en: '![Overview of Cromwell + PAPI operation.](Images/gitc_1001.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Cromwell + PAPI操作概述。](Images/gitc_1001.png)'
- en: Figure 10-1\. Overview of Cromwell + PAPI operation.
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。Cromwell + PAPI操作概述。
- en: Just as in [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work),
    we provide a WDL that describes our workflow to the Cromwell engine, which then
    interprets the workflow and generates the individual jobs that need to be executed.
    What’s new is that instead of having Cromwell hand the jobs over to the local
    executor of the machine it is itself running on, we’re going to point it to PAPI,
    as shown in [Figure 10-1](#overview_of_cromwell_plus_papi_operatio).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第8章](ch08.xhtml#automating_analysis_execution_with_work)中一样，我们提供了一个WDL描述我们的工作流程给Cromwell引擎，然后Cromwell解释工作流并生成需要执行的各个作业。新的是，我们不再让Cromwell将作业交给运行在其自身所在机器上的本地执行器，而是要将其指向PAPI，如[图10-1](#overview_of_cromwell_plus_papi_operatio)所示。
- en: For every job that Cromwell sends to PAPI, the service will create a VM on Google
    Compute Engine with the specified runtime attributes (CPU, RAM, and storage),
    set up the Docker container specified in the WDL, copy input files to the VM’s
    local disk, run the command(s), copy any outputs and logs to their final location
    (typically a GCS bucket), and finally, delete the VM and free up any associated
    compute resources. This makes it phenomenally easy to rapidly marshal a fleet
    of custom VMs, execute workflow tasks, and then walk away with your results without
    having to worry about managing compute resources, because that’s all handled for
    you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Cromwell 发送到 PAPI 的每个作业，该服务将在 Google Compute Engine 上创建一个具有指定运行时属性（CPU、RAM
    和存储）的虚拟机，设置在 WDL 中指定的 Docker 容器，将输入文件复制到虚拟机的本地磁盘，运行命令，将任何输出和日志复制到最终位置（通常是 GCS
    存储桶），最后删除虚拟机并释放任何相关的计算资源。这使得快速调度一批自定义虚拟机、执行工作流任务，并在完成后获取结果变得非常简便，而无需担心管理计算资源，因为这一切都已为您处理。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As we wrap up this book, Google Cloud is rolling out an updated version of this
    service under a new name, “Life Sciences API.” Once we’ve had a chance to try
    it out, we’ll write a blog post about the new service and how to adapt the book
    exercises to use it. In the meantime, we expect that the Genomics Pipelines API
    will remain functional for the foreseeable future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成本书时，Google Cloud 将推出一个更新版本的服务，以新名称“Life Sciences API”发布。一旦我们有机会尝试它，我们将撰写关于新服务以及如何调整书中练习以使用它的博客文章。与此同时，我们预计基因组管道
    API 将在可预见的未来继续可用。
- en: Enabling Genomics API and Related APIs in Your Google Cloud Project
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的 Google Cloud 项目中启用基因组 API 和相关 API
- en: 'To run the exercises in this chapter, you need to have three APIs enabled in
    your Google Cloud Project: [Genomics API](https://oreil.ly/YYEiu), [Cloud Storage
    JSON API](https://oreil.ly/K9jxa), and [Compute Engine API](https://oreil.ly/Byzox).
    You can use the direct links that we’ve included, or you can go to the [APIs &
    Services](https://oreil.ly/tDGTi) section of the Google Cloud console and click
    the “+ Enable APIs and Services” button. Clicking it will take you to the [API
    Library](https://oreil.ly/73Fg6), where you can search for each API by name. If
    you find yourself confused by APIs that have similar names in the search results,
    simply check the logos and descriptions shown in [Figure 10-2](#logos_and_descriptions_for_the_three_re).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章的练习，您需要在您的 Google Cloud 项目中启用三个 API：[基因组 API](https://oreil.ly/YYEiu)，[云存储
    JSON API](https://oreil.ly/K9jxa) 和 [计算引擎 API](https://oreil.ly/Byzox)。您可以使用我们提供的直接链接，或者您可以转到
    Google Cloud 控制台的 [APIs & Services](https://oreil.ly/tDGTi) 部分，并单击“+ 启用 API 和服务”按钮。单击该按钮将带您到
    [API Library](https://oreil.ly/73Fg6)，在那里您可以通过名称搜索每个 API。如果您在搜索结果中看到具有类似名称的 API
    时感到困惑，只需检查图 10-2 中显示的标志和描述即可。
- en: '![Logos and descriptions for the three required APIs: Genomics API, Cloud Storage
    JSON API, and Compute Engine API.](Images/gitc_1002.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![三个必需 API 的标志和描述：基因组 API、云存储 JSON API 和计算引擎 API。](Images/gitc_1002.png)'
- en: 'Figure 10-2\. Logos and descriptions for the three required APIs: Genomics
    API, Cloud Storage JSON API, and Compute Engine API.'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 三个必需 API 的标志和描述：基因组 API、云存储 JSON API 和计算引擎 API。
- en: On each API’s page, you will see a blue button that reads either Enable or Manage.
    The latter means that the API is already enabled in your project and you don’t
    need to do anything. If you see the Enable button, click it to enable the API.
    You might see a message that says “To use this API, you may need credentials.
    Click ‘Create credentials’ to get started.” but you can ignore it—GCP and Cromwell
    will handle authentication without requiring you to supply additional credentials.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个 API 的页面上，您会看到一个蓝色按钮，上面写着“Enable”或“Manage”。后者表示该 API 已在您的项目中启用，您无需采取任何措施。如果看到“Enable”按钮，请单击以启用
    API。您可能会看到一条消息，指出“要使用此 API，您可能需要凭据。单击‘创建凭据’以开始。”但您可以忽略它——GCP 和 Cromwell 将处理身份验证，而无需您提供额外的凭据。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To use the Compute Engine API, you must have a means of payment set up in the
    Billing section even if you’re using the free credits. If you followed the instructions
    in [Chapter 4](ch04.xhtml#first_steps_in_the_cloud), you should be all set; but
    if you skipped straight to here, you’ll need to go back and follow the billing
    setup in that chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用计算引擎 API，即使使用免费信用额度，您也必须在计费部分设置支付方式。如果您按照 [第四章](ch04.xhtml#first_steps_in_the_cloud)
    中的说明操作，您应该已经准备好了；但如果您直接跳到这里，请返回并按照该章节中的计费设置操作。
- en: Directly Dispatching Cromwell Jobs to PAPI
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接将 Cromwell 作业发送到 PAPI
- en: In [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work), we were
    running Cromwell without a configuration file, so by default it sent all jobs
    to the machine’s local executor. Now we’re going to provide a configuration file
    that points Cromwell to PAPI for execution.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.xhtml#automating_analysis_execution_with_work)中，我们在没有配置文件的情况下运行了Cromwell，因此默认将所有作业发送到机器的本地执行器。现在，我们将提供一个配置文件，使Cromwell指向PAPI进行执行。
- en: 'Fun fact: you can configure Cromwell to run jobs through PAPI from anywhere,
    whether you’re running Cromwell on your laptop, on a local server, or even on
    a VM on a different cloud platform. Here, we show you how to do it from your VM
    using GCP given that you already have it set up appropriately (you have Cromwell
    installed and you’re authenticated through GCP), but the basic procedure and requirements
    would be the same anywhere else.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实是，您可以配置Cromwell以从任何地方运行通过PAPI提交的作业，无论您是在笔记本电脑、本地服务器还是不同云平台的虚拟机上运行Cromwell。在这里，我们展示了如何在您的VM上使用GCP执行此操作，前提是您已经适当地设置了（已安装Cromwell并通过GCP进行了身份验证），但基本的操作步骤和要求在其他任何地方也是相同的。
- en: Configuring Cromwell to Communicate with PAPI
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Cromwell以与PAPI通信
- en: 'The Cromwell documentation has a [section on using GCP](https://oreil.ly/MnckS)
    that includes a template configuration file for running on PAPI, which we’ve copied
    to the book bundle for your convenience. You can find it at *~/book/code/config/google.conf*
    on your VM if you followed the instructions provided in [Chapter 4](ch04.xhtml#first_steps_in_the_cloud).
    If you open up the *google.conf* file in a text editor, this is what it looks
    like:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Cromwell文档中有一个关于使用GCP的[部分](https://oreil.ly/MnckS)，其中包括一个用于在PAPI上运行的模板配置文件，我们已将其复制到书籍捆绑包中供您方便使用。如果您按照[第4章](ch04.xhtml#first_steps_in_the_cloud)中提供的说明操作，您可以在VM上的*~/book/code/config/google.conf*找到它。如果您在文本编辑器中打开*google.conf*文件，它看起来像这样：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We know that’s a lot to take in if you’re not used to dealing with this sort
    of thing; the good news is that you can ignore almost all of it, and we’re going
    to walk you through the bits that you do need to care about. As a heads-up before
    we begin, be aware that you’re going to need to edit the file and save the modified
    version as *my-google.conf* in your sandbox directory for this chapter, which
    should be *~/sandbox-10*. Go ahead and create that directory now. We recommend
    using the same procedure as you used in earlier chapters to edit text files:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如果您不习惯处理这种事情，这些信息可能有点多；好消息是，您几乎可以忽略其中的所有内容，我们将会引导您了解您确实需要关心的部分。在我们开始之前，请注意，您需要编辑并将修改后的文件保存为*my-google.conf*，存放在本章的沙盒目录*~/sandbox-10*中。现在就去创建那个目录吧。我们建议您使用与早期章节编辑文本文件相同的步骤：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now set environment variables to point to your sandbox and related directories:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在设置环境变量以指向您的沙盒和相关目录：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In [Chapter 4](ch04.xhtml#first_steps_in_the_cloud), we also defined a `BUCKET`
    environment variable. If you are working in a new terminal, make sure that you
    also redefine this variable because we use it a little later in this chapter.
    Replace `*my-bucket*` in the following command with the value you used in [Chapter 4](ch04.xhtml#first_steps_in_the_cloud):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#first_steps_in_the_cloud)中，我们还定义了一个`BUCKET`环境变量。如果您在新终端中工作，请确保重新定义此变量，因为我们稍后在本章中会用到它。在下面的命令中，用您在[第4章](ch04.xhtml#first_steps_in_the_cloud)中使用的值替换`*my-bucket*`：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that our environment is set up, we can get to work. First, let’s identify
    what makes this configuration file point to GCP and PAPI as opposed to another
    backend. Go ahead and open your copy of the configuration file now:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的环境已经设置好，我们可以开始工作了。首先，让我们确定这个配置文件之所以指向GCP和PAPI而不是其他后端的原因。现在打开您的配置文件副本吧：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see there are many references to Google throughout the file, but the
    key setting is happening here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到文件中有许多关于Google的引用，但关键设置发生在这里：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `PAPIv2` refers to the current name and version of the PAPI.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`PAPIv2`指的是当前PAPI的名称和版本。
- en: 'Scrolling deeper into that section of the file, you’ll find two settings, the
    project ID and the output bucket, which are very important because you must modify
    them in order to get this configuration file to work for you:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件的这一部分滚动深入，您会找到两个设置，项目ID和输出存储桶，这两个设置非常重要，因为您必须修改它们才能使此配置文件为您工作：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You need to replace `*google-project-id*` to specify the Google Project you’re
    working with, and replace `*google-bucket-name*` to specify the location in GCS
    that you want Cromwell to use for storing execution logs and outputs. For example,
    for the test project we’ve been using so far, it looks like this when it’s filled
    in:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要替换`*google-project-id*`以指定您正在使用的 Google 项目，并替换`*google-bucket-name*`以指定 Cromwell
    用于存储执行日志和输出的 GCS 位置。例如，对于我们目前使用的测试项目，填写如下所示：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, you also need to provide the project ID to charge when accessing data
    in GCS buckets that are set to the `requester-pays` mode, as discussed in [Chapter 4](ch04.xhtml#first_steps_in_the_cloud).
    It appears in two places in this configuration file: once toward the beginning
    and once toward the end:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在访问设置为`requester-pays`模式的GCS存储桶中的数据时，您还需要提供项目 ID，如[第四章](ch04.xhtml#first_steps_in_the_cloud)中讨论的那样。此配置文件中有两处出现：一处在文件开头，一处在文件末尾：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Specifically, you need to replace `*google-billing-project-id*` to specify
    the billing project to use for that purpose. This setting allows you to use a
    different billing project from the one used for compute, for example if you’re
    using separate billing accounts to cover different kinds of cost. Here, we just
    use the same project as we just defined earlier:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，您需要替换`*google-billing-project-id*`以指定用于此目的的计费项目。此设置允许您使用与计算不同的计费项目，例如，如果您使用不同的计费账户来支付不同类型的费用。在这里，我们只是使用与前面定义的相同项目：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Make sure to edit both occurrences of this setting in the file.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保在文件中编辑此设置的两处出现。
- en: When you’re done editing the file, be sure to save it as *my-google.conf* in
    your sandbox directory for this chapter, *~/sandbox-10*. You can give it a different
    name and/or put it in a different location if you prefer, but then you’ll need
    to edit the name and path accordingly in the command line in the next section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑完文件后，请务必将其保存为本章节的沙盒目录中的*my-google.conf*，即*~/sandbox-10*。如果您愿意，可以为其指定不同的名称和/或放置在不同的位置，但然后您需要相应地在下一部分的命令行中编辑名称和路径。
- en: Running Scattered HaplotypeCaller via PAPI
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 PAPI 运行散点型杂合体分析器（Scattered HaplotypeCaller）
- en: 'You have one more step to do before you can actually launch the Cromwell `run`
    command to test this configuration: you need to generate a file of credentials
    that Cromwell will give to PAPI to use for authentication. To do so, run the following
    `gcloud` command:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可以实际运行 Cromwell `run` 命令以测试此配置之前，您还需要完成一步操作：生成一个凭据文件，Cromwell 将提供给 PAPI 用于身份验证。要执行此操作，请运行以下`gcloud`命令：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It’s similar to the `gcloud init` command you ran in [Chapter 4](ch04.xhtml#first_steps_in_the_cloud)
    to set up your VM, up to and including the little lecture about security protocols.
    Follow the prompts to log in via your browser and copy the access code to finish
    the process. The credentials file, which the system refers to as Application Default
    Credentials (ADC), will be created in a standard location that the `gcloud` utilities
    can access. You don’t need to do anything more for it to work.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于您在[第四章](ch04.xhtml#first_steps_in_the_cloud)中运行的`gcloud init`命令，用于设置您的 VM，直至包括关于安全协议的简短讲座。按照提示通过浏览器登录并复制访问代码以完成该过程。凭据文件，系统称之为应用程序默认凭据（ADC），将在`gcloud`实用程序可以访问的标准位置中创建。您无需采取任何其他措施使其正常工作。
- en: 'We provide a JSON file of inputs that is prefilled with the paths to test files
    (see *$WF/scatter-hc/scatter-haplotypecaller.gcs.inputs.json*). You can check
    what it contains; you’ll see the files are the same as you’ve been using locally,
    but this time we’re pointing Cromwell to their locations in GCS (in the book bucket):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个输入的 JSON 文件，其中填充了测试文件的路径（参见*$WF/scatter-hc/scatter-haplotypecaller.gcs.inputs.json*）。您可以检查其内容；您会看到这些文件与您本地使用的文件相同，但这次我们将它们的位置指向了
    GCS 中的 Cromwell（在书籍存储桶中）：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With that in hand, it’s time to run the following Cromwell command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个文件，现在是时候运行以下 Cromwell 命令了：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This command calls the same workflow that we last ran in [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work),
    *scatter-haplotypecaller.wdl*, but this time we’re adding the `-Dconfig.file`
    argument to specify the configuration file, which will cause Cromwell to dispatch
    the work to PAPI instead of the local machine. PAPI in turn will orchestrate the
    creation of new VMs on Compute Engine to execute the work. It will also take care
    of pulling in any input files and saving the logs and any outputs to the storage
    bucket you specified in your configuration file. Finally, it will delete each
    VM after its work is done.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令调用了我们在 [第 8 章](ch08.xhtml#automating_analysis_execution_with_work) 中最后运行的相同工作流，*scatter-haplotypecaller.wdl*，但这次我们添加了
    `-Dconfig.file` 参数来指定配置文件，这会导致 Cromwell 将工作分发给 PAPI 而不是本地机器。PAPI 然后会编排在 Compute
    Engine 上创建新的 VM 来执行工作。它还会负责拉取任何输入文件，并将日志和任何输出保存到你在配置文件中指定的存储桶中。最后，它会在每个 VM 完成工作后将其删除。
- en: Monitoring Workflow Execution on Google Compute Engine
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Google Compute Engine 上监视工作流执行
- en: This all sounds great, but how can you know what is actually happening when
    you run that command? As usual Cromwell will output a lot of information to the
    terminal that is rather difficult to parse, so let’s walk through a few approaches
    that you can take to identify what is running where.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很棒，但是当你运行那个命令时，你怎么知道实际上发生了什么？像往常一样，Cromwell 会向终端输出大量信息，这些信息相当难以解析，所以让我们一起走过几种方法，来识别正在运行的内容。
- en: 'First, you should see in the early part of the log output that Cromwell has
    correctly identified that it needs to use PAPI. It’s not super obvious, but you’ll
    see a few lines like these that mention PAPI:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在日志输出的早期部分，你应该能看到 Cromwell 正确识别需要使用 PAPI。这并不是非常明显，但你会看到几行类似提到 PAPI 的内容：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Scrolling a bit farther down, you’ll find a line that references `Call-to-Backend
    assignments`, which lists the task calls from the workflow:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 往下滚动一点，你会找到一行引用 `Call-to-Backend assignments` 的内容，列出了工作流程中的任务调用：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `-> PAPIv2` bit means that each of these tasks was dispatched to PAPI (version
    2) for execution. After that, past the usual detailed listing of the commands
    that Cromwell generated from the WDL and *inputs* JSON file, you’ll see many references
    to `PipelinesApiAsyncBackendJobExecutionActor`, which is another component of
    the PAPI system that is handling your workflow execution.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`-> PAPIv2` 部分表示每个任务都已被分发到 PAPI（版本 2）以进行执行。在此之后，超出通常从 WDL 和 *inputs* JSON 文件生成的命令详细列表，你会看到许多与
    `PipelinesApiAsyncBackendJobExecutionActor` 相关的引用，这是 PAPI 系统中处理你的工作流执行的另一个组件。'
- en: Of course, that tells you only that Cromwell is dispatching work to PAPI, and
    that PAPI is doing something in response, but how do you know what it’s doing?
    The most straightforward way is to go to the [Compute Engine console](https://oreil.ly/-mFEu),
    which lists VM instances running under your project. At the very least, you should
    see the VM that you have been using to work through the book exercises, whose
    name you should recognize. And if you catch them while they’re running, there
    can be up to four VM instances with obviously machine-generated names (*google-pipelines-worker-xxxxx*…),
    as shown in [Figure 10-4](#list_of_active_vm_instances). Those are the VMs created
    by PAPI in response to your Cromwell command.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只告诉你 Cromwell 正在将工作分发给 PAPI，并且 PAPI 在响应中做了一些事情，但你怎么知道它在做什么？最直接的方法是转到 [Compute
    Engine 控制台](https://oreil.ly/-mFEu)，它列出了在你的项目下运行的 VM 实例。至少，你应该看到正在使用来完成书中练习的 VM，你应该能够识别其名称。如果你在它们运行时抓住它们，可能会看到最多四个显然是机器生成名称的
    VM 实例（*google-pipelines-worker-xxxxx*…），如 [Figure 10-4](#list_of_active_vm_instances)
    所示。这些是 PAPI 根据你的 Cromwell 命令创建的 VM。
- en: '![List of active VM instances](Images/gitc_1004.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![活动 VM 实例列表](Images/gitc_1004.png)'
- en: Figure 10-4\. List of active VM instances
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 活动 VM 实例列表
- en: Why has PAPI created multiple VMs, you might ask? As a reminder, this workflow
    divides the work of calling variants into four separate jobs that will run on
    different genomic intervals. When Cromwell launched the first four jobs in parallel,
    PAPI created a separate VM for each of them. The only job Cromwell will have queued
    up for later is the merging task that collects the outputs of the four parallel
    jobs into a single task. That’s because the merging task must wait for all the
    others to be complete in order for its input dependencies to be satisfied. When
    that is the case, PAPI will once more do the honors to get the work done and corral
    the final output to your bucket.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么PAPI创建了多个虚拟机？作为提醒，这个工作流将调用变体分为四个单独的作业，这些作业将在不同的基因组间隔上运行。当Cromwell并行启动了这四个作业时，PAPI为每个作业创建了一个单独的虚拟机。唯一一个稍后排队的工作是将四个并行作业的输出收集到一个单一任务中的合并任务。这是因为合并任务必须等待所有其他任务完成，以满足其输入依赖关系。在这种情况下，PAPI将再次尽责完成工作并将最终输出整理到您的存储桶中。
- en: Not to put too fine a point on it, but that right there is one of the huge advantages
    of using the cloud for this kind of analysis. As long as you can chop the work
    into independent segments, you can parallelize the execution to a much larger
    degree than you can typically afford to do on a local server or cluster with a
    limited number of nodes available at any given time. Even though the cloud is
    not *actually* infinite (sorry, the Tooth Fairy is also not real), it does have
    a rather impressive capacity for accommodating ridiculously large numbers of job
    submissions. The Broad Institute, for example, regularly runs workflows on Cromwell
    across thousands of nodes simultaneously.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不想过分细说，但正是云计算在这种分析中的巨大优势之一。只要你能将工作分割成独立的片段，你就可以比在本地服务器或集群上通常能承受的更大程度地并行执行。即使云计算*实际上*并非无限（抱歉，牙仙也不存在），但它确实具有非常令人印象深刻的能力，可以容纳大量的作业提交。例如，Broad
    Institute经常在Cromwell上同时跨数千个节点运行工作流程。
- en: Note
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The zone of the PAPI-generated VMs may be different from the zone you set for
    the VM you created yourself, as it was in our case here. That’s because we didn’t
    explicitly specify a zone in the configuration file, so PAPI used a default value.
    There are several ways to control the zone where the work will be done, including
    at the workflow level or even at the task level within a workflow, as [described
    in the Cromwell documentation](https://oreil.ly/2YElr).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PAPI生成的虚拟机的区域可能与您自己创建的VM的区域不同，就像我们在这里的情况一样。这是因为我们没有在配置文件中明确指定区域，因此PAPI使用了默认值。有几种方法可以控制工作将在哪个区域完成，包括在工作流程级别或甚至在工作流程内的任务级别上，正如[Cromwell文档中描述的那样](https://oreil.ly/2YElr)。
- en: You can also see an overview of Compute Engine activity in the [Home Dashboard](https://oreil.ly/ChMKK)
    of your project, as shown in [Figure 10-5](#overview_of_compute_engine_activity).
    The advantage of the dashboard, even though it gives you only an aggregate view,
    is that it allows you to see past activity, not just whatever is running at the
    moment you’re looking.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[项目的主页仪表板](https://oreil.ly/ChMKK)中查看计算引擎活动的概述，如[图 10-5](#overview_of_compute_engine_activity)所示。尽管该仪表板只提供了汇总视图，但它允许您查看过去的活动，而不仅仅是当前正在运行的内容。
- en: '![Overview of Compute Engine activity](Images/gitc_1005.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![计算引擎活动概述](Images/gitc_1005.png)'
- en: Figure 10-5\. Overview of Compute Engine activity.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 计算引擎活动概述。
- en: 'Finally, you can check the execution directory in the bucket that you specified
    in the config. You can do so either by navigating to the bucket in the [Cloud
    Storage console](https://oreil.ly/1iQmv), or by using `gsutil`, as we’ve previously
    described. Take a few minutes to explore that directory and see how the outputs
    are structured. It should look the same as in [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work)
    when you ran this same workflow on the VM’s local system. However, keep in mind
    that the contents of the execution directory in the bucket don’t represent what
    is happening in real time on the VMs. A synchronization process updates the execution
    logs and copies any outputs as they become available, operating at intervals that
    start out very short and gradually increase to avoid overburdening the system
    in the case of long-running jobs. The maximum length of these intervals is customizable;
    you can see the relevant code in the configuration file we used earlier:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以检查配置中指定的存储桶中的执行目录。您可以通过导航到[Cloud Storage 控制台](https://oreil.ly/1iQmv)中的存储桶，或者使用`gsutil`，如我们之前描述的那样来执行此操作。花几分钟时间探索该目录，并查看输出的结构。当您在
    VM 的本地系统上运行相同的工作流程时，它应该与[第 8 章](ch08.xhtml#automating_analysis_execution_with_work)中的情况相同。但是，请记住，存储桶中执行目录的内容并不代表
    VM 实时发生的情况。同步过程会更新执行日志，并在可用时复制任何输出，该过程的间隔开始非常短，然后逐渐增加，以避免长时间运行的作业过度负担系统。这些间隔的最大长度是可定制的；您可以在我们之前使用的配置文件中查看相关代码：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The interval time is measured in seconds, so you can see that, by default, the
    maximum time between updates of the bucket is 10 minutes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 时间间隔以秒计，因此您可以看到，默认情况下，存储桶更新之间的最大时间为 10 分钟。
- en: 'The workflow can take up to 10 minutes to complete. At that point, you’ll see
    the usual `finished with status ''Succeeded''` message, which is followed by the
    list of final outputs in JSON format:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程可能需要最多 10 分钟才能完成。在那时，您将看到通常的`finished with status 'Succeeded'`消息，随后是以 JSON
    格式列出的最终输出列表：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You should see that this time, the final output is located in the bucket that
    you specified in the configuration file. The path to the output has the same structure
    as we described for local execution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到，这次最终输出位于配置文件中指定的存储桶中。输出的路径结构与我们为本地执行描述的结构相同。
- en: 'Now, take a moment to think about what you just achieved: using a mostly prebaked
    configuration file and just one command line, you kicked off a process that involved
    marshaling sophisticated computational resources to parallelize the execution
    of a real (if simple) genomics workflow. You could use that exact same process
    to run a full-scale workflow, like the whole genome analysis pipeline we dissected
    in [Chapter 9](ch09.xhtml#deciphering_real_genomics_workflows), or you could hold
    on to your hat while we roll out a few alternative options. Specifically, later
    in this chapter we show you two approaches that involve “wrapping” Cromwell in
    additional layers of tooling that increase both the ease of use and scalability
    of this system.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请花点时间思考一下您刚刚取得的成就：使用大部分预配置的配置文件和仅一个命令行，您启动了一个过程，涉及整理复杂的计算资源以并行执行真实（尽管简单）的基因组工作流程。您可以使用完全相同的过程来运行全面的工作流程，如我们在[第
    9 章](ch09.xhtml#deciphering_real_genomics_workflows)中解析的整个基因组分析流水线，或者您可以在我们展示几种替代选项时保持耐心。具体来说，在本章后面，我们向您展示了两种方法，这些方法涉及将
    Cromwell 包装在附加层中，从而增加此系统的易用性和可扩展性。
- en: However, before we plunge into the verdant ecosystem of Cromwell-enabling add-ons,
    we’re going to take a bit of a side track to discuss the trade-offs and opportunities
    involved in running workflows on the cloud.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在我们跳入 Cromwell 支持的附加组件的葱郁生态系统之前，我们将稍微偏离主题，讨论在云上运行工作流程涉及的权衡和机会。
- en: Understanding and Optimizing Workflow Efficiency
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和优化工作流效率
- en: 'Did you pay attention to how much time it took to run the *scatter-haplotypecaller.wdl*
    workflow through PAPI? About 10 minutes, right? Do you remember how long it took
    to run just on your VM in [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work)?
    More like two minutes? So let’s get this straight: running the same workflow with
    parallelization on multiple machines took five times longer than just running
    the same jobs on a single machine. That sounds…kind of terrible?'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到通过PAPI运行*scatter-haplotypecaller.wdl*工作流需要多长时间？大约10分钟，对吧？还记得在[第8章](ch08.xhtml#automating_analysis_execution_with_work)中在您的VM上运行只需要多长时间吗？大约两分钟？所以让我们明确一下：在多台机器上并行运行相同的工作流比在单台机器上运行相同的作业要慢五倍。这听起来……有点可怕？
- en: 'The good news is that it’s mostly an artifact of the very small scale of the
    jobs we’re running. We’ve been using a set of intervals that cover only a tiny
    region of the genome, and `HaplotypeCaller` itself takes a trivially small amount
    of time to run on such short intervals. When you run the workflow locally on your
    VM, there’s not actually much work to do: the GATK container image and the files
    are already there, so all that really needs to happen is for Cromwell to read
    the WDL and launch the GATK commands, which, as we’ve mentioned, run quickly.
    In contrast, when you tell Cromwell to dispatch work to PAPI, you set into motion
    this massive machinery that involves creating VMs, retrieving container images,
    localizing files from GCS, and so on. That’s all overhead that shows up in the
    form of a longer runtime. So for short tasks, the actual amount of overall runtime
    spent on the “real work” is dwarfed by the time spent on behind-the-scenes setup.
    However, that setup time is roughly constant, so for longer-running tasks (e.g.,
    if you ran this workflow on much larger genomic intervals), the setup time ends
    up being just a drop in the Google bucket.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，这主要是我们运行的作业规模非常小的副产品。我们使用的一组间隔只覆盖基因组的一个小区域，而`HaplotypeCaller`本身在这样短的间隔上运行的时间非常短。当您在本地的VM上运行工作流时，实际上没有太多工作要做：GATK容器映像和文件已经存在，所以实际上需要发生的只是Cromwell读取WDL并启动GATK命令，正如我们之前提到的，这些命令运行得很快。相比之下，当您告诉Cromwell将工作分派给PAPI时，您启动了涉及创建VM、检索容器映像、从GCS本地化文件等庞大机制。所有这些都是作为更长运行时间形式出现的开销。因此，对于短任务来说，“真正工作”的总运行时间实际上被在幕后设置的时间所抹去。然而，这种设置时间大致是恒定的，因此对于运行时间更长的任务（例如，如果您在更大的基因组间隔上运行此工作流），设置时间最终只是谷歌云中的一滴水。
- en: With that example in mind, let’s take a stroll through some of the considerations
    you’ll need to keep in mind, whether you’re planning to develop your own workflows
    or simply using someone else’s on the cloud.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个例子，让我们深入了解一些您需要牢记的考虑因素，无论您是计划开发自己的工作流还是仅仅在云上使用别人的工作流。
- en: Granularity of Operations
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作的粒度
- en: 'The `HaplotypeCaller` workflow that we ran here is intended to run on much
    longer intervals that cover huge amounts of data, so it makes perfect sense to
    execute it through PAPI when processing full-scale datasets. But what if part
    of your workflow involves short-running operations like simple file format conversion
    and indexing? Well, that might be an opportunity to combine several operations
    into a single task in the workflow. We already saw an example of this in the second
    workflow we examined in [Chapter 9](ch09.xhtml#deciphering_real_genomics_workflows),
    in the `CramToBamTask` `command` block:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里运行的`HaplotypeCaller`工作流程旨在运行时间长得多的间隔，覆盖大量数据，因此通过PAPI执行它在处理大规模数据集时是完全合理的。但是，如果您的工作流的一部分涉及简单的文件格式转换和索引操作呢？那么，将几个操作合并为工作流中的单个任务可能是一个机会。我们已经在我们第二个工作流中看到了一个例子，在[第9章](ch09.xhtml#deciphering_real_genomics_workflows)中我们检查了`CramToBamTask`
    `command`块中的一个例子：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The workflow authors could have separated subsets of this `command` block into
    separate WDL tasks to maximize modularity. For example, having a standalone BAM
    indexing task might have been useful and reusable elsewhere. However, they correctly
    identified that doing so would increase the overhead when run through PAPI, so
    they traded off some potential modularity in favor of higher efficiency.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流的作者本可以将这个`command`块的子集分离成单独的WDL任务，以最大化模块化。例如，拥有独立的BAM索引任务可能会在其他地方发挥作用且可重用。但是，他们正确地认识到，这样做会增加通过PAPI运行时的开销，因此他们在模块化和效率之间做了一些权衡。
- en: Balance of Time Versus Money
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间与金钱的平衡
- en: It’s important to understand that these kinds of trade-offs will make a difference
    not just in the amount of time it takes to run your pipeline, but also its cost.
    This is probably not something you’re used to thinking about if you’ve worked
    mostly on local systems that have computational resources already paid for and
    your major limitation is how much quota you have been allotted. On the cloud,
    however, it’s almost entirely pay-as-you-go, so it’s worth thinking things through
    if you’re on a budget.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些折衷方案的重要性很关键，不仅影响到运行管道所需的时间，还会影响到其成本。如果你大部分时间都在本地系统上工作，那些计算资源已经是预付费的，并且你主要受到分配的配额限制，可能不太习惯考虑这些问题。然而，在云上，几乎完全是按需付费，所以如果你预算有限，考虑问题是非常值得的。
- en: 'For example, here’s another trade-off to think about when moving workflows
    to the cloud: how widely should you parallelize the execution of your variant-calling
    workflow? You could simply parallelize it by chromosome, but those still create
    huge amounts of data, especially for whole genome samples, so it will take a very
    long time to process each, and you won’t really be taking advantage of the amazing
    capacity for parallelism of the cloud. In addition, in humans at least, the various
    chromosomes have enormously different sizes; for example, chromosome 1 is about
    five times longer than chromosome 22, so the latter will finish much faster, and
    its results will sit around waiting for the rest of the chromosomes to be done.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当将工作流程移至云端时，还有一个需要考虑的折衷方案：在执行变异调用工作流程时应该如何广泛地并行化？你可以简单地按染色体并行化，但对于整个基因组样本来说，这仍然会产生大量数据，因此每个处理过程需要很长时间，而且你无法充分利用云端并行处理的惊人能力。此外，至少在人类中，各染色体的长度差异巨大；例如，染色体1的长度大约是染色体22的五倍，所以后者会更快完成，并且其结果将在等待其他染色体处理完毕时保持不变。
- en: A more efficient approach is to chop the chromosomes themselves into subsets
    of intervals, preferably in areas of uncertainty where the reference sequence
    has stretches of *N* bases, meaning the content of those regions is unknown, so
    it’s OK to interrupt processing there. Then, you can balance the sizes of the
    intervals in such a way that most of them will take about the same amount of time
    to process. But that still leaves you a lot of leeway for deciding on the average
    length of the intervals when you design that list. The more you chop up the sequence,
    the sooner you can expect to have your results in hand, because you can (within
    reason) run all those intervals in parallel. However, the shorter the runtime
    of each individual interval, the more you will feel the pain—and more to the point,
    the cost—of the overhead involved in dispatching the work to separate VMs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 更有效的方法是将染色体本身切割为不确定性区域的子区间，其中参考序列具有*N*个碱基的区域，意味着这些区域的内容是未知的，因此在这些地方中断处理是可以接受的。然后，你可以平衡区间的大小，使大多数区间的处理时间大致相同。但这仍然给你在设计列表时平均长度的区间留下了很大的自由度。你将序列切分得越细，就越早能够获得结果，因为你可以（在合理范围内）并行运行所有这些区间。然而，每个单独区间的运行时间越短，你就会越感受到涉及将工作分发到单独的虚拟机所带来的开销，更重要的是成本方面的影响。
- en: Not convinced? Suppose that you define three hundred intervals that take three
    hours each to analyze (these are made-up numbers, but should be in the ballpark
    of realistic). For each interval, you’ll pay up to 10 minutes of VM overhead time
    during the setup phase before the actual analysis begins (oh yeah, you get charged
    for that time). That’s 300 × 10 minutes; in other words, 50 hours’ worth of VM
    cost spent on overhead. Assuming that you’re using basic machines that cost about
    $0.03/hour, that amounts to $1.50 which is admittedly not the end of the world.
    Now let’s say you turn up the dial by a factor of 10 on the granularity of your
    intervals, producing 3,000 shorter intervals that take 18 minutes each to analyze.
    You’ll get your results considerably faster, but you’ll pay 3,000 × 10 minutes,
    or 500 hours’ worth of VM cost on overhead. Now you’re spending $15 per sample
    on overhead. Across a large number of samples, that might make a noticeable dent
    in your budget. But we’re not judging; the question is whether the speed is worth
    the money to *you*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 还不确定吗？假设你定义了三百个需要每个分析三小时的区间（这些数字是虚构的，但应该接近现实情况）。对于每个区间，在实际分析开始之前的设置阶段，你将支付最多10分钟的虚拟机额外时间费用（哦是的，你要为那段时间付费）。那就是300
    × 10分钟；换句话说，50小时的虚拟机成本用于额外费用。假设你使用的是每小时大约0.03美元的基本机器，那就是1.50美元，尽管这并非世界末日。现在假设你将你的区间粒度提高10倍，生成3000个较短的区间，每个区间分析需要18分钟。你将支付3000
    × 10分钟，或者500小时的虚拟机额外费用。现在你每个样本在额外费用上花费15美元。在大量样本中，这可能在你的预算中留下明显的痕迹。但我们不评判；问题是速度对*你*是否值得这些钱。
- en: The takeaway here is that the cloud gives you a lot of freedom to find your
    happy point.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的要点是，云计算为你提供了很多自由空间来找到你的幸福点。
- en: When the Broad Institute originally moved its entire genome analysis pipeline
    from its on-premises cluster to GCP, it cost about $45 to run per whole genome
    sample (at 30X coverage). Through a combination of workflow optimizations, and
    in collaboration with GCP engineers, the institute’s team was able to squeeze
    that down to $5 per whole genome sample. These phenomenal savings translated into
    lower production costs, which at the end of the day means more science per research
    dollar.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Broad Institute 最初将其整个基因组分析流程从本地集群迁移到 GCP 时，每个全基因组样本（30X 覆盖率）的运行成本约为45美元。通过工作流优化和与
    GCP 工程师的合作，该研究院团队将成本降低到每个全基因组样本5美元。这些显著的节省转化为更低的生产成本，最终意味着每研究美元可以获得更多科学成果。
- en: Admittedly, the cloud pipeline does take about 23 hours to run to completion
    on a single sample, which is quite a bit longer than the leading accelerated solutions
    that are now available to run GATK faster on specialized hardware, such as Illumina’s
    DRAGEN solution. For context, though, it typically takes about two days to prepare
    and sequence a sample on the wetlab side of the sequencing process, so unless
    you’re in the business of providing urgent diagnostic services, the additional
    day of analysis is usually not a source of concern.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，云管道确实需要大约23小时才能完成单个样本的运行，这比现在可用于在专用硬件上更快运行 GATK 的主流加速解决方案要长得多，例如 Illumina
    的 DRAGEN 解决方案。不过，作为背景，典型情况下湿实验室侧的样本准备和测序过程通常需要大约两天时间，所以除非你从事提供紧急诊断服务的业务，分析的额外一天通常不会成为关注点。
- en: 'Here’s the thing: *that processing time does not monopolize computational resources
    that you could be using for something else*. If you need more VMs, you just ask
    PAPI to summon them. Because there is practically no limit to the number of workflows
    that the operations team can launch concurrently, there is little risk of having
    a backlog build up when an order for a large cohort comes through—unless it’s
    on the scale of [gnomAD](https://oreil.ly/vHPKf), which contains more than 100,000
    exome and 80,000 whole genome samples. Then, it takes a bit of advance planning
    and a courtesy email to your friendly GCP account manager so that they can tell
    their operations team to gird their loins. (We imagine the email: “Brace yourselves;
    gnomAD is coming.”)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于：*处理时间并不占用你本可以用于其他事情的计算资源*。如果你需要更多的虚拟机，只需请求 PAPI 召唤它们。由于运营团队可以同时启动的工作流没有实际上限，当大批订单到来时，几乎不会出现积压的风险，除非是规模达到[gnomAD](https://oreil.ly/vHPKf)的程度，其中包含超过10万个外显子和8万个全基因组样本。那么，就需要一些提前计划和一封友好的
    GCP 账户经理的礼貌邮件，这样他们可以告诉他们的运营团队做好准备。（我们想象这封邮件会这样写：“做好准备，gnomAD 即将到来。”）
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The cost of the Broad’s cloud-based whole genome analysis pipeline unfortunately
    increased to around $8 per sample following the discovery of two major computer
    security vulnerabilities, [Spectre](https://oreil.ly/fbaUw) and [Meltdown](https://oreil.ly/xwa5J),
    that affect computer processors worldwide. The security patches that are required
    to make the machines safe to use add overhead that causes longer runtimes, which
    will apparently remain unavoidable until cloud providers switch out the affected
    hardware. Over time, though, the cost of storage and compute on the cloud continues
    to fall, so we expect the $8 price to decrease over time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于发现影响全球计算机处理器的两个主要安全漏洞[Spectre](https://oreil.ly/fbaUw)和[Meltdown](https://oreil.ly/xwa5J)，Broad
    的基于云的全基因组分析管道的成本每个样本增加到约 8 美元。使机器安全使用所需的安全补丁增加了额外开销，导致更长的运行时间，这似乎将在云供应商更换受影响硬件之前始终无法避免。然而，随着时间的推移，云上存储和计算的成本继续下降，因此我们预计
    8 美元的价格会随时间而降低。
- en: Suggested Cost-Saving Optimizations
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建议的节省成本优化策略
- en: If you’re curious to explore the workflow implementation that the Broad Institute
    team developed to achieve those substantial cost savings, look no further than
    back to [Chapter 9](ch09.xhtml#deciphering_real_genomics_workflows). The optimized
    pipeline we’ve been talking about is none other than the second workflow we dissected
    in that chapter (the one with the subworkflows and task libraries).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解 Broad Institute 团队开发的工作流程实施细节以实现大幅节省成本，无需再看其他，直接参考[第 9 章](ch09.xhtml#deciphering_real_genomics_workflows)即可。我们所讨论的优化流水线，就是该章节中我们分析的第二个工作流程（包含子工作流和任务库）。
- en: Without going too far into the details, here’s a summary of the three most effective
    strategies for optimizing WDL workflows to run cheaply on Google Cloud. You can
    see these in action in the *GermlineVariantDiscovery.wdl* library of tasks that
    we looked at in [Chapter 9](ch09.xhtml#deciphering_real_genomics_workflows).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不要深入细节，以下是优化在 Google Cloud 上便宜运行 WDL 工作流的三种最有效策略的总结。您可以在我们在[第 9 章](ch09.xhtml#deciphering_real_genomics_workflows)中查看的
    *GermlineVariantDiscovery.wdl* 任务库中看到这些策略的应用。
- en: Dynamic sizing for resource allocation
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源分配的动态调整
- en: 'The more storage you request for a VM for a given task, the more it will cost
    you. You can keep that cost down by requesting only the bare minimum amount of
    disk storage that will fit the task, but how do you deal with variability in file
    input sizes without having to check them manually for every sample you need to
    process? Good news: there are WDL functions that allow you to evaluate the size
    of input files going into a task at runtime (but before the VM is requested).
    Then, you can apply some arithmetic (based on reasonable assumptions about what
    the task will produce) to calculate how much disk should be allocated. For example,
    the following code measures the total size of the reference genome files and then
    calculates the desired disk size based on the amount of space needed to account
    for a fraction of the input BAM file (more on that in a minute) plus the reference
    files, plus some padding to account for the output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定任务的虚拟机请求的存储量越多，成本就会越高。您可以通过仅请求最低限度的磁盘存储量来降低成本，但如何在不手动检查每个需要处理的样本的文件输入大小的情况下处理文件输入大小的变化？好消息是：有一些
    WDL 函数允许您在运行时（但在请求虚拟机之前）评估输入文件的大小。然后，您可以应用一些算术（基于对任务产生的文件大小的合理假设）来计算应分配多少磁盘。例如，以下代码测量参考基因组文件的总大小，然后根据需要考虑输入
    BAM 文件的一部分加上参考文件，再加上一些填充以处理输出：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: See [this blog post](https://oreil.ly/P9Vxg) for a more detailed discussion
    of this approach.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[此博文](https://oreil.ly/P9Vxg)以详细讨论这一方法。
- en: File streaming to GATK4 tools
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件流传输到 GATK4 工具
- en: 'This is another great way to reduce the amount of disk space you need to request
    for your task VMs. Normally, Cromwell and PAPI localize all input files to the
    VM as a prerequisite for task execution. However, GATK (starting in version 4.0)
    is capable of streaming data directly from GCS, so if you’re running a GATK tool
    on a genomic interval, you can instruct Cromwell not to localize the file and
    just let GATK handle it, which it will do by retrieving only the subset of data
    specified in the interval. This is especially useful, for example, if you’re running
    `HaplotypeCaller` on a 300 Gb BAM file but you’re parallelizing its operation
    across many intervals to run on segments that are orders of magnitude smaller.
    In the previous code example, that’s why the size of the input BAM file is divided
    by the width of the scatter (i.e., the number of intervals). To indicate in your
    WDL that an input file can be streamed, simply add the following to your task
    definition, for the relevant input variable:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是减少任务 VM 所需磁盘空间请求的另一种好方法。通常，Cromwell 和 PAPI 将所有输入文件本地化到 VM 中作为任务执行的先决条件。然而，GATK（从版本
    4.0 开始）能够直接从 GCS 流式传输数据，因此，如果你在基因组间隔上运行 GATK 工具，可以指示 Cromwell 不本地化文件，而是让 GATK
    处理，它将仅检索间隔中指定的数据子集。例如，如果你在一个 300 Gb BAM 文件上并行运行 `HaplotypeCaller`，但将其操作分解为许多数量级较小的间隔，这就尤为有用。在前面的代码示例中，这就是为什么输入
    BAM 文件的大小会被分割为 scatter 的宽度（即间隔的数量）。要在你的 WDL 中指示输入文件可以进行流式传输，只需为相关的输入变量在任务定义中添加以下内容：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that this currently works only for files in GCS and is not available to
    Picard tools bundled in GATK.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目前此功能仅适用于 GCS 中的文件，并不适用于捆绑在 GATK 中的 Picard 工具。
- en: Preemptible VM instances
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预留 VM 实例
- en: 'This involves using a category of VMs, called [preemptible](https://oreil.ly/lep8v),
    that are much cheaper to use than the normal pricing (currently 20% of list price).
    These discounted VMs come with all the same specifications as normal VMs, but
    here’s the catch: Google can take them away from you at any time. The idea is
    that it’s a pool of spare VMs that you can use normally as long as there’s no
    shortage of resources. If somebody out there suddenly requests a large number
    of machines of the same type as you’re using and there aren’t enough available,
    some or all of the VMs that you are using will be reallocated to them, and your
    job will be aborted.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及使用称为[预留](https://oreil.ly/lep8v)的一类 VM，其使用成本比正常定价便宜得多（当前为定价的 20%）。这些折扣 VM
    具有与正常 VM 相同的所有规格，但这里有个问题：Google 可以随时从你手中收回它们。其理念是这是一个备用 VM 的池子，你可以正常使用，只要资源没有短缺。如果某人突然请求与你使用相同类型的大量机器，而可用资源不足，则你正在使用的部分或所有
    VM 将重新分配给他们，并且你的作业将被中止。
- en: 'To head off the two most common questions, no, there is no (built-in) way to
    save whatever progress had been made before the interruption; and yes, you will
    get charged for the time you used them. On the bright side, PAPI will inform Cromwell
    that your jobs were preempted, and Cromwell will try to restart them automatically
    on new VMs. Specifically, Cromwell will try to use preemptible VMs again, up to
    a customizable number of attempts (three attempts by default in most Broad Institute
    workflows); then, it will fall back to using regular (full-price) VMs. To control
    the number of preemptible attempts that you’re willing to allow for a given task,
    simply set the `preemptible:` property in the `runtime` block. To disable the
    use of preemptibles, set it to 0 or remove that line altogether. In the following
    example, the preemptible count is set using a `preemptible_tries` variable so
    that it can be easily customized on demand, as are the Docker container and the
    disk size:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免最常见的两个问题，不，没有（内置的）方法来保存中断前所做的任何进度；是的，你会被收取使用时间的费用。光明的一面是，PAPI 会通知 Cromwell
    你的作业已被抢占，并且 Cromwell 将尝试在新的 VM 上自动重新启动它们。具体来说，Cromwell 将再次尝试使用预留 VM，最多尝试自定义数量次数（在大多数
    Broad Institute 工作流中，默认为三次尝试）；然后，它将退回到使用常规（全价）VM。要控制对于给定任务你愿意允许的预留尝试次数，只需在 `runtime`
    块中设置 `preemptible:` 属性。要禁用预留使用，将其设置为 0 或完全删除该行。在下面的示例中，使用 `preemptible_tries`
    变量设置了预留尝试次数，以便根据需要轻松自定义，以及 Docker 容器和磁盘大小：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The bottom line? Using preemptible VMs is generally worthwhile for short-running
    jobs because the chance of them getting preempted before they finish is typically
    very small, but the longer the job, the less favorable the odds become.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结论呢？使用抢先式虚拟机（preemptible VMs）通常对短时间运行的作业非常值得，因为它们在完成之前被抢占的机会通常非常小，但随着作业时间的增加，胜算会变得越来越不利。
- en: Platform-Specific Optimization Versus Portability
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平台特定的优化与可移植性
- en: 'All of these optimizations are very specific to running on the cloud—and for
    some of them, not just any cloud—so now we need to talk about their impact on
    portability. As we discussed when we introduced workflow systems in [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work),
    one of the key goals for the development of Cromwell and WDL was portability:
    the idea that you could take the same workflow and run it anywhere to get the
    same results, without having to deal with a whole lot of software dependencies
    or hardware requirements. The benefits of computational portability range from
    making it easier to collaborate with other teams, to enabling any researcher across
    the globe to reproduce and build on your work after you’ve published it.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些优化都非常具体于在云上运行——对于其中一些优化来说，不是所有的云都适用——因此现在我们需要讨论它们对可移植性的影响。正如我们在[第8章](ch08.xhtml#automating_analysis_execution_with_work)介绍工作流系统时讨论的那样，Cromwell
    和 WDL 的开发关键目标之一就是可移植性：即你可以将相同的工作流在任何地方运行并获得相同的结果，而不需要处理大量软件依赖或硬件要求。计算可移植性的好处包括更容易与其他团队合作，以及让全球任何研究人员在你发表后能够重现和建立在你工作基础上的研究。
- en: 'Unfortunately, the process of optimizing a workflow to run efficiently on a
    given platform can make it less portable. For example, earlier versions of WDL
    did not include the `localization_optional: true` idiom, so to use the GATK file-streaming
    capability, workflow authors had to trick Cromwell into not localizing files by
    declaring those input variables as `String` types instead of `File` types. The
    problem? The resulting workflows could not be run on local systems without changing
    the relevant variable declarations. Instant portability fail.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '不幸的是，优化工作流以在特定平台上高效运行的过程可能会降低其可移植性。例如，较早版本的 WDL 没有包含 `localization_optional:
    true` 惯用法，因此为了使用 GATK 的文件流功能，工作流作者必须通过将这些输入变量声明为 `String` 类型而不是 `File` 类型来欺骗 Cromwell
    以避免本地化文件。问题在哪里？结果是，生成的工作流不能在本地系统上运行，除非修改相关的变量声明。即时的可移植性失败。'
- en: 'The introduction of `localization_optional: true` was a major step forward
    because it functions as a “hint” to the workflow management system. Basically,
    it means, “Hey, you don’t have to localize this file if you’re in a situation
    that supports streaming, but if you’re not, please do localize it.” As a result,
    you can run the same workflow in different places, enjoy the optimizations where
    you can, and rest assured that the workflow will still work everywhere else. Portability
    win!'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '引入 `localization_optional: true` 是一大进步，因为它作为工作流管理系统的一种“提示”功能。基本上，它的意思是，“嘿，如果你处于支持流式处理的情况下，你可以不必本地化这个文件，但如果不支持，请务必进行本地化。”
    因此，你可以在不同地方运行相同的工作流，享受你可以的优化，并确保工作流在其他地方也能正常工作。可移植性胜利！'
- en: The other optimizations we showed you have always had this harmless “hint” status,
    in the sense that their presence does not affect portability. Aside from the `docker`
    property, which we discussed in [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work),
    Cromwell will happily ignore any properties specified in the `runtime` block that
    are not applicable to the running environment, so you can safely have `preemptible`,
    `disk`, and so on specified even if you are just running workflows on your laptop.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向你展示的其他优化一直都是一种无害的“提示”状态，意味着它们的存在不会影响可移植性。除了在[第8章](ch08.xhtml#automating_analysis_execution_with_work)中讨论的
    `docker` 属性外，Cromwell 会忽略 `runtime` 块中指定的在当前环境下不适用的任何属性，因此，即使只是在笔记本上运行工作流，你也可以安全地指定
    `preemptible`、`disk` 等属性。
- en: Where things might get tricky is that other cloud providers such as Microsoft
    Azure and AWS have their own equivalents of features like preemptible VM instances
    (on AWS, the closest equivalents are called Spot Instances), but they don’t behave
    exactly the same way. So, inevitably the question will arise as to whether Cromwell
    should use the `preemptible` property to control Spot Instances when running on
    AWS, or provide a separate property for that purpose. If it’s the latter, where
    does it stop? Should every runtime property exist in as many flavors as there
    are cloud platforms?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一些云提供商（如 Microsoft Azure 和 AWS）有自己的类似功能的等价物，如预留 VM 实例（在 AWS 上，最接近的等价物称为 Spot
    Instances），但它们的行为不完全相同。因此，不可避免地会出现这样的问题：当在 AWS 上运行时，Cromwell 是否应该使用 `preemptible`
    属性来控制 Spot Instances，或者提供一个单独的属性来实现这一目的。如果是后者，这会到哪一步？每个运行时属性都应该存在多种云平台的版本吗？
- en: Oh, no, *we* don’t have an answer to that. We’ll be over here heating up some
    popcorn and watching the developers battle it out. The best way to keep track
    of what a given Cromwell backend currently can and can’t do is to take a look
    at the online [backend documentation](https://oreil.ly/7uTh6). For now, let’s
    get back to talking about the most convenient and scalable ways to run Cromwell
    on GCP.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 噢，不，*我们* 没有对此有答案。我们将会在这里加热一些爆米花并看开发人员之间的战斗。要跟踪给定 Cromwell 后端当前能做什么和不能做什么，最好的方法是查看在线的
    [后端文档](https://oreil.ly/7uTh6)。现在，让我们回到讨论在 GCP 上运行 Cromwell 最便捷和可扩展的方法。
- en: Wrapping Cromwell and PAPI Execution with WDL Runner
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 WDL Runner 封装 Cromwell 和 PAPI 执行
- en: It’s worth repeating that you can make Cromwell submit workflows to PAPI from
    just about anywhere, as long as you can run Cromwell and connect to the internet.
    As mentioned earlier, we’re making you use a GCP VM because it minimizes the amount
    of setup required to get to the good parts (running big workflows!), and it reduces
    the risk of things not working immediately for you. But you could absolutely do
    the same thing from your laptop or, if you’re feeling a bit cheeky, from a VM
    on AWS or Azure. The downside of the laptop option, however, is that you would
    need to make sure that your laptop stays on (powered and connected) during the
    entire time that your workflow is running. For a short-running workflow that’s
    probably fine; but for full-scale work like a whole genome analysis, you really
    want to be able to fire off a workflow and then pack up your machine for the night
    without interrupting the execution.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒：请确保你可以从几乎任何地方通过 **Cromwell** 向 PAPI 提交工作流，只要你能运行 Cromwell 并连接到互联网即可。正如之前提到的，我们要求你使用
    GCP VM 是因为它最大程度上减少了设置所需的步骤（运行大型工作流！），并减少了因为一些问题无法立即工作的风险。但是你绝对可以从你的笔记本电脑或者如果你感觉有点狡猾的话，从
    AWS 或 Azure 的 VM 中完成同样的事情。然而，使用笔记本电脑的缺点是，在你的工作流运行期间，你需要确保笔记本保持开启（电源连接并保持连接）。对于短期运行的工作流，这可能没问题；但是对于像全基因组分析这样的大规模工作，你真的希望能够启动一个工作流然后在晚上整理好你的设备而不中断执行。
- en: 'Enter the *WDL Runner*. This open source toolkit acts as a lightweight wrapper
    for Cromwell and PAPI. With a single command line from you, WDL Runner creates
    a VM in GCP, sets up a container with Cromwell configured to use PAPI, and submits
    your workflow to be executed via PAPI, as shown in [Figure 10-6](#overview_of_wdl_runner_operation).
    Does any of that sound familiar? Aside from a few details, this is pretty much
    what you did in earlier chapters: create a VM instance, put Cromwell on it, configure
    it to communicate with PAPI, and run a Cromwell command to launch a workflow.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 *WDL Runner*。这个开源工具包作为 Cromwell 和 PAPI 的轻量级封装。通过你的一个单一命令行，WDL Runner 在 GCP
    中创建一个 VM，设置一个包含配置好使用 PAPI 的 Cromwell 的容器，并通过 PAPI 提交你的工作流进行执行，如图 [Figure 10-6](#overview_of_wdl_runner_operation)
    所示。这些听起来是不是有些耳熟？除了一些细节之外，这基本上就是你在前面章节中所做的：创建一个 VM 实例，安装 Cromwell，配置它与 PAPI 通信，并运行一个
    Cromwell 命令来启动工作流。
- en: '![Overview of WDL Runner operation](Images/gitc_1006.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![WDL Runner 操作概览](Images/gitc_1006.png)'
- en: Figure 10-6\. Overview of WDL Runner operation.
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. WDL Runner 操作概览。
- en: 'So at a basic level, you can view WDL Runner as a way to outsource all of that
    work and achieve the same result without having to manually manage a VM. But there’s
    more: when the workflow completes, WDL Runner will transfer the execution logs
    to a location of your choosing and delete the VM, so it won’t keep costing you
    money if you don’t immediately turn it off. The cherry on the cake is that WDL
    Runner will also copy the final outputs to a location of your choosing, so you
    don’t need to go dig through Cromwell’s (annoyingly deep) execution directories
    to find the result you care about. Oh, and here’s another cherry: WDL Runner also
    comes with monitoring tools that allow you to get a status summary for your workflow.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，你可以将**WDL Runner**视为一种方式来外包所有这些工作，并实现相同的结果，而无需手动管理虚拟机。但还有更多：当工作流程完成时，**WDL
    Runner**会将执行日志传输到你选择的位置并删除虚拟机，因此如果你不立即关闭它，它不会继续花费你的资金。最后的点睛之笔是，**WDL Runner**还会将最终输出复制到你选择的位置，因此你无需深入到**Cromwell**的（令人讨厌的深层）执行目录中找到你关心的结果。哦，还有另一个点睛之笔：**WDL
    Runner**还配备了监控工具，允许你获取工作流程的状态摘要。
- en: Needless to say, you can run WDL Runner from anywhere, but we’re going to have
    you run it from your VM because it’s right there and it has everything you need
    on it. Feel free to try it out from your laptop, as well, of course.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以从任何地方运行**WDL Runner**，但我们建议你从虚拟机中运行，因为它就在那里，而且包含了你需要的一切。当然，你也可以从你的笔记本电脑上尝试。
- en: Setting Up WDL Runner
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置**WDL Runner**
- en: 'Good news: there’s nothing to set up! We’ve included a copy of the WDL Runner
    code, which is [available in GitHub](https://oreil.ly/-upHl), in the book bundle
    that you already copied to your VM. WDL Runner’s main requirements are Python
    2.7, Java 8, and `gcloud`, which are all available on your VM already.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息：不需要进行设置！我们已经在书籍捆绑包中包含了**WDL Runner**代码的副本，可以在[GitHub上获取](https://oreil.ly/-upHl)。**WDL
    Runner**的主要要求是Python 2.7、Java 8和`gcloud`，这些在你的虚拟机上已经都有了。
- en: 'To make the paths easier to manage, we are once again going to use environment
    variables. Two of them are variables that you should already have defined earlier
    in the chapter, `$WF` pointing to the *workflows* directory and `$BUCKET` pointing
    to your Google bucket. In addition, you are going to create a `$WR_CONF` variable
    pointing to the *config* directory and a `$WR_PIPE` variable pointing to the location
    of the *WDL_Runner* pipeline configuration file, *wdl_pipeline.yaml*, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使路径管理更容易，我们再次使用环境变量。其中两个变量是你应该在本章前面已经定义的变量，`$WF`指向*workflows*目录，`$BUCKET`指向你的Google存储桶。此外，你还要创建一个`$WR_CONF`变量，指向*config*目录，以及一个`$WR_PIPE`变量，指向*WDL_Runner*流水线配置文件的位置，即*wdl_pipeline.yaml*，如下所示：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That’s not a mistake, by the way; there are two nested directories that are
    both called *wdl_runner*; the lower-level one refers to code that is specific
    to *wdl_runner* whereas the other one refers to the overall package. If you poke
    around in the latter, you’ll see that beside the other directory called *wdl_runner*,
    there is as a directory called *monitoring_tools*. We’ll give you three guesses
    as to what’s in there, and the first two don’t count.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，这不是错误；事实上，有两个嵌套的目录都被称为*wdl_runner*；更低级的一个指的是特定于*wdl_runner*的代码，而另一个指的是整个包。如果你查看后者，你会发现除了另一个名为*wdl_runner*的目录外，还有一个名为*monitoring_tools*的目录。我们来猜猜看，你有三次猜测的机会，前两次不算。
- en: Running the Scattered HaplotypeCaller Workflow with WDL Runner
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用**WDL Runner**运行分散的**HaplotypeCaller**工作流
- en: 'This is going to be reasonably straightforward. As a heads-up, you’ll notice
    that the command line for launching a workflow with WDL Runner is a bit longer
    than the one for launching it directly with Cromwell itself. That’s because WDL
    Runner gives you a few more options for where to store outputs and so on without
    having to mess around with a configuration file. Here it is:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这将相当直接。需要注意的是，使用**WDL Runner**启动工作流的命令行会比直接使用**Cromwell**启动要长一些。这是因为**WDL Runner**为你提供了更多选项，比如在不需要配置文件的情况下选择存储输出位置等。下面是示例：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: But don’t run it yet! Let’s walk through it briefly so that you know what to
    customize versus what to leave alone.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 但别急着运行它！让我们简要地走一遍，这样你就知道哪些部分需要定制，哪些部分需要保持不变。
- en: The first line tells you that this command is primarily calling `gcloud`, which
    puts an interesting twist on the situation. Previously, we were running Cromwell
    and instructing it to communicate with Google Cloud things. Now, we’re calling
    a Google Cloud thing to have it run Cromwell and make it communicate with Google
    Cloud things. You might not be entirely surprised to learn that WDL Runner was
    originally written by a team at GCP.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行告诉您这个命令主要调用了 `gcloud`，这在情况上有点有趣的转折。之前，我们运行 Cromwell 并指示它与 Google Cloud 事务通信。现在，我们调用
    Google Cloud 事务来运行 Cromwell 并让它与 Google Cloud 事务通信。您可能不会完全惊讶地了解到，WDL Runner 最初是由
    GCP 团队编写的。
- en: The `--pipeline-file` argument takes the path to a sort of configuration file
    that comes prebaked with the WDL Runner code; you would need to modify the path
    if you wanted to run WDL Runner from a different working directory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`--pipeline-file`参数接受指向一种配置文件的路径，该配置文件预先包含了 WDL Runner 代码；如果您希望从不同的工作目录运行 WDL
    Runner，需要修改路径。'
- en: 'The `--regions` argument, unsurprisingly, controls the region where you want
    the VMs to reside. Funny thing: the WDL Runner seems to require this, but it conflicts
    with the default `gcloud` settings that were set on your VM when you originally
    authenticated yourself with `gcloud`. Specifically, the command fails and returns
    this error message:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`--regions`参数，不出所料，控制着您希望 VM 位于的地区。有趣的是：WDL Runner 似乎需要这个参数，但它与您在最初通过 `gcloud`
    认证时设置的默认 `gcloud` 设置相冲突。具体来说，该命令失败，并返回以下错误消息：'
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To work around this issue, you need to unset the default zone by running the
    following `gcloud` command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，您需要通过运行以下 `gcloud` 命令取消设置默认区域：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `--inputs-from-file` argument lists the workflow WDL file, *inputs* JSON
    file, and options file. The options file allows you to specify runtime configuration
    settings that will apply to the entire workflow. We don’t need to specify anything,
    but because WDL Runner requires that we provide one, we included a stub file that
    doesn’t actually contain any settings. As an aside, make sure there is absolutely
    no whitespace between the key:value pairs that specify the files (even around
    the commas), or the command will fail with a weird error about unrecognized arguments.
    Yes, we try to experience all the failures so you don’t have to.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`--inputs-from-file`参数列出了工作流 WDL 文件、*inputs* JSON 文件和选项文件。选项文件允许您指定将应用于整个工作流的运行时配置设置。我们不需要指定任何内容，但由于
    WDL Runner 要求我们提供一个，我们包含了一个实际上并不包含任何设置的存根文件。顺便说一句，请确保在指定文件的键值对之间（甚至逗号周围）绝对没有空白，否则命令将因为关于未识别参数的奇怪错误而失败。是的，我们尽力体验所有可能的失败，这样您就不必亲自经历了。'
- en: The `--env-vars` argument lists your desired locations for storing the execution
    directory and the final outputs; these can be anything you want, as long as they
    are valid GCS paths starting with *gs://*. Here, we’re using the `$BUCKET` environment
    variable that we originally set up in [Chapter 4](ch04.xhtml#first_steps_in_the_cloud),
    and we’re using a directory structure that we find convenient for managing outputs
    and logs. Feel free to chart your own paths, as it were. The `--logging` argument
    does the same thing as the previous argument but for the execution logs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`--env-vars`参数列出了您希望存储执行目录和最终输出的位置；只要是以*gs://*开头的有效 GCS 路径，这些位置可以是任何您想要的东西。在这里，我们使用了
    `$BUCKET` 环境变量，这是我们在[第四章](ch04.xhtml#first_steps_in_the_cloud)中最初设置的，并且我们使用了一个我们认为方便管理输出和日志的目录结构。请随意规划自己的路径，正如您所愿。`--logging`参数和前一个参数的功能相同，但用于执行日志。'
- en: 'When you’re happy with the look of your command, run it and then sit back and
    enjoy the sight of software at work. (Yes, saying this does feel a bit like tempting
    fate, doesn’t it.) If everything is copacetic, you should get a simple one-line
    response that looks like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当您对命令的外观满意时，请运行它，然后坐下来享受软件工作的景象。（是的，说这话确实有点像在招惹命运，是吧。）如果一切顺利，您应该会得到一个简单的单行回复，看起来像这样：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The string of numbers (and sometimes letters) that follows `operations/` is
    a unique identifier, the operations ID, that the service will use to track the
    status of your submission. Be sure to store that ID somewhere because it will
    be essential for the monitoring step, which is coming up next.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`operations/`后面的数字（有时候还会有字母）是一个唯一标识符，即操作 ID，服务将使用它来跟踪您的提交状态。一定要把这个 ID 存放在某个地方，因为它对接下来的监控步骤至关重要。'
- en: Monitoring WDL Runner Execution
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控 WDL Runner 执行
- en: 'And here we are, ready to monitor the status of our workflow submission. Remember
    the not-so-mysterious directory called *monitoring_tools*? Yep, we’re going to
    use that now. Specifically, we’re going to use a shell script called *monitor_wdl_pipeline.sh*
    that is located in that directory. The script itself is fairly typical sysadmin
    fare, meaning that it’s really painful to decipher if you’re not familiar with
    that type of code. Yet, in a stroke of poetic justice, it’s quite easy to use:
    you simply give the script the operations ID that you recorded in the previous
    step, and it will let you know how the workflow execution is going.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备监视我们的工作流程提交的状态。还记得叫做 *monitoring_tools* 的不那么神秘的目录吗？是的，我们现在要用它。具体来说，我们将使用位于该目录中的名为
    *monitor_wdl_pipeline.sh* 的 shell 脚本。这个脚本本身是相当典型的系统管理员工作，这意味着如果您不熟悉这种类型的代码，要解密它实在是很痛苦。然而，出乎意料的是，它非常容易使用：您只需给脚本上一步记录的操作
    ID，它就会告诉您工作流程的执行情况。
- en: Before you jump in, however, we recommend that you open a new SSH window to
    your VM so that you can leave the script running. It’s designed to continue to
    run, polling the `gcloud` service at regular intervals, until it receives word
    that the workflow execution is finished (whether successfully or not). As a reminder,
    you can open any number of SSH windows to your VM from the [Compute Engine](https://oreil.ly/NMIIX)
    console.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在您跳入之前，我们建议您打开一个新的 SSH 窗口到您的 VM，以便您可以让脚本继续运行。它被设计为持续运行，定期轮询 `gcloud` 服务，直到收到工作流程执行完成的消息（无论成功与否）。作为提醒，您可以从
    [Compute Engine](https://oreil.ly/NMIIX) 控制台打开任意数量的 SSH 窗口到您的 VM。
- en: 'When you’re in the new SSH window, move into the first *wdl_runner* directory
    and run the following command, substituting the operations ID you saved from the
    previous step:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在新的 SSH 窗口中时，请进入第一个 *wdl_runner* 目录，并运行以下命令，替换您从上一步保存的操作 ID：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is the early stage output that you would see if you start the monitoring
    script immediately after launching the WDL Runner command. As time progresses,
    the script will produce new updates, which will become increasingly more detailed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您在启动 WDL Runner 命令后立即启动监视脚本时会看到的早期输出。随着时间的推移，脚本将生成新的更新，这些更新会变得越来越详细。
- en: If you’re not satisfied by the content of the monitoring script’s output, you
    can always go to the Compute Engine console as we did earlier to view the list
    of VMs that are working on your workflow, as shown in [Figure 10-7](#list_of_active_vm_instances_left_parent).
    This time, you’ll see up to five new VMs with machine-generated names (in addition
    to the VM you created manually).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对监视脚本输出的内容不满意，您可以像之前那样转到 Compute Engine 控制台，查看正在处理您的工作流程的虚拟机列表，如 [Figure 10-7](#list_of_active_vm_instances_left_parent)
    所示。这次，您将看到多达五个新的带有机器生成名称的 VM（除了您手动创建的 VM）。
- en: '![List of active VM instances (WDL Runner submission)](Images/gitc_1007.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![活动虚拟机实例列表（WDL Runner 提交）](Images/gitc_1007.png)'
- en: Figure 10-7\. List of active VM instances (WDL Runner submission).
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 10-7\. 活动虚拟机实例列表（WDL Runner 提交）。
- en: 'But hang on, why would there be five VMs this time instead of four? If you’re
    confused, have a quick look back at [Figure 10-6](#overview_of_wdl_runner_operation)
    to refresh your memory of how this system works. This should remind you that the
    fifth wheel in this case is quite useful: it’s the VM instance with Cromwell that
    is controlling the work.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等等，这次为什么会有五个 VM 而不是四个？如果您感到困惑，请快速回顾一下 [Figure 10-6](#overview_of_wdl_runner_operation)，以刷新您对这个系统如何工作的记忆。这应该提醒您，这种情况下第五个轮子相当有用：它是控制工作的具有
    Cromwell 的 VM 实例。
- en: 'Getting back to the monitoring script, you should eventually see an update
    that starts with `operation complete`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 回到监视脚本，您最终应该看到一个以 `operation complete` 开头的更新：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There’s a huge amount of output below that, though, so how do you know whether
    it was successful? It’s the next few lines that you need to look out for. Either
    you see something like this and you breathe a happy sigh
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 下面会有大量的输出内容，那么您如何知道是否成功呢？您需要留意接下来的几行内容。要么您会看到类似这样的内容，然后您会松一口气：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'or you see see the dreaded `error:` line immediately following `done: true`,
    and you take a deep breath before diving into the error message details:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '或者您会看到在 `done: true` 后立即出现了可怕的 `error:` 行，然后您深呼吸，再深入查看错误消息的详细信息：'
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We’re not going to sugarcoat this: troubleshooting Cromwell and PAPI errors
    can be rough. If something went wrong with your workflow submission, you can find
    information about the error both in the output from the *monitor_wdl_pipeline.sh*
    script if you’re using WDL Runner, and in the files saved to the bucket, which
    we take a look at after this.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会掩饰这一点：调试 Cromwell 和 PAPI 错误可能会很艰难。如果你的工作流提交出现问题，你可以在 *monitor_wdl_pipeline.sh*
    脚本的输出以及保存到存储桶中的文件中找到错误信息，我们稍后会详细查看这些内容。
- en: Finally, let’s take a look at the bucket to see the output produced by WDL Runner,
    shown in [Figure 10-8](#output_from_the_wdl_runner_submissiondo).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看看存储桶中由 WDL Runner 产生的输出，如 [Figure 10-8](#output_from_the_wdl_runner_submissiondo)
    所示。
- en: '![Output from the WDL Runner submission.](Images/gitc_1008.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![WDL Runner 提交的输出。](Images/gitc_1008.png)'
- en: Figure 10-8\. Output from the WDL Runner submission.
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. WDL Runner 提交的输出。
- en: 'The *logging* output is a text file that contains the Cromwell log output.
    If you peek inside it, you’ll see that it looks different from the log output
    you saw earlier when you directly ran Cromwell. That’s because this instance of
    Cromwell was running in server mode! This is exciting: you ran a Cromwell server
    and didn’t even know you were doing it. Not that you were really taking advantage
    of the server features—but it’s a start. The downside is that the Cromwell server
    logs are extra unreadable for standard-issue humans, so we’re not even going to
    try to read them here.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*logging* 输出是一个文本文件，其中包含 Cromwell 的日志输出。如果你窥视其中，你会发现它与你直接运行 Cromwell 时看到的日志输出不同。这是因为这个
    Cromwell 实例是在服务器模式下运行的！这很激动人心：你运行了一个 Cromwell 服务器，甚至都不知道自己在做什么。虽然你并没有真正利用服务器功能——但这是个开始。不过，Cromwell
    服务器日志对于普通人来说实在太难读了，所以我们甚至不打算在这里试图去读它们。'
- en: The *work* directory is simply a copy of the Cromwell execution directory, so
    it has the same rabbit-hole-like structure and contents as you’ve come to expect
    by now.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*work* 目录只是 Cromwell 执行目录的一个副本，因此它具有你现在已经习惯的兔子洞式的结构和内容。'
- en: 'Finally, the *output* directory is one of the features we really love about
    the WDL Runner: this directory contains a copy of whatever file(s) were identified
    in the WDL as being the final output(s) of the workflow. This means that if you’re
    happy with the result, you can just delete the working directory outright without
    needing to trawl through it to save outputs you care about. When you have workflows
    that produce a lot of large intermediate outputs, this can save you a lot of time—or
    money from not paying storage costs for those intermediates.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*output* 目录是 WDL Runner 中我们非常喜欢的一个特性：该目录包含根据 WDL 中标识的文件作为工作流的最终输出的副本。这意味着如果你对结果满意，可以直接删除工作目录，无需费力去查找你关心的输出内容。当你的工作流产生大量大型中间输出时，这可以节省大量时间——或者因为不需要支付这些中间输出的存储费用而节省金钱。
- en: At the end of the day, we find WDL Runner really convenient for testing workflows
    quickly with little overhead while still being able to “fire and forget”; that
    is, launch the workflows, close your laptop, and walk away (as long as you record
    those operations IDs!). On the downside, it hasn’t been updated in a while and
    is out of step with WDL features; for example, it neither supports subworkflows
    nor task imports because it lacks a way to package more than one WDL file. In
    addition, you need to be very disciplined with keeping track of those operations
    IDs if you find yourself launching a lot of workflows.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们发现 WDL Runner 在快速测试工作流时非常方便，开销很小，而且仍然能够“点火并忘记”；也就是说，启动工作流，关闭笔记本电脑，然后走开（只要记录这些操作
    ID！）。不过，它有一个不足之处，就是它已经有一段时间没有更新，已经与 WDL 的功能脱节；例如，它既不支持子工作流，也不支持任务导入，因为它缺少一种打包超过一个
    WDL 文件的方法。此外，如果你发现自己经常启动许多工作流，你需要非常有纪律性地跟踪这些操作 ID。
- en: Here we showed you how to use WDL Runner as an example of how a project can
    wrap Cromwell, not as a fully-baked solution, and in the next chapter we’re going
    to move on to a system that we like better. However, if you like the concept of
    `wdl_runner`, don’t hesitate to make your voice heard on the project [GitHub page](https://oreil.ly/-upHl)
    to motivate further development.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了如何使用 WDL Runner 作为一个项目封装 Cromwell 的示例，而不是一个完全成熟的解决方案，并且在下一章中，我们将转向我们更喜欢的系统。然而，如果你喜欢
    *wdl_runner* 的概念，请不要犹豫在项目的 [GitHub 页面](https://oreil.ly/-upHl) 上表达你的声音，以推动进一步的开发。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: What time is it? Time to stop your VM again, and this time it’s especially important
    to do so because we won’t use it in any of the book’s remaining exercises. Feel
    free to delete your VM if you don’t anticipate coming back to previous tutorials.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在几点了？现在是时候再次关闭您的VM，这次尤为重要，因为在本书的剩余练习中我们将不再使用它。如果您不打算回到以前的教程，请随时删除您的VM。
- en: Wrap-Up and Next Steps
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和下一步
- en: In this chapter, we showed you how to take advantage of the cloud’s scaling
    power by running your workflows through the GCP Genomics PAPI service, first directly
    from the Cromwell command line and then from a light wrapper called WDL Runner.
    However, although these two approaches gave you the ability to run individual
    workflows of arbitrary complexity in a scalable way in GCP, the overhead involved
    in spinning up and shutting down a new instance of Cromwell every time you want
    to run a new workflow is highly inefficient and will scale poorly if you plan
    to run a lot of workflows.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您展示了如何通过在GCP基因组学PAPI服务中运行工作流来利用云计算的扩展能力，首先直接从Cromwell命令行，然后是一个名为WDL
    Runner的轻量包装器。然而，尽管这两种方法使您能够以可扩展的方式在GCP中运行任意复杂度的单个工作流，但每次运行新工作流时启动和关闭一个新的Cromwell实例所涉及的开销非常低效，如果您计划运行大量工作流，将不利于扩展。
- en: For truly scalable work, we want to use a proper Cromwell server that can receive
    and execute any number of workflows on demand, and can take advantage of sophisticated
    features like call caching, which allows Cromwell to resume failed or interrupted
    workflows from the point of failure. Call caching requires a connection to a database
    and is not supported by WDL Runner. Yet, setting up and administering a server
    and database—not to mention operating them securely in a cloud environment—involves
    more complexity and tech support burden than most of us are equipped to handle.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于真正可扩展的工作，我们希望使用一个合适的Cromwell服务器，可以根据需求接收和执行任意数量的工作流，并能利用复杂功能如调用缓存。调用缓存允许Cromwell从失败点恢复或重新执行中断的工作流。调用缓存需要连接到数据库，并不受WDL
    Runner支持。然而，设置和管理服务器和数据库，更不用说在云环境中安全操作它们，比大多数人所能处理的技术支持负担更复杂。
- en: That is why, in the next chapter, we’re going to show you an alternative path
    to enjoying the power of a Cromwell server without shouldering the maintenance
    burden. Specifically, we’re going to introduce you to Terra, a scalable platform
    for biomedical research built and operated on top of GCP by the Broad Institute
    in collaboration with Verily. Terra includes a Cromwell server that you’ll use
    to run GATK Best Practices workflows at full scale for the first time.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一章中，我们将向您展示一条不必承担维护负担即可享受Cromwell服务器强大功能的替代路径。具体来说，我们将介绍Terra，这是一个由Broad
    Institute与Verily合作在GCP上构建和运行的生物医学研究可扩展平台。Terra包括一个Cromwell服务器，您将首次用它来完整运行GATK最佳实践工作流。
