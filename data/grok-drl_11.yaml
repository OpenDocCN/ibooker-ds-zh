- en: 11 Policy-gradient and actor-critic methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 策略梯度与actor-critic方法
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: You will learn about a family of deep reinforcement learning methods that can
    optimize their performance directly, without the need for value functions.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解一类可以直接优化其性能的深度强化学习方法，而无需价值函数。
- en: You will learn how to use value function to make these algorithms even better.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习如何使用价值函数使这些算法变得更好。
- en: You will implement deep reinforcement learning algorithms that use multiple
    processes at once for very fast learning.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将实现使用多个进程同时进行以实现非常快速学习的深度强化学习算法。
- en: There is no better than adversity. Every defeat, every heartbreak, every loss,
    contains its own seed, its own lesson on how to improve your performance the next
    time.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比逆境更好。每一次失败、每一次心碎、每一次损失，都包含着自己的种子，自己的教训，告诉你如何在下一次做得更好。
- en: — Malcolm X American Muslim minister and human rights activist
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: —— 马尔科姆·X 美国穆斯林牧师和人权活动家
- en: In this book, we’ve explored methods that can find optimal and near-optimal
    policies with the help of value functions. However, all of those algorithms learn
    value functions when what we need are policies.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们探讨了在价值函数的帮助下可以找到最优和近似最优策略的方法。然而，所有这些算法在学习价值函数时，我们需要的却是策略。
- en: In this chapter, we explore the other side of the spectrum and what’s in the
    middle. We start exploring methods that optimize policies directly. These methods,
    referred to as *policy-based* or *policy-gradient* methods, parameterize a policy
    and adjust it to maximize expected returns.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索光谱的另一端以及中间部分的内容。我们开始探讨直接优化策略的方法。这些方法被称为*基于策略*或*策略梯度*方法，它们参数化一个策略并调整它以最大化预期回报。
- en: After introducing foundational policy-gradient methods, we explore a combined
    class of methods that learn both policies and value functions. These methods are
    referred to as actor-critic because the policy, which selects actions, can be
    seen as an actor, and the value function, which evaluates policies, can be seen
    as a critic. Actor-critic methods often perform better than value-based or policy-gradient
    methods alone on many of the deep reinforcement learning benchmarks. Learning
    about these methods allows you to tackle more challenging problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了基础策略梯度方法之后，我们探索了一类结合的方法，这些方法同时学习策略和价值函数。这些方法被称为actor-critic，因为选择动作的策略可以被视为actor，而评估策略的价值函数可以被视为critic。actor-critic方法在许多深度强化学习基准测试中通常比单独的基于值或策略梯度方法表现更好。了解这些方法可以使你应对更具挑战性的问题。
- en: These methods combine what you learned in the previous three chapters concerning
    learning value functions and what you learn about in the first part of this chapter,
    about learning policies. Actor-critic methods often yield state-of-the-art performance
    in diverse sets of deep reinforcement learning benchmarks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法结合了你在前三章中学到的关于学习价值函数的知识，以及在本章第一部分学到的关于学习策略的知识。actor-critic方法在多样化的深度强化学习基准测试中通常能取得最先进的性能。
- en: '![](../Images/11_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11_01.png)'
- en: Policy-based, value-based, and actor-critic methods
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略、基于值和actor-critic方法
- en: 'REINFORCE: Outcome-based policy learning'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: REINFORCE：基于结果的策略学习
- en: In this section, we begin motivating the use of policy-based methods, first
    with an introduction; then we discuss several of the advantages you can expect
    when using these kinds of methods; and finally, we introduce the simplest policy-gradient
    algorithm, called *rEINFORCE*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们开始介绍基于策略的方法的使用动机，首先是一个介绍；然后我们讨论使用这类方法时可以期待的一些优点；最后，我们介绍了最简单的策略梯度算法，称为*rEINFORCE*。
- en: Introduction to policy-gradient methods
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略梯度方法简介
- en: The first point I’d like to emphasize is that in policy-gradient methods, unlike
    in value-based methods, we’re trying to maximize a performance objective. In value-based
    methods, the main focus is to learn to evaluate policies. For this, the objective
    is to minimize a loss between predicted and target values. More specifically,
    our goal is to match the true action-value function of a given policy, and therefore,
    we parameterized a value function and minimized the mean squared error between
    predicted and target values. Note that we didn’t have true target values, and
    instead, we used actual returns in Monte Carlo methods or predicted returns in
    bootstrapping methods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先想强调的是，在策略梯度方法中，与基于值的方法不同，我们试图最大化性能目标。在基于值的方法中，主要关注的是学习评估策略。为此，目标是最小化预测值和目标值之间的损失。更具体地说，我们的目标是匹配给定策略的真实动作值函数，因此我们参数化了一个值函数，并最小化了预测值和目标值之间的均方误差。请注意，我们没有真正的目标值，而是使用蒙特卡洛方法中的实际回报或自举方法中的预测回报。
- en: In policy-based methods, on the other hand, the objective is to maximize the
    performance of a parameterized policy, so we’re running gradient ascent (or executing
    regular gradient descent on the negative performance). It’s rather evident that
    the performance of an agent is the expected total discounted reward from the initial
    state, which is the same thing as the expected state-value function from all initial
    states of a given policy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在基于策略的方法中，目标是最大化参数化策略的性能，因此我们执行梯度上升（或对负性能执行常规梯度下降）。很明显，代理的性能是从初始状态期望的总折现奖励，这与从给定策略的所有初始状态的期望状态值函数是同一回事。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathValue-based vs. policy-based
    methods objectives |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| ![数学展示](../Images/icons_Math.png) | 展示数学：基于值的方法与基于策略的方法的目标'
- en: '|  | ![](../Images/11_01_Sidebar01.png) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏01](../Images/11_01_Sidebar01.png) |'
- en: '| ŘŁ | With An RL AccentValue-based vs. policy-based vs. policy-gradient vs.
    actor-critic methods |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的基于值的方法与基于策略的方法与策略梯度方法与演员-评论家方法'
- en: '|  | **Value-based methods:** Refers to algorithms that learn value functions
    and only value functions.Q-learning, SARSA, DQN, and company are all value-based
    methods.**Policy-based methods:** Refers to a broad range of algorithms that optimize
    policies, including black-box optimization methods, such as genetic algorithms.**Policy-gradient
    methods:** Refers to methods that solve an optimization problem on the gradient
    of the performance of a parameterized policy, methods you’ll learn in this chapter.**Actor-critic
    methods:** Refers to methods that learn both a policy and a value function, primarily
    if the value function is learned with bootstrapping and used as the score for
    the stochastic policy gradient. You learn about these methods in this and the
    next chapter. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | **基于值的方法：**指的是学习值函数并仅学习值函数的算法。Q-learning、SARSA、DQN等都是基于值的方法。**基于策略的方法：**指的是优化策略的广泛算法，包括如遗传算法这样的黑盒优化方法。**策略梯度方法：**指的是在参数化策略的性能梯度上解决优化问题的方法，你将在本章学习这些方法。**演员-评论家方法：**指的是学习策略和值函数的方法，主要是在值函数通过自举学习并用作随机策略梯度评分的情况下。你将在本章和下一章学习这些方法。
    |'
- en: Advantages of policy-gradient methods
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略梯度方法的优势
- en: The main advantage of learning parameterized policies is that policies can now
    be any learnable function. In value-based methods, we worked with discrete action
    spaces, mostly because we calculate the maximum value over the actions. In high-dimensional
    action spaces, this max could be prohibitively expensive. Moreover, in the case
    of continuous action spaces, value-based methods are severely limited.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 学习参数化策略的主要优势是，策略现在可以是任何可学习的函数。在基于值的方法中，我们处理离散动作空间，主要是因为我们计算动作的最大值。在高维动作空间中，这个最大值可能过于昂贵。此外，在连续动作空间的情况下，基于值的方法受到严重限制。
- en: Policy-based methods, on the other hand, can more easily learn stochastic policies,
    which in turn has multiple additional advantages. First, learning stochastic policies
    means better performance under partially observable environments. The intuition
    is that because we can learn arbitrary probabilities of actions, the agent is
    less dependent on the Markov assumption. For example, if the agent can’t distinguish
    a handful of states from their emitted observations, the best strategy is often
    to act randomly with specific probabilities.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于策略的方法可以更容易地学习随机策略，这反过来又具有多个额外的优点。首先，学习随机策略意味着在部分可观察环境中具有更好的性能。直觉是，因为我们可以学习任意动作的概率，所以智能体对马尔可夫假设的依赖性较低。例如，如果智能体无法区分几个状态及其发出的观察结果，最佳策略通常是按照特定概率随机行动。
- en: '![](../Images/11_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![11_02.png](../Images/11_02.png)'
- en: Learning stochastic policies could get us out of trouble
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 学习随机策略可能帮助我们摆脱困境
- en: Interestingly, even though we’re learning stochastic policies, nothing prevents
    the learning algorithm from approaching a deterministic policy. This is unlike
    value-based methods, in which, throughout training, we have to force exploration
    with some probability to ensure optimality. In policy-based methods with stochastic
    policies, exploration is embedded in the learned function, and converging to a
    deterministic policy for a given state while training is possible.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管我们在学习随机策略，但没有任何东西阻止学习算法接近确定性策略。这与基于价值的 方法不同，在训练过程中，我们必须通过某种概率强制进行探索以确保最优性。在具有随机策略的基于策略的方法中，探索被嵌入到学习函数中，并且在训练过程中对于给定状态收敛到确定性策略是可能的。
- en: Another advantage of learning stochastic policies is that it could be more straightforward
    for function approximation to represent a policy than a value function. Sometimes
    value functions are too much information for what’s truly needed. It could be
    that calculating the exact value of a state or state-action pair is complicated
    or unnecessary.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 学习随机策略的另一个优点是，与表示价值函数相比，函数逼近表示策略可能更加直接。有时价值函数包含的信息过多，对于真正需要的信息来说是不必要的。可能计算状态或状态-动作对的精确价值是复杂或不必要的。
- en: '![](../Images/11_03.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![11_03.png](../Images/11_03.png)'
- en: Learning policies could be an easier, more generalizable problem to solve
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 学习策略可能是一个更容易、更易于推广的问题来解决
- en: A final advantage to mention is that because policies are parameterized with
    continuous values, the action probabilities change smoothly as a function of the
    learned parameters. Therefore, policy-based methods often have better convergence
    properties. As you remember from previous chapters, value-based methods are prone
    to oscillations and even divergence. One of the reasons for this is that tiny
    changes in value-function space may imply significant changes in action space.
    A significant difference in actions can create entirely unusual new trajectories,
    and therefore create instabilities.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要提到的一个优点是，由于策略是用连续值参数化的，动作概率会随着学习参数的变化而平滑地变化。因此，基于策略的方法通常具有更好的收敛特性。正如你从之前的章节中记得的那样，基于价值的方法容易产生振荡甚至发散。其中一个原因是价值函数空间中微小的变化可能意味着动作空间中显著的变化。动作的显著差异可以产生全新的轨迹，因此造成不稳定性。
- en: In value-based methods, we use an aggressive operator to change the value function;
    we take the maximum over Q-value estimates. In policy-based methods, we instead
    follow the gradient with respect to stochastic policies, which only progressively
    and smoothly changes the actions. If you directly follow the gradient of the policy,
    you’re guaranteed convergence to, at least, a local optimum.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，我们使用激进的算子来改变价值函数；我们对Q值估计取最大值。在基于策略的方法中，我们相反地跟随随机策略相对于梯度，这只会逐渐和平滑地改变动作。如果你直接跟随策略的梯度，你将保证收敛到至少一个局部最优解。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonStochastic policy for discrete
    action spaces 1/2 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![Python](../Images/icons_Python.png) | 我会说Python随机策略的离散动作空间 1/2 |'
- en: '|  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① This class, FCDAP stands for fully connected discrete-action policy.② The
    parameters allow you to specify a fully connected architecture, activation function,
    and weight and bias max magnitude.③ The __init__ function creates a linear connection
    between the input and the first hidden layer.④ Then, it creates connections across
    all hidden layers.⑤ Last, it connects the final hidden layer to the output nodes,
    creating the output layer.⑥ Here we have the method that takes care of the forward
    functionality.⑦ First, we make sure the state is of the type of variable and shape
    we expect before we can pass it through the network.⑧ Next, we pass the properly
    formatted state into the input layer and then through the activation function.⑨
    Then, we pass the output of the first activation through the sequence of hidden
    layers and respective activations.⑩ Finally, we obtain the output, which are logits,
    preferences over actions. |
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这个类，FCDAP代表全连接离散动作策略。② 参数允许你指定一个全连接架构、激活函数以及权重和偏差的最大幅度。③ `__init__` 函数在输入和第一个隐藏层之间创建线性连接。④
    然后，它创建所有隐藏层之间的连接。⑤ 最后，它将最终隐藏层连接到输出节点，创建输出层。⑥ 这里我们有处理正向功能的方法。⑦ 首先，我们确保状态是我们期望的类型和形状，然后我们才能将其通过网络。⑧
    接着，我们将格式正确的状态传递到输入层，然后通过激活函数。⑨ 然后，我们将第一个激活的输出传递到一系列隐藏层和相应的激活函数。⑩ 最后，我们获得输出，即logits，对动作的偏好。
    |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonStochastic policy for discrete
    action spaces 1/2 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python离散动作空间的随机策略 1/2 |'
- en: '|  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ⑪ This line repeats the last line on the previous page.⑫ Here we do the full
    forward pass. This is a handy function to obtain probabilities, actions, and everything
    needed for training.⑬ The forward pass returns the logits, the preferences over
    actions.⑭ Next, we sample the action from the probability distribution.⑮ Then,
    calculate the log probability of that action and format it for training.⑯ Here
    we calculate the entropy of the policy.⑰ And in here, for stats, we determine
    whether the policy selected was exploratory or not.⑱ Finally, we return an action
    that can be directly passed into the environment, the flag indicating whether
    the action was exploratory, the log probability of the action, and the entropy
    of the policy.⑲ This is a helper function for when we only need sampled action.⑳
    And this one is for selecting the greedy action according to the policy. |
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 这一行重复了上一页的最后一行。⑫ 这里我们进行完整的正向传播。这是一个方便的函数，用于获取概率、动作以及训练所需的一切。⑬ 正向传播返回的是logits，即对动作的偏好。⑭
    接下来，我们从概率分布中采样动作。⑮ 然后，计算该动作的对数概率并将其格式化为训练格式。⑯ 这里我们计算策略的熵。⑰ 在这里，为了统计，我们确定所选策略是探索性的还是不是。⑱
    最后，我们返回一个可以直接传递到环境中的动作，一个标志表示动作是否是探索性的，动作的对数概率以及策略的熵。⑲ 这是一个辅助函数，当我们只需要采样动作时使用。⑳
    这一个是为了根据策略选择贪婪动作。 |
- en: Learning policies directly
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接学习策略
- en: One of the main advantages of optimizing policies directly is that, well, it’s
    the right objective. We learn a policy that optimizes the value function directly,
    without learning a value function, and without taking into account the dynamics
    of the environment. How is this possible? Let me show you.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 直接优化策略的主要优势是，嗯，这是正确的目标。我们学习一个直接优化价值函数的策略，而不学习价值函数，也不考虑环境的动态。这是如何可能的？让我来展示给你看。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathDeriving the policy gradient
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学推导策略梯度 |'
- en: '|  | ![](../Images/11_03_Sidebar05.png) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/11_03_Sidebar05.png) |'
- en: Reducing the variance of the policy gradient
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降低策略梯度的方差
- en: 'It’s useful to have a way to compute the policy gradient without knowing anything
    about the environment’s transition function. This algorithm increases the log
    probability of all actions in a trajectory, proportional to the goodness of the
    full return. In other words, we first collect a full trajectory and calculate
    the full discounted return, and then use that score to weight the log probabilities
    of every action taken in that trajectory: *A*[*t*][*t*], *A*[*t*+1], ..., *A*[T–1].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种方法可以计算策略梯度，而无需了解任何关于环境转移函数的信息。这个算法增加了轨迹中所有动作的log概率，与完整回报的优良程度成正比。换句话说，我们首先收集一个完整的轨迹并计算完整的折现回报，然后使用这个分数来加权轨迹中每个动作的对数概率：*A*[*t*][*t*]，*A*[*t*+1]，...，*A*[T–1]。
- en: '![](../Images/11_04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11_04.png)'
- en: Let’s use only rewards that are a consequence of actions
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们只使用由动作产生的奖励
- en: '| ![](../Images/icons_Math.png) | Show Me The MathReducing the variance of
    the policy gradient |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学Reducing the variance of the policy
    gradient |'
- en: '|  | ![](../Images/11_04_Sidebar06.png) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | ![示例图片](../Images/11_04_Sidebar06.png) |'
- en: '| 0001 | A Bit Of HistoryIntroduction of the REINFORCE algorithm |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍REINFORCE算法 |'
- en: '|  | Ronald J. Williams introduced the REINFORCE family of algorithms in 1992
    in a paper titled “Simple Statistical Gradient-Following Algorithms for Connectionist
    Reinforcement Learning.”In 1986, he coauthored a paper with Geoffrey Hinton et
    al. called “Learning representations by back-propagating errors,” triggering growth
    in artificial neural network (ANN) research at the time. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | Ronald J. Williams于1992年在一篇题为“Simple Statistical Gradient-Following Algorithms
    for Connectionist Reinforcement Learning”的论文中介绍了REINFORCE算法系列。1986年，他与Geoffrey
    Hinton等人合著了一篇名为“Learning representations by back-propagating errors”的论文，这引发了当时人工神经网络（ANN）研究的发展。
    |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonREINFORCE |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonREINFORCE |'
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① This is the REINFORCE algorithm. When you see the <...>, that means code was
    removed for simplicity. Go to the chapter’s Notebook for the complete code.② First,
    we calculate the discounts as with all Monte Carlo methods. The logspace function
    with these parameters returns the series of per time step gammas; for example,
    [1, 0.99, 0.9801, ... ].③ Next, we calculate the sum of discounted returns for
    all time steps.④ To emphasize, this is the returns for every time step in the
    episode, from the initial state at time step 0, to one before the terminal T-1.⑤
    Notice here that we’re using the mathematically correct policy-gradient update,
    which isn’t what you commonly find out there. The extra discount assumes that
    we’re trying to optimize the expect discounted return from the initial state,
    so returns later in an episode get discounted.⑥ This is policy loss; it’s the
    log probability of the actions selected weighted by the returns obtained after
    that action was selected. Notice that because PyTorch does gradient descent by
    default and the performance is something we want to maximize, we use the negative
    mean of the performance to flip the function. Think about it as doing gradient
    ascent on the performance. Also, we account for discounted policy gradients, so
    we multiply the returns by the discounts.⑦ In these three steps, we first zero
    the gradients in the optimizer, then do a backward pass, and then step in the
    direction of the gradient.⑧ This function obtains an action to be passed to the
    environment and all variables required for training.⑨ Still exploring functions
    of the REINFORCE class⑩ The train method is the entry point for training the agent.⑪
    We begin by looping through the episodes.⑫ For each new episode, we initialize
    the variables needed for training and stats.⑬ Then, do the following for each
    time step.⑭ First, we collect experiences until we hit a terminal state.⑮ Then,
    we run one optimization step with the batch of all time steps in the episode.⑯
    Another thing I want you to see is the way I select the policy during evaluation.
    Instead of selecting a greedy policy, I sample from the learned stochastic policy.
    The correct thing to do here depends on the environment, but sampling is the safe
    bet. |
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这就是REINFORCE算法。当你看到<...>时，这意味着为了简化，代码已被删除。请前往该章节的笔记本以获取完整的代码。② 首先，我们像所有蒙特卡洛方法一样计算折扣。具有这些参数的logspace函数返回每一步的伽玛序列；例如，[1,
    0.99, 0.9801, ...]。③ 接下来，我们计算所有时间步的折扣回报总和。④ 为了强调，这是在每一时间步的回报，从时间步0的初始状态到终端T-1之前。⑤
    注意这里我们使用的是数学上正确的策略梯度更新，这并不是你通常能找到的。额外的折扣假设我们试图优化从初始状态期望的折扣回报，因此，在场景中较晚的回报会得到折扣。⑥
    这是策略损失；它是选择动作的对数概率，加权之后是选择该动作后获得的回报。注意，由于PyTorch默认使用梯度下降，并且我们希望最大化性能，所以我们使用性能的负平均值来翻转函数。把它看作是在性能上进行梯度上升。此外，我们考虑了折扣策略梯度，因此我们将回报乘以折扣。⑦
    在这三个步骤中，我们首先在优化器中将梯度置零，然后进行反向传播，然后沿着梯度的方向进行步进。⑧ 这个函数获取要传递给环境的动作以及所有训练所需的变量。⑨ 仍在探索REINFORCE类的功能⑩
    训练方法是训练代理的入口点。⑪ 我们首先通过循环遍历场景。⑫ 对于每个新的场景，我们初始化训练和统计所需的变量。⑬ 然后，对每个时间步执行以下操作。⑭ 首先，我们收集经验，直到达到终端状态。⑮
    然后，我们使用场景中所有时间步的批次运行一个优化步骤。⑯ 我还希望让你看到我在评估期间选择策略的方式。我并不是选择贪婪策略，而是从学习到的随机策略中进行采样。这里正确的做法取决于环境，但采样是安全的赌注。
    |
- en: 'VPG: Learning a value function'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VPG：学习一个值函数
- en: The REINFORCE algorithm you learned about in the previous section works well
    in simple problems, and it has convergence guarantees. But because we’re using
    full Monte Carlo returns for calculating the gradient, its variance is a problem.
    In this section, we discuss a few approaches for dealing with this variance in
    an algorithm called *vanilla policy gradient* or *REINFORCE with baseline*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你在上一个部分学到的REINFORCE算法在简单问题中表现良好，并且有收敛保证。但由于我们使用完整的蒙特卡洛回报来计算梯度，其方差是一个问题。在本节中，我们将讨论几种处理这种方差的方法，这些方法被称为*纯策略梯度*或*带有基线的REINFORCE*。
- en: Further reducing the variance of the policy gradient
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步减少策略梯度的方差
- en: REINFORCE is a principled algorithm, but it has a high variance. You probably
    remember from the discussion in chapter 5 about Monte Carlo targets, but let’s
    restate. The accumulation of random events along a trajectory, including the initial
    state sampled from the initial state distribution—transition function probabilities,
    but now in this chapter with stochastic policies—is the randomness that action
    selection adds to the mix. All this randomness is compounded inside the return,
    making it a high-variance signal that's challenging to interpret.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE是一个原则性的算法，但它具有很高的方差。你可能还记得第5章中关于蒙特卡洛目标的讨论，但让我们再重申一遍。沿着轨迹随机事件的累积，包括从初始状态分布中采样的初始状态——转换函数概率，但现在在本章中是具有随机策略的——是动作选择添加到混合中的随机性。所有这些随机性都累积在回报中，使其成为一个高方差信号，难以解释。
- en: One way for reducing the variance is to use partial returns instead of the full
    return for changing the log probabilities of actions. We already implemented this
    improvement. But another issue is that action log probabilities change in the
    proportion of the return. This means that, if we receive a significant positive
    return, the probabilities of the actions that led to that return are increased
    by a large margin. And if the return is of significant negative magnitude, then
    the probabilities are decreased by of large margin.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 减少方差的一种方法是用部分回报而不是完整回报来改变动作的对数概率。我们已经实现了这一改进。但另一个问题是动作对数概率会随着回报的比例而变化。这意味着，如果我们收到一个显著的正面回报，导致该回报的动作概率会大幅增加。而如果回报是显著的负值，那么概率会大幅降低。
- en: However, imagine an environment such as the cart-pole, in which all rewards
    and returns are positive. In order to accurately separate okay actions from the
    best actions, we need a lot of data. The variance is, otherwise, hard to muffle.
    It would be handy if we could, instead of using noisy returns, use something that
    allows us to differentiate the values of actions in the same state. Recall?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，想象一个像小车-杆这样的环境，其中所有奖励和回报都是正的。为了准确地区分可接受的动作和最佳动作，我们需要大量的数据。否则，方差很难被抑制。如果我们能够不用噪声回报，而用某种允许我们在相同状态下区分动作值的东西，那就方便多了。还记得吗？
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryUsing estimated advantages
    in policy-gradient methods |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Memory.png) | 刷新我的记忆使用策略梯度方法中的估计优势'
- en: '|  | ![](../Images/11_04_Sidebar10.png) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/11_04_Sidebar10.png) |'
- en: Learning a value function
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习值函数
- en: As you see on the previous page, we can further reduce the variance of the policy
    gradient by using an estimate of the action-advantage function, instead of the
    actual return. Using the advantage somewhat centers scores around zero; better-than-average
    actions have a positive score, worse-than-average, a negative score. The former
    decreases the probabilities, and the latter increases them.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上一页看到的，我们可以通过使用动作优势函数的估计值来进一步减少策略梯度的方差，而不是实际回报。使用优势函数可以在一定程度上使分数围绕零中心；优于平均的动作有正分数，而低于平均的动作有负分数。前者会降低概率，而后者会增加概率。
- en: We’re going to do exactly that. Let’s now create two neural networks, one for
    learning the policy, the other for learning a state-value function, V. Then, we
    use the state-value function and the return for calculating an estimate of the
    advantage function, as we see next.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做的是确切如此。现在，让我们创建两个神经网络，一个用于学习策略，另一个用于学习状态值函数V。然后，我们使用状态值函数和回报来计算优势函数的估计值，就像我们接下来看到的那样。
- en: '![](../Images/11_05.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11_05.png)'
- en: Two neural networks, one for the policy, one for the value function
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 两个神经网络，一个用于策略，一个用于值函数
- en: '| ŘŁ | With An RL AccentREINFORCE, vanilla policy gradient, baselines, actor-critic
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的REINFORCE、纯策略梯度、基线、演员-评论家'
- en: '|  | Some of you with prior DRL exposure may be wondering, is this a so-called
    “actor-critic”? It’s learning a policy and a value-function, so it seems it should
    be. Unfortunately, this is one of those concepts where the “RL accent” confuses
    newcomers. Here’s why.First, according to one of the fathers of RL, Rich Sutton,
    policy-gradient methods approximate the gradient of the performance measure, whether
    or not they learn an approximate value function. However, David Silver, one of
    the most prominent figures in DRL, and a former student of Sutton, disagrees.
    He says that policy-based methods don’t additionally learn a value function, only
    actor-critic methods do. But, Sutton further explains that only methods that learn
    the value function using bootstrapping should be called actor-critic, because
    it’s bootstrapping that adds bias to the value function, and thus makes it a “critic.”
    I like this distinction; therefore, REINFORCE and VPG, as presented in this book,
    aren’t considered actor-critic methods. But beware of the lingo, it’s not consistent.
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 一些有先前 DRL 经验的人可能会想，这是所谓的“actor-critic”吗？它正在学习策略和值函数，所以看起来应该是这样的。不幸的是，这是那些“RL
    口音”会混淆新手的那些概念之一。原因如下。首先，根据 RL 的一个创始人 Rich Sutton，策略梯度方法近似性能度量的梯度，无论它们是否学习近似值函数。然而，DRL
    中最杰出的代表人物之一，Sutton 的前学生 David Silver，不同意这种说法。他说，基于策略的方法不会额外学习值函数，只有 actor-critic
    方法才会。但是，Sutton 进一步解释说，只有使用自举学习值函数的方法才能被称为 actor-critic，因为自举会给值函数添加偏差，从而使其成为一个“critic”。我喜欢这种区分；因此，本书中介绍的
    REINFORCE 和 VPG 不被视为 actor-critic 方法。但请注意术语，它并不一致。'
- en: Encouraging exploration
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鼓励探索
- en: Another essential improvement to policy-gradient methods is to add an entropy
    term to the loss function. We can interpret entropy in many different ways, from
    the amount of information one can gain by sampling from a distribution to the
    number of ways one can order a set.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对策略梯度方法的一个基本改进是向损失函数中添加熵项。我们可以从许多不同的角度来解释熵，从从分布中采样可以获得的信息量到对集合进行排序的方式数。
- en: '![](../Images/11_06.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11_06.png)'
- en: The way I like to think of entropy is straightforward. A uniform distribution,
    which has evenly distributed samples, has high entropy, in fact, the highest it
    can be. For instance, if you have two samples, and both can be drawn with a 50%
    chance, then the entropy is the highest it can be for a two-sample set. If you
    have four samples, each with a 25% chance, the entropy is the same, the highest
    it can be for a four-sample set. Conversely, if you have two samples, and one
    has a 100% chance and the other 0%, then the entropy is the lowest it can be,
    which is always zero. In PyTorch, the natural log is used for calculating the
    entropy instead of the binary log. This is mostly because the natural log uses
    Euler’s number, e, and makes math more “natural.” Practically speaking, however,
    there’s no difference and the effects are the same. The entropy in the cart-pole
    environment, which has two actions, is between 0 and 0.6931.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将熵想得很简单。均匀分布，即样本均匀分布，具有高熵，实际上是最高的。例如，如果你有两个样本，并且两者都有 50% 的概率被抽取，那么熵是两个样本集可能达到的最高值。如果你有四个样本，每个样本有
    25% 的概率，熵是相同的，是四个样本集可能达到的最高值。相反，如果你有两个样本，一个有 100% 的概率，另一个有 0%，那么熵是最低的，总是零。在 PyTorch
    中，使用自然对数来计算熵而不是二进制对数。这主要是因为自然对数使用欧拉数 e，使数学更“自然”。然而，从实际的角度来看，这并没有区别，效果是相同的。在有两个动作的
    cart-pole 环境中，熵介于 0 和 0.6931 之间。
- en: The way to use entropy in policy-gradient methods is to add the negative weighted
    entropy to the loss function to encourage having evenly distributed actions. That
    way, a policy with evenly distributed actions, which yields the highest entropy,
    contributes to minimizing the loss. On the other hand, converging to a single
    action, which means entropy is zero, doesn’t reduce the loss. In that case, the
    agent had better converge to the optimal action.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度方法中使用熵的方法是将负加权熵添加到损失函数中，以鼓励动作分布均匀。这样，具有均匀分布动作的策略，即产生最高熵的策略，有助于最小化损失。另一方面，收敛到单个动作，即熵为零，不会减少损失。在这种情况下，智能体最好收敛到最优动作。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathLosses to use for VPG |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 使用 VPG 的数学损失 |'
- en: '|  | ![](../Images/11_06_Sidebar12.png) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/11_06_Sidebar12.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonState-value function neural
    network model |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说 Python状态值函数神经网络模型 |'
- en: '|  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① This is the state-value function neural network. It’s similar to the Q-function
    network we used in the past.② Notice I left handy hyperparameters for you to play
    around with, so go ahead and do so.③ Here we create linear connections between
    the input nodes and the first hidden layer.④ Here we create the connections between
    the hidden layers.⑤ Here we connect the last hidden layer to the output layer,
    which has only one node, representing the value of the state.⑥ This is the forward-pass
    function.⑦ This is formatting the input as we expect it.⑧ Doing a full forward
    pass ...⑨ ... and returning the value of the state. |
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是状态价值函数神经网络。它与过去我们使用的Q函数网络类似。② 注意我为你留下了方便的超参数，你可以随意调整。③ 在这里，我们在输入节点和第一个隐藏层之间创建线性连接。④
    在这里，我们在隐藏层之间创建连接。⑤ 在这里，我们将最后一个隐藏层连接到输出层，输出层只有一个节点，表示状态的价值。⑥ 这是前向传递函数。⑦ 这是按照我们期望的格式化输入。⑧
    进行完整的正向传递...⑨ ...并返回状态的价值。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonVanilla policy gradient a.k.a.
    REINFORCE with baseline |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonVanilla策略梯度，即带有基线的REINFORCE
    |'
- en: '|  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① This is the VPG algorithm. I removed quite a bit of code, so if you want the
    full implementation, head to the chapter’s Notebook.② Very handy way for calculating
    the sum of discounted rewards from time step 0 to T③ I want to emphasize that
    this loop is going through all steps from 0, then 1, 2, 3 all the way to the terminal
    state *T*, and calculating the return from that state, which is the sum of discounted
    rewards from that state at time step t to the terminal state *T*.④ First, calculate
    the value error; then use it to score the log probabilities of the actions. Then,
    discount these to be compatible with the discounted policy gradient. Then, use
    the negative mean.⑤ Calculate the entropy, and add a fraction to the loss.⑥ Now,
    we optimize the policy. Zero the optimizer, do the backward pass, then clip the
    gradients, if desired.⑦ We step the optimizer.⑧ Last, we optimize the value-function
    neural network. |
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是VPG算法。我移除了很多代码，所以如果你想要完整的实现，请查看该章节的笔记本。② 这是一个计算从时间步0到T的折扣奖励总和的非常方便的方法。③
    我想强调的是，这个循环正在遍历所有步骤从0，然后1，2，3一直到最后的状态*T*，并计算从该状态开始的回报，即从时间步t到最终状态*T*的折扣奖励总和。④
    首先，计算价值误差；然后使用它来评分动作的对数概率。然后，将这些概率折扣以与折扣策略梯度兼容。然后，使用负平均值。⑤ 计算熵，并将其添加到损失中。⑥ 现在，我们优化策略。将优化器置零，进行反向传播，然后（如果需要）裁剪梯度。⑦
    我们移动优化器。⑧ 最后，我们优化价值函数神经网络。 |
- en: 'A3C: Parallel policy updates'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A3C：并行策略更新
- en: VPG is a pretty robust method for simple problems; it is, for the most part,
    unbiased because it uses an unbiased target for learning both the policy and value
    function. That is, it uses Monte Carlo returns, which are complete actual returns
    experienced directly in the environment, without any bootstrapping. The only bias
    in the entire algorithm is because we use function approximation, which is inherently
    biased, but since the ANN is only a baseline used to reduce the variance of the
    actual return, little bias is introduced, if any at all.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: VPG对于简单问题来说是一个相当稳健的方法；它大部分是无偏的，因为它使用无偏的目标来学习策略和价值函数。也就是说，它使用蒙特卡洛回报，这是在环境中直接体验到的完整实际回报，没有任何自举。整个算法中唯一的偏差是因为我们使用了函数逼近，这本身是有偏差的，但由于ANN只是用作基线以减少实际回报的方差，因此引入的偏差很小，如果有的话。
- en: However, biased algorithms are necessarily a thing to avoid. Often, to reduce
    variance, we add bias. An algorithm called *asynchronous advantage actor-critic*
    (A3C) does a couple of things to further reduce variance. First, it uses *n*-step
    returns with bootstrapping to learn the policy and value function, and second,
    it uses concurrent actors to generate a broad set of experience samples in parallel.
    Let’s get into the details.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，避免有偏见的算法是必要的。通常，为了减少方差，我们会引入偏差。一种名为**异步优势演员-评论家**（A3C）的算法采取了一些措施来进一步减少方差。首先，它使用带有自举的*n*-步回报来学习策略和价值函数，其次，它使用并发演员并行生成一组广泛的经验样本。让我们深入了解细节。
- en: Using actor-workers
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用演员工作者
- en: One of the main sources of variance in DRL algorithms is how correlated and
    non-stationary online samples are. In value-based methods, we use a replay buffer
    to uniformly sample mini-batches of, for the most part, independent and identically
    distributed data. Unfortunately, using this experience-replay scheme for reducing
    variance is limited to off-policy methods, because on-policy agents cannot reuse
    data generated by previous policies. In other words, every optimization step requires
    a fresh batch of on-policy experiences.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在DRL算法中，方差的主要来源之一是在线样本的相关性和非平稳性。在基于价值的方法中，我们使用重放缓冲区来均匀地采样迷你批次的数据，这些数据大部分是独立且同分布的。不幸的是，使用这种经验重放方案来减少方差仅限于离策略方法，因为在线策略代理不能重用先前策略生成的数据。换句话说，每个优化步骤都需要一个新的在线经验批次。
- en: Instead of using a replay buffer, what we can do in on-policy methods such as
    the policy-gradient algorithms we learn about in this chapter, is have multiple
    workers generating experience in parallel and asynchronously updating the policy
    and value function. Having multiple workers generating experience on multiple
    instances of the environment in parallel decorrelates the data used for training
    and reduces the variance of the algorithm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用重放缓冲区不同，在策略方法中，例如我们在本章中学习的策略梯度算法，我们可以让多个工作员并行生成经验，并异步更新策略和价值函数。让多个工作员在环境的多个实例上并行生成经验，可以解耦用于训练的数据，并减少算法的方差。
- en: '![](../Images/11_07.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11_07.png)'
- en: Asynchronous model updates
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 异步模型更新
- en: '| ![](../Images/icons_Python.png) | I Speak PythonA3C worker logic 1/2 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonA3C工作逻辑 1/2 |'
- en: '|  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① This is the A3C agent.② As usual, these are snippets. You know where to find
    the working code.③ This is the work function each worker loops around in. The
    rank parameter is used as an ID for workers.④ See how we create a unique seed
    per worker. We want diverse experiences.⑤ We create a uniquely seeded environment
    for each worker.⑥ We also use that unique seed for PyTorch, NumPy and Python.⑦
    Handy variables⑧ Here we create a local policy model. See how we initialize its
    weights with the weights of a shared policy network. This network allow us to
    synchronize the agents periodically.⑨ We do the same thing with the value model.
    Notice we don’t need nA for output dimensions.⑩ We start the training loop, until
    the worker is signaled to get out of it.⑪ The first thing is to reset the environment,
    and set the done or is_terminal flag to false.⑫ As you see next, we use *n*-step
    returns for training the policy and value functions.⑬ Let’s continue on the next
    page. |
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是A3C代理。② 如同往常，这些都是代码片段。您知道在哪里可以找到工作代码。③ 这是每个工作员循环的工作函数。rank参数用作工作员的ID。④ 看看我们如何为每个工作员创建一个唯一的种子。我们希望有各种各样的经验。⑤
    我们为每个工作员创建一个唯一种子的环境。⑥ 我们还使用这个唯一的种子为PyTorch、NumPy和Python。⑦ 有用的变量⑧ 在这里，我们创建一个本地策略模型。看看我们如何用共享策略网络的权重初始化它的权重。这个网络允许我们定期同步代理。⑨
    我们对价值模型做同样的事情。注意我们不需要nA作为输出维度。⑩ 我们开始训练循环，直到工作员被信号通知退出。⑪ 首先，重置环境，并将完成或is_terminal标志设置为false。⑫
    正如您接下来看到的，我们使用 *n*-步回报来训练策略和价值函数。⑬ 让我们在下一页继续。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonA3C worker logic 2/2 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonA3C工作逻辑 2/2 |'
- en: '|  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ⑭ I removed eight spaces from the indentation to make it easier to read.⑮ We
    are in the episode loop. The first thing is to collect a step of experience.⑯
    We collect *n*-steps maximum. If we hit a terminal state, we stop there.⑰ We check
    if the time wrapper was triggered or this is a true terminal state.⑱ If it’s a
    failure, then the value of the next state is 0; otherwise, we bootstrap.⑲ Look!
    I’m being sneaky here and appending the next_value to the rewards. By doing this,
    the optimization code from VPG remains largely the same, as you’ll see soon. Make
    sure you see it.⑳ Next we optimize the model. We dig into that function shortly.㉑
    We reset the variables after the optimization step and continue.㉒ And, if the
    state is terminal, of course exit the episode loop.㉓ There is a lot removed. |
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 我从缩进中移除了八个空格，以便更容易阅读。⑮ 我们处于剧集循环中。首先，收集一步的经验。⑯ 我们收集 *n*-步的最大值。如果我们遇到终端状态，我们就停止。⑰
    我们检查是否触发了时间包装器或这是一个真正的终端状态。⑱ 如果是失败，则下一个状态的价值为0；否则，我们进行引导。⑲ 看！我在这里偷偷地将下一个_value附加到奖励上。通过这样做，VPG中的优化代码保持基本不变，您很快就会看到。确保您看到了。⑳
    接下来，我们优化模型。我们稍后会深入研究该函数。㉑ 优化步骤后，我们重置变量并继续。㉒ 当然，如果状态是终端状态，退出剧集循环。㉓ 有很多东西被移除了。|
- en: Using ***n***-step estimates
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 ***n***-步估计
- en: On the previous page, you notice that I append the value of the next state,
    whether terminal or not, to the reward sequence. That means that the reward variable
    contains all rewards from the partial trajectory and the state-value estimate
    of that last state. We can also see this as having the partial return and the
    predicted remaining return in the same place. The partial return is the sequence
    of rewards, and the predicted remaining return is a single-number estimate. The
    only reason why this isn’t a return is that it isn’t a discounted sum, but we
    can take care of that as well.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一页，你注意到我将下一个状态（无论是否为终止状态）的值附加到奖励序列中。这意味着奖励变量包含了部分轨迹中所有奖励以及最后一个状态的状态值估计。我们也可以将其视为在相同位置具有部分回报和预测的剩余回报。部分回报是奖励的序列，预测的剩余回报是一个单一数值估计。唯一的原因是这并不是回报，因为它不是一个折现总和，但我们也可以处理这个问题。
- en: You should realize that this is an *n*-step return, which you learned about
    in chapter 5\. We go out for *n*-steps collecting rewards, and then bootstrap
    after that *n*th state, or before if we land on a terminal state, whichever comes
    first.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该意识到这是一个 *n*-步回报，这是你在第五章中学到的。我们走出 *n*-步收集奖励，然后在第 *n* 个状态之后自举，或者如果我们落在终止状态，则在此之前，哪个先到就先做。
- en: A3C takes advantage of the lower variance of *n*-step returns when compared
    to Monte Carlo returns. We use the value function also to predict the return used
    for updating the policy. You remember that bootstrapping reduces variance, but
    it adds bias. Therefore, we’ve added a critic to our policy-gradient algorithm.
    Welcome to the world of actor-critic methods.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: A3C利用与蒙特卡洛回报相比的 *n*-步回报的较低方差。我们使用价值函数来预测用于更新策略的回报。你还记得自举可以减少方差，但会增加偏差。因此，我们在策略梯度算法中添加了一个评论家。欢迎来到演员-评论家方法的世界。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathUsing *n*-step bootstrapping
    estimates |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学使用 *n*-步自举估计 |'
- en: '|  | ![](../Images/11_07_Sidebar17.png) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/11_07_Sidebar17.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonA3C optimization step 1/2
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonA3C优化步骤 1/2 |'
- en: '|  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① A3C, optimization function② First get the length of the reward. Remember,
    rewards includes the bootstrapping value.③ Next, we calculate all discounts up
    to *n*+1.④ This now is the *n*-step predicted return.⑤ To continue, we need to
    remove the extra elements and format the variables as expected.⑥ Now, we calculate
    the value errors as the predicted return minus the estimated values.⑦ We calculate
    the loss as before.⑧ Notice we now zero the shared policy optimizer, then calculate
    the loss.⑨ Then, clip the gradient magnitude.⑩ Continue on the next page. |
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ① A3C，优化函数② 首先获取奖励的长度。记住，奖励包括自举值。③ 接下来，我们计算到 *n*+1 的所有折扣。④ 现在就是 *n*-步预测回报。⑤
    为了继续，我们需要移除额外的元素，并按预期格式化变量。⑥ 现在，我们计算价值误差，即预测回报减去估计值。⑦ 我们像以前一样计算损失。⑧ 注意我们现在将共享策略优化器归零，然后计算损失。⑨
    然后，剪辑梯度幅度。⑩ 在下一页继续。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonA3C optimization step 2/2
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonA3C优化步骤 2/2 |'
- en: '|  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ⑪ Okay, so check this out. What we’re doing here is iterating over all local
    and shared policy network parameters.⑫ And what we want to do is copy every gradient
    from the local to the shared model.⑬ Once the gradients are copied into the shared
    optimizer, we run an optimization step.⑭ Immediately after, we load the shared
    model into the local model.⑮ Next, we do the same thing but with the state-value
    network. Calculate the loss.⑯ Zero the shared value optimizer.⑰ Backpropagate
    the gradients.⑱ Then, clip them.⑲ Then, copy all the gradients from the local
    model to the shared model.⑳ Step the optimizer.㉑ Finally, load the shared model
    into the local variable. |
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 好的，那么看看这里。我们在这里做的是迭代所有局部和共享策略网络参数。⑫ 我们想要做的是将每个梯度从局部模型复制到共享模型。⑬ 一旦梯度被复制到共享优化器中，我们就运行一个优化步骤。⑭
    立即之后，我们将共享模型加载到局部模型中。⑮ 接下来，我们用状态值网络做同样的事情。计算损失。⑯ 将共享值优化器归零。⑰ 反向传播梯度。⑱ 然后，剪辑它们。⑲
    然后，将所有梯度从局部模型复制到共享模型。⑳ 步进优化器。㉑ 最后，将共享模型加载到局部变量中。 |
- en: Non-blocking model updates
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非阻塞模型更新
- en: One of the most critical aspects of A3C is that its network updates are asynchronous
    and lock-free. Having a shared model creates a tendency for competent software
    engineers to want a blocking mechanism to prevent workers from overwriting other
    updates. Interestingly, A3C uses an update style called a Hogwild!, which is shown
    to not only achieve a near-optimal rate of convergence but also outperform alternative
    schemes that use locking by an order of magnitude.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: A3C最关键的特点之一是其网络更新是异步和无锁的。拥有一个共享模型会使得有能力的软件工程师倾向于想要一个阻塞机制来防止工作者覆盖其他更新。有趣的是，A3C使用了一种名为Hogwild!的更新风格，它不仅实现了接近最优的收敛速率，而且比使用锁定的替代方案快一个数量级。
- en: 'GAE: Robust advantage estimation'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAE：鲁棒的优势估计
- en: 'A3C uses *n*-step returns for reducing the variance of the targets. Still,
    as you probably remember from chapter 5, there’s a more robust method that combines
    multiple *n*-step bootstrapping targets in a single target, creating even more
    robust targets than a single *n*-step: the *λ*-target. *generalized advantage
    estimatio***n** (GAE) is analogous to the *λ*-target in *TD*(*λ*)), but for advantages.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: A3C使用*n*-步回报来减少目标的方差。然而，正如你可能从第5章中记得的那样，还有一种更稳健的方法，它将多个*n*-步自举目标组合成一个单一的目标，从而创建比单个*n*-步更稳健的目标：*λ*-目标。广义优势估计（GAE）在TD(*λ*)中的*λ*-目标类似，但用于优势。
- en: Generalized advantage estimation
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广义优势估计
- en: GAE is not an agent on its own, but a way of estimating targets for the advantage
    function that most actor-critic methods can leverage. More specifically, GAE uses
    an exponentially weighted combination of *n*-step action-advantage function targets,
    the same way the *λ*-target is an exponentially weighted combination of *n*-step
    state-value function targets. This type of target, which we tune in the same way
    as the *λ*-target, can substantially reduce the variance of policy-gradient estimates
    at the cost of some bias.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: GAE本身不是一个代理，而是一种估计优势函数目标的方法，大多数actor-critic方法都可以利用。更具体地说，GAE使用*n*-步动作优势函数目标的指数加权组合，就像*λ*-目标是一组*n*-步状态值函数目标的指数加权组合一样。这种类型的目标，我们以与*λ*-目标相同的方式调整，可以显著减少策略梯度估计的方差，但会牺牲一些偏差。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathPossible policy-gradient
    estimators |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ![数学符号](../Images/icons_Math.png) | 展示数学可能的策略梯度估计器 |'
- en: '|  | ![](../Images/11_07_Sidebar22.png) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/11_07_Sidebar22.png) |'
- en: '| ![](../Images/icons_Math.png) | Show Me The MathGAE is a robust estimate
    of the advantage function |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ![数学符号](../Images/icons_Math.png) | 展示数学GAE是优势函数的鲁棒估计 |'
- en: '|  | ![](../Images/11_07_Sidebar23.png) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/11_07_Sidebar23.png) |'
- en: '| ![](../Images/icons_Math.png) | Show Me The MathPossible value targets |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ![数学符号](../Images/icons_Math.png) | 展示数学GAE的可能值目标 |'
- en: '|  | ![](../Images/11_07_Sidebar24.png) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/11_07_Sidebar24.png) |'
- en: '| 0001 | A Bit Of HistoryIntroduction of the generalized advantage estimations
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍广义优势估计 |'
- en: '|  | John Schulman et al. published a paper in 2015 titled “High-dimensional
    Continuous Control Using Generalized Advantage Estimation,” in which he introduces
    GAE.John is a research scientist at OpenAI, and the lead inventor behind GAE,
    TRPO, and PPO, algorithms that you learn about in the next chapter. In 2018, John
    was recognized by Innovators Under 35 for creating these algorithms, which are
    to this date state of the art. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 约翰·舒尔曼（John Schulman）等人在2015年发表了一篇题为“使用广义优势估计进行高维连续控制”的论文，其中他介绍了GAE。约翰是OpenAI的研究科学家，也是GAE、TRPO和PPO算法的主要发明者，这些算法你将在下一章中学习。2018年，约翰因其创建这些算法而获得35岁以下创新者奖，这些算法至今仍然是业界最先进的。|'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonGAE’s policy optimization
    step |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ![Python符号](../Images/icons_Python.png) | 我会说PythonGAE的策略优化步骤 |'
- en: '|  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '① This is the GAE optimize model logic.② First, we create the discounted returns,
    the way we did with A3C.③ These two lines are creating, first, a NumPy array with
    all the state values, and second, an array with the *(gamma*lambd**a**)^t.* Note,
    lambda is often referred to as tau, too. I’m using that.④ This line creates an
    array of *TD* errors: *R_t + gamma * value_t+1 - value_t, for t=0 to T.*⑤ Here
    we create the GAEs, by multiplying the tau discounts times the *TD* errors.⑥ We
    now use the gaes to calculate the policy loss.⑦ And proceed as usual. |'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是GAE优化模型逻辑。② 首先，我们创建折扣回报，就像我们在A3C中所做的那样。③ 这两行首先创建一个包含所有状态值的NumPy数组，然后创建一个包含
    *(gamma*lambd**a**)^t 的数组。注意，lambda也常被称为tau，我也在使用它。④ 这行创建一个TD误差数组：*R_t + gamma
    * value_t+1 - value_t，对于 t=0 到 T。*⑤ 这里我们通过乘以tau折扣和TD误差来创建GAEs。⑥ 我们现在使用gaes来计算策略损失。⑦
    然后按常规进行。 |
- en: 'A2C: Synchronous policy updates'
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'A2C: 同步策略更新'
- en: In A3C, workers update the neural networks asynchronously. But, asynchronous
    workers may not be what makes A3C such a high-performance algorithm. Advantage
    actor-critic (A2C) is a synchronous version of A3C, which despite the lower numbering
    order, was proposed after A3C and showed to perform comparably to A3C. In this
    section, we explore A2C, along with a few other changes we can apply to policy-gradient
    methods.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在A3C中，工作者异步更新神经网络。但，异步工作者可能并不是使A3C成为一个高性能算法的原因。优势演员-评论家（A2C）是A3C的同步版本，尽管编号较低，但它是A3C之后提出的，并且表现出与A3C相当的性能。在本节中，我们将探讨A2C，以及我们可以应用于策略梯度方法的一些其他改动。
- en: Weight-sharing model
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重共享模型
- en: One change to our current algorithm is to use a single neural network for both
    the policy and the value function. Sharing a model can be particularly beneficial
    when learning from images because feature extraction can be compute-intensive.
    However, model sharing can be challenging due to the potentially different scales
    of the policy and value function updates.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们当前算法的一个改动是使用单个神经网络来处理策略和值函数。当从图像中学习时，共享模型可以特别有益，因为特征提取可能非常计算密集。然而，由于策略和值函数更新的可能不同规模，模型共享可能具有挑战性。
- en: '![](../Images/11_08.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11_08.png)'
- en: Sharing weights between policy and value outputs
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略和值输出之间共享权重
- en: '| ![](../Images/icons_Python.png) | I Speak PythonWeight-sharing actor-critic
    neural network model 1/2 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python共享权重演员-评论家神经网络模型 1/2 |'
- en: '|  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① This is the fully connected actor-critic model.② This is the network instantiation
    process. This is similar to the independent network model.③ Continues ... |
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是一个全连接的演员-评论家模型。② 这是网络实例化过程。这与独立网络模型类似。③ 继续…… |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonWeight-sharing actor-critic
    neural network model 2/2 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python共享权重演员-评论家神经网络模型 2/2 |'
- en: '|  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ④ Okay. Here’s where it’s built, both the value output and the policy output
    connect to the last layer of the hidden layers.⑤ The forward pass starts by reshaping
    the input to match the expected variable type and shape.⑥ And notice how it outputs
    from the policy and a value layers.⑦ This is a handy function to get log probabilities,
    entropies, and other variables at once.⑧ This selects the action or actions for
    the given state or batch of states. |
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 好的。这里就是构建的地方，值输出和政策输出都连接到隐藏层的最后一层。⑤ 前向传递首先通过重塑输入以匹配预期的变量类型和形状开始。⑥ 注意它从策略和值层输出。⑦
    这是一个方便的函数，可以一次性获取对数概率、熵和其他变量。⑧ 这选择给定状态或状态批次的动作或动作。 |
- en: Restoring order in policy updates
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在策略更新中恢复秩序
- en: Updating the neural network in a Hogwild!-style can be chaotic, yet introducing
    a lock mechanism lowers A3C performance considerably. In A2C, we move the workers
    from the agent down to the environment. Instead of having multiple actor-learners,
    we have multiple actors with a single learner. As it turns out, having workers
    rolling out experiences is where the gains are in policy-gradient methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以Hogwild!风格更新神经网络可能会很混乱，但引入锁机制会显著降低A3C的性能。在A2C中，我们将工作者从代理移动到环境。我们不是有多个演员-学习者，而是有多个演员和一个学习者。结果证明，工作者推出经验是策略梯度方法中收益所在的地方。
- en: '![](../Images/11_09.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11_09.png)'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonMulti-process environment
    wrapper 1/2 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python多进程环境包装 1/2 |'
- en: '|  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① This is the multi-process environment class, which creates pipes to communicate
    with the workers, and creates the workers themselves.② Here we create the workers.③
    Here we start them. |
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是我们创建与工作者通信的管道并创建工作者的多进程环境类。② 这里我们创建工作者。③ 这里我们启动它们。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonMulti-process environment
    wrapper 2/2 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python多进程环境包装器2/2'
- en: '|  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ④ Continuation⑤ Workers first create the environment.⑥ Get in this loop listening
    for commands.⑦ Each command calls the respective env function and sends the response
    back to the parent process.⑧ This is the main step function, for instance.⑨ When
    called, it broadcasts the command and arguments to workers.⑩ Workers do their
    part and send back the data, which is collected here.⑪ We automatically reset
    on done.⑫ Last, append and stack the results by observations, rewards, dones,
    infos. |
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 续集⑤ 工作者首先创建环境。⑥ 进入这个循环监听命令。⑦ 每个命令调用相应的env函数并将响应发送回父进程。⑧ 这是主步骤函数，例如。⑨ 当被调用时，它向工作者广播命令和参数。⑩
    工作者完成他们的部分并发送数据，这些数据在这里收集。⑪ 我们在完成时自动重置。⑫ 最后，通过观察、奖励、完成和infos来附加和堆叠结果。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe A2C train logic |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonA2C训练逻辑'
- en: '|  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① This is how we train with the multi-processor environment.② Here, see how
    to create, basically, vectorized environments.③ Here we create a single model.
    This is the actor-critic model with policy and value outputs.④ Look, we reset
    the multi-processor environment and get a stack of states back.⑤ The main thing
    is we work with stacks now.⑥ But, at its core, everything is the same. |
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是我们如何使用多处理器环境进行训练的方式。② 这里，看看如何创建，基本上，向量化的环境。③ 这里我们创建一个单一模型。这是一个具有策略和价值输出的演员-评论员模型。④
    看看，我们重置多处理器环境并获取一系列状态。⑤ 主要的是我们现在处理堆栈。⑥ 但，在其核心，一切都是相同的。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe A2C optimize-model logic
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonA2C优化模型逻辑'
- en: '|  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① This is how we optimize the model in A2C.② The main thing to notice is now
    we work with matrices with vectors of time steps per worker.③ Some operations
    work the same exact way, surprisingly.④ And for some, we just need to add a loop
    to include all workers.⑤ Look how we build a single loss function.⑥ Finally, we
    optimize a single neural network. |
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是我们如何在A2C中优化模型的方式。② 主要要注意的是，我们现在使用的是每个工作者的时间步向量矩阵。③ 一些操作以完全相同的方式进行，令人惊讶。④
    对于某些操作，我们只需要添加一个循环来包含所有工作者。⑤ 看看我们如何构建单个损失函数。⑥ 最后，我们优化单个神经网络。|
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsRunning all policy-gradient
    methods in the CartPole-v1 environment |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| ![细节图标](../Images/icons_Details.png) | 细节之处Running all policy-gradient methods
    in the CartPole-v1 environment'
- en: '|  | To demonstrate the policy-gradient algorithms, and to make comparison
    easier with the value-based methods explored in the previous chapters, I ran experiments
    with the same configurations as in the value-based method experiments. Here are
    the details:REINFORCE:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 为了展示策略梯度算法，并使与之前章节中探索的基于价值的方法的比较更容易，我使用了与基于价值方法实验相同的配置进行了实验。以下是详细信息：REINFORCE：'
- en: Runs a policy network with 4-128-64-2 nodes, Adam optimizer, and lr 0.0007.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行具有4-128-64-2节点的策略网络，使用Adam优化器，学习率lr为0.0007。
- en: Trained at the end of each episode with Monte Carlo returns. No baseline.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个回合结束时使用蒙特卡洛回报进行训练。没有基线。
- en: 'VPG (REINFORCE with Monte Carlo baseline):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: VPG（使用蒙特卡洛基线的REINFORCE）：
- en: Same policy network as REINFORCE, but now we add an entropy term to the loss
    function with 0.001 weight, and clip the gradient norm to 1.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与REINFORCE相同的策略网络，但现在我们在损失函数中添加了一个熵项，权重为0.001，并裁剪梯度范数为1。
- en: We now learn a value function and use it as a baseline, not as a critic. This
    means MC returns are used without bootstrapping and the value function only reduces
    the scale of the returns. The value function is learned with a 4-256-128-1 network,
    RMSprop optimizer, and a 0.001 learning rate. No gradient clippings, though it’s
    possible.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在学习一个价值函数并将其用作基线，而不是作为评论员。这意味着MC回报在没有重新启动的情况下使用，价值函数仅减少回报的规模。价值函数使用4-256-128-1网络，RMSprop优化器，学习率为0.001。虽然没有梯度裁剪，但这是可能的。
- en: 'A3C:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: A3C：
- en: We train the policy and value networks the same exact way.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以完全相同的方式训练策略网络和价值网络。
- en: We now bootstrap the returns every 50 steps maximum (or when landing on a terminal
    state). This is an actor-critic method.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在每50步最多（或当达到终端状态时）重新启动回报。这是一个演员-评论员方法。
- en: We use eight workers each with copies of the networks and doing Hogwild! updates.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用八个工人，每个工人都有网络的副本，并执行Hogwild!更新。
- en: 'GAE:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'GAE:'
- en: Same exact hyperparameter as the rest of the algorithms.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他算法相同的超参数。
- en: Main difference is GAE adds a tau hyperparameter to discount the advantages.
    We use 0.95 for tau here. Notice that the agent style has the same *n*-step bootstrapping
    logic, which might not make this a pure GAE implementation. Usually, you see batches
    of full episodes being processed at once. It still performs pretty well.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要区别在于GAE添加了一个tau超参数来折现优势。我们在这里使用0.95作为tau。请注意，代理风格具有相同的*n*步自举逻辑，这可能不会使这是一个纯粹的GAE实现。通常，你会看到一次处理完整的批次。它仍然表现相当不错。
- en: 'A2C:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'A2C:'
- en: 'A2C does change most of the hyperparameters. To begin with, we have a single
    network: 4-256-128-3 (2 and 1). Train with Adam, lr of 0.002, gradient norm of
    1.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A2C确实改变了大多数超参数。首先，我们有一个单一的网络：4-256-128-3（2和1）。使用Adam进行训练，学习率为0.002，梯度范数为1。
- en: The policy is weighted at 1.0, value function at 0.6, entropy at 0.001.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略权重为1.0，价值函数为0.6，熵为0.001。
- en: We go for 10-step bootstrapping, eight workers, and a 0.95 tau.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们采用10步自举，八个工人，以及0.95的tau。
- en: These algorithms weren’t tuned independently; I’m sure they could do even better.
    |
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法并没有独立调优；我确信它们可以做得更好。|
- en: '| ![](../Images/icons_Tally.png) | Tally it UpPolicy-gradient and actor-critic
    methods on the CartPole-v1 environment |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Tally.png) | CartPole-v1环境上的策略梯度法和演员-评论家方法汇总 |'
- en: '|  | ![](../Images/11_09_Sidebar34.png) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/11_09_Sidebar34.png) |'
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we surveyed policy-gradient and actor-critic methods. First,
    we set up the chapter with a few reasons to consider policy-gradient and actor-critic
    methods. You learned that directly learning a policy is the true objective of
    reinforcement learning methods. You learned that by learning policies, we could
    use stochastic policies, which can have better performance than value-based methods
    in partially observable environments. You learned that even though we typically
    learn stochastic policies, nothing prevents the neural network from learning a
    deterministic policy.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了策略梯度法和演员-评论家方法。首先，我们用几个理由来介绍考虑策略梯度法和演员-评论家方法。你了解到直接学习策略是强化学习方法的真正目标。你了解到通过学习策略，我们可以使用随机策略，这在部分可观察环境中可能比基于价值的方法表现更好。你了解到尽管我们通常学习随机策略，但没有任何东西阻止神经网络学习确定性策略。
- en: You also learned about four algorithms. First, we studied REINFORCE and how
    it’s a straightforward way of improving a policy. In REINFORCE, we could use either
    the full return or the reward-to-go as the score for improving the policy.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了四种算法。首先，我们研究了REINFORCE及其如何是改进策略的直接方法。在REINFORCE中，我们可以使用完整的回报或奖励到去作为改进策略的分数。
- en: You then learned about vanilla policy gradient, also known as REINFORCE with
    baseline. In this algorithm, we learn a value function using Monte Carlo returns
    as targets. Then, we use the value function as a baseline and not as a critic.
    We don’t bootstrap in VPG; instead, we use the reward-to-go, such as in REINFORCE,
    and subtract the learned value function to reduce the variance of the gradient.
    In other words, we use the advantage function as the policy score.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你学习了vanilla策略梯度，也称为带有基线的REINFORCE。在这个算法中，我们使用蒙特卡洛回报作为目标来学习一个价值函数。然后，我们使用价值函数作为基线，而不是作为评论家。在VPG中我们不进行自举；相反，我们使用奖励到去，就像在REINFORCE中一样，并减去学习到的价值函数以减少梯度的方差。换句话说，我们使用优势函数作为策略分数。
- en: We also studied the A3C algorithm. In A3C, we bootstrap the value function,
    both for learning the value function and for scoring the policy. More specifically,
    we use *n*-step returns to improve the models. Additionally, we use multiple actor-learners
    that each roll out the policy, evaluate the returns, and update the policy and
    value models using a Hogwild! approach. In other words, workers update lock-free
    models.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了A3C算法。在A3C中，我们自举价值函数，无论是为了学习价值函数还是为了评分策略。更具体地说，我们使用*n*步回报来改进模型。此外，我们使用多个演员-学习者，每个演员-学习者都执行策略，评估回报，并使用Hogwild!方法更新策略和价值模型。换句话说，工人更新无锁模型。
- en: We then learned about GAE, and how this is a way for estimating advantages analogous
    to *TD*(*λ*)) and the *λ*-return. GAE uses an exponentially weighted mixture of
    all *n*-step advantages for creating a more robust advantage estimate that can
    be easily tuned to use more bootstrapping and therefore bias, or actual returns
    and therefore variance.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着学习了 GAE，以及这是如何估计类似于 *TD*(*λ*) 和 *λ*-回报的优势的方法。GAE 使用所有 *n*-步优势的指数加权混合，以创建一个更稳健的优势估计，可以轻松调整以使用更多的自举和因此偏差，或者实际的回报和因此方差。
- en: Finally, we learned about A2C and how removing the asynchronous part of A3C
    yields a comparable algorithm without the need for implementing custom optimizers.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了 A2C 以及移除 A3C 的异步部分可以得到一个无需实现自定义优化器的相当算法。
- en: By now, you
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Understand the main differences between value-based, policy-based, policy-gradient,
    and actor-critic methods
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于值、基于策略、策略梯度法和演员-评论家方法之间的主要区别
- en: Can implement fundamental policy-gradient and actor-critic methods by yourself
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以自己实现基本的策略梯度法和演员-评论家方法
- en: Can tune policy-gradient and actor-critic algorithms to pass a variety of environments
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以调整策略梯度法和演员-评论家算法以通过各种环境
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tweet.png) | 可分享的工作在自己的工作上努力，并分享你的发现'
- en: '|  | Here are several ideas on how to take what you’ve learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you''ll take advantage of it.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些关于如何将你所学的内容提升到下一个层次的想法。如果你愿意，与世界分享你的结果，并确保查看其他人做了什么。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch11_tf01:** Earlier in this chapter I talked about a fictitious foggy
    lake environment, but hey, it’s only fictitious because you haven’t implemented
    it, right? Go ahead and implement a foggy lake and also a foggy frozen lake environment.
    For this one, make sure the observation passed to the agent is different than
    the actual internal state of the environment. If the agent is in cell 3, for example,
    the internal state is kept secret, and the agent is only able to observe that
    it’s in a foggy cell. In this case, all foggy cells should emit the same observation,
    so the agent can’t tell where it is. After implementing this environment, test
    DRL agents that can only learn deterministic policies (such as in previous chapters),
    and agents that can learn stochastic policies (such as in this chapter). You’ll
    have to do one-hot encoding of the observations to pass into the neural network.
    Create a Python package with the environment, and a Notebook with interesting
    tests and results.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch11_tf01:** 在本章的早期，我提到了一个虚构的雾蒙蒙的湖环境，但嘿，它之所以是虚构的，是因为你还没有实现它，对吧？现在就去实现一个雾蒙蒙的湖环境，以及一个雾蒙蒙的冰冻湖环境。对于这个环境，确保传递给智能体的观察与环境的实际内部状态不同。例如，如果智能体位于单元格
    3，内部状态是保密的，智能体只能观察到它位于一个雾蒙蒙的单元格中。在这种情况下，所有雾蒙蒙的单元格应该发出相同的观察，这样智能体就无法判断自己的位置。实现这个环境后，测试只能学习确定性策略的
    DRL 智能体（如前几章所述），以及可以学习随机策略的智能体（如本章所述）。你将需要对观察进行 one-hot 编码，以便将其传递到神经网络中。创建一个包含环境的
    Python 包，以及一个包含有趣的测试和结果的 Notebook。'
- en: '**#gdrl_ch11_tf02:** In this chapter, we’re still using the CartPole-v1 environment
    as a test bed, but you know swapping environments should be straightforward. First,
    test the same agents in similar environments, such as the LunarLander-v2, or MountainCar-v0\.
    Note what makes it similar is that the observations are low dimensional and continuous,
    and the actions are low dimensional and discrete. Second, test them in different
    environments, high dimensional, or continuous observations or actions.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch11_tf02:** 在本章中，我们仍然使用 CartPole-v1 环境作为测试平台，但你应该知道环境交换应该是简单的。首先，在类似的环境中测试相同的智能体，例如
    LunarLander-v2 或 MountainCar-v0。注意使其相似的是观察是低维连续的，动作是低维离散的。其次，在不同的环境中测试它们，这些环境具有高维观察或动作。'
- en: '**#gdrl_ch11_tf03:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There is no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch11_tf03:** 在每一章中，我都使用最后的标签作为通用的标签。请随意使用这个标签来讨论与本章相关的任何其他内容。没有比你自己创造的任务更令人兴奋的作业了。确保分享你为自己设定的研究内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@提及我 @mimoralea（我会转发），并使用列表中的特定标签来帮助感兴趣的人找到你的结果。没有对错之分；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己更受关注！我们正在等待你的到来！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源列表。查看它在这里
    <link>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的作品。 |
