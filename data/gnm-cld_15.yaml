- en: Chapter 14\. Making a Fully Reproducible Paper
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。制作一个完全可复制的论文
- en: Throughout this book, you’ve been learning how to use an array of individual
    tools and components to perform specific tasks.  You now know everything you need
    to get work done—in small pieces. In this final chapter, we walk you through a
    case study that demonstrates how to bring all of the pieces together into an end-to-end
    analysis, with the added bonus of showcasing methods for ensuring full computational
    reproducibility.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，您已经学习如何使用一系列单独的工具和组件执行特定任务。现在，您已经掌握了完成工作所需的一切——用小片段。在这最后一章中，我们将向您展示一个案例研究，演示如何将所有组件整合到一个端到端分析中，并额外展示确保全面计算再现性的方法。
- en: The challenge posed in the case study is to reproduce a published analysis in
    which researchers identified the contribution of a particular gene to the risk
    for a form of congenital heart disease. The original study was performed on controlled-access
    data, so the first part of the challenge is to generate a synthetic dataset that
    can be substituted for the original. Then, we must re-create the data processing
    and analysis, which include variant discovery, effect prediction, prioritization,
    and clustering based on the information provided in the paper. Finally, we must
    apply these methods to the synthetic dataset and evaluate whether we can successfully
    reproduce the original result. In the course of the chapter, we derive lessons
    from the challenges we face that should guide you in your efforts to make your
    own work computationally reproducible.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 该案例研究中的挑战是复现一项研究，其中研究人员确定了某个基因对特定先天性心脏病风险的贡献。原始研究是在受控数据访问下进行的，因此挑战的第一部分是生成可以替代原始数据的合成数据集。然后，我们必须重新创建数据处理和分析，包括变异发现、效果预测、优先级排序和基于论文提供信息的聚类。最后，我们必须将这些方法应用于合成数据集，并评估我们是否能成功复制原始结果。在本章的过程中，我们从面对的挑战中得出了一些教训，这些教训应该指导您努力使自己的工作在计算上可复制。
- en: Overview of the Case Study
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究概述
- en: We originally conceived this case study as a basis for a couple of workshops
    that we had proposed to deliver at conferences, starting with the general meeting
    of the American Society for Human Genetics (ASHG) in October 2018\. The basic
    premise of our workshop proposal was that we would evaluate the barriers to computational
    reproducibility of genomic methods as they are commonly applied to human medical
    genetics and teach the audience to overcome those barriers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初构思这个案例研究，是为了作为我们提议在会议上进行的一对研讨会的基础，首先是从2018年10月开始的美国人类遗传学会（ASHG）年会。我们研讨会提案的基本前提是，我们将评估在人类医学遗传学中广泛应用的基因组方法的计算再现性障碍，并教会观众如何克服这些障碍。
- en: We started out with certain expectations about some of the key challenges that
    might affect both authors and readers of genomic studies, and we were aware of
    existing solutions for most of those challenges. Our primary goal, therefore,
    was to highlight real occurrences of those known challenges and demonstrate how
    to overcome them in practice, based on approaches and principles recommended by
    experts in the open-science movement. We would then develop educational materials
    with the ultimate goal of popularizing a set of good practices for researchers
    to apply when publishing their own work.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初对可能影响基因组研究的作者和读者的一些关键挑战有一些期望，并且我们知道大多数这些挑战已经存在解决方案。因此，我们的主要目标是突出这些已知挑战的实际发生情况，并演示如何根据开放科学运动中专家推荐的方法和原则实际克服这些挑战。然后，我们将开发教育材料，最终目标是推广一套研究人员在发布自己的工作时应用的良好实践。
- en: Through a series of circumstances that we describe shortly, we selected a study
    about genetic risk factors for a type of congenital heart disease and worked with
    one of its lead authors, Dr. Matthieu J. Miossec, to reproduce the computational
    analysis at the heart of the paper (so to speak). In the course of the project,
    we verified some of our assumptions, but we also encountered obstacles that we
    had not foreseen. As a result, we learned quite a bit more than we had expected
    to, which is not the worst outcome one could imagine.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们不久将描述的一系列情况，我们选择了一项关于一种先天性心脏病的遗传风险因素的研究，并与其主要作者之一，Matthieu J. Miossec博士合作，以复制文中核心的计算分析（可以这么说）。在项目的过程中，我们验证了一些我们的假设，但也遇到了我们没有预料到的障碍。因此，我们学到了比我们预期的要多得多的东西，这并不是我们能想象到的最糟糕的结果。
- en: In this first section of the chapter, we set the stage by discussing the principles
    that guided our decision making, and then we start populating that stage by describing
    the research study that we set out to reproduce. We discuss the challenges that
    we initially identified and describe the logic that we applied to tackle them.
    Finally, we give you an overview of our implementation plans, as a prelude to
    the deep-dive sections in which we’ll examine the nitty-gritty details of each
    phase of the project.^([1](ch14.xhtml#idm45625611443416))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们通过讨论指导我们决策的原则来为舞台设定背景，然后我们开始描述我们试图复现的研究。我们讨论了最初识别的挑战，并描述了我们应用的逻辑来解决它们。最后，我们为你提供了我们实施计划的概述，作为我们深入探讨各个项目阶段细节的序幕。^([1](ch14.xhtml#idm45625611443416))
- en: Computational Reproducibility and the FAIR Framework
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算复现性与FAIR框架
- en: Before we get into the specifics of the analysis that we sought to reproduce,
    it’s worth restating what we mean by *reproducibility* and making sure we distinguish
    it from *replication*. We’ve seen these terms used differently, sometimes even
    swapped for each other, and it’s not clear that any consensus exists on the ultimate
    correct usage. So let’s define them within the scope of this book. If you encounter
    these terms in a different context, we encourage you to verify the author’s intent.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入分析试图复现的分析具体内容之前，值得重申我们所说的*复现性*是什么意思，并确保我们将其与*复制*区分开来。我们看到这些术语有不同的用法，有时甚至可以互换，而且目前并没有明确的共识认为哪种用法是最正确的。因此，让我们在本书的范围内对其进行定义。如果你在不同的上下文中遇到这些术语，请确保了解作者的意图。
- en: We define *reproducibility* with a focus on the repeatability of the analysis
    process and results. When we say we’re reproducing an analysis, we’re trying to
    verify that if we put the same inputs through the same processing, we’ll get the
    same results as we (or someone else) did the first time. This is something we
    need to be able to do in order to build on someone else’s technical work, because
    we typically need to make sure that we’re running their analysis correctly before
    we can begin extending it for our own purposes. As such, it’s an absolutely vital
    accelerator of scientific progress. It’s also essential for training purposes,
    because when we give a learner an exercise, we usually need to ensure that they
    can arrive at the expected result if they follow instructions to the letter—unless
    the point is to demonstrate nondeterministic processes. Hopefully, you’ve found
    the exercises in this book to be reproducible!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义*复现性*，侧重于分析过程和结果的可重复性。当我们说我们正在复现分析时，我们试图验证如果我们通过相同的处理输入相同的输入，我们会得到与我们（或其他人）第一次得到的相同结果。这是我们在扩展他人技术工作之前通常需要做的事情，因为我们通常需要确保我们正确运行他们的分析。因此，这是科学进步的绝对重要加速器。对于培训目的来说也是至关重要的，因为当我们给学习者一个练习时，我们通常需要确保他们严格按照说明可以得到预期的结果——除非目的是展示非确定性过程。希望你在本书中找到的练习是可以复现的！
- en: Replication, on the other hand, is all about confirming the insights derived
    from experimental results (with apologies to Karl Popper, legendary contrarian).
    To replicate the findings of a study, we typically want to apply different approaches
    that are not likely to be subject to the same weaknesses or artifacts, to avoid
    simply falling in the same traps. Ideally, we’ll also want to examine data that
    was independently collected, to avoid confirming any biases originating at that
    stage. If the results still lead us to draw the same conclusions, we can say we’ve
    replicated the findings of the original work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*复制*则完全是关于确认实验结果所得的见解（向传奇的反对者卡尔·波普致歉）。要复制研究的发现，我们通常希望采用不太可能受到相同弱点或人为因素影响的不同方法，以避免简单地陷入同样的陷阱。理想情况下，我们还希望检查独立收集的数据，以避免确认从那个阶段起源的任何偏见。如果结果仍然导致我们得出相同的结论，我们可以说我们复制了原始工作的发现。
- en: This difference, illustrated in [Figure 14-1](#reproducibility_of_an_analysis_versus_r),
    comes down to *plumbing* versus *truth*. In one case, we are trying to verify
    that “the thing runs as expected,” and in the other, “yes, this is how this small
    part of nature works.”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异，如图[14-1](#reproducibility_of_an_analysis_versus_r)所示，归结为*管道*与*真相*。在一个案例中，我们试图验证“事情是否按预期运行”，而在另一个案例中，“是的，这就是自然的这一小部分运作方式”。
- en: '![Reproducibility of an analysis versus replicability of study findings.](Images/gitc_1401.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![分析的可重现性与研究结果的可复制性。](Images/gitc_1401.png)'
- en: Figure 14-1\. Reproducibility of an analysis versus replicability of study findings.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1\. 分析的可重现性与研究结果的可复制性。
- en: Those definitions shouldn’t sound too outlandish given that we’ve already mentioned
    this core concept of reproducibility in earlier chapters. For example, in [Chapter 8](ch08.xhtml#automating_analysis_execution_with_work),
    when we introduced workflows, we noted their great value as a way to encode the
    instructions for performing a complex analysis in a systematic, automatable way.
    We also touched on similar themes when we explored Jupyter Notebook as a more
    flexible, interactive approach to packaging analysis code. We’re confident that
    you won’t be surprised when both workflows and notebooks make an appearance later
    in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们在早期章节已经提到了可重现性的核心概念，这些定义不应该听起来太离奇。例如，在[第8章](ch08.xhtml#automating_analysis_execution_with_work)中，当我们介绍工作流程时，我们指出它们作为一种将复杂分析操作编码为系统化、可自动化方式的重要价值。当我们探索Jupyter
    Notebook作为一种更灵活、交互式的打包分析代码方法时，我们也触及了类似的主题。我们确信，当这两种工具在本章后期出现时，你不会感到惊讶。
- en: 'However, reproducibility is only one facet of the kaleidoscope of open science.
    As we set out to tackle this case study, we made a deliberate decision to look
    at it through the lens of the FAIR framework. As we mentioned briefly in [Chapter 1](ch01.xhtml#introduction),
    *FAIR* is an acronym that stands for findability, accessibility, interoperability,
    and reusability. It refers to a framework for evaluating the openness of research
    assets and services based on those four characteristics. The underlying idea is
    simple enough: all four pillars are requirements that must be satisfied for an
    asset to be considered *open* (as in *open science*). For example, to consider
    that an analysis is open, it is not enough for the code to be technically reusable
    and interoperable with other computing tools; other researchers should also be
    able to find the code and obtain a complete working copy. The [original publication
    of the FAIR principles](https://oreil.ly/JyTlX) highlighted the necessity of applying
    them to scientific data management purposes, but it also made clear that they
    could be applied to a variety of other types of assets or *digital research objects*
    such as code and tools.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可重现性只是开放科学万花筒中的一个方面。当我们着手处理这个案例研究时，我们做出了有意义的决定，通过FAIR框架来审视它。正如我们在[第一章](ch01.xhtml#introduction)中简要提到的那样，*FAIR*是一个缩写，代表着可发现性、可访问性、互操作性和可重复使用性。它指的是一种评估研究资产和服务开放性的框架，基于这四个特征。其基本思想很简单：所有四个支柱都是必须满足的要求，才能将一个资产视为*开放*（如*开放科学*）。例如，要认为分析是开放的，仅仅代码在技术上可重用并且与其他计算工具互操作是不够的；其他研究人员还应该能够找到这段代码并获取一个完整的工作副本。FAIR原则的[原始发布](https://oreil.ly/JyTlX)强调了将其应用于科学数据管理目的的必要性，但也明确指出它们可以应用于各种其他类型的资产或*数字研究对象*，例如代码和工具。
- en: Although the bulk of the case study is focused on reproducibility, which we
    take to be mostly synonymous with reusability as spelled out in the FAIR framework,
    the three other FAIR characteristics will be reflected in our various implementation
    decisions. We’ll circle back to this when we go over the methodology that we chose
    to follow, then again at the conclusion of the chapter when we discuss the final
    outcomes of this work. For now, it’s time to take a look at the original research
    study that we chose to showcase in this project.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然案例研究的大部分内容都集中在可重现性上，我们认为这与FAIR框架中的可重用性基本上是同义词，但在我们的各种实施决策中也会反映出其他三个FAIR特征。当我们回顾我们选择遵循的方法论时，我们会再次回到这一点，在本章结束时，我们会讨论这项工作的最终结果。现在是时候看一看我们选择在这个项目中展示的原始研究了。
- en: Original Research Study and History of the Case Study
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始研究和案例研究的历史
- en: The original study was authored by Drs. Donna J. Page, Matthieu J. Miossec,
    et al., who set out to identify genetic components associated with a form of congenital
    heart disease called nonsyndromic [tetralogy of Fallot](https://oreil.ly/XBv-Q)
    by analyzing exome sequencing data from 829 cases and 1,252 controls collected
    from multiple research centers.^([2](ch14.xhtml#idm45625611406072)) We go over
    the analysis in more detail further down, but to summarize for now, they first
    applied variant discovery methods based on the GATK Best Practices to call variants
    across the full set of samples (including both cases and controls), and then they
    used functional effect prediction to identify probable deleterious variants. Finally,
    they ran a variant load analysis to identify genes more frequently affected by
    deleterious variants in case samples compared to the controls.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的研究由Donna J. Page博士、Matthieu J. Miossec博士等撰写，旨在通过分析来自多个研究中心的829例病例和1252例对照的外显子组测序数据，识别与非综合征性[Fallot四联症](https://oreil.ly/XBv-Q)相关的遗传组分。我们将在稍后更详细地介绍分析，但现在先概述一下，他们首先应用了基于GATK最佳实践的变异发现方法，在全样本集（包括病例和对照）中调用变异体，并使用功能效应预测识别可能的有害变异体。最后，他们进行了变异负荷分析，以识别在病例样本中与对照相比更频繁受有害变异体影响的基因。
- en: As a result of this analysis, the authors identified 49 deleterious variants
    within the *NOTCH1* gene that appeared to associate with the tetralogy of Fallot
    congenital heart disease. Others had previously identified *NOTCH1* variants in
    families with congenital heart defects, including tetralogy of Fallot, so this
    was not a wholly unexpected result. However, this work was the first to scale
    variant analysis of tetralogy of Fallot to a cohort of nearly a thousand case
    samples, and to show that *NOTCH1* is a significant contributor to tetralogy of
    Fallot risk. As of this writing, it is still the largest exome study of non-syndromic
    tetralogy of Fallot that we are aware of.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这项分析，作者们发现了49个在*NOTCH1*基因内的有害变异体，这些变异体似乎与Fallot四联症先天性心脏病相关。此前其他人在患有先天性心脏病，包括Fallot四联症的家庭中已经发现了*NOTCH1*变异体，因此这并非完全意料之外的结果。然而，这项工作是首次将Fallot四联症的变异分析扩展到近千例病例样本的研究，并显示*NOTCH1*是Fallot四联症风险的重要贡献者。截至目前，这仍然是我们所知道的最大的非综合征性Fallot四联症外显子研究。
- en: 'We connected with Dr. Miossec about a workshop on genomic analysis that we
    were developing for a joint conference of several societies in Latin America,
    ISCB-LA SOIBIO EMBnet, to be held in Chile in November in 2018, while the manuscript
    was still at the preprint stage. At time, our team had also committed to developing
    a case study on computational reproducibility for the ASHG 2018 workshop, mentioned
    earlier, which was scheduled for October. Serendipitously, Dr. Miossec’s preprint
    was a great fit for both purposes because it was typical of what our intended
    audience would find relatable: (1) a classic use case for variant discovery and
    association methods, (2) performed in a cohort of participants that was large
    enough to pose some scaling challenges but small enough to be within the means
    of most research groups, and (3) using exome sequencing data as was most common
    at the time. Given the [Goldilocks-like “just right”](https://oreil.ly/XDwv5)
    character of the study itself and Dr. Miossec’s collaborative attitude during
    our initial discussions of the ISCB-LA SOIBIO EMBnet workshop development project,
    we approached him about building a case study from his preprint as basis for both
    workshops. We then worked together over the following months to develop the case
    study that we eventually delivered at the ASHG 2018 meeting and at the ISCB-LA
    SOIBIO EMBnet 2018 conference, respectively.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与Miossec博士讨论了我们正在为拉丁美洲几个学会的联合会议ISCB-LA SOIBIO EMBnet开发的基因组分析研讨会，该会议定于2018年11月在智利举行，此时手稿仍处于预印本阶段。同时，我们的团队还承诺为即将于2018年10月举行的ASHG
    2018研讨会开发关于计算再现性的案例研究，正如前文提到的。巧合的是，Miossec博士的预印本非常适合这两个目的，因为它典型地符合我们预期观众的需求：（1）变异发现和关联方法的经典应用案例，（2）在足够大以引起一些扩展挑战但又足够小以在大多数研究团体能力范围内的参与者队列中进行，（3）使用当时最常见的外显子组测序数据。鉴于该研究本身具有“正好合适”的特点，以及在我们最初讨论ISCB-LA
    SOIBIO EMBnet研讨会开发项目时Miossec博士的合作态度，我们便与他商议以他的预印本为基础开发案例研究。接下来的几个月里，我们共同努力开发了案例研究，并最终分别在ASHG
    2018会议和ISCB-LA SOIBIO EMBnet 2018年会上进行了展示。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To be clear, the selection of this study is not intended to demonstrate GATK
    Best Practices; technically, the authors’ original implementation shows deviations
    from the canonical Best Practices. In addition, this does not constitute an endorsement
    of the biological validity of the study. In this case study, we are focused solely
    on the question of computational reproducibility of the analysis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 明确一点，选择这项研究并不旨在展示GATK最佳实践；从技术上讲，作者的原始实现与经典最佳实践有所偏离。此外，这并不意味着对研究的生物学有效性进行认可。在本案例研究中，我们仅关注分析的计算再现性问题。
- en: Assessing the Available Information and Key Challenges
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估可获得信息和关键挑战
- en: Before we even had the Tetralogy of Fallot study selected, we had been reviewing
    the key challenges that we expected to face based on our experience with scientific
    publishing. As illustrated in [Figure 14-2](#typical_asymmetry_in_the_availability_o),
    a fundamental asymmetry exists between the information and means available to
    the author of a scientific paper compared to what is available to the reader.
    As an author, you generally have full access to an original dataset, which in
    the case of human genomic data is almost always locked down by data access and
    data-use restrictions. You have your computing environment, which might be customized
    with a variety of hardware and software components, and you have tools or code
    that you apply to the data within that environment to generate results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们甚至选择缺血性心脏病的四联症研究之前，我们已经根据我们在科学出版方面的经验回顾了我们预计会面临的关键挑战。如[图 14-2](#typical_asymmetry_in_the_availability_o)所示，科学论文作者和读者之间存在着信息和手段的根本不对称。作为作者，您通常可以完全访问原始数据集，而在人类基因组数据的情况下，这些数据几乎总是受到数据访问和数据使用限制的约束。您拥有您的计算环境，可能通过各种硬件和软件组件进行定制，并且您有工具或代码，可以在该环境中应用于数据以生成结果。
- en: '![Typical asymmetry in the availability of information between author and reader.](Images/gitc_1402.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![作者与读者之间信息可获得性的典型不对称性。](Images/gitc_1402.png)'
- en: Figure 14-2\. Typical asymmetry in the availability of information between author
    and reader.
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-2\. 作者与读者之间信息可获得性的典型不对称性。
- en: On the other side, however, when you’re a reader, you usually first see the
    preprint or published paper, which is in itself a highly processed, curated, and,
    to some extent, censored view of the author’s original results. From the paper,
    you must walk backward up the stream to find a description of the methods that
    were applied, which are often incomplete, as in the dreaded “We called variants
    with GATK Best Practices as described in the online documentation.” (No indication
    of version, specific tools, nada.) If you’re lucky, an unformatted PDF buried
    in the supplemental materials includes a link to code or scripts in a GitHub repository,
    which might or might not be documented. If you’re extremely lucky, you might even
    find a reference to a Docker container image that encapsulates the software environment
    requirements, or at least a Dockerfile to generate one. Finally, you might find
    that the data is subject to access restrictions that could prove unsurmountable.
    You might be able to cobble together the analysis based on the available information,
    code, and software resources in the paper—which, to be fair, can sometimes be
    quite well presented—but without the original data, you have no way to know whether
    it’s running as you expect. So when you go to apply it to your own data, it can
    be difficult to know how much you can trust the results it produces.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当您是读者时，通常首先看到的是预印本或已发表的论文，这本身就是作者原始结果的高度加工、筛选并在某种程度上审查过的视角。从论文中，您必须向后追溯以找到应用的方法描述，这些描述通常是不完整的，就像可怕的“我们按照在线文档中描述的GATK最佳实践进行变异调用。”（没有版本、特定工具的指示。）如果幸运的话，在补充材料中埋藏的未格式化的PDF中可能包含到GitHub仓库中代码或脚本的链接，这些可能或可能没有文档化。如果非常幸运，您甚至可能会找到提供软件环境需求的Docker容器映像的参考，或者至少是用于生成此类映像的Dockerfile。最后，您可能会发现数据受到无法克服的访问限制。您可能能够根据可用信息、代码和论文中的软件资源来拼凑分析过程，公平地说，这有时可能会被呈现得相当完善，但是没有原始数据，您无法知道结果是否符合预期。因此，当您将其应用于自己的数据时，很难知道可以信任其产生的结果有多少。
- en: Now that we’ve painted such a grim picture, how did our situation measure up?
    Well, the preprint that was available at the time for the Tetralogy of Fallot
    paper fell squarely within the norm. [Figure 14-3](#summary_of_the_information_provided_in)
    summarizes the methodological information it contained.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们描绘了这样一个悲观的图景，我们的情况如何？嗯，在Tetralogy of Fallot论文的那个时候，预印本完全符合标准。[Figure 14-3](#summary_of_the_information_provided_in)总结了其中包含的方法论信息。
- en: '![Summary of the information provided in the original preprint of the Tetralogy
    of Fallot paper.](Images/gitc_1403.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Tetralogy of Fallot论文原始预印本中提供的信息总结。](Images/gitc_1403.png)'
- en: Figure 14-3\. Summary of the information provided in the original preprint of
    the Tetralogy of Fallot paper.
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-3。总结了Tetralogy of Fallot论文原始预印本中提供的信息。
- en: The description of the methods used in the early stages of data processing (alignment
    up to variant calling) referenced a third-party pipeline operated at another institution,
    which was definitely too vague to be sufficient to reproduce the work exactly.
    On the bright side, it did reference key tools and their versions, such as `HaplotypeCaller`
    from GATK 3.2, and we knew enough about those tools to figure out their requirements
    by ourselves. For later parts of the analysis, the method descriptions were more
    detailed and pointed to scripts in Bash and Perl code, but these were still not
    entirely sufficient by themselves.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据处理早期阶段使用的方法描述（从比对到变异调用）引用了另一家机构操作的第三方流程，这显然过于模糊，无法确保准确复制工作。但积极的一面是，它确实提到了关键工具及其版本，如来自GATK
    3.2的`HaplotypeCaller`，我们对这些工具有足够的了解，可以自行了解其要求。对于分析的后续部分，方法描述更为详细，并指向Bash脚本和Perl代码，但这些仍然无法完全独立实现。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Spoiler alert: we benefited immensely from having a lead author in our corner
    with detailed knowledge of how the analysis was done; we would not have been successful
    without direct help from Dr. Miossec.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 剧透警告：我们极大地受益于我们的领头作者具有详细的分析方法知识；没有Dr. Miossec的直接帮助，我们是不可能成功的。
- en: 'Where we hit the wall, however, was entirely predictable: we knew early on
    that the data was restricted-access, and as it turned out, we were not able to
    gain access to any of it at any point, even for development purposes. But hang
    on to your hat; that doesn’t mean the project was dead. We get to solutions in
    a few minutes.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们遇到的困难完全可预见：我们很早就知道数据是受限访问的，并且事实证明，我们在任何时候都无法访问任何数据，甚至是为了开发目的。但不要泄气；这并不意味着项目已经失败。我们在几分钟内就能找到解决方案。
- en: Designing a Reproducible Implementation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计可重复实施方案
- en: 'Given these initial conditions, we began thinking about how we were going to
    tackle the project in practice. We wanted to use a methodology that would allow
    us not only to reproduce the analysis ourselves, but also to package the resulting
    implementation for others to reproduce. To that end, we followed the guidance
    provided by Justin Kitzes et al. in *The Practice of Reproducible Research* (University
    of California Press, 2018). You can learn a lot from their book that we won’t
    cover here, so we recommend that you check it out. One of the takeaways we quickly
    implemented was to break down the overall work based on the following questions:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了这些初始条件后，我们开始思考如何在实践中解决这个项目。我们希望使用一种方法论，不仅能够自己复制分析，还能将结果实施打包供他人复制。为此，我们遵循了贾斯汀·基茨等人在《可重复研究的实践》（加州大学出版社，2018年）中提供的指导。你可以从他们的书中学到很多内容，这里我们无法一一涵盖，建议你去查阅。我们迅速实施的一个收获是根据以下问题分解整体工作：
- en: What is the *input* data, and how are we going to make that accessible to others?
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是*输入*数据，我们如何使其对他人可访问？
- en: What (a) *processing* and (b) *analysis* methods need to be applied to the data,
    and how can we make them portable?
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据需要应用哪些（a）*处理*和（b）*分析*方法，并且如何使它们可移植？
- en: How can we bundle all the assets we produce and *share* them for others to use?
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何打包我们生成的所有资源并*分享*给他人使用？
- en: This was a simple but effective framework that reminded us of our priorities
    and guided our decisions. The distinction between processing and analysis methods
    was particularly useful after we agreed what those terms should mean. *Processing*
    would refer to all the up-front data transformations, cleaning up, format changes,
    and so on that are always the same and are typically automated to be run in bulk.
    This corresponds roughly to what is often described as *secondary analysis*. Meanwhile,
    *analysis* proper would refer to the more changing, context-dependent activities
    like modeling and clustering that are typically done interactively, on a case-by-case
    basis. This corresponds to *tertiary analysis*. There was some gray area regarding
    variant calling and effect prediction, but we ultimately decided that variant
    calling should fall into the processing bucket, whereas variant effect prediction
    would go into the analysis bucket.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单而有效的框架，提醒我们我们的优先事项并指导我们的决策。在我们达成一致意见后，处理和分析方法之间的区别尤其有用。*处理* 将指所有前期数据转换、清理、格式更改等操作，通常都是相同的，通常是批量自动运行的。这大致对应于通常描述的*次级分析*。与此同时，*分析*
    则指更改多、依赖于上下文的活动，如建模和聚类，通常是交互式地、基于具体案例进行的。这对应于*三级分析*。在变异调用和效应预测方面存在一些灰色地带，但我们最终决定变异调用应属于处理类别，而变异效应预测则属于分析类别。
- en: Although those distinctions were somewhat arbitrary in nature, they were important
    to us because we had decided that they would guide our efforts regarding how closely
    we would seek to adhere to the original study. Obviously, we wanted to be as close
    to the original as possible, but we knew by then that some of the information
    was incomplete. It was clear that we would need to make compromises to strike
    a balance between achieving perfection and the amount of effort that we could
    afford to devote to the project, which was sadly not infinite. So we made a few
    assumptions. We posited that (1) basic data processing like alignment and variant
    calling should be fairly robust to adaptations as long as it is applied the same
    way to all of the data, avoiding batch effects, and that (2) the effect prediction
    and variant load analysis at the core of the study were more critical to reproduce
    exactly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些区别在某种程度上是任意的，但对我们来说很重要，因为我们已经决定它们将指导我们关于我们将多么密切地遵循原始研究的努力。显然，我们希望尽可能接近原来的情况，但那时我们已经知道一些信息是不完整的。显然，我们需要做出妥协来在追求完美和我们能够投入项目的努力量之间达到平衡，这显然是无限的。所以我们做了一些假设。我们假设（1）基本数据处理如对齐和变异调用应该对适应具有相当的鲁棒性，只要它以相同的方式应用于所有数据，避免批次效应，并且（2）研究核心的效应预测和变异负荷分析更关键，需要精确复制。
- en: 'With these guiding principles in mind, we outlined the following implementation
    strategy:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 谨记这些指导原则，我们概述了以下实施策略：
- en: '*Data input:* To deal with the locked-down data, we decided to generate synthetic
    exome data that would emulate the properties of the original dataset, down to
    the variants of interest. The synthetic data would be completely open and therefore
    satisfy the *Accessibility* and *Reusability* requirements of the FAIR framework.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据输入：* 为了处理被封锁的数据，我们决定生成合成外显子数据，模拟原始数据集的属性，直到感兴趣的变体。合成数据将完全开放，因此满足了FAIR框架对*可访问性*和*可重用性*的要求。'
- en: Data processing and analysis
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据处理与分析
- en: '*Processing:* Because the synthetic data would be fully aligned already, as
    we discuss shortly, we would need to implement only variant calling in this phase.
    Based on what we knew of the original pipeline, we decided to use a combination
    of existing and new workflows written in WDL. The open and portable nature of
    WDL would make the processing *accessible*, *interoperable*, and *reusable* from
    the perspective of FAIR.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*处理：* 由于合成数据已经完全对齐，正如我们马上要讨论的那样，我们只需要在这个阶段实现变异调用。根据我们对原始流程的了解，我们决定使用一套组合现有和新的用WDL编写的工作流。WDL的开放和便携性将使处理从FAIR的角度看起来是*可访问的*，*可互操作的*和*可重用的*。'
- en: '*Analysis:* With Dr. Miossec’s help, we decided to reimplement his original
    scripts in two parts: the prediction of variant effects as a workflow in WDL,
    and the variant load analysis as R code in a Jupyter notebook. As with WDL workflows,
    using Jupyter notebooks would make the analysis *accessible*, *interoperable*,
    and *reusable* from the perspective of FAIR.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分析：* 在 Miossec 博士的帮助下，我们决定分两部分重新实施他的原始脚本：作为 WDL 工作流的变体效果预测，以及作为 Jupyter 笔记本中的
    R 代码的变体负荷分析。与 WDL 工作流一样，使用 Jupyter 笔记本将使分析从 FAIR 的角度变得*可访问*、*可互操作*和*可重用*。'
- en: '*Sharing:* We planned to do all the work in a Terra workspace, which we could
    then share with the community to provide full access to data, WDL workflows, and
    Jupyter notebooks. This easy sharing in Terra would help us round out the FAIR
    sweep by making our case study *findable* and *accessible* from the perspective
    of FAIR.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分享：* 我们计划在 Terra 工作空间中完成所有工作，然后与社区共享，以便完全访问数据、WDL 工作流和 Jupyter 笔记本。在 Terra
    中轻松共享将有助于我们通过使我们的案例研究从 FAIR 的角度变得*可找到*和*可访问*。'
- en: 'That last point was a nice bonus of doing the original work for this case study
    in Terra: we knew that after we were done, we could easily clone the development
    workspace to make a [public workspace](https://oreil.ly/yj2ql) that would contain
    all of the workflows, notebook, and references to data. As a result, we would
    have only minimal work to do to turn our private development environment into
    a fully reproducible methods supplement.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 做这个案例研究的原始工作的最后一个亮点是在 Terra 中完成：我们知道完成后，可以轻松克隆开发工作空间，创建一个[公共工作空间](https://oreil.ly/yj2ql)，其中包含所有工作流、笔记本和数据引用。因此，我们只需做最少的工作，即可将我们的私人开发环境转变为完全可复制的方法补充。
- en: In the next two sections, we explain how we implemented all of this in practice.
    At each stage, we point you to the specific elements of code and data that we
    developed in the workspace so that you can see exactly what we’re talking about.
    We’re not going to have you run anything as part of this chapter, but feel free
    to clone the workspace and try out the various workflows and notebook anyway.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，我们将解释我们如何在实践中实现所有这些。在每个阶段，我们都会指向工作空间中开发的具体代码和数据元素，以便您能够清楚地了解我们在谈论什么。本章节不要求您运行任何内容，但请随时克隆工作空间并尝试各种工作流和笔记本。
- en: 'Now, are you ready to dive into the details? Saddle up and fasten your seatbelt,
    because we’re going to start with what turned out to be the most challenging part:
    creating the synthetic dataset.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您准备好深入了解细节了吗？做好准备，系好安全带，因为我们将从事最具挑战性的部分开始：创建合成数据集。
- en: Generating a Synthetic Dataset as a Stand-In for the Private Data
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成合成数据集作为私人数据的替代品
- en: The original analysis involved 829 cases and 1,252 controls, all produced using
    exome sequencing, and, unfortunately for us, all subject to access restrictions
    that we could not readily overcome. Indeed, even if we had been able to access
    the data, we would definitely not have been able to distribute it with the educational
    resources that we planned to develop. It would still have been useful to have
    the data available for testing, but in any case, we were going to need to resort
    to synthetic data to completely achieve our goals.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 原始分析涉及 829 个病例和 1,252 个对照，全部使用外显子测序生成，不幸的是，所有这些数据都受到我们无法轻易克服的访问限制。事实上，即使我们能够访问数据，我们肯定也无法将其与我们计划开发的教育资源一同分发。虽然仍然有用将数据用于测试，但无论如何，我们都将需要借助合成数据来完全实现我们的目标。
- en: 'But wait...what does that mean exactly, you ask? In general, that can mean
    a lot of things, so let’s clarify that here, we’re specifically referring to *synthetic
    sequence data*: a dataset made entirely of read sequences generated by a computer
    program. The idea was to create such a synthetic dataset that would mimic the
    original data, as illustrated in [Figure 14-4](#replacing_a_real_dataset_that_cannot_be),
    including the presence of the variants of interest in specific proportions, so
    that we could swap it in and still achieve our goal of reproducing the analysis
    despite lacking access to the original data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等……你问，这到底意味着什么？一般来说，这可能意味着很多事情，因此让我们在这里澄清一下，我们具体指的是*合成序列数据*：这是完全由计算机程序生成的读取序列组成的数据集。我们的想法是创建这样一个合成数据集，模仿原始数据，正如
    [图 14-4](#replacing_a_real_dataset_that_cannot_be) 所示，包括特定比例的感兴趣变体的存在，以便我们可以替换它并仍然实现我们复制分析的目标，尽管我们无法访问原始数据。
- en: '![Replacing a real dataset that cannot be distributed with a synthetic dataset
    that mimics the original data’s characteristics.](Images/gitc_1404.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![用合成数据集替换不能分发的真实数据集，以模仿原始数据的特征。](Images/gitc_1404.png)'
- en: Figure 14-4\. Replacing a real dataset that cannot be distributed with a synthetic
    dataset that mimics the original data’s characteristics.
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-4\. 用合成数据集替换不能分发的真实数据集，以模仿原始数据的特征。
- en: Let’s talk about how that works in practice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈实际操作中的工作原理。
- en: Overall Methodology
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总体方法论
- en: The idea of using synthetic genomic data is not new, far from it; people have
    been using synthetic data for some time. For example, the [ICGC-TCGA DREAM Mutation
    challenges](https://oreil.ly/DMj28) were a series of recurring competitions in
    which organizers provided synthetic data that had been engineered to carry specific
    known variants, and participants were challenged to develop analysis methods that
    could identify those mutations with a high degree of accuracy and specificity.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合成基因组数据的概念并不新鲜，人们已经有一段时间在使用合成数据。例如，[ICGC-TCGA DREAM突变挑战](https://oreil.ly/DMj28)是一系列重复的竞赛，组织者在其中提供了经过工程化的合成数据，这些数据已知带有特定变异，参与者的挑战是开发能够高度准确和具体识别这些突变的分析方法。
- en: '[Multiple programs](https://oreil.ly/LjSJe) can generate synthetic sequence
    data for purposes like this; in fact, several were developed in part to enable
    those challenges. The basic mechanism is to simulate reads based on the sequence
    of the reference genome and output these as standard FASTQ or BAM files. The tools
    typically accept a VCF of variant calls as secondary input, which modifies the
    data simulation algorithm in such a way that the resulting sequence data supports
    the variants contained in the input VCF. There are also programs that can introduce
    (or *spike in*) variants into sequence data that already exists. You provide a
    set of variants to the tool, which then modifies a subset of reads to make them
    support the desired variant calls.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[多程序](https://oreil.ly/LjSJe)可以生成合成序列数据，用于此类目的；实际上，有几个程序部分开发出来就是为了支持这些挑战。基本机制是基于参考基因组的序列模拟读取，并将其输出为标准的FASTQ或BAM文件。这些工具通常接受一个VCF变异调用文件作为辅助输入，这会修改数据模拟算法，使得生成的序列数据支持输入VCF中包含的变异。还有一些程序可以将变异引入（或*spike
    in*）已有的序列数据中。你向工具提供一组变异，然后工具修改部分读取以支持所需的变异调用。'
- en: In fact, in an early round of brainstorming, we had considered simply editing
    real samples from the 1000 Genomes Project to carry the variants of interest,
    and have the rest of the dataset stand in as controls. This would allow us to
    avoid having to generate actual synthetic data. However, our preliminary testing
    showed that the exome data from the Low Coverage dataset was not of good-enough
    quality to serve our purposes. At the time, the new High Coverage dataset introduced
    in [Chapter 13](ch13.xhtml#assembling_your_own_workspace_in_terra) was not yet
    available, and because that data was generated using WGS, not exome sequencing,
    it would not have been appropriate for us to use anyway.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在早期的头脑风暴阶段，我们曾考虑简单地编辑来自1000基因组计划的真实样本，使其携带感兴趣的变异，并且让数据集的其余部分充当对照。这将使我们避免生成真实的合成数据。然而，我们的初步测试显示，来自低覆盖数据集的外显子数据不具备足够好的质量来满足我们的目的。当时，尚未推出在[第13章](ch13.xhtml#assembling_your_own_workspace_in_terra)中介绍的新高覆盖数据集，而且因为该数据是使用WGS生成的，而不是外显子测序，因此即使有了也不适合我们使用。
- en: We therefore decided to create a set of synthetic exomes, but base them on the
    1000 Genomes Phase 3 variant calls (one per project participant) so that they
    would have realistic variant profiles. This would essentially amount to reconstructing
    sequencing data for real people based on their previously identified variants.
    We would then mutate a subset of the synthetic exomes, assigned as cases, with
    the variants that Dr. Miossec et al. discovered in the Tetralogy of Fallot study,
    which were listed in the paper. Because there were more participants in the 1000
    Genomes dataset than in the Tetralogy of Fallot cohort, that would give us enough
    elbow room to generate as many samples as we needed. [Figure 14-5](#overview_of_our_implementation_for_gene)
    presents the key steps in this process.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们决定创建一组合成外显子，但是基于1000基因组第三阶段变异调用（每个项目参与者一个）来构建，以便它们具有逼真的变异模式。这本质上相当于根据先前确定的变异重构真实人群的测序数据。然后，我们会对一部分被分配为病例的合成外显子进行突变，使用Miossec博士等在法洛氏四联症研究中发现的变异，这些变异在论文中有详细列出。由于1000基因组数据集中的参与者比法洛氏四联症队列要多，这使得我们有足够的余地生成我们需要的样本。[图 14-5](#overview_of_our_implementation_for_gene)
    展示了这个过程的关键步骤。
- en: In theory, this approach seemed fairly straightforward but, as we rapidly learned,
    the reality of implementing this strategy (especially at the scale that we required)
    was not trivial. The packages we ended up using were very effective and for the
    most part quite well documented, but they were not very easy to use out of the
    box. In our observation, these tools tend to be used mostly by savvy tool developers
    for small-scale testing and benchmarking purposes, which led us to wonder how
    much of the high threshold of effort was a consequence of having an expert target
    audience versus the cause of their limited spread to less savvy users. In any
    case, we have not seen biomedical researchers use them individually for providing
    the kind of reproducible research supplements that we were envisioning, and in
    retrospect we are not surprised given the difficulties we had to overcome in our
    project. As we discuss later, this further motivated us to consider how we could
    capitalize on the results that we achieved to make it easier for others to adopt
    the model of providing synthetic data as a companion to a research study.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这种方法似乎相当直接，但正如我们迅速了解到的那样，实施这一策略（特别是在我们所需的规模上）的现实并不简单。我们最终使用的软件包非常有效，而且在大多数情况下都有很好的文档支持，但是它们并不是开箱即用的。在我们的观察中，这些工具主要被精通工具开发人员用于小规模测试和基准测试，这让我们想知道高门槛努力的多少是由于目标受众是专家，而不是由于它们在普通用户中普及率有限的原因。无论如何，我们没有看到生物医学研究人员单独使用它们来提供我们在项目中克服的困难的可重复研究补充。正如我们后来讨论的那样，这进一步激励我们考虑如何利用我们所取得的结果，使其更容易让其他人采纳提供合成数据作为研究伴侣的模式。
- en: In the next section, we expose the gory details of how we implemented this part
    of the work, occasionally pausing to provide additional color around specific
    challenges that we feel can provide either valuable insight for others or, failing
    that, a touch of comic relief to keep things light.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们详细介绍了如何实施这部分工作的细节，偶尔停下来围绕特定的挑战提供额外的描述，我们认为这些描述可以为其他人提供有价值的见解，或者至少为保持轻松气氛提供一丝幽默感。
- en: '![Overview of our implementation for generating appropriate synthetic data.](Images/gitc_1405.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![生成适当合成数据的实施概述。](Images/gitc_1405.png)'
- en: Figure 14-5\. Overview of our implementation for generating appropriate synthetic
    data.
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-5\. 生成适当合成数据的实施概述。
- en: Retrieving the Variant Data from 1000 Genomes Participants
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从1000基因组参与者中检索变异数据
- en: 'As we mentioned earlier, we decided to base the synthetic data simulation step
    on VCFs from participants from the 1000 Genomes Project. We chose that dataset
    because it was the largest available genomics dataset that was fully public, and
    a copy was freely available in GCS. The convenience stopped there, however. We
    had to jump through hoops from the get-go, first because the 1000 Genomes variant
    calls were provided at the time in the form of multisample VCFs containing variant
    calls from all project participants, split up by chromosome. For our purposes,
    we needed the opposite: a single-sample VCF containing all chromosomes’ worth
    of data for each project participant.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们决定基于来自1000基因组计划参与者的VCF来进行合成数据模拟步骤。我们选择了这个数据集，因为它是最大的可用基因组学数据集，完全公开，并且在GCS中可以免费获取。然而，方便只是到此为止。从一开始，我们就不得不跳过一些障碍，首先是因为当时1000基因组变异调用是以多样本VCF的形式提供的，其中包含来自所有项目参与者的变异调用，按染色体分割。而我们需要的是相反的：一个包含每个项目参与者所有染色体数据的单样本VCF。
- en: So the first thing we implemented was a WDL workflow that, given a participant’s
    identifier, would use GATK `SelectVariants` to extract the variant calls for the
    participant from each of the per-chromosome files, and then Picard `MergeGvcfs`
    to merge the data into a single VCF for that participant. This sounds simple enough,
    right? Well, yes, but this is bioinformatics, so of course things weren’t that
    simple. We ran into weird errors due to malformed file headers when trying to
    process the original multisample VCFs, so we had to add in a call to Picard `FixVcfHeader`
    to patch them up. Then, we ran into sorting errors after the `SelectVariants`
    step, so we had to add a call to Picard `SortVcf` to address those issues. We
    crammed all three commands into a single WDL step to avoid having to do a lot
    of back-and-forth file copying. It wasn’t pretty, but it worked. That became workflow
    [1_Collect-1000G-participants](https://oreil.ly/wYw54), which you can see in action
    in the case study workspace. For all tasks in this workflow, we used the standard
    GATK4 container image, which also contains Picard tools.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先实现的是一个WDL工作流，根据参与者的标识符，使用GATK的`SelectVariants`从每个染色体文件中提取参与者的变异调用，然后使用Picard的`MergeGvcfs`将数据合并为该参与者的单个VCF。听起来足够简单，对吧？是的，但这是生物信息学，所以当然事情并不那么简单。在尝试处理原始多样本VCF时，我们遇到了由于文件头格式不正确而导致的奇怪错误，因此我们不得不添加一个调用Picard的`FixVcfHeader`来修补它们。然后，在`SelectVariants`步骤之后，我们遇到了排序错误，因此我们不得不添加一个调用Picard的`SortVcf`来解决这些问题。我们将所有三个命令压缩到一个WDL步骤中，以避免进行大量来回文件复制。虽然不太美观，但它起作用了。这就成为了工作流[1_Collect-1000G-participants](https://oreil.ly/wYw54)，您可以在案例研究工作空间中看到它的运行情况。在这个工作流中的所有任务中，我们使用了标准的GATK4容器映像，其中也包含了Picard工具。
- en: To run this in our workspace, we set up the participant table with minimal metadata
    from the 1000 Genomes Project, including participant identifiers and the cohort
    they originally belonged to, in case the latter came in handy down the line (it
    hasn’t). Then, we configured the collector workflow to run per row in the table
    of participants and output the resulting VCF to the same row under a *sampleVcf*
    column.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的工作空间中运行这个流程，我们设置了来自1000基因组计划的最少元数据的参与者表，包括参与者标识符和他们最初所属的队列，以防后来有用（实际上并没有）。然后，我们配置了收集器工作流，以每行参与者表中的行运行，并将结果的VCF输出到同一行下的*sampleVcf*列中。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this workspace, we didn’t need the distinction between participant and sample,
    so we decided to use a very simple and flat data structure that would use only
    one of the two. At the time, the participant table was mandatory, so we had no
    choice but to use it. If we were to rebuild this today, we might switch to using
    samples instead to make our tables more directly compatible with what we have
    in other workspaces.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个工作空间中，我们不需要区分参与者和样本，因此我们决定使用一个非常简单和扁平的数据结构，只使用其中之一。当时，参与者表是强制性的，所以我们别无选择，只能使用它。如果我们今天要重新构建这个流程，我们可能会转而使用样本，以使我们的表格更直接地与其他工作空间中的内容兼容。
- en: 'The workflow suffers from a few major inefficiencies, so it took a while to
    run (4.5 hours on average, 12 hours for a set of 100 files run in parallel, which
    includes some preemptions) but it did produce the results we were looking for:
    a complete individual VCF for each participant. With that in hand, we could now
    move on to the next step: generating the synthetic data!'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于工作流程存在一些主要的低效性，因此运行时间较长（平均为 4.5 小时，100 个文件集合并行运行时为 12 小时，其中包括一些抢先停止），但它确实产生了我们所寻找的结果：每个参与者的完整个体
    VCF。有了这些，我们现在可以继续下一步：生成合成数据！
- en: Creating Fake Exomes Based on Real People
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基于真实人物的虚假外显子
- en: 'We wanted to create synthetic exome data that would stand in for the original
    access-controlled dataset, so our first task here was to choose a toolkit with
    the appropriate features because, as we mentioned earlier, there are quite a few
    and they display a fairly wide range of capabilities. After surveying available
    options, we selected the [NEAT-genReads toolkit](https://oreil.ly/2GdMY), an open
    source package developed by Zachary Stephens et al. It had a lot of features that
    we didn’t really need, but it seemed to be well regarded and had the core capability
    we were looking for: the ability to generate either whole genome or exome data
    based on an input VCF of existing variant calls (in our case, the 1000 Genome
    participant VCFs).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望创建合成外显子数据，以代替原始的受控访问数据集，因此我们在这里的第一个任务是选择一个具有适当功能的工具包，因为正如我们之前提到的，有很多选择，并且它们展示了相当广泛的功能。在调查了现有的选项之后，我们选择了由
    Zachary Stephens 等人开发的[NEAT-genReads 工具包](https://oreil.ly/2GdMY)，这是一个开源软件包。它具有很多我们实际上不需要的功能，但似乎广受好评，并且具有我们寻找的核心功能：根据现有变异调用的输入
    VCF（在我们的情况下是 1000 个基因组参与者的 VCF）生成整个基因组或外显子数据。
- en: How does it work? In a nutshell, the program simulates synthetic reads based
    on the reference genome sequence and then modifies the bases in a subset of reads
    at the positions where variants are present in the input VCF, in a proportion
    that matches the genotype in the VCF.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的？简而言之，该程序基于参考基因组序列模拟合成读取，并在输入 VCF 中存在变异的位置上修改某些读取中的碱基，比例与 VCF 中的基因型匹配。
- en: '[Figure 14-6](#neat_genreads_creates_simulated_read_da) illustrates this process.
    In addition, the program is capable of simulating sequencing errors and other
    artifacts by modifying other bases in reads, either randomly or based on an error
    model. By default, the program outputs FASTQ files containing unaligned synthetic
    sequence data, a BAM file containing the same data aligned to the reference genome,
    and a VCF file that serves as a truth set containing the variants that the program
    introduced into the data. For more information on how this works and other options,
    see the GitHub repository referenced earlier as well as the [original publication](https://oreil.ly/GPUGj)
    describing the toolkit in PLoS ONE.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-6](#neat_genreads_creates_simulated_read_da) 描绘了这一过程。此外，该程序能够通过随机或基于错误模型修改读取中的其他碱基，以模拟测序错误和其他人工效应。默认情况下，该程序输出包含未对齐的合成序列数据的
    FASTQ 文件，一个包含同一数据的 BAM 文件（与参考基因组对齐），以及作为真实数据集的 VCF 文件，其中包含程序引入的变异。有关此功能如何工作及其他选项的更多信息，请参阅前面引用的
    GitHub 仓库以及描述 PLoS ONE 工具包的[原始出版物](https://oreil.ly/GPUGj)。'
- en: '![NEAT-Genreads creates simulated read data based on a reference genome and
    list of variants.](Images/gitc_1406.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![NEAT-GenReads 根据参考基因组和变异列表创建模拟读取数据。](Images/gitc_1406.png)'
- en: Figure 14-6\. NEAT-genReads creates simulated read data based on a reference
    genome and list of variants.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-6\. NEAT-genReads 根据参考基因组和变异列表创建模拟读取数据。
- en: In addition to this core functionality, NEAT-genReads accepts various parameters
    that control different aspects of the simulated data. For example, you can specify
    which regions should be covered or not using an intervals file, and at what target
    coverage they should be covered. You can also control read length, fragment length,
    and so on, as well as the mutation rate to use for errors and spontaneous mutations.
    We took advantage of all of these to tailor the resulting synthetic data to resemble
    the original study cohort as closely as possible based on the information available
    in the preprint.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个核心功能外，NEAT-genReads 还接受各种参数，用于控制模拟数据的不同方面。例如，您可以使用间隔文件指定应覆盖或不覆盖的区域，并指定它们的目标覆盖度。您还可以控制读取长度、片段长度等，以及用于错误和自发突变的突变率。我们利用所有这些功能，根据预印本中可用的信息尽可能地调整生成的合成数据，使其尽量接近原始研究队列。
- en: To implement this, we developed the WDL workflow [*2_Generate-synthetic-reads*](https://oreil.ly/i42aY),
    which takes a reference genome and a VCF of variants to use in generating the
    sequence read, and produces a BAM file as well as the truth VCF file. Again, you
    can see it in action in the workspace.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们开发了WDL工作流[*2_Generate-synthetic-reads*](https://oreil.ly/i42aY)，它接受参考基因组和变异位点文件（VCF）用于生成序列读取，并生成BAM文件以及真实的VCF文件。同样，你可以在工作空间中看到它的运行过程。
- en: 'We’d love to say that this one went more smoothly than the first workflow,
    but no. We ran into even more problems that appeared to be due to either malformed
    data or the tools choking on elements that are allowed by the VCF format specification.
    For example, NEAT-genReads did not tolerate variant records that had multiple
    identifiers in the *rsID* field, so we had to apply an `awk` command to sanitize
    the VCFs by removing the contents of the *rsID* field from every record. Another
    fun one was that NEAT-genReads emitted incomplete VCF headers in the truth VCFs,
    so again we trotted out Picard `FixVcfHeader`. It also occasionally emitted malformed
    read names, so to fix that we deployed an unholy combination of `awk` and `samtools`.
    Finally, we also had to add `readgroup` information to the BAM files produced
    by NEAT-genReads, which clearly didn’t consider that metadata important (GATK
    responds: blasphemy!).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很乐意说，第二个工作流比第一个顺利得多，但事实并非如此。我们遇到了更多问题，这些问题似乎要么是由于数据格式不正确，要么是工具无法处理VCF格式规范允许的元素。例如，NEAT-genReads不能容忍在*rsID*字段中具有多个标识符的变异记录，因此我们不得不应用awk命令来清理VCF文件，从每个记录中删除*rsID*字段的内容。另一个有趣的问题是，NEAT-genReads在真实的VCF文件中发出了不完整的VCF头部，因此我们再次使用Picard的`FixVcfHeader`来解决。它有时还会发出格式不正确的读取名称，因此为了解决这个问题，我们采用了awk和samtools的不寻常组合。最后，我们还必须向NEAT-genReads生成的BAM文件中添加`readgroup`信息，这显然并未考虑到该元数据的重要性（GATK回应：亵渎！）。
- en: Is your head spinning yet? Ours certainly was. Don’t worry if you’re not familiar
    with the more nitty-gritty details, though. Ultimately, the point is that after
    you get into a certain level of rolling your own analysis, this is the kind of
    thing you’re likely to encounter, even with a great toolkit like NEAT-genReads
    (yes, we really like it, warts and all). What’s worse is that this is the kind
    of thing that gets left out of methodology descriptions. Imagine that we had simply
    written, “We used NEAT-genReads with parameter *x*, *y*, and *z* to generate the
    synthetic data” and withheld the workflow code. If you tried to reproduce this
    work without our workflow (which includes comments about the issues), you would
    have to rediscover and troubleshoot all of that yourself.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你头晕了吗？我们当然是。不过，如果你对更详细的细节不熟悉，也不要担心。最终，重点是，在你深入进行自己的分析之后，你很可能会遇到这样的事情，即使是在像NEAT-genReads这样的强大工具中（是的，我们真的很喜欢它，无论它有什么问题）。更糟糕的是，这种事情往往被忽略在方法描述中。想象一下，如果我们简单地写道，“我们使用NEAT-genReads和参数*x*、*y*和*z*来生成合成数据”，并隐瞒了工作流代码。如果你尝试在没有我们的工作流（其中包括有关问题的评论）的情况下重现这项工作，你将不得不自己重新发现和排除所有这些问题。
- en: Meanwhile, the other slight complication here was that we couldn’t find a Docker
    container image of NEAT-genReads anywhere, so we had to create one ourselves.
    And this is where we realize with horror that we’re 13-plus chapters into this
    book and we haven’t even shown you how to create your own Docker image. Oops?
    Well, keep in mind that these days most commonly used tools are available on Docker
    images through projects like [BioContainers](https://oreil.ly/pzFEd), which provide
    a great service to the community—they even take requests. Depending on what you
    do for a living, there’s a good chance that you can get most, if not all, of your
    work done on the cloud without ever worrying about creating a Docker image. Yet
    we can’t really let you finish this book without at least showing you the basics,
    so if you ever do need to package your own tool, you’ll know what to expect. Let’s
    take advantage of this NEAT-genReads exposé to do that in the accompanying sidebar.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，这里的另一个小复杂之处在于我们找不到任何关于NEAT-genReads的Docker容器镜像，因此我们不得不自己创建一个。这时我们惊恐地意识到，我们已经写到了这本书的第13章多一点，而我们甚至还没有向你展示如何创建你自己的Docker镜像。糟糕？嗯，记住，如今大多数常用工具都可以通过像[BioContainers](https://oreil.ly/pzFEd)这样的项目提供的Docker镜像来使用，这为社区提供了极好的服务——它们甚至接受请求。根据你的职业，很可能你可以完全依赖云来完成大部分甚至所有的工作，而无需担心创建Docker镜像。然而，我们不能真的让你在没有至少展示基础知识的情况下完成这本书，因此如果你确实需要打包自己的工具，你会知道该期待什么。让我们利用这个NEAT-genReads的揭示来在附带的侧栏中做到这一点。
- en: We used the Dockerfile shown in the sidebar to create the container image and
    then we published it to a public repository on Docker Hub.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用侧边栏中显示的 Dockerfile 创建容器镜像，然后将其发布到 Docker Hub 的公共仓库。
- en: To run the workflow in our Terra workspace, we configured it to run on individual
    rows of the participant table, just like the first workflow. We wired up the *sampleVcf*
    column, populated by the output of the previous workflow, to serve as input for
    this second one. Finally, we configured the two resulting outputs, the BAM file
    and the VCF file, to be written to new columns in the table, *synthExomeBam* and
    *originalCallsVcf*, along with their index files.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的 Terra 工作空间中运行工作流，我们配置它在参与者表的各行上运行，就像第一个工作流一样。我们将由上一个工作流的输出填充的 *sampleVcf*
    列连接起来，作为这个第二个工作流的输入。最后，我们配置了两个结果输出，BAM 文件和 VCF 文件，写入表中的新列 *synthExomeBam* 和 *originalCallsVcf*，以及它们的索引文件。
- en: 'Just like our first workflow, this one too suffers from a few inefficiencies
    due in part to the cleanup tasks that we had to add. It took about the same time
    as the first one to run, though within that amount of time it did quite a bit
    more work: it yielded a complete exome BAM file for each of the participants that
    we ran it on, with coverage and related metrics modeled after the original Tetralogy
    of Fallot cohort exomes reported in the preprint. We opened a few of the output
    BAM files in IGV, and were impressed to see that we could not readily distinguish
    a synthetic exome from a real exome with similar coverage properties.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的第一个工作流一样，这个也因为我们不得不添加的清理任务而存在一些低效率。它运行的时间大约与第一个相同，尽管在这段时间内它做了更多的工作：为我们运行的每个参与者产生了一个完整的外显子
    BAM 文件，覆盖率和相关指标模拟了预印本中报告的法洛四联症队列外显子。我们在 IGV 中打开了一些输出的 BAM 文件，惊讶地发现我们无法轻易区分具有类似覆盖属性的合成外显子和真实外显子。
- en: 'This brings us to the final step in the process of generating the synthetic
    dataset: adding in the variants of interest.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到生成合成数据集过程的最后一步：添加感兴趣的变体。
- en: Mutating the Fake Exomes
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改变假外显子
- en: Walking into this, we were already familiar with an open source tool developed
    by Adam Ewing called [BAMSurgeon](https://oreil.ly/fFw0e), which some of our colleagues
    had previously used successfully to create test cases for variant-calling development
    work. In a nutshell, BAMSurgeon is designed to modify read records in an existing
    BAM file to introduce variant alleles, as illustrated in [Figure 14-7](#bamsurgeon_introduces_mutations_in_read).
    The tool takes a BED file (a tab-delimited file format) listing the position,
    allele, and allele fraction of variants of interest, and will modify the corresponding
    fraction of reads accordingly. Because we had a list of the variants of interest
    identified in the Tetralogy of Fallot cohort, including their alleles and positions,
    we figured that we should be able to introduce them into a subset of our synthetic
    exomes to re-create the relevant characteristics of the case samples.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 进入这个过程时，我们已经熟悉了由 Adam Ewing 开发的一个名为 [BAMSurgeon](https://oreil.ly/fFw0e) 的开源工具，我们的一些同事之前已成功使用它为变异调用开发工作创建测试用例。简而言之，BAMSurgeon
    旨在修改现有 BAM 文件中的读取记录，引入变体等位基因，如 [图 14-7](#bamsurgeon_introduces_mutations_in_read)
    所示。该工具接受列出感兴趣变体的位置、等位基因和等位基因分数的 BED 文件（一个制表符分隔的文件格式），并相应地修改相应比例的读取。因为我们有在法洛四联症队列中确定的感兴趣变体列表，包括它们的等位基因和位置，我们认为我们应该能够将它们引入到我们的一部分合成外显子中，以重新创建案例样本的相关特征。
- en: '![BAMSurgeon introduces mutations in read data.](Images/gitc_1407.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![BAMSurgeon 在读取数据中引入变异。](Images/gitc_1407.png)'
- en: Figure 14-7\. BAMSurgeon introduces mutations in read data.
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-7\. BAMSurgeon 在读取数据中引入变异。
- en: We implemented this as the WDL workflow [3_Mutate-reads-with-BAMSurgeon](https://oreil.ly/H43dN),
    which takes the reference genome, a BAM file, and a BED file listing the variants
    to introduce, and produces the mutated BAM file. In the first iteration, we limited
    it to just adding SNPs, because BAMSurgeon uses different commands for different
    types of variants. In later work, contributors from the community added some logic
    to the workflow to also handle indels and copy-number variants using the relevant
    BAMSurgeon commands.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其实现为 WDL 工作流 [3_Mutate-reads-with-BAMSurgeon](https://oreil.ly/H43dN)，它接受参考基因组、一个
    BAM 文件和一个列出要引入的变体的 BED 文件，并生成突变的 BAM 文件。在第一次迭代中，我们仅限于添加 SNPs，因为 BAMSurgeon 对不同类型的变体使用不同的命令。在后续工作中，社区的贡献者添加了一些逻辑到工作流中，以处理插入缺失和拷贝数变体，使用相关的
    BAMSurgeon 命令。
- en: How much trouble did we have with this one? Hardly any trouble at all, if you
    can believe it. The only real problem we had was when we initially tested this
    on the 1000 Genomes Low Coverage data, because BAMSurgeon refused to add variants
    at positions where the sequence coverage was too low. However, when we switched
    to running on the lovely, plump data produced by NEAT-genReads (which we had given
    a target coverage of 50X), it worked like a charm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个问题上遇到了多少麻烦？几乎没有任何麻烦，如果你能相信的话。我们唯一真正遇到的问题是当我们最初在1000基因组低覆盖数据上进行测试时，因为BAMSurgeon拒绝在序列覆盖度太低的位置添加变异。然而，当我们转而在由NEAT-genReads生成的可爱、丰富数据上运行（我们已经设定了50倍的目标覆盖度），它完美地运行起来了。
- en: As previously, in our Terra workspace, we configured the workflow to run on
    individual rows of the *participant* table. Then, we just needed to determine
    how we would label cases versus controls and how to set up which cases would be
    mutated with which variant.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往一样，在我们的Terra工作空间中，我们配置了工作流以在*participant*表的各行上运行。然后，我们只需确定如何标记病例与对照以及如何设置哪些病例将被突变成哪种变异体。
- en: That’s when we realized that we had oversimplified the experimental design for
    this part. Our original plan was simply to introduce the variants listed in the
    paper into a subset of exomes that would serve as case samples, whereas the rest
    would be left untouched as control samples. Unfortunately, this reasoning had
    two flaws.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到，在这一部分，我们过于简化了实验设计。我们最初的计划只是在一部分外显子中引入论文中列出的变异体作为病例样本，而其余的则保持不变作为对照样本。不幸的是，这种推理存在两个缺陷。
- en: First, not all case samples had yielded variants of interest in the original
    study, so there were fewer variants than case samples. In addition, a number of
    variants in that list fell in genes for which the variant load results were less
    convincing, so we had decided early on to focus on just the *NOTCH1* variants
    for the case study. Because *NOTCH1* was the main focus of the paper itself, we
    felt that reproducing the *NOTCH1* result would be sufficient as a proof of concept.
    Yet we did believe that we should keep the proportion of cases and controls the
    same as in the paper. As a result, many of the synthetic exomes labeled as case
    samples would actually *not* receive a variant of interest.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，并非所有病例样本在原始研究中都产生了感兴趣的变异体，因此变异体少于病例样本。此外，该列表中的许多变异体位于基因中，其变异负荷结果不太令人信服，因此我们早早决定仅专注于*NOTCH1*变异体用于本案研究。因为*NOTCH1*是论文本身的主要焦点，我们认为重现*NOTCH1*结果足以作为概念验证的证据。然而，我们确实认为我们应该保持与论文中相同的病例和对照比例。因此，许多标记为病例样本的合成外显子实际上*不会*接收到感兴趣的变异体。
- en: Second, putting some data through different processing steps compared to the
    rest would expose us to batch effects. What if something in the differential processing
    caused an artifact that affected the analysis? We needed to eliminate this source
    of bias and ensure that all of the data would be processed the same way.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，将一些数据与其余数据经历不同的处理步骤会使我们暴露于批次效应。如果不同处理中的某些因素导致影响分析的人工效应，该怎么办？我们需要消除这种偏倚的来源，并确保所有数据都以相同的方式处理。
- en: To address these flaws, we decided to introduce a neutral variant into any exome
    that would not receive a *NOTCH1* variant, whether it was labeled as a case sample
    or as a control sample. To that end, we designed a synonymous mutation that we
    predicted should not have any effect.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些缺陷，我们决定在任何不接收*NOTCH1*变异的外显子中引入一个中性变异体，无论其被标记为病例样本还是对照样本。为此，我们设计了一个我们预测不会产生任何影响的同义突变。
- en: In the Terra workspace, we added a *role* column to the *participant* table,
    so that we could label participants as *neutral* or *case* based on the mutation
    they would receive. We used the term *neutral* rather than *control* because we
    wanted to give ourselves the flexibility to use participants with the neutral
    mutation either as actual control samples or as case samples that do not have
    a *NOTCH1* mutation. We put the list of participants through a random selection
    process to pick the ones that would become *NOTCH1* case samples and assigned
    them the *case* role in the participant table, labeling all other participants
    as neutral.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Terra工作空间中，我们向*participant*表添加了一个*role*列，以便我们可以根据他们将接收的突变标记参与者为*neutral*或*case*。我们使用术语*neutral*而不是*control*，因为我们想要给自己灵活性，可以将具有中性突变的参与者作为真实的对照样本或作为不具有*NOTCH1*突变的病例样本。我们通过随机选择过程挑选出那些将成为*NOTCH1*病例样本的参与者，并在参与者表中为它们分配了*case*角色，将所有其他参与者标记为中性。
- en: Finally, we created BED files for each of the *NOTCH1* variants reported in
    the paper as well as for the neutral variant we had designed. We randomly assigned
    individual *NOTCH1* variants to the participants labeled *case* and assigned the
    *neutral* variant to all others. For each participant, we linked the corresponding
    BED file in a new column of the participant table named *mutation*. We configured
    the BAMSurgeon workflow to take that field (`this.mutation`) as input for the
    variant to be introduced in each participant’s exome BAM file.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为论文中报告的每一个*NOTCH1*变异创建了BED文件，以及我们设计的中性变异体。我们随机分配了个体*NOTCH1*变异体给标记为*case*的参与者，并将*neutral*变异体分配给其他所有参与者。对于每个参与者，我们在参与者表的新列*mutation*中链接了相应的BED文件。我们配置了BAMSurgeon工作流，以此字段(`this.mutation`)作为每个参与者外显子BAM文件中引入的变异体的输入。
- en: Even though the preparation that we had to do to allocate variants appropriately
    was not trivial, the workflow was straightforward and ran quickly (0.5 hours on
    average, 2.5 hours for a set of 100 files run in parallel, again including some
    preemptions). As planned, it produced mutated exome BAMs for all the participants
    we ran it on. We checked a few of the output BAM files in IGV to verify that the
    desired variant had indeed been introduced, and were satisfied that the process
    had worked as expected.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们需要做适当的准备来合理分配变异体并不是一件简单的事情，但工作流程是直接而且运行速度很快（平均0.5小时，对于一组100个文件并行运行为2.5小时，再次包括一些抢先处理）。按计划，它为我们运行的所有参与者生成了变异的外显子BAM文件。我们在IGV中检查了几个输出的BAM文件，以确认确实引入了所需的变异，并且满意地看到该过程按预期运行。
- en: Generating the Definitive Dataset
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成确定性数据集
- en: 'Having successfully harnessed existing data and tools into the aforementioned
    workflows, we were now in a position to generate a synthetic dataset that could
    adequately stand in for the original Tetralogy of Fallot cohort. However, we had
    to make one more compromise, which was to limit the number of exomes that we created
    to 500 instead of the full 2,081 that we had originally planned on to emulate
    the original cohort. The reason? It’s so pedestrian that it hurts: we simply ran
    out of time before the conference. Sometimes, you just have to move forward with
    what you have.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 成功地将现有数据和工具整合到上述工作流程中后，我们现在有能力生成一个能够充分替代原始Fallot四联症队列的合成数据集。然而，我们不得不做出一个妥协，即将创建的外显子数量限制为500个，而不是我们最初计划的2081个，以模拟原始队列。原因？原因很俗气，让人感到痛心：我们在会议前简直没有时间了。有时候，你只能凭借手头现有的东西继续前行。
- en: So we generated 500 mutated synthetic exomes, which we further organized into
    two participant sets in the workspace, imaginatively named A100 and B500\. The
    A100 set was a subset of 100 participants that included eight *NOTCH1* cases.
    The B500 set was the superset of all participants for which we created synthetic
    exomes, and included all 49 *NOTCH1* cases reported in the paper. You might notice
    that these compositions don’t quite match the proportion of *NOTCH1* cases in
    the overall Tetralogy of Fallot cohort, but the two sets have similar proportions
    to each other. Later, we discuss how this affects the interpretation of the final
    results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们生成了500个变异合成外显子，进一步在工作空间中组织为两个参与者集，富有想象力地命名为A100和B500。A100集是包含八个*NOTCH1*案例的100名参与者子集。B500集是所有参与者的超集，我们为其创建了合成外显子，并包含了论文中报告的所有49个*NOTCH1*案例。您可能注意到，这些组成与总体Fallot四联症队列中*NOTCH1*案例的比例不完全匹配，但两组之间的比例相似。稍后，我们将讨论这如何影响最终结果的解释。
- en: At this point, we’re ready to move on to the next section, in which we attempt
    to reproduce the study methodology itself.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经准备好进入下一部分，尝试复制研究方法学本身。
- en: Re-Creating the Data Processing and Analysis Methodology
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新创建数据处理和分析方法学
- en: We wouldn’t blame you if you had forgotten all about the original study by now,
    so let’s go through a quick refresher. As illustrated in [Figure 14-8](#summary_of_the_two_phases_of_the_study),
    Dr. Miossec et al. started out with exome sequencing data from a cohort of 2,081
    study participants, split almost evenly between cases; that is, patients suffering
    from the congenital heart disease tetralogy of Fallot (specifically, a type called
    *nonsyndromic*), and controls (people not affected by the disease). They applied
    fairly standard processing to the exome data using a pipeline based on the GATK
    Best Practices, consisting mainly of mapping to the reference genome and variant
    calling, which you are an expert in since [Chapter 6](ch06.xhtml#best_practices_for_germline_short_varia).
    Then, they applied a custom analysis that involved predicting the effects of variants
    in order to focus on deleterious variants, before attempting to identify genes
    with a higher variant load than would be expected by chance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在已经忘记了原始研究，我们不会责怪你，所以让我们进行一个快速复习。如图[14-8](#summary_of_the_two_phases_of_the_study)所示，Miossec
    博士等人从2081名研究参与者的外显子测序数据开始，几乎均匀分为两组：病例，即患有先天性心脏病四联症（具体来说是一种称为*非综合征性*的类型），和对照组（未受该疾病影响的人）。他们使用基于
    GATK 最佳实践的管道对外显子数据进行了相当标准的处理，主要包括与参考基因组的比对和变异调用，这些你在[第6章](ch06.xhtml#best_practices_for_germline_short_varia)已经是专家了。然后，他们应用了一个定制的分析，涉及预测变异的影响，以便专注于有害变异，然后尝试识别具有比随机预期更高变异负载的基因。
- en: '![Summary of the two phases of the study: Processing and Analysis.](Images/gitc_1408.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![研究的两个阶段总结：处理和分析。](Images/gitc_1408.png)'
- en: 'Figure 14-8\. Summary of the two phases of the study: Processing and Analysis.'
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-8\. 研究的两个阶段总结：处理和分析。
- en: 'We go into more detail about the meaning and purpose of these procedures in
    a few moments. For now, the key concept we’d like to focus on is the difference
    between these two phases of the study. In the *Processing phase*, which can be
    highly standardized and automated, we’re mainly trying to clean up the data and
    extract what we consider to be useful information out of the immense haystack
    of data we’re presented. This produces variant calls across the entire sequenced
    territory, a long list of differences between an individual’s genome, and a rather
    arbitrary reference. In itself, that list doesn’t provide any real insights; it’s
    still just data. That’s where the *Analysis phase* comes in: we’re going to start
    asking specific questions of the data and, if all goes well, produce insights
    about a particular aspect of the biological system that we’re investigating.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些程序的含义和目的，我们稍后会详细讨论。目前，我们想重点关注的概念是研究的这两个阶段之间的区别。在*处理阶段*，这可能是高度标准化和自动化的，我们主要试图清理数据并从我们面对的大量数据中提取我们认为有用的信息。这会在整个测序领域产生变异调用，即一个人的基因组与一个任意参考之间的差异长列表。这个列表本身并不提供任何真正的见解；它仍然只是数据。这就是*分析阶段*的作用所在：我们将开始对数据提出具体问题，并且如果一切顺利，产生有关我们正在调查的生物系统特定方面的见解。
- en: We highlight this distinction because it was a key factor in our decision making
    about how closely we believed we needed to get to reproducing the exact methods
    used in the original study. Let’s dive in, and you’ll see what we mean.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调这一区别，因为这是我们关于我们认为需要多接近复制原始研究中使用的确切方法的决定的关键因素。让我们深入探讨，你会明白我们的意思。
- en: Mapping and Variant Discovery
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比对和变异发现
- en: Right off the bat, we bump into an important deviation. The original study started
    from exome sequencing data in FASTQ format, which had to be mapped before anything
    else could happen. However, when we generated the synthetic exome dataset, we
    produced BAM files containing sequencing data that was already mapped and ready
    to go. We could have reverted the data to an unmapped state and started the pipeline
    from scratch, but we chose not to do that. Why? Time, cost, and convenience, not
    necessarily in that order. We made the deliberate decision to take a shortcut
    because we estimated that the effect on our results would be minimal. In our experience,
    as long as some key requirements are met and the data is of reasonably good quality,
    the exact implementation of the data processing part of the pipeline does not
    matter quite as much as whether it has been applied consistently in the same way
    to the entire dataset. We did, however, make sure that we were using the same
    reference genome, of course.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我们就遇到了一个重要的偏差。原始研究从FASTQ格式的外显子测序数据开始，必须在发生任何其他事情之前进行映射。然而，当我们生成合成外显子数据集时，我们产生了包含已经映射并准备好的测序数据的BAM文件。我们本可以将数据恢复到未映射状态，并从头开始运行管道，但我们选择不这样做。为什么？时间、成本和便利性，并非按照这个顺序。我们做出了有意的决定采取捷径，因为我们估计这对我们的结果影响将是最小的。根据我们的经验，只要满足一些关键要求并且数据质量合理，数据处理管道的具体实施方式并不像应用于整个数据集时一致性那样重要。但是，我们确保使用了相同的参考基因组，当然。
- en: Note
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note the preconditions; we’re not saying that *anything* goes. Sadly, a full
    discussion on that topic is beyond the scope of this book, but hit us up on Twitter
    if you want to talk.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意前提条件；我们并不是说*什么都可以*。不幸的是，关于这个话题的全面讨论超出了本书的范围，但如果您想讨论，请在Twitter上联系我们。
- en: So with that, we fast-forward to the variant discovery part of the processing.
    The original study used a pipeline called *GenPipes DNAseq* developed and operated
    at the Canadian Centre for Computational Genomics in Montreal. Based on the information
    we found in the preprint and in the online documentation for GenPipes DNAseq,
    the pipeline followed the main tenets of the GATK Best Practices for germline
    short variant discovery. This included performing single-sample variant calling
    with GATK `HaplotypeCaller` to generate a GVCF for each sample and then joint-calling
    all samples together. We did flag one deviation, which was that according to the
    preprint, the variant callset filtering step was done by hard filtering instead
    of using VQSR.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们直接跳到处理的变异发现部分。原始研究使用了在蒙特利尔加拿大计算基因组学中心开发和运行的*GenPipes DNAseq*管道。根据我们在预印本和*GenPipes
    DNAseq*在线文档中找到的信息，该管道遵循了GATK关于种系短变异发现的最佳实践。这包括使用GATK `HaplotypeCaller`对每个样本进行单样本变异调用，生成每个样本的GVCF，然后联合调用所有样本。我们注意到一个偏差，即根据预印本，变异调用集过滤步骤是通过硬过滤而不是使用VQSR进行的。
- en: Accordingly, we repurposed some GATK workflows that we had on hand and customized
    them to emulate the processing described in the materials available to us. This
    produced the two WDL workflows [*4_Call-single-sample-GVCF-GATK*](https://oreil.ly/I8CO6)
    (featuring your favorite `HaplotypeCaller`) and [*5_Joint-call-and-hard-filter-GATK4*](https://oreil.ly/UW5kj)
    (featuring `GenotypeGVCF`s and friend), which you can find in the Terra workspace.
    We configured the first to run the synthetic exome BAM file of each participant
    in the table and output the GVCF to the corresponding row. In contrast, we configured
    the second to run at the sample set level, taking in the GVCFs from all participants
    in the set and producing a filtered multisample VCF for the set.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们重新利用了手头上已有的一些GATK工作流，并根据我们手头可用的材料描述定制了它们的处理。这产生了两个WDL工作流 [*4_Call-single-sample-GVCF-GATK*](https://oreil.ly/I8CO6)（包括您最喜欢的`HaplotypeCaller`）和
    [*5_Joint-call-and-hard-filter-GATK4*](https://oreil.ly/UW5kj)（包括`GenotypeGVCF`s及其友好的特性），您可以在Terra工作区中找到它们。我们配置了第一个工作流以运行表中每个参与者的合成外显子BAM文件，并将GVCF输出到相应行。相比之下，我们配置了第二个工作流以样本集水平运行，接收来自集合中所有参与者的GVCF，并为集合生成经过筛选的多样本VCF。
- en: 'Here we allowed ourselves another notable deviation: instead of using version
    3.2 of the GATK as the original study did, we used GATK 4.0.9.0 in order to take
    advantage of the substantial improvements in speed and scalability that GATK4
    versions offer compared to older versions of GATK. We expected that given the
    high quality of our synthetic data, there would be few differences in the output
    of `HaplotypeCaller` between these versions. We would expect important differences
    to arise only on lower-quality data and in the low-confidence regions of the genome.
    Again, some caveats apply that we don’t get into here, and we were careful to
    apply the same methods consistently across our entire dataset.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们允许自己再做一个显著的偏离：我们没有像原始研究那样使用GATK 3.2版本，而是使用了GATK 4.0.9.0版本，以便利用GATK4版本相比较老版本的速度和可伸缩性方面的显著改进。我们预期，考虑到我们合成数据的高质量，`HaplotypeCaller`在这些版本之间的输出几乎没有差异。我们只期望在低质量数据和基因组的低置信区域才会出现重要的差异。再次强调，这里有一些注意事项我们没有深入讨论，我们一直在整个数据集中一致地应用相同的方法。
- en: All in all, we made quick work of the Processing phase, in part thanks to those
    two deviations from the original methods. Since then, we’ve considered going back
    and reprocessing the data from scratch with a more accurate reimplementation of
    the original methods, to gauge exactly how much difference it would make. It’s
    not exactly keeping us awake at night, but we’re curious to see whether the results
    would support our judgment calls. If you end up doing it as a take-home exercise,
    don’t hesitate to let us know how badly wrong we were.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，我们在处理阶段迅速完成了工作，部分得益于对原始方法的两次偏离。自那时以来，我们考虑过是否回头重新从头处理数据，采用更精确的原始方法重新实现，以衡量这将产生多大的差异。这并没有让我们失眠，但我们很想知道结果是否支持我们的判断。如果你把它当作一项课后作业来完成，别犹豫，告诉我们我们错得多厉害。
- en: For now, it’s time to move on to the really interesting part of the analysis,
    in which we discover whether we’re able to reproduce the main result of the original
    study.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候进入分析的真正有趣的部分了，我们将发现是否能够重现原始研究的主要结果。
- en: Variant Effect Prediction, Prioritization, and Variant Load Analysis
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变异效应预测、优先级和变异负载分析
- en: Let’s take a few minutes to recap the problem statement and go over the experimental
    design for this part given that, so far, we’ve mostly waved off the details. First,
    what do we have in hand, and what are we trying to achieve? Well, we’re starting
    from a cohort-level VCF, which is a long inventory of everyone’s variants, and
    we’re hoping to identify genes that contribute to the risk of developing a form
    of congenital heart disease called tetralogy of Fallot. To be clear, we’re not
    trying to find a *particular variant* that is associated with the disease; we’re
    looking for a *gene* that is associated with the disease, with the understanding
    that different patients can carry different variants located within the same gene.
    What makes this difficult is that variants occur naturally in all genes, and if
    you look at enough variants across enough samples, you can easily find spurious
    associations that mean nothing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花几分钟回顾一下问题陈述，并审视这部分的实验设计，考虑到到目前为止，我们大部分时间都是略过细节的。首先，我们手头有什么，我们想要实现什么目标？我们从一个队列级别的VCF开始，这是每个人变异的长清单，我们希望识别与发育性心脏病变形四联症相关的基因。明确一点，我们不是试图找到与疾病相关的*特定变异*；我们正在寻找与疾病相关的*基因*，理解不同患者可能携带位于同一基因内的不同变异。使这一点困难的是变异在所有基因中自然发生，如果你足够多地观察足够的样本中的足够多变异，你很容易找到毫无意义的伪关联。
- en: The first step to tackling this problem is to narrow the list of variants to
    eliminate as many of them as you can based on how common they are, whether they’re
    present in the control samples, and what kind of biological effect they are likely
    to have, if any. Indeed, the overwhelming majority of the variants in the callset
    are common, boring, and/or unlikely to have any biological effect. We want to
    prioritize; for instance, focus on rare variants that are found only in the case
    samples and are likely to have a deleterious effect on gene function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的第一步是缩小变异列表，尽可能多地消除它们，根据它们的普遍性，它们是否存在于对照样本中，以及它们可能具有的生物学影响类型。确实，通话集中绝大多数变异都是普通的、无聊的，和/或不太可能有任何生物学影响。我们希望优先考虑；例如，集中关注仅在病例样本中发现的罕见变异，这些变异可能对基因功能具有有害影响。
- en: With that much-reduced list of variants in hand, we can then perform a variant
    load analysis, as illustrated in [Figure 14-9](#comparing_variant_load_in_a_gene_across).
    This involves looking for genes that appear to be more frequently mutated than
    you would expect to observe by chance given their size.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个大大减少的变异列表，我们可以进行变异负荷分析，如[图14-9](#comparing_variant_load_in_a_gene_across)所示。这涉及查找那些看起来比你期望的更频繁发生变异的基因，考虑到它们的大小。
- en: '![Comparing variant load in a gene across multiple samples.](Images/gitc_1409.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![比较多个样本中基因的变异负荷。](Images/gitc_1409.png)'
- en: Figure 14-9\. Comparing variant load in a gene across multiple samples.
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-9\. 比较多个样本中基因的变异负荷。
- en: The original study used a tool called `SnpEff` to predict variant effects, and
    then `vt` from GEMINI to prioritize variants based on their predicted effect,
    their frequency in population databases, and their presence in only the case samples.
    We implemented this as a two-step WDL workflow, [*6_Predict-variant-effects-GEMINI*](https://oreil.ly/GBesI),
    that used the same versions and commands for both tools, as well as the same population
    resources. In Terra, we configured the workflow to run at the participant set
    level and output the list of prioritized variants to the same row in the table.
    For this step, we had to add a pedigree file to each participant, specifying whether
    it should be counted as a case sample or as a control sample. For a given participant
    set, we selected one half of the participants, all carrying the neutral mutation,
    to be control samples. We then assigned the rest of the participants in the set,
    including all those carrying *NOTCH1* mutations, to be case samples.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 原研究使用了一个名为`SnpEff`的工具来预测变异效应，然后使用GEMINI中的`vt`来根据它们的预测效应、在人群数据库中的频率以及仅在病例样本中存在的情况对变异进行优先排序。我们将其实现为一个两步骤的WDL工作流，[*6_预测变异效应-GEMINI*](https://oreil.ly/GBesI)，该工作流对两个工具使用相同的版本和命令，以及相同的人群资源。在Terra中，我们配置了工作流以在参与者集合级别运行，并将优先排序的变异列表输出到表中的同一行。对于这一步，我们必须为每个参与者添加一个家系文件，指定它应该被视为病例样本还是对照样本。对于给定的参与者集合，我们选择了一半的参与者，所有携带中性突变的参与者作为对照样本。然后，我们将集合中的其余参与者，包括所有携带*NOTCH1*突变的参与者，分配为病例样本。
- en: Finally, Dr. Miossec helped us rewrite the original Perl scripts for the clustering
    analysis into R in a [Jupyter notebook](https://oreil.ly/cRROQ), which you can
    also find in the workspace. The notebook is set up to pull in the output file
    produced by GEMINI. It then runs through a series of analysis steps, culminating
    in a clustering test that looks for an excess of rare, deleterious variants in
    the case samples. The analysis is documented with explanations for each step,
    so be sure to check it out if you’d like to learn more about this analysis.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Miossec博士帮助我们将用于聚类分析的原始Perl脚本改写为R语言，放入了一个[Jupyter笔记本](https://oreil.ly/cRROQ)，你也可以在工作区中找到。该笔记本设置为导入GEMINI生成的输出文件。然后通过一系列分析步骤，最终进行了一个聚类测试，寻找病例样本中罕见有害变异的过量。该分析有每个步骤的解释文档，所以如果你想了解更多关于这个分析，请务必查看。
- en: In both the workflow and the notebook, we were careful to reproduce the original
    analysis to the letter, because we considered that these were the parts that would
    have the most influence on the final results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流和笔记本中，我们非常小心地逐字复制原始分析，因为我们认为这些部分对最终结果的影响最大。
- en: Analytical Performance of the New Implementation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新实现的分析性能
- en: So the burning question is…did it work? Were we able to pull out *NOTCH1* from
    the haystack? The short answer is, yes, mostly; the long answer is rather more
    interesting.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 那么最关键的问题是…它起作用了吗？我们能够从大海捞针中找到*NOTCH1*吗？简短的答案是，大部分是；长答案则更有趣。
- en: As we mentioned earlier, we ended up being able to generate only 500 synthetic
    exomes in the time we had, so we defined a 100-participant set with eight *NOTCH1*
    cases and a 500-participant set with all 49 *NOTCH1* cases. Although these proportions
    of *NOTCH1* cases were higher than those in the original dataset, what mattered
    to us at that point was that our two participant sets were roughly proportional
    to each other. Because we could not test the method at full scale as originally
    intended, we would at least be able to gauge how the results changed proportionally
    to the dataset size, which was a point of interest from early on.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们最终只能在我们拥有的时间内生成了500个合成外显子，因此我们定义了一个包含8个*NOTCH1*案例的100参与者组，以及一个包含所有49个*NOTCH1*案例的500参与者组。尽管*NOTCH1*案例的比例高于原始数据集中的比例，但在那时对我们来说重要的是，我们的两个参与者组在大致上是成比例的。因为我们不能按照最初的计划在全尺度上测试该方法，所以我们至少能够评估结果如何相对于数据集大小按比例变化，这从早期就是一个感兴趣的点。
- en: We ran the full set of processing and analysis workflows on both participant
    sets and then loaded the results of both into the notebook, where you can see
    the final clustering analysis repeated on both sets. In both cases, *NOTCH1* came
    up as a candidate gene, but with rather different levels of confidence, as shown
    in [Figure 14-10](#ranking_from_the_clustering_test_for_ar). In the 100-participant
    set, *NOTCH1* was ranked only second in the table of candidate genes, failing
    to rise above the background noise. In contrast, in the 500-participant set, *NOTCH1*
    emerged as the uncontested top candidate, clearing the rest of the pack by a wide
    margin.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个参与者组上运行了完整的处理和分析工作流程，然后将两者的结果加载到笔记本中，在那里您可以看到对两组重复进行的最终聚类分析。在两种情况下，*NOTCH1*都被提出作为候选基因，但置信水平却有所不同，如[图14-10](#ranking_from_the_clustering_test_for_ar)所示。在100参与者组中，*NOTCH1*仅在候选基因表中排名第二，未能超越背景噪声。相比之下，在500参与者组中，*NOTCH1*作为毫无争议的头号候选者出现，并以显著的优势领先其他参与者。
- en: '![Ranking from the clustering test for A) 100-participant set, and B) 500-participant
    set.](Images/gitc_1410.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图14-10. A) 100参与者组和B) 500参与者组的聚类测试排名。](Images/gitc_1410.png)'
- en: Figure 14-10\. Ranking from the clustering test for A) 100-participant set,
    and B) 500-participant set.
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-10. A) 100参与者组和B) 500参与者组的聚类测试排名。
- en: We considered that this constituted a successful enough reproduction of the
    analysis because we were able to generate a result that matched our expectations,
    even though we had to make compromises in terms of fidelity.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，尽管在忠实度方面做出了一些妥协，但我们能够生成符合预期的结果，因此这被视为分析的成功再现。
- en: We were particularly encouraged to see that this approach could be used to test
    the scaling of statistical power depending on the dataset size. We can easily
    imagine setting up additional experiments to test the scaling observed here to
    a finer granularity and a larger scale. We would also want to test how varying
    the proportion of *NOTCH1* cases relative to the overall cohort size would affect
    the results, as well, starting with the proportion that was actually observed
    in the original study.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别鼓舞人心地看到，这种方法可以用来测试统计能力的扩展，这取决于数据集的大小。我们可以轻松想象设置额外的实验，以测试观察到的扩展到更细的粒度和更大的尺度。我们还希望测试在*NOTCH1*案例相对于整体队列大小的比例变化如何影响结果，从实际观察到的比例开始。
- en: So many possibilities, so little time. And speaking of which, it’s getting late;
    we’re almost at the end of...
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 机会多，时间少。顺便说一句，时间已经不早了；我们几乎到了最后...
- en: The Long, Winding Road to FAIRness
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FAIR原则的漫漫长路
- en: Given what we just described, you should now be able to reproduce the Tetralogy
    of Fallot analysis in Terra as well as reuse the data and/or the methods for your
    own work if they are applicable. You should also be able to do so outside of Terra
    for the most part because most of the components are directly usable on other
    platforms. You can download the data to use on different platforms, and the Docker
    images and WDL workflows can be run just about anywhere. The notebook might take
    more effort to reuse given that the environment is not as cleanly bundled as the
    Docker images used for workflows. You can access all necessary information regarding
    the computing environment, but you would still need to independently set up an
    equivalent environment. We’re hopeful that this is something that will improve
    further over time; for example, imagine if there were an option to emit a Dockerfile
    that could re-create the software environment of a particular notebook. We would
    also like to see a standardized way to obtain a digital object identifier (DOI)
    for a Terra workspace, to use in publications that include a companion workspace
    as supplemental material. This ties in with important work that others are doing
    to bundle research artifacts for easier archival, retrieval, and reuse. It’s a
    complicated topic, however, as tends to be the case when developing standards.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们刚刚描述的内容，现在你应该能够在Terra中复制法洛氏四联症的分析，并在适用时重复使用数据和/或方法。在大部分情况下，你也应该能够在Terra之外做到这一点，因为大多数组件可以直接在其他平台上使用。你可以下载数据以在不同平台上使用，并且Docker镜像和WDL工作流几乎可以在任何地方运行。由于笔记本环境与用于工作流的Docker镜像不同，可能需要更多的努力来重新使用笔记本。你可以访问有关计算环境的所有必要信息，但仍然需要独立设置一个等效的环境。我们希望随着时间的推移，这方面的情况会进一步改善；例如，想象一下如果有一种选择可以生成一个Dockerfile，以重新创建特定笔记本的软件环境。我们还希望看到一种标准化的方法来获取Terra工作空间的数字对象标识符（DOI），以便在包含伴随工作空间作为补充材料的出版物中使用。这与其他人为了更容易地进行归档、检索和重复使用研究工件而进行的重要工作密切相关。然而，当制定标准时，通常情况下会变得更加复杂。
- en: 'So what can you do to achieve computational reproducibility and FAIRness in
    your own work? In the course of this case study, we pointed out several key factors
    that are typically under your direct control. We discussed many of them in the
    context in which they arose, but we thought it might be helpful to provide a summary
    of what we consider to be the most important guidelines:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为了在你自己的工作中实现计算再现性和FAIR性，你可以做些什么呢？在本案例研究过程中，我们指出了几个通常在您直接控制之下的关键因素。我们在它们出现的背景下讨论了许多因素，但我们认为提供一个我们认为最重要的准则总结可能会有所帮助：
- en: Open source
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 开源
- en: 'It’s hard to overstate the importance of using open source tools and workflows.
    Using open source tooling helps keep your work reproducible in two ways: it ensures
    accessibility to all, and it increases the transparency of the methodology. If
    someone isn’t able to run the exact same code for any reason, they still have
    a chance to read the code and reimplement the algorithm.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源工具和工作流的重要性难以言表。使用开源工具在两个方面有助于保证您的工作的再现性：它确保所有人都可以访问，并增加了方法的透明度。如果由于任何原因某人无法运行完全相同的代码，他们仍然有机会阅读代码并重新实现算法。
- en: Version control
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制
- en: It’s imperative that you track the versions of tools, workflows, and notebooks
    you use as systematically as possible because changes from one version to the
    next can have a major effect on how tools work and what results they produce.
    If you are using other people’s tools, record the versions you use and include
    that information when you publish your analysis. If you develop your own tools
    and workflows, make sure to use a version-control system like GitHub. In Terra,
    workflows are systematically versioned, either through GitHub and Dockstore or
    through the internal Methods Repository. Notebooks are currently not versioned
    in Terra, so we recommend downloading your notebooks regularly and checking them
    into a version-control system like GitHub.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能系统地跟踪您使用的工具、工作流和笔记本的版本是至关重要的，因为从一个版本到下一个版本的变化可能会对工具的工作方式和产生的结果产生重大影响。如果您使用其他人的工具，请记录您使用的版本，并在发布分析时包含这些信息。如果您开发自己的工具和工作流，请确保使用像GitHub这样的版本控制系统。在Terra中，工作流通过GitHub和Dockstore或通过内部方法库系统地进行版本控制。目前在Terra中，笔记本并没有进行版本控制，因此我们建议定期下载您的笔记本，并将其提交到像GitHub这样的版本控制系统中。
- en: Automation and portability
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化和可移植性
- en: When it comes to developing the part of your analysis that you will run many
    times on a lot of data, choose a method that will allow you to automate as much
    as possible and reduce your dependence on a specific environment. For example,
    choose a pipelining language like WDL or CWL, which can easily be run by others,
    rather than writing elaborate scripts in Bash, Python, or R that others will have
    a difficult time running even if you provide the code with your publications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到开发你将在大量数据上多次运行的分析部分时，请选择一种能够尽可能自动化并减少对特定环境依赖的方法。例如，选择像WDL或CWL这样的流水线语言，其他人可以轻松运行，而不是编写繁琐的Bash、Python或R脚本，即使你提供了代码，其他人也可能难以运行。
- en: Built-in documentation
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 内置文档
- en: For analyses that involve a lot of interaction with the data and judgment calls
    in how to go from one step to the next, consider providing a Jupyter notebook
    that recapitulates your analysis with built-in commentary explaining what’s happening
    at each step. Even if you prefer working in an environment that gives you more
    flexibility in your day-to-day work, like RStudio, for example, packaging the
    finished product as a Jupyter notebook will greatly enhance both its reproducibility
    and its intelligibility. Think about the last time you helped a new lab member
    or classmate get up to speed on a new analysis method; imagine being able to give
    them a notebook that explained what to do step by step rather than a loose collection
    of scripts and a *README* document that might or might not be up-to-date.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及大量数据交互和判断步骤如何进行的分析，考虑提供一个Jupyter笔记本，重现你的分析，并在每个步骤解释发生的情况。即使你更喜欢在日常工作中提供更灵活性的环境，比如RStudio，将成品打包为Jupyter笔记本也会极大地增强其可重复性和可理解性。想象一下你上次帮助新的实验室成员或同学快速掌握新的分析方法的情况；想象一下能否给他们一个逐步说明该怎么做的笔记本，而不是一堆松散的脚本和一个或许不是最新的*README*文档。
- en: Open data
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 开放数据
- en: Finally, the elephant in the room is often going to be the data. There will
    be many valid reasons you might not be able to share the original data you worked
    with, especially if you’re working with protected human data. However, you might
    be able to refer to open-access data that is sufficient to demonstrate how your
    analysis works. When that’s not an option, as in the case study we just described,
    consider whether it might be possible to use synthetic data instead.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，房间里的大象通常会是数据。你可能无法分享你所处理的原始数据，尤其是当涉及到受保护的人类数据时，有许多合理的理由。然而，你可以考虑使用足以展示你分析工作方式的开放获取数据。当这不是一个选择时，就像我们刚刚描述的案例研究一样，考虑是否可能使用合成数据。
- en: Does that last point mean you should go through what we did to generate the
    synthetic dataset? Well, hopefully not. You’re most welcome to use the synthetic
    exomes we generated, which are freely accessible, or use the workflows we showed
    you to create your own. Our workflows are admittedly not very efficient in their
    present form and would benefit from some optimization to make them cheaper to
    run and more scalable, but they work. And, hey, they’re open source, so feel free
    to play with them and propose some improvements!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一点是否意味着你应该重复我们生成合成数据集的步骤？希望不是。你可以使用我们生成的免费可访问的合成外显子组数据，或者使用我们展示的工作流来创建你自己的数据。我们的工作流在目前的形式下确实不够高效，并且会受益于一些优化，以使其运行成本更低、更具可扩展性，但它们有效。而且，嘿，它们是开源的，所以请随意玩耍并提出一些改进建议！
- en: Looking at the bigger picture, however, we believe there is an opportunity here
    to develop a community resource to minimize the amount of duplicative work that
    anyone needs to do in this space. Considering how standardized the generation
    of sequencing data has become (at least for the short read technologies), it should
    be possible to identify the data types that are most commonly used and generate
    a collection of generic synthetic datasets. These could then be hosted in the
    cloud and used off the shelf in combination with tools like BAMSurgeon by researchers
    who want to reproduce someone else’s work, or to make their own work more readily
    reproducible, along the lines of what we described in this chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广阔的视角下，我们相信这里存在一个机会，可以开发一个社区资源，以减少在这个领域中任何人需要做的重复工作量。考虑到测序数据生成的标准化程度（至少对于短读取技术来说），应该能够识别出最常用的数据类型，并生成一系列通用的合成数据集。这些数据集可以存储在云端，并与诸如BAMSurgeon之类的工具结合使用，研究人员可以用来重现他人的工作，或使自己的工作更容易可重现，与我们在本章中描述的思路类似。
- en: Final Conclusions
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终结论
- en: Well, here we are at the end of the final chapter. How does it feel? You’ve
    covered a lot of ground in the cloud, so to speak. You started on a puny little
    Cloud Shell, and then moved quickly on to a beefier virtual machine, where you
    ran real genomic analyses with GATK; first manually, step by step, and then through
    simple WDL workflows with Cromwell. You applied your inner detective to deciphering
    mystery workflows, learning in the process how to approach new workflows methodically.
    Next, you graduated to using the Pipelines API, scaling up your workflow chops
    and tasting the freedom of launching a parallelized workflow into a wide-open
    space. From there, you jumped to Terra, where you peeled back layers of functionality—workflows,
    notebooks, data management—to finally find yourself on solid ground with our case
    study of a fully reproducible paper.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，我们来到了最后一章的结尾。感觉怎么样？在云端，你走过了很多路程，可以这么说。你从一个微不足道的云Shell开始，然后迅速转向一个更强大的虚拟机，在那里你用GATK运行了真正的基因组分析；首先是手动逐步，然后通过简单的WDL工作流和Cromwell。你运用内在的侦探精神解密神秘的工作流程，学习如何系统地解决新工作流程的方法。接下来，你升级到使用管道API，提升了你的工作流水平，并体验了在广阔空间中启动并行工作流的自由。然后，你跃升到Terra，在那里你逐步揭示了工作流、笔记本、数据管理的各个层面，最终通过我们完全可重现的论文案例研究找到了自己的立足点。
- en: Based on what you learned in this book, you can now head down the path of using
    these tools in your own work. Take advantage of massive cloud-hosted datasets
    and a wealth of Dockerized tools that you don’t need to figure out how to install.
    Develop your own analyses, execute them at scale, and share them with the world—or
    just your labmates. Whether you continue to use the specific tools we used here
    for those purposes, or with other tools within the growing cloud-based life sciences
    ecosystem, you can rest assured that similar principles and paradigms will apply.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你在本书中学到的知识，你现在可以开始在自己的工作中使用这些工具了。利用大规模托管在云端的数据集和大量的Docker化工具，你无需费心去安装它们。开发你自己的分析，规模化地执行它们，并与世界分享——或者只与你的实验室同事分享。无论你继续使用我们在这里为这些目的使用的具体工具，还是使用日益发展的基于云的生命科学生态系统中的其他工具，你可以放心，类似的原则和范例仍然适用。
- en: Be sure to check back for updates to the [book’s GitHub repository](https://oreil.ly/genomics-repo),
    [companion blog](https://oreil.ly/genomics-blog), and latest developments around
    GATK, WDL, Docker, and Terra.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必定期查看[书籍的GitHub仓库](https://oreil.ly/genomics-repo)，[配套博客](https://oreil.ly/genomics-blog)，以及围绕GATK、WDL、Docker和Terra的最新发展。
- en: Have fun, and keep it reproducible!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 玩得开心，并保持可重现性！
- en: ^([1](ch14.xhtml#idm45625611443416-marker)) This case study was originally developed
    by the Support and Education Team in the Data Sciences Platform at the Broad Institute.
    Because that work was led at the time by one of us (Geraldine), we’ve kept the
    description in the first person plural, but we want to be clear that other team
    members and community members contributed directly to the material presented here,
    either toward the development and delivery of the original workshop or in later
    improvements of the materials.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch14.xhtml#idm45625611443416-marker)) 这个案例研究最初由Broad Institute的数据科学平台支持和教育团队开发。因为当时这项工作由我们其中一位（杰拉尔丁）领导，所以我们保留了第一人称复数的描述，但我们要明确其他团队成员和社区成员直接贡献了此处呈现的材料，无论是对原始研讨会的开发和交付，还是后来的材料改进。
- en: '^([2](ch14.xhtml#idm45625611406072-marker)) Page, et al. originally shared
    their manuscript with the research community as a [preprint in bioRxiv](https://oreil.ly/tztSW)
    in April 2018, and ultimately published it in the peer-reviewed journal *Circulation
    Research* in November 2018: “Whole Exome Sequencing Reveals the Major Genetic
    Contributors to Nonsyndromic Tetralogy of Fallot,” *https://doi.org/10.1161/CIRCRESAHA.118.313250*.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch14.xhtml#idm45625611406072-marker)) Page 等最初于2018年4月将他们的手稿作为[生物预印本在
    bioRxiv 上](https://oreil.ly/tztSW)与研究社区共享，并最终于2018年11月在同行评议的期刊*Circulation Research*上发表了这篇文章：“Whole
    Exome Sequencing Reveals the Major Genetic Contributors to Nonsyndromic Tetralogy
    of Fallot”，*https://doi.org/10.1161/CIRCRESAHA.118.313250*。
