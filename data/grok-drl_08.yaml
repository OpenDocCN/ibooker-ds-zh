- en: 8 Introduction to value-based deep reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 基于价值的深度强化学习简介
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容
- en: You will understand the inherent challenges of training reinforcement learning
    agents with non-linear function approximators.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将理解使用非线性函数逼近器训练强化学习代理的内在挑战。
- en: You will create a deep reinforcement learning agent that, when trained from
    scratch with minimal adjustments to hyperparameters, can solve different kinds
    of problems.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将创建一个深度强化学习代理，当从头开始训练并最小调整超参数时，它可以解决不同类型的问题。
- en: You will identify the advantages and disadvantages of using value-based methods
    when solving reinforcement learning problems.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将识别在使用基于价值的方法解决强化学习问题时，其优缺点。
- en: 'Human behavior flows from three main sources: desire, emotion, and knowledge.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 人类行为源于三个主要来源：欲望、情感和知识。
- en: — Plato A philosopher in Classical Greece and founder of the Academy in Athens
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: — 柏拉图，古希腊哲学家，雅典学院创始人
- en: 'We’ve made a great deal of progress so far, and you’re ready to truly grok
    deep reinforcement learning. In chapter 2, you learned to represent problems in
    a way reinforcement learning agents can solve using Markov decision processes
    (MDP). In chapter 3, you developed algorithms that solve these MDPs: that is,
    agents that find optimal behavior in sequential decision-making problems. In chapter
    4, you learned about algorithms that solve one-step MDPs without having access
    to these MDPs. These problems are uncertain because the agents don’t have access
    to the MDP. Agents learn to find optimal behavior through trial-and-error learning.
    In chapter 5, we mixed these two types of problems—sequential and uncertain—so
    we explore agents that learn to evaluate policies. Agents didn’t find optimal
    policies but were able to evaluate policies and estimate value functions accurately.
    In chapter 6, we studied agents that find optimal policies on sequential decision-making
    problems under uncertainty. These agents go from random to optimal by merely interacting
    with their environment and deliberately gathering experiences for learning. In
    chapter 7, we learned about agents that are even better at finding optimal policies
    by getting the most out of their experiences.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经取得了很大的进步，你现在准备好真正深入理解深度强化学习了。在第2章，你学习了如何用马尔可夫决策过程（MDP）的方式表示问题，以便强化学习代理能够解决。在第3章，你开发了解决这些MDP的算法：也就是说，找到序列决策问题中最佳行为的代理。在第4章，你学习了关于解决一步MDP的算法，而这些算法没有访问这些MDP。这些问题是不确定的，因为代理没有访问MDP。代理通过试错学习来学习找到最佳行为。在第5章，我们将这两种类型的问题——序列和不确定——混合在一起，因此我们探索了学习评估策略的代理。代理没有找到最佳策略，但能够准确评估策略和估计价值函数。在第6章，我们研究了在不确定性下，找到序列决策问题中最佳策略的代理。这些代理通过与环境的互动和故意收集经验来学习，从随机到最佳。在第7章，我们学习了通过充分利用经验来找到最佳策略的代理。
- en: Chapter 2 is a foundation for all chapters in this book. Chapter 3 is about
    planning algorithms that deal with sequential feedback. Chapter 4 is about bandit
    algorithms that deal with evaluative feedback. Chapters 5, 6, and 7 are about
    RL algorithms, algorithms that deal with feedback that is simultaneously sequential
    and evaluative. This type of problem is what people refer to as *tabular reinforcement
    learning*. Starting from this chapter, we dig into the details of deep reinforcement
    learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章是本书所有章节的基础。第3章是关于处理序列反馈的规划算法。第4章是关于处理评估反馈的赌博机算法。第5、6、7章是关于强化学习算法，这些算法处理同时具有序列和评估性的反馈。这类问题就是人们所说的*表格强化学习*。从本章开始，我们深入探讨深度强化学习的细节。
- en: More specifically, in this chapter, we begin our incursion into the use of deep
    neural networks for solving reinforcement learning problems. In deep reinforcement
    learning, there are different ways of leveraging the power of highly non-linear
    function approximators, such as deep neural networks. They’re value-based, policy-based,
    actor-critic, model-based, and gradient-free methods. This chapter goes in-depth
    on value-based deep reinforcement learning methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在本章中，我们开始探讨使用深度神经网络解决强化学习问题。在深度强化学习中，有不同方式利用高度非线性的函数逼近器（如深度神经网络）的力量。它们是价值基础、策略基础、演员-评论家、基于模型和无梯度方法。本章深入探讨了基于价值的深度强化学习方法。
- en: '![](../Images/08_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08_01.png)'
- en: Types of algorithmic approaches you learn about in this book
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍的各种算法方法
- en: The kind of feedback deep reinforcement learning agents use
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习代理使用的反馈类型
- en: In deep reinforcement learning, we build agents that are capable of learning
    from feedback that’s simultaneously evaluative, sequential, and sampled. I’ve
    emphasized this throughout the book because you need to understand what that means.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习中，我们构建能够从同时具有评估性、顺序性和样本性的反馈中学习的代理。我在整本书中都强调了这一点，因为你需要理解这意味着什么。
- en: In the first chapter, I mentioned that deep reinforcement learning is about
    complex sequential decision-making problems under uncertainty. You probably thought,
    “What a bunch of words.” But as I promised, all these words mean something. “Sequential
    decision-making problems” is what you learned about in chapter 3\. “Problems under
    uncertainty” is what you learned about in chapter 4\. In chapters 5, 6, and 7,
    you learned about “sequential decision-making problems under uncertainty.” In
    this chapter, we add the “complex” part back to that whole sentence. Let’s use
    this introductory section to review one last time the three types of feedback
    a deep reinforcement learning agent uses for learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我提到深度强化学习是在不确定性下的复杂顺序决策问题。你可能想，“这么多词。”但正如我承诺的，所有这些词都有意义。“顺序决策问题”是你在第三章中学到的。“不确定性下的问题”是你在第四章中学到的。在第五章、第六章和第七章中，你学习了“不确定性下的顺序决策问题”。在这一章中，我们将“复杂”这个词重新加回到整个句子中。让我们利用这个介绍部分再次回顾一下深度强化学习代理使用的三种类型的学习反馈。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownKinds of feedback in deep reinforcement
    learning |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 深度强化学习中反馈的类型 |'
- en: '|  | **Sequential** (as opposed to one-shot) | **Evaluative** (as opposed to
    supervised) | **Sampled** (as opposed to exhaustive) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | **顺序的**（与一次性相对） | **评估的**（与监督学习相对） | **样本的**（与穷举相对） |'
- en: '| **Supervised learning** | × | × | ✓ |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **监督学习** | × | × | ✓ |'
- en: '| **Planning** (Chapter 3) | ✓ | × | × |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **规划**（第三章） | ✓ | × | × |'
- en: '| **Bandits** (Chapter 4) | × | ✓ | × |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **多臂老虎机**（第四章） | × | ✓ | × |'
- en: '| **Tabular reinforcement learning** (Chapters 5, 6, 7) | ✓ | ✓ | × |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **表格式强化学习**（第五章、第六章、第七章） | ✓ | ✓ | × |'
- en: '| **Deep reinforcement learning** (Chapters 8, 9, 10, 11, 12) | ✓ | ✓ | ✓ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **深度强化学习**（第八章、第九章、第十章、第十一章、第十二章） | ✓ | ✓ | ✓ |'
- en: Deep reinforcement learning agents deal with sequential feedback
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理处理顺序反馈
- en: Deep reinforcement learning agents have to deal with sequential feedback. One
    of the main challenges of sequential feedback is that your agents can receive
    delayed information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习代理必须处理顺序反馈。顺序反馈的一个主要挑战是代理可以接收延迟信息。
- en: You can imagine a chess game in which you make a few wrong moves early on, but
    the consequences of those wrong moves only manifest at the end of the game when
    and if you materialize a loss.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象一场棋局，你一开始走了几步错棋，但这些错棋的后果只有在游戏结束时才会显现，如果你真的输掉了比赛。
- en: Delayed feedback makes it tricky to interpret the source of the feedback. Sequential
    feedback gives rise to the temporal credit assignment problem, which is the challenge
    of determining which state, action, or state-action pair is responsible for a
    reward. When there’s a temporal component to a problem and actions have delayed
    consequences, it becomes challenging to assign credit for rewards.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟反馈使得难以解释反馈的来源。顺序反馈引发了时间信用分配问题，这是确定哪个状态、动作或状态-动作对负责奖励的挑战。当问题有时间成分且动作有延迟后果时，为奖励分配信用变得具有挑战性。
- en: '![](../Images/08_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08_02.png)'
- en: Sequential feedback
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序反馈
- en: But, if it isn’t sequential, what is it?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 但是，如果不是顺序的，那会是什么呢？
- en: The opposite of delayed feedback is immediate feedback. In other words, the
    opposite of sequential feedback is one-shot feedback. In problems that deal with
    one-shot feedback, such as supervised learning or multi-armed bandits, decisions
    don’t have long-term consequences. For example, in a classification problem, classifying
    an image, whether correctly or not, has no bearing on future performance; for
    instance, the images presented in the next model are not any different whether
    the model classified the previous batch correctly or not. In DRL, this sequential
    dependency exists.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟反馈的反面是即时反馈。换句话说，顺序反馈的反面是一次性反馈。在处理一次性反馈的问题中，如监督学习或多臂老虎机，决策没有长期后果。例如，在分类问题中，无论正确与否对图像进行分类，都不会影响未来的性能；例如，在下一个模型中展示的图像与模型是否正确分类前一个批次无关。在深度强化学习中，这种顺序依赖性存在。
- en: '![](../Images/08_03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_03.png)'
- en: Classification problem
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题
- en: Moreover, in bandit problems, there’s also no long-term consequence, though
    it's perhaps a bit harder to see why. Bandits are one-state one-step MDPs in which
    episodes terminate immediately after a single action selection. Therefore, actions
    don’t have long-term consequences in the performance of the agent during that
    episode.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在多臂老虎机问题中，也没有长期后果，尽管这可能有点难以看到原因。老虎机是一状态一步马尔可夫决策过程（MDP），其中在单次动作选择后立即结束。因此，在该次期间，动作没有长期后果。
- en: '![](../Images/08_04.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_04.png)'
- en: Two-armed bandit
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 双臂老虎机
- en: Deep reinforcement learning agents deal with evaluative feedback
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理处理评估反馈
- en: The second property we learned about is that of evaluative feedback. Deep reinforcement
    learning, tabular reinforcement learning, and bandits all deal with evaluative
    feedback. The crux of evaluative feedback is that the goodness of the feedback
    is only relative, because the environment is uncertain. We don’t know the actual
    dynamics of the environment; we don’t have access to the transition function and
    reward signal.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到的第二个特性是评估反馈。深度强化学习、表格强化学习和老虎机都处理评估反馈。评估反馈的核心是反馈的好坏只是相对的，因为环境是不确定的。我们不知道环境的实际动态；我们没有访问到转换函数和奖励信号。
- en: As a result, we must explore the environment around us to find out what’s out
    there. The problem is that, by exploring, we miss capitalizing on our current
    knowledge and, therefore, likely accumulate regret. Out of all this, the exploration-exploitation
    trade-off arises. It’s a constant by-product of uncertainty. While not having
    access to the model of the environment, we must explore to gather new information
    or improve on our current information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须探索我们周围的环境，以了解那里有什么。问题是，通过探索，我们错过了利用我们当前的知识，因此很可能会积累遗憾。从所有这些中，探索-利用权衡产生了。它是不确定性的一个持续副产品。在没有访问到环境模型的情况下，我们必须探索以收集新信息或改进当前信息。
- en: '![](../Images/08_05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_05.png)'
- en: Evaluative feedback
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 评估反馈
- en: But, if it isn’t evaluative, what is it?
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 但是，如果不是评估性的，那它是什么呢？
- en: The opposite of evaluative feedback is supervised feedback. In a classification
    problem, your model receives supervision; that is, during learning, your model
    is given the correct labels for each of the samples provided. There’s no guessing.
    If your model makes a mistake, the correct answer is provided immediately afterward.
    What a good life!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 评估反馈的对立面是监督反馈。在分类问题中，你的模型接收监督；也就是说，在学习过程中，你的模型会为提供的每个样本提供正确的标签。没有猜测的空间。如果你的模型犯了错误，正确的答案会立即提供。多么美好的生活啊！
- en: '![](../Images/08_06.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_06.png)'
- en: Classification is “supervised”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是“监督的”
- en: The fact that correct answers are given to the learning algorithm makes supervised
    feedback much easier to deal with than evaluative feedback. That’s a clear distinction
    between supervised learning problems and evaluative-feedback problems, such as
    multi-armed bandits, tabular reinforcement learning, and deep reinforcement learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将正确答案提供给学习算法的事实使得监督反馈比评估反馈更容易处理。这是监督学习问题与评估反馈问题（如多臂老虎机、表格强化学习和深度强化学习）之间的一个明显区别。
- en: Bandit problems may not have to deal with sequential feedback, but they do learn
    from evaluative feedback. That’s the core issue bandit problems solve. When under
    evaluative feedback, agents must balance exploration versus exploitation requirements.
    If the feedback is evaluative and sequential at the same time, the challenge is
    even more significant. Algorithms must simultaneously balance immediate- and long-term
    goals and the gathering and utilization of information. Both tabular reinforcement
    learning and DRL agents learn from feedback that’s simultaneously sequential and
    evaluative.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机问题可能不需要处理顺序反馈，但它们确实从评估反馈中学习。这是多臂老虎机问题要解决的问题的核心。在评估反馈下，代理必须平衡探索和利用的需求。如果反馈既是评估性的又是顺序的，那么挑战就更大了。算法必须同时平衡短期和长期目标以及信息的收集和利用。表格强化学习和深度强化学习代理都从同时是顺序性和评估性的反馈中学习。
- en: '![](../Images/08_07.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_07.png)'
- en: Bandits deal with evaluative feedback
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 老虎机处理评估反馈
- en: Deep reinforcement learning agents deal with sampled feedback
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理处理采样反馈
- en: What differentiates deep reinforcement learning from tabular reinforcement learning
    is the complexity of the problems. In deep reinforcement learning, agents are
    unlikely to sample all possible feedback exhaustively. Agents need to generalize
    using the gathered feedback and come up with intelligent decisions based on that
    generalization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习与表格强化学习区别在于问题的复杂性。在深度强化学习中，智能体不太可能全面地采样所有可能的反馈。智能体需要使用收集到的反馈进行推广，并基于这种推广做出明智的决策。
- en: Think about it. You can’t expect exhaustive feedback from life. You can’t be
    a doctor and a lawyer and an engineer all at once, at least not if you want to
    be good at any of these. You must use the experience you gather early on to make
    more intelligent decisions for your future. It’s basic. Were you good at math
    in high school? Great, then, pursue a math-related degree. Were you better at
    the arts? Then, pursue that path. Generalizing helps you narrow your path going
    forward by helping you find patterns, make assumptions, and connect the dots that
    help you reach your optimal self.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 想想看。你不能期望从生活中得到全面反馈。你不能同时成为一名医生、律师和工程师，至少如果你想在任何一个领域都做得很好，你不能。你必须利用你早期积累的经验来为你的未来做出更明智的决策。这是基本的。你在高中数学好吗？很好，那么，就追求一个与数学相关的学位。你在艺术方面更擅长？那么，就选择那条道路。推广可以帮助你通过帮助你找到模式、做出假设和连接帮助你达到最佳自我的点来缩小你前进的道路。
- en: 'By the way, supervised learning deals with sampled feedback. Indeed, the core
    challenge in supervised learning is to learn from sampled feedback: to be able
    to generalize to new samples, which is something neither multi-armed bandit nor
    tabular reinforcement learning problems do.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，监督学习处理的是样本反馈。确实，监督学习的核心挑战是从样本反馈中学习：能够推广到新的样本，这是多臂老虎机和表格强化学习问题都无法做到的。
- en: '![](../Images/08_08.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_08.png)'
- en: Sampled feedback
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 样本反馈
- en: But, if it isn’t sampled, what is it?
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 但是，如果不是样本化的，那会是什么呢？
- en: The opposite of sampled feedback is exhaustive feedback. To exhaustively sample
    environments means agents have access to all possible samples. Tabular reinforcement
    learning and bandits agents, for instance, only need to sample for long enough
    to gather all necessary information for optimal performance. To gather exhaustive
    feedback is also why there are optimal convergence guarantees in tabular reinforcement
    learning. Common assumptions, such as “infinite data” or “sampling every state-action
    pair infinitely often,” are reasonable assumptions in small grid worlds with finite
    state and action spaces.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 样本反馈的对立面是全面反馈。要全面地采样环境意味着智能体可以访问所有可能的样本。例如，表格强化学习和老虎机智能体只需要采样足够长的时间来收集所有必要的信息以实现最佳性能。收集全面反馈也是表格强化学习中存在最优收敛保证的原因。在有限的状态和动作空间的小网格世界中，常见的假设，如“无限数据”或“无限频繁地采样每个状态-动作对”，是合理的假设。
- en: '![](../Images/08_09.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_09.png)'
- en: Sequential, evaluative, and exhaustive feedback
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序性、评估性和全面反馈
- en: This dimension we haven’t dealt with until now. In this book so far, we surveyed
    the tabular reinforcement learning problem. Tabular reinforcement learning learns
    from evaluative, sequential, and exhaustive feedback. But, what happens when we
    have more complex problems in which we cannot assume our agents will ever exhaustively
    sample environments? What if the state space is high dimensional, such as a Go
    board with 10^(170) states? How about Atari games with (255³)^(210) × ^(160) at
    60 Hz? What if the environment state space has continuous variables, such as a
    robotic arm indicating joint angles? How about problems with both high-dimensional
    and continuous states or even high-dimensional and continuous actions? These complex
    problems are the reason for the existence of the field of deep reinforcement learning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前还没有处理过这个维度。到目前为止，在这本书中，我们调查了表格强化学习问题。表格强化学习从评估性、顺序性和全面反馈中学习。但是，当我们遇到我们无法假设智能体将全面采样环境的更复杂问题时会发生什么？如果状态空间是高维的，比如有10^(170)个状态的围棋棋盘呢？关于以60
    Hz运行(255³)^(210) × ^(160)的Atari游戏呢？如果环境状态空间有连续变量，比如表示关节角度的机械臂呢？关于具有高维和连续状态或甚至高维和连续动作的问题呢？这些复杂问题是深度强化学习领域存在的原因。
- en: Introduction to function approximation for reinforcement learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习函数逼近简介
- en: It’s essential to understand why we use function approximation for reinforcement
    learning in the first place. It’s common to get lost in words and pick solutions
    due to the hype. You know, if you hear “deep learning,” you get more excited than
    if you hear “non-linear function approximation,” yet they’re the same. That’s
    human nature. It happens to me; it happens to many, I’m sure. But our goal is
    to remove the cruft and simplify our thinking.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么我们首先使用函数逼近来解决强化学习问题至关重要。人们常常因为词汇而迷失方向，并因为炒作而选择解决方案。你知道，如果你听到“深度学习”，你会比听到“非线性函数逼近”更兴奋，尽管它们是相同的。这是人的本性。这发生在我身上，我相信它发生在许多人身上。但我们的目标是去除冗余，简化我们的思考。
- en: In this section, I provide motivations for the use of function approximation
    to solve reinforcement learning problems in general. Perhaps a bit more specific
    to value functions, than RL overall, but the underlying motivation applies to
    all forms of DRL.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我提供了使用函数逼近来解决强化学习问题的一般动机。也许比RL整体更具体一些，但基本的动机适用于所有形式的DRL。
- en: Reinforcement learning problems can have high-dimensional state and action spaces
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习问题可以具有高维的状态空间和动作空间
- en: The main drawback of tabular reinforcement learning is that the use of a table
    to represent value functions is no longer practical in complex problems. Environments
    can have high-dimensional state spaces, meaning that the number of variables that
    comprise a single state is vast. For example, Atari games described above are
    high dimensional because of the 210-by-160 pixels and the three color channels.
    Regardless of the values that these pixels can take when we talk about dimensionality,
    we’re referring to the number of variables that make up a single state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表格式强化学习的主要缺点是，在复杂问题中使用表格来表示价值函数已不再实用。环境可以具有高维的状态空间，这意味着构成单个状态变量的数量非常庞大。例如，上面提到的Atari游戏是高维的，因为它们有210×160像素和三个颜色通道。无论我们谈论维度时这些像素可以取什么值，我们指的是构成单个状态变量的数量。
- en: '![](../Images/08_10.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_10.png)'
- en: High-dimensional state spaces
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 高维状态空间
- en: Reinforcement learning problems can have continuous state and action spaces
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习问题可以具有连续的状态空间和动作空间
- en: Environments can additionally have continuous variables, meaning that a variable
    can take on an infinite number of values. To clarify, state and action spaces
    can be high dimensional with discrete variables, they can be low dimensional with
    continuous variables, and so on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 环境还可以有连续变量，这意味着一个变量可以取无限多个值。为了澄清，状态空间和动作空间可以是高维的，包含离散变量，也可以是低维的，包含连续变量，等等。
- en: Even if the variables aren’t continuous and, therefore, not infinitely large,
    they can still take on a large number of values to make it impractical for learning
    without function approximation. This is the case with Atari, for instance, where
    each image-pixel can take on 256 values (0–255 integer values.) There you have
    a finite state-space, yet large enough to require function approximation for any
    learning to occur.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 即使变量不是连续的，因此不是无限大的，它们仍然可以取大量的值，使得没有函数逼近就无法学习。例如，Atari的情况就是这样，每个图像像素可以取256个值（0-255的整数值。）那里有一个有限的状态空间，但足够大，以至于需要函数逼近才能进行任何学习。
- en: 'But, sometimes, even low-dimension state spaces can be infinitely large state
    spaces. For instance, imagine a problem in which only the x, y, z coordinates
    of a robot compose the state-space. Sure, a three-variable state-space is a pretty
    low-dimensional state-space environment, but what if any of the variables is provided
    in continuous form, that is, that variable can be of infinitesimal precision?
    Say, it could be a 1.56, or 1.5683, or 1.5683256, and so on. Then, how do you
    make a table that takes all these values into account? Yes, you could discretize
    the state space, but let me save you time and get right to it: you need function
    approximation.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有时，即使是低维状态空间也可以是无限大的状态空间。例如，想象一个只有机器人的x、y、z坐标组成状态空间的问题。当然，三个变量的状态空间是一个相当低维的状态空间环境，但如果任何一个变量以连续形式提供，也就是说，该变量可以是无限小的精度呢？比如说，它可以是1.56，或者1.5683，或者1.5683256，等等。那么，你如何制作一个考虑所有这些值的表格呢？是的，你可以对状态空间进行离散化，但让我节省你的时间，直接说：你需要函数逼近。
- en: '![](../Images/08_11.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_11.png)'
- en: Continuous state spaces
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 连续状态空间
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe cart-pole environment
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ![图标](../Images/icons_Concrete.png) | 一个具体例子小车-杆环境 |'
- en: '|  | The cart-pole environment is a classic in reinforcement learning. The
    state space is low dimensional but continuous, making it an excellent environment
    for developing algorithms; training is fast, yet still somewhat challenging, and
    function approximation can help.![](../Images/08_11_Sidebar02.png)This is the
    cart-pole environmentIts state space is comprised of four variables:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 小车-杆环境是强化学习中的一个经典案例。状态空间低维但连续，使其成为开发算法的绝佳环境；训练速度快，但仍有一定挑战性，函数逼近可以帮助！[](../Images/08_11_Sidebar02.png)这是小车-杆环境其状态空间由四个变量组成：'
- en: The cart position on the track (x-axis) with a range from –2.4 to 2.4
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车在轨道上的位置（x 轴）范围从 -2.4 到 2.4
- en: The cart velocity along the track (x-axis) with a range from –inf to inf
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车沿轨道（x 轴）的速度范围从 -inf 到 inf
- en: The pole angle with a range of ~–40 degrees to ~ 40 degrees
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆的角度范围大约在 ~-40 度到 ~40 度之间
- en: The pole velocity at the tip with a range of –inf to inf
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆尖的速度范围从 -inf 到 inf
- en: 'There are two available actions in every state:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个状态下都有两个可用的动作：
- en: Action 0 applies a –1 force to the cart (push it left)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Action 0 对小车施加 -1 的力（向左推）
- en: Action 1 applies a +1 force to the cart (push it right)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Action 1 对小车施加 +1 的力（向右推）
- en: You reach a terminal state if
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你达到终端状态
- en: The pole angle is more than 12 degrees away from the vertical position
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆的角度与垂直位置超过 12 度
- en: The cart center is more than 2.4 units from the center of the track
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车中心距离轨道中心超过 2.4 个单位
- en: The episode count reaches 500 time steps (more on this later)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 播放次数达到 500 个时间步（关于这一点稍后讨论）
- en: The reward function is
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数是
- en: +1 for every time step
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时间步 +1
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: There are advantages when using function approximation
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用函数逼近有优势
- en: I’m sure you get the point that in environments with high-dimensional or continuous
    state spaces, there are no practical reasons for not using function approximation.
    In earlier chapters, we discussed planning and reinforcement learning algorithms.
    All of those methods represent value functions using tables.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你明白了，在高维或连续状态空间的环境中，没有实际的理由不使用函数逼近。在早期章节中，我们讨论了规划和强化学习算法。所有这些方法都使用表格来表示值函数。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryAlgorithms such as value
    iteration and Q-learning use tables for value functions |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Memory.png) | 刷新我的记忆值迭代和 Q-learning 等算法使用表格来表示值函数 |'
- en: '|  | Value iteration is a method that takes in an MDP and derives an optimal
    policy for such MDP by calculating the optimal state-value function, *v**. To
    do this, value iteration keeps track of the changing state-value function, *v*,
    over multiple iterations. In value iteration, the state-value function estimates
    are represented as a vector of values indexed by the states. This vector is stored
    with a lookup table for querying and updating estimates.![](../Images/08_11_Sidebar03a.png)A
    state-value functionThe Q-learning algorithm does not need an MDP and doesn’t
    use a state-value function. Instead, in Q-learning, we estimate the values of
    the optimal action-value function, ***. Action-value functions are not vectors,
    but, instead, are represented by matrices. These matrices are 2D tables indexed
    by states and actions.![](../Images/08_11_Sidebar03b.png)An action-value function
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 值迭代是一种方法，它接受一个马尔可夫决策过程（MDP），并通过计算最优状态值函数 *v* 来推导出此类 MDP 的最优策略。为此，值迭代在多次迭代中跟踪状态值函数
    *v* 的变化。在值迭代中，状态值函数的估计表示为按状态索引的值向量。这个向量存储在查找表中，用于查询和更新估计值！[](../Images/08_11_Sidebar03a.png)状态值函数Q-learning
    算法不需要 MDP，也不使用状态值函数。相反，在 Q-learning 中，我们估计最优动作值函数的值，***。动作值函数不是向量，而是由矩阵表示。这些矩阵是按状态和动作索引的二维表。！[](../Images/08_11_Sidebar03b.png)动作值函数
    |'
- en: '| ![](../Images/icons_Boil.png) | Boil It DownFunction approximation can make
    our algorithms more efficient |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化内容函数逼近可以使我们的算法更高效 |'
- en: '|  | In the cart-pole environment, we want to use generalization because it’s
    a more efficient use of experiences. With function approximation, agents learn
    and exploit patterns with less data (and perhaps faster).![](../Images/08_11_Sidebar04.png)A
    state-value function with and without function approximation |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 在小车-杆环境中，我们希望使用泛化，因为这是更有效利用经验的方法。通过函数逼近，代理可以用更少的数据（也许更快）学习和利用模式！[](../Images/08_11_Sidebar04.png)带有和没有函数逼近的状态值函数
    |'
- en: While the inability of value iteration and Q-learning to solve problems with
    sampled feedback make them impractical, the lack of generalization makes them
    inefficient. What I mean by this is that we could find ways to use tables in environments
    with continuous-variable states, but we’d pay a price for doing so. Discretizing
    values could indeed make tables possible, for instance. But, even if we could
    engineer a way to use tables and store value functions, by doing so, we’d miss
    out on the advantages of generalization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然价值迭代和Q学习在具有采样反馈的问题上无法解决问题，这使得它们不实用，但缺乏泛化使得它们效率低下。我的意思是，我们可以找到在具有连续变量状态的环境中使用表格的方法，但我们会为此付出代价。离散化值确实可以使表格成为可能，例如。但是，即使我们能够设计出一种使用表格并存储价值函数的方法，通过这样做，我们会失去泛化的优势。
- en: For example, in the cart-pole environment, function approximation would help
    our agents learn a relationship in the x distance. Agents would likely learn that
    being 2.35 units away from the center is a bit more dangerous than being 2.2 away.
    We know that 2.4 is the x boundary. This additional reason for using generalization
    isn’t to be understated. Value functions often have underlying relationships that
    agents can learn and exploit. Function approximators, such as neural networks,
    can discover these underlying relationships.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在购物车-杆环境中，函数逼近可以帮助我们的智能体学习x距离中的关系。智能体可能会发现，距离中心2.35个单位比距离中心2.2个单位更危险。我们知道2.4是x边界。这个使用泛化的额外理由不容忽视。价值函数通常具有智能体可以学习和利用的潜在关系。函数逼近器，如神经网络，可以发现这些潜在关系。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownReasons for using function approximation
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化原因：使用函数逼近的理由'
- en: '|  | Our motivation for using function approximation isn’t only to solve problems
    that aren’t solvable otherwise, but also to solve problems more efficiently. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们使用函数逼近的动机不仅是为了解决其他方法无法解决的问题，而且是为了更有效地解决问题。'
- en: 'NFQ: The first attempt at value-based deep reinforcement learning'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NFQ：基于价值的深度强化学习的第一次尝试
- en: The following algorithm is called *neural fitted Q* (NFQ) *iteration*, and it’s
    probably one of the first algorithms to successfully use neural networks as a
    function approximation to solve reinforcement learning problems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下算法被称为 *神经拟合Q* (NFQ) *迭代*，它可能是第一个成功使用神经网络作为函数逼近来解决强化学习问题的算法之一。
- en: For the rest of this chapter, I discuss several components that most value-based
    deep reinforcement learning algorithms have. I want you to see it as an opportunity
    to decide on different parts that we could’ve used. For instance, when I introduce
    using a loss function with NFQ, I discuss a few alternatives. My choices aren’t
    necessarily the choices that were made when the algorithm was originally introduced.
    Likewise, when I choose an optimization method, whether root mean square propagation
    (RMSprop) or adaptive moment estimation (Adam), I give a reason why I use what
    I use, but more importantly, I give you context so you can pick and choose as
    you see fit.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我将讨论大多数基于价值的深度强化学习算法都具有的几个组件。我希望你能将其视为一个机会，决定我们可以使用的不同部分。例如，当我介绍使用NFQ的损失函数时，我会讨论一些替代方案。我的选择并不一定是算法最初引入时所做的选择。同样，当我选择优化方法，无论是均方根传播
    (RMSprop) 还是自适应矩估计 (Adam) 时，我会给出我使用的原因，但更重要的是，我会给你提供背景信息，以便你可以根据自己的需要选择和决定。
- en: What I hope you notice is that my goal is not only to teach you this specific
    algorithm but, more importantly, to show you the different places where you could
    try different things. Many RL algorithms feel this “plug-and-play” way, so pay
    attention.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能注意到，我的目标不仅仅是教你这个特定的算法，更重要的是，向你展示你可以尝试不同事物的不同地方。许多RL算法都有这种“即插即用”的感觉，所以请注意。
- en: 'First decision point: Selecting a value function to approximate'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一个决策点：选择一个要逼近的价值函数
- en: Using neural networks to approximate value functions can be done in many different
    ways. To begin with, there are many different value functions we could approximate.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络逼近价值函数可以以许多不同的方式完成。首先，我们可以逼近许多不同的价值函数。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryValue functions |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Memory.png) | 刷新我的记忆：价值函数'
- en: '|  | You’ve learned about the following value functions:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 你已经学习了以下价值函数：'
- en: The state-value function *v*(*s*)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态价值函数 *v*(*s*)
- en: The action-value function *q*(*s,a*)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作价值函数 *q*(*s,a*)
- en: The action-advantage function *a*(*s,a*)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作优势函数 *a*(*s,a*)
- en: 'You probably remember that the state-value function *v*(*s*), though useful
    for many purposes, isn’t sufficient on its own to solve the control problem. Finding
    *v*(*s*) helps you know how much expected total discounted reward you can obtain
    from state s and using policy *π* thereafter. But, to determine which action to
    take with a V-function, you also need the MDP of the environment so that you can
    do a one-step look-ahead and take into account all possible next states after
    selecting each action.You likely also remember that the action-value function
    (*s,a*) allows us to solve the control problem, so it’s more like what we need
    to solve the cart-pole environment: in the cart-pole environment, we want to learn
    the values of actions for all states in order to balance the pole by controlling
    the cart. If we had the values of state-action pairs, we could differentiate the
    actions that would lead us to either gain information, in the case of an exploratory
    action, or maximize the expected return, in the case of a greedy action.I want
    you to notice, too, that what we want to estimate the optimal action-value function
    and not just an action-value function. However, as we learned in the generalized
    policy iteration pattern, we can do on-policy learning using an epsilon-greedy
    policy and estimate its values directly, or we can do off-policy learning and
    always estimate the policy greedy with respect to the current estimates, which
    then becomes an optimal policy.Last, we also learned about the action-advantage
    function *a(s,a),* which can help us differentiate between values of different
    actions, and it also lets us easily see how much better than average an action
    is. |'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，状态值函数 *v(s*)，虽然对许多目的来说很有用，但仅凭它本身并不能解决控制问题。找到 *v(s*) 帮助你知道从状态 s 出发并使用策略
    *π* 后可以获得多少期望的总折现奖励。但是，为了确定使用 V 函数采取哪个动作，你还需要环境的 MDP，这样你就可以进行一步前瞻，并在选择每个动作后考虑所有可能的后继状态。你可能也记得，动作值函数
    (*s,a*) 允许我们解决控制问题，所以它更像是我们需要解决小车-杆环境的问题：在小车-杆环境中，我们希望通过控制小车来学习所有状态的动作值，以平衡杆。如果我们有了状态-动作对的值，我们就可以区分那些会导致我们获得信息（在探索性动作的情况下）或最大化期望回报（在贪婪动作的情况下）的动作。我还想让你注意到，我们想要估计的是最优动作值函数，而不仅仅是动作值函数。然而，正如我们在广义策略迭代模式中学到的，我们可以使用
    ε-贪婪策略进行在线学习并直接估计其值，或者我们可以进行离线学习，并始终估计相对于当前估计的贪婪策略，这随后成为最优策略。最后，我们还学习了动作优势函数 *a(s,a)*，它可以帮助我们区分不同动作的值，并让我们很容易地看到动作的平均优势。|
- en: We’ll study how to use the *v(s)* and *a(s)* functions in a few chapters. For
    now, let’s settle on estimating the action-value function (*s,a*), just like in
    Q-learning. We refer to the approximate action-value function estimate as *Q*(*s,a;*
    *θ*), which means the Q estimates are parameterized by *θ*, the weights of a neural
    network, a state *s* and an action *a*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几章中研究如何使用 *v(s)* 和 *a(s)* 函数。现在，让我们确定估计动作值函数 (*s,a*)，就像在 Q-learning
    中一样。我们将近似动作值函数的估计称为 *Q*(*s,a;* *θ*)，这意味着 Q 估计由 *θ* 参数化，即神经网络的权重，状态 *s* 和动作 *a*。
- en: 'Second decision point: Selecting a neural network architecture'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二个决策点：选择神经网络架构
- en: We settled on learning the approximate action-value function *Q(s,a;* *θ*).
    But although I suggested the function should be parameterized by *θ**, s,* and
    *a*, that doesn’t have to be the case. The next component we discuss is the neural
    network architecture.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定学习近似动作值函数 *Q(s,a;* *θ*）。尽管我建议该函数应该由 *θ*、s 和 *a* 参数化，但这并不一定非得如此。接下来我们要讨论的是神经网络架构。
- en: '![](../Images/08_12.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_12.png)'
- en: State-action-in-value-out architecture
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 状态-动作-值-输出架构
- en: When we implemented the Q-learning agent, you noticed how the matrix holding
    the action-value function was indexed by state and action pairs. A straightforward
    neural network architecture is to input the state (the four state variables in
    the cart-pole environment), and the action to evaluate. The output would then
    be one node representing the Q-value for that state-action pair.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实现 Q-learning 代理时，你注意到了包含动作值函数的矩阵是如何由状态和动作对索引的。一个直接的神经网络架构是输入状态（小车-杆环境中的四个状态变量），以及要评估的动作。输出将是代表该状态-动作对的
    Q 值的一个节点。
- en: '![](../Images/08_13.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_13.png)'
- en: State-in-values-out architecture
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 状态-值-输出架构
- en: This architecture would work fine for the cart-pole environment. But, a more
    efficient architecture consists of only inputting the state (four for the cart-pole
    environment) to the neural network and outputting the Q-values for all the actions
    in that state (two for the cart-pole environment). This is clearly advantageous
    when using exploration strategies such as epsilon-greedy or softmax, because having
    to do only one pass forward to get the values of all actions for any given state
    yields a high-performance implementation, more so in environments with a large
    number of actions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构对于小车-杆环境来说效果很好。但是，一个更高效的架构只将状态（对于小车-杆环境是四个）输入到神经网络，并输出该状态下所有动作的Q值（对于小车-杆环境是两个）。当使用epsilon-greedy或softmax等探索策略时，这显然是有优势的，因为只需要进行一次前向传递就能得到任何给定状态下所有动作的值，这在动作数量众多的环境中尤其如此。
- en: 'For our NFQ implementation, we use the *state-in-values-out architecture*:
    that is, four input nodes and two output nodes for the cart-pole environment.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的NFQ实现，我们使用的是 *状态-值-输出架构*：也就是说，对于小车-杆环境，有四个输入节点和两个输出节点。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonFully connected Q-function
    (state-in-values-out) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python全连接Q函数（状态-值-输出） |'
- en: '|  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Here you are just defining the input layer. See how we take in input_dim and
    output the first element of the hidden_dims vector.② We then create the hidden
    layers. Notice how flexible this class is in that it allows you to change the
    number of layers and units per layer. Pass a different tuple, say (64, 32, 16),
    to the hidden_dims variable, and it will create a network with three hidden layers
    of 64, 32, and 16 units, respectively.③ We then connect the last hidden layer
    to the output layer.④ In the forward function, we first take in the raw state
    and convert it into a tensor.⑤ We pass it through the input layer and then through
    the activation function.⑥ Then we do the same for all hidden layers.⑦ And finally,
    for the output layer, notice that we don’t apply the activation function to the
    output but return it directly instead. |
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这里你只是在定义输入层。看看我们如何接收input_dim并输出hidden_dims向量的第一个元素。② 我们接着创建隐藏层。注意这个类是多么灵活，它允许你改变层数和每层的单元数。将不同的元组（例如（64，32，16））传递给hidden_dims变量，它将创建一个具有三个隐藏层（64、32和16个单元）的网络。③
    然后我们将最后一个隐藏层连接到输出层。④ 在前向函数中，我们首先接收原始状态并将其转换为张量。⑤ 我们将其通过输入层，然后通过激活函数。⑥ 然后对所有的隐藏层做同样的处理。⑦
    最后，对于输出层，注意我们没有对输出应用激活函数，而是直接返回它。|
- en: 'Third decision point: Selecting what to optimize'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三决策点：选择要优化的内容
- en: Let’s pretend for a second that the cart-pole environment is a supervised learning
    problem. Say you have a dataset with states as inputs and a value function as
    labels. Which value function would you wish to have for labels?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设一下，小车-杆环境是一个监督学习问题。假设你有一个包含状态作为输入和值函数作为标签的数据集。你希望标签中包含哪个值函数？
- en: '| ![](../Images/icons_Math.png) | Show Me The MathIdeal objective |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 显示我数学理想目标 |'
- en: '|  | ![](../Images/08_13_Sidebar08.png) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏08](../Images/08_13_Sidebar08.png) |'
- en: Of course, the dream labels for learning the optimal action-value function are
    the corresponding optimal Q-values (notice that a lowercase q refers to the true
    values; uppercase is commonly used to denote estimates) for the state-action input
    pair. That is exactly what the optimal action-value function *q**(*s,a*) represents,
    as you know.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，学习最优动作值函数的理想标签是对应的状态-动作输入对的优化Q值（注意，小写q指的是真实值；大写通常用来表示估计）。这正是您所知道的最优动作值函数
    *q**(*s,a*) 所表示的内容。
- en: If we had access to the optimal action-value function, we’d use that, but if
    we had access to sampling the optimal action-value function, we could then minimize
    the loss between the approximate and optimal action-value functions, and that
    would be it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能访问最优动作值函数，我们会使用它，但如果我们能采样最优动作值函数，我们就可以最小化近似动作值函数和最优动作值函数之间的损失，这样就可以了。
- en: The optimal action-value function is what we’re after.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们追求的是最优动作值函数。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryOptimal action-value function
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ![记忆图标](../Images/icons_Memory.png) | 刷新我的记忆最优动作值函数 |'
- en: '|  | ![](../Images/08_13_Sidebar09.png) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏09](../Images/08_13_Sidebar09.png) |'
- en: But why is this an impossible dream? Well, the visible part is we don’t have
    the optimal action-value function *(s,a),* but to top that off, we cannot even
    sample these optimal Q-values because we don’t have the optimal policy either.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么这是一个不可能的梦想呢？好吧，明显的是我们没有最优的动作值函数 *(s,a)*，但更糟糕的是，我们甚至无法采样这些最优Q值，因为我们也没有最优策略。
- en: Fortunately, we can use the same principles learned in generalized policy iteration
    in which we alternate between policy-evaluation and policy-improvement processes
    to find good policies. But just so you know, because we’re using non-linear function
    approximation, convergence guarantees no longer exist. It’s the Wild West of the
    “deep” world.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用在广义策略迭代中学习到的相同原则，在这些原则中我们交替进行策略评估和策略改进过程以找到好的策略。但要知道，因为我们使用的是非线性函数逼近，所以不再存在收敛保证。这是“深度”世界的狂野西部。
- en: For our NFQ implementation, we do just that. We start with a randomly initialized
    action-value function (and implicit policy.) Then, we evaluate the policy by sampling
    actions from it, as we learned in chapter 5\. Then, improve it with an exploration
    strategy such as epsilon-greedy, as we learned in chapter 4\. Finally, keep iterating
    until we reach the desired performance, as we learned in chapters 6 and 7.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的NFQ实现，我们正是这样做的。我们从随机初始化的动作值函数（和隐含策略）开始。然后，我们通过从策略中采样动作来评估策略，就像我们在第5章中学到的那样。然后，我们使用epsilon-greedy等探索策略来改进它，就像我们在第4章中学到的那样。最后，继续迭代直到达到期望的性能，就像我们在第6章和第7章中学到的那样。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownWe can’t use the ideal objective
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ![简化](../Images/icons_Boil.png) | 简化它我们无法使用理想的目标'
- en: '|  | We can’t use the ideal objective because we don’t have access to the optimal
    action-value function, and we don’t even have an optimal policy to sample from.
    Instead, we must alternate between evaluating a policy (by sampling actions from
    it), and improving it (using an exploration strategy, such as epsilon-greedy).
    It’s as you learned in chapter 6, in the generalized policy iteration pattern.
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们无法使用理想的目标，因为我们无法访问最优的动作值函数，我们甚至没有最优策略可以从中采样。相反，我们必须在评估策略（通过从中采样动作）和改进策略（使用探索策略，如epsilon-greedy）之间交替。就像你在第6章中学到的，在广义策略迭代模式中。
    |'
- en: 'Fourth decision point: Selecting the targets for policy evaluation'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四个决策点：选择政策评估的目标
- en: There are multiple ways we can evaluate a policy. More specifically, there are
    different *targets* we can use for estimating the action-value function of a policy
    π. The core targets you learned about are the Monte Carlo (MC) target, the temporal-difference
    (TD) target, the *n*-step target, and lambda target.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种方法可以评估一个策略。更具体地说，我们可以使用不同的 *目标* 来估计策略π的动作值函数。你学到的核心目标是蒙特卡洛（MC）目标、时序差分（TD）目标、*n*-步目标和lambda目标。
- en: '![](../Images/08_14.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_14.png)'
- en: MC, TD, *n*-step, and lambda targets
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: MC、TD、*n*-步和lambda目标
- en: We could use any of these targets and get solid results, but this time for our
    NFQ implementation, we keep it simple and use the *TD* target for our experiments.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些目标中的任何一个并得到可靠的结果，但这次在我们的NFQ实现中，我们保持简单，使用*TD*目标进行实验。
- en: You remember that the *TD* targets can be either on-policy or off-policy, depending
    on the way you bootstrap the target. The two main ways for bootstrapping the *TD*
    target are to either use the action-value function of the action the agent will
    take at the landing state, or alternatively, to use the value of the action with
    the highest estimate at the next state.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你记得TD目标可以是策略内或策略外，这取决于你如何自举目标。自举TD目标的主要两种方式是使用代理在最终状态采取的动作的动作值函数，或者，使用下一个状态估计值最高的动作的价值。
- en: Often in the literature, the on-policy version of this target is called the
    SARSA target, and the off-policy version is called the Q-learning target.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，这个目标的策略内版本通常被称为SARSA目标，而策略外版本被称为Q-learning目标。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathOn-policy and off-policy
    TD targets |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ![数学](../Images/icons_Math.png) | 展示数学方法在策略和离策略TD目标'
- en: '|  | ![](../Images/08_13_Sidebar09.png) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏](../Images/08_13_Sidebar09.png) |'
- en: In our NFQ implementation, we use the same off-policy *TD* target we used in
    the Q-learning algorithm. At this point, to get an objective function, we need
    to substitute the optimal action-value function *(s,a),* that we had as the ideal
    objective equation, by the Q-learning target.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的NFQ实现中，我们使用与Q学习算法中相同的离策略TD目标。在这个阶段，为了得到一个目标函数，我们需要将作为理想目标方程的**最优动作值函数**(s,a)替换为Q学习目标。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe Q-learning target, an
    off-policy TD target |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学Q学习目标，一个离策略TD目标 |'
- en: '|  | ![](../Images/08_14_Sidebar12.png) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/08_14_Sidebar12.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonQ-learning target |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonQ学习目标 |'
- en: '|  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① First, we get the values of the Q-function at *s* prime (next state). The
    *s* in next_states means that this is a batch of next_state.② The ‘detach’ here
    is important. We should not be propagating values through this. We’re only calculating
    targets.③ Then, we get the max value of the next state max_a.④ The unsqueeze adds
    a dimension to the vector so the operations that follow work on the correct elements.⑤
    One important step, often overlooked, is to ensure terminal states are grounded
    to zero.⑥ Also, notice is_terminals are batches of is_terminal flags, which are
    merely flags indicating whether the next_state is a terminal state or not.⑦ We
    now calculate the target.⑧ Finally, we get the current estimate of *Q*(*s*,*a*).
    At this point, we’re ready to create our loss function. |
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ① 首先，我们获取在s'（下一个状态）处的Q函数的值。next_states中的s表示这是一个next_state的批次。②这里的'detach'很重要。我们不应该通过这个值进行传播。我们只是在计算目标。③然后，我们获取下一个状态的最大值max_a。④unsqueeze向向量添加一个维度，这样后续的操作才能在正确的元素上工作。⑤一个重要但常被忽视的步骤是确保终端状态被归零。⑥注意is_terminals是is_terminal标志的批次，这些标志仅指示下一个状态是否是终端状态。⑦我们现在计算目标。⑧最后，我们获取当前对Q(s,a)的估计。在这个时候，我们准备好创建我们的损失函数。|
- en: I want to bring to your attention two issues that I, unfortunately, see often
    in DRL implementations of algorithms that use *TD* targets.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我想提醒你两个问题，不幸的是，我在使用TD目标的DRL算法实现中经常看到。|
- en: 'First, you need to make sure that you only backpropagate through the predicted
    values. Let me explain. You know that in supervised learning, you have predicted
    values that come from the learning model, and true values that are commonly constants
    provided in advance. In RL, often the “true values” depend on predicted values
    themselves: they come from the model.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要确保你只反向传播通过预测值。让我解释一下。你知道在监督学习中，你有来自学习模型的预测值和通常提前提供的常量真实值。在强化学习中，通常“真实值”依赖于预测值本身：它们来自模型。
- en: For instance, when you form a *TD* target, you use a reward, which is a constant,
    and the discounted value of the next state, which comes from the model. Notice,
    this value is also not a true value, which is going to cause all sorts of problems
    that we’ll address in the next chapter. But what I also want you to notice now
    is that the predicted value comes from the neural network. You have to make this
    predicted value a constant. In PyTorch, you do this only by calling the *detach*
    method. Please look at the two previous boxes and understand these points. They’re
    vital for the reliable implementation of DRL algorithms.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你形成一个TD目标时，你使用一个奖励，这是一个常量，以及下一个状态的折扣值，这个值来自模型。请注意，这个值也不是一个真实值，这将会在下一章中解决各种问题。但我也想让你现在注意到的是，预测值来自神经网络。你必须将这个预测值变成一个常量。在PyTorch中，你只能通过调用*detach*方法来做这件事。请查看前两个框并理解这些要点。它们对于DRL算法的可靠实现至关重要。
- en: The second issue that I want to raise before we move on is the way terminal
    states are handled when using OpenAI Gym environments. The OpenAI Gym step, which
    is used to interact with the environment, returns after every step a handy flag
    indicating whether the agent just landed on a terminal state. This flag helps
    the agent force the value of terminal states to zero, which, as you remember from
    chapter 2, is a requirement to keep the value functions from diverging. You know
    the value of life after death is nil.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我想提出两个问题，不幸的是，我在使用TD目标算法的DRL实现中经常看到。这两个问题是关于终端状态的处理方式，当使用OpenAI Gym环境时。OpenAI
    Gym步骤，用于与环境交互，在每一步后返回一个方便的标志，指示代理是否刚刚到达终端状态。这个标志帮助代理将终端状态的价值强制设为零，正如你从第2章中记得的，这是保持价值函数不发散的要求。你知道死后生命的价值为零。
- en: '![](../Images/08_15.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08_15.png)'
- en: What’s the value of this state?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个状态的价值是多少？
- en: 'The tricky part is that some OpenAI Gym environments, such as the cart-pole,
    have a wrapper code that artificially terminates an episode after some time steps.
    In CartPole-v0, the time step limit is 200, and in CartPole-v1 it is 500\. This
    wrapper code helps to prevent agents from taking too long to complete an episode,
    which can be useful, but it can get you in trouble. Think about it: what do you
    think the value of having the pole straight up in time step 500 would be? I mean,
    if the pole is straight up, and you get +1 for every step, then the true value
    of straight-up is infinite. Yet, since at time step 500 your agent times out,
    and a terminal flag is passed to the agent, you’ll bootstrap on zero if you’re
    not careful. This is bad. I cannot stress this enough. There is a handful of ways
    you can handle this issue, and here are the two common ones. Instead of bootstrapping
    on zero, bootstrap on the value of the next state as predicted by the network,
    if either you (1) reach the time step limit for the environment or (2) find the
    key “TimeLimit.truncated” in the info dictionary. Let me show you the second way.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 难题在于，一些 OpenAI Gym 环境，例如 cart-pole，有一个包装代码，在经过一定的时间步数后会人为地终止一个回合。在 CartPole-v0
    中，时间步数限制是 200，而在 CartPole-v1 中是 500。这个包装代码有助于防止智能体花费太长时间完成一个回合，这可能是有用的，但它也可能给你带来麻烦。想想看：你认为在时间步数
    500 时，让杆子竖直起来的价值是多少？我的意思是，如果杆子竖直，并且每一步都得到 +1，那么竖直的价值就是无限的。然而，由于在时间步数 500 时你的智能体超时，并且传递给智能体的终端标志，如果你不小心，你将基于零进行引导。这是不好的。我无法强调这一点。你可以用几种方法来处理这个问题，这里有两种常见的方法。与其基于零进行引导，不如基于网络预测的下一个状态值进行引导，如果你（1）达到了环境的最大时间步数限制，或者（2）在信息字典中找到了“TimeLimit.truncated”这个关键字。让我给你展示第二种方法。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonProperly handling terminal
    states |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我说 PythonProperly handling terminal states
    |'
- en: '|  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① We collect an experience tuple as usual.② Then check for the key TimeLimit.truncated.③
    A failure is defined as follows.④ Finally, we add the terminal flag if the episode
    ended in failure. If it isn’t a failure, we want to bootstrap on the value of
    the new_state. |
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们像往常一样收集一个经验元组。② 然后检查信息字典中的“TimeLimit.truncated”关键字。③ 失败的定义如下。④ 最后，如果回合以失败结束，我们添加终端标志。如果不是失败，我们希望基于新状态的价值进行引导。|
- en: 'Fifth decision point: Selecting an exploration strategy'
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第五个决策点：选择探索策略
- en: Another thing we need to decide is which policy improvement step to use for
    our generalized policy iteration needs. You know this from chapters 6 and 7, in
    which we alternate a policy evaluation method, such as MC or *TD*, and a policy
    improvement method that accounts for exploration, such as decaying e-greedy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要决定为我们的广义策略迭代需要使用哪种策略改进步骤。你知道这一点来自第 6 章和第 7 章，其中我们交替使用策略评估方法，如 MC 或 *TD*，以及考虑探索的策略改进方法，如衰减的
    epsilon-greedy。
- en: In chapter 4, we surveyed many different ways to balance the exploration-exploitation
    trade-off, and almost any of those techniques would work fine. But in an attempt
    to keep it simple, we’re going to use an epsilon-greedy strategy on our NFQ implementation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章中，我们调查了许多不同的方法来平衡探索-利用权衡，几乎所有这些技术都可以很好地工作。但是，为了保持简单，我们将在我们的 NFQ 实现中使用
    epsilon-greedy 策略。
- en: 'But, I want to highlight the implication of the fact that we’re training an
    off-policy learning algorithm here. What that means is that there are two policies:
    a policy that generates behavior, which in this case is an epsilon-greedy policy,
    and a policy that we’re learning about, which is the greedy (an ultimately optimal)
    policy.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我想强调的是，我们在这里训练的是一个离线策略学习算法的事实所具有的含义。这意味着有两个策略：一个生成行为的策略，在这个案例中是一个 epsilon-greedy
    策略，以及我们正在学习的策略，即贪婪策略（最终是最佳策略）。
- en: One interesting fact of off-policy learning algorithms you studied in chapter
    6 is that the policy generating behavior can be virtually anything. That is, it
    can be anything as long as it has broad support, which means it must ensure enough
    exploration of all state-action pairs. In our NFQ implementation, I use an epsilon-greedy
    strategy that selects an action randomly 50% of the time during training. However,
    when evaluating the agent, I use the action greedy with respect to the learned
    action-value function.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第 6 章中学习的离线学习算法的一个有趣的事实是，生成行为的策略可以是任何东西。也就是说，只要它有广泛的支持，这意味着它必须确保对所有状态-动作对的足够探索。在我们的
    NFQ 实现中，我在训练期间使用了一个 epsilon-greedy 策略，该策略在 50% 的时间内随机选择动作。然而，在评估智能体时，我使用的是与学习到的动作值函数相关的贪婪动作。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonEpsilon-greedy exploration
    strategy |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonEpsilon-greedy探索策略 |'
- en: '|  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '① The select_action function of the epsilon-greedy strategy’ starts by pulling
    out the Q-values for state *s*.② I make the values “NumPy friendly” and remove
    an extra dimension.③ Then, get a random number and, if greater than epsilon, act
    greedily.④ Otherwise, act randomly in the number of actions.⑤ NOTE: I always query
    the model to calculate stats. But, you shouldn’t do that if your goal is performance!
    |'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ① epsilon-greedy策略的select_action函数首先提取状态 *s* 的Q值。② 我将这些值调整为“NumPy友好”并移除一个额外的维度。③
    然后，获取一个随机数，如果大于epsilon，则贪婪地行动。④ 否则，在动作数量中随机行动。⑤ 注意：我总是查询模型来计算统计数据。但，如果你的目标是性能，那么你不应该这样做！
- en: 'Sixth decision point: Selecting a loss function'
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第六个决策点：选择损失函数
- en: 'A loss function is a measure of how well our neural network predictions are.
    In supervised learning, it’s more straightforward to interpret the loss function:
    given a batch of predictions and their corresponding true values, the loss function
    computes a distance score indicating how well the network has done in this batch.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是我们神经网络预测好坏的度量。在监督学习中，解释损失函数更为直接：给定一批预测值及其对应的真实值，损失函数计算一个距离分数，表示网络在这批数据上的表现如何。
- en: 'There are many different ways for calculating this distance score, but I continue
    to keep it simple in this chapter and use one of the most common ones: MSE (mean
    squared error, or L2 loss). Still, let me restate that one challenge in reinforcement
    learning, as compared to supervised learning, is that our “true values” use predictions
    that come from the network.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法来计算这个距离分数，但我在这章中继续保持简单，并使用最常见的一种：MSE（均方误差，或L2损失）。不过，让我重申一下，与监督学习相比，强化学习中的一个挑战是，我们的“真实值”使用了来自网络的预测。
- en: 'MSE (or L2 loss) is defined as the average squared difference between the predicted
    and true values; in our case, the predicted values are the predicted values of
    the action-value function that come straight from the neural network: all good.
    But the true values are, yes, the *TD* targets, which depend on a prediction also
    coming from the network, the value of the next state.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）或L2损失定义为预测值和真实值之间平均平方差的度量；在我们的情况下，预测值是直接来自神经网络的动作值函数的预测值：一切正常。但真实值是，是的，*TD*目标，这取决于来自网络的下一个状态的价值。
- en: '![](../Images/08_16.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_16.png)'
- en: Circular dependency of the action-value function
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值函数的循环依赖性
- en: As you may be thinking, this circular dependency is bad. It’s not well behaved
    because it doesn’t respect several of the assumptions made in supervised learning
    problems. We’ll cover what these assumptions are later in this chapter, and the
    problems that arise when we violate them in the next chapter.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能所想的，这种循环依赖性是坏的。它表现不佳，因为它不尊重监督学习问题中做出的几个假设。我们将在本章后面讨论这些假设，以及我们在下一章违反这些假设时出现的问题。
- en: 'Seventh decision point: Selecting an optimization method'
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第七个决策点：选择优化方法
- en: 'Gradient descent is a stable optimization method given a couple of assumptions:
    data must be independent and identically distributed (IID), and targets must be
    stationary. In reinforcement learning, however, we cannot ensure any of these
    assumptions hold, so choosing a robust optimization method to minimize the loss
    function can often make the difference between convergence and divergence.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定一些假设的情况下，梯度下降是一种稳定的优化方法：数据必须是独立同分布的（IID），目标必须是平稳的。然而，在强化学习中，我们无法确保这些假设中的任何一个成立，因此选择一个鲁棒的优化方法来最小化损失函数，通常可以在收敛和发散之间产生差异。
- en: If you visualize a loss function as a landscape with valleys, peaks, and planes,
    an optimization method is the hiking strategy for finding areas of interest, usually
    the lowest or highest point in that landscape.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将损失函数想象成一个有山谷、山峰和平地的景观，那么优化方法就是寻找感兴趣区域的徒步策略，通常是在这个景观中的最低点或最高点。
- en: A classic optimization method in supervised learning is called *batch gradient
    descent*. The batch gradient descent algorithm takes the entire dataset at once,
    calculates the gradient of the given dataset, and steps toward this gradient a
    little bit at a time. Then, it repeats this cycle until convergence. In the landscape
    analogy, this gradient represents a signal telling us the direction we need to
    move. Batch gradient descent isn’t the first choice of researchers because it
    isn’t practical to process massive datasets at once. When you have a considerable
    dataset with millions of samples, batch gradient descent is too slow to be practical.
    Moreover, in reinforcement learning, we don’t even have a dataset in advance,
    so batch gradient descent isn’t a practical method for our purpose either.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习中的一个经典优化方法被称为*批量梯度下降*。批量梯度下降算法一次性取整个数据集，计算给定数据集的梯度，并逐步朝这个梯度移动。然后，它重复这个循环直到收敛。在景观类比中，这个梯度代表一个信号，告诉我们需要移动的方向。批量梯度下降不是研究人员的首选，因为它不实用，无法一次性处理大量数据集。当你有一个包含数百万样本的大量数据集时，批量梯度下降太慢，不实用。此外，在强化学习中，我们甚至事先没有数据集，所以批量梯度下降也不是我们目的的实用方法。
- en: '![](../Images/08_17.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_17.png)'
- en: Batch gradient descent
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: An optimization method capable of handling smaller batches of data is called
    mini-batch gradient descent. In mini-batch gradient descent, we use only a fraction
    of the data at a time. We process a mini-batch of samples to find its loss, then
    backpropagate to compute the gradient of this loss, and then adjust the weights
    of the network to make the network better at predicting the values of that mini-batch.
    With mini-batch gradient descent, you can control the size of the mini-batches,
    which allows the processing of large datasets.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一种能够处理较小数据批次的优化方法称为小批量梯度下降。在小批量梯度下降中，我们一次只使用数据的一部分。我们处理一个小批量的样本以找到其损失，然后反向传播来计算这个损失的梯度，然后调整网络的权重，使网络更好地预测这个小批量的值。使用小批量梯度下降，你可以控制小批量的大小，这允许处理大型数据集。
- en: '![](../Images/08_18.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_18.png)'
- en: Mini-batch gradient descent
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: At one extreme, you can set the size of your mini-batch to the size of your
    dataset, in which case, you’re back at batch gradient descent. At the other extreme,
    you can set the mini-batch size to a single sample per step. In this case, you’re
    using an algorithm called *stochastic gradient* descent.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个极端情况下，你可以将你的小批量大小设置为数据集的大小，这样你又会回到批量梯度下降。在另一个极端情况下，你可以将小批量大小设置为每步一个样本。在这种情况下，你正在使用一种称为*随机梯度下降*的算法。
- en: '![](../Images/08_19.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_19.png)'
- en: Stochastic gradient descent
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: The larger the batch, the lower the variance the steps of the optimization method
    have. But use a batch too large, and learning slows down considerably. Both extremes
    are too slow in practice. For these reasons, it’s common to see mini-batch sizes
    ranging from 32 to 1024.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 批次越大，优化方法的步骤方差就越低。但是，如果使用过大的批次，学习速度会显著减慢。在实践中，这两种极端情况都太慢了。因此，常见的小批量大小范围在32到1024之间。
- en: '![](../Images/08_20.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_20.png)'
- en: Zig-zag pattern of mini-batch gradient descent
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降的Z字形模式
- en: An improved gradient descent algorithm is called *gradient descent with momentum*,
    or *momentum* for short. This method is a mini-batch gradient descent algorithm
    that updates the network’s weights in the direction of the moving average of the
    gradients, instead of the gradient itself.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一种改进的梯度下降算法被称为*带有动量的梯度下降*，或简称*动量*。这种方法是一种小批量梯度下降算法，它更新网络权重的方式是朝着梯度的移动平均值的方向，而不是梯度本身。
- en: '![](../Images/08_21.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_21.png)'
- en: Mini-batch gradient descent vs. momentum
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降与动量
- en: An alternative to using momentum is called *root mean square propagation* (RMSprop).
    Both RMSprop and momentum do the same thing of dampening the oscillations and
    moving more directly towards the goal, but they do so in different ways.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用动量的替代方法是称为*均方根传播*（RMSprop）。RMSprop和动量都做同样的事情，即减少振荡并更直接地向目标移动，但它们以不同的方式做到这一点。
- en: While momentum takes steps in the direction of the moving average of the gradients,
    RMSprop takes the safer bet of scaling the gradient in proportion to a moving
    average of the magnitude of gradients. It reduces oscillations by merely scaling
    the gradient in proportion to the square root of the moving average of the square
    of the gradients or, more simply put, in proportion to the average magnitude of
    recent gradients.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当动量沿着梯度移动平均的方向迈步时，RMSprop 则采取更安全的做法，按比例缩放梯度，比例是梯度幅度的移动平均。它通过按比例缩放梯度来减少振荡，比例是梯度平方的移动平均的平方根，或者更简单地说，是最近梯度幅度的平均值。
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyOptimization methods in
    value-based deep reinforcement learning |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| ![Miguel的类比](../Images/icons_Miguel.png) | 基于价值型深度强化学习的优化方法'
- en: '|  | To visualize RMSprop, think of the steepness change of the surface of
    your loss function. If gradients are high, such as when going downhill, and the
    surface changes to a flat valley, where gradients are small, the moving average
    magnitude of gradients is higher than the most recent gradient; therefore, the
    size of the step is reduced, preventing oscillations or overshooting.If gradients
    are small, such as in a near-flat surface, and they change to a significant gradient,
    as when going downhill, the average magnitude of gradients is small, and the new
    gradient large, therefore increasing the step size and speeding up learning. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | 为了可视化 RMSprop，想象一下你的损失函数表面的陡峭度变化。如果梯度很高，例如在下坡时，表面变为一个平坦的谷地，其中梯度很小，那么梯度移动平均的幅度将高于最近的梯度；因此，步长的大小会减小，从而防止振荡或超调。如果梯度很小，例如在接近平坦的表面上，并且它们变为一个显著的梯度，就像在下坡时，那么梯度平均的幅度将很小，而新的梯度很大，因此增加步长并加快学习速度。'
- en: A final optimization method I’d like to introduce is called *adaptive moment
    estimation* (Adam). Adam is a combination of RMSprop and momentum. The Adam method
    steps in the direction of the velocity of the gradients, as in momentum. But,
    it scales updates in proportion to the moving average of the magnitude of the
    gradients, as in RMSprop. These properties make Adam as an optimization method
    a bit more aggressive than RMSprop, yet not as aggressive as momentum.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我想介绍的最后一种优化方法称为 *自适应动量估计*（Adam）。Adam 是 RMSprop 和动量的结合。Adam 方法沿着梯度速度的方向迈步，就像动量一样。但是，它按比例缩放更新，比例是梯度幅度的移动平均，就像
    RMSprop 一样。这些特性使得 Adam 作为优化方法比 RMSprop 更具侵略性，但不如动量那么具侵略性。
- en: In practice, both Adam and RMSprop are sensible choices for value-based deep
    reinforcement learning methods. I use both extensively in the chapters ahead.
    However, I do prefer RMSprop for value-based methods, as you’ll soon notice. RMSprop
    is stable and less sensitive to hyperparameters, and this is particularly important
    in value-based deep reinforcement learning.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，Adam 和 RMSprop 都是价值型深度强化学习方法的合理选择。我在接下来的章节中广泛使用了这两种方法。然而，我更倾向于使用 RMSprop
    作为价值型方法，这一点你很快就会注意到。RMSprop 稳定且对超参数的敏感性较低，这在价值型深度强化学习中尤为重要。
- en: '| 0001 | A Bit Of HistoryIntroduction of the NFQ algorithm |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍 NFQ 算法 |'
- en: '|  | NFQ was introduced in 2005 by Martin Riedmiller in a paper called “Neural
    Fitted Q Iteration − First Experiences with a Data Efficient Neural Reinforcement
    Learning Method.” After 13 years working as a professor at a number of European
    universities, Martin took a job as a research scientist at Google DeepMind. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | NFQ 由 Martin Riedmiller 在 2005 年发表的一篇名为“Neural Fitted Q Iteration − First
    Experiences with a Data Efficient Neural Reinforcement Learning Method”的论文中提出。在担任欧洲多所大学教授
    13 年后，Martin 在 Google DeepMind 找到了一份研究科学家的职位。'
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsThe full neural fitted
    Q-iteration (NFQ) algorithm |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| ![细节](../Images/icons_Details.png) | 详细介绍完整的神经拟合 Q 迭代（NFQ）算法'
- en: '|  | Currently, we’ve made the following selections:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 目前，我们已经做出了以下选择：'
- en: Approximate the action-value function *Q*(*s,a; θ*).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近似动作值函数 *Q*(*s,a; θ*)。
- en: 'Use a state-in-values-out architecture (nodes: 4, 512,128, 2).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用状态-值输出架构（节点：4, 512, 128, 2）。
- en: Optimize the action-value function to approximate the optimal action- value
    function *q**(*s,a*).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化动作值函数以近似最优动作值函数 *q**(*s,a*)。
- en: Use off-policy *TD* targets (*r + γ*max_a’Q*(*s’,a’; θ*)) to evaluate policies.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用离策略 *TD* 目标（*r + γ*max_a’Q*(*s’，a’; θ*）来评估策略。
- en: Use an epsilon-greedy strategy (epsilon set to 0.5) to improve policies.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ε-贪婪策略（ε 设置为 0.5）来改进策略。
- en: Use mean squared error (MSE) for our loss function.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均方误差（MSE）作为我们的损失函数。
- en: Use RMSprop as our optimizer with a learning rate of 0.0005.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RMSprop 作为我们的优化器，学习率为 0.0005。
- en: 'NFQ has three main steps:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: NFQ 有三个主要步骤：
- en: 'Collect E experiences: (*s, a, r, s’, d*) tuples. We use 1024 samples.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集 E 个经验：(*s, a, r, s’，d*) 元组。我们使用 1024 个样本。
- en: 'Calculate the off-policy *TD* targets: *r + γ*max_a’Q*(*s’,a’; θ*).'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算离策略 *TD* 目标：*r + γ*max_a’Q*(*s’，a’; θ*)。
- en: Fit the action-value function *Q*(*s,a; θ*) using MSE and RMSprop.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 MSE 和 RMSprop 来拟合动作值函数 *Q*(*s,a; θ*)。
- en: 'This algorithm repeats steps 2 and 3 *K* number of times before going back
    to step 1\. That’s what makes it fitted: the nested loop. We’ll use 40 fitting
    steps *K*.![](../Images/08_21_Sidebar18.png)NFQ |'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法在返回到步骤 1 之前重复步骤 2 和 3 *K* 次数。这就是它被拟合的原因：嵌套循环。我们将使用 40 次拟合步骤 *K*。![08_21_Sidebar18.png](../Images/08_21_Sidebar18.png)NFQ
    |
- en: '| ![](../Images/icons_Tally.png) | Tally it UpNFQ passes the cart-pole environment
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| ![计数器图标](../Images/icons_Tally.png) | 计数器：NFQ 通过了小车-杆环境 |'
- en: '|  | Although NFQ is far from a state-of-the-art, value-based deep reinforcement
    learning method, in a somewhat simple environment, such as the cart-pole, NFQ
    shows a decent performance.![](../Images/08_21_Sidebar19.png) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | 虽然NFQ远非最先进的基于价值的深度强化学习方法，但在一个相对简单的环境中，如小车-杆，NFQ 表现出相当不错的性能。![08_21_Sidebar19.png](../Images/08_21_Sidebar19.png)
    |'
- en: Things that could (and do) go wrong
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可能（确实）出错的事情
- en: There are two issues with our algorithm. First, because we’re using a powerful
    function approximator, we can generalize across state-action pairs, which is excellent,
    but that also means that the neural network adjusts the values of all similar
    states at once.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们算法有两个问题。首先，因为我们使用了一个强大的函数逼近器，我们可以泛化到状态-动作对，这是非常好的，但也意味着神经网络会一次性调整所有相似状态的价值。
- en: 'Now, think about this for a second: our target values depend on the values
    for the next state, which we can safely assume are similar to the states we are
    adjusting the values of in the first place. In other words, we’re creating a non-stationary
    target for our learning updates. As we update the weights of the approximate Q-function,
    the targets also move and make our most recent update outdated. Thus, training
    becomes unstable quickly.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一下这个问题：我们的目标值取决于下一个状态的价值，我们可以安全地假设这些状态与我们最初调整价值的状态相似。换句话说，我们正在为我们的学习更新创建一个非平稳的目标。当我们更新近似
    Q 函数的权重时，目标也会移动，并使我们的最新更新过时。因此，训练很快变得不稳定。
- en: '![](../Images/08_22.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![08_22.png](../Images/08_22.png)'
- en: Non-stationary target
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 非平稳目标
- en: Second, in NFQ, we batched 1024 experience samples collected online and update
    the network from that mini-batch. As you can imagine, these samples are correlated,
    given that most of these samples come from the same trajectory and policy. That
    means the network learns from mini-batches of samples that are similar, and later
    uses different mini-batches that are also internally correlated, but likely different
    from previous mini-batches, mainly if a different, older policy collected the
    samples.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在 NFQ 中，我们批量处理了在线收集的 1024 个经验样本，并从该小批量更新网络。正如你可以想象的那样，这些样本是相关的，因为大多数这些样本来自相同的轨迹和政策。这意味着网络从相似样本的小批量中学习，后来使用的是也内部相关的不同小批量，但可能与先前的小批量不同，主要是在收集样本的政策不同或较旧时。
- en: 'All this means that we aren’t holding the IID assumption, and this is a problem
    because optimization methods assume the data samples they use for training are
    independent and identically distributed. But we’re training on almost the exact
    opposite: samples on our distribution are not independent because the outcome
    of a new state *s* is dependent on our current state *s*.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都意味着我们没有保持 IID 假设，这是一个问题，因为优化方法假设他们用于训练的数据样本是独立且同分布的。但我们在几乎完全相反的情况下进行训练：我们的分布上的样本不是独立的，因为新状态
    *s* 的结果取决于我们的当前状态 *s*。
- en: And, also, our samples aren’t identically distributed because the underlying
    data generating process, which is our policy, is changing over time. That means
    we don’t have a fixed data distribution. Instead, our policy, which is responsible
    for generating the data, is changing and hopefully improving periodically. Every
    time our policy changes, we receive new and likely different experiences. Optimization
    methods allow us to relax the IID assumption to a certain degree, but reinforcement
    learning problems go all the way, so we need to do something about this, too.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的样本并非同分布，因为生成数据的底层过程，即我们的策略，是随时间变化的。这意味着我们没有固定的数据分布。相反，负责生成数据的策略在定期变化，并有望得到改进。每次我们的策略发生变化时，我们都会获得新的、可能不同的经验。优化方法允许我们在一定程度上放宽独立同分布的假设，但强化学习问题一直存在，因此我们也需要对此采取一些措施。
- en: '![](../Images/08_23.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08_23.png)'
- en: Data correlated with time
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与时间相关的数据
- en: In the next chapter, we look at ways of mitigating these two issues. We start
    by improving NFQ with the algorithm that arguably started the deep reinforcement
    learning revolution, DQN. We then follow by exploring many of the several improvements
    proposed to the original DQN algorithm over the years. We also look at double
    DQN in the next chapter, and then in chapter 10, we look at dueling DQN and PER.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨缓解这两个问题的方法。我们首先通过改进NFQ算法，即有争议地开启了深度强化学习革命的DQN算法，来提升NFQ。随后，我们将探讨多年来对原始DQN算法提出的许多改进。在下一章中，我们还将探讨双DQN，然后在第10章中，我们将探讨对抗DQN和PER。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we gave a high-level overview of how sampled feedback interacts
    with sequential and evaluative feedback. We did so while introducing a simple
    deep reinforcement learning agent that approximates the Q-function, that in previous
    chapters, we would represent in tabular form, with a lookup table. This chapter
    was an introduction to value-based deep reinforcement learning methods.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了采样反馈如何与顺序反馈和评估反馈相互作用。我们在介绍一个简单的深度强化学习代理时这样做，该代理近似Q函数，在之前的章节中，我们会用表格形式或查找表来表示。本章是关于基于价值的深度强化学习方法的介绍。
- en: You learned the difference between high-dimensional and continuous state and
    action spaces. The former indicates a large number of values that make up a single
    state; the latter hints at at least one variable that can take on an infinite
    number of values. You learned that decision-making problems could be both high-dimensional
    and continuous variables, and that makes the use of non-linear function approximation
    intriguing.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解到高维和连续状态空间和动作空间之间的区别。前者表示组成单个状态的大量值；后者暗示至少有一个变量可以取无限多个值。你了解到决策问题可以是高维和连续变量，这使得使用非线性函数近似变得有趣。
- en: You learned that function approximation isn’t only beneficial for estimating
    expectations of values for which we only have a few samples, but also for learning
    the underlying relationships in the state and action dimensions. By having a good
    model, we can estimate values for which we never received samples and use all
    experiences across the board.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解到函数近似不仅对估计只有少量样本的值的期望值有益，而且对学习状态和动作维度中的潜在关系也有益。通过拥有一个好的模型，我们可以估计我们从未收到样本的值，并利用所有经验。
- en: You had an in-depth overview of different components commonly used when building
    deep reinforcement learning agents. You learned you could approximate different
    kinds of value functions, from the state-value function *v(s)* to the action-value
    *q(s, a)*. And, you can approximate these value functions using different neural
    network architectures; we explored the state-action pair in, value out, to the
    more efficient state-in, values out. You learned about using the same objective
    we used for Q-learning, using the *TD* target for off-policy control. And, you
    know there are many different targets you can use to train your network. You surveyed
    exploration strategies, loss functions, and optimization methods. You learned
    that deep reinforcement learning agents are susceptible to the loss and optimization
    methods we select. You learned about RMSprop and Adam as the stable options for
    optimization methods.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你对构建深度强化学习代理时常用的不同组件有了深入的了解。你了解到你可以近似不同类型的值函数，从状态值函数 *v(s)* 到动作值 *q(s, a)*。而且，你可以使用不同的神经网络架构来近似这些值函数；我们探讨了状态-动作对输入，值输出，到更有效的状态输入，值输出。你了解到我们可以使用与Q学习相同的目标，使用
    *TD* 目标进行离策略控制。而且，你知道有许许多多的目标可以用来训练你的网络。你调查了探索策略、损失函数和优化方法。你了解到深度强化学习代理容易受到我们选择的损失和优化方法的影响。你了解到RMSprop和Adam是优化方法的稳定选项。
- en: You learned to combine all of these components into an algorithm called neural
    fitted Q-iteration. You learned about the issues commonly occurring in value-based
    deep reinforcement learning methods. You learned about the IID assumption and
    the stationarity of the targets. You learned that not being careful with these
    two issues can get us into trouble.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你学会了将这些组件组合成一个名为神经拟合Q迭代的算法。你了解了基于值的方法在深度强化学习中常见的问题。你学习了独立同分布假设和目标状态的稳定性。你了解到如果不注意这两个问题，可能会陷入麻烦。
- en: By now, you
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Understand what it is to learn from feedback that is sequential, evaluative,
    and sampled
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解从序列性、评估性和抽样反馈中学习是什么
- en: Can solve reinforcement learning problems with continuous state-spaces
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以解决具有连续状态空间的强化学习问题
- en: Know about the components and issues in value-based DRL methods
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解基于值的DRL方法中的组件和问题
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tweet.png) | 可分享的工作：自己动手并分享你的发现'
- en: '|  | Here are several ideas on how to take what you’ve learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you''ll take advantage of it.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，如何将你学到的知识提升到下一个层次。如果你愿意，与世界分享你的结果，并确保查看其他人做了什么。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch08_tf01:** After tabular reinforcement learning, and before deep
    reinforcement learning, there are a couple things to explore. With this hashtag,
    explore and share results for state discretization and tile coding techniques.
    What are those? Are there any other techniques that we should know about?'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch08_tf01:** 在表格式强化学习之后，在深度强化学习之前，有一些事情需要探索。使用这个标签，探索并分享关于状态离散化和瓦片编码技术的结果。这些是什么？还有其他我们应该了解的技术吗？'
- en: '**#gdrl_ch08_tf02:** The other thing I’d like you to explore is the use of
    linear function approximation, instead of deep neural networks. Can you tell us
    how other function approximation techniques compare? What techniques show promising
    results?'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch08_tf02:** 我还希望你探索的是使用线性函数逼近，而不是深度神经网络。你能告诉我们其他函数逼近技术如何比较吗？哪些技术显示出有希望的结果？'
- en: '**#gdrl_ch08_tf03:** In this chapter, I introduced gradient descent as the
    type of optimization method we use for the remainder of the book. However, gradient
    descent is not the only way to optimize a neural network; did you know? Either
    way, you should go out there and investigate other ways to optimize a neural network,
    from black-box optimization methods, such as genetic algorithms, to other methods
    that aren’t as popular. Share your findings, create a Notebook with examples,
    and share your results.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch08_tf03:** 在本章中，我介绍了梯度下降作为我们在本书剩余部分使用的优化方法。然而，梯度下降并不是优化神经网络的唯一方法；你知道吗？无论如何，你应该去探索其他优化神经网络的方法，从黑盒优化方法，如遗传算法，到不那么流行的其他方法。分享你的发现，创建一个包含示例的笔记本，并分享你的结果。'
- en: '**#gdrl_ch08_tf04:** I started this chapter with a better way for doing Q-learning
    with function approximation. Equally important to knowing a better way is to have
    an implementation of the simplest way that didn’t work. Implement the minimal
    changes to make Q-learning work with a neural network: that is, Q-learning with
    online experiences as you learned in chapter 6\. Test and share your results.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch08_tf04:** 我以一个更好的方法开始了这一章，用于使用函数逼近进行Q学习。同样重要的是要知道一个更好的方法，还要有一个实现最简单方法但没有成功的实现。对Q学习进行最小修改，使其与神经网络一起工作：即，像在第6章中学到的，使用在线经验进行Q学习。测试并分享你的结果。'
- en: '**#gdrl_ch08_tf05:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch08_tf05:** 在每一章中，我都使用最后的标签作为一个总标签。你可以自由使用这个标签来讨论与本章相关的任何其他工作。没有比你自己创造的任务更令人兴奋的作业了。确保分享你设定要调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@提及我 @mimoralea（我会转发），并使用列表中的特定标签来帮助感兴趣的人找到你的结果。没有对错之分；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己更受关注！我们正在等待你的加入！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源列表。查看它在这里<链接>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的作品。|
