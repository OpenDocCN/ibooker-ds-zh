- en: '2 Homogeneous parallel ensembles: Bagging and random forests'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 同质并行集成：Bagging 和随机森林
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Training homogeneous parallel ensembles
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练同质并行集成
- en: Implementing and understanding bagging
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和理解 Bagging
- en: Implementing and understanding how random forests work
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和理解随机森林的工作原理
- en: Training variants with pasting, random subspaces, random patches, and Extra
    Trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用粘贴、随机子空间、随机补丁和Extra Trees训练变体
- en: Using bagging and random forests in practice
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中使用 Bagging 和随机森林
- en: 'In chapter 1, we introduced ensemble learning and created our first rudimentary
    ensemble. To recap, an ensemble method relies on the notion of “wisdom of the
    crowd”: the *combined* answer of many models is often better than any one individual
    answer. We begin our journey into ensemble learning methods in earnest with parallel
    ensemble methods. We begin with this type of ensemble method because, conceptually,
    parallel ensemble methods are easy to understand and implement.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们介绍了集成学习并创建了我们的第一个基本的集成。为了回顾，集成方法依赖于“群众智慧”的概念：许多模型的*综合*答案通常比任何单个答案都要好。我们真正开始我们的集成学习方法之旅，从并行集成方法开始。我们之所以从这种集成方法开始，是因为从概念上讲，并行集成方法易于理解和实现。
- en: Parallel ensemble methods, as the name suggests, train each component base estimator
    independently of the others, which means that they can be trained in parallel.
    As we’ll see, parallel ensemble methods can be further distinguished as homogeneous
    and heterogeneous parallel ensembles depending on the kind of learning algorithms
    they use.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如同其名所示，并行集成方法独立于其他组件基估计器进行训练，这意味着它们可以并行训练。正如我们将看到的，并行集成方法可以根据它们使用的不同学习算法进一步区分为同质并行集成和异质并行集成。
- en: 'In this chapter, you’ll learn about homogeneous parallel ensembles, whose component
    models are all trained using the same machine-learning algorithm. This is in contrast
    to heterogeneous parallel ensembles (covered in the next chapter), whose component
    models are trained using different machine-learning algorithms. The class of homogeneous
    parallel ensemble methods includes two popular machine-learning methods, one or
    both of which you might have come across and even used before: *bagging and random
    forests*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解同质并行集成，其组件模型都是使用相同的机器学习算法进行训练。这与下一章中介绍的异质并行集成形成对比，其组件模型使用不同的机器学习算法进行训练。同质并行集成方法类包括两种流行的机器学习方法，其中之一或两者你可能已经接触过，甚至之前使用过：*Bagging
    和随机森林*。
- en: Recall that the two key components of an ensemble method are ensemble diversity
    and model aggregation. Because homogeneous ensemble methods use the same learning
    algorithm on the same data set, you may wonder how they can generate a set of
    diverse base estimators. They do this through *random sampling* of either the
    training examples (as bagging does), features (as some variants of bagging do),
    or both (as random forests do).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，集成方法的两个关键组成部分是集成多样性和模型聚合。因为同质集成方法在相同的数据集上使用相同的学习算法，你可能想知道它们如何生成一组多样化的基础估计器。它们通过*随机采样*来实现，无论是训练样本（如Bagging所做的那样），特征（如某些Bagging变体所做的那样），还是两者（如随机森林所做的那样）。
- en: Some of the algorithms introduced in this chapter, such as random forests, are
    widely used in medical and bioinformatics applications. In fact, random forests
    are still a strong off-the-shelf baseline algorithm to try on a new data set,
    owing to its efficiency (it can be parallelized or distributed easily over multiple
    processors).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的一些算法，如随机森林，在医学和生物信息学应用中得到了广泛应用。事实上，由于它的效率（它可以在多个处理器上轻松并行化或分布式），随机森林仍然是尝试新数据集的一个强大的现成基线算法。
- en: 'We’ll begin with the most basic parallel homogeneous ensemble: bagging. Once
    you understand how bagging achieves ensemble diversity through sampling, we’ll
    look at the most powerful variant of bagging: random forests.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最基本的并行同质集成开始：Bagging。一旦你理解了Bagging如何通过采样实现集成多样性，我们将探讨Bagging最强大的变体：随机森林。
- en: You’ll also learn about other variants of bagging (pasting, random subspaces,
    random patches) and random forests (Extra Trees). These variants are often effective
    for big data or in applications with high-dimensional data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将了解Bagging的其他变体（粘贴、随机子空间、随机补丁）和随机森林（Extra Trees）。这些变体通常对大数据或高维数据的应用非常有效。
- en: 2.1 Parallel ensembles
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 并行集成
- en: First, we concretely define the notion of a parallel ensemble. This will help
    us put the algorithms in this chapter and the next into a single context, so that
    we can easily see both their similarities and differences.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们具体定义并行集成的概念。这将帮助我们将这些章节和下一章的算法置于一个单一的环境中，这样我们就可以轻松地看到它们的相似之处和不同之处。
- en: 'Recall Dr. Randy Forrest, our ensemble diagnostician from chapter 1\. Every
    time Dr. Forrest gets a new case, he solicits the opinions of all his residents.
    He then determines the final diagnosis from among those proposed by his residents
    (figure 2.1, top). Dr. Forrest’s diagnostic technique is successful for two reasons:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下第一章中提到的我们的集成诊断专家兰迪·福雷斯特博士。每当福雷斯特博士接诊一个新病例时，他会征求所有住院医师的意见。然后，他从住院医师提出的诊断中确定最终的诊断（图2.1，顶部）。福雷斯特博士的诊断技术之所以成功，有两个原因：
- en: He has assembled a *diverse* set of residents, with different medical specializations,
    which means they each think differently about a case. This works out well for
    Dr. Forrest as it puts several different perspectives on the table for him to
    consider.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他组建了一个*多样化的*住院医师团队，拥有不同的医学专业，这意味着他们每个人对病例的看法都不同。这对福雷斯特博士来说是个好事，因为它为他提供了多个不同的视角供他考虑。
- en: He *aggregates* the *independent* opinions of his residents into one final diagnosis.
    Here, he is democratic and selects the majority opinion. However, he can also
    aggregate his residents’ opinions in other ways. For instance, he can weight the
    opinions of his more experienced residents higher. This reflects that he trusts
    some residents more than others, based on factors such as experience or skill,
    which means they are right more often than other residents on the team.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他将住院医师的*独立*意见汇总成一个最终的诊断。在这里，他是民主的，选择了多数意见。然而，他也可以以其他方式汇总住院医师的意见。例如，他可以提高经验更丰富的住院医师的意见权重。这反映了他比其他人更信任某些住院医师，基于诸如经验或技能等因素，这意味着他们比团队中的其他住院医师更经常正确。
- en: Dr. Forrest and his residents are a parallel ensemble (figure 2.1, bottom).
    Each resident in the preceding example is a component base estimator (or base
    learner) that we have to train. Base estimators can be trained using different
    base algorithms (leading to heterogeneous ensembles) or the same base algorithm
    (leading to homogeneous ensembles).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 福雷斯特博士和他的住院医师构成一个并行集成（图2.1，底部）。在先前的例子中，每位住院医师都是一个必须训练的基估计器（或基学习器）。基估计器可以使用不同的基算法进行训练（导致异质集成）或使用相同的基算法（导致同质集成）。
- en: '![CH02_F01_Kunapuli](../Images/CH02_F01_Kunapuli.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F01_Kunapuli](../Images/CH02_F01_Kunapuli.png)'
- en: Figure 2.1 Dr. Randy Forrest’s diagnostic process is an analogy of a parallel
    ensemble method.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 福雷斯特博士的诊断过程是并行集成方法的类比。
- en: 'If we want to put together an effective ensemble similar to Dr. Forrest’s,
    we have to address two problems:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要组建一个类似于福雷斯特博士的有效集成，我们必须解决两个问题：
- en: How do we create a set of base estimators with diverse opinions from a single
    data set? That is, how can we ensure ensemble diversity during training?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何从一个数据集中创建一组具有不同意见的基估计器？也就是说，我们如何在训练过程中确保集成多样性？
- en: How can we aggregate decisions, or predictions, of each individual base estimator
    into a final prediction? That is, how can we perform model aggregation during
    prediction?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将每个个体基估计器的决策或预测汇总成一个最终的预测？也就是说，我们如何在预测过程中执行模型集成？
- en: You’ll see exactly how to do both in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下一节中看到如何做到这两点。
- en: '2.2 Bagging: Bootstrap aggregating'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2.2 Bagging: Bootstrap aggregating'
- en: '*Bagging*, short for *bootstrap aggregating*, was introduced by Leo Breiman
    in 1996\. The name refers to how bagging achieves ensemble diversity (through
    bootstrap sampling) and performs ensemble prediction (through model aggregating).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bagging*，即*bootstrap aggregating*，由Leo Breiman于1996年提出。这个名字指的是Bagging如何通过自助采样实现集成多样性，并通过模型集成执行集成预测。'
- en: 'Bagging is the most basic homogeneous parallel ensemble method we can construct.
    Understanding bagging will be helpful in understanding the other ensemble methods
    in this chapter. These methods further enhance the basic bagging approach in different
    ways: to improve either ensemble diversity or overall computational efficiency.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging是我们能构建的最基本的同质并行集成方法。理解Bagging将有助于理解本章中其他集成方法。这些方法以不同的方式进一步增强了基本的Bagging方法：要么提高集成多样性，要么提高整体计算效率。
- en: 'Bagging uses the *same* base machine-learning algorithm to train base estimators.
    So how can we get multiple base estimators from a single data set and a single
    learning algorithm, let alone diversity? This comes by training base estimators
    on *replicates of the data set*. Bagging consists of two steps, as illustrated
    in figure 2.2:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging使用相同的基机器学习算法来训练基本估计器。那么我们如何从一个数据集和一个学习算法中得到多个基本估计器，更不用说多样性了？这通过在数据集的副本上训练基本估计器来实现。Bagging包括两个步骤，如图2.2所示：
- en: During training, bootstrap sampling, or sampling with replacement, is used to
    generate replicates of the training data set that are different from each other
    but drawn from the original data set. This ensures that base learners trained
    on each of the replicates are also different from each other.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，自举抽样或有放回抽样被用来生成与彼此不同但来自原始数据集的重复数据集副本。这确保了在每个副本上训练的基本学习器彼此也不同。
- en: During prediction, model aggregation is used to combine the predictions of the
    individual base learners into one ensemble prediction. For classification tasks,
    we can combine individual predictions using majority voting. For regression tasks,
    we can combine individual predictions using simple averaging.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预测期间，模型集成被用来将单个基本学习器的预测组合成一个集成预测。对于分类任务，我们可以使用多数投票来组合单个预测。对于回归任务，我们可以使用简单平均来组合单个预测。
- en: '![CH02_F02_Kunapuli](../Images/CH02_F02_Kunapuli.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F02_Kunapuli](../Images/CH02_F02_Kunapuli.png)'
- en: Figure 2.2 Bagging, illustrated. Bagging uses bootstrap sampling to generate
    similar but not exactly identical subsets (observe the replicates here) from a
    single data set. Models are trained on each of these subsets, resulting in similar
    but not exactly identical base estimators. Given a test example, the individual
    base-estimator predictions are aggregated into a final ensemble prediction. Also
    observe that training examples may repeat in the replicated subsets; this is a
    consequence of bootstrap sampling.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 Bagging示意图。Bagging使用自举抽样从单个数据集中生成相似但不完全相同的子集（观察这里的副本）。在这些子集上训练模型，结果得到相似但不完全相同的基本估计器。对于给定的测试示例，单个基本估计器的预测被聚合成一个最终的集成预测。同时观察，训练示例可能在重复的子集中重复；这是自举抽样的结果。
- en: '2.2.1 Intuition: Resampling and model aggregation'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 直觉：重抽样和模型集成
- en: The key challenge for ensemble diversity is that we need to create (and use)
    different base estimators using the same learning algorithm and the same data
    set. We’ll now see how to (1) generate replicates of the data set, which, in turn,
    can be used to train base estimators; and (2) combine predictions of base estimators.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 集成多样性面临的关键挑战是我们需要使用相同的学习算法和相同的数据集来创建（并使用）不同的基本估计器。现在我们将看到如何（1）生成数据集的副本，这些副本可以用来训练基本估计器；（2）结合基本估计器的预测。
- en: 'Bootstrap sampling: Sampling with replacement'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自举抽样：有放回抽样
- en: We’ll use random sampling to easily generate smaller subsets from the original
    data set. To generate same-size replicates of the data set, we’ll need to perform
    *sampling with replacement*, otherwise known as bootstrap sampling.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用随机抽样来轻松地从原始数据集中生成较小的子集。为了生成相同大小的数据集副本，我们需要进行有放回抽样，也称为自举抽样。
- en: When sampling with replacement, some objects that were already sampled have
    a chance to be sampled a second time (or even a third, or fourth, etc.) because
    they were replaced. In fact, some objects may be sampled many times, while some
    objects may never be sampled. Sampling with replacement is illustrated in figure
    2.3, where we see that allowing replacement after sampling leads to repeats.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行有放回的抽样时，一些已经被抽样的对象因为被替换而有机会再次被抽样（甚至第三次、第四次等）。实际上，一些对象可能会被多次抽样，而一些对象可能永远不会被抽样。有放回的抽样在图2.3中得到了说明，我们可以看到抽样后允许替换会导致重复。
- en: '![CH02_F03_Kunapuli](../Images/CH02_F03_Kunapuli.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F03_Kunapuli](../Images/CH02_F03_Kunapuli.png)'
- en: Figure 2.3 Bootstrap sampling illustrated on a data set of six examples. By
    sampling with replacement, we can get a bootstrap sample size of six, containing
    only four unique objects but with repeats. Performing bootstrap sampling several
    times produces several replicates of the original data set—all of them with repeats.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 在六个示例的数据集上展示了自举抽样。通过有放回抽样，我们可以得到一个包含六个对象的自举样本大小，其中只有四个独特的对象但有重复。进行多次自举抽样会产生原始数据集的几个副本——它们都包含重复。
- en: 'Thus, bootstrap sampling naturally partitions a data set into two sets: a bootstrap
    sample (with training examples that were sampled at least once) and an *out-of-bag
    (OOB) sample* (with training examples that were never sampled even once).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，重抽样自然地将数据集分为两部分：一个重抽样样本（包含至少被抽样一次的训练示例）和一个*袋外（OOB）样本*（包含从未被抽样过的训练示例）。
- en: We can use each bootstrap sample for training a different base estimator. Because
    different bootstrap samples will contain different examples repeating a different
    number of times, each base estimator will turn out to be somewhat different from
    the others.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用每个重抽样样本来训练不同的基础估计器。因为不同的重抽样样本将包含不同数量的重复示例，所以每个基础估计器将与其他估计器略有不同。
- en: The out-of-bag sample
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随机袋样本
- en: Just throwing away the OOB sample seems rather wasteful. However, if we train
    a base estimator on the bootstrap sample, the OOB sample is *held out* and never
    seen by the base estimator during learning. Sound familiar?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅丢弃OOB样本似乎相当浪费。然而，如果我们使用重抽样样本训练基础估计器，OOB样本将被*保留*，在学习的整个过程中都不会被基础估计器看到。听起来熟悉吗？
- en: The OOB sample is effectively a *held-out test set* and can be used to evaluate
    the ensemble without the need for a separate validation set or even a cross-validation
    procedure. This is great because it allows us to utilize data more efficiently
    during training. The error estimate computed using OOB instances is called the
    *OOB error* or the *OOB score*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: OOB样本实际上是一个*保留的测试集*，可以用来评估集成，而无需单独的验证集或交叉验证过程。这很棒，因为它允许我们在训练期间更有效地利用数据。使用OOB实例计算的错误估计称为*OOB误差*或*OOB分数*。
- en: 'It’s very easy to generate bootstrap samples with replacement using numpy.random.choice.
    Suppose we have a data set with 50 training examples (say, patient records with
    unique IDs from 0 to 49). We can generate a bootstrap sample, also of size 50
    (same size as the original data set), for training (replace=True to sample with
    replacement):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用numpy.random.choice生成带替换的重抽样样本非常简单。假设我们有一个包含50个训练示例的数据集（比如说，具有独特ID的患者记录，从0到49）。我们可以生成一个大小为50的重抽样样本（与原始数据集大小相同），用于训练（replace=True表示带替换抽样）：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This produces the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Can you spot the repeats in this bootstrap sample? This bootstrap sample now
    serves as one replicate of the original data set and can be used for training.
    The corresponding OOB sample is all the examples *not* in the bootstrap sample:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你能在这个重抽样样本中找到重复项吗？这个重抽样样本现在作为原始数据集的一个重复样本，可以用来训练。相应的OOB样本是所有不在重抽样样本中的示例：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It’s easy to verify that there is no overlap between the bootstrap subset and
    the OOB subset. This means that the OOB sample can be used as a “test set.” To
    summarize: after one round of bootstrap sampling, we get one bootstrap sample
    (for training a base estimator) and a corresponding OOB sample (to evaluate *that*
    base estimator).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易验证重抽样子集和OOB子集之间没有重叠。这意味着OOB样本可以用作“测试集”。总结一下：经过一轮重抽样，我们得到一个重抽样样本（用于训练基础估计器）和一个相应的OOB样本（用于评估*那个*基础估计器）。
- en: NOTE Sampling with replacement drops certain items but, more importantly, replicates
    other items. When applied to a data set, bootstrap sampling can be used to create
    training sets with replicates. You can think of these replicates as *weighted
    training examples*. For instance, if a particular example is replicated four times
    in the bootstrap sample, when used for training a base estimator, these four replicates
    will be equivalent to using a single training example with a weight of 4\. In
    this manner, different random bootstrap samples are effectively randomly sampled
    and weighted training sets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：带替换的抽样会丢弃某些项目，但更重要的是，会复制其他项目。当应用于数据集时，带替换的抽样可以用来创建包含重复项的训练集。你可以将这些重复项视为*加权训练示例*。例如，如果一个特定的示例在重抽样样本中重复了四次，当用于训练基础估计器时，这四个重复项将相当于使用一个权重为4的单个训练示例。以这种方式，不同的随机重抽样样本实际上是从随机抽样和加权训练集中抽取的。
- en: When we repeat this step many times, we’ll have trained several base estimators
    and will also have estimated their individual generalization performances through
    individual OOB errors. The averaged OOB error is a good estimate of the performance
    of the overall ensemble.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们多次重复这一步骤时，我们将训练多个基础估计器，并且也会通过单个OOB误差来估计它们的个别泛化性能。平均OOB误差是对整体集成性能的良好估计。
- en: 0.632 bootstrap
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 0.632重抽样
- en: When sampling with replacement, the bootstrap sample will contain roughly 63.2%
    of the data set, while the OOB sample will contain the other 36.8% of the data
    set. We can show this by computing the probabilities of a data point being sampled.
    If our data set has *n* training examples, the probability of picking one particular
    data point *x* in the bootstrap sample is (1/*n*). The probability of not picking
    *x* in the bootstrap sample (i.e., picking *x* in the OOB sample) is 1 - (1/*n*).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行有放回的抽样时，自助样本将包含大约63.2%的数据集，而OOB样本将包含数据集的其余36.8%。我们可以通过计算数据点被抽样的概率来展示这一点。如果我们的数据集有*n*个训练示例，则在自助样本中选中特定数据点*x*的概率是(1/*n*)。在自助样本中未选中*x*的概率（即，在OOB样本中选中*x*）是1
    - (1/*n*）。
- en: For *n* data points, the overall probability of being selected in the OOB sample
    is
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*n*个数据点，被选入OOB样本的整体概率是
- en: '![CH02_F03_Kunapuli_E03](../Images/CH02_F03_Kunapuli_E03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F03_Kunapuli_E03](../Images/CH02_F03_Kunapuli_E03.png)'
- en: (for sufficiently large *n*). Thus, each OOB sample will contain (approximately)
    36.8% of the training examples, and the corresponding bootstrap sample will contain
    (approximately) the remaining 63.2% of the instances.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: （对于足够大的*n*）。因此，每个OOB样本将包含（大约）36.8%的训练示例，相应的自助样本将包含（大约）剩余的63.2%的实例。
- en: Model aggregation
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 模型聚合
- en: Bootstrap sampling generates diverse replicates of the data set, which allows
    us to train diverse models independently of each other. Once trained, we can use
    this ensemble for prediction. The key is to combine their sometimes-differing
    opinions into a single final answer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自助抽样生成数据集的多样化副本，这使得我们可以独立地训练多样化的模型。一旦训练完成，我们可以使用这个集成进行预测。关键是将它们有时不同的意见结合成一个单一的最终答案。
- en: 'We’ve seen two examples of model aggregation: majority voting and model averaging.
    For classification tasks, majority voting is used to aggregate predictions of
    individual base learners. The majority vote is also known as the *statistical
    mode*. The mode is simply the most frequently occurring element and is a statistic
    similar to the mean or the median.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了两种模型聚合的例子：多数投票和模型平均。对于分类任务，多数投票用于聚合单个基础学习器的预测。多数投票也被称为*统计众数*。众数简单地是最频繁出现的元素，是一种类似于均值或中位数的统计量。
- en: 'We can think of model aggregation as averaging: it smooths out imperfections
    among the chorus and produces a single answer reflective of the majority. If we
    have a set of robust base estimators, model aggregation will smooth out mistakes
    made by individual estimators.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将模型聚合视为平均：它平滑了合唱团中的不完美之处，并产生一个反映多数的单一答案。如果我们有一组稳健的基础估计器，模型聚合将平滑掉单个估计器犯的错误。
- en: Ensemble methods use a variety of aggregation techniques depending on the task,
    including majority vote, mean, weighted mean, combination functions, and even
    another machine-learning model! In this chapter, we’ll stick to majority voting
    as our aggregator. We’ll explore some other aggregation techniques for classification
    in chapter 3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法根据任务使用各种聚合技术，包括多数投票、均值、加权均值、组合函数，甚至另一个机器学习模型！在本章中，我们将坚持使用多数投票作为我们的聚合器。我们将在第3章中探索一些其他用于分类的聚合技术。
- en: 2.2.2 Implementing bagging
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 实现多重抽样
- en: 'We can implement our own version of bagging easily. This illustrates the simplicity
    of bagging and provides a general template for how other ensemble methods in this
    chapter work. Each base estimator in our bagging ensemble is trained *independently*
    using the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松实现自己的版本的多重抽样。这说明了多重抽样的简单性，并为本章中其他集成方法的工作提供了一个通用模板。在我们的多重抽样集成中，每个基础估计器的训练都是**独立地**按照以下步骤进行的：
- en: Generate a bootstrap sample from the original data set.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始数据集中生成一个自助样本。
- en: Fit a base estimator to the bootstrap sample.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将基础估计器拟合到自助样本。
- en: Here, “independently” means that the training stage of each individual base
    estimator takes place without consideration of what is going on with the other
    base estimators.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，“独立地”意味着每个单个基础估计器的训练阶段在没有考虑其他基础估计器的情况下进行。
- en: 'We use decision trees as base estimators; the maximum depth can be set using
    the max_depth parameter. We’ll need two other parameters: n_estimators, which
    is the ensemble size, and max_samples, which is the size of the bootstrap subset,
    that is, the number of training examples to sample (with replacement) per estimator.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用决策树作为基估计器；最大深度可以使用max_depth参数设置。我们还需要两个其他参数：n_estimators，即集成大小，和max_samples，即bootstrap子集的大小，即每个估计器要采样的训练示例数量（有放回）。
- en: Our naïve implementation trains each base decision tree sequentially, as shown
    in listing 2.1\. If it takes 10 seconds to train a single decision tree, and we’re
    training an ensemble of 100 trees, it will take our implementation 10 s × 100
    = 1,000 s of total training time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的天真实现按顺序训练每个基决策树，如列表2.1所示。如果训练单个决策树需要10秒，而我们正在训练一个包含100棵树的集成，那么我们的实现将需要10秒
    × 100 = 1000秒的总训练时间。
- en: 'Listing 2.1 Bagging with decision trees: training'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 使用决策树的Bagging：训练
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Initializes a random seed
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化一个随机种子
- en: ❷ Creates a list of untrained base estimators
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个未训练的基估计器列表
- en: ❸ Generates a bootstrap sample
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成一个bootstrap样本
- en: ❹ Fits a tree to the bootstrap sample
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将树拟合到bootstrap样本
- en: This function will return a list of DecisionTreeClassifier objects. We can use
    this ensemble for prediction, which is implemented in the following listing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将返回一个DecisionTreeClassifier对象的列表。我们可以使用这个集成进行预测，这已在以下列表中实现。
- en: 'Listing 2.2 Bagging with decision trees: prediction'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 使用决策树的Bagging：预测
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Predicts each test example using each estimator in the ensemble
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用集成中的每个估计器预测每个测试示例
- en: ❷ Makes the final predictions by majority voting
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过多数投票进行最终预测
- en: We can test our implementation on 2D data and visualize the results, as shown
    in the following snippet. Our bagging ensemble has 500 decision trees, each of
    depth 12 and trained on bootstrap samples of size 300.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在二维数据上测试我们的实现并可视化结果，如下面的代码片段所示。我们的Bagging集成包含500棵决策树，每棵树的深度为12，并在大小为300的bootstrap样本上训练。
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Creates a 2D data set
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个二维数据集
- en: ❷ Trains a bagging ensemble
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练一个Bagging集成
- en: ❸ Makes the final predictions by majority voting
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过多数投票进行最终预测
- en: 'This snippet produces the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将产生以下输出：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our bagging implementation achieves a test set accuracy of 89.90%. We can now
    see what a bagged ensemble looks like, compared to a single tree, which achieves
    a test set accuracy of 83.84% (figure 2.4).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Bagging实现达到了测试集准确率89.90%。现在我们可以看到Bagging集成与单个树相比的样子，单个树的测试集准确率为83.84%（图2.4）。
- en: '![CH02_F04_Kunapuli](../Images/CH02_F04_Kunapuli.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F04_Kunapuli](../Images/CH02_F04_Kunapuli.png)'
- en: Figure 2.4 A single decision tree (left) overfits the training set and can be
    sensitive to outliers. A bagging ensemble (right) smooths out the overfitting
    effects and misclassifications of several such base estimators and often returns
    a robust answer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 单个决策树（左）对训练集过拟合，并且可能对异常值敏感。Bagging集成（右）平滑了几个此类基估计器的过拟合效应和误分类，通常返回一个鲁棒的答案。
- en: Bagging can learn fairly complex and nonlinear decision boundaries. Even if
    individual decision trees (and, generally, base estimators) are sensitive to outliers,
    the ensemble of base learners will smooth out individual variations and be more
    robust.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging可以学习相当复杂和非线性的决策边界。即使单个决策树（以及通常的基估计器）对异常值敏感，基学习器的集成将平滑个别变化并更加鲁棒。
- en: This smoothing behavior of bagging is due to model aggregation. When we have
    many highly nonlinear classifiers, each trained on a slightly different replicate
    of the training data, each may overfit, but they don’t all overfit the same way.
    More importantly, aggregating leads to smoothing, which effectively reduces the
    effect of overfitting! Thus, when we aggregate predictions, it smooths out the
    errors, improving the ensemble performance! Much like an orchestra, the final
    result is a smooth symphony that can easily overcome the mistakes of any individual
    musician in it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging的这种平滑行为是由于模型聚合。当我们有许多高度非线性的分类器，每个分类器都在略微不同的训练数据副本上训练时，每个分类器可能会过拟合，但它们不会以相同的方式过拟合。更重要的是，聚合导致平滑，这有效地减少了过拟合的影响！因此，当我们聚合预测时，它平滑了错误，提高了集成性能！就像一个管弦乐队一样，最终结果是平滑的交响乐，可以轻松克服其中任何个别音乐家的错误。
- en: 2.2.3 Bagging with scikit-learn
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 使用scikit-learn进行Bagging
- en: Now that we’re armed with an under-the-hood understanding of how bagging works,
    let’s look at how to use scikit-learn’s BaggingClassifier package, as shown in
    the following listing. scikit-learn’s implementation provides additional functionality,
    including support for parallelization, the ability to use other base-learning
    algorithms beyond decision trees, and—most importantly—OOB evaluation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Bagging的工作原理，让我们看看如何使用scikit-learn的BaggingClassifier包，如下所示。scikit-learn的实现提供了额外的功能，包括对并行化的支持，能够使用除决策树之外的其他基础学习算法，最重要的是OOB评估。
- en: Listing 2.3 Bagging with scikit-learn
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3 使用scikit-learn的Bagging
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Sets the base-learning algorithm along with hyperparameters
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置基学习算法及其超参数
- en: ❷ Trains 500 base estimators
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练500个基估计器
- en: ❸ Each base estimator will be trained on a bootstrap sample of size 100.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个基估计器将在大小为100的自助样本上训练。
- en: ❹ Uses an OOB sample to estimate the generalization error
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用OOB样本来估计泛化误差
- en: BaggingClassifier supports OOB evaluation and will return OOB accuracy if we
    set oob_score=True. Recall that for each bootstrap sample, we also have a corresponding
    OOB sample that contains all the data points that weren’t selected during sampling.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: BaggingClassifier支持OOB评估，如果我们设置oob_score=True，它将返回OOB准确率。回想一下，对于每个自助样本，我们还有一个相应的OOB样本，它包含在采样过程中未选中的所有数据点。
- en: 'Thus, each OOB sample is a surrogate for “future data” because it isn’t used
    to train the corresponding base estimator. After training, we can query the learned
    model to obtain the OOB score:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个OOB样本都是“未来数据”的替代品，因为它没有用于训练相应的基估计器。训练后，我们可以查询学习到的模型以获取OOB分数：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The OOB score is an estimate of the bagging ensemble’s predictive (generalization)
    performance (here, 96.6%). In addition to the OOB samples, we’ve also held out
    a test set. We compute another estimate of this model’s generalization on the
    test set:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: OOB分数是对Bagging集成预测（泛化）性能的估计（此处为96.6%）。除了OOB样本外，我们还保留了一个测试集。我们计算了该模型在测试集上的泛化性能的另一个估计：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The test accuracy is 95.2%, which is pretty close to the OOB score. We used
    decision trees of maximum depth 10 as base estimators. Deeper decision trees are
    more complex, which allows them to fit (and even overfit) the training data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率为95.2%，与OOB分数非常接近。我们使用了最大深度为10的决策树作为基础估计器。更深的决策树更复杂，这使得它们能够拟合（甚至过度拟合）训练数据。
- en: TIP Bagging is most effective with complex and nonlinear classifiers that tend
    to overfit the data. Such complex, overfitting models are *unstable*, that is,
    highly sensitive to small variations in the training data. To see why, consider
    that individual decision trees in a bagged ensemble have roughly the same complexity.
    However, due to bootstrap sampling, they have been trained on different replicates
    of the data set and overfit differently. Put another way, they all overfit by
    roughly the same amount, but in different places. Bagging works best with such
    models because its model aggregation reduces overfitting, ultimately leading to
    a more robust and stable ensemble.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: TIP Bagging对于复杂和非线性分类器最有效，这些分类器倾向于过度拟合数据。这样的复杂、过度拟合的模型是不稳定的，也就是说，对训练数据中的微小变化非常敏感。为了了解原因，考虑一下，袋装集成中的单个决策树具有大致相同的复杂性。然而，由于自助采样，它们已经在数据集的不同副本上进行了训练，并且过度拟合的程度不同。换句话说，它们都以大致相同的程度过度拟合，但在不同的地方。Bagging之所以与这样的模型配合得最好，是因为其模型聚合减少了过度拟合，最终导致更稳健和稳定的集成。
- en: We can visualize the smoothing behavior of BaggingClassifier by comparing its
    decision boundary to its component base DecisionTreeClassifiers, as shown in figure
    2.5.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过比较BaggingClassifier的决策边界与其组件基础决策树分类器，如图2.5所示，来可视化BaggingClassifier的平滑行为。
- en: '![CH02_F05_Kunapuli](../Images/CH02_F05_Kunapuli.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F05_Kunapuli](../Images/CH02_F05_Kunapuli.png)'
- en: Figure 2.5 Bootstrap sampling leads to different base estimators overfitting
    differently, while model aggregation averages out individual mistakes and produces
    smoother decision boundaries.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 自助采样导致不同的基估计器以不同的方式过度拟合，而模型聚合平均了个体错误并产生了更平滑的决策边界。
- en: 2.2.4 Faster training with parallelization
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 使用并行化加速训练
- en: Bagging is a parallel ensemble algorithm as it trains each base learner independently
    of other base learners. This means that training bagging ensembles can be parallelized
    if you have access to computing resources such as multiple cores or clusters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging是一种并行集成算法，因为它独立于其他基础学习器训练每个基础学习器。这意味着如果可以访问如多个核心或集群等计算资源，则可以并行化训练Bagging集成。
- en: BaggingClassifier supports the speedup of both training and prediction through
    the n_jobs parameter. By default, this parameter is set to 1, and bagging will
    run on one CPU and train models one at a time, sequentially.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: BaggingClassifier通过n_jobs参数支持训练和预测的加速。默认情况下，此参数设置为1，袋装法将在一个CPU上运行，并逐个顺序训练模型。
- en: Alternatively, you can specify the number of concurrent processes BaggingClassifier
    should use by setting n_jobs. If n_jobs is set to -1, then all available CPUs
    will be used for training, with one ensemble trained per CPU. This, of course,
    allows training to proceed faster as more models are trained simultaneously and
    in parallel.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以通过设置n_jobs来指定BaggingClassifier应使用的并发进程数。如果n_jobs设置为-1，则所有可用的CPU都将用于训练，每个CPU训练一个集成。这当然允许通过同时和并行训练更多模型来加快训练速度。
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ If n_jobs is set to -1, BaggingClassifier uses all available CPUs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果n_jobs设置为-1，BaggingClassifier将使用所有可用的CPU。
- en: '![CH02_F06_Kunapuli](../Images/CH02_F06_Kunapuli.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F06_Kunapuli](../Images/CH02_F06_Kunapuli.png)'
- en: Figure 2.6 Bagging can be parallelized to increase training efficiency.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 袋装法可以并行化以提高训练效率。
- en: Figure 2.6 compares the training efficiency of bagging trained with 1 CPU (n_jobs=1)
    with multiple CPUs (n_jobs=-1) on a machine with six cores. This comparison shows
    that bagging can be effectively parallelized and training times significantly
    reduced if we have access to sufficient computational resources.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6比较了在具有六个核心的机器上使用1个CPU（n_jobs=1）与多个CPU（n_jobs=-1）训练袋装法的训练效率。这种比较表明，如果我们可以访问足够的计算资源，袋装法可以有效地并行化，并且训练时间可以显著减少。
- en: 2.3 Random forests
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 随机森林
- en: We’ve seen how bagging uses random sampling with replacement, or bootstrap sampling,
    for ensemble diversity. Now, let’s look at random forests, a special extension
    of bagging that introduces additional randomization to further promote ensemble
    diversity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用带替换的随机抽样，即自助抽样，来增加集成多样性。现在，让我们看看随机森林，它是袋装法的特殊扩展，引入了额外的随机化来进一步促进集成多样性。
- en: Until the emergence of gradient boosting (see chapters 5 and 6), random forests
    were state of the art and were widely utilized. They are still a popular go-to
    method for many applications, especially in bioinformatics. Random forests can
    be an excellent off-the-shelf baseline for your data, as they are computationally
    efficient to train. They can also rank data features by importance, which makes
    them particularly suited for high-dimensional data analysis.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 直到梯度提升（见第5章和第6章）的出现，随机森林是当时最先进的，并且被广泛使用。它们仍然是许多应用的流行首选方法，尤其是在生物信息学中。随机森林可以作为您数据的优秀现成基线，因为它们训练起来计算效率高。它们还可以按重要性对数据特征进行排序，这使得它们特别适合于高维数据分析。
- en: 2.3.1 Randomized decision trees
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 随机决策树
- en: “Random forest” specifically refers to an ensemble of randomized decision trees
    constructed using bagging. Random forests perform bootstrap sampling to generate
    a training subset (exactly like bagging), and then use randomized decision trees
    as base estimators.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: “随机森林”特指使用袋装法构建的随机决策树集成。随机森林执行自助抽样以生成训练子集（与袋装法完全一样），然后使用随机决策树作为基估计器。
- en: Randomized decision trees are trained using a modified decision-tree learning
    algorithm, which introduces randomness when growing our trees. This additional
    source of randomness increases ensemble diversity and generally leads to better
    predictive performance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 随机决策树使用修改后的决策树学习算法进行训练，该算法在生长我们的树时引入随机性。这种额外的随机性增加了集成多样性，通常会导致更好的预测性能。
- en: The key difference between a standard decision tree and a randomized decision
    tree is how a decision node is constructed. In standard decision-tree construction,
    *all available features* are evaluated exhaustively to identify the best feature
    to split on. Because decision-tree learning is a greedy algorithm, it will choose
    the highest-scoring features to split on.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 标准决策树与随机决策树之间的关键区别在于决策节点的构建方式。在标准决策树构建中，*所有可用的特征*都会被彻底评估以确定最佳的分割特征。由于决策树学习是一个贪婪算法，它将选择得分最高的特征进行分割。
- en: When bagging, this exhaustive enumeration (combined with greedy learning) means
    that it’s often possible that the same small number of dominant features are repeatedly
    used in different trees. This makes the ensemble less diverse.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装法中，这种穷举搜索（与贪婪学习相结合）意味着在多个树中可能会反复使用相同的小数量主导特征。这使得集成多样性降低。
- en: To overcome this limitation of standard decision-tree learning, random forests
    introduce an additional element of randomness into tree learning. Specifically,
    instead of considering *all* the features to identify the best split, a random
    subset of features is evaluated to identify the best feature to split on.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服标准决策树学习的这一局限性，随机森林在树学习中引入了额外的随机元素。具体来说，不是考虑*所有*特征来识别最佳分割，而是评估一个随机特征子集以确定最佳的分割特征。
- en: Thus, random forests use a modified tree learning algorithm, which first randomly
    samples features before creating a decision node. The resulting tree is a *randomized
    decision tree*, which is a new type of base estimator.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机森林使用了一种修改后的树学习算法，该算法首先随机采样特征，然后创建决策节点。生成的树是一个*随机决策树*，这是一种新的基估计器。
- en: 'As you’ll see, random forests essentially extend bagging by using randomized
    decision trees as base estimators. Thus, random forests contain two types of randomization:
    (1) bootstrap sampling, similar to bagging; and (2) random feature sampling for
    learning randomized decision trees.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，随机森林通过使用随机决策树作为基估计器来扩展bagging。因此，随机森林包含两种类型的随机化：（1）与bagging类似的bootstrap采样；（2）用于学习随机决策树的随机特征采样。
- en: 'Example: Randomization in tree learning'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：树学习中的随机化
- en: Consider tree learning on a data set with six features (here, {*f*[1],*f*[2],*f*[3],*f*[4],*f*[5],*f*[6]}).
    In standard tree learning, all six features are evaluated, and the best splitting
    feature is identified (say, *f*[3]).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在具有六个特征的数据集上进行树学习（这里，指{*f*[1],*f*[2],*f*[3],*f*[4],*f*[5],*f*[6]}）。在标准树学习中，所有六个特征都会被评估，并确定最佳分割特征（例如，*f*[3]）。
- en: In randomized decision-tree learning, we first randomly sample a subset of features
    (say, *f*[2],*f*[4],*f*[5]} and then choose the best from among them (which is,
    say, *f*[5]). This means that the feature *f*[3] is no longer available at this
    stage of tree learning. Thus, randomization has inherently forced tree learning
    to split on a different feature. The impact of randomization on the choice of
    the next best split during tree learning is illustrated in figure 2.7.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机决策树学习中，我们首先随机采样一个特征子集（例如，*f*[2],*f*[4],*f*[5]}），然后从中选择最佳特征（例如，*f*[5]）。这意味着特征
    *f*[3] 在这一阶段的树学习中不再可用。因此，随机化本质上迫使树学习在不同的特征上进行分割。随机化对树学习过程中下一个最佳分割选择的影響在图2.7中得到了说明。
- en: '![CH02_F07_Kunapuli](../Images/CH02_F07_Kunapuli.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F07_Kunapuli](../Images/CH02_F07_Kunapuli.png)'
- en: Figure 2.7 Random forests use a modified tree learning algorithm, where a random
    feature subset is first chosen before the best splitting criterion for each decision
    node is identified. The unshaded columns represent features that have been left
    out; the lightly shaded columns represent available features from which the best
    feature is chosen, shown in the darkly shaded columns.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7展示了随机森林使用了一种修改后的树学习算法，在该算法中，首先选择一个随机特征子集，然后确定每个决策节点的最佳分割标准。无阴影的列表示已排除的特征；浅阴影的列表示可用于选择最佳特征的可用特征，这些特征在深阴影的列中显示。
- en: Ultimately, this randomization occurs every time a decision node is constructed.
    Thus, even if we use the same data set, we’ll obtain a different randomized tree
    each time we train. When randomized tree learning (with a random sampling of features)
    is combined with bootstrap sampling (with a random sampling of training examples),
    we obtain an ensemble of randomized decision trees, known as a *random decision
    forest* or simply random forest.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这种随机化会在构建每个决策节点时发生。因此，即使我们使用相同的数据集，每次训练时也会得到不同的随机化树。当随机树学习（带有特征随机采样的随机树学习）与自助采样（带有训练样本随机采样的自助采样）相结合时，我们得到一个随机决策树集合，称为*随机决策森林*或简称为随机森林。
- en: The random forest ensemble will be more diverse than bagging, which only performs
    bootstrap sampling. Next, we’ll see how to use random forests in practice.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林集合将比仅执行自助采样的bagging更加多样化。接下来，我们将看到如何在实践中使用随机森林。
- en: 2.3.2 Random forests with scikit-learn
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 使用scikit-learn的随机森林
- en: scikit-learn provides an efficient implementation of random forests that also
    supports OOB estimation and parallelization. Because random forests are specialized
    to use decision trees as base learners, RandomForestClassifier also takes DecisionTreeClassifier
    parameters such as max_leaf_nodes and max_depth to control tree complexity. The
    following listing demonstrates how to call RandomForestClassifier.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了一个高效的随机森林实现，它还支持OOB估计和并行化。由于随机森林专门使用决策树作为基础学习器，RandomForestClassifier也接受DecisionTreeClassifier参数，如max_leaf_nodes和max_depth来控制树复杂性。以下列表演示了如何调用RandomForestClassifier。
- en: Listing 2.4 Random forests with scikit-learn
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 使用scikit-learn的随机森林
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Controls the complexity of base decision trees
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 控制基础决策树的复杂性
- en: ❷ Parallelizes, if possible
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果可能，并行化
- en: ❸ Uses an OOB sample to estimate the generalization error
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用OOB样本估计泛化误差
- en: Figure 2.8 illustrates a random forest classifier, along with several component
    base estimators.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8展示了随机森林分类器，以及几个组件基础估计器。
- en: '![CH02_F08_Kunapuli](../Images/CH02_F08_Kunapuli.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F08_Kunapuli](../Images/CH02_F08_Kunapuli.png)'
- en: Figure 2.8 Random forest (top left) compared to individual base learners (randomized
    decision trees). Much like bagging, the random forest ensemble also produces a
    smooth and stable decision boundary. Also observe the effect of randomization
    on the individual trees, which are far spikier than regular decision trees.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 随机森林（左上角）与单个基础学习器（随机决策树）的比较。与袋装法类似，随机森林集成也产生平滑且稳定的决策边界。同时观察随机化对单个树的影响，这些树比常规决策树更尖锐。
- en: 2.3.3 Feature importances
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 特征重要性
- en: One benefit of using random forests is that they also provide a natural mechanism
    for scoring features based on their importance. This means that we can rank features
    to identify the most important ones and drop less effective features, thus performing
    feature selection!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林的一个好处是它们还提供了一种基于其重要性的自然机制来评分特征。这意味着我们可以对特征进行排序，以识别最重要的特征，并删除效果较差的特征，从而执行特征选择！
- en: Feature selection
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择
- en: '*Feature selection*, also known as *variable subset selection*, is a procedure
    for identifying the most influential or relevant data features/attributes. Feature
    selection is an important step of the modeling process, especially for high-dimensional
    data.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择**，也称为**变量子集选择**，是一种识别最有影响力或相关数据特征/属性的过程。特征选择是建模过程中的重要步骤，尤其是在高维数据中。'
- en: Dropping the least-relevant features often improves generalization performance
    and minimizes overfitting. It also often improves the computational efficiency
    of training. These concerns are consequences of the *curse of dimensionality*,
    where a large number of features can inhibit a model’s ability to generalize effectively.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 删除最不相关的特征通常可以提高泛化性能并最小化过拟合。这也有助于提高训练的计算效率。这些问题是**维度诅咒**的结果，其中大量的特征可能会抑制模型有效泛化的能力。
- en: 'See *The Art of Feature Engineering: Essentials for Machine Learning* by Pablo
    Duboue (Cambridge University Press, 2020) to learn more about feature selection
    and engineering.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 参考Pablo Duboue的《特征工程的艺术：机器学习必备》（剑桥大学出版社，2020年）以了解更多关于特征选择和工程的信息。
- en: 'We can obtain feature importances for the simple 2D data set with the query
    rf_ens.feature_importances_:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用查询rf_ens.feature_importances_为简单的二维数据集获取特征重要性：
- en: '[PRE13]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This produces the following output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The feature scores for the simple two-dimensional data set suggest that both
    features are roughly equally important. In the case study toward the end of the
    chapter, we’ll compute and visualize the feature importances for a data set from
    a real task: breast cancer diagnosis. We’ll also revisit and delve deeper into
    the topic of feature importances in chapter 9.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的二维数据集，特征分数表明两个特征的重要性大致相等。在章节末尾的案例研究中，我们将计算并可视化来自真实任务的特性重要性：乳腺癌诊断。我们还将回顾并深入探讨第9章中的特性重要性主题。
- en: Note that feature importances sum to 1 and are effectively *feature weights*.
    Less important features have lower weights and can often be dropped without significantly
    affecting the overall quality of the final model, while improving training and
    prediction times.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征重要性之和为1，并且实际上是**特征权重**。不重要的特征具有较低的权重，通常可以删除而不会显著影响最终模型的整体质量，同时提高训练和预测时间。
- en: Feature importances with correlated features
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 具有关联特征的特性重要性
- en: If two features are strongly correlated or dependent, then, intuitively, we
    know that it’s sufficient to use either one of them in the model. However, the
    *order* in which the features are used can affect feature importance. For instance,
    when classifying abalone (sea snails), the features size and weight are highly
    correlated (unsurprising, since bigger snails will be heavier). This means that
    including them in a decision tree will add roughly the same amount of information
    and cause the overall error (or entropy) to decrease by roughly the same amount.
    Thus, we expect that their mean error decrease scores will be the same.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个特征高度相关或依赖，那么直观上我们知道，在模型中使用其中任何一个就足够了。然而，特征使用的*顺序*可能会影响特征重要性。例如，在分类鲍鱼（海蜗牛）时，大小和重量特征高度相关（不出所料，因为更大的蜗牛会更重）。这意味着将它们包含在决策树中会增加大约相同的信息量，并导致整体错误（或熵）大致以相同的方式减少。因此，我们预计它们的平均错误减少分数将是相同的。
- en: However, say we select weight first as a splitting variable. Adding this feature
    to the tree removes information contained in both size and weight features. This
    means that the feature importance of size is reduced because any decrease in error
    that we could have had by including size in our model was already previously decreased
    by including weight. Thus, correlated features can sometimes be assigned imbalanced
    feature importances. Random feature selection mitigates this problem a little,
    but not consistently.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设我们首先选择权重作为分割变量。将此特征添加到树中会移除大小和权重特征中包含的信息。这意味着大小的特征重要性降低，因为我们通过在模型中包含大小来减少错误的可能性已经被包含权重时减少。因此，相关特征有时会被分配不平衡的特征重要性。随机特征选择可以稍微减轻这个问题，但并不一致。
- en: In general, you must proceed with caution when interpreting feature importances
    in the presence of feature correlations so that you don’t miss the whole story
    in the data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在存在特征相关性的情况下解释特征重要性时，你必须谨慎行事，以免错过数据中的整个故事。
- en: 2.4 More homogeneous parallel ensembles
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 更同质化的并行集成
- en: 'We’ve seen two important parallel homogeneous ensemble methods: bagging and
    random forests. Let’s now explore a few variants that were developed for large
    data sets (e.g., recommendation systems) or high-dimensional data (e.g., image
    or text databases). These include bagging variants such as pasting, random subspaces
    and random patches, and an extreme random forest variant called Extra Trees. All
    these methods introduce randomization in different ways to ensure ensemble diversity.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了两种重要的并行同质集成方法：Bagging和随机森林。现在让我们探索一些为大型数据集（例如，推荐系统）或高维数据（例如，图像或文本数据库）开发的变体。这些包括Bagging变体，如粘贴、随机子空间和随机补丁，以及一个称为Extra
    Trees的极端随机森林变体。所有这些方法都以不同的方式引入随机化，以确保集成多样性。
- en: 2.4.1 Pasting
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 粘贴
- en: Bagging uses bootstrap sampling, or sampling with replacement. If, instead,
    we sample subsets for training *without replacement*, we have a variant of bagging
    known as *pasting*. Pasting was designed for very large data sets, where sampling
    with replacement isn’t necessary. Instead, because training full models on data
    sets of such scale is difficult, pasting aims to take small pieces of the data
    by sampling without replacement.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging使用自助采样，或带替换的采样。如果我们不替换地采样训练子集，我们就有了Bagging的一个变体，称为*粘贴*。粘贴是为非常大的数据集设计的，其中不需要带替换的采样。相反，由于在如此规模的数据集上训练完整模型是困难的，粘贴旨在通过不替换地采样来获取数据的小部分。
- en: Pasting exploits the fact that sampling without replacement with a very large
    data set can inherently generate diverse data subsets, which in turn leads to
    ensemble diversity. Pasting also ensures that each training subsample is a small
    piece of the overall data set and can be used to train a base learner efficiently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 粘贴利用了这样一个事实，即使用非常大的数据集进行不替换的采样可以固有地生成多样化的数据子集，这反过来又导致集成多样性。粘贴还确保每个训练子样本是整体数据集的小部分，并且可以用来有效地训练基学习器。
- en: Model aggregation is still used to make a final ensemble prediction. However,
    since each base learner is trained on small pieces of the large data set, we can
    view model aggregation as *pasting the predictions* of the base learners together
    for a final prediction.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 模型聚合仍然用于做出最终的集成预测。然而，由于每个基学习器都是在大型数据集的小部分上训练的，我们可以将模型聚合视为将基学习器的预测*粘贴在一起*以做出最终预测。
- en: TIP BaggingClassifier can easily be extended to perform pasting by setting bootstrap=False
    and making it subsample small subsets for training by setting max_samples to a
    small fraction, say max_samples=0.05.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: TIP BaggingClassifier可以通过设置bootstrap=False并设置max_samples为一个小分数（例如max_samples=0.05）来轻松扩展以执行粘贴，从而通过训练小子集进行采样。
- en: 2.4.2 Random subspaces and random patches
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 随机子空间和随机补丁
- en: We can make the base learners even more diverse by randomly sampling the features
    (see figure 2.9) as well. Instead of sampling training examples, if we generate
    subsets by sampling features (with or without replacement), we obtain a variant
    of bagging called *random subspaces*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过随机采样特征（见图2.9）来使基学习器更加多样化。如果我们通过采样特征（带或不带替换）来生成子集，而不是采样训练示例，我们得到一种称为**随机子空间**的bagging变体。
- en: '![CH02_F09_Kunapuli](../Images/CH02_F09_Kunapuli.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F09_Kunapuli](../Images/CH02_F09_Kunapuli.png)'
- en: Figure 2.9 Bagging compared to random subspaces and random patches. The unshaded
    rows and columns represent training examples and features, respectively, that
    have been left out.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 与随机子空间和随机补丁相比的Bagging。未着色的行和列分别代表被留下的训练示例和特征。
- en: 'BaggingClassifier supports bootstrap sampling of features through two parameters:
    bootstrap_features (default: False) and max_features (default: 1.0, or all the
    features), which are analogous to the parameters bootstrap (default: False) and
    max_samples for sampling training examples, respectively. To implement random
    subspaces, we randomly sample features only:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: BaggingClassifier通过两个参数支持特征的bootstrap采样：bootstrap_features（默认：False）和max_features（默认：1.0，即所有特征），它们分别类似于采样训练示例的参数bootstrap（默认：False）和max_samples。要实现随机子空间，我们只随机采样特征：
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Uses all the training samples
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用所有训练样本
- en: ❷ Bootstrap samples 50% of features
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从特征中抽取50%的Bootstrap样本
- en: 'If we randomly sample *both* training examples and features (with or without
    replacement), we obtain a variant of bagging called *random patches*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们随机采样**所有**的训练示例和特征（带或不带替换），我们得到一种称为**随机补丁**的bagging变体：
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Bootstrap samples 75% of examples
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从示例中抽取75%的Bootstrap样本
- en: ❷ Bootstrap samples 50% of features
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从特征中抽取50%的Bootstrap样本
- en: Note that in the preceding examples, the base estimator is the support vector
    classifier, sklearn.svm.SVC. In general, random subspaces and random patches can
    be applied to any base learner to improve estimator diversity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的例子中，基估计器是支持向量分类器，sklearn.svm.SVC。一般来说，随机子空间和随机补丁可以应用于任何基学习器以提高估计器的多样性。
- en: TIP In practice, these variants of bagging can be especially effective for big
    data. For example, because random subspaces and random patches sample features,
    they can be used to train base estimators more efficiently for data with lots
    of features, such as image data. Alternatively, because pasting performs sampling
    without replacement, it can be used to train base estimators more efficiently
    when you have a big data set with a lot of training instances.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 实际上，这些bagging变体对于大数据特别有效。例如，因为随机子空间和随机补丁采样特征，它们可以用于更有效地训练具有许多特征的数据的基估计器，例如图像数据。或者，因为粘贴执行无替换的采样，当您有一个具有大量训练实例的大数据集时，它可以用于更有效地训练基估计器。
- en: The key difference between random forests and bagging variants, such as random
    subspaces and random patches, is where the feature sampling occurs. Random forests
    exclusively use randomized decision trees as base estimators. Specifically, they
    perform feature sampling *inside* the tree learning algorithm each time they grow
    the tree with a decision node.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林与随机子空间和随机补丁等bagging变体之间的关键区别在于特征采样的位置。随机森林仅使用随机决策树作为基估计器。具体来说，每次它们使用决策节点生长树时，都会在树学习算法中进行特征采样。
- en: Random subspaces and random patches, on the other hand, aren’t restricted to
    tree learning and can use any learning algorithm as a base estimator. They randomly
    sample features once *outside* before calling the base-learning algorithm for
    each base estimator.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，随机子空间和随机补丁不仅限于树学习，可以使用任何学习算法作为基估计器。它们在调用基学习算法之前，对每个基估计器进行一次**外部**的特征采样。
- en: 2.4.3 Extra Trees
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 额外树
- en: Extremely randomized trees take the idea of randomized decision trees to the
    extreme by selecting not just the splitting variable from a random subset of features
    (see figure 2.9) but also the splitting threshold! To understand this more clearly,
    recall that every node in a decision tree tests a condition of the form “is *f*[k]
    < *threshold?*” where *f*[k] is the kth feature, and *threshold* is the split
    value (see section 2.3.1).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机树将随机决策树的想法推向了极致，不仅从随机特征子集中选择分裂变量（见图 2.9），还选择分裂阈值！为了更清楚地理解这一点，请回忆决策树中的每个节点都测试一种形式为“is
    *f*[k] < *threshold?*”的条件，其中 *f*[k] 是第 k 个特征，*threshold* 是分裂值（参见 2.3.1 节）。
- en: Standard decision-tree learning looks at *all the features* to determine the
    best *f*[k] and then looks at all the values of that feature to determine the
    threshold. Randomized decision-tree learning looks at a *random subset* *of features*
    to determine the best *f*[k] and then looks at all the values of that feature
    to determine the threshold.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 标准决策树学习会查看**所有特征**以确定最佳 *f*[k]，然后查看该特征的所有值以确定阈值。随机决策树学习会查看**随机子集**的**特征**以确定最佳
    *f*[k]，然后查看该特征的所有值以确定阈值。
- en: Extremely randomized decision-tree learning also looks at a random subset of
    features to determine the best *f*[k]. But to be even more efficient, it selects
    a random splitting threshold. Note that extremely randomized decision trees are
    yet another type of base learner used for ensembling.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机决策树学习也会查看随机特征子集以确定最佳 *f*[k]。但为了更加高效，它选择一个随机的分裂阈值。请注意，极端随机树是另一种用于集成的基学习器。
- en: This extreme randomization is so effective, in fact, that we can construct an
    ensemble of extremely randomized trees directly from the original data set *without*
    bootstrap sampling! This means that we can construct an Extra Trees ensemble very
    efficiently.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这种极端随机化实际上非常有效，以至于我们可以直接从原始数据集构建极端随机树集成**而不**进行自助采样！这意味着我们可以非常高效地构建 Extra Trees
    集成。
- en: TIP In practice, Extra Trees ensembles are well suited for high-dimensional
    data sets with a large number of continuous features.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示** 在实践中，Extra Trees 集成非常适合具有大量连续特征的高维数据集。'
- en: scikit-learn provides an ExtraTreesClassifier that supports OOB estimation and
    parallelization, much like BaggingClassifier and RandomForestClassifier. Note
    that Extra Trees typically *do not* perform bootstrap sampling (bootstrap=False,
    by default), as we’re able to achieve base-estimator diversity through extreme
    randomization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了一个支持 OOB 估计和并行化的 ExtraTreesClassifier，与 BaggingClassifier 和
    RandomForestClassifier 类似。请注意，Extra Trees 通常**不**执行自助采样（默认为 bootstrap=False），因为我们能够通过极端随机化实现基估计器的多样性。
- en: 'CAUTION scikit-learn provides two very similarly named classes: sklearn. tree.ExtraTreeClassifier
    and sklearn.ensemble.ExtraTreesClassifier. The tree.ExtraTreeClassifier class
    is a base-learning algorithm and should be used for learning individual models
    or as a base estimator with ensemble methods. ensemble.ExtraTreesClassifier is
    the ensemble method discussed in this section. The difference is in the singular
    usage of “Extra Tree” (ExtraTreeClassifier is the base learner) versus the plural
    usage “Extra Trees” (ExtraTreesClassifier is the ensemble method).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告** scikit-learn 提供了两个非常相似命名的类：sklearn.tree.ExtraTreeClassifier 和 sklearn.ensemble.ExtraTreesClassifier。tree.ExtraTreeClassifier
    类是一个基学习算法，应用于学习单个模型或作为集成方法的基估计器。ensemble.ExtraTreesClassifier 是本节讨论的集成方法。区别在于“Extra
    Tree”的单数用法（ExtraTreeClassifier 是基学习器）与复数用法“Extra Trees”（ExtraTreesClassifier 是集成方法）。'
- en: '2.5 Case study: Breast cancer diagnosis'
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 案例研究：乳腺癌诊断
- en: 'Our first case study explores a medical decision-making task: breast cancer
    diagnosis. We’ll see how to use scikit-learn’s homogeneous parallel ensemble modules
    in practice. Specifically, we’ll train and evaluate the performance of three homogeneous
    parallel algorithms, each characterized by increasing randomness: bagging with
    decision trees, random forests, and Extra Trees.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一项案例研究探讨了医疗决策任务：乳腺癌诊断。我们将看到如何在实际中应用 scikit-learn 的同质并行集成模块。具体来说，我们将训练并评估三种同质并行算法的性能，每种算法的特点是随机性逐渐增加：决策树的自举、随机森林和
    Extra Trees。
- en: 'Doctors make many decisions regarding patient care every day: tasks such as
    diagnosis (what disease does the patient have?), prognosis (how will their disease
    progress?), treatment planning (how should the disease be treated?), to name a
    few. They make these decisions based on a patient’s health records, medical history,
    family history, test results, and so on.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 医生每天都会就患者护理做出许多决定：例如诊断（患者患有何种疾病？）、预后（疾病将如何进展？）、治疗计划（如何治疗疾病？）等。他们基于患者的健康记录、病史、家族史、检测结果等进行这些决定。
- en: The specific data set we’ll use is the Wisconsin Diagnostic Breast Cancer (WDBC)
    data set, a common benchmark data set in machine learning. Since 1993, the WDBC
    data has been used to benchmark the performance of dozens of machine-learning
    algorithms.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的具体数据集是威斯康星诊断乳腺癌（WDBC）数据集，这是机器学习中常用的基准数据集。自1993年以来，WDBC数据已被用于评估数十种机器学习算法的性能。
- en: The machine-learning task is to train a classification model that can diagnose
    patients with breast cancer. By modern standards and in the era of big data, this
    is a small data set, but it’s perfectly suited to show the ensemble methods we’ve
    seen so far in action.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习任务是训练一个分类模型，可以诊断患有乳腺癌的患者。按照现代标准和大数据时代，这是一个小数据集，但它非常适合展示我们迄今为止看到的集成方法。
- en: 2.5.1 Loading and preprocessing
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 加载和预处理
- en: The WDBC data set was originally created by applying feature extraction techniques
    on patient biopsy medical images. More concretely, for each patient, the data
    describes the size and texture of the cell nuclei of cells extracted during biopsy.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: WDBC数据集最初是通过在患者活检医学图像上应用特征提取技术创建的。更具体地说，对于每位患者，数据描述了活检过程中提取的细胞核的大小和纹理。
- en: 'WDBC is available in scikit-learn and can be loaded as shown in figure 2.10\.
    In addition, we also create a RandomState so that we can generate randomization
    in a reproducible manner:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: WDBC在scikit-learn中可用，如图2.10所示进行加载。此外，我们还创建了一个RandomState，以便我们可以以可重复的方式生成随机化：
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![CH02_F10_Kunapuli](../Images/CH02_F10_Kunapuli.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F10_Kunapuli](../Images/CH02_F10_Kunapuli.png)'
- en: Figure 2.10 The WDBC data set consists of 569 training examples, each described
    by 30 features. A few of the 30 features for a small subset of patients, along
    with each patient’s *diagnosis* (training label), are shown here. diagnosis=1
    indicates malignant, and diagnosis=0 indicates benign.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 WDBC数据集包含569个训练示例，每个示例由30个特征描述。这里展示了少量患者的一些30个特征，以及每位患者的*诊断*（训练标签）。诊断=1表示恶性，诊断=0表示良性。
- en: 2.5.2 Bagging, random forests, and Extra Trees
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 Bagging、随机森林和Extra Trees
- en: 'Once we’ve preprocessed our data set, we’ll train and evaluate bagging with
    decision trees, random forests, and Extra Trees to answer the following questions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们预处理完数据集，我们将使用决策树、随机森林和Extra Trees进行bagging的训练和评估，以回答以下问题：
- en: How does the ensemble performance change with ensemble size? That is, what happens
    when our ensembles get bigger and bigger?
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成性能如何随着集成大小的变化而变化？也就是说，当我们的集成变得越来越大时会发生什么？
- en: How does the ensemble performance change with base learner complexity? That
    is, what happens when our individual base estimators become more and more complex?
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成性能如何随着基学习器的复杂性而变化？也就是说，当我们的单个基估计器变得越来越复杂时会发生什么？
- en: In this case study, since all three ensemble methods considered use decision
    trees as base estimators, one “measure” of complexity is tree depth, with deeper
    trees being more complex.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，由于考虑的所有三种集成方法都使用决策树作为基估计器，因此复杂性的一个“度量”是树深度，深度越深的树越复杂。
- en: Ensemble size vs. ensemble performance
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 集成大小与集成性能
- en: First, let’s look at how training and testing performance change with ensemble
    size by comparing the behavior of the three algorithms as the parameter n_estimators
    increases. As always, we follow good machine-learning practices and split the
    data set into a training set and a hold-out test set randomly. Our goal will be
    to learn a diagnostic model on the training set and evaluate how well that diagnostic
    model does using the test set.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过比较随着参数n_estimators增加时三个算法的行为，来查看训练和测试性能如何随着集成大小而变化。一如既往，我们遵循良好的机器学习实践，随机将数据集分为训练集和保留测试集。我们的目标将在训练集上学习一个诊断模型，并使用测试集评估该诊断模型的效果。
- en: Recall that because the test set is held out during training, the test error
    is generally a useful estimate of how well we’ll do on future data, that is, generalize.
    However, because we don’t want our learning and evaluation to be at the mercy
    of randomness, we’ll repeat this experiment 20 times and average the results.
    In the following listing, we’ll see how the ensemble size influences model performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，由于测试集在训练期间被保留出来，因此测试错误通常是我们对未来数据表现的一个有用估计，即泛化。然而，由于我们不希望我们的学习和评估受随机性的摆布，我们将重复此实验20次并平均结果。在下面的列表中，我们将看到集成大小如何影响模型性能。
- en: Listing 2.5 Training and test errors with increasing ensemble size
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5 随着集成大小增加的训练和测试错误
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Every base decision tree in every ensemble will have at most eight leaf nodes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个集成中的每个基决策树最多有八个叶节点。
- en: ❷ Initializes arrays to store training errors
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化数组以存储训练错误
- en: ❸ Initializes arrays to store test errors
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化数组以存储测试错误
- en: ❹ Performs 20 runs, each with a different split of train/test data
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 执行20次运行，每次运行具有不同的训练/测试数据分割
- en: ❺ Trains and evaluates bagging for this run and iteration
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 训练并评估此运行和迭代的bagging
- en: ❻ Trains and evaluates random forests for this run and iteration
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 训练并评估此运行和迭代的随机森林
- en: ❼ Trains and evaluates Extra Trees for this run and iteration
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 训练并评估此运行和迭代的Extra Trees
- en: We can now visualize the averaged training and test errors on the WDBC data
    set, as shown in figure 2.11\. As expected, the training error for all the approaches
    decreases steadily as the number of estimators increases. The test error also
    decreases with ensemble size and then stabilizes. As the test error is an estimate
    of the generalization error, our experiment confirms our intuition about the performance
    of these ensemble methods in practice.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以可视化WDBC数据集上平均的训练和测试错误，如图2.11所示。正如预期的那样，随着估计器数量的增加，所有方法的训练错误稳步下降。测试错误也随着集成大小的增加而下降，然后稳定下来。由于测试错误是泛化误差的估计，我们的实验证实了我们对这些集成方法在实际性能上的直觉。
- en: '![CH02_F11_Kunapuli](../Images/CH02_F11_Kunapuli.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F11_Kunapuli](../Images/CH02_F11_Kunapuli.png)'
- en: Figure 2.11 Training and test performance of bagging, random forest, and Extra
    Trees as ensemble size increases. Bagging used decision trees as the base estimator,
    random forest used randomized decision trees, and Extra Trees used extremely randomized
    trees.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 随着集成大小增加，bagging、随机森林和Extra Trees的训练和测试性能。Bagging使用决策树作为基估计器，随机森林使用随机决策树，Extra
    Trees使用极端随机树。
- en: Finally, all three approaches greatly outperform single decision trees (where
    the plot begins). This shows that, in practice, even if single decision trees
    are unstable, ensembles of decision trees are robust and can generalize well.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有三种方法都大大优于单个决策树（如图所示）。这表明，在实践中，即使单个决策树不稳定，决策树的集成也是稳健的，并且可以很好地泛化。
- en: Base learner complexity vs. ensemble performance
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 基学习器复杂度与集成性能
- en: 'Next, we compare the behavior of the three algorithms as the complexity of
    the base learners increases (figure 2.12). There are several ways to control the
    complexity of the base decision trees: maximum depth, maximum number of leaf nodes,
    impurity criteria, and so on. Here, we compare the performance of the three ensemble
    methods with complexity of each base learner determined by max_leaf_nodes.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较随着基学习器复杂度的增加，三种算法的行为（如图2.12）。控制基决策树复杂度的方法有：最大深度、最大叶节点数、不纯度标准等。在这里，我们比较了三种集成方法在基学习器复杂度由max_leaf_nodes确定的性能。
- en: 'This comparison can be performed in a manner similar to the previous one. To
    allow each ensemble method to use increasingly complex base learners, we can steadily
    increase the number of in max_leaf_nodes for each base decision tree. That is,
    in each of BaggingClassifier, RandomForestClassifier, and ExtraTreesClassifier,
    we set max_leaf_nodes = 2, 4, 8, 16, and 32, in turn, through the following parameter:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这种比较可以以类似于之前的方式执行。为了允许每个集成方法使用越来越复杂的基学习器，我们可以逐步增加每个基决策树的最大叶节点数。也就是说，在BaggingClassifier、RandomForestClassifier和ExtraTreesClassifier中，我们依次设置max_leaf_nodes
    = 2, 4, 8, 16和32，通过以下参数：
- en: '[PRE19]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![CH02_F12_Kunapuli](../Images/CH02_F12_Kunapuli.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F12_Kunapuli](../Images/CH02_F12_Kunapuli.png)'
- en: Figure 2.12 Training and test performance of bagging, random forest, and Extra
    Trees as base learner complexity increases. Bagging used decision trees as the
    base estimator, random forest used randomized decision trees, and Extra Trees
    used extremely randomized trees.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 随着基学习器复杂性的增加，bagging、随机森林和Extra Trees的训练和测试性能。Bagging使用决策树作为基估计器，随机森林使用随机决策树，Extra
    Trees使用极端随机树。
- en: Recall that highly complex trees are inherently unstable and sensitive to small
    perturbations in the data. This means that, in general, if we increase the complexity
    of the base learners, we’ll need a lot more of them to successfully reduce the
    variability of the ensemble overall. Here, however, we’ve fixed n_estimators=10.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，高度复杂的树在本质上是不稳定的，并且对数据中的微小扰动非常敏感。这意味着，一般来说，如果我们增加基学习器的复杂性，我们需要更多的它们来成功减少集成整体的变异性。然而，在这里，我们已将n_estimators设置为10。
- en: One key consideration in determining the depth of the base decision trees is
    computational efficiency. Training deeper and deeper trees will take more and
    more time without producing a significant improvement in predictive performance.
    For instance, base decision trees of depths 24 and 32 perform roughly similarly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定基决策树深度的一个关键考虑因素是计算效率。训练越来越深的树将花费更多的时间，而不会在预测性能上产生显著的改进。例如，深度为24和32的基决策树表现大致相同。
- en: 2.5.3 Feature importances with random forests
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 随机森林的特征重要性
- en: Finally, let’s see how we can use feature importances to identify the most predictive
    features for breast cancer diagnosis using the random forest ensemble. Such analysis
    adds *interpretability* to the model and can be very helpful in communicating
    and explaining such models to domain experts such as doctors.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看我们如何可以使用特征重要性来识别使用随机森林集成对乳腺癌诊断最有预测性的特征。这种分析增加了模型的*可解释性*，并且在向领域专家如医生解释此类模型时非常有帮助。
- en: Feature importances from label correlations
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 标签相关性特征重要性
- en: First, let’s peek into the data set to see if we can discover some interesting
    relationships among the features and the diagnosis. This type of analysis is typical
    when we get a new data set, as we try to learn more about it. Here, our analysis
    will try to identify which features are most correlated with each other and with
    the diagnosis (label), so that we can check if random forests can do something
    similar. In the following listing, we use the pandas and seaborn packages to visualize
    feature and label correlations.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们查看数据集，看看我们是否可以发现特征和诊断之间的一些有趣关系。当我们得到一个新的数据集时，这种分析是典型的，因为我们试图更多地了解它。在这里，我们的分析将尝试确定哪些特征彼此之间以及与诊断（标签）最相关，这样我们就可以检查随机森林是否能做类似的事情。在下面的列表中，我们使用pandas和seaborn包来可视化特征和标签的相关性。
- en: Listing 2.6 Visualizing correlations between features and labels
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.6 可视化特征与标签之间的相关性
- en: '[PRE20]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Converts the data into a pandas DataFrame
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据转换为pandas DataFrame
- en: ❷ Computes and plots the correlation between some selected features and the
    label (diagnosis)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算并绘制一些选定的特征与标签（诊断）之间的相关性
- en: 'The output of this listing is shown in figure 2.13\. Several features are highly
    correlated with each other, for example, mean radius, mean perimeter, and mean
    area. Several features are also highly correlated with the label, that is, the
    diagnosis as benign or malignant. Let’s identify the 10 features most correlated
    with the diagnosis label:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表的输出显示在图2.13中。一些特征彼此之间高度相关，例如，平均半径、平均周长和平均面积。一些特征也与标签高度相关，即良性或恶性的诊断。让我们确定与诊断标签最相关的10个特征：
- en: '[PRE21]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This produces the following ranking of the top-10 features:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下前10个特征排名：
- en: '[PRE22]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![CH02_F13_Kunapuli](../Images/CH02_F13_Kunapuli.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F13_Kunapuli](../Images/CH02_F13_Kunapuli.png)'
- en: Figure 2.13 Absolute feature correlations between all 30 features and the label
    (diagnosis)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 所有30个特征与标签（诊断）之间的绝对特征相关性
- en: Thus, our correlation analysis tells us that these 10 features are the most
    highly correlated with the diagnosis; that is, these features are likely most
    helpful in breast cancer diagnosis.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的相关性分析告诉我们，这10个特征与诊断的相关性最高；也就是说，这些特征在乳腺癌诊断中可能最有帮助。
- en: Keep in mind that correlation isn’t always a reliable means of identifying effective
    variables, especially if there are highly nonlinear relationships between features
    and labels. However, it’s often a reasonable guideline as long as we’re aware
    of its limitations.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，相关性并不总是识别有效变量的可靠手段，尤其是如果特征和标签之间存在高度非线性关系时。然而，只要我们了解其局限性，它通常是一个合理的指南。
- en: Feature importances using random forests
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林的特征重要性
- en: Random forests can also provide feature importances, as illustrated in the following
    listing.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林还可以提供特征重要性，如下所示。
- en: Listing 2.7 Feature importances in the WDBC data set using random forests
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 使用随机森林在WDBC数据集中的特征重要性
- en: '[PRE23]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Trains a random forest ensemble
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练一个随机森林集成
- en: ❷ Sets an importance threshold wherein all the features above the threshold
    are important
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置一个重要性阈值，其中所有高于阈值的特征都是重要的
- en: ❸ Prints the “important” features, that is, those above the importance threshold
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印“重要”的特征，即那些高于重要性阈值的特征
- en: Listing 2.7 depends on an importance_threshold, which is set to 0.02 here. Typically,
    such a threshold is set by inspection such that we get a target feature set, or
    using a separate validation set to identify important features so that overall
    performance doesn’t degrade.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7依赖于一个重要性阈值，这里设置为0.02。通常，这样的阈值是通过检查来设置的，以便我们得到一个目标特征集，或者使用一个单独的验证集来识别重要特征，这样整体性能就不会下降。
- en: 'For the WDBC data set, the random forest identifies the following features
    as being important. Observe that a considerable overlap exists between important
    features identified by correlation analysis and random forests, though their relative
    rankings are different:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于WDBC数据集，随机森林识别以下特征为重要。观察发现，通过相关性分析和随机森林识别的重要特征之间存在相当大的重叠，尽管它们的相对排名不同：
- en: '[PRE24]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Finally, the feature importances identified by the random forest ensemble are
    visualized in figure 2.14.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，随机森林集成识别的特征重要性在图2.14中可视化。
- en: '![CH02_F14_Kunapuli](../Images/CH02_F14_Kunapuli.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F14_Kunapuli](../Images/CH02_F14_Kunapuli.png)'
- en: Figure 2.14 The random forest ensemble can score features by their importance.
    This allows us to perform feature selection by only using features with the highest
    scores.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 随机森林集成可以通过特征的重要性来评分。这允许我们仅使用得分最高的特征进行特征选择。
- en: CAUTION Note that feature importances will often change between runs owing to
    randomization during tree construction. Note also that if two features are highly
    correlated, random forests will often distribute the feature importance between
    them, leading to their overall weights appearing smaller than they actually are.
    There are other, more robust ways to compute feature importances for the purposes
    of ensemble interpretability, which we’ll explore in chapter 9.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于在树构建过程中的随机化，特征重要性通常会在运行之间发生变化。注意，如果两个特征高度相关，随机森林通常会在这两个特征之间分配特征重要性，导致它们的总体权重看起来比实际要小。为了集成可解释性的目的，还有其他更稳健的方法来计算特征重要性，我们将在第9章中探讨。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Parallel homogeneous ensembles promote ensemble diversity through randomization:
    random sampling of training examples and of features, or even introducing randomization
    in the base-learning algorithm.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平行同质集成通过随机化促进集成多样性：随机采样训练示例和特征，甚至可以在基学习算法中引入随机化。
- en: Bagging is a simple ensemble method that relies on (1) bootstrap sampling (or
    sampling with replacement) to generate diverse replicates of the data set and
    training diverse models, and (2) model aggregation to produce an ensemble prediction
    from a set of individual base learner predictions.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging是一种简单的集成方法，它依赖于（1）自助采样（或带替换的采样）来生成数据集的多样复制品并训练不同的模型，以及（2）模型聚合来从一组单个基学习器预测中产生集成预测。
- en: Bagging and its variants work best with any unstable estimators (unpruned decision
    trees, support vector machines [SVMs], deep neural networks, etc.), which are
    models of higher complexity and/or nonlinearity.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging及其变体与任何不稳定的估计器（未剪枝决策树、支持向量机[SVMs]、深度神经网络等）工作得最好，这些是更高复杂性和/或非线性的模型。
- en: Random forest refers to a variant of bagging specifically designed to use randomized
    decision trees as base learners. Increasing randomness increases ensemble diversity
    considerably, allowing the ensemble to decrease the variability and smooth out
    predictions.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林是指一种特别设计用来使用随机决策树作为基学习器的bagging变体。增加随机性可以显著提高集成多样性，从而使集成减少变异性并平滑预测。
- en: Pasting, a variant of bagging, samples training examples without replacement
    and can be effective on data sets with a very large number of training examples.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粘贴（Pasting），作为袋装法的变体，在不放回的情况下对训练示例进行采样，对于具有大量训练示例的数据集可能非常有效。
- en: Other variants of bagging, such as random subspaces (sampling features) and
    random patches (sampling both features and training examples), can be effective
    on data sets with high dimensionality.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袋装法的其他变体，例如随机子空间（采样特征）和随机补丁（同时采样特征和训练示例），对于高维数据集可能非常有效。
- en: Extra Trees is another bagging-like ensemble method that is specifically designed
    to use extremely randomized trees as base learners. However, Extra Trees doesn’t
    use bootstrap sampling as additional randomization helps in generating ensemble
    diversity.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外树（Extra Trees）是另一种类似于袋装法的集成方法，它专门设计用来使用极端随机树作为基学习器。然而，额外树不使用自助采样，因为额外的随机化有助于生成集成多样性。
- en: Random forests provide feature importances to rank the most important features
    from a predictive standpoint.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林提供了特征重要性，从预测的角度对最重要的特征进行排序。
