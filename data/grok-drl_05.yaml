- en: 5 Evaluating agents’ behaviors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 评估智能体的行为
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will learn about estimating policies when learning from feedback that is
    simultaneously sequential and evaluative.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习在从同时具有顺序性和评价性的反馈中学习时如何估计策略。
- en: You will develop algorithms for evaluating policies in reinforcement learning
    environments when the transition and reward functions are unknown.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当状态转移和奖励函数未知时，你将开发算法来评估强化学习环境中的策略。
- en: You will write code for estimating the value of policies in environments in
    which the full reinforcement learning problem is on display.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将为在显示完整强化学习问题的环境中估计策略的价值编写代码。
- en: I conceive that the great part of the miseries of mankind are brought upon them
    by false estimates they have made of the value of things.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为人类大部分的痛苦都是由于他们对事物价值的错误估计而带来的。
- en: — Benjamin Franklin Founding Father of the United States an author, politician,
    inventor, and a civic activist
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: —— 本杰明·富兰克林 美国开国元勋，作家，政治家，发明家和公民活动家
- en: 'You know how challenging it is to balance immediate and long-term goals. You
    probably experience this multiple times a day: should you watch movies tonight
    or keep reading this book? One has an immediate satisfaction to it; you watch
    the movie, and you go from poverty to riches, from loneliness to love, from overweight
    to fit, and so on, in about two hours and while eating popcorn. Reading this book,
    on the other hand, won’t really give you much tonight, but maybe, and only maybe,
    will provide much higher satisfaction in the long term.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道平衡短期和长期目标是多么具有挑战性。你可能每天都会多次体验到这一点：今晚你应该看电影还是继续阅读这本书？一部电影能给你带来即时的满足感；你看完电影，就从贫穷到富有，从孤独到爱情，从超重到健康，等等，大约两个小时，而且还能吃爆米花。另一方面，阅读这本书今晚可能不会给你带来太多，但也许，只是也许，会在长期提供更高的满足感。
- en: 'And that’s a perfect lead-in to precisely the other issue we discussed. How
    much more satisfaction in the long term, exactly, you may ask. Can we tell? Is
    there a way to find out? Well, that’s the beauty of life: I don’t know, you don’t
    know, and we won’t know unless we try it out, unless we *utilization*.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们之前讨论的另一个问题的完美过渡。在长期中，你将获得多少额外的满足感，确切地说，你可能会问。我们能知道吗？有没有一种方法可以找出答案？嗯，这就是生活的美妙之处：我不知道，你也不知道，除非我们尝试，除非我们**利用**。
- en: However, in the previous chapter, we studied this challenge in isolation from
    the sequential aspect of RL. Basically, you assume your actions have no long-term
    effect, and your only concern is to find the best thing to do for the current
    situation. For instance, your concern may be selecting a good movie, or a good
    book, but without thinking how the movie or the book will impact the rest of your
    life. Here, your actions don’t have a “compounding effect.”
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在前一章中，我们独立于强化学习的顺序方面研究了这一挑战。基本上，你假设你的行为没有长期影响，你唯一关心的是找到当前情况下最佳的选择。例如，你的关注点可能是选择一部好电影，或者一本好书，但不会考虑电影或书籍将如何影响你余下的生活。在这里，你的行为没有“累积效应。”
- en: Now, in this chapter, we look at agents that learn from feedback that’s simultaneously
    sequential and evaluative; agents need to simultaneously balance *keep on reading*?
    Hint!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在本章中，我们来看看那些从同时具有顺序性和评价性的反馈中学习的智能体；智能体需要同时平衡**继续阅读**？提示！
- en: You’re smart... . In this chapter, we’ll study agents that can learn to *equally
    essential* aspects of RL. In machine learning, the saying goes, “The model is
    only as good as the data.” In RL, I say, “The policy is only as good as the estimates,”
    or, detailed, “The improvement of a policy is only as good as the accuracy and
    precision of its estimates.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你很聪明... . 在本章中，我们将研究能够学习到强化学习**同等重要**方面的智能体。在机器学习中，有句俗语说，“模型的好坏取决于数据。”在强化学习中，我说，“策略的好坏取决于估计的准确性，”或者更详细地说，“策略的改进程度取决于其估计的准确性和精确度。”
- en: Once again, in DRL, agents learn from feedback that’s simultaneously sequential
    (as opposed to one-shot), evaluative (as opposed to supervised) and sampled (as
    opposed to exhaustive). In this chapter, we’re looking at agents that learn from
    feedback that’s simultaneously sequential and evaluative. We’re temporarily shelving
    the “sampled” part, but we’ll open those gates in chapter 8, and there will be
    fun galore. I promise.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，在深度强化学习（DRL）中，智能体从同时具有顺序性（与一次性相反）、评价性（与监督学习相反）和抽样（与穷举相反）的反馈中学习。在本章中，我们关注的是那些从同时具有顺序性和评价性的反馈中学习的智能体。我们暂时将“抽样”部分放在一边，但在第8章中，我们将打开这些大门，那里将充满乐趣。我保证。
- en: Learning to estimate the value of policies
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习估计策略的价值
- en: As I mentioned before, this chapter is about learning to estimate the value
    of existing policies. When I was first introduced to this prediction problem,
    I didn’t get the motivation. To me, if you want to estimate the value of a policy,
    the straightforward way of doing it is running the policy repeatedly and averaging
    what you get.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前所述，本章是关于学习估计现有策略的价值。当我最初接触到这个预测问题时，我没有理解其动机。对我来说，如果你想估计策略的价值，最直接的方法是重复运行策略并平均你所得到的结果。
- en: And, that’s definitely a valid approach, and perhaps the most natural. What
    I didn’t realize back then, however, is that there are many other approaches to
    estimating value functions. Each of these approaches has advantages and disadvantages.
    Many of the methods can be seen as an exact opposite alternative, but there’s
    also a middle ground that creates a full spectrum of algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，这确实是一个有效的途径，也许是最自然的。然而，我当时并没有意识到，还有许多其他方法可以用来估计值函数。这些方法各有优缺点。许多方法可以被视为完全相反的替代方案，但同时也存在一个中间地带，它创造了一个完整的算法谱系。
- en: In this chapter, we’ll explore a variety of these approaches and dig into their
    pros and cons, showing you how they relate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨这些方法的多样性，深入分析它们的优缺点，并展示它们是如何相互关联的。
- en: '| ŘŁ | With An RL AccentReward vs. return vs. value function |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的奖励、回报与值函数 |'
- en: '|  | **Reward**: Refers to the one-step reward signal the agent gets: the agent
    observes a state, selects an action, and receives a reward signal. The reward
    signal is the core of RL, but it is not what the agent is trying to maximize!
    Again, the agent isn’t trying to maximize the reward! Realize that while your
    agent maximizes the one-step reward, in the long-term, it’s getting less than
    it could.**Return**: Refers to the total discounted rewards. Returns are calculated
    from any state and usually go until the end of the episode. That is, when a terminal
    state is reached, the calculation stops. Returns are often referred to as total
    reward, cumulative reward, sum of rewards, and are commonly discounted: total
    discounted reward, cumulative discounted reward, sum of discounted reward. But,
    it’s basically the same: a return tells you how much reward your agent obtained
    in an episode. As you can see, returns are better indicators of performance because
    they contain a long-term sequence, a single-episode history of rewards. But the
    return isn’t what an agent tries to maximize, either! An agent who attempts to
    obtain the highest possible return may find a policy that takes it through a noisy
    path; sometimes, this path will provide a high return, but perhaps most of the
    time a low one.**Value function**: Refers to the expectation of returns. Sure,
    we want high returns, but high in expectation (on average). If the agent is in
    a noisy environment, or if the agent is using a stochastic policy, it’s all fine.
    The agent is trying to maximize the expected total discounted reward, after all:
    value functions. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | **奖励**：指的是智能体获得的一步奖励信号：智能体观察到一个状态，选择一个动作，并接收到奖励信号。奖励信号是强化学习的核心，但并非智能体试图最大化的目标！再次强调，智能体并不是试图最大化奖励！意识到，虽然你的智能体在最大化一步奖励，但从长远来看，它获得的奖励可能少于它本可以获得的。**回报**：指的是总折扣奖励。回报是从任何状态计算得出的，通常持续到剧集结束。也就是说，当达到终端状态时，计算停止。回报通常被称为总奖励、累积奖励、奖励总和，并且通常是折扣的：总折扣奖励、累积折扣奖励、折扣奖励总和。但基本上是相同的：回报告诉你智能体在一个剧集中所获得的奖励有多少。正如你所看到的，回报是更好的性能指标，因为它们包含一个长期序列，一个单剧集的奖励历史。但回报也不是智能体试图最大化的目标！试图获得最高回报的智能体可能会找到一条噪声路径；有时，这条路径会提供高回报，但也许大多数时候回报较低。**值函数**：指的是回报的期望。当然，我们希望获得高回报，但高期望（平均）。如果智能体处于噪声环境中，或者如果智能体使用随机策略，那都是可以的。毕竟，智能体试图最大化的是期望的总折扣奖励：值函数。|'
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyRewards, returns, value
    functions, and life |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Miguel.png) | 米格尔的类比奖励、回报、值函数与生活 |'
- en: '|  | How do you approach life? Do you select actions that are the best for
    you, or are you one of those kind folks who prioritize others before themselves?There’s
    no shame either way! Being selfish, to me, is an excellent reward signal. It takes
    you places. It drives you around. Early on in life, going after the immediate
    reward can be a pretty solid strategy.Many people judge others for being “too
    selfish,” but to me, that’s the way to get going. Go on and do what you want,
    what you dream of, what gives you satisfaction, go after the rewards! You’ll look
    selfish and greedy. But you shouldn’t care.As you keep going, you’ll realize that
    going after the rewards isn’t the best strategy, even for your benefit. You start
    seeing a bigger picture. If you overeat candy, your tummy hurts; if you spend
    all of your money on online shopping, you can go broke.Eventually, you start looking
    at the returns. You start understanding that there’s more to your selfish and
    greedy motives. You drop the greedy side of you because it harms you in the long
    run, and now you can see that. But you stay selfish, you still only think in terms
    of rewards, just now “total” rewards, returns. No shame about that, either!At
    one point, you’ll realize that the world moves without you, that the world has
    many more moving parts than you initially thought, that the world has underlying
    dynamics that are difficult to comprehend. You now know that “what goes around
    comes around,” one way or another, one day or another, but it does.You step back
    once again; now instead of the going after rewards or returns, you go after value
    functions. You wise up! You learn that the more you help others learn, the more
    you learn. Not sure why, but it works. The more you love your significant other,
    the more they love you, crazy! The more you don’t spend (save), the more you can
    spend. How strange! Notice, you’re still selfish!But you become aware of the complex
    underlying dynamics of the world and can understand that the best for yourself
    is to better others—a perfect win-win situation.I’d like the differences between
    rewards, returns, and value functions to be ingrained in you, so hopefully this
    should get you thinking for a bit.Follow the rewards!Then, the returns!Then, the
    value functions. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | 你如何面对生活？你是选择对自己最有利的行动，还是你是那些优先考虑他人而先考虑自己的人？两种方式都没有什么可耻的！对我来说，自私是一种极好的奖励信号。它带你走向成功。它推动你前进。在生活的早期，追求即时奖励可以是一个相当稳固的策略。许多人因为别人“太自私”而评判他们，但对我来说，这是开始行动的方式。去做你想做的事，去追求你的梦想，去做让你满足的事，去追求奖励！你看起来自私且贪婪。但你不应该在意。随着你继续前进，你会意识到追求奖励并不是最好的策略，即使是对你自己的好处。你开始看到更大的图景。如果你吃太多糖果，你的肚子会疼；如果你把所有的钱都花在网购上，你可能会破产。最终，你开始关注回报。你开始理解你的自私和贪婪动机背后还有更多。你放弃了贪婪的一面，因为它会从长远上伤害你，现在你意识到了这一点。但你还保持着自私，你仍然只从奖励的角度思考，但现在是从“总”奖励，即回报的角度。对此也不必感到羞耻！在某个时刻，你会意识到世界在没有你的情况下运转，世界比你最初想象的要复杂得多，世界有难以理解的潜在动态。你现在知道，“善有善报，恶有恶报”，不管以何种方式，不管何时，它都会发生。你再次退后一步；现在，你不再追求奖励或回报，而是追求价值函数。你变得聪明了！你意识到，你帮助他人学习得越多，你学到的就越多。不知道为什么，但这是真的。你越爱你的人生伴侣，他们就越爱你，不可思议！你越不花钱（储蓄），你就能花得越多。多么奇怪！注意，你仍然自私！但你现在意识到了世界的复杂潜在动态，并理解到，对自己最好的方式是帮助他人——这是一个完美的双赢局面。我希望奖励、回报和价值函数之间的区别能深深地印在你的心中，所以希望这能让你思考一会儿。跟随奖励！然后，是回报！然后，是价值函数。
    |'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe random walk environment
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 一个具体的例子随机游走环境 |'
- en: '|  | The primary environment we’ll use through this chapter is called the random
    walk (RW). This is a walk, single-row grid-world environment, with five non-terminal
    states. But it’s peculiar, so I want to explain it in two ways.On the one hand,
    you can think of the RW as an environment in which the probability of going *right*
    with 50% regardless of the action it takes. It’s a random walk, after all. Crazy!![](../Images/05_01_Sidebar03.png)Random
    walk environment MDPBut to me, that was an unsatisfactory explanation of the RW,
    maybe because I like the idea of agents controlling something. What’s the point
    of studying RL (a framework for learning optimal control) in an environment in
    which there’s no possible control!?Therefore, you can think of the RW as an environment
    with a deterministic transition function (meaning that if the agent chooses left,
    the agent moves left, and it moves right if it picks right—as expected). But pretend
    the agent wants to evaluate a stochastic policy that selects actions uniformly
    at random. That’s half the time it chooses left, and the other half, right.Either
    way, the concept is the same: we have a five non-terminal state walk in which
    the agent moves left and right uniformly at random. The goal is to estimate the
    expected total discounted reward the agent can obtain given these circumstances.
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | 在本章中，我们将主要使用一个称为随机游走（RW）的环境。这是一个单行网格世界环境，具有五个非终端状态。但它很特别，所以我想要从两个角度来解释它。一方面，你可以将RW视为一个环境，其中无论采取什么行动，向右走的概率都是50%。毕竟，它是一个随机游走，疯狂！！![随机游走环境
    MDP](../Images/05_01_Sidebar03.png)随机游走环境 MDP但对我来说，这并不是对RW令人满意的解释，也许是因为我喜欢代理控制某物的想法。在没有任何可能控制的环境中进行RL（一个用于学习最优控制的框架）研究有什么意义呢！因此，你可以将RW视为一个具有确定性转移函数的环境（意味着如果代理选择左转，代理就会左转，如果选择右转，就会右转——正如预期的那样）。但假设代理想要评估一个随机策略，该策略以均匀随机的方式选择动作。一半的时间选择左转，另一半时间选择右转。无论如何，概念是相同的：我们有一个五个非终端状态的游走，其中代理以均匀随机的方式左右移动。目标是估计在这些情况下代理可以获得的总折现奖励的期望值。|'
- en: 'First-visit Monte Carlo: Improving estimates after each episode'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 首次访问蒙特卡洛：每轮后改进估计
- en: 'Alright! The goal is to estimate the value of a policy, that is, to learn how
    much total reward to expect from a policy. More properly, the goal is to estimate
    the state-value function *v*π*(s)* of a policy *π*. The most straightforward approach
    that comes to mind is one I already mentioned: it’s to run several episodes with
    this policy collecting hundreds of trajectories, and then calculate averages for
    every state, just as we did in the bandit environments. This method of estimating
    value functions is called *monte Carlo prediction* (MC).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧！目标是估计策略的价值，即学习从策略中期望获得多少总奖励。更确切地说，目标是估计策略π的状态值函数vπ(s)。最直接的方法就是我之前提到的方法：就是运行几个带有此策略的回合，收集数百个轨迹，然后计算每个状态的平均值，就像我们在老虎机环境中做的那样。这种估计值函数的方法称为*蒙特卡洛预测*（MC）。
- en: MC is easy to implement. The agent will first interact with the environment
    using policy *π* until the agent hits a terminal state *s*[*T*]. The collection
    of state *s*[*t*], action *A*[*t*], reward *r*[*t*+1], and next state *s*[*t*+1]
    is called an *experience tuple*. A sequence of experiences is called a *trajectory*.
    The first thing you need to do is have your agent generate a trajectory.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MC易于实现。代理将首先使用策略π与环境交互，直到代理达到终端状态s[T]。状态s[t]，动作A[t]，奖励r[t+1]和下一个状态s[t+1]的集合称为一个*经验元组*。一系列经验称为一个*轨迹*。你需要做的第一件事是让你的代理生成一个轨迹。
- en: 'Once you have a trajectory, you calculate the returns *g*[t:T] for every state
    *s*[t] encountered. For instance, for state *s*[*t*], you go from time step *t*
    forward, adding up and discounting the rewards received along the way: *r*[*t*+1]*,
    R*[t+2]*, R*[t+3]*, ... , R*[*T*], until the end of the trajectory at time step
    *t*. Then, you repeat that process for state *s*[*t*+1], adding up the discounted
    reward from time step *t+1* until you again reach *t*; then for *s*[t+2] and so
    on for all states except *s*[*T*], which by definition has a value of 0\. *g*[t:T]
    will end up using the rewards from time step *t+1*, up to the end of the episode
    at time step *t*. We discount those rewards with an exponentially decaying discount
    factor: *γ*⁰*,* *γ*¹*,* *γ*²*, ... ,* *γ*^(T-1). That means multiplying the corresponding
    discount factor *γ* by the reward *r*, then adding up the products along the way.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了轨迹，你将为遇到的每个状态 *s*[t] 计算回报 *g*[t:T]。例如，对于状态 *s*[*t*]，你从时间步 *t* 开始向前，累加并折现沿途收到的奖励：*r*[*t*+1]*,
    R*[t+2]*, R*[t+3]*, ... , R*[*T*]，直到轨迹在时间步 *t* 结束。然后，你重复这个过程为状态 *s*[*t*+1]，累加从时间步
    *t+1* 到达 *t* 的折现奖励；然后为 *s*[t+2] 以及所有其他状态，除了 *s*[*T*]，根据定义其值为 0。*g*[t:T] 将最终使用时间步
    *t+1* 到轨迹结束的奖励，使用指数衰减折现因子：*γ*⁰*, *γ*¹*, *γ*²*, ... , *γ*^(T-1)。这意味着将相应的折现因子 *γ*
    乘以奖励 *r*，然后累加沿途的乘积。
- en: After generating a trajectory and calculating the returns for all states *s*[t],
    you can estimate the state-value function *v*π*(s)* at the end of every episode
    *e* and final time step *t* by merely averaging the returns obtained from each
    state *s*. In other words, we’re estimating an expectation with an average. As
    simple as that.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成轨迹并计算所有状态 *s*[t] 的回报后，你只需通过平均从每个状态 *s* 获得的回报，就可以在每次剧集 *e* 和最终时间步 *t* 结束时估计状态值函数
    *v*π*(s)*。换句话说，我们正在用平均值估计一个期望值。就这么简单。
- en: '![](../Images/05_02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05_02.png)'
- en: Monte Carlo prediction
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测
- en: '| ![](../Images/icons_Math.png) | Show Me The MathMonte Carlo learning |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Math.png) | 展示数学：蒙特卡洛学习 |'
- en: '|  | ![](../Images/05_02__Sidebar04.png) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/05_02__Sidebar04.png) |'
- en: 'Every-visit Monte Carlo: A different way of handling state visits'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每次访问蒙特卡洛：处理状态访问的不同方式
- en: You probably notice that in practice, there are two different ways of implementing
    an averaging-of-returns algorithm. This is because a single trajectory may contain
    multiple visits to the same state. In this case, should we calculate the returns
    following each of those visits independently and then include all of those targets
    in the averages, or should we only use the first visit to each state?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到在实践中，实现平均回报算法有两种不同的方式。这是因为单个轨迹可能包含对同一状态的多次访问。在这种情况下，我们应该独立计算每次访问的回报，然后将所有这些目标包括在平均值中，还是我们只使用每个状态的首次访问？
- en: Both are valid approaches, and they have similar theoretical properties. The
    more “standard” version is *first-visit MC* (FVMC), and its convergence properties
    are easy to justify because each trajectory is an independent and identically
    distributed (IID) sample of *v*π*(s)*, so as we collect infinite samples, the
    estimates will converge to their true values. *every-visit MC* (EVMC) is slightly
    different because returns are no longer independent and identically distributed
    when states are visited multiple times in the same trajectory. But, fortunately
    for us, EVMC has also been proven to converge given infinite samples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都是有效的，并且它们具有相似的理论特性。更“标准”的版本是首次访问蒙特卡洛（FVMC），其收敛特性很容易证明，因为每个轨迹都是 *v*π*(s)*
    的独立同分布（IID）样本，因此当我们收集无限样本时，估计将收敛到它们的真实值。每次访问蒙特卡洛（EVMC）略有不同，因为当状态在相同轨迹中被多次访问时，回报不再是独立同分布的。但幸运的是，EVMC
    也已被证明在无限样本的情况下会收敛。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownFirst- vs. every-visit MC |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Boil.png) | 简化：首次访问与每次访问蒙特卡洛 |'
- en: '|  | MC prediction estimates *v*[*π*](*s*) as the average of returns of π.
    FVMC uses only one return per state per episode: the return following a first
    visit. EVMC averages the returns following all visits to a state, even if in the
    same episode. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | MC 预测估计 *v*[*π*](*s*) 为 π 的回报的平均值。FVMC 每个状态在每个剧集只使用一个回报：首次访问后的回报。EVMC
    平均所有访问到状态的回报，即使在同一剧集内也是如此。'
- en: '| 0001 | A Bit Of HistoryFirst-visit Monte Carlo prediction |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史：首次访问蒙特卡洛预测 |'
- en: '|  | You’ve probably heard the term “Monte Carlo simulations” or “runs” before.
    Monte Carlo methods, in general, have been around since the 1940s and are a broad
    class of algorithms that use random sampling for estimation. They are ancient
    and widespread. However, it was in 1996 that first- and every-visit MC methods
    were identified in the paper “Reinforcement Learning with Replacing Eligibility
    Traces,” by Satinder Singh and Richard Sutton.Satinder Singh and Richard Sutton
    each obtained their PhD in Computer Science from the University of Massachusetts
    Amherst, were advised by Professor Andy Barto, became prominent figures in RL
    due to their many foundational contributions, and are now Distinguished Research
    Scientists at Google DeepMind. Rich got his PhD in 1984 and is a professor at
    the University of Alberta, whereas Satinder got his PhD in 1994 and is a professor
    at the University of Michigan. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | 你可能之前听说过“蒙特卡洛模拟”或“运行”这个术语。一般来说，蒙特卡洛方法自20世纪40年代以来一直存在，是一类广泛使用的算法，它使用随机抽样进行估计。它们历史悠久且应用广泛。然而，直到1996年，Satinder
    Singh和Richard Sutton在论文“具有替换资格痕迹的强化学习”中首次确定了首次和每次访问的MC方法。Satinder Singh和Richard
    Sutton都从马萨诸塞大学阿默斯特分校获得了计算机科学博士学位，由Andy Barto教授指导，由于他们的许多基础性贡献，成为了强化学习领域的杰出人物，现在他们是Google
    DeepMind的杰出研究科学家。Rich在1984年获得了博士学位，是阿尔伯塔大学的教授，而Satinder在1994年获得了博士学位，是密歇根大学的教授。
    |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonExponentially decaying schedule
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python指数衰减计划 |'
- en: '|  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① This function allows you to calculate all the values for alpha for the full
    training process.② First, calculate the number of steps to decay the values using
    the decay_ratio variable.③ Then, calculate the actual values as an inverse log
    curve. Notice we then normalize between 0 and 1, and finally transform the points
    to lay between init_value and min_value. |
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ① 此函数允许您计算整个训练过程中alpha的所有值。② 首先，使用decay_ratio变量计算衰减值的步数。③ 然后，将实际值计算为倒数对数曲线。注意我们随后将其归一化到0和1之间，最后将点转换到init_value和min_value之间。
    |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonGenerate full trajectories
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python生成完整轨迹 |'
- en: '|  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① This is a straightforward function. It’s running a policy and extracting the
    collection of experience tuples (the trajectories) for off-line processing.② This
    allows you to pass a maximum number of steps so that you can truncate long trajectories
    if desired. |
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是一个简单的函数。它正在运行策略并提取用于离线处理的经验元组（轨迹）。② 这允许你传递最大步数，以便你可以根据需要截断长轨迹。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonMonte Carlo prediction 1/2
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python蒙特卡洛预测 1/2 |'
- en: '|  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '① The mc_prediction function works for both first- and every-visit MC. The
    hyperparameters you see here are standard. Remember, the discount factor, gamma,
    depends on the environment.② For the learning rate, alpha, I’m using a decaying
    value from init_alpha of 0.5 down to min_alpha of 0.01, decaying within the first
    30% (alpha_decay_ratio of 0.3) of the 500 total max_episodes. We already discussed
    max_steps on the previous function, so I’m passing the argument around. And first_visit
    toggles between FVMC and EVMC.③ This is cool. I’m calculating all possible discounts
    at once. This logspace function for a gamma of 0.99 and a max_step of 100 returns
    a 100 number vector: [1, 0.99, 0.9801, ..., 0.3697].④ Here I’m calculating all
    of the alphas!⑤ Here we’re initializing variables we’ll use inside the main loop:
    the current estimate of the state-value function V, and a per-episode copy of
    V for offline analysis.⑥ We loop for every episode. Note that we’re using ‘tqdm’
    here. This package prints a progress bar, and it’s useful for impatient people
    like me. You may not need it (unless you’re also impatient).⑦ Generate a full
    trajectory.⑧ Initialize a visits check bool vector.⑨ This last line is repeated
    on the next page for your reading convenience. |'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ① mc_prediction函数适用于首次访问和每次访问的MC。您在这里看到的超参数是标准的。记住，折现因子gamma取决于环境。② 对于学习率alpha，我使用从init_alpha的0.5衰减到min_alpha的0.01的衰减值，在前500个总最大剧集的30%内衰减（alpha_decay_ratio为0.3）。我们已经在之前的功能中讨论了max_steps，所以我正在传递这个参数。并且first_visit在FVMC和EVMC之间切换。③
    这很酷。我一次计算所有可能折扣。这个logspace函数对于gamma为0.99和最大步数为100返回一个100个数字的向量：[1, 0.99, 0.9801,
    ..., 0.3697]。④ 这里我计算所有的alpha！⑤ 这里我们初始化在主循环内部使用的变量：当前状态价值函数的估计V，以及每个剧集的V副本，用于离线分析。⑥
    我们为每个剧集循环。注意，我们在这里使用‘tqdm’。这个包会打印进度条，对我来说很有用。你可能不需要它（除非你也像我一样不耐烦）。⑦ 生成完整轨迹。⑧ 初始化访问检查布尔向量。⑨
    这最后一行在下一页重复出现是为了方便您阅读。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonMonte Carlo prediction 2/2
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python蒙特卡洛预测 2/2 |'
- en: '|  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ⑩ This first line is repeated on the previous page for your reading convenience.⑪
    We now loop through all experiences in the trajectory.⑫ Check if the state has
    already been visited on this trajectory, and doing FVMC.⑬ And if so, we process
    the next state.⑭ If this is the first visit or we are doing EVMC, we process the
    current state.⑮ First, calculate the number of steps from *t* to *T*.⑯ Then, calculate
    the return.⑰ Finally, estimate the value function.⑱ Keep track of the episode’s
    V.⑲ And return V, and the tracking when done. |
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 这第一行在上一页重复出现是为了方便您阅读。⑪ 我们现在遍历轨迹中的所有经验。⑫ 检查状态是否已经在该轨迹上访问过，并执行FVMC。⑬ 如果是，我们处理下一个状态。⑭
    如果这是第一次访问或者我们在执行EVMC，我们处理当前状态。⑮ 首先，计算从 *t* 到 *T* 的步数。⑯ 然后，计算回报。⑰ 最后，估计价值函数。⑱ 跟踪这一幕的V。⑲
    完成后返回V和跟踪信息。 |
- en: '| ŘŁ | With An RL AccentIncremental vs. sequential vs. trial-and-error |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的增量与顺序与试错 |'
- en: '|  | **Incremental methods**: Refers to the iterative improvement of the estimates.
    Dynamic programming is an incremental method: these algorithms iteratively compute
    the answers. They don’t “interact” with an environment, but they reach the answers
    through successive iterations, incrementally. Bandits are also incremental: they
    reach good approximations through successive episodes or trials. Reinforcement
    learning is incremental, as well. Depending on the specific algorithm, estimates
    are improved on an either per-episode or per-time-step basis, incrementally.**Sequential
    methods**: Refers to learning in an environment with more than one non-terminal
    (and reachable) state. Dynamic programming is a sequential method. Bandits are
    not sequential, they are one-state one-step MDPs. There’s no long-term consequence
    for the agent’s actions. Reinforcement learning is certainly sequential.**Trial-and-error
    methods**: Refers to learning from interaction with the environment. Dynamic programming
    is not trial-and-error learning. Bandits are trial-and-error learning. Reinforcement
    learning is trial-and-error learning, too. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | **增量方法**：指的是估计的迭代改进。动态规划是一种增量方法：这些算法迭代地计算答案。它们不与环境“交互”，而是通过连续迭代、增量地达到答案。多臂老虎机也是增量方法：它们通过连续的剧集或试验达到良好的近似。强化学习也是增量方法。根据具体算法，估计是在每个剧集或每个时间步的基础上增量改进的。**顺序方法**：指的是在具有多个非终止（且可到达）状态的环境中学习。动态规划是一种顺序方法。多臂老虎机不是顺序方法，它们是一状态一步马尔可夫决策过程。代理的行为没有长期后果。强化学习肯定是顺序的。**试错方法**：指的是通过与环境的交互进行学习。动态规划不是试错学习。多臂老虎机是试错学习。强化学习也是试错学习。|'
- en: 'Temporal-difference learning: Improving estimates after each step'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间差分学习：每一步改进估计
- en: One of the main drawbacks of MC is the fact that the agent has to wait until
    the end of an episode when it can obtain the actual return *g*[*t:T*] before it
    can update the state-value function estimate *v*[*T*]*(S*[*t*]). On the one hand,
    MC has pretty solid convergence properties because it updates the value function
    estimate *v*[*T*]*(S*[*t*]) toward the actual return *g*[*t*]:[*T*], which is
    an unbiased estimate of the true state-value function *v*[*π*]*(s)*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法的一个主要缺点是代理必须等待到剧集结束时才能获得实际回报 *g*[*t:T*]，然后才能更新状态值函数估计 *v*[*T*](S*[*t*])。一方面，蒙特卡洛方法具有相当稳健的收敛性质，因为它将值函数估计
    *v*[*T*](S*[*t*]) 更新到实际回报 *g*[*t*]:[*T*]，这是一个对真实状态值函数 *v*[*π*](s) 的无偏估计。
- en: 'However, while the actual returns are pretty accurate estimates, they are also
    not very precise. Actual returns are also high-variance estimates of the true
    state-value function *v*[*π*](*s*). It’s easy to see why: actual returns accumulate
    many random events in the same trajectory; all actions, all next states, all rewards
    are random events. The actual return *G*[*t:T*] collects and compounds all of
    that randomness for multiple time steps, from *t* to *T*. Again, the actual return
    *G*[*t:T*] is unbiased, but high variance.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管实际回报的估计相当准确，但它们也不是非常精确。实际回报也是对真实状态值函数 *v*[*π*](*s*) 的高方差估计。这很容易理解：实际回报在相同的轨迹中累积了许多随机事件；所有动作、所有下一个状态、所有奖励都是随机事件。实际回报
    *G*[*t:T*] 收集并复合了所有这些随机性，跨越多个时间步，从 *t* 到 *T*。再次强调，实际回报 *G*[*t:T*] 是无偏的，但方差很高。
- en: Also, due to the high variance of the actual returns *g*[*t*]:[*T*], MC can
    be sample inefficient. All of that randomness becomes noise that can only be alleviated
    with data, lots of data, lots of trajectories, and actual return *estimate* a
    return, even if just partially estimated? Think!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于实际回报 *g*[*t*]:[*T*] 的高方差，蒙特卡洛方法可能样本效率低下。所有这些随机性都变成了噪声，只能通过数据、大量数据、大量轨迹和实际回报的估计来缓解，即使只是部分估计？想想看！
- en: 'Yes! You can use a single-step reward *r*[*t*+1], and once you observe the
    next state *s*[*t*+1], you can use the state-value function estimates *v(S*[*t*+1])
    as an estimate of the return at the next step *g*[t+1:][*T*]. This is the relationship
    in the equations that *temporal-difference* (TD) methods exploit. These methods,
    unlike MC, can learn from incomplete episodes by using the one-step actual return,
    which is the immediate reward *r*[*t*+1], but then an estimate of the return from
    the next state onwards, which is the state-value function estimate of the next
    state *v(S*[*t*+1]): that is, *r*[*t*+1] *+* *γ**v(S*[*t*+1]), which is called
    the **TD target**.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！你可以使用单步奖励*r*[*t*+1]，一旦你观察到下一个状态*s*[*t*+1]，你可以使用状态值函数估计*v(S*[*t*+1])作为下一个步骤*g*[t+1:][*T*]的回报估计。这是*时序差分*(TD)方法所利用的关系。这些方法与MC不同，可以通过使用单步实际回报（即立即奖励*r*[*t*+1]），然后从下一个状态开始估计回报，即下一个状态的状态值函数估计*v(S*[*t*+1])：即*r*[*t*+1]
    *+* *γ**v(S*[*t*+1])，这被称为**TD目标**。 |
- en: '| ![](../Images/icons_Boil.png) | Boil It DownTemporal-difference learning
    and bootstrapping |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化时序差分学习和自举 |'
- en: '|  | TD methods estimate *v*[*π*](*s*) using an estimate of *v*[*π*](*s*).
    It bootstraps and makes a guess from a guess; it uses an estimated return instead
    of the actual return. More concretely, it uses *r*[*t*+1] + γ*v*[*t*]( *s*[*t*+1])
    to calculate and estimate *v*[*t*+1]( *s*[*t*]).Because it also uses one step
    of the actual return *r*[*t*+1], things work out fine. That reward signal *r*[*t*+1]
    progressively “injects reality” into the estimates. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | TD方法使用对*v*[*π*](*s*)的估计来估计*v*[*π*](*s*)。它通过从猜测中猜测来自举；它使用估计的回报而不是实际回报。更具体地说，它使用*r*[*t*+1]
    + γ*v*[*t*]( *s*[*t*+1])来计算和估计*v*[*t*+1]( *s*[*t*])。因为它也使用了一步的实际回报*r*[*t*+1]，所以一切正常。这个奖励信号*r*[*t*+1]逐渐将“现实”注入到估计中。
    |'
- en: '| ![](../Images/icons_Math.png) | Show Me The MathTemporal-difference learning
    equations |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学时序差分学习方程 |'
- en: '|  | ![](../Images/05_02_Sidebar13.png) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/05_02_Sidebar13.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe temporal-difference learning
    algorithm |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python时序差分学习算法 |'
- en: '|  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① td is a prediction method. It takes in a policy pi, an environment env to
    interact with, and the discount factor gamma.② The learning method has a configurable
    hyperparameter alpha, which is the learning rate.③ One of the many ways of handling
    the learning rate is to exponentially decay it. The initial value is init_alpha,
    min_alpha, the minimum value, and alpha_decay_ratio is the fraction of episodes
    that it will take to decay alpha from init_alpha to min_alpha.④ We initialize
    the variables needed.⑤ And we calculate the learning rate schedule for all episodes
    ...⑥ ... and loop for n_episodes.⑦ We get the initial state and then enter the
    interaction loop.⑧ First thing is to sample the policy pi for the action to take
    in state.⑨ We then use the action to interact with the environment... We roll
    out the policy one step.⑩ We can immediately calculate a target to update the
    state-value function estimates ...⑪ ... and with the target, an error.⑫ Finally
    update *V*(*s*)⑬ Don’t forget to update the state variable for the next iteration.
    Bugs like this can be hard to find!⑭ And return the V function and the tracking
    variable. |
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ① td是一种预测方法。它接受一个策略pi，一个与环境交互的环境env，以及折扣因子gamma。②学习方法有一个可配置的超参数alpha，它是学习率。③处理学习率的一种方法是对其进行指数衰减。初始值是init_alpha，min_alpha，最小值，alpha_decay_ratio是alpha从init_alpha衰减到min_alpha所需剧集的分数。④我们初始化所需的变量。⑤然后我们计算所有剧集的学习率计划...⑥...然后循环n_episodes。⑦我们获取初始状态然后进入交互循环。⑧首先是要采样策略pi以在状态中采取行动。⑨然后我们使用行动与环境交互...我们执行策略的一步。⑩我们可以立即计算一个目标来更新状态值函数的估计...⑪...然后，有了目标，一个误差。⑫最后更新*V*(*s*)⑬不要忘记更新下一次迭代的变量。这种错误可能很难找到！⑭然后返回V函数和跟踪变量。
    |
- en: '| ŘŁ | With An RL AccentTrue vs. actual vs. estimated |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的真实与实际与估计 |'
- en: '|  | **True value function**: Refers to the exact and perfectly accurate value
    function, as if given by an oracle. The true value function is the value function
    agents estimate through samples. If we had the true value function, we could easily
    estimate returns.**Actual return**: Refers to the experienced return, as opposed
    to an estimated return. Agents can only experience actual returns, but they can
    use estimated value functions to estimate returns. Actual return refers to the
    full experienced return.**Estimated value function or estimated return**: Refers
    to the rough calculation of the true value function or actual return. “Estimated”
    means an approximation, a guess. True value functions let you estimate returns,
    and estimated value functions add bias to those estimates. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | **真实值函数**：指的是精确且完美准确的价值函数，就像由先知给出的一样。真实值函数是代理通过样本估计的价值函数。如果我们有真实值函数，我们就可以很容易地估计回报。**实际回报**：指的是经验回报，与估计回报相对。代理只能体验实际回报，但它们可以使用估计的价值函数来估计回报。实际回报指的是完整的经验回报。**估计价值函数或估计回报**：指的是对真实值函数或实际回报的粗略计算。“估计”意味着一个近似值，一个猜测。真实值函数让你估计回报，而估计价值函数会给这些估计增加偏差。|'
- en: Now, to be clear, the *TD* target is a biased estimate of the true state-value
    function *v*π*(s)*, because we use an estimate of the state-value function to
    calculate an estimate of the state-value function. Yeah, weird, I know. This way
    of updating an estimate with an estimate is referred to as bootstrapping, and
    it’s much like what the dynamic programming methods we learned about in chapter
    3 do. The thing is, though, DP methods bootstrap on the one-step expectation while
    *TD* methods bootstrap on a sample of the one-step expectation. That word sample
    makes a whole lot of a difference.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了清楚起见，*TD* 目标是对真实状态值函数 *v*π*(s)* 的有偏估计，因为我们使用状态值函数的估计来计算状态值函数的估计。是的，我知道这听起来很奇怪。这种用估计来更新估计的方法被称为自助法，这与我们在第三章中学习的动态规划方法非常相似。但是，DP方法是在一步期望上自助，而*TD*方法是在一步期望的样本上自助。这个词“样本”有很大的区别。
- en: On the good side, while the new estimated return, the *TD* target, is a *t:T*
    we use in Monte Carlo updates. This is because the *TD* target depends only on
    a single action, a single transition, and a single reward, so there’s much less
    randomness being accumulated. As a consequence, *TD* methods usually learn much
    faster than MC methods.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在好的方面，新的估计回报，即*TD*目标，是我们用于蒙特卡洛更新的*t:T*。这是因为*TD*目标只依赖于单个动作、单个转换和单个奖励，因此积累的随机性要少得多。因此，*TD*方法通常比MC方法学习得更快。
- en: '| 0001 | A Bit Of HistoryTemporal-difference learning |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史时序差分学习'
- en: '|  | In 1988, Richard Sutton released a paper titled “Learning to Predict by
    the Methods of Temporal Differences” in which he introduced the *TD* learning
    method. The RW environment we’re using in this chapter was also first presented
    in this paper. The critical contribution of this paper was the realization that
    while methods such as MC calculate errors using the differences between predicted
    and actual returns, *TD* was able to use the difference between temporally successive
    predictions, thus, the name temporal-difference learning.TD learning is the precursor
    of methods such as SARSA, Q-learning, double Q-learning, deep Q-networks (DQN),
    double deep Q-networks (DDQN), and more. We’ll learn about these methods in this
    book. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 在1988年，理查德·萨顿发表了一篇题为“通过时序差分方法学习预测”的论文，其中他介绍了*TD*学习方法。我们在这章中使用的RW环境也首次在这篇论文中提出。这篇论文的关键贡献是意识到，虽然MC方法使用预测和实际回报之间的差异来计算误差，但*TD*能够使用时序连续预测之间的差异，因此得名时序差分学习。TD学习是SARSA、Q学习、双Q学习、深度Q网络（DQN）、双深度Q网络（DDQN）等方法的前身。我们将在本书中学习这些方法。|'
- en: '![](../Images/05_03.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05_03.png)'
- en: TD prediction
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: TD预测
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsFVMC, EVMC, and *TD*
    on the RW environment |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Details.png) | 它在于细节FVMC, EVMC, 和 *TD* 在RW环境中'
- en: '|  | I ran these three policy evaluation algorithms on the RW environment.
    All methods evaluated an all-left policy. Now, remember, the dynamics of the environment
    make it such that any action, Left or Right, has a uniform probability of transition
    (50% Left and 50% Right). In this case, the policy being evaluated is irrelevant.I
    used the same schedule for the learning rate, alpha, in all algorithms: alpha
    starts at 0.5, and it decreases exponentially to 0.01 in 250 episodes out of the
    500 total episodes. That’s 50% of the total number of episodes. This hyperparameter
    is essential. Often, alpha is a positive constant less than 1\. Having a constant
    alpha helps with learning in non-stationary environments.![](../Images/05_03__Sidebar17_alpha_schedule.png)However,
    I chose to decay alpha to show convergence. The way I’m decaying alpha helps the
    algorithms get close to converging, but because I’m not decreasing alpha all the
    way to zero, they don’t fully converge. Other than that, these results should
    help you gain some intuition about the differences between these methods. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | 我在这三个策略评估算法上运行了 RW 环境。所有方法都评估了全左策略。现在，请记住，环境的动态使得任何动作，无论是左还是右，都有相同的过渡概率（50%
    左和 50% 右）。在这种情况下，被评估的策略是不相关的。我在所有算法中使用了相同的学习率 alpha 的调度：alpha 从 0.5 开始，在 500 个总回合中的
    250 个回合内指数衰减到 0.01。这是总回合数的 50%。这个超参数至关重要。通常，alpha 是一个小于 1 的正常数。在非平稳环境中，保持 alpha
    的常数有助于学习。![图片](../Images/05_03__Sidebar17_alpha_schedule.png)然而，我选择衰减 alpha 以展示收敛。我衰减
    alpha 的方式有助于算法接近收敛，但由于我没有将 alpha 减少到零，它们并没有完全收敛。除此之外，这些结果应该能帮助你了解这些方法之间的差异。 |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpMC and *TD* both nearly converge
    to the true state-value function |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Tally.png) | Tally it UpMC 和 *TD* 都几乎收敛到真实的状态值函数 |'
- en: '|  | ![](../Images/05_03_Sidebar18.png) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/05_03_Sidebar18.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpMC estimates are noisy; *TD*
    estimates are off-target |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Tally.png) | Tally it UpMC 的估计值有噪声；*TD* 的估计值偏离目标 |'
- en: '|  | ![](../Images/05_03_Sidebar19.png) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/05_03_Sidebar19.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpMC targets high variance; *TD*
    targets bias |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Tally.png) | Tally it UpMC 旨在解决高方差问题；*TD* 旨在解决偏差问题
    |'
- en: '|  | ![](../Images/05_03_Sidebar20.png) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/05_03_Sidebar20.png) |'
- en: Learning to estimate from multiple steps
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从多步学习估计
- en: In this chapter, we looked at the two central algorithms for estimating value
    functions of a given policy through interaction. In MC methods, we sample the
    environment all the way through the end of the episode before we estimate the
    value function. These methods spread the actual return, the discounted total reward,
    on all states. For instance, if the discount factor is less than 1 and the return
    is only 0 or 1, as is the case in the RW environment, the MC target will always
    be either 0 or 1 for every single state. The same signal gets pushed back all
    the way to the beginning of the trajectory. This is obviously not the case for
    environments with a different discount factor or reward function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了通过交互估计给定策略的价值函数的两个核心算法。在 MC 方法中，我们在估计值函数之前，对环境进行采样直到回合结束。这些方法将实际回报，即折现的总奖励，分配到所有状态上。例如，如果折现因子小于
    1，并且回报只有 0 或 1，就像 RW 环境中那样，MC 目标将始终为每个单个状态是 0 或 1。相同的信号被推回到轨迹的起点。这显然不适用于具有不同折现因子或奖励函数的环境。
- en: '![](../Images/05_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05_04.png)'
- en: What’s in the middle?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 中间是什么？
- en: 'On the other hand, in *TD* learning, the agent interacts with the environment
    only once, and it estimates the expected return to go to, then estimates the target,
    and then the value function. *TD* methods bootstrap: they form a guess from a
    guess. What that means is that, instead of waiting until the end of an episode
    to get the actual return like MC methods do, *TD* methods use a single-step reward
    but then an estimate of the expected return-to-go, which is the value function
    of the next state.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在 *TD* 学习中，智能体与环境只交互一次，然后估计到达目标状态的预期回报，然后估计目标，然后是值函数。*TD* 方法是自举的：它们从一个猜测形成另一个猜测。这意味着，与
    MC 方法等待直到一个回合的结束以获取实际回报不同，*TD* 方法使用单步奖励，然后估计预期的回报到目标，即下一个状态的价值函数。
- en: But, is there something in between? I mean, that’s fine that *TD* bootstraps
    after one step, but how about after two steps? Three? Four? How many steps should
    we wait before we estimate the expected return and bootstrap on the value function?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，中间有什么东西吗？我的意思是，*TD* 在一步之后进行自举是不错的，但是两步之后呢？三步？四步？我们应该等待多少步之后才去估计期望回报并基于值函数进行自举？
- en: As it turns out, there’s a spectrum of algorithms lying in between MC and *TD*.
    In this section, we’ll look at what’s in the middle. You’ll see that we can tune
    how much bootstrapping our targets depend on, letting us balance bias and variance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在蒙特卡洛（MC）和 *TD* 之间存在着一系列算法。在本节中，我们将探讨中间部分。你会发现我们可以调整我们的目标依赖于多少自举，从而平衡偏差和方差。
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyMC and TD have distinct
    personalities |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Miguel.png) | 米格尔的类比：MC 和 TD 具有不同的个性'
- en: '|  | I like to think of MC-style algorithms as type-A personality agents and
    *TD*-style algorithms as type-B personality agents. If you look it up you’ll see
    what I mean. Type-A people are outcome-driven, time-conscious, and businesslike,
    while type-B are easygoing, reflective, and hippie-like. The fact that MC uses
    actual returns and *TD* uses predicted returns should make you wonder if there
    is a personality to each of these target types. Think about it for a while; I’m
    sure you’ll be able to notice several interesting patterns to help you remember.
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 我喜欢将蒙特卡洛风格的算法视为A型人格的代理人，而 *TD* 风格的算法视为B型人格的代理人。如果你查阅一下，你就会明白我的意思。A型人格的人是结果驱动的，时间观念强，做事有条理，而B型人格的人则随和、深思熟虑，有点嬉皮士的风格。MC使用实际回报，而
    *TD* 使用预测回报，这应该让你怀疑每种目标类型都有其个性。思考一下；我相信你一定能注意到几个有趣的模式来帮助你记忆。'
- en: 'N-step *TD* learning: Improving estimates after a couple of steps'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N步 *TD* 学习：在几步之后改进估计
- en: The motivation should be clear; we have two extremes, Monte Carlo methods and
    temporal-difference methods. One can perform better than the other, depending
    on the circumstances. MC is an infinite-step method because it goes all the way
    until the end of the episode.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 动机应该是清晰的；我们有两种极端，蒙特卡洛方法和时间差分方法。根据具体情况，一种方法可能比另一种方法表现更好。MC是一个无限步方法，因为它一直进行到情节的结束。
- en: I know, “infinite” may sound confusing, but recall in chapter 2, we defined
    a terminal state as a state with all actions and all transitions coming from those
    actions looping back to that same state with no reward. This way, you can think
    of an agent “getting stuck” in this loop forever and therefore doing an infinite
    number of steps without accumulating a reward or updating the state-value function.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，“无限”可能听起来有些令人困惑，但回想一下第2章，我们定义一个终端状态为一个所有动作和所有从这些动作产生的转换都回到该状态的循环状态，没有奖励。这样，你可以想象一个代理人在这个循环中永远“卡住”，因此进行无限多的步骤，而没有积累奖励或更新状态值函数。
- en: TD, on the other hand, is a one-step method because it interacts with the environment
    for a single step before bootstrapping and updating the state-value function.
    You can generalize these two methods into an *n*-step method. Instead of doing
    a single step, like *TD*, or the full episode like MC, why not use *n*-steps to
    calculate value functions and abstract *n* out? This method is called *n*-step
    *TD*, which does an *n*-step bootstrapping. Interestingly, an intermediate *n*
    value often performs the better than either extreme. You see, you shouldn’t become
    an extremist!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*TD* 是一个单步方法，因为它在与环境交互一步之后进行自举和更新状态值函数。你可以将这些两种方法推广为一个 *n* 步方法。与其像 *TD*
    一样只进行一步，或者像 MC 一样进行整个情节，为什么不使用 *n* 步来计算值函数并抽象出 *n* 呢？这种方法被称为 *n* 步 *TD*，它进行 *n*
    步的自举。有趣的是，中间的 *n* 值通常比两种极端都表现更好。你看，你不应该成为一个极端主义者！
- en: '| ![](../Images/icons_Math.png) | Show Me The Math*N*-step temporal-difference
    equations |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Math.png) | 展示数学：N步时间差分方程'
- en: '|  | ![](../Images/05_04_Sidebar22.png) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/05_04_Sidebar22.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak Python*N*-step *TD* |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Python.png) | 我会说 Python：N步 *TD*'
- en: '|  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Here’s my implementation of the *n*-step *TD* algorithm. There are many ways
    you can code this up; this is one of them for your reference.② Here we’re using
    the same hyperparameters as before. Notice n_step is a default of 3\. That is
    three steps and then bootstrap, or less if we hit a terminal state, in which case
    we don’t bootstrap (again, the value of a terminal state is zero by definition.)③
    Here we have the usual suspects.④ Calculate all alphas in advance.⑤ Now, here’s
    a hybrid between MC and *TD*. Notice we calculate the discount factors, but instead
    of going to max_steps like in my MC implementation, we go to n_step + 1 to include
    *n* steps and the bootstrapping estimate.⑥ We get into the episodes loop.⑦ This
    path variable will hold the n_step-most-recent experiences. A partial trajectory.⑧
    We’re going until we hit done and the path is set to none. You’ll see soon.⑨ Here,
    we’re “popping” the first element of the path.⑩ This line repeats on the next
    page.⑪ Same. Just for you to follow the indentation.⑫ This is the interaction
    block. We’re basically collecting experiences until we hit done or the length
    of the path is equal to n_step.⑬ *n* here could be ‘n_step’ but it could also
    be a smaller number if a terminal state is in the ‘path.’⑭ Here we’re extracting
    the state we’re estimating, which isn’t state.⑮ rewards is a vector of all rewards
    encountered from the est_state until *n*.⑯ partial_return is a vector of *discounted*
    rewards from est_state to *n*.⑰ bs_val is the bootstrapping value. Notice that
    in this case next state is correct.⑱ ntd_target is the sum of the partial return
    and bootstrapping value.⑲ This is the error, like we’ve been calculating all along.⑳
    The update to the state-value function㉑ Here we set path to None to break out
    of the episode loop, if path has only one experience and the done flag of that
    experience is True (only a terminal state in path.)㉒ We return V and V_track as
    usual. |
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '① 这里是我的 *n*-步 *TD* 算法的实现。你可以用很多种方式来实现它；这是其中一种供你参考。② 我们在这里使用与之前相同的超参数。注意 n_step
    的默认值是 3。这意味着三步然后进行自举，或者如果遇到终端状态，则更少，在这种情况下我们不会再次进行自举（再次强调，终端状态的价值定义为零。）③ 这里是我们通常会遇到的情况。④
    提前计算所有 alpha 值。⑤ 现在，这里是一个 MC 和 *TD* 的混合体。注意我们计算了折现因子，但与我的 MC 实现不同，我们不是走到 max_steps，而是走到
    n_step + 1，包括 *n* 步和自举估计。⑥ 我们进入情节循环。⑦ 这个路径变量将保存最近的 n_step 经验。这是一个部分轨迹。⑧ 我们会继续进行，直到完成并设置路径为
    none。你很快就会看到。⑨ 这里，我们在“弹出”路径的第一个元素。⑩ 这一行会在下一页重复。⑪ 同样。只是为了让你跟随缩进。⑫ 这是交互块。我们基本上是在收集经验，直到完成或路径长度等于
    n_step。⑬ *n* 这里可以是 ‘n_step’，但如果路径中有一个终端状态，它也可以是一个更小的数字。⑭ 这里我们正在提取我们正在估计的状态，这并不是状态。⑮
    rewards 是从 est_state 到 *n* 遇到的所有奖励的向量。⑯ partial_return 是从 est_state 到 *n* 的 *折现*
    奖励的向量。⑰ bs_val 是自举值。注意在这种情况下，下一个状态是正确的。⑱ ntd_target 是部分回报和自举值的总和。⑲ 这是错误，就像我们一直在计算的那样。⑳
    这是状态值函数的更新。㉑ 这里我们设置路径为 None 以跳出情节循环，如果路径只有一个经验且该经验的完成标志为 True（路径中只有一个终端状态。）㉒ 我们像往常一样返回
    V 和 V_track。| '
- en: 'Forward-view *TD*(*λ*): Improving estimates of all visited states'
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前瞻性 *TD*(*λ*)：改进所有访问状态的估计
- en: 'But, a question emerges: what is a good *n* value, then? When should you use
    a one-step, two-step, three-step, or anything else? I already gave practical advice
    that values of *n* higher than one are usually better, but we shouldn’t go all
    the way out to actual returns either. Bootstrapping helps, but its bias is a challenge.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，一个问题出现了：那么一个好的 *n* 值是什么？你什么时候应该使用一步、两步、三步，或者任何其他步骤？我已经给出了实际的建议，即通常高于一的所有
    *n* 值都更好，但我们也不应该走到实际回报。自举有帮助，但其偏差是一个挑战。
- en: How about using a weighted combination of all *n*-step targets as a single target?
    I mean, our agent could go out and calculate the *n*-step targets corresponding
    to the one-, two-, three-, ..., infinite-step target, then mix all of these targets
    with an exponentially decaying factor. Gotta have it!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有 *n*-步目标的加权组合作为一个单一目标如何？我的意思是，我们的智能体可以出去计算与一、二、三、……无限步目标相对应的 *n*-步目标，然后使用指数衰减因子将这些目标混合在一起。这绝对有必要！
- en: This is what a method called *forward-view* *TD***(***λ***)** does. Forward-view
    *TD*(*λ*)) is a prediction method that combines multiple *n*-steps into a single
    update. In this particular version, the agent will have to wait until the end
    of an episode before it can update the state-value function estimates. However,
    another method, called, *backward-view* *TD***(***λ***)**, can split the corresponding
    updates into partial updates and apply those partial updates to the state-value
    function estimates on every step, like leaving a trail of *TD* updates along a
    trajectory. Pretty cool, right? Let’s take a deeper look.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一种称为**向前视角TD(λ**)的方法。向前视角的**TD(λ**)是一种将多个**n**步合并为单个更新的预测方法。在这个特定版本中，智能体必须等待一个场景的结束，才能更新状态值函数的估计。然而，另一种称为**向后视角TD(λ**)的方法，可以将相应的更新拆分为部分更新，并将这些部分更新应用于每个步骤的状态值函数估计，就像沿着轨迹留下**TD**更新的痕迹。很酷，对吧？让我们更深入地了解一下。
- en: '![](../Images/05_05.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05_05.png)'
- en: Generalized bootstrapping
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化回溯
- en: '| ![](../Images/icons_Math.png) | Show Me The MathForward-view *TD*(*λ*) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 显示数学公式：向前视角的**TD(λ**) |'
- en: '|  | ![](../Images/05_05_Sidebar25.png) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![侧边栏图片](../Images/05_05_Sidebar25.png) |'
- en: '*TD*(*λ*): Improving estimates of all visited states after each step'
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**TD(λ**)：在每个步骤后改进所有访问状态估计'
- en: MC methods are under “the curse of the time step” because they can only apply
    updates to the state-value function estimates after reaching a terminal state.
    With *n*-step bootstrapping, you’re still under “the curse of the time step” because
    you still have to wait until *n* interactions with the environment have passed
    before you can make an update to the state-value function estimates. You’re basically
    playing catch-up with an *n*-step delay. For instance, in a five-step bootstrapping
    method, you’ll have to wait until you’ve seen five (or fewer when reaching a terminal
    state) states, and five rewards before you can make any calculations, a little
    bit like MC methods.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛（MC）方法受到“时间步诅咒”的影响，因为它们只能在达到终端状态后才能对状态值函数估计应用更新。使用**n**步回溯，你仍然受到“时间步诅咒”的影响，因为你仍然必须等待与环境的**n**次交互过去后，才能对状态值函数估计进行更新。你基本上是在一个**n**步延迟中追赶。例如，在五步回溯方法中，你必须等待看到五个（或当达到终端状态时更少）状态和五个奖励，然后才能进行任何计算，有点像蒙特卡洛方法。
- en: 'With forward-view *TD*(*λ*)), we’re back at MC in terms of the time step; the
    forward-view *TD*(*λ*)) must also wait until the end of an episode before it can
    apply the corresponding update to the state-value function estimates. But at least
    we gain something: we can get lower-variance targets if we’re willing to accept
    bias.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在向前视角的**TD(λ**)中，我们回到了蒙特卡洛的时间步；向前视角的**TD(λ**)也必须等待一个场景的结束，才能将相应的更新应用于状态值函数估计。但至少我们得到了一些东西：如果我们愿意接受偏差，我们可以得到更低方差的目标。
- en: In addition to generalizing and unifying MC and *TD* methods, backward-view
    *TD*(*λ*)), or **TD(***λ***)** for short, can still tune the bias/variance trade-off
    in addition to the ability to apply updates on every time step, just like *TD*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了泛化和统一蒙特卡洛（MC）和**TD**方法外，向后视角的**TD(λ**)，或简称为**TD(λ**)，还可以调整偏差/方差权衡，除了在每个时间步应用更新的能力外，就像**TD**一样。
- en: The mechanism that provides *TD*(*λ*)) this advantage is known as *eligibility
    traces*. An eligibility trace is a memory vector that keeps track of recently
    visited states. The basic idea is to track the states that are eligible for an
    update on every step. We keep track, not only of whether a state is eligible or
    not, but also by how much, so that the corresponding update is applied correctly
    to eligible states.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 提供这种优势的机制被称为**资格跟踪**。资格跟踪是一个记忆向量，它跟踪最近访问过的状态。基本思想是跟踪每个步骤上可以更新的状态。我们不仅跟踪状态是否有资格更新，而且还跟踪其程度，以确保相应的更新正确应用于有资格的状态。
- en: '![](../Images/05_06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05_06.png)'
- en: Eligibility traces for a four-state environment during an eight-step episode
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在八步场景中，四个状态环境的资格跟踪
- en: For example, all eligibility traces are initialized to zero, and when you encounter
    a state, you add a one to its trace. Each time step, you calculate an update to
    the value function for all states and multiply it by the eligibility trace vector.
    This way, only eligible states will get updated. After the update, the eligibility
    trace vector is decayed by the *λ* (weight mix-in factor) and *γ* (discount factor),
    so that future reinforcing events have less impact on earlier states. By doing
    this, the most recent states get more significant credit for a reward encountered
    in a recent transition than those states visited earlier in the episode, given
    that *λ* isn’t set to one; otherwise, this is similar to an MC update, which gives
    equal credit (assuming no discounting) to all states visited during the episode.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，所有资格迹都初始化为零，当你遇到一个状态时，你将其迹增加一个一。每次时间步，你计算所有状态的值函数的更新，并将其乘以资格迹向量。这样，只有合格的状态会得到更新。更新后，资格迹向量通过*λ*（权重混合因子）和*γ*（折扣因子）衰减，这样未来的强化事件对早期状态的影响就小了。通过这样做，最近的状态在最近转换中遇到的奖励比在剧集早期访问的状态获得更多的信用，前提是*λ*没有设置为1；否则，这类似于MC更新，它给剧集期间访问的所有状态分配相同的信用（假设没有折扣）。
    |
- en: '| ![](../Images/icons_Math.png) | Show Me The MathBackward-view *TD*(*λ*) —
    *TD*(*λ*) with eligibility traces, “the” *TD*(*λ*) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学Backward-view *TD*(*λ*) — 带资格迹的*TD*(*λ*)，即“the”
    *TD*(*λ*) |'
- en: '|  | ![](../Images/05_06_Sidebar26.png) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/05_06_Sidebar26.png) |'
- en: A final thing I wanted to reiterate is that *TD*(*λ*)) when *λ*=0 is equivalent
    to the *TD* method we learned about before. For this reason, *TD* is often referred
    to as **TD(0)**; on the other hand, *TD*(*λ*)), when *λ*=1 is equivalent to MC,
    well kind of. In reality, it’s equal to MC assuming offline updates, assuming
    the updates are accumulated and applied at the end of the episode. With online
    updates, the estimated state-value function changes likely every step, and therefore
    the bootstrapping estimates vary, changing, in turn, the progression of estimates.
    Still, **TD(1)** is commonly assumed equal to MC. Moreover, a recent method, called
    *true online* *TD***(***λ***)**, is a different implementation of *TD*(*λ*)) that
    achieves perfect equivalence of *TD*(0) with *TD* and *TD*(1) with MC.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我想重申的是，当*λ*=0时，*TD*(*λ*)与我们之前学过的*TD*方法等效。因此，*TD*通常被称为**TD(0)**；另一方面，当*λ*=1时，*TD*(*λ*)相当于MC，某种程度上。实际上，它等于MC假设离线更新，假设更新在剧集结束时累积并应用。对于在线更新，估计的状态值函数可能每步都改变，因此自举估计会变化，进而改变估计的进展。尽管如此，**TD(1)**通常假设等于MC。此外，一种最近的方法，称为*true
    online* *TD***(***λ***)**，是*TD*(*λ*)的不同实现，它实现了*TD*(0)与*TD*和*TD*(1)与MC的完美等价。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe *TD*(*λ*) algorithm,
    a.k.a. backward-view *TD*(*λ*) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python *TD*(*λ*)算法，也称为向后视角*TD*(*λ*)算法
    |'
- en: '|  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '① The method td_lambda has a signature very similar to all other methods. The
    only new hyperparameter is lambda_ (the underscore is because lambda is a restricted
    keyword in Python).② Set the usual suspects.③ Add a new guy: the eligibility trace
    vector.④ Calculate alpha for all episodes.⑤ Here we enter the episode loop.⑥ Set
    E to zero every new episode.⑦ Set initial variables.⑧ Get into the time step loop.⑨
    We first interact with the environment for one step and get the experience tuple.⑩
    Then, we use that experience to calculate the *TD* error as usual.⑪ We increment
    the eligibility of state by 1.⑫ And apply the error update to all eligible states
    as indicated by E.⑬ We decay E ...⑭ ... and continue our lives as usual. |'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ① td_lambda方法具有与其他所有方法非常相似的签名。唯一的新的超参数是lambda_（下划线是因为lambda在Python中是一个受限的关键字）。②
    设置常规的嫌疑对象。③ 添加一个新家伙：资格迹向量。④ 计算所有剧集的alpha。⑤ 这里我们进入剧集循环。⑥ 每个新剧集将E设置为零。⑦ 设置初始变量。⑧
    进入时间步循环。⑨ 我们首先与环境交互一步并获取经验元组。⑩ 然后，我们使用该经验来计算通常的*TD*误差。⑪ 我们将状态资格增加1。⑫ 并将错误更新应用于E指示的所有合格状态。⑬
    我们衰减E...⑭ ...然后继续我们的生活，就像平常一样。 |
- en: '| ![](../Images/icons_Tally.png) | Tally it UpRunning estimates that *n*-step
    *TD* and *TD*(*λ*) produce in the RW environment |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 累计起来运行估计*n*-步*TD*和*TD*(*λ*)在RW环境中的产生 |'
- en: '|  | ![](../Images/05_06_Sidebar28.png) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/05_06_Sidebar28.png) |'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleEvaluating the optimal
    policy of the Russell and Norvig’s Gridworld environment |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 一个具体的例子评估Russell和Norvig的网格世界环境的最佳策略 |'
- en: '|  | Let’s run all algorithms in a slightly different environment. The environment
    is one you’ve probably come across multiple times in the past. It is from Russell
    and Norvig’s book on AI.![](../Images/05_06_Sidebar29a.png)Russell and Norvig’s
    GridworldThis environment, which I will call Russell and Norvig’s Gridworld (RNG),
    is a 3 x 4 grid world in which the agent starts at the bottom-left corner, and
    it has to reach the top-right corner. There is a hole, similar to the frozen lake
    environment, south of the goal, and a wall near the start. The transition function
    has a 20% noise; that is, 80% the action succeeds, and 20% it fails uniformly
    at random in orthogonal directions. The reward function is a –0.04 living penalty,
    a +1 for landing on the goal, and a –1 for landing on the hole.Now, what we’re
    doing is evaluating a policy. I happen to include the optimal policy in chapter
    3’s Notebook: I didn’t have space in that chapter to talk about it. In fact, make
    sure you check all the Notebooks provided with the book.![](../Images/05_06_Sidebar29b.png)Optimal
    policy in the RNG environment |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | 让我们在一个略微不同的环境中运行所有算法。这个环境你可能过去多次遇到过。它来自拉塞尔和诺维格的《人工智能》一书！![拉塞尔和诺维格的Gridworld](../Images/05_06_Sidebar29a.png)拉塞尔和诺维格的Gridworld这个环境，我将称之为拉塞尔和诺维格的网格世界（RNG），是一个3
    x 4的网格世界，其中智能体从左下角开始，必须到达右上角。目标南方有一个洞，类似于冰冻湖环境，起点附近有一堵墙。转移函数有20%的噪声；也就是说，80%的动作成功，20%的动作以随机均匀的方式在正交方向上失败。奖励函数是一个-0.04的生存惩罚，在目标上获得+1分，在洞上获得-1分。现在，我们正在评估一个策略。我恰好将最优策略包含在第3章的笔记本中：在那个章节我没有空间来讨论它。实际上，确保你检查了书中提供的所有笔记本！![RNG环境中的最优策略](../Images/05_06_Sidebar29b.png)
    |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpFVMC, *TD*, *n*-step *TD*, and
    *TD*(*λ*) in the RNG environment |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ![计数器](../Images/icons_Tally.png) | 总结FVMC，*TD*，*n*-步 *TD* 和 *TD*(*λ*)在RNG环境中的表现
    |'
- en: '|  | ![](../Images/05_06_Sidebar30.png) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏30](../Images/05_06_Sidebar30.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpRNG shows a bit better the bias
    and variance effects on estimates |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| ![计数器](../Images/icons_Tally.png) | 总结RNG展示了偏差和方差效应在估计上的影响 |'
- en: '|  | ![](../Images/05_06_Sidebar31.png) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏31](../Images/05_06_Sidebar31.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpFVMC and *TD* targets of the
    RNG’s initial state |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ![计数器](../Images/icons_Tally.png) | 总结RNG的初始状态下的FVMC和*TD*目标 |'
- en: '|  | ![](../Images/05_06_Sidebar32.png) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏32](../Images/05_06_Sidebar32.png) |'
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Learning from sequential feedback is challenging; you learned quite a lot about
    it in chapter 3\. You created agents that balance immediate and long-term goals.
    Methods such as value iteration (VI) and policy iteration (PI) are central to
    RL. Learning from evaluative feedback is also very difficult. Chapter 4 was all
    about a particular type of environment in which agents must learn to balance the
    gathering and utilization of information. Strategies such as epsilon-greedy, softmax,
    optimistic initialization, to name a few, are also at the core of RL.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从顺序反馈中学习是具有挑战性的；你在第3章学到了很多关于它的知识。你创建了平衡即时和长期目标的智能体。价值迭代（VI）和政策迭代（PI）等方法是RL的核心。从评估反馈中学习也非常困难。第4章全部关于一种特定的环境，其中智能体必须学会平衡信息的收集和利用。epsilon-greedy，softmax，乐观初始化等策略也是RL的核心。
- en: And I want you to stop for a second and think about these two trade-offs one
    more time as separate problems. I’ve seen 500-page and longer textbooks dedicated
    to each of these trade-offs. While you should be happy we only put 30 pages on
    each, you should also be wondering. If you want to develop new DRL algorithms,
    to push the state of the art, I recommend you study these two trade-offs independently.
    Search for books on “planning algorithms” and “bandit algorithms,” and put time
    and effort into understanding each of those fields. You’ll feel leaps ahead when
    you come back to RL and see all the connections. Now, if your goal is simply to
    understand DRL, to implement a couple of methods, to use them on your own projects,
    what’s in here will do.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你再停下来思考一下这两个权衡问题，作为独立的问题。我见过有500页甚至更长的教科书专门讨论这些权衡。虽然我们应该为每个权衡只用了30页而感到高兴，但你也应该有所疑问。如果你想开发新的DRL算法，推动技术前沿，我建议你独立研究这两个权衡。寻找关于“规划算法”和“赌博机算法”的书籍，并投入时间和精力去理解这些领域。当你回到RL并看到所有联系时，你会感到有很大的进步。现在，如果你的目标仅仅是理解DRL，实现一些方法，在自己的项目中使用它们，这里的内容就足够了。
- en: In this chapter, you learned about agents that can deal with feedback that’s
    simultaneously sequential and evaluative. And as mentioned before, this is no
    small feat! To simultaneously balance immediate and long-term goals and the gathering
    and utilization of information is something even most humans have problems with!
    Sure, in this chapter, we restricted ourselves to the prediction problem, which
    consists of estimating values of agents’ behaviors. For this, we introduced methods
    such as Monte Carlo prediction and temporal-difference learning. Those two methods
    are the extremes in a spectrum that can be generalized with the *n*-step *TD*
    agent. By merely changing the step size, you can get virtually any agent in between.
    But then we learned about *TD*(*λ*)) and how a single agent can combine the two
    extremes and everything in between in a very innovative way.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了能够处理同时具有顺序性和评估性的反馈的智能体。正如之前提到的，这可不是一件小事！同时平衡短期和长期目标以及收集和利用信息，即使是大多数人类都有困难！当然，在本章中，我们限制了自己只关注预测问题，即估计智能体行为的值。为此，我们介绍了蒙特卡洛预测和时序差分学习等方法。这两种方法是光谱两端的极端，可以用
    *n*-步 *TD* 智能体进行泛化。只需改变步长，你就可以得到介于两者之间的任何智能体。然后我们学习了 *TD*(*λ*)以及单个智能体如何以非常创新的方式结合这两个极端以及两者之间的所有内容。
- en: Next chapter, we’ll look at the control problem, which is nothing but improving
    the agents’ behaviors. The same way we split the policy-iteration algorithm into
    policy evaluation and policy improvement, splitting the reinforcement learning
    problem into the prediction problem and the control problem allows us to dig into
    the details and get better methods.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，我们将探讨控制问题，这实际上就是改善智能体的行为。就像我们将策略迭代算法分为策略评估和策略改进一样，将强化学习问题分为预测问题和控制问题，使我们能够深入了解细节并获得更好的方法。
- en: By now, you
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: evolving environments
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不断变化的环境
- en: Learned how these two challenges combine and give rise to the field of RL
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学到了如何将这两个挑战结合起来，并产生强化学习领域。
- en: Know about many ways of calculating targets for estimating state-value functions
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解了许多计算目标值以估计状态值函数的方法。
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ![推文图标](../Images/icons_Tweet.png) | 可分享的成就：独自工作并分享你的发现 |'
- en: '|  | Here are several ideas on how to take what you have learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you’ll take advantage of it.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，如何将你所学的内容提升到下一个层次。如果你愿意，与世界分享你的结果，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch05_tf01**: None of the methods in this chapter handle the time step
    limit that’s wrapped around many Gym environments. No idea what I’m talking about?
    No worries, I explain it in more detail in chapter 8\. However, for the time being,
    check out this file: [https://github.com/openai/gym/blob/master/gym/envs/__init__.py](https://github.com/openai/gym/blob/master/gym/envs/__init__.py).
    See how many environments, including the frozen lake, have a variable max_episode_steps.
    This is a time step limit imposed over the environments. Think about this for
    a while: how does this time step limit affect the algorithms presented in this
    chapter? Go to the book’s Notebook, and modify the algorithms so that they handle
    the time step limit correctly, and the value function estimates are more accurate.
    Do the value functions change? Why, why not? Note that if you don’t understand
    what I’m referring to, you should continue and come back once you do.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch05_tf01**：本章中没有任何一种方法处理围绕许多 Gym 环境的时间步限制。不明白我在说什么？不用担心，我在第 8 章中会详细解释。然而，目前你可以查看这个文件：[https://github.com/openai/gym/blob/master/gym/envs/__init__.py](https://github.com/openai/gym/blob/master/gym/envs/__init__.py)。看看有多少环境，包括冻结湖，都有一个可变的最大回合步数。这是对环境施加的时间步限制。思考一下：这个时间步限制如何影响本章中提出的算法？去书中的笔记本，修改算法以正确处理时间步限制，并使值函数估计更加准确。值函数会改变吗？为什么，为什么不呢？请注意，如果你不明白我在说什么，你应该继续前进，并在你理解之后再回来。'
- en: '**#gdrl_ch05_tf02:** Comparing and plotting the Monte Carlo and temporal- difference
    targets is useful. One thing that would help you understand the difference is
    to do a more throughout analysis of these two types of targets, and also include
    the *n*-step and *TD*-lambda targets. Go ahead and start by that collecting the
    *n*-step targets for different values of time steps, and do the same for different
    values of lambda in *TD*-lambda targets. How do these compare with MC and *TD*?
    Also, find other ways to compare these prediction methods. But, do the comparison
    with graphs, visuals!'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch05_tf02:** 比较和绘制蒙特卡洛和时差目标是有用的。帮助你理解差异的一件事是对这两种类型的目标进行更全面的分析，并包括n步和TD-lambda目标。首先，收集不同时间步长值的n步目标，并对TD-lambda目标中的不同lambda值做同样的事情。这些与MC和TD相比如何？此外，找到其他比较这些预测方法的方法。但是，用图表和视觉来进行比较！'
- en: '**#gdrl_ch05_tf03:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch05_tf03:** 在每一章中，我都在使用最后的标签作为通用的标签。请随意使用这个标签来讨论与本章相关的工作。没有什么比为自己创造作业更令人兴奋的了。确保分享你打算调查的内容以及你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from this list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@我 @mimoralea（我会转发），并使用这个列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一个博客文章，列出了学习深度强化学习的资源列表。查看它在这里<链接>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的工作。|
