- en: 8 Inferring user activity from Android accelerometer data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从安卓加速度计数据中推断用户活动
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Visualizing positional data from your phone in three dimensions along with time
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在三维空间以及时间维度上可视化您的手机位置数据
- en: Performing exploratory data analysis and identifying patterns in Android phone
    users
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对安卓手机用户进行探索性数据分析并识别模式
- en: Automatically grouping Android phone users by their positional data using clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚类自动将安卓手机用户根据其位置数据进行分组
- en: Visualizing K-means clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化K-means聚类
- en: 'Nowadays, we are pretty much inseparable from a small, thin, usually black
    device that connects us to one another and to the world: our mobile phones. These
    devices are computing marvels, miniaturized chips with powerful microprocessors
    that are much more powerful than desktop computers from a decade ago. Add to that
    capacious connections to Wi-Fi networks that allow broad connectivity to the world
    and Bluetooth, which allows narrow and close-by secure connection to edge devices.
    Soon, Wi-Fi 5G and Bluetooth 6 will increase these connections to geographically
    disparate networks, to terabytes of data and to millions of interconnected devices
    making up the Internet of Things.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们几乎无法与一个小巧、轻薄、通常为黑色的设备分开，这个设备将我们彼此以及与世界连接起来：我们的移动电话。这些设备是计算奇迹，微型芯片，拥有强大的微处理器，比十年前的台式计算机更加强大。再加上对Wi-Fi网络的广泛连接，以及允许与边缘设备进行窄带和近距离安全连接的蓝牙。不久，Wi-Fi
    5G和蓝牙6将使这些连接扩展到地理上分散的网络，到千兆字节的数据，以及构成物联网的数百万个互联设备。
- en: 'Also attached to those powerful phones are a variety of sensors, including
    cameras, temperature sensors, light-sensitive screens, and something perhaps little
    known to you before this chapter: accelerometers. An *accelerometer* detects and
    monitors vibration in rolling machines and can be used to obtain positional data
    in 3D space over time, which is 4D data. All cell phones are equipped with these
    sensors, which provide positional data in the following form:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些强大的手机还附带了各种传感器，包括摄像头、温度传感器、光敏屏幕，以及在本章之前可能对您来说不太为人所知的：加速度计。*加速度计*可以检测和监控滚动机器的振动，并可用于在三维空间中随时间获取位置数据，这是四维数据。所有手机都配备了这些传感器，它们以下列形式提供位置数据：
- en: (*X, Y, Z, t* )
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (*X, Y, Z, t* )
- en: As you walk, talk, climb stairs, and drive in your car, accelerometers collect
    and record this data in the cell phone’s memory, usually a solid-state device
    with hundreds of gigabytes (GB) of information, soon be terabytes (TB). That data—your
    position in space-time as you perform various activities, such as walking, talking,
    and climbing—is recorded and can be used to infer those activities based on your
    position in space and time in an unsupervised fashion. You can conceptualize the
    positional data captured by the cell phone as shown in figure 8.1\. A person moves
    in positive or negative X, Z on a plane, and the vertical position is captured
    by the Y-axis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当您行走、交谈、爬楼梯和开车时，加速度计会在手机的内存中收集和记录这些数据，通常是一个具有数百GB信息的大容量固态设备，很快将变成千兆字节(TB)。这些数据——您在执行各种活动（如行走、交谈和爬楼梯）时的时空位置——被记录下来，可以根据您在时空中的位置以无监督的方式推断这些活动。您可以想象手机捕获的位置数据如图8.1所示。一个人在平面上沿正负X、Z方向移动，而垂直位置由Y轴捕获。
- en: '![CH08_F01_Mattmann2](../Images/CH08_F01_Mattmann2.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F01_Mattmann2](../Images/CH08_F01_Mattmann2.png)'
- en: Figure 8.1 How (X, Y, Z) positional data captured by your cell phone looks in
    the real world. A person walks positively or negatively on the x- or z-axis in
    space over time, and when the person jumps or bends down, movement occurs on the
    y-axis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 在真实世界中，您的手机捕获的(X, Y, Z)位置数据看起来是怎样的。一个人在空间中沿着x轴或z轴正负移动，当这个人跳跃或弯腰时，y轴上发生移动。
- en: As I discussed in chapter 7, machine learning need not require labels to make
    predictions. You can do a lot by applying unsupervised methods such as k-means
    clustering and self-organizing map (SOM) to data points that naturally fit together.
    Like the Blu-ray discs that are lying all over your house after your kids threw
    them everywhere (wait—that doesn’t happen to you?), requiring organization and
    careful placement back in your media cabinet, there is a natural order in which
    you cluster things for organizational purposes. For those Blu-ray discs, the order
    might be by genre or actor.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在第7章中讨论的那样，机器学习不需要标签来做出预测。你可以通过应用无监督方法，如k-means聚类和自组织映射（SOM）到自然适合一起的数据点来做很多事情。就像你的孩子们把蓝光光盘扔得到处都是，在你家里到处都是（等等——这不会发生在你身上？），需要组织和仔细放置回你的媒体架子里一样，你为了组织目的而聚类的物品是有自然顺序的。对于这些蓝光光盘，顺序可能是按类型或演员来分类。
- en: But how can you classify or organize your positional data on your phone? That
    data is likely to have a natural organization; convince yourself of that. While
    you were running, jumping, or driving, all your positional data likely fit some
    trend or pattern. Perhaps the difference from point ti to point ti+1 involves
    a sharp change in velocity or acceleration. Velocity and acceleration are the
    first and second derivative of position (X, Y, Z) in figure 8.1 with respect to
    time (t). Many researchers have discovered that jerk—the third derivative of position
    with respect to time—represents the rate of change of the force acting on a body.
    This jerk turns out to be the appropriate trending and change in motion when you
    shift activities, as captured by your positional data. The magnitude differences
    in jerk provide nice bins for clustering your time-based positional data in an
    unsupervised fashion; these clusters represent your motion activities, such as
    running, climbing, and walking.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你如何在手机上对位置数据进行分类或组织呢？这些数据很可能具有自然组织性；请自己确信这一点。当你跑步、跳跃或驾驶时，所有位置数据很可能符合某种趋势或模式。也许从点ti到点ti+1的差异涉及速度或加速度的急剧变化。速度和加速度是位置（X，Y，Z）相对于时间（t）的8.1图中的第一和第二导数。许多研究人员发现，冲量——位置相对于时间的第三导数——代表了作用于物体上的力的变化率。这个冲量最终被证明是你转换活动时运动趋势和变化的适当趋势，正如你的位置数据所捕捉的那样。冲量的幅度差异为无监督地聚类基于时间的位置数据提供了很好的分类；这些聚类代表你的运动活动，如跑步、攀爬和行走。
- en: In this chapter, you build on the k-means and SOM techniques you’ve already
    learned and apply then to real positional data. This data comes from the University
    of California at Irvine (UCI) and its Machine Learning Repository, which is a
    smorgasbord of open datasets to which you can apply machine-learning techniques
    to generate useful insights. The User Activity from Walking dataset is precisely
    the type of data to which you could apply TensorFlow and unsupervised clustering
    techniques. Let’s get started!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将基于你已经学到的k-means和SOM技术，并将它们应用于实际的位置数据。这些数据来自加州大学欧文分校（UCI）的机器学习库，这是一个开放数据集的大杂烩，你可以应用机器学习技术来生成有用的见解。用户活动从行走数据集正是你可以应用TensorFlow和无监督聚类技术的数据类型。让我们开始吧！
- en: 8.1 The User Activity from Walking dataset
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 用户活动从行走数据集
- en: The User Activity from Walking dataset contains 22 participants and their associated
    Android-based positional data. The data was donated in March 2014 and has been
    accessed more than 69,000 times. The dataset was formed when the accelerometer
    in an Android phone positioned in the chest pocket of a participant collected
    (X, Y, Z, t) positional and time data. The participants walked in the wild and
    performed various activities along a predefined path. The data can be used for
    unsupervised inference of activity type. This dataset is a frequently used benchmark
    in the machine-learning domain because it provides challenges for identifying
    and authenticating people by using motion patterns.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 用户活动从行走数据集包含22名参与者和他们相关的基于Android的位置数据。数据是在2014年3月捐赠的，并且已经被访问超过69,000次。当参与者的Android手机放在胸前的口袋中时，加速度计收集了（X，Y，Z，t）位置和时间数据。参与者沿着预定义的路径在野外行走并执行各种活动。这些数据可以用于无监督地推断活动类型。这个数据集在机器学习领域是一个常用的基准，因为它为通过使用运动模式识别和验证人员提供了挑战。
- en: 'You can download the dataset from [http://mng.bz/aw5B](https://shortener.manning.com/aw5B).
    The dataset unzips into a set of files in a folder structure that looks like the
    following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[http://mng.bz/aw5B](https://shortener.manning.com/aw5B)下载数据集。数据集解压后，文件夹结构如下所示：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each CSV file corresponds to one of the 22 participants, and the contents of
    the CSV files look like positional data of the form (t, X, Y, Z) shown in figure
    8.2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个CSV文件对应22个参与者中的一个，CSV文件的内容看起来像图8.2中所示的(t, X, Y, Z)形式的位置数据。
- en: '![CH08_F02_Mattmann2](../Images/CH08_F02_Mattmann2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F02_Mattmann2](../Images/CH08_F02_Mattmann2.png)'
- en: Figure 8.2 How the participant data looks in the CSV files as a series of data
    points
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2展示了参与者数据在CSV文件中以一系列数据点的形式。
- en: 'The first step to perform on this CSV data is to convert it to something that
    TensorFlow can use for machine learning: a NumPy dataset. To do that, you can
    use TensorFlow’s `FileReader` API. You used this API in chapter 7 to extract feature
    vectors from the audio-file chromagrams and combine them to a NumPy dataset for
    clustering.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在对这个CSV数据进行操作的第一步是将它转换为TensorFlow可以用于机器学习的格式：一个NumPy数据集。为此，你可以使用TensorFlow的`FileReader`
    API。你在第7章中使用了这个API来从音频文件的色谱图中提取特征向量，并将它们组合成一个NumPy数据集用于聚类。
- en: 'In this chapter, instead of creating feature vectors by turning sound into
    numbers with a fast Fourier transform, you convert the numbers you already have—positions
    in (X, Y, Z) space—to jerk magnitudes. You can use Pandas and its nifty dataframe
    API to read the participant CSV files into a big table structure in memory; then
    you can get NumPy arrays out of that structure. Using NumPy arrays as I’ve shown
    you in earlier chapters makes dealing with TensorFlow and machine learning much
    easier, because you can combine the 22 participants and their positional data
    into a TensorFlow-compatible NumPy matrix. The workflow can be generalized to
    the following form (figure 8.3):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不是通过将声音转换为数字（使用快速傅里叶变换）来创建特征向量，而是将你已有的数字——在(X, Y, Z)空间中的位置——转换为加速度大小。你可以使用Pandas及其巧妙的dataframe
    API将参与者CSV文件读入内存中的一个大表格结构；然后你可以从这个结构中获取NumPy数组。使用我在前面章节中展示的NumPy数组处理TensorFlow和机器学习会更容易，因为你可以将22个参与者和他们的位置数据组合成一个TensorFlow兼容的NumPy矩阵。工作流程可以概括为以下形式（图8.3）：
- en: Use TensorFlow’s `FileReader` API to obtain the file contents and associated
    file names.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow的`FileReader` API获取文件内容和相关的文件名。
- en: Extract a feature vector per file as a NumPy array, using a methodology such
    as a chromagram fast Fourier transformation or jerk magnitudes on positional data
    by calculating its *N* th-order derivatives.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用诸如色谱图快速傅里叶变换或通过计算其*N*阶导数在位置数据上计算加速度大小的方法，为每个文件提取一个特征向量作为NumPy数组。
- en: '`S`tack NumPy arrays vertically as a NumPy matrix, creating an N × M array
    where *N* is the number of samples and *M* is the number of features.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将NumPy数组垂直堆叠成一个NumPy矩阵，创建一个N × M的数组，其中*N*是样本数量，*M*是特征数量。
- en: Perform clustering on the N × M NumPy matrix by using k-means, SOM, or some
    other technique.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k-means、SOM或其他技术对N × M的NumPy矩阵进行聚类。
- en: '![CH08_F03_Mattmann2](../Images/CH08_F03_Mattmann2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F03_Mattmann2](../Images/CH08_F03_Mattmann2.png)'
- en: Figure 8.3 The general methodology of preparing data for TensorFlow clustering,
    depicted left to right as input to the clustering process
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3展示了为TensorFlow聚类准备数据的一般方法，从左到右依次为聚类过程的输入。
- en: Listing 8.1 performs the first step in this workflow, traversing the CSV files
    by using TensorFlow and creating a pairwise set of the filename and its contents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1执行了这个工作流程的第一步，使用TensorFlow遍历CSV文件，并为文件名及其内容创建成对的集合。
- en: Listing 8.1 Obtaining the positional contents and filenames of the 22 participant
    CSVs
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1 获取22个参与者CSV文件的位置内容和文件名
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Creates a list of the participant CSV filenames
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建参与者CSV文件名的列表
- en: ❷ Counts the number of CSV filenames
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算CSV文件名的数量
- en: ❸ Reads the file contents by using the TensorFlow WholeFileReader API
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用TensorFlow WholeFileReader API读取文件内容
- en: ❹ Creates filename/content pairs for all participants
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为所有参与者创建文件名/内容对
- en: To start clustering your positional data for the participants, you need a TensorFlow
    dataset to work with. Let’s create one using the `FileReader` API from TensorFlow.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始对参与者的位置数据进行聚类，你需要一个TensorFlow数据集来工作。让我们使用TensorFlow的`FileReader` API创建一个。
- en: 8.1.1 Creating the dataset
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 创建数据集
- en: Keeping with what I showed you in chapter 7, you can structure your code by
    creating a `get_dataset` function, which relies on a `get_feature_vector` function
    (step 2 in the workflow shown in figure 8.3) to convert your raw CSV positional
    data to a feature vector for clustering. The implementation of this function calculates
    jerk and uses it as the feature vector; jerk is the third derivative of position
    with respect to time. When you’ve calculated jerk for all the participant data,
    you need to stack those feature vectors vertically into a big matrix that can
    be clustered (step 3 in the workflow in figure 8.3), which is the point of the
    `get_dataset` function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我在第7章中向你展示的，你可以通过创建一个`get_dataset`函数来组织你的代码，该函数依赖于一个`get_feature_vector`函数（图8.3中显示的工作流程中的步骤2）将你的原始CSV位置数据转换为聚类特征向量。此函数的实现计算加速度并将其用作特征向量；加速度是位置对时间的三阶导数。当你计算了所有参与者的加速度后，你需要将这些特征向量垂直堆叠到一个大矩阵中，以便进行聚类（图8.3中工作流程的步骤3），这正是`get_dataset`函数的目的。
- en: Before you get started extracting your feature vector and computing jerk, a
    good machine-learning practice in general is to do some exploratory data analysis
    and see what type of information you are dealing with by inspecting it visually.
    To get some sample positional data that you can look at, you should try to sketch
    out the `get_dataset` function first. The CSV files, as you saw in figure 8.2,
    come with four columns (Time, XPos, YPos, and ZPos), so you can have Pandas name
    the columns in its dataframe accordingly. Don’t worry that you haven’t written
    your `extract_feature_vector` function yet. Check out the code in listing 8.2
    to begin.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始提取特征向量和计算加速度之前，一般良好的机器学习实践是进行一些数据探索分析，通过视觉检查来了解你正在处理的信息类型。为了获取一些你可以查看的样本位置数据，你应该首先尝试绘制出`get_dataset`函数。如你在图8.2中看到的，CSV文件包含四个列（时间，X位置，Y位置和Z位置），因此你可以让Pandas根据其数据框命名列。不要担心你还没有编写你的`extract_feature_vector`函数。查看列表8.2中的代码以开始。
- en: Listing 8.2 Converting the filename and contents to a NumPy matrix dataset
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2 将文件名和内容转换为NumPy矩阵数据集
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Creates a TensorFlow session and counts the number of files needed to iterate
    through them
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个TensorFlow会话并计算需要遍历的文件数量
- en: ❷ Gets the filename by using TensorFlow
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用TensorFlow获取文件名
- en: ❸ Reads the file contents into a Pandas dataframe with named columns and collects
    them in a list
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将文件内容读取到具有命名列的Pandas数据框中，并将它们收集到一个列表中
- en: ❹ Extracts the feature vector
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 提取特征向量
- en: ❺ Stacks the vectors vertically in a NumPy matrix
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将向量垂直堆叠在NumPy矩阵中
- en: 'Before getting too deep into computing the feature vector, you can inspect
    the data from the Pandas dataframes you created in the `accel_files` variable.
    The X values from the positional data from the first two participants, for example,
    can be easily plotted with Matplotlib like so:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入计算特征向量之前，你可以检查`accel_files`变量中创建的Pandas数据框中的数据。例如，前两个参与者的位置数据中的X值可以很容易地用Matplotlib绘制如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The results in figure 8.4, even only along the X dimension, show overlapping
    patterns, at least through the first 5,000 or so time steps along the x-axis.
    There is something for a machine-learning algorithm to learn.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4的结果，即使只沿X维度，也显示了重叠的模式，至少在x轴的前5,000个或更多时间步内。这为机器学习算法提供了学习的东西。
- en: '![CH08_F04_Mattmann2](../Images/CH08_F04_Mattmann2.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F04_Mattmann2](../Images/CH08_F04_Mattmann2.png)'
- en: Figure 8.4 X positional data for the first two participants, using Pandas and
    Matplotlib. Overlap of points occurs abundantly in the initial 5,000 or so time
    steps (x-axis).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 使用Pandas和Matplotlib显示前两个参与者的X位置数据。在最初的约5,000个时间步内（x轴），点重叠现象很普遍。
- en: If you can spot a pattern, the machine-learning algorithm should be able to
    spot a similar pattern on all the dimensions. The algorithm should be able to
    identify that participants 1 and 2 have a similar X position for the first 5,000
    time steps, for example. You can exploit this information by using TensorFlow
    and easily expand it to deal with all three positional dimensions, as I’ll show
    you in the next section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够发现一个模式，机器学习算法应该能够在所有维度上发现类似的模式。算法应该能够识别出参与者1和2在前5,000个时间步内具有相似的X位置，例如。你可以通过使用TensorFlow利用这一信息，并轻松地将其扩展到处理所有三个位置维度，正如我将在下一节中向你展示的那样。
- en: 8.1.2 Computing jerk and extracting the feature vector
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 计算加速度并提取特征向量
- en: Let’s start by computing jerk magnitudes on all your data. You can use the NumPy
    `np.diff` function, which computes `out = a[n+1] - a[n]` for all `n` in your data,
    leaving you an array of size `n-1`. If you had `foo = [1., 2., 3., 4., 5.]` and
    performed `np.diff(foo)`, the result would be `[1., 1., 1., 1.]`. Computing `np.diff`
    once gives you the first derivative of your input positional data (velocity);
    computing it a second time gives you the second derivative (acceleration), and
    so on. Because jerk is the third derivative of position with respect to time,
    you need to call `np.diff` three times, passing as input the result of each preceding
    call to the subsequent one. Because `np.diff` works with multidimensional input,
    it computes the difference for each dimension M in your N × M input matrix, and
    your input has three dimensions for M, a position consisting of samples of X,
    Y, and Z.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从计算所有数据的急动量开始。你可以使用 NumPy 的 `np.diff` 函数，该函数计算 `out = a[n+1] - a[n]` 对于你的数据中的所有
    `n`，留下一个大小为 `n-1` 的数组。如果你有 `foo = [1., 2., 3., 4., 5.]` 并执行 `np.diff(foo)`，结果将是
    `[1., 1., 1., 1.]`。计算一次 `np.diff` 给你输入位置数据的第一导数（速度）；计算第二次给你第二导数（加速度），依此类推。因为急动量是位置对时间的三阶导数，你需要调用
    `np.diff` 三次，将每个前一个调用的结果传递给后续的调用。因为 `np.diff` 与多维输入一起工作，它计算输入矩阵中每个维度 M 的差异，而你的输入有三个维度，一个由
    X、Y 和 Z 样本组成的位置。
- en: The next important step in exploring your CSV data is understanding the number
    of samples per participant, which corresponds to the N parameter in your N × 3
    participant matrix. You can figure out the number of samples for each participant’s
    data by taking the results of listing 8.1 (a pairwise set of 22 participant filenames
    and their contents) and then plotting a quick histogram of the number of points
    collected per participant, as shown in listing 8.3\. The result, depicted in figure
    8.5, shows a non-uniform distribution of positional data taken for the 22 participants,
    which means that your N is not equal for all participants and their input, at
    least to start. Don’t fret; I’ll show you how to normalize the samples.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 探索你的 CSV 数据的下一个重要步骤是了解每个参与者的样本数量，这对应于你的 N × 3 参与者矩阵中的 N 参数。你可以通过取列表 8.1（一对 22
    个参与者文件名及其内容）的结果，然后绘制每个参与者收集到的位置点数量的快速直方图，如图 8.3 所示，来找出每个参与者数据的样本数量。结果，如图 8.5 所示，显示了为
    22 个参与者收集的位置数据的非均匀分布，这意味着你的 N 对于所有参与者及其输入来说并不相等，至少一开始是这样的。别担心，我会告诉你如何归一化样本。
- en: Listing 8.3 Deciphering positional points per participant
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 解码每个参与者的位置点
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Imports Matplotlib
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 Matplotlib
- en: ❷ Counts number of points per file in accel_files
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 统计 accel_files 中每个文件的位置点数量
- en: ❸ Plots a histogram of uniform width of number of points per participant file
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制每个参与者文件位置点数量的均匀宽度直方图
- en: '![CH08_F05_Mattmann2](../Images/CH08_F05_Mattmann2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F05_Mattmann2](../Images/CH08_F05_Mattmann2.png)'
- en: Figure 8.5 The spread of participant samples across all the 22 participant CSV
    files. Nine of the participants have approximately 5,000 positional samples, 1
    has approximately 17,000 another 20,000, and so on.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 所有 22 个参与者 CSV 文件中参与者样本的分布。有 9 个参与者的位置样本大约有 5,000 个，1 个有大约 17,000 个，另一个有
    20,000 个，等等。
- en: This result is a problem, as it was in chapter 7, because the goal of clustering
    is to have one 1 × M feature vector represent each participant you want to cluster.
    One observation you can make is that across all the time steps in the positional
    data, there is a maximum or dominant jerk magnitude in each of the X, Y, Z positional
    dimensions. Going back to the definition of jerk—rate of change of the force acting
    on a body—selecting the maximum or dominant value for jerk in each of the three
    positional dimensions gives you the highest force in each of the three axes and
    the most responsible for influencing the switching motions from one activity to
    another. Selecting the max jerk magnitude per dimension per sample allows you
    to reduce your matrix to a size N × 1 matrix, giving you a single 1D set of points
    per participant.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果是一个问题，因为它在第 7 章中也是如此，因为聚类的目标是让一个 1 × M 特征向量代表你想要聚类的每个参与者。你可以观察到，在位置数据的所有时间步长中，X、Y、Z
    位置维度中每个维度都有一个最大值或主导的急动量。回到急动量的定义——作用在物体上的力的变化率——在每个三个位置维度中选择最大值或主导值，可以得到三个轴上最高的力，以及最负责影响从一种活动切换到另一种活动的力。选择每个样本每个维度的最大急动量幅度，可以使你将矩阵减少到
    N × 1 的大小，从而为每个参与者提供一个单一的 1D 点集。
- en: To convert this matrix back to a 3D 1 × 3 matrix, you can perform a histogram
    count with three bins on the N sample max jerk magnitudes and then normalize by
    dividing by the number of samples N to produce your feature vector as a 1 × 3
    matrix for each participant. Doing so results in an effective motion signature
    per participant for clustering representations of the positional data independent
    of the number of samples per participant. The code that performs this conversion
    is your `get_feature_vector` function. Listing 8.4 has the gory details.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这个矩阵转换回3D 1 × 3矩阵，你可以对N个样本的最大冲量大小进行直方图计数，然后在样本数量N上归一化，以产生每个参与者的特征向量作为1 × 3矩阵。这样做将为每个参与者产生一个有效的运动特征，用于独立于每个参与者样本数量的位置数据的聚类表示。执行此转换的代码是您的`get_feature_vector`函数。列表8.4有详细的说明。
- en: Listing 8.4 Selecting and computing max jerk magnitudes per dimension
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 选择和计算每个维度的最大冲量大小
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Initializes the jerk matrix and TensorFlow operator for computing max jerk
    for each dimension
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化冲量矩阵和TensorFlow运算符，用于计算每个维度的最大冲量
- en: ❷ We don’t need the time variable, so consider only (X, Y, Z) of (t, X, Y, Z)
    by chopping off t.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们不需要时间变量，因此考虑(t, X, Y, Z)中的(X, Y, Z)，通过截断t来考虑。
- en: ❸ Computes velocity (v_X), acceleration (a_X), and jerk (j_X)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算速度（v_X）、加速度（a_X）和冲量（j_X）
- en: ❹ Computes max jerk by running TensorFlow
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过运行TensorFlow计算最大冲量
- en: ❺ Number of samples is 1; number of features is 3.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 样本数量为1；特征数量为3。
- en: ❻ Returns max jerk magnitude normalized by number of samples (total points)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回按样本数量（总点数）归一化的最大冲量大小
- en: You can convince yourself that in performing this data manipulation, there is
    still something to learn from plotting the resultant normalized jerk magnitudes
    with Matplotlib and inspecting them per participant sample for each of the 22
    participants. Listing 8.5 sets up the plot to perform this exploratory data analysis.
    The first steps are creating a subplot for each participant and their 3D X, Y,
    Z normalized jerk magnitudes, using a different random color for each of the three
    bars per participant. As you can tell from figure 8.6, there is a definite pattern.
    The bars are closely aligned in certain dimensions for some participants, which
    means that the participants either changed motions to perform the same activity
    or shifted away from a particular activity that they were both performing. In
    other words, there are trends that you can spot, and so can a machine-learning
    algorithm for clustering. Now you are ready to cluster similar participants.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用Matplotlib绘制结果归一化冲量大小并检查每个参与者的样本来确信，在进行此数据操作时，仍然可以从绘制归一化冲量大小和检查每个22名参与者的样本中学习到一些东西。列表8.5设置了绘图以执行此探索性数据分析。第一步是为每个参与者和他们的3D
    X、Y、Z归一化冲量大小创建子图，每个参与者的三个条形使用不同的随机颜色。正如您可以从图8.6中看出，存在一个明显的模式。某些参与者在某些维度上的条形紧密对齐，这意味着参与者要么改变了动作以执行相同的活动，要么从他们共同执行的活动中偏离。换句话说，您可以找到趋势，机器学习算法也可以找到聚类。现在您已经准备好对相似参与者进行聚类。
- en: Listing 8.5 Visualizing the normalized jerk magnitudes
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 可视化标准化冲量大小
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The three dimensions of position in X, Y, and Z
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ X、Y、Z中的位置的三维
- en: ❷ Picks a random color for each of the 3 bars for the 22 participants
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为22名参与者的每个条形选择一个随机颜色
- en: ❸ Adds a subplot of each 3-bar for all 22 participants
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为所有22名参与者的每个3条柱子添加子图
- en: '![CH08_F06_Mattmann2](../Images/CH08_F06_Mattmann2.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F06_Mattmann2](../Images/CH08_F06_Mattmann2.png)'
- en: 'Figure 8.6 The normalized jerk magnitudes for all 22 participants in X, Y,
    Z dimensional space. As you can see, there is a definite pattern: participants’
    motions changed depending on their activity in X, Y, and Z.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 所有22名参与者在X、Y、Z三维空间中的标准化冲量大小。如图所示，存在一个明显的模式：参与者的动作取决于他们在X、Y和Z方向上的活动。
- en: 8.2 Clustering similar participants based on jerk magnitudes
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 基于冲量大小对相似参与者进行聚类
- en: As you learned in chapter 7, k-means clustering is an unsupervised clustering
    approach with a single hyperparameter, K, that you can use to set the number of
    desired clusters. Normally, you could pick different values of K to see what happens
    and how closely the resulting clusters fit the data you provided. Because we normalized
    the maximum jerk magnitudes across thousands of data points in time for each participant,
    we are getting a sort of summary of the positional changes (jerk) over that time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第7章中学到的，k-means 聚类是一种无监督聚类方法，它有一个单超参数 K，你可以用它来设置所需聚类的数量。通常，你可以选择不同的 K 值来观察结果，以及生成的聚类与提供的数据的拟合程度。由于我们对每个参与者在时间上成千上万的数据点的最大加速度大小进行了归一化，因此我们得到了该时间段内位置变化（加速度）的一种总结。
- en: An easy value to pick for K to start with is 2, to see whether a natural dividing
    line exists between participants. Perhaps some participants had wild positional
    changes; they ran fast and then stopped, or went from doing nothing to jumping
    high in the air. These sudden changes in motion over time should yield high jerk
    magnitudes, compared with those of other participants who changed their motion
    gradually, slowing down before coming to a stop, for example, or walking or lightly
    jogging the entire time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 K 的一个容易选择的值是 2，以查看是否存在参与者之间的自然分割线。也许有些参与者有极端的位置变化；他们跑得快然后停下来，或者从什么也不做到跳得很高。与那些逐渐改变运动的其他参与者相比，这些随时间变化的运动突然变化应该会产生较高的加速度大小，例如，在停下来之前减速，或者整个时间都在走路或轻快慢跑。
- en: To that end, picking K = 2 implies there may be two natural bins of motion over
    time for our participants. Let’s try it with TensorFlow. In chapter 7, I showed
    you how to use TensorFlow to implement a K-means algorithm and run it on your
    input NumPy matrix of feature vectors. You can carry forward the same code and
    apply it to the dataset resulting from calling the `get_dataset` function from
    listing 8.2.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，选择 K = 2 意味着我们的参与者可能存在两个随时间变化的自然运动区间。让我们用 TensorFlow 来试一试。在第7章中，我向你展示了如何使用
    TensorFlow 实现 K-means 算法，并在你的输入 NumPy 矩阵特征向量上运行它。你可以将相同的代码向前推进，并将其应用于调用列表8.2中的
    `get_dataset` 函数得到的数据集。
- en: 'Listing 8.6 adds functions defined in chapter 7 to implement k-means, runs
    the functions on your input participant data, and clusters the input data. To
    refresh your memory, first define the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6添加了在第7章中定义的函数，以实现 k-means，并在你的输入参与者数据上运行这些函数，对输入数据进行聚类。为了刷新你的记忆，首先定义以下内容：
- en: Hyperparameters, `k`, and `max_iterations`
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数 `k` 和 `max_iterations`
- en: Number of clusters
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类的数量
- en: Maximum times you will update the centroids and assign points to new clusters
    based on their distances from the centroids
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将更新质心并将点分配到新聚类的最大次数，基于它们与质心的距离
- en: The `initial_cluster_centroids` function takes the first K points in the dataset
    as the initial cluster guesses, which could be random. Then the `assign_cluster`
    function uses TensorFlow’s `expand_dims` function to create an additional dimension
    for computing the difference between points in the X dataset and the initially
    guessed centroids. Points farther from the centroids are likely not in the same
    cluster as those with similar distances.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_cluster_centroids` 函数将数据集中的前 K 个点作为初始聚类猜测，这些猜测可能是随机的。然后，`assign_cluster`
    函数使用 TensorFlow 的 `expand_dims` 函数为计算 X 数据集中点与最初猜测的质心之间的差异创建一个额外的维度。距离质心较远的点可能不在与距离相似的点相同的聚类中。'
- en: Creating another dimension allows you to partition those distances into K clusters
    initially selected in the `expanded_centroids` variable, which stores the summed,
    squared distances between X points and the cluster centroids through each iteration
    of the algorithm.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 创建另一个维度允许你将这些距离分割成 `expanded_centroids` 变量中最初选择的 K 个聚类，该变量存储了通过算法的每次迭代，X 点与聚类质心之间的平方距离之和。
- en: Then TensorFlow’s `argmin` function is applied to identify which dimension (0
    to K) is the minimum distance and use that as a mask to identify a particular
    data point and its group. The data point and group masks are summed and divided
    by data-point count to achieve new centroids, which are an average distance from
    all the points in that group. The TensorFlow `unsorted_segment_sum` applies the
    centroid group mask to the specific points in that cluster. Then TensorFlow’s
    `ones_like` creates a mask of count elements in each data point cluster and divides
    the summed distances of each data point from the centroid by the counts to get
    the new centroids.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后应用TensorFlow的`argmin`函数来识别哪个维度（0到K）是最小距离，并使用该掩码来识别特定的数据点和其组。数据点和组掩码相加并除以数据点计数以获得新的质心，这些质心是组中所有点平均距离。TensorFlow的`unsorted_segment_sum`将质心组掩码应用于该聚类的特定点。然后TensorFlow的`ones_like`在每个数据点簇中创建一个计数元素掩码，并将每个数据点从质心到距离的总和除以计数以获得新的质心。
- en: Listing 8.6 Running k-means on the jerk magnitudes of all participants
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 在所有参与者的加速度大小上运行k-means
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Defines hyperparameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义超参数
- en: ❷ Initially takes the first K elements from X to be each centroid
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始时从X中取出前K个元素作为每个质心
- en: ❸ Creates an expanded dimension to handle computing the distances of each point
    from the centroid
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个扩展维度来处理计算每个点与质心的距离
- en: ❹ Creates an expanded dimension to split the distances into K groups from each
    centroid
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建一个扩展维度，将距离分为从每个质心开始的K组
- en: ❺ Computes distances
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算距离
- en: ❻ Creates the group mask by selecting the min distance from each centroid
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 通过选择每个质心的最小距离来创建组掩码
- en: ❼ Sums the min distances for each data point by using the mask
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 通过使用掩码对每个数据点的最小距离求和
- en: ❽ Divides the summed distances by the number of data points and recomputes new
    centroids
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将总距离除以数据点的数量并重新计算新的质心
- en: ❾ Initializes TensorFlow
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 初始化TensorFlow
- en: ❿ Gets the initial dataset of participant normalized jerk magnitudes 22 × 3
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 获取参与者标准化加速度大小的初始数据集 22 × 3
- en: ⓫ Keeps clustering for 100 iterations and computing new center points
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 保持聚类100次迭代并计算新的中心点
- en: ⓬ Prints the centroids and groups
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 打印质心和组
- en: 'After running k-means and the code in listing 8.6, you obtain the two centroid
    points and cluster masks: 0 indicates the first group; otherwise, the label is
    1\. You can also visualize the participants and the two clusters by using some
    simple Matplotlib scatterpoint functionality. You can use your groups mask to
    index into the X dataset points by using 0s and 1s, and you can plot the centroid
    points as Xs, indicating the center of each group. Listing 8.7 has the code, and
    figure 8.7 plots the results. Though you can’t be sure of the label for each cluster,
    it’s clear that the participants’ motion clearly fits the two different categories
    because there is a nice separation of participant data in each group.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行k-means和列表8.6中的代码后，你获得了两个质心点和聚类掩码：0表示第一组；否则，标签为1。你也可以使用一些简单的Matplotlib散点图功能来可视化参与者和两个聚类。你可以使用你的组掩码通过使用0和1索引到X数据集的点，并且你可以绘制质心点作为X，表示每个组的中心。列表8.7有代码，图8.7显示了结果。虽然你不能确定每个聚类的标签，但很明显，参与者的运动清楚地符合两个不同的类别，因为每个组中的参与者数据有很好的分离。
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Listing 8.7 Visualizing the two participant motion clusters
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7 可视化两个参与者运动聚类
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Indexes the X data, using your groups mask
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用你的组掩码索引X数据
- en: ❷ Plots the X center points, using the centroid points
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用质心点绘制X中心点
- en: '![CH08_F07_Mattmann2](../Images/CH08_F07_Mattmann2.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F07_Mattmann2](../Images/CH08_F07_Mattmann2.png)'
- en: Figure 8.7 Two participant clusters of motion. Although you do not know the
    precise label for each cluster, you can see a clear delineation of the participants
    with similar motion over time.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 运动的两个参与者聚类。虽然你不知道每个聚类的精确标签，但你可以看到在时间上具有相似运动的参与者之间的清晰划分。
- en: You’ve used unsupervised clustering and the jerk magnitudes indicating participant
    motion to bin that motion into two categories. You aren’t sure which category
    involves short, sudden motions and which is gradual motions, but you can leave
    that determination for a future classification activity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经使用了无监督聚类和表示参与者运动的加速度大小来将运动分为两类。你不确定哪一类涉及短促的运动，哪一类是渐进的运动，但你可以将这个确定留给未来的分类活动。
- en: Next, instead of looking across all the participant motion, I show you how to
    use k-means to partition a single participant’s motion into motion categories.
    Onward!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，而不是查看所有参与者的动作，我将向您展示如何使用k-means将单个参与者的动作划分为动作类别。继续前进！
- en: 8.3 Different classes of user activity for a single participant
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 单个参与者的不同用户活动类别
- en: I showed you in chapter 7 how to take a single sound file and perform segmentation
    on it, separating the speakers in that sound file at specific time periods and
    classifying them in different groups. You can do something similar with your positional-point
    data per participant by using TensorFlow and k-means clustering.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我向您展示了如何对一个单独的声音文件进行分割，并在特定时间段内分离该声音文件中的说话者，并将他们分类到不同的组中。您可以通过使用TensorFlow和k-means聚类来对每个参与者的位置点数据进行类似操作。
- en: 'UCI’s machine-learning repository, where the User Activity from Walking dataset
    that you are using in this chapter comes from, points to a source paper titled
    “Personalization and User Verification in Wearable Systems Using Biometric Walking
    Patterns.” This paper states that the participants in the study performed five
    classes of activity: climbing, standing, walking, talking, and working. (You can
    obtain the source paper at [http://mng.bz/ggQE](http://mng.bz/ggQE).)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: UCI的机器学习存储库，其中包含你在本章中使用的数据集“步行用户活动”，指向一篇题为“使用生物特征步行模式在可穿戴系统中进行个性化用户验证”的源论文。这篇论文指出，研究中的参与者执行了五类活动：爬楼梯、站立、行走、谈话和工作。（您可以在[http://mng.bz/ggQE](http://mng.bz/ggQE)获取源论文。）
- en: Logically, it makes sense that you could group segments of N samples of participant
    positional data for a single participant and then attempt to automatically partition
    and cluster the points—which represent accelerometer readings at a particular
    time—into the different motions that occurred during activities performed in the
    wild. You can take a single CSV file from the dataset and segment it positionally
    by using the hyperparameter `segment_size`. When the file is segmented, you can
    reuse your `extract_feature_vector` method from listing 8.4 to compute jerk magnitudes
    and their histograms for each segment of size 50 positional points to achieve
    representational position jerk magnitudes to cluster along the 5 possible classes
    of activities. Listing 8.8 creates a modified version of `get_dataset` using `segment_size`
    and thus is named `get_dataset_segemented`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从逻辑上讲，将单个参与者的N个样本位置数据段分组，然后尝试自动分割和聚类代表特定时间加速度读数的点，并将它们聚类到野外活动中发生的不同动作中是有意义的。您可以从数据集中提取一个CSV文件，并使用超参数`segment_size`对其进行位置分割。当文件被分割后，您可以使用列表8.4中的`extract_feature_vector`方法来计算每个大小为50个位置点的段的加速度幅度及其直方图，以实现沿5个可能的动作类别的表示位置加速度幅度聚类。列表8.8创建了一个使用`segment_size`修改后的`get_dataset`版本，因此命名为`get_dataset_segemented`。
- en: Listing 8.8 Segmenting a single participant CSV
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.8 分割单个参与者CSV
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Defines hyperparameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义超参数
- en: ❷ Gets the CSV positional data
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取CSV位置数据
- en: ❸ Number of samples
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 样本数量
- en: ❹ For each set of segment_size, slice off that number of points and extract
    jerk magnitudes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对于每个`segment_size`，切掉相应数量的点并提取加速度的幅度。
- en: ❺ Stacks jerk magnitudes into an N × 3 matrix where N is num samples/segment
    size
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将加速度幅度堆叠成一个N × 3矩阵，其中N是样本数/分割大小
- en: 'After preparing your segmented dataset, you are ready to run TensorFlow again,
    this time with an N × 3 matrix, where *N* is the `number` `of` `samples` `/` `segment_size`.
    When you run k-means with `k` `=` `5` this time, you are asking TensorFlow to
    cluster your representations of 50 positional points in time according to the
    different motions a participant could possibly be performing: climbing, standing,
    walking, talking, or working. The code looks similar to listing 8.6 but is modified
    slightly in listing 8.9.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好你的分割数据集后，你就可以再次运行TensorFlow了，这次使用一个N × 3的矩阵，其中*N*是`样本数` `of` `segment_size`。当你这次运行`k`
    `=` `5`的k-means时，你是在要求TensorFlow根据参与者可能执行的不同动作将50个位置点的时间表示进行聚类：爬楼梯、站立、行走、谈话或工作。代码看起来与列表8.6相似，但在列表8.9中略有修改。
- en: Listing 8.9 Clustering a single participant file across different activity groups
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.9 在不同的活动组中聚类单个参与者文件
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Climbing Stairs, Standing, Walking, Talking, Working from the paper
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从论文中提取的爬楼梯、站立、行走、谈话和工作
- en: ❷ You need only one participant.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 只需要一个参与者。
- en: ❸ Prints the number of segment—in this case, 112
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印分割数量——在这种情况下，112
- en: ❹ Loops through the segments and prints the activity label, 0-4
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历段并打印活动标签，0-4
- en: 'You’ll want to visualize the result of running listing 8.9\. Lo and behold,
    those 112 segments appear to have an interesting distribution of positional 50
    time-step segments. The X group in the bottom left seems to have only a few segments
    by it. The three groups in the middle representing various activities seem to
    be the most dense, and the bottom far-right group has slightly more positional
    segments than the one at bottom left. You can see the result by running the code
    to generate figure 8.8:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你会想可视化运行列表8.9的结果。看吧，那些112个段似乎在位置50时间步段上有有趣的分布。左下角的X组似乎只有几个段。中间的三个组代表各种活动，看起来密度最大，而远在右下角的组比左下角的组有更多的位置段。你可以通过运行生成图8.8的代码来查看结果：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![CH08_F08_Mattmann2](../Images/CH08_F08_Mattmann2.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F08_Mattmann2](../Images/CH08_F08_Mattmann2.png)'
- en: Figure 8.8 Different activities for a single participant based on the provided
    positional data
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 基于提供的位置数据，单个参与者的不同活动
- en: The challenge, of course, is knowing exactly what group label each X corresponds
    to, but that activity is best saved for when you can examine the properties of
    each motion group and perhaps try to characterize them against ground truth. Perhaps
    the jerk magnitudes closest to the bottom-left group correspond to gradual acceleration
    and are likely the walking activity. Only through separate analysis can you figure
    out which categories each label (0-4) corresponds to—walking, jumping, running,
    and so on—but it’s pretty darned cool that machine learning and TensorFlow were
    able to partition the positional data into those groups automatically, without
    any ground truth!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，挑战在于确切知道每个X对应于哪个组标签，但这项活动最好留到你可以检查每个运动组的属性，并可能尝试根据真实情况对它们进行特征化的时候。也许最接近左下角组的加速度变化量对应于逐渐加速，并且很可能是行走活动。只有通过单独分析，你才能弄清楚每个标签（0-4）对应于哪些类别——行走、跳跃、跑步等等——但机器学习和TensorFlow能够自动将这些位置数据划分为那些组，而不需要任何真实情况，这确实非常酷！
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: From your cell phone’s positional data, a machine-learning algorithm using unsupervised
    machine learning and k-means clustering can infer what type of activity you were
    performing.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从你的手机位置数据中，一个使用无监督机器学习和k-means聚类的机器学习算法可以推断出你执行的活动类型。
- en: TensorFlow can easily compute k-means, and you can apply your code from chapter
    7 to it after transforming the data.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow可以轻松计算k-means，你可以在转换数据后应用第7章中的代码。
- en: Fingerprinting positional data is similar to fingerprinting text.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对位置数据的指纹识别类似于对文本的指纹识别。
- en: Preparing and cleaning the data are key steps in machine learning.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备和清理数据是机器学习中的关键步骤。
- en: Using k-means clustering and segmenting an existing participant’s data makes
    it easy to spot activity patterns, which is somewhat more difficult in a bigger
    group.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用k-means聚类和分割现有参与者的数据，可以轻松地发现活动模式，这在更大的群体中要困难一些。
