- en: Chapter 6\. Heavyweight Scraping with Scrapy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章\. 使用Scrapy进行大规模抓取
- en: As your scraping goals get more ambitious, hacking solutions with Beautiful
    Soup and requests can get very messy very fast. Managing the scraped data as requests
    spawn more requests gets tricky, and if your requests are being made synchronously,
    things start to slow down rapidly. A whole load of problems you probably hadn’t
    anticipated start to make themselves known. It’s at this point that you want to
    turn to a powerful, robust library that solves all these problems and more. And
    that’s where Scrapy comes in.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你的网络爬取目标变得更加宏大，使用Beautiful Soup和requests进行黑客式解决方案可能会非常快速变得非常混乱。管理作为请求生成更多请求的抓取数据变得棘手，如果你的请求是同步进行的，事情就会迅速变慢。一系列你可能没有预料到的问题开始显现出来。正是在这个时候，你需要转向一个强大、稳健的库来解决所有这些问题及更多。这就是Scrapy发挥作用的时候。
- en: Where Beautiful Soup is a very handy little penknife for fast and dirty scraping,
    Scrapy is a Python library that can do large-scale data scrapes with ease. It
    has all the things you’d expect, like built-in caching (with expiration times),
    asynchronous requests via Python’s Twisted web framework, user-agent randomization,
    and a whole lot more. The price for all this power is a fairly steep learning
    curve, which this chapter is intended to smooth, using a simple example. I think
    Scrapy is a powerful addition to any dataviz toolkit and really opens up possibilities
    for web data collection.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当Beautiful Soup是一个非常方便的快速且脏的抓取小工具时，Scrapy是一个可以轻松进行大规模数据抓取的Python库。它拥有你期望的所有功能，例如内置缓存（带有过期时间）、通过Python的Twisted
    web框架进行异步请求、用户代理随机化等等。所有这些功能的代价是一个相当陡峭的学习曲线，而本章旨在通过一个简单的例子来平滑这条曲线。我认为Scrapy是任何数据可视化工具包的强大补充，真正为网络数据收集打开了可能性。
- en: In [“Scraping Data”](ch05.xhtml#get_data_scraping), we managed to scrape a dataset
    containing all the Nobel Prize winners by name, year, and category. We did a speculative
    scrape of the winners’ linked biography pages, which showed that extracting the
    country of nationality was going to be difficult. In this chapter, we’ll set the
    bar on our Nobel Prize data a bit higher and aim to scrape objects of the form
    shown in [Example 6-1](#scrapy_target_JSON).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“数据抓取”](ch05.xhtml#get_data_scraping)中，我们成功抓取了包含所有诺贝尔奖获得者姓名、年份和类别的数据集。我们对获奖者链接的传记页面进行了猜测性抓取，显示提取国籍将会很困难。在本章中，我们将把我们的诺贝尔奖数据的目标设定得更高一些，并且旨在爬取类似[示例 6-1](#scrapy_target_JSON)所示形式的对象。
- en: Example 6-1\. Our targeted Nobel JSON object
  id: totrans-4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 我们的目标诺贝尔JSON对象
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In addition to this data, we’ll aim to scrape prizewinners’ photos (where applicable)
    and some potted biographical data (see [Figure 6-1](#scrapy_targets)). We’ll be
    using the photos and body text to add a little character to our Nobel Prize visualization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些数据，我们还将尝试爬取获奖者的照片（如果适用）和一些简短的生物数据（参见[图 6-1](#scrapy_targets)）。我们将使用这些照片和正文来为我们的诺贝尔奖可视化添加一些特色。
- en: '![dpj2 0601](assets/dpj2_0601.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0601](assets/dpj2_0601.png)'
- en: Figure 6-1\. Scraping targets for the prizewinners’ pages
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 爬取获奖者页面的目标
- en: Setting Up Scrapy
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Scrapy
- en: 'Scrapy should be one of the Anaconda packages (see [Chapter 1](ch01.xhtml#chapter_install)),
    so you should already have it on hand. If that’s not the case, then you can install
    it with the following `conda` command line:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy应该是Anaconda包之一（参见[第一章](ch01.xhtml#chapter_install)），所以你应该已经有它了。如果不是这样的话，你可以使用以下`conda`命令行来安装它：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you’re not using Anaconda, a quick `pip` install will do the job:^([1](ch06.xhtml#idm45607786166832))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用Anaconda，只需快速安装`pip`即可完成任务:^([1](ch06.xhtml#idm45607786166832))
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With Scrapy installed, you should have access to the `scrapy` command. Unlike
    the vast majority of Python libraries, Scrapy is designed to be driven from the
    command line within the context of a scraping project, defined by configuration
    files, scraping spiders, pipelines, and so on. Let’s generate a fresh project
    for our Nobel Prize scraping, using the `startproject` option. This is going to
    generate a project folder, so make sure you run it from a suitable work directory:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了Scrapy后，你应该可以访问`scrapy`命令。与大多数Python库不同，Scrapy设计为在爬取项目的上下文中通过命令行驱动，由配置文件、爬虫、管道等定义。让我们使用`startproject`选项为我们的诺贝尔奖爬取生成一个新项目。这将生成一个项目文件夹，所以确保你从一个合适的工作目录运行它：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As the output of `startproject` says, you’ll want to switch to the *nobel_winners*
    directory in order to start driving Scrapy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如`startproject`的输出所说，你需要切换到*nobel_winners*目录，以便开始使用Scrapy。
- en: 'Let’s take a look at the project’s directory tree:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看项目的目录树：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As shown, the project directory has a subdirectory with the same name and a
    config file *scrapy.cfg*. The *nobel_winners* subdirectory is a Python module
    (containing an *__init__.py* file) with a few skeleton files and a *spiders* directory,
    which will contain your scrapers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，项目目录有一个同名的子目录，还有一个配置文件 *scrapy.cfg*。*nobel_winners* 子目录是一个 Python 模块（包含一个
    *__init__.py* 文件），其中有几个骨架文件和一个 *spiders* 目录，其中将包含您的抓取器。
- en: Establishing the Targets
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立目标
- en: In [“Scraping Data”](ch05.xhtml#get_data_scraping), we tried to scrape the Nobel
    winners’ nationalities from their biography pages but found they were missing
    or inconsistently labeled in many cases (see [Chapter 5](ch05.xhtml#chapter_getting_data)).
    Rather than get the country data indirectly, a little Wikipedia searching shows
    a way through. There is a [page](https://oreil.ly/p6pXm) that lists winners by
    country. The winners are presented in titled, ordered lists (see [Figure 6-2](#scrapy_wiki_list)),
    not in tabular form, which makes recovering our basic name, category, and year
    data a little harder. Also the data organization is not ideal (e.g., the country
    header titles and winner lists aren’t in useful, separate blocks). As we’ll see,
    a few well-structured Scrapy queries will easily net us the data we need.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“数据抓取”](ch05.xhtml#get_data_scraping)中，我们尝试从诺贝尔获奖者的传记页面中获取其国籍信息，但发现在许多情况下，这些信息要么缺失，要么标记不一致（请参见[第5章](ch05.xhtml#chapter_getting_data)）。与其间接获取国家数据，不如进行一点维基百科搜索。有一个[页面](https://oreil.ly/p6pXm)按国家列出获奖者。这些获奖者以标题形式呈现，按顺序列出（请参见[图6-2](#scrapy_wiki_list)），而不是以表格形式呈现，这使得恢复我们的基本姓名、类别和年份数据变得更加困难。此外，数据组织不是最理想的（例如，国家标题和获奖者列表并未分开形成有用的区块）。正如我们将看到的，一些结构良好的
    Scrapy 查询将轻松地为我们提供所需的数据。
- en: '[Figure 6-2](#scrapy_wiki_list) shows the starting page for our first spider
    along with the key elements it will be targeting. A list of country name titles
    (A) is followed by an ordered list (B) of their Nobel Prize–winning citizens.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-2](#scrapy_wiki_list)显示了我们第一个爬虫的起始页面及其将要定位的关键元素。国家名称标题（A）列表后面是他们获得诺贝尔奖的公民的有序列表（B）。'
- en: '![dpj2 0602](assets/dpj2_0602.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0602](assets/dpj2_0602.png)'
- en: Figure 6-2\. Scraping Wikipedia’s Nobel Prizes by nationality
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 通过国籍抓取维基百科的诺贝尔奖
- en: 'In order to scrape the list data, we need to fire up our Chrome browser’s DevTools
    (see [“The Elements Tab”](ch04.xhtml#chrome_elements)) and inspect the target
    elements using the Elements tab and its inspector (magnifying glass). [Figure 6-3](#scrapy_wiki_list_source)
    shows the key HTML targets for our first spider: header titles (h2) containing
    a country name and followed by an ordered list (ol) of winners (li).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抓取列表数据，我们需要启动 Chrome 浏览器的 DevTools（请参阅[“元素标签”](ch04.xhtml#chrome_elements)）并使用
    Elements 标签及其检查器（放大镜）检查目标元素。[图6-3](#scrapy_wiki_list_source)显示了我们第一个爬虫的关键 HTML
    目标：包含国家名称的标题（h2）和后面的获奖者列表（ol）。
- en: '![dpj2 0603](assets/dpj2_0603.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0603](assets/dpj2_0603.png)'
- en: Figure 6-3\. Finding the HTML targets for the wikilist
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 找到维基列表的 HTML 目标
- en: Targeting HTML with Xpaths
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Xpath 定位 HTML 目标
- en: Scrapy uses [xpaths](https://oreil.ly/Y67BF) to define its HTML targets. Xpath
    is a syntax for describing parts of an X(HT)ML document, and while it can get
    rather complicated, the basics are straightforward and will often solve the job
    at hand.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 使用[xpaths](https://oreil.ly/Y67BF)来定义其 HTML 目标。Xpath 是描述 X(HT)ML 文档部分的语法，虽然它可能会变得相当复杂，但基础知识是直接的，并且通常能够完成手头的任务。
- en: 'You can get the xpath of an HTML element by using Chrome’s Elements tab to
    hover over the source and then right-clicking and selecting Copy XPath. For example,
    in the case of our Nobel Prize wikilist’s country names (h3 in [Figure 6-3](#scrapy_wiki_list_source)),
    selecting the xpath of Argentina (the first country) gives the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用 Chrome 的元素标签悬停在源上，然后右键单击并选择复制 XPath 来获取 HTML 元素的 xpath。例如，在我们的诺贝尔奖维基列表的国家名称（[图6-3](#scrapy_wiki_list_source)中的
    h3）的情况下，选择阿根廷（第一个国家）的 xpath 如下所示：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can use the following xpath rules to decode it:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下 xpath 规则进行解码：
- en: '`//E`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`//E`'
- en: Element `<E>` anywhere in the document (e.g., `//img` gets all images on the
    page)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的任何元素 `<E>`（例如，`//img` 获取页面上的所有图片）
- en: '`//E[@id="foo"]`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`//E[@id="foo"]`'
- en: Select element `<E>` with ID `foo`
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 ID 为 `foo` 的元素 `<E>`
- en: '`//*[@id="foo"]`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`//*[@id="foo"]`'
- en: Select any element with ID `foo`
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 选择任意带有 ID `foo` 的元素
- en: '`//E/F[1]`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`//E/F[1]`'
- en: First child element `<F>` of element `<E>`
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 元素 `<E>` 的第一个子元素 `<F>`
- en: '`//E/*[1]`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`//E/*[1]`'
- en: First child of element `<E>`
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 元素 `<E>` 的第一个子元素
- en: 'Following these rules shows that our Argentinian title `//*[@id="mw-content-text"]/div[1]/h3[1]`
    is the first header (h2) child of the first `div` of the DOM element with ID `mw-content-text`.
    This is equivalent to the following HTML:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这些规则表明，我们的阿根廷标题`//*[@id="mw-content-text"]/div[1]/h3[1]`是具有ID `mw-content-text`
    的DOM元素的第一个`div`的第一个标题（h2）子元素。这相当于以下HTML：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that unlike Python, the xpaths don’t use a zero-based index but make the
    first member *1*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与Python不同，xpath不使用从零开始的索引，而是将第一个成员设为*1*。
- en: Testing Xpaths with the Scrapy Shell
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Scrapy Shell测试Xpath
- en: 'Getting your xpath targeting right is crucial to good scraping and can involve
    a degree of iteration. Scrapy makes this process much easier by providing a command-line
    shell, which takes a URL and creates a response context in which you can try out
    your xpaths, like so:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正确使用xpath定位非常关键，对良好的抓取至关重要，并可能涉及一定程度的迭代。Scrapy通过提供一个命令行Shell大大简化了这个过程，该Shell接受一个URL，并创建一个响应上下文，在该上下文中可以尝试您的xpath，如下所示：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we have an IPython-based shell with code-complete and syntax highlighting
    in which to try out our xpath targeting. Let’s grab all the `<h3>` headers on
    the wiki page:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个基于IPython的Shell，具有代码完成和语法高亮，可以在其中尝试我们的xpath定位。让我们抓取wiki页面上的所有`<h3>`标题：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting `h3s` is a [SelectorList](https://oreil.ly/zpbqa), a specialized
    Python `list` object. Let’s see how many headers we have:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的`h3s`是一个[SelectorList](https://oreil.ly/zpbqa)，一个专门的Python`list`对象。让我们看看我们有多少个标题：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can grab the first [`Selector` object](https://oreil.ly/uBhdU) and query
    its methods and properties in the Scrapy shell by pressing Tab after appending
    a dot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以获取第一个[`Selector`对象](https://oreil.ly/uBhdU)，并通过在追加点后按Tab键，查询其在Scrapy shell中的方法和属性：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You’ll often use the `extract` method to get the raw result of the xpath selector:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您经常会使用`extract`方法来获取xpath选择器的原始结果：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This shows that our country headers start on the first `<h3>` and contain a
    `span` with class `mw-headline`. We can use the presence of the `mw-headline`
    class as a filter for our country headers and the contents as our country label.
    Let’s try out an xpath, using the selector’s `text` method to extract the text
    from the `mw-headline` span. Note that we use the `xpath` method of the `<h3>`
    selector, which makes the xpath query relative to that element:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们的国家标题从第一个`<h3>`开始，并包含一个类为`mw-headline`的`span`。我们可以使用`mw-headline`类的存在作为我们国家标题的过滤器，内容作为我们的国家标签。让我们尝试一个xpath，使用选择器的`text`方法从`mw-headline`
    span中提取文本。请注意，我们使用`xpath`方法的`<h3>`选择器，使xpath查询相对于该元素：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `extract` method returns a list of possible matches, in our case the single
    `'Argentina'` string. By iterating through the `h3s` list, we can now get our
    country names.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract`方法返回可能的匹配列表，在我们的情况下是单个字符串`''Argentina''`。通过遍历`h3s`列表，我们现在可以获取我们的国家名称。'
- en: 'Assuming we have a country’s `<h3>` header, we now need to get the `<ol>` ordered
    list of Nobel winners following it ([Figure 6-2](#scrapy_wiki_list) B). Handily,
    the xpath `following-sibling` selector can do just that. Let’s grab the first
    ordered list after the Argentina header:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个国家的`<h3>`标题，现在我们需要获取其后跟的诺贝尔获奖者的有序列表（[图 6-2](#scrapy_wiki_list) B）。方便的是，xpath的`following-sibling`选择器正好可以做到这一点。让我们抓取阿根廷标题之后的第一个有序列表：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Looking at the truncated data for `ol_arg` shows that we have selected an ordered
    list. Note that even though there’s only one `Selector`, `xpath` still returns
    a `SelectorList`. For convenience, you’ll generally just select the first member
    directly:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`ol_arg`的截断数据显示我们已选择了一个有序列表。请注意，即使只有一个`Selector`，`xpath`仍然会返回一个`SelectorList`。为了方便起见，通常直接选择第一个成员即可：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that we’ve got the ordered list, let’s get a list of its member `<li>`
    elements (as of mid 2022):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了有序列表，让我们获取其成员`<li>`元素的列表（截至2022年中）：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s examine one of those list elements using `extract`. As a first test,
    we’re looking to scrape the name of the winner and capture the list element’s
    text:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`extract`方法检查其中一个列表元素。作为第一个测试，我们要抓取获奖者的姓名，并捕获列表元素的文本：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Extracting the list element shows a standard pattern: a hyperlinked name to
    the winner’s Wikipedia page followed by a comma-separated winning category and
    year. A robust way to get the winning name is just to select the text of the list
    element’s first `<a>` tag:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 提取列表元素显示了一个标准模式：获奖者维基百科页面的超链接名称，后跟逗号分隔的获奖类别和年份。获取获奖者姓名的一个稳健方法是选择列表元素第一个`<a>`标签的文本：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It’s often useful to get all the text in, for example, a list element, stripping
    the various HTML `<a>`, `<span>`, and other tags. `descendant-or-self` gives us
    a handy way of doing this, producing a list of the descendants’ text:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 常常需要获取例如列表元素中的所有文本，去除各种HTML `<a>`、`<span>`和其他标签。`descendant-or-self`为我们提供了一个便捷的方法来执行此操作，生成后代文本的列表：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can get the full text by joining the list elements together:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过连接列表元素来获取完整的文本：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that the first item of `list_text` is the winner’s name, giving us another
    way to access it if, for example, it were missing a hyperlink.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`list_text`的第一项是获奖者的姓名，如果例如它缺少超链接，这给我们提供了另一种访问它的方式。
- en: Now that we’ve established the xpaths to our scraping targets (the name and
    link text of the Nobel Prize winners), let’s incorporate them into our first Scrapy
    spider.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了我们的爬取目标（诺贝尔奖获得者的姓名和链接文本）的xpath，让我们将它们合并到我们的第一个Scrapy spider中。
- en: Selecting with Relative Xpaths
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用相对Xpath选择
- en: As just shown, Scrapy `xpath` selections return lists of selectors which, in
    turn, have their own `xpath` methods. When using the `xpath` method, it’s important
    to be clear about relative and absolute selections. Let’s make the distinction
    clear using the Nobel page’s table of contents as an example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如刚才展示的，Scrapy的`xpath`选择返回选择器的列表，这些选择器又有其自己的`xpath`方法。在使用`xpath`方法时，清楚相对选择和绝对选择非常重要。让我们通过诺贝尔页面的目录来明确这种区别。
- en: 'The table of contents has the following structure:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 目录具有以下结构：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can select the table of contents of the Nobel wiki page using a standard
    `xpath` query on the response, and getting the `div` with ID `toc`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在响应上使用标准的`xpath`查询来选择诺贝尔wiki页面的目录，并获取带有ID `toc`的`div`：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we want to get all the country `<li>` list tags, we can use a relative `xpath`
    on the selected `toc` div. Looking at the HTML in [Figure 6-3](#scrapy_wiki_list_source)
    shows that the unordered list `ul` of countries is the first list member of the
    second list item of the table of content’s top list. This list can be selected
    by the following equivalent xpaths, both selecting children of the current `toc`
    selection relatively:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想获取所有国家的`<li>`列表标签，可以在所选的`toc` div上使用相对`xpath`。查看[图 6-3](#scrapy_wiki_list_source)中的HTML，显示国家无序列表`ul`是目录顶级列表的第二个列表项的第一个列表成员。以下等效的xpath可以选择此列表，两者都是相对于当前`toc`选择的子级：
- en: '[PRE22]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A common mistake is to use a nonrelative `xpath` selector on the current selection,
    which selects from the whole document, in this case getting all unordered (`<ul>`)
    `<li>` tags:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的错误是在当前选择上使用非相对`xpath`选择器，这会从整个文档中进行选择，在这种情况下获取所有无序（`<ul>`）`<li>`标签：
- en: '[PRE23]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Errors made from mistaking relative and nonrelative queries crop up a lot in
    the forums, so it’s good to be very aware of the distinction and watch those dots.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在论坛上，由于混淆相对和非相对查询而导致的错误经常发生，因此非常重要要非常注意这种区别和观察那些点。
- en: Tip
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Getting the right xpath expression for your target element(s) can be a little
    tricky, and those difficult edge cases can demand a complex nest of clauses. The
    use of a well-written cheat sheet can be a great help here, and thankfully there
    are many good xpath ones. A very nice selection can be found [at devhints.io](https://devhints.io/xpath).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 获取目标元素的正确xpath表达式可能有些棘手，这些难点可能需要复杂的子句嵌套。使用一个写得很好的速查表在这里可以提供很大帮助，幸运的是有许多好的xpath速查表。可以在[devhints.io](https://devhints.io/xpath)找到一个非常好的选择。
- en: A First Scrapy Spider
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个Scrapy Spider
- en: Armed with a little xpath knowledge, let’s produce our first scraper aiming
    to get the country and link text for the winners ([Figure 6-2](#scrapy_wiki_list)
    A and B).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握了一些xpath知识，让我们制作我们的第一个爬虫，旨在获取获奖者的国家和链接文本（[图 6-2](#scrapy_wiki_list) A 和 B）。
- en: 'Scrapy calls its scrapers *spiders*, each of which is a Python module placed
    in the *spiders* directory of your project. We’ll call our first scraper *nwinner_list_spider.py*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy称其爬虫为*spiders*，每个爬虫都是项目*spiders*目录中的Python模块。我们将第一个爬虫称为*nwinner_list_spider.py*：
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Spiders are subclassed `scrapy.Spider` classes, and any placed in the *spiders*
    directory will be automatically detected by Scrapy and made accessible by name
    to the `scrapy` command.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫是`scrapy.Spider`类的子类，放置在项目的*spiders*目录中，Scrapy会自动检测到它们，并通过名称使其可通过`scrapy`命令访问。
- en: The basic Scrapy spider shown in [Example 6-2](#scrapy_spider) follows a pattern
    you’ll be using with most of your spiders. First, you subclass a Scrapy `item`
    to create fields for your scraped data (section A in [Example 6-2](#scrapy_spider)).
    You then create a named spider by subclassing `scrapy.Spider` (section B in [Example 6-2](#scrapy_spider)).
    You will use the spider’s name when calling `scrapy` from the command line. Each
    spider has a `parse` method, which deals with the HTTP requests to a list of start
    URLs contained in a `start_url` class attribute. In our case, the start URL is
    the Wikipedia page for Nobel laureates by country.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 6-2](#scrapy_spider) 中显示的基本 Scrapy 爬虫遵循您将与大多数爬虫一起使用的模式。首先，您通过子类化 Scrapy
    `item` 创建字段来存储抓取的数据（第 A 部分在 [示例 6-2](#scrapy_spider)）。然后，通过子类化 `scrapy.Spider`
    创建一个命名的爬虫（第 B 部分在 [示例 6-2](#scrapy_spider)）。调用 `scrapy` 命令行时，将使用爬虫的名称。每个爬虫都有一个
    `parse` 方法，处理包含在 `start_url` 类属性中的起始 URL 的 HTTP 请求。在我们的情况下，起始 URL 是维基百科的诺贝尔奖得主页面。
- en: Example 6-2\. A first Scrapy spider
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 第一个 Scrapy 爬虫
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO1-1)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO1-1)'
- en: Gets all the `<h3>` headers on the page, most of which will be our target country
    titles.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 获取页面上所有 `<h3>` 标题，其中大多数将是我们的目标国家标题。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO1-2)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO1-2)'
- en: Where possible, gets the text of the `<h3>` element’s child `<span>` with class
    `mw-headline`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，获取带有类名为 `mw-headline` 的 `<h3>` 元素的子 `<span>` 的文本。
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO1-3)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO1-3)'
- en: Gets the list of country winners.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 获取国家获奖者列表。
- en: The `parse` method in [Example 6-2](#scrapy_spider) receives the response from
    an HTTP request to the Wikipedia Nobel Prize page and yields Scrapy items, which
    are then converted to JSON objects and appended to the output file, a JSON array
    of objects.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-2](#scrapy_spider) 中的 `parse` 方法接收来自维基百科诺贝尔奖页面的响应，并生成 Scrapy 项目，然后将其转换为
    JSON 对象并附加到输出文件，即一个 JSON 对象数组。'
- en: 'Let’s run our first spider to make sure we’re correctly parsing and scraping
    our Nobel data. First, navigate to the *nobel_winners* root directory (containing
    the *scrapy.cfg* file) of the scraping project. Let’s see what scraping spiders
    are available:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行我们的第一个爬虫，确保我们正确解析和抓取我们的诺贝尔数据。首先，导航到抓取项目的 *nobel_winners* 根目录（包含 *scrapy.cfg*
    文件）。让我们看看可用的抓取爬虫有哪些：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As expected, we have one `nwinners_list` spider sitting in the *spiders* directory.
    To start it scraping, we use the `crawl` command and direct the output to a *nwinners.json*
    file. By default, we get a lot of Python logging information accompanying the
    crawl:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，我们有一个 `nwinners_list` 爬虫位于 *spiders* 目录中。要启动它进行抓取，我们使用 `crawl` 命令并将输出重定向到
    *nwinners.json* 文件。默认情况下，我们会得到许多伴随抓取的Python日志信息：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO2-1)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO2-1)'
- en: We scraped 1,169 Nobel winners from the page.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从页面上抓取了 1,169 名诺贝尔获奖者。
- en: 'The output of the scrapy `crawl` shows 1,169 items successfully scraped. Let’s
    look at our JSON output file to make sure things have gone according to plan:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy `crawl` 的输出显示成功抓取了 1,169 个条目。让我们查看我们的 JSON 输出文件，确保事情按计划进行：
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you can see, we have an array of JSON objects with the four key fields present
    and correct.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有一个 JSON 对象数组，其中四个关键字段都是正确的。
- en: Now that we have a spider that successfully scrapes the list data for all the
    Nobel winners on the page, let’s start refining it to grab all the data we are
    targeting for our Nobel Prize visualization (see [Example 6-1](#scrapy_target_JSON)
    and [Figure 6-1](#scrapy_targets)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个成功抓取页面上所有诺贝尔获奖者列表数据的爬虫，让我们开始优化它，以抓取我们用于诺贝尔奖可视化的所有目标数据（见[示例 6-1](#scrapy_target_JSON)和[图 6-1](#scrapy_targets)）。
- en: 'First, let’s add all the data we plan to scrape as fields to our `scrapy.Item`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将我们计划抓取的所有数据作为字段添加到我们的 `scrapy.Item` 中：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It’s also sensible to simplify the code a bit and use a dedicated function,
    `process_winner_li`, to process the winners’ link text. We’ll pass a link selector
    and country name to it and return a dictionary containing the scraped data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 简化代码并使用专用函数 `process_winner_li` 处理获奖者链接文本也是明智的选择。我们将向其传递一个链接选择器和国家名称，并返回一个包含抓取数据的字典：
- en: '[PRE30]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `process_winner_li` method is shown in [Example 6-3](#scrapy_process_li).
    A `wdata` dictionary is filled with information extracted from the winner’s `li`
    tag, using a couple of regexes to find the prize year and category.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 6-3](#scrapy_process_li)中显示了`process_winner_li`方法。使用几个正则表达式从获奖者的`li`标签中提取信息，找到获奖年份和类别。
- en: Example 6-3\. Processing a winner’s list item
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. 处理获奖者列表项
- en: '[PRE31]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO3-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO3-1)'
- en: To grab the `href` attribute from the list item’s `<a>` tag (`<li><a href=*/wiki…​*>[winner
    name]</a>…​`), we use the xpath attribute referent @.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要从列表项的`<a>`标签（`<li><a href=*/wiki…​*>[winner name]</a>…​`）中获取`href`属性，我们使用xpath属性引用
    @。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO3-2)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO3-2)'
- en: Here, we use `re`, Python’s built-in regex library, to find the four-digit year
    strings in the list item’s text.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用Python的内置正则表达式库`re`来查找列表项文本中的四位数字年份。
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO3-3)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO3-3)'
- en: Another use of the regex library to find the Nobel Prize category in the text.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式库的另一个用途是在文本中查找诺贝尔奖类别。
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO3-4)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO3-4)'
- en: An asterisk following the winner’s name is used to indicate that the country
    is the winner’s by birth—​not nationality—​at the time of the prize (e.g., `"William
    Lawrence Bragg*, Physics, 1915"` in the list for Australia).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 获奖者姓名后面的星号用于指示国家是获奖者出生时的国家，而非国籍，如在澳大利亚列表中的`"William Lawrence Bragg*，物理学，1915"`。
- en: '[Example 6-3](#scrapy_process_li) returns all the winners’ data available on
    the main Wikipedia Nobels by Country page—that is, the name, year, category, country
    (country of birth or country of nationality when awarded the prize), and a link
    to the individual winners’ pages. We’ll need to use this last information to get
    those biographical pages and use them to scrape our remaining target data (see
    [Example 6-1](#scrapy_target_JSON) and [Figure 6-1](#scrapy_targets)).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-3](#scrapy_process_li) 返回主要维基百科诺贝尔奖按国家页面上所有获奖者的数据，包括姓名、年份、类别、国家（出生国或获奖时的国籍）以及个人获奖者页面的链接。我们需要使用最后这部分信息获取这些传记页面，并用它们来抓取我们剩余的目标数据（参见[示例 6-1](#scrapy_target_JSON)和[图 6-1](#scrapy_targets)）。'
- en: Scraping the Individual Biography Pages
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取个人传记页面
- en: The main Wikipedia Nobels by Country page gave us a lot of our target data,
    but the winner’s date of birth, date of death (where applicable), and gender are
    still to be scraped. It is hoped that this information is available, either implicitly
    or explicitly, on their biography pages (for nonorganization winners). Now’s a
    good time to fire up Chrome’s Elements tab and take a look at those pages to work
    out how we’re going to extract the desired data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的维基百科诺贝尔奖按国家页面为我们提供了大量目标数据，但获奖者的出生日期、死亡日期（如适用）和性别仍需抓取。希望这些信息能够在他们的传记页面上（非组织获奖者）隐式或显式地获取。现在是启动Chrome的Elements选项卡并查看这些页面以确定如何提取所需数据的好时机。
- en: We saw in the last chapter ([Chapter 5](ch05.xhtml#chapter_getting_data)) that
    the visible information boxes on individual’s pages are not a reliable source
    of information and are often missing entirely. Until recently,^([3](ch06.xhtml#idm45607784791328))
    a hidden `persondata` table (see [Figure 6-4](#scrapy_persondata)) gave fairly
    reliable access to such information as place of birth, date of death, and the
    like. Unfortunately, this handy resource has been deprecated.^([4](ch06.xhtml#idm45607784789392))
    The good news is that this is part of an attempt to improve the categorization
    of biographical information by giving it a dedicated space in [Wikidata](https://oreil.ly/ICbBi),
    Wikipedia’s central storage for its structured data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中看到（[第 5 章](ch05.xhtml#chapter_getting_data)）个人页面上的可见信息框不是可靠的信息来源，而且经常完全缺失。直到最近^([3](ch06.xhtml#idm45607784791328))，一个隐藏的`persondata`表（参见[图 6-4](#scrapy_persondata)）相当可靠地提供了出生地点、死亡日期等信息。不幸的是，这个方便的资源已被弃用^([4](ch06.xhtml#idm45607784789392))。好消息是，这是改进生物信息分类的一部分，通过在[Wikidata](https://oreil.ly/ICbBi)上为其提供一个专门的空间，维基百科的结构化数据中心。
- en: '![dpj2 0604](assets/dpj2_0604.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0604](assets/dpj2_0604.png)'
- en: Figure 6-4\. A Nobel Prize winner’s hidden `persondata` table
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 诺贝尔奖获奖者的隐藏`persondata`表
- en: Examining Wikipedia’s biography pages with Chrome’s Elements tab shows a link
    to the relevant Wikidata item (see [Figure 6-5](#scrapy_wikidata_link)), which
    takes you to the biographical data held at [*https://www.wikidata.org*](https://www.wikidata.org).
    By following this link, we can scrape whatever we find there, which we hope will
    be the bulk of our target data—​significant dates and places (see [Example 6-1](#scrapy_target_JSON)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Chrome 的 Elements 标签页中检查维基百科的传记页面，显示了一个指向相关维基数据项的链接（见 [图 6-5](#scrapy_wikidata_link)），它带你到保存在
    [*https://www.wikidata.org*](https://www.wikidata.org) 的传记数据。通过跟随这个链接，我们可以抓取那里找到的任何内容，我们希望那将是我们目标数据的主体部分——重要的日期和地点（见
    [示例 6-1](#scrapy_target_JSON)）。
- en: '![dpj2 0605](assets/dpj2_0605.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0605](assets/dpj2_0605.png)'
- en: Figure 6-5\. Hyperlink to the winner’s Wikidata
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 超链接到获奖者的维基数据
- en: Following the link to Wikidata shows a page containing fields for the data we
    are looking for, such as the date of birth of our prize winner. As [Figure 6-6](#scrapy_wikidata)
    shows, the properties are embedded in a nest of computer-generated HTML, with
    related codes, which we can use as a scraping identifier (e.g., date of birth
    has the code `P569`).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随到维基数据的链接显示了一个页面，其中包含我们正在寻找的数据字段，例如我们的获奖者的出生日期。如 [图 6-6](#scrapy_wikidata)
    所示，这些属性嵌入在由计算机生成的 HTML 巢穴中，带有相关的代码，我们可以将其用作抓取标识符（例如，出生日期的代码为 `P569`）。
- en: '![dpj2 0606](assets/dpj2_0606.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0606](assets/dpj2_0606.png)'
- en: Figure 6-6\. Biographical properties at Wikidata
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 维基数据的传记属性
- en: As [Figure 6-7](#scrapy_wikidata_xpath) shows, the actual data we want, in this
    case a date string, is contained in a further nested branch of HTML, within its
    respective property tag. By selecting the `div` and right-clicking, we can store
    the element’s xpath and use that to tell Scrapy how to get the data it contains.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 6-7](#scrapy_wikidata_xpath) 所示，在这种情况下，我们想要的实际数据，即日期字符串，包含在 HTML 的另一个嵌套分支中，位于其相应的属性标签内。通过选择
    `div` 并右键单击，我们可以存储元素的 xpath，并使用它告诉 Scrapy 如何获取它包含的数据。
- en: '![dpj2 0607](assets/dpj2_0607.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0607](assets/dpj2_0607.png)'
- en: Figure 6-7\. Getting the xpath for a Wikidata property
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 获取维基数据属性的 xpath
- en: Now that we have the xpaths necessary to find our scraping targets, let’s put
    it all together and see how Scrapy chains requests, allowing for complex, multipage
    scraping operations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了找到我们抓取目标所需的 xpath，让我们把所有这些放在一起，看看 Scrapy 如何链式处理请求，允许进行复杂的、多页面的抓取操作。
- en: Chaining Requests and Yielding Data
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接请求和数据产出
- en: In this section we’ll see how to chain Scrapy requests, allowing us to follow
    hyperlinks, scraping data as we go. First, let’s enable Scrapy’s page caching.
    While experimenting with xpath targets, we want to limit the number of calls to
    Wikipedia, and it’s good manners to store our fetched pages. Unlike some datasets
    out there, our Nobel Prize winners change but once a year.^([5](ch06.xhtml#idm45607784684416))
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何链式处理 Scrapy 请求，允许我们在进行数据抓取时跟随超链接。首先，让我们启用 Scrapy 的页面缓存。在尝试 xpath
    目标时，我们希望限制对维基百科的调用次数，并且将我们抓取的页面存储起来是个好习惯。与某些数据集不同，我们的诺贝尔奖获得者每年只变一次。^([5](ch06.xhtml#idm45607784684416))
- en: Caching Pages
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存页面
- en: 'As you might expect, Scrapy has a [sophisticated caching system](https://oreil.ly/ytYWP)
    that gives you fine-grained control over your page caching (e.g., allowing you
    to choose between database or filesystem storage backends, how long before your
    pages are expired, etc.). It is implemented as [middleware](https://oreil.ly/w8v7c)
    enabled in our project’s `settings.py` module. There are various options available
    but for the purposes of our Nobel scraping, simply setting `HTTPCACHE_ENABLED`
    to `True` will suffice:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，Scrapy 拥有一个[复杂的缓存系统](https://oreil.ly/ytYWP)，可以精细地控制页面缓存（例如，允许你选择数据库或文件系统存储后端，页面过期前的时间等）。它被实现为启用在我们项目的
    `settings.py` 模块中的[中间件](https://oreil.ly/w8v7c)。有各种选项可用，但为了我们的诺贝尔奖抓取目的，简单地将 `HTTPCACHE_ENABLED`
    设置为 `True` 就足够了：
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Check out the full range of Scrapy middleware [in Scrapy’s documentation](https://oreil.ly/9CMc4).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 查看完整的 Scrapy 中间件范围，请参阅[Scrapy 文档](https://oreil.ly/9CMc4)。
- en: Having ticked the caching box, let’s see how to chain Scrapy requests.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在勾选了缓存框后，让我们看看如何链式处理 Scrapy 请求。
- en: Yielding Requests
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 产出请求
- en: Our existing spider’s `parse` method cycles through the Nobel winners, using
    the `process_winner_li` method to scrape the country, name, year, category, and
    biography-hyperlink fields. We now want to use the biography hyperlinks to generate
    a Scrapy request that will fetch the bio pages and send them to a custom method
    for scraping.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现有的 spider 的`parse`方法循环遍历诺贝尔获奖者，使用`process_winner_li`方法来抓取国家、姓名、年份、类别和传记超链接字段。我们现在想要使用传记超链接来生成一个
    Scrapy 请求，以获取生物页面并将其发送到一个自定义的抓取方法。
- en: Scrapy implements a Pythonic pattern for chaining requests, using Python’s `yield`
    statement to create a generator,^([6](ch06.xhtml#idm45607784643296)) allowing
    Scrapy to easily consume any extra page requests we make. [Example 6-4](#scrapy_yield)
    shows the pattern in action.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 实现了一种 Python 风格的请求链接模式，利用 Python 的`yield`语句创建生成器，^([6](ch06.xhtml#idm45607784643296))
    使得 Scrapy 能够轻松处理我们所做的任何额外页面请求。[示例 6-4](#scrapy_yield) 展示了该模式的实际应用。
- en: Example 6-4\. Yielding a request with Scrapy
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. 使用 Scrapy 进行请求链接
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO4-1)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO4-1)'
- en: Makes a request to the winner’s biography page, using the link (`wdata[*link*]`)
    scraped from `process_winner_li`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 发起对获奖者传记页面的请求，使用从`process_winner_li`中抓取的链接（`wdata[*link*]`）。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO4-2)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO4-2)'
- en: Sets the callback function to handle the response.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回调函数以处理响应。
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO4-3)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO4-3)'
- en: Creates a Scrapy `Item` to hold our Nobel data and initializes it with the data
    just scraped from `process_winner_li`. This `Item` data is attached to the metadata
    of the request to allow any response access to it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 Scrapy `Item` 来保存我们的诺贝尔数据，并使用刚刚从`process_winner_li`中抓取的数据进行初始化。将此`Item`数据附加到请求的元数据中，以允许任何响应访问它。
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO4-4)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO4-4)'
- en: By yielding the request, we make the `parse` method a generator of consumable
    requests.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`yield`请求，使得`parse`方法成为可消费请求的生成器。
- en: '[![5](assets/5.png)](#co_heavyweight_scraping_with_scrapy_CO4-5)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_heavyweight_scraping_with_scrapy_CO4-5)'
- en: This method handles the callback from our bio-link request. In order to add
    scraped data to our Scrapy `Item`, we first retrieve it from the `response` metadata.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法处理来自我们生物链接请求的回调。为了将抓取的数据添加到我们的 Scrapy `Item` 中，我们首先从响应的元数据中检索它。
- en: Our investigation of the Wikipedia pages in [“Scraping the Individual Biography
    Pages”](#scrapy_indiv_bios) showed that we need to locate a winner’s Wikidata
    link from their biography page and use it to generate a request. We will then
    scrape the date, place, and gender data from the response.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对[“抓取个人传记页面”](#scrapy_indiv_bios)中的维基百科页面的调查表明，我们需要从其传记页面中找到获奖者的 Wikidata
    链接，并使用它来生成请求。然后，我们将从响应中抓取日期、地点和性别数据。
- en: '[Example 6-5](#scrapy_wikidata_source) shows `parse_bio` and `parse_wikidata`,
    the two methods used to scrape our winners’ biographical data. `parse_bio` uses
    the scraped Wikidata link to request the Wikidata page, yielding the `request`
    as it in turn was yielded in the `parse` method. At the end of the request chain,
    `parse_wikidata` retrieves the item and fills in any of the fields available from
    Wikidata, eventually yielding the item to Scrapy.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-5](#scrapy_wikidata_source) 展示了`parse_bio`和`parse_wikidata`这两种方法，用于抓取我们获奖者的生物数据。`parse_bio`使用抓取的
    Wikidata 链接请求 Wikidata 页面，并将请求作为`parse`方法中也使用`yield`返回的`request`进行返回。在请求链的末尾，`parse_wikidata`获取项目并填充来自
    Wikidata 的任何可用字段，最终将项目`yield`给 Scrapy。'
- en: Example 6-5\. Parsing the winners’ biography data
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. 解析获奖者的传记数据
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO5-1)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO5-1)'
- en: Extracts the link to Wikidata identified in [Figure 6-5](#scrapy_wikidata_link).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 提取在[图 6-5](#scrapy_wikidata_link)中标识的 Wikidata 链接。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO5-2)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO5-2)'
- en: Extracts the `wiki_code` from the URL, e.g., [*http://wikidata.org/wiki/Q155525*](http://wikidata.org/wiki/Q155525)
    → Q155525.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从 URL 中提取`wiki_code`，例如，[*http://wikidata.org/wiki/Q155525*](http://wikidata.org/wiki/Q155525)
    → Q155525。
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO5-3)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO5-3)'
- en: Uses the Wikidata link to generate a request with our spider’s `parse_wikidata`
    as a callback to deal with the response.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Wikidata 链接生成一个请求，其中我们的 spider 的`parse_wikidata`作为回调处理响应。
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO5-4)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO5-4)'
- en: These are the property codes we found earlier (see [Figure 6-6](#scrapy_wikidata)),
    with names corresponding to fields in our Scrapy item, `NWinnerItem`. Those with
    a `True` `link` attribute are contained in `<a>` tags.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们之前找到的属性代码（见[图 6-6](#scrapy_wikidata)），名称对应于我们Scrapy项目`NWinnerItem`中的字段。具有`True`
    `link`属性的属性包含在`<a>`标签中。
- en: '[![5](assets/5.png)](#co_heavyweight_scraping_with_scrapy_CO5-5)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_heavyweight_scraping_with_scrapy_CO5-5)'
- en: Finally we yield the item, which at this point should have all the target data
    available from Wikipedia.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回项目，此时应该已从维基百科获得了所有目标数据。
- en: 'With our request chain in place, let’s check that the spider is scraping our
    required data:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的请求链已经建立，让我们检查一下爬虫是否正在爬取我们需要的数据：
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Things are looking good. With the exception of the `born_in` field, which is
    dependent on a name in the main Wikipedia Nobel Prize winners list having an asterisk,
    we’re getting all the data we were targeting. This dataset is now ready to be
    cleaned by pandas in the coming chapter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来一切都很顺利。除了`born_in`字段外，该字段依赖于主维基百科诺贝尔奖获奖者列表中是否有一个带有星号的名称，我们获得了所有我们目标的数据。这个数据集现在已准备好在接下来的章节中由pandas进行清理。
- en: Now that we’ve scraped our basic biographical data for the Nobel Prize winners,
    let’s go scrape our remaining targets, some biographical body text, and a picture
    of the great man or woman, where available.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为诺贝尔奖获奖者爬取了基本的传记数据，让我们继续爬取我们的其余目标，包括一些传记正文和伟大男女士的图片（如果有的话）。
- en: Scrapy Pipelines
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrapy管道
- en: In order to add a little personality to our Nobel Prize visualization, it would
    be good to have a little biographical text and an image of the winner. Wikipedia’s
    biographical pages generally provide these things, so let’s go about scraping
    them.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为我们的诺贝尔奖可视化增添一点个性，最好有一些获奖者的简介文字和一张图片。维基百科的传记页面通常提供这些内容，所以让我们开始爬取它们吧。
- en: Up to now, our scraped data has been text strings. In order to scrape images
    in their various formats, we need to use a Scrapy *pipeline*. [Pipelines](https://oreil.ly/maUyE)
    provide a way of postprocessing the items we have scraped, and you can define
    any number of them. You can write your own or take advantage of those already
    provided by Scrapy, such as the `ImagesPipeline` we’ll be using.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们爬取的数据都是文本字符串。为了爬取各种格式的图片，我们需要使用一个Scrapy *pipeline*。[管道](https://oreil.ly/maUyE)提供了一种对我们爬取的项目进行后处理的方法，您可以定义任意数量的管道。您可以编写自己的管道，或者利用Scrapy已提供的管道，比如我们将要使用的`ImagesPipeline`。
- en: 'In its simplest form, a pipeline need only define a `process_item` method.
    This receives the scraped items and the spider object. Let’s write a little pipeline
    to reject genderless Nobel Prize winners (so we can omit prizes given to organizations
    rather than individuals) using our existing `nwinners_full` spider to deliver
    the items. First, we add a `DropNonPersons` pipeline to the `pipelines.py` module
    of our project:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，一个管道只需定义一个`process_item`方法。这个方法接收到爬取的项目和爬虫对象。让我们编写一个小管道来拒绝无性别的诺贝尔奖获得者（这样我们就可以省略掉授予组织而不是个人的奖项），使用我们现有的`nwinners_full`爬虫来传递项目。首先，我们将一个`DropNonPersons`管道添加到我们项目的`pipelines.py`模块中：
- en: '[PRE36]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO6-1)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO6-1)'
- en: If our scraped item failed to find a gender property at Wikidata, it is probably
    an organization such as the Red Cross. Our visualization is focused on individual
    winners, so here we use `DropItem` to remove the item from our output stream.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们爬取的项目在Wikidata上找不到性别属性，那么它很可能是一个组织，比如红十字会。我们的可视化重点是个人获奖者，所以在这里我们使用`DropItem`将该项目从输出流中删除。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO6-2)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO6-2)'
- en: We need to return the item to further pipelines or for saving by Scrapy.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将项目返回到进一步的管道或由Scrapy保存。
- en: 'As mentioned in the `pipelines.py` header, in order to add this pipeline to
    the spiders of our project, we need to register it in the `settings.py` module
    by adding it to a `dict` of pipelines and setting it to active (`1`):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如`pipelines.py`头部所述，为了将此管道添加到我们项目的爬虫中，我们需要在`settings.py`模块中将其注册到一个管道的`dict`中，并设置为活动状态（`1`）：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now that we’ve got the basic workflow for our pipelines, let’s add a useful
    one to our project.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们的项目创建了基本的管道工作流程，让我们向项目添加一个有用的管道。
- en: Scraping Text and Images with a Pipeline
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道爬取文本和图片
- en: We now want to scrape the winners’ biographies and photos (see [Figure 6-1](#scrapy_targets)),
    where available. We can scrape the biographical text using the same method as
    our last spider, but the photos are best dealt with by an image pipeline.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要抓取获奖者的传记和照片（见 [Figure 6-1](#scrapy_targets)），如果有的话。我们可以使用与上一个爬虫相同的方法来抓取传记文本，但是最好使用图像管道来处理照片。
- en: We could easily write our own pipeline to take a scraped image URL, request
    it from Wikipedia, and save to disk, but to do it properly requires a bit of care.
    For example, we would like to avoid reloading an image that was recently downloaded
    or hasn’t changed in the meantime. Some flexibility in specifying where to store
    the images is a useful feature. It would also be good to have the option of converting
    the images into a common format (e.g., JPG or PNG) or of generating thumbnails.
    Luckily, Scrapy provides an `ImagesPipeline` object with all this functionality
    and more. This is one of its [media pipelines](https://oreil.ly/y9vAT), which
    includes a `FilesPipeline` for dealing with general files.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松编写自己的管道，以获取抓取的图像 URL，从维基百科请求并保存到磁盘，但要正确执行则需要一些小心。例如，我们希望避免重新下载最近下载过或在此期间未更改的图像。指定存储图像位置的灵活性是一个有用的功能。此外，最好有将图像转换为常见格式（例如
    JPG 或 PNG）或生成缩略图的选项。幸运的是，Scrapy 提供了一个 `ImagesPipeline` 对象，具备所有这些功能及更多。这是其 [媒体管道](https://oreil.ly/y9vAT)
    之一，还包括用于处理一般文件的 `FilesPipeline`。
- en: We could add the image and biography-text scraping to our existing `nwinners_full`
    spider, but that’s starting to get a little large, and segregating this character
    data from the more formal categories makes sense. So we’ll create a new spider
    called `nwinners_minibio` that will reuse parts of the previous spider’s `parse`
    method in order to loop through the Nobel winners.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将图像和传记文本抓取添加到现有的 `nwinners_full` 爬虫中，但这开始变得有点庞大，从更正式的类别中分离此字符数据是有意义的。因此，我们将创建一个名为
    `nwinners_minibio` 的新爬虫，它将重用前一个爬虫的 `parse` 方法的部分，以便循环遍历诺贝尔获奖者。
- en: As usual, when creating a Scrapy spider, our first job is to get the xpaths
    for our scraping targets—​in this case, where available that’s the first part
    of the winners’ biographical text and a photograph of them. To do this, we fire
    up Chrome Elements and explore the HTML source of the biography pages looking
    for the targets shown in [Figure 6-8](#scrapy_crick).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，创建 Scrapy 爬虫时，我们的第一步是获取我们的抓取目标的 XPath——在这种情况下，如果有的话，这是获奖者传记文本的第一部分和他们的照片。为此，我们启动
    Chrome Elements 并探索传记页面的 HTML 源代码，查找在 [Figure 6-8](#scrapy_crick) 中显示的目标。
- en: '![dpj2 0608](assets/dpj2_0608.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0608](assets/dpj2_0608.png)'
- en: 'Figure 6-8\. The target elements for our biography scraping: the first part
    of the biography (A) marked by a stop point (B), and the winner’s photograph (C)'
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. 我们传记抓取的目标元素：传记的第一部分（A）由一个停止点（B）标记，以及获奖者的照片（C）
- en: Example 6-6\. Scraping the biographical text
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-6\. 抓取传记文本
- en: '[PRE38]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Investigating with Chrome Elements (see [Example 6-6](#scrapy_bio_paras)) shows
    the biographical text ([Figure 6-8](#scrapy_crick) A) is contained in child paragraphs
    of the `div` with class `mw-parser-output`, which is a child of the `div` with
    ID `mw-content-text`. The paragraphs are sandwiched between a `table` with class
    `infobox` and a table-of-contents `div` with ID `toc`. We can use the xpath `following-sibling`
    and `preceding-sibling` operators to craft a selector that captures the target
    paragraphs:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Chrome Elements 进行调查（参见 [Example 6-6](#scrapy_bio_paras)）显示，传记文本（[Figure 6-8](#scrapy_crick)
    A）包含在 `mw-parser-output` 类的子段落中，后者是具有 ID `mw-content-text` 的 `div` 的子 `div`。这些段落夹在一个具有类
    `infobox` 的 `table` 和具有 ID `toc` 的目录 `div` 之间。我们可以使用 `following-sibling` 和 `preceding-sibling`
    运算符来创建一个选择器，以捕获目标段落：
- en: '[PRE39]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO7-1)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO7-1)'
- en: All paragraphs following the first table in the child `div` of the `div` with
    ID `mw-content-text`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 `div` 的第一个表格之后的所有段落，它是 `mw-content-text` ID 的子 `div` 中的一个。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO7-2)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO7-2)'
- en: Exclude (not) all paragraphs that have a preceding sibling `div` with ID `toc`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 排除（不包括）所有具有前置兄弟 `div` 和 ID `toc` 的段落。
- en: Testing this with the Scrapy shell shows it consistently captures the Nobel
    winners’ mini-bios.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 用 Scrapy shell 测试后，它始终捕获了诺贝尔获奖者的小传。
- en: 'Further exploration of the winners’ pages shows that their photos ([Figure 6-8](#scrapy_crick)
    C) are contained in a table of class `infobox` and are the only image tags (`<img>`)
    in that table:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步探索获奖者页面显示，他们的照片（[Figure 6-8](#scrapy_crick) C）位于 `infobox` 类的表格中，并且是该表格中唯一的图像标签（`<img>`）：
- en: '[PRE40]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The xpath `'//table[contains(@class,"infobox")]//img/@src` will get the source
    address of the image.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: xpath `'//table[contains(@class,"infobox")]//img/@src'` 将获取图像的源地址。
- en: 'As with our first spider, we first need to declare a Scrapy `Item` to hold
    our scraped data. We’ll scrape the bio link and name of the winner, which we can
    use as identifiers for the image and text. We also need somewhere to store our
    `image-urls` (though we will only scrape one bio image, I’ll cover the multiple-image
    use case), the resultant images references (a file path), and a `bio_image` field
    to store the particular image we’re interested in:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的第一个蜘蛛一样，我们首先需要声明一个 Scrapy `Item` 来保存我们爬取的数据。我们将爬取获奖者的传记链接和姓名，这些可以作为图像和文本的标识符使用。我们还需要一个地方来存储我们的
    `image-urls`（尽管我们只会爬取一个生物图像，我将涵盖多图像使用情况），结果图像引用（文件路径），以及一个 `bio_image` 字段来存储我们感兴趣的特定图像：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we reuse the scraping loop over our Nobel Prize winners (see [Example 6-4](#scrapy_yield)
    for details), this time yielding a request to our new `get_mini_bio` method, which
    will scrape the image URLs and bio text:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们重复使用我们的诺贝尔奖获得者爬取循环（详见 [Example 6-4](#scrapy_yield)），这次生成请求到我们的新 `get_mini_bio`
    方法，它将爬取图像 URL 和传记文本：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Our `get_mini_bio` method will add any available photo URLs to the `image_urls`
    list and add all paragraphs of the biography up to the `<p></p>` stop point to
    the item’s `mini_bio` field:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `get_mini_bio` 方法将为 `image_urls` 列表添加任何可用的照片 URL，并将传记的所有段落添加到项目的 `mini_bio`
    字段，直到 `<p></p>` 结束点：
- en: '[PRE43]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO8-1)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO8-1)'
- en: Targets the first (and only) image in the table of class `infobox` and gets
    its source (`src`) attribute (e.g., `<img src='//upload.wikime⁠dia.org/​.../Max_Perutz.jpg'...`).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 定位到 `infobox` 类中的第一个（也是唯一的）图像，并获取其源（`src`）属性（例如 `<img src='//upload.wikime⁠dia.org/​.../Max_Perutz.jpg'...`）。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO8-2)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO8-2)'
- en: Grab our mini-bio paragraphs in a sibling sandwich.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取我们的迷你传记段落在兄弟夹心中。
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO8-3)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO8-3)'
- en: Replaces Wikipedia’s internal hrefs (e.g., */wiki/…​*) with the full addresses
    our visualization will need.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 替换维基百科内部的 href（例如 */wiki/…​*）为我们的可视化所需的完整地址。
- en: With our bio-scraping spider defined, we need to create its complementary pipeline,
    which will take the image URLs scraped and convert them into saved images. We’ll
    use Scrapy’s [images pipeline](https://oreil.ly/MqUuX) for this job.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了我们的生物爬虫后，我们需要创建其相应的管道，用于将爬取的图像 URL 转换为保存的图像。我们将使用 Scrapy 的 [images pipeline](https://oreil.ly/MqUuX)
    完成这项任务。
- en: The `ImagesPipeline` shown in [Example 6-7](#scrapy_images_pipeline) has two
    main methods, `get_media_requests`, which generates the requests for the image
    URLs, and `item_completed`, called after the requests have been consumed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[Example 6-7](#scrapy_images_pipeline) 中展示的 `ImagesPipeline` 有两个主要方法，`get_media_requests`
    用于生成图像 URL 的请求，以及 `item_completed` 在请求消耗后调用。'
- en: Example 6-7\. Scraping images with the image pipeline
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-7\. 使用图像管道爬取图像
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO9-1)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO9-1)'
- en: This takes any image URLs scraped by our *nwinners_minibio* spider and generates
    an HTTP request for their content.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作获取由我们的 *nwinners_minibio* 蜘蛛爬取的任何图像 URL，并生成其内容的 HTTP 请求。
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO9-2)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO9-2)'
- en: After the image URL requests have been made, the results are delivered to the
    `item_completed` method.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 URL 请求完成后，结果将传递给 `item_completed` 方法。
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO9-3)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO9-3)'
- en: This Python list comprehension filters the list of result tuples (of form `[(True,
    Image), (False, Image) …​]`) for those that were successful and stores their file
    paths relative to the directory specified by the `IMAGES_STORE` variable in `settings.py`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此 Python 列表推导式用于过滤结果元组的列表（形如 `[(True, Image), (False, Image) …​]`），筛选出成功的并将其文件路径存储在
    `settings.py` 中 `IMAGES_STORE` 变量指定的目录相对路径下。
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO9-4)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO9-4)'
- en: We use a Scrapy [item adapter](https://oreil.ly/8P6uq), which provides a common
    interface for working with supported item types.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个 Scrapy [item adapter](https://oreil.ly/8P6uq)，它为支持的项目类型提供了一个公共接口。
- en: 'Now that we have the spider and pipeline defined, we just need to add the pipeline
    to our `settings.py` module and set the `IMAGES_STORE` variable to the directory
    we want to save the images in:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了爬虫和管道，我们只需要将管道添加到我们的 `settings.py` 模块，并将 `IMAGES_STORE` 变量设置为我们想要保存图像的目录：
- en: '[PRE45]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s run our new spider from the *nobel_winners* root directory of our project,
    and check its output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们项目的 *nobel_winners* 根目录运行我们的新爬虫，并检查其输出：
- en: '[PRE46]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The spider is correctly harvesting mini-bios and, using its image pipeline,
    photos of the Nobel winners. The image was stored in `image_urls` and successfully
    processed, loading the JPG file stored in the *images* directory we specified
    with `IMAGE_STORE` with a relative path (`full/a5f763b828006e704cb291411b8b643bfb1886c.jpg`).
    The filename is, conveniently enough, a [SHA1 hash](https://oreil.ly/SlSl2) of
    the image’s URL, which allows the image pipeline to check for existing images,
    enabling it to prevent redundant requests.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 正确地收集了迷你传记和诺贝尔获奖者的照片，并利用其图像管道进行处理。图像存储在 `image_urls` 中，并成功处理，加载了存储在我们指定的 `IMAGE_STORE`
    目录中的 JPG 文件的相对路径（`full/a5f763b828006e704cb291411b8b643bfb1886c.jpg`）。恰好，文件名是图像
    URL 的 [SHA1 哈希](https://oreil.ly/SlSl2)，这使得图像管道能够检查现有的图像，并防止冗余请求。
- en: 'A quick listing of our images directory shows a nice array of Wikipedia Nobel
    Prize winner images, ready to be used in our web visualization:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图像目录的快速列表显示了一系列美观的维基百科诺贝尔奖获得者图像，已准备好在我们的网络可视化中使用：
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As we’ll see in [Chapter 16](ch16.xhtml#chapter_building_viz), we will be placing
    these in the *static* folder of our web app, ready to be accessed via the winner’s
    `bio_image` field.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在 [Chapter 16](ch16.xhtml#chapter_building_viz) 中看到的那样，我们将把它们放在我们 web 应用的
    *static* 文件夹中，以便通过获奖者的 `bio_image` 字段访问。
- en: With our images and biography text to hand, we’ve successfully scraped all the
    targets we set ourselves at the beginning of the chapter (see [Example 6-1](#scrapy_target_JSON)
    and [Figure 6-1](#scrapy_targets)). Now, it’s time for a quick summary before
    moving on to clean this inevitably dirty data with help from pandas.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们手头的图像和传记文本，我们已经成功地完成了本章初时设定的所有目标（参见 [Example 6-1](#scrapy_target_JSON) 和
    [Figure 6-1](#scrapy_targets)）。现在，在使用 pandas 的帮助下清理这些不可避免的脏数据之前，我们来进行一个快速总结。
- en: Specifying Pipelines with Multiple Spiders
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定多个爬虫的管道
- en: 'The pipelines enabled in `settings.py` are applied to all spiders in our Scrapy
    project. Often, if you have a number of spiders, you’ll want to be able to specify
    which pipelines are applied on a spider-by-spider basis. There are a [number of
    ways](https://oreil.ly/62Uzn) to achieve this, but the best I’ve seen is to use
    the spiders’ `custom_settings` class property to set the `ITEM_PIPELINES` dictionary
    instead of setting it in `settings.py`. In the case of our `nwinners_minibio`
    spider, this means adapting the `NWinnerSpiderBio` class like so:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Scrapy 项目中启用的管道适用于所有爬虫。通常，如果你有多个爬虫，你可能希望能够指定哪些管道适用于每个爬虫。有几种方法可以实现这一点，但我见过的最好的方法是使用爬虫的
    `custom_settings` 类属性来设置 `ITEM_PIPELINES` 字典，而不是在 `settings.py` 中设置它。对于我们的 `nwinners_minibio`
    爬虫来说，这意味着像这样调整 `NWinnerSpiderBio` 类：
- en: '[PRE48]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now the `NobelImagesPipeline` pipeline will only be applied while scraping the
    Nobel Prize winners’ biographies.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `NobelImagesPipeline` 管道只会在爬取诺贝尔奖获得者传记时应用。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we produced two Scrapy spiders that managed to grab the simple
    statistical dataset of our Nobel Prize winners plus some biographical text (and,
    where available, a photograph, to add some color to the stats). Scrapy is a powerful
    library that takes care of everything you could need in a full-fledged scraper.
    Although the workflow requires more effort to implement than doing some hacking
    with Beautiful Soup, Scrapy has far more power and comes into its own as your
    scraping ambitions increase. All Scrapy spiders follow the standard recipe demonstrated
    here, and the workflow should become routine after you program a few.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们创建了两个 Scrapy 爬虫，成功抓取了我们的诺贝尔奖获得者的简单统计数据，以及一些传记文本（如果有的话，还有一张照片，为数据增添一些色彩）。Scrapy
    是一个功能强大的库，它能够处理你在一个完整的爬虫中可能需要的一切。虽然使用 Scrapy 的工作流比用 Beautiful Soup 进行一些简单的操作需要更多的努力来实现，但是随着你的爬取需求增加，Scrapy
    的强大之处也会显现出来。所有的 Scrapy 爬虫都遵循这里演示的标准流程，并且在编写了几个爬虫后，这种工作流程应该会变得日常化。
- en: I hope this chapter has conveyed the rather hacky, iterative nature of scraping,
    and some of the quiet satisfaction that can be had when producing relatively clean
    data from the unpromising mound of stuff so often found on the web. The fact is
    that now and for the foreseeable future, the large majority of interesting data
    (the fuel for the art and science of data visualization) is trapped in a form
    that is unusable for the web-based visualizations that this book focuses on. Scraping
    is, in this sense, an emancipating endeavor.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本章传达了网页抓取的相当“hacky”、迭代性质，以及在从网上常见的混乱数据中生成相对干净数据时所能获得的某些安静满足感。事实上，现在和可预见的未来，大多数有趣的数据（数据可视化艺术和科学的燃料）都困在一种对于本书关注的基于网络的可视化来说是不可用的形式中。从这个意义上说，网页抓取是一种解放性的努力。
- en: The data we scraped, much of it human-edited, will certainly have some errors—​from
    badly formatted dates to categorical anomalies to missing fields. Making that
    data presentable is the focus of the next pandas-based chapters. But first, we
    need a little introduction to pandas and its building block, NumPy.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们抓取的数据，其中大部分是人工编辑的，肯定会有一些错误——从格式不正确的日期到分类异常和缺失字段。下一章基于 pandas 的重点是使数据变得可呈现。但首先，我们需要简要介绍一下
    pandas 及其构建模块 NumPy。
- en: ^([1](ch06.xhtml#idm45607786166832-marker)) See [the Scrapy install docs](https://oreil.ly/LamAt)
    for platform-specific details.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#idm45607786166832-marker)) 查看 [Scrapy 安装文档](https://oreil.ly/LamAt)
    获取特定平台的详细信息。
- en: ^([2](ch06.xhtml#idm45607785183776-marker)) There are some handy online tools
    for testing regexes, some of them programming-language-specific. [Pyregex](http://www.pyregex.com)
    is a good Python one, with a handy cheat sheet included.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.xhtml#idm45607785183776-marker)) 有一些方便的在线工具可以用来测试正则表达式，其中一些是特定于编程语言的。[Pyregex](http://www.pyregex.com)
    是一个不错的 Python 工具，包含一个方便的速查表。
- en: ^([3](ch06.xhtml#idm45607784791328-marker)) The author got stung by this removal.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.xhtml#idm45607784791328-marker)) 作者被这次删除搞得很烦。
- en: ^([4](ch06.xhtml#idm45607784789392-marker)) See [Wikipedia](https://oreil.ly/pLVcE)
    for an explanation.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.xhtml#idm45607784789392-marker)) 查看 [Wikipedia](https://oreil.ly/pLVcE)
    获取解释。
- en: ^([5](ch06.xhtml#idm45607784684416-marker)) Strictly speaking, there are edits
    being made continually by the Wikipedia community, but the fundamental details
    should be stable until the next set of prizes.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.xhtml#idm45607784684416-marker)) 严格来说，维基百科社区不断进行编辑，但基本细节应该稳定直到下一批奖项。
- en: ^([6](ch06.xhtml#idm45607784643296-marker)) See [Jeff Knupp’s blog, “Everything
    I Know About Python”](https://oreil.ly/qgku4), for a nice rundown of Python generators
    and the use of `yield`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.xhtml#idm45607784643296-marker)) 参阅 [Jeff Knupp 的博客，“Everything I
    Know About Python”](https://oreil.ly/qgku4)，了解 Python 生成器和 `yield` 的使用。
