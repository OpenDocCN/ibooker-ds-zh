- en: 6 How to be effective with AI as a Service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 如何有效地使用AI即服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Structuring a serverless project for rapid and effective development
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为快速和有效开发构建无服务器项目结构
- en: Building a serverless continuous deployment pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建无服务器持续交付管道
- en: Achieving observability with centralized, structured logs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过集中式、结构化日志实现可观察性
- en: Monitoring serverless project metrics in production
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控生产中的无服务器项目指标
- en: Understanding application behavior through distributed tracing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分布式跟踪理解应用程序行为
- en: So far, we have built some very compelling AI-based serverless applications.
    With very little code, these systems have an extraordinary amount of capability.
    You might have observed, however, that our serverless AI applications have many
    moving parts. We have adhered to the single responsibility principle, ensuring
    that each application is composed of many small units, each with a dedicated purpose.
    This chapter is about *effective* AI as a Service. By this, we mean that we move
    beyond simple application prototypes to production-grade applications that are
    capable of serving real users. For this, we need to think not just about how to
    get the basics working, but also about when things might stop working.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了一些非常吸引人的基于AI的无服务器应用程序。这些系统具有非凡的功能，代码量却很少。然而，你可能已经注意到，我们的无服务器AI应用程序有很多组成部分。我们坚持单一职责原则，确保每个应用程序由许多小型单元组成，每个单元都有其特定的目的。本章是关于*有效*的AI即服务。这意味着我们不仅超越了简单的应用原型，还开发出了能够为真实用户服务的生产级应用程序。为此，我们需要考虑的不仅仅是如何让基础知识工作，还要考虑何时可能会出现问题。
- en: We have been clear about the advantages of small units of code and off-the-shelf,
    managed services. Let’s take a step back and think about the pros and cons of
    this approach from the perspective of architects and developers moving from more
    traditional software development.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经清楚地阐述了小型代码单元和现成、托管服务的优势。让我们退一步，从建筑师和开发人员从更传统的软件开发方式转变的角度来思考这种方法的优缺点。
- en: We will outline how the primary challenges relate to structuring, monitoring,
    and deploying your application in ways that ensure you continue to deliver quickly
    without compromising on quality and reliability. This includes having a clear
    project layout, a working continuous delivery pipeline, and the ability to quickly
    gain insight into the application’s behavior when things go wrong.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将概述主要挑战如何与结构化、监控和部署应用程序相关，以确保您在保证质量和可靠性的同时继续快速交付。这包括拥有清晰的项目布局、一个有效的持续交付管道，以及在出现问题时能够快速了解应用程序行为的能力。
- en: This chapter will present practical solutions for overcoming each challenge
    and help you to establish effective serverless development practices.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将提供克服每个挑战的实际解决方案，并帮助您建立有效的无服务器开发实践。
- en: 6.1 Addressing the new challenges of Serverless
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 应对无服务器的全新挑战
- en: Given our success with deploying great serverless AI applications so far in
    this book, it’s easy to be deceived and think it will always be smooth sailing!
    As with any way of developing software, there are drawbacks and pitfalls to be
    aware of. Often, you don’t encounter these until you have built and brought systems
    into production. To help you foresee potential issues and solve problems in advance,
    we will list the benefits and challenges of serverless development. Then, we will
    present a template project that you can use as a basis for your own private projects.
    The aim is to save you the time and frustration that might be spent stumbling
    over these issues as they arise.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书中我们已经成功部署了出色的无服务器AI应用程序，很容易被误导，认为一切都会一帆风顺！就像任何软件开发方式一样，都有其缺点和需要注意的陷阱。通常，这些问题只有在构建并将系统投入生产后才会遇到。为了帮助您预见潜在问题并在问题出现之前解决它们，我们将列出无服务器开发的优缺点。然后，我们将展示一个模板项目，您可以用它作为自己私有项目的起点。目的是节省您在这些问题出现时可能花费的时间和挫折。
- en: 6.1.1 Benefits and challenges of Serverless
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 无服务器的优势和挑战
- en: Table 6.1 lists the primary benefits and challenges of developing serverless
    applications using managed AI services.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1列出了使用托管AI服务开发无服务器应用程序的主要优势和挑战。
- en: Table 6.1 The benefits and challenges of Serverless
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 无服务器应用程序的优势和挑战
- en: '| Benefits | Challenges |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 挑战 |'
- en: '| On-demand computing allows you to get started and scale quickly with no infrastructure
    to manage. | You rely on the cloud vendor’s environment to accurately run your
    code. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 按需计算允许你快速开始并扩展，无需管理任何基础设施。 | 你依赖于云服务提供商的环境来准确运行你的代码。 |'
- en: '| Smaller units of deployment allow you to adhere to a single-responsibility
    principle. These units are fast to develop and relatively easy to maintain, since
    they have a clear purpose and interface. The teams maintaining such components
    do not have to consider the subtle details of the rest of the system. | There
    is a significant learning curve to becoming truly serverless. It takes time to
    understand effective serverless architecture, learn the available managed services,
    and establish an effective project structure. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 较小的部署单元允许你遵守单一责任原则。这些单元开发速度快，维护相对容易，因为它们有明确的目的和接口。维护这些组件的团队不需要考虑整个系统的微妙细节。
    | 要真正实现无服务器，有一个显著的学习曲线。理解有效的无服务器架构、学习可用的管理服务以及建立有效的项目结构都需要时间。 |'
- en: '| Managed services for computation, communication, storage, and machine learning
    give you a huge leap in capability with minimal design and programming effort.
    At the same time, you are relieved of the maintenance and infrastructure burden
    you would have if you had to build this capability in your own organization. |
    The distributed and fragmented nature of a serverless-microservice architecture
    makes it harder to visualize or reason about the behavior of the system as a whole.
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 管理计算、通信、存储和机器学习的服务，只需最小的设计和编程努力，就能大幅提升你的能力。同时，你将不再需要承担如果要在自己的组织中构建这种能力所必须承担的维护和基础设施负担。
    | 无服务器-微服务架构的分布式和碎片化特性使得整体系统行为的可视化和推理变得更加困难。 |'
- en: '| In serverless systems, you pay only for what you use, eliminating waste and
    allowing you to scale in line with business success. | Though serverless reduces
    the number of systems you need to consider in your security responsibility, it
    is quite different from a traditional approach. For instance, a malicious attack
    gaining access to an AWS Lambda execution environment using an over-privileged
    IAM policy might allow the attacker to access your resources and data as well
    as consume potentially unlimited AWS resources, like more Lambda executions, EC2
    instances, or databases. It could incur a significant bill from your cloud vendor.
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 在无服务器系统中，你只需为所使用的部分付费，消除了浪费，并允许你根据业务成功进行扩展。 | 虽然无服务器减少了你需要考虑的安全责任中的系统数量，但它与传统方法相当不同。例如，一个恶意攻击者通过使用过权限的
    IAM 策略访问 AWS Lambda 执行环境，可能会允许攻击者访问你的资源和数据，以及消耗可能无限的 AWS 资源，如更多的 Lambda 执行、EC2
    实例或数据库。这可能会从你的云服务提供商那里产生一笔巨额账单。 |'
- en: '| A serverless approach allows you to select multiple managed database services,
    ensuring the right tool for any job. This “polyglot persistence” is quite different
    from past experiences of trying to pick one database for the majority of cases,
    resulting in a heavy maintenance burden and a poor fit for some data access requirements.
    | Dealing with multiple databases can be a challenge when your team is required
    to have the skills and understanding to use them correctly. Though it is easy
    to get started with something like DynamoDB, managing changes and ensuring optimal
    performance is a new skill that must be acquired through study and experience.
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 无服务器方法允许你选择多个管理数据库服务，确保任何工作都有合适的工具。这种“多语言持久性”与过去尝试为大多数情况选择一个数据库的经验截然不同，这导致了沉重的维护负担，并且不适合某些数据访问需求。
    | 当你的团队需要具备正确使用它们的技能和理解时，处理多个数据库可能是一个挑战。虽然像 DynamoDB 这样的服务容易上手，但管理变更和确保最佳性能是一项必须通过学习和经验获得的新技能。
    |'
- en: '| Serverless projects are cheap to create, so they can be recreated many times
    for different environments. | Dynamically-created cloud resources are typically
    given generated names. Allowing services to be discovered by other components
    is something that must be addressed to ensure the right balance of loose coupling,
    service availability, and ease of deployment. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 无服务器项目创建成本低廉，因此可以针对不同的环境多次重建。 | 动态创建的云资源通常会被赋予生成的名称。允许服务被其他组件发现是确保松散耦合、服务可用性和部署便捷性之间平衡必须解决的问题。
    |'
- en: These challenges and benefits are presented to give a clear and honest picture
    of the reality of serverless software in production. Now that you are aware of
    the pitfalls as well as the potential gains, we are ready to discuss how to avoid
    the pitfalls and maximise the effectiveness of your projects. We will do this
    with the help of a reference project that comes with many solutions to these problems
    out of the box.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战和好处被提出，以提供一个清晰和诚实的无服务器软件在生产中的现实情况。现在，你已经意识到了陷阱以及潜在的收益，我们准备讨论如何避免陷阱并最大化你项目的效果。我们将借助一个包含许多现成解决方案的参考项目来完成这项工作。
- en: 6.1.2 A production-grade serverless template
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 一个生产级无服务器模板
- en: The authors of this book have spent plenty of time building serverless applications
    and experiencing all the benefits and challenges. As a result, we have built up
    a set of best practices. We decided to put all of those practices into a template
    that we can use to start new serverless projects extremely rapidly. We also made
    the decision to open source this project and make it available to anyone building
    production-grade serverless applications. It is intended as a learning resource,
    and allows us to gather ideas and feedback from a much wider community.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书作者在构建无服务器应用程序和体验所有好处与挑战方面投入了大量的时间。因此，我们建立了一套最佳实践。我们决定将这些实践整合到一个模板中，以便我们可以用它来快速启动新的无服务器项目。我们还决定开源这个项目，使其对任何构建生产级无服务器应用程序的人开放。它旨在作为一个学习资源，并允许我们从更广泛的社区中收集想法和反馈。
- en: The project, called *SLIC Starter*, is free to use and open to contributions.
    SLIC stands for Serverless, Lean, Intelligent, and Continuous. You can find it
    on GitHub at [https://github.com/fourTheorem/slic-starter](https://github.com/fourTheorem/slic-starter).
    Creating production-ready serverless applications from scratch can be daunting.
    There are many choices and decisions to be made. SLIC Starter is intended to answer
    80% of those questions so we can start building meaningful business functionality
    as quickly as possible. The areas where decisions need to be made are shown in
    figure 6.1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个名为 *SLIC Starter* 的项目免费使用，并欢迎贡献。SLIC 代表无服务器、精益、智能和持续。你可以在 GitHub 上找到它：[https://github.com/fourTheorem/slic-starter](https://github.com/fourTheorem/slic-starter)。从头开始创建生产级无服务器应用程序可能会令人望而却步。有许多选择和决策需要做出。SLIC
    Starter 的目的是回答 80% 的这些问题，以便我们能够尽可能快地开始构建有意义的业务功能。需要做出决策的领域在图 6.1 中显示。
- en: '![](../Images/CH06_F01_Elger.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F01_Elger.png)'
- en: Figure 6.1 Aspects of a serverless project requiring decision-making. SLIC Starter
    aims to provide a template for each of these topics so adopters are freed up and
    get to production faster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 需要做出决策的无服务器项目方面。SLIC Starter 旨在为这些主题中的每一个提供模板，以便采用者能够更快地进入生产阶段。
- en: 'SLIC Starter is a template that can be applied to any application within any
    industry. It comes with a sample application for managing checklists. The application,
    called *SLIC Lists*, is deliberately simple, but has enough requirements to allow
    us to apply many serverless best practices. Once you have become familiar with
    SLIC Starter, you can replace the SLIC Lists application with the features for
    your own application. The sample SLIC Lists application has the following capabilities:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SLIC Starter 是一个模板，可以应用于任何行业中的任何应用程序。它附带一个用于管理清单的示例应用程序。这个名为 *SLIC Lists* 的应用程序故意设计得简单，但具有足够的需求，使我们能够应用许多无服务器最佳实践。一旦你熟悉了
    SLIC Starter，你可以用你自己的应用程序功能替换 SLIC Lists 应用程序。示例 SLIC Lists 应用程序具有以下功能：
- en: Users can sign up and log in.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户可以注册和登录。
- en: Users can create, edit, and delete checklists.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户可以创建、编辑和删除清单。
- en: Users can create entries in the checklist and mark them as done.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户可以在清单中创建条目并将它们标记为已完成。
- en: Any checklist can be shared with another user by providing their email address.
    The recipient must accept the invitation and log in or create an account to view
    and edit the list.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何清单都可以通过提供他们的电子邮件地址与其他用户共享。收件人必须接受邀请并登录或创建账户以查看和编辑列表。
- en: When a user creates a checklist, they are sent a “welcome email” to notify them
    that they have created the list.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当用户创建清单时，他们会收到一封“欢迎邮件”，通知他们已创建该列表。
- en: 'The components of our system are shown in figure 6.2\. The primary components
    or *services* shown are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统的组件在图 6.2 中展示。显示的主要组件或 *服务* 如下：
- en: The *checklist service* is responsible for storing and retrieving lists and
    their entries. It is backed by a database and provides a public API to authorized
    users.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*清单服务* 负责存储和检索列表及其条目。它由数据库支持，并为授权用户提供公共API。'
- en: '![](../Images/CH06_F02_Elger.png)'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/CH06_F02_Elger.png)'
- en: Figure 6.2 SLIC Starter services for the SLIC Lists application. The application
    is composed of five back-end services. There is also a front-end component as
    well as additional services to deal with certificates and domains.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.2 SLIC列表应用的SLIC启动服务。该应用由五个后端服务组成。还有一个前端组件，以及处理证书和域的附加服务。
- en: The *email service* is responsible for sending emails. Emails are passed to
    this service through an inbound queue.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*邮件服务* 负责发送邮件。邮件通过入站队列传递到这项服务。'
- en: The *user service* manages users and accounts. It also provides an internal
    API for access to user data.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户服务* 管理用户和账户。它还提供了一个内部API，用于访问用户数据。'
- en: The *welcome service* sends welcome notification messages to users when they
    create a checklist.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欢迎服务* 在用户创建清单时向用户发送欢迎通知消息。'
- en: The *sharing service* handles invitations to share lists with new collaborators.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*共享服务* 处理向新合作者发送共享列表的邀请。'
- en: The *front end* handles the front-end web application build, deployment, and
    distribution. It is linked by configuration to the public, back-end services.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前端* 负责前端Web应用的构建、部署和分发。它通过配置与公共、后端服务相连。'
- en: In addition, we have supporting services for certificate deployment and creating
    a public-facing API domain.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还提供支持证书部署和创建面向公众的API域的服务。
- en: What this application does is unlikely to be relevant to your application, but
    *how* this application is built should be very relevant. Figure 6.1 already illustrated
    the foundational considerations that you will eventually need to consider as you
    build a mature, production-grade software application. The checklist application
    provides a template for each of these considerations, and acts as a learning resource,
    helping you to address the challenges without taking too much time to stop and
    perform research into all possible solutions. The first consideration we start
    with is how you structure the project codebase and repository.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用所做的事情可能与你自己的应用不太相关，但*如何*构建这个应用应该非常相关。图6.1已经展示了你在构建成熟、生产级软件应用时最终需要考虑的基础性考虑因素。清单应用为每个这些考虑因素提供了一个模板，并作为学习资源，帮助你应对挑战，而无需花费太多时间停下来研究所有可能的解决方案。我们首先考虑的是如何结构化项目代码库和仓库。
- en: 6.2 Establishing a project structure
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 建立项目结构
- en: It’s a good idea to establish clear practices for project and source repository
    structure before the project scales quickly. If you don’t do this, it becomes
    confusing for team members to make changes and add new features, particularly
    when new team members join a project. There are many options here, but we want
    to optimize for rapid, efficient development in a collaborative environment where
    many developers are working together to build, deploy, and run new features and
    modifications.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目快速扩展之前，建立清晰的项目和源代码库结构是一个好主意。如果你不这样做，团队成员在做出更改和添加新功能时可能会感到困惑，尤其是在新成员加入项目时。这里有许多选择，但我们希望优化在许多开发者共同构建、部署和运行新功能和修改的协作环境中的快速、高效开发。
- en: 6.2.1 The source repository--monorepo or polyrepo
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 源代码库--单代码库或多代码库
- en: The way you organise your teams' code seems like a trivial topic. But as we
    have discovered after many projects, simple decisions on how this is done have
    a big impact on how quickly you can make changes and get them released, and how
    well developers can communicate and collaborate. A big part of this is whether
    you go for polyrepo or monorepo. A *polyrepo* is when multiple source control
    repositories are used for each service, component, or module within an application.
    In a microservices project with multiple front ends (web, mobile, and so on),
    this can result in hundreds or thousands of repositories. A *monorepo* is when
    all services and front end codebases are kept in a single repository.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何组织团队的代码似乎是一个微不足道的话题。但正如我们在许多项目后发现的那样，关于如何进行简单决策会对你能够多快地做出更改并发布它们，以及开发者如何进行沟通和协作产生重大影响。这部分原因在于你是否选择多仓库或单仓库。*多仓库*是指在一个应用程序中，每个服务、组件或模块使用多个源代码控制仓库。在一个具有多个前端（如网页、移动端等）的微服务项目中，这可能导致数百或数千个仓库。*单仓库*是指所有服务和前端代码库都保存在单个仓库中。
- en: Google, Facebook, and Twitter are well known for using a monorepo at ridiculously
    large scale. Of course, it’s never a good idea to go with an approach just because
    Google/Facebook/Twitter said so. Instead, as with everything, measure how this
    impacts you and make the decision that works well for your organization. Figure
    6.3 illustrates the difference between the two approaches.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌、Facebook 和推特因在极其大规模上使用单仓库而闻名。当然，仅仅因为谷歌/Facebook/推特这么说就采取某种方法从来不是一个好主意。相反，就像其他所有事情一样，衡量这种方法对你产生的影响，并做出对你的组织有利的决策。图
    6.3 展示了两种方法之间的差异。
- en: '![](../Images/CH06_F03_Elger.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F03_Elger.png)'
- en: Figure 6.3 Monorepo versus polyrepo. A monorepo includes multiple services,
    supporting libraries, and Infrastructure as Code (IaC) in one repository. A polyrepo
    favours a separate repository for each individual component.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 单仓库与多仓库对比。单仓库包含多个服务、支持库和基础设施即代码（IaC）在一个仓库中。多仓库倾向于为每个单独的组件使用单独的仓库。
- en: The polyrepo approach has certain benefits. For example, each module can be
    separately versioned and can have fine-grained access control. However, in our
    experience, too much time is spent managing the coordination across multiple repositories.
    The overhead can quickly get out of hand as you add more services, libraries,
    and dependencies. Often, polyrepos must be managed with custom tooling to manage
    cross-repository dependencies. A new developer should be able to start working
    on your product as quickly as possible. Avoid unnecessary ceremony and any learning
    curve that is unique to your team or company.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 多仓库（polyrepo）方法有一定的优势。例如，每个模块可以单独版本控制，并且可以拥有细粒度的访问控制。然而，根据我们的经验，在多个仓库之间进行协调管理会花费太多时间。随着更多服务、库和依赖项的增加，开销会迅速失控。通常，需要使用定制工具来管理跨仓库的依赖项。新开发者应该能够尽快开始为你的产品工作。避免不必要的仪式和任何仅适用于你团队或公司的独特学习曲线。
- en: With a monorepo, when a bug fix or feature affects multiple modules/microservices,
    all changes are made in the same repository. There is just one branch on a single
    repository. No more tracking across multiple repositories. Each feature gets a
    single pull request. There is no risk that the feature is going to be partially
    merged.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在单仓库（monorepo）模式下，当修复错误或添加功能影响多个模块/微服务时，所有更改都在同一个仓库中进行。单个仓库只有一个分支。不再需要在多个仓库之间进行跟踪。每个功能都有一个单独的拉取请求（pull
    request）。不会存在功能部分合并的风险。
- en: By sticking with a single repository, your external tests (end-to-end or API
    tests) also belong with the code under test. The same goes for Infrastructure
    as Code. Any changes required in your infrastructure are captured together with
    the application code. If you have common code, utilities, and libraries that are
    consumed by your microservices, keeping them in the same repository makes it quite
    easy to share.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过坚持使用单个仓库，你的外部测试（端到端或 API 测试）也属于测试代码的一部分。同样适用于基础设施即代码（Infrastructure as Code）。任何需要更改的基础设施都会与应用程序代码一起捕获。如果你有被微服务使用的通用代码、实用工具和库，将它们保存在同一个仓库中可以非常容易地进行共享。
- en: 6.2.2 Project folder structure
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 项目文件夹结构
- en: The SLIC Starter repository follows the monorepo approach. The application is
    laid out in a similar way to many of the applications we have already described
    in this book. Each service has its own folder containing a `serverless.yml`. The
    project folder structure in the SLIC Starter monorepo repository is shown in the
    next listing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SLIC Starter 仓库遵循单仓库（monorepo）方法。应用程序的布局与我们在这本书中已经描述的许多应用程序类似。每个服务都有自己的文件夹，包含一个
    `serverless.yml` 文件。SLIC Starter 单仓库仓库中的项目结构在下一列表中展示。
- en: Listing 6.1 SLIC Starter project structure
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 SLIC Starter 项目结构
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 6.2.3 Get the code
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 获取代码
- en: 'To explore this repository with its project structure and to prepare for the
    rest of this chapter, fetch the code from the SLIC Starter GitHub repository.
    If you want to build and deploy the application automatically later in the chapter,
    you will need this code to be in a repository you control. To achieve this, fork
    the SLIC Starter repository ([https://github.com/fourTheorem/slic-starter](https://github.com/fourTheorem/slic-starter))
    before cloning it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这个具有项目结构和为本章剩余部分做准备，请从 SLIC Starter GitHub 仓库获取代码。如果你想在章节的后面自动构建和部署应用程序，你需要将此代码放在你控制的仓库中。为了实现这一点，在克隆之前先对
    SLIC Starter 仓库进行分支（[https://github.com/fourTheorem/slic-starter](https://github.com/fourTheorem/slic-starter)）：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should now have a solid understanding of what it means to have an effective
    project structure. You also have access to a template project that exemplifies
    this structure. Our next consideration deals with automating deployment of the
    project components.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该对有效项目结构的意义有了稳固的理解。你也有权访问一个体现这种结构的模板项目。我们接下来的考虑是关于自动化部署项目组件。
- en: 6.3 Continuous deployment
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 持续部署
- en: So far, all of our serverless applications have been deployed manually. We have
    relied on the Serverless Framework’s `serverless` `deploy` command to deploy each
    service into a specific target environment. This is fine for early development
    and prototyping, especially when our applications are small. But when real users
    depend on our applications and feature development is expected to be frequent
    and rapid, manual deployment is far too slow and error-prone.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的无服务器应用程序都是手动部署的。我们依赖 Serverless Framework 的 `serverless` `deploy`
    命令将每个服务部署到特定的目标环境中。这对于早期开发和原型设计来说是可以的，特别是当我们的应用程序规模较小时。但是，当真实用户依赖于我们的应用程序，并且预期功能开发将频繁且快速时，手动部署就太慢且容易出错。
- en: Can you imagine manual deployment of your application when it is composed of
    hundreds of independently-deployable components? Real-world serverless applications
    are, by their nature, complex distributed systems. You can’t and shouldn’t rely
    on having a clear mental model of how they all fit together. Instead, you rely
    on the power of automation for deployment and testing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的应用程序由数百个独立可部署的组件组成时，你能想象手动部署应用程序的场景吗？现实世界的无服务器应用程序本质上都是复杂的分布式系统。你不能，也不应该依赖对它们如何组合在一起有一个清晰的思维模型。相反，你应该依靠自动化部署和测试的力量。
- en: Effective serverless applications require continuous deployment. *Continuous
    deployment* means that changes in our source code repository are automatically
    delivered to target production environments. When continuous deployment is triggered,
    any components affected by a code change are built and tested. There is also a
    system in place for integration testing our changed components as part of the
    entire system. A proper continuous deployment solution gives us confidence to
    make changes quickly. The principles of continuous deployment are equally valid
    for the deployment of data sets and machine learning models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的无服务器应用程序需要持续部署。*持续部署*意味着我们的源代码仓库中的更改会自动传递到目标生产环境中。当触发持续部署时，受代码更改影响的任何组件都会被构建和测试。还有一个系统用于集成测试我们更改的组件，作为整个系统的一部分。一个合适的持续部署解决方案让我们有信心快速做出更改。持续部署的原则同样适用于数据集和机器学习模型的部署。
- en: Let’s look at the design of a serverless continuous deployment system from a
    high level.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从高层次的角度来看一个无服务器持续部署系统的设计。
- en: 6.3.1 Continuous deployment design
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 持续部署设计
- en: We have already discussed how our approach for serverless applications favors
    source code stored in a monorepo. This has an impact on how the continuous deployment
    process is triggered. If each module or service were stored in its own individual
    repository, changes to that repository could trigger that service’s build. The
    challenge would then become how to coordinate builds across multiple repositories.
    For the monorepo approach, we want to avoid building everything when a small number
    of commits have been made, affecting one or two modules. Take a look at the high-level
    continuous deployment flow illustrated in figure 6.4.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了我们的服务器端应用程序方法如何倾向于存储在单仓库中的源代码。这影响了持续部署过程的触发方式。如果每个模块或服务都存储在其自己的独立仓库中，该仓库的更改可能会触发该服务的构建。那么挑战就变成了如何在多个仓库之间协调构建。对于单仓库方法，我们希望避免在只有少数提交影响一到两个模块时构建一切。请查看图6.4中展示的高级持续部署流程。
- en: 'The phases of the deployment pipeline are as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 部署管道的阶段如下：
- en: A change-detection job determines which modules are affected by source code
    commits.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个变更检测作业确定哪些模块受到源代码提交的影响。
- en: The pipeline then triggers parallel builds of each module. These build jobs
    will also run unit tests for the relevant modules.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道随后触发每个模块的并行构建。这些构建作业也将为相关模块运行单元测试。
- en: '![](../Images/CH06_F04_Elger.png)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/CH06_F04_Elger.png)'
- en: Figure 6.4 Our monorepo approach requires us to detect which modules have changed
    before triggering parallel build and unit test jobs for each affected module.
    Once that is successful, modules are deployed to a staging environment where integration
    tests can be run. Successful test execution triggers a deployment to production.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.4 我们的单仓库方法要求我们在触发每个受影响模块的并行构建和单元测试作业之前，检测哪些模块已更改。一旦成功，模块将被部署到预发布环境，在那里可以运行集成测试。成功的测试执行将触发向生产环境的部署。
- en: When all builds are successful, the modules are deployed to the staging environment.
    The staging environment is a replica of the production environment, not exposed
    to real users.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当所有构建都成功时，模块将被部署到预发布环境。预发布环境是生产环境的副本，不对真实用户公开。
- en: We run a set of automated, end-to-end tests that give confidence that the new
    changes do not break basic features in the system under predictable test conditions.
    Of course, breaking changes under less-predictable production conditions are always
    possible, and you should prepare for that.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行一系列自动化、端到端的测试，这让我们有信心在可预测的测试条件下，新的更改不会破坏系统中的基本功能。当然，在不可预测的生产条件下出现破坏性更改始终是可能的，你应该为此做好准备。
- en: If all tests are successful, the new modules are deployed to production.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果所有测试都成功，新模块将被部署到生产环境。
- en: In our pipeline, we assume two target environments--a staging environment for
    testing new changes before they go live, and a production environment for our
    end users. The staging environment is entirely optional. In fact, it is ideal
    to get changes into production as soon as possible and have effective measures
    in place to mitigate the risk. Such measures include the ability to roll back
    quickly, deployment patterns like blue/green or canary,[1](#pgfId-1101174) and
    good observability practices. Observability is covered later in this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的管道中，我们假设有两个目标环境——一个用于在上线前测试新更改的预发布环境，以及一个用于最终用户的生产环境。预发布环境完全是可选的。实际上，尽快将更改投入生产并采取有效措施来减轻风险是理想的。这些措施包括快速回滚的能力、蓝/绿或金丝雀部署模式[1](#pgfId-1101174)，以及良好的可观察性实践。可观察性将在本章后面讨论。
- en: Now that we have an understanding of the continuous deployment flow, let’s examine
    how we can implement it using managed cloud build services that are themselves
    serverless!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了持续部署流程，让我们来看看我们如何使用自身也是无服务器的托管云构建服务来实现它！
- en: 6.3.2 Implementing continuous deployment with AWS services
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 使用AWS服务实现持续部署
- en: There are many great options for hosting your continuous build and deployment
    environment. These include everything from the immortal Jenkins, to SaaS offerings
    such as CircleCI ([https://circleci.com](https://circleci.com)) and GitHub Actions
    ([https://github.com/features/actions](https://github.com/features/actions)).
    The choice depends on what is most efficient for you and your team. For this chapter,
    we will use AWS build services in keeping with the theme of picking cloud-managed
    services. The neat advantage of this approach is that we will be using the same
    Infrastructure-as-Code approach as our application itself. The continuous deployment
    pipeline will be built using CloudFormation and reside in the same monorepo as
    the other services in SLIC Starter.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于托管您的持续构建和部署环境，有许多优秀的选项。这些包括从不朽的Jenkins到SaaS服务，如CircleCI ([https://circleci.com](https://circleci.com))
    和GitHub Actions ([https://github.com/features/actions](https://github.com/features/actions))。选择取决于对你和你的团队来说什么最有效。对于本章，我们将使用AWS构建服务，以保持选择云托管服务的主题。这种方法的优点是，我们将使用与应用程序本身相同的Infrastructure-as-Code方法。持续部署管道将使用CloudFormation构建，并驻留在SLIC
    Starter中的其他服务相同的单仓库中。
- en: Multi-account and single-account deployment
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多账户和单账户部署
- en: SLIC Starter supports multi-account deployment out of the box. This allows us
    to use separate AWS accounts for our staging and production environments, affording
    us increased isolation and security. We can also use a separate “tooling” account
    where the continuous deployment pipeline and artifacts will reside. This approach
    takes time to set up, and creating multiple accounts may not be feasible for many
    users. For these reasons, a single-account deployment is also possible. This is
    the option we will present in this chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: SLIC Starter支持开箱即用的多账户部署。这允许我们为我们的预发布和生产环境使用单独的AWS账户，从而提供更高的隔离性和安全性。我们还可以使用一个单独的“工具”账户，其中将驻留持续部署管道和工件。这种方法需要时间来设置，并且对于许多用户来说，创建多个账户可能不可行。因此，单账户部署也是可能的。这是我们将在本章中介绍的选择。
- en: Building the continuous deployment pipeline
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建持续部署管道
- en: The AWS services we will use for the pipeline are AWS CodeBuild and AWS CodePipeline.
    CodeBuild allows us to perform build steps like install, compile, and test. A
    build artifact is usually produced as its output. CodePipeline allows us to combine
    multiple actions together into stages. Actions can include source fetching, CodeBuild
    executions, deployment, and manual approval steps. Actions can be run in sequence
    or in parallel.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为管道使用的AWS服务是AWS CodeBuild和AWS CodePipeline。CodeBuild允许我们执行构建步骤，如安装、编译和测试。通常会产生一个构建工件作为其输出。CodePipeline允许我们将多个操作组合成阶段。操作可以包括源获取、CodeBuild执行、部署和人工批准步骤。操作可以按顺序或并行运行。
- en: On each commit or merge to the `master` branch of our repository, we will build
    and deploy affected modules in parallel. To accomplish this, we will create a
    separate pipeline per module. These pipelines will be executed and monitored by
    a single, overall *orchestrator pipeline*. This can all be seen in figure 6.5.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的仓库的`master`分支上每次提交或合并时，我们将并行构建和部署受影响的模块。为了实现这一点，我们将为每个模块创建一个单独的管道。这些管道将由一个单一的、整体的*协调管道*执行和监控。所有这些都可以在图6.5中看到。
- en: '![](../Images/CH06_F05_Elger.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5](../Images/CH06_F05_Elger.png)'
- en: Figure 6.5 The canonical serverless CI/CD architecture is part of SLIC Starter.
    It uses a CodePipeline pipeline for each module. The execution of these pipelines
    in parallel is coordinated by an orchestrator pipeline. The build, deployment,
    and test phases are implemented as CodeBuild projects.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 SLIC Starter的典型无服务器CI/CD架构是其一部分。它使用每个模块的CodePipeline管道。这些管道的并行执行由协调管道协调。构建、部署和测试阶段作为CodeBuild项目实现。
- en: Since we are using AWS services for the build pipeline, we will deploy using
    CloudFormation stacks, just as with our serverless applications. So far, we have
    used the Serverless Framework to construct these stacks. For the deployment stacks,
    we will use the AWS Cloud Development Kit (CDK) instead.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在使用AWS服务构建管道，因此我们将使用CloudFormation堆栈进行部署，就像我们的无服务器应用程序一样。到目前为止，我们已经使用Serverless
    Framework构建了这些堆栈。对于部署堆栈，我们将使用AWS Cloud Development Kit (CDK)代替。
- en: CDK provides a programmatic way to construct CloudFormation templates. There
    are pros and cons to using standard programming languages for Infrastructure as
    Code. We prefer it, as it mirrors how we build the application itself but, for
    many people, infrastructure is better defined using a configuration language like
    JSON or YAML. In this case, it allows us to dynamically create projects and pipelines
    rather than rely on a static configuration. As we add new modules to the application,
    CDK will generate new resources automatically. CDK supports JavaScript, Python,
    Java, and TypeScript. We are using TypeScript, a superset of JavaScript that gives
    us type safety. Type safety is a powerful aid when creating resources with complex
    configuration syntax. It allows us to leverage auto-completion and get immediate
    documentation hints. Detailed coverage of CDK and TypeScript are beyond the scope
    of this book. If you are interested in exploring how the pipelines are built,
    explore the CDK TypeScript code in the `cicd` folder. We will jump straight in
    and deploy our CI/CD pipelines!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: CDK提供了一种程序化的方式来构建CloudFormation模板。使用标准编程语言进行基础设施即代码（IaC）有其优缺点。我们更喜欢这种方式，因为它与我们构建应用程序的方式相似，但对于许多人来说，使用JSON或YAML之类的配置语言来定义基础设施可能更好。在这种情况下，它允许我们动态创建项目和管道，而不是依赖于静态配置。随着我们向应用程序添加新的模块，CDK将自动生成新的资源。CDK支持JavaScript、Python、Java和TypeScript。我们正在使用TypeScript，它是JavaScript的超集，为我们提供了类型安全。类型安全在创建具有复杂配置语法的资源时是一种强大的辅助工具。它允许我们利用自动完成功能并获得即时的文档提示。CDK和TypeScript的详细覆盖超出了本书的范围。如果您对探索如何构建管道感兴趣，请探索`cicd`文件夹中的CDK
    TypeScript代码。我们将直接进入并部署我们的CI/CD管道！
- en: The latest documentation on deploying and running the CI/CD pipeline is in the
    `QUICK_START.md` document in the SLIC Starter repository. Once you have run all
    of the steps, your pipeline is ready. Every commit to the repository will trigger
    the source CodeBuild project and result in execution of the orchestrator pipeline.
    Figure 6.6 shows how this pipeline looks in the AWS CodePipeline Console.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和运行CI/CD管道的最新文档位于SLIC Starter存储库中的`QUICK_START.md`文档中。一旦您运行了所有步骤，您的管道就准备好了。对存储库的每次提交都将触发源CodeBuild项目，并导致编排管道的执行。图6.6显示了在AWS
    CodePipeline控制台中该管道的外观。
- en: Here, we can clearly see the steps of the pipeline that have run. The current
    execution is in the “Approval” stage. This is a special stage that requires the
    user to review and click Approve in order to advance the pipeline. This gives
    us the chance to check and cancel any production deployment. The execution shown
    has successfully deployed to staging, and our test jobs have completed successfully.
    In the SLIC Starter, automated API integration tests and user interface end-to-end
    (E2E) tests are run in parallel against the public API and front end.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以清楚地看到已经运行的管道步骤。当前的执行处于“审批”阶段。这是一个特殊的阶段，需要用户进行审查并点击“批准”才能推进管道。这给了我们检查和取消任何生产部署的机会。显示的执行已成功部署到预发布环境，并且我们的测试作业已成功完成。在SLIC
    Starter中，针对公共API和前端，并行运行自动化的API集成测试和用户界面端到端（E2E）测试。
- en: Once our system has been deployed to production, we need to understand what’s
    going on there. When things go wrong, we need to be able to troubleshoot and answer
    many questions about the state of the application. This brings us to *observability*,
    arguably the most important part of an effective production serverless deployment!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的系统已部署到生产环境，我们需要了解那里发生了什么。当事情出错时，我们需要能够进行故障排除并回答有关应用程序状态的许多问题。这让我们想到了*可观察性*，这可能是有效生产无服务器部署中最重要的一部分！
- en: 6.4 Observability and monitoring
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 可观察性和监控
- en: At the start of this chapter, one of the challenges we described was the fragmented
    nature of serverless systems. This is a common problem with distributed systems
    composed of many small parts. It can lead to a lack of understanding of the running
    behavior of the system, making it difficult to solve problems and make changes.
    The problem has become better-understood as microservice architecture is more
    widely adopted. With serverless applications utilizing third-party managed services,
    the problem is especially prevalent. These managed services are, to some degree,
    black boxes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们描述的挑战之一是无服务器系统的碎片化特性。这是由许多小部分组成的分布式系统的常见问题。它可能导致对系统运行行为的理解不足，使得解决问题和进行更改变得困难。随着微服务架构的更广泛采用，这个问题已经得到了更好的理解。利用第三方托管服务的无服务器应用程序，这个问题尤其普遍。这些托管服务在某种程度上是黑盒。
- en: '![](../Images/CH06_F06_Elger.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F06_Elger.png)'
- en: Figure 6.6 The canonical serverless CI/CD architecture is part of SLIC Starter.
    It uses a CodePipeline pipeline for each module. The execution of these pipelines
    in parallel is coordinated by an orchestrator pipeline. The build, deployment,
    and test phases are implemented as CodeBuild projects.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 典型的无服务器CI/CD架构是SLIC Starter的一部分。它为每个模块使用一个CodePipeline管道。这些管道的并行执行由一个编排管道协调。构建、部署和测试阶段作为CodeBuild项目实现。
- en: How much we can understand them depends on the interfaces those services provide
    to report their status. The degree to which systems report their status is called
    *observability*. This term is increasingly being used instead of the traditional
    term, monitoring.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能理解多少取决于这些服务提供报告其状态的接口。系统报告其状态的程度被称为*可观察性*。这个术语越来越多地被用来代替传统的*监控*术语。
- en: Monitoring versus observability
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 监控与可观察性
- en: '*Monitoring* typically refers to the use of tools to inspect known metrics
    of a system. Monitoring should allow you to detect when problems happen and to
    infer *some* knowledge of the system. If a system does not emit the right outputs,
    the effect of monitoring is limited.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*监控*通常指的是使用工具来检查系统的已知指标。监控应允许你检测问题何时发生，并推断出关于系统的*某些*知识。如果一个系统没有发出正确的输出，监控的效果将受到限制。'
- en: '*Observability*,[2](#pgfId-1103358) a term from control theory, is the property
    of a system that allows you to understand what’s going on inside by looking at
    its outputs. The goal of observability is to be able to understand any given problem
    by inspecting its outputs. For example, if we have to change a system and redeploy
    it in order to understand what’s going on, the system is lacking in observability.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*可观察性*，[2](#pgfId-1103358)，是控制理论中的一个术语，它是指系统的一个属性，允许你通过查看其输出了解其内部情况。可观察性的目标是能够通过检查输出理解任何给定的问题。例如，如果我们必须更改系统并重新部署以了解正在发生的事情，那么该系统缺乏可观察性。'
- en: One way to think about the difference between these two terms is that monitoring
    allows you to detect when known problems occur, and observability aims to provide
    understanding when unknown problems occur.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这两个术语之间的区别的一种方式是，监控允许你检测已知问题何时发生，而可观察性旨在在未知问题发生时提供理解。
- en: As an example, let us suppose that your application has a well-tested, working
    sign-up feature. One day, users complain that they are unable to complete sign-ups.
    By looking at a visual map of the system, you determine that errors in the sign-up
    module result from failures in sending sign-up confirmation emails. By looking
    further into the errors in the email service, you notice that an email sending
    limit has been reached, preventing the emails from being sent. The visual map
    of dependencies between modules and errors led you to the email service logs,
    which gives the root cause details. These observability features helped to resolve
    an unexpected problem.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你的应用程序有一个经过良好测试、正常工作的注册功能。有一天，用户抱怨他们无法完成注册。通过查看系统的视觉图，你确定注册模块中的错误是由于发送注册确认电子邮件失败造成的。通过进一步调查电子邮件服务中的错误，你注意到已经达到了电子邮件发送限制，这阻止了电子邮件的发送。模块之间的依赖关系和错误的视觉图引导你到电子邮件服务日志，其中提供了根本原因的详细信息。这些可观察性功能帮助解决了意外问题。
- en: 'There are many approaches to achieving observability. For our checklist application,
    we are going to look at what we want to observe and how to achieve that using
    AWS-managed services. We will look at four practical areas of observability:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 实现可观察性的方法有很多。对于我们这个清单应用程序，我们将探讨我们想要观察的内容以及如何使用AWS管理服务来实现这一点。我们将探讨可观察性的四个实际领域：
- en: Structured, centralized *logging*
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化、集中的*日志记录*
- en: Service and application *metrics*
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务和应用程序*指标*
- en: '*Alarms* to alert us when abnormal or erroneous conditions occur'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当出现异常或错误条件时，会发出*警报*来提醒我们
- en: '*Traces* to give us visibility into the flow of messages throughout the system'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跟踪*以让我们能够看到整个系统中消息的流动'
- en: 6.5 Logs
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 日志
- en: Logs can be collected from many AWS services. With AWS CloudTrail, it’s even
    possible to collect logs pertaining to resource changes made through the AWS SDK
    or Management Console. Here, we will focus on our application logs, those created
    by our Lambda functions. Our goal is to create log entries for meaningful events
    in our application, including information logs, warnings, and errors. Current
    trends lead us to a *structured logging* approach, and with good reason. Unstructured,
    plain text logs can be difficult to search through. They are also difficult for
    log analysis tools to parse. Structured, JSON-based logs can be parsed, filtered,
    and searched easily. Structured logs can be considered *operational data* for
    your application.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从许多AWS服务中收集日志。使用AWS CloudTrail，甚至可以收集通过AWS SDK或管理控制台进行的资源更改相关的日志。在这里，我们将关注我们的应用程序日志，即由我们的Lambda函数创建的日志。我们的目标是创建应用程序中有意义事件的日志条目，包括信息日志、警告和错误。当前趋势使我们倾向于采用*结构化日志*方法，这是有充分理由的。非结构化的纯文本日志难以搜索。它们也难以被日志分析工具解析。基于结构的、基于JSON的日志可以轻松解析、过滤和搜索。结构化日志可以被视为应用程序的*操作数据*。
- en: In a traditional, non-serverless environment, logs were often collected in files
    or using a log agent. With Lambda, those aren’t really options, so the approach
    becomes much simpler. Any console output (to *standard output* or *standard error*)
    from our Lambda functions appears as logging output. AWS Lambda automatically
    collects this output and stores it in CloudWatch logs. These logs are stored in
    a log group named according to the Lambda function name. For example, if our Lambda
    function is called `checklist-service-dev-get`, its logs will be collected in
    a CloudWatch log group named `/aws/lambda/checklist-service-dev-get`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的、非无服务器环境中，日志通常收集在文件中或使用日志代理。使用Lambda，这些选项实际上并不适用，因此方法变得更加简单。我们Lambda函数的任何控制台输出（无论是输出到*标准输出*还是*标准错误*）都会显示为日志输出。AWS
    Lambda自动收集这些输出并将它们存储在CloudWatch日志中。这些日志存储在以Lambda函数名称命名的日志组中。例如，如果我们的Lambda函数名为`checklist-service-dev-get`，其日志将被收集在名为`/aws/lambda/checklist-service-dev-get`的CloudWatch日志组中。
- en: CloudWatch log concepts
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch日志概念
- en: CloudWatch logs are organized into *log groups*. A log group is a grouping of
    related logs, typically relating to a specific service. Within each log group
    is a set of *log streams*. A stream is a set of logs from the same source. For
    Lambda functions, each provisioned *container* has a single log stream. A log
    stream is made up of a series of *log events*. A log event is simply a record
    logged to the stream and associated with a timestamp.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch日志组织成*日志组*。日志组是一组相关的日志，通常与特定服务相关。在每一个日志组中有一组*日志流*。流是从同一来源的一组日志。对于Lambda函数，每个配置的*容器*只有一个日志流。日志流由一系列*日志事件*组成。日志事件只是记录到流中并关联时间戳的记录。
- en: Logs can be stored in CloudWatch logs for inspection using the APIs or the AWS
    Management Console. Log groups can be configured with a *retention period* to
    govern how long they are persisted. By default, logs are kept forever. This is
    usually not the right choice, since log storage in CloudWatch is significantly
    more expensive than archiving or deleting them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用API或AWS管理控制台将日志存储在CloudWatch日志中以供检查。可以配置日志组以*保留期*来控制它们保留的时间。默认情况下，日志会永久保留。这通常不是最佳选择，因为与存档或删除日志相比，CloudWatch中的日志存储要昂贵得多。
- en: 'Logs can be forwarded to other services using a *subscription filter*. One
    subscription filter may be set per log group, allowing a filter pattern and destination
    to be set. The filter pattern can be optionally used to extract only messages
    that match a string. The destination can be any of the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用*订阅过滤器*将日志转发到其他服务。每个日志组可以设置一个订阅过滤器，允许设置一个过滤器模式和目的地。过滤器模式可以用来可选地提取仅与字符串匹配的消息。目的地可以是以下任何一种：
- en: A Lambda function.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Lambda函数。
- en: A Kinesis data stream.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Kinesis数据流。
- en: A Kinesis Data Firehose delivery stream. A delivery stream can be used to collect
    logs in S3, Elasticsearch, or Splunk.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Kinesis Data Firehose交付流。交付流可以用来收集存储在S3、Elasticsearch或Splunk中的日志。
- en: There are many third-party options for storing centralized logs, including the
    popular combination of Elasticsearch, Logstash, and Kibana, commonly referred
    to as the *ELK Stack*. An ELK solution is tried and tested and very powerful in
    its ability to execute complex queries and generate visualizations of log data.
    For simplicity, and also because it is an adequate solution for many applications,
    we will retain logs in CloudWatch and use CloudWatch Logs Insights to view and
    query them. Setting it up requires a lot less work than an Elasticsearch-based
    solution. First, let’s deal with how we generate structured logs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 存储集中日志的第三方选项有很多，包括流行的Elasticsearch、Logstash和Kibana组合，通常称为*ELK栈*。ELK解决方案经过测试，非常强大，能够执行复杂的查询并生成日志数据的可视化。为了简单起见，也因为它是许多应用程序的充分解决方案，我们将保留日志在CloudWatch中，并使用CloudWatch日志洞察来查看和查询它们。设置它比基于Elasticsearch的解决方案要少得多工作。首先，让我们处理我们如何生成结构化日志。
- en: 6.5.1 Writing structured logs
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 编写结构化日志
- en: 'When choosing how to write logs, the goals should be to make it as easy as
    possible for developers and to minimize the performance impact on the application.
    In Node.js applications, the Pino logger ([https://getpino.io](https://getpino.io))
    fits the bill perfectly. Other options include Bunyan ([https://www.npmjs.com/package/bunyan](https://www.npmjs.com/package/bunyan))
    and Winston ([https://www.npmjs.com/package/winston](https://www.npmjs.com/package/winston)).
    We use Pino since it is specifically designed for high performance and minimum
    overhead. To install it in your serverless modules, add it as a dependency as
    follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择如何编写日志时，目标应该是尽可能让开发者容易操作，并最小化对应用程序性能的影响。在Node.js应用程序中，Pino日志记录器([https://getpino.io](https://getpino.io))完美地符合这一要求。其他选项包括Bunyan([https://www.npmjs.com/package/bunyan](https://www.npmjs.com/package/bunyan))和Winston([https://www.npmjs.com/package/winston](https://www.npmjs.com/package/winston))。我们使用Pino，因为它专门为高性能和最小开销而设计。要在你的无服务器模块中安装它，请按照以下方式将其添加为依赖项：
- en: '[PRE2]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It’s also worth installing `pino-pretty`, a companion module that takes structured
    log output from Pino and makes it human-readable. This is ideal when viewing logs
    on the command line:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得安装`pino-pretty`，这是一个辅助模块，它从Pino接收结构化日志输出并将其转换为人类可读的格式。这在通过命令行查看日志时非常理想：
- en: '[PRE3]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To generate structured logs in our code, we create a new Pino logger and invoke
    a logging function for the desired log level--any of `trace`, `debug`, `info`,
    `warning`, `error`, or `fatal`. The following listing demonstrates how the Pino
    logger is used to generate structured logs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们的代码中生成结构化日志，我们创建一个新的Pino日志记录器并调用一个用于所需日志级别的日志函数--可以是`trace`、`debug`、`info`、`warning`、`error`或`fatal`中的任何一个。以下列表演示了如何使用Pino日志记录器生成结构化日志。
- en: Listing 6.2 Pino log messages with contextual, structured data
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2 带有上下文、结构化数据的Pino日志消息。
- en: '[PRE4]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ A logger is created with a specific name to identify the source of logs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个具有特定名称的日志记录器，以标识日志的来源。
- en: ❷ An information message is logged along with some data. The data is passed
    as an object in the first argument.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 信息消息与一些数据一起记录。数据作为第一个参数以对象的形式传递。
- en: ❸ An error is logged using the property err. This is a special property that
    results in the error being serialized as an object. The object includes the error
    type and the stack trace as a string.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用属性err记录错误。这是一个特殊的属性，它会导致错误被序列化为对象。该对象包括错误类型和字符串形式的堆栈跟踪。
- en: 'The JSON logs for the first log record look like this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条日志记录的JSON日志看起来像这样：
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The error log is difficult to read as JSON. If we pipe the output to `pino-pretty`,
    the result is easier to understand. This is shown in the next listing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 错误日志作为JSON难以阅读。如果我们将其输出通过`pino-pretty`管道，结果将更容易理解。这将在下一个列表中展示。
- en: Listing 6.3 Structured JSON logs are made human-readable using `pino-pretty`
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3 使用`pino-pretty`使结构化JSON日志可读。
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 6.5.2 Inspecting log output
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 检查日志输出
- en: We can trigger some log output by using the SLIC Starter application. Go to
    the URL of the deployed SLIC Lists front end. If you followed the Quick Start
    guide for SLIC Starter, you should have this at hand. In this example, we will
    use the staging environment for the continuously-deployed open source repository,
    [https://stg.sliclists.com](https://stg.sliclists.com).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用SLIC启动器应用程序来触发一些日志输出。访问已部署的SLIC列表前端URL。如果你遵循了SLIC启动器的快速入门指南，你应该已经有了这个。在这个例子中，我们将使用持续部署的开源存储库的测试环境，[https://stg.sliclists.com](https://stg.sliclists.com)。
- en: You will need to sign up and create an account. From there, you can log in and
    create a checklist. You are first presented with a login screen, as shown in figure
    6.7\. Follow the link on that screen to sign up and create your account before
    logging in.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要注册并创建账户。从那里，您可以登录并创建一个清单。您首先会看到一个登录屏幕，如图 6.7 所示。遵循该屏幕上的链接注册并创建您的账户，然后再登录。
- en: '![](../Images/CH06_F07_Elger.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F07_Elger.png)'
- en: Figure 6.7 When you launch SLIC Lists for the first time, you can sign up to
    create an account and log in.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 当您首次启动 SLIC 列表时，您可以注册创建账户并登录。
- en: Once you have logged in, you can create a list, as shown in figure 6.8.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您登录，您就可以创建一个列表，如图 6.8 所示。
- en: '![](../Images/CH06_F08_Elger.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F08_Elger.png)'
- en: Figure 6.8 SLIC Lists allows you to create and manage checklists. Here, we create
    a checklist by entering a title and, optionally, a description. In the serverless
    backend, this creates a DynamoDB item. It also triggers an event-driven workflow,
    resulting in a welcome message being sent by email to the list creator.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 SLIC 列表允许您创建和管理清单。在这里，我们通过输入标题和可选的描述来创建一个清单。在无服务器后端，这会在 DynamoDB 中创建一个项。它还会触发一个事件驱动的流程，结果是通过电子邮件发送欢迎信息给列表创建者。
- en: Finally, you can add some entries to the checklist. This is shown in figure
    6.9.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以为清单添加一些条目。这如图 6.9 所示。
- en: '![](../Images/CH06_F09_Elger.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F09_Elger.png)'
- en: Figure 6.9 Here, we add some items to the checklist. This step adds entries
    to the checklist item we just created. If you are interested in how this is achieved
    with DynamoDB data modelling, check out `services/checklists/entries/entries.js`
    in the `checklist-service` folder.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 在这里，我们向清单中添加了一些项目。这一步将条目添加到我们刚刚创建的清单项中。如果您对如何使用 DynamoDB 数据建模实现这一点感兴趣，请查看
    `checklist-service` 文件夹中的 `services/checklists/entries/entries.js`。
- en: Once you have created the checklist records, you can inspect the logs. Note
    that SLIC Starter produces more logs than you would typically expect in a system
    like this. In particular, information is logged at `INFO` level that you would
    reasonably expect in `DEBUG`-level logs. The cost of CloudWatch logs is a real
    consideration here. In a real production system, you should consider reducing
    the log output, redacting any personally-identifiable user information, and implementing
    sampling[3](#pgfId-1101350) for debug logs.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了清单记录，您就可以检查日志。请注意，SLIC Starter 生成的日志比您在类似系统中通常预期的要多。特别是，以 `INFO` 级别记录的信息，您合理地期望在
    `DEBUG` 级别日志中看到。CloudWatch 日志的成本在这里是一个真正的考虑因素。在真实的生产系统中，您应该考虑减少日志输出，删除任何可识别的个人用户信息，并为调试日志实施采样[3](#pgfId-1101350)。
- en: 'Our first way to inspect the CloudWatch logs is with the Serverless Framework
    CLI. Here, we’ll use `serverless` `logs` to see the latest logs for the `create`
    function. The output is again piped to `pino-pretty` for readability:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查 CloudWatch 日志的第一种方法是使用 Serverless Framework CLI。在这里，我们将使用 `serverless`
    `logs` 来查看 `create` 函数的最新日志。输出再次通过 `pino-pretty` 进行管道传输以提高可读性：
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The log output showing the `INFO`-level logs can be seen in the next listing.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了显示 `INFO` 级别日志的日志输出。
- en: Listing 6.4 `serverless` `logs` fetches log events and prints them to the console
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 `serverless` `logs` 获取日志事件并将它们打印到控制台
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In addition to the structured JSON logs, formatted for readability by `pino-pretty`,
    we see the log entries generated by the Lambda container itself. These include
    the `START`, `END`, and `REPORT` records. The `REPORT` record prints useful records
    concerning the memory used and the function duration. Both are important when
    it comes to optimizing the memory configuration for performance and cost.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了结构化的 JSON 日志，这些日志由 `pino-pretty` 格式化以供阅读外，我们还可以看到 Lambda 容器本身生成的日志条目。这些包括
    `START`、`END` 和 `REPORT` 记录。`REPORT` 记录打印有关使用的内存和函数持续时间的有用记录。这两者在优化内存配置以实现性能和成本方面都至关重要。
- en: Choosing the optimal Lambda memory configuratioN Lambda functions are billed
    per request and per GB-second. As with many services, there is a free tier--1
    million requests and 400,000 GB-seconds per month at the time of writing. This
    means you can do quite a lot of computation before you are charged at all. Once
    you have used up the free tier in a production application, choosing the correct
    size for each function is important in terms of cost and performance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的 Lambda 内存配置 Lambda 函数按请求和每 GB-秒计费。与许多服务一样，有一个免费层——在撰写本文时，每月有 100 万次请求和
    40 万 GB-秒。这意味着在您开始收费之前，您可以进行相当多的计算。一旦在生产应用程序中用完了免费层，选择每个函数的正确大小在成本和性能方面都至关重要。
- en: When you configure a Lambda function, you can choose how much memory is allocated
    to it. Doubling the memory will double the cost per second of execution. However,
    allocating more memory also increases the vCPU allocation to the function linearly.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当你配置Lambda函数时，你可以选择为其分配多少内存。内存加倍将使每秒执行成本加倍。然而，分配更多内存也会线性增加函数的vCPU分配。
- en: Suppose you have a function that takes 212ms to execute in a Lambda function
    with 960MB of memory, but 190ms to execute in a function with 1024MB of memory.
    The GB-second pricing of the higher memory configuration will be about 6% higher
    but, since executions are billed in 100ms units, the lower memory configuration
    will use 50% more units (3 instead of 2). Counterintuitively, the higher memory
    configuration will be significantly cheaper and deliver better performance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个函数，在960MB内存的Lambda函数中执行需要212ms，但在1024MB内存的函数中执行只需要190ms。更高内存配置的GB-秒定价将大约高6%，但由于执行是按100ms单位计费，较低内存配置将使用50%更多的单位（3个而不是2个）。出人意料的是，更高内存配置将显著更便宜，并带来更好的性能。
- en: Similarly, if you have a function that typically executes in 10ms and latency
    is not that critical, you might be better off using a lower memory configuration
    with decreased CPU allocation and letting it execute in a time closer to 100ms.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果你有一个通常在10ms内执行完成的函数，且延迟不是那么关键，你可能更倾向于使用较低的内存配置，减少CPU分配，并让它执行时间接近100ms。
- en: 6.5.3 Searching logs using CloudWatch Logs Insights
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 使用CloudWatch日志洞察搜索日志
- en: We have seen how to inspect logs for a single function on the command line.
    It’s also possible to view individual log streams in the AWS Management Console.
    This is useful during development, but less so when you have many functions deployed
    and frequently executed in a production system. For that, you need large-scale,
    centralized logging capable of searching terabytes of log data. CloudWatch Logs
    Insights is a convenient service for this job, and it requires no setup in advance.
    It can be found under the Insights section of the CloudWatch service in the AWS
    Management Console. Figure 6.10 shows a query for logs relating to checklists
    with the phrase “Kick-off” in the title.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何在命令行中检查单个函数的日志。同样，你还可以在AWS管理控制台中查看单个日志流。这在开发期间很有用，但在你部署了许多函数且在生产系统中频繁执行时，作用就较小了。为此，你需要能够搜索TB级日志数据的大规模、集中式日志记录。CloudWatch日志洞察是一个方便的服务，且无需预先设置。它可以在AWS管理控制台的CloudWatch服务下的洞察部分找到。图6.10显示了有关标题中包含“启动”短语清单的日志查询。
- en: '![](../Images/CH06_F10_Elger.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11](../Images/CH06_F10_Elger.png)'
- en: Figure 6.10 CloudWatch Logs Insights allows you to run complex queries across
    multiple log groups.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 CloudWatch日志洞察允许你在多个日志组中运行复杂的查询。
- en: The query shown here is a simple example. The query syntax supports many functions
    and operations. You can perform arithmetic and statistical operations as well
    as extract fields, sort, and filter. Figure 6.11 shows how we can use statistical
    functions to analyze the memory usage and duration of the Lambda by extracting
    data from the `REPORT` logs for each execution.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的查询是一个简单的示例。查询语法支持许多函数和操作。你可以执行算术和统计运算，以及提取字段、排序和过滤。图6.11展示了我们如何通过从每个执行的`REPORT`日志中提取数据来使用统计函数分析Lambda的内存使用量和持续时间。
- en: '![](../Images/CH06_F11_Elger.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10](../Images/CH06_F11_Elger.png)'
- en: Figure 6.11 Statistical and arithmetic operations can be used with Lambda `REPORT`
    logs to analyze whether functions are configured with the optimal memory amount
    for cost and performance. Here, we show memory usage and compare the maximum memory
    used to the provisioned memory capacity. We also show the 95, 98, and 99.9 percentiles
    for function duration to get a sense of performance.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 使用Lambda `REPORT`日志进行统计和算术运算，可以分析函数是否配置了最优的内存量以实现成本和性能。在此，我们展示了内存使用情况，并将最大内存使用量与配置的内存容量进行比较。我们还展示了函数持续时间的95%，98%和99.9百分位数，以获得性能的直观感受。
- en: In the example shown, we have provisioned much more memory than is required.
    This might justify reducing the memory size for the container to 256MB. Since
    the function being analyzed simply invokes a DynamoDB write operation, it is more
    I/O-bound than CPU-bound. As a result, reducing its memory and CPU allocation
    is unlikely to have a significant impact on the duration of executions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，我们分配了比所需更多的内存。这可能意味着可以将容器的内存大小减少到 256MB。由于正在分析的功能仅调用 DynamoDB 写入操作，它比 CPU
    密集型更偏向 I/O 密集型。因此，减少其内存和 CPU 分配不太可能对执行持续时间产生重大影响。
- en: You should now have a good understanding of how centralized, structured logs
    together with CloudWatch Logs Insights can be used to add observability to your
    application. Next, we’ll look at metrics you can observe and create to gain further
    knowledge of your application’s behavior.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该已经很好地理解了如何通过集中式、结构化的日志以及 CloudWatch 日志洞察来为您的应用程序添加可观察性。接下来，我们将探讨您可以观察和创建的指标，以进一步了解应用程序的行为。
- en: 6.6 Monitoring service and application metrics
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 监控服务和应用指标
- en: As part of the goal of achieving observability, we want to be able to create
    and view metrics. Metrics can be service-specific, like the number of concurrently-executing
    Lambda functions. They can also be application-specific, like the number of entries
    in a checklist. AWS provides a metrics repository called CloudWatch Metrics. This
    service collects individual metrics and allows you to view aggregations on them.
    Note that it is not possible to view individual metric data points once they have
    been collected. Instead, you can request statistics for a given period, such as
    the sum of a count metric per minute.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实现可观察性目标的一部分，我们希望能够创建和查看指标。指标可以是特定于服务的，例如并发执行的 Lambda 函数的数量。它们也可以是特定于应用的，例如清单中的条目数量。AWS
    提供了一个名为 CloudWatch 指标的指标存储库。此服务收集单个指标并允许您查看它们的聚合。请注意，一旦收集了单个指标数据点，就无法查看。相反，您可以请求给定时间段内的统计数据，例如每分钟计数指标的求和。
- en: By default, the minimum period for CloudWatch Metrics is one minute. It is possible
    to add high-resolution custom metrics with a resolution of one second. After three
    hours of retention, high-resolution metrics are aggregated to one-minute intervals.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch 指标的默认最小周期为 1 分钟。可以添加具有 1 秒分辨率的自定义高分辨率指标。保留 3 小时后，高分辨率指标将聚合到 1 分钟间隔。
- en: 6.6.1 Service metrics
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.1 服务指标
- en: Many AWS services publish metrics by default for most services. Whether you
    are using CloudWatch Metrics or another metrics solution, it is important to be
    aware of what metrics are published and which ones you should monitor. Table 6.2
    lists just some of the metrics for a sample of AWS services. We have chosen examples
    that are particularly relevant to the AI applications built in chapters 2-5.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 AWS 服务默认为大多数服务发布指标。无论您是使用 CloudWatch 指标还是其他指标解决方案，了解发布的指标以及您应该监控哪些指标都至关重要。表
    6.2 列出了 AWS 服务样本的一些指标。我们选择了与第 2-5 章中构建的 AI 应用特别相关的示例。
- en: Table 6.2 AWS services publish CloudWatch Metrics that can be monitored to gain
    insight into system behavior. It is really important to understand and observe
    the metrics relevant to services you are using.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.2 AWS 服务发布 CloudWatch 指标，可以监控以深入了解系统行为。理解和观察与您使用的服务相关的指标非常重要。
- en: '| Service | Example metrics |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | 示例指标 |'
- en: '| Lex[4](#pgfId-1101433) | `MissedUtteranceCount`, `RuntimePollyErrors` |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Lex[4](#pgfId-1101433) | `未接收到的话语数量`, `Polly 运行时错误` |'
- en: '| Textract[5](#pgfId-1101440) | `UserErrorCount`, `ResponseTime` |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Textract[5](#pgfId-1101440) | `用户错误数量`, `响应时间` |'
- en: '| Rekognition[6](#pgfId-1101447) | `DetectedFaceCount`, `DetectedLabelCount`
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Rekognition[6](#pgfId-1101447) | `检测到的面孔数量`, `检测到的标签数量` |'
- en: '| Polly[7](#pgfId-1101454) | `RequestCharacters`, `ResponseLatency` |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Polly[7](#pgfId-1101454) | `请求字符数`, `响应延迟` |'
- en: '| DynamoDB[8](#pgfId-1101461) | `ReturnedBytes`, `ConsumedWriteCapacityUnits`
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| DynamoDB[8](#pgfId-1101461) | `返回字节`, `消耗的写入容量单元` |'
- en: '| Lambda[9](#pgfId-1101468) | `Invocations`, `Errors`, `IteratorAge`, `ConcurrentExecutions`
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Lambda[9](#pgfId-1101468) | `调用次数`, `错误`, `迭代器年龄`, `并发执行数` |'
- en: Thorough coverage of all metrics for the services we have used is beyond the
    scope of this book. We recommend that you explore the CloudWatch Metrics section
    of the AWS Management Console with the applications you have built so far while
    reading the book. A comprehensive list of services and their metrics can be found
    in the AWS documentation.[10](#pgfId-1101475)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们所使用的所有服务的所有指标的彻底覆盖超出了本书的范围。我们建议您在阅读本书时，探索您迄今为止构建的应用程序的 AWS 管理控制台中的 CloudWatch
    Metrics 部分。有关服务和它们的指标的综合列表可以在 AWS 文档中找到。[10](#pgfId-1101475)
- en: 6.6.2 Application metrics
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.2 应用指标
- en: 'CloudWatch Metrics can be used as a repository for custom application metrics
    in addition to the built-in metrics published by AWS services. In this section,
    we’ll explore what it takes to add a metric. Let’s revisit the checklist application
    in the SLIC Starter project. We might want to gather application-specific metrics
    that inform us how the product is developed further. Let’s suppose we are thinking
    about developing an Alexa skill for the application. An *Alexa skill* is a serverless
    application in AWS that allows users to interact with a service using a smart
    speaker device. This would be a very similar endeavor to the Lex-driven to-do
    chatbot from chapter 5! In order to design this skill, our User Experience department
    wants to gather statistics on how users are currently using *SLIC Lists*. Specifically,
    we want to understand the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 AWS 服务发布的内置指标外，CloudWatch Metrics 还可以用作自定义应用指标的存储库。在本节中，我们将探讨添加指标需要哪些步骤。让我们回顾一下
    SLIC Starter 项目中的清单应用程序。我们可能想要收集一些特定于应用程序的指标，这些指标可以告诉我们产品是如何进一步开发的。假设我们正在考虑为应用程序开发一个
    Alexa 技能。一个 *Alexa 技能* 是一个 AWS 中的无服务器应用程序，允许用户通过智能扬声器设备与一项服务进行交互。这将与第 5 章中 Lex
    驱动的待办事项聊天机器人非常相似！为了设计这个技能，我们的用户体验部门想要收集有关用户当前如何使用 *SLIC 列表* 的统计数据。具体来说，我们想要了解以下内容：
- en: How many entries are users putting into checklists?
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户在清单中添加了多少条目？
- en: How many words are in a typical checklist entry?
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个典型的清单条目中有多少个单词？
- en: 'With CloudWatch Metrics, there are two ways we could add these metrics:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CloudWatch Metrics，我们可以有两种方法来添加这些指标：
- en: Using the AWS SDK and a call to the `putMetricData` API[11](#pgfId-1101491)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS SDK 并调用 `putMetricData` API[11](#pgfId-1101491)
- en: Using logs specially formatted according to the *Embedded Metric Format*
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用根据 *嵌入式指标格式* 特别格式化的日志
- en: Using the `putMetricData` API has a disadvantage. Making an SDK call like this
    will result in an underlying HTTP request. This adds unwanted latency to our code.
    We will instead use the Embedded Metric Format log. This method requires us to
    create a specially formatted log message that has all the details of the metric
    we want to produce. Since we are using CloudWatch logs, CloudWatch will automatically
    detect, parse, and convert this log message into a CloudWatch metric. The overhead
    of writing this log message will have a negligible impact on the performance of
    our code. In addition, the raw metrics will be available for as long as we retain
    the logs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `putMetricData` API 有一个缺点。进行此类 SDK 调用将导致底层 HTTP 请求。这会给我们的代码添加不必要的延迟。我们将改用嵌入式指标格式日志。这种方法要求我们创建一个特别格式化的日志消息，其中包含我们想要生成的指标的所有详细信息。由于我们使用
    CloudWatch 日志，CloudWatch 将自动检测、解析并将此日志消息转换为 CloudWatch 指标。编写此日志消息的开销将对代码的性能产生微乎其微的影响。此外，原始指标将在我们保留日志的时间内可用。
- en: Let’s take a look at how we produce these metric logs and what the result looks
    like. An outline of the log message format is shown in the following listing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是如何生成这些指标日志以及结果看起来像什么。日志消息格式的概述如下所示。
- en: Listing 6.5 The structure of Embedded Metric Format logs
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 嵌入式指标格式日志的结构
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The _aws property defines the metadata for our metrics.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `_aws` 属性定义了我们指标的元数据。
- en: ❷ A namespace for the metric is the grouping under which this metric falls.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指标的命名空间是此指标所属的分组。
- en: ❸ Each metric can be given up to ten dimensions. A dimension is a name-value
    pair that categorizes the metric.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个指标可以指定多达十个维度。维度是一个将指标分类的名称-值对。
- en: ❹ A single metric is defined here, giving it a name and its unit. There is a
    defined list of supported metric units in the AWS documentation.5
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在这里定义了一个单一指标，给它一个名称和单位。AWS 文档中定义了一个支持的指标单位列表。5
- en: ❺ The value for dimensions named in the metadata is given here.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在元数据中命名的维度的值在这里给出。
- en: ❻ The value for metrics named in the metadata is provided here.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在元数据中命名的指标的值在这里提供。
- en: These JSON structured log messages are automatically recognized by CloudWatch
    and result in CloudWatch Metrics being created with minimal performance overhead.
    It is possible to create this JSON structure and log it to CloudWatch logs in
    our Lambda function code using `console.log`. Another way is to use the `aws-embedded-metrics`
    Node.js module.[13](#pgfId-1101535) This module gives us a number of functions
    for logging metrics. In this case, we’ll use the `createMetricsLogger` function.
    We are going to add the metric logging code in `checklist-service/services/checklists/entries/entries.js`.
    See the following listing for the relevant extract from the `addEntry` function.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些符合JSON结构的日志消息会自动被CloudWatch识别，并导致创建CloudWatch指标，同时最小化性能开销。我们可以在Lambda函数代码中使用`console.log`创建此JSON结构并将其记录到CloudWatch日志中。另一种方法是使用`aws-embedded-metrics`
    Node.js模块。[13](#pgfId-1101535) 此模块为我们提供了一系列用于记录指标的函数。在这种情况下，我们将使用`createMetricsLogger`函数。我们将在`checklist-service/services/checklists/entries/entries.js`中添加指标记录代码。请参阅以下列表，以获取`addEntry`函数的相关摘录。
- en: Listing 6.6 Structured logging compliant with the Embedded Metric Format
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 符合嵌入式指标格式的结构化日志
- en: '[PRE10]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ createMetricsLogger creates a logger that we can call explicitly. The aws-embedded-metrics
    module also provides a wrapper or “decorator” function that avoids the explicit
    flush call.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `createMetricsLogger`创建了一个我们可以显式调用的记录器。`aws-embedded-metrics`模块还提供了一个包装器或“装饰器”函数，以避免显式调用刷新。
- en: ❷ The number of entries in the checklist is recorded as a count metric.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将清单中的条目数量记录为计数指标。
- en: ❸ The number of words in the checklist entry is recorded.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 记录清单条目中的单词数量。
- en: ❹ We flush the metrics to ensure they are written to the console output.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们刷新指标以确保它们被写入控制台输出。
- en: To generate some metrics, we need to invoke this function with varying inputs.
    The SLIC Starter end-to-end integration tests include a test that creates a checklist
    with an entry count and word counts according to a realistic distribution. We
    can run this test a number of times to get some reasonable metrics in CloudWatch.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成一些指标，我们需要用不同的输入调用此函数。SLIC Starter端到端集成测试包括一个创建具有条目计数和单词计数的清单的测试，这些计数根据现实分布。我们可以多次运行此测试以在CloudWatch中获得一些合理的指标。
- en: 'There are some setup steps in SLIC Starter’s integration tests. Check out the
    `README.md` file in the `integration-tests` folder. Once you have prepared the
    tests and verified that you can run them once, we can proceed to running a batch
    of integration tests to simulate some load:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: SLIC Starter的集成测试中有一些设置步骤。查看`integration-tests`文件夹中的`README.md`文件。一旦您已准备好测试并验证可以运行一次，我们就可以运行一批集成测试来模拟一些负载：
- en: '[PRE11]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `load.sh` script runs a random number of integration test executions in
    parallel, and repeats that process until it has been done 100 times. Now, we can
    proceed to the CloudWatch Metrics section of the AWS Management Console to visualize
    statistics on the checklist entries that were created.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`load.sh`脚本并行运行随机数量的集成测试执行，并重复此过程，直到完成100次。现在，我们可以进入AWS管理控制台的CloudWatch指标部分，以可视化清单条目的统计数据。'
- en: When you select CloudWatch Metrics in the console, the view should look something
    like figure 6.12.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在控制台中选择CloudWatch指标时，视图应该类似于图6.12。
- en: '![](../Images/CH06_F12_Elger.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F12_Elger.png)'
- en: Figure 6.12 Browsing to the CloudWatch Metrics view in the AWS Management Console
    allows you to select from custom namespaces and namespaces for AWS services.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 在AWS管理控制台中浏览到CloudWatch指标视图，允许您从自定义命名空间和AWS服务的命名空间中选择。
- en: From here, select the *aws-embedded-metrics* namespace. This brings you to a
    table where you can see the sets of dimensions within the selected namespace.
    This is shown in figure 6.13.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，选择`aws-embedded-metrics`命名空间。这会带您到一个表格，您可以在其中看到所选命名空间内的维度集。如图6.13所示。
- en: '![](../Images/CH06_F13_Elger.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F13_Elger.png)'
- en: Figure 6.13 Once a namespace is selected, the next step is to choose the dimension
    sets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 一旦选择了命名空间，下一步就是选择维度集。
- en: Click through the only option to reveal the viewable metrics. Select the two
    metrics from the `addEntry` function, as shown in figure 6.14.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过唯一选项进行点击以显示可查看的指标。从`addEntry`函数中选择两个指标，如图6.14所示。
- en: '![](../Images/CH06_F14_Elger.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F14_Elger.png)'
- en: Figure 6.14 The CloudWatch Metrics console presents all metrics within the selected
    namespace and dimensions. Clicking the checkbox next to each one adds the metric
    to the graph displayed.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 CloudWatch指标控制台呈现所选命名空间和维度内的所有指标。点击每个旁边的复选框将指标添加到显示的图表中。
- en: We now want to customize the presentation of these metrics. First, let’s add
    to the default average statistic. This can be done by switching to the Graphed
    Metrics tab. Select the Duplicate icon next to each metric. Do this twice for
    both the NumEntries and EntryWords metrics. This will create copies of the average
    metric. Change one copy of each to use the Maximum and p95 statistics. Lastly,
    change the graph type from Line to Number. The resulting view should look like
    figure 6.15.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在想自定义这些指标的展示方式。首先，让我们添加到默认的平均统计信息。这可以通过切换到“图形指标”选项卡来完成。在每个指标旁边选择复制图标。对于NumEntries和EntryWords指标，都这样做两次。这将创建平均指标的副本。将每个副本中的一个更改为使用最大值和p95统计信息。最后，将图表类型从折线图更改为数字图。最终视图应该看起来像图6.15。
- en: '![](../Images/CH06_F15_Elger.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15](../Images/CH06_F15_Elger.png)'
- en: Figure 6.15 Switching to the Graphed Metrics tab allows you to customize and
    copy metrics. Here, we select new statistics for the same two metrics. Switching
    the graph from Line to Number also gives us a plain and simple view of the statistics
    we need.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 切换到“图形指标”选项卡允许您自定义和复制指标。在这里，我们为相同的两个指标选择了新的统计信息。将图表从折线图切换到数字图也为我们提供了所需统计信息的简单直观视图。
- en: Using the Number visualization instead of Line is more useful in this case,
    as the change in values over time as viewed on a line graph is not interesting
    for these metrics. We have ended up with some simple numbers that can help our
    User Experience team design an Alexa skill! We know that most entries have fewer
    than five words, with the average being two words. The average list has around
    8 entries and 95% have 16.6 or fewer.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用数字可视化而不是折线图更有用，因为随着时间的推移在折线图上查看的值的变化对这些指标来说并不有趣。我们最终得到了一些简单的数字，可以帮助我们的用户体验团队设计Alexa技能！我们知道大多数条目少于五个单词，平均为两个单词。平均列表大约有8个条目，95%的条目少于16.6。
- en: 6.6.3 Using metrics to create alarms
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6.3 使用指标创建警报
- en: 'At this point, you have seen the value of understanding and monitoring AWS
    service metrics and custom application metrics. When you have unexplained system
    behavior, this knowledge should help you to start revealing some of the unknowns.
    It’s not a great idea, however, to wait until something goes wrong to start digging
    for answers. It’s preferable to think about what the normal system behavior is
    and to create alarms for when the system behavior deviates from this norm. An
    *alarm* is a notification to system operators when a specified condition is reached.
    Typically, we would set up alarms for deviations such as the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你已经看到了理解和监控 AWS 服务指标以及自定义应用指标的价值。当你遇到无法解释的系统行为时，这些知识应该能帮助你开始揭示一些未知因素。然而，等到出现问题才开始寻找答案并不是一个好主意。最好是思考正常系统行为是什么，并为系统行为偏离这一正常状态时创建警报。*警报*是在达到指定条件时对系统操作员的通知。通常，我们会为以下偏差设置警报：
- en: A metric that counts the number of errors within an AWS service, and is triggered
    when the value is greater than a given number. For example, we might like to be
    alerted when the number of Lambda invocations across all our functions exceeds
    10 in a five-minute period.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个计数AWS服务内错误数量的指标，当值大于给定数字时触发。例如，我们可能希望在五分钟内所有函数的Lambda调用次数超过10次时收到警报。
- en: The level of service to our end users is reaching an unacceptable level. An
    example of this is when the 99th percentile for the API Gateway Latency metric
    for a critical API endpoint exceeds 500ms.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对最终用户的服务水平已经达到了不可接受的程度。一个例子是，对于关键API端点的API网关延迟指标的99百分位数超过500ms。
- en: Business metrics are really valuable for creating alarms. It is often easier
    to create thresholds relating to interactions from an end-user perspective. For
    example, in our SLIC Starter application, we might know that it’s typical for
    between 50 and 60 checklists to be created every hour. If the number falls wildly
    outside this threshold, we can receive an alarm and investigate. This might be
    just a spurious change in activity, or indicative of some underlying technical
    problem we might not have otherwise detected.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 商业指标对于创建警报来说非常有价值。从终端用户的角度来看，通常更容易创建与交互相关的阈值。例如，在我们的SLIC Starter应用程序中，我们可能知道每小时通常会有50到60个清单被创建。如果这个数字远远低于这个阈值，我们就可以收到警报并调查。这可能是活动中的偶然变化，也可能是我们可能没有检测到的潜在技术问题。
- en: In the context of AWS, alerts like these are possible using CloudWatch Alarms.
    Alarms are always based on CloudWatch Metrics. It is possible to define the period
    and statistic used (e.g., the *average* latency over *5 minutes*). The threshold
    for the alarm can be based on a numeric value or based on anomaly detection using
    standard deviation bands. The alerting mechanism for CloudWatch Alarms is through
    SNS Topics. *SNS* (or *Simple Notification Service*) is a Pub/Sub for sending
    events. SNS allows alerts to be delivered via email, SMS or webhook, or to another
    service, including SQS and Lambda.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS的背景下，像这样的警报可以通过CloudWatch Alarms实现。警报始终基于CloudWatch指标。可以定义使用的周期和统计量（例如，*5分钟*内的*平均*延迟）。警报的阈值可以是基于数值，也可以是基于标准差带进行异常检测。CloudWatch
    Alarms的警报机制是通过SNS主题实现的。*SNS*（或*简单通知服务*）是一种用于发送事件的发布/订阅服务。SNS允许通过电子邮件、短信或webhook将警报发送到另一个服务，包括SQS和Lambda。
- en: 'A comprehensive example of creating alarms is beyond the scope of this chapter.
    It is worthwhile to use the AWS Management Console to experiment and create some
    alarms. Once you are familiar with the configuration options for CloudWatch Alarms,
    you can proceed to create them as resources in the `serverless.yml` file for your
    application. The following resources also allow us to create alarms with less
    configuration:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 创建警报的全面示例超出了本章的范围。使用AWS管理控制台进行实验和创建一些警报是值得的。一旦你熟悉了CloudWatch Alarms的配置选项，你就可以在`serverless.yml`文件中将它们作为资源创建。以下资源还可以让我们以更少的配置创建警报：
- en: The *Serverless Application Repository* provides hosted CloudFormation stacks
    that can be included as nested applications within your own application. Other
    organizations have published stacks that simplify the process of creating a reasonable
    set of alarms for serverless applications. One example is the *SAR-cloudwatch-alarms-macro*
    application.[14](#pgfId-1101607) It creates alarms for common errors in AWS Lambda,
    API Gateway, AWS Step Functions, and SQS.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Serverless应用程序仓库*提供了可以包含在您自己的应用程序中的托管CloudFormation堆栈。其他组织已经发布了简化创建合理警报集的堆栈。一个例子是*SAR-cloudwatch-alarms-macro*应用程序。[14](#pgfId-1101607)
    它为AWS Lambda、API Gateway、AWS Step Functions和SQS中的常见错误创建警报。'
- en: Plugins for the Serverless Framework such as the AWS Alerts Plugin ([http://mng.bz/jVre](http://mng.bz/jVre))
    make the process of creating alarms easier.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于Serverless Framework的插件，如AWS Alerts Plugin ([http://mng.bz/jVre](http://mng.bz/jVre))，使创建警报的过程更加简单。
- en: 6.7 Using traces to make sense of distributed applications
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 使用跟踪来理解分布式应用程序
- en: At the start of the chapter, we said that one of the challenges in serverless
    development is the distributed and fragmented nature of systems. This aspect makes
    it harder to visualize or reason about the behavior of the system as a whole.
    The practices of centralized logging, metrics, and alarms can help with that.
    Distributed tracing is an additional tool that makes understanding the flow of
    data through a serverless system possible. Within the AWS ecosystem, distributed
    tracing is provided by X-Ray and CloudWatch ServiceLens. X-Ray is the underlying
    tracing service, and ServiceLens is the area of the CloudWatch console that provides
    tracing visualization integrated with logs and metrics. There are commercial alternatives
    such as Datadog, Lumigo, and Epsagon. Though these are certainly worth exploring,
    we will use the managed AWS services, as they are sufficient to demonstrate and
    learn the concepts of observability and tracing.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们提到，无服务器开发中的一个挑战是系统的分布式和碎片化特性。这一方面使得可视化或推理整个系统的行为变得更加困难。集中式日志、指标和警报的实践可以帮助解决这个问题。分布式追踪是另一个工具，它使得理解无服务器系统中数据流变得可能。在
    AWS 生态系统中，分布式追踪由 X-Ray 和 CloudWatch ServiceLens 提供。X-Ray 是底层追踪服务，ServiceLens 是
    CloudWatch 控制台中提供与日志和指标集成的追踪可视化的区域。还有商业替代方案，如 Datadog、Lumigo 和 Epsagon。尽管这些方案确实值得探索，但我们将使用托管
    AWS 服务，因为它们足以演示和学习可观察性和追踪的概念。
- en: 6.7.1 Enabling X-Ray tracing
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7.1 启用 X-Ray 追踪
- en: The purpose of distributed tracing is to both monitor and profile performance
    of requests as they propagate through many services in a system. The best way
    to illustrate this is with a visual example. Consider the scenario of creating
    a checklist in the SLIC Starter application. From the point when a user clicks
    the Save button in the front end, the sequence shown in figure 6.16 occurs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式追踪的目的是监控和评估请求在系统中通过多个服务时的性能。最好的说明方式是使用一个视觉示例。考虑在 SLIC Starter 应用程序中创建清单的场景。从用户在前端点击保存按钮的那一刻起，图
    6.16 中所示的序列发生。
- en: '![](../Images/CH06_F16_Elger.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F16_Elger.png)'
- en: Figure 6.16 A typical request to a serverless system results in multiple messages
    across many services.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 服务器无服务器系统的一次典型请求会在多个服务之间产生多条消息。
- en: The request goes through API Gateway to a Lambda in the checklist service.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求通过 API Gateway 到达清单服务中的 Lambda。
- en: This Lambda calls DynamoDB.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个 Lambda 调用 DynamoDB。
- en: The Lambda publishes a “list created” event to Amazon EventBridge.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lambda 将“列表创建”事件发布到 Amazon EventBridge。
- en: The event is picked up by the welcome service.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事件被欢迎服务捕获。
- en: The welcome service calls the user service API to look up the checklist owner’s
    email address.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 欢迎服务调用用户服务 API 来查找清单所有者的电子邮件地址。
- en: The welcome service puts an SQS message on the email service’s queue.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 欢迎服务将 SQS 消息放入电子邮件服务的队列中。
- en: The email service accepts incoming SQS messages and send an email using the
    Simple Email Service (SES).
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 电子邮件服务接受传入的 SQS 消息，并使用简单电子邮件服务（SES）发送电子邮件。
- en: This is a relatively simple distributed workflow, but it’s already easy to see
    how this chain-reaction of events can be difficult to comprehend for developers.
    Imagine what it’s like in a system with hundreds or thousands of services! By
    capturing traces for the entire flow, we can view the sequence and timings in
    ServiceLens. Part of this sequence is shown in figure 6.17.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对简单的分布式工作流，但已经可以看出，这种事件链式反应对于开发者来说理解起来有多困难。想象一下在拥有数百或数千个服务的系统中的情况！通过捕获整个流程的轨迹，我们可以在
    ServiceLens 中查看序列和时序。其中一部分序列在图 6.17 中显示。
- en: '![](../Images/CH06_F17_Elger.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F17_Elger.png)'
- en: Figure 6.17 CloudWatch ServiceLens shows individual traces with times for each
    segment.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 CloudWatch ServiceLens 显示了每个段的时序的单独轨迹。
- en: The trace in the figure shows the segments of the distributed request, including
    their timings. Note that this picture relates to a single request. With X-Ray,
    traces are sampled. By default, one request per second is sampled, and 5% of requests
    thereafter. This is configurable through rules in the X-Ray console.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的轨迹显示了分布式请求的段，包括它们的时序。请注意，这张图片与单个请求相关。使用 X-Ray，轨迹被采样。默认情况下，每秒采样一个请求，之后每 5%
    的请求进行采样。这可以通过 X-Ray 控制台中的规则进行配置。
- en: 'X-Ray works by generating trace IDs and propagating these from one service
    to another as the request is fulfilled. In order to enable this behavior, developers
    can use the AWS X-Ray SDK to add automatic tracing instrumentation of AWS SDK
    calls. The effect of this is that tracing headers containing trace and segment
    identifiers are added to requests. Request data, including timings, are also sent
    by the X-Ray SDK to a daemon that collects tracing samples. The following code
    shows how we initialize the X-Ray SDK in our Node.js Lambda function code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: X-Ray通过生成跟踪ID并将这些ID在请求得到满足时从一个服务传播到另一个服务来工作。为了启用此行为，开发者可以使用AWS X-Ray SDK来添加AWS
    SDK调用的自动跟踪仪器。这样做的影响是，包含跟踪和分段标识符的跟踪头被添加到请求中。请求数据，包括时间信息，也由X-Ray SDK发送到收集跟踪样本的守护进程。以下代码显示了我们在Node.js
    Lambda函数代码中初始化X-Ray SDK的方式：
- en: '[PRE12]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This snippet, taken from `slic-tools/aws.js` in SLIC Starter, loads the X-Ray
    SDK before loading the standard AWS SDK. The X-Ray SDK’s `captureAWS` function
    is invoked to intercept all SDK requests and create new segments as part of the
    trace.[15](#pgfId-1106459) The other change required to enable X-Ray traces is
    to turn them on in API Gateway and Lambda configuration. When using the Serverless
    Framework, this involves an addition to the `serverless.ymlprovider` section,
    shown in the following code:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段来自SLIC Starter中的`slic-tools/aws.js`，在加载标准AWS SDK之前加载X-Ray SDK。X-Ray SDK的`captureAWS`函数被调用以拦截所有SDK请求并在跟踪中创建新的分段。[15](#pgfId-1106459)
    要启用X-Ray跟踪，还需要在API Gateway和Lambda配置中将其打开。当使用Serverless Framework时，这涉及到对`serverless.ymlprovider`部分的添加，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is done for all services in SLIC Starter, so you already have everything
    required to view distributed tracing results.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在SLIC Starter中的所有服务中完成的，所以您已经拥有了查看分布式跟踪结果所需的一切。
- en: 6.7.2 Exploring traces and maps
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7.2 探索跟踪和映射
- en: In addition to the individual trace timeline we already saw, the X-Ray console
    and the newer CloudWatch ServiceLens console have the capability to show a full
    map of your services. This is an extremely powerful visualization tool. An example
    of the SLIC Starter service map is shown in figure 6.18.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前看到的单个跟踪时间线之外，X-Ray控制台和较新的CloudWatch ServiceLens控制台都有显示您服务完整映射的能力。这是一个极其强大的可视化工具。图6.18展示了SLIC
    Starter服务映射的一个示例。
- en: '![](../Images/CH06_F18_Elger.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F18_Elger.png)'
- en: Figure 6.18 A map showing the request propagation between services can be shown
    in CloudWatch ServiceLens. Although this diagram shows so many services that it
    becomes hard to read, you have the option to zoom in and filter out when using
    the AWS Console.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 在CloudWatch ServiceLens中可以显示服务之间请求传播的映射。尽管这个图表显示了如此多的服务以至于难以阅读，但您在使用AWS控制台时可以选择放大和过滤。
- en: All visualisations, including maps and traces, show any errors captured. The
    map view shows an error percentage per node. Selecting any node in the map will
    show the request rate, latency, and number of errors. Figure 6.19 shows a selection
    of the service map for the `deleteEntry` function in the checklist service with
    a 50% error rate.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可视化，包括映射和跟踪，都会显示捕获到的任何错误。映射视图显示了每个节点的错误百分比。选择映射中的任何节点将显示请求速率、延迟和错误数量。图6.19显示了清单服务中`deleteEntry`函数的服务映射选择，错误率为50%。
- en: '![](../Images/CH06_F19_Elger.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F19_Elger.png)'
- en: Figure 6.19 Choosing View Connections for any selected node in the service map
    filters the view to show connected services only. Here, we see error incidents
    that can be investigated further by using the correlating request IDs in CloudWatch
    Logs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务映射中选择任何节点以选择“查看连接”将过滤视图以仅显示连接的服务。在这里，我们可以看到可以通过在CloudWatch日志中使用相关请求ID进一步调查的错误事件。
- en: We can choose to select View Traces or View Logs to diagnose further. View Logs
    takes us to CloudWatch Logs Insights for this request and time.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择选择“查看跟踪”或“查看日志”来进一步诊断。选择“查看日志”将带我们进入CloudWatch日志洞察来查看此请求和时间的日志。
- en: 6.7.3 Advanced tracing with annotations and custom metrics
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7.3 使用注释和自定义指标的高级跟踪
- en: 'We are not able to cover all the use cases for X-Ray and ServiceLens. There
    are, however, a few features that are worth mentioning, as they are particularly
    useful when trying to find answers for real production scenarios at scale:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法涵盖X-Ray和ServiceLens的所有用例。然而，有一些特性值得提及，因为它们在尝试寻找大规模真实生产场景的答案时特别有用：
- en: '*Annotations* are indexed key-value pairs that you can assign to trace segments
    using the X-Ray SDK. These are indexed by X-Ray, so you can filter on them in
    the X-Ray console.[16](#pgfId-1101691) It’s also possible to add custom metadata
    to trace segments. These are not indexed but can be viewed in the console.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注释* 是您可以使用 X-Ray SDK 分配到跟踪段的索引键值对。这些由 X-Ray 索引，因此您可以在 X-Ray 控制台中根据它们进行筛选。[16](#pgfId-1101691)
    还可以向跟踪段添加自定义元数据。这些不是索引的，但可以在控制台中查看。'
- en: The X-Ray Analytics console and the AWS SDK support the creation of *groups*,
    defined by a filter expression. The filter expression can include the custom annotations
    created in your code using the X-Ray SDK.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X-Ray 分析控制台和 AWS SDK 支持创建由过滤器表达式定义的 *组*。过滤器表达式可以包括使用 X-Ray SDK 在您的代码中创建的自定义注释。
- en: When groups are defined, X-Ray will create custom metrics and publish them to
    CloudWatch Metrics. These include latency, error, and throttling rates.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当定义组时，X-Ray 将创建自定义指标并将它们发布到 CloudWatch 指标。这些包括延迟、错误和节流速率。
- en: We recommend taking some time to experiment with the features of X-Ray through
    the AWS Management Console. This will help you to create the right annotations,
    metadata, and groups for your own serverless applications.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您花些时间通过 AWS 管理控制台实验 X-Ray 的功能。这将帮助您为您的无服务器应用程序创建正确的注释、元数据和组。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: CodePipeline and CodeBuild can be used to create a serverless continuous deployment
    pipeline.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 CodePipeline 和 CodeBuild 创建无服务器持续部署管道。
- en: The monorepo approach is an effective strategy for structuring a scalable serverless
    application.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一仓库方法是一种有效的策略，用于构建可扩展的无服务器应用程序。
- en: There are challenges with distributed serverless application architectures that
    can be addressed using observability best practices.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可观测性最佳实践可以解决分布式无服务器应用程序架构中的挑战。
- en: Centralized logging can be implemented using structured JSON logs and AWS CloudWatch
    logs.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用结构化 JSON 日志和 AWS CloudWatch 日志实现集中式日志记录。
- en: CloudWatch Logs Insights is used to view and drill down into logs.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CloudWatch 日志洞察查看并深入日志。
- en: Service metrics can be viewed using CloudWatch.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 CloudWatch 查看服务指标。
- en: It is possible to create application-specific custom metrics.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以创建特定于应用程序的自定义指标。
- en: Distributed tracing using X-Ray and ServiceLens allows us to make sense of a
    highly-distributed serverless system.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 X-Ray 和 ServiceLens 进行分布式跟踪使我们能够理解高度分布式的无服务器系统。
- en: In the next chapter, we will continue to look at real-world AI as a Service,
    focusing on integration into existing systems built on vastly different technologies.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探讨现实世界的 AI 即服务，重点关注将其集成到基于截然不同技术的现有系统中。
- en: Warning Please ensure that you fully remove all cloud resources deployed in
    this chapter in order to avoid additional charges!
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 请确保您完全删除本章中部署的所有云资源，以避免额外收费！
- en: '* * *'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 1.For more on these and other deployment strategies, see “Six Strategies for
    Application Deployment,” an article by Etienne Tremel, 21 November 2017, thenewstack.io,
    [https://thenewstack.io/deployment-strategies/.](https://thenewstack.io/deployment-strategies/)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 1.有关这些和其他部署策略的更多信息，请参阅 Etienne Tremel 的文章“应用程序部署的六种策略”，2017年11月21日，thenewstack.io，[https://thenewstack.io/deployment-strategies/](https://thenewstack.io/deployment-strategies/)。
- en: 2.[Introduction to Observability, honeycomb.io, http://mng.bz/aw4X.](http://mng.bz/aw4X)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 2.[可观测性简介，honeycomb.io，http://mng.bz/aw4X.](http://mng.bz/aw4X)
- en: 3.“You need to sample debug logs in production,” Yan Cui, 28 April 2018, [https://hackernoon.com/you-need-to-sample-debug-logs-in-production-171d44087749](https://hackernoon.com/you-need-to-sample-debug-logs-in-production-171d44087749).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 3.“您需要在生产中采样调试日志，” 焉桂，2018年4月28日，[https://hackernoon.com/you-need-to-sample-debug-logs-in-production-171d44087749](https://hackernoon.com/you-need-to-sample-debug-logs-in-production-171d44087749)。
- en: 4.See *Monitoring Amazon Lex with Amazon CloudWatch*[, http://mng.bz/emRq.](http://mng.bz/emRq)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 4.参见 *使用 Amazon CloudWatch 监控 Amazon Lex*[, http://mng.bz/emRq.](http://mng.bz/emRq)
- en: 5.See *CloudWatch Metrics for Amazon Textract*[, http://mng.bz/pzEw.](http://mng.bz/pzEw)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 5.参见 *Amazon Textract 的 CloudWatch 指标*[, http://mng.bz/pzEw.](http://mng.bz/pzEw)
- en: 6.See *CloudWatch Metrics for Rekognition*[, http://mng.bz/OvAa.](http://mng.bz/OvAa)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 6.参见 *Amazon Rekognition 的 CloudWatch 指标*[, http://mng.bz/OvAa.](http://mng.bz/OvAa)
- en: 7.See *Integrating CloudWatch with Amazon Polly*[, http://mng.bz/YxOa.](http://mng.bz/YxOa)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 7.参见 *将 CloudWatch 与 Amazon Polly 集成*[, http://mng.bz/YxOa.](http://mng.bz/YxOa)
- en: 8.See *DynamoDB Metrics and Dimensions*[, http://mng.bz/Gd2J.](http://mng.bz/Gd2J)
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 8.参见 *DynamoDB 指标和维度*[, http://mng.bz/Gd2J.](http://mng.bz/Gd2J)
- en: 9.See *AWS Lambda Metrics*[, http://mng.bz/zrgA.](http://mng.bz/zrgA)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 9.查看 *AWS Lambda 指标*，[http://mng.bz/zrgA](http://mng.bz/zrgA).
- en: 10.AWS Services That Publish CloudWatch Metrics, [http://mng.bz/0Z5v](http://mng.bz/0Z5v).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 10.发布云监控指标的 AWS 服务，[http://mng.bz/0Z5v](http://mng.bz/0Z5v).
- en: 11.AWS JavaScript SDK, putMetricData, [https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CloudWatch.html#putMetricData-property](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CloudWatch.html#putMetricData-property).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 11.AWS JavaScript SDK 的 `putMetricData` 方法，[https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CloudWatch.html#putMetricData-property](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CloudWatch.html#putMetricData-property).
- en: 12.Supported units for CloudWatch Metrics are covered in MetricDatum, [http://mng.bz/9Azr](http://mng.bz/9Azr).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 12.云监控指标支持的单位在 MetricDatum 中介绍，[http://mng.bz/9Azr](http://mng.bz/9Azr).
- en: 13.`aws-embedded-metrics` on GitHub, [https://github.com/awslabs/aws-embedded-metrics-node](https://github.com/awslabs/aws-embedded-metrics-node).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 13.`aws-embedded-metrics` 在 GitHub 上，[https://github.com/awslabs/aws-embedded-metrics-node](https://github.com/awslabs/aws-embedded-metrics-node).
- en: 14.SAR-cloudwatch-alarms-macro from Lumigo, [http://mng.bz/WqeW](http://mng.bz/WqeW).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 14.Lumigo 的 SAR-cloudwatch-alarms-macro，[http://mng.bz/WqeW](http://mng.bz/WqeW).
- en: 15.See Tracing AWS SDK Calls with the X-Ray SDK for Node.js, [http://mng.bz/8GyD](http://mng.bz/8GyD).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 15.使用 Node.js 的 X-Ray SDK 查看 AWS SDK 调用的跟踪，[http://mng.bz/8GyD](http://mng.bz/8GyD).
- en: 16.Add Annotations and Metadata to Segments with the X-Ray SDK for Node.js,
    [http://mng.bz/EEeR](http://mng.bz/EEeR).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 16.使用 Node.js 的 X-Ray SDK 为段添加注释和元数据，[http://mng.bz/EEeR](http://mng.bz/EEeR).
