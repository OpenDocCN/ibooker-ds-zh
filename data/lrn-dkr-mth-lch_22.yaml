- en: 19 Writing and managing application logs with Docker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19 使用Docker编写和管理应用程序日志
- en: 'Logging is usually the most boring part of learning a new technology, but not
    so with Docker. The basic principle is simple: you need to make sure your application
    logs are being written to the standard output stream, because that’s where Docker
    looks for them. There are a couple of ways to achieve that, which we’ll cover
    in this chapter, and then the fun begins. Docker has a pluggable logging framework--you
    need to make sure your application logs are coming out from the container, and
    then Docker can send them to different places. That lets you build a powerful
    logging model, where the application logs from all your containers are sent to
    a central log store with a searchable UI on top of it--all using open source components,
    all running in containers.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志通常是学习新技术中最无聊的部分，但Docker不是这样。基本原理很简单：你需要确保你的应用程序日志被写入标准输出流，因为那是Docker寻找它们的地方。有几个方法可以实现这一点，我们将在本章中介绍，然后乐趣就开始了。Docker有一个可插拔的日志框架——你需要确保你的应用程序日志从容器中输出，然后Docker可以将它们发送到不同的地方。这让你可以构建一个强大的日志模型，其中所有容器的应用程序日志都被发送到一个中央日志存储，并且在其顶部有一个可搜索的用户界面——所有这些都使用开源组件，所有都在容器中运行。
- en: 19.1 Welcome to stderr and stdout!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.1 欢迎来到stderr和stdout！
- en: A Docker image is the snapshot of a filesystem with all your application binaries
    and dependencies, and also some metadata telling Docker which process to start
    when you run a container from the image. That process runs in the foreground,
    so it’s like starting a shell session and then running a command. As long as the
    command is active, it has control of the terminal input and output. Commands write
    log entries to the standard output and standard error streams (called stdout and
    stderr), so in a terminal session you see the output in your window. In a container,
    Docker watches stdout and stderr and collects the output from the streams--that’s
    the source of the container logs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Docker镜像是你应用程序的二进制文件和依赖项的文件系统快照，同时也包含一些元数据，告诉Docker当你从镜像运行容器时应该启动哪个进程。该进程在前台运行，所以就像启动一个shell会话然后运行一个命令一样。只要命令是活跃的，它就控制着终端的输入和输出。命令将日志条目写入标准输出和标准错误流（称为stdout和stderr），所以在终端会话中你会在窗口中看到输出。在容器中，Docker监视stdout和stderr，并从流中收集输出——这就是容器日志的来源。
- en: 'try it now You can see this easily if you run the timecheck app from chapter
    15 in a container. The application itself runs in the foreground and writes log
    entries to stdout:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看！如果你在一个容器中运行第15章中的timecheck应用，你可以很容易地看到这一点。应用程序本身在前台运行，并将日志条目写入stdout：
- en: '` # run the container in the foreground:` ` docker container run diamol/ch15-timecheck:3.0` 
    ` # exit the container with Ctrl-C when you''re done`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '` # 在前台运行容器：` ` docker container run diamol/ch15-timecheck:3.0`  ` # 完成后使用Ctrl-C退出容器`'
- en: You’ll see some log lines in your terminal, and you’ll find you can’t enter
    any more commands--the container is running in the foreground, so it’s just like
    running the app itself in your terminal. Every few seconds the app writes another
    timestamp to stdout, so you’ll see another line in your session window. My output
    is in figure 19.1.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在终端中看到一些日志行，你会发现你无法输入更多命令——容器正在前台运行，所以就像在你的终端中运行应用程序本身一样。每隔几秒钟应用程序就会将另一个时间戳写入stdout，所以你会在会话窗口中看到另一行。我的输出在图19.1中。
- en: '![](../Images/19-1.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-1.jpg)'
- en: Figure 19.1 A container in the foreground takes over the terminal session until
    it exits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.1 前台的容器接管终端会话，直到它退出。
- en: 'This is the standard operating model for containers--Docker starts a process
    inside the container and collects the output streams from that process as logs.
    All the apps we’ve used in this book follow this same pattern: the application
    process runs in the foreground--that could be a Go binary or the Java runtime--and
    the application itself is configured to write logs to stdout (or stderr; Docker
    treats both streams in the same way). Those application logs are written to the
    output stream by the runtime, and Docker collects them. Figure 19.2 shows the
    interaction between the application, the output streams, and Docker.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是容器的标准操作模型——Docker在容器内启动一个进程，并收集该进程的输出流作为日志。我们在这本书中使用的所有应用程序都遵循相同的模式：应用程序进程在前台运行——这可能是一个Go二进制文件或Java运行时——并且应用程序本身被配置为将日志写入stdout（或stderr；Docker以相同的方式处理这两个流）。这些应用程序日志由运行时写入输出流，并由Docker收集。图19.2显示了应用程序、输出流和Docker之间的交互。
- en: '![](../Images/19-2.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-2.jpg)'
- en: Figure 19.2 Docker watches the application process in the container and collects
    its output streams.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.2 Docker监视容器中的应用程序进程并收集其输出流。
- en: Container logs are stored as JSON files, so the log entries remain available
    for detached containers which don’t have a terminal session, and for containers
    that have exited so there is no application process. Docker manages the JSON files
    for you and they have the same life cycle as the container--when the container
    is removed, the log files are removed too.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 容器日志以JSON文件的形式存储，因此日志条目对于没有终端会话的分离容器以及已退出的容器仍然可用。Docker为你管理这些JSON文件，它们具有与容器相同的生命周期--当容器被删除时，日志文件也会被删除。
- en: 'try it now Run a container from the same image in the background as a detached
    container, and check the logs and then the path to the log file:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '现在试试看 Run a container from the same image in the background as a detached container,
    and check the logs and then the path to the log file:'
- en: '` # run a detached container` ` docker container run -d --name timecheck diamol/ch15-timecheck:3.0`
     ` # check the most recent log entry:` ` docker container logs --tail 1 timecheck`
     ` # stop the container and check the logs again:` ` docker container stop timecheck`
    ` docker container logs --tail 1 timecheck`  ` # check where Docker stores the
    container log file:` ` docker container inspect --format=''{{.LogPath}}'' timecheck`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '` # 运行一个分离的容器` ` docker container run -d --name timecheck diamol/ch15-timecheck:3.0`
     ` # 检查最新的日志条目：` ` docker container logs --tail 1 timecheck`  ` # 停止容器并再次检查日志：`
    ` docker container stop timecheck` ` docker container logs --tail 1 timecheck`
     ` # 检查Docker存储容器日志文件的位置：` ` docker container inspect --format=''{{.LogPath}}''
    timecheck`'
- en: If you’re using Docker Desktop with Linux containers, remember that the Docker
    Engine is running inside a VM that Docker manages for you--you can see the path
    to the log file for the container, but you don’t have access to the VM, so you
    can’t read the file directly. If you’re running Docker CE on Linux or you’re using
    Windows containers, the path to the log file will be on your local machine, and
    you can open the file to see the raw contents. You can see my output (using Windows
    containers) in figure 19.3.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是带有Linux容器的Docker Desktop，请记住Docker Engine正在Docker为你管理的VM内部运行--你可以看到容器日志文件的路径，但你无法访问VM，因此无法直接读取文件。如果你在Linux上运行Docker
    CE或使用Windows容器，日志文件的路径将在你的本地机器上，你可以打开文件以查看原始内容。你可以在图19.3中看到我的输出（使用Windows容器）。
- en: '![](../Images/19-3.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-3.jpg)'
- en: Figure 19.3 Docker stores container logs in a JSON file and manages the lifetime
    of that file.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.3 Docker将容器日志存储在JSON文件中并管理该文件的生命周期。
- en: The log file is really just an implementation detail that you don’t usually
    need to worry about. The format is very simple; it contains a JSON object for
    each log entry with the string containing the log, the name of the stream where
    the log came from (stdout or stderr), and a timestamp. Listing 19.1 shows a sample
    of the logs for my timecheck container.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 日志文件实际上只是一个实现细节，你通常不需要担心。其格式非常简单；它包含一个JSON对象，每个日志条目都有一个包含日志的字符串、日志来源的流名称（stdout或stderr）和一个时间戳。列表19.1显示了timecheck容器日志的示例。
- en: Listing 19.1 The raw format for container logs is a simple JSON object
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.1 容器日志的原始格式是一个简单的JSON对象
- en: '` {"log":"Environment: DEV; version: 3.0; time check: 09:42.56\r\n","stream":"stdout","time":"2019-12-19T09:42:56.814277Z"}`
    ` {"log":"Environment: DEV; version: 3.0; time check: 09:43.01\r\n","stream":"stdout","time":"2019-12-19T09:43:01.8162961Z"}`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '` {"log":"环境：DEV；版本：3.0；时间检查：09:42.56\r\n","stream":"stdout","time":"2019-12-19T09:42:56.814277Z"}`
    ` {"log":"环境：DEV；版本：3.0；时间检查：09:43.01\r\n","stream":"stdout","time":"2019-12-19T09:43:01.8162961Z"}`'
- en: The only time you will need to think about the JSON is if you have a container
    that produces lots of logs, and you want to keep all the log entries for a period
    but have them in a manageable file structure. Docker creates a single JSON log
    file for each container by default, and will let it grow to any size (until it
    fills up your disk). You can configure Docker to use rolling files instead, with
    a maximum size limit, so that when the log file fills up, Docker starts writing
    to a new file. You also configure how many log files to use, and when they’re
    all full, Docker starts overwriting the first file. You can set those options
    at the Docker Engine level so the changes apply to every container, or you can
    set them for individual containers. Configuring logging options for a specific
    container is a good way to get small, rotated log files for one application but
    keep all the logs for other containers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在你有一个产生大量日志的容器，并且你希望保留所有日志条目一段时间，但希望它们在一个可管理的文件结构中时，你才需要考虑 JSON。默认情况下，Docker
    为每个容器创建一个单独的 JSON 日志文件，并且允许它增长到任何大小（直到填满你的磁盘）。你可以配置 Docker 使用滚动文件，并设置最大大小限制，这样当日志文件填满时，Docker
    就会开始写入新文件。你还可以配置要使用多少个日志文件，当它们都满了之后，Docker 就会开始覆盖第一个文件。你可以在 Docker 引擎级别设置这些选项，以便更改适用于每个容器，或者你可以为单个容器设置它们。为特定容器配置日志选项是获取一个应用程序的小型轮换日志文件但保留其他容器所有日志的好方法。
- en: 'try it now Run the same app again, but this time specifying log options to
    use three rolling log files with a maximum of 5 KB each:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '现在试试 Run the same app again, but this time specifying log options to use three
    rolling log files with a maximum of 5 KB each:'
- en: '` # run with log options and an app setting to write lots of logs:` ` docker
    container run -d --name timecheck2 --log-opt max-size=5k --log-opt max-file=3
    -e Timer__IntervalSeconds=1 diamol/ch15-timecheck:3.0`  ` # wait for a few minutes`
     ` # check the logs:` ` docker container inspect --format=''{{.LogPath}}'' timecheck2`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '` # 使用日志选项和应用程序设置运行，以写入大量日志：` ` docker container run -d --name timecheck2 --log-opt
    max-size=5k --log-opt max-file=3 -e Timer__IntervalSeconds=1 diamol/ch15-timecheck:3.0`
     ` # 等待几分钟`  ` # 检查日志：` ` docker container inspect --format=''{{.LogPath}}'' timecheck2`'
- en: You’ll see that the log path for the container is still just a single JSON file,
    but Docker is actually rotating log files using that name as the base but with
    a suffix for the log file number. If you’re running Windows containers or Docker
    CE on Linux, you can list the contents of the directory where the logs are kept
    and you’ll see those file suffixes. Mine are shown in figure 19.4.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现容器的日志路径仍然只是一个单一的 JSON 文件，但 Docker 实际上正在使用该名称作为基础，但带有日志文件编号后缀来轮换日志文件。如果你正在运行
    Windows 容器或在 Linux 上运行 Docker CE，你可以列出存储日志的目录的内容，你将看到那些文件后缀。我的后缀如图 19.4 所示。
- en: '![](../Images/19-4.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 19.4](../Images/19-4.jpg)'
- en: Figure 19.4 Rolling log files let you keep a known amount of log data per container.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.4 滚动日志文件允许你为每个容器保留已知数量的日志数据。
- en: There’s a collection and processing stage for the application logs coming from
    stdout, which is where you can configure what Docker does with the logs. In the
    last exercise we configured the log processing to control the JSON file structure,
    and there’s much more you can do with container logs. To take full advantage of
    that, you need to make sure every app is pushing logs out of the container, and
    in some cases that takes a bit more work.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自 stdout 的应用程序日志有一个收集和处理阶段，这是你可以配置 Docker 如何处理日志的地方。在上一个练习中，我们配置了日志处理以控制
    JSON 文件结构，并且你可以对容器日志做更多的事情。为了充分利用这一点，你需要确保每个应用程序都在将日志推送到容器外部，在某些情况下，这需要做更多的工作。
- en: 19.2 Relaying logs from other sinks to stdout
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.2 从其他汇点转发日志到 stdout
- en: Not every app fits nicely with the standard logging model; when you containerize
    some apps, Docker won’t see any logs in the output streams. Some applications
    run in the background as Windows Services or Linux daemons, so the container startup
    process isn’t actually the application process. Other apps might use an existing
    logging framework that writes to log files or other locations (called sinks in
    the logging world), like syslog in Linux or the Windows Event Log. Either way,
    there are no application logs coming from the container start process, so Docker
    doesn’t see any logs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个应用程序都能很好地与标准日志模型兼容；当你将某些应用程序容器化时，Docker 在输出流中看不到任何日志。一些应用程序作为 Windows 服务或
    Linux 守护进程在后台运行，因此容器启动过程实际上并不是应用程序过程。其他应用程序可能使用现有的日志框架，将日志写入日志文件或其他位置（在日志世界中称为“汇”），例如
    Linux 中的 syslogs 或 Windows 事件日志。无论如何，容器启动过程中没有应用程序日志，因此 Docker 看不到任何日志。
- en: 'Try it Now There’s a new version of the timecheck app for this chapter that
    writes logs to a file instead of stdout. When you run this version, there are
    no container logs, although the app logs are being stored in the container filesystem:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 本章节有一个新的timecheck应用程序版本，它将日志写入文件而不是stdout。当您运行这个版本时，没有容器日志，尽管应用程序日志被存储在容器文件系统中：
- en: '` # run a container from the new image:` ` docker container run -d --name timecheck3
    diamol/ch19-timecheck:4.0`  ` # check - there are no logs coming from stdout:`
    ` docker container logs timecheck3`  ` # now connect to the running container,
    for Linux:` ` docker container exec -it timecheck3 sh`  ` # OR windows containers:`
    ` docker container exec -it timecheck3 cmd`  ` # and read the application log
    file:` ` cat /logs/timecheck.log`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '` # 从新镜像运行容器：` ` docker container run -d --name timecheck3 diamol/ch19-timecheck:4.0`
     ` # 检查 - 没有日志从stdout输出：` ` docker container logs timecheck3`  ` # 现在连接到正在运行的容器，对于Linux：`
    ` docker container exec -it timecheck3 sh`  ` # 或者Windows容器：` ` docker container
    exec -it timecheck3 cmd`  ` # 并读取应用程序日志文件：` ` cat /logs/timecheck.log`'
- en: You’ll see that there are no container logs, even though the application itself
    is writing lots of log entries. My output is in figure 19.5--I need to connect
    to the container and read the log file from the container filesystem to see the
    log entries.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您会看到没有容器日志，尽管应用程序本身正在写入大量的日志条目。我的输出在图19.5中——我需要连接到容器并从容器文件系统中读取日志文件以查看日志条目。
- en: '![](../Images/19-5.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-5.jpg)'
- en: Figure 19.5 If the app doesn’t write anything to the output streams, you won’t
    see any container logs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.5 如果应用程序没有向输出流写入任何内容，您将看不到任何容器日志。
- en: This happens because the app is using its own log sink--a file in this exercise--and
    Docker doesn’t know anything about that sink. Docker will only read logs from
    stdout; there’s no way to configure it to read from a different log sink inside
    the container.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为应用程序正在使用它自己的日志接收器——在这个练习中是一个文件，Docker对此接收器一无所知。Docker只会从stdout读取日志；没有方法配置它从容器内的不同日志接收器读取。
- en: The pattern for dealing with apps like this is to run a second process in the
    container startup command, which reads the log entries from the sink that the
    application uses and writes them to stdout. That process could be a shell script
    or a simple utility app, and it is the final process in the start sequence, so
    Docker reads its output stream and the application logs get relayed as container
    logs. Figure 19.6 shows how that works.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 处理此类应用程序的模式是在容器启动命令中运行第二个进程，该进程从应用程序使用的接收器读取日志条目并将它们写入stdout。该进程可以是shell脚本或简单的实用程序应用程序，它是启动序列中的最后一个进程，因此Docker读取其输出流，应用程序日志作为容器日志被转发。图19.6展示了它是如何工作的。
- en: '![](../Images/19-6.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-6.jpg)'
- en: Figure 19.6 You need to package a utility in your container image to relay logs
    from a file.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.6 您需要在容器镜像中打包一个实用工具，以便从文件中转发日志。
- en: 'This is not a perfect solution. Your utility process is running in the foreground,
    so it needs to be robust because if it fails, your container exits, even if the
    actual application is still working in the background. And the reverse is true:
    if the application fails but the log relay keeps running, your container stays
    up even though the app is no longer working. You need health checks in your image
    to prevent that from happening. And lastly, this is not an efficient use of disk,
    especially if your app writes a lot of logs--they’ll be filling up a file in the
    container filesystem and filling up a JSON file on the Docker host machine.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个完美的解决方案。您的实用程序进程在前台运行，因此它需要健壮，因为如果它失败，容器会退出，即使实际的应用程序仍在后台工作。反之亦然：如果应用程序失败但日志转发仍在运行，容器会保持运行，尽管应用程序已经不再工作。您需要在镜像中添加健康检查以防止这种情况发生。最后，这并不是对磁盘的高效使用，尤其是如果您的应用程序写入大量日志——它们会在容器文件系统中填充一个文件，并在Docker主机机器上的JSON文件中填充。
- en: Even so, it’s a useful pattern to know about. If your app runs in the foreground,
    and you can tweak your config to write logs to stdout instead, that’s a better
    approach. But if your app runs in the background, there’s no other option, and
    it’s better to accept the inefficiency and have your app behave like all other
    containers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使如此，了解这个模式也是有用的。如果您的应用程序在前台运行，并且您可以调整配置以将日志写入stdout，那么这是一个更好的方法。但如果您的应用程序在后台运行，就没有其他选择了，并且最好接受低效并让应用程序像所有其他容器一样运行。
- en: There’s an update for the timecheck app in this chapter that adds this pattern,
    building a small utility app to watch the log file and relay the lines to stdout.
    Listing 19.2 shows the final stages of the multi-stage Dockerfile--there are different
    startup commands for Linux and Windows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本章对timecheck应用进行了更新，添加了此模式，构建了一个小型实用程序来监视日志文件并将行传递到stdout。列表19.2显示了多阶段Dockerfile的最终阶段--Linux和Windows有不同的启动命令。
- en: Listing 19.2 Building and packaging a log-relay utility with your app
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.2 使用您的应用程序构建和打包日志中继实用程序
- en: '` # app image` ` FROM diamol/dotnet-runtime AS base` ` ...` ` WORKDIR /app`
    ` COPY --from=builder /out/ .` ` COPY --from=utility /out/ .`  ` # windows` ` FROM
    base AS windows` ` CMD start /B dotnet TimeCheck.dll && dotnet Tail.dll /logs
    timecheck.log`  ` # linux` ` FROM base AS linux` ` CMD dotnet TimeCheck.dll &
    dotnet Tail.dll /logs timecheck.log`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '` # 应用程序镜像` ` FROM diamol/dotnet-runtime AS base` ` ...` ` WORKDIR /app` ` COPY
    --from=builder /out/ .` ` COPY --from=utility /out/ .`  ` # windows` ` FROM base
    AS windows` ` CMD start /B dotnet TimeCheck.dll && dotnet Tail.dll /logs timecheck.log`
     ` # linux` ` FROM base AS linux` ` CMD dotnet TimeCheck.dll & dotnet Tail.dll
    /logs timecheck.log`'
- en: The two `CMD` instructions achieve the same thing, using different approaches
    for the two operating systems. First the .NET application process is started in
    the background, using the `start` command in Windows and suffixing the command
    with a single ampersand `&` in Linux. Then the .NET tail utility is started, configured
    to read the log file the application writes to. The tail utility just watches
    that file and relays each new line as it gets written, so the logs get surfaced
    to stdout and become container logs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 两个`CMD`指令实现了相同的功能，但使用了两种不同的方法来处理两种操作系统。首先，在Windows中使用`start`命令在后台启动.NET应用程序进程，在Linux中在命令后缀一个单
    ampersand `&`。然后启动.NET tail实用程序，配置为读取应用程序写入的日志文件。tail实用程序只是监视该文件，并将每次写入的新行传递出去，因此日志被暴露到stdout并成为容器日志。
- en: 'Try it now Run a container from the new image, and verify that logs are coming
    from the container and that they still get written in the filesystem:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 运行新镜像的容器，并验证日志是否来自容器，并且它们仍然被写入文件系统：
- en: '` # run a container with the tail utility process:` ` docker container run
    -d --name timecheck4 diamol/ch19-timecheck:5.0`  ` # check the logs:` ` docker
    container logs timecheck4`  ` # and connect to the container - on Linux:` ` docker
    container exec -it timecheck4 sh`  ` # OR with Windows containers:` ` docker container
    exec -it timecheck4 cmd`  ` # check the log file:` ` cat /logs/timecheck.log`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '` # 使用tail实用程序进程运行容器：` ` docker container run -d --name timecheck4 diamol/ch19-timecheck:5.0`
     ` # 检查日志：` ` docker container logs timecheck4`  ` # 并连接到容器 - 在Linux上：` ` docker
    container exec -it timecheck4 sh`  ` # 或者使用Windows容器：` ` docker container exec
    -it timecheck4 cmd`  ` # 检查日志文件：` ` cat /logs/timecheck.log`'
- en: Now the logs are coming from the container. It’s a convoluted approach to get
    there, with an extra process running to relay the log file contents to stdout,
    but once the container is running, that’s all transparent. The downside to this
    approach is the extra processing power used by the log relay and the extra disk
    space for storing the logs twice. You can see my output in figure 19.7, which
    shows the log file is still there in the container filesystem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在日志来自容器。这是一个复杂的方法来达到这个目的，需要额外运行一个进程来将日志文件内容传递到stdout，但一旦容器运行，这一切都是透明的。这种方法的不利之处在于日志中继使用的额外处理能力和存储日志所需的额外磁盘空间。您可以在图19.7中看到我的输出，它显示了日志文件仍然在容器文件系统中。
- en: '![](../Images/19-7.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-7.jpg)'
- en: Figure 19.7 A log relay utility gets the application logs out to Docker, but
    uses twice as much disk space.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.7 日志中继实用程序将应用程序日志输出到Docker，但使用了两倍的磁盘空间。
- en: I use a custom utility to relay the log entries in this example, because I want
    the app to work across platforms. I could use the standard Linux `tail` command
    instead, but there’s no Windows equivalent. The custom utility approach is also
    more flexible, because it could read from any sink and relay to stdout. That should
    cover any scenario where your application logs are locked away somewhere in the
    container that Docker doesn’t see.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我使用了一个自定义实用程序来中继日志条目，因为我希望应用程序能够在多个平台上工作。我可以用标准的Linux `tail`命令代替，但没有Windows的等效命令。自定义实用程序方法也更加灵活，因为它可以从任何接收器读取并将数据中继到stdout。这应该涵盖了任何您的应用程序日志被锁定在容器中某个地方，而Docker无法看到的情况。
- en: When you have all your container images set up to write application logs as
    container logs, you can start to make use of Docker’s pluggable logging system
    and consolidate all the logs coming from all your containers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将所有容器镜像配置为以容器日志的形式写入应用程序日志时，你就可以开始利用 Docker 的可插拔日志系统，并整合来自所有容器的所有日志。
- en: 19.3 Collecting and forwarding container logs
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.3 收集和转发容器日志
- en: 'Way back in chapter 2 I talked about how Docker adds a consistent management
    layer over all your apps--it doesn’t matter what’s happening inside the container;
    you start, stop, and inspect everything in the same way. That’s especially useful
    with logs when you bring a consolidated logging system into your architecture,
    and we’ll walk through one of the most popular open source examples of that: Fluentd.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，我谈到了 Docker 如何为所有应用程序添加一个一致的管理层——无论容器内部发生什么；你都可以以相同的方式启动、停止和检查一切。当你在架构中引入一个集中的日志系统时，这尤其有用。我们将通过一个最流行的开源示例来了解这一点：Fluentd。
- en: Fluentd is a unified logging layer. It can ingest logs from lots of different
    sources, filter or enrich the log entries, and then forward them on to lots of
    different targets. It’s a project managed by the Cloud Native Computing Foundation
    (which also manages Kubernetes, Prometheus, and the container runtime from Docker,
    among other projects), and it’s a mature and hugely flexible system. You can run
    Fluentd in a container, and it will listen for log entries. Then you can run other
    containers that use Docker’s Fluentd logging driver instead of the standard JSON
    file, and those container logs will be sent to Fluentd.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 是一个统一的日志层。它可以从许多不同的来源摄取日志，过滤或丰富日志条目，然后将它们转发到许多不同的目标。它是由云原生计算基金会（它还管理
    Kubernetes、Prometheus 以及 Docker 的其他项目，如容器运行时）管理的项目，并且是一个成熟且高度灵活的系统。你可以在容器中运行 Fluentd，它将监听日志条目。然后你可以运行其他容器，这些容器使用
    Docker 的 Fluentd 日志驱动程序而不是标准 JSON 文件，这些容器日志将被发送到 Fluentd。
- en: 'Try it now Fluentd uses a config file to process logs. Run a container with
    a simple configuration that will have Fluentd collect logs and echo them to stdout
    in the container. Then run the timecheck app with that container sending logs
    to Fluentd:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下。Fluentd 使用配置文件来处理日志。运行一个具有简单配置的容器，该配置将使 Fluentd 收集日志并将它们输出到容器的标准输出。然后运行
    timecheck 应用程序，该应用程序使用该容器将日志发送到 Fluentd：
- en: '` cd ch19/exercises/fluentd`  ` # run Fluentd publishing the standard port
    and using a config file:` ` docker container run -d -p 24224:24224 --name fluentd
    -v "$(pwd)/conf:/fluentd/etc" -e FLUENTD_CONF=stdout.conf diamol/fluentd`  ` #
    now run a timecheck container set to use Docker''s Fluentd log driver:` ` docker
    container run -d --log-driver=fluentd --name timecheck5 diamol/ch19-timecheck:5.0`
     ` # check the timecheck container logs:` ` docker container logs timecheck5`
     ` # and check the Fluentd container logs:` ` docker container logs --tail 1 fluentd`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '` cd ch19/exercises/fluentd`  ` # 运行 Fluentd，发布标准端口并使用配置文件：` ` docker container
    run -d -p 24224:24224 --name fluentd -v "$(pwd)/conf:/fluentd/etc" -e FLUENTD_CONF=stdout.conf
    diamol/fluentd`  ` # 现在运行一个设置为使用 Docker Fluentd 日志驱动的 timecheck 容器：` ` docker
    container run -d --log-driver=fluentd --name timecheck5 diamol/ch19-timecheck:5.0`
     ` # 检查 timecheck 容器的日志：` ` docker container logs timecheck5`  ` # 并检查 Fluentd
    容器的日志：` ` docker container logs --tail 1 fluentd`'
- en: You’ll see that you get an error when you try to check logs from the timecheck
    container--not all logging drivers let you see the log entries directly from the
    container. In this exercise they’re being collected by Fluentd, and this configuration
    writes the output to stdout, so you can see the timecheck container’s logs by
    looking at the logs from Fluentd. My output is in figure 19.8.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试从 timecheck 容器检查日志时，你会看到一个错误——并非所有日志驱动程序都允许你直接从容器中查看日志条目。在这个练习中，它们被 Fluentd
    收集，并且这个配置将输出写入标准输出，因此你可以通过查看 Fluentd 的日志来查看 timecheck 容器的日志。我的输出在图 19.8 中。
- en: '![](../Images/19-8.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-8.jpg)'
- en: Figure 19.8 Fluentd collects logs from other containers, and it can store them
    or write to stdout.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.8 显示 Fluentd 从其他容器收集日志，并且它可以存储它们或将它们写入标准输出。
- en: Fluentd adds its own metadata to each record when it stores logs, including
    the container ID and name. This is necessary because Fluentd becomes the central
    log collector for all your containers, and you need to be able to identify which
    log entries came from which application. Using stdout as a target for Fluentd
    is just a simple way to see how everything works. Typically you’d forward logs
    to a central data store. Elasticsearch is a very popular option--it’s a no-SQL
    document database that works well for logs. You can run Elasticsearch in a container
    for log storage and the companion app Kibana, which is a search UI, in another
    container. Figure 19.9 shows what the logging model looks like.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当Fluentd存储日志时，它会为每条记录添加自己的元数据，包括容器ID和名称。这是必要的，因为Fluentd成为所有容器的中央日志收集器，你需要能够识别哪些日志条目来自哪个应用程序。将stdout作为Fluentd的目标只是一个简单的方式来查看一切是如何工作的。通常，你会将日志转发到中央数据存储。Elasticsearch是一个非常流行的选项——它是一个适用于日志的无SQL文档数据库。你可以在容器中运行Elasticsearch进行日志存储，并在另一个容器中运行配套的应用程序Kibana，它是一个搜索用户界面。图19.9显示了日志模型的外观。
- en: '![](../Images/19-9.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-9.jpg)'
- en: Figure 19.9 A centralized logging model sends all container logs to Fluentd
    for processing and storage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.9一个集中式日志模型将所有容器日志发送到Fluentd进行处理和存储。
- en: It looks like a complicated architecture, but as always with Docker, it’s very
    easy to specify all the parts of your logging setup in a Docker Compose file and
    spin up the whole stack with one command. When you have your logging infrastructure
    running in containers, you just need to use the Fluentd logging driver for any
    container where you want to opt in to centralized logging.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来像是一个复杂的架构，但像往常一样，使用Docker，你可以在Docker Compose文件中非常容易地指定所有日志设置的部分，并通过一条命令启动整个堆栈。当你将日志基础设施运行在容器中时，你只需使用Fluentd日志驱动程序来为任何想要加入集中式日志的容器配置。
- en: 'Try it now Remove any running containers and start the Fluentd-Elasticsearch-Kibana
    logging containers. Then run a timecheck container using the Fluentd logging driver:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 移除任何正在运行的容器，并启动Fluentd-Elasticsearch-Kibana日志容器。然后使用Fluentd日志驱动程序运行一个timecheck容器：
- en: '`docker container rm -f $(docker container ls -aq)` `cd ch19/exercises`  `#
    start the logging stack:` `docker-compose -f fluentd/docker-compose.yml up -d` 
    `docker container run -d --log-driver=fluentd diamol/ch19-timecheck:5.0`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker container rm -f $(docker container ls -aq)` `cd ch19/exercises`  `#
    启动日志堆栈：` `docker-compose -f fluentd/docker-compose.yml up -d`  `docker container
    run -d --log-driver=fluentd diamol/ch19-timecheck:5.0`'
- en: Give Elasticsearch a couple of minutes to be ready, and then browse to Kibana
    at http:/ /localhost:5601\. Click the Discover tab, and Kibana will ask for the
    name of the document collection to search against. Enter `fluentd*` as in figure
    19.10.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 给Elasticsearch一点时间准备，然后浏览到Kibana在http:/ /localhost:5601\. 点击Discover标签页，Kibana会要求输入要搜索的文档集合的名称。输入`fluentd*`，如图19.10所示。
- en: '![](../Images/19-10.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-10.jpg)'
- en: Figure 19.10 Elasticsearch stores documents in collections called indexes--Fluentd
    uses its own index.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.10 Elasticsearch将文档存储在名为索引的集合中——Fluentd使用自己的索引。
- en: In the next screen you need to set the field that contains the time filter--select
    `@timestamp` as in figure 19.11.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个屏幕中，你需要设置包含时间过滤器的字段——选择如图19.11所示的`@timestamp`。
- en: '![](../Images/19-11.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-11.jpg)'
- en: Figure 19.11 Fluentd has already saved data in Elasticsearch, so Kibana can
    see the field names.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.11 Fluentd已经将数据保存到Elasticsearch中，因此Kibana可以看到字段名称。
- en: You can automate the Kibana setup, but I haven’t because if you’re new to the
    Elasticsearch stack, it’s worth stepping through it to see how the pieces fit
    together. Every log entry Fluentd collects is saved as a document in Elasticsearch,
    in a document collection that’s named `fluentd-{date}` . Kibana gives you a view
    over all those documents--in the default Discover tab you’ll see a bar chart showing
    how many documents are being created over time, and you can drill into the details
    for individual documents. In this exercise, each document is a log entry from
    the timecheck app. You can see the data in Kibana in figure 19.12.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自动化Kibana的设置，但我还没有这么做，因为如果你是Elasticsearch堆栈的新手，那么逐步了解各个组件是如何组合在一起的会很有价值。Fluentd收集的每个日志条目都保存为Elasticsearch中的一个文档，在一个名为`fluentd-{date}`的文档集合中。Kibana为你提供了所有这些文档的视图——在默认的Discover标签页中，你会看到一个条形图显示随时间创建的文档数量，你可以深入查看单个文档的详细信息。在这个练习中，每个文档都是timecheck应用的日志条目。你可以在图19.12中看到Kibana中的数据。
- en: '![](../Images/19-12.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-12.jpg)'
- en: Figure 19.12 The EFK stack in all its glory--container logs collected and stored
    for simple searching
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.12 EFK堆栈的全貌——收集并存储的容器日志，便于简单搜索
- en: Kibana lets you search across all documents for a specific piece of text, or
    filter documents by date or another data attribute. It also has dashboard functionality
    similar to Grafana, which you saw in chapter 9, so you can build charts showing
    counts of logs per app, or counts of error logs. Elasticsearch is hugely scalable,
    so it’s suitable for large quantities of data in production, and when you start
    sending it all your container logs via Fluentd, you’ll soon find it’s a much more
    manageable approach than scrolling through log lines in the console.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 允许您搜索所有文档中的特定文本，或按日期或其他数据属性过滤文档。它还具有类似于 Grafana 的仪表板功能，您在第 9 章中已经看到，因此您可以构建显示每个应用程序日志计数或错误日志计数的图表。Elasticsearch
    具有巨大的可扩展性，因此适用于生产中的大量数据，当您开始通过 Fluentd 发送所有容器日志时，您很快会发现这比在控制台中滚动日志行要容易管理得多。
- en: 'Try it now Run the image gallery app with each component configured to use
    the Fluentd logging driver:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '现在试试 Run the image gallery app with each component configured to use the Fluentd
    logging driver:'
- en: '` # from the cd ch19/exercises folder` ` docker-compose -f image-gallery/docker-compose.yml
    up -d`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '` # from the cd ch19/exercises folder` ` docker-compose -f image-gallery/docker-compose.yml
    up -d`'
- en: Browse to http:/ /localhost:8010 to generate some traffic, and the containers
    will start writing logs. The Fluentd setup for the image gallery app adds a tag
    to each log, identifying the component that generated it, so log lines can easily
    be identified--more easily than using the container name or container ID. You
    can see my output in figure 19.13\. I’m running the full image gallery application,
    but I’m filtering the logs in Kibana to only show the access-log component--the
    API that records when the app is accessed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览到 http:/ /localhost:8010 生成一些流量，容器将开始写入日志。图像库应用程序的 Fluentd 设置为每个日志添加一个标签，以识别生成它的组件，因此日志行可以轻松识别--比使用容器名称或容器
    ID 更容易识别。您可以在图 19.13 中看到我的输出。我正在运行完整的图像库应用程序，但我正在 Kibana 中过滤日志，只显示 access-log
    组件--记录应用程序访问时间的 API。
- en: '![](../Images/19-13.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-13.jpg)'
- en: Figure 19.13 Logs are being collected in Elasticsearch for the image gallery
    and the timecheck container.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.13 图像库和 timecheck 容器的日志正在 Elasticsearch 中收集。
- en: It’s very easy to add a tag for Fluentd that shows up as the `log_name` field
    for filtering; it’s an option for the logging driver. You can use a fixed name
    or inject some useful identifiers--in this exercise I use `gallery` as the application
    prefix and then add the component name and image name for the container generating
    the logs. That’s a nice way to identify the application, component, and exact
    version running for each log line. Listing 19.3 shows the logging options in the
    Docker Compose file for the image gallery app.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Fluentd 添加一个标签非常简单，它会显示为 `log_name` 字段以进行过滤；这是日志驱动程序的一个选项。您可以使用一个固定名称或注入一些有用的标识符--在这个练习中，我使用
    `gallery` 作为应用程序前缀，然后添加生成日志的组件名称和镜像名称。这是一种识别应用程序、组件以及每行日志的确切版本的好方法。列表 19.3 展示了图像库应用程序
    Docker Compose 文件中的日志选项。
- en: Listing 19.3 Using a tag to identify the source of log entries for Fluentd
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 19.3 使用标签识别 Fluentd 的日志条目来源
- en: '` services:` `   accesslog:` `       image: diamol/ch18-access-log` `                 logging:`
    `           driver: "fluentd"` `           options:` `               tag: " gallery.access-log.{{.ImageName}}"` 
    `   iotd:` `       image: diamol/ch18-image-of-the-day` `       logging:` `           driver:
    "fluentd"` `           options:` `               tag: "gallery.iotd.{{.ImageName}}"` 
    `   image-gallery:` `       image: diamol/ch18-image-gallery` `       logging:`
    `           driver: "fluentd"` `           options:` `               tag: "gallery.image-gallery.{{.ImageName}}"`
    ` ...`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '` services:` `   accesslog:` `       image: diamol/ch18-access-log` `                 logging:`
    `           driver: "fluentd"` `           options:` `               tag: " gallery.access-log.{{.ImageName}}"` 
    `   iotd:` `       image: diamol/ch18-image-of-the-day` `       logging:` `           driver:
    "fluentd"` `           options:` `               tag: "gallery.iotd.{{.ImageName}}"` 
    `   image-gallery:` `       image: diamol/ch18-image-gallery` `       logging:`
    `           driver: "fluentd"` `           options:` `               tag: "gallery.image-gallery.{{.ImageName}}"`
    ` ...`'
- en: The model for centralized logging with a searchable data store and a user-friendly
    UI is one you should definitely consider when you’re getting containers ready
    for production. You’re not limited to using Fluentd--there are many other logging
    drivers for Docker, so you could use other popular tools like Graylog, or commercial
    tools like Splunk. Remember, you can set the default logging driver and options
    at the Engine level in the Docker config, but I think there’s value in doing it
    in the application manifests instead--it makes it clear which logging system you’re
    using in each environment.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当您为生产准备容器时，应考虑使用可搜索的数据存储和用户友好的UI的集中式日志记录模型。您不仅限于使用Fluentd--Docker有许多其他日志驱动程序，因此您可以使用其他流行的工具，如Graylog，或商业工具，如Splunk。记住，您可以在Docker配置的引擎级别设置默认日志驱动程序和选项，但我认为在应用程序清单中这样做更有价值--它清楚地说明了每个环境中使用的日志系统。
- en: Fluentd is a good option if you don’t already have an established logging system.
    It’s easy to use and it scales from a single dev machine to a full production
    cluster, and you use it in the same way in every environment. You can also configure
    Fluentd to enrich the log data to make it easier to work with, and to filter logs
    and send them to different targets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有建立日志系统，Fluentd是一个不错的选择。它易于使用，可以从单个开发机器扩展到完整的生产集群，并且您可以在每个环境中以相同的方式使用它。您还可以配置Fluentd来丰富日志数据，使其更容易处理，并过滤日志将它们发送到不同的目标。
- en: 19.4 Managing your log output and collection
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.4 管理您的日志输出和收集
- en: Logging is a delicate balance between capturing enough information to be useful
    in diagnosing problems and not storing huge quantities of data. Docker’s logging
    model gives you some additional flexibility to help with the balance, because
    you can produce container logs at a more verbose level than you expect to use,
    but filter them out before you store them. Then if you need to see more verbose
    logs, you can alter the filter configuration rather than your app configuration
    so the Fluentd containers get replaced rather than your app containers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志是在捕获足够的信息以在诊断问题时有用和不过度存储大量数据之间保持微妙的平衡。Docker的日志模型为您提供了额外的灵活性来帮助平衡，因为您可以在存储之前以比预期更详细的级别生成容器日志，但过滤掉它们。然后，如果您需要查看更详细的日志，您可以通过更改过滤配置而不是应用程序配置来更改Fluentd容器而不是应用程序容器。
- en: You can configure this level of filtering in the Fluentd config file. The configuration
    from the last exercise sends all logs to Elasticsearch, but the updated configuration
    in listing 19.4 filters out logs from the more verbose access-log component. Those
    logs go to stdout, and the rest of the app logs go to Elasticsearch.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Fluentd配置文件中配置此级别的过滤。上一个练习中的配置将所有日志发送到Elasticsearch，但列表19.4中更新的配置过滤掉了来自更详细的access-log组件的日志。这些日志将发送到stdout，其余的应用程序日志将发送到Elasticsearch。
- en: Listing 19.4 Sending log entries to different targets based on the tag of the
    record
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表19.4 根据记录的标签将日志条目发送到不同的目标
- en: '` <match gallery.access-log.**>` `   @type copy` `   <store>` `       @type
    stdout` `   </store>` ` </match>` ` <match gallery.**>` `   @type copy` `   <store>`
    `       @type elasticsearch` ` ...`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '` <match gallery.access-log.**>` `   @type copy` `   <store>` `       @type
    stdout` `   </store>` ` </match>` ` <match gallery.**>` `   @type copy` `   <store>`
    `       @type elasticsearch` ` ...`'
- en: The `match` blocks tell Fluentd what to do with log records, and the filter
    parameter uses the tag that is set in the logging driver options. When you run
    this updated configuration, the access-log entries will match the first match
    block, because the tag prefix is `gallery.access-log` . Those records will stop
    surfacing in Elasticsearch and will only be available by reading the logs of the
    Fluentd container. The updated config file also enriches all log entries, splitting
    the tag into separate fields for app name, service name, and image name, which
    makes filtering in Kibana much easier.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`match`块告诉Fluentd如何处理日志记录，而filter参数使用在日志驱动程序选项中设置的标签。当您运行此更新后的配置时，access-log条目将匹配第一个match块，因为标签前缀是`gallery.access-log`。这些记录将不再在Elasticsearch中显示，并且只能通过读取Fluentd容器的日志来获取。更新后的配置文件还丰富了所有日志条目，将标签拆分为单独的字段，用于应用程序名称、服务名称和镜像名称，这使得在Kibana中进行过滤变得更容易。'
- en: 'Try it now Update the Fluentd configuration by deploying a Docker Compose override
    file that specifies a new config file, and update the image gallery application
    to generate more verbose logs:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 更新Fluentd配置，通过部署一个指定新配置文件的Docker Compose覆盖文件，并更新图像库应用程序以生成更详细的日志：
- en: '` # update the Fluentd config:` ` docker-compose -f fluentd/docker-compose.yml
    -f fluentd/override-gallery-filtered.yml up -d`  ` # update the application logging
    config:` ` docker-compose -f image-gallery/docker-compose.yml -f image-gallery/override-logging.yml
    up -d`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '` # 更新Fluentd配置：` ` docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery-filtered.yml
    up -d`  ` # 更新应用程序日志配置：` ` docker-compose -f image-gallery/docker-compose.yml
    -f image-gallery/override-logging.yml up -d`'
- en: You can check the contents of those override files, and you’ll see they just
    specify config settings for the applications; all the images are the same. Now
    when you use the app at http:/ /localhost:8010, the access-log entries are still
    generated, but they get filtered out by Fluentd so you won’t see any new logs
    in Kibana. You will see the logs from the other components, and these are enriched
    with the new metadata fields. You can see that in my output in figure 19.14.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查这些覆盖文件的内容，您会看到它们只是指定了应用程序的配置设置；所有图像都是相同的。现在当您使用http:/ /localhost:8010的应用程序时，访问日志条目仍然会生成，但它们会被Fluentd过滤掉，因此您在Kibana中不会看到任何新的日志。您将看到其他组件的日志，并且这些日志被新的元数据字段丰富。您可以在图19.14的输出中看到这一点。
- en: '![](../Images/19-14.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-14.jpg)'
- en: Figure 19.14 Fluentd uses the tag in the log to filter out records and to generate
    new fields.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.14 Fluentd使用日志中的标签来过滤记录并生成新字段。
- en: The access-log entries are still available because they’re writing to stdout
    inside the Fluentd container. You can see them as container logs--but from the
    Fluentd container, not the access-log container.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 访问日志条目仍然可用，因为它们在Fluentd容器内部写入stdout。您可以将它们视为容器日志--但它们来自Fluentd容器，而不是访问日志容器。
- en: 'Try it now Check the Fluentd container logs to be sure the records are still
    available:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试吧 检查Fluentd容器日志以确保记录仍然可用：
- en: '` docker container logs --tail 1 fluentd_fluentd_1`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '` docker container logs --tail 1 fluentd_fluentd_1`'
- en: 'You can see my output in figure 19.15\. The access-log entry has been sent
    to a different target, but it has still been through the same processing to enrich
    the record with the app, service, and image name:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图19.15中查看我的输出。访问日志条目已发送到不同的目标，但它仍然经过了相同的处理，以应用、服务和图像名称丰富记录：
- en: '![](../Images/19-15.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-15.jpg)'
- en: Figure 19.15 These logs are filtered so they’re not stored in Elasticsearch
    but are echoed to stdout.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.15 这些日志被过滤，因此它们不会存储在Elasticsearch中，而是回显到stdout。
- en: This is a nice way of separating core application logs from nice-to-have logs.
    You wouldn’t use stdout in production, but you might have different outputs for
    different classes of logs--performance critical components could send log entries
    to Kafka, user-facing logs could go to Elasticsearch, and the rest could be filed
    in Amazon S3 cloud storage. Those are all supported log stores in Fluentd.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种将核心应用程序日志与希望拥有的日志分开的好方法。在生产中您不会使用stdout，但您可能对不同类别的日志有不同的输出--性能关键组件可以将日志条目发送到Kafka，面向用户的日志可以发送到Elasticsearch，其余的可以存储在Amazon
    S3云存储中。这些都是Fluentd支持的日志存储。
- en: There’s one final exercise for this chapter to reset the logging and put access-log
    entries back into Elasticsearch. This approximates a situation in production where
    you find a system problem and you want to increase the logs to see what’s happening.
    With the logging setup we have, the logs are already being written by the app.
    We can surface them just by changing the Fluentd configuration file.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有一个最后的练习来重置日志并将访问日志条目放回Elasticsearch。这近似于生产环境中您发现系统问题并希望增加日志以查看发生了什么的情况。在我们的日志设置中，日志已经被应用程序写入。我们只需更改Fluentd配置文件就可以暴露它们。
- en: 'Try it now Deploy a new Fluentd configuration that sends access-log records
    to Elasticsearch:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在部署一个新的Fluentd配置，将访问日志记录发送到Elasticsearch：
- en: '` docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery.yml
    up -d`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '` docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery.yml
    up -d`'
- en: This deployment uses a configuration file that removes the `match` block for
    access-log records, so all the gallery component logs get stored in Elasticsearch.
    When you refresh the image gallery page in your browser, the logs will get collected
    and stored. You can see my output in figure 19.16, where the most recent logs
    are shown from both the API and the access-log components.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署使用一个配置文件，移除了访问日志记录的`match`块，因此所有画廊组件的日志都存储在Elasticsearch中。当您在浏览器中刷新图像画廊页面时，日志将被收集并存储。您可以在图19.16中查看我的输出，其中显示了API和访问日志组件的最新日志。
- en: '![](../Images/19-16.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19-16.jpg)'
- en: Figure 19.16 A change to the Fluentd config adds logs back into Elasticsearch
    without any app changes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.16 对Fluentd配置的更改在不更改应用程序的情况下将日志重新添加到Elasticsearch中。
- en: You do need to be aware that there’s the potential for lost log entries with
    this approach. During the deployment, containers could be sending logs when there’s
    no Fluentd container running to collect them. Docker continues gracefully in that
    situation, and your app containers keep running, but the log entries don’t get
    buffered so they’ll be lost. It’s unlikely to be a problem in a clustered production
    environment, but even if it did happen, it’s preferable to restarting an app container
    with increased logging configuration--not least because the new container may
    not have the same issue as the old container, so your new logs won’t tell you
    anything interesting.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要意识到，使用这种方法可能会丢失日志条目。在部署期间，容器可能会在没有任何Fluentd容器运行来收集它们的情况下发送日志。Docker在这种情况下会优雅地继续运行，你的应用程序容器也会继续运行，但日志条目不会被缓冲，因此它们将会丢失。在集群化生产环境中，这不太可能成为问题，但即使发生了这种情况，也比重新启动一个具有增加日志配置的应用程序容器更可取——至少因为新的容器可能不会像旧容器那样有问题，所以你的新日志不会告诉你任何有趣的事情。
- en: 19.5 Understanding the container logging model
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.5 理解容器日志模型
- en: The logging approach in Docker is super-flexible, but only when you make your
    application logs visible as container logs. You can do that directly by having
    your app write logs to stdout, or indirectly by using a relay utility in your
    container that copies log entries to stdout. You need to spend some time making
    sure all your application components write container logs, because once you’ve
    got that working, you can process the logs however you like.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Docker中的日志方法非常灵活，但前提是你必须使你的应用程序日志作为容器日志可见。你可以通过让应用程序直接将日志写入stdout来实现，或者通过在容器中使用一个中继工具间接地复制日志条目到stdout。你需要花一些时间确保所有应用程序组件都写入容器日志，因为一旦你做到了这一点，你就可以按自己的喜好处理日志。
- en: We used the EFK stack in this chapter--Elasticsearch, Fluentd, and Kibana--and
    you’ve seen how easy it is to pull all your container logs into a centralized
    database with a user-friendly search UI. All those technologies are swappable,
    but Fluentd is one of the most used because it’s so simple and so powerful. That
    stack runs nicely in single-machine environments, and it scales for production
    environments too. Figure 19.17 shows how a clustered environment runs a Fluentd
    container on each node, where the Fluentd container collects logs from the other
    containers on that node and sends them to an Elasticsearch cluster--also running
    in containers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了EFK堆栈——Elasticsearch、Fluentd和Kibana——你已经看到了如何轻松地将所有容器日志拉入一个具有用户友好搜索界面的集中式数据库。所有这些技术都是可互换的，但Fluentd是最常用的，因为它既简单又强大。这个堆栈在单机环境中运行良好，也可以扩展到生产环境。图19.17显示了集群化环境中每个节点上运行Fluentd容器的情况，其中Fluentd容器收集该节点上其他容器的日志并将它们发送到Elasticsearch集群——Elasticsearch也运行在容器中。
- en: '![](../Images/19-17.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/19-17.jpg)'
- en: Figure 19.17 The EFK stack works in production with clustered storage and multiple
    Fluentd instances.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.17 EFK堆栈在生产环境中使用集群存储和多个Fluentd实例工作。
- en: I’ll finish with a note of caution before we move on to the lab. Some teams
    don’t like all the processing layers in the container logging model; they prefer
    to write application logs directly to the final store, so instead of writing to
    stdout and having Fluentd send data to Elasticsearch, the application writes directly
    to Elasticsearch. I really don’t like that approach. You save some processing
    time and network traffic in exchange for a complete lack of flexibility. You’ve
    hardcoded the logging stack into all your applications, and if you want to switch
    to Graylog or Splunk, you need to go and rework your apps. I always prefer to
    keep it simple and flexible--write your application logs to stdout and make use
    of the platform to collect, enrich, filter, and store the data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入实验室之前，我要提醒大家注意一点。有些团队不喜欢容器日志模型中的所有处理层；他们更愿意直接将应用程序日志写入最终存储，因此，应用程序不是写入stdout并通过Fluentd将数据发送到Elasticsearch，而是直接写入Elasticsearch。我真的很不喜欢这种方法。你虽然节省了一些处理时间和网络流量，但代价是完全缺乏灵活性。你已经将日志堆栈硬编码到所有应用程序中，如果你想切换到Graylog或Splunk，你需要去重新修改你的应用程序。我总是更喜欢保持简单和灵活——将你的应用程序日志写入stdout，并利用平台来收集、丰富、过滤和存储数据。
- en: 19.6 Lab
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 19.6 实验室
- en: 'I didn’t focus too much on configuring Fluentd in this chapter, but it’s worth
    getting some experience setting that up, so I’m going to ask you to do it in the
    lab. In the lab folder for this chapter there’s a Docker Compose file for the
    random number app and a Docker Compose file for the EFK stack. The app containers
    aren’t configured to use Fluentd, and the Fluentd setup doesn’t do any enrichment,
    so you have three tasks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我没有过多关注 Fluentd 的配置，但获得一些设置经验是值得的，所以我将要求你在实验室中完成这个任务。在本章的实验室文件夹中，有一个用于随机数字应用的
    Docker Compose 文件和一个用于 EFK 堆栈的 Docker Compose 文件。应用容器未配置为使用 Fluentd，Fluentd 设置也不进行任何丰富化，因此你有三个任务：
- en: Extend the Compose file for the numbers app so all the components use the Fluentd
    logging driver, and set a tag with the app name, service name, and image.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展 numbers 应用的 Compose 文件，以便所有组件都使用 Fluentd 日志驱动程序，并设置一个包含应用名称、服务名称和镜像的标签。
- en: Extend the Fluentd configuration file, `elasticsearch.conf` , to split the tag
    into app name, service name, and image name fields for all records from the numbers
    app.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Fluentd 的配置文件 `elasticsearch.conf` 扩展，以便将标签拆分为应用名称、服务名称和镜像名称字段，用于所有来自 numbers
    应用的记录。
- en: Add a failsafe `match` block to the Fluentd configuration so any records that
    aren’t from the numbers app get forwarded to stdout.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Fluentd 配置中添加一个安全失败的 `match` 块，以便将所有不是来自 numbers 应用的记录转发到 stdout。
- en: 'No hints for this one, because this is a case of working through the config
    setup for the image gallery app and seeing which pieces you need to add for the
    numbers app. As always, my solution is up on GitHub for you to check: *[https://github.com/sixeyed/
    diamol/blob/master/ch19/lab/README.md](https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md)*
    .'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一部分没有提示，因为这是一个通过配置图像库应用进行配置设置并查看需要为 numbers 应用添加哪些组件的案例。一如既往，我的解决方案已上传到 GitHub
    供您检查：[https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md](https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md)。
