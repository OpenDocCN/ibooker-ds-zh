- en: front matter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前置内容
- en: preface
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: While computers have been getting more powerful and more capable of chewing
    though larger data sets, our appetite for consuming data grows much faster. Consequently,
    we built new tools to scale big data jobs across multiple machines. This does
    not come for free, and early tools were complicated by requiring users to manage
    not only the data program, but also the health and performance of the cluster
    of machines themselves. I recall trying to scale my own programs, only to be faced
    with the advice to “just sample your data set and get on with your day.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算机变得越来越强大，处理大量数据集的能力也越来越强，但我们消费数据的需求增长得更快。因此，我们开发了新的工具来跨多台机器扩展大数据作业。这并非免费，早期的工具因为需要用户管理不仅数据程序，还要管理机器集群的健康和性能而变得复杂。我记得我试图扩展自己的程序，结果得到的建议是“只采样你的数据集，然后继续你的日子。”
- en: PySpark changes the game. Starting with the popular Python programming language,
    it provides a clear and readable API to manipulate very large data sets. Still,
    while in the driver’s seat, you write code as if you were dealing with a single
    machine. PySpark sits at the intersection of powerful, expressive, and versatile.
    Through a powerful multidimensional data model, you can build your data programs
    with a clear path to scalability, regardless of the data size.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark改变了游戏规则。从流行的Python编程语言开始，它提供了一个清晰易读的API来操作非常大的数据集。尽管如此，当你在驾驶员座位上时，你编写的代码就像你在处理一台单机一样。PySpark位于强大、表达和灵活的交汇点。通过强大的多维数据模型，你可以构建你的数据程序，并有一个清晰的路径来实现可扩展性，无论数据大小如何。
- en: 'I fell in love with PySpark while working as a data scientist for building
    credit risk models. On the cusp of migrating our models to a new big data environment,
    we needed to devise a plan to intelligently convert our data products while “keeping
    the lights on.” As the self-appointed Python guy, I got tasked to help the team
    become familiar with PySpark and help accelerate the transition. This love grew
    exponentially as I got the chance to work with a myriad of clients on different
    use cases. The common thread? Big data and big problems, all solvable through
    a powerful data model. One caveat: most of the material available for learning
    Spark focused on Scala and Java, with Python developers left transliterating the
    code to their favorite programming language. I started writing this book to promote
    PySpark as a great tool for data analysts. In a fortunate turn of events, the
    Spark project really promoted Python as a first-class citizen. Now, more than
    ever, you have a powerful tool for scaling your data programs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在作为构建信用风险模型的数据科学家工作时，我爱上了PySpark。在我们即将将模型迁移到新的大数据环境之际，我们需要制定一个计划，在“保持灯火通明”的同时智能地转换我们的数据产品。作为自封的Python专家，我被指派去帮助团队熟悉PySpark并加速过渡。随着我有机会与众多客户在不同用例上合作，这种爱以指数级增长。共同的主题？大数据和大数据问题，所有这些问题都可以通过强大的数据模型解决。有一个注意事项：大多数用于学习Spark的材料都集中在Scala和Java上，Python开发者不得不将代码转译到他们喜欢的编程语言。我开始写这本书是为了推广PySpark作为数据分析的优秀工具。幸运的是，Spark项目真的将Python提升为一等公民。现在，比以往任何时候，你都有一个强大的工具来扩展你的数据程序。
- en: And big data, once tamed, really feels powerful.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦大数据被驯服，真的感觉非常强大。
- en: acknowledgments
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Although my name is on the cover, this book has been a tremendous team effort,
    and I want to take the time to thank those who helped me along the way.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我的名字在封面上，但这本书是一个巨大的团队努力的结果，我想花时间感谢那些在过程中帮助我的人。
- en: First and foremost, I want to thank my family. Writing a book is a lot of work,
    and with this work comes a lot of complaining. Simon, Catherine, Véronique, Jean,
    *merci du fond du coeur pour votre soutien. Je vous aime énormément*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我要感谢我的家人。写一本书是一项繁重的工作，随之而来的是大量的抱怨。西蒙、凯瑟琳、维罗妮克、让，衷心感谢你们的支持。我非常爱你们。
- en: Regina, in a way, you’ve were my very first PySpark student. Through your leadership,
    you literally changed everything for me career-wise. I will forever cherish the
    time we worked together, and I feel lucky our paths crossed when they did.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 瑞吉娜，从某种意义上说，你是我的第一位PySpark学生。通过你的领导，你实际上改变了我职业上的所有事情。我将永远珍惜我们一起工作的时光，并且我觉得很幸运我们的道路在那时交汇。
- en: I want to thank Renata Pompas, who allowed me to use a color palette made under
    her supervision for the diagrams in my book. I am color-blind, and finding a set
    of safe colors to use that would please me and be consistent was helpful during
    book development. If the figures look good to you, thank her (and the fine Manning
    graphic designers). If they look bad, blame it on me.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢Renata Pompas，她允许我使用在她的监督下制作的调色板来装饰我书中的图表。我色盲，找到一组既让我满意又保持一致的安全颜色在书的发展过程中非常有帮助。如果你觉得这些图表看起来不错，请感谢她（以及优秀的Manning图形设计师）。如果它们看起来不好，那就怪我吧。
- en: Thank you to my team at EPAM, with a special shout-out to Zac, James, Nasim,
    Vahid, Dmitrii, Yuriy, Val, Robert, Aliaksandra, Ihor, Pooyan, Artem, Volha, Ekaterina,
    Sergey, Sergei, Siarhei, Kseniya, Artemii, Anatoly, Yuliya, Nadzeya, Artsiom,
    Denis, Yevhen, Sofiia, Roman, Mykola, Lisa, Gaurav, Megan, and so many more. From
    the day I announced that I was writing a book to when I wrote these words, I felt
    supported and encouraged. Thank you to the Laivly team, Jeff, Rod, Craig, Jordan,
    Abu, Brendan, Daniel, Guy, and Reid, for the opportunity to continue the adventure.
    I promise you that the future is bright.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢EPAM团队，特别感谢Zac、James、Nasim、Vahid、Dmitrii、Yuriy、Val、Robert、Aliaksandra、Ihor、Pooyan、Artem、Volha、Ekaterina、Sergey、Sergei、Siarhei、Kseniya、Artemii、Anatoly、Yuliya、Nadzeya、Artsiom、Denis、Yevhen、Sofiia、Roman、Mykola、Lisa、Gaurav、Megan以及更多。从我宣布我要写一本书到写下这些话的时候，我感受到了支持和鼓励。感谢Laivly团队，Jeff、Rod、Craig、Jordan、Abu、Brendan、Daniel、Guy和Reid，感谢你们给我继续冒险的机会。我向你们保证，未来是光明的。
- en: A warm thank you to those who believed in my “use PySpark, you’ll be grateful
    you did” mantra. There are too many folks to be exhaustive here, but I want to
    give a shout out to Mark Derry, Uma Gopinath, Tom Everett, Dhrun Lauwers, Milena
    Kumurdjieva, Shahid Amlani, Sam Diab, Chris Wagner, JV Eng, Chris Purtill, Naveen
    Pothayath, Vish Tipirneni, and Patrick Kurkiewicz.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 热烈感谢那些相信我的“使用PySpark，你会感激你这么做”的口号的人。这里的人太多，无法一一列举，但我想向Mark Derry、Uma Gopinath、Tom
    Everett、Dhrun Lauwers、Milena Kumurdjieva、Shahid Amlani、Sam Diab、Chris Wagner、JV
    Eng、Chris Purtill、Naveen Pothayath、Vish Tipirneni和Patrick Kurkiewicz表示敬意。
- en: 'During the writing of the book, I had the joy to geek out on PySpark with some
    fine podcast producers: Brian at Test and Code ([https://testandcode.com/](https://testandcode.com/)),
    Lior and Michael at *WHAT the Data?!* ([https://podcast.whatthedatapodcast.com/](https://podcast.whatthedatapodcast.com/)),
    and Ben at *Profitable Python* ([https://anchor.fm/profitablepythonfm](https://anchor.fm/profitablepythonfm)).
    I am so humbled and grateful that you invited me to exchange with you. Thank you
    Alexey Grigorev for having me in your Book of the Week club on Slack—what an awesome
    community you’ve built!'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在写书的过程中，我有幸与一些优秀的播客制作人一起深入研究PySpark：Brian在Test and Code ([https://testandcode.com/](https://testandcode.com/))，Lior和Michael在*WHAT
    the Data?!* ([https://podcast.whatthedatapodcast.com/](https://podcast.whatthedatapodcast.com/))，以及Ben在*Profitable
    Python* ([https://anchor.fm/profitablepythonfm](https://anchor.fm/profitablepythonfm))。我非常谦卑和感激你们邀请我加入你们的交流。感谢Alexey
    Grigorev让我加入你在Slack上的“每周一书”俱乐部——你们建立了多么棒的社区！
- en: 'I want to thank readers who provided comments on the manuscript during development,
    as well as the reviewers who provided excellent feedback: Alex Lucas, David Cronkite,
    Dianshuang Wu, Gary Bake, Geoff Clark, Gustavo Patino, Igor Vieira, Javier Collado
    Cabeza, Jeremy Loscheider, Josh Cohen, Kay Engelhardt, Kim Falk, Michael Kareev,
    Mike Jensen, Patrick A. Mol, Paul Fornia, Peter Hampton, Philippe Van Bergen,
    Rambabu Posa, Raushan Jha, Sergio Govoni, Sriram Macharla, Stephen Oates, and
    Werner Nindl.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢在书的发展过程中提供评论的读者，以及提供出色反馈的审稿人：Alex Lucas、David Cronkite、Dianshuang Wu、Gary
    Bake、Geoff Clark、Gustavo Patino、Igor Vieira、Javier Collado Cabeza、Jeremy Loscheider、Josh
    Cohen、Kay Engelhardt、Kim Falk、Michael Kareev、Mike Jensen、Patrick A. Mol、Paul Fornia、Peter
    Hampton、Philippe Van Bergen、Rambabu Posa、Raushan Jha、Sergio Govoni、Sriram Macharla、Stephen
    Oates和Werner Nindl。
- en: 'Finally, and most importantly, I want to thank the dream team at Manning that
    participated in making this book a reality. There are many folks who made this
    experience incredible: Marjan Bace, Michael Stephens, Rebecca Rinehart, Bert Bates,
    Candace Gillhoolley, Radmila Ercegovac, Aleks Dragosavljević, Matko Hrvatin, Christopher
    Kaufmann, Ana Romac, Branko Latinčić, Lucas Weber, Stjepan Jureković, Goran Ore,
    Keri Hales, Michele Mitchell, Melody Dolab, and the rest of the Manning production
    team.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也是最重要的一点，我要感谢Manning的梦幻团队，他们参与了将这本书变为现实。有许多人让这次经历变得难以置信：Marjan Bace、Michael
    Stephens、Rebecca Rinehart、Bert Bates、Candace Gillhoolley、Radmila Ercegovac、Aleks
    Dragosavljević、Matko Hrvatin、Christopher Kaufmann、Ana Romac、Branko Latinčić、Lucas
    Weber、Stjepan Jureković、Goran Ore、Keri Hales、Michele Mitchell、Melody Dolab以及Manning制作团队的其余成员。
- en: 'Speaking of Manning, I want to thank the authors of two specific books: Noel
    Rappin and Robin Dunn from *wxPython in Action* (Manning, 2016; [https://www.manning.com/books/wxpython-in-action](https://www.manning.com/books/wxpython-in-action)),
    as well as Michael Fogus and Chris Houser from *The Joy of Clojure* (Manning,
    2014; [https://www.manning.com/books/the-joy-of-clojure-second-edition](https://www.manning.com/books/the-joy-of-clojure-second-edition)).
    These books triggered something in my brain and made me plunge headfirst into
    programming (and then data science). In a way, they were the initial spark (bad
    pun intended) that resulted in this book.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 说到 Manning，我想感谢两本书的作者：来自 *《wxPython in Action》* 的诺埃尔·拉平（Noel Rappin）和罗宾·邓恩（Robin
    Dunn）（Manning，2016；[https://www.manning.com/books/wxpython-in-action](https://www.manning.com/books/wxpython-in-action)），以及来自
    *《Clojure 的乐趣》* 的迈克尔·福格斯（Michael Fogus）和克里斯·豪泽（Chris Houser）（Manning，2014；[https://www.manning.com/books/the-joy-of-clojure-second-edition](https://www.manning.com/books/the-joy-of-clojure-second-edition)）。这些书在我的脑海中触发了某种东西，让我一头扎进了编程（然后是数据科学）。从某种意义上说，它们是这本书产生的最初火花（这里有点糟糕的打趣）。
- en: Finally, I want to highlight the team at Manning that helped me stay accountable
    on a day-to-day basis and made this book something I am proud of. Arthur Zubarev,
    I can’t believe we live in the same city and couldn’t meet! Thank you for your
    excellent feedback and answering my many questions. Alex Ott, I don’t think I
    could have wished for a better technical advisor. Databricks is incredibly lucky
    to have you. Last, but certainly not least, I want to thank Marina Michaels for
    supporting me from the moment I had the idea of writing this book. Writing a book
    is a lot more difficult than I originally thought, but you made the whole experience
    enjoyable, formative, and relevant. Thank you from the bottom of my heart.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想强调的是，Manning 团队帮助我在日常工作中保持责任感，使这本书成为我引以为傲的作品。亚瑟·祖巴列夫，我简直不敢相信我们住在同一个城市，却没能见面！感谢你提供的宝贵反馈和回答我提出的许多问题。亚历克斯·奥特，我认为我无法期望更好的技术顾问。Databricks
    非常幸运有你。最后，但同样重要的是，我要感谢玛丽娜·迈克尔斯，从我产生写这本书的想法的那一刻起，她就一直支持我。写一本书比我想象的要难得多，但你让整个过程变得愉快、有教育意义且相关。衷心感谢你。
- en: about this book
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于这本书
- en: '*Data Analysis with Python and PySpark* teaches you how to use PySpark to conduct
    your own big data analysis programs. It takes a practical stance on teaching both
    the how and why of PySpark. You’ll learn about how to effectively ingest, process,
    and work with data at scale as well as how to reason about your own data transformation
    code. After reading this book, you should feel comfortable using PySpark to write
    your own data programs and analyses.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*《使用 Python 和 PySpark 进行数据分析》* 教你如何使用 PySpark 来进行自己的大数据分析程序。本书在教授 PySpark 的如何以及为什么方面采取了实用主义立场。你将学习如何有效地摄取、处理和大规模工作数据，以及如何推理自己的数据转换代码。阅读完这本书后，你应该能够舒适地使用
    PySpark 来编写自己的数据程序和分析。'
- en: Who should read this book
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应该阅读这本书的人
- en: This book is structured around increasingly complicated use cases, moving from
    simple data transformation to machine learning pipelines. We cover the whole cycle,
    from data ingestion to results consumption, adding more elements with regard to
    data source consumption and transformation possibilities.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书围绕越来越复杂的用例构建，从简单的数据转换到机器学习管道。我们涵盖了整个周期，从数据摄取到结果消费，增加了关于数据源摄取和转换可能性的更多元素。
- en: This book caters mostly to data analysts, scientists, and engineers who want
    to scale their Python code to larger data sets. Ideally, you should have written
    a few data programs, either through your work or while learning to program. You’ll
    get more out of this book if you already are comfortable using the Python programming
    language and ecosystem.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要面向那些希望将 Python 代码扩展到更大数据集的数据分析师、科学家和工程师。理想情况下，你应该已经通过工作或学习编程编写了一些数据程序。如果你已经熟悉
    Python 编程语言和生态系统，那么你会从这本书中获得更多收获。
- en: Spark (and PySpark, naturally) borrows a lot from object-oriented and functional
    programming. I do not think it’s reasonable to expect complete knowledge of both
    programming paradigms just to use big data efficiently. If you understand Python
    classes, decorators, and higher-order functions, you’ll have a blast using some
    of the more advanced constructions in the book to bend PySpark to your will. Should
    those concepts be foreign to you, I cover them in the context of PySpark throughout
    the book (when appropriate) and in the appendixes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark（以及PySpark，自然地）借鉴了面向对象和函数式编程的很多内容。我认为，仅仅为了高效地使用大数据，就期望完全掌握这两种编程范式是不合理的。如果你理解Python类、装饰器和高阶函数，你将能够愉快地使用书中一些更高级的结构来使PySpark为你所用。如果你对这些概念感到陌生，我在全书的适当位置和附录中介绍了它们。
- en: 'How this book is organized: A road map'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书是如何组织的：一个路线图
- en: The book is divided into three parts. Part 1, “Get Acquainted,” introduces PySpark
    and its computation model. It also covers building and submitting a simple data
    program, focusing on the core operations that you certainly will use in every
    PySpark program you create, such as selecting, filtering, joining, and grouping
    data in a data frame.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为三个部分。第1部分“熟悉”，介绍了PySpark及其计算模型。它还涵盖了构建和提交一个简单的数据程序，重点关注你肯定会在你创建的每个PySpark程序中使用的核心操作，例如在数据框中选择、过滤、连接和分组数据。
- en: Part 2, “Get Proficient,” goes further into data transformation by introducing
    hierarchical data, a key element of scalable data programs in PySpark. We also
    make our programs more expressive, flexible, and performant through the judicious
    introduction of SQL code, an exploration of resilient distributed datasets/user-defined
    functions, efficient usage of pandas within PySpark, and window functions. We
    also explore Spark’s reporting capabilities and resource management to pinpoint
    potential performance problems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分“精通”通过引入层次化数据，深入探讨了数据转换，这是PySpark可扩展数据程序的关键元素。我们还通过审慎地引入SQL代码、探索弹性分布式数据集/用户定义函数、在PySpark中高效使用pandas以及窗口函数，使我们的程序更具表现力、灵活性和性能。我们还探讨了Spark的报表功能和资源管理，以确定潜在的性能问题。
- en: Finally, Part 3, “Get Confident,” builds on parts 1 and 2 and covers how to
    build a machine learning program in PySpark. We use our data transformation tool
    kit to create and select features before building and evaluating a machine learning
    pipeline. We finish this part with creating our own machine learning pipeline
    components, ensuring maximum usability and readability for our ML programs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第3部分“自信”，建立在第1部分和第2部分的基础上，涵盖了如何在PySpark中构建机器学习程序。我们使用我们的数据转换工具包在构建和评估机器学习管道之前创建和选择特征。我们通过创建我们自己的机器学习管道组件来结束这一部分，确保我们的ML程序具有最大限度的可用性和可读性。
- en: Parts 1 and 2 have exercises throughout the chapters, as well as at the end
    of the chapters. Exercises at the end of a section don’t require you to code;
    you should be able to answer the questions with what you learned.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分和第2部分在章节中以及章节末尾都有练习。章节末尾的练习不需要你编写代码；你应该能够用你所学到的知识回答问题。
- en: 'The book was written with the idea of being read cover to cover, using the
    appendixes as needed. Should you want to dig directly into a topic, I still recommend
    covering part 1 before delving into a specific chapter. Here are the hard and
    soft dependencies to help you navigate the book more efficiently:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是以从头到尾阅读的想法来编写的，根据需要使用附录。如果你想要直接深入研究一个主题，我仍然建议在深入研究特定章节之前先覆盖第1部分。以下是一些硬依赖和软依赖，以帮助你更有效地导航本书：
- en: Chapter 3 is a direct continuation of chapter 2.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3章是第2章的直接延续。
- en: Chapter 5 is a direct continuation of chapter 4.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5章是第4章的直接延续。
- en: Chapter 9 uses some concepts taught in chapter 8, but advanced readers can read
    it on its own.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第9章使用了第8章中教授的一些概念，但高级读者可以单独阅读。
- en: Chapters 12, 13, and 14 are best read one after the other.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第12章、第13章和第14章最好依次阅读。
- en: About the code
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于代码
- en: 'This book works best with Spark version 3.1 or 3.2: Spark introduced many new
    functionalities in version 3, and most commercial offerings are now defaulting
    to this version. When appropriate, I provide backward-compatible instructions
    for Spark version 2.3/2.4\. I do not recommend Spark 2.2 or below. I also recommend
    using Python version 3.6 and above (I used Python 3.8.8 for the book). Installation
    instructions are available in appendix A.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本书最适合与 Spark 版本 3.1 或 3.2 一起使用：Spark 在 3.0 版本中引入了许多新功能，现在大多数商业产品都默认使用这个版本。在适当的情况下，我提供了与
    Spark 版本 2.3/2.4 兼容的说明。我不建议使用 Spark 2.2 或更低版本。我还建议使用 Python 版本 3.6 及以上（本书使用 Python
    3.8.8）。安装说明可在附录 A 中找到。
- en: You can find the companion repository for the book, with data and code, at [https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark).
    When appropriate, it also contains runnable versions of the programs developed
    throughout the book, as well as a few optional exercises. In addition, you can
    get executable snippets of code from the liveBook (online) version of this book
    at [https://livebook.manning.com/book/data-analysis-with-python-and-pyspark.](https://livebook.manning.com/book/data-analysis-with-python-and-pyspark)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark)
    找到本书的配套代码库，其中包含数据和代码。在适当的情况下，它还包含全书开发过程中编写的程序的可运行版本，以及一些可选练习。此外，您还可以从本书的 liveBook（在线）版本中获取可执行的代码片段
    [https://livebook.manning.com/book/data-analysis-with-python-and-pyspark.](https://livebook.manning.com/book/data-analysis-with-python-and-pyspark)
- en: This book contains many examples of source code both in numbered listings and
    in line with normal text. In both cases, source code is formatted in a `fixed-width`
    `font` `like` `this` to separate it from ordinary text. Sometimes code is also
    in bold to highlight code that has changed from previous steps in the chapter,
    such as when a new feature adds to an existing line of code.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书包含许多源代码示例，既有编号列表，也有与普通文本并列的代码。在这两种情况下，源代码都使用 `fixed-width` `font` `like` `this`
    格式化，以将其与普通文本区分开来。有时代码也会加粗，以突出显示与章节中先前步骤相比已更改的代码，例如当新功能添加到现有代码行时。
- en: In many cases, the original source code has been reformatted; we’ve added line
    breaks and reworked indentation to accommodate the available page space in the
    book. In rare cases, even this was not enough, and listings include line-continuation
    markers (➥). Additionally, comments in the source code have often been removed
    from the listings when the code is described in the text. Code annotations accompany
    many of the listings and highlight important concepts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，原始源代码已经被重新格式化；我们添加了换行符并重新调整了缩进，以适应书籍中的可用页面空间。在极少数情况下，即使这样也不够，列表中还包括了行续接标记（➥）。此外，当代码在文本中描述时，源代码中的注释通常也会从列表中移除。许多列表旁边都有代码注释，并突出显示重要概念。
- en: liveBook discussion forum
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: liveBook 讨论论坛
- en: Purchase of *Data Analysis with Python and PySpark* includes free access to
    liveBook, Manning’s online reading platform. Using liveBook’s exclusive discussion
    features, you can attach comments to the book globally or to specific sections
    or paragraphs. It’s a snap to make notes for yourself, ask and answer technical
    questions, and receive help from the author and other users. To access the forum,
    go to [https://livebook.manning.com/book/data-analysis-with-python-and-pyspark/discussion](https://livebook.manning.com/book/data-analysis-with-python-and-pyspark/discussion).
    You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 购买 *使用 Python 和 PySpark 进行数据分析* 包括免费访问 liveBook，Manning 的在线阅读平台。使用 liveBook
    的独家讨论功能，您可以在全球范围内或特定章节或段落中附加评论。为自己做笔记、提问和回答技术问题以及从作者和其他用户那里获得帮助都非常简单。要访问论坛，请访问
    [https://livebook.manning.com/book/data-analysis-with-python-and-pyspark/discussion](https://livebook.manning.com/book/data-analysis-with-python-and-pyspark/discussion)。您还可以在
    [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion)
    了解更多关于 Manning 论坛和行为准则的信息。
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest his interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 曼宁对我们读者的承诺是提供一个场所，在那里个人读者之间以及读者与作者之间可以进行有意义的对话。这不是对作者参与特定数量的承诺，作者对论坛的贡献仍然是自愿的（且未付费）。我们建议您尝试向作者提出一些挑战性的问题，以免他的兴趣转移！只要这本书有售，论坛和以前讨论的存档将可通过出版商的网站访问。
- en: about the author
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于作者
- en: '| ![](../Images/Rioux_author_photo.png)   | Jonathan Rioux uses PySpark inside
    and out on a daily basis. He also teaches large-scale data analysis to data scientists,
    engineers, and data-savvy business analysts.Jonathan spent a decade in various
    analytical positions in the insurance industry before venturing into the consulting
    industry as a machine learning and data analysis expert. He currently works as
    the director of machine learning for Laivly, a company that equips friendly humans
    with intelligent automations and machine learning to create the best customer
    experiences on the planet. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![Rioux作者照片](../Images/Rioux_author_photo.png)   | 乔纳森·里尤斯每天都会全面使用PySpark。他还教授数据科学家、工程师和具有数据洞察力的商业分析师进行大规模数据分析。乔纳森在保险行业担任各种分析职位十年后，作为机器学习和数据分析专家进入咨询行业。他目前是Laivly公司的机器学习总监，该公司为友好的人类配备智能自动化和机器学习，以在地球上创造最佳的客户体验。
    |'
- en: about the cover illustration
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于封面插图
- en: The figure on the cover of *Data Analysis with Python and PySpark* is “Russien,”
    or Russian man, taken from a book by Jacques Grasset de Saint-Sauveur, published
    in 1788\. Each illustration is finely drawn and colored by hand.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 《使用Python和PySpark进行数据分析》封面上的图像是“Russien”，即俄罗斯人，取自雅克·格拉塞·德·圣索沃尔于1788年出版的书籍。每一幅插图都是手工精细绘制和着色的。
- en: In those days, it was easy to identify where people lived and what their trade
    or station in life was just by their dress. Manning celebrates the inventiveness
    and initiative of today’s computer business with book covers based on the rich
    diversity of regional culture centuries ago, brought back to life by pictures
    from collections such as this one.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些日子里，人们通过他们的着装很容易就能识别出他们住在哪里，他们的职业或生活地位是什么。曼宁通过基于几个世纪前丰富的地方文化多样性的书封面来庆祝当今计算机业务的创新精神和主动性，这些文化多样性通过像这一系列这样的图片被重新带回生活。
