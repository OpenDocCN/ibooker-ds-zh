- en: 15 NLP analysis of large text datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 大型文本数据集的 NLP 分析
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Vectorizing texts using scikit-learn
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 向量化文本
- en: Dimensionally reducing vectorized text data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对向量化文本数据进行降维
- en: Clustering large text datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型文本数据集的聚类
- en: Visualizing text clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化文本聚类
- en: Concurrently displaying multiple visualizations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时显示多个可视化
- en: Our previous discussions of natural language processing (NLP) techniques focused
    on toy examples and small datasets. In this section, we execute NLP on large collections
    of real-world texts. This type of analysis is seemingly straightforward, given
    the techniques presented thus far. For example, suppose we’re doing market research
    across multiple online discussion forums. Each forum is composed of hundreds of
    users who discuss a specific topic, such as politics, fashion, technology, or
    cars. We want to automatically extract all the discussion topics based on the
    contents of the user conversations. These extracted topics will be used to plan
    a marketing campaign, which will target users based on their online interests.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前关于自然语言处理（NLP）技术的讨论主要集中在玩具示例和小数据集上。在本节中，我们对大量现实世界文本执行 NLP。鉴于迄今为止介绍的技术，这种分析似乎很简单。例如，假设我们在多个在线讨论论坛中进行市场研究。每个论坛由数百名用户组成，他们讨论特定主题，如政治、时尚、技术或汽车。我们希望根据用户对话的内容自动提取所有讨论主题。这些提取的主题将被用于规划营销活动，该活动将针对用户的在线兴趣进行定位。
- en: 'How do we cluster user discussions into topics? One approach would be to do
    the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将用户讨论聚类成主题？一种方法可以是以下步骤：
- en: Convert all discussion texts into a matrix of word counts using techniques discussed
    in section 13.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第 13 节中讨论的技术将所有讨论文本转换为词频矩阵。
- en: Dimensionally reduce the word count matrix using singular value decomposition
    (SVD). This will allow us to efficiently complete all pairs of text similarities
    with matrix multiplication.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用奇异值分解（SVD）对词频矩阵进行降维。这将使我们能够通过矩阵乘法有效地完成所有文本相似性对。
- en: Utilize the matrix of text similarities to cluster the discussions into topics.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用文本相似度矩阵将讨论聚类成主题。
- en: Explore the topic clusters to identify useful topics for our marketing campaign.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索主题聚类以识别对我们营销活动有用的主题。
- en: Of course, in real life, this simple analysis is not as straightforward as it
    seems. Unanswered questions still remain. How do we efficiently explore the topic
    clusters without reading all the clustered texts one at a time? Also, which of
    the clustering algorithms introduced in section 10 should we utilize to cluster
    the discussions?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在现实生活中，这种简单的分析并不像看起来那么简单。未解决的问题仍然存在。我们如何在不逐个阅读所有聚类文本的情况下有效地探索主题聚类？此外，我们应该利用第
    10 节中介绍的哪个聚类算法来聚类讨论？
- en: Even at the level of pairwise text comparison, we face certain questions. How
    do we deal with common uninformative words such as *the*, *it*, and *they*? Should
    we penalize them? Ignore them? Filter them out entirely? What about other common,
    corpus-specific words such as the names of the websites that host the discussion
    forums?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在成对文本比较的水平上，我们也面临某些问题。我们如何处理诸如 *the*、*it* 和 *they* 这样的常见无信息词？我们应该惩罚它们吗？忽略它们？完全过滤掉它们？那么，关于诸如托管讨论论坛的网站名称之类的其他常见、语料库特定的词怎么办？
- en: All of these questions have answers that can best be understood by actually
    exploring an online forum dataset containing thousands of texts. Scikit-learn
    includes one such real-world dataset among its example data collections. In this
    section, we load, explore, and cluster this large dataset of online forums. Python’s
    external data science libraries, such as scikit-learn and NumPy, will prove invaluable
    in this real-world analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题都有答案，这些答案最好通过实际探索包含数千个文本的在线论坛数据集来理解。Scikit-learn 在其示例数据集中包含这样一个现实世界的数据集。在本节中，我们加载、探索并对这个大型在线论坛数据集进行聚类。Python
    的外部数据科学库，如 scikit-learn 和 NumPy，将在这种现实世界分析中证明其价值。
- en: 15.1 Loading online forum discussions using scikit-learn
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 使用 scikit-learn 加载在线论坛讨论
- en: Scikit-learn provides us with data from Usenet, which is a well-established
    online collection of discussion forums. These Usenet forums are called *newsgroups*.
    Each individual newsgroup focuses on some topic of discussion, which is briefly
    outlined in the newsgroup name. Users in a newsgroup converse by posting messages.
    These user posts are not limited in length, and hence some posts can be quite
    long. Both the diversity and the varying lengths of the posts will give us a chance
    to expand our NLP skills. For training purposes, the scikit-learn library provides
    access to over 10,000 posted messages. We can load these newsgroup posts by importing
    `fetch_20newsgroups` from `sklearn .datasets`. Calling `fetch_20newsgroups()`
    returns a `newsgroups` object that contains the textual data. Furthermore, optionally
    passing `remove=('headers', 'footers')` into the function call removes redundant
    information from the text. (That deleted metadata does not correspond to meaningful
    post content.) Listing 15.1 loads the newsgroup data while filtering redundant
    information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn为我们提供了来自Usenet的数据，Usenet是一个成熟的在线讨论论坛集合。这些Usenet论坛被称为*newsgroups*。每个单独的新闻组都专注于某个讨论主题，这在其新闻组名称中简要概述。新闻组中的用户通过发布消息进行交流。这些用户帖子不受长度限制，因此有些帖子可能相当长。帖子的多样性和不同长度将给我们一个机会来扩展我们的NLP技能。为了训练目的，scikit-learn库提供了超过10,000篇已发布消息的访问权限。我们可以通过从`sklearn.datasets`导入`fetch_20newsgroups`来加载这些新闻组帖子。调用`fetch_20newsgroups()`返回一个包含文本数据的`newsgroups`对象。此外，将`remove=('headers',
    'footers')`作为可选参数传递给函数调用会从文本中删除冗余信息。（删除的元数据并不对应于有意义的帖子内容。）列表15.1在过滤冗余信息的同时加载了新闻组数据。
- en: Warning The newsgroups dataset is quite large. For this reason, it is not prepackaged
    with scikit-learn. Running `fetch_20newsgroups` forces scikit-learn to download
    and store the dataset on a local machine, so an internet connection is required
    when the dataset is first fetched. All subsequent calls to `fetch_20newsgroups`
    will load the dataset locally without requiring an internet connection.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：新闻组数据集相当大。因此，它没有与scikit-learn预包装。运行`fetch_20newsgroups`会强制scikit-learn在本地机器上下载并存储数据集，因此首次获取数据集时需要互联网连接。所有随后的`fetch_20newsgroups`调用都将从本地加载数据集，而不需要互联网连接。
- en: Listing 15.1 Fetching the newsgroup dataset
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.1 获取新闻组数据集
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `newsgroups` object contains posts from 20 different newsgroups. As mentioned,
    each newsgroup’s discussion theme is outlined in its name. We can view these newsgroup
    names by printing `newsgroups.target_names`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`newsgroups`对象包含来自20个不同新闻组的帖子。如前所述，每个新闻组的讨论主题在其名称中概述。我们可以通过打印`newsgroups.target_names`来查看这些新闻组名称。'
- en: Listing 15.2 Printing the names of all 20 newsgroups
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.2 打印所有20个新闻组的名称
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The newsgroup categories vary greatly, from space exploration (*sci.space*)
    to cars (*rec.auto*) to electronics (*sci.electronics*). Some of the categories
    are very broad. For instance, politics (*talk.politics.misc*) can cover a wide
    range of political themes. Other categories are very narrow in scope: for example,
    *comp.sys.mac.hardware* focuses on Mac hardware, while *comp.sys.ibm.pc_hardware*
    focuses on PC hardware. Categorically, these two newsgroups are exceedingly similar:
    the only differentiator is whether the computer hardware belongs to a Mac or a
    PC. Sometimes categorical differences are subtle; boundaries between text topics
    are fluid and not necessarily etched in stone. We need to keep this in mind later
    in the section when we cluster the newsgroup posts.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻组类别差异很大，从太空探索（*sci.space*）到汽车（*rec.auto*）再到电子（*sci.electronics*）。有些类别非常广泛。例如，政治（*talk.politics.misc*）可以涵盖广泛的政治主题。其他类别则非常狭窄：例如，*comp.sys.mac.hardware*专注于Mac硬件，而*comp.sys.ibm.pc_hardware*专注于PC硬件。从类别上讲，这两个新闻组极其相似：唯一的区别是计算机硬件属于Mac还是PC。有时类别差异很微妙；文本主题之间的边界是流动的，并不一定刻在石头上。我们在本节后面的聚类新闻组帖子时需要记住这一点。
- en: Now, let’s turn our attention to the actual newsgroup texts, which are stored
    as a list in the `newsgroups.data` attribute. For example, `newsgroups.data[0]`
    contains the text of the first stored newsgroup post. Let’s output that post.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向实际的新闻组文本，这些文本存储在`newsgroups.data`属性中的列表中。例如，`newsgroups.data[0]`包含第一个存储的新闻组帖子的文本。让我们输出这篇帖子。
- en: Listing 15.3 Printing the first newsgroup post
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.3 打印第一个新闻组帖子
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The post is about a car. It probably was posted to the car discussion newsgroup,
    *rec.autos*. We can confirm by printing `newsgroups.target_names[newsgroups.target[0]]`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇帖子是关于汽车的。它可能被发布在汽车讨论新闻组*rec.autos*中。我们可以通过打印`newsgroups.target_names[newsgroups.target[0]]`来确认。
- en: Note `newsgroups.target[i]` returns the index of the newsgroup name associated
    with the *i*th document.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`newsgroups.target[i]`返回与第*i*个文档相关联的新闻组名称的索引。
- en: Listing 15.4 Printing the newsgroup name at index 0
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.4 打印索引为0的新闻组名称
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we predicted, our car-related post appeared in the car discussion group.
    The presence of a few keywords such as *car*, *bumper*, and *engine*, was sufficient
    to make this distinction. Of course, this is just one post out of many. Categorizing
    the remaining posts may not be so easy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预测的，我们的与汽车相关的帖子出现在汽车讨论组中。一些关键词如*car*、*bumper*和*engine*的存在足以区分这一点。当然，这只是众多帖子中的一个。对剩余帖子进行分类可能不会那么容易。
- en: Let’s dive deeper into our newsgroup dataset by printing out the dataset size.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打印数据集大小来更深入地了解我们的新闻组数据集。
- en: Listing 15.5 Counting the number of newsgroup posts
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.5 计算新闻组帖子的数量
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our dataset contains over 11,000 posts. Our goal is to cluster these posts by
    topic, but carrying out text clustering on this scale requires computational efficiency.
    We need to efficiently compute newsgroup post similarities by representing our
    text data as a matrix. To do so, we need to transform each newsgroup post into
    a term-frequency (TF) vector. As discussed in section 13, a TF vector’s indices
    map to word counts in a document. Previously, we computed these vectorized word
    counts using custom functions. Now we will compute them using scikit-learn.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集包含超过11,000篇帖子。我们的目标是按主题对这些帖子进行聚类，但在这个规模上进行文本聚类需要计算效率。我们需要通过将我们的文本数据表示为矩阵来高效地计算新闻组帖子的相似度。为此，我们需要将每个新闻组帖子转换为一个词频（TF）向量。如第13节所述，TF向量的索引映射到文档中的单词计数。之前，我们使用自定义函数计算这些向量化的单词计数。现在我们将使用scikit-learn来计算它们。
- en: 15.2 Vectorizing documents using scikit-learn
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 使用scikit-learn对文档进行向量化
- en: 'Scikit-learn provides a built-in class for transforming input texts into TF
    vectors: `CountVectorizer`. Initializing `CountVectorizer` creates a `vectorizer`
    object capable of vectorizing our texts. Next, we import `CountVectorizer` from
    `sklearn.feature_ extraction.text` and initialize the class.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了一个内置类，用于将输入文本转换为TF向量：`CountVectorizer`。初始化`CountVectorizer`创建了一个`vectorizer`对象，该对象能够将我们的文本进行向量化。接下来，我们从`sklearn.feature_extraction.text`导入`CountVectorizer`并初始化该类。
- en: Listing 15.6 Initializing a `CountVectorizer` object
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.6 初始化`CountVectorizer`对象
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now ready to vectorize the texts stored in the `newsgroups.data` list.
    All we need to do is run `vectorizer.fit_transform(newsgroups.data)`. The method
    call returns the TF matrix corresponding to the vectorized newsgroup posts. As
    a reminder, a TF matrix stores the counts of words (columns) across all texts
    (rows). Let’s vectorize the posts and then print the resulting TF matrix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好将存储在`newsgroups.data`列表中的文本进行向量化。我们所需做的只是运行`vectorizer.fit_transform(newsgroups.data)`。这个方法调用返回与向量化新闻组帖子对应的TF矩阵。作为提醒，TF矩阵存储了所有文本（行）中单词（列）的计数。让我们将帖子向量化，然后打印出结果TF矩阵。
- en: Listing 15.7 Computing a TF matrix with scikit-learn
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.7 使用scikit-learn计算TF矩阵
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Our printed `tf_matrix` does not appear to be a NumPy array. What sort of data
    structure is it? We can check by printing `type(tf_matrix)`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印的`tf_matrix`看起来不是一个NumPy数组。它是什么类型的数据结构？我们可以通过打印`type(tf_matrix)`来检查。
- en: Listing 15.8 Checking the data type of `tf_matrix`
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.8 检查`tf_matrix`的数据类型
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The matrix is a SciPy object called `csr_matrix`. *CSR* stands for *compressed
    sparse row*, which is a storage format for compressing matrices that are composed
    mostly of zeros. These mostly empty matrices are referred to as *sparse matrices*.
    They can be made smaller by storing only the nonzero elements. This compression
    leads to more efficient memory usage and faster computation. Large-scale text-based
    matrices are usually very sparse, since a single document normally contains just
    a small percentage of the total vocabulary. Thus, scikit-learn automatically converts
    the vectorized text to the CSR format. The conversion is carried out using a `csr_matrix`
    class imported from SciPy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵是一个名为`csr_matrix`的SciPy对象。*CSR*代表*压缩稀疏行*，这是一种用于压缩主要由零组成的矩阵的存储格式。这些主要空矩阵被称为*稀疏矩阵*。通过仅存储非零元素，可以减小矩阵的大小。这种压缩导致更高效的内存使用和更快的计算。大规模基于文本的矩阵通常非常稀疏，因为单个文档通常只包含总词汇量的一小部分。因此，scikit-learn会自动将向量化文本转换为CSR格式。转换是通过从SciPy导入的`csr_matrix`类来完成的。
- en: This interplay between various external data science libraries is useful but
    also a bit confusing. In particular, the differences between a NumPy array and
    a SciPy CSR matrix can be tricky for a newcomer to grasp. This is because arrays
    and CSR matrices share some, but not all, attributes. Also, arrays and CSR matrices
    are compatible with some, but not all, NumPy functions. To minimize confusion,
    we will convert `tf_matrix` into a 2D NumPy array. Most of our subsequent analyses
    will be carried out on that NumPy array. However, periodically, we will compare
    array usage with CSR matrix usage. Doing so will allow us to more fully comprehend
    the similarities and differences between the two matrix representations. Listing
    15.9 converts `tf_matrix` to NumPy by running `tf_matrix.toarray()` and then prints
    the converted result.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种各种外部数据科学库之间的交互很有用，但也有些令人困惑。特别是，对于一个新手来说，理解NumPy数组和SciPy CSR矩阵之间的区别可能有些棘手。这是因为数组和CSR矩阵共享一些属性，但并非全部。此外，数组和CSR矩阵与一些NumPy函数兼容，但并非全部。为了减少混淆，我们将`tf_matrix`转换为2D
    NumPy数组。我们后续的大部分分析都将在这个NumPy数组上进行。然而，我们将会定期比较数组和CSR矩阵的使用情况。这样做将使我们更全面地理解这两种矩阵表示之间的相似性和差异。列表15.9通过运行`tf_matrix.toarray()`将`tf_matrix`转换为NumPy，然后打印转换后的结果。
- en: 'Warning This conversion is very memory intensive, requiring almost 10 GB of
    memory. If your local machine has limited memory available, we suggest you execute
    this code in the cloud using Google Colaboratory (Colab), a free, cloud-based
    Jupyter Notebook environment with 12 GB freely available memory. Google provides
    a comprehensive introduction to using Colab that covers everything that you need
    to get started: [https://colab.research.google.com/notebooks/welcome.ipynb](https://colab.research.google.com/notebooks/welcome.ipynb).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：这种转换非常占用内存，需要近10 GB的内存。如果你的本地机器内存有限，我们建议你在Google Colab（Colab）上执行此代码，Colab是一个免费、基于云的Jupyter
    Notebook环境，提供12 GB的免费内存。Google提供了一个全面的Colab使用介绍，涵盖了您开始所需的一切：[https://colab.research.google.com/notebooks/welcome.ipynb](https://colab.research.google.com/notebooks/welcome.ipynb)。
- en: Listing 15.9 Converting a CSR matrix to a NumPy array
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.9 将CSR矩阵转换为NumPy数组
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The printed matrix is a 2D NumPy array. All previewed matrix elements are zeros,
    confirming that the matrix is rather sparse. Each matrix element corresponds to
    the count of a word in the post. The matrix rows represent the post, while the
    columns represent individual words. Thus, the total column count equals our dataset’s
    vocabulary size. We access that count using the `shape` attribute. This attribute
    is shared by both the CSR matrix and the NumPy array. Let’s use `tf_np_matrix.shape`
    to output the vocabulary size.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的矩阵是一个2D NumPy数组。所有预览的矩阵元素都是零，这证实了矩阵相当稀疏。每个矩阵元素对应于帖子中单词的计数。矩阵行代表帖子，而列代表单个单词。因此，总列数等于我们的数据集的词汇量大小。我们使用`shape`属性来访问这个计数。这个属性由CSR矩阵和NumPy数组共享。让我们使用`tf_np_matrix.shape`来输出词汇量大小。
- en: Listing 15.10 Checking the vocabulary size
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.10 检查词汇量大小
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our data contains 114751 unique words. However, most posts contain only a few
    dozen of these words. We can measure the unique word count of a post at index
    `i` by counting the number of nonzero elements in row `tf_np_matrix[i]`. The easiest
    way to count these nonzero elements is with NumPy. The library allows us to obtain
    all nonzero indices of the vector at `tf_np_matrix[i`]. We simply need to input
    the vector into the `np.flatnonzero` function. Next, we count and output the nonzero
    indices of the car post in `newsgroups.data[0]`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据包含114751个独特的单词。然而，大多数帖子只包含几十个这些单词。我们可以通过在`tf_np_matrix[i]`行中计数非零元素的数量来衡量索引为`i`的帖子的独特单词计数。最容易计数这些非零元素的方法是使用NumPy。这个库允许我们获取`tf_np_matrix[i]`向量中所有非零索引。我们只需将向量输入到`np.flatnonzero`函数中。接下来，我们计数并输出`newsgroups.data[0]`中汽车帖子的非零索引。
- en: Listing 15.11 Counting the unique words in the car post
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.11 计算汽车帖子中的独特单词
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ This is equivalent to running np.nonzero(tf_vector)[0]. The np.nonzero function
    generalizes the computation of nonzero indices across an x-dimensional array.
    It returns a tuple of length x, where each *i*th tuple element represents the
    nonzero indices along the *i*th dimension. Hence, given a 1D tf_vector array,
    the np.nonzero function returns a tuple of the form (non_zero_indices).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这相当于运行`np.nonzero(tf_vector)[0]`。`np.nonzero`函数将非零索引的计算推广到x维数组。它返回一个长度为x的元组，其中每个*i*个元组元素表示第*i*维上的非零索引。因此，给定一个1维tf_vector数组，`np.nonzero`函数返回一个形式为(non_zero_indices)的元组。
- en: The first newsgroup post contains 64 unique words. What are these words? To
    find out, we need a mapping between TF vector indices and word values. That mapping
    can be generated by calling `vectorizer.get_feature_names()`, which returns a
    list of words that we’ll call `words`. Each index `i` corresponds to the *i*th
    word in the list. Thus, running `[words[i] for i in non_zero_indices]` will return
    all unique words in our post.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇新闻组的帖子包含64个独特的单词。这些单词是什么？为了找出这些单词，我们需要在TF向量索引和单词值之间建立一个映射。这个映射可以通过调用`vectorizer.get_feature_names()`生成，它返回一个我们称之为`words`的单词列表。每个索引`i`对应于列表中的第*i*个单词。因此，运行`[words[i]
    for i in non_zero_indices]`将返回我们帖子中的所有独特单词。
- en: Note We can also obtain these words by calling `vectorizer.inverse_ transform(tf_vector)`.
    The `inverse_transform` method returns all words associated with an inputted TF
    vector.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们也可以通过调用`vectorizer.inverse_transform(tf_vector)`来获取这些单词。`inverse_transform`方法返回与输入的TF向量相关联的所有单词。
- en: Listing 15.12 Printing the unique words in the car post
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.12 打印汽车帖子中的独特单词
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We’ve printed all the words in `newsgroups.data[0]`. Of course, not all of these
    words have equal mention counts—some occur more frequently than others. Perhaps
    these frequent words are more relevant to the topic of cars. Listing 15.13 prints
    the 10 most frequent words in the post, along with their associated counts. We
    represent this output as a Pandas table for visualization purposes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经打印了`newsgroups.data[0]`中的所有单词。当然，并不是所有这些单词的提及次数都相等——有些比其他单词出现得更频繁。也许这些频繁出现的单词与汽车的主题更为相关。列表15.13打印了帖子中频率最高的10个单词及其相关计数。为了可视化目的，我们将此输出表示为Pandas表格。
- en: Extracting nonzero elements of 1D NumPy arrays
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 提取1维NumPy数组中的非零元素
- en: '`non_zero_indices = np.flatnonzero(np_vector)`—Returns the nonzero indices
    in a 1D NumPy array'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`non_zero_indices = np.flatnonzero(np_vector)`——返回1维NumPy数组中的非零索引'
- en: '`non_zero_vector = np_vector[non_zero_indices]`—Selects the nonzero elements
    of a 1D NumPy array (assuming `non_zero_indices` corresponds to nonzero indices
    of that array)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`non_zero_vector = np_vector[non_zero_indices]`——选择1维NumPy数组中的非零元素（假设`non_zero_indices`对应于该数组的非零索引）'
- en: Listing 15.13 Printing the most frequent words in the car post
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.13 打印汽车帖子中最频繁的单词
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Sorts the Pandas table based on counts, from highest to lowest
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据计数对Pandas表格进行排序，从高到低
- en: 'Four of the 64 words in the post are mentioned at least four times. One of
    these words is *car*, which is not surprising given that the post appeared in
    a car discussion group. The other three words, however, have nothing to do with
    cars: *the*, *this*, and *was* are among the most common words in the English
    language. They don’t provide a differentiating signal between the car post and
    a post with a different theme—instead, the common words are a source of noise
    and increase the likelihood that two unrelated documents will cluster together.
    NLP practitioners refer to such noisy words as *stop words* because they are blocked
    from appearing in the vectorized results. Stop words are generally deleted from
    the text before vectorization. That is why the `CountVectorizer` class has a built-in
    stop-word deletion option. Running `CountVectorizer(stop_words=''english'')` initializes
    a vectorizer that is primed for stop-word deletion. The vectorizer ignores all
    of the most common English words in the text.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 文章中的 64 个单词中有四个至少被提到了四次。其中一个是 *car*，考虑到文章出现在一个汽车讨论组中，这并不令人惊讶。然而，其他三个单词与汽车无关：*the*、*this*
    和 *was* 是英语中最常见的单词之一。它们不能为汽车帖子与其他主题不同的帖子提供区分信号——相反，常见的单词是噪声的来源，增加了两个无关文档聚在一起的可能性。NLP
    实践者将此类噪声词称为 *停用词*，因为它们被阻止出现在向量化的结果中。通常在向量化之前从文本中删除停用词。这就是为什么 `CountVectorizer`
    类有一个内置的停用词删除选项。运行 `CountVectorizer(stop_words='english')` 将初始化一个准备进行停用词删除的向量器。该向量器会忽略文本中所有最常见的英语单词。
- en: 'Next, we reinitialize a stop-word-aware vectorizer. Then we rerun `fit_transform`
    to recompute the TF matrix. The number of word columns in that matrix will be
    less than our previously computed vocabulary size of 114,751\. We also regenerate
    our `words` list: this time, common stop words such as *the*, *this*, *of*, and
    *it* will be missing.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重新初始化一个停用词感知的向量器。然后我们重新运行 `fit_transform` 来重新计算 TF 矩阵。该矩阵中的单词列数将少于我们之前计算的词汇量
    114,751。我们还重新生成了我们的 `words` 列表：这次，常见的停用词如 *the*、*this*、*of* 和 *it* 将会缺失。
- en: Listing 15.14 Removing stop words during vectorization
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.14 向量化过程中移除停用词
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Checks to ensure that our vocabulary size has gotten smaller
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确保我们的词汇量已经减小
- en: ❷ Common stop words have been filtered out.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 已过滤掉常见的停用词。
- en: All stop words have been deleted from the recomputed `tf_matrix`. Now we can
    regenerate the 10 most frequent words in `newsgroups.data[0]`. Note that in the
    process, we recompute `tf_np_matrix`, `tf_vector`, `unique_words`, `non_zero_indices`,
    and `df`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所有停用词已从重新计算的 `tf_matrix` 中删除。现在我们可以重新生成 `newsgroups.data[0]` 中的前10个最频繁的单词。请注意，在这个过程中，我们重新计算了
    `tf_np_matrix`、`tf_vector`、`unique_words`、`non_zero_indices` 和 `df`。
- en: Warning This regeneration is memory intensive, requiring 2.5 GB of memory.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 这种重新生成过程非常消耗内存，需要 2.5 GB 的内存。
- en: Listing 15.15 Reprinting the top words after stop-word deletion
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.15 停用词删除后重新打印的顶级单词
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After stop-word filtering, 34 words remain. Among them, *car* is the only word
    that is mentioned more than once. The other 33 words share a mention count of
    1 and are treated equally by the vectorizer. However, it’s worth noting that not
    all words are equal in their relevancy. Some words are more relevant to a car
    discussion than others: for instance, the word *model* refers to a car model (although
    of course it could also refer to a supermodel or a machine learning model). Meanwhile,
    the word *really* is more general; it doesn’t refer to anything car related. The
    word is so irrelevant and common that it could almost be a stop word. In fact,
    some NLP practitioners keep *really* on their stop-word list—but others don’t.
    Unfortunately, there is no consensus about which words are always useless and
    which aren’t. However, all practitioners agree that a word becomes less useful
    if it’s mentioned in too many texts. Thus, *really* is less relevant than *model*
    because the former is mentioned in more posts. Therefore, when ranking words by
    relevance, we should use both post frequency and count. If two words share an
    equal count, we should rank them by post frequency, instead.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在停用词过滤后，剩下 34 个单词。其中，*car* 是唯一一个被提及超过一次的单词。其他 33 个单词的提及次数都是 1，并且被向量化器同等对待。然而，值得注意的是，并非所有单词在相关性上都是平等的。有些单词比其他单词更相关于汽车讨论：例如，单词
    *model* 指的是汽车型号（尽管当然它也可能指超级模特或机器学习模型）。同时，单词 *really* 更为通用；它不指代任何与汽车相关的事物。这个单词如此不相关且常见，以至于它几乎可以成为一个停用词。实际上，一些自然语言处理从业者将
    *really* 保留在他们的停用词列表中——但其他人则不这么做。不幸的是，关于哪些单词总是无用的以及哪些不是，并没有共识。然而，所有从业者都同意，如果一个单词在太多文本中被提及，那么它就变得不那么有用。因此，*really*
    比较不相关于 *model*，因为前者在更多帖子中被提及。因此，当根据相关性对单词进行排名时，我们应该使用帖子频率和计数。如果两个单词的计数相等，我们应该根据帖子频率对它们进行排名。
- en: Let’s rerank our 34 words based on both post frequency and count. Then we’ll
    explore how these rankings can be used to improve text vectorization.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据帖子频率和计数重新排名我们的 34 个单词。然后我们将探索如何使用这些排名来改进文本向量化。
- en: Common scikit-learn CountVectorizer methods
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 scikit-learn CountVectorizer 方法
- en: '`vectorizer = CountVectorizer()`—Initializes a `CountVectorizer` object capable
    of vectorizing input texts based on their TF counts.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vectorizer = CountVectorizer()`—初始化一个 `CountVectorizer` 对象，该对象能够根据输入文本的 TF
    计数进行向量化。'
- en: '`vectorizer = CountVectorizer(stopwords=''english'')`—Initializes an object
    capable of vectorizing input texts while filtering for common English words like
    *this* and *the*.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vectorizer = CountVectorizer(stopwords=''english'')`—初始化一个对象，该对象能够在向量化输入文本的同时过滤掉常见的英语单词，如
    *this* 和 *the*。'
- en: '`tf_matrix = vectorizer.fit_transform(texts)`—Executes TF vectorization on
    a list of input texts using the initialized `vectorizer` object, and returns a
    CSR matrix of term frequency values. Each matrix row `i` corresponds to `texts[i]`.
    Each matrix column `j` corresponds to the term frequency of word `j`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf_matrix = vectorizer.fit_transform(texts)`—使用初始化的 `vectorizer` 对象在输入文本列表上执行
    TF 向量化，并返回一个包含词频值的 CSR 矩阵。矩阵的每一行 `i` 对应于 `texts[i]`。矩阵的每一列 `j` 对应于单词 `j` 的词频。'
- en: '`vocabulary_list = vectorizer.get_feature_names()`—Returns the vocabulary list
    associated with the columns of a computed TF matrix. Each column `j` of the matrix
    corresponds to `vocabulary_list[j]`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocabulary_list = vectorizer.get_feature_names()`—返回与计算 TF 矩阵的列相关联的词汇表。矩阵的每一列
    `j` 对应于 `vocabulary_list[j]`。'
- en: 15.3 Ranking words by both post frequency and count
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 通过帖子频率和计数对单词进行排名
- en: Each of the 34 words in `df.Word` appears in a certain fraction of newsgroup
    posts. In NLP, this fraction is referred to as the *document frequency* of a word.
    We hypothesize that the document frequencies can improve our word rankings. As
    scientists, we will now attempt to validate this hypothesis by exploring how the
    document frequencies relate to word importance. Initially, we’ll limit our exploration
    to a single document. Later, we will generalize our insights to the other documents
    in the dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`df.Word` 中的 34 个单词在新闻组帖子中出现的比例各不相同。在自然语言处理（NLP）中，这个比例被称为一个单词的*文档频率*。我们假设文档频率可以改善我们的单词排名。作为科学家，我们现在将尝试通过探索文档频率与单词重要性之间的关系来验证这个假设。最初，我们将我们的探索限制在单个文档上。稍后，我们将我们的见解推广到数据集中的其他文档。'
- en: Note Such open-ended explorations are common to data science. We start by exploring
    a small slice of data. By probing that small sample, we can hone our intuition
    about grander patterns in the dataset. Then we can test that intuition on a larger
    scale.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这种开放式探索在数据科学中很常见。我们首先探索一小部分数据。通过探测这个小样本，我们可以锤炼我们对数据集中更大模式直觉。然后我们可以在更大规模上测试这种直觉。
- en: We now begin our exploration. Our immediate goal is to compute 34 document frequencies,
    to try to improve our word relevancy rankings. We can compute these frequencies
    using a series of NumPy matrix manipulations. First, we want to select the columns
    of `tf_np_matrix` that correspond to the 34 nonzero indices in the `non_zero_indices`
    array. We can obtain this submatrix by running `tf_np_matrix[:,non_zero_indices]`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始探索。我们的直接目标是计算 34 个文档频率，以尝试改进我们的单词相关性排名。我们可以通过一系列 NumPy 矩阵操作来计算这些频率。首先，我们想要选择
    `tf_np_matrix` 中与 `non_zero_indices` 数组中的 34 个非零索引相对应的列。我们可以通过执行 `tf_np_matrix[:,non_zero_indices]`
    来获得这个子矩阵。
- en: Listing 15.16 Filtering matrix columns with `non_zero_indices`
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.16 使用 `non_zero_indices` 过滤矩阵列
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Accesses just those columns of the matrix that hold nonzero values in the
    first matrix row
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅访问矩阵中第一行包含非零值的那些列
- en: 'The first row of `sub_matrix` corresponds to the 34 word counts in `df`. Together,
    all the matrix rows correspond to counts across all posts. However, we are not
    currently interested in exact word counts: we just want to know whether each word
    is present or absent from each post. So, we need to convert our counts into binary
    values. Basically, we require a binary matrix where element `(i, j)` equals 1
    if word `j` is present in post `i`, and 0 otherwise. We can binarize the submatrix
    by importing `binarize` from `sklearn.preprocessing`. Then, running `binarize(sub_matrix)`
    will produce the necessary results.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`sub_matrix` 的第一行对应于 `df` 中的 34 个单词计数。所有矩阵行一起对应于所有帖子的计数。然而，我们目前对精确单词计数不感兴趣：我们只想知道每个单词是否出现在每个帖子中。因此，我们需要将我们的计数转换为二进制值。基本上，我们需要一个二进制矩阵，其中元素
    `(i, j)` 等于 1 如果单词 `j` 出现在帖子 `i` 中，否则为 0。我们可以通过从 `sklearn.preprocessing` 导入 `binarize`
    来二值化子矩阵。然后，执行 `binarize(sub_matrix)` 将产生必要的结果。'
- en: Listing 15.17 Converting word counts to binary values
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.17 将单词计数转换为二进制值
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The binarize function replaces all nonzero elements with ones in any x-dimensional
    array.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `binarize` 函数将任何 x 维数组中的所有非零元素替换为 1。
- en: Now we need to add together the rows of our binary submatrix. Doing so will
    produce a vector of integer counts. Each *i*th vector element will equal the number
    of unique posts in which word `i` is present. To sum the rows of a 2D array, we
    simply need to pass `axis=0` into the `sum` method of the array. Running `binary_matrix.sum(axis=0)`
    returns a vector of unique post counts.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将我们的二进制子矩阵的行加在一起。这样做将产生一个整数计数的向量。每个 *i* 个向量元素将等于包含单词 `i` 的唯一帖子的数量。要计算二维数组的行和，我们只需将
    `axis=0` 传递到数组的 `sum` 方法。执行 `binary_matrix.sum(axis=0)` 返回一个唯一帖子计数的向量。
- en: 'Note A 2D NumPy array contains two axes: axis 0 corresponds to horizontal rows,
    and axis 1 corresponds to vertical columns. Thus, running `binary_ matrix.sum(axis=0)`
    returns a vector of summed rows. Meanwhile, running `binary_matrix.sum(axis=1)`
    returns a vector of summed columns.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：二维 NumPy 数组包含两个轴：axis 0 对应水平行，axis 1 对应垂直列。因此，执行 `binary_matrix.sum(axis=0)`
    返回一个行和的向量。同时，执行 `binary_matrix.sum(axis=1)` 返回一个列和的向量。
- en: Listing 15.18 Summing matrix rows to obtain post counts
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.18 通过求矩阵行和获得帖子计数
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Generally, running multi_dim_array.sum(axis=i) returns a vector of summed
    values across the *i*th axis of a multidimensional array.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通常，执行 multi_dim_array.sum(axis=i) 返回一个多维数组第 *i* 轴上的和的向量。
- en: We should note that the previous three procedures can be combined into a single
    line of code by running `binarize(tf_np_matrix[:,non_zero_indices]).sum(axis=0)`.
    Furthermore, substituting NumPy’s `tf_np_matrix` with SciPy’s `tf_matrix` will
    still produce the same post mention counts.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，前三个过程可以通过执行 `binarize(tf_np_matrix[:,non_zero_indices]).sum(axis=0)`
    合并为单行代码。此外，用 NumPy 的 `tf_np_matrix` 替换 SciPy 的 `tf_matrix` 仍然会产生相同的帖子提及计数。
- en: Listing 15.19 Computing post mention counts in a single line of code
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.19 在单行代码中计算帖子提及计数
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ The numbers in np_post_mentions and csr_post_mentions appear identical. However,
    csr_post_mentions contains an extra set of brackets because the aggregated sum
    of CSR matrix rows doesn’t return a NumPy array; instead, it returns a special
    matrix object. In that object, the 1D vector is represented as a matrix with one
    row and n columns. To convert the matrix into a 1D NumPy array, we must run np.asarray(csr_post_mentions)[0].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `np_post_mentions` 和 `csr_post_mentions` 中的数字看起来相同。然而，`csr_post_mentions`
    包含一个额外的括号集合，因为 CSR 矩阵行的聚合求和不会返回一个 NumPy 数组；相反，它返回一个特殊的矩阵对象。在该对象中，1D 向量表示为一个具有一行和
    n 列的矩阵。要将矩阵转换为 1D NumPy 数组，我们必须运行 `np.asarray(csr_post_mentions)[0]`。
- en: Methods for aggregating matrix rows
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵行聚合方法
- en: '`vector_of_sums = np_matrix.sum(axis=0)`—Sums the rows of a NumPy matrix. If
    `np_matrix` is a TF matrix, then `vector_of_sums[i]` equals the total mention
    count of word `i` in the dataset.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_of_sums = np_matrix.sum(axis=0)`—计算 NumPy 矩阵的行之和。如果 `np_matrix` 是一个
    TF 矩阵，那么 `vector_of_sums[i]` 等于数据集中单词 `i` 的总提及次数。'
- en: '`vector_of_sums = binarize( np_matrix).sum(axis=0)`—Converts a NumPy matrix
    into a binary matrix and then sums its rows. If `np_matrix` is a TF matrix, then
    `vector_of_sums[i]` equals the total count of texts in which word `i` is mentioned.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_of_sums = binarize( np_matrix).sum(axis=0)`—将 NumPy 矩阵转换为二进制矩阵，然后对其行求和。如果
    `np_matrix` 是一个 TF 矩阵，那么 `vector_of_sums[i]` 等于提及单词 `i` 的文本总数。'
- en: '`matrix_1D = binarize(csr_matrix).sum(axis=0)`—Converts a CSR matrix to binary
    and then sums its rows. The returned result is a special one-dimensional matrix
    object—it is not a NumPy vector. `matrix_1D` can be converted into a NumPy vector
    by running `np.asarray(matrix_1D)[0]`.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix_1D = binarize(csr_matrix).sum(axis=0)`—将 CSR 矩阵转换为二进制，然后对其行求和。返回的结果是一个特殊的一维矩阵对象——它不是一个
    NumPy 向量。`matrix_1D` 可以通过运行 `np.asarray(matrix_1D)[0]` 转换为一个 NumPy 向量。'
- en: Based on the printed vector of post mention counts, we know that some words
    appear in thousands of posts. Other words appear in fewer than a dozen posts.
    Let’s transform these counts into document frequencies and align the frequencies
    with `df.Word`. Then we’ll output all the words that are mentioned in at least
    10% of newsgroup posts. These words are likely to appear across the board in a
    variety of posts; thus, we hypothesize that the printed words will not be specific
    to a particular topic. If the hypothesis is correct, these words will not be very
    relevant.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 根据打印的帖子提及计数向量，我们知道一些单词出现在成千上万的帖子中。其他单词出现在不到一打的帖子中。让我们将这些计数转换为文档频率，并将频率与 `df.Word`
    对齐。然后我们将输出至少在 10% 的新组帖子中提及的所有单词。这些单词很可能在各种帖子中普遍出现；因此，我们假设打印的单词不会特定于某个主题。如果假设正确，这些单词将不太相关。
- en: Listing 15.20 Printing the words with the highest document frequency
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.20 打印具有最高文档频率的单词
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ We only choose words with a document frequency greater than 1/10.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们只选择文档频率大于 1/10 的单词。
- en: ❷ As a reminder, the document frequency refers to all our posts. Meanwhile,
    the count refers to just the post at index 0.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 作为提醒，文档频率指的是我们所有的帖子。同时，计数仅指索引 0 的帖子。
- en: Three of the 34 words have a document frequency greater than 0.1\. As expected,
    these words are very general and not car specific. We thus can utilize document
    frequencies for ranking purposes. Let’s rank our words by relevance in the following
    manner. First, we sort the words by count, from greatest to smallest. Then, all
    words with equal count are sorted by document frequency, from smallest to greatest.
    In Pandas, we can execute this dual-column sorting by running `df.sort_values(['Count',
    'Document Frequency'], ascending=[False, True])`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 34 个单词中有 3 个的文档频率大于 0.1。正如预期的那样，这些单词非常通用，并不特定于汽车。因此，我们可以利用文档频率进行排名。让我们以下述方式按相关性对单词进行排名。首先，按计数从大到小对单词进行排序。然后，所有计数相同的单词按文档频率从小到大排序。在
    Pandas 中，我们可以通过运行 `df.sort_values(['Count', 'Document Frequency'], ascending=[False,
    True])` 执行双列排序。
- en: Listing 15.21 Ranking words by both count and document frequency
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.21 按计数和文档频率对单词进行排名
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our sorting was successful. New car-related words, such as *bumper*, are now
    present in our list of top-ranked words. However, the actual sorting procedure
    was rather convoluted: it required us to sort two columns separately. Perhaps
    we can simplify the process by combining the word counts and document frequencies
    into a single score. How can we do this? One approach is to divide each word count
    by its associated document frequency. The resulting value will increase if either
    of the following is true:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的排序是成功的。新的与汽车相关的词汇，例如*保险杠*，现在出现在我们排名前茅的词汇列表中。然而，实际的排序过程相当复杂：它要求我们分别对两列进行排序。也许我们可以通过将词频和文档频率合并为一个单一得分来简化这个过程。我们如何做到这一点？一种方法是将每个词频除以其相关的文档频率。如果以下任何一个条件成立，结果值将会增加：
- en: The word count goes up.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词频增加。
- en: The document frequency goes down.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档频率下降。
- en: Let’s combine the word counts and the document frequencies into a single score.
    We start by computing `1 / document_frequencies`. Doing so produces an array of
    *inverse document frequencies* (IDFs). Next, we multiply `df.Count` by the IDF
    array to compute the combined score. We then add both the IDF values and our combined
    scores to our Pandas table. Finally, we sort on the combined score and output
    the top results.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将词频和文档频率合并为一个单一得分。我们首先计算`1 / document_frequencies`。这样做会产生一个*逆文档频率*（IDFs）数组。接下来，我们将`df.Count`乘以IDF数组来计算组合得分。然后，我们将IDF值和我们的组合得分都添加到我们的Pandas表中。最后，我们根据组合得分进行排序并输出结果。
- en: Listing 15.22 Combining counts and frequencies into a single score
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.22 将计数和频率合并为单一得分
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Our new ranking failed! The word *car* no longer appears at the top of the
    list. What happened? Well, let’s take a look at our table. There is a problem
    with the IDF values: some of them are huge! The printed IDF values range from
    approximately 100 to over 5,000\. Meanwhile, our word-count range is very small:
    from 1 to 4\. Thus, when we multiply word counts by IDF values, the IDF dominates,
    and the counts have no impact on the final results. We need to somehow make our
    IDF values smaller. What should we do?'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新排名失败了！单词*汽车*不再出现在列表的顶部。发生了什么？好吧，让我们看看我们的表格。IDF值存在问题：其中一些非常大！打印的IDF值范围大约从100到超过5,000。同时，我们的词频范围非常小：从1到4。因此，当我们将词频乘以IDF值时，IDF起主导作用，计数对最终结果没有影响。我们需要
    somehow 使我们的IDF值更小。我们应该怎么做？
- en: Data scientists are commonly confronted with numeric values that are too large.
    One way to shrink the values is to apply a logarithmic function. For instance,
    running `np.log10(1000000)` returns `6`. Essentially, a value of 1,000,000 is
    replaced by the count of zeros in that value.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通常会遇到过大的数值。一种缩小数值的方法是应用对数函数。例如，运行`np.log10(1000000)`返回`6`。本质上，1,000,000的值被替换为该值中零的数量。
- en: Listing 15.23 Shrinking a large value using its logarithm
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.23 使用对数缩小大数值
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s recompute our ranking score by running `df.Count * np.log10(df.IDF)`.
    The product of the counts and the shrunken IDF values should lead to a more reasonable
    ranking metric.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行`df.Count * np.log10(df.IDF)`来重新计算我们的排名得分。计数和缩小后的IDF值的乘积应该导致一个更合理的排名指标。
- en: Listing 15.24 Adjusting the combined score using logarithms
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.24 使用对数调整组合得分
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Our adjusted ranking score has yielded good results. The word *car* is once
    again present at the top of the ranked list. Also, *bumper* still appears among
    the top 10 ranked words. Meanwhile, *really* is missing from the list.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调整后的排名得分产生了良好的结果。单词*汽车*再次出现在排名列表的顶部。此外，*保险杠*仍然出现在排名前10的词汇中。同时，*真的*从列表中消失了。
- en: Our effective score is called the *term frequency-inverse document frequency*
    (TFIDF). The TFIDF can be computed by taking the product of the TF (word count)
    and the log of the IDF.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的有效得分被称为*词频-逆文档频率*（TFIDF）。TFIDF可以通过将TF（词频）与IDF的对数相乘来计算。
- en: Note Mathematically, `np.log(1 / x)` is equal to `-np.log(x)`. Therefore, we
    can compute the TFIDF directly from the document frequencies. We simply need to
    run `df.Count * -np.log10(document_frequences)`. Also be aware that other, less
    common formulations of the TFIDF exist in the literature. For instance, when dealing
    with large documents, some NLP practitioners compute the TFIDF as `np.log(df.Count
    + 1) * -np.log10(document_frequences)`. This limits the influence of any very
    common word in a document.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：从数学上讲，`np.log(1 / x)` 等于 `-np.log(x)`。因此，我们可以直接从文档频率中计算TFIDF。我们只需运行 `df.Count
    * -np.log10(document_frequences)`。另外，请注意，文献中存在其他，较少见的TFIDF公式。例如，当处理大型文档时，一些NLP从业者将TFIDF计算为
    `np.log(df.Count + 1) * -np.log10(document_frequences)`。这限制了任何非常常见单词在文档中的影响。
- en: 'The TFIDF is a simple but powerful metric for ranking words in a document.
    Of course, the metric is only relevant if that document is part of a larger document
    group. Otherwise, the computed TFIDF values all equal zero. The metric also loses
    its effectiveness when applied to small collections of similar tests. Nonetheless,
    for most real-world text datasets, the TFIDF produces good ranking results. And
    it has additional uses: it can be utilized to vectorize words in a document. The
    numeric content of `df.Combined` is essentially a vector produced by modifying
    the TF vector stored in `df.Count`. In this same manner, we can transform any
    TF vector into a TFIDF vector. We just need to multiply the TF vector by the log
    of inverse document frequencies.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: TFIDF是一个简单但强大的度量标准，用于对文档中的单词进行排序。当然，该度量标准仅在文档是更大文档组的一部分时才有意义。否则，计算出的TFIDF值都等于零。当应用于相似测试的小集合时，该度量标准也失去了其有效性。尽管如此，对于大多数现实世界的文本数据集，TFIDF产生良好的排序结果。并且它还有其他用途：它可以用来向量化文档中的单词。`df.Combined`的数值内容基本上是由修改存储在`df.Count`中的TF向量产生的向量。以同样的方式，我们可以将任何TF向量转换为TFIDF向量。我们只需要将TF向量乘以逆文档频率的对数。
- en: Is there a benefit to transforming TF vectors into more complicated TFIDF vectors?
    Yes! In larger text datasets, TFIDF vectors provide a greater signal of textual
    similarity and divergence. For example, two texts that are both discussing cars
    are more likely to cluster together if their irrelevant vector elements are penalized.
    Thus, penalizing common words using the IDF improves the clustering of large text
    collections.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将TF向量转换为更复杂的TFIDF向量有什么好处吗？是的！在较大的文本数据集中，TFIDF向量提供了更大的文本相似性和差异信号。例如，两个都在讨论汽车的文本，如果它们的无关向量元素受到惩罚，它们更有可能聚集在一起。因此，使用IDF惩罚常见单词可以改善大型文本集合的聚类。
- en: Note This isn’t necessarily true of smaller datasets, where the number of documents
    is low and the document frequency is high. Consequently, the IDF may be too small
    to meaningfully improve the clustering results.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这并不一定适用于较小的数据集，其中文档数量较少，文档频率较高。因此，IDF可能太小，无法有意义地改进聚类结果。
- en: We therefore stand to gain by transforming our TF matrix into a TFIDF matrix.
    We can easily execute this transformation using custom code. However, it’s more
    convenient to compute the TFIDF matrix with scikit-learn’s built-in `TfidfVectorizer`
    class.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过将我们的TF矩阵转换为TFIDF矩阵，我们可以从中获益。我们可以轻松地使用自定义代码执行此转换。然而，使用scikit-learn内置的`TfidfVectorizer`类计算TFIDF矩阵更为方便。
- en: 15.3.1 Computing TFIDF vectors with scikit-learn
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 使用scikit-learn计算TFIDF向量
- en: That `TfidfVectorizer` class is nearly identical to `CountVectorizer`, except
    that it takes IDF into account during the vectorization process. Next, we import
    `TfidfVectorizer` from `sklearn.feature_extraction.text` and initialize the class
    by running `TfidfVectorizer(stop_words='english')`. The constructed `tfidf_vectorizer`
    object is parameterized to ignore all stop words. Subsequently, executing `tfidf_
    vectorizer.fit_transform(newsgroups.data)` returns a matrix of vectorized TFIDF
    values. The matrix shape is identical to `tf_matrix.shape`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer`类几乎与`CountVectorizer`相同，只是在向量化过程中考虑了IDF。接下来，我们从`sklearn.feature_extraction.text`导入`TfidfVectorizer`并运行`TfidfVectorizer(stop_words=''english'')`来初始化类。构造的`tfidf_vectorizer`对象被参数化为忽略所有停用词。随后，执行`tfidf_vectorizer.fit_transform(newsgroups.data)`返回一个向量化的TFIDF值矩阵。该矩阵的形状与`tf_matrix.shape`相同。'
- en: Listing 15.25 Computing a TFIDF matrix with scikit-learn
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.25 使用scikit-learn计算TFIDF矩阵
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Our `tfidf_vectorizer` has learned the same vocabulary as the simpler TF vectorizer.
    In fact, the indices of words in `tfidf_matrix` are identical to those of `tf_matrix`.
    We can confirm this by calling `tfidf_vectorizer.get_feature_names()`. The method
    call returns an ordered list of words identical to our previously computed `words`
    list.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `tfidf_vectorizer` 学习了与简单的 TF 向量化器相同的词汇表。事实上，`tfidf_matrix` 中单词的索引与 `tf_matrix`
    中的索引相同。我们可以通过调用 `tfidf_vectorizer.get_feature_names()` 来确认这一点。该方法调用返回一个与之前计算的
    `words` 列表相同的有序单词列表。
- en: Listing 15.26 Confirming the preservation of vectorized word indices
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.26 确认向量化单词索引的保留
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Since word order is preserved, we should expect the nonzero indices of `tfidf_
    matrix[0]` to equal our previously computed `non_zero_indices` array. We’ll confirm
    after converting `tfidf_matrix` from a CSR data structure to a NumPy array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于词序被保留，我们应该期望 `tfidf_matrix[0]` 的非零索引等于我们之前计算的 `non_zero_indices` 数组。在将 `tfidf_matrix`
    从 CSR 数据结构转换为 NumPy 数组后，我们将进行确认。
- en: Listing 15.27 Confirming the preservation of nonzero indices
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.27 确认非零索引的保留
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The nonzero indices of `tf_vector` and `tfidf_vector` are identical. We thus
    can add the TFIDF vector as a column in our existing `df` table. Adding a `TFIDF`
    column will allow us to compare scikit-learn’s output with our manually computed
    score.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf_vector` 和 `tfidf_vector` 的非零索引是相同的。因此，我们可以将 TFIDF 向量作为列添加到现有的 `df` 表中。添加
    `TFIDF` 列将允许我们比较 scikit-learn 的输出与手动计算的分数。'
- en: Listing 15.28 Adding a TFIDF vector to the existing Pandas table
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.28 将 TFIDF 向量添加到现有的 Pandas 表中
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Sorting by `df.TFIDF` should produce a relevance ranking that is consistent
    with our previous observations. Let’s verify that both `df.TFIDF` and `df.Combined`
    produce the same word rankings after sorting.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 按 `df.TFIDF` 排序应该产生一个与我们的先前观察一致的关联排名。让我们验证排序后 `df.TFIDF` 和 `df.Combined` 是否产生相同的单词排名。
- en: Listing 15.29 Sorting words by `df.TFIDF`
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.29 按 `df.TFIDF` 排序单词
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Our word rankings have remained unchanged. However, the values of the `TFIDF`
    and `Combined` columns are not identical. Our top 10 manually computed `Combined`
    values are all greater than 1, but all of scikit-learn’s *TFIDF* values are less
    than 1\. Why is this the case?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键词排名保持不变。然而，`TFIDF` 和 `Combined` 列的值并不相同。我们手动计算的顶级 10 个 `Combined` 值都大于
    1，但 scikit-learn 的所有 *TFIDF* 值都小于 1。为什么会这样？
- en: As it turns out, scikit-learn automatically normalizes its TFIDF vector results.
    The magnitude of `df.TFIDF` has been modified to equal 1\. We can confirm by calling
    `norm(df.TFIDF.values).`
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，scikit-learn 自动规范化其 TFIDF 向量结果。`df.TFIDF` 的幅度已被修改为等于 1。我们可以通过调用 `norm(df.TFIDF.values)`
    来确认。
- en: Note To turn off normalization, we must pass `norm=None` into the vectorizer’s
    initialization function. Running `TfidfVectorizer(norm=None, stop_ words='english')`
    returns a vectorizer in which normalization has been deactivated.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要关闭规范化，我们必须在向量化器的初始化函数中传递 `norm=None`。运行 `TfidfVectorizer(norm=None, stop_words='english')`
    返回一个已关闭规范化的向量化器。
- en: Listing 15.30 Confirming that our TFIDF vector is normalized
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.30 确认我们的 TFIDF 向量已规范化
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Why would scikit-learn automatically normalize the vectors? For our own benefit!
    As discussed in section 13, it’s easier to compute text vector similarity when
    all vector magnitudes equal 1\. Consequently, our normalized TFIDF matrix is primed
    for similarity analysis.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 scikit-learn 会自动规范化向量？为了我们自己的利益！如第 13 节所述，当所有向量的幅度等于 1 时，计算文本向量相似性更容易。因此，我们的规范化
    TFIDF 矩阵已准备好进行相似性分析。
- en: Common scikit-learn TfidfVectorizer methods
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 scikit-learn TfidfVectorizer 方法
- en: '`tfidf_vectorizer = TfidfVectorizer(stopwords=''english'')`—Initializes a `TfidfVectorizer`
    object capable of vectorizing input texts based on their TFIDF values. The object
    is preset to filter common English stop words.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tfidf_vectorizer = TfidfVectorizer(stopwords=''english'')` — 初始化一个 `TfidfVectorizer`
    对象，该对象能够根据输入文本的 TFIDF 值进行向量化。该对象预设为过滤常见的英语停用词。'
- en: '`tfidf_matrix = tfidf_vectorizer.fit_transform(texts)`—Executes TFIDF vectorization
    on a list of input texts using the initialized `vectorizer` object and returns
    a CSR matrix of normalized TFIDF values. Each row of the matrix is automatically
    normalized, for easier similarity computation.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tfidf_matrix = tfidf_vectorizer.fit_transform(texts)` — 使用初始化的 `vectorizer`
    对象在输入文本列表上执行 TFIDF 向量化，并返回一个规范化 TFIDF 值的 CSR 矩阵。矩阵的每一行都会自动规范化，以便更容易进行相似性计算。'
- en: '`vocabulary_list = tfidf_vectorizer.get_feature_names()`—Returns the vocabulary
    list associated with the columns of a computed TFIDF matrix. Each column `j` of
    the matrix corresponds to `vocabulary_list[j]`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocabulary_list = tfidf_vectorizer.get_feature_names()`—返回与计算TFIDF矩阵的列关联的词汇表。矩阵的每个`j`列对应于`vocabulary_list[j]`。'
- en: 15.4 Computing similarities across large document datasets
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 在大型文档数据集中计算相似度
- en: 'Let’s answer a simple question: which of our newsgroup posts is most similar
    to `newsgroups.post[0]`? We can get the answer by computing all the cosine similarities
    between `tfidf_np_matrix` and `tf_np_matrix[0]`. As discussed in section 13, these
    similarities can be obtained by taking the product of `tfidf_np_matrix` and `tfidf_
    matrix[0]`. The simple multiplication between the matrix and the vector is sufficient
    because all rows in the matrix have a magnitude of 1.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回答一个简单的问题：我们的新闻组帖子中哪一个与`newsgroups.post[0]`最相似？我们可以通过计算`tfidf_np_matrix`和`tf_np_matrix[0]`之间的所有余弦相似度来得到答案。如第13节所述，这些相似度可以通过取`tfidf_np_matrix`和`tfidf_matrix[0]`的乘积来获得。矩阵和向量之间的简单乘法就足够了，因为矩阵中的所有行都具有长度为1。
- en: Listing 15.31 Computing similarities to a single newsgroup post
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.31 计算与单个新闻组帖子的相似度
- en: '[PRE30]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The matrix-vector product takes a few seconds to complete. Its output is a
    vector of cosine similarities: each *i*th index of the vector corresponds to the
    cosine similarity between `newsgroups.data[0]` and `newsgroups.data[i]`. From
    the printout, we can see that `cosine_similarities[0]` is equal to 1.0\. This
    is not surprising since `newsgroups_data[0]` will have a perfect similarity with
    itself. What is the next-highest similarity in the vector? We can find out by
    calling `np.argsort(cosine_similarities)[-2]`. The `argsort` call sorts the array
    indices by their ascending values. So, the second-to-last index will correspond
    to the post with the second-highest similarity.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵-向量乘法需要几秒钟才能完成。它的输出是一个余弦相似度向量：向量的每个*i*个索引对应于`newsgroups.data[0]`和`newsgroups.data[i]`之间的余弦相似度。从打印输出中，我们可以看到`cosine_similarities[0]`等于1.0。这并不令人惊讶，因为`newsgroups_data[0]`将与自身具有完美的相似度。向量中下一个最高的相似度是多少？我们可以通过调用`np.argsort(cosine_similarities)[-2]`来找出。`argsort`调用按升序值对数组索引进行排序。因此，倒数第二个索引将对应于相似度第二高的帖子。
- en: Note We are assuming that no other post exists with a perfect similarity of
    1\. Also, note that we can achieve the same result by calling `np.argmax(cosine_
    similarities[1:]) + 1`, although this approach only works for posts at index 0.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们假设不存在其他帖子具有完美的相似度1。另外，请注意，我们可以通过调用`np.argmax(cosine_similarities[1:]) +
    1`来达到相同的结果，尽管这种方法仅适用于索引0的帖子。
- en: We now extract that index and print its corresponding similarity. We also print
    the corresponding text to confirm its overlap with the car post stored in `newsgroups
    .data[0]`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在提取该索引并打印其对应的相似度。我们还打印相应的文本以确认其与存储在`newsgroups .data[0]`中的汽车帖子的重叠。
- en: Listing 15.32 Finding the most similar newsgroup post
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.32 寻找最相似的新闻组帖子
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The printed text is a reply to the car post at index 0\. The reply includes
    the original post, which is a question about a certain car brand. We see a detailed
    answer to the question near the very bottom of the reply. Due to textual overlap,
    both the original post and the reply are very similar to each other. Their cosine
    similarity is 0.64, which does not seem like a large number. However, in extensive
    text collections, a cosine similarity greater than 0.6 is a good indicator of
    overlapping content.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的文本是对索引0处的汽车帖子的回复。回复中包含了原始帖子，这是一个关于某个汽车品牌的问题。我们在回复的底部附近看到了对这个问题的详细答案。由于文本重叠，原始帖子和回复彼此之间非常相似。它们的余弦相似度为0.64，这个数字看起来并不大。然而，在大量的文本集合中，余弦相似度大于0.6是一个重叠内容的良好指标。
- en: Note As discussed in section 13, the cosine similarity can easily be converted
    into the Tanimoto similarity, which has a deeper theoretical basis for text overlap.
    We can convert `cosine_similarities` into Tanimoto similarities by running `cosine_similarities
    / (2 - cosine_similarities)`. However, that conversion will not change our final
    results. Choosing the top index of the Tanimoto array will still return the same
    posted reply. Thus, for simplicity’s sake, we focus on the cosine similarity during
    our next few text-comparison examples.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意如第13节所述，余弦相似度可以很容易地转换为Tanimoto相似度，它对文本重叠有更深入的理论基础。我们可以通过运行`cosine_similarities
    / (2 - cosine_similarities)`将`cosine_similarities`转换为Tanimoto相似度。然而，这种转换不会改变我们的最终结果。选择Tanimoto数组的顶部索引仍然会返回相同的帖子回复。因此，为了简单起见，我们在接下来的几个文本比较示例中关注余弦相似度。
- en: Thus far, we’ve only analyzed the car post at index 0\. Let’s extend our analysis
    to another post. We’ll pick a newsgroup post at random, choose its most similar
    neighbor, and then output both posts, along with their cosine similarity. To make
    this exercise more interesting, we’ll first compute a matrix of all-by-all cosine
    similarities. We’ll then use the matrix to select our random pair of similar posts.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只分析了索引为0的汽车帖子。让我们将我们的分析扩展到另一篇帖子。我们将随机选择一个新闻组帖子，选择其最相似的邻居，然后输出这两篇帖子及其余弦相似度。为了使这个练习更有趣，我们首先计算所有帖子之间的余弦相似度矩阵。然后，我们将使用这个矩阵来选择我们的随机相似帖子对。
- en: Note Why are we computing a matrix of all-by-all similarities? Mostly, it’s
    to practice what we’ve learned in the previous section. However, having access
    to that matrix does confer certain benefits. Suppose we wish to increase our network
    of neighboring posts from 2 to 10\. We also wish to include the neighbor of every
    neighbor (similar to our derivation of DBSCAN in section 10). Under such circumstances,
    it is much more efficient to compute all text similarities in advance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意为什么我们要计算所有帖子之间的相似度矩阵？主要原因是练习我们在上一节中学到的内容。然而，能够访问这个矩阵确实带来了一定的好处。假设我们希望将相邻帖子的网络从2增加到10。我们还希望包括每个邻居的邻居（类似于我们在第10节中推导的DBSCAN）。在这种情况下，预先计算所有文本相似度要高效得多。
- en: How do we compute the matrix of all-by-all cosine similarities? The naive approach
    is to multiply `tfidf_np_matrix` with its transpose. However, for reasons discussed
    in section 13, this matrix multiplication is not computationally efficient. Our
    TFIDF matrix has over 100,000 columns. We need to reduce the matrix size before
    executing the multiplication. In the previous section, we learned how to reduce
    the column count using scikit-learn’s `TruncatedSVD` class. The class is able
    to shrink a matrix to a specified number of columns. The reduced column count
    is determined by the `n_components` parameter. According to scikit-learn’s documentation,
    an `n_components` value of 100 is recommended for processing text data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算所有帖子之间的余弦相似度矩阵？直观的方法是将`tfidf_np_matrix`与其转置相乘。然而，如第13节所述，这种矩阵乘法在计算上并不高效。我们的TFIDF矩阵有超过100,000列。在执行乘法之前，我们需要减小矩阵的大小。在上一节中，我们学习了如何使用scikit-learn的`TruncatedSVD`类来减少列数。该类可以将矩阵缩小到指定的列数。减少的列数由`n_components`参数确定。根据scikit-learn的文档，处理文本数据时建议`n_components`值为100。
- en: 'Note Scikit-learn’s documentation occasionally provides useful parameters for
    common algorithm applications. For instance, take a look at the `TruncatedSVD`
    documentation at [http://mng.bz/PXP9](http://mng.bz/PXP9). According to that page,
    “Truncated SVD works on term count/tf-idf matrices as returned by the vectorizers
    in sklearn.feature_extraction.text. In that context, it is known as latent semantic
    analysis (LSA).” Further down, the documentation describes the `n_components`
    parameter like this: “Desired dimensionality of output data. Must be strictly
    less than the number of features. For LSA, a value of 100 is recommended.”'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Scikit-learn的文档偶尔会为常见的算法应用提供有用的参数。例如，看看`TruncatedSVD`的文档[http://mng.bz/PXP9](http://mng.bz/PXP9)。根据该页面，“截断奇异值分解（Truncated
    SVD）在sklearn.feature_extraction.text中的向量器返回的词频/逆文档频率矩阵上工作。在这种情况下，它被称为潜在语义分析（LSA）。”文档进一步描述了`n_components`参数：“输出数据的期望维度。必须严格小于特征数。对于LSA，建议值为100。”
- en: Most NLP practitioners agree that passing `n_components=100` reduces a TFIDF
    matrix to an efficient size while maintaining useful column information. Next,
    we’ll follow this recommendation by running `TruncatedSVD(n_components=100).fit_transform
    (tfidf_matrix)`. The method call will return a 100-column `shrunk_matrix` result
    that will be a 2D NumPy array even if we pass the SciPy-based `tfidf_matrix` as
    our input.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数自然语言处理（NLP）从业者都认为通过传递`n_components=100`可以将TFIDF矩阵减少到高效的大小，同时保持有用的列信息。接下来，我们将遵循这一建议，通过运行`TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)`来实现。方法调用将返回一个100列的`shrunk_matrix`结果，即使我们传递基于SciPy的`tfidf_matrix`作为我们的输入，它也将是一个2D
    NumPy数组。
- en: Listing 15.33 Dimensionally reducing `tfidf_matrix` using SVD
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.33 使用SVD对`tfidf_matrix`进行降维
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ The final SVD output depends on the orientation of the computed eigenvectors.
    As we saw in the previous section, that orientation is determined randomly. Thus,
    we run np.random.seed(0) to ensure consistent results.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 最终的奇异值分解（SVD）输出取决于计算出的特征向量的方向。正如我们在上一节中看到的，这个方向是随机确定的。因此，我们运行`np.random.seed(0)`以确保结果的一致性。
- en: Our shrunk matrix contains just 100 columns. We can now efficiently compute
    the cosine similarities by running `shrunk_matrix @ shrunk_matrix.T`. However,
    first we need to confirm that the matrix rows remain normalized. Let’s check the
    magnitude of `shrunk_matrix[0].`
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们缩小的矩阵只有100列。我们现在可以通过运行`shrunk_matrix @ shrunk_matrix.T`来有效地计算余弦相似度。然而，首先我们需要确认矩阵行仍然保持归一化。让我们检查`shrunk_matrix[0]`的幅度。
- en: Listing 15.34 Checking the magnitude of `shrunk_matrix[0]`
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.34 检查`shrunk_matrix[0]`的幅度
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The magnitude of the row is less than 1\. Scikit-learn’s SVD output has not
    been automatically normalized. We need to manually normalize the matrix before
    computing the similarities. Scikit-learn’s built-in `normalize` function will
    assist us in that process. We import `normalize` from `sklearn.preprocessing`
    and then run `normalize (shrunk_matrix)`. The magnitude of the rows in the resulting
    normalized matrix will subsequently equal 1.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 行的幅度小于1。Scikit-learn的SVD输出没有自动归一化。在计算相似度之前，我们需要手动归一化矩阵。Scikit-learn的内置`normalize`函数将帮助我们完成这个过程。我们从`sklearn.preprocessing`导入`normalize`，然后运行`normalize(shrunk_matrix)`。随后，归一化矩阵中行的幅度将等于1。
- en: Listing 15.35 Normalizing the SVD output
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.35 正规化SVD输出
- en: '[PRE34]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The shrunken matrix has been normalized. Now, running `shrunk_norm_matrix @
    shrunk_norm_matrix.T` should produce a matrix of all-by-all cosine similarities.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 缩小的矩阵已经归一化。现在，运行`shrunk_norm_matrix @ shrunk_norm_matrix.T`应该产生一个所有对之间的余弦相似度矩阵。
- en: Listing 15.36 Computing all-by-all cosine similarities
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.36 计算所有对之间的余弦相似度
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We have our similarity matrix. Let’s use it to choose a random pair of very
    similar texts. We start by randomly selecting a post at some `index1`. We next
    select an index of `cosine_similarities[index1]` that has the second-highest cosine
    similarity. Then, we print both the indices and their similarity before displaying
    the texts.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了相似度矩阵。让我们用它来选择一对非常相似的文本。我们首先随机选择一个位于某个`index1`的帖子。接下来，我们选择`cosine_similarities[index1]`中具有第二高余弦相似度的索引。然后，我们在显示文本之前打印这两个索引及其相似度。
- en: Listing 15.37 Choosing a random pair of similar posts
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.37 选择一对相似的随机帖子
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Listing 15.38 Printing a randomly chosen post
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.38 打印随机选择的帖子
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ This post contains blank lines. We filter out these lines to conserve space.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这篇帖子包含空白行。我们过滤掉这些行以节省空间。
- en: Once again, the printed post is a question. It’s safe to assume that the post
    at `index1` is an answer to that question.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，打印的帖子是一个问题。可以安全地假设`index1`处的帖子是对该问题的回答。
- en: Listing 15.39 Printing the most similar post response
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.39 打印最相似的帖子回复
- en: '[PRE38]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Thus far, we have examined two pairs of similar posts. Each post pair was composed
    of a question and a reply, where the question was included in the reply. Such
    boring pairs of overlapping texts are trivial to extract. Let’s challenge ourselves
    to find something more interesting. We’ll search for clusters of similar texts
    where posts in a cluster share some text without perfectly overlapping.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经检查了两对相似帖子。每一对帖子都由一个问题和一个回复组成，其中问题包含在回复中。这样的重叠文本对非常容易提取。让我们挑战自己找到更有趣的东西。我们将寻找相似文本的簇，其中簇中的帖子共享一些文本，而不完全重叠。
- en: 15.5 Clustering texts by topic
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 通过主题对文本进行聚类
- en: 'In section 10, we introduced two clustering algorithms: K-means and DBSCAN.
    K-means can only cluster on Euclidean distance. Conversely, DBSCAN can cluster
    based on any distance metric. One possible metric is cosine distance, which equals
    1 minus cosine similarity.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 10 节中，我们介绍了两种聚类算法：K-means 和 DBSCAN。K-means 只能在欧几里得距离上进行聚类。相反，DBSCAN 可以基于任何距离度量进行聚类。一个可能的度量是余弦距离，它等于
    1 减去余弦相似度。
- en: Note Why use cosine distance instead of cosine similarity? Well, all clustering
    algorithms assume that two identical data points share a distance of 0\. Meanwhile,
    the cosine similarity equals 0 if two data points have nothing in common. It also
    equals 1 when two data points are perfectly identical. We can fix this discrepancy
    by running `1 - cosine_similarity_matrix`, thus converting our result to cosine
    distance. After the conversion, two identical texts will share a cosine distance
    of 0.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为什么使用余弦距离而不是余弦相似度？好吧，所有聚类算法都假设两个相同的数据点共享一个距离为 0。同时，如果两个数据点没有任何共同点，余弦相似度等于
    0。当两个数据点完全相同的时候，它也等于 1。我们可以通过运行 `1 - cosine_similarity_matrix` 来修复这个差异，从而将我们的结果转换为余弦距离。转换后，两个相同文本将共享一个余弦距离为
    0。
- en: Cosine distance is commonly used in conjunction with DBSCAN. That is why scikit-learn’s
    `DBSCAN` implementation permits us to specify cosine distance directly during
    object initialization. We simply need to pass `metric='cosine'` into the class
    constructor. Doing so will initialize a `cluster_model` object that’s set to cluster
    based on cosine distance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离通常与 DBSCAN 一起使用。这就是为什么 scikit-learn 的 `DBSCAN` 实现允许我们在对象初始化期间直接指定余弦距离。我们只需将
    `metric='cosine'` 传递给类构造函数即可。这样做将初始化一个设置为基于余弦距离进行聚类的 `cluster_model` 对象。
- en: Note Scikit-learn’s `DBSCAN` implementation computes cosine distance by first
    recomputing `cosine_similarity_matrix`. Alternatively, we can avoid the recomputation
    by passing `metric='precomputed'` into the constructor. Doing so initializes a
    `cluster_model` object that’s set to cluster on a matrix of precomputed distances.
    Next, running `cluster_model.fit_transform(1 - cosine_ similarity_matrix)` should
    theoretically return the clustering results. However, practically speaking, negative
    values in the distance matrix (which can arise from floating-point errors) could
    cause issues during clustering. All negative values in the distance matrix must
    be replaced with zero before clustering. This operation would need to be run manually
    in NumPy by executing `x[x < 0] = 0`, where `x = 1 - cosine_similarity_matrix`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Scikit-learn 的 `DBSCAN` 实现通过首先重新计算 `cosine_similarity_matrix` 来计算余弦距离。或者，我们可以通过将
    `metric='precomputed'` 传递给构造函数来避免重新计算。这样做将初始化一个设置为基于预计算距离矩阵进行聚类的 `cluster_model`
    对象。接下来，运行 `cluster_model.fit_transform(1 - cosine_similarity_matrix)` 理论上应该返回聚类结果。然而，实际上，距离矩阵中的负值（可能由浮点误差引起）可能会在聚类过程中造成问题。在聚类之前，距离矩阵中的所有负值都必须替换为零。这个操作需要通过在
    NumPy 中执行 `x[x < 0] = 0` 来手动运行，其中 `x = 1 - cosine_similarity_matrix`。
- en: 'Let’s cluster `shrunk_matrix` with DBSCAN based on cosine distance. During
    clustering, we will make the following reasonable assumptions:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据余弦距离使用 DBSCAN 对 `shrunk_matrix` 进行聚类。在聚类过程中，我们将做出以下合理的假设：
- en: Two newsgroup posts fall in a cluster if they share a cosine similarity of at
    least 0.6 (which corresponds to a cosine distance of no greater than 0.4).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个新闻组帖子共享至少 0.6 的余弦相似度（这对应于余弦距离不超过 0.4），它们就会落在同一个聚类中。
- en: A cluster contains at least 50 newsgroup posts.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个聚类至少包含 50 个新闻组帖子。
- en: Based on these assumptions, the algorithm’s `eps` and `min_samples` parameters
    should equal 0.4 and 50, respectively. Thus, we initialize `DBSCAN` by running
    `DBSCAN(eps=0.4, min_samples=50, metric='cosine')`. Then we use the initialized
    `cluster_model` object to cluster `shrunk_matrix`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些假设，算法的 `eps` 和 `min_samples` 参数应该分别等于 0.4 和 50。因此，我们通过运行 `DBSCAN(eps=0.4,
    min_samples=50, metric='cosine')` 来初始化 `DBSCAN`。然后我们使用初始化的 `cluster_model` 对象对
    `shrunk_matrix` 进行聚类。
- en: Listing 15.40 Clustering newsgroup posts with DBSCAN
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.40 使用 DBSCAN 对新闻组帖子进行聚类
- en: '[PRE39]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We’ve generated an array of clusters. Let’s quickly estimate the clustering
    quality. We already know that the newsgroups dataset covers 20 newsgroup categories.
    Some of the category names are very similar to each other; other topics are incredibly
    broad. Thus, it’s reasonable to assume that the dataset covers 10 to 25 truly
    diverging topics. Consequently, we can expect our `clusters` array to contain
    somewhere between 10 and 25 clusters—otherwise, there’s something wrong with our
    input clustering parameters. We now count the number of clusters.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经生成了一组聚类。让我们快速评估聚类质量。我们已经知道新闻组数据集涵盖了20个新闻组类别。一些类别的名称非常相似；其他主题则极其广泛。因此，我们可以合理地假设数据集涵盖了10到25个真正不同的主题。因此，我们可以预期我们的`clusters`数组将包含大约10到25个聚类——否则，我们的输入聚类参数可能有问题。我们现在计算聚类的数量。
- en: Listing 15.41 Counting the number of DBSCAN clusters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.41 计算DBSCAN聚类数量
- en: '[PRE40]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We’ve generated just three clusters, which is way lower than our expected cluster
    count. Clearly, our DBSCAN parameters were wrong. Is there some algorithmic method
    to adjust these parameters accordingly? Or are there perhaps well-known DBSCAN
    settings in the literature that yield acceptable text clusters? Sadly, no. As
    it happens, DBSCAN clustering of text is highly sensitive to the inputted document
    data. DBSCAN parameters for clustering specific types of texts (such as newsgroup
    posts) are unlikely to transfer well to other document categories (such as news
    articles or emails). Consequently, unlike with SVD, the DBSCAN algorithm lacks
    consistent NLP parameters. This does not mean DBSCAN cannot be applied to our
    text data, but the appropriate `eps` and `min_samples` inputs must be determined
    by trial and error. Unfortunately, DBSCAN lacks a well-established algorithm for
    optimizing these two crucial parameters.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只生成了三个聚类，这远远低于我们预期的聚类数量。显然，我们的DBSCAN参数是错误的。是否有某种算法方法可以相应地调整这些参数？或者文献中是否有已知的DBSCAN设置可以产生可接受的文本聚类？遗憾的是，没有。碰巧的是，文本的DBSCAN聚类对输入的文档数据非常敏感。用于聚类特定类型文本（如新闻组帖子）的DBSCAN参数不太可能很好地转移到其他文档类别（如新闻文章或电子邮件）。因此，与SVD不同，DBSCAN算法缺乏一致的NLP参数。这并不意味着DBSCAN不能应用于我们的文本数据，但适当的`eps`和`min_samples`输入必须通过试错法确定。不幸的是，DBSCAN缺乏一个经过良好建立的算法来优化这两个关键参数。
- en: 'K-means, on the other hand, takes as input a single *K* parameter. We can estimate
    *K* using the elbow plot technique, which we introduced in section 10\. However,
    the K-means algorithm can only cluster based on Euclidean distance: it cannot
    process cosine distance. Is this a problem? Not necessarily. As it happens, we’re
    in luck! All rows in `shrunk_norm_matrix` are normalized unit vectors. In section
    13, we showed how the Euclidean distance of two normalized vectors `v1` and `v2`
    equals `(2 - 2 * v1 @ v2) ** 0.5`. Furthermore, the cosine distance between the
    vectors equals `1 - v1 @ v2`. With basic algebra, we can easily show that the
    Euclidean distance of two normalized vectors is proportional to the square root
    of the cosine distance. The two distance metrics are very closely related! This
    relationship provides us with mathematical justification for clustering `shrunk_norm_matrix`
    using K-means.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，K-means算法以单个*K*参数作为输入。我们可以使用我们在第10节中介绍的肘图技术来估计*K*。然而，K-means算法只能根据欧几里得距离进行聚类：它不能处理余弦距离。这是问题吗？不一定。碰巧的是，我们很幸运！`shrunk_norm_matrix`中的所有行都是归一化单位向量。在第13节中，我们展示了两个归一化向量`v1`和`v2`之间的欧几里得距离等于`(2
    - 2 * v1 @ v2) ** 0.5`。此外，向量之间的余弦距离等于`1 - v1 @ v2`。通过基本的代数运算，我们可以轻松地证明两个归一化向量之间的欧几里得距离与余弦距离的平方根成正比。这两个距离度量非常密切相关！这种关系为我们使用K-means聚类`shrunk_norm_matrix`提供了数学上的合理性。
- en: Warning If two vectors are normalized, their Euclidean distance is an adequate
    substitute for cosine similarity. However, this is not the case for non-normalized
    vectors. Thus, we should never apply K-means to text-derived matrices if these
    matrices have not been normalized.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果两个向量是归一化的，它们的欧几里得距离是余弦相似度的充分替代。然而，对于非归一化向量来说，情况并非如此。因此，如果我们没有对这些矩阵进行归一化，我们永远不应该将K-means应用于从文本派生的矩阵。
- en: Research has shown that K-means clustering provides a reasonable segmentation
    of text data. This may seem confusing, since in previous sections, DBSCAN gave
    us superior results. Regrettably, in data science, the right choice of algorithm
    varies by domain. Rarely can one algorithm solve every type of problem. By analogy,
    not every task requires a hammer—sometimes we need a screwdriver or a wrench.
    Data scientists must remain flexible when choosing the appropriate tool for a
    given task.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，K-means聚类为文本数据提供了合理的分段。这可能会让人感到困惑，因为在之前的章节中，DBSCAN给出了更好的结果。遗憾的是，在数据科学中，算法的正确选择因领域而异。很少有算法可以解决所有类型的问题。打个比方，不是每个任务都需要锤子——有时我们需要螺丝刀或扳手。数据科学家在选择特定任务的适当工具时必须保持灵活。
- en: Note Sometimes we may not know which algorithm to use on a given problem. When
    we are stuck, it helps to read known solutions online. The scikit-learn website
    in particular provides insightful solutions to common problems. For instance,
    the scikit-learn site offers example code for clustering text at [http://mng.bz/wQ9q](http://mng.bz/wQ9q).
    Notably, the documented code illustrates how K-means can cluster text vectors
    (after SVD processing). The documentation also specifies that the vectors must
    be normalized “for better results.”
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有时我们可能不知道在给定的问题上应该使用哪种算法。当我们陷入困境时，阅读在线上的已知解决方案会有所帮助。特别是scikit-learn网站提供了对常见问题的深入解决方案。例如，scikit-learn网站提供了在[http://mng.bz/wQ9q](http://mng.bz/wQ9q)上的聚类文本的示例代码。值得注意的是，文档中的代码说明了K-means如何在SVD处理之后聚类文本向量。文档还指定，向量必须进行归一化“以获得更好的结果。”
- en: Let’s utilize K-means to cluster `shrunk_norm_matrix` into *K* different groups.
    We first need to assign a value for *K*. Supposedly, our texts belong to 20 different
    newsgroup categories. But as mentioned earlier, the actual cluster count may not
    equal 20\. We want to estimate the true value of *K* by generating an elbow plot.
    To this end, we’ll execute K-means across *K* values of 1 through 60 and then
    plot the inertia results.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用K-means将`shrunk_norm_matrix`聚类成*K*个不同的组。我们首先需要为*K*分配一个值。假设我们的文本属于20个不同的新闻组类别。但如前所述，实际的聚类数量可能不等于20。我们希望通过生成肘图来估计*K*的真实值。为此，我们将执行从1到60的*K*值范围内的K-means，然后绘制惯性结果。
- en: However, we face a problem. Our dataset is large, containing over 10,000 points.
    The scikit-learn `KMeans` implementation will take a second or two to cluster
    the data. That lag time is acceptable for a single clustering run, but it’s not
    acceptable for 60 different runs, where the execution time may add up to multiple
    minutes. How can we speed up the K-means running time? Well, one approach is to
    sample randomly from our hefty dataset. We can choose 1,000 random newsgroup posts
    during the K-means centroid calculation, then select another random 1,000 posts,
    and then update the cluster centers based on post content. In this manner, we
    can iteratively estimate the centers through sampling. At no point will we need
    to analyze the full dataset all at once. This modified version of the K-means
    algorithm is known as *mini-batch K-means*. Scikit-learn offers a mini-batch implementation
    with its `MiniBatchKMeans` class. `MiniBatchKMeans` is nearly identical in its
    methods to the standard `KMeans` class. Next, we import both implementations and
    compare their running times.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们面临一个问题。我们的数据集很大，包含超过10,000个点。scikit-learn的`KMeans`实现将花费一秒钟或两秒钟来聚类数据。这种延迟对于单次聚类运行是可以接受的，但对于60次不同的运行，执行时间可能会累计到几分钟。我们如何加快K-means的运行时间？好吧，一种方法是从我们庞大的数据集中随机采样。我们可以在K-means质心计算期间选择1,000篇随机的新闻组帖子，然后选择另外1,000篇随机帖子，然后根据帖子内容更新聚类中心。通过这种方式，我们可以通过采样迭代地估计中心。在任何时候，我们都不需要一次性分析整个数据集。这种修改后的K-means算法被称为*mini-batch
    K-means*。Scikit-learn通过其`MiniBatchKMeans`类提供了一个mini-batch实现。`MiniBatchKMeans`的方法几乎与标准的`KMeans`类相同。接下来，我们将导入这两个实现并比较它们的运行时间。
- en: Note We should emphasize that even with `MiniBatchKMeans`, efficient computation
    time is possible only because we have dimensionally reduced our data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们应该强调，即使是使用`MiniBatchKMeans`，只有在我们已经将数据降维的情况下，才能实现高效的计算时间。
- en: Listing 15.42 Comparing `KMeans` to `MiniBatchKMeans`
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.42 比较`KMeans`与`MiniBatchKMeans`
- en: '[PRE41]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Computes the running time of each clustering algorithm implementation
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算每个聚类算法实现的运行时间
- en: ❷ Running time.time() returns the current time, in seconds.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行time.time()返回当前时间，单位为秒。
- en: '`MiniBatchKMeans` runs approximately 10 times faster than regular `KMeans`.
    That decrease in running time carries a minor cost: it has been shown that `MiniBatchKMeans`
    produces clusters of slightly lower quality than `KMeans`. However, our immediate
    concern isn’t cluster quality; rather, we are interested in estimating *K* using
    an elbow plot across `range(1, 61)`. The speedy `MiniBatchKMeans` implementation
    should serve us just fine as an estimation tool.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`MiniBatchKMeans`的运行速度大约是常规`KMeans`的10倍。这种运行时间的减少带来了一些小的代价：已经证明`MiniBatchKMeans`产生的聚类质量略低于`KMeans`。然而，我们现在的关注点不是聚类质量；我们更感兴趣的是使用肘图在`range(1,
    61)`范围内估计K。快速的`MiniBatchKMeans`实现应该作为估计工具对我们来说足够好了。'
- en: We now generate the plot using mini-batch K-means. We also add grid lines to
    the plot, to better isolate potential elbow coordinates. As seen in section 10,
    we can visualize such grid lines by calling `plt.grid(True)`. Finally, we want
    to compare the elbow with the official newsgroup category count. For this purpose,
    we will plot a vertical line at a *K* value of 20 (figure 15.1).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用mini-batch K-means生成该图。我们还向图中添加了网格线，以更好地隔离潜在的肘坐标。如第10节所示，我们可以通过调用`plt.grid(True)`来可视化这些网格线。最后，我们想要将肘图与官方新闻组类别计数进行比较。为此，我们将在K值为20的位置绘制一条垂直线（图15.1）。
- en: Listing 15.43 Plotting an elbow curve using `MiniBatchKMeans`
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.43 使用`MiniBatchKMeans`绘制肘图
- en: '[PRE42]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](../Images/15-01.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/15-01.png)'
- en: Figure 15.1 An elbow plot generated using mini-batch K-means across *K* values
    ranging from 1 to 61\. The precise location of an elbow is difficult to determine.
    However, the plotted curve is noticeably steeper before a *K* of 20\. Also, it
    begins to flatten after a *K* of 20\. We thus infer that a value of approximately
    20 is an appropriate input for *K*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 使用mini-batch K-means在1到61的K值范围内生成的肘图。肘的确切位置难以确定。然而，在K值为20之前，绘制的曲线明显更陡峭。此外，在K值为20之后，曲线开始变平。因此，我们推断大约20的值是K的一个合适的输入。
- en: 'Our plotted curve decreases smoothly. The precise location of a bent elbow–shaped
    transition is difficult to spot. We do see that the curve is noticeably steeper
    when *K* is less than 20\. Somewhere after 20 clusters, the curve begins to flatten
    out, but there is no singular location at which the elbow suddenly bends. The
    dataset lacks a perfect *K* at which the texts fall into natural clusters. Why?
    For one thing, real-world text is messy and nuanced. Categorical boundaries are
    not always obvious. For instance, we can engage in a conversation about technology
    or a conversation about politics. In addition, we can openly discuss how politics
    is influenced by technology. Seemingly distinct discussion topics can meld together,
    forming novel topics of their own. Due to such complexities, there rarely exists
    a single, smooth transition between text clusters. Consequently, figuring out
    an ideal *K* is hard. But we can make certain useful inferences: based on the
    elbow plot, we can infer that 20 is a reasonable estimate of the *K* parameter.
    Yes, the curve is fuzzy, and perhaps an input of 18 or 22 would also do. However,
    we need to start somewhere, and a *K* of 20 makes more sense than a *K* of 3 or
    50\. Our solution isn’t perfect, but it’s feasible. Sometimes, when we deal with
    real-world data, a feasible solution is the best that we can expect.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制的曲线平滑下降。弯曲肘形状的精确位置难以辨认。我们确实看到当K小于20时，曲线明显更陡峭。在20个聚类之后，曲线开始变平，但没有一个单一的位置肘突然弯曲。数据集没有完美的K值，使得文本自然地落入聚类中。为什么？一方面，现实世界的文本是杂乱和细微的。分类边界并不总是明显的。例如，我们可以就技术进行对话，或者就政治进行对话。此外，我们可以公开讨论政治如何受到技术的影响。看似不同的讨论主题可以融合在一起，形成自己独特的话题。由于这种复杂性，文本聚类之间很少存在单一的、平滑的过渡。因此，确定理想的K值是困难的。但我们可以做出某些有用的推断：基于肘图，我们可以推断20是K参数的一个合理的估计。是的，曲线是模糊的，也许输入18或22也会行得通。然而，我们需要从某个地方开始，K值为20比K值为3或50更有意义。我们的解决方案并不完美，但它是可行的。有时，当我们处理现实世界的数据时，一个可行的解决方案是我们能期望的最好的。
- en: Note If you are uncomfortable choosing the elbow qualitatively by staring at
    the plot, consider using the external Yellowbrick library. The library contains
    a `KElbowVisualizer` class ([http://mng.bz/7lV9](http://mng.bz/7lV9)) that uses
    both Matplotlib and scikit-learn’s mini-batch K-means implementation to highlight
    the elbow location in an automated manner. If we initialize `KElbowVisualizer`
    and apply it to our data, the corresponding object returns a *K* of 23\. Additionally,
    Yellowbrick offers more powerful *K*-selection methodologies, such as the silhouette
    score (which we alluded to in section 10.) The library can be installed by running
    `pip install yellowbrick`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你不习惯通过盯着图表来定性选择肘部，请考虑使用外部 Yellowbrick 库。该库包含一个 `KElbowVisualizer` 类 ([http://mng.bz/7lV9](http://mng.bz/7lV9))，它使用
    Matplotlib 和 scikit-learn 的 mini-batch K-means 实现来以自动化的方式突出显示肘部位置。如果我们初始化 `KElbowVisualizer`
    并将其应用于我们的数据，相应的对象将返回一个 *K* 值为 23。此外，Yellowbrick 还提供了更强大的 *K*-选择方法，例如轮廓分数（我们在第
    10 节中提到过）。可以通过运行 `pip install yellowbrick` 来安装此库。
- en: We will now divide `shrunk_norm_matrix` into 20 clusters. We run the origin
    `KMeans` implementation for maximum accuracy and then store the text indices and
    cluster IDs in a Pandas table for easier analysis.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将 `shrunk_norm_matrix` 划分为 20 个聚类。我们运行原始的 `KMeans` 实现以获得最大精度，然后将文本索引和聚类
    ID 存储在 Pandas 表中，以便更容易分析。
- en: Listing 15.44 Clustering newsgroup posts into 20 clusters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.44 将新闻组帖子聚类到 20 个聚类
- en: '[PRE43]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We have clustered our texts and are ready to explore the cluster contents.
    However, first we must briefly discuss one important consequence of executing
    K-means on large matrix inputs: the resulting clusters may vary slightly across
    different computers, even if we run `np.random.seed(0)`. This divergence is driven
    by how different machines round floating-point numbers. Some computers round small
    numbers up, while others round these numbers down. Normally, these differences
    are not noticeable. Unfortunately, in a 10,000-by-100 element matrix, small differences
    can impact the clustering results. K-means is not deterministic, as we’ve discussed
    in section 10—it can converge in multiple ways to multiple sets of equally valid
    clusters. Thus, your locally run text clusters may differ from outputs in this
    book, but your observations and conclusions should be similar.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对文本进行了聚类，并准备好探索聚类内容。然而，首先我们必须简要讨论执行 K-means 在大型矩阵输入上的一个重要后果：结果聚类可能会在不同计算机上略有不同，即使我们运行
    `np.random.seed(0)`。这种差异是由不同机器处理浮点数的方式不同所驱动的。一些计算机将小数向上舍入，而其他计算机将这些数字向下舍入。通常，这些差异并不明显。不幸的是，在
    10,000 行 100 列的矩阵中，小的差异可能会影响聚类结果。K-means 不是确定性的，正如我们在第 10 节中讨论的——它可以以多种方式收敛到多组同样有效的聚类。因此，你本地运行的文本聚类可能与本书中的输出不同，但你的观察和结论应该是相似的。
- en: With this in mind, let’s proceed with the analysis. We begin by analyzing a
    single cluster. Later, we analyze all the clusters simultaneously
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们继续分析。我们首先分析单个聚类。稍后，我们将同时分析所有聚类。
- en: 15.5.1 Exploring a single text cluster
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.1 探索单个文本聚类
- en: One of our 20 clusters contains the car post at index 0 of `newsgroups.data`.
    Let’s isolate and count the number of texts that group together with that car-themed
    message.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 20 个聚类中的一个包含 `newsgroups.data` 中索引 0 的汽车帖子。让我们隔离并计算与该汽车主题信息一起组合的文本数量。
- en: Listing 15.45 Isolating the car cluster
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.45 隔离汽车聚类
- en: '[PRE44]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Warning As we’ve just discussed, the contents of the cluster may differ slightly
    on your local machine. The total cluster size may minimally diverge from 393\.
    If this happens, the subsequent sequence of code listings may produce different
    results. Regardless of these differences, you should still be able to draw similar
    conclusions from your locally generated outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：正如我们刚才讨论的，聚类的内容可能在你的本地机器上略有不同。总聚类大小可能最小程度地偏离 393。如果发生这种情况，后续的代码列表可能会产生不同的结果。不管这些差异如何，你仍然应该能够从你本地生成的输出中得出相似的结论。
- en: 393 posts cluster with the car-themed texts at index 0\. Presumably, these posts
    are also about cars. If so, then a randomly chosen post should mention an automobile.
    Let’s verify if this is the case.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引 0 处与汽车主题文本相关的帖子有 393 篇。据推测，这些帖子也是关于汽车的。如果是这样，那么随机选择的帖子应该会提到汽车。让我们验证这是否属实。
- en: Listing 15.46 Printing a random post in the car cluster
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.46 打印汽车聚类中的随机帖子
- en: '[PRE45]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Returns the post category of the newsgroup post found at index "index". We
    will reuse the function elsewhere in the section.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回在索引 "index" 找到的新闻组帖子的后类别。我们将在本节的其他地方重用此函数。
- en: The random post discusses a model of Jeep. It was posted in the *rec.autos*
    discussion group. How many of the nearly 400 posts in the cluster belong to *rec.autos*?
    Let’s find out.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 随机帖子讨论了一种吉普车模型。它是在*rec.autos*讨论组中发布的。在簇中属于*rec.autos*的近400个帖子中有多少？让我们找出答案。
- en: Listing 15.47 Checking cluster membership to *rec.autos*
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.47 检查簇成员资格到*rec.autos*
- en: '[PRE46]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In this cluster, 84% of the posts appeared in *rec.autos*. The cluster is thus
    dominated by that car discussion group. What about the remaining 16% of the clustered
    posts? Did they fall erroneously into the cluster? Or are they relevant to the
    topic of automobiles? We’ll soon find out. Let’s isolate the indices of posts
    in `df_car` that do not belong to *rec.autos*. Then we’ll choose a random index
    and print the associated post.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个簇中，84%的帖子出现在*rec.autos*中。因此，这个簇主要由那个汽车讨论组主导。那么，簇中剩余的16%帖子呢？它们是否错误地进入了簇？或者它们与汽车主题相关？我们将很快找到答案。让我们隔离`df_car`中不属于*rec.autos*的帖子的索引。然后，我们将选择一个随机索引并打印相关的帖子。
- en: Listing 15.48 Examining a post that did not appear in *rec.autos*
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.48 检查未出现在*rec.autos*中的帖子
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The random post appeared in an electronics discussion group. The post describes
    the use of radar to measure car speed. Thematically, it’s about automobiles, so
    it appears to have clustered correctly. What about the other 60 or so posts represented
    by the `not_autos_indices` list? How do we evaluate their relevance? We could
    read each post, one by one, but that is not a scalable solution. Instead, we can
    aggregate their content by displaying the top-ranking words across all posts.
    We rank each word by summing its TFIDF across each index in `not_autos_indices`.
    Then we’ll sort the words based on their aggregated TFIDF. Printing out the top
    10 words will help us determine if our content is relevant to cars.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 随机帖子出现在一个电子讨论组中。该帖子描述了使用雷达测量汽车速度。从主题上看，它关于汽车，因此看起来已经正确聚类。那么，`not_autos_indices`列表中代表的其他60多篇帖子呢？我们如何评估它们的相关性？我们可以逐篇阅读帖子，但这不是一个可扩展的解决方案。相反，我们可以通过显示所有帖子中的顶级单词来聚合它们的内容。我们通过在`not_autos_indices`中的每个索引处求和每个单词的TFIDF来对每个单词进行排名。然后，我们将单词根据它们的聚合TFIDF进行排序。打印出前10个单词将帮助我们确定我们的内容是否与汽车相关。
- en: Next, we define a `rank_words_by_tfidf` function. The function takes as input
    a list of indices and ranks the words across these indices using the approach
    described previously. The ranked words are stored in a Pandas table for easier
    display. The summed TFIDF values used to rank the words are also stored in that
    table. Once our function is defined, we will run `rank_words_by_tfidf(not_autos_indices)`
    and output the top 10 ranked results.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`rank_words_by_tfidf`函数。该函数接受一个索引列表作为输入，并使用之前描述的方法在这些索引之间对单词进行排名。排名的单词存储在一个Pandas表中，以便更容易显示。用于排名单词的求和TFIDF值也存储在该表中。一旦我们的函数定义完成，我们将运行`rank_words_by_tfidf(not_autos_indices)`并输出前10个排名结果。
- en: Note Given an `indices` array, we want to aggregate the rows of `tfidf_ np_matrix[indices]`.
    As discussed earlier, we can sum over rows by running `tfidf_np_matrix[indices].sum(axis=0)`.
    Additionally, we can generate that sum by running `tfidf_matrix[indices].sum(axis=0)`,
    where `tfidf_ matrix` is a SciPy CSR object. Summing over the rows of a sparse
    CSR matrix is computationally much faster, but that summation returns a 1-by-*n*
    shaped matrix that is not a NumPy object. We need to convert the output to a NumPy
    array by running `np.asarray(tfidf_matrix[indices].sum(axis=0))[0]`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：给定一个`indices`数组，我们希望聚合`tfidf_ np_matrix[indices]`的行。如前所述，我们可以通过运行`tfidf_np_matrix[indices].sum(axis=0)`来对行求和。此外，我们还可以通过运行`tfidf_matrix[indices].sum(axis=0)`来生成这个和，其中`tfidf_
    matrix`是一个SciPy CSR对象。对稀疏CSR矩阵的行求和在计算上要快得多，但这个求和返回的是一个1-by-*n*形状的矩阵，它不是一个NumPy对象。我们需要通过运行`np.asarray(tfidf_matrix[indices].sum(axis=0))[0]`将输出转换为NumPy数组。
- en: Listing 15.49 Ranking the top 10 words with TFIDF
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.49 使用TFIDF对顶级10个单词进行排名
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ This summation is equivalent to running tfidf_np_matrix[indices].sum(axis=0).
    The simpler NumPy array aggregation takes approximately 1 second to compute. A
    single second may not seem like much, but once we repeat the computation over
    20 clusters, the running time will add up to 20 seconds. The summation over the
    rows of the sparse matrix is noticeably faster.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个求和等同于运行`tfidf_np_matrix[indices].sum(axis=0)`。简单的NumPy数组聚合大约需要1秒钟来计算。一秒钟可能看起来不多，但一旦我们在20个簇上重复计算，运行时间将累计到20秒。对稀疏矩阵的行求和要快得多。
- en: The first two top-ranking words are *car* and *cars*.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个顶级单词是*car*和*cars*。
- en: Note The word *cars* is the plural of *car*. We can aggregate these words together
    based on the *s* at the end of *cars*. This process of reducing a plural to its
    root word is called *stemming*. The external Natural Language Toolkit library
    ([https://www.nltk.org](https://www.nltk.org)) provides useful functions for efficient
    stemming.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：单词*cars*是*car*的复数形式。我们可以根据*cars*结尾的*s*将这些单词聚合在一起。将复数形式还原为其基本单词的过程称为*词干提取*。外部自然语言处理工具包库([https://www.nltk.org](https://www.nltk.org))提供了用于高效词干提取的有用函数。
- en: Elsewhere on the ranked list, we see mentions of *radar*, *odometer*, and *speed*.
    Some of these terms also appeared in our randomly chosen *sci.electronics* post.
    The use of radar technology to measure car speed appears to be a common theme
    in the texts represented by `not_autos_indices`. How do these speed-themed keywords
    compare with the rest of the posts in the car cluster? We can check by inputting
    `df_car.Index.values` into `rank_words_by_tfidf`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在排名列表的其他地方，我们看到提到了*rada*、*里程表*和*速度*。其中一些术语也出现在我们随机选择的*sci.electronics*帖子中。使用雷达技术来测量汽车速度似乎是`not_autos_indices`所代表的文本中的常见主题。这些与速度主题相关的关键词与其他汽车簇中的帖子相比如何？我们可以通过将`df_car.Index.values`输入到`rank_words_by_tfidf`中来检查。
- en: Listing 15.50 Ranking the top 10 words in the car cluster
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.50 按照汽车簇中的前10个单词进行排名
- en: '[PRE49]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Generally, the posts in the `df_car` cluster focus on car engines and car dealers.
    However, a minority of the posts discuss radar measurements of car speed. These
    radar posts are more likely to appear in the *sci.electronics* newsgroup. Nonetheless,
    these posts legitimately discuss cars (as opposed to discussing politics, software,
    or medicine). Thus, our `df_car` cluster appears to be genuine. By examining the
    top keywords, we were able to validate the cluster without having to read each
    clustered post manually.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，`df_car`簇中的帖子主要关注汽车引擎和汽车经销商。然而，少数帖子讨论了汽车速度的雷达测量。这些雷达帖子更有可能出现在*sci.electronics*新闻组中。尽管如此，这些帖子确实讨论了汽车（而不是讨论政治、软件或医学）。因此，我们的`df_car`簇看起来是真实的。通过检查顶级关键词，我们能够验证簇，而无需手动阅读每个簇帖子。
- en: In this same manner, we can utilize `rank_words_by_tfidf` to get the top keywords
    for each of the 20 clusters. The keywords will allow us to understand the topic
    of each cluster. Unfortunately, printing 20 different word tables isn’t very visually
    efficient—the printed tables will take up too much space, adding redundant pages
    to this book. Alternatively, we can visualize these cluster keywords as images
    in a single coherent plot. Let’s learn how to visualize the contents of multiple
    text clusters.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，我们可以使用`rank_words_by_tfidf`来获取20个簇中的每个簇的关键词。这些关键词将帮助我们理解每个簇的主题。不幸的是，打印20个不同的单词表在视觉上并不高效——打印的表格会占用太多空间，给这本书增加多余的页面。作为替代，我们可以将这些簇关键词作为单个连贯图中的图像来可视化。让我们学习如何可视化多个文本簇的内容。
- en: 15.6 Visualizing text clusters
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.6 可视化文本簇
- en: 'Our aim is to visualize ranked keywords across multiple text clusters. First
    we need to solve a simpler problem: how do we visualize the important keywords
    in a single cluster? One approach is just to print the keywords in their order
    of importance. Unfortunately, this sorting lacks any sense of relative significance.
    For instance, in our `df_ranked_words` table, the word *cars* is immediately followed
    by *engine*. However, *cars* has a summed TFIDF score of 17.8, while *engine*
    has a score of 10.9\. Thus, *cars* is approximately 1.6 times more significant
    than *engine*, relative to the car cluster. How do we incorporate relative significance
    into our visualization? Well, we could signify importance using font size: we
    could display *cars* with a font size of 17.8 and *engine* with a font size of
    10.9\. In the display, *cars* would be 1.6 times bigger and thus 1.6 times more
    important. Of course, a font size of 10.9 may be too small to comfortably read.
    We can make the font size bigger by doubling the summed TFIDF significance scores.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在多个文本簇中可视化排名关键词。首先，我们需要解决一个更简单的问题：我们如何可视化单个簇中的重要关键词？一种方法就是按照重要性的顺序打印关键词。不幸的是，这种排序缺乏相对重要性的感觉。例如，在我们的`df_ranked_words`表中，单词*cars*紧随*engine*之后。然而，*cars*的总TFIDF得分为17.8，而*engine*的得分为10.9。因此，*cars*相对于汽车簇的重要性大约是*engine*的1.6倍。我们如何将相对重要性纳入我们的可视化中？嗯，我们可以使用字体大小来表示重要性：我们可以用17.8的字体大小显示*cars*，用10.9的字体大小显示*engine*。在显示中，*cars*将大1.6倍，因此重要1.6倍。当然，10.9的字体大小可能太小，难以舒适阅读。我们可以通过将总TFIDF重要性得分加倍来增加字体大小。
- en: Python does not let us modify font size directly during printing. However, we
    can modify font size using Matplotlib’s `plt.text` function. Running `plt.text(x,
    y, word, fontsize=z)` displays a word at coordinates `(x, y)` and sets the font
    size to equal `z`. The function allows us to visualize the words in a 2D grid
    where word size is proportional to significance. This type of visualization is
    called a *word cloud*. Let’s utilize `plt.text` to generate a word cloud of the
    top words in `df_ranked_words`. We plot the word cloud as a five-word by five-word
    grid (figure 15.2). Each word’s font size equals double its significance score.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Python不允许我们在打印时直接修改字体大小。然而，我们可以使用Matplotlib的`plt.text`函数来修改字体大小。运行`plt.text(x,
    y, word, fontsize=z)`将在坐标`(x, y)`处显示一个词，并将字体大小设置为等于`z`。该函数允许我们在一个二维网格中可视化词，其中词的大小与重要性成比例。这种可视化类型称为*词云*。让我们利用`plt.text`生成`df_ranked_words`中顶级词的词云。我们将词云绘制为一个五词乘五词的网格（图15.2）。每个词的字体大小等于其重要性得分的两倍。
- en: Listing 15.51 Plotting a word cloud with Matplotlib
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.51 使用Matplotlib绘制词云
- en: '[PRE50]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](../Images/15-02.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15-02.png)'
- en: Figure 15.2 A word cloud generated using Matplotlib. The word cloud is a mess
    because of word overlap.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 使用Matplotlib生成的词云。由于词重叠，词云显得混乱。
- en: Our visualization is a mess! Large words like *car* take up too much space.
    They overlap with other words, making the image indecipherable. We need to plot
    our words much more intelligently. No two words should ever overlap. Eliminating
    the overlap of 2D plotted words is not a trivial task. Fortunately, the hard work
    has been done for us by the creators of the external Wordcloud library. The library
    is able to generate word clouds in a manner that’s visually appealing. We now
    install Wordcloud and then import and initialize the library’s `WordCloud` class.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可视化一团糟！像*car*这样的大词占据了太多的空间。它们与其他词重叠，使得图像难以辨认。我们需要更智能地绘制我们的词。两个词不应该有任何重叠。消除二维绘制的词的重叠不是一个简单任务。幸运的是，外部Wordcloud库的创建者已经为我们完成了这项艰苦的工作。该库能够以视觉上吸引人的方式生成词云。我们现在安装Wordcloud，然后导入并初始化库的`WordCloud`类。
- en: Note Call `pip install wordcloud` from the command line terminal to install
    the Wordcloud library.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在命令行终端中输入`pip install wordcloud`来安装Wordcloud库。
- en: Listing 15.52 Initializing the `WordCloud` class
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.52 初始化`WordCloud`类
- en: '[PRE51]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ❶ The positions of the words in the word cloud are generated randomly. To maintain
    output consistency, we must pass the random seed directly using the random_state
    parameter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 词云中词的位置是随机生成的。为了保持输出一致性，我们必须直接通过random_state参数传递随机种子。
- en: Running `WordCloud()` returns a `cloud_generator` object. We’ll use the object’s
    `fit_ words` method to generate a word cloud. Running `cloud_generator.fit_words(words_
    to_score)` will create an image from `words_to_score`, which is a dictionary mapping
    of words to their significance scores.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`WordCloud()`返回一个`cloud_generator`对象。我们将使用该对象的`fit_words`方法来生成词云。运行`cloud_generator.fit_words(words_to_score)`将从`words_to_score`创建一个图像，其中`words_to_score`是一个将词映射到其重要性得分的字典。
- en: Note Please note that running `cloud_generator.generate_from_frequencies(word_to_score)`
    will achieve the same results.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请注意，运行`cloud_generator.generate_from_frequencies(word_to_score)`将得到相同的结果。
- en: Let’s create an image from the most significant words in `df_ranked_words`.
    We’ll store that image in a `wordcloud_image` variable, but we won’t plot the
    image just yet.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个由`df_ranked_words`中最重要词组成的图像。我们将该图像存储在`wordcloud_image`变量中，但暂时不会绘制该图像。
- en: Listing 15.53 Generating a word cloud image
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.53 生成词云图像
- en: '[PRE52]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now we’re ready to visualize `wordcloud_image`. Matplotlib’s `plt.imshow` function
    is able to plot images based on a variety of inputted image formats. Running `plt.imshow
    (wordcloud_image)` will display our generated word cloud (figure 15.3).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备可视化`wordcloud_image`。Matplotlib的`plt.imshow`函数能够根据输入的各种图像格式绘制图像。运行`plt.imshow(wordcloud_image)`将显示我们生成的词云（图15.3）。
- en: '![](../Images/15-03.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15-03.png)'
- en: Figure 15.3 A word cloud generated using the `WordCloud` class. The words no
    longer overlap. However, the background is too dark. Also, some letters appear
    rough around the edges.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 使用`WordCloud`类生成的词云。词不再重叠。然而，背景太暗。此外，一些字母的边缘看起来很粗糙。
- en: Note There are multiple ways of representing images in Python. One approach
    is to store an image as a 2D NumPy array. Alternatively, we can store the image
    using a special class from the Python Imaging Library (PIL). The `plt.imshow`
    function can display images stored as NumPy objects or as PIL `Image` objects.
    It can also display custom image objects that include a `to_image` method, but
    that method’s output must return a NumPy array or a PIL `Image` object.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在Python中表示图像有多种方式。一种方法是将图像存储为2D NumPy数组。或者，我们可以使用Python Imaging Library（PIL）中的特殊类来存储图像。`plt.imshow`函数可以显示存储为NumPy对象或PIL
    `Image`对象的图像。它还可以显示包含`to_image`方法的自定义图像对象，但该方法输出必须返回一个NumPy数组或PIL `Image`对象。
- en: Listing 15.54 Plotting an image using `plt.imshow`
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.54 使用`plt.imshow`绘制图像
- en: '[PRE53]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We’ve visualized the word cloud. Our visualization is not ideal: the dark background
    makes it hard to read the words. We can change the background from black to white
    by running `WordCloud(background_color=''white'')` during initialization. Also,
    the edges of the individual letters are pixelated and blocky: we can smooth all
    the edges in our image plot by passing `interpolation="bilinear"` into `plt.imshow`.
    Let’s regenerate the word cloud with a lighter background while also smoothing
    out the visualized letters (figure 15.4).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可视化了词云。我们的可视化并不理想：深色背景使得阅读单词变得困难。我们可以在初始化时运行`WordCloud(background_color='white')`将背景从黑色改为白色。此外，单个字母的边缘是像素化和块状的：我们可以通过将`interpolation="bilinear"`传递给`plt.imshow`来平滑我们图像图中的所有边缘。让我们用较浅的背景重新生成词云，同时平滑可视化的字母（图15.4）。
- en: '![](../Images/15-04.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/15-04.png)'
- en: Figure 15.4 A word cloud generated using the `WordCloud` class. The background
    is set to white, for better visibility, and the letters are smoothed out at the
    edges.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 使用`WordCloud`类生成的词云。背景设置为白色，以便更好地可见，边缘的字母被平滑处理。
- en: Listing 15.55 Improving the word cloud image quality
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.55 提高词云图像质量
- en: '[PRE54]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The top words in the car cluster have been successfully visualized. The words
    *car* and *cars* clearly dominate over lesser terms, such as *engine* and *dealer*.
    We can interpret the contents of the cluster merely by glancing at the word cloud.
    Of course, we have already examined the car cluster in great detail, and we aren’t
    gleaming anything new from this visualization. Let’s instead apply word cloud
    visualization to a randomly chosen cluster (figure 15.5). The word cloud will
    display the cluster’s 15 most significant words, and we’ll use the display to
    figure out the main topic of the cluster.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车簇中的顶级单词已经成功可视化。单词*car*和*cars*明显优于其他术语，如*engine*和*dealer*。我们可以仅通过查看词云来解释簇的内容。当然，我们已经在很大程度上检查了汽车簇，并且从这个可视化中没有发现任何新的东西。让我们将词云可视化应用于随机选择的簇（图15.5）。词云将显示簇的15个最重要的单词，我们将使用这个显示来确定簇的主要主题。
- en: '![](../Images/15-05.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/15-05.png)'
- en: Figure 15.5 A random cluster’s word cloud. The topic of the cluster appears
    to be technology and computer hardware.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 随机簇的词云。该簇的主题似乎是技术和计算机硬件。
- en: Note The word colors in the word cloud are generated at random, and some of
    the random colors would render poorly in the black-and-white version of this book.
    For this reason, we specifically limit the color selection to a small subset of
    colors using the `color_func` parameter in the `WordCloud` class.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：词云中的单词颜色是随机生成的，并且一些随机颜色在本书的黑白版本中渲染效果不佳。因此，我们特别限制颜色选择为`WordCloud`类中的`color_func`参数的小颜色子集。
- en: Listing 15.56 Plotting a word cloud for a random cluster
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.56 绘制随机簇的词云
- en: '[PRE55]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: ❶ Takes as input a df_cluster table and returns a word cloud image for the top
    max_words words corresponding to the cluster. The previously defined rank_words_by_tfidf
    function is used to rank the words in the cluster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入df_cluster表，并返回对应于簇的前max_words个单词的词云图像。使用先前定义的rank_words_by_tfidf函数对簇中的单词进行排序。
- en: ❷ The WordCloud class includes an optional color_func parameter. The parameter
    expects a color-selection function that assigns a color to each word. Here, we
    define a custom function to control the color settings.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ WordCloud类包含一个可选的color_func参数。该参数期望一个颜色选择函数，为每个单词分配一个颜色。在这里，我们定义一个自定义函数来控制颜色设置。
- en: ❸ Helper function to randomly assign one of five acceptable colors to each word
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为每个单词随机分配五种可接受颜色之一的一个辅助函数
- en: Our randomly chosen cluster includes top words such as *monitor*, *video*, *memory*,
    *card*, *motherboard*, *bit*, and *ram*. The cluster seems to focus on technology
    and computer hardware. We can verify by printing the most common newsgroup category
    in the cluster.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机选择的簇包括诸如 *monitor*（显示器）、*video*（视频）、*memory*（内存）、*card*（卡）、*motherboard*（主板）、*bit*（位）和
    *ram*（RAM）等顶级单词。这个簇似乎专注于技术和计算机硬件。我们可以通过打印簇中最常见的新闻组类别来验证这一点。
- en: Note By observing the words *card*, *video*, and *memory*, we can infer that
    *card* refers to either *video card* or a *memory card*. In NLP, such sequences
    of two consecutive words are called *bigrams*. Generally, a sequence of *n* consecutive
    words is called an *n-gram*. `TfidfVectorizer` is able to vectorize across n-grams
    of arbitrary length. We simply need to pass in an `ngram_range` parameter during
    initialization. Running `TfidfVectorizer(ngram_range(1, 3))` creates a vectorizer
    that tracks all 1-grams (single words), 2-grams (such as *video card*), and 3-grams
    (such as *natural language processing*). Of course, these n-grams cause the vocabulary
    size to rise into the millions. However, we can limit the vocabulary to the top
    100,000 n-grams by passing `max_features=100000` into the vectorizer’s initialization
    method.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通过观察单词 *card*、*video* 和 *memory*，我们可以推断 *card* 指的是 *video card*（显卡）或 *memory
    card*（内存卡）。在自然语言处理（NLP）中，这样的两个连续单词序列被称为 *bigrams*。通常，*n* 个连续单词的序列被称为 *n-gram*。`TfidfVectorizer`
    能够对任意长度的 n-gram 进行向量化。我们只需在初始化时传入一个 `ngram_range` 参数。运行 `TfidfVectorizer(ngram_range(1,
    3))` 创建一个向量器，它跟踪所有 1-gram（单个单词）、2-gram（如 *video card*）和 3-gram（如 *natural language
    processing*）。当然，这些 n-gram 会导致词汇量上升到数百万。然而，我们可以通过将 `max_features=100000` 传递到向量器的初始化方法中，将词汇量限制在最高的
    100,000 个 n-gram。
- en: Listing 15.57 Checking the most common cluster category
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.57 检查最常见的簇类别
- en: '[PRE56]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Many of the posts in the cluster appeared in the *comp.sys.ibm.pc.hardware*
    newsgroup. We’ve thus successfully identified the cluster’s topic of hardware.
    We did this simply by looking at the word cloud.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 该簇中的许多帖子出现在 *comp.sys.ibm.pc.hardware* 新闻组中。因此，我们已经成功识别出簇的主题是硬件。我们只是通过查看词云做到了这一点。
- en: So far, we’ve generated two separate word clouds for two distinct clusters.
    However, our end goal is to display multiple word clouds simultaneously. We’ll
    now visualize all word clouds in a single figure using a Matplotlib concept called
    a *subplot*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经为两个不同的簇生成了两个独立的词云。然而，我们的最终目标是同时显示多个词云。现在，我们将使用 Matplotlib 的一个概念，即
    *subplot*，在一个单独的图中可视化所有词云。
- en: Common methods for visualizing words
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的可视化单词方法
- en: '`plt.text(word, x, y, fontsize=z)`—Plots a word positioned at coordinates `(x,
    y)` with a font size of `z`.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plt.text(word, x, y, fontsize=z)`—在坐标 `(x, y)` 处绘制一个字体大小为 `z` 的单词。'
- en: '`cloud_generator = WordCloud()`—Initializes an object that can generate a word
    cloud. The background of that word cloud is black.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud_generator = WordCloud()`—初始化一个可以生成词云的对象。该词云的背景是黑色。'
- en: '`cloud_generator = WordCloud(background_color=''white'')`—Initializes an object
    that can generate a word cloud. The background of that word cloud is white.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud_generator = WordCloud(background_color=''white'')`—初始化一个可以生成词云的对象。该词云的背景是白色。'
- en: '`wordcloud_image = cloud_generator.fit_words(words_to_score)`—Generates a word
    cloud image from the `words_to_score` dictionary, which maps words to their significance
    scores. The size of every word in `wordcloud_image` is computed relative to its
    significance.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wordcloud_image = cloud_generator.fit_words(words_to_score)`—从 `words_to_score`
    字典生成词云图像，该字典将单词映射到其重要性分数。`wordcloud_image` 中每个单词的大小都是相对于其重要性计算的。'
- en: '`plt.imshow(wordcloud_image)`—Plots the computed `wordcloud_image`.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plt.imshow(wordcloud_image)`—绘制计算出的 `wordcloud_image`。'
- en: '`plt.imshow(wordcloud_image, interpolation="bilinear")`—Plots the computed
    `wordcloud_image` while smoothing out the visualized letters.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plt.imshow(wordcloud_image, interpolation="bilinear")`—绘制计算出的 `wordcloud_image`，同时平滑可视化字母。'
- en: 15.6.1 Using subplots to display multiple word clouds
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.6.1 使用子图显示多个词云
- en: 'Matplotlib allows us to include multiple plots in a single figure. Each distinct
    plot is called a *subplot*. Subplots can be organized in any number of ways, but
    they’re most commonly arranged in a grid-like pattern. We can create a subplot
    grid containing `r` rows and `c` columns by running `plt.subplots(r, c)`. The
    `plt.subplots` function generates the grid while also returning a tuple: `(figure,
    axes)`. The `figure` variable is a special class that tracks the main figure,
    which encompasses the grid. Meanwhile, the `axes` variable is a 2D list containing
    `r` rows and `c` columns. Each element of `axes` is a Matplotlib `AxesSubplot`
    object. Every subplot object can be used to output a unique visualization: running
    `axes[i][j].plot(x, y)` plots `x` versus `y` in the subplot positioned in the
    *i*th row and *j*th column of the grid.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib 允许我们在单个图中包含多个图表。每个独立的图表称为 *子图*。子图可以以任何数量的方式组织，但它们最常见的是以网格状排列。我们可以通过运行
    `plt.subplots(r, c)` 创建一个包含 `r` 行和 `c` 列的子图网格。`plt.subplots` 函数生成网格的同时返回一个元组：`(figure,
    axes)`。`figure` 变量是一个特殊类，它跟踪主图，该图包含网格。同时，`axes` 变量是一个包含 `r` 行和 `c` 列的 2D 列表。`axes`
    的每个元素都是一个 Matplotlib `AxesSubplot` 对象。每个子图对象都可以用来输出独特的可视化：运行 `axes[i][j].plot(x,
    y)` 在网格的第 *i* 行和 *j* 列的子图中绘制 `x` 与 `y`。
- en: Warning Running `subplots(1, z)` or `subplots(z, 1)` returns a 1D `axes` list
    where `len(axes) == z` rather than a 2D grid.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 运行 `subplots(1, z)` 或 `subplots(z, 1)` 返回一个 1D `axes` 列表，其中 `len(axes) ==
    z` 而不是 2D 网格。
- en: Let’s demonstrate the use of `plt.subplots`. We generate a two-by-two grid of
    subplots by running `plt.subplots(2, 2)`. Then we iterate over each row `r` and
    column `c` in the grid. For every unique subplot positioned at `(r, c`), we plot
    a quadratic curve in which `y` = `r * x*x + c * x`. By linking curve parameters
    to the grid position, we generate four distinct curves, all of which appear in
    the bounds of a single figure (figure 15.6).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们演示 `plt.subplots` 的使用。我们通过运行 `plt.subplots(2, 2)` 生成一个 2x2 的子图网格。然后我们遍历网格中的每一行
    `r` 和每一列 `c`。对于网格中位于 `(r, c)` 的每个唯一的子图，我们绘制一个二次曲线，其中 `y` = `r * x*x + c * x`。通过将曲线参数与网格位置链接，我们生成四个不同的曲线，所有这些曲线都出现在单个图的范围之内（图
    15.6）。
- en: Listing 15.58 Generating four subplots using Matplotlib
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.58 使用 Matplotlib 生成四个子图
- en: '[PRE57]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](../Images/15-06.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15-06.png)'
- en: Figure 15.6 Four different curves plotted across four subplots in a single figure
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6 在单个图中绘制四个子图中的四个不同曲线
- en: 'Four different curves appear in the subplots of our grid. We can replace any
    of these curves with a word cloud. Let’s visualize `wordcloud_image` in the lower-left
    quadrant of the grid by running `axes[1][0].imshow(wordcloud_image)` (figure 15.7).
    Let’s also assign a title to that subplot: the title equals `top_category`, which
    is *comp.sys.ibm.pc .hardware*. We set the subplot title by running `axes[r][c].set_title(top_category)`.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网格子图中出现了四个不同的曲线。我们可以用词云替换这些曲线中的任何一个。让我们通过运行 `axes[1][0].imshow(wordcloud_image)`
    在网格的左下角可视化 `wordcloud_image`（图 15.7）。同时，我们也给这个子图添加一个标题：标题等于 `top_category`，即 *comp.sys.ibm.pc
    .hardware*。我们通过运行 `axes[r][c].set_title(top_category)` 来设置子图标题。
- en: Listing 15.59 Plotting a word cloud within a subplot
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.59 在子图中绘制词云
- en: '[PRE58]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![](../Images/15-07.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15-07.png)'
- en: Figure 15.7 Three curves and a word cloud plotted in four subplots. There are
    issues with word cloud readability due to formatting problems and figure size.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7 在四个子图中绘制了三条曲线和一个词云。由于格式问题和图大小问题，词云的可读性存在问题。
- en: We’ve visualized a word cloud in the subplot grid, but there are some issues
    with the visualization. The words in the cloud are hard to read because the subplot
    is so small. We need to make the subplot bigger, which requires us to alter the
    figure size. We can do so using the `figsize` parameter. Passing `figsize=(width,
    height)` into `plt.subplots` creates a figure that is `width` inches wide and
    `height` inches high. Each subplot in the figure is also adjusted to fit the updated
    size.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在子图网格中可视化了一个词云，但可视化存在一些问题。由于子图太小，云中的单词难以阅读。我们需要使子图更大，这需要我们改变图的大小。我们可以使用 `figsize`
    参数来做到这一点。将 `figsize=(width, height)` 传递给 `plt.subplots` 创建一个宽度为 `width` 英寸、高度为
    `height` 英寸的图。图中的每个子图也相应调整以适应更新的大小。
- en: In addition, we can make other minor changes to improve the plot. Reducing the
    visualized word count from 15 to 10 will make the smaller word cloud easier to
    read. We should also remove the axis tick marks from the plot—they take up too
    much space and provide no useful information. We can delete the x-axis and y-axis
    tick marks from `axis[r][c]` by calling `axis[r][c].set_xticks([])` and `axis[r][c].set_yticks([])`,
    respectively. With this in mind, let’s generate a figure that’s 20 inches wide
    and 15 inches high (listing 15.60).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以进行其他一些小的修改来改善图表。将可视化的词数从15减少到10将使较小的词云更容易阅读。我们还应该从图表中移除坐标轴刻度标记——它们占用太多空间且不提供任何有用的信息。我们可以通过调用
    `axis[r][c].set_xticks([])` 和 `axis[r][c].set_yticks([])` 分别删除 `axis[r][c]` 的x轴和y轴刻度标记。考虑到这一点，让我们生成一个宽度为20英寸、高度为15英寸的图表（列表15.60）。
- en: Note The actual dimensions of the figure in the book are not 20 by 15 inches,
    due to image formatting.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于图像格式化，书中图表的实际尺寸不是20英寸乘以15英寸。
- en: The large figure has 20 subplots aligned in a five-by-four grid. Each subplot
    contains a word cloud corresponding to one of our clusters, and every subplot
    title is set to the dominant newsgroup category in a cluster. We also include
    the cluster index in each title for later reference. Finally, we remove the axis
    tick marks from all plots. The final visualization gives us a bird’s-eye-view
    of all the dominant word patterns across all 20 clusters (figure 15.8).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 大图包含20个子图，排列在5行4列的网格中。每个子图包含一个与我们的一个聚类相对应的词云，并且每个子图的标题都设置为聚类中的主导新闻组类别。我们还将在每个标题中包含聚类索引，以便于后续参考。最后，我们从所有图表中移除了坐标轴刻度标记。最终的可视化让我们能够从鸟瞰的角度看到所有20个聚类中的主导词模式（图15.8）。
- en: '![](../Images/15-08.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15-08.png)'
- en: Figure 15.8 20 word clouds visualized across 20 subplots. Each word cloud corresponds
    to one of 20 clusters. The title of each subplot equals the top newsgroup category
    in each cluster. In most of the word clouds, the title corresponds with the displayed
    word content; but certain word clouds (such as those of clusters 1 and 7) do not
    offer informative displays.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8 在20个子图中可视化的20个词云。每个词云对应于20个聚类中的一个。每个子图的标题等于每个聚类中的顶级新闻组类别。在大多数词云中，标题与显示的词内容相对应；但某些词云（如聚类1和7的词云）不提供信息性的显示。
- en: Common subplot methods
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的子图方法
- en: '`figure, axes = plt.subplots(x, y)`—Creates a figure containing an `x`-by-`y`
    grid of subplots. If `x > 1` and `y > 1`, then `axes[r][c]` corresponds to the
    subplot in row `r` and column `c` of the subplot grid.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`figure, axes = plt.subplots(x, y)`—创建一个包含 `x` 行 `y` 列子图的图表。如果 `x > 1` 且 `y
    > 1`，则 `axes[r][c]` 对应于子图网格中行 `r` 和列 `c` 的子图。'
- en: '`figure, axes = plt.subplots(x, y, figsize=(width, height))`—Creates a figure
    containing an `x`-by-`y` grid of subplots. The figure is `width` inches wide and
    `height` inches high.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`figure, axes = plt.subplots(x, y, figsize=(width, height))`—创建一个包含 `x` 行 `y`
    列子图的图表。该图表宽度为 `width` 英寸，高度为 `height` 英寸。'
- en: '`axes[r][c].plot(x_values, y_values)`—Plots data in the subplot positioned
    at row `r` and column `c`.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axes[r][c].plot(x_values, y_values)`—在位于行 `r` 和列 `c` 的子图中绘制数据。'
- en: '`axes[r][c].set_title(title)`—Adds a title to the subplot positioned at row
    `r` and column `c`.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axes[r][c].set_title(title)`—为位于行 `r` 和列 `c` 的子图添加标题。'
- en: Listing 15.60 Visualizing all clusters using 20 subplots
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.60 使用20个子图可视化所有聚类
- en: '[PRE59]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: ❶ Generates a subplot title by combining the cluster ID with the most common
    newsgroup category in a cluster
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过将聚类ID与聚类中最常见的新闻组类别组合来生成子图标题
- en: ❷ Increases the title font to 20 for better readability
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将标题字体增加到20以获得更好的可读性
- en: 'We’ve visualized the top words across all 20 clusters. For the most part, the
    visualized words make sense! The main topic of cluster 0 is cryptography: its
    top words include *encryption*, *secure*, *keys*, and *nsa*. The main topic of
    cluster 2 is space: its top words include *space*, *nasa*, *shuttle, moon*, and
    *orbit*. Cluster 4 is centered on shopping, with top words like *sale*, *offer*,
    *shipping*, and *condition*. Clusters 9 and 18 are sports clusters: their main
    topics are *baseball* and *hockey*, respectively. Posts in cluster 9 frequently
    mention *games*, *runs*, *baseball*, *pitching*, and *team*. Posts in cluster
    18 frequently mention *game*, *team*, *players*, *hockey*, and *nhl*. A majority
    of the clusters are easy to interpret, based on their word clouds. Arguably, 75%
    of the clusters contain top words corresponding with their dominant category titles.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可视化了所有20个集群中的顶级单词。大部分可视化的单词是有意义的！集群0的主要主题是密码学：其顶级单词包括*encryption*、*secure*、*keys*和*nsa*。集群2的主要主题是太空：其顶级单词包括*space*、*nasa*、*shuttle*、*moon*和*orbit*。集群4以购物为中心，顶级单词有*sale*、*offer*、*shipping*和*condition*。集群9和18是体育集群：它们的主要主题分别是*baseball*和*hockey*。集群9中的帖子经常提到*games*、*runs*、*baseball*、*pitching*和*team*。集群18中的帖子经常提到*game*、*team*、*players*、*hockey*和*nhl*。大多数集群根据其词云很容易解释，可以说75%的集群包含与它们主导类别标题对应的顶级单词。
- en: 'Of course, there are issues with our output. Several of the word clouds do
    not make sense: for instance, cluster 1 has a subplot title of `sci.electronics`,
    yet its word cloud is composed of general words like *just*, *like*, *does*, and
    *know*. Meanwhile, cluster 7 has a subplot title of `sci.med`, yet its word cloud
    is composed of words like *pitt*, *msg*, and *gordon*. Unfortunately, word cloud
    visualization isn’t always perfect. Sometimes the underlying clusters are malformed,
    or the dominant language in the clusters is biased toward unexpected text patterns.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的输出存在一些问题。有几个词云没有意义：例如，集群1的子图标题为`sci.electronics`，但其词云由像*just*、*like*、*does*和*kow*这样的通用词组成。同时，集群7的子图标题为`sci.med`，但其词云由像*pitt*、*msg*和*gordon*这样的词组成。不幸的是，词云可视化并不总是完美的。有时底层集群是变形的，或者集群中的主导语言偏向于意外的文本模式。
- en: Note Reading some sampled posts in the clusters can help reveal these biases.
    For instance, many of the electronics questions in cluster 1 include the question,
    “does anyone know?” Also, many of the posts in cluster 7 were written by a student
    named Gordon, who studied at the University of Pittsburgh (*pitt*).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 注意阅读集群中的一些样本帖子可以帮助揭示这些偏差。例如，集群1中的许多电子问题都包括问题，“does anyone know?”。同样，集群7中的许多帖子是由一个名叫Gordon的学生撰写的，他在匹兹堡大学(*pitt*)学习。
- en: Fortunately, there are steps we can take to salvage the indecipherable word
    clouds. For instance, we can filter out obviously useless words and then regenerate
    the cloud. Or we can simply disregard the top *x* words in the cluster and visualize
    the cloud using the next top-ranking words. Let’s remove the top 10 words from
    cluster 7 and recompute its word cloud (figure 15.9).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以采取一些步骤来挽救那些难以辨认的词云。例如，我们可以过滤掉明显无用的单词，然后重新生成云。或者我们可以简单地忽略集群中排名前*x*的单词，并使用下一个排名靠前的单词来可视化云。让我们从集群7中移除前10个单词，并重新计算其词云（图15.9）。
- en: '![](../Images/15-09.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15-09.png)'
- en: Figure 15.9 Cluster 7’s word cloud, recomputed after filtering. It is now clear
    that medicine is the main topic of the cluster.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9 过滤后重新计算的集群7的词云。现在很明显，医学是集群的主要主题。
- en: Listing 15.61 Recomputing a word cloud after filtering
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.61 过滤后重新计算词云
- en: '[PRE60]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: ❶ We visualize the top 15 words since we’re not spatially restricted by a subplot.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们可视化前15个单词，因为我们不受子图的空间限制。
- en: ❷ Note that plt lacks a subplot’s set_title, set_xticks, and set_yticks methods.
    Instead, we must call plt.title, plt.xticks, and plt.yticks to achieve the same
    results.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 注意，plt缺少subplot的set_title、set_xticks和set_yticks方法。相反，我们必须调用plt.title、plt.xticks和plt.yticks来实现相同的结果。
- en: 'Cluster 7 is dominated by words like *disease*, *medical*, *doctor*, *food*,
    *pain*, and *patients*. Its medicinal topic is now clear; by disregarding the
    useless keywords, we’ve managed to elucidate the cluster’s true contents. Of course,
    this simple approach will not always work. NLP is messy, and there is no silver
    bullet that will slay all of our problems. Nonetheless, there’s still much we
    can accomplish, despite the unstructured nature of complex texts. Consider what
    we have achieved: we’ve taken 10,000 diverse real-world texts and clustered them
    into multiple meaningful topics. Furthermore, we’ve visualized these topics in
    a single image, and most of the topics in that image are interpretable. We’ve
    attained these results using a straightforward series of steps, which can be applied
    to any large text dataset. Effectively, we’ve developed a pipeline for clustering
    and visualizing unstructured text data. That pipeline works as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 第7个聚类主要由像*disease*、*medical*、*doctor*、*food*、*pain*和*patients*这样的词组成。其医药主题现在很清晰；通过忽略无用的关键词，我们已经设法阐明了聚类的真正内容。当然，这种方法并不总是有效。自然语言处理很复杂，没有一劳永逸的解决方案能解决我们所有的问题。尽管如此，我们仍然可以取得很多成就，尽管复杂文本的无结构性质。考虑我们所取得的成就：我们已将10,000篇多样化的真实世界文本聚类成多个有意义的主题。此外，我们已将这些主题可视化在一张单独的图像中，该图像中的大多数主题都是可解释的。我们通过一系列简单的步骤实现了这些结果，这些步骤可以应用于任何大型文本数据集。实际上，我们已经开发了一个用于聚类和可视化非结构化文本数据的管道。该管道的工作原理如下：
- en: Transform our text into a normalized TFIDF matrix using the `TfidfVectorizer`
    class.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TfidfVectorizer`类将我们的文本转换为标准化的TFIDF矩阵。
- en: Reduce the matrix to 100 dimensions using the SVD algorithm.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SVD算法将矩阵降低到100维。
- en: Normalize the dimensionally reduced output for clustering purposes.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了聚类目的，对降维后的输出进行归一化。
- en: Cluster the normalized output using K-means. We can estimate *K* by generating
    an elbow plot using mini-batch K-means, which is optimized for speed.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用K-means对标准化输出进行聚类。我们可以通过生成肘图来估计*K*，肘图使用的是优化速度的迷你批K-means。
- en: Visualize the top words in each cluster using a word cloud. All word clouds
    are displayed as subplots in a single figure. Words are ranked based on their
    summed TFIDF values across all the texts in a cluster.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用词云可视化每个聚类的关键词。所有词云都显示在单个图中的子图中。词的排名基于它们在聚类中所有文本的TFIDF值的总和。
- en: Interpret the topic of each cluster using the word cloud visualization. Any
    uninterpretable clusters are examined in more detail.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用词云可视化来解释每个聚类的主题。任何无法解释的聚类都需要更详细地检查。
- en: Given our text-analysis pipeline, we can effectively cluster and interpret almost
    any real-world text dataset.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们的文本分析管道，我们可以有效地聚类和解释几乎任何真实世界的文本数据集。
- en: Summary
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Scikit-learn’s newsgroup dataset contains over 10,000 newsgroup posts spread
    out across 20 newsgroup categories.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn的新组数据集包含超过10,000篇新组帖子，分布在20个新组类别中。
- en: We can convert the posts into a TF matrix using scikit-learn’s `CountVectorizer`
    class. The generated matrix is stored in the *CSR* format. This format is used
    to efficiently analyze sparse matrices, which are composed mostly of zeros.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用scikit-learn的`CountVectorizer`类将帖子转换为TF矩阵。生成的矩阵以*CSR*格式存储。该格式用于高效分析主要由零组成的稀疏矩阵。
- en: Generally, TF matrices are sparse; a single row may only reference a few dozen
    words from the entire dataset vocabulary. We can access these nonzero words with
    the help of the `np.flatnonzero` function.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，TF矩阵是稀疏的；单行可能只引用整个数据集词汇表中的几十个词。我们可以通过`np.flatnonzero`函数访问这些非零词。
- en: The most frequently occurring words in a text tend to be *stop words*, which
    are common English words like *the* or *this*. Stop words should be filtered from
    text datasets before vectorization.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本中最常出现的词往往是*停用词*，如*the*或*this*。在向量化之前应从文本数据集中过滤掉停用词。
- en: Even after stop-word filtering, certain overly common words will remain. We
    can minimize the impact of these words using their document frequencies. A word’s
    *document frequency* equals the total fraction of texts in which that word appears.
    Words that are more common are less significant. Hence, less significant words
    have higher document frequencies.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使经过停用词过滤，某些过于常见的词仍然会保留。我们可以通过它们的文档频率最小化这些词的影响。一个词的*document frequency*等于该词出现在所有文本中的总分数。更常见的词不那么重要。因此，不那么重要的词具有更高的文档频率。
- en: We can combine term frequencies and document frequencies into a single significance
    score called *TFIDF*. Generally, TFIDF vectors are more informative than TF vectors.
    We can convert texts to TFIDF vectors using scikit-learn’s `TfidfVectorizer` class.
    That vectorizer returns a TFIDF matrix whose rows are automatically normalized
    for easier similarity computation.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将词频和文档频率合并成一个称为*TFIDF*的单个显著性分数。通常，TFIDF向量比TF向量更有信息量。我们可以使用scikit-learn的`TfidfVectorizer`类将文本转换为TFIDF向量。该向量器返回一个TFIDF矩阵，其行自动归一化，以便更容易进行相似度计算。
- en: Large TFIDF matrices should be dimensionally reduced before clustering. The
    recommended number of dimensions is 100\. Scikit-learn’s dimensionally reduced
    SVD output needs to be normalized before subsequent analysis.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在聚类之前，大型TFIDF矩阵应该进行降维。推荐的维度数量是100。Scikit-learn的降维SVD输出在后续分析之前需要归一化。
- en: We can cluster normalized, dimensionally reduced text data using either K-means
    or DBSCAN. Unfortunately, it’s hard to optimize DBSCAN’s parameters during text
    clustering. Thus, K-means remains the preferable clustering algorithm. We can
    estimate *K* using an elbow plot. If our dataset is large, we should generate
    the plot using `MiniBatchKMeans` for a faster runtime.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用K-means或DBSCAN对归一化、降维后的文本数据进行聚类。不幸的是，在文本聚类过程中优化DBSCAN的参数比较困难。因此，K-means仍然是首选的聚类算法。我们可以通过绘制肘部图来估计*K*值。如果我们的数据集很大，我们应该使用`MiniBatchKMeans`来生成图表以获得更快的运行时间。
- en: 'For any given text cluster, we want to view the words that are most relevant
    to the cluster. We can rank each word by summing its TFIDF values across all matrix
    rows represented by the cluster. Furthermore, we can visualize the ranked words
    in a *word cloud*: a 2D image composed of words, where word size is proportional
    to significance.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于任何给定的文本聚类，我们希望查看与聚类最相关的单词。我们可以通过将聚类表示的所有矩阵行中的TFIDF值相加来对每个单词进行排名。此外，我们可以在*词云*中可视化排名的单词：由单词组成的2D图像，其中单词大小与重要性成比例。
- en: We can plot multiple word clouds in a single figure using the `plt.subplots`
    function. This visualization gives us a bird’s-eye view of all the dominant word
    patterns across all clusters.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`plt.subplots`函数在一个图中绘制多个词云。这种可视化方式让我们能够从鸟瞰的角度看到所有聚类中的主导词模式。
