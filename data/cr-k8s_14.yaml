- en: 14 Nodes and Kubernetes security
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 节点和Kubernetes安全
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Node hardening and Pod manifest
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点加固和Pod清单
- en: API server security, including RBAC
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API服务器安全，包括RBAC
- en: User authentications and authorization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户身份验证和授权
- en: The Open Policy Agent (OPA)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放策略代理（OPA）
- en: Multi-tenancy in Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的多租户
- en: We just wrapped up securing the Pod in the previous chapter; now we’ll cover
    securing the Kubernetes node. In this chapter, we’ll include more information
    about node security as it relates to possible attacks on nodes and Pods, and we’ll
    provide full examples with a number of configurations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们刚刚完成了Pod的安全设置；现在我们将讨论Kubernetes节点的安全。在本章中，我们将包括更多关于节点安全的信息，这些信息与对节点和Pod的可能攻击有关，并提供带有多种配置的完整示例。
- en: 14.1 Node security
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 节点安全
- en: Securing a node in Kubernetes is analogous to securing any other VM or data
    center server. We’ll cover Transport Layer Security (TLS) certificates to start.
    These certificates allow for securing nodes, but we’ll also look at issues related
    to image immutability, workloads, network policies, and so on. Treat this chapter
    as an à la carte menu of important security topics that you should at least consider
    for running Kubernetes in production.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中保护节点类似于保护任何其他虚拟机或数据中心服务器。我们将从传输层安全（TLS）证书开始介绍。这些证书允许保护节点，但我们还将探讨与镜像不可变性、工作负载、网络策略等相关的问题。将本章视为一个按需菜单，其中包含你至少应该考虑在生产中运行Kubernetes的重要安全主题。
- en: 14.1.1 TLS certificates
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.1 TLS证书
- en: All external communications in Kubernetes generally occur over TLS, although
    this can be configured. However, there are many flavors of TLS. For this reason,
    you can select a cipher suite for the Kubernetes API server to use. Most installers
    or self-hosted versions of Kubernetes will handle the creation of the TLS certificates
    for you. *Cipher suites* are collections of algorithms that, in aggregate, allow
    for TLS to happen securely. Defining a TLS algorithm consists of
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的所有外部通信通常都通过TLS进行，尽管这可以配置。然而，TLS有许多不同的版本。因此，你可以为Kubernetes API服务器选择一个加密套件。大多数安装程序或自托管的Kubernetes版本将为你处理TLS证书的创建。*加密套件*是一系列算法的集合，总体上允许TLS安全地进行。定义TLS算法包括
- en: '*Key exchanges*—Sets up an agreed upon way to exchange keys for encryption/
    decryption'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*密钥交换*——设置一个同意的密钥交换方式，用于加密/解密'
- en: '*Authentication*—Confirms the identity of the sender of a message'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*身份验证*——确认消息发送者的身份'
- en: '*Encryption*—Disguises messages so outsiders can’t read them'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加密*——使消息在外部人员无法阅读的情况下隐藏'
- en: '*Message authentication*—Confirms that messages are coming from a valid source'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*消息认证*——确认消息来自有效源'
- en: 'In Kubernetes, you might find the following cipher-suite: `TLS_ECDHE_ECDSA_WITH
    _AES_256_CBC_SHA384`. Let’s break that down. Each underscore (`_`) in this string
    separates one algorithm from the next. For example, if we set `--tls-cipher-suites`
    in the API server to be something like'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，你可能会找到以下加密套件：`TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384`。让我们来分解一下。在这个字符串中，每个下划线（`_`）将一个算法与下一个算法分开。例如，如果我们设置API服务器中的`--tls-cipher-suites`为类似
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'we can look this specific protocol up at [http://mng.bz/nYZ8](http://mng.bz/nYZ8)
    and then determine how the communication protocol works. For example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[http://mng.bz/nYZ8](http://mng.bz/nYZ8)上查找这个特定的协议，然后确定通信协议的工作方式。例如：
- en: '*Protocol*—Transport Layer Security (TLS)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*协议*——传输层安全（TLS）'
- en: '*Key exchange*—Elliptic Curve Diffie-Hellman Ephemeral (ECDHE)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*密钥交换*——椭圆曲线迪菲-赫尔曼临时（ECDHE）'
- en: '*Authentication*—Elliptic Curve Digital Signature Algorithm (ECDSA)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*身份验证*——椭圆曲线数字签名算法（ECDSA）'
- en: '*Encryption*—Advanced Encryption Standard with 256-bit key in cipher block
    chaining mode (AES 256 CBC)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加密*——通过加密块链模式（AES 256 CBC）使用256位密钥的高级加密标准'
- en: 'The specifics of these protocols are beyond the scope of this book, but it
    is important to note that you need to monitor your TLS security posture, especially
    if it is set by a larger standards body in your organization, to confirm that
    your security model in Kubernetes aligns with the TLS standards for your organization.
    For example, to update the cipher suites used by any Kubernetes service, send
    it the `tls-cipher-suites` argument on startup:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些协议的具体细节超出了本书的范围，但重要的是要注意，你需要监控你的TLS安全状态，特别是如果它是由你组织中的更大标准机构设置的，以确认你的Kubernetes安全模型与组织中的TLS标准相一致。例如，要更新任何Kubernetes服务使用的加密套件，在启动时发送`tls-cipher-suites`参数：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Adding this to your API server ensures that it only connects to other services
    using this cipher suite. As shown, you can support multiple cipher suites by adding
    a comma to separate the values. A comprehensive list of suites is available in
    the help page for any service (for example, [http://mng.bz/voZq](http://mng.bz/voZq)
    shows the help for the Kubernetes scheduler, `kube-scheduler`). It’s also important
    to note that
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将此添加到你的API服务器中可以确保它只通过此密钥套件连接到其他服务。如图所示，你可以通过添加逗号来分隔值以支持多个密钥套件。任何服务的帮助页面都提供了一个全面的套件列表（例如，[http://mng.bz/voZq](http://mng.bz/voZq)显示了Kubernetes调度器`kube-scheduler`的帮助）。还应注意，
- en: If a TLS cipher is exposed as having a vulnerability, you’ll want to update
    the cipher suites in the Kubernetes API server, scheduler, controller manager,
    and kubelet. Each of these components serves content over TLS in one way or another.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个TLS密钥被发现存在漏洞，你将希望更新Kubernetes API服务器、调度器、控制器管理器和kubelet中的密钥套件。这些组件以某种方式通过TLS提供服务。
- en: If your organization doesn’t allow certain cipher suites, you should explicitly
    delist these.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的组织不允许某些密钥套件，你应该明确删除这些套件。
- en: Note If you oversimplify the cipher suites that you allow into your API server,
    you might risk certain types of clients not being able to connect to it. As an
    example of this, Amazon ELBs are known to sometimes use HTTPS health checks to
    ensure that an endpoint is up before forwarding traffic to it, but they don’t
    support some common TLS ciphers used in the Kubernetes API server. Version 1 of
    the AWS load balancer API only supports non-elliptic cipher algorithms, such as
    `TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256`. The result here can be crippling; your
    entire cluster will not work at all! Because it’s common to put several API servers
    behind a single ELB, keep in mind that a TCP health check (rather than HTTPS)
    might be easier to manage over time, especially if you require special security
    ciphers on your API servers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你过于简化允许进入你的API服务器的密钥套件，你可能会面临某些类型的客户端无法连接到它的风险。例如，Amazon ELB有时会使用HTTPS健康检查来确保在转发流量之前端点是可用的，但它们不支持Kubernetes
    API服务器中使用的某些常见TLS密钥。AWS负载均衡器API的版本1仅支持非椭圆曲线密钥算法，例如`TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256`。这里的结果可能是灾难性的；你的整个集群将完全无法工作！因为通常会在单个ELB后面放置多个API服务器，所以请记住，随着时间的推移，TCP健康检查（而不是HTTPS）可能更容易管理，特别是如果你需要在API服务器上使用特殊的安全密钥的话。
- en: 14.1.2 Immutable OSs vs. patching nodes
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.2 不可变操作系统与修补节点
- en: Immutability is something that you cannot change. An *immutable* OS consists
    of components and binaries that are read-only, and ones that you cannot patch.
    Instead of patching and updating software, you can replace the entire OS by wiping
    the OS from the server or the cloud and then deleting the VM and creating a new
    one. Kubernetes has simplified the running of immutable OSs by allowing an administrator
    to more easily move workloads off a node (because it is a built-in feature).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不可变性是你无法更改的东西。一个*不可变*的操作系统由只读组件和二进制文件组成，你不能对其进行修补。与其修补和更新软件，不如通过从服务器或云中擦除操作系统来替换整个操作系统，然后删除虚拟机并创建一个新的。Kubernetes通过允许管理员更容易地将工作负载从节点上移除（因为这是一个内置功能）来简化了不可变操作系统的运行。
- en: Instead of having a system that automatically applies patches with your distribution’s
    package manager, use an immutable OS. Having a Kubernetes cluster removes the
    notion of customized *snowflake servers*. These are servers that run specific
    applications and replace those with a node that is standardized, but running an
    immutable OS as the next logical step. One of the easiest ways to inject a vulnerability
    into a mutable system is to replace a common Linux library.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用你的发行版的包管理器自动应用补丁的系统相比，使用不可变操作系统。拥有一个Kubernetes集群消除了定制*雪花服务器*的概念。这些服务器运行特定的应用程序，并用一个标准化的节点替换它们。作为下一个逻辑步骤，运行不可变操作系统。将漏洞注入可变系统的一种最简单的方法是替换一个常见的Linux库。
- en: Immutable OSs are read-only, and because they are read-only, it becomes impossible
    to make specific changes, because you cannot write those changes to disk, and
    reduces our exposure. Using a distribution that is immutable removes a multitude
    of these opportunities. In general, a Kubernetes control plane (the API server,
    controller manager, and scheduler) node will have
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不可变操作系统是只读的，由于它们是只读的，因此无法进行特定的更改，因为你不能将这些更改写入磁盘，这减少了我们的暴露。使用不可变的发行版消除了这些机会中的许多。一般来说，Kubernetes控制平面（API服务器、控制器管理器和调度器）节点将具有
- en: A kubelet binary
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet二进制文件
- en: The `kube-proxy` binary
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy`二进制文件'
- en: A containerd or other container runtime executable
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: containerd 或其他容器运行时可执行文件
- en: An image for etcd
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 的镜像
- en: All of these are baked in for rapid bootstrapping startup. Meanwhile, a Kubernetes
    worker node will have the same components, with the exception of etcd. This is
    an important distinction because etcd doesn’t run in a supported manner on a Windows
    environment, but some users will want to run Windows worker nodes to run Windows
    Pods.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些功能都是为了快速启动而内置的。同时，Kubernetes 工作节点将拥有相同的组件，除了 etcd。这是一个重要的区别，因为 etcd 在 Windows
    环境中不支持运行，但一些用户可能希望运行 Windows 工作节点来运行 Windows Pod。
- en: Building custom images for Windows workers is extremely important because the
    Windows OS is not redistributable, so end users must build Windows kubelet images
    if they want to use an immutable deployment model. To learn more about immutable
    images, you can evaluate the Tanzu Community Edition project ([https://tanzu .vmware.com/tanzu/community](https://tanzu.vmware.com/tanzu/community)).
    It aims to provide the broader community with a “batteries-included” approach
    to using immutable images along with the Cluster API to bring up usable, production
    grade clusters. Many other hosted Kubernetes services, including Googles GKE,
    also use immutable operating systems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Windows 工作节点构建自定义镜像非常重要，因为 Windows 操作系统不可分发，因此最终用户必须构建 Windows kubelet 镜像，如果他们想使用不可变部署模型。要了解更多关于不可变镜像的信息，您可以评估
    Tanzu Community Edition 项目（[https://tanzu.vmware.com/tanzu/community](https://tanzu.vmware.com/tanzu/community)）。该项目旨在为更广泛的社区提供一个“包含电池”的方法来使用不可变镜像，并配合
    Cluster API 启用可用的、生产级别的集群。许多其他托管 Kubernetes 服务，包括 Google 的 GKE，也使用不可变操作系统。
- en: 14.1.3 Isolated container runtimes
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.3 隔离容器运行时
- en: Containers are amazing, but they do fall short of completely isolating the process
    from the OS. Docker Engine (and other container engines) does not fully sandbox
    a running container from the Linux kernel. There is not a strong security boundary
    between the container and the host, so if the host’s kernel has an exploit, the
    container can probably access the Linux kernel exploit and take advantage of that.
    Docker Engine utilizes Linux namespaces to separate processes from things like
    direct access to the Linux networking stack, but there are still holes. For instance,
    the hosts /sys and /proc filesystems are still read by a process running inside
    a container.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 容器非常出色，但它们并不能完全隔离进程与操作系统。Docker 引擎（以及其他容器引擎）并没有完全将运行中的容器与 Linux 内核沙箱化。容器与宿主机之间没有强大的安全边界，因此如果宿主机的内核存在漏洞，容器可能能够访问
    Linux 内核漏洞并利用它。Docker 引擎利用 Linux 命名空间来隔离进程，例如直接访问 Linux 网络堆栈，但仍然存在漏洞。例如，宿主机的 /sys
    和 /proc 文件系统仍然被容器内运行的过程读取。
- en: Projects like gVisor, IBM Nabla, Amazon Firecracker, and Kata provide a virtual
    Linux kernel that isolates a container’s process from the host’s kernel, thus
    providing a truer sandbox. These projects are still relatively new, at least in
    an open source sense, and are not yet predominantly used in a Kubernetes environment.
    These are just a few of the projects that are quite mature because gVisor is used
    as part of Google Cloud Platform, and Firecracker is used as part of the Amazon
    Web Services platform. Perhaps by the time you read this, more Kubernetes clusters
    containers will run on top of a virtual kernel! We can even think about spinning
    up micro VMs as Pods. These are fun times that we live in!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 gVisor、IBM Nabla、Amazon Firecracker 和 Kata 这样的项目提供了一种虚拟 Linux 内核，它将容器进程与宿主机的内核隔离开来，从而提供了一个更真实的沙箱环境。这些项目在开源领域相对较新，至少目前还没有在
    Kubernetes 环境中得到广泛使用。这些只是其中一些相当成熟的项目，因为 gVisor 被用作 Google Cloud Platform 的一部分，而
    Firecracker 被用作 Amazon Web Services 平台的一部分。也许在你阅读这篇文章的时候，更多的 Kubernetes 集群容器将运行在虚拟内核之上！我们甚至可以思考启动微虚拟机作为
    Pod。我们正生活在有趣的时代！
- en: 14.1.4 Resource attacks
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.4 资源攻击
- en: A Kubernetes node has a finite amount of resources that include CPU, memory,
    and disk. We have a number of Pods, a `kube-proxy`, kubelets, and other Linux
    processes running on the cluster. The node typically has a CNI provider, a logging
    daemon, and other processes supporting the cluster. You need to ensure that the
    container(s) in the Pods do not overwhelm the node resource usage. If you do not
    provide restraints, then a container can overwhelm a node and impact all of the
    other systems. In essence, a *runaway container process* can perform a Denial
    of Service (DoS) attack on a node. Enter the resource limits. . . .
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 节点拥有有限的资源，包括 CPU、内存和磁盘。我们在集群上运行了许多 Pod、`kube-proxy`、kubelets 以及其他
    Linux 进程。节点通常有一个 CNI 提供商、一个日志守护进程以及其他支持集群的进程。您需要确保 Pod 中的容器不会耗尽节点资源。如果您不提供限制，那么一个容器可能会耗尽节点资源，影响所有其他系统。本质上，一个
    *失控的容器进程* 可以对一个节点执行拒绝服务（DoS）攻击。这就是资源限制的用武之地……
- en: 'Resource limits are controlled by implementing three different API-level objects
    and configurations. Pod API objects can have settings that control each of the
    limits. For instance, the following YAML stanza provides CPU, memory, and disk
    space usage limits:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 资源限制通过实现三个不同的 API 级对象和配置来控制。Pod API 对象可以具有控制每个限制的设置。例如，以下 YAML 段落提供了 CPU、内存和磁盘空间使用限制：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Provisions the initial amount of CPU, memory, or storage
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提供初始的 CPU、内存或存储量
- en: ❷ Sets the maximum amount of CPU, memory, and storage allowed
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置允许的最大 CPU、内存和存储量
- en: In terms of security, if any of these values are surpassed, the Pod is restarted.
    And, if the limits are passed again, then the Pod is terminated and not started
    again.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全方面，如果这些值中的任何一个超过了限制，Pod 将会被重启。而且，如果再次超过限制，那么 Pod 将被终止，并且不会再次启动。
- en: Another interesting thing is that resource `requests` and `limits` also impact
    the scheduling of a Pod on a node. The node that hosts a Pod must have the resources
    available for the initial requests for the scheduler to pick the node that hosts
    the Pod. You may notice that we are using units to represent requests and limit
    memory, CPU, and ephemeral storage values.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的事情是，资源 `requests` 和 `limits` 也会影响 Pod 在节点上的调度。托管 Pod 的节点必须具有满足调度器选择托管
    Pod 的节点的初始请求的资源。您可能会注意到我们正在使用单位来表示请求和限制内存、CPU 和临时存储值。
- en: 14.1.5 CPU units
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.5 CPU 单位
- en: 'To measure a CPU, the base unit that Kubernetes uses is 1, which equates to
    one hyperthread on bare metal or one core/vCPU in the cloud. You are also allowed
    to express a CPU unit in a decimal; for example, you can have 0.25 CPU units.
    Moreover, the API also allows you to convert 0.25 decimal CPU units to read as
    250 m. All of these stanzas are allowed for the CPU:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要测量 CPU，Kubernetes 使用的基准单位是 1，这相当于裸金属上的一个超线程或在云中的单个核心/vCPU。您也可以用小数表示 CPU 单位；例如，您可以有
    0.25 个 CPU 单位。此外，API 还允许您将 0.25 个十进制 CPU 单位转换为 250 m。所有这些段落都适用于 CPU：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Sets 42 CPUs (it’s a big server!)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置 42 个 CPU（这是一个大服务器！）
- en: ❷ 0.42 of a CPU that is measured as a unit of 1
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 0.42 个 CPU，以 1 为单位进行测量
- en: ❸ This is the same as 0.42 in the previous code block.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这与上一个代码块中的 0.42 相同。
- en: 14.1.6 Memory units
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.6 内存单位
- en: 'Memory is measured in bytes, in integers, and in fixed-point numbers using
    these suffixes: *E*, *P*, *T*, *G*, *M*, or *K*. Or you can use *Ei*, *Pi*, *Ti*,
    *Gi*, *Mi*, or *Ki*, which represent the power-of-two equivalents. The following
    stanzas are roughly all the same values:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 内存以字节、整数和固定点数进行测量，使用以下后缀：*E*、*P*、*T*、*G*、*M* 或 *K*。或者您可以使用 *Ei*、*Pi*、*Ti*、*Gi*、*Mi*
    或 *Ki*，它们代表二进制的等效值。以下段落大致都是相同的值：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Byte plain number representations (128,974,848 bytes)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 字节纯数字表示（128,974,848 字节）
- en: '❷ 129e6 is sometimes written as 129e+6 in scientific notation: 129e+6 == 129000000\.
    This stanza represents 129,000,000 bytes.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 129e6 有时写作 129e+6 的科学记数法：129e+6 == 129000000。这个段落代表 129,000,000 字节。
- en: 'The next stanzas deal with the typical megabits versus megabytes conversions:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个段落处理的是典型的兆比特与兆字节的转换：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ 129 megabits == 1.613e+7 bytes, which is close to the 129e+6 value.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 129 兆比特 == 1.613e+7 字节，接近 129e+6 的值。
- en: 'Next, megabytes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是兆字节：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ 123 megabytes == 1.613e+7 bytes, which is close to the 129e+6 value.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 123 兆字节 == 1.613e+7 字节，接近 129e+6 的值。
- en: 14.1.7 Storage units
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.7 存储单位
- en: 'The newest API configuration is ephemeral storage requests and limits. Ephemeral
    storage limits apply to three storage components:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 API 配置是临时存储请求和限制。临时存储限制适用于三个存储组件：
- en: emptyDir volumes, except tmpfs
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: emptyDir 卷，除了 tmpfs
- en: Directories holding node-level logs
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存放节点级日志的目录
- en: Writeable container layers
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可写容器层
- en: When a limit is surpassed, the kubelet evicts the Pod. Each node is configured
    with a maximum amount of ephemeral storage, which again impacts the schedule of
    Pods to a node. There is yet another limit where a user can specify specific node
    limits called *extended resources*. You can find more details about extended resources
    in the Kubernetes documentation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当超过限制时，kubelet会驱逐Pod。每个节点都配置了最大临时存储量，这再次影响了Pod到节点的调度。还有一个用户可以指定特定节点限制的“扩展资源”限制。你可以在Kubernetes文档中找到有关扩展资源的更多详细信息。
- en: 14.1.8 Host networks vs. Pod networks
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.8 主机网络与Pod网络比较
- en: 'In section 14.4, we will cover NetworkPolicies. These give you the ability
    to lock down Pod communication using a CNI provider that, typically, implements
    these policies for you. There’s a much more fundamental type of network security
    you should consider, however: not running a Pod on the same network as your hosts.
    This instantly'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14.4节中，我们将介绍NetworkPolicies。这些政策允许你使用CNI提供者锁定Pod通信，通常，这些政策为你实现。然而，你应该考虑一种更基本的网络安全类型：不要将Pod运行在与你的主机相同的网络上。这立即
- en: Limits the access of the outside world to your Pod
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制外部世界对您的Pod的访问
- en: Limits the access of your Pod to the host’s network ports
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制您的Pod对主机网络端口的访问
- en: 'Having a Pod join the host network allows the Pod to have easier access to
    the node and, thus, increases the blast radius upon an attack. If a Pod does not
    have to run on the host network, do not run the Pod on the host network! Should
    a Pod need to run on the host network, then do not expose that Pod to the internet.
    The following code snippet is a partial YAML definition of a Pod that runs on
    the host network. You will often see Pods running on the host network if the Pod
    is performing administrative tasks, such as logging or networking (a CNI provider):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个Pod加入主机网络，将允许Pod更容易地访问节点，从而在攻击时增加爆炸半径。如果一个Pod不需要在主机网络上运行，请不要在主机网络上运行Pod！如果Pod需要在主机网络上运行，那么请不要将该Pod暴露给互联网。以下代码片段是一个在主机网络上运行的Pod的部分YAML定义。如果Pod正在执行管理任务，如日志记录或网络（一个CNI提供者），你通常会看到在主机网络上运行的Pod：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 14.1.9 Pod example
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.9 Pod示例
- en: 'We have covered different Pod API configurations: service account tokens, CPU,
    and other resource settings, security context, and so forth. Here is an example
    that contains all the configurations:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了不同的Pod API配置：服务账户令牌、CPU和其他资源设置、安全上下文等。以下是一个包含所有配置的示例：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Disables automount for the service account token
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 禁用服务账户令牌的自动挂载
- en: ❷ Sets the security context and gives a capability of NET_ADMIN
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置安全上下文并赋予NET_ADMIN能力
- en: ❸ Runs on the host network
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在主机网络上运行
- en: ❹ Sets resource limits
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置资源限制
- en: ❺ Gives the Pod a specific service account
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 给Pod分配一个特定的服务账户
- en: 14.2 API server security
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 API服务器安全
- en: Components like binary authentication use webhooks provided by the admission
    controller. Various controllers are part of the Kubernetes API server and create
    a webhook as an entry point for events. For instance, ImagePolicyWebhook is one
    of the plugins that allows for the system to respond to the webhook and make admission
    decisions about containers. If a Pod does not pass the admission standards, it
    holds it in a pending state—it is not deployed to the cluster. Admission controllers
    can validate the API objects being created in the cluster, mutate or change those
    objects, or do both. From a security standpoint, this provides an immense amount
    of control and auditing capabilities for a cluster.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 像二进制认证这样的组件使用准入控制器提供的webhooks。各种控制器是Kubernetes API服务器的一部分，并创建一个webhook作为事件的入口点。例如，ImagePolicyWebhook是允许系统响应webhook并对容器做出准入决定的插件之一。如果一个Pod没有通过准入标准，它将保持挂起状态——它不会被部署到集群中。准入控制器可以验证集群中正在创建的API对象，修改或更改这些对象，或者两者都做。从安全角度来看，这为集群提供了大量的控制和审计能力。
- en: 14.2.1 Role-based access control (RBAC)
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.1 基于角色的访问控制（RBAC）
- en: First and foremost, role-based access control (RBAC) needs to be enabled on
    your cluster. Currently, RBAC is enabled by most installers and cloud-hosted providers.
    The Kubernetes API server uses the `--authorization-mode=RBAC` flag to enable
    RBAC. If you are using a self-hosted version of Kubernetes, such as GKE, RBAC
    is enabled. The authors are certain there is an edge case where running RBAC does
    not meet business needs. However, the other 99% of the time, you need to enable
    RBAC.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在您的集群上启用基于角色的访问控制（RBAC）是必要的。目前，大多数安装程序和云托管提供商都启用了 RBAC。Kubernetes API 服务器使用
    `--authorization-mode=RBAC` 标志来启用 RBAC。如果您使用的是 Kubernetes 的自托管版本，如 GKE，则 RBAC
    已启用。作者确信存在一种边缘情况，即运行 RBAC 无法满足业务需求。然而，在其余 99% 的情况下，您需要启用 RBAC。
- en: RBAC is a role-based security mechanism that controls user and system access
    to resources. It restricts access to resources to only authorized users and service
    accounts via roles and privileges. How does that apply to Kubernetes? One of the
    most critical components that you want to secure with Kubernetes is the API server.
    When a system user has administrator access to the cluster via the API server,
    that user can drain nodes, delete objects, and otherwise cause a great level of
    disruption. Administrators in Kubernetes are root users in the context of the
    cluster.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC 是一种基于角色的安全机制，它控制用户和系统对资源的访问。它通过角色和权限仅限制授权用户和服务账户对资源的访问。这如何应用于 Kubernetes？您想用
    Kubernetes 保护的最关键组件之一是 API 服务器。当系统用户通过 API 服务器对集群具有管理员访问权限时，该用户可以清理节点、删除对象，并造成极大的破坏。在
    Kubernetes 中，管理员是在集群上下文中的 root 用户。
- en: RBAC is a powerful security component that provides great flexibility in how
    you restrict API access within a cluster. Because it is a powerful mechanism,
    it also has the usual side effect of being quite complex and challenging to debug
    at times.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC 是一个强大的安全组件，它提供了在集群内限制 API 访问的巨大灵活性。因为它是一个强大的机制，所以它也有通常的副作用，即有时相当复杂且难以调试。
- en: Note An average Pod running in Kubernetes should not have access to the API
    server, so you should disable the mounting of the service account token.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在 Kubernetes 中运行的平均 Pod 不应具有访问 API 服务器的权限，因此您应该禁用服务账户令牌的挂载。
- en: 14.2.2 RBAC API definition
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.2 RBAC API 定义
- en: 'The RBAC API defines the following types:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC API 定义了以下类型：
- en: '*Role*—Contains a set of permissions, limited to a namespace'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Role*—包含一组权限，限制在命名空间内'
- en: '*ClusterRole*—Contains a set of permissions that are cluster-wide'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ClusterRole*—包含一组集群范围内的权限'
- en: '*RoleBinding*—Grants a role to a user or a group'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RoleBinding*—将角色授予用户或组'
- en: '*ClusterRole*—Grants a ClusterRole to a user or a group'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ClusterRole*—将 ClusterRole 授予用户或组'
- en: Within the Role and ClusterRole definitions, there are several defined components.
    The first that we will cover is *verbs*, which include API and HTTP verbs. Objects
    within the API server can have a get request; hence, the get request verb definition.
    We often think about this in terms of the create, read, update, and delete (CRUD)
    verbs defined in the creation of REST services. Verbs you can use include
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Role 和 ClusterRole 定义中，有几个定义的组件。我们将首先介绍的是 *动词*，它包括 API 和 HTTP 动词。API 服务器内的对象可以有一个
    get 请求；因此，有 get 请求动词定义。我们经常从创建 REST 服务时定义的创建、读取、更新和删除（CRUD）动词的角度来考虑这个问题。您可以使用的动词包括
- en: '*API request verbs for resource requests*—get, list, create, update, patch,
    watch, proxy, redirect, delete, and deletecollection'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*资源请求的 API 请求动词*—get、list、create、update、patch、watch、proxy、redirect、delete 和
    deletecollection'
- en: '*HTTP request verbs for non-resource requests*—get, post, put, and delete'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*非资源请求的 HTTP 请求动词*—get、post、put 和 delete'
- en: For instance, if you want an operator to be able to watch and update Pods, you
    can
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想使操作员能够监视和更新 Pods，您可以
- en: Define the resource (in this case, a Pod)
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义资源（在这种情况下，是一个 Pod）
- en: Define the verbs that the Role has access to (most likely list and patch)
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义角色可以访问的动词（最可能是 list 和 patch）
- en: Define the API groups (using an empty string denotes the core API group)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 API 组（使用空字符串表示核心 API 组）
- en: 'You are already familiar with API groups because they are the `apiVersion`
    and `kind` that appear in Kubernetes manifests. API groups follow the REST path
    in the API server itself (/apis/$GROUP_NAME/$VERSION) and use `apiVersion $GROUP_NAME/$VERSION`
    (for instance, `batch/v1`). Let’s keep it simple, though, and not deal with API
    groups just yet. We’ll start with the core API group instead. Here is an example
    of a role for a specific namespace. Because roles are limited to namespaces, this
    provides access to perform list and patch verbs on the Pod resource:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经熟悉 API 组了，因为它们是出现在 Kubernetes 清单中的 `apiVersion` 和 `kind`。API 组遵循 API 服务器中的
    REST 路径（/apis/$GROUP_NAME/$VERSION）并使用 `apiVersion $GROUP_NAME/$VERSION`（例如，`batch/v1`）。不过，让我们保持简单，暂时不处理
    API 组。我们将从核心 API 组开始。以下是一个特定命名空间的角色的示例。由于角色仅限于命名空间，这提供了对 Pod 资源执行列表和补丁动词的访问权限：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For the previous example, we can define a service account to use the role in
    that snippet like so:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的示例，我们可以定义一个服务账户来使用该片段中的角色，如下所示：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The previous YAML creates a service account that can be used by a Pod. Next,
    we’ll create a role binding to join the previous service account with the role
    that was also defined previously:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的 YAML 创建了一个服务账户，该账户可以被 Pod 使用。接下来，我们将创建一个角色绑定，将之前定义的服务账户与之前定义的角色连接起来：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now you can launch the Pod within a deployment that has the service account
    assigned to that Pod:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在分配给该 Pod 的服务账户的部署中启动 Pod：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s recap. We have created a role with the permissions to patch and list Pods.
    Then, we created a service account so we can create a Pod and have that Pod use
    the defined user. Next, we defined a role binding to add the service account to
    the role. Lastly, we launched a deployment that has a Pod defined, which uses
    the service account that was previously defined.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下。我们创建了一个具有修补和列出 Pod 权限的角色。然后，我们创建了一个服务账户，以便我们可以创建 Pod 并让该 Pod 使用定义的用户。接下来，我们定义了一个角色绑定，将服务账户添加到之前定义的角色中。最后，我们启动了一个包含定义
    Pod 的部署，该 Pod 使用之前定义的服务账户。
- en: RBAC is nontrivial, but vital to the security of a Kubernetes cluster. The previous
    YAML was taken from the Helmsman RBAC demo located at [http://mng.bz/ZzMa](http://mng.bz/ZzMa).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC（基于角色的访问控制）虽然不复杂，但对 Kubernetes 集群的安全性至关重要。前面的 YAML 是从位于 [http://mng.bz/ZzMa](http://mng.bz/ZzMa)
    的 Helmsman RBAC 演示中提取的。
- en: 14.2.3 Resources and subresources
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.3 资源和子资源
- en: 'Most RBAC resources use a single name, like Pod or Deployment. Some resources
    have subresources, such as in the following code snippet:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 RBAC 资源使用单个名称，如 Pod 或 Deployment。一些资源有子资源，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This API endpoint denotes the path to the subresource log in the `rbac-example`
    namespace for the Pod named Pod-labeler. The definition follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此 API 端点表示 `rbac-example` 命名空间中名为 Pod-labeler 的 Pod 的子资源日志路径。定义如下：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In order to use the subresource of logs, you would define a role. The following
    shows an example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用日志的子资源，你需要定义一个角色。以下是一个示例：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can also further restrict access to the logs of a Pod by naming the Pod.
    For example:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过命名 Pod 来进一步限制对 Pod 日志的访问。例如：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Notice that the `rules` element in the previous YAML is an array. The following
    code snippet shows how you can add multiple permissions to the YAML. The `resources`,
    `resourceNames`, and `verbs` can be of any combination:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到之前的 YAML 中的 `rules` 元素是一个数组。以下代码片段显示了如何向 YAML 添加多个权限。`resources`、`resourceNames`
    和 `verbs` 可以是任何组合：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Resources are things like Pods and nodes, but the API server also includes
    elements that are not resources. These are defined by the actual URI component
    in the API REST endpoint; for example, giving the `Pod-labeler-logs` RBAC role
    access to the /healthz API endpoint, as the following snippet shows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 资源类似于 Pods 和节点，但 API 服务器还包括不是资源的元素。这些元素由 API REST 端点的实际 URI 组件定义；例如，给 `Pod-labeler-logs`
    RBAC 角色访问 /healthz API 端点，如下面的代码片段所示：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ The asterisk (*) in a nonresource URL is a suffix glob match.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 非资源 URL 中的星号 (*) 是后缀通配符匹配。
- en: 14.2.4 Subjects and RBAC
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.4 主题和 RBAC
- en: 'Role bindings can encompass Users, ServiceAccounts, and Groups in Kubernetes.
    In the following example, we will create another service account called `log-reader`
    and add the service account to the role-binding definition in the previous section.
    In the example, we also have a user named `james-bond` and a group named `MI-6`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 角色绑定可以包括 Kubernetes 中的用户、服务账户和组。在以下示例中，我们将创建另一个名为 `log-reader` 的服务账户，并将该服务账户添加到上一节中的角色绑定定义中。在示例中，我们还有一个名为
    `james-bond` 的用户和一个名为 `MI-6` 的组：
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note Users and groups are created by the authentication strategy that is set
    up for the cluster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：用户和组是由为集群设置的认证策略创建的。
- en: 14.2.5 Debugging RBAC
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.5 调试RBAC
- en: Granted, RBAC is complex, and it’s a pain, but we always have the audit log.
    Kubernetes, when enabled, logs an audit trail of security events that have affected
    the cluster. These events include user actions, administrator actions, and/or
    other components inside the cluster. Basically, you get a “who,” “what,” “from
    where,” and “how” if RBAC and other security components are utilized. The audit
    logging is configured via an audit policy file that is passed into the API server
    via `kube-apiserver` `--audit-policy-file`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RBAC很复杂，使用起来也很痛苦，但我们始终有审计日志。当启用时，Kubernetes会记录影响集群的安全事件的审计跟踪。这些事件包括用户操作、管理员操作和/或集群内的其他组件。基本上，如果你使用了RBAC和其他安全组件，你会得到“谁”、“什么”、“从哪里”和“如何”的信息。审计日志是通过一个审计策略文件配置的，该文件通过`kube-apiserver`
    `--audit-policy-file`传递到API服务器。
- en: 'So we have a log of all events—awesome! But wait . . . you now have a cluster
    that has hundreds of roles, a bunch of users, and a plethora of role bindings.
    Now you have to join all of the data together. For that, there’s a couple of tools
    to assist us. The common theme between these tools is the joining of the different
    objects used to define RBAC access. In order to introspect RBAC-based permissions
    in cluster roles, RoleBindings and the Subject need to be joined. The Subject
    can be users, groups, or service accounts:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了所有事件的日志——太棒了！但是等等……现在你有一个拥有数百个角色、许多用户和大量角色绑定的集群。现在你必须将所有这些数据结合起来。为此，有几个工具可以帮助我们。这些工具的共同主题是将用于定义RBAC访问的不同对象结合起来。为了在集群角色、RoleBindings和主体中内省基于RBAC的权限，需要将主体（可以是用户、组或服务账户）结合起来。
- en: '*ReactiveOps has created a tool that allows a user to find the current roles
    that a user, group, or service account is a member of.* `rbac-lookup` is available
    at [https://github.com/reactiveops/rbac-lookup](https://github.com/reactiveops/rbac-lookup).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ReactiveOps*创建了一个工具，允许用户查找用户、组或服务账户当前是成员的角色。`rbac-lookup`可在[https://github.com/reactiveops/rbac-lookup](https://github.com/reactiveops/rbac-lookup)找到。'
- en: '*Another tool that finds which permissions a user or service account has within
    the cluster is* `kubectl-rbac`*.* This tool is located at [https://github.com/octarinesec/kubectl-rbac](https://github.com/octarinesec/kubectl-rbac).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*另一个可以找到用户或服务账户在集群内具有哪些权限的工具是* `kubectl-rbac`*。此工具位于[https://github.com/octarinesec/kubectl-rbac](https://github.com/octarinesec/kubectl-rbac)。'
- en: '*Jordan Liggit has an open source tool called* `audit2rbac`*.* This tool takes
    audit logs and a username and creates a role and role binding that match the requested
    access. Calls are made to the API server, and you can capture the logs. From there,
    you can run `audit2rbac` to generate the needed RBAC (in other words, RBAC reverse
    engineered).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jordan Liggit有一个名为* `audit2rbac`*的开源工具。该工具接受审计日志和用户名，创建与请求访问权限相匹配的角色和角色绑定。对API服务器进行调用，并可以捕获日志。从那里，你可以运行`audit2rbac`来生成所需的RBAC（换句话说，RBAC反向工程）。'
- en: 14.3 Authn, Authz, and Secrets
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 认证、授权和秘密
- en: '*Authn* (user authentication) and *Authz* (authorization) are the group and
    permissions that an authenticated user has. It may seem odd that we are talking
    about Secrets here as well, but some of the same tools used for authentication
    and authorization are used for Secrets, and you often need authentication and
    authorization to access Secrets.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*Authn*（用户认证）和*Authz*（授权）是认证用户拥有的组和权限。我们在这里也谈论秘密可能看起来有些奇怪，但一些用于认证和授权的工具也用于秘密，并且你通常需要认证和授权来访问秘密。'
- en: First and foremost, do not use the default cluster administrative certificates
    that are generated when installing a cluster. You will need an IAM (Identity and
    Access Management) service provider for authenticating and authorizing users.
    Also, do not enable username and password authentication on the API server; use
    the built-in capability for user authentication utilizing TLS certificates.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，不要使用在安装集群时生成的默认集群管理证书。你需要一个IAM（身份和访问管理）服务提供商来认证和授权用户。此外，不要在API服务器上启用用户名和密码认证；使用TLS证书内置的用户认证功能。
- en: '14.3.1 IAM service accounts: Securing your cloud APIs'
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.1 IAM服务账户：保护你的云API
- en: Kubernetes containers have identities that are *cloud native* (these identities
    are aware of the cloud). This is beautiful and terrifying at the same time. Without
    a threat model for your cloud, you cannot have a threat model for your Kubernetes
    cluster.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes容器具有*原生云*身份（这些身份了解云）。这既美丽又可怕。如果没有你的云威胁模型，就无法为你的Kubernetes集群制定威胁模型。
- en: 'Cloud IAM service accounts comprise the basics of security, including authorization
    and authentication for people and systems. Within the data center, Kubernetes
    security configuration is limited to the Linux system, Kubernetes, the network,
    and the containers deployed. When running Kubernetes in the cloud, a new wrinkle
    emerges—the IAM roles of nodes and Pods:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 云IAM服务账户构成了安全的基础，包括对人和系统的授权和认证。在数据中心内，Kubernetes安全配置仅限于Linux系统、Kubernetes、网络以及部署的容器。当在云中运行Kubernetes时，出现了一个新的问题——节点和Pods的IAM角色：
- en: IAM is the role for a specific user or service account, and that role is then
    a member of a group.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IAM是特定用户或服务账户的角色，该角色随后成为某个组的成员。
- en: Every node in a Kubernetes cluster has an IAM role.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes集群中的每个节点都有一个IAM角色。
- en: Pods typically inherit that role.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods通常继承该角色。
- en: 'The nodes in your cluster, specifically those that run on the control plane,
    need IAM roles to run inside the cloud. The reason for this is that a lot of cloud-native
    functionality in Kubernetes comes from the fact that Kubernetes itself has a notion
    of how to talk to its own cloud provider. As an example, let’s take a cue from
    GKE’s official documentation: Google Cloud Platform automatically creates a service
    account named compute engine default service account, and GKE associates it with
    the nodes it creates. Depending on how your project is configured, the default
    service account may or may not have permissions to use other cloud-platform APIs.
    GKE also assigns some limited access scopes to compute instances. Updating the
    default service account’s permissions or assigning more access scopes to compute
    instances is not the recommended way to authenticate to other cloud-platform services
    from Pods running on GKE.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你集群中的节点，特别是那些运行在控制平面上的节点，需要在云中运行时具有IAM角色。这是因为Kubernetes中许多原生功能来自于Kubernetes本身有一个如何与其云提供商通信的概念。例如，让我们从GKE的官方文档中汲取一些启示：Google
    Cloud Platform自动创建一个名为计算引擎默认服务账户的服务账户，GKE将其与它创建的节点关联起来。根据你的项目配置，默认服务账户可能或可能没有权限使用其他云平台API。GKE还向计算实例分配了一些有限的访问范围。更新默认服务账户的权限或将更多访问范围分配给计算实例不是从运行在GKE上的Pods中认证到其他云平台服务的推荐方法。
- en: Your containers, therefore, have privileges in many cases that are on par with
    the nodes themselves. As clouds evolve to make more granular permission models
    for containers, this default will likely improve in the future. It remains the
    case, however, that you need to ensure that the IAM role or roles have the least
    amount of permissions applicable and that there will always be ways to change
    these IAM roles. For instance, when using GKE in Google Cloud Platform, you have
    to create a new IAM role in a project for a cluster. If you do not, the cluster
    usually uses the compute engine default service account, which has editor permissions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你的容器在很多情况下具有与节点本身相当的权限。随着云服务逐渐为容器提供更细粒度的权限模型，这种默认设置很可能会在未来得到改善。然而，仍然需要确保IAM角色或角色具有最少的适用权限，并且始终存在更改这些IAM角色的方法。例如，当在Google
    Cloud Platform中使用GKE时，你必须为集群在项目中创建一个新的IAM角色。如果不这样做，集群通常会使用计算引擎默认服务账户，该账户具有编辑权限。
- en: '*Editor in Google Cloud* allows a given account (in this case, a node in your
    cluster, which translates to potentially any Pod) to edit any resource within
    that project. For example, you could delete an entire fleet of databases, TCP
    load balancers, or cloud DNS entries by simply compromising a given Pod in the
    cluster. Moreover, you should remove the default service account for any project
    that you create in GCP. The same problems exist in AWS, Azure, and others. The
    bottom line is that each cluster is created with its own unique service account,
    and that service account has the least possible permissions. With tools like `kops`
    (Kubernetes Operations), we can go through every permission a Kubernetes cluster
    requires, and `kops` then creates an IAM role specific to the control plane, as
    well as another for the nodes.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*Google Cloud 编辑器* 允许给定账户（在这种情况下，你的集群中的一个节点，这相当于可能任何 Pod）编辑该项目中任何资源。例如，你只需破坏集群中的某个
    Pod，就可以删除整个数据库集群、TCP 负载均衡器或云 DNS 条目。此外，你应该删除你在 GCP 中创建的任何项目的默认服务账户。AWS、Azure 等也存在相同的问题。总之，每个集群都是使用其独特的服务账户创建的，并且该服务账户具有尽可能少的权限。使用
    `kops`（Kubernetes Operations）等工具，我们可以检查 Kubernetes 集群所需的每个权限，然后 `kops` 为控制平面创建一个特定的
    IAM 角色，以及为节点创建另一个角色。'
- en: 14.3.2 Access to cloud resources
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.2 云资源访问
- en: Assuming that you have configured your Kubernetes nodes with the lowest level
    of permissions needed, now that you’ve read this, you may feel safe. In fact,
    if you are running a solution like AKS (Azure Kubernetes Service), you do not
    have to worry about configuring the control plane and only have to be concerned
    with the node-level IAM, but that’s not all. For example, a developer creates
    a service that needs to talk to a hosted cloud service—say, a file store. The
    Pod that is running now needs a service account with the correct roles. There
    are various approaches to this.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经以最低权限配置了你的 Kubernetes 节点，现在你已经阅读了这篇文章，你可能感觉安全。实际上，如果你正在运行 AKS（Azure Kubernetes
    Service）这样的解决方案，你不需要担心配置控制平面，只需关注节点级别的 IAM 即可，但这还不是全部。例如，一个开发者创建了一个需要与托管云服务通信的服务——比如文件存储服务。现在运行的
    Pod 需要一个具有正确角色的服务账户。有各种实现这一点的途径。
- en: Note AKS is probably the easiest solution, but it does create some challenges.
    You need to limit the Pods on the nodes to ones that only need access to the cloud
    resources or accept the risk that all Pods will now have file-store access.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：AKS 可能是解决方案中最简单的，但它确实带来了一些挑战。你需要限制节点上的 Pod，只允许访问云资源，或者接受所有 Pod 现在都将具有文件存储访问的风险。
- en: tip Use a tool like `kube2iam` ([https://github.com/jtblin/kube2iam](https://github.com/jtblin/kube2iam))
    or `kiam` ([https://github.com/uswitch/kiam](https://github.com/uswitch/kiam))
    for this approach.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：使用 `kube2iam` ([https://github.com/jtblin/kube2iam](https://github.com/jtblin/kube2iam))
    或 `kiam` ([https://github.com/uswitch/kiam](https://github.com/uswitch/kiam))
    工具来实现此方法。
- en: Some newly created operators can assign specific service accounts to specific
    Pods. The component on each node intercepts the calls to the cloud API. Instead
    of using the node’s IAM role, it assigns a role to a Pod in your cluster, which
    is denoted via annotations. Some of the hosted cloud providers have similar solutions.
    Some cloud providers, like Google, have sidecars that can run and connect to a
    cloud SQL service. The sidecar is assigned a role and then proxies the applications
    that connect to the database.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一些新创建的操作员可以将特定的服务账户分配给特定的 Pod。每个节点上的组件会拦截对云 API 的调用。它不是使用节点的 IAM 角色，而是将一个角色分配给集群中的
    Pod，这通过注解来表示。一些托管云提供商有类似的解决方案。一些云提供商，如 Google，有可以运行并连接到云 SQL 服务的边车。边车被分配一个角色，然后代理连接到数据库的应用程序。
- en: Probably the most complicated but more robust solution is to use a centralized
    vault server. With this, you can have applications retrieve short-lived IAM tokens
    that allow cloud system access. Often, a sidecar is used to automatically refresh
    the tokens used. We can also use HashiCorp Vault to secure Secrets that are not
    IAM credentials. If your use case requires robust Secrets and IAM management,
    Vault is an excellent solution, but as with all solutions that are mission critical,
    you will need to maintain and support it.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最复杂但更稳健的解决方案是使用集中式的密钥库服务器。使用这种方式，应用程序可以检索短期有效的 IAM 令牌，从而允许访问云系统。通常，会使用边车来自动刷新所使用的令牌。我们还可以使用
    HashiCorp Vault 来保护非 IAM 凭证的机密信息。如果你的用例需要稳健的机密和 IAM 管理，Vault 是一个绝佳的解决方案，但正如所有关键任务解决方案一样，你需要维护和支持它。
- en: tip Use HashiCorp Vault ([https://www.vaultproject.io/](https://www.vaultproject.io/))
    to store Secrets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：使用HashiCorp Vault ([https://www.vaultproject.io/](https://www.vaultproject.io/))来存储密钥。
- en: 14.3.3 Private API servers
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3.3 私有API服务器
- en: The last thing that we are going to cover in this section is the API server’s
    network access. You can either make your API server inaccessible via the internet
    or place the API server on a private network. You will need to utilize a bastion
    host, VPN, or other form of connectivity to the API server if you place the API
    server load balancer on a private network, so this solution is not as convenient.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将要讨论的最后一件事是API服务器的网络访问。您可以选择使API服务器通过互联网不可访问，或者将API服务器放在私有网络上。如果您将API服务器负载均衡器放在私有网络上，您将需要利用堡垒主机、VPN或其他形式的连接到API服务器，因此这种解决方案并不那么方便。
- en: The API server is an extremely sensitive security point and must be guarded
    as such. DoS attacks or general intrusion can cripple a cluster. Moreover, when
    the Kubernetes community finds security problems, they occasionally exist in the
    API server. If you can, put your API server on a private network, or at least
    whitelist the IP addresses that are able to connect to the load balancer that
    fronts your API server.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器是一个极其敏感的安全点，必须像这样进行保护。DoS攻击或一般入侵可能会使集群瘫痪。此外，当Kubernetes社区发现安全问题时，它们偶尔会存在于API服务器中。如果可能，请将您的API服务器放在私有网络上，或者至少将能够连接到您的API服务器前端负载均衡器的IP地址列入白名单。
- en: 14.4 Network security
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 网络安全
- en: 'Again, this is an area of security that is rarely addressed properly. By default,
    a Pod on the Pod network can access any Pod anywhere on the cluster, which also
    includes the API server. This capability exists to allow a Pod to access systems
    like DNS for service lookups. A Pod running on the host network can access just
    about everything: all Pods, all nodes, and the API server. A Pod on the host network
    can even access the kubelet API port if the port is enabled.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这是一个很少得到适当解决的问题的安全领域。默认情况下，Pod网络上的Pod可以访问集群中任何地方的任何Pod，这包括API服务器。这种能力存在是为了允许Pod访问像DNS这样的系统进行服务查找。在主机网络上运行的Pod几乎可以访问任何东西：所有Pod、所有节点和API服务器。如果启用了端口，主机网络上的Pod甚至可以访问kubelet
    API端口。
- en: '*Network policies* are objects that you can define to control network traffic
    between Pods. NetworkPolicy objects allow you to configure access around Pod ingress
    and egress. *Ingress* is traffic that is coming into a Pod, while *egress* is
    network traffic leaving the Pod.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*网络策略*是您可以定义以控制Pod之间网络流量的对象。NetworkPolicy对象允许您配置Pod的进出访问。*进入*是指进入Pod的流量，而*流出*是指离开Pod的网络流量。'
- en: 14.4.1 Network policies
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.1 网络策略
- en: You can create a NetworkPolicy object on any Kubernetes cluster, but you need
    a running security provider such as Calico. Calico is a CNI provider that also
    provides a separate application to implement network policies. If you create a
    network policy without a provider, the policy does nothing. Network policies have
    the following constraints and features. They are
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在任何Kubernetes集群上创建NetworkPolicy对象，但您需要一个正在运行的安全提供程序，如Calico。Calico是一个CNI提供程序，同时也提供了一个单独的应用程序来实现网络策略。如果您在没有提供程序的情况下创建网络策略，该策略将不起作用。网络策略有以下约束和功能。它们是
- en: Applied to Pods
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用到Pods
- en: Matched to specific Pods via label selectors
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过标签选择器与特定Pod匹配
- en: In control of both ingress and egress network traffic
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制进出网络流量
- en: In control of network traffic defined by a CIDR range, a specific namespace,
    or a matched Pod or Pods
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制由CIDR范围、特定命名空间或匹配的Pod或Pods定义的网络流量
- en: Designed to specifically handle TCP, UDP, and SCTP traffic
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专门设计用于处理TCP、UDP和SCTP流量
- en: Capable of handling named ports or specific Pod numbers
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够处理命名端口或特定的Pod编号
- en: 'Let’s try this. To set up a `kind` cluster and install Calico on it, first
    run the following command to create the `kind` cluster, and do not start the default
    CNI. Calico will be installed next:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试这个。为了设置一个`kind`集群并在其上安装Calico，首先运行以下命令来创建`kind`集群，并且不要启动默认的CNI。Calico将在之后安装：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we install the Calico Operator and its custom resources. Use the following
    commands to do this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们安装Calico Operator及其自定义资源。使用以下命令来完成此操作：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can observe the Pod startup. Use the following command:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以观察Pod启动。使用以下命令：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, set up a couple of namespaces, an NGINX server to serve a test web page,
    and a BusyBox container where we can run `wget`. To do this, use the following
    commands:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设置几个命名空间、一个用于提供测试网页的NGINX服务器和一个可以运行`wget`的BusyBox容器。为此，请使用以下命令：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'From the command prompt on the BusyBox container, access the NGINX server installed
    in the `web` namespace. Here’s the command for this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从 BusyBox 容器的命令提示符访问在 `web` 命名空间中安装的 NGINX 服务器。以下是此命令：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, install a network policy that denies all inbound traffic to the NGINX
    Pod. Use the following command:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，安装一个拒绝所有进入 NGINX Pod 的流量的网络策略。使用以下命令：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This command creates a policy so that you can no longer access the NGINX web
    page from the testing Pod. Run the following command on the command line for the
    testing Pod. The command will time out and fail:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建了一个策略，使得您无法从测试 Pod 访问 NGINX 网页。在测试 Pod 的命令行上运行以下命令。命令将超时并失败：
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, open the Pod ingress from the `test-bed` namespace to the `web` namespace.
    Use the following code snippet:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开从 `test-bed` 命名空间到 `web` 命名空间的 Pod 入口。使用以下代码片段：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: On the command line for the testing Pod, enter
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试 Pod 的命令行上输入
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You will notice that the command fails. The reason is that network policies
    match labels, and the `test-bed` namespace is not labeled. The following command
    adds the label:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到命令失败了。原因是网络策略匹配标签，而 `test-bed` 命名空间没有标签。以下命令添加了标签：
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'On the command line for the testing Pod, check that the network policy now
    works. Here’s the command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试 Pod 的命令行上，检查网络策略现在是否生效。以下是命令：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The first recommendation for all firewall configurations is to create a deny-all
    rule. This policy denies all traffic flow within a namespace. Run the following
    command and disable all ingress and egress Pods for the `test-bed` namespace:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有防火墙配置的第一项建议是创建一个拒绝所有规则的规则。此策略拒绝命名空间内的所有流量。运行以下命令并禁用 `test-bed` 命名空间的所有入口和出口
    Pod：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Matches all Pods in a namespace
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 匹配命名空间中的所有 Pod
- en: '❷ Defines the two policy types: ingress and egress'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义了两种策略类型：入口和出口
- en: 'Now, implementing this policy causes some fun side effects. Not only can the
    Pods not talk to anything else (except in their namespace), but now they cannot
    talk to the DNS provider in the `kube-system` namespace. If the Pod does not need
    DNS capability, do not enable it! Let’s apply the following network policy to
    enable egress for DNS:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，实施此策略会产生一些有趣的副作用。不仅 Pod 无法与其他任何东西（除了它们自己的命名空间）通信，而且现在它们无法与 `kube-system`
    命名空间中的 DNS 提供商通信。如果 Pod 不需要 DNS 功能，请不要启用它！让我们应用以下网络策略以启用 DNS 的出口：
- en: '[PRE32]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Matches all Pods in the core Kubernetes namespace
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 匹配核心 Kubernetes 命名空间中的所有 Pod
- en: ❷ Only allows egress
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 只允许出口
- en: ❸ Egress rule that matches a labeled kube-system
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 匹配带有标签的 kube-system 的出口规则
- en: ❹ Only allows UDP over port 53, which is the protocol and port for DNS
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 只允许通过端口 53 的 UDP，这是 DNS 的协议和端口
- en: 'If you run the `wget` command, you will notice that the command still fails.
    We have ingress on the `web` namespace allowed but do not have egress from the
    `test-bed` namespace to the `web` namespace enabled. Run the following command
    to turn on egress to the `web` namespace from the `test-bed` Pod:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行 `wget` 命令，您会注意到命令仍然失败。我们在 `web` 命名空间允许了入口，但没有在 `test-bed` 命名空间到 `web`
    命名空间的出口上启用。运行以下命令以从 `test-bed` Pod 启用到 `web` 命名空间的出口：
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You probably noticed that NetworkPolicy rules can get complex. If you are running
    a cluster where the trust model is *high trust*, implementing network policies
    may *not* benefit your security posture. Using the 80/20 rule, do not start with
    NetworkPolicies if your organization is not updating its images. Yes, network
    policies are complex, and that is partially why using a service mesh in your organization
    may assist your security.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到 NetworkPolicy 规则可能会变得复杂。如果您在一个信任模型为 *高信任* 的集群上运行，实施网络策略可能 *不会* 帮助您的安全态势。使用
    80/20 规则，如果您的组织没有更新其镜像，请不要从 NetworkPolicies 开始。是的，网络策略很复杂，这也是为什么在您的组织中使用服务网格可能有助于安全的原因之一。
- en: Service mesh
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格
- en: 'A *service mesh* is an application that runs on top of a Kubernetes cluster
    and provides various capabilities that often improve observability, monitoring,
    and reliability. Common service meshes include Istio, Linkerd, Consul, and others.
    We mention service meshes in the security chapter because they can assist your
    organization with two key things: mutual TLS and advanced network traffic-flow
    policies. We cover this topic very briefly since there are entire books on this
    subject.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务网格* 是在 Kubernetes 集群之上运行的应用程序，它提供了各种能力，通常可以改善可观察性、监控和可靠性。常见的服务网格包括 Istio、Linkerd、Consul
    以及其他一些。我们在安全章节中提到服务网格，因为它们可以帮助您的组织完成两个关键任务：相互 TLS 和高级网络流量策略。我们对此主题的介绍非常简短，因为关于这个主题有整本书的篇幅。'
- en: 'A service mesh adds a complex layer on top of virtually every application that
    it runs in a cluster, but also provides many good security components. Again,
    use your judgement as to whether adding a service mesh is warranted, but do not
    start with a service mesh on day one. If you want to know whether your cluster
    conforms to the CNCF specification for the NetworkPolicy API, you can run the
    NetworkPolicy test suites using Sonobuoy (which we’ve covered in previous chapters):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格在集群中运行的每个应用程序之上添加了一个复杂的层，但也提供了许多良好的安全组件。再次强调，您需要判断是否需要添加服务网格，但不要从第一天开始就使用服务网格。如果您想知道您的集群是否符合CNCF对NetworkPolicy
    API的规范，您可以使用Sonobuoy（我们在前面的章节中已介绍）运行NetworkPolicy测试套件：
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This outputs a series of table tests that show you exactly how network policies
    are working on your cluster. To learn more about the concepts of NetworkPolicy
    API conformance for CNI providers, check out [http://mng.bz/XW7M](http://mng.bz/XW7M).
    We highly recommend running the NetworkPolicy conformance tests when evaluating
    your CNI provider for compatibility to the Kubernetes network security specifications.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出一系列表格测试，显示您的集群上网络策略的确切工作方式。要了解更多关于CNI提供程序NetworkPolicy API兼容性概念的信息，请查看[http://mng.bz/XW7M](http://mng.bz/XW7M)。我们强烈建议在评估CNI提供程序与Kubernetes网络安全规范的兼容性时运行NetworkPolicy兼容性测试。
- en: 14.4.2 Load balancers
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.2 负载均衡器
- en: One thing to keep in mind is that Kubernetes can create external load balancers
    that expose your applications to the world, and it does so automatically. This
    may seem like common knowledge, but putting the wrong service into a production
    environment can expose a service (such as an administrative user interface) to
    a database. Use tooling during CI (continuous integration) or a tool like the
    Open Policy Agent (OPA) to ensure that external load balancers are not created
    accidentally. Also, use internal load balancers when you can.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Kubernetes可以创建外部负载均衡器，将您的应用程序暴露给世界，并且它是自动完成的。这看起来可能像常识，但将错误的服务放入生产环境可能会将服务（如管理用户界面）暴露给数据库。在CI（持续集成）期间使用工具或像Open
    Policy Agent（OPA）这样的工具来确保不会意外创建外部负载均衡器。此外，当可能时，请使用内部负载均衡器。
- en: 14.4.3 Open Policy Agent (OPA)
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.3 开放策略代理（OPA）
- en: We previously mentioned that operators can help an organization further secure
    a cluster. The OPA, a CNCF project, strives to allow declarative policies that
    are run via the admission controller.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到操作员可以帮助组织进一步保护集群。OPA，一个CNCF项目，致力于允许通过准入控制器运行的声明性策略。
- en: OPA is a lightweight general-purpose policy engine that can be co-located with
    your service. You can integrate OPA as a sidecar, host-level daemon, or library.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: OPA是一个轻量级的通用策略引擎，可以与您的服务一起部署。您可以将OPA集成为一个sidecar、主机级守护进程或库。
- en: Services offload policy decisions to OPA by executing queries. OPA evaluates
    policies and data to produce query results (which are sent back to the client).
    Policies are written in a high-level declarative language and can be loaded into
    OPA via the filesystem or well-defined APIs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 服务通过执行查询将策略决策卸载给OPA。OPA评估策略和数据以生成查询结果（这些结果会发送回客户端）。策略是用高级声明性语言编写的，可以通过文件系统或定义良好的API加载到OPA中。
- en: —Open Policy Agent ([http://mng.bz/RE6O](http://mng.bz/RE6O))
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: —Open Policy Agent ([http://mng.bz/RE6O](http://mng.bz/RE6O))
- en: 'There are two different components that OPA maintains: the OPA admission controller
    and the OPA Gatekeeper. The Gatekeeper does not use a sidecar, utilizes CRDs (custom
    resource definitions), is extensible, and performs audit functionality. The next
    section walks through installing Gatekeeper on a `kind` Kubernetes cluster.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: OPA维护了两个不同的组件：OPA准入控制器和OPA Gatekeeper。Gatekeeper不使用sidecar，使用CRDs（自定义资源定义），可扩展，并执行审计功能。下一节将介绍如何在`kind`
    Kubernetes集群上安装Gatekeeper。
- en: Installing OPA
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 安装OPA
- en: 'First, clean up your cluster running Calico. Then, let’s start another cluster:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，清理运行Calico的集群。然后，让我们启动另一个集群：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, install OPA Gatekeeper using the following command:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令安装OPA Gatekeeper：
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following command prints the names of the Pods installed:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令打印已安装Pod的名称：
- en: '[PRE37]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note You can also use Helm to install OPA Gatekeeper.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您也可以使用Helm安装OPA Gatekeeper。
- en: Gatekeeper CRDs
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Gatekeeper CRDs
- en: 'One of the complexities of OPA is learning a new language (called Rego) to
    write policies. See [http://mng.bz/2jdm](http://mng.bz/2jdm) for more information
    about Rego. With Gatekeeper, you will put policies written in Rego into the supported
    CRDs. You need to create two different CRDs to add a policy:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: OPA 的一个复杂性是学习一种新的语言（称为 Rego）来编写策略。有关 Rego 的更多信息，请参阅 [http://mng.bz/2jdm](http://mng.bz/2jdm)。使用
    Gatekeeper，你将把用 Rego 编写的策略放入支持的 CRD 中。你需要创建两个不同的 CRD 来添加策略：
- en: A constraint template to define the policy and its targets
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个约束模板用于定义策略及其目标
- en: A constraint to enable a constraint template and define how the policy is enabled
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个约束用于启用约束模板并定义如何启用策略
- en: An example of a constraint template and the associated constraint follows. The
    source block contains two CRDs defined in YAML. In this example, the `match` stanza
    supports
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个约束模板及其相关约束的示例。源块包含在 YAML 中定义的两个 CRD。在这个例子中，`match` 段支持
- en: '`kinds`—Defines Kubernetes API objects'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kinds`—定义 Kubernetes API 对象'
- en: '`namespaces`—Specifies a list of namespaces'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespaces`—指定命名空间列表'
- en: '`excludedNamespaces`—Specifies a list of excluded namespaces'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`excludedNamespaces`—指定要排除的命名空间列表'
- en: '`scope`— *, cluster, or namespace'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scope`— *, 集群或命名空间'
- en: '`labelSelector`—Sets the standard Kubernetes label selector'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labelSelector`—设置标准的 Kubernetes 标签选择器'
- en: '`namespaceSelector`—Sets the standard Kubernetes namespace selector'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespaceSelector`—设置标准的 Kubernetes 命名空间选择器'
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Defines EnforceSpecific-ContainerRegistry, a CRD that’s used for the constraint
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了 EnforceSpecific-ContainerRegistry，这是一个用于约束的 CRD
- en: 'Now, let’s take the previous YAML file and save it to two files (one with the
    template and the second with the constraint). On the cluster, install the template
    file first and the constraint second. (For brevity, we do not provide the command.)
    Now we can exercise the policy by running the following commands:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将之前的 YAML 文件保存为两个文件（一个包含模板，另一个包含约束）。在集群上，首先安装模板文件，然后安装约束文件。（为了简洁，我们不提供命令。）现在我们可以通过运行以下命令来测试策略：
- en: '[PRE39]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can check the status of the deployment by running the following command
    (we are expecting that the Pod will not start):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来检查部署的状态（我们预计 Pod 不会启动）：
- en: '[PRE40]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If you execute a `kubectl -n test-ns get Pods`, you will notice that no Pods
    are running. The event logs contain the message that shows Pod creation failing.
    You can view the logs with the following command:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你执行 `kubectl -n test-ns get Pods`，你会注意到没有 Pod 在运行。事件日志中包含显示 Pod 创建失败的消息。你可以使用以下命令查看日志：
- en: '[PRE41]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 14.4.4 Multi-tenancy
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4.4 多租户
- en: 'To categorize multi-tenancy, look at the level of trust that the tenants have
    with one another, and then develop that model. There are three basic buckets or
    security models to categorize as multi-tenancy:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要对多租户进行分类，查看租户之间的信任水平，然后开发该模型。有三个基本类别或安全模型用于将多租户分类：
- en: '*High trust (same company)*—Different departments in the same company are running
    workloads on the same cluster.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高信任（同一家公司）*—同一家公司的不同部门在同一个集群上运行工作负载。'
- en: '*Medium to low trust (different companies)*—External customers are running
    applications on your cluster in different namespaces.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*中低信任（不同公司）*—外部客户在不同的命名空间中运行应用程序。'
- en: '*Zero trust (data governed by law)*—Different applications are running data
    that is governed by laws, where allowing access between different data stores
    can cause legal action against your company.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*零信任（受法律管辖的数据）*—不同的应用程序运行受法律管辖的数据，允许不同数据存储之间的访问可能导致公司面临法律诉讼。'
- en: 'The Kubernetes community has worked on solving these use cases for many years.
    Jessie Frazelle sums it up nicely in her blog post entitled “Hard Multi-Tenancy
    in Kubernetes”:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 社区已经为解决这些用例工作了多年。Jessie Frazelle 在她的博客文章“Kubernetes 中的硬多租户”中很好地总结了这些内容：
- en: The models for multi-tenancy have been discussed at length in the community’s
    multi-tenancy working group. . . . There have also been some proposals offered
    to solve each model. The current model of tenancy in Kubernetes assumes the cluster
    is the security boundary. You can build a SaaS on top of Kubernetes but you will
    need to bring your own trusted API and not just use the Kubernetes API. Of course,
    with that comes a lot of considerations you must also think about when building
    your cluster securely for a SaaS even.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户模型在社区的多租户工作组中已经进行了广泛的讨论。……还提出了一些提案来解决每个模型。Kubernetes 中当前的租户模型假设集群是安全边界。你可以在
    Kubernetes 之上构建 SaaS，但你将需要带来自己的可信 API，而不仅仅是使用 Kubernetes API。当然，这伴随着在为 SaaS 安全构建集群时你必须考虑的许多考虑因素。
- en: —Jessie Frazelle, [http://mng.bz/1jdn](http://mng.bz/1jdn)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ——杰西·弗拉泽尔，[http://mng.bz/1jdn](http://mng.bz/1jdn)
- en: The Kubernetes API was not built with the concept of having multiple isolated
    customers within the same cluster. Docker Engine and other container runtimes
    also have problems with running malicious or untrusted workloads. Software components
    like gVisor have made headway in properly sandboxing containers, but at the time
    of writing this book, we are not at a place where you can run a completely untrusted
    container.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API 并没有考虑到在同一个集群内存在多个隔离的客户的概念。Docker Engine 和其他容器运行时在运行恶意或不信任的工作负载时也存在问题。像
    gVisor 这样的软件组件在正确沙箱化容器方面取得了进展，但在撰写本书时，我们还没有达到可以运行完全不受信任的容器的地步。
- en: 'So where are we? A security person would say it depends on your trust and security
    model. We previously listed three security models: high trust (same company),
    low trust to no trust (different companies), and zero trust (data governed by
    law). Kubernetes can support high trust multi-tenancy and, depending on the model,
    can support trust models in between high and low trust models. When you have zero
    trust or low trust with and between tenants, you need to use separate clusters
    for each client. Some companies run hundreds of clusters so that each small group
    of applications gets its own cluster, but, admittedly, that is a lot of clusters
    to manage.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们现在在哪里？安全人员会说这取决于你的信任和安全模型。我们之前列出了三种安全模型：高度信任（同一公司）、低信任到无信任（不同公司）和零信任（数据受法律管辖）。Kubernetes
    可以支持高度信任的多租户，并且根据模型，可以在高度信任和低信任模型之间支持信任模型。当你与租户之间或租户之间有零信任或低信任时，你需要为每个客户端使用单独的集群。一些公司运行数百个集群，以便每个小型应用程序组都能获得自己的集群，但诚然，这需要管理很多集群。
- en: Even if the clients are part of the same company, it may be necessary to isolate
    Pods on specific nodes due to data sensitivity. Through RBAC, namespaces, network
    policies, and node isolation, it’s possible to gain a decent level of isolation.
    Admittedly, there is a risk of hosting a workload that different companies run
    to use the same Kubernetes cluster. The support for multi-tenancy will grow over
    time.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 即使客户属于同一公司，由于数据敏感性，可能还需要在特定节点上隔离 Pods。通过 RBAC、命名空间、网络策略和节点隔离，可以获得相当程度的隔离。诚然，存在不同公司运行的工作负载托管在同一个
    Kubernetes 集群中的风险。多租户的支持将随着时间的推移而增长。
- en: Note Multi-tenancy also applies to running other environments, like development
    or test in a production cluster. You can, however, introduce bad actor code into
    a cluster by intermingling environments.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：多租户也适用于在生产集群中运行其他环境，如开发或测试。然而，通过混合环境，你可能会将不良行为者的代码引入集群中。
- en: 'There are two main challenges using a single Kubernetes to host multiple customers:
    the API server and node security. After establishing authentication, authorization,
    and RBAC, why is there a problem with the API server and multiple tenants? One
    of the problems is with the URI layout of the API server. A common pattern for
    having multiple tenants using the same API server is to have the user ID, project
    ID, or some unique ID starting the URI.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个 Kubernetes 来托管多个客户主要面临两个挑战：API 服务器和节点安全。在建立身份验证、授权和 RBAC 之后，为什么 API 服务器和多个租户之间会出现问题？其中一个问题是
    API 服务器 URI 布局的问题。对于多个租户使用相同 API 服务器的常见模式是让用户 ID、项目 ID 或某些唯一 ID 作为 URI 的开头。
- en: Having a URI that starts with a unique ID allows a tenant to make a call to
    get *all* namespaces. Because Kubernetes does not have this isolation, you need
    to run `kubectl get namespaces` to get all the namespaces in a cluster. You’ll
    also need an API layer on top of the Kubernetes API to provide this form of isolation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 具有以唯一ID开始的URI允许租户调用以获取*所有*命名空间。因为Kubernetes没有这种隔离，你需要运行`kubectl get namespaces`来获取集群中的所有命名空间。你还需要在Kubernetes
    API之上运行一个API层，以提供这种形式的隔离。
- en: Another pattern for allowing multi-tenants is the capability of nesting resources,
    and the basic resource boundary in Kubernetes is namespaces. Kubernetes namespaces
    do not allow for nesting. Many resources are cross-namespace, including the default
    service account token. Often, tenants want to have fine-grain RBAC capability
    themselves, and giving a user permissions to create RBAC objects within Kubernetes
    can give the user capabilities beyond their shared tenancy.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 允许多租户的另一种模式是资源嵌套的能力，以及Kubernetes中的基本资源边界是命名空间。Kubernetes命名空间不允许嵌套。许多资源是跨命名空间的，包括默认的服务账户令牌。通常，租户希望拥有细粒度的RBAC能力，而授予用户在Kubernetes中创建RBAC对象权限可能会赋予用户超出其共享租户的能力。
- en: 'Regarding node security, the challenge lies within. If you have multiple tenants
    on the same Kubernetes cluster, remember that they all share the following items
    (this is just a short list):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 关于节点安全，挑战在于内部。如果你在同一Kubernetes集群上有多个租户，请记住他们共享以下项目（这只是一个简短的列表）：
- en: The control plane and the API server
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面和API服务器
- en: Add-ons like the DNS server, logging, or TLS certificate generation
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加DNS服务器、日志记录或TLS证书生成等附加组件
- en: Custom resource definitions (CRDs)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义资源定义（CRDs）
- en: Networks
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络
- en: Host resources
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机资源
- en: Overview of trusted multi-tenancy
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 信任多租户概述
- en: 'Many companies want multi-tenancy to reduce costs and management overhead.
    There is value in not running three clusters: one each for development, test,
    and production environments. Simply run a single Kubernetes cluster for all three
    environments. Also, some companies do not want to have separate clusters for different
    products and/or software departments. Again, this is a business and security decision,
    and the organizations we work with usually have budgets and a constraint on human
    resources.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司希望多租户可以降低成本和管理开销。不运行三个集群：一个用于开发、一个用于测试、一个用于生产环境是有价值的。只需运行一个Kubernetes集群用于所有三个环境。此外，一些公司不希望为不同的产品或软件部门拥有单独的集群。这同样是一个业务和安全决策，我们合作的组织通常有预算和人力资源的限制。
- en: 'We are not going to give you step-by-step instructions on how to do multi-tenancy.
    We are simply providing guidelines on what steps you will need to implement. These
    steps will change over time and vary between organizations with different security
    models:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会给你提供如何进行多租户的逐步指导。我们只是提供你需要实施的步骤的指南。这些步骤会随着时间的推移而变化，并且在不同安全模型的组织之间会有所不同：
- en: Write down and design a security model. It may seem obvious, but we have seen
    organizations that do not use a security model. A security model needs to include
    different user roles, including cluster administrators and namespace administrators,
    and one or more tenant roles. A standardized naming convention for all of the
    API objects, users, and other components your organization creates is also critical.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录并设计一个安全模型。这可能看起来很明显，但我们看到一些组织没有使用安全模型。安全模型需要包括不同的用户角色，包括集群管理员和命名空间管理员，以及一个或多个租户角色。为组织创建的所有API对象、用户和其他组件的标准命名约定也是至关重要的。
- en: 'Utilize various API objects:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用各种API对象：
- en: Namespaces
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名空间
- en: NetworkingPolicies
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略
- en: ResourceQuotas
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResourceQuotas
- en: ServiceAccounts and RBACRules
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务账户和RBAC规则
- en: Using tools like a service mesh with features like mutual TLS and network policy
    management can provide another level of security. Using a service mesh does add
    a significant layer of complexity, so only use it when needed.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用具有相互TLS和网络策略管理等功能的服务网格可以提供另一层安全性。使用服务网格确实会增加显著的复杂性，因此只有在需要时才使用它。
- en: Consider using an OPA to assist with applying policy-based controls to the Kubernetes
    cluster.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑使用OPA来帮助将基于策略的控制应用于Kubernetes集群。
- en: tip If you are going to combine multiple environments in a single cluster, there
    are not only security concerns, but also challenges with testing Kubernetes upgrades.
    It is best to test upgrades first on another cluster.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你打算在单个集群中结合多个环境，不仅存在安全问题，还有测试Kubernetes升级的挑战。最好先在另一个集群上测试升级。
- en: 14.5 Kubernetes tips
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.5 Kubernetes技巧
- en: 'Here is a short list of various configurations and setup requirements:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一份简短的各种配置和设置要求的列表：
- en: Have a private API server endpoint, and if you can, do not expose your API server
    to the internet.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有私有API服务器端点，并且如果可能的话，不要将你的API服务器暴露给互联网。
- en: Use RBAC.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RBAC。
- en: Use network policies.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络策略。
- en: Do not enable username and password authorization on the API server.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要在API服务器上启用用户名和密码授权。
- en: Use specific users when creating Pods, and don’t use the default admin accounts.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建Pod时使用特定用户，不要使用默认管理员账户。
- en: Rarely allow Pods to run on the host network.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很少允许Pod在主机网络上运行。
- en: Use `serviceAccountName` if the Pod needs to access the API server; otherwise,
    set `automountServiceAccountToken` to false.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Pod需要访问API服务器，请使用`serviceAccountName`；否则，将`automountServiceAccountToken`设置为false。
- en: Use resource quotas on namespaces and define limits in all Pods.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在命名空间上使用资源配额，并在所有Pod中定义限制。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Node security relies on TLS certificates to secure communication between nodes
    and the control plane.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点安全性依赖于TLS证书来保护节点和控制平面之间的通信。
- en: Using immutable OSs can further harden nodes.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不可变操作系统可以进一步增强节点安全性。
- en: Resource limits can prevent resource-level attacks.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源限制可以防止资源级别的攻击。
- en: Use the Pod network, unless you have to use the host network. The host network
    allows a Pod to talk to the node OS.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Pod网络，除非你不得不使用主机网络。主机网络允许Pod与节点操作系统通信。
- en: RBAC is key to securing an API server. It is non-trivial, but necessary.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBAC是保护API服务器的关键。虽然非同寻常，但这是必要的。
- en: The IAM service accounts allow for the proper isolation of Pod permissions.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IAM服务账户允许正确隔离Pod权限。
- en: Network policies are key to isolating network traffic. Otherwise, everything
    can talk to everything else.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略是隔离网络流量的关键。否则，一切都可以与一切通信。
- en: An Open Policy Agent (OPA) allows a user to write security policies and enforces
    those policies on a Kubernetes cluster.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open Policy Agent (OPA) 允许用户编写安全策略，并在Kubernetes集群上强制执行这些策略。
- en: Kubernetes was not built initially with zero trust multi-tenancy in mind. You’ll
    find forms of mult-tenancy, but they come with tradeoffs.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes最初并非以零信任多租户模式构建。你可以找到多租户的形式，但它们伴随着权衡。
