- en: 12 Advanced actor-critic methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 高级演员-评论家方法
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will learn about more advanced deep reinforcement learning methods, which
    are, to this day, the state-of-the-art algorithmic advancements in deep reinforcement
    learning.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解更多高级深度强化学习方法，这些方法至今仍然是深度强化学习中的最先进算法进步。
- en: You will learn about solving a variety of deep reinforcement learning problems,
    from problems with continuous action spaces, to problem with high-dimensional
    action spaces.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习解决各种深度强化学习问题，从具有连续动作空间的问题到具有高维动作空间的问题。
- en: You will build state-of-the-art actor-critic methods from scratch and open the
    door to understanding more advanced concepts related to artificial general intelligence.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将从头开始构建最先进的演员-评论家方法，并打开理解与人工通用智能相关的高级概念的大门。
- en: Criticism may not be agreeable, but it is necessary. It fulfills the same function
    as pain in the human body. It calls attention to an unhealthy state of things.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 批评可能并不令人愉快，但它是必要的。它起着与人体疼痛相同的作用。它引起了人们对事物不健康状态的注意。
- en: — Winston Churchill British politician, army officer, writer, and Prime Minister
    of the United Kingdom
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: — 温斯顿·丘吉尔 英国政治家、军官、作家和英国首相
- en: In the last chapter, you learned about a different, more direct, technique for
    solving deep reinforcement learning problems. You first were introduced to policy-gradient
    methods in which agents learn policies by approximating them directly. In pure
    policy-gradient methods, we don’t use value functions as a proxy for finding policies,
    and in fact, we don’t use value functions at all. We instead learn stochastic
    policies directly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解了一种不同的、更直接的技术来解决深度强化学习问题。你首先被介绍到策略梯度方法，其中智能体通过直接近似来学习策略。在纯策略梯度方法中，我们不使用价值函数作为寻找策略的代理，实际上，我们根本不使用价值函数。我们而是直接学习随机策略。
- en: However, you quickly noticed that value functions can still play an important
    role and make policy-gradient methods better. And so you were introduced to actor-critic
    methods. In these methods, the agent learns both a policy and a value function.
    With this approach, you could use the strengths of one function approximation
    to mitigate the weaknesses of the other approximation. For instance, learning
    policy can be more straightforward in certain environments than learning a sufficiently
    accurate value function, because the relationships in action space may be more
    tightly related than the relationships of values. Still, even though knowing the
    values of states precisely can be more complicated, a rough approximation can
    be useful for reducing the variance of the policy-gradient objective. As you explored
    in the previous chapter, learning a value function and using it as a baseline
    or for calculating advantages can considerably reduce the variance of the targets
    used for policy-gradient updates. Moreover, reducing the variance often leads
    to faster learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你很快注意到价值函数仍然可以发挥重要作用，并使策略梯度方法得到改进。因此，你被引入了演员-评论家方法。在这些方法中，智能体学习策略和价值函数。通过这种方法，你可以利用一个函数近似的优点来减轻另一个近似的缺点。例如，在某些环境中，学习策略可能比学习足够准确的价值函数更直接，因为动作空间中的关系可能比价值的关系更紧密。尽管如此，即使精确地知道状态的价值可能更复杂，但粗略的近似对于减少策略梯度目标方差是有用的。正如你在上一章中所探索的，学习价值函数并将其用作基线或用于计算优势可以显著减少用于策略梯度更新的目标方差。此外，减少方差通常会导致学习速度更快。
- en: 'However, in the previous chapter, we focused on using the value function as
    a critic for updating a stochastic policy. We used different targets for learning
    the value function and parallelized the workflows in a few different ways. However,
    algorithms used the learned value function in the same general way to train the
    policy, and the policy learned had the same properties, because it was a stochastic
    policy. We scratched the surface of using a learned policy and value function.
    In this chapter, we go deeper into the paradigm of actor-critic methods and train
    them in four different challenging environments: pendulum, hopper, cheetah, and
    lunar lander. As you soon see, in addition to being more challenging environments,
    most of these have a continuous action space, which we face for the first time,
    and it’ll require using unique polices models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在上一章中，我们专注于使用价值函数作为更新随机策略的评论家。我们为学习价值函数使用了不同的目标，并以几种不同的方式并行化了工作流程。然而，算法以相同的一般方式使用学习到的价值函数来训练策略，并且学习到的策略具有相同的属性，因为它是一个随机策略。我们只是触及了使用学习策略和价值函数的表面。在本章中，我们将更深入地探讨演员-评论家方法的范式，并在四个不同的具有挑战性的环境中进行训练：摆锤、跳跃者、猎豹和月球着陆器。正如你很快就会看到的，除了是更具挑战性的环境之外，大多数这些环境都具有连续的动作空间，这是我们第一次面对的，并且需要使用独特的策略模型。
- en: To solve these environments, we first explore methods that learn deterministic
    policies; that is, policies that, when presented with the same state, return the
    same action, the action that’s believed to be optimal. We also study a collection
    of improvements that make deterministic policy-gradient algorithms one of the
    state-of-the-art approaches to date for solving deep reinforcement learning problems.
    We then explore an actor-critic method that, instead of using the entropy in the
    loss function, directly uses the entropy in the value function equation. In other
    words, it maximizes the return along with the long-term entropy of the policy.
    Finally, we close with an algorithm that allows for more stable policy improvement
    steps by restraining the updates to the policy to small changes. Small changes
    in policies make policy-gradient methods show steady and often monotonic improvements
    in performance, allowing for state-of-the-art performance in several DRL benchmarks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些环境，我们首先探索了学习确定性策略的方法；也就是说，当呈现相同的状态时，返回相同动作的策略，即被认为是最优的动作。我们还研究了一系列改进，使确定性策略梯度算法成为迄今为止解决深度强化学习问题的最先进方法之一。然后，我们探索了一种演员-评论家方法，它不是使用损失函数中的熵，而是直接使用价值函数方程中的熵。换句话说，它最大化回报以及策略的长期熵。最后，我们以一个算法结束，该算法通过将策略的更新限制为小的变化，从而允许更稳定的策略改进步骤。策略中的小变化使得策略梯度方法在性能上表现出稳定且通常是单调的改进，从而在几个DRL基准测试中实现了最先进的性能。
- en: 'DDPG: Approximating a deterministic policy'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPG：逼近确定性策略
- en: 'In this section, we explore an algorithm called deep deterministic policy gradient
    (DDPG). DDPG can be seen as an approximate DQN, or better yet, a DQN for continuous
    action spaces. DDPG uses many of the same techniques found in DQN: it uses a replay
    buffer to train an action-value function in an off-policy manner, and target networks
    to stabilize training. However, DDPG also trains a policy that approximates the
    optimal action. Because of this, DDPG is a deterministic policy-gradient method
    restricted to continuous action spaces.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了一种名为深度确定性策略梯度（DDPG）的算法。DDPG可以被视为一个近似的DQN，或者更好的说法，是一个适用于连续动作空间的DQN。DDPG使用了DQN中发现的许多相同技术：它使用重放缓冲区以离线方式训练动作值函数，并使用目标网络来稳定训练。然而，DDPG还训练了一个逼近最优动作的策略。正因为如此，DDPG是一个限制在连续动作空间中的确定性策略梯度方法。
- en: DDPG uses many tricks from DQN
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG借鉴了DQN的许多技巧
- en: 'Start by visualizing DDPG as an algorithm with the same architecture as DQN.
    The training process is similar: the agent collects experiences in an online manner
    and stores these online experience samples into a replay buffer. On every step,
    the agent pulls out a mini-batch from the replay buffer that is commonly sampled
    uniformly at random. The agent then uses this mini-batch to calculate a bootstrapped
    *TD* target and train a Q-function.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将DDPG想象成一个与DQN具有相同架构的算法。训练过程也是类似的：智能体以在线方式收集经验，并将这些在线经验样本存储到重放缓冲区中。在每一步，智能体从重放缓冲区中抽取一个迷你批次，通常是随机均匀抽取的。然后，智能体使用这个迷你批次来计算自举的*TD*目标并训练一个Q函数。
- en: The main difference between DQN and DDPG is that while DQN uses the target Q-function
    for getting the greedy action using an argmax, DDPG uses a target deterministic
    policy function that is trained to approximate that greedy action. Instead of
    using the argmax of the Q-function of the next state to get the greedy action
    as we do in DQN, in DDPG, we directly approximate the best action in the next
    state using a policy function. Then, in both, we use that action with the Q-function
    to get the max value.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 和 DDPG 之间的主要区别在于，虽然 DQN 使用目标 Q 函数通过 argmax 获取贪婪动作，而 DDPG 使用一个目标确定性策略函数，该函数被训练来近似贪婪动作。与我们在
    DQN 中使用下一个状态的 Q 函数的 argmax 获取贪婪动作不同，在 DDPG 中，我们直接使用策略函数来近似下一个状态中的最佳动作。然后，在两者中，我们都使用该动作与
    Q 函数结合来获取最大值。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathDQN vs. DDPG value function
    objectives |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示 MathDQN 与 DDPG 的价值函数目标'
- en: '|  | ![](../Images/12_00_Sidebar01.png) |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar01.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonDDPG’s Q-function network
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说 PythonDDPG 的 Q 函数网络'
- en: '|  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① This is the Q-function network used in DDPG.② Here we start the architecture
    as usual.③ Here we have the first exception. We increase the dimension of the
    first hidden layer by the output dimension.④ Notice the output of the network
    is a single node representing the value of the state-action pair.⑤ The forward
    pass starts as expected.⑥ But we concatenate the action to the states right on
    the first hidden layer.⑦ Then, continue as expected.⑧ Finally, return the output.
    |
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是 DDPG 中使用的 Q 函数网络。② 我们像往常一样开始构建架构。③ 这里有一个例外。我们将第一隐藏层的维度增加到输出维度。④ 注意网络的输出是一个节点，表示状态-动作对的价值。⑤
    前向传播开始如预期。⑥ 但是，我们在第一隐藏层直接将动作连接到状态。⑦ 然后，继续如预期。⑧ 最后，返回输出。
- en: Learning a deterministic policy
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习确定性策略
- en: Now, the one thing we need to add to this algorithm to make it work is a policy
    network. We want to train a network that can give us the optimal action in a given
    state. The network must be differentiable with respect to the action. Therefore,
    the action must be continuous to make for efficient gradient-based learning. The
    objective is simple; we can use the expected Q-value using the policy network,
    mu. That is, the agent tries to find the action that maximizes this value. Notice
    that in practice, we use minimization techniques, and therefore minimize the negative
    of this objective.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要添加到这个算法中的一个东西是策略网络。我们希望训练一个网络，能够在给定状态下给出最佳动作。该网络必须相对于动作是可微分的。因此，动作必须是连续的，以便进行高效的基于梯度的学习。目标是简单的；我们可以使用策略网络
    mu 的期望 Q 值。也就是说，智能体试图找到最大化这个值的动作。注意，在实践中，我们使用最小化技术，因此最小化这个目标的负值。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathDDPG’s deterministic policy
    objective |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示 MathDDPG 的确定性策略目标'
- en: '|  | ![](../Images/12_00_Sidebar03.png) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar03.png) |'
- en: Also notice that, in this case, we don’t use target networks, but the online
    networks for both the policy, which is the action selection portion, and the value
    function (the action evaluation portion). Additionally, given that we need to
    sample a mini-batch of states for training the value function, we can use these
    same states for training the policy network.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在这种情况下，我们不需要使用目标网络，而是使用在线网络，即策略网络（动作选择部分）和价值函数（动作评估部分）。此外，鉴于我们需要为训练价值函数采样状态的小批量，我们可以使用这些相同的状态来训练策略网络。
- en: '| 0001 | A Bit Of HistoryIntroduction of the DDPG algorithm |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍 DDPG 算法'
- en: '|  | DDPG was introduced in 2015 in a paper titled “Continuous control with
    deep reinforcement learning.” The paper was authored by Timothy Lillicrap (et
    al.) while he was working at Google DeepMind as a research scientist. Since 2016,
    Tim has been working as a Staff Research Scientist at Google DeepMind and as an
    Adjunct Professor at University College London.Tim has contributed to several
    other DeepMind papers such as the A3C algorithm, AlphaGo, AlphaZero, Q-Prop, and
    Starcraft II, to name a few. One of the most interesting facts is that Tim has
    a background in cognitive science and systems neuroscience, not a traditional
    computer science path into deep reinforcement learning. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | DDPG是在2015年一篇题为“使用深度强化学习的连续控制”的论文中提出的。这篇论文由当时在谷歌DeepMind担任研究科学家的蒂莫西·利利克拉普（Timothy
    Lillicrap）等人撰写。自2016年以来，蒂姆一直担任谷歌DeepMind的高级研究科学家和伦敦大学学院的客座教授。蒂姆为DeepMind的几篇其他论文做出了贡献，例如A3C算法、AlphaGo、AlphaZero、Q-Prop和星际争霸II等。最有趣的事实之一是，蒂姆在认知科学和系统神经科学方面有背景，而不是传统计算机科学路径进入深度强化学习。
    |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonDDPG’s deterministic policy
    network |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonDDPG的确定性策略网络 |'
- en: '|  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '① This is the policy network used in DDPG: fully connected deterministic policy.②
    Notice the activation of the output layer is different this time. We use the tanh
    activation function to squash the output to (-1, 1).③ We need to get the minimum
    and maximum values of the actions, so that we can rescale the network’s output
    (-1, 1) to the expected range.④ The architecture is as expected: states in, actions
    out.⑤ The forward pass is also straightforward.⑥ Input⑦ Hidden⑧ Output⑨ Notice,
    however, that we activate the output using the output activation function.⑩ Also
    important is that we rescale the action from the -1 to 1 range to the range specific
    to the environment. The rescale_fn isn’t shown here, but you can go to the Notebook
    for details. |'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是DDPG中使用的策略网络：全连接确定性策略。② 注意这次输出层的激活函数不同。我们使用tanh激活函数将输出压缩到(-1, 1)范围内。③ 我们需要获取动作的最小值和最大值，以便将网络的输出(-1,
    1)重新缩放到预期的范围。④ 架构正如预期：输入状态，输出动作。⑤ 前向传播也很直接。⑥ 输入⑦ 隐藏⑧ 输出⑨ 注意，然而，我们使用输出激活函数激活输出。⑩
    同样重要的是，我们将动作从-1到1的范围重新缩放到特定于环境的范围。rescale_fn函数在此处未显示，但你可以去笔记本中查看详细信息。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonDDPG’s model-optimization
    step |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonDDPG的模型优化步骤 |'
- en: '|  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① The optimize_model function takes in a mini-batch of experiences.② With it,
    we calculate the targets using the predicted max value of the next state, coming
    from the actions according to the policy and the values according to the Q-function.③
    We then get the predictions, and calculate the error and the loss. Notice where
    we use the target and online networks.④ The optimization step is like all other
    networks.⑤ Next, we get the actions as predicted by the online policy for the
    states in the mini-batch, then use those actions to get the value estimates using
    the online value network.⑥ Next, we get the policy loss.⑦ Finally, we zero the
    optimizer, do the backward pass on the loss, clip the gradients, and step the
    optimizer. |
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ① optimize_model函数接收一个经验的小批量。② 使用它，我们根据策略预测的下一个状态的最大值、根据策略的动作和根据Q函数的值来计算目标。③
    然后我们获取预测值，计算误差和损失。注意我们使用目标网络和在线网络的位置。④ 优化步骤与其他网络类似。⑤ 接下来，我们获取在线策略预测的迷你批量中的状态的动作，然后使用这些动作通过在线价值网络获取价值估计。⑥
    接下来，我们获取策略损失。⑦ 最后，我们将优化器置零，对损失进行反向传播，裁剪梯度，并更新优化器。|
- en: Exploration with deterministic policies
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '| 探索确定性策略 |'
- en: In DDPG, we train deterministic greedy policies. In a perfect world, this type
    of policy takes in a state and returns the optimal action for that state. But,
    in an untrained policy, the actions returned won’t be accurate enough, yet still
    deterministic. As mentioned before, agents need to balance exploiting knowledge
    with exploring. But again, since the DDPG agent learns a deterministic policy,
    it won’t explore on-policy. Imagine the agent is stubborn and always selects the
    same actions. To deal with this issue, we must explore off-policy. And so in DDPG,
    we inject Gaussian noise into the actions selected by the policy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDPG中，我们训练确定性贪婪策略。在一个理想的世界里，这种策略接受一个状态并返回该状态的最优动作。但是，在一个未经训练的策略中，返回的动作不会足够准确，但仍然是确定的。如前所述，智能体需要平衡利用知识和探索。但是，由于DDPG智能体学习的是确定性策略，它不会在策略上进行探索。想象一下，智能体很固执，总是选择相同的动作。为了解决这个问题，我们必须进行离策略探索。因此，在DDPG中，我们在策略选择的动作中注入高斯噪声。
- en: You’ve learned about exploration in multiple DRL agents. In NFQ, DQN, and so
    on, we use exploration strategies based on Q-values. We get the values of actions
    in a given state using the learned Q-function and explore based on those values.
    In REINFORCE, VPG, and so on, we use stochastic policies, and therefore, exploration
    is on-policy. That is, exploration is taken care of by the policy itself because
    it’s stochastic; it has randomness. In DDPG, the agent explores by adding external
    noise to actions, using off-policy exploration strategies.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了多个DRL智能体中的探索。在NFQ、DQN等中，我们使用基于Q值的探索策略。我们使用学习的Q函数来获取给定状态的动作值，并根据这些值进行探索。在REINFORCE、VPG等中，我们使用随机策略，因此探索是在策略上的。也就是说，探索由策略本身处理，因为它具有随机性；它有随机性。在DDPG中，智能体通过向动作添加外部噪声来进行探索，使用离策略探索策略。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonExploration in deterministic
    policy gradients |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python确定性策略梯度探索 |'
- en: '|  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① This is the select_action function of the strategy.② To maximize exploration,
    we set the noise scale to the maximum action.③ Otherwise, we scale the noise down.④
    We get the greedy action straight from the network.⑤ Next, we get the Gaussian
    noise for the action using the scale and 0 mean.⑥ Add the noise to the action,
    and clip it to be in range.⑦ Next, we update the noise ratio schedule. This could
    be constant, or linear, exponential, and so on. |
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是策略的select_action函数。② 为了最大化探索，我们将噪声尺度设置为最大动作。③ 否则，我们将噪声尺度降低。④ 我们直接从网络中获取贪婪动作。⑤
    接下来，我们使用尺度和0均值来获取动作的高斯噪声。⑥ 将噪声添加到动作中，并将其剪辑到范围内。⑦ 接下来，我们更新噪声比例计划。这可以是常数、线性、指数等。
    |
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe pendulum environment
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 一个具体例子摆锤环境 |'
- en: '|  | The Pendulum-v0 environment consists of an inverted pendulum that the
    agent needs to swing up, so it stays upright with the least effort possible. The
    state-space is a vector of three variables (cos(theta), sin(theta), theta dot)
    indicating the cosine of the angle of the rod, the sine, and the angular speed.The
    action space is a single continuous variable from –2 to 2, indicating the joint
    effort. The joint is that black dot at the bottom of the rod. The action is the
    effort either clockwise or counterclockwise.The reward function is an equation
    based on angle, speed, and effort. The goal is to remain perfectly balanced upright
    with no effort. In such an ideal time step, the agent receives 0 rewards, the
    best it can do. The highest cost (lowest reward) the agent can get is approximately
    –16 reward. The precise equation is *–(theta^2 + 0.1*theta_dt^2 + 0.001*action^2)*.![](../Images/12_00_Sidebar08_pendulum.png)This
    is a continuing task, so there’s no terminal state. However, the environment times
    out after 200 steps, which serves the same purpose. The environment is considered
    unsolved, which means there’s no target return. However, –150 is a reasonable
    threshold to hit. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | Pendulum-v0环境包含一个倒立摆，智能体需要将其摆动起来，使其尽可能少地消耗力气保持直立。状态空间是一个包含三个变量的向量（cos(theta)，sin(theta)，theta
    dot），表示杆的夹角余弦值、正弦值和角速度。动作空间是一个从-2到2的单个连续变量，表示关节力。这个关节就是杆底部的那个黑点。动作是顺时针或逆时针的力。奖励函数是基于角度、速度和力度的方程。目标是完美平衡地直立，不消耗任何力气。在这样的理想时间步长中，智能体获得0奖励，这是它能做到的最好的。智能体可能获得的最高成本（最低奖励）大约是-16奖励。精确的方程是*–(theta^2
    + 0.1*theta_dt^2 + 0.001*action^2)*。![图片](../Images/12_00_Sidebar08_pendulum.png)这是一个持续任务，因此没有终止状态。然而，环境在200步后超时，这起到了相同的作用。环境被认为是未解决的，这意味着没有目标回报。然而，-150是一个合理的阈值。'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpDDPG in the pendulum environment
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Tally.png) | 总结DDPG在摆动环境中的应用'
- en: '|  | ![](../Images/12_00_Sidebar09.png) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/12_00_Sidebar09.png) |'
- en: 'TD3: State-of-the-art improvements over DDPG'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TD3：在DDPG之上的最新改进
- en: DDPG has been one of the state-of-the-art deep reinforcement learning methods
    for control for several years. However, there have been improvements proposed
    that make a big difference in performance. In this section, we discuss a collection
    of improvements that together form a new algorithm called *twin-delayed DDPG*
    (TD3). *TD*3 introduces three main changes to the main DDPG algorithm. First,
    it adds a double learning technique, similar to what you learned in double Q-learning
    and DDQN, but this time with a unique “twin” network architecture. Second, it
    adds noise, not only to the action passed into the environment but also to the
    target actions, making the policy network more robust to approximation error.
    And, third, it delays updates to the policy network, its target network, and the
    twin target network, so that the twin network updates more frequently.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG已经成为了控制领域几年来的最先进的深度强化学习方法之一。然而，已经提出了改进，这些改进在性能上产生了重大影响。在本节中，我们讨论了一系列改进，这些改进共同形成了一个新的算法，称为*twin-delayed
    DDPG*（TD3）。*TD*3对主要的DDPG算法引入了三个主要变化。首先，它增加了一种双重学习技术，类似于你在双重Q学习和DDQN中学到的，但这次有一个独特的“双胞胎”网络架构。其次，它向动作和环境中的目标动作添加了噪声，这使得策略网络对近似误差更加鲁棒。第三，它延迟了对策略网络、目标网络和双胞胎目标网络的更新，从而使双胞胎网络更新得更频繁。
- en: Double learning in DDPG
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG中的双重学习
- en: In *TD*3, we use a particular kind of Q-function network with two separate streams
    that end on two separate estimates of the state-action pair in question. For the
    most part, these two streams are totally independent, so one can think about them
    as two separate networks. However, it’d make sense to share feature layers if
    the environment was image-based. That way CNN would extract common features and
    potentially learn faster. Nevertheless, sharing layers is also usually harder
    to train, so this is something you’d have to experiment with and decide by yourself.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在*TD*3中，我们使用了一种特殊的Q函数网络，它有两个独立的流，分别结束于对所讨论的状态-动作对的两个不同估计。在大多数情况下，这两个流是完全独立的，所以可以认为它们是两个独立的网络。然而，如果环境是基于图像的，那么共享特征层是有意义的。这样，CNN可以提取共同的特征，并可能更快地学习。尽管如此，共享层通常更难训练，所以这需要你自己实验和决定。
- en: In the following implementation, the two streams are completely separate, and
    the only thing being shared between these two networks is the optimizer. As you
    see in the twin network loss function, we add up the losses for each of the networks
    and optimize both networks on that joint loss.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下实现中，两个流是完全独立的，这两个网络之间唯一共享的是优化器。正如你在双网络损失函数中看到的，我们将每个网络的损失加起来，并在联合损失上优化这两个网络。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathTwin target in TD3 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示给我 TD3 中的双目标 |'
- en: '|  | ![](../Images/12_00_Sidebar10.png) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏10.png](../Images/12_00_Sidebar10.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonTD3’s twin Q-network 1/2
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说 PythonTD3 的双 Q 网络 1/2 |'
- en: '|  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① This is the fully connected Twin Q-value network. This is what *TD*3 uses
    to approximate the Q-values, with the twin streams.② Notice we have two input
    layers. Again, these streams are really two separate networks.③ Next, we create
    hidden layers for each of the streams.④ And we end with two output layers, each
    with a single node representing the Q-value.⑤ We start the forward pass, formatting
    the inputs to match what the network expects.⑥ Next, we concatenate the state
    and action and pass them through each stream.⑦ Continues ... |
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是完全连接的双 Q 值网络。这是 *TD*3 使用双流来近似 Q 值的方式。② 注意我们有两个输入层。再次强调，这些流实际上是两个独立的网络。③
    接下来，为每个流创建隐藏层。④ 最后，我们有两个输出层，每个输出层都有一个节点代表 Q 值。⑤ 我们开始正向传播，格式化输入以匹配网络期望的格式。⑥ 接下来，我们将状态和动作连接起来，并通过每个流传递。⑦
    继续…… |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonTD3’s twin Q-network 2/2
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说 PythonTD3 的双 Q 网络 2/2 |'
- en: '|  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ⑧ Here we pass through all the hidden layers and their respective activation
    function.⑨ Finally, we do a pass through the output layers and return their direct
    output.⑩ This is the forward pass through the Qa stream. This is useful for getting
    the values when calculating the targets to the policy updates.⑪ We format the
    inputs, and concatenate them before passing it through the a stream.⑫ Then pass
    through the “a” hidden layers ...⑬ ... all the way through the output layer, as
    if we had only one network to begin with. |
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 我们通过所有隐藏层及其相应的激活函数。⑨ 最后，我们通过输出层并返回它们的直接输出。⑩ 这是 Qa 流的正向传播。这在计算目标以更新策略时很有用。⑪
    我们格式化输入，并在通过 a 流之前将它们连接起来。⑫ 然后通过“a”隐藏层……⑬ ……一直通过输出层，就像我们一开始只有一个网络一样。 |
- en: Smoothing the targets used for policy updates
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用平滑的目标进行策略更新
- en: Remember that to improve exploration in DDPG, we inject Gaussian noise into
    the action used for the environment. In *TD*3, we take this concept further and
    add noise, not only to the action used for exploration, but also to the action
    used to calculate the targets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，为了提高 DDPG 中的探索性，我们在用于环境的动作中注入高斯噪声。在 *TD*3 中，我们进一步发展了这个概念，不仅向用于探索的动作添加噪声，还向用于计算目标值的动作添加噪声。
- en: Training the policy with noisy targets can be seen as a regularizer because
    now the network is forced to generalize over similar actions. This technique prevents
    the policy network from converging to incorrect actions because, early on during
    training, Q-functions can prematurely inaccurately value certain actions. The
    noise over the actions spreads that value over a more inclusive range of actions
    than otherwise.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带噪声的目标训练策略可以看作是一个正则化器，因为现在网络被迫对类似动作进行泛化。这项技术防止策略网络收敛到错误动作，因为在训练的早期阶段，Q 函数可能会过早地不准确地评估某些动作。动作上的噪声将这个值传播到一个更广泛的相关动作范围。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathTarget smoothing procedure
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示给我数学目标平滑过程 |'
- en: '|  | ![](../Images/12_00_Sidebar13.png) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏13.png](../Images/12_00_Sidebar13.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonTD3’s model-optimization
    step 1/2 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说 PythonTD3 的模型优化步骤 1/2 |'
- en: '|  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① To optimize the *TD*3 models, we take in a mini-batch of experiences.② We
    first get the min and max of the environment.③ Get the noise and scale it to the
    range of the actions.④ Get the noise clip min and max.⑤ Then, clip the noise.⑥
    Get the action from the target policy model.⑦ Then, add the noise to the action,
    and clip the action, too. |
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为了优化 *TD*3 模型，我们接收一个经验的小批量。② 我们首先获取环境的最大值和最小值。③ 获取噪声并将其缩放到动作的范围。④ 获取噪声的裁剪最小值和最大值。⑤
    然后，裁剪噪声。⑥ 从目标策略模型获取动作。⑦ 然后，将噪声添加到动作中，并裁剪动作。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonTD3’s model-optimization
    step 2/2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python TD3的模型优化步骤2/2 |'
- en: '|  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ⑧ We use the clamped noisy action to get the max value.⑨ Recall we get the max
    value by getting the minimum predicted value between the two streams, and use
    it for the target.⑩ Next, we get the predicted values coming from both of the
    streams to calculate the errors and the joint loss.⑪ Then, we do the standard
    backpropagation steps for the twin networks.⑫ Notice how we delay the policy updates
    here. I explain this a bit more on the next page.⑬ The update is similar to DDPG,
    but using the single stream ‘Qa.’⑭ But, the loss is the same.⑮ Here are the policy
    optimization steps. The standard stuff. |
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 我们使用夹具噪声动作来获取最大值。⑨ 回想一下，我们通过获取两个流之间的最小预测值来获取最大值，并用于目标。⑩ 接下来，我们获取两个流中来的预测值来计算误差和联合损失。⑪
    然后，我们对双网络进行标准的反向传播步骤。⑫ 注意我们在这里延迟了策略更新。我将在下一页上对此进行更多解释。⑬ 更新类似于DDPG，但使用单流‘Qa。’⑭
    但是，损失是相同的。⑮ 这里是策略优化的步骤。标准的东西。 |
- en: Delaying updates
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟更新
- en: The final improvement that *TD*3 applies over DDPG is delaying the updates to
    the policy network and target networks so that the online Q-function updates at
    a higher rate than the rest. Delaying these networks is beneficial because often,
    the online Q-function changes shape abruptly early on in the training process.
    Slowing down the policy so that it updates after a couple of value function updates
    allows the value function to settle into more accurate values before we let it
    guide the policy. The recommended delay for the policy and target networks is
    every other update to the online Q-function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*TD*3相对于DDPG的最终改进是延迟策略网络和目标网络的更新，以便在线Q函数的更新速率高于其他部分。延迟这些网络是有益的，因为在线Q函数在训练过程的早期往往会突然改变形状。放慢策略，使其在几个值函数更新之后更新，可以让值函数在引导策略之前稳定到更准确的价值。策略和目标网络的推荐延迟是每两次更新一次在线Q函数。'
- en: The other thing that you may notice in the policy updates is that we must use
    one of the streams of the online value model for getting the estimated Q-value
    for the action coming from the policy. In *TD*3, we use one of the two streams,
    but the same stream every time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在策略更新中注意到的另一件事是，我们必须使用在线值模型的一个流来获取来自策略的动作的估计Q值。在*TD*3中，我们使用两个流中的一个，但每次都使用相同的流。
- en: '| 0001 | A Bit Of HistoryIntroduction of the TD3 agent |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史 TD3代理的介绍 |'
- en: '|  | TD3 was introduced by Scott Fujimoto et al. in 2018 in a paper titled
    “Addressing Function Approximation Error in Actor-Critic Methods.”Scott is a graduate
    student at McGill University working on a PhD in computer science and supervised
    by Prof. David Meger and Prof. Doina Precup. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | TD3是由Scott Fujimoto等人于2018年在一篇题为“Addressing Function Approximation Error
    in Actor-Critic Methods”的论文中引入的。Scott是麦吉尔大学的一名研究生，正在攻读计算机科学博士学位，由David Meger教授和Doina
    Precup教授指导。 |'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe hopper environment
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 一个具体的例子 溜槽环境 |'
- en: '|  | The hopper environment we use is an open source version of the MuJoCo
    and Roboschool Hopper environments, powered by the Bullet Physics engine. MuJoCo
    is a physics engine with a variety of models and tasks. While MuJoCo is widely
    used in DRL research, it requires a license. If you aren’t a student, it can cost
    you a couple thousand dollars. Roboschool was an attempt by OpenAI to create open
    source versions of MuJoCo environments, but it was discontinued in favor of Bullet.
    Bullet Physics is an open source project with many of the same environments found
    in MuJoCo.![](../Images/12_00_Sidebar17_hopper.png)The HopperBulletEnv-v0 environment
    features a vector with 15 continuous variables as an unbounded observation space,
    representing the different joints of the hopper robot. It features a vector of
    three continuous variables bounded between –1 and 1 and representing actions for
    the thigh, leg, and foot joints. Note that a single action is a vector with three
    elements at once. The task of the agent is to move the hopper forward, and the
    reward function reinforces that, also promoting minimal energy cost. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们使用的跳跃者环境是MuJoCo和Roboschool Hopper环境的开源版本，由Bullet物理引擎提供支持。MuJoCo是一个具有多种模型和任务的物理引擎。虽然MuJoCo在DRL研究中被广泛使用，但它需要许可证。如果您不是学生，这可能需要花费您几千美元。Roboschool是OpenAI尝试创建MuJoCo环境开源版本的一次尝试，但后来因为Bullet而被放弃。Bullet物理引擎是一个开源项目，其中包含与MuJoCo中相同的大多数环境。![](../Images/12_00_Sidebar17_hopper.png)HopperBulletEnv-v0环境具有一个包含15个连续变量的向量作为无界观察空间，代表跳跃者机器人的不同关节。它具有一个介于-1和1之间的三个连续变量的向量，代表大腿、小腿和脚关节的动作。请注意，单个动作是一个包含三个元素的向量。智能体的任务是使跳跃者向前移动，奖励函数强化这一点，同时也促进最小化能量成本。
    |'
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsTraining TD3 in the
    hopper environment |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Details.png) | 细节中的训练TD3在跳跃者环境中 |'
- en: '|  | If you head to the chapter’s Notebook, you may notice that we train the
    agent until it reaches a 1,500 mean reward for 100 consecutive episodes. In reality,
    the recommended threshold is 2,500\. However, because we train using five different
    seeds, and each training run takes about an hour, I thought to reduce the time
    it takes to complete the Notebook by merely reducing the threshold. Even at 1,500,
    the hopper does a decent job of moving forward, as you can see on the GIFs in
    the Notebook.Now, you must know that all the book’s implementations take a long
    time because they execute one evaluation episode after every episode. Evaluating
    performance on every episode isn’t necessary and is likely overkill for most purposes.
    For our purposes, it’s okay, but if you want to reuse the code, I recommend you
    remove that logic and instead check evaluation performance once every 10–100 or
    so episodes.Also, take a look at the implementation details. The book’s *TD*3
    optimizes the policy and the value networks separately. If you want to train using
    CNNs, for instance, you may want to share the convolutions and optimize all at
    once. But again, that would require much tuning. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 如果您查看该章节的笔记本，您可能会注意到我们训练智能体直到它在100个连续的回合中达到平均奖励为1,500。实际上，推荐的阈值是2,500。然而，因为我们使用五个不同的种子进行训练，并且每次训练运行大约需要一个小时，所以我想到通过仅仅降低阈值来减少完成笔记本所需的时间。即使是在1,500的情况下，跳跃者也能很好地向前移动，正如您可以在笔记本中的GIF中看到的那样。现在，您必须知道，书中所有的实现都需要很长时间，因为它们在每个回合之后都会执行一次评估回合。在每个回合上评估性能并不是必要的，并且对于大多数目的来说可能是过度杀戮。对于我们来说，这是可以接受的，但如果您想重用代码，我建议您移除该逻辑，并在大约每10-100个回合检查一次评估性能。此外，请查看实现细节。书中的TD3分别优化策略和价值网络。如果您想使用CNN进行训练，例如，您可能希望共享卷积并在一次中优化所有内容。但再次强调，这需要大量的调整。
    |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpTD3 in the hopper environment
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 累计TD3在跳跃者环境中的表现 |'
- en: '|  | ![](../Images/12_00_Sidebar19.png) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar19.png) |'
- en: 'SAC: Maximizing the expected return and entropy'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '|  | SAC：最大化期望回报和熵'
- en: The previous two algorithms, DDPG and *TD*3, are off-policy methods that train
    a deterministic policy. Recall, off-policy means that the method uses experiences
    generated by a behavior policy that’s different from the policy optimized. In
    the cases of DDPG and *TD*3, they both use a replay buffer that contains experiences
    generated by several previous policies. Also, because the policy being optimized
    is deterministic, meaning that it returns the same action every time it’s queried,
    they both use off-policy exploration strategies. On our implementation, they both
    use Gaussian noise injection to the action vectors going into the environment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个算法，DDPG和**TD**3，是离策略方法，用于训练确定性策略。回想一下，离策略意味着该方法使用由与优化策略不同的行为策略生成的经验。在DDPG和**TD**3的情况下，它们都使用一个包含由几个先前策略生成的经验的回放缓冲区。此外，因为正在优化的策略是确定性的，这意味着每次查询时它都返回相同的动作，因此它们都使用离策略探索策略。在我们的实现中，它们都使用高斯噪声注入到进入环境的动作向量中。
- en: To put it into perspective, the agents that you learned about in the previous
    chapter learn on-policy. Remember, they train stochastic policies, which by themselves
    introduce randomness and, therefore, exploration. To promote randomness in stochastic
    policies, we add an entropy term to the loss function.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地说明，你之前章节中学到的智能体是按策略学习的。记住，它们训练随机策略，这些策略本身引入了随机性和探索性。为了在随机策略中促进随机性，我们在损失函数中添加了一个熵项。
- en: In this section, we discuss an algorithm called *soft actor-critic* (SAC), which
    is a hybrid between these two paradigms. SAC is an off-policy algorithm similar
    to DDPG and *TD*3, but it trains a stochastic policy as in REINFORCE, A3C, GAE,
    and A2C instead of a deterministic policy, as in DDPG and *TD*3.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了一个名为**软演员-评论家**（SAC）的算法，它是这两种范例的混合体。SAC是一个类似于DDPG和**TD**3的离策略算法，但它像REINFORCE、A3C、GAE和A2C一样训练随机策略，而不是像DDPG和**TD**3那样训练确定性策略。
- en: Adding the entropy to the Bellman equations
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将熵添加到贝尔曼方程中
- en: The most crucial characteristic of SAC is that the entropy of the stochastic
    policy becomes part of the value function that the agent attempts to maximize.
    As you see in this sectiovn, jointly maximizing the expected total reward and
    the expected total entropy naturally encourages behavior that’s as diverse as
    possible while still maximizing the expected return.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: SAC最关键的特征是随机策略的熵成为智能体试图最大化的值函数的一部分。正如你在本节中看到的那样，联合最大化期望总奖励和期望总熵自然地鼓励尽可能多样化的行为，同时仍然最大化期望回报。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe agent needs to also maximize
    the entropy |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 显示数学智能体需要最大化熵 |'
- en: '|  | ![](../Images/12_00_Sidebar20.png) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar20.png) |'
- en: Learning the action-value function
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习动作值函数
- en: In practice, SAC learns the value function in a way similar to *TD*3\. That
    is, we use two networks approximating the Q-function and take the minimum estimate
    for most calculations. A few differences, however, are that, with SAC, independently
    optimizing each Q-function yields better results, which is what we do. Second,
    we add the entropy term to the target values. And last, we don’t use the target
    action smoothing directly as we did in *TD*3\. Other than that, the pattern is
    the same as in *TD*3.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，SAC以类似于**TD**3的方式学习值函数。也就是说，我们使用两个近似Q函数的网络，并取大多数计算的最低估计值。然而，有几个不同之处，即在使用SAC时，独立优化每个Q函数会产生更好的结果，这正是我们所做的。其次，我们将熵项添加到目标值中。最后，我们不像在**TD**3中那样直接使用目标动作平滑。除此之外，模式与**TD**3相同。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathAction-value function target
    (we train doing MSE on this target) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 显示数学Action-value函数目标（我们在该目标上训练MSE） |'
- en: '|  | ![](../Images/12_00_Sidebar21.png) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar21.png) |'
- en: Learning the policy
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习策略
- en: This time for learning the stochastic policy, we use a squashed Gaussian policy
    that, in the forward pass, outputs the mean and standard deviation. Then we can
    use those to sample from that distribution, squash the values with a hyperbolic
    tangent function tanh, and then rescale the values to the range expected by the
    environment.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这次为了学习随机策略，我们使用了一个压缩高斯策略，在正向传递中输出均值和标准差。然后我们可以使用这些值从这个分布中进行采样，使用双曲正切函数tanh压缩值，然后将值重新缩放到环境预期的范围内。
- en: For training the policy, we use the reparameterization trick. This “trick” consists
    of moving the stochasticity out of the network and into an input. This way, the
    network is deterministic, and we can train it without problems. This trick is
    straightforwardly implemented in PyTorch, as you see next.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练策略，我们使用重新参数化技巧。这个“技巧”包括将随机性从网络中移出，放入输入中。这样，网络是确定性的，我们可以没有问题地训练它。这个技巧在PyTorch中直接实现，正如您接下来看到的。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathPolicy objective (we train
    minimizing the negative of this objective) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学策略目标（我们通过最小化这个目标函数进行训练） |'
- en: '|  | ![](../Images/12_00_Sidebar22.png) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar22.png) |'
- en: Automatically tuning the entropy coefficient
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动调整熵系数
- en: The cherry on the cake of SAC is that alpha, which is the entropy coefficient,
    can be tuned automatically. SAC employs gradient-based optimization of alpha toward
    a heuristic expected entropy. The recommended target entropy is based on the shape
    of the action space; more specifically, the negative of the vector product of
    the action shape. Using this target entropy, we can automatically optimize alpha
    so that there’s virtually no hyperparameter to tune related to regulating the
    entropy term.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: SAC的甜点是alpha，即熵系数，可以自动调整。SAC采用基于梯度的alpha优化，以向启发式期望熵进行优化。推荐的目标熵基于动作空间形状；更具体地说，是动作形状的向量积的负值。使用这个目标熵，我们可以自动优化alpha，从而几乎没有与调节熵项相关的超参数需要调整。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathAlpha objective function
    (we train minimizing the negative of this objective) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学Alpha目标函数（我们通过最小化这个目标函数进行训练） |'
- en: '|  | ![](../Images/12_00_Sidebar23.png) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar23.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonSAC Gaussian policy 1/2 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonSAC高斯策略1/2 |'
- en: '|  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '① This is the Gaussian policy that we use in SAC.② We start everything the
    same way as other policy networks: input, to hidden layers.③ But the hidden layers
    connect to the two streams. One represents the mean of the action and the other
    the log standard deviation. |'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是我们SAC中使用的高斯策略。② 我们以与其他策略网络相同的方式开始一切：输入，到隐藏层。③ 但隐藏层连接到两个流。一个表示动作的均值，另一个表示对数标准差。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonSAC Gaussian policy 2/2 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonSAC高斯策略2/2 |'
- en: '|  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ④ Same line to help you keep the flow of the code⑤ Here we calculate *H*, the
    target entropy heuristic.⑥ Next, we create a variable, initialize to zero, and
    create an optimizer to optimize the log alpha.⑦ The forward function is what we’d
    expect.⑧ We format the input variables, and pass them through the whole network.⑨
    Clamp the log std to -20 to *2*, to control the std range to reasonable values.⑩
    And return the values.⑪ In the full pass, we get the mean and log std.⑫ Get a
    Normal distribution with those values.⑬ *r* sample here does the reparameterization
    trick.⑭ Then we squash the action to be in range -1, 1.⑮ Then, rescale to be the
    environment expected range.⑯ We also need to rescale the log probability and the
    mean. |
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 同一行以帮助您保持代码的流程⑤ 这里我们计算*H*，目标熵启发式。⑥ 接下来，我们创建一个变量，初始化为零，并创建一个优化器来优化log alpha。⑦
    前向函数是我们预期的。⑧ 我们格式化输入变量，并将它们通过整个网络传递。⑨ 将log std限制在-20到*2*之间，以控制std的范围在合理的值。⑩ 并返回这些值。⑪
    在完整传递中，我们得到均值和log std。⑫ 获取具有这些值的正态分布。⑬ *r*样本在这里执行重新参数化技巧。⑭ 然后，我们将动作压缩到-1到1的范围内。⑮
    然后，将其缩放到环境期望的范围。⑯ 我们还需要缩放log概率和均值。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonSAC optimization step 1/2
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonSAC优化步骤1/2 |'
- en: '|  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① This is the optimization step in SAC.② First, get the experiences from the
    mini-batch.③ Next, we get the current actions, a-hat, and log probabilities of
    state *s*.④ Here, we calculate the loss of alpha, and here we step alpha’s optimizer.⑤
    This is how we get the current value of alpha.⑥ In these lines, we get the Q-values
    using the online models, and a-hat.⑦ Then, we use the minimum Q-value estimates.⑧
    Here, we calculate the policy loss using that minimum Q-value estimate.⑨ On the
    next page, we calculate the Q-functions loss. |
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是SAC中的优化步骤。② 首先，从迷你批次中获取经验。③ 接下来，我们获取当前的动作，a-hat，以及状态*s*的对数概率。④ 这里，我们计算alpha的损失，并在这里执行alpha优化器的步骤。⑤
    这是我们获取当前alpha值的方法。⑥ 在这些行中，我们使用在线模型和a-hat获取Q值。⑦ 然后，我们使用最小Q值估计。⑧ 这里，我们使用那个最小Q值估计来计算策略损失。⑨
    在下一页，我们计算Q函数损失。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonSAC optimization step 2/2
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ![Python 图标](../Images/icons_Python.png) | 我会说 PythonSAC 优化步骤 2/2 |'
- en: '|  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '⑩ To calculate the value loss, we get the predicted next action.⑪ Using the
    target value models, we calculate the Q-value estimate of the next state-action
    pair.⑫ Get the minimum Q-value estimate, and factor in the entropy.⑬ This is how
    we calculate the target, using the reward plus the discounted minimum value of
    the next state along with the entropy.⑭ Here we get the predicted values of the
    state-action pair using the online model.⑮ Calculate the loss and optimize each
    Q-function separately. First, *a*:⑯ Then, *b*:⑰ Finally, the policy: |'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 为了计算价值损失，我们得到预测的下一个动作。⑪ 使用目标价值模型，我们计算下一个状态-动作对的 Q 值估计。⑫ 获取最小的 Q 值估计，并考虑熵。⑬
    这就是我们的目标计算方法，使用奖励加上下一个状态的折扣最小值以及熵。⑭ 在这里，我们使用在线模型得到状态-动作对的预测值。⑮ 计算损失并分别优化每个 Q 函数。首先，*a*：⑯
    然后，*b*：⑰ 最后，策略：|
- en: '| 0001 | A Bit Of HistoryIntroduction of the SAC agent |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍 SAC 代理 |'
- en: '|  | SAC was introduced by Tuomas Haarnoja in 2018 in a paper titled “Soft
    actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic
    actor.” At the time of publication, Tuomas was a graduate student at Berkeley
    working on a PhD in computer science under the supervision of Prof. Pieter Abbeel
    and Prof. Sergey Levine, and a research intern at Google. Since 2019, Tuomas is
    a research scientist at Google DeepMind. |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | SAC 是由 Tuomas Haarnoja 在 2018 年在一篇题为“Soft actor-critic: Off-policy maximum
    entropy deep reinforcement learning with a stochastic actor”的论文中提出的。在发表时，Tuomas
    是加州大学伯克利分校的一名研究生，在 Pieter Abbeel 教授和 Sergey Levine 教授的指导下攻读计算机科学博士学位，并在谷歌担任研究实习生。自
    2019 年以来，Tuomas 成为谷歌 DeepMind 的研究科学家。'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe cheetah environment
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ![Concrete 图标](../Images/icons_Concrete.png) | 一个具体的例子猎豹环境 |'
- en: '|  | The HalfCheetahBulletEnv-v0 environment features a vector with 26 continuous
    variables for the observation space, representing the joints of the robot. It
    features a vector of 6 continuous variables bounded between –1 and 1, representing
    the actions. The task of the agent is to move the cheetah forward, and as with
    the hopper, the reward function reinforces that also, promoting minimal energy
    cost.![](../Images/12_00_Sidebar29_cheetah.png) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | HalfCheetahBulletEnv-v0 环境具有一个包含 26 个连续变量的向量，代表机器人的关节，动作空间是一个介于 -1 和 1
    之间的 6 个连续变量的向量。代理的任务是使猎豹向前移动，与 hopper 一样，奖励函数也强化了这一点，促进最小能量成本。![猎豹环境侧边栏](../Images/12_00_Sidebar29_cheetah.png)
    |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpSAC on the cheetah environment
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ![Tally 图标](../Images/icons_Tally.png) | 总结猎豹环境上的 SAC |'
- en: '|  | ![](../Images/12_00_Sidebar30.png) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | ![总结侧边栏](../Images/12_00_Sidebar30.png) |'
- en: 'PPO: Restricting optimization steps'
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO：限制优化步骤
- en: In this section, we introduce an actor-critic algorithm called *proximal policy
    optimization* (PPO). Think of PPO as an algorithm with the same underlying architecture
    as A2C. PPO can reuse much of the code developed for A2C. That is, we can roll
    out using multiple environments in parallel, aggregate the experiences into mini-batches,
    use a critic to get GAE estimates, and train the actor and critic in a way similar
    to training in A2C.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一种称为**近端策略优化**（PPO）的演员-评论家算法。将 PPO 视为一个与 A2C 具有相同底层架构的算法。PPO 可以重用为
    A2C 开发的大部分代码。也就是说，我们可以并行使用多个环境进行模拟，将经验汇总成 mini-batch，使用评论家获取 GAE 估计，并以类似于 A2C
    训练的方式训练演员和评论家。
- en: The critical innovation in PPO is a surrogate objective function that allows
    an on-policy algorithm to perform multiple gradient steps on the same mini-batch
    of experiences. As you learned in the previous chapter, A2C, being an on-policy
    method, cannot reuse experiences for the optimization steps. In general, on-policy
    methods need to discard experience samples immediately after stepping the optimizer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 中的关键创新是一个代理目标函数，它允许在线策略算法在相同的经验 mini-batch 上执行多个梯度步骤。正如你在上一章所学到的，A2C 作为一种在线策略方法，不能重复使用经验进行优化步骤。一般来说，在线策略方法需要在优化器步骤后立即丢弃经验样本。
- en: However, PPO introduces a clipped objective function that prevents the policy
    from getting too different after an optimization step. By optimizing the policy
    conservatively, we not only prevent performance collapse due to the innate high
    variance of on-policy policy gradient methods but also can reuse mini-batches
    of experiences and perform multiple optimization steps per mini-batch. The ability
    to reuse experiences makes PPO a more sample-efficient method than other on-policy
    methods, such as those you learned about in the previous chapter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，PPO引入了一个剪裁的目标函数，防止策略在优化步骤后变得过于不同。通过保守地优化策略，我们不仅防止了由于策略梯度方法的内在高方差导致的性能崩溃，而且可以重用经验的小批量，并在每个小批量中执行多个优化步骤。重用经验的能力使PPO比其他策略方法（如你在上一章中学到的那些方法）更有效率。
- en: Using the same actor-critic architecture as A2C
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用与A2C相同的演员-评论家架构
- en: Think of PPO as an improvement to A2C. What I mean by that is that even though
    in this chapter we have learned about DDPG, *TD*3, and SAC, and all these algorithms
    have commonality. PPO should not be confused as an improvement to SAC. *TD*3 is
    a direct improvement to DDPG. SAC was developed concurrently with *TD*3\. However,
    the SAC author published a second version of the SAC paper shortly after the first
    one, which includes several of the features of *TD*3\. While SAC isn’t a direct
    improvement to *TD*3, it does share several features. PPO, however, is an improvement
    to A2C, and we reuse part of the A2C code. More specifically, we sample parallel
    environments to gather the mini-batches of data and use GAE for policy targets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将PPO视为A2C的改进。我的意思是，尽管在本章中我们学习了DDPG、*TD*3和SAC，并且所有这些算法都有共性。PPO不应被视为SAC的改进。*TD*3是DDPG的直接改进。SAC是与*TD*3同时开发的。然而，SAC的作者在第一篇SAC论文发布后不久，又发布了一个SAC的第二版论文，其中包含了*TD*3的一些特性。虽然SAC不是*TD*3的直接改进，但它确实共享了一些特性。然而，PPO是A2C的改进，我们重用了A2C的部分代码。更具体地说，我们采样并行环境以收集数据的小批量，并使用GAE作为策略目标。
- en: '| 0001 | A Bit Of HistoryIntroduction of the PPO agent |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍PPO代理 |'
- en: '|  | PPO was introduced by John Schulman et al. in 2017 in a paper titled “Proximal
    Policy Optimization Algorithms.” John is a Research Scientist, a cofounding member,
    and the co-lead of the reinforcement learning team at OpenAI. He received his
    PhD in computer science from Berkeley, advised by Pieter Abbeel. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | PPO是由John Schulman等人于2017年在一篇题为“Proximal Policy Optimization Algorithms”的论文中提出的。John是一位研究科学家，OpenAI的共同创始人之一，也是强化学习团队的共同负责人。他在加州大学伯克利分校获得计算机科学博士学位，导师是Pieter
    Abbeel。 |'
- en: Batching experiences
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量经验
- en: One of the features of PPO that A2C didn’t have is that with PPO, we can reuse
    experience samples. To deal with this, we could gather large trajectory batches,
    as in NFQ, and “fit” the model to the data, optimizing it over and over again.
    However, a better approach is to create a replay buffer and sample a large mini-batch
    from it on every optimization step. That gives the effect of stochasticity on
    each mini-batch because samples aren’t always the same, yet we likely reuse all
    samples in the long term.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的一个特点，A2C没有的是，我们可以重用经验样本。为了处理这个问题，我们可以像NFQ那样收集大的轨迹批次，并对数据进行“拟合”，反复优化。然而，更好的方法是创建一个重放缓冲区，并在每次优化步骤中从中采样一个大的迷你批次。这给每个迷你批次带来了随机性的效果，因为样本并不总是相同的，但从长远来看，我们很可能重用所有样本。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonEpisode replay buffer 1/4
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我说Python第1/4集 |'
- en: '|  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① This is the fill of the EpisodeBuffer class.② Variables to keep worker information
    grouped③ Here we enter the main loop to fill up the buffer.④ We start by getting
    the current actions, log probabilities, and stats.⑤ We pass the actions to the
    environments and get the experiences.⑥ Then, store the experiences into the replay
    buffer. |
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是EpisodeBuffer类的填充。② 变量用于将工作信息分组。③ 我们进入主循环以填充缓冲区。④ 我们首先获取当前的动作、对数概率和统计数据。⑤
    将动作传递到环境中并获取经验。⑥ 然后，将经验存储到重放缓冲区中。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonEpisode replay buffer 2/4
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我说Python第2/4集 |'
- en: '|  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ⑦ Same line. Also, I removed spaces to make it easier to read.⑧ We create these
    two variables for each worker. Remember, workers are inside environments.⑨ Here
    we manually truncate episodes that go for too many steps.⑩ We check for terminal
    states and preprocess them.⑪ We bootstrap iwhe terminal state was truncated.⑫
    We update the states variable and increase the step count.⑬ Here we process the
    workers if we have terminals.⑭ We process each terminal worker one at a time.
    |
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 同一行。此外，我移除了空格以使其更容易阅读。⑧ 我们为每个工作者创建这两个变量。记住，工作者在环境中。⑨ 这里我们手动截断步骤过多的剧集。⑩ 我们检查终端状态并预处理它们。⑪
    如果终端状态被截断，我们进行自举。⑫ 我们更新状态变量并增加步数。⑬ 如果我们有终端状态，我们处理工作者。⑭ 我们逐个处理每个终端工作者。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonEpisode replay buffer 3/4
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我说Python第3集回放缓冲区 3/4 |'
- en: '|  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ⑮ Further removed spaces⑯ Here we collect statistics to display and analyze
    after the fact.⑰ We append the bootstrapping value to the reward vector. Calculate
    the predicted returns.⑱ Here we get the predicted values, and also append the
    bootstrapping value to the vector.⑲ Here we calculate the generalized advantage
    estimators, and save them into the buffer.⑳ And start resetting all worker variables
    to process the next episode. |
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 进一步移除空格。⑯ 这里我们收集统计信息以事后显示和分析。⑰ 我们将自举值添加到奖励向量中。计算预测回报。⑱ 这里我们获取预测值，并将自举值添加到向量中。⑲
    这里我们计算广义优势估计值，并将它们保存到缓冲区中。⑳ 并开始重置所有工作者变量以处理下一个剧集。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonEpisode replay buffer 4/4
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我说Python第4集回放缓冲区 4/4 |'
- en: '|  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ㉑ Same line, indentation edited again㉒ Check which episode is next in queue
    and break if you have too many.㉓ If buffer isn’t full, we set the id of the new
    episode to the worker.㉔ If we’re in these lines, it means the episode is full,
    so we process the memory for sampling.㉕ Because we initialize the whole buffer
    at once, we need to remove from the memory everything that isn’t a number, in
    the episode and the steps dimensions.㉖ Finally, we extract the statistics to display.㉗
    And return the stats. |
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ㉑ 同一行，再次调整缩进。㉒ 检查下一个排队中的剧集，如果你有太多的话就中断。㉓ 如果缓冲区未满，我们将新剧集的ID设置给工作者。㉔ 如果我们处于这些行，这意味着剧集已满，因此我们处理内存以进行采样。㉕
    因为我们是同时初始化整个缓冲区，所以我们需要从剧集和步骤维度中移除所有非数字的内容。㉖ 最后，我们提取统计信息以显示。㉗ 然后返回统计信息。 |
- en: Clipping the policy updates
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 裁剪策略更新
- en: The main issue with the regular policy gradient is that even a small change
    in parameter space can lead to a big difference in performance. The discrepancy
    between parameter space and performance is why we need to use small learning rates
    in policy-gradient methods, and even so, the variance of these methods can still
    be too large. The whole point of clipped PPO is to put a limit on the objective
    such that on each training step, the policy is only allowed to be so far away.
    Intuitively, you can think of this clipped objective as a coach preventing overreacting
    to outcomes. Did the team get a good score last night with a new tactic? Great,
    but don’t exaggerate. Don’t throw away a whole season of results for a new result.
    Instead, keep improving a little bit at a time.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正则策略梯度的主要问题是，参数空间中的微小变化可能导致性能的巨大差异。参数空间与性能之间的差异是我们需要在策略梯度方法中使用小学习率的原因，即便如此，这些方法的方差仍然可能太大。裁剪PPO的整个目的是对目标函数进行限制，使得在每个训练步骤中，策略只能偏离这么多。直观地说，你可以将这种裁剪目标视为教练防止对结果过度反应。昨晚球队用新战术得分好吗？很好，但不要夸张。不要因为一个新结果而放弃整个赛季的结果。相反，每次只稍微改进一点。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathClipped policy objective
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 给我看数学裁剪策略目标 |'
- en: '|  | ![](../Images/12_00_Sidebar36.png) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar36.png) |'
- en: Clipping the value function updates
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 裁剪值函数更新
- en: 'We can apply a similar clipping strategy to the value function with the same
    core concept: let the changes in parameter space change the Q-values only this
    much, but not more. As you can tell, this clipping technique keeps the variance
    of the things we care about smooth, whether changes in parameter space are smooth
    or not. We don’t necessarily need small changes in parameter space; however, we’d
    like level changes in performance and values.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将类似的裁剪策略应用于值函数，核心概念相同：让参数空间的变化只改变Q值这么多，但不能更多。正如你可以看到的，这种裁剪技术保持了我们所关心的事物变化的平滑性，无论参数空间的变化是否平滑。我们不一定需要参数空间的小变化；然而，我们希望性能和值的变化是水平的。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathClipped value loss |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/icons_Math.png) | 给我看数学裁剪值损失 |'
- en: '|  | ![](../Images/12_00_Sidebar37.png) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/12_00_Sidebar37.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPPO optimization step 1/3
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/icons_Python.png) | 我会说PythonPPO优化步骤 1/3 |'
- en: '|  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '① Now, let’s look at those two equations in code.② First, extract the full
    batch of experiences from the buffer.③ Get the values before we start optimizing
    the models.④ Get the gaes and normalize the batch.⑤ Now, start optimizing the
    policy first for at most the preset epochs.⑥ We sub-sample from the full batch
    a mini-batch.⑦ Extract the mini-batch using the randomly sampled indices.⑧ We
    use the online model to get the predictions.⑨ Here we calculate the ratios: log
    probabilities to ratio of probabilities.⑩ Then, calculate the objective and the
    clipped objective. |'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ① 现在，让我们看看这些代码中的两个方程。② 首先，从缓冲区中提取完整的经验批次。③ 在开始优化模型之前获取值。④ 获取gaes并对批次进行归一化。⑤
    现在，首先对策略进行最多预设的轮次优化。⑥ 从完整批次中子采样一个迷你批次。⑦ 使用随机采样的索引提取迷你批次。⑧ 我们使用在线模型来获取预测值。⑨ 在这里，我们计算比率：对数概率与概率比。⑩
    然后，计算目标和裁剪目标。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPPO optimization step 2/3
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/icons_Python.png) | 我会说PythonPPO优化步骤 2/3 |'
- en: '|  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ⑪ We calculate the loss using the negative of the minimum of the objectives.⑫
    Also, we calculate the entropy loss, and weight it accordingly.⑬ Zero the optimizing
    and start training.⑭ After stepping the optimizer, we do this nice trick of ensuring
    we only optimize again if the new policy is within bounds of the original policy.⑮
    Here we calculate the kl-divergence of the two policies.⑯ And break out of the
    training loop if it’s greater than a stopping condition.⑰ Here, we start doing
    similar updates to the value function.⑱ We grab the mini-batch from the full batch,
    as with the policy. |
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 我们使用目标的最小值的负值来计算损失。⑫ 同时，我们计算熵损失，并相应地加权。⑬ 将优化器置零并开始训练。⑭ 在步进优化器之后，我们执行这个巧妙的技巧，确保只有当新策略在原始策略的范围内时才再次优化。⑮
    在这里，我们计算两个策略的kl散度。⑯ 如果它大于停止条件，则跳出训练循环。⑰ 在这里，我们开始对价值函数进行类似的更新。⑱ 我们从完整批次中获取迷你批次，就像策略一样。
    |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPPO optimization step 3/3
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/icons_Python.png) | 我会说PythonPPO优化步骤 3/3 |'
- en: '|  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ⑲ Get the predicted values according to the model, and calculate the standard
    loss.⑳ Here we calculate the clipped predicted values.㉑ Then, calculate the clipped
    loss.㉒ We use the MSE of the maximum between the standard and clipped loss.㉓ Finally,
    we zero the optimizer, backpropagate the loss, clip the gradient, and step.㉔ We
    can do something similar to early stopping, but with the value function.㉕ Basically
    we check for the MSE of the predicted values of the new and old policies. |
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ⑲ 根据模型获取预测值，并计算标准损失。⑳ 在这里，我们计算裁剪预测值。㉑ 然后，计算裁剪损失。㉒ 我们使用标准损失和裁剪损失之间的最大值的均方误差。㉓
    最后，我们将优化器置零，反向传播损失，裁剪梯度，并步进。㉔ 我们可以做一些类似早期停止的事情，但使用价值函数。㉕ 基本上，我们检查新旧策略预测值的新旧均方误差。
    |
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe LunarLander environment
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/icons_Concrete.png) | 一个具体例子LunarLander环境 |'
- en: '|  | Unlike all the other environments we have explored in this chapter, the
    LunarLander environment features a discrete action space. Algorithms, such as
    DDPG and *TD*3, only work with continuous action environments, whether single-variable,
    such as pendulum, or a vector, such as in hopper and cheetah. Agents such as DQN
    only work in discrete action-space environments, such as the cart-pole. Actor-critic
    methods such as A2C and PPO have a big plus, which is that you can use stochastic
    policy models that are compatible with virtually any action space.![](../Images/12_00_Sidebar41_lander.png)In
    this environment, the agent needs to select one out of four possible actions on
    every step. That is 0 for do nothing; or 1 for fire the left engine; or 2 for
    fire the main engine; or 3 for fire the right engine. The observation space is
    a vector with eight elements, representing the coordinates, angles, velocities,
    and whether its legs touch the ground. The reward function is based on distance
    from the landing pad and fuel consumption. The reward threshold for solving the
    environment is 200, and the time step limit is 1,000. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 与本章中我们探索的所有其他环境不同，LunarLander环境具有离散动作空间。例如DDPG和*TD*3这样的算法，仅适用于连续动作环境，无论是单变量，如摆锤，还是向量，如hopper和cheetah。像DQN这样的智能体仅适用于离散动作空间环境，如cart-pole。演员-评论家方法如A2C和PPO有一个很大的优点，即可以使用与几乎任何动作空间兼容的随机策略模型！![LunarLander环境截图](../Images/12_00_Sidebar41_lander.png)在这个环境中，智能体需要在每一步选择四种可能动作中的一种。这包括0表示什么都不做；或1表示点燃左侧引擎；或2表示点燃主引擎；或3表示点燃右侧引擎。观察空间是一个包含八个元素的向量，表示坐标、角度、速度以及其腿是否接触地面。奖励函数基于与着陆板的距离和燃料消耗。解决环境的奖励阈值是200，时间步限制是1,000。|'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpPPO in the LunarLander environment
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ![计数器图标](../Images/icons_Tally.png) | 在LunarLander环境中的PPO计数|'
- en: '|  | ![](../Images/12_00_Sidebar42.png) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏42截图](../Images/12_00_Sidebar42.png)|'
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we surveyed the state-of-the-art actor-critic and deep reinforcement
    learning methods in general. You first learned about DDPG methods, in which a
    deterministic policy is learned. Because these methods learn deterministic policies,
    they use off-policy exploration strategies and update equations. For instance,
    with DDPG and *TD*3, we inject Gaussian noise into the action-selection process,
    allowing deterministic policies to become exploratory.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了最先进的演员-评论家和深度强化学习方法。您首先了解了DDPG方法，其中学习了一个确定性策略。因为这些方法学习确定性策略，所以它们使用离策略探索策略和更新方程。例如，在DDPG和*TD*3中，我们在动作选择过程中注入高斯噪声，使确定性策略变得具有探索性。
- en: In addition, you learned that *TD*3 improves DDPG with three key adjustments.
    First, *TD*3 uses a double-learning technique similar to that of DDQN, in which
    we “cross-validate” the estimates coming out of the value function by using a
    twin Q-network. Second, *TD*3, in addition to adding Gaussian noise to the action
    passed into the environment, also adds Gaussian noise to target actions, to ensure
    the policy does not learn actions based on bogus Q-value estimates. Third, *TD*3
    delays the updates to the policy network, so that the value networks get better
    estimates before we use them to change the policy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还了解到*TD*3通过三个关键调整改进了DDPG。首先，*TD*3使用与DDQN类似的双学习技术，其中我们通过使用双Q网络“交叉验证”价值函数输出的估计。其次，*TD*3除了向传递给环境的动作添加高斯噪声外，还向目标动作添加高斯噪声，以确保策略不会基于虚假的Q值估计来学习动作。第三，*TD*3延迟对策略网络的更新，这样在用它们改变策略之前，价值网络可以得到更好的估计。
- en: We then explored an entropy-maximization method called SAC, which consists of
    maximizing a joint objective of the value function and policy entropy, which intuitively
    translates into getting the most reward with the most diverse policy. The SAC
    agent, similar to DDPG and *TD*3, learns in an off-policy way, which means these
    agents can reuse experiences to improve policies. However, unlike DDPG and *TD*3,
    SAC learns a stochastic policy, which implies exploration can be on-policy and
    embedded in the learned policy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后探索了一种名为SAC的熵最大化方法，该方法包括最大化价值函数和政策熵的联合目标，直观上可以理解为以最多样化的策略获得最多的奖励。SAC智能体与DDPG和*TD*3类似，以离策略的方式进行学习，这意味着这些智能体可以重用经验来改进策略。然而，与DDPG和*TD*3不同，SAC学习一个随机策略，这意味着探索可以是按策略的，并嵌入到学习到的策略中。
- en: Finally, we explored an algorithm called PPO, which is a more direct descendant
    of A2C, being an on-policy learning method that also uses an on-policy exploration
    strategy. However, because of a clipped objective that makes PPO improve the learned
    policy more conservatively, PPO is able to reuse past experiences for its policy-improvement
    steps.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探索了一种称为PPO的算法，它是A2C的直接后裔，是一种基于策略的学习方法，同时也使用基于策略的探索策略。然而，由于剪裁目标使得PPO更保守地改进学习策略，因此PPO能够重用过去的经验来改进其策略。
- en: In the next chapter, we review several of the research areas surrounding DRL
    that are pushing the edge of a field that many call *artificial general intelligence*
    (AGI). AGI is an opportunity to understand human intelligence by recreating it.
    Physicist Richard Feynman said, “What I cannot create, I don’t understand.” Wouldn’t
    it be nice to understand intelligence?
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾围绕DRL的几个研究领域，这些领域正在推动许多人称之的“通用人工智能”（AGI）领域的边缘。AGI是通过重新创造它来理解人类智能的机会。物理学家理查德·费曼说：“我不能创造的东西，我就不理解。”理解智能不是很好吗？
- en: By now, you
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Understand more advanced actor-critic algorithms and relevant tricks
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解更高级的演员-评论家算法和相关技巧
- en: Can implement state-of-the-art deep reinforcement learning methods and perhaps
    devise improvements to these algorithms that you can share with others
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以实现最先进的深度强化学习方法，并可能设计出可以与他人分享的这些算法的改进
- en: Can apply state-of-the-art deep reinforcement learning algorithms to a variety
    of environments, hopefully even environments of your own
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将最先进的深度强化学习算法应用于各种环境，希望甚至包括你自己的环境
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tweet.png) | 可分享的工作在自己的领域内工作并分享你的发现'
- en: '|  | Here are several ideas on how to take what you’ve learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you''ll take advantage of it.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，如何将你所学的内容提升到下一个层次。如果你愿意，与世界分享你的结果，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch12_tf01:** Pick a continuous action-space environment and test all
    of the agents you learned about in this chapter in that same environment. Notice
    that you’ll have to change PPO for this. But, it’s worth learning how these algorithms
    compare.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch12_tf01:** 选择一个连续动作空间环境，并在该环境中测试本章中你所学到的所有智能体。注意，你可能需要为PPO更改它。但是，了解这些算法如何比较是值得的。'
- en: '**#gdrl_ch12_tf02:** Grab PPO, and add it to the previous chapter’s Notebook.
    Test it in similar environments and compare the results. Notice that this implementation
    of PPO buffers some experiences before it does any updates. Make sure to adjust
    the code or hyperparameters to make the comparison fair. How does PPO compare?
    Make sure to also test on a more challenging environment than cart-pole!'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch12_tf02:** 获取PPO，并将其添加到上一章的笔记本中。在类似的环境中测试它，并比较结果。注意，这个PPO实现会在进行任何更新之前缓冲一些经验。确保调整代码或超参数，以使比较公平。PPO的表现如何？请确保在比cart-pole更具挑战性的环境中进行测试！'
- en: '**#gdrl_ch12_tf03:** There are other maximum-entropy deep reinforcement learning
    methods, such as soft Q-learning. Find a list of algorithms that implement this
    maximum-entropy objective, pick one of them, and implement it yourself. Test it
    and compare your implementation with other agents, including SAC. Create a blog
    post explaining the pros and cons of these kinds of methods.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch12_tf03:** 还有其他最大熵深度强化学习方法，例如软Q学习。找到实现这种最大熵目标的算法列表，选择其中一个，并自己实现它。测试它，并将你的实现与其他智能体（包括SAC）进行比较。创建一篇博客文章，解释这些方法的优缺点。'
- en: '**#gdrl_ch12_tf04:** Test all the algorithms in this chapter in a high-dimensional
    observation-space environment that also has continuous action space. Check out
    the car-racing environment ([https://gym.openai.com/envs/CarRacing-v0/](https://gym.openai.com/envs/CarRacing-v0/)),
    for instance. Any other like that one would do. Modify the code so agents learn
    in these.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch12_tf04:** 在一个具有高维观察空间和连续动作空间的高维观察空间环境中测试本章中的所有算法。例如，查看赛车环境（[https://gym.openai.com/envs/CarRacing-v0/](https://gym.openai.com/envs/CarRacing-v0/)）。任何类似的都可以。修改代码，让智能体在这些环境中学习。'
- en: '**#gdrl_ch12_tf05:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch12_tf05**：在每一章中，我都使用最后的标签作为通用的标签。请随意使用这个标签来讨论任何与本章相关的工作。没有比为自己创造作业更令人兴奋的了。确保分享你打算调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@提及我 @mimoralea（我会转发），并使用列表中的特定标签来帮助感兴趣的人找到你的结果。没有对错之分；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己更受关注！我们正在等待你的加入！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源列表。查看它在这里<链接>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的作品。|
