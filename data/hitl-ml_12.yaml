- en: 9 Advanced data annotation and augmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 高级数据标注和增强
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Evaluating annotation quality for subjective tasks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估主观任务的标注质量
- en: Optimizing annotation quality control with machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习优化标注质量控制
- en: Treating model predictions as annotations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型预测视为标注
- en: Combining embeddings/contextual representations with annotations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将嵌入/上下文表示与标注相结合
- en: Using search and rule-based systems for data annotation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于搜索和基于规则的系统进行数据标注
- en: Bootstrapping models with lightly supervised machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用轻度监督机器学习来引导模型
- en: Expanding datasets with synthetic data, data creation, and data augmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过合成数据、数据创建和数据增强来扩展数据集
- en: Incorporating annotation information into machine learning models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标注信息纳入机器学习模型
- en: For many tasks, simple quality control metrics aren’t enough. Imagine that you
    need to annotate images for labels like “Cyclist” and “Pedestrian.” Some images,
    such as a person pushing a bicycle, are inherently subjective, and an annotator
    should not be punished for having a valid but minority interpretation. Some annotators
    will be more or less familiar with different data items, depending on their familiarity
    with the locations in the images and whether they themselves are cyclists. Machine
    learning can help estimate which annotator is expected to be more or less accurate
    on a given data point. Machine learning can also automate some of the annotation
    processes by presenting candidate annotations for faster human review. If there
    are some contexts with few or no cyclists, you might create new data items synthetically
    to fill the gaps. Knowing that perfect annotation is rare across an entire dataset,
    you may want to remove some items from the data before building a model on that
    data or incorporate the uncertainty into the downstream models. You may also want
    to perform exploratory data analysis on the dataset without necessarily wanting
    to build a downstream model. This chapter covers methods for addressing all these
    advanced problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多任务，简单的质量控制指标是不够的。想象一下，你需要为“骑自行车的人”和“行人”这样的标签标注图像。有些图像，如某人推着自行车，本质上具有主观性，标注者不应因持有有效但少数派的观点而受到惩罚。一些标注者可能对不同的数据项更熟悉或不太熟悉，这取决于他们对图像中位置的熟悉程度以及他们自己是否是骑自行车的人。机器学习可以帮助估计哪个标注者在特定数据点上可能更准确或不太准确。机器学习还可以通过展示候选标注来自动化一些标注过程，以便更快地进行人工审查。如果某些上下文中骑自行车的人很少或没有，你可能需要合成新的数据项来填补空白。鉴于在整个数据集中完美的标注很少见，你可能在基于该数据构建模型之前删除一些项目，或者将不确定性纳入下游模型。你也可能想在不需要构建下游模型的情况下对数据集进行探索性数据分析。本章涵盖了处理所有这些高级问题的方法。
- en: 9.1 Annotation quality for subjective tasks
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 主观任务的标注质量
- en: There is not always one single, correct annotation for a given task. You may
    have a task that is inherently subjective; therefore, you expect different responses.
    We can use our example data from chapter 8, reproduced here in figure 9.1, showing
    an item that may have multiple correct annotations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的任务，并不总是只有一个单一的正确标注。你可能有一个本质上具有主观性的任务；因此，你期望得到不同的响应。我们可以使用第 8 章的示例数据，如图
    9.1 所示，这里展示了可能具有多种正确标注的项目。
- en: '![](../Images/CH09_F01_Munro.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F01_Munro.png)'
- en: Figure 9.1 A copy of an image from chapter 8, showing how task 3 might have
    multiple valid interpretations because of the ambiguity between “Pedestrian” and
    “Cyclist.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 来自第 8 章的图像副本，展示了由于“行人”和“骑自行车的人”之间的歧义，任务 3 可能存在多种有效解释。
- en: There could be multiple reasons why “Pedestrian” or “Cyclist” is favored by
    one annotator over another, including
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么一个标注者比另一个标注者更喜欢“行人”或“骑自行车的人”，可能有多个原因，包括
- en: '*Actual context*—The person is currently on the road or this image is part
    of a video in which the person gets on or off the bike.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实际上下文*—此人目前正在路上，或者此图像是视频中此人上下自行车的片段。'
- en: '*Implied context*—The person looks as though they are getting on or off the
    bike.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐含上下文*—此人看起来像是在上下自行车。'
- en: '*Socially influenced variation*—It is likely that local laws treat a person
    on or off a bicycle differently in different parts of the world. Different laws
    specify whether bicycles are allowed on a footpath, a road, or a dedicated bike
    path, and whether people can push a bicycle in any of those places instead of
    riding it. The laws or common practices with which each annotator is familiar
    could influence their interpretation.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社会影响下的变化*—在世界不同地区，地方法律可能对骑自行车的人和非骑自行车的人有不同的对待。不同的法律规定了自行车是否可以在人行道、道路上或专用自行车道上行驶，以及人们是否可以在这些地方推自行车而不是骑自行车。每个注释员所熟悉的法律或常见做法可能会影响他们的解释。'
- en: '*Personal experience*—We might expect people who are themselves cyclists to
    give different answers from people who are not.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*个人经验*—我们可能预计自己是骑自行车者的人给出的答案与不是骑自行车者的人给出的答案不同。'
- en: '*Personal variation*—Irrespective of social influences and personal experience,
    two people may have different opinions about the difference between a pedestrian
    and a cyclist.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*个人差异*—不考虑社会影响和个人经验，两个人可能对行人与骑自行车者之间的区别有不同的看法。'
- en: '*Linguistic variation*—A cyclist could be strictly interpreted as “anyone who
    cycles” instead of “someone who is currently cycling,” especially if the annotators
    don’t speak English as a first language (common among crowdsourced and outsourced
    annotators) and the translation of *cyclist* into their first language(s) is not
    the same definition as in English.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言变化*—骑自行车者可以严格解释为“任何骑自行车的人”，而不是“目前正在骑自行车的人”，特别是如果注释员不把英语作为第一语言（这在众包和外包注释员中很常见），并且将“骑自行车者”翻译成他们的第一语言（们）的定义与英语中的定义不同。'
- en: '*Ordering effects*—A person might be primed to interpret this image as a cyclist
    or pedestrian based on having seen more of one type or the other in the previous
    annotations.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*顺序效应*—一个人可能会根据之前注释中看到的一种类型或另一种类型更多，而将此图像解释为骑自行车者或行人。'
- en: '*Desire to conform to normality*—A person might themselves think that this
    image is a cyclist but also think that most other people would call it a pedestrian.
    They might choose the answer that they don’t believe in for fear of being penalized
    afterward.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*希望符合正常性*—一个人可能自己认为这张照片是骑自行车者的，但也认为大多数人会称之为行人。他们可能会选择他们不相信的答案，以免之后受到惩罚。'
- en: '*Perceived power imbalances*—A person who thinks that you are collecting this
    data to help with safety for cyclists might favor “Cyclist” because they think
    that you prefer this answer. This kind of accommodation and power imbalance between
    the annotator and the person who created the task can be important for tasks with
    obvious negative answers, such as sentiment analysis.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*感知到的权力不平衡*—认为你收集这些数据是为了帮助骑自行车者安全的人可能会选择“骑自行车者”，因为他们认为你更喜欢这个答案。这种注释员和任务创建者之间的这种适应和权力不平衡对于具有明显负面答案的任务（如情感分析）可能很重要。'
- en: '*Genuine ambiguity*—The photo may be low-resolution or out of focus and not
    clear.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真正的歧义*—照片可能是低分辨率或模糊不清的。'
- en: It may be possible to have detailed guidelines for how our example image should
    be interpreted, which will mean that there is one objective correct answer. This
    will not be the case with all datasets, however, and it is often hard to anticipate
    all the edge cases in advance. So we often want to capture subjective judgments
    in the best way possible to ensure that we collect the full diversity of possible
    responses.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有关于如何解释我们的示例图像的详细指南，这意味着有一个客观正确的答案。然而，并非所有数据集都会是这样，而且通常很难提前预见到所有边缘情况。因此，我们通常希望以最佳方式捕捉主观判断，以确保我们收集到所有可能的响应的全面多样性。
- en: In our example in this chapter, we will assume that there is a set of correct
    answers. For open-ended tasks, this assumption is much harder, and expert review
    is much more important in these cases. See the following expert anecdote for an
    example of what can go wrong when you don’t take subjectivity into account for
    open-ended tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的示例中，我们将假设存在一组正确答案。对于开放式任务，这个假设要困难得多，在这些情况下，专家审查尤为重要。参见以下专家轶事，了解在开放式任务中没有考虑主观性可能导致的问题。
- en: In our example dataset, one thing we know from our example image is that “Animal”
    and “Sign” are not correct answers, so we want an approach to subjective quality
    control that identifies “Pedestrian” and “Cyclist” as valid answers, but not “Animal”
    and “Sign.”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例数据集中，从我们的示例图像中我们知道“动物”和“标志”不是正确答案，因此我们希望有一种主观质量控制方法，能够将“行人”和“骑自行车者”识别为有效答案，而不是“动物”和“标志”。
- en: Annotation bias is no joke
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 标注偏差不是玩笑
- en: '*Expert anecdote by Lisa Braden-Harder*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*丽莎·布拉登-哈德尔的专家轶事*'
- en: Data scientists usually underestimate the effort needed to collect high-quality,
    highly subjective data. Human agreement for relevance tasks is not easy when you
    are trying to annotate data without solid ground truth data, and engaging human
    annotators is successful only with strongly communicated goals, guidelines, and
    quality control measures, especially important when working across languages and
    cultures.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通常低估了收集高质量、高度主观数据所需的努力。在没有坚实的事实数据的情况下尝试标注数据时，人类对相关任务的同意并不容易，并且只有当目标、指南和质量控制措施得到充分沟通时，才能成功吸引人类标注员，这在跨语言和文化工作尤其重要。
- en: I once had a request for Korean knock-knock jokes from a US personal-assistant
    company expanding into South Korea. The conversation wasn’t to explain to the
    product manager why that wouldn’t work and to find culturally appropriate content
    for their application; it unraveled a lot of assumed knowledge. Even among Korean
    speakers, the annotators creating and evaluating the jokes needed to be from the
    same demographics as the intended customers. This case is one example of why the
    strategies to mitigate bias will touch every part of your data pipeline, from
    guidelines to compensation strategies that target the most appropriate annotation
    workforce. Annotation bias is no joke!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经接到一个来自美国个人助理公司扩展到韩国的请求，要求提供韩式敲门笑话。对话不是为了向产品经理解释为什么这不会奏效，以及为他们应用程序找到文化上合适的内
    容；它揭示了许多假设的知识。即使在韩国语使用者中，创建和评估笑话的标注员也需要与目标客户来自相同的群体。这个案例是为什么减轻偏差的策略将触及您数据管道的每一个部分的一个例子，从指南到针对最合适的标注工作力的补偿策略。标注偏差不是玩笑！
- en: '*Lisa Braden-Harder is a mentor at the Global Social Benefit Institute at Santa
    Clara University. She was founder and CEO of the Butler Hill Group, one of the
    largest and most successful annotation companies; and prior to that, she worked
    as a programmer for IBM and completed computer science degrees at Purdue and NYU*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*丽莎·布拉登-哈德是圣克拉拉大学全球社会效益研究所的导师。她是巴特勒山集团（Butler Hill Group）的创始人兼首席执行官，该集团是最大的、最成功的标注公司之一；在此之前，她曾担任IBM的程序员，并在普渡大学和纽约大学完成了计算机科学学位*。'
- en: 9.1.1 Requesting annotator expectations
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 请求标注员期望
- en: When multiple correct answers exist, the easiest way to understand the possible
    answers is to ask the annotators directly, and the best way to frame the task
    is to ask the annotators how they think other annotators might respond. Figure
    9.2 shows an example.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在多个正确答案时，理解可能答案的最简单方法就是直接询问标注员，而最好的任务框架方式就是询问标注员他们认为其他标注员可能会如何回答。图9.2展示了示例。
- en: '![](../Images/CH09_F02_Munro.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2](../Images/CH09_F02_Munro.png)'
- en: Figure 9.2 Asking people what they expect other annotators to choose for answers.
    Here, the annotator has indicated that they think that the image is a pedestrian
    and that 90% of annotators will agree with them, but 10% will think that it is
    a cyclist. This approach motivates people to give honest responses and provides
    data to help you decide when multiple responses are valid. In turn, we can capture
    more diversity in correct answers than the ones offered by any one annotator.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 询问人们他们期望其他标注员会选择哪些答案。在这里，标注员表示他们认为图像是行人，90%的标注员会同意他们，但10%的人会认为它是骑自行车的人。这种方法鼓励人们给出诚实的回答，并提供数据以帮助您决定何时多个回答是有效的。反过来，我们可以捕捉到比任何单个标注员提供的答案更多的正确答案的多样性。
- en: 'The interface is similar to the example in chapter 8 in which we asked annotators
    to give their own confidence for each label, but here, we are asking them about
    other annotators. This relatively simple change has several desirable properties:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 界面与第8章中的示例类似，我们在其中要求标注员为每个标签给出自己的信心度，但在这里，我们是在询问他们关于其他标注员的问题。这种相对简单的变化有几个可取的特性：
- en: The task design explicitly gives people permission to give an answer that they
    don’t think is the majority answer, which encourages diverse responses and reduces
    the pressure to conform.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务设计明确允许人们给出他们认为不是多数答案的答案，这鼓励了多样化的回答，并减少了从众的压力。
- en: You can overcome some limitations in annotator diversity. It may not be possible
    to have an annotator from every demographic that you care about to look at every
    single item. With this method, you need only annotators who have the right intuition
    about the full diversity of responses, even if they do not share every interpretation.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以克服注释者多样性的一些限制。可能不可能让每个您关心的群体中的注释者查看每个单独的项目。使用这种方法，您只需要有对全面多样性反应有正确直觉的注释者，即使他们不共享每个解释。
- en: Problems with perceived power dynamics are reduced because you are asking what
    other annotators think, which makes it easier to report negative responses. This
    strategy can be a good one when you think that power dynamics or personal biases
    are influencing responses. Ask what most people would respond instead of what
    that annotator thinks.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于您询问了其他注释者的看法，因此感知到的权力动态问题减少了，这使得报告负面反应更容易。当您认为权力动态或个人偏见正在影响反应时，这种策略可能是一个好策略。询问大多数人会如何反应，而不是询问注释者认为会如何。
- en: You can create data that separates valid from nonvalid answers. If we score
    every person’s actual answer as 100% for observed and know that they will divide
    their expected numbers across multiple labels, they will give less than a 100%
    score for their expected response for their actual response. So if the actual
    scores for a label exceed the expected scores, we can trust that label, even if
    it has low overall percentages of actual and expected.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以创建数据来区分有效答案和非有效答案。如果我们将每个人的实际答案评分设为100%对于观察到的，并且知道他们将预期数字分配到多个标签中，他们将为他们预期的实际响应给出低于100%的评分。因此，如果一个标签的实际评分超过了预期评分，我们可以相信这个标签，即使它在实际和预期的整体百分比中占比较低。
- en: 'The last is a lesser-known principle of Bayesian reasoning: people tend to
    undervalue the probability of their own response. For this reason, we’ll look
    at a popular method called Bayesian Truth Serum in section 9.4.1.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是贝叶斯推理的一个不太为人所知的原则：人们往往低估自己反应的概率。因此，在9.4.1节中，我们将探讨一个称为贝叶斯真理血清的流行方法。
- en: 9.1.2 Assessing viable labels for subjective tasks
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 评估主观任务的可行标签
- en: To start our analysis of viable labels, we can calculate the likelihood that
    we would have seen each of the labels among the actual annotations, given the
    number of annotators who have worked on the task. This information will help us
    decide which labels are valid. If a valid label is expected to occur in only 10%
    of annotations for a task, but we have only one or two annotators, we wouldn’t
    expect to see an actual annotation for that label.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始分析可行的标签，我们可以计算在给定参与该任务的注释者数量的情况下，我们可能会看到每个标签在实际注释中的可能性。这些信息将帮助我们决定哪些标签是有效的。如果一个有效标签预计只会在任务的10%注释中出现，但我们只有一两个注释者，我们就不期望看到该标签的实际注释。
- en: We calculate the probability that we should have seen each label by using the
    product of the expected probabilities. Just like when we calculate agreement,
    we use the complement of the expected annotation percentages. The complement of
    the expected percentage is calculating the probability that no one annotated a
    given label, and the probability that at least one person chose that annotation
    is the complement. Figure 9.3 shows the calculations for our example data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过计算预期概率的乘积来计算我们应该看到每个标签的概率。就像我们计算一致性时一样，我们使用预期注释百分比的补集。预期百分比的补集是计算没有人注释给定标签的概率，至少有一个人选择该注释的概率是补集。图9.3显示了我们的示例数据的计算。
- en: '![](../Images/CH09_F03_Munro.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F03_Munro.png)'
- en: Figure 9.3 Testing whether a subjective label is viable. Here, the five annotators
    reported their annotation for the label and what percentage of people they think
    would choose each label. Blake thinks that the label is “Pedestrian”, that 90%
    of people would choose “Pedestrian,” and that 5% each would choose “Cyclist” and
    “Animal.” Taking the product of the complement gives us the probability that we
    should have encountered this label with this number of annotations, which we can
    compare with whether we have seen the label.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 测试一个主观标签是否可行。在这里，五位注释者报告了他们对该标签的注释以及他们认为有多少人会选择每个标签。布莱克认为该标签是“行人”，90%的人会选择“行人”，5%的人会选择“骑自行车者”和“动物”。通过计算补集的乘积，我们可以得到在这么多注释下遇到这个标签的概率，我们可以将其与我们是否看到这个标签进行比较。
- en: 'Figure 9.3 shows that for this task, annotators have selected two labels as
    being the most likely: “Pedestrian” and “Cyclist” (the same as for our example
    data) and that people variously believe that “Sign” and “Animal” will be selected
    by 0% or 5% of people. You can find a copy of the spreadsheet in figure 9.3 and
    all the other examples in this chapter at [http://mng.bz/Vd4W](http://mng.bz/Vd4W).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3显示，对于这个任务，标注员选择了两个最可能的标签：“行人”和“骑自行车者”（与我们的示例数据相同），并且人们认为“标志”和“动物”将被0%或5%的人选择。您可以在图9.3中找到电子表格的副本，以及本章中所有其他示例的电子表格，链接为[http://mng.bz/Vd4W](http://mng.bz/Vd4W)。
- en: 'First, let’s imagine that no one chose “Pedestrian” as the actual annotation,
    but people still gave some weight to “Pedestrian” in their expected score. Here
    are the calculations from figure 9.3:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们想象一下，没有人选择“行人”作为实际标注，但人们在他们的预期得分中仍然给予“行人”一些权重。以下是图9.3中的计算结果：
- en: 'Expected: [0.8, 0.9, 0.35, 0.2, 0.6]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 预期：[0.8, 0.9, 0.35, 0.2, 0.6]
- en: 'Not Expected: [0.2, 0.1, 0.65, 0.8, 0.4]'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不期望：[0.2, 0.1, 0.65, 0.8, 0.4]
- en: Product of Not Expected = 0.004
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不期望的乘积 = 0.004
- en: Probability Seen = 1 – 0.004 = 0.996
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 被看到的概率 = 1 – 0.004 = 0.996
- en: With those expected scores, we are 99.6% certain that we should have seen at
    least one actual “Pedestrian.” So we could be fairly certain that this result
    was an error in the annotators’ perception. When there is a high probability that
    a label will be seen, according to the expected annotations, but has not been
    seen, we can more confidently rule it out as a viable label.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些预期得分，我们有99.6%的把握认为我们应该至少看到一个实际的“行人”。因此，我们可以相当确信这个结果是标注员感知上的错误。当根据预期标注，一个标签有很高的概率被看到，但实际上没有被看到时，我们可以更有信心将其排除为可行的标签。
- en: 'Now let’s look at one of the less expected labels in figure 9.3: “Animal.”
    Although three annotators believe that some people will annotate the image as
    “Animal,” there is a 14.3% chance that one of the five annotators will have chosen
    “Animal.” The fact that no one has chosen “Animal” yet doesn’t necessarily rule
    it out. We wouldn’t have expected to see someone choose “Animal” with only five
    annotators, if we trust these numbers, and wouldn’t expect to see one until about
    20 annotators have seen this item. We can take several approaches can to discover
    whether “Animal” is a viable label, each with increasing complexity:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看图9.3中一个不太预期的标签：“动物”。尽管三位标注员相信有些人会将图像标注为“动物”，但有14.3%的可能性，五位标注员中的一人会选择“动物”。到目前为止还没有人选择“动物”的事实并不一定排除它。如果我们信任这些数字，我们不会期望看到有人选择“动物”，直到大约有20位标注员看到这个项目。我们可以采取几种方法来发现“动物”是否是一个可行的标签，每种方法都越来越复杂：
- en: Add more annotators until “Animal” is seen or the probability seen is so high
    that we can rule out “Animal” as a viable label.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多标注员，直到“动物”被看到，或者看到的概率如此之高，以至于我们可以排除“动物”作为一个可行的标签。
- en: Trust an expert annotator to decide whether “Animal” is a viable label when
    that expert annotator is experienced at putting personal biases aside.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当这位专家标注员有经验排除个人偏见时，信任这位专家标注员来决定“动物”是否是一个可行的标签。
- en: Find annotators who correctly annotated items as “Animal” in the ground truth
    data when that annotation was rare but correct and give this task to them (a programmatic
    way to find the best nonexpert).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到在真实数据中将项目正确标注为“动物”的标注员，当这种标注很少但正确时，将这项任务分配给他们（一种找到最佳非专家的程序化方法）。
- en: Although the first option is easiest to implement, it works only if you are
    confident in the diversity of your annotators. There may be people who would correctly
    choose “Animal,” but they are not among your annotators, so that situation never
    arises. On the other hand, it might be objectively incorrect to choose “Animal,”
    but this example is a tough one, and 5% of people would be expected to get it
    wrong. You probably don’t want to select “Animal” in this case.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第一个选项最容易实现，但它只在你对标注员的多样性有信心时才有效。可能会有一些人会正确选择“动物”，但他们不在你的标注员中，所以这种情况从未出现。另一方面，选择“动物”可能是客观上不正确的，但这个例子很难，预计有5%的人会答错。在这种情况下，你可能不想选择“动物”。
- en: Therefore, when there is ambiguity about whether a label is valid for a subjective
    task, you will want to find another annotator (possibly an expert) who can be
    trusted to understand the diversity of possible responses.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当关于一个标签是否适用于主观任务存在歧义时，你将希望找到另一个标注员（可能是专家），可以信任他们理解可能的多样化响应。
- en: 9.1.3 Trusting an annotator to understand diverse responses
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 信任标注员理解多样化的响应
- en: We can calculate our trust in an individual annotator’s expected annotations
    by looking at the difference between their expected annotations and the actual
    annotations calculated across all annotators. The basic concept is simple. If
    an annotator expected a 50:50 split of annotations between two labels and was
    correct that there was a 50:50 split, that annotator should get a score of 100%
    for that task.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看单个注释者预期的注释与所有注释者计算的实际注释之间的差异来计算我们对单个注释者预期注释的信任度。基本概念很简单。如果一个注释者预期两个标签之间的注释是50:50的分割，并且正确地指出确实存在50:50的分割，那么该注释者应该在这个任务上得到100%的分数。
- en: If there were an odd number of annotators, a 50:50 split wouldn’t be possible,
    so we need to take into account the possible precision given the finite number
    of annotators. Figure 9.4 illustrates a slightly more complicated example.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果注释者的数量是奇数，那么50:50的分割是不可能的，因此我们需要考虑有限注释者数量下的可能精度。图9.4展示了稍微复杂一点的例子。
- en: '![](../Images/CH09_F04_Munro.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F04_Munro.png)'
- en: Figure 9.4 The accuracy of one annotator to estimate the range of responses
    across all annotators by comparing the actual fraction of annotations for a given
    label with the number of annotations expected by an annotator. For our example
    data, this would correspond to Cameron’s expectation that 65% of people would
    choose “Cyclist” for this task, compared with the 40% of people who actually chose
    it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 一个注释者通过比较给定标签的实际注释分数与注释者预期的注释数量来估计所有注释者响应范围的准确性。对于我们的示例数据，这对应于Cameron预计65%的人会选择“Cyclist”这个任务，而实际上选择它的人占40%。
- en: In figure 9.4, the annotator overestimated the number of annotators by 0.25\.
    Every value between 0.15 and 0.65 is closer to the actual number of 0.4, and 0.65
    – 0.15 = 0.5\. So 50% of the possible expected values are closer to 0.4\. Given
    enough annotators, however, the true actual value would be higher than 0.4, so
    we adjust by the minimum precision of 0.2, which gives us 0.5 * (1 – 0.2) + 0.2
    = 0.6\. The annotator’s accuracy score is 60%.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.4中，注释者高估了注释者的数量为0.25。在0.15到0.65之间的每个值都更接近实际数量0.4，而0.65 – 0.15 = 0.5。所以50%的可能预期值都更接近0.4。然而，如果有足够的注释者，那么真实实际值将高于0.4，因此我们通过最小精度0.2进行调整，得到0.5
    * (1 – 0.2) + 0.2 = 0.6。注释者的准确度分数是60%。
- en: Figure 9.5 gives the calculation for every estimate by every annotator in our
    example data. To get the overall accuracy for an annotator, you average their
    accuracy across every subjective task in the dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5给出了我们示例数据中每个注释者每个估计的计算方法。为了得到一个注释者的整体准确度，你需要平均他们在数据集中每个主观任务上的准确度。
- en: '![](../Images/CH09_F05_Munro.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F05_Munro.png)'
- en: Figure 9.5 Calculating the accuracy of each annotator’s estimate as the adjusted
    score and then averaging those scores to get a per-annotator score for this task.
    Cameron is 80% accurate in estimating how close the expected distribution was
    to the actual distribution. Evan is the most accurate, with a score of 97%, and
    Blake is the least accurate, with a score of 73%.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5展示了每个注释者每个估计的计算方法，然后平均这些分数以得到这个任务每个注释者的分数。Cameron在估计预期分布与实际分布的接近程度方面准确度为80%。Evan是最准确的，得分为97%，而Blake是最不准确的，得分为73%。
- en: In figure 9.5, epsilon is the same epsilon used in Krippendorff’s alpha in chapter
    8\. It wasn’t important then, because Krippendorff’s alpha calculated epsilon
    over the total number of annotations in the dataset. Here, we are calculating
    epsilon over the annotations within a single task. You can see by comparing the
    raw score and adjusted score that epsilon makes a big difference, adjusting the
    results by 20%.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.5中，epsilon与第8章中Krippendorff的alpha所使用的epsilon相同。当时它并不重要，因为Krippendorff的alpha是在数据集中所有注释的总数上计算epsilon的。在这里，我们是在单个任务内的注释上计算epsilon。通过比较原始分数和调整后的分数，你可以看到epsilon有很大的影响，调整结果达20%。
- en: You can use several variations and extensions if it is especially important
    to know how accurately your annotators can estimate the actual distributions.
    A 0 score won’t be possible for some tasks because the distribution of each annotator’s
    expected annotations has to add up to 1; therefore, they can’t always provide
    the worst estimate for every label. (In figure 9.5, the worst possible score is
    0.44 if an annotator expected that only “Animal” or “Sign” would be chosen.) You
    could normalize for this baseline, as for ground truth accuracy and agreement
    in chapter 8.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特别重要的是要知道你的注释者如何准确地估计实际分布，你可以使用几种变体和扩展。对于某些任务，0分是不可能的，因为每个注释者的预期注释分布总和必须为1；因此，他们不能总是为每个标签提供最差的估计。（在图9.5中，如果注释者预期只有“动物”或“标志”会被选择，最坏的可能分数是0.44。）你可以为此基线进行归一化，就像在第8章中为真实准确性和一致性那样。
- en: Cross-entropy is another way to calculate the difference between the expected
    and the actual distributions. Although cross-entropy is a common way to compare
    probability distributions in machine learning, I have never seen it used to compare
    actual and expected annotations for training data. This technique would be an
    interesting area of research.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是计算预期分布和实际分布之间差异的另一种方法。尽管交叉熵是机器学习中比较概率分布的常见方法，但我从未见过它被用来比较训练数据的实际和预期注释。这项技术将是一个有趣的研究领域。
- en: 9.1.4 Bayesian Truth Serum for subjective judgments
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.4 贝叶斯真理血清法用于主观判断
- en: The method in section 9.1.3 focused on how accurately each annotator predicted
    the frequency of different subjective judgments, but the scores did not take into
    account the actual annotation from each annotator—only their expected scores.
    Bayesian Truth Serum (BTS) is a method that combines the two approaches. BTS was
    created by Dražen Prelec at MIT (see the *Science* paper in section 9.9.1) and
    was the first metric to combine the actual and expected annotations into a single
    score.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 9.1.3节中的方法专注于每个注释者预测不同主观判断频率的准确性，但分数没有考虑每个注释者的实际注释——只有他们的预期分数。贝叶斯真理血清法（BTS）是一种结合两种方法的方法。BTS由麻省理工学院的Dražen
    Prelec创建（参见9.9.1节中的*Science*论文）并且是第一个将实际和预期注释结合成一个单一分数的度量标准。
- en: BTS calculates the score from an information-theoretic point of view. This score
    does not allow you to interpret the accuracy of an annotator or label directly.
    Therefore, BTS looks for responses that are more common than collectively predicted
    by the same annotators, which will not necessarily be the most frequent responses.
    Figure 9.6 shows an example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: BTS从信息论的角度计算分数。这个分数不能直接让你解释注释者或标签的准确性。因此，BTS寻找比相同注释者集体预测更常见的响应，这些响应不一定是最频繁的响应。图9.6展示了示例。
- en: '![](../Images/CH09_F06_Munro.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F06_Munro.png)'
- en: Figure 9.6 BTS combines the person’s actual annotation with the predictions
    for expected annotations into a single score. Info is the information theoretic
    score (Expected × log(Actual / Expected)). The scores for each annotator show
    Cameron with the highest score. For both the expected and the actual annotations,
    the scores are based on information theory. The score is not only about the accuracy
    of each annotator; it is also about how much information each annotator is providing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6中，BTS将个人的实际注释与预期注释的预测结合成一个单一分数。Info是信息论分数（预期 × log(实际 / 预期))。每个注释者的分数显示Cameron的分数最高。对于预期和实际注释，分数都是基于信息理论的。分数不仅关于每个注释者的准确性；还关于每个注释者提供的信息量。
- en: In figure 9.6, Cameron has the highest score from a BTS point of view, primarily
    because there is high information from choosing “Cyclist” as the actual annotation.
    That is, the actual annotation frequency for “Cyclist” was higher than the expected
    frequency compared with “Pedestrian.” Blake has the lowest score, primarily because
    of the prediction that 0.9 of annotations would be “Pedestrian” when only 0.6
    were—the largest error of all the predictions. So our dataset in this section
    is a good example of a case in which the less-frequent subjective label provided
    more information than the more-frequent label. In some cases, however, the highest-frequency
    actual label can provide the most information.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.6中，Cameron从BTS的角度来看得分最高，主要是因为选择“Cyclist”作为实际标注提供了高信息量。也就是说，“Cyclist”的实际标注频率高于与“Pedestrian”相比的期望频率。Blake得分最低，主要是因为预测只有0.6的标注将是“Pedestrian”，而实际上有0.9的标注是——“Pedestrian”——这是所有预测中最大的误差。因此，本节中的数据集是一个很好的例子，说明较少出现的标签提供了比频繁出现的标签更多的信息。然而，在某些情况下，最高频率的实际标签可以提供最多的信息。
- en: Figure 9.6 is also a good example of how information differs from accuracy.
    Recall that in figure 9.5, Evan had the highest score because Evan’s expected
    annotation frequencies were closest to the actual annotation frequencies. For
    BTS, Cameron ended up with the highest score because even though Cameron was less
    accurate than Evan, there was more value in Cameron’s predictions about “Cyclist,”
    the less-frequent label that might have been overlooked.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6也是一个很好的例子，说明了信息与准确性的不同。回想一下，在图9.5中，Evan得分最高，因为Evan的期望标注频率与实际标注频率最接近。对于BTS，Cameron最终得分最高，尽管Cameron的准确性不如Evan，但Cameron对“Cyclist”（一个可能被忽视的较少出现的标签）的预测更有价值。
- en: If you consistently find that an annotator with the highest information score
    via BTS is not the annotator with the highest accuracy in predicting expected
    annotation frequencies, this finding can be evidence of a lack of diversity in
    your annotators. Check whether the annotator with the highest BTS score is typically
    selecting the less-frequent label; if so, you have evidence that your annotator
    pool is choosing the most-frequent label more often than a random or representative
    population would.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你持续发现通过BTS获得最高信息分数的标注者并不是在预测期望标注频率方面最准确的标注者，这一发现可以成为你标注者多样性不足的证据。检查一下BTS分数最高的标注者是否通常选择较少出现的标签；如果是这样，那么你有证据表明你的标注者池更频繁地选择最频繁出现的标签，这比随机或代表性人群更常见。
- en: In an interesting extension to BTS, the inventors observed that when the actual
    percentage of annotations exceeds the average expected percentage for a label,
    this finding is good evidence that the surprisingly popular label is the correct
    one, even if it is not the majority. But this result relies on having enough annotators
    that at least one annotator gets chooses that label, which is unlikely for rare
    but valid labels when you have only a few annotators per task.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在对BTS的一个有趣扩展中，发明者观察到，当实际标注的百分比超过某个标签的平均期望百分比时，这一发现是证据，表明这个意外受欢迎的标签是正确的，即使它不是多数。但这个结果依赖于有足够的标注者，至少有一位标注者选择了该标签，当你每个任务只有少数标注者时，对于罕见但有效的标签，这种情况不太可能发生。
- en: Note that we are not adjusting the BTS score in figure 9.6 for the fact that
    there are only five annotators, so only multiples of 0.2 were possible (epsilon
    in figure 9.5). The example in this section is the original calculation for BTS,
    so for educational purposes, it is taught here as it appears in the literature.
    It would be fine to add this adjustment, but note that BTS has a nice symmetry
    that you will lose in that case; if the weight of expected and actual scores is
    set to 1, as in our example (equal weights), the BTS scores always add to 0\.
    This will not be the case if you adjust for precision, so you won’t be able to
    take advantage of this symmetry with this modification. See section 9.9 for more
    information about extensions to BTS.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在图9.6中没有调整BTS分数，因为只有五位标注者，所以只有0.2的倍数是可能的（图9.5中的epsilon）。本节中的例子是BTS的原始计算，出于教育目的，它在这里以文献中的形式呈现。添加这个调整是可以的，但请注意，BTS有一个很好的对称性，你可能会失去这种对称性；如果期望和实际分数的权重设置为1，就像我们的例子中那样（权重相等），BTS分数总是加起来为0。如果你调整了精确度，情况就不会是这样，因此你将无法利用这种对称性进行这种修改。有关BTS扩展的更多信息，请参阅第9.9节。
- en: 9.1.5 Embedding simple tasks in more complicated ones
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.5 将简单任务嵌入更复杂的任务中
- en: If none of the previous techniques for subjective data works, one simple solution
    is to create an additional question for your task that is not subjective and assume
    that if an annotator gets that response correct, their subjective label is also
    valid. Figure 9.7 shows an example.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果之前的主观数据技术都不奏效，一个简单的解决方案是为你的任务创建一个额外的非主观问题，并假设如果标注者正确回答该问题，他们的主观标签也是有效的。图9.7展示了这样一个例子。
- en: '![](../Images/CH09_F07_Munro.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F07_Munro.png)'
- en: Figure 9.7 A subjective task with an additional question that is objective.
    This example allows easier quality control by assuming that if a person gets the
    objective question correct, their subjective judgment is also correct and not
    an error.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 一个带有附加客观问题的主观任务。这个例子通过假设如果一个人正确回答了客观问题，他们的主观判断也是正确的，而不是错误，从而使得质量控制更容易。
- en: 'In figure 9.7, we are asking an additional question about whether the sky can
    be seen in the message. Unlike the object type, this question should be unambiguous
    and objective: the sky is either visible or not. Therefore, we can easily test
    whether people are getting the product question correct by embedding known answers
    for some questions and/or by looking for agreement between annotators, using the
    techniques discussed in this chapter. Then we assume that the people are equally
    accurate for the subjective task.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.7中，我们询问了一个关于信息中是否可以看到天空的附加问题。与物体类型不同，这个问题应该是明确和客观的：天空要么可见，要么不可见。因此，我们可以通过嵌入已知答案来测试人们是否正确回答了产品问题，或者通过使用本章讨论的技术寻找标注者之间的共识，来轻松地测试人们是否正确回答了这个问题。然后我们假设人们在主观任务上的准确性是相同的。
- en: When using this method, we rely on the assumption that accuracy for the simpler
    objective task will strongly correlate with accuracy for the subjective task,
    which will be more or less true depending on your data. As a general principle,
    the closer the question is to the relevant content, the closer this correlation
    should be. In our example, we are asking about the context of the object, so the
    accuracy should be highly correlated.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这种方法时，我们依赖于这样一个假设，即简单客观任务的准确性将与主观任务的准确性高度相关，这将在很大程度上取决于你的数据。作为一个一般原则，问题与相关内容越接近，这种相关性应该越强。在我们的例子中，我们询问的是物体的上下文，因此准确性应该高度相关。
- en: This approach is most effective when the actual task is time-consuming. If you
    were asking someone to type a summary of a large passage, which typically would
    take many minutes, there is little additional annotation cost to ask an additional
    objective question about the passage.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当实际任务耗时较长时，这种方法最为有效。如果你要求某人键入一段长文章的摘要，这通常需要很多分钟，那么询问关于文章的附加客观问题几乎不会增加额外的标注成本。
- en: 9.2 Machine learning for annotation quality control
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 标注质量控制的机器学习
- en: 'Because most quality control strategies for data annotations are statistically
    driven decision processes, machine learning can be used for the quality control
    process itself. In fact, most of the heuristics in this chapter and chapter 8
    can be modeled as machine learning problems that are trained on held-out data.
    Four types of machine learning-driven quality controls are introduced here, all
    of which use the annotator’s performance on ground truth data and/or agreement
    as training data:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数数据标注的质量控制策略都是基于统计的决策过程，因此机器学习可以用于质量控制过程本身。实际上，本章和第8章中的大多数启发式方法都可以建模为在保留数据上训练的机器学习问题。这里介绍了四种由机器学习驱动的质量控制类型，所有这些类型都使用标注者在真实数据上的表现和/或共识作为训练数据：
- en: Treating the model predictions as an optimization task. Using the annotator’s
    performance on the ground truth data, find a probability distribution for the
    actual label that optimizes a loss function.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型预测视为一个优化任务。使用标注者在真实数据上的表现，找到一个概率分布，该分布优化了损失函数。
- en: Creating a model that predicts whether a single annotation by an annotator is
    correct or incorrect.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个模型来预测标注者的一次标注是正确还是错误。
- en: Creating a model that predicts whether a single annotation by an annotator is
    likely to be in agreement with other annotators.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个模型来预测标注者的一次标注是否可能与其他标注者达成一致。
- en: Predicting whether an annotator is actually a bot.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测标注者是否实际上是机器人。
- en: Some methods can be used independently or in combination. The following sections
    cover these methods in turn.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法可以独立使用或组合使用。以下各节将依次介绍这些方法。
- en: 9.2.1 Calculating annotation confidence as an optimization task
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 将标注置信度计算作为一个优化任务
- en: In chapter 8, you learned that you can take the average confidence across all
    labels. If the confidence in one annotator’s annotation was less than 100%, the
    remaining confidence was spread across the labels that the annotator didn’t choose.
    We can build on this approach by looking at all the annotators’ annotation patterns
    on the ground truth data and then treat our confidence as an optimization problem.
    Figure 9.8 shows an example.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章中，你学习了你可以取所有标签的平均置信度。如果一个标注者的标注置信度低于100%，剩余的置信度会分散到标注者未选择的标签上。我们可以通过查看所有标注者在真实数据上的标注模式来构建这种方法，然后将我们的置信度视为一个优化问题。图9.8展示了示例。
- en: '![](../Images/CH09_F08_Munro.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8](../Images/CH09_F08_Munro.png)'
- en: Figure 9.8 Using performance on the ground truth data to calculate model confidence
    as an optimization task. On the ground truth data, when Alex annotated items as
    “Pedestrian,” they were actually “Pedestrians” 91% of the time, “Signs” 1% of
    the time, “Cyclists” 4% of the time, and “Animals” 4% of the time. When we see
    that Alex has annotated some new item as “Pedestrian,” we can assume the same
    probability distribution. When Dancer annotates an item as “Cyclist,” we know
    that it is actually a “Pedestrian” 72% of the time, showing confusion about these
    categories.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 将在真实数据上的性能用作计算模型置信度的优化任务。在真实数据上，当Alex将项目标注为“行人”时，实际上91%的时间是“行人”，“标志”1%的时间，“骑自行车的人”4%的时间，以及“动物”4%的时间。当我们看到Alex将一些新项目标注为“行人”时，我们可以假设相同的概率分布。当Dancer将一个项目标注为“骑自行车的人”时，我们知道实际上有72%的时间是“行人”，显示出对这些类别的混淆。
- en: Figure 9.8 shows the actual distribution of the annotations in the ground truth
    data. If you have a small amount of ground truth data, you might consider smoothing
    this number with a simple smoothing method such as adding a constant (Laplace
    smoothing).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8显示了真实数据上标注的实际分布。如果你只有少量真实数据，你可能需要考虑使用简单的平滑方法（如添加常数）来平滑这个数字（拉普拉斯平滑）。
- en: A nice property of this approach, compared with the methods in chapter 8, is
    that you might not have to discard all annotations from a low accuracy annotator.
    Dancer is wrong most of the time in figure 9.8 because they are only correct 21%
    of the time when they annotate an item as “Cyclist.” There is useful information,
    however, in the fact that “Pedestrian” was the correct answer 72% of the time.
    So instead of removing Dancer from our annotations because of poor accuracy, we
    can keep their annotations and let them contribute to our overall confidence by
    modeling their accuracy.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与第8章中的方法相比，这种方法的一个优点是你可能不需要丢弃低准确度标注者的所有标注。在图9.8中，Dancer大多数时候都是错误的，因为他们在将项目标注为“骑自行车的人”时只有21%的时间是正确的。然而，在“行人”是正确答案的72%这一事实中，也存在有用的信息。所以，我们不是因为Dancer的低准确度而将其从我们的标注中移除，我们可以保留他们的标注，并通过模拟他们的准确度让他们为我们的整体置信度做出贡献。
- en: 'To calculate the overall confidence, you can take the average of these numbers,
    which would give 68.4% confidence in “Pedestrian,” 2.6% confidence in “Sign,”
    27.2% confidence in “Cyclist,” and 1.8% confidence in “Animal.” The average is
    only one way to calculate the overall confidence, however. You can also treat
    this task as an optimization task and find the probability distribution that minimizes
    a distance function, such as mean absolute error, mean squared error, or cross-entropy.
    If you come from a machine learning background, you will recognize these methods
    as loss functions, and you can think of this problem as a machine learning problem:
    you are optimizing for the least loss by finding a probability distribution that
    best matches the data.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算整体置信度，你可以取这些数字的平均值，这将给出“行人”68.4%的置信度，“标志”2.6%的置信度，“骑自行车的人”27.2%的置信度，以及“动物”1.8%的置信度。然而，平均值只是计算整体置信度的一种方法。你也可以将这个任务视为一个优化任务，找到最小化距离函数（如平均绝对误差、平均平方误差或交叉熵）的概率分布。如果你来自机器学习背景，你会认出这些方法为损失函数，你可以将这个问题视为一个机器学习问题：你通过找到一个最佳匹配数据的概率分布来优化最小损失。
- en: If you try different loss functions on our example data, you will find that
    they don’t differ much from the average. The biggest benefits from making this
    problem a machine learning problem is that you can incorporate information other
    than the annotations themselves into your confidence prediction.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在我们示例数据上尝试不同的损失函数，你会发现它们与平均值的差异不大。将这个问题变成机器学习问题最大的好处是你可以将除了标注本身之外的信息纳入你的置信度预测中。
- en: 9.2.2 Converging on label confidence when annotators disagree
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 当标注者意见不一致时收敛到标签置信度
- en: Building on the treatment of aggregation as a machine learning problem, we can
    use the ground truth data as training data. That is, instead of optimizing the
    probability distributions that are taken from the ground truth data, we can build
    a model that uses the ground truth data as labels. Figure 9.9 shows how the ground
    truth data example from chapter 8 can be expanded to show the feature representation
    for each ground truth item.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在将汇总作为机器学习问题处理的基础上，我们可以使用真实数据作为训练数据。也就是说，我们不是优化从真实数据中提取的概率分布，而是构建一个使用真实数据作为标签的模型。图9.9展示了第8章中的真实数据示例如何扩展以显示每个真实项目的特征表示。
- en: '![](../Images/CH09_F09_Munro.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F09_Munro.png)'
- en: Figure 9.9 Sparse feature representation, with ground truth data as training
    data. We can take every annotation on the ground truth dataset, and use the actual
    annotations as features and the ground truth labels as the labels for a machine
    learning model. Then we have a model that can predict the correct label and give
    a confidence associated with that prediction.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 稀疏特征表示，以真实数据作为训练数据。我们可以对真实数据集上的每个标注进行计数，并将实际的标注作为特征，将真实数据标签作为机器学习模型的标签。然后我们有一个可以预测正确标签并给出与该预测相关的置信度的模型。
- en: If we build a model with the data in figure 9.9, our model will learn to trust
    our annotators relative to their overall accuracy on the ground truth data. We
    don’t explicitly tell the model that the annotations have the same values as the
    labels; the model discovers the correlations itself.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用图9.9中的数据构建模型，我们的模型将学会根据标注者在真实数据上的整体准确度来信任标注者。我们没有明确告诉模型标注的值与标签相同；模型自己发现了相关性。
- en: The biggest shortcoming of this method is that people who have annotated more
    ground truth data will be weighted higher because their features (annotations)
    have appeared in more training data. You can avoid this result by having most
    of the ground truth data annotated early in the annotation process (a good idea
    in any case to determine accuracy and to fine-tune other processes) and by sampling
    an equal number of annotations per annotator in each training epoch when you build
    your model. You can also overcome this shortcoming by aggregating the number of
    labels but ignoring who did the annotations, as shown in figure 9.10.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的最大的缺点是，标注了更多真实数据的人会被赋予更高的权重，因为他们的特征（标注）出现在更多的训练数据中。你可以在标注过程中尽早标注大部分真实数据（无论如何都是一个好主意，以确定准确性和微调其他过程）以及在构建模型时，每个训练周期中为每个标注者采样相同数量的标注来避免这种结果。你还可以通过汇总标签数量但忽略谁进行了标注来克服这一缺点，如图9.10所示。
- en: '![](../Images/CH09_F10_Munro.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F10_Munro.png)'
- en: Figure 9.10 Dense (aggregate) feature representation, with ground truth data
    as training data. The features are the count of each label, thereby ignoring the
    identity of the annotator. We can take every annotation on the ground truth dataset,
    count each annotation as the features, and use the ground truth labels as the
    labels for a machine learning model. This example is more robust than the one
    shown in figure 9.9 when you don’t have many ground truth labels for many of your
    annotators.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 密集（汇总）特征表示，以真实数据作为训练数据。特征是每个标签的计数，从而忽略了标注者的身份。我们可以对真实数据集上的每个标注进行计数，将每个标注作为特征，并使用真实数据标签作为机器学习模型的标签。当你没有很多标注者的真实标签时，这个例子比图9.9中显示的例子更稳健。
- en: You may need to normalize the entries in figure 9.10 if your model expects a
    [0–1] range in feature values. For both the sparse representation in figure 9.9
    and the aggregated information in figure 9.10, you can experiment with using your
    confidence in each prediction instead of counting each annotation as 1\. This
    confidence score could be the self-reported confidence of the annotator, as demonstrated
    in chapter 8, or an expected distribution, as in the subjective judgments in section
    9.1\. You might also have a confidence metric for each annotator that is derived
    from their past work. Whatever number you experiment with, make sure that it’s
    not derived from the same ground truth data that you are about to train on, which
    would overfit your quality prediction model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型期望特征值在[0–1]范围内，你可能需要将图9.10中的条目进行归一化。对于图9.9中的稀疏表示和图9.10中的聚合信息，你可以尝试使用你对每个预测的信心而不是将每个注释计为1。这个信心分数可以是注释者自我报告的信心，如第8章所示，或者是一个预期的分布，如第9.1节中的主观判断。你可能还会有一个基于注释者过去工作的信心指标。无论你实验的是哪个数字，确保它不是从你即将训练的同一天真数据中得出的，这会导致你的质量预测模型过拟合。
- en: As with the sparse example, a single neuron or linear model should be enough
    to give reliable results for figure 9.10 and not overfit the data for the dense
    representation. You should start with a simpler model before experimenting with
    anything more complicated in any case.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 就像稀疏示例一样，一个单独的神经元或线性模型应该足以给出图9.10的可靠结果，而不会对密集表示的数据过拟合。在任何情况下，你应该在尝试任何更复杂的事情之前，从一个更简单的模型开始。
- en: At this point, you may be wondering why you can’t include both the sparse and
    aggregate information as features in a model. You can do this! You can create
    a model that uses these features plus any others that may be relevant for calculating
    how confidently we can aggregate multiple annotations. But even if you decide
    to take the “throw everything into a model” kitchen-sink approach to aggregation,
    you should use the feature representations in figures 9.9 and 9.10 as a baseline
    before you start experimenting with more complicated models and hyperparameter
    tuning.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你可能想知道为什么你不能将稀疏和聚合信息都作为特征包含在模型中。你可以做到这一点！你可以创建一个模型，使用这些特征以及任何可能相关的其他特征来计算我们能够有多自信地聚合多个注释。但即使你决定采取“把所有东西都扔进模型”的混合方法来进行聚合，在开始尝试更复杂的模型和超参数调整之前，你也应该使用图9.9和9.10中的特征表示作为基准。
- en: To evaluate how accurate this model is, you need to split your ground truth
    data into training and evaluation data so that you can evaluate the confidence
    on held-out data. If you are using anything more complicated than a linear model
    or single neuron, such that you are doing hyperparameter tuning, you also need
    a further split to create a validation set that you can use for tuning. Both the
    sparse and aggregate representations are compatible with using a model’s predictions
    as though it were an annotator. For the aggregate representation, you might think
    about whether you want to aggregate model predictions separately from human annotations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估这个模型的准确性，你需要将你的真实数据分成训练数据和评估数据，这样你就可以在保留数据上评估信心。如果你使用比线性模型或单个神经元更复杂的东西，即你正在进行超参数调整，你还需要进一步分割以创建一个用于调整的验证集。稀疏和聚合表示都可以与使用模型的预测作为注释者一样使用。对于聚合表示，你可能需要考虑你是否希望将模型预测与人类注释分开进行聚合。
- en: 9.2.3 Predicting whether a single annotation is correct
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 预测单个注释是否正确
- en: The most flexible way to use machine learning for quality control in annotation
    is as a binary classifier to predict whether an individual annotation was correct.
    The advantage of a simple classification binary task is that you can train a model
    on relatively little data. If you are training on ground truth data, you are unlikely
    to have much data to train on, so this approach allows you to get the most out
    of the limited data you have.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习进行注释质量控制最灵活的方式是将它作为一个二元分类器来预测单个注释是否正确。简单分类二元任务的优点是你可以使用相对较少的数据来训练模型。如果你正在使用真实数据进行训练，你不太可能有很多数据来训练，因此这种方法允许你最大限度地利用你拥有的有限数据。
- en: This method is especially useful if you have a small number of annotators per
    item. You may have the budget for only a single annotator to look at most of your
    items, especially if annotator is a subject-matter expert (SME) who is reliable
    most of the time. In this context, you want to identify the small number of cases
    in which the SME might be wrong, but you don’t have agreement information to help
    with this identification because you have only one annotation most of the time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个项目只有少数标注者，这种方法特别有用。你可能只有预算让一个标注者查看大部分项目，尤其是如果标注者是可靠的领域专家（SME）时。在这种情况下，你想要识别SME可能出错的小部分案例，但你没有帮助识别这些案例的协议信息，因为你大部分时间只有一个标注。
- en: The simplest implementation to start with would include the annotator’s identity
    and their annotation as the features, like in figure 9.9\. Therefore, this model
    will tell you which annotators are the strongest or weakest on particular labels
    in the ground truth data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的实现方式是从标注者的身份和他们的注释作为特征开始，如图9.9所示。因此，这个模型将告诉你哪些标注者在真实数据中的特定标签上最强或最弱。
- en: You can think about the additional features that might provide additional context
    as to whether an annotator might make an error. The features that you might try
    in the model, in addition to annotator identity and annotation, could include
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以考虑可能提供额外背景信息以确定标注者是否可能犯错误的其他特征。除了标注者身份和注释之外，你可能尝试在模型中使用的特征可能包括
- en: The number or percentage of annotators who agree with that annotation (where
    they exist)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同意该注释的标注者数量或百分比（如果存在）
- en: Metadata about the item being annotated (time, place, and other categories)
    and the annotator (relevant demographics, qualifications, experience on this task,
    and so on)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于被标注项的元数据（时间、地点和其他类别）以及标注者（相关人口统计信息、资质、在此任务上的经验等）
- en: Embeddings from the predictive model or other models
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测模型或其他模型的嵌入
- en: The metadata features can help your model identify areas where there might be
    biases or meaningful trends in annotation quality. If a metadata feature captured
    the time of day when a photo was taken, your model might be able to learn that
    photos taken at night are generally harder to annotate accurately. The same is
    true for your annotators. *is* your annotators are themselves cyclists, they might
    have biases about images that contain cyclists, and the model can learn about
    this bias.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据特征可以帮助你的模型识别可能存在偏见或注释质量中的有意义趋势的区域。如果元数据特征捕捉到了照片拍摄的时间，你的模型可能会学会夜间拍摄的照片通常更难准确标注。对于标注者也是如此。*如果*你的标注者本身是骑自行车的人，他们可能会对包含骑自行车者的图像有偏见，模型可以学会这种偏见。
- en: This approach works with subjective data too. If you have subjective data with
    multiple correct answers, each of those correct ones could be correct for the
    binary model. This technique is fairly flexible; it also works for many types
    of machine learning problems, as covered in chapter 10.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也适用于主观数据。如果你有多个正确答案的主观数据，这些正确答案中的每一个都可能适用于二元模型。这项技术相当灵活；它也适用于许多类型的机器学习问题，如第10章所述。
- en: Showing the correct ground truth answers to annotators
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 向标注者展示正确的真实答案
- en: 'You have the option of showing the correct answer to an annotator when they
    get it wrong. This review should improve that annotator’s performance, but it
    will also make it harder to evaluate that annotator’s accuracy. There is a trade-off
    in design: do you tell the annotator every time they make an error, making that
    annotator more accurate, or do you keep some or all ground truth items anonymous
    so that you can perform better quality control on that annotator’s performance?
    You may need to strike a balance.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当标注者答错时，你可以选择向他们展示正确答案。这次审查应该提高该标注者的表现，但也会使评估该标注者的准确性更难。在设计上存在权衡：你是否每次都告诉标注者他们犯的错误，从而使该标注者更准确，或者你保留一些或全部真实答案匿名，以便你可以更好地控制该标注者的表现？你可能需要找到一个平衡点。
- en: For models built on ground truth data, be careful using items for which the
    annotator has learned the correct answer. For example, an annotator might have
    made errors with ground truth data items that had a person pushing a bike. If
    the annotator was told about that error and given the correct answer, however,
    that annotator is less likely to make that same error later. Therefore, your quality
    control model might erroneously predict errors for that annotator on types of
    items for which they are now highly accurate.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于真实数据的模型，在使用标注者已经学会正确答案的项目时要小心。例如，一个标注者可能对有一个人推自行车的真实数据项目犯了错误。然而，如果该标注者被告知了那个错误并给出了正确答案，那么该标注者不太可能后来犯同样的错误。因此，你的质量控制模型可能会错误地预测该标注者在他们现在高度准确的项目类型上的错误。
- en: 9.2.4 Predicting whether a single annotation is in agreement
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 预测单个标注是否达成一致
- en: As an alternative to predicting whether the annotator is correct, you could
    predict whether the annotator agrees with other annotators. This approach can
    increase the number of training items, because you can train a model to predict
    agreement on all the items that have been annotated by multiple people, not only
    the ones in the ground truth data. This model is likely to be more powerful.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预测标注者是否正确的一种替代方法，你可以预测标注者是否与其他标注者达成一致。这种方法可以增加训练项的数量，因为你可以训练一个模型来预测所有由多个人标注的项目的一致性，而不仅仅是那些在真实数据中的项目。这个模型可能更强大。
- en: Predicting agreement can be useful to surface items for which disagreement was
    expected but didn’t occur. Perhaps it was by random chance that a small number
    of annotators agreed with one another. If you can predict confidently that disagreement
    should have occurred, even by annotators who didn’t work on that task, that finding
    can be evidence that additional annotation may be needed for that item.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 预测一致性对于揭示预期会有分歧但未发生的情况是有用的。也许是因为随机机会，少数标注者彼此达成了一致。如果你可以自信地预测分歧本应该发生，即使是由没有参与该任务的标注者，那么这一发现可以成为需要对该项目进行额外标注的证据。
- en: 'You can try both approaches: build one model that predicts when an annotator
    is correct, and build a separate model that predicts when an annotator will agree
    with other annotators. Then you can review the task or elicit additional annotations
    when an annotation is predicted to be an error or predicted to disagree with other
    annotations.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试两种方法：构建一个模型来预测标注者何时正确，以及构建一个独立的模型来预测标注者何时会与其他标注者达成一致。然后，当预测到标注是错误或预测到与其他标注者不一致时，你可以审查任务或引发额外的标注。
- en: 9.2.5 Predicting whether an annotator is a bot
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.5 预测标注者是否为机器人
- en: If you are working with anonymous annotators and discover that one annotator
    was really a bot that was scamming your work, you can make a binary classification
    task to identify other bots. If we discovered that Dancer in our annotation data
    is a bot, we might suspect that the same bot is posing as other human annotators.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你与匿名标注者合作，并发现其中一名标注者实际上是一个在欺骗你工作的机器人，你可以创建一个二元分类任务来识别其他机器人。如果我们发现我们的标注数据中的Dancer是一个机器人，我们可能会怀疑同一个机器人正在冒充其他人类标注者。
- en: If you are certain that some subset of annotators are human, their annotations
    can become human training data for your model. This approach effectively allows
    you to train a model to ask an annotator, “Are we human, or are we Dancer?”
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确信某些标注者子集是人类，他们的标注可以成为你模型的训练数据。这种方法实际上允许你训练一个模型来询问标注者，“我们是人类，还是我们是Dancer？”
- en: Sometimes, a bot is a good addition to the annotation team. Machine learning
    models can annotate the data or create data autonomously or in combination with
    humans. The rest of this chapter is devoted to methods that automate or semi-automate
    data annotation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，机器人可以作为标注团队的一个很好的补充。机器学习模型可以标注数据或自主创建数据，或与人类结合创建数据。本章的其余部分致力于介绍自动化或半自动化数据标注的方法。
- en: 9.3 Model predictions as annotations
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 模型预测作为标注
- en: The simplest approach to semi-automating annotation is to treat the model’s
    predictions as though the model was an annotator. This process is often called
    *semi-supervised learning*, although that term has been applied to pretty much
    any combination of supervised and unsupervised learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 半自动化标注的最简单方法是将模型的预测当作标注者。这个过程通常被称为*半监督学习*，尽管这个术语已经被应用于几乎任何监督和无监督学习的组合。
- en: You can trust a model’s predictions or incorporate the model’s predictions as
    one annotator among many. The two approaches have different implications for how
    you should treat model confidence and the workflows that you might implement to
    review the model’s output, so they are explored separately. You can also use your
    model predictions to look for potential errors in noisy data, which is covered
    in section 9.3.3.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以信任模型的预测，或者将模型的预测作为众多标注员中的一个。这两种方法对你如何处理模型置信度和可能实施的审查模型输出的工作流程有不同的影响，因此它们被分别探讨。你还可以使用你的模型预测来寻找噪声数据中的潜在错误，这将在第9.3.3节中介绍。
- en: Will we replace human annotators?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会取代人工标注员吗？
- en: Every few years since the 1990s, someone has claimed to have solved automated
    labeling. Thirty years later, however, we still need to label data for more than
    99% of supervised machine learning problems.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 自1990年代以来，每隔几年就有人声称解决了自动化标注问题。然而，三十年后，我们仍然需要为超过99%的监督式机器学习问题标注数据。
- en: There are two common problems with many academic papers about automated labeling,
    whether it is using model confidence, a rule-based system, or some other method.
    First, they almost always compare the auto-labeling methods with random sampling.
    As you saw in chapter 2, even a simple active learning system can quickly improve
    the accuracy of your model, so it can be difficult to evaluate the benefit compared
    with active learning from these papers. Second, the papers typically assume that
    the evaluation data already exists, which is true for academic datasets. In the
    real world, however, you still need to set up annotation processes to create your
    evaluation data, manage the annotators, create the annotation guidelines, and
    implement quality control on the annotations. If you are doing all this for your
    evaluation data, why not put in the extra effort in the annotation component to
    create training data too?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自动标注的许多学术论文存在两个常见问题，无论是使用模型置信度、基于规则的系统还是其他方法。首先，它们几乎总是将自动标注方法与随机抽样进行比较。正如你在第二章中看到的，即使是一个简单的主动学习系统也能迅速提高你模型的准确性，因此与这些论文中的主动学习相比，评估其益处可能很困难。其次，这些论文通常假设评估数据已经存在，这在学术数据集中是正确的。然而，在现实世界中，你仍然需要设置标注流程来创建你的评估数据，管理标注员，制定标注指南，并在标注上实施质量控制。如果你为你的评估数据做所有这些，为什么不在标注组件上投入额外的努力来创建训练数据呢？
- en: The reality is rarely an all-or-nothing solution. Although we can’t remove human
    annotators from the majority of supervised machine learning systems, we have some
    exciting ways to improve our models and annotation strategies, such as using model
    predictions as labels, embeddings and contextual representations, rule-based systems,
    semi-supervised machine learning, lightly supervised machine learning, and synthetic
    data. All these techniques have interesting human-in-the-loop implications and
    are introduced in this chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 事实很少是全有或全无的解决方案。尽管我们无法从大多数监督式机器学习系统中移除人工标注员，但我们有一些令人兴奋的方法来改进我们的模型和标注策略，例如使用模型预测作为标签、嵌入和上下文表示、基于规则的系统、半监督机器学习、轻度监督机器学习以及合成数据。所有这些技术都有有趣的人机交互影响，并在本章中介绍。
- en: 9.3.1 Trusting annotations from confident model predictions
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 信任自信模型预测的标注
- en: The simplest way to use a model as an annotator is to trust the model predictions
    as labels, trusting predictions beyond a certain confidence threshold as labels.
    Figure 9.11 shows an example.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型作为标注员最简单的方法是信任模型预测作为标签，信任超过一定置信度阈值的预测作为标签。图9.11展示了示例。
- en: '![](../Images/CH09_F11_Munro.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F11_Munro.png)'
- en: 'Figure 9.11 Treating the most confident predictions as labels. A model predicts
    the items as being Label A or Label B, and the most confidently predicted items
    are treated as the correct label. This example allows us to build a model quickly
    but has a shortcoming: the model is built from items away from the decision boundary,
    which leaves a large amount of error for where that boundary might be.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 将最自信的预测作为标签。模型预测项目为标签A或标签B，最自信预测的项目被视为正确标签。这个例子允许我们快速构建模型，但有一个缺点：模型是从远离决策边界的项目构建的，这为可能存在的边界留下了大量错误。
- en: Figure 9.11 shows how items are labeled automatically by a predictive model.
    We can bootstrap our model from that starting point. This approach is suitable
    if you have an existing model but don’t have access to the data that the model
    is trained on. This situation is common in machine translation. Google released
    the first major machine translation system, and every major machine translation
    system since then has used translated data from Google’s engine. Although this
    approach is less accurate than annotating data directly, it can be effective for
    getting a quick start cheaply.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11展示了预测模型如何自动标记项目。我们可以从那个起点启动我们的模型。如果你有一个现有的模型但没有访问到该模型训练的数据，这种方法是合适的。这种情况在机器翻译中很常见。Google发布了第一个主要的机器翻译系统，从那时起，每个主要的机器翻译系统都使用了来自Google引擎的翻译数据。尽管这种方法比直接标注数据不太准确，但它可以以较低的成本快速启动。
- en: This kind of semi-supervised learning, sometimes known as *bootstrapped semi-
    supervised learning*, rarely works in isolation when adapting an existing model
    to new types of data. If you can confidently classify something correctly, your
    model gains little extra information with the additional items that it is already
    confident about, and you run the risk of amplifying bias. If something is truly
    novel, the model is probably not classifying it confidently or (worse) could be
    misclassifying it. This approach, however, can be effective when used in combination
    with active learning techniques to ensure that there is enough representative
    data. Figure 9.12 shows a typical workflow for trusting model predictions as annotations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种半监督学习，有时被称为*自举半监督学习*，在将现有模型适应新类型数据时很少单独工作。如果你可以自信地正确分类某物，你的模型从它已经自信的项目中获得的额外信息很少，而且你面临放大偏差的风险。如果某物真正新颖，模型可能不会自信地分类它，或者（更糟）可能会错误分类它。然而，当与主动学习技术结合使用以确保有足够代表性的数据时，这种方法是有效的。图9.12展示了将模型预测作为标注的典型工作流程。
- en: '![](../Images/CH09_F12_Munro.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F12_Munro.png)'
- en: Figure 9.12 A workflow for using confident predictions as annotations. The model
    is used to predict the labels of a large number of unlabeled items (potentially
    all of them). Human annotators review some of the labels, and the accepted labels
    become annotations for the training data. The human annotators also use this process
    to tune the threshold at which the labels can be confidently turned into annotations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12展示了使用自信预测作为标注的工作流程。模型用于预测大量未标记项目的标签（可能是所有项目）。人工标注员审查一些标签，接受的标签成为训练数据的标注。人工标注员还使用这个过程来调整可以将标签自信地转换为标注的阈值。
- en: 'Here are some tips for using confident model predictions to create annotations:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些使用自信模型预测来创建标注的技巧：
- en: Margin-of-Confidence and Ratio-of-Confidence are likely to be the best measure
    of confidence because you want the highest confidence relative to the other labels.
    So these metrics are good starting points, but you can test the other uncertainty
    sampling metrics to see what is best for your data.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 置信度边际和置信度比率可能是最好的置信度度量，因为你想相对于其他标签有最高的置信度。所以这些指标是好的起点，但你也可以测试其他不确定性采样指标，看看什么最适合你的数据。
- en: Set a confidence threshold on a per-label basis, or sample the top *N* predictions
    for each label, rather than try to set one confidence threshold for all labels.
    Otherwise, your most confident predictions are likely to be from a small number
    of easy-to-predict labels.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个标签的基础上设置置信度阈值，或者为每个标签采样前*N*个预测，而不是尝试为所有标签设置一个置信度阈值。否则，你最有信心预测的项目可能只来自少数几个容易预测的标签。
- en: 'Keep two models trained in each iteration: one trained on all annotations and
    one trained only on the annotations that a human has seen. Do not trust the predictions
    when confidence is high for the first model but low for the second.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个迭代中训练两个模型：一个在所有标注上训练，另一个只在人工已看到的标注上训练。当第一个模型的置信度很高而第二个模型的置信度很低时，不要信任预测。
- en: Keep track of human-labeled and auto-labeled items, and ensure that a certain
    number of training epochs use only the human-labeled items, to keep your model
    from straying too far. (This strategy is often called *pseudo-labeling*).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪人工标注和自动标注的项目，并确保一定数量的训练周期只使用人工标注的项目，以防止你的模型偏离太远。（这种策略通常被称为*伪标签*）。
- en: Use uncertainty sampling in your next iteration of active learning to focus
    on your new decision boundary.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的主动学习下一次迭代中使用不确定性采样来关注你的新决策边界。
- en: Use representative sampling to find data that was different from what the prior
    model was trained on (See section 7.5.4 for more about using representative sampling
    when combining human and machine labels.)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用代表性抽样来找到与先前模型训练数据不同的数据（有关在结合人类和机器标签时使用代表性抽样的更多信息，请参阅第7.5.4节）。
- en: Using model predictions to generate candidates for human review, instead of
    trusting them fully, can be effective if the annotation task is time-consuming.
    If we had a classification task with hundreds of labels, it would be much faster
    for an annotator to accept or reject the predicted label as a binary classification
    task than to choose among the hundreds of labels manually. This scenario tends
    to be more true of other types of machine learning, such as sequence labeling
    and semantic segmentation, than of labeling. Chapter 10 goes into more detail
    on using model predictions for these use cases.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型预测来生成人类审查的候选对象，而不是完全信任它们，如果标注任务耗时较长，可能会有效。如果我们有一个有数百个标签的分类任务，标注者接受或拒绝预测标签作为二元分类任务会比手动从数百个标签中选择要快得多。这种情况在其他类型的机器学习中更为常见，如序列标注和语义分割，而不是标注。第10章将更详细地介绍如何使用模型预测来解决这些用例。
- en: Review workflows like in figure 9.12 can lead to bias where humans trust the
    model too much, perpetuating and sometimes amplifying the errors. We will cover
    ways to mitigate these errors in chapter 11 when we discuss user experience and
    annotation interfaces.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于图9.12中的审查工作流程可能会导致偏差，因为人类过于信任模型，从而持续甚至放大错误。我们将在第11章讨论用户体验和标注界面时，介绍减轻这些错误的方法。
- en: 9.3.2 Treating model predictions as a single annotator
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 将模型预测视为单个标注者
- en: A second way to incorporate machine learning into the annotation process is
    to include the predictions from your downstream model as though they were annotations
    by one annotator. Suppose that the annotator Evan in our examples is not human;
    it’s our downstream machine learning model. Looking at figure 9.13, we can see
    that Evan is reasonably accurate, getting every label correct except for task
    3, where Evan incorrectly predicted “Cyclist” to be “Pedestrian.” Therefore, if
    we add Evan’s predictions as though Evan were a human annotator, we can apply
    the exact same methods to converge on the right agreement.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习融入标注过程的第二种方法是包括来自下游模型的预测，就像它们是一个标注者的标注一样。假设我们例子中的标注者Evan不是人类；它是我们下游的机器学习模型。查看图9.13，我们可以看到Evan的准确性相当高，除了任务3，Evan错误地将“Cyclist”预测为“Pedestrian”。因此，如果我们把Evan的预测当作一个人类标注者，我们可以应用完全相同的方法来达成正确的共识。
- en: '![](../Images/CH09_F13_Munro.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F13_Munro.png)'
- en: Figure 9.13 Incorporating predictions from a model as though they were annotations.
    From our example data, we can assume that Evan was in fact a predictive model,
    not a human annotator. For any of our methods that take into account the accuracy
    of each annotator, it is generally fine to incorporate model predictions as human
    annotations in this part of the workflow.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 将模型的预测作为标注纳入其中。从我们的示例数据中，我们可以假设Evan实际上是一个预测模型，而不是人类标注者。对于任何考虑每个标注者准确性的方法，在这个工作流程部分将模型预测作为人类标注纳入通常是完全可以接受的。
- en: You can incorporate a model’s prediction as you would the annotations of any
    other annotator. By applying the techniques from section 9.2.1, where we took
    the annotator’s accuracy into account when calculating our final probability distribution,
    we are using the accuracy of the model on the ground truth data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将模型的预测结果纳入其中，就像纳入任何其他标注者的标注一样。通过应用第9.2.1节中的技术，我们在计算最终概率分布时考虑了标注者的准确性，我们正在使用模型在真实数据上的准确性。
- en: You might consider different workflows, depending on how an item was sampled
    to be annotated. If you consider that Evan was trained by past human interactions
    and acted on that knowledge, Evan will be shaped by the past interactions and
    training data and will mirror those human behaviors unless Evan becomes adversarial
    to humans.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要考虑不同的工作流程，这取决于一个项目是如何被采样以进行标注的。如果你认为Evan是通过过去的交互训练并基于该知识行动的，那么Evan将受到过去交互和训练数据的影响，并将模仿那些人类行为，除非Evan变得对人类具有对抗性。
- en: Therefore, if an item was sampled that is similar to past training data and
    was confidently classified by Evan, you might ask one more annotator to confirm
    that annotation instead of the minimum number of annotators that you would otherwise
    use. This approach falls between our strategies of trusting confident predictions
    and treating the model as an annotator.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果一个样本与过去的训练数据相似，并且被Evan自信地分类，你可能会要求另一位标注员确认该标注，而不是使用你通常使用的最少标注员数量。这种方法介于我们信任自信预测和将模型视为标注员之间的策略。
- en: 9.3.3 Cross-validating to find mislabeled data
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 通过交叉验证查找错误标注的数据
- en: If you have an existing annotated dataset and are not certain that all the labels
    are correct, you can use the model to find candidates for human review. When your
    model predicts a different label from the ones that have been annotated, you have
    good evidence that the label might be wrong and that a human annotator should
    review that label.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个现有的标注数据集，并且不确定所有标签是否正确，你可以使用该模型来找到需要人工审核的候选标签。当你的模型预测的标签与已标注的标签不同时，你有很好的证据表明该标签可能错误，并且应该由人工标注员审核该标签。
- en: If you are looking at your existing dataset, however, your model should not
    be trained on the same data that it is evaluating, because your model will overfit
    that data and likely miss many cases. If you cross-validate, such as splitting
    your data into 10 partitions with 90% of the data as training data and 10% as
    evaluation data, you can train and predict on different data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你正在查看现有的数据集，你的模型不应该在评估数据上训练，因为你的模型将过度拟合这些数据，并可能错过许多案例。如果你进行交叉验证，例如将你的数据分成10个部分，其中90%作为训练数据，10%作为评估数据，你可以在不同的数据上训练和预测。
- en: Although there is a large body of literature on training models on noisy data,
    most of it assumes that it is not possible for humans to review or correct the
    wrongly labeled data. At the same time, the literature assumes that it is possible
    to spend a lot of time tuning models to automatically identify and account for
    noisy data (see graduate-student economics from chapter 7). In almost all real-world
    use cases, you should be able to annotate more data. If you know that your data
    is noisy, you should at least set up an annotation process for your evaluation
    data so that you know your actual accuracy.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于在噪声数据上训练模型的大量文献，但其中大部分假设人类无法审核或纠正错误标注的数据。同时，文献假设可以花费大量时间调整模型以自动识别和解释噪声数据（参见第7章的毕业生经济学）。在几乎所有实际应用场景中，你应该能够标注更多的数据。如果你知道你的数据是噪声的，你应该至少为你的评估数据设置一个标注流程，以便你知道你的实际准确率。
- en: There are some legitimate reasons why you might have noisy data that you can’t
    avoid. The data might be inherently ambiguous, you might get a large amount of
    free but noisy labels, or you might have an annotation interface that sacrifices
    a little accuracy for much greater throughput. We’ll return to ways to account
    for noisy data later, with the caveat that it is better to have accurate training
    data in almost all use cases.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些合法的理由，你可能会遇到无法避免的噪声数据。数据可能本身具有歧义，你可能会获得大量免费但噪声标签，或者你可能有一个为了提高吞吐量而牺牲一点准确性的标注界面。我们将在稍后讨论如何处理噪声数据的方法，但前提是在几乎所有用例中，拥有准确的学习数据都是更好的。
- en: 9.4 Embeddings and contextual representations
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 嵌入和上下文表示
- en: 'Much current machine learning research focuses on transfer learning: adapting
    a model from one task to another. This technique opens some interesting possibilities
    for annotation strategies. If your annotation task is especially time-consuming,
    such as semantic segmentation, you may be able to annotate orders of magnitude
    more data in some other way and then use that data in a model that is adapted
    to the semantic segmentation task. We’ll return to this specific example later
    in this section.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 目前许多机器学习研究集中在迁移学习：将一个模型从一个任务调整到另一个任务。这项技术为标注策略开辟了一些有趣的途径。如果你的标注任务特别耗时，例如语义分割，你可能能够以某种方式标注数量级更多的数据，然后使用这些数据在一个适应语义分割任务的模型中。
- en: Because transfer learning is a popular research area right now, there is a lot
    of changing terminology. If a model is specifically built to be adapted to new
    tasks, it is often called a *pretrained* model, and the information in that model
    is referred to as an *embedding* or *contextual representation.* Figure 9.14 shows
    a general architecture for using contextual embeddings.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于迁移学习是目前的一个热门研究领域，因此有很多术语在变化。如果一个模型是专门构建来适应新任务的，它通常被称为**预训练**模型，该模型中的信息被称为**嵌入**或**上下文表示**。图9.14展示了使用上下文嵌入的一般架构。
- en: '![](../Images/CH09_F14_Munro.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14](../Images/CH09_F14_Munro.png)'
- en: Figure 9.14 An example of transfer learning. We have the task of predicting
    whether an item is “A” or “B,” and we think that our existing model that predicts
    “X,” “Y,” or “Z” will have useful information because of similarities between
    the two tasks. So we can use the neurons from the “X,” “Y,” or “Z” model as features
    (a representation) in our model predicting “A” or “B.” This example is similar
    to the examples earlier in the book that use the hidden layers as features for
    clustering and use transfer learning to adapt an existing model to a new task.
    In some cases, we might ignore the inputs “A” and “B,” using only the pretrained
    model as a representation for our new model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 转移学习的示例。我们有一个预测项目是“A”还是“B”的任务，我们认为我们现有的预测“X”、“Y”或“Z”的模型由于两个任务之间的相似性，将包含有用的信息。因此，我们可以使用“X”、“Y”或“Z”模型的神经元作为我们预测“A”或“B”的模型中的特征（表示）。这个例子与书中早些时候的例子类似，那些例子使用隐藏层作为聚类的特征，并使用迁移学习来适应现有模型以适应新任务。在某些情况下，我们可能会忽略“输入A”和“B”，只使用预训练模型作为我们新模型的表示。
- en: You may want to experiment with variations of the architecture in figure 9.1\.
    You could choose to use only some layers in the representation, especially if
    you are worried about having too many dimensions. Alternatively, you might want
    only the predicted labels and not any model-internal representation, which will
    be the only choice if you only have access to model predictions. This approach
    might called something like “using another model’s predictions as features” in
    the literature rather than a representation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想尝试图9.1中架构的变体。你可以选择只使用表示中的某些层，尤其是当你担心维度过多时。或者，你可能只想使用预测标签，而不是任何模型内部表示，如果你只能访问模型预测，这将是你唯一的选择。这种做法在文献中可能被称为“使用另一个模型的预测作为特征”，而不是表示。
- en: You can also decide whether you want to adapt or tune an existing model or to
    use the existing model as features in a new model. We did the latter for adaptive
    transfer learning in chapter 5, and as that chapter pointed out, having one model
    feed into another (as in figure 9.14) is the equivalent of adapting a model with
    frozen weights. If you are training all the models and not using existing ones,
    yet another option is to have a multitask model; you have one model with shared
    layers but with different output layers or transformer heads for different tasks.
    If you start with a pretrained model and adapt it with an adjacent task before
    adapting it again to your actual task, the process is known as *intermediate task
    training*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以决定是否要调整现有模型或将其用作新模型中的特征。我们在第5章中实现了后者，用于自适应迁移学习，正如该章节所指出的，一个模型输入另一个模型（如图9.14所示）相当于调整一个具有冻结权重的模型。如果你正在训练所有模型而不使用现有模型，另一个选择是使用多任务模型；你有一个具有共享层但具有不同输出层或转换头的模型，用于不同任务。如果你从一个预训练模型开始，在适应实际任务之前先使用相邻任务来适应它，这个过程被称为**中间任务训练**。
- en: You might also decide to use multiple model representations in your final model,
    which one of the practical examples in chapter 12 implements.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可能决定在你的最终模型中使用多个模型表示，第12章中的一些实际例子实现了这一点。
- en: Transfer learning, pretrained models, representations, or embeddings?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习、预训练模型、表示或嵌入？
- en: The machine learning community has not yet settled on the names for different
    transfer learning methods and where they fall on the unsupervised-to-supervised
    spectrum of models. Embeddings were historically the result of unsupervised learning,
    but supervised variations of all variants quickly arose. Most recently, NLP researchers
    have started using supervised models with clever ways to get “free” labels, such
    as predicting a missing word in a sentence and predicting whether two sentences
    followed each other in the source documents. Because these models are predicting
    the words or sentences in context, they are often called *contextual representations*
    or *contextual embeddings*, and the models are known as *contextual models*. Because
    the models are specifically trained with transfer learning in mind, they are also
    called *pretrained models*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习社区尚未就不同迁移学习方法的名称以及它们在无监督到监督模型谱系中的位置达成共识。嵌入（Embeddings）在历史上是无监督学习的产物，但所有变种的监督版本很快便出现了。最近，自然语言处理（NLP）研究人员开始使用带有巧妙方法获取“免费”标签的监督模型，例如预测句子中的缺失单词以及预测两个句子在源文档中是否连续。因为这些模型是在上下文中预测单词或句子，所以它们通常被称为*上下文表示*或*上下文嵌入*，而模型则被称为*上下文模型*。因为这些模型是专门针对迁移学习进行训练的，所以它们也被称为*预训练模型*。
- en: The most recent supervised approaches are sometimes referred to as unsupervised,
    either as a continuation of the historical tradition of embeddings being unsupervised
    or because the researchers didn’t have to pay to create training data when predicting
    a word that is removed from an existing sentence. In the literature, you might
    come across any combination of *transfer learning, pretrained models, contextual
    representations*, and *embeddings* alongside learning described as *supervised,
    unsupervised, semi-supervised*, or *self-supervised*. The reduction in annotation
    effort that results from these methods is often referred to as *one-shot, few-shot*,
    or *zero-shot* learning, depending on how many iterations of additional annotations
    are required and how long it takes the model to adapt to the new use case.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一些监督方法有时被称为无监督方法，要么是因为嵌入在历史上一直是无监督的，要么是因为研究人员在预测从现有句子中移除的单词时不需要付费创建训练数据。在文献中，你可能会遇到任何组合的*迁移学习、预训练模型、上下文表示*和*嵌入*，以及描述为*监督、无监督、半监督*或*自监督*的学习方法。这些方法导致的标注工作量减少通常被称为*单次、少量*或*零次*学习，这取决于需要多少次额外的标注迭代以及模型适应新用例需要多长时间。
- en: The terms will no doubt evolve and be added to after this book is published,
    so look closely at what the researchers are talking about in any paper.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语无疑会在本书出版后发展和增加，所以请仔细阅读任何论文中研究人员所讨论的内容。
- en: 'Here are some ways that you can use embeddings and contextual representations
    in your annotation process:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你可以在你的标注过程中使用嵌入和上下文表示的方法：
- en: Use existing embeddings or adapt a pretrained model for your deployed model.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有的嵌入或为你的部署模型调整预训练模型。
- en: Use inherent labels in your data to train a custom set of embeddings on your
    data.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你数据中的固有标签来训练一组定制的嵌入。
- en: Get human annotations much more efficiently on a task that is adjacent to your
    actual task and then build a contextual model from those annotations.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在与你的实际任务相邻的任务上更高效地获取人工标注，然后从这些标注中构建上下文模型。
- en: We’ll cover each of these examples in turn in sections 9.4.1 through 9.4.3.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第9.4.1节至第9.4.3节中依次介绍这些示例。
- en: 9.4.1 Transfer learning from an existing model
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 从现有模型进行迁移学习
- en: To the extent that there is any traditional approach to transfer learning with
    neural models, it is the process of taking a model designed for one task and adapting
    it to another. The best-known task in computer vision adapts an ImageNet model
    to other tasks. You may have experimented with this kind of transfer learning,
    which is the kind of transfer learning used in active transfer learning in chapter
    5, so we won’t go into detail about it again here.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经网络模型的传统迁移学习方法中，是将一个为某个任务设计的模型适应到另一个任务的过程。在计算机视觉中，最著名的任务是将ImageNet模型适应到其他任务。你可能已经尝试过这种类型的迁移学习，这是第5章中主动迁移学习所使用的类型，所以我们在这里不再详细讨论。
- en: One variation that you may not have seen uses a dataset like ImageNet for a
    machine learning task that is more complicated than image-level labeling, such
    as semantic segmentation. Suppose that we are doing semantic segmentation on images
    to identify “Animal,” “Cyclist,” “Pedestrian,” and “Sign” in our example use case.
    Also suppose that we have 2 million images, that it takes about an hour to annotate
    each image for semantic segmentation (typical time for some tasks), and that there
    is a budget for the equivalent of six years of full-time annotation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能没有见过的一种变体是使用像ImageNet这样的数据集来进行比图像级标注更复杂的机器学习任务，例如语义分割。假设我们在图像上进行语义分割，以识别“动物”、“骑自行车的人”、“行人”和“标志”等在我们的示例用例中。还假设我们有200万张图片，标注每张图片需要大约一个小时（某些任务的典型时间），并且有相当于六年全职标注的预算。
- en: Completing semantic segmentation would take 40 hours * 50 weeks * 6 people =
    12,000 images. That is, the training data would contain about 12,000 images (or
    slightly less, because some would be held out as evaluation data). Although 12,000
    is an acceptable number of items to train on, it’s not a large amount and less
    than 1% of the available data. Even with good active learning, there may be only
    1,000 examples of some of the rarest labels.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 完成语义分割需要40小时 * 50周 * 6人 = 12,000张图片。也就是说，训练数据将包含大约12,000张图片（或略少，因为其中一些将作为评估数据保留）。尽管12,000是训练可接受的物品数量，但它并不大，不到可用数据的1%。即使有良好的主动学习，某些稀有标签的示例也可能只有1,000个。
- en: You know, however, that ImageNet has millions of examples of people, bicycles,
    and types of animals. So you use an existing ImageNet database, knowing that neurons
    in that model will contain representations of each of those object types. Therefore,
    you know that the semantic segmentation model, which is trained only on 12,000
    examples, can take advantage of representations in ImageNet that are trained on
    millions of examples. This representation might help your model, and this principle
    can be applied to other types of representations. We will build on this observation
    in section 9.4.2.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你知道ImageNet有数百万个关于人、自行车和动物类型的示例。因此，你使用现有的ImageNet数据库，知道该模型中的神经元将包含每个对象类型的表示。因此，你知道仅用12,000个示例训练的语义分割模型可以利用在数百万个示例上训练的ImageNet中的表示。这种表示可能有助于你的模型，并且这个原则可以应用于其他类型的表示。我们将在第9.4.2节中基于这个观察结果。
- en: 9.4.2 Representations from adjacent easy-to-annotate tasks
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 来自相邻易于标注任务的表示
- en: The shortcoming of using an existing model like ImageNet is that it is trained
    on different labels and most likely trained on different kinds of images. You
    can dedicate some of your annotation budget to image-level labeling of your data
    according to the same labels that you used in your semantic segmentation task.
    Although semantic segmentation is time-consuming, you can create a simple annotation
    task for questions such as “Is there an animal in this image?”, which takes only
    20 seconds per image and is therefore faster than full segmentation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像ImageNet这样的现有模型的缺点是它是在不同的标签上训练的，并且很可能是用不同类型的图像训练的。你可以将一部分你的标注预算用于根据你在语义分割任务中使用的相同标签对你的数据进行图像级标注。尽管语义分割耗时，但你可以为诸如“这张图片中是否有动物？”等问题创建一个简单的标注任务，每个图像只需20秒，因此比完整分割更快。
- en: If you take the budget of six person-years and move one person-year to annotation
    at image level, you get 3 per minute * 60 minutes * 40 hours * 50 weeks = 360,000
    image-level labels for the different object types. Then you can train a model
    on those labels, knowing that the model will contain representations of each of
    those object types and that it covers much more variety than the semantic segmentation
    annotations (now 10,000 from 5 people).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将六人年的预算中的一年移至图像级标注，你将得到每分钟3个 * 60分钟 * 40小时 * 50周 = 360,000个不同对象类型的图像级标签。然后你可以在这些标签上训练一个模型，知道该模型将包含每个对象类型的表示，并且它覆盖的多样性比语义分割标注（现在从5人那里有10,000个）要多得多。
- en: If you have 360,000 relevant labels on your images for a reduction of only 2,000
    fewer semantic segmentations, you can provide much richer information in your
    model. If your model architecture allows efficient embeddings, this strategy is
    one to consider.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为图像上有360,000个相关标签，但仅减少了2,000个语义分割，你可以在你的模型中提供更丰富的信息。如果你的模型架构允许高效的嵌入，这种策略是一个值得考虑的策略。
- en: 'This strategy also has additional advantages: it is easier to implement quality
    control on the labeling task, and you will be able to draw on a broader workforce
    that can’t necessarily work on semantic segmentation but can do labeling.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略还有额外的优势：它更容易在标注任务上实施质量控制，并且您将能够利用更广泛的劳动力，这些人可能无法进行语义分割，但可以进行标注。
- en: The hard thing to predict is whether you will get a net positive from taking
    away 2,000 semantic segmentation training data items to add the 360,000 image-level
    labels for your pretrained model. You may want to start experimenting with a smaller
    number. Recall the workflow example in chapter 8 that used a image-level labeling
    task first, asking “Are there any bicycles in this image?” If you have a similar
    workflow, you are already generating data that can be used in a model to create
    embeddings. This example would be a good place to start experimenting before you
    need to divert any resources.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 难以预测的是，从移除2,000个语义分割训练数据项以添加360,000个图像级标签来为您的预训练模型带来的净收益。您可能想先从小规模开始实验。回想一下第8章中使用的图像级标注任务的工作流程示例，首先询问“这张图片中是否有自行车？”如果您有类似的流程，您已经正在生成可用于模型创建嵌入的数据。在您需要分配任何资源之前，这是一个开始实验的好地方。
- en: '9.4.3 Self-supervision: Using inherent labels in the data'
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.3 自监督：使用数据中的固有标签
- en: The data might have inherent labels that you can use for free to create other
    contextual models. Any metadata associated with the data is a potential source
    of labels on which you can build a model, and that model can be used as a representation
    for your actual task.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能具有固有的标签，您可以用这些标签免费创建其他上下文模型。与数据相关的任何元数据都是构建模型的潜在标签来源，并且该模型可以用作您实际任务的表示。
- en: In our example data, suppose that we have a problem with accuracy in certain
    lighting conditions, but it is too expensive to annotate each image manually according
    to lighting conditions, and some lighting conditions are rare. You have timestamps
    on most of your images, so you can use those timestamps to confidently filter
    a million images into buckets for different times of day (perhaps by hour or by
    daytime and nighttime buckets). Then you can train a model to classify images
    according to time of day, knowing that the model will contain representations
    of the lighting. Without getting humans to analyze the data, you have a model
    that approximates a prediction for lighting conditions that you can use as a representation
    for other tasks. These three examples of how you can incorporate embeddings are
    shown in figure 9.15.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例数据中，假设我们在某些光照条件下存在准确性问题，但手动根据光照条件标注每张图片的成本太高，而且某些光照条件很少见。您的大多数图片都有时间戳，因此您可以使用这些时间戳自信地将一百万张图片过滤到不同时间段的桶中（可能是按小时或白天和晚上的桶）。然后您可以训练一个模型来根据一天中的时间对图像进行分类，知道该模型将包含光照的表示。无需让人类分析数据，您就有了一个近似预测光照条件的模型，可以用作其他任务的表示。这些如何结合嵌入的三个示例在图9.15中展示。
- en: '![](../Images/CH09_F15_Munro.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F15_Munro.png)'
- en: Figure 9.15 An example of how transfer learning can be used to make a model
    more accurate and how it might influence the annotation strategy. Here, three
    other models are feeding into a semantic segmentation model. The top example is
    adapting a model trained on ImageNet, which is the most common type of transfer
    learning. The second model is trained on 300,000 image-level labels for the objects
    we care about. The third model uses the timestamps of the images to train a model
    to predict the time of day. Because the top three models have been trained on
    much more data than the semantic segmentation model, with only 10,000 training
    items, they should have richer representations of the image that can help with
    the semantic segmentation task.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 说明了如何使用迁移学习来提高模型的准确性以及它可能如何影响标注策略。在这里，三个其他模型正在向一个语义分割模型输入。顶部示例是适应在ImageNet上训练的模型，这是最常见的迁移学习类型。第二个模型是在我们关心的对象的300,000个图像级标签上训练的。第三个模型使用图像的时间戳来训练一个预测一天中时间的模型。因为顶部三个模型训练的数据量比语义分割模型多得多，只有10,000个训练项，它们应该有更丰富的图像表示，这有助于语义分割任务。
- en: 'Free labels are appealing, and there are likely to be some options for your
    data. Even noisy labels can help. Every social media company uses models built
    from hashtags for computer vision and natural language processing (NLP) tasks.
    Even though hashtags are used differently by different people, there is enough
    signal in predicting those hashtags that it helps with downstream computer vision
    and NLP tasks. Your final model treats the contextual model as input embeddings
    and weights them accordingly, so errors don’t necessarily propagate. Here are
    some examples that you might find in your data:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 自由标签很有吸引力，你的数据可能有一些选项。即使是噪声标签也可能有所帮助。每个社交媒体公司都使用从标签构建的模型来处理计算机视觉和自然语言处理（NLP）任务。尽管不同的人使用标签的方式不同，但在预测这些标签时，有足够的信号可以帮助下游的计算机视觉和NLP任务。你的最终模型将上下文模型作为输入嵌入，并相应地加权，因此错误不一定传播。以下是一些你可能在数据中找到的例子：
- en: User-generated tags, such as hashtags and user-defined topics
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户生成的标签，例如标签和用户定义的主题
- en: Meaningful time periods, such as day/night and weekday/weekend
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有意义的时期，例如白天/夜晚和周内/周末
- en: Geographic information about the data or the person who created it
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于数据或创建数据的人的地理信息
- en: (Especially for computer vision) The type of device that created the data
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （特别是对于计算机视觉）创建数据的设备类型
- en: (Especially for web text) The domain or URL of linked text
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （特别是对于网络文本）链接文本的域名或URL
- en: (Especially for NLP) The word or token in context, as used in many pretrained
    models
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （特别是对于自然语言处理）上下文中的单词或标记，如许多预训练模型中所用
- en: (Especially for NLP) Whether two sentences or paragraphs follow each other
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （特别是对于自然语言处理）两个句子或段落是否紧接着出现
- en: (Especially for computer vision) The pixel values of a video frame in context
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （特别是对于计算机视觉）在上下文中视频帧的像素值
- en: In brief, if any metadata or linked data can become a label, or if you can meaningfully
    remove part of the data and predict it in context, that data might be a candidate
    for inherent labels that you can use to build a representation. These methods
    of incorporating free adjacent labels into models have been popular since search
    engines first used them in the early 2000s and are enjoying a recent surge of
    popularity with neural models.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果任何元数据或链接数据可以成为标签，或者如果你可以有意义地删除数据的一部分并在上下文中预测它，那么这些数据可能是有用的内在标签的候选者，你可以使用它们来构建表示。这些将自由相邻标签纳入模型的方法自从搜索引擎在21世纪初首次使用它们以来就非常流行，并且随着神经模型的出现而最近再次受到关注。
- en: Note that although the labels are free, dimensionality is a problem, especially
    early in the annotation process, when you don’t have many annotations for your
    actual task. Some of this problem is outside the scope of this book; dimensionality
    is a broad problem in machine learning, and many papers have been written about
    how to solve it when building models on limited data. But some of the problem
    can be mitigated by the design of your contextual models. If you have a near-final
    layer in your model that you will use as your representation, you may want to
    set that layer to be an order of magnitude smaller than the number of training
    data items you have. Your accuracy for that contextual model may go down, but
    the information is now distilled into fewer dimensions (fewer neurons), so it
    might improve the accuracy of your downstream model. Look at the literature on
    model distillation for additional ways to reduce the dimensionality of your models
    without losing too much accuracy. You can also use classic statistical methods
    such as PCA (chapter 4).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管标签是自由的，但维度是一个问题，尤其是在标注过程的早期，当你没有很多标注用于你的实际任务时。这个问题的一部分超出了本书的范围；维度是机器学习中的一个广泛问题，许多论文都探讨了在有限数据上构建模型时如何解决这个问题。但可以通过设计你的上下文模型来减轻一些问题。如果你模型中的接近最终层是你将用作表示的部分，你可能希望将该层设置为比你的训练数据项数量小一个数量级。你那个上下文模型的准确性可能会下降，但信息现在被浓缩到更少的维度（更少的神经元）中，这可能会提高你下游模型的准确性。查看有关模型蒸馏的文献，以了解如何在不损失太多准确性的情况下减少你模型的维度。你还可以使用经典统计方法，如PCA（第4章）。
- en: 9.5 Search-based and rule-based systems
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 基于搜索和基于规则的系统
- en: Rule-based systems predate statistical machine learning, especially in NLP,
    and are still an active area of research. Among the biggest advantages of rule-based
    systems are the senses of ownership and agency that they give annotators, especially
    ones who are SMEs, making them feel as though they are in the driver’s seat. I
    have built rule-based systems on top of machine learning systems specifically
    because the analysts using the system wanted a way to input their expert knowledge
    directly into the system. It isn’t as easy to provide that level of user experience
    in an annotation interface, and we will return to the human–computer interaction
    side of this problem in chapter 11.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的系统在统计机器学习之前就已经存在，尤其是在自然语言处理（NLP）领域，并且仍然是研究的一个活跃领域。基于规则系统最大的优势之一是它们给标注者带来的所有权和代理感，尤其是那些是行业专家的标注者，使他们感觉自己处于驾驶座。我之所以在机器学习系统之上构建基于规则的系统，正是因为使用该系统的分析师想要一种直接将他们的专业知识输入系统的方式。在注释界面中提供这种级别的用户体验并不容易，我们将在第11章回到这个问题的计算机-人交互方面。
- en: Beware of scope creep with rule-based systems
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 小心基于规则的系统中的范围扩张
- en: I have seen many people get locked into rule-based systems through incremental
    scope creep that they had trouble escaping. A popular smart-device company used
    machine learning for converting speech to text but then used a rule-based system
    to classify that text into different commands or questions (intents). A rule-based
    approach made sense while the company was first testing the system on a limited
    set of problems, but this approach became increasingly difficult as the product
    took off, requiring new functionality and support for more languages. The company
    ended up employing hundreds of people in parallel to write new rules for how certain
    combinations of keywords mapped to different commands. The company barely kept
    the system running while it spent more than a year building out the machine learning
    capabilities in parallel and had trouble scaling the management of all the rules
    and how they interacted. The company concluded that the quick start that rules
    provided was not ultimately worthwhile; even a simple machine learning model with
    good training data would have been a better start.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到许多人因为渐进式的范围扩张而陷入基于规则的系统，难以逃脱。一家流行的智能设备公司使用机器学习将语音转换为文本，但随后使用基于规则的系统将文本分类为不同的命令或问题（意图）。当公司在有限的几个问题上测试系统时，基于规则的途径是有意义的，但随着产品的推广，需要新的功能和对更多语言的支持，这种方法变得越来越困难。公司最终雇佣了数百人并行编写新的规则，以确定某些关键词组合如何映射到不同的命令。公司在花费超过一年时间并行构建机器学习能力的同时，几乎无法维持系统的运行，并且难以扩展所有规则及其相互作用的管理。公司得出结论，规则提供的快速启动最终并不值得；即使是一个简单的机器学习模型，只要有良好的训练数据，也会是一个更好的起点。
- en: 9.5.1 Data filtering with rules
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 使用规则进行数据筛选
- en: Manually crafted rule-based systems are widely used for data filtering. For
    stratified sampling, this approach can make a lot of sense. To continue an example
    in this chapter, if you are classifying images outdoors and care about lighting
    conditions, you could create a rule-based system to sample an even number of images
    from different times of day to make the data more balanced.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 手工构建的基于规则的系统在数据筛选中被广泛使用。对于分层抽样，这种方法可以非常有意义。为了继续本章的例子，如果你在户外对图像进行分类，并且关心光照条件，你可以创建一个基于规则的系统，从一天中的不同时间采样相同数量的图像，以使数据更加平衡。
- en: On the other hand, if rules are being used to filter data based on untested
    intuitions, you may end up with biased data and a system that won’t perform well
    when applied to real-world data. This situation is especially likely in language
    tasks; any keyword-based rules are biased against rarer spellings, people with
    less literacy in a language might make more errors, or synonyms may not be known
    by the person who created the rule.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果规则是基于未经检验的直觉来筛选数据，你可能会得到有偏差的数据，并且当应用于现实世界数据时，系统可能表现不佳。这种情况在语言任务中尤其可能；任何基于关键词的规则都可能对罕见拼写有偏见，对语言素养较低的人可能犯更多错误，或者规则制定者可能不知道同义词。
- en: Even if you can use a rule-based system for a labeling task and don’t need annotations
    (except for your evaluation data), you still might be better off using the rule-based
    system to autoannotate the data annotation and then build a machine learning model
    on those annotations, rather than use a rule-based system in production. It is
    difficult to add contextual models to rule-based systems, so creating a machine
    learning version of your rule-based system will make it easier to integrate with
    pretrained models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你可以使用基于规则的系统进行标记任务并且不需要注释（除了你的评估数据），你仍然可能更倾向于使用基于规则的系统来自动注释数据，然后在那些注释上构建机器学习模型，而不是在生产中使用基于规则的系统。将上下文模型添加到基于规则的系统中是困难的，因此创建你基于规则的系统的机器学习版本将使其更容易与预训练模型集成。
- en: 9.5.2 Training data search
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 训练数据搜索
- en: Search engine interfaces provide a nice middle ground between rule-based systems
    and machine learning systems. An SME can search for items that they think belong
    to a certain category (label) and quickly accept or reject the items returned
    by that search. If the SME knows that some items are going to be tricky for the
    model or are inherently important for their application, they have a way to dive
    into the relevant data quickly. This example is similar to our workflow in which
    an expert reviews earlier annotations, but in this case, the expert is driving
    the entire process.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎界面在基于规则的系统和机器学习系统之间提供了一个很好的中间地带。一个领域专家可以搜索他们认为属于某个类别（标签）的项目，并快速接受或拒绝由该搜索返回的项目。如果领域专家知道某些项目对模型来说可能会很棘手，或者对于他们的应用来说本质上很重要，他们有快速深入相关数据的方法。这个例子类似于我们的工作流程，其中专家审查早期的注释，但在这个情况下，专家是推动整个过程的。
- en: Training data search can be thought of as a type of annotator-driven diversity
    sampling, in which the person responsible for finding all the relevant data to
    sample is also creating the annotations. If that person is routing the data to
    other people to annotate, the process is almost like the reverse of the workflow
    for expert review. The process starts with the SME, who finds the most important
    data points manually; then nonexpert annotators complete the more time-consuming
    annotation tasks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据搜索可以被视为一种注释者驱动的多样性采样，其中负责找到所有相关数据以进行采样的个人也在创建注释。如果那个人将数据路由到其他人进行注释，这个过程几乎像是专家审查工作流程的反向。这个过程从领域专家开始，他们手动找到最重要的数据点；然后非专家注释者完成更耗时的注释任务。
- en: There are advantages to allowing search functionality for stakeholders other
    than domain exports. An annotator can get a good idea of the type of data they
    are annotating by being permitted to search that data. A machine learning scientist
    can quickly test their assumptions about which features will be important in their
    models. This form of exploratory data analysis is also valuable in lightly supervised
    systems.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 允许利益相关者进行搜索功能，而不仅仅是领域导出，有它的优势。注释者可以通过允许搜索这些数据来获得他们正在注释的数据类型的好印象。机器学习科学家可以快速测试他们对哪些特征将在他们的模型中重要的假设。这种形式的数据探索性分析在轻度监督系统中也非常有价值。
- en: 9.5.3 Masked feature filtering
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.3 遮蔽特征过滤
- en: If you are quickly building a model with training data by rules or by search,
    you should consider masking the features that you used to generate that training
    data when you are training your model. If you are quickly building a sentiment
    analysis classifier, and you create your initial training data by searching or
    filtering for text that says “happy” and text that says “angry,” consider masking
    the terms “happy” and “angry” in your feature space. Otherwise, your model can
    easily overfit the terms “happy” and “angry” and not learn the surrounding words
    that should contribute to the sentiment of the text.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过规则或搜索快速构建一个训练数据模型，你应该考虑在训练模型时屏蔽你用来生成训练数据的特征。如果你快速构建一个情感分析分类器，并且通过搜索或过滤出表示“快乐”和表示“愤怒”的文本来创建初始训练数据，那么考虑在你的特征空间中屏蔽“快乐”和“愤怒”这些术语。否则，你的模型可能会轻易地过拟合“快乐”和“愤怒”这些术语，而无法学习到应该对文本情感有贡献的周围词语。
- en: You can consider different masking strategies. You might mask these words in
    50% of your training epochs, for example, so that your model needs to spend 50%
    of the time learning about words that weren’t part of the search or rule strategy.
    This approach can be thought of as a targeted variation of dropouts to mitigate
    biased propagating from your data collection methods. If you will have later iterations
    of active learning, you could remove these words from the early iterations, minimizing
    their bias early in the process while knowing that models in later iterations
    will include them to maximize their accuracy for the model that will be deployed
    for your application.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以考虑不同的掩码策略。例如，你可能在50%的训练轮次中掩码这些单词，这样你的模型就需要花费50%的时间学习那些不是搜索或规则策略一部分的单词。这种方法可以被视为针对从你的数据收集方法中传播的偏差的针对性变体。如果你将进行后续的主动学习迭代，你可以在早期迭代中移除这些单词，在早期过程中最小化它们的偏差，同时知道后续迭代的模型将包括它们，以最大化部署到你的应用程序的模型的准确性。
- en: 9.6 Light supervision on unsupervised models
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 对无监督模型进行轻量级监督
- en: One of the most widely used methods for exploratory data analysis is allowing
    annotators, typically SMEs, to interact with unsupervised models. One of the examples
    in chapter 12 is an implementation of exploratory data analysis. Figure 9.16 shows
    a simple extension to the clustering method for diversity sampling (chapter 4).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析最广泛使用的方法之一是允许注释者，通常是行业专家，与无监督模型进行交互。第12章中的一个例子是实现探索性数据分析。图9.16展示了针对多样性采样（第4章）的聚类方法的简单扩展。
- en: There are many variations in figure 9.16 that you may want to experiment with.
    In addition to clustering, you could use the related topic modeling techniques,
    especially for text data. In addition to distance-based clustering, you could
    use cosine distance (chapter 4), proximity-based clustering such as K-Nearest
    Neighbors (KNN), or graph-based clustering.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16中有许多变体，你可能想要尝试。除了聚类，你还可以使用相关的主题建模技术，特别是对于文本数据。除了基于距离的聚类，你还可以使用余弦距离（第4章）、基于邻近度的聚类，如K-最近邻（KNN）或基于图的聚类。
- en: '![](../Images/CH09_F16_Munro.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16](../Images/CH09_F16_Munro.png)'
- en: Figure 9.16 An example of light supervision. The data is clustered, and a small
    number of items are sampled from each cluster. For every cluster in which all
    the labels are the same, the entire cluster is given that label. A supervised
    model can be built on all the items, ignoring the items that did not get a label
    in clusters where there was disagreement.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 轻量级监督的一个示例。数据被聚类，并且从每个聚类中采样少量项目。对于所有标签都相同的聚类，整个聚类都被赋予那个标签。可以在所有项目上构建监督模型，忽略在存在分歧的聚类中没有获得标签的项目。
- en: 9.6.1 Adapting an unsupervised model to a supervised model
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.1 将无监督模型调整为监督模型
- en: 'The clustering algorithm in figure 9.16 extends the clustering examples in
    chapter 4 by assuming that all clusters with only one label have that label for
    all items. There are other ways to convert this type of model to a fully supervised
    one:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16中的聚类算法通过假设所有只有一个标签的聚类对所有的项目都有那个标签，扩展了第4章中的聚类示例。将此类模型转换为完全监督模型的其他方法包括：
- en: Recursively cluster for the clusters containing items with more than one label.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对包含多个标签的项目的聚类进行递归聚类。
- en: Switch to uncertainty sampling after initially using the methods in figure 9.16.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最初使用图9.16中的方法之后，切换到不确定性采样。
- en: De-weighting or removing the auto-labeled items over time.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随时间减少或移除自动标记的项目。
- en: 9.6.2 Human-guided exploratory data analysis
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.2 人类引导的探索性数据分析
- en: Sometimes, a data scientist’s goal is pure exploration, not necessarily to build
    a supervised classification model. In this case, the annotator might not have
    a predefined set of labels. A scientist might use clustering or other unsupervised
    techniques to look for trends in the data and decide based on those trends what
    labels might apply.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据科学家的目标是纯粹的探索，不一定是为了构建一个监督分类模型。在这种情况下，注释者可能没有预定义的标签集。科学家可能使用聚类或其他无监督技术来寻找数据中的趋势，并根据这些趋势决定可能适用的标签。
- en: Search and rule-based systems might be used alongside unsupervised methods and
    time-based trends. Supervised systems can be used for tagging the data and to
    segment the analysis. For example, a person may want to cluster social media messages
    after they have been divided into positive and negative sentiment to see the trends
    within each sentiment extreme.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索和基于规则的系统可以与无监督方法和基于时间趋势的方法一起使用。监督系统可用于标记数据和对分析进行分段。例如，一个人可能希望在将社交媒体消息分为正面和负面情绪之后对它们进行聚类，以查看每个情绪极端内的趋势。
- en: 9.7 Synthetic data, data creation, and data augmentation
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 合成数据、数据创建和数据增强
- en: Synthetic data is useful when the raw data is not available and when creating
    the data from scratch is cheaper than annotating the data. For use cases such
    as speech recognition, it is common to use created data. If you are creating a
    speech recognition system for hospitals, you might get people to read a list of
    medical-related words or sentences. It wouldn’t be feasible to find audio recordings
    of every relevant medical word in every accent or language that you care about
    in a generally available speech data corpus, so data creation is used.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当原始数据不可用且从头开始创建数据比标注数据更便宜时，合成数据很有用。对于语音识别等用例，通常使用创建的数据。如果你为医院创建一个语音识别系统，你可能让人们阅读一份与医疗相关的单词或句子列表。在通常可用的语音数据语料库中找到每个相关医疗单词在每个你关心的口音或语言中的音频录音是不切实际的，因此数据创建被使用。
- en: 9.7.1 Synthetic data
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.7.1 合成数据
- en: 'Small, manually created evaluation data is common. You might create an evaluation
    dataset with known pathological edge cases that you use with each model that you
    build. Or you might build a small evaluation dataset with some easy-to-classify
    examples and make 100% accuracy on this dataset a precondition for shipping a
    new model—the machine learning equivalent of a software developer’s unit test.
    Purely synthetic training data, often created programmatically instead of manually,
    is most useful in one or more of these situations:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 小型手动创建的评估数据很常见。你可能为每个构建的模型创建一个包含已知病态边缘情况的评估数据集。或者你可能构建一个包含一些易于分类示例的小型评估数据集，并将在这个数据集上实现100%的准确率作为发布新模型的先决条件——这是机器学习对软件开发人员单元测试的等效。纯合成训练数据，通常通过编程而不是手动创建，在以下一种或多种情况下最有用：
- en: There is a constrained problem, such as restructuring data that started in a
    structured format but ended up with predictable types of noise.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在约束性问题，例如，数据最初以结构化格式开始，但最终以可预测的噪声类型结束的数据重构。
- en: There are barriers to getting enough data (such as cost or rarity).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取足够数据存在障碍（例如成本或稀有性）。
- en: There are privacy or security concerns with using real data.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实数据时存在隐私或安全问题。
- en: There are acceptable fallbacks to humans when the model fails.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型失败时，有可接受的回退方案供人类使用。
- en: 'I am aware of only one widely used case of purely synthetic data for machine
    learning: scanning credit card numbers. If you have added your credit card number
    to an application on your phone, you may have noticed the option to take a photograph
    of your credit card instead of typing the numbers. The model that recognizes your
    credit card numbers was almost certainly built on purely synthetic data with no
    human annotation. It qualifies for all four cases above. Your credit card number
    started as structured data, but it was printed on a physical card and a photo
    was taken of that printed number, which is a constrained problem restructuring
    16 numbers. There are no large open data repositories of scanned credit cards.
    Privacy and security concerns would arise if data scientists and annotators could
    see all the scanned images from actual cards to annotate. Finally, the end users
    are generally OK with typing their card numbers manually if the scan doesn’t work.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我只知道一个纯合成数据在机器学习中被广泛使用的案例：扫描信用卡号码。如果你已经将信用卡号码添加到手机上的应用程序中，你可能已经注意到可以选择拍照信用卡而不是输入数字。识别你的信用卡号码的模型几乎肯定是在没有任何人工标注的纯合成数据上构建的。它符合上述所有四种情况。你的信用卡号码最初是结构化数据，但它被打印在实体卡上，然后拍摄了该打印数字的照片，这是一个重构16个数字的约束性问题。没有扫描信用卡的大规模公开数据仓库。如果数据科学家和标注者能看到实际卡片的所有扫描图像进行标注，将引发隐私和安全问题。最后，如果扫描不成功，最终用户通常可以手动输入他们的卡号。
- en: Most applications using synthetic data still include some data annotation, so
    the following strategies are typically used to complement human annotation. If
    you could create all the data you need for the model programmatically, you probably
    wouldn’t need machine learning in the first place.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数使用合成数据的应用仍然包括一些数据标注，因此以下策略通常用于补充人工标注。如果你能够通过编程创建模型所需的所有数据，那么你可能一开始就不需要机器学习。
- en: 9.7.2 Data creation
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.7.2 数据创建
- en: One effective method to address the lack of data is to ask the annotators to
    create it. This approach is a common one for creating speech data (chapter 10).
    For text data, this approach can be effective for addressing gaps in the data.
    Although not as realistic as spontaneous text, this approach can be preferable
    to having no data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 解决数据缺乏的一个有效方法是要求标注者创建数据。这种方法是创建语音数据（第10章）的常见方法。对于文本数据，这种方法可以有效地解决数据中的空白。尽管不如自发文本真实，但这种方法可能比没有数据更好。
- en: Data about disease outbreaks
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 疾病爆发数据
- en: When I started writing this book, I included an example of data creation based
    on the observation that there were few news headlines about disease outbreaks
    in North America. Sadly, that’s no longer true during COVID-19.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始写这本书时，我包括了一个基于观察的数据创建示例，即北美关于疾病爆发的新闻标题很少。遗憾的是，在COVID-19期间，这种情况不再成立。
- en: For the dataset creation task, I asked annotators to imagine that they were
    experiencing a disease outbreak and used a rule-based system to generate a different
    prompt for each annotator. The rules varied the prompts by factors such as whether
    they were directly experiencing, witnessing, or hearing about the outbreaks secondhand;
    by how many people were infected or exposed; and so on. This approach was designed
    to get as much variety as possible to overcome the limitations in artificial text
    over spontaneous text.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集创建任务中，我要求标注者想象他们正在经历疾病爆发，并使用基于规则的系统为每位标注者生成不同的提示。规则通过他们是否直接经历、目睹或间接听说爆发；感染或暴露的人数等因素来变化。这种方法旨在尽可能多地获得多样性，以克服人工文本在自发文本中的局限性。
- en: I’m leaving this dataset out of the book and will consider releasing it when
    the pandemic is over. It may be interesting to see how someone’s lived experience
    changes how realistically they can create example data at that time.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我将这个数据集从书中排除，并考虑在疫情结束后发布。届时可能会很有趣地看到某人的生活经历如何影响他们当时创建示例数据的现实程度。
- en: Some interesting recent techniques for automated data creation combine data
    creation and synthetic data, including generative adversarial networks (GANs)
    for images and language models for text. If you want pictures of bicycles, you
    can train GANs on existing pictures of bicycles to create new but realistic pictures
    of bicycles. Similarly, you can train language models to create novel sentences
    containing certain phrases or about certain topics. These models are often the
    same types of pretrained models that are used for contextual embeddings. In both
    cases, the data is rarely 100% accurate, so human review can help filter which
    generated data is realistic.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有趣的自动化数据创建技术结合了数据创建和合成数据，包括用于图像的生成对抗网络（GANs）和用于文本的语言模型。如果你想看自行车的图片，你可以训练GANs在现有的自行车图片上创建新的但逼真的自行车图片。同样，你可以训练语言模型来创建包含特定短语或关于特定主题的新句子。这些模型通常是用于上下文嵌入的预训练模型。在两种情况下，数据很少达到100%的准确性，因此人工审查可以帮助筛选哪些生成数据是真实的。
- en: When data is created by humans or automated processes, it can help address the
    sensitivity of training data. You might have a language model built on data scraped
    from the web that effectively captures some sensitive data, such as people’s addresses,
    which could leave a model vulnerable to reverse engineering to expose those addresses.
    If you can rewrite all the sequences with a language model and test that the new
    sequences do not occur in the original data, however, you can build a second model
    on that new data. That model should be much harder to reverse-engineer to discover
    the sensitive information. Data sensitivity is outside the scope of this book,
    but it is flagged here as an important area in which human-in-the-loop machine
    learning can help.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据由人类或自动化过程创建时，这有助于解决训练数据的敏感性。你可能有一个基于从网络抓取的数据构建的语言模型，该模型有效地捕捉了一些敏感数据，例如人们的地址，这可能会使模型容易受到逆向工程的影响，从而暴露这些地址。然而，如果你可以使用语言模型重写所有序列并测试新序列是否出现在原始数据中，那么你可以在这些新数据上构建第二个模型。该模型应该更难被逆向工程以发现敏感信息。数据敏感性超出了本书的范围，但在这里将其标记为一个重要领域，其中人机交互的机器学习可以帮助。
- en: 9.7.3 Data augmentation
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.7.3 数据增强
- en: If you work in computer vision, you are familiar with data augmentation techniques
    such as flipping, cropping, rotating, darkening, and otherwise modifying some
    of the training data items to create more items or more diversity within those
    items. A similar technique exists in NLP, replacing words with synonyms from a
    database of synonyms or programmatically from words with similar embeddings.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从事计算机视觉工作，你应该熟悉数据增强技术，如翻转、裁剪、旋转、变暗等，这些技术通过修改一些训练数据项以创建更多项目或在这些项目内增加多样性。在自然语言处理（NLP）中，也存在类似的技巧，即用同义词数据库中的同义词或通过具有相似嵌入的词语替换原词。
- en: In machine translation and other use cases, back translation is a popular data
    augmentation method, translating a sentence into another language and back again
    to create potentially new but synonymous sentences. If you translated “This is
    great” into French and back into English, the sentence might come back as “This
    is very good.” You can treat “This is very good” as another valid translation.
    This approach works for other use cases too. If you are implementing sentiment
    analysis and have “This is great” as a data point labeled as positive sentiment,
    you can use back translation to create “This is very good” as another labeled
    data item for positive sentiment.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器翻译和其他用例中，反向翻译是一种流行的数据增强方法，即将句子翻译成另一种语言，然后再翻译回原语言，以创建可能新颖但同义的句子。如果你将“这太棒了”翻译成法语再翻译回英语，句子可能变成“这非常好。”你可以将“这非常好”视为另一个有效的翻译。这种方法也适用于其他用例。如果你正在实施情感分析，并且将“这太棒了”作为标记为积极情感的语料库数据点，你可以使用反向翻译来创建“这非常好”作为另一个标记为积极情感的语料库数据项。
- en: Masked language modeling with pretrained models is a similar technique. Recall
    that commonly used pretrained models predict missing words in context. This technique
    can be used to create similar sentences. You could take the sentence “Alex drove
    to the shop” and ask the system to predict the MASK in the sentence “Alex [MASK]
    to the shop.” This example might produce sentences like “Alex went to the shop,”
    “Alex walked to the shop,” and sentences with similar meanings, which can make
    a much larger dataset quickly and efficiently.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型的掩码语言建模是一种类似的技术。回想一下，常用的预训练模型可以预测上下文中的缺失词语。这项技术可以用来创建类似的句子。你可以取句子“亚历克斯开车去商店”并要求系统预测句子“亚历克斯
    MASK 去商店。”中的MASK。这个例子可能产生句子如“亚历克斯去了商店”、“亚历克斯步行去商店”以及具有相似意义的句子，这样就可以快速有效地创建一个更大的数据集。
- en: 9.8 Incorporating annotation information into machine learning models
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.8 将注释信息整合到机器学习模型中
- en: You can’t always avoid wrongly labeled data. But you can use several strategies
    to get the most accurate possible model downstream even when you know that not
    all the labels are correct.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你无法总是避免错误标记的数据。但即使你知道并非所有标签都是正确的，你仍然可以使用几种策略来获取尽可能准确的下游模型。
- en: 9.8.1 Filtering or weighting items by confidence in their labels
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.8.1 根据对标签的信心过滤或加权项目
- en: The easiest method is to drop all training data items with low annotation confidence.
    You can tune the right number to drop using held-out validation data. This approach
    almost always improves the accuracy of a model but is too often overlooked because
    people want to use every possible annotation. If you are dropping some items,
    make sure that you at least spot-check what you are dropping so that you aren’t
    creating biases in your data. Something might be low-confidence because it comes
    from an underrepresented demographic. Use diversity sampling to help rebalance
    the data in this case.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是删除所有低标注置信度的训练数据项。你可以使用保留的验证数据调整要删除的正确数量。这种方法几乎总是可以提高模型的准确性，但往往被忽视，因为人们想要使用尽可能多的标注。如果你正在删除某些项目，请确保你至少检查你正在删除的内容，这样你就不会在数据中产生偏差。某些内容可能因为来自代表性不足的人口群体而置信度低。在这种情况下，使用多样性采样来帮助重新平衡数据。
- en: Instead of dropping low-confidence items, you can de-weight them in your model.
    Some models allow you to weight items differently as part of their inputs. If
    that isn’t the case with your model, you can programmatically select items in
    your training epochs according to your confidence in the labels, selecting the
    more confident labels more often.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是删除低置信度项目，你可以在模型中降低它们的权重。一些模型允许你作为其输入的一部分以不同的方式对项目进行加权。如果你的模型不是这种情况，你可以根据你对标签的置信度，在训练周期中编程选择项目。你应该能够使用验证数据调整这个过程，以便获得最佳的预测精度，同时仍然有一个包含标注者身份的模型。
- en: 9.8.2 Including the annotator identity in inputs
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.8.2 在输入中包含标注者身份
- en: Including the annotators’ identities as a feature in your model can increase
    your model’s predictive abilities, especially for predicting uncertainty. You
    can include additional binary fields that indicate which annotator contributed
    to the label. This approach is similar to including annotator identity in models
    to converge on the right label when annotators disagreed, but here, we are including
    their identities in the downstream model that we are deploying on new data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 将标注者的身份作为模型中的一个特征可以增加模型的预测能力，尤其是在预测不确定性方面。你可以包含额外的二进制字段，指示哪个标注者对标签做出了贡献。这种方法类似于在标注者意见不一致时将标注者身份包含在模型中，以收敛到正确的标签，但在这里，我们是在包含标注者身份的下游模型中部署新数据。
- en: Obviously, your unlabeled data does not have annotators associated with it.
    You can get predictions from the model without any annotator fields for your actual
    prediction. Then you can get additional predictions with the different annotator
    fields set. If the prediction changes based on the different field, your model
    is telling you that different annotators would have annotated that data point
    differently. This information is useful for identifying items on which agreement
    among annotators may have been low.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你的未标记数据没有与之关联的标注者。你可以从模型中获得预测，而无需为实际预测设置任何标注者字段。然后，你可以通过设置不同的标注者字段来获得额外的预测。如果预测根据不同的字段而改变，那么你的模型正在告诉你不同的标注者可能会对数据点进行不同的标注。这个信息对于识别标注者之间可能达成一致的项目非常有用。
- en: The accuracy of the overall model might go down if you are introducing a field
    to capture the annotator identity. In that case, you can set all the annotator
    fields to 0 for some of the training items, either in the data itself or as a
    mask for some training epochs. You should be able to tune this process with validation
    data so that you can get the optimal predictive accuracy but still have a model
    that incorporates the annotator identities.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你引入一个字段来捕捉标注者身份，整体模型的准确性可能会下降。在这种情况下，你可以将一些训练项目的所有标注者字段设置为0，无论是在数据本身中还是在某些训练周期中的掩码。你应该能够使用验证数据调整这个过程，以便获得最佳的预测精度，同时仍然有一个包含标注者身份的模型。
- en: 9.8.3 Incorporating uncertainty into the loss function
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.8.3 将不确定性纳入损失函数
- en: 'The most direct way to use label uncertainty in the downstream model is to
    incorporate it directly into the loss function. For many machine learning tasks,
    you will be encoding your labels as all-or-nothing one-hot encodings:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在下游模型中使用标签不确定性的最直接方法是将它直接纳入损失函数。对于许多机器学习任务，你将把你的标签编码为全或无的一热编码：
- en: '| Animal | Cyclist | Pedestrian | Sign |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 动物 | 骑自行车的人 | 行人 | 标志 |'
- en: '| 0 | 1 | 0 | 0 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 | 0 |'
- en: 'Suppose, however, that this was your actual label confidence from your annotations:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，然而，这是你从标注中得到的实际标签置信度：
- en: '| Animal | Cyclist | Pedestrian | Sign |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 动物 | 骑自行车的人 | 行人 | 标志 |'
- en: '| 0 | 0.7 | 0.3 | 0 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.7 | 0.3 | 0 |'
- en: 'Instead of declaring “Cyclist” to be the correct label and encoding it as 1,
    your model might allow your objective function to take 0.7 as the value that your
    loss function is trying to minimize. That is, you are asking the model to converge
    on 0.7 instead of 1.0 for this example. If you have confidence intervals, you
    have some more options. Suppose that our confidence is 0.7, plus or minus 0.1:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 与将“骑行者”声明为正确标签并将其编码为1不同，您的模型可能允许您的目标函数将0.7作为损失函数试图最小化的值。也就是说，您要求模型收敛到0.7而不是1.0，在这个例子中。如果您有置信区间，您有更多的选择。假设我们的置信度为0.7，加减0.1：
- en: '| Animal | Cyclist | Pedestrian | Sign |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 动物 | 骑行者 | 行人 | 标志 |'
- en: '| 0 | 0.7 (±0.1) | 0.3 (±0.1) | 0 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.7 (±0.1) | 0.3 (±0.1) | 0 |'
- en: In this case, we might be equally happy if the model converges on any value
    between 0.6 and 0.8 for “Cyclist.” So, we can modify our training to account for
    this result. Depending on your architecture, you may not have to change the output
    of the loss function itself; you may be able to skip this item in any training
    epoch when the model is predicting “Cyclist” between 0.6 and 0.8.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果模型收敛到“骑行者”0.6到0.8之间的任何值，我们可能都会感到同样高兴。因此，我们可以修改我们的训练以考虑这个结果。根据您的架构，您可能不需要更改损失函数本身的输出；您可能能够在模型预测“骑行者”在0.6到0.8之间时跳过任何训练周期的这一项。
- en: If you have a more finely calibrated understanding of your label confidence,
    you can modify the output of the loss function itself. If you are 0.7 confident
    in the label, but with a Gaussian degree of certainty on either side of that 0.7
    number, you might be able to incorporate the degree of uncertainty into your loss
    function, forgiving some but not all of the loss, as the prediction is closer
    to 0.7.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对标签置信度有更精细的理解，您可以修改损失函数本身的输出。如果您对标签的置信度为0.7，但在这0.7数字的两侧有高斯程度的确定性，您可能能够将不确定性的程度纳入您的损失函数，对一些损失进行宽恕，而不是全部宽恕，因为预测更接近0.7。
- en: You can experiment with the methods in this section programmatically, so it
    should be relatively easy to try different ways to incorporate the annotators
    and annotation uncertainty in your models to see how they work for your tasks.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过编程方式尝试本节中的方法，因此尝试将注释者和注释不确定性纳入您的模型的不同方式应该相对容易，以查看它们对您的任务如何工作。
- en: 9.9 Further reading for advanced annotation
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.9 高级注释的进一步阅读
- en: This chapter mostly used the relatively simple example of labeling at the image
    and document level, with some extensions to semantic segmentation and machine
    translation. Chapter 10 talks about how these methods can be applied to many types
    of machine learning problems. The same principles apply, but some techniques are
    better or worse for certain problems.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要使用了相对简单的图像和文档级别的标签示例，并对语义分割和机器翻译进行了一些扩展。第10章讨论了这些方法如何应用于许多类型的机器学习问题。相同的原理适用，但某些技术在某些问题上的效果更好或更差。
- en: Some of the further reading in this section assumes more complicated tasks than
    labeling, so depending on the paper, you may want to read chapter 10 before returning
    to the literature.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的一些进一步阅读假设的任务比标签更复杂，因此根据论文的不同，您可能需要在返回文献之前先阅读第10章。
- en: 9.9.1 Further reading for subjective data
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.1 主观数据的进一步阅读
- en: In 2017, Dražen Prelec, H. Sebastian Seung, and John McCoy published “A solution
    to the single-question crowd wisdom problem” ([http://mng.bz/xmgg](http://mng.bz/xmgg)),
    specifically looking at answers for which the actual response rate is more popular
    than predicted response, even if it is not the most popular response overall.
    (This paper is not open.) Dražen Prelec’s original manuscript for BTS for subjective
    data is at [https://economics.mit.edu/files/1966](https://economics.mit.edu/files/1966).
    A shorter version, later published in *Science* is at [http://mng.bz/A0qg](http://mng.bz/A0qg).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，Dražen Prelec、H. Sebastian Seung和John McCoy发表了“解决单问题群体智慧问题的方案”（[http://mng.bz/xmgg](http://mng.bz/xmgg)），特别关注那些实际响应率比预测响应率更受欢迎的答案，即使它不是整体上最受欢迎的答案。（这篇论文不是公开的。）Dražen
    Prelec为主观数据编写的BTS原始手稿可在[https://economics.mit.edu/files/1966](https://economics.mit.edu/files/1966)找到。后来发表在《Science》上的简短版本可在[http://mng.bz/A0qg](http://mng.bz/A0qg)找到。
- en: For an interesting extension to BTS that addresses some of the concerns raised
    in this chapter, see “A Robust Bayesian Truth Serum for Non-Binary Signals,” by
    Goran Radanovic and Boi Faltings ([https://www.aaai.org/ocs/index.php/AAAI/AAAI13/
    paper/view/6451](https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6451)).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解决本章中提出的一些担忧的BTS有趣扩展，请参阅Goran Radanovic和Boi Faltings的“用于非二元信号的鲁棒贝叶斯真理血清”，[https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6451](https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6451)。
- en: 9.9.2 Further reading for machine learning for annotation quality control
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.2 关于机器学习用于注释质量控制的进一步阅读
- en: 'For methods to calculate confidence that combines machine and human confidence,
    see “Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance,”
    by Gagan Bansa, Besmira Nushi, Ece Kamar, Walter Lasecki, Daniel Weld, and Eric
    Horvitz ([http://mng.bz/ZPM5](http://mng.bz/ZPM5)).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 关于结合机器和人类置信度的计算置信度方法，请参阅Gagan Bansa、Besmira Nushi、Ece Kamar、Walter Lasecki、Daniel
    Weld和Eric Horvitz的“超越准确性：心理模型在人类-人工智能团队绩效中的作用”，[http://mng.bz/ZPM5](http://mng.bz/ZPM5)。
- en: For the problems with annotator bias in NLP tasks, see “Are We Modeling the
    Task or the Annotator? An Investigation of Annotator Bias in Natural Language
    Understanding Datasets,” by Mor Geva, Yoav Goldberg, and Jonathan Berant, who
    suggest that evaluation data (test sets) should be created by different annotators
    from those creating the training data ([http://mng.bz/RX6D](http://mng.bz/RX6D)).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP任务中注释者偏差的问题，请参阅Mor Geva、Yoav Goldberg和Jonathan Berant的“我们在建模任务还是注释者？对自然语言理解数据集中注释者偏差的调查”，他们建议评估数据（测试集）应由创建训练数据的不同注释者创建，[http://mng.bz/RX6D](http://mng.bz/RX6D)。
- en: “Learning from Noisy Singly-Labeled Data,” by Ashish Khetan, Zachary C. Lipton,
    and Anima Anandkumar, gives a detailed method for estimating confidence in annotations
    by using both annotator performance and model predictions ([http://mng.bz/2ed9](http://mng.bz/2ed9)).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Ashish Khetan、Zachary C. Lipton和Anima Anandkumar的“从有噪声的单标签数据中学习”，详细介绍了通过使用注释者性能和模型预测来估计注释置信度的方法，[http://mng.bz/2ed9](http://mng.bz/2ed9)。
- en: One of the earliest and most influential papers about using model predictions
    as labels is “Learning from Labeled and Unlabeled Data with Label Propagation,”
    by Xiaojin Zhu and Zoubin Ghahramani ([http://mng.bz/1rdy](http://mng.bz/1rdy)).
    Both authors continue to publish papers related to active learning and semi-supervised
    learning, which are also worth looking into.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用模型预测作为标签的最早和最有影响力的论文之一是Xiaojin Zhu和Zoubin Ghahramani的“从标记和无标记数据中学习，使用标签传播”，[http://mng.bz/1rdy](http://mng.bz/1rdy)。两位作者继续发表与主动学习和半监督学习相关的论文，这些论文也值得一看。
- en: 9.9.3 Further reading for embeddings/contextual representations
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.3 嵌入/上下文表示的进一步阅读
- en: More than any other machine learning research featured in this book, the literature
    for transfer learning looks the least far into the past. Embeddings started with
    methods such as latent semantic indexing (LSI) in information retrieval to support
    search engines in the 1990s and the 2000s saw many supervised variations of LSI,
    often with clever ways of getting free labels, such as looking at links between
    documents. Supervised embeddings became widely popular in computer vision in the
    early 2010s, particularly with transfer learning from large computer vision datasets
    like ImageNet, and in NLP in the late 2010s. NLP and computer vision scientists,
    however, rarely reference one another or early information retrieval work. If
    you are interested in this topic, I recommend looking into all three fields.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中介绍的任何其他机器学习研究相比，迁移学习的文献似乎对过去的探索最少。嵌入技术始于20世纪90年代和21世纪初的信息检索中的潜在语义索引（LSI）方法，以支持搜索引擎。2000年代见证了LSI的许多监督变体，通常采用巧妙的方法来获取免费标签，例如查看文档之间的链接。监督嵌入在2010年代初在计算机视觉中变得非常流行，特别是从像ImageNet这样的大型计算机视觉数据集中进行迁移学习，以及在2010年代末的自然语言处理（NLP）中。然而，NLP和计算机视觉科学家很少相互引用或早期信息检索工作。如果你对这个主题感兴趣，我建议你研究这三个领域。
- en: You can start with the seminal 1990 paper “Indexing by Latent Semantic Analysis,”
    by Scott Deerwester, Susan Dumais, George Furnas, Thomas Landauer, and Richard
    Harshman ([http://mng.bz/PPqg](http://mng.bz/PPqg)).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从1990年的开创性论文“通过潜在语义分析进行索引”开始，由Scott Deerwester、Susan Dumais、George Furnas、Thomas
    Landauer和Richard Harshman撰写，[http://mng.bz/PPqg](http://mng.bz/PPqg)。
- en: 'For cutting-edge research on how contextual models with more labels on adjacent
    tasks can help, see “Intermediate-Task Transfer Learning with Pretrained Language
    Models: When and Why Does It Work?”, by Yada Pruksachatkun, Jason Phang, Haokun
    Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina
    Kann, and Samuel R. Bowman ([http://mng.bz/JDqP](http://mng.bz/JDqP)). As an extension
    to this paper in multilingual settings, see this paper by the same authors, with
    Jason Phang as lead researcher: “English Intermediate-Task Training Improves Zero-Shot
    Cross-Lingual Transfer Too” ([http://mng.bz/w9aW](http://mng.bz/w9aW)).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何通过在相邻任务上具有更多标签的上下文模型来帮助的最新研究，请参阅Yada Pruksachatkun、Jason Phang、Haokun Liu、Phu
    Mon Htut、Xiaoyi Zhang、Richard Yuanzhe Pang、Clara Vania、Katharina Kann和Samuel R.
    Bowman的“使用预训练语言模型的中间任务迁移学习：何时以及为什么它有效？”（[http://mng.bz/JDqP](http://mng.bz/JDqP)）。作为这篇论文在多语言环境中的扩展，请参阅同一作者团队，以Jason
    Phang为首席研究员的这篇论文：“英语中间任务训练改善零样本跨语言迁移”（[http://mng.bz/w9aW](http://mng.bz/w9aW)）。
- en: 9.9.4 Further reading for rule-based systems
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.4 基于规则系统的进一步阅读
- en: 'For current research on rule-based systems, see “Snorkel: Rapid Training Data
    Creation with Weak Supervision,” by Alexander Ratner, Stephen H. Bach, Henry Ehrenberg,
    Jason Fries, Sen Wu, and Christopher Ré ([http://mng.bz/q9vE](http://mng.bz/q9vE))
    and the list of applications and resources on their website: [https://www.snorkel.org/resources](https://www.snorkel.org/resources).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于规则系统的当前研究，请参阅亚历山大·拉特纳（Alexander Ratner）、斯蒂芬·H·巴赫（Stephen H. Bach）、亨利·埃伦伯格（Henry
    Ehrenberg）、贾森·弗赖斯（Jason Fries）、森武（Sen Wu）和克里斯托弗·雷（Christopher Ré）的“Snorkel：使用弱监督快速创建训练数据”（[http://mng.bz/q9vE](http://mng.bz/q9vE)），以及他们网站上列出的应用和资源：[https://www.snorkel.org/resources](https://www.snorkel.org/resources)。
- en: 'For a nonfree resource that dives deep into these technologies, see Russell
    Jurney’s upcoming (at the time of publication) book *Weakly Supervised Learning:
    Doing More with Less data* (O’Reilly).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深入探讨这些技术的非自由资源，请参阅拉塞尔·朱尼（Russell Jurney）即将出版的（在出版时）书籍《弱监督学习：用更少的数据做更多的事情》（O’Reilly）。
- en: 9.9.5 Further reading for incorporating uncertainty in annotations into the
    downstream models
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.5 将标注的不确定性纳入下游模型的进一步阅读
- en: For recent research on ways to model uncertainty about annotations in downstream
    models, “Learning from noisy singly-labeled data” (section 9.9.2) is a good starting
    point, tackling the hard task that arises in the case of little agreement information
    and many errors by annotators.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在下游模型中建模标注不确定性的最近研究，“从噪声单标签数据中学习”（第9.9.2节）是一个良好的起点，它解决了在标注者之间信息很少且错误很多的情况下出现的困难任务。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Subjective tasks have items with multiple correct annotations. You can elicit
    the set of valid responses that people might give from the annotators and then
    use methods such as BTS to discover all the valid responses and avoid penalizing
    correct but rarer annotations.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主观任务包含具有多个正确标注的项目。你可以从标注者那里引出人们可能给出的有效响应集合，然后使用BTS等方法发现所有有效响应，并避免惩罚正确但较罕见的标注。
- en: Machine learning can be used to calculate the confidence of a single annotation
    and to resolve disagreements among annotators. For many annotation tasks, simple
    heuristics are not enough to accurately calculate annotation quality or to aggregate
    the annotations of different people, so machine learning gives us more powerful
    ways to create the most accurate labels from human annotations.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习可以用来计算单个标注的置信度，并解决标注者之间的分歧。对于许多标注任务，简单的启发式方法不足以准确计算标注质量或聚合不同人的标注，因此机器学习为我们提供了更强大的方法来从人类标注中创建最准确的标签。
- en: The predictions from a model can be used as the source of annotations. By using
    the most confident predictions from a model or by treating a model as one annotator
    among other annotators, you can reduce the overall number of human annotations
    that are needed. This technique can be especially helpful when you want to take
    the predictions from an old model and use them in a new model architecture and
    when annotation is a time-consuming task compared with accepting or rejecting
    model predictions.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的预测可以作为标注的来源。通过使用模型中最自信的预测，或者将模型视为众多标注者之一，你可以减少所需的人类标注总数。这种技术在你想将旧模型的预测用于新模型架构，并且标注与接受或拒绝模型预测相比是耗时任务时，尤其有帮助。
- en: Embeddings and contextual representations allow you to adapt the knowledge from
    existing models into your target model as feature embeddings or tuning pretrained
    models. This approach can inform your annotation strategy. If you can find a related
    task that is 10 times or 100 times faster to annotate than your target task, for
    example, you might get a more accurate model if you devote some resources to the
    simpler task and use the simpler task as an embedding in the actual task.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入和上下文表示允许您将现有模型的知识适应到目标模型中，作为特征嵌入或调整预训练模型。这种方法可以指导您的标注策略。例如，如果您能找到一个比目标任务快10倍或100倍的相关任务，那么如果您将一些资源投入到这个简单的任务中，并使用它作为实际任务中的嵌入，您可能会得到一个更精确的模型。
- en: Search-based and rule-based systems allow you to quickly filter and possibly
    label your data. These systems are especially useful for annotating a model quickly
    with noisy data and finding important low-frequency data to annotate.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于搜索和基于规则的系统允许您快速过滤和可能标注数据。这些系统在快速标注模型，特别是使用噪声数据标注模型以及寻找重要的低频数据标注时特别有用。
- en: Light supervision on unsupervised models are common ways that annotators, especially
    SMEs, bootstrap a model from a small number of labels or perform exploratory data
    analysis when the goal is improved human understanding of the data, not necessarily
    a supervised model.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在无监督模型上进行轻量级监督是标注者，尤其是SMEs（行业专家），从少量标签中启动模型或当目标是提高人类对数据的理解而不是监督模型时进行探索性数据分析的常见方法。
- en: Synthetic data, data creation, and data augmentation are related strategies
    that create novel data items, especially useful when the available unlabeled data
    does not contain the required diversity of data, often because data is rare or
    sensitive.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合成数据、数据创建和数据增强是相关策略，它们创建新的数据项，特别适用于可用的未标记数据不包含所需数据多样性时，通常因为数据稀缺或敏感。
- en: 'There are several ways to incorporate annotation uncertainty into a downstream
    model: filtering out or de-weighting items with uncertain label accuracy, including
    the annotator identities in the training data, and incorporating the uncertainty
    into the loss function while training. These methods can help prevent annotation
    errors from becoming unwanted biases in your models.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标注不确定性融入下游模型的方法有几种：过滤掉或降低标签准确度不确定的项的权重，包括标注者在训练数据中的身份，以及在训练过程中将不确定性纳入损失函数。这些方法有助于防止标注错误成为模型中的不希望出现的偏差。
