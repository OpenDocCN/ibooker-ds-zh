- en: 17 Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17 分类
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Classifying with decision trees
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行分类
- en: Building a random forest classifier
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建随机森林分类器
- en: Creating a support vector machine
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建支持向量机
- en: Evaluating classification accuracy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类精度
- en: Understanding complex models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解复杂模型
- en: Data analysts frequently need to predict a categorical outcome from a set of
    predictor variables. Some examples include
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师经常需要从一组预测变量中预测一个分类结果。一些例子包括
- en: Predicting whether an individual will repay a loan, given their demographics
    and financial history
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据个人的人口统计和财务历史预测个人是否会偿还贷款
- en: Determining whether an ER patient is having a heart attack, based on their symptoms
    and vital signs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据患者的症状和生命体征判断一个ER患者是否正在经历心脏病发作
- en: Deciding whether an email is spam, given the presence of key words, images,
    hypertext, header information, and origin
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据关键词、图像、超文本、标题信息和来源判断一封电子邮件是否为垃圾邮件
- en: Each of these cases involves the prediction of a binary categorical outcome
    (good credit risk/bad credit risk; heart attack/no heart attack; spam/not spam)
    from a set of predictors (also called *features*). The goal is to find an accurate
    method of classifying new cases into one of the two groups.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些案例中的每一个都涉及从一组预测变量（也称为*特征*）中预测一个二元分类结果（良好信用风险/不良信用风险；心脏病发作/无心脏病发作；垃圾邮件/非垃圾邮件）。目标是找到一种准确的方法将新案例分类到两个组之一。
- en: The field of supervised machine learning offers numerous classification methods
    for predicting categorical outcomes, including logistic regression, decision trees,
    random forests, support vector machines, and artificial neural networks. The first
    four are discussed in this chapter. Artificial neural networks are beyond the
    scope of this book. See Ciaburro and Venkateswaran (2017) and Chollet and Allaire
    (2018) to learn more about them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习领域提供了许多用于预测分类结果的分类方法，包括逻辑回归、决策树、随机森林、支持向量机和人工神经网络。前四种在本章中讨论。人工神经网络超出了本书的范围。参见Ciaburro和Venkateswaran（2017）以及Chollet和Allaire（2018）以了解更多相关信息。
- en: Supervised learning starts with a set of observations containing values for
    both the predictor variables and the outcome. The dataset is then divided into
    a training sample and a test sample. A predictive model is developed using the
    data in the training sample and tested for accuracy using the data in the test
    sample. Both samples are needed because classification techniques maximize prediction
    for a given set of data. Estimates of their effectiveness will be overly optimistic
    if they’re evaluated using the same data that generated the model. By applying
    the classification rules developed on a training sample to a separate test sample,
    you can obtain a more realistic accuracy estimate. Once you’ve created an effective
    predictive model, you can use it to predict outcomes in situations when only the
    predictor variables are known.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习从包含预测变量和结果值的观察值集合开始。然后，数据集被分为训练样本和测试样本。使用训练样本中的数据开发一个预测模型，并使用测试样本中的数据测试其准确性。需要这两个样本，因为分类技术最大化给定数据集的预测。如果使用生成模型的数据来评估其有效性，估计将过于乐观。通过将训练样本上开发的分类规则应用于单独的测试样本，可以获得更现实的准确性估计。一旦创建了有效的预测模型，就可以使用它来预测只有预测变量已知的情况下的结果。
- en: 'In this chapter, you’ll use the `rpart`, `rattle`, and `partykit` packages
    to create and visualize decision trees; the `randomForest` package to fit random
    forests; and the `e1071` package to build support vector machines. Logistic regression
    will be fit with the `glm(`) function in the base R installation. Before starting,
    be sure to install the necessary packages:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将使用`rpart`、`rattle`和`partykit`包来创建和可视化决策树；使用`randomForest`包来拟合随机森林；以及使用`e1071`包来构建支持向量机。逻辑回归将通过基础R安装中的`glm()`函数进行拟合。在开始之前，请确保安装必要的包：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The primary example used in this chapter comes from the Wisconsin Breast Cancer
    data originally posted to the UCI Machine Learning Repository. The goal will be
    to develop a model for predicting whether a patient has breast cancer from the
    characteristics of a fine-needle tissue aspiration (a tissue sample taken with
    a thin hollow needle from a lump or mass just under the skin).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的首要示例来自威斯康星州乳腺癌数据，该数据最初发布在UCI机器学习库中。目标是开发一个模型，从细针穿刺组织抽吸（从皮肤下肿块或团块中用细空心针取出的组织样本）的特征来预测患者是否患有乳腺癌。
- en: 17.1 Preparing the data
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 准备数据
- en: The Wisconsin Breast Cancer dataset is available as a comma-delimited text file
    on the UCI Machine Learning Server ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
    The dataset contains 699 fine-needle aspirate samples, where 458 (65.5%) are benign
    and 241 (34.5%) are malignant. The dataset contains 11 variables and doesn’t include
    the variable names in the file. Sixteen samples have missing data and are coded
    in the text file with a question mark (?).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星州乳腺癌数据集作为逗号分隔的文本文件可在UCI机器学习服务器上获得（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）。该数据集包含699个细针穿刺样本，其中458个（65.5%）为良性，241个（34.5%）为恶性。数据集包含11个变量，文件中不包含变量名。16个样本有缺失数据，并在文本文件中以问号（?）编码。
- en: 'The variables are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 变量如下：
- en: ID
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ID
- en: Clump thickness
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块状厚度
- en: Uniformity of cell size
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细胞大小均匀性
- en: Uniformity of cell shape
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细胞形状均匀性
- en: Marginal adhesion
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边缘粘附
- en: Single epithelial cell size
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个上皮细胞大小
- en: Bare nuclei
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无核裸露
- en: Bland chromatin
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏染色质
- en: Normal nucleoli
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常核仁
- en: Mitoses
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有丝分裂
- en: Class
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别
- en: The first variable is an ID variable (which you’ll drop), and the last variable
    (class) contains the outcome (coded `2=benign`, `4=malignant`). You’ll also exclude
    the observations containing missing values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变量是一个ID变量（你将删除它），最后一个变量（类别）包含结果（编码为`2=良性`，`4=恶性`）。你还将排除包含缺失值的观测值。
- en: For each sample, nine cytological characteristics previously found to correlate
    with malignancy are also recorded. Each of these variables is scored from 1 (closest
    to benign) to 10 (most anaplastic). But no one predictor alone can distinguish
    between benign and malignant samples. The challenge is to find a set of classification
    rules that can be used to accurately predict malignancy from some combination
    of these nine cell characteristics. See Mangasarian and Wolberg (1990) for details.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个样本，记录了之前发现与恶性相关的九个细胞学特征。这些变量中的每一个都从1（最接近良性）评分到10（最不典型）。但没有任何一个预测因子可以单独区分良性和恶性样本。挑战在于找到一组分类规则，可以用来从这些九个细胞特征的某些组合中准确预测恶性。有关详细信息，请参阅Mangasarian和Wolberg（1990）。
- en: In the following listing, the comma-delimited text file containing the data
    is downloaded from the UCI repository and randomly divided into a training sample
    (70%) and a test sample (30%).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，包含数据的逗号分隔的文本文件从UCI存储库下载，并随机分为训练样本（70%）和测试样本（30%）。
- en: Listing 17.1 Preparing the breast cancer data
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.1 准备乳腺癌数据
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The training sample has 478 cases (302 benign, 176 malignant), and the test
    sample has 205 cases (142 benign, 63 malignant).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本有478个案例（302个良性，176个恶性），测试样本有205个案例（142个良性，63个恶性）。
- en: The training sample will be used to create classification schemes using logistic
    regression, a decision tree, a conditional decision tree, a random forest, and
    a support vector machine. The test sample will be used to evaluate the effectiveness
    of these schemes. By using the same example throughout the chapter, you can compare
    the results of each approach.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本将用于创建分类方案，使用逻辑回归、决策树、条件决策树、随机森林和支撑向量机。测试样本将用于评估这些方案的有效性。通过在整个章节中使用相同的示例，你可以比较每种方法的结果。
- en: 17.2 Logistic regression
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 逻辑回归
- en: '*Logistic* *regression* is a type of generalized linear model that is often
    used to predict a binary outcome from a set of numeric variables (see section
    13.2 for details). The `glm()` function in the base R installation is used for
    fitting the model. Categorical predictors (factors) are automatically replaced
    with a set of dummy coded variables. All the predictors in the Wisconsin Breast
    Cancer data are numeric, so dummy coding is unnecessary. The next listing provides
    a logistic regression analysis of the data.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*逻辑回归*是一种广义线性模型，常用于从一组数值变量中预测二元结果（有关详细信息，请参阅第13.2节）。在基础R安装中使用的`glm()`函数用于拟合模型。分类预测因子（因子）会自动替换为一组虚拟编码变量。威斯康星州乳腺癌数据中的所有预测因子都是数值的，因此不需要虚拟编码。下一个列表提供了数据集的逻辑回归分析。'
- en: Listing 17.2 Logistic regression with `glm()`
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.2 使用`glm()`进行逻辑回归
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Fits the logistic regression
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 拟合逻辑回归
- en: ❷ Examines the model
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检验模型
- en: ❸ Classifies new cases
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对新案例进行分类
- en: ❹ Evaluates the predictive accuracy
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 评估预测准确性
- en: First, a logistic regression model is fit using `class` as the dependent variable
    and the remaining variables as predictors ❶. The model is based on the cases in
    the train data frame. The coefficients for the model are displayed next ❷. Section
    13.2 provides guidelines for interpreting logistic model coefficients.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`class`作为因变量，其余变量作为预测变量，拟合一个逻辑回归模型 ❶。该模型基于训练数据框中的案例。接下来显示模型的系数 ❷。第13.2节提供了解释逻辑模型系数的指南。
- en: Next, the prediction equation developed on the train dataset is used to classify
    cases in the test dataset. By default, the `predict()` function predicts the log
    odds of having a malignant outcome. By using the `type="response"` option, the
    probability of obtaining a malignant classification is returned instead ❸. In
    the next line, cases with probabilities greater than 0.5 are classified into the
    malignant group, and cases with probabilities less than or equal to 0.5 are classified
    as benign.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用训练数据集上开发的预测方程对测试数据集中的案例进行分类。默认情况下，`predict()`函数预测恶性结果的log odds。通过使用`type="response"`选项，返回获得恶性分类的概率而不是log
    odds。在下一条线中，概率大于0.5的案例被分类为恶性组，而概率小于或等于0.5的案例被分类为良性。
- en: Finally, a cross-tabulation of actual status and predicted status (called a
    confusion matrix) is printed ❹. It shows that 140 cases that were benign were
    classified as benign, and 60 cases that were malignant were classified as malignant.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，打印出实际状态和预测状态（称为混淆矩阵）的交叉表 ❹。它显示140个良性案例被分类为良性，60个恶性案例被分类为恶性。
- en: The total number of cases correctly classified (also called the accuracy) was
    (140 + 60) / 205 or 98% in the test sample. Statistics for evaluating the accuracy
    of a classification scheme are discussed more fully in section 17.6.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试样本中，正确分类的案例总数（也称为准确率）为（140 + 60）/ 205 或 98%。在17.6节中更详细地讨论了评估分类方案准确性的统计方法。
- en: Before moving on, note that three of the predictor variables (`sizeUniformity`,
    `shapeUniformit`y, and `singleEpithelialCellSize`) have coefficients that don’t
    differ from zero at the p < .10 level. What, if anything, should you do with predictor
    variables that have nonsignificant coefficients?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请注意，有三个预测变量（`sizeUniformity`、`shapeUniformit`y和`singleEpithelialCellSize`）的系数在p
    < .10水平上与零没有差异。对于具有非显著系数的预测变量，你将如何处理？如果有的话。
- en: In a prediction context, it’s often useful to remove such variables from the
    final model. This is especially important when a large number of non-informative
    predictor variables are adding what is essentially noise to the system.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测上下文中，通常从最终模型中删除此类变量是有用的。当大量非信息性预测变量向系统中添加噪声时，这一点尤为重要。
- en: In this case, stepwise logistic regression can be used to generate a smaller
    model with fewer variables. Predictor variables are added or removed to obtain
    a model with a smaller AIC value. In the current context, you could use
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以使用逐步逻辑回归生成一个变量更少的较小模型。通过添加或删除预测变量以获得具有较小AIC值的模型。在当前上下文中，你可以使用
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: to obtain a more parsimonious model. The reduced model excludes the three previously
    mentioned variables. When used to predict outcomes in the test dataset, this reduced
    model performs equally well. Try it out.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以获得一个更简约的模型。简化后的模型排除了之前提到的三个变量。当用于预测测试数据集中的结果时，这个简化模型表现同样出色。试一试。
- en: The next approach we’ll consider involves the creation of decision or classification
    trees.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要考虑的方法涉及决策树或分类树的创建。
- en: 17.3 Decision trees
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 决策树
- en: '*Decision* *trees* are popular in data mining contexts. They involve creating
    a set of binary splits on the predictor variables in order to create a tree that
    can be used to classify new observations into one of two groups. In this section,
    we’ll look at two types of decision trees: classical trees and conditional inference
    trees.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*在数据挖掘领域很受欢迎。它们涉及在预测变量上创建一系列二元分割，以创建一个可以用于将新观测值分类为两组之一的树。在本节中，我们将探讨两种类型的决策树：经典树和条件推断树。'
- en: 17.3.1 Classical decision trees
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.1 经典决策树
- en: 'The process of building a *classical decision tree* starts with a binary outcome
    variable (in this case, benign/malignant) and a set of predictor variables (the
    nine cytology measurements). The algorithm is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 构建*经典决策树*的过程从二元结果变量（在本例中为良性/恶性）和一组预测变量（九项细胞学测量）开始。算法如下：
- en: Choose the predictor variable that best splits the data into two groups such
    that the purity (homogeneity) of the outcome in the two groups is maximized (that
    is, as many benign cases in one group and malignant cases in the other as possible).
    If the predictor is continuous, choose a cut-point that maximizes purity for the
    two groups. If the predictor variable is categorical (not applicable in this case),
    combine the categories to obtain two groups with maximum purity.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择将数据分为两组的最佳预测变量，使得两组的输出纯度（同质性）最大化（即在一个组中尽可能多的良性病例和在另一个组中的恶性病例）。如果预测变量是连续的，选择一个分割点，以最大化两组的纯度。如果预测变量是分类的（在本例中不适用），将类别组合起来，以获得具有最大纯度的两组。
- en: Separate the data into these two groups and continue the process for each subgroup.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为这两个组，并对每个子组继续进行这个过程。
- en: Repeat steps 1 and 2 until a subgroup contains fewer than a minimum number of
    observations or no splits decrease the impurity beyond a specified threshold.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1和2，直到子组包含少于最小观察数或没有分割可以降低超过指定阈值的纯度。
- en: The subgroups in the final set are called terminal nodes. Each terminal node
    is classified as one category of the outcome or the other, based on the most frequent
    value of the outcome for the sample in that node.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终集合中的子组被称为终端节点。每个终端节点根据该节点样本中输出最频繁的值被分类为输出的一类或另一类。
- en: To classify a case, run it down the tree to a terminal node, and assign it the
    modal outcome value assigned in step 3.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要对案例进行分类，将其沿树向下运行到终端节点，并分配在步骤3中分配的模态输出值。
- en: Unfortunately, this process tends to produce a tree that is too large and suffers
    from overfitting. As a result, new cases aren’t classified well. To compensate,
    you can prune the tree by choosing the tree with the lowest 10-fold cross-validated
    prediction error. This pruned tree is then used for future predictions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个过程往往会产生一个过大且过度拟合的树。因此，新的案例分类效果不佳。为了补偿，你可以通过选择具有最低10折交叉验证预测误差的树来修剪树。然后，这个修剪后的树将用于未来的预测。
- en: In R, decision trees can be grown and pruned using the `rpart``()` and `prune()`
    functions in the `rpart` package. The following listing creates a decision tree
    for classifying the cell data as benign or malignant.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，可以使用`rpart`包中的`rpart()`和`prune()`函数来生长和修剪决策树。以下列表创建了一个用于将细胞数据分类为良性或恶性的决策树。
- en: Listing 17.3 Creating a classical decision tree with `rpart()`
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.3 使用`rpart()`创建经典决策树
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Grows the tree
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生长树
- en: ❷ Prunes the tree
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 修剪树
- en: ❸ Classifies new cases
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对新案例进行分类
- en: First, the tree is grown using the `rpart()` function ❶. You can use `print(dtree)`
    and `summary(dtree)` to examine the fitted model (not shown here). The tree may
    be too large and require pruning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`rpart()`函数❶生长树。你可以使用`print(dtree)`和`summary(dtree)`来检查拟合的模型（此处未显示）。树可能太大，需要修剪。
- en: To choose a final tree size, examine the `cptable` component of the list returned
    by `rpart()`. It contains data about the prediction error for various tree sizes.
    The complexity parameter (`cp`) is used to penalize larger trees. Tree size is
    defined by the number of branch splits (`nsplit`). A tree with n splits has n
    + 1 terminal nodes. The `rel` error column contains the error rate for a tree
    of a given size in the training sample. The cross-validated error (`xerror`) is
    based on 10-fold cross-validation (also using the training sample). The `xstd`
    column contains the standard error of the cross-validation error.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择最终的树大小，检查由`rpart()`返回的列表中的`cptable`组件。它包含有关各种树大小的预测误差的数据。复杂性参数（`cp`）用于惩罚较大的树。树的大小由分支分割的数量（`nsplit`）定义。具有n个分割的树有n
    + 1个终端节点。`rel`误差列包含给定大小的树在训练样本中的误差率。交叉验证误差（`xerror`）基于10折交叉验证（也使用训练样本）。`xstd`列包含交叉验证误差的标准误差。
- en: The `plotcp()` function plots the cross-validated error against the complexity
    parameter (see figure 17.1). A good choice for the final tree size is the smallest
    tree with a cross-validated error within one standard error of the minimum cross-validated
    error value.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`plotcp()`函数绘制交叉验证误差与复杂性参数（见图17.1）的关系。对于最终树大小的一个好选择是具有在最小交叉验证误差值一个标准误差范围内的交叉验证误差的最小树。'
- en: '![](Images/CH17_F01_Kabacoff3.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH17_F01_Kabacoff3.png)'
- en: Figure 17.1 Complexity parameter vs. cross-validated error. The dotted line
    is the upper limit of the one standard deviation rule (0.16 + 0.03 = 0.19). The
    plot suggests selecting the tree with the leftmost `cp` value below the line.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1 复杂度参数与交叉验证误差的关系。虚线是标准差规则的上限（0.16 + 0.03 = 0.19）。该图表明应选择线下最左侧的 `cp` 值对应的树。
- en: The minimum cross-validated error is 0.16 with a standard error of 0.03\. In
    this case, the smallest tree with a cross-validated error within 0.16 ± 0.03 (that
    is, between 0.13 and 0.19) is selected. Looking at the `cptable` table in listing
    17.3, a tree with two splits (cross-validated error = 0.16) fits this requirement.
    Equivalently, you can select the tree size associated with the largest complexity
    parameter below the line in figure 17.1\. Results again suggest a tree with two
    splits (three terminal nodes).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最小交叉验证误差为 0.16，标准误差为 0.03。在这种情况下，选择交叉验证误差在 0.16 ± 0.03（即介于 0.13 和 0.19 之间）的最小树。查看列表
    17.3 中的 `cptable` 表，具有两个分割（交叉验证误差 = 0.16）的树符合这一要求。等效地，您可以选择图 17.1 中线下最大复杂度参数对应的树大小。结果再次表明，具有两个分割（三个终端节点）的树。
- en: The `prune()` function uses the complexity parameter to cut back a tree to the
    desired size. It takes the full tree and snips off the least important splits
    based on the desired complexity parameter. Based on the earlier discussion, we’ll
    use `prune (dtree,` `cp=0.01705)`, which returns a tree with the desired size
    ❷. The function returns the largest tree with a complexity parameter below the
    specified `cp` value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`prune()` 函数使用复杂度参数来将树裁剪到所需的大小。它接受完整的树，并根据所需的复杂度参数剪掉最不重要的分割。根据前面的讨论，我们将使用 `prune
    (dtree, cp=0.01705)`，它返回一个具有所需大小 ❷ 的树。该函数返回复杂度参数低于指定 `cp` 值的最大树。'
- en: The `fancyRpartPlot()` function in the rattle package is used to draw an attractive
    plot of the final decision tree (see figure 17.2). This function has several options
    (see `?fancyRpartPlot` for details). The `type=2` option (default) draws the split
    labels below each node. Additionally, the proportion of each class and the percentage
    of observations in each node are displayed. To classify an observation, start
    at the top of the tree, moving to the left branch if a condition is true or to
    the right otherwise. Continue moving down the tree until you hit a terminal node.
    Classify the observation using the label of the node.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: rattle 包中的 `fancyRpartPlot()` 函数用于绘制最终决策树的吸引人图表（见图 17.2）。此函数有几个选项（有关详细信息，请参阅
    `?fancyRpartPlot`）。`type=2` 选项（默认）在每个节点下方绘制分割标签。此外，还显示了每个类的比例以及每个节点中的观测值百分比。要分类一个观测值，从树的顶部开始，如果条件为真则移动到左侧分支，否则移动到右侧。继续沿着树向下移动，直到遇到终端节点。使用节点的标签对观测值进行分类。
- en: '![](Images/CH17_F02_Kabacoff3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH17_F02_Kabacoff3.png)'
- en: Figure 17.2 Traditional (pruned) decision tree for predicting cancer status.
    Start at the top of the tree, moving left if a condition is true or right otherwise.
    When an observation hits a terminal node, it’s classified. Each node contains
    the probability of the classes in that node, along with the percentage of the
    sample.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2 用于预测癌症状态的常规（裁剪）决策树。从树的顶部开始，如果条件为真则向左移动，否则向右移动。当一个观测值遇到终端节点时，对其进行分类。每个节点包含该节点中类的概率以及样本的百分比。
- en: Finally, the `predict()` function is used to classify each observation in the
    test sample ❸. A cross-tabulation of the actual status against the predicted status
    is provided. The overall accuracy was 96% in the test sample. Note that decision
    trees can be biased toward selecting predictors that have many levels or many
    missing values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 `predict()` 函数对测试样本中的每个观测值进行分类 ❸。提供了实际状态与预测状态之间的交叉表。测试样本的整体准确率为 96%。请注意，决策树可能会偏向于选择具有许多级别或许多缺失值的预测变量。
- en: 17.3.2 Conditional inference trees
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.2 条件推断树
- en: Before moving on to random forests, let’s look at an important variant of the
    traditional decision tree called a *conditional inference tree*. Conditional inference
    trees are similar to traditional trees, but variables and splits are selected
    based on significance tests rather than purity/homogeneity measures. The significance
    tests are permutation tests (discussed in chapter 12).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论随机森林之前，让我们看看传统决策树的一个重要变体，称为条件推断树。条件推断树与传统树类似，但变量和分割是根据显著性测试而不是纯度/同质性度量来选择的。显著性测试是置换测试（在第
    12 章中讨论）。
- en: 'In this case, the algorithm is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，算法如下：
- en: Calculate p-values for the relationship between each predictor and the outcome
    variable.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个预测变量与结果变量之间关系的p值。
- en: Select the predictor with the lowest p-value.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择p值最低的预测变量。
- en: Explore all possible binary splits on the chosen predictor and dependent variable
    (using permutation tests) and pick the most significant split.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索所选预测变量和因变量上所有可能的二元分割（使用置换检验）并选择最显著的分割。
- en: Separate the data into these two groups and continue the process for each subgroup.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为这两组，并对每个子组继续进行过程。
- en: Continue until splits are no longer significant or the minimum node size is
    reached.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续直到分割不再显著或达到最小节点大小。
- en: Conditional inference trees are provided by the `ctree()` function in the `party`
    package. In the next listing, a conditional inference tree is grown for the breast
    cancer data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`party`包中的`ctree()`函数提供了条件推断树。在下一段代码中，为乳腺癌数据生成了一个条件推断树。'
- en: Listing 17.4 Creating a conditional inference tree with `ctree()`
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.4 使用`ctree()`创建条件推断树
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that pruning isn’t required for conditional inference trees, and the process
    is somewhat more automated. Additionally, the `partykit` package has attractive
    plotting options. Figure 17.3 plots the conditional inference tree. The shaded
    area of each node represents the proportion of malignant cases in that node.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，条件推断树不需要剪枝，并且该过程相对自动化。此外，`partykit`包提供了吸引人的绘图选项。图17.3绘制了条件推断树。每个节点的阴影区域代表该节点中恶性案例的比例。
- en: '![](Images/CH17_F03_Kabacoff3.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH17_F03_Kabacoff3.png)'
- en: Figure 17.3 Conditional inference tree for the breast cancer data
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3 乳腺癌数据的条件推断树
- en: Displaying an rpart() tree with a ctree()-like graph
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似图17.3的图形显示rpart()树
- en: If you create a classical decision tree using `rpart()`, but you’d like to display
    the resulting tree using a plot like the one in figure 17.3, the `partykit` package
    can help. You can use the statement `plot(as.party(an.rpart.tree))` to create
    the desired graph. For example, try creating a graph like figure 17.3 using the
    `dtree.pruned` object created in listing 17.3, and compare the results to the
    plot presented in figure 17.2.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`rpart()`创建经典决策树，但希望使用类似图17.3的图形显示结果树，`partykit`包可以提供帮助。您可以使用语句`plot(as.party(an.rpart.tree))`创建所需的图形。例如，尝试使用列表17.3中创建的`dtree.pruned`对象创建类似图17.3的图形，并将结果与图17.2中展示的图形进行比较。
- en: The decision trees grown by the traditional and conditional methods can differ
    substantially. In the current example, the accuracy of each is similar (96% versus
    97%), but the trees are quite different. In the next section, a large number of
    decision trees are grown and combined to classify cases into groups.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法和条件方法生成的决策树可能存在显著差异。在当前示例中，它们的准确性相似（96%对97%），但树结构相当不同。在下文中，将生成并组合大量决策树以将案例分类到不同的组别。
- en: 17.4 Random forests
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4 随机森林
- en: A random forest is an ensemble learning approach to supervised learning. Multiple
    predictive models are developed, and the results are aggregated to improve classification
    rates. You can find a comprehensive introduction to random forests, written by
    Leo Breiman and Adele Cutler, at [http://mng.bz/7Nul](http://mng.bz/7Nul).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种监督学习的集成学习方法。开发了多个预测模型，并将结果汇总以提高分类率。您可以在Leo Breiman和Adele Cutler撰写的[http://mng.bz/7Nul](http://mng.bz/7Nul)找到关于随机森林的全面介绍。
- en: The algorithm for a random forest involves sampling cases and variables to create
    a large number of decision trees. Each case is classified by each decision tree.
    The most common classification for that case is then used as the outcome.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的算法涉及对案例和变量进行采样以创建大量决策树。每个案例由每个决策树进行分类。然后使用该案例最常见的分类作为结果。
- en: 'Assume that *N* is the number of cases in the training sample and *M* is the
    number of variables. Then the algorithm is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*N*是训练样本中的案例数量，*M*是变量数量。然后算法如下：
- en: Grow a large number of decision trees by sampling *N* cases with replacement
    from the training set.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从训练集中有放回地采样*N*个案例来生成大量决策树。
- en: Sample *m < M* variables at each node. These variables are considered candidates
    for splitting in that node. The value *m* is the same for each node.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个节点采样*m < M*个变量。这些变量被认为是该节点分割的候选变量。值*m*对每个节点相同。
- en: Grow each tree fully without pruning (the minimum node size is set to 1).
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不进行剪枝，完整地生成每棵树（最小节点大小设置为1）。
- en: Terminal nodes are assigned to a class based on the mode of cases in that node.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 终端节点根据该节点中案例的众数分配给一个类别。
- en: Classify new cases by sending them down all the trees and taking a vote—majority
    rules.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将新案例发送到所有树并采取投票的方式进行分类——多数决定。
- en: An out-of-bag (OOB) error estimate is obtained by classifying the cases that
    aren’t selected when building a tree, using that tree. This is an advantage when
    a test sample is unavailable. Random forests also provide a natural measure of
    variable importance, as you’ll see.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用在构建树时未选择的案例进行分类来获得袋外（OOB）错误估计。当没有测试样本时，这是一个优点。随机森林也提供变量重要性的自然度量，您将看到这一点。
- en: Random forests are grown using the `randomForest()` function in the `random-Forest`
    package. The default number of trees is 500, the default number of variables sampled
    at each node is `sqrt(M)`, and the minimum node size is 1.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是通过`randomForest`包中的`randomForest()`函数生成的。默认的树的数量是500，默认在每个节点上抽取的变量数量是`sqrt(M)`，最小节点大小是1。
- en: The following listing provides the code and results for predicting malignancy
    status in the breast cancer data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表提供了预测乳腺癌数据中恶性肿瘤状态的代码和结果。
- en: Listing 17.5 Random forest
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.5 随机森林
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Grows the forest
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成森林
- en: ❷ Determines variable importance
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确定变量重要性
- en: ❸ Classifies new cases
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 分类新案例
- en: First, the `randomForest()` function is used to grow 500 traditional decision
    trees by sampling 489 observations with replacement from the training sample and
    sampling 3 variables at each node of each tree ❶.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`randomForest()`函数通过从训练样本中重复抽取489个观测值以及在每个树的每个节点上抽取3个变量来生成500棵传统的决策树❶。
- en: 'Random forests can provide a natural measure of variable importance, requested
    with the `information=TRUE` option, and printed with the `importance()` function
    ❷. Both the `rattle` and `randomForest` packages have a function called `importance``()`.
    Since we’ve loaded both packages in this chapter, `randomForest:: importance()`
    is used in the code to assure that the correct function is called. The relative
    importance measure specified by the `type=2` option is the total decrease in node
    impurities (heterogeneity) from splitting on that variable, averaged over all
    trees. Node impurity is measured with the Gini coefficient. `sizeUniformity` is
    the most important variable and `mitosis` is the least important.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以通过`information=TRUE`选项提供变量重要性的自然度量，并通过`importance()`函数打印出来❷。`rattle`和`randomForest`包都有一个名为`importance()`的函数。由于我们在本章中加载了这两个包，因此在代码中使用`randomForest::importance()`以确保调用正确的函数。由`type=2`选项指定的相对重要性度量是从该变量分割中平均所有树的节点纯度（异质性）的总减少量。节点纯度用基尼系数来衡量。`sizeUniformity`是最重要的变量，而`mitosis`是最不重要的。
- en: Finally, the test sample is classified using the random forest, and the predictive
    accuracy is calculated ❸. Note that cases with missing values in the test sample
    aren’t classified. The prediction accuracy (98% overall) is excellent.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用随机森林对测试样本进行分类，并计算预测准确性❸。请注意，测试样本中缺失值的案例不会被分类。预测准确性（总体为98%）非常出色。
- en: Whereas the `randomForest` package provides forests based on traditional decision
    trees, the `cforest()` function in the `party` package can be used to generate
    random forests based on conditional inference trees. If predictor variables are
    highly correlated, a random forest using conditional inference trees may provide
    better predictions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`randomForest`包提供基于传统决策树的森林，但`party`包中的`cforest()`函数可以用来生成基于条件推断树的随机森林。如果预测变量高度相关，使用条件推断树的随机森林可能提供更好的预测。
- en: Random forests tend to be very accurate compared with other classification methods.
    Additionally, they can handle large problems (many observations and variables),
    large amounts of missing data in the training set, and cases in which the number
    of variables is much greater than the number of observations. The provision of
    OOB error rates and measures of variable importance are also significant advantages.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他分类方法相比，随机森林通常非常准确。此外，它们可以处理大型问题（许多观测值和变量）、训练集中大量缺失数据，以及变量数量远大于观测值数量的情况。提供OOB错误率和变量重要性度量也是显著的优点。
- en: A significant disadvantage is that it’s difficult to understand the classification
    rules (there are 500 trees!) and communicate them to others. Additionally, you
    need to store the entire forest to classify new cases.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显著的缺点是理解分类规则（有500棵树！）并将其传达给他人很困难。此外，你需要存储整个森林来对新案例进行分类。
- en: A random forests is a *black box* model. Predictor values go in and accurate
    predictions come out, but it is difficult to understand what’s happening in the
    box (model). We’ll address this issue in section 17.7.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一个*黑盒*模型。预测值进入，准确的预测结果出来，但很难理解盒子里（模型）发生了什么。我们将在第17.7节中解决这个问题。
- en: The final classification model we’ll consider is the support vector machine.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要考虑的最终分类模型是支持向量机。
- en: 17.5 Support vector machines
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.5 支持向量机
- en: '*Support* *vector machines (SVMs)* are a group of supervised machine learning
    models that can be used for classification and regression. They’re popular at
    present, in part because of their success in developing accurate prediction models
    and in part because of the elegant mathematics that underlie the approach. We’ll
    focus on using SVMs for binary classification.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*支持向量机（SVMs）*是一组监督机器学习模型，可用于分类和回归。它们目前很受欢迎，部分原因是它们在开发准确预测模型方面的成功，部分原因是支撑该方法的优雅数学。我们将专注于使用SVMs进行二元分类。'
- en: SVMs seek an optimal hyperplane for separating two classes in a multidimensional
    space. The hyperplane is chosen to maximize the *margin* between the two classes’
    closest points. The points on the boundary of the margin are called *support vectors*
    (they help define the margin), and the middle of the margin is the separating
    hyperplane.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVMs）寻求在多维空间中分离两个类别的最优超平面。超平面被选择以最大化两个类别最近点之间的*间隔*。间隔边界上的点被称为*支持向量*（它们有助于定义间隔），间隔的中间是分离超平面。
- en: For an *N*-dimensional space (that is, with *N* predictor variables), the optimal
    hyperplane (also called a *linear decision surface*) has *N* – 1 dimensions. If
    there are two variables, the surface is a line. For three variables, the surface
    is a plane. For 10 variables, the surface is a 9-dimensional hyperplane. Trying
    to picture it will give you headache.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个*N*-维空间（即，有*N*个预测变量），最优超平面（也称为*线性决策表面*）有*N* - 1个维度。如果有两个变量，表面是一条线。对于三个变量，表面是一个平面。对于10个变量，表面是一个9维超平面。试图想象它会让你头疼。
- en: Consider the two-dimensional example shown in figure 17.4\. Circles and triangles
    represent the two groups. The *margin* is the gap, represented by the distance
    between the two dashed lines. The points on the dashed lines (filled circles and
    triangles) are the support vectors. In the two-dimensional case, the optimal hyperplane
    is the black line in the middle of the gap. In this idealized example, the two
    groups are linearly separable—the line can completely separate the two groups
    without errors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图17.4中所示的两个维度的例子。圆圈和三角形代表两组。*间隔*是间隙，由两条虚线之间的距离表示。虚线上的点（实心圆圈和三角形）是支持向量。在二维情况下，最优超平面是间隙中间的黑线。在这个理想化的例子中，两组是线性可分的——直线可以完全分离两组而不出错。
- en: '![](Images/CH17_F04_Kabacoff3.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH17_F04_Kabacoff3.png)'
- en: Figure 17.4 Two-group classification problem where the two groups are linearly
    separable. The separating hyperplane is indicated by the solid black line. The
    margin is the distance from the line to the dashed line on either side. The filled
    circles and triangles are the support vectors.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4展示了两组线性可分的两组分类问题。分离超平面由实心黑线指示。间隔是线到两侧虚线的距离。实心圆圈和三角形是支持向量。
- en: The optimal hyperplane is identified using quadratic programming to optimize
    the margin under the constraint that the data points on one side have an outcome
    value of +1 and the data on the other side has an outcome value of –1\. If the
    data points are *almost* separable (not all the points are on one side or the
    other), a penalizing term is added to the optimization to account for errors,
    and *soft* margins are produced.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二次规划来识别最优超平面，以在约束条件下优化间隔，即一侧的数据点具有+1的输出值，另一侧的数据具有-1的输出值。如果数据点几乎可分（不是所有点都在一侧或另一侧），则优化中添加一个惩罚项来考虑错误，并产生*软间隔*。
- en: But the data may be fundamentally nonlinear. In the example in figure 17.5,
    no line can correctly separate the circles and triangles. SVMs use kernel functions
    to transform the data into higher dimensions in the hope that they will become
    more linearly separable. Imagine transforming the data in figure 17.5 in such
    a way that the circles lift off the page. One way to do this is to transform the
    two-dimensional data into three dimensions using
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但是数据可能是根本非线性的。在图17.5的例子中，没有任何一条线能够正确地分离圆和三角形。支持向量机（SVMs）使用核函数将数据转换到更高维度的空间，希望它们将变得更加线性可分。想象一下将图17.5中的数据以这种方式转换，使得圆从页面上抬起。一种方法是将二维数据转换到三维空间，使用
- en: '![](Images/CH17_F04_Kabacoff3-EQ01.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH17_F04_Kabacoff3-EQ01.png)'
- en: '![](Images/CH17_F05_Kabacoff3.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH17_F05_Kabacoff3.png)'
- en: Figure 17.5 Two-group classification problem where the two groups aren’t linearly
    separable. The groups can’t be separated with a hyperplane (line).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5展示了两个组非线性可分的二组分类问题。这些组不能通过超平面（线）来分离。
- en: Then you can separate the triangles from the circles using a rigid sheet of
    paper (that is, a two-dimensional plane in what is now a three-dimensional space).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用一张刚性纸片（即在现在是三维空间中的二维平面）将三角形和圆分开。
- en: The mathematics of SVMs is complex and well beyond the scope of this book. Statnikov,
    Aliferis, Hardin, and Guyon (2011) offer a lucid and intuitive presentation of
    SVMs that goes into quite a bit of conceptual detail without getting bogged down
    in higher math.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机的数学非常复杂，超出了本书的范围。Statnikov, Aliferis, Hardin, 和 Guyon (2011) 提供了一个清晰且直观的SVM介绍，它深入到了相当多的概念细节，而没有陷入高等数学的泥潭。
- en: SVMs are available in R using the `ksvm()` function in the `kernlab` package
    and the `svm()` function in the `e1071` package. The former is more powerful,
    but the latter is a bit easier to use. The example in the next listing uses the
    latter (easy is good) to develop an SVM for the Wisconsin breast cancer data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，可以使用`kernlab`包中的`ksvm()`函数和`e1071`包中的`svm()`函数来使用SVMs。前者更强大，但后者使用起来更简单。下一个列表中的例子使用了后者（简单是好的）来为威斯康星乳腺癌数据开发SVM。
- en: Listing 17.6 A support vector machine
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.6 支持向量机
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because predictor variables with larger variances typically have a greater influence
    on the development of SVMs, the `svm()` function scales each variable to a mean
    of 0 and standard deviation of 1 before fitting the model by default. As you can
    see, the predictive accuracy (99%) is very good.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于具有较大方差的预测变量通常对SVMs的发展有更大的影响，`svm()`函数默认情况下在拟合模型之前将每个变量缩放到均值为0和标准差为1。正如你所见，预测精度（99%）非常好。
- en: 17.5.1 Tuning an SVM
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.5.1 调整SVM
- en: By default, the `svm()` function uses a *radial basis function* (RBF) to map
    samples into a higher-dimensional space. The RBF kernel is often a good choice
    because it’s a nonlinear mapping that can handle relations between class labels
    and predictors that are nonlinear.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`svm()`函数使用*径向基函数*（RBF）将样本映射到更高维度的空间。RBF核通常是一个很好的选择，因为它是一个非线性映射，可以处理类标签和预测变量之间的非线性关系。
- en: 'When fitting an SVM with the RBF kernel, two parameters can affect the results:
    *gamma* and *cost*. Gamma is a kernel parameter that controls the shape of the
    separating hyperplane. Larger values of gamma typically result in a larger number
    of support vectors. Gamma can also be thought of as a parameter that controls
    how widely a training sample *reaches*, with larger values meaning far and smaller
    values meaning close. Gamma must be greater than zero.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用RBF核拟合SVM时，有两个参数可以影响结果：*gamma*和*cost*。Gamma是一个核参数，它控制着分离超平面的形状。较大的gamma值通常会导致更多的支持向量。Gamma也可以被视为一个参数，它控制着训练样本*达到*的范围，较大的值意味着远，较小的值意味着近。Gamma必须大于零。
- en: The cost parameter represents the cost of making errors. A large value severely
    penalizes errors and leads to a more complex classification boundary. There will
    be fewer misclassifications in the training sample, but overfitting may result
    in poor predictive ability in new samples. Smaller values lead to a flatter classification
    boundary but may result in underfitting. Like gamma, cost is always positive.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 成本参数表示犯错误的代价。较大的值会严重惩罚错误，导致更复杂的分类边界。训练样本中的误分类将更少，但过拟合可能导致在新样本中的预测能力较差。较小的值会导致更平的分类边界，但可能导致欠拟合。与gamma一样，成本总是正的。
- en: By default, the `svm()` function sets gamma to 1 / (number of predictors) and
    cost to 1\. But a different combination of gamma and cost may lead to a more effective
    model. You can try fitting SVMs by varying parameter values one at a time, but
    a grid search is more efficient. You can specify a range of values for each parameter
    using the `tune.svm()` function. `tune.svm()` fits every combination of values
    and reports on the performance of each. An example is given in the next listing.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`svm()`函数将gamma设置为1 /（预测因子数量）并将cost设置为1。但不同的gamma和cost组合可能会导致更有效的模型。您可以尝试通过逐个调整参数值来拟合SVM，但网格搜索更有效。您可以使用`tune.svm()`函数为每个参数指定一个值范围。`tune.svm()`拟合每个值的组合并报告每个的性能。下一个列表提供了一个示例。
- en: Listing 17.7 Tuning an RBF support vector machine
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.7 调整RBF支持向量机
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Varies the parameters
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 改变参数
- en: ❷ Prints the best model
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印最佳模型
- en: ❸ Fits the model with these parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用这些参数拟合模型
- en: ❹ Evaluates the cross-validation performance
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 评估交叉验证性能
- en: First, an SVM model is fit with an RBF kernel and varying values of gamma and
    cost ❶. Eight values of gamma (ranging from 0.000001 to 10) and 21 values of cost
    (ranging from 0.0000000001 to 100000000) are specified. In all, 168 models (8
    × 21) are fit and compared. The model with the fewest 10-fold cross-validated
    errors in the training sample has gamma = 0.01 and cost = 1.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用RBF核和变化的gamma和cost值 ❶ 拟合一个SVM模型。指定了8个gamma值（从0.000001到10）和21个cost值（从0.0000000001到100000000）。总共拟合并比较了168个模型（8
    × 21）。在训练样本中具有最少10折交叉验证错误的模型具有gamma = 0.01和cost = 1。
- en: Using these parameter values, a new SVM is fit to the training sample ❸. The
    model is then used to predict outcomes in the test sample ❹, and the number of
    errors is displayed. Tuning the model ❷ had virtually no effect on the number
    of errors. This is not surprising for this example. The default parameter values
    (cost = 1, gamma = 0.111) are very similar to the tuned values (cost = 1, gamma
    = 0.01). In many cases, tuning the SVM parameters will lead to greater gains.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数值，一个新的支持向量机（SVM）被拟合到训练样本 ❸。然后使用该模型在测试样本中预测结果 ❹，并显示错误数量。调整模型 ❷ 对错误数量的影响几乎可以忽略不计。对于这个例子来说，这并不令人惊讶。默认参数值（cost
    = 1，gamma = 0.111）与调整后的值（cost = 1，gamma = 0.01）非常相似。在许多情况下，调整SVM参数将导致更大的收益。
- en: As stated, SVMs are popular because they work well in many situations. They
    can also handle situations in which the number of variables is much larger than
    the number of observations. This has made them popular in the field of biomedicine,
    where the number of variables collected in a typical DNA microarray study of gene
    expressions may be one or two orders of magnitude larger than the number of cases
    available.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如所述，SVM之所以受欢迎，是因为它们在许多情况下都表现良好。它们还可以处理变量数量远大于观测数量的情况。这使得它们在生物医学领域非常受欢迎，在典型的DNA微阵列研究中收集的变量数量可能比可用案例数量大一个或两个数量级。
- en: One drawback of SVMs is that, like random forests, the resulting classification
    rules are difficult to understand and communicate. Again, they’re essentially
    a black box. Additionally, SVMs don’t scale as well as random forests when building
    models from large training samples. But once a successful model is built, classifying
    new observations scales quite well.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机的一个缺点是，与随机森林一样，产生的分类规则难以理解和传达。同样，它们基本上是一个黑盒。此外，当从大型训练样本构建模型时，SVM的扩展性不如随机森林。但一旦构建了一个成功的模型，对新观测的分类扩展性相当好。
- en: 17.6 Choosing a best predictive solution
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.6 选择最佳预测解
- en: 'In sections 17.4 through 17.5, fine-needle aspiration samples were classified
    as malignant or benign using several supervised machine learning techniques. Which
    approach was most accurate? To answer this question, we need to define the term
    *accurate* in a binary classification context:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在第17.4节至第17.5节中，使用几种监督机器学习技术将细针穿刺样本分类为恶性或良性。哪种方法最准确？为了回答这个问题，我们需要在二元分类的上下文中定义术语
    *准确*：
- en: The most commonly reported statistic is the *accuracy*, or how often the classifier
    is correct. Although informative, accuracy is insufficient by itself. Additional
    information is needed to evaluate the utility of a classification scheme.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常报告的统计量是 *准确率*，即分类器正确率的频率。尽管信息量很大，但准确率本身是不够的。还需要额外的信息来评估分类方案的有效性。
- en: 'Consider a set of rules for classifying individuals as schizophrenic or non-schizophrenic.
    Schizophrenia is a rare disorder, with a prevalence of roughly 1% in the general
    population. If you classify everyone as non-schizophrenic, you’ll be right 99%
    of time. But this isn’t a good classifier because it will also misclassify every
    schizophrenic as non-schizophrenic. In addition to the accuracy, you should ask
    these questions:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑一套将个人分类为精神分裂症或非精神分裂症的规则。精神分裂症是一种罕见的疾病，在普通人群中患病率约为1%。如果你将所有人分类为非精神分裂症，你将有99%的时间是正确的。但这不是一个好的分类器，因为它还会将每个精神分裂症患者错误地分类为非精神分裂症患者。除了准确性之外，你还应该问这些问题：
- en: What percentage of schizophrenics are correctly identified?
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确识别的精神分裂症患者占多少百分比？
- en: What percentage of nonschizophrenics are correctly identified?
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确识别的非精神分裂症患者占多少百分比？
- en: If a person is classified as schizophrenic, how likely is it that this classification
    will be correct?
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个人被分类为精神分裂症，这种分类正确的可能性有多大？
- en: If a person is classified as nonschizophrenic, how likely is it that this classification
    is correct?
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个人被分类为非精神分裂症，这种分类正确的可能性有多大？
- en: These questions pertain to a classifier’s sensitivity, specificity, positive
    predictive power, and negative predictive power. Each is defined in table 17.1.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题涉及分类器的灵敏度、特异性、阳性预测力和阴性预测力。每个都在表17.1中定义。
- en: Table 17.1 Measures of predictive accuracy
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表17.1 预测准确度指标
- en: '| Statistic | Interpretation |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 统计量 | 解释 |'
- en: '| Sensitivity | Probability of getting a positive classification when the true
    outcome is positive (also called true positive rate or recall) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 灵敏度 | 当真实结果为阳性时获得阳性分类的概率（也称为真正率或召回率）|'
- en: '| Specificity | Probability of getting a negative classification when the true
    outcome is negative (also called true negative rate) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 特异性 | 当真实结果为阴性时获得阴性分类的概率（也称为真正率）|'
- en: '| Positive predictive value | Probability that an observation with a positive
    classification is correctly identified as positive (also called precision) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 阳性预测值 | 具有阳性分类的观察结果被正确识别为阳性的概率（也称为精确度）|'
- en: '| Negative predictive value | Probability that an observation with a negative
    classification is correctly identified as negative |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 阴性预测值 | 具有阴性分类的观察结果被正确识别为阴性的概率|'
- en: '| Accuracy | Proportion of observations correctly identified (also called ACC)
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 正确识别的观察结果比例（也称为ACC）|'
- en: The following listing provides a function for calculating these statistics.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表提供了一个用于计算这些统计数据的函数。
- en: Listing 17.8 Function for assessing binary classification accuracy
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.8 评估二元分类准确度的函数
- en: '[PRE9]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Extracts frequencies
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提取频率
- en: ❷ Calculates statistics
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算统计数据
- en: ❸ Prints results
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印结果
- en: The `performance()` function takes a table containing the true outcome (rows)
    and predicted outcome (columns) and returns the five accuracy measures. First,
    the number of *true negatives* (benign tissue identified as benign), *false positives*
    (benign tissue identified as malignant), *false negatives* (malignant tissue identified
    as benign), and *true positives* (malignant tissue identified as malignant) are
    extracted ❶. Next, these counts are used to calculate the sensitivity, specificity,
    positive and negative predictive values, and accuracy ❷. Finally, the results
    are formatted and printed ❸.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`performance()` 函数接受一个包含真实结果（行）和预测结果（列）的表，并返回五个准确度指标。首先，提取了*真正负值*（良性组织被识别为良性）、*假阳性*（良性组织被识别为恶性）、*假阴性*（恶性组织被识别为良性）和*真正阳性*（恶性组织被识别为恶性）的数量
    ❶。接下来，使用这些计数来计算灵敏度、特异性、阳性预测值和阴性预测值以及准确度 ❷。最后，格式化并打印结果 ❸。'
- en: In the following listing, the `performance()` function is applied to each of
    the five classifiers developed in this chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，`performance()` 函数被应用于本章开发的五个分类器中的每一个。
- en: Listing 17.9 Performance of breast cancer data classifiers
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.9 乳腺癌数据分类器的性能
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Each of these classifiers (logistic regression, traditional decision tree, conditional
    inference tree, random forest, and support vector machine) performed exceedingly
    well on each of the accuracy measures. This won’t always be the case!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分类器（逻辑回归、传统决策树、条件推断树、随机森林和支持向量机）在每个准确度指标上都表现出色。这并不总是如此！
- en: In this particular instance, the award appears to go to the support vector machine
    model (although the differences are so small, they may be due to chance). For
    the SVM model, 98% of malignancies were correctly identified (sensitivity), 98%
    of benign samples were correctly identified (specificity), and the overall percent
    of correct classifications is 98% (accuracy). A diagnosis of malignancy was correct
    95% of the time (positive predictive value), and a benign diagnosis was correct
    99% of the time (negative predictive value). For diagnoses of cancer, the specificity
    (proportion of malignant samples correctly identified as malignant) is particularly
    important.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定实例中，奖项似乎授予了支持向量机模型（尽管差异如此之小，它们可能是由偶然造成的）。对于SVM模型，98%的恶性肿瘤被正确识别（灵敏度），98%的良性样本被正确识别（特异性），总体正确分类的百分比是98%（准确率）。恶性肿瘤的诊断在95%的时间内是正确的（阳性预测值），良性诊断在99%的时间内是正确的（阴性预测值）。对于癌症的诊断，特异性（正确识别为恶性肿瘤的恶性肿瘤样本比例）尤为重要。
- en: Although it’s beyond the scope of this chapter, you can often improve a classification
    system by trading specificity for sensitivity and vice versa. In the logistic
    regression model, `predict()` was used to estimate the probability that a case
    belonged in the malignant group. If the probability was greater than 0.5, the
    case was assigned to that group. The 0.5 value is called the *threshold* or *cutoff
    value*. If you vary this threshold, you can increase the sensitivity of the classification
    model at the expense of its specificity. `predict()` can also generate probabilities
    for decision trees, random forests, and SVMs (although the syntax varies by method).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这超出了本章的范围，但通常可以通过以特异性换取灵敏度或反之来提高分类系统。在逻辑回归模型中，`predict()`用于估计一个案例属于恶性肿瘤组的概率。如果概率大于0.5，该案例将被分配到该组。这个0.5值被称为**阈值**或**截止值**。如果你改变这个阈值，你可以在牺牲特异性的情况下提高分类模型的灵敏度。`predict()`还可以为决策树、随机森林和SVM生成概率（尽管语法因方法而异）。
- en: The impact of varying the threshold value is typically assessed using a *receiver
    operating characteristic* (*ROC*) curve. An ROC curve plots sensitivity versus
    specificity for a range of threshold values. You can then select a threshold with
    the best balance of sensitivity and specificity for a given problem. Many R packages
    generate ROC curves, including `ROCR` and `pROC`. Analytic functions in these
    packages can help you select the best threshold values for a given scenario or
    compare the ROC curves produced by different classification algorithms to choose
    the most useful approach. To learn more, see Kuhn and Johnson (2013). A more advanced
    discussion is offered by Fawcett (2005).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用**接收者操作特征**（ROC）曲线来评估改变阈值值的影响。ROC曲线绘制了灵敏度与特异性的对比图，针对一系列的阈值值。然后，你可以选择一个在给定问题中灵敏度与特异性最佳平衡的阈值。许多R包可以生成ROC曲线，包括`ROCR`和`pROC`。这些包中的分析函数可以帮助你为特定场景选择最佳阈值值，或者比较不同分类算法生成的ROC曲线，以选择最有效的方法。欲了解更多信息，请参阅Kuhn和Johnson（2013）。Fawcett（2005）提供了更深入的讨论。
- en: 17.7 Understanding black box predictions
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.7 理解黑盒预测
- en: Classification models are used to make real work decisions that can have significant
    human consequences. Imagine that you’ve applied for and been turned down for a
    bank loan and you want to understand why. If the decision was based on a logistic
    regression or classification tree model, the reason can be determined by looking
    at the model coefficient in the former case or the decision tree in the latter
    case. But what if the classification was based on a random forest, support vector
    machine model, or artificial neural network model? Until recently, the answer
    was “because the computer said so,” which is a very unsatisfactory answer!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型用于做出具有重大人类后果的实际工作决策。想象一下，你申请了银行贷款并被拒绝，你想要了解原因。如果决策是基于逻辑回归或分类树模型，原因可以通过查看前者的模型系数或后者的决策树来确定。但如果分类是基于随机森林、支持向量机模型或人工神经网络模型呢？直到最近，答案还是“因为计算机说了这样”，这是一个非常令人不满意的答案！
- en: In recent years, there has been a movement to understand black box models employing
    methods and techniques called *Explainable Artificial Intelligence* (XAI, [http://ema.drwhy.ai](http://ema.drwhy.ai)).
    The goal of XAI is to better understand how black box models work in general (global
    understanding) and when making individual predictions (local understanding).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，有一种趋势是使用称为*可解释人工智能*（XAI，[http://ema.drwhy.ai](http://ema.drwhy.ai)）的方法和技术来理解黑盒模型。XAI的目标是更好地理解黑盒模型在一般情况下（全局理解）以及在进行个别预测时（局部理解）是如何工作的。
- en: 'Consider a patient named Alex. Alex’s biopsy produced the following lab values:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个名叫Alex的患者。Alex的活检产生了以下实验室值：
- en: '[PRE11]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Running these values through the random forest model developed in section 17.4
    yields a prediction of malignancy (with a probability of 0.658). In the remainder
    of this section, we’ll use XAI techniques to explore how our random forest model
    produced this diagnosis. We’ll be using the `DALEX` package, so be sure to install
    it (`install.packages("DALEX"`) before continuing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些值通过第17.4节中开发的随机森林模型运行，得到恶性预测（概率为0.658）。在本节的剩余部分，我们将使用XAI技术来探索我们的随机森林模型是如何产生这种诊断的。我们将使用`DALEX`包，所以在继续之前请确保安装它（`install.packages("DALEX"`)）。
- en: 17.7.1 Break-down plots
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.7.1 分解图
- en: 'Our goal is to partition Alex’s predictor scores into their unique contributions
    to the final classification (malignant with probability 0.658). To accomplish
    this, we’ll use *break-down* values:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将Alex的预测分数划分为对最终分类（恶性概率为0.658）的独特贡献。为了实现这一点，我们将使用*分解值*：
- en: Using the training sample, calculate the average predicted response value (probability
    of malignancy in this case) across all observations (0.365). Call this the intercept.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练样本，计算所有观察到的平均预测响应值（在本例中为恶性概率）（0.365）。将其称为截距。
- en: For all observations where `bareNuclei` `=` `9`, calculate the average predicted
    res-ponse (0.544). The contribution of `bareNuclei` in this case is 0.544 – 0365
    = +0.179.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有`bareNuclei`等于`9`的观察值，计算平均预测响应（0.544）。在这种情况下，`bareNuclei`的贡献是0.544 - 0.365
    = +0.179。
- en: For all observations where `bareNuclei` `=` `9` *and* `sizeUniformity` = 1,
    calculate the average predicted response (0.476). The contribution of `sizeUniformity`
    0.476 – 0.544 = –0.068.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有`bareNuclei`等于`9`且`sizeUniformity`等于`1`的观察值，计算平均预测响应（0.476）。`sizeUniformity`的贡献是0.476
    - 0.544 = -0.068。
- en: For all observations where `bareNuclei` `=` `9`, *and* `sizeUniformity` `=`
    `1`, *and* `shapeUniformity` `=` `1`, calculate the average predicted response
    (0.42). The contribution of `shapeUniformity` is –0.05.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有`bareNuclei`等于`9`、`sizeUniformity`等于`1`、`shapeUniformity`等于`1`的观察值，计算平均预测响应（0.42）。`shapeUniformity`的贡献是-0.05。
- en: Keep going until you have included all predictor values for the observation.
    The individual predictor contributions will add up the model’s prediction for
    that case. Positive contributions increase the likelihood of a malignancy diagnosis.
    Negative contribution decreases this likelihood. The magnitude of the contribution
    assesses the variable’s impact on the final prediction.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续进行，直到你包括了观察值的所有预测值。个别预测值的贡献将汇总为该案例的模型预测。正贡献增加恶性诊断的可能性。负贡献降低这种可能性。贡献的大小评估了变量对最终预测的影响。
- en: The following listing provides the code for calculating break-down values, and
    figure 17.6 displays a break-down plot.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表提供了计算分解值的代码，图17.6显示了分解图。
- en: Listing 17.10 Using a break-down plot to understand a black box prediction
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.10 使用分解图理解黑盒预测
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Observation to explore
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 探索的观察值
- en: ❷ Predicts outcome for that observation
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测该观察值的结局
- en: ❸ Builds an explainer object
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建解释器对象
- en: ❹ Produces the break-down plot
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成分解图
- en: After loading the `DALEX` package, the case of interest is input as a data frame
    ❶. The `predict()` function runs the case through the random forest and produces
    the prediction. Since `type="prob"`, the probabilities for a benign and malignant
    outcome are returned ❷. Next, a `DALEX explainer` object is created ❸. The object
    takes the random forest model, the training data, the outcome variable, and the
    function used to predict the outcome as parameters. Here we specify that we want
    to predict the malignant category. Using the `[,2]` in the prediction function
    returns just the probability of malignancy. Finally, we generate the break-down
    plot using the `explainer` object and observation to be explained ❹.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载`DALEX`包后，感兴趣的案例以数据框的形式输入❶。`predict()`函数将案例通过随机森林运行并产生预测。由于`type="prob"`，返回良性结果和恶性结果的概率❷。接下来，创建一个`DALEX
    explainer`对象❸。该对象以随机森林模型、训练数据、结果变量以及用于预测结果的功能作为参数。在这里，我们指定我们想要预测恶性类别。使用预测函数中的`[,2]`返回仅恶性概率。最后，我们使用`explainer`对象和要解释的观测值生成分解图❹。
- en: '![](Images/CH17_F06_Kabacoff3.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH17_F06_Kabacoff3.png)'
- en: Figure 17.6 A break-down plot for one individual’s random forest prediction.
    The average predicted probability of a malignant biopsy is 0.365\. Given this
    individual’s scores, the predicted probability is 0.722\. Since this probability
    is greater than 0.5, the prediction is that the biopsy represents malignancy.
    The individual contribution of each predictor value for this individual can also
    be seen. Green bars represent positive contributions to the predicted outcome,
    while red bars represent negative contributions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6展示了某个人随机森林预测的分解图。恶性活检的平均预测概率为0.365。根据这个个体的得分，预测概率为0.722。由于这个概率大于0.5，预测结果是活检代表恶性。也可以看到每个预测值对这一个体的个体贡献。绿色条表示对预测结果的正面贡献，而红色条表示对预测结果的负面贡献。
- en: Looking at figure 17.6, you can see that for Alex, `bareNuclei` `=` `9` and
    `clumpThickness` `=` `6` had major contributions to the malignant diagnosis. The
    `sizeUniformity` `=` `1` and `shapeUniformity` `=` `1` scores reduced the probability
    of the cells being malignant. Taking all nine predictor values into account led
    to a malignant probability of 0.658\. Since 0.658 > 0.50, the random forest model
    classified Alex’s sample as malignant.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图17.6，可以看到对于Alex来说，`bareNuclei` `=` `9` 和 `clumpThickness` `=` `6` 对恶性诊断有重大贡献。`sizeUniformity`
    `=` `1` 和 `shapeUniformity` `=` `1` 的得分降低了细胞为恶性的概率。考虑所有九个预测值，导致恶性概率为0.658。由于0.658
    > 0.50，随机森林模型将Alex的样本分类为恶性。
- en: 17.7.2 Plotting Shapley values
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.7.2 绘制Shapley值
- en: Break-down plots can be very helpful in understanding why an individual received
    a given prediction from a black box model. However, note that the break-down is
    order dependent. If there are interaction effects between two or more predictor
    variables, you will get different break-down results for different variable orders.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 分解图对于理解个体从黑盒模型获得特定预测的原因非常有帮助。然而，请注意分解是顺序相关的。如果两个或多个预测变量之间存在交互效应，不同变量顺序将得到不同的分解结果。
- en: '*Shapley additive explanations* or *SHAP* values get around this reliance on
    order by calculating break-down contributions for many predictor variable orders
    and averaging the results for each variable. Continuing the example in listing
    17.10,'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shapley加性解释*或*SHAP*值通过计算许多预测变量排序的分解贡献并平均每个变量的结果来绕过对顺序的依赖。继续列表17.10中的示例，'
- en: '[PRE13]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: produces the plot in figure 17.7.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 产生图17.7的图。
- en: '![](Images/CH17_F07_Kabacoff3.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH17_F07_Kabacoff3.png)'
- en: Figure 17.7 A SHAP values plot for one individual’s random forest prediction
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7展示了某个人随机森林预测的SHAP值图
- en: The box plots show the range of break-down contributions for a variable over
    different variable orderings. The bars represent the mean break-down value. Larger
    bars represent a greater impact on the prediction for this individual. Positive
    (green) bars indicate contributions to a malignant diagnosis, while negative (red)
    bars indicate contributions to a benign diagnosis.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图显示了不同变量排序下变量的分解贡献范围。条形表示平均分解值。较长的条形表示对个体预测的影响更大。正（绿色）条形表示对恶性诊断的贡献，而负（红色）条形表示对良性诊断的贡献。
- en: The XAI methods discussed are model agnostic—they can be used with any machine
    learning approach, including those covered in this chapter. The `DALEX` package
    offers many other such statistics and is worth checking out. Besides, *WHO DOESN’T
    LIKE DALEX?* (pun intended!)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论中的XAI方法是无模型特定的—they可以与任何机器学习方法一起使用，包括本章中涵盖的方法。《DALEX》包提供了许多其他此类统计信息，值得一看。此外，*谁不喜欢DALEX？（双关语！）*
- en: 17.8 Going further
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.8 深入学习
- en: Building a predictive model using machine learning techniques is a complex,
    iterative process. Common steps include
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习技术构建预测模型是一个复杂、迭代的过程。常见的步骤包括
- en: '*Data splitting*—Available data is segregated into training and testing datasets.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据拆分*—可用数据被分割成训练集和测试集。'
- en: '*Data preprocessing*—Predictor variables are selected and possibly transformed.
    For example, support vector machines often work best with standardized predictors.
    Highly correlated variables may be combined into composite variables or dropped.
    Missing values must be imputed or deleted.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据预处理*—选择预测变量，并可能进行转换。例如，支持向量机通常与标准化预测变量工作得最好。高度相关的变量可能被组合成复合变量或被删除。缺失值必须被估计或删除。'
- en: '*Model building*—Many possible candidate prediction models are developed. Most
    will have hyperparameters that have to be tuned. Hyperparameters are model parameters
    that control the learning process and are selected by trial and error. The complexity
    parameter in classification trees, the number of candidate variables for a split
    in random forests, and the cost and gamma parameters in support vector machines
    are all examples. You vary these parameters and select the values that result
    in the most predictive and robust models.'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型构建*—开发了许多可能的候选预测模型。大多数模型都会有需要调整的超参数。超参数是控制学习过程的模型参数，通过试错法进行选择。分类树中的复杂性参数、随机森林中分割的候选变量数量，以及支持向量机中的成本和gamma参数都是例子。您会调整这些参数，并选择导致预测性和鲁棒性最强的模型。'
- en: '*Model comparisons*—After fitting a series of models, their performances are
    compared, and a final model is chosen.'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型比较*—在拟合一系列模型之后，它们的性能会被比较，并选择一个最终模型。'
- en: '*Model release*—Once the final model is chosen, it is used to make future predictions.
    Since the environment may change, it is important to continuously evaluate the
    model’s effectiveness over time.'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型发布*—一旦选择了最终模型，它就被用来进行未来的预测。由于环境可能会变化，因此重要的是要持续评估模型随时间的变化的有效性。'
- en: The task is further complicated by the fact that machine learning techniques
    exist in many different packages and use different syntaxes. To simplify the workflow,
    omnibus or meta packages have been developed. These packages serve as wrappers
    to other packages and provide a simplified and consistent interface for building
    predictive models. Learning one of these approaches can greatly simplify the task
    of building an effective predictive system.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习技术存在于许多不同的包中，并且使用不同的语法，任务进一步复杂化。为了简化工作流程，已经开发了综合或元包。这些包作为其他包的包装器，为构建预测模型提供了简化和一致的接口。学习这些方法之一可以极大地简化构建有效预测系统的工作。
- en: Three of the most popular approaches are the `caret` package, the `mlr3` framework,
    and the `tidymodels` framework (frameworks consist of several interrelated packages).
    The `caret` package ([http://topepo.github.io/caret](http://topepo.github.io/caret))
    is perhaps the most mature package and supports more than 230 machine learning
    algorithms. The `mlr3` framework ([https://mlr3.mlr-org.com/](https://mlr3.mlr-org.com/))
    is highly structured and will appeal to object-oriented programmers. The `tidymodels`
    framework ([https://www.tidymodels.org/](https://www.tidymodels.org/)) is the
    newest and perhaps the most flexible. Since it was created by the same individuals
    responsible for the `caret` package, I suspect that it will supersede the `caret`
    package over time. If you are going to use R for machine learning, I suggest that
    you select one of these frameworks and study it in depth. Which one you choose
    is a matter of personal preference.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的三种方法分别是`caret`包、`mlr3`框架和`tidymodels`框架（框架由几个相互关联的包组成）。`caret`包（[http://topepo.github.io/caret](http://topepo.github.io/caret)）可能是最成熟的包，支持超过230种机器学习算法。`mlr3`框架（[https://mlr3.mlr-org.com/](https://mlr3.mlr-org.com/)）结构高度严谨，对面向对象程序员有吸引力。`tidymodels`框架（[https://www.tidymodels.org/](https://www.tidymodels.org/)）是最新的，也许是最灵活的。由于它是由负责`caret`包的同一个人创建的，我怀疑它最终会取代`caret`包。如果你打算用R进行机器学习，我建议你选择这些框架之一，并深入研究。你选择哪个取决于个人喜好。
- en: To learn more about the functions in R that support prediction and classification,
    look in the CRAN Task View for Machine Learning and Statistical Learning ([http://mng.bz/I1Lm](http://mng.bz/I1Lm)).
    Other good resources include books by Kuhn and Johnson (2013), Forte (2015), Lanz
    (2015), and Torgo (2017).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于R中支持预测和分类的函数，请查看CRAN任务视图中的机器学习和统计学习（[http://mng.bz/I1Lm](http://mng.bz/I1Lm)）。其他好的资源包括Kuhn和Johnson（2013年）、Forte（2015年）、Lanz（2015年）和Torgo（2017年）的书籍。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A common task in data science is the prediction of a binary categorical outcome
    (pass/fail; succeed/fail; live/die; benign/malignant), and these predictions can
    have real-world consequences for individuals.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学中的一个常见任务是预测二元分类结果（通过/不通过；成功/失败；生存/死亡；良性/恶性），这些预测对个人可能产生现实世界的后果。
- en: Predictive models can range from relatively simple (logistic regression, classification
    trees) to exceeding complex (random forest, support vector machine, artificial
    neural network).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测模型可以从相对简单（逻辑回归、分类树）到极其复杂（随机森林、支持向量机、人工神经网络）不等。
- en: Models often have hyperparameters (parameters that control the learning process)
    that must be tuned by trial and error.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型通常具有超参数（控制学习过程的参数），这些参数必须通过试错来调整。
- en: The effectiveness of predictive models is evaluated by using performance measures
    such as accuracy, sensitivity, specificity, positive predictive power, and negative
    predictive power.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测模型的有效性是通过使用性能指标如准确率、灵敏度、特异性、阳性预测力和阴性预测力来评估的。
- en: New techniques for exploring highly complex black box models have been developed.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经开发出探索高度复杂的黑盒模型的新技术。
- en: The process of developing a predictive model involves numerous iterative steps,
    which can be simplified by using an omnibus package or framework such as `caret`,
    `mlr2`, or `tidymodels`.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发预测模型的过程涉及众多迭代步骤，这些步骤可以通过使用综合包或框架，如`caret`、`mlr2`或`tidymodels`来简化。
