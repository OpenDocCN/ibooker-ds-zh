- en: '12 Setting the stage: Preparing features for machine learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 设置场景：为机器学习准备特征
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How investing in a solid data manipulation foundation makes data preparation
    a breeze
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投资于坚实的数据操作基础如何使数据准备变得轻松
- en: Addressing big data quality problems with PySpark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark解决大数据质量问题
- en: Creating custom features for your ML model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的ML模型创建自定义特征
- en: Selecting compelling features for your model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的模型选择引人注目的特征
- en: Using transformers and estimators as part of the feature engineering process
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征工程过程中使用转换器和估计器
- en: 'I get excited doing machine learning, but not for the reasons most people do.
    I love getting into a new data set and trying to solve a problem. Each data set
    sports its own problems and idiosyncrasies, and getting it “ML ready” is extremely
    satisfying. Building a model gives purpose to data transformation; you ingest,
    clean, profile, and torture the data for a higher purpose: solving a real-life
    problem. This chapter focuses on the most important stage of machine learning
    regarding your use case: exploring, understanding, preparing, and giving purpose
    to your data. More specifically, we focus on preparing a data set by cleaning
    the data, creating new *features*, which are fields that will serve in training
    the model (chapter 13), and then looking at selecting a curated set of features
    based on how promising they look. At the end of the chapter, we will have a clean
    data set with well-understood features that will be ready for machine learning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我对机器学习感到兴奋，但原因与大多数人不同。我喜欢深入研究新的数据集，并尝试解决问题。每个数据集都有其独特的问题和特性，将其“ML ready”化是一种极大的满足感。构建模型赋予了数据转换目的；你摄入、清理、分析并折磨数据，都是为了一个更高的目的：解决现实生活中的问题。本章重点介绍机器学习中最重要的一步，即针对你的用例探索、理解、准备和赋予数据目的。更具体地说，我们专注于通过清理数据、创建新的*特征*（这些字段将在第13章中用于训练模型）以及根据它们看起来有多有希望来选择一组精选的特征。在本章结束时，我们将拥有一个干净的数据集，其中包含易于理解的特征，这将准备好用于机器学习。
- en: This is not a masterclass in machine learning
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一门机器学习的大师课
- en: This chapter and the next assume a little familiarity with machine learning.
    I explain the concepts as I go along, but I can’t cover the full modeling process,
    as it would be a book on its own. Furthermore, we can’t reasonably learn about
    honing our intuition about data and modeling in a single chapter. Consider this
    section one way to proceed to get a model, and I encourage you to experiment with
    this data set (and others!) to build your own intuition.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和下一章假设你对机器学习有一定的了解。我会边走边解释概念，但无法涵盖完整的建模过程，因为这将是一本单独的书。此外，我们无法在单章内合理地学习如何锤炼我们对数据和建模的直觉。请将本节视为获取模型的一种方法，并鼓励你尝试这个数据集（以及其他数据集！）来建立你自己的直觉。
- en: If you are interested in learning more about machine learning, I strongly recommend
    *Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor
    Hastie, and Robert Tibshirani (Springer, 2021, freely available online at [https://www.statlearning.com](https://www.statlearning.com)).
    It uses R, but the concepts transcend languages. For a more practical (and Python-based)
    introduction, I enjoyed *Real-World Machine Learning* by Henrik Brink, Joseph
    W. Richards, and Mark Fetherolf (Manning, 2016).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对学习机器学习感兴趣，我强烈推荐Gareth James、Daniela Witten、Trevor Hastie和Robert Tibshirani合著的《统计学习基础》（Springer，2021年出版，可在[https://www.statlearning.com](https://www.statlearning.com)免费在线获取）。这本书使用R语言，但其中的概念超越了语言界限。对于更实用的（基于Python的）介绍，我喜欢Henrik
    Brink、Joseph W. Richards和Mark Fetherolf合著的《现实世界机器学习》（Manning，2016年）。
- en: 12.1 Reading, exploring, and preparing our machine learning data set
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 阅读和准备我们的机器学习数据集
- en: This section covers the ingestion and exploration of our machine learning data
    set. More specifically, we’ll review the content of our data frame, look at incoherences,
    and prepare our data for feature engineering.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了机器学习数据集的摄取和探索。更具体地说，我们将回顾数据框的内容，查看不一致性，并为特征工程准备数据。
- en: For our ML model, I chose a data set of 20,057 dish names that contain 680 columns
    characterizing the ingredient list, the nutritional content, and the category
    of the dish. Our goal here is to predict if this dish is a dessert. It is a simple,
    mostly unambiguous question—you can probably classify a dish as a dessert or not
    just by reading the name—which makes it perfect for a simple ML model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的机器学习模型，我选择了一个包含 20,057 个菜名且包含 680 列的数据集，这些列描述了配料清单、营养成分和菜品的类别。我们的目标是预测这道菜是否是甜点。这是一个简单、基本上没有歧义的问题——你只需通过阅读名称就可以判断一道菜是否是甜点——这使得它非常适合简单的机器学习模型。
- en: At their core, data cleanup, exploration, and feature preparation are purpose-driven
    data transformation. Because the data is a CSV file, we reuse the content from
    chapters 4 and 5\. We will use schema information to determine the type of data
    each column contains (chapter 6), and even use UDFs (chapters 8 and 9) for some
    specialized column transformations. All of the skills learned thus far will be
    put to good use!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，数据清理、探索和特征准备都是目的驱动的数据转换。因为数据是 CSV 文件，我们重用了第 4 章和第 5 章的内容。我们将使用模式信息来确定每列包含的数据类型（第
    6 章），甚至使用 UDF（第 8 章和第 9 章）进行一些特殊的列转换。到目前为止学到的所有技能都将得到良好的应用！
- en: 'The data set is available online on Kaggle, an online community for ML enthusiasts
    that hosts modeling competitions, as well as interesting data sets ([https://www.kaggle.com/hugodarwood/epirecipes](https://www.kaggle.com/hugodarwood/epirecipes)).
    I also included the data in the book’s companion repository (under `data/recipes/epi_r.csv`).
    We start our program by setting our `SparkSession` (listing 12.1): we are allocating
    8 gibibytes of RAM to my driver (see chapter 11 for more information about how
    Spark allocates memory). The code in listing 12.2 reads the data frame (using
    the CSV specialized `SparkReader` object seen in chapter 4) and prints the dimensions
    of the data frame: 20,057 rows and 680 columns. We will track those dimensions
    as we clean the data frame to see how many records are impacted or filtered out.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可在 Kaggle 上在线获取，Kaggle 是一个面向机器学习爱好者的在线社区，举办建模竞赛以及有趣的数据集（[https://www.kaggle.com/hugodarwood/epirecipes](https://www.kaggle.com/hugodarwood/epirecipes)）。我还将数据包含在本书的配套仓库中（在
    `data/recipes/epi_r.csv` 下）。我们通过设置 `SparkSession` 开始我们的程序（列表 12.1）：我们为我的驱动器分配了
    8 吉字节（gibibytes）的 RAM（有关 Spark 如何分配内存的更多信息，请参阅第 11 章）。列表 12.2 中的代码读取数据框（使用第 4
    章中看到的 CSV 专用 `SparkReader` 对象）并打印数据框的维度：20,057 行和 680 列。我们将跟踪这些维度，以便在清理数据框时查看受影响的记录或过滤掉的记录数量。
- en: Listing 12.1 Starting our `SparkSession` for our machine learning program
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.1 为我们的机器学习程序启动 `SparkSession`
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Listing 12.2 Ingesting our data set and printing the dimension and schema
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.2 读取我们的数据集并打印维度和模式
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Our data set starts with 20,057 rows and 680 columns.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的数据集开始时有 20,057 行和 680 列。
- en: '❷ Some of the columns contains undesirable characters, such as a # . . .'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ 一些列包含不希望出现的字符，例如 # . . .'
- en: ❸ . . . or a space . . .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ . . . 或者一个空格 . . .
- en: ❹ or some invalid characters!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 或者一些无效字符！
- en: Before we even start looking at the data, let’s make our life easier by standardizing
    the names of our columns. The next section will show a trick to rename the whole
    data frame in one fell swoop.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始查看数据之前，让我们通过标准化列名来简化我们的工作。下一节将展示一个一次性重命名整个数据框的技巧。
- en: 12.1.1 Standardizing column names using toDF()
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 使用 toDF() 标准化列名
- en: In this section, we process all the column names to give them a uniform look
    and facilitate their subsequent usage. We will remove anything that isn’t a letter
    or a number, standardize the spaces and other separators to use the underscore
    (`_`) character, and replace the ampersand (`&`) with its English equivalent `and`.
    While not mandatory, this will help us in writing a clearer program and improving
    the consistency of our column names by reducing typos and mistakes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们处理所有列名，使它们具有统一的外观，并便于后续使用。我们将删除任何不是字母或数字的内容，将空格和其他分隔符标准化为下划线（`_`）字符，并将和号（`&`）替换为其英文等价词
    `and`。虽然这不是强制性的，但这将帮助我们编写更清晰的程序，并通过减少拼写错误和错误来提高列名的一致性。
- en: Just by looking at the schema in section 12.1, we can already see some not-so-desirable
    column names. While `#cakeweek` is relatively easy to type, `crêpe` might be a
    little harder if you don’t have a French keyboard, and don’t get me started on
    `cr??me` `de` `cacao`! When working with data, I like my columns to all be lowercase,
    with underscores between the words.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看第 12.1 节中的模式，我们就可以看到一些不太理想的列名。虽然 `#cakeweek` 较容易输入，但如果没有法语键盘，`crêpe` 可能会稍微困难一些，更不用说
    `cr??me` `de` `cacao` 了！在处理数据时，我喜欢我的列都是小写，单词之间用下划线分隔。
- en: 'To do so, listing 12.3 has a simple Python function, `sanitize_column_name()`,
    that will take a “dirty” column name and return it clean. The function is then
    applied to all the columns in my data frame in one fell swoop, using the `toDF()`
    method introduced in chapter 4\. `toDF()`, when used to rename the columns of
    a data frame, takes as parameters *N* strings, where *N* is the number of columns
    in our data frame. Since we can access the columns of our data frame via `food.columns`,
    a quick list comprehension takes care of renaming everything. I also unpack my
    list into distinct attributes using the `star` operator (see appendix C for more
    details). Having a consistent column naming scheme will make subsequent code easier
    to write, read, and maintain in the long run: I treat column names like variables
    in a regular program.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，列表 12.3 中的简单 Python 函数 `sanitize_column_name()` 会接受一个“脏”列名并将其转换为干净的名称。然后，该函数一次性应用于我的数据框中的所有列，使用第
    4 章中介绍的 `toDF()` 方法。当 `toDF()` 用于重命名数据框的列时，它接受 *N* 个字符串作为参数，其中 *N* 是我们数据框中的列数。由于我们可以通过
    `food.columns` 访问数据框的列，快速列表推导式就可以处理所有的重命名。我还使用 `star` 操作符将列表解包到不同的属性中（更多详情请见附录
    C）。保持一致的列命名方案将使后续的代码编写、阅读和维护更加容易：我将列名视为常规程序中的变量。
- en: Listing 12.3 Sanitizing my columns in one operation
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.3 一次性清理我的列
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ I iterate over the characters I want to get rid of, replacing them with something
    more consistent.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我遍历想要移除的字符，并用更一致的东西替换它们。
- en: ❷ We only keep letters, numbers, and underscores.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们只保留字母、数字和下划线。
- en: With this out of the way, we can now start exploring the data. In this section,
    we ingested and cleaned the column names of our data, making the data frame friendlier
    to work with. In the next section, we’ll classify our columns as different kinds
    of features, assess the quality of our data, and fill in the gaps.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 清理完这些后，我们现在可以开始探索数据了。在本节中，我们处理并清洗了数据列的名称，使数据框更易于使用。在下一节中，我们将对列进行分类，作为不同类型的特征，评估数据质量，并填补空白。
- en: 12.1.2 Exploring our data and getting our first feature columns
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 探索我们的数据并获得我们的第一个特征列
- en: 'This section covers digging into our data and encoding our first machine learning
    features. I introduce the main kind of machine learning features and how to easily
    keep track of those that we feed into our model training. While we are iteratively
    exploring data and creating ML features, keeping a tally of the ones we think
    are promising is the best way for us to stay organized and keep our code organized.
    At this stage, think of your code as a collection of lab notes: the tidier they
    are, the easier it’ll be to review your work and then production-ize your results!'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涉及深入挖掘我们的数据并编码我们的第一个机器学习特征。我介绍了主要类型的机器学习特征以及如何轻松跟踪我们输入模型训练的特征。当我们迭代探索数据和创建机器学习特征时，记录我们认为有潜力的特征是我们保持组织并使代码保持整洁的最佳方式。在这个阶段，将你的代码视为实验笔记的集合：它们越整洁，回顾你的工作以及将结果投入生产就越容易！
- en: Exploring data for machine learning is similar to exploring data when performing
    a transformation in the sense that we manipulate the data to uncover some inconsistencies,
    patterns, or gaps. Because of this, all the material in previous chapters applies
    here. Talk about convenience! On the other hand, machine learning has a few idiosyncrasies
    that impact how we reason about and prepare data. In listing 12.4, I print a summary
    table for each of the columns in our `food` data frame. This takes a while but
    gives us a decent summary of the data contained in each column. Unlike single-node
    data processing, PySpark cannot necessarily assume that a column will fit into
    memory, so we can’t go crazy with charts and extensive data profiling tools.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 探索机器学习中的数据与在执行转换时探索数据类似，因为我们通过操纵数据来揭示一些不一致性、模式或缺口。正因为如此，前几章的所有材料都适用于这里。多么方便！另一方面，机器学习有一些独特之处，这影响了我们对数据和数据准备的推理和准备方式。在列表12.4中，我为我们的`food`数据框中的每一列打印了一个汇总表。这需要一些时间，但为我们提供了每个列中数据的良好总结。与单节点数据处理不同，PySpark不能必然假设一个列可以放入内存中，因此我们无法使用图表和广泛的数据分析工具。
- en: Listing 12.4 Creating a summary table of all our columns
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.4 创建所有列的汇总表
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The clove column contains 20,052 non-null values (so five records are null).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 肉豆蔻列包含20,052个非空值（因此有五个记录为空）。
- en: ❷ Since the quartile distribution is only 0.0 and 1.0, there is a very good
    chance that our column is binary (0, 1).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于四分位数分布仅为0.0和1.0，我们的列很可能是二元的（0，1）。
- en: 'Tip One of the best tips when processing data in PySpark is recognizing that
    your data is small enough to be gathered into a single node. For pandas DataFrames,
    you can use the excellent `pandas-profiling` library to automate a lot of the
    data profiling if your data is pandas size ([https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)).
    Remember: your Python knowledge doesn’t go away when you use PySpark!'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：在PySpark中处理数据时，最好的建议之一是认识到你的数据足够小，可以收集到一个单独的节点中。对于pandas DataFrames，如果你的数据大小是pandas的，你可以使用出色的`pandas-profiling`库来自动化大部分的数据分析（[https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)）。记住：当你使用PySpark时，你的Python知识并没有消失！
- en: 'In our summary data, we are looking at numerical columns. In machine learning,
    we classify numerical features into two categories: *categorical* or *continuous*.
    A categorical feature is when your column takes a discrete number, such as the
    month of the year (1 to 12). A continuous feature is when the column can have
    infinite possibilities, such as the price of an item. We can subdivide the categorical
    family into three main types:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的汇总数据中，我们正在查看数值列。在机器学习中，我们将数值特征分为两类：*分类*或*连续*。分类特征是指你的列取离散数值，例如一年的月份（1到12）。连续特征是指列可以有无限的可能性，例如物品的价格。我们可以将分类家族细分为三种主要类型：
- en: '*Binary* (or *dichotomous*), when you have only two choices (0/1, true/false)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二元*（或*二分*），当你只有两个选择（0/1，真/假）'
- en: '*Ordinal*, when the categories have a certain ordering (e.g., the position
    in a race) that matters'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有序*，当类别有某种顺序时（例如，比赛中的位置）并且这个顺序很重要'
- en: '*Nominal*, when the categories have no specific ordering (e.g., the color of
    an item)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*名义*，当类别没有特定的顺序时（例如，物品的颜色）'
- en: Identifying your variables as categorical (with the proper subtype) or continuous
    has a direct impact on the data preparation and, down the road, the performance
    of your ML model (use the decision tree in figure 12.1 to assist you). Proper
    identification is dependent on the context (what does the column mean?) and how
    you want to encode its meaning. You’ll develop a stronger intuition as you develop
    more ML programs. Don’t worry if you don’t get it right the first time; you can
    always come back and touch up your feature types. In chapter 13, we introduce
    ML pipelines, which provide a nice abstraction for feature preparation, making
    it easy to evolve your ML code over time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的变量识别为分类（带有适当的子类型）或连续变量，这将对数据准备以及未来你的机器学习模型（使用图12.1中的决策树来帮助你）的性能产生直接影响。正确的识别取决于上下文（这一列代表什么？）以及你想要如何编码其含义。随着你开发更多的机器学习程序，你将培养出更强的直觉。如果你第一次没有做对，不要担心；你总是可以回来调整你的特征类型。在第13章中，我们介绍了机器学习管道，它为特征准备提供了一个很好的抽象，使得随着时间的推移轻松地进化你的机器学习代码。
- en: '![](../Images/12-01.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-01.png)'
- en: Figure 12.1 The different types of numerical features in a decision tree. Answer
    the questions to get which feature you are!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 决策树中数值特征的类型。回答问题以确定你关注的特征！
- en: Looking at our summary data, it seems that we have a lot of potentially binary
    columns. In the case of the `clove` column, the minimum and three quartile values
    are all zero. To verify this, we’ll group the entire data frame and collect a
    set of distinct values. If we have only two values for a given column, binary
    it is! In listing 12.5, we create a temporary data frame, `is_binary`, to identify
    the binary columns. We collect the results into a pandas DataFrame—since the resulting
    data frame has only one row—and un-pivot the result using the `unstack()` method
    available through pandas (PySpark has no easy way to un-pivot). Most columns are
    binary. I am personally not convinced about `cakeweek` and `wasteless`. Time to
    investigate!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们的汇总数据，似乎我们有很多可能为二元的列。在`clove`列的情况下，最小值和三个四分位数都是零。为了验证这一点，我们将整个数据框分组并收集一组不同的值。如果我们给定列中只有两个值，那么它就是二元的！在列表12.5中，我们创建了一个临时数据框`is_binary`来识别二元列。我们将结果收集到一个pandas
    DataFrame中——因为结果数据框只有一行——并使用pandas（PySpark没有简单的反转方法）的`unstack()`方法来反转结果。大多数列都是二元的。我个人对`cakeweek`和`wasteless`并不确信。是时候进行调查了！
- en: Listing 12.5 Identifying the binary columns from our data frame
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.5 从我们的数据框中识别二元列
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ pandas will display a handful of rows at a time. Setting this option will
    print at most 1,000 rows, which will be helpful when exploring data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ pandas会一次显示几行。设置此选项将打印最多1,000行，这在探索数据时将非常有帮助。
- en: ❷ collect_set() will create a set of the distinct values as an array, and size()
    returns the length of the array. Two distinct values means that it’s probably
    binary.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `collect_set()`将创建一个包含不同值的数组集合，`size()`返回数组的长度。两个不同的值意味着它可能是一个二元值。
- en: ❸ unstack un-pivots a pandas DataFrame, making a wide data frame easier to analyze
    in the terminal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ `unstack`反转pandas DataFrame，使宽数据框在终端中更容易分析。
- en: This section covered the main types of numerical features we encounter in machine
    learning and identified the binary features in our data frame. In the next section,
    we’ll perform some analysis on the remaining columns and establish our base set
    of features.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了我们在机器学习中遇到的主要数值特征类型，并识别了我们数据框中的二元特征。在下一节中，我们将对剩余的列进行一些分析，并建立我们的基本特征集。
- en: 12.1.3 Addressing data mishaps and building our first feature set
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.3 解决数据错误并构建我们的第一个特征集
- en: In this section, we investigate some seemingly incoherent features and, following
    our findings, clean our data set. We also identify our first feature set, along
    with each feature type. This section is an example of forensic data exploration,
    a tremendously important part of a data analyst and scientist’s job. In this case,
    some columns are not consistent compared to other related (binary) columns. We
    explore the content of the suspicious columns, address the gaps, and continue
    our exploration. The result? A more consistent, more robust feature set that will
    lead to a better ML model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们调查了一些看似不连贯的特征，并根据我们的发现清理了数据集。我们还确定了我们的第一个特征集，以及每个特征类型。本节是法医数据探索的一个例子，这是数据分析师和科学家工作中极其重要的一部分。在这种情况下，一些列与其他相关（二元）列相比不一致。我们探索了可疑列的内容，解决了差距，并继续我们的探索。结果？一个更一致、更健壮的特征集，这将导致更好的机器学习模型。
- en: 'At the end of section 12.1.2, we concluded that the vast majority of our feature
    columns in our data set were binary. Furthermore, there were two columns that
    were suspicious: `cakeweek` and `wasteless`. In the next listing, I display the
    discrete values both columns can take and then show the records where one of them
    contains a nonbinary value.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在12.1.2节的结尾，我们得出结论，我们数据集中的大多数特征列都是二元的。此外，还有两列可疑：`cakeweek`和`wasteless`。在下一个列表中，我显示了这两列可以取的离散值，然后显示了其中一列包含非二元值的记录。
- en: Listing 12.6 Identifying the distinct values for our two suspicious columns
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.6 识别我们两个可疑列的离散值
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ I print the first few records and the last records to see potential data alignment
    problems.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我打印了前几条记录和最后几条记录，以查看潜在的数据对齐问题。
- en: For three records, it seems like our data set had a bunch of quotation marks
    along with some commas that confused PySpark’s otherwise robust parser. In our
    case, since we have a small number of records affected, I did not bother with
    realigning the data and deleted them outright. I keep the `null` values as well.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三条记录，我们的数据集似乎有一堆引号和一些逗号，这使PySpark的强大解析器感到困惑。在我们的情况下，由于受影响的记录数量很少，我没有麻烦重新对齐数据，而是直接删除了它们。我也保留了`null`值。
- en: Should a large number of records be *misaligned*, where the CSV records boundaries
    that do not align consistently with what you’d expect, you can try to read the
    record as text and use a UDF to extract the relevant information manually, before
    saving the results into a better data format such as Parquet (see chapter 6 and
    10). There is no silver bullet for problematic CSV data, unfortunately.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果大量记录存在*不匹配*的情况，即CSV记录边界与预期的结果不一致，你可以尝试将记录作为文本读取，并使用UDF手动提取相关信息，然后再将结果保存到更好的数据格式，如Parquet（参见第6章和第10章）。不幸的是，对于有问题的CSV数据，没有银弹。
- en: Listing 12.7 Keeping only the legit values for `cakeweek` and `wasteless`
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.7 仅保留`cakeweek`和`wasteless`的合法值
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '❶ This reads as follows: "''if cakeweek and wasteless are both either 0.0,
    1.0, or null.''"'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这读起来如下：“'如果cakeweek和wasteless都是0.0、1.0或null。””
- en: ❷ We lost three records, as expected.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如预期，我们失去了三条记录。
- en: In listing 12.7, I check my filtering by printing the dimensions of my data
    frame. It’s not a perfect way to know if my code is bug-free, but it helps to
    validate that I’m removing only the three offending records. If something goes
    wrong data-wise, this information can help to pinpoint where in the code our data
    went wrong. For instance, if after filtering the data frame I lost 10,000 records
    when I expected to lose only three, this would tell me that I might be heavy-handed
    with my filtering.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.7中，我通过打印我的数据框的维度来检查我的过滤。这并不是一个完美的知道我的代码是否无错误的方法，但它有助于验证我是否只移除了三个有问题的记录。如果数据有问题，这些信息可以帮助确定代码中数据出错的地方。例如，如果在过滤数据框后，我失去了10,000条记录，而预期只失去三条，这将告诉我我可能在过滤时过于严厉。
- en: 'Now that we have identified two binary-in-hiding feature columns, we can identify
    our feature set and our target variable. The *target* (or *label*) is the column
    containing the value we want to predict. In our case, the column is aptly named
    `dessert`. In listing 12.8, I create all-caps variables containing the four main
    sets of columns I care about:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了两个隐藏的二进制特征列，我们可以确定我们的特征集和目标变量。*目标*（或*标签*）是包含我们想要预测的值的列。在我们的案例中，该列恰当地命名为`dessert`。在列表12.8中，我创建了包含我关心的四个主要列集的全大写变量：
- en: The *identifiers*, which are the column(s) that contain the information unique
    to each record
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标识符*，这些是包含每个记录独特信息的列（s）'
- en: The *targets*, which are the column(s) (most often one) that contain the value
    we wish to predict
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*目标*，这些是包含我们希望预测的值的列（s）（通常是单个）'
- en: The *continuous* columns, containing continuous features
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连续*列，包含连续特征'
- en: The *binary* columns, containing binary features
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二进制*列，包含二进制特征'
- en: The data set does not seem to contain categorical variables.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集似乎不包含分类变量。
- en: Listing 12.8 Creating four top-level variables
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.8 创建四个顶级变量
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Although I have only one target, I find it convenient to put it into a list
    to be consistent with the other VARIABLES.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 尽管我只有一个目标，但我发现将其放入列表中以便与其他变量保持一致很方便。
- en: I like to keep track of my features through variables instead of deleting them
    from my data frame. It removes some of the guesswork when you get the data ready
    for the model training—which columns are features, again?—and it serves as lightweight
    documentation when reading your code the next time. It’s basic, but it serves
    our purpose well here.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢通过变量而不是从我的数据框中删除它们来跟踪我的特征。当数据准备好进行模型训练时，这减少了猜测工作——哪些列是特征？——并且当你在下次阅读代码时，它充当轻量级的文档。这很简单，但在这里很好地满足了我们的目的。
- en: In this section, we rapidly cleaned our data. In practice, this stage will take
    more than half your time when building an ML model. Fortunately, data cleaning
    is principled data manipulation, so you can leverage everything in the PySpark
    tool kit you’ve built so far. We also identified our features and their type and
    grouped them into lists, which makes it easier to reference in the next sections.
    In the next section, we’ll take care of removing useless records and filling the
    `null` values of the binary features.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们迅速清理了我们的数据。在实践中，当构建机器学习模型时，这一阶段将占用你超过一半的时间。幸运的是，数据清理是原则性的数据操作，因此你可以利用你迄今为止构建的PySpark工具包中的所有功能。我们还确定了我们的特征及其类型，并将它们分组到列表中，这使得在下一节中引用它们变得更加容易。在下一节中，我们将处理移除无用记录和填充二进制特征的`null`值。
- en: 12.1.4 Weeding out useless records and imputing binary features
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.4 移除无用记录和填充二进制特征
- en: 'This section covers the deletion of useless records, those that provide no
    information to our ML model. In our case, this means removing two types of records:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了删除无用记录，那些对我们机器学习模型没有任何信息的记录。在我们的案例中，这意味着移除两种类型的记录：
- en: Those where all the features are `null`
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有特征都是 `null` 的那些
- en: Those where the target is `null`
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是 `null` 的那些
- en: Furthermore, we will impute, meaning that we will provide a *default value*
    for, our binary features. Since each of them are `0/1`, where zero is `False`
    and one is `True`, we equate `null` to `False` and fill zero as a default value.
    Given the context of our model, this is a reasonable assumption. Those operations
    are common to every ML model. We always face a point where we want to ensure every
    record will provide some sort of information to our ML model. I like to perform
    this early in the process, as filtering completely `null` records needs to happen
    before any imputation. Once you’ve filled `null` values on certain columns, you
    won’t know which record was entirely `null`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将进行插补，这意味着我们将为我们的二元特征提供一个 *默认值*。由于每个都是 `0/1`，其中零是 `False`，一是一 `True`，我们将
    `null` 等同于 `False` 并用零作为默认值。考虑到我们模型的环境，这是一个合理的假设。这些操作对每个机器学习模型都是常见的。我们总是面临一个想要确保每条记录都会为我们的机器学习模型提供某种信息的点。我喜欢在早期阶段进行这项操作，因为完全
    `null` 记录的过滤需要在任何插补之前发生。一旦你在某些列中填充了 `null` 值，你就不会知道哪条记录是完全 `null` 的。
- en: Keep some lab notes!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 记录一些实验笔记！
- en: In this section and the next, we do quite a bit of yo-yoing. I did my best to
    relatively order the data-cleaning portion of our program. In all honesty, when
    building the original script for this chapter, I rearranged many sections as I
    profiled and examined the data. Your own attempt at cleaning the data would have
    certainly yielded a very different program.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节和下一节中，我们进行了大量的反复操作。我尽力相对排序我们程序的数据清理部分。说实话，在为这一章构建原始脚本时，我在分析并检查数据的过程中重新排列了许多部分。你自己的数据清理尝试肯定会产生一个非常不同的程序。
- en: Data preparation for machine learning is part art, part science. Intuition and
    experience come into play; you will end up recognizing some data patterns and
    create a personal library of strategies to deal with them. Because of this, it’s
    crucial to document your steps to make it easier on your future self (and your
    colleagues!) when you pull that code back. I keep a notebook by my side when cleaning
    data, and I make sure to collect my “lab” notes in a format that will be easy
    to share.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的数据准备部分是艺术，部分是科学。直觉和经验发挥作用；你最终会识别出一些数据模式，并创建一个个人策略库来处理它们。正因为如此，记录你的步骤对于将来（以及你的同事！）更容易地重新提取代码至关重要。我在清理数据时会在旁边放一个笔记本，并确保以易于分享的格式收集我的“实验室”笔记。
- en: The code in listing 12.9 uses the `dropna()` method, seen in chapter 5 with
    two subsets. The first one is *every column but the name of the recipe* (stored
    in the `IDENTIFIERS` variable). The second one is the `TARGET_COLUMN`. We lose
    five records in the process. Because of the low number of records lost, I won’t
    bother with *manual labeling*, or manually inputting the values for each record
    according to my best judgment. Labeling is always a labor-intensive operation,
    but sometimes, for instance, when your target is spotty or you have very little
    data,[¹](#pgfId-1018043) you can’t get around it. Robert (Munro) Monarch dedicates
    a complete book on the topic in *Human-in-the-Loop Machine Learning* (Manning,
    2021).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.9 中的代码使用了在第五章中提到的 `dropna()` 方法，涉及两个子集。第一个是除了“食谱”名称之外的所有列（存储在 `IDENTIFIERS`
    变量中）。第二个是 `TARGET_COLUMN`。在这个过程中我们失去了五条记录。由于丢失的记录数量很少，我不会去麻烦进行 *人工标记*，即根据我的最佳判断手动输入每条记录的值。标记始终是一项劳动密集型操作，但有时，例如，当你的目标是零散的或者你数据非常少时，[¹](#pgfId-1018043)
    你无法避免它。Robert (Munro) Monarch 在 *Human-in-the-Loop Machine Learning*（Manning,
    2021）这本书中为这个主题奉献了一整本书。
- en: Listing 12.9 Removing the records that have only `null` values
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.9 移除只包含 `null` 值的记录
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ I can use the feature group variables instead of having to remember which
    column is which.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我可以使用特征组变量，而不必记住哪一列是哪一列。
- en: ❷ We lose five records in the process (20,054 - 5 = 20,049).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在这个过程中我们失去了五条记录（20,054 - 5 = 20,049）。
- en: As a second step, I impute a default value to all my binary columns. As a rule
    of thumb, `1` means `True` and `0` means `False`. In the context of our binary
    variable (presence of an ingredient or classification of the dish according to
    a certain label), an absence of a value can be thought of as being conceptually
    closer to false than true, so we’ll default every binary feature column to `0.0`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第二步，我为所有的二元列插补一个默认值。作为一个经验法则，`1` 表示 `True`，`0` 表示 `False`。在我们的二元变量（成分的存在或根据某个标签对菜肴的分类）的背景下，值的缺失可以被认为是概念上更接近于
    `False` 而不是 `True`，因此我们将每个二元特征列的默认值设为 `0.0`。
- en: Listing 12.10 Setting a default value of `0.0` to every binary feature column
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.10 将每个二元特征列的默认值设为 `0.0`
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This section covered the filtration of useless records using `dropna()`, as
    well as the imputation of binary features according to a scalar value. In the
    next section, we look at continuous columns by exploring their distribution and
    checking the ranges of values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了使用 `dropna()` 过滤无用记录，以及根据标量值对二元特征进行插补。在下一节中，我们将通过探索分布和检查值域来查看连续列。
- en: '12.1.5 Taking care of extreme values: Cleaning continuous columns'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.5 处理极端值：清理连续列
- en: This section covers the analysis of continuous values in the context of feature
    preparation. More specifically, we review the distribution of numerical columns
    to account for extreme or unrealistic values. Many ML models don’t deal well with
    extreme values (see chapter 13, when we discuss feature normalization). Just like
    we did with binary columns, taking the time to assess the fit of our numerical
    columns will pay dividends since we will not feed the wrong information to our
    ML model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在特征准备背景下对连续值的分析。更具体地说，我们回顾了数值列的分布，以考虑极端或不切实际的价值。许多机器学习模型处理极端值的能力不佳（参见第13章，当我们讨论特征归一化时）。就像我们对二元列所做的那样，花时间评估我们的数值列的拟合度将带来回报，因为我们不会向机器学习模型提供错误的信息。
- en: Warning In this section, we process extreme values using the knowledge we have
    about the data. We do not build a blanket blueprint to be applied regardless of
    the situation. Careless data transformation can introduce anomalies in your data,
    so be sure to take the time to understand the problem at hand.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 在本节中，我们使用我们对数据的了解来处理极端值。我们不会建立一个无论情况如何都要应用的通用蓝图。粗心大意的数据转换可能会在你的数据中引入异常，所以请确保花时间理解手头的问题。
- en: Before we can start exploring the distribution of our continuous features, we
    need to make sure they are properly typed (see chapter 6 for a refresher on types).
    Looking back at our schema in listing 12.2, because of some data misalignment,
    PySpark inferred the type of the `rating` and `calories` column as a string, where
    they should have been numerical. In listing 12.11, a simple UDF takes a string
    column and returns `True` if the value is a floating-point number (or a `null`—PySpark
    will allow `null` values in a `Double` column) and `False` otherwise. I am doing
    this more as an exploration than as a bona fide cleaning step; since a string
    value in any of those two columns means that the data is misaligned, I will drop
    the record rather than try to fix it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索连续特征的分布之前，我们需要确保它们被正确地类型化（参见第6章以复习类型）。回顾我们的方案列表12.2，由于某些数据错位，PySpark
    推断 `rating` 和 `calories` 列的类型为字符串，而它们应该是数值类型。在列表12.11中，一个简单的 UDF 接受一个字符串列，如果值是浮点数（或
    `null`——PySpark 将允许 `Double` 列中的 `null` 值）则返回 `True`，否则返回 `False`。我这样做更多的是作为一种探索，而不是作为一个真正的清理步骤；因为这两个列中的任何字符串值都意味着数据错位，我将删除记录而不是尝试修复它。
- en: The UDF looks rather complicated, but if we take it slowly, it’s very simple.
    I return `True` right off the bat if the value is `null`. If I have a non-`null`
    value, I try to cast the value as a Python `float`. If it fails, `False` it is!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: UDF 看起来相当复杂，但如果我们慢慢来，它是非常简单的。如果值是 `null`，我立即返回 `True`。如果我有非 `null` 值，我尝试将值转换为
    Python `float`。如果失败，则返回 `False`！
- en: Listing 12.11 Non-numerical values in the `rating` and `calories` columns
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.11 `rating` 和 `calories` 列中的非数值值
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The underscore means “perform the work, but I don’t care about the result.”
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 下划线表示“执行工作，但我不在乎结果。”
- en: ❷ We have one last rogue record!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们还有一个最后的异常记录！
- en: We have a single remaining rogue record (damn those pesky unaligned CSVs!) that
    I remove in the next listing before confidently casting the columns as a double.
    Our continuous feature columns are now all numerical.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只剩下一个顽固的记录（该死的未对齐CSV文件！）在下一个列表中移除之前，我将其作为双重值。现在我们的连续特征列都是数值的。
- en: Listing 12.12 Casting the `rating` and `calories` columns into double
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.12 将`rating`和`calories`列转换为double
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ One record lost!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一条记录丢失！
- en: Now we want to look at the actual values to remove any ridiculous values that
    would break the computation of the average. We repeat the summary table displayed
    in rapid fire during our initial data exploration (listing 12.4) in listing 12.13\.
    We immediately see that some dishes are over the top!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想查看实际值，以去除任何会导致平均数计算中断的荒谬值。我们重复显示在初始数据探索过程中快速展示的总结表（列表12.4），在列表12.13中。我们立刻看到有些菜式已经过分夸张了！
- en: Just like with binary features, we need to use our judgment for the best course
    of action to address this data quality issue. I could filter the records once
    more, but this time, I’ll cap the values to the 99th percentile, avoiding extreme
    (and potentially wrong) values.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就像二进制特征一样，我们需要用我们的判断来决定最佳行动方案以解决这个数据质量问题。我可以再次过滤记录，但这次，我将值限制在第99百分位数，避免极端（可能错误）的值。
- en: Listing 12.13 Looking at the values in our continuous feature columns
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.13 查看我们的连续特征列中的值
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the next listing, I hardcode the `maximum` acceptable values for each column,
    and then I apply those maximums iteratively to my food data frame.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我硬编码了每个列的`最大`可接受值，然后我将这些最大值迭代地应用到我的食物数据框中。
- en: Listing 12.14 Imputing the average value for four continuous columns
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.14 插补四个连续列的平均值
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ I hardcode the values here to make sure my analysis is consistent across runs.
    If the data changes, I might want to recheck if the 99th percentile is still a
    good measure before automating the imputation, but I am comfortable with those
    exact values for the time being.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我在这里硬编码值以确保我的分析在运行之间是一致的。如果数据发生变化，我可能想在自动化插补之前重新检查第99百分位数是否仍然是一个好的衡量标准，但就目前而言，我对这些确切值感到满意。
- en: ❷ Because I want to preserve the null values, I keep them through a when clause.
    The least function will only apply to non-null records.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 因为我想保留空值，所以我通过when子句保留它们。最小函数只会应用于非空记录。
- en: There is no surefire way to take care of outliers or identify them in the first
    place; 5,661 mg of sodium is still criminally high, but more realistic considering
    some outrageous recipes available in the wild. In this chapter, I won’t come back
    to it, but this would be one instance where I’d leave some breadcrumbs after completing
    a full cycle to tweak my approach.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种万无一失的方法来处理异常值或首先识别它们；5,661毫克的钠含量仍然触目惊心，但考虑到野外一些荒谬的食谱，这更加现实。在本章中，我不会再回到这个问题，但这将是一个在完成整个周期后留下一些线索以调整我的方法的例子。
- en: Note What about `null` imputation here, like we did on for the binary features?
    PySpark provides a convenient mechanism through the `Imputer` estimator. We see
    this useful topic in section 12.3.1.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：关于这里的`null`插补，就像我们在二进制特征上所做的那样？PySpark提供了一个方便的机制，通过`Imputer`估计器。我们将在12.3.1节中看到这个有用的主题。
- en: In this section, we imputed `null` records globally on our data set. We also
    cleaned the categorical feature columns using a small UDF. In the next section,
    we head back to binary columns to remove low-occurring features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们在数据集上全局插补了`null`记录。我们还使用一个小UDF清理了分类特征列。在下一节中，我们将回到二进制列以移除低发生率的特征。
- en: 12.1.6 Weeding out the rare binary occurrence columns
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.6 清理罕见的二进制发生列
- en: 'In this section, I remove the columns that are not present enough in the data
    set to be considered reliable predictors. Binary features with only a few zeroes
    or ones are not helpful in classifying a recipe as a dessert: if every recipe
    (or no recipe) has a certain feature as true, then that feature does not *discriminate*
    properly, meaning that our model has no use for it.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我移除了数据集中不足以作为可靠预测因子的列。只有少数零或一的二进制特征在将食谱分类为甜点时没有帮助：如果每个食谱（或没有食谱）都有一个特征为真，那么这个特征就不能*区分*，这意味着我们的模型对此没有用处。
- en: Rarely occurring features are an annoyance when building a model, as the machine
    can pick up a signal that is there by chance. For example, if you are flipping
    a fair coin and get heads and use that as a feature for a model predicting the
    next flip, you might get a dummy model that will predict 100% heads. It’ll work
    perfectly until you get tails. In the same vein, you want to have enough representation
    for each feature that goes into your model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 稀有发生的特征在构建模型时是一个烦恼，因为机器可能会捕捉到偶然存在的信号。例如，如果你在抛一个公平的硬币，并把它作为预测下一次翻转的模型的特征，你可能会得到一个预测100%正面的假模型。它将完美工作，直到你得到反面。同样，你希望每个进入你模型的特征都有足够的代表性。
- en: For this model, I choose 10 to be my threshold. I do not want binary features
    with less than 10 of `0.0` or `1.0` in my model. In listing 12.15, I compute the
    sum of each binary column; this will give me the number of 1.0 since the sum of
    the ones is equal to their count. If the count of the ones/sum of a column is
    below 10 or above the number of records minus 10, I collect the column name to
    remove it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，我选择10作为我的阈值。我不希望我的模型中有小于10个`0.0`或`1.0`的二进制特征。在列12.15中，我计算了每个二进制列的总和；这将给我1.0的数量，因为1的总和等于它们的计数。如果一个列中1的计数/总和低于10或高于记录数减去10，我就收集列名以移除它。
- en: Listing 12.15 Removing the binary features that happen too little or too often
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列12.15 移除发生得太少或太多的二进制特征
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Since a row is just like a Python dictionary, I can bring the row back to
    the driver and process it locally.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于一行就像一个Python字典，我可以将行带回驱动程序并本地处理。
- en: ❷ Rather than deleting the columns from the data frame, I just remove them from
    my BINARY_COLUMNS list.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 而不是从数据帧中删除列，我只是从我的BINARY_COLUMNS列表中移除它们。
- en: We removed 167 features that are either too rare or too frequent. While this
    number seems high, some of the features are very precise for a data set with a
    few thousand recipes. When creating your own model, you will certainly want to
    play around with different values to see if some parameters are still too rare
    to provide reliable predictions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了167个要么太稀少要么太频繁的特征。虽然这个数字看起来很高，但其中一些特征对于只有几千个菜谱的数据集来说非常精确。当你创建自己的模型时，你肯定会想尝试不同的值，看看是否有些参数仍然太稀少，无法提供可靠的预测。
- en: In this section, we removed rare binary features, reducing our feature space
    by 167 elements. In the next section, we look at creating custom features that
    might improve the predictive power of our model. We also generate and refine new
    features using a combination of those that already exist.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们移除了稀有的二进制特征，通过减少167个元素来缩小我们的特征空间。在下一节中，我们将探讨创建可能提高我们模型预测能力的自定义特征。我们还通过结合那些已经存在的特征来生成和细化新特征。
- en: 12.2 Feature creation and refinement
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 特征创建和细化
- en: 'This section covers two important steps of model building: *feature creation*
    (also called *feature engineering*) and *refinement*. Feature creation and refinement
    are where the data scientists can express their judgment and creativity. Our ability
    to encode meaning and recognize patterns in the data means that our model can
    more easily pick up on the signal. We could potentially spend a lot of time crafting
    more and more sophisticated features. Since my goal is to provide an end-to-end
    model using PySpark, we look at the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了模型构建的两个重要步骤：*特征创建*（也称为*特征工程*）和*细化*。特征创建和细化是数据科学家表达他们的判断力和创造力的地方。我们编码意义和识别数据中模式的能力意味着我们的模型可以更容易地捕捉到信号。我们可能会花很多时间精心制作越来越复杂的特征。由于我的目标是使用PySpark提供一个端到端模型，我们来看看以下内容：
- en: Creating a few custom features using our continuous feature columns
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们的连续特征列创建几个自定义特征
- en: Measuring correlation over original and generated continuous features
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量原始和生成连续特征之间的相关性
- en: These are by no means the only ways we could approach this, but the steps give
    a good overview of what can be done in PySpark.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些绝对不是我们能够采取的唯一方法，但步骤提供了一个很好的概述，说明了在PySpark中可以做什么。
- en: 12.2.1 Creating custom features
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 创建自定义特征
- en: In this section, we look at creating new features from the data we have at hand.
    Doing so can improve our model interpretability and predictive power. I show one
    example of feature preparation that places a few continuous features on the same
    scale as our binary ones.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨从我们手头的数据中创建新特征。这样做可以提高我们模型的可解释性和预测能力。我展示了一个将几个连续特征放置在我们二进制特征相同尺度上的特征准备示例。
- en: 'Fundamentally, creating a custom feature in PySpark is nothing more than creating
    a new column, with a little more thought and notes on the side. Manual feature
    creation is one of the secret weapons of a data scientist: you can embed business
    knowledge into highly custom features that can improve your model’s accuracy and
    interpretability. As an example, we’ll take the `protein` and `fat` columns representing
    the quantity (in grams) of protein and fat in the recipe, respectively. With the
    information in those two columns, I create two features representing the percentage
    of calories attributed to each macro nutriment.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在PySpark中创建自定义特征，本质上不过是创建一个新列，只需稍微多加思考和旁边的一些笔记。手动创建特征是数据科学家的一项秘密武器：你可以将业务知识嵌入到高度定制的特征中，从而提高模型准确性和可解释性。例如，我们将选取表示食谱中蛋白质和脂肪含量的`protein`和`fat`列（分别以克为单位）。利用这两个列中的信息，我创建了两个特征，分别表示分配给每种宏量营养素的卡路里百分比。
- en: Listing 12.16 Creating new features that compute calories attributable to protein
    and fat
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.16 创建计算蛋白质和脂肪可归因卡路里的新特征
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ There are 4 kcal per grams of protein and 9 kcal per grams of fat.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每克蛋白质有4千卡，每克脂肪有9千卡。
- en: ❷ I add the two columns in my set of continuous features.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我将这两个列添加到我的连续特征集中。
- en: By creating these two columns, I integrate new knowledge into my data. Without
    adding the energy per gram of fat and protein, nothing in the data set would have
    provided this. The model could have drawn a relationship between the actual quantity
    of fat/proteins and the total calories count independently, but we’re making this
    more obvious by allowing the model to have access to the ratio of protein/fat
    (and carbs; see the sidebar at the end of this section) directly.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建这两个列，我将新知识整合到我的数据中。如果不添加每克脂肪和蛋白质的能量，数据集中没有任何内容能提供这一点。模型可以独立地绘制脂肪/蛋白质的实际数量与总卡路里计数之间的关系，但我们通过允许模型直接访问蛋白质/脂肪（以及碳水化合物；参见本节末尾的侧边栏）的比例，使这种关系更加明显。
- en: 'Before we get to modeling, we’ll want to remove the correlation between our
    continuous variables and assemble all our features into a single, clean entity.
    This section was extremely short, but keep this lesson close to you when building
    a model: you can embed new knowledge into your data set by creating custom features.
    In PySpark, creating new features is done simply by creating columns with the
    information you want; this means you can create simple or highly sophisticated
    features.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行建模之前，我们希望消除连续变量之间的相关性，并将所有特征组合成一个单一、干净的实体。本节内容非常简短，但在构建模型时请牢记这个教训：通过创建自定义特征，你可以将新知识嵌入到数据集中。在PySpark中，创建新特征只需创建包含所需信息的列；这意味着你可以创建简单或高度复杂的特征。
- en: Why not do the same with carbs? Avoiding multicollinearity
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不也对碳水化合物做同样的事情？避免多重共线性
- en: Without going too deep into how food translates, I did not compute a carbs ratio
    as a custom feature. Beyond the fact that the total amount of carbs is not provided
    (and that carbs absorption is a little more complex), we have to consider the
    *linear dependence* (or *multicollinearity*) of our variables when working with
    certain types of models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在不深入探讨食物转换过程的情况下，我没有计算碳水化合物比率作为自定义特征。除了碳水化合物的总量没有提供（以及碳水化合物的吸收稍微复杂一些）之外，当我们使用某些类型的模型时，我们必须考虑变量的*线性依赖性*（或*多重共线性*）。
- en: A linear dependence between variables happens when you have one column that
    can be represented as a linear combination of others. You might be tempted to
    approximate the ratio of calories coming from carbs using the formula
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有一个列可以被表示为其他列的线性组合时，就会发生变量之间的线性依赖。你可能想用以下公式来近似碳水化合物来源的卡路里比率
- en: Total calories = 4 * (g of carbs) + 4 * (g of proteins) + 9 * (g of fat)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 总卡路里 = 4 *（碳水化合物克数）+ 4 *（蛋白质克数）+ 9 *（脂肪克数）
- en: 'or using a ratio-based approach:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 或者使用基于比率的办法：
- en: 1 = (% of calories from carbs) + (% of calories from proteins) + (% of calories
    from fat)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 1 = （碳水化合物卡路里百分比）+（蛋白质卡路里百分比）+（脂肪卡路里百分比）
- en: 'In both cases, we introduce a linear dependency: we (and the machine) can compute
    the values of a column using nothing but the values of other columns. When using
    a model that has a linear component, such as the linear regression and the logistic
    regression, this will cause problems with your model’s accuracy (either under-
    or over-fitting).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们引入了线性依赖性：我们（以及机器）可以使用其他列的值来计算列的值。当使用具有线性组件的模型，如线性回归和逻辑回归时，这会导致模型准确性的问题（要么欠拟合，要么过拟合）。
- en: Multicollinearity can happen even if you pay attention to your variable selection.
    For more information, I recommend referring to *Introduction to Statistical Learning*
    (Springer, 2021), section 3.3.3\.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你在变量选择上很注意，也可能出现多重共线性。更多信息，我建议参考*《统计学习引论》*（Springer，2021），第3.3.3节。
- en: 12.2.2 Removing highly correlated features
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 移除高度相关的特征
- en: In this section, we take our set of continuous variables and look at the correlation
    between them in order to improve our model accuracy and explainability. I explain
    how PySpark builds a correlation matrix and the `Vector` and `DenseMatrix` objects
    and how we can extract data from those objects for decision making.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们查看我们的连续变量集，并查看它们之间的相关性，以提高我们的模型准确性和可解释性。我解释了PySpark如何构建相关性矩阵以及`Vector`和`DenseMatrix`对象，以及我们如何从这些对象中提取数据以进行决策。
- en: 'Correlation in linear models is not always bad; as a matter of fact, you want
    your features to be correlated with your target (this provides predictive power).
    On the other hand, we want to avoid correlation between features for two main
    reasons:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型中的相关性并不总是坏事；事实上，你希望你的特征与目标相关（这提供了预测能力）。另一方面，我们想要避免特征之间的相关性，主要有两个原因：
- en: If two features are highly correlated, it means that they provide almost the
    same information. In the context of machine learning, this can confuse the fitting
    algorithm and create model or numerical instability.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个特征高度相关，这意味着它们提供了几乎相同的信息。在机器学习的背景下，这可能会使拟合算法困惑，并导致模型或数值不稳定性。
- en: The more complex your model, the more complex the maintenance. Highly correlated
    features rarely provide improved accuracy, yet complicate the model. Simple is
    better.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的模型越复杂，维护起来就越复杂。高度相关的特征很少能提供改进的准确性，反而会使模型复杂化。简单才是更好的选择。
- en: For computing correlation between variables, PySpark provides the `Correlation`
    object. `Correlation` has a single method, `corr`, that computes the correlation
    between features in a `Vector`. Vectors are like PySpark arrays but with a special
    representation optimized for ML work (see chapter 13 for a more detailed introduction).
    In listing 12.17, I use the `VectorAssembler` transformer on the `food` data frame
    to create a new column, `continuous_features`, that contains a `Vector` of all
    our continuous features.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算变量之间的相关性，PySpark提供了`Correlation`对象。`Correlation`有一个单一的方法，`corr`，它计算`Vector`中特征之间的相关性。向量类似于PySpark数组，但具有针对ML工作优化的特殊表示（有关更详细的介绍，请参阅第13章）。在列表12.17中，我使用`VectorAssembler`变换器对`food`数据帧进行操作，创建了一个包含所有连续特征的`Vector`的新列，`continuous_features`。
- en: A transformer is a preconfigured object that, as its name indicates, transforms
    a data frame. Independently, it looks like unnecessary complexity, but it shines
    when applied within a pipeline. I cover transformers in greater detail in chapter
    13.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是一个预配置的对象，正如其名称所示，它转换数据帧。独立来看，它可能看起来是多余的复杂性，但将其应用于管道中时则大放异彩。我在第13章中更详细地介绍了变换器。
- en: Listing 12.17 Assembling feature columns into a single `Vector` column
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.17 将特征列组装成一个单独的`Vector`列
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Vector columns cannot have null values (and they don’t make sense when computing
    correlation), so we remove them through a series of where() (see chapter 4).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向量列不能有null值（在计算相关性时它们也没有意义），因此我们通过一系列的where()函数将其移除（参见第4章）。
- en: Note Correlation will not work well if you blend categorical and/or binary features
    together. Choosing the appropriate dependency measure depends on your model, your
    interpretability needs, and your data. See, for instance, the Jaccard distance
    measure for noncontinuous data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果将分类和/或二元特征混合在一起，相关性可能不会很好地工作。选择适当的依赖度量取决于你的模型、你的可解释性需求以及你的数据。例如，对于非连续数据，可以参考Jaccard距离度量。
- en: 'In listing 12.18, I apply the `Correlation.corr()` function on my continuous
    feature vector and export the correlation matrix into an easily interpretable
    pandas DataFrame. PySpark returns the correlation matrix in a `DenseMatrix` column
    type, which is like a two-dimensional vector. In order to extract the values in
    an easy-to-read format, we have to do a little method juggling:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表12.18中，我对我连续特征向量应用了`Correlation.corr()`函数，并将相关性矩阵导出为易于解释的pandas DataFrame。PySpark以`DenseMatrix`列类型返回相关性矩阵，类似于二维向量。为了以易于阅读的格式提取值，我们必须进行一些方法上的调整：
- en: We extract a single record as a list of `Row` using `head()`.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`head()`函数提取一个记录作为`Row`列表。
- en: A `Row` is like an ordered dictionary, so we can access the first (and only)
    field containing our correlation matrix using list slicing.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Row` 类似于有序字典，因此我们可以使用列表切片来访问包含我们的相关性矩阵的第一个（也是唯一一个）字段。'
- en: A `DenseMatrix` can be converted into a pandas-compatible array by using the
    `toArray()` method on the matrix.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过在矩阵上使用 `toArray()` 方法将 `DenseMatrix` 转换为与 pandas 兼容的数组。
- en: We can directly create a pandas DataFrame from our Numpy array. Inputting our
    column names as an index (in this case, they’ll play the role of “row names”)
    makes our correlation matrix very readable.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以直接从我们的 Numpy 数组创建一个 pandas DataFrame。将我们的列名作为索引输入（在这种情况下，它们将扮演“行名”的角色），这使得我们的相关性矩阵非常易于阅读。
- en: Listing 12.18 Creating a correlation matrix in PySpark
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.18 在 PySpark 中创建相关性矩阵
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The corr method takes a data frame and a Vector column reference as a parameter
    and generates a single-row, single-column data frame containing the correlation
    matrix.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `corr` 方法接受一个数据框和一个 Vector 列引用作为参数，并生成一个包含相关性矩阵的单行单列数据框。
- en: ❷ The resulting DenseMatrix (shown as matrix in the schema) is not easily accessible
    by itself.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 结果的 DenseMatrix（在方案中显示为矩阵）本身并不容易访问。
- en: ❸ Since the data frame is small enough to be brought locally, we extract the
    first record with head() and the first column via an index slice, and we export
    the matrix as a NumPy array via toArray().
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于数据框足够小，可以本地传输，我们使用 `head()` 提取第一条记录，并通过索引切片提取第一条列，然后通过 `toArray()` 将矩阵导出为
    NumPy 数组。
- en: ❹ The easiest way to interpret the correlation matrix is to create a pandas
    DataFrame. We can pass out column names as both index and columns for easy interpretability.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 解释相关性矩阵的最简单方法是通过创建一个 pandas DataFrame。我们可以将列名作为索引和列传递，以便于解释。
- en: ❺ The correlation matrix gives the correlation between each field of the vector.
    The diagonal is always 1.0 because each variable is perfectly correlated with
    itself.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 相关性矩阵给出了向量中每个字段之间的相关性。对角线始终为 1.0，因为每个变量与其自身完全相关。
- en: 'When working with summary measures, such as the correlation of hypothesis tests,
    PySpark will often delegate the extraction of values to a simple NumPy or pandas
    conversion. Instead of remembering a different series of method juggling for each
    scenario, I use the REPL and the inline documentation:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理总结度量，如假设检验的相关性时，PySpark 通常会将值的提取委托给简单的 NumPy 或 pandas 转换。而不是记住每个场景的不同方法组合，我使用
    REPL 和内联文档：
- en: 'Look at the schema of your data frame and the documentation of the method/
    function used: `matrix` or `vector`? They are NumPy arrays in disguise.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看你的数据框的方案和使用的函数/方法的文档：`matrix` 或 `vector`？它们都是伪装成 NumPy 数组的。
- en: Since your data frame will always fit in memory, bring the desired records and
    extract the structures using `head()`, `take()`, and the methods available on
    `Row` objects.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于你的数据框总是会适应内存，你可以使用 `head()`、`take()` 以及 `Row` 对象上可用的方法来提取所需的记录和结构。
- en: Finally, wrap your data in a pandas DataFrame, a list, or your structure of
    choice.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将你的数据包裹在一个 pandas DataFrame、列表或你选择的结构中。
- en: Once again, our `CONTINUOUS_COLUMNS` variable avoided a ton of typing and potential
    errors, which helps in keeping track of our features when manipulating our data
    frame.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们的 `CONTINUOUS_COLUMNS` 变量避免了大量的输入和潜在的错误，这有助于在操作数据框时跟踪我们的特征。
- en: 'The last step from our correlation computation is to assess which variables
    we want to keep and which we want to drop. There is no absolute threshold for
    keeping or removing correlated variables (nor is there a protocol for which variable
    to keep). From the correlation matrix in listing 12.18, we see high correlation
    between sodium, calories, protein, and fat. Surprisingly, we see little correlation
    between our custom features and the columns that contributed to their creation.
    In my lab notes, I’d collect the following action items:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的相关性计算的最后一步是评估我们想要保留哪些变量以及我们想要删除哪些变量。没有绝对的阈值来保留或删除相关性变量（也没有保留变量的协议）。从列表 12.18
    的相关性矩阵中，我们看到钠、卡路里、蛋白质和脂肪之间存在高度相关性。令人惊讶的是，我们看到我们的自定义特征与它们的创建贡献列之间的相关性很小。在我的实验笔记中，我会收集以下行动项目：
- en: Explore the relationship between calorie count and the ratio of macro nutriments
    (and sodium). Is there a pattern there, or is the calorie count (or size of portions)
    just all over the place?
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索卡路里计数与宏量营养素（和钠）比率的关联。那里是否有模式，或者卡路里计数（或份量的大小）只是到处都是？
- en: Is the calorie/protein/fat/sodium content related to the “dessert-ness” of the
    recipes? I can’t imagine a dessert being very salty.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡路里/蛋白质/脂肪/钠含量与食谱的“甜度”有关吗？我想象不出一个甜点会非常咸。
- en: Run the model with all features, then with calories and protein removed. What
    is the impact on performance?
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行包含所有特征的模型，然后移除卡路里和蛋白质，这对性能有何影响？
- en: 'The correlation analysis raised more questions than it answered. This is a
    very good thing: data quality is a complicated thing to assess. By keeping track
    of elements to explore in greater detail, we can move quickly to modeling and
    have a (crude) benchmark to anchor our next modeling cycles. We refactor our Python
    programs, and ML models are no exception!'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性分析提出的问题比回答的问题多。这是一件好事：数据质量是一个复杂的问题。通过跟踪需要更详细探索的元素，我们可以快速进入建模阶段，并有一个（粗略的）基准来锚定我们的下一个建模周期。我们重构了我们的Python程序，机器学习模型也不例外！
- en: In this section, I covered how PySpark computes the correlation between variables
    and provides the results in a matrix. We saw the `Vector` and `Matrix` objects
    and how we can extract values from them. Finally, we assessed the correlation
    between our continuous variables and made a decision about their inclusion in
    our first model. In the next section, we get into preparing our features for machine
    learning with the usage of transformers and estimators.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我介绍了PySpark如何计算变量之间的相关性，并以矩阵形式提供结果。我们看到了`Vector`和`Matrix`对象，以及如何从它们中提取值。最后，我们评估了连续变量之间的相关性，并决定是否将它们包含在我们的第一个模型中。在下一节中，我们将探讨使用转换器和估计器准备机器学习特征。
- en: 12.3 Feature preparation with transformers and estimators
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 使用转换器和估计器进行特征准备
- en: 'This section provides an overview of transformers and estimators in the context
    of feature preparation. We use transformers and estimators as an abstraction over
    common operations in machine learning modeling. We explore two relevant examples
    of transformers and estimators:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了在特征准备方面的转换器和估计器的概念。我们将转换器和估计器用作机器学习建模中常见操作的抽象。我们探讨了两个相关的转换器和估计器示例：
- en: Null imputation, where we provide a value to replace `null` occurrences in a
    column (e.g., the mean)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空值填充，即我们提供一个值来替换列中的`null`出现（例如，平均值）
- en: Scaling features, where we normalize the values of a column, so they are on
    a more logical scale (e.g., between zero and one)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征缩放，即我们规范化列的值，使它们处于更合理的尺度上（例如，在零和一之间）
- en: Transformers and estimators are powerful on their own but are especially relevant
    in the context of ML pipelines, introduced in chapter 13.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器和估计器本身就很强大，但在第13章中介绍的ML管道的上下文中尤其相关。
- en: We saw an example of a transformer in section 12.2.2 through the `VectorAssembler`
    object. The best way to think about a transformer is by translating its behavior
    into a function. In figure 12.2, I compare a `VectorAssembler` to a function `assemble_
    vector()` that performs the same work, which is to create a `Vector` named after
    the argument to `outputCol`, which contains all the values in the columns passed
    to `inputCols`. Don’t focus on the actual work here, but more on the mechanism
    of application.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在12.2.2节中，我们通过`VectorAssembler`对象看到了转换器的一个示例。思考转换器的最佳方式是将它的行为转换为函数。在图12.2中，我将`VectorAssembler`与执行相同工作的函数`assemble_vector()`进行比较，该函数创建一个以`outputCol`参数命名的`Vector`，其中包含传递给`inputCols`的所有列的值。在这里，不要关注实际的工作，而更多地关注应用机制。
- en: '![](../Images/12-02.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-02.png)'
- en: Figure 12.2 Comparing a function applied on a data frame to a transformer. The
    transformer object separates parameterization (at instantiation) and application
    on the data (via the `transform()` method).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 比较对数据框应用函数与转换器。转换器对象将参数化（在实例化时）与应用数据（通过`transform()`方法）分离。
- en: The function is applied to the data frame, along with the parameterization,
    and returns a transformed data frame.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 函数应用于数据框，并带有参数化，然后返回一个转换后的数据框。
- en: The transformer object has a two-staged process. First, when instantiating the
    transformer, we provide the parameters necessary for its application, but *not*
    the data frame on which it’ll be applied. This echoes the separation of data and
    instructions we saw in chapter 1\. Then, we use the instantiated transformer’s
    `transform()` method on the data frame to get a transformed data frame.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器对象有一个两阶段的过程。首先，在实例化转换器时，我们提供其应用所需的参数，但不是它将应用到的数据框。这呼应了我们在第1章中看到的数据和指令的分离。然后，我们使用实例化转换器的`transform()`方法对数据框进行操作，以获取转换后的数据框。
- en: Tip We could reproduce that two-staged process using the `transform()` method
    and a transform-enabled function. See appendix C for an introduction to the topic.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：我们可以使用`transform()`方法和一个启用转换的功能来重现这个两阶段过程。有关该主题的介绍，请参阅附录C。
- en: This separation of instructions and data is key in creating serializable ML
    pipelines, which leads to easier ML experiments and model portability. Chapter
    13 covers this topic in greater detail when we build our model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这种指令和数据分离对于创建可序列化的ML管道至关重要，这导致ML实验更容易进行，模型可移植性更高。在第13章中，当我们构建模型时，将更详细地介绍这个主题。
- en: Estimators are like a transformers factory. The next two sections introduce
    estimators through the usage of `Imputer` and `MinMaxScaler`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器就像是一个转换器工厂。接下来的两个部分通过使用`Imputer`和`MinMaxScaler`介绍估计器。
- en: 12.3.1 Imputing continuous features using the Imputer estimator
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 使用`Imputer`估计值估算连续特征
- en: In this section, we cover the `Imputer` estimator and introduce the concept
    of an estimator. Estimators are the main abstraction used by Spark for any data-dependent
    transformation, including ML models, so they are pervasive in any ML code using
    PySpark.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍`Imputer`估计器和估计器的概念。估计器是Spark用于任何数据相关转换的主要抽象，包括ML模型，因此在任何使用PySpark的ML代码中都很普遍。
- en: At the core, an estimator is a transformer-generating object. We instantiate
    an estimator object just like a transformer by providing the relevant parameters
    to its constructor. To apply an estimator, we call the `fit()` method, which returns
    a `Model` object, which is, for all purposes, the same as a transformer. Estimators
    allow for automatically creating transformers that depend on the data. As a perfect
    example, the `Imputer` and its companion model, `ImputerModel`, are depicted in
    figure 12.3.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，估计器是一个生成转换器的对象。我们通过向其构造函数提供相关参数来实例化估计器对象，就像实例化转换器一样。要应用估计器，我们调用`fit()`方法，它返回一个`Model`对象，在所有目的上，它与转换器相同。估计器允许自动创建依赖于数据的转换器。作为一个完美的例子，`Imputer`及其伴随模型`ImputerModel`在图12.3中有所展示。
- en: '![](../Images/12-03.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-03.png)'
- en: Figure 12.3 The `Imputer` and its companion transformer/model, `ImputerModel`.
    When `fit()` is called on an instantiated estimator, a fully parameterized transformer
    (called Model) is created.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 `Imputer`及其伴随的转换器/模型`ImputerModel`。当对一个实例化的估计器调用`fit()`时，会创建一个完全参数化的转换器（称为模型）。
- en: As an example, we want our `Imputer` to impute the mean value to every record
    in the `calories`, `protein`, `fat`, and `sodium` columns when the record is `null`.
    We would parameterize the `Imputer` object like in the next listing, providing
    the relevant parameters seen in figure 12.3 (the `missingValue` and `relativeError`
    are okay with their default parameters).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们希望当记录为`null`时，我们的`Imputer`将平均值估算到`calories`、`protein`、`fat`和`sodium`列中的每一条记录。我们将在下一个列表中参数化`Imputer`对象，提供图12.3中显示的相关参数（`missingValue`和`relativeError`使用默认参数即可）。
- en: Listing 12.19 Instantiating and applying the `Imputer` to create an `ImputerModel`
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.19 实例化并应用`Imputer`以创建`ImputerModel`
- en: '[PRE18]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ We use the mean of each column to fill the null values.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用每列的平均值来填充空值。
- en: ❷ Since we have four columns to impute, we pass their name to inputCols.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于我们有四个列需要估算，我们将它们的名称传递给inputCols。
- en: ❸ I give an _i suffix to every variable to give me a visual cue about how they
    were created.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我给每个变量添加一个_i后缀，以便给我一个关于它们是如何创建的视觉提示。
- en: ❹ The ImputerModel object is created as a result of the fit() method being called
    with the food data frame.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ `ImputerModel`对象是在使用食物数据框调用fit()方法后创建的。
- en: ❺ I adjust the CONTINUOUS_COLUMNS variable to account for the new column names.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我调整`CONTINUOUS_COLUMNS`变量以考虑新的列名。
- en: Warning If a transformer or an estimator has `inputCol/outputCol` *and* `inputCols/outputCols`
    as parameters, it means they can be applied on either one or many columns. You
    can only choose one of those options. When in doubt, review the signature and
    the documentation for the object.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果一个转换器或估计器有`inputCol/outputCol`和`inputCols/outputCols`作为参数，这意味着它们可以应用于一个或多个列。您只能选择这些选项中的一个。如有疑问，请查看对象的签名和文档。
- en: We apply the resulting `ImputerModel` just like with any transformer by using
    the `transform()` method. In the next listing, we see that `calories` is correctly
    imputed to approximately 475.52 calories. Neat!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像使用任何转换器一样应用结果`ImputerModel`，通过使用`transform()`方法。在下一个列表中，我们看到`calories`被正确地估算为大约475.52卡路里。真不错！
- en: Listing 12.20 Using the `ImputerModel` object just like a transformer
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.20 像使用转换器一样使用`ImputerModel`对象
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '❶ Our Imputer works as expected: null calories means 475.52 calories_i.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的Imputer按预期工作：空卡路里意味着475.52卡路里_i。
- en: Estimators and transformers will make a comeback in chapter 13, as they are
    the building blocks of ML pipelines. In this section, we reviewed the estimator
    and its relationship to the transformer through the `Imputer`. In the next section,
    we see another example through the `MinMaxScaler`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器和转换器将在第13章中回归，因为它们是ML管道的构建块。在本节中，我们通过`Imputer`回顾了估计器及其与转换器的关系。在下一节中，我们将通过`MinMaxScaler`看到另一个例子。
- en: 'Model or model: It depends on the angle you see it from'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 模型或模型：这取决于你看它的角度
- en: Although we are covering this topic in greater detail in chapter 13, there is
    a clash of words between model (like a machine learning model) and `Model`, the
    output of a fitted estimator.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在第13章中更详细地讨论了这个主题，但模型（如机器学习模型）和`Model`（拟合估计器的输出）之间存在词语冲突。
- en: In Spark’s world, an estimator is a data-dependent construction that, upon calling
    the `fit()` method, yields a `Model` object. This means that any ML model (lowercase
    “m”) available in PySpark is an estimator that we `fit()` on the data to get a
    trained `Model` object.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的世界里，估计器是一个数据相关的构造，在调用`fit()`方法时，会产生一个`Model`对象。这意味着PySpark中可用的任何ML模型（小写“m”）都是一个估计器，我们通过在数据上`fit()`来获取训练好的`Model`对象。
- en: On the flip side, although our `ImputerModel` is a `Model` object (capital “M”),
    it is not really an ML model. For PySpark, we used the `fit()` method with some
    data to generate a `Model` object, so the definition stands as far as Spark is
    concerned.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，尽管我们的`ImputerModel`是一个`Model`对象（大写“M”），但它实际上不是一个ML模型。对于PySpark，我们使用`fit()`方法和一些数据生成一个`Model`对象，所以从Spark的角度来看，这个定义是成立的。
- en: 12.3.2 Scaling our features using the MinMaxScaler estimator
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.2 使用MinMaxScaler估计器缩放我们的特征
- en: This section covers variable scaling using the `MinMaxScaler` transformer. Scaling
    variables means performing a mathematical transformation on the variables so that
    they are all on the same numeric scale. When using a linear model, having scaled
    features means that your model coefficients (the weight of each feature) are comparable.
    This provides a tremendous boost to the interpretability of the model, a useful
    asset to have when assessing model performance. For some types of models, such
    as neural nets, scaled features also help with model performance.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了使用`MinMaxScaler`转换器进行变量缩放。缩放变量意味着对变量执行数学变换，使它们都在相同的数值尺度上。当使用线性模型时，缩放特征意味着你的模型系数（每个特征的权重）是可比较的。这极大地提高了模型的解释性，在评估模型性能时是一个有用的资产。对于某些类型的模型，如神经网络，缩放特征也有助于提高模型性能。
- en: As an example, if you have a variable that contains a number between 0.0 and
    1.0 and another that contains a number between 1,000.0 and 2,500.0, if they both
    have the same model coefficient (let’s say 0.48), you might be tempted to think
    that both are equally important. In reality, the second variable is much more
    important because 0.48 times something in the thousands is much higher than 0.48
    times something between zero and one.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一个变量包含介于0.0和1.0之间的数字，另一个变量包含介于1,000.0和2,500.0之间的数字，如果它们都有相同的模型系数（比如说0.48），你可能会想认为它们同样重要。实际上，第二个变量要重要得多，因为0.48乘以几千比0.48乘以零到一之间的任何数字都要高得多。
- en: 'To choose the right scaling algorithm, we need to look at our variables as
    a whole. Since we have so many binary variables, it is convenient to have every
    variable be between zero and one. Our `protein_ratio` and `fat_ratio` are ratios
    between zero and one too! PySpark provides the `MinMaxScaler` for this use case:
    for each value in the input column, it creates a normalized output between `0.0`
    and `1.0` (further options for the `MinMaxScaler` are introduced in chapter 13).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择正确的缩放算法，我们需要整体考虑我们的变量。由于我们有很多二元变量，让每个变量都在零到一之间是很方便的。我们的`protein_ratio`和`fat_ratio`也是介于零到一之间的比率！PySpark为此用例提供了`MinMaxScaler`：对于输入列中的每个值，它创建一个介于`0.0`和`1.0`之间的归一化输出（`MinMaxScaler`的更多选项在第13章中介绍）。
- en: '![](../Images/12-04.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-04.png)'
- en: Figure 12.4 The `MinMaxScaler` and its companion model, the `MinMaxScalerModel`—the
    same model of operation as the `Imputer` estimator
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 `MinMaxScaler`及其伴随模型`MinMaxScalerModel`——与`Imputer`估计器相同的操作模型
- en: In listing 12.21, we create a `MinMaxScaler` estimator and provide the vector
    column input and output columns as strings. This follows the same steps as the
    `Imputer/ ImputerModel`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 12.21 中，我们创建了一个 `MinMaxScaler` 估算器，并将向量列输入和输出列作为字符串提供。这遵循了与 `Imputer/ ImputerModel`
    相同的步骤。
- en: Listing 12.21 Scaling our nonscaled continuous variables
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.21 缩放未缩放的连续变量
- en: '[PRE20]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tip For your own models, check the `pyspark.ml.feature` module for other scalers.
    `StandardScaler`, which normalizes your variables by subtracting the mean and
    then dividing by the standard deviation, is also a favorite among data scientists.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：对于你自己的模型，检查 `pyspark.ml.feature` 模块中的其他缩放器。`StandardScaler` 通过减去平均值然后除以标准差来规范化你的变量，也是数据科学家中的宠儿。
- en: All the variables in our `continuous_scaled` vector are now between zero and
    one. Our continuous variables are ready; our binary variables are ready. I think
    we are now ready to assemble our data set for machine learning! In this section,
    we reviewed the `MinMaxScaler` estimator and how we can scale variables so that
    they have the same amplitude.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 `continuous_scaled` 向量中的所有变量现在都在零和一之间。我们的连续变量准备好了；我们的二进制变量也准备好了。我认为我们现在可以组装我们的数据集以进行机器学习了！在本节中，我们回顾了
    `MinMaxScaler` 估算器以及我们如何缩放变量以使它们具有相同的幅度。
- en: This concludes the data preparation chapter. From a raw data set, we tamed the
    column names; explored the data; set a target; encoded columns into binary and
    continuous features; created bespoke features; and selected, imputed, and scaled
    some features. Is the work done? Not by a long shot!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了数据准备章节。从原始数据集开始，我们驯化了列名；探索了数据；设定了目标；将列编码为二进制和连续特征；创建了定制特征；并选择、插补和缩放了一些特征。工作完成了吗？远远没有！
- en: In the next chapter, we’ll finally get to modeling. At the same time, we will
    revisit our program through the ML pipeline abstraction, which will provide the
    necessary flexibility to put the science in data science and experiment with multiple
    scenarios. We set the stage, now let’s model!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们终于要进入建模阶段了。同时，我们将通过 ML 管道抽象重新审视我们的程序，这将提供必要的灵活性，将科学融入数据科学，并实验多个场景。我们已经搭建了舞台，现在让我们开始建模吧！
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A big part of creating a machine learning model is data manipulation. For this,
    everything we’ve learned within `pyspark.sql` can be leveraged.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建机器学习模型的一个重要部分是数据操作。为此，我们可以利用在 `pyspark.sql` 中学到的所有知识。
- en: The first step in creating an ML model is assessing data quality and addressing
    potential data problems. Moving from large data sets in PySpark to small summaries
    in pandas or plain Python speeds up data discovery and assessment.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建机器学习模型的第一步是评估数据质量并解决潜在的数据问题。将 PySpark 中的大数据集转换为 pandas 或纯 Python 中的小摘要可以加快数据发现和评估的速度。
- en: Feature creation and selection can either be done manually using the PySpark
    data manipulation API or by leveraging some `pyspark.ml`-specific constructors,
    such as the correlation matrix.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征创建和选择可以通过使用 PySpark 数据操作 API 手动完成，或者通过利用一些 `pyspark.ml` 特定的构造函数，例如相关矩阵。
- en: Multiple common feature engineering patterns, like imputing and scaling features,
    are provided through PySpark transformers and estimators.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 PySpark 的转换器和估算器提供了多种常见的特征工程模式，如特征插补和缩放。
- en: '* * *'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ You would probably not be using PySpark if this were the case.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 如果是这样，你可能不会使用 PySpark。
