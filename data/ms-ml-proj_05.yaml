- en: 5 Diving into the problem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 深入问题
- en: 'This chapter covers:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖：
- en: Getting and verifying access to the data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取和验证数据访问权限
- en: Revisiting, verifying, and refining business understanding
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾、验证和细化业务理解
- en: Developing UX and model utilization concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发用户体验和模型利用概念
- en: Getting the versioning and pipelining system in place and working
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立和运行版本控制和管道系统
- en: Building the initial pipelines to deliver a data set to the team
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立初始管道以向团队交付数据集
- en: Starting to build data tests to make your pipelines robust
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始构建数据测试以使你的管道健壮
- en: In sprint 1, the team puts in place and starts using the infrastructure to support
    the delivery project, then they open the data that’s going to underpin the ML
    project. In order to crack the data open, they will use the infrastructure (particularly
    the pipelines and testing systems) that they construct.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一轮冲刺中，团队建立并开始使用支持交付项目的基础设施，然后他们打开将支撑机器学习项目的数据。为了打开数据，他们将使用他们构建的基础设施（特别是管道和测试系统）。
- en: 5.1 Sprint 1 backlog
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 第一轮冲刺待办事项
- en: The sprint 1 backlog provides tasks that are described in this chapter (S1.1
    - S1.4) and in chapter 6 (S1.5 - S1.7). With sprint 1, you prepare for the core
    ML activity of creating and evaluating useful models using ML algorithms. The
    work is to dig deeper into the data resources and develop the team’s expertise
    and capability to use them for modeling. You also need to build the supporting
    infrastructure that lifts and shifts the data from where it’s resting to where
    you need it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮冲刺的待办事项包括本章（S1.1 - S1.4）和第6章（S1.5 - S1.7）中描述的任务。通过第一轮冲刺，你将准备核心机器学习活动，即使用机器学习算法创建和评估有用的模型。这项工作是要深入挖掘数据资源，并发展团队使用这些资源进行建模的专业技能和能力。你还需要建立支持性基础设施，将数据从其静止状态转移到你需要的地方。
- en: Table 5.1 Backlog for sprint 1
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 第一轮冲刺待办事项
- en: '| Task # | Item |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 任务编号 | 项目 |'
- en: '| --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| S1.1 | Undertake a data survey; scan and inspect appropriate tables for completeness,
    coverage, and quality.Conduct data tests to address bias, poisoning, quality,
    coverage, and accuracy (of labeling).Write up and review the data survey. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| S1.1 | 执行数据调查；扫描和检查适当的表以检查完整性、覆盖范围和质量。进行数据测试以解决偏差、中毒、质量、覆盖范围和标签准确性问题。编写并审查数据调查。|'
- en: '| S1.2 | Develop a business application description.Develop the application
    user story backlog (S2.1, S3.1).Validate the application description and user
    story backlog with users.Identify and validate the ML model performance requirements.Create
    UX designs. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| S1.2 | 制定业务应用描述。开发应用程序用户故事待办事项（S2.1，S3.1）。与用户验证应用程序描述和用户故事待办事项。识别并验证机器学习模型性能要求。创建用户体验设计。|'
- en: '| S1.3 | Aggregate and fuse relevant data into an integrated picture.Implement
    and manage data pipelines.Design and implement data tests. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| S1.3 | 将相关数据聚合和融合成一个综合图。实施和管理数据管道。设计和实施数据测试。|'
- en: '| S1.4 | Commission and adopt a model repository.Identify and record all artefacts
    for use in ML pipelines. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| S1.4 | 委派并采用模型存储库。识别并记录所有用于机器学习管道的工件。|'
- en: '| S1.5 | Plan and design EDA.Write and share the EDA report. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| S1.5 | 规划和设计数据探索分析（EDA）。编写并分享EDA报告。|'
- en: '| S1.6 | Check ethics in light of the emerging understanding. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| S1.6 | 根据新的理解检查伦理问题。|'
- en: '| S1.7 | Define and implement the model baselines. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| S1.7 | 定义并实施模型基线。|'
- en: The first step to delivering this sprint is to deepen your (and the team’s)
    understanding of the data. We’ll cover that in the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个冲刺的第一步是加深你对数据（以及团队）的理解。我们将在下一节中介绍这一点。
- en: 5.2 Understanding the data
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 理解数据
- en: Your team is on board and funding is in place to work on the client’s problem.
    In sprint 0, you acquired an overview of the data resources for the job. Now,
    the task at hand is to do a rapid but systematic evaluation of the available data
    resources.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你的团队已经准备好，资金到位，可以开始解决客户的问题。在冲刺0中，你获得了这项工作所需的数据资源的概述。现在，手头的任务是进行快速但系统的现有数据资源评估。
- en: In this book, this task is called a *data survey*, but it could be called an
    inspection or an overview. It’s a fast, structured investigation, which produces
    results that you can document, discuss, and share to build understanding and insight.
    Most importantly, this creates a checkpoint for obvious problems with the data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，这项任务被称为*数据调查*，但也可以称为检查或概述。这是一项快速、结构化的调查，可以产生你可以记录、讨论和分享的结果，以建立理解和洞察。最重要的是，这为数据中的明显问题创建了一个检查点。
- en: 'Data survey ticket: S1.1'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据调查票据：S1.1
- en: Undertake a data survey; scan and inspect the appropriate tables for completeness,
    coverage, and quality.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行数据调查；扫描和检查适当的表以检查完整性、覆盖率和质量。
- en: Conduct data tests to address bias, poisoning, quality, coverage, and accuracy
    (of labeling).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行数据测试以解决偏差、中毒、质量、覆盖率和标签的准确性问题。
- en: Write up and review the data survey.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写并审查数据调查。
- en: Because you built a narrative about the data in sprint 0, you have a map for
    the team to work with, even if it’s labeled “there be dragons!” But, as they say,
    the map is not the territory. Now, you need to use your data story in combination
    with your carefully assembled team of data ninjas to find more dragons.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你在冲刺0阶段构建了关于数据的故事，所以团队有一个可以工作的地图，即使它被标记为“这里可能有龙！”但，正如他们所说，地图不是领土。现在，你需要结合你的数据故事和精心组建的数据忍者团队来寻找更多的龙。
- en: It may be that you don’t yet have the team, access, or infrastructure to start
    work on the core problems such as handling the data, articulating an understanding
    of the problem or creating the infrastructure to support work to solve these issues.
    If you lack required permissions and access, you are in trouble; the team can’t
    work and can’t progress. There may be some scope to work on the UX, but in reality,
    the team can do little until this is sorted out.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可能你还没有组建团队、获取访问权限或建立基础设施来开始处理核心问题，比如处理数据、阐述对问题的理解或创建支持解决这些问题的基础设施。如果你缺乏必要的权限和访问权限，你将陷入困境；团队无法工作，也无法进步。可能有一些范围可以工作在用户体验（UX）上，但现实中，团队在这些问题解决之前几乎无法做出什么贡献。
- en: 5.2.1 The data survey
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 数据调查
- en: There are three steps to the data investigation that this book recommends. We
    did the first step, the data story, back in chapter 4\. The data story elicits
    and records the recommendations and information about the data resources that
    are available. Now, we need to do the data survey, which validates the data story
    and provides more information about the nonfunctional and system properties of
    the data assets. Later, when we have the data in the right place, the exploratory
    data analysis (EDA) looks at the statistical properties of the data. With the
    EDA, we find out what can really be done with the data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书推荐的数据调查有三个步骤。我们已经在第4章中完成了第一步，即数据故事。数据故事会激发并记录关于可用数据资源的建议和信息。现在，我们需要进行数据调查，这验证了数据故事，并提供了更多关于数据资产的非功能性和系统属性的信息。稍后，当我们把数据放在正确的位置时，探索性数据分析（EDA）将研究数据的统计特性。通过EDA，我们可以找出数据真正能做什么。
- en: The purpose of the data survey is to reduce uncertainty about the contents of
    the data resources. The data story will be full of assumptions and assertions
    about what is in the data sources and where it comes from. Now, we need to check
    that the assumptions made in the data story are plausible.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据调查的目的是减少对数据资源内容的疑虑。数据故事将充满关于数据源中包含的内容及其来源的假设和断言。现在，我们需要检查数据故事中做出的假设是否合理。
- en: What are the drivers for a structured and systematic investigation? First, it’s
    relatively cheap in terms of time and effort. The queries that need to be written
    for this are easy enough for any data engineer, and the queries should all run
    fast. This may not be true for the later EDA exercise and almost certainly won’t
    be true in the modeling phase. The engineering required for that phase may take
    a lot of thought, and the queries may take a while to run, test, and debug. The
    relatively low effort spent on the data survey now avoids expensive mistakes later.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化和系统化调查的驱动因素是什么？首先，从时间和努力的角度来看，它相对便宜。为此需要编写的查询对任何数据工程师来说都足够简单，并且所有查询都应该运行得很快。这可能不适用于后续的EDA练习，几乎肯定不适用于建模阶段。那个阶段的工程可能需要很多思考，查询可能需要一段时间才能运行、测试和调试。现在在数据调查上投入的相对较低的努力可以避免以后的昂贵错误。
- en: The second nice feature of the survey is that because it’s simple, quick, and
    cheap, it can be comprehensive. In contrast, an EDA can uncover a huge number
    of avenues for investigation in an interesting data set. The team won’t get through
    them all; in fact, they will probably only have time to explore a relatively small
    fraction of the data set properly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 调查的第二个优点是，因为它简单、快捷且成本低，所以可以全面进行。相比之下，EDA可以在有趣的数据集中揭示大量调查途径。团队可能无法全部完成；事实上，他们可能只有时间对数据集的相对较小部分进行适当的探索。
- en: 'A narrow investigation means that potential show-stopping issues can lurk undetected.
    suddenly appearing at the end of the project, derailing everything, and embarrassing
    everyone. A broad but shallow survey lets you and the team make rational decisions
    about where to focus deep and time-consuming EDA effort. As for the survey itself,
    the following checks on the data are a good way to start:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 狭义的调查意味着潜在的致命问题可能潜伏未被发现，可能在项目结束时突然出现，破坏一切，并让每个人都感到尴尬。广泛的但浅显的调查让您和团队能够就集中精力进行深入且耗时的EDA工作做出合理的决定。至于调查本身，以下对数据的检查是一个良好的开始：
- en: Can all component elements described in the data story be identified and located
    in the customer’s system? If in the worst case, you don’t have access to the system,
    can a technical support person or a data catalog be used to provide a proxy for
    determining that the data is there?
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否可以在客户的系统中识别和定位数据故事中描述的所有组件元素？如果最坏的情况下无法访问系统，是否可以使用技术支持人员或数据目录来提供确定数据存在的代理？
- en: Can you get a count of records for today or last week or last period, and can
    you get a file size in bytes (Tb, Gb, Mb, Kb) for the data assets? The size and
    structure of each data resource identified should correspond to the expected values,
    given the data life history. Sometimes, the table scans to determine if all the
    required records are present can’t be done on the operational infrastructure.
    Smaller queries, however, are often possible without disrupting the data system.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能否获取今天或上周或上一个时期的记录数，以及能否获取数据资产的字节大小（Tb、Gb、Mb、Kb）？每个识别出的数据资源的尺寸和结构都应与数据生命周期相符的预期值相匹配。有时，在运营基础设施上无法执行表扫描以确定所有必需的记录是否存在。然而，较小的查询通常可以在不破坏数据系统的情况下进行。
- en: Are the oldest and newest records in the data as expected, given the customer’s
    description?
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据客户的描述，数据中最老和最新的记录是否如预期的那样？
- en: What are the largest and smallest values in the key columns, the basic aggregates
    (mean, median), and the range of data?
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键列中的最大和最小值，基本聚合（平均值、中位数）以及数据的范围是多少？
- en: Are there changes in scale, format, or type in the records for periods before
    and after major events in the data set’s history (migrations, replatforming, data
    quality programs, federation, and integration episodes)?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据集历史中重大事件（迁移、重新平台化、数据质量计划、联邦和集成事件）之前和之后的记录中，是否存在规模、格式或类型的更改？
- en: To explore the idea of a data survey, imagine an illustrative project to manage
    an intelligent building. One of the tables might contain temperature data from
    a bunch of sensors, and you and the team want to integrate this with data on sunlight,
    building usage, and climate to create an energy-saving control system for the
    building. If the team has access to a good data management system or a data catalog,
    then they should find that the required databases and tables are in place and
    have the expected number of records and storage sizes. Even so, it’s worthwhile
    to cross-check what the records show in case the metadata is out of sync with
    the actual data store.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索数据调查的想法，想象一个管理智能建筑的有说明性的项目。其中一个表可能包含来自大量传感器的温度数据，你和团队希望将其与阳光、建筑使用和气候数据集成，以创建建筑节能控制系统。如果团队可以访问良好的数据管理系统或数据目录，那么他们应该会发现所需的数据库和表已经就位，并且具有预期的记录数和存储大小。即便如此，交叉检查记录显示的内容也是值得的，以防元数据与实际数据存储不同步。
- en: In the next few paragraphs, we’ll see an example of how useful investigating
    what’s actually in the data tables with simple ad hoc queries can be. The investigation
    shows that the data is as reported in the DBMS and is available for queries. We’ll
    use a few snippets of SQL to illustrate, but don’t worry if you can’t read the
    code, the text explains what’s happening.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几段中，我们将看到一个例子，说明通过简单的即席查询调查数据表中实际内容是多么有用。调查表明，数据与DBMS中报告的一致，并且可用于查询。我们将使用一些SQL片段来展示，但如果您无法阅读代码，请不要担心，文本解释了正在发生的事情。
- en: 'These investigations run like a stream of consciousness as you find oddities
    and check your understanding of the data. What’s important is to make sure that
    these four points are probed: it’s all there, it’s the right size, the records
    look like you expect them to, and any major issues are uncovered. There are three
    types of data in the resources that you will need to use:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些调查就像一股意识流，当你发现异常并检查你对数据的理解时。重要的是要确保这四个要点被探测到：所有内容都在那里，大小合适，记录看起来像你预期的样子，并且任何重大问题都被揭露。你将需要使用以下三种类型的数据：
- en: '*Numerical fields:* Represent measured quantities such as sizes, weights, densities,
    frequencies, wavelengths, times, concentrations, or temperatures.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数值字段:* 表示测量量，如尺寸、重量、密度、频率、波长、时间、浓度或温度。'
- en: '*Categorical fields:* Represent labels that are applied to things such as colors,
    genera, species, textures, or product identifiers.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类字段:* 表示应用于颜色、属、种、纹理或产品标识符等事物的标签。'
- en: '*Unstructured fields:* Represent images, text, or sounds. Examples of unstructured
    data include product descriptions, example images, sequences or timeseries, social
    media messages, customer support emails, conversations between traders, or incident
    notes.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*非结构化字段:* 表示图像、文本或声音。非结构化数据的例子包括产品描述、示例图像、序列或时间序列、社交媒体消息、客户支持电子邮件、交易员之间的对话或事件记录。'
- en: 'The next section provides examples of how you might treat each of these fields
    in a data survey exercise. A common way of starting the survey is to get a list
    of all the data tables that exist in the different customer databases. Unfortunately,
    different databases use different tools and commands that can be used to perform
    the survey. For example, we can access the tables in an Oracle database with the
    following command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分提供了如何在数据调查练习中处理这些字段的示例。开始调查的一种常见方式是获取所有不同客户数据库中存在的所有数据表的列表。不幸的是，不同的数据库使用不同的工具和命令来执行调查。例如，我们可以使用以下命令访问Oracle数据库中的表：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This command shows that there are two tables available: temperature_readings
    and incidents. Temperature_readings contains numerical data so we will use this
    as the first table addressed in the survey.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令显示有两个可用的表：temperature_readings和incidents。Temperature_readings包含数值数据，因此我们将使用这个作为调查中第一个提到的表。
- en: 5.2.2 Surveying numerical data
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 调查数值数据
- en: For the smart building, it makes sense to start by checking that the right number
    of records are in the temperature_readings table. This is done using a `select
    count(*)` statement.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于智能建筑，首先检查temperature_readings表中是否有正确数量的记录是有意义的。这是通过使用`select count(*)`语句来完成的。
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Does the result make sense? We are looking at about five years of sensor records,
    so there will be a leap year in there. The count then is 5 * 365 + 1 = 1,826 days,
    so 262,944,000 / 1,826 = 144,000\. That’s a nice round number. If it reminds you
    of 1,440 (24 * 60), it’s almost as if 100 sensors have reported without fail every
    minute for five years of perfect 24-hour days! A bit odd if you think about it
    for a minute, but because of this, there’s approximately the right amount of data
    in the table.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是否有意义？我们正在查看大约五年的传感器记录，所以其中会有一个闰年。那么计数就是5 * 365 + 1 = 1,826天，所以262,944,000
    / 1,826 = 144,000。这是一个很好的整数。如果你想起了1,440（24 * 60），那么几乎就像100个传感器在五年中每分钟都无故障地报告，完美地过了24小时！如果你稍微思考一下，这有点奇怪，但正因为如此，表中大约有正确数量的数据。
- en: 'There’s more to check though. Let’s make sure that the records are actually
    useful. Instead of using a `select count (*)`, we’ll use a `select *` in the following
    code snippet, which means “get everything.” Of course, getting everything would
    be tedious because reading thousands of records can be painful. For this reason,
    we’ll add a `limit 1` clause to the statement, which means “start getting everything
    but then stop as soon as you have one”:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有更多需要检查的。让我们确保记录实际上是有效的。而不是使用`select count (*)`，我们将在下面的代码片段中使用`select *`，这意味着“获取一切”。当然，获取一切会非常繁琐，因为读取数千条记录可能会很痛苦。因此，我们将添加一个`limit
    1`子句到语句中，这意味着“开始获取一切，但一旦你有一个就停止”：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At least one of the records is useful. Often, rather than setting `limit 1`,
    it’s more sensible to scan a handful of the records (maybe 10 or 20) to check
    the data. We can do that by changing the `limit` number (but you knew that!).
    It also might be smart to use the `where` clause to probe for records in different
    years. Hopefully, the data schema is available and understood, but if not, then
    why not check it too and record the outcome?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有一条记录是有用的。通常，与其设置`limit 1`，不如扫描一些记录（可能是10或20条）来检查数据。我们可以通过更改`limit`数字来实现（但你知道这一点！）。也许使用`where`子句来探测不同年份的记录也是明智的。希望数据模式是可用的并且被理解，如果不是，那么为什么不检查它并记录结果呢？
- en: 'In the query results from temperature_readings, 21 is the sensor number, followed
    by the year, the month, the day, and the time of the reading: `21,2021,September,17,00:00:10`.
    In the example, it’s unknown (by anyone) what produces the *op* at the end of
    the results; this isn’t unusual in data projects. However, what you found should
    go on the project backlog as an investigation item. It might be that *op* means
    *out of parameters* and indicates that the sensor is broken. Alternatively, *op*
    might mean *operational*, an indicator that the sensor is functioning properly.
    The point is that someone needs to check this soon.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在温度读数查询结果中，21是传感器编号，后面是年份、月份、日期和读数时间：`21,2021,September,17,00:00:10`。在示例中，结果末尾的*op*是什么（任何人都不清楚）；这在数据项目中并不罕见。然而，你发现的问题应该作为调查项添加到项目待办事项中。可能*op*代表*超出参数*，表明传感器损坏。或者，*op*可能代表*操作中*，表明传感器正常工作。关键是有人需要尽快检查这一点。
- en: 'Having established that there are useful records in the database, the next
    thing to check is that there’s meaningful information in it. The dependent variable
    here is `temperature`. It’s shown as 04.3 in the previous record. A sensible check
    is to see if the dependent variables are really recorded in the data. For example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库中已经确认存在有用的记录后，接下来需要检查的是其中是否有有意义的信息。这里的因变量是`温度`。在之前的记录中显示为04.3。一个合理的检查是查看因变量是否真的记录在数据中。例如：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Hmm, that means that there are 360,959 (262944000 – 262583041)null temperature
    readings in the data set—about 0.13%. That’s not much, but it’s something that
    the team needs to know.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这意味着数据集中有360,959（262944000 – 262583041）个null温度读数——大约0.13%。这并不多，但这是团队需要知道的事情。
- en: 'Also, how many of the temperature readings are 0 or not? The temperatures recorded
    could indeed be 0.0° C, but in many data systems, (not so) smart programmers replace
    “bad” readings with 0 to make their data pipelines work smoothly. A count of 0.0
    values is worth doing:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有多少温度读数是0或者不是0？记录的温度确实可能是0.0°C，但在许多数据系统中，（不太）聪明的程序员会用0替换“坏”读数，以使他们的数据管道顺利工作。对0.0值的计数是值得做的：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Which, again, is not an unreasonable number, overall, 360959 + 1890030 = 2250989,
    so less than 0.86% of the data might be junk (however, the 0.0 values could be
    real!). Even if there are pathological concentrations of noise created by these
    missing values, their overall rarity means that it’s likely that the results of
    the modeling will not be completely invalidated. Obviously, some scenarios could
    make these problems more pressing. For example, it could be that the null values
    appear when temperatures go over a certain value only (as the sensor fails, for
    instance).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这个数字并不算不合理，总体来说，360959 + 1890030 = 2250989，所以不到0.86%的数据可能是垃圾数据（然而，0.0值可能是真实的！）。即使这些缺失值产生了病态的噪声集中，但它们的总体稀少意味着建模的结果不太可能完全无效。显然，某些场景可能会使这些问题更加紧迫。例如，当温度超过某个特定值时，可能只有null值出现（例如，传感器故障）。
- en: Depending on the significance of the table and the time available to build the
    survey, it might be that you attain your goals with such simple queries, and the
    task is complete. Perhaps the project is dependent on many tables with many dependent
    variables, but there’s nothing complex jumping out. If this is the case, then
    getting a view of the quality of the tables and databases at a high level could
    be the right allocation of effort.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表格的重要性以及构建调查所需的时间，可能你只需使用这样的简单查询就能达到目标，任务就完成了。也许项目依赖于许多具有许多因变量的表格，但并没有什么复杂的问题跳出来。如果是这种情况，那么从高层次上了解表格和数据库的质量可能是正确的努力分配。
- en: 'On the other hand, if the scale of the work is more manageable and you have
    ample time, or if there is a higher level of caution in the data story about the
    data quality, then delving deeper would be smart. In this case, you might contemplate
    doing the following (based on the smart building scenario):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果工作的规模更易于管理，你有足够的时间，或者如果数据质量的故事中有关数据质量的谨慎程度更高，那么深入挖掘会更明智。在这种情况下，你可能考虑以下行动（基于智能建筑场景）：
- en: Check that ranges that cross 0 work as expected. How many negative temperature
    readings are there? Should there be any?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查跨越0的范围是否按预期工作。有多少个负温度读数？是否应该有？
- en: Check the limits of real-world attributes such as temperature. How many readings
    above 60° or below –35° are there?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查现实世界属性（如温度）的限制。有多少次读数超过60°或低于-35°？
- en: Determine what are the entity counts. For example, how many records are from
    sensor 21? How many sensors are recorded? How many records have “op” and what
    different values are there in that code field?
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定实体计数。例如，有多少条记录来自传感器21？记录了多少个传感器？有多少条记录有“op”，以及该代码字段中有哪些不同的值？
- en: 'Additionally, in the work that you did to compile the data story, you may have
    already uncovered some known issues with the data. Now is the time to focus on
    those and to dig in. Looking again at the smart building example, let’s imagine
    that there was a known change in sensor design after the first year of operation.
    The question is, did the change make any difference to the data that’s present?
    We saw that there are 360,959 null values in the data set. What’s the distribution
    of them year by year:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在你编译数据故事的工作中，你可能已经发现了数据中的一些已知问题。现在是时候关注这些问题并深入挖掘。再次看看智能建筑示例，让我们假设在第一年运营后，传感器设计发生了已知的变化。问题是，这个变化是否对现有的数据有任何影响？我们看到了数据集中有360,959个空值。它们按年份的分布如下：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: About 0.5% of the data in the first year is null, and the first year is the
    year that the sensors were changed. That’s something to further investigate and
    take into account when modelling. It may be that that year’s data should be disregarded
    altogether. The job of the data survey is to find problems, and it informs the
    team as to what they can expect in the data resource. The survey shows them that
    what they were expecting is there or not.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第一年的数据中有大约0.5%是空的，而第一年是传感器更换的那一年。这是需要进一步调查并考虑建模时的事项。可能那年的数据应该完全忽略。数据调查的工作是找出需要进一步审查的数据问题，并告知团队他们可以期待的数据资源。调查显示他们所期待的是否存在。
- en: The previous examples deal with numeric values. Of course, numeric data will
    not be the only data that you will need to use, so what kind of actions should
    you take to survey categorical or unstructured data?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子处理的是数值。当然，数值数据不会是你唯一需要使用的数据，那么在调查分类或非结构化数据时，你应该采取什么行动？
- en: 5.2.3 Surveying categorical data
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 调查分类数据
- en: 'With categorical data, it’s often important to know the distribution of records
    across the categories. In the smart building example, there is a sensor type code,
    which was seen when we ran the query:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类数据，了解记录在各个类别中的分布通常很重要。在智能建筑示例中，有一个传感器类型代码，我们在运行查询时看到了它：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The unknown is `op`, but `tx` is a type of sensor. How many types of sensors
    are there? The data about sensor types is categorical (manufacturer ID), so getting
    a sense of it requires a slightly more complex query that first gets all the distinct
    sensor types from the temperature_readings table and then counts them:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 未知的是 `op`，但 `tx` 是一种传感器类型。有多少种传感器类型？关于传感器类型的数据是分类的（制造商ID），因此要了解它需要稍微复杂一些的查询，首先从
    `temperature_readings` 表中获取所有不同的传感器类型，然后进行计数：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The number 7 is low enough to make it worth enumerating them:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数字7足够低，值得列举它们：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The appearance of UNKNOWN is an issue that we will need to investigate, including
    what the different types of sensors are. At this point, however, we can stop this
    line of investigation because the EDA exercise may commit significant resources
    to dig further. Remember, the data survey is about establishing what’s important
    or difficult about the data that needs further scrutiny, not necessarily about
    doing that scrutiny now.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: UNKNOWN的出现是我们需要调查的问题，包括不同类型的传感器。然而，此时我们可以停止这条调查线，因为EDA练习可能会投入大量资源进行进一步挖掘。记住，数据调查是关于建立需要进一步审查的数据的重要或困难之处，而不一定是现在就进行审查。
- en: If the sensor types are determined to be important, then in the EDA exercise,
    we should look at the number of each type, how many different readings each produce,
    the impact that the different qualities of sensor have on the data, the range
    of readings from each, the outliers each type generates, what granularity of temperature
    each sensor registers, and so on. The point of the data survey is to establish
    what these questions are, which is what we’ve done now. When the survey is reviewed,
    the issues and questions can be prioritized for the EDA team to follow up on.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果确定传感器类型很重要，那么在EDA练习中，我们应该查看每种类型的数量，每种类型产生多少不同的读数，不同质量的传感器对数据的影响，每个传感器的读数范围，每个类型产生的异常值，每个传感器记录的温度粒度等等。数据调查的目的是确定这些问题是什么，这是我们目前所做的事情。当调查被审查时，问题和问题可以被优先排序，以便EDA团队跟进。
- en: 5.2.4 Surveying unstructured data
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 调查非结构化数据
- en: Dealing with unstructured data in the survey is different. Delving into the
    properties of a set of unstructured items requires deep ML techniques. We’ll cover
    investigating unstructured data in more detail as part of the EDA exercise in
    chapter 6, but these approaches are often too involved and time consuming for
    a data survey. In the survey, we need to map out the issues and problems that
    might be lurking in the data. Some will be irrelevant to the models that the team
    develops and the applications that consume them. The important thing is to know
    where they are so that they can either be investigated and resolved or avoided.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查中处理非结构化数据是不同的。深入研究一组非结构化项目的属性需要深度机器学习技术。我们将在第6章的EDA练习部分更详细地介绍调查非结构化数据，但这些方法通常过于复杂且耗时，不适合数据调查。在调查中，我们需要描绘出数据中可能潜伏的问题和问题。其中一些可能与团队开发的模型以及使用它们的应用程序无关。重要的是要知道它们在哪里，以便它们可以被调查和解决，或者被避免。
- en: 'With unstructured data, it’s important to establish what useful unstructured
    data resources are available and to assess their quality. In the smart building
    database, there is an incidents table that needs to be included in the survey:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非结构化数据，重要的是要确定哪些有用的非结构化数据资源可用，并评估它们的质量。在智能建筑数据库中，有一个需要包含在调查中的事件表：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The query shows us that the text fields are incident_description and resolution.
    Let’s gauge the quality of these:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 查询显示文本字段是事件描述和解决方案。让我们评估这些字段的质量：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It seems that there is potentially interesting information in these snippets.
    It might be hard to translate that into data because parsing out which floor the
    problems are occurring in may be challenging. On the other hand, it’s probably
    going to be easy to relate some of these incidents to strange sensor readings.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些片段中似乎有潜在的有用信息。可能很难将其转换为数据，因为解析出问题发生在哪一层可能具有挑战性。另一方面，将这些事件与奇怪的传感器读数联系起来可能很容易。
- en: Typically, you would do this kind of inspection over more records because it’s
    easy to scroll through scores or even hundreds of records to see if they do (or
    do not) contain rich-looking information. It would also be normal for the analyst
    doing the survey to check how many records are empty or contain the word *NULL*.
    A quick inspection might also show other indicators of useless records (such as
    “.” or “n/a”), which were used to gloss over mandatory fields in their administration
    application.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会对更多的记录进行此类检查，因为很容易滚动浏览数十条或数百条记录，以查看它们是否（或没有）包含看起来丰富的信息。对于进行调查的分析师来说，检查有多少条记录为空或包含单词*NULL*也是正常的。快速检查也可能显示其他无用记录的指标（例如“。”或“n/a”），这些指标被用来掩盖其管理应用程序中的必填字段。
- en: Similar approaches can be used for other types of unstructured data as well.
    Skilled analysts can quickly extract and render hundreds of images from a database
    and scan them for issues. For instance, a selection of 100 images might contain
    90 that are simply of blue sky. This might be normal and expected and still indicate
    useful data, or it might be a problem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的方法也可以用于其他类型的非结构化数据。熟练的分析师可以快速从数据库中提取和渲染数百张图片，并扫描它们以查找问题。例如，100张图片的选择中可能有90张只是蓝色的天空。这可能是正常和预期的，并且仍然表明有用的数据，或者它可能是一个问题。
- en: Spotting regularities, patterns, and oddities in unstructured data quickly allows
    you to ask more questions and to delve down more deeply into the data. Perhaps
    the 90 blue sky images show up in the analyst’s query because there’s some quirk
    in the database that makes them appear in a random selection. The EDA team can
    get to the bottom of this and provide the evidence that you need to show your
    sponsors that there is or isn’t an issue. The point is that you will do this all
    before you’ve committed serious time and resources to the modeling and evaluation
    activities.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 快速在非结构化数据中找出规律性、模式和异常，可以让你提出更多问题，并更深入地挖掘数据。也许90张蓝天图像出现在分析师的查询中，因为数据库中存在某种怪癖，使它们随机出现在选择中。EDA团队可以深入了解这个问题，并提供你需要向赞助商展示的证明，以表明是否存在问题。关键是，你将在投入大量时间和资源进行建模和评估活动之前完成所有这些工作。
- en: 5.2.5 Reporting and using the survey
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.5 报告和使用调查
- en: The data survey needs to be documented and reviewed before it can be used. Table
    5.2 lists as a suggestion the items that could be recorded in a survey report.
    Typically, there will be many pages of uninteresting findings, reporting on the
    existence of tables with the right number of records and good integrity, for example,
    so it’s useful to attach a cover sheet to the survey with key deltas recorded.
    This alerts everyone to the main foibles that were found in the records.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在可以使用之前，数据调查需要被记录和审查。表5.2建议在调查报告中记录的项目。通常，会有许多页不感兴趣的结果，报告了具有正确记录数和良好完整性的表格的存在，例如，因此附上带有关键变化的封面页对调查很有用。这会让每个人都注意到在记录中发现的主要缺点。
- en: Table 5.2 Data survey report contents
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 数据调查报告内容
- en: '| Item | Note (examples) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 项目 | 备注（示例） |'
- en: '| --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Description of data | Discursive description of what the data is (domain,
    source systems, scale, time period, etc.). |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 数据描述 | 对数据是什么的描述（领域、源系统、规模、时间范围等）。 |'
- en: '| List tables provided | Name of table and list of attributes. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 提供的列表表格 | 表格名称和属性列表。 |'
- en: '| For each table: |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 对于每个表格： |'
- en: '| Known problems | Relate the description of known problems with the data.
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 已知问题 | 将已知问题的描述与数据相关联。 |'
- en: '| Number of entities | If it’s possible to live count the records; otherwise,
    taken from a data catalog. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 实体数 | 如果可以实时计数记录，则从数据目录中获取。 |'
- en: '| Sample entity | A fresh sample taken from a table. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 样本实体 | 从表格中取出的新鲜样本。 |'
- en: '| Number of records with nulls | Possibly focused on attributes of interest.
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 具有nulls的记录数 | 可能关注感兴趣的属性。 |'
- en: '| Number of records with 0 values | Possibly focused on attributes of interest.
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 具有零值的记录数 | 可能关注感兴趣的属性。 |'
- en: '| Specific queries to investigate known or revealed data integrity issues |
    `Query 1``Result``Query 2``Result` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 调查已知或已揭示的数据完整性问题的特定查询 | `查询1``结果``查询2``结果` |'
- en: A survey report is useful for the team; they would have to find this information
    themselves to use the data, so it saves repeated work. It also creates a talking
    point about the data properties. Additionally, more queries to investigate the
    integrity of the data as is will get added into it over time as different people
    raise suspicions and bright ideas about the data. Gradually, the data resource
    becomes a well-described entity for the project.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 调查报告对团队很有用；他们必须自己找到这些信息才能使用数据，因此可以节省重复工作。它还创造了关于数据属性的话题点。此外，随着时间的推移，随着不同的人对数据提出怀疑和好主意，将添加更多查询以调查数据的完整性。逐渐地，数据资源成为项目的一个描述良好的实体。
- en: The most important use for the survey, however, is to allow the team to determine
    where to spend effort during the EDA exercise. The survey provides a map, showing
    what infrastructure is needed to allow for the questions that need to be resolved
    before building a model. The survey document also becomes a handy way to start
    data testing. (Data testing is needed to support the implementation of data pipelines
    for the modelling and for subsequent production exercises.) Finally, the survey
    may already have uncovered problems with the data that jeopardize your project.
    If this is the case, then get them on the risk register and make the client aware
    of these.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，调查最重要的用途是允许团队确定在EDA练习期间在哪里投入精力。调查提供了一张地图，显示了在构建模型之前需要解决哪些基础设施问题。调查文档也成为开始数据测试的便捷方式。（数据测试对于支持建模和随后的生产活动中的数据管道的实施是必要的。）最后，调查可能已经发现了危及项目的数据问题。如果是这样，那么将它们列入风险登记册，并让客户了解这些问题。
- en: 5.3 Business problem refinement, UX, and application design
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 商业问题细化、用户体验和应用程序设计
- en: Moving on from the data survey, the next step in sprint 1 is to develop a deeper
    understanding of the business problem. Documenting and understanding business
    issues provide the information that enable the team’s work on the project and
    data infrastructure to be efficient, directed, and purposeful.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据调查过渡到冲刺1的下一步是更深入地理解商业问题。记录和理解商业问题提供了信息，使团队在项目和数据基础设施上的工作变得高效、有方向和有目的。
- en: 'Business problem refinement ticket: S1.2'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 商业问题细化票据：S1.2
- en: Develop a business application description.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发商业应用程序描述。
- en: Develop the application user story backlog (S2.1, S3.1).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发应用程序用户故事待办事项（S2.1，S3.1）。
- en: Validate the application description and user story backlog with users.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与用户验证应用程序描述和用户故事待办事项。
- en: Identify and validate the ML model performance requirements.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定和验证机器学习模型性能要求。
- en: Create UX designs.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用户体验设计。
- en: Understanding a business properly is a daunting challenge that can consume months
    or years. A technical team typically only gradually evolves a deep appreciation
    of the demands and constraints on a business, and it’ll take a long time for them
    to really see the heart of the issues that are most important. Yet, pinning down
    the business problem that the team is addressing must be done, and the team must
    develop a view on this if it is to be successful. To accelerate eliciting and
    documenting the core business issues, it’s necessary to adopt a few tactics and
    tricks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正确理解业务是一个艰巨的挑战，可能需要数月或数年。技术团队通常只能逐渐发展对业务需求和约束的深刻理解，并且他们需要很长时间才能真正看到最重要问题的核心。然而，确定团队正在解决的商业问题必须完成，并且团队必须对此形成观点，以便取得成功。为了加速收集和记录核心商业问题，有必要采用一些策略和技巧。
- en: In the pre-project, you built a project hypothesis, and in sprint 0, you turned
    that into a project roadmap. These flesh out your team’s mandate for the work
    that you’re delivering, and the customer reviewed both. In addition, you created
    user and model stories while doing the project hypothesis. These outputs should
    be helpful in beginning the work that is needed to create an effective application
    design, but there’s a lot more to be done!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目前期，你构建了项目假设，在冲刺0阶段，你将其转化为项目路线图。这些明确了你团队交付工作的授权，并且客户也进行了审查。此外，在构建项目假设的过程中，你创建了用户和模型故事。这些输出对于开始创建有效的应用程序设计的工作是有帮助的，但还有更多的工作要做！
- en: To make progress, the team needs to use the information they have so far to
    talk to the next layer of experts and users. Agile practitioners say, “a story
    card is a promise for a conversation” [3]. The user stories you compiled in the
    pre-project phase can be further developed and formalized into story cards. *Story
    cards* lay out the application concept, who is impacted by it and how, and what
    the story does for the organization. Table 5.3 shows the contents of a typical
    story card.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了取得进展，团队需要利用他们目前拥有的信息与下一层的专家和用户交谈。敏捷实践者说：“故事卡是进行对话的承诺” [3]。在项目前期阶段编制的用户故事可以进一步发展和正式化为故事卡。*故事卡*概述了应用程序的概念、受其影响的人以及如何影响，以及故事对组织的作用。表5.3显示了典型故事卡的内容。
- en: Table 5.3 The contents of a story card for an ML project. Note the final five
    rows, which establish how to go about creating this.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.3 机器学习项目故事卡的内容。注意最后的五行，它们确定了如何创建此内容。
- en: '| Item | Notes (example) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 项目 | 备注（例如） |'
- en: '| --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Concept | A simple explanation of the top-level area the project addresses;
    for example, customer service or inventory management. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 概念 | 对项目解决的最高级别区域的简单解释；例如，客户服务或库存管理。 |'
- en: '| List of stakeholders | Jo Bloggs (IT), Sam Smith (Proc), Arthur Aske (Logistics)
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 利益相关者列表 | 乔·博客斯（IT），山姆·史密斯（采购），亚瑟·阿斯克（物流） |'
- en: '| Business priorities | The list of focus areas for the business: customer
    service uplift, revenue, cost reduction, growth, diversification, capital efficiency,
    cash flow, competition, social responsibility, for example. |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 商业优先事项 | 商业关注的重点领域列表：客户服务提升、收入、成本降低、增长、多元化、资本效率、现金流、竞争、社会责任等。 |'
- en: '| Impact statement for business | How the solution delivers value; what problem
    it solves for the customer. For example, this analysis provides insights of interest
    to the Primary Products division and. . . . |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 商业影响声明 | 解决方案如何创造价值；它为客户解决了什么问题。例如，这项分析为初级产品部门提供了有价值的见解。 |'
- en: '| Impact to business priorities | States the link between the impact statement
    and the business priorities of the company. For example, these insights inform
    purchasing and manufacturing how to better drive revenue and improve capital efficiency.
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 对商业优先级的影响 | 陈述影响声明与公司商业优先级之间的联系。例如，这些见解指导采购和制造如何更好地推动收入和改善资本效率。 |'
- en: '| Data resources | Identifies the data resources the system will draw on, both
    for training and for production. Describes how the data will get to the model
    in production. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 数据资源 | 识别系统将用于训练和生产的数据资源。描述数据如何在生产中到达模型。 |'
- en: '| Model concept | Describes the model to be used, what it does, and why it’s
    expected to work. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 模型概念 | 描述将要使用的模型，它做什么，以及为什么预期它将有效。 |'
- en: '| Functional/nonfunctional requirements | Relates how good the model must be
    at its job. How much throughput, latency, and availability for the model? How
    much does serving it cost? |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 功能/非功能需求 | 说明模型在其工作中必须有多好。模型需要多少吞吐量、延迟和可用性？提供服务需要多少成本？ |'
- en: '| How it will be used | Explains how the effect delivered will be materialized
    as a business activity. What will be done with the information from the model
    or the decisions from the system? Also provides information via visualization
    and a dashboard for decision makers. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 如何使用 | 解释效果将如何作为一项商业活动实现。将如何处理模型的信息或系统的决策？还通过可视化仪表板为决策者提供信息。 |'
- en: '| Problems and issues | Notes factors that could derail the implementation
    of the system or prevent it from being useful. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 问题与挑战 | 记录可能导致系统实施失败或使其无用的因素。 |'
- en: Documenting organizational aspects like this provides context and explains the
    story’s purpose. Also, documenting the model’s data and why it’s thought that
    the model will work or can work shows that the work to be done is purposeful.
    The team sees a feasible way to employ ML to deliver the business value that is
    hoped for.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 记录此类组织方面的内容提供了背景并解释了故事的目的。此外，记录模型的数据以及为什么认为模型将有效或可以工作，表明将要进行的工作是有目的的。团队看到了一种可行的方式来应用机器学习（ML）以实现所期望的商业价值。
- en: 'The last three components of the story card describe the model’s constraints.
    Functional and nonfunctional requirements spell out the performance that the model
    requires to be useful for the client:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 故事卡的最后三个组成部分描述了模型的约束。功能和非功能需求明确指出模型需要达到的性能，以便对客户有用：
- en: Functional requirements typify the ability of the model to produce valuable
    classifications or predictions.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能需求典型地代表了模型产生有价值分类或预测的能力。
- en: Non-functional requirements spell out the need for the model to do so quickly,
    economically, and robustly.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非功能需求明确指出模型需要快速、经济和稳健地执行。
- en: The next part of the card asks for documentation about how the model is to be
    used. This is something that must be clarified now. If there isn’t a point in
    the business process where the output of the model can be created, picked up,
    and used, then even if the model is perfect, it will be perfectly useless. Finally,
    problems and issues should be considered and notated. Are there any factors that
    negate the value of the model in the business process? For example, if the model
    produces some advice for a user, but when the user most needs that information,
    they are typically busy, then work is needed to make the model’s output effectively
    consumable.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 卡片的下一部分要求记录有关如何使用模型的信息。这是现在必须明确的事情。如果业务流程中没有点可以创建、获取和使用模型输出，那么即使模型完美，也将毫无用处。最后，应考虑并记录问题与挑战。是否存在任何否定模型在业务流程中价值的因素？例如，如果模型为用户提供了一些建议，但当用户最需要这些信息时，他们通常很忙，那么就需要工作来使模型输出有效可消费。
- en: The process of gathering the information required to populate story cards can
    be time-consuming and challenging, so it is important to prioritize. Because you
    are working on an ML project, you need to place extra emphasis on some elements
    of the story card. For example, the data resources section of the card is key,
    so it’s useful to relate what’s said in discussions on the stories to what’s known
    from the data story and survey. Issues that are identified when developing these
    cards can usefully feed into the survey document and on to the EDA for clarification.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 收集填充故事卡所需信息的流程可能既耗时又具有挑战性，因此优先排序很重要。因为你正在从事一个机器学习项目，你需要特别强调故事卡的一些元素。例如，卡片的数据资源部分是关键，因此将讨论中提到的内容与数据故事和调查中已知的内容联系起来是有用的。在开发这些卡片时确定的问题可以有效地输入到调查文档中，并进一步用于澄清的EDA。
- en: 'Beyond emphasizing the data aspects in the story, the model’s functional and
    nonfunctional performance requirements have a strong bearing on the designs that
    we will develop in chapter 7\. These include:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了强调故事中的数据方面，模型的功能性和非功能性性能要求对我们将在第7章中开发的设计有重要影响。这包括：
- en: '*The required latency and responsiveness:* How fast will the model need to
    be to produce predictions from the data for it to be useful?'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*所需的延迟和响应性:* 模型需要多快才能从数据中产生预测，以便它是有用的？'
- en: '*The expected throughput for the model:* How many cases can be dealt with per
    second or per day?'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型预期的吞吐量:* 每秒或每天可以处理多少案例？'
- en: '*The expected mode of use:* Will it run in batch mode (for example, updating
    millions of records in a database overnight) or online (for example, responding
    to a user on a web site)?'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预期的使用模式:* 它将以批量模式运行（例如，在夜间更新数据库中的数百万条记录）还是在线运行（例如，响应网站上的用户）？'
- en: '*The required robustness:* How often can a failure of the model be tolerated?'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*所需的鲁棒性:* 模型的失败可以容忍多频繁？'
- en: '*The model’s accuracy:* How accurate do the different stakeholders believe
    a useful model should be?'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型的准确性:* 不同利益相关者认为一个有用的模型应该有多准确？'
- en: Model accuracy is a complicated concept, and there is a lot to do to properly
    evaluate and measure it (see chapter 8 for a long and tortured discussion of this).
    For now, it’s important to get an understanding of the depth of the challenge
    the team faces.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型准确性是一个复杂的概念，为了正确评估和衡量它有很多工作要做（参见第8章，其中对这一问题的长篇讨论）。现在，了解团队面临的挑战的深度很重要。
- en: '*The number of mistakes:* What kinds of mistakes will the different users tolerate?
    What mistakes erode confidence in the model? How costly is a false negative classification
    versus a false positive one? (As an example, if the model misses an incidence
    of a disease in a sample, how costly is that compared to the model falsely detecting
    a disease in error?) What consequences will these mistakes have?'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误数量:* 不同用户可以容忍哪些类型的错误？哪些错误会侵蚀对模型的信心？错误分类的误判成本与误报成本相比如何？（例如，如果模型在样本中错过了一种疾病的病例，那么与模型错误检测疾病相比，成本是多少？这些错误将产生什么后果？'
- en: '*The performance criteria:* How is the system’s performance related to business
    value? At what point does the system’s performance (or lack of) destroy value?
    Are there any thresholds where diminishing returns kick in? When is performance
    improvement unimportant?'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能标准:* 系统的性能与业务价值有何关联？在什么情况下，系统的性能（或缺乏性能）会破坏价值？是否存在任何阈值，当边际效益递减时开始发挥作用？何时性能改进不再重要？'
- en: 'For each story created and gathered, you need to consider two things: validation
    and interaction. First, the story card needs to be validated. You need to run
    through it with the people who are impacted and make sure it reflects their concept
    of reality. Additionally, ask yourself, are these the stories that money should
    be spent on? Rigorously prioritize and prune the stories to understand the minimum
    set that creates value.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个创建和收集的故事，你需要考虑两件事：验证和交互。首先，故事卡需要经过验证。你需要与受影响的人一起运行它，并确保它反映了他们的现实概念。此外，问问自己，这些是应该花钱的故事吗？严格优先排序和修剪故事，以了解创造价值的最低集合。
- en: The demands of the stories on the models developed are also important. Stories
    that impose challenging demands on the models are likely to be expensive. Eliminating
    them, if possible, makes the project much more viable. The question is, “is this
    important and valuable enough to justify focusing the project on the requirements
    of this particular story?”
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对开发出的模型的需求也很重要。对模型提出挑战性要求的故事可能会很昂贵。如果可能的话，消除它们可以使项目更具可行性。问题是，“这是否重要且有价值，足以证明将项目集中在特定故事的要求数据上？”
- en: Second, the UX experts in your team need to work up ideas about how the users
    will interact with the system, and ultimately, will interact with the models underpinning
    it. At this point in the project, the team should have a good, if still necessarily
    vague, idea of what the final product should look like. The benefit of developing
    UX concepts now is that by producing the wireframes and mock screens, you start
    to bring the concept to life for its consumer as well as the team. This creates
    engagement and builds a shared understanding of what’s possible, what it’s going
    to do, and what it’s going to look like. (Chapter 9 has an extensive discussion
    about the style and structure of the applications that can use ML.)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，你们团队的用户体验专家需要提出关于用户如何与系统交互的想法，以及最终将如何与支撑它的模型交互的想法。在项目这个阶段，团队应该有一个良好、尽管仍然必要模糊的想法，即最终产品应该是什么样子。现在开发用户体验概念的好处是，通过制作线框图和模拟屏幕，你开始将概念为消费者以及团队带来生命力。这创造了参与感，并建立了一个共同的理解，即可能实现什么，它将做什么，以及它将看起来如何。（第9章对可以使用机器学习的应用程序的风格和结构进行了广泛的讨论。）
- en: These findings can be written as an application description and reviewed with
    project stakeholders. This creates and documents a joint agreement about what
    the application might look like and how it might behave. The requirements in the
    report that you’ve extracted also need to be validated with the modeling team,
    your data scientists, and your data engineers. Potentially, this creates either
    red flags and risks for the risk register, or green flags and negotiation points
    with the users when agreeing on the requirements that the team can deliver. It
    explains which stories are in, which stories are out, and what it is that the
    models need to do to deliver them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现可以写成应用程序描述，并与项目利益相关者进行审查。这创建并记录了关于应用程序可能看起来如何以及可能如何表现的一致意见。你在报告中提取的要求也需要与建模团队、数据科学家和数据工程师进行验证。可能地，这为风险登记册创建了红旗和风险，或者在同意团队可以交付的要求时，与用户达成绿灯和谈判点。它解释了哪些故事是包含在内的，哪些故事是排除在外的，以及模型需要做什么来交付它们。
- en: At this point, it’s appropriate to make a comment about agility, as in “this
    is an agile project.” The set of selected stories and the projected application
    remain as discussion points. They cannot be concretely agreed on yet because the
    models do not exist and may never exist. Instead, they are an agreement that this
    is the first set of things that the team will build; if all goes well, then these
    are the best options for spending the client’s money. If performant models cannot
    be extracted from the data, then the story cards need to be reshuffled, and you
    need another take on solving the customers problems and creating business value.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，适当地对敏捷性发表评论是合适的，比如“这是一个敏捷项目。”所选的故事集和预期的应用程序仍然是讨论点。由于模型尚不存在且可能永远不存在，因此它们还不能具体达成一致。相反，这是一个协议，即这是团队将构建的第一套东西；如果一切顺利，那么这些就是花费客户资金的最佳选择。如果无法从数据中提取出性能模型，那么故事卡需要重新洗牌，你需要另辟蹊径来解决客户的问题并创造商业价值。
- en: A clear set of stories describing the value and approach to modeling sets the
    scene for the development of useful output from the project. In order to support
    this, there’s more background work to do. The next task that must be tackled is
    to develop the first set of data pipelines. The pipeline takes the raw data and
    transforms and moves it to the environment that the team will use to explore and
    manipulate it for modeling.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一套清晰的描述建模价值和方法的故亊，为从项目中开发有用的输出设定了场景。为了支持这一点，还需要做更多的工作。下一个必须解决的问题是要开发第一套数据管道。该管道将原始数据转换并移动到团队将用于探索和操作以进行建模的环境。
- en: 5.4 Building data pipelines
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 构建数据管道
- en: The data pipeline infrastructure is the lifeblood of an ML project. The availability
    of useful, clean data and the ability to transform and enrich data rapidly in
    response to changes or new results is hugely enabling. Having a flexible and easy-to-manage
    pipeline infrastructure makes the team more productive and responsive to the project’s
    requirements. Importantly, it helps them cope if things go wrong and approaches
    and algorithms don’t live up to their expectations. Task S1.3 defines what needs
    to be done at this stage. Because this task delivers the data pipeline to the
    team’s development environment, work to replicate this pipeline or to reuse it
    for test and production data flows is required in sprint 1.3.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道基础设施是机器学习项目的生命线。有用、干净数据的可用性以及能够快速响应变化或新结果进行数据转换和丰富的能力，具有巨大的促进作用。拥有灵活且易于管理的管道基础设施可以使团队更高效，并能更好地响应项目需求。重要的是，它有助于他们在事情出错时应对，如果方法和算法没有达到预期效果。任务S1.3定义了这一阶段需要完成的工作。因为这项任务将数据管道交付给团队的开发环境，所以在冲刺1.3中需要复制此管道或将其用于测试和生产数据流的重用。
- en: 'Data pipeline ticket: S1.3'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道工单：S1.3
- en: Aggregate and fuse relevant data into an integrated picture.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相关数据聚合和融合成一个综合的图景。
- en: Implement and manage data pipelines.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施和管理数据管道。
- en: Design and implement data tests.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和实施数据测试。
- en: Data is often distributed in a complex ecosystem of containers, databases, data
    marts, and data warehouses across intranets, clouds, and organizations. Managing
    the collection and the manipulation of these data assets can quickly become overwhelmingly
    complex.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通常分布在复杂的容器、数据库、数据集市和数据仓库生态系统中，这些系统跨越内部网络、云和各个组织。管理和操作这些数据资产集合可能会迅速变得极其复杂。
- en: Before the EDA exercise (in chapter 6), the team needs to build an infrastructure
    that lets them examine and process the data quickly and conveniently, and they
    want the results of the investigation to be reliable and reproducible. This work
    can then be built on to create a system that distills the available data into
    forms that can be readily used to train and test models. Normally, this means
    using a series of aggregations and transformations to reduce many (possibly hundreds)
    data assets into a single table or stream of relatively simple records for consumption
    by a few algorithms. Data needs to be rebalanced to cope with uneven distribution
    of the types of items present, enabling a robust model to be extracted by the
    ML algorithms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中的EDA练习之前，团队需要建立一个基础设施，使他们能够快速方便地检查和处理数据，并且他们希望调查结果可靠且可重复。然后，这项工作可以在此基础上构建，以创建一个系统，将可用数据提炼成可以用于训练和测试模型的格式。通常，这意味着使用一系列聚合和转换来将许多（可能是数百个）数据资产减少到一个单一的表格或相对简单的记录流，以便供少数算法消费。数据需要重新平衡，以应对现有项目类型的不均匀分布，从而通过机器学习算法提取出稳健的模型。
- en: In the case of unstructured data, this often needs to be rendered to a standard
    format or size, and for corrupted items, it needs to be filtered and distorted.
    Data augmentation is a process of using transformations and alterations to create
    examples for training [5].
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非结构化数据，这通常需要将其转换为标准格式或大小，对于损坏的项目，需要过滤和扭曲。数据增强是一个使用转换和修改来创建训练示例的过程[5]。
- en: 'Later, in the development process, the team extends the pipelines to service
    the algorithms that the data scientists want to use and evaluate. As the models
    are tested and compared, novel manipulations can be tried to improve and fettle
    the results. Constructing the data infrastructure to accommodate these additions
    and changes is essential for promoting agility. Before the team gets to that though,
    they need to bring together the data tables that contain the raw information and
    provide it in a way that is consumable. The task is to:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程的后期，团队扩展管道以服务于数据科学家想要使用和评估的算法。随着模型的测试和比较，可以尝试新的操作来改进和调整结果。构建数据基础设施以适应这些新增和更改对于提高敏捷性至关重要。然而，在团队达到这一目标之前，他们需要汇集包含原始信息的表格，并以可消费的方式提供。任务是：
- en: Create a live and maintainable resource. It’s possible to use this resource
    to both work on up-to-date data and to reproduce past results from specific checkpoints.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个实时且可维护的资源。可以使用此资源来处理最新数据，并从特定的检查点重新生成过去的结果。
- en: Identify and deal with any problems in the source data. This sets a solid foundation
    for the next phase of the project.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别并处理源数据中的任何问题。这为项目的下一阶段奠定了坚实的基础。
- en: Create an asset that reflects the reality of the domain that it represents.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个反映其所代表领域现实情况的资产。
- en: 'The pipeline must support all the required phases of processing to achieve
    these three objectives and to create a usable training data resource. Figure 5.1
    shows an overview of the process required to support data engineering for an ML
    project. The process in figure 5.1 has different names and is often presented
    with different nuances and divisions between stages. One of the most familiar
    framings is ETL (extract, transform, load) in the context of data engineering.
    This process presented here emphasizes the particular requirements of an ML project.
    From figure 5.1, you can see that the data must be:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 管道必须支持所有必要的处理阶段，以实现这三个目标并创建一个可用的训练数据资源。图5.1展示了支持ML项目数据工程所需的过程概览。图5.1中的过程有不同的名称，并且通常以不同的细微差别和阶段划分来呈现。最熟悉的一种框架是在数据工程背景下ETL（提取、转换、加载）。这里呈现的过程强调了ML项目的特定要求。从图5.1中，你可以看到数据必须：
- en: Ingested from its sources. These can be data streams from sources such as RabbitMQ
    or Kafka or can be from data files stored as XLXS, CSV, or Parquet formats.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从其来源摄取。这些可以是来自RabbitMQ或Kafka等来源的数据流，也可以是存储为XLXS、CSV或Parquet格式的数据文件。
- en: A common ingestion requirement is to use SQL queries to pull data from several
    tables and databases. Another ingestion requirement is to bring data from different
    infrastructures into a target infrastructure. Often the data is brought in from
    different cloud environments, SAS applications, or on-premise data stores into
    the cloud infrastructure. Once there, it can be conveniently processed by the
    ML processing facilities that are often available on demand in a cloud environment.
    It’s important to note, though, that restrictive security requirements pertaining
    to a particular data set can drive data from disparate environments back into
    the on-premises infrastructure where the sensitive data resides.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个常见的摄取要求是使用SQL查询从多个表和数据库中提取数据。另一个摄取要求是将来自不同基础设施的数据带入目标基础设施。通常，数据是从不同的云环境、SAS应用程序或本地数据存储带入云基础设施的。一旦到达那里，它就可以方便地由云环境中通常按需提供的ML处理设施进行处理。然而，需要注意的是，与特定数据集相关的限制性安全要求可能会将数据从不同的环境驱回敏感数据所在的本地基础设施。
- en: Once data is ingested into the target infrastructure and data engine(s) that
    are used to process it, the data must be manipulated and laid out for use in creating
    models. Sometimes this step is called *integration* because often the requirement
    is to create a single table that can be passed to an ML algorithm to create a
    model. Often, though, it’s better to create intermediate tables and data stores
    that put the acquired data into convenient and accessible forms for the modeling
    team. Manipulation of this data includes cleaning it and creating features from
    different aspects of the data.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦数据被摄取到目标基础设施和用于处理它的数据引擎中，就必须对这些数据进行操作和布局，以便用于创建模型。有时这一步被称为*集成*，因为通常的要求是创建一个可以传递给ML算法以创建模型的单一表。然而，通常更好的做法是创建中间表和数据存储，将获取的数据以方便和可访问的形式呈现给建模团队。这种数据的操作包括清洗它并从数据的不同方面创建特征。
- en: Having laid out the data for the modeling team, it’s important to ensure that
    the data can be appropriately and easily accessed by the right members of the
    team. IAM frameworks for many infrastructures can be used to provide this kind
    of infrastructure, ensuring that only the members with the right credentials can
    access the resource.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在为建模团队布局数据后，确保数据可以被团队的正确成员适当且容易地访问是很重要的。许多基础设施的IAM框架可以用来提供这种类型的框架，确保只有拥有正确凭证的成员才能访问资源。
- en: Beyond access control, the other requirement is to set up a serving system to
    read the data appropriately. This might be an SQL query, access to a large data
    file, or an API that the team uses to request training, test data, or data streams.
    A great practice is to provide documentation that lets the modelling team self-serve
    from the resource created by the pipelines.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了访问控制之外，另一个要求是设置一个服务系统来适当读取数据。这可能是一个SQL查询、对大型数据文件的访问，或者是一个团队用来请求训练数据、测试数据或数据流的API。一个很好的做法是提供文档，让建模团队能够从管道创建的资源中自助服务。
- en: '![](../Images/05-01.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-01.png)'
- en: Figure 5.1 The processing requirements delivered using the dev environment data
    pipeline infrastructure. This provides the data resources for the ML modeling
    team in sprint 2 and beyond.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 使用开发环境数据管道基础设施提供的数据处理需求。这为第2个冲刺及以后的机器学习建模团队提供了数据资源。
- en: There are two pitfalls that can cause serious issues in an ML project if they
    are not avoided at this stage. First, building a data set from a variety of sources
    can create statistical problems, and second, failing to address this task in a
    disciplined way as an engineering problem can create technical debt and mire the
    team in detail and difficulty later. The next section addresses the statistical
    problems that can arise when data from multiple sources is integrated to create
    a training set for ML systems.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段如果没有避免，有两个陷阱可能会导致机器学习项目出现严重问题。首先，从各种来源构建数据集可能会造成统计问题，其次，未能以工程问题的严谨方式处理这项任务可能会造成技术债务，并在后期使团队陷入细节和困难的泥潭。下一节将讨论当将来自多个来源的数据整合以创建机器学习系统的训练集时可能出现的统计问题。
- en: 5.4.1 Data fusion challenges
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 数据融合挑战
- en: A relatively new practice constructs modeling data sets from multiple data sources
    to build the data picture. Traditionally, statisticians gathered data using a
    single process (a survey or a sampling protocol) for a specific purpose. Independent
    experiments to create and gather data were done to reproduce or invalidate previous
    studies. Meta-analysis was used to combine the results of multiple studies of
    a phenomenon and to gain greater confidence about the results. Although retrieving
    and using other people’s data is difficult and expensive, it was generally accepted
    that this approach proved easier and more fruitful when getting new primary data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一种相对较新的实践是从多个数据源构建建模数据集，以构建数据图。传统上，统计学家为了特定目的使用单一过程（调查或抽样方案）收集数据。为了复制或验证先前的研究，进行了独立实验以创建和收集数据。元分析被用来结合多个现象的研究结果，以获得对结果的更大信心。尽管检索和使用他人的数据困难且昂贵，但普遍认为，在获取新的原始数据时，这种方法证明更容易且更富有成效。
- en: The context of data being stored and available for reuse and repurpose is new.
    Starting in the 1970s, data sets were systematically prepared and stored in digital
    archives for replication and reuse, but the computational resources to manipulate
    them didn’t exist. Nonetheless, statisticians studied the process of recombining
    data under the name of *data fusion* [1]. Nowadays, it’s common for data to be
    acquired and recombined to create new insights, but there are some problems with
    this process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储和可重复使用以及重新定制的背景是新的。从20世纪70年代开始，数据集被系统地准备并存储在数字档案中，以供复制和重用，但当时没有处理它们的计算资源。尽管如此，统计学家在*数据融合*的名称下研究了重新组合数据的过程[1]。如今，数据被获取和重新组合以产生新的见解是常见的，但这个过程存在一些问题。
- en: Combining data from a particular part of the population with data from general
    observations can introduce bias. You can gather data as part of an experiment
    that you use to study parts of the populations to detect a particular effect.
    For example, you might use a test negative protocol to ensure that a control group
    and a target group are selected from the same part of a population. These studies
    yield controlled, high-quality data sets, and it’s tempting to combine them with
    observational data to model some population-level behavior. Unfortunately, the
    selection protocols that control bias with respect to one variable versus a set
    of cofounding factors noise that interferes with the result) can introduce bias
    with respect to other variables.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将特定人群的一部分数据与一般观察数据相结合可能会引入偏差。你可以收集数据作为实验的一部分，你使用这个实验来研究人群的某些部分以检测特定效果。例如，你可能使用测试阴性方案来确保对照组和目标组来自人群的同一部分。这些研究产生了受控、高质量的数据集，将它们与观察数据相结合来模拟某些人口层面的行为是有吸引力的。不幸的是，控制一个变量相对于一组混杂因素的偏差（干扰结果的噪声）的筛选方案可能会引入其他变量的偏差。
- en: One of the most significant ways that bias is introduced is when we take data
    from different time periods and treat that as part of a single asset. When the
    data was collected is often a hidden feature; perhaps data sets were created from
    operational snapshots, which themselves contain records from different ranges
    of time. For example, you might collect aggregate retail data as a snapshot for
    several years and store it on the same date each year. This might all look consistent
    on first inspection, but because the day of the week that the snapshot is taken
    varies (for example, the first of April might be a Tuesday one year and a Friday
    the next), prime trading days can be shuffled from one period to another. Poor
    practice in the modeling process that consumes this data can lead to models that
    have significant distortions and then fail to work in production.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 引入偏差的最显著方式之一是我们从不同的时间段收集数据，并将其视为单一资产的一部分。数据收集的时间通常是隐藏的特征；也许数据集是从操作快照中创建的，而这些快照本身包含来自不同时间范围的数据记录。例如，你可能会收集多年的零售数据快照，并将其存储在每年的同一日期。在初次检查时，这看起来可能是一致的，但由于快照的日期（例如，一年的4月1日可能是星期二，而下一年的星期五）会变化，主要交易日可能会从一个时期转移到另一个时期。在消耗这些数据的建模过程中的不良实践可能导致模型存在重大扭曲，并在生产中无法正常工作。
- en: Rare entities in data sets can either create distortion by appearing disproportionately
    in samples because of random chance or they can create blind spots for modeling
    because they are missing from the data asset. Moreover, you can over-represent
    rare entities because they are more noticeable in data than normal entities and
    because they are more important to the client organization. For example, repeat
    and serious offenders appear disproportionately in criminal databases because
    petty offenders are not of interest to police. It’s less likely that the police
    will record petty crime, but that doesn’t mean that it’s uncommon. Building profiles
    of offenders from police records does not create representative profiles of all
    offenders, just the ones the police are interested in. This might mean that using
    this data to prevent crime can prove ineffective because the interventions predicted
    as successful may be inappropriate for the majority of criminals.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的稀有实体可能由于随机机会在样本中不成比例地出现而造成扭曲，或者由于它们缺失于数据资产中而给建模造成盲点。此外，你可以过度代表稀有实体，因为它们在数据中比普通实体更引人注目，并且对客户组织来说更重要。例如，在犯罪数据库中，重复和严重的违法者不成比例地出现，因为小违法者对警方没有兴趣。警方不太可能记录小犯罪，但这并不意味着它不常见。从警方记录中构建违法者档案并不能创建所有违法者的代表性档案，而只是警方感兴趣的违法者的档案。这可能导致使用这些数据预防犯罪的效果不佳，因为预测为成功的干预措施可能不适合大多数犯罪分子。
- en: Sensor data sets may contain data gathered with different qualities. In fact,
    the *same* data set can contain data with different qualities. For example, you
    might want to sample some sensors every minute and some sensors every hour. Some
    survey data may have values of agreement in the range 1-5 and some from 1-10;
    some tables may contain the average temperature for an area, whereas others may
    have samples of temperature within an area.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器数据集可能包含不同质量的数据。事实上，*相同*的数据集可能包含不同质量的数据。例如，你可能希望每分钟采样一些传感器，而每小时采样一些传感器。一些调查数据可能有1-5范围内的协议值，而另一些可能有1-10范围内的值；一些表格可能包含一个区域的平均温度，而另一些可能包含该区域内的温度样本。
- en: 'Understanding the semantics of the data sources at this level is crucial for
    deciding whether it’s meaningful and useful to combine them. Normalization errors
    can make a big difference to the models you derive from subsequent fused data
    sets. Just bolting the data together blindly may not be a good idea. There is
    no substitute for developing a strong understanding of the domain and thinking
    through the issues that might create a distorted distribution in the data set.
    Luckily, your team is in place and the work that you have done provides the information
    and awareness of these potential distortions: you need to qualify the processes
    as being well-founded and reflective of reality. At a minimum, it’s worth checking
    that:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个层面上理解数据源的语义对于决定是否将它们组合起来有意义和有用至关重要。规范化错误会对从后续融合数据集中得出的模型产生重大影响。盲目地将数据拼凑在一起可能不是一个好主意。对领域有深入的理解并思考可能导致数据集分布扭曲的问题是没有替代品的。幸运的是，你的团队已经到位，你所做的工作提供了关于这些潜在扭曲的信息和意识：你需要证明这些过程是建立在坚实的基础上并反映现实的。至少，检查以下内容是值得的：
- en: The data entities were collected on the same basis, whether that’s with a principled
    selection or with general observations.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据实体是在相同的基础上收集的，无论是基于原则的选择还是一般观察。
- en: There are no hidden variables (such as offender identity or time of collection)
    that hide distortion.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有隐藏的变量（如罪犯身份或收集时间）隐藏扭曲。
- en: Sensors capturing the entity’s behavior haven’t quantitatively or qualitatively
    changed over time.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获实体行为的传感器在时间上没有数量上或质量上的变化。
- en: There will be times when data that violates these strictures is useful, and
    you will be able to develop valuable models using such data. If so, then it’s
    important that you deliberately exercise your judgement and decide that’s the
    case rather than relying on blind luck!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会遇到违反这些严格规定的数据是有用的，你将能够使用这些数据开发出有价值的模型。如果是这样，那么你故意运用你的判断力并决定这是正确的，而不是依赖盲目运气是非常重要的！
- en: 5.4.2 Pipeline jungles
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 管道丛林
- en: 'Now, let’s return to the second challenge that can emerge as you construct
    the data infrastructure for the project: failing to address the task S1.3 in a
    disciplined way as an engineering problem.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到在构建项目数据基础设施时可能出现的第二个挑战：没有以纪律的方式将任务S1.3作为一个工程问题来处理。
- en: 'A common malady in ML projects is the emergence of a pipeline jungle [4]. This
    creates significant technical debt, leading to a situation in which the project
    is difficult to manage and both technically and functionally unreliable. Pipeline
    jungles emerge in projects where significant glue code for the data transformations
    and aggregations are constructed in an ad hoc way with no regard to management
    and maintenance. No team wants to create a pipeline jungle, but they emerge incrementally
    over time, creeping up on the team until they suddenly find themselves enmeshed
    and mired in complexity. The process reminds me of a famous quote about bankruptcy:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习项目中常见的疾病是管道丛林的出现[4]。这导致了显著的技术债务，使得项目难以管理，在技术和功能上都不可靠。管道丛林出现在那些数据转换和聚合的粘合代码以临时方式构建的项目中，没有考虑到管理和维护。没有团队愿意创建管道丛林，但它们随着时间的推移逐渐出现，悄无声息地逼近团队，直到他们突然发现自己陷入复杂之中。这个过程让我想起了关于破产的著名引言：
- en: How did you go bankrupt? Two ways. Gradually, then suddenly.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何破产的？两种方式。逐渐地，然后突然地。
- en: — Ernest Hemingway, The Sun Also Rises
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ——欧内斯特·海明威，《太阳照常升起》
- en: Some common issues can precipitate the start of a pipeline jungle. In some cases,
    specialist data infrastructures need to be handled by the team to cope with the
    scale, speed, or special properties of the data used in the project. For example,
    old proprietary data warehouses sometimes stored their data in arcane binary formats,
    which can be difficult to exfiltrate data without other proprietary export tools.
    The skills to deal with these tools may be beyond your project team, and you may
    have to collaborate with experts in the customer organization.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见问题可能导致管道丛林的出现。在某些情况下，团队需要处理专门的数据基础设施来应对项目中使用的数据的规模、速度或特殊属性。例如，旧有的专有数据仓库有时以晦涩的二元格式存储数据，没有其他专有导出工具的情况下，可能很难导出数据。处理这些工具的技能可能超出了你的项目团队的能力，你可能需要与客户组织的专家合作。
- en: 'Specialist tools can be a necessary evil, but they may be the only way to make
    the data available for modeling with acceptable performance characteristics or
    to provide compliance with software licensing terms. When using specialist or
    proprietary tools and adapters, it’s a good idea to wrap them within a standard
    pipelining tool, ensuring that the pipelines are integrated into the generic infrastructure.
    Because the wrapping and the integration work may be an overhead for the team,
    it’s tempting to hack a one-time process to get the data you need but be warned:
    that way madness lies. Doing it properly means that you have control over how
    data flows and the ability to spot issues and fix them as needed.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 专业的工具可能是必要的恶行，但它们可能是唯一能够以可接受的性能特征使数据可用于建模或满足软件许可条款的方式。当使用专业或专有工具和适配器时，将它们包裹在标准管道工具中是一个好主意，确保管道被集成到通用基础设施中。因为包装和集成工作可能对团队来说是一个额外的开销，所以可能会诱使你通过一次性的过程来获取所需的数据，但请警告：那样会导致混乱。正确地做意味着你能够控制数据流动，并能够根据需要发现和解决问题。
- en: Another driver is a failure to document and version the pipelines as for any
    other code. Data pipelines can turn into one of the most significant code investments
    that the team will make, so treat it as such. Pipelines need to be documented
    with respect to how they work, but they also need to be discoverable and well
    known. If you use a pipeline management and scheduling tool such as Apache Airflow,
    then pipeline discovery and identification will be looked after for you. Be alert
    for exceptions or special cases that somehow get missed. Make sure they are corralled;
    for example, by wrapping them with a single-step pipeline and the standard tooling.
    The single-step pipeline calls to the tool that you used for that component of
    the infrastructure. However, it will do so from the pipeline infrastructure.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个驱动因素是未能像其他代码一样记录和版本控制管道。数据管道可能成为团队将做出的最重要的代码投资之一，因此要像对待它一样。管道需要根据其工作方式来记录，但它们也需要可被发现且广为人知。如果你使用Apache
    Airflow等管道管理和调度工具，那么管道的发现和识别将为你处理。要警惕那些意外被忽略的异常或特殊情况。确保它们被控制住；例如，通过将它们包裹在一个单步管道和标准工具中。单步管道调用用于该基础设施组件的工具。然而，它将从管道基础设施中进行调用。
- en: Finally, as for other code pipelines, pipeline steps must be instrumented and
    tested. Each step and pipeline should report when it is invoked and whether it
    succeeds or fails. Failure conditions can be asserted by creating timers or timeouts
    that fire when an operation or pipeline takes too long or when completion occurs
    suspiciously fast. Setting these constraints catch problems that evade data testing
    because sometimes the data tests aren’t as good as they should be. Often, however,
    the data tests don’t pick up a problem in the technical execution of the process.
    For example, a data set that should be appended if certain conditions are met
    might appear to pass tests for distribution or integrity even when the call to
    the database to retrieve the appending data fails. However, because the failure
    occurs very fast (because the join is tiny) or after several timeouts (because
    a connection can’t be established), this should alert the team to a problem.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，就像其他代码管道一样，管道步骤必须被监控和测试。每个步骤和管道都应该报告其何时被调用以及是否成功。可以通过创建计时器或超时来断言失败条件，当操作或管道运行时间过长或完成速度可疑地快时，这些计时器或超时会被触发。设置这些限制可以捕捉到数据测试无法捕捉到的问题，因为有时数据测试并不像应该的那样好。然而，通常数据测试不会在过程的技术执行中捕捉到问题。例如，当满足某些条件时应该附加的数据集可能看起来通过了分布或完整性的测试，即使调用数据库检索附加数据时失败。然而，因为失败发生得非常快（因为连接很小）或经过几次超时（因为无法建立连接），这应该让团队意识到一个问题。
- en: 5.4.3 Data testing
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 数据测试
- en: As well as building pipeline tests, the team should invest in data testing.
    There is a tripwire to step over with data testing, though. Data tests can prove
    expensive and slow to run. This is unsurprising because there is often a lot of
    data, and tests can involve a lot of cross comparisons, which are computationally
    expensive. If you can’t afford data testing that’s exorbitantly expensive or that
    brings the pipelines grinding to a halt, it must be deployed strategically.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 除了构建管道测试，团队还应该在数据测试上投入。然而，数据测试也存在一个需要跨越的陷阱。数据测试可能成本高昂且运行缓慢。这并不令人惊讶，因为通常数据量很大，测试可能涉及大量的交叉比较，这些比较在计算上很昂贵。如果你负担不起过于昂贵或会导致管道停止运行的数据测试，那么它必须被战略性地部署。
- en: 'The level and target of data testing is a matter of judgement, but obviously,
    as for tests in normal software, the more *high-quality tests* that are implemented,
    the more the team can be confident of their work and their results. Types of data
    testing commonly implemented include:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据测试的级别和目标是判断问题，但显然，对于正常软件中的测试，实施的高质量测试越多，团队对其工作和结果就越有信心。常见的数据测试类型包括：
- en: '*Tests for duplication:* A common problem with a data pipeline is the replay
    of data from a source that encountered a problem exfiltrating data and then was
    programmed to start fresh. Duplication can also come from bugs in loop conditions
    in code that copies data (for example, a loop variable that doesn’t update on
    the last iteration of the loop or after some condition) or from manual cut-and-paste
    mess ups. Note that you may not even be aware that way back in the data’s story,
    someone cut and paste into a database, but this happens a lot!'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重复测试:* 数据管道的一个常见问题是来自遇到数据泄露问题的源的数据重放，然后被编程从头开始。重复也可能来自复制数据的代码中循环条件中的错误（例如，循环变量在循环的最后一次迭代或某些条件之后没有更新）或手动剪切和粘贴错误。请注意，你可能甚至没有意识到在数据的故事中，有人在数据库中剪切和粘贴，但这经常发生！'
- en: '*Tests for volumes:* Over time, volumes of data should be somewhat predictable.
    Use this fact to create a check on what’s arrived and what’s expected to be implemented.
    Try to establish bounds and bottom limits on what’s reasonable.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*体积测试:* 随着时间的推移，数据量应该是可预测的。利用这一事实来检查到达的数据和预期实现的数据。尝试确定合理范围内的界限和下限。'
- en: '*Tests for preconditions in input data:* This kind of test establishes that
    the input to a task is as expected. A partial and incomplete list of tests of
    this sort include testing that all expected columns are present, testing that
    the columns contain the same number of non-null data points (for example, if sensor
    x is off, then reading x is null), and statistical testing that shows that the
    distribution of data approximates the expectations (the standard deviation of
    the data is within a certain bound, for example).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入数据中的先决条件测试:* 这种测试确定任务输入是否符合预期。此类测试的部分和不完整的列表包括测试所有预期的列都存在，测试列包含相同数量的非空数据点（例如，如果传感器x关闭，则读取x为null），以及统计测试表明数据的分布接近预期（例如，数据的标准差在某个界限内）。'
- en: '*Tests for postconditions:* This test determines if what is produced is as
    expected. As for the previous tests for preconditions, you can establish something
    about the output of a data pipeline.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后置条件测试:* 此测试确定产生的结果是否符合预期。至于先前的先决条件测试，你可以对数据管道的输出建立一些认识。'
- en: '*Tests of constraints from precondition to postcondition:* Check that the expected
    relations between these conditions hold. For example, all non-null preconditions
    have a non-null post condition or that the same number of 0’s is present in the
    pre- and post-process data (if that is what is expected).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从先决条件到后置条件的约束测试:* 检查这些条件之间预期的关系是否成立。例如，所有非空先决条件都有一个非空的后置条件，或者预处理和后处理数据中0的数量相同（如果这是预期的）。'
- en: '*Tests of performance:* As mentioned, the data pipeline steps may have nonfunctional
    instrumentation: the amount of time, memory, cost, and number of invocations,
    for instance. These parameters are useful for monitoring the development infrastructure
    and will be important for monitoring the production infrastructure in the deployed
    ML system, but they can also be exploited for testing. These tests can be absolute
    (the task should complete in less than three seconds, for example), or they can
    be relative (the task should complete in no more than two standard deviations
    of the time for the task to normally complete). In the worst case, steps in the
    pipeline do not complete due to nonfunctional stresses or run out of order, creating
    significant corruption.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能测试:* 如前所述，数据管道步骤可能具有非功能性指标：例如时间、内存、成本和调用次数。这些参数对于监控开发基础设施很有用，对于监控部署的机器学习系统中的生产基础设施也非常重要，但它们也可以用于测试。这些测试可以是绝对的（例如，任务应在少于三秒内完成），也可以是相对的（任务应在正常完成任务时间的两个标准差内完成）。在最坏的情况下，由于非功能性压力或顺序错误，管道中的步骤可能无法完成，从而造成重大损坏。'
- en: '*Tests for reactions t*o extreme values, test null, NaN, large_number, small_number
    and 0 behaviors.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对极端值、空值、NaN、大数、小数和0行为的反应测试*。'
- en: Ideally the testing infrastructure feeds into a monitoring system. It’s important
    that system-level failures (such as a database becoming unreachable) are logged
    and notifications and alerts distributed. Automated alerting means that the infrastructure
    team can rapidly respond and sort things out before the ML team has to put down
    their tools and play table tennis instead.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，测试基础设施应集成到监控系统。系统级故障（如数据库不可达）的记录以及通知和警报的分发至关重要。自动警报意味着基础设施团队能够迅速响应并解决问题，以免机器学习团队不得不放下工具去打乒乓球。
- en: 5.5 Model repository and model versioning
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 模型存储库和模型版本控制
- en: Task S1.4 requires the team to build the model management and versioning infrastructure.
    This allows an agile, yet controlled development and deployment of the ML models
    that will be developed in sprint 2.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 任务S1.4要求团队构建模型管理和版本控制基础设施。这允许敏捷且受控地开发和部署在冲刺2中开发的机器学习模型。
- en: 'Model versioning ticket: S1.4'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 模型版本控制工单：S1.4
- en: Commission and adopt a model repository.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 授权并采用模型存储库。
- en: Identify and record all artefacts for use in ML pipelines.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别并记录所有工件以用于机器学习管道。
- en: During system development, it is expected that each model will require a large
    number of iterations to find the appropriate combination of training process,
    hyperparameters, algorithm, architecture, and so on. In order to manage the experiments
    generated by a typical iterative modeling process, you need to implement a model
    repository. The repository records the specific model created during a particular
    iteration or experiment, the parameters (data, features), hyperparameters, algorithm,
    architecture, and other model components, along with the evaluation metrics for
    this iteration.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统开发过程中，预计每个模型都需要进行大量迭代以找到适当的训练过程、超参数、算法、架构等组合。为了管理典型迭代建模过程产生的实验，您需要实现模型存储库。存储库记录了特定迭代或实验期间创建的特定模型，参数（数据、特征）、超参数、算法、架构以及其他模型组件，以及此迭代的评估指标。
- en: 'An evolution of the model chosen and the information that informed the decision
    making that drove the evolution are important factors that determine how the model
    will behave in the future and how governable the developed system will be. Recording
    the models’ evolution can inform investigations into the system’s future stability,
    failures, and blind spots. By maintaining this information, the system becomes
    accountable, and you can document your choices with factual evidence. You’ll use
    the model repository to record the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择及其决策背后的信息演变是决定模型未来行为以及开发系统可管理性的重要因素。记录模型的演变可以告知对系统未来稳定性、故障和盲点的调查。通过维护这些信息，系统变得可问责，您可以使用事实证据记录您的选择。您将使用模型存储库来记录以下内容：
- en: The identity of each model, which is the name that links to the binary or declarative
    specification that is used in testing and production (but ideally, both).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个模型的身份，即链接到用于测试和生产（但理想情况下，两者都使用）的二进制或声明性规范的名称。
- en: The evaluation results that were created for the model during the development
    process.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发过程中为模型创建的评估结果。
- en: The test results (if any) that were created for the model during qualification
    and selection.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在资格认证和选择模型期间为该模型创建的测试结果（如果有）。
- en: A list of all technical artifacts used by the model and those that were used
    to develop it (for example, foundation models or features).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型使用的所有技术工件及其开发中使用的工件列表（例如，基础模型或特征）。
- en: The status of the data pipeline (running or not) and the data pipeline identities
    that were used to provide the training, validation, and test sets.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道的状态（运行或不运行）以及用于提供训练、验证和测试集的数据管道标识符。
- en: The test results and monitoring information created when the pipelines were
    used to build the training, validation, and test sets.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用管道构建训练、验证和测试集时创建的测试结果和监控信息。
- en: Beyond implementing the model repository, the team must commit to using it.
    The core task of modeling has not yet started, but early experiments, off-the-book
    tests, and baseline developments are information that should be captured because
    this sets the context for the rest of the project.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实现模型存储库之外，团队必须承诺使用它。建模的核心任务尚未开始，但早期实验、非正式测试和基线开发都是应该捕获的信息，因为这为项目的其余部分设定了背景。
- en: 5.5.1 Features, foundational models, and training regimes
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 特征、基础模型和训练方案
- en: Providing a model versioning system is important because it provides control
    and automation for the team when they build, evaluate, and integrate models in
    production. As mentioned, there are other artefacts that the team will likely
    use in the project. The infrastructure to handle these is going to be important
    to the smooth delivery of the project as well.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一个模型版本控制系统非常重要，因为它为团队在生产中构建、评估和集成模型时提供了控制和自动化。如前所述，团队在项目中可能会使用其他工件。处理这些工件的基础设施对于项目的顺利交付也同样重要。
- en: You should record and track explicitly all the tools and components that the
    team uses. The editors, interpreters, compilers, libraries, and virtual machines
    all need to be recorded and approved by the customer. Many clients have architectural
    compliance policies that cover the validation and selection of libraries and tools
    for software projects; these almost always have exception processes to allow a
    new tool to be adopted if necessary. You can’t dodge these processes, so use them
    to either get assurance that your toolset is compliant or to get it formally registered
    as an exception.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该明确记录和跟踪团队使用的所有工具和组件。编辑器、解释器、编译器、库和虚拟机都需要被记录并经客户批准。许多客户都有针对软件项目库和工具的验证和选择架构合规政策；这些政策几乎总是有例外流程，以便在必要时采用新工具。你不能规避这些流程，所以利用它们来确保你的工具集符合规范，或者正式将其注册为例外。
- en: The development of reusable foundation models created a new class of artifact
    that is pivotal to an ML project. It’s good to check licensing conditions and
    to make sure that the customer knows the model that’s used, and it’s essential
    that it’s registered in their repositories and catalogs. Also essential is making
    sure that you use the right versions of the foundation model in all processes
    and pipelines. Generating embeddings using one model and then trying to match
    them using a different model generally creates poor results. Using a hash function
    such as MD5 [2] to create a unique identifier for the model and then embedding
    that into the model-serving code as an on-load check guarantees that the correct
    model is used in production as well as in development and test environments.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 可重用基础模型的开发创造了一种新的工件类别，这对于机器学习项目至关重要。检查许可条件并确保客户知道所使用的模型是好的，而且它必须在他们的存储库和目录中注册。同样重要的是确保你在所有流程和管道中使用正确的基础模型版本。使用一个模型生成嵌入，然后尝试使用不同的模型进行匹配通常会产生较差的结果。使用MD5
    [2]之类的哈希函数为模型创建一个唯一的标识符，并将其嵌入到模型服务代码中的加载检查，可以保证在生产、开发和测试环境中都使用正确的模型。
- en: Of course, the team will use specific libraries and tools to provide the algorithms
    that will extract the model from the data. Versioning is again, important. Watch
    for anomalies like the team adopting a nightly build or downloading a build in
    defiance (or ignorance) of corporate policies. Expect builds to be explicitly
    checked by security teams before production and understand that a rogue build
    may not only result in a corporate slap on the wrists (or your summary dismissal)
    but also in a dependency issue that then prevents deployment.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，团队将使用特定的库和工具来提供从数据中提取模型的算法。版本控制再次显得非常重要。注意团队是否采用夜间构建或无视（或无知）公司政策下载构建版本等异常情况。预期构建版本在生产前会被安全团队明确检查，并理解一个恶意构建不仅可能导致公司对你进行处罚（或你的总结性解雇），还可能导致依赖性问题，从而阻止部署。
- en: Another pitfall that can arise is testing builds to avoid issues with licensing
    costs or to get over the lack of particular hardware in tests. For example, a
    virtual machine used in testing may dispense with floating-point calculations
    in order to enable rapid completion of integration or system tests. This is a
    normal thing for deployment and platform teams to set up, but if the test virtual
    machine or library somehow makes it into the production build, then the production
    system won’t work.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能出现的陷阱是测试构建以避免与许可成本相关的问题或弥补测试中特定硬件的缺乏。例如，用于测试的虚拟机可能省略浮点计算，以便快速完成集成或系统测试。这对于部署和平台团队来说是一个正常的事情，但如果有测试虚拟机或库意外地进入生产构建，那么生产系统将无法工作。
- en: 5.5.2 Overview of versioning
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 版本控制概述
- en: 'ML is a fast-moving area, and new components that support an ML system are
    emerging rapidly, so it’s impossible to enumerate every required item to be versioned
    to make a solid production system. However, implementing some systematic tests
    and processes in the project to control versioning helps. Tests that you can use
    to develop confidence in the system’s versioning include:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个快速发展的领域，支持机器学习系统的新组件正在迅速出现，因此不可能列出所有需要版本化的项目来构建一个坚实的生产系统。然而，在项目中实施一些系统性的测试和流程来控制版本化是有帮助的。您可以使用以下测试来建立对系统版本化的信心：
- en: Using checksums to establish that the right information is present in the artifact.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用校验和来确认工件中存在正确的信息。
- en: Using signed binaries to establish that the right owner and source of the artifact
    is in use and that it can be trusted.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用签名二进制文件来确认正在使用正确的所有者和来源的工件，并且它可以被信任。
- en: Sampling known values from the binary, then checking that they are correct.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从二进制中采样已知值，然后检查它们是否正确。
- en: In many projects and settings, taking these steps to identify, register, secure,
    and validate the project’s dependencies may seem like overkill. You can be assured
    it really is worthwhile! This enables problems to be tracked, discovered early,
    and quickly resolved. It will also allow the CI/CD processes required for rapid
    and effective production to be smoothly implemented. Build management systems
    like Jenkins, GCP Cloud Build, and AWS Code Build need to be hydrated with the
    right versions of all components to deliver the right configuration for the system
    into production. Good control of the component versions allows configurations
    to be rapidly updated and delivered. If you end up doing it by hand, then expect
    very slow going.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多项目和环境中，采取这些步骤来识别、注册、保护和验证项目的依赖项可能看起来有些过度。但请放心，这确实是值得的！这使问题能够被追踪、及早发现并迅速解决。它还将允许顺利实施快速有效的生产所需的CI/CD流程。像Jenkins、GCP
    Cloud Build和AWS Code Build这样的构建管理系统需要用所有组件的正确版本进行填充，以便将正确的系统配置部署到生产中。对组件版本的良好的控制允许快速更新和交付配置。如果您手动完成这项工作，那么请预期进度会非常缓慢。
- en: With data pipelines, data testing, and model and feature stores in place, commissioned
    and adopted by the team, it’s now feasible for the team to dive into the data.
    Once the team can easily manipulate the data in place, they can get a real sense
    of what it is they’ve got for modelling. Everything is now in place for work to
    begin on the EDA and then on the model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据管道、数据测试、模型和特征存储被团队采用并启动后，团队现在可以深入挖掘数据。一旦团队可以轻松地操作现有数据，他们就可以真正地感受到他们为建模所拥有的数据。现在，一切准备就绪，可以开始进行EDA工作，然后是模型开发。
- en: This chapter covered the work the team needed to do to come to grips with the
    real technical problems that they are going to face before delivering the project.
    We discussed mapping out the data resources and understanding what issues need
    to be further explored and checked, building insight into the business issues
    that can be resolved, setting up data pipelines that support the exploration of
    the data assets and their use in modelling, and commissioning the model versioning
    infrastructure. In chapter 6, we’ll look at the rest of sprint 1, including the
    EDA process that will deepen the team’s insight into the statistical properties
    of the data set and the development of the first baseline ML models. At the end
    of chapter 6, you will find The Bike Shop narrative that covers all of the work
    in sprint 1\.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了团队在交付项目之前需要做的工作，以应对他们将要面临的真实技术问题。我们讨论了绘制数据资源图、理解需要进一步探索和检查的问题、建立对可解决业务问题的洞察、建立支持数据资产探索及其在建模中使用的数据管道，以及启动模型版本化基础设施。在第6章中，我们将回顾1号冲刺的其余部分，包括深化团队对数据集统计特性的洞察的EDA过程以及第一个基线机器学习模型的开发。在第6章的结尾，您将找到涵盖1号冲刺所有工作的《自行车店》叙述。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A data survey establishes that the expected data resources exist and have a
    level of integrity that will allow the team to meaningfully work on them.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据调查确定预期的数据资源存在，并且具有允许团队有意义地工作的完整性水平。
- en: By developing story cards and UX prototypes, you’ll generate a deeper understanding
    and agreement about the direction of the project and the requirements on the ML
    modeling activity that’s at the core of the project’s hypothesis.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过开发故事卡和UX原型，您将更深入地理解并就项目的方向以及项目假设核心的机器学习建模活动的需求达成一致。
- en: Model repositories and versioning infrastructures for all the artefacts required
    in the project development need to be established, commissioned, and adopted (turn
    them on and use them).
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要建立、委托和采用（开启并使用）项目开发所需的所有工件（模型）的存储库和版本控制基础设施。
- en: Systematically build a data pipeline infrastructure to support agile development
    of modeling later in the project. The pipeline must provide support for ingesting
    the data, transforming it for use, and providing access to it for the modeling
    team.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统性地构建数据管道基础设施，以支持项目后期建模的敏捷开发。该管道必须提供支持数据摄取、转换以供使用以及为建模团队提供访问的功能。
- en: Take careful note of the motivations and approach to data gathering for the
    data resources that the project uses.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细记录项目使用的数据资源的数据收集动机和方法。
- en: Establish infrastructures for data testing and data pipeline testing to provide
    assurance during model development and into production that the team is working
    with the correct data.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立数据测试和数据管道测试的基础设施，以确保在模型开发和生产过程中，团队使用的是正确的数据。
