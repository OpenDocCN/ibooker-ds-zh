- en: 4 Diversity sampling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 多样性采样
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using outlier detection to sample data that is unknown to your current model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用异常检测来采样当前模型未知的数据
- en: Using clustering to sample more diverse data before annotation starts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在标注开始之前，使用聚类来采样更多样化的数据
- en: Using representative sampling to target data most like where your model is deployed
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用代表性采样来针对与模型部署位置最相似的数据
- en: Improving real-world diversity with stratified sampling and active learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分层采样和主动学习提高现实世界的多样性
- en: Using diversity sampling with different types of machine learning architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同类型的机器学习架构进行多样性采样
- en: Evaluating the success of diversity sampling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估多样性采样的成功
- en: 'In chapter 3, you learned how to identify where your model is uncertain: what
    your model “knows it doesn’t know.” In this chapter, you will learn how to identify
    what’s missing from your model: what your model “doesn’t know that it doesn’t
    know” or the “unknown unknowns.” This problem is a hard one, made even harder
    because what your model needs to know is often a moving target in a constantly
    changing world. Just like humans are learning new words, new objects, and new
    behaviors every day in response to a changing environment, most machine learning
    algorithms are deployed in a changing environment.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，你学习了如何识别模型的不确定性：模型“知道它不知道”的地方。在本章中，你将学习如何识别模型中缺失的部分：模型“不知道它不知道”的“未知未知”，或称为“未知未知”。这个问题很棘手，因为模型需要知道的信息通常是一个在不断变化的世界中的移动目标。就像人类每天都会学习新单词、新物体和新行为来适应不断变化的环境一样，大多数机器学习算法都是在不断变化的环境中部署的。
- en: For example, if we are using machine learning to classify or process human language,
    we typically expect the applications to adapt to new words and meanings, rather
    than remain stale and understand the language only up to one historical point
    in time. We’ll explore a couple of use cases in speech recognition and computer
    vision in the upcoming chapters to illustrate the value of diversity sampling
    for different kinds of machine learning problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们使用机器学习来分类或处理人类语言，我们通常期望应用能够适应新单词和新含义，而不是保持陈旧，只理解历史某个时间点的语言。在接下来的章节中，我们将探讨语音识别和计算机视觉中的几个用例，以说明多样性采样对不同类型机器学习问题的价值。
- en: Imagine that your job is to build a voice assistant that can be successful for
    as many users as possible. Your company’s leaders expect your machine learning
    algorithms to have much broader knowledge than any one human. A typical English
    speaker knows about 40,000 words of English’s 200,000-word vocabulary, which is
    only 20% of the language, but your model should have closer to 100% coverage.
    You have a lot of unlabeled recordings that you can label, but some of the words
    that people use are rare. If you randomly sampled the recordings, you would miss
    the rare words. So you need to explicitly try to get training data covering as
    many different words as possible. You might also want to see what words are most
    commonly used when people speak to their voice assistants and sample more of those
    words.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的工作是构建一个可以为尽可能多的用户成功的语音助手。你公司的领导期望你的机器学习算法比任何一个人都拥有更广泛的知识。典型的英语使用者知道大约40,000个单词，占英语20万词汇量的20%，但你的模型应该接近100%的覆盖率。你有很多未标记的录音可以标记，但其中一些单词的使用频率很低。如果你随机采样这些录音，你会错过这些罕见的单词。因此，你需要明确尝试获取尽可能覆盖不同单词的训练数据。你也可能想了解人们在与其语音助手交谈时最常使用的单词，并采样更多这些单词。
- en: You are also worried about demographic diversity. The recordings are predominantly
    from one gender and from people living in a small number of locations, so resulting
    models are likely to be more accurate for that gender and for only some accents.
    You want to sample as fairly as possible from different demographics to make the
    model equally accurate for all demographics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你还担心人口统计学上的多样性。录音主要来自一个性别，以及生活在少数几个地区的人，因此生成的模型可能只对那个性别和某些口音更准确。你希望尽可能公平地从不同的群体中采样，以使模型对所有群体都同样准确。
- en: Finally, many people don’t speak English and would like a voice assistant, but
    you have little non-English data. You may have to be open and honest about this
    limitation in diversity.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多人不会说英语，但希望有一个语音助手，但你几乎没有非英语数据。你可能不得不公开诚实地承认这种多样性方面的限制。
- en: This problem is harder than simply knowing when your model is confused, so the
    solutions for diversity sampling are themselves more algorithmically diverse than
    those for uncertainty sampling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题比仅仅知道模型何时困惑更难，因此多样性采样的解决方案本身在算法上比不确定性采样的解决方案更加多样化。
- en: '4.1 Knowing what you don’t know: Identifying gaps in your model’s knowledge'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 了解你所不知道的：识别模型知识中的差距
- en: 'We explore four approaches to diversity sampling in this chapter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了四种多样性采样的方法：
- en: '*Model-based outlier sampling*—Determining which items are unknown to the model
    in its current state (compared with uncertain, as in chapter 3). In our voice
    assistant example, model-based outlier sampling would help identify words that
    our voice assistant hasn’t encountered before.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于模型的异常值采样*—确定在当前状态下模型不知道哪些项目（与第三章中的不确定相比）。在我们的语音助手例子中，基于模型的异常值采样将有助于识别我们的语音助手之前未曾遇到过的单词。'
- en: '*Cluster-based sampling*—Using statistical methods independent of your model
    to find a diverse selection of items to label. In our example, cluster-based sampling
    would help identify natural trends in the data so we can make sure that we don’t
    miss any rare but meaningful trends.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于聚类的采样*—使用独立于你的模型的统计方法来找到用于标记的多样化项目选择。在我们的例子中，基于聚类的采样将有助于识别数据中的自然趋势，这样我们就可以确保不遗漏任何罕见但有意义的变化。'
- en: '*Representative sampling*—Finding a sample of unlabeled items that look most
    like your target domain, compared with your training data. In our example, let’s
    imagine that people used your voice assistant mostly to request songs. Representative
    sampling, therefore, would target examples of song requests.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代表性采样*—找到与你的目标领域最相似的未标记项目样本，与你的训练数据相比。在我们的例子中，让我们假设人们主要使用你的语音助手来请求歌曲。因此，代表性采样将针对歌曲请求的示例。'
- en: '*Sampling for real-world diversity*—Ensuring that a diverse range of real-world
    entities are in our training data to reduce real-world bias. In our example, this
    approach could include targeting recordings for as many accents, ages, and genders
    as possible.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*针对现实世界多样性的采样*—确保我们的训练数据中包含各种现实世界实体的多样性，以减少现实世界的偏差。在我们的例子中，这种方法可能包括针对尽可能多的口音、年龄和性别进行录音。'
- en: As you learned in the book’s introduction, the phrase *uncertainty sampling*
    is widely used in active learning, but *diversity sampling* goes by different
    names in different fields, often tackling only part of the problem. You may have
    seen diversity sampling referred to as stratified sampling, representative sampling,
    outlier detection, or anomaly detection. A lot of the time, the algorithms that
    we use for diversity sampling are borrowed from other use cases. Anomaly detection,
    for example, is primarily used for tasks such as identifying new phenomena in
    astronomical databases or detecting strange network activity for security.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书的引言中所学到的，短语*不确定性采样*在主动学习中广泛使用，但*多样性采样*在不同领域有不同的名称，通常只解决部分问题。你可能看到多样性采样被称为分层采样、代表性采样、异常检测或异常检测。很多时候，我们用于多样性采样的算法是从其他用例借用的。例如，异常检测主要用于识别天文数据库中的新现象或检测安全中的异常网络活动等任务。
- en: 'So as not to confuse the non-active learning use cases and to provide consistency,
    we’ll use the phrase *diversity sampling* in this text. This phrase intentionally
    invokes diversity in the sense of the demographics of people represented in the
    data. Although only the fourth kind of diversity sampling that we look at explicitly
    targets demographic diversity, the other three types correlate with real-world
    diversity. Chances are that your unlabeled data is biased toward the most privileged
    demographics: languages from the wealthiest nations, images from the wealthiest
    economies, videos created by the wealthiest individuals, and other biases that
    result from power imbalances. If you build models only on randomly sampled raw
    data, you could amplify that bias. Any method that increases the diversity of
    the items that you sample for active learning will likely increase the diversity
    of the people who can benefit from models built from that data.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不混淆非主动学习用例并提供一致性，我们将在本文中使用“多样性采样”这个短语。这个短语故意唤起数据中代表的人口的多样性。尽管我们明确针对的是第四种多样性采样，旨在实现人口多样性，但其他三种类型与真实世界的多样性相关。你的未标记数据可能偏向于最有特权的群体：来自最富裕国家的语言、来自最富裕经济体的图像、由最富裕的个人创建的视频，以及其他由权力不平衡产生的偏见。如果你只基于随机采样的原始数据构建模型，可能会放大这种偏见。任何增加你为主动学习采样的项目多样性的方法，都可能增加从这些数据构建的模型中受益的人的多样性。
- en: Even if you are not worried about biases in demographics of people, you probably
    still want to overcome sample bias in your data. If you are processing images
    from agriculture and happen to have one type of crop overrepresented in your raw
    data, you probably want a sampling strategy that will rebalance the data to represent
    many types of crops. Also, there may be deeper biases related to people. If you
    have more examples of one type of crop, is that crop more common in wealthier
    countries, and do you have more photographs because tractors in wealthier countries
    are more likely to have cameras? Data bias and real-world bias tend to be closely
    related when we dig deep. Figure 4.1 repeats the example of diversity sampling
    that you saw in chapter 1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不在意人口统计学上的偏见，你可能仍然想要克服数据中的样本偏差。如果你正在处理农业图像，并且你的原始数据中恰好有一种作物过度代表，你可能需要一个采样策略来重新平衡数据，以代表许多类型的作物。此外，还可能存在与人群相关的更深层次的偏见。如果你有一种作物的例子更多，这种作物在富裕国家是否更常见，你是否有更多的照片，因为富裕国家的拖拉机更有可能配备摄像头？当我们深入挖掘时，数据偏差和现实世界偏差往往密切相关。图4.1重复了你在第一章中看到的多样性采样示例。
- en: '![](../Images/CH04_F01_Munro.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F01_Munro.png)'
- en: Figure 4.1 Diversity sampling, showing items selected to be labeled that are
    maximally different from the existing training items and from one another. You
    want to sample items that are not like the items that are currently in your training
    data and are also not like one another.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 多样性采样，展示了被选中的用于标记的项目，这些项目与现有的训练项目以及彼此之间差异最大。你想要采样与当前训练数据中的项目不同，并且彼此之间也不相似的项目。
- en: For uncertainty sampling, you want to see only what is near your current decision
    boundary or what varies most across multiple predictions—a relatively small and
    well-defined feature space. For diversity sampling, you want to explore the much
    larger problem of every corner of the feature space and expand the decision boundary
    into new parts of that space. Needless to say, the set of algorithms that you
    can use are more diverse and sometimes more complicated than those used for uncertainty
    sampling.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不确定性采样，你只想看到靠近你当前决策边界的内容，或者在不同预测中变化最大的内容——一个相对较小且定义明确的特征空间。对于多样性采样，你想要探索特征空间中每个角落的更大问题，并将决策边界扩展到该空间的新部分。不用说，你可以使用的算法集比不确定性采样更丰富，有时也更复杂。
- en: You may not need to worry about every data point if you are looking only at
    academic datasets, but the problem of diversity is much more common in real-world
    datasets. See the following sidebar for more information about the difference
    between real-world and academic datasets.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只关注学术数据集，可能不需要担心每个数据点，但在真实世界数据集中，多样性问题更为常见。请参阅以下侧边栏，了解更多关于真实世界数据集与学术数据集之间差异的信息。
- en: The difference between academic and real-world data labeling
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 学术数据与真实世界数据标注之间的差异
- en: '*Expert anecdote by Jia Li*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*专家观点，来自贾莉*'
- en: It is much harder to deploy machine learning in the real world than for academic
    research, and the main difference is the data. Real-world data is messy and often
    hard to access due to institutional hurdles. It is fine to conduct research on
    clean, unchanging datasets, but when you take those models into the real world,
    it can be hard to predict how they will perform.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中部署机器学习比学术研究要困难得多，主要区别在于数据。现实世界的数据杂乱无章，往往由于制度障碍而难以获取。在干净、不变的数据集上进行研究是可以的，但当你将这些模型带入现实世界时，很难预测它们的性能。
- en: When I was helping to build ImageNet, we didn’t have to worry about every possible
    image class that we might encounter in the real world. We could limit the data
    to images that were a subset of concepts in the WordNet hierarchy. In the real
    world, we don’t have that luxury. For example, we can’t collect large amounts
    of medical images related to rare diseases. Labeling of such images further requires
    domain expertise, which poses even more challenges. Real-world systems need both
    AI technologists and domain experts collaborating closely to inspire research,
    provide data and analysis, and develop algorithms to solve the problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在帮助构建ImageNet时，我们不必担心在现实世界中可能遇到的每一个可能的图像类别。我们可以将数据限制在WordNet层次结构中概念子集的图像。在现实世界中，我们没有这种奢侈。例如，我们无法收集大量与罕见疾病相关的医学图像。这些图像的标注还需要领域专业知识，这带来了更多的挑战。现实世界系统需要AI技术专家和领域专家紧密合作，以激发研究、提供数据和数据分析，并开发算法来解决这些问题。
- en: '*Jia Li is CEO and co-founder of Dawnlight, a health-care company that uses
    machine learning. She previously led research divisions at Google, Snap, and Yahoo!,
    and has a PhD from Stanford*.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*李佳是Dawnlight医疗公司的首席执行官和联合创始人，该公司使用机器学习。她之前曾领导过Google、Snap和Yahoo！的研究部门，并在斯坦福大学获得博士学位*。'
- en: 4.1.1 Example data for diversity sampling
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 多样性采样的示例数据
- en: 'In this chapter, we will build on our example from chapter 2 with disaster
    response messages. Recall from chapter 2 that we wanted to label news headlines
    as disaster-related or not disaster-related. We implemented a basic outlier detection
    algorithm in that chapter, which we will now expand on with more sophisticated
    diversity sampling algorithms. The code is in the same library that you used for
    chapter 2: [https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning).
    The code that we will use in this chapter is in these two files: diversity_sampling.py
    and active_learning.py.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将基于第二章中的示例，即灾害响应信息。回想一下第二章，我们想要将新闻标题标注为与灾害相关或不相关。在那一章中，我们实现了一个基本的异常检测算法，现在我们将通过更复杂的多样性采样算法来扩展它。代码位于与第二章相同的库中：[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)。本章我们将使用的代码在这两个文件中：diversity_sampling.py和active_learning.py。
- en: We will cover many types of diversity sampling strategies in this chapter. For
    our example data, you can imagine that a machine learning model could be useful
    for tracking disasters as they are being reported and to distinguish eyewitness
    reports from secondhand (or thirdhand) information. If you want to deploy this
    kind of system to track disasters in real time, you want as diverse a set of past
    training data items as possible. Only one or two news articles about floods might
    have been reported in your past training data, for example, which could easily
    have been missed if you chose items at random for humans to label.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍多种多样性采样策略。对于我们的示例数据，你可以想象一个机器学习模型在灾害发生时进行跟踪，以及区分目击者报告和二手（或三手）信息是有用的。如果你想要部署这种系统来实时跟踪灾害，你希望尽可能拥有多样化的过去训练数据项。例如，过去训练数据中可能只有一两条关于洪水的新闻报道，如果你随机选择项目供人类标注，很容易就会遗漏。
- en: You can also imagine new types of disasters, such as disease outbreaks that
    have a pattern of infection that hasn’t been observed before. If people are talking
    about these new disasters in new ways, you want to ensure that you aren’t missing
    these items and that the items get human labels as quickly as possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以想象新的灾害类型，例如具有以前未观察到的感染模式的疾病爆发。如果人们以新的方式谈论这些新灾害，你想要确保你不会错过这些项目，并且这些项目能够尽快获得人类标注。
- en: Finally, you might want to start incorporating new sources of data. If some
    of the new sources are US English instead of UK English, if they use different
    slang, or if they are not in English, your model will not be accurate on those
    new sources of information. You want to make sure that your model can adapt to
    these new data sources and their stylistic differences as quickly as possible,
    as it is adapting to new types of information in the text itself.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能想要开始整合新的数据来源。如果其中一些新来源是美式英语而不是英式英语，或者它们使用不同的俚语，或者它们不是英语，那么你的模型在这些新的信息来源上可能不会准确。你想要确保你的模型能够尽可能快地适应这些新的数据来源及其风格差异，因为它本身也在适应文本中的新类型信息。
- en: It is important to reduce bias at every step. If you use your model predictions
    to find more examples of floods, but your existing model only has data from floods
    in Australia, you might get more examples from floods in Australia for human review
    and from no other part of the world, so you would never get away from the initial
    bias in your model. For that reason, most diversity sampling algorithms are independent
    of the model we are using.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中减少偏差都很重要。如果你使用你的模型预测来找到更多洪水示例，但你的现有模型只有来自澳大利亚洪水的数据，那么你可能会从澳大利亚获得更多洪水示例供人类审查，而来自世界其他地方的示例则没有，因此你永远无法摆脱模型中的初始偏差。出于这个原因，大多数多样性采样算法与我们使用的模型无关。
- en: 4.1.2 Interpreting neural models for diversity sampling
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 解释用于多样性采样的神经网络模型
- en: For some of the sampling strategies in this chapter, we will need new ways to
    interpret our models. If you access the raw outputs of a linear activation function
    in your final layer instead of the softmax output, you can more accurately separate
    true outliers from items that are the result of conflicting information. An activation
    function that also includes a negative range, like Leaky ReLU, is ideal; otherwise,
    you might end up with a lot of zeroed-out scores with no way to determine which
    is the biggest outlier.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中的一些采样策略，我们需要新的方法来解释我们的模型。如果你访问你最终层的线性激活函数的原始输出而不是softmax输出，你可以更准确地分离真正的异常值和由冲突信息导致的项目。包括负范围的激活函数，如Leaky
    ReLU，是理想的；否则，你可能会得到很多归零的分数，无法确定哪个是最大的异常值。
- en: You will learn how to access and interpret the different layers of a PyTorch
    model in section 4.1.3\. But you may not have a say about the architecture of
    the activation function in the final layer. Softmax may the most accurate activation
    function for predicting labels precisely because it can ignore the absolute values
    of its inputs. In these cases, you may still be able to persuade your algorithms
    team to expose other layers for analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在第4.1.3节中学习如何访问和解释PyTorch模型的不同层。但你可能无法对最后一层的激活函数架构发表意见。Softmax可能是预测标签最准确的激活函数，因为它可以忽略其输入的绝对值。在这些情况下，你可能仍然能够说服你的算法团队公开其他层以供分析。
- en: What if I don’t control my model architecture?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我不控制我的模型架构怎么办？
- en: 'If you don’t have a say on the architecture of the predictive algorithm, you
    might be able to convince your algorithms team to expose the logits or retrain
    only the final layer of the model with a Leaky ReLU activation function. Retraining
    the last layer of the model will be orders of magnitude faster than retraining
    an entire model. This approach should appeal to someone who is worried about the
    cost of retraining: they are supporting a new use case for not much extra work
    with a fun parallel architecture. If you are using Transformer models, the same
    concept applies, but you would train a new Attention head. (Don’t worry if you
    are not familiar with Transformer models; they are not important for this chapter.)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法对预测算法的架构发表意见，你可能能够说服你的算法团队公开logits或仅使用Leaky ReLU激活函数重新训练模型的最后一层。重新训练模型的最后一层将比重新训练整个模型快得多。这种方法应该会吸引那些担心重新训练成本的人：他们通过支持一个不需要额外太多工作的有趣并行架构来支持一个新的用例。如果你使用Transformer模型，同样的概念也适用，但你将训练一个新的注意力头。（如果你不熟悉Transformer模型，不用担心；它们对本章来说并不重要。）
- en: If you encounter resistance to the idea of retraining the last layer, or if
    there are technical barriers, your next-best option is to use the second-to-last
    layer of the model. Regardless, it might be interesting to compare outlier sampling
    methods on different layers of the model to see what works best with your particular
    data and model architecture. These kinds of model analytics are some of the most
    exciting areas of machine learning research today and also allow for transfer
    learning, which is used in techniques in most of the upcoming chapters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到对重新训练最后一层想法的阻力，或者如果存在技术障碍，你的下一个最佳选择是使用模型的倒数第二层。无论如何，比较模型不同层的异常值采样方法，看看哪种方法最适合你的特定数据和模型架构，可能会很有趣。这类模型分析是当今机器学习研究中最激动人心的领域之一，同时也允许迁移学习，这在即将到来的章节中的技术中得到了应用。
- en: In this chapter, we will limit ourselves to simple but effective ways to interpret
    your model. The two scenarios in figure 4.2 interpret the last layer or the second-to-last
    layer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将限制自己使用简单但有效的方式来解释你的模型。图 4.2 中的两种场景解释了最后一层或倒数第二层。
- en: The second method, using the second-to-last layer, work bests on deeper networks
    in which the second-to-last layer more closely resembles the last layer and there
    are fewer neurons in that layer. More neurons will introduce more random variability
    that can be more difficult to overcome, statistically.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法，使用倒数第二层，在更深层的网络中效果最好，其中倒数第二层更接近最后一层，并且该层中的神经元更少。更多的神经元会引入更多的随机变异性，这在统计上可能更难克服。
- en: Regardless of which architecture you use, you are left with a set (vector/tensor)
    of numbers representing a level of activation at/near the output of your predictive
    model. For simplicity, we’ll refer to either vector as *z* even though z is typically
    reserved to mean only the logits from the last layer. We will also use *n* to
    indicate the size of that vector (the number of neurons), regardless of whether
    it happens to be the final layer and therefore also the number of labels or a
    middle layer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用哪种架构，你都会得到一个表示你的预测模型输出处/附近的激活水平的数字集合（向量/张量）。为了简单起见，我们将把这两个向量都称为 *z*，尽管
    z 通常仅保留表示最后一层的 logits。我们还将使用 *n* 来表示该向量的大小（神经元的数量），无论它是否恰好是最后一层，因此也是标签的数量或中间层。
- en: '*Low activation* means that this item is more likely to be an outlier. Mathematically,
    an outlier could be any unusual vector, atypically high or atypically low. But
    when we are interpreting model predictions to find outliers, we are concerned
    only with the low-activation items—the items that the model has little information
    about today.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*低激活*意味着这个项目更有可能是异常值。从数学上讲，异常值可以是任何不寻常的向量，可能是异常高或异常低。但当我们解释模型预测以寻找异常值时，我们只关注低激活项——即模型今天关于这些项目信息很少的项。'
- en: '![](../Images/CH04_F02_Munro.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F02_Munro.png)'
- en: Figure 4.2 Two neural architectures and how you can interpret them for outlier
    detection. In the top example, you can use the model scores (known as z or logits),
    which retain their absolute values before they are normalized via softmax. In
    the bottom example, you have lost the absolute values in the final layer because
    of the softmax function, so you can use the activation in the second-to-last layer
    to determine whether an item is an outlier.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 两种神经网络架构以及如何解释它们进行异常值检测。在上面的例子中，你可以使用模型得分（称为 z 或 logits），在它们通过 softmax
    归一化之前保留它们的绝对值。在下面的例子中，由于 softmax 函数，你在最终层失去了绝对值，因此你可以使用倒数第二层的激活来确定一个项目是否是异常值。
- en: 4.1.3 Getting information from hidden layers in PyTorch
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 从 PyTorch 的隐藏层获取信息
- en: 'To get the z values (logits) of the values from hidden layers in the model,
    we will need to modify our code so that we can access this information. Fortunately,
    the code is simple in PyTorch. First, as a reminder, here’s the code from chapter
    2 that you used for the feed-forward steps in training and for generating the
    confidences and label predictions in inference:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取模型隐藏层中值的 z 值（logits），我们需要修改我们的代码，以便我们可以访问这些信息。幸运的是，在 PyTorch 中，代码很简单。首先，作为提醒，以下是你在第
    2 章中用于训练中的前馈步骤以及生成推理中的置信度和标签预测的代码：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can see that the middle layer and outputs are variables (`hidden1` and `output`)
    that hold the activation from each layer (PyTorch tensors in this case, which
    will be 1D arrays). So we can simply add a parameter to return all layers, modifying
    the code accordingly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到中间层和输出是变量（`hidden1` 和 `output`），它们保存了每一层的激活（在这种情况下是 PyTorch 张量，它们将是 1D
    数组）。因此，我们可以简单地添加一个参数来返回所有层，并相应地修改代码。
- en: Listing 4.1 Allowing our model to return hidden layers in addition to softmax
    values
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 允许我们的模型返回除了 softmax 值之外的所有隐藏层
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Same as the return function but pulled out into a variable
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与返回函数相同，但被提取到一个变量中
- en: ❷ The only real new line, returning all the layers when return_all_layers=True
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当 return_all_layers=True 时，唯一的真正新行是返回所有层
- en: That’s it! You’ll see this modified code in active_learning.py. Now we can use
    any part of our model to find outliers within the model. Also, we have other ways
    to query our model’s hidden layers.[¹](#pgfId-1008064) I prefer encoding the option
    explicitly in the inference function, as with the `forward()` function. We are
    going to query our model in many ways in future chapters, and this approach makes
    the simplest code to build on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！您将在 active_learning.py 中看到这个修改后的代码。现在我们可以使用模型的任何部分来找到模型内的异常值。此外，我们还有其他方法来查询模型隐藏层。[¹](#pgfId-1008064)
    我更喜欢在推理函数中显式编码选项，就像在 `forward()` 函数中一样。我们将在未来的章节中以多种方式查询我们的模型，这种方法使得构建最简单的代码变得简单。
- en: Good coding practices for active learning
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习的良好编码实践
- en: 'As a note on good coding practices, you may want to change the `return log_softmax`
    line in your `forward()` function to also return an array: `return [log_softmax]`.
    That way, your function is returning the same data type (an array) no matter what
    parameters are passed to it, which is a better software development practice.
    The downside is that it’s not backward-compatible, so you’ll have to change every
    piece of code that is calling the function. If you’re an experienced PyTorch user,
    you may be accustomed to using a feature in the function that knows when it is
    in training mode or evaluation mode. This feature can be handy for some common
    machine learning strategies, such as masking neurons in training but not when
    predicting. But resist the temptation to use this feature here; it is bad software
    development in this context because global variables make unit tests harder to
    write and will make your code harder to read in isolation. Use named parameters
    like `return_all_layers=True/False`; you want to extend code in the most transparent
    way possible.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为良好编码实践的说明，您可能希望将 `forward()` 函数中的 `return log_softmax` 行更改为也返回一个数组：`return
    [log_softmax]`。这样，无论传递给函数什么参数，函数都返回相同的数据类型（一个数组），这是一个更好的软件开发实践。缺点是它不具有向后兼容性，因此您必须更改调用该函数的每一块代码。如果您是经验丰富的
    PyTorch 用户，您可能习惯于使用函数中的一个功能，该功能知道它处于训练模式或评估模式。这个功能对于一些常见的机器学习策略很有用，例如在训练时屏蔽神经元但预测时不屏蔽。但请抵制在这里使用这个功能的诱惑；在这种情况下，这是不良的软件开发，因为全局变量使得编写单元测试更加困难，并且会使您的代码在独立阅读时更难以理解。使用命名参数，如
    `return_all_layers=True/False`；您希望以最透明的方式扩展代码。
- en: 'With the addition of code to access all layers of the model in inference, we
    can use that code to determine outliers. Recall that in chapter 2, you got the
    log probabilities from your model with this line:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加代码以访问模型的所有层进行推理，我们可以使用该代码来确定异常值。回想一下，在第 2 章中，您使用以下行从您的模型中获取了对数概率：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now you can choose which layer of the model you want to use by calling the
    function with this line:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以通过调用函数并使用以下行来选择您想要使用的模型层：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You have the hidden layer, the logits (z), and the log_probabilities of the
    model for your item.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您有隐藏层、logits（z）和模型物品的对数概率。
- en: Recall from chapter 3 and the appendix that our logits (scores from the last
    layer) lose their absolute values when converted to probability distributions
    via softmax. Figure 4.3 reproduces some of these examples from the expanded section
    on softmax in the appendix.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第 3 章和附录，我们的 logits（来自最后一层的分数）在通过 softmax 转换为概率分布时失去了它们的绝对值。图 4.3 重新生成了附录中
    softmax 扩展部分的一些示例。
- en: '![](../Images/CH04_F03_Munro.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F03_Munro.png)'
- en: Figure 4.3 Four identical probability distributions that are derived from different
    inputs via softmax with base e
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 通过 softmax（以 e 为底）从不同输入中导出的四个相同的概率分布
- en: Our probability distributions, therefore, don’t tell us the difference between
    uncertainty that derives from the lack of information (as in the left example
    in figure 4.3) and uncertainty due to conflicting but highly confident information
    (as in the right example in figure 4.3). So it is better to use the logits (scores
    from the last layer) to differentiate the two types of uncertainty.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的概率分布并不能告诉我们不确定性之间的区别，这种不确定性源于信息不足（如图4.3左例所示）和由于冲突但高度自信的信息（如图4.3右例所示）。因此，使用logits（最后一层的分数）来区分这两种类型的不确定性会更好。
- en: Beyond uncertainty, we can find outliers that were certain but wrong. The most
    valuable unlabeled items to label are those that were incorrectly predicted and
    far from the decision boundary—that is, items that the current model is predicting
    confidently but incorrectly. Low activation across all neurons is often a good
    signal that there is not yet enough training data with the features found in that
    item.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了不确定性之外，我们还可以找到那些确定但错误的异常值。最值得标记的无标签项目是那些预测错误且远离决策边界的项目——也就是说，当前模型自信但错误地预测的项目。所有神经元低激活通常是一个很好的信号，表明还没有足够多的训练数据包含在该项目中的特征。
- en: 4.2 Model-based outlier sampling
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 基于模型的异常值采样
- en: Now that we can interpret our model, we can query our model to find outliers.
    A *model outlier* in a neural model is defined as the item with the lowest activation
    in a given layer. For our final layer, this activation is the logits.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够解释我们的模型，我们可以查询模型以找到异常值。在神经网络中，*模型异常值*被定义为给定层中激活最低的项目。对于我们的最后一层，这种激活就是logits。
- en: 'The biggest barrier to choosing the right metric for determining an outlier
    is knowing the distribution of values from your neurons. You were taught in high
    school that any data point greater than three standard deviations from the mean
    is an outlier, but this is true only for normal distributions. Unfortunately,
    your linear activation functions are not creating normal distributions: they should
    be bimodally distributed if they are modeling your task accurately. If you’ve
    investigated models in the past, you’ll also know that some of the neurons might
    be modeling noise or simple passing through values and can vary even when you
    train a model twice on identical data. Furthermore, unless you have a simple architecture,
    you will have different activation functions for different parts of your network,
    so they will not be directly comparable.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 选择用于确定异常值的正确指标的最大障碍是了解你神经元值的分布。你在高中时被教导，任何大于平均数三个标准差的数据点都是异常值，但这只适用于正态分布。不幸的是，你的线性激活函数并没有创建正态分布：如果它们准确模拟你的任务，它们应该是双峰分布的。如果你过去调查过模型，你也会知道一些神经元可能是在模拟噪声或简单通过值，即使在相同的数据上训练模型两次，它们也可能变化。此外，除非你有简单的架构，否则你网络的各个部分将具有不同的激活函数，因此它们将无法直接比较。
- en: Just like we couldn’t trust the absolute values for confidence for uncertainty
    sampling, we can’t trust the absolute values of our neurons to determine outliers.
    But just like we could trust the ranked order confidence to find the most uncertain
    predictions, we can trust the ranked order of neuron activation to find the least
    activated. Rank order is a robust method that lets us avoid determining the actual
    distribution of activation in every neuron.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们不能相信不确定性采样的置信度的绝对值一样，我们也不能相信我们神经元的绝对值来确定异常值。但就像我们可以相信排名顺序的置信度来找到最不确定的预测一样，我们也可以相信神经元激活的排名顺序来找到最不活跃的。排名顺序是一种稳健的方法，它让我们避免确定每个神经元中激活的实际分布。
- en: 'Here’s a simple example of ranked order for determining how much of an outlier
    some item is. Let’s assume that we have made predictions for 10 items and that
    these predictions were the result from a neuron, ordered (ranked) from largest
    to smallest:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的例子，用于确定某个项目是异常值到什么程度。假设我们对10个项目进行了预测，并且这些预测是由一个神经元产生的，按从大到小的顺序排列（排名）：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The item with an activation of –0.36 (underlined) is the ninth-lowest of the
    10 items, so we can give it an outlier score of 9/10 = 0.9\. At either end of
    the scale, an item with –0.42 activation would have a score of 1.0, and an item
    with 2.43 activation would have a score of 0\. So we can convert this ranked order
    of activation for each neuron into a scale. The question, then, is what data to
    use to generate the ranking.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 激活值为-0.36（下划线）的项目是10个项目中的第九低，因此我们可以给它一个异常值分数为9/10 = 0.9。在刻度的两端，激活值为-0.42的项目将得到1.0的分数，而激活值为2.43的项目将得到0的分数。因此，我们可以将每个神经元的这种激活排名顺序转换为刻度。那么，问题是什么数据用于生成排名。
- en: 4.2.1 Use validation data to rank activations
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 使用验证数据对激活进行排名
- en: 'We can’t use the training data for our rankings, because the model has trained
    on that data and some neurons will have overfit that data more than others, so
    we have to use data from the same distribution as our training data. That is,
    we have to use a validation data set drawn from the same distribution as our training
    data. This is not a big difference from an implementation point of view: we simply
    calculate the rankings on the validation data and then use that ranking to get
    the outlier score on our unlabeled data, as you’ll see in this section.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能使用训练数据来排名，因为模型已经在这个数据上进行了训练，一些神经元可能比其他神经元更多地过度拟合了这些数据，因此我们必须使用与我们的训练数据具有相同分布的数据。从实现的角度来看，这并不是一个很大的区别：我们只是在验证数据上计算排名，然后使用这个排名来获取我们未标记数据的异常值分数，正如你将在本节中看到的那样。
- en: 'The main difference is that we will get values from the unlabeled data that
    are between two values in the rankings. We can use simple linear interpolation
    to calculate these values. Suppose that our validation data consists of only 10
    items, which happen to be the same as in section 4.2:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别是我们将获得排名中两个值之间的未标记数据值。我们可以使用简单的线性插值来计算这些值。假设我们的验证数据只包含10个项目，恰好与第4.2节中的相同：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now imagine an unlabeled item with a value of –0.35 (above where it would fall
    in the ranking). That value is halfway between the eighth- and ninth-lowest items,
    so we can give the item an outlier score of 8.5/10 = 85%. Similarly, if the unlabeled
    item has a value of –0.355, which is three-quarters of the distance between the
    eighth and ninth item, the score would be 87.5%. We treat values above the first
    item as 1 and values below the last item as 0, giving us a [0–1] range in which
    the biggest outlier has a value of 100%.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一个未标记的项目，其值为-0.35（高于其在排名中的位置）。这个值位于第八和第九低值项目之间的一半，因此我们可以给这个项目一个异常值分数为8.5/10
    = 85%。同样，如果未标记的项目值为-0.355，这是第八和第九项目之间距离的三分之四，分数将是87.5%。我们将高于第一个项目的值视为1，低于最后一个项目的值视为0，这给我们一个[0–1]的范围，其中最大的异常值分数为100%。
- en: 'There are different ways to combine the scores across the neurons for each
    item. It is statistically safest to take the average activation across all the
    neurons for each item. Especially if you are using activation from one of the
    hidden layers, you may have some neurons that are essentially spitting out random
    values and therefore generating a falsely high maximum for what would otherwise
    be an outlier. Your logits are more likely to be reliable for every value, so
    you could experiment with the equivalent of least confidence for logits: the lowest
    maximum score across all neurons. To see the results of model-based outlier sampling,
    run'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来组合每个项目的神经元之间的分数。从统计学的角度来看，最安全的方法是取每个项目所有神经元的平均激活值。特别是如果你使用的是隐藏层之一的激活值，你可能有一些神经元实际上在输出随机值，因此产生了虚假的最高值，这本来可能是一个异常值。你的logits对于每个值来说更有可能是可靠的，所以你可以尝试logits的最小置信度：所有神经元中的最低最大分数。为了查看基于模型的异常值采样的结果，请运行
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As in chapter 2, the code will choose this sampling strategy and select 95
    unlabeled items for you to annotate, along with 5 randomly selected items from
    remaining unlabeled items. As in chapter 2, you always want to include a small
    number of random items as a safety net. If you don’t want to evaluate any random
    items, you can add a `random=0` option:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如同第2章所述，代码将选择这种采样策略，并为你选择95个未标记的项目进行标注，以及从剩余未标记的项目中随机选择的5个项目。如同第2章所述，你总是希望包含少量随机项目作为安全网。如果你不想评估任何随机项目，可以添加`random=0`选项：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can play around with other numbers to see and/or annotate more or less than
    95, too. If you skipped chapter 2, you will first be asked to annotate a purely
    random sample until you have enough initial training and test options. This time
    spent annotating will be important for evaluating accuracy and understanding the
    data, so please do these annotations now if you didn’t previously!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试调整其他数字，看看和/或注释多于或少于 95 的结果。如果您跳过了第 2 章，您将首先被要求注释一个纯随机样本，直到您有足够的初始训练和测试选项。这次注释的时间对于评估准确性和理解数据非常重要，所以如果您之前没有做过，请现在就做这些注释！
- en: The code for calculating the rank model outlier score is broken into four chunks
    of code. The model outlier function takes the current model, the unlabeled data,
    and held-out validation data taken from the same distribution as the training
    data. First, we create the rankings on our held-out validation data, which you
    can see within diversity_ sampling.py.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 计算排名模型异常分数的代码被拆分为四个代码块。模型异常函数接受当前模型、未标记数据和从与训练数据相同分布中提取的保留验证数据。首先，我们在保留的验证数据上创建排名，您可以在
    diversity_ sampling.py 中看到这些。
- en: Listing 4.2 Get activation rankings using validation data
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 使用验证数据获取激活排名
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ We get the results from all model layers here.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在这里获取所有模型层的成果。
- en: ❷ We store the logit score for each validation item and each neuron.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们存储每个验证项和每个神经元的对数分数。
- en: ❸ Rank-order each neuron according the scores from the held-out validation data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据保留验证数据中的分数对每个神经元进行排名顺序。
- en: In the second step, we order each unlabeled data item according to each neuron.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们根据每个神经元对每个未标记的数据项进行排序。
- en: Listing 4.3 Code for model-based outliers in PyTorch
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 PyTorch 中基于模型的异常值代码
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Call to get the activation on validation data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调用以获取验证数据的激活。
- en: ❷ We get the results from all model layers here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们在这里获取所有模型层的成果。
- en: ❸ We get the rank order for each unlabeled item.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们为每个未标记项获取排名顺序。
- en: The ranking function takes the activation value for one unlabeled item for one
    neuron and the rankings for that neuron that were calculated on the validation
    data. Use the following code for ordering each unlabeled item according to the
    validation rankings.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 排名函数接受一个未标记项的一个神经元的激活值和该神经元在验证数据上计算的排名。使用以下代码根据验证排名对每个未标记项进行排序。
- en: Listing 4.4 Return the rank order of an item in terms of validation activation
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 根据验证激活返回项的排名顺序
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This listing is simply the implementation of the ranked-ordering example. Don’t
    worry too much about the linear interpolation part; the code is a little opaque
    when implemented, but it is not capturing anything more complicated than you saw
    in the examples.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表只是实现了排名排序示例。不要过于担心线性插值部分；代码在实现时有点晦涩，但它并没有捕捉到比您在示例中看到更复杂的内容。
- en: 4.2.2 Which layers should I use to calculate model-based outliers?
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 我应该使用哪些层来计算基于模型的异常值？
- en: You may want to try outlier detection on different layers of your model to see
    whether they produce better outliers for sampling. In general, the earlier the
    layer, the closer the neurons will be to the raw data. If you chose the input
    layer from the model, which is the feature vector, outliers from the input layer
    are almost identical to the outlier detection method that you implemented in chapter
    2\. Any hidden layer is going to fall somewhere between representing the raw data
    (early layers) and representing the predictive task (later layers).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想尝试在不同的模型层上进行异常检测，看看它们是否能产生更好的异常值用于采样。一般来说，层越早，神经元就越接近原始数据。如果您选择了模型的输入层，即特征向量，输入层的异常值几乎与您在第
    2 章中实现的异常检测方法相同。任何隐藏层都将介于表示原始数据（早期层）和表示预测任务（后期层）之间。
- en: You could also choose to look at multiple layers within the same sample. This
    approach is used in transfer learning with pretrained models; the model is “flattened”
    to create one single vector combining all the layers. You could use a flattened
    model for outlier detection too, but you may want to normalize by the amount of
    neurons per layer. In our model, the 128 neurons in the hidden layer would become
    the main contributor to an outlier detection algorithm that also included the
    2 neurons from the final layer, so you might want to calculate the outlier ranking
    for the layers independently and then combine the two results.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以选择查看同一样本内的多个层。这种方法在迁移学习和预训练模型中使用；模型被“展平”以创建一个包含所有层的单个向量。你也可以使用展平的模型进行异常值检测，但你可能希望按每层的神经元数量进行归一化。在我们的模型中，隐藏层中的128个神经元将成为包含最终层2个神经元的异常值检测算法的主要贡献者，因此你可能希望独立计算各层的异常值排名，然后将两个结果合并。
- en: Alternatively, you could sample from both, taking half your model-outliers from
    the logits and half from the hidden layer. Note that the 128 neurons in the hidden
    layer probably aren’t too informative if you still have only 1,000 or so training
    items. You should expect the hidden layers to be noisy and some neurons to be
    random until you have many more labeled training items than neurons in your hidden
    layer—ideally, two or more orders of magnitude more training items than neurons
    in the layer (more than 10,000 labeled items).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以从两者中采样，将一半的模型异常值从logits中采样，另一半从隐藏层中采样。请注意，如果你仍然只有大约1,000个训练项目，隐藏层中的128个神经元可能不太具有信息量。你应该预计隐藏层将是嘈杂的，一些神经元是随机的，直到你拥有的标记训练项目比隐藏层中的神经元多得多——理想情况下，比层中的神经元多两个或更多数量级（超过10,000个标记项目）。
- en: If you are using layers near the input, be careful when your feature values
    don’t represent activation. For our text example, the inputs *do* represent a
    form of activation, because they represent how often a word occurs. For computer
    vision, however, a higher input value may simply represent a lighter RGB color.
    In these cases, the layers toward the output of the model and the logits will
    be more reliable.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用接近输入的层，当你的特征值不表示激活时，要小心。对于我们的文本示例，输入*确实*代表了一种激活形式，因为它们代表了一个词出现的频率。然而，对于计算机视觉来说，更高的输入值可能只是代表更亮的RGB颜色。在这些情况下，模型输出层和logits层将更加可靠。
- en: 4.2.3 The limitations of model-based outliers
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 基于模型的异常值的局限性
- en: 'Here is a summary of the main shortcomings of using your model for sampling
    outliers:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用你的模型进行异常值采样的主要缺点的总结：
- en: The method can generate outliers that are similar and therefore lack diversity
    within an active learning iteration.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法可能会生成在活跃学习迭代中相似且缺乏多样性的异常值。
- en: It’s hard to escape some statistical biases that are inherent in your model,
    so you may continually miss some types of outliers.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很难避免模型中固有的某些统计偏差，因此你可能会持续错过某些类型的异常值。
- en: You still need a model in place before you start, and this approach gets better
    with more training data, so model-based outlier sampling is not suited to a cold
    start.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始之前，你仍然需要一个模型，并且这种方法随着更多训练数据的增加而变得更好，因此基于模型的异常值采样不适合冷启动。
- en: We are determining an outlier by using our unlabeled data. It is easy to accidentally
    sample the opposite of what we want—things that look least like the data that
    we are trying to adapt to with new labels. For this reason, we use validation
    data to get our rankings, and you should follow this practice for any other kind
    of model-based outlier detection.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是通过使用我们的未标记数据来确定异常值的。很容易意外地采样到我们不想得到的东西——那些看起来最不像我们试图用新标签适应的数据。因此，我们使用验证数据来获取我们的排名，你应该遵循这种做法来检测任何其他类型的基于模型的异常值。
- en: We will cover some solutions to the first issue in chapter 5, with algorithms
    that combine outlier detection and transfer learning. The second, third, and fourth
    issues are harder to overcome. Therefore, if you are sampling model-based outliers,
    you should consider using other methods of diversity sampling at the same time,
    including the methods that you can use from a cold start, such as clustering,
    which we cover next.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5章中介绍解决第一个问题的方法，包括结合异常值检测和迁移学习的算法。第二个、第三个和第四个问题更难克服。因此，如果你正在采样基于模型的异常值，你应该考虑同时使用其他多样性采样方法，包括你可以从冷启动中使用的那些方法，例如聚类，我们将在下一节中介绍。
- en: 4.3 Cluster-based sampling
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 基于聚类的采样
- en: 'Clustering can help you target a diverse selection of data from the start.
    The strategy is fairly straightforward: instead of sampling training data randomly
    to begin with, we also divide our data into a large number of clusters and sample
    evenly from each cluster.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以帮助你从一开始就针对数据多样性进行选择。策略相当直接：我们不是一开始就随机采样训练数据，我们还把我们的数据分成大量聚类，并从每个聚类中均匀采样。
- en: The reason why this works should be equally straightforward. By now, you have
    probably noticed that there are tens of thousands of news articles about local
    Australian sports teams in the headlines. If we randomly sample the data for human
    review, we are going to spend a lot of time manually annotating similar headlines
    about the results of sporting matches. If we precluster our data, however, these
    headlines are likely to end up together in one cluster, so we will need to annotate
    only a handful of examples from this sports-related cluster. This approach will
    save a lot of time, which we can instead spend annotating data from other clusters.
    Those other clusters may represent rarer types of headlines that are important
    but so rare that they would have been missed with random sampling. So clustering
    is saving time and increasing diversity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法之所以有效应该同样简单明了。到现在为止，你可能已经注意到有关当地澳大利亚体育团队的新闻文章标题有成千上万条。如果我们随机采样数据供人类审查，我们将花费大量时间手动标注关于体育比赛结果的相似标题。然而，如果我们预先聚类我们的数据，这些标题很可能会最终聚集在一个聚类中，因此我们只需要从这个与体育相关的聚类中标注少量示例。这种方法将节省大量时间，我们可以用这些时间来标注来自其他聚类的数据。那些其他聚类可能代表更罕见的标题类型，这些标题虽然重要但非常罕见，在随机采样中可能会被遗漏。所以聚类既节省了时间，又增加了多样性。
- en: Clustering is by far the most common method used for diversity sampling in real-world
    machine learning. It is the second method discussed in this chapter because it
    fit the flow of the book better. In practice, you will probably try this method
    of diversity sampling first.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是迄今为止在现实世界机器学习中用于多样性采样的最常用方法。它是本章讨论的第二种方法，因为它更适合本书的流程。在实践中，你可能首先尝试这种方法进行多样性采样。
- en: You have probably encountered unsupervised learning, and you’re most likely
    familiar with k-means, the clustering algorithm that we will be using. The approaches
    to unsupervised clustering and clustering for active learning are the same, but
    we will be using the clusters to sample items for human review for labeling instead
    of interpreting the clusters or using the clusters themselves in downstream processing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经遇到过无监督学习，并且你很可能熟悉k-means聚类算法，这是我们将会使用的算法。无监督聚类和用于主动学习的聚类方法相同，但我们将会使用聚类来采样项目供人类审查进行标注，而不是解释聚类或在下一次处理中使用聚类本身。
- en: 4.3.1 Cluster members, centroids, and outliers
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 聚类成员、质心和异常值
- en: The item that is closest to the center of a cluster is known as the *centroid*.
    In fact, some clustering algorithms explicitly measure the distance from the centroid
    item rather them from the cluster properties as a whole.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 距离聚类中心最近的项被称为*质心*。实际上，一些聚类算法明确地测量质心项的距离，而不是整个聚类属性的距离。
- en: 'You calculated outliers from the entire dataset in chapter 2, and you can also
    calculate outliers when using clustering. Outliers are the statistical counterpoint
    of the centroid: they are farthest from the center of any cluster.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第二章中从整个数据集中计算了异常值，在使用聚类时也可以计算异常值。异常值是质心的统计对立面：它们距离任何聚类的中心最远。
- en: 'Figure 4.4 shows an example with five clusters, with a centroid and outlier
    for two of the clusters indicated. The majority of items in figure 4.4 are in
    one cluster: the large one in the middle. So if we sampled randomly instead of
    by clustering, we would end up spending most of the time labeling similar items.
    By clustering first and sampling from each cluster, we can ensure more diversity.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4展示了五个聚类的示例，其中两个聚类的质心和异常值被标出。图4.4中的大多数项目都在一个聚类中：中间的大聚类。所以如果我们不是通过聚类而是随机采样，我们就会花费大部分时间标注相似的项目。通过首先聚类并从每个聚类中采样，我们可以确保更多的多样性。
- en: '![](../Images/CH04_F04_Munro.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F04_Munro.png)'
- en: Figure 4.4 An example clustering algorithm applied to the data, splitting it
    into five separate clusters. For each cluster, the most central item is known
    as the *centroid*, and the items farthest from the centers are *outliers*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4展示了应用于数据的示例聚类算法，将其分为五个独立的聚类。对于每个聚类，最中心的项目被称为*质心*，距离中心最远的项目被称为*异常值*。
- en: 'We will sample from clusters in three ways:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以三种方式从聚类中进行采样：
- en: '*Random*—Sampling items at random from each cluster. This strategy is close
    to random sampling but will spread out our selection across our feature space
    more evenly than purely random sampling.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机*—从每个聚类中随机采样项目。这种策略接近随机采样，但比纯随机采样更均匀地分散我们的选择在整个特征空间中。'
- en: '*Centroids*—Sampling the centroids of clusters to represent the core of significant
    trends within our data.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*质心*—采样聚类的质心来代表我们数据中有意义的趋势的核心。'
- en: '*Outliers*—Sampling the outliers from our clustering algorithm to find potentially
    interesting data that might have been missed in the clusters. Outliers within
    clustering are sometimes known as *proximity-based* outliers.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常值*—从我们的聚类算法中采样异常值，以找到可能被聚类遗漏的潜在有趣数据。聚类中的异常值有时被称为*基于邻近度的*异常值。'
- en: Within a single cluster, the ranked centroids are likely to be similar. That
    is, the item that is the closest to the center is likely to be similar to that
    item that is second-closest to the center. So we sample randomly within the cluster
    or chose only the centroid.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个聚类内，排名靠前的质心可能是相似的。也就是说，离中心最近的项可能类似于离中心第二近的项。因此，我们在聚类内部随机采样或仅选择质心。
- en: 'Similarly, we probably need to sample only a small number of outliers per cluster.
    It’s possible that the outliers are meaningful trends that the algorithm is missing,
    but it is more likely that they are genuinely rare: repeated rare words in the
    case of text or noisy/corrupted images in the case of computer vision. Typically,
    you need to sample only a small number of outliers, perhaps only one outlier from
    each cluster if you have a large number of clusters.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可能只需要每个聚类中采样少量异常值。可能这些异常值是算法遗漏的有意义趋势，但更有可能的是它们确实是真正罕见的：在文本中是重复的罕见单词，在计算机视觉中是嘈杂或损坏的图像。通常，您只需要采样少量异常值，如果您有大量聚类，可能每个聚类只采样一个异常值。
- en: To keep the example simple, assume that we are sampling the centroid of each
    cluster, the single biggest outlier from each cluster, and three additional randomly
    sampled items within each cluster. To use cluster-based sampling, run
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使示例简单，假设我们正在采样每个聚类的质心，每个聚类的单个最大异常值，以及每个聚类内部额外随机采样的三个项目。要使用基于聚类的采样，请运行
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This command samples 95 unlabeled items via cluster-based sampling for you to
    annotate, along with 5 randomly selected items from remaining unlabeled items.
    I recommend running the code with the `verbose` flag, which prints three random
    items from each cluster as the code runs. You can get an idea of how well the
    clusters are capturing meaningful differences by examining whether the items in
    the cluster seem to be semantically related. In turn, this approach will give
    you an idea of how many meaningful trends in the data are being surfaced for human
    annotation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令通过基于聚类的采样为您采样95个未标记的项目进行标注，同时还有从剩余未标记项目中随机选择的5个项目。我建议使用`verbose`标志运行代码，该标志在代码运行时打印出每个聚类的三个随机项目。您可以通过检查聚类中的项目是否在语义上似乎相关，来了解聚类是否很好地捕捉到了有意义的差异。反过来，这种方法将给您一个关于数据中有多少有意义的趋势被呈现出来供人工标注的印象。
- en: 4.3.2 Any clustering algorithm in the universe
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 任何宇宙中的聚类算法
- en: As far as I know, no one has studied in depth whether one clustering algorithm
    is consistently better than another for active learning. Many pairwise studies
    look at variations on particular clustering algorithms, but no comprehensive broad
    study exists, so if you are interested in this topic, this situation would make
    a great research study.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 就我所知，没有人深入研究过是否有一个聚类算法始终比另一个聚类算法更好用于主动学习。许多成对研究查看特定聚类算法的变体，但没有全面广泛的研究，所以如果您对这个主题感兴趣，这种情况将是一个很好的研究课题。
- en: Some clustering algorithms need only a single pass over the data, and some can
    be O(N³) complexity or worse. Although the more compute-intensive algorithms reach
    more mathematically well-motivated clusters within your data, the distribution
    of information across clusters won’t necessarily be any better or worse for sampling
    items that need to be labeled.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一些聚类算法只需要对数据进行单次遍历，而一些算法的复杂度可能为O(N³)或更差。尽管计算密集型算法可以在您的数据中找到更多数学上合理的聚类，但聚类中信息分布的分布对于需要标记的项目采样来说，不一定更好或更差。
- en: For the system we will implement here, we don’t want to make the people using
    the system wait a long time for the clustering algorithm to find the best clusters,
    so we’ll choose an efficient clustering algorithm. We are going to use a variation
    of k-means that uses cosine similarity as the distance measure rather than the
    more typical Euclidean distance (figure 4.5). We have high-dimensional data, and
    Euclidean distance doesn’t work well in high dimensions. One way to think about
    this problem is to think of many corners in your data. Almost all clustering algorithms
    are prone to producing unreliable results with high-dimensional data. In figure
    4.4, we have examples in two dimensions and only four corners where outliers can
    hide from the center of the data distributions. If we had features in three dimensions,
    outliers could occupy eight corners. (Think of the eight corners of a cube.) By
    the time we get to 300 features, the data has 10^90 corners, and 10^90 is more
    than the number of atoms in the observable universe. You will certainly have more
    than 300 features in almost any natural language processing (NLP) task, so outliers
    can occur in a lot of corners of the space. For data with more than 10 dimensions,
    more than 99% of the space is in the corners, so if the data is uniformly distributed
    or even a Gaussian distribution, you will be measuring an artifact of the corners
    more than the distance, which can be unreliable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们将要实施的系统，我们不希望使用系统的人需要等待很长时间，直到聚类算法找到最佳的聚类。因此，我们将选择一个高效的聚类算法。我们将使用一种变种的k-means算法，它使用余弦相似度作为距离度量，而不是更典型的欧几里得距离（图4.5）。我们处理的是高维数据，而在高维空间中欧几里得距离效果不佳。思考这个问题的方法之一是想象数据中的许多角落。几乎所有的聚类算法在高维数据中都容易产生不可靠的结果。在图4.4中，我们有两个维度的示例，只有四个角落，异常值可以隐藏在数据分布的中心。如果我们有三个维度的特征，异常值可以占据八个角落。（想象一下立方体的八个角落。）当我们达到300个特征时，数据就有10^90个角落，而10^90大于可观测宇宙中的原子数量。在几乎任何自然语言处理（NLP）任务中，你肯定会有超过300个特征，因此异常值可能会出现在空间的许多角落。对于超过10维度的数据，超过99%的空间都在角落里，所以如果数据是均匀分布的，甚至高斯分布，你将测量到角落的伪影，而不是距离，这可能是不可靠的。
- en: '![](../Images/CH04_F05_Munro.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F05_Munro.png)'
- en: Figure 4.5 An example clustering algorithm using cosine similarity. For each
    cluster, the center is defined as a vector from 0, and the membership of that
    cluster is the angle between the vector representing the cluster and the vector
    representing the item. Note that although this example looks less like real clusters
    than the spheroidal clusters in figure 4.4, it is limited by being 2D. For higher-dimensional
    sparse data, which you are more likely to be using with your models, this kind
    of clustering is often better than the spheroid type shown here.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 使用余弦相似度的聚类算法示例。对于每个聚类，中心被定义为从0出发的向量，该聚类的成员资格是表示该聚类的向量和表示项目的向量之间的角度。请注意，尽管这个例子看起来不像图4.4中的球形聚类那样像真实的聚类，但它受限于二维。对于更高维度的稀疏数据，你更有可能在使用你的模型时使用这种数据，这种聚类通常比这里展示的球形类型更好。
- en: You can think of cosine similarity in terms of looking at stars in the night
    sky. If you drew a straight line from yourself toward two stars and measured the
    angle between those lines, that angle would give you the cosine similarity. In
    the night-sky example, you have only three physical dimensions, but your data
    has one dimension for each feature. Cosine similarity is not immune to the problems
    of high dimensionality, but it tends to perform better than Euclidean distance
    especially for sparse data, such as our text encodings.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将余弦相似度想象成在夜空中观察星星。如果你从自己出发，向两颗星星画一条直线，并测量这两条线之间的角度，那么这个角度就会给你余弦相似度。在夜空示例中，你只有三个物理维度，但你的数据对于每个特征都有一个维度。余弦相似度并不是对高维问题的免疫，但它通常比欧几里得距离表现得更好，尤其是在稀疏数据方面，比如我们的文本编码。
- en: 'Cosine similarity measures whether two vectors are pointing in the same direction
    but does not measure the distance. There might be a small angle between two stars
    in the sky, but one happens to be much farther away. Because you are measuring
    only the angle, you are treating the stars as being equally far away. For this
    reason, cosine similarity is sometimes called *spherical k-means*, with all data
    points treated as though they were the same distance from 0 on a multidimensional
    sphere. This example does bring up an issue: data points can accidentally be in
    the same direction and therefore erroneously seem to be similar. The chance that
    this issue will occur in high-dimensional data is low, however, so high dimensions
    help (and make our calculations simpler). We can calculate a cluster’s vector
    as the sum of all the vectors (features) of the items in that cluster and not
    worry about normalizing by the number of items, because cosine is not sensitive
    to the absolute values of the distance function.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度衡量两个向量是否指向同一方向，但不衡量距离。天空中两颗星星之间可能有一个很小的角度，但其中一颗可能远得多。因为你只测量角度，所以你将星星视为距离相等。因此，余弦相似度有时被称为*球面k-means*，其中所有数据点都被视为在多维球面上距离0相同的距离。这个例子确实提出了一个问题：数据点可能意外地指向同一方向，因此错误地看起来很相似。然而，在高度数据中发生这种问题的可能性很低，因此高维度有助于（并使我们的计算更简单）。我们可以将聚类的向量计算为该聚类中所有项目（特征）向量的总和，而不用担心按项目数量进行归一化，因为余弦对距离函数的绝对值不敏感。
- en: 4.3.3 K-means clustering with cosine similarity
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 使用余弦相似度的K-means聚类
- en: Given two feature-vectors of the same size, v[1] and v[2], you can calculate
    the cosine of the angle between those vectors as
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个相同大小的特征向量v[1]和v[2]，你可以计算这两个向量之间角度的余弦值如下
- en: 𝛟[CS](v[1], v[2]) = (v[1] ⋅ v[2]) / (‖v[1]‖[2] ⋅ ‖v[2]‖[2])
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 𝛟[CS](v[1], v[2]) = (v[1] ⋅ v[2]) / (‖v[1]‖[2] ⋅ ‖v[2]‖[2])
- en: Cosine similarity is a native function in PyTorch, so we won’t go too deeply
    into the implementation here. The double-line notation indicates the norm of the
    vector. The intuition of angles between stars in the night sky and the example
    in figure 4.5 (section 4.3.2) should be enough for you to understand what is going
    on. (If you are interested in reading more about cosine similarity or looking
    at other distance functions in PyTorch, you can start with the documentation at
    [http://mng.bz/XdzM](http://mng.bz/XdzM).)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度是PyTorch的内置函数，所以我们不会在这里深入探讨其实现。夜空中星星之间的角度和图4.5（第4.3.2节）中的示例应该足以让你理解正在发生的事情。（如果你对余弦相似度感兴趣或想查看PyTorch中的其他距离函数，可以从[http://mng.bz/XdzM](http://mng.bz/XdzM)的文档开始。）
- en: Other popular machine learning libraries also have a lot of implementations
    of clustering algorithms. These other algorithms could work as well as the example
    that you are implementing here. There is a commonly held belief that clustering
    algorithms shouldn’t be used for datasets with more than 10,000 items, but it
    isn’t true. There have always been clustering algorithms that work reasonably
    well with a single pass of the data, so you shouldn’t think of any limitation
    according to dataset size unless you are trying to trim your processing time to
    a few seconds. Even with compute-intensive clustering algorithms, you can often
    build the clusters in smaller subsets (batches) of the data to build clusters,
    and using the resulting clusters will be almost as good as using the entire dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其他流行的机器学习库也实现了大量的聚类算法。这些其他算法可能和你在这里实现的示例一样有效。有一种普遍的看法认为，聚类算法不应用于超过10,000个项目的数据集，但这并不正确。始终存在一些聚类算法，它们可以通过单次数据遍历合理地工作，因此你不应该根据数据集大小考虑任何限制，除非你试图将处理时间缩短到几秒钟。即使对于计算密集型的聚类算法，你通常也可以在数据的小子集（批次）中构建聚类，使用这些结果聚类几乎和整个数据集一样好。
- en: 'The general strategy for k-means is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: K-means的一般策略如下：
- en: Select the number of clusters you want by working backward from the number of
    annotations that you need.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从所需的注释数量反向工作来选择你想要的聚类数量。
- en: Randomly add the data items to one of the initial clusters.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机将数据项添加到初始聚类之一。
- en: Iterate through the items and move them to another cluster if they are closer
    to that cluster.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历项目，如果它们更接近另一个聚类，则将它们移动到另一个聚类。
- en: Repeat step 3 until there are no more items to move or you have reached some
    predefined limit on the number of epochs through the data.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 3，直到没有更多项目可以移动或者达到通过数据预定义的epoch数量限制。
- en: Cosine similarity and cosine distance are the same thing
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度和余弦距离是同一回事
- en: You might see cosine similarity referred to as cosine distance in the literature.
    These terms mean the same thing. In general, clustering algorithms are more likely
    to use the term distance than similarity, and in the strictest definitions, distance
    = 1 – similarity. Cosine similarity does not follow the strict definition of the
    triangle inequality property (Schwarz inequality), however, so cosine similarity
    does not meet the formal definition of a distance metric—hence the name similarity.
    The terminology is confusing enough in this chapter when we are treating centroids
    and outliers as complements to get our [0, 1] range for every sampled item, so
    don’t let this add to your confusion!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在文献中看到余弦相似度被称作余弦距离。这些术语意味着同一件事。一般来说，聚类算法更可能使用“距离”这个术语，而在最严格的定义中，距离 = 1 –
    相似度。然而，余弦相似度并不遵循严格的三角不等式性质（Schwarz 不等式），因此余弦相似度不符合距离度量的正式定义——因此得名“相似度”。在这个章节中，当我们把中心点和异常值作为互补来获取每个采样项目的
    [0, 1] 范围时，术语已经足够令人困惑了，所以不要让这增加你的困惑！
- en: As noted in step 1, you should work backward and choose the number of clusters
    that makes the most sense given how many items you want to sample from each cluster.
    If you want to sample 5 items per cluster (1 centroid, 1 outlier, and 3 randomly
    chosen), and you want to annotate 100 items in this active learning iteration
    through this sampling strategy, you would select 20 clusters, as 20 × 5 = 100.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如步骤 1 所述，你应该反向工作，并选择最合理的簇数量，这取决于你从每个簇中想要采样的项目数量。如果你想从每个簇中采样 5 个项目（1 个中心点，1 个异常值和
    3 个随机选择），并且你想要在这个主动学习迭代中通过这种采样策略注释 100 个项目，那么你应该选择 20 个簇，因为 20 × 5 = 100。
- en: 'For completeness, the full code for k-means clustering with cosine similarity
    was implemented in the example code for this book, and you can see it at [http://mng.bz/MXQm](http://mng.bz/MXQm).
    This k-means strategy is the same regardless of the distance measure. The k-means
    function takes only two arguments: the data, which can be unlabeled or labeled
    (in which case the labels are ignored), and the number of clusters you want. You
    can see the k-means strategy in diversity_sampling.py with the main function in
    the following listing.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，本书示例代码中实现了带有余弦相似度的 k-means 聚类算法的完整代码，你可以在 [http://mng.bz/MXQm](http://mng.bz/MXQm)
    上看到它。这个 k-means 策略与距离度量无关。k-means 函数只接受两个参数：数据，可以是未标记的或标记的（在这种情况下，标签被忽略），以及你想要的簇数量。你可以在
    diversity_sampling.py 中的 k-means 策略中看到，以下列表中的主函数。
- en: Listing 4.5 Cluster-based sampling in PyTorch
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 PyTorch 中的基于簇的采样
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Initialize clusters with random assignments.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用随机分配初始化簇。
- en: ❷ Move each item to the cluster that it is the best fit for, and repeat.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将每个项目移动到最适合它的簇中，并重复。
- en: ❸ Sample the best-fit (centroid) from each cluster.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从每个簇中采样最佳拟合（中心点）。
- en: ❹ Sample the biggest outlier in each cluster.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从每个簇中采样最大的异常值。
- en: ❺ Sample three random items from each cluster, and pass the verbose parameter
    to get an idea of what is in each cluster.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从每个簇中随机采样三个项目，并传递 verbose 参数以了解每个簇中有什么。
- en: 'You could substitute cosine for any other distance/similarity measure, and
    it might work equally well. One tactic that you may want to try to speed the process
    is to create the clusters on a subset of the data and then assign the rest of
    the data to its clusters. That approach gives you the best of both worlds: creating
    clusters quickly and sampling from the entire dataset. You may also want to experiment
    with a different number of clusters and a different number of random selections
    per cluster.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用余弦替换任何其他距离/相似度度量，并且可能效果一样好。你可能想要尝试的一种策略是为了加快过程，在数据的一个子集上创建簇，然后将剩余的数据分配到其簇中。这种方法让你两全其美：快速创建簇并从整个数据集中采样。你可能还想要尝试不同的簇数量和每个簇中不同的随机选择数量。
- en: You’ll remember from high school mathematics that cosine(90°) = 0 and cosine(0°)
    = 1\. This makes our goal of a [0,1] range easy, because cosine similarity already
    returns values in a [0,1] range when calculated only on positive feature values.
    For our centroids, we can take the cosine similarity directly as our diversity
    score for each item. For the outliers, we will subtract the values from 1 so that
    we are consistent in our active learning ranking strategies and always sampling
    the highest numbers. As we said in chapter 3, consistency is important for downstream
    tasks.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会记得，从高中数学中学到的余弦函数值：余弦(90°) = 0 和余弦(0°) = 1。这使得我们的目标值在[0,1]范围内变得容易实现，因为当仅对正特征值进行计算时，余弦相似度已经返回了[0,1]范围内的值。对于我们的质心，我们可以直接将余弦相似度作为每个项目的多样性得分。对于异常值，我们将从1中减去这些值，以确保我们在主动学习排名策略中保持一致性，并且总是采样最高值。正如我们在第3章中提到的，一致性对于下游任务很重要。
- en: 4.3.4 Reduced feature dimensions via embeddings or PCA
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 通过嵌入或PCA减少特征维度
- en: Clustering works better for text than for images. If you come from a computer
    vision background, you know this already. When you looked at the clusters in your
    examples in this chapter, you could see semantic relationships between items in
    each cluster. All the clusters contain news headlines with similar topics, for
    example. But the same wouldn’t be true if cosine similarity were applied to images,
    because individual pixels are more abstracted from the content of the images than
    sequences of characters are from the content of the text. If you applied cosine
    similarity to images, you might get a cluster of images that are landscapes, but
    that cluster might also erroneously include an image of a green car in front of
    a blue wall.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类对于文本比对图像来说效果更好。如果你来自计算机视觉背景，你早已知道这一点。当你查看本章中的示例中的聚类时，你可以看到每个聚类中项目之间的语义关系。所有聚类都包含具有相似主题的新闻标题，例如。但如果将余弦相似度应用于图像，情况可能就不一样了，因为单个像素比字符序列从图像内容中抽象出来得更多。如果你将余弦相似度应用于图像，你可能会得到一个包含风景图像的聚类，但这个聚类可能错误地包含一张绿色汽车停在蓝色墙前的图像。
- en: The most common method for reducing the dimensionality of data is principal
    component analysis (PCA). PCA reduces the dimensionality of a dataset by combining
    highly-correlated features. If you have been doing machine learning for some time,
    you probably thought that PCA was your first option to reduce the dimensionality
    of the data. PCA was common for early non-neural machine learning algorithms that
    degraded in quality more when there was a high number of dimensions (features)
    with correlations between features. Neural model-based embeddings are more common
    in academia today, but PCA is more common in industry.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 减少数据维度最常见的方法是主成分分析（PCA）。PCA通过组合高度相关的特征来减少数据集的维度。如果你已经从事机器学习一段时间了，你可能认为PCA是减少数据维度时的首选选项。PCA对于早期非神经机器学习算法来说很常见，当有大量维度（特征）之间存在相关性时，其质量会下降。基于神经模型的嵌入在学术界更为常见，但PCA在工业界更为常见。
- en: Implementing PCA is outside the scope of this book. It’s a good technique to
    know in machine learning regardless, so I recommend reading more about it so that
    you have several tools for dimensionality reduction. PCA is not a native function
    in PyTorch (although I would not be surprised if it were added fairly soon), but
    the core operation of PCA is singular value decomposition (SVD), which is covered
    at [https://pytorch.org/docs/stable/torch.html#torch.svd](https://pytorch.org/docs/stable/torch.html#torch.svd).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的实现超出了本书的范围。尽管如此，PCA是机器学习中一个很好的技术，因此我建议你阅读更多关于它的内容，以便你拥有几个降维的工具。PCA不是PyTorch的本地函数（尽管我并不惊讶它很快就会被添加），但PCA的核心操作是奇异值分解（SVD），这在[https://pytorch.org/docs/stable/torch.html#torch.svd](https://pytorch.org/docs/stable/torch.html#torch.svd)中有介绍。
- en: 'As an alternative to PCA, you can use embeddings from your model—that is, use
    the hidden layers of your model or from another model that has been trained on
    other data. You can use these layers as representations for modeling directly.
    Alternatively, you can use model distillation to lower the dimensionality within
    the clustering process, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 作为PCA的替代方案，你可以使用你模型的嵌入——也就是说，使用你模型的隐藏层或另一个在其它数据上训练过的模型。你可以将这些层用作直接建模的表现。或者，你可以使用模型蒸馏来降低聚类过程中的维度，如下所示：
- en: Select the number of clusters you want.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你想要的聚类数量。
- en: Cluster the items according to your existing (high-dimensional) feature space.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你现有的（高维）特征空间对项目进行聚类。
- en: Treat each cluster as a label, and build a model to classify items into each
    cluster.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个聚类视为一个标签，并构建一个模型来将项目分类到每个聚类中。
- en: Using the hidden layer from your new middle as your new feature set, continue
    the process of reassigning items to the best cluster.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你新中间层的隐藏层作为新的特征集，继续将项目重新分配到最佳聚类的过程。
- en: 'Model design is important here. For text data, your architecture from section
    4.2 is probably enough: a single hidden layer with 128 neurons. For image data,
    you probably want to have more layers and to use a convolutional neural network
    (CNN) or a similar network to help generalize away from specific pixel locations.
    In either case, use your intuition from building models for the amount of data
    you have and the chosen number of clusters (labels).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 模型设计在这里很重要。对于文本数据，你第4.2节中的架构可能就足够了：一个包含128个神经元的单个隐藏层。对于图像数据，你可能需要更多层，并使用卷积神经网络（CNN）或类似网络来帮助泛化到特定的像素位置。在两种情况下，使用你构建模型时的直觉，根据你拥有的数据量和选择的聚类（标签）数量。
- en: Note that if you have negative values in your vector, as you would if you are
    clustering on a hidden layer with LeakyReLU as the activation function, cosine
    similarity will return values in a [–1,1] range instead of a [0,1] range. For
    consistency, therefore, you would want to normalize by adding 1 and halving the
    result of cosine similarity to get a [0,1] range.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你的向量中有负值，比如你在使用LeakyReLU作为激活函数的隐藏层上进行聚类时，余弦相似度将返回[-1,1]范围内的值，而不是[0,1]范围内的值。因此，为了保持一致性，你可能需要通过加1并减半余弦相似度的结果来归一化，以获得[0,1]的范围。
- en: For a denser feature vector, whether from a model or from PCA, you might also
    consider a distance function other than cosine. Cosine similarity is best for
    large, sparse vectors, such as our word representations. You may not want to treat
    activations of [0.1, 0.1] the same as activations of [10.1,10.1], as cosine similarity
    does. PyTorch also has a built-in distance function for pairwise distance, which
    might be more appropriate in that case. You can see this function commented out
    where the cosine function now exists in the pytorch_clusters.py file. You can
    experiment with different distance functions to see whether you get more meaningful
    clusters. As the code says, you may need to normalize your cluster vectors according
    to the number of items in that cluster; otherwise, you should be able to sub in
    other distance functions without making other changes to the code.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更密集的特征向量，无论是来自模型还是来自PCA，你也可能考虑使用除了余弦以外的其他距离函数。余弦相似度最适合大型稀疏向量，例如我们的词表示。你可能不想像余弦相似度那样将[0.1,
    0.1]的激活与[10.1, 10.1]的激活同等对待。PyTorch还内置了一个用于成对距离的距离函数，在这种情况下可能更合适。你可以在pytorch_clusters.py文件中看到这个函数被注释掉，现在余弦函数存在的地方。你可以尝试不同的距离函数，看看是否可以得到更有意义的聚类。正如代码所述，你可能需要根据该聚类中的项目数量来归一化你的聚类向量；否则，你应该能够在不修改代码的其他部分的情况下替换其他距离函数。
- en: As one final point on advanced clustering for computer vision, if you are clustering
    for diversity sampling, it may not matter if the clusters aren’t semantically
    meaningful. From a sampling point of view, you might get good diversity of images
    from across your clusters even if the clusters themselves aren’t semantically
    consistent. That is, you might be able to ignore embeddings and PCA, and cluster
    directly on the pixel values. This approach might give you equal success. Cosine
    similarity will create identical vectors for RGB = (50,100,100) and RGB = (100,200,200),
    so lighter, more saturated versions of the same image may be identical, but this
    may not matter. I’m not aware of any in-depth research on whether pixel-level
    clustering for images is always worse than using a reduced dimension when sampling
    for active learning, so this research topic would be a valuable topic for anyone
    who’s interested in pursuing it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉高级聚类的最后一个要点，如果你是为了多样性采样而聚类，那么聚类本身是否具有语义意义可能并不重要。从采样的角度来看，即使聚类本身在语义上不一致，你仍然可能从你的聚类中获得来自不同图像的良好多样性。也就是说，你可能可以忽略嵌入和PCA，并直接在像素值上聚类。这种方法可能同样成功。余弦相似度将为RGB
    = (50,100,100)和RGB = (100,200,200)创建相同的向量，因此相同图像的较亮、较饱和的版本可能是相同的，但这可能并不重要。我不了解任何关于图像像素级聚类是否总是比在主动学习采样时使用降维更差的研究，因此这个研究主题对于任何有兴趣追求它的人来说都是一个有价值的主题。
- en: 4.3.5 Other clustering algorithms
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 其他聚类算法
- en: 'In addition to other variations of k-means, you may want to experiment with
    other clustering algorithms and related unsupervised machine learning algorithms.
    It is beyond the scope of this book to talk about every popular clustering algorithm;
    many good books have been written on clustering. In this book, however, we will
    take a high-level look at three algorithms:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 k-means 的其他变体之外，你可能还想尝试其他聚类算法和相关无监督机器学习算法。本书的范围不涉及每个流行的聚类算法；关于聚类已有许多优秀的书籍。然而，在本书中，我们将从高层次的角度审视三种算法：
- en: Proximity-based clustering, such as k-nearest neighbors (KNN) and spectral clustering
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于邻近度的聚类，例如 k-最近邻（KNN）和谱聚类
- en: Gaussian mixture models (GMM)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）
- en: Topic modeling
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: 'You are probably familiar with KNN algorithms. KNN forms clusters based on
    proximity between a small number of items in that cluster (k items, instead of
    that cluster as a whole. A strength and limitation of k-means is that all clusters
    have a meaningful center: the mean itself. You can imagine L-shape clusters or
    other patterns that have no meaningful center; KNN allows you to capture these
    kinds of clusters. The same is true of spectral clustering, which is a vector-based
    clustering method that can also discover more complicated cluster shapes by representing
    the feature space in new vectors.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能熟悉 KNN 算法。KNN 通过该簇中少数几个项目之间的邻近度来形成簇（k 个项目，而不是整个簇。k-means 的一个优点和局限性是所有簇都有一个有意义的中心：即均值本身。你可以想象
    L 形状的簇或其他没有有意义的中心的模式；KNN 允许你捕捉这些类型的簇。谱聚类也是如此，它是一种基于向量的聚类方法，可以通过在新向量中表示特征空间来发现更复杂的簇形状。
- en: There is no clear evidence, however, that proximity-based clustering is consistently
    better than k-means clustering for active learning. You may want to capture data
    points separately at two different extremes in an L-shape because they are sufficiently
    different even if there is an unbroken link of items between them. Furthermore,
    your k-means algorithms will be discovering different kinds of shapes in your
    features if you build your clusters on hidden layers or PCA-derived vectors, as
    you learned earlier. Your k-means algorithm will discover simple spheroid clusters
    only in the vectors that it learns from, but if those vectors are abstracted from
    a greater number of features, your clusters would be more complicated if they
    are mapped back into those features. In fact, applying k-means to a vector from
    a hidden layer is similar using spectral clustering to discover different cluster
    shapes. So there is no clear advantage for spectral clustering for active learning—at
    least, no one has yet researched this topic in depth, to the point that one method
    is clearly better in most active learning use cases.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，没有明确的证据表明基于邻近度的聚类在主动学习方面始终优于 k-means 聚类。你可能想在 L 形状的两个不同极端分别捕获数据点，因为即使它们之间有连续的项目链接，它们也足够不同。此外，如果你的
    k-means 算法建立在隐藏层或 PCA 导出的向量上，那么它将在你学习的向量中发现不同类型的形状，正如你之前所学的。你的 k-means 算法只会在它学习的向量中发现简单的球状簇，但如果这些向量是从更多特征中抽象出来的，那么当它们映射回这些特征时，簇就会变得更加复杂。实际上，将
    k-means 应用于隐藏层中的向量与使用谱聚类来发现不同的簇形状相似。因此，对于主动学习来说，谱聚类没有明显的优势——至少，还没有人深入研究这个话题，以至于在大多数主动学习用例中，一种方法明显优于另一种方法。
- en: A GMM allows an item to be a member of multiple clusters at the same time. This
    algorithm can lead to more mathematically well-motivated clusters compared with
    k-means, which tries to force a cluster boundary where two clusters naturally
    overlap. You may see GMMs and related algorithms referred to as soft versus hard
    clustering or as fuzzy clustering. As with proximity-based clustering, there’s
    no strong evidence that GMMs produce better samples for active learning than k-means.
    Early in my career, I worked with mixture models and active learning at the same
    time but never combined the two; I never felt that other active learning techniques
    fell short in a way that needed GMMs or similar algorithms to overcome them. So
    from practical experience, I can report that I never found it necessary to try
    to combine the two, but I haven’t tested GMMs for active learning in depth either.
    This topic is another potentially exciting research area.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: GMM允许一个项目同时属于多个聚类。与试图强制在两个自然重叠的聚类之间设置聚类边界的k-means相比，这种算法可以产生更数学上合理的聚类。你可能会看到GMMs和相关算法被称为软聚类与硬聚类或模糊聚类。与基于邻近度的聚类一样，没有强有力的证据表明GMMs比k-means在主动学习方面产生更好的样本。在我职业生涯的早期，我同时从事混合模型和主动学习，但从未将两者结合起来；我从未觉得其他主动学习技术存在需要GMMs或类似算法来克服的不足。所以从实践经验来看，我可以报告说，我从未觉得有必要尝试将两者结合起来，但我也没有深入测试GMMs在主动学习中的应用。这个主题是另一个可能令人兴奋的研究领域。
- en: Topic modeling is used almost exclusively for text. Topic models explicitly
    discover sets of related words in a topic and the distributions of those topics
    across documents. The most popular algorithm is Latent Dirichlet Allocation (LDA),
    and you might see topic modeling referred to as LDA in the literature. Unlike
    GMMs, topic modeling is used a lot in practice, and it is especially common in
    social media monitoring tools. The related words in a single topic are often semantically
    related, so an expert user can generate topics and then select the most interesting
    ones for further analysis. This approach is a form of light supervision, an important
    human-in-the-loop strategy that we will return to in chapter 9\. Within diversity
    sampling, you could generate clusters as topics and sample items from each topic
    as you would with any other clustering mechanism.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型几乎仅用于文本。主题模型明确地发现一个主题中相关词的集合以及这些主题在文档中的分布。最流行的算法是潜在狄利克雷分配（LDA），你可能在文献中看到主题模型被称为LDA。与GMMs不同，主题模型在实际应用中使用得很多，尤其是在社交媒体监控工具中。一个主题中的相关词通常在语义上是相关的，因此专家用户可以生成主题，然后选择最有趣的主题进行进一步分析。这种方法是一种轻量级监督，是我们将在第9章中再次讨论的重要的人机交互策略。在多样性采样中，你可以将聚类作为主题，并从每个主题中采样项目，就像使用任何其他聚类机制一样。
- en: Although any clustering algorithm may not be *better* than k-means for modeling
    the data, it will be *different* which will increase diversity. So if you have
    multiple clustering algorithms producing samples for active learning, you are
    less likely to have biases resulting from the mathematical assumptions of any
    one clustering method. If you’re already using clustering algorithms on your data
    for some other reason, try them out as a sampling strategy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然任何聚类算法可能都不如k-means在建模数据方面更好，但它将带来不同的结果，这将增加多样性。所以如果你有多个聚类算法为主动学习生成样本，你就不太可能因为任何一种聚类方法的数学假设而产生偏差。如果你已经因为其他原因在你的数据上使用聚类算法，那么尝试将它们作为采样策略来使用。
- en: 4.4 Representative sampling
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 代表性采样
- en: '*Representative* *sampling* refers to explicitly calculating the difference
    between the training data and the application domain where we are deploying the
    model. In the model-based outliers and cluster-based sampling methods, we did
    not explicitly try to model the gap between our model and the data where we are
    evaluating our model’s accuracy. So the natural next step is to try to find items
    that fit this profile: what unlabeled data looks most like the domain where we
    are deploying our model? This step can be as useful for you as a data scientist
    as it is for your model: learning what data looks most like where you are adapting
    it will give you good intuition about that dataset as a whole and the problems
    you might face. An example is shown in figure 4.6.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*代表性* *采样* 指的是明确计算训练数据与我们部署模型的应用域之间的差异。在基于模型的异常值和基于聚类的采样方法中，我们没有明确尝试建模我们的模型与评估模型准确性的数据之间的差距。因此，自然的下一步是尝试找到符合这一特征的项目：哪些未标记数据看起来最像我们部署模型所在的领域？这一步骤对于你作为数据科学家来说，就像对于你的模型一样有用：了解哪些数据看起来最像你正在适应的地方，将给你关于整个数据集以及你可能会遇到的问题的良好直觉。一个示例如图4.6所示。'
- en: '![](../Images/CH04_F06_Munro.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F06_Munro.png)'
- en: Figure 4.6 An example of representative sampling, showing that the current training
    data is from a different distribution of the data from the application domain.
    Representative sampling maximizes sampling of items that look the most like the
    application domain relative to the current training data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 代表性采样的一个示例，显示了当前训练数据来自与应用域数据不同的分布。代表性采样最大化了相对于当前训练数据，采样与应用域看起来最像的项目。
- en: 4.4.1 Representative sampling is rarely used in isolation
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 代表性采样很少单独使用
- en: It would be understandable if you assumed that representative sampling is the
    best method for active learning. If we can sample data that looks the most like
    where we want to deploy our models, doesn’t this solve most of our diversity problems?
    While the general intuition is correct and representative sampling is one of the
    most powerful active learning strategies, it also one of the most prone to errors
    and overfitting. So, we will cover some of the limitations before jumping into
    the implementation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为代表性采样是主动学习中最优的方法，这是可以理解的。如果我们能够采样看起来最像我们希望部署模型的地方的数据，这难道不是解决了我们大部分的多样性问题吗？虽然这种直觉是正确的，代表性采样也是最强大的主动学习策略之一，但它也是最容易出现错误和过拟合的。因此，在深入实施之前，我们将探讨一些局限性。
- en: 'For one thing, in most real-world scenarios, your unlabeled data is not from
    the domain where you will deploy your model. If you are deploying a model to identify
    future news headlines (as in our example) or to help autonomous vehicles navigate
    on the road at some point in the future, you do not have a sample of data from
    your target domain; you have a sample from an earlier intermediate time. This
    fact will be true of most real-world scenarios: you are deploying your model in
    the future. So if you tune your training data too closely to your unlabeled data,
    your model will be stuck in the past when it is deployed to future data.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在大多数现实场景中，你的未标记数据并不是来自你将部署模型的领域。如果你正在部署一个模型来识别未来的新闻标题（如我们的示例）或帮助自动驾驶汽车在未来某个时刻在道路上导航，你并没有来自目标域的数据样本；你有一个来自早期中间时间点的样本。这一事实在大多数现实场景中都是正确的：你将在未来部署你的模型。因此，如果你将训练数据调整得太接近你的未标记数据，当模型部署到未来数据时，它将陷入过去。
- en: In some deployment scenarios, such as a centralized model processing news headlines,
    you may be able to adapt to new data in near real time, so you won’t have a big
    problem. In other use cases, such as autonomous vehicles, adapting the model in
    near real time and deploying it to every vehicle will be impossible. In either
    case, you still need a greater diversity of training items than only those that
    look the most like your current unlabeled data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些部署场景中，例如集中式模型处理新闻标题，你可能能够几乎实时地适应新数据，因此你不会遇到大问题。在其他用例中，例如自动驾驶汽车，几乎实时地调整模型并将其部署到每辆车上将是不可能的。在两种情况下，你仍然需要比仅看起来最像你当前未标记数据的训练项目更多样化的训练项目。
- en: Representative sampling is the most prone to noise of all the active learning
    strategies in this book. If you have clean training data, the noise in your unlabeled
    data is often the most different from that training data. In NLP tasks, this noise
    would include corrupted text, text from a language that is not part of your target
    domain, text that came from a list of places names that didn’t exist in your training
    data, and so on. For computer vision, noise would include corrupted image files;
    photos taken accidentally (while pointing the camera lens at the ground, for example);
    and artifacts that arise from using different cameras, resolutions, or compression
    techniques. Chances are that none of these types of noise is interesting for your
    task, so they will not produce an interesting or diverse range of samples to label.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的所有主动学习策略中，代表性采样最容易出现噪声。如果你有干净的训练数据，你的未标记数据中的噪声通常与训练数据最不同。在自然语言处理任务中，这种噪声可能包括损坏的文本、不属于你的目标域的语言文本、来自你的训练数据中不存在的地名列表的文本，等等。对于计算机视觉，噪声可能包括损坏的图像文件；例如，在将相机镜头对准地面时意外拍摄的图片；以及由于使用不同相机、分辨率或压缩技术而产生的伪影。这些类型的噪声中，没有一种对你的任务是有趣的，因此它们不会产生有趣或多样化的样本范围以进行标记。
- en: Finally, representative sampling can do more harm than good if you apply it
    only during later cycles of the active learning process, especially when you don’t
    have a domain adaptation problem. Suppose that you used uncertainty sampling for
    a few iterations of active learning and then applied representative sampling for
    later iterations. You have oversampled items *near* the decision boundary in early
    iterations, so representative sampling will oversample items *away* from the decision
    boundary in later iterations. This method will be worse than random sampling if
    you implement it this way.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你只在主动学习过程的后期周期中应用代表性采样，那么它可能会产生比好处更多的坏处，尤其是在你没有领域适应问题时。假设你使用了不确定性采样进行几次主动学习迭代，然后在后续迭代中应用代表性采样。你在早期迭代中过度采样了接近决策边界的项目，因此代表性采样将在后续迭代中过度采样远离决策边界的项目。如果你这样实施，这种方法将比随机采样更差。
- en: For these reasons, representative sampling is rarely used in isolation; it is
    most often used in algorithms or processes that combine representative sampling
    with uncertainty sampling. You might use representative sampling only for items
    that are also near the decision boundary, for example. In some of the foundational
    academic papers on representative sampling, you might see that what they mean
    by *representative sampling* is a combination of diversity and uncertainty. We
    will return to combinations of approaches in chapter 5, which is where we will
    get the most out of all the sampling techniques. In this chapter, we’ll introduce
    representative sampling in isolation so that you understand the basic principles
    before learning how to combine it with other methods.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，代表性采样很少单独使用；它最常用于将代表性采样与不确定性采样结合的算法或过程中。例如，你可能只为也接近决策边界的项目使用代表性采样。在一些关于代表性采样的基础学术论文中，你可能会看到他们所说的*代表性采样*是多样性和不确定性的结合。我们将在第5章中回到方法的组合，在那里我们将充分利用所有采样技术。在这一章中，我们将单独介绍代表性采样，以便你在学习如何将其与其他方法结合之前理解其基本原理。
- en: With these caveats, representative sampling can be useful for domain adaptation.
    In academic research, there is a focus on domain adaptation without any additional
    labels, which is often called *discrepancy* rather than *representation*. In industry,
    I have yet to encounter domain adaptation without additional human intervention,
    so it should become an important tool in your belt.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些注意事项，代表性采样对于领域适应可能是有用的。在学术研究中，人们关注的是没有任何额外标签的领域适应，这通常被称为*差异*而不是*表示*。在工业界，我还没有遇到没有额外人工干预的领域适应，因此它应该成为你工具箱中的重要工具。
- en: 4.4.2 Simple representative sampling
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 简单代表性采样
- en: 'As with our clustering example in section 4.4.1, we can use many algorithms
    for representative sampling. We mentioned one in chapter 2, in which a small modification
    to our outlier detection method calculated whether something was an outlier for
    the training data but not an outlier for the unlabeled data. Here, we’ll step
    up the sophistication a little and use cosine similarity from our training data
    to our unlabeled data, as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 4.4.1 节中的聚类示例一样，我们可以使用许多算法进行代表性采样。我们在第 2 章中提到过一种，其中对异常检测方法进行微小修改，计算某事物是否是训练数据的异常，但不是未标记数据的异常。在这里，我们将提高一点复杂性，并使用从我们的训练数据到未标记数据的余弦相似度，如下所示：
- en: Create one cluster containing the training data.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含训练数据的簇。
- en: Create a second cluster containing the unlabeled data.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含未标记数据的第二个簇。
- en: Sample items that have the greatest outlier score from the training relative
    to their outlier score from the unlabeled data.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据相对于未标记数据的异常分数中采样具有最大异常分数的项目。
- en: To try representative sampling, run
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试代表性采样，请运行
- en: '[PRE13]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This command samples 95 unlabeled items using representative sampling for you
    to annotate, along with 5 randomly selected items from remaining unlabeled items.
    The representative sampling function takes the training data and unlabeled data
    as arguments to find the unlabeled data items that are the most representative
    of the unlabeled data relative to the training data. Using our existing implementation
    for clustering, we can see that this is only a few lines of additional code.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令使用代表性采样为您采样 95 个未标记项目以便标注，以及从剩余未标记项目中随机选择的 5 个项目。代表性采样函数将训练数据和未标记数据作为参数，以找到相对于训练数据的未标记数据项目中最具代表性的项目。使用我们现有的聚类实现，我们可以看到这只需要几行额外的代码。
- en: Listing 4.6 Representative sampling in PyTorch
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.6 PyTorch 中的代表性采样
- en: '[PRE14]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Create a cluster for the training data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为训练数据创建一个簇。
- en: ❷ Create a cluster for the unlabeled data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为未标记数据创建一个簇。
- en: ❸ For each unlabeled item, calculate how close it is to the unlabeled data relative
    to the labeled data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对于每个未标记项目，计算它与标记数据相对于未标记数据的接近程度。
- en: As with the clustering code, if you are applying this sampling strategy to images,
    you might want to use a lower-dimension vector that has abstracted the image away
    from individual pixels. Nothing in the code needs to change if you are using a
    different dimensionality of features; you are only plugging that new data vector
    directly into the algorithm.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 与聚类代码一样，如果您将此采样策略应用于图像，您可能希望使用一个低维向量，该向量将图像从单个像素中抽象出来。如果您使用不同的特征维度，代码中不需要进行任何更改；您只需将新的数据向量直接插入算法中即可。
- en: 4.4.3 Adaptive representative sampling
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 自适应代表性采样
- en: A small change to our code means that we can make our representative sampling
    strategy adaptive within each active learning iteration. When we have sampled
    the most representative item, we know that the item will get a label later, even
    if we don’t know yet what that label will be. So we can add that single item to
    the hypothetical training data and then run representative sampling again for
    the next item. This approach will help prevent representative sampling from sampling
    only similar items. To try adaptive representative sampling, run
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们代码的微小修改意味着我们可以在每个主动学习迭代中使我们的代表性采样策略自适应。当我们采样了最具代表性的项目时，我们知道该项目将在以后获得标签，即使我们还不确定那个标签是什么。因此，我们可以将那个单独的项目添加到假设的训练数据中，然后对下一个项目再次运行代表性采样。这种方法将有助于防止代表性采样只采样相似的项目。要尝试自适应代表性采样，请运行
- en: '[PRE15]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This command samples 95 unlabeled items using adaptive representative sampling
    for you to annotate, along with 5 randomly selected items from remaining unlabeled
    items. The new code is even shorter, taking the same arguments and calling the
    representative sampling function once for each new item.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令使用自适应代表性采样为您采样 95 个未标记项目以便标注，以及从剩余未标记项目中随机选择的 5 个项目。新代码甚至更短，使用相同的参数，并对每个新项目调用一次代表性采样函数。
- en: Listing 4.7 Adaptive representative sampling in PyTorch
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.7 PyTorch 中的自适应代表性采样
- en: '[PRE16]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With our building blocks of clusters and representative sampling, it is a small
    extension codewise to start implementing more sophisticated active learning strategies.
    We will cover more of these advanced techniques in detail in chapter 5\. The code
    will be as short in most cases, but it is important to know the building blocks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的集群和代表性采样的构建块，在代码上开始实施更复杂的主动学习策略只是一个小的扩展。我们将在第5章中详细介绍这些高级技术。在大多数情况下，代码将保持简短，但了解构建块很重要。
- en: Note that this function takes a while to run because it needs to reevaluate
    the representative score for every unlabeled data point you are sampling. So if
    you are running this code on a smaller server or personal computer, you may want
    to lower the `number` of items to sample or the `limit` on items to consider so
    that you can see the results of this sampling strategy without waiting a long
    time.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个函数运行需要一段时间，因为它需要重新评估您所采样的每个未标记数据点的代表性得分。因此，如果您在一个较小的服务器或个人电脑上运行此代码，您可能希望降低要采样的`数量`或考虑的`限制`，这样您就可以在不等待很长时间的情况下看到此采样策略的结果。
- en: 4.5 Sampling for real-world diversity
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 为现实世界多样性进行采样
- en: 'Strategies for identifying and reducing bias are complicated and could fill
    a book of their own. In this text, we will concentrate on the data annotation
    problem: ensuring that training data represents real-world diversity as fairly
    as possible. As you read in the introduction to this chapter, we expect more from
    machine learning in some cases than we do from people. We expect many models to
    include something closer to English’s 200,000-word vocabulary than the ~40,000
    words known by a typical fluent human, for example. Therefore, this section covers
    the current best practice for ensuring that models are fair from an active learning
    point of view, knowing that measuring and reducing real-world bias is a complicated
    field that is far from solved.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 识别和减少偏差的策略很复杂，可能足以填满一本书。在本文本中，我们将集中讨论数据标注问题：确保训练数据尽可能公平地代表现实世界的多样性。正如您在本章引言中所读到的，在某些情况下，我们对机器学习的期望比对人还高。例如，我们期望许多模型包含接近英语20万个词汇量，而不是典型流利人士所知的约4万个词汇。因此，本节涵盖了确保模型从主动学习角度公平的最佳实践，同时知道衡量和减少现实世界偏差是一个复杂且远未解决的问题的领域。
- en: 'The demographics for real-world diversity can be any real-world division that
    is meaningful for your data. Here is a (nonexhaustive) list of the kind of demographics
    that we might care about for our disaster-response examples:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的多样性人口可以是任何对您的数据有意义的现实世界划分。以下是一个（非详尽）的列表，列出了我们可能对我们灾害响应示例中关心的多样性人口类型：
- en: '*Language*—Can we more accurately identify disaster-related content written
    in certain languages? There is an obvious bias here, as the data is mostly English.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言*—我们能否更准确地识别用某些语言编写的与灾害相关的内容？这里存在明显的偏见，因为数据大部分是英文。'
- en: '*Geography*—Can we more accurately identify disaster-related content from/
    about some countries? There is a high chance of bias here, as some countries will
    have more media reporting their disasters, and there will also be country-level
    population biases.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*地理*—我们能否更准确地识别来自/关于某些国家的与灾害相关的内容？这里存在很高的偏差可能性，因为一些国家会有更多媒体报道其灾害，并且还会存在国家层面的人口偏差。'
- en: '*Gender*—Can we more accurately identify disaster-related content from/about
    people of one gender? It is possible that more males are writing the content than
    other genders, and this could be reflected in the style of writing.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性别*—我们能否更准确地识别来自/关于某一性别人群的与灾害相关的内容？可能男性比其他性别更多地编写内容，这可能会反映在写作风格中。'
- en: '*Socioeconomics*—Can we more accurately identify disaster-related content from/
    about people with different incomes? There is often more reporting about wealthy
    nations, so perhaps this situation leads to bias in the data and models.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社会经济*—我们能否更准确地识别来自/关于不同收入人群的与灾害相关的内容？关于富裕国家的报道通常更多，因此这种情况可能导致数据和模型中的偏差。'
- en: '*Race and ethnicity*—Can we more accurately identify disaster-related content
    from/about people of certain races or ethnicity? The media articles often portray
    the same type of event, such as a shooting by a lone man, as part of a war of
    terror for some ethnicities (and therefore disaster-related) but as an individual
    crime for other ethnicities (and therefore not disaster-related).'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*种族和民族*——我们能否更准确地识别来自或关于某些种族或民族的与灾难相关的内容？媒体文章往往将同一类型的事件，如一名男子单独进行的枪击，描述为某些民族的恐怖战争的一部分（因此与灾难相关），但对于其他民族来说则被视为个人犯罪（因此与灾难无关）。'
- en: '*Date and time*—Can we more accurately identify disaster-related content at
    certain times of the day, days of the week, or months of the year? Fewer articles
    are published on weekends, and those articles tend to be more human-interest focused.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日期和时间*——我们能否在一天中的某些时间、一周中的某一天或一年中的某个月份更准确地识别与灾难相关的内容？周末发布的文章较少，而且这些文章往往更关注人文兴趣。'
- en: The biases may be different in combination, a situation known as *intersectional
    bias*. A bias toward people of a certain gender might be better, worse, or even
    inverted for some races and ethnicities, for example.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差可能在不同组合中有所不同，这种情况被称为*交叉性偏差*。对某些性别的人的偏见可能对某些种族和民族来说更好、更差，甚至可能相反。
- en: Depending on where you deploy your model, you may need to conform to local laws.
    In California, for example, labor laws prevent discrimination in several demographics,
    including many of the ones in the preceding list, as well as age, immigration
    status, sexual orientation, and religion. In some cases, solving the problem by
    encoding the data to change the sampling strategy may not be the right choice;
    instead, you need to solve the problem while you are collecting the data.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你部署模型的位置，你可能需要遵守当地法律。例如，在加利福尼亚州，劳动法禁止在多个人口统计中歧视，包括前述列表中的许多人口统计，以及年龄、移民状态、性取向和宗教。在某些情况下，通过编码数据以改变采样策略来解决问题可能不是正确的选择；相反，你需要在收集数据时解决问题。
- en: 4.5.1 Common problems in training data diversity
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 训练数据多样性的常见问题
- en: 'Three common problems for fairness in data are summarized in figure 4.7\. Each
    of the three demographics in figure 4.7 shows common problems that you will find
    when trying to create training data:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7总结了数据公平性的三个常见问题。图4.7中的三个人口统计显示了在尝试创建训练数据时可能会遇到的一些常见问题：
- en: A demographic that is overrepresented in your training data but not from the
    same distribution as your training data (X)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的训练数据中过度代表但并非来自相同分布的人口统计（X）
- en: A demographic that is from a distribution similar to the overall data distribution
    but not yet represented in a balanced way in the training data (O)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种人口统计，其分布与整体数据分布相似，但在训练数据中尚未以平衡的方式表示（O）
- en: A demographic that is underrepresented in the training data in such a way that
    the resulting model might be worse than if random sampling were used (Z)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种在训练数据中代表性不足的人口统计，以至于产生的模型可能比使用随机采样更差（Z）
- en: '![](../Images/CH04_F07_Munro.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F07_Munro.png)'
- en: Figure 4.7 An example of the problems that diversity sampling tries to address.
    Here, we have items mapped to three real-world demographics that we’re calling
    X, O, and Z.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7展示了多样性采样试图解决的问题的例子。在这里，我们有项目映射到三个现实世界的人口统计，我们称之为X、O和Z。
- en: Demographic X looks pretty good. All the examples we have so far are within
    the current training data. X is not the same distribution as the training data
    as a whole. This problem isn’t typical of neural models, but it could be a problem
    with simpler models such as Naive Bayes. X is typical of a privileged demographic
    with a positive bias, such as standard English data within a multilingual dataset.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 人口统计X看起来相当不错。到目前为止，我们所有的例子都在当前训练数据中。X与整体训练数据的分布并不相同。这个问题并不典型于神经网络模型，但它可能是像朴素贝叶斯这样的简单模型的问题。X是特权人口统计的典型例子，具有积极的偏差，例如在多语言数据集中标准英语数据。
- en: Demographic O is partially in the training data today and partially outside
    it. O is distributed across the entire feature range fairly evenly. So if we can
    collect training data that is representative of the entire feature space, we are
    least worried about O. O is typical of a demographic with minimal bias (positive
    or negative), such as time-based demographics, in which each item was collected
    carefully over a certain period.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 人口统计学O今天部分在训练数据中，部分在训练数据外。O在整个特征范围内分布得相当均匀。因此，如果我们能够收集代表整个特征空间的训练数据，我们对O就最不担心。O是具有最小偏差（正面或负面）的人口统计学的典型代表，例如基于时间的
    demographics，其中每个项目都在一定时期内被仔细收集。
- en: By contrast, demographic Z is clustered outside the current training data. Even
    worse, a Z data point inside the current training data appears to be an outlier
    for Z. The model may not have information about Z and may actually be modeling
    Z incorrectly. Z is typical of an underrepresented demographic, such as an underrepresented
    ethnicity that does not appear in the dataset except when that person happens
    to share features with a more privileged demographic.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，人口统计学Z在当前训练数据之外聚集。更糟糕的是，当前训练数据中的Z数据点似乎对Z来说是异常值。模型可能没有关于Z的信息，实际上可能错误地建模了Z。Z是代表性不足的人口统计学的典型代表，例如在数据集中没有出现的代表性不足的种族，除非那个人偶然与更受优待的人口统计学共享特征。
- en: Machine learning algorithms themselves are not prone to many inherent biases
    that are not already in the data, although those biases are possible. Most of
    the time, when an algorithm shows bias, it is reflecting or amplifying a bias
    that comes from the training data or the way that the training data is represented
    as features for the model. Even if bias comes solely from the model itself, you
    are probably responsible for creating the evaluation data to detect and measure
    that bias. If the source of data leads to poor results, you are also responsible
    for identifying that fact when you start to annotate the data. So if you are responsible
    for annotating the data, you may have more influence on model fairness than anyone
    else in your organization.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法本身并不容易受到数据中已经存在的许多固有偏差的影响，尽管这些偏差是可能的。大多数时候，当一个算法表现出偏差时，它是在反映或放大来自训练数据或训练数据表示为模型特征的方式中的偏差。即使偏差完全来自模型本身，你也可能需要负责创建用于检测和测量该偏差的评估数据。如果数据来源导致结果不佳，当你开始标注数据时，你也负责识别这一事实。所以如果你负责标注数据，你可能在模型公平性方面比组织中的任何人都更有影响力。
- en: Note that many researchers in AI ethics use a broader definition of *algorithm*
    than most computer scientists do, including the treatment of the data for the
    machine learning models and the interpretation of the output. This definition
    is not inherently better or worse—only different. Be mindful about exactly which
    parts of an application using machine learning are being referred to when you
    read about algorithms in the AI ethics literature.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，许多人工智能伦理学研究人员使用的“算法”定义比大多数计算机科学家更广泛，包括机器学习模型的数据处理和输出解释。这种定义本身并不更好或更差——只是不同。当你阅读人工智能伦理文献中的算法时，请注意具体指的是应用程序中哪些使用机器学习的部分。
- en: 4.5.2 Stratified sampling to ensure diversity of demographics
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 确保人口统计学多样性的分层抽样
- en: 'Without a reference data set of unlabeled items from each demographic, you
    need to apply active learning strategies that you have applied before, but now
    in a stratified manner across all your data:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 没有来自每个人口统计学的未标记项目参考数据集，你需要应用之前已经应用过的主动学习策略，但现在以分层方式应用于所有数据：
- en: Apply least confidence sampling for every demographic, selecting an equal number
    of items in each demographic where that demographic was the most-confident prediction.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个人口统计学应用最小置信度抽样，在每个人口统计学中选择与该人口统计学最自信预测相等的项目数量。
- en: Apply margin of confidence sampling for every demographic, selecting an equal
    number of items in each demographic where that demographic was the most-confident
    or second-most-confident. Recall that margin of confidence is explicitly looking
    at the two most confident.)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个人口统计学应用置信度边际抽样，在每个人口统计学中选择与该人口统计学最自信或次自信相等的项目数量。记住，置信度边际是明确查看两个最自信的项目。）
- en: Apply model-based outlier detection for each demographic.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个人口统计学应用基于模型的异常值检测。
- en: Apply cluster-based sampling within each demographic.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个人口统计学内应用基于聚类的抽样。
- en: Basically, in the same way that we wanted the best possible dataset from our
    unlabeled data as a whole, we want to do this for each demographic, being careful
    to stratify the sampling across those demographics.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们希望从整体未标记数据中获得最佳可能的数据集，同样地，我们也希望为每个群体做这件事，同时注意在那些群体之间进行分层抽样。
- en: There is no separate code for this task in this chapter. You should be able
    to divide up the data according to demographics that you care about and apply
    the sampling strategies only to data for each demographic.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中没有为这个任务提供单独的代码。你应该能够根据你关心的群体划分数据，并且只对每个群体的数据进行抽样策略的应用。
- en: '4.5.3 Represented and representative: Which matters?'
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.3 代表性和代表性：哪个更重要？
- en: 'There is a subtle but important difference between having data that is representative
    of a demographic and having the demographic be well-represented in the data. The
    distinction is especially important depending on the type of model you are using,
    so we will tease them apart here:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有代表一个群体的数据与该群体在数据中得到良好代表之间有一个微妙但重要的区别。这种区别在使用的模型类型不同时尤其重要，因此我们将在这里区分它们：
- en: '*Representative demographic data*—Your data is representative of a demographic
    if it is drawn from the same distribution as that demographic. In statistical
    terms, your labeled data is representative if it is independent and identically
    distributed (IDD) from a data that is randomly drawn from that demographic.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代表性群体数据*—如果你的数据是从与该群体相同的分布中抽取的，那么你的数据就是代表该群体的。在统计术语中，如果你的标记数据是从随机抽取该群体的数据中独立且同分布（IDD）的，那么你的标记数据就是代表性的。'
- en: '*A demographic that is well-represented*—A demographic is well-represented
    if there is sufficient data representing that demographic for your model to perform
    fairly, but the data is not required to be IDD.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个得到良好代表的群体*—一个群体如果存在足够的数据代表该群体，以便你的模型能够公平地执行，但数据不需要是IDD。'
- en: If you know that your unlabeled data fairly represents the demographic you care
    about and it is accurately encoded for that demographic, you can create an additional
    evaluation data set that draws randomly from within each demographic. If your
    demographics are not equally frequent, this approach will be faster than creating
    evaluation data by randomly sampling from across the entire data set. But you
    can use this dataset only for evaluating your per-demographic accuracy (section
    4.5.4).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道你的未标记数据公平地代表了你所关心的群体，并且它对该群体进行了准确编码，你可以创建一个额外的评估数据集，该数据集从每个群体中随机抽取。如果你的群体不是同等频繁的，这种方法将比从整个数据集中随机抽样创建评估数据更快。但你可以只使用这个数据集来评估你的每个群体的准确性（第4.5.4节）。
- en: Remember that your unlabeled data may *not* be representative of each demographic.
    Data in this chapter that comes from an Australian media organization focuses
    on news within Australia and countries that are geographically or politically
    close to Australia. Articles about Uganda, for example, are not going to be representative
    of actual events in Uganda; the data will be biased toward events that are perceived
    as more important for Australia. It is not possible to get representative data
    for Uganda in this case. Instead, you should use clustering to get as diverse
    a set of articles about Uganda as possible so that at very least, articles about
    Uganda are well-represented.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你的未标记数据可能*不*代表每个群体。本章中来自澳大利亚媒体组织的这一章的数据专注于澳大利亚及其地理或政治上接近的国家新闻。例如，关于乌干达的文章不会代表乌干达的实际事件；数据将偏向于那些被认为对澳大利亚更重要的事件。在这种情况下，不可能获得代表乌干达的数据。相反，你应该使用聚类来获取尽可能多样化的关于乌干达的文章集合，这样至少可以保证关于乌干达的文章得到良好代表。
- en: If you are using a neural model, you might be OK if you have well-represented
    data that is *not* representative. Provided that there is enough data, a neural
    model can be accurate for all items in a given demographic, even if it was trained
    on data that was imbalanced within that demographic. The Uganda news articles,
    for example, might be balanced too much toward sports-related articles. Provided
    there are sufficient examples of the other types of news from Uganda for your
    model to be accurate on those topics, it won’t matter that sports-related news
    is overrepresented; your model can be equally accurate on all types of news from
    Uganda.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是神经网络模型，那么即使数据有很好的代表性但并不具有代表性，你可能也还可以接受。只要数据足够多，神经网络模型可以针对给定的人口统计中的所有项目保持准确性，即使它在训练时使用的数据在该人口统计中是不平衡的。例如，乌干达的新闻文章可能过多地偏向于体育相关文章。只要你的模型有足够的乌干达其他类型新闻的例子来确保在这些主题上的准确性，那么体育相关新闻的过度代表就不会产生影响；你的模型可以对乌干达的所有类型新闻都保持相同的准确性。
- en: If you are using generative models, however, especially a simpler one like Naive
    Bayes, your model is explicitly trying to model the classification task by assuming
    representative data. In this case, you need to work harder to ensure that your
    data is representative or to try to encode representativeness in your model by
    manipulating parameters such as the prior probability of certain data types.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用的是生成模型，尤其是像朴素贝叶斯这样的简单模型，你的模型会明确地通过假设代表性数据来尝试对分类任务进行建模。在这种情况下，你需要更加努力地确保你的数据具有代表性，或者通过操纵某些参数（如某些数据类型的先验概率）来尝试在模型中编码代表性。
- en: This approach separates sampling for real-world diversity from stratified sampling.
    In the social sciences, *stratified sampling* is a technique for ensuring that
    data is as representative as possible and for weighting the results of activities
    such as surveys to account for demographic imbalances. Depending on the neural
    model, it might be enough that the data exists in the training data and the bias
    won’t be perpetuated. On the other hand, a model might amplify any bias. So, the
    situation becomes a little more complicated and needs to be tackled holistically,
    taking into account the machine learning architecture. If you care about the real-world
    diversity of your models, the literature on stratified sampling is still a good
    place to start, knowing that this sampling strategy will not necessarily be the
    only solution to the problem.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将现实世界多样性的采样与分层采样分开。在社会科学中，*分层采样*是一种确保数据尽可能具有代表性的技术，并且用于调整调查等活动的结果，以考虑人口统计的不平衡。根据神经网络模型的不同，数据存在于训练数据中可能就足够了，偏见不会持续存在。另一方面，一个模型可能会放大任何偏见。因此，情况变得稍微复杂一些，需要整体解决，考虑到机器学习架构。如果你关心你模型的现实世界多样性，分层采样的文献仍然是一个好的起点，了解这种采样策略不一定是解决问题的唯一方案。
- en: 4.5.4 Per-demographic accuracy
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.4 按人口统计的准确性
- en: If we have real-world demographics in our data, we can calculate a variation
    of macro accuracy according to those demographics. For each item belonging to
    a certain demographic, how many of those were predicted correctly for their given
    labels? Note that each “error” will be both a false positive and a false negative.
    So unless you are excluding certain labels from your accuracy or are setting a
    threshold for trusted predictions, you will have identical precision and recall
    values for per-demographic accuracy (the same situation as for micro precision
    and recall). Let *d* indicate membership in each demographic. Precision and recall,
    therefore, are
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据中有现实世界的人口统计数据，我们可以根据这些统计数据计算宏观准确性的变体。对于属于某个特定人口统计的每个项目，有多少个被正确预测了它们的给定标签？请注意，每个“错误”都将既是误报也是漏报。因此，除非你从准确性中排除了某些标签，或者为可信预测设置了阈值，否则你将具有相同的精确度和召回率值（与微观精确度和召回率相同的情况）。用*d*表示属于每个人口统计的成员。因此，精确度和召回率是
- en: '![](../Images/CH04_F07_Munro_E01.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F07_Munro_E01.png)'
- en: I haven’t seen this technique used often in industry, but that doesn’t mean
    that it shouldn’t be adopted. Most studies of demographic inequality tend to be
    ad-hoc. For face recognition, for example, there are many examples of popular
    media organizations selecting a small number of images of people who represent
    different ethnicities and looking for different levels of accuracy across those
    ethnicities. In those use cases, the media organizations are testing precision
    only, and on a small (and possibly nonrepresentative) sample. That approach works
    for a media story, but it won’t work if we are serious about improving the fairness
    of our models.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有在工业界经常看到这种技术的使用，但这并不意味着它不应该被采用。大多数关于人口统计不平等的研究往往是临时的。例如，在人脸识别方面，有许多流行媒体机构选择少量代表不同种族的人的图片，并寻找这些种族之间的不同准确度水平。在这些用例中，媒体机构仅测试精确度，并且是在一个小（可能不具有代表性）的样本上。这种方法适用于媒体故事，但如果我们认真对待提高我们模型公平性的话，这种方法是行不通的。
- en: 'If you are responsible for building the model and ensuring that it is as fair
    as possible, you should look at a broader range of ways to measure accuracy. You
    may want to refine demographic-based accuracy further, according to your use case.
    Here are some options:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你负责构建模型并确保其尽可能公平，你应该考虑更广泛的方式来衡量精度。你可能希望根据你的用例进一步细化基于人口统计的精度。以下是一些选项：
- en: '*Minimum accuracy*—The lowest precision, recall, and/or F-score of any demographic.
    If you want to treat your model as being only as strong as its weakest link in
    terms of fairness across demographics, you should take the minimum accuracy. You
    could take the minimum F-score from one demographic. For a harsher metric, take
    the minimum precision and minimum recall, possibly from different labels, and
    apply the F-score to those.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小精度*—任何人口统计中的最低精确度、召回度或F分数。如果你想将你的模型视为在人口统计公平性方面仅与其最薄弱环节一样强大，你应该取最小精度。你可以从一个特定的人口统计中取最小F分数。对于更严格的指标，可以取最小精确度和最小召回度，可能来自不同的标签，并将F分数应用于这些。'
- en: '*Harmonic accuracy*—The harmonic mean of the per-demographic accuracy, which
    will be harsher than the average demographic accuracy but not as harsh as taking
    the minimum (unless there are 0 values). As we take the harmonic mean of precision
    and recall to get the F-score, instead of the arithmetic mean, we could also take
    the harmonic mean. The harmonic mean will punish outlier low accuracies more than
    it rewards outlier high accuracies, but not as much as taking the minimum.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*谐波精度*—按人口统计的精度计算出的调和平均值，这将比平均人口统计精度更严格，但不会像取最小值（除非有0值）那样严格。正如我们通过取精确度和召回率的调和平均值来得到F分数，而不是算术平均值一样，我们也可以取调和平均值。调和平均值会更多地惩罚异常低精度，而不是奖励异常高精度，但不会像取最小值那样多。'
- en: 4.5.5 Limitations of sampling for real-world diversity
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.5 实际世界多样性的采样局限性
- en: The biggest shortcoming of sampling for real-world diversity is that you have
    no way to guarantee that the model will be perfect, but you can measure the bias
    more accurately and ensure that your models will be much fairer than if you used
    only random sampling. Sometimes, you won’t be able to make up for the bias, simply
    because not enough unlabeled data is available. I have worked in disaster response
    in languages such as Haitain Kreyol and Urdu, where there simply wasn’t enough
    available data to cover the same breadth of potential disasters that we have for
    the English headlines. There is no way to fix this problem with labeling alone.
    Data collection is outside the scope of this book, but we will return to some
    other relevant techniques in chapter 9, when we cover methods for creating synthetic
    data.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 实际世界多样性采样的最大缺点是你无法保证模型是完美的，但你可以更准确地测量偏差，并确保你的模型将比仅使用随机采样时的模型公平得多。有时，你可能无法弥补偏差，仅仅是因为可用的未标记数据不足。我在海地克里奥尔语和乌尔都语等语言中从事灾害响应工作，那里根本没有足够的数据来覆盖与英语标题相同的潜在灾害范围。仅通过标记是无法解决这个问题。数据收集超出了本书的范围，但我们在第9章中介绍创建合成数据的方法时，将回到一些其他相关技术。
- en: 4.6 Diversity sampling with different types of models
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 使用不同类型模型的多样性采样
- en: You can apply diversity sampling to any type of model architecture. Similar
    to what we learned in chapter 3 for uncertainty sampling, sometimes diversity
    sampling for other models is the same as for neural models, and sometimes diversity
    sampling is unique to a given type of model.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将多样性采样应用于任何类型的模型架构。类似于我们在第3章学习的不确定性采样，有时其他模型的多样性采样与神经网络模型相同，有时多样性采样是特定于某种类型模型的。
- en: 4.6.1 Model-based outliers with different types of models
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.1 使用不同类型模型的基于模型的异常值
- en: 'For models that use linear regression, you can calculate model outliers in
    the same way as for a neural model: which items have the least activation across
    all labels? Use the prenormalized prediction scores, if you have access to them,
    as you did with the logits in this chapter.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用线性回归的模型，你可以像神经网络模型一样计算模型异常值：哪些项目在所有标签上的激活度最低？如果你可以访问它们，可以使用预先归一化的预测分数，就像你在本章中对logits所做的那样。
- en: In the case of Bayesian models, a model-based outlier has the lowest overall
    probability of each label. As with our neural models here, you could calculate
    lowest overall as the lowest average or the lowest maximum, depending on what
    makes the most sense for your use case.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯模型的情况下，基于模型的异常值具有每个标签最低的整体概率。正如我们在这里的神经网络模型一样，你可以根据你的用例中最有意义的方式计算最低的整体值，是最低的平均值还是最低的最大值。
- en: 'In the case of SVMs, you can look for predictions that are near the hyperplane
    (decision boundary) but are the maximal distance from the support vectors themselves:
    the training items that determine the decision boundary. These items will be the
    equivalent of the model outliers that have high uncertainty in neural models.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVMs的情况下，你可以寻找接近超平面（决策边界）但与支持向量本身的最大距离的预测：确定决策边界的训练项目。这些项目将是神经网络模型中具有高不确定性的模型异常值的等价物。
- en: 4.6.2 Clustering with different types of models
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.2 使用不同类型模型的聚类
- en: You can use the unsupervised clustering methods in this chapter, such as k-means,
    to sample for any supervised machine learning algorithm. There is no need to change
    the k-means approach for different types of supervised machine learning algorithms,
    so you can start with the ones in this chapter and then think about refining them
    based on your model and data.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用本章中提到的无监督聚类方法，例如k-means，为任何监督机器学习算法进行采样。不需要为不同类型的监督机器学习算法改变k-means方法，因此你可以从本章中的方法开始，然后根据你的模型和数据考虑对其进行优化。
- en: If you want to go deeper into cluster-based sampling, a lot of research was
    done on diversity sampling in the early 2000s. SVMs were at their peak popularity
    at the same time, so you will need to brush up on your SVM knowledge to get the
    most out of the research done at that time.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要深入了解基于聚类的采样，2000年代初对多样性采样进行了大量研究。SVMs在当时达到了顶峰，因此你需要复习你的SVM知识，以充分利用当时的研究成果。
- en: 4.6.3 Representative sampling with different types of models
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.3 使用不同类型模型的代表性采样
- en: As mentioned earlier in the chapter, you could use Naive Bayes or Euclidean
    distance for representative sampling instead of cosine similarity. Any distance
    function could be as good for your particular data; we used cosine similarity
    in this book only because of the continuity from section 4.3 on clustering. If
    you changed the distance function in the clustering algorithm from cosine similarity
    to the probability of cluster membership, this edit of a few lines of code would
    be enough to you to try Bayesian clustering.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，你可以使用朴素贝叶斯或欧几里得距离进行代表性采样，而不是余弦相似度。任何距离函数都可能适用于你的特定数据；我们在这本书中只使用余弦相似度，是因为从第4.3节关于聚类的连续性。如果你将聚类算法中的距离函数从余弦相似度更改为聚类成员的概率，那么这仅仅需要几行代码的编辑，你就可以尝试贝叶斯聚类。
- en: Decision trees offer a unique type of diversity sampling. You can look at where
    the number of predictions in different leaves differs from training to evaluation
    data. Suppose that your decision tree has 10 leaves, and all 10 leaves have an
    equal number of items when predicting your validation data. Now imagine that when
    you apply the model to your unlabeled data, 90% of that data ends up in one leaf.
    That leaf obviously represents the type of data in your target domain much better
    than your training data so far. So you should sample more items from within the
    leaf with 90% of the data, knowing that the data is more important for where you
    will deploy your model.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树提供了一种独特的多样性采样类型。你可以查看不同叶子中的预测数量在训练数据和评估数据之间的差异。假设你的决策树有10个叶子，当预测你的验证数据时，所有10个叶子中的项目数量都相等。现在想象一下，当你将模型应用于你的未标记数据时，90%的数据最终落在了一个叶子上。这个叶子显然比迄今为止的训练数据更好地代表了目标域中的数据类型。因此，你应该从包含90%数据的叶子中抽取更多项目，知道这些数据对你将部署模型的地方更重要。
- en: 4.6.4 Sampling for real-world diversity with different types of models
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.4 使用不同类型的模型进行现实世界多样性采样
- en: The strategies for improving diversity in your neural models can be applied
    to other types of machine learning models. You want to ensure that you are optimizing
    for the same number of labels for each demographic and for equal accuracy for
    each demographic.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 提高你的神经网络模型多样性的策略可以应用于其他类型的机器学习模型。你想要确保你正在为每个人口统计优化相同数量的标签，并为每个人口统计的相等精度进行优化。
- en: 4.7 Diversity sampling cheat sheet
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 多样性采样速查表
- en: Figure 4.8 is a cheat sheet for the four diversity sampling approaches that
    you implemented in this chapter. If you are confident about these strategies,
    keep this cheat sheet handy as a quick reference.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8是本章中你实施的四种多样性采样方法的速查表。如果你对这些策略有信心，请将此速查表放在手边，以便快速查阅。
- en: '![](../Images/CH04_F08_Munro.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F08_Munro.png)'
- en: 'Figure 4.8 Cheat sheet for the types of diversity sampling covered in this
    chapter: model-based outlier sampling, cluster-based sampling, representative
    sampling, and sampling for real-world diversity. These four strategies ensure
    diversity and representation in your data—respectively, items that are unknown
    to your model in its current state; items that are statistically representative
    of the entire distribution of your data; items that are maximally representative
    of where you are going to deploy your model; and items that are most representative
    of real-world demographics.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8本章涵盖的多样性采样类型的速查表：基于模型的异常值采样、基于聚类的采样、代表性采样和现实世界多样性采样。这四种策略确保了数据中的多样性和代表性——分别对应于模型当前状态下未知的项目；统计上代表整个数据分布的项目；最大程度代表你将部署模型的地方的项目；以及最代表现实世界人口统计的项目。
- en: 4.8 Further reading
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 进一步阅读
- en: You will have to go outside of the machine learning literature for many of the
    most important papers related to diversity sampling. If you are focused on collecting
    the right data, then the language documentation and archiving literature starting
    in the early 2000s is the best starting place. If you are focused on stratified
    sampling within your data, then there is a century of social science literature
    that is relevant in fields as varied as education and economics. This section
    limits the further reading to the machine learning literature, so keep in mind
    that the best papers are building on advances in other fields.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 你将不得不超出机器学习文献的范围，去寻找与多样性采样相关的许多最重要的论文。如果你专注于收集正确的数据，那么从2000年代初开始的文档语言和存档文献是最好的起点。如果你专注于数据中的分层抽样，那么有一百年的社会科学文献与教育、经济学等各个领域相关。本节将进一步阅读限制在机器学习文献中，所以请记住，最好的论文是建立在其他领域的进步之上的。
- en: 4.8.1 Further reading for model-based outliers
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1 基于模型的异常值进一步阅读
- en: The model-based outlier algorithms are ones that I’ve personally developed and
    have not yet published outside this book except in informal presentations and
    classes. The literature on neural-based methods for determining outliers is growing
    but tends to focus on statistical outliers not low activation.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的异常值算法是我个人开发的，尚未在本书以外的出版物中发表，除了在非正式的演示和课程中。关于确定异常值的基于神经的方法的文献正在增长，但往往关注的是统计异常值而不是低激活。
- en: The practice of investigating a neural model to determine its knowledge (or
    lack thereof) is sometimes called *probing*. While there aren’t yet papers on
    probing to discover outliers for active learning, there are no doubt some good
    techniques in the broader model probing literature that could be adapted for this
    purpose.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 调查神经网络模型以确定其知识（或缺乏知识）的实践有时被称为*探查*。虽然还没有关于探查以发现主动学习中的异常值的论文，但无疑在更广泛模型探查文献中存在一些可以为此目的改编的良好技术。
- en: 4.8.2 Further reading for cluster-based sampling
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2 基于聚类的采样进一步阅读
- en: For cluster-based sampling, the best starting point is “Active Learning Using
    Pre-clustering,” by Hieu T. Ngyuen and Arnold Smeulders ([http://mng.bz/ao6Y](http://mng.bz/ao6Y)).
    For the most cutting-edge research on cluster-based sampling, look for papers
    that cited these authors recently and are themselves highly cited.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于聚类的采样，最佳起点是Hieu T. Ngyuen和Arnold Smeulders的《使用预聚类进行主动学习》([http://mng.bz/ao6Y](http://mng.bz/ao6Y))。对于基于聚类的采样最前沿的研究，寻找近期被这些作者引用且自身引用率很高的论文。
- en: Note that Ngyuen and Smeulders used an active learning metric that combines
    clustering with uncertainty sampling. As noted earlier in this chapter, this combination
    is the most common way to use clustering within active learning. The topics are
    taught separately in this text so you can understand them in isolation before
    learning how to combine them. Before jumping into the research, you may want to
    read chapter 5, in which you combine clustering and uncertainty sampling.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Ngyuen和Smeulders使用了一个结合聚类与不确定性采样的主动学习度量。正如本章前面所提到的，这种组合是主动学习中使用聚类的最常见方式。本文本中这些主题是分别教授的，这样你可以在学习如何结合它们之前单独理解它们。在深入研究之前，你可能想阅读第5章，其中你将聚类与不确定性采样结合起来。
- en: The earliest papers that look at clustering for active learning came from scientists
    in Russia. The first English version of these papers that I’m aware of is Novosibirk
    Zagoruiko’s “Classification and Recognition” ([http://mng.bz/goXn](http://mng.bz/goXn)).
    If you can read Russian, you can find even earlier papers from scientists who
    were thinking about this problem more than 50 years ago!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最早关注主动学习聚类问题的论文来自俄罗斯的科学家。我所知道的这些论文的第一版英文翻译是Novosibirk Zagoruiko的《分类与识别》([http://mng.bz/goXn](http://mng.bz/goXn))。如果你能阅读俄语，你甚至可以找到50多年前就开始思考这个问题的科学家的更早论文！
- en: 4.8.3 Further reading for representative sampling
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.3 代表性采样进一步阅读
- en: The principles of representative sampling were first explored in “Employing
    EM and Pool-Based Active Learning for Text Classification,” by Andrew Kachites
    McCallum and Kamal Nigam ([http://mng.bz/e54Z](http://mng.bz/e54Z)). For the most
    cutting-edge research on representative sampling, look for papers that cited these
    authors recently and are themselves highly cited.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性采样的原则首先在Andrew Kachites McCallum和Kamal Nigam的《使用EM和池式主动学习进行文本分类》中进行了探索([http://mng.bz/e54Z](http://mng.bz/e54Z))。对于代表性采样的最前沿研究，寻找近期被这些作者引用且自身引用率很高的论文。
- en: 4.8.4 Further reading for sampling for real-world diversity
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.4 为现实世界多样性采样的进一步阅读
- en: 'Here are two good papers on machine learning for real-world diversity, one
    each in computer vision and NLP. Both find that popular models are more accurate
    for people from wealthier backgrounds and that training data is biased toward
    the object seen by wealthier people and the languages spoken by wealthier/majority
    populations:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两篇关于现实世界多样性的机器学习的好论文，一篇在计算机视觉领域，一篇在自然语言处理领域。两者都发现，对于来自更富裕背景的人来说，流行的模型更准确，训练数据偏向于更富裕的人看到的对象和富裕/多数人群使用的语言：
- en: “Does Object Recognition Work for Everyone?”, by Terrance DeVries, Ishan Misra,
    Changhan Wang, and Laurens van der Maaten ([http://mng.bz/pVG0](http://mng.bz/pVG0)).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Terrance DeVries、Ishan Misra、Changhan Wang和Laurens van der Maaten的《是否对象识别对每个人都是有效的？》([http://mng.bz/pVG0](http://mng.bz/pVG0))。
- en: “Incorporating Dialectal Variability for Socially Equitable Language dentification,”
    by David Jurgens, Yulia Tsvetkov, and Dan Jurafsky ([http://mng.bz/OEyO](http://mng.bz/OEyO)).
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David Jurgens、Yulia Tsvetkov和Dan Jurafsky的《为社会公平的语言识别引入方言变异性》([http://mng.bz/OEyO](http://mng.bz/OEyO))。
- en: 'For a critical review of bias in the language technology literature, including
    how inconsistently the term bias is used, I recommend Su Lin Blodgett, Solon Barocas,
    Hal Daumé III, and Hanna Wallach’s paper “Language (Technology) Is Power: A Critical
    Survey of ‘Bias’ in NLP” ([http://mng.bz/Yq0Q](http://mng.bz/Yq0Q)).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言技术文献中偏差的批判性回顾，包括该术语使用的不一致性，我推荐苏琳·布洛杰特、索隆·巴罗卡斯、哈尔·道姆三世和汉娜·沃拉奇的论文“语言（技术）是力量：NLP中‘偏差’的批判性调查”（[http://mng.bz/Yq0Q](http://mng.bz/Yq0Q)）。
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter covered four common approaches to diversity sampling: model-based
    outlier sampling, cluster-based sampling, representative sampling, and sampling
    for real-world diversity. These techniques can help you understand the kinds of
    “unknown unknowns” in your models.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章介绍了四种常见的多样性抽样方法：基于模型的异常值抽样、基于聚类的抽样、代表性抽样和针对现实世界多样性的抽样。这些技术可以帮助你了解模型中的“未知未知”类型。
- en: Model-based outlier sampling allows you to sample items that are unknown to
    your model in its current state, helping you expand your model’s knowledge where
    there are currently gaps.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的异常值抽样允许你抽取当前模型状态下未知的项目，帮助你扩展模型的知识，填补当前存在的知识空白。
- en: Cluster-based sampling allows you to sample items that are statistically representative
    of the entire distribution of your data, helping you expand your model’s knowledge
    to capture all the meaningful trends in your data, including the rarer ones that
    would likely be missed with random sampling.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于聚类的抽样允许你抽取那些在统计上能代表你数据整体分布的样本，帮助你扩展模型的知识，以捕捉数据中的所有有意义的趋势，包括那些随机抽样可能遗漏的较罕见趋势。
- en: Representative sampling can be used to sample items that are maximally representative
    of where you are going to deploy your model, helping you adapt your model to domains
    that are different from your current training data, which is a common problem
    in real-world machine learning.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表性抽样可以用来抽取那些最能代表你将要部署模型的地方的样本，帮助你将模型适应到与当前训练数据不同的领域，这在现实世界的机器学习中是一个常见问题。
- en: To support real-world diversity, you need to deploy all your techniques from
    uncertainty sampling and diversity sampling to make your applications more accurate
    across a diverse set of users and, therefore, more fair.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了支持现实世界的多样性，你需要部署所有从不确定性抽样和多样性抽样到的不确定性抽样技术，以使你的应用在多样化的用户群体中更加准确，因此更加公平。
- en: Accuracy metrics such as micro and macro F-score can be applied across real-world
    demographics as one way to measure potential biases in a model.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微分和宏分等准确度指标可以应用于现实世界的群体，作为衡量模型潜在偏差的一种方式。
- en: Interpreting the layers of a neural model for diversity sampling lets you access
    as much information as possible for active learning, giving you more options for
    calculating model outliers and providing a building block for advanced transfer
    learning techniques.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释用于多样性抽样的神经网络模型的层，让你能够尽可能多地获取信息以进行主动学习，为你计算模型异常值提供更多选项，并为高级迁移学习技术提供构建块。
- en: The strategies for deciding how many items should be reviewed by humans when
    implementing diversity sampling is different from uncertainty sampling, because
    in some cases, they can be adaptive within each iteration of active learning.
    Adaptive sampling methods allow you to make the human-in-the-loop machine learning
    feedback loop more efficient because you don’t have to wait for your model to
    retrain.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定在实施多样性抽样时应该由人类审查多少项目的方法与不确定性抽样不同，因为在某些情况下，它们可以在主动学习的每个迭代中自适应调整。自适应抽样方法允许你使人类在循环中的机器学习反馈循环更加高效，因为你不必等待模型重新训练。
- en: Implementing diversity sampling is possible with any supervised machine learning
    algorithm, including neural models, Bayesian models, SVMs, and decision trees.
    You can implement active learning with any type of machine learning algorithm
    that you are currently using; you don’t need to switch to the neural models that
    are the focus of the examples in this book. You might even decide to try some
    of these additional algorithms for active learning to take advantage of their
    unique properties.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用任何监督机器学习算法，包括神经网络模型、贝叶斯模型、SVM和决策树，都可以实现多样性抽样。你可以使用你目前正在使用的任何类型的机器学习算法来实现主动学习；你不需要切换到本书示例中重点关注的神经网络模型。你甚至可能决定尝试一些这些额外的主动学习算法，以利用它们的独特特性。
- en: '* * *'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)An alternative way to get hidden layers in PyTorch are the `hook()` methods.
    See the documentation at [http://mng.bz/XdzM](http://mng.bz/XdzM).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) 在 PyTorch 中获取隐藏层的一种替代方法是 `hook()` 方法。请参阅[http://mng.bz/XdzM](http://mng.bz/XdzM)上的文档。
