- en: 4 Scaling with the compute layer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 与计算层扩展
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Designing scalable infrastructure that allows data scientists to handle computationally
    demanding projects
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计可扩展的基础设施，使数据科学家能够处理计算密集型项目
- en: Choosing a cloud-based compute layer that matches your needs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择符合您需求的基于云的计算层
- en: Configuring and using compute layers in Metaflow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Metaflow中配置和使用计算层
- en: Developing robust workflows that handle failures gracefully
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发能够优雅处理失败的稳健工作流程
- en: What are the most fundamental building blocks of all data science projects?
    First, by definition, *data* science projects use *data*. At least small amounts
    of data are needed by all machine learning and data science projects. Second,
    the *science* part of data science implies that we don’t merely collect data but
    we use it for something, that is, we *compute* something using data. Correspondingly,
    *data* and *compute* are the two most foundational layers of our data science
    infrastructure stack, depicted in figure 4.1.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据科学项目的最基本构建块是什么？首先，根据定义，**数据**科学项目使用**数据**。至少，所有机器学习和数据科学项目都需要至少少量的数据。其次，数据科学中的**科学**部分意味着我们不仅收集数据，我们还用它来做些事情，即我们使用数据**计算**一些东西。相应地，**数据**和**计算**是我们数据科学基础设施堆栈的两个最基础层，如图4.1所示。
- en: '![CH04_F01_Tuulos](../../OEBPS/Images/CH04_F01_Tuulos.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F01_Tuulos](../../OEBPS/Images/CH04_F01_Tuulos.png)'
- en: Figure 4.1 Data science infrastructure stack with the compute layer highlighted
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1突出显示计算层的科学基础设施堆栈
- en: 'Managing and accessing data is such a deep and broad topic that we postpone
    an in-depth discussion about it until chapter 7\. In this chapter, we focus on
    the *compute* layer of the stack, which answers a seemingly simple question: After
    a data scientist has defined a piece of code, such as a step in a workflow, where
    should we execute it?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 管理和访问数据是一个如此深入和广泛的话题，我们将其深入讨论推迟到第7章。在本章中，我们专注于堆栈的**计算**层，它回答了一个看似简单的问题：数据科学家定义了一块代码，比如工作流程中的一个步骤，我们应该在哪里执行它？
- en: 'A straightforward answer, which we touched in chapter 2, is to execute the
    task on a laptop or on a cloud workstation. But what if the task is too demanding
    for the laptop—say it requires 64 GB of memory? Or, what if the workflow includes
    a foreach construct that launches 100 tasks? A single workstation doesn’t have
    enough CPU cores to run them in parallel, and running them sequentially may be
    inconveniently slow. This chapter proposes a solution: we can execute tasks outside
    a personal workstation on a cloud-based compute layer.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直接的答案，我们在第2章中提到过，是在笔记本电脑或云工作站上执行任务。但是，如果任务对笔记本电脑来说要求过高——比如说需要64GB的内存？或者，如果工作流程包括一个foreach结构，它启动了100个任务？单个工作站没有足够的CPU核心来并行运行它们，而顺序运行可能速度太慢。本章提出了一种解决方案：我们可以在个人工作站之外，在基于云的计算层上执行任务。
- en: You have many ways to implement a compute layer. The exact choice depends on
    your specific requirements and use cases. We will walk through a number of common
    choices and discuss how you can choose one that fits your needs. We will introduce
    an easy option, a managed cloud service called *AWS Batch*, to demonstrate the
    compute layer in action using Metaflow. Building on the foundation laid out in
    this chapter, the next chapter will provide more hands-on examples.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您有多种方式来实现计算层。确切的选择取决于您的具体需求和用例。我们将介绍一些常见的选择，并讨论您如何选择一个适合您需求的选择。我们将介绍一个简单选项，一个名为*AWS
    Batch*的托管云服务，以使用Metaflow演示计算层的实际应用。在本章奠定基础的基础上，下一章将提供更多动手示例。
- en: Fundamentally, we care about the compute layer because it allows projects to
    handle more computation and more data. In other words, the compute layer allows
    projects to be more *scalable*. Before we dive deep into the technical details
    of the compute layer, we start by exploring what scalability, and its sibling
    concept *performance*, mean, and why and when they matter for data science projects.
    As you will learn, the topic of scalability is a surprisingly nuanced one. Understanding
    it better will help you make the right technical choices for your projects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们关注计算层，因为它允许项目处理更多的计算和数据。换句话说，计算层允许项目更具**可扩展性**。在我们深入探讨计算层的详细技术细节之前，我们首先来探讨可扩展性及其兄弟概念**性能**的含义，以及为什么和何时它们对数据科学项目很重要。正如您将学到的，可扩展性的主题是一个令人惊讶的微妙话题。更好地理解它将帮助您为项目做出正确的技术选择。
- en: 'From the infrastructure point of view, our aspirational goal is to allow data
    scientists to work on any business problem effectively and efficiently without
    being constrained by the size of the problem. Many data scientists feel that being
    able to harness large amounts of data and compute resources gives them superpowers.
    The feeling is justified: using the techniques introduced in this chapter, a data
    scientist can harness computing power that would have required a supercomputer
    a few decades ago with just tens of lines of Python.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础设施的角度来看，我们的理想目标是让数据科学家能够有效地解决任何业务问题，而不会因为问题规模的大小而受到限制。许多数据科学家认为，能够利用大量数据和计算资源赋予他们超能力。这种感觉是合理的：使用本章介绍的技术，数据科学家可以用仅仅几十行Python代码来利用几十年前需要超级计算机才能提供的计算能力。
- en: A downside of any compute layer is that tasks can fail in surprising ways compared
    to the tight confines of a laptop. To boost the productivity of data scientists,
    we want to handle as many errors automatically as possible. When unrecoverable
    errors happen, we want to make the debugging experience as painless as possible.
    We will discuss this toward the end of the chapter. You can find all code listings
    for this chapter at [http://mng.bz/d2lN](http://mng.bz/d2lN).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 任何计算层的缺点是，与笔记本电脑紧密的封闭环境相比，任务可能会以令人惊讶的方式失败。为了提高数据科学家的生产力，我们希望尽可能自动处理尽可能多的错误。当发生无法恢复的错误时，我们希望使调试体验尽可能痛苦。我们将在本章的末尾讨论这个问题。你可以在这个章节找到所有代码列表：[http://mng.bz/d2lN](http://mng.bz/d2lN)。
- en: 4.1 What is scalability?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 什么是可扩展性？
- en: '*Alex feels a great sense of pride for having built the first application that
    trains a model and produces predictions. But Bowie is concerned: as their cupcake
    business grows, hopefully exponentially, can Alex’s Python scripts handle the
    scale of data that they might be facing in the future? Alex is no scalability
    expert. Although Bowie’s concerns are understandable, Alex feels that they might
    be a bit premature. Their business is nowhere near such scale yet. In any case,
    if Alex could choose, the easiest solution would be to get a big enough laptop
    that can run the existing scripts with bigger data. Instead of spending time reengineering
    the existing scripts, Alex would rather focus on perfecting the models and understanding
    the data better.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*亚历克斯为能够构建第一个训练模型并产生预测的应用感到无比自豪。但鲍伊却有所担忧：随着他们的蛋糕生意有望指数级增长，亚历克斯的Python脚本能否处理他们未来可能面临的数据规模？亚历克斯并非可扩展性专家。尽管鲍伊的担忧是可以理解的，但亚历克斯认为他们可能有些过于急切。他们的业务规模还远未达到那种程度。无论如何，如果亚历克斯可以选择，最简单的解决方案就是购买一台足够大的笔记本电脑，以便运行现有的脚本处理更大的数据。亚历克斯宁愿专注于完善模型和更好地理解数据，而不愿花费时间重构现有的脚本。*'
- en: '![CH04_F01_UN01_Tuulos](../../OEBPS/Images/CH04_F01_UN01_Tuulos.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F01_UN01_Tuulos](../../OEBPS/Images/CH04_F01_UN01_Tuulos.png)'
- en: 'In this scenario, whose concerns are more valid? Bowie has a valid concern
    from the engineering point of view: the Python scripts running on Alex’s laptop
    won’t be able to handle data of arbitrary size. Alex’s concern is valid, too:
    the scripts may be adequate for their situation today and possibly for the near
    future. In the business point of view, it might make more sense to focus on the
    quality of results rather than the scale.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，谁的关注更有道理？鲍伊从工程角度来看有合理的担忧：运行在亚历克斯笔记本电脑上的Python脚本将无法处理任意大小的数据。亚历克斯的担忧也是合理的：脚本可能适合他们目前的状况，也可能适合近未来的情况。从商业角度来看，可能更合理的是关注结果的质量，而不是规模。
- en: Also, although Alex’s dream of handling scalability just by getting a bigger
    laptop may sound silly and unrealistic from a technical point of view, it is a
    reasonable idea from a productivity point of view. Theoretically, an infinitely
    large laptop would make it possible to use existing code without changes, allowing
    Alex to focus on data science instead of the intricacies of distributed computing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然亚历克斯仅通过购买一台更大的笔记本电脑来处理可扩展性的梦想在技术角度来看可能听起来很愚蠢且不切实际，但从生产力的角度来看，这是一个合理的想法。从理论上讲，一台无限大的笔记本电脑将使得无需更改现有代码即可使用，从而让亚历克斯能够专注于数据科学，而不是分布式计算的复杂性。
- en: If you were Harper, a business leader, would you side with Bowie and suggest
    that Alex reengineer the code to make it scalable, diverting Alex’s attention
    from models, or let Alex focus on improving the models, which may lead to failures
    in the future? It is not an easy call. Many people would say “it depends.” A wise
    leader might prefer a balanced approach that makes the code just scalable enough
    so that it won’t collapse under realistic loads in the near future and let Alex
    spend the remaining time on ensuring the quality of the results.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是哈珀，一位商业领导者，你会站在鲍伊一边，建议亚历克斯重新设计代码以使其可扩展，从而分散亚历克斯对模型的注意力，还是会让亚历克斯专注于改进模型，这可能会导致未来的失败？这并不是一个容易的决定。许多人会说“这取决于。”一个明智的领导者可能会选择一种平衡的方法，使代码仅具有足够可扩展性，以便在不久的将来不会在现实负载下崩溃，并让亚历克斯花剩余的时间确保结果的质量。
- en: Finding such a balanced approach is the main theme of this section. We want
    to optimize the productivity of data scientists, business needs, and engineering
    concerns simultaneously, emphasizing the dimensions that are pertinent for each
    use case. We want to provide generalized infrastructure that allows each application
    to be pragmatic, not dogmatic about scalability, striking a balance that fits
    their specific needs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 找到这样的平衡方法是本节的主要主题。我们希望同时优化数据科学家、业务需求和工程关注点的生产力，强调每个用例相关的维度。我们希望提供通用的基础设施，允许每个应用程序具有实用性，而不是对可扩展性过于教条，找到一个适合他们特定需求的平衡点。
- en: 4.1.1 Scalability across the stack
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 堆栈层面的可扩展性
- en: 'If you have worked on a data science project that involves demanding training
    or data processing steps, it is likely that you have heard or thought questions
    like “Does it scale?” or “Is it fast enough?” In casual discussions, the terms
    *scalability* and *performance* are used interchangeably, but they are in fact
    independent concerns. Let’s start with the definition of *scalability*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你参与过一个涉及要求严格的训练或数据处理步骤的数据科学项目，你很可能会听到或思考过类似“它是否可扩展？”或“它是否足够快？”的问题。在非正式讨论中，术语*可扩展性*和*性能*被互换使用，但实际上它们是独立的问题。让我们从*可扩展性*的定义开始：
- en: Scalability is the property of a system to handle a growing amount of work by
    adding resources to the system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性是指系统通过向系统中添加资源来处理日益增长的工作量的属性。
- en: 'Let’s unpack the definition as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们如下解释这个定义：
- en: Scalability is about *growth*. It doesn’t make sense to talk about the scalability
    of a static system with static inputs. However, you can talk about the *performance*
    of such a system—more about that soon.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可扩展性关乎*增长*。对于具有静态输入的静态系统，讨论其可扩展性是没有意义的。然而，你可以讨论这样一个系统的*性能*——关于这一点我们很快就会讨论。
- en: Scalability implies that the system has to perform *more work*, for example,
    handle more data or train a larger number of models. Scalability is not about
    optimizing the performance for a fixed amount of work.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可扩展性意味着系统必须执行*更多的工作*，例如，处理更多的数据或训练更多的模型。可扩展性不是关于优化固定工作量性能的。
- en: Scalable systems leverage *additional resources* added to the system efficiently.
    If the system is able to handle more work by adding some resources, such as more
    computers or more memory, the system is scalable.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可扩展的系统有效地利用添加到系统中的*额外资源*。如果一个系统能够通过添加一些资源（如更多的计算机或更多的内存）来处理更多的工作，那么这个系统就是可扩展的。
- en: 'Whereas scalability is about growth, *performance* is about the system’s capabilities,
    independent of growth. For instance, we can measure your performance in making
    an omelet: how quickly can you make one, what’s the quality of the result, or
    how much waste is produced as a side effect? There isn’t a single measure of performance
    or scalability; you have to define the dimension you are interested in.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 而可扩展性关乎增长，*性能*则关乎系统的能力，独立于增长。例如，我们可以衡量你制作煎蛋卷的性能：你能多快地制作一个，结果的质量如何，或者作为副作用产生了多少浪费？没有单一的衡量性能或可扩展性的标准；你必须定义你感兴趣的维度。
- en: If you are building a single application, you can focus on making that particular
    application scalable. When building infrastructure, you need to consider not only
    how individual applications can be made scalable, but also how the whole infrastructure
    scales when the number of distinct applications grows. Also, the infrastructure
    needs to support a growing number of engineers and data scientists who build the
    applications.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建单个应用，你可以专注于使该特定应用可扩展。当构建基础设施时，你需要考虑不仅如何使单个应用可扩展，而且当不同应用的数量增加时，整个基础设施如何扩展。此外，基础设施还需要支持越来越多的工程师和数据科学家，他们构建这些应用。
- en: 'Hence, when building effective infrastructure, we are not only concerned about
    the scalability of a particular algorithm or a workflow. Rather, we want to optimize
    scalability across all layers of the infrastructure stack. Remember the following
    four Vs we introduced in chapter 1:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当构建有效的基础设施时，我们不仅关注特定算法或工作流的可扩展性。相反，我们希望优化基础设施堆栈所有层的可扩展性。记住我们在第一章中引入的以下四个Vs：
- en: '*Volume*—We want to support *a large number* of data science applications.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*规模*—我们希望支持*大量*的数据科学应用。'
- en: '*Velocity*—We want to make it easy and *quick* to prototype and productionize
    data science applications.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*速度*—我们希望使原型设计和生产化数据科学应用变得容易且*快速*。'
- en: '*Validity*—We want to make sure that the results are valid and consistent.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*有效性*—我们希望确保结果有效且一致。'
- en: '*Variety*—We want to support *many different kinds* of data science models
    and applications.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*多样性*—我们希望支持*多种不同类型*的数据科学模型和应用。'
- en: Almost all the Vs are related to scalability or performance. *Volume* is concerned
    with a growing number of applications, which is the motivation for having generalized
    infrastructure in the first place. *Velocity* is concerned about speed—speed of
    code, projects, and people, that is, *performance*. Contrasting *validity* against
    scalability is such an important topic that it deserves a discussion of its own
    in the next chapter. Finally, *variety* refers to our ability to work on an increasingly
    diverse set of use cases, using a diverse set of tools. We shouldn’t assume there’s
    a silver bullet solution to scalability, despite what ads may try to tell you.
    All in all, scalability in its different forms is a fundamental thread that runs
    throughout this book. Figure 4.2 shows how scalability touches all layers of the
    data science infrastructure stack.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的Vs都与可扩展性或性能相关。*规模*关注的是应用数量的增长，这是最初拥有通用基础设施的动机。*速度*关注的是速度—代码、项目和人的速度，即*性能*。将*有效性*与可扩展性进行对比是一个如此重要的主题，以至于它值得在下一章中单独讨论。最后，*多样性*指的是我们能够使用各种工具处理日益多样化的用例的能力。尽管广告可能试图告诉你，但我们不应该假设存在一个银弹解决方案来解决可扩展性问题。总的来说，不同形式的可扩展性是贯穿本书的基本线索。图4.2显示了可扩展性如何触及数据科学基础设施堆栈的所有层。
- en: '![CH04_F02_Tuulos](../../OEBPS/Images/CH04_F02_Tuulos.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F02_Tuulos](../../OEBPS/Images/CH04_F02_Tuulos.png)'
- en: Figure 4.2 Types of scalability across the infrastructure stack
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 基础设施堆栈中的可扩展性类型
- en: 'Let’s unpack the figure. On the leftmost column, we have the building blocks
    of a data science application. They form a hierarchy: an algorithm is contained
    by a task, which is contained by a workflow, and so on. This hierarchy expands
    the one we covered in the context of Metaflow in figure 3.18.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个图。在最左侧的列中，我们有数据科学应用的基本构建块。它们形成一个层次结构：一个算法包含在一个任务中，一个任务包含在一个工作流中，依此类推。这个层次结构扩展了我们在图3.18中讨论Metaflow时覆盖的层次结构。
- en: 'Each of these building blocks can scale independently. Following our definition,
    scalability involves two factors: more work and more resources. The More work
    column shows the type of work that the corresponding building block needs to handle,
    that is, its major dimension of scalability. The More resources column shows resources
    we can add to the building block to make it scale. The Infrastructure layers column
    shows the piece of infrastructure that manages the resources. By design, the layers
    cooperate, so there’s some overlap between their responsibilities. Let’s go through
    the blocks one by one as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些构建块都可以独立扩展。根据我们的定义，可扩展性涉及两个因素：更多工作和更多资源。更多工作这一列显示了相应的构建块需要处理的工作类型，即其可扩展性的主要维度。更多资源这一列显示了我们可以添加到构建块中的资源。基础设施层这一列显示了管理资源的部分基础设施。按照设计，这些层是协作的，因此它们的责任之间有一些重叠。让我们逐一介绍以下块：
- en: At the core of an application, there’s typically *an algorithm* that performs
    numerical optimization, trains a model, and so forth. Often, the algorithm is
    provided by an off-the-shelf library like TensorFlow or Scikit-Learn. Usually,
    the algorithm needs to scale when it must handle a larger amount of data, but
    other dimensions of scalability exist as well, such as the complexity of the model.
    Modern algorithms can use all available resources, CPU cores, GPUs, and RAM on
    the compute instance effectively, so you can scale them by increasing the capacity
    of the instance.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用程序的核心，通常有一个执行数值优化、训练模型等操作的*算法*。通常，算法由现成的库如TensorFlow或Scikit-Learn提供。通常，当算法必须处理更多数据时，它需要扩展，但还存在其他可扩展性维度，例如模型的复杂性。现代算法可以有效地使用计算实例上的所有可用资源，包括CPU核心、GPU和RAM，因此你可以通过增加实例容量来扩展它们。
- en: The algorithm doesn’t run itself. It needs to be called by the user code, such
    as a Metaflow task. The task is an operating system-level process. To make it
    scale, you can use various tools and techniques (more about them in section 4.2)
    to employ all available CPU cores and RAM on the instance. Often, when using highly
    optimized algorithms, you can outsource all scalability concerns to the algorithm,
    and the task can stay relatively simple, like we did with Scikit-Learn in section
    3.3.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法不会自行运行。它需要被用户代码调用，例如Metaflow任务。任务是一个操作系统级别的进程。为了使其可扩展，你可以使用各种工具和技术（更多内容在第4.2节中介绍）来利用实例上的所有CPU核心和RAM。通常，当使用高度优化的算法时，你可以将所有可扩展性关注点外包给算法，而任务可以保持相对简单，就像我们在第3.3节中与Scikit-Learn所做的那样。
- en: A data science application or workflow consists of multiple tasks. In fact,
    a workflow can spawn an arbitrary number of tasks when leveraging data parallelism,
    such as dynamic branches, which we covered in section 3.2.3\. To handle a large
    number of concurrent tasks, the workflow can fan out the work to multiple compute
    instances. We will practice these topics later in this chapter and in the next
    chapter.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学应用程序或工作流程由多个任务组成。实际上，当利用数据并行性时，工作流程可以产生任意数量的任务，例如动态分支，这在第3.2.3节中已介绍。为了处理大量并发任务，工作流程可以将工作分散到多个计算实例上。我们将在本章和下一章中练习这些主题。
- en: To encourage experimentation, we should allow data scientists to test multiple
    different *versions* of their workflows, maybe with slightly different variations
    in data or the model architecture. To save time, it is convenient to be able to
    test the versions in parallel. To handle multiple parallel workflow executions,
    we need a scalable workflow orchestrator (see chapter 6) as well as many compute
    instances.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了鼓励实验，我们应该允许数据科学家测试他们工作流程的多个不同*版本*，可能是在数据或模型架构上略有不同。为了节省时间，能够并行测试这些版本是很方便的。为了处理多个并行工作流程执行，我们需要一个可扩展的工作流程编排器（见第6章）以及许多计算实例。
- en: It is common for a data science organization to work on a wide variety of data
    science *projects* concurrently. Each project has its own business goals, represented
    by bespoke workflows and versions. It is important that we minimize interference
    among projects. For each project, we should be able to choose the architecture,
    algorithms, and scalability requirements independently. Chapter 6 will shed more
    light on the questions of versioning, namespaces, and dependency management.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学组织通常同时进行多种数据科学*项目*。每个项目都有自己的业务目标，由定制的流程和版本表示。我们应尽量减少项目之间的干扰。对于每个项目，我们应能够独立选择架构、算法和可扩展性要求。第6章将更详细地探讨版本控制、命名空间和依赖关系管理的问题。
- en: It is desirable for the *organization* to be able to scale the number of concurrent
    projects by hiring more people. Importantly, a well-designed infrastructure can
    help with this scalability challenge as well, as discussed later.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于组织来说，能够通过雇佣更多的人来扩展并行项目的数量是很有吸引力的。重要的是，一个精心设计的基础设施也可以帮助解决这一可扩展性挑战，如后文所述。
- en: 'To summarize, data science projects operate on two resources: people and compute.
    It is the job of the data science infrastructure to match the two effectively.
    Scalability is not only about making individual workflows finish faster through
    more compute resources, but also about enabling more people to work on more versions
    and more projects, that is, enabling the culture of experimentation and innovation.
    Because this important aspect is often ignored in technical discussions, we spend
    a few pages on this topic before diving into technical details.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据科学项目依赖于两种资源：人和计算。数据科学基础设施的工作是有效地匹配这两者。可扩展性不仅关乎通过更多的计算资源使单个工作流程更快完成，还关乎使更多的人能够参与更多版本和更多项目的工作，即促进实验和创新文化。由于这个重要方面在技术讨论中经常被忽视，我们在深入研究技术细节之前，花了几页篇幅来讨论这个话题。
- en: 4.1.2 Culture of experimentation
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 实验室文化
- en: A modern, effective data science organization encourages data scientists to
    innovate and experiment with new approaches and alternative implementations relatively
    freely, without being constrained by the technical limitations of the compute
    layer. This sounds good on paper, but why isn’t it a reality at most organizations
    today?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现代、有效的数据科学组织鼓励数据科学家相对自由地创新和实验新的方法和替代实现，而不受计算层技术限制的约束。这听起来很好，但为什么在大多数组织中今天这不是现实呢？
- en: 'The price of compute cycles has gone down drastically over time, partly thanks
    to the cloud, whereas the price of talented people has gone up. An effective infrastructure
    can arbitrage this imbalance: we can provide expensive people with easy access
    to cheap compute resources to maximize their productivity. We want to enable access
    in a manner that allows the organization itself to scale.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，计算周期的价格大幅下降，部分得益于云计算，而人才的价格却上涨。有效的基础设施可以平衡这种不平衡：我们可以为昂贵的人才提供轻松访问廉价计算资源，以最大化他们的生产力。我们希望以允许组织本身扩展的方式实现访问。
- en: A fundamental reason scaling organizations is hard is the communication overhead.
    For a group of N people to communicate with one another, they need N ² lines of
    communication. In other words, the communication overhead grows quadratically
    with the number of people. A classic solution is a hierarchical organization,
    which restricts the information flow to avoid quadratic growth. However, many
    modern, innovative data science organizations would rather avoid a strict hierarchy
    and information bottlenecks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 组织规模扩大的一个基本原因是通信开销。对于一组N个人相互沟通，他们需要N²条通信线路。换句话说，通信开销随着人数的增长呈二次方增长。一个经典的解决方案是层级组织，它限制了信息流以避免二次方增长。然而，许多现代、创新的数据科学组织宁愿避免严格的层级和信息瓶颈。
- en: Why do people need to communicate in the first place? Coordination and knowledge
    sharing are common reasons. Historically, in many environments, accessing shared
    resources, like computers, has required quite a bit of both. This situation is
    illustrated in figure 4.3.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 人们为什么需要沟通呢？协调和知识共享是常见的原因。历史上，在许多环境中，访问共享资源，如计算机，需要相当多的协调和知识共享。这种情况在图4.3中得到了说明。
- en: '![CH04_F03_Tuulos](../../OEBPS/Images/CH04_F03_Tuulos.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03_Tuulos](../../OEBPS/Images/CH04_F03_Tuulos.png)'
- en: Figure 4.3 Coordinating access to shared compute resources
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 协调对共享计算资源的访问
- en: Imagine you worked at a college computer lab in the 1960s. The lab might have
    a single computer, a large *mainframe*. Because the compute power and storage
    was extremely limited, you probably needed to coordinate with all your colleagues
    who got to use the computer. The communication overhead would be quadratic relative
    to the number of colleagues. In an environment like this, to maintain your sanity,
    you would actively try to limit the number of people who could access the computer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在20世纪60年代的一所大学计算机实验室工作。实验室可能只有一台计算机，一台大型*主机*。由于计算能力和存储空间极其有限，你可能需要与所有能够使用计算机的同事协调。相对于同事的数量，通信开销将是二次方的。在这样的环境中，为了保持你的理智，你会积极尝试限制能够访问计算机的人数。
- en: Let’s say you worked at a midsize company in the early 2000s. The company has
    its own datacenter, where it can provision compute resources like *fixed-size
    clusters* to different teams based on business requirements and guidance from
    the capacity-planning team. This model is clearly more scalable than the mainframe
    model, because each individual team can coordinate access to their dedicated resources
    among themselves. A downside is that the model is quite rigid—provisioning more
    resources as a team’s needs grow could take weeks, if not months.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在2000年代初在一家中型公司工作。该公司有自己的数据中心，可以根据业务需求和容量规划团队的建议，为不同的团队提供如*固定大小的集群*等计算资源。这种模式显然比主机模型更具可扩展性，因为每个团队可以自行协调对专用资源的访问。缺点是这种模式相当僵化——随着团队需求的增长而增加更多资源可能需要数周甚至数月。
- en: Today, the *cloud* provides a semblance of unlimited compute capacity. The compute
    resources are not scarce anymore. Because the paradigm shift has happened quite
    quickly, it is understandable that many organizations still treat the cloud as
    if it was a fixed-size cluster or a mainframe. However, the cloud allows us to
    change that mindset and get rid of the coordination overhead altogether.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，*云*提供了看似无限的计算能力。计算资源不再稀缺。由于范式转变发生得相当快，许多组织仍然将云视为一个固定大小的集群或主机是可以理解的。然而，云允许我们改变这种心态，并完全消除协调开销。
- en: Instead of humans carefully coordinating access to a shared, scarce resource
    among themselves, we can rely on the infrastructure to facilitate relatively open
    access to the cornucopia of compute resources. A cloud-based compute layer, which
    we discuss in the next section, makes it easy to execute nearly arbitrary amounts
    of compute in a cost-effective manner. It allows data scientists to experiment
    freely and handle large datasets without having to constantly worry about resource
    overconsumption or interfering with their colleagues work, which is a huge boon
    for productivity.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类仔细协调彼此之间对共享、稀缺资源的访问相比，我们可以依赖基础设施来促进对计算资源宝库的相对开放访问。我们将在下一节讨论的基于云的计算层，使得以成本效益的方式执行几乎任意数量的计算变得容易。它允许数据科学家自由实验，处理大数据集，而无需不断担心资源过度消耗或干扰同事的工作，这对生产力是一个巨大的福音。
- en: As a result, organizations can handle more projects, teams can experiment more
    effectively, and individual scientists can work on much larger-scale problems.
    Besides the quantitative change of more scale, the cloud enables a qualitative
    change as well—we can double-down on the idea of *data scientist autonomy*, as
    described in section 1.3\. Instead of having to coordinate work with machine learning
    engineers, data engineers, and DevOps engineers, an individual data scientist
    can drive the prototyping loop and interaction with production deployments all
    by themselves.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，组织可以处理更多的项目，团队可以更有效地进行实验，个人科学家可以从事更大规模的问题。除了更多规模的量化变化之外，云还带来了质的飞跃——我们可以加倍重视*数据科学家自主性*这一理念，正如第1.3节所述。不再需要与机器学习工程师、数据工程师和DevOps工程师协调工作，单个数据科学家可以独自驱动原型循环和与生产部署的交互。
- en: Minimize interference to maximize scalability
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化干扰以最大化可扩展性
- en: Why has it been so critical to control and coordinate access to compute resources
    previously? One reason is scarcity. If there are more tasks, workflows, versions,
    or projects than what the system has capacity to handle, some control is needed.
    Today, in most scenarios, the cloud provides enough capacity that this argument
    becomes moot.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么过去控制和管理计算资源访问如此关键？一个原因是稀缺。如果任务、工作流程、版本或项目多于系统能够处理的，就需要一些控制。今天，在大多数情况下，云提供了足够的容量，使得这个论点变得无关紧要。
- en: Another argument is *fragility*. If an inattentive user can break the system,
    having a layer of supervision seems like a good idea. Or maybe the system is designed
    in a way that workloads can easily interfere with each other. This is a valid
    argument for many systems that are still actively used today. For instance, a
    single bad query can impact all users of a shared database.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个论点是*脆弱性*。如果一个粗心的用户可以破坏系统，那么有一层监督似乎是个好主意。或者，也许系统设计得使得工作负载可以轻易相互干扰。这对于许多今天仍在积极使用的系统来说是一个有效的论点。例如，一个错误的查询可能会影响共享数据库的所有用户。
- en: If we want to maximize the scalability of organizations, we want to minimize
    the communication and coordination overhead between people. Optimally, we want
    to remove the need for coordination—particularly if coordination is required to
    avoid breakage. As an infrastructure provider, our goal, at least aspirationally,
    is to make sure that no workload can break the system or cause adverse side effects
    for its neighbors.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要最大化组织的可扩展性，我们希望最小化人与人之间的通信和协调开销。理想情况下，我们希望消除协调的需求——尤其是当协调是为了避免故障时。作为一个基础设施提供商，我们的目标，至少是理想化的目标，是确保没有任何工作负载能够破坏系统或对其邻居产生不良影响。
- en: Because data science workloads tend to be experimental in nature, we can’t expect
    that the workloads themselves are particularly well behaving. Instead, we must
    make sure that they are properly *isolated*, so even if they fail hard, they have
    a limited *blast radius*, that is, they cause a minimal amount of collateral damage
    by interfering with other workloads.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据科学工作负载往往具有实验性质，我们无法期望工作负载本身表现得特别良好。相反，我们必须确保它们得到适当的*隔离*，即使它们出现严重故障，它们的*破坏半径*也有限，也就是说，它们通过干扰其他工作负载造成的附带损害最小。
- en: Scalability tip Minimizing interference among workloads through isolation is
    an excellent way to reduce the need for coordination. The less coordination is
    needed, the more scalable the system becomes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性技巧 通过隔离最小化工作负载之间的干扰是减少协调需求的一种极好方式。需要的协调越少，系统就越可扩展。
- en: Luckily, modern cloud infrastructure, container management systems in particular,
    help us achieve this relatively easily as we will learn in the next section. Other
    key elements of isolation are versioning, namespaces, and dependency management,
    which we will cover in more detail in chapter 6.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，现代云基础设施，特别是容器管理系统，帮助我们相对容易地实现这一点，正如我们将在下一节中学习的那样。隔离的其他关键元素包括版本控制、命名空间和依赖管理，这些内容我们将在第6章中更详细地介绍。
- en: 4.2 The compute layer
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 计算层
- en: '*Instead of a giant laptop, it would be desirable to have a setup that allows
    Alex to throw any number of tasks, large and small, to a cloud-based compute environment
    that would automatically scale to handle the tasks. Alex wouldn’t have to change
    the code or care about any other details besides collecting the results. Optimally,
    the environment would be such that Bowie would have to spend only little time
    maintaining it.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*与其拥有一个巨大的笔记本电脑，不如有一个设置，让Alex能够将任何数量的大小任务抛向基于云的计算环境，该环境会自动扩展以处理任务。Alex不需要更改代码或关心任何其他细节，除了收集结果。理想情况下，环境应该是Bowie只需花费很少的时间来维护。*'
- en: '![CH04_F03_UN02_Tuulos](../../OEBPS/Images/CH04_F03_UN02_Tuulos.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03_UN02_Tuulos](../../OEBPS/Images/CH04_F03_UN02_Tuulos.png)'
- en: Let’s start with the big picture. Figure 4.4 shows how the layers of the infrastructure
    stack participate in workflow execution. We use workflows as the user interface
    to define tasks that need to be executed, but the compute layer doesn’t have to
    care about the workflow per se—it cares only about individual tasks. We will use
    a workflow orchestrator, the job scheduler layer in our stack, to determine *how*
    to schedule individual tasks and *when* the workflow should be executed. More
    about this in chapter 6.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从大局开始。图4.4展示了基础设施堆栈的各个层如何参与工作流程执行。我们使用工作流程作为用户界面来定义需要执行的任务，但计算层不必关心工作流程本身——它只关心单个任务。我们将使用工作流程编排器，即我们堆栈中的作业调度层，来确定如何调度单个任务以及何时执行工作流程。关于这一点，我们将在第6章中详细介绍。
- en: '![CH04_F04_Tuulos](../../OEBPS/Images/CH04_F04_Tuulos.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F04_Tuulos](../../OEBPS/Images/CH04_F04_Tuulos.png)'
- en: 'Figure 4.4 The role of the computer layer: Where tasks are executed'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 计算层的作用：任务执行的位置
- en: Also, the compute layer doesn’t need to care *what* is being computed and *why*—the
    data scientist answers these questions as they architect the application. This
    corresponds to the architecture layer in our infrastructure stack. The compute
    layer needs to decide only *where* to execute a task, in other words, *finding
    a big enough computer that can execute a task*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，计算层不需要关心正在计算的内容以及为什么需要计算——数据科学家在架构应用时会回答这些问题。这对应于我们基础设施堆栈中的架构层。计算层只需要决定在哪里执行一个任务，换句话说，就是找到足够大的计算机来执行任务。
- en: 'To do its job, the compute layers needs to provide a simple interface: it accepts
    a task together with resource requirements (how many CPUs or how much RAM the
    task requires), executes it (maybe after a delay), and allows the requester to
    query the status of the work performed. Although doing this may seem straightforward,
    building a robust compute layer is a highly nontrivial engineering challenge.
    Consider the following requirements:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成其工作，计算层需要提供一个简单的接口：它接受一个任务以及资源需求（任务需要多少CPU或多少RAM），执行它（可能经过延迟后），并允许请求者查询所执行工作的状态。尽管这样做可能看起来很简单，但构建一个健壮的计算层是一个高度非平凡的工程挑战。考虑以下要求：
- en: The system needs to handle a large number of concurrent tasks, potentially hundreds
    of thousands or millions of them.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统需要处理大量并发任务，可能多达数十万或数百万。
- en: The system needs to manage a pool of physical computers that are used to execute
    tasks. Preferably, physical computers can be added and removed from the pool on
    the fly without causing any downtime.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统需要管理一个用于执行任务的物理计算机池。理想情况下，物理计算机可以随时添加到或从池中移除，而不会造成任何停机时间。
- en: Tasks have different resource requirements. The system needs to match each task
    to a computer that has at least the required amount of resources available. Doing
    the matching, or *packing*, efficiently at scale is a notoriously hard problem.
    If you are a theory geek, you can search the phrases Bin Packing problem and Knapsack
    problem to learn more about the computational complexity of task placement.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务有不同的资源需求。系统需要将每个任务匹配到至少有所需资源可用的一台计算机。在规模上高效地进行匹配，或称为“打包”，是一个众所周知的问题。如果你是理论爱好者，你可以搜索“Bin
    Packing problem”和“Knapsack problem”来了解任务放置的计算复杂度。
- en: The system must anticipate that any computer can fail, data centers can catch
    fire, any task may behave badly or even maliciously, and software has bugs. Regardless,
    the system shouldn’t go down under any circumstances.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统必须预料到任何计算机都可能发生故障，数据中心可能会起火，任何任务可能表现不佳或甚至恶意行为，软件存在缺陷。无论如何，系统在任何情况下都不应该崩溃。
- en: For many decades, building large-scale systems that fulfill these requirements
    was the realm of *high-performance computing* (HPC). The industry was dominated
    by specialized vendors delivering expensive systems to governments, research institutions,
    and large companies. Smaller companies and institutions relied on various home-grown
    solutions that were often brittle and expensive to maintain, at least in terms
    of person hours.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 几十个世纪以来，构建满足这些要求的大型系统一直是高性能计算（HPC）的领域。该行业由专门向政府、研究机构和大型公司提供昂贵系统的供应商主导。较小的公司和机构依赖各种自制的解决方案，这些解决方案通常脆弱且维护成本高昂，至少在人力成本方面是这样。
- en: The advent of public clouds like AWS has changed the landscape drastically.
    Today, with a few clicks, you can provision a compute layer that operates robustly
    at the scale of relatively recent supercomputers. Of course, most of us don’t
    need a supercomputer-scale compute layer. The cloud allows us to start small—starting
    from the capacity of a small laptop—and keep scaling resources elastically as
    the needs grow. The best part is that you pay only for what you use, which means
    that a small compute layer used occasionally can be more affordable than a physical
    laptop of the same size.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 公共云如AWS的出现极大地改变了这一领域。如今，只需几点击，你就可以部署一个在相对较新的超级计算机规模上稳健运行的计算层。当然，我们大多数人并不需要超级计算机规模的计算层。云允许我们从一个小型笔记本电脑的容量开始，随着需求的增长，弹性地扩展资源。最好的部分是，你只需为使用的部分付费，这意味着偶尔使用的小型计算层可能比同等大小的物理笔记本电脑更经济。
- en: The public cloud mostly frees us from having to deal with the previous requirements
    by ourselves. However, the level of abstraction they provide—answering the question
    of *where* to execute tasks and executing them for us—is still pretty low-level.
    To make the compute layer usable and useful for data science workloads, one needs
    to make a number of architectural choices on top of the interfaces provided by
    the cloud.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 公共云在很大程度上使我们免于自己处理之前的要求。然而，它们提供的抽象级别——回答“在哪里”执行任务并为我们执行任务的问题——仍然相当低级。为了使计算层对数据科学工作负载有用，需要在云提供的接口之上做出许多架构选择。
- en: Different systems make different engineering tradeoffs. Some optimize for latency—that
    is, how fast tasks start—some for the types of computers available, some for the
    maximum scale, some for high availability, and some for cost. As a result, it
    is not feasible to think that there is or will be a single universal compute layer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的系统做出不同的工程权衡。一些优化延迟——即任务启动的速度——一些优化可用的计算机类型，一些优化最大规模，一些优化高可用性，还有一些优化成本。因此，认为存在或将来会有一个单一的通用计算层是不切实际的。
- en: Also, different workflows and applications have different compute requirements,
    so it is beneficial for the data science infrastructure to support a selection
    of compute layers, ranging from local laptops and cloud workstations to special-purpose
    clusters of GPUs or other hardware accelerators for ML and AI. Fortunately, we
    can abstract a good amount of this inevitable diversity away. The different flavors
    of compute layers can adhere to a common interface and architecture, as discussed
    in the next section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不同的工作流和应用有不同的计算需求，因此对于数据科学基础设施来说，支持从本地笔记本电脑和云工作站到专门用于机器学习和人工智能的 GPU 或其他硬件加速器的集群的计算层选择是有益的。幸运的是，我们可以抽象出很大一部分这种不可避免的多样性。不同的计算层可以遵循下一节中讨论的通用接口和架构。
- en: 4.2.1 Batch processing with containers
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 使用容器进行批处理
- en: A system that processes tasks that start, take input data, perform some processing,
    produce output, and terminate is said to perform *batch processing*. Fundamentally,
    the compute layer we describe here is a batch-processing system. In our workflow
    paradigm, illustrated in figure 4.5, a step in the workflow defines one or more
    tasks that are executed as batch jobs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个处理开始、接收输入数据、执行一些处理、生成输出并终止的任务的系统被称为执行 *批处理*。从根本上说，我们在这里描述的计算层是一个批处理系统。在我们的工作流范式（如图
    4.5 所示）中，工作流中的一个步骤定义了一个或多个作为批处理作业执行的任务。
- en: '![CH04_F05_Tuulos](../../OEBPS/Images/CH04_F05_Tuulos.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F05_Tuulos](../../OEBPS/Images/CH04_F05_Tuulos.png)'
- en: Figure 4.5 A batch job, for instance, a task in a workflow
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 批处理作业，例如，工作流中的一个任务
- en: Batch processing vs. stream processing
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理与流处理
- en: 'An alternative to batch processing, which deals with discrete units of computation,
    is *stream processing**,* which deals with a continuous stream of data. Historically,
    the vast majority of ML systems and applications requiring high-performance computing
    have been based on batch processing: data goes in, some processing is done, and
    results come out.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理的一个替代方案是 *流处理*，它处理连续的数据流。历史上，绝大多数机器学习系统和需要高性能计算的应用程序都基于批处理：数据进入，进行一些处理，然后输出结果。
- en: During the past decade, increased sophistication of applications has driven
    demand for stream processing, because it allows results to update with a much
    lower delay, say, in a matter of seconds or minutes, in contrast to batch jobs,
    which are typically run at most once an hour. Today, popular frameworks for stream
    processing include Kafka, Apache Flink, or Apache Beam. In addition, all major
    public cloud providers offer stream-processing-as-a-service, such as Amazon Kinesis
    or Google Dataflow.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，应用复杂性的增加推动了流处理的需求，因为它允许结果以更低的延迟更新，例如，在几秒钟或几分钟内，与通常每小时最多运行一次的批处理作业相比。今天，流行的流处理框架包括
    Kafka、Apache Flink 或 Apache Beam。此外，所有主要的公共云提供商都提供流处理即服务，例如 Amazon Kinesis 或 Google
    Dataflow。
- en: Fortunately, the choice is not either/or. You can have an application use the
    two paradigms side by side. Many large-scale ML systems today, such as the recommendation
    system at Netflix, are mostly based on batch processing with some stream processing
    included for components that need to update frequently.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，选择不是非此即彼。你可以让应用程序同时使用这两种范式。今天，许多大规模机器学习系统，如 Netflix 的推荐系统，主要基于批处理，其中包含一些流处理，用于需要频繁更新的组件。
- en: A major benefit of batch jobs is that they are easier to develop, easier to
    reason about, and easier to scale than their streaming counterparts. Hence, unless
    your application really requires stream processing, it is reasonable to start
    with a workflow of batch jobs as discussed in this chapter. We will discuss more
    advanced use cases that require real-time predictions and/or stream processing
    in chapter 8.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理作业的主要好处是它们比流处理对应物更容易开发、更容易推理和更容易扩展。因此，除非你的应用程序确实需要流处理，否则从本章讨论的批处理作业工作流开始是合理的。我们将在第
    8 章讨论需要实时预测和/或流处理的更高级用例。
- en: A batch job consists of arbitrary code defined by the user. In the case of Metaflow,
    each task, defined by a step method, becomes a single batch job. For instance,
    the train_svm step in the next code sample, copied from listing 3.13, would be
    a batch job.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理作业由用户定义的任意代码组成。在Metaflow的情况下，每个由步骤方法定义的任务都成为一个单独的批处理作业。例如，下一个代码示例中的train_svm步骤，复制自列表3.13，将是一个批处理作业。
- en: Listing 4.1 An example batch job
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 批处理作业示例
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ An external dependency
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 外部依赖
- en: A job scheduler takes this snippet, which we call *user code*, sends it to the
    compute layer for execution, and waits for the execution to complete before continuing
    to the next steps in the workflow. Easy enough!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作业调度器将这个片段，我们称之为*用户代码*，发送到计算层进行执行，并在执行完成后继续工作流程的下一步。这很简单！
- en: 'One more important detail: in this example, the user code refers to an external
    dependency, the sklearn library. If we tried to execute the user code in a pristine
    environment that doesn’t have the library installed, the code would fail to execute.
    To execute successfully, the batch job needs to package both the user code as
    well as any dependencies that the code requires.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的细节：在这个例子中，用户代码引用了一个外部依赖，即sklearn库。如果我们尝试在一个没有安装该库的纯净环境中执行用户代码，代码将无法执行。为了成功执行，批处理作业需要打包用户代码以及代码所需的任何依赖。
- en: Today, it is common to package user code and its dependencies as a *container
    image*. A *container* is a way to provide an isolated execution environment inside
    of a physical computer. Providing such “virtual computers” inside a physical computer
    is called *virtualization*. Virtualization is beneficial, because it allows us
    to pack multiple tasks inside a single physical computer while letting each task
    operate as if they occupied a whole computer by themselves. As discussed in section
    4.1.1, providing strong isolation like this allows each user to focus on their
    work, boosting productivity, because they don’t have to worry about interfering
    with anyone else’s work.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，将用户代码及其依赖打包成*容器镜像*是很常见的。*容器*是在物理计算机内部提供隔离执行环境的一种方式。在物理计算机内部提供这样的“虚拟计算机”被称为*虚拟化*。虚拟化是有益的，因为它允许我们在单个物理计算机中打包多个任务，同时让每个任务像它们独自占据整个计算机一样运行。如第4.1.1节所述，提供这种强大的隔离允许每个用户专注于自己的工作，提高生产力，因为他们不必担心干扰到其他人的工作。
- en: Why do containers matter?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 容器为什么很重要？
- en: A container allows us to package, ship, and isolate the execution of batch jobs.
    To give a real-world analogue, consider that a container is a physical container,
    like an animal crate. First, you can visit an animal shelter (*a container registry*)
    and find a prepackaged feral cat in a crate (*a container image*). The container
    contains both a cat (*user code*) as well as its dependencies, for example, food
    (*libraries*). Next, you can deploy the container (or several) in your house (*a
    physical computer*). As each cat is *containerized*, they can’t cause damage to
    your house or to each other. Without containerization, the house would likely
    turn into a battleground.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 容器允许我们打包、运输和隔离批处理作业的执行。为了给出一个现实世界的类比，考虑一下容器就像一个物理容器，比如动物笼子。首先，你可以访问一个动物收容所（*容器注册库*），在那里你可以在笼子里找到一个预先打包的野猫（*容器镜像*）。容器包含了一只猫（*用户代码*）以及它的依赖，例如食物（*库*）。接下来，你可以在你的家里（*物理计算机*）部署这个容器（或多个）。由于每只猫都被*容器化*了，它们不会对你的房子或彼此造成损害。如果没有容器化，房子很可能会变成战场。
- en: From the point of view of a compute layer, the user code submitted to the system
    resembles feral cats. We shouldn’t assume that any code behaves well. Although
    we don’t assume that data scientists are malevolent per se, it gives users great
    freedom to experiment and overall peace of mind if they know that, in the worst
    case, they can break only their own code. The system guarantees that no matter
    what, the user can never interfere with production systems or tasks of their colleagues.
    Containers help to provide such a guarantee. Figure 4.6 summarizes the discussion.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算层的角度来看，提交给系统的用户代码就像野猫。我们不应该假设任何代码都能良好地运行。尽管我们并不认为数据科学家本身是恶意的，但如果他们知道在最坏的情况下，他们只能破坏自己的代码，这会给用户带来极大的实验自由度，并且总体上让人感到安心。系统保证无论发生什么情况，用户都无法干扰生产系统或同事的任务。容器有助于提供这样的保证。图4.6总结了这次讨论。
- en: '![CH04_F06_Tuulos](../../OEBPS/Images/CH04_F06_Tuulos.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Tuulos](../../OEBPS/Images/CH04_F06_Tuulos.png)'
- en: Figure 4.6 Container on a compute instance
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 计算实例上的容器
- en: Productivity tip Containers boost productivity by granting users the freedom
    to experiment without having to fear that they can break something by accident
    or interfere with their colleagues’ work. Without containers, a rogue process
    can hog an arbitrary amount of CPU or memory or fill the disk, which can cause
    failures in neighboring but unrelated processes on the same instance. Compute-
    and data-intense machine learning processes are particularly prone to these issues.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 生产力技巧 容器通过授予用户自由进行实验而不必担心意外破坏某些东西或干扰同事的工作来提高生产力。没有容器，一个恶意进程可能会占用任意数量的CPU或内存，或者填满磁盘，这可能导致同一实例上相邻但无关的进程失败。计算和数据处理密集型的机器学习过程尤其容易遇到这些问题。
- en: The outer box represents a single computer. The computer provides certain fixed
    hardware, such as CPU cores, possibly GPUs, RAM (memory), and disk. The computer
    runs an operating system, such as Linux. The operating system provides mechanisms
    to execute one or more isolated containers. Inside the container, which provides
    all the necessary dependencies, the user code is executed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 外部框代表一台单独的计算机。计算机提供某些固定的硬件，例如CPU核心、可能还有GPU、RAM（内存）和磁盘。计算机运行一个操作系统，例如Linux。操作系统提供执行一个或多个隔离容器的机制。在容器内部，它提供了所有必要的依赖项，用户代码被执行。
- en: A number of different container formats exist, but today, *Docker* is the most
    popular one. Creating and executing Docker containers isn’t particularly hard
    (if you are curious, see [https://docs.docker.com)](https://docs.docker.com),
    but we shouldn’t assume that data scientists package their own code as containers
    manually. Packaging every iteration of the code as a separate container image
    would just slow down their prototyping loop and thus hurt their productivity.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着多种不同的容器格式，但今天，*Docker*是最受欢迎的一个。创建和执行Docker容器并不特别困难（如果你好奇，请参阅[https://docs.docker.com](https://docs.docker.com)），但我们不应该假设数据科学家会手动将他们的代码打包成容器。将代码的每个迭代都打包成单独的容器镜像只会减慢他们的原型设计循环，从而损害他们的生产力。
- en: Instead, we can containerize their code and dependencies automatically, as exemplified
    by Metaflow in section 4.3\. Under the hood, the data science infrastructure can
    leverage containers to their fullest potential without ever having to surface
    containers, a technical detail, directly to the user. The data scientist can merely
    declare the code they want to execute (steps in workflow) and dependencies they
    require. We will cover dependency management in detail in chapter 7.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以自动将它们的代码和依赖项容器化，正如4.3节中Metaflow所展示的。在底层，数据科学基础设施可以利用容器发挥其最大潜力，而无需将技术细节，即容器，直接暴露给用户。数据科学家只需声明他们想要执行的代码（工作流程中的步骤）以及他们所需的依赖项。我们将在第7章详细讨论依赖项管理。
- en: From a container to a scalable compute layer
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从容器到可扩展的计算层
- en: Now we have learned that we can define a batch job as a container that contains
    the user code and its dependencies. However, when it comes to scalability and
    performance, containerization doesn’t benefit anything by itself. Executing a
    piece of code inside a Docker container on your laptop isn’t any faster or more
    scalable than executing it as a normal process.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解到，我们可以将批处理作业定义为包含用户代码及其依赖项的容器。然而，当涉及到可扩展性和性能时，容器化本身并不会带来任何好处。在你的笔记本电脑上在Docker容器内执行一段代码并不比将其作为正常进程执行更快或更可扩展。
- en: '*Scalability* is what makes a compute layer interesting. Remember the definition
    of scalability: a scalable system is able to handle a growing amount of work by
    adding resources to the system*.* Correspondingly, a compute layer is scalable
    if it can handle more tasks by adding more computers, or instances, to the system.
    This is the exact feature that makes cloud-based compute layers so appealing.
    They are able to increase or decrease the number of physical computers handling
    tasks automatically based on demand. Figure 4.7 illustrates how a scalable compute
    layer works in the context of workflow orchestration.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*可扩展性*是使计算层有趣的地方。记住可扩展性的定义：一个可扩展的系统能够通过向系统添加资源来处理不断增长的工作量。相应地，如果一个计算层能够通过添加更多的计算机或实例来处理更多的任务，那么它就是可扩展的。这正是基于云的计算层如此吸引人的原因。它们能够根据需求自动增加或减少处理任务的物理计算机数量。图4.7说明了在工作流程编排的背景下，可扩展的计算层是如何工作的。'
- en: '![CH04_F07_Tuulos](../../OEBPS/Images/CH04_F07_Tuulos.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F07_Tuulos](../../OEBPS/Images/CH04_F07_Tuulos.png)'
- en: Figure 4.7 Task-scheduling cycle
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 任务调度周期
- en: 'Let’s go through the figure step by step:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地分析这张图：
- en: A job scheduler, like Metaflow’s internal scheduler invoked with the run command,
    begins executing a workflow. It walks through the steps of the workflow in order.
    Each step yields one or more tasks, as we learned in chapter 3.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像使用run命令调用的Metaflow内部调度器一样，作业调度器开始执行工作流。它按顺序遍历工作流的步骤。每个步骤产生一个或多个任务，正如我们在第3章中学到的。
- en: The scheduler submits each task to the compute layer as an independent batch
    job. In the case of a foreach branch, a large number of tasks may be submitted
    to the compute layer simultaneously.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调度器将每个任务作为一个独立的批量作业提交给计算层。在foreach分支的情况下，可能同时向计算层提交大量任务。
- en: The compute layer manages a pool of instances as well as a queue of tasks. It
    tries to match tasks to computers that have suitable resources to execute the
    task.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算层管理一个实例池和一个任务队列。它试图将任务与具有执行任务所需资源的计算机相匹配。
- en: If the compute layer notices way more tasks exist than available resources,
    it can decide to increase the number of instances in the instance pool. In other
    words, it provisions more computers to handle the load.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果计算层发现任务数量远远超过可用资源，它可以决定增加实例池中的实例数量。换句话说，它提供更多的计算机来处理负载。
- en: Eventually, a suitable instance is found where a task can be executed. The task
    is executed in a container. Once the task has finished, the scheduler is notified,
    so it can proceed to the next step in the workflow graph and the cycle restarts.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，找到一个合适的实例，可以在其中执行任务。任务在容器中执行。一旦任务完成，调度器就会收到通知，因此它可以继续到工作流程图中的下一个步骤，然后循环重新开始。
- en: Note that the compute layer can handle incoming tasks from any number of workflows
    concurrently. As illustrated here, the compute layer gets a constant stream of
    task submissions. It executes them without caring *what* the task does internally,
    *why* it needs to be executed, or *when* it was scheduled. Simply, it finds an
    instance *where* to execute the task.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，计算层可以同时处理来自任何数量工作流的任务。如图所示，计算层获得一个恒定的任务提交流。它执行这些任务，而不关心任务内部做什么，为什么需要执行，或者何时被调度。简单地说，它找到执行任务的位置。
- en: To make the steps 3-5 happen, the compute layer needs several components internally.
    Figure 4.8 shows the high-level architecture of a typical compute layer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使步骤3-5发生，计算层内部需要几个组件。图4.8显示了典型计算层的高级架构。
- en: '![CH04_F08_Tuulos](../../OEBPS/Images/CH04_F08_Tuulos.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F08_Tuulos](../../OEBPS/Images/CH04_F08_Tuulos.png)'
- en: Figure 4.8 The architecture of a typical compute layer
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 典型计算层的架构
- en: In the middle, we have a pool of instances. Each instance is a computer like
    the one depicted in figure 4.6\. They are machines that execute one or more concurrent
    containers, which in turn are used to execute the user code.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在中间，我们有一个实例池。每个实例就像图4.6中描述的计算机一样。它们是执行一个或多个并发容器的机器，而这些容器又用于执行用户代码。
- en: At the bottom, a component called the *cluster management system* is depicted.
    This system is responsible for managing the pool of instances. In this case, we
    have a pool of three instances. The cluster management system adds and removes
    instances from the pool as the demand—the number of pending tasks—increases or
    decreases, or as instances are detected to be unhealthy. Note that the instances
    don’t need to have uniform hardware. Some instances may have more CPUs, some more
    GPUs, and some more RAM.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在底部，描绘了一个称为**集群管理系统**的组件。该系统负责管理实例池。在这种情况下，我们有一个包含三个实例的实例池。集群管理系统根据需求——待处理任务的数量——增加或减少实例池中的实例，或者当检测到实例不健康时，从池中添加或删除实例。请注意，实例不需要具有统一的硬件。一些实例可能有更多的CPU，一些可能有更多的GPU，一些可能有更多的RAM。
- en: At the top, we have a *container orchestration system**.* It is responsible
    for maintaining a queue of pending tasks and placing and executing them in containers
    on the underlying instances. It is the job of this system to match tasks, based
    on their resource requirements, to the underlying instances. For instance, if
    a task requires a GPU, the system needs to find an instance with a GPU in the
    underlying pool and wait until the instance is not busy executing previous tasks
    before placing the task on the instance.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在顶部，我们有一个**容器编排系统**。它负责维护一个待处理任务的队列，并将任务放置在底层实例上的容器中执行。该系统的任务是根据任务资源需求将任务与底层实例相匹配。例如，如果任务需要一个GPU，系统需要找到底层池中带有GPU的实例，并等待该实例在执行之前的任务后空闲，然后将任务放置在该实例上。
- en: Fortunately, we don’t need to implement container orchestration systems or cluster
    management systems from scratch—they are notoriously complex pieces of software.
    Instead, we can leverage existing battle-hardened compute layers provided either
    as open source or as managed services by the cloud providers. We list a selection
    of such systems in the next section. As you evaluate these systems by yourself,
    it is good to keep these figures in mind, because they can help you understand
    how the systems work under the hood and motivate various tradeoffs that the systems
    have needed to make.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要从头开始实现容器编排系统或集群管理系统——它们是臭名昭著的复杂软件。相反，我们可以利用云提供商提供的现有经过实战考验的计算层，无论是开源的还是托管服务。我们将在下一节列出此类系统的选择。当你自己评估这些系统时，记住这些图是个好主意，因为它们可以帮助你理解系统在底层是如何工作的，并激励系统做出各种权衡。
- en: 4.2.2 Examples of compute layers
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 计算层示例
- en: Let’s go through some compute layers that you can start using today. Now that
    you have a basic understanding of how these systems work under the hood, you can
    appreciate that each system optimizes for slightly different characteristics—no
    system is perfect at everything. Luckily, we are not limited to a single choice.
    Our infrastructure stack can provide different compute layers for different use
    cases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看你可以开始使用的计算层。现在，你已经对这些系统在底层的工作原理有了基本的了解，你可以欣赏到每个系统都针对略微不同的特性进行了优化——没有系统在所有事情上都完美。幸运的是，我们不受单一选择的限制。我们的基础设施堆栈可以为不同的用例提供不同的计算层。
- en: Figure 4.9 illustrates why supporting multiple compute layers can come in handy.
    Don’t worry if you don’t recognize the names of the compute layers in the figure—Spark,
    AWS Batch, SageMaker, and AWS Lambda. We will cover them in more detail soon.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9说明了支持多个计算层为何会很有用。如果你不认识图中计算层的名称——Spark、AWS Batch、SageMaker和AWS Lambda——请不要担心。我们很快会详细介绍它们。
- en: '![CH04_F09_Tuulos](../../OEBPS/Images/CH04_F09_Tuulos.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F09_Tuulos](../../OEBPS/Images/CH04_F09_Tuulos.png)'
- en: Figure 4.9 Examples of workflows that use multiple compute layers
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9展示了使用多个计算层的流程示例
- en: 'The figure depicts the following three projects, each with a workflow of their
    own:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了以下三个项目，每个项目都有其自己的工作流程：
- en: Project 1 is a large, advanced project. It needs to process a large amount of
    data, say a text corpus of 100 GB, and train a massive deep neural network model
    based on it. First, large-scale data processing is performed with Spark, which
    is optimized for the job. Additional data preparation is performed on a large
    instance managed by AWS Batch. Training a large-scale neural network requires
    a compute layer optimized for the job. We can use Amazon SageMaker to train the
    model on a cluster of GPU instances. Finally, we can send a notification that
    the model is ready using a lightweight task launched on AWS Lambda.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目1是一个大型、高级项目。它需要处理大量数据，比如100 GB的文本语料库，并基于它训练一个大规模的深度神经网络模型。首先，使用针对该任务优化的Spark进行大规模数据处理。然后在由AWS
    Batch管理的大型实例上执行额外的数据准备。训练大规模神经网络需要一个针对该任务优化的计算层。我们可以使用Amazon SageMaker在GPU实例集群上训练模型。最后，我们可以通过在AWS
    Lambda上启动一个轻量级任务来发送模型已准备好的通知。
- en: Project 2 trains a decision tree using a medium-scale, say, 50 GB, dataset.
    We can process data of this scale, train a model, and publish results, on standard
    CPU instances with, say, 128 GB of RAM. A general-purpose compute layer like AWS
    Batch can handle the job easily.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目2使用中等规模的数据集，比如50 GB，来训练决策树。我们可以使用具有128 GB RAM的标准CPU实例处理此类规模的数据、训练模型并发布结果。像AWS
    Batch这样的通用计算层可以轻松处理这项工作。
- en: Project 3 represents an experiment conducted by a data scientist. The project
    involves training a small model for each country in the world. Instead of training
    200 models sequentially on their laptop, they can parallelize model training using
    AWS Lambda, speeding up their prototyping loop.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3代表一位数据科学家进行的实验。该项目涉及为世界上每个国家训练一个小型模型。他们不必在笔记本电脑上依次训练200个模型，而是可以使用AWS Lambda并行化模型训练，从而加快他们的原型设计循环。
- en: As figure 4.9 illustrates, the choice of compute layers depends on the type
    of projects you will need to support. It is a good idea to start with a single,
    general-purpose system like AWS Batch and add more options as the *variety* of
    use cases increases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如图4.9所示，计算层的选取取决于你需要支持的项目类型。从单一、通用的系统如AWS Batch开始，随着用例种类的增加，添加更多选项是个好主意。
- en: Crucially, although the infrastructure stack may support multiple compute layers,
    we can limit the amount of complexity exposed to the user. All we have to do is
    to write workflows in Python, maybe using specific libraries in the case of specialized
    compute layers. Also keep in mind the ergonomics of the two loops, prototyping
    and production deployments, which we discussed in section 2.1\. Typically, prototyping
    requires quick iterations with smaller amounts of data, whereas production deployments
    emphasize scalability.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，尽管基础设施堆栈可能支持多个计算层，但我们仍然可以限制向用户暴露的复杂性。我们只需用Python编写工作流程，在特定计算层的情况下，可能需要使用特定的库。还要记住，我们在第2.1节中讨论的两个循环的工效学，即原型设计和生产部署。通常，原型设计需要快速迭代和较少的数据量，而生产部署则强调可扩展性。
- en: 'How should one evaluate the strengths and weaknesses of different compute layers?
    You can pay attention to the following features:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如何评估不同计算层的优缺点？你可以关注以下特性：
- en: '*Workload support*—Some systems are specialized for certain types of workloads,
    for example, big data processing or managing multiple GPUs, whereas others are
    general purpose, working with any types of tasks.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作负载支持*—一些系统针对特定类型的工作负载进行了优化，例如大数据处理或管理多个GPU，而其他系统则是通用的，可以处理任何类型的任务。'
- en: '*Latency*—Some systems try to guarantee that tasks start with a minimal delay.
    This is convenient during prototyping, when it might be frustrating to wait for
    minutes for a task to start. On the other hand, startup latency doesn’t make any
    difference to scheduled nightly runs.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟*—一些系统试图保证任务以最小的延迟开始。在原型设计阶段，这可能很方便，因为等待几分钟才能启动任务可能会让人感到沮丧。另一方面，启动延迟对夜间计划运行没有影响。'
- en: '*Workload management*—How does the system behave when it receives more tasks
    than what it can deploy to the instance pool immediately? Some systems start declining
    tasks, some add them to a queue, and some may start killing, or *preempting,*
    already executing tasks so that higher priority tasks can execute in their place.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作负载管理*—当系统接收到的任务数量超过其可以立即部署到实例池的任务时，系统会如何表现？一些系统开始拒绝任务，一些将它们添加到队列中，而一些可能开始终止或*抢占*正在执行的任务，以便更高优先级的任务可以替代它们执行。'
- en: '*Cost-efficiency*—As described earlier, a key lever to cost optimization is
    utilization. Some systems are much more aggressive at driving up utilization whereas
    others take a laxer approach. Also, the granularity of billing in cloud systems
    varies: some bill by the hour, some by the second, and some even by the millisecond.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本效益*—如前所述，成本优化的关键杠杆是利用率。一些系统在提高利用率方面更为激进，而其他系统则采取更为宽松的方法。此外，云系统中的计费粒度也各不相同：一些按小时计费，一些按秒计费，甚至有的按毫秒计费。'
- en: '*Operational complexity*—Some systems are rather easy to deploy, debug, and
    maintain, whereas others may require constant monitoring and upkeep.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*操作复杂性*—一些系统部署、调试和维护相对简单，而其他系统可能需要持续的监控和维护。'
- en: Next, we list a few popular choices for compute layers. The list is not exhaustive
    by any means, but it gives you an idea how to compare relative benefits of various
    options.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们列出了一些计算层的流行选择。这个列表并不全面，但它可以给你一个比较各种选项相对优势的思路。
- en: Kubernetes
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes (often abbreviated as K8S) is the most popular open source container
    orchestration system today. It originates from Google, which had been operating
    a similar compute layer internally for years. You can deploy K8S in a private
    data center, even on your laptop (search for *Minikube* for instructions), but
    it is commonly used as a managed cloud service, such as Elastic Kubernetes Service
    (EKS) by AWS.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes（通常简称为K8S）是目前最受欢迎的开源容器编排系统。它起源于谷歌，谷歌内部已经运营了多年的类似计算层。你可以在私有数据中心部署K8S，甚至可以在你的笔记本电脑上部署（搜索*Minikube*获取说明），但它通常用作托管云服务，例如AWS的弹性Kubernetes服务（EKS）。
- en: Kubernetes is an extremely flexible system. Consider it a toolkit for building
    your own compute layer or a microservice platform. With flexibility comes a great
    deal of complexity. Kubernetes and services around it evolve quickly, so it takes
    expertise and effort to stay up-to-date with its ecosystem. However, if you need
    an infinitely extensible foundation for a custom compute layer, Kubernetes is
    a great starting point. See table 4.1 for characteristics of Kubernetes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个极其灵活的系统。将其视为构建您自己的计算层或微服务平台的工具包。灵活性伴随着大量的复杂性。Kubernetes及其周围的服务发展迅速，因此需要专业知识和努力才能跟上其生态系统。然而，如果您需要一个无限可扩展的自定义计算层的基础，Kubernetes是一个很好的起点。查看表4.1以了解Kubernetes的特点。
- en: Table 4.1 Characteristics of Kubernetes
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 Kubernetes的特点
- en: '| **Workload support** | General-purpose. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载支持** | 通用。|'
- en: '| **Latency** | K8S is primarily a container orchestration system. You can
    configure it to work with various cluster management systems that handle scalability.
    The choice has a major effect on startup latency of tasks. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | K8S主要是一个容器编排系统。您可以配置它与各种集群管理系统一起工作，这些系统处理可伸缩性。选择对任务的启动延迟有重大影响。|'
- en: '| **Workload management** | Although K8S provides only minimal workload management
    out of the box, you can make K8S work with any work queue. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载管理** | 虽然K8S提供的是开箱即用的最小工作负载管理，但您可以使K8S与任何工作队列一起工作。|'
- en: '| **Cost efficiency** | Configurable; depends mainly on the underlying cluster
    management system. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **成本效率** | 可配置的；主要取决于底层集群管理系统。|'
- en: '| **Operational complexity** | High; K8S has a steep learning curve. Managed
    cloud solutions like EKS make this a bit easier. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **操作复杂性** | 高；K8S的学习曲线陡峭。像EKS这样的托管云解决方案使这变得容易一些。|'
- en: AWS Batch
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Batch
- en: 'AWS provides a number of container orchestration systems: ECS (Elastic Container
    Service), which runs containers on top of EC2 instances that you can manage; Fargate,
    which is a serverless orchestrator (i.e., no EC2 instances to be managed); and
    EKS, which manages containers with Kubernetes. AWS Batch is a layer on top of
    these systems, providing batch-compute capabilities for the underlying orchestrators,
    in particular, a task queue.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了一系列容器编排系统：ECS（弹性容器服务），它运行在您可以管理的EC2实例之上；Fargate，它是一个无服务器编排器（即不需要管理EC2实例）；以及EKS，它使用Kubernetes管理容器。AWS
    Batch是这些系统之上的一个层，为底层的编排器提供批处理计算能力，特别是任务队列。
- en: AWS Batch is one the simplest solutions for operating a cloud-based compute
    layer. You define the types of instances you want to have in your instance pool,
    called the *compute environment*, and one or more *job queues*, which store pending
    tasks. After this, you can start submitting tasks to the queue. AWS Batch takes
    care of provisioning instances, deploying containers, and waiting until they have
    executed successfully. A downside of this simplicity is that AWS Batch provides
    only a limited amount of extensibility and configurability for more advanced use
    cases. You can read more about AWS Batch in section 4.3\. See table 4.2 for characteristics
    of AWS Batch.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Batch是操作基于云的计算层最简单的解决方案之一。您定义您希望在实例池中拥有的实例类型，称为*计算环境*，以及一个或多个*作业队列*，用于存储待处理任务。之后，您就可以开始向队列提交任务。AWS
    Batch负责提供实例、部署容器，并等待它们成功执行。这种简单性的缺点是，AWS Batch只为更高级的使用案例提供了有限的扩展性和可配置性。您可以在第4.3节中了解更多关于AWS
    Batch的信息。查看表4.2以了解AWS Batch的特点。
- en: Table 4.2 Characteristics of AWS Batch
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2 AWS Batch的特点
- en: '| **Workload support** | General purpose. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载支持** | 通用。|'
- en: '| **Latency** | Relatively high; as the name implies, AWS Batch is designed
    for batch processing with the assumption that startup latencies are not a major
    issue. A task may take anywhere between seconds to a few minutes to start. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | 相对较高；正如其名所示，AWS Batch是为批处理设计的，假设启动延迟不是主要问题。一个任务可能需要几秒钟到几分钟才能开始。|'
- en: '| **Workload management** | Includes a built-in work queue. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载管理** | 包含内置的工作队列。|'
- en: '| **Cost efficiency** | Configurable; you can use AWS Batch with any instance
    types without extra cost. It also supports spot instances*,* which are considerably
    cheaper than normal, on-demand EC2 instances. Spot instances may be terminated
    abruptly, but this is usually not an issue for batch jobs that can be retried
    automatically. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| **成本效率** | 可配置的；您可以使用AWS Batch与任何实例类型，无需额外费用。它还支持*spot instances*，比普通的按需EC2实例便宜得多。Spot实例可能会突然终止，但对于可以自动重试的批处理作业来说，这通常不是问题。|'
- en: '| **Operational complexity** | Low; relatively simple to set up and almost
    maintenance-free. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| **操作复杂性** | 低；相对简单设置，几乎无需维护。 |'
- en: AWS Lambda
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda
- en: AWS Lambda is often characterized as a *function-as-a-service*. Instead of defining
    servers or even containers, you simply define a snippet of code, a task in our
    parlance, which AWS Lambda executes when a triggering event occurs without any
    user-visible instances. Since December 2020, AWS Lambda allows tasks to be defined
    as container images, which makes AWS Lambda a valid option for a compute layer.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda 通常被描述为一种 *函数即服务*。您不需要定义服务器或容器，只需定义一段代码，在我们的术语中称为任务，AWS Lambda 就会在触发事件发生时执行该代码，而无需任何用户可见的实例。自
    2020 年 12 月以来，AWS Lambda 允许将任务定义为容器镜像，这使得 AWS Lambda 成为计算层的有效选择。
- en: Compared to AWS Batch, the biggest difference is that Lambda doesn’t expose
    the instance pool (aka compute environment) at all. Although tasks can request
    additional CPU cores and memory, options for resource requirements are much more
    limited. This makes AWS Lambda most suitable for lightweight tasks with modest
    requirements. For instance, you can use Lambda to process small to medium amounts
    of data rapidly during prototyping. See table 4.3 for characteristics of AWS Lambda.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS Batch 相比，最大的不同之处在于 Lambda 完全不暴露实例池（也称为计算环境）。尽管任务可以请求额外的 CPU 核心和内存，但资源需求的选择范围要小得多。这使得
    AWS Lambda 最适合具有适度要求的轻量级任务。例如，您可以在原型设计期间使用 Lambda 快速处理小到中等量的数据。请参阅表 4.3 了解 AWS
    Lambda 的特性。
- en: Table 4.3 Characteristics of AWS Lambda
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.3 AWS Lambda 特性
- en: '| **Workload support** | Limited to lightweight tasks with a relatively short
    runtime. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载支持** | 限于相对较短运行时间的轻量级任务。 |'
- en: '| **Latency** | Low; AWS Lambda is optimized for tasks that start in a second
    or less. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | 低；AWS Lambda针对在1秒或更短时间内启动的任务进行了优化。 |'
- en: '| **Workload management** | In the *Asynchronous Invocation* mode, Lambda includes
    a work queue. The queue is more opaque than the job queue of AWS Batch, for instance.
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载管理** | 在 *异步调用* 模式下，Lambda 包含一个工作队列。与 AWS Batch 的工作队列相比，队列的透明度更低。 |'
- en: '| **Cost efficiency** | Great; tasks are billed by millisecond, so you truly
    pay only for what you use. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **成本效益** | 非常高；按毫秒计费，因此您只需为实际使用的部分付费。 |'
- en: '| **Operational complexity** | Very low; simple to set up and practically maintenance-free.
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **操作复杂性** | 非常低；设置简单，几乎无需维护。 |'
- en: Apache Spark
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Apache Spark is a popular open source engine for large-scale data processing.
    It differs from the previously listed services by relying on a specific programming
    paradigm and data structures to achieve scalability. It is not suitable for executing
    arbitrary containers. However, Spark allows code to be written in JVM-based languages,
    Python, or SQL, so it can be used to execute arbitrary code as long as the code
    adheres with the Spark paradigm. You can deploy a Spark cluster on your own instances,
    or you can use it as a managed cloud service, for example, through AWS Elastic
    MapReduce (EMR). See table 4.4 for characteristics of Apache Spark.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个流行的开源大数据处理引擎。它与之前列出的服务不同，因为它依赖于特定的编程范式和数据结构来实现可伸缩性。它不适用于执行任意容器。然而，Spark
    允许使用基于 JVM 的语言、Python 或 SQL 编写代码，因此只要代码遵循 Spark 范式，就可以用来执行任意代码。您可以在自己的实例上部署 Spark
    集群，或者将其用作托管云服务，例如通过 AWS Elastic MapReduce (EMR)。请参阅表 4.4 了解 Apache Spark 的特性。
- en: Table 4.4 Characteristics of Apache Spark
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.4 Apache Spark 特性
- en: '| **Workload support** | Limited to code written with Spark constructs. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载支持** | 限于使用 Spark 构造编写的代码。 |'
- en: '| **Latency** | Depends on the underlying cluster management policy. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | 取决于底层集群管理策略。 |'
- en: '| **Workload management** | Includes a built-in work queue. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载管理** | 包含内置的工作队列。 |'
- en: '| **Cost efficiency** | Configurable, depending on the cluster setup. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **成本效益** | 可配置，取决于集群设置。 |'
- en: '| **Operational complexity** | Relatively high; Spark is a sophisticated engine
    that requires specialized knowledge to operate and maintain. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **操作复杂性** | 相对较高；Spark 是一个复杂的引擎，需要专业知识来操作和维护。 |'
- en: Distributed training platforms
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练平台
- en: Although you can use a general-purpose compute layer like Kubernetes or AWS
    Batch to train sizable models, especially when powered by GPU instances, a specialized
    compute layer is needed to train the largest deep neural network models, such
    as for massive-scale compute vision. It is possible to build such a system using
    open source components, such as using a project called *Horovod*, which originated
    from Uber, or *TensorFlow distributed training*, but many companies may find it
    easier to use a managed cloud service such as distributed training by SageMaker
    or Google’s Cloud TPU.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以使用像 Kubernetes 或 AWS Batch 这样的通用计算层来训练大型模型，尤其是在 GPU 实例的支持下，但训练最大的深度神经网络模型，如大规模计算视觉，需要一个专门的计算层。可以使用开源组件构建这样的系统，例如使用名为
    *Horovod* 的项目，该项目起源于 Uber，或者 *TensorFlow 分布式训练*，但许多公司可能发现使用 SageMaker 或 Google
    的 Cloud TPU 这样的托管云服务更容易。
- en: These systems are optimized for very specific workloads. They employ large clusters
    of GPUs, and sometimes custom hardware, to speed up tensor or matrix computations
    required by modern neural networks. If your use cases require the training of
    massive-scale neural networks, having such a system as a part of your infrastructure
    may be necessary. See table 4.5 for characteristics of distributed training platforms.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统针对非常具体的工作负载进行了优化。它们使用大量的 GPU 集群，有时还使用定制硬件，以加快现代神经网络所需的张量或矩阵计算。如果你的用例需要训练大规模神经网络，那么在你的基础设施中拥有这样的系统可能是必要的。参见表
    4.5 了解分布式训练平台的特征。
- en: Table 4.5 Characteristics of distributed training platforms
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.5 分布式训练平台的特征
- en: '| **Workload support** | Very limited; optimized for training massive-scale
    models. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **工作量支持** | 非常有限；优化用于训练大规模模型。|'
- en: '| **Latency** | High; optimized for batch processing. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | 高；优化用于批量处理。|'
- en: '| **Workload management** | Task-specific, opaque workload management. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **工作量管理** | 任务特定，不透明的工作量管理。|'
- en: '| **Cost efficiency** | Typically very expensive. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **成本效率** | 通常非常昂贵。|'
- en: '| **Operational complexity** | Relatively high, although cloud services are
    considerably easier to operate and maintain compared to on-premise solutions.
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **操作复杂性** | 相对较高，尽管与本地解决方案相比，云服务在操作和维护方面要容易得多。|'
- en: Local processes
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本地进程
- en: Historically, most data science workloads have been executed on personal computers,
    such as on laptops. A modern take on this is a cloud workstation, as described
    in section 2.1.2\. Although a workstation is not a compute layer in the sense
    of what is shown in figure 4.8, it can be used to execute processes and containers,
    and for most companies, it is the first compute layer supported in the absence
    of other systems.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，大多数数据科学工作负载都是在个人计算机上执行的，例如在笔记本电脑上。对这一点的现代看法是云工作站，如第 2.1.2 节所述。尽管工作站不是图
    4.8 所示的计算层，但它可以用来执行进程和容器，并且对于大多数公司来说，在没有其他系统的情况下，它是第一个支持的计算层。
- en: When viewed from the point of view of compute, a personal workstation has one
    major upside and one major downside. The upside is that the workstation provides
    a very low latency and, hence, a quick prototyping loop. The downside is that
    it doesn’t scale. Therefore, workstations are best used for prototyping, while
    all heavy lifting is offloaded to other systems. See table 4.6 for characteristics
    of local processes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算的角度来看，个人工作站有一个主要优点和一个主要缺点。优点是工作站提供了非常低的延迟，因此有快速的原型设计循环。缺点是它无法扩展。因此，工作站最适合用于原型设计，而所有重负载都转移到其他系统上。参见表
    4.6 了解本地进程的特征。
- en: Table 4.6 Characteristics of local processes
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.6 本地进程的特征
- en: '| **Workload support** | General purpose. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **工作量支持** | 通用。|'
- en: '| **Latency** | Very low; processes start instantly. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | 非常低；进程立即启动。|'
- en: '| **Workload management** | Configurable, nothing by default. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **工作量管理** | 可配置的，默认无设置。|'
- en: '| **Cost efficiency** | Cheap, but the amount of compute is limited. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| **成本效率** | 价格低廉，但计算量有限。|'
- en: '| **Operational complexity** | Moderate; workstations need maintenance and
    debugging. Providing and maintaining a uniform environment for all users can be
    hard. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| **操作复杂性** | 中等；工作站需要维护和调试。为所有用户提供统一的环境可能很困难。|'
- en: Comparison
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 比较
- en: As the variety of use cases supported by the infrastructure increases, so does
    the need to provide compute layers optimized for particular workloads. As the
    provider of the data science infrastructure, you need to evaluate what systems
    should be included in your stack, when, how, and why.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基础设施支持的使用案例种类的增加，对提供针对特定工作负载优化的计算层的需要也随之增加。作为数据科学基础设施的提供者，您需要评估应该将哪些系统包含在您的堆栈中，何时、如何以及为什么这么做。
- en: To help you with the task, table 4.7 provides a rough summary of the main strengths
    and weaknesses of the systems we covered. One star indicates that the system doesn’t
    excel at a particular area, two stars indicates acceptable behavior, and three
    stars indicate that the system shines at the task.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您完成这项任务，表4.7提供了我们所涵盖的系统的主要优缺点的简要总结。一颗星表示系统在特定领域表现不佳，两颗星表示可接受的行为，三颗星表示系统在该任务上表现卓越。
- en: Table 4.7 Comparison of a selection of common compute layers
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.7 常见计算层的比较
- en: '|  | Local | Kubernetes | Batch | Lambda | Spark | Distributed training |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | 本地 | Kubernetes | 批量 | Lambda | Spark | 分布式训练 |'
- en: '| **Excels at general- purpose compute** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)  |
    ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| **擅长通用计算** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)  |
    ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) |'
- en: '| **Excels at data processing** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **擅长数据处理** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) |'
- en: '| **Excels at model training** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **擅长模型训练** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    |'
- en: '| **Tasks start quickly** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| **任务启动迅速** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    |'
- en: '| **Can queue a large number of pending tasks** | ![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **能够排队大量待处理任务** | ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) |'
- en: '| **Inexpensive** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **经济实惠** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    |'
- en: '| **Easy to deploy and operate** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)  |
    ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| **易于部署和操作** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)  |
    ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    |'
- en: '| **Extensibility** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)  |
    ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)  |
    ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)
    | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png) | ![star](../../OEBPS/Images/star.png)![star](../../OEBPS/Images/star.png)
    |'
- en: Don’t get too concerned about individual assessments—they can be challenged.
    The main take-home message is that there isn’t a single system that can handle
    all workloads optimally. Also, if you compare columns, you can see that some systems
    overlap in functionality (e.g., Kubernetes and Batch) versus others that are more
    complementary (e.g., Lambda and Spark).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过于关注个别评估——它们是可以被挑战的。主要的启示信息是没有一个系统可以最优地处理所有工作负载。此外，如果您比较列，您可以看到一些系统在功能上有所重叠（例如，Kubernetes
    和 Batch）与那些更互补的系统（例如，Lambda 和 Spark）。
- en: As an exercise, you can create your own version of table 4.7\. Include features
    that matter to you as rows and systems that you may consider using as columns.
    The outcome of the exercise should be a complementary set of systems that match
    the needs of data science projects you need to support. If you are unsure, a good
    starting point follows.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项练习，您可以创建自己版本的表 4.7。将您关心的特性作为行，将您可能考虑使用的系统作为列。练习的结果应该是一套互补的系统，以满足您需要支持的数据科学项目的需求。如果您不确定，以下是一个良好的起点。
- en: Rule of thumb Provide one general-purpose compute layer like Kubernetes or AWS
    Batch for heavy lifting and a low-latency system like local processes for prototyping.
    Use more specialized systems as required by your use cases.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧规则 提供一个通用计算层，如 Kubernetes 或 AWS Batch，用于重负载处理，以及一个低延迟系统，如本地进程，用于原型设计。根据您的用例需求，使用更多专用系统。
- en: No matter what systems you end up choosing, make sure they can be integrated
    seamlessly into a cohesive user experience for the data scientist. From the user’s
    point of view, it is a nuisance that multiple systems need to exist in the first
    place. However, pretending that this is not the reality often leads to even more
    friction and frustration.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您最终选择什么系统，请确保它们可以无缝集成到数据科学家的一致用户体验中。从用户的角度来看，多个系统最初就需要存在是一种麻烦。然而，假装这不是现实往往会导致更多的摩擦和挫败感。
- en: In section 4.3, we get to roll up our sleeves and start practicing compute layers
    and scalability for real using Metaflow. Also, the section will serve as an example
    of how multiple compute layers can happily coexist behind a single cohesive user
    interface.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在4.3节中，我们将开始动手实践使用Metaflow进行计算层和可扩展性。此外，本节还将作为一个示例，说明多个计算层如何在一个统一的用户界面下愉快地共存。
- en: Considering cost
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑成本
- en: 'Many companies are concerned about the cost of using a cloud-based compute
    layer. When it comes to cost optimization, a key observation is that an idling
    instance costs the same amount of money as an instance that performs work. Hence,
    a key lever to minimize cost is to maximize utilization, that is, the share of
    time spent on useful work. We define utilization as the percentage of the total
    uptime that is used to execute tasks, as shown here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司都担心使用基于云的计算层的成本。当涉及到成本优化时，一个关键的观察结果是闲置实例的成本与执行工作的实例的成本相同。因此，降低成本的关键杠杆是最大化利用率，即用于有用工作的时段比例。我们定义利用率为用于执行任务的总体运行时间的百分比，如图所示：
- en: '![CH04_EQ01](../../OEBPS/Images/CH04_EQ01.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_EQ01](../../OEBPS/Images/CH04_EQ01.png)'
- en: 'Now, assuming that we can’t affect the time that it takes for tasks to execute,
    we achieve the minimal cost when the time used to execute tasks equals the total
    instance uptime, that is, we attain 100% utilization. In practice, utilization
    of a typical compute layer is far from 100%. In particular, old-school data centers
    might have utilization of 10% or less. You can drive up utilization in the following
    two ways:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们无法影响任务执行所需的时间，当用于执行任务的时间等于总实例运行时间时，我们达到最小成本，即我们达到100%的利用率。在实践中，典型计算层的利用率远低于100%。特别是，老式数据中心可能只有10%或更低的利用率。您可以通过以下两种方式提高利用率：
- en: You can minimize the *total instance uptime* by shutting down instances as soon
    as tasks finish.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过在任务完成时立即关闭实例来最小化*总实例运行时间*。
- en: You can maximize *time used to execute tasks*, for example, by sharing the instances
    with as many projects and workflows as possible, so the instances don’t run out
    of work while they are up.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过尽可能多地与项目和工作流程共享实例来最大化*用于执行任务的时间*，这样实例在运行期间就不会因工作耗尽而停止。
- en: You can use the compute layer described in this section to achieve both goals.
    First, it is easy to shut down cloud-based instances when they run out of work
    automatically, so you pay only for the exact set of tasks that you need to execute.
    Second, thanks to strong isolation guarantees provided by virtualization and containerization,
    you can safely share the same instances with multiple teams. This increases the
    number of tasks submitted to the compute layer, which drives up utilization.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用本节中描述的计算层来实现这两个目标。首先，当云实例自动完成工作后，关闭它们很容易，因此您只需为需要执行的精确任务集付费。其次，由于虚拟化和容器化提供的强大隔离保证，您可以安全地与多个团队共享相同的实例。这增加了提交给计算层的任务数量，从而提高了利用率。
- en: It is good to keep in mind that in most cases, the hourly cost of a data scientist
    is much higher than that of an instance. Any opportunity to save data scientist
    hours by using more instance hours is usually worth it. For instance, it might
    be more cost effective to run experiments using naive, inefficient code, knowing
    that it requires more instance hours, rather than spend many days or weeks optimizing
    the code by hand.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在大多数情况下，数据科学家的每小时成本远高于实例的成本。通过使用更多实例小时来节省数据科学家的时间的机会通常都是值得的。例如，使用原始、低效的代码进行实验可能更经济，因为这样需要更多的实例小时，而不是花费许多天或几周手动优化代码。
- en: 4.3 The compute layer in Metaflow
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 Metaflow中的计算层
- en: '*Alex’s laptop sounds like a jet engine every time it executes a model-training
    step. Instead of buying noise-canceling headphones, it seems like a smart idea
    to leverage the cloud for demanding compute tasks. Bowie helps Alex to configure
    Metaflow to use AWS Batch as a compute layer, which enables Alex to prototype
    workflows locally and execute them in the cloud with a click of a button. For
    Alex, this feels like a silent superpower!*'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*每次Alex执行模型训练步骤时，他的笔记本电脑听起来就像喷气式发动机。与其购买降噪耳机，不如利用云来处理计算密集型任务似乎是个明智的选择。Bowie帮助Alex配置Metaflow以使用AWS
    Batch作为计算层，这使得Alex可以在本地原型化工作流程，并只需点击一下按钮即可在云中执行它们。对Alex来说，这感觉就像是一种无声的超能力！*'
- en: '![CH04_F09_UN03_Tuulos](../../OEBPS/Images/CH04_F09_UN03_Tuulos.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F09_UN03_Tuulos](../../OEBPS/Images/CH04_F09_UN03_Tuulos.png)'
- en: Metaflow supports pluggable compute layers. For instance, you can execute lightweight
    tasks locally but offload heavy data processing and model training to a cloud-based
    compute layer. Or, if your company has an existing container management system
    like Kubernetes, you can use it as a centralized compute layer instead of having
    to operate a separate system for data science.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow支持可插拔的计算层。例如，您可以在本地执行轻量级任务，但将重型数据处理和模型训练卸载到基于云的计算层。或者，如果您的公司已经有一个像Kubernetes这样的现有容器管理系统，您可以使用它作为集中的计算层，而不是必须为数据科学运行一个单独的系统。
- en: By default, Metaflow uses local processes running on your personal workstation
    as the compute layer. This is convenient for quick prototyping. To demonstrate
    how local processes work in practice, take a look at the next listing.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Metaflow使用运行在您个人工作站上的本地进程作为计算层。这对于快速原型设计来说很方便。为了展示本地进程在实际中的工作方式，请查看下一个列表。
- en: Listing 4.2 Local processes as a compute layer
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 本地进程作为计算层
- en: '[PRE1]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Initializes a global variable
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化全局变量
- en: ❷ Modifies the value of the global variable
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 修改全局变量的值
- en: 'Save the code to process_demo.py. Here, global_value is initialized as a module-level
    global variable. Its value is changed from 5 to 9 in the start step. The value
    is printed again in the end step. Can you guess whether the value printed at the
    end step will be 5 or 9? Execute the following code to test it:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到process_demo.py。在这里，global_value被初始化为模块级全局变量。其值在起始步骤从5变为9。在结束步骤再次打印值。你能猜到结束步骤打印的值是5还是9吗？执行以下代码来测试它：
- en: '[PRE2]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The value is 9 at the start step as expected. The value at the end step is 5\.
    If start and end were ordinary Python functions executed sequentially, the value
    would stay as 9\. However, Metaflow executes each task as a separate local process,
    so the value of global_value is reset back to 5 at the beginning of each task.
    If you want to persist the change across tasks, you should store global_value
    as a data artifact in self, instead of relying on a module-level variable. You
    can also see that the process ID is distinct for the two tasks, which wouldn’t
    be the case if the tasks were executed by the same Python process. Executing Metaflow
    tasks as independent units of computation matters for the compute layer.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 预期地，起始步骤的值是9。结束步骤的值是5。如果起始和结束是按顺序执行的普通Python函数，值将保持为9。然而，Metaflow将每个任务作为一个单独的本地进程执行，因此global_value的值在每个任务开始时重置为5。如果您想在任务之间持久化更改，您应该将global_value作为self中的数据工件存储，而不是依赖于模块级变量。您还可以看到，两个任务的进程ID是不同的，如果任务由同一个Python进程执行，这种情况是不会发生的。将Metaflow任务作为独立的计算单元执行对于计算层很重要。
- en: Note Metaflow tasks are isolated units of computation that can be executed on
    various compute layers. A single workflow can farm out tasks to many different
    compute layers, using the most suitable system for each task.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Metaflow任务是可以执行在各种计算层上的隔离的计算单元。单个工作流程可以将任务分发给许多不同的计算层，使用最适合每个任务的系统。
- en: 'Metaflow’s approach to computation is based on the following three assumptions
    on the nature of generalized infrastructure for data science:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow的计算方法基于以下三个关于数据科学通用基础设施本质的假设：
- en: '*Infrastructure needs to support a wide variety of projects that have varying
    needs for computation*. Some need lots of memory on a single instance, some need
    many small instances, and some require specialized hardware like GPUs. There is
    no one-size-fits-all approach for all compute needs.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基础设施需要支持各种具有不同计算需求的项目*。有些需要单个实例上的大量内存，有些需要许多小型实例，还有些需要像GPU这样的专用硬件。没有一种适合所有计算需求的一劳永逸的方法。'
- en: '*A single project or a workflow has varying needs for computation*. Data processing
    steps may be IO-intensive, possibly requiring lots of memory. Model training may
    require specialized hardware. Small coordination steps should execute quickly.
    Although technically, one could run the whole workflow on the largest possible
    instances, in many cases, it would be cost prohibitive. It is better to give the
    user an option to *adjust the resource requirements for each step individually*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*单个项目或工作流程在计算方面的需求是变化的*。数据处理步骤可能是I/O密集型的，可能需要大量的内存。模型训练可能需要专用硬件。小的协调步骤应该快速执行。虽然技术上可以在最大的实例上运行整个工作流程，但在许多情况下，这可能会成本过高。更好的做法是给用户一个选项，可以*单独调整每个步骤的资源需求*。'
- en: '*The needs of a project vary over its lifetime*. It is convenient to prototype
    the first version quickly using local processes. After this, you should be able
    to test the workflow at scale using more compute resources. Finally, the production
    version should be both robust and scalable. You are ready to make different kinds
    of tradeoffs when it comes to latency, scalability, and reliability at different
    points in the project’s life cycle.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*项目在其生命周期中的需求是变化的*。使用本地进程快速原型化第一个版本是方便的。在此之后，您应该能够使用更多的计算资源来测试工作流程。最后，生产版本应该是既健壮又可扩展的。在项目的生命周期中，您可以根据延迟、可扩展性和可靠性在不同阶段做出不同的权衡。'
- en: Next, we show how you can set up infrastructure that works this way using local
    processes and AWS Batch.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示如何使用本地进程和AWS Batch设置这种工作方式的基础设施。
- en: 4.3.1 Configuring AWS Batch for Metaflow
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 为Metaflow配置AWS Batch
- en: AWS Batch provides a convenient abstraction for use cases that need to execute
    units of computation—jobs—to completion without any user intervention. Under the
    hood, AWS Batch is a relatively simple job queue that offloads the management
    of compute resources to other AWS services.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Batch为需要执行计算单元（作业）到完成且无需用户干预的使用场景提供了一个方便的抽象。在底层，AWS Batch是一个相对简单的作业队列，将计算资源的管理工作卸载给其他AWS服务。
- en: You can find step-by-step installation instructions for AWS Batch in Metaflow’s
    online documentation (see the Administrator’s Guide to Metaflow at [metaflow.org](https://metaflow.org/)).
    You can use either the provided *CloudFormation* template that sets up everything
    for you with a click of a button, or you can follow the manual installation instructions
    if you want to set it up by yourself, possibly customizing the setup as you go.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Metaflow的在线文档中找到AWS Batch的逐步安装说明（请参阅[metaflow.org](https://metaflow.org/)上的Metaflow管理员指南）。您可以使用提供的*CloudFormation*模板，只需点击一下按钮即可为您设置一切，或者如果您想自己设置，可能还会在设置过程中进行自定义，可以遵循手动安装说明。
- en: After you have configured AWS Batch for Metaflow, data scientists can use it
    in a straightforward manner, as described in the next section, without having
    to worry about implementation details. However, it is beneficial for the operators
    of the system to understand the high-level architecture, which is depicted in
    figure 4.10.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在您为Metaflow配置了AWS Batch之后，数据科学家可以像下一节所描述的那样直接使用它，无需担心实现细节。然而，对于系统的操作者来说，了解高级架构是有益的，如图4.10所示。
- en: '![CH04_F10_Tuulos](../../OEBPS/Images/CH04_F10_Tuulos.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F10_Tuulos](../../OEBPS/Images/CH04_F10_Tuulos.png)'
- en: Figure 4.10 The architecture of AWS Batch
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 AWS Batch的架构
- en: 'Let’s start with the following four concepts, boldfaced in figure 4.10, which
    you see mentioned often in the documentation of AWS Batch:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下四个概念开始，这些概念在AWS Batch的文档中经常被提及，如图4.10所示加粗显示：
- en: '*Job definition*—Configures the execution environment for a job: CPU, memory,
    environment variables, and so on. Metaflow takes care of creating a suitable job
    definition for each step automatically, so you don’t have to worry about it.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*作业定义*——配置作业的执行环境：CPU、内存、环境变量等。Metaflow会自动为每个步骤创建合适的作业定义，因此您无需担心这一点。'
- en: '*Job*—A single unit of computation. Each job is executed as an independent
    con-tainer. Metaflow maps each Metaflow task to a single Batch job automatically.
    Hence, in the context of AWS Batch, we can talk about tasks and jobs interchangeably.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*作业*——单个计算单元。每个作业作为一个独立的容器执行。Metaflow会自动将每个Metaflow任务映射到单个Batch作业。因此，在AWS Batch的上下文中，我们可以互换地谈论任务和作业。'
- en: '*Job queue*—Jobs are sent to a job queue to wait for execution. A queue may
    have any number of tasks pending. You can set up multiple queues, for example,
    to distinguish between low-priority and high-priority jobs. The figure illustrates
    two queues: one with two jobs and another with one job.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*作业队列*—作业被发送到作业队列等待执行。一个队列可能有任何数量的待处理任务。你可以设置多个队列，例如，区分低优先级和高优先级作业。图示展示了两个队列：一个有两个作业，另一个有一个作业。'
- en: '*Compute environment*—A pool of compute resources where jobs get executed.
    AWS Batch can manage the compute environment for you, adding more compute resources
    to the environment when the queue gets longer, or you can manage the compute environment
    by yourself. Thanks to autoscaling compute environments, AWS Batch can be used
    as an elastically scaling compute layer. The figure illustrates two compute environments:
    one using EC2 instances and another one using Fargate. A detailed discussion follows
    later.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算环境*—一个执行作业的计算资源池。AWS Batch 可以为你管理计算环境，当队列变长时，向环境中添加更多计算资源，或者你可以自行管理计算环境。得益于自动扩展的计算环境，AWS
    Batch 可以用作弹性扩展的计算层。图示展示了两个计算环境：一个使用 EC2 实例，另一个使用 Fargate。更详细的讨论将在后面进行。'
- en: 'When you start a Metaflow run that uses AWS Batch, the execution proceeds as
    follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当你启动一个使用 AWS Batch 的 Metaflow 运行时，执行过程如下：
- en: Prior to starting any tasks, Metaflow makes sure that the correct job definitions
    are created on Batch.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始任何任务之前，Metaflow 确保在 Batch 上创建了正确的作业定义。
- en: Metaflow creates *a job package* that includes all Python code corresponding
    to the flow. The package is uploaded to the datastore in AWS S3 (more about job
    packages in chapter 7).
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow 创建一个包含与流程对应的全部 Python 代码的 *作业包*。该包被上传到 AWS S3 中的数据存储（关于作业包的更多信息请参阅第
    7 章）。
- en: Metaflow walks through the DAG. When it encounters a task that should be executed
    on Batch, it submits a job request to a job queue that has been configured in
    advance.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow 遍历 DAG。当它遇到应在 Batch 上执行的任务时，它会向预先配置的作业队列提交一个作业请求。
- en: If there aren’t enough compute resources in the compute environment and it hasn’t
    reached its maximum limit, Batch scales up the environment.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果计算环境中没有足够的计算资源，并且尚未达到其最大限制，Batch 会扩展环境。
- en: Once resources are available, Batch schedules a job for execution.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦资源可用，Batch 会为执行安排一个作业。
- en: The Metaflow task encapsulated in a Batch job is executed in a container.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含在 Batch 作业中的 Metaflow 任务在容器中执行。
- en: Metaflow polls the status of the task. Once Batch reports that the task has
    completed successfully, Metaflow continues executing subsequent tasks, going back
    to step 3, until the end step has completed.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow 检查任务的状况。一旦 Batch 报告任务已成功完成，Metaflow 继续执行后续任务，回到步骤 3，直到完成最后一步。
- en: Choosing the Compute Environment
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 选择计算环境
- en: If you let AWS Batch manage the compute environment for you, it will use a container
    management service provided by AWS like *Elastic Container Service* (ECS) to execute
    containers. Behind the scenes, ECS launches EC2 compute instances using a managed
    Auto Scaling group, which is a set of instances that can grow and shrink automatically
    on demand basis. These instances will show up in the EC2 console like any other
    instances in your account.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你让 AWS Batch 为你管理计算环境，它将使用 AWS 提供的容器管理服务，如 *弹性容器服务*（ECS）来执行容器。在幕后，ECS 使用一个托管自动扩展组启动
    EC2 计算实例，这是一个可以根据需求自动增长和缩小的实例集合。这些实例将像你账户中的任何其他实例一样出现在 EC2 控制台中。
- en: A benefit of using ECS is that you can use any EC2 instance types in your compute
    environment. You can choose a selection of instances with a high amount of memory,
    many CPU cores, or even multiple GPUs. ECS schedules the job on the most suitable
    instance that can accommodate its resource requirements.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ECS 的好处是，你可以在计算环境中使用任何 EC2 实例类型。你可以选择具有大量内存、许多 CPU 核心或甚至多个 GPU 的实例集合。ECS
    会根据其资源需求，在最适合的实例上安排作业。
- en: Alternatively, you can choose to use *AWS Fargate* as a compute environment.
    Fargate doesn’t use EC2 instances directly, so you won’t see any instances on
    your EC2 dashboard. Also, you can’t choose the instance types directly. Fargate
    finds a suitable instance for each job automatically based on its resource requirements.
    However, the range of supported resource requirements is more limited compared
    to ECS. The biggest benefit of Fargate over ECS is that jobs start quicker.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以选择使用 *AWS Fargate* 作为计算环境。Fargate 不直接使用 EC2 实例，因此您在 EC2 仪表板上看不到任何实例。此外，您也不能直接选择实例类型。Fargate
    会根据每个作业的资源需求自动找到合适的实例。然而，与 ECS 相比，支持的资源需求范围更为有限。Fargate 相比 ECS 的最大优势是作业启动更快。
- en: As a yet another alternative, you can manage your *own pool of EC2 instances*
    behind ECS. Though this route is more tedious, it allows the maximum degree of
    customizability. You can set up the instances however you want. This approach
    may be useful if you have special security or compliance requirements.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一种选择，您可以在 ECS 后面管理自己的 *EC2 实例池*。虽然这种方法更为繁琐，但它允许最大程度的可定制性。您可以按自己的意愿设置实例。如果您的安全或合规性要求特殊，这种方法可能很有用。
- en: From the point of view of a Metaflow task, the compute environment doesn’t make
    any difference. Once a suitable instance is found for the task, it is executed
    in a container using the same container image, regardless of the environment.
    As mentioned earlier in this chapter, the compute layer determines only *where*
    the task is executed.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Metaflow 任务的视角来看，计算环境并没有任何区别。一旦为任务找到合适的实例，它就会使用相同的容器镜像在容器中执行，无论环境如何。如本章前面所述，计算层仅决定任务执行的
    *位置*。
- en: 'Finally, here is the good news for cost-conscious companies: you don’t pay
    any premium for using the instances with AWS Batch. You pay only the per-second
    price of the EC2 instances of your choosing, which makes AWS Batch one of the
    most cost-effective compute layers. You can reduce costs even further by using
    *Spot Instances*, which are the same EC2 instances but come with a caveat that
    they may be interrupted at any point in time. This isn’t as bad as it sounds—Metaflow
    can retry interrupted jobs automatically using the @retry decorator (see section
    4.4). The main cost is an extra delay in execution time when interruptions occur.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于注重成本的公司来说，这是一个好消息：您使用 AWS Batch 的实例无需支付任何额外费用。您只需支付您选择的 EC2 实例的每秒费用，这使得
    AWS Batch 成为最具成本效益的计算层之一。您还可以通过使用 *Spot 实例* 进一步降低成本，这些实例与 EC2 实例相同，但有一个前提，即它们可能在任何时间点被中断。这并不像听起来那么糟糕——Metaflow
    可以使用 @retry 装饰器自动重试中断的作业（请参阅第 4.4 节）。主要成本是在发生中断时执行时间的额外延迟。
- en: Configuring the container
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 配置容器
- en: Whereas the compute environment determines how hardware like CPUs and memory
    are made available for jobs, the container settings determine the software environment
    for Metaflow tasks. Pay attention to the two settings described next.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算环境决定了如何为作业提供硬件资源，如 CPU 和内存，但容器设置决定了 Metaflow 任务的软件环境。请注意以下两个设置。
- en: First, you must configure the security profile, aka *an IAM role*, that determines
    what AWS resources Metaflow tasks are allowed to access. At the minimum, they
    need to be able to access an S3 bucket, which is used as the Metaflow datastore.
    If you use AWS Step Functions for job scheduling as described in chapter 6, you
    must allow access to a DynamoDB table as well. If you use the provided CloudFormation
    template, a suitable IAM role is created for you automatically.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您必须配置安全配置文件，即 *IAM 角色*，它决定了 Metaflow 任务允许访问哪些 AWS 资源。至少，它们需要能够访问用作 Metaflow
    数据存储的 S3 存储桶。如果您使用 AWS Step Functions 进行作业调度，如第 6 章所述，您还必须允许访问 DynamoDB 表。如果您使用提供的
    CloudFormation 模板，将为您自动创建合适的 IAM 角色。
- en: Second, optionally, you can configure *the default container image* that is
    used to execute tasks. The image determines what libraries are available for the
    task by default. For instance, if you have set up cloud-based workstations as
    described in chapter 2, you can use the same image for workstations and task execution
    (you’ll learn more about dependency management in chapter 6). If you don’t specify
    any image, Metaflow chooses a generic Python image.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，您可以配置用于执行任务的 *默认容器镜像*。该镜像决定了任务默认可用的库。例如，如果您已如第 2 章所述设置了基于云的工作站，您可以为工作站和任务执行使用相同的镜像（您将在第
    6 章中了解更多关于依赖关系管理的知识）。如果您未指定任何镜像，Metaflow 将选择一个通用的 Python 镜像。
- en: The first run with AWS Batch
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS Batch 的第一次运行
- en: To use AWS Batch with Metaflow, you need to complete the following steps. The
    steps are executed automatically by the provided CloudFormation template, but
    it is not hard to complete them manually. See Metaflow’s online documentation
    for detailed instructions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用AWS Batch与Metaflow，你需要完成以下步骤。这些步骤由提供的CloudFormation模板自动执行，但手动完成它们并不困难。请参阅Metaflow的在线文档以获取详细说明。
- en: 'First, start by installing and configuring awscli, a command-line tool for
    interacting with AWS, as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，按照以下步骤安装和配置awscli，这是一个用于与AWS交互的命令行工具：
- en: '[PRE3]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you don’t use the CloudFormation template but you want to configure AWS
    Batch manually, execute the following steps:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用CloudFormation模板，但想手动配置AWS Batch，请执行以下步骤：
- en: Initialize an S3 bucket for the Metaflow datastore.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Metaflow数据存储初始化一个S3存储桶。
- en: Set up a VPC network for the compute environment.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为计算环境设置一个VPC网络。
- en: Set up a Batch job queue.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个批处理作业队列。
- en: Set up a Batch compute environment.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个批处理计算环境。
- en: Set up an IAM role for containers.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为容器设置一个IAM角色。
- en: After you have either executed the CloudFormation template or the previous manual
    steps, run metaflow configure aws to configure the services for Metaflow. That’s
    it!
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在你执行了CloudFormation模板或之前的手动步骤之后，运行metaflow configure aws来配置Metaflow的服务。这就完成了！
- en: 'After you have completed these steps, let’s test that the integration works.
    Execute the following command, which runs process_demo.py from listing 4.2 using
    AWS Batch:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，让我们测试一下集成是否正常工作。执行以下命令，它使用AWS Batch运行列表4.2中的process_demo.py：
- en: '[PRE4]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The command should produce output that looks like this:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 命令应该生成如下所示的输出：
- en: '[PRE5]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The example omits Metaflow’s standard prefix on each line to save space. The
    long ID in square brackets is the AWS Batch job ID corresponding to the Metaflow
    task. You can use it to cross-reference Metaflow tasks and AWS Batch jobs visible
    in the AWS Console UI.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 示例省略了Metaflow每行的标准前缀以节省空间。方括号中的长ID是AWS Batch作业ID，对应于Metaflow任务。你可以用它来在AWS控制台UI中交叉引用可见的Metaflow任务和AWS
    Batch作业。
- en: 'The first four “Task is starting” lines indicate the state of the task in the
    Batch queue as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 前四条“任务正在启动”的行指示了任务在批处理队列中的状态如下：
- en: '*SUBMITTED*—The task is entering the queue.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*已提交*——任务正在进入队列。'
- en: '*RUNNABLE*—The task is pending in the queue, waiting for a suitable instance
    to become available.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可运行*——任务在队列中等待，等待合适的实例变得可用。'
- en: '*STARTING*—A suitable instance was found, and the task is starting on it.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开始*——找到了合适的实例，任务正在该实例上启动。'
- en: '*RUNNING*—The task is running on the instance.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正在运行*——任务正在实例上运行。'
- en: It is typical for tasks to stay in the RUNNABLE state for up to a few minutes
    as AWS Batch scales up the compute environment. If the compute environment has
    reached its maximum size, the task needs to wait for previous tasks to finish,
    which might take even longer.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 当AWS Batch扩展计算环境时，任务通常会保持在可运行状态长达几分钟。如果计算环境已达到其最大大小，任务需要等待之前任务完成，这可能会更长。
- en: The run should complete successfully after a few minutes. Its output should
    be similar to that of the local run. Although this first run doesn’t seem like
    much—the run using AWS Batch completes *slower* than the local run due to the
    overhead of launching tasks in the cloud—you now have a virtually unlimited amount
    of processing power at your disposal! We will put this capability into action
    in the next section and even more in the next chapter.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，运行应该成功完成。其输出应该与本地运行类似。尽管这次运行看起来并不起眼——使用AWS Batch的运行比本地运行慢，因为云中启动任务的开销——你现在几乎拥有无限的计算能力！我们将在下一节中利用这种能力，甚至在下一章中还会更多。
- en: Troubleshooting RUNNABLE tasks
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 排查可运行任务
- en: A common symptom of AWS Batch not working correctly is a task that seems to
    be stuck in the RUNNABLE state forever. A number of things may cause this, all
    related to the compute environment (CE).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Batch无法正常工作的一个常见症状是任务似乎永远卡在可运行状态。这可能是由于与计算环境（CE）相关的一系列原因造成的。
- en: 'If you are using an EC2-backed compute environment (not Fargate), you can check
    what instances have been created in the CE by logging on to the EC2 console and
    searching for the tag aws:autoscaling:groupName: followed by the CE name. Based
    on the returned list of instances, you can troubleshoot the issue as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用由EC2支持的计算环境（非Fargate），你可以通过登录到EC2控制台并搜索以aws:autoscaling:groupName:开头后跟CE名称的标签来检查在CE中创建了哪些实例。根据返回的实例列表，你可以按以下方式排查问题：
- en: '*No instances*—If no instances were returned, it is possible that your CE is
    unable to launch instances of the desired type. For instance, your AWS account
    may have reached its limit of EC2 instances. You may be able to see why there
    are no instances by checking the status of the Auto Scaling group named after
    the CTE.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*没有实例*—如果没有返回实例，可能是因为你的 CE 无法启动所需类型的实例。例如，你的 AWS 账户可能已经达到 EC2 实例的限制。你可能可以通过检查以
    CTE 命名的自动扩展组的状态来了解为什么没有实例。'
- en: '*Some instances but no other tasks running*—It is possible that your task requests
    resources, for example, memory or GPUs using the @resources decorator discussed
    later, can’t be fulfilled by the CE. In this case, the task will stay in the queue
    forever. You can kill the task (job) in the AWS Batch console.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一些实例但无其他任务运行*—可能你的任务请求的资源，例如，使用稍后讨论的 @resources 装饰器，无法由 CE 满足，例如内存或 GPU。在这种情况下，任务将永远停留在队列中。你可以在
    AWS Batch 控制台中终止任务（作业）。'
- en: '*Some instances and other running tasks*—Your cluster may be busy processing
    the other tasks. Wait for the other tasks to finish first.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一些实例和其他正在运行的任务*—你的集群可能正忙于处理其他任务。请先等待其他任务完成。'
- en: If the problem persists, you can contact the online support of Metaflow.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题仍然存在，你可以联系 Metaflow 的在线支持。
- en: 4.3.2 @batch and @resources decorators
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 @batch 和 @resources 装饰器
- en: Now that you have configured AWS Batch, you can choose to execute any run in
    the cloud just by using run --with batch. All features of Metaflow, such as artifacts,
    experiment tracking, parameters, and the Client API, work in exactly the same
    way as before when using AWS Batch as the compute layer.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经配置了 AWS Batch，你可以选择仅通过使用 run --with batch 在云中执行任何运行。Metaflow 的所有功能，如工件、实验跟踪、参数和客户端
    API，在作为计算层使用 AWS Batch 时与之前完全相同。
- en: 'As discussed at the beginning of this chapter, the main motivation for having
    a cloud-based compute layer is scalability: you can handle more demanding compute,
    more data, than what you can handle at a local workstation. Let’s test scalability
    in practice. The next code listing presents a flow that tries to allocate 8 GB
    of memory by creating a string of eight billion characters.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所述，拥有基于云的计算层的主要动机是可扩展性：你可以处理比在本地工作站上更多的计算和更多的数据。让我们在实践中测试可扩展性。下面的代码列表展示了一个尝试通过创建包含八十亿个字符的字符串来分配
    8 GB 内存的工作流程。
- en: Listing 4.3 A flow that uses a lot of memory
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 使用大量内存的工作流程
- en: '[PRE6]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Sets the length to eight billion characters. The underscores are added to
    aid readability.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将长度设置为八十亿个字符。下划线被添加以帮助阅读。
- en: ❷ Tries to allocate 8 GB of memory
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 尝试分配 8 GB 的内存
- en: 'Save the code in long_string.py. If you have at least 8 GB of memory available
    on your workstation, you can start by running the flow locally like so:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存在 long_string.py 中。如果你的工作站上至少有 8 GB 的内存可用，你可以通过以下方式在本地运行流程：
- en: '[PRE7]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The run may fail with a MemoryError if not enough memory is available. Next,
    let’s execute the flow on Batch as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有足够的内存，运行可能会因 MemoryError 而失败。接下来，让我们按照以下方式在 Batch 上执行流程：
- en: '[PRE8]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Metaflow executes the task on Batch with its default memory settings, which
    provide less than 8 GB of memory for the task. The task is likely to fail with
    a message like the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 使用默认的内存设置在 Batch 上执行任务，这些设置为任务提供的内存少于 8 GB。任务可能会失败，并显示如下消息：
- en: '[PRE9]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Although you can’t easily increase the amount of memory on your workstation
    like on a laptop, we can request more memory from our compute layer. Rerun the
    flow as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你无法像在笔记本电脑上那样轻松增加工作站上的内存量，但我们可以从我们的计算层请求更多的内存。按照以下方式重新运行流程：
- en: '[PRE10]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The memory=10000 attribute instructs Metaflow to request 10 GB of memory for
    every step of the flow. The unit for memory is megabytes, so 10,000 MB equals
    10 GB. Note that if your compute environment doesn’t provide instances with at
    least 10 GB of memory, the run will get stuck in the RUNNABLE state. Assuming
    suitable instances can be found, the run should complete successfully.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: memory=10000 属性指示 Metaflow 为流程的每个步骤请求 10 GB 的内存。内存的单位是兆字节，所以 10,000 MB 等于 10
    GB。请注意，如果你的计算环境不提供至少 10 GB 内存实例，运行将卡在 RUNNABLE 状态。假设可以找到合适的实例，运行应该可以成功完成。
- en: 'This is vertical scaling in action! We are able to request instances with specific
    hardware requirements simply on the command line. Besides memory, you can request
    a minimum number of CPU cores with the cpu attribute or even GPUs with the gpu
    attribute. For instance, the next command-line code provides every task with 8
    CPU cores and 8 GB of memory:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是垂直扩展的实际应用！我们只需在命令行中请求具有特定硬件要求的实例。除了内存外，您还可以使用cpu属性请求最小数量的CPU核心，甚至可以使用gpu属性请求GPU。例如，下面的命令行代码为每个任务提供了8个CPU核心和8GB的内存：
- en: '[PRE11]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Because data science workloads tend to be resource hungry, it is very convenient
    to be able to test code and scale workloads this easily. You can request any amount
    of memory that is supported by EC2 instances in your compute environment. As of
    the writing of this book, the largest instance has 768 GB of memory, so with a
    suitable compute environment, you can request up to --with batch:memory=760000,
    leaving 8 GB for the operating system on the instance.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据科学工作负载往往对资源需求很大，能够轻松地测试代码和扩展工作负载非常方便。您可以在计算环境中请求EC2实例支持的任何内存量。截至本书编写时，最大的实例有768GB的内存，因此，在合适的计算环境中，您可以请求高达--with
    batch:memory=760000的内存，为实例上的操作系统留下8GB。
- en: You can handle quite sizable datasets with this much memory. If you are worried
    about cost, consider that the function executes for less than a minute. Even if
    you executed the task on the largest and the most expensive instance, it would
    cost only about 10 cents, thanks to per-second billing. You could push the cost
    down even further by using spot instances in your compute environment as discussed
    earlier.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这么多内存处理相当大的数据集。如果您担心成本，请考虑函数执行时间不到一分钟。即使您在最大且最昂贵的实例上执行任务，成本也只有大约10美分，这得益于按秒计费。您可以通过在计算环境中使用之前讨论过的spot实例来进一步降低成本。
- en: Specifying resource requirements in the code
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中指定资源需求
- en: 'Let’s say you share long_string.py with a colleague. Following the previous
    approach, they would need to know the specific command-line code, run --with batch:
    memory=10000, to run the flow successfully. We know that the amount of memory
    is a strict requirement—the flow won’t succeed without at least 8 GB of memory—so
    we can annotate the requirement directly in the code by adding'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '假设您与同事共享long_string.py。按照之前的做法，他们需要知道特定的命令行代码，即run --with batch: memory=10000，才能成功运行流程。我们知道内存量是一个严格的要求——没有至少8GB的内存，流程将无法成功——因此我们可以在代码中直接添加要求，通过添加'
- en: '[PRE12]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: above @step for the start step. Remember to add from metaflow import batch at
    the top of the file, too.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 上述@step的起始步骤。请记住，在文件顶部添加from metaflow import batch。
- en: Now, your colleagues can run the flow with the run command without any extra
    options. As an additional benefit, only the start step annotated with the @batch
    decorator is executed on AWS Batch, and the end step, which doesn’t have any resource
    requirements, executes locally. This illustrates how you can use multiple compute
    layers rather seamlessly in a single workflow.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的同事可以使用运行命令来运行流程，而无需任何额外选项。作为额外的好处，只有用@batch装饰器注解的起始步骤在AWS Batch上执行，而无需任何资源要求的结束步骤则在本地上执行。这说明了您如何在单个工作流程中无缝地使用多个计算层。
- en: Note The --with option is shorthand for assigning a decorator, like batch, to
    every step on the fly. Hence run --with batch is equivalent to adding the @batch
    decorator manually to every step of the flow and executing run. Correspondingly,
    any attributes added after a colon, like batch:memory=10000, map directory to
    the arguments given to the decorator, like @batch(memory=10000).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：--with选项是分配装饰器（如batch）的快捷方式，就像在飞行中添加@batch装饰器一样。因此，run --with batch相当于手动将@batch装饰器添加到流程的每个步骤并执行run。相应地，在冒号之后添加的任何属性，如batch:memory=10000，将映射到装饰器提供的参数，如@batch(memory=10000)。
- en: 'Now imagine that you share a version of long_string.py that is annotated with
    @batch publicly. A stranger wants to execute the code, but their compute layer
    is Kubernetes, not AWS Batch. Technically, they should be able to execute the
    flow successfully on a 10 GB instance provided by Kubernetes. For situations like
    this, Metaflow provides another decorator, @resources, which lets you specify
    the resource requirements in a compute layer-agnostic manner. You could replace
    the @batch decorator with the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你公开了一个带有 @batch 注解的 long_string.py 版本。一个陌生人想要执行这段代码，但他们的计算层是 Kubernetes，而不是
    AWS Batch。技术上，他们应该能够在 Kubernetes 提供的 10 GB 实例上成功执行流程。对于这种情况，Metaflow 提供了另一个装饰器，@resources，它允许你以计算层无关的方式指定资源需求。你可以用以下方式替换
    @batch 装饰器：
- en: '[PRE13]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: However, in contrast to @batch, the @resources decorator doesn’t determine which
    compute layer is used. If you run the flow without options, it executes locally,
    and @resources has no effect. To run the flow with AWS Batch, you can use run
    --with batch without any attributes. The @batch decorator knows to pick the resource
    requirements from @resources. Correspondingly, the stranger could run the flow
    on their Kubernetes cluster using something akin to run --with kubernetes.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 @batch 相比，@resources 装饰器并不确定使用哪个计算层。如果你不带选项运行流程，它将在本地执行，并且 @resources 没有作用。要使用
    AWS Batch 运行流程，你可以使用 run --with batch 而不带任何属性。@batch 装饰器知道从 @resources 中选择资源需求。相应地，陌生人可以使用类似于
    run --with kubernetes 的方式在他们自己的 Kubernetes 集群上运行流程。
- en: It is considered a best practice to annotate steps that have high resource requirements
    with @resources. If the step code can’t succeed without a certain amount of memory
    or, say, a model training step won’t execute quickly enough without a certain
    number of CPU or GPU cores, you should make the requirement explicit in the code.
    In general, it is preferable to use @resources rather than @batch or other compute-layer-specific
    decorators when possible, so anyone running the flow can choose a suitable compute
    layer on the fly.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 将具有高资源需求的步骤用 @resources 注解被认为是最佳实践。如果步骤代码没有一定数量的内存就无法成功执行，或者，例如，没有一定数量的 CPU
    或 GPU 内核，模型训练步骤就无法快速执行，你应该在代码中明确要求。一般来说，当可能时，使用 @resources 而不是 @batch 或其他计算层特定的装饰器更可取，这样任何运行流程的人都可以动态选择合适的计算层。
- en: 'We will go through many more examples of scalability using @resources and AWS
    Batch in the next chapter. Before getting there, though, we will cover an inevitable
    fact of life: surprises happen, and things don’t always work as expected.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中通过使用 @resources 和 AWS Batch 的更多例子来探讨可扩展性。然而，在到达那里之前，我们将涵盖生活中不可避免的事实：意外会发生，事情并不总是按预期进行。
- en: 4.4 Handling failures
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 处理失败
- en: '*One day, the cloud-based compute environment at Caveman Cupcakes started behaving
    erratically. Alex’s tasks that had been running flawlessly for weeks started failing
    without a clear reason. Bowie noticed that the cloud provider’s status dashboard
    reported “increased error rates.” Alex and Bowie couldn’t do anything about the
    situation besides wait for the cloud to fix itself and trying to limit the impact
    to their production workflows.*'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '*有一天，Caveman Cupcakes 的基于云的计算环境开始出现异常行为。Alex 那些已经完美运行了几周的作业开始无缘无故地失败。Bowie
    注意到云提供商的状态仪表板报告了“错误率增加。”Alex 和 Bowie 除了等待云自行修复并尝试将影响限制在他们的生产工作流程中之外，别无他法。*'
- en: '![CH04_F10_UN04_Tuulos](../../OEBPS/Images/CH04_F10_UN04_Tuulos.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F10_UN04_Tuulos](../../OEBPS/Images/CH04_F10_UN04_Tuulos.png)'
- en: 'Alex’s and Bowie’s scenario is hardly hypothetical. Although the cloud provides
    a rather practical illusion of infinite scalability, it doesn’t always work flawlessly.
    Errors in the cloud tend to be stochastic in nature, so the higher the number
    of concurrent jobs, the more likely you will hit a random transient error. Because
    these errors are an inevitable fact of life, we should be prepared to handle them
    proactively. It is useful to distinguish between two types of failures, shown
    here:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Alex 和 Bowie 的场景几乎不是假设性的。尽管云提供了一种相当实用的无限可扩展性的错觉，但它并不总是完美无缺。云中的错误往往具有随机性，因此并发作业的数量越多，你遇到随机瞬时错误的可能性就越大。因为这些错误是生活中不可避免的事实，我们应该做好积极应对的准备。区分两种类型的失败是有用的，如下所示：
- en: '*Failures in the user code*—The user-written code in steps may contain bugs,
    or it may call other services that behave erroneously.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*用户代码中的失败*——步骤中的用户编写的代码可能包含错误，或者它可能调用其他表现错误的服务。'
- en: '*Platform errors*—The compute layer that executes the step code may fail for
    a number of reasons, such as hardware failures, networking failures, or inadvertent
    changes in configuration.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*平台错误*——执行步骤代码的计算层可能会因多种原因而失败，例如硬件故障、网络故障或配置的意外更改。'
- en: Failures that happen in the user code, such as failed database connections,
    can be typically handled inside the user code itself, which distinguishes the
    first category from the second. There’s nothing you can do in your Python code
    to recover from, say, the underlying container management system failing. Consider
    the example shown in the next code listing.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 发生在用户代码中的故障，如失败的数据库连接，通常可以在用户代码内部处理，这区分了第一类错误和第二类错误。在你的 Python 代码中，你无法从底层容器管理系统失败等情况中恢复。考虑下一个代码列表中展示的示例。
- en: Listing 4.4 Failing due to division by zero
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 由于除以零而失败
- en: '[PRE14]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ This will fail with ZeroDivisionError.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这将因 ZeroDivisionError 而失败。
- en: 'Save the flow in zerodiv.py and run it as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 将流程保存在 zerodiv.py 中，并按以下方式运行：
- en: '[PRE15]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The run will fail and throw an exception, ZeroDivisionError: division by zero.
    This is clearly a logical error in the user code—accidental division-by-zero errors
    are quite common in numerical algorithms. If we suspect that a block of code may
    fail, we can use Python’s standard exception-handling mechanisms to handle it.
    Fix the divide step as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '运行将失败并抛出异常，ZeroDivisionError: 除以零。这显然是用户代码中的逻辑错误——在数值算法中，意外的除以零错误相当常见。如果我们怀疑某段代码可能会失败，我们可以使用
    Python 的标准异常处理机制来处理它。按照以下方式修复除法步骤：'
- en: '[PRE16]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After the fix, the flow will run successfully. Following this pattern, it is
    advisable to try to handle as many exceptions in the step code as possible for
    the following reasons:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 修复后，流程将成功运行。遵循这个模式，建议尽可能在步骤代码中处理尽可能多的异常，以下是一些原因：
- en: If you consider possible error paths while writing the code, you can also implement
    the paths for error recovery, for example, what exactly should happen when ZeroDivisionError
    is raised. You can implement a “plan B” as a part of your logic, because only
    you know the right course of action in your particular application.
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你编写代码时考虑了可能的错误路径，你也可以实现错误恢复路径，例如，当 ZeroDivisionError 被抛出时，应该确切发生什么。你可以将“计划
    B”作为你逻辑的一部分实现，因为只有你知道在你特定的应用程序中正确的行动方案。
- en: It is faster to recover from errors in the user code. For instance, if you are
    calling an external service, such as a database, in your step, you can implement
    a retry logic (or rely on the database client’s built-in logic) that retries a
    failed connection without having to retry the whole Metaflow task, which incurs
    a much higher overhead.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从用户代码中的错误中恢复更快。例如，如果你在步骤中调用外部服务，如数据库，你可以实现重试逻辑（或依赖数据库客户端的内置逻辑），在不需要重试整个 Metaflow
    任务的情况下重试失败的连接，这会带来更高的开销。
- en: Even if you follow this advice, tasks may still fail. They may fail because
    you didn’t consider some unforeseen error scenario, or they may fail due to a
    platform error, say, a data center may catch fire. Metaflow provides an additional
    layer of error handling that can help in these scenarios.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你遵循这些建议，任务仍然可能会失败。它们可能因为未考虑到的错误场景而失败，或者可能因为平台错误而失败，例如，数据中心可能发生火灾。Metaflow
    提供了额外的错误处理层，可以帮助处理这些场景。
- en: 4.4.1 Recovering from transient errors with @retry
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 使用 @retry 从短暂错误中恢复
- en: 'The flow presented in listing 4.4 fails predictably every time it is executed.
    Most platform errors and some errors in the user code behave more randomly—for
    instance, AWS Batch may schedule a task to be executed on an instance with failing
    hardware. The compute environment will eventually detect the hardware failure
    and decommission the instance, but this may take a few minutes. The best course
    of action is to retry the task automatically. There’s a good chance that it won’t
    hit the same transient error again. The following listing simulates an unlucky
    transient error: it fails every other second.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 中展示的流程每次执行都会可预测地失败。大多数平台错误和一些用户代码中的错误行为更为随机——例如，AWS Batch 可能会调度一个任务在硬件故障的实例上执行。计算环境最终会检测到硬件故障并退役该实例，但这可能需要几分钟。最好的做法是自动重试任务。有很大可能性它不会再次遇到相同的短暂错误。以下列表模拟了一个不幸的短暂错误：每隔一秒就会失败一次。
- en: Listing 4.5 Retry decorator
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 重试装饰器
- en: '[PRE17]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The conditional is true, depending on the second when the line is executed.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 条件为真，取决于执行该行的第二个时刻。
- en: 'Save the flow in retryflow.py and run it as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 将流程保存在 retryflow.py 中，并按以下方式运行：
- en: '[PRE18]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You should see a bunch of exceptions printed out when the execution hits the
    “Bad luck” branch. Thanks to the @retry flag, any failure in the start step causes
    it to be retried automatically. It is likely that the execution will eventually
    succeed. You can rerun the flow a few times to see the effect.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行遇到“Bad luck”分支时，你应该会看到一堆异常被打印出来。多亏了 @retry 标志，任何在起始步骤中的失败都会导致它自动重试。执行最终成功的可能性很大。你可以多次重新运行流程以查看效果。
- en: A key feature of @retry is that it also handles platform errors. For instance,
    if a container fails on AWS Batch for any reason, including the data center catching
    fire, the @retry decorator will cause the task to be retried. Thanks to the sophistication
    of the cloud, there’s a good chance that a retried task will be rerouted to a
    nonburning data center and will succeed eventually.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '@retry 的一个关键特性是它还处理平台错误。例如，如果容器在 AWS Batch 中由于任何原因失败，包括数据中心着火，@retry 装饰器将导致任务重试。多亏了云的复杂性，重试的任务有很大机会被重新路由到一个非燃烧的数据中心，并最终成功。'
- en: Note that when you start a run on your workstation, the run will succeed only
    if the workstation stays alive for the whole duration of the execution. The retrying
    mechanism of the @retry decorator is implemented by the DAG scheduler, which in
    the case of local runs is the built-in scheduler of Metaflow. If the scheduler
    itself dies, it will take all executions down with it, which is not desirable
    for business-critical production runs. Addressing this shortcoming is a key topic
    of chapter 6, which focuses on production deployments. We will learn how we can
    make the scheduling itself tolerant against platform errors.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你开始在工作站上启动运行时，只有当工作站在整个执行过程中保持存活时，运行才会成功。@retry 装饰器的重试机制是由 DAG 调度器实现的，在本地运行的情况下是
    Metaflow 的内置调度器。如果调度器本身死亡，它将把所有执行都带走，这对于业务关键的生产运行来说是不理想的。解决这个缺点是第 6 章的关键主题，该章专注于生产部署。我们将学习如何使调度本身能够容忍平台错误。
- en: Escaping a burning data center
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 逃离燃烧的数据中心
- en: How can a task succeed even if a data center is on fire? AWS has the concept
    of Availability Zones (AZ), which are physically-distanced data centers, limiting
    the impact radius of any real-world disaster. In the case of AWS Batch, a compute
    environment may launch instances on multiple AZs transparently, so when instances
    become unavailable in one AZ, instances in another AZ can take over.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 即使数据中心着火，任务如何还能成功？AWS 有一个概念叫做可用区（AZ），它们是物理上分离的数据中心，限制了任何现实世界灾难的影响范围。在 AWS Batch
    的情况下，计算环境可以透明地在多个 AZ 上启动实例，所以当一个 AZ 中的实例不可用时，另一个 AZ 中的实例可以接管。
- en: Avoiding retries selectively
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性地避免重试
- en: You may wonder why the user has to worry about the @retry decorator—couldn’t
    Metaflow retry all tasks automatically? A challenge is that steps may have side
    effects. Imagine a step that increments a value in a database, for instance, the
    balance of a bank account. If the step crashes after the increment operation and
    was retried automatically, the bank account would get credited twice.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会 wonder 为什么用户需要担心 @retry 装饰器——Metaflow 不能自动重试所有任务吗？一个挑战是步骤可能有副作用。想象一下一个步骤，比如在数据库中增加一个值，例如，银行账户的余额。如果该步骤在增加操作后崩溃并被自动重试，银行账户就会被借记两次。
- en: If you have a step like this that shouldn’t be retried, you can annotate it
    with a decorator @retry(times=0). Now, anyone can run the flow simply by executing
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个不应该重试的步骤，你可以用装饰器 @retry(times=0) 来注释它。现在，任何人都可以通过简单地执行
- en: '[PRE19]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: which will add a @retry decorator to every step, but the step with @retry(times=0)
    will be retried zero times. You can also use the times attribute to adjust the
    number of retries to be higher than the default. In addition, you can specify
    another attribute, minutes_between_retries, which tells the scheduler to wait
    for the given number of minutes between retries.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为每个步骤添加一个 @retry 装饰器，但带有 @retry(times=0) 的步骤将不会重试。你还可以使用 times 属性来调整重试次数，使其高于默认值。此外，你可以指定另一个属性，minutes_between_retries，它告诉调度器在重试之间等待指定数量的分钟数。
- en: Recommendation Whenever you run a flow in the cloud, such as when using AWS
    Batch, it is a good idea to run it --with retry, which takes care of transient
    errors automatically. If your code shouldn’t be retried, annotate it with @retry(times=0).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：无论何时你在云中运行流程，例如使用 AWS Batch，运行它时带上 --with retry 是一个好主意，它会自动处理瞬态错误。如果你的代码不应该重试，请用
    @retry(times=0) 来注释它。
- en: 4.4.2 Killing zombies with @timeout
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 使用 @timeout 杀死僵尸
- en: Not all errors manifest themselves as exceptions or crashes. A particularly
    annoying class of errors causes tasks to get stuck, blocking the execution of
    a workflow. In machine learning, this situation can happen with numerical optimization
    algorithms that converge very slowly with a particular dataset. Or, you may call
    an external service that never returns a proper response, causing the function
    call to block forever.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有错误都表现为异常或崩溃。一类特别令人烦恼的错误会导致任务卡住，阻止工作流程的执行。在机器学习中，这种情况可能发生在与特定数据集缓慢收敛的数值优化算法中。或者，你可能调用一个永远不会返回正确响应的外部服务，导致函数调用永远阻塞。
- en: You can use the @timeout decorator to limit the total execution time of a task.
    The next listing simulates a task that takes too long to complete occasionally.
    When that happens, the task is interrupted and retried.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用@timeout装饰器来限制任务的总体执行时间。下面的列表模拟了一个偶尔需要很长时间才能完成的任务。当这种情况发生时，任务会被中断并重试。
- en: Listing 4.6 Timeout decorator
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 超时装饰器
- en: '[PRE20]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ The task will time out after 5 seconds.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 任务将在5秒后超时。
- en: ❷ This block of code takes from 0-9 seconds to execute.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这段代码的执行时间从0到9秒不等。
- en: 'Save the flow in timeoutflow.py and run it as follows:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 将流程保存在timeoutflow.py中，并按以下方式运行：
- en: '[PRE21]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If you get lucky, the run may succeed on the first try. You can try again to
    see @timeout and @retry in action. The start task is interrupted after 5 seconds.
    When this happens, @retry takes care of retrying the step. Without @retry, the
    run would crash when a timeout occurs. Besides seconds, you can set the timeout
    value as minutes or hours or a combination of thereof.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运气好，运行可能第一次尝试就成功。你可以再次尝试，看看@timeout和@retry是如何工作的。启动任务在5秒后中断。当这种情况发生时，@retry会负责重试该步骤。如果没有@retry，当发生超时时，运行会崩溃。除了秒数，你还可以将超时值设置为分钟或小时，或者它们的组合。
- en: '4.4.3 The decorator of last resort: @catch'
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 最后的手段装饰器：@catch
- en: Machine learning workflows can power business-critical systems and products
    used by millions of people. In critical production settings like this, the infrastructure
    should ensure that workflows degrade gracefully in the presence of errors. In
    other words, even when errors happen, they shouldn’t cause the whole workflow
    to fail.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流程可以为数百万人的业务关键系统和产品提供动力。在像这样的关键生产环境中，基础设施应确保在出现错误的情况下，工作流程能够优雅地降级。换句话说，即使发生错误，也不应该导致整个工作流程失败。
- en: Let’s say you have a step in a workflow that connects to a database to retrieve
    fresh data used to update a model. One day, the database is down, and the connection
    fails. Hopefully, your step has the @retry decorator, so the task is retried a
    few times. What if the database outage persists over all retries? When the maximum
    number of retries has been reached, the workflow crashes. Not great.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在工作流程中的一个步骤中连接到数据库以检索用于更新模型的最新数据。有一天，数据库宕机，连接失败。希望你的步骤有@retry装饰器，所以任务会重试几次。如果数据库中断持续到所有重试，会怎样？当达到最大重试次数时，工作流程会崩溃。这可不是什么好事。
- en: 'Or consider another real-life scenario: a workflow trains a separate model
    for each country of the world using a 200-way foreach. The input dataset contains
    a daily batch of events by country. One day, the model training step fails for
    Andorra, because there were no new events produced for the small country. Naturally,
    the data scientist should have included quality checks for data before it is fed
    into the model, but this issue never occurred during testing, so it is an understandable
    human error. Also in this case, the whole workflow failed, leading to a few hours
    of frantic troubleshooting.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 或者考虑另一个现实场景：一个工作流程使用200个foreach为世界上每个国家训练一个单独的模型。输入数据集包含按国家划分的每日事件批次。有一天，由于安道尔这个小国没有产生新事件，模型训练步骤失败了。当然，数据科学家应该在数据被输入模型之前进行质量检查，但这个问题在测试期间从未发生，所以这是一个可以理解的错误。在这种情况下，整个工作流程也失败了，导致几小时的疯狂故障排除。
- en: Metaflow provides a decorator of last resort, @catch, which is executed after
    all retries have been exhausted. The @catch decorator swallows all errors produced
    by the task, allowing execution to continue, even if the task failed to produce
    anything useful. Crucially, @catch allows the creation of an indicator artifact,
    so subsequent steps can handle failed tasks gracefully.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow提供了一个最后的手段装饰器，@catch，它在所有重试耗尽后执行。@catch装饰器吞没了任务产生的所有错误，即使任务未能产生任何有用的结果，也允许执行继续。关键的是，@catch允许创建一个指示性工件，这样后续步骤可以优雅地处理失败的任务。
- en: 'Let’s apply the @catch decorator to our previous example, DivideByZeroFlow
    from listing 4.4\. Let’s call the new version, shown here, CatchDivideByZeroFlow.
    Structurally, this example is similar to the Andorra example: it has a foreach
    with a faulty task that shouldn’t cause the whole workflow to fail.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 @catch 装饰器应用到我们之前的示例中，即列表 4.4 中的 DivideByZeroFlow。让我们称新的版本为 CatchDivideByZeroFlow。从结构上看，这个例子与安道尔例子相似：它有一个带有故障任务的
    foreach，这个任务不应该导致整个工作流程失败。
- en: Listing 4.7 Demonstrating the @catch decorator
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.7 展示了 @catch 装饰器
- en: '[PRE22]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Creates an indicator artifact, divide_failed, which is set to True if the
    task fails
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个指标工件，divide_failed，如果任务失败则设置为 True
- en: ❷ Retrying is futile in this case.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在这种情况下，重试是徒劳的。
- en: 'Save the flow in catchflow.py and run it as follows:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 将流程保存在 catchflow.py 中，并按以下方式运行：
- en: '[PRE23]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Notice that one of the divide tasks fails and is retried, and finally @catch
    takes over and prints an error message about ZeroDivisionError. Crucially, @catch
    allows the execution to continue. It creates an artifact, divide_failed=True,
    for the task that failed—you can name the artifact freely. The subsequent join
    step uses this artifact to include results only from tasks that succeeded. If
    you are curious, you can run the flow with AWS Batch as follows:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到其中一个除法任务失败并重试，最后 @catch 接管并打印关于 ZeroDivisionError 的错误消息。关键的是，@catch 允许执行继续。它为失败的任务创建了一个工件，divide_failed=True，你可以自由命名这个工件。后续的
    join 步骤使用这个工件只包括成功任务的输出。如果你好奇，你可以按照以下方式使用 AWS Batch 运行流程：
- en: '[PRE24]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can see that the decorators will work the same way, regardless of the compute
    layer.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，装饰器将以相同的方式工作，无论计算层如何。
- en: Use @catch to annotate complex steps, such as model training or database access,
    which may fail in unforeseeable ways but whose failure shouldn’t crash the whole
    workflow. Just make sure that you handle missing results gracefully in the subsequent
    steps.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 @catch 注解可能以不可预见的方式失败但不应导致整个工作流程崩溃的复杂步骤，例如模型训练或数据库访问。只需确保你在后续步骤中优雅地处理缺失的结果。
- en: 'Summary: Hardening a workflow gradually'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要：逐步加固工作流程
- en: 'This section introduced four mechanisms for dealing with failures proactively:
    Python’s standard try-except construct, @retry, @timeout, and @catch. These decorators
    are optional, acknowledging the fact that handling failures isn’t the first priority
    when prototyping a new project. However, as the project matures, you can use the
    decorators to harden your workflow and make it more production-ready. You can
    gradually harden your workflows in this order:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了四种主动处理失败的方法：Python 的标准 try-except 构造、@retry、@timeout 和 @catch。这些装饰器是可选的，承认处理失败并不是在原型设计新项目时的首要任务。然而，随着项目的成熟，你可以使用装饰器来加固你的工作流程，使其更适合生产环境。你可以按以下顺序逐步加固你的工作流程：
- en: Use try-except blocks in your code to handle obvious exceptions. For instance,
    you can wrap any data processing in try-except because the data may evolve in
    surprising ways. Also, it is a good idea to wrap any calls to external services
    like databases and possibly include use case-specific retrying logic.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的代码中使用 try-except 块来处理明显的异常。例如，你可以将任何数据处理包裹在 try-except 中，因为数据可能会以令人惊讶的方式演变。此外，将任何对数据库等外部服务的调用包裹在
    try-except 中，并可能包括特定用例的重试逻辑是一个好主意。
- en: Use @retry to handle any transient platform errors, that is, any random issues
    happening in the cloud. In particular, using @retry is crucial with workflows
    that launch many tasks, such as using foreach. You can execute any flow robustly
    in the cloud simply by using run --with batch --with retry. For extra safety,
    you can make @retry more patient—for example, @retry(times=4, minutes_ between_retries=20)
    gives the task over one hour to succeed.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 @retry 处理任何瞬态平台错误，即云中发生的任何随机问题。特别是，对于启动许多任务的工作流程，如使用 foreach，使用 @retry 是至关重要的。你只需使用
    run --with batch --with retry 就可以在云中可靠地执行任何流程。为了额外的安全性，你可以使 @retry 更有耐心——例如，@retry(times=4,
    minutes_between_retries=20) 给任务超过一个小时的失败时间。
- en: Use @timeout to annotate steps that may get stuck or that may execute for an
    arbitrarily long time.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 @timeout 注解可能卡住或可能执行任意长时间步骤。
- en: Use @catch to prevent complex steps, for example, model training or data processing,
    from crashing the whole workflow. Remember to check the indicator artifact created
    by @catch in subsequent steps to account for missing results.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 @catch 防止复杂步骤，例如模型训练或数据处理，导致整个工作流程崩溃。记住在后续步骤中检查由 @catch 创建的指标工件，以考虑缺失的结果。
- en: Summary
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Effective infrastructure helps data science scale at many levels. It allows
    you to handle more people, more projects, more workflows, more compute, and more
    data.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的基础设施有助于数据科学在多个层面进行扩展。它允许您处理更多的人员、更多项目、更多工作流程、更多计算和更多数据。
- en: Versioning and isolation helps to scale people and the number of projects, because
    less coordination is required. A cloud-based compute layer allows data scientists
    to scale the compute resources, allowing them to handle more demanding models
    and more data.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制和隔离有助于扩展人员和项目数量，因为所需的协调较少。基于云的计算层允许数据科学家扩展计算资源，使他们能够处理更复杂模型和更多数据。
- en: Use a cloud-based compute layer to handle more tasks (horizontal scalability)
    and larger tasks (vertical scalability) than what a single workstation can handle.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于云的计算层可以处理比单个工作站更多的任务（横向扩展）和更大的任务（纵向扩展）。
- en: Leverage existing container management systems to execute isolated units of
    batch computation in the cloud.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用现有的容器管理系统在云中执行隔离的批量计算单元。
- en: The infrastructure can support a selection of compute layers, each optimized
    for a specific set of workloads.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施可以支持多种计算层，每一层都针对特定的工作负载进行了优化。
- en: An easy way to get started with cloud-based compute is to use AWS Batch with
    Metaflow.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要开始使用基于云的计算，一个简单的方法是使用AWS Batch与Metaflow结合。
- en: Use the @resources decorator to annotate the resource requirements for each
    step.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用@resources装饰器来注释每一步的资源需求。
- en: 'Metaflow provides three decorators that allow you to gradually harden your
    workflows against failures: @retry, @catch, and @timeout.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaflow提供了三个装饰器，允许您逐步增强工作流程以应对失败：@retry、@catch和@timeout。
