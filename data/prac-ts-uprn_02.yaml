- en: Chapter 2\. Finding and Wrangling Time Series Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。找到和整理时间序列数据。
- en: In this chapter we discuss problems that might arise while you are preprocessing
    time series data. Some of these problems will be familiar to experienced data
    analysts, but there are specific difficulties posed by timestamps. As with any
    data analysis task, cleaning and properly processing data is often the most important
    step of a timestamp pipeline. Fancy techniques can’t fix messy data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了在预处理时间序列数据时可能出现的问题。其中一些问题对经验丰富的数据分析师来说可能很熟悉，但时间戳带来了特定的困难。与任何数据分析任务一样，清理和正确处理数据通常是时间戳流水线中最重要的步骤。花哨的技术不能修复混乱的数据。
- en: Most data analysts will need to find, align, scrub, and smooth their own data
    either to learn time series analysis or to do meaningful work in their organizations.
    As you prepare data, you’ll need to do a variety of tasks, from joining disparate
    columns to resampling irregular or missing data to aligning time series with different
    time axes. This chapter helps you along the path to an interesting and properly
    prepared time series data set.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据分析师需要找到、对齐、清理和平滑他们自己的数据，无论是为了学习时间序列分析还是为了在其组织中做有意义的工作。在准备数据时，你需要完成各种任务，从合并不同列到重新采样不规则或缺失的数据，再到将时间序列与不同时间轴对齐。本章将帮助你走向一个有趣且适当准备的时间序列数据集。
- en: 'We discuss the following skills useful for finding and cleaning up time series
    data:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了以下对于查找和清理时间序列数据有用的技能：
- en: Finding time series data from online repositories
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从在线存储库中找到时间序列数据。
- en: Discovering and preparing time series data from sources not originally intended
    for time series
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现和准备不原本用于时间序列的数据来源的时间序列数据。
- en: Addressing common conundrums you will encounter with time series data, especially
    the difficulties that arise from timestamps
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理时间序列数据时你会遇到的常见难题，特别是由于时间戳而引起的困难。
- en: After reading this chapter, you will have the skills needed to identify and
    prepare interesting sources of time series data for downstream analysis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你将掌握识别和准备有趣的时间序列数据源进行下游分析所需的技能。
- en: Where to Find Time Series Data
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何找到时间序列数据。
- en: 'If you are interested in where to find time series data and how to clean it,
    the best resource for you in this chapter depends on which of these is your main
    goal:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对如何找到时间序列数据和如何清理它感兴趣，那么在这一章中对你最有帮助的资源取决于这两者哪个是你的主要目标：
- en: Finding an appropriate data set for learning or experimentation purposes
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到适合学习或实验目的的合适数据集。
- en: Creating a time series data set out of existing data that is not stored in an
    explicitly time-oriented form
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从不明确存储在面向时间的形式中的现有数据中创建一个时间序列数据集。
- en: In the first case, you should find existing data sets with known benchmarks
    so you can see whether you are doing your analysis correctly. These are most often
    found as contest data sets (such as Kaggle) or repository data sets. In these
    cases, you will likely need to clean your data for a specific purpose even if
    some preliminary work has been done for you.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，你应该找到已知基准的现有数据集，以便确定你是否正确地进行了分析。这些通常作为竞赛数据集（如Kaggle）或存储库数据集来找到。在这些情况下，即使已经为你做了一些初步工作，你可能也需要为特定目的清理数据。
- en: In the second case, you should think about effective ways to identify interesting
    timestamped data, turn it into a series, clean it, and align it with other timestamped
    data to make interesting time series data. I will refer to these found in the
    wild data sets as *found time series* (this is my own term and not technical language).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，你应该考虑有效的方法来识别有趣的时间戳数据，将其转换为系列，清理它，并将其与其他时间戳数据对齐，以制作有趣的时间序列数据。我将这些在野数据集称为*发现的时间序列*（这是我自己的术语，不是技术语言）。
- en: We discuss both prepared data sets and found time series next.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们讨论准备好的数据集和发现的时间序列。
- en: Prepared Data Sets
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好的数据集。
- en: The best way to learn an analytical or modeling technique is to run through
    it on a variety of data sets and see both how to apply it and whether it helps
    you reach a concrete goal. In such cases it is helpful to have prepared options.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 学习分析或建模技术的最佳方式是在各种数据集上运行它，并看看如何应用它以及它是否帮助你达到具体目标。在这种情况下，准备好的选项是很有帮助的。
- en: While time series data is everywhere, it is not always easy to find the kind
    of data you want when you want it. If you often find yourself in this position,
    you will want to get familiar with some commonly used time series data repositories,
    so we’ll discuss a few options you should consider.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管时间序列数据随处可见，但并非总是能够在需要时找到想要的数据类型。如果您经常处于这种位置，您可能希望熟悉一些常用的时间序列数据存储库，因此我们将讨论您应该考虑的几个选项。
- en: The UCI Machine Learning Repository
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: UCI机器学习库
- en: The [UCI Machine Learning Repository](https://perma.cc/M3XC-M9HU) (see [Figure 2-1](#fig-0201))
    contains around 80 time series data sets, ranging from hourly air quality samples
    in an Italian city to Amazon file access logs to diabetes patients’ records of
    activity, food, and blood glucose information. These are very different kinds
    of data, and a look at the files shows they reflect distinct ways of tracking
    information across time, yet each is a time series.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[UCI机器学习库](https://perma.cc/M3XC-M9HU)（见[图 2-1](#fig-0201)）包含大约80个时间序列数据集，从意大利城市的每小时空气质量样本到亚马逊文件访问日志再到糖尿病患者的活动、食物和血糖信息记录。这些都是非常不同类型的数据，浏览文件显示它们反映了跨时间跟踪信息的不同方式，但每个都是时间序列。'
- en: '![](assets/ptsa_0201.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0201.png)'
- en: Figure 2-1\. The UCI Machine Learning Repository includes an annotated list
    of time series data sets.
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. UCI机器学习库包含时间序列数据集的带注释列表。
- en: Consider the [very first data set](https://perma.cc/8E7D-ESGM) listed under
    the Time Series section in the UCI repository, which is a data set about absenteeism
    at work (see [Figure 2-2](#fig-0202)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑UCI存储库中时间序列部分下列出的[第一个数据集](https://perma.cc/8E7D-ESGM)，这是关于工作缺勤的数据集（见[图 2-2](#fig-0202)）。
- en: A quick look at the data reveals that the time columns are limited to “Month
    of absence,” “Day of the week,” and “Seasons,” with no column for year. There
    are duplicate time indices, but there is also a column indicating employee identity
    so that we can differentiate between these duplicate time points. Finally, there
    are various employee attribute columns.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 快速查看数据显示，时间列限制为“缺勤月份”、“星期几”和“季节”，没有年份列。存在重复的时间索引，但还有一列指示员工身份，以便我们可以区分这些重复的时间点。最后，还有各种员工属性列。
- en: '![](assets/ptsa_0202.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0202.png)'
- en: Figure 2-2\. The absenteeism at work data set is the first in the list of time
    series data sets in the UCI Machine Learning Repository.
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 工作缺勤数据集是UCI机器学习库时间序列数据集列表中的第一个。
- en: This data set could be quite challenging to process, because you would first
    need to determine whether the data was all from one year or whether the cycling
    of months from 1 to 12 several times through the row progressions indicates that
    the year is changing. You would also need to decide whether to look at the problem
    in the aggregate, from a total absenteeism per unit time perspective, or to look
    at absenteeism per ID for those reported in the data set (see [Figure 2-3](#fig-02new03)).
    In the first case you would have a single time series, whereas in the latter case
    you would have multiple time series with overlapping timestamps. How you looked
    at the data would depend on what question you wanted to answer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集可能非常具有挑战性，因为您首先需要确定数据是否全来自一年，或者行进展中1到12个月的循环是否表明年份在变化。您还需要决定是从整体上看问题，从单位时间的缺勤率角度看数据，还是查看数据集中报告的每个ID的缺勤情况（见[图 2-3](#fig-02new03)）。在第一种情况下，您将拥有一个单一的时间序列，而在后一种情况下，您将拥有多个具有重叠时间戳的时间序列。您如何查看数据将取决于您想要回答的问题。
- en: '![](assets/ptsa_0203.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0203.png)'
- en: Figure 2-3\. The first few lines of one file in the Australian sign language
    data set. As you can see, this is a wide file format.
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 澳大利亚手语数据集中一个文件的前几行。正如您所看到的，这是一种宽文件格式。
- en: Contrast the absenteeism data set with another data set early on in the list,
    the [Australian Sign Language signs data set](https://perma.cc/TC5E-Z6H4), which
    includes recordings from a Nintendo PowerGlove as subjects signed in Australian
    Sign Language. The data is formatted as wide CSV files, each within a folder indicating
    the individual measured and with a filename indicating the sign.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将缺勤数据集与列表中早期的另一个数据集进行对比，即[澳大利亚手语标志数据集](https://perma.cc/TC5E-Z6H4)，其中包括使用Nintendo
    PowerGlove录制的主题使用澳大利亚手语的录音。数据格式为宽CSV文件，每个文件夹表示个体测量，并且文件名指示手语标志。
- en: In this data set, the columns are unlabeled and do not have a timestamp. This
    is nonetheless time series data; the time axis counts the time steps forward,
    regardless of when the actual events happened. Notice that for the purpose of
    thinking about signs as time series, it doesn’t matter what the unit of time is;
    the point is the sequencing rather than the exact time. In that case, all you
    would care about is the ordering of the event, and whether you could assume or
    confirm from reading the data description that the measurements were taken at
    regular intervals.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，列没有标签，也没有时间戳。尽管如此，这是时间序列数据；时间轴计算时间步长的前进，而不管实际事件发生的时间。注意，对于将符号视为时间序列的目的来说，时间单位并不重要；重要的是事件的顺序，以及您是否可以从数据描述中假设或确认测量是按照规律间隔进行的。
- en: 'As we can see from inspecting these two data sets, you will run into all sorts
    of data munging challenges. Some of the problems we’ve already noticed are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从检查这两个数据集可以看出，您将遇到各种数据处理挑战。我们已经注意到的一些问题有：
- en: Incomplete timestamps
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不完整的时间戳
- en: Time axes can be horizontal or vertical in your data
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间轴可以是数据中的水平或垂直轴
- en: Varying notions of time
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间概念的变化
- en: The UEA and UCR Time Series Classification Repository
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: UEA和UCR时间序列分类库
- en: The [UEA and UCR Time Series Classification Repository](https://perma.cc/56Q5-YPNT)
    is a newer effort providing a common set of time series data available for experimentation
    and research in time series classification tasks. It also shows a very diverse
    set of data. We can see this by looking at two data sets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[UEA和UCR时间序列分类库](https://perma.cc/56Q5-YPNT)是一个新的努力，提供一组常见的时间序列数据，可用于时间序列分类任务的实验和研究。它还展示了非常多样化的数据。我们可以通过查看两个数据集来看到这一点。'
- en: One data set is a yoga movement classification task. The [classification task](https://perma.cc/U6MU-2SCZ)
    involves distinguishing between two individual actors who performed a series of
    transitions between yoga poses while images were recorded. The images were converted
    to a one-dimensional series. The data is stored in a CSV file, with the label
    as the leftmost column and the remaining columns representing time steps. Time
    passes from left to right across the columns, rather than from top to bottom across
    the rows.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个数据集是瑜伽动作分类任务。[分类任务](https://perma.cc/U6MU-2SCZ)涉及区分两位表演者在录制图像时执行瑜伽姿势转换系列的能力。图像被转换为一维系列。数据存储在CSV文件中，标签为最左侧列，其余列表示时间步长。时间从左到右穿过列，而不是从上到下穿过行。
- en: Plots of two sample time series per gender are shown in [Figure 2-4](#fig-0203).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了每个性别的两个样本时间序列图，见[图 2-4](#fig-0203)。
- en: Univariate Versus Multivariate Time Series
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单变量与多变量时间序列
- en: The data sets we have looked at so far are *univariate* time series; that is,
    they have just one variable measured against time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们查看的数据集是*单变量*时间序列；即它们只有一个变量随时间变化。
- en: '*Multivariate* time series are series with multiple variables measured at each
    timestamp. They are particularly rich for analysis because often the measured
    variables are interrelated and show temporal dependencies between one another.
    We will encounter multivariate time series data later.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*多变量*时间序列是在每个时间戳测量多个变量的序列。它们特别适合分析，因为通常测量的变量彼此相关，并显示彼此之间的时间依赖性。我们稍后会遇到多变量时间序列数据。'
- en: '![](assets/ptsa_0204.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0204.png)'
- en: Figure 2-4\. Plots of a male and a female actor performing a yoga move repeatedly.
    We plot two sample time series per actor. There are no explicit time labels on
    the x-axis. Rather than the unit of time, what is important is whether the x-axis
    data points are evenly spaced, as they are presented here.
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 展示了一个男性和一个女性演员重复执行瑜伽动作的情况。我们绘制了每个演员的两个样本时间序列。x 轴上没有显式的时间标签。重要的不是时间单位，而是
    x 轴数据点是否均匀分布，正如这里所展示的那样。
- en: Also consider the [wine data set](https://perma.cc/CJ7A-SXFD), in which wines
    were classified by region according to the shapes of their spectra. So what makes
    this relevant to time series analysis? A *spectrum* is a plot of light wavelength
    versus intensity. Here we see a time series classification task where there is
    no passage of time at all. Time series analysis applies, however, because there
    is a unique and meaningful ordering of the x-axis, with a concrete meaning of
    distance along that axis. Time series analysis distinguishes itself from cross-sectional
    analysis by using the additional information provided by ordering on the x-axis,
    whether it is time or wavelength or something else. We can see a plot of such
    a “time” series in [Figure 2-5](#fig-0204). There is no temporal element, but
    we are nonetheless looking at an ordered series of data, so the usual ideas of
    time series apply.^([1](ch02.html#idm45576046843816))
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 也要考虑[葡萄酒数据集](https://perma.cc/CJ7A-SXFD)，其中葡萄酒根据其光谱的形状被分类为不同地区。那么这与时间序列分析有何关联呢？*光谱*是光波长与强度的图示。在这里，我们看到一个时间序列分类任务，其中根本没有时间的流逝。然而，时间序列分析适用，因为在
    x 轴上有一个独特且有意义的排序，其具体含义是沿该轴的距离。时间序列分析通过利用 x 轴的顺序提供的额外信息，无论是时间还是波长或其他东西，与横截面分析有所不同。我们可以在
    [图 2-5](#fig-0204) 中看到这样一个“时间”序列的图示。虽然没有时间元素，但我们仍在看一个有序的数据系列，因此时间序列的常规概念同样适用。^([1](ch02.html#idm45576046843816))
- en: '![](assets/ptsa_0205.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0205.png)'
- en: Figure 2-5\. A sample spectrum of the wine sampled in the UCI wine data set.
    Peaks in the curve indicate wavelength regions that have particularly high rates
    of absorption. The wavelengths are uniformly spaced along the x-axis, whereas
    the y-axis indicates the rate of absorption, also on a linear scale. We can use
    time series analysis to compare curves, such as the one above, to one another.
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. UCI葡萄酒数据集中葡萄酒的样本光谱。曲线中的峰值指示具有特别高吸收率的波长区域。波长在 x 轴上均匀分布，而 y 轴表示吸收率，也是线性刻度。我们可以使用时间序列分析来比较诸如上述曲线之类的曲线。
- en: Government time series data sets
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 政府时间序列数据集
- en: The US government has been a reliable provider of time series data for decades,
    or even centuries. For example, the [NOAA National Centers for Environmental Information](https://perma.cc/EA5R-TP5L)
    publishes a variety of time series data relating to temperatures and precipitation
    at granularities as fine as every 15 minutes for all weather stations across the
    country. The [Bureau of Labor Statistics](https://www.bls.gov/) publishes a monthly
    index of the national unemployment rate. The [Centers for Disease Control and
    Prevention](https://perma.cc/Y6KG-T948) publishes weekly flu case counts during
    the flu season. The [Federal Reserve Bank of St. Louis](https://fred.stlouisfed.org/)
    offers a particularly generous and helpful set of economic time series data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年甚至几个世纪以来，美国政府一直是可靠的时间序列数据提供者。例如，[NOAA国家环境信息中心](https://perma.cc/EA5R-TP5L)发布了与全国各气象站有关的各种时间序列数据，涉及温度和降水，甚至可以每15分钟精细化。[劳工统计局](https://www.bls.gov/)每月发布全国失业率指数。[疾病控制与预防中心](https://perma.cc/Y6KG-T948)在流感季节期间每周发布流感病例统计。[圣路易斯联邦储备银行](https://fred.stlouisfed.org/)提供了一系列非常丰富和有用的经济时间序列数据。
- en: For initial forays into time series analysis, I recommend that you access these
    real-world government data sets only for exploratory analysis and visualization.
    It can be difficult to learn on these data sets because they present extremely
    complicated problems. For example, many economists spend their entire careers
    trying to predict the unemployment rate in advance of its official publication,
    with only limited success.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初次涉足时间序列分析，我建议您仅对这些现实世界的政府数据集进行探索性分析和可视化。学习这些数据集可能会很困难，因为它们提出了极为复杂的问题。例如，许多经济学家在官方发布前整个职业生涯都在努力预测失业率，但仅有有限的成功。
- en: For the important but intractable problems faced by governments, predicting
    the future would not only be socially beneficial but also highly remunerative.
    Many smart and well-trained people are attacking these problems even as the state
    of the art remains somewhat disappointing. It is great to work on difficult problems,
    but it is not a good idea to learn on such problems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于政府面临的重要但难以解决的问题，预测未来不仅在社会上有益，而且在经济上也很有利可图。许多聪明而受过良好训练的人正在解决这些问题，尽管技术水平仍然有些令人失望。解决困难问题是很好的，但不建议在这些问题上学习。
- en: Additional helpful sources
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他有用的来源
- en: 'While we cannot extensively cover all good sources of time series data, there
    are several other repositories that you should explore:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不能详尽地覆盖所有优秀的时间序列数据源，但还有几个其他的资源库你应该探索：
- en: '[CompEngine](https://comp-engine.org)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Comp   [CompEngine](https://comp-engine.org)'
- en: This “self organizing database of time-series data” has more than 25,000 time
    series databases that total almost 140 million individual data points. The emphasis
    of this repository, and the associated software it offers on its web interface,
    is to facilitate and promote *highly comparative time-series analysis* (hctsa).
    The goal of such analysis is to generate high-level insights and understanding
    of how many kinds of temporal behavior can be understood without discipline-specific
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“自组织时间序列数据数据库”拥有超过25,000个时间序列数据库，总计接近1.4亿个单独的数据点。这个资源库及其在网页界面上提供的相关软件，重点是促进和推广*高度比较时间序列分析*（hctsa）。这种分析的目标是生成高级洞见，理解各种时间行为的多种类型，而无需特定学科的数据。
- en: '[Mcomp](https://cran.r-project.org/package=Mcomp) and [M4comp2018](https://github.com/carlanetto/M4comp2018)
    R packages'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mcomp](https://cran.r-project.org/package=Mcomp) 和 [M4comp2018](https://github.com/carlanetto/M4comp2018)
    R 包'
- en: These R packages provide the competition data from the 1982 M-competition (1,001
    time series), the M3 competition in 2000 (3,003 time series), and the M4 competition
    in 2018 (100,000 time series). These time series forecasting competitions were
    previously discussed in the [Chapter 1](ch01.html#time_series_an_overview_and_a_quick_history)
    mention of Professor Rob Hyndman’s history of time series forecasting. Additional
    time series forecasting competition data is included in R’s [*tscompdata*](https://github.com/robjhyndman/tscompdata)
    package. Finally, more specialized time series data sets can also be found in
    a variety of packages described in the [CRAN repository listing of time series
    packages](https://perma.cc/2694-D79K) under the “Time Series Data” header.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 R 包提供了1982年M竞赛（1,001个时间序列）、2000年M3竞赛（3,003个时间序列）和2018年M4竞赛（100,000个时间序列）的竞赛数据。这些时间序列预测竞赛之前在[第一章](ch01.html#time_series_an_overview_and_a_quick_history)中提到了Rob
    Hyndman教授关于时间序列预测的历史。R的[*tscompdata*](https://github.com/robjhyndman/tscompdata)包中还包含了更多的时间序列预测竞赛数据。最后，还可以在CRAN的时间序列包资源库列表中找到更多专业的时间序列数据集，详情请见[时间序列包的CRAN资源库列表](https://perma.cc/2694-D79K)中的“时间序列数据”头部。
- en: Found Time Series
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现时间序列
- en: Earlier in the chapter, we discussed the concept of a found time series, which
    is time series data we put together ourselves from data sources in the wild. More
    specifically, such time series would be put together from individual data points
    recorded without any special allowances for time series analysis but with enough
    information to construct a time series. Putting together a time series of transactions
    for a particular customer from a SQL database that stores a company’s transactions
    is a clean example. In such a case, the time series could be constructed so long
    as a timestamp, or some proxy for a timestamp, was saved in the database.^([2](ch02.html#idm45576046801992))
    We can also imagine other time series constructed from the same data, such as
    a time series of total transaction volume per day for the company or total dollar
    volume for female customers per week. We can even imagine generating multivariate
    time series data, such as a time series that would separately indicate total volume
    of all customers under 18 per week, total dollars spent by women over 65 per week,
    and total ad spend by the company per week. This would give us three indicators
    at each time step, a multivariate time series.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早些时候，我们讨论了“发现时间序列”的概念，即我们从野外数据源中自行整理出来的时间序列数据。更具体地说，这些时间序列是从没有特别为时间序列分析设置的单个数据点中整理出来的，但具有足够的信息来构建时间序列。以从存储公司交易的
    SQL 数据库中为特定客户拼接的交易时间序列为例，这就是一个干净的例子。在这种情况下，只要数据库中保存了时间戳或某种时间戳的代理，就可以构建时间序列。^([2](ch02.html#idm45576046801992))
    我们还可以想象从相同的数据中构建的其他时间序列，例如公司每天的总交易量时间序列或每周女性客户的总美元交易量时间序列。我们甚至可以想象生成多变量时间序列数据，例如一个时间序列，分别表示每周所有未满18岁客户的总交易量、每周65岁以上女性的总支出和公司每周的广告支出。这将在每个时间步骤给我们提供三个指标，一个多变量时间序列。
- en: 'Finding time series data in structured data not explicitly stored as a time
    series can be easy in the sense that timestamping is ubiquitous. Here are a few
    examples of where you’ll see timestamps in your database:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化数据中找到时间序列数据，即使未明确存储为时间序列，也可以很容易，因为时间戳是无处不在的。以下是一些在数据库中看到时间戳的示例：
- en: Timestamped recordings of events
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 事件的时间戳记录
- en: If there is a timestamp on your data, you have the potential to construct a
    time series. Even if all you do is record the time a file was accessed with no
    other information, you have a time series. For example, in that case, you could
    model the delta time between timestamps with each marked according to its later
    timestamp, so that your time series would consist of time on your temporal axis
    and delta time on your value axis. You could go further, aggregating these delta
    times as means or totals over larger periods, or you could keep them individually
    recorded.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据有时间戳，您就有可能构建时间序列。即使您只记录了文件访问时的时间而没有其他信息，您也有了一个时间序列。例如，在这种情况下，您可以模拟时间戳之间的时间差，并将其标记为其后时间戳，使得您的时间序列将由时间轴上的时间和值轴上的时间差组成。您可以进一步进行聚合，将这些时间差作为更大时间段的平均值或总和，或者您可以单独记录它们。
- en: “Timeless” measurements where another measurement substitutes for time
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: “无时间性”测量，其中另一种测量替代时间
- en: In some cases, time is not explicit in data but is accounted for in the underlying
    logic of the data set. For example, you may think of your data as “distance versus
    value” when the distance is being caused by a known experimental parameter, such
    as retracting a sensor from a position at a known rate. If you can map one of
    your variables to time, you have a time series. Alternately, if one of your axes
    has a known distance and ordering relationship (such as wavelength), you are also
    looking at time series data such as the case of the wine spectra mentioned earlier.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，数据中时间并不显式，但在数据集的基础逻辑中有所体现。例如，当距离由已知的实验参数引起时，您可能会将数据视为“距离与值”的关系。如果您能够将其中一个变量映射到时间上，那么您就拥有了一个时间序列。或者，如果您的一个轴具有已知的距离和排序关系（例如波长），那么您也正在查看时间序列数据，例如前面提到的葡萄酒光谱案例。
- en: Physical traces
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 物理迹象
- en: Many scientific disciplines record physical traces, be it for medicine, audiology,
    or weather. These used to be physically generated traces collected via analog
    processes, but nowadays they are stored in a digital format. These are also time
    series, although they may stored in a format that doesn’t make this obvious, such
    as in an image file or in a single vector within a single field of a database.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 许多科学学科记录物理迹象，无论是医学、听力还是天气。这些曾经是通过模拟过程生成的物理迹象，但现在它们以数字格式存储。尽管它们可能存储在不显而易见的格式中，例如图像文件或数据库的单个向量字段中，但这些也是时间序列。
- en: Retrofitting a Time Series Data Collection from a Collection of Tables
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一组表中的时间序列数据收集进行改装
- en: The quintessential example of a found time series is one extracted from state-type
    and event-type data stored in a SQL database. This is also the most relevant example
    because so much data continues to be stored in traditional structured SQL databases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 发现的时间序列的典型例子是从存储在SQL数据库中的状态类型和事件类型数据中提取的。这也是最相关的示例，因为仍然有大量数据存储在传统的结构化SQL数据库中。
- en: 'Imagine working for a large nonprofit organization. You have been tracking
    a variety of factors that could lend themselves to time series analysis:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下为一个大型非营利组织工作。您一直在跟踪各种可能适合进行时间序列分析的因素：
- en: 'An email recipient’s reaction to emails over time: Did they open the emails
    or not?'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件接收者对电子邮件的反应随时间变化：他们是否打开了邮件？
- en: 'A membership history: Were there periods when a member let their membership
    lapse?'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 会员历史记录：会员是否有间断的会员资格期？
- en: 'Transaction history: When does an individual buy and can we predict this?'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易历史记录：个人何时购买以及我们能预测吗？
- en: 'You could look at the data with several time series techniques:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用几种时间序列技术来查看数据：
- en: You can generate a 2D histogram of member responses to emails over time with
    a member-specific time line to get an idea of whether members develop fatigue
    from emails. (We’ll illustrate the use of 2D histograms for understanding time
    series in [Chapter 3](ch03.html#exploratory_data_analysis_for_time_series).)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以生成成员对电子邮件响应的二维直方图以了解成员是否因电子邮件而感到疲劳的时间线。（我们将在[第三章](ch03.html#exploratory_data_analysis_for_time_series)中说明使用二维直方图来理解时间序列的方法。）
- en: You can turn donation predictions into a time series forecasting problem. (We’ll
    discuss classic statistical forecasting in [Chapter 4](ch04.html#simulating_time_series_data).)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将捐赠预测转化为时间序列预测问题。（我们将在 [第 4 章](ch04.html#simulating_time_series_data) 中讨论经典的统计预测。）
- en: You could examine whether there are typical patterns of trajectories for member
    behavior in important situations. For example, is there a typical pattern of events
    that indicates when a member is about to leave your organization (perhaps three
    email deletes in a row)? In time series analysis, you could frame this as detecting
    a member’s underlying state based on external actions. (We’ll cover this when
    we discuss state space methods of time series analysis in [Chapter 7](ch07.html#state_space_models_for_time_series).)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以检查在重要情况下成员行为轨迹的典型模式是否存在。例如，是否存在一种典型的事件模式表明一个成员即将离开您的组织（也许连续三封电子邮件被删除）？在时间序列分析中，你可以将这视为基于外部动作检测成员潜在状态的方法。（我们将在
    [第 7 章](ch07.html#state_space_models_for_time_series) 中讨论时间序列分析的状态空间方法。）
- en: As we can see, there are many time series questions and answers in a simple
    SQL database. In many cases, organizations don’t plan for time series analysis
    when they are designing their database schema. In such examples, we need to collect
    and assemble time series from disparate tables and sources.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，在一个简单的 SQL 数据库中有许多时间序列的问题和答案。在许多情况下，组织在设计数据库架构时并未计划进行时间序列分析。在这种情况下，我们需要从不同的表和来源收集和组装时间序列。
- en: 'A Worked Example: Assembling a Time Series Data Collection'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个案例：组装时间序列数据收集
- en: 'If you are lucky enough to have several related data sources available, you
    will need to line them up together, possibly dealing with disparate timestamping
    conventions or different levels of granularity in the data. Let’s create some
    numbers for the nonprofit example we were using. Suppose you have data shown in
    [Table 2-1](#year-joined-table) through [Table 2-3](#member-donation-table):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有幸有几个相关的数据源可用，你将需要将它们排列在一起，可能要处理不同的时间戳约定或数据中不同的粒度级别。让我们为我们正在使用的非营利组织示例创建一些数字。假设你有如下所示的数据：[表 2-1](#year-joined-table)
    到 [表 2-3](#member-donation-table)：
- en: Table 2-1\. The year each member joined and current status of membership
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. 每位成员加入的年份和当前会员状态
- en: '| MemberId | YearJoined | MemberStatus |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 成员ID | 加入年份 | 成员状态 |'
- en: '| --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 2017 | gold |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2017 | 金 |'
- en: '| 2 | 2018 | silver |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2018 | 银 |'
- en: '| 3 | 2016 | inactive |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2016 | 无效 |'
- en: Table 2-2\. Number of emails you sent out in a given week that were opened by
    the member
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-2\. 一周内成员打开的电子邮件数量
- en: '| MemberId | Week | EmailsOpened |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 成员ID | 周 | 打开的邮件数 |'
- en: '| --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2 | 2017-01-08 | 3 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2017-01-08 | 3 |'
- en: '| 2 | 2017-01-15 | 2 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2017-01-15 | 2 |'
- en: '| 1 | 2017-01-15 | 1 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2017-01-15 | 1 |'
- en: Table 2-3\. Time a member donated to your organization
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-3\. 成员捐赠给您组织的时间
- en: '| MemberId | Timestamp | DonationAmount |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 成员ID | 时间戳 | 捐赠金额 |'
- en: '| --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2 | 2017-05-22 11:27:49 | 1,000 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2017-05-22 11:27:49 | 1,000 |'
- en: '| 2 | 2017-04-13 09:19:02 | 350 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2017-04-13 09:19:02 | 350 |'
- en: '| 1 | 2018-01-01 00:15:45 | 25 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2018-01-01 00:15:45 | 25 |'
- en: You have likely worked with data in this tabular form. With such data, you can
    answer many questions, such as how the overall number of emails opened by a member
    correlates with the overall donations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经用过这种表格形式的数据工作过。有了这样的数据，你可以回答许多问题，比如成员打开的邮件总数与捐赠总数之间的关系。
- en: You can also answer time-oriented questions, such as whether a member donates
    soon after joining or long after. Without putting this data into a more time-series-friendly
    format, however, you cannot get at more granular behaviors that might help you
    predict when someone is likely to make a donation (say, based on whether they
    have recently been opening emails).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以回答与时间有关的问题，比如一个成员是在加入后不久捐赠还是很久之后捐赠。然而，如果不将这些数据转换为更适合时间序列的格式，你就无法深入了解可能帮助你预测某人何时可能捐赠的更精细行为（比如根据他们最近是否在打开邮件）。
- en: You need to put this data into a sensible format for time series analysis. There
    are a number of challenges you will need to address.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将这些数据放入一个合理的时间序列分析格式中。你将需要解决一些挑战。
- en: 'You should start by considering the temporal axes of the data we have. In the
    preceding tables we have three levels of temporal resolution:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该从考虑我们已有数据的时间轴开始。在上述表格中，我们有三个层次的时间分辨率：
- en: A yearly member status
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个年度成员状态
- en: A weekly tally of emails opened
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一周内打开的电子邮件总数
- en: Instantaneous timestamps of donations
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捐赠的即时时间戳
- en: 'You will also need to examine whether the data means what you think it means.
    For example, you would want to determine whether the member status is a yearly
    status or just the most recent status. One way to answer this is to check whether
    any member has more than one entry:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要检查数据是否意味着您认为它意味着什么。例如，您可能想要确定成员状态是年度状态还是最近的状态。回答这个问题的一个方法是检查是否有任何成员有多个条目：
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we can see that all 1,000 members have only one status, so that the year
    they joined is indeed likely to be the `YearJoined`, accompanied by a status that
    may be the member’s current status or status when they joined. This distinction
    affects how you would use the status variable, so if you were going to analyze
    this data further you’d want to clarify with someone who knows the data pipeline.
    If you were applying a member’s current status to an analysis of past data, that
    would be a *lookahead* because you would be inputting something into a time series
    model that could not be known at the time. This is why you would not want to use
    a status variable, such as `YearJoined`, without knowing when it was assigned.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到所有1,000名成员只有一个状态，因此他们加入的年份很可能确实是`YearJoined`，并且伴随一个可能是成员当前状态或加入时的状态。这影响如何使用状态变量，因此，如果您打算进一步分析这些数据，您需要与了解数据管道的人澄清。如果您将成员的当前状态应用于过去数据的分析中，这将是一种*预测*，因为您会向时间序列模型输入在当时无法知道的内容。这就是为什么在不知道分配时间的状态变量（例如`YearJoined`）时，您不应该使用它的原因。
- en: Looking at the emails table, both the column name `week` and its contents suggest
    that the data is a weekly timestamp or time period. This must be an aggregate
    over the week, so we should think of these timestamps as weekly periods rather
    than timestamps occurring one week apart.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 查看电子邮件表，`week`列及其内容都表明数据是一个周时间戳或时间段。这必须是一周的汇总，所以我们应该将这些时间戳视为每周期而不是相隔一周的时间戳。
- en: You should assess some important characteristics. For example, you could start
    by asking how the weeks are reported in time. While we may not have information
    to restructure the table, if the week is divided in a somewhat strange way relative
    to our industry, we might want to know about this too. For analyzing human activities,
    it generally makes sense to look at the calendar week of Sunday through Saturday
    or Monday through Sunday rather than weeks that are less in line with the cycle
    of human activity. So, for example, don’t arbitrarily start your week with January
    1st.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该评估一些重要的特征。例如，您可以从询问时间中如何报告周开始。虽然我们可能没有信息来重新组织表格，但如果与我们的行业相比，一周的划分方式有些奇怪，我们可能也想了解这一点。对于分析人类活动，通常有意义的是查看星期日至星期六或星期一至星期日的日历周，而不是与人类活动周期不太一致的周。因此，例如，不要随意从一月一日开始您的一周。
- en: You could also ask whether null weeks are reported? That is, do the weeks in
    which the member opened 0 emails have a place in the table? This matters when
    we want to do time-oriented modeling. In such cases we need to always have the
    null weeks present in the data because a 0 week is still a data point.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以问空周是否报告了？也就是说，成员在表中打开0封电子邮件的周是否有位置？当我们想要进行面向时间的建模时，这很重要。在这种情况下，我们需要始终在数据中有空周存在，因为0周仍然是一个数据点。
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are two possibilities: either nulls are not reported or members always
    have at least one email event. Anyone who has worked with email data knows that
    it’s difficult to get people to open emails, so the hypothesis that members always
    open at least one email per week is quite unlikely. In this case, we can resolve
    this by looking at the history of just one user:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种可能性：要么空值没有报告，要么成员总是至少有一个电子邮件事件。任何使用电子邮件数据的人都知道，让人们打开电子邮件很难，因此成员每周至少打开一封电子邮件的假设是相当不可能的。在这种情况下，我们可以通过查看仅一个用户的历史来解决这个问题。
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see that some weeks are missing. There aren’t any December 2017 email
    events after December 18, 2017.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些周是缺失的。在2017年12月18日后，2017年12月没有任何电子邮件事件。
- en: 'We can check this more mathematically by calculating how many weekly observations
    we should have between the first and last event for that member. First we calculate
    the length of the member’s tenure, in weeks:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算该成员的第一个和最后一个事件之间应该有多少周观察，我们可以更数学地检查这一点。首先，我们计算成员任期的长度，以周为单位。
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then we see how many weeks of data we have for that member:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看看该成员有多少周的数据：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have 24 rows here, but we should have 26\. This shows some weeks of data
    are missing for this member. Incidentally, we could also run this calculation
    on all members simultaneously with group-by operations, but it’s more approachable
    to think about just one member for example purposes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里有 24 行，但应该有 26 行。这显示这个成员的一些周数据是缺失的。顺便说一句，我们也可以通过分组操作同时对所有成员运行此计算，但是仅考虑一个成员更易于理解示例。
- en: We’ll move on to filling in the blanks so that we have a complete data set now
    that we have confirmed we do indeed have missing weeks. We can’t be sure of identifying
    all missing weeks, since some may have occurred before our earliest recorded date
    or after our last recorded date. What we can do, however, is fill in the missing
    values between the first and last time a member had a non-null event.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已确认确实有缺失的周，我们将继续填补空白，以便我们现在拥有完整的数据集。我们不能确定识别所有缺失的周，因为有些可能发生在我们记录的日期之前或之后。然而，我们可以做的是填补成员在非空事件发生之前和之后第一个和最后一个时间之间的缺失值。
- en: 'It’s a lot easier to fill in all missing weeks for all members by exploiting
    Pandas’ indexing functionality, rather than writing our own solution. We can generate
    a `MultiIndex` for a Pandas data frame, which will create all combinations of
    weeks and members—that is, a Cartesian product:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 Pandas 的索引功能，填充所有成员的所有缺失周要比编写我们自己的解决方案更容易。我们可以为 Pandas 数据框生成一个 `MultiIndex`，这将创建周和成员的所有组合——即笛卡尔积：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We use this index to reindex the original table and fill in the missing values—in
    this case with 0 on the assumption that nothing recorded means there was nothing
    to record. We also reset the index to make the member and week information available
    as columns, and then name those columns:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用此索引重新索引原始表格，并填充缺失的值——在这种情况下，假设没有记录意味着没有可记录的内容，我们还重置索引以使成员和周信息作为列可用，并将这些列命名为：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s take a look at member 998 again:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看成员 998：
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Notice that we have a large number of zeros at the start. These are likely
    before the member joined the organization, so they would not have been on an email
    list. There are not too many kinds of analyses where we’d want to keep the member’s
    truly null weeks around—specifically those weeks before the member ever indicated
    opening an email. If we had the precise date a member started receiving emails,
    we would have an objective cutoff. As it is, we will let the data guide us. For
    each member we determine the `start_date` and `end_date` cutoffs by grouping the
    email `DataFrame` per member and selecting the maximum and minimum week values:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们一开始就有大量的零。这些可能是成员加入组织之前的时间，因此他们不会出现在电子邮件列表中。在我们想要保留成员真正空闲周的分析中，没有太多种类。如果我们有成员开始接收电子邮件的确切日期，我们将有一个客观的截止日期。由于现状如此，我们将让数据指导我们。对于每个成员，我们通过将电子邮件
    `DataFrame` 按成员分组，并选择最大和最小周值来确定 `start_date` 和 `end_date` 截止日期：
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We drop rows from the `DataFrame` that don’t contribute sensibly to the chronology,
    specifically 0 rows before each member’s first nonzero count:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `DataFrame` 中删除那些在时间线上没有贡献意义的行，具体来说是每个成员的第一个非零计数之前的 0 行：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: < or <= ?
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: < 还是 <= ?
- en: We use the < and > operations, without equality, because the `start_date` and
    `end_date` are inclusive of the meaningful data points and because we are dropping
    data, not retaining data, as our code is written. In this case we want to include
    those weeks in our analysis because they were the first and last meaningful data
    points.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 < 和 > 操作符，不包括等号，因为 `start_date` 和 `end_date` 包含了有意义的数据点，并且因为我们正在删除数据，而不是保留数据，按照我们的代码编写。在这种情况下，我们希望在分析中包括这些周，因为它们是第一个和最后一个有意义的数据点。
- en: You will do well to work with your data engineers and database administrators
    to convince them to store the data in a time aware manner, especially with respect
    to how timestamps are created and what they mean. The more you can solve problems
    upstream, the less work you have to do downstream in the data pipeline.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你最好与你的数据工程师和数据库管理员一起工作，说服他们以时间感知的方式存储数据，特别是关于如何创建时间戳及其含义。你能在上游解决的问题越多，下游数据管道的工作就越少。
- en: 'Now that we have cleaned up our email data, we can consider new questions.
    For example, if we want to think about the relationship of member email behavior
    to donations, we can do a few things:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清理了电子邮件数据，我们可以考虑新的问题。例如，如果我们想考虑成员电子邮件行为与捐赠的关系，我们可以做一些事情：
- en: Aggregate the `DonationAmount` to weekly so that the time periods are comparable.
    Then it becomes reasonable to ask whether donations correlate in some way to member
    responses to emails.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`DonationAmount`聚合到每周，以便时间段可比。然后可以合理地询问捐赠是否与成员对电子邮件的响应有某种关联。
- en: Treat the prior week’s `EmailsOpened` as a predictor for a given week’s `DonationAmount`.
    Note that we have to use the prior week because `EmailsOpened` is a summary statistic
    for the week. If we want to predict a Wednesday donation, and our `EmailsOpened`
    summarizes email opening behavior from Monday to Sunday, then using the same week’s
    information will potentially give us information about what the member did subsequent
    to when we could have known it (for example, whether they opened an email on the
    Friday after the donation).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将上周的`EmailsOpened`作为给定周的`DonationAmount`的预测因子。请注意，我们必须使用上周的数据，因为`EmailsOpened`是一周的汇总统计数据。如果我们想要预测星期三的捐款，并且我们的`EmailsOpened`总结了从周一到周日的电子邮件开启行为，那么使用同一周的信息可能会告诉我们成员在我们能知道之后的行为（例如，他们是否在捐款后的星期五打开了电子邮件）。
- en: Constructing a Found Time Series
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个基础时间序列。
- en: Consider how to relate the email and donations data to one another. We can downsample
    the donation data to turn it into a weekly time series, comparable to the email
    data. As an organization, we are interested in the total weekly amounts, so we
    aggregate the timestamps into weekly periods by summing. More than one donation
    in a week is unlikely, so the weekly donation amounts will reflect the individual
    donation amounts for most donors.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑如何将电子邮件和捐款数据相互关联。我们可以将捐款数据降采样，使其变成一个每周的时间序列，可与电子邮件数据进行比较。作为一个组织，我们对每周总金额感兴趣，因此我们通过求和将时间戳聚合为每周周期。一周内捐款超过一个的情况不太可能发生，因此每周捐款金额将反映大多数捐赠者的个人捐款金额。
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this code we first convert a string character to a proper timestamped data
    class so as to benefit from Pandas’ built-in date-related indexing. We set the
    timestamp as an index, as is necessary for resampling a data frame. Finally for
    the data frame that we obtain from subsetting down to each member, we group and
    sum donations by week, drop weeks that do not have donations, and then collect
    these together.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们首先将字符串字符转换为适当的时间戳数据类，以便从Pandas的内置日期相关索引中获益。我们将时间戳设为索引，这是对数据帧进行重新采样所必需的。最后，对于从子集到每个会员得到的数据帧，我们按周分组并汇总捐款，删除没有捐款的周，然后将它们收集在一起。
- en: Note that we resampled with an anchored week so that we will match the same
    weekly dates we already have in our email table. Note also that a week anchored
    to “Monday” makes sense from a human perspective.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们进行了带锚定周的重新采样，以便与我们电子邮件表中已有的同一周日期匹配。还要注意，从人类角度来看，将周锚定在“星期一”是有意义的。
- en: 'We now have donation information and email information sampled at the same
    frequency, and we can join them. Pandas makes this simple so long as we anchor
    the weeks to the same day of the week, as we’ve done already. We can iterate through
    each member and merge the data frames per member:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了捐款信息和电子邮件信息以相同频率采样的数据，我们可以将它们连接起来。只要我们已经将周的数据锚定到同一周的某一天，Pandas使这变得很简单。我们可以迭代每个会员，并合并数据帧：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We now have our email and donations data lined up per member. For each member
    we include only meaningful weeks, and not weeks that appear to be before or after
    their membership period.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了按会员排列的电子邮件和捐款数据。对于每个会员，我们只包括有意义的周，而不是会员期前后的周。
- en: 'We might treat the email behavior as a “state” variable relative to the donation
    behavior, but we probably want to carry forward the state from the previous week
    so as to avoid a lookahead. Suppose, for example, we were building a model that
    uses email behavior to predict a member’s next donation. In this case we might
    consider looking at the pattern of email opening over time as a possible indicator.
    We would need to line up a given week’s donation with the previous week’s email
    behavior. We can easily take our data, processed to align week-to-week, and then
    shift by the appropriate number of weeks. If, say, we want to shift the donation
    a week forward, we can easily do so with the shift operator, although we would
    need to be sure to do this on a per-member basis:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会将电子邮件行为视为与捐赠行为相关的“状态”变量，但我们可能希望从上周保留状态，以避免预先看到。例如，假设我们正在构建一个模型，该模型使用电子邮件行为来预测会员的下次捐赠。在这种情况下，我们可能会考虑观察一周内电子邮件打开的模式作为潜在的指标。我们需要将给定周的捐赠与上周的电子邮件行为对齐。我们可以轻松地对我们的数据进行处理，使其按周对齐，然后根据适当的周数进行移动。例如，如果我们想将捐赠向前推移一周，我们可以使用shift运算符轻松实现，尽管我们需要确保对每个会员都这样做：
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It’s good practice to store this target in a new column rather than overwrite
    the old, particularly if you are not also bringing forward the timestamp for the
    donation amount separately. We have shifted the donation amount one week into
    the future using Pandas’ built-in shift functionality. You can also shift back
    in time with negative numbers. Generally you will have more predictors than targets,
    so it makes sense to shift your targets. We can see the outcome of the code here:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最好将这个目标存储在一个新列中，而不是覆盖旧列，特别是如果你没有将捐赠金额的时间戳也同时前移。我们使用Pandas内置的shift功能，将捐赠金额向未来推移了一周。你也可以用负数向过去推移。通常情况下，你会有更多的预测因子而不是目标变量，因此将目标变量向后移动是合理的。我们可以在这里看到代码的结果：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have filled in the missing rows, we have the desired 26 rows for
    member 998\. Our data is now cleaner and more complete.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经填补了缺失的行，为会员998创建了所需的26行。我们的数据现在更加清洁和完整。
- en: 'To recap, these are the time-series-specific techniques we used to restructure
    the data:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这些是我们用来重构数据的特定于时间序列的技术：
- en: '*Recalibrate the resolution of our data* to suit our question. Often data comes
    with more specific time information than we need.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*重新校准我们数据的分辨率*以适应我们的问题。通常数据提供的时间信息比我们实际需要的更加具体。'
- en: Understand how we can *avoid lookahead* by not using data for timestamps that
    produce the data’s availability.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免使用数据来生成时间戳，从而*避免预先看到*数据的可用性。
- en: Record *all relevant time periods* even if “nothing happened.” A zero count
    is just as informative as any other count.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录*所有相关时间段*，即使“什么也没发生”。零计数与其他计数同样具有信息量。
- en: '*Avoid lookahead* by not using data for timestamps that produce information
    we shouldn’t yet know about.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过不使用数据来生成我们还不应该知道的信息的时间戳，*避免预先看到*。
- en: So far, we have created raw found time series by aligning our donation and email
    time series to be sampled for the same points in time and at the same frequency.
    However, we haven’t done a thorough job of cleaning up this data or fully exploring
    before analysis. That’s something we’ll do in [Chapter 3](ch03.html#exploratory_data_analysis_for_time_series).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过使捐赠和电子邮件时间序列在相同时间点和相同频率采样来创建了原始发现时间序列。然而，在进行分析之前，我们没有彻底清理这些数据或者完全探索它们。这是我们将在[第三章](ch03.html#exploratory_data_analysis_for_time_series)中进行的工作。
- en: Timestamping Troubles
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间戳问题
- en: Timestamps are quite helpful for time series analysis. From timestamps, we can
    extrapolate a number of interesting features, such as time of day or day of the
    week. Such features can be important to understanding your data, especially for
    data concerning human behavior. However, timestamps are tricky. Here we discuss
    some of the difficulties of timestamped data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳对于时间序列分析非常有帮助。通过时间戳，我们可以推断出许多有趣的特征，比如时间的具体时段或一周中的某一天。这些特征对于理解数据尤为重要，尤其是涉及人类行为的数据。然而，时间戳也有其难点。在这里，我们讨论了时间戳数据的一些困难。
- en: Whose Timestamp?
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪个时间戳？
- en: The first question you should ask when seeing a timestamp is what process generated
    the timestamp, how, and when. Often an event happening is not coincident with
    an event being recorded. For example, a researcher might write something down
    in their notebook and later transcribe it to a CSV file used as a log. Does the
    timestamp indicate when they wrote it down or when they transcribed it to the
    CSV file? Or a mobile app user may take actions that trigger logging when their
    phone is offline, so that the data is only later uploaded to your server with
    some combination of timestamps. Such timestamps could reflect when the behavior
    took place, when the action was recorded by the app, when the metadata was uploaded
    to the server, or when the data was last accessed for download from the server
    back to the app (or any number of other events along the data pipeline).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当看到时间戳时，你应该首先问的问题是：这个时间戳是由什么过程生成的，是怎样生成的，以及何时生成的。通常发生的事件并不一定与记录事件的时间一致。例如，研究人员可能会在笔记本上写下一些内容，然后稍后将其转录到用作日志的CSV文件中。时间戳是否表示他们写下内容的时间，还是将其转录到CSV文件的时间？或者移动应用程序用户可能会在他们的手机离线时执行操作，以便数据稍后通过一些时间戳的组合上传到您的服务器。这些时间戳可以反映行为发生的时间，应用程序记录行为的时间，元数据上传到服务器的时间，或者从服务器下载数据返回到应用程序时的时间（或数据管道中的任何其他事件）。
- en: Timestamps may appear at first to offer clarity but this quickly falls away
    if proper documentation is lacking. When you’re looking at novel timestamps, your
    first step should always be to find out as best you can what established the time
    of an event.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，时间戳可能会提供清晰度，但如果缺乏适当的文档，这种清晰度很快就会消失。当您查看新的时间戳时，您的第一步应始终是尽可能好地了解事件时间的来源。
- en: A concrete example will illustrate these difficulties. Suppose you are looking
    at data taken from a mobile app for weight loss and see a meal diary with entries
    such as those in [Table 2-4](#timestamp-example-table).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具体的例子将说明这些困难。假设您正在查看从减肥移动应用程序中获取的数据，并看到餐饮日记，其中包含如表2-4所示的条目。
- en: Table 2-4\. Sample meal diary from a weight loss app
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2-4\. 减肥应用程序的样本餐饮日记
- en: '| Time | Intake |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | 摄入 |'
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mon, April 7, 11:14:32 | pancakes |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Mon, April 7, 11:14:32 | 煎饼 |'
- en: '| Mon, April 7, 11:14:32 | sandwich |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Mon, April 7, 11:14:32 | 三明治 |'
- en: '| Mon, April 7, 11:14:32 | pizza |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Mon, April 7, 11:14:32 | pizza |'
- en: It’s possible this user had pancakes, a sandwich, and a pizza all at once, but
    there are more likely scenarios. Did the user specify this time or was it automatically
    created? Does the interface perhaps offer an automatic time that the user can
    adjust or choose to ignore? Some answers to these questions would explain the
    identical timestamps better than the possibility that a user trying to lose weight
    would have had pancakes with a side of pizza and a sandwich appetizer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是这位用户同时吃了煎饼、三明治和披萨，但更有可能的情况是什么？用户是否指定了这个时间，还是它是自动创建的？界面是否提供了用户可以调整或选择忽略的自动时间？对这些问题的一些答案可以更好地解释相同时间戳背后的原因，而不是一个试图减肥的用户同时吃了煎饼、披萨和三明治作为开胃菜。
- en: Even if the user did have all these foods at 11:14, where in the world was it
    11:14? Was this the user’s local time or a global clock? Even in the unlikely
    case that the user had all this food at one meal, we still don’t know very much
    about the temporal aspect of the meal from these rows alone. We don’t know if
    this was breakfast, lunch, dinner, or a snack. To say something interesting to
    the user, we’d want to be able to talk in concrete terms about local hour of the
    day, which we can’t do without time zone information.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 即使用户确实在11:14时吃了所有这些食物，但世界上哪里是11:14呢？这是用户的本地时间还是全球时钟？即使在用户在一顿饭中吃了所有这些食物的情况下（尽管可能性很小），仅凭这些行就不能很好地了解到这顿饭的时间方面。我们不知道这是早餐、午餐、晚餐还是小吃。要向用户提供有趣的信息，我们需要能够具体讨论一天中的当地小时，而没有时区信息我们无法做到这一点。
- en: The best way to address these questions is to see all the code that collects
    and stores the data or talk to the people who wrote that code. After going through
    all available human and technical data specifications on the system, you should
    also try the full system out yourself to ensure that the data behaves as you have
    been told it does. The better you understand your data pipeline, the less likely
    you are to ask the wrong questions because your timestamps don’t really mean what
    you think they do.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的最佳方法是查看所有收集和存储数据的代码，或者与编写该代码的人交谈。在研究系统上所有可用的人类和技术数据规范后，你还应该亲自尝试整个系统，以确保数据的行为与你被告知的一致。你对数据管道了解得越深入，你就越不太可能因为时间戳实际上不是你认为的那样而问错问题。
- en: You bear the ultimate responsibility for understanding the data. People who
    work upstream in the pipeline don’t know what you have in mind for analysis. Try
    to be as hands-on as possible in assessing how timestamps are generated. So, if
    you are analyzing data from a mobile app pipeline, download the app, trigger an
    event in a variety of scenarios, and see what your own data looks like. You’re
    likely to be surprised about how your actions were recorded after speaking to
    those who manage the data pipeline. It’s hard to track multiple clocks and contingencies,
    so most data sets will flatten the temporal realities. You need to know exactly
    how they do so.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你需全面了解数据，对于上游管道的工作人员来说，他们并不知道你为分析准备了什么。尽可能地亲自评估时间戳生成的方式。因此，如果你正在分析来自移动应用管道的数据，请下载该应用，在各种情况下触发事件，并查看你自己的数据是什么样子。在与管理数据管道的人交谈后，你可能会对你的行为如何记录感到惊讶。追踪多个时钟和不同情况很困难，因此大多数数据集会简化时间的真实情况。你需要确切了解它们的处理方式。
- en: Guesstimating Timestamps to Make Sense of Data
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 猜测时间戳以理解数据含义
- en: 'If you are dealing with legacy data pipelines or undocumented data, you may
    not have the option of exploring a working pipeline and talking to those who maintain
    it. You will need to do some empirical investigation to understand whether you
    can make inferences about what the timestamps mean:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理遗留数据管道或未记录的数据，你可能无法探索工作中的管道或与维护它的人交谈的选项。你需要进行一些实证调查，以了解是否能推断出时间戳的含义：
- en: Reading the data as we did in the previous example, you can generate initial
    hypotheses about what the timestamps mean. In the preceding case, look at data
    for multiple users to see whether the same pattern (multiple rows with identical
    timestamps and improbable single meal contents) held or whether this was an anomaly.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如同前面的示例中所做的那样阅读数据，你可以对时间戳含义产生初步的假设。在前述情况下，查看多个用户的数据，看看是否存在相同模式（具有相同时间戳和不太可能的单一餐内容的多行），或者这是否是个异常情况。
- en: 'Using aggregate-level analyses, you can test hypotheses about what timestamps
    mean or probably mean. For the preceding data, there are a couple of open questions:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚合级别分析，你可以测试关于时间戳含义或可能含义的假设。对于前面的数据，存在一些未解的问题：
- en: Is the timestamp local or universal time?
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间戳是本地时间还是世界协调时间（UTC）？
- en: Does the time reflect a user action or some external constraint, such as connectivity?
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间是否反映了用户的操作或某种外部约束，比如连接性？
- en: Local or Universal Time?
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地时间还是世界协调时间？
- en: Most timestamps are stored in universal (UTC) time or in a single time zone,
    depending on the server’s location but independent of the user’s location. It
    is quite unusual to store data according to local time. However, we should consider
    both possibilities, because both are found in “the wild.”
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时间戳都以世界协调时间（UTC）或单一时区存储，这取决于服务器的位置，而与用户的位置无关。按照本地时间存储数据是相当不寻常的。但我们应该考虑这两种可能性，因为“野外”中都有这两种情况。
- en: We form the hypothesis that if the time is a local timestamp (local to each
    user), we should see daily trends in the data reflecting daytime and nighttime
    behavior. More specifically, we should expect not to see much activity during
    the night when our users are sleeping. For our mobile app example, if we create
    a histogram of meal time counts per hour, there should be hours with significantly
    fewer meals logged, because in most cultures people don’t eat in the middle of
    the night.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设，如果时间是本地时间戳（每个用户的本地时间），我们应该在数据中看到反映白天和黑夜行为的日常趋势。更具体地说，我们应该期待在夜间，即使在大多数文化中人们不会在深夜吃饭的时段，也应该看到餐次数显著减少。对于我们移动应用程序的示例，如果我们创建一天内每小时餐次计数的直方图，应该会发现几个小时内记录的餐次较少。
- en: If we did not see a day-oriented pattern in the hours as displayed, we could
    conclude that the data was most likely timestamped by some universal clock and
    that the user base must be spread out across many time zones. In such a case,
    it would be quite challenging to extrapolate individual users’ local hours (assuming
    time zone information wasn’t available). We might consider individual explorations
    per user to see if we could code heuristics to label users approximately by time
    zone, but such work is both computationally taxing and also not always accurate.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在显示的小时内没有看到以日为导向的模式，我们可以得出结论，数据很可能是由某个全球时钟标记的，并且用户群体必须分布在许多不同的时区。在这种情况下，推断个别用户的当地时间将非常具有挑战性（假设没有时区信息）。我们可能会考虑每个用户的个别探索，看看是否可以编写启发式代码来近似标记用户的时区，但这样的工作既计算密集又不总是准确。
- en: 'Even if you cannot pinpoint an exact user time zone, it’s still useful to have
    a global timestamp available. For starters, you can determine likely usage patterns
    for your app’s servers, knowing when meals are most frequently logged at a given
    time of day and day of the week. You can also calculate the time differential
    between each meal the user recorded, knowing that since the timestamps are in
    absolute time, you don’t need to worry about whether the user changed time zones.
    In addition to sleuthing, this would be interesting as a form of feature generation:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你无法准确确定用户的时区，拥有全局时间戳仍然很有用。首先，你可以确定你的应用服务器在何时何地最频繁记录饭菜时间。你还可以计算用户记录的每餐之间的时间差异，因为时间戳是绝对时间，所以不需要担心用户是否更改了时区。除了侦探工作之外，这也是一种有趣的特征生成方式：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `dt` column would be a feature you could pass forward in your analysis.
    Using this time differential could also give you an opportunity to estimate each
    user’s time zone. You could look at the time of day when the user generally had
    a long `dt`, which could point to when nighttime occurs for that user. From there
    you could begin to figure out each individual’s “nighttime” without having to
    do peak-to-peak-style analysis.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`dt`列将是你可以在分析中使用的特征。使用这种时间差异也可以让你有机会估计每个用户的时区。你可以查看用户通常具有长`dt`的时间，这可能指向该用户的夜间发生时间。从那里开始，你可以开始确定每个人的“夜间”时间，而不必进行峰对峰分析。'
- en: User behavior or network behavior?
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户行为还是网络行为？
- en: To return to another question posed by our short data sample, we asked whether
    our user had a strange meal or whether our timestamps relate to upload activity.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们短数据样本提出的另一个问题，我们问的是我们的用户是否吃了奇怪的饭菜，或者我们的时间戳是否与上传活动相关。
- en: The same analyses used to pinpoint a user’s time zone are applicable to determining
    whether the timestamps are a function of user or network behavior. Once you had
    a `dt` column (as previously calculated), you could look for clusters of 0s and
    qualitatively determine whether they were likely a single behavioral event or
    a single network event. You could also look at whether the `dt`s appeared to be
    periodic across different days. If they were a function of user behavior, they
    would be more likely to be periodic than if they were a function of network connectivity
    or other software-related behaviors.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 用来确定用户时区的相同分析方法也适用于确定时间戳是否是用户或网络行为的功能。一旦你有了`dt`列（如先前计算的），你可以寻找0值的聚类，并且可以定性地确定它们是单个行为事件还是单个网络事件。你还可以查看`dt`是否在不同的日期间隔内表现出周期性。如果它们是用户行为的功能，它们更可能是周期性的，而不是网络连接或其他软件相关行为的功能。
- en: 'To summarize, here are some questions you could likely address with the available
    data set, even with little or no information about how timestamps are generated:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，以下是一些你可能能够用现有数据集解决的问题，即使几乎没有关于时间戳如何生成的信息：
- en: Use differences in timestamps per user to get a sense of spacing between meals
    or spacing between data entries (depending on your working hypothesis of what
    the times indicate, a user behavior or a network behavior).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用每个用户的时间戳差异来了解餐食间隔或数据条目间隔（根据你的工作假设，时间指示用户行为或网络行为）。
- en: Describe aggregate user behavior to determine when your servers are most likely
    to be active in the 24-hour cycle.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述聚合用户行为，以确定在24小时周期内你的服务器最可能活跃的时间。
- en: What’s a Meaningful Time Scale?
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是有意义的时间尺度？
- en: You should take the temporal resolution of the timestamping you receive with
    a grain of salt, based on domain knowledge about the behavior you are studying
    and also based on the details you can determine relating to how the data was collected.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该基于关于您正在研究的行为的领域知识以及您能确定的与数据收集方式相关的细节，对您接收到的时间戳的时间分辨率持保留态度。
- en: For example, imagine you are looking at daily sales data, but you know that
    in many cases managers wait until the end of the week to report figures, estimating
    rough daily numbers rather than recording them each day. The measurement error
    is likely to be substantial due to recall problems and innate cognitive biases.
    You might consider changing the resolution of your sales data from daily to weekly
    to reduce or average out this systematic error. Otherwise, you should build a
    model that factors in the possibility of biased errors across different days of
    the week. For example, it may be that managers systematically overestimate their
    Monday performance when they report the numbers on a Friday.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下您正在查看日销售数据，但您知道在许多情况下，经理们会等到周末报告数据，估计粗略的日常数字而不是每天记录它们。由于回忆问题和内在认知偏差，测量误差可能会相当大。您可以考虑将销售数据的分辨率从日常更改为每周，以减少或平均这种系统误差。否则，您应该构建一个模型，考虑不同工作日之间可能存在的偏误。例如，可能经理在星期五报告数据时系统性地高估他们的星期一业绩。
- en: Psychological Time Discounting
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 心理时间贴现
- en: '*Time discounting* is a manifestation of a phenomenon known as *psychological
    distance*, which names our tendency to be more optimistic (and less realistic)
    when making estimates or assessments that are more “distant” from us. Time discounting
    predicts that data reported from further in the past will be biased systematically
    compared to data reported from more recent memory. This is distinct from the more
    general problem of forgetting and implies a nonrandom error. You should keep this
    in mind whenever you are looking at human-generated data that was entered manually
    but not contemporaneously with the event recorded.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*时间贴现*是一种称为*心理距离*的现象的表现，它描述了我们在进行距离较远的估算或评估时更加乐观（也更不现实）的倾向。时间贴现预测，与最近记忆中记录的数据相比，从较远过去报告的数据将系统性地存在偏差。这与更普遍的遗忘问题不同，并暗示了非随机误差。每当您查看手动输入但未与记录事件同时发生的人工生成数据时，请记住这一点。'
- en: Another situation involves physical knowledge of the system. For example, there
    is a limit to how quickly a person’s blood glucose can change, so if you are looking
    at a series of blood glucose measurements within seconds of one another, you should
    probably average them rather than treating them as distinct data points. Any physician
    will tell you that you are studying the error of the device rather than the rate
    of change of blood glucose if you are looking at many measurements within a few
    seconds of one another.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种情况涉及对系统的物理知识。例如，一个人的血糖水平变化速度存在一定的限制，因此，如果您在几秒钟内查看一系列血糖测量值，您可能应该对它们进行平均处理，而不是将它们视为不同的数据点。任何医生都会告诉您，如果您在几秒钟内查看了许多测量值，那么您正在研究设备的误差而不是血糖变化的速率。
- en: Humans Know Time Is Passing
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类知道时间在流逝
- en: Whenever you are measuring humans, keep in mind that people respond in more
    than one way to the passage of time. For example, recent research shows how manipulating
    the speed of a clock that is in a person’s view influences how quickly that person’s
    blood glucose level changes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 每当您在测量人类时，请记住人们对时间的流逝有多种反应方式。例如，最近的研究显示，调整一个人视野中的时钟速度会影响该人血糖水平变化的速度。
- en: Cleaning Your Data
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗您的数据
- en: 'In this section we will tackle the following common problems in time series
    data sets:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解决时间序列数据集中的以下常见问题：
- en: Missing data
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失数据
- en: Changing the frequency of a time series (that is, upsampling and downsampling)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改时间序列的频率（即上采样和下采样）
- en: Smoothing data
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑数据
- en: Addressing seasonality in data
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据的季节性
- en: Preventing unintentional lookaheads
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止无意识的超前看法
- en: Handling Missing Data
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: 'Missing data is surprisingly common. For example, in healthcare, missing data
    in medical time series can have a number of causes:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据非常普遍。例如，在医疗保健领域，医疗时间序列中的缺失数据可能有多种原因：
- en: The patient didn’t comply with a desired measurement.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者未遵守所需的测量。
- en: The patient’s health stats were in good shape, so there was no need to take
    a particular measurement.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者的健康统计数据很好，因此不需要进行特定的测量。
- en: The patient was forgotten or undertreated.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者被遗忘或未经适当治疗。
- en: A medical device had a random technical malfunction.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗设备发生了随机技术故障。
- en: There was a data entry error.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发生了数据输入错误。
- en: 'To generalize at great peril, missing data is even more common in time series
    analysis than it is in cross-sectional data analysis because the burden of longitudinal
    sampling is particularly heavy: incomplete time series are quite common and so
    methods have been developed to deal with holes in what is recorded.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一般化地说，与横截面数据分析相比，时间序列分析中缺失数据更为常见，因为纵向采样的负担特别重：不完整的时间序列非常普遍，因此已经开发出方法来处理记录中的空缺。
- en: 'The most common methods to address missing data in time series are:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间序列缺失数据最常见的方法有：
- en: Imputation
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 插补
- en: When we fill in missing data based on observations about the entire data set.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们根据对整个数据集的观察填补缺失数据时。
- en: Interpolation
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 插值
- en: When we use neighboring data points to estimate the missing value. Interpolation
    can also be a form of imputation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用相邻数据点来估计缺失值时。插值也可以是一种插补形式。
- en: Deletion of affected time periods
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 删除受影响的时间段
- en: When we choose not to use time periods that have missing data at all.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们选择根本不使用具有缺失数据的时间段时。
- en: We will discuss imputations and interpolations as well as illustrate the mechanics
    of these methods soon. We focus on preserving data, whereas a method such as deleting
    time periods with missing data will result in less data for your model. Whether
    to preserve the data or throw out problematic time periods will depend on your
    use case and whether you can afford to sacrifice the time periods in question
    given the data needs of your model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论插补和插值，并很快展示这些方法的机制。我们关注保留数据，而像删除具有缺失数据的时间段这样的方法会导致模型的数据减少。是否保留数据或丢弃问题时间段将取决于您的用例以及考虑到模型数据需求是否能够牺牲这些时间段。
- en: Preparing a data set to test missing data imputation methodologies
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据集以测试缺失数据插补方法
- en: 'We will work with [monthly unemployment data](https://data.bls.gov/timeseries/LNS14000000)
    that has been released by the US government since 1948 (this is freely available
    for download). We will then generate two data sets from this baseline data: one
    in which data is truly missing at random, and one in which are the highest unemployment
    months in the history of the time series. This will give us two test cases to
    see how imputation behaves in the presence of both random and systematic missing
    data.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用自1948年以来由美国政府发布的[月度失业数据](https://data.bls.gov/timeseries/LNS14000000)（可免费下载）。然后我们将从这些基础数据生成两个数据集：一个是真正随机缺失数据的情况，另一个是时间序列历史上最高失业率的月份。这将为我们提供两个测试案例，以查看在随机和系统性缺失数据存在的情况下，插补的行为如何。
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We move to R for the next example. We will freely switch between R and Python
    throughout the book. I assume you have some background in working with data frames,
    as well as matrices, in both R and Python.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将转向R来进行下一个示例。在整本书中，我们将自由切换使用R和Python。我假设您在使用数据框架和R与Python中的矩阵方面具有一定的背景知识。
- en: '[PRE15]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Because we have deleted rows from our data tables to create a data set with
    missing data, we will need to read the missing dates and NA values, and for this
    the `data.table` package’s *rolling join* functionality is quite useful.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们从数据表中删除了行来创建一个带有缺失数据的数据集，所以我们需要读取缺失的日期和NA值，对于这一点，`data.table`包的*rolling
    join*功能非常有用。
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With a rolling join, we generate the sequence of all dates that should be available
    between the start and end date of the data set. This gives us rows in the data
    set to fill in as `NA`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用滚动连接，我们生成数据集开始和结束日期之间应该可用的所有日期序列。这为我们提供了数据集中要填充为`NA`的行。
- en: 'Now that we have data sets with missing values, we will take a look at a few
    specific ways to fill in numbers for those missing values:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了带有缺失值的数据集，我们将看一下填充这些缺失值的几种具体方法：
- en: Forward fill
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向填充
- en: Moving average
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动平均
- en: Interpolation
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插值
- en: We will compare the performance of these methods in both the randomly missing
    and systematically missing data sets. Since we have generated these data sets
    from a complete data set, we can in fact determine how they did instead of speculating.
    In the real world, of course, we will never have the missing data to check our
    data imputation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较这些方法在随机缺失和系统缺失数据集中的性能。由于我们从完整数据集生成了这些数据集，因此实际上可以确定它们的表现而不是推测。当然，在现实世界中，我们永远不会有缺失数据来检查我们的数据插补。
- en: Forward fill
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向填充
- en: One of the simplest ways to fill in missing values is to carry forward the last
    known value prior to the missing one, an approach known as *forward fill*. No
    mathematics or complicated logic is required. Simply consider the experience of
    moving forward in time with the data that was available, and you can see that
    at a missing point in time, all you can be confident of is what data has already
    been recorded. In such a case, it makes sense to use the most recent known measurement.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 填补缺失值的最简单方法之一是向前传递缺失点前的最后已知值，这种方法被称为*前向填充*。不需要数学或复杂的逻辑。只需考虑随着可用数据的时间推移经验，您可以看到在时间的缺失点，您唯一可以确信的是已记录的数据。在这种情况下，使用最近的已知测量是有意义的。
- en: 'Forward fill can be accomplished easily using `na.locf` from the `zoo` package:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`zoo`包中的`na.locf`轻松实现前向填充：
- en: '[PRE17]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will result in a plot that looks natural except where you see repeated
    values to account for missing data, as in [Figure 2-6](#fig-0205). As you will
    notice in the plot, the forward-filled values usually do not deviate far from
    the true values.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致一个看起来很自然的图形，除非您看到重复的数值以解决缺失数据，如在[图 2-6](#fig-0205)中所示。正如您在图中所注意到的，前向填充的数值通常不会偏离真实数值太远。
- en: '![](assets/ptsa_0206.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0206.png)'
- en: Figure 2-6\. The original time series plotted with a solid line and the time
    series with forward filled values for randomly missing points plotted with a dashed
    line. The forward filled values are marked with downward pointing triangles.
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 原始时间序列用实线绘制，以及用虚线绘制的具有随机缺失点的前向填充值的时间序列。前向填充的值标记有向下指向的三角形。
- en: We can also compare the values in the series by plotting the values of the series
    against one another. That is, for each time step, we plot the true known value
    against the value at the same time from the series with imputed values. Most values
    should match exactly since most data is present. We see that manifested in the
    1:1 line in [Figure 2-7](#fig-0206). We also see scattered points off this line,
    but they do not appear to be systematically off.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过将系列的值相互绘制来比较这些系列的值。也就是说，对于每个时间步，我们将真实已知值与具有插值值的同一时间的值进行绘制。大多数值应完全匹配，因为大多数数据是存在的。我们在[图 2-7](#fig-0206)中看到这一点体现在1:1线上。我们还看到一些点散布在该线外，但它们似乎并没有系统偏离。
- en: '![](assets/ptsa_0207.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0207.png)'
- en: Figure 2-7\. Plotting the true unemployment rate versus the forward-filled series.
    This plot shows that forward fill did not systematically distort the data.
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 绘制真实失业率与前向填充系列的图形。这张图表明，前向填充并没有系统地扭曲数据。
- en: Backward Fill
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向后填充
- en: Just as you can bring values from the past forward to fill in missing data,
    you can also choose to propagate values backward. However, this is a case of a
    lookahead, so you should only do this when you are not looking to predict the
    future from the data and when, from domain knowledge, it makes more sense to fill
    in data backward rather than forward in time.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以将过去的值带入以填补缺失数据一样，您也可以选择向后传播值。然而，这是一种预测，所以只有在您不打算从数据中预测未来，并且从领域知识来看，向后填充数据比向前填充更合理时，才应该这样做。
- en: In some settings, forward fill makes sense as the best way to complete missing
    data, even if “fancier methods” are possible. For example, in medical settings,
    a missing value often indicates that a medical worker did not think it necessary
    to remeasure a value, likely because the patient’s measurement was expected to
    be normal. In many medical cases, this means we could apply forward fill to missing
    values with the last known value since this was the presumption motivating the
    medical worker not to retake a measurement.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，前向填充是完成缺失数据的最佳方法，即使“更高级的方法”也是可能的。例如，在医疗环境中，缺失值通常表示医务人员认为不必重新测量值，可能是因为预期患者的测量值是正常的。在许多医疗情况下，这意味着我们可以应用前向填充到缺失值，使用最后已知的值，因为这是激励医务人员不重新测量的假设。
- en: 'There are many advantages to forward fill: it is not computationally demanding,
    it can be easily applied to live-streamed data, and it does a respectable job
    with imputation. We will see an example soon.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 向前填充有许多优点：它计算上不那么要求，可以轻松应用于实时流数据，并且在插补方面表现出色。我们很快会看到一个例子。
- en: Moving average
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移动平均
- en: We can also impute data with either a rolling mean or median. Known as a *moving
    average*, this is similar to a forward fill in that you are using past values
    to “predict” missing future values (imputation can be a form of prediction). With
    a moving average, however, you are using input from *multiple* recent times in
    the past.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用滚动均值或中位数来填充数据。被称为*移动平均*，它类似于向前填充，因为您使用过去的值来“预测”缺失的未来值（插补可以是一种预测形式）。然而，使用移动平均时，您将使用*多个*最近过去时间的输入。
- en: There are many situations where a moving average data imputation is a better
    fit for the task than a forward fill. For example, if the data is noisy, and you
    have reason to doubt the value of any individual data point relative to an overall
    mean, you should use a moving average rather than a forward fill. Forward filling
    could include random noise more than the “true” metric that interests you, whereas
    averaging can remove some of this noise.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，移动平均数据插补比向前填充更适合任务。例如，如果数据嘈杂，并且你有理由怀疑任何个别数据点相对于整体平均值的价值，你应该使用移动平均而不是向前填充。向前填充可能会包括比你感兴趣的“真实”度量更多的随机噪声，而平均化可以去除部分噪声。
- en: 'To prevent a lookahead, use only the data that occurred before the missing
    data point. So, your implementation would like something like this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止前瞻，仅使用发生在缺失数据点之前的数据。因此，您的实现可能会像这样：
- en: '[PRE18]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We set the value of the missing data to the mean of the values that come before
    it (because we index off the final value and use this value to determine whether
    it is missing and how to replace it).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将缺失数据的值设置为其之前值的平均值（因为我们以最终值为索引，并使用此值来确定其是否缺失以及如何替换它）。
- en: Tip
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A moving average doesn’t have to be an arithmetic average. For example, exponentially
    weighted moving averages would give more weight to recent data than to past data.
    Alternately, a geometric mean can be helpful for time series that exhibit strong
    serial correlation and in cases where values compound over time.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 移动平均不一定是算术平均。例如，指数加权移动平均会更加重视近期数据而不是过去的数据。另外，几何平均在展现强序列相关性的时间序列和值随时间复合的情况下会有帮助。
- en: 'When imputing missing data with a moving average, consider whether you need
    to know the value of the moving average with only forward-looking data, or whether
    you are comfortable building in a lookahead. If you are unconcerned about a lookahead,
    your best estimate will include points both before and after the missing data
    because this will maximize the information that goes into your estimates. In that
    case, you can implement a rolling window, as illustrated here with the `zoo` package’s
    `rollapply()` functionality:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在用移动平均插补缺失数据时，考虑您是否需要仅使用前瞻数据来了解移动平均值的值，或者是否愿意构建前瞻。如果您不关心前瞻，您的最佳估计将包括缺失数据之前和之后的点，因为这将最大化输入到您估计中的信息。在这种情况下，您可以实现一个滚动窗口，如使用`zoo`包的`rollapply()`功能所示：
- en: '[PRE19]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Using both past and future information is useful for visualizations and recordkeeping
    applications, but, as mentioned before, it is not appropriate if you are preparing
    your data to be fed into a predictive model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化和记录应用程序中使用过去和未来信息对于数据预处理很有用，但正如前面提到的，如果您正在准备将数据馈送到预测模型中，则不合适。
- en: The results for both a forward-looking moving average and a moving average computed
    with future and past data are shown in [Figure 2-8](#fig-0207).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 前瞻移动平均和使用未来和过去数据计算的移动平均的结果显示在[图 2-8](#fig-0207)中。
- en: '![](assets/ptsa_0208.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0208.png)'
- en: Figure 2-8\. The dotted line shows the moving average imputation without a lookahead,
    while the dashed line shows the moving average imputation with a lookahead. Likewise,
    the squares show the nonlookahead imputed points while the upside down triangles
    show the moving average with a lookahead.
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 虚线显示了没有前瞻的移动平均插补，而虚线则显示了具有前瞻的移动平均插补。同样，方形显示了非前瞻插补点，而倒三角形显示了具有前瞻的移动平均。
- en: A rolling mean data imputation reduces variance in the data set. This is something
    you need to keep in mind when calculating accuracy, R² statistics, or other error
    metrics. Your calculation may overestimate your model’s performance, a frequent
    problem when building time series models.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动平均数据插补可以减少数据集的方差。这是在计算准确性、R²统计或其他误差指标时需要记住的事情。你的计算可能会高估模型的性能，这是构建时间序列模型时经常遇到的问题。
- en: Using a Data Set’s Mean to Impute Missing Data
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据集的平均值来填补缺失数据
- en: In a cross-sectional context, it is common to impute missing data by filling
    in the mean or median for that variable where it is missing. While this can be
    done with time series data, it is not appropriate for most cases. Knowing the
    mean for the data set involves looking into the future…and that’s a lookahead!
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在横断面的情况下，常见的方法是通过在变量缺失的地方填入平均值或中位数来填补缺失数据。虽然这可以用于时间序列数据，但大多数情况下并不合适。知道数据集的平均值涉及看向未来……这就是预先看！
- en: Interpolation
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 插值
- en: Interpolation is a method of determining the values of missing data points based
    on geometric constraints regarding how we want the overall data to behave. For
    example, a linear interpolation constrains the missing data to a linear fit consistent
    with known neighboring points.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 插值是一种根据我们希望整体数据行为的几何约束来确定缺失数据点值的方法。例如，线性插值将缺失数据限制为与已知相邻点一致的线性拟合。
- en: Linear interpolation is particularly useful and interesting because it allows
    you to use your knowledge of how your system behaves over time. For example, if
    you know a system behaves in a linear fashion, you can build that knowledge in
    so that only linear trends will be used to impute missing data. In Bayesian speak,
    it allows you to inject a *prior* into your imputation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 线性插值特别有用和有趣，因为它允许你利用你对系统随时间行为的了解。例如，如果你知道系统以线性方式行为，你可以将这种知识应用进去，这样只有线性趋势会用于填补缺失数据。在贝叶斯术语中，它允许你向你的插补注入*先验*。
- en: 'As with a moving average, interpolation can be done such that it is looking
    at both past and future data or looking in only one direction. The usual caveats
    apply: allow your interpolation to have access to future data only if you accept
    that this creates a lookahead and you are sure this is not a problem for your
    task.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 与移动平均类似，插值可以这样做，以便同时查看过去和未来的数据，或者只查看一个方向。通常的注意事项是：如果你接受这样做会导致预先看的影响，并且你确定这对你的任务不是问题，那么允许你的插值只有未来数据访问。
- en: 'Here we apply interpolation using both past and future data points (see [Figure 2-9](#fig-0208)):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用过去和未来的数据点进行插值（参见[图2-9](#fig-0208)）：
- en: '[PRE20]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](assets/ptsa_0209.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0209.png)'
- en: Figure 2-9\. The dashed line shows the linear interpolation while the dotted
    line shows the spline interpolation.
  id: totrans-272
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9\. 虚线显示线性插值，而点线显示样条插值。
- en: There are many situations where a linear (or spline) interpolation is appropriate.
    Consider mean average weekly temperature where there is a known trend of rising
    or falling temperatures depending on the time of year. Or consider yearly sales
    data for a growing business. If the trend has been for business volume to increase
    linearly from year to year, it is a reasonable data imputation to fill in missing
    data based on that trend. In other words, we could use a linear interpolation,
    which would account for the trend, rather than a moving average, which would not.
    If there were a trend of increasing values, the moving average would systematically
    underestimate the missing values.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多情况适合使用线性（或样条）插值。考虑平均每周温度，其中根据一年中的时间，有升高或降低温度的已知趋势。或者考虑增长中的企业的年销售数据。如果趋势是每年业务量线性增长，基于该趋势填补缺失数据是合理的数据插补。换句话说，我们可以使用线性插值，这将考虑到趋势，而不是移动平均，它将不会。如果存在增值趋势，则移动平均将系统地低估缺失值。
- en: There are also plenty of situations for which linear (or spline) interpolation
    is not appropriate. For example, if you are missing precipitation data in a weather
    data set, you should not extrapolate linearly between the known days; as we all
    know, that’s not how precipitation works. Similarly, if we are looking at someone’s
    hours of sleep per day but are missing a few days of data, we should not linearly
    extrapolate the hours of sleep between the last known days. As an example, one
    of the known endpoints might include an all-nighter of studying followed by a
    30-minute catnap. This is unlikely to estimate the missing data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 也有很多情况不适合线性（或样条）插值。例如，如果您在天气数据集中缺少降水数据，您不应该在线性插值已知天数之间外推；我们都知道，降水并不是这样工作的。同样，如果我们看的是某人每天的睡眠小时数，但是缺少了几天的数据，我们不应该在线性外推睡眠小时数之间。例如，已知的端点之一可能包括熬夜学习后的30分钟小睡。这不太可能估算缺失的数据。
- en: Overall comparison
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体比较
- en: Now that we have performed a few different kinds of imputations, we can take
    a look at the results to compare how the different data imputations behaved on
    this data set.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经进行了几种不同类型的插补，我们可以查看结果，比较这些不同的数据插补在这个数据集上的表现如何。
- en: 'We generated two data sets with missing data, one in which data was missing
    at random and another in which unfavorable data points (high unemployment) were
    missing. When we compare the methods we employed to see which yields the best
    results, we can see that the mean square error can differ by a high percentage:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了两个具有缺失数据的数据集，一个是随机缺失数据，另一个是缺失了不利数据点（高失业率）。当我们比较我们采用的方法以确定哪种方法产生了最佳结果时，我们可以看到均方误差可能会有很大的百分比差异：
- en: '[PRE21]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Remember that many of the preceding methods include a lookahead. The only methods
    that do not are the forward fill and the moving average without a lookahead (there
    is also a moving average with a lookahead). For this reason, it’s not surprising
    that there is a range of difference in errors and that the methods without a lookahead
    do not perform as well as the others.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，前述的许多方法都包含了前瞻性。唯一不包含前瞻性的方法是前向填充和没有前瞻性的移动平均（也有带有前瞻性的移动平均）。因此，不足为奇的是在错误方面存在一系列差异，并且没有前瞻性的方法表现不如其他方法。
- en: Final notes
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最终备注
- en: 'Here we covered the simplest and most often-used methods of missing data imputation
    for time series applications. Data imputation remains an important area of data
    science research. The more significant the decisions you are making, the more
    important it is to carefully consider to the potential reasons for data to be
    missing and the potential ramifications of your corrections. Here are a few cautionary
    tips you should keep in mind:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们涵盖了时间序列应用中最简单和最常用的缺失数据插补方法。数据插补仍然是数据科学研究的一个重要领域。你做出的决策越重要，就越需要仔细考虑数据缺失的潜在原因及其修正可能带来的潜在影响。以下是你应该记住的一些谨慎提示：
- en: It is impossible to prove that data is truly missing at random, and it is unlikely
    that missingness is truly random in most real-world occurrences.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法证明数据真的是随机缺失，而且在大多数真实世界的情况下，缺失并不真的是随机的，这也不足为奇。
- en: Sometimes the probability that a measurement is missing is explainable by the
    variables you have measured, but sometimes not. Wide data sets with many features
    are the best way to investigate possible explanations for patterns of missing
    data, but these are not the norm for time series analysis.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时测量数据缺失的概率可以通过您已经测量的变量来解释，但有时则不然。具有许多特征的广泛数据集是调查缺失数据模式可能解释的最佳方式，但这并不是时间序列分析的常规方法。
- en: When you need to understand the uncertainty introduced by imputing values to
    missing data, you should run through a variety of scenarios and also speak to
    as many people involved in the data collection process as possible.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您需要了解将缺失数据插入值引入的不确定性时，您应该运行各种场景，并尽可能与参与数据收集过程的人交流。
- en: How you handle missing data should account for your downstream use of that data.
    You must guard carefully against lookaheads or decide how seriously a lookahead
    will affect the validity of your subsequent work.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该考虑如何处理缺失数据以适应后续数据使用。您必须小心防止前瞻性，或者决定前瞻性将如何严重影响您后续工作的有效性。
- en: Upsampling and Downsampling
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上采样和下采样
- en: Often, related time series data from different sources will not have the same
    sampling frequency. This is one reason, among many, that you might wish to change
    the sampling frequency of your data. Of course you cannot change the actual rate
    at which information was measured, but you can change the frequency of the timestamps
    in your data collection. This is called *upsampling* and *downsampling*, for increasing
    or decreasing the timestamp frequency, respectively.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 常常来自不同来源的相关时间序列数据将不具有相同的采样频率。这是你可能希望改变数据采样频率的众多原因之一。当然，你不能改变信息实际被测量的速率，但你可以改变数据收集中时间戳的频率。这被称为*上采样*和*下采样*，分别用于增加或减少时间戳的频率。
- en: We downsampled temporal data in [“Retrofitting a Time Series Data Collection
    from a Collection of Tables”](#retrofitting-sect). Here, we address the topic
    more generally, learning the how’s and why’s of both downsampling and upsampling.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[“从一组表中重新调整时间序列数据收集”](#retrofitting-sect)中对时间数据进行了降采样。在这里，我们更加通用地讨论了降采样和上采样的如何和为什么。
- en: Note
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Downsampling is subsetting data such that the timestamps occur at a lower frequency
    than in the original time series. Upsampling is representing data as if it were
    collected more frequently than was actually the case.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 降采样是将数据子集化，使时间戳的频率低于原始时间序列。上采样则是将数据表示为如果它被更频繁地收集的情况。
- en: Downsampling
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降采样
- en: Anytime you reduce frequency of your data, you are downsampling. This is most
    often done in the following cases.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 任何时候，当你降低数据的频率，你就在进行**降采样**。这通常发生在以下情况下。
- en: The original resolution of the data isn’t sensible
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据的原始分辨率不合理
- en: There can be many reasons that the original granularity of the data isn’t sensible.
    For example, you may be measuring something too often. Suppose you have a data
    set where someone had measured the outside air temperature every second. Common
    experience dictates that this measurement is unduly frequent and likely offers
    very little new information relative to the additional data storage and processing
    burden. In fact, it’s likely the measurement error could be as large as the second-to-second
    air temperature variation. So, you likely don’t want to store such excessive and
    uninformative data. In this case—that is, for regularly sampled data—downsampling
    is as simple as selecting out every *n*th element.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据粒度不合理的原因很多。例如，你可能测量得太频繁了。假设你有一个数据集，其中某人每秒测量一次室外空气温度。常识告诉我们，这种测量过于频繁，相对于额外的数据存储和处理负担，可能提供的新信息非常有限。事实上，测量误差可能与秒与秒之间的空气温度变化一样大。因此，你可能不希望存储这种过多且无信息量的数据。在这种情况下——也就是对定期采样的数据来说——降采样就像是选择每
    *n* 个元素一样简单。
- en: Focus on a particular portion of a seasonal cycle
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关注季节周期的特定部分
- en: Instead of worrying about seasonal data in a time series, you might choose to
    create a subseries focusing on only one season. For example, we can apply downsampling
    to create a subseries, as in this case, where we generate a time series of January
    measurements out of what was originally a monthly time series. In the process,
    we have downsampled the data to a yearly frequency.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列中不必担心季节数据，你可以选择创建一个只关注一个季节的子系列。例如，我们可以应用降采样来创建一个子系列，就像在这种情况下，我们从原始的月度时间序列中生成了一组一月份的测量值。在这个过程中，我们将数据降低到了年度频率。
- en: '[PRE22]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Match against data at a lower frequency
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与低频率数据进行匹配
- en: You may want to downsample data so that you can match it with other low-frequency
    data. In such cases you likely want to aggregate the data or downsample rather
    than simply dropping points. This can be something simple like a mean or a sum,
    or something more complicated like a weighted mean, with later values given more
    weight. We saw earlier in the donation data the idea of summing all donations
    over a single week, since it was the total amount donated that was likely to be
    most interesting.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望降低数据的频率，以便与其他低频数据匹配。在这种情况下，你可能希望对数据进行聚合或降采样，而不仅仅是丢弃数据点。这可以是简单的平均值或总和，也可以是更复杂的加权平均，后续值具有更高的权重。我们之前在捐赠数据中看到，将一周内的所有捐款总额相加的想法可能更加有趣。
- en: 'In contrast, for our economic data, what is most likely to be interesting is
    a yearly average. We use a mean instead of a rolling mean because we want to summarize
    the year rather than get the latest value of that year to emphasize recency. (Note
    the difference from data imputation.) We group by formatting the date into a string,
    representing its year as an example of how you can creatively exploit SQL-like
    operations for time series functionality:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，对于我们的经济数据，最有可能感兴趣的是年均值。我们使用均值而不是滚动均值，因为我们想要总结一年的情况，而不是获取该年的最新值，强调的是最近性。我们通过将日期格式化为字符串，将其年份作为例子来分组，展示如何可以创造性地利用类似SQL的操作进行时间序列功能：
- en: '[PRE23]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Upsampling
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上采样
- en: 'Upsampling is not simply the inverse of downsampling. Downsampling makes inherent
    sense as something that can be done in the real world; it’s simple to decide to
    measure less often. In contrast, upsampling can be like trying to get something
    for free—that is, not taking a measurement but still somehow thinking you can
    get high-resolution data from infrequent measurements. To quote the author of
    the popular `R` time series package [`XTS`](https://perma.cc/83E9-4N79):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样并不简单等同于下采样的逆过程。下采样在现实世界中是有意义的；决定更少频繁地进行测量是简单的。相比之下，上采样可能就像试图白嫖一样——也就是说，不进行实时测量却希望从不频繁的测量中获取高分辨率数据。引用流行的`R`时间序列包[`XTS`](https://perma.cc/83E9-4N79)的作者的话：
- en: It is not possible to convert a series from a lower periodicity to a higher
    periodicity - e.g., weekly to daily or daily to 5 minute bars, as that would require
    magic.
  id: totrans-304
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不可能将低周期性的系列转换为高周期性的系列 - 例如，从每周到每日或每日到5分钟柱，因为那需要魔法。
- en: However, there are legitimate reasons to want to label data at a higher frequency
    than its default frequency. You simply need to keep in mind the data’s limitations
    as you do so. Remember that you are adding more time labels but not more information.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有合理的理由希望以比其默认频率更高的频率标记数据。在这样做时，你需要记住数据的限制。请记住，你只是增加了更多的时间标签，而不是增加了更多信息。
- en: Let’s discuss a few situations where upsampling can make sense.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些上采样有意义的情况。
- en: Irregular time series
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不规则时间序列
- en: 'A very common reason to upsample is that you have an irregularly sampled time
    series and you want to convert it to a regularly timed one. This is a form of
    upsampling because you are converting all the data to a frequency that is likely
    higher than indicated by the lags between your data. If you are upsampling for
    this reason, you already know how to do it with a rolling join, as we did this
    in the case of filling in missing economic data, via R:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样的一个非常常见的原因是你有一个不规则采样的时间序列，你希望将其转换为定期的时间序列。这是一种上采样，因为你将所有数据转换为一个频率，这个频率可能高于数据之间滞后所指示的频率。如果出于这个原因进行上采样，你已经知道如何通过滚动连接来实现，就像我们在填补缺失的经济数据时所做的那样，通过R：
- en: '[PRE24]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Inputs sampled at different frequencies
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同频率下的输入采样
- en: Sometimes you need to upsample low-frequency information simply to carry it
    forward with your higher-frequency information in a model that requires your inputs
    to be aligned and sampled contemporaneously. You must be vigilant with respect
    to lookahead, but if we assume that known states are true until a new known state
    comes into the picture, we can safely upsample and carry our data forward. For
    example, suppose we know it’s (relatively) true that most new jobs start on the
    first of the month. We might decide we feel comfortable using the unemployment
    rate for a given month indicated by the jobs report for the *entire* month (not
    considering it a lookahead because we make the assumption that the unemployment
    rate stays steady for the month).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你仅仅需要上采样低频信息，以便将其与高频信息一起在需要输入对齐和同时采样的模型中传递。你必须对前瞻性保持警惕，但如果我们假设已知状态在出现新的已知状态之前是真实的，我们可以安全地上采样并传递我们的数据。例如，假设我们知道（相对地）大多数新的工作岗位都是在每月的第一天开始的。我们可能会决定使用给定月份的失业率来代表整个月份（不考虑它是前瞻性，因为我们假设失业率在整个月份内保持稳定）。
- en: '[PRE25]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Knowledge of time series dynamics
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间序列动态的知识
- en: If you have underlying knowledge of the usual temporal behavior of a variable,
    you may also be able to treat an upsampling problem as a missing data problem.
    In that case, all the techniques we’ve discussed already still apply. An interpolation
    is the most likely way to produce new data points, but you would need to be sure
    the dynamics of your system could justify your interpolation decision.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对变量通常的时间行为有基础知识，你也可能能够将上采样问题视为缺失数据问题。在这种情况下，我们已经讨论过的所有技术仍然适用。插值是产生新数据点的最有可能的方法，但你需要确保你的系统动态能够证明你的插值决策是合理的。
- en: As discussed earlier, upsampling and downsampling will routinely happen even
    in the cleanest data set because you will almost always want to compare variables
    of different timescales. It should also be noted that Pandas has particularly
    handy upsampling and downsampling functionality with the `resample` method.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，即使在最干净的数据集中，上采样和下采样也会经常发生，因为你几乎总是希望比较不同时间尺度的变量。还应该指出，Pandas具有特别方便的上采样和下采样功能，使用`resample`方法。
- en: Smoothing Data
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平滑数据
- en: 'Smoothing data can be done for a variety of reasons, and often real-world time
    series data is smoothed before analysis, especially for visualizations that aim
    to tell an understandable story about the data. In this section we discuss further
    why smoothing is done, as well as the most common time series smoothing technique:
    exponential smoothing.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平滑可以出于各种原因进行，尤其是在进行数据可视化之前，常常会对真实世界的时间序列数据进行平滑，以便讲述数据背后的可理解故事。在本节中，我们进一步讨论为什么进行平滑以及最常见的时间序列平滑技术：指数平滑。
- en: Purposes of smoothing
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平滑的目的
- en: While outlier detection is a topic in and of itself, if you have reason to believe
    your data should be smoothed, you can do so with a moving average to eliminate
    measurement spikes, errors of measurement, or both. Even if the spikes are accurate,
    they may not reflect the underlying process and may be more a matter of instrumentation
    problems; this is why it’s quite common to smooth data.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管异常值检测是一个独立的主题，如果你有理由认为数据应该平滑，你可以通过移动平均值来消除测量的突变，测量误差或两者都有的情况。即使这些突变是准确的，它们可能也不反映底层过程，并且可能更多地是仪器问题的问题；这就是为什么平滑数据是相当常见的。
- en: Smoothing data is strongly related to imputing missing data, and so some of
    those techniques are relevant here as well. For example, you can smooth data by
    applying a rolling mean, with or without a lookahead, as that is simply a matter
    of the point’s position relative to the window used to calculate its smoothed
    value.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑数据与填充缺失数据密切相关，因此这里也涉及到一些技术。例如，你可以通过应用滚动均值来平滑数据，无论是否有前瞻，因为这只是计算其平滑值时所用窗口的点相对位置的问题。
- en: 'When you are smoothing, you want to think about a number of questions:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在平滑数据时，你需要考虑一些问题：
- en: 'Why are you smoothing? Smoothing can serve a number of purposes:'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要平滑？平滑可以达到多种目的：
- en: Data preparation
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据准备
- en: Is your raw data unsuitable? For example, you may know very high values are
    unlikely or unphysical, but you need a principled way to deal with them. Smoothing
    is the most straightforward solution.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的原始数据是否不适用？例如，你可能知道非常高的值不太可能或不符合物理规律，但你需要一种有原则的方法来处理它们。平滑是最直接的解决方案。
- en: Feature generation
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征生成
- en: The practice of taking a sample of data, be it many characteristics about a
    person, image, or anything else, and summarizing it with a few metrics. In this
    way a fuller sample is collapsed along a few dimensions or down to a few traits.
    Feature generation is especially important for machine learning.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从数据中取样，无论是关于一个人、图像或其他任何内容的许多特征，然后用几个度量标准对其进行总结。通过这种方式，一个更全面的样本被折叠成几个维度或减少到几个特征。特征生成对机器学习尤为重要。
- en: Prediction
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测
- en: The simplest form of prediction for some kinds of processes is mean reversion,
    which you get by making predictions from a smoothed feature.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对某些类型的过程来说，最简单的预测形式是均值回归，你可以通过从平滑特征进行预测来获得这种形式。
- en: Visualization
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可视化
- en: Do you want to add some signal to what seems like a noisy scatter plot? If so,
    what is your intention in doing so?
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你想要在看似杂乱的散点图中增加一些信号吗？如果是这样，你这样做的意图是什么？
- en: How will your outcomes be affected by smoothing or not smoothing?
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑或不平滑将如何影响你的结果？
- en: Does your model assume noisy and uncorrelated data, whereby your smoothing could
    compromise this assumption?
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的模型是否假设数据是嘈杂和不相关的，从而使你的平滑可能会影响这种假设？
- en: Will you need to smooth in a live production model? If so, you need to choose
    a smoothing method that does not employ a lookahead.
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实时生产模型中是否需要平滑？如果是的话，您需要选择一种不使用前瞻的平滑方法。
- en: Do you have a principled way to smooth, or will you simply do a hyperparameter
    grid search? If the latter, how will you make sure that you use a time-aware form
    of cross-validation such that future data does not leak backward in time?
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您是否有一个原则性的平滑方法，或者您将只进行超参数网格搜索？如果是后者，您将如何确保使用时间感知的交叉验证形式，以确保未来的数据不会向过去泄漏？
- en: Exponential smoothing
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指数平滑
- en: You often won’t want to treat all time points equally when smoothing. In particular,
    you may want to treat more recent data as more informative data, in which case
    exponential smoothing is a good option. In contrast to the moving average we looked
    at before—where each point where data was missing could be imputed to the mean
    of its surrounding points—exponential smoothing is geared to be more temporally
    aware, weighting more recent points higher than less recent points. So, for a
    given window, the nearest point in time is weighted most heavily and each point
    earlier in time is weighted exponentially less (hence the name).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑时，通常不会将所有时间点等同看待。特别是，您可能希望将最近的数据视为信息更多的数据，这种情况下指数平滑是一个不错的选择。与我们之前讨论过的移动平均相比——在那种情况下，每个数据缺失的点可以被补充为其周围点的平均值——指数平滑更加关注时间性，将更近期的点权重更高，而不太近期的点权重则指数级减少（因此得名）。
- en: 'The mechanics of exponential smoothing work as follows. For a given time period
    *t*, you find the smoothed value of a series by computing:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 指数平滑的工作原理如下。对于给定的时间段 *t*，通过计算来找到系列的平滑值：
- en: Smoothed value at time *t* <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo>
    <msub><mi>S</mi> <mi>t</mi></msub> <mo>=</mo> <mi>d</mi> <mo>×</mo> <mo>;</mo>
    <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>–</mo> <mi>d</mi> <mo>)</mo></mrow> <mo>×</mo>
    <msub><mi>x</mi> <mi>t</mi></msub></mrow></math>
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 时间 *t* 的平滑值 <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo>
    <msub><mi>S</mi> <mi>t</mi></msub> <mo>=</mo> <mi>d</mi> <mo>×</mo> <mo>;</mo>
    <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>–</mo> <mi>d</mi> <mo>)</mo></mrow> <mo>×</mo>
    <msub><mi>x</mi> <mi>t</mi></msub></mrow></math>
- en: 'Think about how this propagates over time. The smoothed value at time (*t*
    – 1) is itself a product of the same thing:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这如何随时间传播。时间 (*t* – 1) 的平滑值本身就是同样东西的产物：
- en: <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>d</mi> <mo>×</mo> <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>–</mo> <mi>d</mi> <mo>)</mo></mrow>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>d</mi> <mo>×</mo> <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>–</mo> <mi>d</mi> <mo>)</mo></mrow>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
- en: 'So we can see a more complex expression for the smoothed value at time *t*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到在时间 *t* 的平滑值有一个更复杂的表达式：
- en: <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi> <mo>×</mo>
    <mrow><mo>(</mo> <mi>d</mi> <mo>×</mo> <msub><mi>S</mi> <mrow><mi>t</mi><mo>–</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>–</mo> <mi>d</mi> <mo>)</mo></mrow>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>d</mi>
    <mo>)</mo></mrow> <mo>×</mo> <msub><mi>x</mi> <mi>t</mi></msub></mrow></math>
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi> <mo>×</mo>
    <mrow><mo>(</mo> <mi>d</mi> <mo>×</mo> <msub><mi>S</mi> <mrow><mi>t</mi><mo>–</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>–</mo> <mi>d</mi> <mo>)</mo></mrow>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>d</mi>
    <mo>)</mo></mrow> <mo>×</mo> <msub><mi>x</mi> <mi>t</mi></msub></mrow></math>
- en: 'Mathematically inclined readers will notice that we have a series of the form:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 数学倾向的读者会注意到我们有以下形式的系列：
- en: <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>d</mi> <mn>3</mn></msup>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>3</mn></mrow></msub>
    <mo>+</mo> <msup><mi>d</mi> <mn>2</mn></msup> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mi>d</mi> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>d</mi> <mn>3</mn></msup>
    <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>3</mn></mrow></msub>
    <mo>+</mo> <msup><mi>d</mi> <mn>2</mn></msup> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mi>d</mi> <mo>×</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
- en: In fact, it is thanks to this form that exponential moving averages are quite
    tractable. More details are widely available online and in textbooks; see my favorite
    summaries in [“More Resources”](#more-resources-02).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，正是因为这种形式，指数移动平均相当容易处理。更多详细信息可以在在线和教科书中广泛找到；参见我的最喜爱的摘要在[“更多资源”](#more-resources-02)中。
- en: I will illustrate smoothing in Python, as Pandas includes a variety of smoothing
    options. Smoothing options are also widely available in R, including base R, as
    well as many time series packages.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在 Python 中进行平滑演示，因为 Pandas 包含多种平滑选项。平滑选项在 R 中也广泛可用，包括基础 R，以及许多时间序列包中。
- en: 'While we have been looking at US unemployment rate data, we will switch to
    another commonly used data set: the airline passenger data set (which dates back
    to Box and Jenkins’s famous time series book and is widely available). The original
    data set is an account of the thousands of monthly airline passengers broken down
    by month:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们一直在看美国失业率数据，我们将转向另一个常用的数据集：航空公司乘客数据集（追溯到 Box 和 Jenkins 的著名时间序列书籍并广泛可用）。原始数据集记录了按月分解的数千名航空公司乘客：
- en: '[PRE26]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can easily smooth the values of passengers using a variety of decay factors
    and applying the Pandas `ewma()` function, like so:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用各种衰减因子以及应用 Pandas 的 `ewma()` 函数轻松地平滑乘客的值，如下所示：
- en: '[PRE27]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As we can see, the level of the `alpha` parameter, also called the *smoothing
    factor*, affects how much the value is updated to its current value versus retaining
    information from the existing average. The higher the `alpha` value, the more
    quickly the value is updated closer to its current price. Pandas accepts a number
    of parameters, all of which plug into the same equation, but they offer multiple
    ways to think about specifying an exponential moving average.^([3](ch02.html#idm45576049612264))
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，`alpha` 参数的水平，也称为*平滑系数*，影响值更新到其当前值与保留现有平均值信息的程度。`alpha` 值越高，值就越快地更新到其当前价格。Pandas
    接受多个参数，所有这些参数都插入相同的方程，但它们提供了多种方式来思考如何指定指数移动平均数。^([3](ch02.html#idm45576049612264))
- en: '[PRE28]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: However, simple exponential smoothing does not perform well (for prediction)
    in the case of data with a long-term trend. Holt’s Method and Holt–Winters smoothing
    , are two exponential smoothing methods applied to data with a trend, or with
    a trend and seasonality.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，简单的指数平滑在具有长期趋势的数据（用于预测）中表现不佳。霍尔特方法和霍尔特-温特斯平滑是两种应用于具有趋势或趋势和季节性数据的指数平滑方法。
- en: There are many other widely used smoothing techniques. For example, Kalman filters
    smooth the data by modeling a time series process as a combination of known dynamics
    and measurement error. LOESS (short for “locally estimated scatter plot smoothing”)
    is a nonparametric method of locally smoothing data. These methods, and others,
    gradually offer more complex ways of understanding smoothing but at increased
    computational cost. Of note, Kalman and LOESS incorporate data both earlier and
    later in time, so if you use these methods keep in mind the leak of information
    backward in time, as well as the fact that they are usually not appropriate for
    preparing data to be used in forecasting applications.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多广泛使用的平滑技术。例如，卡尔曼滤波器通过将时间序列过程建模为已知动态和测量误差的组合来平滑数据。LOESS（局部估计散点平滑）是一种非参数方法，用于局部平滑数据。这些方法及其他方法逐渐提供了更复杂的理解平滑的方式，但计算成本也随之增加。需要注意的是，卡尔曼滤波和LOESS都包含了早期和晚期的数据，因此如果使用这些方法，要注意信息向时间向后的泄露，以及它们通常不适合用于准备用于预测应用的数据。
- en: Smoothing is a commonly used form of forecasting, and you can use a smoothed
    time series (without lookahead) as one easy null model when you are testing whether
    a fancier method is actually producing a successful forecast.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑是一种常用的预测形式，当你测试更复杂的方法是否实际上产生成功的预测时，可以使用平滑后的时间序列（无前瞻）作为一种简单的空模型。
- en: Seasonal Data
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 季节性数据
- en: Seasonality in data is any kind of recurring behavior in which the frequency
    of the behavior is stable. It can occur at many different frequencies at the same
    time. For example, human behavior tends to have a daily seasonality (lunch at
    the same time every day), a weekly seasonality (Mondays are similar to other Mondays),
    and a yearly seasonality (New Year’s Day has low traffic). Physical systems also
    demonstrate seasonality, such as the period the Earth takes to revolve around
    the sun.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的季节性是任何一种频率稳定的重复行为。它可以在同一时间发生多种不同的频率。例如，人类行为倾向于具有每日季节性（每天固定时间吃午餐），每周季节性（星期一类似于其他星期一）和每年季节性（元旦交通量较低）。物理系统也展示了季节性，比如地球绕太阳公转的周期。
- en: Identifying and dealing with seasonality is part of the modeling process. On
    the other hand, it’s also a form of data cleaning, such as the economically important
    [US Jobs Report](https://perma.cc/GX6J-QJG9). Indeed, many government statistics,
    particularly economic ones, are deseasonalized in their released form.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 辨别和处理季节性是建模过程的一部分。另一方面，它也是一种数据清理的形式，例如经济上重要的[美国就业报告](https://perma.cc/GX6J-QJG9)。事实上，许多政府统计数据，特别是经济数据，在发布时都会进行非季节性调整。
- en: To see what seasonal data smoothing can do, we return to the canonical data
    set on airline passenger counts. A plot quickly reveals that this is highly seasonal
    data, but only if you make the right plot.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解季节性数据平滑能做什么，我们回到经典的航空公司乘客计数数据集。通过绘图可以快速显示这是高度季节性的数据，但前提是你制作正确的图表。
- en: Notice the difference between using R’s default plot (which uses points; see
    [Figure 2-10](#fig-0209)) versus adding the argument to indicate that you want
    a line ([Figure 2-11](#fig-0210)).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用 R 的默认图（使用点; 参见[图2-10](#fig-0209)）与添加参数来指示你想要一条线的区别（参见[图2-11](#fig-0210)）。
- en: '![](assets/ptsa_0210.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0210.png)'
- en: Figure 2-10\. The increasing mean and variance of the data are apparent from
    a scatter plot, but we do not see an obvious seasonal trend.
  id: totrans-362
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 从散点图中明显可见数据均值和方差的增加，但我们看不到明显的季节性趋势。
- en: '![](assets/ptsa_0211.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0211.png)'
- en: Figure 2-11\. A line plot makes the seasonality abundantly clear.
  id: totrans-364
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 线图清楚地显示了季节性。
- en: If you were looking only at the default R plot, the seasonal nature of the data
    might elude you. Hopefully that wouldn’t remain the case for long, as no doubt
    you would also do things to explore your data more fully, perhaps with an autocorrelation
    plot (discussed in [Chapter 3](ch03.html#exploratory_data_analysis_for_time_series))
    or other diagnostics.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仅查看默认的 R 图表，可能会忽略数据的季节性特征。希望这种情况不会持续太久，毫无疑问，您还会做其他探索数据的工作，也许是使用自相关图（在[第 3
    章](ch03.html#exploratory_data_analysis_for_time_series)中讨论）或其他诊断工具。
- en: Humans Are Creatures of Habit
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类是习惯的动物。
- en: With human behavior data there is almost always some form of seasonality, even
    with several cycles (an hourly pattern, a weekly pattern, a summer-winter pattern,
    etc.).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 人类行为数据几乎总是具有某种形式的季节性，即使有几个周期（每小时的模式，每周的模式，夏冬季的模式等）。
- en: The scatter plot does show some information more clearly than the line plot.
    The variance of our data is increasing, as is the mean, which is most obvious
    when we see a cloud of data points beaming outward in a conical shape, tilted
    upward. This data clearly has a trend, and so we would likely transform it with
    a log transform or differencing, depending on the demands of our model. This data
    also clearly has a trend of increasing variance. We’ll talk more about model-specific
    data transformations on seasonal data in the modeling chapters, so we won’t elaborate
    here.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图确实比线图更清楚地显示了一些信息。我们的数据的方差正在增加，均值也在增加，当我们看到数据点以锥形向外散布时，这一点尤为明显，倾向于向上。这些数据显然具有增长趋势，因此我们可能会对其进行对数转换或差分处理，具体取决于我们模型的需求。这些数据也显然具有增加方差的趋势。在建模章节中，我们将更详细地讨论特定于模型的季节性数据转换，所以这里不再详细说明。
- en: We also get useful information apart from the evidence for seasonality from
    the line plot. We get information about what *kind* of seasonality. That is, we
    see that the data is not only seasonal, but seasonal in a multiplicative way.
    As the overall values get larger, so do the season swings (think of this as sizing
    from peak to trough).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 除了线图中关于季节性的证据外，我们还获得了有用的信息。我们了解到季节性的种类。也就是说，我们看到数据不仅仅是季节性的，而且是以乘法方式的季节性。随着总体值的增加，季节波动也增加（可以将这视为从峰值到低谷的大小变化）。
- en: 'We can decompose the data into its seasonal, trend, and remainder components
    very easily as shown here with just one line of R:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样轻松地将数据分解为其季节性、趋势和剩余部分，只需一行 R 代码即可显示。
- en: '[PRE29]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The resulting plot seems very sensible based on the original data (see [Figure 2-12](#fig-0211)).
    We can imagine adding the seasonal, trend, and remainder data back together to
    get the original series. We can also see that this particular decomposition did
    not take into account the fact that this series shows a multiplicative seasonality
    rather than an additive one because the residuals are greatest at the beginning
    and end of the time series. It seems like this decomposition settled on the average
    seasonal variance as the variance for the seasonal component.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 根据原始数据，得出的图表看起来非常合理（参见[图 2-12](#fig-0211)）。我们可以想象将季节性、趋势和剩余数据重新组合以获得原始序列。我们还可以看到，这种特定的分解没有考虑到这一系列显示的是乘法季节性而不是加法季节性，因为残差在时间序列的开始和结束时最大。看起来，这种分解将平均季节变异性作为季节性分量的变异性。
- en: '![](assets/ptsa_0212.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0212.png)'
- en: Figure 2-12\. A decomposition of the original time series into a seasonal component,
    a trend, and the residuals. Pay attention to each plot’s y-axis, as they are quite
    different. Note that this is the reason for the gray bars on the right side of
    each plot. These gray bars are all the same absolute size (in units of the y-axis),
    so that their relatively different display is a visual reminder of the different
    y-axis scales across different components.
  id: totrans-374
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 将原始时间序列分解为季节性组成部分、趋势和残差。请注意每个图表的 y 轴，它们非常不同。请注意，这是每个图表右侧灰色条的原因。这些灰色条的绝对大小相同（以
    y 轴单位表示），因此它们的相对不同显示是对不同组成部分不同 y 轴比例的视觉提醒。
- en: 'To get an initial understanding of how this works, we can look at the official
    R documentation:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 要初步了解其工作原理，我们可以查看官方的 R 文档：
- en: The seasonal component is found by LOESS smoothing the seasonal subseries (the
    series of all January values, ...). If s.window = “periodic” smoothing is effectively
    replaced by taking the mean. The seasonal values are removed, the remainder smoothed
    to find the trend. The overall level is removed from the seasonal component and
    added to the trend component. This process is iterated a few times. The remainder
    component is the residuals from the seasonal plus trend fit.
  id: totrans-376
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 季节性分量是通过LOESS平滑季节子序列（所有一月值的系列...）找到的。如果`s.window = “periodic”`，平滑实际上被取均值替代。季节性值被移除，余下的被平滑以找到趋势。总体水平从季节分量中移除并加到趋势分量中。这个过程迭代几次。余数分量是季节加趋势拟合的残差。
- en: LOESS, introduced earlier, is a computationally taxing method for smoothing
    data points that involves a moving window to estimate the smoothed value of each
    point based on its neighbors (I hope your lookahead alarm is sounding!).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 早先引入的LOESS方法是一种计算量大的平滑数据点的方法，它涉及使用移动窗口根据其邻近点估计每个点的平滑值（我希望你的前瞻警报正在响起！）。
- en: Time Zones
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时区
- en: 'Time zones are intrinsically tedious, painful, and difficult to get right even
    with a lot of effort. That is why you should never use your own solution. From
    their very invention, time zones have been complicated, and they have only gotten
    more so with the advent of personal computers. There are many reasons for this:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 时区本质上是乏味、痛苦且难以正确处理，即使花费大量精力也是如此。这就是为什么你绝不应该使用自己的解决方案。时区自诞生以来就非常复杂，随着个人计算机的出现变得更加复杂。其中有许多原因：
- en: Time zones are shaped by political and social decisions.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时区受政治和社会决策的影响。
- en: There is no standard way to transport time zone information between languages
    or via an HTTP protocol.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有标准的方式在不同语言之间或通过HTTP协议传输时区信息。
- en: There is no single protocol for naming time zones or for determining start and
    end dates of daylight savings offsets.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有单一的协议来命名时区或确定夏令时偏移的开始和结束日期。
- en: Because of daylight savings, some times occur twice a year in their time zones!
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因夏令时的存在，某些时区一年中会出现两次特定的时间！
- en: Most languages rely on the underlying operating system to get time zone information.
    Unfortunately, the built-in automatic time retrieval function in Python, `datetime.datetime.now()`,
    does not return a time zone–aware timestamp. Part of this is by design. Some decisions
    made in the standard library include forbidding time zone information in the `datetime`
    module (because this information changes so often) and allowing `datetime` objects
    both with and without time zone information. However, comparing an object with
    a time zone to one without a time zone will cause a `TypeError`.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数语言依赖于底层操作系统获取时区信息。不幸的是，Python内置的自动时间检索函数`datetime.datetime.now()`不返回带时区信息的时间戳。部分原因在于设计上的考虑。标准库中的一些决定包括禁止在`datetime`模块中包含时区信息（因为这些信息经常更改），允许`datetime`对象既带有时区信息又不带有时区信息。然而，将带有时区和不带有时区的对象进行比较将引发`TypeError`。
- en: Some bloggers claim that a majority of libraries are written with the assumption
    that `tzinfo==None`. Although this is a difficult claim to substantiate, it is
    consistent with much experience. People also report difficulty pickling time zone–stamped
    objects, so this is also something to check on early if you plan to use pickling.^([4](ch02.html#idm45576049285592))
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 一些博主声称，多数库都假定`tzinfo==None`。虽然这一说法难以证实，但与大多数经验一致。人们也报告难以pickle带有时区标记的对象，因此如果你打算使用pickling，这也是一个早期需要检查的事项。^([4](ch02.html#idm45576049285592))
- en: So let’s have a look at working with time zones in Python. The main libraries
    you are likely to use are `datetime`, `pytz`, and `dateutil`. Additionally Pandas
    offers convenient time zone–related functionality built on top of the latter two
    libraries.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在Python中使用时区的方法。你可能会使用的主要库包括`datetime`、`pytz`和`dateutil`。此外，Pandas提供了基于后两个库的便捷时区相关功能。
- en: We’ll cover the most important time zone functionality next.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将介绍最重要的时区功能。
- en: 'First, notice that when you retrieve a “now” from the `datetime` module, it
    does not come with time zone information, even though it will give you the appropriate
    time for your time zone. Note, for example, the difference in the responses for
    `now()` and `utcnow()`:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 首先注意，当你从`datetime`模块检索“现在”时，它并不包含时区信息，尽管它会给出适合你所在时区的正确时间。例如，注意`now()`和`utcnow()`的响应差异：
- en: '[PRE30]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Notice that if we do pass in a time zone, we will get the correct information,
    but this is not the default behavior. To work with time zones in Python, we create
    a time zone object, such `western` as for the US Pacific time zone:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们确实传入一个时区，我们将得到正确的信息，但这不是默认行为。在Python中处理时区时，我们创建一个时区对象，例如美国太平洋时间的`western`时区：
- en: '[PRE31]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can then use these objects to `localize` a time zone as follows:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用这些对象来`localize`一个时区，如下所示：
- en: '[PRE32]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Note, however, that passing the time zone directly into the `datetime` constructor
    will often not produce the result we were expecting:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，然而，直接将时区传递给`datetime`构造函数通常不会产生我们期望的结果：
- en: '[PRE33]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is important, such as when you are calculating time deltas. The first
    example of the following three is the gotcha:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点非常重要，比如在计算时间差时。以下三个示例中的第一个是要注意的地方：
- en: '[PRE34]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`pytz` provides a list of common time zones and time zones by country, both
    of which can be handy references:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '`pytz`提供了常见时区和按国家划分的时区列表，这两者都可以作为方便的参考：'
- en: '[PRE35]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A particularly hairy issue is the matter of daylight savings. Certain human-readable
    times exist twice (falling behind in the autumn), while others do not exist at
    all (skip ahead in the spring):'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 特别棘手的问题是夏令时问题。某些人类可读的时间存在两次（在秋季落后），而其他时间根本不存在（在春季提前跳过）：
- en: '[PRE36]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Time zone concerns may not be important to your work, so the utility of this
    knowledge will depend on the nature of your data. There are certainly situations
    where getting this wrong could be catastrophic (say, generating weather forecasts
    for commercial airliners flying during the time change and suddenly finding their
    positions drastically altered).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你的工作来说，时区问题可能并不重要，因此这些知识的实用性取决于你数据的性质。当然，在某些情况下，如果出错可能会有灾难性后果（比如，在时区变更期间飞行的商业航班生成天气预报，突然发现它们的位置发生了
    drastical 改变）。
- en: Preventing Lookahead
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防止前瞻性
- en: Lookahead is dangerously easy to introduce into a modeling pipeline, especially
    with vectorized functional data manipulation interfaces, such as those offered
    by R and Python. It is easy to shift a variable in the wrong direction, shift
    it more or less than you intended, or otherwise find yourself with data that is
    not entirely “honest” in that you have data available before it would have been
    known in your system.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 前瞻性在建模流水线中特别容易引入，特别是在使用R和Python提供的向量化功能数据操作界面时。很容易将变量向错误的方向移动、移动得比预期的多或少，或者以其他方式发现自己得到的数据并非完全“诚实”，因为在你的系统中，你可能在计划拥有数据之前就有了数据。
- en: Unfortunately, there isn’t a definitive statistical diagnosis for lookahead—after
    all, the whole endeavor of time series analysis is modeling the unknown. Unless
    a system is somewhat deterministic with known dynamical laws, it can be difficult
    to distinguish a very good model from a model with lookahead—that is, until you
    put a model into production and realize either that you are missing data when
    you planned to have it, or simply that your results in production do not reflect
    what you see during training.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于前瞻性，并没有一个明确的统计诊断—毕竟，时间序列分析的整个尝试就是对未知数进行建模。除非系统在某种程度上是确定性的，具有已知的动力学规律，否则很难区分非常好的模型和具有前瞻性的模型—也就是说，直到你将一个模型投入生产，并意识到你计划拥有它的数据时，你是否丢失了数据，或者简单地说，在生产中得到的结果并不反映你在训练过程中看到的情况。
- en: The best way to prevent this embarrassment is constant vigilance. Whenever you
    are time-shifting data, smoothing data, imputing data, or upsampling data, ask
    yourself whether you could know something at a given time. Remember that doesn’t
    just include calendar time. It also includes realistic time lags to reflect how
    long a delay there is between something happening and your organization having
    that data available. For example, if your organization scrapes Twitter only weekly
    to gather its sentiment analysis data, you need to include this weekly periodicity
    in your training and validation data segmentation. Similarly, if you can retrain
    your model only once a month, you need to figure out what model would apply to
    what data over time. You can’t, for example, train a model for July and then apply
    it to July for testing, because in a real situation, you wouldn’t have that model
    trained up in time if training takes you a long time.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 防止这种尴尬的最佳方法是保持持续警惕。每当你进行数据的时间转移、平滑处理、填补数据或者上采样时，都要问自己在某个特定时间点是否能知道一些信息。记住，这不仅仅包括日历时间，还包括真实的时间滞后，以反映某件事情发生后到你的组织获得相关数据之间的延迟时间。例如，如果你的组织仅每周从Twitter上获取情感分析数据，你需要在训练和验证数据的分割中包括这种每周周期性。同样，如果你只能每月重新训练你的模型一次，你需要确定随着时间推移哪种模型适用于哪些数据。例如，你不能仅仅为七月份训练一个模型，然后将其应用于七月份进行测试，因为在实际情况下，如果训练需要很长时间，你将来不及准备好那个模型。
- en: 'Here are some other ideas to use as a general checklist. Keep them in mind
    both when planning to build a model and when auditing your process after the fact:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些其他的想法，可以作为一个通用的检查清单。在规划构建模型时和事后审计过程时，都要记住这些想法：
- en: If you are smoothing data or imputing missing data, think carefully about whether
    it might impact your results by introducing a lookahead. And don’t just think
    about it—experiment as we did earlier and see how the imputations and smoothing
    work. Do they seem to be forward looking? If so, can you justify using them? (Probably
    not.)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你在平滑数据或填充缺失数据，要仔细考虑是否可能通过引入一个前瞻来影响你的结果。不仅要考虑这个问题，还要像之前进行实验一样，看看填补和平滑的效果如何。它们是否看起来是向前看的？如果是，你能否证明使用它们是合理的？（可能不是。）
- en: Build your entire process with a very small data set (only a few rows in a `data.table`
    or a few row time steps in whatever data format). Then, do random spot checks
    at each step in the process and see whether you accidentally shift any information
    temporally to an inappropriate place.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用一个非常小的数据集（只有一个`data.table`中的几行或者任何数据格式中的几行时间步长）构建你的整个过程。然后，在每个步骤中进行随机抽查，看看是否意外地将任何信息在时间上移动到不适当的位置。
- en: For each kind of data, find out what the lag is for it relative to its own timestamp.
    For example, if the timestamp is when the data “happened” but not when it was
    uploaded to your servers, you need to know that. Different columns of a data frame
    may have different lags. To address this, you can either customize your lag per
    data frame or (better and more realistic) pick the biggest lag and apply that
    to everything. While you won’t want to unduly pessimize your model, it’s a good
    starting point after which you can relax these overly constrained rules one at
    a time, carefully!
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一种数据，找出相对于其自身时间戳的滞后时间。例如，如果时间戳是数据“发生”的时间，而不是上传到服务器的时间，你需要知道这一点。数据帧的不同列可能具有不同的滞后时间。为了解决这个问题，你可以根据数据帧自定义滞后时间，或者（更好且更现实的方法）选择最大的滞后时间并应用于所有数据。虽然你不希望过度悲观地影响你的模型，但这是一个很好的起点，之后你可以逐步放松这些过于约束的规则，小心地进行一项一项的调整！
- en: Use time-aware error (rolling) testing or cross-validation. This will be discussed
    in [Chapter 11](ch11.html#measuring_error_chapter), but remember that randomizing
    your training versus testing data sets does not work with time series data. You
    do not want information from the future to leak into models for the past.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时间感知的错误（滚动）测试或交叉验证。这将在[第11章](ch11.html#measuring_error_chapter)中讨论，但要记住，对于时间序列数据，随机化训练与测试数据集是行不通的。你不希望未来的信息泄漏到过去的模型中。
- en: Intentionally introduce a lookahead and see how your model behaves. Try various
    degrees of lookahead, so you have an idea how it shifts accuracy. If you have
    some idea of the accuracy with lookahead, you have an idea of what the ceiling
    on a real model without unfair knowledge of the future will do. Remember that
    many time series problems are extremely difficult, so a model with a lookahead
    may seem great until you realize you are dealing with a high-noise/low-signal
    data set.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有意引入先行查看，并查看您的模型行为。尝试不同程度的先行查看，这样您就有了模型准确性如何变化的概念。如果您有一些先行查看的准确性概念，您就知道没有未来不公平知识的真实模型的上限将会如何。请记住，许多时间序列问题非常困难，因此一个具有先行查看的模型可能看起来很棒，直到您意识到您正在处理的是高噪声/低信号数据集。
- en: Add features slowly, particularly features you might be processing, so that
    you can look for jumps. One sign of a lookahead is when a particular feature is
    unexpectedly good, and there isn’t a very good explanation. At the top of your
    explanation list should always be “lookahead.”
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓慢添加功能，特别是您可能正在处理的功能，以便您可以查找跳跃。先行查看的一个标志是当某个特定功能表现出乎意料的好，并且没有很好的解释。在您的解释列表的顶部，应该始终是“先行查看”。
- en: Note
  id: totrans-414
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Processing and cleaning time-related data can be a tedious and detail-oriented
    process.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 处理和清理与时间相关的数据可能是一个繁琐而细致的过程。
- en: There is tremendous danger in data cleaning and processing of introducing a
    *lookahead*! You should have lookaheads only if they are intentional, and this
    is rarely appropriate.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清理和处理中引入*先行查看*是非常危险的！只有当它们是有意的时候，才应该使用先行查看，而这很少是适当的。
- en: More Resources
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多资源
- en: 'On missing data:'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于缺失数据：
- en: Steffen Moritz et al., [“Comparison of Different Methods for Univariate Time
    Series Imputation in R,”](https://perma.cc/M4LJ-2DFB) unpublished research paper,
    October 13, 2015, https://perma.cc/M4LJ-2DFB.
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Steffen Moritz等，《比较R中单变量时间序列插补的不同方法》，未发表的研究论文，2015年10月13日，[链接](https://perma.cc/M4LJ-2DFB)，https://perma.cc/M4LJ-2DFB。
- en: This thorough summary from 2015 outlines available methods for imputing time
    series data in the case of univariate time series data. Univariate time series
    data is a particular challenge because many advanced missing data imputation methods
    rely on looking at distributions among covariates, an option that is not available
    in the case of a univariate time series. This paper summarizes both the usability
    and performance of various R packages as well as empirical results of the methods
    available on a variety of data sets.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这份详尽的2015年总结概述了在单变量时间序列数据情况下输入时间序列数据的可用方法。单变量时间序列数据是一个特殊的挑战，因为许多先进的缺失数据插补方法依赖于观察协变量之间的分布，而在单变量时间序列情况下这种选择是不可用的。本文总结了各种R包的可用性和性能，以及这些方法在各种数据集上的实证结果。
- en: 'James Honaker and Gary King, [“What to Do About Missing Values in Time-Series
    Cross-Section Data,”](https://perma.cc/8ZLG-SMSX) *American Journal of Political
    Science* 54, no. 2 (2010): 561–81, https://perma.cc/8ZLG-SMSX.'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: James Honaker和Gary King，《关于时间序列横截面数据中缺失值的处理方法》，《美国政治科学杂志》54卷，2期（2010年）：561-81，[链接](https://perma.cc/8ZLG-SMSX)，https://perma.cc/8ZLG-SMSX。
- en: This article explores best practices for missing data in time series with wide
    panels of covariates.
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文探讨了在宽面板协变量时间序列中缺失数据的最佳实践。
- en: Léo Belzile, [“Notes on Irregular Time Series and Missing Values,”](https://perma.cc/8LHP-92FP)
    n.d. https://perma.cc/8LHP-92FP.
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Léo Belzile，《不规则时间序列和缺失值笔记》，未标明日期，[链接](https://perma.cc/8LHP-92FP)，https://perma.cc/8LHP-92FP。
- en: The author provides examples of working with irregular data as a missing data
    problem and an overview of some commonly used R packages.
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者提供了处理不规则数据作为缺失数据问题的示例，以及一些常用R包的概述。
- en: 'On time zones:'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于时区：
- en: Tom Scott, [The Problem with Time & Timezones](https://oreil.ly/iKHkp), Computerphile
    video, December 30, 2013, https://oreil.ly/iKHkp.
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Tom Scott，《时区和时间问题》，Computerphile视频，2013年12月30日，[链接](https://oreil.ly/iKHkp)，https://oreil.ly/iKHkp。
- en: This 10-minute YouTube video with over 1.5 million views outlies the perils
    and challenges of dealing with time zones, particularly in the context of a web
    application.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段时长10分钟、观看量超过150万的YouTube视频详细描述了处理时区的危险和挑战，特别是在Web应用程序的背景下。
- en: Wikipedia, [“Time Zone,”](https://perma.cc/J6PB-232C) https://perma.cc/J6PB-232C.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Wikipedia，《时区》，[链接](https://perma.cc/J6PB-232C)，https://perma.cc/J6PB-232C。
- en: This is a fascinating history in short form that gives some insight into how
    timekeeping worked before the last century and how technology advancements (beginning
    with the railway) led to the increasing need for people in different locations
    to coordinate their time. There are also some fun maps of time zones.
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个简要而迷人的历史，揭示了在上个世纪之前时间的记录方式以及从铁路开始的技术进步如何导致不同地点的人们需要协调他们的时间。还有一些有趣的时区地图。
- en: Declan Butler, [“GPS Glitch Threatens Thousands of Scientific Instruments,”](https://perma.cc/RPT6-AQBC)
    *Nature*, April 3, 2019, https://perma.cc/RPT6-AQBC.
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Declan Butler，[“GPS故障威胁数千台科学仪器”，](https://perma.cc/RPT6-AQBC)《自然》杂志，2019年4月3日，https://perma.cc/RPT6-AQBC.
- en: This article is not directly related to time zones but to the problem of timestamping
    more generally. It describes a recent problem in which a bug in the US Global
    Positioning System could cause a problem with timestamped data because it transmits
    a binary 10-digit “week number” that began counting from January 6, 1980\. This
    system can cover only 1,024 weeks total (2 raised to the power of 10). This count
    was reached for the second time in April 2019\. Devices that were not designed
    to accommodate this limitation reset to zero and incorrectly timestamped scientific
    and industrial data. This article describes some of the difficulties with a fairly
    limited representation of time combined with scientific devices that were not
    future-proofed to account for this problem.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文与时区无直接关系，而是涉及时间戳问题的更普遍问题。它描述了一个最近的问题，即美国全球定位系统中的一个错误可能导致时间戳数据问题，因为它传输了一个从1980年1月6日开始计数的二进制10位“周数”。该系统总共只能涵盖1024周（2的10次方）。这个计数在2019年4月第二次达到。未设计以应对此限制的设备将会重置为零，并错误地为科学和工业数据标记时间戳。本文描述了一些关于时间限制与科学设备不足以预见这一问题的困难。
- en: 'On smoothing and seasonality:'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于平滑和季节性：
- en: 'Rob J. Hyndman and George Athanasopoulos,[“Exponential Smoothing,”](https://perma.cc/UX4K-2V5N)
    in *Forecasting: Principles and Practices*, 2nd ed. (Melbourne: OTexts, 2018),
    https://perma.cc/UX4K-2V5N.'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Rob J. Hyndman 和 George Athanasopoulos，[“指数平滑”，](https://perma.cc/UX4K-2V5N)
    收录于《预测：原理与实践》，第2版（墨尔本：OTexts，2018），https://perma.cc/UX4K-2V5N.
- en: This chapter of Hyndman and Athanasopoulos’s introductory academic textbook
    covers exponential smoothing methods for time series data, including a helpful
    taxonomy of exponential smoothing and extensions of exponential smoothing to forecasting
    applications.
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hyndman 和 Athanasopoulos 的介绍性学术教材中的这一章节，涵盖了时间序列数据的指数平滑方法，包括指数平滑的分类以及扩展到预测应用的方法。
- en: David Owen, [“The Correct Way to Start an Exponential Moving Average,”](https://perma.cc/ZPJ4-DJJK)
    Forward Motion blog, January 31, 2017, https://perma.cc/ZPJ4-DJJK.
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: David Owen，[“启动指数移动平均的正确方式”，](https://perma.cc/ZPJ4-DJJK) Forward Motion 博客，2017年1月31日，https://perma.cc/ZPJ4-DJJK.
- en: As highlighted in a side note earlier, exponential moving averages are conceptually
    simple and easy to calculate, with the complication that how to “start” off the
    calculation is a little complicated. We want to make sure our moving average adapts
    to new information in a way that recognizes how long it has been recording information.
    If we start a moving average without giving consideration to this, even a new
    moving average will behave as though it has infinite lookback and unduly discount
    new information relative to what it should do. More details and a computational
    solution are presented in this blog post.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如前面的一个侧记所强调的，指数移动平均的概念简单易懂，计算起来也很容易，但是如何“启动”计算则稍显复杂。我们希望确保我们的移动平均能够根据记录信息的时间长短来适应新信息的更新。如果我们在考虑这一点时启动移动平均，即使是一个新的移动平均也会像具有无限回溯的平均值一样行事，从而在应对新信息时不公平地进行折价。有关更多详细信息和计算解决方案，请参阅本博客文章。
- en: Avner Abrami, Aleksandr Arovkin, and Younghun Kim, [“Time Series Using Exponential
    Smoothing Cells,”](https://perma.cc/2JRX-K2JZ) unpublished research paper, last
    revised September 29, 2017, https://perma.cc/2JRX-K2JZ.
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Avner Abrami，Aleksandr Arovkin 和 Younghun Kim，[“使用指数平滑单元的时间序列”，](https://perma.cc/2JRX-K2JZ)
    未发表研究论文，最后修订于2017年9月29日，https://perma.cc/2JRX-K2JZ.
- en: This is an extremely accessible time series research paper elaborating on the
    idea of simple exponential smoothing to develop “exponential smoothing cells.”
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一篇非常易于理解的时间序列研究论文，详细阐述了简单指数平滑的概念，以开发“指数平滑单元”。
- en: 'On functional data analysis:'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于函数数据分析：
- en: Jane-Ling Wang, Jeng-Min Chiou, and Hans-Georg Müller, [“Review of Functional
    Data Analysis,”](https://perma.cc/3DNT-J9EZ) *Annual Reviews of Statistics and
    its Application*, 2015, https://perma.cc/3DNT-J9EZ.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简-王，J.-M. Chiou，和H.-G. Müller，《功能数据分析综述》，《统计学及其应用年度评论》，2015年，https://perma.cc/3DNT-J9EZ。
- en: This statistics article offers an approachable mathematical overview of important
    functional data analysis techniques. There are also helpful visualization techniques
    illustrated throughout the article.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本统计文章提供了一种易于理解的数学概述，介绍了重要的功能数据分析技术。文章中还展示了一些有用的可视化技术。
- en: 'Shahid Ullah and Caroline F. Finch, [“Applications of Functional Data Analysis:
    A Systematic Review,”](https://perma.cc/VGK5-ZEUX) *BMC Medical Research Methodologyv*,
    13, no. 43 (2013), https://perma.cc/VGK5-ZEUX.'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Shahid Ullah 和 Caroline F. Finch，《功能数据分析应用：系统综述》，《BMC医学研究方法学》，第13卷，第43期（2013年），https://perma.cc/VGK5-ZEUX。
- en: This review takes a multidisciplinary approach to surveying recently published
    analyses using functional data analysis. The authors argue that the techniques
    of functional data analysis have much wider applicability than is currently appreciated
    and make the case that the biological and health sciences could benefit from more
    use of these techniques to look at medical time series data.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本评述采用跨学科的方法，调查了最近发表的使用功能数据分析进行的分析。作者认为功能数据分析技术具有比目前认知更广泛的适用性，并提出生物和健康科学可以通过更多地使用这些技术来查看医学时间序列数据的情况。
- en: ^([1](ch02.html#idm45576046843816-marker)) To learn more about this kind of
    data analysis, also see references on
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#idm45576046843816-marker)) 要了解更多关于这种数据分析的信息，还可以参考
- en: ^([2](ch02.html#idm45576046801992-marker)) SQL databases are traditional databases
    using a table-based mechanism of storing data. For example, if you wanted to store
    customer transactions in a SQL database, you might have a table with customer
    information, including a unique identifier, and another table with transactions,
    each one including one of those unique customer identifiers.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#idm45576046801992-marker)) SQL 数据库是使用基于表的数据存储机制的传统数据库。例如，如果您想在
    SQL 数据库中存储客户交易记录，您可能会有一个包含客户信息（包括唯一标识符）的表，以及另一个包含交易记录的表，每个交易记录都包含其中一个唯一的客户标识符。
- en: ^([3](ch02.html#idm45576049612264-marker)) You can specify the alpha smoothing
    factor, the halflife, the span, or the center of mass according to which you feel
    comfortable with. Details are available in the [documentation](https://perma.cc/4265-4U8L).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#idm45576049612264-marker)) 您可以根据自己的喜好指定阿尔法平滑因子、半衰期、跨度或质心。详情请参阅[文档](https://perma.cc/4265-4U8L)。
- en: ^([4](ch02.html#idm45576049285592-marker)) Pickling is the process of storing
    Python objects in byte format. This is done via the popular built-in `pickle`
    module. Most of the time pickling will work well, but sometimes time related objects
    can be difficult to pickle.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#idm45576049285592-marker)) Pickling 是将 Python 对象以字节格式存储的过程。这通过流行的内置
    `pickle` 模块完成。大多数情况下，泡菜工作得很好，但有时与时间相关的对象可能难以泡制。
