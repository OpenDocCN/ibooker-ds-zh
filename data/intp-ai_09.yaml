- en: 6 Understanding layers and units
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 理解层和单元
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Dissecting a black-box convolutional neural network to understand the features
    or concepts that are learned by the layers and units
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剖析黑盒卷积神经网络以理解层和单元学习到的特征或概念
- en: Running the network dissection framework
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行网络剖析框架
- en: Quantifying the interpretability of layers and units in the convolutional neural
    network and how to visualize them
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化卷积神经网络中层和单元的可解释性以及如何可视化它们
- en: Strengths and weaknesses of the network dissection framework
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络剖析框架的优势和劣势
- en: In chapters 3, 4, and 5, we focused our attention on black-box models and how
    to interpret them using various techniques such as partial dependence plots (PDPs),
    LIME, SHAP, anchors, and saliency maps. In chapter 5, we specifically focused
    on convolutional neural networks (CNNs) and visual attribution methods such as
    gradients and activation maps that highlight the salient features that the model
    is focusing on. All these techniques focused on interpreting the complex processing
    and operations that happen within a black-box model by reducing its complexity.
    PDPs, for instance, are model-agnostic and show the marginal or average global
    effects of feature values on model prediction. Techniques like LIME, SHAP, and
    anchors are also model-agnostic—they create a proxy model that behaves similarly
    to the original black-box model but is simpler and easier to interpret. Visual
    attribution methods and saliency maps are weakly model-dependent and help highlight
    a small portion of the input that is salient, or important, for the model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3、4和5章中，我们关注了黑盒模型以及如何使用各种技术如部分依赖图（PDPs）、LIME、SHAP、锚点和显著性图来解释它们。在第5章中，我们特别关注了卷积神经网络（CNNs）和视觉归因方法，如梯度图和激活图，这些方法突出了模型关注的显著特征。所有这些技术都集中在通过降低其复杂性来解释黑盒模型内部发生的复杂处理和操作。例如，PDPs是模型无关的，显示了特征值对模型预测的边际或平均全局影响。LIME、SHAP和锚点等技术的模型无关性也体现在它们创建了一个与原始黑盒模型行为相似但更简单、更容易解释的代理模型。视觉归因方法和显著性图对模型有轻微的依赖性，有助于突出对模型来说显著或重要的输入部分。
- en: In this chapter and the next, we focus on interpreting representations or features
    learned by deep neural networks. This chapter specifically focuses on CNNs that
    are used for visual tasks such as image classification, object detection, and
    image segmentation. The large number of operations that happen within a CNN are
    organized into layers and units. By interpreting model representations, we aim
    to understand the role and structure of the data flowing through these layers
    and units. You’ll specifically learn about the network dissection framework in
    this chapter. This framework will shed more light into the features and high-level
    concepts learned by the CNN. It will also help us go from visualizations like
    saliency maps that are typically evaluated qualitatively to more quantitative
    interpretations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，我们关注解释深度神经网络学习到的表示或特征。本章特别关注用于视觉任务（如图像分类、目标检测和图像分割）的CNNs。CNN内部发生的操作数量被组织成层和单元。通过解释模型表示，我们旨在理解通过这些层和单元流动的数据的角色和结构。你将在本章中具体了解网络剖析框架。这个框架将使我们对CNN学习到的特征和高级概念有更深入的了解。它还将帮助我们从通常定性评估的显著性图等可视化过渡到更定量的解释。
- en: We will first introduce the ImageNet and Places datasets and the associated
    image classification task. We will then give a quick recap of CNNs and visual
    attribution methods, focusing on the limitations of these methods. This is to
    demonstrate the benefits of the network dissection framework. The remainder of
    the chapter will focus on this framework and how we can use it to understand the
    representations learned by CNNs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将介绍ImageNet和Places数据集以及相关的图像分类任务。然后，我们将快速回顾卷积神经网络（CNNs）和视觉归因方法，重点关注这些方法的局限性。这是为了展示网络剖析框架的优势。本章的剩余部分将专注于这个框架以及我们如何利用它来理解CNNs学习到的表示。
- en: 6.1 Visual understanding
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 视觉理解
- en: In this chapter, we will focus on the task of training an agent or an intelligent
    system to recognize real-world objects, places, and scenes. The task of this system
    is to perform multiclass classification. To train such an agent, we need access
    to large volumes of labeled data. The ImageNet dataset ([http://www.image-net.org/](http://www.image-net.org/))
    was created for the purpose of recognizing objects. It is a large-scale ontology
    of images built on the backbone of WordNet. WordNet is a lexical database of English
    nouns, verbs, adjectives, and adverbs that are organized into sets of synonyms,
    also called synsets. ImageNet also follows a similar structure where images are
    grouped by hierarchical synsets, or categories. Figure 6.1 shows an example of
    this structure. In this example, images of animals are organized into three categories.
    The highest-level category consists of images of mammals. The next level consists
    of images of carnivores followed by the final level, which consists of images
    of dogs. The full ImageNet database contains more than 14 million images grouped
    into 27 high-level categories. The number of synsets, or subcategories, ranges
    from 51 to 3,822\. When it comes to building image classifiers, ImageNet is one
    of the most common datasets used.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于训练一个智能体或智能系统来识别现实世界中的物体、地点和场景的任务。该系统的任务是执行多类分类。为了训练这样的智能体，我们需要访问大量标记数据。ImageNet数据集([http://www.image-net.org/](http://www.image-net.org/))是为了识别物体而创建的。它是在WordNet的基础上构建的一个大规模图像本体。WordNet是一个英语名词、动词、形容词和副词的词汇数据库，这些词汇被组织成一组同义词，也称为synsets。ImageNet也遵循类似的结构，其中图像根据层次同义词或类别分组。图6.1展示了这种结构的示例。在这个例子中，动物图像被组织成三个类别。最高级别的类别包括哺乳动物的图像。下一级包括食肉动物的图像，然后是最终级别，包括狗的图像。完整的ImageNet数据库包含超过1400万张图像，分为27个高级类别。同义词或子类别的数量从51到3822不等。当涉及到构建图像分类器时，ImageNet是最常用的数据集之一。
- en: '![](../Images/CH06_F01_Thampi.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F01_Thampi.png)'
- en: Figure 6.1 An illustration of synsets, or categories, in the ImageNet dataset
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 ImageNet数据集中同义词或类别的示意图
- en: For the task of recognizing places and scenes, we will use the Places dataset
    ([http://places2.csail.mit.edu/](http://places2.csail.mit.edu/)). Knowing the
    place, scene, or context in which objects appear in the real world is an important
    aspect of building an intelligent system like a self-driving car trying to navigate
    a city. The Places dataset organizes images into different levels of scene categories.
    Figure 6.2 shows an example that illustrates the semantic structure of the dataset.
    The example shows a high-level scene category called Outdoor. Under this category
    are three subcategories called Cathedral, Building, and Stadium. In total, the
    Places dataset holds more than 10 million images organized into 400 unique scene
    categories. Using this dataset, we can train a model to learn features for various
    place- and scene-recognition tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于识别地点和场景的任务，我们将使用Places数据集([http://places2.csail.mit.edu/](http://places2.csail.mit.edu/))。了解物体在现实世界中出现的地点、场景或上下文是构建像自动驾驶汽车这样的智能系统的重要方面，该系统试图在城市中导航。Places数据集将图像组织成不同级别的场景类别。图6.2展示了说明数据集语义结构的示例。该示例显示了一个高级场景类别，称为户外。在这个类别下有三个子类别，分别是大教堂、建筑和体育场。总共有超过1000万张图像被组织成400个独特的场景类别。使用这个数据集，我们可以训练一个模型来学习各种地点和场景识别任务的特征。
- en: '![](../Images/CH06_F02_Thampi.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F02_Thampi.png)'
- en: Figure 6.2 An illustration of categories in the Places dataset
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 Places数据集中类别的示意图
- en: With the ImageNet and Places datasets, we are now ready to train the intelligent
    system. Fortunately, we can use models for various state-of-the-art CNN architectures
    pretrained on the ImageNet and Places datasets. This will save us the effort,
    time, and money of training the models from scratch. In the next section, we will
    see how to leverage these pretrained models. We will also provide a recap of CNNs
    and the techniques that we have learned so far to interpret the output of these
    models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ImageNet和Places数据集的加入，我们现在已经准备好训练智能系统。幸运的是，我们可以使用在ImageNet和Places数据集上预训练的各种最先进的CNN架构模型。这将节省我们从头开始训练模型的努力、时间和金钱。在下一节中，我们将看到如何利用这些预训练模型。我们还将回顾CNN以及我们迄今为止学到的技术，以解释这些模型的输出。
- en: '6.2 Convolutional neural networks: A recap'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 卷积神经网络：回顾
- en: In this section, we provide a quick recap of CNNs that we learned in chapter
    5\. Figure 6.3 illustrates a CNN architecture that can be used to classify an
    image in the ImageNet dataset as either being a dog or not.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们快速回顾了我们在第 5 章中学到的 CNN。图 6.3 展示了一个 CNN 架构，可以用于将 ImageNet 数据集中的图像分类为狗或不是狗。
- en: '![](../Images/CH06_F03_Thampi.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_Thampi.png)'
- en: Figure 6.3 An illustration of a convolutional neural network (CNN)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 卷积神经网络 (CNN) 的示意图
- en: The architecture consists of a sequence of layers called convolution and pooling
    layers, followed by a set of fully connected layers. The convolution and pooling
    layers combined are called the *feature-learning layers*. These layers extract
    hierarchical features from the input image. The first few layers extract low-level
    features, such as edges, colors, and gradients. Subsequent layers learn high-level
    features. The fully connected layers are used for classification. The features
    learned in the feature-learning layers are fed as inputs into the fully connected
    layers. The final output is a probability measure of how likely it is that the
    input image is of a dog.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构由一系列称为卷积和池化层的层组成，随后是一组全连接层。卷积和池化层合称为 *特征学习层*。这些层从输入图像中提取层次特征。前几层提取低级特征，如边缘、颜色和梯度。后续层学习高级特征。全连接层用于分类。特征学习层中学习的特征被作为输入馈送到全连接层。最终输出是输入图像是狗的概率度量。
- en: 'In the previous chapter, we also learned how to initialize state-of-the-art
    CNN architectures using the `torchvision` package in PyTorch. We specifically
    focused on the ResNet architecture that was 18 layers deep, called ResNet-18\.
    We will continue to use this architecture in this chapter as well. The ResNet-18
    model can be initialized in PyTorch as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们也学习了如何使用 PyTorch 中的 `torchvision` 包初始化最先进的 CNN 架构。我们特别关注了深度为 18 层的 ResNet
    架构，称为 ResNet-18。我们将在本章中也继续使用这个架构。在 PyTorch 中可以按照以下方式初始化 ResNet-18 模型：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Imports the torchvision package
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 torchvision 包
- en: ② Initializes the ResNet model with random weights
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用随机权重初始化 ResNet 模型
- en: 'By setting the `pretrained` parameter to `False`, we initialize the ResNet
    model with random weights. To initialize the model pretrained on the ImageNet
    dataset, we have to set the `pretrained` parameter to `True`. This is shown next:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `pretrained` 参数设置为 `False`，我们使用随机权重初始化 ResNet 模型。要初始化在 ImageNet 数据集上预训练的模型，我们必须将
    `pretrained` 参数设置为 `True`。这将在下面展示：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Initializes the ResNet model pretrained on the ImageNet dataset
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化在 ImageNet 数据集上预训练的 ResNet 模型
- en: Other CNN architectures, like AlexNet, VGG, Inception, and ResNeXT, can also
    be initialized using `torchvision`. All the supported architectures can be found
    at [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 CNN 架构，如 AlexNet、VGG、Inception 和 ResNeXT，也可以使用 `torchvision` 初始化。所有支持的架构都可以在
    [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)
    找到。
- en: 'For the Places dataset, you can find pretrained PyTorch models for various
    architectures at [https://github.com/CSAILVision/places365](https://github.com/CSAILVision/places365).
    You can download the pretrained PyTorch model for use with the ResNet-18 architecture
    from [http://mng.bz/GGmA](http://mng.bz/GGmA). Because the file size is more than
    40 MB, I encourage you to download it locally. Once it is downloaded, you can
    load the model pretrained on the Places dataset as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Places 数据集，你可以在 [https://github.com/CSAILVision/places365](https://github.com/CSAILVision/places365)
    找到各种架构的预训练 PyTorch 模型。你可以从 [http://mng.bz/GGmA](http://mng.bz/GGmA) 下载用于 ResNet-18
    架构的预训练 PyTorch 模型。由于文件大小超过 40 MB，我鼓励你本地下载。一旦下载完成，你可以按照以下方式加载在 Places 数据集上预训练的模型：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Imports the PyTorch library
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 PyTorch 库
- en: ② Sets this variable to the full path where the pretrained ResNet model has
    been downloaded
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将此变量设置为预训练 ResNet 模型下载的完整路径
- en: ③ Loads the ResNet model pretrained on the Places dataset
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 加载在 Places 数据集上预训练的 ResNet 模型
- en: 'We also learned various visual attribution methods that can be used to interpret
    CNNs, as summarized in figure 6.4\. Three broad categories of visual attribution
    methods exist: perturbations, gradients, and activations. Techniques like LIME
    and SHAP are perturbation-based methods. These model-agnostic, post hoc, and local
    interpretability techniques use proxy models that behave similarly to the complex
    CNN but are easier to interpret. These techniques highlight segments, or superpixels,
    in the image that are important for the model prediction. These techniques are
    great and can be applied to any complex model. Gradient-based and activation-based
    methods are post hoc and local interpretability techniques. They are, however,
    weakly dependent on the model and highlight only a small portion of the input
    image that is salient or important for the model. For gradient-based methods like
    vanilla backpropagation, guided backpropagation, integrated gradients, and SmoothGrad,
    we obtain the salient pixels in the image by computing the gradient of the target
    class with respect to the input image. For activation-based methods like Grad-CAM
    and guided Grad-CAM, activations in the final convolutional layer are weighted
    based on the gradient of the target class with respect to the activation or feature
    map.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了各种可以用来解释CNN的视觉归因方法，如图6.4所示。存在三种视觉归因方法的广泛类别：扰动、梯度和激活。像LIME和SHAP这样的技术是基于扰动的。这些模型无关、事后和局部可解释的技术使用与复杂CNN行为相似但更容易解释的代理模型。这些技术突出了图像中对模型预测重要的部分或超像素。这些技术非常好，可以应用于任何复杂模型。基于梯度和基于激活的方法是事后和局部可解释的技术。然而，它们对模型有轻微的依赖性，并且只突出了输入图像中显著或对模型重要的很小一部分。对于像vanilla
    backpropagation、guided backpropagation、integrated gradients和SmoothGrad这样的基于梯度的方法，我们通过计算目标类别相对于输入图像的梯度来获得图像中的显著像素。对于像Grad-CAM和guided
    Grad-CAM这样的基于激活的方法，最终卷积层的激活是基于目标类别相对于激活或特征图的梯度进行加权的。
- en: '![](../Images/CH06_F04_Thampi.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F04_Thampi.png)'
- en: Figure 6.4 A recap of visual attribution methods
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 视觉归因方法回顾
- en: All the visual attribution methods shown in figure 6.4 highlight the important
    pixels or superpixels for the final model prediction. We typically assess the
    visualizations generated by these methods qualitatively, so the interpretations
    are subjective. Moreover, these techniques do not give us any information on the
    low-level and high-level concepts or features that are learned by the feature-learning
    layers and units in the CNN. In the following section, we will learn about the
    network dissection framework. This framework will help us dissect the CNN and
    come up with more quantitative interpretations. We will also be able to understand
    what human-understandable concepts are learned by the feature-learning layers
    in the CNN.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4中展示的所有视觉归因方法都突出了最终模型预测中重要的像素或超像素。我们通常对这些方法生成的可视化进行定性评估，因此解释是主观的。此外，这些技术并没有给我们提供关于CNN中特征学习层和单元所学习的低级和高级概念或特征的信息。在下一节中，我们将了解网络剖析框架。这个框架将帮助我们剖析CNN，并提出更多定量解释。我们还将能够理解CNN中的特征学习层学习到哪些人类可理解的概念。
- en: 6.3 Network dissection framework
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 网络剖析框架
- en: 'The network dissection framework was proposed by Zhou, Bolei, et al., researchers
    from MIT, in 2018 (see [https://arxiv.org/pdf/1711.05611.pdf](https://arxiv.org/pdf/1711.05611.pdf)).
    The fundamental questions that the framework aims to answer follow:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剖析框架由麻省理工学院的周博磊等研究人员在2018年提出（参见[https://arxiv.org/pdf/1711.05611.pdf](https://arxiv.org/pdf/1711.05611.pdf)）。该框架旨在回答的基本问题如下：
- en: How does the CNN decompose the task of understanding an image?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN是如何分解理解图像的任务的？
- en: Does the CNN identify any features or concepts that are understandable by humans?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN是否识别出任何人类可理解的特征或概念？
- en: 'The framework answers these questions by finding units in the convolutional
    layers in the CNN that match meaningful, predefined semantic concepts. The interpretability
    of those units is quantified by measuring the alignment of the unit responses
    to those predefined concepts. Dissecting the network in this way is interesting
    because it makes the deep neural network less opaque. The network dissection framework
    consists of the following three key steps, as summarized in figure 6.5:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架通过在CNN的卷积层中找到与有意义的、预定义的语义概念匹配的单元来回答这些问题。这些单元的可解释性通过测量单元响应与预定义概念的对齐程度来量化。以这种方式剖析网络是有趣的，因为它使得深度神经网络不那么神秘。网络剖析框架包括以下三个关键步骤，如图6.5所示：
- en: First, define a broad set of meaningful concepts that can be used to dissect
    the network.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，定义一组广泛的有意义的概念，这些概念可以用来剖析网络。
- en: Then, probe the network by finding units that respond to those predefined concepts.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过寻找对那些预定义概念做出响应的单元来探测网络。
- en: Last, measure the quality or interpretability of those units to those concepts.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，测量这些单元对这些概念的质量或可解释性。
- en: '![](../Images/CH06_F05_Thampi.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F05_Thampi.png)'
- en: Figure 6.5 The network dissection framework
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 网络剖析框架
- en: We will break down each of these steps in greater detail in the following subsections.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的小节中更详细地分解这些步骤。
- en: 6.3.1 Concept definition
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 概念定义
- en: The first and most crucial step in the network dissection framework is data
    collection. The data must consist of images that are labeled pixelwise with concepts
    of different abstraction levels. The ImageNet and Places datasets, introduced
    in section 6.1, can be used to train models to detect real-world objects and scenes.
    For the purposes of network dissection, we need another independent dataset consisting
    of labeled concepts. We will not use this dataset for model training but rather
    to probe the network to understand what high-level concepts are learned by the
    feature-learning layers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剖析框架的第一步也是最重要的一步是数据收集。数据必须由用不同抽象级别的概念逐像素标记的图像组成。在第6.1节中介绍的ImageNet和Places数据集可以用来训练模型以检测现实世界中的物体和场景。对于网络剖析的目的，我们需要另一个独立的、包含标记概念的独立数据集。我们不会使用这个数据集进行模型训练，而是用它来探测网络，以了解特征学习层学到了哪些高级概念。
- en: To dissect models trained to detect real-world objects and scenes using datasets
    like ImageNet and Places, Zhou, Bolei, et al. combined five different datasets
    to create an independent labeled dataset consisting of high-level concepts called
    *Broden*. Broden stands for *bro*adly and *den*sely labeled dataset. The five
    datasets that Broden unifies are ADE ([http://mng.bz/zQD6/](http://mng.bz/zQD6/)),
    Open-Surfaces ([http://mng.bz/0wJE](http://mng.bz/0wJE)), PASCAL- Context ([http://mng.bz/9KEq](http://mng.bz/9KEq)),
    PASCAL-Part ([http://mng.bz/jyD8](https://shortener.manning.com/jyD8)), and the
    Describable Textures Dataset ([http://mng.bz/W7Xl](http://mng.bz/W7Xl)). These
    datasets consist of annotated images of a broad range of concept categories, from
    low-level concept categories like colors, textures, and materials to more high-level
    concept categories like parts, objects, and scenes. Figure 6.6 provides an illustration
    of an image labeled with various concepts. In the Broden dataset, a segmented
    image is created for each of the concepts in an image. If we take the tree object
    in figure 6.6 as an example, pixels within the bounding box containing the tree
    have a label of 1, and pixels outside the bounding box, not containing the tree,
    have a label of 0\. Concepts need to be labeled at the pixel level. Labels from
    all the five datasets are unified in the Broden dataset. Concepts with similar
    synonyms are also merged. Broden contains more than 1,000 visual concepts.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分解使用 ImageNet 和 Places 等数据集训练以检测现实世界对象和场景的模型，周、博雷等结合了五个不同的数据集，创建了一个独立的、称为
    *Broden* 的高层次概念标注数据集。Broden 代表 *bro*adly 和 *den*sely 标注数据集。Broden 统一了以下五个数据集：ADE
    ([http://mng.bz/zQD6/](http://mng.bz/zQD6/))、Open-Surfaces ([http://mng.bz/0wJE](http://mng.bz/0wJE))、PASCAL-Context
    ([http://mng.bz/9KEq](http://mng.bz/9KEq))、PASCAL-Part ([http://mng.bz/jyD8](https://shortener.manning.com/jyD8))
    和可描述的纹理数据集 ([http://mng.bz/W7Xl](http://mng.bz/W7Xl))。这些数据集包含广泛概念类别的标注图像，从低级概念类别如颜色、纹理和材料到更高级的概念类别如部分、物体和场景。图
    6.6 提供了一个带有各种概念的图像的插图。在 Broden 数据集中，为图像中的每个概念创建了一个分割图像。如果我们以图 6.6 中的树对象为例，包含树的边界框内的像素有一个标签
    1，而边界框外、不包含树的像素有一个标签 0。概念需要在像素级别进行标注。所有五个数据集的标签在 Broden 数据集中统一。具有相似同义词的概念也被合并。Broden
    包含超过 1,000 个视觉概念。
- en: '![](../Images/CH06_F06_Thampi.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F06_Thampi.png)'
- en: Figure 6.6 An illustration of an image with labeled concepts
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 一个带有标注概念的图像的插图
- en: Because creating a dataset with labeled concepts is a crucial step in the network
    dissection framework, let’s take a step back and look at how to create a new dataset.
    We will focus specifically on the tools that we can use for this purpose and the
    methodology to follow to obtain consistent, high-quality labeled concepts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于创建带有标注概念的数据集是网络分解框架中的关键步骤，让我们退一步看看如何创建一个新的数据集。我们将特别关注我们可以用于此目的的工具和遵循的方法，以获得一致、高质量标注的概念。
- en: We can use various tools for labeling images. LabelMe ([http://mng.bz/8lE5](http://mng.bz/8lE5))
    and Make Sense ([https://www.makesense.ai/](https://www.makesense.ai/)) are free
    web-based image-annotation tools. In LabelMe, we can easily create an account,
    upload images, and label them. Through the sharing functionality, we can create
    annotations collaboratively as well. Images uploaded on LabelMe, however, are
    considered public. Make Sense is a very similar tool, but it does not allow you
    to collaborate and share annotations with others. The tool also does not save
    the state of an annotation project. Therefore, if you start a project in Make
    Sense, the annotations for the images in that project must be finished in one
    go. The tool does not allow you to save the state and start annotating from where
    you left off. Both LabelMe and Make Sense support multiple label types, like rectangles,
    lines, points, and polygons. Both tools are used mostly by researchers using datasets
    that are meant to be public.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用各种工具来标注图像。LabelMe ([http://mng.bz/8lE5](http://mng.bz/8lE5)) 和 Make Sense
    ([https://www.makesense.ai/](https://www.makesense.ai/)) 是免费的基于网络的图像标注工具。在 LabelMe
    中，我们可以轻松创建账户，上传图像，并对它们进行标注。通过共享功能，我们还可以协作创建标注。然而，在 LabelMe 上上传的图像被认为是公开的。Make
    Sense 是一个非常类似的工具，但它不允许您与他人协作和共享标注。该工具也不保存标注项目的状态。因此，如果您在 Make Sense 中开始一个项目，该项目中图像的标注必须一次性完成。该工具不允许您保存状态并从上次离开的地方继续标注。LabelMe
    和 Make Sense 都支持多种标签类型，如矩形、线条、点和多边形。这两个工具主要被使用数据集旨在公开的研究人员使用。
- en: For enterprise, business, or more private needs, you could host your own labeling
    service. The Computer Vision Annotation Tool (CVAT; [https://github.com/openvinotool
    kit/cvat](https://github.com/openvinotoolkit/cvat)) and Visual Object Tagging
    Tool (VoTT; [https://github.com/microsoft/VoTT](https://github.com/microsoft/VoTT))
    are free, open source web services that you can deploy on your own web servers.
    If you do not want to deal with the hassle of hosting your own labeling service,
    you could also use managed services such as LabelBox ([https://labelbox.com/](https://labelbox.com/)),
    Amazon SageMaker Ground Truth ([https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/)),
    or the labeling services provided by Azure Machine Learning ([http://mng.bz/ExgX](https://shortener.manning.com/ExgX))
    or Google Cloud ([http://mng.bz/Nx9v](https://shortener.manning.com/Nx9v)). If
    you do not have a team of labelers who can annotate images for you, you could
    crowdsource the labeling effort and obtain labels using Amazon Mechanical Turk
    ([https://www.mturk.com/](https://www.mturk.com/)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业、商业或更私人的需求，您可以托管自己的标注服务。计算机视觉标注工具（CVAT；[https://github.com/openvinotoolkit/cvat](https://github.com/openvinotoolkit/cvat)）和视觉对象标记工具（VoTT；[https://github.com/microsoft/VoTT](https://github.com/microsoft/VoTT)）是免费的、开源的Web服务，您可以将它们部署在自己的Web服务器上。如果您不想处理托管自己的标注服务的麻烦，您也可以使用如LabelBox
    ([https://labelbox.com/](https://labelbox.com/))）、Amazon SageMaker Ground Truth
    ([https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/))或Azure
    Machine Learning提供的标注服务([http://mng.bz/ExgX](https://shortener.manning.com/ExgX))或Google
    Cloud([http://mng.bz/Nx9v](https://shortener.manning.com/Nx9v)))等托管服务。如果您没有一组可以为您标注图像的标注者团队，您也可以通过Amazon
    Mechanical Turk ([https://www.mturk.com/](https://www.mturk.com/))进行众包标注并获取标签。
- en: 'It is also important to have a good labeling methodology to ensure high-quality
    and consistent labels. The protocol for the labeling task has to be clearly specified
    so that the labelers know the full list of concepts with clear definitions for
    them. The labels obtained through this process, however, can be quite noisy, especially
    if they are crowdsourced. To ensure consistency in the labels, take a random subset
    of the images and get them annotated by the same set of labelers. By doing so,
    you can now quantify how consistent the labels are by looking at the following
    three types of errors, as detailed in [http://mng.bz/DxaA](https://shortener.manning.com/DxaA),
    which introduces the ADE20K dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要有良好的标注方法，以确保高质量的、一致的标签。标注任务的协议必须明确指定，以便标注者知道所有概念的完整列表，并对它们有明确的定义。然而，通过这个过程获得的标签可能会相当嘈杂，尤其是如果它们是众包的。为了确保标签的一致性，随机抽取图像的一个子集，并让同一组标注者进行标注。通过这样做，您现在可以通过查看以下三种类型的错误来量化标签的一致性，这些错误在[http://mng.bz/DxaA](https://shortener.manning.com/DxaA)中详细说明，该链接介绍了ADE20K数据集：
- en: '*Segmentation quality*—This error quantifies the precision of the segmentation
    of concepts. A given concept could be segmented differently by different labelers
    and even by the same labeler.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分割质量*—这种错误量化了概念分割的精确度。同一个概念可能被不同的标注者或同一个标注者以不同的方式分割。'
- en: '*Concept naming*—Differences in concept naming can occur where a given pixel
    is given a different concept name by the same labeler or a different labeler.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概念命名*—概念命名的差异可能出现在同一个标注者或不同的标注者给同一个像素分配不同的概念名称的情况下。'
- en: '*Segmentation quantity*—Some images could contain more labeled concepts than
    others. You can quantify this error by looking at the variance in the number of
    concepts in a certain image across multiple labelers.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分割数量*—某些图像可能包含比其他图像更多的标注概念。您可以通过查看多个标注者对某个图像中概念数量的方差来量化这种错误。'
- en: We can circumvent the segmentation quality and quantity errors by increasing
    the number of labelers so that we can take a consensus or by getting the images
    annotated by more experienced labelers. We can avoid the concept naming error
    by having a clearly defined labeling protocol with precise terminology. As mentioned
    earlier, creating a dataset with labeled concepts is the most important step in
    the network dissection framework. It is also the most time-consuming and costly
    step. We will see the value of this dataset through the lens of interpretability
    in the following sections.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过增加标注者的数量来规避分割质量和数量的错误，以便我们可以达成共识，或者通过让更有经验的标注者对图像进行标注。我们可以通过有一个明确定义的标注协议和精确的术语来避免概念命名的错误。如前所述，创建带有标注概念的数据库是网络剖析框架中最重要的一步。这也是最耗时和成本最高的步骤。我们将在以下章节中通过可解释性的视角来了解这个数据库的价值。
- en: 6.3.2 Network probing
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 网络探测
- en: Once you have a labeled dataset of visual concepts, the next step is to probe
    the pretrained neural network to understand how the network responds to those
    concepts. Let’s first look at how this works for a simple deep neural network.
    A simplified representation of a deep neural network is shown in figure 6.7, where
    the number of units decreases as you go from the input layer to the output layer.
    A representation of the input data is learned in the intermediate layers of the
    network, and this is represented as *R*. To understand the network better, we
    would like to probe the network by quantifying how the representation *R* maps
    to a given query concept *Q* that we care about. The mapping from the representation
    *R* to query concept *Q* is called the *computation model* and is represented
    as *f* in the figure. Let’s now define *R*, *Q* and *f* in the context of CNNs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了视觉概念的标记数据集，下一步就是探测预训练神经网络，以了解网络对这些概念的反应。让我们首先看看这在一个简单的深度神经网络中是如何工作的。图6.7展示了深度神经网络的简化表示，其中单元的数量从输入层到输出层逐渐减少。网络中间层学习到输入数据的表示，这被表示为*R*。为了更好地理解网络，我们希望通过量化表示*R*如何映射到我们关心的给定查询概念*Q*来探测网络。从表示*R*到查询概念*Q*的映射称为*计算模型*，在图中表示为*f*。现在，让我们在CNN的背景下定义*R*、*Q*和*f*。
- en: '![](../Images/CH06_F07_Thampi.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 探测卷积神经网络中的第4层概念](../Images/CH06_F07_Thampi.png)'
- en: Figure 6.7 Probing a deep neural network for concepts
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 探测深度神经网络中的概念
- en: Figure 6.8 illustrates a CNN where layer 4 of the network is probed. In the
    figure, we are probing the network with an image of a dog and determining what
    concepts (like color and object) are learned by the units in layer 4 in the pretrained
    CNN. The first step, therefore, is to forward-propagate the image of the dog through
    the CNN. The weights of the CNN are frozen, and there is no need for training
    or backpropagation. Next, we pick a convolutional layer to probe (in this case,
    layer 4). We then obtain the output feature map or activation map after forward
    propagation from that layer. In general, as you go deeper into a CNN, the size
    of the activation map reduces. Therefore, to compare the activation map with labeled
    concepts in the input image, we have to up-sample, or scale, the lower-resolution
    activation map to the same resolution as that of the input image. This forms the
    representation *R* of convolutional layer 4 in the CNN. Repeat this process for
    all the images in the labeled-concepts dataset, and store the activation maps
    for all images. We can also repeat this process for other layers in the CNN.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8展示了探测网络第4层的CNN。在图中，我们使用狗的图像探测网络，确定预训练CNN中第4层的单元学习到了哪些概念（如颜色和物体）。因此，第一步是将狗的图像通过CNN进行前向传播。CNN的权重被冻结，不需要训练或反向传播。接下来，我们选择一个卷积层进行探测（在这种情况下，第4层）。然后，我们从该层获得前向传播后的输出特征图或激活图。一般来说，随着你深入CNN，激活图的大小会减小。因此，为了将激活图与输入图像中的标记概念进行比较，我们必须将低分辨率激活图上采样或缩放，使其与输入图像具有相同的分辨率。这形成了CNN中第4层卷积层的表示*R*。对标记概念数据集中的所有图像重复此过程，并存储所有图像的激活图。我们也可以对CNN中的其他层重复此过程。
- en: '![](../Images/CH06_F08_Thampi.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 探测卷积神经网络中的第4层概念](../Images/CH06_F08_Thampi.png)'
- en: Figure 6.8 Probing layer 4 in a convolutional neural network for concepts
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 探测卷积神经网络中的第4层概念
- en: Now, how do we interpret what high-level concepts are contained in these representations
    *R*? In other words, how do we map these representations *R* with query concepts
    *Q*? This requires us to determine a computational model *f* that maps *R* to
    *Q*. Also, how do you up-sample, or scale, the low-resolution activation map to
    the same resolution as the input image? This is broken down in figure 6.9.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何解释这些表示*R*中包含的高级概念？换句话说，我们如何将这些表示*R*与查询概念*Q*进行映射？这需要我们确定一个计算模型*f*，将*R*映射到*Q*。此外，你如何上采样或缩放低分辨率激活图，使其与输入图像具有相同的分辨率？这已在图6.9中分解。
- en: '![](../Images/CH06_F09_Thampi.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 探测卷积神经网络中的第4层概念](../Images/CH06_F09_Thampi.png)'
- en: Figure 6.9 An illustration of up-sampling and how to map representation R to
    query concept Q
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 上采样和如何将表示R映射到查询概念Q的示意图
- en: In figure 6.9, we can see that input image *i* is forward-propagated through
    the CNN. For illustration purposes, suppose that we are specifically interested
    in unit *k* in convolutional layer *l*. The output of this convolutional layer
    is represented as the low-resolution activation map *A[i]*. The network dissection
    framework then up-samples, or resizes, the activation map to the same resolution
    as the input image *i*. This is shown as the input image-resolution activation
    map *S[i]* in figure 6.9\. The bilinear interpolation algorithm is used in the
    framework. Bilinear interpolation extends linear interpolation to a two-dimensional
    plane. It estimates the values of new unknown pixels in the resized image based
    on known values in surrounding pixels. The estimates or interpolants are centered
    around each unit’s response in the original activation map.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 6.9 中，我们可以看到输入图像 *i* 通过 CNN 的前向传播。为了说明目的，假设我们特别关注卷积层 *l* 中的单元 *k*。这个卷积层的输出表示为低分辨率激活图
    *A[i]*。网络剖析框架随后对激活图进行上采样或调整大小，使其与输入图像 *i* 的分辨率相同。这如图 6.9 中的输入图像分辨率激活图 *S[i]* 所示。框架中使用的是双线性插值算法。双线性插值将线性插值扩展到二维平面。它根据周围像素中的已知值估计调整大小后的图像中新未知像素的值。估计或插值值位于原始激活图中每个单元响应的中心。
- en: Once you have the image-resolution activation map *S[i]*, the framework then
    maps this representation onto a given query concept *Q[c]* by performing a simple
    thresholding. The thresholding is performed at the unit level so that the response
    of each unit in the convolutional layer can be compared with the query concept.
    In figure 6.9, the query concept *Q[c]* is the segmented image for the color brown
    in the original labeled-concepts dataset. The binary unit segmentation map for
    unit *k* after thresholding is shown as *M[k]*. The threshold used by the computation
    model *f* is *T[k]*, where
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了图像分辨率激活图 *S[i]*，框架随后通过执行简单的阈值化将此表示映射到给定的查询概念 *Q[c]*。阈值化是在单元级别进行的，以便可以将卷积层中每个单元的响应与查询概念进行比较。在图
    6.9 中，查询概念 *Q[c]* 是原始标记概念数据集中棕色颜色的分割图像。阈值化后的单元 *k* 的二进制单元分割图显示为 *M[k]*。计算模型 *f*
    使用的阈值是 *T[k]*，其中
- en: '*M[k]* = *S[i]* ≥ *T[k]*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*M[k]* = *S[i]* ≥ *T[k]*'
- en: The binary unit segmentation map *M[k]* highlights all the regions for which
    the activation exceeds the threshold *T[k]*. The threshold *T[k]* is dependent
    on the unit that we are probing in the CNN. How do we compute this threshold?
    The framework looks at the distribution of the unit’s activation across all the
    images in the labeled-concepts dataset. Let *a[k]* be the value of a unit’s activation
    in the low-resolution activation map *A[i]* for a given input image *i*. Once
    you have the distribution of *a[k]* across all the images, the threshold *T[k]*
    is then computed as the top quantile level such that
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制单元分割图 *M[k]* 强调了所有激活值超过阈值 *T[k]* 的区域。阈值 *T[k]* 取决于我们在 CNN 中探测的单元。我们如何计算这个阈值？框架会查看单元激活在标记概念数据集所有图像中的分布。设
    *a[k]* 为给定输入图像 *i* 中低分辨率激活图 *A[i]* 中一个单元的激活值。一旦你得到了所有图像中 *a[k]* 的分布，阈值 *T[k]*
    就被计算为最高分位数水平，使得
- en: ℙ(*a[k]* > *T[k]*) = 0.005
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ℙ(*a[k]* > *T[k]*) = 0.005
- en: '*T[k]* measures the 0.005 quantile level. In other words, 0.5% of all unit
    activations (*a[k]*) across all the images in the labeled-concepts dataset are
    greater than *T[k]*. Once we have mapped the representation learned by the CNN
    to the binary unit segmentation map, the next step is to quantify the alignment
    of this segmentation map with all query concepts *Q[c]*. This is detailed in the
    following subsection.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*T[k]* 衡量的是 0.005 分位数水平。换句话说，标记概念数据集中所有图像中所有单元激活 (*a[k]*) 的 0.5% 大于 *T[k]*。一旦我们将
    CNN 学到的表示映射到二进制单元分割图，下一步就是量化该分割图与所有查询概念 *Q[c]* 的对齐程度。这将在以下小节中详细说明。'
- en: 6.3.3 Quantifying alignment
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 量化对齐
- en: After you have probed the network and obtained the binary unit segmentation
    map for all the units in the representation layers, the final step in the framework
    is to quantify the alignment of the segmentation maps with all the query concepts
    in the dataset. Figure 6.10 shows how to quantify the alignment for a given binary
    unit segmentation map *M[k]* and query concept *Q[c]*. The alignment is measured
    using the Intersection over Union (IoU) score. IoU is a useful metric for measuring
    the accuracy of how well a unit detects a given concept. It measures the overlap
    of the binary unit segmentation map with the pixelwise segmented image of the
    query concept. The higher the IoU score, the better the accuracy. If the binary
    segmentation map perfectly overlaps with the concept, we obtain a perfect IoU
    score of 1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在你已经探测了网络并获得了表示层中所有单元的二值单元分割图之后，框架中的最后一步是对分割图与数据集中所有查询概念的对齐进行量化。图6.10展示了如何量化给定二值单元分割图
    *M[k]* 和查询概念 *Q[c]* 的对齐。对齐是通过交并比（IoU）分数来衡量的。IoU是衡量一个单元检测给定概念准确性的有用指标。它衡量二值单元分割图与查询概念的像素级分割图像的重叠程度。IoU分数越高，准确性越好。如果二值分割图与概念完美重叠，我们获得完美的IoU分数为1。
- en: '![](../Images/CH06_F10_Thampi.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F10_Thampi.png)'
- en: Figure 6.10 Quantifying alignment with concept
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 用概念量化对齐
- en: The value of IoU for a given binary segmentation map *M[k]* and query concept
    *Q[c]* is the accuracy of unit *k* in detecting concept *c*. It quantifies the
    interpretability of unit *k* by measuring how good it is at detecting concept
    *c*. In the network dissection framework, an IoU threshold of 0.04 is used, where
    a unit *k* is considered a detector of concept *c* if the IoU score is greater
    than 0.04\. The value of *0.04* was picked arbitrarily by the authors of the framework,
    and in their paper, available at [https://arxiv.org/pdf/1711.05611.pdf](https://arxiv.org/pdf/1711.05611.pdf),
    the authors show through human evaluation that the quality of the interpretation
    is insensitive to the IoU threshold. To quantify the interpretability of a convolutional
    layer, the framework counts the number of unique concepts aligned with units,
    that is, the number of unique concept detectors. With this understanding of how
    the network dissection framework works, let’s see it in action in the following
    section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 给定二值分割图 *M[k]* 和查询概念 *Q[c]* 的IoU值是单元 *k* 检测概念 *c* 的准确性。它通过衡量单元 *k* 检测概念 *c*
    的能力来量化单元 *k* 的可解释性。在网络分解框架中，使用0.04的IoU阈值，如果一个单元 *k* 的IoU分数大于0.04，则认为该单元 *k* 是概念
    *c* 的检测器。*0.04* 这个值是框架的作者任意选择的，在他们提供的论文中，[https://arxiv.org/pdf/1711.05611.pdf](https://arxiv.org/pdf/1711.05611.pdf)，作者通过人工评估表明，解释的质量对IoU阈值不敏感。为了量化卷积层的可解释性，框架计算与单元对齐的独特概念的数目，即独特概念检测器的数量。有了对网络分解框架如何工作的理解，让我们在下一节中看看它是如何实际应用的。
- en: 6.4 Interpreting layers and units
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 解释层和单元
- en: In this section, we put the network dissection framework to the test by interpreting
    layers and units in CNN models that are pretrained on the ImageNet and Places
    datasets. As mentioned in section 6.2, we will focus on the ResNet-18 architecture,
    but the network dissection framework can be applied to any CNN model. We saw in
    section 6.2 how to load ResNet-18 models pretrained on the ImageNet and Places
    datasets. The authors of the paper have created a library called NetDissect ([https://github.com/CSAILVision/NetDissect](https://github.com/CSAILVision/NetDissect))
    that implements this framework. This library supports both the PyTorch and Caffe
    deep learning frameworks. We will, however, use an improved implementation called
    NetDissect-Lite ([https://github.com/CSAILVision/NetDissect-Lite](https://github.com/CSAILVision/NetDissect-Lite))
    that is lighter and faster than the original implementation. This library is written
    in PyTorch and Python 3.6\. We will need to make some minor changes to the library
    to support later versions of Python (3.7 and above), and we will discuss this
    in the next subsection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过解释在ImageNet和Places数据集上预训练的CNN模型中的层和单元来测试网络剖析框架。如6.2节所述，我们将重点关注ResNet-18架构，但网络剖析框架可以应用于任何CNN模型。我们在6.2节中看到了如何加载在ImageNet和Places数据集上预训练的ResNet-18模型。论文的作者创建了一个名为NetDissect的库（[https://github.com/CSAILVision/NetDissect](https://github.com/CSAILVision/NetDissect)），该库实现了这个框架。这个库支持PyTorch和Caffe深度学习框架。然而，我们将使用一个改进的实现，称为NetDissect-Lite（[https://github.com/CSAILVision/NetDissect-Lite](https://github.com/CSAILVision/NetDissect-Lite)），它比原始实现更轻更快。这个库是用PyTorch和Python
    3.6编写的。我们需要对库进行一些小的修改以支持Python的后续版本（3.7及以上），我们将在下一小节中讨论这个问题。
- en: 'We can clone the NetDissect-Lite library to our local repository from GitHub
    using the following command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令从GitHub将NetDissect-Lite库克隆到我们的本地存储库中：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This library is also added to the repository associated with this book as a
    Git submodule. If you have cloned this book’s repository from GitHub, then you
    can pull the submodule by running the following command from the local directory
    where you have cloned the repository:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库也被添加到与本书相关的存储库中，作为一个Git子模块。如果你已经从GitHub克隆了本书的存储库，那么你可以通过在本地克隆存储库的目录中运行以下命令来拉取子模块：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once you have cloned the NetDissect-Lite repository, change into that directory
    locally. Then, run the following command to download the Broden dataset. The Broden
    dataset requires more than 1 GB of storage. Please take note of the path where
    this dataset is downloaded because we will need it later:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你克隆了NetDissect-Lite存储库，请本地切换到该目录。然后，运行以下命令下载Broden数据集。Broden数据集需要超过1GB的存储空间。请注意数据集下载的路径，因为我们稍后会用到它：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can also download the ResNet-18 model pretrained on the Places dataset
    by running the following command from the NetDissect-Lite directory. Again, please
    note the path where the model is downloaded because we will need it later:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过在NetDissect-Lite目录中运行以下命令来下载在Places数据集上预训练的ResNet-18模型。同样，请注意模型下载的路径，因为我们稍后会用到它：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 6.4.1 Running network dissection
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 运行网络剖析
- en: In this section, we will learn how to use the NetDissect-Lite library to probe
    the ResNet-18 models pretrained on the ImageNet and Places datasets by using labeled
    concepts from the Broden dataset. We can configure the library using the settings.py
    file at the root of the NetDissect-Lite library. We will not be covering all the
    settings because for most of them, we will be using the default values provided
    by the library. We will, therefore, focus on the key settings, which are summarized
    in table 6.1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用NetDissect-Lite库，通过使用Broden数据集中的标记概念来探测在ImageNet和Places数据集上预训练的ResNet-18模型。我们可以通过NetDissect-Lite库根目录下的settings.py文件来配置库。我们不会涵盖所有设置，因为对于大多数设置，我们将使用库提供的默认值。因此，我们将重点关注关键设置，这些设置总结在表6.1中。
- en: Table 6.1 Settings for the NetDissect-Lite library
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 NetDissect-Lite库的设置
- en: '| Setting | Descriptio | Possible Values |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 描述 | 可能的值 |'
- en: '| `GPU` | This is a Boolean setting that can be used to load the model and
    run network dissection on a GPU. | Possible values are `True` and `False`. If
    set to `True`, the GPU is used. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `GPU` | 这是一个布尔设置，可以用来在GPU上加载模型并运行网络剖析。 | 可能的值是 `True` 和 `False`。如果设置为 `True`，则使用GPU。
    |'
- en: '| `MODEL` | This is a String setting that sets the model architecture for the
    pre-trained model. | Possible values are `resnetlB`, `alexnet`, `resnetS0`, `densenet161`,
    etc. In this section, we will be setting the value to `resnetlB`. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `MODEL` | 这是一个字符串设置，用于设置预训练模型的模型架构。| 可能的值有`resnetlB`、`alexnet`、`resnetS0`、`densenet161`等。在本节中，我们将设置此值为`resnetlB`。|'
- en: '| `DATASET` | This is a string setting that lets the library know which dataset
    was used to train the CNN model. | Possible values are `imagenet` and `places`
    `365`. In this section, we will be using both values to compare the interpretability
    of the layers and units. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `DATASET` | 这是一个字符串设置，让库知道使用了哪个数据集来训练CNN模型。| 可能的值有`imagenet`和`places` `365`。在本节中，我们将使用这两个值来比较层和单元的可解释性。|'
- en: '| `CATEGORIES` | This setting is a list of strings that defines the high-level
    categories in the labeled concepts dataset. | For the Broden dataset, the list
    can contain the following values: `object`, `part`, `scene`, `material`, `texture`,
    and `color`. In this section, we will drop the `material` concept and look at
    the other five categories. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `CATEGORIES` | 这是一个字符串列表设置，定义了标记概念数据集中的高级类别。| 对于Broden数据集，列表可以包含以下值：`object`、`part`、`scene`、`material`、`texture`和`color`。在本节中，我们将删除`material`概念，并查看其他五个类别。|'
- en: '| `OUTPUT_FOLDER` | This is a string setting that provides the path to the
    labeled concepts dataset to the library. | The default value for this setting
    is the path where the `./script/dlbroden.sh` script downloads the Broden dataset.
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `OUTPUT_FOLDER` | 这是一个字符串设置，为库提供标记概念数据集的路径。| 此设置的默认值是`./script/dlbroden.sh`脚本下载Broden数据集的路径。|'
- en: '| `FEATURE_NAMES` | This setting is a list of strings that lets the library
    know which feature-learning layers in the CNN to probe. | For the Resnet18 model,
    the list can contain the following values: `layerl`, `layer2`, `layer3`, and/or
    `layer4`. In this chapter, we will be using all four values to compare the interpretability
    of the units across all four feature-learning layers. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `FEATURE_NAMES` | 这是一个字符串列表设置，让库知道在CNN中要探测哪些特征学习层。| 对于Resnet18模型，列表可以包含以下值：`layer1`、`layer2`、`layer3`和/或`layer4`。在本章中，我们将使用所有四个值来比较所有四个特征学习层中单元的可解释性。|'
- en: '| `MODEL_FILE` | This string setting is used to provide the library with the
    path to the pre-trained model. | For the Resnet18 model pre-trained on the Places
    dataset, set the value of this setting to the path where the `script/dlzoo_example.sh`
    script downloaded the model. For models pre-trained on the lmageNet dataset, set
    the value to `None`. This will let the library know to load the model *f*rom the
    `torchvision` package. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `MODEL_FILE` | 这是一个字符串设置，用于向库提供预训练模型的路径。| 对于在Places数据集上预训练的Resnet18模型，将此设置的值设置为`script/dlzoo_example.sh`脚本下载模型的路径。对于在ImageNet数据集上预训练的模型，将值设置为`None`。这将让库知道从`torchvision`包中加载模型。|'
- en: '| `MODEL_PARALLEL` | This is a Boolean setting that is used to let the library
    know if the model was trained in multi-GPU. | Possible values are `True` and `False`.
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `MODEL_PARALLEL` | 这是一个布尔设置，用于让库知道模型是否在多GPU上训练。| 可能的值是`True`和`False`。|'
- en: 'Before running the network dissection framework, ensure that the settings.py
    file is updated with the right settings. To probe all the feature-learning layers
    in the ResNet-18 model pretrained on the ImageNet dataset, we set the key settings
    in the settings.py file to the following values:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行网络剖析框架之前，请确保settings.py文件已更新为正确的设置。为了探测在ImageNet数据集上预训练的ResNet-18模型中的所有特征学习层，我们在settings.py文件中将关键设置设置为以下值：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Ensure that the `DATA_DIRECTORY` setting is set to the path where the Broden
    dataset is downloaded. Also, if you would like to use the GPU for faster processing,
    set the `GPU` setting to `True`. As mentioned earlier, the library provides a
    few subsettings. These are not explicitly set in the previous code, and you can
    use the default values for them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将`DATA_DIRECTORY`设置设置为Broden数据集下载的路径。此外，如果您想使用GPU进行更快的处理，请将`GPU`设置设置为`True`。如前所述，该库提供了一些子设置。这些设置在之前的代码中并未明确设置，您可以使用它们的默认值。
- en: 'To probe all the feature-learning layers in the ResNet-18 model pretrained
    on the Places dataset, we update just the following settings. The rest of the
    settings are the same as those for the ImageNet dataset. Ensure that the `MODEL_FILE`
    setting is set to the path where the ResNet-18 model pretrained on Places is downloaded:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要探测在 Places 数据集上预训练的 ResNet-18 模型中的所有特征学习层，我们只需更新以下设置。其余的设置与 ImageNet 数据集的设置相同。确保
    `MODEL_FILE` 设置设置为下载预训练的 ResNet-18 模型的路径：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once we have set the values for the settings, we are now ready to initialize
    and run the framework. Run the following lines of code to probe the network and
    to extract the activation maps from the feature-learning layers:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置了设置值，我们现在就可以初始化并运行框架了。运行以下代码行以探测网络并从特征学习层提取激活图：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Imports all the settings from the settings.py file
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从 settings.py 文件导入所有设置
- en: ② Imports the loadmodel *f*unction from the loader/model_loader module
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从 loader/model_loader 模块导入 loadmodel *f* 函数
- en: ③ Imports the hook_feature function and FeatureOperator class from the feature_operation
    module
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从特征操作模块导入 hook_feature 函数和 FeatureOperator 类
- en: ④ Initializes the FeatureOperator object
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化 FeatureOperator 对象
- en: ⑤ Loads the model and passes the hook_feature function to add hooks to the feature-learning
    layers
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 加载模型并将 hook_feature 函数传递给添加特征学习层的钩子
- en: ⑥ Runs feature extraction on the model to obtain the activation maps from the
    feature-learning layers
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 在模型上运行特征提取以从特征学习层获取激活图
- en: 'The `loadmodel` function loads the model based on the `MODEL` setting. The
    models are loaded the same way as we saw in section 6.2\. The function also adds
    hooks to each of the feature-learning layers based on the `FEATURE_NAMES` setting.
    These hooks are used by the `FeatureOperator` object to extract the activation
    maps from those layers. The `FeatureOperator` class is the main class that implements
    steps 2 and 3 in the network dissection framework. In the previous code snippet,
    we are running a part of step 2 that extracts the low-resolution activation maps
    from the feature-learning layers using the `feature_extraction` function. This
    function loads the images from the Broden dataset, forward-propagates them through
    the model, extracts the activation maps using the hooks, and then saves them in
    a file called feature_size.npy. The file is saved in the `OUTPUT_FOLDER` path
    as set in settings.py. The function `feature_extraction` also returns two variables:
    `features` and `maxfeatures`. The `features` variable contains the activation
    maps for all the feature-learning layers and input images. The `maxfeatures` variable
    stores the maximum value activation for each image, which we will use later when
    generating the summary results.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`loadmodel` 函数根据 `MODEL` 设置加载模型。模型的加载方式与我们看到的第 6.2 节中的方式相同。该函数还根据 `FEATURE_NAMES`
    设置为每个特征学习层添加钩子。这些钩子由 `FeatureOperator` 对象用于从这些层中提取激活图。`FeatureOperator` 类是实现网络剖析框架中的步骤
    2 和 3 的主要类。在前面的代码片段中，我们正在运行步骤 2 的一部分，使用 `feature_extraction` 函数从特征学习层提取低分辨率激活图。此函数从
    Broden 数据集中加载图像，通过模型前向传播它们，使用钩子提取激活图，然后将其保存到名为 feature_size.npy 的文件中。该文件保存在 settings.py
    中设置的 `OUTPUT_FOLDER` 路径。`feature_extraction` 函数还返回两个变量：`features` 和 `maxfeatures`。`features`
    变量包含所有特征学习层和输入图像的激活图。`maxfeatures` 变量存储每个图像的最大值激活，我们将在生成摘要结果时使用它。'
- en: 'Once we have extracted the low-resolution activation maps, we can run the following
    lines of code to calculate the threshold *T[k]* (the 0.005 quantile level) for
    all the units in the feature-learning layers, up-sample the low-resolution activation
    maps and generate the binary unit segmentation maps, calculate the IoU scores,
    and finally generate a summary of the results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们提取了低分辨率激活图，我们可以运行以下代码行来计算特征学习层中所有单元的阈值 *T[k]*（0.005 分位数水平），上采样低分辨率激活图并生成二值单元分割图，计算
    IoU 分数，并最终生成结果摘要：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Imports the generate_html_summary function from the visualize/report module
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从可视化/报告模块导入 generate_html_summary 函数
- en: ② Iterates through each of the feature-learning layers
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历每个特征学习层
- en: ③ Calculates the 0.005 quantile level for all units in the feature-learning
    layers
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算特征学习层中所有单元的 0.005 分位数水平
- en: ④ Calculates the IoU scores after up-sampling and generating the binary unit
    segmentation maps
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在上采样并生成二值单元分割图后计算 IoU 分数
- en: ⑤ Generates a summary of the results in HTML form
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 以 HTML 格式生成结果摘要
- en: 'In this code snippet, we are iterating through each of the feature-learning
    layers in FEATURE_NAMES and executing the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码片段中，我们正在遍历FEATURE_NAMES中的每个特征学习层，并执行以下操作：
- en: Using the `quantile_threshold` function in the `FeatureOperator` class, calculate
    the 0.005 quantile level (*T[k]*) for all the units in each feature-learning layer.
    These quantile levels, or thresholds, are saved in a file (called quantile_{layer}.csv)
    for each layer in the `OUTPUT_FOLDER` path. The function also returns the thresholds
    as a NumPy array.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`FeatureOperator`类中的`quantile_threshold`函数，计算每个特征学习层中所有单元的0.005分位数水平（*T[k]*)。这些分位数水平或阈值保存在`OUTPUT_FOLDER`路径下的每个层的文件中（称为quantile_{layer}.csv）。该函数还返回阈值作为NumPy数组。
- en: Using the `tally` function in the `FeatureOperator` class, up-sample the low-
    resolution activation map for each feature-learning layer into the same resolution
    as the input image. The `tally` function also generates binary unit segmentation
    maps based on the up-sampled activation maps and thresholds calculated for each
    of the units. The function finally calculates the IoU scores and measures the
    alignment of the binary unit segmentation maps with the segmented concepts in
    the Broden dataset. The aggregated IoU scores for each of the high-level concepts
    are saved in a file (called tally_{layer}.csv) for each layer in the OUTPUT_FOLDER
    path. These results are also returned as a dictionary object.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`FeatureOperator`类中的`tally`函数，将每个特征学习层的低分辨率激活图上采样到与输入图像相同的分辨率。`tally`函数还会根据每个单元的上采样激活图和计算出的阈值生成基于二进制单元分割图。该函数最终计算IoU分数并测量二进制单元分割图与Broden数据集中分割概念的匹配度。每个高级概念的聚合IoU分数都保存在OUTPUT_FOLDER路径下的一个文件中（称为tally_{layer}.csv），每个层一个文件。这些结果也作为字典对象返回。
- en: Finally, use the `generate_html_summary` function to create a summary of the
    results in HTML form.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用`generate_html_summary`函数创建结果的HTML形式摘要。
- en: In the following section, we will explore the results summary generated by the
    library and visualize the concepts learned by the units in the feature-learning
    layers.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探索库生成的结果摘要，并可视化特征学习层中单元学习到的概念。
- en: Running network dissection on a custom dataset
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在自定义数据集上运行网络分解
- en: 'It is important to understand the structure of the Broden dataset folder so
    that we can mimic that for our custom dataset and concepts. The folder at a high
    level consists of the following files and folder:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Broden数据集文件夹的结构非常重要，这样我们才能为我们的自定义数据集和概念模仿它。在高级别上，该文件夹包含以下文件和文件夹：
- en: '*images* (folder)—Contains all the images in either JPEG or PNG format. The
    folder should contain the original images in {filename}.jpg format and the segmented
    images for each of the concepts in {filename}_{concept}.jpg format.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*images*（文件夹）—包含所有JPEG或PNG格式的图像。该文件夹应包含以{filename}.jpg格式存储的原始图像以及每个概念的分割图像，格式为{filename}_{concept}.jpg。'
- en: '*index.csv*—Contains a list of all the images in the dataset with details on
    the labeled concepts. The first column is image filename with the relative path
    to the image. This is then followed by columns that contain information on the
    image height and width and the segmentation height and width dimensions. This
    is then followed by a column for each concept containing the relative path to
    the segmented image for that concept.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*index.csv*—包含数据集中所有图像的列表，以及有关标记概念的详细信息。第一列是图像文件名，包含图像的相对路径。然后是包含图像高度和宽度以及分割高度和宽度维度的列。然后是每个概念的列，包含该概念的分割图像的相对路径。'
- en: '*category.csv*—Lists all the concept categories followed by some summary statistics
    on the concepts. The first column is the concept name, followed by a count of
    the number of labels that belong to that concept category and also the frequency
    of the number of images with that labeled concept.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*category.csv*—列出所有概念类别，随后是一些关于概念的摘要统计信息。第一列是概念名称，然后是属于该概念类别的标签数量以及具有该标记概念的图像频率。'
- en: '*label.csv*—Lists all the labels and the corresponding concept that each belongs
    to, followed by some summary statistics on the labels. The first column is a label
    number (or identifier), followed by the label name and the category that it belongs
    to. Summary statistics include the frequency of the number of images with that
    label, pixel portions or the coverage of images with that label, and the total
    number of images with that label.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*label.csv*—列出所有标签及其对应的每个标签所属的概念，随后是一些关于标签的摘要统计。第一列是标签编号（或标识符），然后是标签名称和它所属的类别。摘要统计包括具有该标签的图像数量频率、像素部分或图像的覆盖范围，以及具有该标签的图像总数。'
- en: '*c_{concept}.csv*—One file per concept category that contains all the labels,
    frequency of images, and coverage details.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c_{concept}.csv*—每个概念类别一个文件，包含所有标签、图像频率和覆盖细节。'
- en: 'The new dataset that you create with your own labeled concepts should follow
    the same structure as the Broden dataset to ensure compatibility with the network
    dissection framework. Once you have structured your dataset as detailed earlier,
    you can then update the following settings in settings.py:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用自己的标记概念创建的新数据集应遵循Broden数据集相同的结构，以确保与网络剖析框架兼容。一旦你将数据集结构化如前所述，你就可以在settings.py中更新以下设置：
- en: '`DATA_DIRECTORY`—Points to the directory where your custom dataset is stored.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DATA_DIRECTORY`—指向存储你的自定义数据集的目录。'
- en: '`CATEGORIES`—Lists all the concept categories in your custom dataset, that
    is, in the category.csv file.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CATEGORIES`—列出你自定义数据集中的所有概念类别，即在category.csv文件中。'
- en: '`IMG_SIZE`—The dimension of the image in the images folder. The dimension should
    match the dimension in the index.csv file.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IMG_SIZE`—图像文件夹中图像的维度。维度应与index.csv文件中的维度匹配。'
- en: 'These settings will ensure that the new custom concepts dataset is loaded by
    the library. If you have your own pretrained model on a dataset that is different
    from ImageNet or Places, you will also need to update the following settings:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置将确保库加载新的自定义概念数据集。如果你在不同于ImageNet或Places的数据集上有一个自己的预训练模型，你还需要更新以下设置：
- en: '`DATASET`—Set to the name of the dataset that the models have been trained
    on.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DATASET`—设置为模型已训练的数据集名称。'
- en: '`NUM_CLASSES`—Set to the number of classes or labels that the model could output.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NUM_CLASSES`—设置为模型可能输出的类别或标签数量。'
- en: '`FEATURE_NAMES`—Lists the feature layer names in your custom pretrained model.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FEATURE_NAMES`—列出你自定义预训练模型中的特征层名称。'
- en: '`MODEL_FILE`—Contains the full path to your pretrained model in PyTorch.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_FILE`—包含你的预训练模型在PyTorch中的完整路径。'
- en: '`MODEL_PARALLEL`—If your custom model was trained in multi-GPU, this setting
    must be `True`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_PARALLEL`—如果你的自定义模型是在多GPU上训练的，此设置必须为`True`。'
- en: 6.4.2 Concept detectors
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 概念检测器
- en: We will now analyze the results after running the network dissection framework.
    We will first focus on the final convolutional layer (i.e., layer 4) in the ResNet-18
    model and look at the number of unique concept detectors in that layer. The number
    of unique detectors is a measure of the interpretability of the network and measures
    the number of unique concepts learned by the units in that feature-learning layer.
    The higher the number of unique detectors, the more diverse the trained network
    is in detecting human-understandable concepts.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将分析运行网络剖析框架后的结果。我们首先将关注ResNet-18模型中的最终卷积层（即第4层），并查看该层中独特概念检测器的数量。独特检测器的数量是网络可解释性的度量，衡量该特征学习层中单元学习的独特概念数量。独特检测器的数量越多，训练的网络在检测人类可理解的概念方面就越多样化。
- en: 'Let’s first look at the structure of the output of the results folder for the
    network dissection framework. The OUTPUT_FOLDER setting gives you the path to
    the results folder. We saw in the previous section the relevant files that are
    saved in that folder. Let’s now process tally_layer4.csv to compute the number
    of unique detectors in layer 4 of the ResNet-18 model and the proportion of the
    units covered by those unique detectors. The following function can be used to
    compute the relevant statistics. The function takes in the following keyword arguments:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看网络剖析框架结果文件夹的输出结构。OUTPUT_FOLDER设置提供了结果文件夹的路径。我们在上一节中看到了保存在该文件夹中的相关文件。现在，让我们处理tally_layer4.csv来计算ResNet-18模型第4层的独特检测器的数量以及这些独特检测器覆盖的单元比例。以下函数可以用来计算相关统计。该函数接受以下关键字参数：
- en: '`network_names`—A list of the models for which we need to compute the number
    of unique detectors. We are focusing only on the ResNet-18 model in this chapter,
    so this keyword argument is a list containing only one element—`resnet18`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network_names`—需要计算唯一检测器数量的模型列表。在本章中，我们只关注ResNet-18模型，因此这个关键字参数是一个只包含一个元素的列表—`resnet18`。'
- en: '`datasets`—This argument is a list of the datasets that the models are pre-trained
    on. In this chapter, we are focusing on `imagenet` and `places365`.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasets`—这个参数是一个列表，包含了模型预训练的数据集。在本章中，我们关注的是`imagenet`和`places365`。'
- en: '`results_dir`—The parent directory where the results for each of the pretrained
    models are stored.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`results_dir`—存储每个预训练模型结果的父目录。'
- en: '`categories`—A list of all the concept categories for which we need to count
    the number of unique detectors.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categories`—需要计算唯一检测器数量的所有概念类别列表。'
- en: '`iou_thres`—The threshold for the IoU score for which we consider a unit as
    a detector for a concept. As we saw in section 6.3.3, the default value for this
    threshold is 0.04.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iou_thres`—这是IoU得分的阈值，我们根据这个阈值将一个单元视为一个概念检测器。正如我们在6.3.3节中看到的，这个阈值的默认值是0.04。'
- en: '`layer`—The feature-learning layer that we are interested in. In this case,
    we are focusing on the final layer, which is layer 4.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer`—我们感兴趣的特性学习层。在这种情况下，我们关注的是最终层，即第4层。'
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Imports the relevant modules for the function
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入函数所需的模块
- en: ② Function that computes the number of unique detectors. It takes a set of keyword
    arguments.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算唯一检测器数量的函数。它接受一组关键字参数。
- en: ③ The network_names keyword argument is the list of the models for which we
    need to compute the number of unique detectors
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ③ `network_names`关键字参数是我们需要计算唯一检测器数量的模型列表
- en: ④ A list of the datasets that the models are pretrained on
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 模型预训练的数据集列表
- en: ⑤ Points to the parent directory where the network dissection framework saves
    the results for each of the models
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 指向网络剖析框架为每个模型保存结果的父目录
- en: ⑥ A list of all the concept categories of interest
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 所有感兴趣的概念类别列表
- en: ⑦ The IoU threshold to measure if a unit is a concept detector; set to 0.04
    by default
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 测量单元是否为概念检测器的IoU阈值；默认设置为0.04
- en: ⑧ The layer argument is set to the final layer in the ResNet-18 model by default.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ `layer`参数默认设置为ResNet-18模型的最终层。
- en: ⑨ Initializes an empty list to store the results of the number of unique detectors
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 初始化一个空列表来存储唯一检测器数量的结果
- en: ⑩ Iterates through each network or model
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 遍历每个网络或模型
- en: ⑪ Iterates through each dataset that the model is pretrained on
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 遍历模型预训练的每个数据集
- en: ⑫ Loads the tally_{layer}.csv file as a Pandas DataFrame
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 将tally_{layer}.csv文件加载为Pandas DataFrame
- en: ⑬ Initializes an OrderedDict data structure to store the results for a given
    network and dataset
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 初始化一个OrderedDict数据结构来存储给定网络和数据的检测结果
- en: ⑭ Initializes the number of unique detectors to 0
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 将唯一检测器的数量初始化为0
- en: ⑮ Iterates through each concept category
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 遍历每个概念类别
- en: ⑯ Obtains the results for that concept category
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ⑯ 获取该概念类别的结果
- en: ⑰ Filters units for which the IoU score is greater than the threshold
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ⑰ 过滤IoU得分大于阈值的单元
- en: ⑱ The number of rows in the resulting DataFrame is the number of unique detectors
    for that concept category.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ⑱ 结果DataFrame中的行数是该概念类别唯一检测器的数量。
- en: ⑲ Computes the proportion of the total number of units that detect that concept
    category
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ⑲ 计算检测该概念类别的单元占总单元数量的比例
- en: ⑳ Increments the count of number of unique detectors
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ⑳ 增加唯一检测器数量的计数
- en: ㉑ Stores the unique detector results in the OrderedDict data structure
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ㉑ 将唯一检测器结果存储在OrderedDict数据结构中
- en: ㉒ Appends the results in the list
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ㉒ 将结果追加到列表中
- en: ㉓ Converts the list of results to a Pandas DataFrame
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ㉓ 将结果列表转换为Pandas DataFrame
- en: ㉔ Returns the DataFrame
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ㉔ 返回DataFrame
- en: 'We can obtain the number of unique detectors for the final layer of the ResNet-18
    model pretrained on ImageNet and Places by running the following line of code.
    Note that no keyword arguments are provided to the function because the default
    values for the arguments will compute the stats for the ResNet-18 model pretrained
    on ImageNet and Places for the final feature-learning layer:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码行来获取在ImageNet和Places上预训练的ResNet-18模型最终特征学习层的唯一检测器数量。请注意，该函数没有提供任何关键字参数，因为参数的默认值将计算ImageNet和Places上预训练的ResNet-18模型最终特征学习层的统计数据：
- en: '[PRE12]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we wanted to compute the statistics for, say, the third feature-learning
    layer, we could call the function as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要计算，比如说，第三特征学习层的统计数据，我们可以按照以下方式调用函数：
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once we have obtained the number of unique detectors as a Pandas DataFrame,
    we use the following function to plot the results:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了独特检测器的数量作为Pandas DataFrame，我们使用以下函数来绘制结果：
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Plots the number of unique detectors; takes in the DataFrame returned by the
    compute_unique_detectors function and certain keyword arguments
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ① 绘制独特检测器的数量；接受compute_unique_detectors函数返回的DataFrame和某些关键字参数
- en: ② A list of all the concept categories that we are interested in
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们感兴趣的所有概念类别的列表
- en: ③ List of column names that contains the number of unique detectors for each
    concept category
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 包含每个概念类别独特检测器数量的列名列表
- en: ④ List of column names that contains the proportion of unique detectors for
    each concept category
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 包含每个概念类别独特检测器比例的列名列表
- en: ⑤ Dictionary to rename the column names to the capitalized concept category
    names
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将列名重命名为大写概念类别名称的字典
- en: ⑥ Indexes the DataFrame by the network name and dataset
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 按网络名称和数据集索引DataFrame
- en: ⑦ Renames the column names to the capitalized concept category names
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 将列名重命名为大写概念类别名称
- en: ⑧ Creates a Matplotlib figure with two subplot rows
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 创建一个具有两个子图行的Matplotlib图
- en: ⑨ On the first subplot, visualizes the number of unique detectors as a stacked
    bar plot
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 在第一个子图中，将独特检测器的数量以堆叠条形图的形式可视化
- en: ⑩ On the second subplot, visualizes the proportion of unique detectors as a
    stacked bar plot
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 在第二个子图中，将独特检测器的比例以堆叠条形图的形式可视化
- en: ⑪ Returns the Matplotlib figure and axis
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 返回Matplotlib图和坐标轴
- en: 'Plot the number of unique detectors and proportions as follows. The resulting
    figure is shown in figure 6.11:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式绘制独特检测器和比例。结果图如图6.11所示：
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The top row in figure 6.11 shows the absolute number of unique detectors in
    the final feature-learning layer for the two ResNet-18 models pretrained on the
    ImageNet and the Places datasets. The bottom row shows the count as a proportion
    of the total number of units in the final layer. The total number of units in
    the final feature-learning layer in ResNet-18 is 512\. We can see that the ImageNet
    model has 302 unique detectors, and this accounts for roughly 59% of the total
    units. The Places model, on the other hand, has 435 unique detectors, and this
    accounts for roughly 85% of the total units. Overall, it looks like the model
    trained on the Places dataset has a much more diverse set of concept detectors
    than ImageNet. Places are typically composed of multiple scenes. This is why we
    see a lot more scene detectors emerging in the model trained on the Places dataset
    than on the ImageNet dataset. The ImageNet dataset consists of a lot more objects.
    This is why we see a lot more object detectors emerging on the ImageNet model.
    We can also observe a lot more high-level concepts like objects and scenes emerging
    in the final feature-learning layer than low-level concepts like colors, textures,
    and parts.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11的最上面一行显示了在ImageNet和Places数据集上预训练的两个ResNet-18模型在最终特征学习层中的独特检测器的绝对数量。最下面一行显示了作为最终层中单元总数比例的计数。ResNet-18的最终特征学习层中的单元总数为512。我们可以看到，ImageNet模型有302个独特检测器，这大约占总单元的59%。另一方面，Places模型有435个独特检测器，这大约占总单元的85%。总的来说，看起来在Places数据集上训练的模型比ImageNet拥有更多样化的概念检测器集。地点通常由多个场景组成。这就是为什么我们在Places数据集上训练的模型中比在ImageNet数据集中看到更多的场景检测器。ImageNet数据集包含更多的物体。这就是为什么我们在ImageNet模型上看到更多的物体检测器。我们还可以观察到在最终特征学习层中比低级概念（如颜色、纹理和部分）出现更多的更高级概念，如物体和场景。
- en: '![](../Images/CH06_F11_Thampi.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F11_Thampi.png)'
- en: Figure 6.11 Number of unique detectors—ImageNet versus Places
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 独特检测器的数量——ImageNet与Places的比较
- en: Let’s now extend the `compute_unique_detectors` function to compute unique detectors
    for all the feature-learning layers in the ResNet-18 model. This is so that we
    can observe what concepts are learned by all the layers in the network. As an
    exercise, I encourage you to update the function and use a `layers` keyword argument
    that represents a list of feature-learning layers. Also add a nested for loop
    to iterate through all the layers to compute the number of unique detectors for
    each layer. The solution to this exercise can be found in the GitHub repository
    associated with this book at [http://mng.bz/KBdZ](http://mng.bz/KBdZ).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将扩展`compute_unique_detectors`函数，以计算ResNet-18模型中所有特征学习层的独特检测器。这样我们可以观察到网络中所有层学习到的概念。作为一个练习，我鼓励你更新这个函数，并使用一个表示特征学习层列表的`layers`关键字参数。还要添加一个嵌套循环来遍历所有层，计算每层的独特检测器数量。这个练习的解决方案可以在与本书相关的GitHub存储库中找到，网址为[http://mng.bz/KBdZ](http://mng.bz/KBdZ)。
- en: 'Once you have obtained the DataFrame with the number of unique detectors for
    all the layers, you can use the following helper function to plot the statistics
    as a line graph:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你获得了包含所有层独特检测器数量的DataFrame，你可以使用以下辅助函数将统计数据绘制成折线图：
- en: '[PRE16]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Plots the proportion of unique detectors for all the layers in the network
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ① 绘制网络中所有层的独特检测器的比例
- en: ② Plots the proportion of unique detectors for all the layers in a network pretrained
    on a given dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ② 绘制在给定数据集上预训练的网络中所有层的独特检测器比例
- en: ③ Extracts the statistics for all the concept categories
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 提取所有概念类别的统计数据
- en: ④ Plots the statistics as a line chart
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将统计数据绘制成折线图
- en: ⑤ Displays the legend
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 显示图例
- en: ⑥ Labels the x-ticks for all the layers in the network
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 标记网络中所有层的x轴刻度
- en: ⑦ Labels the y-axis
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 标记y轴
- en: ⑧ Filters rows from the source DataFrame for the network pretrained on the Places
    dataset
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 从源DataFrame中筛选出在Places数据集上预训练的网络的过滤器行
- en: ⑨ Filters rows from the source DataFrame for the network pretrained on the ImageNet
    dataset
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 从源DataFrame中筛选出在ImageNet数据集上预训练的网络的过滤器行
- en: ⑩ Creates a Matplotlib figure with two subplot rows
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 创建一个具有两个子图行的Matplotlib图
- en: ⑪ Plots the statistics for all the layers in the first subplot row for the ImageNet
    dataset
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 绘制ImageNet数据集第一子图行中所有层的统计数据
- en: ⑫ Plots the statistics for all the layers in the second subplot row for the
    Places dataset
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 绘制Places数据集第二子图行中所有层的统计数据
- en: ⑬ Returns the figure and axis
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 返回图和轴
- en: 'We can obtain the plot in figure 6.12 by running the following line of code:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码行获得图6.12中的图表：
- en: '[PRE17]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/CH06_F12_Thampi.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F12_Thampi.png)'
- en: Figure 6.12 Number of unique detectors by layers—ImageNet versus Places
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 层数中独特检测器的数量——ImageNet与Places
- en: The top row in figure 6.12 shows the proportion of unique detectors across all
    the layers in the ResNet-18 model pretrained on the ImageNet dataset. The bottom
    row shows the same statistics for the model trained on the Places dataset. We
    can see that for both models, low-level concept categories like colors and textures
    emerge in the lower feature-learning layers, and high-level concept categories
    like parts, objects, and scenes emerge in the higher or deeper layers. This means
    that more high-level concepts are learned at the deeper layers. We can see that
    the representational ability of the network increases with the layer depth. Deeper
    layers have more capacity to learn complex visual concepts like objects and scenes.
    In the following section, we will dissect the network further by looking at specific
    labels and concepts that are learned by each of the units in the network.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12的顶部行显示了在ImageNet数据集上预训练的ResNet-18模型中所有层的独特检测器的比例。底部行显示了在Places数据集上训练的模型的相同统计数据。我们可以看到，对于这两个模型，低级概念类别如颜色和纹理在较低的特征学习层中出现，而高级概念类别如部分、物体和场景在较高或较深的层中出现。这意味着在较深的层中学习了更多的高级概念。我们可以看到，随着层深度的增加，网络的表征能力也在增加。较深的层有更多的能力学习复杂的视觉概念，如物体和场景。在下一节中，我们将通过查看网络中每个单元学习到的特定标签和概念来进一步剖析网络。
- en: 6.4.3 Concept detectors by training task
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 训练任务的概念检测器
- en: 'In the previous section, we visualized the number of unique detectors for all
    the high-level concept categories. Let’s now dig deeper and visualize the number
    of unique detectors for each concept or label in the Broden dataset. We will focus
    on the final feature-learning layers and three top-ranked concept categories in
    terms of the number of unique detectors: textures, objects, and scenes.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们可视化了所有高级概念类别中独特检测器的数量。现在让我们深入挖掘，并可视化Broden数据集中每个概念或标签的独特检测器数量。我们将关注最终的特征学习层以及按独特检测器数量排名前三的概念类别：纹理、物体和场景。
- en: We will need to extend the `compute_unique_detectors` function developed in
    section 6.4.2 to compute the statistics per concept or label. As an exercise,
    I highly encourage you to do this because it will give you a better understanding
    of the format of the tally_{layer}.csv file generated by the NetDissect-Lite library.
    You can pass in a new keyword argument that lets the function know whether to
    aggregate by concept category or by concept or label. For aggregating by concept
    or label, you will need to group by both `category` and `label` and count the
    number of units for which the category IoU score is greater than the threshold.
    The solution to this exercise can be found in the GitHub repository associated
    with this book. Invoke the new function and store the result in a DataFrame called
    `df_cat_label_ud`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将6.4.2节中开发的`compute_unique_detectors`函数扩展到按概念或标签计算统计数据。作为一个练习，我强烈建议您这样做，因为它将使您更好地理解NetDissect-Lite库生成的tally_{layer}.csv文件的格式。您可以传递一个新的关键字参数，让函数知道是否按概念类别或按概念或标签聚合。对于按概念或标签聚合，您需要按`category`和`label`分组，并计算类别IoU分数大于阈值的单元数量。这个练习的解决方案可以在与本书相关的GitHub存储库中找到。调用新函数并将结果存储在名为`df_cat_label_ud`的DataFrame中。
- en: 'We will first look at the texture concept category. Extract the DataFrames
    for the texture concept category using the following code snippet:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将查看纹理概念类别。使用以下代码片段提取纹理概念类别的DataFrames：
- en: '[PRE18]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Extracts the statistics for the ResNet-18 model pretrained on ImageNet and
    sorts the IoU scores in descending order
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从在ImageNet上预训练的ResNet-18模型中提取统计数据，并按IoU分数降序排列
- en: ② Extracts the statistics for the ResNet-18 model pretrained on Places and sorts
    the IoU scores in descending order
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从在Places上预训练的ResNet-18模型中提取统计数据，并按IoU分数降序排列
- en: 'You can now visualize the number of unique detectors for the various texture
    concepts using the following code. The resulting figure is shown in figure 6.13:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用以下代码可视化各种纹理概念的独特检测器数量。结果图如图6.13所示：
- en: '[PRE19]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Imports the Seaborn library
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入Seaborn库
- en: ② Creates a Matplotlib figure with two subplot columns
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个Matplotlib图，包含两个子图列
- en: ③ Plots the number of unique detectors for all the texture concepts learned
    by the ImageNet model
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 绘制ImageNet模型学习到的所有纹理概念的独特检测器数量
- en: ④ Plots the number of unique detectors for all the texture concepts learned
    by the Places model
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 绘制Places模型学习到的所有纹理概念的独特检测器数量
- en: In the previous section (see figure 6.11), we observed that the ImageNet model
    has more unique detectors in the final layer for the texture concept category
    than the model trained on the Places dataset. But how diverse are the concepts
    learned by the units in this layer? Figure 6.13 aims to answer this. We can see
    that the ImageNet model covers 27 texture concepts, whereas the Places model covers
    21\. The top three textures for the ImageNet model account for 19 unique detectors.
    They are striped, waffled, and spiraled. The top three textures for the Places
    model, on the other hand, account for 10 unique detectors. They are interlaced,
    checkered, and stratified. Although the number of textures learned by the units
    in the final layer are less for the Places model, we see a higher proportion of
    unique detectors for this model in the lower feature-learning layers (as seen
    in figure 6.12).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节（见图6.11）中，我们观察到，对于纹理概念类别，ImageNet模型在最终层的独特检测器数量比在Places数据集上训练的模型更多。但是，这个层中单元学习到的概念有多多样？图6.13旨在回答这个问题。我们可以看到，ImageNet模型涵盖了27个纹理概念，而Places模型涵盖了21个。ImageNet模型的前三个纹理概念占19个独特检测器。它们是条纹、波浪和螺旋。另一方面，Places模型的前三个纹理概念占10个独特检测器。它们是交织、棋盘和层状。尽管Places模型在最终层中学习的纹理数量较少，但我们看到在较低的特征学习层中，这个模型的独特检测器比例更高（如图6.12所示）。
- en: '![](../Images/CH06_F13_Thampi.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F13_Thampi.png)'
- en: Figure 6.13 Number of unique texture detectors—ImageNet versus Places
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 唯一纹理检测器数量——ImageNet与Places
- en: Let’s now visualize the number of unique detectors for various object and scene
    concepts. As an exercise, extend the code written for the textures concept category
    to objects and scenes. The resulting figure for the object concept category is
    shown in figure 6.14\. Because the model trained on the Places dataset detects
    a lot more scenes in the final layer, the visualization for the scene concept
    category has been split into two separate figures. Figure 6.15 shows the number
    of scene detectors for the ImageNet model, and figure 6.16 shows the number of
    scene detectors for the Places model.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化各种对象和场景概念的唯一检测器数量。作为一个练习，将用于纹理概念类别的代码扩展到对象和场景。对象概念类别的结果图显示在图6.14中。因为基于Places数据集训练的模型在最终层检测到很多场景，所以场景概念类别的可视化被分成两个单独的图表。图6.15显示了ImageNet模型的场景检测器数量，图6.16显示了Places模型的场景检测器数量。
- en: '![](../Images/CH06_F14_Thampi.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F14_Thampi.png)'
- en: Figure 6.14 Number of unique object detectors—ImageNet versus Places
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 唯一对象检测器数量——ImageNet与Places
- en: Let’s first look at figure 6.14\. In the previous section, we observed that
    the ImageNet model has a higher proportion of unique detectors for the high-level
    objects category because the ImageNet dataset has a lot more objects in it. If
    we look at how diverse the concepts learned are, however, we can see that many
    more objects emerge in the model trained on the Places dataset. The Places model
    detects 45 objects as opposed to the 36 objects detected by the ImageNet model
    in the final feature-learning layer. The top object detected by the ImageNet model
    is dog, which accounts for 25 unique detectors—a high proportion of labeled images
    of dogs are present in the ImageNet dataset. The top object detected by the Places
    model is airplane, which accounts for 11 unique detectors.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看图6.14。在前一节中，我们观察到ImageNet模型在高级对象类别中有更高比例的唯一检测器，因为ImageNet数据集中包含更多的对象。然而，如果我们观察学习到的概念有多样化，我们可以看到在Places数据集上训练的模型中出现了更多的对象。Places模型检测到45个对象，而ImageNet模型在最终特征学习层中检测到的对象是36个。ImageNet模型检测到的顶级对象是狗，占25个唯一检测器——ImageNet数据集中有大量标记的狗的图像。Places模型检测到的顶级对象是飞机，占11个唯一检测器。
- en: '![](../Images/CH06_F15_Thampi.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F15_Thampi.png)'
- en: Figure 6.15 Number of unique scene detectors—ImageNet dataset
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 唯一场景检测器数量——ImageNet数据集
- en: If we now compare figures 6.15 and 6.16, we can see that the model trained on
    the Places dataset is able to identify a much more diverse set of scenes than
    the ImageNet model (119 versus 30). This is expected because the Places dataset
    contains a lot of labeled places, which are typically made up of a lot of scenes.
    Note that in figure 6.16, although the model trained on the Places dataset is
    able to identify 119 scenes in total, the figure is only showing the top 40 scenes
    to make it easier to read the figure.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在比较图6.15和图6.16，我们可以看到在Places数据集上训练的模型能够识别比ImageNet模型（119个对30个）更多样化的场景集。这是预期的，因为Places数据集包含大量的标记地点，这些地点通常由许多场景组成。注意，在图6.16中，尽管在Places数据集上训练的模型总共能够识别119个场景，但图中只显示了前40个场景，以便更容易阅读图表。
- en: '![](../Images/CH06_F16_Thampi.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F16_Thampi.png)'
- en: Figure 6.16 Number of unique scene detectors—Places dataset
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 唯一场景检测器数量——Places数据集
- en: By going deeper and visualizing the number of unique detectors for each concept,
    we can ensure that the dataset used to train the model is diverse enough and has
    good coverage of the concepts of interest. We can also use these visualizations
    to understand what concepts the units are focusing on in each layer of the neural
    network.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入挖掘并可视化每个概念的唯一检测器数量，我们可以确保用于训练模型的数据库足够多样化，并且对感兴趣的概念有良好的覆盖。我们还可以使用这些可视化来了解单元在每个神经网络层的关注点是什么。
- en: Transfer learning
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Transfer learning is a technique whereby a model trained for a particular task
    is used as a starting point for another task. For example, let’s say we have a
    model trained on the ImageNet dataset that is great at detecting objects. We would
    like to use this model as a starting point to detect places and scenes. To do
    this, we can load the weights of the ImageNet model and use those weights as a
    starting point before training and fine-tuning them to the Places dataset. The
    general idea behind transfer learning is that features learned in one domain can
    be reused in another domain, provided there is some overlap between the two domains.
    When a pretrained network in one domain is trained for a task in another domain,
    the training time is typically faster and yields more accurate results.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种技术，其中针对特定任务训练的模型被用作另一个任务的起点。例如，假设我们有一个在ImageNet数据集上训练的模型，该模型在检测对象方面表现良好。我们希望将此模型用作起点来检测地点和场景。为此，我们可以加载ImageNet模型的权重，并在训练和微调到Places数据集之前使用这些权重作为起点。迁移学习背后的基本思想是，在一个领域学习到的特征可以在另一个领域重用，前提是这两个领域之间有一些重叠。当一个领域的预训练网络在另一个领域的任务上进行训练时，通常训练时间更快，并且会产生更准确的结果。
- en: The authors of the network dissection framework analyzed how the interpretability
    of units evolves during transfer learning in their paper, available at [https://arxiv.org/
    pdf/1711.05611.pdf](https://arxiv.org/pdf/1711.05611.pdf). They used the AlexNet
    model pretrained on the ImageNet dataset and fine-tuned it to the Places dataset.
    The authors observed that the number of unique detectors increased for the model
    pretrained on ImageNet but fine-tuned to Places. Units that detected dogs initially
    evolved to other objects and scenes like horse, cow, and waterfall. A lot of the
    places in the Places dataset contain those animals and scenes. If the model pretrained
    on Places is fine-tuned to the ImageNet dataset, the authors observed a drop in
    the number of unique detectors. For the Places-to-ImageNet network, a lot of the
    units evolve to dog detectors because the proportion of labeled data for dogs
    is much higher in ImageNet.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 网络分解框架的作者在其论文中分析了单元的可解释性在迁移学习过程中的演变，该论文可在[https://arxiv.org/pdf/1711.05611.pdf](https://arxiv.org/pdf/1711.05611.pdf)找到。他们使用了在ImageNet数据集上预训练并微调到Places数据集的AlexNet模型。作者观察到，对于在ImageNet上预训练但微调到Places的模型，独特检测器的数量有所增加。最初检测狗的单元演变为其他对象和场景，如马、牛和瀑布。Places数据集中的许多地方都包含这些动物和场景。如果预训练在Places上的模型微调到ImageNet数据集，作者观察到独特检测器的数量有所下降。对于Places-to-ImageNet网络，许多单元演变为狗检测器，因为ImageNet中狗的标记数据比例要高得多。
- en: 6.4.4 Visualizing concept detectors
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 可视化概念检测器
- en: In the previous sections, we quantified the interpretability of the units in
    each feature-learning layer in the CNN by looking at the number of unique detectors
    for each concept category and individual concepts. In this section, we will visualize
    the binary unit segmentation maps generated for each of the units in the feature-learning
    layers by the NetDissect library. The library will overlay the binary unit segmentation
    maps on the original images and generate JPG files for us. This is useful if we
    want to visualize the specific pixels in the original image that the unit is focusing
    on for a given concept.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们通过查看每个概念类别和个别概念的独特检测器数量来量化CNN中每个特征学习层中单元的可解释性。在本节中，我们将通过NetDissect库可视化特征学习层中每个单元生成的二值单元分割图。该库将在原始图像上叠加二值单元分割图并为我们生成JPG文件。如果我们想可视化原始图像中单元针对特定概念关注的特定像素，这将非常有用。
- en: 'Due to lack of space, we cannot visualize all the images generated for all
    the units and models. We will, therefore, focus on the model pretrained on the
    Places dataset, as well as specific units with the maximally activated images
    for certain concepts. The following function can be used to obtain the binary
    segmented images generated by the library:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间有限，我们无法可视化所有单元和模型生成的所有图像。因此，我们将重点关注在Places数据集上预训练的模型，以及某些概念的最大激活图像的特定单元。以下函数可以用来获取由库生成的二值分割图像：
- en: '[PRE20]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Imports the mpimg module provided by Matplotlib to load and display images
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入Matplotlib提供的mpimg模块以加载和显示图像
- en: ② Gets the binary unit segmentation map overlayed on the image and the associated
    stats for a given unit
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取给定单元在图像上叠加的二值单元分割图及其相关统计数据
- en: ③ Obtains the network name, dataset, results directory, feature-learning layer,
    and unit of interest
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 获取网络名称、数据集、结果目录、特征学习层和感兴趣的单元
- en: ④ Loads the tally results file as a Pandas DataFrame
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将计数字段文件加载为Pandas DataFrame
- en: ⑤ Loads the binary unit segmented map overlaid on the original image
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 加载叠加在原始图像上的二进制单元分割图
- en: ⑥ Filters the DataFrame to obtain statistics for the layer and unit of interest
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤DataFrame以获取感兴趣层和单元的统计数据
- en: ⑦ Initializes the stats variable to None
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化统计数据变量为None
- en: ⑧ If there are results for the layer and unit of interest, extracts the category,
    label, and IoU from the DataFrame and stores them in dict
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感兴趣层和单元有结果，从DataFrame中提取类别、标签和IoU，并将它们存储在字典中
- en: ⑨ Returns the image and associated statistics for the layer and unit of interest
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 返回感兴趣层和单元的图像和相关统计数据
- en: 'We will now focus on units 247 and 39, which detect the airplane object. We
    saw in the previous section (see figure 6.14) that the airplane object has the
    most unique detectors among all the objects in the Places model. The units are
    zero-indexed and are saved as four-digit strings by the NetDissect library. We,
    therefore, need to pass the strings “0246” and “0038” for units 247 and 39, respectively,
    as the unit keyword argument in the `get_image_and_stats` function. The following
    code snippet will obtain the image and associated statistics and visualize them
    in Matplotlib. The resulting plot is shown in figure 6.17:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将重点关注单元247和39，它们检测飞机对象。我们在上一节中看到（见图6.14），飞机对象在Places模型中的所有对象中具有最独特的检测器。这些单元按零索引，并由NetDissect库保存为四位字符串。因此，我们需要将字符串“0246”和“0038”分别作为单元247和39的单元关键字参数传递给`get_image_and_stats`函数。以下代码片段将获取图像和相关统计数据，并在Matplotlib中可视化。结果图如图6.17所示：
- en: '[PRE21]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Extracts the image and statistics for unit 247
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 从单元247提取图像和统计数据
- en: ② Extracts the image and statistics for unit 39
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 从单元39提取图像和统计数据
- en: ③ Creates a Matplotlib figure with two subplot rows
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个具有两个子图行的Matplotlib图像
- en: ④ Displays the image for unit 247 on the top row and displays the stats in the
    title
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部行显示单元247的图像并在标题中显示统计数据
- en: ⑤ Displays the image for unit 39 on the bottom row and displays the stats in
    the title
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部行显示单元39的图像并在标题中显示统计数据
- en: '![](../Images/CH06_F17_Thampi.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F17_Thampi.png)'
- en: Figure 6.17 Visualization of object concept detector—airplane
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17对象概念检测器——飞机的可视化
- en: The top row in figure 6.17 shows the segmentation generated for the 10 maximally
    activated Broden images for unit 247\. The average IoU is 0.19\. Because the binary
    unit segmentation map is overlaid on the original image, the pixels that are activated
    are those where *S*[i] >= *T*[k]. The pixels that are not activated are shown
    as black. From the images, we can see that the unit is focusing on airplanes and
    not on any other random object. The bottom row in figure 6.17 shows the segmentation
    generated for the 10 maximally activated Broden images for unit 39\. The average
    IoU is 0.06 and is lower in this case. From the images, we can see that the unit
    activated on airplanes as well as on general concepts like birds, flight, sky,
    and the color blue.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17顶部行显示了为单元247的10个最大激活Broden图像生成的分割。平均IoU为0.19。由于二进制单元分割图叠加在原始图像上，激活的像素是那些满足*S*[i]
    >= *T*[k]的像素。未激活的像素显示为黑色。从图像中我们可以看到，该单元专注于飞机，而不是任何其他随机对象。图6.17底部行显示了为单元39的10个最大激活Broden图像生成的分割。平均IoU为0.06，并且在这种情况下较低。从图像中我们可以看到，该单元在飞机上以及一般概念（如鸟类、飞行、天空和蓝色）上激活。
- en: Figure 6.18 shows the binary segmented images generated for three objects, namely,
    train (top row), bus (middle row), and track (bottom row). The specific units
    are 168, 386, and 218, respectively. For the train concept detector, we can see
    the activated pixels highlighting engines and railway tracks. The average IoU
    is high in this case, at 0.27\. For the bus concept detector, the activated pixels
    seem to highlight buses and general concepts like any vehicle with large windows
    and a relatively flat front. The average IoU in this case is 0.24\. The track
    concept detector is interesting. The activated pixels seem to highlight images
    with two parallel tracks, which include railway tracks, bowling alley lanes, and
    a sushi conveyor belt. The average IoU is 0.06.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18显示了为三个对象生成的二值分割图像，分别是火车（顶部行）、公共汽车（中间行）和轨道（底部行）。具体的单元分别是168、386和218。对于火车概念检测器，我们可以看到激活的像素突出显示引擎和铁路轨道。在这种情况下，平均IoU很高，为0.27。对于公共汽车概念检测器，激活的像素似乎突出显示公共汽车和像任何大型窗户和相对平坦前端的车辆这样的通用概念。在这种情况下，平均IoU为0.24。轨道概念检测器很有趣。激活的像素似乎突出显示具有两条平行轨道的图像，包括铁路轨道、保龄球道和寿司传送带。在这种情况下，平均IoU为0.06。
- en: '![](../Images/CH06_F18_Thampi.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F18_Thampi.png)'
- en: Figure 6.18 Visualization of object concept detector—train, bus, and track
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 物体概念检测器可视化——火车、公共汽车和轨道
- en: Finally, figure 6.19 shows the segmented images for scenes that are not directly
    represented in the training set. We are specifically focusing on units 379 and
    370, which highlight the highway and nursery scenes, respectively. The top row
    shows the highway scene, and the bottom row shows the nursery scene. We can see
    that the model trained on the Places dataset is learning these high-level scene
    concepts really well.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，图6.19显示了训练集中未直接表示的场景的分割图像。我们特别关注单元379和370，分别突出显示高速公路和托儿所场景。顶部行显示高速公路场景，底部行显示托儿所场景。我们可以看到，在Places数据集上训练的模型正在很好地学习这些高级场景概念。
- en: '![](../Images/CH06_F19_Thampi.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F19_Thampi.png)'
- en: Figure 6.19 Visualization of scene concept detector—highway and nursery
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 场景概念检测器可视化——高速公路和托儿所
- en: 6.4.5 Limitations of network dissection
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.5 网络解剖的局限性
- en: 'The network dissection framework is a great tool that helps us open up black-box
    neural networks. It overcomes the limitations of visual attribution methods by
    coming up with quantifiable interpretations. We can see how a CNN decomposes the
    task of identifying an image by visualizing the features or concepts learned by
    each of the units in the feature-learning layers. The network dissection framework,
    however, has the following limitations, as highlighted in the original paper by
    the authors of the framework:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 网络解剖框架是一个伟大的工具，帮助我们打开黑盒神经网络。它通过提出可量化的解释来克服视觉归因方法的局限性。我们可以通过可视化每个单元在特征学习层中学习的特征或概念，来了解CNN如何分解识别图像的任务。然而，网络解剖框架具有以下局限性，如框架原作者在原始论文中所强调的：
- en: The framework requires a labeled dataset of concepts at the pixel level. This
    is the most crucial step in the framework and can be quite time-consuming and
    costly. Moreover, concepts that are not expressed in the dataset will not show
    up when interpreting the units, even if the network has learned them.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该框架需要像素级别的概念标记数据集。这是框架中最关键的一步，可能非常耗时且成本高昂。此外，数据集中未表达的概念在解释单元时不会显示出来，即使网络已经学习了它们。
- en: The framework cannot identify groups of units that jointly represent one concept.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该框架无法识别代表一个概念的多个单元的组合。
- en: The interpretability of units is quantified by the “number of unique detectors”
    metric. This metric favors larger and deeper networks that have the capacity to
    learn more high-level concepts.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元的可解释性通过“唯一检测器数量”指标来量化。该指标倾向于更大、更深的网络，这些网络具有学习更多高级概念的能力。
- en: Dissecting neural network is an active area of research, and the research community
    is exploring many promising avenues, such as automatic identification of concepts
    and using concept scores to identify adversarial attacks on neural networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的解剖分析是一个活跃的研究领域，研究社区正在探索许多有希望的道路，例如自动识别概念和使用概念分数来识别对神经网络的对抗攻击。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The visual attribution methods we learned in the previous chapter have some
    limitations. They are typically assessed qualitatively and are quite subjective.
    These techniques do not give us any information on the low-level and high-level
    concepts or features that are learned by the feature-learning layers and units
    in a convolutional neural network (CNN).
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在上一章中学到的视觉归因方法存在一些局限性。它们通常通过定性评估，并且相当主观。这些技术并没有给我们提供关于卷积神经网络（CNN）中特征学习层和单元所学习的低级和高级概念或特征的信息。
- en: The network dissection framework discussed in this chapter overcomes the limitations
    of visual attribution methods by coming up with more quantitative interpretations.
    By using the framework, we will also be able to understand what human-understandable
    concepts are learned by the feature-learning layers in the CNN.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章讨论的网络剖析框架通过提出更定量的解释来克服视觉归因方法的局限性。通过使用该框架，我们还将能够理解CNN的特征学习层学习了哪些人类可理解的概念。
- en: 'The framework consists of three key steps: concept definition, network probing,
    and alignment measurement. The concept definition step is the most crucial step
    because it requires us to collect a labeled dataset of concepts at the pixel level.
    The network probing step is about finding units in the network that respond to
    those predefined concepts. Finally, the alignment measurement step quantifies
    how well the unit activation aligns with those concepts.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该框架包括三个关键步骤：概念定义、网络探测和对齐测量。概念定义步骤是最关键的步骤，因为它要求我们收集一个像素级别的概念标记数据集。网络探测步骤是关于在网络中找到对那些预定义概念做出响应的单元。最后，对齐测量步骤量化了单元激活与那些概念之间的对齐程度。
- en: We learned how to run the network dissection framework using the NetDissect
    library on PyTorch models trained on the ImageNet and Places datasets. We used
    the Broden dataset for the concepts.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们学习了如何使用NetDissect库在PyTorch模型上运行网络剖析框架，这些模型是在ImageNet和Places数据集上训练的。我们使用了Broden数据集来定义概念。
- en: We learned how to quantify the interpretability of the units by using the “number
    of unique detectors” metric and visualized the interpretability of units for various
    concept categories and individual concepts.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们学习了如何通过使用“独特检测器数量”指标来量化单元的可解释性，并可视化不同概念类别和个别概念单元的可解释性。
- en: We also learned how to visualize the binary unit segmented images generated
    by the library to see what pixels a unit is focusing on for a particular concept.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还学习了如何可视化由库生成的二值单元分割图像，以查看单元对特定概念关注的像素。
- en: The network dissection framework is a great tool that helps us open up black-box
    neural networks. It suffers from a few limitations, however. Creating a labeled
    dataset of concepts can be quite time-consuming and costly. The framework cannot
    identify groups of units that jointly represent one concept. The “number of unique
    detectors” metric favors larger and deeper networks, which have the capacity to
    learn more high-level concepts.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络剖析框架是一个帮助我们打开黑盒神经网络的优秀工具。然而，它也有一些局限性。创建一个概念标记数据集可能非常耗时且成本高昂。该框架无法识别代表一个概念的单元组。独特的检测器数量指标倾向于更大的、更深的网络，这些网络有学习更多高级概念的能力。
