- en: Chapter 14\. Singular Value Decomposition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 14 章. 奇异值分解
- en: The previous chapter was really dense! I tried my best to make it comprehensible
    and rigorous, without getting too bogged down in details that have less relevance
    for data science.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章真的很密集！我尽力使其易于理解和严谨，而不至于陷入对数据科学少相关的细节中。
- en: Fortunately, most of what you learned about eigendecomposition applies to the
    SVD. That means that this chapter will be easier and shorter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大部分你学到的关于特征分解的内容都适用于奇异值分解。这意味着本章会更容易也更简短。
- en: 'The purpose of the SVD is to decompose a matrix into the product of three matrices,
    called the left singular vectors ( <math alttext="bold upper U"><mi>𝐔</mi></math>
    ), the singular values (<math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>),
    and the right singular vectors ( <math alttext="bold upper V"><mi>𝐕</mi></math>
    ):'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解的目的是将矩阵分解为三个矩阵的乘积，称为左奇异向量（ <math alttext="粗体大写 U"><mi>𝐔</mi></math> ）、奇异值（<math
    alttext="粗体大写 Sigma"><mi mathvariant="bold">Σ</mi></math>）和右奇异向量（ <math alttext="粗体大写
    V"><mi>𝐕</mi></math> ）：
- en: <math alttext="bold upper A equals bold upper U bold upper Sigma bold upper
    V Superscript upper T" display="block"><mrow><mi mathvariant="bold">A</mi> <mo>=</mo>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup></mrow></math>
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A equals bold upper U bold upper Sigma bold upper
    V Superscript upper T" display="block"><mrow><mi mathvariant="bold">A</mi> <mo>=</mo>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup></mrow></math>
- en: This decomposition should look similar to eigendecomposition. In fact, you can
    think of the SVD as a generalization of eigendecomposition to nonsquare matrices—or
    you can think of eigendecomposition as a special case of the SVD for square matrices.^([1](ch14.xhtml#idm45733291549488))
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解看起来类似于特征分解。事实上，你可以把奇异值分解看作是对非方阵的特征分解的一般化，或者你可以把特征分解看作是奇异值分解在方阵情况下的特殊情况。^([1](ch14.xhtml#idm45733291549488))
- en: The singular values are comparable to eigenvalues, while the singular vectors
    matrices are comparable to eigenvectors (these two sets of quantities are the
    same under some circumstances that I will explain later).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值可与特征值相比较，而奇异向量矩阵则与特征向量可比较（在某些情况下这两组量是相同的，我稍后会解释）。
- en: The Big Picture of the SVD
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解的全局视角
- en: I want to introduce you to the idea and interpretation of the matrices, and
    then later in the chapter I will explain how to compute the SVD.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我想先让你了解矩阵的概念和解释，然后在本章后面解释如何计算奇异值分解。
- en: '[Figure 14-1](#fig_14_1) shows the overview of the SVD.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-1](#fig_14_1) 显示了奇异值分解的概览。'
- en: '![The SVD](assets/plad_1401.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](assets/plad_1401.png)'
- en: Figure 14-1\. The big picture of the SVD
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-1. 奇异值分解的全局视角
- en: 'Many important features of the SVD are visible in this diagram; I will go into
    these features in more detail throughout this chapter, but to put them into a
    list:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中可以看到奇异值分解的许多重要特性；我将在本章中更详细地解释这些特性，但先简要列举一下：
- en: Both <math alttext="bold upper U"><mi>𝐔</mi></math> and <math alttext="bold
    upper V"><mi>𝐕</mi></math> are square matrices, even when <math alttext="bold
    upper A"><mi>𝐀</mi></math> is nonsquare.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="粗体大写 U"><mi>𝐔</mi></math> 和 <math alttext="粗体大写 V"><mi>𝐕</mi></math>
    都是方阵，即使 <math alttext="粗体大写 A"><mi>𝐀</mi></math> 不是方阵。
- en: The matrices of singular vectors <math alttext="bold upper U"><mi>𝐔</mi></math>
    and <math alttext="bold upper V"><mi>𝐕</mi></math> are orthogonal, meaning <math
    alttext="bold upper U Superscript upper T Baseline bold upper U equals bold upper
    I"><mrow><msup><mi>𝐔</mi> <mtext>T</mtext></msup> <mi>𝐔</mi> <mo>=</mo> <mi>𝐈</mi></mrow></math>
    and <math alttext="bold upper V Superscript upper T Baseline bold upper V equals
    bold upper I"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup> <mi>𝐕</mi> <mo>=</mo>
    <mi>𝐈</mi></mrow></math> . As a reminder, this means that each column is orthogonal
    to each other column, and any subset of columns is orthogonal to any other (nonoverlapping)
    subset of columns.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异向量矩阵 <math alttext="粗体大写 U"><mi>𝐔</mi></math> 和 <math alttext="粗体大写 V"><mi>𝐕</mi></math>
    是正交的，意味着 <math alttext="粗体大写 U 上标 T 上标 bold upper U 等于 bold upper I"><mrow><msup><mi>𝐔</mi>
    <mtext>T</mtext></msup> <mi>𝐔</mi> <mo>=</mo> <mi>𝐈</mi></mrow></math> 和 <math
    alttext="粗体大写 V 上标 T 上标 bold upper V 等于 bold upper I"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup>
    <mi>𝐕</mi> <mo>=</mo> <mi>𝐈</mi></mrow></math> 。提醒一下，这意味着每一列都与其他列正交，且任何一个列子集与任何其他（非重叠）列子集也正交。
- en: The first *r* columns of <math alttext="bold upper U"><mi>𝐔</mi></math> provide
    orthogonal basis vectors for the column space of the matrix <math alttext="bold
    upper A"><mi>𝐀</mi></math> , while the rest of the columns provide orthogonal
    basis vectors for the left-null space (unless *r* = *M*, in which case the matrix
    is full column-rank and the left-null space is empty).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="粗体大写 U"><mi>𝐔</mi></math> 的前 *r* 列提供了矩阵 <math alttext="粗体大写 A"><mi>𝐀</mi></math>
    的列空间的正交基向量，而其余列则提供了左零空间的正交基向量（除非 *r* = *M*，此时矩阵具有满列秩且左零空间为空）。
- en: The first *r* rows of <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup></math> (which are the columns of <math alttext="bold upper
    V"><mi>𝐕</mi></math> ) provide orthogonal basis vectors for the row space, while
    the rest of the rows provide orthogonal basis vectors for the null space.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi> <mtext>T</mtext></msup></math>
    的前*r*行（即<math alttext="bold upper V"><mi>𝐕</mi></math> 的列）为行空间提供正交基向量，而其余行为零空间提供正交基向量。
- en: The singular values matrix is a diagonal matrix of the same size as <math alttext="bold
    upper A"><mi>𝐀</mi></math> . The singular values are always sorted from largest
    (top-left) to smallest (lower-right).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值矩阵是与<math alttext="bold upper A"><mi>𝐀</mi></math>相同大小的对角矩阵。奇异值始终按从最大（左上角）到最小（右下角）排序。
- en: All singular values are nonnegative and real-valued. They cannot be complex
    or negative, even if the matrix contains complex-valued numbers.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有奇异值都是非负实数。它们不能是复数或负数，即使矩阵包含复数。
- en: The number of nonzero singular values equals the matrix rank.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非零奇异值的数量等于矩阵的秩。
- en: 'Perhaps the most amazing thing about the SVD is that it reveals all four subspaces
    of the matrix: the column space and left-null space are spanned by the first *r*
    and last *M* − *r* through *M* columns of <math alttext="bold upper U"><mi>𝐔</mi></math>
    , while the row space and null space are spanned by the first *r* and last *N*
    − *r* through *N* rows of <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup></math> . For a rectangular matrix, if *r* = *M*, then
    the left-null space is empty, and if *r* = *N*, then the null space is empty.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或许SVD最惊人的地方在于它揭示了矩阵的四个子空间：列空间和左空间由<math alttext="bold upper U"><mi>𝐔</mi></math>的前*r*列和最后*M*
    − *r*到*M*列，而行空间和零空间由<math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup></math> 的前*r*行和最后*N* − *r*到*N*行提供。对于矩形矩阵，如果*r* = *M*，则左空间为空，如果*r*
    = *N*，则零空间为空。
- en: Singular Values and Matrix Rank
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奇异值与矩阵的秩
- en: The rank of a matrix is defined as the number of nonzero singular values. The
    reason comes from the previous discussion that the column space and the row space
    of the matrix are defined as the left and right singular vectors that are scaled
    by their corresponding singular values to have some “volume” in the matrix space,
    whereas the left and right null spaces are defined as the left and right singular
    vectors that are scaled to zeros. Thus, the dimensionality of the column and row
    spaces are determined by the number of nonzero singular values.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的秩被定义为非零奇异值的数量。这个原因来自于前面的讨论，即矩阵的列空间和行空间被定义为通过它们对应的奇异值进行缩放以在矩阵空间中具有一定“体积”的左奇异向量和右奇异向量，而左空间和右空间被定义为通过它们进行缩放为零的左奇异向量和右奇异向量。因此，列空间和行空间的维数由非零奇异值的数量决定。
- en: 'In fact, we can peer into the NumPy function `np.linalg.matrix_rank` to see
    how Python computes matrix rank (I’ve edited the code slightly to focus on the
    key concepts):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以查看NumPy函数`np.linalg.matrix_rank`来看Python如何计算矩阵的秩（我稍作修改以便专注于关键概念）：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The returned value is the number of singular values that exceed the value of
    `tol`. What is `tol`? That’s a tolerance level that accounts for possible rounding
    errors. It is defined as the machine precision for this data type (`eps`), scaled
    by the largest singular value and the size of the matrix.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的值是超过`tol`值的奇异值数量。`tol`是什么？这是一个容忍度水平，考虑可能的舍入误差。它被定义为此数据类型的机器精度(`eps`)，按照最大奇异值和矩阵的大小进行缩放。
- en: 'Thus, we yet again see the difference between “chalkboard math” and precision
    math implemented on computers: the rank of the matrix is not actually computed
    as the number of nonzero singular values, but instead as the number of singular
    values that are larger than some small number. There is a risk that small but
    truly nonzero singular values are ignored, but that outweighs the risk of incorrectly
    inflating the rank of the matrix when truly zero-valued singular values appear
    nonzero due to precision errors.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们再次看到“黑板上的数学”与计算机上实现的精确数学之间的差异：矩阵的秩实际上并不是计算为非零奇异值的数量，而是计算为大于某个小数的奇异值的数量。有可能忽略一些真正非零但由于精度误差而被忽略的奇异值，但这相对于在真正值为零的情况下由于精度误差而错误地增加矩阵的秩的风险来说更为合理。
- en: SVD in Python
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的SVD
- en: 'The SVD is fairly straightforward to compute in Python:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中计算SVD相对直观：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are two features of NumPy’s `svd` function to keep in mind. First, the
    singular values are returned as a vector, not a matrix of the same size as <math
    alttext="bold upper A"><mi>𝐀</mi></math> . This means that you need some extra
    code to get the <math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>
    matrix:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 的 `svd` 函数有两个需要注意的特点。首先，奇异值作为一个向量返回，而不是与 <math alttext="bold upper A"><mi>𝐀</mi></math>
    相同大小的矩阵。这意味着你需要一些额外的代码来获取 <math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>
    矩阵：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You might initially think of using `np.diag(s)`, but that only produces the
    correct singular values matrix for a square matrix <math alttext="bold upper A"><mi>𝐀</mi></math>
    . Therefore, I first create the correctly sized matrix of zeros, and then fill
    in the diagonal with the singular values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能最初想到使用 `np.diag(s)`，但那只适用于方阵 <math alttext="bold upper A"><mi>𝐀</mi></math>
    的正确奇异值矩阵。因此，我首先创建了正确大小的零矩阵，然后用奇异值填充对角线。
- en: The second feature is that NumPy returns the matrix <math alttext="bold upper
    V Superscript upper T"><msup><mi>𝐕</mi> <mtext>T</mtext></msup></math> , not <math
    alttext="bold upper V"><mi>𝐕</mi></math> . This may be confusing for readers coming
    from a MATLAB background, because the MATLAB `svd` function returns the matrix
    <math alttext="bold upper V"><mi>𝐕</mi></math> . The hint is in the docstring,
    which describes matrix `vh`, where the `h` is for Hermitian, the name of a symmetric
    complex-valued matrix.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个特点是 NumPy 返回矩阵 <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup></math> ，而不是 <math alttext="bold upper V"><mi>𝐕</mi></math>
    。这对于习惯于 MATLAB 的读者可能会感到困惑，因为 MATLAB 的 `svd` 函数返回矩阵 <math alttext="bold upper V"><mi>𝐕</mi></math>
    。在文档字符串中有提示，描述了矩阵 `vh`，其中的 `h` 表示 Hermitian，即对称复值矩阵的名称。
- en: '[Figure 14-2](#fig_14_2) shows the outputs of the `svd` function (with the
    singular values converted into a matrix).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-2](#fig_14_2) 展示了 `svd` 函数的输出（奇异值已转换为矩阵）。'
- en: '![The SVD](assets/plad_1402.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![奇异值分解](assets/plad_1402.png)'
- en: Figure 14-2\. The big picture of the SVD shown for an example matrix
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-2\. 显示了奇异值分解的示例矩阵的整体情况
- en: SVD and Rank-1 “Layers” of a Matrix
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解和矩阵的秩-1“层”
- en: 'The first equation I showed in the previous chapter was the vector-scalar version
    of the eigenvalue equation ( <math alttext="bold upper A bold v equals lamda bold
    v"><mrow><mi>𝐀</mi> <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></math>
    ). I opened this chapter with the *matrix* SVD equation (<math alttext="bold upper
    A equals bold upper U bold upper Sigma bold upper V Superscript upper T"><mrow><mi
    mathvariant="bold">A</mi><mo>=</mo><mi mathvariant="bold">U</mi><mi mathvariant="bold">Σ</mi><msup><mi
    mathvariant="bold">V</mi> <mtext>T</mtext></msup></mrow></math> ); what does that
    equation look like for one vector? We can write it in two different ways that
    highlight different features of the SVD:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一章节展示的第一个方程是特征值方程的向量-标量版本（ <math alttext="bold upper A bold v equals lamda
    bold v"><mrow><mi>𝐀</mi> <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></math>
    ）。我在本章开头使用了*矩阵*奇异值分解方程（ <math alttext="bold upper A equals bold upper U bold upper
    Sigma bold upper V Superscript upper T"><mrow><mi mathvariant="bold">A</mi><mo>=</mo><mi
    mathvariant="bold">U</mi><mi mathvariant="bold">Σ</mi><msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup></mrow></math> ）；对于一个向量，这个方程是什么样子？我们可以用两种不同的方式来书写它，突出奇异值分解的不同特点：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals bold u sigma 2nd Row 1st Column bold u Superscript upper T Baseline bold
    upper A 2nd Column equals sigma bold v Superscript upper T EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>𝐮</mi> <mi>σ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>𝐮</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <msup><mi>𝐯</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals bold u sigma 2nd Row 1st Column bold u Superscript upper T Baseline bold
    upper A 2nd Column equals sigma bold v Superscript upper T EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>𝐮</mi> <mi>σ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>𝐮</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <msup><mi>𝐯</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
- en: 'Those equations are kind of similar to the eigenvalue equation except that
    there are two vectors instead of one. The interpretations are, therefore, slightly
    more nuanced: in general, those equations say that the effect of the matrix on
    one vector is the same as the effect of a scalar on a different vector.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程有点类似于特征值方程，只是有两个向量而不是一个。因此，它们的解释略微更为细腻：总体来说，这些方程表明矩阵对一个向量的作用与一个不同向量的标量作用相同。
- en: Notice that the first equation means that <math alttext="bold u"><mi>𝐮</mi></math>
    is in the column space of <math alttext="bold upper A"><mi>𝐀</mi></math> , with
    <math alttext="bold v"><mi>𝐯</mi></math> providing the weights for combining the
    columns. Same goes for the second equation, but <math alttext="bold v"><mi>𝐯</mi></math>
    is in the row space of <math alttext="bold upper A"><mi>𝐀</mi></math> with <math
    alttext="bold u"><mi>𝐮</mi></math> providing the weights.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第一个方程意味着 <math alttext="bold u"><mi>𝐮</mi></math> 在 <math alttext="bold upper
    A"><mi>𝐀</mi></math> 的列空间中，而 <math alttext="bold v"><mi>𝐯</mi></math> 提供了组合列的权重。第二个方程同样如此，但
    <math alttext="bold v"><mi>𝐯</mi></math> 在 <math alttext="bold upper A"><mi>𝐀</mi></math>
    的行空间中，<math alttext="bold u"><mi>𝐮</mi></math> 提供了权重。
- en: But that’s not what I want to focus on in this section; I want to consider what
    happens when you multiply one left singular vector by one right singular vector.
    Because the singular vectors are paired with the same singular value, we need
    to multiply the *i*th left singular vector by the *i*th singular value by the
    *i*th right singular vector.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是我想在本节中集中讨论的内容；我想考虑的是当你将一个左奇异向量乘以一个右奇异向量时会发生什么。因为奇异向量与相同的奇异值配对，我们需要将第*i*个左奇异向量乘以第*i*个奇异值，再乘以第*i*个右奇异向量。
- en: 'Note the orientations in this vector-vector multiplication: column on the left,
    row on the right ([Figure 14-3](#fig_14_3)). That means that the result will be
    an outer product of the same size as the original matrix. Furthermore, that outer
    product is a rank-1 matrix whose norm is determined by the singular value (because
    the singular vectors are unit-length):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个向量-向量乘法中的方向：左边是列，右边是行（见[图 14-3](#fig_14_3)）。这意味着结果将是一个与原始矩阵大小相同的外积矩阵。此外，该外积是一个秩-1矩阵，其范数由奇异值决定（因为奇异向量是单位长度的）：
- en: <math alttext="bold u 1 sigma 1 bold v 1 Superscript upper T Baseline equals
    bold upper A 1" display="block"><mrow><msub><mi>𝐮</mi> <mn>1</mn></msub> <msub><mi>σ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <mo>=</mo>
    <msub><mi>𝐀</mi> <mn>1</mn></msub></mrow></math>![layer](assets/plad_1403.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold u 1 sigma 1 bold v 1 Superscript upper T Baseline equals
    bold upper A 1" display="block"><mrow><msub><mi>𝐮</mi> <mn>1</mn></msub> <msub><mi>σ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <mo>=</mo>
    <msub><mi>𝐀</mi> <mn>1</mn></msub></mrow></math>![layer](assets/plad_1403.png)
- en: Figure 14-3\. Outer product of singular vectors creates a matrix “layer”
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-3\. 奇异向量的外积创建一个矩阵“层”
- en: The subscripted *1* in the equation indicates using the first singular vectors
    and first (largest) singular value. I call the result <math alttext="bold upper
    A 1"><msub><mi>𝐀</mi> <mn>1</mn></msub></math> because it’s not the original matrix
    <math alttext="bold upper A"><mi>𝐀</mi></math> ; instead, it’s a rank-1 matrix
    of the same size as <math alttext="bold upper A"><mi>𝐀</mi></math> . And not just
    any rank-1 matrix—it is the most important “layer” of the matrix. It’s the most
    important because it has the largest singular value (more on this point in a later
    section).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中的下标*1*表示使用第一个奇异向量和第一个（最大的）奇异值。我称之为<math alttext="bold upper A 1"><msub><mi>𝐀</mi>
    <mn>1</mn></msub></math>的结果，因为它不是原始矩阵<math alttext="bold upper A"><mi>𝐀</mi></math>；相反，它是与<math
    alttext="bold upper A"><mi>𝐀</mi></math>相同大小的秩-1矩阵。不仅如此，它是最重要的“层”之一。它之所以如此重要，是因为它具有最大的奇异值（关于这一点的详细内容将在后面的部分介绍）。
- en: 'With this in mind, we can reconstruct the original matrix by summing all SVD
    “layers” associated with <math alttext="sigma greater-than 0"><mrow><mi>σ</mi>
    <mo>></mo> <mn>0</mn></mrow></math> ^([2](ch14.xhtml#idm45733291241712)):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个理解，我们可以通过对所有与<math alttext="sigma greater-than 0"><mrow><mi>σ</mi> <mo>></mo>
    <mn>0</mn></mrow></math>相关的SVD“层”求和来重构原始矩阵 ^([2](ch14.xhtml#idm45733291241712))：
- en: <math alttext="bold upper A equals sigma-summation Underscript i equals 1 Overscript
    r Endscripts bold u Subscript i Baseline sigma Subscript i Baseline bold v Subscript
    i Superscript upper T" display="block"><mrow><mi>𝐀</mi> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>r</mi></munderover> <msub><mi>𝐮</mi>
    <mi>i</mi></msub> <msub><mi>σ</mi> <mi>i</mi></msub> <msubsup><mi>𝐯</mi> <mi>i</mi>
    <mtext>T</mtext></msubsup></mrow></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A equals sigma-summation Underscript i equals 1 Overscript
    r Endscripts bold u Subscript i Baseline sigma Subscript i Baseline bold v Subscript
    i Superscript upper T" display="block"><mrow><mi>𝐀</mi> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>r</mi></munderover> <msub><mi>𝐮</mi>
    <mi>i</mi></msub> <msub><mi>σ</mi> <mi>i</mi></msub> <msubsup><mi>𝐯</mi> <mi>i</mi>
    <mtext>T</mtext></msubsup></mrow></math>
- en: The point of showing this summation is that you don’t necessarily need to use
    all *r* layers; instead, you can reconstruct some other matrix, let’s call it
    <math alttext="bold upper A overTilde"><mover accent="true"><mi>𝐀</mi> <mo>˜</mo></mover></math>
    , which contains the first *k* < *r* layers. This is called a *low-rank approximation*
    of matrix <math alttext="bold upper A"><mi>𝐀</mi></math> —in this case, a rank-*k*
    approximation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 展示这个求和的要点在于，你不一定需要使用所有的*r*层；相反，你可以重构另一个矩阵，让我们称之为<math alttext="bold upper A
    overTilde"><mover accent="true"><mi>𝐀</mi> <mo>˜</mo></mover></math>，其中包含了前*k*
    < *r*层。这被称为矩阵<math alttext="bold upper A"><mi>𝐀</mi></math>的*低秩逼近* — 在这种情况下，是一个秩为*k*的逼近。
- en: Low-rank approximations are used, for example, in data cleaning. The idea is
    that information associated with small singular values makes little contribution
    to the total variance of the dataset, and therefore might reflect noise that can
    be removed. More on this in the next chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在数据清理中使用低秩逼近。其思想是与小奇异值相关联的信息对数据集的总方差贡献较小，因此可能反映出可以去除的噪声。在下一章中会详细介绍。
- en: SVD from EIG
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVD from EIG
- en: OK, at this point in the chapter you know the basics of understanding and interpreting
    the SVD matrices. I’m sure you are wondering what this magical formula is to produce
    the SVD. Perhaps it’s so incredibly complicated that only Gauss could understand
    it? Or maybe it would take so long to explain that it doesn’t fit into one chapter?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，在本章的这一点上，你已经了解了理解和解释SVD矩阵的基础。我相信你想知道的是如何产生SVD的这个神奇公式。也许它非常复杂，只有高斯能理解？或者可能需要很长时间来解释，不适合放在一个章节里？
- en: Wrong!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 错了！
- en: 'In fact, the SVD is really easy (conceptually; performing an SVD by hand is
    another matter). It simply comes from computing the eigendecomposition of the
    matrix times its transpose. The following equations show how to derive the singular
    values and the left singular vectors:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，SVD非常简单（在概念上；手动执行SVD又是另一回事）。它只是来自计算矩阵乘以其转置的特征分解。以下方程显示如何推导奇异值和左奇异向量：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold upper A Superscript
    upper T Baseline equals 2nd Column left-parenthesis bold upper U bold upper Sigma
    bold upper V Superscript upper T Baseline right-parenthesis left-parenthesis bold
    upper U bold upper Sigma bold upper V Superscript upper T Baseline right-parenthesis
    Superscript upper T 2nd Row 1st Column equals 2nd Column bold upper U bold upper
    Sigma bold upper V Superscript upper T Baseline bold upper V bold upper Sigma
    Superscript upper T Baseline bold upper U Superscript upper T 3rd Row 1st Column
    equals 2nd Column bold upper U bold upper Sigma squared bold upper U Superscript
    upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi
    mathvariant="bold">A</mi> <msup><mi mathvariant="bold">A</mi> <mtext>T</mtext></msup>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mrow><mo>(</mo> <mi mathvariant="bold">U</mi>
    <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <msup><mrow><mo>(</mo><mi mathvariant="bold">U</mi><mi mathvariant="bold">Σ</mi><msup><mi
    mathvariant="bold">V</mi> <mtext>T</mtext></msup> <mo>)</mo></mrow> <mtext>T</mtext></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi>
    <mtext>T</mtext></msup> <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">U</mi> <msup><mi mathvariant="bold">Σ</mi> <mn>2</mn></msup>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold upper A Superscript
    upper T Baseline equals 2nd Column left-parenthesis bold upper U bold upper Sigma
    bold upper V Superscript upper T Baseline right-parenthesis left-parenthesis bold
    upper U bold upper Sigma bold upper V Superscript upper T Baseline right-parenthesis
    Superscript upper T 2nd Row 1st Column equals 2nd Column bold upper U bold upper
    Sigma bold upper V Superscript upper T Baseline bold upper V bold upper Sigma
    Superscript upper T Baseline bold upper U Superscript upper T 3rd Row 1st Column
    equals 2nd Column bold upper U bold upper Sigma squared bold upper U Superscript
    upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi
    mathvariant="bold">A</mi> <msup><mi mathvariant="bold">A</mi> <mtext>T</mtext></msup>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mrow><mo>(</mo> <mi mathvariant="bold">U</mi>
    <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <msup><mrow><mo>(</mo><mi mathvariant="bold">U</mi><mi mathvariant="bold">Σ</mi><msup><mi
    mathvariant="bold">V</mi> <mtext>T</mtext></msup> <mo>)</mo></mrow> <mtext>T</mtext></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi>
    <mtext>T</mtext></msup> <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi
    mathvariant="bold">U</mi> <msup><mi mathvariant="bold">Σ</mi> <mn>2</mn></msup>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
- en: In other words, the eigenvectors of <math alttext="bold upper A bold upper A
    Superscript upper T"><mrow><mi>𝐀</mi> <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math>
    are the left-singular vectors of <math alttext="bold upper A"><mi>𝐀</mi></math>
    , and the squared eigenvalues of <math alttext="bold upper A bold upper A Superscript
    upper T"><mrow><mi>𝐀</mi> <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math>
    are the singular values of <math alttext="bold upper A"><mi>𝐀</mi></math> .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，<math alttext="bold upper A bold upper A Superscript upper T"><mrow><mi>𝐀</mi>
    <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math>的特征向量是<math alttext="bold
    upper A"><mi>𝐀</mi></math>的左奇异向量，<math alttext="bold upper A bold upper A Superscript
    upper T"><mrow><mi>𝐀</mi> <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math>的平方特征值是<math
    alttext="bold upper A"><mi>𝐀</mi></math>的奇异值。
- en: 'This insight reveals three features of the SVD: (1) singular values are nonnegative
    because squared numbers cannot be negative, (2) singular values are real-valued
    because symmetric matrices have real-valued eigenvalues, and (3) singular vectors
    are orthogonal because the eigenvectors of a symmetric matrix are orthogonal.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这一洞察揭示了SVD的三个特点：（1）奇异值非负，因为平方数不能为负数；（2）奇异值是实数，因为对称矩阵具有实数特征值；（3）奇异向量正交，因为对称矩阵的特征向量是正交的。
- en: 'The right-singular values come from premultiplying the matrix transpose:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 右奇异值来自矩阵转置的预乘：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A Superscript upper
    T Baseline bold upper A equals 2nd Column left-parenthesis bold upper U bold upper
    Sigma bold upper V Superscript upper T Baseline right-parenthesis Superscript
    upper T Baseline left-parenthesis bold upper U bold upper Sigma bold upper V Superscript
    upper T Baseline right-parenthesis 2nd Row 1st Column equals 2nd Column bold upper
    V bold upper Sigma Superscript upper T Baseline bold upper U Superscript upper
    T Baseline bold upper U bold upper Sigma bold upper V Superscript upper T 3rd
    Row 1st Column equals 2nd Column bold upper V bold upper Sigma squared bold upper
    V Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msup><mi mathvariant="bold">A</mi> <mtext>T</mtext></msup>
    <mi mathvariant="bold">A</mi> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mrow><mo>(</mo><mi
    mathvariant="bold">U</mi><mi mathvariant="bold">Σ</mi><msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mo>)</mo></mrow> <mtext>T</mtext></msup> <mrow><mo>(</mo>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi>
    <mtext>T</mtext></msup> <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi>
    <mn>2</mn></msup> <msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A Superscript upper
    T Baseline bold upper A equals 2nd Column left-parenthesis bold upper U bold upper
    Sigma bold upper V Superscript upper T Baseline right-parenthesis Superscript
    upper T Baseline left-parenthesis bold upper U bold upper Sigma bold upper V Superscript
    upper T Baseline right-parenthesis 2nd Row 1st Column equals 2nd Column bold upper
    V bold upper Sigma Superscript upper T Baseline bold upper U Superscript upper
    T Baseline bold upper U bold upper Sigma bold upper V Superscript upper T 3rd
    Row 1st Column equals 2nd Column bold upper V bold upper Sigma squared bold upper
    V Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msup><mi mathvariant="bold">A</mi> <mtext>T</mtext></msup>
    <mi mathvariant="bold">A</mi> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><msup><mrow><mo>(</mo><mi
    mathvariant="bold">U</mi><mi mathvariant="bold">Σ</mi><msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mo>)</mo></mrow> <mtext>T</mtext></msup> <mrow><mo>(</mo>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi>
    <mtext>T</mtext></msup> <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi>
    <mn>2</mn></msup> <msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
- en: 'In fact, you can rearrange the SVD equation to solve for the right-singular
    vectors without having to compute the eigendecomposition of <math alttext="bold
    upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> :'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，您可以重新排列SVD方程以解出右奇异向量，而无需计算<math alttext="bold upper A Superscript upper
    T Baseline bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math>的特征分解：
- en: <math alttext="bold upper V Superscript upper T Baseline equals bold upper Sigma
    Superscript negative 1 Baseline bold upper U Superscript upper T Baseline bold
    upper A" display="block"><mrow><msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup>
    <mo>=</mo> <msup><mi mathvariant="bold">Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">A</mi></mrow></math>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper V Superscript upper T Baseline equals bold upper Sigma
    Superscript negative 1 Baseline bold upper U Superscript upper T Baseline bold
    upper A" display="block"><mrow><msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup>
    <mo>=</mo> <msup><mi mathvariant="bold">Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">A</mi></mrow></math>
- en: Of course, there is a complementary equation for deriving <math alttext="bold
    upper U"><mi>𝐔</mi></math> if you already know <math alttext="bold upper V"><mi>𝐕</mi></math>
    .
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您已经知道<math alttext="bold upper V"><mi>𝐕</mi></math> ，则可以推导出<math alttext="bold
    upper U"><mi>𝐔</mi></math> 的互补方程。
- en: SVD of <math alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math>
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVD of <math alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math>
- en: 'Briefly, if a matrix can be expressed as <math alttext="bold upper S equals
    bold upper A Superscript upper T Baseline bold upper A"><mrow><mi>𝐒</mi> <mo>=</mo>
    <msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math> , then its left-
    and right-singular vectors are equal. In other words:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果一个矩阵可以表示为<math alttext="bold upper S equals bold upper A Superscript
    upper T Baseline bold upper A"><mrow><mi>𝐒</mi> <mo>=</mo> <msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> ，那么它的左奇异向量和右奇异向量是相等的。换句话说：
- en: <math alttext="bold upper S equals bold upper U bold upper Sigma bold upper
    V Superscript upper T Baseline equals bold upper V bold upper Sigma bold upper
    U Superscript upper T Baseline equals bold upper U bold upper Sigma bold upper
    U Superscript upper T Baseline equals bold upper V bold upper Sigma bold upper
    V Superscript upper T" display="block"><mrow><mi mathvariant="bold">S</mi> <mo>=</mo>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Σ</mi>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup> <mo>=</mo> <mi mathvariant="bold">U</mi>
    <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Σ</mi> <msup><mi
    mathvariant="bold">V</mi> <mtext>T</mtext></msup></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper S equals bold upper U bold upper Sigma bold upper
    V Superscript upper T Baseline equals bold upper V bold upper Sigma bold upper
    U Superscript upper T Baseline equals bold upper U bold upper Sigma bold upper
    U Superscript upper T Baseline equals bold upper V bold upper Sigma bold upper
    V Superscript upper T" display="block"><mrow><mi mathvariant="bold">S</mi> <mo>=</mo>
    <mi mathvariant="bold">U</mi> <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">V</mi>
    <mtext>T</mtext></msup> <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Σ</mi>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup> <mo>=</mo> <mi mathvariant="bold">U</mi>
    <mi mathvariant="bold">Σ</mi> <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Σ</mi> <msup><mi
    mathvariant="bold">V</mi> <mtext>T</mtext></msup></mrow></math>
- en: The proof of this claim comes from writing out the SVD of <math alttext="bold
    upper S"><mi>𝐒</mi></math> and <math alttext="bold upper S Superscript upper T"><msup><mi>𝐒</mi>
    <mtext>T</mtext></msup></math> , then considering the implication of <math alttext="bold
    upper S equals bold upper S Superscript upper T"><mrow><mi>𝐒</mi> <mo>=</mo> <msup><mi>𝐒</mi>
    <mtext>T</mtext></msup></mrow></math> . I’m leaving this one for you to explore
    on your own! And I also encourage you to confirm it in Python using random symmetric
    matrices.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个断言的证明源于将<math alttext="bold upper S"><mi>𝐒</mi></math>和<math alttext="bold
    upper S Superscript upper T"><msup><mi>𝐒</mi> <mtext>T</mtext></msup></math>的SVD写出来，然后考虑<math
    alttext="bold upper S equals bold upper S Superscript upper T"><mrow><mi>𝐒</mi>
    <mo>=</mo> <msup><mi>𝐒</mi> <mtext>T</mtext></msup></mrow></math>的含义。我留给你自己去探索！我还鼓励您在Python中使用随机对称矩阵来确认。
- en: In fact, for a symmetric matrix, SVD is the same thing as eigendecomposition.
    This has implications for principal components analysis, because PCA can be performed
    using eigendecomposition of the data covariance matrix, the SVD of the covariance
    matrix, or the SVD of the data matrix.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于对称矩阵，SVD与特征分解是相同的。这对主成分分析有影响，因为PCA可以使用数据协方差矩阵的特征分解，协方差矩阵的SVD或数据矩阵的SVD来执行。
- en: Converting Singular Values to Variance, Explained
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将奇异值转换为解释的方差
- en: The sum of the singular values is the total amount of “variance” in the matrix.
    What does that mean? If you think of the information in the matrix as being contained
    in a bubble, then the sum of the singular values is like the volume of that bubble.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值的总和是矩阵中的总“方差”量。这是什么意思？如果你将矩阵中的信息视为包含在一个气泡中，那么奇异值的总和就像是那个气泡的体积。
- en: The reason why all the variance is contained in the singular values is that
    the singular vectors are normalized to unit magnitude, which means they provide
    no magnitude information (that is, <math alttext="parallel-to bold upper U bold
    w parallel-to equals parallel-to bold w parallel-to"><mrow><mo>∥</mo> <mi>𝐔</mi>
    <mi>𝐰</mi> <mo>∥</mo> <mo>=</mo> <mo>∥</mo> <mi>𝐰</mi> <mo>∥</mo></mrow></math>
    ).^([3](ch14.xhtml#idm45733291075808)) In other words, the singular vectors point,
    and the singular values say how far.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所有方差都包含在奇异值中的原因是奇异向量被规范化为单位大小，这意味着它们不提供大小信息（即，<math alttext="parallel-to bold
    upper U bold w parallel-to equals parallel-to bold w parallel-to"><mrow><mo>∥</mo>
    <mi>𝐔</mi> <mi>𝐰</mi> <mo>∥</mo> <mo>=</mo> <mo>∥</mo> <mi>𝐰</mi> <mo>∥</mo></mrow></math>
    ）。^([3](ch14.xhtml#idm45733291075808)) 换句话说，奇异向量指向某个方向，而奇异值则表示距离有多远。
- en: The “raw” singular values are in the numerical scale of the matrix. That means
    that if you multiply the data by a scalar, then the singular values will increase.
    And this in turn means that the singular values are difficult to interpret, and
    are basically impossible to compare across different datasets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: “原始”奇异值处于矩阵的数值尺度中。这意味着如果将数据乘以一个标量，那么奇异值将会增加。而这又意味着，奇异值难以解释，并且基本上不可能在不同数据集之间进行比较。
- en: 'For this reason, it is often useful to convert the singular values to percent
    total variance explained. The formula is simple; each singular value *i* is normalized
    as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将奇异值转换为解释的总百分比通常是有用的。公式很简单；每个奇异值 *i* 规范化如下：
- en: <math alttext="sigma Subscript i Baseline overTilde equals StartFraction 100
    sigma Subscript i Baseline Over sigma-summation sigma EndFraction" display="block"><mrow><mover
    accent="true"><msub><mi>σ</mi> <mi>i</mi></msub> <mo>˜</mo></mover> <mo>=</mo>
    <mfrac><mrow><mn>100</mn><msub><mi>σ</mi> <mi>i</mi></msub></mrow> <mrow><mo>∑</mo><mi>σ</mi></mrow></mfrac></mrow></math>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="sigma Subscript i Baseline overTilde equals StartFraction 100
    sigma Subscript i Baseline Over sigma-summation sigma EndFraction" display="block"><mrow><mover
    accent="true"><msub><mi>σ</mi> <mi>i</mi></msub> <mo>˜</mo></mover> <mo>=</mo>
    <mfrac><mrow><mn>100</mn><msub><mi>σ</mi> <mi>i</mi></msub></mrow> <mrow><mo>∑</mo><mi>σ</mi></mrow></mfrac></mrow></math>
- en: This normalization is common in principal components analysis, for example,
    to determine the number of components that account for 99% of the variance. That
    can be interpreted as an indicator of system complexity.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在主成分分析中，这种归一化常用来确定解释 99% 方差的成分数量。这可以解释为系统复杂性的一个指标。
- en: Importantly, this normalization does not affect the relative distances between
    singular values; it merely changes the numerical scale into one that is more readily
    intepretable.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这种归一化不会影响奇异值之间的相对距离；它只是将数值尺度改变为更容易解释的尺度。
- en: Condition Number
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件数
- en: I’ve hinted several times in this book that the condition number of a matrix
    is used to indicate the numerical stability of a matrix. Now that you know about
    singular values, you can better appreciate how to compute and interpret the condition
    number.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中我多次提到，矩阵的条件数用于指示矩阵的数值稳定性。现在你了解了奇异值，可以更好地理解如何计算和解释条件数。
- en: 'The condition number of a matrix is defined as the ratio of the largest to
    the smallest singular value. It’s often given the letter <math alttext="kappa"><mi>κ</mi></math>
    (Greek letter *kappa*):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的条件数定义为最大奇异值与最小奇异值的比值。通常用希腊字母 *κ*（kappa）表示：
- en: <math alttext="kappa equals StartFraction sigma Subscript m a x Baseline Over
    sigma Subscript min Baseline EndFraction" display="block"><mrow><mi>κ</mi> <mo>=</mo>
    <mfrac><msub><mi>σ</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <msub><mi>σ</mi>
    <mo form="prefix" movablelimits="true">min</mo></msub></mfrac></mrow></math>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="kappa equals StartFraction sigma Subscript m a x Baseline Over
    sigma Subscript min Baseline EndFraction" display="block"><mrow><mi>κ</mi> <mo>=</mo>
    <mfrac><msub><mi>σ</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <msub><mi>σ</mi>
    <mo form="prefix" movablelimits="true">min</mo></msub></mfrac></mrow></math>
- en: The condition number is often used in statistics and machine learning to evaluate
    the stability of a matrix when computing its inverse and when using it to solve
    systems of equations (e.g., least squares). Of course, a noninvertible matrix
    has a condition number of NaN because <math alttext="sigma slash 0 equals"><mrow><mi>σ</mi>
    <mo>/</mo> <mn>0</mn> <mo>=</mo></mrow></math> ‽.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学和机器学习中，条件数常用于评估矩阵在计算其逆矩阵和使用其解决方程组（如最小二乘法）时的稳定性。当然，非可逆矩阵的条件数为 NaN，因为 <math
    alttext="sigma slash 0 equals"><mrow><mi>σ</mi> <mo>/</mo> <mn>0</mn> <mo>=</mo></mrow></math>
    ‽。
- en: But a numerically full-rank matrix with a large condition number can still be
    unstable. Though theoretically invertible, in practice the matrix inverse may
    be unreliable. Such matrices are called *ill-conditioned*. You might have seen
    that term in warning messages in Python, sometimes accompanied by phrases like
    “result is not guaranteed to be accurate.”
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，数值上满秩且条件数较大的矩阵仍可能不稳定。虽然在理论上可逆，但实际上其逆矩阵可能不可靠。这样的矩阵被称为*病态矩阵*。你可能在Python的警告消息中见过这个术语，有时伴随着“结果可能不准确”的短语。
- en: What’s the problem with an ill-conditioned matrix? As the condition number increases,
    the matrix tends toward being singular. Therefore, an ill-conditioned matrix is
    “almost singular” and its inverse becomes untrustworthy due to increased risk
    of numerical errors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于条件数较大的病态矩阵有什么问题？随着条件数的增加，矩阵趋向于奇异。因此，病态矩阵几乎是“几乎奇异的”，其逆由于数值误差风险增加而不可信。
- en: There are a few ways to think about the impact of an ill-conditioned matrix.
    One is as the decrease in the precision of a solution due to round-off errors.
    For example, a condition number on the order of 10⁵ means that the solution (e.g.,
    the matrix inverse or least squares problem) loses five significant digits (this
    would mean, for example, going from a precision of 10^(−16) to 10^(−11)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以思考病态矩阵的影响。一种是由于舍入误差导致解的精度降低。例如，条件数约为10⁵意味着解（例如矩阵逆或最小二乘问题）失去了五个有效数字（例如，从精度10^(−16)降至10^(−11)）。
- en: A second interpretation, related to the previous, is as an amplification factor
    for noise. If you have a matrix with a condition number on the order of 10⁴, then
    noise could impact the solution to a least squares problem by 10⁴. That might
    sound like a lot, but it could be an insigificant amplification if your data has
    a precision of 10^(−16).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种解释与前述有关，是噪声放大因子。如果矩阵的条件数约为10⁴，那么噪声可能会使最小二乘问题的解受到10⁴的影响。这听起来可能很多，但如果您的数据精度为10^(−16)，这可能是一个微不足道的放大效应。
- en: Thirdly, the condition number indicates the sensitivity of a solution to perturbations
    in the data matrix. A well-conditioned matrix can be perturbed (more noise added)
    with minimal change in the solution. In contrast, adding a small amount of noise
    to an ill-conditioned matrix can lead to wildly different solutions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，条件数表示解对数据矩阵扰动的敏感性。良好条件的矩阵可以在添加噪声时保持解的最小变化。相比之下，对病态矩阵添加少量噪声可能会导致完全不同的解。
- en: What is the threshold for a matrix to be ill-conditioned? There is none. There
    is no magic number that separates a well-conditioned from an ill-conditioned matrix.
    Different algorithms will apply different thresholds that depend on the numerical
    values in the matrix.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵何时被认为是病态？没有确定的阈值。没有一个魔法数字可以将良好条件的矩阵与病态矩阵分开。不同的算法将适用不同的阈值，这些阈值取决于矩阵中的数值。
- en: 'This much is clear: take warning messages about ill-conditioned matrices seriously.
    They usually indicate that something is wrong and the results should not be trusted.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显：要认真对待关于病态矩阵的警告信息。它们通常表明某些地方出了问题，结果不可信。
- en: SVD and the MP Pseudoinverse
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVD和MP伪逆
- en: 'The SVD of a matrix inverse is quite elegant. Assuming the matrix is square
    and invertible, we get:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的奇异值分解逆矩阵非常优雅。假设矩阵是方阵且可逆，我们有：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A Superscript negative
    1 2nd Column equals left-parenthesis bold upper U bold upper Sigma bold upper
    V Superscript upper T Baseline right-parenthesis Superscript negative 1 Baseline
    2nd Row 1st Column Blank 2nd Column equals bold upper V bold upper Sigma Superscript
    negative 1 Baseline bold upper U Superscript negative 1 Baseline 3rd Row 1st Column
    Blank 2nd Column equals bold upper V bold upper Sigma Superscript negative 1 Baseline
    bold upper U Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msup><mi mathvariant="bold">A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><mi mathvariant="bold">U</mi><mi
    mathvariant="bold">Σ</mi><msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi mathvariant="bold">V</mi> <msup><mi
    mathvariant="bold">Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">U</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A Superscript negative
    1 2nd Column equals left-parenthesis bold upper U bold upper Sigma bold upper
    V Superscript upper T Baseline right-parenthesis Superscript negative 1 Baseline
    2nd Row 1st Column Blank 2nd Column equals bold upper V bold upper Sigma Superscript
    negative 1 Baseline bold upper U Superscript negative 1 Baseline 3rd Row 1st Column
    Blank 2nd Column equals bold upper V bold upper Sigma Superscript negative 1 Baseline
    bold upper U Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msup><mi mathvariant="bold">A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><mi mathvariant="bold">U</mi><mi
    mathvariant="bold">Σ</mi><msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi mathvariant="bold">V</mi> <msup><mi
    mathvariant="bold">Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi mathvariant="bold">U</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
- en: In other words, we only need to invert <math alttext="bold upper Sigma"><mi
    mathvariant="bold">Σ</mi></math>, because <math alttext="bold upper U Superscript
    negative 1 Baseline equals bold upper U Superscript upper T"><mrow><msup><mi>𝐔</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>=</mo> <msup><mi>𝐔</mi> <mtext>T</mtext></msup></mrow></math>
    . Furthermore, because <math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>
    is a diagonal matrix, its inverse is obtained simply by inverting each diagonal
    element. On the other hand, this method is still subject to numerical instabilities,
    because tiny singular values that might reflect precision errors (e.g., 10^(−15))
    become galatically large when inverted.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们只需求逆<math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>，因为<math
    alttext="bold upper U Superscript negative 1 Baseline equals bold upper U Superscript
    upper T"><mrow><msup><mi>𝐔</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>=</mo>
    <msup><mi>𝐔</mi> <mtext>T</mtext></msup></mrow></math>。此外，因为<math alttext="bold
    upper Sigma"><mi mathvariant="bold">Σ</mi></math>是对角矩阵，其逆可以通过倒转每个对角元素来获得。然而，这种方法仍然受到数值不稳定性的影响，因为微小的奇异值可能反映出精度错误（例如10^(−15)），在求逆时会变得极大。
- en: Now for the algorithm to compute the MP pseudoinverse. You’ve waited many chapters
    for this; I appreciate your patience.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下计算MP伪逆的算法。您已经等待了许多章节；我感谢您的耐心。
- en: The MP pseudoinverse is computed almost exactly as the full inverse shown in
    the previous example; the only modification is to invert the *nonzero* diagonal
    elements in <math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>
    instead of trying to to invert all diagonal elements. (In practice, “nonzero”
    is implemented as above a threshold to account for precision errors.)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: MP伪逆几乎与前面示例中显示的完全逆矩阵计算相同；唯一的修改是反转<math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>中的*非零*对角元素，而不是尝试反转所有对角元素。（在实践中，“非零”被实现为高于某个阈值以应对精度误差。）
- en: And that’s it! That’s how the pseudoinverse is computed. You can see that it’s
    very simple and intuitive, but requires a considerable amount of background knowledge
    about linear algebra to understand.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！这就是伪逆的计算方法。您可以看到它非常简单和直观，但需要相当多的线性代数背景知识才能理解。
- en: 'Even better: because the SVD works on matrices of any size, the MP pseudoinverse
    can be applied to nonsquare matrices. In fact, the MP pseudoinverse of a tall
    matrix equals its left-inverse, and the MP pseudoinverse of a wide matrix equals
    its right-inverse. (Quick reminder that the pseudoinverse is indicated as <math
    alttext="bold upper A Superscript plus"><msup><mi>𝐀</mi> <mo>+</mo></msup></math>
    , <math alttext="bold upper A Superscript asterisk"><msup><mi>𝐀</mi> <mo>*</mo></msup></math>
    , or <math alttext="bold upper A Superscript dagger"><msup><mi>𝐀</mi> <mo>†</mo></msup></math>
    .)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是：因为SVD适用于任何大小的矩阵，MP伪逆可以应用于非方阵。实际上，高矩阵的MP伪逆等于其左逆，而宽矩阵的MP伪逆等于其右逆。（快速提醒，伪逆表示为<math
    alttext="bold upper A Superscript plus"><msup><mi>𝐀</mi> <mo>+</mo></msup></math>，<math
    alttext="bold upper A Superscript asterisk"><msup><mi>𝐀</mi> <mo>*</mo></msup></math>或<math
    alttext="bold upper A Superscript dagger"><msup><mi>𝐀</mi> <mo>†</mo></msup></math>。）
- en: You’ll gain more experience with the pseudoinverse by implementing it yourself
    in the exercises.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在练习中自己实现，您将更多地了解伪逆。
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'I hope you agree that after putting in the effort to learn about eigendecomposition,
    a little bit of extra effort goes a long way toward understanding the SVD. The
    SVD is arguably the most important decomposition in linear algebra, because it
    reveals rich and detailed information about the matrix. Here are the key points:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您同意，在努力学习特征分解后，再多付出一点努力就能更好地理解SVD。SVD可以说是线性代数中最重要的分解方法，因为它揭示了关于矩阵的丰富和详细信息。以下是其关键点：
- en: SVD decomposes a matrix (of any size and rank) into the product of three matrices,
    termed *left singular vectors* <math alttext="bold upper U"><mi>𝐔</mi></math>
    , *singular values* <math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math>,
    and *right singular vectors* <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup></math> .
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD将一个（任意大小和秩）矩阵分解为三个矩阵的乘积，称为*左奇异向量* <math alttext="bold upper U"><mi>𝐔</mi></math>
    ，*奇异值* <math alttext="bold upper Sigma"><mi mathvariant="bold">Σ</mi></math> 和*右奇异向量*
    <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi> <mtext>T</mtext></msup></math>
    。
- en: The first *r* (where *r* is the matrix rank) left singular vectors provide an
    orthonormal basis set for the column space of the matrix, while the later singular
    vectors provide an orthonormal basis set for the left-null space.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前*r*（其中*r*是矩阵秩）个左奇异向量为矩阵的列空间提供一组标准正交基，而后续的奇异向量为左零空间提供一组标准正交基。
- en: 'It’s a similar story for the right singular vectors: the first *r* vectors
    provide an orthonormal basis set for the row space, while the later vectors provide
    an orthonormal basis set for the null space. Be mindful that the right singular
    vectors are actually the *rows* of <math alttext="bold upper V"><mi>𝐕</mi></math>
    , which are the *columns* of <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup></math> .'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于右奇异向量也是类似的情况：前*r*个向量为行空间提供一组标准正交基，而后续的向量为零空间提供一组标准正交基。请注意，右奇异向量实际上是<math alttext="bold
    upper V"><mi>𝐕</mi></math>的*行*，这些行是<math alttext="bold upper V Superscript upper
    T"><msup><mi>𝐕</mi> <mtext>T</mtext></msup></math>的*列*。
- en: The number of nonzero singular values equals the rank of the matrix. In practice,
    it can be difficult to distinguish between very small nonzero singular values
    versus precision errors on zero-valued singular values. Programs like Python will
    use a tolerance threshold to make this distinction.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非零奇异值的数量等于矩阵的秩。在实践中，很难区分非常小的非零奇异值与零值奇异值的精度误差。像Python这样的程序会使用容差阈值来区分它们。
- en: The outer product of the *k*th left singular vector and the *k*th right singular
    vector, scalar multiplied by the *k*th singular value, produces a rank-1 matrix
    that can be interpreted as a “layer” of the matrix. Reconstructing a matrix based
    on layers has many applications, including denoising and data compression.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 *k* 个左奇异向量和第 *k* 个右奇异向量的外积，乘以第 *k* 个奇异值，产生一个秩-1矩阵，可以解释为矩阵的“层”。基于层重建矩阵具有许多应用，包括去噪和数据压缩。
- en: Conceptually, the SVD can be obtained from the eigendecomposition of <math alttext="bold
    upper A bold upper A Superscript upper T"><mrow><mi>𝐀</mi> <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math>
    .
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从概念上讲，SVD可以从 <math alttext="bold upper A bold upper A Superscript upper T"><mrow><mi>𝐀</mi>
    <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math> 的特征分解中获得。
- en: The super-duper important Moore-Penrose pseudoinverse is computed as <math alttext="bold
    upper V bold upper Sigma Superscript plus Baseline bold upper U Superscript upper
    T"><mrow><mi mathvariant="bold">V</mi><msup><mi mathvariant="bold">Σ</mi> <mo>+</mo></msup>
    <msup><mi mathvariant="bold">U</mi> <mtext>T</mtext></msup></mrow></math> , where
    <math alttext="bold upper Sigma Superscript plus"><msup><mi mathvariant="bold">Σ</mi>
    <mo>+</mo></msup></math> is obtained by inverting the nonzero singular values
    on the diagonal.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超级重要的摩尔-彭罗斯伪逆计算为 <math alttext="bold upper V bold upper Sigma Superscript plus
    Baseline bold upper U Superscript upper T"><mrow><mi mathvariant="bold">V</mi><msup><mi
    mathvariant="bold">Σ</mi> <mo>+</mo></msup> <msup><mi mathvariant="bold">U</mi>
    <mtext>T</mtext></msup></mrow></math> ，其中 <math alttext="bold upper Sigma Superscript
    plus"><msup><mi mathvariant="bold">Σ</mi> <mo>+</mo></msup></math> 通过反转对角线上的非零奇异值获得。
- en: Code Exercises
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码练习。
- en: Exercise 14-1\.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 14-1。
- en: You learned that for a symmetric matrix, the singular values and the eigenvalues
    are the same. How about the singular vectors and eigenvectors? Use Python to answer
    this question using a random <math alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo>
    <mn>5</mn></mrow></math> <math alttext="bold upper A Superscript upper T Baseline
    bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math>
    matrix. Next, try it again using the additive method for creating a symmetric
    matrix ( <math alttext="bold upper A Superscript upper T Baseline plus bold upper
    A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mo>+</mo> <mi>𝐀</mi></mrow></math>
    ). Pay attention to the signs of the eigenvalues of <math alttext="bold upper
    A Superscript upper T Baseline plus bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mo>+</mo> <mi>𝐀</mi></mrow></math> .
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你学到对于对称矩阵，奇异值和特征值是相同的。那么奇异向量和特征向量呢？使用一个随机 <math alttext="5 times 5"><mrow><mn>5</mn>
    <mo>×</mo> <mn>5</mn></mrow></math> <math alttext="bold upper A Superscript upper
    T Baseline bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math>
    矩阵来回答这个问题。接下来，使用加法方法再次尝试创建对称矩阵（ <math alttext="bold upper A Superscript upper
    T Baseline plus bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mo>+</mo>
    <mi>𝐀</mi></mrow></math> ）。注意 <math alttext="bold upper A Superscript upper T
    Baseline plus bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mo>+</mo>
    <mi>𝐀</mi></mrow></math> 的特征值的符号。
- en: Exercise 14-2\.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 14-2。
- en: Python can optionally return the “economy” SVD, which means that the singular
    vectors matrices are truncated at the smaller of *M* or *N*. Consult the docstring
    to figure out how to do this. Confirm with tall and wide matrices. Note that you
    would typically want to return the full matrices; economy SVD is mainly used for
    really large matrices and/or really limited computational power.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Python 可以选择返回“经济”SVD，这意味着奇异向量矩阵在较小的*M*或*N*处被截断。请查阅文档字符串以了解如何实现此功能。与高和宽矩阵确认。注意，通常应返回完整矩阵；经济型SVD主要用于非常大的矩阵和/或计算能力非常有限的情况。
- en: Exercise 14-3\.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 14-3。
- en: One of the important features of an orthogonal matrix (such as the left and
    right singular vectors matrices) is that they rotate, but do not scale, a vector.
    This means that the magnitude of a vector is preserved after multiplication by
    an orthogonal matrix. Prove that <math alttext="parallel-to bold upper U bold
    w parallel-to equals parallel-to bold w parallel-to"><mrow><mo>∥</mo> <mi>𝐔</mi>
    <mi>𝐰</mi> <mo>∥</mo> <mo>=</mo> <mo>∥</mo> <mi>𝐰</mi> <mo>∥</mo></mrow></math>
    . Then demonstrate this empirically in Python by using a singular vectors matrix
    from the SVD of a random matrix and a random vector.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正交矩阵（如左和右奇异向量矩阵）的一个重要特性是它们可以旋转向量但不改变其大小。这意味着经过正交矩阵乘法后，向量的大小保持不变。通过在Python中使用来自随机矩阵SVD的奇异向量矩阵和随机向量来证明
    <math alttext="parallel-to bold upper U bold w parallel-to equals parallel-to
    bold w parallel-to"><mrow><mo>∥</mo> <mi>𝐔</mi> <mi>𝐰</mi> <mo>∥</mo> <mo>=</mo>
    <mo>∥</mo> <mi>𝐰</mi> <mo>∥</mo></mrow></math>。然后通过实证方法演示这一点。
- en: Exercise 14-4\.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 14-4。
- en: Create a random tall matrix with a specified condition number. Do this by creating
    two random square matrices to be <math alttext="bold upper U"><mi>𝐔</mi></math>
    and <math alttext="bold upper V"><mi>𝐕</mi></math> , and a rectangular <math alttext="bold
    upper Sigma"><mi mathvariant="bold">Σ</mi></math>. Confirm that the empirical
    condition number of <math alttext="bold upper U bold upper Sigma bold upper V
    Superscript upper T"><mrow><mi mathvariant="bold">U</mi><mi mathvariant="bold">Σ</mi><msup><mi
    mathvariant="bold">V</mi> <mtext>T</mtext></msup></mrow></math> is the same as
    the number you specified. Visualize your results in a figure like [Figure 14-4](#fig_14_4).
    (I used a condition number of 42.^([4](ch14.xhtml#idm45733290915728)))
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个具有指定条件数的随机高瘦矩阵。通过创建两个随机方阵<math alttext="bold upper U"><mi>𝐔</mi></math>和<math
    alttext="bold upper V"><mi>𝐕</mi></math>，以及一个矩形的<math alttext="bold upper Sigma"><mi
    mathvariant="bold">Σ</mi></math>来实现这一点。确认<math alttext="bold upper U bold upper
    Sigma bold upper V Superscript upper T"><mrow><mi mathvariant="bold">U</mi><mi
    mathvariant="bold">Σ</mi><msup><mi mathvariant="bold">V</mi> <mtext>T</mtext></msup></mrow></math>的经验条件数与您指定的数字相同。像[图
    14-4](#fig_14_4)一样在图中可视化您的结果。（我使用了一个条件数为 42。^([4](ch14.xhtml#idm45733290915728))）
- en: '![exercise 14-4](assets/plad_1404.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![习题 14-4](assets/plad_1404.png)'
- en: Figure 14-4\. Results of Exercise 14-3
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-4\. 习题 14-3 的结果
- en: Exercise 14-5\.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 习题 14-5\.
- en: 'Your goal here is simple: write code to reproduce [Figure 14-5](#fig_14_5).
    What does this figure show? Panel A shows a <math alttext="30 times 40"><mrow><mn>30</mn>
    <mo>×</mo> <mn>40</mn></mrow></math> random matrix that I created by smoothing
    random numbers (implemented as the 2D convolution between a 2D Gaussian and random
    numbers; if you are not familiar with image processing and filtering, then please
    feel free to copy the code to create this matrix from my code solution). The rest
    of panel A shows the SVD matrices. It’s interesting to note that the earlier singular
    vectors (associated with the larger singular values) are smoother while the later
    ones are more rugged; this comes from the spatial filtering.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里的目标很简单：编写代码以重现[图 14-5](#fig_14_5)。这个图显示了什么？面板 A 展示了一个<math alttext="30 times
    40"><mrow><mn>30</mn> <mo>×</mo> <mn>40</mn></mrow></math>的随机矩阵，我通过对随机数进行平滑（实现为
    2D 高斯和随机数之间的 2D 卷积）创建了它；如果你对图像处理和滤波不熟悉，请随意从我的代码解决方案中复制创建这个矩阵的代码。面板 A 的其余部分展示了
    SVD 矩阵。有趣的是，早期的奇异向量（与较大的奇异值相关联）更加平滑，而后来的则更加粗糙；这是来自空间滤波的效果。
- en: Panel B shows a “scree plot,” which is the singular values normalized to percent
    variance explained. Notice that the first few components account for most of the
    variance in the image, while the later components each account for relatively
    little variance. Confirm that the sum over all normalized singular values is 100\.
    Panel C shows the first four “layers”—rank-1 matrices defined as <math alttext="bold
    u Subscript i Baseline sigma Subscript i Baseline bold v Subscript i Superscript
    upper T"><mrow><msub><mi>𝐮</mi> <mi>i</mi></msub> <msub><mi>σ</mi> <mi>i</mi></msub>
    <msubsup><mi>𝐯</mi> <mi>i</mi> <mtext>T</mtext></msubsup></mrow></math> —on the
    top row and the cumulative sum of those layers on the bottom row. You can see
    that each layer adds more information to the matrix; the lower-right image (titled
    “L 0:3”) is a rank-4 matrix and yet appears visually very similar to the original
    rank-30 matrix in panel A.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 面板 B 展示了一个“scree plot”，即归一化为百分比方差解释的奇异值图。请注意，前几个分量占了图像中大部分的方差，而后续的分量每个都只解释了相对较少的方差。确认所有归一化奇异值的总和为
    100\. 面板 C 展示了前四个“层次”——定义为<math alttext="bold u Subscript i Baseline sigma Subscript
    i Baseline bold v Subscript i Superscript upper T"><mrow><msub><mi>𝐮</mi> <mi>i</mi></msub>
    <msub><mi>σ</mi> <mi>i</mi></msub> <msubsup><mi>𝐯</mi> <mi>i</mi> <mtext>T</mtext></msubsup></mrow></math>——在顶部行和这些层次的累积和在底部行。你可以看到每个层次都向矩阵添加了更多信息；右下角的图像（标题为“L
    0:3”）是一个秩为 4 的矩阵，但在视觉上与面板 A 中原始的秩 30 矩阵非常相似。
- en: '![exercise 14-5](assets/plad_1405.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![习题 14-5](assets/plad_1405.png)'
- en: Figure 14-5\. Results of Exercise 14-5
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-5\. 习题 14-5 的结果
- en: Exercise 14-6\.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 习题 14-6\.
- en: Implement the MP pseduoinverse based on the description in this chapter. You’ll
    need to define a tolerance to ignore tiny-but-nonzero singular values. Please
    don’t look up NumPy’s implementation—and don’t check back to earlier code in this
    chapter—but instead use your knowledge of linear algebra to come up with your
    own tolerance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本章描述实现 MP 伪逆。你需要定义一个容差来忽略微小但非零的奇异值。请不要查阅 NumPy 的实现，也不要回顾本章的早期代码，而是利用你对线性代数的知识自行设计容差。
- en: Test your code on a <math alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math>
    rank-3 matrix. Compare your result against the output of NumPy’s `pinv` function.
    Finally, inspect the source code for `np.linalg.pinv` to make sure you understand
    the implementation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 <math alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math>
    秩为3的矩阵上测试你的代码。将结果与NumPy的 `pinv` 函数的输出进行比较。最后，检查 `np.linalg.pinv` 的源代码以确保你理解其实现。
- en: Exercise 14-7\.
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 14-7\.
- en: Demonstrate that the MP pseudoinverse equals the left-inverse for a full column-rank
    matrix by computing the explicit left-inverse of a tall full matrix ( <math alttext="left-parenthesis
    bold upper A Superscript upper T Baseline bold upper A right-parenthesis Superscript
    negative 1 Baseline bold upper A Superscript upper T"><mrow><msup><mrow><mo>(</mo><msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math> ) and the pseudoinverse
    of <math alttext="bold upper A"><mi>𝐀</mi></math> . Repeat for the right inverse
    with a wide full row-rank matrix.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算高列满秩矩阵的显式左逆证明MP伪逆等于左逆，（ <math alttext="left-parenthesis bold upper A Superscript
    upper T Baseline bold upper A right-parenthesis Superscript negative 1 Baseline
    bold upper A Superscript upper T"><mrow><msup><mrow><mo>(</mo><msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>𝐀</mi> <mtext>T</mtext></msup></mrow></math> )，以及 <math alttext="bold
    upper A"><mi>𝐀</mi></math> 的伪逆。对于宽行满秩矩阵，重复右逆的情况。
- en: Exercise 14-8\.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 14-8\.
- en: 'Consider the eigenvalue equation <math alttext="bold upper A bold v equals
    lamda bold v"><mrow><mi>𝐀</mi> <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></math>
    . Now that you know about the pseudoinverse, you can play around with that equation
    a bit. In particular, use the <math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo>
    <mn>2</mn></mrow></math> matrix used at the outset of [Chapter 13](ch13.xhtml#Chapter_13)
    to compute <math alttext="bold v Superscript plus"><msup><mi>𝐯</mi> <mo>+</mo></msup></math>
    and confirm that <math alttext="bold v bold v Superscript plus Baseline equals
    1"><mrow><mi>𝐯</mi> <msup><mi>𝐯</mi> <mo>+</mo></msup> <mo>=</mo> <mn>1</mn></mrow></math>
    . Next, confirm the following identities:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑特征值方程 <math alttext="bold upper A bold v equals lamda bold v"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></math> 。现在你已经了解了伪逆，可以稍微玩弄一下这个方程。特别是，使用在[第13章](ch13.xhtml#Chapter_13)开始时使用的
    <math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn></mrow></math>
    矩阵计算 <math alttext="bold v Superscript plus"><msup><mi>𝐯</mi> <mo>+</mo></msup></math>
    并确认 <math alttext="bold v bold v Superscript plus Baseline equals 1"><mrow><mi>𝐯</mi>
    <msup><mi>𝐯</mi> <mo>+</mo></msup> <mo>=</mo> <mn>1</mn></mrow></math> 。接下来，确认以下恒等式：
- en: <math alttext="StartLayout 1st Row 1st Column bold v Superscript plus Baseline
    bold upper A bold v 2nd Column equals lamda bold v Superscript plus Baseline bold
    v 2nd Row 1st Column bold upper A bold v bold v Superscript plus 2nd Column equals
    lamda bold v bold v Superscript plus EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msup><mi>𝐯</mi> <mo>+</mo></msup> <mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <msup><mi>𝐯</mi> <mo>+</mo></msup>
    <mi>𝐯</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi>
    <msup><mi>𝐯</mi> <mo>+</mo></msup></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>λ</mi> <mi>𝐯</mi> <msup><mi>𝐯</mi> <mo>+</mo></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold v Superscript plus Baseline
    bold upper A bold v 2nd Column equals lamda bold v Superscript plus Baseline bold
    v 2nd Row 1st Column bold upper A bold v bold v Superscript plus 2nd Column equals
    lamda bold v bold v Superscript plus EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msup><mi>𝐯</mi> <mo>+</mo></msup> <mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <msup><mi>𝐯</mi> <mo>+</mo></msup>
    <mi>𝐯</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi>
    <msup><mi>𝐯</mi> <mo>+</mo></msup></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>λ</mi> <mi>𝐯</mi> <msup><mi>𝐯</mi> <mo>+</mo></msup></mrow></mtd></mtr></mtable></math>
- en: ^([1](ch14.xhtml#idm45733291549488-marker)) The SVD is not the same as eigendecomposition
    for all square matrices; more on this later.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch14.xhtml#idm45733291549488-marker)) 对于所有方阵，奇异值分解与特征分解并不相同；稍后详细讨论。
- en: ^([2](ch14.xhtml#idm45733291241712-marker)) There is no point in summing the
    zero-valued singular values, because that’s just adding zeros matrices.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch14.xhtml#idm45733291241712-marker)) 没有意义对零值奇异值求和，因为那只是在加零矩阵。
- en: ^([3](ch14.xhtml#idm45733291075808-marker)) The proof of this statement is in
    [Exercise 14-3](#exercise_14_3).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch14.xhtml#idm45733291075808-marker)) 这个陈述的证明在[练习 14-3](#exercise_14_3)中。
- en: ^([4](ch14.xhtml#idm45733290915728-marker)) Yes, that’s another reference to
    *Hitchhiker’s Guide to the Galaxy*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch14.xhtml#idm45733290915728-marker)) 这是对《银河系漫游指南》的另一个引用。
