- en: 16 Training neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16 训练神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Classifying images of handwritten digits as vector data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将手写数字图像分类为向量数据
- en: Designing a type of neural network called a multilayer perceptron
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一种称为多层感知器的神经网络类型
- en: Evaluating a neural network as a vector transformation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经网络作为向量变换进行评估
- en: Fitting a neural network to data with a cost function and gradient descent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用成本函数和梯度下降法拟合神经网络
- en: Calculating partial derivatives for neural networks in backpropagation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在反向传播中计算神经网络的偏导数
- en: 'In the final chapter of this book, we combine almost everything you’ve learned
    so far to introduce one of the most famous machine learning tools used today:
    artificial neural networks. *Artificial neural networks*, or neural networks for
    short, are mathematical functions whose structure is loosely based on the structure
    of the human brain. These are called artificial to distinguish from the “organic”
    neural networks that exist in the brain. This might sound like a lofty and complex
    goal, but it’s all based on a simple metaphor for how the brain works.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章，我们将你迄今为止学到的几乎所有内容结合起来，介绍今天最著名的机器学习工具之一：人工神经网络。*人工神经网络*，简称神经网络，是一种数学函数，其结构大致基于人脑的结构。这些被称为“人工”的，是为了与大脑中存在的“有机”神经网络相区别。这听起来可能是一个高远且复杂的目标，但它全部基于对大脑工作原理的一个简单隐喻。
- en: Before explaining the metaphor, I’ll preface this discussion by reminding you
    that I’m not a neurologist. The rough idea is that the brain is a big clump of
    interconnected cells called *neurons* and, when you think certain thoughts, what’s
    actually happening is electrical activity at specific neurons. You can see this
    electrical activity in the right kind of brain scan where various parts of the
    brain light up (figure 16.1).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释这个隐喻之前，我想先提醒你，我不是神经学家。粗略的想法是，大脑是一大团相互连接的细胞，称为*神经元*，当你思考某些想法时，实际上发生的是特定神经元的电活动。你可以在适当的脑扫描中看到这种电活动，大脑的各个部分会亮起（图16.1）。
- en: '![](../Images/CH16_F01_Orland.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F01_Orland.png)'
- en: Figure 16.1 Different kinds of brain activity cause different neurons to electrically
    activate, showing bright areas in a brain scan.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 不同类型的脑活动导致不同的神经元电激活，在脑扫描中显示出明亮区域。
- en: As opposed to the billions of neurons in the human brain, the neural networks
    we build in Python have only a few dozen neurons, and the degree to which a specific
    neuron is turned on is represented by a single number called its *activation*.
    When a neuron activates in the brain or in our artificial neural network, it can
    cause adjacent, connected neurons to turn on as well. This allows one idea to
    lead to another, which we can loosely see as creative thinking.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与人脑中的数十亿个神经元相比，我们在Python中构建的神经网络只有几十个神经元，而特定神经元被激活的程度由一个称为其*激活*的单个数字表示。当一个神经元在大脑或我们的人工神经网络中激活时，它可以导致相邻的、连接的神经元也被激活。这允许一个想法导致另一个想法，我们可以将其松散地视为创造性思维。
- en: Mathematically, the activation of a neuron in our neural network is a function
    of the numerical activation values of neurons it is connected to. If a neuron
    connects to four others with the activation values *a*[1] , *a*[2] , *a*[3] ,
    and *a*[4] , then its activation will be some mathematical function applied to
    those four values, say *f*(*a*[1] , *a*[2] , *a*[3] , *a*[4] ).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，我们神经网络中神经元的激活是一个函数，该函数取决于与之相连的神经元的数值激活值。如果一个神经元连接到四个其他神经元，其激活值分别为 *a*[1]
    ， *a*[2] ， *a*[3] 和 *a*[4] ，那么它的激活将是一个数学函数应用于这四个值的结果，比如说 *f*(*a*[1] ， *a*[2] ，
    *a*[3] ， *a*[4] )。
- en: Figure 16.2 shows a schematic diagram with all of the neurons drawn as circles.
    I’ve shaded the neurons differently to indicate that they have different levels
    of activation, kind of like the brighter or darker areas of a brain scan.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2显示了一个示意图，其中所有神经元都被绘制成圆形。我以不同的阴影来表示它们具有不同的激活水平，有点像脑扫描中较亮或较暗的区域。
- en: '![](../Images/CH16_F02_Orland.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F02_Orland.png)'
- en: Figure 16.2 Picturing neuron activation as a mathematical function, where *a*[1]
    , *a*[2] , *a*[3] , and *a*[4] are the activation values applied to the function
    f.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2 将神经元激活描绘为数学函数，其中 *a*[1] ， *a*[2] ， *a*[3] 和 *a*[4] 是应用于函数 f 的激活值。
- en: If each of *a*[1] , *a*[2] , *a*[3] , and *a*[4] depend on the activation of
    other neurons, the value of *a* could depend on even more numbers. With more neurons
    and more connections, you can build an arbitrarily complicated mathematical function,
    and the goal is to model arbitrarily complicated ideas.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*a*[1]、*a*[2]、*a*[3]和*a*[4]的值依赖于其他神经元的激活，那么*a*的值可能依赖于更多的数字。随着神经元和连接的增加，您可以构建一个任意复杂的数学函数，目标是模拟任意复杂的思想。
- en: The explanation I’ve just given you is a somewhat philosophical introduction
    to neural networks, and it’s definitely not enough for you to start coding. In
    this chapter, I show you, in detail, how to run with these ideas and build your
    own neural network. As in the last chapter, the problem we’ll solve with neural
    networks is *classification*. There are many steps in building a neural network
    and training it to perform well on classification, so before we dive in, I’ll
    lay out the plan.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚刚向您解释的内容是对神经网络的一种哲学性的介绍，但这绝对不足以让您开始编码。在本章中，我将详细向您展示如何运用这些想法并构建您自己的神经网络。正如上一章一样，我们将使用神经网络解决的问题是**分类**。构建神经网络并训练其在分类任务上表现良好有许多步骤，因此在深入之前，我会先制定计划。
- en: 16.1 Classifying data with neural networks
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 使用神经网络进行数据分类
- en: 'In this section, I focus on a classic application of neural networks: classifying
    images. Specifically, we’ll use low resolution images of handwritten digits (numbers
    from 0 to 9), and we want our neural network to identify which digit is shown
    in a given image. Figure 16.3 shows some example images for these digits.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我专注于神经网络的一个经典应用：图像分类。具体来说，我们将使用手写数字的低分辨率图像（从0到9的数字），我们希望我们的神经网络能够识别给定图像中显示的是哪个数字。图16.3展示了这些数字的一些示例图像。
- en: '![](../Images/CH16_F03_Orland.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F03_Orland.png)'
- en: Figure 16.3 Low resolution images of some handwritten digits
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3 一些手写数字的低分辨率图像
- en: '![](../Images/CH16_F04_Orland.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F04_Orland.png)'
- en: Figure 16.4 How our Python neural network function classifies images of digits.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4 我们的Python神经网络函数如何对数字图像进行分类。
- en: If you identified the digits in figure 16.3 as 6, 0, 5, and 1, then congratulations!
    Your organic neural network (that is, your brain) is well trained. Our goal here
    is to build an artificial neural network that looks at such an image and classifies
    it as one of ten possible digits, perhaps as well as a human could.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将图16.3中的数字识别为6、0、5和1，那么恭喜您！您的有机神经网络（即您的头脑）训练得很好。我们的目标在这里是构建一个人工神经网络，它可以观察这样的图像并将其分类为十个可能的数字之一，也许和人类一样好。
- en: In chapter 15, the classification problem amounted to looking at a 2D vector
    and classifying it in one of two classes. In this problem, we look at 8x8 pixel
    grayscale images, where each of the 64 pixels is described by one number that
    tells us its brightness. Just as we treated images as vectors in chapter 6, we’ll
    treat the 64-pixel brightness values as a 64-dimensional vector. We want to put
    each 64-dimensional vector in one of ten classes, indicating which digit it represents.
    Thus, our classification function will have more inputs and more outputs than
    the one in chapter 15.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在第15章中，分类问题相当于观察一个二维向量并将其分类为两个类别之一。在本问题中，我们观察8x8像素的灰度图像，其中每个64像素由一个数字描述，该数字告诉我们其亮度。正如我们在第6章中将图像视为向量一样，我们将64像素的亮度值视为一个64维向量。我们希望将每个64维向量放入十个类别之一，表示它代表的数字。因此，我们的分类函数将比第15章中的函数有更多的输入和输出。
- en: Concretely, the neural network classification function we’ll build in Python
    will look like a function with 64 inputs and 10 outputs. In other words, it’s
    a (non-linear!) vector transformation from ℝ^(64) to ℝ^(10) . The input numbers
    are the pixel darkness values, scaled from 0 to 1, and the ten output values represent
    how likely the image is to be any of the ten digits. The index of the largest
    output number is the answer. In the following case (shown by figure 16.4), an
    image of a 5 is passed in, and the neural network returns its largest value in
    the fifth slot, so it correctly identifies the digit in the image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将用Python构建的神经网络分类函数将看起来像一个具有64个输入和10个输出的函数。换句话说，它是一个从ℝ^(64)到ℝ^(10)的（非线性！）向量变换。输入数字是像素的亮度值，从0到1缩放，十个输出值代表图像是十个数字中的哪一个的可能性。最大输出值的索引是答案。在以下情况（如图16.4所示）中，一个5的图像被传入，神经网络在第五个槽位返回其最大值，因此它正确地识别了图像中的数字。
- en: The neural network function in the middle of figure 16.4 is nothing more than
    a mathematical function. Its structure will be more complex than the ones we’ve
    seen so far, and in fact, the formula defining it is too long to write on paper.
    Evaluating a neural network is more like carrying out an algorithm. I’ll show
    you how to do this and implement it in Python.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4中间的神经网络函数不过是一个数学函数。它的结构将比我们之前看到的更复杂，实际上，定义它的公式太长了，无法写在纸上。评估神经网络更像是在执行一个算法。我将向您展示如何做这件事，并在Python中实现它。
- en: Just as we tested many different logistic functions in the previous chapter,
    we could try many different neural networks and see which one has the best predictive
    accuracy. Once again, the systematic way to do this is a gradient descent. While
    a linear function is determined by the two constants *a* and *b* in the formula
    *f*(*x*) = *ax* + *b*, a neural network of a given shape can have thousands of
    constants determining how it behaves. That’s a lot of partial derivatives to take!
    Fortunately, due to the form of the functions connecting neurons in our neural
    network, there’s a shortcut algorithm for taking the gradient, which is called
    *backpropagation*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章测试了许多不同的逻辑函数一样，我们也可以尝试许多不同的神经网络，看看哪一个具有最佳的预测准确性。再次强调，系统地做这件事的方法是梯度下降。虽然一个线性函数由公式
    *f*(*x*) = *ax* + *b* 中的两个常数 *a* 和 *b* 决定，但给定形状的神经网络可以有数千个常数决定其行为方式。这需要计算很多偏导数！幸运的是，由于连接我们神经网络中神经元的函数的形式，存在一个用于计算梯度的快捷算法，这被称为*反向传播*。
- en: It’s possible to derive the backpropagation algorithm from scratch and implement
    it using only the math we’ve covered so far, but unfortunately, that’s too big
    of a project to fit in this book. Instead, I’ll show you how to use a famous Python
    library called scikit-learn (“sci” pronounced as in “science”) to do the gradient
    descent for us, so it automatically trains the neural network to predict as well
    as possible for our data set. Finally, I’ll leave you with a teaser of the math
    behind backpropagation. I hope this will be just the starting point for your prolific
    career in machine learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始推导反向传播算法并仅使用我们至今为止所学的数学知识来实现它是有可能的，但不幸的是，这个项目太大，不适合放在这本书中。相反，我将向您展示如何使用一个名为scikit-learn（“sci”发音类似于“science”）的著名Python库来为我们执行梯度下降，这样它就可以自动训练神经网络，以尽可能好地预测我们的数据集。最后，我将向您透露反向传播背后的数学原理。我希望这将是您在机器学习领域辉煌职业生涯的起点。
- en: 16.2 Classifying images of handwritten digits
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 手写数字图像分类
- en: Before we start implementing our neural network, we need to prepare the data.
    The digit images I use are among the extensive, free test data that comes with
    the scikit-learn data. Once we download those, we need to convert them into 64-dimensional
    vectors with values scaled between zero and one. The data set also comes with
    the correct answers for each digit image, represented as Python integers from
    zero to nine.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始实现神经网络之前，我们需要准备数据。我使用的数字图像是scikit-learn数据中附带的大量免费测试数据之一。一旦我们下载了这些数据，我们需要将它们转换为介于零和一之间的64维向量。数据集还包含了每个数字图像的正确答案，表示为从零到九的Python整数。
- en: Then we build two Python functions to practice the classification. The first
    is a fake digit identification function called `random_classifier`, which takes
    64 numbers representing an image and (randomly) outputs 10 numbers representing
    the certainty that the image represents each digit from 0 to 9\. The second is
    a function called `test_digit_classify`, which takes a classifier and automatically
    plugs in every image in the data set, returning a count of how many correct answers
    come out. Because our `random_classifier` produces random results, it should only
    guess the right answer 10% of the time. This sets the stage for improvement when
    we replace it with a real neural network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建了两个Python函数来练习分类。第一个是一个名为`random_classifier`的假数字识别函数，它接受代表图像的64个数字，并（随机地）输出代表图像代表从0到9每个数字的确定性的10个数字。第二个是一个名为`test_digit_classify`的函数，它接受一个分类器，并自动将数据集中的每个图像插入其中，返回正确答案的数量。由于我们的`random_classifier`产生随机结果，它应该只有10%的时间猜对答案。这为我们用真正的神经网络替换它时提供了改进的基础。
- en: 16.2.1 Building the 64-dimensional image vectors
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.1 构建64维图像向量
- en: 'If you’re working with the Anacondas Python distribution as described in appendix
    A, you should already have the scikit-learn library available as `sklearn`. If
    not, you can install it with pip. To open `sklearn` and import the digits data
    set, you need the following code:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用附录A中描述的Anacondas Python发行版，你应该已经有了名为`sklearn`的scikit-learn库。如果没有，你可以使用pip安装它。要打开`sklearn`并导入数字数据集，你需要以下代码：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each entry of digits is a 2D NumPy array (a matrix), giving the pixel values
    of one image. For instance, `digits.images[0]` gives the pixel values of the first
    image in the data set, which is an 8-by-8 matrix of values:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数字中的每个条目都是一个2D NumPy数组（一个矩阵），给出了一个图像的像素值。例如，`digits.images[0]`给出了数据集中第一张图像的像素值，它是一个8x8的值矩阵：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/CH16_F05_Orland.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F05_Orland.png)'
- en: Figure 16.5 The first image in sklearn’s digit data set, which looks like a
    zero
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 sklearn数字数据集中的第一张图像，看起来像零
- en: You can see that the range of grayscale values is limited. The matrix consists
    only of whole numbers from 0 to 15.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到灰度值的范围是有限的。矩阵仅由0到15的整数组成。
- en: 'Matplotlib has a useful built-in function called `imshow`, which shows the
    entries of a matrix as an image. With the correct grayscale specification, the
    zeroes in the matrix appear as white and the bigger non-zero values appear as
    darker shades of gray. For instance, figure 16.5 shows the first image in the
    data set, which looks like a zero, resulting from `imshow` :'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib有一个有用的内置函数叫做`imshow`，它可以将矩阵的条目显示为图像。使用正确的灰度指定，矩阵中的零值显示为白色，较大的非零值显示为较深的灰色阴影。例如，图16.5显示了数据集中的第一张图像，看起来像零，这是由`imshow`生成的：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To emphasize once more how we’re going to think of this image as a 64-dimensional
    vector, figure 16.6 shows a version of the image with each of the 64-pixel brightness
    values overlaid on the corresponding pixels.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了再次强调我们将如何将此图像视为一个64维向量，图16.6显示了图像的一个版本，其中每个64像素的亮度值都叠加在相应的像素上。
- en: '![](../Images/CH16_F06_Orland.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F06_Orland.png)'
- en: Figure 16.6 An image from the digit data set with brightness values overlaid
    on each pixel.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6数字数据集中的一张图像，每个像素的亮度值都叠加在相应的像素上。
- en: 'To turn this 8-by-8 matrix of numbers into a single 64-entry vector, we can
    use a built-in NumPy function called `np .matrix.flatten`. This function builds
    a vector starting with the first row of the matrix, followed by the second row,
    and so on, giving us a vector representation of an image similar to the one we
    used in chapter 6\. Flattening the first image matrix indeed gives us a vector
    with 64 entries:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这个8x8的数字矩阵转换成一个包含64个元素的向量，我们可以使用一个内置的NumPy函数，称为`np.matrix.flatten`。这个函数从矩阵的第一行开始构建一个向量，接着是第二行，以此类推，从而给我们一个与第6章中使用的类似图像的向量表示。将第一张图像矩阵展平确实给我们一个包含64个元素的向量：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To keep our analysis numerically tidy, we’ll once again scale our data so that
    the values are between 0 and 1\. Because all the pixel values for every entry
    in this data set are between 0 and 15, we can scalar multiply these vectors by
    1 / 15 to get scaled versions. NumPy overloads the `*` and `/` operators to automatically
    work as scalar multiplication (and division) of vectors, so we can simply type
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持我们的分析在数值上整洁，我们再次将数据缩放，使值在0到1之间。因为数据集中每个条目的像素值都在0到15之间，我们可以将这些向量乘以1/15来得到缩放版本。NumPy重载了`*`和`/`运算符，以便自动作为向量的标量乘法（和除法）操作，所以我们只需简单地输入
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: and we’ll get a scaled result. Now we can build a sample digit classifier to
    plug these values into.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到一个缩放的结果。现在我们可以构建一个样本数字分类器来将这些值插入其中。
- en: 16.2.2 Building a random digit classifier
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.2 构建随机数字分类器
- en: The input to the digit classifier is a 64-dimensional vector, like the ones
    we just constructed, and the output is a 10-dimensional vector with each entry
    value between 0 and 1\. For our first example, the output vector entries can be
    randomly generated, but we interpret them as the classifier’s certainty that the
    image represents each of the ten digits.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数字分类器的输入是一个64维向量，就像我们刚才构建的那样，输出是一个包含每个元素值在0到1之间的10维向量。在我们的第一个例子中，输出向量的元素可以随机生成，但我们将它们解释为分类器对图像代表十个数字中的每一个的确定性。
- en: 'Because we’re okay with random outputs for now, this is easy to implement;
    NumPy has a function, `np.random.rand`, that produces an array of random numbers
    between 0 and 1 of a specified size. For instance, `np.random.rand(10)` gives
    us a NumPy array of 10 random numbers between 0 and 1\. Our `random_classifier`
    function takes an input vector, ignores it, and returns a random vector:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在对随机输出没有问题，这很容易实现；NumPy有一个函数，`np.random.rand`，它产生一个指定大小的介于0和1之间的随机数数组。例如，`np.random.rand(10)`
    给我们一个介于0和1之间的10个随机数的NumPy数组。我们的 `random_classifier` 函数接受一个输入向量，忽略它，并返回一个随机向量：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To classify the first image in the data set, we can run the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要对数据集中的第一张图像进行分类，我们可以运行以下代码：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The largest entry of this output is about `0.915`, occurring at index 4\. Returning
    this vector, our classifier tells us that there’s some chance that the image represents
    any of the digits and that it is most likely a 4\. To get the index of a maximum
    value programmatically, we can use the following Python code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出中的最大条目大约是 `0.915`，出现在索引4处。返回这个向量，我们的分类器告诉我们，图像代表任何数字的可能性，它最可能是4。要程序化地获取最大值的索引，我们可以使用以下Python代码：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, `max(result)` finds the largest entry of the array, and `list(result)`
    treats the array as an ordinary Python list. Then we can use the built-in `list`
    index function to find the index of the maximum value. The return value of 4 is
    incorrect; we saw previously that the picture is a 0, and we can check the official
    result as well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`max(result)` 找到数组中的最大条目，`list(result)` 将数组视为普通的Python列表。然后我们可以使用内置的 `list`
    索引函数来找到最大值的索引。返回值4是不正确的；我们之前看到这张图片是0，我们也可以检查官方结果。
- en: 'The correct digit for each image is stored at the corresponding index in the
    `digits.target` array. For the image `digits.images[0]`, the correct value is
    `digits.target[0]`, which is zero as we expected:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像的正确数字存储在 `digits.target` 数组的相应索引中。对于图像 `digits.images[0]`，正确值是 `digits.target[0]`，正如我们所期望的是零：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our random classifier predicted the image to be a 4 when in fact it was a 0\.
    Because it is guessing at random, it should be wrong 90% of the time, and we can
    confirm this by testing it on a lot of test examples.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的随机分类器预测图像为4，而实际上它是0。因为它是在随机猜测，所以90%的时间应该是错误的，我们可以通过在大量测试示例上测试它来确认这一点。
- en: 16.2.3 Measuring performance of the digit classifier
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.3 测量数字分类器的性能
- en: 'Now we’ll write the function `test_digit_classify`, which takes a classifier
    function and measures its performance on a large set of digit images. Any classifier
    function will have the same shape; it takes a 64-dimensional input vector and
    returns a 10-dimensional output vector. The `test_digit_classify` function goes
    through all of the test images and known correct answers and sees if the classifier
    produces the right answer:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写函数 `test_digit_classify`，它接受一个分类器函数并测量其在大量数字图像上的性能。任何分类器函数都将具有相同的形状；它接受一个64维输入向量并返回一个10维输出向量。`test_digit_classify`
    函数遍历所有测试图像和已知正确答案，查看分类器是否产生正确答案：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Starts the counter of correct classifications at 0
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将正确分类的计数器从0开始
- en: ❷ Loops over pairs of images in the test set with corresponding targets, giving
    the correct answer for the digit
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在测试集中循环图像对及其对应的目标，给出数字的正确答案
- en: ❸ Flattens the image matrix into a 64D vector and scales it appropriately
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将图像矩阵展平成一个64D向量，并适当地缩放
- en: ❹ Passes the image vector through the classifier to get a 10D result
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将图像向量通过分类器以获得一个10D结果
- en: ❺ Finds the index of the largest entry in this result, which is the classifier’s
    best guess
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 找到这个结果中最大条目的索引，这是分类器的最佳猜测
- en: ❻ If this matches our answer, increments the counter
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 如果这与我们的答案匹配，则增加计数器
- en: ❼ Returns the number of correct classifications as a fraction of the total number
    of test data points
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 返回正确分类的数量与总测试数据点的比例
- en: 'We expect our random classifier to get about 10% of the answers right. Because
    it acts randomly, it might do better on some trials than others, but because we’re
    testing on so many images, the result should be somewhere close to 10% every time.
    Let’s give it a try:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计我们的随机分类器大约能正确回答10%的问题。因为它随机行动，它可能在某些试验中做得比其他试验好，但由于我们在这么多图像上进行了测试，结果应该每次都接近10%。让我们试一试：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this test, our random classifier did slightly better than expected at 10.7%.
    This isn’t too interesting on its own, but now we’ve got our data organized and
    a baseline example to beat so we can start building our neural network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个测试中，我们的随机分类器在10.7%的准确率上略好于预期。这本身并不太有趣，但现在我们已经组织好了数据并有了可以超越的基线示例，因此我们可以开始构建我们的神经网络。
- en: 16.2.4 Exercises
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.4 练习
- en: '| **Exercise 16.1**: Suppose *a* digit classifier function outputs the following
    NumPy array. What digit has it concluded is in the image?'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习16.1**：假设一个数字分类器函数输出以下NumPy数组。它推断图像中包含的是哪个数字？'
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Solution**: The largest number in this array is `9.98060276e-01` , or approximately
    0.998, which appears fifth, or in index 4\. Therefore, this output says the image
    is classified as a 4. |'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**：此数组中的最大数是`9.98060276e-01`，或大约0.998，出现在第五位，或索引4。因此，这个输出表示图像被分类为4。'
- en: '| **Exercise 16.2-Mini Project**: Find the average of all the images of 9’s
    in the data set in the same way we took averages of the images in chapter 6\.
    Plot the resulting image. What does it look like?**Solution**: This code takes
    an integer *i* and averages the images in the data set that represent the digit
    *i*. Because the digit images are represented as NumPy arrays, which support addition
    and scalar multiplication, we can average them using the ordinary Python `sum`
    function and division operator:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习16.2-迷你项目**：以我们在第6章中取平均值的方式，找到数据集中所有9的图像的平均值。绘制结果图像。**解决方案**：此代码接受一个整数*i*，并计算表示数字*i*的数据集中图像的平均值。因为数字图像以NumPy数组的形式表示，支持加法和标量乘法，我们可以使用普通的Python
    `sum`函数和除法运算符来计算平均值：'
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With this code, `average_img(9)` computes an 8-by-8 matrix representing the
    average of all the images of 9’s, and it looks like this:![](../Images/CH16_F06_Orland_UN01.png)
    |
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，`average_img(9)`计算一个8-by-8矩阵，表示所有9的图像的平均值，其外观如下：![](../Images/CH16_F06_Orland_UN01.png)
    |
- en: '| **Exercise 16.3-Mini Project**: Build a better classifier than the random
    one by finding the average image of each kind of digit in the test data set and
    comparing a target image with all of the averages. Specifically, return a vector
    of the dot products of the target image with each average digit image.**Solution**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习16.3-迷你项目**：通过找到测试数据集中每种数字的平均图像并与目标图像进行比较，构建一个比随机分类器更好的分类器。具体来说，返回目标图像与每个平均数字图像的点积向量。**解决方案**：'
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Testing this classifier, we get 85% of the digits correct in the test data set.
    Not bad!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 测试这个分类器，我们在测试数据集中正确识别了85%的数字。还不错！
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 16.3 Designing a neural network
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 设计神经网络
- en: In this section, I show you how to think of a neural network as a mathematical
    function and how you can expect it to behave, depending on its structure. That
    sets us up for the next section, where we implement our first neural network as
    a Python function in order to classify digit images.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我向您展示如何将神经网络视为一个数学函数，以及根据其结构，您可以期望它如何表现。这为我们下一节做准备，在那里我们将我们的第一个神经网络作为Python函数实现，以便对数字图像进行分类。
- en: For our image classification problem, our neural network has 64 input values
    and 10 output values, and requires hundreds of operations to evaluate. For that
    reason, in this section, I stick with a simpler neural network with three inputs
    and two outputs. This makes it possible to picture the whole network and walk
    through every step of its evaluation. Once we cover this, it will be easy to write
    the evaluation steps that work on a neural network of any size in general Python
    code.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的图像分类问题，我们的神经网络有64个输入值和10个输出值，需要数百次操作来评估。因此，在本节中，我坚持使用一个更简单的神经网络，它有三个输入和两个输出。这使得我们可以想象整个网络并逐步了解其评估的每一步。一旦我们覆盖了这一点，编写适用于任何大小神经网络的评估步骤在一般Python代码中就会变得容易。
- en: 16.3.1 Organizing neurons and connections
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.1 组织神经元和连接
- en: As I described in the beginning of this chapter, the model for a neural network
    is a collection of neurons, where a given neuron activates, depending on how much
    its connected neurons activate. Mathematically, turning on a neuron is a function
    of the activations of the connected neurons. Depending on how many neurons are
    used, which neurons are connected, and the functions that connect them, the behavior
    of a neural network can be different. In this chapter, we’ll restrict our attention
    to one of the simplest useful kinds of neural networks−a *multilayer perceptron*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本章开头所描述的，神经网络的模型是一组神经元，其中给定神经元的激活取决于其连接神经元的激活程度。从数学上讲，激活一个神经元是连接神经元激活的函数。根据使用的神经元数量、连接的神经元以及连接它们的函数，神经网络的行为可能不同。在本章中，我们将关注最简单且有用的神经网络类型之一−*多层感知器*。
- en: A multilayer perceptron, abbreviated MLP, consists of several columns of neurons
    called *layers*, arranged from left to right. Each neuron’s activation is a function
    of the activations in the previous layer, which is the layer immediately to the
    left. The leftmost layer depends on no other neurons, and its activation is based
    on training data. Figure 16.7 provides a schematic of a four-layer MLP.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器，简称 MLP，由从左到右排列的几列神经元组成，称为*层*。每个神经元的激活是前一层激活的函数，即紧靠左侧的一层。最左侧的层不依赖于其他任何神经元，其激活基于训练数据。图16.7展示了四层
    MLP 的示意图。
- en: '![](../Images/CH16_F07_Orland.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F07_Orland.png)'
- en: Figure 16.7 A schematic of a multilayer perceptron (MLP), consisting of several
    layers of neurons
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 多层感知器（MLP）的示意图，由几层神经元组成
- en: In figure 16.7, each circle is a neuron, and lines between the circles show
    connected neurons. Turning on a neuron depends only on the activations of neurons
    from the previous layer, and it influences the activations of every neuron in
    the next layer. I arbitrarily chose the number of neurons in each layer, and in
    this particular schematic, the layers consist of three, four, three, and two neurons,
    respectively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图16.7中，每个圆圈代表一个神经元，圆圈之间的线条表示连接的神经元。一个神经元的激活仅取决于前一层的神经元激活，并影响下一层中每个神经元的激活。我任意选择了每层的神经元数量，在这个特定的示意图中，层分别由三个、四个、三个和两个神经元组成。
- en: Because there are 12 total neurons, there are 12 total activation values. Often
    there can be many more neurons (we’ll use 90 for digit classification), so we
    can’t give a letter variable name to every neuron. Instead, we represent all activations
    with the letter *a* and index them with superscripts and subscripts. The superscript
    indicates the layer, and the subscript indicates which neuron we’re talking about
    within the layer. For instance, a
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因为总共有12个神经元，所以总共有12个激活值。通常可以有更多的神经元（我们将在数字分类中使用90个），所以不能给每个神经元一个字母变量名。相反，我们用字母
    *a* 来表示所有激活，并用上标和下标来索引它们。上标表示层，下标表示层内讨论的神经元。例如，a
- en: '![](../Images/w_gifs_750.gif) is a number representing the activation of the
    second neuron in the second layer.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/w_gifs_750.gif) 代表第二层第二个神经元的激活。'
- en: 16.3.2 Data flow through a neural network
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.2 神经网络中的数据流
- en: To evaluate a neural network as a mathematical function, there are three basic
    steps, which I describe in terms of the activation values. I’ll walk through them
    conceptually, and then I’ll show you the formulas. Remember, a neural network
    is just a function that takes an input vector and produces an output vector. The
    steps in between are just a recipe for getting to the output from the given input.
    Here’s the first step in the pipeline.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要将神经网络作为一个数学函数来评估，有三个基本步骤，我将用激活值来描述。我将从概念上解释它们，然后展示公式。记住，神经网络只是一个接受输入向量并产生输出向量的函数。中间的步骤只是从给定输入得到输出的方法。这是管道中的第一步。
- en: 'Step 1: Set the input layer activations to the entries of the input vector'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步：将输入层激活设置为输入向量的条目
- en: The *input* layer is another word for the first or leftmost layer. The network
    in figure 16.7 has three neurons in the input layer, so this neural network can
    take 3D vectors as inputs. If our input vector is (0.3, 0.9, 0.5), then we can
    perform this first step by setting *a*[1]⁰ = 0.3, *a*[2]⁰ = 0.9, and *a*[3]⁰ =
    0.5\. That fills in 3 of the 12 total neurons in the network (figure 16.8).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**层是第一层或最左侧层的另一种说法。图16.7中的网络在输入层有三个神经元，所以这个神经网络可以接受3D向量作为输入。如果我们的输入向量是(0.3,
    0.9, 0.5)，那么我们可以通过将*a*[1]⁰ = 0.3、*a*[2]⁰ = 0.9和*a*[3]⁰ = 0.5来执行这一步。这填满了网络中总共12个神经元中的3个（图16.8）。'
- en: '![](../Images/CH16_F08_Orland.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F08_Orland.png)'
- en: Figure 16.8 Setting the input layer activations to the entries of the input
    vector (left)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8 将输入层激活设置为输入向量的条目（左侧）
- en: Each activation value in layer one is a function of the activations in layer
    zero. Now we have enough information to calculate them, so that’s step 2.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层中的每个激活值都是零层激活的函数。现在我们有了足够的信息来计算它们，所以这是步骤2。
- en: 'Step 2: Calculate each activation in the next layer as a function of all of
    the activations in the input layer'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2：将下一层的每个激活计算为输入层所有激活的函数
- en: This step is the meat of the calculation, and I’ll return to it once I’ve gone
    through all of the steps conceptually. The important thing to know for now is
    that each activation in the next layer is usually given by a *distinct function*
    of the previous layer activations. Say we want to calculate *a*[0]¹.This acti-vation
    is some function of *a*[1]⁰ , *a*[2]⁰ and *a*[3]⁰ ,which we can simply write as
    *a* 1¹ = *f*(*a*[1]⁰ , *a*[2]⁰ , *a*[3]⁰ )for now. Suppose, for instance, we calculate
    *f*(0.3, 0.9, 0.5) and the answer is 0.6\. Then the value of *a* 1¹ becomes 0.6
    in our calculation (figure 16.9).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步是计算的核心，在我概念性地走完所有步骤之后，我会回到这里。现在需要知道的重要一点是，下一层的每个激活通常是由前一层激活的**一个独特函数**给出的。比如说，我们要计算*a*[0]。这个激活是*a*[1]⁰、*a*[2]⁰和*a*[3]⁰的某个函数，我们可以暂时将其写作*a*
    1¹ = *f*(*a*[1]⁰ , *a*[2]⁰ , *a*[3]⁰)。假设，例如，我们计算*f*(0.3, 0.9, 0.5)的结果是0.6。那么在我们的计算中*a*
    1¹的值就变成了0.6（图16.9）。
- en: '![](../Images/CH16_F09_Orland.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F09_Orland.png)'
- en: Figure 16.9 Calculating an activation in layer one as some function of the activations
    in layer zero
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9 将第一层的激活计算为零层激活的某个函数
- en: When we calculate the next activation in layer one, *a*[2]¹ ,it is also a function
    of the input activations *a*[1]⁰ , *a*[2]⁰ ,and *a*[3]⁰ ,but in general, it is
    a different function, say *a*[2]¹ = *g*(*a*[1]⁰ , *a*[2]⁰ , *a*[3]⁰ ).The result
    still depends on the same inputs, but as a different function, it’s likely we’ll
    get a different result. Let’s say, *g*(0.3, 0.9, 0.5) = 0.1, then that’s our value
    for *a*[2]¹ (figure 16.10).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算第一层的下一个激活*a*[2]¹时，它也是输入激活*a*[1]⁰、*a*[2]⁰和*a*[3]⁰的函数，但在一般情况下，它是一个不同的函数，比如说*a*[2]¹
    = *g*(*a*[1]⁰ , *a*[2]⁰ , *a*[3]⁰)。结果仍然依赖于相同的输入，但由于是一个不同的函数，我们可能会得到不同的结果。比如说，*g*(0.3,
    0.9, 0.5) = 0.1，那么这就是*a*[2]¹的值（图16.10）。
- en: '![](../Images/CH16_F10_Orland.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F10_Orland.png)'
- en: Figure 16.10 Calculating another activation in layer one with another function
    of the input layer activations
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10 使用输入层激活的另一个函数计算第一层的另一个激活
- en: I used *f* and *g* because those are simple placeholder function names. There
    are two more distinct functions for *a*[3]¹ and *a*[4]¹ in terms of the input
    layer. I won’t keep naming these functions, because we’ll quickly run out of letters,
    but the important point is that each activation has a special function of the
    previous layer activations. Once we calculate all of the activations in layer
    one, we’ve 7 of the 12 total activations filled in. The numbers here are still
    made up, but the result might look something like that shown in figure 16.11.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用*f*和*g*作为简单的占位符函数名。关于输入层，*a*[3]¹和*a*[4]¹还有另外两个独特的函数。我不会继续命名这些函数，因为我们很快就会用完字母，但重要的是每个激活都有一个特殊的函数，该函数作用于前一层激活。一旦我们计算出第一层的所有激活，我们就已经填满了12个总激活中的7个。这里的数字仍然是虚构的，但结果可能看起来像图16.11中所示的那样。
- en: '![](../Images/CH16_F11_Orland.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F11_Orland.png)'
- en: Figure 16.11 Two layers of activations for our multilayer perceptron (MLP) calculated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11 计算了我们的多层感知器（MLP）的两个激活层
- en: From here on out, we repeat the process until we’ve calculated the activation
    of every neuron in the network, which is step 3.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们重复这个过程，直到我们计算出网络中每个神经元的激活，这是步骤3。
- en: 'Step 3: Repeat this process, calculating the activations of each subsequent
    layer based on the activations in the preceding layer'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤3：重复此过程，根据前一层的激活计算后续每一层的激活
- en: We start by calculating *a*[1]² as a function of the layer one activations,
    *a*[1]¹ , *a*[2]¹ , *a*[3]¹ ,and *a*[4]¹ .Then we move on to *a*[2]² and *a*[3]²
    ,which are given by their own functions. Finally, we calculate *a*[1]³ and *a*[2]³
    as their own functions of the layer two activations. At this point, we have an
    activation for every neuron in the network (figure 16.12).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算*a*[1]²作为一层激活*a*[1]¹、*a*[2]¹、*a*[3]¹和*a*[4]¹的函数。然后我们继续计算*a*[2]²和*a*[3]²，它们由它们自己的函数给出。最后，我们计算*a*[1]³和*a*[2]³作为它们自己关于二层激活的函数。此时，网络中每个神经元的激活都已计算出来（图16.12）。
- en: '![](../Images/CH16_F12_Orland.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F12_Orland.png)'
- en: Figure 16.12 An example of an MLP with all activations calculated
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12 一个所有激活都已计算的MLP示例
- en: At this point, our calculation is done. We have activations calculated for the
    middle layers, called *hidden layers*, and the final layer, called the *output
    layer*. All we need to do now is to read off the activations of the output layer
    to get our result and that’s step 4.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的计算已经完成。我们已经计算了中间层（称为*隐藏层*）和最终层（称为*输出层*）的激活。我们现在需要做的就是读取输出层的激活以获得我们的结果，这就是步骤4。
- en: 'Step 4: Return a vector whose entries are the activations of the output layer'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤4：返回一个向量，其条目是输出层的激活
- en: In this case, the vector is (0.2, 0.9), so evaluating our neural network as
    a function of the input vector (0.3, 0.9, 0.5) produces the output vector (0.2,
    0.9).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，向量是(0.2, 0.9)，因此将我们的神经网络作为输入向量(0.3, 0.9, 0.5)的函数评估，产生输出向量(0.2, 0.9)。
- en: That’s all there is to it! The only thing I didn’t cover is how to calculate
    individual activations, and these are what make the neural network distinct. Every
    neuron, except for those in the input layer, has its own function, and the parameters
    defining those functions are the numbers we’ll tweak to make the neural network
    do what we want.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！我没有涵盖的唯一内容是如何计算单个激活，这正是神经网络独特之处。除了输入层中的神经元外，每个神经元都有自己的函数，定义这些函数的参数是我们将调整以使神经网络做我们想要的事情的数字。
- en: 16.3.3 Calculating activations
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.3 计算激活
- en: 'The good news is that we’ll use a familiar form of function to calculate the
    activations in one layer as a function of those in the previous layer: logistic
    functions. The tricky part is that our neural network has 9 neurons outside the
    input layer, so there are 9 distinct functions to keep track of. What’s more,
    there are several constants to determine the behavior of each logistic function.
    Most of the work will be keeping track of all of these constants.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们将使用一种熟悉的形式的函数来计算一层的激活作为前一层的函数：逻辑函数。棘手的部分是，我们的神经网络在输入层之外有9个神经元，因此有9个不同的函数需要跟踪。更重要的是，有几个常数需要确定每个逻辑函数的行为。大部分工作将是跟踪所有这些常数。
- en: 'To focus on a specific example, we noted that in our sample MLP, we have the
    activation depend on the three input layer activations: *a*[1]⁰ , *a*[2]⁰ , and
    *a*[3]⁰ .The function giving *a*[1]¹ is a linear function of these inputs (including
    a constant) passed into a sigmoid function. There are four free parameters here,
    which I name *a*, *B*, *C*, and *D* for the moment (figure 16.13).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了专注于一个具体的例子，我们注意到在我们的样本MLP中，我们的激活依赖于三个输入层激活：*a*[1]⁰、*a*[2]⁰和*a*[3]⁰。给出*a*[1]¹的函数是这些输入（包括一个常数）通过sigmoid函数传递的线性函数。这里有四个自由参数，我暂时命名为*a*、*B*、*C*和*D*（图16.13）。
- en: '![](../Images/CH16_F13_Orland.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F13_Orland.png)'
- en: Figure 16.13 The general form of the function to calculate a11as a function
    of the input layer activations
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/CH16_F13_Orland.png)'
- en: We need to tune the variables *a*, *B*, *C*, and *D* to make *a*[1]¹ respond
    appropriately to inputs. In chapter 15, we thought of logistic functions as taking
    in several numbers and making a yes-or-no decision about them, reporting the answer
    as a certainty of “yes” from zero to one. In that sense, you can think of the
    neurons in the middle of the network as breaking the overall classification problem
    into smaller yes-or-no classifications.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要调整变量*a*、*B*、*C*和*D*，使*a*[1]¹能够适当地响应输入。在第15章中，我们将逻辑函数视为接受几个数字并就它们做出是或否的决定，将答案报告为零到一的“是”的确定性。从这个意义上说，你可以将网络中间的神经元视为将整体分类问题分解成更小的是或否分类。
- en: For every connection in the network, there is a constant telling us how strongly
    the input neuron activation affects the output neuron activation. In this case,
    the constant *a* tells us how strongly *a*[1]⁰ affects *a*[1]¹ ,while *B* and
    *C* tell us how strongly *a*[2]⁰ and *a*[3]⁰ affect *a*[1]¹ ,respectively. These
    constants are called *weights* for the neural network, and there is one weight
    for every line segment in the neural network general diagram used throughout this
    chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的每个连接都有一个常数告诉我们输入神经元激活如何强烈地影响输出神经元激活。在这种情况下，常数 *a* 告诉我们 *a*[1]⁰ 如何强烈地影响 *a*[1]¹，而
    *B* 和 *C* 分别告诉我们 *a*[2]⁰ 和 *a*[3]⁰ 如何强烈地影响 *a*[1]¹。这些常数被称为神经网络的 *权重*，网络中每条线段在整章中使用的通用图中都有一个权重。
- en: The constant *D* doesn’t affect the connection, but instead, independently increases
    or decreases the value of *a*[1]¹ ,which is not dependent on an input activation.
    This is appropriately named the *bias* for the neuron because it measures the
    inclination to make a decision without any input. The word *bias* sometimes comes
    with a negative connotation, but it’s an important part of any decision-making
    process; it helps avoid outlier decisions unless there is strong evidence.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 常数 *D* 不影响连接，而是独立地增加或减少 *a*[1]¹ 的值，这个值不依赖于输入激活。这恰当地被命名为神经元的 *偏置*，因为它衡量了在没有任何输入的情况下做出决策的倾向。单词
    *偏置* 有时带有负面含义，但它却是任何决策过程中的重要部分；它有助于避免异常决策，除非有强有力的证据。
- en: As messy as it might look, we need to index these weights and biases rather
    than giving them names like *a*, *B*, *C*, and *D*. We’ll write the weights in
    the form *w[ij]^l* , where *l* is the layer on the right of the connection, *i*
    is the index of the previous neuron in layer *l* − 1, and *j* is the index of
    the target neuron in layer *l*. For instance, the weight *a*, which impacts the
    first neuron of layer one based on the value of the first neuron of layer zero
    is denoted by *w*[11]¹ .The weight connecting the second neuron of layer three
    to the first neuron of the previous layer is *w*[21]³ (figure 16.14).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管看起来可能很混乱，我们需要对这些权重和偏置进行索引，而不是像 *a*、*B*、*C* 和 *D* 这样命名。我们将权重写成 *w[ij]^l* 的形式，其中
    *l* 是连接右侧的层，*i* 是层 *l* − 1 中前一个神经元的索引，*j* 是层 *l* 中目标神经元的索引。例如，影响第一层第一个神经元的权重 *a*，基于零层第一个神经元的值，表示为
    *w*[11]¹。连接第三层第二个神经元到前一层的第一个神经元的权重是 *w*[21]³（图16.14）。
- en: '![](../Images/CH16_F14_Orland.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F14_Orland.png)'
- en: Figure 16.14 Showing the connections corresponding to weights *w*[11]¹ and *w*[32]¹
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14 显示了与权重 *w*[11]¹ 和 *w*[32]¹ 对应的连接
- en: 'The biases correspond to neurons, not pairs of neurons, so there is one bias
    for each neuron: *b*[j]^l for the bias of the *j* th neuron in the *i^(th)* layer.
    In terms of these naming conventions, we could write the formula for *a*[1]¹ as'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置对应于神经元，而不是神经元对，因此每个神经元都有一个偏置：*b*[j]^l 表示第 *i* 层中第 *j* 个神经元的偏置。根据这些命名约定，我们可以写出
    *a*[1]¹ 的公式如下
- en: '*a*[1]¹ = σ(*w*[11]¹ *a*[1]⁰ + *w*[12]² *a*[2]⁰ + *w*[13]³ *a*[3]⁰ + *b*[1]¹)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[1]¹ = σ(*w*[11]¹ *a*[1]⁰ + *w*[12]² *a*[2]⁰ + *w*[13]³ *a*[3]⁰ + *b*[1]¹)'
- en: or the formula for *a*[3]² as
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 或者 *a*[3]² 的公式如下
- en: '*a*[3]² = σ(*w*[31]² *a*[1]¹ + *w*[32]² *a*[2]¹ + *w*[33]² *a*[3]¹ + *w*[34]²
    *a*[4]¹ + *b*[3]²)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[3]² = σ(*w*[31]² *a*[1]¹ + *w*[32]² *a*[2]¹ + *w*[33]² *a*[3]¹ + *w*[34]²
    *a*[4]¹ + *b*[3]²)'
- en: As you can see, computing activations to evaluate an MLP is not difficult, but
    the number of variables can make it a tedious and error-prone process. Fortunately,
    we can simplify the process and make it easier to implement using the notation
    of matrices we covered in chapter 5.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，计算激活以评估多层感知器（MLP）并不困难，但变量的数量可能会使其变得繁琐且容易出错。幸运的是，我们可以使用第5章中介绍的矩阵符号来简化这个过程，使其更容易实现。
- en: 16.3.4 Calculating activations in matrix notation
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.4 使用矩阵符号计算激活
- en: 'As nasty as it could be, let’s do a concrete example and write the formula
    for the activations of a whole layer of the network, and then we’ll see how to
    simplify it in matrix notation and write a reusable formula. Let’s take layer
    two. The formulas for the three activations are as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可能很复杂，让我们做一个具体的例子，并写出整个网络层的激活公式，然后我们将看到如何使用矩阵符号简化它并编写一个可重用的公式。让我们以第二层为例。三个激活的公式如下：
- en: '*a*[1]² = σ(*w*[11]² *a*[1]¹ + *w*[12]² *a*[2]¹ + *w*[13]² *a*[3]¹ + *w*[14]²
    *a*[4]¹ + *b*[1]²)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[1]² = σ(*w*[11]² *a*[1]¹ + *w*[12]² *a*[2]¹ + *w*[13]² *a*[3]¹ + *w*[14]²
    *a*[4]¹ + *b*[1]²)'
- en: '*a*[2]² = σ(*w*[21]² *a*[1]¹ + *w*[22]² *a*[2]¹ + *w*[23]² *a*[3]¹ + *w*[24]²
    *a*[4]¹ + *b*[2]²)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[2]² = σ(*w*[21]² *a*[1]¹ + *w*[22]² *a*[2]¹ + *w*[23]² *a*[3]¹ + *w*[24]²
    *a*[4]¹ + *b*[2]²)'
- en: '*a*[3]² = σ(*w*[31]² *a*[1]¹ + *w*[32]² *a*[2]¹ + *w*[33]² *a*[3]¹ + *w*[34]²
    *a*[4]¹ + *b*[3]²)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[3]² = σ(*w*[31]² *a*[1]¹ + *w*[32]² *a*[2]¹ + *w*[33]² *a*[3]¹ + *w*[34]²
    *a*[4]¹ + *b*[3]²)'
- en: It turns out to be useful to name the quantities inside the sigmoid function.
    Let’s denote the three quantities *z*[1]², *z*[2]², and *z*[3]²,so that by definition
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在sigmoid函数内部命名这些量是有用的。让我们用 *z*[1]²、*z*[2]² 和 *z*[3]² 来表示这三个量，因此根据定义
- en: '*a*[1]² = σ(*z*[1]²)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[1]² = σ(*z*[1]²)'
- en: '*a*[2]² = σ(*z*[2]²)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[2]² = σ(*z*[2]²)'
- en: and
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: and
- en: '*a*[3]² = σ(*z*[3]²)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[3]² = σ(*z*[3]²)'
- en: The formulas for these *z* values are nicer because they are all linear combinations
    of the previous layer activations, plus a constant. That means we can write them
    in matrix vector notation. Starting with
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 *z* 值的公式更简洁，因为它们都是前一层激活的线性组合，加上一个常数。这意味着我们可以用矩阵向量表示法来写它们。从以下开始：
- en: '*z*[1]² = *w*[11]² *a*[1]¹ + *w*[12]² *a*[2]¹ + *w*[13]² *a*[3]¹ + *w*[14]²
    *a*[4]¹ + *b*[1]²'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*[1]² = *w*[11]² *a*[1]¹ + *w*[12]² *a*[2]¹ + *w*[13]² *a*[3]¹ + *w*[14]²
    *a*[4]¹ + *b*[1]²'
- en: '*z*[2]² = *w*[21]² *a*[1]¹ + *w*[22]² *a*[2]¹ + *w*[23]² *a*[3]¹ + *w*[24]²
    *a*[4]¹ + *b*[2]²'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*[2]² = *w*[21]² *a*[1]¹ + *w*[22]² *a*[2]¹ + *w*[23]² *a*[3]¹ + *w*[24]²
    *a*[4]¹ + *b*[2]²'
- en: '*z*[3]² = *w*[31]² *a*[1]¹ + *w*[32]² *a*[2]¹ + *w*[33]² *a*[3]¹ + *w*[34]²
    *a*[4]¹ + *b*[3]²'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*[3]² = *w*[31]² *a*[1]¹ + *w*[32]² *a*[2]¹ + *w*[33]² *a*[3]¹ + *w*[34]²
    *a*[4]¹ + *b*[3]²'
- en: we can write all three equations as a vector
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这三个方程写成一个向量
- en: '![](../Images/CH16_F14_Orland_EQ06.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F14_Orland_EQ06.png)'
- en: 'and then pull out the biases as a vector sum:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将偏差作为一个向量求和：
- en: '![](../Images/CH16_F14_Orland_EQ07.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F14_Orland_EQ07.png)'
- en: 'This is just a 3D vector addition. Even though the big vector in the middle
    looks like a larger matrix, it is just a column of three sums. This big vector,
    however, can be expanded into a matrix multiplication as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个三维向量加法。尽管中间的大向量看起来像是一个更大的矩阵，但它实际上只是一个三个求和的列。然而，这个大向量可以展开成如下矩阵乘法：
- en: '![](../Images/CH16_F14_Orland_EQ08.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F14_Orland_EQ08.png)'
- en: The activations in layer two are then obtained by applying σ to every entry
    of the resulting vector. This is nothing more than a notational simplification,
    but it is useful psychologically to pull out the numbers *w[ij]^l* and *b*[j]^l
    into their own matrices. These are the numbers that define the neural network
    itself, as opposed to the activations *a* jl that are the incremental steps in
    the evaluation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层的激活是通过将σ函数应用于结果向量的每个条目来获得的。这仅仅是一个符号简化，但从心理上讲，将数字 *w[ij]^l* 和 *b*[j]^l 提取到它们自己的矩阵中是有用的。这些数字定义了神经网络本身，而不是定义评估增量步骤的激活
    *a* jl。
- en: To see what I mean, you can compare evaluating a neural network to evaluating
    the function *f*(*x*) = *ax* + *b*. The input variable is *x*, and by contrast,
    *a* and *b* are the constants that define the function; the space of possible
    linear functions is defined by the choice of *a* and *b*. The quantity *ax*, even
    if we relabeled it something like *q*, is merely an incremental step in the calculation
    of *f*(*x*). The analogy is that, once you’ve decided the number of neurons per
    layer in your MLP, the matrices of weights and vectors of biases for each layer
    are really the data defining the neural network. With that in mind, we can implement
    the MLP in Python.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明我的意思，你可以将评估神经网络与评估函数 *f*(*x*) = *ax* + *b* 进行比较。输入变量是 *x*，相比之下，*a* 和 *b*
    是定义函数的常数；可能的线性函数空间由 *a* 和 *b* 的选择定义。即使我们将 *ax* 重命名为类似 *q* 的东西，它也仅仅是 *f*(*x*) 计算中的一个增量步骤。类比是，一旦你决定了你的多层感知器（MLP）每层的神经元数量，每层的权重矩阵和偏差向量实际上就是定义神经网络的参数。考虑到这一点，我们可以在Python中实现MLP。
- en: 16.3.5 Exercises
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.5 练习
- en: '| **Exercise 16.4**: What neuron and layer is represented by the activation
    *a*[2]³ ? What value does this activation have in the following image? (Neurons
    and layers are indexed as throughout the previous sections.)![](../Images/CH16_F14_Orland_UN02.png)**Solution**:
    The superscript indicates the layer, and the subscript indicates the neuron within
    the layer. The activation *a*[2]³ ,therefore, corresponds to the second neuron
    in layer 3\. In the image, it has an activation value of 0.9. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **练习 16.4**：激活 *a*[2]³ 代表哪个神经元和层？在以下图像中，这个激活的值是多少？（神经元和层的索引方式与前面的章节相同。）![图片](../Images/CH16_F14_Orland_UN02.png)**解答**：上标表示层，下标表示层内的神经元。因此，激活
    *a*[2]³ 对应于第3层的第二个神经元。在图像中，它的激活值为0.9。|'
- en: '| **Exercise 16.5**: If layer 5 of a neural network has 10 neurons and layer
    6 has 12 neurons, how many total connections are there between neurons in layers
    5 and 6?**Solution**: Each of the 10 neurons in layer 5 is connected to each of
    the 12 neurons in layer 6\. That’s 120 total connections. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **练习16.5**：如果一个神经网络的第5层有10个神经元，第6层有12个神经元，第5层和第6层之间共有多少个神经元连接？**解答**：第5层的每个神经元都与第6层的每个神经元相连。总共有120个连接。'
- en: '| **Exercise 16.6**: Suppose we have an MLP with 12 layers. What are the indices
    *l*, *i*, and *j* of the weight *w[ij]^l* ,connecting the third neuron of layer
    4 to the seventh neuron of layer 5?**Solution**: Remember that *l* is the destination
    layer of the connection, so *l* = 5 in this case. The indices *i* and *j* refer
    to the neurons in layers *l* and *l* − 1, respectively, so *i* = 7 and *j* = 3\.
    The weight is labeled *w*[73]⁵ . |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **练习16.6**：假设我们有一个有12层的MLP。连接第4层的第三个神经元到第5层的第七个神经元的权重*w[ij]^l*的索引*l*、*i*和*j*是什么？**解答**：记住*l*是连接的目标层，所以在这个例子中*l*
    = 5。索引*i*和*j*分别指代层*l*和*l* - 1中的神经元，所以*i* = 7，*j* = 3。这个权重被标记为*w*[73]⁵。'
- en: '| **Exercise 16.7**: Where is the weight *w*[31]³ in the network used throughout
    the section?**Solution**: There is no such weight. This would connect to a third
    neuron in layer three, the output layer, but there are only two neurons in this
    layer. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **练习16.7**：在本节的整个网络中，权重*w*[31]³在哪里使用？**解答**：没有这样的权重。这将连接到第三层的第三个神经元，即输出层，但这个层只有两个神经元。'
- en: '| **Exercise 16.8**: In the neural network from this section, what’s a formula
    for *a*[1]³ in terms of the activations of layer 2 and the weights and biases?**Solution**:
    The previous layer activations are *a*[1]² , *a*[2]² ,and *a*[2]³ ,and the weights
    connecting them to *a*[1]³ are *w*[11]³ ,*w*[12]³ ,and *w*[1]³ 3.The bias for
    activation *a*[1]³ is denoted *b*[1]³ ,so the formula is as follows:![](../Images/CH16_F14_Orland_UN02_EQ09.png)
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| **练习16.8**：在本节的神经网络中，*a*[1]³的公式是什么，用第2层的激活值、权重和偏差表示？**解答**：前一层激活是*a*[1]²、*a*[2]²和*a*[2]³，连接到*a*[1]³的权重是*w*[11]³、*w*[12]³和*w*[1]³。激活*a*[1]³的偏差表示为*b*[1]³，所以公式如下！[](../Images/CH16_F14_Orland_UN02_EQ09.png)'
- en: '| **Exercise 16.9−Mini Project**: Write a Python function `sketch_mlp(*layer
    _sizes)` that takes layer sizes of a neural network and outputs a diagram like
    the ones used throughout this section. Show all of the neurons with labels and
    draw their connections with straight lines. Calling `sketch_mlp(3,4,3,2)` should
    produce the example from the diagram we have used to represent the neural net
    throughout.**Solution**: See the source code for this book for an implementation.
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| **练习16.9-迷你项目**：编写一个Python函数`sketch_mlp(*layer_sizes)`，它接受神经网络的层大小，并输出本节中使用的类似图表。显示所有神经元及其标签，并用直线绘制它们的连接。调用`sketch_mlp(3,4,3,2)`应该产生我们用来表示整个神经网络所用的示例图。**解答**：请参阅本书的源代码以获取实现方法。'
- en: 16.4 Building a neural network in Python
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 在Python中构建神经网络
- en: In this section, I show you how to take the procedure for evaluating an MLP
    that I explained in the last section and implement it in Python. Specifically,
    we’ll implement a Python class called `MLP` that stores weights and biases (randomly
    generated at first), and provides an `evaluate` method that takes a 64-dimensional
    input vector and returns the output 10-dimensional vector. This code is a somewhat
    rote translation of the MLP design I described in the last section into Python,
    but once we’re done with the implementation, we can test it at the task of classifying
    handwritten digits.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何将我在上一节中解释的评估MLP的步骤在Python中实现。具体来说，我们将实现一个名为`MLP`的Python类，该类存储权重和偏差（最初随机生成），并提供一个`evaluate`方法，该方法接受一个64维输入向量并返回一个10维输出向量。这段代码是将我在上一节中描述的MLP设计转换为Python的某种例行公事，但一旦我们完成实现，我们就可以在分类手写数字的任务上对其进行测试。
- en: As long as the weights and biases are randomly selected, it probably won’t do
    better than the random classifier we built to start with. But once we have the
    structure of a neural network to predict for us, we can tune the weights and biases
    to make it more predictive. We’ll turn to that problem in the next section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 只要权重和偏差是随机选择的，它可能不会比我们最初构建的随机分类器表现得更好。但一旦我们有了预测为我们工作的神经网络结构，我们就可以调整权重和偏差，使其更具预测性。我们将在下一节中转向这个问题。
- en: 16.4.1 Implementing an MLP class in Python
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.1 在Python中实现MLP类
- en: If we want our class to represent an MLP, we need to specify how many layers
    we want and how many neurons we want per layer. To initialize our MLP with the
    structure we want, our constructor can take a list of numbers, representing the
    number of neurons in each layer.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想让我们的类表示一个MLP，我们需要指定我们想要多少层以及每层想要多少个神经元。为了用我们想要的架构初始化我们的MLP，我们的构造器可以接受一个数字列表，表示每层的神经元数量。
- en: The data we need to evaluate the MLP are the weights and biases for every layer
    after the input layer. As we just covered, we can store the weights as a matrix
    (a NumPy array) and the biases as a vector (also a NumPy array). To start, we
    can use random values for all of the weights and biases, and then when we train
    the network, we can gradually replace these values with more meaningful ones.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要评估MLP的数据是输入层之后每一层的权重和偏差。正如我们刚刚讨论的，我们可以将权重存储为一个矩阵（一个NumPy数组）和偏差存储为一个向量（也是一个NumPy数组）。首先，我们可以为所有的权重和偏差使用随机值，然后在训练网络时，我们可以逐渐用更有意义的值替换这些值。
- en: Let’s quickly review the dimensions of the weight matrices and bias vectors
    that we want. If we pick a layer with *m* neurons, and the previous layer has
    *n* neurons, then our weights describe the linear part of the transformation from
    an *n* -dimensional vector of activations to an *m* -dimensional vector of activations.
    That’s described by an *m* -by- *n* matrix, in other words, one with *m* rows
    and *n* columns. To see this, we can return to the example from section 16.3,
    where the weights connecting a layer with four neurons to a layer with three neurons
    made up a 4-by−3 matrix as shown in figure 16.15.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下我们想要的权重矩阵和偏差向量的维度。如果我们选择一个有*m*个神经元的层，并且前一层有*n*个神经元，那么我们的权重描述了从*n*维激活向量到*m*维激活向量的转换的线性部分。这由一个*m*×*n*的矩阵描述，换句话说，一个有*m*行和*n*列的矩阵。为了看到这一点，我们可以回到16.3节中的例子，其中连接四个神经元层到三个神经元层的权重构成一个4×3的矩阵，如图16.15所示。
- en: '![](../Images/CH16_F15_Orland.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F15_Orland.png)'
- en: Figure 16.15 The weight matrix connecting a four neuron layer to a three neuron
    layer is a 3-by−4 matrix.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.15 连接四个神经元层到三个神经元层的权重矩阵是一个3×4矩阵。
- en: 'The biases for a layer of *m* neurons simply make up a vector with *m* entries,
    one for each neuron. Now that we’ve reminded ourselves how to find the size of
    the weight matrix and bias vector for each layer, we’re ready to have our class
    constructor create them. Notice that we iterate over `layer_sizes[1:]`, which
    gives us the sizes of layers in the MLP, skipping the input layer which comes
    first:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*m*个神经元的层的偏差简单地组成一个有*m*个条目的向量，每个条目对应一个神经元。现在我们已经提醒了自己如何找到每一层的权重矩阵和偏差向量的尺寸，我们就可以准备让我们的类构造器创建它们了。注意，我们遍历`layer_sizes[1:]`，这给出了MLP中层的尺寸，跳过了最先的输入层：
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Initializes the MLP with a list of layer sizes, giving the number of neurons
    for each layer
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用一个包含每层神经元数量的层大小列表初始化MLP
- en: ❷ The weight matrices are n-by-m matrices with random entries ...
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 权重矩阵是n×m的矩阵，包含随机条目...
- en: ❸ ... where m and n are the number of neurons of adjacent layers in the neural
    network.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ ... 其中m和n是神经网络中相邻层的神经元数量。
- en: ❹ The bias for each layer (skipping the first) is a vector with one entry per
    neuron in the layer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 每一层的偏差（跳过第一层）是一个包含每层中每个神经元的条目的向量。
- en: 'With this implemented, we can double-check that a two-layer MLP has exactly
    one weight matrix and one bias vector, and the dimensions match. Let’s say the
    first layer has two neurons and the second layer has three neurons. Then we can
    run this code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了这一点之后，我们可以再次确认一个两层的MLP恰好有一个权重矩阵和一个偏差向量，并且它们的维度匹配。假设第一层有两个神经元，第二层有三个神经元。然后我们可以运行以下代码：
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This confirms that we’ve a single 3-by−2 weight matrix and a single 3D bias
    vector, both populated with random entries.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这确认了我们有一个3×2的权重矩阵和一个3D的偏差向量，两者都填充了随机条目。
- en: The number of neurons in the input layer and output layer should match the dimensions
    of vectors we want to pass in and receive as output. Our problem of image classification
    calls for a 64D input vector and a 10D output vector. For this chapter, I stick
    with a 64-neuron input layer, a 10-neuron output layer, and a single 16-neuron
    layer in between. There is some combination of art and science to picking the
    right number of layers and layer sizes to get a neural network to perform well
    on a given task, and that’s the kind of thing machine learning experts get paid
    big bucks for. For the purpose of this chapter, I say this structure is sufficient
    to get us a good, predictive model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层的神经元数量应该与我们要传递的向量的维度以及作为输出接收的向量的维度相匹配。我们图像分类的问题需要64维的输入向量和10维的输出向量。对于本章，我坚持使用64个神经元的输入层，10个神经元的输出层，以及一个介于两者之间的16个神经元的单层。选择合适的层数和层大小以使神经网络在特定任务上表现良好，这需要一些艺术和科学相结合，这也是机器学习专家获得高薪的原因。为了本章的目的，我说这种结构足以让我们得到一个好的、可预测的模型。
- en: Our neural network can be initialized as `MLP([64,16,10])`, and it is much bigger
    than any of the ones we’ve drawn so far. Figure 16.16 shows what it looks like.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将神经网络初始化为`MLP([64,16,10])`，它比我们之前画的所有网络都要大得多。图16.16显示了它的样子。
- en: '![](../Images/CH16_F16_Orland.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F16_Orland.png)'
- en: Figure 16.16 An MLP with three layers of 64, 16, and 10 neurons, respectively
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16 一个具有64、16和10个神经元的三个层的MLP
- en: Fortunately, once we implement our evaluation method, it’s no harder for us
    to evaluate a big neural network than a small one. That’s because Python does
    all of the work for us!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一旦我们实现了评估方法，评估大型神经网络对我们来说并不比评估小型神经网络更难。这是因为Python为我们做了所有的工作！
- en: 16.4.2 Evaluating the MLP
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.2 评估MLP
- en: 'An evaluation method for our `MLP` class should take a 64D vector as input
    and return a 10D vector as output. The procedure to get from the input to the
    output is based on calculating the activations layer-by-layer from the input layer
    all the way to the output layer. As you’ll see when we discuss backpropagation,
    it’s useful to keep track of all of the activations as we go, even for the hidden
    layers in the middle of the network. For that reason, I’ll build the `evaluate`
    function in two steps: first, I’ll build a method to calculate all of the activations,
    and then I’ll build another one to pull the last layer activation values and produce
    the results.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们`MLP`类的评估方法应该接受一个64维向量作为输入，并返回一个10维向量作为输出。从输入到输出的过程是基于从输入层到输出层逐层计算激活。正如我们在讨论反向传播时将看到的，在过程中跟踪所有激活，即使是网络中间的隐藏层也很有用。因此，我将分两步构建`evaluate`函数：首先，我将构建一个方法来计算所有的激活，然后我将构建另一个方法来提取最后一层的激活值并生成结果。
- en: 'I call the first method `feedforward`, which is a common name for the procedure
    of calculating activations layer-by-layer. The input layer activations are given,
    and to get to the next layer, we need to multiply the vector of these activations
    by the weight matrix, add the next layer biases, and pass the coordinates of the
    result through the sigmoid function. We repeat this process until we get to the
    output layer. Here’s what it looks like:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我把第一种方法称为`feedforward`，这是按层计算激活过程的常用名称。输入层的激活给出后，为了到达下一层，我们需要将这些激活的向量与权重矩阵相乘，加上下一层的偏差，然后将结果的坐标通过sigmoid函数传递。我们重复这个过程，直到到达输出层。下面是这个过程的示意图：
- en: '[PRE17]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Initializes with an empty list of activations
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用空激活列表进行初始化
- en: ❷ The first layer activations are exactly the entries of the input vector; we
    append those to the list of activations.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一层的激活正好是输入向量的条目；我们将这些添加到激活列表中。
- en: ❸ Iterates over the layers with one weight matrix and bias vector per layer
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对每一层的单个权重矩阵和偏差向量进行迭代
- en: ❹ The vector z is the matrix product of the weights with the previous layer
    activations plus the bias vector.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 向量z是权重矩阵与前一层的激活的矩阵乘积加上偏差向量。
- en: ❺ Takes the sigmoid function of every entry of z to get the activation
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对z的每个条目应用sigmoid函数以获得激活
- en: ❻ Adds the new computed activation vector to the list of activations
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将新计算的激活向量添加到激活列表中
- en: 'The last layer activations are the results we want, so an evaluate method for
    the neural network simply runs the `feedforward` method for the input vector and
    then extracts the last activation vector like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层的激活是我们想要的结果，因此神经网络的一个评估方法简单地运行输入向量的`feedforward`方法，然后像这样提取最后一个激活向量：
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: That’s it! You can see that the matrix multiplication saved us a lot of loops
    over neurons we’d otherwise be writing to calculate the activations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！你可以看到矩阵乘法为我们节省了很多原本需要编写的神经元激活循环。
- en: 16.4.3 Testing the classification performance of an MLP
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.3 测试MLP的分类性能
- en: 'With an appropriately sized MLP, it can now accept a vector for a digit image
    and output a result:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了适当大小的MLP，它现在可以接受一个代表数字图像的向量并输出结果：
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That’s passing in a 64-dimensional vector representing an image and returning
    a 10-dimensional vector as an output, so our neural network is behaving as a correctly
    shaped vector transformation. Because the weights and biases are random, these
    numbers should not be a good prediction of what digit the image is likely to be.
    (Incidentally, the numbers are all close to 1 because all of our weights, biases,
    and input numbers are positive, and the sigmoid sends big positive numbers to
    values close to 1.) Even so, there is a *biggest* entry in this output vector,
    which happens to be the number at index 2\. This (incorrectly) predicts that image
    0 in the data set represents the number 2.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着输入一个代表图像的64维向量，并返回一个10维向量作为输出，因此我们的神经网络正在作为一个正确形状的向量转换器运行。由于权重和偏差是随机的，这些数字不应该是对图像可能代表的数字的良好预测。（顺便说一句，这些数字都接近1，因为我们的所有权重、偏差和输入数字都是正的，而sigmoid函数将大的正数发送到接近1的值。）即便如此，输出向量中有一个最大的条目，恰好是索引2处的数字。这（错误地）预测数据集中的图像0代表数字2。
- en: 'The randomness suggests that our MLP only guesses 10% of the answers correctly.
    We can confirm this with a `test_digit_classify` function. For the random MLP
    I initialized, it gave exactly 10%:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性表明我们的MLP只猜对了10%的答案。我们可以通过`test_digit_classify`函数来证实这一点。对于我初始化的随机MLP，它给出了正好10%：
- en: '[PRE20]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This may not seem like much progress, but we can pat ourselves on the back for
    getting the classifier working, even if it’s not good at its task. Evaluating
    a neural network is much more involved than evaluating a simple function like
    *f*(*x*) = *ax* + *b*, but we’ll see the payoff soon as we *train* the neural
    network to classify images more accurately.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来没有多少进步，但我们可以为自己感到高兴，因为我们的分类器已经工作，即使它在这个任务上并不擅长。评估神经网络比评估像 *f*(*x*) = *ax*
    + *b* 这样的简单函数要复杂得多，但当我们训练神经网络以更准确地分类图像时，我们很快就会看到回报。
- en: 16.4.4 Exercises
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.4 练习
- en: '| **Exercise 16.10-Mini Project**: Rewrite the `feedforward` method using explicit
    loops over the layers and weights rather than using NumPy matrix multiplication.
    Confirm that your result matches exactly with the previous implementation. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **练习16.10-迷你项目**：使用显式遍历层和权重而不是使用NumPy矩阵乘法重写`feedforward`方法。确认你的结果与之前的实现完全一致。
    |'
- en: 16.5 Training a neural network using gradient descent
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.5 使用梯度下降训练神经网络
- en: Training *a* neural network might sound like an abstract concept, but it just
    means finding the best weights and biases that makes the neural network do the
    task at hand as well as possible. We can’t cover the whole algorithm here, but
    I show you how it works conceptually and how to use a third-party library to do
    it automatically. By the end of this section, we’ll have adjusted the weights
    and biases of our neural network to predict which digit is represented by an image
    to a high degree of accuracy. We can then run it through `test_digit_classify`
    again and measure how well it does.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个神经网络可能听起来像是一个抽象的概念，但这只是意味着找到最佳权重和偏差，使神经网络尽可能好地完成手头的任务。我们在这里不能涵盖整个算法，但我将向你展示它是如何从概念上工作的，以及如何使用第三方库来自动完成它。在本节结束时，我们将调整我们神经网络的权重和偏差，以高度准确度预测图像代表的数字。然后我们可以再次运行`test_digit_classify`并衡量其表现。
- en: 16.5.1 Framing training as a minimization problem
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.5.1 将训练视为最小化问题
- en: In the previous chapters for a linear function *ax* + *b* or a logistic function
    σ(*ax* + *by* + *c*), we created a cost function that measured the failure of
    the linear or logistic function, depending on the constants in the formula, to
    match the data exactly. The constants for the linear function were the slope and
    *y* -intercept *a* and *b*, so the cost function had the form *C*(*a*, *b*). The
    logistic function had the constants *a*, *b*, and *c*(to be determined), so its
    cost function had the form *C*(*a*, *b*, *c*). Internally, both of these cost
    functions depended on *all* of the training examples. To find the best parameters,
    we’ll use gradient descent to minimize the cost function.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，对于线性函数 *ax* + *b* 或对数函数 σ(*ax* + *by* + *c*)，我们创建了一个成本函数来衡量线性或对数函数的失败，这取决于公式中的常数，以精确匹配数据。线性函数的常数是斜率和
    *y* -截距 *a* 和 *b*，因此成本函数的形式是 *C*(*a*, *b*)。对数函数有常数 *a*，*b* 和 *c*（待确定），因此其成本函数的形式是
    *C*(*a*, *b*, *c*)。内部，这两个成本函数都依赖于 *所有* 训练示例。为了找到最佳参数，我们将使用梯度下降来最小化成本函数。
- en: 'The big difference for an MLP is that its behavior can depend on hundreds or
    thousands of constants: all of its weights *w[ij]*¹and biases *b*[j]^l for every
    layer *l* and valid neuron indices *i* and *j*. Our neural network with 64, 16,
    and 10 neurons and its three layers have 64 · 16 = 1,024 weights between the first
    two layers and 16 · 10 = 160 weights between the second two. It has 16 biases
    in the hidden layer and 10 biases in the output layer. All in all, that’s 1,210
    constants we need to tune. You can picture our cost function as a function of
    these 1,210 values, which we need to minimize. If we write it out, it would look
    something like this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多层感知器（MLP）来说，一个很大的不同之处在于它的行为可以依赖于数百或数千个常数：它所有层 *l* 和有效神经元索引 *i* 和 *j* 的权重
    *w[ij]* 和偏差 *b*[j]^l。我们的具有64、16和10个神经元及其三层神经网络有第一层和第二层之间1,024个权重，第二层和第三层之间160个权重。它在隐藏层有16个偏差，在输出层有10个偏差。总的来说，我们需要调整1,210个常数。你可以想象我们的成本函数是这些1,210个值的函数，我们需要最小化它。如果我们把它写出来，它看起来可能像这样：
- en: '![](../Images/CH16_F16_Orland_EQ10.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F16_Orland_EQ10.png)'
- en: In the equation, where I’ve written the ellipses, there are over a thousand
    more weights and 24 more biases I didn’t write out. It’s worth thinking briefly
    about how to create the cost function, and as a mini-project, you can try implementing
    it yourself.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，我写省略号的地方，还有超过一千个未写出的权重和24个未写出的偏差。简要思考一下如何创建成本函数是值得的，并且作为一个迷你项目，你可以尝试自己实现它。
- en: Our neural network outputs vectors, but we consider the answer to the classification
    problem to be the digit represented by the image. To resolve this, we can think
    of the correct answer as the 10-dimensional vector that a perfect classifier would
    have as output. For instance, if an image clearly represents the digit 5, we would
    like to see 100% certainty that the image is a 5 and 0% certainty that the image
    is any other digit. That means a 1 in the fifth index and 0’s elsewhere (figure
    16.17).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络输出向量，但我们认为分类问题的答案是图像所代表的数字。为了解决这个问题，我们可以将正确答案视为一个完美的分类器作为输出的10维向量。例如，如果一个图像清楚地表示数字5，我们希望看到100%的确定性，即图像是5，而0%的确定性表示图像是任何其他数字。这意味着第五个索引处有一个1，其他地方都是0（图16.17）。
- en: '![](../Images/CH16_F17_Orland.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland.png)'
- en: 'Figure 16.17 Ideal output from a neural network: 1.0 in the correct index and
    0.0 elsewhere'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17 神经网络的理想输出：正确索引处为1.0，其他地方为0.0
- en: Just as our previous attempts at regression never fit the data exactly, neither
    will our neural network. To measure the error from our 10-dimensional output vector
    to the ideal output vector, we can use the square of their distance in 10 dimensions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前的回归尝试从未完全拟合数据一样，我们的神经网络也不会。为了衡量我们的10维输出向量到理想输出向量的误差，我们可以使用它们在10维空间中的距离平方。
- en: 'Suppose the ideal output is written *y* = (*y*[1] , *y*[1] , *y*[2] , ...,
    *y*[10] ). Note that I’m following the math convention for indexing from 1 rather
    than the Python convention of indexing from 0\. That’s actually the same convention
    I used for neurons within a layer, so the output layer (layer two) activations
    are indexed (*a*[1]² , *a*[2]² , *a*[3]² ,..., *a*[10]¹).The squared distance
    between these vectors is the sum:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 假设理想输出是写为 *y* = (*y*[1] , *y*[1] , *y*[2] , ..., *y*[10] )。请注意，我遵循的是从1开始的数学索引惯例，而不是Python从0开始的索引惯例。这实际上是我用于层内神经元的相同惯例，因此输出层（第二层）的激活是按
    (*a*[1]² , *a*[2]² , *a*[3]² ,..., *a*[10]¹) 索引的。这些向量之间的平方距离是它们的和：
- en: '![](../Images/CH16_F17_Orland_EQ11.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F17_Orland_EQ11.png)'
- en: As another potentially confusing point, the superscript 2 above the *a* values
    indicates the output layer is layer two in our network, while the 2 outside the
    parentheses means squaring the quantity. To get a total cost relative to the data
    set, you can evaluate the neural network for all of the sample images and take
    the average squared distance. At the end of the section, you can try the mini-project
    for implementing this yourself in Python.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个可能令人困惑的点，*a* 值上方的上标2表示我们的网络中的输出层是第二层，而括号外的2表示对数量进行平方。为了得到相对于数据集的总成本，你可以评估所有样本图像的神经网络并取平均平方距离。在本节的末尾，你可以尝试迷你项目，在Python中自己实现这一功能。
- en: 16.5.2 Calculating gradients with backpropagation
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.5.2 使用反向传播计算梯度
- en: With the cost function *C*(*w*[11]¹ ,*w*[12]¹ , ..., *b*[1]¹ , *b*[2]¹ , ...)
    coded in Python, we could write a 1,210-dimensional version of gradient descent.
    This would mean taking 1,210 partial derivatives in each step to get a gradient.
    That gradient would be the 1,210-dimensional vector of the partial derivatives
    at the point, having this form
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在Python中编写的成本函数 *C*(*w*[11]¹ ,*w*[12]¹ , ..., *b*[1]¹ , *b*[2]¹ , ...)，我们可以编写一个1,210维的梯度下降版本。这意味着在每一步都要计算1,210个偏导数以获得一个梯度。这个梯度将是该点的1,210维偏导数向量，具有以下形式
- en: '![](../Images/CH16_F17_Orland_EQ12.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F17_Orland_EQ12.png)'
- en: Estimating so many partial derivatives would be computationally expensive because
    each would require evaluating *C* twice to test the effect of tweaking one of
    its input variables. In turn, evaluating *C* requires looking at every image in
    the training set and passing it through the network. It might be possible to do
    this, but the computation time would be prohibitively long for most real-world
    problems like ours.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 估计如此多的偏导数在计算上会很昂贵，因为每个都需要评估 *C* 两次以测试调整其输入变量的效果。反过来，评估 *C* 需要查看训练集中的每一张图片并通过网络传递。这可能是有可能的，但对于我们这样的大多数现实世界问题，计算时间会过长。
- en: 'Instead, the best way to calculate the partial derivatives is to find their
    exact formulas using methods similar to those we covered in chapter 10\. I won’t
    completely cover how to do this, but I’ll give you a teaser in the last section.
    The key is that while there are 1,210 partial derivatives to take, they all have
    the form:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，计算偏导数最好的方法是使用与我们在第10章中介绍的方法类似的方法找到它们的精确公式。我不会完全介绍如何做这件事，但我会给你一个预告，在最后一节。关键是虽然需要计算1,210个偏导数，但它们都具有以下形式：
- en: '![](../Images/CH16_F17_Orland_EQ13a.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F17_Orland_EQ13a.png)'
- en: or
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '![](../Images/CH16_F17_Orland_EQ13b.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH16_F17_Orland_EQ13b.png)'
- en: for some choice of indices *l*, *i*, and *j*. The algorithm of *backpropagation*
    calculates all of these partial derivatives recursively, working backward from
    the output layer weights and biases, all the way to layer one.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些选择的索引 *l*，*i* 和 *j*。反向传播算法递归地计算所有这些偏导数，从输出层的权重和偏差反向工作到第一层。
- en: If you’re interested in learning more about backpropagation, stay tuned for
    the last section of the chapter. For now, I’ll turn to the scikit-learn library
    to calculate costs, carry out backpropagation, and complete the gradient descent
    automatically.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于反向传播的信息，请关注本章的最后部分。现在，我将转向scikit-learn库来计算成本、执行反向传播并自动完成梯度下降。
- en: 16.5.3 Automatic training with scikit-learn
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.5.3 使用scikit-learn进行自动训练
- en: We don’t need any new concepts to train an MLP with scikit-learn; we just need
    to tell it to set up the problem the same way we have and then find the answer.
    I won’t explain everything the scikit-learn library can do, but I will step you
    through the code to train the MLP for digit classification.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练MLP不需要任何新概念；我们只需要告诉它以与我们相同的方式设置问题，然后找到答案。我不会解释scikit-learn库能做什么，但我将逐步引导你通过代码来训练用于数字分类的MLP。
- en: 'The first step is to put all of our training data (in this case, the digit
    images as 64-dimensional vectors) into a single NumPy array. Using the first 1,000
    images in the data set gives us a 1,000-by−64 matrix. We’ll also put the first
    1,000 answers in an output list:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将所有我们的训练数据（在这种情况下，数字图像作为64维向量）放入一个单一的NumPy数组中。使用数据集中的前1,000个图像给我们一个1,000-by-64的矩阵。我们还将前1,000个答案放入一个输出列表中：
- en: '[PRE21]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we use the `MLP` class that comes with scikit-learn to initialize an
    MLP. The sizes of the input and output layers are determined by the data, so we
    only need to specify the size of our single hidden layer in the middle. Additionally,
    we include parameters telling the MLP how we want it to be trained. Here’s the
    code:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 scikit-learn 提供的 `MLP` 类来初始化一个 MLP。输入和输出层的尺寸由数据决定，所以我们只需要指定中间隐藏层的尺寸。此外，我们还包括参数来告诉
    MLP 我们希望如何训练它。以下是代码：
- en: '[PRE22]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Specifies that we want a hidden layer with 16 neurons
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定我们想要一个包含 16 个神经元的隐藏层
- en: ❷ Specifies that we want to use logistic (ordinary sigmoid) activation functions
    in the network
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定我们希望在网络上使用逻辑（普通 sigmoid）激活函数
- en: ❸ Sets the maximum number of gradient descent steps to take in case there are
    convergence issues
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置梯度下降步骤的最大数量，以防出现收敛问题
- en: ❹ Selects that the training process provides verbose logs
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 选择训练过程提供详细日志
- en: ❺ Initializes the MLP with random weights and biases
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用随机权重和偏差初始化 MLP
- en: ❻ Decides the learning rate or what multiple of the gradient to move in each
    iteration of the gradient descen
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 决定学习率或每次梯度下降迭代中移动梯度的倍数
- en: 'Once this is done, we can train the neural network to the input data *x* and
    corresponding output data *y* in one line:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这项工作，我们就可以在一行中将神经网络训练到输入数据 *x* 和相应的输出数据 *y*：
- en: '[PRE23]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When you run this line of code, you’ll see a bunch of text print in the terminal
    window as the neural network trains. This logging shows how many gradient descent
    steps it takes and the value of the cost function, which scikit-learn calls “loss”
    instead of “cost.”
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这一行代码时，你会在终端窗口中看到许多文本打印出来，作为神经网络训练的日志。这些日志显示了它需要多少次梯度下降迭代以及成本函数的值，scikit-learn
    将其称为“损失”而不是“成本。”
- en: '[PRE24]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'At this point, after 60 iterations of gradient descent, a minimum has been
    found and the MLP is trained. You can test it on image vectors using the `_predict`
    method. This method takes an array of inputs, meaning an array of 64-dimensional
    vectors, and returns the output vectors for all of them. For instance, `mlp._predict(*x*)`
    gives the 10-dimensional output vectors for all 1,000 image vectors stored in
    *x*. The result for the zero^(th) training example is the zero^(th) entry of the
    result:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，经过 60 次梯度下降迭代后，找到了最小值，MLP 已经被训练。你可以使用 `_predict` 方法在图像向量上测试它。此方法接受一个输入数组，意味着一个
    64 维向量的数组，并返回所有这些向量的输出向量。例如，`mlp._predict(*x*)` 给出存储在 *x* 中的所有 1,000 个图像向量的 10
    维输出向量。对于零^(th) 个训练示例的结果是结果中的零^(th) 个条目：
- en: '[PRE25]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It takes some squinting at these numbers in scientific notation, but the first
    one is 0.9998, while the others are all less than 0.001\. This correctly predicts
    that the zero^(th) training example is a picture of the digit 0\. So far so good!
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字在科学记数法中需要仔细观察，但第一个是 0.9998，而其他所有数字都小于 0.001。这正确地预测了零^(th) 个训练示例是一张数字 0 的图片。到目前为止一切顺利！
- en: 'With a small wrapper, we can write a function that uses this MLP to do *one*
    prediction, taking a 64-dimensional image vector and outputting a 10-dimensional
    result. Because scikit-learn’s MLP works on collections of input vectors and produces
    arrays of results, we just need to put our input vector in a list before passing
    it to `mlp._predict` :'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个小包装器，我们可以编写一个函数，使用这个 MLP 进行 *一次* 预测，接受一个 64 维图像向量并输出一个 10 维结果。因为 scikit-learn
    的 MLP 在输入向量的集合上工作并产生结果数组，我们只需要在传递给 `mlp._predict` 之前将我们的输入向量放入一个列表中：
- en: '[PRE26]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'At this point, the vector has the correct shape to have its performance tested
    by our `test_digit_classify` function. Let’s see what percentage of the test digit
    images it correctly identifies:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，向量具有正确的形状，可以通过我们的 `test_digit_classify` 函数测试其性能。让我们看看它正确识别测试数字图像的百分比：
- en: '[PRE27]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: That’s an astonishing 100% accuracy! You might be skeptical of this result;
    after all, we’re testing on the same data set that the neural network used to
    train. In theory, when storing 1,210 numbers, the neural net could have just memorized
    every example in the training set. If you test the images the neural network hasn’t
    seen before, you’ll see this isn’t the case; it still does an impressive job classifying
    the images correctly as digits. I found that it had 96.2% accuracy on the next
    500 images in the data set, and you can test this yourself in an exercise.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个惊人的100%准确率！你可能对这个结果持怀疑态度；毕竟，我们是在使用神经网络训练所用的相同数据集进行测试。从理论上讲，当存储1,210个数字时，神经网络可能只是记住了训练集中的每一个例子。如果你测试神经网络之前未见过的新图像，你会发现情况并非如此；它仍然能够出色地将图像正确分类为数字。我发现它在数据集中接下来的500个图像上的准确率为96.2%，你可以在练习中自己测试这一点。
- en: 16.5.4 Exercises
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.5.4 练习
- en: '| **Exercise 16.11**: Modify the `test_digit_classify` function to work on
    a custom range of examples in the test set. How does it do on the next 500 examples
    after the 1,000 training examples? |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| **练习16.11**：修改`test_digit_classify`函数，使其能够在测试集的定制范围内工作。它在1,000个训练例子之后的下一个500个例子上的表现如何？|'
- en: '| **Solution**: Here I’ve added a `start` keyword argument to indicate which
    test example to start with. The `test_count` keyword argument still indicates
    the number of examples to test:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '| **解答**：在这里，我添加了一个`start`关键字参数来指示从哪个测试例子开始。`test_count`关键字参数仍然表示要测试的例子数量：'
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '❶ Calculates the end index for test data we want to consider❷ Loops only over
    the test data between the start and end indicesMy trained MLP identifies 96.2%
    of these fresh digit images correctly:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算我们想要考虑的测试数据的结束索引❷ 只在起始和结束索引之间的测试数据上循环My trained MLP正确识别了96.2%的这些新数字图像：
- en: '[PRE29]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Exercise 16.12**: Using the squared distance cost function, what is the
    cost of your randomly generated MLP for the first 1,000 training examples? What
    is the cost of the scikit-learn MLP?**Solution**: First, we can write a function
    to give us the ideal output vector for a given digit. For instance, for the digit
    5, we’d like an output vector *y*, which is all zeros except for a one in the
    fifth index.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习16.12**：使用平方距离成本函数，你的随机生成的MLP在第一个1,000个训练例子上的成本是多少？scikit-learn的MLP的成本是多少？**解答**：首先，我们可以编写一个函数来给出给定数字的理想输出向量。例如，对于数字5，我们希望输出向量*y*，除了第五个索引处有一个1之外，其余都是0。'
- en: '[PRE30]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The cost of one test example is the sum of squared distance from what the classifier
    outputs to the ideal result. That’s the sum of squared differences in the coordinates
    added up:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一个测试例子的成本是分类器输出与理想结果之间的平方距离之和。这就是坐标差的平方和的总和：
- en: '[PRE31]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The total cost for a classifier is the average cost over all of the 1,000 training
    examples:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类器的总成本是所有1,000个训练例子上的平均成本：
- en: '[PRE32]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As expected, a randomly initialized MLP with only 10% predictive accuracy has
    a much higher cost than a 100% accurate MLP produced by scikit-learn:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，一个随机初始化的MLP，其预测准确率为10%，比由scikit-learn生成的100%准确率的MLP具有更高的成本：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Exercise 16.13-Mini Project**: Extract the `MLPClassifier` weights and
    biases using its properties `coefs_` and `intercepts_`, respectively. Plug these
    weights and biases into the `MLP` class we built from scratch earlier in this
    chapter and show that your resulting MLP performs well on digit classification.**Solution**:
    If you try this, you’ll notice one problem; where we expect the weight matrices
    to be 16-by−64 and 10-by−16, the `coefs_` property of `MLPClassifier` gives a
    64-by−16 matrix and a 16-by−10 matrix. It looks like scikit-learn uses a convention
    that stores columns of the weight matrices versus our convention that stores rows.
    There’s a quick way to fix this.NumPy arrays have a `T` property returning the
    *transpose* of a matrix (a matrix obtained by pivoting the matrix so that the
    rows become the columns of the result). With this trick in mind, we can plug the
    weights and biases into our neural network and test it:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习16.13-迷你项目**：使用`MLPClassifier`的属性`coefs_`和`intercepts_`分别提取`MLPClassifier`的权重和偏置。将这些权重和偏置插入到本章前面从头构建的`MLP`类中，并展示你的MLP在数字分类上的表现良好。**解答**：如果你尝试这样做，你会注意到一个问题；我们期望权重矩阵是16-by-64和10-by-16，而`MLPClassifier`的`coefs_`属性给出的是一个64-by-16的矩阵和一个16-by-10的矩阵。看起来scikit-learn使用了一个与我们不同的惯例来存储权重矩阵的列。有一个快速的方法可以解决这个问题。NumPy数组有一个`T`属性，返回矩阵的**转置**（通过旋转矩阵使得行成为结果的列）。有了这个技巧，我们可以将权重和偏置插入到我们的神经网络中，并对其进行测试：'
- en: '[PRE34]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Sets our weight matrices to the ones from the scikit-learn MLP, after transposing
    them to agree with our convention❷ Sets our network’s biases to the ones from
    the scikit-learn MLP❸ Tests the performance of our neural network at the classification
    task with new weights and biasesThis is 96.2% accurate on the 500 images after
    the training data set, just like the MLP produced by scikit-learn directly. |
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将我们的权重矩阵设置为 scikit-learn MLP 中的矩阵，在转置后与我们的约定一致❷ 将我们的网络偏差设置为 scikit-learn MLP
    中的偏差❸ 使用新的权重和偏差测试我们的神经网络在分类任务中的性能This is 96.2% accurate on the 500 images after
    the training data set, just like the MLP produced by scikit-learn directly. |
- en: 16.6 Calculating gradients with backpropagation
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.6 使用反向传播计算梯度
- en: This section is completely optional. Frankly, because you know how to train
    an MLP using scikit-learn, you’re ready to solve real-world problems. You can
    test neural networks of different shapes and sizes on classification problems
    and experiment with their design to improve classification performance. Because
    this is the last section in the book, I wanted to give you some final, challenging
    (but doable!) math to chew on−calculating partial derivatives of the cost function
    by hand.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节完全是可选的。坦白说，因为你已经知道如何使用 scikit-learn 训练 MLP，你现在可以解决实际问题了。你可以在分类问题上测试不同形状和大小的神经网络，并尝试它们的结构设计以提高分类性能。因为这是本书的最后一节，我想给你一些最后的、具有挑战性（但可行！）的数学问题来思考−手动计算成本函数的偏导数。
- en: 'The process of calculating partial derivatives of an MLP is called *backpropagation*
    because it’s efficient to start with the weights and biases of the last layer
    and work backwards. Backpropagation can be broken into four steps: calculating
    the derivatives with respect to the last layer weights, last layer biases, hidden
    layer weights, and hidden layer biases. I’ll show you how to get the partial derivatives
    with respect to the weights in the last layer, and you can try running with this
    approach to do the rest.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 计算多层感知器（MLP）的偏导数的过程称为 *反向传播*，因为它从最后一层的权重和偏差开始，逆向工作非常高效。反向传播可以分为四个步骤：计算与最后一层权重、最后一层偏差、隐藏层权重和隐藏层偏差的偏导数。我将向您展示如何获取与最后一层权重相关的偏导数，您可以用这种方法尝试完成剩余部分。
- en: 16.6.1 Finding the cost in terms of the last layer weights
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.6.1 以最后一层权重表示的成本
- en: Let’s call the index of the last layer of the MLP *L*. That means that the last
    weight matrix consists of the weights *w[ij]^L* ,where *l* = *L*, in other words,
    the weights *w[ij]^L* .The biases in this layer are *b* jL and the activations
    are labeled *a[j]^L* .
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称 MLP 的最后一层的索引为 *L*。这意味着最后一层的权重矩阵由权重 *w[ij]^L* 组成，其中 *l* = *L*，换句话说，权重 *w[ij]^L*。这一层的偏差是
    *b* jL，激活值标记为 *a[j]^L*。
- en: The formula to get the *j*^(th) neuron’s activation in the last layer *a[j]^L*
    is a sum of the contribution from every neuron in layer *L* − *l*, indexed by
    *i*. In a made-up notation, it becomes
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最后一层中第 *j* 个神经元的激活值 *a[j]^L* 的公式是层 *L* 中每个神经元贡献的总和，这些贡献由索引 *i* 表示。用虚构的符号表示，它变为
- en: '![](../Images/CH16_F17_Orland_EQ14.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH16_F17_Orland_EQ14.png)'
- en: 'The sum is taken over all values of *i* from one to the number of neurons in
    layer *L* − *l*. Let’s write the number of neurons in layer *l* as *ni*, with
    *i* ranging from *l* to *n[L]*[−1] in our sum. In proper mathematical summation
    notation, this sum is written:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 求和是对层 *L* 中从 1 到神经元数量的所有 *i* 值进行的。让我们将层 *l* 中的神经元数量写成 *ni*，其中 *i* 从 *l* 到 *n[L]*[−1]
    在我们的求和中。在适当的数学求和符号中，这个求和写成：
- en: '![](../Images/CH16_F17_Orland_EQ15.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH16_F17_Orland_EQ15.png)'
- en: 'The English translation of this formula is “fixing values of *L* and *j* by
    adding up the values of the expression *w[ij]^L* *a*[i]^(*L*−1) for every *i*
    from one to *n[L]*.” This is nothing more than the formula for matrix multiplication
    written as a sum. In this form, the activation is as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式的英文翻译是“通过将 *L* 和 *j* 的值固定，将表达式 *w[ij]^L* *a*[i]^(*L*−1) 的值从 1 到 *n[L]* 的每个
    *i* 相加。”这不过是将矩阵乘法写成求和的形式。在这种形式下，激活如下：
- en: '![](../Images/CH16_F17_Orland_EQ16.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH16_F17_Orland_EQ16.png)'
- en: Given an actual training example, we can have some ideal output vector **y**
    with a 1 in the correct slot and 0’s elsewhere. The cost is the squared distance
    between the activation vector *a[j]^L* and the ideal output values *y[j]*. That
    is,
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个实际的训练示例，我们可以有一个理想的输出向量 **y**，其中正确的槽位为 1，其他地方为 0。成本是激活向量 *a[j]^L* 和理想输出值
    *y[j]* 之间的平方距离。即，
- en: '![](../Images/CH16_F17_Orland_EQ17.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH16_F17_Orland_EQ17.png)'
- en: The impact of a weight *w[ij]^L* on *C* is indirect. First, it is multiplied
    by an activation from the previous layer, added to the bias, passed through a
    sigmoid, and then passed through the quadratic cost function. Fortunately, we
    covered how to take derivatives of compositions of functions in chapter 10\. This
    example is a bit more complicated, but you should be able to recognize it as the
    same chain rule we saw before.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 权重 *w[ij]^L* 对 *C* 的影响是间接的。首先，它被前一层的一个激活值乘以，加上偏差，通过sigmoid函数，然后通过二次成本函数。幸运的是，我们在第10章中已经介绍了如何求函数复合的导数。这个例子稍微复杂一些，但你应该能够认出它为之前看到的相同的链式法则。
- en: 16.6.2 Calculating the partial derivatives for the last layer weights using
    the chain rule
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.6.2 使用链式法则计算最后一层权重的偏导数
- en: 'Let’s break it down into three steps to get from *w[ij]^L* to *C*. First, we
    can calculate the value to be passed into the sigmoid, which we called *z[j]^L*
    earlier in the chapter:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将它分解为三个步骤，从 *w[ij]^L* 到 *C*。首先，我们可以计算传递给sigmoid函数的值，我们之前在章节中称之为 *z[j]^L*：
- en: '![](../Images/CH16_F17_Orland_EQ18.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ18.png)'
- en: 'Then we can pass *z[j]^L* into the sigmoid function to get the activation *a[j]^L*
    :'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将 *z[j]^L* 传递给sigmoid函数以获得激活 *a[j]^L*：
- en: '![](../Images/CH16_F17_Orland_EQ19.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ19.png)'
- en: 'And finally, we can compute the cost:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以计算成本：
- en: '![](../Images/CH16_F17_Orland_EQ20.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ20.png)'
- en: To find the partial derivative of *C* with respect to *w[ij]^L* ,we multiply
    the derivatives of these three “composed” expressions together. The derivative
    of *z[j]^L* with respect to *one* particular *w[ij]^L* is the specific activation
    *a[j]^L* [−1] that it’s multiplied by. This is similar to the derivative of *y*(*x*)
    = *ax* with respect to *x,* which is the constant *a*. The partial derivative
    is
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到 *C* 对 *w[ij]^L* 的偏导数，我们需要将这些三个“复合”表达式的导数相乘。*z[j]^L* 对一个特定的 *w[ij]^L* 的导数是它乘以的特定激活
    *a[j]^L* [−1]。这与 *y*(*x*) = *ax* 对 *x* 的导数相似，其导数是常数 *a*。偏导数是
- en: '![](../Images/CH16_F17_Orland_EQ21.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ21.png)'
- en: The next step is applying the sigmoid function, so the derivative of *a[j]^L*
    with respect to *z[j]^L* is the derivative of σ. It turns out, and you can confirm
    this as an exercise, that the derivative of σ(*x*) is σ(*x*)(1 − σ(*x*)). This
    nice formula follows in part from the fact that *ex* is its own derivative. That
    gives us
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是应用sigmoid函数，所以 *a[j]^L* 对 *z[j]^L* 的导数是σ的导数。实际上，你可以作为一个练习来验证这一点，σ(*x*)的导数是σ(*x*)(1
    − σ(*x*))。这个漂亮的公式部分来自于 *ex* 是它自己的导数这一事实。这给了我们
- en: '![](../Images/CH16_F17_Orland_EQ22.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ22.png)'
- en: 'This is an ordinary derivative, not a partial derivative, because *a[j]^L*
    is a function of only one input: *z[j]^L*. Finally, we need the derivative of
    *C* with respect to *a[j]^L* . Only one term of the sum depends on *w[ij]^L* ,so
    we just need the derivative of (*a[j]^L* − *y[j]*)² with respect to *a[j]^L* .In
    this context, *y[j]* is a constant, so the derivative is 2*a[j]^L* . This comes
    from the power rule, telling us that if *f*(*x*) = *x*² , then *f*''(*x*) = 2*x*.
    For our last derivative, we need'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个普通导数，而不是偏导数，因为 *a[j]^L* 只是一个输入的函数：*z[j]^L*。最后，我们需要求 *C* 对 *a[j]^L* 的导数。求和中的只有一个项依赖于
    *w[ij]^L*，所以我们只需要求 (*a[j]^L* − *y[j]*)² 对 *a[j]^L* 的导数。在这个上下文中，*y[j]* 是一个常数，所以导数是
    2*a[j]^L*。这来自于幂规则，告诉我们如果 *f*(*x*) = *x*²，那么 *f*'(*x*) = 2*x*。对于我们的最后一个导数，我们需要
- en: '![](../Images/CH16_F17_Orland_EQ23.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ23.png)'
- en: 'The multivariable version of the chain rule says the following:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则的多变量版本如下所示：
- en: '![](../Images/CH16_F17_Orland_EQ24.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ24.png)'
- en: 'This looks a little bit different from the version we saw in chapter 10, which
    covered only composition of two functions of one variable. The principle is the
    same here though: with *C* written in terms of *a[j]^L* , *a[j]^L* written in
    terms of *z[j]^L*,and *z[j]^L* written in terms of *w[ij]^L* ,we have *C* written
    in terms of *w[ij]^L* .What the chain rule says is that to get the derivative
    of the whole chain, we multiply together the derivatives of each step. Plugging
    in the derivatives, the result is'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来与我们在第10章中看到的版本有点不同，当时只涉及一个变量的两个函数的复合。但原理在这里是相同的：将 *C* 用 *a[j]^L* 表示，将 *a[j]^L*
    用 *z[j]^L* 表示，将 *z[j]^L* 用 *w[ij]^L* 表示，我们就有 *C* 用 *w[ij]^L* 表示。链式法则告诉我们，要得到整个链的导数，我们需要将每一步的导数相乘。将导数代入，结果是
- en: '![](../Images/CH16_F17_Orland_EQ25.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH16_F17_Orland_EQ25.png)'
- en: This formula is one of the four we need to find the whole gradient of *C*. Specifically,
    this gives us the partial derivative for any weight in the last layer. There are
    16 × 10 of these, so we’ve covered 160 of the 1,210 total partial derivatives
    we need to have the complete gradient.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式是我们需要找到整个梯度*C*的四个公式之一。具体来说，这给出了最后一层中任何权重的偏导数。这里有16 × 10个这样的，所以我们已经覆盖了1,210个总偏导数中的160个。
- en: The reason I’ll stop here is because derivatives of other weights require more
    complicated applications of the chain rule. An activation influences every subsequent
    activation in the neural network, so every weight influences every subsequent
    activation. This isn’t beyond your capabilities, but I feel I’d owe you a better
    explanation of the multivariable chain rule before digging in. If you’re interested
    in going deeper, there are excellent resources online that walk through all the
    steps of backpropagation in gory detail. Otherwise, you can stay tuned for the
    (fingers crossed) sequel to this book. Thanks for reading!
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我会在这里停止，因为其他权重的导数需要更复杂的链式法则的应用。激活影响神经网络中的每个后续激活，所以每个权重影响每个后续激活。这并不超出你的能力范围，但我感觉在深入挖掘之前，我需要给你一个更好的多元链式法则的解释。如果你有兴趣深入了解，网上有很好的资源，详细介绍了反向传播的所有步骤。否则，你可以期待这本书的（
    fingers crossed）续集。感谢阅读！
- en: 16.6.3 Exercises
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.6.3 练习
- en: '| **Exercise 16.14-Mini Project**: Use SymPy or your own code from chapter
    10 to automatically find the derivative of the sigmoid function![](../Images/CH16_F17_Orland_EQ26.png)Show
    that the answer you get is equal to σ(*x*)(1 − σ(*x*)).**Solution**: In SymPy,
    we can quickly get a formula for the derivative:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '| **练习16.14-迷你项目**：使用SymPy或第10章中的代码来自动找到sigmoid函数的导数![图](../Images/CH16_F17_Orland_EQ26.png)证明你得到的答案是σ(*x*)(1
    − σ(*x*))。**解答**：在SymPy中，我们可以快速得到导数的公式：'
- en: '[PRE35]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In math notation, that’s![](../Images/CH16_F17_Orland_EQ27.png)The computation
    to show this expression equals σ(*x*)(1 − σ(*x*)) and requires a bit of rote algebra,
    but it’s worth it to convince yourself that this formula is valid. Multiplying
    the top and bottom by *e^x* and noting that *e^x* · *e*^(−*x*) = 1, we get![](../Images/CH16_F17_Orland_EQ28.png)
    |
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学符号表示，那就是![图](../Images/CH16_F17_Orland_EQ27.png)证明这个表达式等于σ(*x*)(1 − σ(*x*))需要一点死记硬背的代数，但这是值得的，以使你自己相信这个公式是有效的。将分子和分母都乘以*e^x*，并注意到*e^x*
    · *e*^(−*x*) = 1，我们得到![图](../Images/CH16_F17_Orland_EQ28.png) |
- en: Summary
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: An artificial neural network is a mathematical function whose computation mirrors
    the flow of signals in the human brain. As a function, it takes a vector as input
    and returns another vector as output.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络是一种数学函数，其计算过程与人类大脑中信号流动的流程相似。作为一个函数，它接受一个向量作为输入，并返回另一个向量作为输出。
- en: 'A neural network can be used to classify vector data: for instance, images
    converted to vectors of grayscale pixel values. The output of the neural network
    is a vector of numbers that express confidence that the input vector should be
    classified in any of the possible classes.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以用来对向量数据进行分类：例如，将图像转换为灰度像素值的向量。神经网络的输出是一个表示输入向量应该被分类到任何可能的类别中的置信度的数字向量。
- en: A multilayer perceptron (MLP) is a particular kind of artificial neural network
    consisting of several ordered layers of neurons, where the neurons in each layer
    are connected to and influenced by the neurons in the previous layer. During evaluation
    of the neural network, each neuron gets a number that is its activation. You can
    think of an activation as an intermediate yes-or-no answer on the way to solving
    the classification problem.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）是一种特殊的人工神经网络，由几个有序的神经元层组成，其中每一层的神经元都与前一层的神经元相连并受到其影响。在评估神经网络时，每个神经元都会得到一个表示其激活的数字。你可以将激活看作是在解决分类问题过程中，对中间是或否答案的肯定。
- en: To evaluate a neural network, the activations of the first layer of neurons
    are set to the entries of the input vector. Each subsequent layer of activations
    is calculated as a function of the previous layer. The final layer of activations
    is treated as a vector and returned as the result vector of the calculation.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了评估一个神经网络，神经元第一层的激活被设置为输入向量的条目。每个后续层的激活都是作为前一层的一个函数来计算的。最后一层的激活被当作一个向量并作为计算的结果向量返回。
- en: The activation of a neuron is based on a linear combination of the activations
    of all neurons in the previous layer. The coefficients in the linear combination
    are called *weights*. Each neuron also has a *bias*, a number which is added to
    the linear combination. This value is passed through a sigmoid function to get
    the activation function.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元的激活基于前一层中所有神经元的激活的线性组合。线性组合中的系数被称为*权重*。每个神经元还有一个*偏差*，这是一个加到线性组合中的数字。这个值通过sigmoid函数传递以获得激活函数。
- en: Training a neural network means tuning the values of all of the weights and
    biases so that it performs its task optimally. To do this, you can measure the
    error of the neural network’s predictions relative to actual answers from a training
    data set with a cost function. With a fixed training data set, the cost function
    depends only on the weights and biases.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络意味着调整所有权重和偏差的值，以便它能够最优地执行其任务。为此，你可以使用成本函数来衡量神经网络预测的错误相对于训练数据集中实际答案的错误。在固定的训练数据集下，成本函数只依赖于权重和偏差。
- en: Gradient descent allows us to find the values of weights and biases that minimize
    the cost function and yield the best neural network.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降法使我们能够找到权重和偏差的值，以最小化成本函数并得到最佳的神经网络。
- en: Neural networks can be trained efficiently because there are simple, exact formulas
    for the partial derivatives of the cost function with respect to the weights and
    biases. These are found using an algorithm called *backpropagation*, which in
    turn makes use of the chain rule from calculus.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以高效地训练，因为成本函数关于权重和偏差的偏导数有简单的、精确的公式。这些公式是通过一种称为*反向传播*的算法找到的，该算法反过来又利用了微积分中的链式法则。
- en: Python’s scikit-learn library has a built in `MLPClassifer` class that can automatically
    be trained on classified vector data.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python的scikit-learn库有一个内置的`MLPClassifer`类，它可以自动对分类向量数据进行训练。
