- en: appendix D. Writing and managing application logs with Docker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录D. 使用Docker编写和管理应用程序日志
- en: 'Logging is usually the most boring part of learning a new technology, but not
    so with Docker. The basic principle is simple: you need to make sure your application
    logs are being written to the standard output stream, because that’s where Docker
    looks for them. There are a couple of ways to achieve that, which we’ll cover
    in this chapter, and then the fun begins. Docker has a pluggable logging framework—you
    need to make sure your application logs are coming out from the container, and
    then Docker can send them to different places. That lets you build a powerful
    logging model, where the application logs from all your containers are sent to
    a central log store with a searchable UI on top of it—all using open source components,
    all running in containers.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志通常是学习新技术中最无聊的部分，但Docker不是这样。基本原理很简单：你需要确保你的应用程序日志被写入标准输出流，因为那是Docker寻找它们的地方。有几个方法可以实现这一点，我们将在本章中介绍，然后乐趣就开始了。Docker有一个可插拔的日志框架——你需要确保你的应用程序日志从容器中输出，然后Docker可以将它们发送到不同的地方。这让你可以构建一个强大的日志模型，其中所有容器的应用程序日志都被发送到一个中央日志存储，并在其上方有一个可搜索的用户界面——所有这些都使用开源组件，所有都在容器中运行。
- en: This appendix is reproduced from chapter 19, "Writing and Managing Application
    Logs with Docker," from *Learn Docker in a Month of Lunches* by Elton Stoneman
    (Manning, 2020). Any chapter references or references to code repositories refer
    to the chapters or code repositories of that book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录摘自第19章，“使用Docker编写和管理应用程序日志”，来自Elton Stoneman的《一个月午餐时间学习Docker》（Manning，2020）。任何章节引用或对代码仓库的引用都指该书的相关章节或代码仓库。
- en: D.1 Welcome to stderr and stdout!
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.1 欢迎来到stderr和stdout！
- en: A Docker image is the snapshot of a filesystem with all your application binaries
    and dependencies, and also some metadata telling Docker which process to start
    when you run a container from the image. That process runs in the foreground,
    so it’s like starting a shell session and then running a command. As long as the
    command is active, it has control of the terminal input and output. Commands write
    log entries to the standard output and standard error streams (called *stdout*
    and *stderr*), so in a terminal session you see the output in your window. In
    a container, Docker watches stdout and stderr and collects the output from the
    streams—that’s the source of the container logs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Docker镜像是你应用程序的二进制文件和依赖项的文件系统快照，同时也包含一些元数据，告诉Docker当你从镜像运行容器时应该启动哪个进程。该进程在前景运行，所以就像启动一个shell会话然后运行一个命令。只要命令是活跃的，它就控制着终端的输入和输出。命令将日志条目写入标准输出和标准错误流（称为*stdout*和*stderr*），所以在终端会话中你会在你的窗口中看到输出。在容器中，Docker会监视stdout和stderr，并从流中收集输出——这就是容器日志的来源。
- en: 'try it now You can see this easily if you run a simple timecheck app in a container.
    The application itself runs in the foreground and writes log entries to stdout:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看！如果你在容器中运行一个简单的timecheck应用，你可以轻松地看到这一点。应用程序本身在前台运行，并将日志条目写入stdout：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll see some log lines in your terminal, and you’ll find you can’t enter
    any more commands—the container is running in the foreground, so it’s just like
    running the app itself in your terminal. Every few seconds the app writes another
    timestamp to stdout, so you’ll see another line in your session window. My output
    is in figure D.1.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在你的终端中看到一些日志行，你会发现你不能再输入任何命令——容器正在前台运行，所以就像在你的终端中运行应用程序本身一样。每隔几秒钟应用程序就会向stdout写入另一个时间戳，所以你会在你的会话窗口中看到另一行。我的输出在图D.1中。
- en: '![](../Images/D-1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/D-1.jpg)'
- en: Figure D.1 A container in the foreground takes over the terminal session until
    it exits.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.1 前台的容器接管终端会话，直到它退出。
- en: 'This is the standard operating model for containers—Docker starts a process
    inside the container and collects the output streams from that process as logs.
    All the apps we’ve used in this book follow this same pattern: the application
    process runs in the foreground—that could be a Go binary or the Java runtime—and
    the application itself is configured to write logs to stdout (or stderr; Docker
    treats both streams in the same way). Those application logs are written to the
    output stream by the runtime, and Docker collects them. Figure D.2 shows the interaction
    between the application, the output streams, and Docker.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是容器的标准操作模型——Docker在容器内启动一个进程，并收集该进程的输出流作为日志。本书中我们使用的所有应用程序都遵循相同的模式：应用程序进程在前台运行——这可能是一个Go可执行文件或Java运行时——应用程序本身配置为将日志写入stdout（或stderr；Docker以相同的方式处理这两个流）。这些应用程序日志由运行时写入输出流，并由Docker收集。图D.2显示了应用程序、输出流和Docker之间的交互。
- en: '![](../Images/D-2.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/D-2.jpg)'
- en: Figure D.2 Docker watches the application process in the container and collects
    its output streams.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.2 Docker监视容器中的应用程序进程，并收集其输出流。
- en: Container logs are stored as JSON files, so the log entries remain available
    for detached containers which don’t have a terminal session, and for containers
    that have exited so there is no application process. Docker manages the JSON files
    for you and they have the same life cycle as the container—when the container
    is removed, the log files are removed too.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 容器日志以JSON文件的形式存储，因此即使没有终端会话的分离容器，或者已经退出的容器（没有应用程序进程），日志条目仍然可用。Docker会为您管理这些JSON文件，它们的生命周期与容器相同——当容器被移除时，日志文件也会被移除。
- en: 'try it now Run a container from the same image in the background as a detached
    container, and check the logs and then the path to the log file:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：在后台以分离容器的方式运行与同一镜像的容器，然后检查日志以及日志文件路径：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you’re using Docker Desktop with Linux containers, remember that the Docker
    Engine is running inside a VM that Docker manages for you—you can see the path
    to the log file for the container, but you don’t have access to the VM, so you
    can’t read the file directly. If you’re running Docker CE on Linux or you’re using
    Windows containers, the path to the log file will be on your local machine, and
    you can open the file to see the raw contents. You can see my output (using Windows
    containers) in figure D.3.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是带有Linux容器的Docker Desktop，请记住Docker Engine是在Docker为您管理的虚拟机（VM）中运行的——你可以看到容器日志文件的路径，但你无法访问VM，因此无法直接读取文件。如果你在Linux上运行Docker
    CE或使用Windows容器，日志文件的路径将在你的本地机器上，你可以打开文件以查看原始内容。你可以在图D.3中看到我的输出（使用Windows容器）。
- en: '![](../Images/D-3.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/D-3.jpg)'
- en: Figure D.3 Docker stores container logs in a JSON file and manages the lifetime
    of that file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.3 Docker将容器日志存储在JSON文件中，并管理该文件的生命周期。
- en: The log file is really just an implementation detail that you don’t usually
    need to worry about. The format is very simple; it contains a JSON object for
    each log entry with the string containing the log, the name of the stream where
    the log came from (stdout or stderr), and a timestamp. Listing D.1 shows a sample
    of the logs for my timecheck container.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 日志文件实际上只是一个实现细节，你通常不需要担心。其格式非常简单；它包含一个JSON对象，每个日志条目都有一个包含日志的字符串、日志来源的流名称（stdout或stderr）和一个时间戳。列表D.1显示了timecheck容器日志的示例。
- en: Listing D.1 The raw format for container logs is a simple JSON object
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 列表D.1 容器日志的原始格式是一个简单的JSON对象
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The only time you will need to think about the JSON is if you have a container
    that produces lots of logs, and you want to keep all the log entries for a period
    but have them in a manageable file structure. Docker creates a single JSON log
    file for each container by default, and will let it grow to any size (until it
    fills up your disk). You can configure Docker to use rolling files instead, with
    a maximum size limit, so that when the log file fills up, Docker starts writing
    to a new file. You also configure how many log files to use, and when they’re
    all full, Docker starts overwriting the first file. You can set those options
    at the Docker Engine level so the changes apply to every container, or you can
    set them for individual containers. Configuring logging options for a specific
    container is a good way to get small, rotated log files for one application but
    keep all the logs for other containers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您唯一需要考虑JSON的情况是，如果您有一个产生大量日志的容器，并且您希望保留所有日志条目一段时间，但希望它们在一个可管理的文件结构中。默认情况下，Docker为每个容器创建一个单一的JSON日志文件，并允许它增长到任何大小（直到填满您的磁盘）。您可以配置Docker使用滚动文件，并设置最大大小限制，这样当日志文件填满时，Docker开始写入新文件。您还可以配置要使用多少个日志文件，当它们都满了之后，Docker开始覆盖第一个文件。您可以在Docker引擎级别设置这些选项，以便更改适用于每个容器，或者您可以为单个容器设置它们。为特定容器配置日志选项是获取一个小型轮换日志文件的一种好方法，但可以保留其他容器的所有日志。
- en: 'try it now Run the same app again, but this time specifying log options to
    use three rolling log files with a maximum of 5 KB each:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：再次运行相同的应用程序，但这次指定使用三个滚动日志文件，每个文件最大5 KB：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You’ll see that the log path for the container is still just a single JSON file,
    but Docker is actually rotating log files using that name as the base but with
    a suffix for the log file number. If you’re running Windows containers or Docker
    CE on Linux, you can list the contents of the directory where the logs are kept
    and you’ll see those file suffixes. Mine are shown in figure D.4.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您会看到容器的日志路径仍然只是一个单一的JSON文件，但Docker实际上正在使用该名称作为基础，但带有日志文件编号后缀来轮换日志文件。如果您正在运行Windows容器或在Linux上运行Docker
    CE，您可以列出存储日志的目录的内容，您将看到那些文件后缀。我的在图D.4中显示。
- en: '![](../Images/D-4.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图D-4](../Images/D-4.jpg)'
- en: Figure D.4 Rolling log files let you keep a known amount of log data per container.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.4 滚动日志文件允许您为每个容器保留已知数量的日志数据。
- en: There’s a collection and processing stage for the application logs coming from
    stdout, which is where you can configure what Docker does with the logs. In the
    last exercise we configured the log processing to control the JSON file structure,
    and there’s much more you can do with container logs. To take full advantage of
    that, you need to make sure every app is pushing logs out of the container, and
    in some cases that takes a bit more work.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自stdout的应用程序日志有一个收集和处理阶段，您可以在其中配置Docker如何处理日志。在上一个练习中，我们配置了日志处理以控制JSON文件结构，并且您可以使用容器日志做更多的事情。为了充分利用这一点，您需要确保每个应用程序都将日志推送到容器外，在某些情况下这可能需要更多的工作。
- en: D.2 Relaying logs from other sinks to stdout
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.2 从其他sinks中转发日志到stdout
- en: Not every app fits nicely with the standard logging model; when you containerize
    some apps, Docker won’t see any logs in the output streams. Some applications
    run in the background as Windows Services or Linux daemons, so the container startup
    process isn’t actually the application process. Other apps might use an existing
    logging framework that writes to log files or other locations (called *sinks*
    in the logging world), like syslog in Linux or the Windows Event Log. Either way,
    there are no application logs coming from the container start process, so Docker
    doesn’t see any logs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个应用程序都与标准日志模型完美匹配；当您将某些应用程序容器化时，Docker在输出流中看不到任何日志。一些应用程序作为Windows服务或Linux守护进程在后台运行，因此容器启动过程实际上不是应用程序过程。其他应用程序可能使用现有的日志框架，将日志写入日志文件或其他位置（在日志世界中称为*sinks*），如Linux中的syslog或Windows事件日志。无论如何，容器启动过程中没有来自应用程序的日志，因此Docker看不到任何日志。
- en: 'Try it Now There’s a new version of the timecheck app for this chapter that
    writes logs to a file instead of stdout. When you run this version, there are
    no container logs, although the app logs are being stored in the container filesystem:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：本章有一个新的timecheck应用程序版本，它将日志写入文件而不是stdout。当您运行此版本时，没有容器日志，尽管应用程序日志正在存储在容器文件系统中：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You’ll see that there are no container logs, even though the application itself
    is writing lots of log entries. My output is in figure D.5—I need to connect to
    the container and read the log file from the container filesystem to see the log
    entries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管应用程序本身写入了很多日志条目，但你不会看到任何容器日志。我的输出在图D.5中——我需要连接到容器并从容器文件系统中读取日志文件以查看日志条目。
- en: '![](../Images/D-5.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图D.5](../Images/D-5.jpg)'
- en: Figure D.5 If the app doesn’t write anything to the output streams, you won’t
    see any container logs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.5 如果应用程序没有写入任何内容到输出流，你将看不到任何容器日志。
- en: This happens because the app is using its own log sink—a file in this exercise—and
    Docker doesn’t know anything about that sink. Docker will only read logs from
    stdout; there’s no way to configure it to read from a different log sink inside
    the container.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为应用正在使用自己的日志接收器——在这个练习中是一个文件，而Docker对此接收器一无所知。Docker只会从stdout读取日志；没有方法可以配置它从容器内的不同日志接收器读取。
- en: The pattern for dealing with apps like this is to run a second process in the
    container startup command, which reads the log entries from the sink that the
    application uses and writes them to stdout. That process could be a shell script
    or a simple utility app, and it is the final process in the start sequence, so
    Docker reads its output stream and the application logs get relayed as container
    logs. Figure D.6 shows how that works.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 处理此类应用的模式是在容器启动命令中运行第二个进程，该进程从应用程序使用的接收器读取日志条目并将它们写入stdout。这个过程可以是shell脚本或简单的实用应用，并且它是启动序列中的最后一个进程，因此Docker读取其输出流，应用程序日志作为容器日志被传递。图D.6显示了它是如何工作的。
- en: '![](../Images/D-6.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图D.6](../Images/D-6.jpg)'
- en: Figure D.6 You need to package a utility in your container image to relay logs
    from a file.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.6 你需要在容器镜像中打包一个实用工具来从文件中传递日志。
- en: 'This is not a perfect solution. Your utility process is running in the foreground,
    so it needs to be robust because if it fails, your container exits, even if the
    actual application is still working in the background. And the reverse is true:
    if the application fails but the log relay keeps running, your container stays
    up even though the app is no longer working. You need health checks in your image
    to prevent that from happening. And lastly, this is not an efficient use of disk,
    especially if your app writes a lot of logs—they’ll be filling up a file in the
    container filesystem and filling up a JSON file on the Docker host machine.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个完美的解决方案。您的实用进程正在前台运行，因此它需要健壮，因为如果它失败，容器会退出，即使实际的应用程序仍在后台工作。反之亦然：如果应用程序失败但日志中继仍在运行，容器会保持运行，尽管应用程序已经不再工作。您需要在镜像中添加健康检查以防止这种情况发生。最后，这并不是对磁盘的高效使用，特别是如果您的应用程序写入大量日志——它们会在容器文件系统中填充一个文件，并在Docker主机机器上填充一个JSON文件。
- en: Even so, it’s a useful pattern to know about. If your app runs in the foreground,
    and you can tweak your config to write logs to stdout instead, that’s a better
    approach. But if your app runs in the background, there’s no other option, and
    it’s better to accept the inefficiency and have your app behave like all other
    containers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 即使如此，了解这个模式也是有用的。如果你的应用在前台运行，并且你可以调整配置将日志写入stdout，那么这是一种更好的方法。但如果你的应用在后台运行，就没有其他选择了，最好是接受低效并让应用像所有其他容器一样运行。
- en: There’s an update for the timecheck app in this chapter that adds this pattern,
    building a small utility app to watch the log file and relay the lines to stdout.
    Listing D.2 shows the final stages of the multi-stage Dockerfile—there are different
    startup commands for Linux and Windows.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本章对timecheck应用进行了更新，添加了此模式，构建了一个小型实用应用来监视日志文件并将行传递到stdout。列表D.2显示了多阶段Dockerfile的最终阶段——Linux和Windows有不同的启动命令。
- en: Listing D.2 Building and packaging a log-relay utility with your app
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表D.2 使用您的应用构建和打包日志中继实用工具
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The two `CMD` instructions achieve the same thing, using different approaches
    for the two operating systems. First the .NET application process is started in
    the background, using the `start` command in Windows and suffixing the command
    with a single ampersand `&` in Linux. Then the .NET tail utility is started, configured
    to read the log file the application writes to. The tail utility just watches
    that file and relays each new line as it gets written, so the logs get surfaced
    to stdout and become container logs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个`CMD`指令实现了相同的功能，但使用了两种不同的操作系统方法。首先，在Windows中使用`start`命令在后台启动.NET应用程序进程，在Linux中在命令后添加单个与号`&`。然后启动.NET
    tail实用程序，配置为读取应用程序写入的日志文件。tail实用程序只是监视该文件，并将新写入的每一行转发，因此日志被暴露到stdout并成为容器日志。
- en: 'Try it now Run a container from the new image, and verify that logs are coming
    from the container and that they still get written in the filesystem:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 运行新镜像的容器，并验证日志是否来自容器，并且它们仍然被写入文件系统：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now the logs are coming from the container. It’s a convoluted approach to get
    there, with an extra process running to relay the log file contents to stdout,
    but once the container is running, that’s all transparent. The downside to this
    approach is the extra processing power used by the log relay and the extra disk
    space for storing the logs twice. You can see my output in figure D.7, which shows
    the log file is still there in the container filesystem.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在日志来自容器。这是一个复杂的方法来达到这个目的，需要额外运行一个进程来将日志文件内容转发到标准输出（stdout），但一旦容器运行起来，这一切都是透明的。这种方法的缺点是日志转发使用了额外的处理能力，并且需要额外的磁盘空间来存储两次日志。您可以在图D.7中看到我的输出，它显示了日志文件仍然存在于容器文件系统中。
- en: '![](../Images/D-7.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/D-7.jpg)'
- en: Figure D.7 A log relay utility gets the application logs out to Docker, but
    uses twice as much disk space.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.7 一个日志转发实用程序将应用程序日志输出到Docker，但使用了两倍的磁盘空间。
- en: I use a custom utility to relay the log entries in this example, because I want
    the app to work across platforms. I could use the standard Linux `tail` command
    instead, but there’s no Windows equivalent. The custom utility approach is also
    more flexible, because it could read from any sink and relay to stdout. That should
    cover any scenario where your application logs are locked away somewhere in the
    container that Docker doesn’t see.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我使用了一个自定义的实用程序来转发日志条目，因为我希望应用程序能够在多个平台上工作。我可以用标准的Linux `tail`命令代替，但在Windows中没有等效的命令。自定义实用程序方法也更加灵活，因为它可以从任何接收器读取并将数据转发到stdout。这应该涵盖了任何场景，其中您的应用程序日志被锁定在容器中的某个地方，而Docker无法看到。
- en: When you have all your container images set up to write application logs as
    container logs, you can start to make use of Docker’s pluggable logging system
    and consolidate all the logs coming from all your containers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将所有容器镜像设置为将应用程序日志作为容器日志写入时，您就可以开始利用Docker的可插拔日志系统，并整合来自所有容器的所有日志。
- en: D.3 Collecting and forwarding container logs
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.3 收集和转发容器日志
- en: 'Docker adds a consistent management layer over all your apps—it doesn’t matter
    what’s happening inside the container; you start, stop, and inspect everything
    in the same way. That’s especially useful with logs when you bring a consolidated
    logging system into your architecture, and we’ll walk through one of the most
    popular open source examples of that: Fluentd.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Docker在所有应用程序上添加了一个一致的管理层——无论容器内部发生什么；您以相同的方式启动、停止和检查一切。当您将集中式日志系统引入架构时，这尤其有用，我们将通过最流行的开源示例之一：Fluentd，来了解这一点。
- en: Fluentd is a unified logging layer. It can ingest logs from lots of different
    sources, filter or enrich the log entries, and then forward them on to lots of
    different targets. It’s a project managed by the Cloud Native Computing Foundation
    (which also manages Kubernetes, Prometheus, and the container runtime from Docker,
    among other projects), and it’s a mature and hugely flexible system. You can run
    Fluentd in a container, and it will listen for log entries. Then you can run other
    containers that use Docker’s Fluentd logging driver instead of the standard JSON
    file, and those container logs will be sent to Fluentd.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd是一个统一的日志层。它可以从许多不同的来源摄取日志，过滤或丰富日志条目，然后将它们转发到许多不同的目标。它是由云原生计算基金会（它还管理Kubernetes、Prometheus以及Docker的容器运行时等项目）管理的项目，它是一个成熟且高度灵活的系统。您可以在容器中运行Fluentd，它将监听日志条目。然后您可以运行其他容器，这些容器使用Docker的Fluentd日志驱动程序而不是标准的JSON文件，这些容器日志将被发送到Fluentd。
- en: 'Try it now Fluentd uses a config file to process logs. Run a container with
    a simple configuration that will have Fluentd collect logs and echo them to stdout
    in the container. Then run the timecheck app with that container sending logs
    to Fluentd:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试吧 Fluentd 使用配置文件来处理日志。运行一个具有简单配置的容器，该配置将使 Fluentd 收集日志并将它们回显到容器中的 stdout。然后运行带有该容器发送日志到
    Fluentd 的 timecheck 应用程序：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You’ll see that you get an error when you try to check logs from the timecheck
    container—not all logging drivers let you see the log entries directly from the
    container. In this exercise they’re being collected by Fluentd, and this configuration
    writes the output to stdout, so you can see the timecheck container’s logs by
    looking at the logs from Fluentd. My output is in figure D.8.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当您尝试检查来自 timecheck 容器的日志时，您会看到一个错误——并不是所有的日志驱动程序都允许您直接从容器中查看日志条目。在这个练习中，它们被
    Fluentd 收集，并且这个配置将输出写入 stdout，因此您可以通过查看 Fluentd 的日志来查看 timecheck 容器的日志。我的输出如图
    D.8 所示。
- en: '![](../Images/D-8.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/D-8.jpg)'
- en: Figure D.8 Fluentd collects logs from other containers, and it can store them
    or write to stdout.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.8 Fluentd 从其他容器收集日志，并且它可以存储它们或将它们写入 stdout。
- en: Fluentd adds its own metadata to each record when it stores logs, including
    the container ID and name. This is necessary because Fluentd becomes the central
    log collector for all your containers, and you need to be able to identify which
    log entries came from which application. Using stdout as a target for Fluentd
    is just a simple way to see how everything works. Typically you’d forward logs
    to a central data store. Elasticsearch is a very popular option—it’s a no-SQL
    document database that works well for logs. You can run Elasticsearch in a container
    for log storage and the companion app Kibana, which is a search UI, in another
    container. Figure D.9 shows what the logging model looks like.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Fluentd 存储日志时，它会为每条记录添加自己的元数据，包括容器 ID 和名称。这是必要的，因为 Fluentd 成为您所有容器的中央日志收集器，您需要能够识别哪些日志条目来自哪个应用程序。将
    stdout 作为 Fluentd 的目标只是一个简单的查看一切如何工作的方法。通常，您会将日志转发到中央数据存储。Elasticsearch 是一个非常受欢迎的选择——它是一个适用于日志的无
    SQL 文档数据库。您可以在容器中运行 Elasticsearch 以存储日志，并在另一个容器中运行配套的搜索 UI Kibana。图 D.9 显示了日志模型的外观。
- en: '![](../Images/D-9.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/D-9.jpg)'
- en: Figure D.9 A centralized logging model sends all container logs to Fluentd for
    processing and storage.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.9 集中式日志模型将所有容器日志发送到 Fluentd 进行处理和存储。
- en: It looks like a complicated architecture, but as always with Docker, it’s very
    easy to specify all the parts of your logging setup in a Docker Compose file and
    spin up the whole stack with one command. When you have your logging infrastructure
    running in containers, you just need to use the Fluentd logging driver for any
    container where you want to opt in to centralized logging.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来很复杂，但像 Docker 一样，始终很容易在 Docker Compose 文件中指定所有日志设置的部分，并使用一条命令启动整个堆栈。当您的日志基础设施在容器中运行时，您只需为任何希望加入集中式日志的容器使用
    Fluentd 日志驱动程序即可。
- en: 'Try it now Remove any running containers and start the Fluentd-Elasticsearch-Kibana
    logging containers. Then run a timecheck container using the Fluentd logging driver:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试吧 删除任何正在运行的容器，并启动 Fluentd-Elasticsearch-Kibana 日志容器。然后使用 Fluentd 日志驱动程序运行一个
    timecheck 容器：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Give Elasticsearch a couple of minutes to be ready, and then browse to Kibana
    at http://localhost:5601\. Click the Discover tab, and Kibana will ask for the
    name of the document collection to search against. Enter `fluentd*` as in figure
    D.10.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 给 Elasticsearch 几分钟的时间准备，然后浏览到 http://localhost:5601。点击 Discover 选项卡，Kibana
    将要求输入要搜索的文档集合的名称。输入 `fluentd*`，如图 D.10 所示。
- en: '![](../Images/D-10.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/D-10.jpg)'
- en: Figure D.10 Elasticsearch stores documents in collections called indexes-Fluentd
    uses its own index.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.10 Elasticsearch 将文档存储在名为 indexes 的集合中——Fluentd 使用它自己的索引。
- en: In the next screen you need to set the field that contains the time filter—select
    `@timestamp` as in figure D.11.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一屏幕中，您需要设置包含时间过滤器的字段——选择 `@timestamp`，如图 D.11 所示。
- en: '![](../Images/D-11.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/D-11.jpg)'
- en: Figure D.11 Fluentd has already saved data in Elasticsearch, so Kibana can see
    the field names.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.11 Fluentd 已经将数据保存到 Elasticsearch 中，因此 Kibana 可以看到字段名称。
- en: You can automate the Kibana setup, but I haven’t because if you’re new to the
    Elasticsearch stack, it’s worth stepping through it to see how the pieces fit
    together. Every log entry Fluentd collects is saved as a document in Elasticsearch,
    in a document collection that’s named `fluentd-{date}`. Kibana gives you a view
    over all those documents—in the default Discover tab you’ll see a bar chart showing
    how many documents are being created over time, and you can drill into the details
    for individual documents. In this exercise, each document is a log entry from
    the timecheck app. You can see the data in Kibana in figure D.12.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自动化 Kibana 的设置，但我还没有这么做，因为如果你是 Elasticsearch 堆栈的新手，逐步操作以了解各个组件如何组合在一起是值得的。Fluentd
    收集的每条日志条目都保存为 Elasticsearch 中的一个文档，在一个名为 `fluentd-{date}` 的文档集中。Kibana 让你可以查看所有这些文档——在默认的
    Discover 选项卡中，你会看到一个柱状图显示随时间创建的文档数量，并且你可以深入查看单个文档的详细信息。在这个练习中，每个文档都是来自 timecheck
    应用的日志条目。你可以在图 D.12 中看到 Kibana 中的数据。
- en: '![](../Images/D-12.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/D-12.jpg)'
- en: Figure D.12 The EFK stack in all its glory—container logs collected and stored
    for simple searching
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.12 EFK 堆栈的全貌——收集并存储的容器日志，便于简单搜索
- en: Kibana lets you search across all documents for a specific piece of text, or
    filter documents by date or another data attribute. It also has dashboard functionality
    similar to Grafana, so you can build charts showing counts of logs per app, or
    counts of error logs. Elasticsearch is hugely scalable, so it’s suitable for large
    quantities of data in production, and when you start sending it all your container
    logs via Fluentd, you’ll soon find it’s a much more manageable approach than scrolling
    through log lines in the console.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 允许你在所有文档中搜索特定的文本片段，或按日期或其他数据属性过滤文档。它还具有类似于 Grafana 的仪表板功能，因此你可以构建显示每个应用的日志计数或错误日志计数的图表。Elasticsearch
    具有巨大的可扩展性，因此适用于生产中的大量数据，当你开始通过 Fluentd 发送所有容器的日志时，你很快会发现这比在控制台中滚动日志行要容易管理得多。
- en: 'Try it now Run the image gallery app with each component configured to use
    the Fluentd logging driver:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看：运行带有每个组件配置为使用 Fluentd 日志驱动的图像库应用：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Browse to http://localhost:8010 to generate some traffic, and the containers
    will start writing logs. The Fluentd setup for the image gallery app adds a tag
    to each log, identifying the component that generated it, so log lines can easily
    be identified—more easily than using the container name or container ID. You can
    see my output in figure D.13\. I’m running the full image gallery application,
    but I’m filtering the logs in Kibana to only show the access-log component—the
    API that records when the app is accessed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览到 http://localhost:8010 生成一些流量，容器将开始写入日志。图像库应用的 Fluentd 设置为每条日志添加一个标签，标识生成它的组件，因此日志行可以轻松识别——比使用容器名称或容器
    ID 更容易识别。你可以在图 D.13 中看到我的输出。我正在运行完整的图像库应用，但我正在 Kibana 中过滤日志，只显示 access-log 组件——记录应用访问时间的
    API。
- en: '![](../Images/D-13.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/D-13.jpg)'
- en: Figure D.13 Logs are being collected in Elasticsearch for the image gallery
    and the timecheck container.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.13 图像库和时间检查容器正在 Elasticsearch 中收集日志。
- en: It’s very easy to add a tag for Fluentd that shows up as the `log_name` field
    for filtering; it’s an option for the logging driver. You can use a fixed name
    or inject some useful identifiers—in this exercise I use `gallery` as the application
    prefix and then add the component name and image name for the container generating
    the logs. That’s a nice way to identify the application, component, and exact
    version running for each log line. Listing D.3 shows the logging options in the
    Docker Compose file for the image gallery app.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Fluentd 添加一个标签非常容易，它会显示为 `log_name` 字段用于过滤；这是日志驱动程序的一个选项。你可以使用一个固定的名称或注入一些有用的标识符——在这个练习中，我使用
    `gallery` 作为应用前缀，然后添加生成日志的容器的组件名称和镜像名称。这是一种很好的方式来识别应用程序、组件以及每个日志行的确切版本。列表 D.3
    显示了图像库应用的 Docker Compose 文件中的日志选项。
- en: Listing D.3 Using a tag to identify the source of log entries for Fluentd
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 D.3 使用标签识别 Fluentd 日志条目的来源
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The model for centralized logging with a searchable data store and a user-friendly
    UI is one you should definitely consider when you’re getting containers ready
    for production. You’re not limited to using Fluentd—there are many other logging
    drivers for Docker, so you could use other popular tools like Graylog, or commercial
    tools like Splunk. Remember, you can set the default logging driver and options
    at the Engine level in the Docker config, but I think there’s value in doing it
    in the application manifests instead—it makes it clear which logging system you’re
    using in each environment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当您为生产准备容器时，中央日志记录模型，包括可搜索的数据存储和用户友好的UI，是一个您绝对应该考虑的模型。您不仅限于使用Fluentd——Docker有许多其他日志驱动程序，因此您可以使用其他流行的工具，如Graylog，或商业工具，如Splunk。记住，您可以在Docker配置的引擎级别设置默认日志驱动程序和选项，但我认为在应用程序清单中这样做更有价值——它清楚地说明了每个环境中使用的日志系统。
- en: Fluentd is a good option if you don’t already have an established logging system.
    It’s easy to use and it scales from a single dev machine to a full production
    cluster, and you use it in the same way in every environment. You can also configure
    Fluentd to enrich the log data to make it easier to work with, and to filter logs
    and send them to different targets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有建立日志系统，Fluentd是一个很好的选择。它易于使用，可以从单个开发机器扩展到完整的生产集群，并且您可以在每个环境中以相同的方式使用它。您还可以配置Fluentd以丰富日志数据，使其更容易处理，并过滤日志将它们发送到不同的目标。
- en: D.4 Managing your log output and collection
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.4 管理您的日志输出和收集
- en: Logging is a delicate balance between capturing enough information to be useful
    in diagnosing problems and not storing huge quantities of data. Docker’s logging
    model gives you some additional flexibility to help with the balance, because
    you can produce container logs at a more verbose level than you expect to use,
    but filter them out before you store them. Then if you need to see more verbose
    logs, you can alter the filter configuration rather than your app configuration
    so the Fluentd containers get replaced rather than your app containers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志需要在捕获足够信息以用于诊断问题和不过度存储大量数据之间取得微妙的平衡。Docker的日志模型为您提供了额外的灵活性，以帮助实现这种平衡，因为您可以在存储之前以比预期更冗长的级别生成容器日志，但过滤掉它们。然后，如果您需要查看更冗长的日志，您可以通过更改过滤配置而不是应用程序配置来替换Fluentd容器，而不是应用程序容器。
- en: You can configure this level of filtering in the Fluentd config file. The configuration
    from the last exercise sends all logs to Elasticsearch, but the updated configuration
    in Listing D.4 filters out logs from the more verbose access-log component. Those
    logs go to stdout, and the rest of the app logs go to Elasticsearch.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Fluentd配置文件中配置此级别的过滤。上一练习中的配置将所有日志发送到Elasticsearch，但D.4列表中更新的配置过滤掉了来自更冗长的access-log组件的日志。这些日志将发送到stdout，其余的应用程序日志将发送到Elasticsearch。
- en: Listing D.4 Sending log entries to different targets based on the tag of the
    record
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表D.4 根据记录的标签将日志条目发送到不同的目标
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `match` blocks tell Fluentd what to do with log records, and the filter
    parameter uses the tag that is set in the logging driver options. When you run
    this updated configuration, the access-log entries will match the first match
    block, because the tag prefix is `gallery.access-log`. Those records will stop
    surfacing in Elasticsearch and will only be available by reading the logs of the
    Fluentd container. The updated config file also enriches all log entries, splitting
    the tag into separate fields for app name, service name, and image name, which
    makes filtering in Kibana much easier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`match`块告诉Fluentd如何处理日志记录，而过滤参数使用在日志驱动程序选项中设置的标签。当您运行此更新后的配置时，access-log条目将匹配第一个match块，因为标签前缀是`gallery.access-log`。这些记录将不再在Elasticsearch中显示，并且只能通过读取Fluentd容器的日志来获取。更新的配置文件还丰富了所有日志条目，将标签拆分为应用程序名称、服务名称和镜像名称的单独字段，这使得在Kibana中的过滤变得更加容易。'
- en: 'Try it now Update the Fluentd configuration by deploying a Docker Compose override
    file that specifies a new config file, and update the image gallery application
    to generate more verbose logs:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 更新Fluentd配置，通过部署一个指定新配置文件的Docker Compose覆盖文件，并更新图像库应用程序以生成更冗长的日志：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can check the contents of those override files, and you’ll see they just
    specify config settings for the applications; all the images are the same. Now
    when you use the app at http://localhost:8010, the access-log entries are still
    generated, but they get filtered out by Fluentd so you won’t see any new logs
    in Kibana. You will see the logs from the other components, and these are enriched
    with the new metadata fields. You can see that in my output in figure D.14.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查这些覆盖文件的內容，您会发现它们只是指定了应用程序的配置设置；所有镜像都是相同的。现在当您使用 http://localhost:8010 上的应用程序时，访问日志条目仍然会被生成，但它们会被
    Fluentd 过滤掉，因此您在 Kibana 中不会看到任何新的日志。您将看到来自其他组件的日志，这些日志被添加了新的元数据字段。您可以在图 D.14 的我的输出中看到这一点。
- en: '![](../Images/D-14.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 D-14](../Images/D-14.jpg)'
- en: Figure D.14 Fluentd uses the tag in the log to filter out records and to generate
    new fields.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.14 Fluentd 使用日志中的标签来过滤记录并生成新的字段。
- en: The access-log entries are still available because they’re writing to stdout
    inside the Fluentd container. You can see them as container logs—but from the
    Fluentd container, not the access-log container.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 访问日志条目仍然可用，因为它们在 Fluentd 容器内部写入到 stdout。您可以将它们视为容器日志，但它们来自 Fluentd 容器，而不是访问日志容器。
- en: 'Try it now Check the Fluentd container logs to be sure the records are still
    available:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：检查 Fluentd 容器日志以确保记录仍然可用：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can see my output in figure D.15\. The access-log entry has been sent to
    a different target, but it has still been through the same processing to enrich
    the record with the app, service, and image name:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图 D.15 中看到我的输出。访问日志条目已被发送到不同的目标，但它仍然经过了相同的处理，以丰富记录中的应用程序、服务和镜像名称：
- en: '![](../Images/D-15.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 D-15](../Images/D-15.jpg)'
- en: Figure D.15 These logs are filtered so they’re not stored in Elasticsearch but
    are echoed to stdout.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.15 这些日志被过滤，因此它们不会存储在 Elasticsearch 中，而是回显到 stdout。
- en: This is a nice way of separating core application logs from nice-to-have logs.
    You wouldn’t use stdout in production, but you might have different outputs for
    different classes of logs—performance critical components could send log entries
    to Kafka, user-facing logs could go to Elasticsearch, and the rest could be filed
    in Amazon S3 cloud storage. Those are all supported log stores in Fluentd.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种将核心应用程序日志与可选日志分开的好方法。在生产环境中，您不会使用 stdout，但您可能对不同类别的日志有不同的输出——性能关键组件可以将日志条目发送到
    Kafka，面向用户的日志可以发送到 Elasticsearch，其余的可以存储在 Amazon S3 云存储中。这些都是 Fluentd 支持的日志存储。
- en: There’s one final exercise for this chapter to reset the logging and put access-log
    entries back into Elasticsearch. This approximates a situation in production where
    you find a system problem and you want to increase the logs to see what’s happening.
    With the logging setup we have, the logs are already being written by the app.
    We can surface them just by changing the Fluentd configuration file.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有一个最后的练习来重置日志并将访问日志条目重新添加到 Elasticsearch。这模拟了生产环境中您发现系统问题并希望增加日志以查看发生了什么的情况。在我们现有的日志设置中，日志已经被应用程序写入。我们只需更改
    Fluentd 配置文件就可以将其暴露出来。
- en: 'Try it now Deploy a new Fluentd configuration that sends access-log records
    to Elasticsearch:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：部署一个新的 Fluentd 配置，将访问日志记录发送到 Elasticsearch：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This deployment uses a configuration file that removes the `match` block for
    access-log records, so all the gallery component logs get stored in Elasticsearch.
    When you refresh the image gallery page in your browser, the logs will get collected
    and stored. You can see my output in figure D.16, where the most recent logs are
    shown from both the API and the access-log components.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署使用一个配置文件，移除了访问日志记录的 `match` 块，因此所有画廊组件的日志都存储在 Elasticsearch 中。当您在浏览器中刷新图像画廊页面时，日志将被收集并存储。您可以在图
    D.16 的我的输出中看到这些日志，其中显示了来自 API 和访问日志组件的最新日志。
- en: '![](../Images/D-16.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 D-16](../Images/D-16.jpg)'
- en: Figure D.16 A change to the Fluentd config adds logs back into Elasticsearch
    without any app changes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 D.16 通过对 Fluentd 配置的修改，无需更改应用程序即可将日志重新添加到 Elasticsearch。
- en: You do need to be aware that there’s the potential for lost log entries with
    this approach. During the deployment, containers could be sending logs when there’s
    no Fluentd container running to collect them. Docker continues gracefully in that
    situation, and your app containers keep running, but the log entries don’t get
    buffered so they’ll be lost. It’s unlikely to be a problem in a clustered production
    environment, but even if it did happen, it’s preferable to restarting an app container
    with increased logging configuration—not least because the new container may not
    have the same issue as the old container, so your new logs won’t tell you anything
    interesting.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你确实需要意识到，这种方法可能会导致日志条目丢失。在部署期间，容器可能会发送日志，但此时没有Fluentd容器在运行以收集它们。Docker在这种情况下会优雅地继续运行，你的应用程序容器也会继续运行，但日志条目不会被缓冲，因此它们将会丢失。在集群生产环境中，这不太可能成为问题，但即使发生了这种情况，重启应用程序容器并增加日志配置也是更好的选择——至少因为新的容器可能不会像旧容器那样有问题，所以你的新日志不会告诉你任何有趣的事情。
- en: D.5 Understanding the container logging model
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.5 理解容器日志模型
- en: The logging approach in Docker is super-flexible, but only when you make your
    application logs visible as container logs. You can do that directly by having
    your app write logs to stdout, or indirectly by using a relay utility in your
    container that copies log entries to stdout. You need to spend some time making
    sure all your application components write container logs, because once you’ve
    got that working, you can process the logs however you like.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Docker中的日志记录方法非常灵活，但前提是你需要将你的应用程序日志作为容器日志可见。你可以直接通过让应用程序将日志写入stdout来实现，或者间接地通过在你的容器中使用一个中继工具，将日志条目复制到stdout。你需要花一些时间确保所有应用程序组件都写入容器日志，因为一旦你做到了这一点，你就可以按你喜欢的方式处理日志。
- en: We used the EFK stack in this chapter—Elasticsearch, Fluentd, and Kibana—and
    you’ve seen how easy it is to pull all your container logs into a centralized
    database with a user-friendly search UI. All those technologies are swappable,
    but Fluentd is one of the most used because it’s so simple and so powerful. That
    stack runs nicely in single-machine environments, and it scales for production
    environments too. Figure D.17 shows how a clustered environment runs a Fluentd
    container on each node, where the Fluentd container collects logs from the other
    containers on that node and sends them to an Elasticsearch cluster—also running
    in containers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了EFK堆栈——Elasticsearch、Fluentd和Kibana——你已经看到了如何轻松地将所有容器日志拉入一个具有用户友好搜索界面的集中式数据库。所有这些技术都是可互换的，但Fluentd是最常用的之一，因为它既简单又强大。该堆栈在单机环境中运行良好，并且也可以扩展到生产环境。图D.17显示了集群环境在每个节点上运行Fluentd容器的情况，其中Fluentd容器收集该节点上其他容器的日志并将它们发送到Elasticsearch集群——该集群也运行在容器中。
- en: '![](../Images/D-17.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/D-17.jpg)'
- en: Figure D.17 The EFK stack works in production with clustered storage and multiple
    Fluentd instances.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图D.17 EFK堆栈在生产环境中使用集群存储和多个Fluentd实例工作。
- en: I’ll finish with a note of caution before we move on to the lab. Some teams
    don’t like all the processing layers in the container logging model; they prefer
    to write application logs directly to the final store, so instead of writing to
    stdout and having Fluentd send data to Elasticsearch, the application writes directly
    to Elasticsearch. I really don’t like that approach. You save some processing
    time and network traffic in exchange for a complete lack of flexibility. You’ve
    hardcoded the logging stack into all your applications, and if you want to switch
    to Graylog or Splunk, you need to go and rework your apps. I always prefer to
    keep it simple and flexible-write your application logs to stdout and make use
    of the platform to collect, enrich, filter, and store the data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入实验室之前，我要提醒大家注意一点。有些团队不喜欢容器日志模型中的所有处理层；他们更喜欢直接将应用程序日志写入最终存储，因此，而不是写入stdout并让Fluentd将数据发送到Elasticsearch，应用程序直接写入Elasticsearch。我真的很不喜欢这种方法。你节省了一些处理时间和网络流量，但代价是完全缺乏灵活性。你将日志堆栈硬编码到所有应用程序中，如果你想切换到Graylog或Splunk，你需要去重新工作你的应用程序。我总是更喜欢保持简单和灵活——将你的应用程序日志写入stdout并利用平台来收集、丰富、过滤和存储数据。
- en: D.6 Lab
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.6 实验室
- en: 'I didn’t focus too much on configuring Fluentd in this chapter, but it’s worth
    getting some experience setting that up, so I’m going to ask you to do it in the
    lab. In the lab folder for this chapter there’s a Docker Compose file for the
    random number app and a Docker Compose file for the EFK stack. The app containers
    aren’t configured to use Fluentd, and the Fluentd setup doesn’t do any enrichment,
    so you have three tasks:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我没有过多关注 Fluentd 的配置，但获得一些设置该配置的经验是值得的，因此我将在实验室中要求您完成这项任务。在本章的实验室文件夹中，有一个随机数字应用的
    Docker Compose 文件和一个 EFK 堆栈的 Docker Compose 文件。应用容器尚未配置为使用 Fluentd，Fluentd 设置也没有进行任何丰富操作，因此您有三个任务：
- en: Extend the Compose file for the numbers app so all the components use the Fluentd
    logging driver, and set a tag with the app name, service name, and image.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展 numbers 应用的 Compose 文件，以便所有组件都使用 Fluentd 日志驱动程序，并设置一个包含应用名称、服务名称和镜像的标签。
- en: Extend the Fluentd configuration file, `elasticsearch.conf`, to split the tag
    into app name, service name, and image name fields for all records from the numbers
    app.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Fluentd 的配置文件 `elasticsearch.conf` 扩展，以便将标签拆分为应用名称、服务名称和镜像名称字段，以处理来自 numbers
    应用的所有记录。
- en: Add a failsafe `match` block to the Fluentd configuration so any records that
    aren’t from the numbers app get forwarded to stdout.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Fluentd 配置中添加一个安全检查 `match` 块，以便将所有非 numbers 应用记录转发到 stdout。
- en: 'No hints for this one, because this is a case of working through the config
    setup for the image gallery app and seeing which pieces you need to add for the
    numbers app. As always, my solution is up on GitHub for you to check: [https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md](https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一部分没有提示，因为这是一个通过配置图像库应用来处理配置设置并查看需要为 numbers 应用添加哪些组件的案例。一如既往，我的解决方案已上传到
    GitHub 供您检查：[https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md](https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md)。
