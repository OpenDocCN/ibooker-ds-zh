- en: 2 Kubernetes cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 Kubernetes 集群
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Control plane and worker node components in a multinode cluster
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多节点集群中的控制平面和工作节点组件
- en: Upgrading control plane components with kubeadm
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 kubeadm 升级控制平面组件
- en: Investigating Pod and node networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查 Pod 和节点网络
- en: Backing up and restoring etcd
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和恢复 etcd
- en: Taints and tolerations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 污点和容忍
- en: The Kubernetes cluster architecture is almost impossible to grasp without getting
    inside a running cluster and discovering the components firsthand. No matter how
    you look at it, as Kubernetes administrators, we have to know what’s happening
    under the hood. When you finish this chapter, you will feel comfortable accessing
    all the components in a Kubernetes cluster, upgrading components when needed,
    and backing up the cluster configuration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有进入运行中的集群并亲自发现组件，几乎不可能理解 Kubernetes 集群架构。无论你怎么看，作为 Kubernetes 管理员，我们必须了解底层的运作情况。当你完成本章后，你将能够舒适地访问
    Kubernetes 集群中的所有组件，在需要时升级组件，并备份集群配置。
- en: The cluster architecture, installation, and configuration
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 集群架构、安装和配置
- en: This chapter reviews part of the cluster architecture, installation, and configuration
    domain of the CKA curriculum. This domain covers the elements of a Kubernetes
    cluster, including key components and how to maintain a healthy Kubernetes cluster.
    The chapter encompasses the following competencies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章回顾了 CKA 课程中集群架构、安装和配置领域的部分内容。该领域涵盖了 Kubernetes 集群的元素，包括关键组件以及如何维护一个健康的 Kubernetes
    集群。本章包括以下能力。
- en: '| Competency | Chapter section |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 能力 | 章节部分 |'
- en: '| --- | --- |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Provision underlying infrastructure to deploy a Kubernetes cluster. | 2.1
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 提供底层基础设施以部署 Kubernetes 集群。 | 2.1 |'
- en: '| Perform a version upgrade on a Kubernetes cluster using kubeadm. | 2.1 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 使用 kubeadm 在 Kubernetes 集群上执行版本升级。 | 2.1 |'
- en: '| Implement etcd backup and restore. | 2.2 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 实施 etcd 备份和恢复。 | 2.2 |'
- en: '| Manage a highly available Kubernetes cluster. | 2.2 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 管理一个高可用性的 Kubernetes 集群。 | 2.2 |'
- en: 2.1 Kubernetes cluster components
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 Kubernetes 集群组件
- en: The control plane Pods work together to form the control plane. Those components
    are the API server, the controller manager, the scheduler, and etcd. In the Kubernetes
    clusters that you will face on exam day, these components exist as Pods running
    on the control plane node. You can see the cluster components by viewing the Pods
    running in the `kube-system` namespace. You can view those Pods with the command
    `kubectl get po -n kube-system`, and you’ll see output similar to figure 2.1\.
    We first run the command `docker exec -it kind-control-plane bash` to get a shell
    to the control plane node.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面 Pods 协同工作以形成控制平面。这些组件包括 API 服务器、控制器管理器、调度器和 etcd。在你在考试当天可能会遇到的 Kubernetes
    集群中，这些组件以在控制平面节点上运行的 Pods 的形式存在。你可以通过查看运行在 `kube-system` 命名空间中的 Pods 来查看集群组件。你可以使用命令
    `kubectl get po -n kube-system` 来查看这些 Pods，你将看到类似于图 2.1 的输出。我们首先运行命令 `docker exec
    -it kind-control-plane bash` 来获取控制平面节点的 shell。
- en: '![](../../OEBPS/Images/02-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-01.png)'
- en: Figure 2.1 The Pods in the `kube-system` namespace
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 `kube-system` 命名空间中的 Pods
- en: In the coming sections of this chapter, you’ll learn how to manage these components,
    including how to access them, modify their configuration, back them up, and upgrade
    them. Now, let’s get our hands dirty working in the terminal.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后续部分，你将学习如何管理这些组件，包括如何访问它们、修改它们的配置、备份它们以及升级它们。现在，让我们在终端中动手操作。
- en: 2.1.1 Kubernetes version upgrade
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 Kubernetes 版本升级
- en: The CKA exam will test your knowledge of maintaining a Kubernetes cluster. This
    includes upgrading the control plane components to a certain version. For example,
    the exam question will say something like the following.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CKA 考试将测试你维护 Kubernetes 集群的知识。这包括将控制平面组件升级到特定版本。例如，考试问题可能会说如下内容。
- en: '| Exam task There’s a need for company X to upgrade the Kubernetes controller
    to version 1.24 or higher, due to a bug that affects Pod scheduling. Perform the
    update with minimal downtime and loss of service. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 考试任务 公司 X 需要升级 Kubernetes 控制器到 1.24 或更高版本，因为一个影响 Pod 调度的错误。以最少的停机时间和服务损失执行更新。
    |'
- en: Knowing that you should use kubeadm to do so will allow you to breeze through
    that task on the exam and feel confident you’re heading toward a passing score.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 知道你应该使用 kubeadm 来这样做，将使你能够轻松地完成这项任务，并对你正在朝着及格分数前进感到自信。
- en: If you don’t already have access to an existing Kubernetes cluster, creating
    a Kubernetes cluster with kind is explained in appendix A. As soon as your kind
    cluster is built, use the `kubectl` tool preinstalled on the control plane node.
    You can get a Bash shell to the control plane node by typing the command `docker
    exec -it kind-control-plane bash` and following along. Having a cluster that is
    built with kubeadm means we can also use kubeadm to view the versions of our control
    plane components, as shown in figure 2.2, and upgrade the cluster. Using the command
    `kubeadm upgrade plan` from the control plane shows a list of control plane components
    and displays the current version in the `CURRENT` column, as well as the version
    to which you can upgrade in the `TARGET` column.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有访问现有的Kubernetes集群，附录A中解释了如何使用kind创建Kubernetes集群。一旦你的kind集群构建完成，就可以使用预安装在控制平面节点上的`kubectl`工具。你可以通过输入命令`docker
    exec -it kind-control-plane bash`并跟随操作来获取控制平面节点的Bash shell。使用kubeadm构建的集群意味着我们也可以使用kubeadm查看我们的控制平面组件的版本，如图2.2所示，并升级集群。从控制平面运行`kubeadm
    upgrade plan`命令会显示控制平面组件列表，并在`CURRENT`列中显示当前版本，以及在`TARGET`列中显示你可以升级到的版本。
- en: '![](../../OEBPS/Images/02-02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-02.png)'
- en: Figure 2.2 View the control plane components and how to upgrade them, if necessary.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 查看控制平面组件以及如何升级它们（如果需要的话）。
- en: NOTE The `TARGET` column doesn’t show the latest release of Kubernetes. This
    is because we can only update to the current version of kubeadm. If kubeadm was
    at version 1.26.0, the `TARGET` column would show v1.26.3\. To upgrade kubeadm
    from 1.26.0 to 1.26.3, run the command `apt update; apt install -y kubeadm=1.26.3-00`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：`TARGET`列不显示Kubernetes的最新版本。这是因为我们只能更新到当前版本的kubeadm。如果kubeadm是1.26.0版本，`TARGET`列将显示v1.26.3。要将kubeadm从1.26.0升级到1.26.3，运行命令`apt
    update; apt install -y kubeadm=1.26.3-00`。
- en: If you must upgrade kubeadm to the latest release, first download the GPG key
    and add kubeadm to your local `apt` packages (still in a shell on the control
    plane node). To download the GPG key, run the command `curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg
    https://packages.cloud.google.com/apt/doc/apt-key.gpg`. To add kubeadm to your
    local `apt` packages, run the command `echo "deb [signed-by=/ etc/apt/keyrings/kubernetes-archive-keyring.gpg]
    https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list`.
    Having done that, rerun the command `apt update; apt install -y kubeadm=1.26.3-00`
    to upgrade kubeadm to version 1.26.3\. When you run the `kubeadm upgrade plan`
    command again, you’ll notice that your `TARGET` column has a higher version of
    1.24.3 to which you can upgrade. See the abbreviated output from this command
    in figure 2.3.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你必须将kubeadm升级到最新版本，首先下载GPG密钥并将kubeadm添加到你的本地`apt`软件包（仍然在控制平面节点上的shell中）。要下载GPG密钥，运行命令`curl
    -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg`。要将kubeadm添加到你的本地`apt`软件包，运行命令`echo
    "deb [signed-by=/ etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/
    kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list`。完成这些操作后，重新运行命令`apt
    update; apt install -y kubeadm=1.26.3-00`以将kubeadm升级到版本1.26.3。当你再次运行`kubeadm upgrade
    plan`命令时，你会注意到你的`TARGET`列显示了一个比1.24.3更高的版本，你可以升级到这个版本。请参见图2.3中此命令的缩略输出。
- en: '![](../../OEBPS/Images/02-03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-03.png)'
- en: Figure 2.3 When you upgrade kubeadm, you can upgrade your control plane components
    as well.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 当你升级kubeadm时，你也可以升级你的控制平面组件。
- en: Congratulations! You’ve successfully upgraded the control plane components of
    a Kubernetes cluster, also known as “upgrading Kubernetes.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功升级了Kubernetes集群的控制平面组件，也就是“升级Kubernetes”。
- en: 2.1.2 The control plane
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 控制平面
- en: The control plane and worker nodes have different responsibilities. Worker nodes
    carry the application workload and run the Pods that contain those application
    containers, whereas the control plane runs an initial set of Pods, comprised of
    the control plane components we just saw when we upgraded the cluster. We refer
    to them as *system Pods* because they contain the necessary structure around the
    running of the entire Kubernetes system. The `kube-apiserver`, `kube-controller-manager`,
    `kube-scheduler`, and etcd all run as system Pods on the control plane node. These
    system Pods will exist regardless of where your Kubernetes cluster resides or
    how it was built, as they are essential to the core of Kubernetes. You can see
    the system Pods by running the command `k get po -o wide -A --field-selector spec.nodeName=kind-control-plane`,
    which will show output similar to figure 2.4.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面和工作节点有不同的职责。工作节点承载应用程序的工作负载并运行包含这些应用程序容器的Pods，而控制平面运行一组初始的Pods，包括我们在升级集群时看到的控制平面组件。我们称它们为*系统Pods*，因为它们包含了围绕整个Kubernetes系统运行的必要结构。《kube-apiserver》、《kube-controller-manager》、《kube-scheduler》和etcd都在控制平面节点上作为系统Pod运行。这些系统Pod将存在于你的Kubernetes集群所在的位置或其构建方式如何，因为它们对于Kubernetes的核心至关重要。你可以通过运行命令`k
    get po -o wide -A --field-selector spec.nodeName=kind-control-plane`来查看系统Pods，该命令将显示类似于图2.4的输出。
- en: '![](../../OEBPS/Images/02-04.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-04.png)'
- en: Figure 2.4 System Pods that reside on the control plane node
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 控制平面节点上的系统Pods
- en: CoreDNS is also a Pod that runs on the control plane node, but it’s considered
    a plugin, not a core system Pod. In addition, the kube-proxy Pod will run on all
    nodes, regardless of whether it’s a control plane node or a worker node. Let’s
    focus on the core components of a control plane node, as illustrated in figure
    2.5.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: CoreDNS也是一个运行在控制平面节点上的Pod，但它被视为一个插件，而不是核心系统Pod。此外，kube-proxy Pod将在所有节点上运行，无论它是一个控制平面节点还是工作节点。让我们专注于控制平面节点的核心组件，如图2.5所示。
- en: '![](../../OEBPS/Images/02-05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-05.png)'
- en: Figure 2.5 System Pods and plugins on the Kubernetes control plane node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 Kubernetes控制平面节点上的系统Pods和插件
- en: Each Pod plays a specific role in the operation of Kubernetes. The controller
    manager Pod maintains cluster operations, ensuring Deployments are running the
    correct number of replicas. The scheduler Pod is responsible for detecting available
    resources on a node, so you can place a Pod on that node. The act of assigning
    a Pod to a node is called *scheduling* ; from now on, when I say “a Pod is scheduled
    to a node,” I mean that a new Pod has been started on that node. The API server
    Pod will expose the API interface and is the communication hub for all other components,
    including those components on a worker node and control plane node. Finally, the
    etcd Pod is the etcd datastore for storing cluster configuration data, as we’ve
    discussed in this chapter as well as the previous chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Pod在Kubernetes的操作中都扮演着特定的角色。控制器管理器Pod维护集群操作，确保部署正在运行正确的副本数。调度器Pod负责检测节点上的可用资源，以便你可以在该节点上放置Pod。将Pod分配给节点的行为称为*调度*；从现在起，当我说“一个Pod被调度到节点上”时，我的意思是该节点上已启动了一个新的Pod。API服务器Pod将公开API接口，并且是所有其他组件的通信中心，包括工作节点和控制平面节点上的组件。最后，etcd
    Pod是用于存储集群配置数据的etcd数据存储，正如我们在本章以及上一章所讨论的。
- en: 2.1.3 Taints and tolerations
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 污点（Taints）和容忍度（Tolerations）
- en: 'By default, application Pods will not run on the control plane node. Why is
    that? Well, the control plane has a special attribute assigned to it called a
    *taint*. A taint will repel work, meaning it will disable scheduling to that node
    unless a certain specification exists in the YAML spec called a *toleration*.
    Let’s see what a taint looks like on our control plane node (`docker exec -it
    kind-control-plane bash`) by typing the command `kubectl describe no | grep Taints`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，应用程序Pod不会在控制平面节点上运行。为什么是这样呢？因为控制平面有一个特殊的属性，称为*污点*。污点会排斥工作，这意味着除非YAML规范中存在一个称为*容忍度*的特定规范，否则将禁用对该节点的调度。让我们通过在控制平面节点上（`docker
    exec -it kind-control-plane bash`）输入命令`kubectl describe no | grep Taints`来查看我们的控制平面节点上的污点是什么样的：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As we can see, a taint has been applied to our control plane node as part of
    the cluster creation process, and the taint is `node-role.kubernetes.io/master:NoSchedule`.
    The taint has three parts: a key, an effect, and a value. A taint doesn’t need
    to have a value but must have a key and an effect. So, in this case, the key is
    `node-role.kubernetes.io` and the effect is `NoSchedule`. This means that unless
    there is a toleration for that taint, the Pod will not get scheduled to that node
    with that taint. In figure 2.6, you’ll see a demonstration of this, as the Pod
    doesn’t have a toleration for the taint, so it’s not scheduled to that node.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在集群创建过程中，已经将污点应用于我们的控制平面节点，污点为`node-role.kubernetes.io/master:NoSchedule`。污点由三部分组成：一个键、一个效果和一个值。污点不需要有值，但必须有键和效果。因此，在这种情况下，键是`node-role.kubernetes.io`，效果是`NoSchedule`。这意味着除非有对该污点的容忍，否则Pod不会调度到具有该污点的节点。在图2.6中，您将看到这一演示，因为Pod没有对该污点的容忍，所以它没有被调度到该节点。
- en: '![](../../OEBPS/Images/02-06.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-06.png)'
- en: Figure 2.6 A taint is applied to the control plane node, and there is no toleration;
    therefore, the Pod is not scheduled.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 将污点应用于控制平面节点，但没有容忍；因此，Pod没有被调度。
- en: Let’s apply a taint to one of our worker nodes and see what a taint with a key,
    effect, and value looks like, using the command `kubectl taint no kind-worker
    decdicated=special-user:NoSchedule`. This command is deconstructed in figure 2.7
    to show which item is the key versus which item is the value and effect.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的一个工作节点上应用一个污点，并看看具有键、效果和值的污点是什么样的，使用命令`kubectl taint no kind-worker decdicated=special-user:NoSchedule`。这个命令在图2.7中被分解，以显示哪个项目是键，哪个项目是值和效果。
- en: '![](../../OEBPS/Images/02-07.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-07.png)'
- en: Figure 2.7 Breakdown of the parts of a taint into key, value, and effect
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 污点各部分的分解：键、值和效果
- en: A full taint (key, value, and effect) is appropriate when you want to be more
    specific about the qualification a Pod must have to “get past” a taint. Yes, there’s
    a way to schedule a Pod to a node even though that node may have a taint applied.
    This is called a toleration. This is important to remember because tolerations
    do not mean that you are selecting that particular node; they mean that the scheduler
    may choose to schedule the Pod there if the conditions are right among all the
    other nodes. This is how Pods like the DNS Pod run on the control plane. These
    Pods have toleration for the taint. To view the tolerations within the Pod `YAML`,
    run the command `kubectl get po coredns-558bd4d5db-7dmrp -o yaml -n kube-system
    | grep tolerations -A14`. This command will get the YAML output of the Pod, named
    `core-dns-64897985d-4th9h,` in the `kube-system` namespace, and then use the grep
    tool to filter the results and give us 14 lines after the result. The output will
    be similar to figure 2.8, which you can compare to the taint that’s placed on
    the control plane node.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想更具体地说明Pod必须具备的资格才能“绕过”污点时，使用完整的污点（键、值和效果）是合适的。是的，有一种方法可以将Pod调度到节点，即使该节点可能已应用污点。这被称为容忍。这一点很重要，因为容忍并不意味着您正在选择那个特定的节点；它们意味着调度器可能会在所有其他节点条件合适的情况下选择将Pod调度到那里。这就是像DNS
    Pod这样的Pod如何在控制平面运行的原因。这些Pod对污点有容忍。要查看Pod `YAML`中的容忍，请运行命令`kubectl get po coredns-558bd4d5db-7dmrp
    -o yaml -n kube-system | grep tolerations -A14`。此命令将获取名为`core-dns-64897985d-4th9h`的Pod在`kube-system`命名空间中的YAML输出，然后使用grep工具过滤结果，并给出结果后的14行。输出将类似于图2.8，您可以将其与放置在控制平面节点上的污点进行比较。
- en: '![](../../OEBPS/Images/02-08.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-08.png)'
- en: Figure 2.8 Comparison between how the taint is set and the toleration for it
    in the CoreDNS Pod
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 CoreDNS Pod中污点设置与容忍的比较
- en: As we see in the YAML spec for this CoreDNS Pod, within the list of tolerations,
    the key `node-role.kubernetes.io/master` and the effect `NoSchedule` match our
    taint. This is how the CoreDNS Pod was able to be scheduled to the control plane
    node. Remember that your CoreDNS Pod will have a different name, so you’ll want
    to replace `core-dns-64897985d-4th9h` with the name of your CoreDNS Pod. If you
    don’t know the name of the Pod, you can get the name by typing `kubectl get po
    -A.`
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在该CoreDNS Pod的YAML规范中看到的那样，在容忍列表中，键`node-role.kubernetes.io/master`和效果`NoSchedule`与我们的污点相匹配。这就是CoreDNS
    Pod能够调度到控制平面节点的原因。请记住，您的CoreDNS Pod将有一个不同的名称，因此您需要将`core-dns-64897985d-4th9h`替换为您的CoreDNS
    Pod的名称。如果您不知道Pod的名称，可以通过输入`kubectl get po -A`来获取名称。
- en: As we see in this case, the toleration must match the key and the effect exactly.
    If it’s off by even a little, then the Pod will not be scheduled to the intended
    node. So, as you can imagine, having more loosely defined rules for tolerations
    makes it easier to define a toleration when you’re writing the YAML. In figure
    2.8, you may also notice other fields like `operator` and `tolerationSeconds`.
    `operator`, if not defined explicitly, like in the toleration for the `node-role`
    of the control plane taint, will default to `Equal` of the two possible values
    of `Exists` or `Equal`. Because the operator was not defined for the `node-role`
    taint toleration, it defaulted to `Equal`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，容忍度必须与键和效果完全匹配。如果稍有偏差，则 Pod 将不会被调度到目标节点。因此，正如你所想象的，为容忍度定义更宽松的规则使得在编写
    YAML 时定义容忍度更容易。在图 2.8 中，你还可以注意到其他字段，如 `operator` 和 `tolerationSeconds`。如果未明确定义
    `operator`，例如在控制平面污点的 `node-role` 容忍度中，它将默认为两个可能值 `Exists` 或 `Equal` 中的 `Equal`。因为
    `node-role` 污点容忍度没有定义运算符，它默认为 `Equal`。
- en: A toleration matches a taint if the keys and effects are the same and the operator
    is equal. If there is a value, then the toleration matches a taint if the keys,
    effects, and values all match exactly and the operator is `Equal`. If the operator
    is `Exists`, no value should be specified, but the key and the effect must match.
    Referring to the command output in figure 2.8, the effect `NoExecute` and key
    `node.kubernetes.io/not-ready` must match exactly because the operator is `Exists`.
    The `tolerationSeconds` is an optional parameter that allows you to specify how
    long the Pod will stay bound to the node after the taint is added, in this case,
    300 seconds. There is an example of this in figure 2.9, where the toleration matches
    the taint of the node; therefore, it’s successfully scheduled to the node.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果键和效果相同，并且运算符是相等的，则容忍度与污点匹配。如果有值，则容忍度与污点匹配，如果键、效果和值都完全匹配，并且运算符是 `Equal`。如果运算符是
    `Exists`，则不应指定值，但键和效果必须匹配。参考图 2.8 的命令输出，效果 `NoExecute` 和键 `node.kubernetes.io/not-ready`
    必须完全匹配，因为运算符是 `Exists`。`tolerationSeconds` 是一个可选参数，允许你指定 Pod 在添加污点后绑定到节点的持续时间，在这种情况下，300
    秒。图 2.9 中有一个这样的例子，其中容忍度与节点的污点匹配；因此，它成功调度到该节点。
- en: '![](../../OEBPS/Images/02-09.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-09.png)'
- en: Figure 2.9 The toleration applied to the Pod ensures that it can be scheduled
    to the tainted node.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 应用到 Pod 上的容忍度确保它可以调度到污点节点。
- en: 'Now let’s create a Pod that has a toleration for the taint that we applied
    to `kind-worker`. This taint had an effect of `NoSchedule`, a value of `special-user`,
    and a key named `dedicated`. This means that we must match all of those while
    using the default operator `Equal`. Let’s start with building out a Pod template
    with the command `k run pod-tolerate --image=nginx --dry-run=client -o yaml >
    pod-tolerate.yaml`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个具有针对我们应用于 `kind-worker` 的污点的容忍度的 Pod。这个污点有 `NoSchedule` 的效果，`special-user`
    的值，以及名为 `dedicated` 的键。这意味着我们必须使用默认运算符 `Equal` 匹配所有这些。让我们从使用命令 `k run pod-tolerate
    --image=nginx --dry-run=client -o yaml > pod-tolerate.yaml` 构建一个 Pod 模板开始：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: NOTE We build this Pod template with the `dry-run` flag and send the output
    of the command to a file, which is much easier than typing out the file from scratch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们使用 `dry-run` 标志构建这个 Pod 模板，并将命令的输出发送到文件，这比从头开始键入文件要容易得多。
- en: 'Let’s open the file `pod-tolerate.yaml` using the Vim text editor with the
    command `vim pod-tolerate.yaml`. We can add the tolerations we need to match the
    taint that’s applied to our worker node. We see in figure 2.10 in the output of
    the command `kubectl describe no kind-worker | grep Taints` that the key is `dedicated`,
    the value is `special-user`, and the effect is `NoSchedule`. With the file still
    open, in line with the word `containers`, insert `tolerations:`. Just below that,
    insert the line `- key: "dedicated"`, followed by `value: "special-user"`, and
    on the next line, `effect: "NoSchedule"`. You can see exactly what it should look
    like in figure 2.10.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们使用 Vim 文本编辑器通过命令 `vim pod-tolerate.yaml` 打开文件 `pod-tolerate.yaml`。我们可以添加所需的容忍度以匹配应用在我们工作节点上的污点。在命令
    `kubectl describe no kind-worker | grep Taints` 的输出中，我们看到键是 `dedicated`，值是 `special-user`，效果是
    `NoSchedule`。在文件仍然打开的情况下，与单词 `containers` 同行插入 `tolerations:`。紧接着，插入行 `- key:
    "dedicated"`，然后是 `value: "special-user"`，下一行是 `effect: "NoSchedule"`。你可以在图 2.10
    中看到它应该是什么样子。'
- en: '![](../../OEBPS/Images/02-10.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-10.png)'
- en: Figure 2.10 How tolerations for a taint are applied to effectively schedule
    a Pod
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 如何将容忍度应用于污点以有效地调度 Pod
- en: 'Once you’ve added the toleration to the Pod `YAML` manifest, you can save and
    close the file. Then you can submit the YAML to the API server with the command
    `kubectl create -f pod-tolerate.yaml`, and the Pod will be created in the default
    namespace. You can see which node the Pod is running on with the command `kubectl
    get po -o wide`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您将容忍度（toleration）添加到 Pod 的 `YAML` 清单中，您就可以保存并关闭文件。然后，您可以使用命令 `kubectl create
    -f pod-tolerate.yaml` 将 YAML 提交到 API 服务器，Pod 将在默认命名空间中创建。您可以使用命令 `kubectl get
    po -o wide` 查看Pod正在运行的节点：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You’ll see from the wide output that the Pod is running on the `kind-worker`
    node, but it could have just as easily been scheduled to the `kind-worker2` because
    tolerations don’t explicitly select which node the scheduler should choose. If
    you’re still having trouble creating the Pod, you can run this command, which
    already has the YAML intact, then you can compare and see where you went wrong.
    Just type the command `k apply -f https://raw.githubusercontent.com/chadmcrowell/k8s/main/
    manifests/ pod-tolerate.yaml.`
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从宽输出中，您会看到 Pod 正在 `kind-worker` 节点上运行，但它同样可以被调度到 `kind-worker2`，因为容忍度并没有明确选择调度器应该选择哪个节点。如果您在创建
    Pod 时仍然遇到困难，您可以运行这个命令，该命令已经包含了完整的 YAML，然后您可以比较并查看您出错的地方。只需输入命令 `k apply -f https://raw.githubusercontent.com/chadmcrowell/k8s/main/manifests/pod-tolerate.yaml.`
    即可。
- en: Next, we’ll talk about node selectors and node names you can add to your Pod
    `YAML` that will make such explicit statements to the scheduler. Node selectors,
    as the name implies, allow you to select a node to which a Pod is scheduled based
    on that node(s) label. Many labels are applied to a node by default. You can see
    a list of labels by performing the command `k get no --show-labels`. Labels don’t
    change the operation of the node but can be used to query the nodes with a particular
    label or to schedule Pods to that node using its label (which we’ll discuss in
    the coming sections of this chapter). Labels can also be applied to Pods. You
    can see Pod labels on Pods in the `kube-system` namespace using the command `k
    get po -n kube-system --show-labels`. The output will be similar to figure 2.11.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论您可以添加到 Pod `YAML` 中的节点选择器和节点名，这些选择器将对调度器做出明确的声明。节点选择器，正如其名所示，允许您根据节点标签选择一个节点，以便将
    Pod 调度到该节点。默认情况下，许多标签都会应用到节点上。您可以通过执行命令 `k get no --show-labels` 来查看标签列表。标签不会改变节点的操作，但可以用来查询具有特定标签的节点，或者使用其标签调度
    Pod 到该节点（我们将在本章的后续部分讨论）。标签也可以应用到 Pod 上。您可以使用命令 `k get po -n kube-system --show-labels`
    在 `kube-system` 命名空间中查看 Pod 标签。输出将类似于图 2.11。
- en: '![](../../OEBPS/Images/02-11.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-11.png)'
- en: Figure 2.11 The default labels applied to nodes and Pods when a Kubernetes cluster
    is created
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 创建 Kubernetes 集群时默认应用到节点和 Pod 上的标签
- en: Node names allow you to select a single node by its hostname. Let’s keep the
    taint applied to the node so we can work through this together and better visualize
    how these node selectors are applied in Kubernetes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 节点名允许您通过主机名选择单个节点。让我们保留对节点的污点（taint）应用，这样我们可以一起工作，更好地可视化这些节点选择器在 Kubernetes
    中的应用。
- en: 2.1.4 Nodes
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 节点
- en: kind stands for *Kubernetes in Docker*. kind creates a cluster’s Kubernetes
    node inside of a Docker container. Depicted in figure 2.12, the container exists
    inside the Pod, which is inside the node, which, in this case (with kind), is
    also a container comprised of a Kubernetes cluster in Docker.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: kind 代表 *Kubernetes in Docker*。kind 在 Docker 容器内创建集群的 Kubernetes 节点。如图 2.12
    所示，容器存在于 Pod 内，Pod 又存在于节点内，在这种情况下（使用 kind），节点本身也是一个容器，由 Docker 中的 Kubernetes 集群组成。
- en: '![](../../OEBPS/Images/02-12.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-12.png)'
- en: Figure 2.12 Kind cluster architecture, with nodes running as containers on our
    local system
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12 Kind 集群架构，节点作为容器在我们的本地系统上运行
- en: 'Let’s start by viewing the Docker container that kind created for us. You can
    view the Docker container with the command `docker container ls`, and your output
    will look exactly like this, but with a different container ID:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先查看 kind 为我们创建的 Docker 容器。您可以使用命令 `docker container ls` 查看Docker容器，您的输出将与此完全相同，但容器
    ID 不同：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The container name is `kind-control-plane`, which you may have guessed consists
    of the control plane components. What about the worker node? Well, because we
    are using this cluster for testing (and for simplicity’s sake), we have combined
    the control plane and worker components in the same node, but for all other production
    scenarios, these would be two or more separate nodes. To prevent getting sidetracked
    and to keep things simple, let’s continue. We’ll build some multinode clusters
    later in this chapter, since they will be included on the CKA exam. You can also
    see the nodes in a Kubernetes cluster using the `kubectl` tool with the command
    `kubectl get nodes`, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 容器名称是 `kind-control-plane`，您可能已经猜到它由控制平面组件组成。那么工作节点呢？嗯，因为我们使用这个集群进行测试（为了简单起见），我们将控制平面和工作组件组合在同一个节点上，但在所有其他生产场景中，这些将是两个或更多单独的节点。为了防止走题并保持简单，让我们继续。我们将在本章的后面部分构建一些多节点集群，因为它们将包含在
    CKA 考试中。您还可以使用 `kubectl` 工具通过 `kubectl get nodes` 命令查看 Kubernetes 集群中的节点，如下所示：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you see in table 2.1, we can use shorthand for addressing certain resources
    in Kubernetes, which will come in handy for the exam as it saves a lot of keystrokes.
    Also in table 2.1, you can see other abbreviations or shorthand for addressing
    different Kubernetes resources.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在表 2.1 中所见，我们可以在 Kubernetes 中使用缩写来指定某些资源，这对于考试来说很有用，因为它可以节省很多按键。此外，在表 2.1
    中，您还可以看到用于指定不同 Kubernetes 资源的其他缩写或缩写。
- en: Table 2.1 Kubernetes resource abbreviations and examples of use with `kubectl`
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 Kubernetes 资源缩写及其与 `kubectl` 的使用示例
- en: '| Resource | Abbreviation | Example |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 缩写 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Namespace | `ns` | `k get ns` |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 命名空间 | `ns` | `k get ns` |'
- en: '| Pod | `po` | `k get po` |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Pod | `po` | `k get po` |'
- en: '| Deployment | `deploy` | `k get deploy` |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 部署 | `deploy` | `k get deploy` |'
- en: '| ReplicaSet | `rs` | `k get rs` |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ReplicaSet | `rs` | `k get rs` |'
- en: '| Service | `svc` | `k get svc` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | `svc` | `k get svc` |'
- en: '| Service Account | `sa` | `k get sa` |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 服务账户 | `sa` | `k get sa` |'
- en: '| ConfigMap | `cm` | `k get cm` |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 配置映射 | `cm` | `k get cm` |'
- en: '| DaemonSet | `ds` | `k get ds` |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| DaemonSet | `ds` | `k get ds` |'
- en: '| Persistent volume | `pv` | `k get pvc` |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 持久卷 | `pv` | `k get pvc` |'
- en: '| Persistent volume claim | `pvc` | `k get pvc` |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 持久卷声明 | `pvc` | `k get pvc` |'
- en: '| Storage class | `sc` | `k get sc` |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 存储类 | `sc` | `k get sc` |'
- en: '| Network policy | `netpol` | `k get netpol` |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 网络策略 | `netpol` | `k get netpol` |'
- en: '| Ingress | `ing` | `k get ing` |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 入口 | `ing` | `k get ing` |'
- en: '| Endpoints | `ep` | `k get ep` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 端点 | `ep` | `k get ep` |'
- en: You can also see by typing `kubectl get no -o wide` that containerd is in fact
    used instead of the `dockershim` in our kind Kubernetes cluster. The `-o wide`
    is a way to retrieve a verbose output from the `get` command. We’ll explore this
    more throughout the rest of this book.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以通过输入 `kubectl get no -o wide` 来看到，在我们的 kind Kubernetes 集群中实际上使用的是 containerd
    而不是 `dockershim`。`-o wide` 是从 `get` 命令检索详细输出的方式。我们将在本书的其余部分进一步探讨这一点。
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Getting back to our single-node cluster, let’s now get a shell inside of the
    container so we can look inside. We can do this by typing `docker exec -it kind-control-plane
    bash` into our terminal, and we’ll see this exact output, which means that you
    are operating as root inside the `kind-control-plane` container:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的单节点集群，现在让我们在容器内部获取一个 shell，这样我们就可以查看其内部情况。我们可以通过在终端中输入 `docker exec -it
    kind-control-plane bash` 来做到这一点，我们会看到这个确切输出，这意味着您正在 `kind-control-plane` 容器内部以
    root 身份操作：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From this point forward, we’ll perform commands inside the container (after
    `root@kind-control-plane:/#`), as `kubectl` is already preinstalled and we can
    view additional information about our cluster. For example, we can use the `crictl`
    tool to list the control plane components as containers. The `crictl` tool is
    another command-line utility used to inspect and debug containers that are CRI
    (container runtime interface) compatible. CRI is a standard for how container
    runtimes are built and provides the kubelet with a consistent interface for interacting
    with the container runtime, whether Docker, containerd, or any other container
    runtime that is CRI compliant. With the command `crictl ps`, you can list the
    containers running inside of our `kind-control-plane` container:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将在容器内部执行命令（在 `root@kind-control-plane:/#` 之后），因为 `kubectl` 已经预安装，我们可以查看有关我们集群的更多信息。例如，我们可以使用
    `crictl` 工具列出控制平面组件作为容器。`crictl` 工具是另一种用于检查和调试与 CRI（容器运行时接口）兼容的容器的命令行实用程序。CRI
    是容器运行时构建的标准，并为 kubelet 提供了一个与容器运行时交互的一致接口，无论是 Docker、containerd 还是任何其他符合 CRI 标准的容器运行时。使用
    `crictl ps` 命令，您可以列出在 `kind-control-plane` 容器内部运行的容器：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `local-path-provisioner` is used for persistent storage in our cluster,
    `coredns` is used for resolving names to IP addresses in our cluster (DNS), and
    the `kindnet-cni` is used for Pod-to-Pod communication in our cluster. Pods are
    the smallest deployable unit in Kubernetes and can contain one or many containers.
    We’ll see in future chapters that Pods can run independently or as a part of a
    Deployment, ReplicaSet, and StatefulSet.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`local-path-provisioner` 用于我们集群的持久存储，`coredns` 用于我们集群中名称解析到 IP 地址（DNS），而 `kindnet-cni`
    用于我们集群中的 Pod 到 Pod 通信。Pod 是 Kubernetes 中最小的可部署单元，可以包含一个或多个容器。我们将在未来的章节中看到 Pods
    可以独立运行，也可以作为 Deployment、ReplicaSet 和 StatefulSet 的一部分运行。'
- en: 'Let’s now talk about the components that exist on a worker node. These components
    will be the same for each node in the cluster, whether you have just one worker
    node or thousands of worker nodes. They all contain the kubelet, the kube-proxy,
    and the container runtime. Let’s look at the kube-proxy Pods that run on the worker
    nodes in our cluster with the command `kubectl get po -o wide -A | grep worker`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈存在于工作节点上的组件。这些组件对于集群中的每个节点都是相同的，无论您只有一个工作节点还是有成千上万个工作节点。它们都包含 kubelet、kube-proxy
    和容器运行时。让我们使用命令 `kubectl get po -o wide -A | grep worker` 查看我们集群中工作节点上运行的 kube-proxy
    Pods：
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The kube-proxy creates our iptables rules and makes sure when Services are created
    that we can get to the Pods associated with that Service, as mentioned previously
    in the discussion about the kube-proxy on the control plane node. The kindnet
    Pod that we see there alongside the kube-proxy Pod is for CNI, but again, it is
    considered a plugin, not part of core Kubernetes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy 创建我们的 iptables 规则，并确保当创建服务时，我们可以访问与该服务关联的 Pods，正如之前在控制平面节点上关于 kube-proxy
    的讨论中提到的。我们看到的与 kube-proxy Pods 并列的 kindnet Pods 是用于 CNI 的，但同样，它被视为插件，而不是核心 Kubernetes
    的一部分。
- en: 'Next, let’s take a look at the container runtime that’s installed in each node.
    For this, we’ll run the command `kubectl get no -o wide | grep containerd`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下每个节点上安装的容器运行时。为此，我们将运行命令 `kubectl get no -o wide | grep containerd`：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see in the last column of that output, containerd is the container
    runtime that’s used for our nodes. Containerd is a lightweight daemon for Linux
    and utilizes cgroups and namespaces to run the containers that are inside of our
    Pods running in Kubernetes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在输出结果的最后一列中看到的，containerd 是用于我们节点的容器运行时。Containerd 是一个轻量级的 Linux 守护进程，它利用
    cgroups 和命名空间来运行 Kubernetes 中运行的 Pods 内部的容器。
- en: 'Finally, we’ll take a look at the kubelet. Because the kubelet is a systemd
    service running on the node, and because our nodes are really containers in kind,
    we’ll have to get a shell into the container that is our worker node. First type
    `exit` to exit from the control plane shell. You can now get a shell to the worker
    node with the command `docker exec -it kind-worker bash``.` Now, get the status
    of the service by running the command `systemctl status kubelet` like so:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将查看 kubelet。因为 kubelet 是在节点上运行的 systemd 服务，而且因为我们的节点实际上是容器，所以我们需要进入我们的工作节点容器。首先输入
    `exit` 退出控制平面 shell。现在，您可以使用命令 `docker exec -it kind-worker bash` 获取到工作节点的 shell。现在，通过运行命令
    `systemctl status kubelet` 如此查看服务的状态：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the systemd service is active and running, which is a good thing.
    There may be a question on the exam where you need to remember how to check if
    the kubelet service is running. Remember, if the kubelet is not running, your
    node status will show as `NotReady`. You can even test this by running the command
    `systemctl stop kubelet`, exiting out of the container shell, and then running
    the command `kubectl get no`. Go ahead and try it!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，systemd 服务处于活动状态并正在运行，这是好事。考试中可能会有一个问题需要您记住如何检查 kubelet 服务是否正在运行。记住，如果
    kubelet 没有运行，您的节点状态将显示为 `NotReady`。您甚至可以通过运行命令 `systemctl stop kubelet`，退出容器 shell，然后运行命令
    `kubectl get no` 来测试这一点。试试看吧！
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: NOTE It may take a few seconds for the message that the kubelet service is not
    running to get back to the API server, so if the node doesn’t show `NotReady`
    right away, don’t worry.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：kubelet 服务未运行的消息返回到 API 服务器可能需要几秒钟，所以如果节点没有立即显示 `NotReady`，请不要担心。
- en: 'To start the kubelet service again, simply get a shell inside the worker node
    again, then run the command `systemctl start kubelet`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要重新启动 kubelet 服务，只需再次在工作节点内部获取一个 shell，然后运行命令 `systemctl start kubelet`：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We’ll talk more about troubleshooting Services in chapter 8, but I just couldn’t
    wait to show you this valuable tool for determining if the kubelet service has
    not started in your Kubernetes cluster. For this next section, we’ll exit out
    of the `kind-worker` shell with the `exit` command.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第8章中更详细地讨论服务故障排除，但我实在忍不住想向您展示这个确定Kubernetes集群中kubelet服务是否未启动的宝贵工具。在接下来的这一节中，我们将使用`exit`命令退出`kind-worker`
    shell。
- en: 2.2 Datastore etcd
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 数据存储etcd
- en: The datastore etcd contains all of our cluster configuration data, which means
    information about how many Pods or Deployments are running in our cluster (including
    historical information), as well as which ports are exposed on which Services.
    If you didn’t have this data, you’d be lost when recreating Kubernetes resources
    from scratch. Just to be clear, we’re not talking about the data associated with
    your application. We’re talking about the configuration of the Kubernetes cluster
    itself. This is depicted in figure 2.13, where the etcd datastore runs inside
    of a Pod in the Kubernetes cluster, containing cluster configuration data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储etcd包含我们集群的所有配置数据，这意味着有关我们集群中运行了多少Pod或Deployment的信息（包括历史信息），以及哪些端口在哪些服务上暴露。如果您没有这些数据，您在从头开始重新创建Kubernetes资源时会感到迷茫。为了明确，我们不是在谈论与您的应用程序相关的数据。我们谈论的是Kubernetes集群本身的配置。这如图2.13所示，其中etcd数据存储在Kubernetes集群中的一个Pod内部，包含集群配置数据。
- en: '![](../../OEBPS/Images/02-13.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-13.png)'
- en: Figure 2.13 Etcd exists as a Pod in the `kube-system` namespace.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 Etcd作为Pod存在于`kube-system`命名空间中。
- en: 2.2.1 Working with etcdctl
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 使用etcdctl
- en: The CKA exam will test your knowledge of both backing up and restoring the etcd
    database. Luckily, there’s a tool preinstalled in the exam terminal called `etcdctl`,
    which you can use for both of these operations. Knowing this on exam day will
    be valuable as you are rewarded for answering etcd-related questions correctly,
    on your way to a passing score. An exam question might look something like the
    following.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: CKA考试将测试您备份和恢复etcd数据库的知识。幸运的是，考试终端中预装了一个名为`etcdctl`的工具，您可以使用它执行这两个操作。在考试当天了解这一点将非常有价值，因为您将因正确回答与etcd相关的问题而获得奖励，这将帮助您达到及格分数。一个考试问题可能看起来像以下这样。
- en: '| Exam task The cluster k8s has been misconfigured and needs to be restored
    from a backup located at `/tmp/c02dkjs0-001.db`. Perform the restore and verify
    that the DaemonSet kube-proxy has been restored to the cluster. |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 实考任务 集群k8s配置错误，需要从位于`/tmp/c02dkjs0-001.db`的备份中恢复。执行恢复并验证DaemonSet kube-proxy是否已恢复到集群中。|'
- en: To begin, we’ll get a shell to our control plane node with the command `docker
    exec -it kind-control-plane bash`. To accomplish this task, we must first back
    up etcd using a command-line utility for etcd called `etcdctl`. We can install
    `etcdctl` with the command `apt update; apt install -y etcd-client`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用命令`docker exec -it kind-control-plane bash`获取到控制平面节点的shell。为了完成这个任务，我们首先必须使用名为`etcdctl`的etcd命令行工具备份etcd。我们可以使用命令`apt
    update; apt install -y etcd-client`安装`etcdctl`。
- en: NOTE The `etcdctl` client will be available for you on the exam, so you won’t
    need to know how to install it during the exam.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：考试中`etcdctl`客户端将为您可用，因此您不需要在考试期间知道如何安装它。
- en: 'Once we have `etcdctl` installed, we need to set it to version 3\. We can do
    this with the command `export ETCDCTL_API=3`, which creates an environment variable
    named `ETCDCTL_API` that sets it to the value `3`. The `etcdctl` tool is now set
    to the correct version, so the command will be available to perform the backup.
    Run the command `etcdctl -v` to retrieve the version of `etcdctl`, which should
    now be set to version 3:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们安装了`etcdctl`，我们需要将其设置为版本3。我们可以通过命令`export ETCDCTL_API=3`来完成，这创建了一个名为`ETCDCTL_API`的环境变量，并将其设置为值`3`。现在`etcdctl`工具已设置为正确的版本，因此该命令将可用于执行备份。运行命令`etcdctl
    -v`以检索`etcdctl`的版本，它现在应设置为版本3：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have the version set, run the command `etcdctl snapshot save snapshotdb
    --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/ etcd/server.crt
    --key /etc/kubernetes/pki/etcd/server.key` to perform a snapshot backup of the
    etcd datastore. Run `ls | grep snapshotdb` to list the snapshot in the current
    directory:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了版本，运行以下命令以对etcd数据存储执行快照备份：`etcdctl snapshot save snapshotdb --cacert
    /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key
    /etc/kubernetes/pki/etcd/server.key`。运行`ls | grep snapshotdb`以列出当前目录中的快照：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we can check to see if there’s any data in that snapshot file using the
    command `etcdctl snapshot status snapshotdb --write-out=table`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用命令 `etcdctl snapshot status snapshotdb --write-out=table` 来检查那个快照文件中是否有任何数据：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s go ahead and perform the restore of etcd. To simulate something to restore,
    delete the DaemonSet that was created for our kube-proxy service, which will allow
    us to verify that our restore operation was successful after we restore it. To
    delete the DaemonSet, perform the command
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行 etcd 的恢复操作。为了模拟恢复过程，我们需要删除为我们的 kube-proxy 服务创建的 DaemonSet，这样我们就可以在恢复后验证我们的恢复操作是否成功。要删除
    DaemonSet，请执行以下命令
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If we view the DaemonSets in the `kube-system` namespace, we’ll notice that
    there’s only the kindnet DaemonSet, and the kube-proxy DaemonSet is no longer
    there:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 `kube-system` 命名空间中查看 DaemonSet，我们会注意到只有一个 kindnet DaemonSet，而 kube-proxy
    DaemonSet 已经不再存在：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: EXAM TIP The `-A` at the end of the command is shorthand for `--all-namespaces`.
    This is a real time-saver for the CKA exam.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 考试技巧：命令末尾的 `-A` 是 `--all-namespaces` 的缩写。这对于 CKA 考试来说是一个节省时间的真实技巧。
- en: 'Now that we’ve modified the cluster state, and the keys inside of our etcd
    datastore are different than our backup, we can perform the restore. To do this,
    we’ll use the command `etcdctl snapshot restore snapshotdb --data-dir /var/lib/etcd-restore`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经修改了集群状态，并且我们 etcd 数据存储中的键与我们备份的不同，我们可以执行恢复操作。为此，我们将使用命令 `etcdctl snapshot
    restore snapshotdb --data-dir /var/lib/etcd-restore`：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We used the `--data-dir` parameter to specify a format that the cluster can
    read as well as relocate our backup to a directory that’s already in use by Kubernetes.
    Now that we’ve prepared the snapshot for use by Kubernetes and stored it in the
    `/var/lib/etcd-restore` directory, we’ll go ahead and change the location where
    Kubernetes looks for the etcd data. This can be changed in the YAML specification
    for the API server, which will always be located on the control plane node in
    the `/etc/kubernetes/manifests` directory. Any YAML files that you place in this
    directory will automatically schedule the Kubernetes resources within.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `--data-dir` 参数指定了一个集群可以读取的格式，并将我们的备份移动到 Kubernetes 已经使用的目录。现在我们已经为 Kubernetes
    准备了快照并将其存储在 `/var/lib/etcd-restore` 目录中，我们将继续更改 Kubernetes 查找 etcd 数据的位置。这可以在
    API 服务器 YAML 规范中更改，它始终位于 `/etc/kubernetes/manifests` 目录中的控制平面节点上。您放置在该目录中的任何 YAML
    文件都将自动调度 Kubernetes 资源。
- en: 'The `kind-control-plane` doesn’t have Vim installed, so we’ll run the command
    `apt install update; apt install vim` to install it and edit our file. Open the
    file with the command `vim /etc/kubernetes/manifests/etcd.yaml`. Scroll down to
    the very bottom and change the path for the volume from `/var/lib/etcd` to `/var/lib/etcd-restore`
    (make sure to press the I key for insert mode). Here’s a snippet from that file,
    so you can check your work:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`kind-control-plane` 上没有安装 Vim，所以我们将运行命令 `apt install update; apt install vim`
    来安装它并编辑我们的文件。使用命令 `vim /etc/kubernetes/manifests/etcd.yaml` 打开文件。滚动到文件底部，并将卷的路径从
    `/var/lib/etcd` 更改为 `/var/lib/etcd-restore`（确保按 I 键进入插入模式）。以下是该文件的片段，以便您可以检查您的操作：'
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: NOTE To exit out of Vim, you’ll press the Esc key on your keyboard, followed
    by the colon (`:`) and the letter `w` to write and the letter `q` to quit (`:wq`).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要退出 Vim，您需要在键盘上按 Esc 键，然后按冒号（`:`）和字母 `w` 以写入，按字母 `q` 以退出（`:wq`）。
- en: That’s all you have to do to complete the restore! It may take a few seconds
    to get a response from the API server with your new cluster data, but shortly
    after, you’ll be able to run `kubectl get ds -A` and see both the kube-proxy and
    kindnet DaemonSets back where they should be. Congratulations, you’ve successfully
    backed up and restored etcd!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 完成恢复所需的所有操作就这些了！从 API 服务器获取您新集群数据的响应可能需要几秒钟，但不久之后，您将能够运行 `kubectl get ds -A`
    并看到 kube-proxy 和 kindnet DaemonSet 都回到了它们应该的位置。恭喜，您已成功备份并恢复了 etcd！
- en: 2.2.2 Client and server certificates
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 客户端和服务器证书
- en: 'In the previous section, we talked about etcd needing authentication to the
    cluster. This method of authenticating between client and server is called PKI
    (public key infrastructure). PKI is the client-server model that Kubernetes imposes,
    authenticating any requests that come into the API using a certificate. `kubectl`
    also uses a certificate to authenticate to the Kubernetes API, and we refer to
    this as the kubeconfig. However, `kubectl` is not the only tool or object that’s
    trying to access the API. In addition to `kubectl`, the controller manager, the
    scheduler, and the kubelet all need to use certificates to authenticate to the
    API. These certificates are all generated by the bootstrap process (via kubeadm).
    You don’t have to manage these certificates, but it is good to know where they
    reside for the CKA exam. In figure 2.14, you’ll see a visual representation of
    every client or server certification used for every component within a Kubernetes
    cluster. Let’s run the command `ls /etc/kubernetes` to list the contents of that
    directory and see the client certificates for the kubelet, scheduler, and controller
    manager:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了etcd需要通过认证到集群。这种客户端和服务器之间的认证方法称为PKI（公钥基础设施）。PKI是Kubernetes强加的客户端-服务器模型，使用证书认证所有进入API的请求。`kubectl`也使用证书来认证到Kubernetes
    API，我们将其称为kubeconfig。然而，`kubectl`并不是唯一试图访问API的工具或对象。除了`kubectl`之外，控制器管理器、调度器和kubelet都需要使用证书来认证到API。这些证书都是由引导过程（通过kubeadm）生成的。您不需要管理这些证书，但了解它们的位置对于CKA考试是有好处的。在图2.14中，您将看到Kubernetes集群中每个组件使用的每个客户端或服务器证书的视觉表示。让我们运行命令`ls
    /etc/kubernetes`来列出该目录的内容，并查看kubelet、调度器和控制器管理器的客户端证书：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../../OEBPS/Images/02-14.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-14.png)'
- en: Figure 2.14 Client and server certificate placement in Kubernetes
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 Kubernetes中客户端和服务器证书的放置
- en: 'NOTE You’ll still need a shell to the `kind-control-plane`. In case you forgot,
    here’s the command: `docker exec -it kind-control-plane bash`.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您仍然需要一个到`kind-control-plane`的shell。如果您忘记了，这里有一个命令：`docker exec -it kind-control-plane
    bash`。
- en: 'You’ll see `controller-manager.conf, kubelet.conf`, and `scheduler.conf`, which
    are representative of the kubeconfig used to authenticate to the Kubernetes API.
    If you view the contents of `kubelet.conf`, for example, you’ll see a config similar
    to your `kubectl` kubeconfig (`in ~/.kube/config`). Type the command `cat /etc/kubernetes/
    kubelet.conf`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到`controller-manager.conf, kubelet.conf`和`scheduler.conf`，这些是用于认证到Kubernetes
    API的kubeconfig的代表性配置。例如，如果您查看`kubelet.conf`的内容，您将看到一个类似于您的`kubectl` kubeconfig（位于`~/.kube/config`）的配置。输入命令`cat
    /etc/kubernetes/kubelet.conf`：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Notice the user is the system user, and the client certificate is located on
    the node in `/var/lib/kubelet/pki/kubelet-client-current.pem`. This will be the
    case for all nodes in Kubernetes, as all nodes run the kubelet service.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意用户是系统用户，客户端证书位于节点上的`/var/lib/kubelet/pki/kubelet-client-current.pem`。在Kubernetes中，所有节点都会运行kubelet服务，所以情况对所有节点都是如此。
- en: 'You may have noticed a `pki` directory in `/etc/kubernetes`, and if you list
    the contents of that directory, you’ll find more client and server certificates.
    Let’s take a look with the command `ls /etc/kubernetes/pki`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到了`/etc/kubernetes`中的`pki`目录，如果您列出该目录的内容，您将找到更多的客户端和服务器证书。让我们使用命令`ls
    /etc/kubernetes/pki`来查看：
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this `pki` directory, you’ll see another kubelet certificate (`kubelet.crt`),
    which is a server certificate. Yes, you can have a client and server certificate
    for the same service in Kubernetes. This is also apparent in the two files named
    `apiserver.crt` and `apiserver-etcd-client.crt`, a server and client certificate,
    respectively. And because the kubelet has a server certificate, it must have a
    client certificate, which is the file named `apiserver-kubelet-client.crt`. The
    `apiserver.crt` is the server certificate to the client certificates that we looked
    at in the `/etc/kubernetes` directory just a moment ago.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`pki`目录中，您将看到另一个kubelet证书（`kubelet.crt`），这是一个服务器证书。是的，在Kubernetes中，您可以为同一个服务拥有客户端和服务器证书。这一点在名为`apiserver.crt`和`apiserver-etcd-client.crt`的两个文件中也很明显，分别代表服务器和客户端证书。由于kubelet有一个服务器证书，它必须有一个客户端证书，这个证书就是名为`apiserver-kubelet-client.crt`的文件。`apiserver.crt`是客户端证书，正如我们刚才在`/etc/kubernetes`目录中看到的，它是服务器证书。
- en: Finally, there’s etcd. In the `/etc/kubernetes/pki/etcd` directory, you’ll also
    see a `server.crt` file, which pertains to the server certificate for etcd. The
    API server is constantly trying to authenticate to the etcd datastore and obtain
    the cluster config data. In this way, not just anyone can view or modify the cluster
    state.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有etcd。在`/etc/kubernetes/pki/etcd`目录中，你还会看到一个`server.crt`文件，它属于etcd的服务器证书。API服务器不断尝试验证到etcd数据存储并获取集群配置数据。这样，不是任何人都可以查看或修改集群状态。
- en: 2.3 Exam exercises
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 练习
- en: Increase your efficiency with running `kubectl` commands by shortening `kubectl`
    and creating a shell alias to `k`.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过缩短`kubectl`命令并创建到`k`的shell别名来提高使用`kubectl`命令的效率。
- en: Using the `kubectl` CLI tool, get the output of the Pods running in the `kube-system`
    namespace and show the Pod IP addresses. Save the output of the command to a file
    named `pod-ip-output.txt`.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl` CLI工具，获取在`kube-system`命名空间中运行的Pod的输出并显示Pod IP地址。将命令的输出保存到名为`pod-ip-output.txt`的文件中。
- en: Use the CLI tool, which allows you to view the client certificate that the kubelet
    uses to authenticate to the Kubernetes API. Output the results to a file named
    `kubelet-config.txt`.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用CLI工具，该工具允许你查看kubelet用于向Kubernetes API进行身份验证的客户端证书。将结果输出到名为`kubelet-config.txt`的文件中。
- en: Using the `etcdctl` CLI tool, back up the etcd datastore to a snapshot file
    named `etcdbackup1`. Once that backup is complete, send the output of the command
    `etcdctl snapshot status etcdbackup1` to a file named `snapshot-status.txt`.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`etcdctl` CLI工具，将etcd数据存储备份到名为`etcdbackup1`的快照文件中。一旦备份完成，将命令`etcdctl snapshot
    status etcdbackup1`的输出发送到名为`snapshot-status.txt`的文件中。
- en: Using the `etcdctl` CLI tool, restore the etcd datastore using the same `etcdbackup1`
    file from the previous exercise. When you complete the restore operation, `cat`
    the etcd YAML and save it to a file named `etcd-restore.yaml`.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`etcdctl` CLI工具，使用之前练习中相同的`etcdbackup1`文件恢复etcd数据存储。当你完成恢复操作后，使用`cat`命令查看etcd
    YAML并将其保存到名为`etcd-restore.yaml`的文件中。
- en: Upgrade the control plane components using kubeadm. When completed, check that
    everything, including kubelet and `kubectl`, is upgraded to version 1.24.0
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用kubeadm升级控制平面组件。完成后，检查包括kubelet和`kubectl`在内的一切是否已升级到版本1.24.0。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The bootstrapping process hides the complexity of creating a Kubernetes cluster,
    such as the PKI and etcd creation, generating kubeconfigs, and more.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引导过程隐藏了创建Kubernetes集群的复杂性，如PKI和etcd创建、生成kubeconfig等。
- en: The kubeconfig is created by kubeadm in the bootstrap process and is how we
    authenticate with the API using `kubectl`. This file is located in the `~/.kube/
    config` directory.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubeconfig是在引导过程中由kubeadm创建的，是我们使用`kubectl`与API进行身份验证的方式。此文件位于`~/.kube/config`目录中。
- en: Services are Kubernetes objects, providing load balancing to one or more Pods
    based on the Pod labels. Services have their own unique IP address and DNS name
    through which Pods can be reached.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务是Kubernetes对象，根据Pod标签提供对一个或多个Pod的负载均衡。服务拥有自己的唯一IP地址和DNS名称，通过这些名称可以访问Pod。
- en: The client and server certificates in the directory `/etc/kubernetes/pki` allow
    the control plane and worker nodes to authenticate to the API. Multiple Kubernetes
    components can have both client and server certificates.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目录`/etc/kubernetes/pki`中的客户端和服务器证书允许控制平面和工作节点对API进行身份验证。多个Kubernetes组件都可以有客户端和服务器证书。
- en: Etcd is a key-value datastore that contains the cluster configuration, including
    how many objects exist. To back up etcd, the command `etcdctl snapshot save` is
    used, and to restore from snapshot, the command `etcdctl snapshot restore` is
    used.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd是一个键值数据存储，包含集群配置，包括存在多少对象。要备份etcd，使用命令`etcdctl snapshot save`，要从快照恢复，使用命令`etcdctl
    snapshot restore`。
- en: The directory `/etc/kubernetes/manifests` contains YAML files for the API server,
    controller manager, scheduler, and etcd. These files are not detected by the scheduler;
    therefore, kubelet will automatically pick up and create whatever is in this directory.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目录`/etc/kubernetes/manifests`包含API服务器、控制器管理器、调度器和etcd的YAML文件。这些文件不会被调度器检测；因此，kubelet将自动获取并创建此目录中的内容。
- en: In a multinode cluster, the worker node runs kubelet, kube-proxy, and the container
    runtime, whereas the control plane also runs the kubelet, controller manager,
    scheduler, and etcd. To prevent application Pods from running on the control plane,
    a taint is applied during the bootstrap process.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多节点集群中，工作节点运行kubelet、kube-proxy和容器运行时，而控制平面也运行kubelet、控制器管理器、调度器和etcd。为了防止应用Pod在控制平面运行，在引导过程中应用了一个污点（taint）。
