- en: 16 Production infrastructure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16 生产基础设施
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing passive retraining with the use of a model registry
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型注册表实现被动再训练
- en: Utilizing a feature store for model training and inference
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用特征存储进行模型训练和推理
- en: Selecting an appropriate serving architecture for ML solutions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习解决方案选择合适的托管架构
- en: Utilizing ML in a real-world use case to solve a complex problem is challenging.
    The sheer number of skills needed to take a company’s data (frequently messy,
    partially complete, and rife with quality issues), select an appropriate algorithm,
    tune a pipeline, and validate that the prediction output of a model (or an ensemble
    of models) solves the problem to the satisfaction of the business is daunting.
    The complexity of an ML-backed project does not end with the creation of an acceptably
    performing model, though. The architectural considerations and implementation
    details can add significant challenges to a project if they aren’t made correctly.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际用例中利用机器学习解决复杂问题具有挑战性。需要掌握的技能数量巨大，以从公司的数据（通常是杂乱无章、部分完整且充满质量问题）中提取数据，选择合适的算法，调整管道，并验证模型（或模型集合）的预测输出是否满足业务需求，这令人望而生畏。尽管创建了一个可接受性能的模型，但机器学习支持的项目复杂性并未结束。如果做得不正确，架构考虑和实现细节可能会给项目带来重大挑战。
- en: Every day there seems to be a new open sourced tech stack that promises an easier
    deployment strategy or a magical automated solution that meets the needs of all.
    With this constant deluge of tools and platforms, making a decision on where to
    go to meet the needs of a particular project can be intimidating.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 每天似乎都有新的开源技术栈承诺更简单的部署策略或神奇的自动化解决方案，以满足所有需求。随着这些工具和平台的不断涌入，决定如何满足特定项目的需求可能会令人感到畏惧。
- en: A cursory glance at the offerings available may seem to indicate that the most
    logical plan is to stick to a single paradigm for everything (for example, deploy
    every model as a REST API service). Keeping every ML project aligned in a common
    architecture and implementation certainly simplifies the release deployment. However,
    nothing could be further from the truth. Just as when selecting algorithms, there’s
    no “one size fits all” for production infrastructure.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 初看可用的产品，可能会让人觉得最合理的计划是坚持单一范式（例如，将每个模型作为REST API服务部署）。确保每个机器学习项目都遵循共同的架构和实现确实简化了发布部署。然而，事实并非如此。正如在选择算法时，没有“一刀切”的生产基础设施。
- en: The goal of this chapter is to introduce common generic themes and solutions
    that can be applied to model prediction architecture. After covering the basic
    tooling that obfuscates the complexity and minutiae of production ML services,
    we will delve into generic architectures that can be employed to meet the needs
    of different projects.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍可以应用于模型预测架构的通用通用主题和解决方案。在介绍掩盖生产机器学习服务复杂性和细节的基本工具之后，我们将深入研究可以满足不同项目需求的通用架构。
- en: The goal in any serving architecture is to build the minimally featured, least
    complex, and cheapest solution that still meets the needs of consuming the model’s
    output. With consistency and efficiency in serving (SLA and prediction-volume
    considerations) as the primary focus for production work, there are several key
    concepts and methodologies to be aware of to make this last-mile aspect of ML
    project work as painless as possible.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 任何托管架构的目标是构建具有最少功能、最简单和最经济的解决方案，同时仍能满足消费模型输出的需求。以服务的一致性和效率（SLA和预测量考虑）作为生产工作的主要焦点，有几个关键概念和方法需要了解，以使机器学习项目工作的最后一公里尽可能无痛。
- en: 16.1 Artifact management
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 艺术品管理
- en: Let’s imagine that we’re still working at the fire-risk department of the forest
    service introduced in chapter 15\. In our efforts to effectively dispatch personnel
    and equipment to high-risk areas in the park system, we’ve arrived at a solution
    that works remarkably well. Our features are locked in and are stable over time.
    We’ve evaluated the performance of the predictions and are seeing genuine value
    from the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象我们仍在第15章中介绍的森林服务火灾风险部门的火险部门工作。在我们努力有效地派遣人员和设备到公园系统中的高风险区域的过程中，我们找到了一个效果显著的方法。我们的特征已经锁定，并且随着时间的推移保持稳定。我们已经评估了预测的性能，并从模型中看到了真正的价值。
- en: Throughout this process of getting the features into a good state, we’ve been
    iterating through the improvement cycle, shown in figure 16.1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在将特征状态调整得好的整个过程中，我们一直在迭代改进周期，如图16.1所示。
- en: '![16-01](../Images/16-01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![16-01](../Images/16-01.png)'
- en: Figure 16.1 Improvements to a deployed model on the road to production steady-state
    operation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 部署模型在生产稳态操作过程中的改进
- en: As this cycle shows, we’ve been iteratively releasing new versions of the model,
    testing against a baseline deployment, collecting feedback, and working to improve
    the predictions. At some point, however, we’ll be going into model-sustaining
    mode.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如此循环所示，我们一直在迭代发布模型的新版本，与基线部署进行测试，收集反馈，并努力改进预测。然而，在某个时候，我们将进入模型维持模式。
- en: We’ve worked as hard as we can to improve the features going into the model
    and have found that the return on investment (ROI) of continuing to add new data
    elements to the project is simply not worth it. We’re now in the position of scheduled
    passive retraining of our model based on new data coming in over time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经尽最大努力改进进入模型的特征，并发现继续向项目中添加新数据元素的投资回报率（ROI）根本不值得。我们现在处于根据随时间到来的新数据进行模型计划被动重新训练的位置。
- en: When we’re at this steady-state point, the last thing that we want to do is
    to have one of the DS team members spend an afternoon manually retraining a model,
    manually comparing its results to the current production-deployed model with ad
    hoc analysis, and deciding on whether the model should be updated.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们达到这个稳态点时，我们最不想做的就是让DS团队的一员花一个下午的时间手动重新训练一个模型，手动比较其结果与当前生产部署的模型，并通过临时分析来决定是否更新模型。
- en: Oh, come on. No one does this manually.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，拜托。没有人会手动做这件事。
- en: From my own history as a DS, I didn’t start using passive retraining for the
    first six years of solving problems. It wasn’t due to a lack of need, nor a lack
    of tooling. It was pure and simple ignorance. I had no idea how big of a problem
    drift could be (I learned that the hard way several times over by having a solution
    devolve into irrelevance because of my neglect). Nor did I understand or appreciate
    the importance of attribution calculations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从我作为一个数据科学家（DS）的历史来看，我在解决问题的前六年没有开始使用被动重新训练。这并不是因为缺乏需求，也不是因为缺乏工具。纯粹是因为无知。我不知道问题漂移可能造成多大的问题（我多次通过解决方案变得无关紧要来艰难地了解到这一点，因为我忽视了它）。我也不理解或欣赏归因计算的重要性。
- en: 'Over years of repeatedly screwing up my solutions, I found techniques that
    others had written about through researching solutions to my self-imposed woes
    of inadequately engineered projects. I came to embrace the ideas that led me to
    DS work to begin with: automating annoying and repetitive tasks. By removing the
    manual activity of monitoring the health of my projects (via ad hoc drift tracking),
    I found that I had solved two primary problems that were plaguing me.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 经过多年的反复犯错，我发现了一些通过研究解决我自设的项目工程不足的难题而写下的技术。我接受了最初让我进入DS工作的想法：自动化令人讨厌和重复的任务。通过移除手动监控项目健康状况的活动（通过临时漂移跟踪），我发现我解决了困扰我的两个主要问题。
- en: First, I freed up my time. Doing ad hoc analyses on prediction results and feature
    stability takes a lot of time. In addition, it’s incredibly boring work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我释放了自己的时间。对预测结果和特征稳定性进行临时分析花费了很多时间。此外，这项工作极其无聊。
- en: The second big problem was in accuracy. Manually evaluating model performance
    is repetitive and error-prone. Missing details through a manual analysis can mean
    deploying a model version that is worse than the currently deployed one, introducing
    issues that are far more significant than a slightly poorer prediction performance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个大问题是准确性。手动评估模型性能是重复且易出错的。通过手动分析遗漏的细节可能导致部署的模型版本比当前部署的版本更差，引入的问题比略微较差的预测性能要严重得多。
- en: I’ve learned my lesson about automating retraining (typically opting for passive
    retraining systems rather than the far more complex active ones if I can get away
    with it). As with everything else I’ve learned in my career, I’ve learned it by
    screwing it up. Hopefully, you can avoid the same fate.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经从自动化重新训练中吸取了教训（通常在可能的情况下选择被动重新训练系统而不是更复杂的主动系统）。就像我在职业生涯中学到的其他所有东西一样，我是通过犯错误来学习的。希望你们能避免同样的命运。
- en: The measurement, adjudication, and decision on whether to replace the model
    with a newly retrained one can be automated with a passive retraining system.
    Figure 16.2 shows this concept of a scheduled retraining event.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用被动重新训练系统自动测量、裁决以及决定是否用新重新训练的模型替换现有模型是可能的。图16.2展示了计划重新训练事件的概念。
- en: '![16-02](../Images/16-02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![16-02](../Images/16-02.png)'
- en: Figure 16.2 Logical diagram of a passive retraining system
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2被动重新训练系统的逻辑图
- en: With this automation of scheduled retraining in place, the primary concern with
    this system is knowing what is running in production. For instance, what happens
    if a problem is uncovered in production after a new version is released? What
    can we do to recover from a concept drift that has dramatically affected a retraining
    event? How do we roll back the model to the previous version without having to
    rebuild it? We can allay these concerns by using a model registry.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在此安排的重新训练自动化实施后，这个系统的主要关注点是了解生产中正在运行的内容。例如，如果在新版本发布后生产中发现了问题，会发生什么？我们如何从对重新训练事件产生重大影响的漂移概念中恢复过来？我们如何在不重建模型的情况下将模型回滚到上一个版本？我们可以通过使用模型注册表来缓解这些担忧。
- en: 16.1.1 MLflow’s model registry
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.1 MLflow的模型注册表
- en: In this situation that we find ourselves in, with scheduled updates to a model
    happening autonomously, it is important for us to know the state of production
    deployment. Not only do we need to know the current state, but if questions arise
    about performance of a passive retraining system in the past, we need to have
    a means of investigating the historical provenance of the model. Figure 16.3 compares
    using and not using a registry for tracking provenance in order to explain a historical
    issue.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们目前所处的这种情况下，模型自动进行计划更新，了解生产部署的状态对我们来说非常重要。我们不仅需要了解当前状态，而且如果关于被动重新训练系统过去的表现出现疑问，我们需要有一种方法来调查模型的历史来源。图16.3比较了使用和不使用注册表来跟踪来源，以解释历史问题。
- en: '![16-03](../Images/16-03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![16-03](../Images/16-03.png)'
- en: Figure 16.3 Passive retraining schedule with a historic issue found far in the
    future
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3被动重新训练计划中发现的远期历史问题
- en: As you can see, the process for attempting to re-create a past run is fraught
    with peril; we have a high risk of being unable to reproduce the issue that the
    business found in historical predictions. With no registry to record the artifacts
    utilized in production, manual work must be done to re-create the model’s original
    conditions. This can be incredible challenging (if not impossible) in most companies
    because changes may have occurred to the underlying data used to train the model,
    rendering it impossible to re-create that state.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，尝试重新创建过去运行的流程充满了危险；我们面临很高的风险，无法重现业务在历史预测中发现的那个问题。由于没有注册表来记录生产中使用的工件，必须手动工作来重新创建模型的原始条件。这在大多数公司中可能非常具有挑战性（如果不是不可能的话），因为用于训练模型的基本数据可能已经发生变化，使得无法重新创建那种状态。
- en: The preferred approach, as shown in figure 16.3, is to utilize a model registry
    service. MLflow, for instance, offers exactly this functionality within its APIs,
    allowing us to log details of each retraining run to the tracking server, handle
    production promotion if the scheduled retraining job performs better on holdout
    data, and archive the older model for future reference. If we had used this framework,
    the process of testing conditions of a model that had at one point run in production
    would be as simple as recalling the artifact from the registry entry, loading
    it into a notebook environment, and generating the explainable correlation reports
    with tools such as `shap`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如图16.3所示，首选的方法是利用模型注册表服务。例如，MLflow在其API中提供了这一功能，允许我们将每次重新训练运行的详细信息记录到跟踪服务器，如果计划中的重新训练作业在保留数据上表现更好，则处理生产推广，并将旧模型存档以供将来参考。如果我们使用了这个框架，测试曾经在生产中运行过的模型的条件过程将简单到只需从注册条目中召回工件，将其加载到笔记本环境中，并使用`shap`等工具生成可解释的相关报告。
- en: Is a registry really that important?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注册表真的那么重要吗？
- en: Well, in two words, “It depends.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，用两个字来说，“视情况而定。”
- en: I remember with a distinct spine-chilling horror one of my first major, real-deal,
    no-kidding, really serious ML implementations that I built. It wasn’t by any means
    my first production release of a solution, but it was the first one that had serious
    attention being paid to it. It helped to run a rather significant part of the
    business, and as such, was closely scrutinized by a lot of people. Rightfully
    so, if I may add.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得我第一次真正的大规模、严肃的、认真的机器学习实现，有一种令人毛骨悚然的恐惧。这绝对不是我的第一个解决方案的生产发布，但这是第一个受到严重关注的。它帮助运行了业务的一个重要部分，因此受到了许多人的密切审查。如果我可以补充的话，这是理所当然的。
- en: My deployment (if it could be called that) involved a passive-like retraining
    system that stored the last-known-good hyperparameters of the previous day’s tuning
    run, using those values as a starting point to begin automated tuning. After optimizing
    to all of the new feature-training data available, it chose the best-performing
    model, ran a prediction on the new data, and overwrote a serving table with the
    predictions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我的部署（如果可以这么称呼的话）涉及一个类似被动式再训练的系统，该系统存储了前一天调优运行中最后已知的良好超参数，使用这些值作为起点开始自动化调优。在优化了所有新的特征训练数据后，它选择了表现最佳的模型，对新数据进行预测，并用预测结果覆盖了服务表。
- en: It wasn’t until a full three months into the project’s production run that the
    first serious question came up regarding why the model was predicting in an unexpected
    way with certain customers. The business leaders couldn’t figure out why it was
    doing that, so they approached me and asked me to investigate.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 直到项目生产运行满三个月后，才出现了关于为什么模型以某种意想不到的方式预测某些客户的第一个严重问题。业务领导无法弄清楚为什么它会这样做，所以他们来找我，让我调查。
- en: Having no record of the model (it wasn’t even saved anywhere) and realizing
    that the training data was changing consistently over time as the features updated
    made it completely impossible for me to explain the model’s historical performance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有模型的记录（甚至没有保存任何地方），并且意识到训练数据随着时间的推移而持续变化，因为特征在更新，这使得我完全无法解释模型的历史性能。
- en: The business was less than pleased with this answer. Although the model didn’t
    get shut off (it probably should have), it made me realize the importance of storing
    and cataloguing models for the precise purpose of being able to explain why the
    solution behaves the way it does, even if that explanation is months past the
    point at which it was being used.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 业务对这种回答不太满意。尽管模型没有被关闭（它可能应该被关闭），但它让我意识到存储和编目模型的重要性，以便能够解释为什么解决方案以这种方式表现，即使这种解释是在它被使用后的几个月。
- en: 16.1.2 Interfacing with the model registry
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1.2 与模型注册接口
- en: To get a feel for how this code would look to support an integration with the
    model registry service of MLflow, let’s adapt our use case to support this passive
    retraining functionality. To start, we need to create an adjudication system that
    checks the current production model’s performance against the scheduled retraining
    results. After building that comparison, we can interface with the registry service
    to replace the current production model with the newer model (if it’s better),
    or stay with the current production model based on its performance against the
    same holdout data that the new model was tested against.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这段代码如何支持与MLflow模型注册服务的集成，让我们将我们的用例调整为支持这种被动式再训练功能。首先，我们需要创建一个裁决系统，该系统检查当前生产模型的性能与计划再训练结果之间的比较。在构建了这种比较之后，我们可以与注册服务接口，用较新的模型（如果它更好）替换当前的生产模型，或者根据与新的模型测试的相同保留数据，保持当前的生产模型。
- en: Let’s look at an example of how to interface with the MLflow model registry
    to support automated passive retraining that retains provenance of the model’s
    state over time. Listing 16.1 establishes the first portion of what we need to
    build to have a historical status table of each scheduled retraining event.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何与MLflow模型注册接口支持自动化的被动式再训练，并保留模型状态随时间变化的来源。列表16.1建立了构建每个计划再训练事件的历史状态表所需的第一部分。
- en: NOTE To see all of the `import` statements and the full example that integrates
    with these snippets, see the companion notebook to this chapter in the GitHub
    repository for this book at [https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要查看所有 `import` 语句和与这些片段集成的完整示例，请参阅 GitHub 仓库中本书此章节的配套笔记本，网址为 [https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering)。
- en: Listing 16.1 Registry state row generation and logging
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.1 注册状态行生成和记录
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ A data class to wrap the data we’re going to be logging
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于包装我们将要记录的数据的数据类
- en: ❷ Class for converting the registration data to a Spark DataFrame to write a
    row to a delta table for provenance
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于将注册数据转换为 Spark DataFrame 以将行写入用于溯源的 delta 表的类
- en: ❸ Accesses the members of the data class in a shorthand fashion to cast to a
    pandas DataFrame and then a Spark DataFrame (leveraging implicit type inferences)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 以简写方式访问数据类的成员，以将其转换为 pandas DataFrame，然后转换为 Spark DataFrame（利用隐式类型推断）
- en: ❹ Builds the Spark DataFrame row at class initialization
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在类初始化时构建 Spark DataFrame 行
- en: ❺ Method for determining if the delta table has been created yet
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 确定delta表是否已经创建的方法
- en: ❻ Writes the log data to Delta in append mode and creates the table reference
    in the Hive Metastore if it doesn’t already exist
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 以追加模式将日志数据写入 Delta，并在 Hive Metastore 中创建表引用（如果尚未存在）
- en: This code helps set the stage for the provenance of the model-training history.
    Since we’re looking to automate the retraining on a schedule, it’s far easier
    to have a tracking table that refers to the history of changes in a centralized
    location. If we have multiple builds of this model, as well as other projects
    that are registered, we can have a single snapshot view of the state of production
    passive retraining without needing to do anything more than write a simple query.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有助于为模型训练历史的溯源做好准备。由于我们希望按计划自动化重新训练，因此拥有一个引用集中位置中更改历史的跟踪表要容易得多。如果我们有多个此模型的构建，以及其他已注册的项目，我们可以有一个单个快照视图来查看生产被动重新训练的状态，而无需做任何更多的事情，只需编写一个简单的查询。
- en: Listing 16.2 illustrates what a query of this table would look like. With multiple
    models logged to a transaction history table like this, adding `df.filter(F.col("model_
    name"``==``"<project``title>")` allows for rapid access to the historical log
    for a single model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.2 展示了查询此表将看起来是什么样子。将多个模型记录到这种事务历史表中，添加 `df.filter(F.col("model_name")
    == "<project title>")` 可以快速访问单个模型的日志历史。
- en: Listing 16.2 Querying the registry state table
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.2 查询注册状态表
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Since we’ve registered the table in our row-input stage earlier, we can refer
    to it directly by <database>.<table_name> reference. We can then order the commits
    chronologically.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于我们之前在行输入阶段已注册了该表，我们可以通过 <database>.<table_name> 引用直接引用它。然后我们可以按时间顺序对提交进行排序。
- en: Executing this code results in figure 16.4\. In addition to this log, the model
    registry within MLflow also has a GUI. Figure 16.5 shows a screen capture of the
    GUI that matches to the registry table from listing 16.2.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将产生图 16.4。除了这个日志之外，MLflow 中的模型注册也提供了一个 GUI。图 16.5 展示了与列表 16.2 中的注册表相匹配的
    GUI 屏幕截图。
- en: '![16-04](../Images/16-04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![16-04](../Images/16-04.png)'
- en: Figure 16.4 Querying the registry state transaction table
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4 查询注册状态事务表
- en: Now that we’ve set up the historical tracking functionality, we can write the
    interface to MLflow’s registry server to support passive retraining. Listing 16.3
    shows the implementation for leveraging the tracking server’s entries, the registry
    service for querying current production metadata, and an automated state transition
    of the retrained model for supplanting the current production model if it performs
    better.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了历史跟踪功能，我们可以编写与 MLflow 注册服务器的接口以支持被动重新训练。列表 16.3 展示了利用跟踪服务器条目、查询当前生产元数据的注册服务以及自动状态转换重新训练模型以取代当前生产模型的实现。
- en: '![16-05](../Images/16-05.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![16-05](../Images/16-05.png)'
- en: Figure 16.5 The MLflow model registry GUI for our experiments
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5 我们实验的 MLflow 模型注册 GUI
- en: Listing 16.3 Passive retraining model registration logic
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.3 被动重新训练模型注册逻辑
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Extracts all the previous run data for the history of the production deployment
    and returns the run ID that has the best performance against the validation data
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从生产部署的历史中提取所有之前的运行数据，并返回针对验证数据的最佳性能的运行 ID
- en: ❷ Query for the model currently registered as “production deployed’ in the registry
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 查询当前在注册表中注册为“生产部署”的模型
- en: ❸ Method for determining if the current scheduled passive retraining run is
    performing better than production on its holdout data. It will return the run_id
    of the best logged run.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 确定当前计划中的被动重新训练运行是否在其保留数据上优于生产的查询方法。它将返回最佳记录运行的 run_id。
- en: ❹ Utilizes the MLflow Model Registry API to register the new model if it is
    better, and de-registers the current production model if it’s being replaced
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果新模型表现更好，则利用 MLflow 模型注册表 API 进行注册，如果正在替换，则注销当前生产模型
- en: ❺ Acquires the current production model for batch inference on a Spark DataFrame
    using a Python UDF
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 Python UDF 在 Spark DataFrame 上获取当前生产模型以进行批量推理
- en: This code allows us to fully manage the passive retraining of this model implementation
    (see the companion GitHub repository for this book for the full code). By leveraging
    the MLflow Model Registry API, we can meet the needs of production-scheduled predictions
    through having a one-line access to the model artifact.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码允许我们完全管理此模型实现的被动重新训练（有关完整代码，请参阅本书的配套 GitHub 仓库）。通过利用 MLflow 模型注册表 API，我们可以通过一行代码访问模型工件来满足生产调度预测的需求。
- en: This greatly simplifies the prediction batch-scheduled job, but also meets the
    needs of the investigation we began discussing in this section. Having the ability
    to retrieve the model with such ease, we can manually test the feature data against
    that model, run simulations with the use of tools like `shap`, and rapidly answer
    business questions without having to struggle with re-creating a potentially impossible
    state.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这极大地简化了预测批量调度作业，同时也满足了我们在本节开始讨论的调查需求。有了如此轻松检索模型的能力，我们可以手动将特征数据与该模型进行测试，使用 `shap`
    等工具进行模拟，并快速回答业务问题，而无需努力重新创建可能无法实现的状态。
- en: In the same vein of using a model registry to keep track of the model artifacts,
    the features being used to train models and predict with the use of models can
    be catalogued for efficiency’s sake as well. This concept is realized through
    feature stores.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用模型注册表跟踪模型工件的同一路线上，用于训练模型和用模型进行预测的特征也可以为了效率而编目。这一概念通过特征存储来实现。
- en: That’s cool and all, but what about active retraining?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这很酷，但关于主动重新训练呢？
- en: The primary difference between passive retraining and active retraining lies
    in the mechanism of initiating retraining.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 被动重新训练和主动重新训练之间的主要区别在于触发重新训练的机制。
- en: Passive, scheduled by CRON, is a “best hope” strategy that attempts to find
    an improved model fit by incorporating new training data in the effort to counteract
    drift. Active, on the other hand, monitors the state of predictions and features
    to determine algorithmically when it makes sense to trigger a retraining.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 被动式，由 CRON 调度，是一种“最佳希望”策略，试图通过结合新的训练数据来寻找改进的模型拟合度，以对抗漂移。另一方面，主动式则监控预测状态和特征，以算法方式确定何时触发重新训练。
- en: Because it is designed to respond to unpredictable performance degradation,
    an active system can be beneficial if drift is happening at unpredictable rates—for
    instance, a model has been performing well for weeks, falls apart in the span
    of a few days, gets retrained, and performs well for only a few days before needing
    retraining. To create this responsive feedback loop to trigger a retraining event,
    prediction quality needs to be monitored. A system needs to be built to generate
    a retraining signal; this system ingests the predictions, merges the highly variable
    nature of ground-truth results that arrive at a later point (in some cases, seconds,
    at other times, weeks later), and effectively sets statistically significant thresholds
    on aggregated result states over time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它旨在应对不可预测的性能下降，如果漂移以不可预测的速度发生，则主动系统可能有益——例如，一个模型已经表现良好几周，但在几天内崩溃，重新训练后仅表现良好几天，然后又需要重新训练。为了创建这种响应式反馈循环以触发重新训练事件，需要监控预测质量。需要构建一个系统来生成重新训练信号；该系统消耗预测，合并后来到达的（在某些情况下，几秒后，在其他情况下，几周后）高度可变的地标结果，并在时间上对聚合结果状态设置统计上显著的阈值。
- en: These systems are highly dependent on the nature of the problem being solved
    by the ML, and as such, vary in their design and implementation so much that even
    a generic example architecture is irrelevant for presentation here.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统高度依赖于ML解决的问题的本质，因此它们的设计和实现差异很大，以至于即使是通用的示例架构在这里也不相关。
- en: For instance, if you’re trying to determine the success of a model’s ability
    to predict the weather in the next hour in a certain location, you can get feedback
    within an hour. You could build a system that merges the hour-lagged real weather
    against the predictions, feeding the actual model accuracy into a windowed aggregation
    of accuracy rate over the last 48 hours. Should the aggregated rate of success
    in weather forecasting drop below a defined threshold of 70%, a retraining of
    the model can be initiated autonomously. This newly trained model can be compared
    against the current production model by validating both models through a standard
    (new) holdout validation dataset. The new model can then be used either immediately
    through a blue/green deployment strategy or gradually by having traffic dynamically
    allocated to it with a multi-bandit algorithm that routes traffic based on relative
    performance improvement compared to the current production model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你试图确定某个地点在下一个小时内预测天气模型成功性的成功，你可以在一个小时内获得反馈。你可以构建一个系统，将滞后一个小时的实时天气与预测合并，将实际模型准确性输入到过去48小时内准确性率的窗口聚合中。如果天气预测的成功率聚合低于定义的70%阈值，可以自动启动模型的再训练。这个新训练的模型可以通过通过验证两个模型的标准（新）保留验证数据集来与当前生产模型进行比较。然后，可以通过蓝/绿部署策略立即使用新模型，或者通过具有多臂老虎机算法的动态流量分配来逐渐使用它，该算法根据与当前生产模型的相对性能改进来路由流量。
- en: Active retraining is complex, in a nutshell. I recommend that people investigate
    it only after finding that passive retraining simply isn’t cutting it anymore,
    rather than just because it seems like it’s important. There are far more moving
    parts, services, and infrastructure to handle when autonomously handling retraining.
    The cloud services bill that you get when using active retraining will reflect
    the increase in complexity as well (it’s expensive).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 概括来说，主动再训练是复杂的。我建议人们在发现被动再训练已经不再奏效时才去研究它，而不是仅仅因为它看起来很重要。在自主处理再训练时，有更多的部分、服务和基础设施需要处理。使用主动再训练时，你收到的云服务账单也会反映出复杂性的增加（它很昂贵）。
- en: 16.2 Feature stores
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 特征存储
- en: We briefly touched on using a feature store in the preceding chapter. While
    it is important to understand the justification for and benefits of implementing
    a feature store (namely, that of consistency, reusability, and testability), seeing
    an application of a relatively nascent technology is more relevant than discussing
    the theory. Here, we’re going to look at a scenario that I struggled through,
    involving the importance of utilizing a feature store to enforce consistency throughout
    an organization leveraging both ML and advanced analytics.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章简要提到了使用特征存储。虽然理解实施特征存储的合理性和好处（即一致性、可重用性和可测试性）很重要，但看到相对较新的技术的应用比讨论理论更为相关。在这里，我们将探讨一个我努力克服的场景，涉及利用特征存储在利用机器学习和高级分析的组织中强制执行一致性的重要性。
- en: Let’s imagine that we work at a company that has multiple DS teams. Within the
    engineering group, the main DS team focuses on company-wide initiatives. This
    team works mostly on large-scale projects involving critical services that can
    be employed by any group within the company, as well as customer-facing services.
    Spread among departments are a smattering of independent contributor DS employees
    who have been hired by and report to their respective department heads. While
    collaboration occurs, the main datasets used by the core DS team are not open
    for the independent DS employees’ use.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想我们在一个拥有多个数据科学（DS）团队的公司工作。在工程组内，主要的DS团队专注于公司范围内的项目。这个团队主要处理涉及关键服务的大型项目，这些服务可以被公司内的任何团队使用，以及面向客户的服务。在各个部门中散布着一些独立的贡献型DS员工，他们由各自的部门负责人雇佣并汇报。虽然存在协作，但核心DS团队使用的主要数据集并不对独立的DS员工开放。
- en: At the start of a new year, a department head hires a new DS straight out of
    a university program. Well-intentioned, driven, and passionate, this new hire
    immediately gets to work on the initiatives that this department head wants investigated.
    In the process of analyzing the characteristics of the customers of the company,
    the new hire come across a production table that contains probabilities for customers
    to make a call-center complaint. Curious, the new DS begins analyzing the predictions
    against the data that is in the data warehouse for their department.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在新年的开始，部门主管雇佣了一位刚从大学项目中毕业的新DS。这位新员工有良好的意图，有动力，充满热情，立即开始着手进行这位部门主管希望调查的倡议。在分析公司客户特征的过程中，这位新员工遇到了一个包含客户拨打客服中心投诉概率的生产表。出于好奇，这位新的DS开始分析预测与他们在部门数据仓库中的数据之间的对比。
- en: Unable to reconcile any feature data to the predictions, the DS begins working
    on a new model prototype to try to improve upon the complaint prediction solution.
    After spending a few weeks, the DS presents their findings to their department
    head. Given the go-ahead to work on this project, the DS proceeds to build a project
    in their analytics department workspace. After several months, the DS presents
    their findings at a company all-hands meeting.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 无法将任何特征数据与预测结果相匹配，数据科学家（DS）开始着手构建一个新的模型原型，试图改进投诉预测解决方案。经过几周的努力，DS向他们的部门主管展示了他们的发现。在获得继续进行这个项目的批准后，DS开始在他们的分析部门工作空间中构建项目。几个月后，DS在公司全体员工会议上展示了他们的发现。
- en: 'Confused, the core DS team asks why this project is being worked on and for
    further details on the implementation. In less than an hour, the core DS team
    is able to explain why the independent DS’s solution worked so well: they leaked
    the label. Figure 16.6 illustrates the core DS team’s explanation: the data required
    to build any new model or perform extensive analysis of the data collected from
    users is walled off by the silo surrounding the core DS team’s engineering department.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 感到困惑，核心DS团队询问为什么这个项目正在进行，并要求进一步了解实施细节。不到一个小时，核心DS团队就能解释为什么独立DS的解决方案如此有效：他们泄露了标签。图16.6展示了核心DS团队的解释：构建任何新模型或对从用户收集的数据进行广泛分析所需的数据被核心DS团队工程部门周围的隔阂所隔离。
- en: '![16-06](../Images/16-06.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![16-06](../Images/16-06.png)'
- en: Figure 16.6 The engineering silo that keeps raw data and calculated features
    away from the rest of the organization
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 隔离工程部门，将原始数据和计算特征与其他组织部分隔离开
- en: The data being used for training that was present in the department’s data warehouse
    was being fed from the core DS team’s production solution. Each source feature
    used to train the core model was inaccessible to anyone apart from engineering
    and production processes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练的数据，存在于部门的数据仓库中，是由核心DS团队的生产解决方案提供的。用于训练核心模型的每个源特征对工程和生产流程之外的人不可访问。
- en: While this scenario is extreme, it did, in fact, happen. The core team could
    have helped to avoid this by providing an accessible source for the generated
    feature data, opening the access to allow other teams to utilize these highly
    curated data points for additional projects. By registering their data with appropriate
    labels and documentation, they could have saved this poor DS a lot of effort.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个场景是极端的，但它确实发生了。核心团队本可以通过提供一个可访问的生成特征数据源，开放访问权限，允许其他团队利用这些高度精选的数据点进行额外项目，从而帮助避免这种情况。通过将他们的数据与适当的标签和文档注册，他们可以节省这位可怜的DS大量的努力。
- en: 16.2.1 What a feature store is used for
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.1 特征存储的用途
- en: Solving the data silo issue in our scenario is among the most compelling reasons
    to use a feature store. When dealing with a distributed DS capability throughout
    an organization, the benefits of standardization and accessibility are seen through
    a reduction in redundant work, incongruous analyses, and general confusion surrounding
    the veracity of solutions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 解决我们场景中的数据孤岛问题是使用特征存储的最有说服力的理由之一。当在一个组织内处理分布式DS能力时，通过减少重复工作、不一致的分析和围绕解决方案真实性的普遍困惑，可以看到标准化和可访问性的好处。
- en: However, having a feature store enables an organization to do far more with
    its data than just quality-control it. To illustrate these benefits, figure 16.7
    shows a high-level code architecture for model building and serving with and without
    a feature store.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有特征存储使组织能够利用其数据进行比仅仅进行质量控制更多的操作。为了说明这些好处，图16.7展示了带有和不带有特征存储的模型构建和服务的代码架构。
- en: The top portion of figure 16.7 shows the historical reality of ML development
    for projects. Tightly coupled feature-engineering code is developed inline to
    the model tuning and training code to generate models that are more effective
    than they would be if trained on the raw data. While this architecture makes sense
    from a development perspective of generating a good model, it creates an issue
    when developing the prediction code base (as shown at the top right of figure
    16.7).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7的上半部分显示了ML开发项目的历史现实。紧密耦合的特征工程代码直接与模型调优和训练代码一起开发，以生成比仅用原始数据进行训练更有效的模型。虽然从开发角度来看，这种架构对于生成一个好的模型是有意义的，但它会在开发预测代码库时产生问题（如图16.7右上角所示）。
- en: '![16-07](../Images/16-07.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![16-07](../Images/16-07.png)'
- en: Figure 16.7 Comparison of using a feature store versus not using one for ML
    development
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 使用特征存储与不使用特征存储进行机器学习开发的比较
- en: 'Any operations that are done to the raw data now need to be ported over to
    this serving code, presenting an opportunity for errors and inconsistencies in
    the model vector. Alternatives to this approach can help eliminate the chances
    of data inconsistency, however:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在需要对原始数据进行的所有操作都需要迁移到这个服务代码中，这为模型向量中的错误和不一致性提供了机会。然而，这种方法的替代方案可以帮助消除数据不一致的可能性：
- en: Use a pipeline (most major ML frameworks have them).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道（大多数主要机器学习框架都有）。
- en: Abstract feature-engineering code into a package that training and serving can
    both call.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征工程代码抽象成一个包，训练和提供都可以调用。
- en: Write traditional ETL to generate features and store them.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写传统的ETL来生成特征并将它们存储起来。
- en: Each of these approaches has its own downsides, though. Pipelines are great
    and should be used, but they entangle useful feature-engineering logic with a
    particular model’s implementation, isolating it from being utilized elsewhere.
    There’s simply no easy way to reuse the features for other projects (not to mention
    it’s nearly impossible for an analyst to decouple the feature-engineering stages
    from an ML pipeline without help).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都有其自身的缺点。管道很棒，应该使用，但它们将有用的特征工程逻辑与特定模型的实现纠缠在一起，使其无法在其他地方被利用。简单地没有容易的方法来重用这些特征用于其他项目（更不用说分析师在没有帮助的情况下几乎不可能将特征工程阶段从机器学习管道中分离出来）。
- en: Abstracting feature-engineering code certainly helps with code reusability and
    solves the consistency problem for the projects requiring the use of those features.
    But access to these features outside the DS team is still walled off. The other
    downside is that it’s another code base that needs to be maintained, tested, and
    frequently updated.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象特征工程代码确实有助于代码重用，并解决了需要使用这些特征的项目的一致性问题。但访问这些特征在DS团队之外仍然受到限制。另一个缺点是，它又是一个需要维护、测试和频繁更新的代码库。
- en: Let’s look at an example of interacting with a feature store, using the Databricks
    implementation to see the benefits in action.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看与特征存储交互的例子，使用Databricks实现来看到实际的好处。
- en: NOTE Implementations of features of this nature that are built by a company
    are subject to change. APIs, feature details, and associated functionality may
    change, sometimes quite significantly, over time. This example of one such implementation
    of a feature store is presented for demonstration purposes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：公司构建的此类特征实现可能会发生变化。API、特征细节和相关功能可能会随着时间的推移而改变，有时变化相当显著。此示例展示了一个特征存储的实现，用于演示目的。'
- en: 16.2.2 Using a feature store
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.2 使用特征存储
- en: The first step in utilizing a feature store is to define a DataFrame representation
    of the processing involved in creating the features we’d like to use for modeling
    and analytics. The following listing shows a list of functions that are acting
    on a raw dataset to generate new features.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 利用特征存储的第一步是定义一个DataFrame表示，用于创建我们希望用于建模和分析的特征的涉及的处理过程。以下列表显示了一组函数，这些函数正在对原始数据集进行操作以生成新特征。
- en: Listing 16.4 Feature-engineering logic
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.4 特征工程逻辑
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ General cleanup to strip out leading whitespaces from the dataset’s string
    columns
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对数据集的字符串列进行一般清理，去除前导空白
- en: ❷ Converts placeholder unknown values to a more useful string
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将未知占位符转换为更有用的字符串
- en: ❸ Converts the target from a string to a Boolean binary value
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将目标从字符串转换为布尔二进制值
- en: ❹ Creates new encoded features for the model
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为模型创建新的编码特征
- en: ❺ Executes all the feature-engineering stages, returning a Spark DataFrame
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行所有特征工程阶段，返回一个Spark DataFrame
- en: Once we execute this code, we’re left with a `DataFrame` and the requisite embedded
    logic for creating those additional columns. With this, we can initialize the
    feature store client and register the table, as shown in the next listing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们执行此代码，我们将得到一个 `DataFrame` 和创建那些附加列所需的嵌入式逻辑。有了这个，我们可以初始化特征存储客户端并注册表，如下一个列表所示。
- en: Listing 16.5 Register the feature engineering to the feature store
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.5 将特征工程注册到特征存储
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The library that contains the APIs to interface with the feature store
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 包含与特征存储接口 API 的库
- en: ❷ Initializes the feature store client to interact with the feature store APIs
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化特征存储客户端以与特征存储 API 交互
- en: ❸ The database and table name where this feature table will be registered
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将此功能表注册到的数据库和表名
- en: ❹ A primary key to affect joins
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 一个主键以影响连接
- en: ❺ Sets a partition key to make querying perform better if operations utilize
    that key
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 设置分区键以使查询在利用该键的操作中表现更好
- en: ❻ Specifies the processing history for the DataFrame that will be used to define
    the feature store table (from listing 16.4)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 指定用于定义特征存储表 DataFrame 的处理历史（来自列表 16.4）
- en: ❼ Adds a description to let others know this table’s content
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 添加描述以让其他人了解此表的内容
- en: After executing the registration of the feature table, we can ensure that it
    is populated with new data as it comes in through a lightweight scheduled ETL.
    The following listing shows how simple this is.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行功能表注册后，我们可以通过轻量级的计划 ETL 确保它随着新数据的到来而填充。以下列表显示了这有多么简单。
- en: Listing 16.6 Feature store ETL update
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.6 特征存储 ETL 更新
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Reads in new raw data that needs processing through the feature-generation
    logic
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取需要通过特征生成逻辑处理的新原始数据
- en: ❷ Processes the data through the feature logic
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过特征逻辑处理数据
- en: ❸ Writes the new feature data through the previously registered feature table
    in merge mode to append new rows
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 以合并模式通过先前注册的功能表写入新特征数据以追加新行
- en: Now that we’ve registered the table, the real key to its utility is in registering
    a model using it as input. To start accessing the defined features within a feature
    table, we need to define lookup accessors to each of the fields. The next listing
    shows how to do this data acquisition on the fields that we want to utilize for
    our income prediction model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经注册了表，其实用性的真正关键是注册一个使用它的模型。为了开始访问特征表中定义的特征，我们需要为每个字段定义查找访问器。下一个列表显示了如何进行我们想要用于我们的收入预测模型的数据获取。
- en: Listing 16.7 Feature acquisition for modeling
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.7 用于建模的特征获取
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The API to interface with the feature store to obtain references for modeling
    purposes
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于建模目的与特征存储接口的 API
- en: ❷ The list of field names that our model will be using
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们模型将使用的字段名称列表
- en: ❸ The lookup objects for each of the features
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个特征的查找对象
- en: Now that we’ve defined the lookup references, we can employ them in the training
    of a simple model, as shown in listing 16.8.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了查找引用，我们可以在训练简单模型时使用它们，如列表 16.8 所示。
- en: NOTE This is an abbreviated snippet of the full code. Please see the companion
    code in the book’s repository at [https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering)
    for the full-length example.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: NOTE 这只是完整代码的简略片段。请参阅书中存储库中的配套代码 [https://github.com/BenWilson2/ML-Engineering](https://github.com/BenWilson2/ML-Engineering)
    以获取完整示例。
- en: Listing 16.8 Register a model integrated with feature store
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.8 注册与特征存储集成的模型
- en: '[PRE7]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Specifies the fields that will be used for training the model by using the
    lookups defined in the preceding listing
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过使用前一个列表中定义的查找来指定用于训练模型的字段
- en: ❷ Converts the Spark DataFrame to a pandas DataFrame to utilize catboost
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将 Spark DataFrame 转换为 pandas DataFrame 以利用 catboost
- en: ❸ Registers the model to the feature store API so the feature-engineering tasks
    will be merged to the model artifact
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将模型注册到特征存储 API，以便特征工程任务将合并到模型工件中
- en: With this code, we have a data source defined as a linkage to a feature store
    table, a model utilizing those features for training, and a registration of the
    artifact dependency chain to the feature store’s integration with MLflow.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此代码，我们定义了一个数据源，作为对特征存储表的链接，一个利用这些特征进行训练的模型，以及将工件依赖链注册到特征存储与 MLflow 的集成。
- en: The final aspect of a feature store’s attractiveness from a consistency and
    utility perspective is in the serving of the model. Suppose we want to do a daily
    batch prediction using this model. If we were to use something other than the
    feature store approach, we’d have to either reproduce the feature-generation logic
    or call an external package, processing on the raw data, to get our features.
    Instead, we must write only a few lines of code to get an output of batch predictions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从一致性和实用性角度来看，特征存储的吸引力最终体现在模型的提供服务上。假设我们想使用此模型进行每日批处理预测。如果我们不使用特征存储方法，我们就必须重新生成特征生成逻辑或调用外部包，对原始数据进行处理，以获取我们的特征。相反，我们只需编写几行代码即可获得批处理预测的输出。
- en: Listing 16.9 Run batch predictions with feature store registered model
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.9 使用已注册的特征存储模型运行批处理预测
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Retrieves the experiment registered to MLflow through the feature store API
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过特征存储API检索注册到MLflow的实验
- en: ❷ Gets the individual run ID that we’re interested in from the experiment (here,
    the latest run)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从实验中获取我们感兴趣的个别运行ID（在此处，最新的运行）
- en: ❸ Applies the model to the defined feature table without having to write ingestion
    logic and perform a batch prediction
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在不编写摄入逻辑和执行批处理预测的情况下，将模型应用于定义的特征表
- en: While batch predictions such as this one comprise a large percentage of historical
    ML use cases, the API supports registering an external OLTP database or an in-memory
    database as a sink. With a published copy of the feature store populated to a
    service that can support low latency and elastic serving needs, all server-side
    (non-edge-deployed) modeling needs can be met with ease.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像这样的批处理预测占历史机器学习用例的大比例，但API支持注册外部OLTP数据库或内存数据库作为接收器。通过将特征存储的发布副本填充到支持低延迟和弹性服务需求的服务中，可以轻松满足所有服务器端（非边缘部署）的建模需求。
- en: 16.2.3 Evaluating a feature store
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2.3 评估特征存储
- en: 'The elements to consider when choosing a feature store (or building one yourself)
    are as varied as the requirements within different companies for data storage
    paradigms. In consideration of both current and potential future growth needs
    of such a service, functionality for a given feature store should be evaluated
    carefully, while keeping these important needs in mind:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 选择特征存储（或自己构建）时需要考虑的要素，就像不同公司对数据存储模式的需求一样多样化。在考虑此类服务的当前和潜在未来增长需求时，应仔细评估特定特征存储的功能，同时牢记这些重要需求：
- en: Synchronization of the feature store to external data serving platforms to support
    real-time serving (OLTP or in-memory database)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征存储与外部数据服务平台同步，以支持实时服务（OLTP或内存数据库）
- en: Accessibility to other teams for analytics, modeling, and BI use cases
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可供其他团队用于分析、建模和BI用例的访问权限
- en: Ease of ingestion to the feature store through batch and streaming sources
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过批处理和流式源轻松将数据摄入到特征存储中
- en: Security considerations for adhering to legal restrictions surrounding data
    (access controls)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵守围绕数据（访问控制）的法律限制的安全考虑
- en: Ability to merge JIT data to feature store data (data generated by users) for
    predictions
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将即时（JIT）数据合并到特征存储数据（用户生成数据）中进行预测的能力
- en: Data lineage and dependency tracking to see which projects are creating and
    consuming the data stored in the feature store
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据血缘和依赖关系跟踪，以查看哪些项目正在创建和消耗存储在特征存储中的数据
- en: With effective research and evaluation, a feature store solution can greatly
    simplify the production serving architecture, eliminate consistency bugs between
    training and serving, and reduce the chances of others duplicating effort across
    an organization. They’re incredibly useful frameworks, and I certainly see them
    being a part of all future ML efforts within industry.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过有效的研发和评估，特征存储解决方案可以极大地简化生产服务架构，消除训练和提供服务之间的不一致性错误，并减少其他人跨组织重复工作的可能性。它们是非常有用的框架，我确实看到它们将成为行业内所有未来机器学习努力的组成部分。
- en: OK, feature stores are cool and all, but do I really need one?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，特征存储很酷，但我真的需要它吗？
- en: “We got along just fine without one for years.”
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: “我们多年来没有它也能过得很好。”
- en: 'I’m usually a bit of a Luddite when it comes to new hype in technology. With
    a highly skeptical eye, I tend to take a rather pessimistic view of anything new
    that comes along, particularly if it claims to solve a lot of challenging problems
    or just sounds too good to be true. Honestly, most announcements in the ML space
    do exactly that: they gloss over the fine details of why the problem they’re purporting
    to solve was difficult for others to solve in the past. It’s only when I start
    road-testing the “new, hot tech” that the cracks begin to appear.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常对新技术的炒作持怀疑态度。我倾向于用高度怀疑的眼光看待任何新事物，尤其是那些声称能解决许多挑战性问题或听起来好得令人难以置信的事物。说实话，机器学习领域的许多公告正是如此：它们忽略了为什么他们声称要解决的问题在过去对其他人来说如此困难的原因。只有当我开始对“新潮技术”进行实地测试时，问题才开始显现。
- en: I haven’t had this experience with feature stores. Quite the contrary. I most
    certainly did take a skeptical view of them at first. But testing out the functionality
    and seeing the benefits of having centralized tracking of features, reusability
    of the results of complex feature-engineering logic, and the ability to decouple
    and monitor features from external scheduled jobs has made me a believer. Being
    able to monitor the health of features, not having to maintain separate logic
    of calculated features for additional projects, and being able to create features
    that can be leveraged for BI use cases is invaluable.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我还没有在特征存储库上有过这样的体验。恰恰相反。我最初确实对他们持怀疑态度。但测试其功能，看到集中跟踪特征、复用复杂特征工程逻辑的结果以及能够解耦和监控外部计划作业中的特征的好处，让我成为了信徒。能够监控特征的健康状况，无需为额外项目维护单独的计算特征逻辑，以及能够创建可用于BI用例的特征是无价的。
- en: These systems are useful during the development of projects as well. With a
    feature store, you’re not modifying production tables that are created through
    ETL. With the speed and dynamic nature of feature-engineering efforts, a lightweight
    ETL can be performed on these feature tables that does not require the large-scale
    change management associated with changes to production data in a data lake or
    data warehouse. With the data fully under the purview of the DS team (still held
    to production code-quality standards, of course!), the larger-scale changes to
    the rest of the organization are mitigated as compared to changes to DE jobs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统在项目开发期间也很有用。有了特征存储库，你不会修改通过ETL创建的生产表。由于特征工程工作的速度和动态性，可以在这些特征表上执行轻量级的ETL，而不需要与数据湖或数据仓库中生产数据变更相关的大规模变更管理。数据完全处于数据科学团队的监管之下（当然，仍然遵守生产代码质量标准！），与DE作业的变更相比，对整个组织的更大规模变更得到了缓解。
- en: Do you absolutely need a feature store? No, you don’t. But the benefits of having
    one to utilize for development, production deployment, and data reuse are of such
    a large magnitude that it simply doesn’t make sense not to use one.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你绝对需要一个特征存储库吗？不，你不需要。但拥有一个用于开发、生产部署和数据重用的好处如此之大，以至于不使用一个是不合逻辑的。
- en: 16.3 Prediction serving architecture
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 预测服务架构
- en: Let’s pretend for a moment that our company is working toward getting its first
    model into production. For the past four months, the DS team has been working
    studiously at fine-tuning a price optimizer for hotel rooms. The end goal of this
    project is to generate a curated list of personalized deals that have more relevancy
    to individual users than the generic collections in place now.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设一下，我们的公司正在努力将第一个模型投入生产。在过去四个月里，数据科学团队一直在努力微调酒店房间价格优化器。这个项目的最终目标是生成一个精选的个性化交易列表，比现在现有的通用集合更相关于个别用户。
- en: For each user, the team’s plan is to generate predictions each day for probable
    locations to visit (or locations the user has visited in the past), generating
    lists of deals to be shown during region searches. The team realizes early on
    a need to adapt prediction results to the browsing activity of the user’s current
    session.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个用户，团队的计划是每天生成预测，预测可能的访问地点（或用户过去访问过的地点），生成在区域搜索期间要显示的交易列表。团队很早就意识到需要将预测结果适应用户当前会话的浏览活动。
- en: To solve this dynamic need, the team generates overly large precalculated lists
    for each member based on available deals in regions that were like those that
    they’ve traveled to in the past. Fallback and cold-start logic for this project
    simply use the existing global heuristics that were in place before the project.
    Figure 16.8 shows the planned general architecture that the team has in mind for
    serving the predictions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这种动态需求，团队为每位成员根据过去旅行过的类似地区的可用交易生成过大的预计算列表。该项目备选和冷启动逻辑简单地使用了项目开始前就存在的现有全局启发式方法。图16.8显示了团队为提供预测而构思的计划总体架构。
- en: '![16-08](../Images/16-08.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![16-08](../Images/16-08.png)'
- en: Figure 16.8 Initial serving architecture design
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8初始服务架构设计
- en: Initially, after building this infrastructure, QA testing looks solid. The response
    SLA from the NoSQL-backed REST API is performing well, the batch prediction and
    heuristics logic from the model’s output is optimized for cost, and the fallback
    logic failover is working flawlessly. The team is ready to start testing the solution
    with an A/B test.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 初始阶段，在构建这个基础设施之后，QA测试看起来很稳固。基于NoSQL的REST API的响应SLA表现良好，模型输出的批量预测和启发式逻辑在成本上进行了优化，备选逻辑故障转移工作得非常完美。团队准备开始通过A/B测试来测试这个解决方案。
- en: 'Unfortunately, the test group’s booking rate is no different from the control
    group’s rates. Upon analyzing the results, the team finds that fewer than 5% of
    sessions utilized the predictions, forcing the remaining 95% of page displays
    to show the fallback logic (which is the same data being shown to the control
    group). Whoops. To fix this poor performance, the DS team decides to focus on
    two areas:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，测试组的预订率与控制组的预订率没有不同。在分析结果后，团队发现不到5%的会话使用了预测，迫使剩余的95%的页面显示使用备选逻辑（这与向控制组展示的数据相同）。哎呀。为了修复这种糟糕的性能，DS团队决定专注于两个领域：
- en: Increasing the number of predictions per user per geographic region
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加每个用户在每个地理区域内的预测数量
- en: Increasing the number of regions being predicted per user to cover
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加每个用户预测的区域数量以覆盖
- en: This solution dramatically affects their storage costs. What could they have
    done differently? Figure 16.9 shows a significantly different architecture that
    could have solved this problem without incurring such a massive cost in processing
    and storage.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案极大地影响了他们的存储成本。他们本可以如何不同地做？图16.9显示了显著不同的架构，该架构可以解决这个问题，而无需在处理和存储上产生如此巨大的成本。
- en: '![16-09](../Images/16-09.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![16-09](../Images/16-09.png)'
- en: Figure 16.9 A more cost-effective architecture for this use case
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9此用例的更具成本效益的架构
- en: While these changes are neither trivial nor, likely, welcome for either the
    DS team or the site-engineering team, they provide a clear picture about why serving
    predictions should never be an afterthought for a project. To effectively provide
    value, several considerations for serving architecture development should be evaluated
    at the outset of the project. The subsequent subsections cover these considerations
    and the sorts of architecture required to meet the scenarios.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些变化既不微不足道，也不太可能受到DS团队或网站工程团队的热烈欢迎，但它们清楚地说明了为什么服务预测永远不应该是一个项目的后续考虑。为了有效地提供价值，应在项目初期评估服务架构开发的几个考虑因素。接下来的小节将涵盖这些考虑因素以及满足这些场景所需的架构类型。
- en: 16.3.1 Determining serving needs
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.1 确定服务需求
- en: The team in our performance scenario initially failed to design a serving architecture
    that fully supported the needs of the project. Performing this selection is not
    a trivial endeavor to get right. However, with a thorough evaluation of a few
    critical characteristics of a project, the appropriate serving paradigm can be
    employed to enable an ideal delivery method for predictions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的性能场景中，团队最初未能设计出一个完全支持项目需求的服务架构。进行这一选择并非易事，但通过对项目几个关键特征的彻底评估，可以采用适当的服务范式，以实现预测的理想交付方法。
- en: When evaluating the needs of a project, it’s important to consider the following
    characteristics of the problem being solved to ensure that the serving design
    is neither overengineered nor under-engineered.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估项目需求时，考虑以下正在解决的问题的特征非常重要，以确保服务设计既不过度设计也不过度简化。
- en: This sounds like a developer problem, not a “me problem”
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来像是一个开发者问题，而不是“我的问题”
- en: It may seem like it’s better to just have a software engineering group worry
    about how to utilize a model artifact. They, after all (in most cases), are better
    at software development than a DS group is, and have exposure to more infrastructure
    tools and implementation techniques than are applicable to the realm of ML.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 可能看起来最好是让软件工程团队来担心如何利用模型工件。毕竟（在大多数情况下），他们在软件开发方面比数据科学团队更擅长，并且接触到了更多适用于机器学习领域的工具和实现技术。
- en: In my experience, I’ve never had much success with “punting a model over the
    wall” to another team. Depending on the use case, the data manipulation requirements
    (those requiring specific packages or other algorithms that are highly esoteric
    to the DS realm), post-prediction heuristics needs, and artifact update velocity
    can be challenging for a developer to integrate. Without a close, collaborative
    effort with a production infrastructure development team, deploying a service
    that integrates with existing systems can be an exercise in frustration and a
    massive generator of technical debt.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的经验中，我从未在将模型“踢过墙”给另一个团队方面取得过太多成功。根据用例，数据操作需求（那些需要特定包或其他对数据科学领域高度专业的算法）、预测后的启发式需求以及工件更新速度可能对开发者来说具有挑战性，难以整合。没有与生产基础设施开发团队的紧密协作，部署一个与现有系统集成服务的尝试可能是一次令人沮丧的练习，并且会产生大量的技术债务。
- en: Most times, after discussing a project’s integration needs with development
    teams, we’ve come upon clever methodologies to store predictions, perform manipulations
    of data at massive scale, and collaborate on designs that serve the project’s
    SLA needs at the lowest possible cost. Without input from the DS team on what
    it is that the model is doing, the development team is ill-prepared to make optimized
    architecture decisions. Similarly, without the advice and collaboration of the
    development team, the DS team is likely to create a solution that doesn’t meet
    the SLA needs or will be too costly to be justified running for very long.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，在与开发团队讨论项目的集成需求后，我们发现了一些巧妙的方法来存储预测，在大量规模上操作数据，并在最低成本下满足项目SLA需求的设计上进行协作。如果没有数据科学团队对模型所做之事的反馈，开发团队将无法为优化架构决策做好准备。同样，如果没有开发团队的指导和协作，数据科学团队可能创建的解决方案不符合SLA需求，或者成本过高，无法长期运行。
- en: Collaboration is key when evaluating a serving architecture; many times, this
    collaboration helps inform the very structure and design of the ML solution’s
    output. It’s best to involve the “engineering consumers” of your model solutions
    early in the project design phase. The earlier that they’re involved in the project
    (data engineers for batch bulk prediction solutions, software engineers for real-time
    serving solutions), the more of a positive influence they can have on the decision
    being made about how the solution is built.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估服务架构时，协作至关重要；很多时候，这种协作有助于告知机器学习解决方案输出结构的设计。最好在项目设计阶段早期就涉及模型的“工程消费者”。他们越早参与项目（数据工程师对于批量批量预测解决方案，软件工程师对于实时服务解决方案），他们就能对如何构建解决方案的决策产生越多的积极影响。
- en: SLA
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: SLA
- en: The original intention of the team earlier in our scenario was to ensure that
    their predictions would not interrupt the end user’s app experience. Their design
    encompassed a precalculated set of recommendations, held in an ultra-low-latency
    storage system to eliminate the time burden that they assumed would be involved
    in running a VM-based model service.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们场景的早期，团队的原意是确保他们的预测不会打断最终用户的APP体验。他们的设计包括一组预先计算的推荐，存储在一个超低延迟的存储系统中，以消除他们假设运行基于虚拟机的模型服务所涉及的时间负担。
- en: SLA considerations are one of the most important facets of an ML architecture
    design for serving. In general, the solution that is built must consider the budget
    for serving delays and ensure that for most of the time, this budget is not extended
    or violated. Regardless of how amazingly a model performs from a prediction accuracy
    or efficacy standpoint, if it can’t be used or consumed in the amount of time
    allotted, it’s worthless.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: SLA考虑因素是机器学习架构设计中服务于用户的最重要方面之一。一般来说，构建的解决方案必须考虑服务延迟的预算，并确保在大多数时间里，这个预算不会被延长或违反。无论模型在预测准确度或效能方面表现得多么出色，如果它不能在分配的时间内被使用或消费，那么它就是无价值的。
- en: The other consideration that needs to be balanced with that of the SLA requirements
    is the actual monetary budget. Materialized as a function of infrastructure complexity,
    the general rule is that the faster a prediction can be served at a larger scale
    of requests, the more expensive the solution is going to be to host and develop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 需要与SLA要求平衡的其他考虑因素是实际的货币预算。作为基础设施复杂性的函数，一般规则是，预测可以以更大的请求规模更快地提供服务，解决方案托管和开发的成本就会更高。
- en: Cost
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: Figure 16.10 shows a relationship between prediction freshness (how long after
    a prediction is made it is intended to be utilized or acted upon) and the volume
    of predictions that need to be made as a factor of cost and complexity.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10显示了预测新鲜度（预测做出后多长时间打算使用或采取行动）与需要做出的预测量之间的关系，这作为成本和复杂性的因素。
- en: '![16-10](../Images/16-10.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![16-10](../Images/16-10.png)'
- en: Figure 16.10 Architectural implications to meet SLA and prediction volume needs
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10 满足SLA和预测量需求时的架构影响
- en: The top portion of figure 16.10 shows a traditional paradigm for batch serving.
    For extremely large production inference volumes, a batch prediction job using
    Apache Spark Structured Streaming in a `trigger-once` operation will likely be
    the cheapest option.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10的上半部分展示了一个传统的批处理服务范式。对于极大规模的生产推理量，使用Apache Spark Structured Streaming在“触发一次”操作中进行的批预测作业可能是最经济的选项。
- en: The bottom portion of figure 16.10 involves immediate-use ML solutions. When
    predictions are intended to be used in a real-time interface, the architecture
    begins to change dramatically from the batch-inspired use cases. REST API interfaces,
    elastic scalability of serving containers, and traffic distribution to those services
    become required as prediction volumes increase.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10的下半部分涉及即时使用的ML解决方案。当预测打算用于实时界面时，架构开始从受批处理启发的用例发生重大变化。随着预测量的增加，需要REST API接口、服务容器的弹性可伸缩性和对这些服务的流量分配。
- en: Recency
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 近期性
- en: '*Recency*, the delay between when feature data is generated and when a prediction
    can be acted upon, is one of the most important aspects of designing a serving
    paradigm for a project’s model. SLA considerations are by and large the defining
    characteristics for choosing a specific serving layer architecture for ML projects.
    However, edge cases related to recency of the data available for usage in prediction
    can modify the final scalable and cost-effective design employed for a project.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*近期性*，即特征数据生成与预测可以采取行动之间的延迟，是设计项目模型服务范式最重要的方面之一。SLA（服务等级协议）的考虑因素在很大程度上是选择特定服务层架构的ML项目的定义特征。然而，与可用于预测的数据的近期性相关的边缘情况可能会修改项目最终采用的最终可扩展和成本效益的设计。'
- en: Depending on a particular situation, the recency of the data and the end use-case
    for the project can override the general SLA-based design criteria for serving.
    Figure 16.11 illustrates a set of examples of data recency and consumption layer
    patterns to show how the architecture can change from the purely SLA-focused designs
    in figure 16.10.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据具体情况，数据的近期性和项目的最终用例可能会覆盖基于SLA的一般设计标准。图16.11展示了一系列数据近期性和消费层模式示例，以说明架构如何从图16.10中纯粹基于SLA的设计转变为。
- en: '![16-11](../Images/16-11.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![16-11](../Images/16-11.png)'
- en: Figure 16.11 The effects of data recency and common usage patterns on serving
    architectures
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11 数据近期性和常见使用模式对服务架构的影响
- en: These examples are by no means exhaustive. There are as many edge case considerations
    for serving model predictions as there are nuanced approaches to solving problems
    with ML. The intention is to open the discussion around which serving solution
    is appropriate by evaluating the nature of the incoming data, identifying the
    project’s needs, and seeking the least complex solution possible that addresses
    the constraints of a project. By considering all aspects of project serving needs
    (data recency, SLA needs, prediction volumes, and prediction consumption paradigms),
    the appropriate architecture can be utilized to meet the usage pattern needs while
    adhering to a design that is only as complex and expensive as it needs to be.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子绝对不是详尽的。在为模型预测提供服务时，边缘情况考虑的多样性不亚于用机器学习解决问题的细微方法。目的是通过评估传入数据的特点、确定项目的需求，并寻求尽可能简单的解决方案来满足项目的限制，从而开启关于哪种服务解决方案是合适的讨论。通过考虑项目服务需求的各个方面（数据的新鲜度、服务级别协议需求、预测量以及预测消费模式），可以利用适当的架构来满足使用模式需求，同时遵守一个既不复杂也不昂贵的原则。
- en: But why don’t we just build real-time serving for everything?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们为什么不为所有东西都构建实时服务呢？
- en: Simplifying ML deployments around a one-size-fits-all pattern may be tempting.
    For some organizations, reducing ML engineering complexity in this manner might
    make sense (for instance, serving everything in Kubernetes). It certainly seems
    like it would be easier if every single project just needed to use some form of
    framework that supported a single deployment strategy.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕一种适合所有情况的模式简化机器学习部署可能很有吸引力。对于一些组织来说，以这种方式减少机器学习工程复杂性可能是有意义的（例如，在Kubernetes中提供一切服务）。当然，如果每个项目只需要使用某种形式的框架，该框架支持单一的部署策略，这似乎会更容易。
- en: This does make sense if your company has only a single type of ML use case.
    If all your company ever does is fraud prediction on behalf of small companies,
    it might make sense to stick with Seldon and Kubernetes to deliver REST API endpoints
    for all your models. If you’re focused on doing marketplace price optimizations
    based on asynchronous but low-traffic-volume models, a Docker container with a
    simple Flask server running inside it will do nicely.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的公司只有一种机器学习用例，这确实是有道理的。如果您的公司所做的只是代表小型公司进行欺诈预测，那么坚持使用Seldon和Kubernetes来为所有模型提供REST
    API端点可能是有意义的。如果您专注于基于异步但低流量量的模型进行市场定价优化，那么一个运行简单Flask服务器的Docker容器将非常适合。
- en: Most companies aren’t myopically focused on a single ML use case, though. For
    many companies, internal use cases would benefit from a simplistic batch prediction
    that’s written to a table in a database. Most groups have needs that can be solved
    with far simpler (and cheaper!) infrastructures for some of their use cases that
    don’t involve spinning up a VM cluster that can support hundreds of thousands
    of requests per second. Using such advanced infrastructure for a use case that’s
    at most going to be queried a few dozen times per day is wasteful (in development
    time, maintenance, and money) and negligent.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数公司并没有只关注单一机器学习用例，但许多公司的内部用例将受益于简单的批量预测，这些预测被写入数据库中的表。大多数团队都有需求，可以通过一些用例的简单（且更便宜！）基础设施来解决，这些用例不需要启动一个每秒可以支持数十万请求的虚拟机集群。对于每天最多查询几十次的情况使用如此先进的基础设施是浪费的（在开发时间、维护和金钱上），并且是疏忽的。
- en: It’s critically important for the long-term success of an ML solution to choose
    an architecture that fits the needs of consumption patterns, data volume sizes,
    and delivery time guarantees. This doesn’t mean to overengineer everything just
    in case, but rather to select the appropriate solution that meets your project’s
    needs. Nothing less, and most certainly, nothing more.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习解决方案的长期成功来说，选择一个适合消费模式、数据量大小和交付时间保证的架构至关重要。这并不意味着为了以防万一而过度设计，而是选择满足您项目需求的适当解决方案。不多也不少，当然，更不能更多。
- en: 'When an ML project’s output is destined for consumption within the walls of
    a company, the architectural burdens are generally far lower than any other scenario.
    However, this doesn’t imply that shortcuts can be taken. Utilizing MLOps tools,
    following robust data management processes, and writing maintainable code are
    just as critical here as they are for any other serving paradigm. Internal use-case
    modeling efforts can be classified into two general groups: bulk precomputation
    and lightweight ad hoc microservice.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习项目的输出旨在公司内部使用时，架构负担通常远低于其他任何场景。然而，这并不意味着可以走捷径。使用MLOps工具、遵循稳健的数据管理流程和编写可维护的代码在这里与其他任何服务范式一样至关重要。内部用例建模工作可以划分为两大类：批量预计算和轻量级临时微服务。
- en: Serving from a database or data warehouse
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据库或数据仓库中提供服务
- en: Predictions that are intended for within-workday usage usually utilize a batch
    prediction paradigm. Models are applied to the new data that has arrived up until
    the start of the workday, predictions are written to a table (typically in an
    overwrite mode), and end users within the company can utilize the predictions
    in an ad hoc manner.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在工作日内使用的预测通常利用批量预测范式。模型应用于工作日开始前到达的新数据，预测写入表格（通常以覆盖模式），公司内部最终用户可以临时使用这些预测。
- en: Regardless of the interface method (BI tool, SQL, internal GUI, etc.), the predictions
    are scheduled to occur at a fixed time (hourly, daily, weekly, etc.), and the
    only infrastructure burden that the DS team has is ensuring that the predictions
    are made and make their way to the table. Figure 16.12 shows an example architecture
    supporting this implementation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 无论接口方法（BI工具、SQL、内部GUI等）如何，预测都安排在固定时间（每小时、每天、每周等）进行，DS团队唯一的基础设施负担是确保预测得以生成并到达表格。图16.12展示了支持这种实现的示例架构。
- en: '![16-12](../Images/16-12.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![16-12](../Images/16-12.png)'
- en: Figure 16.12 Batch serving generic architecture
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12 批量服务通用架构
- en: This architecture is as bare-bones a solution as ML can get. A trained model
    is retrieved from a registry, data is queried from a source system (preferably
    from a feature store table), predictions are made, drift monitoring validation
    occurs, and finally the prediction data is written to an accessible location.
    For internal use cases on bulk-prediction data, not much more is required from
    an infrastructure perspective.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构是机器学习可以得到的最为简化的解决方案。从注册表中检索训练好的模型，从源系统（最好是特征存储表）查询数据，进行预测，执行漂移监控验证，最后将预测数据写入可访问的位置。对于需要大量预测数据的内部用例，从基础设施角度来看，不需要更多要求。
- en: Serving from a microservice framework
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 从微服务框架中提供服务
- en: For internal use cases that rely on more up-to-date predictions on an ad hoc
    basis or those that allow for a user to specify aspects of the feature vector
    to receive on-demand predictions (optimization simulations, for instance), precomputation
    isn’t an option. This paradigm focuses instead on having a lightweight serving
    layer to host the model, providing a simple REST API interface to ingest data,
    generate predictions, and return the predictions to the end user.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些基于临时需求依赖更及时预测的内部用例，或者允许用户指定特征向量以接收按需预测（例如优化模拟）的用例，预计算不是一种选择。这种范式专注于拥有一个轻量级的服务层来托管模型，提供一个简单的REST
    API接口以接收数据、生成预测并将预测返回给最终用户。
- en: Most implementations with these requirements are done through BI tools and internal
    GUIs. Figure 16.13 shows an example of such an architectural setup to support
    ad hoc predictions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数满足这些要求的实现都是通过BI工具和内部GUI完成的。图16.13展示了支持这种架构设置的示例，以支持临时预测。
- en: '![16-13](../Images/16-13.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![16-13](../Images/16-13.png)'
- en: Figure 16.13 Lightweight low-volume REST microservice architecture
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13 轻量级低容量REST微服务架构
- en: The simplicity of this style of deployment is appealing for many use cases of
    model serving for an internal use-case application. Capable of supporting up to
    a few dozen requests per second, a lightweight flask deployment of a model can
    be an attractive alternative to brute-force bulk computing of possible end-use
    permutations of potential predictions. Although this is technically a real-time
    serving implementation, it is of critical importance to realize that this is wildly
    inappropriate for low-latency, high-volume prediction needs or anything that could
    be customer-facing.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这种部署风格的简单性对许多用于内部应用场景的模型服务的用例具有吸引力。能够支持每秒数十个请求，轻量级的Flask模型部署可以是一个吸引人的替代方案，用于可能的最终用户预测排列的大规模暴力计算。
- en: It’s OK, we know that team
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，我们知道团队
- en: It can be rather tempting for internal-use projects to cut corners. Perhaps
    recording passive retraining histories seems like overkill for an internal project.
    It may be tempting to ship a code base to a scheduled job with a poor design that
    lacks appropriate refactoring that would have been done for a customer-facing
    model. Spending extra time optimizing the data storage design to support end-user
    query performance may seem like a waste of time.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内部使用项目来说，走捷径可能相当诱人。也许记录被动的再训练历史对于内部项目来说似乎是过度杀鸡用牛刀。可能有人会诱人地将代码库发送给一个设计糟糕、缺乏适当重构（如果用于面向客户的模型则会进行重构）的计划任务。花费额外的时间优化数据存储设计以支持最终用户查询性能可能看起来是浪费时间。
- en: After all, they’re fellow employees. They’ll understand if it doesn’t work perfectly,
    right?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，他们都是同事。如果它不能完美工作，他们会理解的，对吧？
- en: Nothing can be further from the truth. In my experience, the company’s collective
    perception of a DS team is based on these internal use-case projects. The perceived
    capability, capacity, and competency of the DS team is directly influenced by
    how well these internal tools work for the users within departments in the company.
    It’s critically important to build these solutions with the same level of engineering
    rigor and discipline as a solution that is used by customers. It’s your reputation
    on the line in ways that you might not realize.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比这更远离真相了。根据我的经验，公司对数据科学团队的集体认知是基于这些内部用例项目。数据科学团队感知的能力、容量和竞争力直接受到这些内部工具在公司内部用户中工作效果的影响。以与客户使用解决方案相同的工程严谨性和纪律性构建这些解决方案至关重要。你的声誉就悬于一线，而你可能没有意识到这一点。
- en: Perception of capability becomes important in internal projects for no larger
    reason than that these internal groups will be engaging your team for future projects.
    If these groups perceive the DS team as generating broken, unstable, and buggy
    solutions for their team’s use, the chances that they will want to have your team
    work on something that is customer-facing is somewhere in the vicinity of zero.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部项目中，感知能力变得重要，原因无他，这些内部团队将需要你的团队参与未来的项目。如果这些团队认为数据科学团队为他们提供的解决方案是破损的、不稳定的和有缺陷的，那么他们希望你的团队参与面向客户的项目的可能性几乎为零。
- en: The first customers that you have, after all, are the internal teams within
    the company. You’ll do well to make sure your primary customers—the business units—are
    confident in your ability to deliver stable and useful solutions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，你们都是公司内部的同事。确保你的主要客户——业务部门——对你交付稳定和有用解决方案的能力有信心，你会做得很好。
- en: 16.3.2 Bulk external delivery
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.2 批量外部交付
- en: The considerations for bulk external delivery aren’t substantially different
    from internal use serving to a database or data warehouse. The only material differences
    between these serving cases are in the realms of delivery time and monitoring
    of the predictions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 批量外部交付的考虑因素与内部使用数据库或数据仓库的服务没有实质性区别。这些服务案例之间的唯一实质性差异在于交付时间和预测的监控。
- en: Delivery consistency
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 交付一致性
- en: Bulk delivery of results to an external party has the same relevancy requirements
    as any other ML solution. Whether you’re building something for an internal team
    or generating predictions that will be end-user-customer facing, the goal of creating
    useful predictions doesn’t change.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果批量交付给外部方的相关要求与其他任何机器学习解决方案相同。无论你是为内部团队构建东西还是生成面向最终用户客户的预测，创建有用预测的目标不会改变。
- en: The one thing that does change with providing bulk predictions to an outside
    organization (generally applicable to business-to-business companies) when compared
    to other serving paradigms is in the timeliness of the delivery. While it may
    be obvious that a failure to deliver an extract of bulk predictions entirely is
    a bad thing, an inconsistent delivery can be just as detrimental. There is a simple
    solution to this, however, illustrated in the bottom portion of figure 16.14.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他服务范例相比，向外部组织（通常适用于B2B公司）提供大量预测时发生变化的唯一一件事是交付的及时性。虽然显然未能完全交付大量预测的摘要是坏事，但不一致的交付同样有害。然而，有一个简单的解决方案，如图16.14的底部部分所示。
- en: Figure 16.14 shows the comparison of gated and ungated serving to an external
    user group. By controlling a final-stage egress from the stored predictions in
    a scheduled batch prediction job, as well as coupling feature-generation logic
    to an ETL process governed by a feature store, delivery consistency from a chronological
    perspective can be guaranteed. While this may not seem an important consideration
    from the DS perspective of the team generating the predictions, having a predictable
    data-availability schedule can dramatically increase the perceived professionalism
    of the serving company.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14显示了对外部用户组进行门控和非门控服务的比较。通过控制计划批量预测作业中存储预测的最终阶段出口，以及将特征生成逻辑与受特征存储管理的ETL过程耦合，可以从时间角度保证交付的一致性。虽然从生成预测的团队的DS角度来看，这可能不是一个重要的考虑因素，但具有可预测的数据可用性计划可以显著提高服务公司的专业形象。
- en: '![16-14](../Images/16-14.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![16-14](../Images/16-14.png)'
- en: Figure 16.14 Comparison of ungated versus gated batch serving
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14 未加门控与加门控批量服务的比较
- en: Quality assurance
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 质量保证
- en: An occasionally overlooked aspect of serving bulk predictions externally (external
    to the DS and analytics groups at a company) is ensuring that a thorough quality
    check is performed on those predictions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在公司数据科学和数据分析团队外部（即公司DS和数据分析团队外部）提供大量预测服务时，一个有时被忽视的方面是确保对这些预测进行彻底的质量检查。
- en: An internal project may rely on a simple check for overt prediction failures
    (for example, silent failures are ignored that result in null values, or a linear
    model predicts infinity). When sending data products externally, additional steps
    should be done to minimize the chances of end users of predictions finding fault
    with them. Since we, as humans, are so adept at finding abnormalities in patterns,
    a few scant issues in a batch-delivered prediction dataset can easily draw the
    focus of a consumer of the data, deteriorating their faith in the efficacy of
    the solution to the point of disuse.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一个内部项目可能依赖于对明显的预测失败的简单检查（例如，忽略导致空值的静默失败，或线性模型预测无穷大）。当将数据产品发送到外部时，应采取额外步骤以最大限度地减少预测的最终用户对其提出异议的机会。由于我们人类在寻找模式中的异常方面非常擅长，因此批量交付的预测数据集中的一些少量问题很容易引起数据消费者的关注，从而降低他们对解决方案有效性的信心，甚至导致其不再使用。
- en: 'In my experience, when delivering bulk predictions external to a team of data
    specialists, I’ve found it worthwhile to perform a few checks before releasing
    the data:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的经验中，当向数据专家团队外部交付大量预测时，我在发布数据之前进行了一些检查是值得的：
- en: 'Validate the predictions against the training data:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预测与训练数据进行验证：
- en: '*Classification problems*—Comparing aggregated class counts'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类问题*—比较聚合的类别计数'
- en: '*Regression problems*—Comparing prediction distribution'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回归问题*—比较预测分布'
- en: '*Unsupervised problems*—Evaluating group membership counts'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督问题*—评估组成员计数'
- en: Check for prediction outliers (applicable to regression problems).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查预测异常值（适用于回归问题）。
- en: Build (if applicable) heuristics rules based on knowledge from SMEs to ensure
    that predictions are not outside the realm of possibility for the topic.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于知识渊博的领域专家的知识（如果适用）建立启发式规则，以确保预测不会超出主题的可能性范围。
- en: Validate incoming features (particularly encoded ones that may use a generic
    catchall encoding if the encoding key is previously unseen) to ensure that the
    data is fully compatible with the model as it was trained.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证传入的特征（尤其是可能使用通用捕获所有编码的编码特征，如果编码键之前未见）以确保数据与训练时的模型完全兼容。
- en: By running a few extra validation steps on the output of a batch prediction,
    a great deal of confusion and potential lessening of trust in the final product
    can be avoided in the eyes of end users.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在批量预测的输出上运行一些额外的验证步骤，可以在最终产品面前避免大量的困惑和信任度降低的可能性。
- en: 16.3.3 Microbatch streaming
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.3 微批量流式处理
- en: The applications of streaming prediction paradigms are rather limited. Unable
    to meet the strict SLA requirements that would force a decision to utilize a REST
    API service, as well as being complete overkill for small-scale batch prediction
    needs, streaming prediction holds a unique space in ML serving infrastructure.
    This niche spot is firmly centered in the needs of a project having a relatively
    high SLA (measured in the range of whole seconds to weeks) and a large inference
    dataset size.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 流式预测范式的应用相当有限。无法满足严格的SLA要求，这会迫使决策者利用REST API服务，以及对于小规模批量预测需求来说过于冗余，流式预测在机器学习服务基础设施中占据一个独特的空间。这个细分市场位置牢牢地集中在项目对相对较高的SLA（以秒到周的范围衡量）和大型推理数据集大小的需求上。
- en: The attractiveness of streaming for high SLA needs lies in cost and complexity
    reduction. Instead of building out a scalable infrastructure to support bulk predictions
    sent to a REST API service (or similar microservice capable of doing paginated
    bulk predictions of large data), a simple Apache Spark Structured Streaming job
    can be configured to allow for draining row-based data from a streaming source
    (such as Kafka or cloud object storage queue indices) and natively running predictions
    upon the stream with a serialized model artifact. This helps dramatically reduce
    complexity, can support streaming-as-batch stateful computation, and can prevent
    costly infrastructure from having to run when not needed for prediction.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理对高SLA需求的吸引力在于成本和复杂性的降低。而不是构建一个可扩展的基础设施来支持发送到REST API服务（或类似微服务，能够执行分页批量预测大数据）的大批量预测，可以配置一个简单的Apache
    Spark结构化流式处理作业，允许从流式源（如Kafka或云对象存储队列索引）中提取基于行的数据，并使用序列化模型工件在流上本地运行预测。这有助于显著降低复杂性，可以支持流式批处理状态计算，并防止在不需要进行预测时运行昂贵的硬件。
- en: From the perspective of large data sizes, streaming can reduce the required
    infrastructure size that would otherwise be needed for large dataset predictions
    in a traditional batch prediction paradigm. By streaming the data through a comparatively
    smaller cluster of machines than would be required to hold the entire dataset
    in memory, the infrastructure burden is far less.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从大数据量的角度来看，流式处理可以减少在传统批量预测范式下进行大数据集预测所需的底层基础设施规模。通过将数据流经一个比在内存中保留整个数据集所需的机器集群规模更小的集群，基础设施负担大大减轻。
- en: This directly translates into lower total cost of ownership for an ML solution
    with a relatively high SLA. Figure 16.15 shows a simple structured streaming approach
    to serve predictions at a lower complexity and cost than traditional batch or
    REST API solutions.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这直接转化为具有相对较高服务级别协议（SLA）的机器学习解决方案的总拥有成本降低。图16.15展示了简单结构化流式处理方法，以比传统批量或REST API解决方案更低的复杂性和成本提供预测。
- en: '![16-15](../Images/16-15.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![16-15](../Images/16-15.png)'
- en: Figure 16.15 A simple structured streaming prediction pipeline architecture
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.15 简单结构化流式处理预测管道架构
- en: While not able to solve the vast majority of ML serving needs, this architecture
    still has its place as an attractive alternative to batch prediction for extremely
    large datasets and to REST APIs when SLAs are not particularly stringent. Implementing
    this serving methodology is worth it, if it fits this niche, simply for the reduction
    in cost.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不能解决机器学习服务的大部分需求，但这种架构在处理极大数据集和当SLA不是特别严格时，作为批量预测的吸引人替代品仍有其位置。如果适合这个细分市场，实施这种服务方法还是值得的，仅仅是因为成本的降低。
- en: 16.3.4 Real-time server-side
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.4 实时服务器端
- en: The defining characteristic of real-time serving is that of a low SLA. This
    directly informs the basic architectural design of serving predictions. Any system
    supporting this paradigm requires a model artifact to be hosted as a service,
    coupled with an interface for accepting data passed into it, a computational engine
    to perform the predictions, and a method of returning a prediction to the originating
    requestor.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 实时服务的主要特征是低SLA。这直接影响了服务预测的基本架构设计。支持此范式的任何系统都需要托管模型工件作为服务，并配有一个接口用于接受传入的数据，一个计算引擎用于执行预测，以及一个方法将预测返回给原始请求者。
- en: 'The details of implementing a real-time serving architecture can be defined
    through the classification of levels of traffic, split into three main groupings:
    low volume, low volume with burst capacity, and high volume. Each requires different
    infrastructure design and tooling implementation to allow for a high availability
    and minimally expensive solution.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 实时服务架构实现的细节可以通过对流量级别的分类来定义，分为三个主要分组：低流量、低流量带突发容量和高流量。每个都需要不同的基础设施设计和工具实现，以实现高可用性和最小成本解决方案。
- en: Low volume
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 低流量
- en: The general architecture for low volume (low-rate requests) is no different
    from a REST microservice container architecture. Regardless of what REST server
    is used, what container service is employed to run the application, or what VM
    management suite is used, the only primary addition for externally facing endpoints
    is to ensure that the REST service is running on managed hardware. This doesn’t
    necessarily mean that a fully managed cloud service needs to be used, but the
    requirement for even a low-volume production service is that the system needs
    to stay up.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 低流量（低速率请求）的一般架构与REST微服务容器架构没有不同。无论使用什么REST服务器，使用什么容器服务运行应用程序，还是使用什么虚拟机管理套件，对外部端点的主要补充只是确保REST服务运行在管理硬件上。这并不一定意味着需要使用完全管理的云服务，但对于即使是低流量的生产服务，系统也需要保持运行。
- en: This infrastructure running the container that you’re building should be monitored
    from not only an ML perspective, but from a performance consideration as well.
    The memory utilization of the container on the hosting VM, the CPU utilization,
    network latency, and request failures and retries should all be monitored in real
    time with a redundant backup available to fail over to if issues arise with fulfilling
    serving requests.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 运行你构建的容器的这个基础设施不仅应该从机器学习的角度进行监控，还应该从性能考虑进行监控。托管虚拟机上的容器内存利用率、CPU利用率、网络延迟以及请求失败和重试都应该实时监控，并且要有冗余备份，以便在满足服务请求时出现问题时进行故障转移。
- en: The scalability and complexity of traffic routing doesn’t become an issue with
    low-volume solutions (tens to thousands of requests per minute), provided that
    the SLA requirements for the project are being met, so a simpler deployment and
    monitoring architecture is called for with low-volume use cases.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在低流量解决方案（每分钟数到数千个请求）中，流量路由的可扩展性和复杂性不会成为问题，只要项目的SLA要求得到满足，因此对于低流量用例，需要更简单的部署和监控架构。
- en: Burst volume and high volume
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 突发流量和高流量
- en: When moving to scales that support burst traffic, integrating elasticity into
    the serving layer is a critical addition to the architecture. Since an individual
    VM has only so many threads to process predictions, a flood of requests that come
    in for prediction and that exceed the execution capacity of a single VM can overwhelm
    that VM. Unresponsiveness, REST time-outs, and VM instability (potentially crashing)
    can render a single-VM model deployment unusable. The solution for handling burst
    volume and high-capacity serving is to incorporate process isolation and routing
    in the form of elastic load balancing.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当迁移到支持突发流量的规模时，将弹性集成到服务层是架构中的一个关键补充。由于单个虚拟机只有有限的线程来处理预测，因此对于预测的请求洪水，如果超过了单个虚拟机的执行能力，可能会使该虚拟机过载。无响应、REST超时和虚拟机不稳定（可能崩溃）可能会使单个虚拟机模型部署不可用。处理突发流量和高容量服务的方法是在弹性负载均衡中结合进程隔离和路由。
- en: Load balancing is, as the name implies, a means of routing requests in a sharded
    fleet of VMs (duplicated containers of a model serving application). With many
    containers running in parallel, request loads can be scaled horizontally to support
    truly staggering volumes of requests. These services (each cloud has its own flavor
    that essentially does the same thing) are transparent to both the ML team deploying
    a container and to the end user. With a single endpoint for requests to come into
    and a single container image to build and deploy, the load-balancing system will
    ensure that distribution of load burdens happens autonomously to prevent service
    disruption and instability.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡，正如其名所示，是一种在分片虚拟机群（模型服务应用的重复容器）中路由请求的方法。由于许多容器并行运行，请求负载可以水平扩展以支持真正惊人的请求量。这些服务（每个云都有自己的风味，本质上做的是同样的事情）对部署容器的机器学习团队和最终用户都是透明的。由于有一个请求进入的单一点端和一个构建和部署的单个容器镜像，负载均衡系统将确保负载分布是自动发生的，以防止服务中断和不稳定。
- en: A common design pattern, leveraging cloud-agnostic services, is shown in figure
    16.16\. Utilizing a simple Python REST framework (Flask) that interfaces with
    the model artifact from within a container allows for scalable predictions that
    can support high-volume and burst traffic needs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16展示了利用云无关服务的一个常见设计模式。通过使用一个简单的Python REST框架（Flask），该框架与容器内的模型工件接口，可以实现可扩展的预测，从而支持高流量和突发流量需求。
- en: This relatively bare-bones architecture is a basic template for an elastically
    scaling real-time REST-based service to provide predictions. Missing from this
    diagram are other critical components that we’ve discussed in previous chapters
    (monitoring of features, retraining triggers, A/B testing, and model versioning),
    but it has the core components that differentiate a smaller-scale real-time system
    from that of a service that can handle large traffic volume.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相对简单的架构是一个弹性扩展的基于REST的实时服务的基本模板，用于提供预测。图中缺少的是我们在前几章中讨论的其他关键组件（特征监控、重新训练触发器、A/B测试和模型版本控制），但它具有区分较小规模的实时系统与能够处理大量流量的服务的基本组件。
- en: At its core, the load balancer shown in figure 16.16 is what makes the system
    scale from a single VM’s limit of available cores (putting Gunicorn in front of
    Flask will allow all cores of the VM to concurrently process requests) to horizontally
    scaling out to handling hundreds of concurrent predictions (or more). This scalability
    comes with a caveat, though. Adding this functionality translates to greater complexity
    and cost for a serving solution.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，图16.16中所示的负载均衡器使得系统可以从单个虚拟机的可用核心限制（在Flask前面放置Gunicorn将允许虚拟机的所有核心同时处理请求）扩展到水平扩展以处理数百个并发预测（或更多）。然而，这种可扩展性也伴随着一个警告。添加此功能意味着服务解决方案的复杂性和成本都会增加。
- en: '![16-16](../Images/16-16.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![16-16](../Images/16-16.png)'
- en: Figure 16.16 Cloud-native REST API model serving architecture
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16 云原生REST API模型服务架构
- en: Figure 16.17 shows a more thorough design of a large-scale REST API solution.
    This architecture can support extremely high rates of prediction traffic and all
    the services that need to be orchestrated to hit volume, SLA, and analytics use
    cases for a production deployment.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17展示了大规模REST API解决方案的更详细设计。这种架构可以支持极高的预测流量速率，以及所有需要编排以实现生产部署的流量、服务级别协议（SLA）和数据分析用例。
- en: '![16-17](../Images/16-17.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![16-17](../Images/16-17.png)'
- en: Figure 16.17 Automated infrastructure and services for large-scale REST API
    model serving
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17 大规模REST API模型服务自动化的基础设施和服务
- en: These systems have a lot of components. It’s quite easy for the complexity to
    grow to the point that dozens of disparate systems are glued together in an application
    stack to fulfill the needs of the project’s use case. Therefore, it is of the
    utmost importance to explain not only the complexity involved in supporting these
    systems, but the cost as well, to the business unit interested in having a solution
    built that requires this architecture.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统有很多组件。复杂性很容易增长到几十个不同的系统被粘合在一起形成一个应用堆栈，以满足项目用例的需求。因此，向对构建需要这种架构的解决方案感兴趣的业务部门解释支持这些系统的复杂性以及成本至关重要。
- en: Typically, because of this magnitude of complexity, this isn’t a setup that
    a DS team maintains on its own. DevOps, core engineering, backend developers,
    and software architects are involved with the design, deployment, and maintenance
    of services like this. The cloud services bill is one thing to consider for the
    total cost of ownership, but the other outstanding factor is the human capital
    investment required to keep a service like this operational constantly.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，由于这种复杂性的程度，这不是一个数据科学团队可以独立维护的设置。DevOps、核心工程、后端开发人员和软件架构师都参与此类服务的架构、部署和维护。云服务账单是考虑总拥有成本的一个因素，但另一个突出的因素是保持此类服务持续运行所需的人力资本投资。
- en: If your SLA requirements and scale are this complicated, it would be wise to
    identify these needs as early in the project as possible, be honest about the
    investment, and make sure that the business understands the magnitude of the undertaking.
    If they agree that the investment is worth it, go ahead and build it. However,
    if the prospect of designing and building one of these behemoths is daunting to
    business leaders, it’s best not to force them into allowing it to be built at
    the very end of development when so much time and effort has been put into the
    project.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的SLA要求和规模如此复杂，那么在项目早期就识别这些需求是明智的，诚实地评估投资，并确保业务理解这项工作的规模。如果他们认为投资是值得的，那么就继续构建。然而，如果设计并构建这些庞然大物对业务领导来说是令人畏惧的，那么最好不要在项目开发后期强迫他们允许这样做，那时已经投入了大量的时间和精力。
- en: 16.3.5 Integrated models (edge deployment)
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.3.5 集成模型（边缘部署）
- en: '*Edge* *deployment* is the ultimate stage in low-latency serving for certain
    use cases. As it deploys a model artifact and all dependent libraries as part
    of a container image, it has scalability levels that outweigh any other approach.
    However, this deployment paradigm carries with it a large burden on the part of
    app developers:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*边缘* *部署* 是某些用例中低延迟服务的高级阶段。因为它将模型工件和所有依赖库作为容器镜像的一部分进行部署，它具有超过任何其他方法的可扩展性级别。然而，这种部署范式给应用程序开发者带来了很大的负担：'
- en: Deployment of new models or retrained models needs to be scheduled with app
    deployments and upgrades.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新模型或重新训练的模型的部署需要与应用程序部署和升级一起安排。
- en: Monitoring of predictions and generated features is dependent on internet connectivity.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控预测和生成的特征依赖于互联网连接。
- en: Heuristics or last-mile corrections to predictions cannot be done server-side.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的启发式或最后一英里校正不能在服务器端完成。
- en: Models and infrastructure within the serving container need deeper and more
    complex integration testing to ensure proper functionality.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务容器内的模型和基础设施需要更深入和更复杂的集成测试，以确保其正常功能。
- en: Device capabilities can restrict model complexity, forcing simpler and more
    lightweight modeling solutions.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备能力可以限制模型复杂性，迫使采用更简单、更轻量级的建模解决方案。
- en: For these reasons, edge deployment might not be very appealing for many use
    cases. The velocity of changes to the models is incredibly low, drift impacts
    to models can render edge-deployed models irrelevant far more quickly than a new
    build can be pushed out, and the lack of monitoring available for some end users
    can provide such intense disadvantages to this paradigm as to leave it inapplicable
    to most projects. For those that don’t suffer from the detractors for edge deployment,
    a typical architecture for this serving style is shown in figure 16.18.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，边缘部署对于许多用例可能并不很有吸引力。模型变化的速率非常低，模型漂移的影响可以使边缘部署的模型比新构建的模型更快地变得不相关，而且某些最终用户缺乏监控可能导致这种范式在大多数项目中无法应用。对于那些不受边缘部署的负面影响的人，这种服务风格的典型架构如图16.18所示。
- en: '![16-18](../Images/16-18.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![16-18](../Images/16-18.png)'
- en: Figure 16.18 A simplified architecture of an edge-deployed model artifact container
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.18 边缘部署模型工件容器的简化架构
- en: As you can see, an edge deployment is tightly coupled to the application code
    base. Because of the large numbers of required packaged libraries involved in
    a runtime that can support the predictions being made by the included model, containerizing
    the artifact prevents the app development team from maintaining an environment
    that is mirrored to that of the DS team. This can mitigate many of the issues
    that can plague non-container-based model edge deployments (namely, environment
    dependency management, language choice standardization, and library synchronization
    for features in a shared code base).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，边缘部署与应用程序代码库紧密耦合。由于运行时需要大量打包的库来支持包含模型所做的预测，容器化工件阻止了应用程序开发团队维护与DS团队环境相匹配的环境。这可以减轻许多可能困扰基于容器模型边缘部署的问题（即环境依赖管理、语言选择标准化和共享代码库中功能库的同步）。
- en: The projects that can leverage edge deployment, particularly those focused on
    tasks such as image classification, can dramatically reduce infrastructure costs.
    The defining aspect of what can qualify for edge deployment is in the state of
    stationarity in the features being utilized by the model. If the functional nature
    of the model’s input data will not be changing particularly often (such as with
    imaging use cases), edge deployment can greatly simplify infrastructure and keep
    total ownership costs of an ML solution incredibly low.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 可以利用边缘部署的项目，尤其是那些专注于图像分类等任务的，可以显著降低基础设施成本。能够符合边缘部署的条件是模型所利用的特征的稳定性状态。如果模型输入数据的函数性质不会特别频繁地变化（例如，在成像用例中），边缘部署可以极大地简化基础设施，并将机器学习解决方案的总拥有成本保持在极低水平。
- en: Summary
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Model registry services will help ensure effective state management of deployed
    and archived models, enabling effective passive retraining and active retraining
    solutions without requiring manual intervention.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型注册服务将有助于确保已部署和存档模型的有效状态管理，从而实现无需人工干预的有效被动重新训练和主动重新训练解决方案。
- en: Feature stores segregate feature-generation logic from modeling code, allowing
    for faster retraining processes, reuse of features across projects, and a far
    simpler method of monitoring for feature drift.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征存储将特征生成逻辑与建模代码分离，从而允许更快地重新训练过程，跨项目复用特征，以及一种远更简单的监控特征漂移的方法。
- en: 'To choose an appropriate architecture for serving, we must weigh many characteristics
    of the project: employing the right level of services and infrastructure to support
    the required SLA, prediction volume, and recency of data to ensure that a prediction
    service is cost-effective and stable.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了选择一个合适的架构用于服务，我们必须权衡项目的许多特性：采用适当的服务和基础设施水平以支持所需的SLA、预测量和数据的时效性，以确保预测服务具有成本效益且稳定。
