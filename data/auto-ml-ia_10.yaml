- en: 7 Customizing the search method of AutoML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 自定义AutoML的搜索方法
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding sequential search methods
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解顺序搜索方法
- en: Customizing a random search method
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义随机搜索方法
- en: Vectorizing hyperparameters for the model-based search method
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为基于模型的搜索方法向量化超参数
- en: Understanding and implementing a Bayesian optimization search method
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和实现贝叶斯优化搜索方法
- en: Understanding and implementing an evolutionary search method
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和实现进化搜索方法
- en: 'In this chapter, we will explore how to customize a sequential search method
    to iteratively explore the hyperparameter search space and discover better hyperparameters.
    You will learn how to implement different sequential search methods for selecting
    pipelines from the search space in each trial. These search methods fall into
    the following two categories:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何自定义顺序搜索方法以迭代探索超参数搜索空间并发现更好的超参数。您将学习如何实现不同的顺序搜索方法，以在每个试验中选择搜索空间中的管道。这些搜索方法分为以下两类：
- en: '*History-independent* sequential search methods cannot be updated during the
    search process. For example, grid search, which we looked at in chapter 2, traverses
    all the possible combinations of values in the candidate hyperparameter sets,
    and in chapter 6, we used the random search method to select hyperparameter combinations
    from the search space randomly. These are the two most representative history-independent
    methods. Some other advanced random search methods make use of history, such as
    the quasi-random search method using Sobol sequences ([http://mng.bz/6Z7A](http://mng.bz/6Z7A)),
    but here we’ll consider only the vanilla uniform random search method.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*历史无关*的顺序搜索方法在搜索过程中不能更新。例如，我们在第2章中讨论的网格搜索遍历候选超参数集中所有可能值的组合，在第6章中，我们使用随机搜索方法从搜索空间中随机选择超参数组合。这些是两种最典型的历史无关方法。一些其他高级随机搜索方法利用历史记录，例如使用Sobol序列的准随机搜索方法([http://mng.bz/6Z7A](http://mng.bz/6Z7A))，但在这里我们将仅考虑经典的均匀随机搜索方法。'
- en: '*History-dependent* sequential search methods, such as Bayesian optimization,
    are able to improve the effectiveness of the search by leveraging the previous
    results.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*历史依赖*的顺序搜索方法，例如贝叶斯优化，能够通过利用先前结果来提高搜索的有效性。'
- en: 7.1 Sequential search methods
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 顺序搜索方法
- en: 'In chapter 6, you learned how to customize a tuner to control the AutoML search
    loop (see figure 7.1). ML pipelines are generated by iteratively calling the oracle
    (search method). Models learned from the ML pipeline are evaluated, and the results
    are fed back to the oracle to update it so it can better explore the search space.
    Because the oracle generates ML pipelines in a sequential manner, we call it a
    *sequential search method*. It generally consists of the following two steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，您学习了如何自定义调谐器以控制AutoML搜索循环（见图7.1）。机器学习管道是通过迭代调用算子（搜索方法）生成的。从机器学习管道中学习的模型被评估，并将结果反馈给算子以更新它，以便它能更好地探索搜索空间。因为算子以顺序方式生成机器学习管道，所以我们称它为*顺序搜索方法*。它通常包括以下两个步骤：
- en: '*Hyperparameter sampling*—Sampling hyperparameters from the search space to
    create the ML pipelines.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*超参数采样*——从搜索空间中采样超参数以创建机器学习管道。'
- en: '*Oracle update* (optional)—Updating the search method, leveraging the history
    of existing models and evaluations. The goal is to increase the speed at which
    better ML pipelines are identified in the search space. This step differs across
    search methods, and it takes place only in history-dependent methods. For example,
    grid search and random search do not take history into account, so with these
    methods, the oracle does not need to be updated during the search process.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*算子更新*（可选）——更新搜索方法，利用现有模型和评估的历史记录。目标是增加在搜索空间中识别更好的机器学习管道的速度。这一步骤在不同搜索方法中有所不同，并且仅在历史依赖方法中发生。例如，网格搜索和随机搜索不考虑历史记录，因此在这些方法中，算子不需要在搜索过程中更新。'
- en: '![07-01](../Images/07-01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![07-01](../Images/07-01.png)'
- en: Figure 7.1 A single search loop when using a sequential search method
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 使用顺序搜索方法时的单个搜索循环
- en: 'As mentioned previously, if the oracle can leverage historical evaluations
    to update itself and guide its sampling of new hyperparameters from the search
    space, we can divide the sequential search methods into two categories: history-dependent
    methods and history-independent methods. The history-dependent methods can be
    further classified into the following two main categories based on how the updating
    is done:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果oracle可以利用历史评估来更新自身并指导其从搜索空间中采样新的超参数，我们可以将顺序搜索方法分为两类：依赖于历史的方法和独立于历史的方法。根据更新的方式，依赖于历史的方法可以进一步分为以下两个主要类别：
- en: '*Heuristic methods*—Often inspired by biological behavior. A representative
    example is the *evolutionary method*, which generates new samples by simulating
    the evolution of an animal population across generations. We will introduce how
    to create an evolutionary search method in the last section of this chapter.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*启发式方法*—通常受到生物行为的启发。一个典型的例子是*进化方法*，它通过模拟动物种群在代际间的进化来生成新的样本。我们将在本章的最后部分介绍如何创建一个进化搜索方法。'
- en: '*Model-based methods*—Leverages certain ML models, such as the decision tree
    model, to predict which hyperparameters in the search space will be good choices.
    The historical evaluations of the previous hyperparameter sets are used as training
    data to train the ML model. A representative method is the Bayesian optimization
    method that we used in the previous chapter. You’ll learn how to implement this
    method in section 7.3.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于模型的方法*—利用某些机器学习模型，如决策树模型，来预测搜索空间中哪些超参数是好的选择。使用先前超参数集的历史评估作为训练数据来训练机器学习模型。一个代表性的方法是我们在上一章中使用的贝叶斯优化方法。你将在第7.3节中学习如何实现这种方法。'
- en: 'Together, these are probably the most widely used sequential search methods
    in the existing literature. We’ll begin, however, with a history-independent method:
    random search. We’ll continue here with the example of tuning LightGBM models
    for the California housing price-prediction problem used in chapter 6\. The code
    will mainly focus on the oracle. The code for loading data and the implementation
    of the tuner class are unchanged, and won’t be repeated here. The complete code
    can be found in the book’s GitHub repository: [http://mng.bz/oaep](http://mng.bz/oaep).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法可能是现有文献中最广泛使用的顺序搜索方法。然而，我们将从一种与历史无关的方法开始：随机搜索。我们将继续使用第6章中使用的加利福尼亚房价预测问题的LightGBM模型调优示例。代码将主要关注oracle。加载数据和tuner类的实现代码保持不变，此处不再重复。完整的代码可以在本书的GitHub仓库中找到：[http://mng.bz/oaep](http://mng.bz/oaep)。
- en: 7.2 Getting started with a random search method
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 使用随机搜索方法入门
- en: In this section, we will introduce how to create a random search method with
    KerasTuner to explore the search space and find better hyperparameters. The random
    search method is one of the simplest and most traditional ways of doing hyperparameter
    tuning in AutoML. The vanilla random search method randomly explores the hyperparameter
    combinations in the search space. This approach has been empirically shown to
    be more powerful than the grid search method in most cases.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何使用KerasTuner创建随机搜索方法来探索搜索空间并找到更好的超参数。随机搜索方法是AutoML中进行超参数调整的最简单和最传统的方法之一。标准的随机搜索方法随机探索搜索空间中的超参数组合。这种方法在大多数情况下已被经验证明比网格搜索方法更强大。
- en: Why is random search often better than grid search?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么随机搜索通常比网格搜索更好？
- en: 'We’ll use an example to describe this. You can find more details in the paper
    “Random Search for Hyper-Parameter Optimization” by James Bergstra and Yoshua
    Bengio ([www.jmlr.org/papers/v13/bergstra12a.html](http://www.jmlr.org/papers/v13/bergstra12a.html)).
    Suppose we have two continuous hyperparameters, *x* and *y*, forming a two-dimensional
    search space. Assume the model performance is a function related to these hyperparameters.
    More specifically, it’s an additive function of two functions, each dependent
    on one of the hyperparameters: ![07-01-EQ01](../Images/07-01-EQ01.png).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个例子来描述这一点。更多细节可以在James Bergstra和Yoshua Bengio的论文“Random Search for Hyper-Parameter
    Optimization”中找到（[www.jmlr.org/papers/v13/bergstra12a.html](http://www.jmlr.org/papers/v13/bergstra12a.html)）。假设我们有两个连续的超参数，*x*和*y*，形成一个二维搜索空间。假设模型性能是一个与这些超参数相关的函数。更具体地说，它是由两个函数的加法函数组成的，每个函数都依赖于一个超参数：![07-01-EQ01](../Images/07-01-EQ01.png)。
- en: Different hyperparameters have different effects on the final model’s performance,
    so some will have less of an effect than others. Let’s assume the hyperparameter
    *y* has marginal effects compared to *x*, indicating that ![07-01-EQ02](../Images/07-01-EQ02.png).
    Along the two boundaries (left and upper) of the space, we provide two function
    curves forming a one-dimensional subspace of each hyperparameter and its function.
    The height of each function curve can also be understood as indicating the importance
    of the hyperparameter to the evaluation of the final models. If we use the grid
    search method to explore the search space with nine trials, it will bucketize
    the search space and sample a grid of points (see figure a), which gives an even
    coverage of the space. In this case, even though the hyperparameters are of different
    importance, the grid search provides equal coverage of the subspace of each one,
    whereas random search provides more thorough coverage of the subspace of hyperparameter
    *y* (the important one) as shown in figure b.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的超参数对最终模型性能的影响不同，因此有些超参数的影响会比其他超参数小。假设超参数*y*相对于*x*具有边际效应，表示为 ![07-01-EQ02](../Images/07-01-EQ02.png)。在空间的两个边界（左侧和上方）上，我们提供了两个函数曲线，形成每个超参数及其函数的一维子空间。每个函数曲线的高度也可以理解为指示超参数对最终模型评估的重要性。如果我们使用网格搜索方法通过九次试验来探索搜索空间，它将搜索空间划分为桶，并采样一个网格点（见图a），这给出了空间的均匀覆盖。在这种情况下，尽管超参数的重要性不同，但网格搜索为每个超参数的子空间提供了相等的覆盖，而随机搜索则提供了对超参数*y*（重要的超参数）子空间更彻底的覆盖，如图b所示。
- en: '![07-01-unnumb](../Images/07-01-unnumb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![07-01-unnumb](../Images/07-01-unnumb.png)'
- en: Grid search versus random search
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索与随机搜索的比较
- en: The search method in KerasTuner is implemented as an Oracle object that can
    be called by the tuners. Before implementing the oracle, we need to understand
    the relationship (reference logic) between the functions of the oracle and those
    of the tuner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: KerasTuner中的搜索方法实现为一个可以被调用的Oracle对象。在实现Oracle之前，我们需要理解Oracle函数与调优器函数之间的（参考逻辑）关系。
- en: The main functions called during the search process are shown in listing 7.1\.
    The search() function of the tuner will call two main functions in a loop. The
    first one, create_trial(), is a function of the oracle. It creates a trial object
    containing the hyperparameters selected by the oracle in the current trial and
    sets the status of the trial to RUNNING, meaning that the trial is executing.
    The sampling of the hyperparameters is done in a private method called populate_space(),
    which is the main function of the oracle we need to implement. If the search method
    is history-dependent, we will need to update it before sampling, based on the
    evaluations. After the trial object is created, it will carry the hyperparameters
    to the main function of the tuner (run_trial()), which as we learned in chapter
    6, is used to instantiate, train, evaluate, and save the ML model in the current
    trial.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索过程中调用的主要函数如列表7.1所示。调优器的search()函数将在循环中调用两个主要函数。第一个，create_trial()，是Oracle的一个函数。它创建一个包含Oracle在当前试验中选择的超参数的试验对象，并将试验的状态设置为RUNNING，意味着试验正在执行。超参数的采样是在一个名为populate_space()的私有方法中完成的，这是我们需要实现的Oracle的主要函数。如果搜索方法是历史依赖的，我们将在采样之前根据评估结果更新它。试验对象创建后，它将携带超参数到调优器的主函数run_trial()，正如我们在第6章所学，该函数用于实例化、训练、评估和保存当前试验中的ML模型。
- en: Listing 7.1 The reference logic between the functions of the tuner and the oracle
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 调优器函数与Oracle函数之间的参考逻辑
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Because KerasTuner has already helped us encapsulate some functions in the base
    Oracle class (such as the create_trial function), we can extend the base class
    and implement only one core function—populate_space(—which conducts hyperparameter
    sampling and updates the oracle.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于KerasTuner已经帮助我们封装了基Oracle类中的一些函数（例如create_trial函数），我们可以扩展基类并仅实现一个核心函数——populate_space()（该函数执行超参数采样并更新Oracle）。
- en: Note the Oracle class contains an update_trial() function, which uses the returned
    value from Tuner.run_trial() to update the oracle. However, it is not required
    to use this function to update the search method. If the search method needs to
    be updated based on the historical evaluations, we can take care of this with
    the populate_space() function before doing hyperparameter sampling. You’ll learn
    how to do this in section 7.3, when implementing a history-dependent search method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意奥拉类包含一个update_trial()函数，该函数使用Tuner.run_trial()返回的值来更新奥拉。然而，不需要使用此函数来更新搜索方法。如果搜索方法需要根据历史评估进行更新，我们可以在进行超参数采样之前使用populate_space()函数来处理这个问题。您将在7.3节中学习如何实现依赖历史的搜索方法。
- en: Because the random search method is history-independent, all the populate_ space()
    function needs to do is uniformly random-sample the hyperparameters. We use a
    private utility method of the base Tuner class, _random_values, to generate random
    samples from the search space. The output of the populate_space() function should
    be a dictionary containing the status of the trial and the sampled hyperparameter
    values of this trial. If the search space is empty or all the hyperparameters
    are fixed, we should set the trial status to STOPPED to end this trial.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机搜索方法是历史无关的，populate_space()函数需要做的只是均匀随机采样超参数。我们使用基Tuner类的私有实用方法_random_values从搜索空间中生成随机样本。populate_space()函数的输出应该是一个包含试验状态和本次试验采样超参数值的字典。如果搜索空间为空或所有超参数都已固定，我们应该将试验状态设置为STOPPED以结束此试验。
- en: Listing 7.2 shows how the random search oracle is implemented. Though it’s negligible,
    we include the initialization function here for reference. You may use some hyperparameters
    to help control the search algorithm, such as the random seed, so you can add
    attributes for these hyperparameters. It is worth pointing out that the hyperparameters
    of the search method are not contained in the search space, and we need to tune
    these ourselves. They’re considered *hyper-hyperparameters*, which are the hyperparameters
    used to control the hyperparameter tuning process. We will see some examples in
    the following sections.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2展示了如何实现随机搜索奥拉。尽管这可以忽略不计，但我们在这里包含初始化函数以供参考。您可以使用一些超参数来帮助控制搜索算法，例如随机种子，因此您可以添加这些超参数的属性。值得注意的是，搜索方法中的超参数不包含在搜索空间中，我们需要自己调整这些参数。它们被认为是*超超参数*，即用于控制超参数调整过程的超参数。我们将在接下来的章节中看到一些示例。
- en: Listing 7.2 The random search oracle
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2 随机搜索奥拉
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The initialization function of the oracle
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 奥拉（Oracle）的初始化函数
- en: ❷ Randomly sampled hyperparameter values from the search space
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从搜索空间中随机采样的超参数值
- en: ❸ Checks whether the sampled hyperparameter values are valid
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查采样到的超参数值是否有效
- en: ❹ Returns the selected hyperparameter values and the correct trial status
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回选定的超参数值和正确的试验状态
- en: 'Listing 7.3 shows how we can apply the random search oracle to tune a gradient-boosted
    decision tree (GBDT) model implemented with the LightGBM library to address the
    California housing price-prediction task. A GBDT model builds up multiple trees
    sequentially and orients each newly constructed tree to address the erroneous
    classifications or weak predictions of the previous tree ensemble. More details
    can be found in appendix B, if you’re not familiar with this model. The code for
    loading the dataset and implementing the tuner is the same as in the previous
    chapter, so we won’t show that again. Here, we tune three hyperparameters of the
    GBDT model: the number of leaves in each tree, the number of trees (n_estimators),
    and the learning rate. After searching for 100 trials, the best discovered model
    achieves an MSE of 0.2204 on the test set.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3展示了如何将随机搜索奥拉应用于调整使用LightGBM库实现的梯度提升决策树（GBDT）模型，以解决加利福尼亚房价预测任务。GBDT模型按顺序构建多个树，并将每个新构建的树定位以解决先前树集成中的错误分类或弱预测。如果您不熟悉此模型，更多细节可以在附录B中找到。加载数据集和实现调整器的代码与上一章相同，因此我们不再展示。在这里，我们调整了GBDT模型的三个超参数：每棵树中的叶子数、树的数量（n_estimators）和学习率。经过100次搜索后，最佳发现的模型在测试集上实现了0.2204的均方误差（MSE）。
- en: Listing 7.3 Using the customized random search oracle to tune a GBDT model
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3 使用自定义的随机搜索奥拉调整GBDT模型
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Provides the customized random search oracle to the tuner
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为调整器提供自定义的随机搜索奥拉
- en: ❷ Retrieves and evaluates the best discovered model
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检索并评估最佳发现的模型
- en: To show what the search process looks like, we extract the evaluation performance
    of all the searched models and plot them in order. The models are recorded in
    the order the trials finished as a list in the end_order attribute of the oracle,
    which is random_ tuner.oracle.end_order in our example. The finishing order of
    the trials is the same as the starting order because we’re not doing parallel
    trials in this case. Code for plotting the search curve is shown in listing 7.4.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示搜索过程的样子，我们提取了所有搜索模型的评估性能，并按顺序绘制它们。模型按照试验完成的顺序记录在oracle的end_order属性中，在我们的例子中是random_tuner.oracle.end_order。试验的完成顺序与开始顺序相同，因为我们在这个案例中没有进行并行试验。绘制搜索曲线的代码显示在列表7.4中。
- en: Listing 7.4 Plotting the search process
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4 绘制搜索过程
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In figure 7.2, we can see that the evaluation results of models discovered during
    the random search process fluctuate considerably. Because the random search cannot
    take historical evaluations into account, the models discovered later do not benefit
    from previous results and do not tend to be better than the earlier ones.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.2中，我们可以看到在随机搜索过程中发现的模型评估结果波动很大。因为随机搜索不能考虑历史评估，所以后来发现的模型没有从先前结果中受益，并且不一定比早期的模型更好。
- en: '![07-02](../Images/07-02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![07-02](../Images/07-02.png)'
- en: Figure 7.2 The model evaluation results during the random search process
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 随机搜索过程中的模型评估结果
- en: In the next section, we will introduce a history-dependent sequential search
    method that can leverage the historical evaluations to boost search efficiency.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍一种依赖于历史评估的顺序搜索方法，该方法可以利用历史评估来提高搜索效率。
- en: 7.3 Customizing a Bayesian optimization search method
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 自定义贝叶斯优化搜索方法
- en: 'In this section, we introduce a model-based sequential search method called
    *Bayesian optimization*. It was designed to optimize *black-box functions*, which
    are functions that do not have the analytical form of solutions. This is frequently
    the case in the context of AutoML, where the function to be optimized is the model
    evaluation performance. Black-box functions are often expensive to evaluate, making
    it impractical to find the global optimal solution via brute-force random sampling
    and evaluation. Due to the cost of model training and evaluation, it may not be
    possible to conduct numerous hyperparameter search trials. The key idea of the
    Bayesian optimization method for solving this challenge is correlated with the
    following two functions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一种基于模型的顺序搜索方法，称为*贝叶斯优化*。它被设计用来优化*黑盒函数*，这些函数没有解析形式的解。在AutoML的背景下，这种情况很常见，因为要优化的函数是模型评估性能。黑盒函数通常评估成本很高，这使得通过暴力随机采样和评估来找到全局最优解变得不切实际。由于模型训练和评估的成本，可能无法进行多次超参数搜索试验。贝叶斯优化方法解决这一挑战的关键思想与以下两个函数相关：
- en: We train a function (or model) called the *surrogate function* (or *surrogate
    model* ) to approximate the model evaluation performance. Statistically, this
    surrogate function is a probability model that approximates the objective function.
    We estimate its prior distribution ourselves, based on our belief of what we think
    the objective function will look like (e.g., later we’ll use the Gaussian process
    prior, which is the most commonly used prior). The surrogate model is trained
    with the historical evaluations of the ML models and serves as a much cheaper
    way to get the performance of previously unseen models, albeit approximated. This
    process is very similar to solving regression tasks, in which each model is an
    instance. The hyperparameters are the features of the instances, and the model
    performance is the target. Theoretically, with a good enough surrogate model,
    we wouldn’t have to conduct real training and evaluations of the ML models. But
    because we have only limited training data (the model evaluations), this is often
    practically impossible in an AutoML problem.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们训练一个称为*代理函数*（或*代理模型*）的函数来近似模型评估性能。从统计学的角度来看，这个代理函数是一个概率模型，它近似目标函数。我们根据我们对目标函数外观的信念（例如，稍后我们将使用高斯过程先验，这是最常用的先验）自行估计其先验分布。代理模型使用ML模型的历史评估进行训练，作为一种更便宜的方式来获取先前未见过的模型性能，尽管是近似的。这个过程与解决回归任务非常相似，其中每个模型都是一个实例。超参数是实例的特征，模型性能是目标。理论上，如果代理模型足够好，我们就不必对ML模型进行真实的训练和评估。但由于我们只有有限的学习数据（模型评估），在AutoML问题中这通常在实际上是不可能的。
- en: Once we have a surrogate model, we can sample a new hyperparameter combination
    to create a model for evaluation. To proceed with the sampling, we need to design
    another function, called the *acquisition function*, based on the surrogate function.
    This function specifies the criteria for comparing the ML models (determined by
    the hyperparameters) so that we can pick the most promising one to train and evaluate.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们有了代理模型，我们就可以采样一个新的超参数组合来创建一个用于评估的模型。为了进行采样，我们需要基于代理函数设计另一个函数，称为*获取函数*。此函数指定了比较ML模型（由超参数确定）的标准，以便我们可以选择最有希望的模型进行训练和评估。
- en: 'As you can see, the two functions correspond to the two steps in the search
    loop of the sequential AutoML process. In the update step, we train the surrogate
    model based on the historical evaluations. In the sampling step, we use the acquisition
    function to sample the next model to be evaluated. Iterating the two steps will
    provide us with additional historical samples to help train a more accurate surrogate
    model. In the remainder of this section, we will provide a step-by-step implementation
    of a Bayesian optimization search method. Along the way, you will learn the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这两个函数对应于序列AutoML流程中的两个步骤。在更新步骤中，我们根据历史评估训练代理模型。在采样步骤中，我们使用获取函数来采样下一个要评估的模型。迭代这两个步骤将为我们提供额外的历史样本，以帮助训练一个更准确的代理模型。在本节的剩余部分，我们将提供一种贝叶斯优化搜索方法的逐步实现。在这个过程中，您将学习以下内容：
- en: How to vectorize hyperparameters for training the surrogate model
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何向量化超参数以训练代理模型
- en: The kind of surrogate model you should select
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该选择哪种代理模型
- en: How to initialize the process for training the initial surrogate model
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何初始化训练初始代理模型的流程
- en: How to design an acquisition function and sample the hyperparameters to be evaluated
    based on it
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何设计一个获取函数并根据它采样要评估的超参数
- en: 7.3.1 Vectorizing the hyperparameters
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 向量化超参数
- en: Because a Bayesian optimization search method, like other model-based search
    methods, trains a model based on the visited samples, a natural question is how
    to convert the hyperparameters into model-acceptable features. The most common
    way to do this is to encode the hyperparameters selected in each trial as a numerical
    vector, representing the features of an ML pipeline chosen in this trial. An inverse
    conversion will be applied to decode a vector selected in the sampling step into
    the original hyperparameters for instantiating an ML pipeline.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于贝叶斯优化搜索方法，就像其他基于模型的搜索方法一样，是根据访问过的样本来训练模型的，一个自然的问题是如何将超参数转换为模型可接受的特征。最常见的方法是将每个试验中选择的超参数编码为一个数值向量，表示在本试验中选择的ML管道的特征。将应用逆转换将采样步骤中选择的向量解码为原始超参数，以实例化ML管道。
- en: 'Let’s first implement the function for vectorizing the hyperparameters. We’ll
    make this a private method of the Oracle class named _vectorize_trials. The key
    idea is to extract all the hyperparameters one by one and concatenate them into
    a vector. During the search process, all the trials are saved in a dictionary
    attribute of the Oracle class named self.trials. The values and keys represent
    the trial objects and their unique IDs, respectively. The hyperparameters are
    saved in an attribute of the trial object (trial.hyperparameters). This is a hyperparameter
    container that contains the selected hyperparameters of the trial as well as the
    whole search space structure. We can retrieve the selected hyperparameters in
    each trial into a dictionary using trial.hyperparameters.values. Then, converting
    the hyperparameters selected in a trial becomes a matter of converting the values
    of a dictionary into a vector. If all our hyperparameter values are originally
    numerical, such as learning rate, number of units, and number of layers, we can
    directly concatenate them one by one. However, you need to pay attention to the
    following issues:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先实现一个用于向量化超参数的函数。我们将将其作为Oracle类的私有方法实现，命名为_vectorize_trials。关键思想是逐个提取所有超参数并将它们连接成一个向量。在搜索过程中，所有试验都保存在Oracle类的名为self.trials的字典属性中。值和键分别代表试验对象及其唯一ID。超参数保存在试验对象的属性中（trial.hyperparameters）。这是一个超参数容器，它包含试验中选定的超参数以及整个搜索空间结构。我们可以使用trial.hyperparameters.values检索每个试验中选定的超参数并将其放入字典中。然后，将试验中选定的超参数转换为向量就变成了将字典的值转换为向量的问题。如果我们的所有超参数值最初都是数值型的，例如学习率、单元数量和层数，我们可以直接逐个连接它们。然而，您需要注意以下问题：
- en: '*Handling hyperparameters with fixed values*—Because these hyperparameters
    do not affect the comparison between models, we can explicitly remove them so
    that the search method will not consider them. This can reduce the burden on the
    search method and avoid introducing extra noise into the update of the search
    method.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理具有固定值的超参数*—因为这些超参数不会影响模型之间的比较，我们可以明确地删除它们，这样搜索方法就不会考虑它们。这可以减轻搜索方法的负担并避免在搜索方法的更新中引入额外的噪声。'
- en: '*Dealing with inactive conditional hyperparameters*—Some conditional hyperparameters
    may not be selected in every trial. For example, suppose we have a hyperparameter
    called ''model_type'' to select between MLPs and CNNs. The hyperparameters of
    a CNN, such as the number of filters, will not be selected and used if the model
    selected for a trial is an MLP. This will cause the converted vectors to be of
    different lengths, so the elements in the same position in two vectors may not
    correspond to the same hyperparameter. A naive way of solving this problem is
    to use the default values of any inactive (not selected) hyperparameters in the
    vector. The hyperparameter container provides a method called is_active() to check
    whether a hyperparameter has been selected. You can append the selected value
    of the hyperparameter if it’s active or extract the default value saved in hyperparameters.default
    and append that instead if it isn’t.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理不活跃的条件超参数*—某些条件超参数可能不会在每次试验中被选中。例如，假设我们有一个名为''model_type''的超参数，用于在MLP和CNN之间进行选择。如果试验中选定的模型是MLP，则CNN的超参数（如滤波器数量）将不会被选中和使用。这会导致转换后的向量长度不同，因此两个向量中相同位置的元素可能不对应于同一超参数。解决这个问题的简单方法是在向量中使用任何不活跃（未选中）的超参数的默认值。超参数容器提供了一个名为is_active()的方法来检查超参数是否已被选中。如果超参数是活跃的，您可以附加其选中的值；如果不是，则提取保存在hyperparameters.default中的默认值并替换附加。'
- en: '*Dealing with hyperparameters with different scales*—Hyperparameters are often
    on different scales. For example, the learning rate is often smaller than 1, and
    the number of trees in a GBDT model could be larger than 100\. To normalize the
    hyperparameters, we can use the cumulative probability to transform them to values
    between 0 and 1\. Figure 7.3 shows two examples for converting the discrete search
    space and the continuous search space into the corresponding cumulative distributions.
    For the continuous search space, we directly map it into the intervals of 0 and
    1\. It will apply a log transformation if the hyperparameter is sampled in the
    logarithmic scale. For the discrete search space, we assume each value is uniformly
    distributed, and the probability unit will be equally bucketized based on the
    number of value choices in the space. We use the center value in each probability
    bucket to represent each value choice.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理不同尺度的超参数*——超参数通常在不同的尺度上。例如，学习率通常小于1，GBDT模型中的树的数量可能大于100。为了归一化超参数，我们可以使用累积概率将它们转换为0到1之间的值。图7.3显示了将离散搜索空间和连续搜索空间转换为相应的累积分布的两个示例。对于连续搜索空间，我们直接将其映射到0和1的区间。如果超参数在对数尺度上采样，将应用对数变换。对于离散搜索空间，我们假设每个值是均匀分布的，概率单位将根据空间中的值选择数量进行等分。我们使用每个概率桶中的中心值来表示每个值选择。'
- en: '*Dealing with categorical hyperparameters such as the model type*—To convert
    categorical hyperparameters to numerical features, we can use the index of the
    features in the list. For example, if we have four models to be selected, [MLP,
    CNN, RNN, GBDT], the list can be converted to [0, 1, 2, 3], where the models are
    represented by 0, 1, 2, and 3, respectively. The vector is then further normalized
    into 0 and 1 based on the mechanism for converting the discrete search space into
    the cumulative probabilities.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理模型类型等分类超参数*——为了将分类超参数转换为数值特征，我们可以使用列表中特征的索引。例如，如果我们有四个要选择的模型[MLP, CNN, RNN,
    GBDT]，列表可以转换为[0, 1, 2, 3]，其中模型分别用0, 1, 2, 3表示。然后，该向量进一步根据将离散搜索空间转换为累积概率的机制归一化到0和1。 '
- en: '![07-03](../Images/07-03.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![07-03](../Images/07-03.png)'
- en: Figure 7.3 Normalizing hyperparameter values based on cumulative probabilities
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 基于累积概率归一化超参数值
- en: The code in listing 7.5 describes the details of the vectorization process.
    We loop through all the existing trials to convert all the hyperparameters into
    feature vectors and the evaluation score of the corresponding model into one response
    vector. For each trial, we ignore the fixed hyperparameters and loop through the
    rest. If a hyperparameter is detected as active (used in the pipeline selected
    in the current trial), we will directly use the value selected by the search method.
    Otherwise, the default value is used to pad the vector to the same length as the
    others. Values in the vector are further substituted by the cumulative probabilities
    for normalization purposes. If a trial is completed, the evaluation result is
    appended in the response vector y. Because for some metrics, smaller values are
    better (such as the MSE), and for others, larger values are better (such as classification
    accuracy), we unify them such that larger values are better in all cases by multiplying
    the values of the first type of metric by -1.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5中的代码描述了向量化过程的细节。我们遍历所有现有的试验，将所有超参数转换为特征向量，并将对应模型的评估分数转换为一个响应向量。对于每个试验，我们忽略固定的超参数，遍历其余部分。如果一个超参数被检测为活动状态（在当前试验选择的管道中使用），我们将直接使用搜索方法选择的值。否则，使用默认值填充向量，使其与其他向量长度相同。为了归一化目的，向量中的值进一步替换为累积概率。如果一个试验完成，评估结果将附加到响应向量y中。因为对于某些指标，较小的值更好（例如MSE），而对于其他指标，较大的值更好（例如分类准确率），我们将它们统一，使得所有情况下较大的值更好，通过将第一种类型指标的值乘以-1。
- en: Listing 7.5 The private method to encode the hyperparameters as vectors
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 将超参数编码为向量的私有方法
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Loops through all the trials
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历所有试验
- en: ❷ Records the hyperparameters that are not fixed
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 记录未固定的超参数
- en: ❸ Detects if a hyperparameter is selected in the current trial
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检测当前试验中是否选择了超参数
- en: ❹ Uses the default value for the unused hyperparameter
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为未使用的超参数使用默认值
- en: ❺ Unifies the evaluation score so that larger values are always better
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 统一评估分数，使得较大的值始终更好
- en: 'Once we have a new set of hyperparameters represented in a vector format sampled
    based on the acquisition function, which will be introduced later, we need to
    feed the vector into the hyperparameter container as the values. The inverse transformation
    is straightforward and involves the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了基于获取函数（稍后介绍）采样的向量格式的新的超参数集，我们需要将向量作为值输入到超参数容器中。逆变换过程简单，涉及以下步骤：
- en: Convert the cumulative probabilities in the vector to the real values of each
    hyperparameter.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将向量中的累积概率转换为每个超参数的真实值。
- en: Feed the values of each hyperparameter into the hyperparameter container.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个超参数的值输入到超参数容器中。
- en: By looping through all the hyperparameters in the search space, we convert each
    value in the vector in order using these two steps. For each fixed hyperparameter,
    the default value (hp.value in the following listing) is put into the container.
    All the values are saved in a dictionary of the hyperparameter container (hps.values)
    and returned to help create the next trial. The implementation of the inverse
    transformation function is introduced in listing 7.6\. We will use this in the
    populate_ space() function to help convert the vectors selected by the acquisition
    function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遍历搜索空间中的所有超参数，我们按照这两个步骤依次转换向量中的每个值。对于每个固定的超参数，默认值（以下列表中的hp.value）被放入容器中。所有值都保存在超参数容器的字典（hps.values）中，并返回以帮助创建下一个试验。逆变换函数的实现介绍在列表7.6中。我们将在populate_空间()函数中使用它来帮助转换由获取函数选择的向量。
- en: Listing 7.6 The private method to decode vectors into hyperparameters
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 将向量解码为超参数的私有方法
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Creates an empty hyperparameter container
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个空的超参数容器
- en: ❷ Merges the hyperparameter into the container
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将超参数合并到容器中
- en: ❸ Uses the default value if the hyperparameter is fixed
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果超参数被固定，则使用默认值
- en: ❹ Converts the cumulative probability back to the hyperparameter value
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将累积概率转换回超参数值
- en: ❺ Puts the original value of the hyperparameter into the container
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将超参数的原始值放入容器中
- en: The encoding of hyperparameters should match the surrogate model adopted in
    the search method. We’ll use the Gaussian process as our surrogate model in the
    next stage, which takes vector inputs, so we adopt the vector representation here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的编码应与搜索方法中采用的代理模型相匹配。在下一阶段，我们将使用高斯过程作为我们的代理模型，它接受向量输入，因此我们在这里采用向量表示。
- en: Note Some recent work in the research community instead represents the hyperparameters
    as trees or graphs, where each node in the tree or graph represents a hyperparameter,
    and its leaves denote its conditional hyperparameters. These structures are good
    at representing the conditional hierarchy among the hyperparameters, and we can
    use some advanced tree-based or graph-based search methods to directly traverse
    the trees or graphs for sampling new combinations of hyperparameters. You can
    find more of them in the survey paper “Techniques for Automated Machine Learning,”
    by Yi-Wei Chen et al. (ACM SIGKDD Explorations Newsletter, 2020).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：研究社区中的一些近期工作将超参数表示为树或图，其中树或图中的每个节点代表一个超参数，其叶子表示其条件超参数。这些结构擅长表示超参数之间的条件层次结构，我们可以使用一些基于树或图的先进搜索方法来直接遍历树或图以采样新的超参数组合。你可以在Yi-Wei
    Chen等人撰写的调查论文“自动机器学习技术”（ACM SIGKDD Explorations Newsletter, 2020）中找到更多内容。
- en: 7.3.2 Updating the surrogate function based on historical model evaluations
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 基于历史模型评估更新代理函数
- en: In the context of AutoML, before it’s given any data, the surrogate function
    is only a prior denoting our subjective belief of what we think the real hyperparameter
    evaluation function will look like. For example, a common selection is a Gaussian
    process prior, shown in figure 7.4(a), which can be understood as a distribution
    function composed of infinitely many Gaussian random variables depicting the evaluation
    performance of all the models in our search space. The Gaussian process is fully
    specified by a mean function and a covariance function of all the Gaussian variables.
    The curve in the middle is the mean function presenting the mean values of all
    the Gaussian random variables, which we can denote by ![07-03-EQ03](../Images/07-03-EQ03.png).
    *x* denotes vectorized hyperparameters in AutoML (here, we have only one hyperparameter
    for illustrative purposes). The gray ranges indicate the standard deviation (STD)
    of the Gaussian variables, which can be denoted by ![07-04-EQ04](../Images/07-04-EQ04.png).
    In this case, each longitudinal section represents a Gaussian distribution. The
    mean value approximates the evaluation performance of an ML pipeline given the
    selected hyperparameters, *x*. The variance (or STD) indicates the uncertainty
    of the approximation. Because the variables have correlations, to fully describe
    the Gaussian process we need to define a covariance function, ![07-04-EQ05](../Images/07-04-EQ05.png) (often
    called the *kernel function*), to model the covariance between any two of the
    Gaussian variables, specifically, ![07-04-EQ06](../Images/07-04-EQ06.png). The
    covariance between variables is quite important to help predict the distribution
    given unseen hyperparameters. For example, if all the variables are independent,
    it means there are no conditional correlations in the performance of any two ML
    pipelines given their hyperparameters.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在AutoML的背景下，在给它任何数据之前，代理函数仅仅是一个先验，表示我们对真实超参数评估函数外观的主观信念。例如，一个常见的选择是高斯过程先验，如图7.4(a)所示，它可以理解为由无限多个高斯随机变量组成的分布函数，描述了搜索空间中所有模型的评估性能。高斯过程由所有高斯变量的均值函数和协方差函数完全指定。中间的曲线是均值函数，表示所有高斯随机变量的平均值，我们可以用![07-03-EQ03](../Images/07-03-EQ03.png)表示。*x*表示AutoML中的向量化超参数（在这里，我们只有一个超参数用于说明目的）。灰色范围表示高斯变量的标准差（STD），可以用![07-04-EQ04](../Images/07-04-EQ04.png)表示。在这种情况下，每个纵向截面代表一个高斯分布。平均值近似于给定所选超参数*x*的ML管道的评估性能。方差（或STD）表示近似的不确定性。因为变量之间存在相关性，为了完全描述高斯过程，我们需要定义一个协方差函数，![07-04-EQ05](../Images/07-04-EQ05.png)（通常称为*核函数*），来模拟任意两个高斯变量之间的协方差，具体来说，![07-04-EQ06](../Images/07-04-EQ06.png)。变量之间的协方差对于帮助预测给定未见过的超参数的分布非常重要。例如，如果所有变量都是独立的，这意味着在给定其超参数的任何两个ML管道的性能中不存在条件相关性。
- en: '![07-04](../Images/07-04.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![07-04](../Images/07-04.png)'
- en: Figure 7.4 The update of the Gaussian process surrogate model
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 高斯过程代理模型的更新
- en: In this case, the noise we have is white noise (corresponding to the *white
    kernel* in the Gaussian process). The only way to measure the performance of the
    ML pipelines is to evaluate them one by one on the real data, and the noise of
    each model is estimated by evaluating it multiple times. This is often not what
    we want because we expect to reduce the number of model evaluations, and it is
    often not the case in practice because similar hyperparameter settings tend to
    generate ML pipelines that have closer performance. As more and more data is collected
    (models are evaluated), the predicted mean function will pass through the new
    points, and the uncertainty (STD) will decrease, as shown in figure 7.4(b).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有的噪声是白噪声（对应于高斯过程中的*白核*）。衡量ML管道性能的唯一方法是对它们进行逐个评估，并通过对每个模型进行多次评估来估计每个模型的噪声。这通常不是我们想要的，因为我们期望减少模型评估的数量，而在实践中通常不是这样，因为相似的超参数设置往往会生成性能更接近的ML管道。随着收集越来越多的数据（评估模型），预测的均值函数将穿过新的点，不确定性（STD）将降低，如图7.4(b)所示。
- en: The selection of the kernel function depends on our assumption of the smoothness
    of the objective function. It is a hyper-hyperparameter that is not included in
    the search space and should be manually selected or tuned. A common kernel selection
    is the *Matérn kernel*. It has a parameter (![07-04-EQ07](../Images/07-04-EQ07.png))
    to set the degree of smoothness of the function. We often set ![07-04-EQ07](../Images/07-04-EQ07.png) to
    0.5, 1.5, or 2.5, corresponding to our assumption that the function should be
    one, two, or three times differentiable, respectively. When ![07-04-EQ07](../Images/07-04-EQ07.png) approaches
    infinity, the Matérn kernel becomes closer to a kernel called the *squared exponential
    kernel* (also known as the *RBF kernel*, which you may recall from the SVM model
    in the previous chapter), reflecting that the objective function is infinitely
    differentiable. There are also some kernels for modeling periodic functions, such
    as the *Exp-Sine-Squared kernel*. You can learn more about different kernels in
    the book *Gaussian Processes for Machine Learning* by Carl Edward Rasmussen and
    Christopher K. I. Williams (MIT Press, 2006).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数的选择取决于我们对目标函数平滑度的假设。它是一个超超参数，不包括在搜索空间中，应手动选择或调整。常见的核函数选择是 *Matérn 核函数*。它有一个参数
    ![07-04-EQ07](../Images/07-04-EQ07.png) 用于设置函数的平滑度程度。我们通常将 ![07-04-EQ07](../Images/07-04-EQ07.png)
    设置为 0.5、1.5 或 2.5，分别对应于我们假设函数应该是一、二或三次可微的。当 ![07-04-EQ07](../Images/07-04-EQ07.png)
    接近无穷大时，Matérn 核函数就接近一个称为 *平方指数核函数*（也称为 *径向基函数核*，您可能在前一章的 SVM 模型中记得），这反映了目标函数是无限可微的。还有一些用于建模周期函数的核函数，例如
    *Exp-Sine-Squared 核函数*。您可以在 Carl Edward Rasmussen 和 Christopher K. I. Williams
    所著的《机器学习中的高斯过程》（MIT Press，2006年）一书中了解更多关于不同核函数的信息。
- en: To implement a Gaussian process model for Bayesian optimization, we can use
    the gaussian_process module in the scikit-learn library. When initializing the
    oracle, we can create a Gaussian process model with the Matérn kernel. The alpha
    parameter is used to specify the amount of random noise introduced during the
    evaluation of the models. We often set it to be a small number, and empirically,
    this is enough and good for taking environmental noise into account. We implement
    the initial training and sequential update of the Gaussian process model in the
    populate_space() function. The Gaussian process model is updated once we have
    the evaluation of the new models searched in each round. In the beginning, we
    randomly sample several models for evaluation, then conduct the initial training
    of the Gaussian process model. The number of random samples is defined as two,
    if not provided in the num_initial_points attribute, to make sure the kernel function
    can feasibly be applied. (Empirically, the square root of the number of hyperparameters
    in the search space is a good number of random points to use to initialize the
    Gaussian process model.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现用于贝叶斯优化的高斯过程模型，我们可以使用 scikit-learn 库中的 gaussian_process 模块。在初始化预言者时，我们可以使用
    Matérn 核函数创建一个高斯过程模型。alpha 参数用于指定在模型评估过程中引入的随机噪声量。我们通常将其设置为一个小数，并且根据经验，这足以并且对考虑环境噪声来说是好的。我们在
    populate_space() 函数中实现了高斯过程模型的初始训练和顺序更新。一旦我们有了每一轮中搜索的新模型的评估，高斯过程模型就会更新。最初，我们随机采样几个模型进行评估，然后对高斯过程模型进行初始训练。如果未在
    num_initial_points 属性中提供，则随机样本的数量定义为 2，以确保核函数可以实际应用。（根据经验，搜索空间中超参数数量的平方根是初始化高斯过程模型时使用的随机点的良好数量。）
- en: Once we have enough random samples, we vectorize the hyperparameters and the
    evaluations and fit the Gaussian process model by calling the fit() function,
    as shown in listing 7.7\. Later, whenever a new model is evaluated during the
    sequential search process, we will fit the model again based on all the completed
    trials. Here we describe only the update of the Gaussian process model and do
    not show the sampling process based on the acquisition function. You’ll see the
    full implementation of the populate_space() function in the next step.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了足够的随机样本，我们将超参数和评估向量化，并通过调用 fit() 函数来拟合高斯过程模型，如列表 7.7 所示。后来，在顺序搜索过程中，每当评估新的模型时，我们都会基于所有完成的试验重新拟合模型。在这里，我们只描述高斯过程模型的更新，而不展示基于获取函数的采样过程。您将在下一步中看到
    populate_space() 函数的完整实现。
- en: Listing 7.7 Creating and updating the Gaussian process model in the oracle
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.7 在预言者中创建和更新高斯过程模型
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Uses 2 as the initial number of random points if not specified
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果未指定，则使用 2 作为初始随机点的数量
- en: ❷ Initializes the Gaussian process model
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化高斯过程模型
- en: ❸ Conducts random sampling for initializing the Gaussian process model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对初始化高斯过程模型进行随机采样
- en: ❹ Vectorizes all the trials
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 向量化所有试验
- en: ❺ Fits the Gaussian process model based on the completed trials
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 根据完成的试验拟合高斯过程模型
- en: 'The complexity for fitting *n* data points would be *O*(*n*³), which is quite
    time-consuming. This is the major drawback of the Gaussian process for the model-based
    search method. Different methods use other surrogate models, such as tree-based
    models (e.g., random forests) and neural networks, to overcome this. You can learn
    more about surrogate models used for Bayesian optimization in AutoML from the
    book *Automated Machine Learning: Methods, Systems, Challenges* by Frank Hutter,
    Lars Kotthoff, and Joaquin Vanschoren (Springer Nature, 2019).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合 *n* 个数据点的复杂度会是 *O*(*n*³)，这相当耗时。这是高斯过程在基于模型的搜索方法中的主要缺点。不同的方法使用其他代理模型，例如基于树的模型（例如，随机森林）和神经网络，来克服这一点。您可以从Frank
    Hutter、Lars Kotthoff和Joaquin Vanschoren合著的《自动机器学习：方法、系统、挑战》（Springer Nature，2019年）一书中了解更多关于用于自动机器学习中的贝叶斯优化的代理模型。
- en: 7.3.3 Designing the acquisition function
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 设计获取函数
- en: Once we have a surrogate model, we need an acquisition function to help sample
    the next ML pipeline for evaluation and create a closed sequential search loop.
    Let’s first introduce the design criteria of an acquisition function and then
    discuss how to sample a point based on that function.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了代理模型，我们需要一个获取函数来帮助采样下一个机器学习管道以进行评估，并创建一个封闭的顺序搜索循环。让我们首先介绍获取函数的设计标准，然后讨论如何根据该函数采样一个点。
- en: Design criteria of an acquisition function
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 获取函数的设计标准
- en: 'A good acquisition function should measure how desirable a point is for sampling.
    Desirability is a tradeoff between two aspects: *exploitation* and *exploration*.
    Exploitation means we want to discover the points that the surrogate model predicts
    to be good. This would take into account the promising regions we have already
    explored but lack the power of exploring unknown regions. For example, in figure
    7.5, suppose our real objective function curve is *f*(*x*), and given five points,
    we fit a Gaussian process with mean function ![07-03-EQ03](../Images/07-03-EQ03.png) running
    through three of the points. The region around the hyperparameter *x[a]* has been
    explored more than the region around point *x[b]*, leading the STD around *x[b]*
    to be much larger than the one around *x[a]*. If we consider only the predicted
    mean function, which takes full advantage of the exploitation power, *x[a]* is
    better than *x[b]*. However, *x[a]* is worse than *x[b]* on the objective function.
    This requires an acquisition function to balance both exploitation (mean) and
    exploration (variance).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的获取函数应该衡量一个点对于采样的吸引力。吸引力是两个方面的权衡：*利用*和*探索*。利用意味着我们希望发现代理模型预测为好的点。这将考虑到我们已经探索过的有希望的领域，但缺乏探索未知领域的能力。例如，在图7.5中，假设我们的真实目标函数曲线是
    *f*(*x*)，并且给定五个点，我们通过三个点拟合了一个高斯过程，其均值函数为 ![07-03-EQ03](../Images/07-03-EQ03.png)。超参数
    *x[a]* 附近的区域比点 *x[b]* 附近的区域探索得更多，导致 *x[b]* 附近的STD比 *x[a]* 附近的STD大得多。如果我们只考虑预测的均值函数，充分利用利用能力，*x[a]*
    比较好于 *x[b]*。然而，*x[a]* 在目标函数上比 *x[b]* 差。这需要一个获取函数来平衡利用（均值）和探索（方差）。
- en: '![07-05](../Images/07-05.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![07-05](../Images/07-05.png)'
- en: Figure 7.5 Updating the Gaussian process surrogate model
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 更新高斯过程代理模型
- en: Let’s now look at three commonly used acquisition functions and their implementations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看三种常用的获取函数及其实现。
- en: Upper confidence bound
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上置信界
- en: Assuming larger values for *y* are better, the *upper confidence bound* (UCB)
    balances exploitation and exploration in a straightforward way by adding the mean
    and STD functions together: ![07-05-EQ08](../Images/07-05-EQ08.png), where ![07-05-EQ09](../Images/07-05-EQ09.png) is
    a user-specified positive parameter for balancing the tradeoff between the two
    terms. As shown in figure 7.6, the curve *g*(*x*) is the UCB acquisition function
    when ![07-05-EQ10](../Images/07-05-EQ10.png). If smaller values are preferable,
    we can use ![07-05-EQ11](../Images/07-05-EQ11.png).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *y* 的较大值更好，*上置信界* (UCB) 通过将均值和STD函数相加以简单直接地平衡利用和探索： ![07-05-EQ08](../Images/07-05-EQ08.png)，其中 ![07-05-EQ09](../Images/07-05-EQ09.png)
    是用户指定的用于平衡两个术语之间权衡的正参数。如图7.6所示，曲线 *g*(*x*) 是当 ![07-05-EQ10](../Images/07-05-EQ10.png)
    时的UCB获取函数。如果较小的值更可取，我们可以使用 ![07-05-EQ11](../Images/07-05-EQ11.png)。
- en: '![07-06](../Images/07-06.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![07-06](../Images/07-06.png)'
- en: Figure 7.6 Upper confidence bound
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6上置信界
- en: Example code for implementing the function is shown in the following listing.
    Here, self.gpr denotes the fitted Gaussian process regressor, and *x* denotes
    the vectorized hyperparameters of an ML pipeline.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实现该函数的示例代码如下所示。在这里，self.gpr表示拟合的高斯过程回归器，*x*表示ML管道的超参数向量。
- en: Listing 7.8 Calculating the upper confidence bound
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8计算上置信界
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Probability of improvement
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 改善概率
- en: '*Probability of improvement* (PI) measures the probability of a sample achieving
    better performance than the best sample found so far. As shown in figure 7.7,
    given the best point, ![07-06-EQ12](../Images/07-06-EQ12.png), and the corresponding
    objective value, ![07-06-EQ13](../Images/07-06-EQ13.png), the probability that
    the point *x* would achieve better performance than ![07-06-EQ12](../Images/07-06-EQ12.png) equals
    the shaded region of the Gaussian distribution defined by ![07-03-EQ03](../Images/07-03-EQ03.png) and ![07-04-EQ04](../Images/07-04-EQ04.png).
    We can calculate this with the help of the cumulative distribution function of
    the normal distribution.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*改善概率*（PI）衡量一个样本实现比迄今为止找到的最佳样本更好的性能的概率。如图7.7所示，给定最佳点![07-06-EQ12](../Images/07-06-EQ12.png)和相应的目标值![07-06-EQ13](../Images/07-06-EQ13.png)，点*x*实现比![07-06-EQ12](../Images/07-06-EQ12.png)更好的性能的概率等于由![07-03-EQ03](../Images/07-03-EQ03.png)和![07-04-EQ04](../Images/07-04-EQ04.png)定义的高斯分布的阴影区域。我们可以借助正态分布的累积分布函数来计算这个概率。'
- en: '![07-07](../Images/07-07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![07-07](../Images/07-07.png)'
- en: Figure 7.7 Probability of improvement
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7改进概率
- en: The code implementation of PI is shown in listing 7.9\. We can directly use
    the evaluation objective if we assume no environmental noise will exist, or we
    can set a noise value (alpha!=0 when creating the regressor with scikit-learn)
    and select the best point using the predicted value given by the noise-based Gaussian
    process regressor. A problem with PI is that it tends to query the points that
    are close to the ones we’ve already evaluated, especially the best point, leading
    to a high tendency of exploitation and low exploration power of underexplored
    regions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PI的代码实现如列表7.9所示。如果我们假设不存在环境噪声，我们可以直接使用评估目标，或者我们可以设置一个噪声值（当使用scikit-learn创建回归器时alpha不等于0）并使用基于噪声的高斯过程回归器给出的预测值选择最佳点。PI的一个问题是它倾向于查询接近我们已评估的点，尤其是最佳点，导致对已探索区域的利用度高，探索能力低。
- en: Listing 7.9 Calculating the probability of improvement
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9计算改进概率
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Vectorizes all the trials
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向量化所有试验
- en: ❷ Calculates the best surrogate score found so far
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算迄今为止找到的最佳代理分数
- en: ❸ Calculates mean and standard deviation via surrogate function
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过代理函数计算均值和标准差
- en: ❹ Calculates the probability of improvement
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算改进概率
- en: Expected improvement
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 期望改进
- en: '*Expected improvement* (EI) mitigates the problem of PI by weighting the calculation
    of the probability of improvement using the magnitude of improvement (see figure
    7.8). This is equivalent to calculating the expectation of the improvement over
    the optimal value found so far: ![07-07-EQ14](../Images/07-07-EQ14.png).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*期望改进*（EI）通过使用改进幅度的量来加权计算改进概率，从而缓解了PI的问题（见图7.8）。这相当于计算到目前为止找到的最优值的期望改进：![07-07-EQ14](../Images/07-07-EQ14.png)。'
- en: '![07-08](../Images/07-08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![07-08](../Images/07-08.png)'
- en: Figure 7.8 Expected improvement of a point
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8某点的期望改进
- en: Listing 7.10 provides an explicit equation to calculate EI leveraging the probability
    density function (norm.pdf) and cumulative distribution function (norm.pdf) of
    the normal distribution. The detailed derivation of the equation can be found
    in the paper “Efficient Global Optimization of Expensive Black-Box Functions,”
    by Donald R. Jones, Matthias Schonlau, and William J. Welch ([http://mng.bz/5KlO](http://mng.bz/5KlO)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10提供了一个显式方程，用于利用正态分布的概率密度函数（norm.pdf）和累积分布函数（norm.pdf）来计算EI。该方程的详细推导可以在Donald
    R. Jones、Matthias Schonlau和William J. Welch的论文“Efficient Global Optimization of
    Expensive Black-Box Functions”中找到（[http://mng.bz/5KlO](http://mng.bz/5KlO)）。
- en: Listing 7.10 Calculating the expected improvement
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10计算期望改进
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To enhance the exploration power of PI and EI, we can also add a positive parameter
    to the optimal objective value: (![07-08-EQ15](../Images/07-08-EQ15.png)). A larger
    value for the parameter ![07-08-EQ16](../Images/07-08-EQ16.png) will lead to a
    greater amount of exploration. In practice, UCB and EI are the most commonly used
    types of acquisition functions in existing AutoML libraries. UCB is comparably
    more straightforward in balancing exploration and exploitation.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强 PI 和 EI 的探索能力，我们还可以将一个正参数添加到最优目标值中：([07-08-EQ15](../Images/07-08-EQ15.png))。参数
    ![07-08-EQ16](../Images/07-08-EQ16.png) 的值越大，探索量就越大。在实践中，UCB 和 EI 是现有 AutoML 库中最常用的获取函数类型。UCB
    在平衡探索和利用方面相对更直接。
- en: 7.3.4 Sampling the new hyperparameters via the acquisition function
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 通过获取函数采样新的超参数
- en: Now that you know how to create an acquisition function, it is time to leverage
    the function to sample the hyperparameter values evaluated in the next trial.
    The goal is to find the hyperparameter vector that achieves the maximum value
    of the acquisition function. This is a constraint optimization problem, because
    each hyperparameter is bounded by the defined search space. A popular optimization
    method for minimizing functions with bound constraints is the *L-BFGS-B* *algorithm*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何创建一个获取函数，现在是时候利用这个函数来采样下一个试验中评估的超参数值。目标是找到使获取函数达到最大值的超参数向量。这是一个约束优化问题，因为每个超参数都被定义的搜索空间所限制。一种用于最小化有边界约束的函数的流行优化方法是
    *L-BFGS-B 算法*。
- en: 'Note BFGS is an optimization algorithm that starts with an initial vector and
    iteratively refines it toward the local optimum based on an estimate of the inverse
    Hessian matrix. We can optimize multiple times with different initialization vectors
    to achieve a better local optimum. L-BFGS optimizes it with approximation so that
    the amount of memory used during the optimization can be limited. L-BFGS-B further
    extends the algorithm to handle bounding box constraints of the search space.
    More details can be found in the article by D. C. Liu and J. Nocedal, “On the
    Limited Memory Method for Large Scale Optimization,” *Mathematical Programming*
    45, no. 3 (1989): 503-528 (doi:10.1007/BF01589116).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '注意：BFGS 是一种优化算法，它从一个初始向量开始，根据逆海森矩阵的估计迭代地将其优化到局部最优。我们可以使用不同的初始化向量多次优化以获得更好的局部最优。L-BFGS
    通过近似优化它，以便在优化过程中使用的内存量可以限制。L-BFGS-B 进一步扩展了算法以处理搜索空间的边界框约束。更多细节可以在 D. C. Liu 和
    J. Nocedal 的文章中找到，“关于大规模优化的有限内存方法”，*Mathematical Programming* 45, no. 3 (1989):
    503-528 (doi:10.1007/BF01589116)。'
- en: The method is implemented with the optimize module in the scipy Python toolkit,
    as shown in listing 7.11\. We first implement a function called get_hp_bounds()
    to collect the bounds of the hyperparameters. Because we’ve normalized the hyperparameters
    based on the cumulative probabilities, the bounds are set to be 0 to 1 for each
    hyperparameter. We optimize the acquisition function 50 times with 50 different
    initialization vectors uniformly generated in the x_seeds. The optimization is
    done in a continuous vector space, and the optimal vector can be transformed back
    to the original hyperparameter values using the function self._vector_to_values
    defined in the Oracle base class.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法使用 scipy Python 工具包中的 optimize 模块实现，如列表 7.11 所示。我们首先实现一个名为 get_hp_bounds()
    的函数来收集超参数的边界。因为我们已经根据累积概率对超参数进行了归一化，所以边界被设置为每个超参数的 0 到 1。我们在 x_seeds 中均匀生成 50
    个不同的初始化向量，并使用 50 个不同的初始化向量优化获取函数 50 次。优化是在连续向量空间中进行的，并且可以使用 Oracle 基类中定义的 self._vector_to_values
    函数将最优向量转换回原始超参数值。
- en: Listing 7.11 Sample based on the UCB acquisition function
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.11 基于 UCB 获取函数的样本
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Appends the normalized bound for a hyperparameter
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加超参数的归一化边界
- en: ❷ Does random search for training the initial Gaussian process regressor
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对训练初始高斯过程回归器的随机搜索
- en: ❸ The sign of the UCB score is flipped for minimization purposes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为了最小化的目的，UCB得分的符号被反转。
- en: ❹ Uniformly generates 50 random vectors within the boundaries
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在边界内均匀生成 50 个随机向量
- en: ❺ Minimizes the flipped acquisition function
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 最小化反转的获取函数
- en: ❻ Maps the optimal vector to the original hyperparameter values
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将最优向量映射到原始超参数值
- en: By combining the vectorization function, the sampling function, and the function
    for creating the Gaussian process regressor, we can create a complete Bayesian
    optimization oracle for conducting AutoML tasks. Next, we’ll use it to tune the
    GBDT model for the housing price-prediction task.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合向量化函数、采样函数和创建高斯过程回归器的函数，我们可以创建一个完整的贝叶斯优化算子，用于执行AutoML任务。接下来，我们将使用它来调整房价预测任务的GBDT模型。
- en: 7.3.5 Tuning the GBDT model with the Bayesian optimization method
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.5 使用贝叶斯优化方法调整GBDT模型
- en: We load and split the data the same way we did in the random search section
    and use the customized tuner for tuning the GBDT model. The only difference is
    that we change the random search oracle to the Bayesian optimization oracle, as
    shown in the following listing. The best model achieves an MSE of 0.2202 on the
    final test set.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与随机搜索部分相同的方式加载数据并分割数据，并使用定制的调整器来调整GBDT模型。唯一的区别是我们将随机搜索算子更改为贝叶斯优化算子，如下所示。最佳模型在最终测试集上的均方误差（MSE）为0.2202。
- en: Listing 7.12 Sampling based on the UCB acquisition function
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.12 基于UCB获取函数的采样
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Uses the customized Bayesian optimization search oracle
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用定制的贝叶斯优化搜索算子
- en: Let’s compare the results of Bayesian optimization search with random search
    to understand the two methods better. We extract the evaluation performance of
    all the models discovered in order. Figure 7.9(a) directly displays the MSE of
    the discovered models evaluated on the validation set during the search process.
    Unlike with the random search method, the performance of models searched by Bayesian
    optimization gradually stabilizes as the search process continues. This is because
    Bayesian optimization takes the historical information into account and can leverage
    exploitation for searching, so the models discovered later are likely to achieve
    comparable or even better performance than the ones discovered earlier. Figure
    7.9(b) plots the performance of the best model found so far as the search process
    proceeds. We can see that the random search performs a bit better at the beginning
    but becomes worse in the later stages. This is because random search can provide
    a better exploration of the search space than Bayesian optimization search at
    the beginning, but as the amount of historical data collected increases, that
    information can be exploited to benefit the search process.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较贝叶斯优化搜索与随机搜索的结果，以更好地理解这两种方法。我们按顺序提取了在搜索过程中发现的全部模型的评估性能。图7.9(a)直接显示了在验证集上评估的发现模型的均方误差（MSE）。与随机搜索方法不同，贝叶斯优化搜索的模型性能随着搜索过程的继续而逐渐稳定。这是因为贝叶斯优化考虑了历史信息，并可以利用利用性进行搜索，因此后来发现的模型可能比早期发现的模型具有相当甚至更好的性能。图7.9(b)显示了随着搜索过程的进行，迄今为止找到的最佳模型的性能。我们可以看到，随机搜索在开始时表现略好，但在后期变得较差。这是因为随机搜索在开始时比贝叶斯优化搜索能更好地探索搜索空间，但随着收集的历史数据量增加，这些信息可以被利用来改善搜索过程。
- en: Although the Bayesian optimization method outperforms random search in this
    example, this is not always the case in practice, especially when the search space
    is small and there are a lot of categorical and conditional hyperparameters. We
    should select and tune different search methods, taking into account the size
    of the search space, the number of search iterations, and the time and resource
    constraints (the vanilla Bayesian optimization method runs much slower than random
    search, as you may have experienced). If no specific constraints are specified,
    Bayesian optimization search is a good place to start.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个例子中贝叶斯优化方法优于随机搜索，但在实际应用中并不总是如此，尤其是在搜索空间较小且存在大量分类和条件超参数的情况下。我们应该根据搜索空间的大小、搜索迭代次数以及时间和资源限制（如你所经历的，纯贝叶斯优化方法运行速度远慢于随机搜索）来选择和调整不同的搜索方法。如果没有指定具体限制，贝叶斯优化搜索是一个不错的起点。
- en: '![07-09](../Images/07-09.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![07-09](../Images/07-09.png)'
- en: Figure 7.9 Comparing the results of Bayesian optimization and random search
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 比较贝叶斯优化和随机搜索的结果
- en: 'In addition to the increased complexity, another problem that you may face
    when applying Bayesian optimization is the local optimum issue. Though we’ve tried
    to explore multiple initialization points when optimizing the acquisition function
    for sampling, it is still likely to always sample from a local region if the surrogate
    model is not fit well based on the historical samples or if the acquisition function
    favors exploitation too much. Doing this will lead to concentrated exploitation
    of a local region while ignoring exploring other areas if the evaluation performance
    surface is not convex. Besides increasing the exploration preference of the acquisition
    function, such as by reducing the ![07-05-EQ09](../Images/07-05-EQ09.png) parameter
    in the UCB acquisition function, we have the following two commonly used tricks
    to prevent Bayesian optimization from converging to a local optimum:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了增加的复杂性之外，当应用贝叶斯优化时，你可能会遇到另一个问题，即局部最优问题。尽管我们在优化采样获取函数时尝试探索多个初始化点，但如果基于历史样本的代理模型拟合不佳，或者获取函数过于偏向于利用，那么仍然很可能会总是从局部区域采样。这样做会导致对局部区域的集中利用，而忽略探索其他区域，如果评估性能表面不是凸的。除了增加获取函数的探索偏好，例如通过减少UCB获取函数中的![07-05-EQ09](../Images/07-05-EQ09.png)参数，我们还有以下两个常用的技巧来防止贝叶斯优化收敛到局部最优：
- en: Conduct Bayesian optimization search multiple times, and use different random
    seeds to sample different random points to fit the initial surrogate model.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多次进行贝叶斯优化搜索，并使用不同的随机种子来采样不同的随机点以拟合初始代理模型。
- en: 'Combine Bayesian optimization and random search in a dynamic way: conduct a
    random search iteration after every several iterations (say, five) of Bayesian
    optimization search, alternating the two.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以动态方式结合贝叶斯优化和随机搜索：在贝叶斯优化搜索的每几次迭代（比如说，五次）之后进行一次随机搜索迭代，交替进行两种方法。
- en: In addition, if the number of search iterations is large and you have the time,
    it’s a good habit to use cross-validation for each discovered model rather than
    to simply evaluate each model on a fixed validation set. This can help prevent
    the search method from overfitting on the validation set and is often practically
    useful for any search algorithm.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果搜索迭代次数较多，并且你有时间，那么对每个发现的模型使用交叉验证而不是简单地在一个固定的验证集上评估每个模型，这是一个好习惯。这有助于防止搜索方法在验证集上过度拟合，并且对于任何搜索算法通常都是实际有用的。
- en: 7.3.6 Resuming the search process and recovering the search method
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.6 恢复搜索过程和恢复搜索方法
- en: Because the AutoML process is often quite long and may be unexpectedly interrupted,
    we can add two auxiliary functions to help resume the search process and recover
    the oracle (see listing 7.13). The base Oracle class of KerasTuner provides two
    functions that can be extended to memorize and reload the historical trials and
    metadata to recover the oracle. First, we can extend the get_state() function,
    which memorizes the state of the historical trials and parameters of the oracle
    during the search process. This function will be called in every search loop to
    save the current state of the trials and the oracle. To implement it, we first
    need to call the get_state() function of the base class to get the state dictionary
    of the current trial, then update it with the unique hyperparameters of the search
    method. For example, we can save the random seeds, the number of random initialization
    trials, and the exploitation-exploration tradeoff parameter in the UCB acquisition
    function in the state object. Second, to reload the state of the oracle, we can
    extend the set_state() function. The function will access the previous state reloaded
    from the disk and retrieve information on all the historical trials and the oracle’s
    parameters. For example, in a Bayesian optimization oracle, we can call the set_state()
    function to retrieve all the model evaluation information and recover the attributes
    of the oracle one by one using the loaded state dictionary.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AutoML过程通常相当长，并且可能会意外中断，我们可以添加两个辅助函数来帮助恢复搜索过程并恢复预言者（参见列表7.13）。KerasTuner的基本Oracle类提供了两个可以扩展的函数，用于记忆和重新加载历史试验和元数据以恢复预言者。首先，我们可以扩展get_state()函数，该函数在搜索过程中记忆预言者历史试验的状态和参数。此函数将在每个搜索循环中被调用以保存试验和预言者的当前状态。为了实现它，我们首先需要调用基类的get_state()函数以获取当前试验的状态字典，然后使用搜索方法的唯一超参数更新它。例如，我们可以在状态对象中保存随机种子、随机初始化试验的数量以及UCB获取函数中的利用-探索权衡参数。其次，为了重新加载预言者的状态，我们可以扩展set_state()函数。该函数将访问从磁盘重新加载的先前状态，并检索有关所有历史试验和预言者参数的信息。例如，在贝叶斯优化预言者中，我们可以调用set_state()函数来检索所有模型评估信息，并使用加载的状态字典逐个恢复预言者的属性。
- en: Listing 7.13 Resuming the oracle
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.13 恢复预言者
- en: '[PRE12]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Saves the oracle-specific configurations in the state
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在状态中保存特定于预言者的配置
- en: ❷ Reloads the historical state
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重新加载历史状态
- en: ❸ Resumes the Bayesian optimization oracle
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 恢复贝叶斯优化预言者
- en: When resuming the search process, we can initialize the tuner with the name
    of the project we want it to resume from and conduct the search in the same way
    as before. The only difference is that we set the overwrite argument to False
    during the initialization so that the tuner will automatically resume the search
    process if there is an existing project with the same name (bo_tuner in the following
    listing) in the working directory. The set_state() function we’ve implemented
    will then be called to help recover the oracle.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在恢复搜索过程时，我们可以使用想要从中恢复的项目名称初始化tuner，并以前述方式进行搜索。唯一的区别是在初始化期间将overwrite参数设置为False，这样tuner将自动恢复与工作目录中现有项目（以下列表中的bo_tuner）具有相同名称的搜索过程。我们已实现的set_state()函数将随后被调用以帮助恢复预言者。
- en: Listing 7.14 Resuming the oracle
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.14 恢复预言者
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Does not overwrite the project that’s named, if it already exists
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果已存在，则不会覆盖命名项目
- en: ❷ Provides the project name to resume and/or to save the search process
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提供要恢复和/或保存搜索过程的项目名称
- en: The next section will introduce another commonly used history-dependent method,
    which does not require the selection of a surrogate model or acquisition function.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将介绍另一种常用的基于历史的方法，它不需要选择代理模型或获取函数。
- en: 7.4 Customizing an evolutionary search method
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 自定义进化搜索方法
- en: 'An *evolutionary search method* is a heuristic search method inspired by biological
    behaviors. One of the most popular methods that has been used in AutoML is the
    *population-based* evolutionary search method, which simulates the evolution of
    a biological population by following these four steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*进化搜索方法*是一种受生物行为启发的启发式搜索方法。在AutoML中已被广泛使用的一种最流行的方法是基于*种群*的进化搜索方法，它通过以下四个步骤模拟生物种群的发展：'
- en: '*Initial population generation*—Randomly generate a set of initial ML pipelines
    and evaluate them to form the initial population. We should predefine the size
    of the population before starting.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*初始种群生成*—随机生成一组初始机器学习管道并评估它们以形成初始种群。在开始之前，我们应该预先定义种群的大小。'
- en: '*Parent selection*—Select the fittest pipelines, called parents, for breeding
    the new child pipeline (offspring) to be evaluated in the next trial.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*父代选择*—选择最适应的管道，称为父代，用于繁殖新的子管道（后代）以在下一个试验中进行评估。'
- en: '*Crossover and mutation*—These operations can be used to breed new offspring
    based on the parents. *Crossover* means we swap some of the hyperparameters of
    two parental pipelines to form two new pipelines. *Mutation* means we randomly
    change some of the hyperparameters of the parents or the offspring generated from
    the crossover operation to introduce some variation. This operation imitates the
    “tweak in the chromosome” of genetic mutation, to enhance the exploration power
    during the search process. Both the crossover and mutation operations do not have
    to be carried out. We can use just one of them to generate the new offspring.
    For example, rather than combining two pipelines, we can select one parent ML
    pipeline from the population in each trial and mutate one or more of its hyperparameters
    to generate the next pipeline to be evaluated.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*交叉和变异*—这些操作可以根据父代繁殖新的后代。*交叉*意味着我们交换两个父代管道的一些超参数以形成两个新的管道。*变异*意味着我们随机改变父代或从交叉操作生成的后代的一些超参数以引入一些变异。此操作模仿遗传变异中的“染色体中的调整”，以增强搜索过程中的探索能力。交叉和变异操作不必都执行。我们可以只用其中一个来生成新的后代。例如，我们不必结合两个管道，我们可以在每个试验中从种群中选择一个父代机器学习管道，并对其一个或多个超参数进行变异，以生成下一个要评估的管道。'
- en: '*Survivor selection (population regeneration)*—After the new offspring have
    been evaluated, this step recreates a new population set of ML pipelines by replacing
    the least-fit pipelines with the new offspring.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*生存者选择（种群再生）*—在新的后代被评估后，此步骤通过用新的后代替换最不适应的管道来重新创建一组新的机器学习管道种群。'
- en: Steps 2 to 4 are carried out iteratively during the search process to incorporate
    the new evaluations, as shown in figure 7.10.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2到4在搜索过程中迭代执行，以纳入新的评估，如图7.10所示。
- en: '![07-10](../Images/07-10.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![07-10](../Images/07-10.png)'
- en: Figure 7.10 Population-based evolutionary search life cycle
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 基于人群的进化搜索生命周期
- en: 7.4.1 Selection strategies in the evolutionary search method
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 进化搜索方法中的选择策略
- en: Although the crossover and mutation step determines how we create new offspring
    from the existing pipelines, the chosen strategies in the two selection steps,
    parent selection and survivor selection, can be more important in designing a
    good evolutionary method. The selection steps should balance exploitation and
    exploration during the search process. Exploitation here represents how intensively
    we want to select a pipeline with good evaluation performance as the parent. Exploration
    means introducing more randomness to try out unexplored regions, rather than focusing
    only on the fittest pipelines. The tradeoff between exploitation and exploration
    is also called the balance between *selection intensity* and *selection diversity*
    in the literature of the evolutionary method. Let’s look at three popular selection
    methods.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然交叉和变异步骤决定了我们如何从现有管道中创建新的后代，但在设计良好的进化方法中，两个选择步骤中选择的策略（父代选择和生存者选择）可能更为重要。选择步骤应在搜索过程中平衡利用和探索。这里的利用代表我们希望多么强烈地选择具有良好评估性能的管道作为父代。探索意味着引入更多的随机性来尝试未探索的区域，而不是仅仅关注最适应的管道。利用和探索之间的权衡也被称为进化方法文献中*选择强度*和*选择多样性*之间的平衡。让我们看看三种流行的选择方法。
- en: Proportional selection
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 比例选择
- en: In *proportional selection* (or *roulette wheel selection*), we select an individual
    based on a probability distribution. The probability of selecting an individual
    is proportional to its fitness. For example, if we want to select an ML pipeline
    for classification, we can use the accuracy to measure the fitness of each pipeline.
    The higher the pipeline’s accuracy, the larger the probability we should assign
    to it. A pipeline *i* could be selected as a parent with probability ![07-10-EQ17](../Images/07-10-EQ17.png),
    where *f[i]* denotes the non-negative accuracy of pipeline *i*, and the denominator
    sums up the accuracy of all the pipelines in the population. In the survivor selection
    step, we can use the accuracy summation of all the pipelines explored so far and
    sample multiple individuals without duplicates to form the population for the
    next search loop. Although this is a popular method, it suffers from a few problems.
    Notably, there’s a risk of premature convergence because if certain pipelines
    have a much higher accuracy than others, they will tend to be selected repeatedly.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *比例选择*（或 *轮盘赌选择*）中，我们根据概率分布选择一个个体。选择个体的概率与其适应度成正比。例如，如果我们想选择一个用于分类的机器学习管道，我们可以使用准确度来衡量每个管道的适应度。管道的准确度越高，我们应该分配给它的概率就越大。一个管道
    *i* 可以以概率![07-10-EQ17](../Images/07-10-EQ17.png)被选为父代，其中 *f[i]* 表示管道 *i* 的非负准确度，分母是种群中所有管道准确度的总和。在幸存者选择步骤中，我们可以使用迄今为止探索的所有管道的准确度总和，并采样多个个体（无重复）来形成下一次搜索循环的种群。尽管这是一个流行的方法，但它存在一些问题。值得注意的是，存在过早收敛的风险，因为如果某些管道的准确度远高于其他管道，它们倾向于被反复选择。
- en: Ranking selection
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 排名选择
- en: '*Ranking selection* adopts a strategy similar to proportional selection but
    uses the fitness ranks of the pipelines to calculate the probability. For example,
    suppose we have three pipelines in the population whose accuracies are ranked
    1, 2, and 3\. We can assign them the probabilities ![07-10-EQ18](../Images/07-10-EQ18.png), ![07-10-EQ19](../Images/07-10-EQ19.png),
    and ![07-10-EQ20](../Images/07-10-EQ20.png), respectively, to select a parent
    from them. The design of the probability balances the selection intensity and
    diversity. The example here is a linear ranking selection strategy, in which the
    probability is proportional to the rank of the individual. We can also use nonlinear
    probability to enhance exploitation or exploration. For instance, by assigning
    a higher proportional probability to the pipelines with higher ranks, we favor
    exploitation more than exploration during the selection.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*排名选择* 采用与比例选择相似的战略，但使用管道的适应度排名来计算概率。例如，假设我们在种群中有三个管道，其准确度排名为1、2和3。我们可以分别赋予它们![07-10-EQ18](../Images/07-10-EQ18.png)、![07-10-EQ19](../Images/07-10-EQ19.png)和![07-10-EQ20](../Images/07-10-EQ20.png)的概率，从它们中选择一个父代。概率的设计平衡了选择强度和多样性。这里的例子是一个线性排名选择策略，其中概率与个体的排名成正比。我们也可以使用非线性概率来增强利用或探索。例如，通过给排名更高的管道分配更高的比例概率，我们在选择过程中更倾向于利用而不是探索。'
- en: Ranking selection often performs better than proportional selection because
    it avoids the scale issue in proportional selection by mapping all the individuals
    into a uniform scale. For example, if all pipelines have close accuracy scores,
    ranking selection will still be able to distinguish them based on their ranks.
    Also, if an individual pipeline is better than all the rest of the pipelines,
    no matter how fit it is relative to the others, the probability of it being selected
    as the parent or survivor will not be changed. This sacrifices some selection
    intensity compared to proportional search but provides a more robust balance of
    selection intensity and diversity in general situations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 排名选择通常比比例选择表现更好，因为它通过将所有个体映射到统一尺度来避免比例选择中的尺度问题。例如，如果所有管道的准确度得分都很接近，排名选择仍然可以根据它们的排名来区分它们。此外，如果一个管道比其他所有管道都好，无论它的适应度相对于其他管道如何，它被选为父代或幸存者的概率不会改变。与比例搜索相比，这牺牲了一些选择强度，但在一般情况下提供了更稳健的选择强度和多样性平衡。
- en: Tournament selection
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛选择
- en: '*Tournament selection* is a two-step selection process. It first randomly selects
    a certain number of candidates and then picks the best one among them as the parent
    to conduct crossover and mutation. It can be converted to a special type of ranking
    selection if we assign 0 probability to the bottom *k* individuals, where *k*
    is the number of candidate individuals to be selected for comparison in the tournament
    selection. The probability assigned to the rest of the individuals is ![07-10-EQ21](../Images/07-10-EQ21.png),
    where *r[i]* denotes the rank of pipeline *i* among the pipelines, *p* is the
    population size, and ![07-10-EQ22](../Images/07-10-EQ22.png) is the binomial coefficients
    (![07-10-EQ23](../Images/07-10-EQ23.png)). By increasing the number of candidates
    in the tournament selection, we can increase the selection intensity (exploitation)
    and reduce the selection diversity (exploration) because only the best one among
    the candidates will be selected as the parent. Considering two extra situations,
    if the candidate size (*k*) is 1, it is equivalent to selecting an individual
    from the population randomly. If the candidate size equals the population size,
    the selection intensity is maximized, so the best individual in the population
    will be selected as the parent.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*锦标赛选择* 是一个两步选择过程。它首先随机选择一定数量的候选人，然后从他们中挑选出最佳者作为父代进行交叉和变异。如果我们给排名最后的 *k* 个个体分配
    0 概率，其中 *k* 是锦标赛选择中用于比较的候选个体数量，那么它可以转换为一个特殊的排序选择类型。其余个体分配的概率为 ![07-10-EQ21](../Images/07-10-EQ21.png)，其中
    *r[i]* 表示管道 *i* 在管道中的排名，*p* 是种群大小，而 ![07-10-EQ22](../Images/07-10-EQ22.png) 是二项式系数
    (![07-10-EQ23](../Images/07-10-EQ23.png))。通过增加锦标赛选择中的候选人数，我们可以增加选择强度（利用）并减少选择多样性（探索），因为只有候选者中的最佳者将被选为父代。考虑两种额外情况，如果候选大小
    (*k*) 为 1，则相当于从种群中随机选择一个个体。如果候选大小等于种群大小，选择强度达到最大，因此种群中的最佳个体将被选为父代。'
- en: In addition to the model evaluation performance, we can specify other objectives
    during the selection based on our desire for the optimal model. For example, we
    can create a function to consider both the accuracy and the complexity measure
    (such as floating-point operations per second, or FLOPS) of the pipelines and
    use the function value to assign a probability to each pipeline. For those who
    are interested in more details and other selection strategies, please refer to
    the book *Evolutionary Optimization Algorithms* by Dan Simon (Wiley, 2013).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型评估性能外，我们还可以根据我们对最优模型的期望在选择过程中指定其他目标。例如，我们可以创建一个函数来考虑管道的准确性和复杂度度量（如每秒浮点运算次数，或
    FLOPS），并使用函数值来为每个管道分配一个概率。对于那些对更多细节和其他选择策略感兴趣的人，请参阅 Dan Simon 所著的《进化优化算法》（Wiley，2013年）。
- en: 7.4.2 The aging evolutionary search method
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 老化进化搜索方法
- en: 'In this section, we will implement an evolutionary search method called *aging
    evolutionary search*, proposed by researchers from Google Brain in “Regularized
    Evolution for Image Classifier Architecture Search” ([https://arxiv.org/abs/1802.01548](https://arxiv.org/abs/1802.01548)).
    It was originally proposed for searching for the best neural network architectures
    but can be generalized to various AutoML tasks. The method uses tournament selection
    for selecting the parent pipeline from which to breed the pipeline and uses a
    heuristic aging selection strategy for survivor selection. The “age” of a pipeline
    (or a trial) means the number of iterations during a search process. When the
    trial is born (started), we define it as 0\. The age becomes *N* when *N* more
    trials are selected and executed afterward. Coupling this with the four core steps
    of the population-based evolutionary search method, we elaborate the aging evolutionary
    search method as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一种名为 *老化进化搜索* 的进化搜索方法，该方法由 Google Brain 的研究人员在“Regularized Evolution
    for Image Classifier Architecture Search”（[https://arxiv.org/abs/1802.01548](https://arxiv.org/abs/1802.01548)）中提出。它最初是为了搜索最佳神经网络架构而提出的，但可以推广到各种
    AutoML 任务。该方法使用锦标赛选择来选择父代管道进行繁殖，并使用启发式老化选择策略进行幸存者选择。管道的“年龄”（或试验）意味着搜索过程中的迭代次数。当试验出生（开始）时，我们将其定义为
    0。当有 *N* 个更多试验被选择并执行后，年龄变为 *N*。结合基于种群的进化搜索方法的四个核心步骤，我们详细阐述老化进化搜索方法如下：
- en: '*Initial population generation*—Randomly sample a set of ML pipelines and evaluate
    them to form the initial population.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*初始种群生成*—随机采样一组机器学习管道并评估它们以形成初始种群。'
- en: '*Parent selection*—In each search iteration, a parent is selected from the
    population based on the tournament selection method.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*父代选择*——在每次搜索迭代中，根据锦标赛选择方法从种群中选择一个父代。'
- en: '*Mutation*—Randomly select a hyperparameter of the parent, and randomly change
    its value to a different one. If the generated offspring has been explored before,
    we will treat it as a collision and will retry the mutation step until a valid
    offspring is selected or the maximum number of collisions has been reached. We
    use a hashing string to represent the hyperparameters in a trial to check whether
    an offspring has been explored already.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*变异*——随机选择父代的一个超参数，并将其值随机改变为另一个不同的值。如果生成的后代之前已经被探索过，我们将将其视为碰撞，并将重试变异步骤，直到选择一个有效的后代或达到最大碰撞次数。我们使用哈希字符串来表示试验中的超参数，以检查后代是否已经被探索过。'
- en: '*Survivor selection*—After a new offspring is generated, we keep the latest
    sampled trials as the new population. For example, suppose our population size
    is 100\. When trial 101 is finished, the first (oldest) trial will be removed
    from the population, and the new (youngest) trial will be added to it. This is
    why the method is called the *aging* evolutionary method. Selecting the latest
    trials as survivors should enhance the exploitation power, because we assume the
    older ones will have performed worse than the latest ones.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*幸存者选择*——在生成新的后代之后，我们保留最新的采样试验作为新种群。例如，假设我们的种群大小是100。当试验101完成时，种群中的第一个（最旧的）试验将被移除，新的（最年轻的）试验将被添加进去。这就是为什么这种方法被称为*老化*进化方法。选择最新的试验作为幸存者应该增强利用能力，因为我们假设旧的试验将比最新的试验表现得更差。'
- en: The process is visualized in figure 7.11\. We can see that the crossover operation
    is not used in this method; only the mutation operation is used to generate the
    new offspring.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在图7.11中进行了可视化。我们可以看到，这种方法没有使用交叉操作；仅使用变异操作来生成新的后代。
- en: '![07-11](../Images/07-11.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![07-11](../Images/07-11.png)'
- en: Figure 7.11 The aging evolutionary search life cycle
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 老化进化搜索生命周期
- en: 'We need to predefine two main hyper-hyperparameters to control the algorithm:
    the population size and the candidate size for the tournament selection strategy.
    They help balance exploration and exploitation. If we use a large population size,
    more old trials would be kept as a survivor and might be selected as the parent
    to breed the offspring. This will increase the exploration power because older
    trials are often worse than the younger ones, and the diversity in the population
    would be increased. If we select the larger candidate size, the selection intensity
    would be increased, as we’ve mentioned earlier, which increases the exploitation
    power of the method.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要预先定义两个主要的超超参数来控制算法：锦标赛选择策略中的种群大小和候选大小。它们有助于平衡探索和利用。如果我们使用较大的种群大小，更多的旧试验将作为幸存者被保留，并可能被选为父代以繁殖后代。这将增加探索能力，因为旧试验通常比新试验差，种群中的多样性也会增加。如果我们选择较大的候选大小，选择强度将增加，正如我们之前提到的，这增加了该方法利用能力。
- en: Listing 7.15 shows how the aging evolutionary oracle is implemented. We create
    a list to save the IDs of the population trials. The number of random initialization
    trials should be larger than the population size so that the population list can
    be filled. Look at the core function, populate_space(). In the beginning, trials
    are randomly sampled to form the population. After the population has been created,
    in each search loop we conduct survivor selection based on the ending order of
    the trials to maintain a fixed population size. Then we perform tournament selection
    by randomly selecting a set of candidates and picking the best one among them
    as the parent (best_candidate_trial). We mutate a randomly selected hyperparameter
    of the parent trial using the _mutate function, and the hyperparameter values
    of the offspring are returned and put into a dictionary along with the status
    of the offspring trial. The status is set as RUNNING, meaning the trial is ready
    for evaluation.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.15展示了老化进化算子的实现方式。我们创建一个列表来保存种群试验的ID。随机初始化试验的数量应该大于种群大小，以便种群列表能够被填满。看看核心函数，populate_space()。一开始，试验是随机抽取来形成种群的。种群创建后，在每次搜索循环中，我们根据试验的结束顺序进行生存选择，以维持固定的种群大小。然后我们通过随机选择一组候选者并进行锦标赛选择，从中挑选出最佳者作为父代（best_candidate_trial）。我们使用_mutate函数变异父代试验的随机选择的超参数，并将后代的超参数值以及后代试验的状态返回并放入一个字典中。状态设置为RUNNING，意味着试验准备就绪，可以进行评估。
- en: Listing 7.15 Evolutionary search oracle
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.15 进化搜索算子
- en: '[PRE14]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Makes sure the random initialization trials can fill the population
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确保随机初始化试验能够填满种群
- en: ❷ A list to keep the IDs of the population trials
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一个列表用于保存种群试验的ID
- en: ❸ Random selection for initializing the population
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 随机选择用于初始化种群的个体
- en: ❹ Survivor selection based on the age of trials
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 基于试验年龄的生存选择
- en: ❺ Selects candidate trials from the population
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从种群中选择候选试验
- en: ❻ Gets the best candidate for parent based on the performance
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 根据性能获取最佳父代候选者
- en: ❼ Mutates a random selected hyperparameter of the parent
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 变异父代随机选择的超参数
- en: ❽ Stops the trial if the offspring is invalid (has already been evaluated)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 如果后代无效（已经评估过）则停止试验
- en: Now let’s see how to implement the mutation operation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何实现变异操作。
- en: 7.4.3 Implementing a simple mutation operation
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 实现简单的变异操作
- en: Ideally, as long as a hyperparameter is not fixed, we can mutate it into other
    values. However, if the selected hyperparameter is a conditional hyperparameter,
    changing it may affect other hyperparameters. For example, if we selected the
    model type hyperparameter to mutate and its value is changed from MLP to decision
    tree, the tree depth hyperparameter, which was not active originally, will become
    active, and we will need to assign it a specific value (see figure 7.12). Thus,
    we need to check whether the mutation hyperparameter is a conditional hyperparameter.
    If it is, we need to assign its *descendant hyperparameters* (hyperparameters
    represented by the child nodes in the figure) randomly selected values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，只要超参数没有被固定，我们就可以将其变异为其他值。然而，如果选定的超参数是一个条件超参数，改变它可能会影响其他超参数。例如，如果我们选择了模型类型超参数进行变异，并且其值从MLP变为决策树，原本不活跃的树深度超参数将变为活跃，我们需要为其分配一个特定的值（见图7.12）。因此，我们需要检查变异超参数是否为条件超参数。如果是，我们需要为其*后代超参数*（图中由子节点表示的超参数）随机分配值。
- en: So, we first collect the nonfixed and active hyperparameters in the parent trial
    (best_ trial) and randomly select a hyperparameter to mutate from among them.
    Then we create a hyperparameter container instance named hps to save the hyperparameter
    values of the new offspring into. By looping through all the hyperparameters in
    the search space, we generate their values one by one and feed them into the container.
    Note that we need to loop through all the active hyperparameters in the search
    space rather than just processing the active hyperparameters in the parent trial,
    because some of the nonactive hyperparameters could become active if a conditional
    hyperparameter is mutated (see figure 7.12).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先收集父试验（best_trial）中的非固定和活动超参数，并从中随机选择一个超参数进行变异。然后我们创建一个名为 hps 的超参数容器实例，用于保存新后代的超参数值。通过遍历搜索空间中的所有超参数，我们逐一生成它们的值并将它们输入到容器中。请注意，我们需要遍历搜索空间中的所有活动超参数，而不仅仅是处理父试验中的活动超参数，因为某些非活动超参数在条件超参数变异的情况下可能会变为活动状态（参见图
    7.12）。
- en: '![07-12](../Images/07-12.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![07-12](../Images/07-12.png)'
- en: Figure 7.12 A nonactive hyperparameter may become active due to the mutation
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 由于变异，一个非活动超参数可能变为活动状态
- en: For any hyperparameter that is active in the parent trial but is not the selected
    mutation hyperparameter, we assign its original value to it and continue the mutation
    operation. For the selected mutation hyperparameter, we randomly select a new
    value. Suppose the selected mutation hyperparameter is a conditional hyperparameter.
    After its value is mutated, its descendant hyperparameters, which are changed
    to be active in the offspring, will also be assigned randomly selected values.
    Whether a descendant hyperparameter is active is determined by the statement hps.is_active(hp),
    shown in listing 7.16\. After the new offspring is generated, we check whether
    it has been evaluated before with the help of a hashing function (_compute_values_hash)
    inherited from the base Oracle class. If the offspring collides with a previous
    trial, we repeat the mutation process. We continue doing this until a valid offspring
    is generated or the maximum number of collisions is reached. The hashing values
    of the final offspring are collected in a Python set (self._tried_so_far) for
    checking the future trials.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在父试验中活动但不是所选变异超参数的任何超参数，我们将它的原始值分配给它并继续变异操作。对于所选变异超参数，我们随机选择一个新的值。假设所选变异超参数是一个条件超参数。在其值变异后，其子代超参数，在后代中变为活动状态，也将被随机选择值分配。子代超参数是否活动由
    hps.is_active(hp) 语句确定，如列表 7.16 所示。新后代生成后，我们通过从基 Oracle 类继承的哈希函数 (_compute_values_hash)
    检查它是否已经被评估过。如果后代与之前的试验冲突，我们重复变异过程。我们继续这样做，直到生成一个有效的后代或达到最大冲突次数。最终后代的哈希值被收集到一个
    Python 集合（self._tried_so_far）中，用于检查未来的试验。
- en: Listing 7.16 Mutating the hyperparameters in the parent trial
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.16 在父试验中变异超参数
- en: '[PRE15]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Extracts the hyperparameters in the best trial
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从最佳试验中提取超参数
- en: ❷ Collects the nonfixed and active hyperparameters
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 收集非固定和活动超参数
- en: ❸ Randomly selects a hyperparameter to mutate
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 随机选择一个超参数进行变异
- en: ❹ Loops through all the active hyperparameters in the search space
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历搜索空间中的所有活动超参数
- en: ❺ Checks whether the current hyperparameter needs to be mutated
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 检查当前超参数是否需要变异
- en: ❻ Conducts a random mutation
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 执行随机变异
- en: ❼ Generates the hashing string for the new offspring
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 为新后代生成哈希字符串
- en: ❽ Checks whether the offspring has been evaluated already
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 检查后代是否已经被评估
- en: 'The following two points are worth noting:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两点值得注意：
- en: 'The determination of whether a hyperparameter is a descendant of a conditional
    hyperparameter leverages a property of KerasTuner: the descendant hyperparameters
    will always appear after the conditional hyperparameter in the hyperparameter
    list (self.hyperparameters.space). In fact, the hyperparameter search space can
    be treated as a graph, where each node represents a hyperparameter, and the links
    indicate their order of appearance in an ML pipeline or their conditional correlations.
    KerasTuner uses the topological order of the graph to save the hyperparameter
    search space in a list.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判断一个超参数是否是条件超参数的后代，利用了 KerasTuner 的一个特性：后代超参数将始终出现在条件超参数之后（self.hyperparameters.space）。实际上，超参数搜索空间可以被视为一个图，其中每个节点代表一个超参数，而链接表示它们在机器学习管道中的出现顺序或它们的条件相关性。KerasTuner
    使用图的拓扑顺序将超参数搜索空间保存在一个列表中。
- en: To make sure the algorithm is aware of the conditional correlations between
    hyperparameters and detects whether a conditional hyperparameter’s descendants
    are active when it is mutated, we need to explicitly define the conditional scope
    in the search space, as described in chapter 5\. An illustrative definition of
    a synthetic search space is shown in the following listing, where conditional_
    choice is a conditional hyperparameter, whose descendant hyperparameters are child1_choice
    and child2_choice. During the search process, one of its two children will be
    active, depending on its value.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了确保算法了解超参数之间的条件相关性，并在突变时检测条件超参数的后代是否活跃，我们需要在搜索空间中显式定义条件作用域，如第 5 章所述。以下列表展示了合成搜索空间的说明性定义，其中
    conditional_choice 是一个条件超参数，其后代超参数是 child1_choice 和 child2_choice。在搜索过程中，其两个子代中的一个将根据其值而活跃。
- en: Listing 7.17 A conditional search space
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.17 一个条件搜索空间
- en: '[PRE16]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The conditional scope of the hyperparameters
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 超参数的条件作用域
- en: Combining the mutation function with the sampling function of the oracle learned
    previously, we finish the core implementation of the oracle. We can also add the
    functions to help save and resume the oracle. The hyper-hyperparameters used to
    control the oracle are saved in the state dictionary, and the population list
    should be reinitialized along with these hyper-hyperparameters when resuming the
    oracle in the set_state() function, as shown in the next listing.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 将先前学习的突变函数与采样函数相结合，我们完成了 Oracle 的核心实现。我们还可以添加帮助保存和恢复 Oracle 的函数。用于控制 Oracle
    的超超参数保存在状态字典中，当在 set_state() 函数中恢复 Oracle 时，应将这些超超参数与种群列表一起重新初始化，如以下列表所示。
- en: Listing 7.18 Helping to resume an evolutionary search oracle
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.18 帮助恢复进化搜索 Oracle
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Saves the population size and candidate size
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 保存种群大小和候选大小
- en: ❷ Reinitializes the population list during the oracle’s resuming
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在 Oracle 恢复期间重新初始化种群列表
- en: Finally, let’s evaluate the aging evolutionary search method on the same regression
    task used in the previous sections (California housing price prediction) and compare
    it with the random search and Bayesian optimization search methods.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在之前章节中使用的相同回归任务（加利福尼亚房价预测）上评估老化进化搜索方法，并将其与随机搜索和贝叶斯优化搜索方法进行比较。
- en: 7.4.4 Evaluating the aging evolutionary search method
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 评估老化进化搜索方法
- en: To evaluate the aging evolutionary search method, we search for 100 trials and
    set the population size and candidate size to be 20 and 5, respectively. These
    two hyperparameters are often set subjectively based on your sense of the search
    space and the empirical tuning results. Usually, if the search space is large,
    we use a large population size to cumulate enough diversified trials to breed
    the offspring. A population size of around 100 should be large enough to handle
    most situations. Here we’ve used a conservative choice (20) because the search
    space contains only three hyperparameters. Using half of or a quarter of the population
    size as the candidate size, as we’ve done here, often provides good performance
    empirically.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估老化进化搜索方法，我们进行了 100 次试验，并将种群大小和候选大小分别设置为 20 和 5。这两个超参数通常基于你对搜索空间的感觉和经验调优结果主观设置。通常，如果搜索空间很大，我们使用较大的种群大小来累积足够的多样化试验以培育后代。大约
    100 的种群大小应该足够处理大多数情况。在这里，我们使用了保守的选择（20），因为搜索空间中只包含三个超参数。使用种群大小的一半或四分之一作为候选大小，如我们在这里所做的那样，通常在经验上提供良好的性能。
- en: We list only the code for calling different search methods in the next code
    listing. The rest of the implementation for data loading, search space creation,
    and tuner customization is the same as we’ve used before and is available in a
    Jupyter notebook at [https://github.com/datamllab/automl-in-action-notebooks](https://github.com/datamllab/automl-in-action-notebooks).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码列表中，我们只列出调用不同搜索方法的代码。数据加载、搜索空间创建和调谐器定制的其余实现与之前使用的方法相同，可在 Jupyter 笔记本中找到，网址为
    [https://github.com/datamllab/automl-in-action-notebooks](https://github.com/datamllab/automl-in-action-notebooks)。
- en: Listing 7.19 Calling different search methods
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.19 调用不同的搜索方法
- en: '[PRE18]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Uses the aging evolutionary method for searching
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用老化进化方法进行搜索
- en: ❷ Uses the built-in random search method of KerasTuner
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 KerasTuner 内置的随机搜索方法
- en: ❸ Uses the built-in Bayesian optimization search method of KerasTuner
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 KerasTuner 内置的贝叶斯优化搜索方法
- en: Figure 7.13 shows the evaluation performance of the best model found by each
    of the three methods as the search process progressed. We can see that the Bayesian
    optimization method performs the best among the three methods. Although the evolutionary
    method achieves more steps of improvement during the search process, it distinguishes
    less compared to random search. Each step of improvement is small because we let
    the mutation happen on only one hyperparameter at each stage, and improvements
    continue until late in the search process. This means the selection intensity
    (exploitation power) could be improved in the early stages.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 展示了随着搜索过程的进行，三种方法各自找到的最佳模型的评估性能。我们可以看到，在三种方法中，贝叶斯优化方法表现最好。尽管进化方法在搜索过程中实现了更多的改进步骤，但与随机搜索相比，它的区分度较小。每一步的改进都很小，因为我们让突变在每个阶段只发生在单个超参数上，并且改进一直持续到搜索过程的后期。这意味着选择强度（利用能力）可以在早期阶段得到改善。
- en: '![07-13](../Images/07-13.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![07-13](../Images/07-13.png)'
- en: Figure 7.13 Comparing the search results of three search methods
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 比较三种搜索方法的搜索结果
- en: Now let’s increase the candidate size to 20 (which is an extreme choice because
    our population size is also 20) to enhance the exploitation power and see what
    happens. In figure 7.14, we can see that more improvement happens in the early
    stages, which confirms that the selection intensity (exploitation ability) can
    be enhanced at the beginning by increasing the candidate size. Although in this
    example, the final result is not improved much (and may even become worse than
    the result obtained using a smaller candidate size if we try further trials, due
    to the lack of exploration ability), this suggests that a larger candidate size
    could help us achieve comparable results with fewer trials if we consider only
    100 trials. In practice, you can adjust these sizes based on your tolerance and
    available time. If you don’t mind taking more time to explore the search space
    more thoroughly, you can select a smaller candidate size (and larger population
    size). If you expect to achieve a moderately good model within fewer trials, you
    could use a larger candidate size (and smaller population size).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将候选者数量增加到 20（这是一个极端的选择，因为我们的种群大小也是 20）以增强利用能力，看看会发生什么。在图 7.14 中，我们可以看到在早期阶段有更多的改进，这证实了通过增加候选者数量可以在开始时增强选择强度（利用能力）。尽管在这个例子中，最终结果改进不大（如果我们尝试进一步的试验，结果甚至可能比使用较小候选者数量获得的结果更差，因为缺乏探索能力），但这表明如果我们只考虑
    100 次试验，较大的候选者数量可以帮助我们以更少的试验次数达到可比的结果。在实际应用中，你可以根据自己的容忍度和可用时间调整这些大小。如果你不介意花更多的时间更彻底地探索搜索空间，你可以选择较小的候选者数量（以及较大的种群大小）。如果你期望在较少的试验中实现一个适度的好模型，你可以使用较大的候选者数量（以及较小的种群大小）。
- en: '![07-14](../Images/07-14.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![07-14](../Images/07-14.png)'
- en: Figure 7.14 Comparison of different search methods
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 不同搜索方法的比较
- en: In this example, Bayesian optimization performs the best of all the search algorithms.
    This is because the hyperparameters we’re trying to tune are either continuous
    hyperparameters or have ordinal values. Generally, if our search space is dominated
    by hyperparameters with continuous or ordinal values, we prefer to use the Bayesian
    optimization algorithm. If the hyperparameters are mostly categorical or there
    are many conditional hyperparameters, the evolutionary method would be a good
    choice. Random mutation is not a good choice for exploring continuous hyperparameters.
    Some feasible solutions to improve this include using a logarithmic scale for
    some hyperparameters (such as learning rate) or combining the evolutionary method
    with the model-based method by adding a surrogate model to help guide the mutation—that
    is, conduct mutations randomly multiple times and select the best trial as the
    offspring based on the surrogate model. The random search method can provide a
    strong baseline when the number of trials you want to explore is too few compared
    to the size of the search space. This is a common case in tasks that involve designing
    and tuning deep neural networks (neural architecture search).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，贝叶斯优化在所有搜索算法中表现最佳。这是因为我们试图调整的超参数要么是连续超参数，要么具有序数值。通常，如果我们的搜索空间主要由具有连续或序数值的超参数主导，我们更倾向于使用贝叶斯优化算法。如果超参数大多是分类的或者存在许多条件超参数，进化方法将是一个不错的选择。对于探索连续超参数，随机变异不是一个好的选择。一些可行的改进方案包括对某些超参数（如学习率）使用对数尺度，或者通过添加代理模型来结合进化方法和基于模型的方法，以帮助引导变异——也就是说，多次随机变异并基于代理模型选择最佳试验作为后代。当想要探索的试验数量与搜索空间大小相比太少时，随机搜索方法可以提供一个强大的基线。这在涉及设计和调整深度神经网络（神经架构搜索）的任务中是一个常见情况。
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'A sequential search method iteratively samples and evaluates hyperparameters
    from the search space. It generally consists of two steps: hyperparameter sampling
    and an optional update step to incorporate historical evaluations.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序搜索方法迭代地从搜索空间中采样和评估超参数。它通常包括两个步骤：超参数采样和一个可选的更新步骤，以纳入历史评估。
- en: History-dependent search methods can leverage the evaluated hyperparameters
    to better sample from the search space. Heuristic and model-based methods are
    the two main categories of history-dependent methods.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史依赖性搜索方法可以利用评估过的超参数来更好地从搜索空间中进行采样。启发式方法和基于模型的方法是历史依赖性方法的两大类。
- en: The Bayesian optimization method is the most widely used model-based method
    in AutoML. It uses a surrogate model to approximate the model evaluation performance
    and an acquisition function to balance exploration and exploitation when sampling
    new hyperparameters from the search space. The most commonly used surrogate model
    is the Gaussian process model, and some popular acquisition functions include
    the upper confidence bound (UCB), probability of improvement, and expected improvement.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化方法是AutoML中最广泛使用的基于模型的方法。它使用代理模型来近似模型评估性能，并使用获取函数在从搜索空间中采样新超参数时平衡探索和利用。最常用的代理模型是高斯过程模型，一些流行的获取函数包括上置信界（UCB）、改进概率和期望改进。
- en: 'The evolutionary method is a heuristic search method that generates new samples
    by simulating the evolution of an animal population in new generations. It involves
    four steps: initial population generation, parent selection, crossover and mutation,
    and survivor selection. The aging evolutionary search method is a popular evolutionary
    search method originally proposed for neural architecture search. It utilizes
    the existing ages of trials for survivor selection and tournament selection for
    parent selection.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化方法是一种启发式搜索方法，通过模拟动物种群在新一代中的进化来生成新的样本。它包括四个步骤：初始种群生成、父代选择、交叉和变异以及幸存者选择。老化进化搜索方法是一种流行的进化搜索方法，最初是为神经架构搜索提出的。它利用现有试验的年龄进行幸存者选择，并使用锦标赛选择进行父代选择。
