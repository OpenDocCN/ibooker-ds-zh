- en: 16 Securing applications with policies, contexts, and admission control
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16 使用策略、上下文和准入控制确保应用程序安全
- en: Containers are a lightweight wrapper around application processes. They start
    quickly and add little overhead to your app because they use the operating system
    kernel of the machine on which they’re running. That makes them super efficient,
    but at the cost of strong isolation—containers can be compromised, and a compromised
    container could provide unrestricted access to the server and to all the other
    containers running on it. Kubernetes has many features to secure your applications,
    but none of them are enabled by default. In this chapter, you’ll learn how to
    use the security controls in Kubernetes and how to set up your cluster so those
    controls are required for all your workloads.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 容器是围绕应用程序进程的轻量级包装。它们启动速度快，并且由于它们使用运行在其上的机器的操作系统内核，因此对应用程序的额外开销很小。这使得它们超级高效，但代价是强大的隔离性——容器可能会被破坏，一个被破坏的容器可能会为服务器以及运行在其上的所有其他容器提供不受限制的访问。Kubernetes有许多功能来确保你的应用程序安全，但默认情况下，它们都没有启用。在本章中，你将学习如何使用Kubernetes中的安全控制，以及如何设置你的集群，以便这些控制对所有工作负载都是必需的。
- en: Securing applications in Kubernetes is about limiting what containers can do,
    so if an attacker exploits an app vulnerability to run commands in the container,
    they can’t get beyond that container. We can do this by restricting network access
    to other containers and the Kubernetes API, restricting mounts of the host’s filesystem,
    and limiting the operating system features the container can use. We’ll cover
    the essential approaches, but the security space is large and evolving. This chapter
    is even longer than the others—you’re about to learn a lot, but it will be only
    the start of your journey to a secure Kubernetes environment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中确保应用程序安全是关于限制容器可以做什么，所以如果攻击者利用应用程序漏洞在容器中运行命令，他们无法超出该容器。我们可以通过限制对其他容器和Kubernetes
    API的网络访问，限制主机文件系统的挂载，以及限制容器可以使用操作系统功能来做到这一点。我们将介绍基本方法，但安全领域很大且在不断发展。本章甚至比其他章节更长——你即将学到很多东西，但这只是你通往安全Kubernetes环境的旅程的开始。
- en: 16.1 Securing communication with network policies
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 使用网络策略确保通信安全
- en: Restricting network access is one of the simplest ways to secure your applications.
    Kubernetes has a flat networking model, where every Pod can reach every other
    Pod by its IP address, and Services are accessible throughout the cluster. There’s
    no reason why the Pi web application should access the to-do list database, or
    why the Hello, World web app should use the Kubernetes API, but by default, they
    can. You learned in chapter 15 how you can use Ingress resources to control access
    to HTTP routes, but that applies only to external traffic coming into the cluster.
    You also need to control access within the cluster, and for that, Kubernetes offers
    *network policies*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 限制网络访问是确保应用程序安全的最简单方法之一。Kubernetes具有扁平的联网模型，其中每个Pod都可以通过其IP地址到达其他Pod，并且服务在整个集群中都是可访问的。没有理由让Pi网络应用程序访问待办事项数据库，或者让Hello,
    World网络应用程序使用Kubernetes API，但默认情况下，它们可以。你在第15章中学习了如何使用Ingress资源来控制对HTTP路由的访问，但这仅适用于进入集群的外部流量。你还需要控制集群内的访问，为此，Kubernetes提供了*网络策略*。
- en: Network policies work like firewall rules, blocking traffic to or from Pods
    at the port level. The rules are flexible and use label selectors to identify
    objects. You can deploy a blanket deny-all policy to stop outgoing traffic from
    all Pods, or you can deploy a policy that restricts incoming traffic to a Pod’s
    metrics port so it can be accessed only from Pods in the monitoring namespace.
    Figure 16.1 shows how that looks in the cluster.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略就像防火墙规则一样工作，在端口级别阻止到或从Pod的流量。规则是灵活的，并使用标签选择器来识别对象。你可以部署一个全面拒绝所有流量的策略来阻止所有Pod的出站流量，或者你可以部署一个策略，限制对Pod的指标端口的入站流量，以便只能从监控命名空间中的Pod访问。图16.1显示了这在集群中的样子。
- en: '![](../Images/16-1.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图16-1](../Images/16-1.jpg)'
- en: Figure 16.1 Network policy rules are granular—you can apply clusterwide defaults
    with Pod overrides.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 网络策略规则是细粒度的——你可以应用集群默认值，并通过Pod覆盖来应用。
- en: NetworkPolicy objects are separate resources, which means they could be modeled
    outside of the application by a security team, or they could be built by the product
    team. Or, of course, each team might think the other team has it covered, and
    apps make it to production without any policies, which is a problem. We’ll deploy
    an app that has slipped through with no policies and look at the problems it has.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkPolicy对象是独立的资源，这意味着它们可以被安全团队在应用程序之外建模，或者它们可以被产品团队构建。或者，当然，每个团队可能都认为其他团队已经覆盖了，应用在没有策略的情况下进入生产，这是一个问题。我们将部署一个没有策略就通过的app，并查看它存在的问题。
- en: Try it now Deploy the Astronomy Picture of the Day (APOD) app, and confirm that
    the app components can be accessed by any Pod.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试 Deploy 天文图片每日一图（APOD）应用，并确认应用组件可以被任何Pod访问。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can clearly see the issue in this exercise—the whole of the cluster is wide
    open, so from the sleep Pod, you can access the APOD API and the metrics from
    the access log component. Figure 16.2 shows my output. Let’s be clear that there’s
    nothing special about the sleep Pod; it’s just a simple way to demonstrate the
    problem. Any container in the cluster can do the same.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地在这个练习中看到问题——整个集群都是开放的，所以从sleep Pod，你可以访问APOD API和访问日志组件的指标。图16.2显示了我的输出。让我们明确一点，sleep
    Pod没有什么特别之处；它只是演示问题的简单方式。集群中的任何容器都可以做同样的事情。
- en: '![](../Images/16-2.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16-2.jpg)'
- en: Figure 16.2 The downside of Kubernetes’s flat networking model is that every
    Pod is accessible.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2 Kubernetes平面网络模型的缺点是每个Pod都是可访问的。
- en: Pods should be isolated so they receive traffic only from the components that
    need to access them, and they send traffic only to components they need to access.
    Network policies model that with *ingress rules* (don’t confuse them with Ingress
    resources), which restrict incoming traffic, and *egress rules*, which restrict
    outgoing traffic. In the APOD app, the only component that should have access
    to the API is the web app. Listing 16.1 shows this as an ingress rule in a NetworkPolicy
    object.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Pods应该被隔离，以便它们只接收需要访问它们的组件的流量，并且只向它们需要访问的组件发送流量。网络策略通过*入口规则*（不要与入口资源混淆）来模拟这一点，这些规则限制进入的流量，以及*出口规则*，这些规则限制出去的流量。在APOD应用中，唯一应该能够访问API的组件是Web应用。列表16.1显示了在NetworkPolicy对象中的入口规则。
- en: Listing 16.1 networkpolicy-api.yaml, restricting access to Pods by their labels
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.1 networkpolicy-api.yaml，通过标签限制对Pod的访问
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The NetworkPolicy spec is fairly straightforward, and rules can be deployed
    in advance of the application so it’s secure as soon as the Pods start. Ingress
    and egress rules follow the same pattern, and both can use namespace selectors
    as well as Pod selectors. You can create global rules and then override them with
    more fine-grained rules at the application level.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkPolicy规范相当简单，规则可以在应用程序部署之前部署，这样Pod启动时就是安全的。入口和出口规则遵循相同的模式，并且两者都可以使用命名空间选择器和Pod选择器。你可以创建全局规则，然后在应用程序级别用更细粒度的规则覆盖它们。
- en: One big problem with network policy—when you deploy the rules, they probably
    won’t do anything. Just like Ingress objects need an ingress controller to act
    on them, NetworkPolicy objects rely on the network implementation in your cluster
    to enforce them. When you deploy this policy in the next exercise, you’ll probably
    be disappointed to find the APOD API is still not restricted to the web app.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略的一个大问题——当你部署规则时，它们可能不会做任何事情。就像Ingress对象需要一个入口控制器来对其执行操作一样，NetworkPolicy对象依赖于集群中的网络实现来强制执行它们。当你在这个下一个练习中部署这个策略时，你可能会失望地发现APOD
    API仍然没有限制为Web应用。
- en: Try it now Apply the network policy, and see if your cluster actually enforces
    it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试 Apply 网络策略，看看你的集群是否真的强制执行了它。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see in figure 16.3 that the sleep Pod can access the API—the NetworkPolicy
    that limits ingress to the web Pods is completely ignored. I’m running this on
    Docker Desktop, but you’ll get the same results with a default setup in K3s, AKS,
    or EKS.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图16.3中看到sleep Pod可以访问API——限制进入Web Pods的NetworkPolicy被完全忽略。我正在Docker Desktop上运行这个，但你在K3s、AKS或EKS的默认设置中也会得到相同的结果。
- en: '![](../Images/16-3.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16-3.jpg)'
- en: Figure 16.3 The network setup in your Kubernetes cluster may not enforce network
    policies.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3 在你的Kubernetes集群中，网络设置可能不会强制执行网络策略。
- en: The networking layer in Kubernetes is pluggable, and not every network plugin
    supports NetworkPolicy enforcement. The simple networks in standard cluster deployments
    don’t have support, so you get into this tricky situation where you can deploy
    all your NetworkPolicy objects, but you don’t know whether they’re being enforced
    unless you test them. Cloud platforms have different levels of support here. You
    can specify a network policy option when you create an AKS cluster; with EKS,
    you need to manually install a different network plugin after you create the cluster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的网络层是可插拔的，并不是每个网络插件都支持 NetworkPolicy 的强制执行。标准集群部署中的简单网络没有支持，所以你可能会遇到这样一个棘手的情况：你可以部署所有的
    NetworkPolicy 对象，但你不知道它们是否被强制执行，除非你测试它们。云平台在这里有不同的支持级别。当你创建 AKS 集群时，你可以指定网络策略选项；对于
    EKS，你需要在创建集群后手动安装不同的网络插件。
- en: This is all very frustrating for you following along with these exercises (and
    me writing them), but it causes a much more dangerous disconnect for organizations
    using Kubernetes in production. You should look to adopt security controls early
    in the build cycle, so NetworkPolicy rules are applied in your development and
    test environments to run the app with a close-to-production configuration. A misconfigured
    network policy can easily break your app, but you won’t know that if your nonproduction
    environments don’t enforce policy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跟随这些练习（以及我编写它们）的你来说，这都很令人沮丧，但对于在生产中使用 Kubernetes 的组织来说，这会导致一个更加危险的脱节。你应该在构建周期的早期就采用安全控制措施，以便在开发和测试环境中应用
    NetworkPolicy 规则，以运行接近生产配置的应用。配置不当的网络策略可能会轻易破坏你的应用，但如果你在非生产环境中不强制执行策略，你可能不会知道这一点。
- en: 'If you want to see NetworkPolicy in action, the next exercise creates a custom
    cluster using Kind with Calico, an open source network plugin that enforces policy.
    You’ll need Docker and the Kind command line installed for this. Be warned: **This
    exercise alters the Linux configuration for Docker and will make your original
    cluster unusable**. Docker Desktop users can fix everything with the *Reset Kubernetes*
    button, and Kind users can replace their old cluster with a new one, but other
    setups might not be so lucky. It’s fine to skip these exercises and just read
    through my output; we’ll switch back to your normal cluster in the next section.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想看到 NetworkPolicy 的实际应用，接下来的练习将创建一个使用 Calico 的自定义集群，Calico 是一个开源的网络插件，它强制执行策略。你需要安装
    Docker 和 Kind 命令行才能完成这个练习。警告：**这个练习会改变 Docker 的 Linux 配置，并使你的原始集群无法使用**。Docker
    Desktop 用户可以通过点击 *重置 Kubernetes* 按钮来修复一切，而 Kind 用户可以将他们的旧集群替换为新的，但其他配置可能不那么幸运。跳过这些练习，只阅读我的输出是可以的；我们将在下一节切换回你的正常集群。
- en: Try it now Create a new cluster with Kind, and deploy a custom network plugin.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 创建一个新的 Kind 集群，并部署一个自定义网络插件。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: My output in figure 16.4 is abbreviated; you’ll see many more objects being
    created in the Calico deployment. At the end, I have a new cluster that enforces
    network policy. Unfortunately, the only way to know if your cluster uses a network
    plugin that does enforce policy is to set up your cluster with a network plugin
    that you know enforces policy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4 中的输出被简化了；你将在 Calico 部署中看到许多更多被创建的对象。最后，我有一个新的集群，它强制执行网络策略。不幸的是，了解你的集群是否使用了一个强制执行策略的网络插件，唯一的办法是设置一个你知道强制执行策略的网络插件。
- en: '![](../Images/16-4.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16-4.jpg)'
- en: Figure 16.4 Installing Calico gives you a cluster with network policy support-at
    the cost of your other cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4 安装 Calico 为你提供了一个具有网络策略支持的集群——但代价是牺牲了你的其他集群。
- en: Now we can try again. This cluster is completely new, with nothing running,
    but, of course, Kubernetes manifests are portable, so we can quickly deploy the
    APOD app again and try it out. (Kind supports multiple clusters running different
    Kubernetes versions with different configurations, so it’s a great option for
    test environments, but it’s not as developer-friendly as Docker Desktop or K3s).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以再次尝试。这个集群是完全新的，没有任何服务在运行，但当然，Kubernetes 清单是可移植的，所以我们可以快速重新部署 APOD 应用并尝试它。（Kind
    支持运行不同 Kubernetes 版本和不同配置的多个集群，因此它是测试环境的一个很好的选择，但它不如 Docker Desktop 或 K3s 对开发者友好）。
- en: Try it now Repeat the APOD and sleep deployments, and confirm that the network
    policy blocks unauthorized traffic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 重复 APOD 和 sleep 部署，并确认网络策略阻止了未经授权的流量。
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Figure 16.5 shows what we expected the first time around: only the APOD web
    app can access the API, and the sleep app times out when it tries to connect because
    the network plugin blocks the traffic.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5显示了我们的第一次预期：只有APOD网络应用可以访问API，当睡眠应用尝试连接时超时，因为网络插件阻止了流量。
- en: '![](../Images/16-5.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16-5.jpg)'
- en: Figure 16.5 Calico enforces policy so traffic to the API Pod is allowed only
    from the web Pod.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 Calico强制执行策略，因此只有来自Web Pod的流量被允许访问API Pod。
- en: Network policies are an important security control in Kubernetes, and they’re
    attractive to infrastructure teams who are used to firewalls and segregated networks.
    But you need to understand where policies fit in your developer workflow if you
    do choose to adopt them. If engineers run their own clusters without enforcement
    and you apply policy only later in the pipeline, your environments have very different
    configurations, and something will get broken.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略是Kubernetes中的一个重要安全控制，对于习惯于防火墙和隔离网络的架构团队来说很有吸引力。但如果你选择采用它们，你需要了解策略在你的开发工作流程中的位置。如果工程师在没有强制执行的情况下运行自己的集群，而你只在管道的后期应用策略，你的环境配置非常不同，某些东西将会被破坏。
- en: 'I’ve covered only the basic details of the NetworkPolicy API here, because
    the complexity is more in the cluster configuration than in the policy resources.
    If you want to explore further, there’s a great GitHub repository full of network
    policy recipes published by Ahmet Alp Balkan, an engineer at Google: [https://github.com/ahmetb/
    kubernetes-network-policy-recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里只介绍了NetworkPolicy API的基本细节，因为复杂性更多地在于集群配置而不是策略资源。如果你想进一步探索，有一个由Google的工程师Ahmet
    Alp Balkan发布的充满网络策略菜谱的GitHub仓库非常棒：[https://github.com/ahmetb/kubernetes-network-policy-recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes)。
- en: Now let’s clear up the new cluster and see if your old cluster still works.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们清理新的集群，看看你的旧集群是否仍然工作。
- en: Try it now Remove the Calico cluster, and see if the old cluster is still accessible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 移除Calico集群，看看旧集群是否仍然可访问。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Your previous cluster is probably no longer accessible because of the network
    changes Calico made, even though Calico isn’t running now. Figure 16.6 shows me
    about to hit the *Reset Kubernetes* button in Docker Desktop; if you’re using
    Kind, you’ll need to delete and recreate your original cluster, and if you’re
    using something else and it doesn’t work . . . I did warn you.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你的前一个集群可能因为Calico所做的网络更改而无法访问，即使Calico现在没有运行。图16.6显示我即将在Docker Desktop中点击*重置Kubernetes*按钮；如果你使用Kind，你需要删除并重新创建你的原始集群，如果你使用其他东西并且它不起作用……我确实警告过你。
- en: '![](../Images/16-6.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16-6.jpg)'
- en: Figure 16.6 Calico running in a container was able to reconfigure my network
    and break things.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 在容器中运行的Calico能够重新配置我的网络并破坏事物。
- en: Now that we’re all back to normal (hopefully), we can move on to securing containers
    themselves, so applications don’t have privileges like reconfiguring the network
    stack.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们都恢复正常了（希望如此），我们可以继续进行保护容器本身，这样应用程序就不会有重新配置网络栈等特权。
- en: 16.2 Restricting container capabilities with security contexts
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 使用安全上下文限制容器功能
- en: Container security is really about Linux security and the access model for the
    container user (Windows Server containers have a different user model that doesn’t
    have the same issues). Linux containers usually run as the *root* super-admin
    account, and unless you explicitly configure the user, root inside the container
    is root on the host, too. If an attacker can break out of a container running
    as root, they’re in charge of your server now. That’s a problem for all container
    runtimes, but Kubernetes adds a few more problems of its own.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 容器安全实际上关于Linux安全以及容器用户的访问模型（Windows Server容器有一个不同的用户模型，没有相同的问题）。Linux容器通常以*root*超级管理员账户运行，除非你明确配置用户，容器内的root也是主机上的root。如果一个攻击者能够从以root运行的容器中突破，那么他们现在就控制了你的服务器。这是一个所有容器运行时的问题，但Kubernetes又添加了一些它自己的问题。
- en: In the next exercise, you’ll run the Pi web application with a basic Deployment
    configuration. That container image is packaged on top of the official .NET Core
    application run-time image from Microsoft. The Pod spec isn’t deliberately insecure,
    but you’ll see that the defaults aren’t encouraging.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，你将使用基本的部署配置运行Pi网络应用。这个容器镜像是在微软的官方.NET Core应用运行时镜像之上打包的。Pod规范并非故意不安全，但你将看到默认设置并不令人鼓舞。
- en: Try it now Run a simple application, and check the default security situation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：运行一个简单的应用程序，并检查默认的安全情况。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The behavior is scary: the app runs as root, it has access to the Kubernetes
    API server, and it even has a token set up so it can authenticate with Kubernetes.
    Figure 16.7 shows it all in action. Running as root magnifies any exploit an attacker
    can find in the application code or the runtime. Having access to the Kubernetes
    API means an attacker doesn’t even need to break out of the container—they can
    use the token to query the API and do interesting things, like fetch the contents
    of Secrets (depending on the access permissions for the Pod, which you’ll learn
    about in chapter 17).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为令人担忧：应用程序以root身份运行，它有权访问Kubernetes API服务器，并且它甚至设置了一个令牌，以便它可以与Kubernetes进行身份验证。图16.7显示了所有这些操作。以root身份运行放大了攻击者可以在应用程序代码或运行时找到的任何漏洞。有权访问Kubernetes
    API意味着攻击者甚至不需要从容器中逃逸——他们可以使用令牌查询API并做些有趣的事情，比如获取Secrets的内容（取决于Pod的访问权限，你将在第17章中了解这些内容）。
- en: '![](../Images/16-7.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图16.7](../Images/16-7.jpg)'
- en: Figure 16.7 If you’ve heard the phrase “secure by default,” it wasn’t said about
    Kubernetes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 如果你听说过“默认安全”这个短语，那么它并不是针对Kubernetes说的。
- en: Kubernetes provides multiple security controls at the Pod and container levels,
    but they’re not enabled by default because they could break your app. You can
    run containers as a different user, but some apps work only if they’re running
    as root. You can drop Linux capabilities to restrict what the container can do,
    but then some app features could fail. This is where automated testing comes in,
    because you can increasingly tighten the security around your apps, running tests
    at each stage to confirm everything still works.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在Pod和容器级别提供了多个安全控制，但它们默认并未启用，因为它们可能会破坏你的应用程序。你可以以不同的用户身份运行容器，但有些应用程序只有在以root身份运行时才能工作。你可以降低Linux能力来限制容器能做什么，但这样一些应用程序功能可能会失败。这就是自动化测试发挥作用的地方，因为你可以不断加强应用程序的安全性，在每个阶段运行测试以确认一切仍然正常工作。
- en: The main control you’ll use is the SecurityContext field, which applies security
    at the Pod and container levels. Listing 16.2 shows a Pod SecurityContext that
    explicitly sets the user and the Linux group (a collection of users), so all of
    the containers in the Pod run as the *unknown* user rather than root.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你将主要使用的是SecurityContext字段，它在Pod和容器级别应用安全策略。列表16.2展示了如何显式设置用户和Linux组（用户集合），因此Pod中的所有容器都以*unknown*用户身份运行，而不是root用户。
- en: Listing 16.2 deployment-podsecuritycontext.yaml, running as a specific user
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.2 deployment-podsecuritycontext.yaml，以特定用户运行
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: That’s simple enough, but moving away from root has repercussions, and the Pi
    spec needs a few more changes. The app listens on port 80 inside the container,
    and Linux requires elevated permissions to listen on that port. Root has the permission,
    but the new user doesn’t, so the app will fail to start. It needs some additional
    configuration in an environment variable to set the app to listen on port 5001
    instead, which is valid for the new user. This is the sort of detail you need
    to drive out for each app or each class of application, and you’ll find the requirements
    only when the app stops working.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单，但离开root用户会有影响，Pi规范需要一些更多的更改。应用程序在容器内部监听80端口，而Linux需要提升权限才能监听该端口。root用户有权限，但新用户没有，所以应用程序将无法启动。需要在环境变量中进行一些额外的配置，将应用程序设置为监听5001端口，这对于新用户是有效的。这种细节你需要为每个应用程序或应用程序类别逐一解决，你将在应用程序停止工作时发现这些需求。
- en: Try it now Deploy the secured Pod spec. This uses a nonroot user and an unrestricted
    port, but the port mapping in the Service hides that detail from consumers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：部署受保护Pod规范。这使用了一个非root用户和一个不受限制的端口，但服务中的端口映射隐藏了这一细节，对消费者来说是不可见的。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Running as a nonroot user addresses the risk of an application exploit escalating
    into a full server takeover, but as shown in figure 16.8, it doesn’t solve all
    the problems. The Kubernetes API token is mounted with permissions for any account
    to read it, so an attacker could still use the API in this setup. What they can
    do with the API depends on how your cluster is configured—in early versions of
    Kubernetes, they’d be able to do everything. The identity to access the API server
    is different from the Linux user, and it might have administrator rights in the
    cluster, even though the container process is running as a least-privilege user.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以非root用户身份运行可以解决应用程序利用风险升级为完全服务器接管的问题，但如图16.8所示，它并不能解决所有问题。Kubernetes API令牌具有任何账户都可以读取的权限，因此攻击者仍然可以使用此设置中的API。他们可以使用API做什么取决于你的集群是如何配置的——在Kubernetes的早期版本中，他们可以做任何事情。访问API服务器的身份与Linux用户不同，它可能在集群中拥有管理员权限，即使容器进程是以最低权限用户身份运行的。
- en: An option in the Pod spec stops Kubernetes from mounting the access token, which
    you should include for every app that doesn’t actually need to use the Kubernetes
    API—which will be pretty much everything, except workloads like ingress controllers,
    which need to find Service endpoints. It’s a safe option to set, but the next
    level of run-time control will need more testing and evaluation. The SecurityContext
    field in the container spec allows for more fine-grained control than at the Pod
    level. Listing 16.3 shows a set of options that work for the Pi app.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Pod规范中的一个选项阻止Kubernetes挂载访问令牌，你应该为每个实际上不需要使用Kubernetes API的应用程序包含此选项——这几乎将是一切，除了需要找到服务端点的工作负载，如入口控制器。这是一个安全的选项来设置，但下一级别的运行时控制需要更多的测试和评估。容器规范中的SecurityContext字段在Pod级别上提供了更细粒度的控制。列表16.3显示了适用于Pi应用程序的一组选项。
- en: '![](../Images/16-8.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/16-8.jpg)'
- en: Figure 16.8 You need an in-depth security approach to Kubernetes; one setting
    is not enough.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8 你需要对Kubernetes采取深入的安全方法；一个设置是不够的。
- en: Listing 16.3 deployment-no-serviceaccount-token.yaml, tighter security policies
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.3 deployment-no-serviceaccount-token.yaml，更严格的安全策略
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The capabilities field lets you explicitly add and remove Linux kernel capabilities.
    This app works happily with all capabilities dropped, but other apps will need
    some added back in. One feature this app doesn’t support is the `readOnlyRootFilesystem`
    option. That’s a powerful one to include if your app can work with a read-only
    filesystem, because it means attackers can’t write files, so they can’t download
    malicious scripts or binaries. How far you take this depends on the security profile
    of your organization. You can mandate that all apps need to run as nonroot, with
    all capabilities dropped and with a read-only filesystem, but that might mean
    you need to rewrite most of your apps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: capabilities字段允许您显式添加和删除Linux内核能力。此应用程序在没有能力的情况下也能愉快地运行，但其他应用程序可能需要添加一些能力。此应用程序不支持的一个功能是`readOnlyRootFilesystem`选项。如果你的应用程序可以与只读文件系统一起工作，这是一个非常有用的选项，因为它意味着攻击者无法写入文件，因此他们无法下载恶意脚本或二进制文件。你采取多远取决于你组织的安全配置文件。你可以强制要求所有应用程序都需要以非root用户身份运行，所有能力都被删除，并且具有只读文件系统，但这可能意味着你需要重写大多数应用程序。
- en: A pragmatic approach is to secure your existing apps as tightly as you can at
    the container level and make sure you have security in depth around the rest of
    your policies and processes. The final spec for the Pi app is not perfectly secure,
    but it’s a big improvement on the defaults—and the application still works.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实用方法是尽可能在容器级别保护现有的应用程序，并确保你围绕其他政策和流程有深入的安全措施。Pi应用程序的最终规范并不完美安全，但与默认设置相比有了很大的改进——并且应用程序仍然可以工作。
- en: Try it now Update the Pi app with the final security configuration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 更新Pi应用程序的最终安全配置。
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As shown in figure 16.9, the app can still reach the Kubernetes API server,
    but it has no access token, so an attacker would need to do more work to send
    valid API requests. Applying a NetworkPolicy to deny ingress to the API server
    would remove that option altogether.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如图16.9所示，应用程序仍然可以访问Kubernetes API服务器，但它没有访问令牌，因此攻击者需要做更多工作才能发送有效的API请求。应用一个NetworkPolicy来拒绝API服务器的入站访问将完全移除这个选项。
- en: '![](../Images/16-9.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/16-9.jpg)'
- en: Figure 16.9 The secured app is the same for users but much less fun for attackers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9 对于用户来说，安全的应用程序是一样的，但对于攻击者来说则乐趣大减。
- en: 'You need to invest in adding security to your apps, but if you have a reasonably
    small range of application platforms, you can build up generic profiles: you might
    find that all your .NET apps can run as nonroot but need a writable filesystem,
    and all your Go apps can run with a read-only filesystem but need some Linux capabilities
    added. The challenge, then, is in making sure your profiles actually are applied,
    and Kubernetes has a nice feature for that: *admission control*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在您的应用程序中投资增加安全性，但如果您的应用程序平台范围相对较小，您可以构建通用的配置文件：您可能会发现所有您的 .NET 应用程序都可以以非root用户身份运行，但需要一个可写文件系统，而所有您的
    Go 应用程序都可以使用只读文件系统运行，但需要添加一些 Linux 功能。那么，挑战就在于确保您的配置文件实际上被应用了，Kubernetes 有一个很好的功能来实现这一点：*准入控制*。
- en: 16.3 Blocking and modifying workloads with webhooks
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 使用 webhook 阻止和修改工作负载
- en: Every object you create in Kubernetes goes through a process to check if it’s
    okay for the cluster to run that object. That process is admission control, and
    we saw an admission controller at work in chapter 12, trying to deploy a Pod spec
    that requested more resources than the namespace had available. The ResourceQuota
    admission controller is a built-in controller, which stops workloads from running
    if they exceed quotas, and Kubernetes has a plug-in system so you can add your
    own admission control rules.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您在 Kubernetes 中创建的每个对象都会经过一个检查过程，以确定集群是否可以运行该对象。这个过程是准入控制，我们在第 12 章中看到了准入控制器在工作，尝试部署一个请求比命名空间可用资源更多的
    Pod 规范。ResourceQuota 准入控制器是一个内置控制器，它会阻止超出配额的工作负载运行，Kubernetes 有一个插件系统，因此您可以添加自己的准入控制规则。
- en: 'Two other controllers add that extensibility: the ValidatingAdmissionWebhook,
    which works like ResourceQuota to allow or block object creation, and the MutatingAdmissionWebhook,
    which can actually edit object specs, so the object that is created is different
    from the request. Both controllers work in the same way: you create a configuration
    object specifying the object life cycles you want to control and a URL for a web
    server that applies the rules. Figure 16.10 shows how the pieces fit together.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 两个其他控制器增加了这种可扩展性：ValidatingAdmissionWebhook，它类似于 ResourceQuota，允许或阻止对象创建，以及
    MutatingAdmissionWebhook，它可以实际编辑对象规范，因此创建的对象与请求不同。这两个控制器以相同的方式工作：您创建一个配置对象，指定您想要控制的对象生命周期和应用于规则的
    web 服务器的 URL。图 16.10 显示了这些组件是如何组合在一起的。
- en: '![](../Images/16-10.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/16-10.jpg)'
- en: Figure 16.10 Admission webhooks let you apply your own rules when objects are
    created.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.10 准入 webhook 允许在创建对象时应用您自己的规则。
- en: Admission webhooks are hugely powerful because Kubernetes calls into your own
    code, which can be running in any language you like and can apply whatever rules
    you need. In this section, we’ll apply some webhooks I’ve written in Node.js.
    You won’t need to edit any code, but you can see in listing 16.4 that the code
    isn’t particularly complicated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 准入 webhook 非常强大，因为 Kubernetes 会调用您自己的代码，该代码可以运行在任何您喜欢的语言中，并应用您需要的任何规则。在本节中，我们将应用一些我使用
    Node.js 编写的 webhook。您不需要编辑任何代码，但您可以在列表 16.4 中看到代码并不特别复杂。
- en: Listing 16.4 validate.js, custom logic for a validating webhook
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.4 validate.js，验证 webhook 的自定义逻辑
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Webhook servers can run anywhere—inside or outside of the cluster—but they must
    be served on HTTPS. The only complication comes if you want to run webhooks inside
    your cluster signed by your own certificate authority (CA), because the webhook
    configuration needs a way to trust the CA. This is a common scenario, so we’ll
    walk through that complexity in the next exercises.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: webhook 服务器可以在任何地方运行——集群内部或外部——但它们必须通过 HTTPS 提供服务。唯一的问题是如果您想在您的集群内部运行由您自己的证书颁发机构（CA）签名的
    webhook，因为 webhook 配置需要一种信任 CA 的方式。这是一个常见的场景，所以我们将在这个练习中逐步解决这个复杂性。
- en: Try it now Start by creating a certificate and deploying the webhook server
    to use that certificate.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 首先创建一个证书并将 webhook 服务器部署以使用该证书。
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The last command in that exercise will fill your screen with Base64-encoded
    text, which you’ll need in the next exercise (don’t worry about writing it down,
    though; we’ll automate all the steps). You now have the webhook server running,
    secured by a TLS certificate issued by a custom CA. My output appears in figure
    16.11.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那个练习中的最后一个命令会在您的屏幕上填充 Base64 编码的文本，您将在下一个练习中使用它（尽管不必担心记录下来；我们将自动化所有步骤）。现在您已经有了运行中的
    webhook 服务器，由自定义 CA 签发的 TLS 证书进行保护。我的输出如图 16.11 所示。
- en: '![](../Images/16-11.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/16-11.jpg)'
- en: Figure 16.11 Webhooks are potentially dangerous, so they need to be secured
    with HTTPS.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/16-11.jpg)'
- en: 'The Node.js app is running and has two endpoints: a validating webhook, which
    checks that all Pod specs have the `automountServiceAccountToken` field set to
    false, and a mutating webhook, which applies a container SecurityContext set with
    the `runAsNonRoot` flag. Those two policies are intended to work together to ensure
    a base level of security for all applications. Listing 16.5 shows the spec for
    the ValidatingWebhookConfiguration object.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js 应用正在运行，并有两个端点：一个验证 webhook，它检查所有 Pod 规范是否将 `automountServiceAccountToken`
    字段设置为 false，以及一个变异 webhook，它应用一个带有 `runAsNonRoot` 标志的容器 SecurityContext。这两个策略旨在协同工作，以确保所有应用程序都有一个基本的安全级别。列表
    16.5 显示了 ValidatingWebhookConfiguration 对象的规范。
- en: Listing 16.5 validatingWebhookConfiguration.yaml, applying a webhook
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.5 validatingWebhookConfiguration.yaml，应用 webhook
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Webhook configurations are flexible: you can set the types of operation and
    the types of object on which the webhook operates. You can have multiple webhooks
    configured for the same object—validating webhooks are all called in parallel,
    and any one of them can block the operation. This YAML file is part of a Helm
    chart that I’m using just for this config object, as an easy way to inject the
    CA certificate. A more advanced Helm chart would include a job to generate the
    certificate and deploy the webhook server along with the configurations—but then
    you wouldn’t see how it all fits together.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Webhook 配置是灵活的：您可以设置操作类型和 webhook 操作的对象类型。您可以针对同一对象配置多个 webhook——验证 webhook
    都会并行调用，并且任何一个都可以阻止操作。此 YAML 文件是我为这个配置对象使用的一个 Helm 图表的一部分，作为注入 CA 证书的简单方法。一个更高级的
    Helm 图表将包括一个生成证书并部署 webhook 服务器以及配置的任务——但那样您就看不到它们是如何结合在一起的。
- en: Try it now Deploy the webhook configuration, passing the CA cert from the generator
    Pod as a value to the local Helm chart. Then try to deploy an app, which fails
    the policy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看：部署 webhook 配置，将生成器 Pod 作为值传递给本地 Helm 图表中的 CA 证书。然后尝试部署一个应用程序，该应用程序会违反策略。
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this exercise, you can see the strength and the limitation of validating
    webhooks. The webhook operates at the Pod level, and it stops Pods from being
    created if they don’t match the service token rule. But it’s the ReplicaSet and
    the Deployment that try to create the Pod, and they don’t get blocked by the admission
    controller, so you have to dig a bit deeper to find why the app isn’t running.
    My output is shown in figure 16.12, where the `describe` command is abridged to
    show just the error line.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您可以看到验证 webhook 的优势和局限性。webhook 在 Pod 级别操作，如果 Pod 不匹配服务令牌规则，它会阻止 Pod
    的创建。但是，是 ReplicaSet 和 Deployment 尝试创建 Pod，它们不会被 admission 控制器阻止，所以您需要深入挖掘以找到应用程序为何无法运行的原因。我的输出显示在图
    16.12 中，其中 `describe` 命令被简化以仅显示错误行。
- en: '![](../Images/16-12.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.12](../Images/16-12.jpg)'
- en: Figure 16.12 Validating webhooks can block object creation, whether it was instigated
    by a user or a controller.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.12 验证 webhook 可以阻止对象创建，无论是由用户还是控制器触发的。
- en: You need to think carefully about the objects and operations you want your webhook
    to act on. This validation could happen at the Deployment level instead, which
    would give a better user experience, but it would miss Pods created directly or
    by other types of controllers. It’s also important to return a clear message in
    the webhook response, so users know how to fix the issue. The ReplicaSet will
    just keep trying to create the Pod and failing (it’s tried 18 times on my cluster
    while I’ve been writing this), but the failure message tells me what to do, and
    this one is easy to fix.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要仔细思考您希望 webhook 作用的对象和操作。这种验证可以在 Deployment 级别进行，这将提供更好的用户体验，但它会错过直接创建或由其他类型的控制器创建的
    Pods。在 webhook 响应中返回一个清晰的消息也很重要，这样用户就知道如何修复问题。ReplicaSet 将会不断尝试创建 Pod 并失败（在我写这段话的时候，我的集群已经尝试了
    18 次），但失败信息告诉我该怎么做，而且这个问题很容易解决。
- en: One of the problems with admission webhooks is that they score very low on discoverability.
    You can use kubectl to check if there are any validating webhooks configured,
    but that doesn’t tell you anything about the actual rules, so you need to have
    that documented outside of the cluster. The situation gets even more confusing
    with mutating webhooks, because if they work as expected, they give users a different
    object from the one they tried to create. In the next exercise, you’ll see that
    a well-intentioned mutating webhook can break applications.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Admission webhook的一个问题是它们的可发现性得分非常低。你可以使用kubectl来检查是否配置了任何验证webhook，但这不会告诉你任何关于实际规则的信息，因此你需要将它们在集群外部进行文档化。当涉及到mutating
    webhook时，情况变得更加混乱，因为如果它们按预期工作，它们会向用户提供一个与尝试创建的对象不同的对象。在下一个练习中，你会看到一个有良好意图的mutating
    webhook可能会破坏应用。
- en: Try it now Configure a mutating webhook using the same webhook server but a
    different URL path. This webhook adds security settings to Pod specs. Deploy another
    app, and you’ll see the changes from the webhook stop the app from running.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看。使用相同的webhook服务器但不同的URL路径配置一个mutating webhook。这个webhook为Pod规范添加安全设置。部署另一个应用，你会看到来自webhook的更改阻止了应用运行。
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Oh dear. The mutating webhook adds a SecurityContext to the Pod spec with the
    `runAsNonRoot` field set to true. That flag tells Kubernetes not to run any containers
    in the Pod if they’re configured to run as root—which this app is, because it’s
    based on the official Nginx image, which does use root. As you can see in figure
    16.13, describing the Pod tells you what the problem is, but it doesn’t state
    that the spec has been mutated. Users will be highly confused when they check
    their YAML again and find no `runAsNonRoot` field.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。mutating webhook将一个带有`runAsNonRoot`字段设置为true的SecurityContext添加到Pod规范中。这个标志告诉Kubernetes不要在Pod中运行任何配置为以root身份运行的容器——这个应用就是这样，因为它基于官方的Nginx镜像，它确实使用了root。正如你在图16.13中可以看到的，描述Pod会告诉你问题是什么，但它没有声明规范已经被修改。当用户再次检查他们的YAML文件并发现没有`runAsNonRoot`字段时，他们会非常困惑。
- en: '![](../Images/16-13.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图16.13](../Images/16-13.jpg)'
- en: Figure 16.13 Mutating webhooks can cause application failures, which are difficult
    to debug.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13 Mutating webhook可能会导致应用失败，这些失败很难调试。
- en: The logic inside a mutating webhook is entirely up to you—you can accidentally
    change objects to set an invalid spec they will never deploy. It’s a good idea
    to have a more restrictive object selector for your webhook configurations. Listing
    16.5 applies to every Pod, but you can add namespace and label selectors to narrow
    the scope. This webhook has been built with sensible rules, and if the Pod spec
    already contains a `runAsNonRoot` value, the webhook leaves it alone, so apps
    can be modeled to explicitly require the root user.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: mutating webhook内部的逻辑完全由你决定——你可能会意外地更改对象以设置一个无效的规范，它们将永远不会部署。为你的webhook配置设置一个更严格的对象选择器是个好主意。列表16.5适用于每个Pod，但你也可以添加命名空间和标签选择器来缩小范围。这个webhook已经构建了合理的规则，如果Pod规范已经包含`runAsNonRoot`值，webhook就会保持不变，这样应用就可以被建模为明确要求root用户。
- en: Admission controller webhooks are a useful tool to know about, and they let
    you do some cool things. You can add sidecar containers to Pods with mutating
    webhooks, so you could use a label to identify all the apps that write log files
    and have a webhook automatically add a logging sidecar to those Pods. Webhooks
    can be dangerous, which is something you can mitigate with good testing and selective
    rules in your config objects, but they will always be invisible because the logic
    is hidden inside the webhook server.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Admission controller webhook是一个有用的工具，你应该了解它，并且它让你可以做些很酷的事情。你可以使用mutating webhook向Pod添加sidecar容器，所以你可以使用标签来识别所有写入日志文件的应用，并让webhook自动将这些Pod添加一个日志sidecar。Webhook可能很危险，但你可以通过良好的测试和配置对象中的选择性规则来减轻这种风险，但它们始终是看不见的，因为逻辑隐藏在webhook服务器内部。
- en: In the next section, we’ll look at an alternative approach that uses validating
    webhooks under the hood but wraps them in a management layer. *Open Policy Agent*
    (OPA) lets you define your rules in Kubernetes objects, which are discoverable
    in the cluster and don’t require custom code.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一种替代方法，它使用验证webhook作为底层，但将其包装在管理层中。*Open Policy Agent* (OPA)允许你在Kubernetes对象中定义你的规则，这些规则在集群中是可发现的，并且不需要自定义代码。
- en: 16.4 Controlling admission with Open Policy Agent
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 使用Open Policy Agent控制准入
- en: OPA is a unified approach to writing and implementing policies. The goal is
    to provide a standard language for describing all kinds of policy and integrations
    to apply policies in different platforms. You can describe data access policies
    and deploy them in SQL databases, and you can describe admission control policies
    for Kubernetes objects. OPA is another CNCF project that provides a much cleaner
    alternative to custom validating webhooks with OPA Gatekeeper.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: OPA 是一种统一的方法来编写和实施策略。目标是提供一个标准语言来描述所有类型的策略，以及在不同平台上应用策略的集成。你可以描述数据访问策略并在 SQL
    数据库中部署它们，你也可以描述 Kubernetes 对象的准入控制策略。OPA 是另一个 CNCF 项目，它为使用 OPA Gatekeeper 的自定义验证
    webhook 提供了一个更干净的替代方案。
- en: 'OPA Gatekeeper features three parts: you deploy the Gatekeeper components in
    your cluster, which include a webhook server and a generic ValidatingWebhookConfiguration;
    then you create a *constraint template*, which describes the admission control
    policy; and then you create a specific *constraint* based on the template. It’s
    a flexible approach where you can build a template for the policy “all Pods must
    have the expected labels” and then deploy a constraint to say which labels are
    needed in which namespace.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: OPA Gatekeeper 有三个部分：你在你的集群中部署 Gatekeeper 组件，这些组件包括一个 webhook 服务器和一个通用的 ValidatingWebhookConfiguration；然后你创建一个
    *约束模板*，它描述了准入控制策略；然后你基于模板创建一个具体的 *约束*。这是一个灵活的方法，你可以为策略“所有 Pod 必须具有预期的标签”构建一个模板，然后部署一个约束来指定在哪个命名空间中需要哪些标签。
- en: We’ll start by removing the custom webhooks we added and deploying OPA Gatekeeper,
    ready to apply some admission policies.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从移除我们添加的自定义 webhook 并部署 OPA Gatekeeper 开始，准备应用一些准入策略。
- en: Try it now Uninstall the webhook components, and deploy Gatekeeper.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 取消卸载 webhook 组件，并部署 Gatekeeper。
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: I’ve abbreviated my output in figure 16.14—when you run the exercise, you’ll
    see the OPA Gatekeeper deployment installs many more objects, including things
    we haven’t come across yet called CustomResourceDefinitions (CRDs). We’ll cover
    those in more detail in chapter 20 when we look at extending Kubernetes, but for
    now, it’s enough to know that CRDs let you define new types of object that Kubernetes
    stores and manages for you.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我在图 16.14 中简化了我的输出——当你运行练习时，你会看到 OPA Gatekeeper 部署安装了许多更多的对象，包括我们尚未遇到的东西，称为
    CustomResourceDefinitions（CRDs）。我们将在第 20 章中详细讨论这些内容，当我们查看扩展 Kubernetes 时，但现在，了解
    CRDs 允许你定义 Kubernetes 为你存储和管理的新类型对象就足够了。
- en: '![](../Images/16-14.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/16-14.jpg)'
- en: Figure 16.14 OPA Gatekeeper takes care of all the tricky parts of running a
    webhook server.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.14 OPA Gatekeeper 负责处理运行 webhook 服务器的所有复杂部分。
- en: 'Gatekeeper uses CRDs so you can create templates and constraints as ordinary
    Kubernetes objects, defined in YAML and deployed with kubectl. The template contains
    the generic policy definition in a language called Rego (pronounced “ray-go”).
    It’s an expressive language that lets you evaluate the properties of some input
    object to check if they meet your requirements. It’s another thing to learn, but
    Rego has some big advantages: policies are fairly easy to read, and they live
    in your YAML files, so they’re not hidden in the code of a custom webhook; and
    there are lots of sample Rego policies to enforce the kind of rules we’ve looked
    at in this chapter. Listing 16.6 shows a Rego policy that requires objects to
    have labels.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Gatekeeper 使用 CRDs，因此你可以创建模板和约束作为普通的 Kubernetes 对象，这些对象在 YAML 中定义，并使用 kubectl
    部署。模板包含在称为 Rego 的语言中定义的通用策略定义（发音为“ray-go”）。它是一种表达性语言，允许你评估某些输入对象的属性，以检查它们是否满足你的要求。这是一件需要学习的事情，但
    Rego 有一些显著的优势：策略相对容易阅读，并且它们位于你的 YAML 文件中，因此它们不会隐藏在自定义 webhook 的代码中；并且有许多示例 Rego
    策略来强制执行我们在本章中查看的规则。列表 16.6 显示了一个要求对象具有标签的 Rego 策略。
- en: Listing 16.6 requiredLabels-template.yaml, a basic Rego policy
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.6 requiredLabels-template.yaml，一个基本的 Rego 策略
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You deploy that policy with Gatekeeper as a constraint template, and then you
    deploy a constraint object that enforces the template. In this case, the template,
    called RequiredLabels, uses parameters to define the labels that are required.
    Listing 16.7 shows a specific constraint for all Pods to have `app` and `version`
    labels.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用 Gatekeeper 作为约束模板部署该策略，然后部署一个强制执行模板的约束对象。在这种情况下，名为 RequiredLabels 的模板使用参数来定义所需的标签。列表
    16.7 显示了一个特定约束，要求所有 Pod 都必须有 `app` 和 `version` 标签。
- en: Listing 16.7 requiredLabels.yaml, a constraint from a Gatekeeper template
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.7 requiredLabels.yaml，来自 Gatekeeper 模板的一个约束
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is much easier to read, and you can deploy many constraints from the same
    template. The OPA approach lets you build a standard policy library, which users
    can apply in their application specs without needing to dig into the Rego. In
    the next exercise, you’ll deploy the constraint from listing 16.7 with another
    constraint that requires all Deployments, Services, and ConfigMaps to have a `kiamol`
    label. Then you’ll try to deploy a version of the to-do app that fails all those
    policies.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这更容易阅读，你可以从同一个模板部署许多约束。OPA方法让你能够构建一个标准的策略库，用户可以在他们的应用程序规范中应用，而无需深入研究Rego。在下一个练习中，你将部署列表16.7中的约束，以及另一个要求所有Deployments、Services和ConfigMaps都具有`kiamol`标签的约束。然后你将尝试部署一个待办事项应用程序的版本，该版本将违反所有这些策略。
- en: Try it now Deploy required label policies with Gatekeeper, and see how they
    are applied.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：使用Gatekeeper部署所需的标签策略，并查看它们是如何应用的。
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can see in figure 16.15 that this user experience is clean—the objects we’re
    trying to create don’t have the required labels, so they get blocked, and we see
    the message from the Rego policy in the output from kubectl.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图16.15中看到，这个用户体验是干净的——我们试图创建的对象没有所需的标签，因此它们被阻止，我们在kubectl的输出中看到了Rego策略的消息。
- en: '![](../Images/16-15.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图16-15](../Images/16-15.jpg)'
- en: Figure 16.15 Deployment failures show a clear error message returned from the
    Rego policy.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.15 部署失败显示了Rego策略返回的清晰错误消息。
- en: Gatekeeper evaluates constraints using a validating webhook, and it’s very obvious
    when failures arise in the object you’re creating. It’s a bit less clear when
    objects created by controllers fail validation, because the controller itself
    can be fine. We saw that in section 16.3, and because Gatekeeper uses the same
    validation mechanism, it has the same issue. You’ll see that if you update the
    to-do app so the Deployment meets the label requirements but the Pod spec doesn’t.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Gatekeeper使用验证webhook来评估约束，当你在创建的对象中遇到失败时，这非常明显。当由控制器创建的对象失败验证时，这就不那么明显了，因为控制器本身可能没有问题。我们在16.3节中看到了这一点，因为Gatekeeper使用相同的验证机制，所以它也有同样的问题。如果你更新待办事项应用程序，使其部署满足标签要求，但Pod规范不满足，你将会看到这一点。
- en: Try it now Deploy an updated to-do list spec, which has the correct labels for
    all objects except the Pod.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：部署一个更新的待办事项列表规范，其中所有对象（除了Pod）都有正确的标签。
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You’ll find in this exercise that the admission policy worked, but you see the
    problem only when you dig into the description for the failing ReplicaSet, as
    in figure 16.16\. That’s not such a great user experience. You could fix this
    with a more sophisticated policy that applies at the Deployment level and checks
    labels in the Pod template—that could be done with extended logic in the Rego
    for the constraint template.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你会发现招生政策是有效的，但你只有在深入查看失败的ReplicaSet的描述时，如图16.16所示，才会看到问题。这并不是一个很好的用户体验。你可以通过一个更复杂的政策来修复这个问题，该政策在部署级别应用并检查Pod模板中的标签——这可以通过约束模板的Rego扩展逻辑来实现。
- en: '![](../Images/16-16.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图16-16](../Images/16-16.jpg)'
- en: Figure 16.16 OPA Gatekeeper makes for a better process, but it’s still a wrapper
    around validating webhooks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16 OPA Gatekeeper提供了一个更好的流程，但它仍然是一个验证webhook的包装器。
- en: 'We’ll finish this section with the following set of admission policies that
    cover some more production best practices, all of which help to make your apps
    more secure:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用以下一系列招生政策结束本节，这些政策涵盖了更多生产最佳实践，所有这些都有助于使你的应用程序更加安全：
- en: All Pods must have container probes defined. This is for keeping your apps healthy,
    but a failed healthcheck could also indicate unexpected activity from an attack.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有Pod都必须定义容器探测。这是为了保持你的应用程序健康，但失败的健康检查也可能表明攻击的意外活动。
- en: Pods can run containers only from approved image repositories. Restricting containers
    to a set of “golden” repositories with secured production images ensures malicious
    payloads can’t be deployed.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods只能从批准的镜像仓库运行容器。将容器限制在一系列“黄金”仓库中，这些仓库包含受保护的生产镜像，确保恶意有效载荷无法部署。
- en: All containers must have memory and CPU limits set. This prevents a compromised
    container maxing out the compute resources of the node and starving all the other
    Pods.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有容器都必须设置内存和CPU限制。这可以防止受损害的容器耗尽节点的计算资源，并使所有其他Pod饿死。
- en: These generic policies apply to pretty much every organization. You can add
    to them with constraints that require network policies for every app and security
    contexts for every Pod. As you’ve learned in this chapter, not all rules are universal,
    so you might need to be selective on how you apply those constraints. In the next
    exercise, you’ll apply the production constraint set to a single namespace.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些通用策略适用于几乎每个组织。您可以通过为每个应用程序的网络策略和每个Pod的安全上下文添加约束来扩展它们。正如您在本章中学到的，并非所有规则都是通用的，因此您可能需要选择性地应用这些约束。在下一个练习中，您将应用生产约束集到单个命名空间。
- en: Try it now Deploy a new set of constraints and a version of the to-do app where
    the Pod spec fails most of the policies.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 部署一组新的约束和一个待办事项应用程序版本，其中Pod规范大多数策略都失败了。
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Figure 16.17 shows that the Pod spec fails all the rules except one—my image
    repository policy allows any images from Docker Hub in the `kiamol` organization,
    so the to-do app image is valid. But there’s no version label, no health probes,
    and no resource limits, and this spec is not fit for production.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17 显示Pod规范除了一个规则外都失败了——我的镜像仓库策略允许`kiamol`组织中的任何来自Docker Hub的镜像，因此待办事项应用程序的镜像有效。但没有版本标签，没有健康检查，没有资源限制，这个规范不适合生产。
- en: '![](../Images/16-17.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/16-17.jpg)'
- en: Figure 16.17 All constraints are evaluated, and you see the full list of errors
    in the Rego output.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17 所有约束都已评估，您可以在Rego输出中看到完整的错误列表。
- en: Just to prove those policies are achievable and OPA Gatekeeper will actually
    let the to-do app run, you can apply an updated spec that meets all the rules
    for production. If you compare the YAML files in the production folder and the
    update folder, you’ll see the new spec just adds the required fields to the Pod
    template; there are no significant changes in the app.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 只为证明这些策略是可行的，OPA Gatekeeper实际上会允许待办事项应用程序运行，您可以应用一个更新的规范，该规范符合所有生产规则。如果您比较生产文件夹和更新文件夹中的YAML文件，您会看到新规范只是向Pod模板添加了所需的字段；应用程序没有显著变化。
- en: Try it now Apply a production-ready version of the to-do spec, and confirm the
    app really runs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 应用一个生产就绪版本的待办事项规范，并确认应用程序确实正在运行。
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Figure 16.18 shows the app running, after the updated deployment has been permitted
    by OPA Gatekeeper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.18 显示了应用程序在更新部署被OPA Gatekeeper允许后正在运行。
- en: '![](../Images/16-18.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/16-18.jpg)'
- en: Figure 16.18 Constraints are powerful, but you need to make sure apps can actually
    comply.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.18 约束功能强大，但您需要确保应用程序实际上可以遵守。
- en: Open Policy Agent is a much cleaner way to apply admission controls than custom
    validating webhooks, and the sample policies we’ve looked at are only some simple
    ideas to get you started. Gatekeeper doesn’t have mutation functionality, but
    you can combine it with your own webhooks if you have a clear case to modify specs.
    You could use constraints to ensure every Pod spec includes an application-profile
    label and then mutate specs based on your profiles—setting your .NET Core apps
    to run as a nonroot user and switching to a read-only filesystem for all your
    Go apps.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Open Policy Agent是应用准入控制比自定义验证webhook更干净的方法，我们查看的示例策略只是让您入门的一些简单想法。Gatekeeper没有突变功能，但如果您有明确的案例来修改规范，您可以将它与您自己的webhook结合使用。您可以使用约束来确保每个Pod规范都包含一个应用程序配置文件标签，然后根据您的配置文件突变规范——将您的.NET
    Core应用程序设置为以非root用户运行，并将所有Go应用程序切换到只读文件系统。
- en: Securing your apps is about closing down exploit paths, and a thorough approach
    includes all the tools we’ve covered in this chapter and more. We’ll finish up
    with a look at a secure Kubernetes landscape.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 保护您的应用程序是关闭利用路径的过程，一个彻底的方法包括本章中涵盖的所有工具以及更多。我们将以查看一个安全的Kubernetes景观作为结束。
- en: 16.5 Understanding security in depth in Kubernetes
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.5 在Kubernetes中深入理解安全性
- en: Build pipelines can be compromised, container images can be modified, containers
    can run vulnerable software as privileged users, and attackers with access to
    the Kubernetes API could even take control of your cluster. You won’t know your
    app is 100% secure until it has been replaced and you can confirm no security
    breaches occurred during its operation. Getting to that happy place means applying
    security in depth across your whole software supply chain. This chapter has focused
    on securing apps at run time, but you should start before that by scanning container
    images for known vulnerabilities.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 构建管道可能会被破坏，容器图像可能会被修改，容器可以以特权用户身份运行易受攻击的软件，并且能够访问 Kubernetes API 的攻击者甚至可以控制你的集群。直到应用程序被替换并且你可以确认在它的操作期间没有发生安全漏洞，你才知道你的应用程序是
    100% 安全的。达到这个快乐的地方意味着在整个软件供应链中应用深度安全。本章重点介绍了运行时应用程序的安全性，但你应该在此之前开始扫描容器图像以查找已知漏洞。
- en: Security scanners look inside an image, identify the binaries, and check them
    on CVE (Common Vulnerabilities and Exposures) databases. Scans tell you if known
    exploits are in the application stack, dependencies, or operating system tools
    in your image. Commercial scanners have integrations with managed registries (you
    can use Aqua Security with Azure Container Registry), or you can run your own
    (Harbor is the CNCF registry project, and it supports the open source scanners
    Clair and Trivy; Docker Desktop has an integration with Snyk for local scans).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 安全扫描器会检查图像内部，识别二进制文件，并在 CVE（常见漏洞和暴露）数据库中进行检查。扫描结果会告诉你应用堆栈、依赖项或图像中的操作系统工具中是否存在已知的漏洞利用。商业扫描器与受管理的注册表集成（你可以使用与
    Azure 容器注册表集成的 Aqua Security），或者你可以运行自己的（Harbor 是 CNCF 注册表项目，它支持开源扫描器 Clair 和
    Trivy；Docker Desktop 与 Snyk 集成以进行本地扫描）。
- en: You can set up a pipeline where images are pushed to a production repository
    only if the scan is clear. Combine that with a repository admission policy, and
    you can effectively ensure that containers run only if the image is safe. A secure
    image running in a securely configured container is still a target, though, and
    you should look at run-time security with a tool that monitors containers for
    unusual activity and can generate alerts or shut down suspicious behavior. Falco
    is the CNCF project for run-time security, and there are supported commercial
    options from Aqua and Sysdig (among others).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以设置一个管道，只有当扫描结果为清白时，图像才会被推送到生产存储库。结合存储库准入策略，你可以有效地确保只有当图像安全时，容器才会运行。然而，在安全配置的容器中运行的图像仍然是一个目标，你应该使用一个工具来监控容器的异常活动，并可以生成警报或关闭可疑行为，以关注运行时安全性。Falco
    是 CNCF 的运行时安全项目，Aqua 和 Sysdig（以及其他一些公司）提供了受支持的商业选项。
- en: Overwhelmed? You should think about securing Kubernetes as a road map that starts
    with the techniques I’ve covered in this chapter. You can adopt security contexts
    first, then network policies, and then move on to admission control when you’re
    clear about the rules that matter to you. Role-based access control, which we
    cover in chapter 17, is the next stage. Security scanning and run-time monitoring
    are further steps you can take if your organization has enhanced security requirements.
    But I won’t throw anything more at you now—let’s tidy up and get ready for the
    lab.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 感到不知所措？你应该将 Kubernetes 的安全视为一个起点，这个起点是我在本章中介绍的技术。你可以首先采用安全上下文，然后是网络策略，当你清楚对你重要的规则时，再转向准入控制。基于角色的访问控制，我们在第
    17 章中介绍，是下一个阶段。如果你的组织有增强的安全要求，安全扫描和运行时监控是你可以采取的进一步步骤。但现在我不会给你更多的东西——让我们整理一下，为实验室做好准备。
- en: Try it now Delete all the objects we created.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 删除我们创建的所有对象。
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 16.6 Lab
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.6 实验室
- en: At the start of the chapter, I said that volume mounts for host paths are a
    potential attack vector, but we didn’t address that in the exercises, so we’ll
    do it in the lab. This is a perfect scenario for admission control, where Pods
    should be blocked if they use volumes that mount sensitive paths on the host.
    We’ll use OPA Gatekeeper, and I’ve written the Rego for you, so you just need
    to write a constraint.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我说过主机路径的卷挂载是一个潜在的攻击向量，但在练习中我们没有解决这个问题，所以我们在实验室中解决它。这是一个完美的准入控制场景，如果 Pod
    使用挂载在主机上敏感路径的卷，它们应该被阻止。我们将使用 OPA Gatekeeper，我已经为你编写了 Rego，所以你只需要编写一个约束条件。
- en: Start by deploying `gatekeeper.yaml` in the lab folder.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先在实验室文件夹中部署 `gatekeeper.yaml`。
- en: Then deploy the constraint template in `restrictedPaths-template.yaml`—you’ll
    need to look at the spec to see how to build your constraint.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后部署 `restrictedPaths-template.yaml` 中的约束模板——你需要查看规范以了解如何构建你的约束条件。
- en: 'Write and deploy a constraint that uses the template and restricts these host
    paths: `/`, `/bin`, and `/etc`. The constraint should apply only to Pods with
    the label `kiamol=ch16-lab`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写并部署一个使用模板并限制以下主机路径的约束：`/`、`/bin` 和 `/etc`。该约束应仅应用于带有标签 `kiamol=ch16-lab` 的
    Pods。
- en: Deploy `sleep.yaml` in the lab folder. Your constraint should prevent the Pod
    from being created because it uses restricted volume mounts.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实验室文件夹中部署 `sleep.yaml`。你的约束应该阻止 Pod 的创建，因为它使用了受限的卷挂载。
- en: 'This one is fairly straightforward, although you’ll need to read about match
    expressions, which is how Gatekeeper implements label selectors. My solution is
    up on GitHub: [https://github.com/sixeyed/kiamol/blob/master/ch16/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch16/lab/README.md).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题相当直接，尽管你可能需要阅读有关匹配表达式的相关内容，这是 Gatekeeper 实现标签选择器的方式。我的解决方案已上传至 GitHub：[https://github.com/sixeyed/kiamol/blob/master/ch16/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch16/lab/README.md)。
