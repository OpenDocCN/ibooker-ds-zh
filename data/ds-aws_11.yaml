- en: Chapter 11\. Streaming Analytics and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。流式分析与机器学习
- en: In the previous chapters, we assumed that we have all of our data available
    in a centralized static location, such as our S3-based data lake. Real-world data
    is continuously streaming from many different sources across the world simultaneously.
    We need to perform machine learning on streams of data for use cases such as fraud
    prevention and anomaly detection where the latency of batch processing is not
    acceptable. We may also want to run continuous analytics on real-time data streams
    to gain competitive advantage and shorten the time to business insights.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们假设所有数据都在一个集中的静态位置可用，比如我们基于S3的数据湖。现实世界的数据是从全球多个不同来源同时持续流入的。我们需要对数据流执行机器学习，用于欺诈预防和异常检测等用例，这些用例中批处理的延迟是不可接受的。我们可能还希望对实时数据流进行持续分析，以获得竞争优势并缩短业务洞察的时间。
- en: In this chapter, we move from our customer reviews training dataset into a real-world
    scenario. We will focus on analyzing a continuous stream of product review messages
    that we collect from all available online channels. Customer-product feedback
    appears everywhere, including social media channels, partner websites, and customer
    support systems. We need to capture this valuable customer sentiment about our
    products as quickly as possible to spot trends and react fast.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从客户评论的训练数据集转向了现实场景。我们将专注于分析我们从所有可用的在线渠道收集的产品评论消息的连续流。客户对产品的反馈无处不在，包括社交媒体渠道、合作伙伴网站和客户支持系统。我们需要尽快捕获这些有价值的客户对我们产品的情感，以便及时发现趋势并迅速做出反应。
- en: With streaming analytics and machine learning, we are able to analyze continuous
    data streams such as application logs, social media feeds, ecommerce transactions,
    customer support tickets, and product reviews. For example, we may want to detect
    quality issues by analyzing real-time product reviews.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流式分析和机器学习，我们能够分析连续的数据流，例如应用程序日志、社交媒体信息流、电子商务交易、客户支持票务和产品评论。例如，我们可能希望通过分析实时产品评论来检测质量问题。
- en: In a first step, we will analyze the sentiment of the customer, so we can identify
    which customers might need high-priority attention. Next, we will run continuous
    streaming analytics over the incoming review messages to capture the average sentiment
    per product category. We will visualize the continuous average sentiment in a
    metrics dashboard for the line of business (LOB) owners. The LOB owners can now
    detect sentiment trends quickly and take action. We will also calculate an anomaly
    score of the incoming messages to detect anomalies in the data schema or data
    values. In case of a rising anomaly score, we will alert the application developers
    in charge to investigate the root cause. As a last metric, we will also calculate
    a continuous approximate count of the received messages. This number of online
    messages could be used by the digital marketing team to measure effectiveness
    of social media campaigns.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将分析客户的情感，以便识别哪些客户可能需要高优先级的关注。接下来，我们将对传入的评论消息进行连续流分析，以捕获每个产品类别的平均情感。我们将在业务线（LOB）所有者的度量仪表板中可视化连续的平均情感。LOB所有者现在可以快速检测情感趋势并采取行动。我们还将计算传入消息的异常分数，以检测数据架构或数据值中的异常。如果异常分数上升，我们将警告负责的应用程序开发人员来调查根本原因。作为最后一个指标，我们还将计算接收消息的连续近似计数。数字营销团队可以使用这些在线消息数量来衡量社交媒体活动的效果。
- en: This chapter provides examples of both descriptive analytics (summary statistics)
    and predictive analytics using the BERT-based SageMaker models that we trained,
    tuned, and deployed in the previous chapters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了描述性分析（汇总统计）和使用我们在前几章中训练、调优和部署的基于BERT的SageMaker模型的预测性分析示例。
- en: Online Learning Versus Offline Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线学习与离线学习
- en: In [Chapter 9](ch09.html#deploy_models_to_production), we demonstrated how to
    perform near-real-time “online learning” by continuously training a reinforcement-learning
    model using real-time reward data from an example customer review application.
    Online, or incremental, machine learning is a small subset of machine learning
    and somewhat difficult to adapt to classical offline algorithms to effectively
    train online. With online learning, new data is incorporated into the model without
    requiring a complete retrain with the full dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第9章](ch09.html#deploy_models_to_production) 中，我们演示了如何通过实时奖励数据持续训练强化学习模型，以实现几乎实时的“在线学习”，使用的是来自示例客户评论应用程序的数据。在线学习，或增量学习，是机器学习的一个小子集，有些难以适应传统离线算法以有效地进行在线训练。在线学习将新数据整合到模型中，无需使用完整数据集进行完全重新训练。
- en: In general, linear algorithms such as linear regression, logistic regression,
    and K-Means Clustering are a bit easier to train with real-time data because of
    the relatively simple mathematics behind them. Scikit-learn supports incremental
    learning using the `partial_fit()` functions on certain linear algorithms. Apache
    Spark supports streaming versions of linear regression and K-Means Clustering.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，像线性回归、逻辑回归和K-Means聚类这样的线性算法更容易通过实时数据进行训练，因为它们背后的数学比较简单。Scikit-learn支持在某些线性算法上使用`partial_fit()`函数进行增量学习。Apache
    Spark支持线性回归和K-Means聚类的流式版本。
- en: Deep learning algorithms are also capable of online learning as well, since
    they continuously make small adjustments to the learned weights using mini-batches
    of new data. In fact, any time we train a deep learning model from an existing
    model checkpoint or pre-trained model (versus random initial weights), we are
    essentially performing online, incremental training—albeit relatively slowly as
    the data is usually presented to the algorithm from disk and not from a stream.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法也能够进行在线学习，因为它们不断地使用新数据的小批量进行学习权重的微调。实际上，每当我们从现有模型检查点或预训练模型（而不是随机初始权重）中训练深度学习模型时，我们本质上是在进行在线增量训练，尽管数据通常是从磁盘而不是流中提供给算法的。
- en: Streaming Applications
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理应用程序
- en: 'Streaming application data is not like traditional application data typically
    handled by REST APIs and relational databases. The 3 Vs that characterize big
    data also apply to streaming data: volume, velocity, and variety. The data, typically
    small and potentially with a different structure, comes in large quantities—and
    much more quickly than typical application data. The overhead of REST APIs and
    referential integrity of relational databases usually can’t keep up with the performance
    requirements of high-volume and high-velocity streaming applications that may
    consume semistructured or unstructured data.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理应用程序数据与传统应用程序数据不同，传统应用程序数据通常由REST API和关系型数据库处理。大数据的3V特征也适用于流数据：数据量、速度和多样性。这些数据通常是小的，可能具有不同的结构，并且以大量的方式传入——比典型的应用程序数据更快。REST
    API的开销和关系数据库的引用完整性通常无法满足高容量和高速率流处理应用程序的性能要求，这些应用程序可能消费半结构化或非结构化数据。
- en: Distributed streaming systems such as Apache Kafka and Amazon Kinesis require
    multiple instances to communicate across a network to scale and share the load
    of processing high-volume and high-velocity streams. Since multiple instances
    are required to communicate across a network, the instances can sometimes process
    the data at different paces based on network stalls, hardware failures, and other
    unexpected conditions. As such, distributed streaming systems cannot guarantee
    that data will be consumed from the stream in the same order that it was placed
    onto the stream—often called “total order.”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式流处理系统如Apache Kafka和Amazon Kinesis需要多个实例通过网络通信来扩展并共享处理高容量和高速率流的负载。由于多个实例需要通过网络通信，这些实例有时会因网络故障、硬件故障和其他意外情况而导致处理数据的速度不同步。因此，分布式流处理系统无法保证从流中消费数据的顺序与放入流中的顺序相同，这通常被称为“全序”。
- en: Streaming applications need to adjust for this lack of total order guarantee
    and maintain their own concept of order. While we don’t cover total-order guarantee
    in great detail in this chapter, it is something to consider when building streaming
    applications. Some distributed streaming systems allow us to enable total order,
    but total order will negatively impact performance and may negate the benefits
    of building a streaming application.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用程序需要调整以适应这种缺乏总排序保证并维护自己的排序概念。虽然本章节不会详细介绍总排序保证，但在构建流应用程序时需要考虑这一点。一些分布式流系统允许我们启用总排序，但总排序会对性能产生负面影响，并可能抵消构建流应用程序的好处。
- en: Streaming technologies provide us with the tools to collect, process, and analyze
    data streams in real time. AWS offers a wide range of streaming-technology options,
    including Amazon MSK and the Kinesis services. With Kinesis Data Firehose, we
    can prepare and load the data continuously to a destination of our choice. With
    Kinesis Data Analytics, we can process and analyze the data as it arrives using
    SQL or Apache Flink applications. Apache Flink, written in Scala and Java, provides
    advanced streaming-analytics features, including checkpoints for reduced downtime
    and parallel executions for increased performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 流技术为我们提供了实时收集、处理和分析数据流的工具。AWS 提供了多种流技术选项，包括 Amazon MSK 和 Kinesis 服务。通过 Kinesis
    数据输送管道，我们可以持续准备和加载数据到我们选择的目的地。使用 Kinesis 数据分析，我们可以使用 SQL 或 Apache Flink 应用程序在数据到达时处理和分析数据。Apache
    Flink 使用 Scala 和 Java 编写，提供了包括减少停机时间的检查点和增加性能的并行执行在内的高级流分析功能。
- en: With Kinesis Data Streams, we can manage the ingest of data streams for custom
    applications. And with Kinesis Video Streams, we can capture and store video streams
    for analytics. AWS Glue Data Catalog helps us define and enforce the schema of
    structured-streaming data. We can use a self-describing file format like Apache
    Avro with AWS Glue Data Catalog, Kafka, and Kinesis to maintain structured data
    throughout our streaming applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Kinesis 数据流，我们可以管理自定义应用程序的数据流摄取。而使用 Kinesis 视频流，我们可以捕获和存储视频流用于分析。AWS Glue
    数据目录帮助我们定义和强制结构化流数据的模式。我们可以使用像 Apache Avro 这样的自描述文件格式与 AWS Glue 数据目录、Kafka 和 Kinesis
    一起维护整个流应用程序中的结构化数据。
- en: Windowed Queries on Streaming Data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流数据的窗口查询
- en: Descriptive streaming analytics is usually bound by windows to process—either
    by time or by number of input records. For example, we can specify a 30-second
    window or 1,000 input records.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性流分析通常受窗口限制以处理——无论是时间还是输入记录数。例如，我们可以指定一个 30 秒的窗口或 1,000 条输入记录。
- en: If we implement a time-based window, our input records need to contain a timestamp
    column. Kinesis Data Analytics automatically adds a timestamp column called `ROWTIME`
    that we can use in our SQL queries to define time-based windows.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们实现基于时间的窗口，我们的输入记录需要包含一个时间戳列。Kinesis 数据分析会自动添加一个名为 `ROWTIME` 的时间戳列，我们可以在
    SQL 查询中使用它来定义基于时间的窗口。
- en: 'Kinesis Data Analytics supports three different types of windows: Stagger Windows,
    Tumbling Windows, and Sliding Windows. Later, we will use windowed queries to
    implement our streaming analytics and machine learning use cases with streaming
    product-review data.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 数据分析支持三种不同类型的窗口：Stagger 窗口、Tumbling 窗口和 Sliding 窗口。稍后，我们将使用窗口查询来实现使用产品评论数据的流式分析和机器学习用例。
- en: Stagger Windows
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Stagger 窗口
- en: 'Stagger windows are time-based windows that open as new data arrives and are
    the recommended way to aggregate data since they reduce late or out-of-order data.
    Hence, stagger windows are a great choice if we need to analyze groups of data
    that arrive at inconsistent times but should be aggregated together. We specify
    a partition key to identify which records belong together. The stagger window
    will open when the first event matching the partition key arrives. To close the
    window, we specify a window age, which is measured from the time the window opened.
    We define a stagger window with the Kinesis-specific SQL clause `WINDOWED BY`.
    The stagger window takes partition keys and window length as parameters:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Stagger 窗口是基于时间的窗口，随着新数据的到达而打开，是推荐的聚合数据的方式，因为它们减少了延迟或乱序数据。因此，如果我们需要分析在不一致时间到达但应该一起聚合的数据组，Stagger
    窗口是一个很好的选择。我们指定一个分区键来标识哪些记录应该聚合在一起。当匹配分区键的第一个事件到达时，Stagger 窗口将打开。为了关闭窗口，我们指定一个窗口年龄，从窗口打开时开始计算。我们使用
    Kinesis 特定的 SQL 子句 `WINDOWED BY` 来定义 Stagger 窗口。Stagger 窗口以分区键和窗口长度作为参数定义：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The partition key could be the time our product review message appeared, together
    with the product category:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分区键可以是产品评论消息出现的时间，以及产品类别：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The resulting stagger window is shown in [Figure 11-1](#stagger_windows_are_time_based_and_open).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的错开窗口显示在[图 11-1](#stagger_windows_are_time_based_and_open)中。
- en: '![](assets/dsaw_1101.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1101.png)'
- en: Figure 11-1\. Stagger windows.
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-1\. 错开窗口。
- en: 'In this example, we see four data records arriving:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们看到四个数据记录到达：
- en: '| ROWTIME | message_time | product_category |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ROWTIME | message_time | product_category |'
- en: '| --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 11:00:20 | 11:00:10 | BOOKS |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 11:00:20 | 11:00:10 | BOOKS |'
- en: '| 11:00:30 | 11:00:20 | BOOKS |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 11:00:30 | 11:00:20 | BOOKS |'
- en: '| 11:01:05 | 11:00:55 | BOOKS |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 11:01:05 | 11:00:55 | BOOKS |'
- en: '| 11:01:15 | 11:01:05 | BOOKS |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 11:01:15 | 11:01:05 | BOOKS |'
- en: 'Let’s assume we are calculating a count over the data records per product category
    in our SQL query. The one-minute stagger window would aggregate the records like
    this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在计算SQL查询中每个产品类别的数据记录数。一分钟的错开窗口将聚合记录如下：
- en: '| ROWTIME | message_time | product_category | count |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ROWTIME | message_time | product_category | count |'
- en: '| --- | --- | --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 11:01:20 | 11:00:00 | BOOKS | 3 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 11:01:20 | 11:00:00 | BOOKS | 3 |'
- en: '| 11:02:15 | 11:01:00 | BOOKS | 1 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 11:02:15 | 11:01:00 | BOOKS | 1 |'
- en: Our stagger window is grouping on a one-minute interval. The window opens when
    we receive the first message for each product category. In the case of `BOOKS`,
    this is happening at a `ROWTIME` of 11:00:20\. The one-minute window expires at
    11:01:20\. When this happens, one record is emitted with the results that fall
    into this one-minute window (based on `ROWTIME` and `message_time`). The count
    in this example would be 3\. The fourth data record has a `message_time` outside
    of the one-minute window and is aggregated separately. This happens because `message_time`
    is specified in the partition key. For example, the partition key for `message_time`
    in the first window is 11:00.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的错开窗口正在按一分钟间隔进行分组。当我们接收到每个产品类别的第一条消息时，窗口就会打开。对于`BOOKS`，这发生在`ROWTIME`为11:00:20时。一分钟窗口在11:01:20到期。在此时，根据`ROWTIME`和`message_time`落入此一分钟窗口内的结果将被发出。例如，这个示例中的计数将为3。第四个数据记录的`message_time`在一分钟窗口外，将单独聚合。这是因为`message_time`在分区键中指定。例如，第一个窗口中`message_time`的分区键是11:00。
- en: Tumbling Windows
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滚动窗口
- en: Tumbling windows process the streaming data records in nonoverlapping windows
    and are best suited for distinct time-based windows that open and close at regular
    intervals. Here, each data record belongs to a specific window and is only processed
    once, as shown in [Figure 11-2](#tumbling_windows_process_the_streaming).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动窗口在非重叠的窗口中处理流式数据记录，最适合于在规则间隔打开和关闭的不同基于时间的窗口。在这里，每个数据记录属于特定的窗口，并且仅处理一次，如[图 11-2](#tumbling_windows_process_the_streaming)所示。
- en: 'Aggregation queries using the `GROUP BY` SQL clause process rows in a tumbling
    window:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`GROUP BY` SQL子句的聚合查询在滚动窗口中处理行：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](assets/dsaw_1102.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1102.png)'
- en: Figure 11-2\. Tumbling windows.
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-2\. 滚动窗口。
- en: In this example, the tumbling window is a time-based, one-minute window. We
    group the records by `ROWTIME`. The `STEP` function rounds down the `ROWTIME`
    to the nearest minute. Note that `STEP` can round values down to an arbitrary
    interval, whereas the `FLOOR` function can only round time values down to a whole-time
    unit, such as an hour, minute, or second.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，滚动窗口是基于时间的，为一分钟窗口。我们按`ROWTIME`对记录进行分组。`STEP`函数将`ROWTIME`向下舍入到最接近的分钟。请注意，`STEP`可以将值向下舍入到任意间隔，而`FLOOR`函数只能将时间值向下舍入到整数时间单位，如小时、分钟或秒。
- en: Sliding Windows
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动窗口
- en: Sliding windows aggregate data continuously using a fixed interval and fixed
    size. They continuously slide with time. We can create sliding windows with an
    explicit `WINDOW` clause instead of a `GROUP BY` clause and the interval can be
    time based or row based. Sliding windows can overlap, and a data record can be
    part of multiple windows. If a data record is part of multiple windows, the records
    get processed in each window, as shown in [Figure 11-3](#sliding_windows_aggregate_data_continuo).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口连续使用固定的间隔和固定的大小进行数据聚合。它们随时间连续滑动。我们可以使用显式的`WINDOW`子句而不是`GROUP BY`子句来创建滑动窗口，间隔可以是基于时间或行。滑动窗口可以重叠，数据记录可以属于多个窗口。如果数据记录属于多个窗口，则在每个窗口中处理记录，如[图 11-3](#sliding_windows_aggregate_data_continuo)所示。
- en: '![](assets/dsaw_1103.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1103.png)'
- en: Figure 11-3\. Sliding windows.
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. 滑动窗口。
- en: 'The following example creates a one-minute sliding window:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例创建一个一分钟的滑动窗口：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can also define sliding windows based on number of rows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以根据行数定义滑动窗口：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this example, we create a 2-row sliding window and a 10-row sliding window.
    The 2-row sliding window will overlap the 10-row sliding window. Such a scenario
    is useful if we calculate average metrics over different-sized record batches.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个 2 行滑动窗口和一个 10 行滑动窗口。2 行滑动窗口将重叠在 10 行滑动窗口上。如果我们计算不同大小记录批次的平均指标，这种情况非常有用。
- en: Now that we have a better understanding of how to work with windowed queries,
    let’s implement our online product reviews example using AWS.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对如何处理窗口查询有了更好的理解，让我们使用 AWS 实现我们的在线产品评论示例。
- en: Streaming Analytics and Machine Learning on AWS
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS 上的流式分析和机器学习
- en: We will use the Kinesis services to implement our online product reviews example.
    For simplicity, let’s assume the streaming team already parsed the social media
    feed messages and attached a unique review ID and the relevant product category
    to each message.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Kinesis 服务来实现我们的在线产品评论示例。为简单起见，假设流媒体团队已经解析了社交媒体反馈消息，并为每条消息附加了唯一的评论 ID
    和相关的产品类别。
- en: We begin with the ingest of those messages. We set up a Kinesis Data Firehose
    delivery stream, which receives the messages and continuously delivers them to
    an S3 location, as shown in the Ingest and Store Messages column in [Figure 11-4](#streaming_data_architecture_for_online).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这些消息的摄入开始。我们设置了 Kinesis Data Firehose 传递流，接收消息并将其连续传送到 S3 位置，如 [图 11-4](#streaming_data_architecture_for_online)
    中的摄入和存储消息列所示。
- en: We also want to enrich the messages with the customer sentiment. We can leverage
    our fine-tuned BERT-based model from the previous chapters to classify the messages
    into star ratings, as shown in the Detect Customer Sentiment column of [Figure 11-4](#streaming_data_architecture_for_online).
    The star rating will act as a proxy metric for sentiment. We can map a predicted
    star rating of 4 to 5 to a positive sentiment, the star rating of 3 to a neutral
    sentiment, and a star rating of 1 to 2 to a negative sentiment.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望通过客户情感来丰富消息。我们可以利用前几章中调优过的基于 BERT 的模型，将消息分类为星级评分，如 [图 11-4](#streaming_data_architecture_for_online)
    中的检测客户情感列所示。星级评分将作为情感的代理指标。我们可以将预测的星级评分 4 到 5 映射为积极情感，星级评分 3 映射为中性情感，星级评分 1 到
    2 映射为负面情感。
- en: '![](assets/dsaw_1104.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1104.png)'
- en: Figure 11-4\. Streaming data architecture for online product review messages.
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-4\. 在线产品评论消息的流数据架构。
- en: Next, we want to analyze our messages. We set up Kinesis Data Analytics to process
    our sentiment-enriched messages, as shown in the Analyze and Calculate Metrics
    column in [Figure 11-4](#streaming_data_architecture_for_online). Kinesis Data
    Analytics enables us to run SQL queries on streaming data. Kinesis Data Analytics
    SQL is based on the ANSI 2008 SQL standard with extensions to process streaming
    data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要分析我们的消息。我们设置了 Kinesis 数据分析来处理我们的情感丰富消息，如 [图 11-4](#streaming_data_architecture_for_online)
    中的分析和计算指标列所示。Kinesis 数据分析使我们能够在流数据上运行 SQL 查询。Kinesis 数据分析 SQL 基于 ANSI 2008 SQL
    标准，具有处理流数据的扩展功能。
- en: We define a SQL query that continuously calculates the average star rating to
    reflect the change in sentiment and push the results to a real-time metrics dashboard,
    as shown in the Visualize and Consume Metrics column in [Figure 11-4](#streaming_data_architecture_for_online).
    We define another SQL query that continuously calculates an anomaly score based
    on the message data to catch any unexpected schema or data values. For example,
    we suddenly receive a star rating of 100, which doesn’t exist. The application
    parsing the messages must have an error. In that case, we want to notify the team
    in charge to investigate the possible root cause and fix the issue. In a third
    SQL query, we continuously calculate an approximate count of messages that could
    be consumed by an application from the digital marketing team to evaluate and
    steer social media campaigns.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个 SQL 查询，持续计算平均星级评分以反映情感的变化，并将结果推送到实时指标仪表板，如 [图 11-4](#streaming_data_architecture_for_online)
    中的可视化和消费指标列所示。我们定义另一个 SQL 查询，持续根据消息数据计算异常分数，以捕获任何意外的模式或数据值。例如，我们突然接收到一个星级评分为 100，这是不存在的。解析消息的应用程序可能存在错误。在这种情况下，我们希望通知负责团队调查可能的根本原因并修复问题。在第三个
    SQL 查询中，我们持续计算可能被数字营销团队的应用程序消耗的消息的近似计数。
- en: The SQL queries run continuously over our data stream of incoming product review
    messages. We can define mini-batches of streaming data records via time-based
    or row-based windows. We can limit our SQL query to just those mini-batches (windows)
    of streaming data records when calculating averages and approximate counts for
    each batch. This type of SQL query is called a *windowed query*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 查询持续运行在我们的入站产品评审消息数据流上。我们可以通过基于时间或行的窗口定义流数据记录的小批处理。当计算每批次的平均值和近似计数时，我们可以将
    SQL 查询限制为仅针对这些小批处理（窗口）的流数据记录。这种类型的 SQL 查询称为*窗口查询*。
- en: Classify Real-Time Product Reviews with Amazon Kinesis, AWS Lambda, and Amazon
    SageMaker
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon Kinesis、AWS Lambda 和 Amazon SageMaker 对实时产品评审进行分类
- en: We set up a Kinesis Data Firehose delivery stream to receive and transform the
    real-time messages from our customers, as shown in [Figure 11-5](#receive_and_transform_data_records_wit).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了一个 Kinesis Data Firehose 传送流，用于接收和转换来自客户的实时消息，如[图 11-5](#receive_and_transform_data_records_wit)所示。
- en: '![](assets/dsaw_1105.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1105.png)'
- en: Figure 11-5\. Receive and transform data records with Kinesis Data Firehose.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-5\. 使用 Kinesis Data Firehose 接收和转换数据记录。
- en: We receive the real-time input data and predict the star rating to derive the
    customer sentiment. The sentiment can be used to quickly identify customers who
    might need our high-priority attention.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接收实时输入数据并预测星级评分，以推断客户情感。这种情感可以用来快速识别可能需要我们高优先级关注的客户。
- en: Kinesis Firehose allows us to transform our data records with the help of a
    Lambda function. We build a Lambda function that receives the Firehose data records
    and sends the review message to a SageMaker Endpoint that hosts our fine-tuned
    BERT-based model.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kinesis Firehose 允许我们通过 Lambda 函数来转换我们的数据记录。我们构建了一个 Lambda 函数，接收 Firehose 数据记录，并将评审消息发送到托管我们精调的基于
    BERT 模型的 SageMaker Endpoint。
- en: The model predicts the `star_rating`, which our Lambda function adds to the
    original data record; the function then returns the new record back to our Firehose
    delivery stream.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型预测`star_rating`，我们的 Lambda 函数将其添加到原始数据记录中；然后函数将新记录返回给我们的 Firehose 传送流。
- en: The Firehose delivery stream then delivers the transformed data records to an
    S3 bucket we specify. Kinesis Firehose also allows us to keep a backup of the
    original data records. We can deliver those backup data records to another S3
    bucket.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Firehose 传送流然后将转换后的数据记录传送到我们指定的一个 S3 存储桶。Kinesis Firehose 还允许我们备份原始数据记录。我们可以将这些备份数据记录传送到另一个
    S3 存储桶。
- en: Implement Streaming Data Ingest Using Amazon Kinesis Data Firehose
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon Kinesis Data Firehose 实现流数据摄取
- en: Kinesis Data Firehose is a fully managed service for delivering real-time streaming
    data to destinations such as Amazon S3, Redshift, Elasticsearch, or any custom
    HTTP endpoint.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Data Firehose 是一个完全托管的服务，用于将实时流数据传送到目的地，如 Amazon S3、Redshift、Elasticsearch
    或任何自定义 HTTP 终端。
- en: As a data source, we can select `DirectPut` or a Kinesis Data Stream. With `DirectPut`
    we can send data directly to the delivery stream or retrieve data from AWS IoT,
    CloudWatch Logs, or CloudWatch Events. We will choose `DirectPut` in our example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据源，我们可以选择`DirectPut`或 Kinesis 数据流。使用`DirectPut`，我们可以直接将数据发送到传送流，或从 AWS IoT、CloudWatch
    Logs 或 CloudWatch Events 检索数据。在我们的示例中，我们将选择`DirectPut`。
- en: Create Lambda Function to Invoke SageMaker Endpoint
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Lambda 函数以调用 SageMaker Endpoint
- en: Before we create our Kinesis Firehose delivery stream, we need to create the
    Lambda function to invoke our SageMaker Endpoint. Lambda functions help scale
    our Python code by providing a simple mechanism to dynamically increase or decrease
    the number of Python-based Lambda functions—each running its own Python interpreter—as
    the streaming load increases or decreases. This is similar in concept to scaling
    Python on a single instance by adding more processes and interpreters running
    on the instance. This auto-scaling feature of Lambda functions is similar to the
    auto-scaling feature of SageMaker Endpoints that we presented in [Chapter 9](ch09.html#deploy_models_to_production).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 Kinesis Firehose 传送流之前，我们需要创建 Lambda 函数以调用我们的 SageMaker Endpoint。Lambda
    函数通过提供一种简单的机制来动态增加或减少基于 Python 的 Lambda 函数的数量（每个都运行其自己的 Python 解释器），来帮助扩展我们的 Python
    代码。这类似于在单个实例上通过增加更多进程和解释器来扩展 Python。Lambda 函数的这种自动扩展功能类似于我们在[第 9 章](ch09.html#deploy_models_to_production)中展示的
    SageMaker Endpoint 的自动扩展功能。
- en: We create a Lambda function that receives data records from the Kinesis Firehose
    delivery stream. In addition to Kinesis metadata such as the `recordID`, each
    data record consists of the `review_id`, the `product_category`, and the actual
    `review_body`. We parse the `review_body` and send it to the specified SageMaker
    Endpoint. We receive the prediction result, add it to our data record, and return
    the modified data record with the original `recordID` to Kinesis Data Firehose.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个Lambda函数，从Kinesis Firehose传递流接收数据记录。除了Kinesis元数据如`recordID`外，每个数据记录包括`review_id`、`product_category`和实际的`review_body`。我们解析`review_body`并发送到指定的SageMaker
    Endpoint。我们接收预测结果，将其添加到我们的数据记录中，并将带有原始`recordID`的修改后数据记录返回给Kinesis数据Firehose。
- en: 'Following is an excerpt from the Python code for our Lambda function that invokes
    the SageMaker Endpoint as new data is pushed to the Kinesis Stream:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们Lambda函数的Python代码摘录，它在推送新数据到Kinesis流时调用SageMaker Endpoint：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can create the Lambda function directly in the AWS Console or programmatically
    using the Python SDK as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在AWS控制台中创建Lambda函数，或者使用Python SDK进行编程，如下所示：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can update the Lambda function with an environment variable referencing
    the SageMaker model endpoint to invoke:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用引用SageMaker模型端点的环境变量更新Lambda函数以调用：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can now create our Kinesis Data Firehose Delivery Stream.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建我们的Kinesis数据Firehose传递流。
- en: Create the Kinesis Data Firehose Delivery Stream
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Kinesis数据Firehose传递流
- en: We configure the delivery stream type as `DirectPut` so that we can put our
    product reviews directly on the stream. Also, to store the streaming data records,
    we define the `ExtendedS3DestinationConfiguration` pointing to the S3 bucket.
    We add the Lambda function, which calls the SageMaker Endpoint and adds the predicted
    star rating to our data in `ProcessingConfiguration`. We specify another S3 bucket
    in `S3BackupConfiguration` to back up the original product reviews (before transformation).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将传递流类型配置为`DirectPut`，这样我们可以直接将产品评价放在流上。此外，为了存储流数据记录，我们定义了指向S3存储桶的`ExtendedS3DestinationConfiguration`。我们在`ProcessingConfiguration`中添加调用SageMaker
    Endpoint并将预测星级评分添加到我们数据中的Lambda函数。我们在`S3BackupConfiguration`中指定另一个S3存储桶，用于备份转换前的原始产品评价。
- en: 'Here is the code to programmatically create the Kinesis Data Firehose delivery
    stream with all the above-mentioned configurations:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于通过编程方式创建具有上述所有配置的Kinesis数据Firehose传递流的代码：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We need to wait a few seconds for the delivery stream to become `active`. Then,
    we can put some live messages on our Kinesis Data Firehose delivery stream and
    see the results.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要等待几秒钟，直到传递流变为`active`状态。然后，我们可以将一些实时消息放在我们的Kinesis数据Firehose传递流上，并查看结果。
- en: Put Messages on the Stream
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将消息放在流上
- en: 'To simulate our continuous stream of online product review messages, we can
    read in our sample customer reviews from the Amazon Customer Reviews Dataset and
    send messages containing the `review_id`, `product_category`, and `review_body`
    to Kinesis Data Firehose as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟我们连续的在线产品评价消息流，我们可以从亚马逊客户评价数据集中读取我们的样本客户评价，并将包含`review_id`、`product_category`和`review_body`的消息发送到Kinesis数据Firehose，如下所示：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the messages arrive, Firehose calls `InvokeSageMakerEndpointFromKinesis`,
    the specified Lambda function, to transform the data. We can see the original
    message format, which contains the `review_id`, `product_category`, and `review_body`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦消息到达，Firehose调用`InvokeSageMakerEndpointFromKinesis`，指定的Lambda函数，以转换数据。我们可以看到原始消息格式，其中包含`review_id`、`product_category`和`review_body`：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Our Lambda function that parses the `review_body`, `"It's good"`, sends the
    `review_body` to the SageMaker Endpoint, receives the endpoint response, and decodes
    the `star_rating` prediction result of 5.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Lambda函数解析`review_body`，`"It's good"`，将`review_body`发送到SageMaker Endpoint，接收端点响应，并解码5的`star_rating`预测结果。
- en: 'In the last step, the Lambda function adds the star rating to the original
    data record and returns it back to Kinesis Data Firehose:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，Lambda函数将星级评分添加到原始数据记录中，并将其返回给Kinesis数据Firehose：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can also check the specified S3 bucket destination for Kinesis Data Firehose.
    Here, we should find the transformed data records:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查Kinesis数据Firehose指定的S3存储桶目的地。在这里，我们应该找到转换后的数据记录：
- en: 'And indeed, at *s3://<bucket>/kinesis-data-firehose/<year>/<month>/<day>/<hour>*
    we find a file with the following (shortened) output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，确实，在*s3://<bucket>/kinesis-data-firehose/<year>/<month>/<day>/<hour>*我们找到了一个包含以下（缩短的）输出的文件。
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'These are our transformed data records. We also configured Firehose to back
    up our source data records. Similarly, we can check the S3 bucket we specified
    for the backup:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们转换后的数据记录。我们还配置了 Firehose 来备份我们的源数据记录。类似地，我们可以检查为备份指定的 S3 存储桶：
- en: '*s3://<bucket>/kinesis-data-firehose-source-record/<year>/<month>/<day>/<hour>*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*s3://<bucket>/kinesis-data-firehose-source-record/<year>/<month>/<day>/<hour>*'
- en: 'And we find another file with the source records similar to this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到另一个与此类似的源记录文件：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note the missing star rating. The star rating is missing here, as this is the
    originally received product review message. This data represents the product review
    message before we invoked our BERT-based model (via the Lambda function) to predict
    and add the star rating to the streaming data record. We keep this original data
    as a backup.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意缺失的星级评分。这里缺少星级评分，因为这是最初收到的产品评论消息。这些数据表示我们在调用基于 BERT 模型的 Lambda 函数（用于预测并添加星级评分到流数据记录中）之前的产品评论消息。我们保留这些原始数据作为备份。
- en: This shows that the streaming data ingest and data transformation with Kinesis
    Data Firehose works. Now let’s move on to the next step.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明使用 Kinesis 数据 Firehose 进行流数据摄取和数据转换是有效的。现在让我们进入下一步。
- en: Summarize Real-Time Product Reviews with Streaming Analytics
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流分析总结实时产品评论
- en: The first business metric we want to continuously calculate is the average sentiment
    per product category. We could push the results to a real-time metrics dashboard.
    In our sample implementation, we will publish the average star rating (as a proxy
    metric for sentiment) to Amazon CloudWatch. The LOB owners can now detect sentiment
    trends quickly and take action.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要持续计算的第一个业务指标是每个产品类别的平均情感值。我们可以将结果推送到实时指标仪表板。在我们的示例实现中，我们将平均星级评分（作为情感的代理指标）发布到亚马逊
    CloudWatch。业务部门现在可以快速检测情感趋势并采取行动。
- en: Another business metric we continuously calculate is an anomaly score based
    on the message data to catch any unexpected schema or data values. In case of
    an application error, we want to notify the team in charge to investigate the
    possible root cause and fix it fast. For our implementation, we will use the Amazon
    Simple Notification Service (Amazon SNS) to send the calculated anomaly scores
    via email. Amazon SNS is a fully managed service to send SMS, email, and mobile
    push notifications.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们持续计算的业务指标是基于消息数据计算的异常分数，以捕捉任何意外的模式或数据值。在应用程序错误的情况下，我们希望通知负责团队调查可能的根本原因并快速修复。在我们的实现中，我们将使用亚马逊简单通知服务（Amazon
    SNS）通过电子邮件发送计算的异常分数。亚马逊 SNS 是一个完全托管的服务，用于发送短信、电子邮件和移动推送通知。
- en: As a last metric, we continuously calculate an approximate count of product
    review messages that can be consumed by the digital marketing team to evaluate
    and steer online campaigns. For our implementation, we will deliver the approximate
    count as a stream of continuous records to a Kinesis Data Stream. The digital
    marketing team could develop a custom application that reads the data records
    off the Kinesis Data Stream and processes the records as needed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一个指标，我们持续计算产品评论消息的近似计数，这些计数可以供数字营销团队使用，以评估和引导在线活动。在我们的实施中，我们将近似计数作为连续记录流传送到
    Kinesis 数据流。数字营销团队可以开发一个自定义应用程序，从 Kinesis 数据流中读取数据记录并根据需要处理记录。
- en: '[Figure 11-6](#analyze_a_continuous_stream_of_product) shows our evolved streaming
    data use case implementation.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-6](#analyze_a_continuous_stream_of_product) 展示了我们演进的流数据用例实现。'
- en: '![](assets/dsaw_1106.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1106.png)'
- en: Figure 11-6\. Analyze a continuous stream of product review messages with Kinesis
    Data Analytics.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. 使用 Kinesis 数据分析分析连续流的产品评论消息。
- en: Setting Up Amazon Kinesis Data Analytics
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置亚马逊 Kinesis 数据分析
- en: We will set up a Kinesis Data Analytics application to analyze our product review
    messages. Kinesis Data Analytics enables us to run SQL queries on streaming data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将建立一个 Kinesis 数据分析应用程序来分析我们的产品评论消息。Kinesis 数据分析使我们能够在流数据上运行 SQL 查询。
- en: We will use the Kinesis Data Firehose delivery stream as an input source for
    the Kinesis Data Analytics application. We will then develop a Kinesis Data Analytics
    application to execute SQL queries to calculate the average sentiment of the incoming
    messages, the anomaly score, and the approximate count.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Kinesis 数据 Firehose 交付流作为 Kinesis 数据分析应用程序的输入源。然后我们将开发一个 Kinesis 数据分析应用程序来执行
    SQL 查询，计算传入消息的平均情感、异常分数和近似计数。
- en: Similar to Kinesis Data Firehose, we have the option to preprocess the incoming
    streaming data. We will reuse our existing Lambda function to invoke the SageMaker
    Endpoint and receive the star rating for our incoming messages. The star rating
    will act again as our proxy metric for sentiment.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与Kinesis Firehose类似，我们有选项对传入的流数据进行预处理。我们将重用现有的Lambda函数来调用SageMaker端点并接收我们传入消息的星级评分。星级评分将再次作为我们的情感代理度量。
- en: Note
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why not reuse the transformed data records from Kinesis Firehose that already
    contain the star rating? Those transformed records get delivered straight to the
    S3 destination bucket. We only receive the source data records from the Firehose
    delivery stream in Kinesis Data Analytics.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不重用已经包含星级评分的来自Kinesis Firehose的转换数据记录？这些转换后的记录直接传送到S3目标存储桶。我们在Kinesis Data
    Analytics中只接收来自Firehose传送流的源数据记录。
- en: Kinesis Data Analytics supports various destinations to send the analytics results
    to. We will set up two Lambda functions and an Kinesis Data Stream as destinations.
    We can leverage the Lambda functions to integrate with Amazon CloudWatch and Amazon
    SNS. Let’s implement the needed components for this architecture, starting with
    the destinations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis数据分析支持将分析结果发送到各种目的地。我们将设置两个Lambda函数和一个Kinesis数据流作为目的地。我们可以利用Lambda函数与Amazon
    CloudWatch和Amazon SNS集成。让我们从目的地开始实现这种架构所需的组件。
- en: Create a Kinesis Data Stream to Deliver Data to a Custom Application
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个Kinesis数据流以将数据传送到自定义应用程序
- en: Kinesis Data Streams are used to ingest large amounts of data in real time,
    store the data, and make the data available to consumer applications. The unit
    of data stored by Kinesis Data Streams is a data record. A data stream represents
    a group of data records. The data records in a data stream are distributed into
    shards. A shard has a sequence of data records in a stream. When we create a stream,
    we specify the number of shards for the stream. The total capacity of a stream
    is the sum of the capacities of its shards. We can increase or decrease the number
    of shards in a stream as needed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis数据流用于实时摄取大量数据、存储数据并使数据可用于消费者应用程序。Kinesis数据流存储的数据单元是数据记录。数据流代表一组数据记录。数据流中的数据记录分布在分片中。每个分片都有数据流中的数据记录序列。创建数据流时，需要指定数据流的分片数量。数据流的总容量是其分片容量之和。根据需要可以增加或减少数据流中的分片数量。
- en: In the context of streaming data, we often speak of producers and consumers.
    A producer is an application or service that generates data. A consumer is an
    application or service that receives the streaming data for further processing.
    [Figure 11-7](#the_kinesis_data_stream_architecture_co) shows the high-level architecture
    of a Kinesis Data Stream.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在流数据的上下文中，我们经常谈论生产者和消费者。生产者是生成数据的应用程序或服务。消费者是接收流数据以进一步处理的应用程序或服务。[图 11-7](#the_kinesis_data_stream_architecture_co)显示了Kinesis数据流的高级架构。
- en: '![](assets/dsaw_1107.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1107.png)'
- en: Figure 11-7\. The Kinesis Data Stream architecture consists of data producers
    and consumers with data records distributed into shards.
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-7\. Kinesis数据流架构由数据生产者和消费者组成，数据记录分布在分片中。
- en: Note that data is only stored temporarily in Kinesis Data Streams. The default
    data retention period for a Kinesis Data Stream is currently limited to 24 hours.
    However, we can increase the data retention period for long-term retention of
    up to one year. A longer retention period can help address compliance requirements
    without the need to move the data to longer-term storage like S3\. The higher
    retention time helps in back-pressure scenarios as well, where the consumers cannot
    keep up with the producers during an unexpected increase in data pushed to the
    stream. In this case, Kinesis stores the streaming data until the consumers scale
    out to handle the spike—or the volume of data decreases and the consumers can
    catch up. A longer retention period also allows us to train models more quickly
    with online data directly from Kinesis—or combine the online Kinesis data with
    offline data in S3.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在Kinesis数据流中数据只会暂时存储。Kinesis数据流的默认数据保留期目前限制为24小时。但是，我们可以增加数据保留期长达一年以实现长期保留。更长的保留期可以帮助满足合规要求，无需将数据移动到像S3这样的长期存储中。较长的保留期还有助于在背压场景下，即使在意外数据推送量增加时，消费者无法跟上生产者的情况下，Kinesis也可以存储流数据。在这种情况下，Kinesis将存储流式数据，直到消费者扩展以处理高峰量，或者数据量减少并且消费者可以追赶上来。较长的保留期还允许我们使用从Kinesis直接获取的在线数据更快地训练模型，或者将在线Kinesis数据与S3中的离线数据结合使用。
- en: In our example, the Kinesis Data Stream will receive the results from our approximate
    count of messages, hence the producer is the Kinesis Data Analytics application.
    The consumer could be any custom application. In our example, we suggested an
    application from the digital marketing team. [Figure 11-8](#a_kinesis_data_stream_is_used_as_the_ki)
    highlights the current step in our architecture that we are about to implement.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，Kinesis数据流将接收来自我们消息近似计数的结果，因此生产者是Kinesis数据分析应用程序。消费者可以是任何自定义应用程序。在我们的示例中，我们建议使用来自数字营销团队的应用程序。[图11-8](#a_kinesis_data_stream_is_used_as_the_ki)突出了我们即将实施的架构当前步骤。
- en: '![](assets/dsaw_1108.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1108.png)'
- en: Figure 11-8\. A Kinesis Data Stream is used as the Kinesis Data Analytics destination
    for the approximate count.
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8\. Kinesis数据流用作Kinesis数据分析的近似计数目标。
- en: 'Here’s the code to create the Kinesis Data Stream:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建Kinesis数据流的代码：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We need to wait a few minutes for the Kinesis Data Stream to become `active`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要等待几分钟，让Kinesis数据流变为`active`。
- en: 'We can programmatically check for the status of the stream and wait for the
    stream to become `active` with the following code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码编程地检查流的状态，并等待流变为`active`：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, let’s create an Lambda function that acts as the Kinesis Data Analytics
    destination for our anomaly score.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个Lambda函数，作为我们异常分数的Kinesis数据分析目标。
- en: Create AWS Lambda Function to Send Notifications via Amazon SNS
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建AWS Lambda函数以通过Amazon SNS发送通知
- en: In our Kinesis Data Analytics application, we will calculate an anomaly score
    for the data. In case the anomaly score rises, we want to notify the application
    developers to investigate and fix the issue. To send out notifications, we leverage
    Amazon SNS. We will send an email to the team in charge with the latest anomaly
    score calculated across our incoming messages.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的Kinesis数据分析应用程序中，我们将为数据计算异常分数。如果异常分数升高，我们希望通知应用程序开发人员进行调查和修复。为了发送通知，我们利用Amazon
    SNS。我们将向负责团队发送一封包含我们接收到的最新异常分数的电子邮件。
- en: As Amazon SNS is not directly supported as a Kinesis Data Analytics destination,
    we create another Lambda function as a proxy destination. [Figure 11-9](#an_aws_lambda_function_is_used_as_the)
    highlights the step in our architecture that we are about to implement.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Amazon SNS不直接支持作为Kinesis数据分析的目标，我们创建另一个Lambda函数作为代理目标。[图11-9](#an_aws_lambda_function_is_used_as_the)突出了我们即将实施的架构步骤。
- en: '![](assets/dsaw_1109.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1109.png)'
- en: Figure 11-9\. An Lambda function is used as the Kinesis Data Analytics destination
    for the anomaly score.
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-9\. Lambda函数用作Kinesis数据分析的异常分数目标。
- en: 'Here is the code to create our Amazon SNS topic:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建我们的Amazon SNS主题的代码：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Following is an excerpt from our Lambda function code, *push_notification_to_sns.py*,
    which records the highest anomaly score from the batch of input records and publishes
    the score to an Amazon SNS topic:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们Lambda函数代码的摘录，*push_notification_to_sns.py*，该函数记录了批处理输入记录中的最高异常分数，并将分数发布到Amazon
    SNS主题：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Similar to the previous Lambda function, we can create this Lambda function
    programmatically and update the function with an environment variable set to our
    Amazon SNS Topic ARN.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的 Lambda 函数类似，我们可以通过编程方式创建此 Lambda 函数，并更新该函数，其中环境变量设置为我们的 Amazon SNS 主题 ARN。
- en: 'We can subscribe to the Amazon SNS topic to receive the Amazon SNS notifications
    as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以订阅 Amazon SNS 主题以接收 Amazon SNS 通知如下：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have one more Lambda function to implement.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个 Lambda 函数要实现。
- en: Create AWS Lambda Function to Publish Metrics to Amazon CloudWatch
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 AWS Lambda 函数以将指标发布到 Amazon CloudWatch
- en: In our Kinesis Data Analytics application, we will also calculate the average
    sentiment over windows of streaming messages. We want to publish the average sentiment
    results as a custom metric to CloudWatch. Again, we will use the star rating as
    our proxy metric for sentiment. As CloudWatch is not directly supported as a Kinesis
    Data Analytics destination, we need another Lambda function as a proxy destination.
    [Figure 11-10](#an_aws_lambda_function_is_used_as_the_k) highlights the step in
    our architecture that we are about to implement.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Kinesis Data Analytics 应用程序中，我们还将计算流消息窗口的平均情感。我们希望将平均情感结果作为自定义指标发布到 CloudWatch。同样，我们将使用星级评分作为我们的情感代理指标。由于
    CloudWatch 不直接支持作为 Kinesis Data Analytics 目的地，我们需要另一个 Lambda 函数作为代理目的地。[图 11-10](#an_aws_lambda_function_is_used_as_the_k)
    强调了我们即将实施的架构步骤。
- en: '![](assets/dsaw_1110.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1110.png)'
- en: Figure 11-10\. An Lambda function is used as the Kinesis Data Analytics destination
    for average star rating.
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-10\. 用作 Kinesis Data Analytics 平均星级评分的 Lambda 函数目的地。
- en: 'Following is an excerpt from our Lambda function code, *deliver_metrics_to_cloudwatch.py*,
    to publish the average star rating as a custom metric to CloudWatch:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们 Lambda 函数代码 *deliver_metrics_to_cloudwatch.py* 的摘录，用于将平均星级评分作为自定义指标发布到
    CloudWatch：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After we create the Lambda function, we have all Kinesis Data Analytics application
    destinations in place and can now create the Kinesis Data Analytics application.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Lambda 函数后，我们已经安装了所有 Kinesis Data Analytics 应用程序的目的地，并且现在可以创建 Kinesis Data
    Analytics 应用程序。
- en: Transform Streaming Data in Kinesis Data Analytics
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换 Kinesis Data Analytics 中的流数据
- en: Similar to the data transformation feature in Kinesis Data Firehose, we can
    transform the incoming streaming data in Kinesis Data Analytics. We can use an
    Lambda function to transform, convert, enrich, or filter our streaming data. This
    step is executed before the Data Analytics application creates a schema for the
    data stream. In our example, we will reuse the Lambda function we created for
    the Kinesis Data Firehose data transformation. We will use the function to enrich
    our messages again with the star rating. [Figure 11-11](#preprocess_streaming_data_in_kinesis_da)
    visualizes the details of this step.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Kinesis Data Firehose 中的数据转换功能类似，我们可以在 Kinesis Data Analytics 中转换传入的流数据。我们可以使用
    Lambda 函数来转换、转换、丰富或过滤我们的流数据。此步骤在数据分析应用程序为数据流创建架构之前执行。在我们的示例中，我们将重用为 Kinesis Data
    Firehose 数据转换创建的 Lambda 函数。我们将再次使用该函数来再次用星级评分丰富我们的消息。[图 11-11](#preprocess_streaming_data_in_kinesis_da)
    可视化了此步骤的详细信息。
- en: '![](assets/dsaw_1111.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1111.png)'
- en: Figure 11-11\. Preprocess streaming data in Kinesis Data Analytics.
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-11\. 在 Kinesis Data Analytics 中预处理流数据。
- en: 'The workflow looks as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流如下所示：
- en: We receive the product review messages on the Kinesis Data Firehose delivery
    stream, which delivers the records to S3.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 Kinesis Data Firehose 交付流上接收产品评审消息，该流将记录传递到 S3。
- en: We set up the Kinesis Data Analytics application with the Firehose delivery
    stream as the input stream. The application receives the product review messages
    from the Firehose delivery stream and sends them to a Lambda function for preprocessing.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了 Kinesis Data Analytics 应用程序，以 Firehose 交付流作为输入流。该应用程序从 Firehose 交付流接收产品评审消息，并将其发送到
    Lambda 函数进行预处理。
- en: We are reusing the Lambda function `InvokeSageMakerEndpointFromKinesis`, which
    invokes the BERT-based model hosted on a SageMaker Endpoint to predict the star
    rating based on the review text in our product review message.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们正在重用 Lambda 函数`InvokeSageMakerEndpointFromKinesis`，该函数调用托管在 SageMaker Endpoint
    上的基于 BERT 的模型，以预测基于产品评审消息中的评审文本的星级评分。
- en: The Lambda function receives the predicted star rating from our model and attaches
    it to the product review message.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lambda 函数从我们的模型接收预测的星级评分，并将其附加到产品评审消息中。
- en: The Lambda function returns the product review message enriched with the star
    rating to the Kinesis Data Analytics application. The enriched product review
    messages are now used as the input for all subsequent SQL queries.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lambda 函数将产品评论消息返回到 Kinesis 数据分析应用程序，并将其与星级评分一起丰富。丰富的产品评论消息现在用作所有后续 SQL 查询的输入。
- en: As we already have the Lambda function in place, we can continue to develop
    the SQL queries for our application.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有了 Lambda 函数，我们可以继续为我们的应用程序开发 SQL 查询。
- en: Understand In-Application Streams and Pumps
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解应用程序流和泵
- en: An important concept in Kinesis Data Analytics applications is in-application
    streams and pumps. In our example, we will use the Firehose delivery stream as
    an input to our Data Analytics application. This input stream needs to be mapped
    to an in-application stream in the Data Analytics application. Once the mapping
    is done, the data continuously flows from the input stream into the in-application
    stream. We can think of the in-application stream as a table that we can then
    query using SQL statements. As we are not really dealing with a table, but with
    a continuous data flow, we call it a stream.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 数据分析应用程序中的一个重要概念是应用程序流和泵。在我们的示例中，我们将使用 Firehose 传输流作为数据分析应用程序的输入。这个输入流需要映射到数据分析应用程序中的应用程序流。一旦映射完成，数据将持续从输入流流入应用程序流中。我们可以将应用程序流视为一个表，然后可以使用
    SQL 语句查询它。由于我们实际上不是在处理表，而是在处理连续的数据流，所以我们称其为流。
- en: Note that Kinesis Data Analytics in-application streams only exist within the
    analytics application. They store the intermediate results of our SQL query. If
    we want to process the results outside of the application, we need to map the
    in-application stream to a supported Kinesis Data Analytics destination. Therefore,
    we set up three different destinations to capture the results of our in-application
    streams.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Kinesis 数据分析中的应用程序流仅存在于分析应用程序内部。它们存储我们 SQL 查询的中间结果。如果我们希望在应用程序外部处理结果，我们需要将应用程序流映射到支持的
    Kinesis 数据分析目的地。因此，我们设置了三个不同的目的地来捕获我们应用程序流的结果。
- en: 'Here is an example for how to create an in-application stream (`MY_STREAM`)
    with three columns:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建具有三列的应用程序流（`MY_STREAM`）的示例：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To insert data into this stream, we need a pump. Think of a pump as a continuously
    running insert query that inserts data from one in-application stream into another
    in-application stream.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要向此流插入数据，我们需要一个泵。将泵视为一个持续运行的插入查询，将数据从一个应用程序流插入到另一个应用程序流。
- en: 'Here’s an example that creates a pump (`MY_PUMP`) and inserts data into `MY_STREAM`
    by selecting data records from another `INPUT_STREAM`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，创建了一个泵（`MY_PUMP`），并通过从另一个 `INPUT_STREAM` 中选择数据记录将数据插入到 `MY_STREAM` 中：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s assume that the input stream (our Firehose delivery stream) in our Data
    Analytics application is called `SOURCE_SQL_STREAM_001`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在数据分析应用程序中的输入流（我们的 Firehose 传输流）称为 `SOURCE_SQL_STREAM_001`。
- en: Amazon Kinesis Data Analytics Applications
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊 Kinesis 数据分析应用程序
- en: Let’s create three in-application streams to calculate the average `star_rating`,
    anomaly score, and approximate count of messages.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建三个应用程序流来计算平均 `star_rating`、异常分数和消息的近似计数。
- en: Calculate Average Star Rating
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算平均星级评分
- en: Our first in-application stream is called `AVG_STAR_RATING_SQL_STREAM`. We calculate
    the average star rating over a five-second tumbling window of received messages
    using the `GROUP BY` statement, which specifies `INTERVAL ‘5’`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个应用程序流名为 `AVG_STAR_RATING_SQL_STREAM`。我们使用 `GROUP BY` 语句计算接收到的消息的五秒滚动窗口内的平均星级评分，指定了
    `INTERVAL ‘5’`。
- en: 'Here is the SQL code to implement this:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现此目的的 SQL 代码：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Detect Anomalies in Streaming Data
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测流数据中的异常
- en: The second in-application stream is called `ANOMALY_SCORE_SQL_STREAM`. We leverage
    a built-in `RANDOM_CUT_FOREST` implementation to calculate an anomaly score across
    a sliding window of messages.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个应用程序流名为 `ANOMALY_SCORE_SQL_STREAM`。我们利用内置的 `RANDOM_CUT_FOREST` 实现，在消息的滑动窗口中计算异常分数。
- en: The random cut forest (RCF) implementation in Kinesis Data Analytics is based
    on the [“Robust Random Cut Forest Based Anomaly Detection on Streams” research
    paper](https://oreil.ly/0pDkv), coauthored by AWS. The paper details using RCF
    for online learning with real-time data streams. However, AWS offers RCF for offline
    batch training using a built-in SageMaker algorithm. RCF is also used for anomaly
    detection in QuickSight.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 数据分析中的随机切割森林 (RCF) 实现基于由 AWS 共同撰写的 [“Robust Random Cut Forest Based
    Anomaly Detection on Streams” 研究论文](https://oreil.ly/0pDkv)。该论文详细介绍了在实时数据流中使用
    RCF 进行在线学习。但是，AWS 提供了用于离线批量训练的内置 SageMaker 算法。RCF 还用于 QuickSight 中的异常检测。
- en: The `RANDOM_CUT_FOREST` function in Kinesis Data Analytics builds a machine
    learning model to calculate an anomaly score for numeric values in each message.
    The score indicates how different the value is compared to the observed trend.
    The function also calculates an attribution score for each column, which reflects
    how anomalous the data in that particular column is. The sum of all attribution
    scores of all columns is the overall anomaly score.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 数据分析中的 `RANDOM_CUT_FOREST` 函数构建一个机器学习模型，用于计算每条消息中数值的异常分数。该分数表示该值与观察到的趋势相比有多不同。该函数还计算每列的归因分数，反映了该特定列中数据的异常程度。所有列的所有归因分数之和即为总体异常分数。
- en: 'As `RANDOM_CUT_FOREST` works on numeric values, we will calculate the anomaly
    score based on the star rating. The only required parameter for the `RANDOM_CUT_FOREST`
    function is a pointer to our input stream, which we define with the `CURSOR` function.
    Here is the SQL code to implement this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `RANDOM_CUT_FOREST` 是基于数值值的，我们将根据星级评分计算异常分数。`RANDOM_CUT_FOREST` 函数的唯一必需参数是指向我们输入流的指针，我们使用
    `CURSOR` 函数定义这一点。以下是实现此操作的 SQL 代码：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Calculate Approximate Counts of Streaming Data
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算流数据的近似计数
- en: The third in-application stream is called `APPROXIMATE_COUNT_SQL_STREAM`. We
    calculate an approximate count over a five-second tumbling window of incoming
    messages. Kinesis Data Analytics has a built-in function to calculate an approximate
    count using `COUNT_DISTINCT_ITEMS_TUMBLING`, with the tumbling window size set
    to five seconds. The function uses the HyperLogLog algorithm, which stores a large
    number of approximate counts in a small data structure.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个应用程序内流被称为 `APPROXIMATE_COUNT_SQL_STREAM`。我们在五秒滚动窗口内计算传入消息的近似计数。Kinesis 数据分析具有内置函数，使用
    `COUNT_DISTINCT_ITEMS_TUMBLING` 计算近似计数，滚动窗口大小设置为五秒。该函数使用 HyperLogLog 算法，该算法在小型数据结构中存储大量近似计数。
- en: 'The following SQL code implements the approximate count of distinct items of
    the `review_id` column over a five-second tumbling window:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 SQL 代码实现了在五秒滚动窗口上对 `review_id` 列的近似不同项计数：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Create Kinesis Data Analytics Application
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Kinesis 数据分析应用程序
- en: 'We are now fully equipped to create our Kinesis Data Analytics application,
    so let’s first create a combined SQL statement that contains our three SQL queries
    to calculate the average star rating, detect anomalies, and calculate the approximate
    count of streaming data over a given window size. We pass this combined SQL query
    as the `ApplicationCode` when we create the application. Here is the code:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完全准备好创建我们的 Kinesis 数据分析应用程序，所以让我们首先创建一个联合的 SQL 语句，其中包含我们的三个 SQL 查询，以计算平均星级评分、检测异常以及计算给定窗口大小下的流数据的近似计数。当我们创建应用程序时，我们将这个联合
    SQL 查询作为 `ApplicationCode` 传递。以下是代码：
- en: '[PRE25]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, let’s create the Kinesis Data Analytics application. We set the application
    input to our Firehose delivery stream and configure the `InputProcessingConfiguration`
    to call our Lambda function invoking the BERT-based model. We then define the
    `InputSchema` to match our enriched product review messages with `review_id`,
    `star_rating`, `product_category`, and `review_body`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建 Kinesis 数据分析应用程序。我们将应用程序输入设置为我们的 Firehose 传递流，并配置 `InputProcessingConfiguration`
    调用我们的 Lambda 函数来调用基于 BERT 模型的数据。然后，我们定义 `InputSchema` 来匹配我们丰富的产品评论消息，其中包括 `review_id`、`star_rating`、`product_category`
    和 `review_body`。
- en: 'For the application outputs, we reference the in-application stream names of
    our three SQL queries and define the destinations. We set the destinations `AVG_STAR_RATING_SQL_STREAM`
    and `ANOMALY_SCORE_SQL_STREAM` to the corresponding Lambda functions. We connect
    the `APPROXIMATE_COUNT_SQL_STREAM` to the Kinesis Data Stream destination. Here
    is the code that creates the Kinesis Data Application and references the `sql_code`
    defined earlier:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应用程序的输出，我们引用了三个 SQL 查询的应用程序内流名称，并定义了目标。我们将目标 `AVG_STAR_RATING_SQL_STREAM`
    和 `ANOMALY_SCORE_SQL_STREAM` 设置为相应的 Lambda 函数。我们将 `APPROXIMATE_COUNT_SQL_STREAM`
    连接到 Kinesis 数据流目标。以下是创建 Kinesis 数据应用程序并引用先前定义的 `sql_code` 的代码：
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Start the Kinesis Data Analytics Application
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动 Kinesis 数据分析应用程序。
- en: 'After creating a Kinesis Data Analytics application, we have to explicitly
    start the application to receive and process the data. Here is the code to start
    our Kinesis Data Analytics application:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Kinesis 数据分析应用程序后，我们必须显式启动应用程序以接收和处理数据。以下是启动我们的 Kinesis 数据分析应用程序的代码：
- en: '[PRE27]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Put Messages on the Stream
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将消息放入流中。
- en: Once the application is running, we can test our streaming pipeline by putting
    messaging onto the stream. In order to simulate our continuous stream of online
    product review messages, we reuse our code from earlier. We read in our sample
    customer reviews from the Amazon Customer Reviews Dataset and send messages containing
    the `review_id`, `product_category`, and `review_body` to Kinesis Data Firehose.
    Our Kinesis Data Analytics application is configured to use the Firehose delivery
    stream as an input source.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序运行后，我们可以通过将消息放入流中来测试我们的流水线。为了模拟我们连续的在线产品评论消息流，我们重复使用之前的代码。我们从亚马逊客户评论数据集中读取我们的示例客户评论，并将包含
    `review_id`、`product_category` 和 `review_body` 的消息发送到 Kinesis Data Firehose。我们的
    Kinesis 数据分析应用程序配置为使用 Firehose 交付流作为输入源。
- en: Let’s review the results from our Data Analytics application. If we open the
    Kinesis Data Analytics application in the AWS console, we can see the source and
    destination configurations, as shown in Figures [11-12](#kinesis_data_analytics_applicationcomm)
    and [11-13](#kinesis_data_analytics_applicationcomma).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们审查一下我们的数据分析应用程序的结果。如果我们在 AWS 控制台中打开 Kinesis 数据分析应用程序，我们可以看到源和目标配置，如 [11-12](#kinesis_data_analytics_applicationcomm)
    和 [11-13](#kinesis_data_analytics_applicationcomma) 所示。
- en: '![](assets/dsaw_1112.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1112.png)'
- en: Figure 11-12\. Kinesis Data Analytics application, source configuration.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 11-12\. Kinesis 数据分析应用程序，源配置。
- en: The Firehose delivery stream gets mapped to the in-application stream `SOURCE_SQL_STREAM_001`.
    We also perform preprocessing of our input records with the Lambda function `InvokeSageMakerEndpointFromKinesis`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Firehose 交付流映射到应用程序内流 `SOURCE_SQL_STREAM_001`。我们还使用 Lambda 函数 `InvokeSageMakerEndpointFromKinesis`
    对输入记录进行预处理。
- en: '![](assets/dsaw_1113.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1113.png)'
- en: Figure 11-13\. Kinesis Data Analytics application, destination configuration.
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 11-13\. Kinesis 数据分析应用程序，目标配置。
- en: The destination configuration shows the correct mapping from our three in-application
    streams `AVG_STAR_RATING_SQL_STREAM`, `ANOMALY_SCORE_SQL_STREAM`, and `APPROXIMATE_COUNT_SQL_STREAM`
    to their corresponding destinations.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 目标配置显示了我们的三个应用程序内流 `AVG_STAR_RATING_SQL_STREAM`、`ANOMALY_SCORE_SQL_STREAM` 和
    `APPROXIMATE_COUNT_SQL_STREAM` 与其相应目标的正确映射。
- en: From that console, we can also open the real-time analytics dashboard to see
    our SQL query execution results as messages arrive. If we select the `Source`
    tab, we can see the incoming messages, as shown in [Figure 11-14](#input_stream_of_messages).
    The messages are already preprocessed by our Lambda function and contain the star
    rating.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从该控制台，我们还可以打开实时分析仪表板，查看 SQL 查询执行结果作为消息到达。如果我们选择 `Source` 标签，我们可以看到传入的消息，如 [Figure 11-14](#input_stream_of_messages)
    所示。这些消息已经通过我们的 Lambda 函数预处理，并包含星级评分。
- en: '![](assets/dsaw_1114.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1114.png)'
- en: Figure 11-14\. Input stream of messages.
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 11-14\. 输入消息流。
- en: If we select the Real-time analytics tab, we can see the results of our three
    in-application streams, including average star ratings, number of distinct items,
    and anomaly scores, as shown in [Figure 11-15](#in_application_stream_results_for_anoma).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择实时分析标签，我们可以看到我们的三个应用程序内流的结果，包括平均星级评分、不同项数和异常分数，如 [Figure 11-15](#in_application_stream_results_for_anoma)
    所示。
- en: '![](assets/dsaw_1115.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1115.png)'
- en: Figure 11-15\. In-application stream results for `ANOMALY_SCORE_SQL_STREAM`.
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 11-15\. `ANOMALY_SCORE_SQL_STREAM` 的应用程序内流结果。
- en: Finally, let’s review our destinations. If we navigate to CloudWatch Metrics,
    we can find our custom metric `AVGStarRating`. We can add the metric to a graph
    and see the real-time sentiment trend of incoming messages. Our Amazon SNS topic
    also received the latest anomaly score and notified the application team via email.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们回顾一下我们的目标地点。如果我们导航到 CloudWatch Metrics，我们可以找到我们的自定义指标 `AVGStarRating`。我们可以将该指标添加到图表中，查看传入消息的实时情感趋势。我们的
    Amazon SNS 主题还接收到了最新的异常分数，并通过电子邮件通知了应用团队。
- en: Classify Product Reviews with Apache Kafka, AWS Lambda, and Amazon SageMaker
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Kafka、AWS Lambda 和 Amazon SageMaker 对产品评价进行分类
- en: 'Amazon MSK is a fully managed service for an Apache Kafka distributed streaming
    cluster. We can create a Lambda function to invoke our SageMaker Endpoint using
    data from the Amazon MSK stream as prediction input and enrich our Kafka stream
    with the prediction output. This is similar to how our Kinesis stream triggered
    a Lambda function that invoked our SageMaker Endpoint with data from the Kinesis
    stream as prediction input and enriched our Kinesis stream with the prediction
    output. [Figure 11-16](#receive_and_transform_data_records_wi) shows how to receive
    and transform data records with Amazon MSK, and we can describe the steps as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon MSK 是用于 Apache Kafka 分布式流处理集群的全管理服务。我们可以创建一个 Lambda 函数，使用 Amazon MSK
    流中的数据调用我们的 SageMaker 终端节点进行预测输入，并用预测输出丰富我们的 Kafka 流。这类似于我们的 Kinesis 流触发 Lambda
    函数，Lambda 函数调用 SageMaker 终端节点，使用 Kinesis 流中的数据作为预测输入，并用预测输出丰富 Kinesis 流。[图 11-16](#receive_and_transform_data_records_wi)
    展示了如何使用 Amazon MSK 接收和转换数据记录，我们可以描述步骤如下：
- en: '![](assets/dsaw_1116.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1116.png)'
- en: Figure 11-16\. Receive and transform data records with Amazon MSK.
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-16\. 使用 Amazon MSK 接收和转换数据记录。
- en: We receive the real-time input data and predict the star rating to derive the
    customer sentiment.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接收实时输入数据并预测星级评分以推断客户情感。
- en: Amazon MSK allows us to transform our data records with the help of an Lambda
    function. We build a Lambda function that receives the Kafka data records and
    sends the review message to a SageMaker Endpoint that hosts our fine-tuned BERT-based
    model.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Amazon MSK 允许我们通过 Lambda 函数转换数据记录。我们创建了一个 Lambda 函数，接收 Kafka 数据记录，并将评论消息发送到托管我们精调的基于
    BERT 模型的 SageMaker 终端节点。
- en: The model predicts the `star_rating`, which our Lambda function adds to the
    original data record; the function then returns the new record back to our Kafka
    stream.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型预测 `star_rating`，我们的 Lambda 函数将其添加到原始数据记录中；然后函数将新记录返回到我们的 Kafka 流。
- en: The Kafka stream then delivers the transformed data records to an S3 bucket
    using an Amazon S3 sink connector for Kafka.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，Kafka 流使用 Amazon S3 的 Kafka S3 沉降连接器将转换后的数据记录传送到 S3 存储桶。
- en: To set this up, we need to create an Amazon MSK cluster, a Kafka input topic
    (input stream) for the model inputs, and a Kafka output topic (output stream)
    for the model predictions. Next, we need to create a Lambda event source mapping
    using the Amazon MSK Python API `create_event_source_mapping()` to map our Kafka
    input stream to the input of the Lambda function that invokes our SageMaker Endpoint
    and writes the prediction to the Kafka output stream.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置这一过程，我们需要创建 Amazon MSK 集群，为模型输入创建 Kafka 输入主题（输入流），并为模型预测创建 Kafka 输出主题（输出流）。接下来，我们需要使用
    Amazon MSK Python API 的 `create_event_source_mapping()` 创建 Lambda 事件源映射，将我们的 Kafka
    输入流映射到调用 SageMaker 终端节点的 Lambda 函数输入，并将预测写入 Kafka 输出流。
- en: 'Here is the code to create the event source mapping between the Amazon MSK
    cluster and the Lambda function through the `reviews` topic:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建 Amazon MSK 集群和 Lambda 函数之间事件源映射的代码，通过 `reviews` 主题：
- en: '[PRE28]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Reduce Cost and Improve Performance
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降低成本和提升性能
- en: We can further optimize the streaming data architecture for cost and performance.
    For example, Lambda functions are eligible for Compute Savings Plans, which offer
    a discount for one- or three-year term compute usage commitments. There are a
    couple of ways to reduce cost with Kinesis services. One best practice is to aggregate
    smaller data records into one `PUT` request. We can also consider Kinesis Firehose
    versus Data Streams to save money. We can improve the performance of Kinesis Data
    Streams by enabling enhanced fan-out (EFO).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步优化流数据架构以降低成本和提升性能。例如，Lambda 函数符合计算节省计划的资格，该计划为一年或三年的计算使用承诺提供折扣。有几种方式可以通过
    Kinesis 服务降低成本。一种最佳实践是将较小的数据记录聚合成一个 `PUT` 请求。我们还可以考虑使用 Kinesis Firehose 而不是 Data
    Streams 来节省费用。通过启用增强型分发（EFO），我们可以提高 Kinesis Data Streams 的性能。
- en: Aggregate Messages
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合消息
- en: The cost of Kinesis Data Streams is based on the provisioned number of shards
    and our message `PUT` payloads in units of 25 KB. A best practice to reduce cost
    is to aggregate smaller messages into one `PUT` request. We can implement this
    technique with the Kinesis Producer Library (KPL). KPL aggregates and compresses
    multiple logical data records into one Kinesis data record, which we can then
    put efficiently into the stream.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Data Streams 的成本基于预留的分片数量和我们的消息 `PUT` 负载，以 25 KB 单位计量。减少成本的最佳实践是将较小的消息聚合成一个
    `PUT` 请求。我们可以使用 Kinesis Producer Library (KPL) 实现这种技术。KPL 将多个逻辑数据记录聚合和压缩为一个 Kinesis
    数据记录，然后我们可以有效地将其放入流中。
- en: Consider Kinesis Firehose Versus Kinesis Data Streams
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑 Kinesis Firehose 与 Kinesis Data Streams
- en: Kinesis Data Firehose is best for use cases that require zero administration
    and can tolerate some data processing latency. Firehose provides near-real-time
    processing. It is fully managed by AWS and automatically scales to match the throughput
    requirements. We can also batch and compress the data to minimize the storage
    footprint at the destination. With Firehose, we only pay for the data that is
    processed.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Firehose 最适合需要零管理并且可以容忍一些数据处理延迟的用例。Firehose 提供准实时处理。它由 AWS 完全管理，并自动扩展以匹配吞吐需求。我们还可以批量和压缩数据，以最小化目的地的存储占用。使用
    Firehose，我们只支付处理的数据。
- en: Kinesis Data Streams is best for use cases that require custom processing for
    each incoming record. It provides real-time processing. We have to manage the
    throughput capacity of our Kinesis Data Stream ourselves. The cost of Kinesis
    Data Streams is based on the processed data *and* the number of shards provisioned
    to meet our throughput needs.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Data Streams 最适合需要为每个传入记录进行自定义处理的用例。它提供实时处理。我们必须自行管理 Kinesis Data Stream
    的吞吐能力。Kinesis Data Streams 的成本基于处理的数据 *和* 预留分片的数量来满足我们的吞吐需求。
- en: Note
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If we choose to serve trained models using Lambda functions, we can connect
    a Kinesis Data Stream directly to the Lambda function. Lambda functions read the
    records directly from the Kinesis Data Stream and perform the prediction synchronously
    using the event data as the prediction input.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择使用 Lambda 函数来提供训练模型的服务，我们可以直接将 Kinesis Data Stream 连接到 Lambda 函数。Lambda
    函数直接从 Kinesis Data Stream 中读取记录，并使用事件数据同步执行预测。
- en: Enable Enhanced Fan-Out for Kinesis Data Streams
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用 Kinesis Data Streams 的增强扇出
- en: Without EFO, all consumers are contending for the read-throughput limit of each
    shard. This limits the number of consumers per stream and requires fanning out
    to additional streams in order to scale to a large number of consumers, as shown
    in [Figure 11-17](#scaling_consumers_without_enhanced_fan).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 没有 EFO，所有消费者都在竞争每个分片的读取吞吐量限制。这限制了每个流的消费者数量，并且需要扩展到额外的流以扩展到大量的消费者，如 [图 11-17](#scaling_consumers_without_enhanced_fan)
    所示。
- en: '![](assets/dsaw_1117.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1117.png)'
- en: Figure 11-17\. Scaling consumers without EFO using multiple streams.
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-17\. 在没有 EFO 的情况下扩展消费者使用多个流。
- en: With EFO, each shard–consumer combination can leverage its own dedicated, full
    read-throughput limit. [Figure 11-18](#scaling_consumers_with_enhanced_fan_out)
    shows the dedicated shard–consumer pipes with full read throughput.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 EFO，每个分片-消费者组合可以利用自己的专用的全读取吞吐量限制。[图 11-18](#scaling_consumers_with_enhanced_fan_out)
    显示了具有全读取吞吐量的专用分片-消费者管道。
- en: '![](assets/dsaw_1118.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1118.png)'
- en: Figure 11-18\. Scaling consumers with EFO using a single stream with dedicated,
    full-throughput shard–consumer connections.
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-18\. 使用单流并使用专用的全吞吐量分片-消费者连接来扩展消费者的 EFO 缩放。
- en: In order to enable EFO, we use the functions `register_stream_consumer()` and
    `subscribe_to_share()` from the Kinesis Data Streams Python API. When registering
    our consumers with EFO, Kinesis Data Streams will push data to the consumer using
    the highly parallel, nonblocking HTTP/2 protocol. This push mechanism results
    in more reactive, low-latency, and high-performance streaming applications that
    scale to a large number of consumers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用 EFO，我们使用 Kinesis Data Streams Python API 中的 `register_stream_consumer()`
    和 `subscribe_to_share()` 函数。当注册我们的消费者到 EFO 时，Kinesis Data Streams 将使用高度并行、非阻塞的
    HTTP/2 协议向消费者推送数据。这种推送机制导致更具响应性、低延迟和高性能的流式应用程序，可以扩展到大量的消费者。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we showed how to perform streaming analytics and machine learning
    with streaming data. We set up an end-to-end streaming data pipeline using Kinesis
    streaming technologies to capture our product reviews, perform descriptive analytics,
    and apply predictive machine learning. We calculated summary statistics over the
    continuous flow of product reviews, performed anomaly detection on the streaming
    data, and enriched the data with predictions from our BERT-based SageMaker model.
    We visualized the results in a CloudWatch Metrics dashboard, sent email notifications
    to alert teams, and made results available to additional applications.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何使用流数据进行流分析和机器学习。我们使用Kinesis流技术建立了端到端的流数据管道，用于捕获我们的产品评论，执行描述性分析，并应用预测性机器学习。我们对持续流动的产品评论进行了摘要统计，对流数据进行了异常检测，并使用基于BERT的SageMaker模型对数据进行了增强预测。我们在CloudWatch
    Metrics仪表盘中可视化了结果，发送电子邮件通知以警示团队，并使结果可供其他应用程序使用。
- en: In [Chapter 12](ch12.html#secure_data_science_on_aws), we will discuss how to
    secure data science and machine learning projects on AWS. After introducing the
    AWS shared responsibility model and discussing common security considerations,
    we will highlight security best practices for Amazon SageMaker in the context
    of access management, compute and network isolation, encryption, governance, and
    auditability.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第12章](ch12.html#secure_data_science_on_aws)中，我们将讨论如何在AWS上保护数据科学和机器学习项目。在介绍AWS共享责任模型并讨论常见的安全考虑因素后，我们将重点介绍Amazon
    SageMaker的安全最佳实践，涉及访问管理、计算和网络隔离、加密、治理以及可审计性。
