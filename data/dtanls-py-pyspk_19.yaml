- en: Appendix B. Installing PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B. 安装 PySpark
- en: This appendix covers the installation of standalone Spark and PySpark on your
    own computer, whether it’s running Windows, macOS, or Linux. I also briefly cover
    cloud offerings, should you want to easily take advantage of PySpark’s distributed
    nature.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录涵盖了在您的计算机上安装独立Spark和PySpark的过程，无论它是运行Windows、macOS还是Linux。如果您想轻松利用PySpark的分布式特性，我还简要介绍了云服务。
- en: Having a local PySpark cluster means that you’ll be able to experiment with
    the syntax using smaller data sets. You don’t have to acquire multiple computers
    or spend money on managed PySpark on the cloud until you’re ready to scale your
    programs. Once you’re ready to work on a larger data set, you can easily transfer
    your program to a cloud instance of Spark for additional power.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个本地的PySpark集群意味着您将能够使用较小的数据集进行语法实验。在您准备好扩展程序之前，您不必购买多台计算机或花费云上托管PySpark的费用。一旦您准备好处理更大的数据集，您就可以轻松地将程序转移到Spark的云实例上以获得额外的动力。
- en: B.1 Installing PySpark on your local machine
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 在本地机器上安装PySpark
- en: 'This section covers installing Spark and Python on your own computer. Spark
    is a complex piece of software, and, while the installation process is simple,
    most guides out there overcomplicate the installation process. We’ll take a much
    simpler approach by installing the bare minimum to start and building from there.
    Our goals are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了在您的计算机上安装Spark和Python的过程。Spark是一个复杂的软件包，尽管安装过程很简单，但大多数指南都过于复杂。我们将通过安装最基本的部分来启动，并在此基础上构建。我们的目标是以下内容：
- en: Install Java (Spark is written in Scala, which runs on the Java Virtual Machine,
    or JVM).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Java（Spark是用Scala编写的，它运行在Java虚拟机，或JVM上）。
- en: Install Spark.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Spark。
- en: Install Python 3 and IPython.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Python 3和IPython。
- en: Launch a PySpark shell using IPython.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用IPython启动PySpark shell。
- en: (Optional) Install Jupyter and use it with PySpark.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）安装Jupyter并与PySpark一起使用。
- en: In the next sections, we cover instructions for Windows, macOS, and Linux OSes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍Windows、macOS和Linux操作系统的安装说明。
- en: Note When working with Spark locally, you might get a `21/10/26` `17:49:14`
    `WARN` `NativeCodeLoader:` `Unable` `to` `load` `native-hadoop` `library` `for`
    `your` `platform...` `using` `builtin-java` `classes` `where` `applicable` message.
    You do not need to worry about it; it simply means that Hadoop isn’t found on
    your system (it is only available on *nix platforms). As we are working locally,
    it does not matter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当在本地使用Spark时，您可能会收到一条`21/10/26` `17:49:14` `WARN` `NativeCodeLoader:` `Unable`
    `to` `load` `native-hadoop` `library` `for` `your` `platform...` `using` `builtin-java`
    `classes` `where` `applicable`的消息。您无需担心，这仅仅意味着您的系统上没有找到Hadoop（它仅在*nix平台上可用）。由于我们是在本地工作，这并不重要。
- en: B.2 Windows
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 Windows
- en: When working on Windows, you have the option to either install Spark directly
    on Windows or to use WSL (Windows Subsystem for Linux). If you want to use WSL,
    follow the instructions at [https://aka.ms/wslinstall](https://aka.ms/wslinstall)
    and then follow the instructions for GNU/Linux. If you want to install on plain
    Windows, follow the rest of this section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Windows上工作时，您可以选择直接在Windows上安装Spark，或者使用WSL（Windows Subsystem for Linux）。如果您想使用WSL，请按照[https://aka.ms/wslinstall](https://aka.ms/wslinstall)上的说明操作，然后按照GNU/Linux的说明进行操作。如果您想在纯Windows上安装，请按照本节的其余部分进行操作。
- en: B.2.1 Install Java
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.1 安装 Java
- en: The easiest way to install Java on Windows is to go to [https://adoptopenjdk.net](https://adoptopenjdk.net.)
    and follow the download and installation instructions for downloading Java 8 or
    11.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上安装Java的最简单方法是访问[https://adoptopenjdk.net](https://adoptopenjdk.net.)，并按照下载和安装说明下载Java
    8或11。
- en: Warning Because Java 11 is incompatible with certain third-party libraries,
    I recommend staying on Java 8\. Spark 3.0+ works using Java 11+ as well, but some
    third-party libraries trail behind.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：由于Java 11与某些第三方库不兼容，我建议继续使用Java 8。Spark 3.0+也支持使用Java 11+，但一些第三方库可能落后。
- en: B.2.2 Install 7-zip
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.2 安装 7-zip
- en: Spark is available as a GZIP archive (.tgz) file on Spark’s website. By default,
    Windows doesn’t provide a native way to extract those files. The most popular
    option is 7-zip ([https://www.7-zip.org/](https://www.7-zip.org/)). Simply go
    to the website, download the program, and follow the installation instructions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以在Spark的网站上以GZIP存档文件（.tgz）的形式获得。默认情况下，Windows不提供提取这些文件的原生方式。最流行的选项是7-zip（[https://www.7-zip.org/](https://www.7-zip.org/)）。只需访问网站，下载程序，并按照安装说明进行操作。
- en: B.2.3 Download and install Apache Spark
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.3 下载并安装Apache Spark
- en: Go on the Apache website ([https://spark.apache.org/](https://spark.apache.org/))
    and download the latest Spark release. Accept the default options, but figure
    B.1 displays those I see when I navigate to the download page. Make sure you download
    the signatures and checksums if you want to validate the download (step 4 on the
    page).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Apache 网站 ([https://spark.apache.org/](https://spark.apache.org/)) 并下载最新的
    Spark 版本。接受默认选项，但图 B.1 显示了我导航到下载页面时看到的内容。如果你想要验证下载（页面上的第 4 步），请确保下载签名和校验和。
- en: '![](../Images/B-01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/B-01.png)'
- en: Figure B.1 The options to download Spark
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.1 下载 Spark 的选项
- en: Once you have downloaded the file, unzip the file using 7-zip. I recommend putting
    the directory under `C:\Users\[YOUR_USER_NAME]\spark`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完文件后，使用 7-zip 解压文件。我建议将目录放在 `C:\Users\[YOUR_USER_NAME]\spark` 下。
- en: Next, we need to download a `winutils.exe` to prevent some cryptic Hadoop errors.
    Go to the [https://github.com/cdarlint/winutils](https://github.com/cdarlint/winutils)
    repository and download the winutils.exe file in the `hadoop-X.Y.Z/bin` directory
    where X.Y matches the Hadoop version that was used for the selected Spark version
    in figure B.1\. Keep the `README.md` of the repository handy. Place the `winutils.exe`
    in the `bin` directory of your Spark installation (`C:\Users\[YOUR_USER_NAME\spark]`).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要下载一个 `winutils.exe` 以防止一些神秘的 Hadoop 错误。前往 [https://github.com/cdarlint/winutils](https://github.com/cdarlint/winutils)
    仓库，并在 `hadoop-X.Y.Z/bin` 目录下下载 winutils.exe 文件，其中 X.Y 与图 B.1 中选择的 Spark 版本所使用的
    Hadoop 版本相匹配。请保留仓库的 `README.md` 文件。将 `winutils.exe` 放在你的 Spark 安装目录的 `bin` 目录中（`C:\Users\[YOUR_USER-NAME\spark]`）。
- en: Next, we set two environment variables to provide our shell knowledge about
    where to find Spark. Think of environment variables as OS-level variables that
    any program can use; for instance, `PATH` indicates where to find executables
    to run. Here, we set `SPARK_HOME` (the main directory where the Spark executables
    are located), and we append the value of `SPARK_HOME` to the `PATH` environment
    variable. To do so, open the Start menu and search for “Edit the system environment
    variables.” Click on the Environment variables button (see figure B.2), and then
    add them there. You will also need to set `SPARK_HOME` to the directory of your
    Spark installation (`C:\Users\[YOUR-USER-NAME]\spark`). Finally, add the `%SPARK_HOME%\bin`
    directory to your `PATH` environment variable.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置两个环境变量，以提供我们的 shell 关于 Spark 所在位置的知识。将环境变量视为操作系统级别的变量，任何程序都可以使用；例如，`PATH`
    指示了可执行文件的位置。在这里，我们设置 `SPARK_HOME`（Spark 可执行文件所在的主要目录），并将 `SPARK_HOME` 的值附加到 `PATH`
    环境变量中。为此，打开开始菜单并搜索“编辑系统环境变量”。点击环境变量按钮（见图 B.2），然后添加它们。你还需要将 `SPARK_HOME` 设置为你的
    Spark 安装目录（`C:\Users\[YOUR-USER-NAME]\spark`）。最后，将 `%SPARK_HOME%\bin` 目录添加到你的
    `PATH` 环境变量中。
- en: '![](../Images/B-02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/B-02.png)'
- en: Figure B.2 Setting environment variables for Hadoop on Windows
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.2 在 Windows 上设置 Hadoop 的环境变量
- en: Note For the `PATH` variable, you most certainly will already have some values
    in there (akin to a list). To avoid removing other useful variables that might
    be used by other programs, double click on the `PATH` variable and append `%SPARK_HOME%\bin`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对于 `PATH` 变量，你肯定已经有一些值在里面（类似于一个列表）。为了避免移除其他可能被其他程序使用的有用变量，双击 `PATH` 变量，并追加
    `%SPARK_HOME%\bin`。
- en: B.2.4 Configure Spark to work seamlessly with Python
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.4 配置 Spark 以无缝与 Python 一起工作
- en: 'If you are using Spark 3.0+ with Java 11+, you need to input some additional
    configuration to seamlessly work with Python. To do so, we need to create a spark-defaults.conf
    file under the `$SPARK_HOME/conf` directory. When reaching this directory, there
    should be a spark-defaults.conf.template file already there, along with some other
    files. Make a copy of spark-defaults.conf.template, and name it spark-defaults.conf.
    Inside this file, include the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 Spark 3.0+ 与 Java 11+，你需要输入一些额外的配置才能无缝地与 Python 一起工作。为此，我们需要在 `$SPARK_HOME/conf`
    目录下创建一个 spark-defaults.conf 文件。当到达这个目录时，应该已经有一个 spark-defaults.conf.template 文件在那里，还有一些其他文件。复制
    spark-defaults.conf.template，并将其命名为 spark-defaults.conf。在这个文件中，包括以下内容：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will prevent the pesky `java.lang.UnsupportedOperationException:` `sun.misc
    .Unsafe` `or` `java.nio.DirectByteBuffer.(long,` `int)` `not` `available` error
    that happens when you try to pass data between Spark and Python (chapter 8 onward).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将防止当你尝试在 Spark 和 Python 之间传递数据时出现的讨厌的 `java.lang.UnsupportedOperationException:`
    `sun.misc .Unsafe` `or` `java.nio.DirectByteBuffer.(long,` `int)` `not` `available`
    错误（从第 8 章开始）。
- en: B.2.5 Install Python
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.5 安装 Python
- en: The easiest way to get Python 3 is to use the Anaconda Distribution. Go to [https://www.anaconda.com/distribution](https://www.anaconda.com/distribution)
    and follow the installation instructions, making sure you’re getting the 64-bits
    graphical installer for Python 3.0 and above for your OS.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 获取Python 3的最简单方法是使用Anaconda发行版。访问[https://www.anaconda.com/distribution](https://www.anaconda.com/distribution)，并按照安装说明进行操作，确保您正在获取适用于您操作系统的Python
    3.0及以上版本的64位图形安装程序。
- en: 'Once Anaconda is installed, we can activate the Python 3 environment by selecting
    the Anaconda PowerShell Prompt in the Start menu. If you want to create a dedicated
    virtual environment for PySpark, use the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了Anaconda，我们就可以通过在开始菜单中选择Anaconda PowerShell Prompt来激活Python 3环境。如果您想为PySpark创建一个专门的虚拟环境，请使用以下命令：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Warning Python 3.8+ is only supported using Spark 3.0+. If you use Spark 2.4.X
    or older, be sure to specify Python 3.7 in your environment creation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 Python 3.8+仅支持Spark 3.0+。如果您使用Spark 2.4.X或更早版本，请确保在创建环境时指定Python 3.7。
- en: Then, to select your newly created environment, just input `conda` `activate`
    `pyspark` in the Anaconda prompt.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，要选择您新创建的环境，只需在Anaconda提示符中输入`conda activate pyspark`。
- en: B.2.6 Launching an IPython REPL and starting PySpark
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.6 启动IPython REPL并启动PySpark
- en: 'If you have configured the `SPARK_HOME` and `PATH` variables, your Python REPL
    will have access to a local instance of PySpark. Follow the next code block to
    launch IPython:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已配置了`SPARK_HOME`和`PATH`变量，您的Python REPL将能够访问PySpark的本地实例。按照下一个代码块中的说明启动IPython：
- en: Tip If you aren’t comfortable with the Command Line or PowerShell, I recommend
    *Learn Windows PowerShell in a Month of Lunches* by Don Jones and Jeffery D. Hicks
    (Manning, 2016).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士 如果您不熟悉命令行或PowerShell，我推荐Don Jones和Jeffery D. Hicks的《一个月午餐学Windows PowerShell》（Manning,
    2016）。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, within the REPL, you can import PySpark and start it:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在REPL中，您可以导入PySpark并启动它：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note Spark provides a `pyspark.cmd` helper command through the `bin` directory
    of your Spark installation. I prefer accessing PySpark through a regular Python
    REPL when working locally, as I find it easier to install libraries and know exactly
    which Python you’re using. It also interfaces well with your favorite editor.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Spark通过您Spark安装的`bin`目录提供了一个`pyspark.cmd`辅助命令。我在本地工作时更喜欢通过常规Python REPL访问PySpark，因为这使我更容易安装库并确切知道我正在使用哪个Python。它还与您最喜欢的编辑器很好地集成。
- en: B.2.7 (Optional) Install and run Jupyter to use a Jupyter notebook
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.7（可选）安装并运行Jupyter以使用Jupyter笔记本
- en: 'Since we have configured PySpark to be imported from a regular Python process,
    we don’t have any further configuration to do to use it with a notebook. In your
    Anaconda PowerShell window, install Jupyter using the following command:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将PySpark配置为可以从常规Python进程导入，因此我们不需要进行任何其他配置即可使用它与笔记本一起使用。在您的Anaconda PowerShell窗口中，使用以下命令安装Jupyter：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can now run a Jupyter notebook server using the following command. Use
    `cd` to move to the directory where your source code is before doing so:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用以下命令运行Jupyter笔记本服务器。在这样做之前，请使用`cd`命令将目录移动到您的源代码所在的目录：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Start a Python kernel, and get started the same way you would using IPython.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个Python内核，并以您使用IPython相同的方式开始。
- en: Note Some alternate installation instructions will create a separate environment
    for Python programs and PySpark programs, which is where you might see more than
    one kernel option. Using this set of instructions, use the `Python` `3` kernel.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意某些替代安装说明将为Python程序和PySpark程序创建一个单独的环境，您可能会看到多个内核选项。使用这组说明，使用`Python` `3`内核。
- en: B.3 macOS
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 macOS
- en: With macOS, the easiest option—by far—is to use the Homebrew `apache-spark`
    package. It takes care of all dependencies (I still recommend using Anaconda for
    managing Python environments for simplicity).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在macOS上，最简单的方法——无疑是使用Homebrew的`apache-spark`软件包。它会处理所有依赖项（我仍然建议为了简单起见使用Anaconda来管理Python环境）。
- en: B.3.1 Install Homebrew
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.1 安装Homebrew
- en: Homebrew is a package manager for OS.X. It provides a simple command-line interface
    to install many popular software packages and keep them up to date. While you
    can follow the manual’s download and install steps that you’ll find on the Windows
    OS with little change, Homebrew will simplify the installation process to a few
    commands.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Homebrew是OS.X的软件包管理器。它提供了一个简单的命令行界面来安装许多流行的软件包并保持它们更新。虽然您可以在Windows OS上找到的下载和安装步骤进行少量修改，但Homebrew会将安装过程简化为几个命令。
- en: To install Homebrew, go to [https://brew.sh](https://brew.sh) and follow the
    installation instructions. You’ll be able to interact with Homebrew through the
    `brew` command.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Homebrew，请访问[https://brew.sh](https://brew.sh)，并按照安装说明操作。你将通过`brew`命令与Homebrew交互。
- en: 'Apple M1: Rosetta or no Rosetta'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果M1：使用Rosetta还是不使用Rosetta
- en: If you are using a Mac with the new Apple M1 chip, you have the option to run
    using Rosetta (an emulator for x64 instrutions). The instructions in this section
    will work.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是配备新苹果M1芯片的Mac，你有选择使用Rosetta（x64指令的模拟器）运行。本节中的说明将适用。
- en: If you want to use a JVM specialized for the Apple M1, I use the Azul Zulu VM
    that you can download using Homebrew ([https://github.com/mdogan/homebrew-zulu](https://github.com/mdogan/homebrew-zulu)).
    All the code in the book works (faster than on an equivalent Intel Mac, dare I
    say), with the exception of the Spark BigQuery Connector, which fails on an ARM
    platform (see [http://mng.bz/p298](http://mng.bz/p298)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用针对苹果M1优化的JVM，我使用的是Azul Zulu VM，你可以通过Homebrew下载它（[https://github.com/mdogan/homebrew-zulu](https://github.com/mdogan/homebrew-zulu)）。书中所有的代码都适用（比在同等配置的英特尔Mac上运行得更快，我敢这么说），除了Spark
    BigQuery Connector，它在ARM平台上无法运行（见[http://mng.bz/p298](http://mng.bz/p298)）。
- en: B.3.2 Install Java and Spark
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.2 安装Java和Spark
- en: 'Input the following command in a terminal:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中输入以下命令：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can specify the version you want; I recommend getting the latest by passing
    no parameters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以指定你想要的版本；我建议不传递任何参数来获取最新版本。
- en: 'If Homebrew did not set `$SPARK_HOME` when installing Spark on your machine
    (test by restarting your terminal and typing `echo` `$SPARK_HOME`), you will need
    to add the following to your `~/.zshrc`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Homebrew在安装Spark到你的机器时没有设置`$SPARK_HOME`（通过重启终端并输入`echo $SPARK_HOME`来测试），你需要在你的`~/.zshrc`中添加以下内容：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Make sure you are inputting the right version number in lieu of `X.Y.Z`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你输入的是正确的版本号，而不是`X.Y.Z`。
- en: Warning Homebrew will update Spark the moment it has a new version installed.
    When you install a new package, watch for a “rogue” upgrade of `apache-spark`
    and change the `SPARK_HOME` version number as needed. While writing this book,
    it happened to me a few times!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：Homebrew将在安装新版本时立即更新Spark。当你安装新包时，请注意`apache-spark`的“rogue”升级，并根据需要更改`SPARK_HOME`版本号。在编写这本书的时候，这发生在我身上好几次！
- en: B.3.3 Configure Spark to work seamlessly with Python
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.3 配置Spark以无缝与Python协同工作
- en: 'If you are using Spark 3.0+ with Java 11+, you need to input some additional
    configurations to seamlessly work with Python. To do so, we need to create a spark-defaults.conf
    file under the `$SPARK_HOME/conf` directory. When reaching this directory, there
    should be a spark-defaults.conf.template file already there, along with some other
    files. Make a copy of spark-defaults.conf.template, and name it spark-defaults.conf.
    Inside this file, include the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Spark 3.0+与Java 11+，你需要输入一些额外的配置才能无缝地与Python协同工作。为此，我们需要在`$SPARK_HOME/conf`目录下创建一个spark-defaults.conf文件。当你到达这个目录时，应该已经有一个spark-defaults.conf.template文件和一些其他文件。复制spark-defaults.conf.template，并将其命名为spark-defaults.conf。在这个文件中，包括以下内容：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will prevent the pesky `java.lang.UnsupportedOperationException:` `sun.misc
    .Unsafe` `or` `java.nio.DirectByteBuffer.(long,` `int)` `not` `available` error
    that happens when you try to pass data between Spark and Python (chapter 8 onward).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这将防止在尝试在Spark和Python之间传递数据时出现的讨厌的`java.lang.UnsupportedOperationException:`
    `sun.misc.Unsafe` 或 `java.nio.DirectByteBuffer.(long, int)` `not available` 错误（从第8章开始）。
- en: B.3.4 Install Anaconda/Python
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.4 安装Anaconda/Python
- en: 'The easiest way to get Python 3 is to use the Anaconda Distribution. Go to
    [https://www.anaconda.com/distribution](https://www.anaconda.com/distribution)
    and follow the installation instructions, making sure you’re getting the 64-bits
    Graphical installer for Python 3.0 and above for your OS:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 获取Python 3的最简单方法是使用Anaconda发行版。访问[https://www.anaconda.com/distribution](https://www.anaconda.com/distribution)，按照安装说明操作，确保你获得的是适用于你的操作系统的64位图形安装程序，Python
    3.0及以上版本：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If it’s your first time using Anaconda, follow the instructions to register
    your shell.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你第一次使用Anaconda，请按照说明注册你的shell。
- en: Warning Python 3.8+ is supported only using Spark 3.0\. If you use Spark 2.4.X
    or before, be sure to specify Python 3.7 in your environment creation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：Python 3.8+仅支持使用Spark 3.0。如果你使用Spark 2.4.X或更早版本，确保在创建环境时指定Python 3.7。
- en: Then, to select your newly created environment, just input `conda` `activate`
    `pyspark` in the terminal.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了选择你新创建的环境，只需在终端中输入`conda activate pyspark`。
- en: B.3.5 Launching an IPython REPL and starting PySpark
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.5 启动IPython REPL并开始PySpark
- en: 'Homebrew should have the `SPARK_HOME` and `PATH` environment variables, so
    your Python shell (also called REPL, or *read eval print loop*) will have access
    to a local instance of PySpark. You just have to type the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Homebrew 应该已经设置了 `SPARK_HOME` 和 `PATH` 环境变量，因此您的 Python shell（也称为 REPL，或 *read
    eval print loop*）将能够访问 PySpark 的本地实例。您只需输入以下命令即可：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, within the REPL, you can import PySpark and get rolling:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 REPL 中，您可以导入 PySpark 并开始使用：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: B.3.6 (Optional) Install and run Jupyter to use Jupyter notebook
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.6（可选）安装并运行 Jupyter 以使用 Jupyter 笔记本
- en: 'Since we have configured PySpark to be discovered from a regular Python process,
    we don’t have to do any further configuration to use it with a notebook. In your
    Anaconda PowerShell window, install Jupyter using the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将 PySpark 配置为可以从常规 Python 进程中检测到，因此我们无需进行任何进一步配置即可在笔记本中使用它。在您的 Anaconda
    PowerShell 窗口中，使用以下命令安装 Jupyter：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can now run a Jupyter notebook server using the following command. Use
    `cd` to move to the directory where your source code is before doing so:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用以下命令运行 Jupyter 笔记本服务器。在这样做之前，请使用 `cd` 命令移动到您的源代码所在的目录：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Start a Python kernel, and get started the same way you would using IPython.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Python 内核，并以与使用 IPython 相同的方式开始。
- en: Note Some alternate installation instructions will create a separate environment
    for Python programs and PySpark programs, which is where you might see more than
    one kernel option. Using this set of instructions, use the `Python` `3` kernel.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：某些替代安装说明将为 Python 程序和 PySpark 程序创建一个单独的环境，您可能会看到多个内核选项。使用本套件说明，请使用 `Python
    3` 内核。
- en: B.4 GNU/Linux and WSL
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 GNU/Linux 和 WSL
- en: B.4.1 Install Java
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.1 安装 Java
- en: Warning Because Java 11 is incompatible with certain third-party libraries,
    I recommend staying on Java 8\. Spark 3.0 and above works using Java 11 and above
    as well, but some libraries might trail behind.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：由于 Java 11 与某些第三方库不兼容，我建议继续使用 Java 8。Spark 3.0 及以上版本可以使用 Java 11 及以上版本，但某些库可能落后。
- en: 'Most GNU/Linux distributions provide a package manager. OpenJDK version 11
    is available through the software repository:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 GNU/Linux 发行版都提供软件包管理器。OpenJDK 版本 11 可通过软件仓库获得：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: B.4.2 Installing Spark
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.2 安装 Spark
- en: Go on the Apache website and download the latest Spark release. You shouldn’t
    have to change the default options, but figure B.1 displays the ones I see when
    I navigate to the download page. Make sure to download the signatures and checksums
    if you want to validate the download (step 4 on the page).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Apache 网站，下载最新的 Spark 版本。您通常不需要更改默认选项，但图 B.1 显示了我访问下载页面时看到的选项。如果您想验证下载（页面上的第
    4 步），请确保下载签名和校验和。
- en: 'Tip On WSL (and sometimes Linux), you don’t have a graphical user interface
    available. The easiest way to download Spark is to go to the website, follow the
    line, copy the link of the nearest mirror, and pass it along with the `wget` command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：在 WSL（有时是 Linux）上，您没有图形用户界面可用。下载 Spark 最简单的方法是访问网站，遵循指示，复制最近镜像的链接，并使用 `wget`
    命令传递：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you want to know more about using the command line on Linux (and Os.X) proficiently,
    a good free reference is *The Linux Command Line* by William Shotts ([http://linuxcommand.org/](http://linuxcommand.org/)).
    It is also available as a paper or e-book (No Starch Press, 2019).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于在 Linux（和 Os.X）上熟练使用命令行的方法，一本好的免费参考资料是 William Shotts 的 *The Linux
    Command Line* ([http://linuxcommand.org/](http://linuxcommand.org/))。它也以纸质或电子书的形式提供（No
    Starch Press，2019）。
- en: 'Once you have downloaded the file, unzip it. If you are using the command line,
    the following command will do the trick. Make sure you’re replacing `spark-[...].gz`
    with the name of the file you just downloaded:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，请解压它。如果您正在使用命令行，以下命令将解决问题。请确保将 `spark-[...].gz` 替换为您刚刚下载的文件名：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will unzip the content of the archive into a directory. You can now rename
    and move the directory to your liking. I usually put it under `/home/[MY-USER-NAME]/bin/spark-X.Y.Z/`
    (and rename it if the name is not identical), and the instructions will use that
    directory.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将解压存档内容到目录中。现在您可以按需重命名和移动目录。我通常将其放在 `/home/[MY-USER-NAME]/bin/spark-X.Y.Z/`（如果名称不相同，则重命名），说明将使用该目录。
- en: Warning Make sure to replace `X.Y.Z` with the appropriate Spark version.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：请确保将 `X.Y.Z` 替换为适当的 Spark 版本。
- en: 'Set the following environment variables:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 设置以下环境变量：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: B.4.3 Configure Spark to work seamlessly with Python
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.3 配置 Spark 以无缝与 Python 一起工作
- en: 'If you are using Spark 3.0 and above with Java 11 and above, you need to input
    some additional configuration to seamlessly work with Python. To do so, we need
    to create a spark-defaults.conf file under the $`SPARK_HOME/conf` directory. When
    reaching this directory, there should be a spark-defaults.conf.template file already
    there, along with some other files. Make a copy of spark-defaults.conf.template,
    and name it spark-defaults.conf. Inside this file, include the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 Spark 3.0 及以上版本与 Java 11 及以上版本，你需要输入一些额外的配置才能无缝地与 Python 一起工作。为此，我们需要在
    `$``SPARK_HOME/conf` 目录下创建一个 spark-defaults.conf 文件。当你到达这个目录时，应该已经有一个 spark-defaults.conf.template
    文件在那里，还有一些其他文件。复制 spark-defaults.conf.template，并将其命名为 spark-defaults.conf。在这个文件中，包括以下内容：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This will prevent the pesky `java.lang.UnsupportedOperationException:` `sun.misc
    .Unsafe` `or` `java.nio.DirectByteBuffer.(long,` `int)` `not` `available` error
    that happens when you try to pass data between Spark and Python (chapter 8 onward).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这将防止在尝试在 Spark 和 Python 之间传递数据时出现的讨厌的 `java.lang.UnsupportedOperationException:`
    `sun.misc .Unsafe` `or` `java.nio.DirectByteBuffer.(long,` `int)` `not` `available`
    错误（从第 8 章开始）。
- en: B.4.4 Install Python 3, IPython, and the PySpark package
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.4 安装 Python 3、IPython 和 PySpark 包
- en: 'Python 3 is already provided; you just have to install IPython. Input the following
    command in a terminal:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Python 3 已经提供；你只需要安装 IPython。在终端中输入以下命令：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Tip You can also use Anaconda on GNU/Linux. Follow the instructions in the macOS
    section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：你还可以在 GNU/Linux 上使用 Anaconda。遵循 macOS 部分的说明。
- en: 'Then install PySpark using pip. This will allow you to import PySpark in a
    Python REPL:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 pip 安装 PySpark。这将允许你在 Python REPL 中导入 PySpark：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: B.4.5 Launch PySpark with IPython
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.5 使用 IPython 启动 PySpark
- en: 'Launch an IPython shell:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个 IPython shell：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, within the REPL, you can import PySpark and get started:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 REPL 中，你可以导入 PySpark 并开始使用：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: B.4.6 (Optional) Install and run Jupyter to use Jupyter notebook
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.6（可选）安装和运行 Jupyter 以使用 Jupyter notebook
- en: 'Since we have configured PySpark to be discovered from a regular Python process,
    we don’t have to do any further configuration to use it with a notebook. In your
    terminal, input the following to install Jupyter:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将 PySpark 配置为可以从常规 Python 进程中检测到，因此我们不需要进行任何进一步的配置就可以在笔记本中使用它。在你的终端中，输入以下内容来安装
    Jupyter：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can now run a Jupyter notebook server using the following command. Use
    `cd` to move to the directory where your source code is before doing so:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用以下命令运行一个 Jupyter notebook 服务器。在这样做之前，使用 `cd` 命令移动到你的源代码所在的目录：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Start a Python kernel, and get started the same way you would using IPython.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个 Python 内核，并像使用 IPython 一样开始。
- en: Note Some alternate installation instructions will create a separate environment
    for Python programs and PySpark programs, which is where you might see more than
    one kernel option. Using this set of instructions, use the `Python` `3` kernel.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：某些替代安装说明将为 Python 程序和 PySpark 程序创建一个单独的环境，你可能会看到多个内核选项。使用这些说明，使用 `Python`
    `3` 内核。
- en: B.5 PySpark in the cloud
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.5 云中的 PySpark
- en: We finish this appendix with a very quick review of the main options for using
    PySpark in the cloud. There are many options—too many to review—but I decided
    to limit myself to the three main cloud providers (AWS, Azure, GCP). For completeness,
    I also added a section on Databricks since they are the team behind Spark and
    provide a great cloud option for managed Spark that spans all three major clouds.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用对在云中使用 PySpark 的主要选项的快速回顾来结束这个附录。有许多选项——太多以至于无法一一回顾——但我决定只限制在三个主要的云服务提供商（AWS、Azure、GCP）上。为了完整性，我还添加了一个关于
    Databricks 的部分，因为它们是 Spark 的团队，并为所有三个主要云提供了出色的托管 Spark 选项。
- en: Cloud offerings are very much moving targets. While writing this book, every
    provider adjusted their API, sometimes in a significant fashion. Because of this,
    I provide direct links to the relevant articles and the knowledge base I used
    to get Spark running with each provider. With most of them, the documentation
    evolves quickly, but the concepts remain the same. At their core, they all provide
    Spark access; the differences are in the UIs provided for creating, managing,
    and profiling clusters. I recommend, once you pick your preferred option, that
    you read through the documentation to understand some of the idiosyncrasies of
    a given provider.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供的内容非常具有动态性。在撰写这本书的过程中，每个提供商都调整了他们的API，有时甚至是非常重大的调整。因此，我提供了直接链接到相关文章和知识库，这些是我用来在每个提供商上运行Spark的。对于大多数提供商，文档演变迅速，但概念保持不变。在核心上，它们都提供了Spark访问；差异在于它们提供的用于创建、管理和分析集群的用户界面。我建议，一旦你选择了你偏好的选项，就阅读一下文档，以了解特定提供商的一些独特之处。
- en: Note A lot of cloud providers provide some small VM with Spark for you to test.
    They are useful if you can’t install Spark locally on your machine (because of
    work limitations or some other reason). Check options for “single-node” when creating
    your cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意许多云提供商提供了一些带有Spark的小型虚拟机供你测试。如果你无法在本地机器上安装Spark（由于工作限制或其他原因），它们非常有用。在创建你的集群时，检查“单节点”选项。
- en: A (small) difference when working with cloud Spark
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与云Spark一起工作时的（小）差异
- en: When working with a Spark cluster, especially on the cloud, I strongly recommend
    that you install the libraries you wish to use (pandas, scikit-learn, etc.) at
    *cluster creation time*. Managing dependencies on a running cluster is annoying
    at best, and most often you are better off destroying the whole thing and creating
    a new one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当与Spark集群一起工作时，尤其是在云上，我强烈建议你在创建集群时安装你希望使用的库（pandas、scikit-learn等）。在运行中的集群上管理依赖项最多是令人烦恼的，而大多数情况下，你最好是销毁整个集群并创建一个新的。
- en: Each cloud provider will give you instructions on how to create startup actions
    to install libraries. If this is something you end up doing repeatedly, check
    into opportunities for automation, such as Ansible, Puppet, Terraform, and so
    on. When working on personal projects, I usually create a simple shell script.
    Most cloud providers provide a CLI interface to interact with their API in a programmatic
    way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个云提供商都会提供如何创建启动动作以安装库的说明。如果你最终需要重复执行这项操作，请考虑自动化机会，例如Ansible、Puppet、Terraform等。在处理个人项目时，我通常创建一个简单的shell脚本。大多数云提供商都提供了一个CLI接口，以编程方式与其API交互。
- en: B.6 AWS
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.6 AWS
- en: 'Amazon offers two products with Spark: EMR (Elastic Map-Reduce) and Glue. While
    they are pretty different and cater to different needs, I find that Spark is usually
    more up to date on EMR, and the pricing is also better if you are running sporadic
    jobs in the context of getting familiar.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊提供了两个带有Spark的产品：EMR（弹性MapReduce）和Glue。虽然它们相当不同，并且满足不同的需求，但我发现Spark在EMR上通常更新得更及时，如果你在熟悉的环境中运行零星的作业，价格也更优惠。
- en: EMR provides a complete Hadoop environment with a trove of open source tools,
    including Spark. The documentation is available through [https://aws.amazon.com/emr/resources/](https://aws.amazon.com/emr/resources/).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: EMR提供了一个完整的Hadoop环境，其中包括Spark在内的众多开源工具。文档可通过[https://aws.amazon.com/emr/resources/](https://aws.amazon.com/emr/resources/)获取。
- en: Glue is advertised as a serverless ETL service, which includes Spark as part
    of the tools. Glue extends Spark with some AWS-specific notions, such as `DynamicFrame`
    and `GlueContext`, which are pretty powerful but not usable outside of Glue itself.
    The documentation is available through [https://aws.amazon.com/glue/resources/](https://aws.amazon.com/glue/resources/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Glue被宣传为一种无服务器ETL服务，其中包括Spark作为工具的一部分。Glue通过一些AWS特定的概念扩展了Spark，例如`DynamicFrame`和`GlueContext`，这些功能非常强大，但只能在Glue本身中使用。文档可通过[https://aws.amazon.com/glue/resources/](https://aws.amazon.com/glue/resources/)获取。
- en: B.7 Azure
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.7 Azure
- en: 'Azure provides a managed Spark service through the HDInsight umbrella. The
    documentation for the product is available through [https://docs.microsoft.com/en-us/azure/hdinsight/](https://docs.microsoft.com/en-us/azure/hdinsight/).
    Microsoft really segments the different products offered on a Hadoop cluster,
    so make sure you follow the Spark instructions. With Azure, I usually prefer using
    the GUI: the instructions on [http://mng.bz/OGQR](http://mng.bz/OGQR) are very
    easy to follow, and for exploring large-scale data processing, Azure will give
    you hourly pricing as you build your cluster.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Azure通过HDInsight服务提供托管Spark服务。该产品的文档可通过[https://docs.microsoft.com/en-us/azure/hdinsight/](https://docs.microsoft.com/en-us/azure/hdinsight/)获取。微软确实对Hadoop集群上提供的不同产品进行了细分，所以请确保遵循Spark的说明。使用Azure时，我通常更喜欢使用GUI：[http://mng.bz/OGQR](http://mng.bz/OGQR)上的说明非常容易理解，并且对于探索大规模数据处理，Azure会根据你构建的集群提供每小时定价。
- en: Azure also offers single-node Spark through its Data Science Virtual Machine
    for Linux (documentation available through [http://mng.bz/YgwB](http://mng.bz/YgwB)).
    This is a lower-cost option to use if you don’t want to bother with setting up
    an environment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Azure还通过其Linux数据科学虚拟机提供单节点Spark（文档可通过[http://mng.bz/YgwB](http://mng.bz/YgwB)获取）。如果你不想麻烦设置环境，这是一个成本较低的选项。
- en: B.8 GCP
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.8 GCP
- en: Google offers managed Spark through Google Dataproc. The documentation is available
    through [https://cloud.google.com/dataproc/docs](https://cloud.google.com/dataproc/docs).
    I’ve used GCP Dataproc for most of the “scaled-out” examples in the book, as I
    find the command-line utilities very easy to learn and that the documentation
    works well.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Google通过Google Dataproc提供托管Spark服务。文档可通过[https://cloud.google.com/dataproc/docs](https://cloud.google.com/dataproc/docs)获取。我在这本书的大部分“扩展”示例中使用了GCP
    Dataproc，因为我发现命令行工具很容易学习，而且文档工作得很好。
- en: The easiest way to get up and running when learning Spark with Google Dataproc
    is to use the option for single-node clusters that Google offers. The documentation
    for single-node clusters is a little hard to find; it is available at [http://mng.bz/GGOv](http://mng.bz/GGOv).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Google Dataproc学习Spark时，最简单的方法是使用Google提供的单节点集群选项。单节点集群的文档有点难找；它可在[http://mng.bz/GGOv](http://mng.bz/GGOv)找到。
- en: B.9 Databricks
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.9 Databricks
- en: Databricks was founded in 2013 by the creator of Apache Spark. Since then, they
    have grown a complete ecosystem around Spark, which spans data warehousing (Delta
    Lake), a solution for MLOps (MLFlow), and even a secure data interchange functionality
    (Delta Share).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks由Apache Spark的创建者于2013年创立。从那时起，他们围绕Spark建立了一个完整的生态系统，涵盖了数据仓库（Delta
    Lake）、MLOps解决方案（MLFlow）甚至安全数据交换功能（Delta Share）。
- en: Tip If you just want to get started on Databricks with a minimum amount of fuss,
    check out the Databricks community edition at [https://community.cloud.databricks.com/login.html](https://community.cloud.databricks.com/login.html).
    This provides a small cluster for you to get started, with no installation up
    front. This section covers using a full-blown (paid) Databricks instance for when
    you need more power.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你只想在Databricks上以最少的麻烦开始，请查看Databricks社区版[https://community.cloud.databricks.com/login.html](https://community.cloud.databricks.com/login.html)。这为你提供了一个小集群以开始使用，无需预先安装。本节涵盖了使用完整的（付费）Databricks实例，当你需要更多功能时。
- en: 'Databricks anchors its Spark distribution around *Databricks Runtime*, which
    is a cohesive set of libraries (Python, Java, Scala, R) tied to a specific Spark
    version. Their runtimes are available in a few flavors:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks将其Spark发行版围绕*Databricks Runtime*构建，这是一个与特定Spark版本绑定的库集合（Python、Java、Scala、R）。它们的运行时提供几种不同的版本：
- en: Databricks runtime is the standard option, which features a complete ecosystem
    for running Spark on Databricks ([https://docs.databricks.com/runtime/dbr.html](https://docs.databricks.com/runtime/dbr.html)).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks运行时是标准选项，它为在Databricks上运行Spark提供了一个完整的生态系统。[https://docs.databricks.com/runtime/dbr.html](https://docs.databricks.com/runtime/dbr.html)。
- en: Databricks runtime for machine learning provides a curated set of popular ML
    libraries (such as TensorFlow, PyTorch, Keras, and XGBoost) on top of the standard
    options. This runtime ensures you have a cohesive set of ML libraries that play
    well with one another ([https://docs.databricks.com/runtime/mlruntime.html](https://docs.databricks.com/runtime/mlruntime.html)).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks运行时为机器学习提供了一套精选的流行ML库（如TensorFlow、PyTorch、Keras和XGBoost），这些库建立在标准选项之上。这个运行时确保你有一套协同工作的ML库。[https://docs.databricks.com/runtime/mlruntime.html](https://docs.databricks.com/runtime/mlruntime.html)。
- en: Photon is a new, faster, but feature-incomplete reimplementation of the Spark
    query engine in C++. It already is becoming a seductive option because of its
    increased performance ([https://docs.databricks.com/runtime/photon.html](https://docs.databricks.com/runtime/photon.html)).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Photon 是 C++ 中 Spark 查询引擎的新实现，它更快，但功能尚不完整。由于其性能提升（[https://docs.databricks.com/runtime/photon.html](https://docs.databricks.com/runtime/photon.html)），它已经变得很有吸引力。
- en: Databricks prices their services based on DBU (*Databricks Units*), which are
    analog to a “standard compute node for one hour.” The more powerful the cluster
    (either by having more nodes or by making them more powerful), the more DBUs you
    consume, and the more expensive it gets. You also need to factor in the price
    of the underlying cloud resources (VM, storage, network, etc.). This can make
    the pricing quite opaque; I usually use a pricing estimator (both Databricks’s
    and the cloud provider’s) to get a sense of the hourly cost.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 根据每 DBU（*Databricks 单位*）对服务进行定价，这与“一个小时的标准化计算节点”类似。集群越强大（无论是通过增加节点还是通过使它们更强大），您消耗的
    DBU 就越多，成本也就越高。您还需要考虑底层云资源（虚拟机、存储、网络等）的价格。这可能会使定价变得相当不透明；我通常使用定价估算器（Databricks
    和云提供商的）来了解每小时的成本。
- en: Note Review each cloud provider’s page for a price per DBU. They are not consistent
    between providers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请查看每个云提供商的页面以获取每 DBU 的价格。它们在不同提供商之间并不一致。
- en: For the rest of this appendix, I’ll walk through the main steps to set up, use,
    and destroy a workspace in Databricks. I use Google Cloud Platform, but the general
    steps apply to Azure and AWS as well. You won’t find a complete guide to administering
    Databricks, but this should provide you with a working environment to run and
    scale the book’s examples.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录的其余部分，我将介绍在 Databricks 中设置、使用和销毁工作区的主要步骤。我使用 Google Cloud Platform，但这些一般步骤也适用于
    Azure 和 AWS。您不会找到一个完整的 Databricks 管理指南，但这将为您提供一个运行和扩展本书示例的工作环境。
- en: Warning Using Databricks costs money the moment you create a workspace. Using
    a powerful cluster will cost a lot of money. Be sure to shut down your clusters
    and your workspace once you’re done!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：一旦创建工作区，使用 Databricks 就会产生费用。使用强大的集群将花费很多钱。一旦完成，请务必关闭您的集群和工作区！
- en: To start with Databricks, we have to enable the service and create a workspace.
    For this, search for Databricks in the search bar and activate the trial. Carefully
    read the terms and conditions, as well as the permissions required to use the
    service. Once you are done, click on the Manage on Provider button and sign in
    with your GCP account. You will reach a screen like figure B.3, which has an empty
    list and a Create workspace button.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，要开始使用 Databricks，我们需要启用服务并创建一个工作区。为此，在搜索栏中搜索 Databricks 并激活试用。仔细阅读条款和条件以及使用该服务所需的权限。完成后，点击“在提供者上管理”按钮并使用您的
    GCP 账户登录。您将到达一个类似于图 B.3 的屏幕，其中有一个空列表和一个创建工作区按钮。
- en: '![](../Images/B-03.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/B-03.png)'
- en: Figure B.3 The landing page for the Databricks Workspace (here using GCP)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.3 Databricks 工作区的登录页面（此处使用 GCP）
- en: 'To start using Databricks, we need to create a workspace, which serves as an
    umbrella for clusters, notebooks, pipelines, and so on. Organizations typically
    use workspaces as logical separations (by team, project, environment, etc.). In
    our case, we just need one; in figure B.4, we see the simple form to create a
    new workspace. If you don’t have your Google Cloud Project ID, go to the landing
    page for the GCP console and check the top left box: mine is `focus-archway-214221`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Databricks，我们需要创建一个工作区，它作为集群、笔记本、管道等的总称。组织通常使用工作区作为逻辑分离（按团队、项目、环境等）。在我们的情况下，我们只需要一个；在图
    B.4 中，我们看到创建新工作区的简单表单。如果您没有 Google Cloud 项目 ID，请转到 GCP 控制台主页并检查右上角的框：我的 ID 是 `focus-archway-214221`。
- en: '![](../Images/B-04.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/B-04.png)'
- en: Figure B.4 Creating a workspace that will hold our data, notebooks, and clusters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.4 创建一个将存储我们的数据、笔记本和集群的工作区
- en: Once the workspace is created, Databricks will provide a page with a URL to
    reach the workbench; check the right section of figure B.5 for a unique URL ending
    with [gcp.databricks.com](http://gcp.databricks.com). On the top right of this
    page, pay attention to the dropdown Configure menu. We will use it to destroy
    the workspace once done.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 工作区创建后，Databricks 将提供一个包含到达工作台的 URL 的页面；检查图 B.5 的右侧部分以查看以 [gcp.databricks.com](http://gcp.databricks.com)
    结尾的唯一 URL。在此页面的右上角，请注意配置下拉菜单。一旦完成，我们将使用它来销毁工作区。
- en: '![](../Images/B-05.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/B-05.png)'
- en: Figure B.5 Our new workspace created and ready for action. Click the unique
    URL on the right to access the workbench.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.5 我们创建的新工作空间已准备好投入使用。点击右侧的唯一URL以访问工作台。
- en: 'The workbench is really where we start working with Databricks. If you work
    in a corporate environment, you probably have your workspace(s) configured for
    you. You start using Databricks through the screen displayed in figure B.6\. For
    this simple example, we limit ourselves to the Spark-centric functionalities of
    Databricks: notebooks/code, clusters, and data. As discussed at the beginning
    of the section, Databricks contains a complete ecosystem for ML experiments, data
    management, data sharing, data exploration/business intelligence, and version
    control/library management. As you get familiar with the general workflow, seek
    the documentation for those additional components.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 工作台是我们开始使用Databricks的地方。如果你在企业环境中工作，你可能有你的工作空间已经配置好了。你通过图B.6显示的屏幕开始使用Databricks。在这个简单的例子中，我们限制自己使用Databricks以Spark为中心的功能：笔记本/代码、集群和数据。如本节开头所述，Databricks包含一个完整的生态系统，用于ML实验、数据管理、数据共享、数据探索/商业智能和版本控制/库管理。随着你对一般工作流程的熟悉，请查阅那些附加组件的文档。
- en: '![](../Images/B-06.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-06.png)'
- en: Figure B.6 The landing page for our workspace workbench. From this landing page,
    we can create, access, and manage clusters, as well as run jobs and notebooks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.6 我们工作空间工作台的主页。从该主页，我们可以创建、访问和管理集群，以及运行作业和笔记本。
- en: It’s time to start a cluster. Click on New Cluster (or the Cluster menu on the
    sidebar) and fill in the instructions for the cluster configuration. The menu,
    displayed in figure B.5, is pretty self-explanatory. If you are working with small
    data sets, I recommend the single-node Cluster Mode option, which will emulate
    the setup on your local machine (driver plus worker on the same machine). If you
    want to experiment with a larger cluster, set the min/max workers to appropriate
    values. Databricks will start with the min value, scaling automatically, up to
    max as needed.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候启动一个集群了。点击“新建集群”（或侧边栏上的“集群”菜单）并填写集群配置的说明。如图B.5所示的菜单相当直观。如果你正在处理小型数据集，我建议选择单节点集群模式选项，这将模拟你在本地机器上的设置（驱动程序和工人在同一台机器上）。如果你想要实验一个更大的集群，请设置最小/最大工作员到适当的值。Databricks将从最小值开始，根据需要自动扩展，直到达到最大值。
- en: Note By default, GCP has pretty strict usage quotas. When I started using Databricks,
    I had to request for two additional quotas to be increased so that I could launch
    a cluster. I asked for `SSD_TOTAL_GB` to be set to `10000` (10,000 GB of SSD usable)
    and `CPUS` for the relevant region (`us-east4`; see figure B.2) to `100` (100
    CPUs). If you run into issues where the cluster gets destroyed upon creation,
    check the logs; chances are you’ve busted your quota.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GCP有相当严格的用量配额。当我开始使用Databricks时，我不得不申请增加两个额外的配额，以便我可以启动一个集群。我要求将`SSD_TOTAL_GB`设置为`10000`（10,000
    GB的SSD可用）以及相关区域的`CPUS`（`us-east4`；见图B.2）设置为`100`（100个CPU）。如果你在创建集群时遇到集群被销毁的问题，请检查日志；很可能是你超出了配额。
- en: For most use cases, the default configuration, displayed in figure B.7 (`n1-highmem-4`,
    with 26 GB of RAM and 4 cores), is plenty. If necessary, for instance, when performing
    a lot of joins, you can beef up the machines to something more powerful. For GCP,
    I have found that high-memory machines provide the sweetest spot performance-cost-wise.
    Remember that DBU costs are *in addition to* the VM costs GCP will charge you.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数用例，默认配置（如图B.7所示，`n1-highmem-4`，具有26 GB的RAM和4个核心）已经足够。如果需要，例如，在进行大量连接操作时，你可以将机器升级为更强大的配置。对于GCP，我发现高内存机器在性能成本方面提供了最佳平衡。记住，DBU成本是*除了*GCP将向您收取的虚拟机成本之外的。
- en: '![](../Images/B-07.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-07.png)'
- en: Figure B.7 Creating a small cluster with one to two worker nodes, each containing
    26 GB of RAM and 4 cores. Each node costs 0.87 DBU.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.7 创建一个包含一到两个工作节点的小型集群，每个节点包含26 GB的RAM和4个核心。每个节点成本为0.87 DBU。
- en: While the cluster is creating (it will take a few minutes), let’s upload the
    data for running our program. I picked the Gutenberg books, but any data follows
    the same process. Click Create Table on the workbench landing page, choose Upload
    File, and drag and drop the files you want to upload. Pay attention to the DBFS
    Target Directory (here, `/FileStore/tables/gutenberg_books`), which we need to
    reference when reading the data in PySpark.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当集群正在创建时（这将需要几分钟），让我们上传数据以运行我们的程序。我选择了 Gutenberg 书籍，但任何数据都遵循相同的流程。在工作台首页点击“创建表”，选择“上传文件”，然后拖放你想要上传的文件。请注意
    DBFS 目标目录（此处为 `/FileStore/tables/gutenberg_books`），我们在 PySpark 读取数据时需要引用它。
- en: '![](../Images/B-08.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B-08](../Images/B-08.png)'
- en: Figure B.8 Upload data (here, the Gutenberg books from chapters 2 and 3) in
    DBFS (Databricks File System)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.8 在 DBFS（Databricks 文件系统）中上传数据（此处为第 2 章和第 3 章的 Gutenberg 书籍）
- en: Once the cluster is operational and the data is in DBFS, we can create a notebook
    to start coding. Click Create Notebook on the workbench landing page and select
    the name of your cluster, which your notebook will be attached to (like in figure
    B.9).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群运行正常且数据在 DBFS 中，我们就可以创建一个笔记本开始编码。在工作台首页点击“创建笔记本”，并选择你的笔记本将要附加到的集群名称（如图 B.9
    所示）。
- en: '![](../Images/B-09.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B-09](../Images/B-09.png)'
- en: Figure B.9 Creating a notebook on `SmallCluster` to run our analysis
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.9 在 `SmallCluster` 上创建笔记本以运行我们的分析
- en: Upon creation, you’ll see a window like figure B.10\. Databricks notebooks look
    like Jupyter notebooks, with different styling and a few additional features.
    Each cell can contain either Python or SQL code, as well as markdown text that
    will be rendered. When executing a cell, Databricks will provide a progress bar
    during the execution and will give information about how much time each cell took.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，你会看到一个类似于图 B.10 的窗口。Databricks 笔记本看起来像 Jupyter 笔记本，具有不同的样式和一些附加功能。每个单元格可以包含
    Python 或 SQL 代码，以及将被渲染的 markdown 文本。在执行单元格时，Databricks 将在执行过程中提供进度条，并给出每个单元格花费的时间信息。
- en: '![](../Images/B-10.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B-10](../Images/B-10.png)'
- en: Figure B.10 Our notebook, operational and ready to rumble! Databricks notebooks
    look like Jupyter notebooks, with a few Spark-specific additions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.10 我们的操作笔记本，已准备就绪！Databricks 笔记本看起来像 Jupyter 笔记本，增加了一些 Spark 特定的功能。
- en: 'It’s worth mentioning two things when working in Databricks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 中工作时有两点值得提及：
- en: If you upload the data in DBFS, you can access it through `dbfs:/[DATA-LOCATION]`.
    In figure B.8, I take the value directly from the location set in figure B.6 when
    uploading data. Unlike when referring to a URL (such as [https://www.manning.com](https://www.manning.com)),
    here, we have only one forward slash.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你将数据上传到 DBFS，你可以通过 `dbfs:/[DATA-LOCATION]` 访问它。在图 B.8 中，我直接从图 B.6 中设置的数据位置获取值。与引用
    URL（例如 [https://www.manning.com](https://www.manning.com)）不同，这里只有一个正斜杠。
- en: Databricks provides a handy `display()` function that replaces the `show()`
    method. `display()` shows by default 1,000 rows in a rich table format that you
    can scroll through. At the bottom of the third cell in figure B.8, you can also
    see buttons to create a chart or download the data in multiple format. You can
    also use `display()` to show visualizations using popular libraries (see [http://mng.bz/zQEB](http://mng.bz/zQEB)
    for more information).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 提供了一个方便的 `display()` 函数，它替代了 `show()` 方法。`display()` 默认以丰富的表格格式显示
    1,000 行，你可以滚动浏览。在图 B.8 的第三单元格底部，你还可以看到创建图表或以多种格式下载数据的按钮。你还可以使用 `display()` 来显示使用流行库的视觉化（更多信息请见
    [http://mng.bz/zQEB](http://mng.bz/zQEB)）。
- en: Tip If you want even more control over what to display, you can display HTML
    code using the `displayHTML()` function. See [http://mng.bz/0w1N](http://mng.bz/0w1N)
    for more information.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你想对显示的内容有更多控制，可以使用 `displayHTML()` 函数显示 HTML 代码。更多信息请见 [http://mng.bz/0w1N](http://mng.bz/0w1N)。
- en: Once you are done with your analysis, you should turn off your cluster by pressing
    the Stop button in the cluster page of the workbench. If you don’t plan to use
    Spark/Databricks for a longer period of time (more than a few hours) and are using
    a personal subscription, I recommend destroying the workspace, as Databricks spins
    off a few VMs to manage it. You can also go in GCP’s Storage tab and delete the
    buckets Databricks created for hosting data and cluster metadata if you want to
    bring your cloud expenditure to zero dollars.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成分析，你应该通过在工作台的集群页面中点击停止按钮来关闭你的集群。如果你不打算长时间（超过几个小时）使用Spark/Databricks，并且使用的是个人订阅，我建议销毁工作区，因为Databricks会启动几个虚拟机来管理它。如果你想要将云支出降至零美元，也可以进入GCP的存储标签页并删除Databricks为托管数据和集群元数据创建的存储桶。
- en: Databricks provides an attractive way to interact with PySpark in the cloud.
    Each vendor has a different approach to managed Spark in the cloud, from close-to-the-metal
    (GCP Dataproc, AWS EMR) to auto-piloted (AWS Glue). From a user perspective, the
    differences are mostly in how much you are expected to configure your environment
    (and how expensive it is). Just like with Databricks, some additional bundle tools
    are provided to simplify code and data management or provide optimized code to
    speed up key operations. Fortunately, Spark is the common denominator between
    those environments. What you learn in this book should apply regardless of your
    favorite Spark flavor.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks提供了一种吸引人的方式在云端与PySpark交互。每个供应商在云中管理Spark的方法都不同，从接近硬件（GCP Dataproc、AWS
    EMR）到自动驾驶（AWS Glue）。从用户的角度来看，差异主要在于你预计需要配置多少环境（以及它的成本）。就像Databricks一样，一些额外的工具包被提供出来以简化代码和数据管理，或者提供优化代码以加快关键操作。幸运的是，Spark是这些环境中的共同分母。你在本书中学到的知识应该适用于你喜欢的任何Spark版本。
