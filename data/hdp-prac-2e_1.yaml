- en: Part 2\. Data logistics
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2部分\. 数据物流
- en: If you’ve been thinking about how to work with Hadoop in production settings,
    you’ll benefit from this part of the book, which covers the first set of hurdles
    you’ll need to jump. These chapters detail the often-overlooked yet crucial topics
    that deal with data management in Hadoop.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在思考如何在生产环境中使用Hadoop，那么这本书的这一部分将对你有所帮助，它涵盖了你需要跨越的第一个障碍。这些章节详细介绍了经常被忽视但至关重要的主题，这些主题涉及Hadoop中的数据管理。
- en: '[Chapter 3](kindle_split_013.html#ch03) looks at ways to work with data stored
    in different formats, such as XML and JSON, paving the way for a broader examination
    of data formats such as Avro and Parquet that work best with big data and Hadoop.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](kindle_split_013.html#ch03)探讨了如何处理存储在不同格式中的数据，例如XML和JSON，为更广泛地研究数据格式如Avro和Parquet铺平了道路，这些格式与大数据和Hadoop配合得最好。'
- en: '[Chapter 4](kindle_split_014.html#ch04) examines some strategies for laying
    out your data in HDFS, and partitioning and compacting your data. This chapter
    also covers ways of working with small files, as well as how compression can save
    you from many storage and computational headaches.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](kindle_split_014.html#ch04)探讨了在HDFS中布局你的数据、分区和压缩数据的一些策略。本章还涵盖了处理小文件的方法，以及压缩如何帮助你避免许多存储和计算上的麻烦。'
- en: '[Chapter 5](kindle_split_015.html#ch05) looks at ways to manage moving large
    quantities of data into and out of Hadoop. Examples include working with relational
    data in RDBMSs, structured files, and HBase.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](kindle_split_015.html#ch05)探讨了如何管理将大量数据移动到Hadoop中以及从Hadoop中移出的方法。示例包括在RDBMS中处理关系数据、结构化文件和HBase。'
- en: Chapter 3\. Data serialization—working with text and beyond
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章\. 数据序列化——处理文本及其他
- en: '*This chapter covers*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Working with text, XML, and JSON
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文本、XML和JSON
- en: Understanding SequenceFile, Avro, Protocol Buffers, and Parquet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解SequenceFile、Avro、Protocol Buffers和Parquet
- en: Working with custom data formats
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理自定义数据格式
- en: MapReduce offers straightforward, well-documented support for working with simple
    data formats such as log files. But MapReduce has evolved beyond log files to
    more sophisticated data-serialization formats—such as text, XML, and JSON—to the
    point where its documentation and built-in support runs dry. The goal of this
    chapter is to document how you can work with common data-serialization formats,
    as well as to examine more structured serialization formats and compare their
    fitness for use with MapReduce.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce为处理简单的数据格式（如日志文件）提供了简单、文档齐全的支持。但是，MapReduce已经从日志文件扩展到更复杂的数据序列化格式——如文本、XML和JSON——以至于其文档和内置支持已经用尽。本章的目标是记录如何处理常见的数据序列化格式，以及检查更结构化的序列化格式并比较它们与MapReduce使用的适用性。
- en: Imagine that you want to work with the ubiquitous XML and JSON data-serialization
    formats. These formats work in a straightforward manner in most programming languages,
    with several tools being available to help you with marshaling, unmarshaling,
    and validating where applicable. Working with XML and JSON in MapReduce, however,
    poses two equally important challenges. First, MapReduce requires classes that
    can support reading and writing a particular data-serialization format; if you’re
    working with a custom file format, there’s a good chance it doesn’t have such
    classes to support the serialization format you’re working with. Second, MapReduce’s
    power lies in its ability to parallelize reading your input data. If your input
    files are large (think hundreds of megabytes or more), it’s crucial that the classes
    reading your serialization format be able to split your large files so multiple
    map tasks can read them in parallel.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要处理无处不在的XML和JSON数据序列化格式。这些格式在大多数编程语言中都以直接的方式工作，有多个工具可以帮助你进行序列化、反序列化和验证（如果适用）。然而，在MapReduce中使用XML和JSON却提出了两个同样重要的挑战。首先，MapReduce需要能够支持读取和写入特定数据序列化格式的类；如果你正在使用自定义文件格式，那么它很可能没有支持你所使用的序列化格式的类。其次，MapReduce的强大之处在于其并行读取输入数据的能力。如果你的输入文件很大（想想几百兆或更多），那么能够将你的大文件分割以便多个map任务并行读取的类至关重要。
- en: We’ll start this chapter by tackling the problem of how to work with serialization
    formats such as XML and JSON. Then we’ll compare and contrast data-serialization
    formats that are better suited to working with big data, such as Avro and Parquet.
    The final hurdle is when you need to work with a file format that’s proprietary,
    or a less common file format for which no read/write bindings exist in MapReduce.
    I’ll show you how to write your own classes to read/write your file format.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始，解决如何处理如XML和JSON这样的序列化格式的问题。然后我们将比较和对比更适合处理大数据的数据序列化格式，例如Avro和Parquet。最后的挑战是当你需要处理一个专有格式或MapReduce中没有读写绑定的较少见格式时。我会向你展示如何编写自己的类来读取/写入你的文件格式。
- en: '|  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: XML and JSON formats
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: XML和JSON格式
- en: This chapter assumes you’re familiar with the XML and JSON data formats. Wikipedia
    provides some good background articles on XML and JSON, if needed. You should
    also have some experience writing MapReduce programs and should understand the
    basic concepts of HDFS and MapReduce input and output. Chuck Lam’s book, *Hadoop
    in Action* (Manning, 2010), is a good resource on this topic.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设你熟悉XML和JSON数据格式。如果需要，维基百科提供了关于XML和JSON的一些很好的背景文章。你还应该有一些编写MapReduce程序的经验，并应理解HDFS和MapReduce输入输出的基本概念。Chuck
    Lam的书籍《Hadoop in Action》（Manning，2010）是这个主题的一个很好的资源。
- en: '|  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Data serialization support in MapReduce is a property of the input and output
    classes that read and write MapReduce data. Let’s start with an overview of how
    MapReduce supports data input and output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce中的数据序列化支持是读取和写入MapReduce数据的输入和输出类的属性。让我们从MapReduce如何支持数据输入和输出的概述开始。
- en: 3.1\. Understanding inputs and outputs in MapReduce
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 理解MapReduce中的输入和输出
- en: Your data might be XML files sitting behind a number of FTP servers, text log
    files sitting on a central web server, or Lucene indexes in HDFS.^([[1](#ch03fn01)])
    How does MapReduce support reading and writing to these different serialization
    structures across the various storage mechanisms?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据可能是位于多个FTP服务器后面的XML文件，位于中央Web服务器上的文本日志文件，或者HDFS中的Lucene索引。[1](#ch03fn01)。MapReduce是如何支持通过不同的存储机制读取和写入这些不同的序列化结构的？
- en: ¹ Apache Lucene is an information retrieval project that stores data in an inverted
    index data structure optimized for full-text search. More information is available
    at [http://lucene.apache.org/](http://lucene.apache.org/).
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ Apache Lucene是一个信息检索项目，它使用优化了全文搜索的倒排索引数据结构来存储数据。更多信息请访问[http://lucene.apache.org/](http://lucene.apache.org/)。
- en: '[Figure 3.1](#ch03fig01) shows the high-level data flow through MapReduce and
    identifies the actors responsible for various parts of the flow. On the input
    side, you can see that some work (*Create split*) is performed outside of the
    map phase, and other work is performed as part of the map phase (*Read split*).
    All of the output work is performed in the reduce phase (*Write output*).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.1](#ch03fig01) 展示了通过MapReduce的高级数据流，并确定了负责流中各个部分的演员。在输入端，你可以看到一些工作（*创建分割*）在映射阶段之外执行，而其他工作作为映射阶段的一部分执行（*读取分割*）。所有的输出工作都在归约阶段（*写入输出*）执行。'
- en: Figure 3.1\. High-level input and output actors in MapReduce
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1\. MapReduce中的高级输入和输出演员
- en: '![](03fig01_alt.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig01_alt.jpg)'
- en: '[Figure 3.2](#ch03fig02) shows the same flow with a map-only job. In a map-only
    job, the Map-Reduce framework still uses the `OutputFormat` and `RecordWriter`
    classes to write the outputs directly to the data sink.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.2](#ch03fig02) 展示了仅映射作业的相同流程。在仅映射作业中，Map-Reduce框架仍然使用`OutputFormat`和`RecordWriter`类直接将输出写入数据接收器。'
- en: Figure 3.2\. Input and output actors in MapReduce with no reducers
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2\. 没有归约器的MapReduce中的输入和输出演员
- en: '![](03fig02_alt.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig02_alt.jpg)'
- en: Let’s walk through the data flow and discuss the responsibilities of the various
    actors. As we do this, we’ll also look at the relevant code from the built-in
    `TextInputFormat` and `TextOutputFormat` classes to better understand the concepts.
    The `TextInputFormat` and `TextOutputFormat` classes read and write line-oriented
    text files.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们遍历数据流并讨论各个演员的责任。在这个过程中，我们还将查看内置的`TextInputFormat`和`TextOutputFormat`类中的相关代码，以更好地理解概念。`TextInputFormat`和`TextOutputFormat`类读取和写入面向行的文本文件。
- en: 3.1.1\. Data input
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 数据输入
- en: The two classes that support data input in MapReduce are `InputFormat` and `RecordReader`.
    The `InputFormat` class is consulted to determine how the input data should be
    partitioned for the map tasks, and the `RecordReader` performs the reading of
    data from the inputs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 支持在 MapReduce 中进行数据输入的两个类是 `InputFormat` 和 `RecordReader`。`InputFormat` 类用于确定输入数据应该如何分区以供
    map 任务使用，而 `RecordReader` 执行从输入读取数据。
- en: InputFormat
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: InputFormat
- en: 'Every job in MapReduce must define its inputs according to contracts specified
    in the `InputFormat` abstract class. `InputFormat` implementers must fulfill three
    contracts: they describe type information for map input keys and values, they
    specify how the input data should be partitioned, and they indicate the `RecordReader`
    instance that should read the data from source. [Figure 3.3](#ch03fig03) shows
    the `InputFormat` class and how these three contracts are defined.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MapReduce 中，每个作业都必须根据 `InputFormat` 抽象类中指定的合同定义其输入。`InputFormat` 实现者必须满足三个合同：他们描述了
    map 输入键和值的类型信息，他们指定了如何对输入数据进行分区，并且指出了应该从源读取数据的 `RecordReader` 实例。[图 3.3](#ch03fig03)
    展示了 `InputFormat` 类以及这三个合同是如何定义的。
- en: Figure 3.3\. The annotated InputFormat class and its three contracts
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3\. 带注释的 InputFormat 类及其三个合同
- en: '![](03fig03_alt.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig03_alt.jpg)'
- en: Arguably, the most crucial contract is that of determining how to divide the
    input data. In MapReduce nomenclature, these divisions are referred to as *input
    splits*. The input splits directly impact the map parallelism, because each split
    is processed by a single map task. Working with an `InputFormat` that’s unable
    to create multiple input splits over a single data source (such as a file) will
    result in a slow map phase, because the file will be processed sequentially.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，最重要的合同是确定如何划分输入数据。在 MapReduce 术语中，这些划分被称为 *输入分割*。输入分割直接影响 map 并行性，因为每个分割由单个
    map 任务处理。如果使用无法在单个数据源（如文件）上创建多个输入分割的 `InputFormat`，将会导致 map 阶段变慢，因为文件将按顺序处理。
- en: 'The `TextInputFormat` class (view source at [http://mng.bz/h728](http://mng.bz/h728))
    provides an implementation of the `InputFormat` class’s `createRecordReader` method,
    but it delegates the calculation of input splits to its parent class, `FileInputFormat`.
    The following code shows the relevant parts of the `TextInputFormat` class:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextInputFormat` 类（查看源代码位于 [http://mng.bz/h728](http://mng.bz/h728)）为 `InputFormat`
    类的 `createRecordReader` 方法提供了一个实现，但它将输入分割的计算委托给其父类 `FileInputFormat`。以下代码展示了 `TextInputFormat`
    类的相关部分：'
- en: '![](064fig01_alt.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](064fig01_alt.jpg)'
- en: 'The code in `FileInputFormat` (source at [http://mng.bz/CZB8](http://mng.bz/CZB8))
    that determines the input splits is a little more complicated. A simplified form
    of the code is shown in the following example to portray the main elements of
    the `getSplits` method:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `FileInputFormat` 中的代码（源代码位于 [http://mng.bz/CZB8](http://mng.bz/CZB8)）用于确定输入分割，这部分代码稍微复杂一些。以下示例展示了
    `getSplits` 方法的简化形式，以展示其主要元素：
- en: '![](065fig01_alt.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](065fig01_alt.jpg)'
- en: 'The following code shows how you can specify the `InputFormat` to use for a
    MapReduce job:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何指定 MapReduce 作业使用的 `InputFormat`：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: RecordReader
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RecordReader
- en: You’ll create and use the `RecordReader` class in the map tasks to read data
    from an input split and to provide each record in the form of a key/value pair
    for use by mappers. A task is commonly created for each input split, and each
    task has a single `RecordReader` that’s responsible for reading the data for that
    input split. [Figure 3.4](#ch03fig04) shows the abstract methods you must implement.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在 map 任务中创建和使用 `RecordReader` 类来从输入分割中读取数据，并将每个记录以键/值对的形式提供给 mappers 使用。通常为每个输入分割创建一个任务，并且每个任务都有一个单独的
    `RecordReader` 负责读取该输入分割的数据。[图 3.4](#ch03fig04) 展示了你必须实现的抽象方法。
- en: Figure 3.4\. The annotated RecordReader class and its abstract methods
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.4\. 带注释的 RecordReader 类及其抽象方法
- en: '![](03fig04_alt.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig04_alt.jpg)'
- en: 'As shown previously, the `TextInputFormat` class creates a `LineRecordReader`
    to read records from the input splits. The `LineRecordReader` directly extends
    the `RecordReader` class and uses the `LineReader` class to read lines from the
    input split. The `LineRecordReader` uses the byte offset in the file for the map
    key, and the contents of the line for the map value. The following example shows
    a simplified version of the `LineRecordReader` (source at [http://mng.bz/mYO7](http://mng.bz/mYO7)):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`TextInputFormat`类创建一个`LineRecordReader`来从输入拆分中读取记录。`LineRecordReader`直接扩展了`RecordReader`类，并使用`LineReader`类从输入拆分中读取行。`LineRecordReader`使用文件中的字节偏移量作为map键，并使用行的内容作为map值。以下示例展示了`LineRecordReader`的简化版本（源代码见[http://mng.bz/mYO7](http://mng.bz/mYO7)）：
- en: '![](066fig01_alt.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](066fig01_alt.jpg)'
- en: Because the `LineReader` class is easy, we’ll skip that code. The next step
    is to look at how MapReduce supports data outputs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`LineReader`类很简单，我们将跳过那段代码。下一步是看看MapReduce如何支持数据输出。
- en: 3.1.2\. Data output
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 数据输出
- en: 'MapReduce uses similar processes for supporting both output and input data.
    Two classes must exist: an `OutputFormat` and a `RecordWriter`. The `OutputFormat`
    performs some basic validation of the data sink properties, and the `RecordWriter`
    writes each reducer output to the data sink.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce使用类似的过程来支持输出和输入数据。必须存在两个类：一个`OutputFormat`和一个`RecordWriter`。`OutputFormat`执行一些基本的数据目标属性验证，而`RecordWriter`将每个reducer输出写入数据目标。
- en: OutputFormat
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OutputFormat
- en: 'Much like the `InputFormat` class, the `OutputFormat` class, as shown in [figure
    3.5](#ch03fig05), defines the contracts that implementers must fulfill: checking
    the information related to the job output, providing a `RecordWriter`, and specifying
    an output committer, which allows writes to be staged and then made “permanent”
    upon task or job success. (Output committing is covered in [section 3.5.2](#ch03lev2sec15).)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`InputFormat`类一样，如图3.5所示的`OutputFormat`类定义了实现者必须满足的契约：检查与作业输出相关的信息，提供一个`RecordWriter`，并指定一个输出提交者，这允许在任务或作业成功后将写入分阶段并最终“永久化”。（输出提交将在[3.5.2节](#ch03lev2sec15)中讨论。）
- en: Figure 3.5\. The annotated OutputFormat class
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5\. 注释的OutputFormat类
- en: '![](03fig05_alt.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig05_alt.jpg)'
- en: 'Just like the `TextInputFormat`, the `TextOutputFormat` also extends a base
    class, `FileOutputFormat`, which takes care of some complicated logistics such
    as output committing, which we’ll cover later in this chapter. For now, let’s
    take a look at the work that `TextOutputFormat` performs (source at [http://mng.bz/lnR0](http://mng.bz/lnR0)):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`TextInputFormat`一样，`TextOutputFormat`也扩展了一个基类`FileOutputFormat`，它负责一些复杂的物流，例如输出提交，我们将在本章后面讨论。现在，让我们看看`TextOutputFormat`执行的工作（源代码见[http://mng.bz/lnR0](http://mng.bz/lnR0)）：
- en: '![](067fig01_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](067fig01_alt.jpg)'
- en: 'The following code shows how you can specify the `OutputFormat` that should
    be used for a MapReduce job:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何指定MapReduce作业应使用的`OutputFormat`：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: RecordWriter
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RecordWriter
- en: You’ll use the `RecordWriter` to write the reducer outputs to the destination
    data sink. It’s a simple class, as [figure 3.6](#ch03fig06) illustrates.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用`RecordWriter`将reducer输出写入目标数据目标。它是一个简单的类，如图3.6所示。
- en: Figure 3.6\. The annotated RecordWriter class overview
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6\. 注释的RecordWriter类概述
- en: '![](03fig06_alt.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig06_alt.jpg)'
- en: 'The `TextOutputFormat` returns a `LineRecordWriter` object, which is an inner
    class of `Text-OutputFormat`, to perform the writing to the file. A simplified
    version of that class (source at [http://mng.bz/lnR0](http://mng.bz/lnR0)) is
    shown in the following example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextOutputFormat`返回一个`LineRecordWriter`对象，它是`Text-OutputFormat`的内部类，用于执行文件写入。以下示例展示了该类的简化版本（源代码见[http://mng.bz/lnR0](http://mng.bz/lnR0)）：'
- en: '![](068fig01_alt.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](068fig01_alt.jpg)'
- en: Whereas on the map side it’s the `InputFormat` that determines how many map
    tasks are executed, on the reducer side the number of tasks is solely based on
    the value for `mapred.reduce.tasks` set by the client (or if it isn’t set, the
    value is picked up from mapred-site.xml, or from mapred-default.xml if it doesn’t
    exist in the site file).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在map端，是`InputFormat`决定了执行多少个map任务，而在reducer端，任务的数量完全基于客户端设置的`mapred.reduce.tasks`值（如果没有设置，则从mapred-site.xml中获取值，如果该文件不存在于站点文件中，则从mapred-default.xml中获取值）。
- en: Now that you know what’s involved in working with input and output data in Map-Reduce,
    it’s time to apply that knowledge to solving some common data-serialization problems.
    Your first step in this journey is to learn how to work with common file formats
    such as XML.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了在Map-Reduce中处理输入和输出数据所涉及的内容，现在是时候将这项知识应用到解决一些常见的数据序列化问题上了。在这个旅程的第一步，你需要学习如何处理常见的文件格式，如XML。
- en: 3.2\. Processing common serialization formats
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 处理常见的序列化格式
- en: XML and JSON are industry-standard data interchange formats. Their ubiquity
    in the technology industry is evidenced by their heavy adoption in data storage
    and exchange. In this section we’ll look at how you can read and write these data
    formats in MapReduce.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: XML和JSON是行业标准的数据交换格式。它们在技术行业中的普遍性体现在它们在数据存储和交换中的广泛采用。在本节中，我们将探讨如何在MapReduce中读取和写入这些数据格式。
- en: 3.2.1\. XML
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. XML
- en: XML has existed since 1998 as a mechanism to represent data that’s readable
    by machine and human alike. It became a universal language for data exchange between
    systems and is employed by many standards today, such as SOAP and RSS, and it’s
    used as an open data format for products such as Microsoft Office.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: XML自1998年以来作为一种机制存在，可以由机器和人类 alike 读取数据。它成为系统间数据交换的通用语言，并被许多标准采用，如SOAP和RSS，它还被用作Microsoft
    Office等产品作为开放数据格式。
- en: Technique 8 MapReduce and XML
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧8 MapReduce和XML
- en: MapReduce comes bundled with an `InputFormat` that works with text, but it doesn’t
    come with one that supports XML. Working on a single XML file in parallel in MapReduce
    is tricky because XML doesn’t contain a synchronization marker in its data format.^([[2](#ch03fn02)])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce附带了一个与文本一起工作的`InputFormat`，但它没有附带支持XML的`InputFormat`。在MapReduce中并行处理单个XML文件是棘手的，因为XML在其数据格式中不包含同步标记.^([[2](#ch03fn02)])
- en: ² A synchronization marker is typically some binary data used to demarcate record
    boundaries. It allows a reader to perform a random seek into a file and determine
    where the next record starts by reading until a synchronization marker is found.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 同步标记通常是用于界定记录边界的二进制数据。它允许读者在文件中进行随机查找，并通过读取直到找到同步标记来确定下一个记录的开始位置。
- en: Problem
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with large XML files in MapReduce and be able to split and
    process them in parallel.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce中处理大型XML文件，并且能够并行地分割和处理它们。
- en: Solution
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Mahout’s `XMLInputFormat` can be used to work with XML files in HDFS with MapReduce.
    It reads records that are delimited by specific XML begin and end tags. This technique
    also explains how XML can be emitted as output in MapReduce output.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout的`XMLInputFormat`可用于使用MapReduce在HDFS中处理XML文件。它读取由特定的XML开始和结束标签分隔的记录。这项技术还解释了如何在MapReduce输出中发出XML。
- en: Discussion
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: '![](069fig01_alt.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](069fig01_alt.jpg)'
- en: 'MapReduce doesn’t contain built-in support for XML, so we’ll turn to another
    Apache project—Mahout, a machine learning system—to provide an XML `InputFormat`.
    To showcase the XML `InputFormat`, you can write a MapReduce job that uses Mahout’s
    XML input format to read property names and values from Hadoop’s configuration
    files. The first step is to set up the job configuration:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce不包含对XML的内置支持，因此我们将转向另一个Apache项目——Mahout，一个机器学习系统，以提供XML `InputFormat`。为了展示XML
    `InputFormat`，你可以编写一个MapReduce作业，使用Mahout的XML输入格式从Hadoop的配置文件中读取属性名称和值。第一步是设置作业配置：
- en: 'Mahout’s XML input format is rudimentary; you need to tell it the exact start
    and end XML tags that will be searched for in the file, and files are split (and
    records extracted) using the following approach:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout的XML输入格式是基本的；你需要告诉它将在文件中搜索的确切开始和结束XML标签，并且文件使用以下方法分割（并提取记录）：
- en: '**1**.  Files are split into discrete sections along HDFS block boundaries
    for data locality.'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 文件沿HDFS块边界分割成离散部分，以实现数据局部性。'
- en: '**2**.  Each map task operates on a specific input split. The map task seeks
    to the start of the input split, and then continues to process the file until
    it hits the first `xmlinput.start`.'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 每个map任务在特定的输入分割上操作。map任务定位到输入分割的开始，然后继续处理文件，直到遇到第一个`xmlinput.start`。'
- en: '**3**.  The content between `xmlinput.start` and `xmlinput.end` is repeatedly
    emitted until the end of the input split is reached.'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 在`xmlinput.start`和`xmlinput.end`之间的内容会重复发出，直到输入分割的末尾。'
- en: Next you need to write a mapper to consume Mahout’s XML input format. The XML
    element in `Text` form has been supplied, so you’ll need to use an XML parser
    to extract content from the XML.^([[3](#ch03fn03)])
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要编写一个mapper来消费Mahout的XML输入格式。XML元素以`Text`形式提供，因此您需要使用XML解析器从XML中提取内容。[^3](#ch03fn03)
- en: '³ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XMLMapReduceReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XMLMapReduceReader.java).'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XMLMapReduceReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XMLMapReduceReader.java)。
- en: Listing 3.1\. Extracting content with Java’s STAX parser
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 使用Java的STAX解析器提取内容
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The map is given a `Text` instance, which contains a String representation of
    the data between the start and end tags. In this code, you use Java’s built-in
    Streaming API for XML (StAX) parser to extract the key and value for each property
    and output them.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Map被赋予一个`Text`实例，它包含起始和结束标签之间的数据字符串表示。在此代码中，您使用Java内置的XML（StAX）解析器API提取每个属性的键和值，并将它们输出。
- en: 'If you run the MapReduce job against Cloudera’s core-site.xml and use the HDFS
    `cat` command to show the output, you’ll see the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行MapReduce作业针对Cloudera的core-site.xml，并使用HDFS的`cat`命令显示输出，您将看到以下内容：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This output shows that you’ve successfully worked with XML as an input serialization
    format with MapReduce. Not only that, you can support huge XML files because the
    input format supports splitting XML.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出表明您已成功使用MapReduce将XML作为输入序列化格式进行处理。不仅如此，您还可以支持巨大的XML文件，因为输入格式支持分割XML。
- en: Writing XML
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 写入XML
- en: Having successfully read XML, the next question is how to write XML. In your
    reducer, you have callbacks that occur before and after your main reduce method
    is called, which you can use to emit a start and end tag, as shown in the following
    example.^([[4](#ch03fn04)])
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 成功读取XML后，下一个问题是如何写入XML。在您的reducer中，在调用主reduce方法之前和之后发生回调，您可以使用这些回调来发射起始和结束标签，如下例所示。[^4](#ch03fn04)
- en: '⁴ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XmlMapReduceWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XmlMapReduceWriter.java).'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XmlMapReduceWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/xml/XmlMapReduceWriter.java)。
- en: Listing 3.2\. A reducer to emit start and end tags
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2\. 发射起始和结束标签的reducer
- en: '![](ch03ex02-0.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![ch03ex02-0.jpg]'
- en: '![](ch03ex02-1.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![ch03ex02-1.jpg]'
- en: This could also be embedded in an `OutputFormat`, but I’ll leave that as a project
    for you to experiment with. Writing an `OutputFormat` class is covered in [section
    3.5.1](#ch03lev2sec14).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以嵌入到`OutputFormat`中，但我会把它留给你作为一个实验项目。编写`OutputFormat`类的介绍在第3.5.1节。[^7](#ch03lev2sec14)
- en: Pig
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Pig
- en: If you want to work with XML in Pig, the Piggy Bank library^([[5](#ch03fn05)])
    (a user-contributed library of useful Pig code) contains an `XMLLoader`. It works
    much like this technique and captures all of the content between a start and end
    tag, supplying it as a single byte array field in a Pig tuple.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在Pig中处理XML，Piggy Bank库[^5](#ch03fn05)（一个有用的Pig代码的用户贡献库）包含一个`XMLLoader`。它的工作方式与这种技术类似，并捕获起始和结束标签之间的所有内容，将其作为Pig元组中的一个单独的字节数组字段提供。
- en: '⁵ Piggy Bank—user-defined pig functions: [https://cwiki.apache.org/confluence/display/PIG/PiggyBank](https://cwiki.apache.org/confluence/display/PIG/PiggyBank).'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ Piggy Bank—用户定义的Pig函数：[https://cwiki.apache.org/confluence/display/PIG/PiggyBank](https://cwiki.apache.org/confluence/display/PIG/PiggyBank)。
- en: Hive
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive
- en: Currently, no means exist for working with XML in Hive. You’d have to write
    a custom SerDe, which we’ll cover in [chapter 9](kindle_split_022.html#ch09).
    ^([[6](#ch03fn06)])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Hive中还没有处理XML的方法。您必须编写一个自定义的SerDe，我们将在第9章中介绍。[^6](#ch03fn06)
- en: ⁶ SerDe is a shortened form of Serializer/Deserializer; it’s the mechanism that
    allows Hive to read and write data in HDFS.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ SerDe是序列化/反序列化（Serializer/Deserializer）的缩写；它是Hive读取和写入HDFS数据的机制。
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Mahout’s `XmlInputFormat` certainly helps you work with XML. But it’s sensitive
    to an exact string match of both the start and end element names. If the element
    tag can contain attributes with variable values, or if the generation of the element
    can’t be controlled and could result in XML namespace qualifiers being used, then
    this approach may not work for you. Also problematic will be situations where
    the element name you specify is used as a descendant child element.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout的`XmlInputFormat`当然可以帮助你处理XML。但它对起始和结束元素名称的精确字符串匹配很敏感。如果元素标签可以包含具有可变值的属性，或者如果元素的生成无法控制，可能会使用XML命名空间限定符，那么这种方法可能不适合你。还有可能的问题是，你指定的元素名称被用作子元素。
- en: If you have control over the XML laid out in the input, this exercise can be
    simplified by having a single XML element per line. This will let you use the
    built-in Map-Reduce text-based input formats (such as `TextInputFormat`), which
    treat each line as a record and split to preserve that demarcation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你控制着输入中的XML布局，这个练习可以通过每行一个XML元素来简化。这将允许你使用内置的基于文本的Map-Reduce输入格式（如`TextInputFormat`），这些格式将每一行视为一个记录，并分割以保留这种分隔。
- en: Another option worth considering is that of a preprocessing step, where you
    could convert the original XML into a separate line per XML element, or convert
    it into an altogether different data format, such as a SequenceFile or Avro, both
    of which solve the splitting problem for you.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得考虑的选项是预处理步骤，其中你可以将原始XML转换为每个XML元素一行，或者将其转换为完全不同的数据格式，如SequenceFile或Avro，这两种格式都能为你解决分割问题。
- en: Now that you have a handle on how to work with XML, let’s tackle another popular
    serialization format, JSON.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经掌握了如何处理XML，让我们来处理另一种流行的序列化格式，JSON。
- en: 3.2.2\. JSON
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. JSON
- en: JSON shares the machine- and human-readable traits of XML and has existed since
    the early 2000s. It’s less verbose than XML, and it doesn’t have the rich typing
    and validation features available in XML.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: JSON与XML共享机器和人类可读的特性，自2000年代初以来就存在。它比XML更简洁，并且没有XML中可用的丰富类型和验证功能。
- en: Technique 9 MapReduce and JSON
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧9 MapReduce和JSON
- en: Imagine you have some code that’s downloading JSON data from a streaming REST
    service, and every hour it writes a file into HDFS. The amount of data being downloaded
    is large, so each file produced is multiple gigabytes in size.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你有一些代码，它从流式REST服务中下载JSON数据，并且每小时将一个文件写入HDFS。下载的数据量很大，所以每个生成的文件大小都是多吉字节。
- en: 'You’ve been asked to write a MapReduce job that can take as input these large
    JSON files. What you have here is a problem in two parts: first, MapReduce doesn’t
    come with an `InputFormat` that works with JSON; second, how does one even go
    about splitting JSON?'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你被要求编写一个MapReduce作业，它可以接受这些大型JSON文件作为输入。这里的问题是两个部分：首先，MapReduce没有与JSON一起工作的`InputFormat`；其次，如何分割JSON？
- en: '[Figure 3.7](#ch03fig07) shows the problem with splitting JSON. Imagine that
    MapReduce created a split as shown in the figure. The map task that operates on
    this input split will perform a seek to the start of the input split, and then
    needs to determine the start of the next record. With file formats such as JSON
    and XML, it’s challenging to know when the next record starts due to the lack
    of a synchronization marker, or any other indicator that identifies the start
    of a record.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.7](#ch03fig07)展示了分割JSON的问题。想象一下MapReduce创建了一个如图所示的分割。在这个输入分割上运行的map任务将执行一个到输入分割开始的查找，然后需要确定下一个记录的开始。对于JSON和XML这样的文件格式，由于缺乏同步标记或任何其他标识记录开始的指示器，很难知道下一个记录的开始。'
- en: Figure 3.7\. Example of issue with JSON and multiple input splits
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7\. JSON和多个输入分割的问题示例
- en: '![](03fig07_alt.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig07_alt.jpg)'
- en: JSON is harder to partition into distinct segments than a format such as XML
    because JSON doesn’t have a token (like an end tag in XML) to denote the start
    or end of a record.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与XML这样的格式相比，JSON更难以分割成不同的段，因为JSON没有标记（如XML中的结束标签）来表示记录的开始或结束。
- en: Problem
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with JSON inputs in MapReduce, and also to ensure that input
    JSON files can be partitioned for concurrent reads.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce中处理JSON输入，并确保输入JSON文件可以被分区以进行并发读取。
- en: Solution
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: The Elephant Bird `LzoJsonInputFormat` input format is used as a basis to create
    an input format class to work with JSON elements. This technique also discusses
    another approach using my open source project that can work with multiline JSON.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Elephant Bird 的 `LzoJsonInputFormat` 输入格式被用作创建一个用于处理 JSON 元素的输入格式类的基准。这种技术还讨论了使用我的开源项目来处理多行
    JSON 的另一种方法。
- en: Discussion
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Elephant Bird ([https://github.com/kevinweil/elephant-bird](https://github.com/kevinweil/elephant-bird)),
    an open source project that contains useful utilities for working with LZOP compression,
    has an `LzoJsonInputFormat` that can read JSON, though it requires that the input
    file be LZOP-compressed. You can use the Elephant Bird code as a template for
    your own JSON `InputFormat` that doesn’t have the LZOP compression requirement.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Elephant Bird ([https://github.com/kevinweil/elephant-bird](https://github.com/kevinweil/elephant-bird))
    是一个开源项目，它包含用于处理 LZOP 压缩的有用工具，它有一个 `LzoJsonInputFormat` 可以读取 JSON，尽管它要求输入文件是 LZOP
    压缩的。你可以使用 Elephant Bird 代码作为模板来创建自己的 JSON `InputFormat`，该模板不需要 LZOP 压缩要求。
- en: This solution assumes that each JSON record is on a separate line. Your `JsonRecordFormat`
    is simple and does nothing other than construct and return a `JsonRecordFormat`,
    so we’ll skip over that code. The `JsonRecordFormat` emits `LongWritable`, `MapWritable`
    key/value pairs to the mapper, where the `MapWritable` is a map of JSON element
    names and their values.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案假设每个 JSON 记录都在单独的一行上。你的 `JsonRecordFormat` 简单，除了构建和返回一个 `JsonRecordFormat`
    之外什么都不做，所以我们略过那段代码。`JsonRecordFormat` 向映射器发射 `LongWritable`、`MapWritable` 键/值对，其中
    `MapWritable` 是 JSON 元素名称及其值的映射。
- en: Let’s take a look at how this `RecordReader` works. It uses the `LineRecordReader`,
    which is a built-in MapReduce reader that emits a record for each line. To convert
    the line to a `MapWritable`, the reader uses the following method:^([[7](#ch03fn07)])
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个 `RecordReader` 是如何工作的。它使用 `LineRecordReader`，这是一个内置的 MapReduce 读取器，为每一行输出一个记录。为了将行转换为
    `MapWritable`，读取器使用以下方法：^([[7](#ch03fn07)])
- en: '⁷ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/json/JsonInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/json/JsonInputFormat.java).'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/json/JsonInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/json/JsonInputFormat.java).
- en: '[PRE4]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The reader uses the json-simple parser ([http://code.google.com/p/json-simple/](http://code.google.com/p/json-simple/))
    to parse the line into a JSON object, and then iterates over the keys in the JSON
    object and puts them, along with their associated values, into a `MapWritable`.
    The mapper is given the JSON data in `LongWritable`, `MapWritable` pairs and can
    process the data accordingly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 读者使用 json-simple 解析器 ([http://code.google.com/p/json-simple/](http://code.google.com/p/json-simple/))
    将行解析为 JSON 对象，然后遍历 JSON 对象中的键，并将它们及其相关值放入一个 `MapWritable` 中。映射器以 `LongWritable`、`MapWritable`
    对的形式获得 JSON 数据，并相应地处理数据。
- en: 'The following shows an example JSON object:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了一个示例 JSON 对象：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This technique assumes one JSON object per line. The following code shows the
    JSON file you’ll work with in this example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术假设每行一个 JSON 对象。以下代码显示了你在本例中将要处理的 JSON 文件：
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now copy the JSON file into HDFS and run your MapReduce code. The MapReduce
    code writes each JSON key/value pair to the output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将 JSON 文件复制到 HDFS 并运行你的 MapReduce 代码。MapReduce 代码将每个 JSON 键/值对写入输出：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Writing JSON
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编写 JSON
- en: An approach similar to what we looked at in [section 3.2.1](#ch03lev2sec3) for
    writing XML could also be used to write JSON.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类似于我们在 [第 3.2.1 节](#ch03lev2sec3) 中查看的用于编写 XML 的方法也可以用来编写 JSON。
- en: Pig
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Pig
- en: Elephant Bird contains a `JsonLoader` and an `LzoJsonLoader`, which you can
    use to work with JSON in Pig. These loaders work with line-based JSON. Each Pig
    tuple contains a `chararray` field for each JSON element in the line.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Elephant Bird 包含一个 `JsonLoader` 和一个 `LzoJsonLoader`，你可以使用它们在 Pig 中处理 JSON。这些加载器处理基于行的
    JSON。每个 Pig 元组包含一行中每个 JSON 元素的一个 `chararray` 字段。
- en: Hive
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive
- en: Hive contains a `DelimitedJSONSerDe` class which can serialize JSON, but unfortunately
    can’t deserialize it, so you can’t load data into Hive using this SerDe.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 包含一个 `DelimitedJSONSerDe` 类，它可以序列化 JSON，但遗憾的是不能反序列化它，因此你不能使用这个 SerDe 将数据加载到
    Hive 中。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This solution assumes that the JSON input is structured with one line per JSON
    object. How would you work with JSON objects that were across multiple lines?
    An experimental project on GitHub^([[8](#ch03fn08)]) works with multiple input
    splits over a single JSON file. This approach searches for a specific JSON member
    and retrieves the containing object.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此解决方案假设JSON输入是按每行一个JSON对象的结构化。您将如何处理跨越多行的JSON对象？GitHub上的一个实验性项目（^[[8](#ch03fn08)）在单个JSON文件上处理多个输入拆分。这种方法搜索特定的JSON成员并检索包含的对象。
- en: '⁸ A multiline JSON `InputFormat`: [https://github.com/alexholmes/json-mapreduce](https://github.com/alexholmes/json-mapreduce).'
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ 多行JSON `InputFormat`：[https://github.com/alexholmes/json-mapreduce](https://github.com/alexholmes/json-mapreduce)。
- en: You can also review a Google Code project called hive-json-serde ([http://code.google.com/p/hive-json-serde/](http://code.google.com/p/hive-json-serde/)),
    which can support both serialization and deserialization.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看一个名为hive-json-serde的Google Code项目（[http://code.google.com/p/hive-json-serde/](http://code.google.com/p/hive-json-serde/)），该项目可以支持序列化和反序列化。
- en: As you can see, using XML and JSON in MapReduce is kludgy and has rigid requirements
    about how to lay out your data. Support for these two formats in MapReduce is
    also complex and error-prone, because neither lends itself naturally to splitting.
    Clearly, you need to look at alternative file formats that have built-in support
    for splittability.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在MapReduce中使用XML和JSON是笨拙的，并且对数据的布局有严格的要求。MapReduce中对这两种格式的支持也复杂且容易出错，因为它们都不自然地适合拆分。显然，您需要考虑具有内置拆分支持的替代文件格式。
- en: The next step is to look at more sophisticated file formats that are better
    suited to working with MapReduce, such as Avro and SequenceFile.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是查看更复杂的文件格式，这些格式更适合与MapReduce一起使用，例如Avro和SequenceFile。
- en: 3.3\. Big data serialization formats
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 大数据序列化格式
- en: Unstructured text works well when you’re working with scalar or tabular data.
    Semistructured text formats such as XML and JSON can model more sophisticated
    data structures that include composite fields or hierarchical data. But when you’re
    working with big data volumes, you’ll need serialization formats with compact
    serialized forms that natively support partitioning and have schema evolution
    features.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当您处理标量或表格数据时，非结构化文本效果良好。半结构化文本格式，如XML和JSON，可以模拟更复杂的数据结构，包括复合字段或层次数据。但是，当您处理大量大数据时，您将需要具有紧凑序列化形式、原生支持分区和具有模式演变功能的序列化格式。
- en: In this section we’ll compare the serialization formats that work best with
    big data in MapReduce and follow up with how you can use them with MapReduce.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将比较与MapReduce配合使用效果最佳的数据序列化格式，并随后介绍如何使用它们与MapReduce结合。
- en: 3.3.1\. Comparing SequenceFile, Protocol Buffers, Thrift, and Avro
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 比较SequenceFile、Protocol Buffers、Thrift和Avro
- en: 'In my experience, the following characteristics are important when selecting
    a data serialization format:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，在选择数据序列化格式时，以下特性很重要：
- en: '***Code generation*** —Some serialization formats are accompanied by libraries
    with code-generation abilities that allow you to generate rich objects, making
    it easier for you to interact with your data. The generated code also provides
    the added benefit of type-safety to make sure that your consumers and producers
    are working with the right data types.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***代码生成*** —一些序列化格式伴随着具有代码生成能力的库，这允许您生成丰富的对象，使您更容易与数据交互。生成的代码还提供了额外的类型安全优势，以确保您的消费者和生成器正在使用正确的数据类型。'
- en: '***Schema evolution*** —Data models evolve over time, and it’s important that
    your data formats support your need to modify your data models. Schema evolution
    allows you to add, modify, and in some cases delete attributes, while at the same
    time providing backward and forward compatibility for readers and writers.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***模式演变*** —数据模型会随着时间的推移而演变，因此您的数据格式需要支持您修改数据模型的需求。模式演变允许您添加、修改，在某些情况下删除属性，同时为读取器和写入器提供向前和向后兼容性。'
- en: '***Language support*** —It’s likely that you’ll need to access your data in
    more than one programming language, and it’s important that the mainstream languages
    have support for a data format.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***语言支持*** —您可能需要使用多种编程语言访问您的数据，并且主流语言需要支持数据格式。'
- en: '***Transparent compression*** —Data compression is important given the volumes
    of data you’ll work with, and a desirable data format has the ability to internally
    compress and decompress data on writes and reads. It’s a much bigger headache
    for you as a programmer if the data format doesn’t support compression, because
    it means that you’ll have to manage compression and decompression as part of your
    data pipeline (as is the case when you’re working with text-based file formats).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***透明压缩*** —考虑到你将处理的数据量，数据压缩非常重要，一个理想的数据格式能够在写入和读取时内部压缩和解压缩数据。如果数据格式不支持压缩，那么作为程序员，这将给你带来更大的麻烦，因为它意味着你将不得不将压缩和解压缩作为数据管道的一部分来管理（就像你在处理基于文本的文件格式时那样）。'
- en: '***Splittability*** —Newer data formats understand the importance of supporting
    multiple parallel readers that are reading and processing different chunks of
    a large file. It’s crucial that file formats contain synchronization markers (and
    thereby support the ability for a reader to perform a random seek and scan to
    the start of the next record).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可分割性*** —新的数据格式理解到支持多个并行读取器的重要性，这些读取器正在读取和处理大文件的不同部分。文件格式包含同步标记（从而支持读取器执行随机查找和扫描到下一个记录的开始）是至关重要的。'
- en: '***Support in MapReduce and the Hadoop ecosystem*** —A data format that you
    select must have support in MapReduce and other critical Hadoop ecosystem projects,
    such as Hive. Without this support, you’ll be responsible for writing the code
    to make a file format work with these systems.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***MapReduce 和 Hadoop 生态系统中的支持*** —你选择的数据格式必须在 MapReduce 和其他关键 Hadoop 生态系统项目中具有支持，例如
    Hive。如果没有这种支持，你将负责编写代码使文件格式与这些系统一起工作。'
- en: '[Table 3.1](#ch03table01) compares the more popular data serialization frameworks
    to see how they stack up against each other. Additional background on these technologies
    is provided in the following discussion.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3.1](#ch03table01) 比较了更流行的数据序列化框架，以了解它们如何相互比较。以下讨论提供了这些技术的更多背景信息。'
- en: Table 3.1\. Feature comparison of data serialization frameworks
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.1\. 数据序列化框架特性比较
- en: '| Library | Code generation | Schema evolution | Language support | Transparent
    compression | Splittable | Native support in MapReduce | Pig and Hive support
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 库 | 代码生成 | 架构演变 | 语言支持 | 透明压缩 | 可分割 | MapReduce 中原生支持 | Pig 和 Hive 支持 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SequenceFile | No | No | Java, Python | Yes | Yes | Yes | Yes |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| SequenceFile | 否 | 否 | Java, Python | 是 | 是 | 是 | 是 |'
- en: '| Protocol Buffers | Yes (optional) | Yes | C++, Java, Python, Perl, Ruby |
    No | No | No | No |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Protocol Buffers | 是（可选） | 是 | C++, Java, Python, Perl, Ruby | 否 | 否 | 否
    | 否 |'
- en: '| Thrift | Yes (mandatory) | Yes | C, C++, Java, Python, Ruby, Perl | No^([[a](#ch03fn09)])
    | No | No | No |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Thrift | 是（强制） | 是 | C, C++, Java, Python, Ruby, Perl | 否^([[a](#ch03fn09)])
    | 否 | 否 | 否 |'
- en: '| Avro | Yes (optional) | Yes | C, C++, Java, Python, Ruby, C# | Yes | Yes
    | Yes | Yes |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Avro | 是（可选） | 是 | C, C++, Java, Python, Ruby, C# | 是 | 是 | 是 | 是 |'
- en: '| Parquet | No | Yes | Java, Python (C++ planned in 2.0) | Yes | Yes | Yes
    | Yes |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Parquet | 否 | 是 | Java, Python (C++ 计划在 2.0 中实现) | 是 | 是 | 是 | 是 |'
- en: ^a Thrift does support compression, but not in the Java library.
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a Thrift 支持压缩，但不在 Java 库中。
- en: Let’s look at each of these formats in more detail.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些格式。
- en: SequenceFile
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SequenceFile
- en: The SequenceFile format was created to work with MapReduce, Pig, and Hive, and
    therefore integrates well with all of those tools. Its shortcomings are mainly
    its lack of code generation and versioning support, as well as limited language
    support.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: SequenceFile 格式是为了与 MapReduce、Pig 和 Hive 一起使用而创建的，因此与所有这些工具都很好地集成。其缺点主要是缺乏代码生成和版本支持，以及有限的语言支持。
- en: Protocol Buffers
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Protocol Buffers
- en: The Protocol Buffers format has been used heavily by Google for interoperability.
    Its strengths are its versioning support and compact binary format. Downsides
    include its lack of support in MapReduce (or in any third-party software) for
    reading files generated by Protocol Buffers serialization. Not all is lost, however;
    we’ll look at how Elephant Bird uses Protocol Buffers serialization within a higher-level
    container file in [section 3.3.3](#ch03lev2sec7).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Google 严重使用了 Protocol Buffers 格式来实现互操作性。其优势在于其版本支持和小型二进制格式。缺点包括它在 MapReduce（或任何第三方软件）中不支持由
    Protocol Buffers 序列化生成的文件。然而，并非一切都已失去；我们将在[第 3.3.3 节](#ch03lev2sec7)中探讨 Elephant
    Bird 如何在高级容器文件中使用 Protocol Buffers 序列化。
- en: Thrift
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Thrift
- en: Thrift was developed at Facebook as a data-serialization and RPC framework.
    It doesn’t have support in MapReduce for its native data-serialization format,
    but it can support different wire-level data representations, including JSON and
    various binary encodings. Thrift also includes an RPC layer with various types
    of servers, including a nonblocking implementation. We’ll ignore the RPC capabilities
    for this chapter and focus on the data serialization.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Thrift 是在 Facebook 开发的数据序列化和 RPC 框架。它对其原生数据序列化格式在 MapReduce 中没有支持，但它可以支持不同的底层数据表示，包括
    JSON 和各种二进制编码。Thrift 还包括一个具有各种类型服务器的 RPC 层，包括非阻塞实现。在本章中，我们将忽略 RPC 功能，专注于数据序列化。
- en: Avro
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Avro
- en: The Avro format is Doug Cutting’s creation to help address the shortcomings
    of SequenceFile.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 格式是 Doug Cutting 为了解决 SequenceFile 的不足而创造的。
- en: Parquet
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Parquet
- en: Parquet is a columnar file format with rich Hadoop system support, and it works
    well with data models such as Avro, Protocol Buffers, and Thrift. Parquet is covered
    in depth in [section 3.4](#ch03lev1sec4).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种具有丰富 Hadoop 系统支持的列式文件格式，并且与 Avro、Protocol Buffers 和 Thrift 等数据模型配合良好。Parquet
    在 [第 3.4 节](#ch03lev1sec4) 中有详细介绍。
- en: Based on certain evaluation criteria, Avro seems to be the best fit as a data
    serialization framework in Hadoop. SequenceFile is a close second due to its inherent
    compatibility with Hadoop (it was designed for use with Hadoop).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 根据某些评估标准，Avro 似乎是最适合作为 Hadoop 数据序列化框架的。由于与 Hadoop 的固有兼容性（它是为与 Hadoop 一起使用而设计的），SequenceFile
    排名第二。
- en: You can review a useful jvm-serializers project at [https://github.com/eishay/jvm-serializers/wiki/](https://github.com/eishay/jvm-serializers/wiki/),
    which runs various benchmarks to compare file formats based on items such as serialization
    and deserialization times. It contains benchmarks for Avro, Protocol Buffers,
    and Thrift, along with a number of other frameworks.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://github.com/eishay/jvm-serializers/wiki/](https://github.com/eishay/jvm-serializers/wiki/)
    上查看一个有用的 jvm-serializers 项目，该项目运行各种基准测试，以比较基于序列化和反序列化时间等项目的文件格式。它包含 Avro、Protocol
    Buffers 和 Thrift 的基准测试，以及许多其他框架。
- en: After looking at how the various data-serialization frameworks compare, we’ll
    dedicate the next few sections to working with them. We’ll start off with a look
    at SequenceFile.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看各种数据序列化框架的比较后，我们将用接下来的几节内容来介绍如何使用它们。我们将从查看 SequenceFile 开始。
- en: 3.3.2\. SequenceFile
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2. SequenceFile
- en: Because SequenceFile was created for use with MapReduce, this format arguably
    offers the highest level of integration support in conjunction with MapReduce,
    Pig, and Hive. SequenceFile is a splittable binary file format that stores data
    in the form of key/value pairs. All SequenceFiles share the same header format,
    as shown in [figure 3.8](#ch03fig08).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SequenceFile 是为与 MapReduce 一起使用而创建的，因此这种格式与 MapReduce、Pig 和 Hive 一起提供了最高级别的集成支持。SequenceFile
    是一种可分割的二进制文件格式，以键/值对的形式存储数据。所有 SequenceFiles 都共享相同的头部格式，如图 3.8 [图](#ch03fig08)
    所示。
- en: Figure 3.8\. SequenceFile header format
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.8. SequenceFile 头部格式
- en: '![](03fig08.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![03fig08.jpg](03fig08.jpg)'
- en: SequenceFiles come in three types, which vary based on how you apply compression.
    In addition, each type has its own corresponding `Writer` classes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: SequenceFiles 有三种类型，它们根据您应用的压缩方式而有所不同。此外，每种类型都有自己的相应 `Writer` 类。
- en: Uncompressed
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未压缩
- en: Uncompressed SequenceFiles are written using the `SequenceFile.Writer` class.
    No advantage exists for this over the compressed formats, because compression
    generally reduces your storage footprint and is more efficient for reads and writes.
    The file format is shown in [figure 3.9](#ch03fig09).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 未压缩 SequenceFiles 使用 `SequenceFile.Writer` 类编写。与压缩格式相比，这种格式没有优势，因为压缩通常可以减少您的存储占用，并且在读写方面更高效。文件格式如图
    3.9 [图](#ch03fig09) 所示。
- en: Figure 3.9\. File format for record-compressed and uncompressed SequenceFiles
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.9. 记录压缩和非压缩 SequenceFiles 的文件格式
- en: '![](03fig09_alt.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![03fig09_alt.jpg](03fig09_alt.jpg)'
- en: Record-compressed
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 记录压缩
- en: Record-compressed SequenceFiles are written using the `SequenceFile.RecordCompressWriter`
    class. When a record is added to the SequenceFile, it’s immediately compressed
    and written to the file. The disadvantage of this approach is that your compression
    ratio will suffer compared to block compression. This file format, which is essentially
    the same as that of uncompressed SequenceFiles, is shown in [figure 3.9](#ch03fig09).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 记录压缩 SequenceFiles 使用 `SequenceFile.RecordCompressWriter` 类编写。当记录被添加到 SequenceFile
    中时，它会立即压缩并写入文件。这种方法的缺点是，与块压缩相比，您的压缩率会受到影响。这种文件格式与未压缩 SequenceFiles 的格式基本相同，如图
    3.9 [图](#ch03fig09) 所示。
- en: Block-compressed
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 块压缩
- en: Block-compressed SequenceFiles are written using the `SequenceFile.BlockCompressWriter`
    class. By default, the block size is the same as the HDFS block size, although
    this can be overridden. The advantage to this compression is that it’s more aggressive;
    the whole block is compressed, rather than compressing at the record level. Data
    isn’t written until it reaches the block size, at which point the whole block
    is compressed, resulting in good overall compression. The file format is shown
    in [figure 3.10](#ch03fig10).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 块压缩的SequenceFiles使用`SequenceFile.BlockCompressWriter`类编写。默认情况下，块大小与HDFS块大小相同，尽管这可以被覆盖。这种压缩的优势在于它更加激进；整个块被压缩，而不是在记录级别上进行压缩。数据只有在达到块大小时才会写入，此时整个块被压缩，从而实现良好的整体压缩。文件格式如图3.10所示。
- en: Figure 3.10\. Block-compressed SequenceFile format
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10\. 块压缩的SequenceFile格式
- en: '![](03fig10_alt.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig10_alt.jpg)'
- en: You only need one `Reader` class (`SequenceFile.Reader`) to read all three types
    of SequenceFiles. Even the `Writer` is abstracted, because you can call `SequenceFile.createWriter`
    to choose the preferred format, and it returns a base class that can be used for
    writing regardless of compression.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要一个`Reader`类（`SequenceFile.Reader`）来读取所有三种类型的SequenceFiles。即使是`Writer`也是抽象的，因为你可以调用`SequenceFile.createWriter`来选择首选格式，它返回一个基类，可以用于无论压缩与否的写入。
- en: 'SequenceFiles have a pluggable serialization framework. Written keys and values
    must have a related `org.apache.hadoop.io.serializer.Serializer` and `Deserializer`
    for marshaling and unmarshaling. Hadoop comes with four serializers: Avro, Java,
    Tether (for binary data contained within a `TetherData` class), and `Writable`
    (the default serializer).^([[9](#ch03fn10)])'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: SequenceFiles拥有一个可插拔的序列化框架。写入的键和值必须有一个相关的`org.apache.hadoop.io.serializer.Serializer`和`Deserializer`用于序列化和反序列化。Hadoop自带四种序列化器：Avro、Java、Tether（用于包含在`TetherData`类中的二进制数据），以及`Writable`（默认序列化器）.^([[9](#ch03fn10)])
- en: ⁹ `Writable` is an interface in Hadoop used to support general-purpose data
    serialization, and it’s used for sending data across the wire between Hadoop components.
    Yahoo has a good introduction to `Writable`s at [http://developer.yahoo.com/hadoop/tutorial/module5.html#writable](http://developer.yahoo.com/hadoop/tutorial/module5.html#writable).
  id: totrans-200
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ `Writable`是Hadoop中用于支持通用数据序列化的一个接口，它用于在Hadoop组件之间发送数据。Yahoo在[http://developer.yahoo.com/hadoop/tutorial/module5.html#writable](http://developer.yahoo.com/hadoop/tutorial/module5.html#writable)有一个关于`Writable`的良好介绍。
- en: '|  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Custom SequenceFile serialization
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自定义SequenceFile序列化
- en: If you want your SequenceFile to contain objects that aren’t `Writable` or `Serializable`,
    you’ll need to implement your own Serializer and register it. You register it
    by updating core-site.xml and appending the class name of the custom serialization
    implementation to the `io.serializations` property.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望你的SequenceFile包含不是`Writable`或`Serializable`的对象，你需要实现自己的序列化器并将其注册。你可以通过更新core-site.xml并将自定义序列化实现的类名追加到`io.serializations`属性来注册它。
- en: '|  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: SequenceFiles are splittable because a synchronization marker is written approximately
    every 6 KiB (1 kibibyte = 1024 bytes) in the file for record-based files, and
    before every block for block-based files.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: SequenceFiles是可分割的，因为对于基于记录的文件，大约每6 KiB（1 kibibyte = 1024 bytes）写入一个同步标记，而对于基于块的文件，在每个块之前写入。
- en: Now let’s look at how to use SequenceFiles in MapReduce.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何在MapReduce中使用SequenceFiles。
- en: Technique 10 Working with SequenceFiles
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧10 使用SequenceFiles
- en: Working with text in MapReduce can start to get tricky when you have to support
    complex types of data, which may include nonscalar data types such as lists or
    dictionaries. In addition, large compressed text files require some additional
    wrangling if Map-Reduce’s data locality properties are important to you. These
    challenges can be overcome by using a file format such as SequenceFile.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要在MapReduce中支持复杂的数据类型，包括非标量数据类型，如列表或字典时，处理文本可能会变得复杂。此外，如果Map-Reduce的数据局部性属性对你很重要，那么处理大型压缩文本文件需要一些额外的处理。使用如SequenceFile这样的文件格式可以克服这些挑战。
- en: Problem
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with a structured file format in MapReduce that you can use
    to model complex data structures and that also supports compression and splittable
    inputs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce中使用一个结构化文件格式，可以用来建模复杂的数据结构，同时也支持压缩和可分割的输入。
- en: Solution
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: This technique looks at how the SequenceFile file format can be used from both
    standalone applications and MapReduce.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术探讨了如何从独立应用程序和MapReduce中使用SequenceFile文件格式。
- en: Discussion
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The SequenceFile format offers a high level of integration with computational
    tools such as MapReduce and can also model complex data structures. We’ll examine
    how to read and write SequenceFiles, and also how to use them with MapReduce,
    Pig, and Hive.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: SequenceFile格式与计算工具（如MapReduce）提供了高级别的集成，并且还可以模拟复杂的数据结构。我们将探讨如何读取和写入SequenceFiles，以及如何与MapReduce、Pig和Hive一起使用。
- en: We’ll work with the stock data for this technique. The most common serialization
    method used with SequenceFiles is `Writable`, so you’ll need to create a `Writable`
    to represent the stock data. The key elements of writing a complex `Writable`
    are extending the `Writable` class and defining serialization and deserialization
    methods, as shown in the following listing.^([[10](#ch03fn11)])
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用股票数据来应用这项技术。与SequenceFiles一起使用的最常见序列化方法是`Writable`，因此你需要创建一个`Writable`来表示股票数据。编写复杂`Writable`的关键要素是扩展`Writable`类并定义序列化和反序列化方法，如下所示：^([[10](#ch03fn11)])
- en: '^(10) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockPriceWritable.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockPriceWritable.java).'
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([10]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockPriceWritable.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockPriceWritable.java).
- en: Listing 3.3\. A `Writable` implementation to represent a stock price
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3\. 表示股票价格的`Writable`实现
- en: '![](ch03ex03-0.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](ch03ex03-0.jpg)'
- en: '![](ch03ex03-1.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](ch03ex03-1.jpg)'
- en: Now that you have your `Writable`, you’ll need to write some code that will
    create a SequenceFile. You’ll read the stocks file from the local disk, create
    the `StockWritable`, and write it to your SequenceFile, using the stock symbol
    as your key:^([[11](#ch03fn12)])
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了你的`Writable`，你需要编写一些代码来创建一个SequenceFile。你将从本地磁盘读取股票文件，创建`StockWritable`，并将其写入你的SequenceFile，使用股票代码作为你的键：^([[11](#ch03fn12)])
- en: '^(11) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockWriter.java).'
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([11]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockWriter.java).
- en: '![](082fig01_alt.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](082fig01_alt.jpg)'
- en: Great! Now how do you go about reading the files created with your writer?^([[12](#ch03fn13)])
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！现在你该如何阅读使用你的作者创建的文件呢？^([[12](#ch03fn13)])
- en: '^(12) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockReader.java).'
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([11]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockReader.java).
- en: '![](082fig02_alt.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](082fig02_alt.jpg)'
- en: 'Now you need to prove that it works by writing and reading a file:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要通过编写和读取文件来证明它的工作：
- en: '[PRE8]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How would you process this SequenceFile in MapReduce? Luckily, both `SequenceFileInputFormat`
    and `SequenceFileOutputFormat` integrate nicely with MapReduce. Remember earlier
    in this chapter when we talked about how the default SequenceFile serialization
    supports `Writable` classes for serialization? Because `Writable` is the native
    data format in MapReduce, using SequenceFiles with MapReduce is totally transparent.
    See if you agree. The following code shows a MapReduce job with an identity mapper
    and reducer:^([[13](#ch03fn14)]), ^([[14](#ch03fn15)])
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何在MapReduce中处理这个SequenceFile？幸运的是，`SequenceFileInputFormat`和`SequenceFileOutputFormat`与MapReduce很好地集成。记得在本章前面我们讨论过默认的SequenceFile序列化支持序列化`Writable`类吗？因为`Writable`是MapReduce中的原生数据格式，所以使用SequenceFile与MapReduce集成是完全透明的。看看你是否同意。以下代码展示了具有身份映射器和还原器的MapReduce作业：^([[13](#ch03fn14)]),
    ^([[14](#ch03fn15)])
- en: '^(13) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockMapReduce.java).'
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([13]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockMapReduce.java).
- en: ^(14) An identity function is a mathematical term to denote a function that
    returns the same value that was used as its argument. In MapReduce this means
    the same thing—the map identity function emits all the key/value pairs that it
    is supplied, as does the reducer, without any transformation or filtering. A job
    that doesn’t explicitly set a map or reduce class results in Hadoop using a built-in
    identity function.
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（14）恒等函数是一个数学术语，用来表示返回其参数相同值的函数。在 MapReduce 中，这意味着相同的意思——map 恒等函数会发出它所提供的所有键/值对，就像
    reducer 一样，没有任何转换或过滤。未显式设置 map 或 reduce 类的作业会导致 Hadoop 使用内置的恒等函数。
- en: '![](083fig01_alt.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](083fig01_alt.jpg)'
- en: 'Now you can run the identity MapReduce job against the stocks SequenceFile
    that you created earlier in this technique:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以运行一个身份 MapReduce 作业，针对你在此技术中之前创建的股票 SequenceFile：
- en: '[PRE9]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Because all it’s doing is echoing the input to the output, you should see identical
    content in both files. You can make sure that’s the case by reading in the job
    output file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它所做的只是将输入回显到输出，所以你应该在两个文件中看到相同的内容。你可以通过读取作业输出文件来确保这一点。
- en: 'First of all, how do you verify that the output is a SequenceFile? Easy, just
    `cat` it—the first three bytes of a SequenceFile are *SEQ*, followed by a fourth
    byte containing the SequenceFile version, which is then followed by the key and
    value classes:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你如何验证输出是一个 SequenceFile？很简单，只需使用 `cat` 命令查看——SequenceFile 的前三个字节是 *SEQ*，然后是一个包含
    SequenceFile 版本的第四个字节，接着是键和值类：
- en: '![](084fig01_alt.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](084fig01_alt.jpg)'
- en: 'Looks good. Now try using the SequenceFile reader code you wrote earlier to
    dump it to standard output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。现在尝试使用你之前编写的 SequenceFile 读取器代码将其转储到标准输出：
- en: '[PRE10]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That was easy. Because SequenceFiles are key/value-based, and the default serialization
    data format for SequenceFiles is `Writable`, the use of SequenceFiles is completely
    transparent to your map and reduce classes. We demonstrated this by using Map-Reduce’s
    built-in identity map and reduce classes with the SequenceFile as input. The only
    work you had to do was to tell MapReduce to use the SequenceFile-specific input
    and output format classes, which are built into MapReduce.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。因为 SequenceFiles 是基于键/值的，并且 SequenceFiles 的默认序列化数据格式是 `Writable`，所以 SequenceFiles
    的使用对你的 map 和 reduce 类是完全透明的。我们通过使用 Map-Reduce 内置的恒等 map 和 reduce 类，并将 SequenceFile
    作为输入来演示这一点。你唯一需要做的工作是告诉 MapReduce 使用 SequenceFile 特定的输入和输出格式类，这些类是内置在 MapReduce
    中的。
- en: Reading SequenceFiles in Pig
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 Pig 中读取 SequenceFiles
- en: By writing your own `Writable` you created more work for yourself with non-MapReduce
    tools such as Pig. Pig works well with Hadoop’s built-in scalar `Writable`s such
    as `Text` and `IntWritable`, but it doesn’t have support for custom `Writable`s.
    You’ll need to write your own `LoadFunc` to support the `StockPriceWritable`.
    This will work well with MapReduce, but Pig’s `SequenceFileLoader` won’t work
    with your custom `Writable`, which means that you’ll need to write your own Pig
    loader to process your files. The appendix contains details on installing Pig.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编写自己的 `Writable`，你在 Pig 等非 MapReduce 工具上为自己增加了更多的工作。Pig 与 Hadoop 内置的标量 `Writable`，如
    `Text` 和 `IntWritable`，配合得很好，但它不支持自定义 `Writable`。你需要编写自己的 `LoadFunc` 来支持 `StockPriceWritable`。这将很好地与
    MapReduce 一起工作，但 Pig 的 `SequenceFileLoader` 不会与你的自定义 `Writable` 一起工作，这意味着你需要编写自己的
    Pig 加载器来处理你的文件。附录中包含了安装 Pig 的详细信息。
- en: The `LoadFunc` for Pig is straightforward, as can be seen in the following listing.^([[15](#ch03fn16)])
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Pig 的 `LoadFunc` 很简单，如下所示。（[[15](#ch03fn16)]）
- en: '^(15) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockLoader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockLoader.java).'
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（15）GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockLoader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/writable/SequenceFileStockLoader.java)。
- en: Listing 3.4\. A Pig loader function that converts a `StockPriceWritable` into
    a Pig tuple
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.4\. 将 `StockPriceWritable` 转换为 Pig 元组的 Pig 加载器函数
- en: '[PRE11]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now you can try to load and dump the stock SequenceFile in Pig:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在 Pig 中尝试加载和转储股票 SequenceFile：
- en: '[PRE12]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Hive
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive
- en: Hive contains built-in support for SequenceFiles, but it has two restrictions.
    First, it ignores the key portion of each record. Second, out of the box it only
    works with SequenceFile values that are `Writable`, and it supports them by performing
    a `toString()` to convert the value into a `Text` form.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 包含对 SequenceFiles 的内置支持，但它有两个限制。首先，它忽略了每个记录的关键部分。其次，它默认只与 `Writable` 类型的
    SequenceFile 值一起工作，并通过执行 `toString()` 将值转换为 `Text` 形式来支持它们。
- en: In our example, you have a custom `Writable`, so you had to write a Hive SerDe,
    which deserialized your `Writable` into a form Hive could understand. The resulting
    data definition language (DDL) statement is as follows:^([[16](#ch03fn17)])
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，你有一个自定义的 `Writable`，因此你必须编写一个 Hive SerDe，将你的 `Writable` 反序列化为 Hive 能够理解的形式。结果的数据定义语言（DDL）语句如下：^([16](#ch03fn17))
- en: ^(16) The code for `StockWritableSerDe` is on GitHub at [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockWritableSerDe.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockWritableSerDe.java).
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([16](#ch03fn17)) `StockWritableSerDe` 的代码在 GitHub 上，[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockWritableSerDe.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/StockWritableSerDe.java)。
- en: '[PRE13]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We’ll cover custom Hive SerDe examples in more detail in [chapter 9](kindle_split_022.html#ch09).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第 9 章中更详细地介绍自定义 Hive SerDe 示例。[章节 9](kindle_split_022.html#ch09)。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: 'SequenceFiles are useful in that they solve two problems that make using MapReduce
    challenging: they’re natively splittable, and they also have built-in support
    for compression, which makes it transparent to the user. They’re also useful as
    containers for other file formats that don’t integrate as well into MapReduce.
    The thorn in the side of SequenceFiles is that they lack multilanguage support,
    which limits the range of tools that can interoperate with your data. But if your
    data mostly stays in HDFS and is processed with MapReduce (or Hive/Pig), SequenceFiles
    may be just what you’re looking for.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: SequenceFiles 有用，因为它们解决了使用 MapReduce 时遇到的两个问题：它们是本地可分割的，并且它们还内置了对压缩的支持，这使得对用户来说是透明的。它们也适用于作为其他文件格式的容器，这些文件格式与
    MapReduce 的集成不是很好。SequenceFiles 的痛点是它们缺乏多语言支持，这限制了可以与你的数据交互的工具范围。但如果你大部分数据都留在
    HDFS 中，并且使用 MapReduce（或 Hive/Pig）进行处理，SequenceFiles 可能正是你所需要的。
- en: Another challenge for SequenceFiles is their lack of schema evolution when working
    with `Writable`s—making a change to your `Writable` won’t be backward or forward
    compatible unless you build that into your implementation. This can be solved
    by using Protocol Buffers as your key/value type.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SequenceFiles 来说，另一个挑战是它们在与 `Writable`s 一起工作时缺乏模式演变——除非你在实现中构建它，否则对 `Writable`
    的更改不会向前或向后兼容。这可以通过使用 Protocol Buffers 作为你的键/值类型来解决。
- en: This technique looked at how to use SequenceFiles with `Writable`s, which SequenceFile
    knows how to encode and decode within its file format. How about making SequenceFiles
    work with data other than `Writable`s?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术探讨了如何使用 SequenceFiles 与 `Writable`s，SequenceFile 知道如何在文件格式内对其进行编码和解码。那么，让
    SequenceFiles 与除 `Writable`s 之外的数据一起工作呢？
- en: Technique 11 Using SequenceFiles to encode Protocol Buffers
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 11 使用 SequenceFiles 编码 Protocol Buffers
- en: '`Writable`s are first-class citizens in SequenceFiles, and the APIs have specific
    methods to read and write `Writable` instances, which you saw in the previous
    technique. This doesn’t mean that SequenceFiles are limited to working with `Writable`s—in
    fact, they can work with any data type as long as there’s a serialization implementation
    for your data type that plugs into Hadoop’s serialization framework.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`Writable`s 是 SequenceFiles 的一等公民，APIs 有特定的方法来读取和写入 `Writable` 实例，这在之前的技巧中已经看到。这并不意味着
    SequenceFiles 限于与 `Writable`s 一起工作——实际上，只要你的数据类型有适合插入 Hadoop 序列化框架的序列化实现，它们就可以与任何数据类型一起工作。'
- en: Protocol Buffers is a sophisticated data format that Google open-sourced; it
    provides schema evolution and efficient data-encoding capabilities. (More details
    on Protocol Buffers are presented in [section 3.3.3](#ch03lev2sec7)). In this
    technique, you’ll implement a Protocol Buffers serialization and see how it allows
    you to work with native Protocol Buffers objects in MapReduce.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Protocol Buffers 是一种复杂的数据格式，由 Google 开源；它提供了模式演变和高效的数据编码能力。（关于 Protocol Buffers
    的更多细节请参阅[第 3.3.3 节](#ch03lev2sec7)）。在本技巧中，你将实现 Protocol Buffers 序列化，并了解它如何允许你在
    MapReduce 中使用本地的 Protocol Buffers 对象。
- en: Problem
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with Protocol Buffers data in MapReduce.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在 MapReduce 中处理 Protocol Buffers 数据。
- en: Solution
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Write a Protocol Buffers serializer, which enables you to encode Protocol Buffers
    serialized data within SequenceFiles.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个 Protocol Buffers 序列化器，它使你能够在 SequenceFiles 中编码 Protocol Buffers 序列化数据。
- en: Discussion
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Hadoop uses its own serialization framework to serialize and deserialize data
    for performance reasons. An example use of this framework is when map outputs
    are written to disk as part of the shuffle phase. All map outputs must have a
    corresponding Hadoop serialization class that knows how to read and write data
    to a stream. `Writable`s, which are the most commonly used data types in MapReduce,
    have a `WritableSerialization` class that uses the `readFields` and `writeFields`
    methods on the `Writable` interface to perform the serialization.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 使用自己的序列化框架来序列化和反序列化数据，以提高性能。这个框架的一个例子是在洗牌阶段将映射输出写入磁盘。所有映射输出都必须有一个相应的
    Hadoop 序列化类，该类知道如何将数据读取和写入流。`Writable`，是 MapReduce 中最常用的数据类型，有一个 `WritableSerialization`
    类，它使用 `Writable` 接口上的 `readFields` 和 `writeFields` 方法来执行序列化。
- en: SequenceFiles use the same serialization framework to serialize and deserialize
    data within their key/value records, which is why SequenceFiles support `Writable`s
    out of the box. Therefore, encoding a data type into a SequenceFile is just a
    matter of writing your own Hadoop serialization instance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: SequenceFiles 使用相同的序列化框架在它们的键/值记录中序列化和反序列化数据，这就是为什么 SequenceFiles 可以直接支持 `Writable`。因此，将数据类型编码到
    SequenceFile 中只是编写自己的 Hadoop 序列化实例的问题。
- en: Your first step in getting Protocol Buffers to work with SequenceFiles is to
    write your own serialization class. Each serialization class must support serialization
    and deserialization, so let’s start with the serializer, whose job is to write
    records to an output stream.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要使 Protocol Buffers 与 SequenceFiles 一起工作，你的第一步是编写自己的序列化类。每个序列化类都必须支持序列化和反序列化，所以让我们从序列化器开始，其任务是向输出流写入记录。
- en: The following code uses the `MessageLite` class as the type; it’s a superclass
    of all generated Protocol Buffers classes. The `MessageLite` interface provides
    methods to write Protocol Buffers to an output stream and read them from an input
    stream, as you’ll see in the following code:^([[17](#ch03fn18)])
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用 `MessageLite` 类作为类型；它是所有生成的 Protocol Buffers 类的超类。`MessageLite` 接口提供了将
    Protocol Buffers 写入输出流和从输入流读取它们的方法，正如你将在以下代码中看到的：^([17](#ch03fn18))。
- en: '^(17) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java).'
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([17](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java))
    GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java)。
- en: '![](088fig01_alt.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图片](088fig01_alt.jpg)'
- en: 'Next up is the deserializer, whose job is to populate a Protocol Buffers object
    from an input stream. Things are a little trickier here compared to the serializer,
    as Protocol Buffers objects can only be engineered via their builder classes:
    ^([[18](#ch03fn19)])'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是反序列化器，其任务是使用输入流填充 Protocol Buffers 对象。与序列化器相比，这里的事情要复杂一些，因为 Protocol Buffers
    对象只能通过它们的构建器类来构建：^([18](#ch03fn19))。
- en: '^(18) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java).'
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([18](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java))
    GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java)。
- en: '![](088fig02_alt.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图片](088fig02_alt.jpg)'
- en: Now you need to configure Hadoop’s serialization framework to use your new serializer.
    This is accomplished by appending your new serializer to the `io.serializations`
    property. It’s usually good to write a helper method to make this easy for clients.
    The following example shows the standard serializers bundled with Hadoop 2 being
    appended with the serialization class you just created. The source for `ProtobufSerialization`
    isn’t shown here, but all it does is return instances of `ProtobufSerializer`
    and `ProtobufDeserializer`:^([[19](#ch03fn20)])
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要配置 Hadoop 的序列化框架以使用你的新序列化器。这是通过将你的新序列化器附加到 `io.serializations` 属性来完成的。通常，编写一个辅助方法来简化客户端的使用是很好的。以下示例显示了与
    Hadoop 2 一起捆绑的标准序列化器，以及你刚刚创建的序列化类。`ProtobufSerialization` 的源代码在此处未显示，但它只是返回 `ProtobufSerializer`
    和 `ProtobufDeserializer` 的实例：^([19](#ch03fn20))。
- en: '^(19) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java).'
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([19]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/ProtobufSerialization.java).
- en: '[PRE14]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next you need to generate a new Protocol Buffers–encoded SequenceFile. The key
    item here is that you’re calling the `register` method (shown in the preceding
    code) prior to using the SequenceFile writer:^([[20](#ch03fn21)])
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要生成一个新的Protocol Buffers编码的SequenceFile。这里的关键是，在使用SequenceFile编写器之前，你调用了`register`方法（如前所述）：^([[20](#ch03fn21)])
- en: '^(20) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufWriter.java).'
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([20]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufWriter.java).
- en: '[PRE15]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: On to the MapReduce code. What’s great about your new serializer is that the
    map and reduce classes can work with the Protocol Buffers objects directly. Again,
    the key thing here is that you’re configuring the job to make available the Protocol
    Buffers serializer. In the following example you use an identity function to demonstrate
    how Protocol Buffers objects can be used as first-class citizens in MapReduce
    when encoded in SequenceFiles:^([[21](#ch03fn22)])
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是MapReduce代码。你新序列化器的优点在于map和reduce类可以直接与Protocol Buffers对象一起工作。再次强调，这里的关键是你正在配置作业以使Protocol
    Buffers序列化器可用。在下面的示例中，你使用一个身份函数来演示当在SequenceFiles中编码时，Protocol Buffers对象可以作为MapReduce中的第一类公民使用：^([[21](#ch03fn22)])
- en: '^(21) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufMapReduce.java).'
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([21]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/seqfile/protobuf/SequenceFileProtobufMapReduce.java).
- en: '![](090fig01_alt.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](090fig01_alt.jpg)'
- en: 'Now you can write a SequenceFile with Protocol Buffers values, run the identity
    MapReduce job over that data, and then dump the contents of the job output:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以编写一个包含Protocol Buffers值的SequenceFile，在数据上运行一个身份MapReduce作业，然后转储作业输出的内容：
- en: '[PRE16]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next up, we’ll examine additional ways that you can integrate Protocol Buffers
    into MapReduce.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨将Protocol Buffers集成到MapReduce中的其他方法。
- en: 3.3.3\. Protocol Buffers
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3. Protocol Buffers
- en: Google developers invented Protocol Buffers to help them exchange data between
    services written in multiple languages in a compact and efficient manner. Protocol
    Buffers is now Google’s de facto format for data—there are over 48,000 different
    message types defined in Google across more than 12,000.proto files.^([[22](#ch03fn23)])
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Google开发者发明了Protocol Buffers，以帮助他们以紧凑和高效的方式在多种语言编写的服务之间交换数据。现在，Protocol Buffers是Google的事实上的数据格式——在Google中，有超过48,000种不同的消息类型定义在超过12,000个.proto文件中.^([[22](#ch03fn23)])
- en: '^(22) Protocol Buffers usage statistics taken from Google’s Protocol Buffers
    Developer Guide: [http://code.google.com/apis/protocolbuffers/docs/overview.html](http://code.google.com/apis/protocolbuffers/docs/overview.html).'
  id: totrans-289
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([22]) 从Google的Protocol Buffers开发者指南中获取的Protocol Buffers使用统计信息：[http://code.google.com/apis/protocolbuffers/docs/overview.html](http://code.google.com/apis/protocolbuffers/docs/overview.html).
- en: There’s been a ticket open since 2008 with the goal of adding native support
    for Protocol Buffers in MapReduce.^([[23](#ch03fn24)]) As a result, you’ll need
    to turn to alternative methods of working with Protocol Buffers in Hadoop. The
    previous technique covered one approach that can be used, which is to encode Protocol
    Buffers within SequenceFiles. Other options exist, such as using either Elephant
    Bird^([[24](#ch03fn25)]) or Avro, which support Protocol Buffers by wrapping them
    within their own file formats. Ultimately, these are all stop-gap measures until
    we get full support for Protocol Buffers in Hadoop.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 自2008年以来，有一个工单的目标是在MapReduce中添加对Protocol Buffers的原生支持.^([[23](#ch03fn24)]) 因此，你需要转向在Hadoop中处理Protocol
    Buffers的替代方法。之前的技术介绍了一种可以使用的方法，即在SequenceFiles中编码Protocol Buffers。其他选项包括使用Elephant
    Bird^([[24](#ch03fn25)])或Avro，它们通过将Protocol Buffers包装在其自己的文件格式中来支持Protocol Buffers。最终，这些都是权宜之计，直到我们在Hadoop中得到对Protocol
    Buffers的全面支持。
- en: ^(23) See [https://issues.apache.org/jira/browse/MAPREDUCE-377](https://issues.apache.org/jira/browse/MAPREDUCE-377).
  id: totrans-291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（23）参见 [https://issues.apache.org/jira/browse/MAPREDUCE-377](https://issues.apache.org/jira/browse/MAPREDUCE-377)。
- en: ^(24) Using Elephant Bird means you have to use LZOP; ostensibly, it would be
    possible to derive a version of their classes and remove the LZOP dependency,
    but it’s probably worth looking elsewhere if you’re not already using LZOP.
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（24）使用 Elephant Bird 意味着你必须使用 LZOP；表面上，可以推导出他们类的一个版本并移除 LZOP 依赖，但如果你还没有使用 LZOP，那么可能值得在其他地方寻找。
- en: 'There are a number of ways that you can work with Protocol Buffers in Hadoop:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你有几种方式可以在 Hadoop 中使用 Protocol Buffers：
- en: You can serialize Protocol Buffers objects in binary form within SequenceFiles,
    as was shown in the previous technique.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在 SequenceFiles 中以二进制形式序列化 Protocol Buffers 对象，就像在先前的技术中展示的那样。
- en: Elephant Bird ([https://github.com/kevinweil/elephant-bird](https://github.com/kevinweil/elephant-bird)),
    an open source project out of Twitter, supports Protocol Buffers within their
    own binary file format.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elephant Bird ([https://github.com/kevinweil/elephant-bird](https://github.com/kevinweil/elephant-bird))
    是 Twitter 中的一个开源项目，支持他们自己的二进制文件格式中的 Protocol Buffers。
- en: Parquet, a columnar file format that is covered in [section 3.4](#ch03lev1sec4),
    has support for the Protocol Buffers object model and allows you to effectively
    write and read Protocol Buffers into a columnar form.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet，一种列式文件格式，在第 3.4 节中介绍，支持 Protocol Buffers 对象模型，并允许你有效地将 Protocol Buffers
    写入和读取到列式形式。
- en: Of these options, Parquet is the recommended way of working with Protocol Buffers—not
    only does it allow you to work natively with Protocol Buffers, but it also opens
    up the number of tools that can work with your data (due to Parquet’s extensive
    Hadoop tooling support). This chapter’s coverage of Parquet includes a look at
    how Avro can be used with Parquet, and Parquet can be used in a similar way to
    support Protocol Buffers.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些选项中，Parquet 是推荐与 Protocol Buffers 一起工作的方式——它不仅允许你以原生方式与 Protocol Buffers
    一起工作，而且还打开了可以与你的数据一起工作的工具数量（由于 Parquet 的广泛 Hadoop 工具支持）。本章对 Parquet 的介绍包括如何使用
    Avro 与 Parquet 一起使用，以及 Parquet 可以以类似的方式支持 Protocol Buffers。
- en: Thrift is another data format, which, like Protocol Buffers, doesn’t have out-of-the-box
    support with MapReduce. Again, you must rely on other tools to work with Thrift
    data in Hadoop, as you’ll discover in the next section.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Thrift 是另一种数据格式，与 Protocol Buffers 一样，它没有与 MapReduce 的开箱即用支持。同样，你必须依赖其他工具在 Hadoop
    中处理 Thrift 数据，正如你将在下一节中发现的那样。
- en: 3.3.4\. Thrift
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4. Thrift
- en: Facebook created Thrift to help with efficient data representation and transport.
    Facebook uses Thrift for a number of applications, including search, logging,
    and its ads platform.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook 创建了 Thrift 来帮助提高数据表示和传输的效率。Facebook 使用 Thrift 用于包括搜索、日志记录和其广告平台在内的多个应用程序。
- en: The same three options for working with Protocol Buffers also apply to Thrift,
    and once again, the recommendation is to use Parquet as the file format. Head
    on over to the section on Parquet ([section 3.4](#ch03lev1sec4)) to learn more
    about how Parquet integrates with these different data models.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Protocol Buffers 一起工作的相同三个选项也适用于 Thrift，并且再次推荐使用 Parquet 作为文件格式。前往 Parquet
    的章节（第 3.4 节），了解更多关于 Parquet 如何与这些不同的数据模型集成的信息。
- en: Let’s look at what’s likely the most capable data serialization format of all
    our options, Avro.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们所有选项中最有可能是最强大的数据序列化格式，那就是 Avro。
- en: 3.3.5\. Avro
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5. Avro
- en: Doug Cutting created Avro, a data serialization and RPC library, to help improve
    data interchange, interoperability, and versioning in MapReduce. Avro utilizes
    a compact binary data format—which you have the option to compress—that results
    in fast serialization times. Although it has the concept of a schema, similar
    to Protocol Buffers, Avro improves on Protocol Buffers because its code generation
    is optional, and it embeds the schema in the container file format, allowing for
    dynamic discovery and data interactions. Avro has a mechanism to work with schema
    data that uses generic data types (an example of which can be seen in [chapter
    4](kindle_split_014.html#ch04)).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Doug Cutting 创建了 Avro，一个数据序列化和 RPC 库，以帮助改善 MapReduce 中的数据交换、互操作性和版本控制。Avro 使用一种紧凑的二进制数据格式——你可以选择压缩——这导致快速序列化时间。尽管它有类似于
    Protocol Buffers 的模式概念，但 Avro 在 Protocol Buffers 之上进行了改进，因为它的代码生成是可选的，并且它将模式嵌入到容器文件格式中，允许动态发现和数据交互。Avro
    有一种使用通用数据类型（例如，在 [第 4 章](kindle_split_014.html#ch04) 中可以看到的示例）与模式数据一起工作的机制。
- en: The Avro file format is shown in [figure 3.11](#ch03fig11). The schema is serialized
    as part of the header, which makes deserialization simple and loosens restrictions
    around users having to maintain and access the schema outside of the Avro data
    files being interacted with. Each data block contains a number of Avro records,
    and by default is 16 KB in size.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 文件格式如图 3.11 所示。模式作为头部的一部分进行序列化，这使得反序列化变得简单，并放宽了对用户必须在外部维护和访问与 Avro 数据文件交互的模式方面的限制。每个数据块包含多个
    Avro 记录，默认大小为 16 KB。
- en: Figure 3.11\. Avro container file format
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.11\. Avro 容器文件格式
- en: '![](03fig11_alt.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig11_alt.jpg)'
- en: The holy grail of data serialization supports code generation, versioning, and
    compression, and has a high level of integration with MapReduce. Equally important
    is schema evolution, and that’s the reason why Hadoop SequenceFiles aren’t appealing—they
    don’t support the notion of a schema or any form of data evolution.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 数据序列化的圣杯支持代码生成、版本控制和压缩，并且与 MapReduce 有高度集成。同样重要的是模式演变，这也是为什么 Hadoop SequenceFiles
    不吸引人的原因——它们不支持模式或任何形式的数据演变。
- en: In this section you’ll get an overview of Avro’s schema and code-generation
    capabilities, how to read and write Avro container files, and the various ways
    Avro can be integrated with MapReduce. At the end we’ll also look at Avro support
    in Hive and Pig.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解 Avro 的模式和代码生成能力、如何读取和写入 Avro 容器文件，以及 Avro 如何与 MapReduce 集成的各种方式。最后，我们还将探讨
    Avro 在 Hive 和 Pig 中的支持。
- en: Let’s get rolling with a look at Avro’s schema and code generation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看 Avro 的模式和代码生成来开始吧。
- en: Technique 12 Avro’s schema and code generation
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 12 Avro 的模式和代码生成
- en: 'Avro has the notion of generic data and specific data:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 有通用数据和具体数据的概念：
- en: '*Generic data* allows you to work with data at a low level without having to
    understand schema specifics.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通用数据* 允许你在不了解模式具体信息的情况下以低级别处理数据。'
- en: '*Specific data* allows you to work with Avro using code-generated Avro primitives,
    which supports a simple and type-safe method of working with your Avro data.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*具体数据* 允许你使用代码生成的 Avro 原语与 Avro 一起工作，这支持了一种简单且类型安全的处理 Avro 数据的方法。'
- en: This technique looks at how to work with specific data in Avro.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术探讨了如何在 Avro 中处理具体数据。
- en: Problem
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to define an Avro schema and generate code so you can work with your
    Avro records in Java.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 你想定义一个 Avro 模式并生成代码，以便在 Java 中处理你的 Avro 记录。
- en: Solution
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Author your schema in JSON form, and then use Avro tools to generate rich APIs
    to interact with your data.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以 JSON 格式编写你的模式，然后使用 Avro 工具生成丰富的 API 以与你的数据交互。
- en: Discussion
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'You can use Avro in one of two ways: either with code-generated classes or
    with its generic classes. In this technique we’ll work with the code-generated
    classes, but you can see an example of how Avro’s generic records are used in
    technique 29 in [chapter 4](kindle_split_014.html#ch04).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用两种方式之一使用 Avro：要么使用代码生成的类，要么使用其通用类。在本技术中，我们将使用代码生成的类，但你可以在第 4 章的第 29 个技巧中看到
    Avro 通用记录的使用示例。[章节链接](kindle_split_014.html#ch04).
- en: '|  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Getting Avro
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 获取 Avro
- en: The appendix contains instructions on how to get your hands on Avro.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 附录中包含如何获取 Avro 的说明。
- en: '|  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the code-generated approach, everything starts with a schema. The first step
    is to create an Avro schema to represent an entry in the stock data:^([[25](#ch03fn26)])
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码生成方法中，一切从模式开始。第一步是创建一个 Avro 模式来表示股票数据中的一个条目：^([[25](#ch03fn26)])
- en: '^(25) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/stock.avsc](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/stock.avsc).'
  id: totrans-327
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(25) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/stock.avsc](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/stock.avsc).
- en: '[PRE17]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Avro supports code generation for schema data as well as RPC messages (which
    aren’t covered in this book). To generate Java code for a schema, use the Avro
    tools JAR as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 支持对模式数据和 RPC 消息（本书未涵盖）进行代码生成。要为模式生成 Java 代码，请使用以下 Avro 工具 JAR：
- en: '![](094fig01_alt.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](094fig01_alt.jpg)'
- en: Generated code will be put into the `hip.ch3.avro.gen` package. Now that you
    have generated code, how do you use it to read and write Avro container files?^([[26](#ch03fn27)])
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的代码将被放入 `hip.ch3.avro.gen` 包中。现在你已经生成了代码，如何使用它来读取和写入 Avro 容器文件？^([[26](#ch03fn27)])
- en: '^(26) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileWrite.java).'
  id: totrans-332
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(26) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileWrite.java).
- en: Listing 3.5\. Writing Avro files from outside of MapReduce
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5\. 从MapReduce外部写入Avro文件
- en: '![](095fig01_alt.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![095fig01_alt.jpg](095fig01_alt.jpg)'
- en: As you see, you can specify the compression codec that should be used to compress
    the data. In this example you’re using Snappy, which, as shown in [chapter 4](kindle_split_014.html#ch04),
    is the fastest codec for reads and writes.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，你可以指定用于压缩数据的压缩编解码器。在这个例子中，你使用的是Snappy，正如[第4章](kindle_split_014.html#ch04)中所示，这是读写速度最快的编解码器。
- en: 'The following code example shows how you can marshal a `Stock` object from
    a line in the input file. As you can see, the generated `Stock` class is a POJO
    with a bunch of setters (and matching getters):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例显示了如何从输入文件的一行中序列化一个`Stock`对象。正如你所见，生成的`Stock`类是一个POJO，包含许多setter（以及相应的getter）：
- en: '[PRE18]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, how about reading the file you just wrote?^([[27](#ch03fn28)])
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于读取你刚刚写入的文件呢？^([[27](#ch03fn28)])
- en: '^(27) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileRead.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileRead.java).'
  id: totrans-339
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(27) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileRead.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroStockFileRead.java).
- en: '![](095fig02_alt.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![095fig02_alt.jpg](095fig02_alt.jpg)'
- en: 'Go ahead and execute this writer and reader pair:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 继续执行这个写入器和读取器对：
- en: '![](096fig01_alt.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![096fig01_alt.jpg](096fig01_alt.jpg)'
- en: 'Avro comes bundled with some tools to make it easy to examine the contents
    of Avro files. To view the contents of an Avro file as JSON, simply run this command:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Avro附带了一些工具，可以轻松检查Avro文件的内容。要查看Avro文件的内容作为JSON，只需运行此命令：
- en: '[PRE19]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This assumes that the file exists on the local filesystem. Similarly, you can
    get a JSON representation of your Avro file with the following command:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设文件存在于本地文件系统中。同样，你可以使用以下命令获取Avro文件的JSON表示：
- en: '[PRE20]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can run the Avro tools without any options to view all the tools you can
    use:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以不使用任何选项运行Avro工具，以查看你可以使用的所有工具：
- en: '[PRE21]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'One shortcoming of the `tojson` tool is that it doesn’t support reading data
    in HDFS. I’ve therefore bundled a utility with the book’s code called AvroDump
    that can dump a text representation of Avro data in HDFS, which we’ll use shortly
    to examine the output of Avro MapReduce jobs:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '`tojson`工具的一个缺点是它不支持在HDFS中读取数据。因此，我在本书的代码中捆绑了一个名为AvroDump的实用程序，它可以导出Avro数据在HDFS中的文本表示，我们将很快使用它来检查Avro
    MapReduce作业的输出：'
- en: '[PRE22]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This utility supports multiple files (they need to be CSV-delimited) and globbing,
    so you can use wildcards. The following example shows how you would dump out the
    contents of a MapReduce job that produced Avro output into a directory called
    mr-output-dir:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 此实用程序支持多个文件（它们需要是CSV分隔的）和通配符匹配，因此你可以使用通配符。以下示例显示了如何将MapReduce作业生成的Avro输出内容导出到名为mr-output-dir的目录中：
- en: '[PRE23]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let’s see how Avro integrates with MapReduce.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Avro如何与MapReduce集成。
- en: Technique 13 Selecting the appropriate way to use Avro in MapReduce
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧13 在MapReduce中选择适当的Avro使用方式
- en: Avro supports more than one way to work with your Avro data in MapReduce. This
    technique enumerates the different ways you can work with your data and provides
    guidance on which situations call for which approach.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Avro支持在MapReduce中处理你的Avro数据的方式不止一种。这项技术列举了你可以使用数据的不同方式，并提供了关于在何种情况下应采用哪种方法的指导。
- en: Problem
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use Avro in your MapReduce job, but it’s unclear which of the available
    integration options you should choose.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce作业中使用Avro，但不确定应该选择哪种可用的集成选项。
- en: Solution
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Learn more about each integration option, and pick the one best suited for your
    use case.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 了解每个集成选项的更多信息，并选择最适合你用例的一个。
- en: Discussion
  id: totrans-360
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'There are three ways that you can use Avro in MapReduce, and the specific details
    on how to use each are discussed in techniques that follow this one. These are
    the three approaches:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在MapReduce中使用Avro的三种方式，每种方式的具体使用方法将在接下来的技巧中讨论。这些是三种方法：
- en: '***Mixed-mode*** —Appropriate when you want to mix Avro data with non-Avro
    data in your job'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***混合模式*** —当你想在作业中混合Avro数据和非Avro数据时适用'
- en: '***Record-based*** —Useful when data is supplied in a non-key/value way'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***基于记录的*** —当数据以非键/值方式提供时很有用'
- en: '***Key/value-based*** —For when your data must fit a specific model'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于键/值** — 当你的数据必须符合特定模型时'
- en: Let’s cover each method in more detail.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地介绍每种方法。
- en: Mixed-mode
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混合模式
- en: 'This use case is for instances where any one of these conditions holds true:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这种用例适用于以下任一条件成立的情况：
- en: Your mapper input data isn’t in Avro form.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的 mapper 输入数据不是 Avro 格式。
- en: You don’t want to emit intermediate data between your mappers and reducers using
    Avro.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不希望在 mapper 和 reducer 之间使用 Avro 发射中间数据。
- en: Your job output data isn’t in Avro form.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的作业输出数据不是 Avro 格式。
- en: In any of these cases, the Avro mapper and reducer classes won’t help you, as
    they are designed with the assumption that Avro data is flowing end-to-end in
    your MapReduce job. In this case, you’ll want to use the regular MapReduce mapper
    and reducer classes and construct your job in a way that allows you to still work
    with Avro data.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何这些情况下，Avro mapper 和 reducer 类都不会帮助你，因为它们的设计假设 Avro 数据在 MapReduce 作业中端到端流动。在这种情况下，你将想要使用常规的
    MapReduce mapper 和 reducer 类，并以一种允许你仍然处理 Avro 数据的方式构建你的作业。
- en: Record-based
  id: totrans-372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于记录
- en: Avro data is record-based, which results in a impedance mismatch when compared
    with MapReduce, which is key/value-based. To support Avro’s record-based roots,
    Avro comes bundled with a mapper class that isn’t key/value-based, and instead
    only supplies derived classes with a single record.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 数据是基于记录的，与基于键/值（key/value）的 MapReduce 相比，这会导致阻抗不匹配。为了支持 Avro 的基于记录的根源，Avro
    随带了一个 mapper 类，它不是基于键/值的，而是只向派生类提供一个记录。
- en: Key/value-based
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于键/值
- en: If your Avro data internally follows a key/value structure, you can use some
    Avro-supplied mapper classes that will transform your Avro records and supply
    them in a key/value form to your mapper. With this method, you’re restricted to
    schemas that literally have “key” and “value” elements.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 Avro 数据内部遵循键/值结构，你可以使用一些 Avro 提供的 mapper 类，这些类将转换你的 Avro 记录，并以键/值形式将它们提供给
    mapper。使用这种方法，你将限于具有“键”和“值”元素的架构。
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Selecting the right level of integration with Avro is a function of your inputs
    and outputs, and how you want to work with data inside of Avro. This technique
    examined three ways of integrating with Avro so that you can pick the right method
    for your use case. In the following techniques, we’ll look at how to use each
    of these integration methods in your MapReduce jobs.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 选择与 Avro 集成的正确级别取决于你的输入和输出，以及你如何在 Avro 内部处理数据。这项技术探讨了三种与 Avro 集成的途径，以便你可以为你的用例选择正确的方法。在以下技术中，我们将探讨如何在
    MapReduce 作业中使用这些集成方法。
- en: Technique 14 Mixing Avro and non-Avro data in MapReduce
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术第 14 章：在 MapReduce 中混合 Avro 和非 Avro 数据
- en: This level of Avro integration in MapReduce is suitable in cases where you have
    non-Avro input and generate Avro outputs, or vice versa, in which case the Avro
    mapper and reducer classes aren’t suitable. In this technique, we’ll look at how
    to work in a mixed-mode fashion with Avro.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MapReduce 中，这种级别的 Avro 集成适用于你有非 Avro 输入并生成 Avro 输出，或反之亦然的情况，在这种情况下，Avro mapper
    和 reducer 类不适用。在这项技术中，我们将探讨如何以混合模式与 Avro 一起工作。
- en: Problem
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use Avro in a mixed mode in your MapReduce job, which isn’t supported
    by the Avro-bundled mapper and reducer classes.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在 MapReduce 作业中使用 Avro 的混合模式，而 Avro 随带的 mapper 和 reducer 类不支持这种模式。
- en: Solution
  id: totrans-382
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use low-level methods to set up your job and drive Avro data through your Map-Reduce
    job using the regular Hadoop mapper and reducer classes.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 使用低级方法设置你的作业，并使用常规 Hadoop mapper 和 reducer 类通过 Map-Reduce 作业驱动 Avro 数据。
- en: Discussion
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Avro comes with some mapper and reducer classes that you can subclass to work
    with Avro. They’re useful in situations where you want your mappers and reducers
    to exchange Avro objects. But if you don’t have a requirement to pass Avro objects
    between your map and reduce tasks, you’re better off using the Avro input and
    output format classes directly, as you’ll see in the following code, which produces
    an average of all of the opening stock values.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 随带了一些 mapper 和 reducer 类，你可以对它们进行子类化以处理 Avro。在你想让你的 mapper 和 reducer 交换
    Avro 对象的情况下，它们很有用。但是，如果你没有在 map 和 reduce 任务之间传递 Avro 对象的要求，你最好直接使用 Avro 输入和输出格式类，正如你将在以下代码中看到的那样，它产生所有开盘价值的平均值。
- en: We’ll start with a look at the job configuration. Your job is to consume stock
    data and produce stock averages, both in Avro formats.^([[28](#ch03fn29)]) To
    do this, you need to set the job configuration with the schema information for
    both schemas. You also need to specify Avro’s input and output format classes:^([[29](#ch03fn30)])
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看一下作业配置。你的任务是消费股票数据并生成股票平均值，这些数据都将以Avro格式输出.^([[28](#ch03fn29)]) 要完成这个任务，你需要设置包含两个模式信息的作业配置。你还需要指定Avro的输入和输出格式类:^([[29](#ch03fn30)])
- en: ^(28) Even though this technique is about mixing Avro and non-Avro data together
    in your jobs, I show Avro being used throughout the job so that you can pick which
    aspect you wish to integrate into your job. For example, if you have text inputs
    and Avro outputs, you’d use a regular `TextInputFormat`, and set the Avro output
    format.
  id: totrans-387
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(28) 尽管这项技术是关于在你的作业中将Avro和非Avro数据混合在一起，但我展示了在整个作业中使用Avro，这样你可以选择你希望集成到作业中的哪个方面。例如，如果你有文本输入和Avro输出，你会使用一个常规的`TextInputFormat`，并设置Avro输出格式。
- en: '^(29) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java).'
  id: totrans-388
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(29) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java).
- en: '![](100fig01_alt.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](100fig01_alt.jpg)'
- en: Next up is the `Map` class. The entire Avro record is supplied as the input
    key to your map function, because Avro supports records, not key/value pairs (although,
    as you’ll see later, Avro does have a way to provide data to your map function
    using key/value pairs if your Avro schema has fields called `key` and `value`).
    From an implementation perspective, your `map` function extracts the necessary
    fields from the stock record and emits them to the reducer, with the stock symbol
    and the opening stock price as the key/value pairs:^([[30](#ch03fn31)])
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`Map`类。整个Avro记录作为输入键提供给你的映射函数，因为Avro支持记录，而不是键/值对（尽管，如你稍后所见，Avro确实有一种方式可以通过键/值对提供数据给你的映射函数，如果你的Avro模式中有名为`key`和`value`的字段）。从实现的角度来看，你的`map`函数从股票记录中提取必要的字段并将它们以股票符号和开盘价作为键/值对的形式输出到reducer:^([[30](#ch03fn31)])
- en: '^(30) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java).'
  id: totrans-391
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(30) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java).
- en: '![](100fig02_alt.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](100fig02_alt.jpg)'
- en: '|  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Why is the “old” MapReduce API being used?
  id: totrans-394
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么使用“旧”的MapReduce API？
- en: You may have noticed that the example in this technique uses the older `org.apache.hadoop.mapred`
    API. This is because the `AvroInputFormat` and `AvroOutputFormat` classes used
    in this technique only support the old API.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，这个技术示例使用了较旧的`org.apache.hadoop.mapred` API。这是因为在这个技术中使用的`AvroInputFormat`和`AvroOutputFormat`类只支持旧API。
- en: '|  |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Finally, the reduce function sums together all of the stock prices for each
    stock and outputs an average price:^([[31](#ch03fn32)])
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，reduce函数将每个股票的所有股票价格加在一起并输出平均价格:^([[31](#ch03fn32)])
- en: '^(31) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java).'
  id: totrans-398
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(31) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroMixedMapReduce.java).
- en: '![](101fig01_alt.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](101fig01_alt.jpg)'
- en: 'You can run the MapReduce code as follows:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下方式运行MapReduce代码：
- en: '[PRE24]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Your MapReduce job is outputting a different Avro object (`StockAvg`) from
    the job input. You can verify that the job produced the output you expected by
    writing some code (not listed) to dump your Avro objects:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 你的MapReduce作业正在输出与作业输入不同的Avro对象（`StockAvg`）。你可以通过编写一些代码（未列出）来转储你的Avro对象来验证作业是否生成了你预期的输出：
- en: '[PRE25]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Summary
  id: totrans-404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique is useful in cases where you don’t want intermediary map outputs
    in Avro form, or if you have non-Avro inputs or outputs. Next we’ll look at the
    Avro-native way of working with data in MapReduce.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在以下情况下很有用：你不想在Avro格式中保留中间映射输出，或者你有非Avro输入或输出。接下来，我们将探讨在MapReduce中使用Avro原生方式处理数据。
- en: Technique 15 Using Avro records in MapReduce
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧15 在MapReduce中使用Avro记录
- en: Avro isn’t a native key/value serialization format, unlike SequenceFile, so
    it can require a little shoehorning to get it to work with MapReduce. In this
    technique you’ll examine the Avro-specific mapper and reducer classes that expose
    a record-based interface you can use to input and output data.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 与SequenceFile不同，Avro不是一个本地的键/值序列化格式，因此它可能需要一些调整才能与MapReduce一起工作。在这个技术中，你将检查特定的Avro
    mapper和reducer类，这些类提供了一个基于记录的接口，你可以用它来输入和输出数据。
- en: Problem
  id: totrans-408
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use Avro end-to-end in your MapReduce job, and you also wish to
    interact with your input and output data in record-oriented form.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce作业中从头到尾使用Avro，并且你还希望以记录形式与你的输入和输出数据交互。
- en: Solution
  id: totrans-410
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Extend the `AvroMapper` and `AvroReducer` classes to implement your MapReduce
    job.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展`AvroMapper`和`AvroReducer`类以实现你的MapReduce作业。
- en: Discussion
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Avro comes with two classes that abstract away the key/value nature of MapReduce
    and instead expose a record-based API. In this technique you’ll implement the
    same MapReduce job as in the prior technique (calculating the average open prices
    for each stock symbol), and use Avro throughout the job.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Avro提供了两个类，它们抽象了MapReduce的键/值特性，而是暴露了一个基于记录的API。在这个技术中，你将实现与先前技术相同的MapReduce作业（计算每个股票代码的平均开盘价），并在整个作业中使用Avro。
- en: 'First, let’s look at the `Mapper` class, which will extend `AvroMapper`: ^([[32](#ch03fn33)])'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看`Mapper`类，它将扩展`AvroMapper`：^([[32](#ch03fn33)])
- en: '^(32) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java).'
  id: totrans-415
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(32) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java).
- en: '![](102fig01_alt.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![图片](102fig01_alt.jpg)'
- en: The first thing to notice is that there are two types defined in the class definition,
    not four as is the norm in MapReduce. The `AvroMapper` abstracts away the key/value
    traits of the mapper inputs and outputs, replacing each with a single type.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，要注意的是，在类定义中定义了两种类型，而不是MapReduce中常见的四种。`AvroMapper`抽象了mapper输入和输出的键/值特性，用单一类型替换了每个特性。
- en: If you had a map-only job, the types that you’d define would be the input and
    output types. But if you’re running a full-blown MapReduce job, you’ll need to
    use the `Pair` class so that you can define the map output key/value pairs. The
    `Pair` class requires that an Avro schema exists for the key and value parts,
    which is why the `Utf8` class is used instead of a straight Java string.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个只映射的作业，你将定义的类型将是输入和输出类型。但是，如果你正在运行一个完整的MapReduce作业，你需要使用`Pair`类，这样你就可以定义映射输出键/值对。`Pair`类要求键和值部分存在Avro模式，这就是为什么使用`Utf8`类而不是直接的Java字符串。
- en: Let’s now take a peek at the AvroReducer implementation. This time there are
    three types you need to define—the map output key and value types, and the reducer
    output type:^([[33](#ch03fn34)])
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看AvroReducer的实现。这次你需要定义三种类型——映射输出键和值类型，以及减少输出类型:^([[33](#ch03fn34)])
- en: '^(33) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java).'
  id: totrans-420
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(33) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java).
- en: '![](103fig01_alt.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![图片](103fig01_alt.jpg)'
- en: Now you can plumb it all together in the driver. Here you’ll define the input
    and output types and the desired output compression, if any:^([[34](#ch03fn35)])
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以一起在驱动程序中配置它。在这里，你将定义输入和输出类型以及所需的输出压缩（如果有）:^([[34](#ch03fn35)])
- en: '^(34) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java).'
  id: totrans-423
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(34) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroRecordMapReduce.java).
- en: '[PRE26]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Done! Give it a whirl, and check the outputs after the job completes:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 完成！试一试，并在作业完成后检查输出：
- en: '[PRE27]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Summary
  id: totrans-427
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique is handy in situations where you want to keep your data in Avro
    form throughout the MapReduce job, and you don’t have a requirement that your
    input or output data be key/value-based.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在你想在整个MapReduce作业中保持数据以Avro格式，并且没有要求你的输入或输出数据基于键/值的情况下非常有用。
- en: But what if you do need your data be key/value-based, and you still want to
    use Avro goodies such as compact serialization size and built-in compression?
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你确实需要基于键/值的数据，并且仍然想使用 Avro 的优点，如紧凑的序列化大小和内置压缩呢？
- en: Technique 16 Using Avro key/value pairs in MapReduce
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 16 在 MapReduce 中使用 Avro 键/值对
- en: MapReduce’s native data model is key/value pairs, and as I’ve mentioned earlier,
    Avro’s is record-based. Avro doesn’t have native support for key/value data, but
    some helper classes exist in Avro to help model key/value data and to use this
    natively in MapReduce.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 的原生数据模型是键/值对，正如我之前提到的，Avro 的是基于记录的。Avro 没有原生支持键/值数据，但 Avro 中存在一些辅助类来帮助建模键/值数据，并在
    MapReduce 中原生使用。
- en: Problem
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use Avro as a data format and container, but you want to model your
    data using key/value pairs in Avro and use them as native key/value pairs in MapReduce.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望使用 Avro 作为数据格式和容器，但你想在 Avro 中使用键/值对来建模你的数据，并将它们用作 MapReduce 中的原生键/值对。
- en: Solution
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `AvroKeyValue`, `AvroKey`, and `AvroValue` classes to work with Avro
    key/value data.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `AvroKeyValue`、`AvroKey` 和 `AvroValue` 类来处理 Avro 键/值数据。
- en: Discussion
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Avro has an `AvroKeyValue` class that encapsulates a generic Avro record containing
    two records named `key` and `value`. `AvroKeyValue` serves as a helper class so
    that you can easily read and write key/value data. The types of these records
    are defined by you.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 有一个 `AvroKeyValue` 类，它封装了一个包含两个名为 `key` 和 `value` 的记录的泛型 Avro 记录。`AvroKeyValue`
    作为一个辅助类，使你能够轻松地读取和写入键/值数据。这些记录的类型由你定义。
- en: In this technique you’ll repeat the average stock MapReduce job, but this time
    using Avro’s key/value framework. You’ll first need to generate the input data
    for your job. In this case, we’ll put the stock symbol in the key and the `Stock`
    object in the value:^([[35](#ch03fn36)])
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，你将重复平均股票 MapReduce 作业，但这次使用 Avro 的键/值框架。你首先需要为你的作业生成输入数据。在这种情况下，我们将股票代码放在键中，将
    `Stock` 对象放在值中：^([[35](#ch03fn36)])
- en: '^(35) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueFileWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueFileWrite.java).'
  id: totrans-439
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([35](#ch03fn36)) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueFileWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueFileWrite.java)。
- en: '![](104fig01_alt.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](104fig01_alt.jpg)'
- en: 'Go ahead and generate a file in HDFS containing the stock data in key/value
    format:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 继续在 HDFS 中生成一个包含股票数据的键/值格式文件：
- en: '[PRE28]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you’re curious to know the Avro schema definition of the file you just generated,
    use the tip highlighted in technique 12 to extract the schema from the file. In
    addition, you can use the AvroDump utility to show the contents of the file:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你好奇你刚刚生成的文件的 Avro 模式定义，请使用技巧 12 中突出显示的提示从文件中提取模式。此外，你可以使用 AvroDump 实用程序来显示文件的内容：
- en: '[PRE29]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now for some MapReduce code—you’ll define your mapper, reducer, and driver in
    one shot:^([[36](#ch03fn37)])
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候展示一些 MapReduce 代码了——你将一次性定义你的映射器、归约器和驱动程序：^([[36](#ch03fn37)])
- en: '^(36) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueMapReduce.java).'
  id: totrans-446
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([36](#ch03fn37)) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/avro/AvroKeyValueMapReduce.java)。
- en: '[PRE30]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, the `AvroKey` and `AvroValue` wrappers are used to supply input
    data in the mapper, as well as output data in the reducer. The neat thing here
    is that Avro is smart enough to support Hadoop `Writable` objects and automatically
    convert them into their Avro counterparts, which is why you don’t need to tell
    Avro the schema type of the output key.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`AvroKey` 和 `AvroValue` 包装器用于在映射器中提供输入数据，以及在归约器中的输出数据。这里很酷的一点是，Avro 足够智能，可以支持
    Hadoop `Writable` 对象，并自动将它们转换为它们的 Avro 对应物，这就是为什么你不需要告诉 Avro 输出键的模式类型。
- en: 'You can run the MapReduce job with the following command:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令运行 MapReduce 作业：
- en: '[PRE31]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And again, you can view the output with the AvroDump tool:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你可以使用 AvroDump 工具查看输出：
- en: '[PRE32]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Summary
  id: totrans-453
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This concludes our coverage of the three Avro approaches for working with your
    data in MapReduce. Each of the methods is suited to a particular task, and you
    can select whichever one works best for your needs.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对在 MapReduce 中使用 Avro 的三种方法的介绍。每种方法都适合特定的任务，你可以选择最适合你需求的任何一种。
- en: Let’s wrap up our Avro and MapReduce coverage by looking at how you can customize
    ordering characteristics of Avro data in MapReduce.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下关于 Avro 和 MapReduce 的内容，看看如何在 MapReduce 中自定义 Avro 数据的排序特性。
- en: Technique 17 Controlling how sorting worksin MapReduce
  id: totrans-456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 17 控制在 MapReduce 中排序的工作方式
- en: If you decide to use Avro data as intermediary map outputs, you may be wondering
    what control you have over how partitioning, sorting, and grouping work.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定使用 Avro 数据作为中间映射输出，你可能想知道你如何控制分区、排序和分组的工作方式。
- en: Problem
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want control over how MapReduce sorts your reducer inputs.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望控制 MapReduce 对你的 reducer 输入的排序方式。
- en: Solution
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Modify the Avro schema to alter ordering behavior.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 修改 Avro 架构以改变排序行为。
- en: Discussion
  id: totrans-462
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'If an Avro object is used as the key output in a mapper, the following happens
    by default:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个 Avro 对象被用作 mapper 的键输出，默认情况下会发生以下情况：
- en: All the fields in the Avro object are used for partitioning, sorting, and grouping.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Avro 对象中的所有字段都用于分区、排序和分组。
- en: The fields are ordered using their ordinal position in the schema. This means
    that if you have a schema with two elements, the first element in the schema is
    used for sorting first, followed by the second element.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段将按照它们在架构中的序号位置进行排序。这意味着如果你有一个包含两个元素的架构，架构中的第一个元素将首先用于排序，然后是第二个元素。
- en: Within an element, sorting occurs using comparisons that are specific to the
    type. So if strings are being compared, the sorting will be lexicographical, and
    if numbers are being compared, numerical comparison is used.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个元素内部，排序是通过特定于该类型的比较来进行的。因此，如果正在比较字符串，排序将是字典序的，如果正在比较数字，则使用数值比较。
- en: 'Some of this behavior can be changed. The following is a modified version of
    the `Stock` schema:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 一些这种行为是可以改变的。以下是对 `Stock` 架构的修改版本：
- en: '[PRE33]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You can modify the sorting behavior for a field by decorating it with an `order`
    attribute and specifying that descending order should be used. Alternatively,
    you can exclude a field from partitioning, sorting, and grouping by setting the
    order to `ignore`.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过添加一个 `order` 属性并指定使用降序来修改字段的排序行为。或者，你可以通过将顺序设置为 `ignore` 来排除字段进行分区、排序和分组。
- en: Note that these are schema-wide settings, and there’s no easy way to specify
    custom partition/sort/group settings on a per-job basis. You can go ahead and
    write your own partition, sort, and group functions (just like you would for a
    `Writable`), but it would be useful if Avro had helper functions to simplify this
    process.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些是架构级别的设置，没有简单的方法可以指定每个作业的定制分区/排序/分组设置。你可以继续编写自己的分区、排序和分组函数（就像为 `Writable`
    做的那样），但如果 Avro 有助于简化此过程的辅助函数将会很有用。
- en: Technique 18 Avro and Hive
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 18 Avro 和 Hive
- en: It wasn’t until recently that the Hive project had built-in support for Avro.
    This technique looks at how you can work with Avro data in Hive.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，Hive 项目才内置了对 Avro 的支持。这项技术探讨了如何在 Hive 中处理 Avro 数据。
- en: Problem
  id: totrans-473
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with Avro data in Hive.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在 Hive 中处理 Avro 数据。
- en: Solution
  id: totrans-475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Hive’s Avro Serializer/Deserializer.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hive 的 Avro 序列化/反序列化器。
- en: Discussion
  id: totrans-477
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Hive version 0.9.1 and newer come bundled with an Avro SerDe, short for Serializer/Deserializer,
    which allows Hive to read data in from a table and write it back out to a table.
    The appendix has instructions on how to install Hive.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 版本 0.9.1 及更高版本附带了一个 Avro SerDe（序列化/反序列化器），这使得 Hive 能够从表中读取数据并将其写回到表中。附录中包含了如何安装
    Hive 的说明。
- en: 'You need to copy the Avro schemas bundled with this book into HDFS, and also
    create a directory containing some example Avro stock records:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将本书附带捆绑的 Avro 架构复制到 HDFS 中，并创建一个包含一些示例 Avro 股票记录的目录：
- en: '[PRE34]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, fire up the Hive console and create an external Hive table for the directory
    you just created. You also need to specify the location of the Avro schema in
    HDFS. Replace `YOUR-HDFS-USERNAME` with your HDFS username:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，启动 Hive 控制台并为刚刚创建的目录创建一个外部 Hive 表。你还需要指定 HDFS 中 Avro 架构的位置。将 `YOUR-HDFS-USERNAME`
    替换为你的 HDFS 用户名：
- en: '[PRE35]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'AvroSerDe actually supports three ways to define a schema for an Avro table—for
    this technique, I picked the method that you’ll most likely want to use in production,
    but for more details on the other ways to specify a schema, refer to the AvroSerDe
    site: [https://cwiki.apache.org/confluence/display/Hive/AvroSerDe](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe).'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: AvroSerDe 实际上支持三种定义 Avro 表模式的方法——对于这个技巧，我选择了你可能在生产中最可能想要使用的方法，但有关指定模式的其他方式的更多详细信息，请参阅
    AvroSerDe 网站：[https://cwiki.apache.org/confluence/display/Hive/AvroSerDe](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe).
- en: 'Just like with any Hive table, you can query Hive to describe the schema for
    a table:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何 Hive 表一样，你可以查询 Hive 来描述表的模式：
- en: '[PRE36]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Run a query to verify that everything’s working. The following Hive Query Language
    (HiveQL) will count the number of stock records for each stock symbol:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一个查询以验证一切是否正常工作。以下 Hive 查询语言（HiveQL）将计算每个股票符号的股票记录数：
- en: '[PRE37]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'What if you wanted to write data to an Avro-backed Hive table? The following
    example shows how you would copy a subset of the records in the stocks table and
    insert them into a new table. This example also highlights how you’d use the Snappy
    compression codec for any writes into the new table:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将数据写入由 Avro 支持的 Hive 表中？以下示例展示了如何复制股票表中的记录子集并将其插入到新表中。此示例还突出了如何使用 Snappy
    压缩编解码器对写入新表中的任何数据进行压缩：
- en: '[PRE38]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: For more details on Hive, please refer to [chapter 9](kindle_split_022.html#ch09).
    Next we’ll look at how you’d perform the same sequence of actions in Pig.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Hive 的更多详细信息，请参阅[第 9 章](kindle_split_022.html#ch09)。接下来，我们将看看如何在 Pig 中执行相同的操作序列。
- en: Technique 19 Avro and Pig
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 19 Avro 和 Pig
- en: Much like Hive, Pig also has built-in support for Avro, which is covered in
    this technique.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Hive 一样，Pig 也内置了对 Avro 的支持，这在本技巧中有所介绍。
- en: Problem
  id: totrans-493
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to read and write Avro data using Pig.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用 Pig 读写 Avro 数据。
- en: Solution
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `AvroStorage` class in Pig’s Piggy Bank library.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pig 的 Piggy Bank 库中使用 `AvroStorage` 类。
- en: Discussion
  id: totrans-497
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Piggy Bank is a library that contains a useful collection of Pig utilities,
    one of which is the `AvroStorage` class you can use to read and write Avro data
    in HDFS. In this technique you’ll mirror the steps you took in the previous Hive
    technique—you’ll read in some stock data, perform some simple aggregations, and
    store some filtered data back into HDFS.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: Piggy Bank 是一个包含有用 Pig 工具集合的库，其中之一是你可以用来在 HDFS 中读写 Avro 数据的 `AvroStorage` 类。在这个技巧中，你将复制之前在
    Hive 技巧中采取的步骤——你将读取一些股票数据，执行一些简单的聚合，并将一些过滤后的数据存储回 HDFS。
- en: 'Before you get started, load some Avro stock data into a directory in HDFS:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，将一些 Avro 股票数据加载到 HDFS 中的一个目录中：
- en: '[PRE39]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In Pig-land, your first step is to register the JARs required for `AvroStorage`
    to work. You may have to hunt down the specific location of the JARs bundled with
    the Hadoop distribution that you’re using. The locations in the following code
    assume that Apache Hadoop and Pig were installed under /usr/local:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pig-land 中，你的第一步是注册 `AvroStorage` 工作所需的 JAR 文件。你可能需要找到你使用的 Hadoop 分发版中捆绑的
    JAR 文件的特定位置。以下代码中的位置假设 Apache Hadoop 和 Pig 安装在 /usr/local 下：
- en: '[PRE40]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, load the stocks into a Pig relation and then display the schema details
    using the `LOAD` and `DESCRIBE` operators:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将股票加载到 Pig 关系中，然后使用 `LOAD` 和 `DESCRIBE` 操作符显示模式细节：
- en: '[PRE41]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Notice that you didn’t have to supply details about the Avro schema. That’s
    because the Avro container format you used had the schema embedded in the header.
    If your files don’t have the schema embedded, `AvroStorage` can still support
    your data, but you’ll need to upload the Avro schema to HDFS (like you did in
    Hive) and use the “schema_file” option—check out the Pig documentation for more
    details.^([[37](#ch03fn38)])
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你不必提供关于 Avro 模式的详细信息。这是因为你使用的 Avro 容器格式在头部中嵌入了模式。如果你的文件没有嵌入模式，`AvroStorage`
    仍然可以支持你的数据，但你需要将 Avro 模式上传到 HDFS（就像你在 Hive 中做的那样）并使用“schema_file”选项——查看 Pig 文档以获取更多详细信息.^([[37](#ch03fn38)])
- en: '^(37) More Avro and Pig integration details are available on the `AvroStorage`
    page: [https://cwiki.apache.org/confluence/display/PIG/AvroStorage](https://cwiki.apache.org/confluence/display/PIG/AvroStorage).'
  id: totrans-506
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（37）更多 Avro 和 Pig 集成细节可以在 `AvroStorage` 页面上找到：[https://cwiki.apache.org/confluence/display/PIG/AvroStorage](https://cwiki.apache.org/confluence/display/PIG/AvroStorage).
- en: 'To validate that Avro and Pig are working together, you can perform a simple
    aggregation and count the number of stock records for each stock symbol:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 Avro 和 Pig 是否协同工作，你可以执行一个简单的聚合操作，并计算每个股票符号的股票记录数：
- en: '[PRE42]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following example shows how you can write out Avro data in Pig. The example
    filters the Google stocks from the input data and writes them into a new output
    directory in HDFS. This also shows how you can compress job outputs using Snappy:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了您如何在 Pig 中写出 Avro 数据。该示例从输入数据中过滤出谷歌股票，并将它们写入 HDFS 中的新输出目录。这也展示了您如何使用
    Snappy 压缩作业输出：
- en: '[PRE43]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: When writing Avro data to HDFS, you’ll need to specify the Avro schema of the
    data you’re persisting. The preceding example uses the `data` option to tell `AvroStorage`
    to use the Avro schema embedded in files under your input directory.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 当将 Avro 数据写入 HDFS 时，您需要指定您要持久化的数据的 Avro 模式。前面的示例使用 `data` 选项告诉 `AvroStorage`
    使用输入目录下文件中嵌入的 Avro 模式。
- en: As with loading files, there are various other methods for telling `AvroStorage`
    your schema details, and these are documented on Pig’s wiki.^([[38](#ch03fn39)])
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 与加载文件一样，有各种其他方法可以告诉 `AvroStorage` 您的模式细节，这些在 Pig 的维基百科上有记录。[^（38）](#ch03fn39)
- en: ^(38) Additional resources on AvroStorage are at [https://cwiki.apache.org/confluence/display/PIG/AvroStorage](https://cwiki.apache.org/confluence/display/PIG/AvroStorage).
  id: totrans-513
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（38）有关 AvroStorage 的附加资源请参阅 [https://cwiki.apache.org/confluence/display/PIG/AvroStorage](https://cwiki.apache.org/confluence/display/PIG/AvroStorage)。
- en: Summary
  id: totrans-514
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The last few techniques have demonstrated how easy and straightforward it is
    to use Avro with MapReduce, Hive, and Pig. Using Avro to store your data gives
    you a number of useful free features, such as versioning support, compression,
    splittability, and code generation. Avro’s strong integration with MapReduce,
    Hive, Pig, and numerous other tools, such as Impala and Flume, means that it’s
    worth consideration as your data format of choice.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几种技术展示了如何轻松且直接地使用 Avro 与 MapReduce、Hive 和 Pig 一起使用。使用 Avro 存储您的数据为您提供了许多有用的免费功能，例如版本支持、压缩、可分割性和代码生成。Avro
    与 MapReduce、Hive、Pig 以及众多其他工具（如 Impala 和 Flume）的强大集成意味着它值得考虑作为您首选的数据格式。
- en: Until now we’ve focused on row-based file formats, which aren’t always the best
    way to lay out data. In the next section you’ll learn about the advantages of
    columnar storage and see examples of Parquet, a columnar storage, in action.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于基于行的文件格式，这并不总是布局数据的最佳方式。在下一节中，您将了解列式存储的优势，并看到 Parquet（一种列式存储）的实际应用示例。
- en: 3.4\. Columnar storage
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 列式存储
- en: When data is written to an I/O device (say a flat file, or a table in a relational
    database), the most common way to lay out that data is row-based, meaning that
    all the fields for the first row are written first, followed by all the fields
    for the second row, and so on. This is how most relational databases write out
    tables by default, and the same goes for most data serialization formats such
    as XML, JSON, and Avro container files.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据写入 I/O 设备（例如平面文件或关系型数据库中的表）时，最常见的数据布局方式是基于行的，这意味着首先写入第一行的所有字段，然后是第二行的所有字段，依此类推。这是大多数关系型数据库默认写入表的方式，对于大多数数据序列化格式（如
    XML、JSON 和 Avro 容器文件）也是如此。
- en: Columnar storage works differently—it lays out data by column first, and then
    by row. All the values of the first field across all the records are written first,
    followed by the second field, and so on. [Figure 3.12](#ch03fig12) highlights
    the differences between the two storage schemes in how the data is laid out.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 列式存储的工作方式不同——它首先按列布局数据，然后按行。首先写入所有记录的第一个字段的值，然后是第二个字段，依此类推。[图 3.12](#ch03fig12)
    强调了两种存储方案在数据布局方面的差异。
- en: Figure 3.12\. How row and column storage systems lay out their data
  id: totrans-520
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.12\. 行和列存储系统如何布局其数据
- en: '![](03fig12.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig12.jpg)'
- en: 'There are two main benefits to storing data in columnar form:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 存储数据在列式形式中有两个主要好处：
- en: Systems that read columnar data can efficiently extract a subset of the columns,
    reducing I/O. Row-based systems typically need to read the entire row even if
    just one or two columns are needed.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取列式数据的系统可以有效地提取列的子集，从而减少 I/O。基于行的系统通常需要读取整个行，即使只需要一两个列也是如此。
- en: Optimizations can be made when writing columnar data, such as run-length encoding
    and bit packing, to efficiently compress the size of the data being written. General
    compression schemes also work well for compressing columnar data because compression
    works best on data that has a lot of repeating data, which is the case when columnar
    data is physically colocated.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编写列式数据时可以进行优化，例如运行长度编码和位打包，以有效地压缩正在写入的数据的大小。通用的压缩方案也适用于压缩列式数据，因为压缩在具有大量重复数据的数据上效果最佳，而当列式数据在物理上集中时，这种情况就会发生。
- en: As a result, columnar file formats work best when working with large datasets
    where you wish to filter or project data, which is exactly the type of work that’s
    commonly performed in OLAP-type use cases, as well as MapReduce.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在处理大型数据集时，当您希望过滤或投影数据时，列式文件格式工作得最好，这正是 OLAP 类用例以及 MapReduce 常见的工作类型。
- en: 'The majority of data formats used in Hadoop, such as JSON and Avro, are row-ordered,
    which means that you can’t apply the previously mentioned optimizations when reading
    and writing these files. Imagine that the data in [figure 3.12](#ch03fig12) was
    in a Hive table and you were to execute the following query:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 中使用的多数数据格式，如 JSON 和 Avro，都是行顺序的，这意味着在读取和写入这些文件时无法应用之前提到的优化。想象一下，[图 3.12](#ch03fig12)
    中的数据在一个 Hive 表中，您要执行以下查询：
- en: '[PRE44]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If the data was laid out in a row-based format, each row would have to be read,
    even though the only column being operated on is `price`. In a column-oriented
    store, only the `price` column would be read, which could result in drastically
    reduced processing times when you’re working with large datasets.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据以行格式排列，则必须读取每一行，尽管只操作的是 `price` 列。在列式存储中，只需读取 `price` 列，这在处理大型数据集时可能会显著减少处理时间。
- en: 'There are a number of columnar storage options that can be used in Hadoop:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 中可以使用多种列式存储选项：
- en: '*RCFile* was the first columnar format available in Hadoop; it came out of
    a collaboration between Facebook and academia in 2009.^([[39](#ch03fn40)]) RCFile
    is a basic columnar store that supports separate column storage and column compression.
    It can support projection during reads, but misses out on the more advanced techniques
    such as run-length encoding. As a result, Facebook has been moving away from RCFile
    to ORC file.^([[40](#ch03fn41)])'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RCFile* 是 Hadoop 中第一个可用的列式格式；它源于 2009 年 Facebook 与学术界之间的合作。[39](#ch03fn40)
    RCFile 是一种基本的列式存储，支持单独的列存储和列压缩。它可以在读取时支持投影，但缺少了诸如运行长度编码等更高级的技术。因此，Facebook 已经开始从
    RCFile 转向 ORC 文件。[40](#ch03fn41)'
- en: '^(39) Yongqiang He, et al., “RCFile: A Fast and Space-efficient Data Placement
    Structure in MapReduce-based Warehouse Systems,” ICDE Conference 2011: [www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf](http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf).'
  id: totrans-531
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '(39) Yongqiang He, et al., “RCFile: A Fast and Space-efficient Data Placement
    Structure in MapReduce-based Warehouse Systems,” ICDE Conference 2011: [www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf](http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf).'
- en: ^(40) Facebook Engineering Blog, “Scaling the Facebook data warehouse to 300
    PB,” [https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/](https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/).
  id: totrans-532
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (40) Facebook 工程师博客， “Scaling the Facebook data warehouse to 300 PB,” [https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/](https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/).
- en: '*ORC file* was created by Facebook and Hortonworks to address RCFile’s shortcomings,
    and its serialization optimizations have yielded smaller data sizes compared to
    RCFile.^([[41](#ch03fn42)]) It also uses indexes to enable predicate pushdowns
    to optimize queries so that a column that doesn’t match a filter predicate can
    be skipped. ORC file is also fully integrated with Hive’s type system and can
    support nested structures.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ORC 文件* 由 Facebook 和 Hortonworks 创建，旨在解决 RCFile 的不足，其序列化优化与 RCFile 相比产生了更小的数据大小。[41](#ch03fn42)
    它还使用索引来启用谓词下推以优化查询，这样就可以跳过不符合过滤谓词的列。ORC 文件也与 Hive 的类型系统完全集成，并可以支持嵌套结构。'
- en: ^(41) Owen O’Malley, “ORC File Introduction,” [www.slideshare.net/oom65/orc-fileintro](http://www.slideshare.net/oom65/orc-fileintro).
  id: totrans-534
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (41) Owen O’Malley, “ORC File Introduction,” [www.slideshare.net/oom65/orc-fileintro](http://www.slideshare.net/oom65/orc-fileintro).
- en: '*Parquet* is a collaboration between Twitter and Cloudera and employs many
    of the tricks that ORC file uses to generate compressed files.^([[42](#ch03fn43)])
    Parquet is a language-independent format with a formal specification.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Parquet* 是 Twitter 和 Cloudera 的合作成果，并采用了 ORC 文件用于生成压缩文件所使用的一些技巧。[42](#ch03fn43)
    Parquet 是一种与语言无关的格式，具有正式的规范。'
- en: ^(42) Features such as column stats and indexes are planned for the Parquet
    2 release.
  id: totrans-536
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (42) 列统计和索引等特性计划在 Parquet 2 版本中推出。
- en: RCFile and ORC file were designed to support Hive as their primary usage, whereas
    Parquet is independent of any other Hadoop tool and tries to maximize compatibility
    with the Hadoop ecosystem. [Table 3.2](#ch03table02) shows how these columnar
    formats integrate with various tools and languages.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: RCFile 和 ORC 文件被设计为支持 Hive 作为它们的主要用途，而 Parquet 则独立于任何其他 Hadoop 工具，并试图最大化与 Hadoop
    生态系统的兼容性。[表 3.2](#ch03table02) 展示了这些列式格式如何与各种工具和语言集成。
- en: Table 3.2\. Columnar storage formats supported in Hadoop
  id: totrans-538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.2\. Hadoop 支持的列式存储格式
- en: '| Format | Hadoop support | Supported object models | Supported programming
    languages | Advanced compression support |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | Hadoop 支持 | 支持的对象模型 | 支持的编程语言 | 高级压缩支持 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| RCFile | MapReduce, Pig, Hive (0.4+), Impala | Thrift, Protocol Buffers^([[a](#ch03fn44)])
    | Java | No |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| RCFile | MapReduce, Pig, Hive (0.4+), Impala | Thrift, Protocol Buffers^([[a](#ch03fn44)])
    | Java | 否 |'
- en: '| ORC file | MapReduce, Pig, Hive (0.11+) | None | Java | Yes |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| ORC 文件 | MapReduce, Pig, Hive (0.11+) | 无 | Java | 是 |'
- en: '| Parquet | MapReduce, Pig, Hive, Impala | Avro, Protocol Buffers, Thrift |
    Java, C++, Python | Yes |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| Parquet | MapReduce, Pig, Hive, Impala | Avro, Protocol Buffers, Thrift |
    Java, C++, Python | 是 |'
- en: ^a Elephant Bird provides the ability to use Thrift and Protocol Buffers with
    RCFile.
  id: totrans-544
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a Elephant Bird 提供了使用 Thrift 和 Protocol Buffers 与 RCFile 一起使用的能力。
- en: For this section, I’ll focus on Parquet due to its compatibility with object
    models such as Avro.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我将专注于 Parquet，因为它与 Avro 等对象模型具有兼容性。
- en: 3.4.1\. Understanding object models and storage formats
  id: totrans-546
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 理解对象模型和存储格式
- en: 'Before we get started with the techniques, we’ll cover a few Parquet concepts
    that are important in understanding the interplay between Parquet and Avro (and
    Thrift and Protocol Buffers):'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始介绍技术之前，我们将介绍一些重要的 Parquet 概念，这些概念对于理解 Parquet 与 Avro（以及 Thrift 和 Protocol
    Buffers）之间的交互至关重要：
- en: '*Object models* are in-memory representations of data. Parquet exposes a simple
    object model that’s supplied more as an example than anything else. Avro, Thrift,
    and Protocol Buffers are full-featured object models. An example is the Avro `Stock`
    class, which was generated by Avro to richly model the schema using Java POJOs.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对象模型* 是数据的内存表示。Parquet 提供了一个简单的对象模型，这更多的是作为一个示例，而不是其他任何东西。Avro、Thrift 和 Protocol
    Buffers 都是功能齐全的对象模型。一个例子是 Avro 的 `Stock` 类，它是通过 Avro 生成的，用于丰富地使用 Java POJOs 模型化模式。'
- en: '*Storage formats* are serialized representations of a data model. Parquet is
    a storage format that serializes data in columnar form. Avro, Thrift, and Protocol
    Buffers also have their own storage formats that serialize data in row-oriented
    formats.^([[43](#ch03fn45)]) Storage formats can be thought of as the at-rest
    representation of data.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*存储格式* 是数据模型的序列化表示。Parquet 是一种以列式形式序列化数据的存储格式。Avro、Thrift 和 Protocol Buffers
    也都有自己的存储格式，它们以行导向格式序列化数据.^([[43](#ch03fn45)]) 存储格式可以被视为数据的静态表示。'
- en: '^(43) Avro does have a columnar storage format called Trevni: [http://avro.apache.org/docs/1.7.6/trevni/spec.html](http://avro.apache.org/docs/1.7.6/trevni/spec.html).'
  id: totrans-550
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(43) Avro 确实有一个名为 Trevni 的列式存储格式：[http://avro.apache.org/docs/1.7.6/trevni/spec.html](http://avro.apache.org/docs/1.7.6/trevni/spec.html)].
- en: '*Parquet object model converters* are responsible for converting an object
    model to Parquet’s data types, and vice versa. Parquet is bundled with a number
    of converters to maximize the interoperability and adoption of Parquet.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Parquet 对象模型转换器* 负责将对象模型转换为 Parquet 的数据类型，反之亦然。Parquet 随带了许多转换器，以最大化互操作性和
    Parquet 的采用。'
- en: '[Figure 3.13](#ch03fig13) shows how these concepts work in the context of Parquet.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.13](#ch03fig13) 展示了这些概念在 Parquet 上下文中的工作方式。'
- en: Figure 3.13\. Parquet storage format and object model converters
  id: totrans-553
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.13\. Parquet 存储格式和对象模型转换器
- en: '![](03fig13_alt.jpg)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig13_alt.jpg)'
- en: 'What’s unique about Parquet is that it has converters that allow it to support
    common object models such as Avro. Behind the scenes, the data is stored in Parquet
    binary form, but when you’re working with your data, you’re using your preferred
    object model, such as Avro objects. This gives you the best of both worlds: you
    can continue to use a rich object model such as Avro to interact with your data,
    and that data will be efficiently laid out on disk using Parquet.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 的独特之处在于它具有转换器，允许它支持常见的对象模型，如 Avro。在幕后，数据以 Parquet 二进制形式存储，但当你处理数据时，你使用的是你首选的对象模型，例如
    Avro 对象。这给你带来了两全其美的效果：你可以继续使用像 Avro 这样的丰富对象模型与你的数据交互，而这些数据将使用 Parquet 在磁盘上高效地布局。
- en: '|  |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Storage format interoperability
  id: totrans-557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 存储格式互操作性
- en: Storage formats generally aren’t interoperable. When you’re combining Avro and
    Parquet, you’re combining Avro’s object model and Parquet’s storage format. Therefore,
    if you have existing Avro data sitting in HDFS that was serialized using Avro’s
    storage format, you can’t read that data using Parquet’s storage format, as they
    are two very different ways of encoding data. The reverse is also true—Parquet
    can’t be read using the normal Avro methods (such as the `AvroInputFormat` in
    MapReduce); you must use Parquet implementations of input formats and Hive SerDes
    to work with Parquet data.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 存储格式通常是不可互操作的。当你结合 Avro 和 Parquet 时，你是在结合 Avro 的对象模型和 Parquet 的存储格式。因此，如果你有使用
    Avro 存储格式序列化的现有 Avro 数据存储在 HDFS 中，你不能使用 Parquet 的存储格式来读取这些数据，因为它们是两种非常不同的数据编码方式。反之亦然——Parquet
    不能使用正常的 Avro 方法（如 MapReduce 中的 `AvroInputFormat`）来读取；你必须使用 Parquet 的输入格式实现和 Hive
    SerDes 来处理 Parquet 数据。
- en: '|  |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: To summarize, choose Parquet if you want your data to be serialized in a columnar
    form. Once you’ve selected Parquet, you’ll need to decide which object model you’ll
    be working with. I recommend you pick the object model that has the most traction
    in your organization. Otherwise I recommend going with Avro ([section 3.3.5](#ch03lev2sec9)
    explains why Avro can be a good choice).
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果你想以列式形式序列化你的数据，请选择 Parquet。一旦你选择了 Parquet，你将需要决定你将使用哪种对象模型。我建议你选择在你组织中最受欢迎的对象模型。否则，我建议选择
    Avro（[第 3.3.5 节](#ch03lev2sec9)解释了为什么 Avro 可以是一个好的选择）。
- en: '|  |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The Parquet file format
  id: totrans-562
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Parquet 文件格式
- en: The Parquet file format is beyond the scope of this book; for more details,
    take a look at the Parquet project page at [https://github.com/Parquet/parquet-format](https://github.com/Parquet/parquet-format).
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件格式超出了本书的范围；对于更多细节，请查看 Parquet 项目的页面 [https://github.com/Parquet/parquet-format](https://github.com/Parquet/parquet-format)。
- en: '|  |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 3.4.2\. Parquet and the Hadoop ecosystem
  id: totrans-565
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. Parquet 和 Hadoop 生态系统
- en: The goal of Parquet is to maximize support throughout the Hadoop ecosystem.
    It currently supports MapReduce, Hive, Pig, Impala, and Spark, and hopefully we’ll
    see it being supported by other systems such as Sqoop.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 的目标是最大化在整个 Hadoop 生态系统中的支持。它目前支持 MapReduce、Hive、Pig、Impala 和 Spark，并希望我们能看到它被其他系统（如
    Sqoop）支持。
- en: Because Parquet is a standard file format, a Parquet file that’s written by
    any one of these technologies can also be read by the others. Maximizing support
    across the Hadoop ecosystem is critical to the success of a file format, and Parquet
    is poised to become the ubiquitous file format in big data.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Parquet 是一种标准文件格式，因此任何这些技术中的一种所编写的 Parquet 文件也可以被其他技术读取。在 Hadoop 生态系统中最大化支持对于文件格式的成功至关重要，而
    Parquet 有望成为大数据中的通用文件格式。
- en: It’s also reassuring that Parquet isn’t focused on a particular subset of technologies—in
    the words of the Parquet home page, “We are not interested in playing favorites”
    when it comes to ecosystem support ([http://parquet.io](http://parquet.io)). This
    implies that a primary goal of the project is to maximize its support for the
    tools that you’re likely to use, which is important as new tools continue to pop
    up on our radars.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 令人欣慰的是，Parquet 并没有专注于特定的技术子集——正如 Parquet 主页上所说，“在生态系统支持方面，我们并不偏袒任何一方”（[http://parquet.io](http://parquet.io)）。这表明项目的主要目标是在你可能会使用的工具中最大化其支持，这对于新工具不断出现在我们的雷达上非常重要。
- en: 3.4.3\. Parquet block and page sizes
  id: totrans-569
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3\. Parquet 块和页面大小
- en: '[Figure 3.14](#ch03fig14) shows a high-level representation of the Parquet
    file format and highlights the key concepts.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.14](#ch03fig14) 展示了 Parquet 文件格式的概要表示，并突出了关键概念。'
- en: Figure 3.14\. Parquet’s file format
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.14\. Parquet 的文件格式
- en: '![](03fig14_alt.jpg)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig14_alt.jpg)'
- en: 'A more detailed overview of the file format can be seen at the project’s home
    page: [https://github.com/Parquet/parquet-format](https://github.com/Parquet/parquet-format).'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的文件格式概述可以在项目的主页上查看：[https://github.com/Parquet/parquet-format](https://github.com/Parquet/parquet-format)。
- en: Technique 20 Reading Parquet files via the command line
  id: totrans-574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 20 通过命令行读取 Parquet 文件
- en: Parquet is a binary storage format, so using the standard `hadoop fs -cat` command
    will yield garbage on the command line. In this technique we’ll explore how you
    can use the command line to not only view the contents of a Parquet file, but
    also to examine the schema and additional metadata contained in Parquet files.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种二进制存储格式，因此使用标准的 `hadoop fs -cat` 命令将在命令行上产生垃圾。在这个技术中，我们将探讨如何使用命令行不仅查看
    Parquet 文件的 内容，还可以检查 Parquet 文件中包含的模式和附加元数据。
- en: Problem
  id: totrans-576
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use the command line to examine the contents of a Parquet file.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望使用命令行来检查Parquet文件的内容。
- en: Solution
  id: totrans-578
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the utilities bundled with the Parquet tools.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Parquet工具捆绑的实用程序。
- en: Discussion
  id: totrans-580
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Parquet is bundled with a tools JAR containing some useful utilities that can
    dump information in Parquet files to standard output.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet附带一个包含一些有用实用程序的工具JAR，可以将Parquet文件中的信息转储到标准输出。
- en: 'Before you get started, you’ll need to create a Parquet file so that you can
    test out the tools. The following example creates a Parquet file by writing Avro
    records:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始之前，你需要创建一个Parquet文件，这样你就可以测试这些工具。以下示例通过写入Avro记录来创建一个Parquet文件：
- en: '[PRE45]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The first Parquet tool you’ll use is `cat`, which performs a simple dump of
    the data in the Parquet file to standard output:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用的第一个Parquet工具是`cat`，它将Parquet文件中的数据简单地转储到标准输出：
- en: '[PRE46]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You can use the Parquet `head` command instead of `cat` in the preceding example
    to emit only the first five records. There’s also a `dump` command that allows
    you to specify a subset of the columns that should be dumped, although the output
    isn’t as human-readable.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Parquet的`head`命令代替前面的`cat`，只输出前五个记录。还有一个`dump`命令，允许你指定要转储的列的子集，尽管输出不是那么易于阅读。
- en: 'Parquet has its own internal data types and schema that are mapped to external
    object models by converters. The internal Parquet schema can be viewed using the
    `schema` option:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet有其自己的内部数据类型和模式，这些类型通过转换器映射到外部对象模型。可以使用`schema`选项查看内部Parquet模式：
- en: '[PRE47]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Parquet also allows object models to use the metadata to store information
    needed for deserialization. Avro, for example, uses the metadata to store the
    Avro schema, as can be seen in the output of the command that follows:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet还允许对象模型使用元数据来存储反序列化所需的信息。例如，Avro使用元数据来存储Avro模式，如下面的命令输出所示：
- en: '[PRE48]'
  id: totrans-590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Next let’s look at how you can write and read Parquet files.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看你如何可以写入和读取Parquet文件。
- en: Technique 21 Reading and writing Avro data in Parquet with Java
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧21：使用Java在Parquet中读取和写入Avro数据
- en: One of the first things you’ll want to do when working with a new file format
    is to understand how a standalone Java application can read and write data. This
    technique shows how you can write Avro data into a Parquet file and read it back
    out.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始处理一个新的文件格式时，你首先想要做的一件事是了解一个独立的Java应用程序如何读取和写入数据。这个技术展示了你如何可以将Avro数据写入Parquet文件并读取出来。
- en: Problem
  id: totrans-594
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to read and write Parquet data directly from your Java code outside
    of Hadoop using an Avro object model.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望直接从Java代码中读取和写入Parquet数据，而不使用Hadoop，并使用Avro对象模型。
- en: Solution
  id: totrans-596
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `AvroParquetWriter` and `AvroParquetReader` classes.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`AvroParquetWriter`和`AvroParquetReader`类。
- en: Discussion
  id: totrans-598
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Parquet, a columnar storage format for Hadoop, has support for Avro, which allows
    you to work with your data using Avro classes, and to efficiently encode the data
    using Parquet’s file format so that you can take advantage of the columnar layout
    of your data. It sounds odd to mix data formats like this, so let’s investigate
    why you’d want to do this and how it works.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet是Hadoop的列式存储格式，它支持Avro，这允许你使用Avro类来处理数据，并使用Parquet的文件格式有效地编码数据，以便你可以利用数据的列式布局。混合这种数据格式听起来很奇怪，所以让我们调查一下为什么你想这样做以及它是如何工作的。
- en: Parquet is a storage format, and it has a formal programming language–agnostic
    specification. You could use Parquet directly without any other supporting data
    format such as Avro, but Parquet is at heart a simple data format and doesn’t
    support complex types such as maps or unions. This is where Avro comes into play,
    as it supports these richer types as well as features such as code generation
    and schema evolution. As a result, marrying Parquet and a rich data format such
    as Avro creates a perfect match of sophisticated schema capabilities coupled with
    efficient data encoding.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet是一种存储格式，它有一个正式的、与编程语言无关的规范。你可以直接使用Parquet而无需任何其他支持数据格式，如Avro，但Parquet本质上是一种简单的数据格式，不支持如映射或联合等复杂类型。这就是Avro发挥作用的地方，因为它支持这些更丰富的类型以及代码生成和模式演变等功能。因此，将Parquet与像Avro这样的丰富数据格式结合起来，就形成了一个复杂的模式能力与高效数据编码的完美匹配。
- en: For this technique, we’ll continue to use the Avro Stock schema. First, let’s
    look at how you can write a Parquet file using these `Stock` objects.^([[44](#ch03fn46)])
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个技术，我们将继续使用Avro股票模式。首先，让我们看看你如何可以使用这些`Stock`对象来写入Parquet文件.^([[44](#ch03fn46)])
- en: '^(44) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockWriter.java).'
  id: totrans-602
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^((44) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockWriter.java))。
- en: '![](120fig01_alt.jpg)'
  id: totrans-603
  prefs: []
  type: TYPE_IMG
  zh: '![](120fig01_alt.jpg)'
- en: 'The following command generates a Parquet file by executing the preceding code:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令通过执行前面的代码生成一个Parquet文件：
- en: '[PRE49]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The previous technique showed you how to use the Parquet tools to dump the file
    to standard output. But what if you wanted to read the file in Java?^([[45](#ch03fn47)])
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的技术展示了如何使用Parquet工具将文件转储到标准输出。但如果你想在Java中读取文件呢？^([[45](#ch03fn47)])。
- en: '^(45) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockReader.java).'
  id: totrans-607
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^((45) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ParquetAvroStockReader.java))。
- en: '[PRE50]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The following command shows the output of the preceding code:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令显示了前面代码的输出：
- en: '[PRE51]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Technique 22 Parquet and MapReduce
  id: totrans-611
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术二十二：Parquet和MapReduce
- en: This technique examines how you can work with Parquet files in MapReduce. Using
    Parquet as a data source as well as a data sink in MapReduce will be covered.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术探讨了如何在MapReduce中处理Parquet文件。将Parquet作为MapReduce中的数据源和数据汇将得到介绍。
- en: Problem
  id: totrans-613
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with Avro data serialized as Parquet in MapReduce.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce中处理序列化为Parquet的Avro数据。
- en: Solution
  id: totrans-615
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `AvroParquetInputFormat` and `AvroParquetOutputFormat` classes.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`AvroParquetInputFormat`和`AvroParquetOutputFormat`类。
- en: Discussion
  id: totrans-617
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The Avro subproject in Parquet comes with MapReduce input and output formats
    to let you read and write your Avro data using Parquet as the storage format.
    The following example calculates the average stock price for each symbol:^([[46](#ch03fn48)])
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet中的Avro子项目附带MapReduce输入和输出格式，让你可以使用Parquet作为存储格式来读取和写入你的Avro数据。以下示例计算每个符号的平均股票价格:^([[46](#ch03fn48)])。
- en: '^(46) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroParquetMapReduce.java).'
  id: totrans-619
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^((46) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroParquetMapReduce.java))。
- en: '![](121fig01_alt.jpg)'
  id: totrans-620
  prefs: []
  type: TYPE_IMG
  zh: '![](121fig01_alt.jpg)'
- en: 'Working with Avro in Parquet is very simple, and arguably easier than working
    with Avro-serialized data.^([[47](#ch03fn49)]) You can run the example:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 在Parquet中处理Avro非常简单，并且可以说比处理Avro序列化数据更容易.^([[47](#ch03fn49)]) 你可以运行示例：
- en: ^(47) The input and output formats supplied with Avro to support Avro’s storage
    format wrap the Avro objects, requiring a level of indirection.
  id: totrans-622
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^((47) Avro提供的输入和输出格式用于支持Avro的存储格式，它封装了Avro对象，需要一定程度的间接引用。
- en: '[PRE52]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Parquet comes with some tools to help you work with Parquet files, and one
    of them allows you to dump the contents to standard output:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet附带一些工具来帮助你处理Parquet文件，其中之一允许你将内容转储到标准输出：
- en: '[PRE53]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: You may have noticed that there’s an additional file in the output directory
    called _metadata. When the Parquet `OutputComitter` runs upon job completion,
    it reads the footer of all the output files (which contains the file metadata)
    and generates this summarized metadata file. This file is used by subsequent MapReduce
    (or Pig/Hive) jobs to reduce job startup times.^([[48](#ch03fn50)])
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到输出目录中有一个名为_metadata的额外文件。当Parquet的`OutputComitter`在作业完成后运行时，它会读取所有输出文件的尾部（包含文件元数据）并生成这个汇总的元数据文件。这个文件被后续的MapReduce（或Pig/Hive）作业用来减少作业启动时间.^([[48](#ch03fn50)])。
- en: ^(48) Calculating the input splits can take a long time when there are a large
    number of input files that need to have their footers read, so having the ability
    to read a single summary file is a useful optimization.
  id: totrans-627
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^((48) 当需要读取大量输入文件的尾部时，计算输入拆分可能需要很长时间，因此能够读取单个汇总文件是一种有用的优化。
- en: Summary
  id: totrans-628
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this technique, you saw how to use code-generated Avro object files with
    Parquet. If you don’t want to work with Avro object files, you have a few options
    that allow you to work with Avro data generically using Avro’s `GenericData` class:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你看到了如何使用代码生成的Avro对象文件与Parquet一起使用。如果你不想处理Avro对象文件，你有一些选项允许你使用Avro的`GenericData`类通用地处理Avro数据：
- en: If you wrote the Avro data using `GenericData` objects, then that’s the format
    in which Avro will supply them to your mappers.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用`GenericData`对象编写了Avro数据，那么Avro将以这种格式将它们提供给你的mapper。
- en: Excluding the JAR containing your Avro-generated code will also result in `GenericData`
    objects being fed to your mappers.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除包含你的Avro生成代码的JAR文件，也会导致`GenericData`对象被喂给你的mapper。
- en: You can trick Avro by mutating the input schema so that Avro can’t load the
    specific class, forcing it to supply the `GenericData` instance instead.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过修改输入模式来欺骗Avro，使其无法加载特定的类，从而迫使它提供`GenericData`实例。
- en: The following code shows how you would perform the third option—you’re essentially
    taking the original schema and duplicating it, but in the process you’re supplying
    a different classname, which Avro won’t be able to load (see `"foobar"` in the
    first line):^([[49](#ch03fn51)])
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何执行第三种选择——你实际上是在复制原始模式，但在过程中提供了不同的类名，Avro将无法加载（参见第一行的`"foobar"`）：^([[49](#ch03fn51)])
- en: ^(49) GitHub source [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroGenericParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroGenericParquetMapReduce.java).
  id: totrans-634
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（49）GitHub源代码 [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroGenericParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroGenericParquetMapReduce.java)。
- en: '[PRE54]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: What if you want to work with the Parquet data natively? Parquet comes with
    an example object model that allows you to work with any Parquet data, irrespective
    of the object model that was used to write the data. It uses a `Group` class to
    represent records, and provides some basic getters and setters to retrieve fields.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想以原生方式处理Parquet数据呢？Parquet附带了一个示例对象模型，允许你处理任何Parquet数据，无论使用什么对象模型来写入数据。它使用`Group`类来表示记录，并提供了一些基本的getter和setter来检索字段。
- en: The following code once again shows how to calculate the stock averages. The
    input is the Avro/Parquet data, and the output is a brand new Parquet schema:^([[50](#ch03fn52)])
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码再次展示了如何计算股票平均值。输入是Avro/Parquet数据，输出是一个全新的Parquet模式：^([[50](#ch03fn52)])
- en: '^(50) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ExampleParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ExampleParquetMapReduce.java).'
  id: totrans-638
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（50）GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ExampleParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/ExampleParquetMapReduce.java).
- en: '[PRE55]'
  id: totrans-639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The example object model is pretty basic and is currently missing some functionality—for
    example, there are no getters for double types, which is why the preceding code
    accesses the stock value using the `getValueToString` method. But there’s work
    afoot to provide better object models, including a POJO adapter.^([[51](#ch03fn53)])
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 示例对象模型相当基础，目前缺少一些功能——例如，没有为double类型提供getter，这就是为什么前面的代码使用`getValueToString`方法访问股票价值。但正在努力提供更好的对象模型，包括POJO适配器。^([[51](#ch03fn53)])
- en: ^(51) See the GitHub ticket number 325 titled “Pojo Support for Parquet” at
    [https://github.com/Parquet/parquet-mr/pull/325](https://github.com/Parquet/parquet-mr/pull/325).
  id: totrans-641
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（51）请参阅GitHub上的问题单号325，标题为“Parquet的POJO支持”的[https://github.com/Parquet/parquet-mr/pull/325](https://github.com/Parquet/parquet-mr/pull/325)。
- en: Technique 23 Parquet and Hive/Impala
  id: totrans-642
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇 23：Parquet与Hive/Impala
- en: Parquet comes into its own when utilized in Hive and Impala. Columnar storage
    is a natural fit for these systems by virtue of its ability to use pushdowns to
    optimize the read path.^([[52](#ch03fn54)]) This technique shows how Parquet can
    be used in these systems.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 当Parquet在Hive和Impala中使用时，其优势得以体现。由于其能够利用下推（pushdowns）来优化读取路径，列式存储是这些系统的自然选择。这项技术展示了如何在这些系统中使用Parquet。^[[52](#ch03fn54)])
    该技术展示了如何在这些系统中使用Parquet。
- en: ^(52) Pushdowns are covered in more detail in the next technique.
  id: totrans-644
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（52）下一种技术中会更详细地介绍下推（Pushdowns）。
- en: Problem
  id: totrans-645
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to be able to work with your Parquet data in Hive and Impala.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望能够在Hive和Impala中处理你的Parquet数据。
- en: Solution
  id: totrans-647
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Hive’s and Impala’s built-in support for Parquet.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hive和Impala内置对Parquet的支持。
- en: Discussion
  id: totrans-649
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Hive requires that data exists in a directory, so you first need to create
    a directory and copy the stocks Parquet file into it:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: Hive要求数据存在于目录中，因此你首先需要创建一个目录并将股票Parquet文件复制到其中：
- en: '[PRE56]'
  id: totrans-651
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, you’ll create an external Hive table and define the schema. If you’re
    unsure about the structure of your schema, use one of the earlier techniques to
    view the schema information in the Parquet files that you’re working with (use
    the `schema` command in the Parquet tools):'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将创建一个外部Hive表并定义其模式。如果你不确定模式的结构，可以使用之前的技术查看你正在处理的Parquet文件中的模式信息（在Parquet工具中使用`schema`命令）：
- en: '[PRE57]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '|  |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Hive 0.13
  id: totrans-655
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive 0.13
- en: 'Support for Parquet as a native Hive store was only added in Hive 0.13 (see
    [https://issues.apache.org/jira/browse/HIVE-5783](https://issues.apache.org/jira/browse/HIVE-5783)).
    If you’re using an older version of Hive, you’ll need to manually load all the
    Parquet JARs using the `ADD JAR` command and use the Parquet input and output
    formats. Cloudera has an example on its blog; see “How-to: Use Parquet with Impala,
    Hive, Pig, and Map-Reduce,” [http://blog.cloudera.com/blog/2014/03/how-to-use-parquet-with-impala-hive-pig-mapreduce/](http://blog.cloudera.com/blog/2014/03/how-to-use-parquet-with-impala-hive-pig-mapreduce/).'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在Hive 0.13中添加了对Parquet作为本地Hive存储的支持（见[https://issues.apache.org/jira/browse/HIVE-5783](https://issues.apache.org/jira/browse/HIVE-5783)）。如果你使用的是Hive的旧版本，你需要使用`ADD
    JAR`命令手动加载所有Parquet JARs，并使用Parquet输入和输出格式。Cloudera在其博客上有一个示例；请参阅“如何：在Impala、Hive、Pig和Map-Reduce中使用Parquet”，[http://blog.cloudera.com/blog/2014/03/how-to-use-parquet-with-impala-hive-pig-mapreduce/](http://blog.cloudera.com/blog/2014/03/how-to-use-parquet-with-impala-hive-pig-mapreduce/)。
- en: '|  |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'You can run a simple query to extract the unique stock symbols from the data:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行一个简单的查询来从数据中提取唯一的股票代码：
- en: '[PRE58]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: You can use the same syntax to create the table in Impala.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用相同的语法在Impala中创建表。
- en: Technique 24 Pushdown predicates and projection with Parquet
  id: totrans-661
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧24：使用Parquet进行下推谓词和投影
- en: Projection and predicate pushdowns involve an execution engine pushing the projection
    and predicates down to the storage format to optimize the operations at the lowest
    level possible. This yields space and time advantages, as columns that aren’t
    required for the query don’t need to be fetched and supplied to the execution
    engine.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 投影和谓词下推涉及执行引擎将投影和谓词下推到存储格式，以尽可能优化最低级别的操作。这带来了空间和时间上的优势，因为不需要查询的列不需要被检索并提供给执行引擎。
- en: This is especially useful for columnar stores, as pushdowns allow the storage
    format to skip over entire column groups that aren’t required for the query, and
    columnar formats can perform this operation very efficiently.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于列式存储尤其有用，因为下推允许存储格式跳过查询中不需要的整个列组，并且列式格式可以非常高效地执行此操作。
- en: In this technique you’ll look at the steps required to use these pushdowns in
    your Hadoop pipelines.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，你将查看在Hadoop管道中使用这些下推所需的步骤。
- en: Problem
  id: totrans-665
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use pushdowns in Hadoop to optimize your jobs.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在Hadoop中使用下推来优化你的作业。
- en: Solution
  id: totrans-667
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Using Hive and Pig in conjunction with Parquet provides out-of-the-box projection
    pushdowns. With MapReduce there are some manual steps that you need to take in
    the driver code to enable pushdowns.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hive和Pig与Parquet结合使用提供开箱即用的投影下推。在MapReduce中，你需要在驱动代码中执行一些手动步骤来启用下推。
- en: Discussion
  id: totrans-669
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Once again our focus with this technique is Avro. The `AvroParquetInputFormat`
    has two methods that you can use for predicate and projection pushdowns. In the
    following example, only two fields of the `Stock` object are projected, and a
    predicate is added so that only Google stocks are selected:^([[53](#ch03fn55)])
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们使用此技术的重点是Avro。`AvroParquetInputFormat`有两个你可以用于谓词和投影下推的方法。在下面的示例中，只投影了`Stock`对象的两个字段，并添加了一个谓词，以便只选择谷歌股票：^([[53](#ch03fn55)])
- en: '^(53) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroProjectionParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroProjectionParquetMapReduce.java).'
  id: totrans-671
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (53) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroProjectionParquetMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/parquet/AvroProjectionParquetMapReduce.java).
- en: '![](126fig01_alt.jpg)'
  id: totrans-672
  prefs: []
  type: TYPE_IMG
  zh: '![](126fig01_alt.jpg)'
- en: '![](126fig02_alt.jpg)'
  id: totrans-673
  prefs: []
  type: TYPE_IMG
  zh: '![](126fig02_alt.jpg)'
- en: '|  |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Predicate filter null values
  id: totrans-675
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 谓词过滤器null值
- en: When the predicate that you supply filters out a record, a `null` value is supplied
    to your mapper. That’s why you have to check for `null` before working with the
    mapper input.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 当你提供的谓词过滤掉一个记录时，你的mapper会接收到一个`null`值。这就是为什么在处理mapper输入之前你必须检查`null`的原因。
- en: '|  |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'If you run the job and examine the output, you’ll only find the average for
    the Google stock, demonstrating that the predicate worked:'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行作业并检查输出，你只会找到谷歌股票的平均值，这表明谓词是有效的：
- en: '[PRE59]'
  id: totrans-679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Summary
  id: totrans-680
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique doesn’t include any Hive or Pig pushdown details, as both tools
    automatically perform these pushdowns as part of their execution. Pushdowns are
    an important part of your job-optimization work, and if you’re using a third-party
    library or tool that doesn’t expose pushdowns when working with Parquet, you can
    help the community by opening a feature request.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术不包括任何Hive或Pig向下推送的细节，因为这两个工具在执行时会自动执行这些向下推送。向下推送是您作业优化工作的重要组成部分，如果您使用的是不暴露向下推送的第三方库或工具，您可以通过提出功能请求来帮助社区。
- en: 3.4.4\. Parquet limitations
  id: totrans-682
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4\. Parquet限制
- en: 'There are a number of points that you should be aware of when working with
    Parquet:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Parquet时，你应该注意以下几点：
- en: Parquet requires a lot of memory when writing files because it buffers writes
    in memory to optimize the encoding and compressing of the data. Either increase
    the heap size (2 GB is recommended), or decrease the `parquet.block.size` configurable
    if you encounter memory issues when writing Parquet files.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet在写入文件时需要大量的内存，因为它在内存中缓冲写入以优化数据的编码和压缩。如果遇到写入Parquet文件时的内存问题，可以增加堆大小（建议2
    GB），或者减少可配置的`parquet.block.size`。
- en: Using a heavily nested data structure with Parquet will likely limit some of
    the optimizations that Parquet makes for pushdowns. If possible, try to flatten
    your schema.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Parquet的深度嵌套数据结构可能会限制Parquet在向下推送时的一些优化。如果可能的话，尽量简化你的模式。
- en: Hive doesn’t yet support `decimal` and `timestamp` data types when working with
    Parquet because Parquet doesn’t support them as native types. Work is being tracked
    in a JIRA ticket titled “Implement all Hive data types in Parquet” ([https://issues.apache.org/jira/browse/HIVE-6384](https://issues.apache.org/jira/browse/HIVE-6384)).
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive在处理Parquet时还不支持`decimal`和`timestamp`数据类型，因为Parquet不支持它们作为原生类型。相关工作正在JIRA票据“在Parquet中实现所有Hive数据类型”中跟踪（[https://issues.apache.org/jira/browse/HIVE-6384](https://issues.apache.org/jira/browse/HIVE-6384)）。
- en: Impala doesn’t support nested data in Parquet or complex data types such as
    maps, structs, or arrays. This should be fixed in the Impala 2.x release.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala不支持Parquet中的嵌套数据或复杂数据类型，如maps、structs或arrays。这应该在Impala 2.x版本中得到修复。
- en: Tools such as Impala work best when a Parquet file contains a single row group
    and when the entire file fits inside an HDFS block. In reality, it’s hard to achieve
    this goal when you’re writing Parquet files in systems such as MapReduce, but
    it’s good to keep this in mind as you’re producing Parquet files.
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当Parquet文件包含单个行组且整个文件适合HDFS块时，工具如Impala工作得最好。在现实世界中，当你使用MapReduce等系统写入Parquet文件时很难实现这一目标，但当你生成Parquet文件时，记住这一点是好的。
- en: We’ve covered working with common file formats and working with various data
    serialization tools for tighter compatibility with MapReduce. It’s time to look
    at how you can support file formats that may be proprietary to your organization,
    or even public file formats for which no input or output formats exist for MapReduce.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了处理常见文件格式和使用各种数据序列化工具以实现与MapReduce更紧密兼容性的方法。现在是时候看看你如何支持可能属于你组织的专有文件格式，或者对于MapReduce没有输入或输出格式的公共文件格式。
- en: 3.5\. Custom file formats
  id: totrans-690
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 自定义文件格式
- en: In any organization you’ll typically find a plethora of custom or uncommon file
    formats that litter its datacenters. There may be back-end servers dumping out
    audit files in a proprietary format, or old code or systems that write files using
    formats that aren’t in common use any longer. If you want to work with such data
    in MapReduce, you’ll need to write your own input and output format classes to
    work with your data. This section will walk you through that process.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何组织中，你通常都会发现大量的自定义或非标准文件格式散布在其数据中心。可能有后端服务器以专有格式输出审计文件，或者旧的代码或系统使用不再通用的格式写入文件。如果你想在MapReduce中处理此类数据，你需要编写自己的输入和输出格式类来处理你的数据。本节将指导你完成这个过程。
- en: 3.5.1\. Input and output formats
  id: totrans-692
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1\. 输入和输出格式
- en: At the start of this chapter, we took a high-level look at the functions of
    input and output format classes in MapReduce. Input and output classes are required
    to feed data to map functions and to write the outputs of reduce functions.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我们概述了MapReduce中输入和输出格式类的作用。输入和输出类是向map函数提供数据和写入reduce函数输出的必需品。
- en: Technique 25 Writing input and output formats for CSV
  id: totrans-694
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧25：为CSV编写输入和输出格式
- en: Imagine you have a bunch of data sitting around in CSV files and you’re writing
    multiple MapReduce jobs that read and write data in CSV form. Because CSV is text,
    you could use the built-in `TextInputFormat` and `TextOutputFormat`, and handle
    parsing the CSV in your MapReduce code. This can quickly get tiring, however,
    and result in the same parsing code being copied and pasted across all of your
    jobs.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一堆数据存储在CSV文件中，你正在编写多个MapReduce作业，这些作业以CSV形式读取和写入数据。因为CSV是文本格式，你可以使用内置的`TextInputFormat`和`TextOutputFormat`，并在MapReduce代码中处理CSV的解析。然而，这可能会很快变得令人疲惫，并且导致相同的解析代码被复制粘贴到所有的作业中。
- en: If you thought MapReduce had any built-in CSV input and output formats that
    could take care of this parsing, you’d be out of luck—there are none.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为MapReduce有任何内置的CSV输入和输出格式可以处理这种解析，那么你可能运气不佳——没有。
- en: Problem
  id: totrans-697
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with CSV in MapReduce and have CSV records presented to you
    in a richer format than you’d get if you were using a `TextInputFormat` that would
    supply a string representing a line.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce中使用CSV，并且希望以比使用`TextInputFormat`提供的字符串行表示更丰富的格式来展示CSV记录。
- en: Solution
  id: totrans-699
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Write an input and output format that works with CSV.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个与CSV一起工作的输入和输出格式。
- en: Discussion
  id: totrans-701
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: We’ll cover all of the steps required to write your own format classes to work
    with CSV input and output. CSV is one of the simpler file formats to work with,
    which will make it easier to focus on MapReduce format specifics without having
    to think too much about the file format.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖编写自己的格式类以与CSV输入和输出一起工作的所有步骤。CSV是易于处理的一种文件格式，这将使得在不太多考虑文件格式的情况下，更容易关注MapReduce格式细节。
- en: Your custom `InputFormat` and `RecordReader` classes will parse CSV files and
    supply the data to the mapper in a user-friendly format. You’ll also support a
    custom field separator for non-comma delimiters. Because you don’t want to reinvent
    the wheel, you’ll use the CSV parser in the open source OpenCSV project ([http://opencsv.sourceforge.net/](http://opencsv.sourceforge.net/)),
    which will take care of quoted fields and ignore separator characters in quoted
    fields.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 你的自定义`InputFormat`和`RecordReader`类将解析CSV文件，并以用户友好的格式向mapper提供数据。你还将支持非逗号分隔符的自定义字段分隔符。因为你不想重新发明轮子，你将使用开源OpenCSV项目中的CSV解析器([http://opencsv.sourceforge.net/](http://opencsv.sourceforge.net/))，它将处理引号字段并忽略引号字段中的分隔符。
- en: '|  |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Overview of `InputFormat` and`OutputFormat`
  id: totrans-705
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`InputFormat`和`OutputFormat`概述'
- en: I provided a detailed overview of `InputFormat` and `OutputFormat` and their
    related classes at the start of this chapter. It may be worth looking back at
    that discussion prior to looking at the code in this technique.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章开头提供了对`InputFormat`和`OutputFormat`及其相关类的详细概述。在查看本技术中的代码之前，回顾一下那个讨论可能是有价值的。
- en: '|  |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The InputFormat
  id: totrans-708
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入格式
- en: Your first step is to define the `InputFormat`. The function of `InputFormat`
    is to validate the set of inputs supplied to the job, identify input splits, and
    create a `RecordReader` class to read input from the sources. The following code
    reads the separator (if supplied) from the job configuration and constructs a
    `CSVRecordReader`:^([[54](#ch03fn56)])
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一步是定义`InputFormat`。`InputFormat`的功能是验证作业提供的输入集，识别输入分割，并创建一个`RecordReader`类来从源读取输入。以下代码从作业配置中读取分隔符（如果提供）并构建一个`CSVRecordReader`类：^([[54](#ch03fn56)])
- en: '^(54) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java).'
  id: totrans-710
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(54) GitHub源代码：](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java)
- en: '![](130fig01_alt.jpg)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![130fig01_alt.jpg](130fig01_alt.jpg)'
- en: '|  |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '`InputFormat` and compressed files'
  id: totrans-713
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`InputFormat`和压缩文件'
- en: In the preceding code, you saw that when the was compressed, a flag was returned
    to indicate that it couldn’t be split. The reason for doing this is that compression
    codecs aren’t splittable, apart from LZOP. But splittable LZOP can’t work with
    regular `InputFormat` classes—it needs special-case LZOP `InputFormat` classes.
    These details are covered in [chapter 4](kindle_split_014.html#ch04).
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你看到当数据被压缩时，会返回一个标志来指示它不能被分割。这样做的原因是压缩编解码器是不可分割的，除了LZOP。但是可分割的LZOP不能与常规的`InputFormat`类一起工作——它需要特殊的LZOP
    `InputFormat`类。这些细节在[第4章](kindle_split_014.html#ch04)中有详细说明。
- en: '|  |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Your `InputFormat` class is complete. You extended the `FileInputFormat` class,
    which contains code that calculates input splits along HDFS block boundaries,
    keeping you from having to handle calculating the input splits yourself. The `FileInputFormat`
    manages all of the input files and splits for you. Now let’s move on to the `RecordReader`,
    which will require a little more effort.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 `InputFormat` 类已经完成。您扩展了 `FileInputFormat` 类，其中包含计算输入分片沿 HDFS 块边界的代码，这样您就无需自己处理计算输入分片。`FileInputFormat`
    会为您管理所有输入文件和分片。现在让我们继续到 `RecordReader`，这需要更多的努力。
- en: '`RecordReader` performs two main functions. It must first open the input source
    based on the input split supplied, and it optionally seeks into a specific offset
    in that input split. The second function of the `RecordReader` is to read individual
    records from the input source.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecordReader` 执行两个主要功能。它必须首先根据提供的输入分片打开输入源，并且可以选择在该输入分片中的特定偏移量处进行查找。`RecordReader`
    的第二个功能是从输入源读取单个记录。'
- en: 'In this example, a logical record equates to a line in the CSV file, so you’ll
    use the existing `LineRecordReader` class in MapReduce to handle working with
    the file. When the `RecordReader` is initialized with the `InputSplit`, it will
    open the input file, seek to the start of the input split, and keep reading characters
    until it reaches the start of the next record, which in the case of a line means
    a newline. The following code shows a simplified version of the `LineRecordReader.initialize`
    method:'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，逻辑记录等同于 CSV 文件中的一行，所以您将使用现有的 `LineRecordReader` 类在 MapReduce 中处理文件。当
    `RecordReader` 使用 `InputSplit` 初始化时，它将打开输入文件，跳转到输入分片的开头，并继续读取字符，直到它到达下一个记录的开始，在这种情况下，行意味着换行符。以下代码显示了
    `LineRecordReader.initialize` 方法的简化版本：
- en: '![](131fig01_alt.jpg)'
  id: totrans-719
  prefs: []
  type: TYPE_IMG
  zh: '![图片](131fig01_alt.jpg)'
- en: The `LineRecordReader` returns key/value pairs for each line in `LongWritable`/`Text`
    form. Because you’ll want to provide some functionality in the `Record Reader`,
    you need to encapsulate the `LineRecordReader` within your class. The `RecordReader`
    needs to supply a key/value pair representation of the record to the mapper, and
    in this case the key is the byte offset in the file, and the value is an array
    containing the tokenized parts of the CSV line:^([[55](#ch03fn57)])
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '`LineRecordReader` 为 `LongWritable`/`Text` 形式的每一行返回键值对。因为您希望在 `Record Reader`
    中提供一些功能，所以您需要将 `LineRecordReader` 封装在您的类中。`RecordReader` 需要向 mapper 提供记录的键值对表示，在这种情况下，键是文件中的字节偏移量，值是包含
    CSV 行分词部分的数组：^([[55](#ch03fn57)])'
- en: '^(55) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java).'
  id: totrans-721
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([55](#ch03fn57)) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java)。
- en: '![](132fig01_alt.jpg)'
  id: totrans-722
  prefs: []
  type: TYPE_IMG
  zh: '![图片](132fig01_alt.jpg)'
- en: Next you need to provide methods to read the next record and to get at the key
    and value for that record:^([[56](#ch03fn58)])
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要提供读取下一个记录和获取该记录键值的方法：^([[56](#ch03fn58)])
- en: '^(56) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java).'
  id: totrans-724
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([56](#ch03fn58)) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVInputFormat.java)。
- en: '![](132fig02_alt.jpg)'
  id: totrans-725
  prefs: []
  type: TYPE_IMG
  zh: '![图片](132fig02_alt.jpg)'
- en: At this point, you’ve created an `InputFormat` and a `RecordReader` that both
    can work with CSV files. Now that you’ve completed the `InputFormat`, it’s time
    to move on to the `OutputFormat`.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经创建了一个可以处理 CSV 文件的 `InputFormat` 和 `RecordReader`。现在您已经完成了 `InputFormat`，是时候转向
    `OutputFormat` 了。
- en: OutputFormat
  id: totrans-727
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OutputFormat
- en: '`OutputFormat` classes follow a pattern similar to `InputFormat` classes; the
    `OutputFormat` class handles the logistics around creating the output stream and
    then delegates the stream writes to the `RecordWriter`.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '`OutputFormat` 类遵循与 `InputFormat` 类相似的模式；`OutputFormat` 类处理创建输出流的相关后勤工作，然后将流写入委托给
    `RecordWriter`。'
- en: The `CSVOutputFormat` indirectly extends the `FileOutputFormat` class (via `TextOutputFormat`),
    which handles all of the logistics related to creating the output filename, creating
    an instance of a compression codec (if compression was enabled), and output committing,
    which we’ll discuss shortly.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '`CSVOutputFormat` 间接扩展了 `FileOutputFormat` 类（通过 `TextOutputFormat`），它处理与创建输出文件名、创建压缩编解码器实例（如果启用了压缩）以及输出提交相关的所有后勤工作，我们将在稍后讨论。'
- en: That leaves the `OutputFormat` class with the tasks of supporting a custom field
    delimiter for your CSV output file, and of creating a compressed `OutputStream`
    if required. It must also return your `CSVRecordWriter`, which will write CSV
    lines to the output stream:^([[57](#ch03fn59)])
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 这就留下了 `OutputFormat` 类，它负责支持 CSV 输出文件的定制字段分隔符，并在需要时创建压缩的 `OutputStream`。它还必须返回你的
    `CSVRecordWriter`，该 `CSVRecordWriter` 将 CSV 行写入输出流：^([[57](#ch03fn59)])
- en: '^(57) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java).'
  id: totrans-731
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([57) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java)).
- en: '![](133fig01_alt.jpg)'
  id: totrans-732
  prefs: []
  type: TYPE_IMG
  zh: '![](133fig01_alt.jpg)'
- en: In the following code, your `RecordWriter` writes each record emitted by the
    reducer to the output destination. You require that the reducer output key be
    in array form representing each token in the CSV line, and you specify that the
    reducer output value must be a `NullWritable`, which means that you don’t care
    about the value part of the output.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，你的 `RecordWriter` 将归约器发出的每个记录写入输出目的地。你需要归约器输出键以数组形式表示 CSV 行中的每个标记，并指定归约器输出值必须是
    `NullWritable`，这意味着你不在乎输出值部分。
- en: Let’s take a look at the `CSVRecordWriter` class. The constructor, which only
    sets the field separator and the output stream, is excluded, as shown in the following
    listing.^([[58](#ch03fn60)])
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `CSVRecordWriter` 类。构造函数，它只设置字段分隔符和输出流，被省略了，如下所示：^([[58](#ch03fn60)])
- en: '^(58) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java).'
  id: totrans-735
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([58) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVOutputFormat.java)).
- en: Listing 3.6\. A `RecordWriter` that produces MapReduce output in CSV form
  id: totrans-736
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.6\. 一个生成 MapReduce CSV 输出的 `RecordWriter`
- en: '![](ch03ex06-0.jpg)'
  id: totrans-737
  prefs: []
  type: TYPE_IMG
  zh: '![](ch03ex06-0.jpg)'
- en: '![](ch03ex06-1.jpg)'
  id: totrans-738
  prefs: []
  type: TYPE_IMG
  zh: '![](ch03ex06-1.jpg)'
- en: Now you need to apply the new input and output format classes in a MapReduce
    job.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要在 MapReduce 作业中应用新的输入和输出格式类。
- en: MapReduce
  id: totrans-740
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce
- en: Your MapReduce job will take CSV as input, and it’ll produce CSV that’s separated
    by colons, not commas. The job will perform identity map and reduce functions,
    which means that you won’t be changing the data as it passes through MapReduce.
    Your input file will be delimited with the tab character, and your output file
    will be comma-separated. Your input and output format classes will support the
    notion of custom delimiters via Hadoop configuration properties.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 MapReduce 作业将以 CSV 作为输入，并生成以冒号分隔的 CSV，而不是逗号分隔。作业将执行恒等映射和归约函数，这意味着你不会在 MapReduce
    传输过程中更改数据。你的输入文件将以制表符为分隔符，你的输出文件将以逗号分隔。你的输入和输出格式类将通过 Hadoop 配置属性支持自定义分隔符的概念。
- en: The MapReduce code is as follows:^([[59](#ch03fn61)])
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 代码如下：^([[59](#ch03fn61)])
- en: '^(59) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java).'
  id: totrans-743
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([59) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java)).
- en: '![](135fig01_alt.jpg)'
  id: totrans-744
  prefs: []
  type: TYPE_IMG
  zh: '![](135fig01_alt.jpg)'
- en: The map and reduce functions don’t do much other than echo their inputs to output,
    but include them so you can see how to work with the CSV in your MapReduce code:^([[60](#ch03fn62)])
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 映射和归约函数除了将输入回显到输出之外，没有做太多的事情，但包括它们，以便你可以看到如何在 MapReduce 代码中处理 CSV：^([[60](#ch03fn62)])
- en: '^(60) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java).'
  id: totrans-746
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([60) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch3/csv/CSVMapReduce.java)).
- en: '![](136fig01_alt.jpg)'
  id: totrans-747
  prefs: []
  type: TYPE_IMG
  zh: '![](136fig01_alt.jpg)'
- en: 'If you run this example MapReduce job against a tab-delimited file, you can
    examine the mapper output and see if the results are as expected:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个示例 MapReduce 作业针对制表符分隔的文件，你可以检查映射器的输出，看看结果是否符合预期：
- en: '[PRE60]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: You now have a functional `InputFormat` and `OutputFormat` that can consume
    and produce CSV in MapReduce.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有一个功能性的 `InputFormat` 和 `OutputFormat`，可以在 MapReduce 中消费和生成 CSV。
- en: Pig
  id: totrans-751
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 猪肉
- en: Pig’s piggybank library contains a `CSVLoader` that can be used to load CSV
    files into tuples. It supports double-quoted fields in the CSV records and provides
    each item as a byte array.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: Pig的piggybank库包含一个`CSVLoader`，它可以用来将CSV文件加载到元组中。它支持CSV记录中的双引号字段，并将每个项目作为字节数组提供。
- en: There’s a GitHub project called csv-serde ([https://github.com/ogrodnek/csv-serde](https://github.com/ogrodnek/csv-serde)),
    which has a Hive SerDe that can both serialize and deserialize CSV. Like the previous
    `InputFormat` example, it also uses the OpenCSV project for reading and writing
    CSV.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个名为csv-serde ([https://github.com/ogrodnek/csv-serde](https://github.com/ogrodnek/csv-serde))的GitHub项目，它有一个Hive
    SerDe，可以序列化和反序列化CSV。像之前的`InputFormat`示例一样，它也使用OpenCSV项目来读取和写入CSV。
- en: Summary
  id: totrans-754
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique demonstrated how you can write your own MapReduce format classes
    to work with text-based data. Work is currently underway in MapReduce to add a
    CSV input format (see [https://issues.apache.org/jira/browse/MAPREDUCE-2208](https://issues.apache.org/jira/browse/MAPREDUCE-2208)).
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术展示了如何编写自己的MapReduce格式类来处理基于文本的数据。目前MapReduce正在进行添加CSV输入格式的工作（见[https://issues.apache.org/jira/browse/MAPREDUCE-2208](https://issues.apache.org/jira/browse/MAPREDUCE-2208))。
- en: Arguably, it would have been simpler to use the `TextInputFormat` and split
    the line in the mapper. But if you need to do this multiple times, you’re likely
    suffering from the copy-paste antipattern, because the same code to tokenize the
    CSV likely exists in multiple locations. If the code is written with code reuse
    in mind, you’ll be covered.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 争议性地讲，使用`TextInputFormat`并在映射器中分割行可能更简单。但如果你需要多次这样做，你很可能是受到了复制粘贴反模式的困扰，因为用于标记CSV的相同代码可能存在于多个位置。如果代码是考虑到代码复用而编写的，那么你将得到保障。
- en: We’ve looked at how you can write your own I/O format classes to work with a
    custom file format in MapReduce. Now we need to look at a crucial aspect of working
    with output formats—output committing.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何编写自己的I/O格式类来与MapReduce中的自定义文件格式一起工作。现在我们需要关注与输出格式一起工作的一个关键方面——输出提交。
- en: 3.5.2\. The importance of output committing
  id: totrans-758
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2. 输出提交的重要性
- en: In the CSV `OutputFormat` example in the previous technique, you extended `FileOutputFormat`,
    which takes care of committing output after the task has succeeded. Why do you
    need commits in MapReduce, and why should you care?
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一种技术中的CSV`OutputFormat`示例中，你扩展了`FileOutputFormat`，它在任务成功后负责提交输出。为什么在MapReduce中需要提交，你为什么应该关心这个问题？
- en: As a job and its tasks are executing, they will start writing job output at
    some point. Tasks and jobs can fail, they can be restarted, and they can also
    be speculatively executed.^([[61](#ch03fn63)]) To allow `OutputFormat`s to correctly
    handle these scenarios, MapReduce has the notion of an `OutputCommitter`, which
    is a mechanism by which MapReduce invokes a callback when an individual task as
    well as the overall job have completed.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 在作业及其任务执行过程中，它们将在某个时刻开始写入作业输出。任务和作业可能会失败，它们可以被重新启动，也可以进行推测性执行.^([[61](#ch03fn63)])
    为了允许`OutputFormat`s正确处理这些场景，MapReduce有一个名为`OutputCommitter`的概念，这是一种机制，MapReduce在单个任务以及整个作业完成时调用回调。
- en: ^(61) Speculative executing is when MapReduce executes multiple tasks for the
    same input data to guard against slow or misbehaving nodes slowing down the overall
    job. By default, both map-side and reduce-side speculative execution is enabled.
    The `mapred.map.tasks.speculative.execution` and `mapred.reduce.tasks.speculative.execution`
    control this behavior.
  id: totrans-761
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(61) 推测性执行是指MapReduce对相同输入数据执行多个任务，以防止缓慢或行为异常的节点减慢整体作业的速度。默认情况下，映射端和减少端的推测性执行都是启用的。`mapred.map.tasks.speculative.execution`和`mapred.reduce.tasks.speculative.execution`控制这种行为。
- en: Most `OutputFormat`s in MapReduce use `FileOutputFormat`, which uses `FileOutput-Committer`
    for its output committing. When the `FileOutputFormat` is initially consulted
    about the location of the output files, it delegates the decision of where the
    output should be located to the `FileOutputCommitter`, which in turn specifies
    that the output should go to a temporary directory under the job output directory
    (<job-output>/_temporary/<task-attempt-id>). Only after the overall task has completed
    will the `FileOutputCommitter` be notified, at which point the temporary output
    is moved to the job output directory. When the overall job has successfully completed,
    the `FileOutput-Committer` is again notified, and this time it touches a `_SUCCESS`
    file in the job output directory to help downstream processors know the job succeeded.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 中大多数 `OutputFormat` 都使用 `FileOutputFormat`，它使用 `FileOutputCommitter`
    来处理输出提交。当 `FileOutputFormat` 首次查询输出文件的存储位置时，它将决定输出文件应该存储在哪里的决策委托给 `FileOutputCommitter`，后者指定输出应该放在作业输出目录下的一个临时目录中（<job-output>/_temporary/<task-attempt-id>）。只有在整体任务完成之后，`FileOutputCommitter`
    才会被通知，此时临时输出会被移动到作业输出目录。当整体作业成功完成后，`FileOutputCommitter` 再次被通知，这次它在作业输出目录中创建一个
    `_SUCCESS` 文件，以帮助下游处理器知道作业已成功完成。
- en: This is great if your data sink is HDFS, where you can use `FileOutputFormat`
    and its committing mechanism. Things start to get trickier when you’re working
    with data sources other than files, such as a database. If, in such cases, idempotent
    writes (where the same operation can be applied multiple times without changing
    the result) are necessary, you’ll need to factor that into the design of your
    destination data store or `OutputFormat`.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据目的地是 HDFS，你可以使用 `FileOutputFormat` 和其提交机制，这很好。当你处理文件以外的数据源，如数据库时，事情开始变得复杂。在这种情况下，如果需要幂等写操作（即相同的操作可以多次应用而不改变结果），你需要将其考虑进目标数据存储或
    `OutputFormat` 的设计中。
- en: This topic is examined in more detail in [chapter 5](kindle_split_015.html#ch05),
    which covers exporting data from Hadoop to databases.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题在 [第 5 章](kindle_split_015.html#ch05) 中有更详细的探讨，该章涵盖了从 Hadoop 导出数据到数据库。
- en: 3.6\. Chapter summary
  id: totrans-765
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6. 章节总结
- en: The goal for this chapter was to show you how to work with common file formats
    such as XML and JSON in MapReduce. We also looked at more sophisticated file formats
    such as SequenceFile, Avro, and Parquet, which provide useful features for working
    with big data, such as versioning, compression, and complex data structures. We
    also walked through the process of working with custom file formats to ensure
    they’ll work in MapReduce.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是向你展示如何在 MapReduce 中处理常见的文件格式，如 XML 和 JSON。我们还探讨了更复杂的文件格式，如 SequenceFile、Avro
    和 Parquet，它们为处理大数据提供了有用的功能，例如版本控制、压缩和复杂的数据结构。我们还介绍了处理自定义文件格式的流程，以确保它们能在 MapReduce
    中正常工作。
- en: At this point, you’re equipped to work with any file format in MapReduce. Now
    it’s time to look at some storage patterns so you can effectively work with your
    data and optimize storage and disk/network I/O.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经准备好在 MapReduce 中处理任何文件格式了。现在，是时候看看一些存储模式，以便你能够有效地处理你的数据并优化存储和磁盘/网络
    I/O。
- en: Chapter 4\. Organizing and optimizing data in HDFS
  id: totrans-768
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 章. 在 HDFS 中组织和优化数据
- en: '*This chapter covers*'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Tips for laying out and organizing your data
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据布局和组织技巧
- en: Data access patterns to optimize reading and writing your data
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化读写数据的数据访问模式
- en: The importance of compression, and choosing the best codec for your needs
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩的重要性以及选择最适合你需求的编解码器
- en: In the previous chapter, we looked at how to work with different file formats
    in MapReduce and which ones were ideally suited for storing your data. Once you’ve
    honed in on the data format that you’ll be using, it’s time to start thinking
    about how you’ll organize your data in HDFS. It’s important that you give yourself
    enough time early on in the design of your Hadoop system to understand how your
    data will be accessed so that you can optimize for the more important use cases
    that you’ll be supporting.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何在 MapReduce 中处理不同的文件格式，以及哪些格式最适合存储你的数据。一旦你确定了将要使用的数据格式，就需要开始考虑如何在
    HDFS 中组织你的数据。在设计你的 Hadoop 系统的早期阶段，给自己留出足够的时间来理解数据如何被访问，这样你就可以针对你将要支持的更重要用例进行优化。
- en: There are numerous factors that will impact your data organization decisions,
    such as whether you’ll need to provide SQL access to your data (likely, you will),
    which fields will be used to look up the data, and what access-time SLAs you’ll
    need to support. At the same time, you need to make sure that you don’t apply
    unnecessary heap pressure on the HDFS NameNode with a large number of small files,
    and you also need to learn how to work with huge input datasets.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多因素会影响你的数据组织决策，例如你是否需要提供SQL访问你的数据（很可能你需要），哪些字段将用于查找数据，以及你需要支持什么访问时间的SLA。同时，你需要确保不会因为大量的小文件而对HDFS
    NameNode施加不必要的堆压力，你还需要学习如何处理巨大的输入数据集。
- en: This chapter is dedicated to looking at ways to efficiently store and access
    big data in HDFS. I’ll first cover ways you can lay out data in HDFS and present
    some methods for partitioning and combining data to relieve NameNode heap pressure.
    Then I’ll discuss some data access patterns to help you work with disparate data
    as well as huge data sets. And finally, we’ll look at compression as a big data
    pattern to maximize your storage and processing capabilities.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 本章致力于探讨在HDFS中高效存储和访问大数据的方法。我首先会介绍你如何在HDFS中布局数据，并展示一些用于分区和合并数据以减轻NameNode堆压力的方法。然后，我会讨论一些数据访问模式，以帮助你处理不同的数据以及大量数据集。最后，我们将探讨压缩作为大数据模式，以最大化你的存储和处理能力。
- en: '|  |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Chapter prerequisites
  id: totrans-777
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 章节先决条件
- en: This chapter assumes you have a basic understanding of HDFS concepts and that
    you have experience working directly with HDFS. If you need to become familiar
    with the topic, *Hadoop in Action* by Chuck Lam (Manning, 2010) offers the background
    information you’ll need on HDFS.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设你已对HDFS概念有基本的了解，并且你有直接与HDFS工作的经验。如果你需要熟悉这个主题，Chuck Lam的《Hadoop实战》（Manning,
    2010）提供了你需要的关于HDFS的背景信息。
- en: '|  |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We’ll start things off with a look at how you can organize and manage your data.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从如何组织和管理工作数据开始。
- en: 4.1\. Data organization
  id: totrans-781
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 数据组织
- en: Data organization is one of the most challenging aspects of working with Hadoop.
    You have pressure from different groups in your organization, such as data scientists
    and your cluster administrators, each coming to you with competing requirements.
    What’s more, these requirements often come after your data applications are in
    production and you’ve already amassed large amounts of data.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 数据组织是使用Hadoop时最具挑战性的方面之一。你的组织中的不同群体，如数据科学家和你的集群管理员，都会向你提出竞争性的要求。更重要的是，这些要求通常在数据应用投入生产并积累了大量数据之后才会出现。
- en: There are multiple dimensions to data organization in Hadoop. You first need
    to decide how to organize your data in HDFS, after which you’ll be faced with
    operational issues such as how to partition and compact your data. You’ll need
    to decide whether to enable Kerberos to secure your cluster and how to manage
    and communicate data changes. These are all complex issues, and the goal of this
    chapter is to focus on some of the more challenging aspects of data organization,
    including data partitioning and compaction, starting off with how you can structure
    your data in HDFS.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop中的数据组织有多个维度。首先，你需要决定如何在HDFS中组织你的数据，之后你将面临如何分区和压缩数据等操作问题。你需要决定是否启用Kerberos来保护你的集群，以及如何管理和沟通数据变更。这些都是复杂的问题，本章的目标是专注于数据组织的一些更具挑战性的方面，包括数据分区和压缩，从如何在HDFS中结构化你的数据开始。
- en: 4.1.1\. Directory and file layout
  id: totrans-784
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1. 目录和文件布局
- en: Having a cluster-wide standard that defines how data is organized is a worthwhile
    pursuit, as it makes it easier to discover where data is located, and it also
    helps apply structure and manage common areas that you want to address with data
    storage in general. Because we’re working within the confines of what a filesystem
    can express, a common approach to arranging data is to create a hierarchy of tiers
    that aligns with your organizational or functional structure. For example, if
    you work on the analytics team and you’re bringing a new dataset to the cluster,
    then one way to organize your directory would be as shown in [figure 4.1](#ch04fig01).
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群范围内有一个定义数据组织方式的标准化是值得追求的，因为它使得发现数据所在位置变得更容易，同时也帮助你在一般的数据存储中应用结构和管理工作需要解决的问题。由于我们是在文件系统可以表达的范围内工作，组织数据的一种常见方法就是创建一个与你的组织或功能结构相一致的多级层次结构。例如，如果你在分析团队工作，并且你将一个新的数据集带到集群中，那么组织你的目录的一种方法可以如图4.1所示。
- en: Figure 4.1\. An example HDFS directory layout
  id: totrans-786
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1\. 一个示例 HDFS 目录布局
- en: '![](04fig01.jpg)'
  id: totrans-787
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig01.jpg)'
- en: Data revolution
  id: totrans-788
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据革命
- en: Hopefully you’ve settled on a data format such as Avro, which offers you the
    ability to evolve your schema over time. That’s great, but how do you support
    a move to the Next Big Data Format, which will no doubt arrive as soon as everyone
    has migrated to Avro? Well, you can look to other software fields where semantic
    versioning concepts permeate interfaces such as URLs and adopt a similar strategy
    in your directory structure. By sticking a version number in your structure, you
    can give yourself the flexibility to move to the data format of tomorrow and communicate
    the differing file formats using the directory path.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你已经确定了一种数据格式，例如 Avro，它允许你在时间上演变你的模式。这很好，但当你支持迁移到下一个大数据格式时，这个格式无疑会在每个人都迁移到
    Avro 之后到来，你该如何应对呢？嗯，你可以参考其他软件领域，其中语义版本控制概念渗透到接口，如 URL，并在你的目录结构中采用类似的策略。通过在你的结构中添加版本号，你可以给自己提供灵活性，以便迁移到明天的数据格式，并使用目录路径来传达不同的文件格式。
- en: Once you’ve embraced putting a version number in your directory, the only challenge
    left is communicating future changes to the consumers of your data. If this becomes
    a challenge, you may want to look at HCatalog as a way to abstract away data formats
    from your clients.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你接受了在目录中放置版本号的做法，剩下的唯一挑战就是向你的数据消费者传达未来的变化。如果这成为一个挑战，你可能想看看 HCatalog 作为一种将数据格式从客户端抽象出来的方法。
- en: Partitioning by date and other fields
  id: totrans-791
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 按日期和其他字段分区
- en: You may need your directory structure to model your organizational and data
    evolution needs, but why would you need further partitioning by date? This is
    a technique that Hive used early on to help speed up queries. If you put all of
    your data into a single directory, you’re essentially doing the Hadoop equivalent
    of a full table scan every time you need to access the data. Instead, it’s smarter
    to partition your data based on how you expect your data to be accessed.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要你的目录结构来模拟你的组织和数据演变需求，但你为什么还需要按日期进一步分区呢？这是一种 Hive 在早期用来帮助加速查询的技术。如果你把所有数据放入一个单独的目录中，每次需要访问数据时，你实际上是在做
    Hadoop 的全表扫描。相反，更明智的做法是根据你预期如何访问数据来对数据进行分区。
- en: It can be hard to know ahead of time exactly how data is going to be accessed,
    but a reasonable first attempt at partitioning is to segment data by the date
    when it was generated. If your data doesn’t have a date, then talk to the data
    producers about adding one, as the time at which an event or record was created
    is a critical data point that should always be captured.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 在事先很难确切知道数据将如何被访问的情况下，一个合理的初步分区尝试是按数据生成日期来分割数据。如果你的数据没有日期，那么与数据生产者谈谈添加一个日期，因为事件或记录被创建的时间是一个关键的数据点，应该始终被捕获。
- en: 4.1.2\. Data tiers
  id: totrans-794
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 数据层级
- en: In his 2012 Strata talk, Eric Sammer presented the idea of storing different
    tiers of data.^([[1](#ch04fn01)]) This is a powerful concept, and it also ties
    in nicely with one of the primary tenets of Nathan Marz’s Lambda Architecture—that
    of never deleting or modifying your raw data.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2012 年的 Strata 演讲中，Eric Sammer 提出了存储不同层级数据的想法.^([[1](#ch04fn01)]) 这是一个强大的概念，并且它与
    Nathan Marz 的 Lambda 架构的一个主要原则——永不删除或修改原始数据——很好地结合在一起。
- en: ¹ Eric Sammer, “Large scale ETL with Hadoop,” [www.slideshare.net/OReillyStrata/large-scale-etl-with-hadoop](http://www.slideshare.net/OReillyStrata/large-scale-etl-with-hadoop).
  id: totrans-796
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ Eric Sammer，“使用 Hadoop 进行大规模 ETL”，[www.slideshare.net/OReillyStrata/large-scale-etl-with-hadoop](http://www.slideshare.net/OReillyStrata/large-scale-etl-with-hadoop)。
- en: At first glance, this may not seem to make any sense—surely once you extract
    the important parts of a data source, you can discard the rest! While it may seem
    wasteful to keep around raw data, especially if there are parts that aren’t being
    actively used, ask yourself this question—could some organizational value be extracted
    from the data in the future? It’s hard to answer this with a resounding “no.”
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这似乎没有意义——当然，一旦你提取了数据源的重要部分，你可以丢弃其余部分！虽然保留原始数据可能看起来很浪费，尤其是如果有些部分没有被积极使用，但问问自己这个问题——未来能否从数据中提取一些组织价值？很难给出一个响亮的“不”。
- en: There are also occasionally bugs in our software. Imagine that you’re streaming
    data from the Twitter fire hose, producing some aggregations and discarding the
    source data. What happens if you discover a bug in your aggregation logic? You
    have no way to go back and regenerate the aggregated data.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的软件偶尔也会有错误。想象一下，您正在从Twitter的数据流中提取数据，生成一些聚合数据并丢弃源数据。如果您发现聚合逻辑中的错误怎么办？您将无法回溯并重新生成聚合数据。
- en: 'Therefore, it’s recommended that you think of your data in terms of the following
    tiers:'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，建议您将数据视为以下层级：
- en: '*Raw data* is the first tier. It’s the unaltered data you capture from the
    source. Data at this tier should never be modified because there’s a chance that
    your logic that produces derivatives or aggregations has bugs, and if you discard
    the raw data, you’ll remove your ability to regenerate your derived data upon
    discovering your bugs.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原始数据* 是第一层。这是您从源捕获的未修改数据。这一层级的数据不应被修改，因为您的逻辑可能存在产生派生数据或聚合数据的错误，如果您丢弃原始数据，您将失去在发现错误后重新生成派生数据的能力。'
- en: '*Derived data* is created from the raw data. Here you can perform deduplication,
    sanitation, and any other cleansing.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*派生数据* 是从原始数据创建的。在这里，您可以执行去重、净化以及其他任何清理工作。'
- en: '*Aggregated data* is calculated from the derived data and will likely be fed
    into systems such as HBase or your NoSQL system of choice for real-time access
    to your data, both in production and for analytical purposes.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚合数据* 是从派生数据计算得出的，并可能被输入到HBase或您选择的NoSQL系统中，以便在生产和分析目的上实时访问您的数据。'
- en: Data tiers should also be expressed in your directory layout so that users can
    easily differentiate between the tiers.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 数据层级也应反映在目录布局中，以便用户可以轻松区分层级。
- en: Once you’ve decided on a directory layout for partitioning your data, the next
    step is figuring out how you’re going to get your data into these partitions.
    That’s covered next.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您为数据分区确定了目录布局，下一步就是确定如何将数据放入这些分区中。这一点将在下一部分介绍。
- en: 4.1.3\. Partitioning
  id: totrans-805
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3. 分区
- en: Partitioning is the process by which you take a dataset and split it into distinct
    parts. These parts are the partitions, and they represent a meaningful division
    of your data. An example of a common partition in data is time, as it allows those
    querying the data to narrow in on a specific window of time. The previous section
    included time as a key element in deciding how to lay out your data in HDFS.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 分区是将数据集分割成不同部分的过程。这些部分是分区，它们代表了您数据的有意义划分。数据中常见的分区示例是时间，因为它允许查询数据的人缩小到特定的时间窗口。上一节将时间作为决定如何在HDFS中布局数据的关键元素。
- en: Great! You have a large dataset in HDFS and you need to partition it. How do
    you go about doing that? In this section I’ll present two methods you can employ
    to partition your data.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！您在HDFS中有一个大型数据集，并且需要对其进行分区。您将如何进行？在本节中，我将介绍您可以使用两种方法来分区您的数据。
- en: Technique 26 Using MultipleOutputs to partition your data
  id: totrans-808
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧26 使用MultipleOutputs进行数据分区
- en: Imagine a situation where you have stock prices being streamed into HDFS, and
    you want to write a MapReduce job to partition your stock data based on the day
    of the stock quote. To do this, you’ll need to write to multiple output files
    in a single task. Let’s look at how you can make that happen.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这种情况：您有股票价格被流式传输到HDFS，并且您想编写一个MapReduce作业来根据股票报价的日期对股票数据进行分区。为此，您需要在单个任务中写入多个输出文件。让我们看看如何实现这一点。
- en: Problem
  id: totrans-810
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You need to partition your data, but most output formats only create a single
    output file per task.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要分区数据，但大多数输出格式在每个任务中只创建一个输出文件。
- en: Solution
  id: totrans-812
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `MultipleOutputs` class bundled with MapReduce.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与MapReduce捆绑的`MultipleOutputs`类。
- en: Discussion
  id: totrans-814
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The `MultipleOutputs` class in Hadoop bypasses the normal channel by which outputs
    are produced in Hadoop. It provides a separate API to write partitioned outputs,
    and it writes output directly to the task attempt directory in HDFS. This is powerful,
    as you can continue to collect output using the standard write method on the `Context`
    object supplied to your job, and also use `MultipleOutputs` to write partitioned
    output. Of course, you can also choose to only use the `MultipleOutputs` class
    and ignore the standard `Context`-based output.
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop中的`MultipleOutputs`类绕过了在Hadoop中产生输出的正常通道。它提供了一个单独的API来写入分区输出，并且直接将输出写入HDFS中的任务尝试目录。这非常强大，因为你可以继续使用提供给作业的`Context`对象的标准写入方法来收集输出，同时也可以使用`MultipleOutputs`来写入分区输出。当然，你也可以选择只使用`MultipleOutputs`类并忽略基于`Context`的标准输出。
- en: 'In this technique, you’ll use `MultipleOutputs` to partition stocks by their
    quote date. The first step is to set up `MultipleOutputs` for use in your job.
    In your driver, you’ll indicate the output format and the key and value types:'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你将使用`MultipleOutputs`根据股票报价日期对股票进行分区。第一步是为你的作业设置`MultipleOutputs`。在你的驱动程序中，你将指示输出格式以及键和值类型：
- en: '![](143fig01_alt.jpg)'
  id: totrans-817
  prefs: []
  type: TYPE_IMG
  zh: '![图片](143fig01_alt.jpg)'
- en: '|  |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Why do you need to name the output in the driver?
  id: totrans-819
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么你需要在驱动程序中命名输出？
- en: You may be wondering why `MultipleOutputs` requires you to specify a output
    name (`partition` in the preceding example). This is because `MultipleOutputs`
    supports two modes of operation—static partitions and dynamic partitions.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么`MultipleOutputs`要求你指定一个输出名称（在先前的例子中是`partition`）。这是因为`MultipleOutputs`支持两种操作模式——静态分区和动态分区。
- en: '*Static partitions* work well if you know ahead of time the partition names;
    this gives you the additional flexibility of specifying a different output format
    for each partition (you’d just have multiple calls to `MultipleOutputs.addNamedOutput`
    with different named outputs). With static partitions, the output name you specify
    when calling `addNamedOutput` is the same name that you use when emitting output
    in your mapper or reducer.'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '*静态分区*在你知道分区名称的情况下效果很好；这为你提供了额外的灵活性，可以为每个分区指定不同的输出格式（你只需对`MultipleOutputs.addNamedOutput`进行多次调用，使用不同的命名输出）。在静态分区中，你在调用`addNamedOutput`时指定的输出名称与你在mapper或reducer中发出输出时使用的名称相同。'
- en: This technique focuses on *dynamic partitions*, which you’re likely to find
    more useful, because in most cases you won’t know the partitions ahead of time.
    In this case, you still need to supply a output name, but for all intents and
    purposes, it’s ignored, as you can dynamically specify the partition name in your
    mapper or reducer.
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术侧重于*动态分区*，这可能会更实用，因为在大多数情况下，你事先不知道分区。在这种情况下，你仍然需要提供一个输出名称，但就实际目的而言，它被忽略，因为你可以动态地在你的mapper或reducer中指定分区名称。
- en: '|  |'
  id: totrans-823
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As you can see in the following code, your map (or reduce) class, will get a
    handle to a `MultipleOutputs` instance and then use its write method to write
    partitioned outputs. Notice that the third argument is the partition name, which
    is the stock date:^([[2](#ch04fn02)])
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码所示，你的map（或reduce）类将获取一个`MultipleOutputs`实例的句柄，然后使用其写入方法来写入分区输出。注意，第三个参数是分区名称，即股票日期:^([[2](#ch04fn02)])
- en: '² GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/MultipleOutputsJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/MultipleOutputsJob.java).'
  id: totrans-825
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/MultipleOutputsJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/MultipleOutputsJob.java).
- en: '![](144fig01_alt.jpg)'
  id: totrans-826
  prefs: []
  type: TYPE_IMG
  zh: '![图片](144fig01_alt.jpg)'
- en: '|  |'
  id: totrans-827
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Don’t forget the close method!
  id: totrans-828
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不要忘记调用close方法！
- en: It’s important that you call the close method on `MultipleOutputs` in the cleanup
    method of your task. Otherwise it’s possible that you’ll have data missing from
    your output or even a corrupt file.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的任务清理方法中调用`MultipleOutputs`的close方法非常重要。否则，你的输出中可能会缺少数据，甚至可能出现损坏的文件。
- en: '|  |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Let’s take a peek at this class in action. As you can see in the following
    output, running the previous example produces a number of partitioned files for
    the single mapper. You can also see the original map output file, which is empty
    because you haven’t emitted any records using the `Context` object:'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个类在实际中的应用。如以下输出所示，运行前面的示例为单个mapper生成了多个分区文件。你还可以看到原始的map输出文件，它是空的，因为你没有使用`Context`对象发出任何记录：
- en: '[PRE61]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In this example you used a map-only job, but in production you’ll probably
    want to limit the number of tasks that create partitions. There are two ways you
    can do this:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你使用了仅映射的作业，但在生产中你可能希望限制创建分区的任务数量。你可以通过两种方式来实现：
- en: Use the `CombineFileInputFormat` or a custom input format to limit the number
    of mappers in your job.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `CombineFileInputFormat` 或自定义输入格式来限制你的作业中的 mapper 数量。
- en: Use a reducer where you can explicitly specify a reasonable number of reducers.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可以显式指定合理数量的 reducer 的 reducer 中使用。
- en: Summary
  id: totrans-836
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: 'There are plenty of things to like about `MultipleOutputs`: its support for
    both “old” and “new” MapReduce APIs and its support for multiple output format
    classes. But using `MultipleOutputs` does carry with it some constraints that
    you should be aware of:'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultipleOutputs` 有许多值得喜欢的地方：它支持“旧”和“新”的 MapReduce API，以及支持多个输出格式类。但使用 `MultipleOutputs`
    也伴随着一些你应该注意到的限制：'
- en: Be cautious when using `MultipleOutputs` in a mapper—remember that you’ll end
    up with NumberOfMappers * NumberOfPartition output files, which in my experience
    can bring down clusters with large numbers of both values!
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 mapper 中使用 `MultipleOutputs` 时要小心——记住，你最终会得到 NumberOfMappers * NumberOfPartition
    输出文件，这在我的经验中可能会因为大量值而使集群崩溃！
- en: Each partition incurs the overhead of an HDFS file handle for the duration of
    the task.
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分区都会在任务执行期间产生一个 HDFS 文件句柄的开销。
- en: You can often end up with a large number of small files that accumulate across
    multiple uses of your partitioner. You’ll probably want to make sure that you
    have a compaction strategy in place to mitigate this problem (see [section 4.1.4](#ch04lev2sec4)
    for more details).
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能会最终得到大量的小文件，这些文件会在多次使用你的分区器时累积。你可能需要确保你有一个压缩策略来减轻这个问题（更多详情请见[第4.1.4节](#ch04lev2sec4)）。
- en: Although Avro comes with the `AvroMultipleOutputs` class, it’s quite slow due
    to some inefficiencies in the code.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管 Avro 随带 `AvroMultipleOutputs` 类，但由于代码中的一些低效，它相当慢。
- en: In addition to the `MultipleOutputs` approach, Hadoop also comes with a `MultipleOutputFormat`
    class that has features similar to `MultipleOutputs`. Its primary pitfalls are
    that it only supports the old MapReduce API and only one output format can be
    used for all the partitions.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `MultipleOutputs` 方法之外，Hadoop 还提供了一个具有类似功能的 `MultipleOutputFormat` 类。它主要的缺点是它只支持旧的
    MapReduce API，并且所有分区只能使用一个输出格式。
- en: Another partitioning strategy that you can employ is to use the MapReduce partitioner,
    which can help mitigate the large number of files that may be produced using `MultipleOutputs`.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以采用另一种分区策略，即使用 MapReduce 分区器，这可以帮助减轻使用 `MultipleOutputs` 可能产生的文件数量过多的问题。
- en: Technique 27 Using a custom MapReduce partitioner
  id: totrans-844
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 27 使用自定义 MapReduce 分区器
- en: Another partitioning approach is to use the partitioning facilities built into
    Map-Reduce. By default, MapReduce uses a hash partitioner that calculates the
    hash of each map output key and performs a modulo over the number of reducers
    to determine which reducer the record should be sent to. You can control how partitioning
    occurs by writing your own custom partitioner and then route records according
    to your partitioning scheme.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分区方法是使用 Map-Reduce 内置的分区功能。默认情况下，MapReduce 使用一个哈希分区器，它计算每个映射输出键的哈希值，并对 reducer
    的数量进行取模运算，以确定记录应该发送到哪个 reducer。你可以通过编写自己的自定义分区器来控制分区方式，然后根据你的分区方案路由记录。
- en: This technique has an added benefit over the previous technique in that you’ll
    generally end up with fewer output files because each reducer will only create
    a single output file, as opposed to `MultipleOutputs`, where each map or reduce
    task will generate *N* output files—one for each partition.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一种技术相比，这种技术有一个额外的优点，那就是你通常会得到更少的输出文件，因为每个 reducer 只会创建一个输出文件，而 `MultipleOutputs`
    则是每个映射或减少任务都会生成 *N* 个输出文件——每个分区一个。
- en: Problem
  id: totrans-847
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to partition your input data.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要对输入数据进行分区。
- en: Solution
  id: totrans-849
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Write a custom partitioner that partitions records to the appropriate reducer.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个自定义分区器，将记录分区到适当的 reducer。
- en: Discussion
  id: totrans-851
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Let’s look at the custom partitioner first. It exposes a helper method to the
    Map-Reduce driver that allows you to define a mapping from a date to a partition,
    and it writes this mapping to the job configuration. Then, when MapReduce loads
    the partitioner, MapReduce calls the `setConf` method; in this partitioner you’ll
    read the mappings into a map, which is subsequently used when partitioning.^([[3](#ch04fn03)])
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们看看自定义分区器。它向 Map-Reduce 驱动程序提供了一个辅助方法，允许你定义一个从日期到分区的映射，并将这个映射写入作业配置。然后，当
    MapReduce 加载分区器时，MapReduce 会调用 `setConf` 方法；在这个分区器中，你会将映射读入一个映射中，这个映射随后在分区时会被使用.^([[3](#ch04fn03)])
- en: '³ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java).'
  id: totrans-853
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java).
- en: '![](146fig01_alt.jpg)'
  id: totrans-854
  prefs: []
  type: TYPE_IMG
  zh: '![](146fig01_alt.jpg)'
- en: Your driver code needs to set up the custom partitioner configuration. The partitions
    in this example are dates, and you want to make sure that each reducer will correspond
    to a unique date. The stocks example data has 10 unique dates, so you configure
    your job with 10 reducers. You also call the partition helper function that was
    defined previously to set up the configuration that maps each unique date to a
    unique reducer.^([[4](#ch04fn04)])
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 你的驱动代码需要设置自定义分区器配置。在这个例子中，分区是日期，你需要确保每个减少器都对应一个唯一的日期。股票示例数据有 10 个唯一的日期，所以你配置你的作业使用
    10 个减少器。你还调用之前定义的分区辅助函数来设置配置，将每个唯一的日期映射到一个唯一的减少器上.^([[4](#ch04fn04)])
- en: '⁴ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java).'
  id: totrans-856
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java).
- en: '![](147fig01_alt.jpg)'
  id: totrans-857
  prefs: []
  type: TYPE_IMG
  zh: '![](147fig01_alt.jpg)'
- en: The mapper does little other than extract the stock date from the input data
    and emit it as the output key:^([[5](#ch04fn05)])
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 映射器除了从输入数据中提取股票日期并将其作为输出键发出之外，几乎不做其他事情:^([[5](#ch04fn05)])
- en: '⁵ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java).'
  id: totrans-859
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CustomPartitionerJob.java).
- en: '![](147fig02_alt.jpg)'
  id: totrans-860
  prefs: []
  type: TYPE_IMG
  zh: '![](147fig02_alt.jpg)'
- en: 'The command to run the preceding example is as follows:'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面示例的命令如下：
- en: '[PRE62]'
  id: totrans-862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This job will generate 10 output files, each containing the stocks for that
    day.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 这个作业将生成 10 个输出文件，每个文件包含该天的股票数据。
- en: Summary
  id: totrans-864
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Using the MapReduce framework to naturally partition your data gives you a
    couple of advantages:'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MapReduce 框架自然地分区你的数据为你带来了一些优势：
- en: Data in your partitions will be sorted because the shuffle will ensure that
    all data streamed to a reducer will be sorted. This allows you to use optimized
    join strategies on your data.
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你分区中的数据将会被排序，因为洗牌会确保所有发送到减少器的数据流都是排序的。这允许你使用优化的连接策略来处理你的数据。
- en: You can deduplicate data in the reducer, again as a benefit of the shuffle phase.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在减少器中去除重复数据，这同样是洗牌阶段的益处。
- en: The main problem to look out for with this technique is data skew. You want
    to make sure that you can spread the load across reducers as much as possible,
    which may be a challenge if there’s a natural skew in your data. For example,
    if your partitions are days, then it’s possible that the majority of your records
    will be for a single day, and you may have only a few records for either a previous
    or following day. In this case, you’ll ideally want to partition records in a
    way that allocates the majority of the reducers to a single day, and then maybe
    one or two for the previous or following days. You can also sample your inputs
    and dynamically determine the optimal number of reducers based on your sample
    data.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术时需要注意的主要问题是数据倾斜。你想要确保尽可能地将负载分散到减少器上，如果数据中存在自然倾斜，这可能是一个挑战。例如，如果你的分区是按天的话，那么可能大多数记录都是某一天的数据，而你可能只有少量记录是前一天或后一天的数据。在这种情况下，你理想的做法是按记录分区，将大多数减少器分配给某一天，然后可能为前一天或后一天分配一个或两个。你也可以采样你的输入，并根据样本数据动态确定最优的减少器数量。
- en: Once you’ve produced your partitioned output, the next challenge is how to deal
    with the potentially large number of small files that have resulted from the partitioning.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你生成了分区输出，下一个挑战是如何处理分区后可能产生的大量小文件。
- en: 4.1.4\. Compacting
  id: totrans-870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. 压缩
- en: 'Sometimes having small files in HDFS can’t be avoided—maybe you’re using a
    partitioning technique similar to those described previously, or maybe your data
    organically lands in HDFS in small file sizes. Either way, you’ll be exposing
    some weaknesses in HDFS and MapReduce, including the following:'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候在HDFS中拥有小文件是无法避免的——也许你正在使用与之前描述类似的数据分区技术，或者也许你的数据自然地以小文件大小落在HDFS中。无论如何，你都会暴露HDFS和MapReduce的一些弱点，包括以下内容：
- en: Hadoop’s NameNode keeps all the HDFS metadata in memory for fast metadata operations.
    Yahoo! estimated that each file, on average, occupies 600 bytes of space in memory,^([[6](#ch04fn06)])
    which translates to a metadata overhead of one billion files amounting to 60 GB,
    all of which needs to be stored in the NameNode’s memory. That’s a lot of memory
    for a single process, even with today’s mid-tier server RAM capacities.
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop的NameNode将所有HDFS元数据保留在内存中，以便快速进行元数据操作。雅虎估计，每个文件平均占用600字节的内存空间，^([[6](#ch04fn06)])
    这相当于10亿个文件导致的60GB元数据开销，所有这些都需要存储在NameNode的内存中。即使是今天的中端服务器RAM容量，这也需要大量的内存来处理单个进程。
- en: ⁶ According to Yahoo! statistics, each block or file inode uses less than 200
    bytes of memory, and on average each file occupies 1.5 blocks with a 3x replication
    factor. See Yahoo!’s page titled “Scalability of the Hadoop Distributed File System,”
    [http://developer.yahoo.com/blogs/hadoop/posts/2010/05/scalability_of_the_hadoop_dist/](http://developer.yahoo.com/blogs/hadoop/posts/2010/05/scalability_of_the_hadoop_dist/)
    and a JIRA ticket titled “Name-node memory size estimates and optimization proposal,”
    [https://issues.apache.org/jira/browse/HADOOP-1687](https://issues.apache.org/jira/browse/HADOOP-1687).
  id: totrans-873
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ 根据雅虎的统计，每个块或文件inode使用的内存不到200字节，平均每个文件占用1.5个块，复制因子为3。参见雅虎页面“Hadoop分布式文件系统的可扩展性”，[http://developer.yahoo.com/blogs/hadoop/posts/2010/05/scalability_of_the_hadoop_dist/](http://developer.yahoo.com/blogs/hadoop/posts/2010/05/scalability_of_the_hadoop_dist/)
    和一个标题为“Name-node内存大小估计和优化建议”的JIRA工单，[https://issues.apache.org/jira/browse/HADOOP-1687](https://issues.apache.org/jira/browse/HADOOP-1687)。
- en: If your input to a MapReduce job is a large number of files, the number of mappers
    that will run (assuming your files are text or splittable) would be equivalent
    to the number of blocks that these files occupy. If you run a MapReduce job whose
    input is thousands or millions of files, your job will spend more time at the
    kernel layer dealing with creating and destroying your map task processes than
    it will on its work.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你提交给MapReduce作业的输入是大量文件，将要运行的mapper数量（假设你的文件是文本或可分割的）将与这些文件占用的块数量相当。如果你运行一个输入是数千或数百万个文件的MapReduce作业，你的作业将花费更多的时间在内核层处理创建和销毁你的map任务进程，而不是在它的工作上。
- en: Finally, if you’re running in a controlled environment where there’s a scheduler,
    you may have a cap on the number of tasks your MapReduce job can use. Because
    each file (by default) results in at least one map task, this could cause your
    job to be rejected by the scheduler.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果你在一个有调度器的受控环境中运行，你的MapReduce作业可能对可用的任务数量有限制。因为每个文件（默认情况下）至少会产生一个map任务，这可能导致你的作业被调度器拒绝。
- en: If you’re thinking you won’t have this problem, think again. What percentage
    of your files are smaller than the HDFS block size?^([[7](#ch04fn07)]) And how
    much smaller are they—50%, 70%, 90%? What if your big data project takes off and
    suddenly you need to be able to scale to handle datasets that are several orders
    of magnitude greater in size? Isn’t that why you use Hadoop in the first place?
    To scale, you want to be able to add more nodes and then get back to your morning
    coffee. You don’t want to have to go back and redesign your use of Hadoop and
    deal with migrating your files. Thinking and preparing for this eventuality is
    best done early in your design phase.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为你不会遇到这个问题，再想想看。你有多少比例的文件大小小于HDFS块大小？^([[7](#ch04fn07)]) 它们又小了多少——50%，70%，90%？如果你的大数据项目突然起飞，你需要能够扩展以处理比现在大几个数量级的数据集呢？这难道不是你最初使用Hadoop的原因吗？为了扩展，你希望能够添加更多的节点，然后回到你的早晨咖啡。你不想不得不回去重新设计你的Hadoop使用方式，处理文件迁移。思考和准备这种可能性最好在设计的早期阶段进行。
- en: ⁷ The default block size is 1,238 MB. Check the value of `dfs.block.size` to
    see what it’s set to in your cluster.
  id: totrans-877
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ 默认的块大小为1,238 MB。检查您的集群中`dfs.block.size`的值以查看其设置。
- en: This section examines some techniques that you can use to combine your data
    in HDFS. I’ll start off by discussing a utility called filecrush, which can compact
    small files together to create a smaller number of larger files. I’ll also show
    you how Avro can be used as a container format to store files that can’t be easily
    compacted, such as binary files.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了您可以用于在HDFS中合并数据的某些技术。我将从一个名为filecrush的工具开始讨论，该工具可以将小文件压缩在一起以创建更少的较大文件。我还会向您展示如何使用Avro作为容器格式来存储难以压缩的文件，例如二进制文件。
- en: Technique 28 Using filecrush to compact data
  id: totrans-879
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧28：使用filecrush压缩数据
- en: Compacting is the act of combining small files together to produce larger files—this
    helps alleviate heap pressure on the NameNode. In this technique, you’ll learn
    about an open source utility you can use to compact data and help keep your cluster
    administrator happy.
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩是将小文件合并成大文件的行为——这有助于减轻NameNode的堆压力。在此技术中，您将了解一个开源工具，您可以使用它来压缩数据并帮助您的集群管理员保持愉快。
- en: '|  |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Compatibility with Hadoop versions
  id: totrans-882
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与Hadoop版本的兼容性
- en: Currently the filecrush utility only works with Hadoop version 1\. I’m writing
    a simple file compacter that’s compatible with Hadoop 2 at [https://github.com/alexholmes/hdfscompact](https://github.com/alexholmes/hdfscompact).
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，filecrush工具仅与Hadoop版本1兼容。我在[https://github.com/alexholmes/hdfscompact](https://github.com/alexholmes/hdfscompact)编写了一个简单的文件压缩器，该压缩器与Hadoop
    2兼容。
- en: '|  |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Problem
  id: totrans-885
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to combine small files to reduce the metadata that the NameNode needs
    to keep in memory.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望合并小文件以减少NameNode需要保留在内存中的元数据。
- en: Solution
  id: totrans-887
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the filecrush utility.
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 使用filecrush工具。
- en: Discussion
  id: totrans-889
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The filecrush utility^([[8](#ch04fn08)]) combines or compacts multiple small
    files to form larger files. The utility is quite sophisticated and gives you the
    ability to
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: filecrush工具^([[8](#ch04fn08)])将多个小文件合并或压缩成大文件。该工具相当复杂，并赋予您以下能力：
- en: ⁸ The filecrush GitHub project page is located at [https://github.com/edwardcapriolo/filecrush](https://github.com/edwardcapriolo/filecrush).
  id: totrans-891
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ filecrush GitHub项目页面位于[https://github.com/edwardcapriolo/filecrush](https://github.com/edwardcapriolo/filecrush)。
- en: Determine the size threshold below which files will be compacted (and by association,
    leave files that are large enough alone)
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定文件压缩的阈值大小，低于此大小的文件将被压缩（并且相应地，将保留足够大的文件）
- en: Specify the maximum size of the compacted files
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定压缩文件的最大大小
- en: Work with different input and output formats and different input and output
    compression codecs (useful for moving to a different file format or compression
    codec)
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与不同的输入和输出格式以及不同的输入和输出压缩编解码器（对于迁移到不同的文件格式或压缩编解码器很有用）
- en: Swap smaller files with newer compacted files in place
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在原地用新的压缩文件替换较小的文件
- en: We’ll use filecrush on a straightforward example—we’ll crush a single directory
    of small text files and replace them with gzipped SequenceFiles.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个简单的示例中使用filecrush——我们将压缩单个目录中的小文本文件，并用gzip压缩的SequenceFiles替换它们。
- en: 'First, artificially create 10 input files in a directory in HDFS:'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在HDFS中的一个目录中人工创建10个输入文件：
- en: '[PRE63]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now run filecrush. In this example, you’ll replace the small files with the
    new large file, and also convert from a text file to a compressed SequenceFile:'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行filecrush。在此示例中，您将用新的大文件替换小文件，并将文本文件转换为压缩的SequenceFile：
- en: '![](150fig01_alt.jpg)'
  id: totrans-900
  prefs: []
  type: TYPE_IMG
  zh: '![](150fig01_alt.jpg)'
- en: 'After running filecrush, you’ll observe that the files in the input directory
    have been replaced by a single SequenceFile:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 运行filecrush后，您会观察到输入目录中的文件已被单个SequenceFile替换：
- en: '[PRE64]'
  id: totrans-902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'You can also run the `text` Hadoop command to view the text representation
    of the SequenceFile:'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以运行`text` Hadoop命令来查看SequenceFile的文本表示：
- en: '[PRE65]'
  id: totrans-904
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You’ll also notice that the original small files have all been moved to the
    output directory that you specified in your command:'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会注意到，原始的小文件已经全部移动到您在命令中指定的输出目录中：
- en: '[PRE66]'
  id: totrans-906
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: If you had run filecrush without the `--clone` option, the input files would
    have remained intact, and the crushed file would have been written to the output
    directory.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有使用`--clone`选项运行filecrush，输入文件将保持完整，压缩文件将被写入输出目录。
- en: Input and output file size thresholds
  id: totrans-908
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出文件大小阈值
- en: How does filecrush determine whether files need to be crushed? It looks at each
    file in the input directory and compares it to the block size (or in Hadoop 2,
    the size that you specified in `-Ddfs.block.size` in the command). If the file
    is less than 75% of the block size, it will be crushed. This threshold can be
    customized by supplying the `--threshold` argument—for example, if you wanted
    to raise the value to 85%, you’d specify `--threshold 0.85`.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: filecrush如何确定文件是否需要被压缩？它会查看输入目录中的每个文件，并将其与块大小（或在Hadoop 2中，你在命令中指定的`-Ddfs.block.size`的大小）进行比较。如果文件小于块大小的75%，它将被压缩。可以通过提供`--threshold`参数来自定义此阈值——例如，如果你想将值提高到85%，你将指定`--threshold
    0.85`。
- en: Similarly, filecrush uses the block size to determine the output file sizes.
    By default, it won’t create output files that occupy more than eight blocks, but
    this can be customized with the `--max-file-blocks` argument.
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，filecrush使用块大小来确定输出文件的大小。默认情况下，它不会创建占用超过八个块的输出文件，但可以通过`--max-file-blocks`参数进行自定义。
- en: Summary
  id: totrans-911
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Filecrush is a simple and quick way to combine small files together. It supports
    any type of input or output files as long as there are associated input format
    and output format classes. Unfortunately, it doesn’t work with Hadoop 2, and there
    hasn’t been much activity in the project over the last few years, so these points
    may rule out this utility for your environment.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: Filecrush是一种简单快捷地将小文件合并在一起的方法。只要存在相关的输入格式和输出格式类，它就支持任何类型的输入或输出文件。不幸的是，它不与Hadoop
    2兼容，而且过去几年中项目活动不多，因此这些点可能排除这个实用工具在你的环境中使用。
- en: The example presented in this technique works well in situations where the directory
    being crushed is an external Hive table, or if you’re running it against a directory
    in a standard location where other users in a cluster expect your data to exist.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术中展示的示例在以下情况下效果良好：正在压缩的目录是一个外部Hive表，或者如果你正在对标准位置中的目录运行它，而集群中的其他用户预期你的数据将存在于该位置。
- en: Currently, the filecrush project doesn’t work with Hadoop 2\. If you’re looking
    for a solution for Hadoop 2, take a look at another HDFS compactor that I’m currently
    working on at [https://github.com/alexholmes/hdfscompact](https://github.com/alexholmes/hdfscompact).
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，filecrush项目不与Hadoop 2兼容。如果你正在寻找Hadoop 2的解决方案，请查看我目前正在[https://github.com/alexholmes/hdfscompact](https://github.com/alexholmes/hdfscompact)上开发的其他HDFS压缩器。
- en: Because filecrush requires input and output formats, one use case where it falls
    short is if you’re working with binary data and you need a way to combine small
    binary files together.
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 由于filecrush需要输入和输出格式，一个它表现不佳的用例是如果你正在处理二进制数据，并且需要一种方法来合并小二进制文件。
- en: Technique 29 Using Avro to store multiple small binary files
  id: totrans-916
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧29 使用Avro存储多个小二进制文件
- en: Let’s say that you’re working on a project akin to Google Images, where you
    crawl the web and download image files from websites. Your project is internet-scale,
    so you’re downloading millions of files and storing them individually in HDFS.
    You already know that HDFS doesn’t work well with a large number of small files,
    but you’re dealing with binary data, so the previous technique doesn’t fit your
    needs.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在从事一个类似于谷歌图片的项目，你从网站上爬取网页并下载图片文件。你的项目是互联网规模的，因此你正在下载数百万个文件，并将它们分别存储在HDFS中。你已经知道HDFS不适合处理大量的小文件，但你现在处理的是二进制数据，所以之前的技术不适合你的需求。
- en: This technique shows how you can use Avro as a container file format for binary
    data in HDFS.
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术展示了你如何使用Avro作为HDFS中二进制数据的容器文件格式。
- en: Problem
  id: totrans-919
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to store a large number of binary files in HDFS, and to do so without
    hitting the NameNode memory limits.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在HDFS中存储大量二进制文件，并且在不触及NameNode内存限制的情况下做到这一点。
- en: Solution
  id: totrans-921
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: The easiest way to work with small binary files in HDFS is to package them into
    a larger containing file. For this technique, you’ll read all of the files in
    a directory stored on local disk and save them in a single Avro file in HDFS.
    You’ll also see how to use the Avro file in MapReduce to process the contents
    of the original files.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 在HDFS中处理小二进制文件的最简单方法是将它们打包到一个更大的容器文件中。对于这个技巧，你将读取存储在本地磁盘上的目录中的所有文件，并将它们保存为HDFS中的一个单独的Avro文件。你还将了解如何使用Avro文件在MapReduce中处理原始文件的内容。
- en: Discussion
  id: totrans-923
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: '[Figure 4.2](#ch04fig02) shows the first part of this technique, where you
    create the Avro file in HDFS. In doing so, you create fewer files in HDFS, which
    means less data to be stored in NameNode memory, which also means you can store
    more stuff.'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.2](#ch04fig02)展示了这种技术的第一部分，其中你在HDFS中创建Avro文件。这样做可以减少HDFS中的文件数量，这意味着存储在NameNode内存中的数据更少，这也意味着你可以存储更多东西。'
- en: Figure 4.2\. Storing small files in Avro allows you to store more.
  id: totrans-925
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2\. 在Avro中存储小文件可以使你存储更多。
- en: '![](04fig02_alt.jpg)'
  id: totrans-926
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig02_alt.jpg)'
- en: Avro is a data serialization and RPC library invented by Doug Cutting, the creator
    of Hadoop. Avro has strong schema-evolution capabilities that give it an advantage
    over competitors such as SequenceFile. Avro and its competitors were covered extensively
    in [chapter 3](kindle_split_013.html#ch03).
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: Avro是由Hadoop的创造者Doug Cutting发明的一种数据序列化和RPC库。Avro具有强大的模式演变能力，使其在SequenceFile等竞争对手中具有优势。在第3章中详细介绍了Avro及其竞争对手。
- en: Take a look at the Java code in the following listing, which will create the
    Avro file.^([[9](#ch04fn09)])
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下列表中的Java代码，该代码将创建Avro文件.^([[9](#ch04fn09)])
- en: '⁹ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesWrite.java).'
  id: totrans-929
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesWrite.java).
- en: Listing 4.1\. Read a directory containing small files and produce a single Avro
    file in HDFS
  id: totrans-930
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1\. 读取包含小文件的目录并生成HDFS中的单个Avro文件
- en: '![](ch04ex01-0.jpg)'
  id: totrans-931
  prefs: []
  type: TYPE_IMG
  zh: '![](ch04ex01-0.jpg)'
- en: '![](ch04ex01-1.jpg)'
  id: totrans-932
  prefs: []
  type: TYPE_IMG
  zh: '![](ch04ex01-1.jpg)'
- en: '|  |'
  id: totrans-933
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Compression dependency
  id: totrans-934
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 压缩依赖
- en: To run the code in this chapter, you’ll need to have both the Snappy and LZOP
    compression codecs installed on your host. Please refer to the appendix for details
    on how to install and configure them.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的代码，你需要在你的主机上安装Snappy和LZOP压缩编解码器。请参阅附录以获取有关如何安装和配置它们的详细信息。
- en: '|  |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Let’s see what happens when you run this script against Hadoop’s config directory
    (replace `$HADOOP_CONF_DIR` with the directory containing your Hadoop configuration
    files):'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当你运行此脚本针对Hadoop的配置目录（将`$HADOOP_CONF_DIR`替换为包含你的Hadoop配置文件的目录）时会发生什么：
- en: '[PRE67]'
  id: totrans-938
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Looks promising—let’s make sure that the output file is in HDFS:'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很有希望——让我们确保输出文件在HDFS中：
- en: '[PRE68]'
  id: totrans-940
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: To be sure everything’s working as expected, you can also write some code that
    will read the Avro file from HDFS and output the MD5 hash for each file’s content:^([[10](#ch04fn10)])
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保一切按预期工作，你也可以编写一些代码来从HDFS读取Avro文件并输出每个文件内容的MD5哈希值:^([[10](#ch04fn10)])
- en: '^(10) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesRead.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesRead.java).'
  id: totrans-942
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesRead.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesRead.java).
- en: '![](154fig01_alt.jpg)'
  id: totrans-943
  prefs: []
  type: TYPE_IMG
  zh: '![](154fig01_alt.jpg)'
- en: 'This code is simpler than the write. Because Avro writes the schema into every
    Avro file, you don’t need to give Avro any information about the schema as part
    of deserialization. Give the code a spin:'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码比写入简单。因为Avro将模式写入每个Avro文件，所以在反序列化过程中你不需要向Avro提供任何关于模式的信息。给这段代码试一下：
- en: '![](154fig02_alt.jpg)'
  id: totrans-945
  prefs: []
  type: TYPE_IMG
  zh: '![](154fig02_alt.jpg)'
- en: At this point you have Avro files in HDFS. Even though this chapter is about
    HDFS, the next thing you’ll likely want to do is process the files that you wrote
    in MapReduce. Let’s look at how to do that, writing a map-only MapReduce job that
    can read the Avro records as input and write out a text file containing the filenames
    and MD5 hashes of the file contents, as shown in [figure 4.3](#ch04fig03).
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经在HDFS中有Avro文件。尽管本章是关于HDFS的，但接下来你很可能会想要处理你在MapReduce中编写的文件。让我们看看如何做到这一点，编写一个只包含map的MapReduce作业，它可以读取Avro记录作为输入，并输出包含文件名和文件内容MD5哈希值的文本文件，如图4.3所示。
- en: Figure 4.3\. Map job to read Avro files and write out a text file
  id: totrans-947
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3\. 将Map作业映射到读取Avro文件并输出文本文件
- en: '![](04fig03_alt.jpg)'
  id: totrans-948
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig03_alt.jpg)'
- en: The next listing shows the code for this MapReduce job.^([[11](#ch04fn11)])
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了此MapReduce作业的代码.^([[11](#ch04fn11)])
- en: '^(11) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesMapReduce.java).'
  id: totrans-950
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/SmallFilesMapReduce.java).
- en: Listing 4.2\. A MapReduce job that takes as input Avro files containing the
    small files
  id: totrans-951
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.2\. 一个以包含小文件的Avro文件为输入的MapReduce作业
- en: '![](ch04ex02-0.jpg)'
  id: totrans-952
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch04ex02-0.jpg)'
- en: '![](ch04ex02-1.jpg)'
  id: totrans-953
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch04ex02-1.jpg)'
- en: 'If you run this MapReduce job over the Avro file you created earlier, the job
    log files will contain your filenames and hashes:'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个之前创建的Avro文件上运行这个MapReduce作业，作业日志文件将包含你的文件名和哈希值：
- en: '![](156fig01_alt.jpg)'
  id: totrans-955
  prefs: []
  type: TYPE_IMG
  zh: '![图片](156fig01_alt.jpg)'
- en: In this technique, it was assumed that you were working with a file format (such
    as image files) that couldn’t have separate files concatenated together. If your
    files can be concatenated, you should consider that option. If you go this route,
    try your best to make sure that the file size is at least as large as the HDFS
    block size to minimize the data stored in NameNode.
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种技术中，假设你正在处理一个无法将单独文件连接在一起的文件格式（例如图像文件）。如果你的文件可以连接，你应该考虑这个选项。如果你选择这条路，尽量确保文件大小至少与HDFS块大小一样大，以最小化存储在NameNode中的数据。
- en: Summary
  id: totrans-957
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: You could have used Hadoop’s SequenceFile as a mechanism to hold your small
    files. SequenceFile is a more mature technology, having been around longer than
    Avro files. But SequenceFiles are Java-specific, and they don’t provide the rich
    interoperability and versioning semantics you get with Avro.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 你本可以使用Hadoop的SequenceFile作为存储小文件的机制。SequenceFile是一种更成熟的技术，它的历史比Avro文件更长。但是SequenceFiles是Java特定的，并且它们不提供与Avro相同的丰富互操作性和版本语义。
- en: Google’s Protocol Buffers, as well as Apache Thrift (which originated from Facebook),
    can also be used to store small files. But neither has a input format that works
    with native Thrift or Protocol Buffers files.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: Google的Protocol Buffers以及Apache Thrift（起源于Facebook）也可以用来存储小文件。但它们都没有与原生Thrift或Protocol
    Buffers文件一起工作的输入格式。
- en: Another approach you could use is to write the files into a zip file. The downsides
    to this approach are first that you’d have to write a custom input format^([[12](#ch04fn12)])
    to process the zip file, and second that zip files aren’t splittable (as opposed
    to Avro files and SequenceFiles). This could be mitigated by generating multiple
    zip files and attempting to make them close to the HDFS block size.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用将文件写入zip文件的方法。这种方法的不利之处首先在于，你必须编写一个自定义输入格式^([[12](#ch04fn12)])来处理zip文件，其次在于zip文件是不可分割的（与Avro文件和SequenceFiles相反）。这可以通过生成多个zip文件并尝试使它们接近HDFS块大小来缓解。
- en: ^(12) There has been a ticket open since 2008 asking for a zip input format
    implementation; see [https://issues.apache.org/jira/browse/MAPREDUCE-210](https://issues.apache.org/jira/browse/MAPREDUCE-210).
  id: totrans-961
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12) 自2008年以来，一直有一个请求zip输入格式实现的工单；请参阅[https://issues.apache.org/jira/browse/MAPREDUCE-210](https://issues.apache.org/jira/browse/MAPREDUCE-210)。
- en: Hadoop also has a `CombineFileInputFormat` that can feed multiple input splits
    (across multiple files) into a single map task, which greatly decreases the number
    of map tasks needed to run.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop还有一个`CombineFileInputFormat`，可以将多个输入拆分（跨越多个文件）输入到单个map任务中，这大大减少了运行所需的map任务数量。
- en: You also could have created a tarball file containing all the files, and then
    produced a separate text file that contained the locations of the tarball file
    in HDFS. This text file would be supplied as the input to the MapReduce job, and
    the mapper would open the tarball directly. But that approach would circumvent
    the locality in Map-Reduce, because the mappers would be scheduled to execute
    on the node that contained the text file, and would therefore likely need to read
    the tarball blocks from remote HDFS nodes, incurring unnecessary network I/O.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以创建一个包含所有文件的tarball文件，然后生成一个包含tarball文件在HDFS中的位置的单独文本文件。这个文本文件将被提供给MapReduce作业，mapper将直接打开tarball。但这种方法将绕过Map-Reduce中的局部性，因为mapper将被调度在包含文本文件的节点上执行，因此很可能需要从远程HDFS节点读取tarball块，从而产生不必要的网络I/O。
- en: Hadoop Archive files (HARs) are Hadoop files specifically created to solve the
    problem of small files. They are a virtual filesystem that sits on top of HDFS.
    The disadvantages of HAR files are that they can’t be optimized for local disk
    access in Map-Reduce, and they can’t be compressed.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop存档文件（HARs）是专门为解决小文件问题而创建的Hadoop文件。它们是一个虚拟文件系统，位于HDFS之上。HAR文件的不利之处在于，它们不能在Map-Reduce中对本地磁盘访问进行优化，并且不能压缩。
- en: Hadoop version 2 supports HDFS Federation, where HDFS is partitioned into multiple
    distinct namespaces, with each independently managed by a separate NameNode. This,
    in effect, means that the overall impact of keeping block information in memory
    can be spread across multiple NameNodes, thereby supporting a much larger number
    of small files. Hortonworks has a good blog post that contains more details about
    HDFS Federation (“An Introduction to HDFS Federation” [August 23, 2011], [http://hortonworks.com/an-introduction-to-hdfs-federation/](http://hortonworks.com/an-introduction-to-hdfs-federation/)).
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2版本支持HDFS联邦，其中HDFS被划分为多个不同的命名空间，每个命名空间由一个单独的NameNode独立管理。实际上，这意味着将块信息保留在内存中的总体影响可以分散到多个NameNode上，从而支持更多的少量文件。Hortonworks有一篇很好的博客文章，其中包含有关HDFS联邦的更多详细信息（“HDFS联邦简介”
    [2011年8月23日]，[http://hortonworks.com/an-introduction-to-hdfs-federation/](http://hortonworks.com/an-introduction-to-hdfs-federation/))。
- en: Finally, MapR, which provides a Hadoop distribution, has its own distributed
    filesystem that supports large numbers of small files. Using MapR for your distributed
    storage is a big change to your system, so it’s unlikely you’ll move to MapR to
    mitigate this problem with HDFS.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，MapR，它提供了一个Hadoop发行版，有自己的分布式文件系统，支持大量的小文件。使用MapR进行分布式存储对你的系统来说是一个很大的改变，因此你不太可能迁移到MapR来缓解HDFS中的这个问题。
- en: You may encounter times when you’ll want to work with small files in Hadoop,
    and using them directly would result in bloated NameNode memory use and MapReduce
    jobs that run slowly. This technique helps you mitigate these issues by packaging
    small files into larger container files. I picked Avro for this technique because
    of its support for splittable files and compression and its expressive schema
    language, which will help with versioning.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到想要在Hadoop中处理少量文件的情况，直接使用它们会导致NameNode内存使用量增加，并且MapReduce作业运行缓慢。这项技术通过将小文件打包到更大的容器文件中来帮助你缓解这些问题。我选择Avro来使用这项技术，因为它支持可分割文件和压缩，以及其表达式的模式语言，这有助于版本控制。
- en: What if you have the opposite problem, where your files are big and you want
    to be more efficient about how you store your data? Our coverage of compression
    in Hadoop ([section 4.2](#ch04lev1sec2)) will come to your rescue in these situations.
    But before we get to that section, let’s continue with our look at data organization
    and discover some tips on how to move data atomically in HDFS.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有相反的问题，即你的文件很大，你想要更有效地存储数据，会发生什么？在我们的Hadoop中关于压缩的覆盖（[第4.2节](#ch04lev1sec2)）将在这些情况下帮助你。但在我们到达该部分之前，让我们继续关注数据组织，并发现一些关于如何在HDFS中原子性地移动数据的技巧。
- en: 4.1.5\. Atomic data movement
  id: totrans-969
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5\. 原子数据移动
- en: 'Activities such as partitioning and compacting tend to follow a similar pattern—they
    produce output files in a staging directory, and then need to atomically move
    them to their final destination once all the output files have been successfully
    staged. This may bring up some questions:'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 活动如分区和压缩往往遵循类似的模式——它们在临时目录中生成输出文件，然后一旦所有输出文件都成功放置，就需要将它们原子性地移动到最终目的地。这可能会引发一些问题：
- en: What trigger do you use to determine that you’re ready to perform the atomic
    move?
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你使用什么触发器来确定你准备好执行原子移动？
- en: How do you move data atomically in HDFS?
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何在HDFS中原子性地移动数据？
- en: What impact does your data movement have on any readers of the final data?
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据移动对最终数据的任何读者有什么影响？
- en: It may be tempting to perform the atomic move as a postprocessing step within
    your MapReduce driver, but what will happen if the client process dies before
    the Map-Reduce application completes? This is where using the `OutputCommitter`
    in Hadoop is useful, because you can perform any atomic file movement as part
    of your job, as opposed to using the driver. An example of the `OutputCommitter`
    is shown in [section 3.5.2](kindle_split_013.html#ch03lev2sec15).
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会诱使你在MapReduce驱动程序中作为一个后处理步骤执行原子移动，但如果客户端进程在Map-Reduce应用程序完成之前死亡，会发生什么？这就是在Hadoop中使用`OutputCommitter`有用的地方，因为你可以将任何原子文件移动作为作业的一部分执行，而不是使用驱动程序。`OutputCommitter`的一个示例在[第3.5.2节](kindle_split_013.html#ch03lev2sec15)中展示。
- en: 'The next question is how you can move data atomically in HDFS. For the longest
    time, it was thought that the `rename` method on the `DistributedFileSystem` class
    (which is the concrete implementation supporting HDFS) was atomic. But it turns
    out that there are situations where this isn’t an atomic operation. This was remedied
    in HADOOP-6240, but for backward compatibility reasons, the `rename` method wasn’t
    updated. As a result, the `rename` method is still not truly atomic; instead,
    you need to use a new API. As you can see, the code is cumbersome and it only
    works with newer versions of Hadoop:'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的问题是您如何在 HDFS 中进行原子性数据移动。长期以来，人们认为 `DistributedFileSystem` 类（HDFS 的具体实现支持）上的
    `rename` 方法是原子的。但事实并非如此，在某些情况下，这并不是一个原子操作。这在 HADOOP-6240 中得到了修复，但由于向后兼容性的原因，`rename`
    方法没有被更新。因此，`rename` 方法仍然不是真正原子的；相反，您需要使用一个新的 API。如您所见，代码相当繁琐，并且仅适用于 Hadoop 的新版本：
- en: '[PRE69]'
  id: totrans-976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: One thing that’s missing from HDFS is the ability to atomically swap directories.
    This would be useful in situations such as compacting, where you need to replace
    the entire contents of a directory that is being used by other processes such
    as Hive. There’s an open JIRA ticket titled “Atomic Directory Swapping Operation”
    ([https://issues.apache.org/jira/browse/HDFS-5902](https://issues.apache.org/jira/browse/HDFS-5902))
    that will hopefully provide this ability in the future.
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 缺少的一项功能是原子性地交换目录的能力。这在诸如压缩等场景中非常有用，比如需要替换由其他进程（如 Hive）使用的目录的全部内容。有一个名为“原子目录交换操作”的开放
    JIRA 工单（[https://issues.apache.org/jira/browse/HDFS-5902](https://issues.apache.org/jira/browse/HDFS-5902)），希望未来能提供这项功能。
- en: It’s important that you factor the points discussed here into the design of
    your system. And if you’re using a third-party utility or library, try to determine
    whether it’s atomically moving data.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 将这里讨论的点纳入您系统的设计中非常重要。如果您使用第三方实用程序或库，尝试确定它是否进行原子性数据移动。
- en: This concludes our look at data organization techniques. Let’s switch to another
    important data management topic in Hadoop, that of data compression.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对数据组织技术的探讨。让我们转向 Hadoop 中另一个重要的数据管理主题——数据压缩。
- en: 4.2\. Efficient storage with compression
  id: totrans-980
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 使用压缩进行高效存储
- en: Data compression is a mechanism that reduces data to a more compact form to
    save on storage space and to make it more efficient to transfer the data. Compression
    is an important aspect of dealing with files, and it becomes all the more important
    when dealing with the data sizes that Hadoop supports. Your goal with Hadoop is
    to be as efficient as possible when working with your data, and picking a suitable
    compression codec will result in your jobs running faster and allow you to store
    more data in your cluster.^([[13](#ch04fn13)])
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩是一种将数据压缩成更紧凑的形式以节省存储空间并提高数据传输效率的机制。压缩是处理文件的重要方面，当处理 Hadoop 支持的数据大小时，这一点尤为重要。使用
    Hadoop 的目标是尽可能高效地处理数据，选择合适的压缩编解码器可以使您的作业运行得更快，并允许您在集群中存储更多的数据。[^([13](#ch04fn13))]
- en: ^(13) A compression codec is a programming implementation capable of reading
    and writing a given compression format.
  id: totrans-982
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[^([13](#ch04fn13))] 压缩编解码器是一种能够读取和写入特定压缩格式的编程实现。'
- en: Technique 30 Picking the right compression codec for your data
  id: totrans-983
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 30 选择适合您数据的压缩编解码器
- en: Using compression with HDFS isn’t as transparent as it is on filesystems such
    as ZFS,^([[14](#ch04fn14)]) especially when dealing with compressed files that
    can be split (more on that later in this chapter). One of the advantages of working
    with file formats such as Avro and SequenceFile is their built-in compression
    support, making compression almost completely transparent to users. But you lose
    that luxury when working with file formats such as text.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HDFS 进行压缩并不像在 ZFS 等文件系统上那样透明，尤其是在处理可以分割的压缩文件时（关于这一点将在本章后面详细说明）。与 Avro 和 SequenceFile
    等文件格式一起工作时，其内置的压缩支持使得压缩对用户来说几乎完全透明。但是，当与文本等文件格式一起工作时，您就失去了这种便利。
- en: ^(14) ZFS, short for Z File System, is a filesystem developed by Sun Microsystems
    that provides innovative features to enhance data integrity.
  id: totrans-985
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[^([14](#ch04fn14))] ZFS，即 Z 文件系统，是由 Sun Microsystems 开发的一种文件系统，它提供了一些创新功能来增强数据完整性。'
- en: Problem
  id: totrans-986
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to evaluate and determine the optimal compression codec for use with
    your data.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望评估和确定最适合您数据使用的压缩编解码器。
- en: Solution
  id: totrans-988
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Snappy, a compression codec from Google, offers the best combination of compressed
    size and read/write execution times. But LZOP is the best codec when working with
    large compressed files that must support splittability.
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: Snappy，来自谷歌的压缩编解码器，提供了压缩大小和读写执行时间的最佳组合。但 LZOP 是处理必须支持可分割性的大压缩文件时最佳的编解码器。
- en: Discussion
  id: totrans-990
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Let’s kick things off with a quick look at the compression codecs available
    for use in Hadoop, shown in [table 4.1](#ch04table01).
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从快速查看 Hadoop 中可用的压缩编解码器开始，如 [表 4.1](#ch04table01) 所示。
- en: Table 4.1\. Compression codecs
  id: totrans-992
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.1\. 压缩编解码器
- en: '| Codec | Background |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| 编解码器 | 背景 |'
- en: '| --- | --- |'
  id: totrans-994
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Deflate | Deflate is similar to zlib, which is the same compression algorithm
    that gzip uses but without the gzip headers. |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
  zh: '| Deflate | Deflate 与 zlib 类似，这是与 gzip 使用相同的压缩算法，但没有 gzip 头部。|'
- en: '| gzip | The gzip file format consists of a header and a body, which contains
    a Deflate-compressed payload. |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '| gzip | gzip 文件格式由一个头部和一个主体组成，其中包含一个 Deflate 压缩的有效负载。|'
- en: '| bzip2 | bzip2 is a space-efficient compression codec. |'
  id: totrans-997
  prefs: []
  type: TYPE_TB
  zh: '| bzip2 | bzip2 是一种节省空间的压缩编解码器。|'
- en: '| LZO | LZO is a block-based compression algorithm that allows the compressed
    data to be split. |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
  zh: '| LZO | LZO 是一种基于块的压缩算法，允许压缩数据被分割。|'
- en: '| LZOP | LZOP is LZO with additional headers. At one time, LZO/LZOP came bundled
    with Hadoop, but they have since been removed due to GPL licensing restrictions.
    |'
  id: totrans-999
  prefs: []
  type: TYPE_TB
  zh: '| LZOP | LZOP 是带有额外头部的 LZO。曾经，LZO/LZOP 与 Hadoop 一起捆绑提供，但由于 GPL 许可证限制，它们已经被移除。|'
- en: '| LZ4 | LZ4 is a speedy derivative of the same compression algorithm on which
    LZO is based. |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
  zh: '| LZ4 | LZ4 是基于与 LZO 相同压缩算法的快速衍生版本。|'
- en: '| Snappy | Snappy ([http://code.google.com/p/hadoop-snappy/](http://code.google.com/p/hadoop-snappy/))
    is a recent addition to the codec options in Hadoop. It’s Google’s open source
    compression algorithm. Google uses it for compressing data in both MapReduce and
    BigTable.^([[a](#ch04fn15)]) Snappy’s main drawback is that it’s not splittable.
    If you’re working with file formats that support splitting, such as Avro or Parquet,
    or your file sizes are smaller than or equal to your HDFS block size, you can
    ignore this drawback. |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
  zh: '| Snappy | Snappy ([http://code.google.com/p/hadoop-snappy/](http://code.google.com/p/hadoop-snappy/))
    是 Hadoop 编解码器选项中的最新成员。它是谷歌的开源压缩算法。谷歌在 MapReduce 和 BigTable 中使用它进行数据压缩。[a](#ch04fn15)
    Snappy 的主要缺点是它不可分割。如果你正在处理支持分割的文件格式，如 Avro 或 Parquet，或者你的文件大小小于或等于你的 HDFS 块大小，你可以忽略这个缺点。|'
- en: '^a BigTable is Google’s proprietary database system; see Fay Chang et al.,
    “Bigtable: A Distributed Storage System for Structured Data,” [http://research.google.com/archive/bigtable.html](http://research.google.com/archive/bigtable.html).'
  id: totrans-1002
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a BigTable 是谷歌的专有数据库系统；参见 Fay Chang 等人，“Bigtable：一种用于结构化数据的分布式存储系统”，[http://research.google.com/archive/bigtable.html](http://research.google.com/archive/bigtable.html)。
- en: 'To properly evaluate the codecs, you first need to specify your evaluation
    criteria, which should be based on functional and performance traits. For compression,
    your criteria are likely to include the following:'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确评估编解码器，你首先需要指定你的评估标准，这些标准应该基于功能和性能特性。对于压缩，你的标准可能包括以下内容：
- en: '***Space/time trade-off*** —Generally, the more computationally expensive compression
    codecs yield better compression ratios, resulting in smaller compressed outputs.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***空间/时间权衡*** —一般来说，计算成本更高的压缩编解码器会产生更好的压缩比率，从而产生更小的压缩输出。'
- en: '***Splittability*** —Can a compressed file be split for use by multiple mappers?
    If a compressed file can’t be split, only a single mapper will be able to work
    on it. If that file spans multiple blocks, you’ll lose out on data locality because
    the map will likely have to read blocks from remote DataNodes, incurring the overhead
    of network I/O.'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可分割性*** —一个压缩文件能否被分割以供多个映射器使用？如果一个压缩文件不能被分割，那么只有一个映射器能够处理它。如果该文件跨越多个块，你将失去数据局部性，因为映射器可能不得不从远程数据节点读取块，从而产生网络
    I/O 的开销。'
- en: '***Native compression support*** —Is there a native library that performs compression
    and decompression? This will usually outperform a compression codec written in
    Java with no underlying native library support.'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***本地压缩支持*** —是否存在执行压缩和解压缩的本地库？这通常会比没有底层本地库支持的用 Java 编写的压缩编解码器表现更好。'
- en: '[Table 4.2](#ch04table02) compares the compression codecs currently available
    (we’ll cover the space/time comparison in the next section).'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4.2](#ch04table02) 比较了目前可用的压缩编解码器（我们将在下一节中介绍空间/时间比较）。'
- en: Table 4.2\. Comparison of compression codecs
  id: totrans-1008
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.2\. 压缩编解码器比较
- en: '| Codec | Extension | Licensing | Splittable | Java-only compression support
    | Native compression support |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
  zh: '| Codec | 扩展名 | 许可证 | 可分割 | 仅 Java 压缩支持 | 原生压缩支持 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Deflate | .deflate | zlib | No | Yes | Yes |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '| Deflate | .deflate | zlib | 否 | 是 | 是 |'
- en: '| gzip | .gz | GNU GPL | No | Yes | Yes |'
  id: totrans-1012
  prefs: []
  type: TYPE_TB
  zh: '| gzip | .gz | GNU GPL | 否 | 是 | 是 |'
- en: '| bzip2 | .gz | BSD | Yes ^([[a](#ch04fn16)]) | Yes | Yes^([[b](#ch04fn17)])
    |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '| bzip2 | .gz | BSD | 是 ^([[a](#ch04fn16)]) | 是 | 是^([[b](#ch04fn17)]) |'
- en: '| LZO | .lzo_deflate | GNU GPL | No | No | Yes |'
  id: totrans-1014
  prefs: []
  type: TYPE_TB
  zh: '| LZO | .lzo_deflate | GNU GPL | 否 | 否 | 是 |'
- en: '| LZOP | .lzo | GNU GPL | Yes ^([[c](#ch04fn18)]) | No | Yes |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '| LZOP | .lzo | GNU GPL | 是 ^([[c](#ch04fn18)]) | 否 | 是 |'
- en: '| LZ4 | .lz4 | New BSD | No | No | Yes |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
  zh: '| LZ4 | .lz4 | 新 BSD | 否 | 否 | 是 |'
- en: '| Snappy | .gz | New BSD | No | No | Yes |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '| Snappy | .gz | 新 BSD | 否 | 否 | 是 |'
- en: ^a The Java version of bzip2 is splittable in Hadoop 2 and 1.1.0 onward (see
    [https://issues.apache.org/jira/browse/HADOOP-4012](https://issues.apache.org/jira/browse/HADOOP-4012)).
    The native version isn’t currently splittable.
  id: totrans-1018
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a Hadoop 2 及其 1.1.0 版本及以后的 Java 版本 bzip2 支持分割（参见 [https://issues.apache.org/jira/browse/HADOOP-4012](https://issues.apache.org/jira/browse/HADOOP-4012)）。原生版本目前不支持分割。
- en: ^b Native bzip2 support was added in Hadoop 2.1 (see [https://issues.apache.org/jira/browse/HADOOP-8462](https://issues.apache.org/jira/browse/HADOOP-8462)).
  id: totrans-1019
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^b 原生 bzip2 支持是在 Hadoop 2.1 中添加的（参见 [https://issues.apache.org/jira/browse/HADOOP-8462](https://issues.apache.org/jira/browse/HADOOP-8462)）。
- en: ^c LZOP files aren’t natively splittable. You need to preprocess them to create
    an index file, which is then used by their respective CompressionCodec implementations
    to determine the file splits. We’ll cover how you can achieve this in technique
    32.
  id: totrans-1020
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^c LZOP 文件本身不支持分割。您需要预处理它们以创建索引文件，然后由相应的 CompressionCodec 实现使用该索引文件来确定文件分割。我们将在技术
    32 中介绍如何实现这一点。
- en: '|  |'
  id: totrans-1021
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Native vs. Java bzip2
  id: totrans-1022
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 原生与 Java bzip2
- en: Native support for bzip2 was recently added to Hadoop (starting from versions
    2.0 and 1.1.0). Native bzip2 support is the default, but it doesn’t support splittability.
    If you need splittability with bzip2, you’ll need to enable the Java bzip2, which
    can be specified by setting `io.compression.codec.bzip2.library` to `java-builtin`.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 最近添加了对 bzip2 的原生支持（从 2.0 版本和 1.1.0 版本开始）。原生 bzip2 支持是默认的，但它不支持分割性。如果您需要与
    bzip2 一起使用分割性，您需要启用 Java bzip2，可以通过设置 `io.compression.codec.bzip2.library` 为 `java-builtin`
    来指定。
- en: '|  |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Now that you understand the codecs, how do they square up when looking at their
    space/time trade-offs? I used a 100 MB (10^8) Wikipedia XML file (enwik8.zip from
    [http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)),
    to compare the codec run times and their compression sizes. The results of these
    tests can be seen in [table 4.3](#ch04table03).
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了编解码器，它们在空间/时间权衡方面表现如何？我使用了一个 100 MB（10^8）的维基百科 XML 文件（来自 [http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)
    的 enwik8.zip），来比较编解码器的运行时间和它们的压缩大小。这些测试的结果可以在 [表 4.3](#ch04table03) 中看到。
- en: Table 4.3\. Performance comparison of compression codecs on a 100 MB text file
  id: totrans-1026
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.3\. 100 MB 文本文件上压缩编解码器的性能比较
- en: '| Codec | Compression time (secs) | Decompression time (secs) | Compressed
    file size | Compressed percentage |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
  zh: '| Codec | 压缩时间（秒） | 解压缩时间（秒） | 压缩文件大小 | 压缩百分比 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Deflate | 9.21 | 1.02 | 36,548,921 | 36.55% |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
  zh: '| Deflate | 9.21 | 1.02 | 36,548,921 | 36.55% |'
- en: '| gzip | 9.09 | 0.90 | 36,548,933 | 36.55% |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '| gzip | 9.09 | 0.90 | 36,548,933 | 36.55% |'
- en: '| bzip2 (Java) | 47.33 | 6.45 | 29,007,274 | 29.01% |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '| bzip2 (Java) | 47.33 | 6.45 | 29,007,274 | 29.01% |'
- en: '| bzip2 (native) | 11.59 | 4.66 | 29,008,758 | 29.01% |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '| bzip2 (native) | 11.59 | 4.66 | 29,008,758 | 29.01% |'
- en: '| LZO | 2.10 | 0.46 | 53,926,001 | 53.93% |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '| LZO | 2.10 | 0.46 | 53,926,001 | 53.93% |'
- en: '| LZOP | 2.09 | 0.45 | 53,926,043 | 53.93% |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| LZOP | 2.09 | 0.45 | 53,926,043 | 53.93% |'
- en: '| LZ4 | 1.72 | 0.28 | 57,337,587 | 57.34% |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '| LZ4 | 1.72 | 0.28 | 57,337,587 | 57.34% |'
- en: '| Snappy | 1.75 | 0.29 | 58,493,673 | 58.49% |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '| Snappy | 1.75 | 0.29 | 58,493,673 | 58.49% |'
- en: '|  |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Running your own tests
  id: totrans-1038
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行自己的测试
- en: When you’re performing your own evaluation, I recommend you perform your tests
    using your own data, and preferably on hosts similar to your production nodes.
    This way you’ll have a good sense of the expected compression and run times for
    the codecs.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 当您进行自己的评估时，我建议您使用自己的数据，并在尽可能与您的生产节点相似的主机上运行测试。这样，您将能够很好地了解编解码器的预期压缩和运行时间。
- en: 'Also make sure your cluster has native codecs enabled. You can check this by
    running the following command:'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的集群已启用原生编解码器。您可以通过运行以下命令来检查：
- en: '[PRE70]'
  id: totrans-1041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '|  |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[Figure 4.4](#ch04fig04) shows the compressed sizes in bar graph form.'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.4](#ch04fig04) 以条形图形式显示了压缩大小。'
- en: Figure 4.4\. Compressed file sizes for a single 100 MB text file (smaller values
    are better)
  id: totrans-1044
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.4\. 单个 100 MB 文本文件的压缩文件大小（值越小越好）
- en: '![](04fig04_alt.jpg)'
  id: totrans-1045
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig04_alt.jpg)'
- en: '[Figure 4.5](#ch04fig05) shows the compressed times in bar graph form. These
    times will vary significantly based on hardware, and they’re only supplied here
    to give a sense of how they relate to each other.'
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.5](#ch04fig05) 以条形图形式显示了压缩时间。这些时间将根据硬件的不同而有很大差异，这里只提供这些数据，以给出它们之间相互关系的概念。'
- en: Figure 4.5\. Compression and decompression times for a single 100 MB text file
    (smaller values are better)
  id: totrans-1047
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.5\. 单个 100 MB 文本文件的压缩和解压缩时间（值越小越好）
- en: '![](04fig05_alt.jpg)'
  id: totrans-1048
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig05_alt.jpg)'
- en: What do the space and time results tell you? If squeezing as much data into
    your cluster as possible is your top priority and you can live with long compression
    times, then bzip2 may be the right codec for you. If you want to compress your
    data but introduce the least amount of CPU overhead when it comes to reading and
    writing compressed files, you should look at LZ4. Anyone looking for a balance
    between compression and execution times would have to eliminate the Java version
    of bzip2 from the picture.
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: 空间和时间结果告诉你什么？如果你将尽可能多地挤压数据到你的集群作为首要任务，并且你可以忍受较长的压缩时间，那么 bzip2 可能是你正确的编解码器。如果你想压缩你的数据，但在读取和写入压缩文件时引入最少的
    CPU 开销，你应该看看 LZ4。任何寻求压缩和执行时间之间平衡的人都必须从图中排除 Java 版本的 bzip2。
- en: Being able to split your compressed files is important, and here you have to
    choose between bzip2 and LZOP. The native bzip2 codec doesn’t support splitting,
    and the Java bzip2 times will likely give most people pause. The only advantage
    of bzip2 over LZOP is that its Hadoop integration is much easier to work with
    than LZOP’s. Although LZOP is the natural winner here, it requires some effort
    to work with, as you’ll see in technique 32.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 能够分割你的压缩文件很重要，在这里你必须在 bzip2 和 LZOP 之间做出选择。原生的 bzip2 编解码器不支持分割，而 Java bzip2 的时间可能会让大多数人犹豫。bzip2
    比 LZOP 的唯一优势是它的 Hadoop 集成比 LZOP 更容易处理。尽管 LZOP 在这里自然是赢家，但它需要一些努力才能与之合作，正如你在技术 32
    中将看到的。
- en: Summary
  id: totrans-1051
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The best codec for you will depend on your criteria. LZ4 is the most promising
    codec if you don’t care about splitting your files, and LZOP is what you should
    be looking at if you want splittable files.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 适合你的最佳编解码器将取决于你的标准。如果你不介意分割你的文件，LZ4 是最有希望的编解码器；如果你想得到可分割的文件，你应该看看 LZOP。
- en: Another factor to consider is the long-term storage retention required for the
    data. If you’re keeping data for a long time, you’ll probably want to maximize
    the compression of your files, for which I would recommend a zlib-based codec
    (such as gzip). Because gzip isn’t splittable, though, it would be prudent to
    use it in combination with a block-based file format such as Avro or Parquet so
    that your data can still be split. Or you could size your outputs so they occupy
    a single block in HDFS so that splittability isn’t a concern.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是数据长期存储保留所需的时间。如果你要保留数据很长时间，你可能希望最大化文件的压缩，对于这一点，我会推荐基于 zlib 的编解码器（如
    gzip）。由于 gzip 不可分割，因此明智的做法是将其与基于块的文件格式（如 Avro 或 Parquet）结合使用，以便你的数据仍然可以分割。或者，你可以调整你的输出大小，使它们在
    HDFS 中占用单个块，这样可分割性就不再是问题。
- en: Bear in mind that compressed sizes will vary based on whether your file is text
    or binary and depending on its contents. To get accurate numbers, you should run
    similar tests against your own data.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，压缩大小将根据你的文件是文本还是二进制以及其内容而有所不同。为了获得准确的数据，你应该对你的数据进行类似的测试。
- en: Compressing data in HDFS has many benefits, including reduced file sizes and
    faster MapReduce job runtimes. A number of compression codecs are available for
    use in Hadoop, and I evaluated them based on features and performance. Now you’re
    ready to start using compression. Let’s look at how you can compress files and
    use them with tools such as MapReduce, Pig, and Hive.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HDFS 中压缩数据有许多好处，包括减少文件大小和加快 MapReduce 作业的运行时间。Hadoop 中有多个编解码器可供使用，我根据功能和性能对它们进行了评估。现在你准备好开始使用压缩了。让我们看看你如何压缩文件以及如何使用
    MapReduce、Pig 和 Hive 等工具使用它们。
- en: Technique 31 Compression with HDFS, MapReduce, Pig, and Hive
  id: totrans-1056
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇 31 使用 HDFS、MapReduce、Pig 和 Hive 进行压缩
- en: Because HDFS doesn’t provide built-in support for compression, it can be a challenge
    to work with compression in Hadoop. The onus falls on you to figure out how to
    work with compressed files. Also, splittable compression isn’t for the faint of
    heart, because it doesn’t come out of the box with Hadoop.^([[15](#ch04fn19)])
    If you’re dealing with medium-size files that compress down to near-HDFS block
    size, this technique will be the quickest and simplest way to reap the benefits
    of compression in Hadoop.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HDFS没有提供内置的压缩支持，因此在Hadoop中使用压缩可能是一个挑战。责任在于你自己找出如何处理压缩文件。此外，可分割的压缩对于新手来说可能有些困难，因为它不是Hadoop的默认功能。[15](#ch04fn19)
    如果你处理的是压缩后接近HDFS块大小的中等大小文件，这种技术将是获得Hadoop压缩优势最快、最简单的方法。
- en: ^(15) Technically, you can get out-of-the-box splittable compression with bzip2,
    but its performance traits, as shown earlier in this section, rule it out as a
    serious compression codec.
  id: totrans-1058
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(15)] 技术上，你可以直接使用bzip2获得可分割的压缩，但如本节前面所示，其性能特性使其不适合作为主要的压缩编解码器。
- en: Problem
  id: totrans-1059
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to read and write compressed files in HDFS and also use them with Map-Reduce,
    Pig, and Hive.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在HDFS中读取和写入压缩文件，并使用Map-Reduce、Pig和Hive。
- en: Solution
  id: totrans-1061
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Working with compressed files in MapReduce involves updating the MapReduce configuration
    file mapred-site.xml and registering the compression codec you’re using. After
    you do this, working with compressed input files in MapReduce requires no additional
    steps, and producing compressed MapReduce output is a matter of setting the `mapred.output.compress`
    and `mapred.output.compression.codec` MapReduce properties.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中处理压缩文件需要更新MapReduce配置文件mapred-site.xml并注册你使用的压缩编解码器。完成此操作后，在MapReduce中处理压缩输入文件无需额外步骤，生成压缩MapReduce输出只需设置`mapred.output.compress`和`mapred.output.compression.codec`
    MapReduce属性。
- en: Discussion
  id: totrans-1063
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The first step is to figure out how to read and write files using any of the
    codecs evaluated earlier in this chapter. All of the codecs detailed in this chapter
    are bundled with Hadoop except for LZO/LZOP and Snappy, so if you want to work
    with those three, you’ll need to download and build them yourself (I’ll walk you
    through how to work with LZO/LZOP later in this section).
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是确定如何使用本章前面评估的任何编解码器读取和写入文件。本章详细介绍的编解码器都包含在Hadoop中，除了LZO/LZOP和Snappy，所以如果你想使用这三个，你需要自己下载和构建它们（我将在本节后面带你了解如何使用LZO/LZOP）。
- en: To use the compression codecs, you need to know their class names, which are
    listed in [table 4.4](#ch04table04).
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用压缩编解码器，你需要知道它们的类名，这些类名在[表4.4](#ch04table04)中列出。
- en: Table 4.4\. Codec classes
  id: totrans-1066
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.4\. 编解码器类
- en: '| Codec | Class | Default extension |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
  zh: '| 编解码器 | 类 | 默认扩展名 |'
- en: '| --- | --- | --- |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Deflate | org.apache.hadoop.io.compress.DeflateCodec | deflate |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
  zh: '| Deflate | org.apache.hadoop.io.compress.DeflateCodec | deflate |'
- en: '| gzip | org.apache.hadoop.io.compress.GzipCodec | gz |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '| gzip | org.apache.hadoop.io.compress.GzipCodec | gz |'
- en: '| bzip2 | org.apache.hadoop.io.compress.BZip2Codec | bz2 |'
  id: totrans-1071
  prefs: []
  type: TYPE_TB
  zh: '| bzip2 | org.apache.hadoop.io.compress.BZip2Codec | bz2 |'
- en: '| LZO | com.hadoop.compression.lzo.LzoCodec | lzo_deflate |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '| LZO | com.hadoop.compression.lzo.LzoCodec | lzo_deflate |'
- en: '| LZOP | com.hadoop.compression.lzo.LzopCodec | lzo |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '| LZOP | com.hadoop.compression.lzo.LzopCodec | lzo |'
- en: '| LZ4 | org.apache.hadoop.io.compress.Lz4Codec | lz4 |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '| LZ4 | org.apache.hadoop.io.compress.Lz4Codec | lz4 |'
- en: '| Snappy | org.apache.hadoop.io.compress.SnappyCodec | snappy |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
  zh: '| Snappy | org.apache.hadoop.io.compress.SnappyCodec | snappy |'
- en: Using compression in HDFS
  id: totrans-1076
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在HDFS中使用压缩
- en: How would you compress an existing file in HDFS using any one of the codecs
    mentioned in the previous table? The following code supports doing that:^([[16](#ch04fn20)])
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 你将如何使用前面表中提到的任何编解码器压缩HDFS中的现有文件？以下代码支持这样做：^[(16)]
- en: '^(16) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileWrite.java).'
  id: totrans-1078
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(16)] GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileWrite.java)。
- en: '![](164fig01_alt.jpg)'
  id: totrans-1079
  prefs: []
  type: TYPE_IMG
  zh: '![164fig01_alt.jpg]'
- en: '|  |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Codec caching
  id: totrans-1081
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编解码器缓存
- en: One of the overheads to using compression codecs is that they can be expensive
    to create. When you use the Hadoop `ReflectionUtils` class, some of the reflection
    overhead associated with creating the instance will be cached in `ReflectionUtils`,
    which should speed up subsequent creation of the codec. A better option would
    be to use the `CompressionCodecFactory`, which provides caching of the codecs
    themselves.
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 使用压缩编解码器的一个开销是它们可能很昂贵来创建。当你使用Hadoop的`ReflectionUtils`类时，与创建实例相关的部分反射开销将被缓存到`ReflectionUtils`中，这应该会加快后续编解码器的创建。更好的选择是使用`CompressionCodecFactory`，它提供了编解码器的缓存。
- en: '|  |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Reading this compressed file is as simple as writing it:^([[17](#ch04fn21)])
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 读取这个压缩文件就像写入它一样简单：^[[17](#ch04fn21)])
- en: '^(17) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileRead.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileRead.java).'
  id: totrans-1085
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[17] GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileRead.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedFileRead.java).
- en: '![](165fig01_alt.jpg)'
  id: totrans-1086
  prefs: []
  type: TYPE_IMG
  zh: '![](165fig01_alt.jpg)'
- en: Super simple. Now that you can create compressed files, let’s look at how you
    can work with them in MapReduce.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单。现在你能够创建压缩文件了，让我们看看如何在MapReduce中处理它们。
- en: Using compression in MapReduce
  id: totrans-1088
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在MapReduce中使用压缩
- en: To work with compressed files in MapReduce, you need to set some configuration
    options for your job. For the sake of brevity, let’s assume identity mappers and
    reducers^([[18](#ch04fn22)]) in this example:^([[19](#ch04fn23)])
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: 要在MapReduce中处理压缩文件，你需要为你的作业设置一些配置选项。为了简洁起见，让我们假设在这个例子中使用了身份映射器和reducer^[[18](#ch04fn22)])：^[[19](#ch04fn23)]
- en: ^(18) An identity task is one that emits all of the input it receives as output,
    without any transformation or filtering.
  id: totrans-1090
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[18] 一个身份任务是指它将接收到的所有输入作为输出发射，没有任何转换或过滤。
- en: '^(19) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedMapReduce.java).'
  id: totrans-1091
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[19] GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/CompressedMapReduce.java).
- en: '![](165fig02_alt.jpg)'
  id: totrans-1092
  prefs: []
  type: TYPE_IMG
  zh: '![](165fig02_alt.jpg)'
- en: The only differences between a MapReduce job that works with uncompressed versus
    compressed I/O are the three annotated lines in the previous example.
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 与未压缩的I/O相比，MapReduce作业的唯一区别是前一个示例中的三个注解行。
- en: Not only can a job’s input and output be compressed, but so can the intermediate
    map output, because it’s spilled first to disk, and then eventually over the network
    to the reducer. The effectiveness of compressing the map output will ultimately
    depend on the type of data being emitted, but as a general rule, you should see
    some job speed-up by making this change.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅一个工作的输入和输出可以被压缩，中间映射输出也可以，因为它首先被写入磁盘，然后最终通过网络发送到reducer。压缩映射输出的有效性最终将取决于被发射的数据类型，但作为一个一般规则，你应该看到通过这个改变而使工作速度有所提升。
- en: Why didn’t you have to specify the compression codec for the input file in the
    preceding code? By default the `FileInputFormat` class uses the `CompressionCodecFactory`
    to determine if the input file extension matches any of the registered codecs.
    If it finds a codec that’s associated with that file extension, it automatically
    uses that codec to decompress the input files.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在前面的代码中你不需要指定输入文件的压缩编解码器？默认情况下，`FileInputFormat`类使用`CompressionCodecFactory`来确定输入文件扩展名是否与已注册的编解码器匹配。如果它找到一个与该文件扩展名关联的编解码器，它将自动使用该编解码器来解压缩输入文件。
- en: 'How does MapReduce know which codecs to use? You need to specify the codecs
    in mapred-site.xml. The following code shows how to register all of the codecs
    we’ve evaluated. Remember that other than gzip, Deflate, and bzip2, all compression
    codecs need to be built and made available on your cluster before you can register
    them:'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是如何知道使用哪些编解码器的？你需要指定mapred-site.xml中的编解码器。以下代码显示了如何注册我们评估的所有编解码器。记住，除了gzip、Deflate和bzip2之外，所有压缩编解码器都需要在你可以注册它们之前在你的集群上构建并使其可用：
- en: '[PRE71]'
  id: totrans-1097
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Now that you’ve mastered compression with MapReduce, it’s time to look higher
    up the Hadoop stack. Because compression can also be used in conjunction with
    Pig and Hive, let’s see how you can mirror your MapReduce compression accomplishment
    using Pig and Hive. (As I’ll show in [chapter 9](kindle_split_022.html#ch09),
    Hive is a higher-level language that abstracts away some of the complex details
    of MapReduce.)
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经掌握了MapReduce中的压缩，是时候查看Hadoop堆栈中的更高层次了。因为压缩也可以与Pig和Hive一起使用，让我们看看您如何使用Pig和Hive来复制您的MapReduce压缩成就。（正如我将在[第9章](kindle_split_022.html#ch09)中展示的，Hive是一种高级语言，它抽象了一些MapReduce的复杂细节。）
- en: Using compression in Pig
  id: totrans-1099
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在Pig中使用压缩
- en: 'If you’re working with Pig, there’s no extra effort required to use compressed
    input files. All you need to do is ensure your filename extension maps to the
    appropriate compression codec (see [table 4.4](#ch04table04)). The following example
    gzips a local password file, loads it into Pig, and dumps out the usernames:'
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Pig，则无需额外努力即可使用压缩输入文件。您需要确保文件扩展名映射到适当的压缩编解码器（见[表4.4](#ch04table04)）。以下示例将本地密码文件gzip压缩，然后将其加载到Pig中，并导出用户名：
- en: '![](166fig01_alt.jpg)'
  id: totrans-1101
  prefs: []
  type: TYPE_IMG
  zh: '![166fig01_alt.jpg](166fig01_alt.jpg)'
- en: 'Writing out a gzipped file is the same—make sure you specify the extension
    for a compression codec. The following example stores the results of Pig relation
    `B` in a file in HDFS, and then copies them to the local filesystem to examine
    the contents:'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 将gzip文件写入的过程相同——确保指定压缩编解码器的扩展名。以下示例将Pig关系`B`的结果存储在HDFS中的一个文件中，然后将它们复制到本地文件系统以检查内容：
- en: '[PRE72]'
  id: totrans-1103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: That was straightforward—let’s hope things are equally smooth with Hive.
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单——让我们希望事情在Hive中同样顺利。
- en: Using compression in Hive
  id: totrans-1105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在Hive中使用压缩
- en: 'As with Pig, all you need to do is specify the codec extension when defining
    the filename:'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: 与Pig类似，您只需在定义文件名时指定编解码器扩展名：
- en: '![](167fig01_alt.jpg)'
  id: totrans-1107
  prefs: []
  type: TYPE_IMG
  zh: '![167fig01_alt.jpg](167fig01_alt.jpg)'
- en: The previous example loaded a gzipped file into Hive. In this situation, Hive
    moves the file being loaded into Hive’s warehouse directory and continues to use
    the raw file as its storage for the table.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的示例将一个gzip压缩的文件加载到Hive中。在这种情况下，Hive将正在加载的文件移动到Hive的仓库目录中，并继续使用原始文件作为表的存储。
- en: 'What if you want to create another table and also specify that it should be
    compressed? The following example achieves this by setting some Hive configs to
    enable MapReduce compression (because a MapReduce job will be executed to load
    the new table in the last statement):'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想创建另一个表并指定它应该被压缩，以下示例通过设置一些Hive配置来启用MapReduce压缩（因为最后一条语句将执行MapReduce作业来加载新表）来实现这一点：
- en: '[PRE73]'
  id: totrans-1110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'You can verify that Hive is indeed compressing the storage for the new apachelog_backup
    table by looking at it in HDFS:'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在HDFS中查看它来验证Hive确实正在压缩新创建的apachelog_backup表的存储：
- en: '[PRE74]'
  id: totrans-1112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: It should be noted that Hive recommends using SequenceFile as the output format
    for tables because SequenceFile blocks can be individually compressed.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，Hive建议使用SequenceFile作为表的输出格式，因为SequenceFile块可以单独压缩。
- en: Summary
  id: totrans-1114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique provides a quick and easy way to get compression running in Hadoop.
    It works well for files that aren’t too large because it offers a fairly transparent
    way of working with compression.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术提供了一种快速简单的方法来在Hadoop中启用压缩。它适用于不太大的文件，因为它提供了一种相当透明的方式来处理压缩。
- en: If your compressed file sizes are much larger than the HDFS block size, read
    on for compression techniques that can split your files.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的压缩文件大小远大于HDFS块大小，请继续阅读，了解可以分割文件的压缩技术。
- en: Technique 32 Splittable LZOP with MapReduce, Hive, and Pig
  id: totrans-1117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧32：使用MapReduce、Hive和Pig的Splittable LZOP
- en: Imagine that you’re working with large text files that, even when compressed,
    are many times larger than the HDFS block size. To avoid having one map task process
    an entire large compressed file, you’ll need to pick a compression codec that
    can support splitting that file.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在处理大型文本文件，即使压缩后，其大小也远大于HDFS块大小。为了避免一个map任务处理整个大型压缩文件，您需要选择一个可以支持分割该文件的压缩编解码器。
- en: LZOP fits the bill, but working with it is more complex than the examples detailed
    in the previous technique because LZOP is not in and of itself splittable. “Wait,”
    you may be thinking, “didn’t you state earlier that LZOP is splittable?” LZOP
    is block-based, but you can’t perform a random seek into an LZOP file and determine
    the next block’s starting point. This is the challenge we’ll tackle in this technique.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: LZOP符合要求，但与之前技术示例中的操作相比，使用LZOP更为复杂，因为LZOP本身并不是可分割的。“等等，”你可能正在想，“你之前不是说过LZOP是可分割的吗？”LZOP是基于块的，但你不能在LZOP文件中随机查找并确定下一个块的起始点。这正是我们将在这个技术中解决的问题。
- en: Problem
  id: totrans-1120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use a compression codec that will allow MapReduce to work in parallel
    on a single compressed file.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望使用一种压缩编解码器，使得MapReduce可以在单个压缩文件上并行工作。
- en: Solution
  id: totrans-1122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: In MapReduce, splitting large LZOP-compressed input files requires the use of
    LZOP-specific input format classes, such as `LzoInputFormat`. The same principle
    applies when working with LZOP-compressed input files in both Pig and Hive.
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中，将大型的LZOP压缩输入文件分割需要使用LZOP特定的输入格式类，例如`LzoInputFormat`。在Pig和Hive中处理LZOP压缩输入文件时，同样适用这个原则。
- en: Discussion
  id: totrans-1124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The LZOP compression codec is one of only two codecs that allow for compressed
    files to be split, and therefore to be worked on in parallel by multiple reducers.
    The other codec, bzip2, suffers from compression times that are so slow they arguably
    render the codec unusable. LZOP also offers a good compromise between compression
    and speed.
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: LZOP压缩编解码器是仅有的两个允许分割压缩文件并因此允许多个reducer并行工作的编解码器之一。另一个编解码器bzip2，其压缩时间非常慢，以至于可以说该编解码器不可用。LZOP在压缩和速度之间提供了一个良好的折衷方案。
- en: '|  |'
  id: totrans-1126
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: What’s the difference between LZO and LZOP?
  id: totrans-1127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LZO和LZOP之间的区别是什么？
- en: Both LZO and LZOP codecs are supplied for use with Hadoop. LZO is a stream-based
    compression store that doesn’t have the notion of blocks or headers. LZOP has
    the notion of blocks (that are checksummed), and therefore is the codec you want
    to use, especially if you want your compressed output to be splittable. Confusingly,
    the Hadoop codecs by default treat files ending with the .lzo extension to be
    LZOP-encoded, and files ending with the .lzo_deflate extension to be LZO-encoded.
    Also, much of the documentation seems to use LZO and LZOP interchangeably.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: LZO和LZOP编解码器都可用于与Hadoop一起使用。LZO是一种基于流的压缩存储，它没有块或头部的概念。LZOP有块的概念（这些块是经过校验和的），因此是你要使用的编解码器，尤其是如果你想使压缩输出可分割时。令人困惑的是，Hadoop编解码器默认将文件扩展名为.lzo的文件视为LZOP编码，将文件扩展名为.lzo_deflate的文件视为LZO编码。此外，大部分文档似乎都将LZO和LZOP互换使用。
- en: '|  |'
  id: totrans-1129
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Preparing your cluster for LZOP
  id: totrans-1130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为LZOP准备你的集群
- en: Unfortunately, Hadoop doesn’t bundle LZOP for licensing reasons.^([[20](#ch04fn24)])
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于许可原因，Hadoop没有捆绑LZOP。[^([20](#ch04fn24))]
- en: ^(20) LZOP used to be included with Hadoop, but with the work performed in JIRA
    ticket [https://issues.apache.org/jira/browse/HADOOP-4874](https://issues.apache.org/jira/browse/HADOOP-4874),
    it was removed from Hadoop version 0.20 and newer releases due to LZOP’s GPL licensing
    limiting its redistribution.
  id: totrans-1132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(20)](https://issues.apache.org/jira/browse/HADOOP-4874) LZOP曾经包含在Hadoop中，但由于LZOP的GPL许可限制了其再分发，它在Hadoop
    0.20版本及更高版本中被移除。
- en: Getting all the prerequisites compiled and installed on your cluster is laborious,
    but rest assured that there are detailed instructions in the appendix. To compile
    and run the code in this section, you’ll need to follow the instructions in the
    appendix.
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的集群上编译和安装所有先决条件是一项繁重的工作，但请放心，附录中有详细的说明。要编译和运行本节中的代码，你需要遵循附录中的说明。
- en: Reading and writing LZOP files in HDFS
  id: totrans-1134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在HDFS中读取和写入LZOP文件
- en: We covered how to read and write compressed files in [section 4.2](#ch04lev1sec2).
    To perform the same activity with LZOP requires you to specify the LZOP codec
    in your code. This code is shown in the following listing.^([[21](#ch04fn25)])
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4.2节](#ch04lev1sec2)中介绍了如何读取和写入压缩文件。要使用LZOP执行相同的活动，你需要指定代码中的LZOP编解码器。以下列表显示了此代码。[^([21](#ch04fn25))]
- en: '^(21) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopFileReadWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopFileReadWrite.java).'
  id: totrans-1136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(21)](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopFileReadWrite.java)
    GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopFileReadWrite.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopFileReadWrite.java)。
- en: Listing 4.3\. Methods to read and write LZOP files in HDFS
  id: totrans-1137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.3\. 在HDFS中读取和写入LZOP文件的方法
- en: '[PRE75]'
  id: totrans-1138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Let’s write and read an LZOP file, and then make sure that LZOP utilities can
    work with the generated file (replace `$HADOOP_CONF_HOME` with the location of
    your Hadoop config directory):'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写一个 LZOP 文件并读取它，然后确保 LZOP 工具可以与生成的文件一起工作（将 `$HADOOP_CONF_HOME` 替换为你 Hadoop
    配置目录的位置）：
- en: '[PRE76]'
  id: totrans-1140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The preceding code will generate a core-site.xml.lzo file in HDFS.
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将在 HDFS 中生成一个 core-site.xml.lzo 文件。
- en: 'Now make sure you can use this LZOP file with the `lzop` binary. Install an
    `lzop` binary on your host.^([[22](#ch04fn26)]) Copy the LZOP file from HDFS to
    local disk, uncompress it with the native `lzop` binary, and compare it with the
    original file:'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在确保你可以使用这个 LZOP 文件与 `lzop` 二进制文件一起使用。在你的主机上安装一个 `lzop` 二进制文件。[^([22](#ch04fn26))]
    将 LZOP 文件从 HDFS 复制到本地磁盘，使用本地的 `lzop` 二进制文件解压缩，并与原始文件进行比较：
- en: ^(22) For RedHat and Centos, you can install the rpm from [http://pkgs.repoforge.org/lzop/lzop-1.03-1.el5.rf.x86_64.rpm](http://pkgs.repoforge.org/lzop/lzop-1.03-1.el5.rf.x86_64.rpm).
  id: totrans-1143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[^([22])] 对于 RedHat 和 Centos，你可以从 [http://pkgs.repoforge.org/lzop/lzop-1.03-1.el5.rf.x86_64.rpm](http://pkgs.repoforge.org/lzop/lzop-1.03-1.el5.rf.x86_64.rpm)
    安装 rpm。'
- en: '[PRE77]'
  id: totrans-1144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The `diff` verified that the file compressed with the LZOP codec could be decompressed
    with the `lzop` binary.
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: '`diff` 命令验证了使用 LZOP 编解码器压缩的文件可以用 `lzop` 二进制文件解压缩。'
- en: Now that you have your LZOP file, you need to index it so that it can be split.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了你的 LZOP 文件，你需要对其进行索引以便它可以被分割。
- en: Creating indexes for your LZOP files
  id: totrans-1147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为你的 LZOP 文件创建索引
- en: Earlier I made the paradoxical statement that LZOP files can be split, but that
    they’re not natively splittable. Let me clarify what that means—the lack of block-delimiting
    synchronization markers means you can’t do a random seek into an LZOP file and
    start reading blocks. But because internally it does use blocks, all you need
    is a preprocessing step that can generate an index file containing the block offsets.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我做出了一个矛盾的陈述，即 LZOP 文件可以被分割，但它们不是原生可分割的。让我澄清一下这意味着什么——缺乏块定界同步标记意味着你不能在 LZOP
    文件中进行随机查找并开始读取块。但是因为内部确实使用了块，你需要的只是一个预处理步骤，它可以生成包含块偏移量的索引文件。
- en: The LZOP file is read in its entirety, and block offsets are written to the
    index file as the read is occurring. The index file format, shown in [figure 4.6](#ch04fig06),
    is a binary file containing a consecutive series of 64-bit numbers that indicate
    the byte offset for each block in the LZOP file.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: LZOP 文件被完整读取，并且随着读取的进行，块偏移量被写入索引文件。索引文件格式，如 [图 4.6](#ch04fig06) 所示，是一个包含一系列连续
    64 位数字的二进制文件，这些数字表示 LZOP 文件中每个块的字节偏移量。
- en: Figure 4.6\. An LZOP index file is a binary containing a consecutive series
    of 64-bit numbers.
  id: totrans-1150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6\. LZOP 索引文件是一个包含一系列连续 64 位数字的二进制文件。
- en: '![](04fig06.jpg)'
  id: totrans-1151
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6](04fig06.jpg)'
- en: 'You can create index files in one of two ways, as shown in the following two
    code snippets. If you want to create an index file for a single LZOP file, here
    is a simple library call that will do this for you:'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下两种方式之一创建索引文件，如以下两个代码片段所示。如果你想为单个 LZOP 文件创建索引文件，这里有一个简单的库调用，这将为你完成这项工作：
- en: '[PRE78]'
  id: totrans-1153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The following option works well if you have a large number of LZOP files and
    you want a more efficient way to generate the index files. The indexer runs a
    MapReduce job to create the index files. Both files and directories (which are
    scanned recursively for LZOP files) are supported:'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有大量 LZOP 文件并且想要一种更有效的方法来生成索引文件，以下选项将工作得很好。索引器运行一个 MapReduce 作业来创建索引文件。支持文件和目录（递归扫描以查找
    LZOP 文件）：
- en: '[PRE79]'
  id: totrans-1155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Both approaches depicted in [figure 4.6](#ch04fig06) will generate an index
    file in the same directory as the LZOP file. The index filename is the original
    LZOP filename suffixed with .index. Running the previous commands would yield
    the filename core-site.xml.lzo.index.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.6](#ch04fig06) 中展示的两种方法都会在 LZOP 文件相同的目录下生成索引文件。索引文件的名称是原始 LZOP 文件名后缀为
    .index。运行前面的命令会产生文件名为 core-site.xml.lzo.index。'
- en: 'Now let’s look at how you can use the `LzoIndexer` in your Java code. The following
    code (from the `main` method of `LzoIndexer`) will result in the index file being
    created synchronously:'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看你如何在 Java 代码中使用 `LzoIndexer`。以下代码（来自 `LzoIndexer` 的 `main` 方法）将导致索引文件同步创建：
- en: '[PRE80]'
  id: totrans-1158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: With the `DistributedLzoIndexer`, the MapReduce job will launch and run with
    *N* mappers, one for each .lzo file. No reducers are run, so the (identity) mapper,
    via the custom `LzoSplitInputFormat` and `LzoIndexOutputFormat`, writes the index
    files directly. If you want to run the MapReduce job from your own Java code,
    you can use the `DistributedLzoIndexer` code as an example.
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DistributedLzoIndexer`，MapReduce作业将以*N*个mapper启动和运行，每个mapper对应一个.lzo文件。不会运行任何reducer，因此通过自定义的`LzoSplitInputFormat`和`LzoIndexOutputFormat`，(身份)mapper直接写入索引文件。如果您想从自己的Java代码中运行MapReduce作业，您可以使用`DistributedLzoIndexer`代码作为示例。
- en: You need the LZOP index files so that you can split LZOP files in your MapReduce,
    Pig, and Hive jobs. Now that you have the aforementioned LZOP index files, let’s
    look at how you can use them with MapReduce.
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要LZOP索引文件，以便您可以在MapReduce、Pig和Hive作业中分割LZOP文件。现在您已经拥有了上述LZOP索引文件，让我们看看您如何可以使用它们与MapReduce一起使用。
- en: MapReduce and LZOP
  id: totrans-1161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce和LZOP
- en: 'After you’ve created index files for your LZOP files, it’s time to start using
    your LZOP files with MapReduce. Unfortunately, this brings us to the next challenge:
    none of the existing, built-in Hadoop-file-based input formats will work with
    splittable LZOP because they need specialized logic to handle input splits using
    the LZOP index file. You need specific input format classes to work with splittable
    LZOP.'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 在您为LZOP文件创建了索引文件之后，是时候开始使用您的LZOP文件与MapReduce一起使用了。不幸的是，这带我们来到了下一个挑战：现有的所有内置基于Hadoop文件的输入格式都无法与可分割的LZOP一起工作，因为它们需要专门的逻辑来处理使用LZOP索引文件进行输入分割。您需要特定的输入格式类来与可分割的LZOP一起工作。
- en: The LZOP library provides an `LzoTextInputFormat` implementation for line-oriented
    LZOP-compressed text files with accompanying index files.^([[23](#ch04fn27)])
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: LZOP库为具有伴随索引文件的行定向LZOP压缩文本文件提供了一个`LzoTextInputFormat`实现。^([[23](#ch04fn27)])
- en: ^(23) The LZOP input formats also work well with LZOP files that don’t have
    index files.
  id: totrans-1164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([23]) LZOP输入格式也与没有索引文件的LZOP文件很好地工作。
- en: The following code shows the steps required to configure the MapReduce job to
    work with LZOP. You would perform these steps for a MapReduce job that had text
    LZOP inputs and outputs:^([[24](#ch04fn28)])
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了配置MapReduce作业以与LZOP一起工作的步骤。您将为具有文本LZOP输入和输出的MapReduce作业执行这些步骤：^([[24](#ch04fn28)])
- en: '^(24) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopMapReduce.java).'
  id: totrans-1166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([24]) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch4/LzopMapReduce.java)。
- en: '[PRE81]'
  id: totrans-1167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Compressing intermediary map output will also speed up the overall execution
    time of your MapReduce jobs:'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩中间映射输出也将加快您MapReduce作业的整体执行时间：
- en: '[PRE82]'
  id: totrans-1169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'You can easily configure your cluster to always compress your map output by
    editing hdfs-site.xml:'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松配置您的集群始终压缩您的映射输出，通过编辑hdfs-site.xml：
- en: '[PRE83]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The number of splits per LZOP file is a function of the number of LZOP blocks
    that the file occupies, not the number of HDFS blocks that the file occupies.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 每个LZOP文件的分片数量是文件占用的LZOP块数量的函数，而不是文件占用的HDFS块数量的函数。
- en: Now that we’ve covered MapReduce, let’s look at how Pig and Hive can work with
    splittable LZOP.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了MapReduce，让我们看看Pig和Hive如何与可分割的LZOP一起工作。
- en: Pig and Hive
  id: totrans-1174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Pig和Hive
- en: Elephant Bird,^([[25](#ch04fn29)]) a Twitter project containing utilities to
    work with LZOP, provides a number of useful MapReduce and Pig classes. Elephant
    Bird has an `LzoPigStorage` class that works with text-based, LZOP-compressed
    data in Pig.
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: Elephant Bird，^([[25](#ch04fn29)))，一个包含用于与LZOP一起工作的实用工具的Twitter项目，提供了一些有用的MapReduce和Pig类。Elephant
    Bird有一个`LzoPigStorage`类，可以在Pig中与基于文本的、LZOP压缩的数据一起工作。
- en: ^(25) See the appendix for more details on Elephant Bird.
  id: totrans-1176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([25]) 有关Elephant Bird的更多详细信息，请参阅附录。
- en: Hive can work with LZOP-compressed text files by using the `com.hadoop.mapred.DeprecatedLzoTextInputFormat`
    input format class found in the LZO library.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: Hive可以通过使用LZO库中找到的`com.hadoop.mapred.DeprecatedLzoTextInputFormat`输入格式类来与LZOP压缩的文本文件一起工作。
- en: Summary
  id: totrans-1178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Working with splittable compression in Hadoop is tricky. If you’re fortunate
    enough to be able to store your data in Avro or Parquet, they offer the simplest
    way to work with files that can be easily compressed and split. If you want to
    compress other file formats and need them to be split, LZOP is the only real candidate.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop中处理可分割压缩很棘手。如果您有幸能够将数据存储在Avro或Parquet中，它们提供了处理可以轻松压缩和分割的文件的最简单方法。如果您想压缩其他文件格式并且需要它们被分割，LZOP是唯一真正的候选者。
- en: As I mentioned earlier, the Elephant Bird project provides some useful LZOP
    input formats that will work with LZOP-compressed file formats such as XML and
    plain text. If you need to work with an LZOP-compressed file format that isn’t
    supported by either Todd Lipcon’s LZO project or Elephant Bird, you’ll have to
    write your own input format. This is a big hurdle for developers. I hope at some
    point Hadoop will be able to support compressed files with custom splitting logic
    so that end users don’t have to write their own input formats for compression.
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，Elephant Bird项目提供了一些有用的LZOP输入格式，这些格式可以与LZOP压缩文件格式（如XML和平文）一起使用。如果你需要处理LZOP项目或Elephant
    Bird不支持的其他LZOP压缩文件格式，你必须编写自己的输入格式。这对开发者来说是一个巨大的障碍。我希望Hadoop在某个时刻能够支持具有自定义拆分逻辑的压缩文件，这样最终用户就不必为压缩编写自己的输入格式。
- en: Compression is likely to be a hard-and-fast requirement for any production environment
    where resources are always scarce. Compression also allows faster execution times
    for your computational jobs, so it’s a compelling aspect of storage. In the previous
    section I showed you how to evaluate and pick the codec best suited for your data.
    We also covered how to use compression with HDFS, MapReduce, Pig, and Hive. Finally,
    we tackled the tricky subject of splittable LZOP compression.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源总是稀缺的任何生产环境中，压缩可能是一个硬性要求。压缩还允许计算作业的执行时间更快，因此它是存储的一个有吸引力的方面。在前一节中，我展示了如何评估和选择最适合你的数据的编解码器。我们还涵盖了如何使用HDFS、MapReduce、Pig和Hive进行压缩。最后，我们解决了可拆分LZOP压缩的棘手问题。
- en: 4.3\. Chapter summary
  id: totrans-1182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 章节总结
- en: Big data in the form of large numbers of small files brings to light a limitation
    in HDFS, and in this chapter we worked around it by looking at how you can package
    small files into larger Avro containers.
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 以大量小文件形式存在的大数据在HDFS中暴露出了一些限制，在本章中，我们通过探讨如何将小文件打包成更大的Avro容器来解决这个问题。
- en: Compression is a key part of any large cluster, and we evaluated and compared
    the different compression codecs. I recommended codecs based on various criteria
    and also showed you how to compress and work with these compressed files in Map-Reduce,
    Pig, and Hive. We also looked at how to work with LZOP to achieve compression
    as well as blazing-fast computation with multiple input splits.
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩是任何大型集群的关键部分，我们评估并比较了不同的压缩编解码器。我根据各种标准推荐了编解码器，并展示了如何在Map-Reduce、Pig和Hive中使用压缩文件以及如何与这些压缩文件一起工作。我们还探讨了如何使用LZOP实现压缩以及通过多个输入拆分实现快速计算。
- en: This and the previous chapter were dedicated to looking at techniques for picking
    the right file format and working effectively with big data in MapReduce and HDFS.
    It’s now time to apply this knowledge and look at how to move data in and out
    of Hadoop. That’s covered in the next chapter.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和上一章致力于探讨选择合适的文件格式以及如何在MapReduce和HDFS中有效地处理大数据的技术。现在是时候应用这些知识，看看如何将数据移动到Hadoop中及从中移出了。这将在下一章中介绍。
- en: Chapter 5\. Moving data into and out of Hadoop
  id: totrans-1186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章\. 将数据移动到Hadoop中及从中移出
- en: '*This chapter covers*'
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding key design considerations for data ingress and egress tools
  id: totrans-1188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据导入和导出工具的关键设计考虑因素
- en: Low-level methods for moving data into and out of Hadoop
  id: totrans-1189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据移动到Hadoop中及从中移出的低级方法
- en: Techniques for moving log files and relational and NoSQL data, as well as data
    in Kafka, in and out of HDFS
  id: totrans-1190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志文件、关系型数据库和NoSQL数据，以及Kafka中的数据在HDFS中移动的技术
- en: Data movement is one of those things that you aren’t likely to think too much
    about until you’re fully committed to using Hadoop on a project, at which point
    it becomes this big scary unknown that has to be tackled. How do you get your
    log data sitting across thousands of hosts into Hadoop? What’s the most efficient
    way to get your data out of your relational and No/NewSQL systems and into Hadoop?
    How do you get Lucene indexes generated in Hadoop out to your servers? And how
    can these processes be automated?
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 数据移动是那些你不太可能过多思考的事情之一，直到你完全承诺在一个项目中使用Hadoop，这时它变成了一个巨大的、令人恐惧的未知数，必须去应对。你如何将分布在不同数千个主机上的日志数据移动到Hadoop中？将你的数据从关系型数据库和No/NewSQL系统中高效地移动到Hadoop中有什么最佳方法？你如何将Hadoop中生成的Lucene索引移动到你的服务器上？以及如何自动化这些流程？
- en: Welcome to [chapter 5](#ch05), where the goal is to answer these questions and
    set you on your path to worry-free data movement. In this chapter you’ll first
    see how data across a broad spectrum of locations and formats can be moved into
    Hadoop, and then you’ll see how data can be moved out of Hadoop.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到[第5章](#ch05)，目标是回答这些问题，并让你走上无忧无虑的数据移动之路。在本章中，你将首先看到如何将来自广泛位置和格式的数据移动到Hadoop中，然后你将看到如何将数据从Hadoop中移动出去。
- en: This chapter starts by highlighting key data-movement properties, so that as
    you go through the rest of this chapter you can evaluate the fit of the various
    tools. It goes on to look at low-level and high-level tools that can be used to
    move your data. We’ll start with some simple techniques, such as using the command
    line and Java for ingress,^([[1](#ch05fn01)]) but we’ll quickly move on to more
    advanced techniques like using NFS and DistCp.
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先强调关键的数据移动属性，以便你在阅读本章的其余部分时可以评估各种工具的适用性。接着，它将探讨用于移动数据的低级和高级工具。我们将从一些简单的技术开始，例如使用命令行和Java进行入口，^([[1](#ch05fn01)])
    但很快我们将转向更高级的技术，如使用NFS和DistCp。
- en: ¹ *Ingress* and *egress* refer to data movement into and out of a system, respectively.
  id: totrans-1194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ *入口*和*出口*分别指数据进入和离开系统。
- en: Once the low-level tooling is out of the way, we’ll survey higher-level tools
    that have simplified the process of ferrying data into Hadoop. We’ll look at how
    you can automate the movement of log files with Flume, and how Sqoop can be used
    to move relational data. So as not to ignore some of the emerging data systems,
    you’ll also be introduced to methods that can be employed to move data from HBase
    and Kafka into Hadoop.
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦低级工具被排除在外，我们将调查高级工具，这些工具简化了将数据传输到Hadoop的过程。我们将探讨如何使用Flume自动化日志文件的移动，以及如何使用Sqoop移动关系型数据。为了不忽略一些新兴的数据系统，你还将了解到可以将数据从HBase和Kafka移动到Hadoop的方法。
- en: We’ll cover a lot of ground in this chapter, and it’s likely that you’ll have
    specific types of data you need to work with. If this is the case, feel free to
    jump directly to the section that provides the details you need.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖大量内容，你可能会遇到需要处理的具体数据类型。如果情况如此，请直接跳转到提供所需详细信息的部分。
- en: Let’s start things off with a look at key ingress and egress system considerations.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看关键入口和出口系统考虑因素开始。
- en: 5.1\. Key elements of data movement
  id: totrans-1198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 数据移动的关键要素
- en: Moving large quantities of data in and out of Hadoop offers logistical challenges
    that include consistency guarantees and resource impacts on data sources and destinations.
    Before we dive into the techniques, however, we need to discuss the design elements
    you should be aware of when working with data movement.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop中移动大量数据会带来物流挑战，包括一致性保证以及对数据源和目的地的资源影响。然而，在我们深入探讨技术之前，我们需要讨论在设计元素方面你应该注意的事项，当你在处理数据移动时。
- en: Idempotence
  id: totrans-1200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 幂等性
- en: An idempotent operation produces the same result no matter how many times it’s
    executed. In a relational database, the inserts typically aren’t idempotent, because
    executing them multiple times doesn’t produce the same resulting database state.
    Alternatively, updates often are idempotent, because they’ll produce the same
    end result.
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等操作无论执行多少次都会产生相同的结果。在关系型数据库中，插入通常不是幂等的，因为多次执行不会产生相同的数据库状态。另一方面，更新通常是幂等的，因为它们会产生相同的结果。
- en: Any time data is being written, idempotence should be a consideration, and data
    ingress and egress in Hadoop are no different. How well do distributed log collection
    frameworks deal with data retransmissions? How do you ensure idempotent behavior
    in a MapReduce job where multiple tasks are inserting into a database in parallel?
    We’ll examine and answer these questions in this chapter.
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: 任何数据写入时，都应该考虑幂等性，Hadoop中的数据入口和出口也不例外。分布式日志收集框架处理数据重传的能力如何？你如何在多个任务并行向数据库插入的MapReduce作业中确保幂等行为？我们将在本章中探讨并回答这些问题。
- en: Aggregation
  id: totrans-1203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聚合
- en: The data aggregation process combines multiple data elements. In the context
    of data ingress, this can be useful because moving large quantities of small files
    into HDFS potentially translates into NameNode memory woes, as well as slow MapReduce
    execution times. Having the ability to aggregate files or data together mitigates
    this problem and is a feature to consider.
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 数据聚合过程结合多个数据元素。在数据导入的背景下，这可能很有用，因为将大量小文件移动到HDFS可能会转化为NameNode内存问题，以及MapReduce执行时间变慢。能够聚合文件或数据可以减轻这个问题，并且是一个值得考虑的功能。
- en: Data format transformation
  id: totrans-1205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据格式转换
- en: The data format transformation process converts one data format into another.
    Often your source data isn’t in a format that’s ideal for processing in tools
    such as Map-Reduce. If your source data is in multiline XML or JSON form, for
    example, you may want to consider a preprocessing step. This would convert the
    data into a form that can be split, such as one JSON or XML element per line,
    or convert it into a format such as Avro. [Chapter 3](kindle_split_013.html#ch03)
    contains more details on these data formats.
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 数据格式转换过程将一种数据格式转换为另一种格式。通常，您的源数据可能不是Map-Reduce等工具处理的最理想格式。例如，如果您的源数据是多行XML或JSON形式，您可能需要考虑一个预处理步骤。这将把数据转换成可以分割的形式，比如每行一个JSON或XML元素，或者转换成Avro等格式。[第3章](kindle_split_013.html#ch03)包含了关于这些数据格式的更多详细信息。
- en: Compression
  id: totrans-1207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 压缩
- en: Compression not only helps by reducing the footprint of data at rest, but also
    has I/O advantages when reading and writing data.
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩不仅有助于减少静态数据的大小，而且在读取和写入数据时也有I/O优势。
- en: Availability and recoverability
  id: totrans-1209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可用性和可恢复性
- en: Recoverability allows an ingress or egress tool to retry in the event of a failed
    operation. Because it’s unlikely that any data source, sink, or Hadoop itself
    can be 100% available, it’s important that an ingress or egress action be retried
    in the event of failure.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 可恢复性允许在操作失败的情况下，导入或导出工具重试。由于任何数据源、接收器或Hadoop本身都不太可能达到100%的可用性，因此，在失败的情况下重试导入或导出操作非常重要。
- en: Reliable data transfer and data validation
  id: totrans-1211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可靠的数据传输和数据验证
- en: In the context of data transportation, checking for correctness is how you verify
    that no data corruption occurred as the data was in transit. When you work with
    heterogeneous systems such as Hadoop data ingress and egress, the fact that data
    is being transported across different hosts, networks, and protocols only increases
    the potential for problems during data transfer. A common method for checking
    the correctness of raw data, such as storage devices, is Cyclic Redundancy Checks
    (CRCs), which are what HDFS uses internally to maintain block-level integrity.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据传输的背景下，检查正确性是您验证数据在传输过程中没有发生数据损坏的方法。当您与像Hadoop数据导入和导出这样的异构系统一起工作时，数据需要在不同的主机、网络和协议之间传输的事实，只会增加数据传输过程中出现问题的可能性。检查原始数据（如存储设备）正确性的常见方法是对称冗余检查（CRCs），这是HDFS内部用于维护块级完整性的方法。
- en: In addition, it’s possible that there are problems in the source data itself
    due to bugs in the software generating the data. Performing these checks at ingress
    time allows you to do a one-time check, instead of dealing with all the downstream
    consumers of the data that would have to be updated to handle errors in the data.
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，源数据本身可能由于生成数据的软件中的错误而存在问题。在导入时进行这些检查允许您进行一次性检查，而不是处理所有下游数据消费者，这些消费者必须更新以处理数据中的错误。
- en: Resource consumption and performance
  id: totrans-1214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源消耗和性能
- en: Resource consumption and performance are measures of system resource utilization
    and system efficiency, respectively. Ingress and egress tools don’t typically
    impose significant load (resource consumption) on a system, unless you have appreciable
    data volumes. For performance, the questions to ask include whether the tool performs
    ingress and egress activities in parallel, and if so, what mechanisms it provides
    to tune the amount of parallelism. For example, if your data source is a production
    database and you’re using MapReduce to ingest that data, don’t use a large number
    of concurrent map tasks to import data.
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 资源消耗和性能分别是系统资源利用率和系统效率的衡量标准。除非您有可观的的数据量，否则导入和导出工具通常不会对系统造成显著的负载（资源消耗）。对于性能，需要问的问题包括工具是否并行执行导入和导出活动，如果是的话，它提供了什么机制来调整并行度。例如，如果您的数据源是生产数据库，您正在使用MapReduce导入数据，请不要使用大量的并发映射任务来导入数据。
- en: Monitoring
  id: totrans-1216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监控
- en: 'Monitoring ensures that functions are performing as expected in automated systems.
    For data ingress and egress, monitoring breaks down into two elements: ensuring
    that the processes involved in ingress and egress are alive, and validating that
    source and destination data are being produced as expected. Monitoring should
    also include verifying that the data volumes being moved are at expected levels;
    unexpected drops or highs in your data will alert you to potential system issues
    or bugs in your software.'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 监控确保在自动化系统中函数按预期执行。对于数据入口和出口，监控分为两个要素：确保参与入口和出口的过程处于活跃状态，并验证源和目标数据是否按预期产生。监控还应包括验证正在移动的数据量是否处于预期水平；数据量的意外下降或上升会提醒您潜在的系统问题或软件中的错误。
- en: Speculative execution
  id: totrans-1218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 投机执行
- en: MapReduce has a feature called *speculative execution* that launches duplicate
    tasks near the end of a job for tasks that are still executing. This helps prevent
    slow hardware from impacting job execution times. But if you’re using a map task
    to perform inserts into a relational database, for example, you should be aware
    that you could have two parallel processes inserting the same data.^([[2](#ch05fn02)])
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 有一个名为 *投机执行* 的功能，在作业接近结束时启动重复任务，用于仍在执行的任务。这有助于防止慢速硬件影响作业执行时间。但是，如果您正在使用地图任务将数据插入到关系数据库中，例如，您应该意识到您可能有两个并行过程插入相同的数据.^([[2](#ch05fn02)])
- en: ² Map- and reduce-side speculative execution can be disabled via the `mapreduce.map.speculative`
    and `mapreduce.reduce.speculative` configurables in Hadoop 2.
  id: totrans-1220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 Hadoop 2 中，可以通过 `mapreduce.map.speculative` 和 `mapreduce.reduce.speculative`
    配置项来禁用地图和减少侧的投机执行。
- en: On to the techniques. Let’s start with how you can leverage Hadoop’s built-in
    ingress mechanisms.
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是技术。让我们从如何利用 Hadoop 内置的入口机制开始。
- en: 5.2\. Moving data into Hadoop
  id: totrans-1222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 将数据移动到 Hadoop
- en: 'The first step in working with data in Hadoop is to make it available to Hadoop.
    There are two primary methods that can be used to move data into Hadoop: writing
    external data at the HDFS level (a data push), or reading external data at the
    MapReduce level (more like a pull). Reading data in MapReduce has advantages in
    the ease with which the operation can be parallelized and made fault tolerant.
    Not all data is accessible from MapReduce, however, such as in the case of log
    files, which is where other systems need to be relied on for transportation, including
    HDFS for the final data hop.'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 中处理数据的第一个步骤是使其对 Hadoop 可用。有两种主要方法可以将数据移动到 Hadoop：在 HDFS 层面上写入外部数据（数据推送），或在
    MapReduce 层面上读取外部数据（更像是拉取）。在 MapReduce 中读取数据具有操作易于并行化和容错的优势。然而，并非所有数据都可以从 MapReduce
    访问，例如日志文件，这时需要依赖其他系统进行传输，包括 HDFS 进行最终的数据跳跃。
- en: In this section we’ll look at methods for moving source data into Hadoop. I’ll
    use the design considerations in the previous section as the criteria for examining
    and understanding the different tools.
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨将源数据移动到 Hadoop 的方法。我将使用上一节中的设计考虑因素作为检查和理解不同工具的标准。
- en: We’ll get things started with a look at some low-level methods you can use to
    move data into Hadoop.
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看一些可以用于将数据移动到 Hadoop 的低级方法开始。
- en: 5.2.1\. Roll your own ingest
  id: totrans-1226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 自行构建摄入
- en: Hadoop comes bundled with a number of methods to get your data into HDFS. This
    section will examine various ways that these built-in tools can be used for your
    data movement needs. The first and potentially easiest tool you can use is the
    HDFS command line.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 随带了许多将数据导入 HDFS 的方法。本节将探讨这些内置工具的各种用法，以满足您的数据移动需求。您可以使用的第一个可能也是最容易的工具是
    HDFS 命令行。
- en: '|  |'
  id: totrans-1228
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Picking the right ingest tool for the job
  id: totrans-1229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择合适的摄入工具以完成任务
- en: The low-level tools in this section work well for one-off file movement activities,
    or when working with legacy data sources and destinations that are file-based.
    But moving data in this way is quickly becoming obsolete by the availability of
    tools such as Flume and Kafka (covered later in this chapter), which offer automated
    data movement pipelines.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的低级工具适用于一次性文件移动活动，或当与基于文件的旧数据源和目标一起工作时。但是，随着 Flume 和 Kafka 等工具（本章后面将介绍）的可用性，以这种方式移动数据正迅速变得过时，这些工具提供了自动数据移动管道。
- en: Kafka is a much better platform for getting data from A to B (and B can be a
    Hadoop cluster) than the old-school “let’s copy files around!” With Kafka, you
    only need to pump your data into Kafka, and you have the ability to consume the
    data in real time (such as via Storm) or in offline/batch jobs (such as via Camus).
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 是从 A 到 B 获取数据（B 可以是一个 Hadoop 集群）的一个比传统的“让我们复制文件！”更好的平台。使用 Kafka，你只需将数据泵入
    Kafka，就有能力实时（例如通过 Storm）或离线/批量作业（例如通过 Camus）消费数据。
- en: File-based ingestion flows are, to me at least, a relic of the past (because
    everybody knows how scp works :-P), and they primarily exist for legacy reasons—the
    upstream data sources may have existing tools to create file snapshots (such as
    dump tools for the database), and there’s no infrastructure to migrate or move
    the data into a real-time messaging system such as Kafka.
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文件的摄取流程，至少对我来说，是过去的遗迹（因为每个人都知道 scp 的工作原理 :-P），它们主要存在于遗留原因——上游数据源可能已有现有工具来创建文件快照（例如数据库的转储工具），并且没有基础设施来迁移或移动数据到实时消息系统，如
    Kafka。
- en: '|  |'
  id: totrans-1233
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Technique 33 Using the CLI to load files
  id: totrans-1234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 33 使用 CLI 加载文件
- en: If you have a manual activity that you need to perform, such as moving the examples
    bundled with this book into HDFS, then the HDFS command-line interface (CLI) is
    the tool for you. It’ll allow you to perform most of the operations that you’re
    used to performing on a regular Linux filesystem. In this section we’ll focus
    on copying data from a local filesystem into HDFS.
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个需要手动执行的活动，例如将本书附带示例移动到 HDFS，那么 HDFS 命令行界面（CLI）就是你的工具。它将允许你执行在常规 Linux
    文件系统上执行的大多数操作。在本节中，我们将重点介绍将数据从本地文件系统复制到 HDFS 的操作。
- en: Problem
  id: totrans-1236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to copy files into HDFS using the shell.
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用 shell 将文件复制到 HDFS 中。
- en: Solution
  id: totrans-1238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: The HDFS command-line interface can be used for one-off moves, or it can be
    incorporated into scripts for a series of moves.
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 命令行界面可用于一次性移动，或者可以将其集成到脚本中以进行一系列移动。
- en: Discussion
  id: totrans-1240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Copying a file from local disk to HDFS is done with the `hadoop` command:'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件从本地磁盘复制到 HDFS 使用 `hadoop` 命令完成：
- en: '[PRE84]'
  id: totrans-1242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The behavior of the Hadoop `-put` command differs from the Linux `cp` command—in
    Linux if the destination already exists, it is overwritten; in Hadoop the copy
    fails with an error:'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop `-put` 命令的行为与 Linux `cp` 命令不同——在 Linux 中，如果目标已存在，则会覆盖它；在 Hadoop 中，复制操作会失败并显示错误：
- en: '[PRE85]'
  id: totrans-1244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The `-f` option must be added to force the file to be overwritten:'
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: 必须添加 `-f` 选项来强制覆盖文件：
- en: '[PRE86]'
  id: totrans-1246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Much like with the Linux `cp` command, multiple files can be copied using the
    same command. In this case, the final argument must be the directory in HDFS into
    which the local files are copied:'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Linux `cp` 命令类似，可以使用相同的命令复制多个文件。在这种情况下，最后一个参数必须是本地文件要复制到的 HDFS 目录：
- en: '[PRE87]'
  id: totrans-1248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'You can also use Linux pipes to pipe the output of a command into an HDFS file—use
    the same `-put` command and add a separate hyphen after it, which tells Hadoop
    to read the input from standard input:'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 Linux 管道将命令的输出重定向到 HDFS 文件中——使用相同的 `-put` 命令，并在其后添加一个单独的连字符，这告诉 Hadoop
    从标准输入读取输入：
- en: '[PRE88]'
  id: totrans-1250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'To test for the existence of a file or directory, use the `-test` command with
    either the `-e` or `-d` option to test for file or directory existence, respectively.
    The exit code of the command is `0` if the file or directory exists, and `1` if
    it doesn’t:'
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试文件或目录是否存在，请使用 `-test` 命令并选择 `-e` 或 `-d` 选项分别测试文件或目录的存在性。如果文件或目录存在，命令的退出码为
    `0`，如果不存在，则为 `1`：
- en: '[PRE89]'
  id: totrans-1252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'If all you want to do is “touch” a file in HDFS (create a new empty file),
    the `touchz` option is what you’re looking for:'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想在 HDFS 中“touch”一个文件（创建一个新空文件），那么 `touchz` 选项就是你要找的：
- en: '[PRE90]'
  id: totrans-1254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'There are many more operations supported by the `fs` command—to see the full
    list, run the command without any options:'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: '`fs` 命令支持许多更多操作——要查看完整列表，请在没有任何选项的情况下运行该命令：'
- en: '[PRE91]'
  id: totrans-1256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: The CLI is designed for interactive HDFS activities, and it can also be incorporated
    into scripts for some tasks you wish to automate. The disadvantage of the CLI
    is that it’s low-level and doesn’t have any automation mechanisms built in, so
    you’ll need to look elsewhere if that’s your goal. It also requires a fork for
    each command, which may be fine if you’re using it in a bash script, but it likely
    isn’t what you want to use if you’re trying to integrate HDFS functionality into
    a Python or Java application. In that case, the overhead of launching an external
    process for each command, in addition to the brittle nature of launching and interacting
    with an external process, is likely something you’ll want to avoid.
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行界面（CLI）是为交互式HDFS活动设计的，它也可以集成到脚本中以自动化一些任务。CLI的缺点是它是低级的，并且没有内置任何自动化机制，所以如果你有这个目标，你可能需要另寻他法。它还需要为每个命令进行一次分叉，如果你在bash脚本中使用它，这可能没问题，但如果你试图将HDFS功能集成到Python或Java应用程序中，这很可能不是你想要的。在这种情况下，为每个命令启动外部进程的开销，以及启动和与外部进程交互的脆弱性，可能是你想要避免的。
- en: The next technique is more suited to working with HDFS in programming languages
    such as Python.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个技术更适合在Python等编程语言中与HDFS一起工作。
- en: Technique 34 Using REST to load files
  id: totrans-1259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇 34 使用REST加载文件
- en: The CLI is handy for quickly running commands and for scripting. However, it
    incurs the overhead of forking a separate process for each command, which is overhead
    that you’ll probably want to avoid, especially if you’re interfacing with HDFS
    in a programming language. This technique covers working with HDFS in languages
    other than Java (which is covered in a subsequent section).
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: CLI对于快速运行命令和脚本来说很方便。然而，它需要为每个命令分叉一个单独的进程，这是你可能想要避免的开销，尤其是如果你正在用编程语言与HDFS交互。本技术涵盖了在Java（将在下一节中介绍）之外的语言中与HDFS一起工作。
- en: Problem
  id: totrans-1261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to be able to interact with HDFS from a programming language that doesn’t
    have a native interface to HDFS.
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望能够从没有HDFS原生接口的编程语言中与HDFS交互。
- en: Solution
  id: totrans-1263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Hadoop’s WebHDFS interface, which offers a full-featured REST API for HDFS
    operations.
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop的WebHDFS接口，它为HDFS操作提供了一个功能齐全的REST API。
- en: Discussion
  id: totrans-1265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Before you get started, you’ll need to make sure WebHDFS is enabled on your
    cluster (by default it’s not). This is governed by the `dfs.webhdfs.enabled` property.
    If it’s not enabled, you’ll need to update hdfs-site.xml and add the following:'
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始之前，你需要确保你的集群启用了WebHDFS（默认情况下是未启用的）。这由`dfs.webhdfs.enabled`属性控制。如果它没有启用，你需要更新hdfs-site.xml并添加以下内容：
- en: '[PRE92]'
  id: totrans-1267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: In this technique, we’ll cover running WebHDFS on an unsecured Hadoop cluster.^([[3](#ch05fn03)])
    If you’re working on a secure Hadoop cluster, you won’t supply the `user.name`
    argument; instead you’ll authenticate with Kerberos using `kinit` prior to interacting
    with WebHDFS, and then supply `--negotiate -u:youruser` in the curl command line.
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，我们将介绍在未加密的Hadoop集群上运行WebHDFS。[^[[3](#ch05fn03)]] 如果你在一个安全的Hadoop集群上工作，你不会提供`user.name`参数；相反，你将在与WebHDFS交互之前使用`kinit`通过Kerberos进行身份验证，然后在curl命令行中提供`--negotiate
    -u:youruser`。
- en: ³ In an unsecured Hadoop cluster (which is the default setup), any user can
    masquerade as another user in the cluster. This is especially problematic with
    WebHDFS, which exposes the username directly in the URL, making it trivial to
    enter some other user’s name. Hadoop security in the form of Kerberos will prevent
    this from happening because it requires that users be authenticated via LDAP or
    Active Directory prior to interacting with Hadoop.
  id: totrans-1269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ 在一个未加密的Hadoop集群（默认设置），任何用户都可以伪装成集群中的另一个用户。这对于WebHDFS来说尤其成问题，因为它直接在URL中暴露用户名，这使得输入其他用户的名称变得非常简单。Hadoop的安全机制Kerberos将防止这种情况发生，因为它要求用户在与Hadoop交互之前通过LDAP或Active
    Directory进行身份验证。
- en: '|  |'
  id: totrans-1270
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Warning: Running WebHDFS on an unsecured cluster'
  id: totrans-1271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告：在未加密的集群上运行WebHDFS
- en: If WebHDFS is enabled for a cluster where security is turned off, then it can
    easily be used to run commands as arbitrary users in your cluster (simply change
    the username in the URL to be any user in the cluster). It’s recommended that
    you only run WebHDFS with security turned on.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在一个关闭了安全性的集群中启用了WebHDFS，那么它可以很容易地用来以任意用户身份在集群中运行命令（只需在URL中更改用户名即可）。建议你只在启用了安全性的情况下运行WebHDFS。
- en: '|  |'
  id: totrans-1273
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Because you’re using HTTP to communicate with the NameNode in this technique,
    you’ll need to know the host and port that the NameNode RPC service is running
    on. This is configured with the `dfs.namenode.http-address` property. In a pseudo-distributed
    setup, this is most likely set to `0.0.0.0:50070`. We’ll assume a pseudo-distributed
    setup for the rest of this technique—substitute the appropriate host and port
    for your setup.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您使用 HTTP 与 NameNode 通信，所以您需要知道 NameNode RPC 服务正在运行的宿主和端口。这通过 `dfs.namenode.http-address`
    属性进行配置。在伪分布式设置中，这最可能是设置为 `0.0.0.0:50070`。我们将假设伪分布式设置用于本技术的其余部分——替换您设置中适当的主机和端口。
- en: 'You can start by creating a file in HDFS using the CLI:'
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 CLI 在 HDFS 中创建文件：
- en: '[PRE93]'
  id: totrans-1276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'You can use WebHDFS to get at all sorts of interesting metadata about the file
    (replace `aholmes` in the following URL with your username):'
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 WebHDFS 获取有关文件的各种有趣元数据（在以下 URL 中的 `aholmes` 替换为您的用户名）：
- en: '[PRE94]'
  id: totrans-1278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The syntax for commands is composed of two parts: first the path, followed
    by the operation being performed. You also need to supply the username that you
    wish to execute the operation as; otherwise HDFS will assume you’re an anonymous
    user with restricted access. [Figure 5.1](#ch05fig01) highlights these parts of
    the URL path.'
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的语法由两部分组成：首先是路径，然后是执行的操作。您还需要提供要执行操作的用户的用户名；否则，HDFS 将假设您是一个具有受限访问权限的匿名用户。[图
    5.1](#ch05fig01) 强调了 URL 路径的这些部分。
- en: Figure 5.1\. Dissecting the WebHDFS URL path
  id: totrans-1280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1\. 解构 WebHDFS URL 路径
- en: '![](05fig01_alt.jpg)'
  id: totrans-1281
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig01_alt.jpg)'
- en: 'Reading the file from HDFS is just a matter of specifying `OPEN` as the operation:'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 从 HDFS 读取文件只需指定 `OPEN` 作为操作：
- en: '[PRE95]'
  id: totrans-1283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Writing a file using WebHDFS is a two-step process. The first step informs
    the Name-Node of your intent to create a new file. You do that with an HTTP `PUT`
    command:'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 WebHDFS 写入文件是一个两步过程。第一步通知 NameNode 您要创建新文件的意图。您可以通过 HTTP `PUT` 命令来完成：
- en: '[PRE96]'
  id: totrans-1285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'At this point, the file hasn’t been written yet—you just gave the NameNode
    the opportunity to determine which DataNode you’ll be writing to, which was specified
    in the “Location” header in the response. You’ll need to grab that URL and then
    issue a second HTTP `PUT` to perform the actual write:'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，文件尚未写入——您只是给了 NameNode 确定您将写入哪个 DataNode 的机会，这已在响应中的“Location”标题中指定。您需要获取该
    URL，然后发出第二个 HTTP `PUT` 来执行实际的写入：
- en: '[PRE97]'
  id: totrans-1287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'You can verify that the write was successful with a read of the file:'
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过读取文件来验证写入是否成功：
- en: '[PRE98]'
  id: totrans-1289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: WebHDFS supports all the HDFS operations that you can perform using the regular
    command line,^([[4](#ch05fn04)]) and it’s more useful because it gives you access
    to metadata in a structured JSON form, which makes it easier to parse the data.
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: WebHDFS 支持您使用常规命令行执行的所有 HDFS 操作，^([[4](#ch05fn04)]) 并且更有用，因为它以结构化的 JSON 形式提供对元数据的访问，这使得解析数据更容易。
- en: ⁴ See the WebHDFS REST API page ([http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html))
    for the full set of operations you can perform.
  id: totrans-1291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ 请参阅 WebHDFS REST API 页面 ([http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html))，了解您可以执行的全部操作集。
- en: It’s worth mentioning some additional features provided by WebHDFS. First, data
    locality is present for the first block of the file. The NameNode redirects the
    client to the DataNode that hosts the first block, giving you strong data locality.
    For subsequent blocks in the file, the DataNode acts as a proxy and streams data
    to and from the node that holds the block’s data.
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，WebHDFS 提供的一些附加功能。首先，文件的第一块具有数据局部性。NameNode 将客户端重定向到托管第一块数据的 DataNode，为您提供强大的数据局部性。对于文件中的后续块，DataNode
    充当代理，并将数据从持有块数据的节点流式传输到节点。
- en: WebHDFS is also integrated with Hadoop’s secure authentication, meaning that
    you can enable Kerberos and use the delegation tokens in your HTTP requests. Additionally,
    the API will maintain wire-level compatibility across Hadoop releases, meaning
    that the commands you issue today will work with future versions of Hadoop (and
    vice versa). This is a useful tool for accessing multiple clusters running different
    versions of Hadoop.
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: WebHDFS 也集成了 Hadoop 的安全认证，这意味着您可以在 HTTP 请求中启用 Kerberos 并使用委托令牌。此外，API 将在 Hadoop
    版本之间保持网络级别的兼容性，这意味着您今天发出的命令将与 Hadoop 的未来版本（反之亦然）兼容。这对于访问运行不同版本 Hadoop 的多个集群是一个有用的工具。
- en: There are several projects that provide WebHDFS libraries in various languages
    (listed in [table 5.1](#ch05table01)) to make it easier for you to get up and
    running with it.^([[5](#ch05fn05)])
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个项目提供了多种语言的WebHDFS库（列于[表5.1](#ch05table01)中），以便您更容易地开始使用它.^([[5](#ch05fn05)])
- en: ⁵ In fact, a new C client was written in Hadoop 2 called libwebhdfs to leverage
    WebHDFS. See [https://issues.apache.org/jira/browse/HDFS-2656](https://issues.apache.org/jira/browse/HDFS-2656).
  id: totrans-1295
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ 事实上，为了利用 WebHDFS，在 Hadoop 2 中编写了一个新的 C 客户端，名为 libwebhdfs。请参阅 [https://issues.apache.org/jira/browse/HDFS-2656](https://issues.apache.org/jira/browse/HDFS-2656)。
- en: Table 5.1\. WebHDFS libraries
  id: totrans-1296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1\. WebHDFS库
- en: '| Language | Link |'
  id: totrans-1297
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | 链接 |'
- en: '| --- | --- |'
  id: totrans-1298
  prefs: []
  type: TYPE_TB
- en: '| C | libwebhdfs (bundled with Hadoop) |'
  id: totrans-1299
  prefs: []
  type: TYPE_TB
  zh: '| C | libwebhdfs (与 Hadoop 一起打包) |'
- en: '| Python | [https://github.com/drelu/webhdfs-py](https://github.com/drelu/webhdfs-py)
    |'
  id: totrans-1300
  prefs: []
  type: TYPE_TB
  zh: '| Python | [https://github.com/drelu/webhdfs-py](https://github.com/drelu/webhdfs-py)
    |'
- en: '| Ruby | [https://github.com/kzk/webhdfs](https://github.com/kzk/webhdfs) [https://rubygems.org/gems/webhdfs](https://rubygems.org/gems/webhdfs)
    |'
  id: totrans-1301
  prefs: []
  type: TYPE_TB
  zh: '| Ruby | [https://github.com/kzk/webhdfs](https://github.com/kzk/webhdfs) [https://rubygems.org/gems/webhdfs](https://rubygems.org/gems/webhdfs)
    |'
- en: '| Perl | [http://search.cpan.org/~afaris/Apache-Hadoop-WebHDFS-0.04/lib/Apache/Hadoop/WebHDFS.pm](http://search.cpan.org/~afaris/Apache-Hadoop-WebHDFS-0.04/lib/Apache/Hadoop/WebHDFS.pm)
    |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
  zh: '| Perl | [http://search.cpan.org/~afaris/Apache-Hadoop-WebHDFS-0.04/lib/Apache/Hadoop/WebHDFS.pm](http://search.cpan.org/~afaris/Apache-Hadoop-WebHDFS-0.04/lib/Apache/Hadoop/WebHDFS.pm)
    |'
- en: WebHDFS is useful when the client has access to all the NameNodes and DataNodes.
    In locked-down environments, this may not be the case, and you may need to look
    at HttpFS.
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端可以访问所有NameNodes和数据Nodes时，WebHDFS非常有用。在受限环境中，情况可能并非如此，你可能需要考虑使用HttpFS。
- en: Technique 35 Accessing HDFS from behind a firewall
  id: totrans-1304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 35 从防火墙后面访问 HDFS
- en: Production Hadoop environments are often locked down to protect the data in
    these clusters. Part of the security procedures could include putting your cluster
    behind a firewall, which is a nuisance if you’re trying to read from or write
    to HDFS from outside of the firewall. This technique looks at the HttpFS gateway,
    which can provide HDFS access using HTTP (which is often opened up on firewalls).
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境中的 Hadoop 通常会被锁定以保护这些集群中的数据。安全程序的一部分可能包括将您的集群置于防火墙之后，如果您试图从防火墙外部读取或写入 HDFS，这将会是一个麻烦。这项技术探讨了
    HttpFS 网关，它可以通过 HTTP（通常在防火墙上开放）提供对 HDFS 的访问。
- en: Problem
  id: totrans-1306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to write to HDFS, but there’s a firewall restricting access to the
    NameNode and/or the DataNodes.
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: 你想向HDFS写入数据，但有一个防火墙限制了访问NameNode和/或DataNodes。
- en: Solution
  id: totrans-1308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the HttpFS gateway, which is a standalone server that provides access to
    HDFS over HTTP. Because it’s a separate service and it’s HTTP, it can be configured
    to run on any host that has access to the Hadoop nodes, and you can open a firewall
    rule to allow traffic to the service.
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HttpFS网关，这是一个独立的服务器，它通过HTTP提供对HDFS的访问。因为它是一个独立的服务，并且使用HTTP，所以它可以配置在任何可以访问Hadoop节点的宿主上运行，并且你可以开启防火墙规则以允许流量访问该服务。
- en: Discussion
  id: totrans-1310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: HttpFS is useful because not only does it allow you use REST to access HDFS,
    but it has a complete Hadoop filesystem implementation, which means you can use
    the CLI and native HDFS Java clients to talk to HDFS, as shown in [figure 5.2](#ch05fig02).
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: HttpFS非常有用，因为它不仅允许你使用REST访问HDFS，而且还拥有完整的Hadoop文件系统实现，这意味着你可以使用CLI和本地的HDFS Java客户端与HDFS进行通信，如图5.2所示。[figure
    5.2](#ch05fig02)。
- en: Figure 5.2\. The HttpFS gateway architecture
  id: totrans-1312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2\. HttpFS 网关架构
- en: '![](05fig02_alt.jpg)'
  id: totrans-1313
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig02_alt.jpg)'
- en: To get HttpFS up and running, you’re going to have to designate a proxy user.
    This is the user that will run the HttpFS process, and this user will also be
    configured in Hadoop as the proxy user.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 要使 HttpFS 运行起来，您需要指定一个代理用户。这个用户将运行 HttpFS 进程，并且这个用户也将被配置在 Hadoop 中作为代理用户。
- en: 'Suppose you have a user called foxyproxy that you’re going to designate as
    your proxy user. You’d update your core-site.xml with this:'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
- en: '![](184fig01_alt.jpg)'
  id: totrans-1316
  prefs: []
  type: TYPE_IMG
  zh: '![图片](184fig01_alt.jpg)'
- en: Basically, what you’ve done here is indicate that Hadoop should only accept
    proxy requests from host `localhost`, and that foxyproxy can impersonate any user
    (you can lock down the set of users that can be impersonated by supplying a comma-separated
    list of group names). Change the username, host, and group values so that they
    make sense in your environment.
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve made your changes to core-site.xml, you’ll have to bounce Hadoop.
    Next you’ll need to start the HttpFS process:'
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您对core-site.xml进行了更改，您将不得不重启Hadoop。接下来，您需要启动HttpFS进程：
- en: '[PRE99]'
  id: totrans-1319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Now you can issue the same curl commands that you used in the previous technique
    with WebHDFS. This is one of the nice things about the HttpFS gateway—the syntax
    is exactly the same. To perform a directory listing on the root directory, you’d
    do the following:'
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用与之前技术中相同的curl命令来使用WebHDFS。这是HttpFS网关的一个优点——语法完全相同。要在根目录上执行目录列表，您将执行以下操作：
- en: '[PRE100]'
  id: totrans-1321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: The only difference between this curl command and the ones you used in the previous
    technique is the port number. HttpFS by default runs on port 14000, but this can
    be changed by editing `httpfs-env.sh`. Some of the more interesting properties
    that can be changed in the file are shown in [table 5.2](#ch05table02).
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: 与您在之前技术中使用的curl命令相比，唯一的区别是端口号。HttpFS默认运行在端口14000上，但可以通过编辑`httpfs-env.sh`来更改。文件中可以更改的一些有趣属性在[表5.2](#ch05table02)中显示。
- en: Table 5.2\. HttpFS properties
  id: totrans-1323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.2\. HttpFS属性
- en: '| Property | Default value | Description |'
  id: totrans-1324
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 默认值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-1325
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| HTTPFS_HTTP_PORT | 14000 | The HTTP port that HttpFS listens on. |'
  id: totrans-1326
  prefs: []
  type: TYPE_TB
  zh: '| HTTPFS_HTTP_PORT | 14000 | HttpFS监听的HTTP端口。 |'
- en: '| HTTPFS_ADMIN_PORT | 14001 | The admin port for HttpFS. |'
  id: totrans-1327
  prefs: []
  type: TYPE_TB
  zh: '| HTTPFS_ADMIN_PORT | 14001 | HttpFS的管理端口。 |'
- en: '| HTTPFS_LOG | ${HTTPFS_HOME}/logs | The logs directory for HttpFS. |'
  id: totrans-1328
  prefs: []
  type: TYPE_TB
  zh: '| HTTPFS_LOG | ${HTTPFS_HOME}/logs | HttpFS的日志目录。 |'
- en: '| HTTPFS_HTTP_HOSTNAME | `hostname -f` | The command used to determine which
    host HttpFS is running on. This information is passed to the NameNode so it can
    compare this to the value of hadoop.proxyuser.${user}.hosts that you configured
    earlier in core-site.xml. |'
  id: totrans-1329
  prefs: []
  type: TYPE_TB
  zh: '| HTTPFS_HTTP_HOSTNAME | `hostname -f` | 用于确定HttpFS运行在哪个主机上的命令。此信息传递给NameNode，以便它可以将其与您在core-site.xml中之前配置的hadoop.proxyuser.${user}.hosts的值进行比较。
    |'
- en: There are additional Kerberos and user- and group-level settings that can be
    configured in httpfs-site.xml.^([[6](#ch05fn06)])
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: 在httpfs-site.xml中还可以配置额外的Kerberos和用户及组级别设置。[6](#ch05fn06)
- en: ⁶ Consult the “HttpFS configuration properties” web page at [http://hadoop.apache.org/docs/stable/hadoop-hdfs-httpfs/httpfs-default.html](http://hadoop.apache.org/docs/stable/hadoop-hdfs-httpfs/httpfs-default.html).
  id: totrans-1331
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ 请参考“HttpFS配置属性”网页[http://hadoop.apache.org/docs/stable/hadoop-hdfs-httpfs/httpfs-default.html](http://hadoop.apache.org/docs/stable/hadoop-hdfs-httpfs/httpfs-default.html)。
- en: Differences between WebHDFS and HttpFS
  id: totrans-1332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: WebHDFS和HttpFS之间的差异
- en: The primary difference between WebHDFS and HttpFS is the accessibility of the
    client to all the data nodes. If your client has access to all the data nodes,
    then WebHDFS will work for you, as reading and writing files involves the client
    talking directly to the data nodes for data transfer. On the other hand, if you’re
    behind a firewall, your client probably doesn’t have access to all the data nodes,
    in which case the HttpFS option will work best for you. With HttpFS, the server
    will talk to the data nodes, and your client just needs to talk to the single
    HttpFS server.
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: WebHDFS和HttpFS之间的主要区别是客户端对所有数据节点的可访问性。如果您的客户端可以访问所有数据节点，那么WebHDFS将适用于您，因为读写文件涉及客户端直接与数据节点进行数据传输。另一方面，如果您在防火墙后面，您的客户端可能无法访问所有数据节点，在这种情况下，HttpFS选项将最适合您。使用HttpFS时，服务器将与数据节点通信，而您的客户端只需要与单个HttpFS服务器通信。
- en: If you have a choice, pick WebHDFS because there’s an inherent advantage in
    clients talking directly to the data nodes—it allows you to easily scale the number
    of concurrent clients across multiple hosts without hitting the network bottleneck
    of all the data being streamed via the HttpFS server. This is especially true
    if your clients are running on the data nodes themselves, as you’ll be using the
    data locality benefits of WebHDFS by directly streaming any locally hosted HDFS
    data blocks from the local filesystem instead of over the network.
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有选择，请选择WebHDFS，因为客户端直接与数据节点通信具有固有的优势——它允许您轻松地跨多个主机扩展并发客户端的数量，而不会遇到所有数据通过HttpFS服务器流过的网络瓶颈。这尤其适用于您的客户端在数据节点上运行的情况，因为您将通过直接从本地文件系统流式传输任何本地托管的HDFS数据块，而不是通过网络，来使用WebHDFS的数据本地化优势。
- en: Technique 36 Mounting Hadoop with NFS
  id: totrans-1335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧36 使用NFS挂载Hadoop
- en: Often it’s a lot easier to work with Hadoop data if it’s accessible as a regular
    mount to your filesystem. This allows you to use existing scripts, tools, and
    programming languages and to interact with your data in HDFS. This section looks
    at how you can easily copy data in and out of HDFS using an NFS mount.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Hadoop数据可以通过常规挂载访问你的文件系统，那么与Hadoop数据交互通常会容易得多。这允许你使用现有的脚本、工具和编程语言，以及与HDFS中的数据交互。本节将探讨如何使用NFS挂载轻松地将数据复制到HDFS和从HDFS中复制出来。
- en: Problem
  id: totrans-1337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to treat HDFS as a regular Linux filesystem and use standard Linux
    tools to interact with HDFS.
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望将HDFS视为一个常规的Linux文件系统，并使用标准的Linux工具与HDFS交互。
- en: Solution
  id: totrans-1339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Hadoop’s NFS implementation to access data in HDFS.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop的NFS实现来访问HDFS中的数据。
- en: Discussion
  id: totrans-1341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Prior to Hadoop 2.1, the only way to NFS-mount HDFS was with FUSE. It wasn’t
    recommended for general use due to various performance and reliability issues.
    It also introduced an additional burden of requiring the driver to be installed
    on any client machine (in other words, it didn’t provide an NFS gateway).
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 2.1之前，唯一可以NFS挂载HDFS的方式是使用FUSE。由于各种性能和可靠性问题，它不建议用于通用用途。它还引入了额外的负担，即需要在任何客户端机器上安装驱动程序（换句话说，它没有提供NFS网关）。
- en: The new NFS implementation in Hadoop addresses all of the shortcomings with
    the old FUSE-based system. It’s a proper NFSv3 implementation, and it allows you
    to run one or more NFS gateways for increased availability and throughput.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop中新的NFS实现解决了旧FUSE系统所有的不足。它是一个正确的NFSv3实现，并允许你运行一个或多个NFS网关以增加可用性和吞吐量。
- en: '[Figure 5.3](#ch05fig03) shows the various Hadoop NFS components in action.'
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.3](#ch05fig03)显示了各种Hadoop NFS组件正在运行。'
- en: Figure 5.3\. Hadoop NFS
  id: totrans-1345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3\. Hadoop NFS
- en: '![](05fig03_alt.jpg)'
  id: totrans-1346
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3的替代文本](05fig03_alt.jpg)'
- en: 'To get the NFS services up and running, you’ll first need to stop the NFS services
    running on your host. On Linux systems this can be achieved with the following
    commands:'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动NFS服务，你首先需要停止主机上运行的NFS服务。在Linux系统上，可以使用以下命令实现：
- en: '[PRE101]'
  id: totrans-1348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Next you need to start the Hadoop NFS services. The first service you’ll launch
    is portmap, which provides a registry service for protocols and their associated
    transports and ports. It runs on a restricted port, so it needs to be launched
    as a root user:'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你需要启动Hadoop NFS服务。你将启动的第一个服务是portmap，它为协议及其相关的传输和端口提供注册服务。它运行在受限端口上，因此需要以root用户身份启动：
- en: '[PRE102]'
  id: totrans-1350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Next you need to start the actual NFS service. It’s important that the user
    running this service be the same user that you use to run HDFS:'
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你需要启动实际的NFS服务。重要的是运行此服务的用户必须与运行HDFS的用户相同：
- en: '[PRE103]'
  id: totrans-1352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Verify that the services are running by running `rpcinfo` and `showmount`—you
    should see output similar to the following:'
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行`rpcinfo`和`showmount`来验证服务是否正在运行——你应该看到类似于以下输出的内容：
- en: '[PRE104]'
  id: totrans-1354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Now you need to mount HDFS on a directory on your host. In the following example
    I’ve picked /hdfs as the mount directory. The second mount command verifies that
    the mount has been created:'
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要在主机上的一个目录上挂载HDFS。在下面的示例中，我选择了/hdfs作为挂载目录。第二个挂载命令验证挂载已创建：
- en: '[PRE105]'
  id: totrans-1356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: You’re all set! Now you can manipulate HDFS directly using the mounted filesystem.
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经准备好了！现在你可以直接使用挂载的文件系统来操作HDFS。
- en: 'There are a few things to consider when using the NFS gateway:'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用NFS网关时，有一些事情需要考虑：
- en: HDFS is an append-only filesystem. You can append to files, but you can’t perform
    random writes. If you have a hard-and-fast requirement that you need to work with
    Hadoop using a filesystem that supports random writes, you should take a look
    at MapR’s distribution of Hadoop.
  id: totrans-1359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS是一个只允许追加的文件系统。你可以向文件追加内容，但不能执行随机写入。如果你有严格的要求，需要使用支持随机写入的文件系统来与Hadoop一起工作，你应该看看MapR的Hadoop发行版。
- en: Hadoop version 2.2 doesn’t support secure Hadoop (Kerberos), and there’s an
    open ticket to add that support.^([[7](#ch05fn07)])
  id: totrans-1360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 2.2版本不支持安全的Hadoop（Kerberos），并且有一个开放的任务单来添加该支持.^([[7](#ch05fn07)])
- en: ⁷ Kerberos support in the NFS gateway is being tracked in [https://issues.apache.org/jira/browse/HDFS-5539](https://issues.apache.org/jira/browse/HDFS-5539).
  id: totrans-1361
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ 在NFS网关中跟踪Kerberos支持的任务单是[https://issues.apache.org/jira/browse/HDFS-5539](https://issues.apache.org/jira/browse/HDFS-5539)。
- en: Support for proxy users isn’t available until Hadoop 2.4 (or 3). This essentially
    means that previous versions of Hadoop will execute all commands as superuser,
    because there’s a requirement that the NFS gateway run as the same user as HDFS
    itself.
  id: totrans-1362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理用户的支持直到Hadoop 2.4（或3）才可用。这基本上意味着Hadoop的早期版本将执行所有命令作为超级用户，因为NFS网关需要以与HDFS相同的用户身份运行。
- en: Due the these restrictions, it’s advised that the NFS gateway be reserved for
    experimental use, or for use in a single-tenant cluster where user-level security
    isn’t a concern.
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些限制，建议将 NFS 网关保留用于实验用途，或者用于用户级安全不是关注点的单个租户集群。
- en: Technique 37 Using DistCp to copy data within and between clusters
  id: totrans-1364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号 37：使用 DistCp 在集群内部和之间复制数据
- en: Imagine that you have a large amount of data you want to move into or out of
    Hadoop. With most of the techniques in this section, you have a bottleneck because
    you’re funneling the data through a single host, which is the host on which you’re
    running the process. To optimize data movement as much as possible, you want to
    leverage MapReduce to copy data in parallel. This is where DistCp comes into play,
    and this technique examines several ways that you can use DistCp to efficiently
    copy data between Hadoop clusters, as well as into and out of an NFS mount.
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一大批数据想要移动到或从 Hadoop 中移动。在本节的大部分技术中，你有一个瓶颈，因为你正在通过单个主机（即运行进程的主机）将数据通过管道传输。为了尽可能优化数据移动，你想要利用
    MapReduce 并行复制数据。这就是 DistCp 发挥作用的地方，这项技术探讨了你可以使用 DistCp 在 Hadoop 集群之间以及到和从 NFS
    挂载中高效复制数据的几种方法。
- en: Problem
  id: totrans-1366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to efficiently copy large amounts of data between Hadoop clusters and
    have the ability for incremental copies.
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要在 Hadoop 集群之间高效地复制大量数据，并且具有增量复制的功能。
- en: Solution
  id: totrans-1368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use DistCp, a parallel file-copy tool built into Hadoop.
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置在 Hadoop 中的并行文件复制工具 DistCp。
- en: Discussion
  id: totrans-1370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: In this section, we’ll start by covering the important configuration aspects
    of DistCp. After that, we’ll go on to look at specific scenarios where you’ll
    want to use DistCp, and the best way to configure and run it.
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍 DistCp 的重要配置方面。之后，我们将探讨你想要使用 DistCp 的具体场景，以及配置和运行它的最佳方式。
- en: '|  |'
  id: totrans-1372
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: DistCp version 2
  id: totrans-1373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DistCp 版本 2
- en: This technique covers the newer version of DistCp available in Hadoop 2, called
    DistCp 2\. This code was backported into Hadoop 1.2.0 and is available by using
    `distcp2` as the command—on Hadoop 2 it replaces the existing DistCp so the normal
    `distcp` command can be used.
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术涵盖了 Hadoop 2 中可用的 DistCp 的新版本，称为 DistCp 2。此代码被回滚到 Hadoop 1.2.0，可以通过使用 `distcp2`
    作为命令来使用——在 Hadoop 2 中，它替换了现有的 DistCp，因此可以使用正常的 `distcp` 命令。
- en: 'DistCp 2 supports the same set of command-line arguments as the legacy version
    of DistCp, but brings with it a number of useful advantages:'
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: DistCp 2 支持与旧版 DistCp 相同的命令行参数集，但它带来了一系列有用的优势：
- en: Reduced setup and execution time when working with a large number of files,
    as the driver no longer needs to preprocess all the inputs (this is now deferred
    to the mappers).
  id: totrans-1376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当处理大量文件时，减少了设置和执行时间，因为驱动程序不再需要预处理所有输入（现在已推迟到映射器）。
- en: It now has a full-featured Java interface and removes the need for Java clients
    to serialize arguments into strings.
  id: totrans-1377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在它有一个完整的 Java 接口，并消除了 Java 客户端将参数序列化为字符串的需求。
- en: Atomic commits allow all-or-none copying semantics.
  id: totrans-1378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原子提交允许所有或无的复制语义。
- en: Using option `-update` to skip files that already exist in the destination will
    result in file attributes being changed if they differ from the source files.
  id: totrans-1379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `-update` 选项跳过目标中已存在的文件将导致文件属性发生变化，如果它们与源文件不同。
- en: Empty directories are no longer skipped as part of the copy.
  id: totrans-1380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空目录不再在复制过程中被跳过。
- en: '|  |'
  id: totrans-1381
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'DistCp utilizes a map-only MapReduce job to perform a copy. A very simple example
    follows, where it’s used within a single Hadoop cluster to copy the source directory,
    /hello, into a destination directory, /world:'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: DistCp 使用仅映射的 MapReduce 作业来执行复制。以下是一个非常简单的示例，它在一个单独的 Hadoop 集群中使用，将源目录 /hello
    复制到目标目录 /world：
- en: '[PRE106]'
  id: totrans-1383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: This command will create the /world directory if it doesn’t already exist, and
    then copy the contents of /hello (all its files and directories recursively) into
    /world. You may be wondering how DistCp deals with files that already exist in
    the destination—keep on reading for details.
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将在不存在的情况下创建 /world 目录，然后将 /hello 目录（及其所有文件和子目录）的内容复制到 /world。你可能想知道 DistCp
    如何处理目标中已存在的文件——继续阅读以获取详细信息。
- en: Dealing with destination files that already exist
  id: totrans-1385
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理已存在的目标文件
- en: Files and directories that already exist in the destination are left untouched
    (even if the files are different). You can change this behavior by adding the
    arguments shown in [table 5.3](#ch05table03).
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 目标中已存在的文件和目录保持不变（即使文件不同）。你可以通过添加 [表 5.3](#ch05table03) 中显示的参数来更改此行为。
- en: Table 5.3\. DistCp arguments that impact where files are copied, and the behavior
    should destination files preexist
  id: totrans-1387
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.3. 影响文件复制位置以及目标文件是否已存在的DistCp参数
- en: '| Argument | Description |'
  id: totrans-1388
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-1389
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| None (neither -update nor -overwrite) | Source files are never recopied if
    the destination already exists. |'
  id: totrans-1390
  prefs: []
  type: TYPE_TB
  zh: '| 无（既不使用`-update`也不使用`-overwrite`） | 如果目标已存在，则永远不会重新复制源文件。 |'
- en: '| -update | Source files are recopied if any of the following are true:'
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: '| -update | 如果以下任何一项为真，则重新复制源文件：'
- en: Source and destination file sizes are different.
  id: totrans-1392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源文件和目标文件的大小不同。
- en: Source and destination file CRCs don’t match.^([[a](#ch05fn08)])
  id: totrans-1393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源文件和目标文件的CRC不匹配.^([[a](#ch05fn08)])
- en: ^a File CRC checks can be turned off with the -skipcrccheck argument.
  id: totrans-1394
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a 可以使用`-skipcrccheck`参数关闭文件CRC检查。
- en: Source and destination file block sizes don’t match.
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源文件和目标文件的块大小不匹配。
- en: '|'
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| -overwrite | Source files are always recopied if the destination file already
    exists. |'
  id: totrans-1397
  prefs: []
  type: TYPE_TB
  zh: '| -overwrite | 如果目标文件已存在，则始终重新复制源文件。 |'
- en: 'You can see the number of files that are skipped by looking at the SKIP counter
    that’s dumped to standard output when the job completes:'
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看作业完成后输出到标准输出的SKIP计数器来查看跳过的文件数量：
- en: '[PRE107]'
  id: totrans-1399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Another factor to understand about the `-update` and `-overwrite` arguments
    is that they subtly change the behavior of what is copied. Without these options,
    if the source is a directory, that directory is created under the destination
    directory. With either the `-update` or `-overwrite` arguments, only the files
    and subdirectories are copied, and not the source directory. This is best demonstrated
    with an example:'
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`-update`和`-overwrite`参数的另一个需要理解的因素是，它们微妙地改变了复制行为。没有这些选项时，如果源是目录，则该目录将在目标目录下创建。使用`-update`或`-overwrite`参数之一时，仅复制文件和子目录，而不是源目录。这最好通过以下示例来说明：
- en: '[PRE108]'
  id: totrans-1401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Ignoring errors
  id: totrans-1402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 忽略错误
- en: When you’re using DistCp to copy over a large number of files, it’s wise to
    execute the command with the `-i` flag to ignore errors. This way a single error
    won’t cause your entire copy process to fail, and you can reattempt to copy any
    failed files by reissuing the same DistCp command with the `-update` option.
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用DistCp复制大量文件时，使用`-i`标志执行命令以忽略错误是明智的。这样，单个错误不会导致您的整个复制过程失败，并且您可以通过重新发出带有`-update`选项的相同DistCp命令来重新尝试复制任何失败的文件。
- en: Dynamic copy strategy
  id: totrans-1404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动态复制策略
- en: The default behavior for DistCp is to preallocate work for each mapper by evenly
    spreading all the files in such a way that all the mappers are copying approximately
    the same number of bytes. In theory, this sounds like a great way to fairly allocate
    work, but in reality, factors such as differing hardware, hardware errors, and
    poor configuration often results in long-tail job execution, where a handful of
    straggler mappers take much longer than the others.
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: DistCp的默认行为是为每个mapper平均分配所有文件的工作，以便所有mapper复制大约相同数量的字节。从理论上讲，这似乎是一种公平分配工作的好方法，但在现实中，由于不同的硬件、硬件错误和配置不当等因素，往往会导致长尾作业执行，其中少数几个落后mapper的执行时间比其他mapper长得多。
- en: With DistCp 2 you can use an alternative strategy, where the mappers pick up
    work directly as opposed to having it preallocated. This is called the *dynamic
    copy strategy*, and it’s activated with the `-strategy dynamic` argument. The
    net effect of adding this argument is improved copy times, as the faster mappers
    can pick up the slack of the slower mappers.
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DistCp 2，您可以使用一种替代策略，其中mapper直接获取工作，而不是预先分配。这被称为*动态复制策略*，可以通过使用`-strategy
    dynamic`参数来激活。添加此参数的净效果是提高了复制时间，因为速度较快的mapper可以弥补速度较慢mapper的不足。
- en: Atomic commits
  id: totrans-1407
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 原子提交
- en: Another useful feature in DistCp 2 is the notion of atomic commits. The default
    behavior of DistCp is for each file to be written to a temporary file and then
    moved to the final destination. This means that there would be no way to undo
    any files that were copied prior to an error encountered in the job.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: DistCp 2中的另一个有用功能是原子提交的概念。DistCp的默认行为是每个文件都写入一个临时文件，然后移动到最终目标。这意味着在作业中遇到错误之前，无法撤销已复制的任何文件。
- en: Atomic commits therefore allow you to defer the actual “commit” until the end
    of the job when all files have been copied so that you don’t see any partial writes
    if an error is encountered. This feature can be enabled using the `-atomic` argument.
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，原子提交允许您将实际的“提交”推迟到作业结束时，此时所有文件都已复制，这样在遇到错误时就不会看到任何部分写入。此功能可以通过使用`-atomic`参数来启用。
- en: Parallelism and number of mappers
  id: totrans-1410
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 并行性和mapper数量
- en: Currently the most granular unit of work for DistCp is at the file level. Therefore,
    only one mapper will be used to copy each file, regardless of how large the files
    are. Bumping up the number of mappers for a job won’t have any effect on speeding
    up the copy.
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: 目前DistCp最细粒度的作业单位是文件级别。因此，无论文件有多大，每个文件都只使用一个mapper进行复制。增加作业的mapper数量不会对加快复制速度有任何影响。
- en: By default, DistCp runs with 20 mappers, and which files each mapper copies
    are determined by the copy strategy you have selected. The Hadoop developers put
    some thought into the default setting for the number of mappers—choosing the right
    value is a function of how much network bandwidth you want to utilize (discussed
    next), and how many tasks you want to occupy during the copy.
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，DistCp使用20个mapper，每个mapper复制的哪些文件由你选择的复制策略决定。Hadoop开发者对mapper数量的默认设置进行了深思熟虑——选择正确的值取决于你想要利用多少网络带宽（将在下文讨论），以及你希望在复制过程中占用多少任务。
- en: You can change the number of mappers by specifying `-m` followed by your desired
    value.
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过指定`-m`后跟你的期望值来更改mapper的数量。
- en: Bandwidth
  id: totrans-1414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带宽
- en: A final consideration worth mentioning is the network bandwidth used during
    a copy. Large copies can saturate and overwhelm the network between clusters.
    One way to keep on the good side of the network operations folks in your organization
    is to use the `-bandwidth` argument to specify a cap on the amount of bandwidth
    each map task consumes during a copy. The value for this argument is in megabytes
    per second (MBps).
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在复制过程中使用的网络带宽。大容量复制可能会使集群之间的网络饱和并超负荷。为了保持你组织中网络操作人员的良好关系，可以使用`-bandwidth`参数来指定每个map任务在复制过程中消耗的带宽上限。此参数的值以每秒兆字节（MBps）为单位。
- en: Additional options
  id: totrans-1416
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他选项
- en: So far we’ve looked at some of the more interesting options in DistCp. To see
    the full list of options, you can run the `distcp` command without any options,
    or head on over to the online Hadoop docs at [http://hadoop.apache.org/docs/r1.2.1/distcp2.html](http://hadoop.apache.org/docs/r1.2.1/distcp2.html).
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了DistCp中一些更有趣的选项。要查看完整的选项列表，可以在没有任何选项的情况下运行`distcp`命令，或者直接访问在线Hadoop文档[http://hadoop.apache.org/docs/r1.2.1/distcp2.html](http://hadoop.apache.org/docs/r1.2.1/distcp2.html)。
- en: Copying data from an NFS mount into HDFS
  id: totrans-1418
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从NFS挂载复制数据到HDFS
- en: 'DistCp may be a good fit if you have files sitting on a filer or a NAS that
    you want to copy into HDFS. This will only work if all the DataNodes have the
    data mounted, because the DistCp mappers running on the DataNodes require access
    to both the source and destination. The following example shows how you would
    perform the copy. Note the `file` scheme used to tell Hadoop that the local filesystem
    should be used as the source:'
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一些文件位于文件系统或NAS上，并且想要将它们复制到HDFS中，DistCp可能是一个不错的选择。这仅当所有DataNode都挂载数据时才有效，因为运行在DataNode上的DistCp
    mapper需要访问源和目标。以下示例显示了如何执行复制。注意用于告诉Hadoop使用本地文件系统作为源的`file`方案：
- en: '[PRE109]'
  id: totrans-1420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Copying data within the same cluster
  id: totrans-1421
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在同一集群内复制数据
- en: In what situations would you use DistCp in place of a regular `hadoop fs -cp`
    command? The regular `cp` command is a single-threaded approach to copying data—it
    goes file by file, and streams the data from the server to the client and back
    out to the server. Compare that to DistCp, which launches a MapReduce job that
    uses multiple mappers to perform the copy. As a rule of thumb, you should use
    the regular copy process when dealing with tens of GBs and consider DistCp when
    working with hundreds of GBs or more.
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
  zh: 在什么情况下你会使用DistCp代替常规的`hadoop fs -cp`命令？常规的`cp`命令是一种单线程的复制数据方法——它逐个文件进行，从服务器流数据到客户端，然后再从客户端流回服务器。与之相比，DistCp会启动一个MapReduce作业，使用多个mapper来执行复制。一般来说，当处理数十GB的数据时，应使用常规复制过程；当处理数百GB或更多数据时，应考虑使用DistCp。
- en: 'When the same cluster is both the source and the destination, nothing special
    is required to qualify the source or destination:'
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: 当同一个集群既是源也是目标时，不需要对源或目标进行特殊指定：
- en: '[PRE110]'
  id: totrans-1424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Copying between two clusters running the same version of Hadoop
  id: totrans-1425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在运行相同版本Hadoop的两个集群之间复制
- en: Now let’s look at copying data between two clusters running the same version
    of Hadoop. This approach optimizes for the fact that they’re both running the
    same version of Hadoop by using Hadoop-native filesystem reads and writes, which
    emphasize data locality. Unfortunately the Hadoop RPC is sensitive to the fact
    that the client and server versions are identical, so this won’t work if the versions
    differ. In that situation you’ll need to skip to the next subsection.
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在运行相同版本 Hadoop 的两个集群之间复制数据。这种方法通过使用 Hadoop 原生文件系统的读写操作来优化它们都运行相同版本 Hadoop
    的事实，这强调了数据局部性。不幸的是，Hadoop RPC 对客户端和服务器版本相同的事实很敏感，因此如果版本不同，则此方法将不起作用。在这种情况下，你需要跳到下一个小节。
- en: 'Imagine that you have two HDFS setups, one running on nn1 and the other on
    nn2, and both NameNodes are running on the default RPC port.^([[8](#ch05fn09)])
    Copying files from the /source to the /dest directories between the clusters would
    be achieved with the following command:'
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有两个 HDFS 设置，一个运行在 nn1 上，另一个运行在 nn2 上，并且两个 NameNode 都运行在默认 RPC 端口上.^([[8](#ch05fn09)])
    在集群之间从 /source 到 /dest 目录复制文件可以使用以下命令：
- en: ⁸ To figure out the actual host and port for each NameNode, examine the value
    of `fs.default.name` or `fs.defaultFS` in core-site.xml.
  id: totrans-1428
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ 要确定每个 NameNode 的实际主机和端口，请检查 core-site.xml 中 `fs.default.name` 或 `fs.defaultFS`
    的值。
- en: '[PRE111]'
  id: totrans-1429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: With two clusters in play, you may be wondering which cluster you should use
    to run DistCp. If you have a firewall sitting between the clusters and ports can
    only be opened in one direction, then you’ll have to run the job on the cluster
    that has read or write access to the other cluster.
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到两个集群时，你可能想知道应该使用哪个集群来运行 DistCp。如果你在集群之间有一个防火墙，并且端口只能单向打开，那么你必须在具有对另一个集群的读取或写入访问权限的集群上运行作业。
- en: Next let’s look at how to run DistCp between clusters on different Hadoop versions.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何在运行不同 Hadoop 版本的集群之间运行 DistCp。
- en: Copying between clusters running different versions of Hadoop
  id: totrans-1432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在运行不同版本 Hadoop 的集群之间复制
- en: The previous approach won’t work when your clusters are running different versions
    of Hadoop. Hadoop’s RPC doesn’t have backward or forward compatibility built into
    it, so a newer version of the Hadoop client can’t talk to an older version of
    a Hadoop cluster, and vice versa.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的集群运行不同版本的 Hadoop 时，先前的这种方法将不起作用。Hadoop 的 RPC 没有内置向前或向后兼容性，因此较新版本的 Hadoop
    客户端无法与较旧版本的 Hadoop 集群通信，反之亦然。
- en: 'With recent versions of Hadoop, you have two options for the copy: the older
    HFTP and the newer WebHDFS. Let’s first look at the legacy method, HFTP.'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最近版本的 Hadoop，你有两种复制选项：较旧的 HFTP 和较新的 WebHDFS。让我们首先看看传统方法，HFTP。
- en: 'HFTP is a version-independent interface on HDFS that uses HTTP as the transport
    mechanism. It offers a read-only view into HDFS, so by definition this means that
    you’ll have to always use it as the source in your DistCp. It’s enabled via the
    `hftp` scheme in the NameNode URI, as seen in the following example:'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: HFTP 是 HDFS 上的一个版本无关的接口，它使用 HTTP 作为传输机制。它提供了对 HDFS 的只读视图，因此从定义上讲，这意味着你将不得不始终将其用作
    DistCp 中的源。它通过 NameNode URI 中的 `hftp` 方案启用，如下例所示：
- en: '[PRE112]'
  id: totrans-1436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Look at hdfs-site.xml (and hdfs-default.xml if you don’t see it in hdfs-site.xml)
    to figure out the host and port to use for HFTP (specifically `dfs.http.port`,
    or `dfs.namenode.http-address` if it’s not set). If securing the data in transit
    is important to you, look at using the HFTPS scheme, which uses HTTPS for transport
    (configure or examine `dfs.hftp.https.port`, which if not set will default to
    `dfs.https.port` for the port).
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 hdfs-site.xml（如果你在 hdfs-site.xml 中看不到它，请查看 hdfs-default.xml）以确定用于 HFTP 的主机和端口（特别是
    `dfs.http.port`，如果未设置，则为 `dfs.namenode.http-address`）。如果你认为在传输过程中保护数据很重要，请考虑使用
    HFTPS 方案，该方案使用 HTTPS 进行传输（配置或检查 `dfs.hftp.https.port`，如果未设置，则默认为 `dfs.https.port`）。
- en: With HFTP(S), you’ll have to run the DistCp command on the destination cluster
    so that HDFS writes using the same Hadoop client version as the destination. But
    what if this is too constrained for your environment—what if you have a firewall
    that doesn’t allow you to run DistCp on the destination? That’s where WebHDFS
    comes into play.
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HFTP(S)，你必须在目标集群上运行 DistCp 命令，以便使用与目标相同的 Hadoop 客户端版本来写入 HDFS。但如果这在你的环境中过于受限——如果你的防火墙不允许你在目标上运行
    DistCp，那该怎么办？这就是 WebHDFS 发挥作用的地方。
- en: 'WebHDFS has the advantage over HFTP of providing both a read and write interface.
    You can use it for either the source or destination in your DistCp, as shown here:'
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: WebHDFS 比起 HFTP 的优势在于提供了读写接口。你可以将其用作 DistCp 中的源或目标，如下所示：
- en: '[PRE113]'
  id: totrans-1440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: WebHDFS has an additional benefit in the form of data locality—it uses HTTP
    redirection when reading and writing data so that reads and writes are performed
    with the actual DataNode that stores the data. It’s highly recommended that you
    use WebHDFS rather than HFTP for both its writing abilities and the performance
    improvements.
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: WebHDFS具有额外的优势，即数据本地性——在读取和写入数据时使用HTTP重定向，以便使用实际存储数据的DataNode进行读写。强烈建议您使用WebHDFS而不是HFTP，因为WebHDFS不仅具有出色的写入能力，而且性能也得到了提升。
- en: Examine the value of `dfs.namenode.http-address` to determine the host and port
    that you should use with WebHDFS.
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`dfs.namenode.http-address`的值，以确定您应该与WebHDFS一起使用的宿主和端口。
- en: Other destinations
  id: totrans-1443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他目的地
- en: DistCp works with any implementation of the Hadoop filesystem interface; [table
    5.4](#ch05table04) shows the most popular implementations that are bundled with
    Hadoop.
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: DistCp与任何Hadoop文件系统接口的实现都兼容；[表5.4](#ch05table04)显示了与Hadoop捆绑的最流行的实现。
- en: Table 5.4\. URI schemes and their related Hadoop filesystem implementations
  id: totrans-1445
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.4\. URI方案及其相关的Hadoop文件系统实现
- en: '| Scheme | Details |'
  id: totrans-1446
  prefs: []
  type: TYPE_TB
  zh: '| Scheme | 详情 |'
- en: '| --- | --- |'
  id: totrans-1447
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| hdfs | Provides native access to Hadoop’s own HDFS. The only downside is
    that backward and forward compatibility aren’t supported. |'
  id: totrans-1448
  prefs: []
  type: TYPE_TB
  zh: '| hdfs | 提供对Hadoop自己的HDFS的本地访问。唯一的缺点是不支持前后兼容性。 |'
- en: '| file | Used to read and write from the local filesystem. |'
  id: totrans-1449
  prefs: []
  type: TYPE_TB
  zh: '| file | 用于从本地文件系统读取和写入。 |'
- en: '| hftp and hsftp | A legacy, read-only view on top of HDFS that emphasized
    API compatibility to support any version of Hadoop. It was the old-school way
    of copying data between clusters running different versions of Hadoop. hsftp provides
    an implementation that uses HTTPS for transport for added security. |'
  id: totrans-1450
  prefs: []
  type: TYPE_TB
  zh: '| hftp和hsftp | 在HDFS之上提供了一种传统的只读视图，强调API兼容性以支持任何版本的Hadoop。这是在运行不同版本Hadoop的集群之间复制数据的老式方法。hsftp提供了一个使用HTTPS进行传输的实现，以增加安全性。
    |'
- en: '| webhdfs | Can be used with both WebHDFS (see technique 34) if your client
    has access to the Hadoop cluster, and the HttpFS gateway (see technique 35) for
    accessing HDFS from behind a firewall. This is the replacement for the read-only
    hftp implementation. It supports a read and write interface to HDFS. In addition,
    this filesystem can be used to read and write between different versions of Hadoop.
    |'
  id: totrans-1451
  prefs: []
  type: TYPE_TB
  zh: '| webhdfs | 如果您的客户端可以访问Hadoop集群，则可以与WebHDFS（见技术34）一起使用，并且可以通过HttpFS网关（见技术35）从防火墙后面访问HDFS。这是read-only
    hftp实现的替代品。它支持对HDFS的读写接口。此外，此文件系统可以用于在不同版本的Hadoop之间读取和写入。 |'
- en: '| ftp | Uses FTP as the storage implementation. |'
  id: totrans-1452
  prefs: []
  type: TYPE_TB
  zh: '| ftp | 使用FTP作为存储实现。 |'
- en: '| s3 and s3n | Provides access to Amazon’s S3 filesystem. s3n provides native
    access to S3, whereas the s3 scheme stores data in a block-based manner to work
    around S3’s maximum file-size constraints. |'
  id: totrans-1453
  prefs: []
  type: TYPE_TB
  zh: '| s3和s3n | 提供对Amazon的S3文件系统的访问。s3n提供对S3的本地访问，而s3方案以块为基础存储数据，以绕过S3的最大文件大小限制。
    |'
- en: Summary
  id: totrans-1454
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: DistCp is a powerful tool for moving data into and between Hadoop filesystems.
    Features such as incremental copies enable it to be used in a near-continuous
    fashion to synchronize directories on two systems. And its ability to copy data
    between Hadoop versions means that it’s a very popular way of synchronizing data
    across multiple Hadoop clusters.
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: DistCp是一个在Hadoop文件系统之间移动数据的强大工具。增量复制等特性使其能够以近乎连续的方式使用，以同步两个系统上的目录。而且，它能够在不同版本的Hadoop之间复制数据，这意味着它是跨多个Hadoop集群同步数据的一种非常流行的方式。
- en: '|  |'
  id: totrans-1456
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Executing DistCp
  id: totrans-1457
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 执行DistCp
- en: When you’re running a DistCp command, it’s recommended that you execute it within
    a screen session,^([[9](#ch05fn10)]) or at least use `nohup` to redirect the output
    to a local file.
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行DistCp命令时，建议您在screen会话中执行它，^([[9](#ch05fn10)]) 或者至少使用`nohup`将输出重定向到本地文件。
- en: ⁹ Screen is a Linux utility that manages virtual shells and allows them to persist
    even when the parent shell has terminated. Matt Cutts has an excellent overview
    on his site called “A quick tutorial on screen,” [www.mattcutts.com/blog/a-quick-tutorial-on-screen/](http://www.mattcutts.com/blog/a-quick-tutorial-on-screen/).
  id: totrans-1459
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ Screen是一个Linux实用工具，用于管理虚拟外壳，并允许它们在父外壳终止后仍然持续存在。马特·库茨在他的网站上有一个关于“Screen快速教程”的优秀概述，[www.mattcutts.com/blog/a-quick-tutorial-on-screen/](http://www.mattcutts.com/blog/a-quick-tutorial-on-screen/)。
- en: '|  |'
  id: totrans-1460
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: One limitation of DistCp is that it supports multiple source directories but
    only a single destination directory. This means you can’t use a single DistCp
    job to perform a one-directional synchronization between clusters (unless you
    only need to sync a single directory). In this situation, you could run multiple
    DistCp jobs, or you could run a single job and sync to a staging directory, and
    then follow up the copy with a `fs -mv` to move the staged files into the ultimate
    destinations.
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: DistCp的一个限制是它支持多个源目录，但只有一个目标目录。这意味着你不能使用单个DistCp作业在集群之间执行单向同步（除非你只需要同步单个目录）。在这种情况下，你可以运行多个DistCp作业，或者你可以运行一个作业并将数据同步到一个临时目录，然后使用`fs
    -mv`命令将临时文件移动到最终目标。
- en: Technique 38 Using Java to load files
  id: totrans-1462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇38 使用Java加载文件
- en: Let’s say you’ve generated a number of Lucene indexes in HDFS and you want to
    pull them out to an external host. Maybe, as part of pulling the data out, you
    want to manipulate the files in some way using Java. This technique shows how
    the Java HDFS API can be used to read and write data in HDFS.
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经在HDFS中生成了多个Lucene索引，并且你想将它们拉取到外部主机。也许，在拉取数据的过程中，你希望使用Java以某种方式操作文件。这项技术展示了如何使用Java
    HDFS API在HDFS中读取和写入数据。
- en: Problem
  id: totrans-1464
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to incorporate writing to HDFS into your Java application.
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在Java应用程序中集成写入HDFS的功能。
- en: Solution
  id: totrans-1466
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the Hadoop Java API to access data in HDFS.
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop Java API访问HDFS中的数据。
- en: Discussion
  id: totrans-1468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The HDFS Java API is nicely integrated with Java’s I/O model, which means you
    can work with regular `InputStream`s and `OutputStream`s for I/O. To perform filesystem-level
    operations such as creating, opening, and removing files, Hadoop has an abstract
    class called `FileSystem`, which is extended and implemented for specific filesystems
    that can be leveraged in Hadoop.
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS Java API与Java的I/O模型很好地集成，这意味着你可以使用常规的`InputStream`和`OutputStream`进行I/O操作。为了执行文件系统级别的操作，如创建、打开和删除文件，Hadoop有一个名为`FileSystem`的抽象类，它被扩展并实现为可以在Hadoop中利用的特定文件系统。
- en: 'Earlier, in technique 33, you saw an example of how you can use the CLI to
    stream data from standard input to a file in HDFS:'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，在技术33中，你看到了一个例子，展示了如何使用命令行界面（CLI）将标准输入中的数据流式传输到HDFS中的文件：
- en: '[PRE114]'
  id: totrans-1471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Let’s explore how to do that in Java. There are two main parts to writing the
    code that does this: getting a handle to the `FileSystem` and creating the file,
    and then copying the data from standard input to the `OutputStream`:'
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何在Java中实现这一点。实现这一功能的代码主要有两个部分：获取`FileSystem`的句柄和创建文件，然后将数据从标准输入复制到`OutputStream`：
- en: '![](195fig01_alt.jpg)'
  id: totrans-1473
  prefs: []
  type: TYPE_IMG
  zh: '![195fig01_alt.jpg](195fig01_alt.jpg)'
- en: 'You can see how this code works in practice by running the following command:'
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来查看这段代码在实际中的工作方式：
- en: '[PRE115]'
  id: totrans-1475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Let’s circle back into the code to understand how it worked. The following code
    snippet was used to get a handle to the `FileSystem`. But how did Hadoop know
    which concrete filesystem to return?
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到代码中，了解它是如何工作的。以下代码片段用于获取`FileSystem`的句柄。但Hadoop是如何知道返回哪个具体文件系统的呢？
- en: '[PRE116]'
  id: totrans-1477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: The key is in the `conf` object that’s passed into the `get` method. What’s
    happening is that the `FileSystem` class examines the value of the `fs.defaultFS`
    property,^([[10](#ch05fn11)]) which contains a URI identifying the filesystem
    that should be used. By default, this is configured to be the local filesystem
    (`file:///`), which is why if you try running Hadoop out of the box without any
    configuration, you’ll be using your local filesystem and not HDFS.
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于传递给`get`方法的`conf`对象。发生的情况是`FileSystem`类检查`fs.defaultFS`属性的值，该值包含一个URI，用于标识应使用的文件系统。默认情况下，它配置为本地文件系统（`file:///`），这就是为什么如果你尝试在没有任何配置的情况下运行Hadoop，你会使用本地文件系统而不是HDFS。
- en: ^(10) `fs.default.name` is the deprecated property used in Hadoop 1.
  id: totrans-1479
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(10) `fs.default.name`是Hadoop 1中使用的已弃用属性。]
- en: 'In a pseudo-distributed setup like the one in the appendix, one of the first
    things you would do is configure core-site.xml with an HDFS filesystem:'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录中描述的伪分布式设置中，你首先会做的事情之一是配置core-site.xml以使用HDFS文件系统：
- en: '[PRE117]'
  id: totrans-1481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Hadoop takes the scheme from the URL (`hdfs` in the preceding example) and
    performs a lookup to discover the concrete filesystem. There are two ways that
    a filesystem can be discovered:'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop从URL（前例中的`hdfs`）中获取方案，并执行查找以发现具体的文件系统。文件系统可以通过两种方式被发现：
- en: Built-in filesystems are automatically discovered, and their `getScheme` methods
    are called to determine their schemes. In the example of HDFS, the implementation
    class is `org.apache.hadoop.hdfs.DistributedFileSystem` and the `getScheme` method
    returns `hdfs`.
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置的文件系统会自动被发现，并调用它们的 `getScheme` 方法来确定它们的方案。在 HDFS 的例子中，实现类是 `org.apache.hadoop.hdfs.DistributedFileSystem`，而
    `getScheme` 方法返回 `hdfs`。
- en: Filesystems that aren’t built into Hadoop can be identified by updating coresite
    .xml with `fs.$scheme.impl`, where `$scheme` would be replaced with the scheme
    identified in the URI.
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有内置到 Hadoop 中的文件系统可以通过更新 coresite.xml 中的 `fs.$scheme.impl` 来识别，其中 `$scheme`
    将被 URI 中识别的方案所替换。
- en: 'The `FileSystem` class has a number of methods for manipulating a filesystem—some
    of the more commonly used methods are listed here:'
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
  zh: '`FileSystem` 类有多个用于操作文件系统的方法——一些更常用的方法在此列出：'
- en: '[PRE118]'
  id: totrans-1486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 5.2.2\. Continuous movement of log and binary files into HDFS
  id: totrans-1487
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 将日志和二进制文件持续移动到 HDFS
- en: Log data has long been prevalent across all applications, but with Hadoop came
    the ability to process the large volumes of log data produced by production systems.
    Various systems produce log data, from network devices and operating systems to
    web servers and applications. These log files all offer the potential for valuable
    insights into how systems and applications operate, as well as how they’re used.
    What unifies log files is that they tend to be in text form and line-oriented,
    making them easy to process.
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: 日志数据长期以来在所有应用程序中都很普遍，但随着 Hadoop 的出现，处理生产系统产生的海量日志数据的能力也随之而来。从网络设备、操作系统到 Web
    服务器和应用程序，各种系统都会产生日志数据。这些日志文件都提供了深入了解系统和应用程序如何运行以及如何被使用的宝贵见解。统一日志文件的是，它们通常以文本形式和面向行的方式存在，这使得它们易于处理。
- en: In the previous section we covered low-level methods that you can use to copy
    data into Hadoop. Rather than build your own data-movement tools using these methods,
    this section introduces some higher-level tools that simplify moving your log
    and binary data into Hadoop. Tools like Flume, Sqoop, and Oozie provide mechanisms
    to periodically (or continuously) move data from various data sources such as
    files, relational databases, and messaging systems into Hadoop, and they’ve already
    solved many of the hard problems of dealing with multiple data sources spread
    across different hosts.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了您可以使用的方法来将数据复制到 Hadoop 的低级方法。而不是使用这些方法构建自己的数据移动工具，本节介绍了一些高级工具，这些工具简化了将日志和二进制数据移动到
    Hadoop 的过程。像 Flume、Sqoop 和 Oozie 这样的工具提供了机制，可以定期（或持续）将数据从各种数据源（如文件、关系数据库和消息系统）移动到
    Hadoop，并且它们已经解决了处理分布在不同主机上的多个数据源时遇到的许多难题。
- en: Let’s get started by looking at how Flume can be used to ingest log files into
    HDFS.
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看 Flume 如何将日志文件导入 HDFS 开始。
- en: '|  |'
  id: totrans-1491
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Preferred data-movement methods
  id: totrans-1492
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏好的数据移动方法
- en: The techniques in this section work well if you’re working in a constrained
    legacy environment where you have files that you need to automatically move into
    HDFS.
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在一个需要自动将文件移动到 HDFS 的受限传统环境中工作，本节中的技术效果很好。
- en: An alternative architecture would be to use Kafka as a mechanism to transport
    your data, which would allow you to decouple the producers from the consumers
    and at the same time enable multiple consumers to operate on the data in different
    ways. In this situation, you’d use Kafka to both land data on Hadoop and provide
    a feed into a real-time data-streaming system such as Storm or Spark Streaming,
    which you could then use to perform near-real-time computations. One scenario
    that this enables is a Lambda Architecture, which allows you to calculate aggregated
    data in real time in small increments, and to use the batch tier to perform functions
    such as error correction and adding new data points, thus playing to the strengths
    of both real-time and batch systems.
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种架构方案是使用 Kafka 作为传输数据的机制，这将允许您将生产者与消费者解耦，同时使多个消费者能够以不同的方式处理数据。在这种情况下，您将使用
    Kafka 将数据加载到 Hadoop，并为实时数据流系统（如 Storm 或 Spark Streaming）提供数据源，然后您可以使用这些系统执行近实时计算。这种方案的一个场景是
    Lambda 架构，它允许您以小增量实时计算聚合数据，并使用批量层执行错误纠正和添加新数据点等功能，从而发挥实时和批量系统的优势。
- en: '|  |'
  id: totrans-1495
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Technique 39 Pushing system log messages into HDFS with Flume
  id: totrans-1496
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 39 使用 Flume 将系统日志消息推送到 HDFS
- en: A bunch of log files are being produced by multiple applications and systems
    across multiple servers. There’s no doubt there’s valuable information to be mined
    from these logs, but your first challenge is a logistical one of moving these
    logs into your Hadoop cluster so that you can perform some analysis.
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: 多个应用程序和系统在多个服务器上生成大量的日志文件。毫无疑问，这些日志中包含着有价值的信息，但您的第一个挑战是将这些日志移动到您的 Hadoop 集群中，以便您可以进行一些分析。
- en: '|  |'
  id: totrans-1498
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Versioning caveat emptor
  id: totrans-1499
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 版本说明
- en: This section on Flume covers release 1.4\. As with all software, there are no
    guarantees that the techniques, code, and configuration covered here will work
    out of the box with different versions of Flume. Further, Flume 1.4 requires some
    updates to get it to work with Hadoop 2—see the Flume section in the appendix
    for more details.
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 Flume 的 1.4 版本。与所有软件一样，这里介绍的技术、代码和配置并不能保证在 Flume 的不同版本上都能直接使用。此外，Flume
    1.4 版本需要一些更新才能与 Hadoop 2 版本兼容——有关更多详细信息，请参阅附录中的 Flume 部分。
- en: '|  |'
  id: totrans-1501
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Problem
  id: totrans-1502
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to push all of your production server’s system log files into HDFS.
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将所有生产服务器的系统日志文件推送到 HDFS。
- en: Solution
  id: totrans-1504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: For this technique you’ll use Flume, a data collection system, to push a Linux
    log file into HDFS.
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个技术，您将使用 Flume，一个数据收集系统，将 Linux 日志文件推送到 HDFS。
- en: Discussion
  id: totrans-1506
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Flume, at its heart, is a log file collection and distribution system, and collecting
    system logs and transporting them to HDFS is its bread and butter. Your first
    step in this technique will involve capturing all data appended to /var/log/messages
    and transporting it to HDFS. You’ll run a single Flume agent (more details on
    what that means later), which will do all this work for you.
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 在本质上是一个日志文件收集和分发系统，收集系统日志并将它们传输到 HDFS 是它的本职工作。在这个技术步骤中，您的第一步将涉及捕获追加到 /var/log/messages
    的所有数据并将其传输到 HDFS。您将运行一个单独的 Flume 代理（稍后将有更多关于这意味什么的详细信息），它将为您完成所有这些工作。
- en: 'A Flume agent needs a configuration file to tell it what to do, so let’s go
    ahead and define one for this use case:'
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 代理需要一个配置文件来告诉它要做什么，因此让我们为这个用例定义一个：
- en: '[PRE119]'
  id: totrans-1509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: We’ll examine the contents of this file shortly, but before we do that, let’s
    see Flume in action.
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后检查该文件的内容，但在做之前，让我们看看 Flume 的实际应用。
- en: '|  |'
  id: totrans-1511
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: System prerequisites
  id: totrans-1512
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 系统先决条件
- en: For the following example to work, you’ll need to make sure that you’re working
    on a host that has access to a Hadoop cluster (see the appendix if you need to
    get up one up and running), and that your `HADOOP_HOME` is configured appropriately.
    You’ll also need to have Flume downloaded and installed and have `FLUME_HOME`
    set to point to the installation directory.
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使以下示例正常工作，您需要确保您正在使用一个可以访问 Hadoop 集群的宿主机（如果您需要启动一个集群，请参阅附录），并且您的 `HADOOP_HOME`
    已经正确配置。您还需要下载并安装 Flume，并将 `FLUME_HOME` 设置为指向安装目录。
- en: '|  |'
  id: totrans-1514
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Copy the preceding file into your Flume conf directory using the filename tail-hdfs-part1.conf.
    Once you do that, you’re ready to start an instance of a Flume agent:'
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文件名 tail-hdfs-part1.conf 将前面的文件复制到您的 Flume 配置目录中。一旦完成，您就可以启动一个 Flume 代理实例：
- en: '[PRE120]'
  id: totrans-1516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'This should generate a lot of output, but ultimately you should see output
    similar to the following, indicating that everything came up OK:'
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成大量输出，但最终您应该看到类似于以下输出，表明一切正常：
- en: '[PRE121]'
  id: totrans-1518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'At this point, you should start to see some data appearing in HDFS:'
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该开始在 HDFS 中看到一些数据出现：
- en: '[PRE122]'
  id: totrans-1520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'The .tmp suffix means that Flume has the file open and will continue to write
    to it. Once it’s done, it’ll rename the file and remove the suffix:'
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
  zh: .tmp 后缀表示 Flume 已经打开了文件，并将继续向其中写入。一旦完成，它将重命名文件并移除后缀：
- en: '[PRE123]'
  id: totrans-1522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: You can `cat` this file to examine its contents—the contents should line up
    with `tail /var/log/messages`.
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `cat` 命令来检查文件内容——内容应该与 `tail /var/log/messages` 的输出一致。
- en: If you got this far, you’ve completed your first data move with Flume!
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经走到这一步，您已经使用 Flume 完成了您的第一次数据迁移！
- en: Dissecting a Flume agent
  id: totrans-1525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分析 Flume 代理
- en: 'Let’s take a few steps back and examine what you did. There were two main parts
    to your work: defining the Flume configuration file, and running the Flume agent.
    The Flume configuration file contains details on your *sources*, *channels*, and
    *sinks*. These are all Flume concepts that impact different parts of Flume’s data
    flow. [Figure 5.4](#ch05fig04) shows these concepts in action in a Flume agent.'
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下您所做的工作。您的工作有两个主要部分：定义 Flume 配置文件，并运行 Flume 代理。Flume 配置文件包含有关您的 *数据源*、*通道*
    和 *接收器* 的详细信息。这些都是影响 Flume 数据流不同部分的 Flume 概念。[图 5.4](#ch05fig04) 展示了这些概念在 Flume
    代理中的实际应用。
- en: Figure 5.4\. Flume components illustrated within the context of an agent
  id: totrans-1527
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.4\. 在代理上下文中展示的 Flume 组件
- en: '![](05fig04_alt.jpg)'
  id: totrans-1528
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig04_alt.jpg)'
- en: Let’s step through these Flume concepts and look at their purpose and how they
    work.
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解这些 Flume 概念，并查看它们的目的以及它们是如何工作的。
- en: Sources
  id: totrans-1530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据源
- en: Flume *sources* are responsible for reading data from external clients or from
    other Flume sinks. A unit of data in Flume is defined as an *event*, which is
    essentially a payload and optional set of metadata. A Flume source sends these
    events to one or more Flume channels, which deal with storage and buffering.
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: Flume *数据源* 负责从外部客户端或其他 Flume 源读取数据。Flume 中的数据单元定义为 *事件*，它本质上是一个有效载荷和可选的元数据集。Flume
    数据源将这些事件发送到一个或多个 Flume 通道，这些通道处理存储和缓冲。
- en: 'Flume has an extensive set of built-in sources, including HTTP, JMS, and RPC,
    and you encountered one of them just a few moments ago.^([[11](#ch05fn12)]) Let’s
    take a look at the source-specific configuration properties that you set:'
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 具有一系列内置的数据源，包括 HTTP、JMS 和 RPC，您在几分钟前就遇到了其中一个.^([[11](#ch05fn12)]) 让我们看看您设置的特定于源配置属性：
- en: ^(11) The full set of Flume sources can be seen at [http://flume.apache.org/FlumeUserGuide.html#flume-sources](http://flume.apache.org/FlumeUserGuide.html#flume-sources).
  id: totrans-1533
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11) Flume 的完整数据源集合可以在 [http://flume.apache.org/FlumeUserGuide.html#flume-sources](http://flume.apache.org/FlumeUserGuide.html#flume-sources)
    查看。
- en: '[PRE124]'
  id: totrans-1534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: The `exec` source allows you to execute a Unix command, and each line emitted
    in standard output is captured as an event (standard error is ignored by default).
    In the preceding example, the `tail -F` command is used to capture system messages
    as they are produced.^([[12](#ch05fn13)]) If you have more control over your files
    (if, for example, you can move them into a directory after all writes have completed),
    consider using Flume’s spooling directory source (called `spooldir`), as it offers
    reliability semantics that you don’t get with the `exec` source.
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
  zh: '`exec` 数据源允许您执行 Unix 命令，并且标准输出中发出的每一行都被捕获为一个事件（默认情况下忽略标准错误）。在上面的示例中，使用 `tail
    -F` 命令来捕获系统消息，就像它们被生成时一样.^([[12](#ch05fn13)]) 如果您对文件有更多控制权（例如，您可以在所有写入完成后将它们移动到目录中），请考虑使用
    Flume 的轮询目录数据源（称为 `spooldir`），因为它提供了 `exec` 数据源所不具备的可靠性语义。'
- en: ^(12) Use of the capital `F` in `tail` means that `tail` will continue to retry
    opening the file, which is useful in situations where the file is rotated.
  id: totrans-1536
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12) 在 `tail` 中使用大写 `F` 表示 `tail` 将继续尝试打开文件，这在文件被轮换的情况下很有用。
- en: '|  |'
  id: totrans-1537
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Only use `tail` for testing
  id: totrans-1538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 仅在测试时使用 `tail`
- en: Using `tail` for anything other than testing is discouraged.
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: 除了测试之外，不建议使用 `tail`。
- en: '|  |'
  id: totrans-1540
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Another feature highlighted in this configuration is interceptors, which allow
    you to add metadata to events. Recall that the data in HDFS was organized according
    to a timestamp—the first part was the date, and the second part was the time:'
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中强调的另一个功能是拦截器，它允许您向事件添加元数据。回想一下，HDFS 中的数据是根据时间戳组织的——第一部分是日期，第二部分是时间：
- en: '[PRE125]'
  id: totrans-1542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: You were able to do this because you modified each event with a timestamp interceptor,
    which inserted into the event header the time in milliseconds when the source
    processed the event. This timestamp was then used by the Flume HDFS sink to determine
    where an event was written.
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
  zh: 您能够做到这一点是因为您使用时间戳拦截器修改了每个事件，该拦截器将源处理事件时的毫秒时间戳插入到事件头中。然后，Flume HDFS 源使用此时间戳来确定事件被写入的位置。
- en: 'To conclude our brief dive into Flume sources, let’s summarize some of the
    interesting abilities that they provide:'
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们对 Flume 数据源的简要探讨，让我们总结一下它们提供的一些有趣功能：
- en: '*Transactional semantics*, which allow data to be reliably moved with at-least-once
    semantics. Not all data sources support this.^([[13](#ch05fn14)])'
  id: totrans-1545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事务语义*，允许数据以至少一次语义可靠地移动。并非所有数据源都支持此功能.^([[13](#ch05fn14)])'
- en: ^(13) The `exec` source used in this technique is an example of a source that
    doesn’t provide any data-reliability guarantees.
  id: totrans-1546
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（13）在此技术中使用的`exec`源是一个不提供任何数据可靠性保证的源的示例。
- en: '*Interceptors*, which provide the ability to modify or drop events. They are
    useful for annotating events with host, time, and unique identifiers, which are
    useful for deduplication.'
  id: totrans-1547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*拦截器*提供了修改或丢弃事件的能力。它们对于使用主机、时间和唯一标识符等注释事件非常有用，这些信息对于去重很有帮助。'
- en: '*Selectors*, which allow events to be fanned out or multiplexed in various
    ways. You can fan out events by replicating them to multiple channels, or you
    can route them to different channels based on event headers.'
  id: totrans-1548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*选择器*允许事件以各种方式分发或多路复用。您可以通过将事件复制到多个通道来分发事件，或者根据事件标题将它们路由到不同的通道。'
- en: Channels
  id: totrans-1549
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通道
- en: Flume *channels* provide data storage facilities inside an agent. Sources add
    events to a channel, and sinks remove events from a channel. Channels provide
    durability properties inside Flume, and you pick a channel based on which level
    of durability and throughput you need for your application.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: Flume的*通道*在代理内部提供数据存储设施。源将事件添加到通道中，而端点则从通道中移除事件。通道在Flume内部提供持久性属性，并且您可以根据应用程序所需的持久性和吞吐量级别选择通道。
- en: 'There are three channels bundled with Flume:'
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: Flume附带三个通道：
- en: '*Memory channels* store events in an in-memory queue. This is very useful for
    high-throughput data flows, but they have no durability guarantees, meaning that
    if an agent goes down, you’ll lose data.'
  id: totrans-1552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内存通道*将事件存储在内存队列中。这对于高吞吐量数据流非常有用，但它们没有持久性保证，这意味着如果代理关闭，您将丢失数据。'
- en: '*File channels* persist events to disk. The implementation uses an efficient
    write-ahead log and has strong durability properties.'
  id: totrans-1553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文件通道*将事件持久化到磁盘。该实现使用高效的预写日志，并具有强大的持久性属性。'
- en: '*JDBC channels* store events in a database. This provides the strongest durability
    and recoverability properties, but at a cost to performance.'
  id: totrans-1554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*JDBC通道*将事件存储在数据库中。这提供了最强的持久性和可恢复性属性，但会牺牲性能。'
- en: 'In the previous example, you used an in-memory channel and capped the number
    of events that it would store at 100,000\. Once the maximum number of events is
    reached in a memory channel, it will start refusing additional requests from sources
    to add more events. Depending on the type of source, this means that the source
    will either retry or drop the event (the `exec` source will drop the event):'
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，您使用了一个内存通道，并限制了其存储的事件数量为100,000。一旦内存通道中的事件达到最大数量，它将开始拒绝来自源添加更多事件的额外请求。根据源的类型，这意味着源将重试或丢弃事件（`exec`源将丢弃事件）：
- en: '[PRE126]'
  id: totrans-1556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Additional details on Flume channels can be seen at [http://flume.apache.org/FlumeUserGuide.html#flume-channels](http://flume.apache.org/FlumeUserGuide.html#flume-channels).
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Flume通道的更多详细信息，请参阅[http://flume.apache.org/FlumeUserGuide.html#flume-channels](http://flume.apache.org/FlumeUserGuide.html#flume-channels)。
- en: Sinks
  id: totrans-1558
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 端点
- en: A Flume *sink* drains events out of one or more Flume channels and will either
    forward these events to another Flume source (in a multihop flow), or handle the
    events in a sink-specific manner. There are a number of sinks built into Flume,
    including HDFS, HBase, Solr, and Elasticsearch.
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
  zh: Flume的*sink*从一个或多个Flume通道中提取事件，并将这些事件转发到另一个Flume源（在多跳流中），或者以特定于端点的方式处理这些事件。Flume内置了多个端点，包括HDFS、HBase、Solr和Elasticsearch。
- en: 'In the previous example, you configured the flow to use an HDFS sink:'
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，您已配置流使用HDFS端点：
- en: '[PRE127]'
  id: totrans-1561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: You configured the sink to write files based on a timestamp (note the `%y` and
    other timestamp aliases). You could do this because you decorated the events with
    a timestamp interceptor in the `exec` source. In fact, you can use any header
    value to determine the output location for events (for example, you can add a
    host interceptor and then write files according to which host produced the event).
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
  zh: 您已配置端点根据时间戳写入文件（注意`%y`和其他时间戳别名）。您之所以能够这样做，是因为在`exec`源中您已经用时间戳拦截器装饰了事件。实际上，您可以使用任何标题值来确定事件的输出位置（例如，您可以添加一个主机拦截器，然后根据产生事件的哪个主机来写入文件）。
- en: The HDFS sink can be configured in various ways to determine how files are rolled.
    When a sink reads the first event, it will open a new file (if one isn’t already
    open) and write to it. By default, the sink will continue to keep the file open
    and write events into it for 30 seconds, after which it will close it out. The
    rolling behavior can be changed with the properties in [table 5.5](#ch05table05).
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 溢出可以通过各种方式配置，以确定文件如何滚动。当溢出读取第一个事件时，它将打开一个新文件（如果尚未打开），并向其写入。默认情况下，溢出将保持文件打开，并将事件写入其中
    30 秒，之后将其关闭。可以通过 [表 5.5](#ch05table05) 中的属性更改滚动行为。
- en: Table 5.5\. Rollover properties for Flume’s HDFS sink
  id: totrans-1564
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 5.5\. Flume 的 HDFS 溢出滚动属性
- en: '| Property | Default value | Description |'
  id: totrans-1565
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 默认值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-1566
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| hdfs.rollInterval | 30 | Number of seconds to wait before rolling current
    file (0 = never roll based on time interval) |'
  id: totrans-1567
  prefs: []
  type: TYPE_TB
  zh: '| hdfs.rollInterval | 30 | 在滚动当前文件之前等待的秒数（0 = 不根据时间间隔进行滚动）|'
- en: '| hdfs.rollSize | 1024 | File size to trigger roll, in bytes (0 = never roll
    based on file size) |'
  id: totrans-1568
  prefs: []
  type: TYPE_TB
  zh: '| hdfs.rollSize | 1024 | 触发滚动操作的文件大小，以字节为单位（0 = 不根据文件大小进行滚动）|'
- en: '| hdfs.rollCount | 10 | Number of events written to file before it rolls (0
    = never roll based on number of events) |'
  id: totrans-1569
  prefs: []
  type: TYPE_TB
  zh: '| hdfs.rollCount | 10 | 在文件滚动之前写入文件的事件数（0 = 不根据事件数进行滚动）|'
- en: '| hdfs.idleTimeout | 0 | Timeout after which inactive files get closed (0 =
    disable automatic closing of idle files) |'
  id: totrans-1570
  prefs: []
  type: TYPE_TB
  zh: '| hdfs.idleTimeout | 0 | 在文件不活跃后关闭超时时间（0 = 禁用自动关闭不活跃文件）|'
- en: '| hdfs.batchSize | 100 | Number of events written to file before it’s flushed
    to HDFS |'
  id: totrans-1571
  prefs: []
  type: TYPE_TB
  zh: '| hdfs.batchSize | 100 | 在将事件刷新到 HDFS 之前写入文件的事件数|'
- en: '|  |'
  id: totrans-1572
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Default settings for the HDFS sink
  id: totrans-1573
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: HDFS 溢出的默认设置
- en: The default HDFS sink settings shouldn’t be used in production, as they’ll result
    in a large number of potentially small files. It’s recommended that you either
    bump up the values or use a downstream compaction job to coalesce these small
    files.
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中不应使用 HDFS 溢出的默认设置，因为它们会导致大量可能很小的文件。建议您提高这些值或使用下游压缩作业将这些小文件合并。
- en: '|  |'
  id: totrans-1575
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The HDFS sink allows you to specify how events are serialized when writing
    files. By default, they’re serialized in text format, without any headers added
    by interceptors. If, for example, you want to write data in Avro, which also includes
    event headers, you can use the serializer configuration to do this. In doing so,
    you can also specify a Hadoop compression codec that Avro uses internally to compress
    data:'
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 溢出允许您指定在写入文件时事件如何序列化。默认情况下，它们以文本格式序列化，不添加任何拦截器添加的标题。例如，如果您想以 Avro 格式写入数据，它还包括事件标题，您可以使用序列化配置来完成此操作。这样做时，您还可以指定
    Avro 内部用于压缩数据的 Hadoop 压缩编解码器：
- en: '[PRE128]'
  id: totrans-1577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Summary
  id: totrans-1578
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Reliability in Flume is determined by the type of channel you use, whether your
    data sources have the ability to retransmit events, and whether you multiplex
    events to multiple sources to mitigate against unrecoverable node failure. In
    this technique, the memory channel and exec source were used, but neither provides
    reliability in the face of failure. One way to add that reliability would be to
    replace the exec source with a spooling directory source and replace the memory
    channel with a disk channel.
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 的可靠性取决于您使用的通道类型，您的数据源是否有重新传输事件的能力，以及您是否将事件多路复用到多个源以减轻不可恢复节点故障的影响。在此技术中，使用了内存通道和
    exec 源，但它们在面临故障时都不提供可靠性。增加这种可靠性的方法之一是将 exec 源替换为轮转目录源，并将内存通道替换为磁盘通道。
- en: You’ve used Flume on a single machine running a single agent with a single source,
    channel, and sink. But Flume can support a fully distributed setup where you have
    agents running on multiple hosts with multiple agent hops between the source and
    final destinations. [Figure 5.5](#ch05fig05) shows one example of how Flume can
    function in a distributed environment.
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在单个机器上使用单个代理、单个源、单个通道和单个溢出运行了 Flume。但 Flume 可以支持完全分布式的设置，其中代理在多个主机上运行，源和最终目的地之间存在多个代理跳转。[图
    5.5](#ch05fig05) 展示了 Flume 在分布式环境中如何工作的一个示例。
- en: Figure 5.5\. A Flume setup that uses load balancing and fan-in to move log4j
    logs into HDFS
  id: totrans-1581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.5\. 使用负载均衡和扇入将 log4j 日志移动到 HDFS 的 Flume 设置
- en: '![](05fig05_alt.jpg)'
  id: totrans-1582
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig05_alt.jpg)'
- en: The goal of this technique is to move data into HDFS. Flume, however, can support
    various data sinks, including HBase, a file roll, Elasticsearch, and Solr. Using
    Flume to write to Elasticsearch or Solr enables a powerful near-real-time indexing
    strategy.
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术的目标是移动数据到 HDFS。然而，Flume 支持各种数据接收器，包括 HBase、文件滚动、Elasticsearch 和 Solr。使用 Flume
    将数据写入 Elasticsearch 或 Solr 可以实现强大的近实时索引策略。
- en: Flume, then, is a very powerful data movement project, which can easily support
    moving your data into HDFS as well as many other locations. It moves data continuously
    and supports various levels of resiliency to work around failures in your systems.
    And it’s a simple system to configure and run.
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Flume 是一个非常强大的数据移动项目，可以轻松支持将数据移动到 HDFS 以及许多其他位置。它持续移动数据，并支持各种级别的弹性以应对系统中的故障。而且，它是一个简单易配置和运行的系统。
- en: One area that Flume isn’t really optimized for is working with binary data.
    It can support moving binary data, but it loads the entire binary event into memory,
    so moving files that are gigabytes in size or larger won’t work. The next technique
    looks at how such data can be moved into HDFS.
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 并非特别优化处理二进制数据。它可以支持移动二进制数据，但会将整个二进制事件加载到内存中，因此对于大小为吉字节或更大的文件，移动操作将无法进行。接下来的技术将探讨如何将此类数据移动到
    HDFS。
- en: Technique 40 An automated mechanism to copy files into HDFS
  id: totrans-1586
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇 40：自动将文件复制到 HDFS 的机制
- en: You’ve learned how to use log-collecting tools like Flume to automate moving
    data into HDFS. But these tools don’t support working with semistructured or binary
    data out of the box. In this technique, we’ll look how to automate moving such
    files into HDFS.
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了如何使用 Flume 等日志收集工具自动化地将数据移动到 HDFS。但是，这些工具并不支持直接处理半结构化或二进制数据。在这个技术中，我们将探讨如何自动化将此类文件移动到
    HDFS。
- en: Production networks typically have network silos where your Hadoop clusters
    are segmented away from other production applications. In such cases, it’s possible
    that your Hadoop cluster won’t be able to pull data from other data sources, leaving
    you with no option but to push data into Hadoop.
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 生产网络通常有网络隔离，你的 Hadoop 集群被分割在其他生产应用之外。在这种情况下，你的 Hadoop 集群可能无法从其他数据源拉取数据，你将别无选择，只能将数据推送到
    Hadoop。
- en: You need a mechanism to automate the process of copying files of any format
    into HDFS, similar to the Linux tool rsync. The mechanism should be able to compress
    files written in HDFS and offer a way to dynamically determine the HDFS destination
    for data-partitioning purposes.
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个机制来自动化将任何格式的文件复制到 HDFS 的过程，类似于 Linux 工具 rsync。该机制应能够压缩 HDFS 中写入的文件，并提供一种动态确定数据分区目的地的
    HDFS 目标的方法。
- en: Existing file transportation mechanisms such as Flume, Scribe, and Chukwa are
    geared toward supporting log files. What if you have different formats for your
    files, such as semistructured or binary? If the files were siloed in a way that
    the Hadoop slave nodes couldn’t directly access, then you couldn’t use Oozie to
    help with file ingress either.
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的文件传输机制，如 Flume、Scribe 和 Chukwa，主要是为了支持日志文件。如果你的文件格式不同，比如半结构化或二进制格式，怎么办？如果文件以某种方式隔离，使得
    Hadoop 从节点无法直接访问，那么你也无法使用 Oozie 来帮助进行文件导入。
- en: Problem
  id: totrans-1591
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You need to automate the process by which files on remote servers are copied
    into HDFS.
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要自动化远程服务器上的文件复制到 HDFS 的过程。
- en: Solution
  id: totrans-1593
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: The open source HDFS File Slurper project can copy files of any format into
    and out of HDFS. This technique covers how it can be configured and used to copy
    data into HDFS.
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
  zh: 开源 HDFS File Slurper 项目可以将任何格式的文件复制到和从 HDFS 中。本技术涵盖了如何配置和使用它来复制数据到 HDFS。
- en: Discussion
  id: totrans-1595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: You can use the HDFS File Slurper project (which I wrote) to assist with your
    automation ([https://github.com/alexholmes/hdfs-file-slurper](https://github.com/alexholmes/hdfs-file-slurper)).
    The HDFS File Slurper is a simple utility that supports copying files from a local
    directory into HDFS and vice versa.
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用我编写的 HDFS File Slurper 项目（[https://github.com/alexholmes/hdfs-file-slurper](https://github.com/alexholmes/hdfs-file-slurper)）来帮助你自动化。HDFS
    File Slurper 是一个简单的实用工具，支持将文件从本地目录复制到 HDFS，反之亦然。
- en: '[Figure 5.6](#ch05fig06) provides a high-level overview of the Slurper (my
    nickname for the project), with an example of how you can use it to copy files.
    The Slurper reads any files that exist in a source directory and optionally consults
    with a script to determine the file placement in the destination directory. It
    then writes the file to the destination, after which there’s an optional verification
    step. Finally, the Slurper moves the source file to a completed folder upon successful
    completion of all of the previous steps.'
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.6](#ch05fig06) 提供了 Slurper（我为这个项目取的昵称）的高级概述，以及如何使用它来复制文件的示例。Slurper 读取源目录中存在的任何文件，并可选地与脚本协商以确定目标目录中的文件位置。然后，它将文件写入目标位置，之后有一个可选的验证步骤。最后，在所有前面的步骤成功完成后，Slurper
    将源文件移动到完成文件夹。'
- en: Figure 5.6\. HDFS File Slurper data flow for copying files
  id: totrans-1598
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.6\. HDFS File Slurper 文件复制数据流
- en: '![](05fig06_alt.jpg)'
  id: totrans-1599
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig06_alt.jpg)'
- en: 'With this technique, there are a few challenges you need to make sure to address:'
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，有几个挑战您需要确保解决：
- en: How do you effectively partition your writes to HDFS so that you don’t lump
    everything into a single directory?
  id: totrans-1601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何有效地分区你的 HDFS 写入，以便你不把所有东西都堆叠到单个目录中？
- en: How do you determine that your data in HDFS is ready for processing (to avoid
    reading files that are mid-copy)?
  id: totrans-1602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何确定 HDFS 中的数据已经准备好处理（以避免读取正在复制中的文件）？
- en: How do you automate regular execution of your utility?
  id: totrans-1603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何自动化常规执行你的实用工具？
- en: 'Your first step is to download the latest HDFS File Slurper tarball from [https://github.com/alexholmes/hdfs-file-slurper/releases](https://github.com/alexholmes/hdfs-file-slurper/releases)
    and install it on a host that has access to both a Hadoop cluster and a local
    install of Hadoop:'
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一步是从 [https://github.com/alexholmes/hdfs-file-slurper/releases](https://github.com/alexholmes/hdfs-file-slurper/releases)
    下载最新的 HDFS File Slurper tarball，并将其安装在一个可以访问 Hadoop 集群和本地 Hadoop 安装的宿主机上：
- en: '[PRE129]'
  id: totrans-1605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: Configuration
  id: totrans-1606
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 配置
- en: 'Before you can run the code, you’ll need to edit /usr/local/hdfs-slurper/conf/slurper-env.sh
    and set the location of the `hadoop` script. The following code is an example
    of what slurper-eng.sh file looks like if you followed the Hadoop installation
    instructions in the appendix:'
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行代码之前，您需要编辑 /usr/local/hdfs-slurper/conf/slurper-env.sh 并设置 `hadoop` 脚本的位置。以下代码是如果遵循附录中的
    Hadoop 安装说明，slurper-env.sh 文件可能看起来像的示例：
- en: '[PRE130]'
  id: totrans-1608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'The Slurper comes bundled with a /usr/local/hdfs-slurper/conf/slurper.conf
    file, which contains details on the source and destination directories, along
    with other options. The file contains the following default settings, which you
    can change:'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: Slurper 包含一个 /usr/local/hdfs-slurper/conf/slurper.conf 文件，其中包含源目录和目标目录的详细信息，以及其他选项。该文件包含以下默认设置，您可以根据需要更改：
- en: '![](206fig01_alt.jpg)'
  id: totrans-1610
  prefs: []
  type: TYPE_IMG
  zh: '![](206fig01_alt.jpg)'
- en: 'Let’s take a closer look at these settings:'
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些设置：
- en: '**`DATASOURCE_NAME`** —This specifies the name for the data being transferred.
    It’s used for the log filename when launched via the Linux `init` daemon management
    system, which we’ll cover shortly.'
  id: totrans-1612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`DATASOURCE_NAME`** — 这指定了正在传输的数据的名称。当通过 Linux `init` 守护进程管理系统启动时，它用于日志文件名，我们将在稍后介绍。'
- en: '**`SRC_DIR`** —This specifies the source directory. Any files moved into here
    are automatically copied to the destination directory (with an intermediary hop
    to the staging directory).'
  id: totrans-1613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`SRC_DIR`** — 这指定了源目录。任何移动到这里的文件都会自动复制到目标目录（通过中间的暂存目录进行跳转）。'
- en: '**`WORK_DIR`** —This is the work directory. Files from the source directory
    are moved here before the copy to the destination starts.'
  id: totrans-1614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`WORK_DIR`** — 这是工作目录。在开始复制到目标之前，源目录的文件会被移动到这里。'
- en: '**`COMPLETE_DIR`** —This specifies the complete directory. After the copy has
    completed, the file is moved from the work directory into this directory. Alternatively,
    the `--remove-after-copy` option can be used to delete the source file, in which
    case the `--complete-dir` option shouldn’t be supplied.'
  id: totrans-1615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`COMPLETE_DIR`** — 这指定了完整目录。复制完成后，文件将从工作目录移动到这个目录。或者，可以使用 `--remove-after-copy`
    选项来删除源文件，在这种情况下不应提供 `--complete-dir` 选项。'
- en: '**`ERROR_DIR`** —This is the error directory. Any errors encountered during
    the copy result in the source file being moved into this directory.'
  id: totrans-1616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`ERROR_DIR`** — 这是错误目录。在复制过程中遇到的任何错误都会导致源文件被移动到这个目录。'
- en: '**`DEST_DIR`** —This sets the final destination directory for source files.'
  id: totrans-1617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`DEST_DIR`** — 这设置源文件的最终目标目录。'
- en: '**`DEST_STAGING_DIR`** —This specifies the staging directory. A file is first
    copied into this directory, and once the copy succeeds, the Slurper moves the
    copy into the destination to avoid the possibility of the destination directory
    containing partially written files (in the event of failure).'
  id: totrans-1618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`DEST_STAGING_DIR`** — 这指定了临时目录。文件首先被复制到这个目录，一旦复制成功，Slurper 就会将副本移动到目标目录，以避免目标目录可能包含部分写入的文件（在失败的情况下）。'
- en: You’ll notice that all of the directory names are HDFS URIs. HDFS distinguishes
    between different filesystems in this way. The file:/ URI denotes a path on the
    local filesystem, and the hdfs:/ URI denotes a path in HDFS. In fact, the Slurper
    supports any Hadoop filesystem, as long as you configure Hadoop to use it.
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到所有目录名都是 HDFS URI。HDFS 以这种方式区分不同的文件系统。file:/ URI 表示本地文件系统上的路径，而 hdfs:/ URI
    表示 HDFS 中的路径。实际上，Slurper 支持任何 Hadoop 文件系统，只要配置 Hadoop 使用它。
- en: Running
  id: totrans-1620
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行
- en: 'Let’s create a local directory called /tmp/slurper/in, write an empty file
    into it, and run the Slurper:'
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 /tmp/slurper/in 的本地目录，向其中写入一个空文件，然后运行 Slurper：
- en: '[PRE131]'
  id: totrans-1622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: A key feature in the Slurper’s design is that it doesn’t work with partially
    written files. Files must be atomically moved into the source directory (file
    moves in both the Linux and HDFS filesystems are atomic).^([[14](#ch05fn15)])
    Alternatively, you can write to a filename that starts with a period (.), which
    is ignored by the Slurper, and after the file write completes, you can rename
    the file to a name without the period prefix.
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
  zh: Slurper 设计中的一个关键特性是它不与部分写入的文件一起工作。文件必须原子性地移动到源目录中（Linux 和 HDFS 文件系统中的文件移动都是原子的）。^([[14](#ch05fn15)])
    或者，你可以写入一个以点（.）开头的文件名，Slurper 会忽略它，文件写入完成后，你可以将文件重命名为不带点前缀的名称。
- en: ^(14) Moving files is atomic only if both the source and destination are on
    the same partition. In other words, moving a file from an NFS mount to a local
    disk results in a copy, which isn’t atomic.
  id: totrans-1624
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(14) 只有当源和目标都在同一分区上时，移动文件才是原子的。换句话说，将文件从 NFS 挂载移动到本地磁盘会导致复制，这不是原子的。
- en: Be aware that copying multiple files with the same filename will result in the
    destination being overwritten—the onus is on the user to make sure that files
    are unique to prevent this from happening.
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用相同文件名的多个文件进行复制会导致目标被覆盖——确保文件唯一性的责任在于用户，以防止这种情况发生。
- en: Dynamic destination paths
  id: totrans-1626
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动态目标路径
- en: The previous approach works well if you’re moving a small number of files into
    HDFS on a daily basis. But if you’re dealing with a large volume of files, you’ll
    want to think about partitioning them into separate directories. This has the
    benefit of giving you more fine-grained control over the input data for your MapReduce
    jobs, as well as helping with the overall organization of your data in the filesystem
    (you wouldn’t want all the files on your computer to reside in a single flat directory).
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你每天只是将少量文件移动到 HDFS 上，那么之前的方法效果很好。但是，如果你处理的是大量文件，你可能需要考虑将它们分区到不同的目录中。这样做的好处是，你可以对
    MapReduce 作业的输入数据有更细粒度的控制，同时也有助于在文件系统中对数据进行整体组织（你不会希望电脑上的所有文件都位于单个扁平目录中）。
- en: How can you have more dynamic control over the destination directory and the
    filename that the Slurper uses? The Slurper configuration file has a `SCRIPT`
    option (which is mutually exclusive of the `DEST_DIR` option), where you can specify
    a script that provides dynamic mapping of the source files to destination files.
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何能够对 Slurper 使用的目标目录和文件名有更多的动态控制？Slurper 配置文件有一个 `SCRIPT` 选项（与 `DEST_DIR`
    选项互斥），你可以指定一个脚本，该脚本提供源文件到目标文件的动态映射。
- en: 'Let’s assume that the files you’re working with contain a date in the filename,
    and you’ve decided that you want to organize your data in HDFS by date. You can
    write a script to perform this mapping activity. The following example is a Python
    script that does this:'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理的文件包含在文件名中的日期，并且你已经决定想要按照日期在 HDFS 中组织你的数据。你可以编写一个脚本来执行这个映射活动。以下是一个执行此操作的
    Python 脚本示例：
- en: '[PRE132]'
  id: totrans-1630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Now you can update /usr/local/hdfs-slurper/conf/slurper.conf, set `SCRIPT`,
    and comment out `DEST_DIR`, which results in the following entry in the file:'
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以更新 /usr/local/hdfs-slurper/conf/slurper.conf，设置 `SCRIPT`，并注释掉 `DEST_DIR`，这将导致文件中以下条目：
- en: '[PRE133]'
  id: totrans-1632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'If you run the Slurper again, you’ll notice that the destination path is now
    partitioned by date by the Python script:'
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次运行 Slurper，你会注意到目标路径现在已经被 Python 脚本按日期分区：
- en: '[PRE134]'
  id: totrans-1634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Compression and verification
  id: totrans-1635
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 压缩和验证
- en: What if you want to compress the output file in HDFS and also verify that the
    copy is correct? You’ll need to use the `COMPRESSION_CODEC` option, whose value
    is a class that implements the `CompressionCodec` interface. If your compression
    codec is LZO or LZOP, you can also add a `CREATE_LZO_INDEX` option so that LZOP
    indexes are created. If you don’t know what this means, take a look at the LZO
    coverage in [chapter 4](kindle_split_014.html#ch04).
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想压缩HDFS中的输出文件并验证复制是否正确，你需要使用`COMPRESSION_CODEC`选项，其值是一个实现`CompressionCodec`接口的类。如果你的压缩编解码器是LZO或LZOP，你还可以添加一个`CREATE_LZO_INDEX`选项，以便创建LZOP索引。如果你不知道这是什么意思，请参阅第4章中的LZO覆盖[章节4](kindle_split_014.html#ch04)。
- en: Also available is a verification feature, which rereads the destination file
    after the copy has completed and ensures that the checksum of the destination
    file matches the source file. This results in longer processing times, but it
    adds an additional level of assurance that the copy was successful.
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个验证功能，它在复制完成后重新读取目标文件，并确保目标文件的校验和与源文件匹配。这会导致处理时间更长，但它为复制成功添加了一个额外的保证级别。
- en: 'The following configuration fragment shows the LZOP codec, LZO indexing, and
    file verification enabled:'
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的配置片段显示了启用了LZOP编解码器、LZO索引和文件验证：
- en: '[PRE135]'
  id: totrans-1639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Let’s run the Slurper again:'
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次运行Slurper：
- en: '[PRE136]'
  id: totrans-1641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Continuous operation
  id: totrans-1642
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 持续运行
- en: Now that you have the basic mechanics in place, your final step is to run the
    tool as a daemon so that it continuously looks for files to transfer. To do this,
    you can use a script called bin/slurper-inittab.sh, which is designed to work
    with the `inittab` respawn.^([[15](#ch05fn16)])
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经设置了基本机制，你的最后一步是将工具作为守护进程运行，以便它持续查找要传输的文件。为此，你可以使用名为bin/slurper-inittab.sh的脚本，该脚本旨在与`inittab`重生一起工作.^([[15](#ch05fn16)])
- en: '^(15) Inittab is a Linux process-management tool that you can configure to
    supervise and restart a process if it goes down. See INITTAB(5) in the Linux System
    Administrator’s Manual: [http://unixhelp.ed.ac.uk/CGI/mancgi?inittab+5](http://unixhelp.ed.ac.uk/CGI/mancgi?inittab+5).'
  id: totrans-1644
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([15]) Inittab是一个Linux进程管理工具，你可以配置它来监督和重启一个进程，如果它崩溃。请参阅Linux系统管理员手册中的INITTAB(5)：[http://unixhelp.ed.ac.uk/CGI/mancgi?inittab+5](http://unixhelp.ed.ac.uk/CGI/mancgi?inittab+5)。
- en: This script won’t create a PID file or perform a `nohup`—neither makes sense
    in the context of respawn, because inittab is managing the process. It uses the
    `DATASOURCE_NAME` configuration value to create the log filename. This means that
    multiple Slurper instances can all be launched with different config files logging
    to separate log files.
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本不会创建PID文件或执行`nohup`——在重生上下文中，这两者都没有意义，因为inittab正在管理进程。它使用`DATASOURCE_NAME`配置值来创建日志文件名。这意味着多个Slurper实例都可以使用不同的配置文件启动，并将日志记录到不同的日志文件中。
- en: Summary
  id: totrans-1646
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The Slurper is a handy tool for data ingress from a local filesystem to HDFS.
    It also supports data egress by copying from HDFS to the local filesystem. It
    can be useful in situations where MapReduce doesn’t have access to the filesystem
    and the files being transferred are in a form that doesn’t work with tools such
    as Flume.
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: Slurper是一个方便的工具，可以从本地文件系统到HDFS进行数据导入。它还支持通过从HDFS复制到本地文件系统进行数据导出。在MapReduce无法访问文件系统且传输的文件格式不适合Flume等工具的情况下，它可能很有用。
- en: Now let’s look at automated pulls for situations where MapReduce or HDFS has
    access to your data sources.
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在MapReduce或HDFS可以访问您的数据源的情况下，自动提取的情况。
- en: Technique 41 Scheduling regular ingress activities with Oozie
  id: totrans-1649
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧41 使用Oozie安排定期的数据导入活动
- en: If your data is sitting on a filesystem, web server, or any other system accessible
    from your Hadoop cluster, you’ll need a way to periodically pull that data into
    Hadoop. Tools exist to help with pushing log files and pulling from databases
    (which we’ll cover in this chapter), but if you need to interface with some other
    system, it’s likely you’ll need to handle the data ingress process yourself.
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据位于文件系统、Web服务器或任何可以从你的Hadoop集群访问的系统上，你需要一种方法来定期将数据拉入Hadoop。存在一些工具可以帮助推送日志文件和从数据库中提取数据（我们将在本章中介绍），但如果你需要与某些其他系统接口，你可能需要自己处理数据导入过程。
- en: '|  |'
  id: totrans-1651
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Oozie versions
  id: totrans-1652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Oozie版本
- en: This technique covers using Oozie version 4.0.0.
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术涵盖了使用Oozie版本4.0.0。
- en: '|  |'
  id: totrans-1654
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'There are two parts to this data ingress process: how you import data from
    another system into Hadoop, and how you regularly schedule the data transfer.'
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据导入过程有两个部分：如何将数据从另一个系统导入到Hadoop，以及如何定期安排数据传输。
- en: Problem
  id: totrans-1656
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to automate a daily task to download content from an HTTP server into
    HDFS.
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
  zh: 你想自动化一个每日任务，从HTTP服务器下载内容到HDFS。
- en: Solution
  id: totrans-1658
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Oozie can be used to move data into HDFS, and it can also be used to execute
    post-ingress activities such as launching a MapReduce job to process the ingested
    data. Now an Apache project, Oozie started life inside Yahoo1!. It’s a Hadoop
    workflow engine that manages data processing activities. Oozie also has a coordinator
    engine that can start workflows based on data and time triggers.
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: Oozie可以用来将数据移动到HDFS，它也可以用来执行后入活动，例如启动一个MapReduce作业来处理导入的数据。现在是一个Apache项目，Oozie最初诞生于Yahoo1！它是一个Hadoop工作流引擎，用于管理数据处理活动。Oozie还有一个协调器引擎，可以根据数据和时间触发器启动工作流。
- en: Discussion
  id: totrans-1660
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: In this technique, you’ll perform a download from a number of URLs every 24
    hours, using Oozie to manage the workflow and scheduling. The flow for this technique
    is shown in [figure 5.7](#ch05fig07). You’ll use Oozie’s triggering capabilities
    to kick off a MapReduce job every 24 hours. The appendix contains Oozie installation
    instructions.
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你将每24小时从多个URL进行下载，使用Oozie来管理工作流和调度。这个技术的流程在[图5.7](#ch05fig07)中展示。你将使用Oozie的触发能力每24小时启动一个MapReduce作业。附录包含Oozie安装说明。
- en: Figure 5.7\. Data flow for this Oozie technique
  id: totrans-1662
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7.本Oozie技术的数据流
- en: '![](05fig07.jpg)'
  id: totrans-1663
  prefs: []
  type: TYPE_IMG
  zh: '![05fig07.jpg](05fig07.jpg)'
- en: The first step is to look at the coordinator XML configuration file. This file
    is used by Oozie’s coordination engine to determine when it should kick off a
    workflow. Oozie uses a template engine and expression language to perform parameterization,
    as you’ll see in the following code. Create a file called coordinator.xml with
    the following content:^([[16](#ch05fn17)])
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是查看协调器XML配置文件。这个文件由Oozie的协调引擎用来确定何时启动工作流。Oozie使用模板引擎和表达式语言来进行参数化，正如你将在下面的代码中看到的。创建一个名为coordinator.xml的文件，内容如下：^([[16](#ch05fn17)])
- en: '^(16) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/coordinator.xml](https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/coordinator.xml).'
  id: totrans-1665
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（16）GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/coordinator.xml](https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/coordinator.xml).
- en: Listing 5.1\. Using a template engine to perform parameterization with Oozie
  id: totrans-1666
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1.使用模板引擎在Oozie中进行参数化
- en: '![](ch05ex01-0.jpg)'
  id: totrans-1667
  prefs: []
  type: TYPE_IMG
  zh: '![ch05ex01-0.jpg](ch05ex01-0.jpg)'
- en: '![](ch05ex01-1.jpg)'
  id: totrans-1668
  prefs: []
  type: TYPE_IMG
  zh: '![ch05ex01-1.jpg](ch05ex01-1.jpg)'
- en: What can be confusing about Oozie’s coordinator is that the start and end times
    don’t relate to the actual times when the jobs will be executed. Rather, they
    refer to the dates that will be created (“materialized”) for each workflow execution.
    This is useful in situations where you have data being generated at periodic intervals
    and you want to be able to go back in time to a certain point and perform some
    work on that data. In this example, you don’t want to go back in time, but instead
    want to schedule a job every 24 hours going forward. But you won’t want to wait
    until the next day, so you can set the start date to be yesterday, and the end
    date to be some far-off date in the future.
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
  zh: Oozie的协调器可能会让人感到困惑的是，开始和结束时间并不与作业实际执行的时间相关。相反，它们指的是为每个工作流执行创建（“实现”）的日期。这在有数据以周期性间隔生成，并且你想能够回到过去某个时间点对那些数据进行一些工作的场景中很有用。在这个例子中，你不想回到过去，而是想每24小时调度一个作业。但你不想等到第二天，因此你可以将开始日期设置为昨天，结束日期设置为未来的某个遥远日期。
- en: Next you need to define the actual workflow, which will be executed for every
    interval in the past, and, going forward, when the wall clock reaches an interval.
    To do this, create a file called workflow.xml with the content shown in the next
    listing.^([[17](#ch05fn18)])
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要定义实际的流程，这个流程将在过去的每个间隔执行，并且，当系统时钟达到一个间隔时，也将向前执行。为此，创建一个名为workflow.xml的文件，内容如下一节所示。^([[17](#ch05fn18)])
- en: '^(17) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/workflow.xml](https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/workflow.xml).'
  id: totrans-1671
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（17）GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/workflow.xml](https://github.com/alexholmes/hiped2/blob/master/src/main/oozie/http-download/workflow.xml).
- en: Listing 5.2\. Defining the past workflow using Oozie’s coordinator
  id: totrans-1672
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.2.使用Oozie协调器定义过去的工作流
- en: '![](ch05ex02-0.jpg)'
  id: totrans-1673
  prefs: []
  type: TYPE_IMG
  zh: '![ch05ex02-0.jpg](ch05ex02-0.jpg)'
- en: '![](ch05ex02-1.jpg)'
  id: totrans-1674
  prefs: []
  type: TYPE_IMG
  zh: '![ch05ex02-1.jpg](ch05ex02-1.jpg)'
- en: '|  |'
  id: totrans-1675
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Working with the new MapReduce APIs in Oozie
  id: totrans-1676
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在Oozie中使用新的MapReduce API
- en: 'By default, Oozie expects that your map and reduce classes use the “old” MapReduce
    APIs. If you want to use the “new” APIs, you need to specify additional properties:'
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Oozie 预期你的 map 和 reduce 类使用的是“旧”的 MapReduce API。如果你想使用“新”的 API，你需要指定额外的属性：
- en: '[PRE137]'
  id: totrans-1678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '|  |'
  id: totrans-1679
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The last step is to define your properties file, which specifies how to get
    to HDFS, MapReduce, and the location of the two XML files previously identified
    in HDFS. Create a file called job.properties, as shown in the following code:'
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是定义你的属性文件，该文件指定了如何访问 HDFS、MapReduce 以及之前在 HDFS 中确定的两个 XML 文件的位置。创建一个名为 job.properties
    的文件，如下面的代码所示：
- en: '![](213fig01_alt.jpg)'
  id: totrans-1681
  prefs: []
  type: TYPE_IMG
  zh: '![](213fig01_alt.jpg)'
- en: '|  |'
  id: totrans-1682
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: JobTracker property for different Hadoop versions
  id: totrans-1683
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不同 Hadoop 版本的 JobTracker 属性
- en: If you’re targeting Hadoop 1, you should use the JobTracker RPC port in the
    `jobTracker` property (the default is 8021). Otherwise use the YARN ResourceManager
    RPC port (the default is 8032).
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你针对的是 Hadoop 1，你应该在 `jobTracker` 属性中使用 JobTracker RPC 端口（默认为 8021）。否则使用 YARN
    ResourceManager RPC 端口（默认为 8032）。
- en: '|  |'
  id: totrans-1685
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'In the previous snippet, the location in HDFS indicates where the coordinator.xml
    and workflow.xml files that you wrote earlier in this chapter are. Now you need
    to copy the XML files, your input file, and the JAR file containing your MapReduce
    code into HDFS:'
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的片段中，HDFS 中的位置指示了你在本章前面编写的 coordinator.xml 和 workflow.xml 文件的位置。现在你需要将 XML
    文件、你的输入文件以及包含你的 MapReduce 代码的 JAR 文件复制到 HDFS：
- en: '[PRE138]'
  id: totrans-1687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Finally, run your job in Oozie:'
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 Oozie 中运行你的作业：
- en: '[PRE139]'
  id: totrans-1689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'You can use the job ID to get some information about the job:'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用作业 ID 获取一些关于作业的信息：
- en: '[PRE140]'
  id: totrans-1691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: This output tells you that the job resulted in one run, and you can see the
    nominal time for the run. The overall state is `RUNNING`, which means that the
    job is waiting for the next interval to occur. When the overall job has completed
    (after the end date has been reached), the status will transition to `SUCCEEDED`.
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出告诉你作业只运行了一次，你可以看到运行的正常时间。总体状态是 `RUNNING`，这意味着作业正在等待下一个间隔发生。当总体作业完成（在结束日期到达后），状态将过渡到
    `SUCCEEDED`。
- en: 'You can confirm that there is an output directory in HDFS corresponding to
    the materialized date:'
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以确认在 HDFS 中存在与物化日期对应的输出目录：
- en: '[PRE141]'
  id: totrans-1694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'As long as the job is running, it’ll continue to execute until the end date,
    which in this example has been set as the year 2026\. If you wish to stop the
    job, use the `-suspend` option:'
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
  zh: 只要作业在运行，它将继续执行，直到结束日期，在这个例子中已经设置为 2026 年。如果你想停止作业，请使用 `-suspend` 选项：
- en: '[PRE142]'
  id: totrans-1696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Oozie also has the ability to resume suspended jobs, as well as to kill a workflow,
    using the `-resume` and `-kill` options, respectively.
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
  zh: Oozie 还具有恢复挂起作业以及使用 `-resume` 和 `-kill` 选项分别终止工作流的能力。
- en: Summary
  id: totrans-1698
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: I showed you one example of the use of the Oozie coordinator, which offers cron-like
    capabilities to launch periodic Oozie workflows. The Oozie coordinator can also
    be used to trigger a workflow based on data availability (if no data is available,
    the workflow isn’t triggered). For example, if you had an external process, or
    even MapReduce generating data on a regular basis, you could use Oozie’s data-driven
    coordinator to trigger a workflow, which could aggregate or process that data.
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 我向你展示了 Oozie 协调器的一个使用示例，它提供了类似于 cron 的能力来启动定期的 Oozie 工作流。Oozie 协调器还可以根据数据可用性来触发工作流（如果没有数据可用，则不会触发工作流）。例如，如果你有一个外部进程，或者甚至是定期生成数据的
    MapReduce，你可以使用 Oozie 的数据驱动协调器来触发工作流，该工作流可以聚合或处理这些数据。
- en: In this section, we covered three automated mechanisms that can be used for
    data ingress purposes. The first technique covered Flume, a powerful tool for
    shipping your log data into Hadoop, and the second technique looked at the HDFS
    File Slurper, which automates the process of pushing data into HDFS. The final
    technique looked at how Oozie could be used to periodically launch a MapReduce
    job to pull data into HDFS or MapReduce.
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了三种可用于数据导入目的的自动化机制。第一种技术介绍了 Flume，这是一个将日志数据传输到 Hadoop 的强大工具，第二种技术探讨了
    HDFS 文件 Slurper，它自动化了将数据推入 HDFS 的过程。最后一种技术探讨了如何使用 Oozie 定期启动 MapReduce 作业以将数据拉入
    HDFS 或 MapReduce。
- en: At this point in our exploration of data ingress, we’ve looked at pushing log
    files, pushing files from regular filesystems, and pulling files from web servers.
    Another data source that will be of interest to most organizations is relational
    data sitting in OLTP databases. Next up is a look at how you can access that data.
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索数据导入的过程中，我们已经探讨了推送日志文件、从常规文件系统中推送文件以及从Web服务器中拉取文件。对大多数组织来说，另一个有趣的数据源是位于OLTP数据库中的关系型数据。接下来，我们将看看如何访问这些数据。
- en: 5.2.3\. Databases
  id: totrans-1702
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3. 数据库
- en: Most organizations’ crucial data exists across a number of OLTP databases. The
    data stored in these databases contains information about users, products, and
    a host of other useful items. If you wanted to analyze this data, the traditional
    way to do so would be to periodically copy that data into an OLAP data warehouse.
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数组织的核心数据存在于多个OLTP数据库中。这些数据库中存储的数据包含有关用户、产品和许多其他有用信息。如果您想分析这些数据，传统的做法是定期将数据复制到一个OLAP数据仓库中。
- en: 'Hadoop has emerged to play two roles in this space: as a replacement to data
    warehouses, and as a bridge between structured and unstructured data and data
    warehouses. [Figure 5.8](#ch05fig08) shows the first role, where Hadoop is used
    as a large-scale joining and aggregation mechanism prior to exporting the data
    to an OLAP system (a commonly used platform for business intelligence applications).'
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop在这个领域扮演了两个角色：作为数据仓库的替代品，以及作为结构化和非结构化数据以及数据仓库之间的桥梁。[图5.8](#ch05fig08)显示了第一个角色，其中Hadoop被用作在将数据导出到OLAP系统（商业智能应用的常用平台）之前的大规模连接和聚合机制。
- en: Figure 5.8\. Using Hadoop for data ingress, joining, and egress to OLAP
  id: totrans-1705
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8：使用Hadoop进行数据导入、连接和导出到OLAP
- en: '![](05fig08.jpg)'
  id: totrans-1706
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig08.jpg)'
- en: Facebook is an example of an organization that has successfully utilized Hadoop
    and Hive as an OLAP platform for working with petabytes of data. [Figure 5.9](#ch05fig09)
    shows an architecture similar to that of Facebook’s. This architecture also includes
    a feedback loop into the OLTP system, which can be used to push discoveries made
    in Hadoop, such as recommendations for users.
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook是一家成功利用Hadoop和Hive作为OLAP平台来处理PB级数据的组织示例。[图5.9](#ch05fig09)显示了与Facebook架构相似的一个架构。这个架构还包括一个反馈循环到OLTP系统，可以用来将Hadoop中发现的发现（如用户推荐）推送到OLTP系统。
- en: Figure 5.9\. Using Hadoop for OLAP and feedback to OLTP systems
  id: totrans-1708
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.9：使用Hadoop进行OLAP和反馈到OLTP系统
- en: '![](05fig09.jpg)'
  id: totrans-1709
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig09.jpg)'
- en: In either usage model, you need a way to bring relational data into Hadoop,
    and you also need to export it into relational databases. In this section, you’ll
    use Sqoop to streamline moving your relational data into Hadoop.
  id: totrans-1710
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何使用模型中，您都需要一种将关系型数据引入Hadoop的方法，同时也需要将其导出到关系型数据库中。在本节中，您将使用Sqoop简化将关系型数据移动到Hadoop的过程。
- en: Technique 42 Using Sqoop to import data from MySQL
  id: totrans-1711
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号42：使用Sqoop从MySQL导入数据
- en: Sqoop is a project that you can use to move relational data into and out of
    Hadoop. It’s a great high-level tool as it encapsulates the logic related to the
    movement of the relational data into Hadoop—all you need to do is supply Sqoop
    the SQL queries that will be used to determine which data is exported. This technique
    provides the details on how you can use Sqoop to move some stock data in MySQL
    to HDFS.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop是一个可以将关系型数据在Hadoop中导入和导出的项目。作为一个高级工具，它封装了与关系型数据移动到Hadoop相关的逻辑——您需要做的是向Sqoop提供将用于确定哪些数据被导出的SQL查询。本技术提供了如何使用Sqoop将一些MySQL中的库存数据移动到HDFS的详细信息。
- en: '|  |'
  id: totrans-1713
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Versioning
  id: totrans-1714
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 版本控制
- en: This section uses version 1.4.4 of Sqoop. The code and scripts used in this
    technique may not work with other versions of Sqoop, especially Sqoop 2, which
    is implemented as a web application.
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用Sqoop的1.4.4版本。本技术中使用的代码和脚本可能与其他版本的Sqoop不兼容，尤其是作为Web应用程序实现的Sqoop 2。
- en: '|  |'
  id: totrans-1716
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Problem
  id: totrans-1717
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to load relational data into your cluster and ensure your writes are
    efficient and also idempotent.
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将关系型数据加载到您的集群中，并确保您的写入操作既高效又幂等。
- en: Solution
  id: totrans-1719
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: In this technique, we’ll look at how you can use Sqoop as a simple mechanism
    to bring relational data into Hadoop clusters. We’ll walk through the process
    of importing data from MySQL into Sqoop. We’ll also cover bulk imports using the
    fast connector (connectors are database-specific components that provide database
    read and write access).
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项技术中，我们将探讨如何使用Sqoop作为一个简单的机制将关系型数据引入Hadoop集群。我们将逐步介绍将数据从MySQL导入Sqoop的过程。我们还将介绍使用快速连接器（连接器是特定于数据库的组件，提供数据库的读写访问）进行批量导入。
- en: Discussion
  id: totrans-1721
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Sqoop is a relational database import and export system. It was created by Cloudera
    and is currently an Apache project in incubation status.
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop 是一个关系数据库导入和导出系统。它由 Cloudera 创建，目前处于 Apache 项目孵化状态。
- en: 'When you perform an import, Sqoop can write to HDFS, Hive, and HBase, and for
    exports it can do the reverse. Importing is divided into two activities: connecting
    to the data source to gather some statistics, and then firing off a MapReduce
    job that performs the actual import. [Figure 5.10](#ch05fig10) shows these steps.'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: 当您执行导入时，Sqoop 可以写入 HDFS、Hive 和 HBase，对于导出，它可以执行相反的操作。导入分为两个活动：连接到数据源以收集一些统计信息，然后启动一个执行实际导入的
    MapReduce 作业。[图 5.10](#ch05fig10) 展示了这些步骤。
- en: 'Figure 5.10\. Sqoop import overview: connecting to the data source and using
    MapReduce'
  id: totrans-1724
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10\. Sqoop 导入概述：连接数据源和使用 MapReduce
- en: '![](05fig10_alt.jpg)'
  id: totrans-1725
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig10_alt.jpg)'
- en: 'Sqoop has the notion of *connectors*, which contain the specialized logic needed
    to read and write to external systems. Sqoop comes with two classes of connectors:
    *common connectors* for regular reads and writes, and *fast connectors* that use
    database-proprietary batch mechanisms for efficient imports. [Figure 5.11](#ch05fig11)
    shows these two classes of connectors and the databases that they support.'
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop 有 *连接器* 的概念，它包含读取和写入外部系统所需的专业逻辑。Sqoop 包含两类连接器：*通用连接器* 用于常规的读取和写入，以及使用数据库专有批量机制的
    *快速连接器*，以实现高效的导入。[图 5.11](#ch05fig11) 展示了这两类连接器及其支持的数据库。
- en: Figure 5.11\. Sqoop connectors used to read and write to external systems
  id: totrans-1727
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.11\. 用于读取和写入外部系统的 Sqoop 连接器
- en: '![](05fig11.jpg)'
  id: totrans-1728
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig11.jpg)'
- en: 'Before you can continue, you’ll need access to a MySQL database and the MySQL
    JDBC JAR will need to be available.^([[18](#ch05fn19)]) The following script will
    create the necessary MySQL user and schema and load the data for this technique.
    The script creates a `hip_sqoop_user` MySQL user, and creates a sqoop_test database
    with three tables: stocks, stocks_export, and stocks_staging. It then loads the
    stocks sample data into the stocks table. All of these steps are performed by
    running the following command:'
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
  zh: 在您继续之前，您需要访问一个 MySQL 数据库，并且 MySQL JDBC JAR 需要可用.^([[18](#ch05fn19)]) 以下脚本将创建必要的
    MySQL 用户和模式，并为此技术加载数据。该脚本创建了一个 `hip_sqoop_user` MySQL 用户，并创建了一个包含三个表（stocks、stocks_export
    和 stocks_staging）的 sqoop_test 数据库。然后，它将股票样本数据加载到 stocks 表中。所有这些步骤都是通过运行以下命令来执行的：
- en: ^(18) MySQL installation instructions can be found in the appendix, if you don’t
    already have it installed. That section also includes a link to get the JDBC JAR.
  id: totrans-1730
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^((18) MySQL 安装说明可在附录中找到，如果您尚未安装，该部分还包括获取 JDBC JAR 的链接。
- en: '[PRE143]'
  id: totrans-1731
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Here’s a quick peek at what the script does:'
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
  zh: 下面简要介绍一下脚本做了什么：
- en: '[PRE144]'
  id: totrans-1733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: Follow the instructions in the appendix to install Sqoop. Those instructions
    also contain important steps for installing Sqoop dependencies, such as MySQL
    JDBC drivers.
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: 按照附录中的说明安装 Sqoop。这些说明还包含安装 Sqoop 依赖项的重要步骤，例如 MySQL JDBC 驱动程序。
- en: 'Your first Sqoop command will be a basic import, where you’ll specify connection
    information for your MySQL database and the table you want to export:'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一个 Sqoop 命令将是一个基本的导入，您将指定 MySQL 数据库和您想要导出的表的联系信息：
- en: '[PRE145]'
  id: totrans-1736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '|  |'
  id: totrans-1737
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: MySQL table names
  id: totrans-1738
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MySQL 表名
- en: MySQL table names in Linux are case-sensitive. Make sure that the table name
    you supply in the Sqoop commands uses the correct case.
  id: totrans-1739
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 中的 MySQL 表名是区分大小写的。请确保您在 Sqoop 命令中提供的表名使用正确的大小写。
- en: '|  |'
  id: totrans-1740
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: By default, Sqoop uses the table name as the destination in HDFS for the MapReduce
    job that it launches to perform the import. If you run the same command again,
    the MapReduce job will fail because the directory already exists.
  id: totrans-1741
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Sqoop 使用表名作为它在 HDFS 中启动的 MapReduce 作业的目标。如果您再次运行相同的命令，MapReduce 作业将失败，因为目录已经存在。
- en: 'Let’s take a look at the stocks directory in HDFS:'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 HDFS 中的 stocks 目录：
- en: '[PRE146]'
  id: totrans-1743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: Import data formats
  id: totrans-1744
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 导入数据格式
- en: Sqoop has imported the data as comma-separated text files. It supports a number
    of other file formats, which can be activated with the arguments listed in [table
    5.6](#ch05table06).
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop 已将数据导入为逗号分隔的文本文件。它支持多种其他文件格式，可以通过 [表 5.6](#ch05table06) 中列出的参数激活。
- en: Table 5.6\. Sqoop arguments that control the file formats of import commands
  id: totrans-1746
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 5.6\. 控制导入命令文件格式的 Sqoop 参数
- en: '| Argument | Description |'
  id: totrans-1747
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-1748
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| --as-avrodatafile | Data is imported as Avro files. |'
  id: totrans-1749
  prefs: []
  type: TYPE_TB
  zh: '| --as-avrodatafile | 数据以 Avro 文件的形式导入。|'
- en: '| --as-sequencefile | Data is imported as SequenceFiles. |'
  id: totrans-1750
  prefs: []
  type: TYPE_TB
  zh: '| --as-sequencefile | 数据以 SequenceFiles 格式导入。|'
- en: '| --as-textfile | The default file format; data is imported as CSV text files.
    |'
  id: totrans-1751
  prefs: []
  type: TYPE_TB
  zh: '| --as-textfile | 默认文件格式；数据以CSV文本文件导入。|'
- en: 'If you’re importing large amounts of data, you may want to use a file format
    such as Avro, which is a compact data format, and use it in conjunction with compression.
    The following example uses the Snappy compression codec in conjunction with Avro
    files. It also writes the output to a different directory from the table name
    by using the `--target-dir` option and specifies that a subset of rows should
    be imported by using the `--where` option. Specific columns to be extracted can
    be specified with `--columns`:'
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在导入大量数据，您可能希望使用像Avro这样的紧凑数据格式，并与其结合压缩使用。以下示例使用Snappy压缩编解码器与Avro文件结合使用。它还使用`--target-dir`选项将输出写入与表名不同的目录，并使用`--where`选项指定应导入的行子集。可以使用`--columns`指定要提取的特定列：
- en: '[PRE147]'
  id: totrans-1753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Note that the compression that’s supplied on the command line must be defined
    in the config file, core-site.xml, under the `io.compression.codecs` property.
    The Snappy compression codec requires you to have the Hadoop native libraries
    installed. See [chapter 4](kindle_split_014.html#ch04) for more details on compression
    setup and configuration.
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，命令行上提供的压缩必须在配置文件core-site.xml中的`io.compression.codecs`属性下定义。Snappy压缩编解码器要求您安装Hadoop原生库。有关压缩设置和配置的更多详细信息，请参阅[第4章](kindle_split_014.html#ch04)。
- en: 'You can introspect the structure of the Avro file to see how Sqoop has laid
    out the records by using the AvroDump tool introduced in technique 12\. Sqoop
    uses Avro’s `GenericRecord` for record-level storage (more details on that in
    [chapter 3](kindle_split_013.html#ch03)). If you run AvroDump against the Sqoop-generated
    files in HDFS, you’ll see the following:'
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用在第12项技术中介绍的AvroDump工具来检查Avro文件的结构，以查看Sqoop如何布局记录。Sqoop使用Avro的`GenericRecord`进行记录级存储（更多细节请参阅[第3章](kindle_split_013.html#ch03)）。如果您在HDFS中对由Sqoop生成的文件运行AvroDump，您将看到以下内容：
- en: '[PRE148]'
  id: totrans-1756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '|  |'
  id: totrans-1757
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Using Sqoop in conjunction with SequenceFiles
  id: totrans-1758
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结合Sqoop和SequenceFiles的使用
- en: 'One of the things that makes SequenceFiles hard to work with is that there
    isn’t a generic way to access data in a SequenceFile. You must have access to
    the `Writable` class that was used to write the data. In Sqoop’s case, it code-generates
    this file, which introduces a major problem: if you move to a newer version of
    Sqoop, and that version modifies the code generator, there’s a chance your older
    code-generated class won’t work with SequenceFiles generated with the newer version
    of Sqoop. You’ll either need to migrate all of your old SequenceFiles to the new
    version, or have code that can work with different versions of these SequenceFiles.
    Due to this restriction, I don’t recommend using SequenceFiles with Sqoop. If
    you’re looking for more information on how SequenceFiles work, run the Sqoop import
    tool and look at the stocks.java file that’s generated within your working directory.'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: 使SequenceFiles难以使用的一个原因是，没有通用的方式来访问SequenceFile中的数据。您必须有权访问用于写入数据的`Writable`类。在Sqoop的情况下，它通过代码生成此文件，这引入了一个主要问题：如果您切换到Sqoop的新版本，并且该版本修改了代码生成器，那么您的旧代码生成的类可能无法与使用新版本Sqoop生成的SequenceFiles一起工作。您可能需要将所有旧SequenceFiles迁移到新版本，或者拥有可以与这些SequenceFiles的不同版本一起工作的代码。由于这种限制，我不建议将SequenceFiles与Sqoop一起使用。如果您想了解更多关于SequenceFiles如何工作的信息，请运行Sqoop导入工具，并查看您工作目录内生成的stocks.java文件。
- en: '|  |'
  id: totrans-1760
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'You can take things a step further and specify the entire query with the `--query`
    option as follows:'
  id: totrans-1761
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以进一步使用`--query`选项指定整个查询，如下所示：
- en: '![](219fig01_alt.jpg)'
  id: totrans-1762
  prefs: []
  type: TYPE_IMG
  zh: '![图片](219fig01_alt.jpg)'
- en: Securing passwords
  id: totrans-1763
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 保护密码
- en: Up until now you’ve been using passwords in the clear on the command line. This
    is a security hole, because other users on the host can easily list the running
    processes and see your password. Luckily Sqoop has a few mechanisms that you can
    use to avoid leaking your password.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您一直在命令行上明文使用密码。这是一个安全漏洞，因为主机上的其他用户可以轻松列出正在运行的过程并看到您的密码。幸运的是，Sqoop有几个机制可以帮助您避免泄露密码。
- en: The first approach is to use the `-P` option, which will result in Sqoop prompting
    you for the password. This is the most secure approach, as it doesn’t require
    you to store your password, but it means you can’t automate your Sqoop commands.
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是使用`-P`选项，这将导致Sqoop提示您输入密码。这是最安全的方法，因为它不需要您存储密码，但这也意味着您不能自动化您的Sqoop命令。
- en: The second approach is to use the `--password-file` option, where you specify
    a file that contains your password. Note that this file must exist in the configured
    filesystem (mostly likely HDFS), not on a disk local to the Sqoop client. You’ll
    probably want to lock the file down so that only you have read access to this
    file. This still isn’t the most secure option, as root users on the filesystem
    would still be able to pry into the file, and unless you’re running secure Hadoop,
    it’s fairly easy even for non-root users to gain access.
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是使用 `--password-file` 选项，其中您指定一个包含您密码的文件。请注意，此文件必须存在于配置的文件系统中（很可能是 HDFS），而不是在
    Sqoop 客户端本地的磁盘上。您可能希望锁定该文件，以便只有您才有权读取此文件。但这还不是最安全的选项，因为文件系统上的 root 用户仍然能够窥探该文件，并且除非您运行的是安全的
    Hadoop，否则即使是非 root 用户也很容易获得访问权限。
- en: 'The last option is to use an options file. Create a file called ~/.sqoop-import-opts:'
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个选项是使用选项文件。创建一个名为 ~/.sqoop-import-opts 的文件：
- en: '[PRE149]'
  id: totrans-1768
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'Don’t forget to lock down the file to avoid prying eyes:'
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记锁定文件以防止他人窥探：
- en: '[PRE150]'
  id: totrans-1770
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Then you can supply this filename to your Sqoop job via the `--options-file`
    option, and Sqoop will read the options specified in the file, which means you
    don’t need to supply them on the command line:'
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以通过 `--options-file` 选项将此文件名提供给您的 Sqoop 作业，Sqoop 将读取文件中指定的选项，这意味着您不需要在命令行上提供它们：
- en: '[PRE151]'
  id: totrans-1772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: Data splitting
  id: totrans-1773
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据拆分
- en: How is Sqoop able to parallelize imports across multiple mappers?^([[19](#ch05fn20)])
    In [figure 5.10](#ch05fig10) I showed how Sqoop’s first step is to pull metadata
    from the database. It inspects the table being imported to determine the primary
    key, and runs a query to determine the lower and upper bounds of the data in the
    table (see [figure 5.12](#ch05fig12)). A somewhat even distribution of data within
    the minimum and maximum keys is assumed by Sqoop as it divides the delta (the
    range between the minimum and maximum keys) by the number of mappers. Each mapper
    is then fed a unique query containing a range of the primary key.
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop 如何能够在多个映射器之间并行化导入？^([[19](#ch05fn20)]) 在 [图 5.10](#ch05fig10) 中，我展示了 Sqoop
    的第一步是从数据库中提取元数据。它检查要导入的表以确定主键，并运行一个查询以确定表中数据的上下限（见 [图 5.12](#ch05fig12)）。Sqoop
    假设数据在最小和最大键之间分布得相对均匀，在划分 delta（最小和最大键之间的范围）时，将其除以映射器的数量。然后，每个映射器都会接收到一个包含主键范围的唯一查询。
- en: ^(19) By default Sqoop runs with four mappers. The number of mappers can be
    controlled with the `--num-mappers` argument.
  id: totrans-1775
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(19) 默认情况下，Sqoop 使用四个映射器。映射器的数量可以通过 `--num-mappers` 参数来控制。
- en: Figure 5.12\. Sqoop preprocessing steps to determine query splits
  id: totrans-1776
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.12\. Sqoop 预处理步骤以确定查询拆分
- en: '![](05fig12.jpg)'
  id: totrans-1777
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig12.jpg)'
- en: You can configure Sqoop to use a nonprimary key with the `--split-by` argument.
    This can be useful in situations where the primary key doesn’t have an even distribution
    of values between the minimum and maximum values. For large tables, however, you
    need to be careful that the column specified in `--split-by` is indexed to ensure
    optimal import times.
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `--split-by` 参数配置 Sqoop 使用非主键。这在主键在最小和最大值之间没有均匀分布值的情况下可能很有用。然而，对于大型表，您需要小心，确保在
    `--split-by` 中指定的列已建立索引，以确保最佳导入时间。
- en: You can use the `--boundary-query` argument to construct an alternative query
    to determine the minimum and maximum values.
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `--boundary-query` 参数构建一个替代查询以确定最小和最大值。
- en: Incremental imports
  id: totrans-1780
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增量导入
- en: 'You can also perform incremental imports. Sqoop supports two types: `append`
    works for numerical data that’s incrementing over time, such as auto-increment
    keys; `lastmodified` works on timestamped data. In both cases you need to specify
    the column using `--check-column`, the mode via the `--incremental` argument (the
    value must be either `append` or `lastmodified`), and the actual value to use
    to determine the incremental changes via `--last-value`.'
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以执行增量导入。Sqoop 支持两种类型：`append` 适用于随时间递增的数值数据，例如自增键；`lastmodified` 适用于带时间戳的数据。在这两种情况下，您都需要使用
    `--check-column` 指定列，通过 `--incremental` 参数（值必须是 `append` 或 `lastmodified`）指定模式，并通过
    `--last-value` 指定用于确定增量更改的实际值。
- en: 'For example, if you want to import stock data that’s newer than January 1,
    2005, you’d do the following:'
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想导入 2005 年 1 月 1 日之后的新股票数据，您将执行以下操作：
- en: '[PRE152]'
  id: totrans-1783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: Assuming that there’s another system that’s continuing to write into the stocks
    table, you’d use the `--last-value` output of this job as the input to the subsequent
    Sqoop job so that only rows newer than that date will be imported.
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
  zh: 假设还有另一个系统正在继续向股票表写入，你会使用此作业的`--last-value`输出作为后续Sqoop作业的输入，以便只导入比该日期新的行。
- en: Sqoop jobs and the metastore
  id: totrans-1785
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sqoop作业和元存储
- en: 'You can see in the command output the last value that was encountered for the
    increment column. How can you best automate a process that can reuse that value?
    Sqoop has the notion of a *job*, which can save this information and reuse it
    in subsequent executions:'
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在命令输出中看到增量列的最后一个值。如何最好地自动化一个可以重用该值的过程？Sqoop有一个名为*作业*的概念，它可以在后续执行中保存此信息并重用它：
- en: '![](222fig01_alt.jpg)'
  id: totrans-1787
  prefs: []
  type: TYPE_IMG
  zh: '![222fig01_alt.jpg](222fig01_alt.jpg)'
- en: Executing the preceding command creates a named job in the Sqoop *metastore*,
    which keeps track of all jobs. By default, the metastore is contained in your
    home directory under .sqoop and is only used for your own jobs. If you want to
    share jobs between users and teams, you’ll need to install a JDBC-compliant database
    for Sqoop’s metastore and use the `--meta-connect` argument to specify its location
    when issuing job commands.
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的命令在Sqoop *元存储*中创建一个命名作业，它跟踪所有作业。默认情况下，元存储位于你的家目录下的.sqoop中，并且仅用于你自己的作业。如果你想在不同用户和团队之间共享作业，你需要为Sqoop的元存储安装一个JDBC兼容的数据库，并在发出作业命令时使用`--meta-connect`参数指定其位置。
- en: 'The job `create` command executed in the previous example didn’t do anything
    other than add the job to the metastore. To run the job, you need to explicitly
    execute it as shown here:'
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中执行的`create`作业命令除了将作业添加到元存储外，没有做任何事情。要运行作业，你需要明确地按照以下方式执行它：
- en: '![](222fig02_alt.jpg)'
  id: totrans-1790
  prefs: []
  type: TYPE_IMG
  zh: '![222fig02_alt.jpg](222fig02_alt.jpg)'
- en: The metadata displayed by the `--show` argument includes the last value of your
    incremental column. This is actually the time when the command was executed, and
    not the last value in the table. If you’re using this feature, make sure that
    the database server and any clients interacting with the server (including the
    Sqoop client) have their clocks synced with the Network Time Protocol (NTP).
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
  zh: '`--show`参数显示的元数据包括增量列的最后一个值。这实际上是命令执行的时间，而不是表中的最后一个值。如果你使用此功能，请确保数据库服务器以及与服务器交互的任何客户端（包括Sqoop客户端）的时钟与网络时间协议（NTP）同步。'
- en: Sqoop will prompt for a password when running the job. To make this work in
    an automated script, you’ll need to use Expect, a Linux automation tool, to supply
    the password from a local file when it detects Sqoop prompting for a password.
    An Expect script that works with Sqoop can be found on GitHub at [https://github.com/alexholmes/hadoop-book/blob/master/bin/sqoop-job.exp](https://github.com/alexholmes/hadoop-book/blob/master/bin/sqoop-job.exp).
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行作业时，Sqoop将提示输入密码。为了使自动化脚本能够工作，你需要使用Expect，一个Linux自动化工具，在检测到Sqoop提示输入密码时，从本地文件中提供密码。一个与Sqoop一起工作的Expect脚本可以在GitHub上找到：[https://github.com/alexholmes/hadoop-book/blob/master/bin/sqoop-job.exp](https://github.com/alexholmes/hadoop-book/blob/master/bin/sqoop-job.exp)。
- en: 'Sqoop jobs can also be deleted as shown here:'
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
  zh: 如此所示，Sqoop作业也可以被删除：
- en: '[PRE153]'
  id: totrans-1794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: Fast MySQL imports
  id: totrans-1795
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 快速MySQL导入
- en: 'What if you want to bypass JDBC altogether and use the fast MySQL Sqoop connector
    for a high-throughput load into HDFS? This approach uses the `mysqldump` utility
    shipped with MySQL to perform the load. You must make sure that `mysqldump` is
    in the path of the user running the MapReduce job. To enable use of the fast connector
    you must specify the `--direct` argument:'
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想完全绕过JDBC并使用快速MySQL Sqoop连接器将高吞吐量数据加载到HDFS，该怎么办？这种方法使用MySQL附带`mysqldump`实用程序来执行加载。你必须确保`mysqldump`在运行MapReduce作业的用户路径中。要启用快速连接器的使用，你必须指定`--direct`参数：
- en: '[PRE154]'
  id: totrans-1797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: What are the disadvantages of fast connectors? Fast connectors only work with
    text output files—specifying Avro or SequenceFile as the output format of the
    import won’t work.
  id: totrans-1798
  prefs: []
  type: TYPE_NORMAL
  zh: 快速连接器的缺点是什么？快速连接器仅适用于文本输出文件——将Avro或SequenceFile指定为导入的输出格式将不起作用。
- en: Importing to Hive
  id: totrans-1799
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 导入到Hive
- en: The final step in this technique is to use Sqoop to import your data into a
    Hive table. The only difference between an HDFS import and a Hive import is that
    the Hive import has a postprocessing step where the Hive table is created and
    loaded, as shown in [figure 5.13](#ch05fig13).
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
  zh: 此技术的最后一步是使用Sqoop将你的数据导入到Hive表中。HDFS导入和Hive导入之间的唯一区别是，Hive导入有一个后处理步骤，其中创建并加载Hive表，如[图5.13](#ch05fig13)所示。
- en: Figure 5.13\. The Sqoop Hive import sequence of events
  id: totrans-1801
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13. Sqoop Hive导入事件序列
- en: '![](05fig13_alt.jpg)'
  id: totrans-1802
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig13_alt.jpg)'
- en: When data is loaded into Hive from an HDFS file or directory, as in the case
    of Sqoop Hive imports (step 4 in the figure), Hive moves the directory into its
    warehouse rather than copying the data (step 5) for the sake of efficiency. The
    HDFS directory that the Sqoop MapReduce job writes to won’t exist after the import.
  id: totrans-1803
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据从 HDFS 文件或目录加载到 Hive 中，例如在 Sqoop Hive 导入的情况下（图中的第 4 步），Hive 将目录移动到其仓库中，而不是复制数据（第
    5 步），以提高效率。Sqoop MapReduce 作业写入的 HDFS 目录在导入后不会存在。
- en: 'Hive imports are triggered via the `--hive-import` argument. Just like with
    the fast connector, this option isn’t compatible with the `--as-avrodatafile`^([[20](#ch05fn21)])
    and `--as -sequencefile` options:'
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 导入是通过 `--hive-import` 参数触发的。就像快速连接器一样，此选项与 `--as-avrodatafile`^([[20](#ch05fn21)])
    和 `--as-sequencefile` 选项不兼容：
- en: ^(20) See [https://issues.apache.org/jira/browse/SQOOP-324](https://issues.apache.org/jira/browse/SQOOP-324)
    for a potential future fix.
  id: totrans-1805
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (20) 请参阅[https://issues.apache.org/jira/browse/SQOOP-324](https://issues.apache.org/jira/browse/SQOOP-324)以获取可能的未来修复。
- en: '[PRE155]'
  id: totrans-1806
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '|  |'
  id: totrans-1807
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Importing strings containing Hive delimiters
  id: totrans-1808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 导入包含 Hive 分隔符的字符串
- en: 'You’ll likely have downstream processing issues if you’re importing columns
    that can contain any of Hive’s delimiters (the `\n`, `\r`, and `\01` characters).
    You have two options in such cases: either specify `--hive-drop-import-delims`,
    which will remove conflicting characters as part of the import, or specify `--hive-delims-replacement`,
    which will replace them with a different character.'
  id: totrans-1809
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在导入可能包含 Hive 分隔符（`\n`、`\r` 和 `\01` 字符）的列，您可能会遇到下游处理问题。在这种情况下，您有两个选择：要么指定
    `--hive-drop-import-delims`，这将作为导入过程的一部分删除冲突字符，要么指定 `--hive-delims-replacement`，这将用不同的字符替换它们。
- en: '|  |'
  id: totrans-1810
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If the Hive table already exists, the data will be appended to the existing
    table. If this isn’t the desired behavior, you can use the `--hive-overwrite`
    argument to indicate that the existing table should be replaced with the imported
    data.
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Hive 表已经存在，数据将被追加到现有表中。如果这不是您期望的行为，您可以使用 `--hive-overwrite` 参数来指示应使用导入数据替换现有表。
- en: 'You can also tell Sqoop to compress data being written to Hive tables. Sqoop
    currently only supports text outputs for Hive, so the LZOP compression codec is
    the best option here as it can be split in Hadoop (see [chapter 4](kindle_split_014.html#ch04)
    for details).^([[21](#ch05fn22)]) The following example shows how to use `--hive-overwrite`
    in conjunction with LZOP compression. For this to work, you’ll need to have LZOP
    built and installed on your cluster, because it isn’t bundled with Hadoop (or
    CDH) by default. Refer to [chapter 4](kindle_split_014.html#ch04) for more details:'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以告诉 Sqoop 对写入 Hive 表的数据进行压缩。目前 Sqoop 只支持 Hive 的文本输出，因此 LZOP 压缩编解码器是这里最好的选择，因为它可以在
    Hadoop 中分割（详情见[第 4 章](kindle_split_014.html#ch04)）。以下示例展示了如何结合使用 `--hive-overwrite`
    和 LZOP 压缩。为了使此操作生效，您需要在您的集群上构建和安装 LZOP，因为它默认情况下并未包含在 Hadoop（或 CDH）中。更多详情请参考[第
    4 章](kindle_split_014.html#ch04)：
- en: ^(21) bzip2 is also a splittable compression codec that can be used in Hadoop,
    but its write performance is so poor that in practice it’s rarely used.
  id: totrans-1813
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (21) bzip2 也是一个可分割的压缩编解码器，可以在 Hadoop 中使用，但其写入性能非常差，因此在实践中很少使用。
- en: '[PRE156]'
  id: totrans-1814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'Finally, you can use the `--hive-partition-key` and `--hive-partition-value`
    arguments to create different Hive partitions based on the value of a column being
    imported. For example, if you want to partition your input by stock name, you
    do the following:'
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用 `--hive-partition-key` 和 `--hive-partition-value` 参数根据导入列的值创建不同的 Hive
    分区。例如，如果您想按股票名称分区输入，您可以这样做：
- en: '[PRE157]'
  id: totrans-1816
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: Now, the previous example isn’t optimal by any means. Ideally, a single import
    would be able to create multiple Hive partitions. Because you’re limited to specifying
    a single key and value, you’d need to run the import once per unique partition
    value, which is laborious. You’d be better off importing into a nonpartitioned
    Hive table, and then retroactively creating partitions on the table after it had
    been loaded.
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，前面的示例无论如何都不是最优的。理想情况下，单个导入应该能够创建多个 Hive 分区。由于您只能指定一个键和一个值，您需要为每个唯一的分区值运行一次导入，这非常耗时。您最好是将数据导入到一个非分区的
    Hive 表中，然后在数据加载后回溯性地在表上创建分区。
- en: Also, the SQL query that you supply to Sqoop must also take care of filtering
    out the results, so that only those that match the partition are included. In
    other words, it would have been useful if Sqoop had updated the `WHERE` clause
    with `symbol = "AAPL"` rather than you having to do this yourself.
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你提供给 Sqoop 的 SQL 查询还必须注意过滤结果，以确保只包含匹配分区的结果。换句话说，如果 Sqoop 能够用 `symbol = "AAPL"`
    更新 `WHERE` 子句，而不是你自己这样做，那将是有用的。
- en: Continuous Sqoop execution
  id: totrans-1819
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 连续 Sqoop 执行
- en: 'If you need to regularly schedule imports into HDFS, Oozie has Sqoop integration
    that will allow you to periodically perform imports and exports. A sample Oozie
    workflow.xml example follows:'
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要定期安排导入到 HDFS，Oozie 有 Sqoop 集成，这将允许你定期执行导入和导出。以下是一个 Oozie workflow.xml 示例：
- en: '[PRE158]'
  id: totrans-1821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'Single and double quotes aren’t supported within the `<command>` element, so
    if you need to specify arguments that contain spaces, you’ll need to use the `<arg>`
    element instead:'
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: 单引号和双引号在 `<command>` 元素内不受支持，所以如果你需要指定包含空格的参数，你需要使用 `<arg>` 元素代替：
- en: '[PRE159]'
  id: totrans-1823
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: One other consideration when using Sqoop from Oozie is that you’ll need to make
    the JDBC driver JAR available to Oozie. You can either copy the JAR into the workflow’s
    lib/ directory or update your Hadoop installation’s lib directory with the JAR.
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Oozie 的 Sqoop 时，还需要考虑的另一点是，你需要将 JDBC 驱动 JAR 文件提供给 Oozie。你可以将 JAR 文件复制到工作流程的
    lib/ 目录中，或者使用 JAR 更新你的 Hadoop 安装 lib 目录。
- en: Summary
  id: totrans-1825
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Obviously, for Sqoop to work, your Hadoop cluster nodes need to have access
    to the MySQL database. Common sources of error are either misconfiguration or
    lack of connectivity from the Hadoop nodes. It’s probably wise to log on to one
    of the Hadoop nodes and attempt to connect to the MySQL server using the MySQL
    client, or attempt access with the mysqldump utility (if you’re using a fast connector).
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，为了使 Sqoop 能够工作，你的 Hadoop 集群节点需要能够访问 MySQL 数据库。常见的错误来源是配置错误或 Hadoop 节点之间的连接问题。登录到
    Hadoop 节点之一并尝试使用 MySQL 客户端连接到 MySQL 服务器，或者使用 mysqldump 实用程序尝试访问（如果你使用的是快速连接器），这可能是一个明智的选择。
- en: Another important point when using a fast connector is that it’s assumed that
    mysqldump is installed on each Hadoop node and is in the path of the user running
    the map tasks.
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用快速连接器时，另一个重要点是假设 mysqldump 已安装在每个 Hadoop 节点上，并且位于运行 map 任务的用户的路径中。
- en: This wraps up our review of using Sqoop to import data from relational databases
    into Hadoop. We’ll now transition from relational stores to a NoSQL store, HBase,
    which excels at data interoperability with Hadoop because it uses HDFS to store
    its data.
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了使用 Sqoop 从关系数据库导入数据到 Hadoop 的回顾。我们现在将从关系存储转换到 NoSQL 存储 HBase，它在与 Hadoop
    数据互操作性方面表现出色，因为它使用 HDFS 来存储其数据。
- en: 5.2.4\. HBase
  id: totrans-1829
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. HBase
- en: Our final foray into moving data into Hadoop involves taking a look at HBase.
    HBase is a real-time, distributed, data storage system that’s often either colocated
    on the same hardware that serves as your Hadoop cluster or is in close proximity
    to a Hadoop cluster. Being able to work with HBase data directly in MapReduce,
    or to push it into HDFS, is one of the huge advantages when picking HBase as a
    solution.
  id: totrans-1830
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据移动到 Hadoop 的最终尝试包括查看 HBase。HBase 是一个实时、分布式、数据存储系统，它通常位于作为你的 Hadoop 集群硬件的同一位置，或者位于
    Hadoop 集群附近。能够在 MapReduce 中直接处理 HBase 数据，或者将其推送到 HDFS，是选择 HBase 作为解决方案时的巨大优势之一。
- en: In the first technique, I’ll show you how to use a tool that HBase is bundled
    with to save an HBase table into HDFS.
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种技术中，我将向你展示如何使用 HBase 附带的工具将 HBase 表保存到 HDFS 中。
- en: Technique 43 HBase ingress into HDFS
  id: totrans-1832
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 43 HBase 进入 HDFS
- en: What if you had customer data sitting in HBase that you wanted to use in MapReduce
    in conjunction with data in HDFS? You could write a MapReduce job that takes as
    input the HDFS dataset and pulls data directly from HBase in your map or reduce
    code. But in some cases it may be more useful to take a dump of the data in HBase
    directly into HDFS, especially if you plan to utilize that data in multiple Map-Reduce
    jobs and the HBase data is immutable or changes infrequently.
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一些客户数据存储在 HBase 中，你希望将其与 HDFS 中的数据一起用于 MapReduce，你会怎么办？你可以编写一个 MapReduce
    作业，它以 HDFS 数据集为输入，并在你的 map 或 reduce 代码中直接从 HBase 拉取数据。但在某些情况下，直接将 HBase 中的数据转储到
    HDFS 可能更有用，特别是如果你计划在多个 Map-Reduce 作业中利用这些数据，并且 HBase 数据是不可变的或很少更改。
- en: Problem
  id: totrans-1834
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to get HBase data into HDFS.
  id: totrans-1835
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望将 HBase 数据导入到 HDFS。
- en: Solution
  id: totrans-1836
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: HBase includes an `Export` class that can be used to import HBase data into
    HDFS in SequenceFile format. This technique also walks through code that can be
    used to read the imported HBase data.
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
  zh: HBase 包含一个 `Export` 类，可以用来以 SequenceFile 格式将 HBase 数据导入 HDFS。这项技术还介绍了可以用来读取导入的
    HBase 数据的代码。
- en: Discussion
  id: totrans-1838
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Before we get started with this technique, you need to get HBase up and running.^([[22](#ch05fn23)])
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始这项技术之前，你需要让 HBase 运行起来.^([22](#ch05fn23)])
- en: ^(22) The appendix contains installation instructions and additional resources
    for working with HBase.
  id: totrans-1840
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([22]) 附录包含使用 HBase 的安装说明和附加资源。
- en: To be able to export data from HBase you first need to load some data into HBase.
    The loader creates an HBase table called stocks_example with a single column family,
    `details`. You’ll store the HBase data as Avro binary-serialized data. I won’t
    show the code here, but it’s available on GitHub.^([[23](#ch05fn24)])
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 HBase 导出数据，你首先需要将一些数据加载到 HBase 中。加载器创建了一个名为 stocks_example 的 HBase 表，它只有一个列族，`details`。你将存储
    HBase 数据为 Avro 二进制序列化数据。这里不展示代码，但它在 GitHub 上有。^([23](#ch05fn24)])
- en: '^(23) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/HBaseWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/HBaseWriter.java).'
  id: totrans-1842
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([23]) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/HBaseWriter.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/HBaseWriter.java).
- en: 'Run the loader and use it to load the sample stock data into HBase:'
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
  zh: 运行加载器并使用它将示例股票数据加载到 HBase 中：
- en: '[PRE160]'
  id: totrans-1844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'You can use the HBase shell to look at the results of the load. The `list`
    command, without any arguments, will show you all of the tables in HBase, and
    the `scan` command, with a single argument, will dump all of the contents of a
    table:'
  id: totrans-1845
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 HBase shell 来查看加载的结果。没有参数的 `list` 命令将显示 HBase 中的所有表，而带有单个参数的 `scan` 命令将转储一个表的所有内容：
- en: '[PRE161]'
  id: totrans-1846
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'With your data in place, you’re ready to export it to HDFS. HBase comes with
    an `org.apache.hadoop.hbase.mapreduce.Export` class that will dump an HBase table.
    An example of using the `Export` class is shown in the following snippet. With
    this command, you can export the whole HBase table:'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据就绪后，你可以将其导出到 HDFS。HBase 包含一个 `org.apache.hadoop.hbase.mapreduce.Export`
    类，该类可以将 HBase 表导出。以下代码片段展示了如何使用 `Export` 类的示例。使用此命令，你可以导出整个 HBase 表：
- en: '![](228fig01_alt.jpg)'
  id: totrans-1848
  prefs: []
  type: TYPE_IMG
  zh: '![图 228-1](228fig01_alt.jpg)'
- en: 'The `Export` class also supports exporting only a single column family, and
    it can also compress the output:'
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: '`Export` 类还支持仅导出单个列族，并且它还可以压缩输出：'
- en: '![](228fig02_alt.jpg)'
  id: totrans-1850
  prefs: []
  type: TYPE_IMG
  zh: '![图 228-2](228fig02_alt.jpg)'
- en: The `Export` class writes the HBase output in the SequenceFile format, where
    the HBase row key is stored in the SequenceFile record key using `org.apache.hadoop.hbase.io.ImmutableBytesWritable`,
    and the HBase value is stored in the SequenceFile record value using `org.apache.hadoop.hbase.client.Result`.
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
  zh: '`Export` 类将 HBase 输出写入 SequenceFile 格式，其中使用 `org.apache.hadoop.hbase.io.ImmutableBytesWritable`
    存储HBase行键在 SequenceFile 记录键中，并使用 `org.apache.hadoop.hbase.client.Result` 存储HBase值在
    SequenceFile 记录值中。'
- en: What if you want to process that exported data in HDFS? The following listing
    shows an example of how you’d read the HBase SequenceFile and extract the Avro
    stock records.^([[24](#ch05fn25)])
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在 HDFS 中处理这些导出的数据怎么办？以下列表展示了如何读取 HBase SequenceFile 并提取 Avro 股票记录的示例.^([24](#ch05fn25)])
- en: '^(24) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ExportedReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ExportedReader.java).'
  id: totrans-1853
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([24]) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ExportedReader.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ExportedReader.java).
- en: Listing 5.3\. Reading the HBase SequenceFile to extract Avro stock records
  id: totrans-1854
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3\. 读取 HBase SequenceFile 以提取 Avro 股票记录
- en: '![](ch05ex03-0.jpg)'
  id: totrans-1855
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3-0](ch05ex03-0.jpg)'
- en: '![](ch05ex03-1.jpg)'
  id: totrans-1856
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3-1](ch05ex03-1.jpg)'
- en: 'You can run the code against the HDFS directory that you used for the export
    and view the results:'
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行代码针对你用于导出的 HDFS 目录，并查看结果：
- en: '[PRE162]'
  id: totrans-1858
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: The `HBaseExportedStockReader` class is able to read and dump out the contents
    of the SequenceFile used by HBase’s `Export` class.
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
  zh: '`HBaseExportedStockReader` 类能够读取并转储 HBase 的 `Export` 类所使用的 SequenceFile 的内容。'
- en: Exporting data from HBase into HDFS is made easier with the built-in HBase `Export`
    class. But what if you don’t want to write HBase data into HDFS, but instead want
    to process it directly in a MapReduce job? Let’s look at how you can use HBase
    as a data source for a MapReduce job.
  id: totrans-1860
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置的HBase `Export`类将数据从HBase导出到HDFS变得更容易。但如果你不想将HBase数据写入HDFS，而是想在MapReduce作业中直接处理它呢？让我们看看如何将HBase作为MapReduce作业的数据源。
- en: Technique 44 MapReduce with HBase as a data source
  id: totrans-1861
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧44：使用HBase作为数据源的MapReduce
- en: The built-in HBase exporter writes out HBase data using SequenceFile, which
    isn’t supported by programming languages other than Java and doesn’t support schema
    evolution. It also only supports a Hadoop filesystem as the data sink. If you
    want to have more control over HBase data extracts, you may have to look beyond
    the built-in HBase facilities.
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的HBase导出器使用SequenceFile将HBase数据写入，这仅由Java以外的编程语言支持，并且不支持模式演变。它还仅支持Hadoop文件系统作为数据接收器。如果你想要对HBase数据提取有更多控制，你可能不得不超越内置的HBase功能。
- en: Problem
  id: totrans-1863
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to operate on HBase directly within your MapReduce jobs without the
    intermediary step of copying the data into HDFS.
  id: totrans-1864
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce作业中直接操作HBase，而不需要将数据复制到HDFS的中间步骤。
- en: Solution
  id: totrans-1865
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: HBase has a `TableInputFormat` class that can be used in your MapReduce job
    to pull data directly from HBase.
  id: totrans-1866
  prefs: []
  type: TYPE_NORMAL
  zh: HBase有一个`TableInputFormat`类，可以在你的MapReduce作业中使用，直接从HBase拉取数据。
- en: Discussion
  id: totrans-1867
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: HBase provides an `InputFormat` class called `TableInputFormat`, which can use
    HBase as a data source in MapReduce. The following listing shows a MapReduce job
    that uses this input format (via the `TableMapReduceUtil.initTableMapperJob` call)
    to read data from HBase.^([[25](#ch05fn26)])
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
  zh: HBase提供了一个名为`TableInputFormat`的`InputFormat`类，可以在MapReduce中使用HBase作为数据源。以下列表显示了一个使用此输入格式（通过`TableMapReduceUtil.initTableMapperJob`调用）从HBase读取数据的MapReduce作业.^([25](#ch05fn26))
- en: '^(25) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ImportMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ImportMapReduce.java).'
  id: totrans-1869
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([25](#ch05fn26)) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ImportMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch5/hbase/ImportMapReduce.java).
- en: Listing 5.4\. Importing HBase data into HDFS using MapReduce
  id: totrans-1870
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.4：使用MapReduce将HBase数据导入HDFS
- en: '![](ch05ex04-0.jpg)'
  id: totrans-1871
  prefs: []
  type: TYPE_IMG
  zh: '![](ch05ex04-0.jpg)'
- en: '![](ch05ex04-1.jpg)'
  id: totrans-1872
  prefs: []
  type: TYPE_IMG
  zh: '![](ch05ex04-1.jpg)'
- en: 'You can run this MapReduce job as follows:'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按以下方式运行此MapReduce作业：
- en: '[PRE163]'
  id: totrans-1874
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'A quick peek in HDFS should tell you whether or not your MapReduce job worked
    as expected:'
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
  zh: 快速查看HDFS应该会告诉你MapReduce作业是否按预期工作：
- en: '[PRE164]'
  id: totrans-1876
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: This output confirms that the MapReduce job works as expected.
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出确认MapReduce作业按预期工作。
- en: Summary
  id: totrans-1878
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The `TableInputFormat` class examines HBase and creates an input split for each
    HBase table region. If there are 10 HBase regions, 10 map tasks will execute.
    The input format also includes the server that hosts the region in the input split,
    which means that the map tasks will be scheduled to execute on the same nodes
    as the HRegionServer hosting the data. This gives you locality at the HBase level,
    but also at the HDFS level. Data being read from the region will likely be coming
    from local disk, because after some time, all of a region’s data will be local
    to it. This all assumes that the HRegion-Servers are running on the same hosts
    as the DataNodes.
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
  zh: '`TableInputFormat`类检查HBase并为每个HBase表区域创建一个输入拆分。如果有10个HBase区域，将执行10个map任务。输入格式还包括输入拆分中托管区域的服务器，这意味着map任务将被调度在托管数据的HRegionServer相同的节点上执行。这为您在HBase级别，也在HDFS级别提供了局部性。从区域读取的数据可能来自本地磁盘，因为经过一段时间，一个区域的所有数据都将本地化。这一切都假设HRegion-Servers运行在与DataNodes相同的宿主上。'
- en: Our focus over the last couple of sections has been on persistent stores, covering
    relational databases and HBase, a NoSQL store. We’re now going to change directions
    and look at how a publish-subscribe system can be leveraged to move data into
    Hadoop.
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几节中，我们的重点是持久化存储，包括关系数据库和HBase，一个NoSQL存储。现在我们将改变方向，看看如何利用发布-订阅系统将数据移动到Hadoop。
- en: 5.2.5\. Importing data from Kafka
  id: totrans-1881
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5. 从Kafka导入数据
- en: Kafka, a distributed publish-subscribe system, is quickly becoming a key part
    of our data pipelines thanks to its strong distributed and performance properties.
    It can be used for many functions, such as messaging, metrics collection, stream
    processing, and log aggregation. Another effective use of Kafka is as a vehicle
    to move data into Hadoop. This is useful in situations where you have data being
    produced in real time that you want to land in Hadoop.
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka，一个分布式发布-订阅系统，由于其强大的分布式和性能特性，正迅速成为我们数据管道的关键部分。它可以用于许多功能，如消息传递、指标收集、流处理和日志聚合。Kafka的另一个有效用途是将数据移动到Hadoop中。这在以下情况下很有用：你有实时产生并希望存入Hadoop的数据。
- en: A key reason to use Kafka is that it decouples data producers and consumers.
    It notably allows you to have multiple independent producers (possibly written
    by different development teams), and, likewise, multiple independent consumers
    (again possibly written by different teams). Also, consumption can be real-time/synchronous
    or batch/offline/asynchronous. The latter property is a big differentiator when
    you’re looking at other pub-sub tools like RabbitMQ.
  id: totrans-1883
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kafka的一个关键原因是它解耦了数据生产者和消费者。它特别允许你拥有多个独立的生产者（可能由不同的开发团队编写），同样，也有多个独立的消费者（可能由不同的团队编写）。此外，消费可以是实时/同步的，也可以是批量/离线/异步的。当你查看其他发布/订阅工具如RabbitMQ时，这种后者的属性是一个很大的区别点。
- en: 'Kafka has a handful of concepts that you’ll need to understand:'
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka有几个概念你需要理解：
- en: '***Topics*** —A topic is a feed of related messages.'
  id: totrans-1885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***主题*** —主题是一系列相关的消息。'
- en: '***Partitions*** —Each topic is made up of one or more partitions, which are
    ordered sequences of messages backed by log files.^([[26](#ch05fn27)])'
  id: totrans-1886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分区*** —每个主题由一个或多个分区组成，这些分区是有序的消息序列，由日志文件支持。[^([26](#ch05fn27))]'
- en: ^(26) I’m not talking about logging files here; Kafka employs log files to store
    data flowing through Kafka.
  id: totrans-1887
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([26]) 我在这里不是在谈论日志文件；Kafka使用日志文件来存储通过Kafka流动的数据。
- en: '***Producers and consumers*** —Producers and consumers write messages to and
    read them from partitions.'
  id: totrans-1888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***生产者和消费者*** —生产者和消费者向分区写入消息并从中读取。'
- en: '***Brokers*** —Brokers are the Kafka processes that manage topics and partitions
    and serve producer and consumer requests.'
  id: totrans-1889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***代理*** —代理是管理主题和分区并处理生产者和消费者请求的Kafka进程。'
- en: Kafka does not guarantee “total” ordering for a topic—instead, it only guarantees
    that the individual partitions that make up a topic are ordered. It’s up to the
    consumer application to enforce, if needed, a “global” per-topic ordering.
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka不保证主题的“完全”顺序—相反，它只保证组成主题的各个分区是有序的。如果需要，由消费者应用程序强制执行“全局”的每个主题顺序。
- en: '[Figure 5.14](#ch05fig14) shows a conceptual model of how Kafka works and [figure
    5.15](#ch05fig15) shows an example of how partitions could be distributed in an
    actual Kafka deployment.'
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.14](#ch05fig14) 展示了Kafka工作原理的概念模型，[图5.15](#ch05fig15) 展示了在实际Kafka部署中分区可能如何分布。'
- en: Figure 5.14\. Conceptual Kafka model showing producers, topics, partitions,
    and consumers
  id: totrans-1892
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.14\. 概念Kafka模型，展示了生产者、主题、分区和消费者
- en: '![](05fig14_alt.jpg)'
  id: totrans-1893
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig14_alt.jpg)'
- en: Figure 5.15\. A physical Kafka model showing how partitions can be distributed
    across brokers
  id: totrans-1894
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.15\. 一个物理Kafka模型，展示了分区如何在代理之间分布
- en: '![](05fig15_alt.jpg)'
  id: totrans-1895
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig15_alt.jpg)'
- en: To support fault tolerance, topics can be replicated, which means that each
    partition can have a configurable number of replicas on different hosts. This
    provides increased fault tolerance and means that a single server dying isn’t
    catastrophic for your data or for the availability of your producers and consumers.
  id: totrans-1896
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持容错性，主题可以被复制，这意味着每个分区可以在不同的主机上配置可配置数量的副本。这提供了更高的容错性，并且意味着单个服务器的故障不会对你的数据或生产者和消费者的可用性造成灾难性影响。
- en: '|  |'
  id: totrans-1897
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Versioning
  id: totrans-1898
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 版本控制
- en: Technique 45 employs Kafka version 0.8 and the 0.8 branch of Camus.
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧45使用Kafka版本0.8和Camus的0.8分支。
- en: '|  |'
  id: totrans-1900
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: This wraps up our quick dive into how Kafka works. For more details, please
    refer to Kafka’s online documentation.
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对Kafka工作原理的快速探索。更多详情，请参阅Kafka的在线文档。
- en: Technique 45 Using Camus to copy Avro data from Kafka into HDFS
  id: totrans-1902
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧45 使用Camus将Avro数据从Kafka复制到HDFS
- en: This technique is useful in situations where you already have data flowing in
    Kafka for other purposes and you want to land that data in HDFS.
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在以下情况下很有用：你已经有了其他目的的数据在Kafka中流动，并且你希望将那些数据存入HDFS。
- en: Problem
  id: totrans-1904
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use Kafka as a data-delivery mechanism to get your data into HDFS.
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望使用Kafka作为数据交付机制，将你的数据存入HDFS。
- en: Solution
  id: totrans-1906
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Camus, a LinkedIn-developed solution for copying data in Kafka into HDFS.
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Camus，这是 LinkedIn 开发的将 Kafka 中的数据复制到 HDFS 的解决方案。
- en: Discussion
  id: totrans-1908
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Camus is an open-source project developed by LinkedIn. Kafka is heavily deployed
    at LinkedIn, and where Camus is used as a tool to copy data from Kafka into HDFS.
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: Camus 是由 LinkedIn 开发的开源项目。LinkedIn 在 Kafka 上的部署非常广泛，Camus 被用作将数据从 Kafka 复制到
    HDFS 的工具。
- en: 'Out of the box, Camus supports two data formats in Kafka: JSON and Avro. In
    this technique we’re going to get Camus working with Avro data. Camus’s built-in
    support of Avro requires that Kafka publishers write the Avro data in a proprietary
    way, so for this technique we’re going to assume that you want to work with vanilla
    Avro-serialized data in Kafka.'
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Camus 支持 Kafka 中的两种数据格式：JSON 和 Avro。在这个技术中，我们将使 Camus 与 Avro 数据一起工作。Camus
    内置对 Avro 的支持要求 Kafka 发布者以专有方式写入 Avro 数据，因此，为了这项技术，我们将假设您想在 Kafka 中使用纯 Avro 序列化数据。
- en: 'There are three parts to getting this technique to work: you’ll first write
    some Avro data into Kafka, then you’ll write a simple class to help Camus deserialize
    your Avro data, and finally you’ll run a Camus job to perform the data import.'
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
  zh: 要使这项技术生效，有三个部分：首先，您将一些 Avro 数据写入 Kafka，然后您将编写一个简单的类来帮助 Camus 反序列化您的 Avro 数据，最后您将运行一个
    Camus 作业以执行数据导入。
- en: Writing data into Kafka
  id: totrans-1912
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将数据写入 Kafka
- en: To get going, you’ll write some Avro records into Kafka. In the following code,
    you set up a Kafka producer by configuring some required Kafka properties, load
    some Avro records from file, and write them out to Kafka:^([[27](#ch05fn28)])
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您将一些 Avro 记录写入 Kafka。在以下代码中，您通过配置一些必需的 Kafka 属性设置 Kafka 生产者，从文件中加载一些 Avro
    记录，并将它们写入 Kafka：^([[27](#ch05fn28)])
- en: '^(27) The complete set of Kafka properties you can set can be viewed in the
    Kafka documentation: [https://kafka.apache.org/documentation.html](https://kafka.apache.org/documentation.html).'
  id: totrans-1914
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(27) 您可以设置的所有 Kafka 属性的完整集合可以在 Kafka 文档中查看：[https://kafka.apache.org/documentation.html](https://kafka.apache.org/documentation.html)。
- en: '![](234fig01_alt.jpg)'
  id: totrans-1915
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](234fig01_alt.jpg)'
- en: 'You can load the sample stock data into a Kafka topic called `test` with the
    following command:'
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令将示例股票数据加载到名为 `test` 的 Kafka 主题中：
- en: '[PRE165]'
  id: totrans-1917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'The Kafka console consumer can be used to verify that the data has been written
    to Kafka. This will dump the binary Avro data to your console:'
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 控制台消费者可以用来验证数据是否已写入 Kafka。这将将二进制 Avro 数据转储到您的控制台：
- en: '[PRE166]'
  id: totrans-1919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: Once that’s done, you’re ready for the next part—writing some Camus code so
    that you can read these Avro records in Camus.
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，您就准备好进行下一部分了——编写一些 Camus 代码，以便您可以在 Camus 中读取这些 Avro 记录。
- en: Writing a Camus decoder and schema registry
  id: totrans-1921
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编写 Camus 解码器和模式注册表
- en: 'There are three Camus concepts that you need to understand:'
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要了解 Camus 的三个概念：
- en: '***Decoders*** —The decoder’s job is to convert raw data pulled from Kafka
    into a Camus format.'
  id: totrans-1923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***解码器*** —解码器的任务是将从中拉取的原始数据转换为 Camus 格式。'
- en: '***Encoders*** —Encoders serialize decoded data into the format that will be
    stored in HDFS.'
  id: totrans-1924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***编码器*** —编码器将解码后的数据序列化成将在 HDFS 中存储的格式。'
- en: '***Schema registry*** —The schema registry provides schema information about
    Avro data being encoded.'
  id: totrans-1925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***模式注册表*** —模式注册表提供了正在编码的 Avro 数据的模式信息。'
- en: As mentioned earlier, Camus supports Avro data, but it does so in a way that
    requires Kafka producers to write data using the Camus `KafkaAvroMessageEncoder`
    class, which prefixes the Avro-serialized binary data with some proprietary data,
    presumably so that the decoder in Camus can verify that it was written by that
    class.
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Camus 支持 Avro 数据，但它以这种方式支持，要求 Kafka 发布者使用 Camus `KafkaAvroMessageEncoder`
    类来写入数据，该类在 Avro 序列化的二进制数据前添加一些专有数据，这可能是为了让 Camus 中的解码器验证它是由该类写入的。
- en: 'In this example you’re serializing using the straight Avro serialization, so
    you need to write your own decoder. Luckily this is simple to do:'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，您使用的是直接的 Avro 序列化，因此您需要编写自己的解码器。幸运的是，这很简单：
- en: '![](235fig01_alt.jpg)'
  id: totrans-1928
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](235fig01_alt.jpg)'
- en: '|  |'
  id: totrans-1929
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Versioning
  id: totrans-1930
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 版本控制
- en: You may have noticed that we wrote a specific Avro record into Kafka, but in
    Camus we’re reading the record as a generic Avro record, not a specific Avro record.
    This is due to the fact that the `CamusWrapper` class only supports generic Avro
    records. Otherwise, specific Avro records would have been simpler to work with,
    as you can work with generated code and have all the type-safety goodness that
    comes along with that.
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们向 Kafka 中写入了一个特定的 Avro 记录，但在 Camus 中我们是以通用 Avro 记录的形式读取记录，而不是特定的 Avro
    记录。这是因为 `CamusWrapper` 类仅支持通用 Avro 记录。否则，特定的 Avro 记录会更容易处理，因为您可以使用生成的代码，并拥有随之而来的所有类型安全的好处。
- en: '|  |'
  id: totrans-1932
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'The `CamusWrapper` object is an envelope for the data being extracted from
    Kafka. The reason this class exists is that it allows you to stick metadata into
    the envelope, such as a timestamp, a server name, and the service details. It’s
    highly recommended that any data you work with have some meaningful timestamp
    associated with each record (typically this would be the time at which the record
    was created or generated). You can then use a `CamusWrapper` constructor that
    accepts the timestamp as an argument:'
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
  zh: '`CamusWrapper` 对象是从 Kafka 提取的数据的包装器。这个类存在的原因是它允许您将元数据（如时间戳、服务器名称和服务详情）放入包装器中。强烈建议您处理的所有数据都与每个记录相关联一些有意义的时戳（通常这将是记录创建或生成的时间）。然后您可以使用接受时间戳作为参数的
    `CamusWrapper` 构造函数：'
- en: '[PRE167]'
  id: totrans-1934
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: If the timestamp isn’t set, then Camus will create a new timestamp at the time
    the wrapper is created. This timestamp and other metadata is used in Camus when
    determining the HDFS location of output records. You’ll see an example of this
    shortly.
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有设置时间戳，那么 Camus 将在包装器创建时创建一个新的时间戳。这个时间戳和其他元数据用于 Camus 确定输出记录的 HDFS 位置。您很快就会看到这个示例。
- en: 'Next you need to write a schema registry so that the Camus Avro encoder knows
    the schema details for the Avro records being written to HDFS. When registering
    the schema, you also specify the name of the Kafka topic from which the Avro record
    was pulled:'
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要编写一个模式注册表，以便 Camus Avro 编码器知道写入 HDFS 的 Avro 记录的模式详情。在注册模式时，您还指定了从其中提取
    Avro 记录的 Kafka 主题的名称：
- en: '[PRE168]'
  id: totrans-1937
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: That’s it for the coding side of things! Let’s move on and see Camus in action.
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是关于编码方面的全部内容！让我们继续前进，看看加缪（Camus）的实际应用。
- en: Running Camus
  id: totrans-1939
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行 Camus
- en: 'Camus runs as a MapReduce job on the Hadoop cluster where you want to import
    the Kafka data. You need to feed a bunch of properties to Camus, and you can do
    so using the command line, or alternatively using a properties file. We’ll use
    the properties file for this technique:'
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
  zh: Camus 作为 MapReduce 作业在您想要导入 Kafka 数据的 Hadoop 集群上运行。您需要向 Camus 提供一些属性，您可以通过命令行或使用属性文件来实现。我们将使用属性文件来实现这项技术：
- en: '[PRE169]'
  id: totrans-1941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: As you can see from the properties, you don’t need to explicitly tell Camus
    which topics you want to import. Camus automatically communicates with Kafka to
    discover the topics (and partitions), and the current start and end offsets.
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从属性中看到的，您不需要明确告诉 Camus 您想导入哪些主题。Camus 会自动与 Kafka 通信以发现主题（和分区），以及当前的起始和结束偏移量。
- en: 'If you want control over exactly which topics are imported, you can whitelist
    (to limit the topics) or blacklist (to exclude topics) using `kafka.whitelist.topics`
    and `kafka.blacklist.topics`, respectively. Multiple topics can be specified using
    a comma as the delimiter. Regular expressions are also supported, as shown in
    the following example, which matches on topic “topic1” or any topics that start
    with “abc” followed by one or more digits. Blacklists can be specified using the
    exact same syntax for the value:'
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想控制导入的确切主题，可以使用 `kafka.whitelist.topics` 和 `kafka.blacklist.topics` 分别进行白名单（限制主题）或黑名单（排除主题）。可以使用逗号作为分隔符指定多个主题。正则表达式也受到支持，如下面的示例所示，它匹配主题“topic1”或以“abc”开头后跟一个或多个数字的任何主题。可以使用相同的语法指定黑名单：
- en: '[PRE170]'
  id: totrans-1944
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: 'Once your properties are all set, you’re ready to run the Camus job:'
  id: totrans-1945
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的属性都设置好了，您就可以运行 Camus 作业了：
- en: '[PRE171]'
  id: totrans-1946
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: 'This will result in the Avro data landing in HDFS. Let’s take a look at what’s
    in HDFS:'
  id: totrans-1947
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致 Avro 数据落在 HDFS 中。让我们看看 HDFS 中有什么：
- en: '[PRE172]'
  id: totrans-1948
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: The first file is the file that you’re interested in, as it contains the data
    that’s been imported. The other files are there for Camus’s housekeeping.
  id: totrans-1949
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个文件是您感兴趣的文件，因为它包含已导入的数据。其他文件是 Camus 的维护文件。
- en: 'The data files in HDFS can be viewed using the AvroDump utility:'
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AvroDump 工具可以查看 HDFS 中的数据文件：
- en: '[PRE173]'
  id: totrans-1951
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: So what actually happened when the Camus job was running? The Camus import process
    is executed as a MapReduce job, as seen in [figure 5.16](#ch05fig16).
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: 那么当Camus作业运行时实际上发生了什么？Camus导入过程作为一个MapReduce作业执行，如图[图5.16](#ch05fig16)所示。
- en: Figure 5.16\. A look at how a Camus job executes
  id: totrans-1953
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.16\. 查看Camus作业的执行过程
- en: '![](05fig16_alt.jpg)'
  id: totrans-1954
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig16_alt.jpg)'
- en: As Camus tasks in MapReduce succeed, the Camus OutputCommitter (a MapReduce
    construct that allows for custom work to be performed upon task completion) atomically
    moves the tasks’ data files to the destination directory. The OutputCommitter
    additionally creates the offset files for all the partitions that the tasks were
    working on. It’s possible that other tasks in the same job may fail, but this
    doesn’t impact the state of tasks that succeed—the data and offset outputs of
    successful tasks will still exist, so that subsequent Camus executions will resume
    processing from the last-known successful state.
  id: totrans-1955
  prefs: []
  type: TYPE_NORMAL
  zh: 当Camus任务在MapReduce中成功执行时，Camus OutputCommitter（一个允许在任务完成后执行自定义工作的MapReduce构造）将任务的数据文件原子性地移动到目标目录。OutputCommitter还创建了所有任务正在工作的所有分区的偏移量文件。可能同一作业中的其他任务可能会失败，但这不会影响成功任务的州——成功任务的数据和偏移量输出仍然存在，这样后续的Camus执行将可以从最后一个已知的成功状态恢复处理。
- en: Next, let’s take a look at where Camus writes the imported data and how you
    can control the behavior.
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看Camus将导入的数据写入何处以及如何控制其行为。
- en: Data partitioning
  id: totrans-1957
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据分区
- en: Earlier you saw the location where Camus imported the Avro data sitting in Kafka.
    Let’s take a closer look at the HDFS path structure, shown in [figure 5.17](#ch05fig17),
    and see what you can do to determine the location.
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前，你看到了Camus从Kafka导入Avro数据的位置。让我们更仔细地看看HDFS路径结构，如图[图5.17](#ch05fig17)所示，并看看你能做什么来确定位置。
- en: Figure 5.17\. Dissecting the Camus output path for exported data in HDFS
  id: totrans-1959
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.17\. 分析HDFS中导出数据的Camus输出路径
- en: '![](05fig17_alt.jpg)'
  id: totrans-1960
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig17_alt.jpg)'
- en: The date/time part of the path is determined by the timestamp extracted from
    the `CamusWrapper`. You’ll recall from our earlier discussion that you can extract
    timestamps from your records in Kafka in your `MessageDecoder` and supply them
    to the `CamusWrapper`, which will allow your data to be partitioned by dates that
    are meaningful to you, as opposed to the default, which is simply the time at
    which the Kafka record is read in MapReduce.
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: 路径的日期/时间部分由从`CamusWrapper`中提取的时间戳确定。你可能会记得，在我们的早期讨论中，你可以在你的`MessageDecoder`中从你的Kafka记录中提取时间戳，并将它们提供给`CamusWrapper`，这将允许你的数据根据对你有意义的日期进行分区，而不是默认的，即Kafka记录在MapReduce中被读取的时间。
- en: Camus supports a pluggable partitioner, which allows you to control the part
    of the path shown in [figure 5.18](#ch05fig18).
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
  zh: Camus支持可插拔的分区器，这允许你控制[图5.18](#ch05fig18)中显示的路径部分。
- en: Figure 5.18\. The Camus partitioner path
  id: totrans-1963
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.18\. Camus分区器路径
- en: '![](05fig18_alt.jpg)'
  id: totrans-1964
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig18_alt.jpg)'
- en: 'The Camus `Partitioner` interface provides two methods that you must implement:'
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: Camus `Partitioner`接口提供了两个你必须实现的方法：
- en: '[PRE174]'
  id: totrans-1966
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: As an example, a custom partitioner could create a path that could be leveraged
    for Hive partitions.
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个自定义分区器可以创建一个路径，可以用于Hive分区。
- en: Summary
  id: totrans-1968
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Camus provides a complete solution to landing data from Kafka in HDFS, and it
    takes care of maintaining state and error handling when things go wrong. It can
    be easily automated by integrating it with Azkaban or Oozie, and it performs some
    simple data-management facilities by organizing HDFS data based on the time that
    messages are ingested. It’s worth mentioning that when it comes to ETL, it’s bare-boned
    in its features compared to Flume.
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
  zh: Camus为将数据从Kafka加载到HDFS提供了一个完整的解决方案，并在出错时处理状态维护和错误处理。它可以很容易地通过将其与Azkaban或Oozie集成来自动化，并且它通过根据消息摄入的时间组织HDFS数据来提供一些简单的数据管理功能。值得注意的是，与Flume相比，在ETL方面，它的功能相对简单。
- en: Kafka comes bundled with a mechanism that pulls data into HDFS. It has a `KafkaETLInputFormat`
    input format class that can be used to pull data from Kafka in a MapReduce job.
    It requires you to write the MapReduce job to perform the import, but the advantage
    is that you can use the data directly in your MapReduce flow, as opposed to using
    HDFS as intermediary storage for your data.
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka附带了一个将数据拉入HDFS的机制。它有一个`KafkaETLInputFormat`输入格式类，可以在MapReduce作业中用于从Kafka拉取数据。它要求你编写MapReduce作业以执行导入，但优点是你可以直接在你的MapReduce流程中使用数据，而不是像使用HDFS作为数据的中介存储那样。
- en: The Flume project is also in the process of adding a Kafka source and sink,
    although at the time of writing that work is still in progress.^([[28](#ch05fn29)])
    Once this is ready for production, you’ll be able to leverage all the other goodies
    that Flume offers, such as Morphlines and Solr indexing as part of moving Kafka
    data into Hadoop.
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: Flume项目也正在添加Kafka源和目标，尽管在撰写本文时这项工作仍在进行中。[^([28](#ch05fn29))] 一旦这项工作准备就绪，你将能够利用Flume提供的所有其他好处，例如Morphlines和Solr索引，作为将Kafka数据移动到Hadoop的一部分。
- en: ^(28) For more details on Flume and Kafka see [https://issues.apache.org/jira/browse/FLUME-2242](https://issues.apache.org/jira/browse/FLUME-2242).
  id: totrans-1972
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(28) 关于Flume和Kafka的更多详细信息，请参阅 https://issues.apache.org/jira/browse/FLUME-2242](https://issues.apache.org/jira/browse/FLUME-2242)。
- en: That concludes our examination of how to move data into Hadoop. We covered a
    broad range of data types, tools, and technologies. Next we’re going to flip things
    around and look at how to get data that resides in Hadoop out to other systems,
    such as filesystems and other stores.
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对如何将数据移动到Hadoop的探讨。我们涵盖了广泛的数据类型、工具和技术。接下来，我们将反过来看看如何将驻留在Hadoop中的数据导出到其他系统，例如文件系统和其他存储系统。
- en: 5.3\. Moving data out of Hadoop
  id: totrans-1974
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 从Hadoop中移动数据
- en: Once you’ve used Hadoop to perform some critical function, be it data mining
    or data aggregations, the next step is typically to externalize that data into
    other systems in your environment. For example, it’s common to rely on Hadoop
    to perform offline aggregations on data that’s pulled from your real-time systems,
    and then to feed the derived data back into your real-time systems. A more concrete
    example would be building recommendations based on user-behavior patterns.
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你使用Hadoop执行了一些关键功能，无论是数据挖掘还是数据聚合，下一步通常是将其数据外部化到你的环境中的其他系统中。例如，依赖于Hadoop对从你的实时系统中提取的数据进行离线聚合是很常见的，然后将派生的数据反馈到你的实时系统中。一个更具体的例子是根据用户行为模式构建推荐。
- en: This section examines some of the more common scenarios where you want to get
    data out of Hadoop, and the tools that will help you with that work. We’ll start
    with a look at the lower-level tools that exist, most of which are built into
    Hadoop, and then go on to look at how to push data to relational databases and
    HBase.
  id: totrans-1976
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨一些更常见的场景，在这些场景中，你想要从Hadoop中导出数据，以及帮助你完成这项工作的工具。我们将从查看现有的底层工具开始，这些工具大多数都集成在Hadoop中，然后继续探讨如何将数据推送到关系数据库和HBase。
- en: To start off, we’ll look at how to copy files out of Hadoop using the command
    line.
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨如何使用命令行从Hadoop中复制文件。
- en: 5.3.1\. Roll your own egress
  id: totrans-1978
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 自行实现出口
- en: This section covers some built-in mechanisms in Hadoop for copying data out
    of HDFS. These techniques can either be manually executed, or you’ll need to automate
    them using a scheduling system such as Azkaban, Oozie, or even cron.
  id: totrans-1979
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了Hadoop中用于从HDFS复制数据的内置机制。这些技术可以手动执行，或者你需要使用Azkaban、Oozie或甚至cron等调度系统来自动化它们。
- en: Technique 46 Using the CLI to extract files
  id: totrans-1980
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧46 使用CLI提取文件
- en: Imagine that you’ve run some jobs in Hadoop to aggregate some data, and now
    you want to get it out. One method you can use is the HDFS command-line interface
    (CLI) to pull out directories and files into your local filesystem. This technique
    covers some basic CLI commands that can help you out.
  id: totrans-1981
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经运行了一些Hadoop作业来聚合一些数据，现在你想要将其导出。你可以使用的一种方法是使用HDFS命令行界面（CLI）将目录和文件拉取到你的本地文件系统中。这项技术涵盖了可以帮助你的一些基本CLI命令。
- en: Problem
  id: totrans-1982
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to copy files from HDFS to a local filesystem using the shell.
  id: totrans-1983
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用shell将文件从HDFS复制到本地文件系统。
- en: Solution
  id: totrans-1984
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: The HDFS CLI can be used for one-off moves, or the same commands can be incorporated
    into scripts for more regularly utilized moves.
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS CLI可用于一次性移动，或者可以将相同的命令集成到脚本中以实现更频繁的移动。
- en: Discussion
  id: totrans-1986
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Copying a file from HDFS to local disk is achieved via the `hadoop` command:'
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`hadoop`命令从HDFS复制文件到本地磁盘：
- en: '[PRE175]'
  id: totrans-1988
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: 'The behavior of the Hadoop `put` command differs from the Linux `cp` command—in
    Linux if the destination already exists, it’s overwritten; in Hadoop the copy
    fails with an error:'
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的`put`命令的行为与Linux的`cp`命令不同——在Linux中，如果目标已存在，则会覆盖它；在Hadoop中，复制会因错误而失败：
- en: '[PRE176]'
  id: totrans-1990
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'The `-f` option must be added to force the file to be overwritten:'
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: 必须添加`-f`选项来强制覆盖文件：
- en: '[PRE177]'
  id: totrans-1992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'Much like with the Linux `cp` command, multiple files can be copied using the
    same command. In this case, the final argument must be the directory in the local
    filesystem into which the HDFS files are copied:'
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Linux `cp` 命令类似，可以使用相同的命令复制多个文件。在这种情况下，最后一个参数必须是本地文件系统中 HDFS 文件复制的目录：
- en: '[PRE178]'
  id: totrans-1994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: 'Often, one is copying a large number of files from HDFS to local disk—an example
    is a MapReduce job output directory that contains a file for each task. If you’re
    using a file format that can be concatenated, you can use the `-getmerge` command
    to combine multiple files. By default, a newline is added at the end of each file
    during concatenation:'
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人们会从 HDFS 复制大量文件到本地磁盘——一个例子是包含每个任务文件的 MapReduce 作业输出目录。如果您使用的是可以连接的文件格式，您可以使用
    `-getmerge` 命令来合并多个文件。默认情况下，在连接每个文件时都会添加一个换行符：
- en: '[PRE179]'
  id: totrans-1996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: There are many more operations supported by the `fs` command—to see the full
    list, run the command without any options.
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
  zh: '`fs` 命令支持许多更多操作——要查看完整列表，请在没有任何选项的情况下运行该命令。'
- en: The challenge with using the CLI is that it’s very low-level, and it won’t be
    able to assist you with your automation needs. Sure, you could use the CLI within
    shell scripts, but once you graduate to more sophisticated programming languages,
    forking a process for every HDFS command isn’t ideal. In this situation you may
    want to look at using the REST, Java, or C HDFS APIs. The next technique looks
    at the REST API.
  id: totrans-1998
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CLI 的挑战在于它非常底层，并且无法协助您满足自动化需求。当然，您可以在 shell 脚本中使用 CLI，但一旦您过渡到更复杂的编程语言，为每个
    HDFS 命令启动一个进程并不是理想的选择。在这种情况下，您可能想考虑使用 REST、Java 或 C HDFS API。下一技巧将探讨 REST API。
- en: Technique 47 Using REST to extract files
  id: totrans-1999
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 47 使用 REST 提取文件
- en: Using the CLI is handy for quickly running commands and for scripting, but it
    incurs the overhead of forking a separate process for each command, which is overhead
    that you’ll probably want to avoid, especially if you’re interfacing with HDFS
    in a programming language. This technique covers working with HDFS in languages
    other than Java.
  id: totrans-2000
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CLI 对于快速运行命令和脚本来说很方便，但它会产生为每个命令启动一个单独进程的开销，这是您可能想要避免的开销，尤其是如果您正在用编程语言与 HDFS
    交互。
- en: Problem
  id: totrans-2001
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to be able to interact with HDFS from a programming language that doesn’t
    have a native interface to HDFS.
  id: totrans-2002
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望能够使用没有 HDFS 原生接口的编程语言与 HDFS 交互。
- en: Solution
  id: totrans-2003
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Hadoop’s WebHDFS interface, which offers a full-featured REST API for HDFS
    operations.
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hadoop 的 WebHDFS 接口，它为 HDFS 操作提供了一整套功能的 REST API。
- en: Discussion
  id: totrans-2005
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Before you get started, you’ll need to enable WebHDFS on your cluster—see technique
    34 for details on how to do that.
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始之前，您需要在您的集群上启用 WebHDFS——有关如何操作的详细信息，请参阅技巧 34。
- en: 'Let’s start by creating a file in HDFS using the CLI:'
  id: totrans-2007
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用 CLI 在 HDFS 中创建一个文件开始：
- en: '[PRE180]'
  id: totrans-2008
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: 'Reading the file from HDFS is a matter of specifying `OPEN` as the operation:'
  id: totrans-2009
  prefs: []
  type: TYPE_NORMAL
  zh: 从 HDFS 读取文件只需指定 `OPEN` 作为操作：
- en: '[PRE181]'
  id: totrans-2010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: Consult technique 34 for additional information on using WebHDFS, including
    how it can be leveraged in different programming languages.
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用 WebHDFS 的更多信息，包括如何在不同的编程语言中利用它，请参考技巧 34。
- en: Technique 48 Reading from HDFS when behind a firewall
  id: totrans-2012
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 48 在防火墙后面从 HDFS 读取
- en: Production Hadoop environments are often locked down to protect the data residing
    in these clusters. Part of the security procedures could include putting your
    cluster behind a firewall, and this can be a nuisance if the destination for your
    Hadoop cluster is outside of the firewall. This technique looks at using the HttpFS
    gateway to provide HDFS access over port 80, which is often opened up on firewalls.
  id: totrans-2013
  prefs: []
  type: TYPE_NORMAL
  zh: 生产 Hadoop 环境通常被锁定以保护这些集群中驻留的数据。安全程序的一部分可能包括将您的集群置于防火墙之后，如果您的 Hadoop 集群的目标位于防火墙之外，这可能会很麻烦。本技巧探讨了使用
    HttpFS 网关通过端口 80 提供对 HDFS 的访问，端口 80 通常在防火墙上已打开。
- en: Problem
  id: totrans-2014
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to pull data out of HDFS, but you’re sitting behind a firewall that’s
    restricting access to HDFS.
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
  zh: 您想从 HDFS 中提取数据，但您坐在一个限制对 HDFS 访问的防火墙后面。
- en: Solution
  id: totrans-2016
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the HttpFS gateway, which is a standalone server that provides access to
    HDFS over HTTP. Because it’s a separate service and it’s HTTP, it can be configured
    to run on any host that has access to the Hadoop nodes, and you can open a firewall
    rule to allow traffic to the service.
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HttpFS 网关，这是一个独立的服务器，它通过 HTTP 提供对 HDFS 的访问。因为它是一个独立的服务，并且是 HTTP，所以它可以配置在任何可以访问
    Hadoop 节点的宿主上运行，并且您可以打开防火墙规则以允许流量访问该服务。
- en: Discussion
  id: totrans-2018
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: HttpFS is useful because not only can you use REST to access HDFS, but it has
    a complete Hadoop filesystem implementation, which means you can use the CLI and
    native HDFS Java clients to talk to HDFS. Consult technique 35 for instructions
    on how to get HttpFS up and running.
  id: totrans-2019
  prefs: []
  type: TYPE_NORMAL
  zh: HttpFS很有用，因为它不仅可以通过REST访问HDFS，而且它有一个完整的Hadoop文件系统实现，这意味着你可以使用CLI和本地的HDFS Java客户端与HDFS通信。有关如何设置HttpFS的说明，请参考技巧
    35。
- en: Once it’s running, you can issue the same curl commands that you used in the
    previous technique with WebHDFS (the only difference is URL host and port, which
    need to point to where your HttpFS is deployed). This is one of the nice things
    about the HttpFS gateway—the syntax is exactly the same.
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行起来，你就可以使用与之前技巧中WebHDFS相同的curl命令（唯一的区别是URL的主机和端口，需要指向你的HttpFS部署位置）。这是HttpFS网关的一个优点——语法完全相同。
- en: 'To dump the contents of the file /tmp/hdfs-file.txt, you’d do the following:'
  id: totrans-2021
  prefs: []
  type: TYPE_NORMAL
  zh: 要转储文件 /tmp/hdfs-file.txt 的内容，你会这样做：
- en: '[PRE182]'
  id: totrans-2022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: Swing on over to technique 35 for additional details on how HttpFS works.
  id: totrans-2023
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于HttpFS如何工作的细节，请查看技巧 35。
- en: Technique 49 Mounting Hadoop with NFS
  id: totrans-2024
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 49 使用NFS安装Hadoop
- en: Often it’s a lot easier to work with Hadoop data if it’s accessible as a regular
    mount to your filesystem. This allows you to use existing scripts, tools, and
    programming languages and easily interact with your data in HDFS. This section
    looks at how you can easily copy data out of HDFS using an NFS mount.
  id: totrans-2025
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Hadoop数据可以作为常规挂载访问你的文件系统，那么与Hadoop数据交互通常会容易得多。这允许你使用现有的脚本、工具和编程语言，并轻松地在HDFS中与数据交互。本节将探讨如何使用NFS挂载轻松地将数据从HDFS复制出来。
- en: Problem
  id: totrans-2026
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to treat HDFS as a regular Linux filesystem and use standard Linux
    tools to interact with HDFS.
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望将HDFS视为一个常规的Linux文件系统，并使用标准的Linux工具与HDFS交互。
- en: Solution
  id: totrans-2028
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Hadoop’s NFS implementation to access data in HDFS.
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop的NFS实现来访问HDFS中的数据。
- en: Discussion
  id: totrans-2030
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Technique 36 has setup instructions for NFS access to HDFS. Once that’s set
    up, you can perform normal filesystem operations such as copying files from HDFS
    to a local filesystem. The following example shows this, assuming that HDFS is
    mounted under /hdfs:'
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧 36 提供了NFS访问HDFS的设置说明。一旦设置好，你就可以执行正常的文件系统操作，例如将文件从HDFS复制到本地文件系统。以下示例展示了这一点，假设HDFS挂载在
    /hdfs 下：
- en: '[PRE183]'
  id: totrans-2032
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: For more details on how NFS works in Hadoop, head on over to technique 36.
  id: totrans-2033
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于NFS在Hadoop中如何工作的细节，请查看技巧 36。
- en: Technique 50 Using DistCp to copy data out of Hadoop
  id: totrans-2034
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 50 使用DistCp从Hadoop中复制数据
- en: Imagine that you have a large amount of data you want to move out of Hadoop.
    With most of the techniques in this section, you have a bottleneck because you’re
    funneling the data through a single host, which is the host on which you’re running
    the process. To optimize data movement as much as possible, you want to leverage
    MapReduce to copy data in parallel. This is where DistCp comes into play, and
    this technique examines one way you can pull out data to an NFS mount.
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一大批数据想要从Hadoop中移出。在本节的大多数技巧中，你会有一个瓶颈，因为你正在通过单个主机传输数据，这个主机是你运行进程的主机。为了尽可能优化数据传输，你想要利用MapReduce并行复制数据。这就是DistCp发挥作用的地方，本技巧将探讨一种将数据拉到NFS挂载的方法。
- en: Problem
  id: totrans-2036
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to efficiently pull data out of Hadoop and parallelize the copy.
  id: totrans-2037
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望高效地从Hadoop中提取数据并并行复制。
- en: Solution
  id: totrans-2038
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use DistCp.
  id: totrans-2039
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DistCp。
- en: Discussion
  id: totrans-2040
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Technique 37 covers DistCp in detail and includes details on how to copy data
    between different Hadoop clusters. But DistCp can’t be used to copy data from
    Hadoop to a local filesystem (or vice versa), because DistCp runs as a MapReduce
    job, and your cluster won’t have access to your local filesystem. Depending on
    your situation you have a couple of options:'
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧 37 详细介绍了DistCp，并包括在不同Hadoop集群之间复制数据的详细信息。但是，DistCp不能用来从Hadoop复制数据到本地文件系统（或反之亦然），因为DistCp作为一个MapReduce作业运行，你的集群将无法访问本地文件系统。根据你的情况，你有几个选择：
- en: Use the HDFS File Slurper to copy the local files.
  id: totrans-2042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HDFS File Slurper复制本地文件。
- en: Copy your files to an NFS that’s also available to all the DataNodes in your
    cluster.
  id: totrans-2043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的文件复制到一个对所有集群DataNode都可用的NFS。
- en: 'If you go with the second option, you can use DistCp and write to a locally
    mounted NFS mount on each DataNode, an example of which follows:'
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择第二个选项，你可以使用DistCp并将数据写入每个DataNode上可用的本地挂载的NFS，以下是一个示例：
- en: '[PRE184]'
  id: totrans-2045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: 'Note that your NFS system may not handle a large number of parallel reads or
    writes, so you’ll likely want to run this with a smaller number of mappers than
    the default of 20—the following example runs with 5 mappers:'
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你的NFS系统可能无法处理大量的并行读写操作，因此你可能希望使用比默认的20个更少的映射器数量来运行此操作——以下示例使用5个映射器运行：
- en: '[PRE185]'
  id: totrans-2047
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: Technique 51 Using Java to extract files
  id: totrans-2048
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧51 使用Java提取文件
- en: Let’s say you’ve generated a number of Lucene indexes in HDFS, and you want
    to pull them out to an external host. Maybe you want to manipulate the files in
    some way using Java. This technique shows how the Java HDFS API can be used to
    read data in HDFS.
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在HDFS中生成了一些Lucene索引，你想要将它们拉到一个外部主机上。也许你想用Java以某种方式操作这些文件。这项技术展示了如何使用Java
    HDFS API读取HDFS中的数据。
- en: Problem
  id: totrans-2050
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to copy files in HDFS to the local filesystem.
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望将HDFS中的文件复制到本地文件系统。
- en: Solution
  id: totrans-2052
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Hadoop’s filesystem API to copy data out of HDFS.
  id: totrans-2053
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop的文件系统API将数据从HDFS中复制出来。
- en: Discussion
  id: totrans-2054
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The HDFS Java API is nicely integrated with Java’s I/O model, which means you
    can work with regular input streams and output streams for I/O.
  id: totrans-2055
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS Java API与Java的I/O模型很好地集成，这意味着你可以使用常规输入流和输出流进行I/O操作。
- en: 'To start off, you need to create a file in HDFS using the command line:'
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要使用命令行在HDFS中创建一个文件：
- en: '[PRE186]'
  id: totrans-2057
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'Now copy that file to the local filesystem using the command line:'
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请使用命令行将文件复制到本地文件系统：
- en: '[PRE187]'
  id: totrans-2059
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: 'Let’s explore how you can replicate this copy in Java. There are two main parts
    to writing the code to do this—the first part is getting a handle to the `FileSystem`
    and creating the file, and the second part is copying the data from standard input
    to the `OutputStream`:'
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索如何在Java中复制这个操作。编写执行此操作的代码有两个主要部分——第一部分是获取`FileSystem`句柄并创建文件，第二部分是将数据从标准输入复制到`OutputStream`：
- en: '![](245fig01_alt.jpg)'
  id: totrans-2061
  prefs: []
  type: TYPE_IMG
  zh: '![245fig01_alt.jpg]'
- en: 'You can see how this code works in practice by running the following command:'
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来查看此代码的实际工作方式：
- en: '[PRE188]'
  id: totrans-2063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: So far we’ve covered the low-level tools that are bundled with Hadoop to help
    you pull out data. Next we’ll look at a method for near-continuous movement of
    data from HDFS to a local filesystem.
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了Hadoop附带的一些低级工具，这些工具可以帮助你提取数据。接下来，我们将探讨一种将数据从HDFS到本地文件系统进行近似连续移动的方法。
- en: 5.3.2\. Automated file egress
  id: totrans-2065
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2. 自动化文件出口
- en: Up until now you’ve seen different options for copying data out of HDFS. Most
    of these mechanisms don’t have automation or scheduling capabilities; they’re
    ultimately low-level methods for accessing data. If you’re looking to automate
    your data copy, you can wrap one of these low-level techniques inside of a scheduling
    engine such as cron or Quartz. However, if you’re looking for out-of-the-box automation,
    then this section is for you.
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了从HDFS中复制数据的不同选项。其中大部分机制没有自动化或调度功能；它们最终是访问数据的基础级方法。如果你想要自动化数据复制，你可以在cron或Quartz等调度引擎中包装这些低级技术之一。然而，如果你正在寻找即插即用的自动化，那么这一节就是为你准备的。
- en: 'Earlier in this chapter we looked at two mechanisms that can move semistructured
    and binary data into HDFS: the open source HDFS File Slurper project, and Oozie,
    which triggers a data ingress workflow. The challenge in using a local filesystem
    for egress (and ingress for that matter) is that map and reduce tasks running
    on clusters won’t have access to the filesystem on a specific server. You have
    three broad options for moving data from HDFS to a filesystem:'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早期，我们探讨了两种可以将半结构化和二进制数据移动到HDFS的机制：开源的HDFS File Slurper项目，以及触发数据入口工作流的Oozie。使用本地文件系统进行出口（以及入口）的挑战在于，在集群上运行的映射和归约任务将无法访问特定服务器的文件系统。从HDFS到文件系统移动数据，你有三种广泛的选择：
- en: You can host a proxy tier on a server, such as a web server, which you would
    then write to using MapReduce.
  id: totrans-2068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在服务器上托管一个代理层，例如Web服务器，然后使用MapReduce写入它。
- en: You can write to the local filesystem in MapReduce and then, as a postprocessing
    step, trigger a script on the remote server to move that data.
  id: totrans-2069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在MapReduce中写入本地文件系统，然后作为后处理步骤，在远程服务器上触发一个脚本以移动这些数据。
- en: You can run a process on the remote server to pull data from HDFS directly.
  id: totrans-2070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在远程服务器上运行一个进程，直接从HDFS中提取数据。
- en: The third option is the preferred approach because it’s the simplest and most
    efficient, and as such it’s the focus of this section. We’ll look at how you can
    use the HDFS File Slurper to automatically move files from HDFS out to a local
    filesystem.
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是首选方法，因为它最简单、最有效，因此本节将重点介绍。我们将探讨如何使用 HDFS 文件 Slurper 自动将文件从 HDFS 移动到本地文件系统。
- en: Technique 52 An automated mechanism to export files from HDFS
  id: totrans-2072
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇 52 从 HDFS 自动导出文件的方法
- en: Let’s say you have files being written in HDFS by MapReduce, and you want to
    automate their extraction to a local filesystem. This kind of feature isn’t supported
    by any Hadoop tools, so you have to look elsewhere.
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在 HDFS 中通过 MapReduce 编写文件，并且想要自动化将它们提取到本地文件系统中。这种功能任何 Hadoop 工具都不支持，所以你必须另寻他法。
- en: Problem
  id: totrans-2074
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to automate moving files from HDFS to a local filesystem.
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要自动化将文件从 HDFS 移动到本地文件系统。
- en: Solution
  id: totrans-2076
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: The HDFS File Slurper can be used to copy files from HDFS to a local filesystem.
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 文件 Slurper 可以用来将文件从 HDFS 复制到本地文件系统。
- en: Discussion
  id: totrans-2078
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The goal here is to use the HDFS File Slurper project ([https://github.com/alexholmes/hdfs-file-slurper](https://github.com/alexholmes/hdfs-file-slurper))
    to assist with the automation. We covered the HDFS File Slurper in detail in technique
    40—please read that section before continuing with this technique.
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是使用 HDFS 文件 Slurper 项目 ([https://github.com/alexholmes/hdfs-file-slurper](https://github.com/alexholmes/hdfs-file-slurper))
    来辅助自动化。我们在技术 40 中详细介绍了 HDFS 文件 Slurper，请在继续本技术之前阅读该部分。
- en: 'In addition to the way you used it in technique 40, the HDFS Slurper also supports
    moving data from HDFS out to a local directory. All you need to do is flip around
    the source and destination directories, as you can see from the following subsection
    of the Slurper’s configuration file:'
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
  zh: 除了你在技术 40 中使用的方法之外，HDFS Slurper 还支持将数据从 HDFS 移动到本地目录。你只需要交换源目录和目标目录，就像你可以在 Slurper
    的配置文件以下子节中看到的那样：
- en: '[PRE189]'
  id: totrans-2081
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: You’ll notice that not only is the source directory in HDFS, but also the work,
    complete, and error directories are there. This is because you need to be able
    to atomically move files between directories without incurring the expensive overhead
    of copying the files across filesystems.
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，不仅源目录在 HDFS 中，工作、完成和错误目录也在那里。这是因为你需要能够在目录之间原子性地移动文件，而不会产生跨文件系统复制文件的高昂开销。
- en: Summary
  id: totrans-2083
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: At this point you may wonder how you can trigger the Slurper to copy a directory
    that was just written with a MapReduce job. When a MapReduce job completes successfully,
    it creates a file called _SUCCESS in the job output directory. This would seem
    like the perfect trigger to kick off an egress process to copy that content to
    a local file-system. As it turns out, Oozie has a mechanism that can trigger a
    workflow when it detects these Hadoop “success” files, but again the challenge
    here is that any work performed by Oozie is performed in MapReduce, so it can’t
    be used to perform the transfer directly.
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道如何触发 Slurper 复制一个刚刚通过 MapReduce 作业编写的目录。当 MapReduce 作业成功完成后，它会在作业输出目录中创建一个名为
    _SUCCESS 的文件。这似乎是启动出口过程将内容复制到本地文件系统的完美触发器。实际上，Oozie 有一种机制可以在检测到这些 Hadoop “成功”文件时触发工作流，但同样，这里的挑战在于
    Oozie 执行的所有工作都是在 MapReduce 中进行的，因此不能直接用于执行传输。
- en: You could write your own script that polls HDFS for completed directories and
    then triggers a file copy process. That file copy process could be the Slurper
    or a simple `hadoop fs -get` command if the source files need to be kept intact.
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以编写自己的脚本来轮询 HDFS 中完成的目录，然后触发文件复制过程。这个文件复制过程可以是 Slurper 或简单的 `hadoop fs -get`
    命令，如果需要保持源文件完整的话。
- en: In the next topic we’ll look at writing data from Hadoop out to relational databases.
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个主题中，我们将探讨如何将数据从 Hadoop 写入关系型数据库。
- en: 5.3.3\. Databases
  id: totrans-2087
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3. 数据库
- en: 'Databases are usually the target of Hadoop data egress in one of two circumstances:
    either when you move data back into production databases to be used by production
    systems, or when you move data into OLAP databases to perform business intelligence
    and analytics functions.'
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库通常在两种情况下成为 Hadoop 数据出口的目标：要么是将数据移回生产数据库以供生产系统使用，要么是将数据移入 OLAP 数据库以执行商业智能和分析功能。
- en: In this section we’ll use Apache Sqoop to export data from Hadoop to a MySQL
    database. Sqoop is a tool that simplifies database imports and exports. Sqoop
    is covered in detail in technique 42.
  id: totrans-2089
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Apache Sqoop将数据从Hadoop导出到MySQL数据库。Sqoop是一个简化数据库导入和导出的工具。Sqoop在技术42中有详细说明。
- en: We’ll walk through the process of exporting data from HDFS to Sqoop. We’ll also
    cover methods for using the regular connector, as well as how to perform bulk
    imports using the fast connector.
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步介绍从HDFS导出数据到Sqoop的过程。我们还将介绍使用常规连接器的方法，以及如何使用快速连接器执行批量导入。
- en: Technique 53 Using Sqoop to export data to MySQL
  id: totrans-2091
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号53 使用Sqoop将数据导出到MySQL
- en: Hadoop excels at performing operations at scales that defeat most relational
    databases, so it’s common to extract OLTP data into HDFS, perform some analysis,
    and then export it back out to a database.
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop擅长执行大多数关系型数据库无法应对的规模操作，因此通常会将OLTP数据提取到HDFS中，进行一些分析，然后再将其导出回数据库。
- en: Problem
  id: totrans-2093
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to write data to relational databases, and at the same time ensure
    that writes are idempotent.
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将数据写入关系型数据库，同时确保写入是幂等的。
- en: Solution
  id: totrans-2095
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: This technique covers how Sqoop can be used to export text files to a relational
    database and also looks at how Sqoop can be configured to work with files with
    custom field and record delimiters. We’ll also cover idempotent exports to make
    sure that failed exports don’t leave your database in an inconsistent state.
  id: totrans-2096
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术涵盖了如何使用Sqoop将文本文件导出到关系型数据库，以及如何配置Sqoop以与具有自定义字段和记录分隔符的文件一起工作。我们还将介绍幂等导出，以确保失败的导出不会使您的数据库处于不一致的状态。
- en: Discussion
  id: totrans-2097
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: This technique assumes you’ve already followed the instructions in technique
    42 to install MySQL and create the schema.
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术假设您已经按照技术42中的说明安装了MySQL并创建了模式。
- en: Sqoop exports require that the database table you’re exporting into already
    exists. Sqoop can support both inserts and updates of rows in the table.
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop导出要求您要导出的数据库表已经存在。Sqoop可以支持表中行的插入和更新。
- en: 'Exporting data to a database shares many of the arguments that we examined
    in the import section. The differences are that exports require the `--export-dir`
    argument to determine the HDFS directory to export. You’ll also create another
    options file for exports to keep from insecurely supplying the password on the
    command line:'
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据导出到数据库与我们在导入部分中检查的许多参数相同。区别在于导出需要`--export-dir`参数来确定导出的HDFS目录。您还将为导出创建另一个选项文件，以避免在命令行上不安全地提供密码：
- en: '[PRE190]'
  id: totrans-2101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: 'Your first step will be to export data from MySQL to HDFS to ensure you have
    a good starting point, as shown in the following commands:'
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一步将是将数据从MySQL导出到HDFS，以确保您有一个良好的起点，如下所示：
- en: '[PRE191]'
  id: totrans-2103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'The result of the Sqoop import is a number of CSV files in HDFS, as you can
    see in the following code:'
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
  zh: 如下代码所示，Sqoop导入的结果是在HDFS中生成多个CSV文件：
- en: '[PRE192]'
  id: totrans-2105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: 'For the Sqoop export from HDFS to MySQL, you’ll specify that the target table
    should be stocks_export and that it should export data from the HDFS stocks directory:'
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从HDFS到MySQL的Sqoop导出，您将指定目标表应为stocks_export，并且应从HDFS的stocks目录导出数据：
- en: '[PRE193]'
  id: totrans-2107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: By default, Sqoop exports will perform an `INSERT` into the target database
    table. It can support updates with the `--update-mode` argument. A value of `updateonly`
    means that if there’s no matching key, the updates will fail. A value of `allowinsert`
    results in an insert if a matching key doesn’t exist. The table column name that’s
    used to perform the update is supplied in the `--update-key` argument.
  id: totrans-2108
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Sqoop导出将向目标数据库表执行`INSERT`操作。它可以支持使用`--update-mode`参数进行更新。`updateonly`的值表示如果没有匹配的键，更新将失败。`allowinsert`的值表示如果不存在匹配的键，则进行插入。用于执行更新的表列名由`--update-key`参数提供。
- en: 'The following example indicates that only an update should be attempted, using
    the primary key for the update:'
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例表明，仅尝试使用更新主键进行更新：
- en: '[PRE194]'
  id: totrans-2110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: Input data formatting
  id: totrans-2111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入数据格式
- en: You can use several options to override the default Sqoop settings for parsing
    the input data. [Table 5.7](#ch05table07) lists these options.
  id: totrans-2112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用几种选项来覆盖默认的Sqoop设置以解析输入数据。[表5.7](#ch05table07)列出了这些选项。
- en: Table 5.7\. Formatting options for input data
  id: totrans-2113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.7\. 输入数据的格式化选项
- en: '| Argument | Default | Description |'
  id: totrans-2114
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 默认值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-2115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| --input-enclosed-by | (None) | The field enclosing character. Every field
    must be enclosed with this character. (If the field enclosing character can occur
    inside a field, the --input-optionally-enclosed-by option should be used to enclose
    that field.) |'
  id: totrans-2116
  prefs: []
  type: TYPE_TB
  zh: '| --input-enclosed-by | (None) | 字段包围字符。每个字段都必须用此字符包围。（如果字段包围字符可以出现在字段内部，应使用
    --input-optionally-enclosed-by 选项来包围该字段。）|'
- en: '| --input-escaped-by | (None) | Escape character, where the next character
    is extracted literally and isn’t parsed. |'
  id: totrans-2117
  prefs: []
  type: TYPE_TB
  zh: '| --input-escaped-by | (None) | 转义字符，其中下一个字符被字面提取，不会被解析。|'
- en: '| --input-fields-terminated-by | , | The field separator. |'
  id: totrans-2118
  prefs: []
  type: TYPE_TB
  zh: '| --input-fields-terminated-by | , | 字段分隔符。|'
- en: '| --input-lines-terminated-by | \n | The line terminator. |'
  id: totrans-2119
  prefs: []
  type: TYPE_TB
  zh: '| --input-lines-terminated-by | \n | 行终止符。|'
- en: '| --input-optionally-enclosed-by | (None) | The field enclosing character.
    This argument is the same as --input-enclosed-by, except that it’s applied only
    to fields that contain the field separator character. For example, in CSV it’s
    common for fields to be enclosed by double quotes only when they contain commas.
    |'
  id: totrans-2120
  prefs: []
  type: TYPE_TB
  zh: '| --input-optionally-enclosed-by | (None) | 字段包围字符。此参数与 --input-enclosed-by
    相同，但仅应用于包含字段分隔符的字段。例如，在 CSV 中，字段通常仅在包含逗号时才用双引号包围。|'
- en: Idempotent exports
  id: totrans-2121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 幂等导出
- en: The Sqoop map tasks that perform the exports use multiple transactions for their
    database writes. If a Sqoop export MapReduce job fails, your table could contain
    partial writes. For idempotent database writes, Sqoop can be instructed to perform
    the MapReduce writes to the staging table. After successful job completion, the
    staging table is moved to the target table in a single transaction, which is idempotent.
    You can see the sequence of events in [figure 5.19](#ch05fig19).
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
  zh: 执行导出的 Sqoop 映射任务使用多个事务进行数据库写入。如果 Sqoop 导出 MapReduce 作业失败，您的表可能包含部分写入。为了进行幂等数据库写入，Sqoop
    可以被指示将 MapReduce 写入到临时表中。作业成功完成后，临时表将作为一个事务移动到目标表，这是幂等的。您可以在[图 5.19](#ch05fig19)中查看事件序列。
- en: Figure 5.19\. Sqoop staging sequence of events, which helps ensure idempotent
    writes
  id: totrans-2123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.19\. Sqoop 事件序列的临时表，有助于确保幂等写入
- en: '![](05fig19.jpg)'
  id: totrans-2124
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig19.jpg)'
- en: 'In the following example, the staging table is `stocks_staging`, and you’re
    also telling Sqoop to clear it out before the MapReduce job starts with the `--clear-staging-table`
    argument:'
  id: totrans-2125
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，临时表是 `stocks_staging`，并且您还告诉 Sqoop 在 MapReduce 作业开始前使用 `--clear-staging-table`
    参数将其清空：
- en: '[PRE195]'
  id: totrans-2126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: Direct exports
  id: totrans-2127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 直接导出
- en: 'You used the fast connector in the import technique, which was an optimization
    that used the mysqldump utility. Sqoop exports also support using the fast connector,
    which uses the mysqlimport tool. As with mysqldump, all of the nodes in your cluster
    need to have mysqlimport installed and available in the path of the user that’s
    used to run MapReduce tasks. And as with the import, the `--direct` argument enables
    utilization of the fast connectors:'
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
  zh: 您在导入技术中使用了快速连接器，这是一种使用 mysqldump 工具的优化。Sqoop 导出也支持使用快速连接器，该连接器使用 mysqlimport
    工具。与导入一样，您的集群中的所有节点都需要安装 mysqlimport 并在运行 MapReduce 任务的用户的路径中可用。并且与导入一样，`--direct`
    参数启用快速连接器的使用：
- en: '[PRE196]'
  id: totrans-2129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: Idempotent exports with mysqlimport
  id: totrans-2130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 mysqlimport 进行幂等导出
- en: 'Sqoop doesn’t support using fast connectors in conjunction with a staging table,
    which is how you achieve idempotent writes with regular connectors. But it’s still
    possible to achieve idempotent writes with fast connectors with a little extra
    work at your end. You need to use the fast connector to write to a staging table,
    and then trigger the `INSERT` statement, which atomically copies the data into
    the target table. The steps would look like the following:'
  id: totrans-2131
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop 不支持与临时表一起使用快速连接器，这是使用常规连接器实现幂等写入的方式。但通过您的一点点额外工作，仍然可以使用快速连接器实现幂等写入。您需要使用快速连接器将数据写入临时表，然后触发
    `INSERT` 语句，将数据原子性地复制到目标表中。步骤如下：
- en: '[PRE197]'
  id: totrans-2132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: This breaks the earlier rule about exposing credentials on the command line,
    but it’s easy to write a wrapper script that can read these settings from a configuration
    file.
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
  zh: 这打破了之前关于在命令行上暴露凭证的规则，但可以轻松编写一个包装脚本，从配置文件中读取这些设置。
- en: Summary
  id: totrans-2134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Sqoop provides a simplified usage model compared to using the `DBInputFormat`
    format classes that are provided in MapReduce. But using the `DBInputFormat` classes
    will give you the added flexibility to transform or preprocess your data in the
    same MapReduce job that performs the database export. The advantage of Sqoop is
    that it doesn’t require you to write any code, and it has some useful notions,
    such as staging, to help you achieve your idempotent goals.
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop 相比于在 MapReduce 中使用提供的 `DBInputFormat` 格式类提供了简化的使用模型。但使用 `DBInputFormat`
    类将为您提供在执行数据库导出的同一 MapReduce 作业中转换或预处理数据的额外灵活性。Sqoop 的优势在于它不需要您编写任何代码，并且有一些有用的概念，例如临时存储，以帮助您实现幂等目标。
- en: The final step in this section, and in the chapter, is to look at exporting
    data to HBase.
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
  zh: 本节和本章的最后一步是查看将数据导出到 HBase。
- en: 5.3.4\. NoSQL
  id: totrans-2137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4. NoSQL
- en: MapReduce is a powerful and efficient way to bulk-load data into external systems.
    So far we’ve covered how Sqoop can be used to load relational data, and now we’ll
    look at NoSQL systems, and specifically HBase.
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 是将数据批量加载到外部系统的一种强大且高效的方式。到目前为止，我们已经介绍了如何使用 Sqoop 加载关系数据，现在我们将探讨 NoSQL
    系统，特别是 HBase。
- en: Apache HBase is a distributed key/value, column-oriented data store. Earlier
    in this chapter we looked at how to import data from HBase into HDFS, as well
    as how to use HBase as a data source for a MapReduce job.
  id: totrans-2139
  prefs: []
  type: TYPE_NORMAL
  zh: Apache HBase 是一个分布式键/值、列式数据存储。在本章早期，我们探讨了如何从 HBase 导入数据到 HDFS，以及如何将 HBase 作为
    MapReduce 作业的数据源。
- en: The most efficient way to load data into HBase is via its built-in bulk-loading
    mechanism, which is described in detail on the HBase wiki page titled “Bulk Loading”
    at [https://hbase.apache.org/book/arch.bulk.load.html](https://hbase.apache.org/book/arch.bulk.load.html).
    But this approach bypasses the write-ahead log (WAL), which means that the data
    being loaded isn’t replicated to slave HBase nodes.
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到 HBase 中最有效的方法是通过其内置的大批量加载机制，这在 HBase wiki 页面上有详细描述，标题为“批量加载”，网址为 [https://hbase.apache.org/book/arch.bulk.load.html](https://hbase.apache.org/book/arch.bulk.load.html)。但这种方法绕过了预写日志（WAL），这意味着正在加载的数据不会被复制到从属
    HBase 节点。
- en: HBase also comes with an `org.apache.hadoop.hbase.mapreduce.Export` class, which
    will load HBase tables from HDFS, similar to how the equivalent import worked
    earlier in this chapter. But you must have your data in SequenceFile form, which
    has disadvantages, including no support for versioning.
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: HBase 还附带了一个 `org.apache.hadoop.hbase.mapreduce.Export` 类，该类可以从 HDFS 加载 HBase
    表，类似于本章早期等效的导入操作。但您必须以 SequenceFile 形式存储数据，这有一些缺点，包括不支持版本控制。
- en: You can also use the `TableOutputFormat` class in your own MapReduce job to
    export data to HBase, but this approach is slower than the bulk-loading tool.
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在您的 MapReduce 作业中使用 `TableOutputFormat` 类将数据导出到 HBase，但这种方法比批量加载工具慢。
- en: We’ve now concluded our examination of Hadoop egress tools. We covered how you
    can use the HDFS File Slurper to move data out to a filesystem and how to use
    Sqoop for idempotent writes to relational databases, and we wrapped up with a
    look at ways to move Hadoop data into HBase.
  id: totrans-2143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了对 Hadoop 导出工具的考察。我们介绍了如何使用 HDFS 文件吞噬者将数据移动到文件系统，以及如何使用 Sqoop 对关系数据库进行幂等写入，并以查看将
    Hadoop 数据移动到 HBase 的方法结束。
- en: 5.4\. Chapter summary
  id: totrans-2144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4. 章节总结
- en: Moving data in and out of Hadoop is a critical part of the Hadoop architecture.
    In this chapter we covered a broad spectrum of techniques that you can use to
    perform data ingress and egress activities and that work with a variety of data
    sources. Of note, we covered Flume, a data collection and distribution solution,
    Sqoop, a tool for moving relational data in and out of Hadoop, and Camus, a tool
    for ingesting Kafka data into HDFS.
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 中移动数据进出是 Hadoop 架构的关键部分。在本章中，我们介绍了一系列技术，您可以使用这些技术执行数据导入和导出活动，并且与各种数据源兼容。值得注意的是，我们介绍了
    Flume，一个数据收集和分发解决方案，Sqoop，一个用于在 Hadoop 中移动关系数据的工具，以及 Camus，一个用于将 Kafka 数据导入 HDFS
    的工具。
- en: Now that your data is tucked away in HDFS, it’s time to look at some interesting
    processing patterns that you can apply to that data.
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的数据已经存储在 HDFS 中，是时候查看一些可以应用于该数据的有趣处理模式了。
