- en: Chapter 9\. Cloud Entity Resolution Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章\. 云实体解析服务
- en: In the last chapter, we saw how to scale up our entity resolution process to
    run on a Google Cloud–managed Spark cluster. This approach allowed us to match
    larger datasets in a reasonable time but it required us to do quite a bit of setup
    and management ourselves.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何将我们的实体解析过程扩展到运行在 Google Cloud 管理的 Spark 集群上。这种方法使我们能够在合理的时间内匹配更大的数据集，但它要求我们自己进行相当多的设置和管理。
- en: An alternative approach is to use entity resolution API provided by a cloud
    provider to perform the hard work for us. Google, Amazon, and Microsoft all offer
    these services.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用云提供商提供的实体解析 API 来为我们执行繁重的工作。Google、Amazon 和 Microsoft 都提供这些服务。
- en: 'In this chapter, we will use the entity reconciliation service, provided as
    part of Google’s Enterprise Knowledge Graph API, to resolve the MCA and Companies
    House datasets we examined in Chapters [6](ch06.html#chapter_6) and [8](ch08.html#chapter_8).
    We will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用作为 Google 企业知识图 API 的一部分提供的实体协调服务，来解析我们在第[6](ch06.html#chapter_6)章和第[8](ch08.html#chapter_8)章中检查的
    MCA 和 Companies House 数据集。我们将：
- en: Upload our standardized datasets to Google’s data warehouse, BigQuery.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的标准化数据集上传到 Google 的数据仓库 BigQuery。
- en: Provide a mapping of our data schema to a standard ontology.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供我们数据模式到标准本体的映射。
- en: Invoke the API from the console (we will also invoke the API using a Python
    script).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从控制台调用 API（我们还将使用 Python 脚本调用 API）。
- en: Use some basic SQL to process the results.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一些基本的 SQL 来处理结果。
- en: ​​To complete the chapter we will examine how well the service performs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ​​为了完成本章，我们将检查该服务的性能如何。
- en: Introduction to BigQuery
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BigQuery 简介
- en: BigQuery is Google’s fully managed, serverless data warehouse that enables scalable
    analysis over petabytes of data. It is a platform as a service that supports data
    querying and analysis using a dialect of SQL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 是 Google 的完全托管的、无服务器的数据仓库，支持使用 SQL 方言进行可扩展的数据查询和分析。它是一个平台即服务，支持数据查询和分析。
- en: To begin, we select the BigQuery product from the Google Cloud console. Under
    ANALYSIS we select “SQL workspace.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请从 Google Cloud 控制台中选择 BigQuery 产品。在分析下，我们选择“SQL 工作区”。
- en: Our first step is to select “Create dataset” from the ellipsis menu alongside
    your project name, as shown in [Figure 9-1](#fig-9-1).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是从您的项目名称旁边的省略菜单中选择“创建数据集”，如[图 9-1](#fig-9-1)所示。
- en: '![](assets/hoer_0901.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0901.png)'
- en: Figure 9-1\. BigQuery Create dataset
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. BigQuery 创建数据集
- en: In the pop-up window, as shown in [Figure 9-2](#fig-9-2), we need to name the
    Dataset ID as Chapter9, and then select a Location Type. You can then select a
    specific Region if you prefer or simply accept the Multi-region default. Optionally,
    you can add a number of days after which the table expires automatically.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在弹出窗口中，如[图 9-2](#fig-9-2)所示，我们需要将数据集 ID 命名为 Chapter9，然后选择位置类型。然后，您可以选择特定的区域，或者只需接受多区域默认值。可选地，您可以添加一个在表格自动过期的天数。
- en: Once we have an empty dataset created, our next task is to upload our MCA and
    Companies House tables. We can upload these tables from the data we saved in the
    Google Cloud Storage bucket in [Chapter 8](ch08.html#chapter_8).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了一个空数据集，下一个任务是上传我们的 MCA 和 Companies House 表格。我们可以从我们在[第 8章](ch08.html#chapter_8)中保存的
    Google Cloud 存储存储桶中的数据上传这些表格。
- en: '![](assets/hoer_0902.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0902.png)'
- en: Figure 9-2\. BigQuery Create dataset config
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. BigQuery 创建数据集配置
- en: With the dataset selected, we can click “+ Add,” or Add Data and then select
    Google Cloud Storage as the source (as shown in [Figure 9-3](#fig-9-3)). You can
    then browse to your Cloud Storage bucket and select the *mari_clean.csv* file.
    Select the Chapter9 dataset as the destination and name the table *mari*. Under
    Schema, click the “Auto detect” checkbox. You can accept the remainder of the
    default settings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 选择数据集后，我们可以单击“+ 添加”，或添加数据，然后选择 Google Cloud 存储作为源（如[图 9-3](#fig-9-3)所示）。然后，您可以浏览到您的
    Cloud 存储存储桶并选择*mari_clean.csv*文件。选择Chapter9数据集作为目的地，并将表格命名为*mari*。在模式下，单击“自动检测”复选框。您可以接受其余默认设置。
- en: '![](assets/hoer_0903.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0903.png)'
- en: Figure 9-3\. BigQuery Create table
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. BigQuery 创建表
- en: Repeat this procedure for the *basic_clean.csv* file, naming it *basic*. You
    can then select the table from the dataset to examine the schema. Selecting Preview
    will give you a view of the first few rows, as shown in [Figure 9-4](#fig-9-4).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*basic_clean.csv*文件，重复此过程，将其命名为*basic*。然后，您可以选择数据集中的表格以查看架构。选择预览将显示前几行，如[图 9-4](#fig-9-4)所示。
- en: '![](assets/hoer_0904.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0904.png)'
- en: Figure 9-4\. BigQuery table schema
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. BigQuery表模式
- en: Now that we have successfully loaded our data, we need to tell the Enterprise
    Knowledge Graph API how to map our schema and then run a reconciliation job.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已成功加载数据，需要告知企业知识图谱API如何映射我们的模式，然后运行一个协调作业。
- en: Enterprise Knowledge Graph API
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业知识图谱API
- en: The Google Enterprise Knowledge Graph API provides a lightweight entity resolution
    service that they call Entity Reconciliation. The service uses an AI model trained
    on Google data. It uses a parallel version of *hierarchical agglomerative clustering*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Google企业知识图谱API提供了一种轻量级实体解析服务，称为实体协调。该服务使用在Google数据上训练的AI模型。它使用了*分层聚合聚类*的并行版本。
- en: Hierarchical Agglomerative Clustering
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层聚合聚类
- en: This is a “bottom-up” approach to clustering entities. Each entity starts in
    its own cluster and then they are aggregated depending upon their similarity.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种“自下而上”的聚类实体方法。每个实体首先位于自己的簇中，然后根据它们的相似性进行聚合。
- en: At the time of writing, the Entity Reconciliation service is at Preview status
    and is made available on Pre-GA terms, details of which are available on the [Google
    Cloud website](https://oreil.ly/dThBk).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本文时，实体协调服务处于预览状态，并按照Pre-GA条款提供，详细信息请访问[Google Cloud网站](https://oreil.ly/dThBk)。
- en: To enable the API, select Enterprise KG under Artificial Intelligence from the
    console navigation menu. From here you can click “Enable the Enterprise Knowledge
    Graph API” for your project.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用API，请从控制台导航菜单中选择人工智能下的企业KG。从这里，您可以为您的项目点击“启用企业知识图谱API”。
- en: Schema Mapping
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式映射
- en: 'To set up our entity resolution job, we first need to map our data schema onto
    the schema that the Google Entity Reconciliation API understands. We do this by
    creating a mapping file for each data source we are going to use. The API uses
    a human-readable simple format language called YARRRML to define the mappings
    between source schema and a target ontology from [schema.org](https://schema.org).
    It supports three different entity types: Organization, Person, and Local Business.
    For our example, we will use the Organization schema.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置我们的实体解析作业，我们首先需要将我们的数据模式映射到Google实体协调API理解的模式上。我们通过为每个要使用的数据源创建一个映射文件来完成这一点。API使用一种称为YARRRML的人类可读的简单格式语言来定义源模式与来自[schema.org](https://schema.org)的目标本体之间的映射。它支持三种不同的实体类型：组织、个人和本地企业。在我们的示例中，我们将使用组织模式。
- en: To begin, we click on Schema Mapping and then select “Create a Mapping” in the
    Organization box. This brings us to an editor where we can modify and save a template
    mapping file. The mapping file is divided into a prefix section that tells the
    API which model and schema reference we are going to use. The mapping section
    then lists each entity type contained in the dataset. For each entity type, we
    specify the sources, a subject key (`s`) that uniquely refers to an entity in
    the dataset, and then the predicate list (`po`) which specifies the attributes
    of the entity we wish to match on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们点击Schema Mapping，然后在组织框中选择“创建映射”。这将带我们进入一个编辑器，在这里我们可以修改并保存模板映射文件。映射文件分为一个前缀部分，告诉API我们将使用哪个模型和模式参考。映射部分然后列出数据集中包含的每种实体类型。对于每种实体类型，我们指定源、唯一标识实体的主键(`s`)，然后是谓词列表(`po`)，指定我们希望匹配的实体属性。
- en: 'The default template is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 默认模板如下：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Starting with the mapping file for the MCA dataset, edit the default template
    as follows, remembering to insert your project name in the source line. This file
    is also available in the repository as *Chapter9SchemaMari*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从MCA数据集的映射文件开始，按如下编辑默认模板，记得在源行中插入您的项目名称。这个文件也可以在存储库中作为*Chapter9SchemaMari*获得：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note here that we are pointing the API to the *mari* BigQuery table we created
    earlier in the Chapter9 dataset. We are using the `unique_id` column as our subject
    key, and we are mapping our `Postcode` field to the `postalCode` property in the
    schema and our `CompanyName` field to the `name` property.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里我们正在指向先前在Chapter9数据集中创建的*mari* BigQuery表的API。我们使用`unique_id`列作为我们的主键，并将我们的`Postcode`字段映射到模式中的`postalCode`属性，将我们的`CompanyName`字段映射到`name`属性。
- en: 'Save this edited file into your Google Storage bucket under the *handsonentityresolution*
    directory as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将编辑后的文件保存到您的Google Storage存储桶中，路径为*handsonentityresolution*目录下：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Repeat this process to create a mapping file for the Companies House dataset,
    saving in the same location as *Chapter9SchemaBasic*. Remember to substitute *basic*
    for *mari* in the relevant lines and reference these entities as *company2*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 重复此过程为英国公司注册处数据集创建一个映射文件，保存在与 *Chapter9SchemaBasic* 相同的位置。记得在相关行中将 *basic* 替换为
    *mari* 并引用这些实体为 *company2*：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now have our datasets and our mapping files, so we can run an entity resolution
    (or reconciliation) job.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的数据集和映射文件，所以我们可以运行一个实体解析（或对账）作业。
- en: Reconciliation Job
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对账作业
- en: To start a reconciliation job, select Jobs from the Enterprise KG section in
    the console navigation menu, as shown in [Figure 9-5](#fig-9-5).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始一个对账作业，请在控制台导航菜单中的企业 KG 部分中选择作业，如 [图 9-5](#fig-9-5) 所示。
- en: '![](assets/hoer_0905.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0905.png)'
- en: Figure 9-5\. Start a reconciliation job
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 开始一个对账作业
- en: Select the RUN A JOB tab, as shown in [Figure 9-6](#fig-9-6).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 选择“运行作业”选项卡，如 [图 9-6](#fig-9-6) 所示。
- en: '![](assets/hoer_0906.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0906.png)'
- en: Figure 9-6\. Run an API Job for Entity Reconciliation
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-6\. 运行实体对账 API 作业
- en: 'From the pop-up menu:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从弹出菜单中：
- en: 'Step 1: Click “Select entity type”'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 1: 点击“选择实体类型”'
- en: Select Organization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 选择组织。
- en: 'Step 2: Add BigQuery data sources'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 2: 添加 BigQuery 数据源'
- en: Browse to the BigQuery path and select the *mari* table. Then select the matching
    mapping table by browsing to the *handsonentityresolution* directory in your bucket
    and selecting the *Chapter9SchemaMari* file we created earlier.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览到 BigQuery 路径，然后选择 *mari* 表。然后通过浏览到您的存储桶中的 *handsonentityresolution* 目录并选择我们之前创建的
    *Chapter9SchemaMari* 文件来选择匹配的映射表。
- en: Click Add Another BigQuery Datasource and repeat the process for the *basic*
    table and mapping file.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 点击添加另一个 BigQuery 数据源，然后重复*基础*表和映射文件的过程。
- en: 'Step 3: Set BigQuery data destination'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 3: 设置 BigQuery 数据目标'
- en: Browse and select the Chapter9 BigQuery dataset to tell the API where to write
    its results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览并选择 Chapter9 BigQuery 数据集以告知 API 写入其结果的位置。
- en: 'Step 4: Advanced settings (optional)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 4: 高级设置（可选）'
- en: For the final step, we can specify a previous result table so that the entity
    reconciliation service assigns consistent IDs to entities across different jobs,
    as shown in [Figure 9-7](#fig-9-7). This can be particularly useful to update
    existing entity records as new data is added.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一步，我们可以指定一个之前的结果表，这样实体对账服务就会为不同作业中的实体分配一致的 ID，如 [图 9-7](#fig-9-7) 所示。随着新数据的添加，这对更新现有实体记录特别有用。
- en: '![](assets/hoer_0907.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0907.png)'
- en: Figure 9-7\. Entity Reconciliation API advanced settings
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. 实体对账 API 高级设置
- en: The number of clustering rounds (iterations of the entity resolution model)
    can be specified; the higher the number, the more loosely entities are merged
    into the same cluster. The default is fine for our use case.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指定聚类轮数（实体解析模型的迭代次数）；轮数越高，实体合并得越松散。对我们的使用情况来说，默认设置就很好。
- en: Finally, we can click Done and start our job. Assuming all is well, we should
    then see a new job created under Job History, as shown in [Figure 9-8](#fig-9-8).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，点击“完成”并开始我们的作业。假设一切顺利，我们应该会看到一个新的作业在作业历史记录下创建，如 [图 9-8](#fig-9-8) 所示。
- en: '![](assets/hoer_0908.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0908.png)'
- en: Figure 9-8\. Entity Reconciliation Job History
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-8\. 实体对账作业历史记录
- en: We can watch the Job Display Status column to monitor the progress of our job
    as it moves sequentially through the display states shown in [Table 9-1](#table-9-1),
    and then finally displays Finished when complete.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察作业显示状态列，以监控作业的进度，它按照 [表 9-1](#table-9-1) 中显示的顺序顺序地进行显示状态，最后在完成时显示“已完成”。
- en: Table 9-1\. Job display state
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-1\. 作业显示状态
- en: '| Job display state | Code state | Description |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 作业显示状态 | 代码状态 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Running | `JOB_<wbr>STATE_<wbr>RUNNING` | The job is in progress. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 运行中 | `JOB_<wbr>STATE_<wbr>RUNNING` | 作业正在进行中。 |'
- en: '| Knowledge extraction | `JOB_<wbr>STATE_<wbr>KNOWLEDGE_<wbr>EXTRACTION` |
    Enterprise Knowledge Graph is pulling data out from BigQuery and creating features.
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 知识提取 | `JOB_<wbr>STATE_<wbr>KNOWLEDGE_<wbr>EXTRACTION` | 企业知识图正在从 BigQuery
    提取数据并创建特征。 |'
- en: '| Reconciliation preprocessing | `JOB_<wbr>STATE_<wbr>RECON_<wbr>​PRE⁠PROCESSING`
    | The job is at the reconciliation preprocessing step. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 对账预处理 | `JOB_<wbr>STATE_<wbr>RECON_<wbr>​PRE⁠PROCESSING` | 作业处于对账预处理步骤。 |'
- en: '| Clustering | `JOB_<wbr>STATE_<wbr>CLUSTERING` | The job is at the clustering
    step. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | `JOB_<wbr>STATE_<wbr>CLUSTERING` | 作业正在进行聚类步骤。 |'
- en: '| Exporting clusters | `JOB_<wbr>STATE_<wbr>EXPORTING_<wbr>CLUSTERS` | The
    job is writing output into the BigQuery destination dataset. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 导出聚类 | `JOB_<wbr>STATE_<wbr>EXPORTING_<wbr>CLUSTERS` | 作业正在将输出写入 BigQuery
    目标数据集。 |'
- en: This job should take approximately 1 hour 20 minutes but the duration varies
    widely at this Preview stage of the product.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此作业应该大约需要 1 小时 20 分钟，但在产品的此预览阶段，持续时间会有很大变化。
- en: When the job is finished, if we look in the BigQuery SQL workspace we should
    see a new table in our Chapter9 dataset called something like clusters_15719257497877843494,
    as shown in [Figure 9-9](#fig-9-9).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当作业完成后，如果我们查看 BigQuery SQL 工作空间，我们应该会看到我们的 Chapter9 数据集中的一个名为类似 clusters_15719257497877843494
    的新表，如[图9-9](#fig-9-9)所示。
- en: '![](assets/hoer_0909.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0909.png)'
- en: Figure 9-9\. BigQuery clusters results table
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9。BigQuery 聚类结果表
- en: Selecting the clusters_15719257497877843494 table and then selecting the Preview
    tab gives us a view of the results. [Figure 9-10](#fig-9-10) shows the first few
    rows.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 clusters_15719257497877843494 表，然后选择“预览”选项卡，我们可以查看结果。[图9-10](#fig-9-10)展示了前几行。
- en: '![](assets/hoer_0910.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0910.png)'
- en: Figure 9-10\. BigQuery cluster results preview
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10。BigQuery 聚类结果预览
- en: 'Let’s consider the columns in the output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑输出中的列：
- en: The *cluster_id* gives the unique reference of the cluster to which the Entity
    Reconciliation API has assigned the source entity.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cluster_id* 给出了实体协调 API 分配给源实体的聚类的唯一引用。'
- en: The *source_name* column gives us the name of the source table, in our case
    either *mari* or *basic*.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*source_name* 列给出了源表的名称，在我们的情况下是 *mari* 或 *basic*。'
- en: The *source_key* column contains the `unique_id` of the row in the source table.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*source_key* 列包含源表中行的 `unique_id`。'
- en: The *confidence* score, between 0 and 1, indicates how strongly a record is
    associated with a given cluster.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*confidence* 分数介于 0 到 1 之间，表示记录与给定聚类相关联的强度。'
- en: The *assignment_age* column is an internal API reference.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*assignment_age* 列是一个内部 API 参考。'
- en: The *cloud_kg_mid* column contains an MID value link to the entity in the Google
    Cloud Enterprise Knowledge Graph if the API can resolve a match. This can be used
    to look up additional details that Google has on the entity using the Cloud Enterprise
    Knowledge Graph API.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cloud_kg_mid* 列包含 Google Cloud 企业知识图中实体的 MID 值链接，如果 API 能够解析出匹配。可以使用 Cloud
    企业知识图 API 查找有关该实体的额外细节。'
- en: As every entity in both the *mari* and *basic* tables is assigned to a cluster,
    the row count for this table is the sum of the row counts for the source tables.
    In our case, this is over 5 million rows. At a glance, it’s not easy to identify
    which entities the API has matched, so we need to refine this data a little.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *mari* 和 *basic* 表中的每个实体都分配给一个聚类，因此此表的行数是源表行数的总和。在我们的情况下，这超过了 500 万行。乍一看，很难确定
    API 匹配了哪些实体，因此我们需要稍微调整一下这些数据。
- en: Result Processing
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果处理
- en: With our entity reconciliation results we can then use BigQuery SQL to process
    this raw information into an easier form for us to examine the resolved entities.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的实体协调结果，我们就可以使用 BigQuery SQL 将这些原始信息处理成更容易让我们检查已解析实体的形式。
- en: To start, we click “Compose a New Query”, which takes us to a SQL editor. You
    can cut and paste the SQL template from the *Chapter9.sql* file.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请点击“撰写新查询”，这将带我们进入一个 SQL 编辑器。您可以从*Chapter9.sql*文件中剪切并粘贴 SQL 模板。
- en: First we need to create a temporary table containing only rows whose `cluster_id`
    has at least one MCA match. We do this by building a subset of the cluster table
    whose rows have “mari” as the `source_name`. Then we find the intersection between
    the rows of this subset and the rows of the full cluster table using an `INNER
    JOIN` on matching `cluster_id`s.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个仅包含其 `cluster_id` 至少有一个 MCA 匹配的行的临时表。我们通过构建一个其行具有“mari”作为 `source_name`
    的聚类表子集来实现这一点。然后，我们通过在匹配的 `cluster_id` 上使用 `INNER JOIN` 找到此子集的行与完整聚类表的行的交集。
- en: 'Make sure to replace the cluster table name with the name of your results table,
    which will be in the format `clusters_*<job reference>*`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 确保用您的结果表的名称替换聚类表的名称，格式为 `clusters_*<job reference>*`：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The resulting temporary table now has only 151 rows. Next we create a second
    temporary table, this time with the subset of clusters that have both an MCA match
    and at least one Companies House match; i.e., we remove clusters with only an
    MCA match.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果临时表现在只有151行。接下来，我们创建第二个临时表，这次是包含同时具有 MCA 匹配和至少一个 Companies House 匹配的聚类子集；即，我们删除了只有
    MCA 匹配的聚类。
- en: To do this we select those `cluster_id`s with a count of greater than 1 and
    again find the intersection of this subset with the first temporary table using
    an `INNER JOIN` on the matching `cluster_id`s.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们选择那些具有大于 1 的`cluster_id`，然后再次找到这个子集与第一个临时表的交集，使用匹配的`cluster_id`进行`INNER
    JOIN`。
- en: 'Now we have a table of clusters containing only rows where the entity is found
    in both the Companies House and MCA datasets:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含只有在 Companies House 和 MCA 数据集中都找到实体的行的集群表：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This table now has 106 rows. We have the population we are looking for, so we
    can create a persistent results table picking up the `CompanyName` and `Postcode`
    from the source tables so that we can examine the results.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表现在有 106 行。我们已经得到了我们寻找的人口，所以我们可以创建一个持久的结果表，从源表中挑选`CompanyName`和`Postcode`，以便我们可以检查结果。
- en: 'We need to build this table in two parts. First, for the rows that refer to
    the Companies House data we need to look up the identifier in the `source_key`
    column and use that to retrieve the corresponding name and postcode. Then we need
    to do the same for rows that refer to the MCA data. We use the `UNION ALL` statement
    to join these two datasets and then `ORDER BY` `confidence` first and then `cluster_id`.
    This means that entities assigned to the same cluster are adjacent in the table
    for easy viewing:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要分两部分构建这张表。首先，对于指向 Companies House 数据的行，我们需要查找`source_key`列中的标识符，并使用它来检索相应的名称和邮政编码。然后，我们需要对指向
    MCA 数据的行执行相同操作。我们使用`UNION ALL`语句将这两个数据集连接起来，然后按`confidence`首先，然后按`cluster_id`排序。这意味着分配到相同集群的实体在表中是相邻的，以便于查看：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This gives us our results table, which looks like that given in [Figure 9-11](#fig-9-11).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个结果表，看起来像[图 9-11](#fig-9-11)所示。
- en: '![](assets/hoer_0911.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0911.png)'
- en: Figure 9-11\. Processed results table
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-11\. 处理后的结果表
- en: In the first row, we can see that both the MCA entity with `CompanyName` CREW
    AND CONCIERGE, `Postcode` BS31 1TP, and `unique_id` 18 has been assigned to cluster
    r-03fxqun0t2rjxn. In the second row, the Companies House entity with `CompanyName`
    CREW and CONCIERGE, the same `Postcode`, and `unique_id` 1182534 has been assigned
    to the same cluster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，我们可以看到 MCA 实体与`CompanyName` CREW AND CONCIERGE，`Postcode` BS31 1TP 和`unique_id`
    18 都被分配给集群 r-03fxqun0t2rjxn。在第二行中，具有`CompanyName` CREW and CONCIERGE，相同的`Postcode`
    和`unique_id` 1182534 的 Companies House 实体也被分配到同一个集群中。
- en: This means the Google Entity Reconciliation API has grouped these records into
    the same cluster, i.e., resolved these rows as referring to the same real-world
    entity, with a confidence rating of 0.7.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 Google 实体对账 API 已将这些记录分组到相同的集群中，即将这些行解析为同一个现实世界实体，并且置信度为 0.7。
- en: Before we examine these results in detail, we’ll take a quick detour to see
    how to invoke the API from Python instead of the cloud console.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细检查这些结果之前，我们将快速了解如何从 Python 调用 API 而不是从云控制台。
- en: Entity Reconciliation Python Client
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实体对账 Python 客户端
- en: The Google Enterprise Knowledge Graph API also supports a Python client to create,
    cancel, and delete entity reconciliation jobs. We can use the Cloud Shell virtual
    machine to run these Python scripts and launch these jobs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Google 企业知识图谱 API 还支持 Python 客户端来创建、取消和删除实体对账作业。我们可以使用 Cloud Shell 虚拟机来运行这些
    Python 脚本并启动这些作业。
- en: To activate Google Cloud Shell, click on the terminal symbol in the top right
    of the console. This will open a window with a command-line prompt.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要激活 Google Cloud Shell，请点击控制台右上角的终端符号。这将打开一个带有命令行提示符的窗口。
- en: 'A Python script to invoke the entity reconciliation job is included in the
    repository. To transfer a copy to your Cloud Shell machine we can use:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 包含用于调用实体对账作业的 Python 脚本已包含在仓库中。要将副本传输到您的 Cloud Shell 机器，我们可以使用：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A pop-up window will ask you to authorize the Cloud Shell to connect to your
    bucket.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 弹出窗口将要求您授权 Cloud Shell 连接到您的存储桶。
- en: 'The script, *Chapter9.py*, is reproduced here. You can use the Cloud Shell
    editor to edit this file to reference your project and bucket:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本*Chapter9.py*如下所示。您可以使用 Cloud Shell 编辑器编辑此文件，以引用您的项目和存储桶：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The Cloud Shell has Python installed so we can simply run this script from
    the command prompt with the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Shell 中已安装 Python，因此我们可以简单地从命令提示符运行此脚本：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To process the results, we can use the SQL script we examined previously. To
    copy this from your Cloud Storage bucket:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理结果，我们可以使用我们之前检查过的 SQL 脚本。要从您的 Cloud Storage 存储桶复制这些：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we run this BigQuery script using:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用以下 BigQuery 脚本运行：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note that if the results table has already been created by running this query
    from the SQL workspace, this command will fail because the table already exists.
    You can delete the table using:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，如果结果表已通过从 SQL 工作区运行此查询创建，则此命令将失败，因为表已经存在。您可以使用以下命令删除表： '
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now we can examine how well the API performed on our example.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查 API 在我们的示例上的表现。
- en: Measuring Performance
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量性能
- en: Recall from our Preview of the BigQuery results table that we have 106 rows.
    The distribution of match confidence is shown in [Table 9-2](#table-9-2).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅我们的 BigQuery 结果表预览中，我们有 106 行。匹配置信度的分布如[表 9-2](#table-9-2)所示。
- en: Table 9-2\. Match confidence
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-2\. 匹配置信度
- en: '| Number of matches | Confidence |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 匹配数 | 置信度 |'
- en: '| --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 6 | 0.7 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.7 |'
- en: '| 1 | 0.8 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.8 |'
- en: '| 45 | 0.99 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 45 | 0.99 |'
- en: '| 44 | No match found |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 44 | 未找到匹配项 |'
- en: Two of the MCA entities matched to two of the Companies House entities.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 MCA 实体与两个 Companies House 实体匹配。
- en: Looking back to [Figure 9-11](#fig-9-11), we can see the first seven matches
    in ascending order of confidence. You can see that the entity reconciliation service
    has been able to match these entities in spite of minor spelling differences or
    postcode variations. The remainder are exact matches on both `CompanyName` and
    `Postcode` with the exception of a mismatched hyphen between INDIE PEARL and INDIE-PEARL,
    which has not affected the confidence score.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[图 9-11](#fig-9-11)，我们可以看到按置信度升序排列的前七个匹配项。您可以看到，尽管存在轻微的拼写差异或邮政编码变化，实体对账服务已能够匹配这些实体。其余的匹配项在`CompanyName`和`Postcode`上都是精确匹配，除了INDIE
    PEARL和INDIE-PEARL之间的连字符不匹配，但这并没有影响置信度分数。
- en: 'If we assume that the unique matches are true positive matches and that the
    two additional matches are false positives, then we can evaluate our performance
    as:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设独特的匹配是真正的正匹配，并且另外两个匹配是误报，则我们可以评估我们的表现如下：
- en: <math alttext="upper T r u e p o s i t i v e m a t c h e s left-parenthesis
    upper T upper P right-parenthesis equals 52"><mrow><mi>T</mi> <mi>r</mi> <mi>u</mi>
    <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi>
    <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi> <mi>s</mi>
    <mo>(</mo> <mi>T</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo> <mn>52</mn></mrow></math>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper T r u e p o s i t i v e m a t c h e s left-parenthesis
    upper T upper P right-parenthesis equals 52"><mrow><mi>T</mi> <mi>r</mi> <mi>u</mi>
    <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi>
    <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi> <mi>s</mi>
    <mo>(</mo> <mi>T</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo> <mn>52</mn></mrow></math>
- en: <math alttext="upper F a l s e p o s i t i v e m a t c h e s left-parenthesis
    upper F upper P right-parenthesis equals 2"><mrow><mi>F</mi> <mi>a</mi> <mi>l</mi>
    <mi>s</mi> <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo> <mn>2</mn></mrow></math>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper F a l s e p o s i t i v e m a t c h e s left-parenthesis
    upper F upper P right-parenthesis equals 2"><mrow><mi>F</mi> <mi>a</mi> <mi>l</mi>
    <mi>s</mi> <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo> <mn>2</mn></mrow></math>
- en: <math alttext="upper F a l s e n e g a t i v e m a t c h e s left-parenthesis
    upper F upper N right-parenthesis equals 44"><mrow><mi>F</mi> <mi>a</mi> <mi>l</mi>
    <mi>s</mi> <mi>e</mi> <mi>n</mi> <mi>e</mi> <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>N</mi> <mo>)</mo> <mo>=</mo> <mn>44</mn></mrow></math>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper F a l s e n e g a t i v e m a t c h e s left-parenthesis
    upper F upper N right-parenthesis equals 44"><mrow><mi>F</mi> <mi>a</mi> <mi>l</mi>
    <mi>s</mi> <mi>e</mi> <mi>n</mi> <mi>e</mi> <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>N</mi> <mo>)</mo> <mo>=</mo> <mn>44</mn></mrow></math>
- en: <math alttext="upper P r e c i s i o n equals StartFraction upper T upper P
    Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction
    equals StartFraction 52 Over left-parenthesis 52 plus 2 right-parenthesis EndFraction
    almost-equals 96 percent-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>c</mi>
    <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>2</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>96</mn> <mo>%</mo></mrow></math>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P r e c i s i o n equals StartFraction upper T upper P
    Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction
    equals StartFraction 52 Over left-parenthesis 52 plus 2 right-parenthesis EndFraction
    almost-equals 96 percent-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>c</mi>
    <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>2</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>96</mn> <mo>%</mo></mrow></math>
- en: <math alttext="upper R e c a l l equals StartFraction upper T upper P Over left-parenthesis
    upper T upper P plus upper F upper N right-parenthesis EndFraction equals StartFraction
    52 Over left-parenthesis 52 plus 44 right-parenthesis EndFraction almost-equals
    54.2 percent-sign"><mrow><mi>R</mi> <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi>
    <mi>l</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>44</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>54</mn> <mo>.</mo> <mn>2</mn> <mo>%</mo></mrow></math>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R e c a l l equals StartFraction upper T upper P Over left-parenthesis
    upper T upper P plus upper F upper N right-parenthesis EndFraction equals StartFraction
    52 Over left-parenthesis 52 plus 44 right-parenthesis EndFraction almost-equals
    54.2 percent-sign"><mrow><mi>R</mi> <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi>
    <mi>l</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>44</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>54</mn> <mo>.</mo> <mn>2</mn> <mo>%</mo></mrow></math>
- en: So the entity reconciliation gives us excellent precision but with relatively
    poor recall.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 所以实体对账为我们提供了极好的精确度，但相对较差的召回率。
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter we have seen how to use the Google Cloud Entity Reconciliation
    API to resolve our organization entities. We have seen how to configure and run
    matching jobs from both the cloud console and via the Python client.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到如何使用 Google Cloud 实体对账 API 解决我们的组织实体。我们还看到如何从云控制台和 Python 客户端配置和运行匹配作业。
- en: Using the API abstracts us away from much of the complexity of configuring our
    own matching process. It is also inherently scalable to very large datasets (hundreds
    of millions of rows). However, we are constrained to using a set of predefined
    schemas and we don’t have the freedom to tune the matching algorithm to optimize
    the recall/precision trade-off for our use case.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 API 抽象我们远离了配置自己的匹配过程的复杂性。它也天然适用于非常大的数据集（数亿行）。但是，我们受到使用一组预定义模式的限制，并且没有自由调整匹配算法以优化我们用例的召回率/精确度权衡。
