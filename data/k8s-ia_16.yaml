- en: Chapter 14\. Managing pods’ computational resources
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第14章 管理Pod的计算资源
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Requesting CPU, memory, and other computational resources for containers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为容器请求CPU、内存和其他计算资源
- en: Setting a hard limit for CPU and memory
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为CPU和内存设置硬性限制
- en: Understanding Quality of Service guarantees for pods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Pod服务质量保证
- en: Setting default, min, and max resources for pods in a namespace
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在命名空间中为Pod设置默认、最小和最大资源
- en: Limiting the total amount of resources available in a namespace
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制命名空间中可用的总资源量
- en: Up to now you’ve created pods without caring about how much CPU and memory they’re
    allowed to consume. But as you’ll see in this chapter, setting both how much a
    pod is expected to consume and the maximum amount it’s allowed to consume is a
    vital part of any pod definition. Setting these two sets of parameters makes sure
    that a pod takes only its fair share of the resources provided by the Kubernetes
    cluster and also affects how pods are scheduled across the cluster.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你创建Pod时并没有关心它们可以消耗多少CPU和内存。但正如你将在本章中看到的，设置Pod预期消耗的量和最大允许消耗的量是任何Pod定义的关键部分。设置这两组参数确保Pod只占用Kubernetes集群提供的资源中的公平份额，并影响Pod在集群中的调度方式。
- en: 14.1\. Requesting resources for a pod’s containers
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 14.1\. 为Pod的容器请求资源
- en: When creating a pod, you can specify the amount of CPU and memory that a container
    needs (these are called requests) and a hard limit on what it may consume (known
    as limits). They’re specified for each container individually, not for the pod
    as a whole. The pod’s resource requests and limits are the sum of the requests
    and limits of all its containers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建一个Pod时，你可以指定容器需要的CPU和内存量（这些被称为请求）以及它可能消耗的硬性限制（称为限制）。这些限制是针对每个容器单独指定的，而不是针对整个Pod。Pod的资源请求和限制是所有容器请求和限制的总和。
- en: 14.1.1\. Creating pods with resource requests
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 14.1.1\. 使用资源请求创建Pod
- en: Let’s look at an example pod manifest, which has the CPU and memory requests
    specified for its single container, as shown in the following listing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个示例Pod清单，它为其单个容器指定了CPU和内存请求，如下所示。
- en: 'Listing 14.1\. A pod with resource requests: requests-pod.yaml'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1\. 具有资源请求的Pod：requests-pod.yaml
- en: '`apiVersion: v1 kind: Pod metadata:   name: requests-pod spec:   containers:
      - image: busybox     command: ["dd", "if=/dev/zero", "of=/dev/null"]     name:
    main` `1` `resources:` `1` `requests:` `1` `cpu: 200m` `2` `memory: 10Mi` `3`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata:   name: requests-pod spec:   containers:
      - image: busybox     command: ["dd", "if=/dev/zero", "of=/dev/null"]     name:
    main` `1` `resources:` `1` `requests:` `1` `cpu: 200m` `2` `memory: 10Mi` `3`'
- en: 1 You’re specifying resource requests for the main container.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 你正在为主容器指定资源请求。
- en: 2 The container requests 200 millicores (that is, 1/5 of a single CPU core’s
    time).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 容器请求200毫核（即单个CPU核心时间的1/5）。
- en: 3 The container also requests 10 mebibytes of memory.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 容器还请求10兆字节（mebibytes）的内存。
- en: In the pod manifest, your single container requires one-fifth of a CPU core
    (200 millicores) to run properly. Five such pods/containers can run sufficiently
    fast on a single CPU core.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pod清单中，你的单个容器需要CPU核心的五分之一（200毫核）来正常运行。五个这样的Pod/容器可以在单个CPU核心上足够快地运行。
- en: When you don’t specify a request for CPU, you’re saying you don’t care how much
    CPU time the process running in your container is allotted. In the worst case,
    it may not get any CPU time at all (this happens when a heavy demand by other
    processes exists on the CPU). Although this may be fine for low-priority batch
    jobs, which aren’t time-critical, it obviously isn’t appropriate for containers
    handling user requests.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当你没有指定CPU的请求时，你表示你不在乎运行在容器中的进程被分配了多少CPU时间。在最坏的情况下，它可能根本得不到任何CPU时间（这发生在CPU上有其他进程的重需求时）。虽然这可能适用于低优先级的批处理作业，这些作业不是时间敏感的，但它显然不适用于处理用户请求的容器。
- en: In the pod spec, you’re also requesting 10 mebibytes of memory for the container.
    By doing that, you’re saying that you expect the processes running inside the
    container to use at most 10 mebibytes of RAM. They might use less, but you’re
    not expecting them to use more than that in normal circumstances. Later in this
    chapter you’ll see what happens if they do.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pod规范中，你还在容器中请求了10兆字节（mebibytes）的内存。通过这样做，你表示你期望容器内运行的进程最多使用10兆字节的RAM。它们可能使用更少，但你并不期望它们在正常情况下使用超过这个量。稍后在本章中，你将看到如果它们这样做会发生什么。
- en: Now you’ll run the pod. When the pod starts, you can take a quick look at the
    process’ CPU consumption by running the `top` command inside the container, as
    shown in the following listing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将运行 pod。当 pod 启动时，您可以通过在容器内运行 `top` 命令来快速查看进程的 CPU 消耗，如下所示。
- en: Listing 14.2\. Examining CPU and memory usage from within a container
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.2\. 从容器内部检查 CPU 和内存使用情况
- en: '`$ kubectl exec -it requests-pod top` `Mem: 1288116K used, 760368K free, 9196K
    shrd, 25748K buff, 814840K cached CPU:  9.1% usr 42.1% sys  0.0% nic 48.4% idle 
    0.0% io  0.0% irq  0.2% sirq Load average: 0.79 0.52 0.29 2/481 10   PID  PPID
    USER     STAT   VSZ %VSZ CPU %CPU COMMAND     1     0 root     R     1192  0.0  
    1` `50.2` `dd if /dev/zero of /dev/null     7     0 root     R     1200  0.0  
    0  0.0 top`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec -it requests-pod top` `内存：1288116K 已使用，760368K 可用，9196K 共享，25748K
    缓冲，814840K 缓存 CPU：9.1% 用户 42.1% 系统 0.0% 网络接口 48.4% 空闲 0.0% I/O 0.0% 中断 0.2% 软中断
    负载平均：0.79 0.52 0.29 2/481 10 PID PPID 用户 STAT VSZ %VSZ CPU %CPU 命令 1 0 root R
    1192 0.0 1` `50.2` `dd if /dev/zero of /dev/null 7 0 root R 1200 0.0 0 0.0 top`'
- en: The `dd` command you’re running in the container consumes as much CPU as it
    can, but it only runs a single thread so it can only use a single core. The Minikube
    VM, which is where this example is running, has two CPU cores allotted to it.
    That’s why the process is shown consuming 50% of the whole CPU.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您在容器中运行的 `dd` 命令会尽可能多地消耗 CPU，但它只运行一个线程，因此它只能使用一个核心。Minikube 虚拟机，这是本例运行的地方，分配给它两个
    CPU 核心。这就是为什么进程显示消耗了整个 CPU 的 50%。
- en: Fifty percent of two cores is obviously one whole core, which means the container
    is using more than the 200 millicores you requested in the pod specification.
    This is expected, because requests don’t limit the amount of CPU a container can
    use. You’d need to specify a CPU limit to do that. You’ll try that later, but
    first, let’s see how specifying resource requests in a pod affects the scheduling
    of the pod.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 两个核心的百分之五十显然是一个完整的核心，这意味着容器正在使用超过您在 pod 规范中请求的 200 millicores。这是预期的，因为请求并不限制容器可以使用的
    CPU 量。您需要指定 CPU 限制才能做到这一点。您稍后会尝试这样做，但首先，让我们看看在 pod 中指定资源请求如何影响 pod 的调度。
- en: 14.1.2\. Understanding how resource requests affect scheduling
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 14.1.2\. 理解资源请求如何影响调度
- en: By specifying resource requests, you’re specifying the minimum amount of resources
    your pod needs. This information is what the Scheduler uses when scheduling the
    pod to a node. Each node has a certain amount of CPU and memory it can allocate
    to pods. When scheduling a pod, the Scheduler will only consider nodes with enough
    unallocated resources to meet the pod’s resource requirements. If the amount of
    unallocated CPU or memory is less than what the pod requests, Kubernetes will
    not schedule the pod to that node, because the node can’t provide the minimum
    amount required by the pod.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定资源请求，您指定了 pod 需要的最小资源量。调度器在将 pod 调度到节点时使用这些信息。每个节点可以分配给 pod 一定数量的 CPU 和内存。在调度
    pod 时，调度器将只考虑有足够未分配资源来满足 pod 资源要求的节点。如果未分配的 CPU 或内存量小于 pod 请求的量，Kubernetes 不会将
    pod 调度到该节点，因为节点无法提供 pod 所需的最小量。
- en: Understanding how the Scheduler determines if a pod can fit on a node
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 理解调度器如何确定 pod 是否可以放置在节点上
- en: What’s important and somewhat surprising here is that the Scheduler doesn’t
    look at how much of each individual resource is being used at the exact time of
    scheduling but at the sum of resources requested by the existing pods deployed
    on the node. Even though existing pods may be using less than what they’ve requested,
    scheduling another pod based on actual resource consumption would break the guarantee
    given to the already deployed pods.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是并且有些令人惊讶的是，调度器在调度时不会查看每个单独的资源在确切时间点使用了多少，而是查看节点上已部署的 pod 请求的资源总和。即使现有
    pod 可能使用的资源少于它们请求的量，基于实际资源消耗来调度另一个 pod 也会破坏对已部署 pod 给出的保证。
- en: This is visualized in [figure 14.1](#filepos1333248). Three pods are deployed
    on the node. Together, they’ve requested 80% of the node’s CPU and 60% of the
    node’s memory. Pod D, shown at the bottom right of the figure, cannot be scheduled
    onto the node because it requests 25% of the CPU, which is more than the 20% of
    unallocated CPU. The fact that the three pods are currently using only 70% of
    the CPU makes no difference.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这在[图 14.1](#filepos1333248)中得到了可视化。节点上部署了三个 pod。总共，它们请求了节点 CPU 的 80% 和内存的 60%。图中最右下角的
    pod D 不能调度到该节点，因为它请求了 25% 的 CPU，这超过了未分配 CPU 的 20%。这三个 pod 当前只使用了 70% 的 CPU 的事实无关紧要。
- en: Figure 14.1\. The Scheduler only cares about requests, not actual usage.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1. 调度器只关心请求，而不是实际使用情况。
- en: '![](images/00081.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00081.jpg)'
- en: Understanding how the Scheduler uses pods’ requests when selectin- ng the best
    node for a pod
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 理解调度器在为Pod选择最佳节点时如何使用Pod的请求
- en: 'You may remember from [chapter 11](index_split_087.html#filepos1036287) that
    the Scheduler first filters the list of nodes to exclude those that the pod can’t
    fit on and then prioritizes the remaining nodes per the configured prioritization
    functions. Among others, two prioritization functions rank nodes based on the
    amount of resources requested: `LeastRequestedPriority` and `MostRequestedPriority`.
    The first one prefers nodes with fewer requested resources (with a greater amount
    of unallocated resources), whereas the second one is the exact opposite—it prefers
    nodes that have the most requested resources (a smaller amount of unallocated
    CPU and memory). But, as we’ve discussed, they both consider the amount of requested
    resources, not the amount of resources actually consumed.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得从[第11章](index_split_087.html#filepos1036287)中提到的，调度器首先过滤掉那些Pod无法放置的节点列表，然后根据配置的优先级函数对剩余节点进行排序。其中，两个优先级函数根据请求的资源量对节点进行排名：`LeastRequestedPriority`和`MostRequestedPriority`。第一个优先级函数更倾向于具有较少请求资源（具有更多未分配资源）的节点，而第二个优先级函数则正好相反——它更倾向于具有最多请求资源（较少未分配CPU和内存）的节点。但是，正如我们之前讨论的，它们都考虑了请求的资源量，而不是实际消耗的资源量。
- en: The Scheduler is configured to use only one of those functions. You may wonder
    why anyone would want to use the `MostRequestedPriority` function. After all,
    if you have a set of nodes, you usually want to spread CPU load evenly across
    them. However, that’s not the case when running on cloud infrastructure, where
    you can add and remove nodes whenever necessary. By configuring the Scheduler
    to use the `Most-RequestedPriority` function, you guarantee that Kubernetes will
    use the smallest possible number of nodes while still providing each pod with
    the amount of CPU/memory it requests. By keeping pods tightly packed, certain
    nodes are left vacant and can be removed. Because you’re paying for individual
    nodes, this saves you money.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器被配置为只使用这些功能中的一个。你可能想知道为什么有人会想使用`MostRequestedPriority`函数。毕竟，如果你有一组节点，你通常希望在这些节点之间均匀地分配CPU负载。然而，在云基础设施上运行时并非如此，你可以随时添加和删除节点。通过配置调度器使用`Most-RequestedPriority`函数，你可以确保Kubernetes在使用节点时，仍然能够为每个Pod提供其请求的CPU/内存量，同时使用尽可能少的节点。通过紧密打包Pod，某些节点会空闲出来，可以被移除。因为你为单个节点付费，这可以为你节省费用。
- en: Inspecting a node’s capacity
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 检查节点的容量
- en: Let’s see the Scheduler in action. You’ll deploy another pod with four times
    the amount of requested resources as before. But before you do that, let’s see
    your node’s capacity. Because the Scheduler needs to know how much CPU and memory
    each node has, the Kubelet reports this data to the API server, making it available
    through the Node resource. You can see it by using the `kubectl describe` command
    as in the following listing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看调度器是如何工作的。你将部署一个Pod，其请求的资源量是之前的四倍。但在你这样做之前，让我们看看你的节点容量。因为调度器需要知道每个节点有多少CPU和内存，Kubelet会将这些数据报告给API服务器，使其通过节点资源可用。你可以通过使用`kubectl
    describe`命令来查看，如下面的列表所示。
- en: Listing 14.3\. A node’s capacity and allocatable resources
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.3. 节点的容量和可分配资源
- en: '`$ kubectl describe nodes` `Name:       minikube ... Capacity:` `1` `cpu:          
    2` `1` `memory:        2048484Ki` `1` `pods:          110` `1` `Allocatable:`
    `2` `cpu:           2` `2` `memory:        1946084Ki` `2` `pods:          110`
    `2` `...`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe nodes` `Name:       minikube ... Capacity:` `1` `cpu:          
    2` `1` `memory:        2048484Ki` `1` `pods:          110` `1` `Allocatable:`
    `2` `cpu:           2` `2` `memory:        1946084Ki` `2` `pods:          110`
    `2` `...`'
- en: 1 The overall capacity of the node
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 节点的总体容量
- en: 2 The resources allocatable to pods
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 可分配给Pod的资源
- en: 'The output shows two sets of amounts related to the available resources on
    the node: the node’s capacity and allocatable resources. The capacity represents
    the total resources of a node, which may not all be available to pods. Certain
    resources may be reserved for Kubernetes and/or system components. The Scheduler
    bases its decisions only on the allocatable resource amounts.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了与节点上可用资源相关的两组金额：节点的容量和可分配资源。容量代表节点的总资源，其中可能并非所有资源都对Pod可用。某些资源可能被保留给Kubernetes和/或系统组件。调度器仅基于可分配资源量做出决策。
- en: In the previous example, the node called `minikube` runs in a VM with two cores
    and has no CPU reserved, making the whole CPU allocatable to pods. Therefore,
    the Scheduler should have no problem scheduling another pod requesting 800 millicores.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，名为`minikube`的节点在一个具有两个核心的虚拟机上运行，没有预留CPU，因此整个CPU都可以分配给Pod。因此，调度器应该没有问题安排另一个请求800毫核的Pod。
- en: 'Run the pod now. You can use the YAML file in the code archive, or run the
    pod with the `kubectl run` command like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行Pod。你可以使用代码存档中的YAML文件，或者使用以下`kubectl run`命令运行Pod：
- en: '`$ kubectl run requests-pod-2 --image=busybox --restart Never`![](images/00006.jpg)`--requests=''cpu=800m,memory=20Mi''
    -- dd if=/dev/zero of=/dev/null` `pod "requests-pod-2" created`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run requests-pod-2 --image=busybox --restart Never`![](images/00006.jpg)`--requests=''cpu=800m,memory=20Mi''
    -- dd if=/dev/zero of=/dev/null` `pod "requests-pod-2" created`'
- en: 'Let’s see if it was scheduled:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否已经安排好了：
- en: '`$ kubectl get po requests-pod-2` `NAME             READY     STATUS    RESTARTS  
    AGE requests-pod-2   1/1       Running   0          3m`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po requests-pod-2` `NAME             READY     STATUS    RESTARTS  
    AGE requests-pod-2   1/1       Running   0          3m`'
- en: Okay, the pod has been scheduled and is running.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，Pod已经被安排并正在运行。
- en: Creating a pod that doesn’t fit on any node
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个无法在任何节点上运行的Pod
- en: 'You now have two pods deployed, which together have requested a total of 1,000
    millicores or exactly 1 core. You should therefore have another 1,000 millicores
    available for additional pods, right? You can deploy another pod with a resource
    request of 1,000 millicores. Use a similar command as before:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在有两个Pod已部署，它们总共请求了1,000毫核或正好1个核心。因此，你应该还有1,000毫核可用于其他Pod，对吧？你可以部署另一个请求1,000毫核资源的Pod。使用与之前类似的命令：
- en: '`$ kubectl run requests-pod-3 --image=busybox --restart Never`![](images/00006.jpg)`--requests=''cpu=1,memory=20Mi''
    -- dd if=/dev/zero of=/dev/null` `pod "requests-pod-2" created`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run requests-pod-3 --image=busybox --restart Never`![](images/00006.jpg)`--requests=''cpu=1,memory=20Mi''
    -- dd if=/dev/zero of=/dev/null` `pod "requests-pod-2" created`'
- en: '|  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This time you’re specifying the CPU request in whole cores (`cpu=1`) instead
    of millicores (`cpu=1000m`).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这次你指定的是整核CPU请求（`cpu=1`），而不是毫核（`cpu=1000m`）。
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'So far, so good. The pod has been accepted by the API server (you’ll remember
    from the previous chapter that the API server can reject pods if they’re invalid
    in any way). Now, check if the pod is running:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。Pod已被API服务器接受（你可能会记得，在前一章中，如果Pod以任何方式无效，API服务器可以拒绝Pod）。现在，检查Pod是否正在运行：
- en: '`$ kubectl get po requests-pod-3` `NAME             READY     STATUS    RESTARTS  
    AGE requests-pod-3   0/1` `Pending``   0          4m`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po requests-pod-3` `NAME             READY     STATUS    RESTARTS  
    AGE requests-pod-3   0/1` `Pending``   0          4m`'
- en: Even if you wait a while, the pod is still stuck at Pending. You can see more
    information on why that’s the case by using the `kubectl describe` command, as
    shown in the following listing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你等待一段时间，Pod仍然处于挂起状态。你可以通过使用`kubectl describe`命令来查看更多关于这种情况的信息，如下所示。
- en: Listing 14.4\. Examining why a pod is stuck at Pending with `kubectl describe
    pod`
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.4. 使用`kubectl describe pod`检查Pod为什么处于挂起状态
- en: '`$ kubectl describe po requests-pod-3` `Name:       requests-pod-3 Namespace: 
    default Node:       /` `1` `... Conditions:   Type           Status   PodScheduled  
    False` `2` `... Events: ... Warning` `FailedScheduling``No nodes are available`
    `3` `that match all of the` `3` `following predicates::` `3``Insufficient cpu`
    `(1).` `3`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe po requests-pod-3` `Name:       requests-pod-3 Namespace: 
    default Node:       /` `1` `... Conditions:   Type           Status   PodScheduled  
    False` `2` `... Events: ... Warning` `FailedScheduling``No nodes are available`
    `3` `that match all of the` `3` `following predicates::` `3``Insufficient cpu`
    `(1).` `3`'
- en: 1 No node is associated with the pod.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 没有节点与该Pod关联。
- en: 2 The pod hasn’t been scheduled.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 Pod没有被安排。
- en: 3 Scheduling has failed because of insufficient CPU.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 由于CPU不足，调度失败。
- en: The output shows that the pod hasn’t been scheduled because it can’t fit on
    any node due to insufficient CPU on your single node. But why is that? The sum
    of the CPU requests of all three pods equals 2,000 millicores or exactly two cores,
    which is exactly what your node can provide. What’s wrong?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示Pod尚未被安排，因为它由于你的单个节点上CPU不足而无法在任何节点上运行。但为什么会这样呢？所有三个Pod的CPU请求总和为2,000毫核或正好两个核心，这正是你的节点可以提供的。问题出在哪里？
- en: Determining why a pod isn’t being scheduled
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 确定Pod为什么没有被安排
- en: You can figure out why the pod isn’t being scheduled by inspecting the node
    resource. Use the `kubectl describe node` command again and examine the output
    more closely in the following listing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查节点资源来找出 pod 为什么没有被调度。再次使用 `kubectl describe node` 命令，并仔细检查以下列表的输出。
- en: Listing 14.5\. Inspecting allocated resources on a node with `kubectl describe
    node`
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.5\. 使用 `kubectl describe node` 检查节点上的已分配资源
- en: '`$ kubectl describe node` `Name:                   minikube ... Non-terminated
    Pods:    (7 in total)   Namespace    Name            CPU Requ.   CPU Lim.  Mem
    Req.    Mem Lim.   ---------    ----            ----------  --------  ---------  
    --------   default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)
      default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)   kube-system 
    dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)   kube-system  kube-addon-... 
    5m (0%)     0 (0%)    50Mi (2%)   0 (0%)   kube-system  kube-dns-26...  260m (13%) 
    0 (0%)    110Mi (5%)  170Mi (8%)   kube-system  kubernetes-...  0 (0%)      0
    (0%)    0 (0%)      0 (0%)   kube-system  nginx-ingre...  0 (0%)      0 (0%)   
    0 (0%)      0 (0%)` `Allocated resources:` `(Total limits may be over 100 percent,
    i.e., overcommitted.)` `CPU Requests``CPU Limits      Memory Requests Memory Limits
      ------------  ----------      --------------- -------------` `1275m (63%)``  
    10m (0%)        210Mi (11%)     190Mi (9%)`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe node` `Name:                   minikube ... 非终止 Pods:   
    (总共 7 个)   Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem
    Lim.   ---------    ----            ----------  --------  ---------   --------
      default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)   default     
    requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)   kube-system  dflt-http-b... 
    10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)   kube-system  kube-addon-...  5m
    (0%)     0 (0%)    50Mi (2%)   0 (0%)   kube-system  kube-dns-26...  260m (13%) 
    0 (0%)    110Mi (5%)  170Mi (8%)   kube-system  kubernetes-...  0 (0%)      0
    (0%)    0 (0%)      0 (0%)   kube-system  nginx-ingre...  0 (0%)      0 (0%)   
    0 (0%)      0 (0%)` `已分配资源:` `(总限制可能超过 100%，即超配。)` `CPU 请求``CPU 限制      内存请求 内存限制
      ------------  ----------      --------------- -------------` `1275m (63%)``  
    10m (0%)        210Mi (11%)     190Mi (9%)`'
- en: If you look at the bottom left of the listing, you’ll see a total of 1,275 millicores
    have been requested by the running pods, which is 275 millicores more than what
    you requested for the first two pods you deployed. Something is eating up additional
    CPU resources.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看列表的左下角，你会看到正在运行的 pods 请求了总共 1,275 个毫核心，比你部署的前两个 pods 请求的多了 275 个毫核心。有些东西正在消耗额外的
    CPU 资源。
- en: You can find the culprit in the list of pods in the previous listing. Three
    pods in the `kube-system` namespace have explicitly requested CPU resources. Those
    pods plus your two pods leave only 725 millicores available for additional pods.
    Because your third pod requested 1,000 millicores, the Scheduler won’t schedule
    it to this node, as that would make the node overcommitted.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上一列表中的 pods 列表中找到罪魁祸首。在 `kube-system` 命名空间中有三个 pods 明确请求了 CPU 资源。这些 pods
    加上你的两个 pods，只剩下 725 个毫核心可用于额外的 pods。因为你的第三个 pod 请求了 1,000 个毫核心，调度器不会将其调度到这个节点，因为这会使节点超配。
- en: Freeing resources to get the pod scheduled
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 释放资源以使 pod 被调度
- en: The pod will only be scheduled when an adequate amount of CPU is freed (when
    one of the first two pods is deleted, for example). If you delete your second
    pod, the Scheduler will be notified of the deletion (through the watch mechanism
    described in [chapter 11](index_split_087.html#filepos1036287)) and will schedule
    your third pod as soon as the second pod terminates. This is shown in the following
    listing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当释放足够的 CPU 资源时（例如删除前两个 pods 中的一个），pod 才会被调度。如果你删除第二个 pod，调度器会通过 [第 11 章](index_split_087.html#filepos1036287)
    中描述的监视机制通知删除操作，并在第二个 pod 终止后立即调度你的第三个 pod。这在下述列表中显示。
- en: Listing 14.6\. Pod is scheduled after deleting another pod
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.6\. 删除另一个 pod 后 pod 被调度
- en: '`$ kubectl delete po requests-pod-2` `pod "requests-pod-2" deleted` `$ kubectl
    get po` `NAME             READY     STATUS        RESTARTS   AGE requests-pod    
    1/1       Running       0          2h requests-pod-2   1/1` `Terminating``0         
    1h requests-pod-3   0/1` `Pending``0          1h` `$ kubectl get po` `NAME            
    READY     STATUS    RESTARTS   AGE requests-pod     1/1       Running   0         
    2h requests-pod-3   1/1` `Running``   0          1h`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po requests-pod-2` `pod "requests-pod-2" 已删除` `$ kubectl
    get po` `NAME             READY     STATUS        RESTARTS   AGE requests-pod    
    1/1       运行中       0          2h requests-pod-2   1/1` `正在终止``0          1h requests-pod-3  
    0/1` `挂起``0          1h` `$ kubectl get po` `NAME             READY     STATUS   
    RESTARTS   AGE requests-pod     1/1       运行中   0          2h requests-pod-3  
    1/1` `运行中``   0          1h`'
- en: In all these examples, you’ve specified a request for memory, but it hasn’t
    played any role in the scheduling because your node has more than enough allocatable
    memory to accommodate all your pods’ requests. Both CPU and memory requests are
    treated the same way by the Scheduler, but in contrast to memory requests, a pod’s
    CPU requests also play a role elsewhere—while the pod is running. You’ll learn
    about this next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些示例中，你指定了内存请求，但在调度中它并没有发挥作用，因为你的节点有足够的可分配内存来容纳所有Pod的请求。调度器以相同的方式处理CPU和内存请求，但与内存请求不同，Pod的CPU请求在Pod运行时也发挥作用。你将在下一节中了解这一点。
- en: 14.1.3\. Understanding how CPU requests affect CPU time sharing
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 14.1.3. 理解CPU请求如何影响CPU时间共享
- en: You now have two pods running in your cluster (you can disregard the system
    pods right now, because they’re mostly idle). One has requested 200 millicores
    and the other one five times as much. At the beginning of the chapter, we said
    Kubernetes distinguishes between resource requests and limits. You haven’t defined
    any limits yet, so the two pods are in no way limited when it comes to how much
    CPU they can each consume. If the process inside each pod consumes as much CPU
    time as it can, how much CPU time does each pod get?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，你的集群中有两个Pod正在运行（你可以暂时忽略系统Pod，因为它们大部分是空闲的）。一个请求了200毫芯，另一个请求了五倍于此。在章节开头，我们说Kubernetes区分资源请求和限制。你还没有定义任何限制，所以这两个Pod在CPU消耗方面没有任何限制。如果每个Pod内部的进程尽可能多地消耗CPU时间，每个Pod将获得多少CPU时间？ '
- en: The CPU requests don’t only affect scheduling—they also determine how the remaining
    (unused) CPU time is distributed between pods. Because your first pod requested
    200 millicores of CPU and the other one 1,000 millicores, any unused CPU will
    be split among the two pods in a 1 to 5 ratio, as shown in [figure 14.2](#filepos1348606).
    If both pods consume as much CPU as they can, the first pod will get one sixth
    or 16.7% of the CPU time and the other one the remaining five sixths or 83.3%.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CPU请求不仅影响调度，还决定了剩余（未使用）的CPU时间如何在Pod之间分配。因为你的第一个Pod请求了200毫芯的CPU，而另一个请求了1000毫芯，任何未使用的CPU将以1到5的比例在两个Pod之间分配，如[图14.2](#filepos1348606)所示。如果两个Pod都尽可能多地消耗CPU，第一个Pod将获得六分之一或16.7%的CPU时间，而另一个Pod将获得剩余的五分之六或83.3%。
- en: Figure 14.2\. Unused CPU time is distributed to containers based on their CPU
    requests.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.2. 未使用的CPU时间根据其CPU请求分配给容器。](images/00099.jpg)'
- en: '![](images/00099.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00099.jpg)'
- en: But if one container wants to use up as much CPU as it can, while the other
    one is sitting idle at a given moment, the first container will be allowed to
    use the whole CPU time (minus the small amount of time used by the second container,
    if any). After all, it makes sense to use all the available CPU if no one else
    is using it, right? As soon as the second container needs CPU time, it will get
    it and the first container will be throttled back.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果一个容器想要尽可能多地使用CPU，而另一个容器在某个时刻处于空闲状态，第一个容器将被允许使用整个CPU时间（减去第二个容器使用的少量时间，如果有的话）。毕竟，如果没有人使用，使用所有可用的CPU是有意义的，对吧？一旦第二个容器需要CPU时间，它将获得它，第一个容器将被限制。
- en: 14.1.4\. Defining and requesting custom resources
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 14.1.4. 定义和请求自定义资源
- en: Kubernetes also allows you to add your own custom resources to a node and request
    them in the pod’s resource requests. Initially these were known as Opaque Integer
    Resources, but were replaced with Extended Resources in Kubernetes version 1.8\.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还允许你向节点添加自己的自定义资源，并在Pod的资源请求中请求它们。最初这些被称为不透明整数资源，但在Kubernetes 1.8版本中被扩展资源所取代。
- en: First, you obviously need to make Kubernetes aware of your custom resource by
    adding it to the Node object’s `capacity` field. This can be done by performing
    a `PATCH` HTTP request. The resource name can be anything, such as `example.org/my-resource`,
    as long as it doesn’t start with the `kubernetes.io` domain. The quantity must
    be an integer (for example, you can’t set it to 100 millis, because 0.1 isn’t
    an integer; but you can set it to 1000m or 2000m or, simply, 1 or 2). The value
    will be copied from the `capacity` to the `allocatable` field automatically.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，显然你需要通过将其添加到节点对象的`capacity`字段中，让Kubernetes知道你的自定义资源。这可以通过执行`PATCH` HTTP请求来完成。资源名称可以是任何东西，例如`example.org/my-resource`，只要它不以`kubernetes.io`域开头。数量必须是一个整数（例如，你不能将其设置为100毫秒，因为0.1不是一个整数；但你可以将其设置为1000m或2000m，或者简单地设置为1或2）。值将自动从`capacity`复制到`allocatable`字段。
- en: Then, when creating pods, you specify the same resource name and the requested
    quantity under the `resources.requests` field in the container spec or with `--requests`
    when using `kubectl run` like you did in previous examples. The Scheduler will
    make sure the pod is only deployed to a node that has the requested amount of
    the custom resource available. Every deployed pod obviously reduces the number
    of allocatable units of the resource.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在创建 Pod 时，你可以在容器规范中的 `resources.requests` 字段下指定相同的资源名称和请求的数量，或者在像之前示例中那样使用
    `kubectl run` 时使用 `--requests`。调度器将确保 Pod 只部署到具有所需自定义资源数量的节点。显然，每个部署的 Pod 都会减少资源的可分配单元数量。
- en: An example of a custom resource could be the number of GPU units available on
    the node. Pods requiring the use of a GPU specify that in their requests. The
    Scheduler then makes sure the pod is only scheduled to nodes with at least one
    GPU still unallocated.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源的例子可以是节点上可用的 GPU 单元数量。需要使用 GPU 的 Pod 在其请求中指定这一点。然后调度器确保 Pod 只被调度到至少有一个未分配
    GPU 的节点。
- en: 14.2\. Limiting resources available to a container
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 14.2\. 限制容器可用的资源
- en: Setting resource requests for containers in a pod ensures each container gets
    the minimum amount of resources it needs. Now let’s see the other side of the
    coin—the maximum amount the container will be allowed to consume.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 Pod 中为容器设置资源请求确保每个容器都能获得其所需的最小资源量。现在让我们看看硬币的另一面——容器将被允许消耗的最大资源量。
- en: 14.2.1\. Setting a hard limit for the amount of resources a container can use
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 14.2.1\. 为容器可使用的资源数量设置硬限制
- en: We’ve seen how containers are allowed to use up all the CPU if all the other
    processes are sitting idle. But you may want to prevent certain containers from
    using up more than a specific amount of CPU. And you’ll always want to limit the
    amount of memory a container can consume.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，如果其他所有进程都在空闲状态，容器可以消耗掉所有的 CPU。但你可能希望防止某些容器使用超过特定数量的 CPU。而且你总是希望限制容器可以消耗的内存量。
- en: CPU is a compressible resource, which means the amount used by a container can
    be throttled without affecting the process running in the container in an adverse
    way. Memory is obviously different—it’s incompressible. Once a process is given
    a chunk of memory, that memory can’t be taken away from it until it’s released
    by the process itself. That’s why you need to limit the maximum amount of memory
    a container can be given.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 是一种可压缩资源，这意味着容器使用的数量可以被限制，而不会以不利的方式影响容器中运行的进程。内存显然不同——它是不可压缩的。一旦进程被分配了一块内存，这块内存就不能从它那里拿走，直到进程本身释放它。这就是为什么你需要限制容器可以获得的内存最大量。
- en: Without limiting memory, a container (or a pod) running on a worker node may
    eat up all the available memory and affect all other pods on the node and any
    new pods scheduled to the node (remember that new pods are scheduled to the node
    based on the memory requests and not actual memory usage). A single malfunctioning
    or malicious pod can practically make the whole node unusable.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不限制内存，运行在工作节点上的容器（或 Pod）可能会消耗掉所有可用的内存，并影响节点上的所有其他 Pod 以及任何新调度到该节点的 Pod（记住，新
    Pod 是根据内存请求而不是实际内存使用来调度到节点的）。一个故障或恶意 Pod 实际上可以使整个节点无法使用。
- en: Creating a pod with resource limits
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 创建具有资源限制的 Pod
- en: To prevent this from happening, Kubernetes allows you to specify resource limits
    for every container (along with, and virtually in the same way as, resource requests).
    The following listing shows an example pod manifest with resource limits.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况发生，Kubernetes 允许你为每个容器指定资源限制（与资源请求一样，实际上以相同的方式）。以下列表显示了一个具有资源限制的示例 Pod
    清单。
- en: 'Listing 14.7\. A pod with a hard limit on CPU and memory: limited-pod.yaml'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.7\. 具有硬限制的 CPU 和内存的 Pod：limited-pod.yaml
- en: '`apiVersion: v1 kind: Pod metadata:   name: limited-pod spec:   containers:
      - image: busybox     command: ["dd", "if=/dev/zero", "of=/dev/null"]     name:
    main     resources:` `1` `limits:` `1` `cpu: 1` `2` `memory: 20Mi` `3`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata: name: limited-pod spec: containers: - image:
    busybox command: ["dd", "if=/dev/zero", "of=/dev/null"] name: main resources:
    limits: cpu: 1 memory: 20Mi`'
- en: 1 Specifying resource limits for the container
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 为容器指定资源限制
- en: 2 This container will be allowed to use at most 1 CPU core.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 此容器将被允许使用最多 1 个 CPU 核心。
- en: 3 The container will be allowed to use up to 20 mebibytes of memory.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 容器将被允许使用最多 20 兆字节的内存。
- en: This pod’s container has resource limits configured for both CPU and memory.
    The process or processes running inside the container will not be allowed to consume
    more than 1 CPU core and 20 mebibytes of memory.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 pod 的容器为 CPU 和内存都配置了资源限制。容器内运行的进程或进程将不允许消耗超过 1 个 CPU 核心和 20 兆字节的内存。
- en: '|  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Because you haven’t specified any resource requests, they’ll be set to the same
    values as the resource limits.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您没有指定任何资源请求，所以它们将被设置为与资源限制相同的值。
- en: '|  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Overcommitting limits
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 超出限制的过度提交
- en: Unlike resource requests, resource limits aren’t constrained by the node’s allocatable
    resource amounts. The sum of all limits of all the pods on a node is allowed to
    exceed 100% of the node’s capacity ([figure 14.3](#filepos1355578)). Restated,
    resource limits can be overcommitted. This has an important consequence—when 100%
    of the node’s resources are used up, certain containers will need to be killed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与资源请求不同，资源限制不受节点可分配资源数量的约束。节点上所有 pod 的所有限制总和允许超过节点容量的 100% ([图 14.3](#filepos1355578))。重申一下，资源限制可以被过度提交。这有一个重要的后果——当节点资源使用率达到
    100% 时，某些容器将需要被杀死。
- en: Figure 14.3\. The sum of resource limits of all pods on a node can exceed 100%
    of the node’s capacity.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3\. 节点上所有 pod 的资源限制总和可能超过节点容量的 100%。
- en: '![](images/00116.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00116.jpg)'
- en: You’ll see how Kubernetes decides which containers to kill in [section 14.3](index_split_108.html#filepos1366450),
    but individual containers can be killed even if they try to use more than their
    resource limits specify. You’ll learn more about this next.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在[第 14.3 节](index_split_108.html#filepos1366450)中看到 Kubernetes 如何决定杀死哪些容器，但即使容器试图使用超过其资源限制指定的更多资源，也可以杀死单个容器。您将在下一部分了解更多关于这一点。
- en: 14.2.2\. Exceeding the limits
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 14.2.2\. 超出限制
- en: What happens when a process running in a container tries to use a greater amount
    of resources than it’s allowed to?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个在容器中运行的过程试图使用比允许的更多资源时，会发生什么？
- en: You’ve already learned that CPU is a compressible resource, and it’s only natural
    for a process to want to consume all of the CPU time when not waiting for an I/O
    operation. As you’ve learned, a process’ CPU usage is throttled, so when a CPU
    limit is set for a container, the process isn’t given more CPU time than the configured
    limit.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解到 CPU 是一种可压缩的资源，当进程不在等待 I/O 操作时，它想要消耗所有 CPU 时间是很自然的。正如您所学的，进程的 CPU 使用率是受限制的，因此当为容器设置
    CPU 限制时，进程不会获得超过配置限制的更多 CPU 时间。
- en: 'With memory, it’s different. When a process tries to allocate memory over its
    limit, the process is killed (it’s said the container is `OOMKilled`, where OOM
    stands for Out Of Memory). If the pod’s restart policy is set to `Always` or `OnFailure`,
    the process is restarted immediately, so you may not even notice it getting killed.
    But if it keeps going over the memory limit and getting killed, Kubernetes will
    begin restarting it with increasing delays between restarts. You’ll see a `CrashLoopBackOff`
    status in that case:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内存来说，情况不同。当进程试图分配超过其限制的内存时，进程将被杀死（容器被称为 `OOMKilled`，其中 OOM 代表内存不足）。如果 pod
    的重启策略设置为 `Always` 或 `OnFailure`，进程将立即重启，因此您可能甚至没有注意到它被杀死。但如果它继续超过内存限制并被杀死，Kubernetes
    将开始以增加的重启间隔重启它。在这种情况下，您将看到 `CrashLoopBackOff` 状态：
- en: '`$ kubectl get po` `NAME        READY     STATUS             RESTARTS   AGE
    memoryhog   0/1` `CrashLoopBackOff``   3          1m`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME        READY     STATUS             RESTARTS   AGE
    memoryhog   0/1` `CrashLoopBackOff``   3          1m`'
- en: The `CrashLoopBackOff` status doesn’t mean the Kubelet has given up. It means
    that after each crash, the Kubelet is increasing the time period before restarting
    the container. After the first crash, it restarts the container immediately and
    then, if it crashes again, waits for 10 seconds before restarting it again. On
    subsequent crashes, this delay is then increased exponentially to 20, 40, 80,
    and 160 seconds, and finally limited to 300 seconds. Once the interval hits the
    300-second limit, the Kubelet keeps restarting the container indefinitely every
    five minutes until the pod either stops crashing or is deleted.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrashLoopBackOff` 状态并不意味着 Kubelet 已经放弃。这意味着在每次崩溃后，Kubelet 都会增加在重启容器之前的时间间隔。第一次崩溃后，它立即重启容器，然后如果再次崩溃，将等待
    10 秒后再重启它。在随后的崩溃中，这个延迟将以指数方式增加到 20、40、80 和 160 秒，并最终限制为 300 秒。一旦间隔达到 300 秒的限制，Kubelet
    将无限期地每五分钟重启容器，直到 pod 停止崩溃或被删除。'
- en: To examine why the container crashed, you can check the pod’s log and/or use
    the `kubectl describe pod` command, as shown in the following listing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查容器崩溃的原因，你可以检查 pod 的日志和/或使用 `kubectl describe pod` 命令，如下所示。
- en: Listing 14.8\. Inspecting why a container terminated with `kubectl describe
    pod`
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.8\. 检查容器为何以 `kubectl describe pod` 终止
- en: '`$ kubectl describe pod` `Name:       memoryhog ... Containers:   main:    
    ...     State:          Terminated` `1` `Reason:       OOMKilled` `1` `Exit Code:   
    137       Started:      Tue, 27 Dec 2016 14:55:53 +0100       Finished:     Tue,
    27 Dec 2016 14:55:58 +0100     Last State:     Terminated` `2` `Reason:      
    OOMKilled` `2` `Exit Code:    137       Started:      Tue, 27 Dec 2016 14:55:37
    +0100       Finished:     Tue, 27 Dec 2016 14:55:50 +0100     Ready:         
    False ...`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe pod` `Name:       memoryhog ... Containers:   main:    
    ...     State:          Terminated` `1` `Reason:       OOMKilled` `1` `Exit Code:   
    137       Started:      Tue, 27 Dec 2016 14:55:53 +0100       Finished:     Tue,
    27 Dec 2016 14:55:58 +0100     Last State:     Terminated` `2` `Reason:      
    OOMKilled` `2` `Exit Code:    137       Started:      Tue, 27 Dec 2016 14:55:37
    +0100       Finished:     Tue, 27 Dec 2016 14:55:50 +0100     Ready:         
    False ...`'
- en: 1 The current container was killed because it was out of memory (OOM).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 当前容器被杀死是因为它内存不足（OOM）。
- en: 2 The previous container was also killed because it was OOM
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 之前的容器也是因为 OOM 被杀死的
- en: The `OOMKilled` status tells you that the container was killed because it was
    out of memory. In the previous listing, the container went over its memory limit
    and was killed immediately.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`OOMKilled` 状态告诉你，容器被杀死是因为内存不足。在先前的列表中，容器超过了其内存限制并被立即杀死。'
- en: It’s important not to set memory limits too low if you don’t want your container
    to be killed. But containers can get `OOMKilled` even if they aren’t over their
    limit. You’ll see why in [section 14.3.2](index_split_108.html#filepos1376243),
    but first, let’s discuss something that catches most users off-guard the first
    time they start specifying limits for their containers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不希望容器被杀死，不要设置太低的内存限制。但即使容器没有超过其限制，它们也可能被 `OOMKilled`。你将在 [第 14.3.2 节](index_split_108.html#filepos1376243)
    中看到原因，但首先，让我们讨论一下大多数用户在第一次开始为他们的容器指定限制时通常会感到意外的某个问题。
- en: 14.2.3\. Understanding how apps in containers see limits
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 14.2.3\. 理解容器中的应用程序如何看到限制
- en: 'If you haven’t deployed the pod from [listing 14.7](#filepos1353257), deploy
    it now:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有从 [列表 14.7](#filepos1353257) 部署 pod，现在就部署它：
- en: '`$ kubectl create -f limited-pod.yaml` `pod "limited-pod" created`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f limited-pod.yaml` `pod "limited-pod" created`'
- en: Now, run the `top` command in the container, the way you did at the beginning
    of the chapter. The command’s output is shown in the following listing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在容器中运行 `top` 命令，就像你在本章开头所做的那样。命令的输出如下所示。
- en: Listing 14.9\. Running the `top` command in a CPU- and memory-limited container
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.9\. 在 CPU 和内存受限的容器中运行 `top` 命令
- en: '`$ kubectl exec -it limited-pod top``Mem: 1450980K used, 597504K free``, 22012K
    shrd, 65876K buff, 857552K cached` `CPU: 10.0% usr 40.0% sys``  0.0% nic 50.0%
    idle  0.0% io  0.0% irq  0.0% sirq Load average: 0.17 1.19 2.47 4/503 10   PID 
    PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND     1     0 root     R     1192 
    0.0   1 49.9 dd if /dev/zero of /dev/null     5     0 root     R     1196  0.0  
    0  0.0 top`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec -it limited-pod top``Mem: 1450980K used, 597504K free``, 22012K
    shrd, 65876K buff, 857552K cached` `CPU: 10.0% usr 40.0% sys``  0.0% nic 50.0%
    idle  0.0% io  0.0% irq  0.0% sirq Load average: 0.17 1.19 2.47 4/503 10   PID 
    PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND     1     0 root     R     1192 
    0.0   1 49.9 dd if /dev/zero of /dev/null     5     0 root     R     1196  0.0  
    0  0.0 top`'
- en: First, let me remind you that the pod’s CPU limit is set to 1 core and its memory
    limit is set to 20 MiB. Now, examine the output of the `top` command closely.
    Is there anything that strikes you as odd?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我提醒你，pod 的 CPU 限制设置为 1 个核心，其内存限制设置为 20 MiB。现在，仔细检查 `top` 命令的输出。有什么让你觉得奇怪的吗？
- en: Look at the amount of used and free memory. Those numbers are nowhere near the
    20 MiB you set as the limit for the container. Similarly, you set the CPU limit
    to one core and it seems like the main process is using only 50% of the available
    CPU time, even though the `dd` command, when used like you’re using it, usually
    uses all the CPU it has available. What’s going on?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 查看已使用和空闲的内存量。这些数字与你为容器设置的 20 MiB 限制相去甚远。同样，你将 CPU 限制设为单个核心，但看起来主进程只使用了 50% 的可用
    CPU 时间，尽管 `dd` 命令在你使用它的方式下，通常会用尽它所有的 CPU。这是怎么回事？
- en: Understanding that containers always see the node’s memory, not the container’s
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 理解容器总是看到节点的内存，而不是容器的内存
- en: The `top` command shows the memory amounts of the whole node the container is
    running on. Even though you set a limit on how much memory is available to a container,
    the container will not be aware of this limit.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`top`命令显示了容器运行的整个节点的内存量。即使你为容器设置了内存限制，容器也不会意识到这个限制。'
- en: This has an unfortunate effect on any application that looks up the amount of
    memory available on the system and uses that information to decide how much memory
    it wants to reserve.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这对任何查找系统上可用的内存量并使用该信息来决定它想要保留多少内存的应用程序都有不幸的影响。
- en: The problem is visible when running Java apps, especially if you don’t specify
    the maximum heap size for the Java Virtual Machine with the `-Xmx` option. In
    that case, the JVM will set the maximum heap size based on the host’s total memory
    instead of the memory available to the container. When you run your containerized
    Java apps in a Kubernetes cluster on your laptop, the problem doesn’t manifest
    itself, because the difference between the memory limits you set for the pod and
    the total memory available on your laptop is not that great.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行Java应用程序时，这个问题尤为明显，尤其是如果你没有使用`-Xmx`选项为Java虚拟机指定最大堆大小。在这种情况下，JVM将根据主机的总内存而不是容器的可用内存来设置最大堆大小。当你将你的容器化Java应用程序在你的笔记本电脑上的Kubernetes集群中运行时，问题不会显现出来，因为你在Pod上设置的内存限制与你的笔记本电脑上总内存之间的差异并不大。
- en: But when you deploy your pod onto a production system, where nodes have much
    more physical memory, the JVM may go over the container’s memory limit you configured
    and will be `OOMKilled`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当你将你的Pod部署到生产系统上，其中节点拥有更多的物理内存时，JVM可能会超过你配置的容器内存限制，并会被`OOMKilled`。
- en: And if you think setting the `-Xmx` option properly solves the issue, you’re
    wrong, unfortunately. The `-Xmx` option only constrains the heap size, but does
    nothing about the JVM’s off-heap memory. Luckily, new versions of Java alleviate
    that problem by taking the configured container limits into account.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为正确设置`-Xmx`选项可以解决这个问题，那么很遗憾，你错了。`-Xmx`选项仅限制堆大小，但对JVM的堆外内存没有任何作用。幸运的是，Java的新版本通过考虑配置的容器限制来减轻这个问题。
- en: Understanding that containers also see all the node’s CPU cores
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 理解容器也会看到所有节点的CPU核心
- en: Exactly like with memory, containers will also see all the node’s CPUs, regardless
    of the CPU limits configured for the container. Setting a CPU limit to one core
    doesn’t magically only expose only one CPU core to the container. All the CPU
    limit does is constrain the amount of CPU time the container can use.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 就像内存一样，容器也会看到所有节点的CPU，无论为容器配置的CPU限制是多少。将CPU限制设置为单个核心并不会神奇地只向容器暴露一个CPU核心。所有CPU限制所做的只是限制容器可以使用的CPU时间量。
- en: A container with a one-core CPU limit running on a 64-core CPU will get 1/64th
    of the overall CPU time. And even though its limit is set to one core, the container’s
    processes will not run on only one core. At different points in time, its code
    may be executed on different cores.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在64核心CPU上运行的单核心CPU限制的容器将获得整体CPU时间的1/64。即使其限制设置为单个核心，容器的进程也不会只在单个核心上运行。在不同的时间点，其代码可能会在不同的核心上执行。
- en: Nothing is wrong with this, right? While that’s generally the case, at least
    one scenario exists where this situation is catastrophic.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这并没有什么问题，对吧？虽然通常情况下是这样的，但至少存在一种情况，这种情况下这种情况是灾难性的。
- en: Certain applications look up the number of CPUs on the system to decide how
    many worker threads they should run. Again, such an app will run fine on a development
    laptop, but when deployed on a node with a much bigger number of cores, it’s going
    to spin up too many threads, all competing for the (possibly) limited CPU time.
    Also, each thread requires additional memory, causing the apps memory usage to
    skyrocket.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 某些应用程序会查找系统上的CPU数量来决定它们应该运行多少个工作线程。同样，这样的应用程序在开发笔记本电脑上运行良好，但当部署到具有更多核心的节点上时，它将启动过多的线程，所有线程都在竞争（可能是）有限的CPU时间。此外，每个线程都需要额外的内存，导致应用程序的内存使用量激增。
- en: 'You may want to use the Downward API to pass the CPU limit to the container
    and use it instead of relying on the number of CPUs your app can see on the system.
    You can also tap into the cgroups system directly to get the configured CPU limit
    by reading the following files:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想使用Downward API将CPU限制传递给容器，并使用它而不是依赖于你的应用程序在系统上可以看到的CPU数量。你也可以直接访问cgroups系统，通过读取以下文件来获取配置的CPU限制：
- en: /sys/fs/cgroup/cpu/cpu.cfs_quota_us
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /sys/fs/cgroup/cpu/cpu.cfs_quota_us
- en: /sys/fs/cgroup/cpu/cpu.cfs_period_us
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /sys/fs/cgroup/cpu/cpu.cfs_period_us
- en: 14.3\. Understanding pod QoS classes
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 14.3\. 理解 pod QoS 类别
- en: We’ve already mentioned that resource limits can be overcommitted and that a
    node can’t necessarily provide all its pods the amount of resources specified
    in their resource limits.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，资源限制可以被过度承诺，并且节点不一定能为其所有 pod 提供其资源限制中指定的资源量。
- en: Imagine having two pods, where pod A is using, let’s say, 90% of the node’s
    memory and then pod B suddenly requires more memory than what it had been using
    up to that point and the node can’t provide the required amount of memory. Which
    container should be killed? Should it be pod B, because its request for memory
    can’t be satisfied, or should pod A be killed to free up memory, so it can be
    provided to pod B?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 想象有两个 pod，其中 pod A 正在使用节点内存的 90%，然后 pod B 突然需要比之前使用更多的内存，而节点无法提供所需的内存量。应该杀死哪个容器？应该杀死
    pod B，因为其内存请求无法得到满足，还是应该杀死 pod A 以释放内存，以便提供给 pod B？
- en: 'Obviously, it depends. Kubernetes can’t make a proper decision on its own.
    You need a way to specify which pods have priority in such cases. Kubernetes does
    this by categorizing pods into three Quality of Service (QoS) classes:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这取决于具体情况。Kubernetes 无法独立做出正确的决定。你需要一种方式来指定在这种情况下哪些 pod 具有优先级。Kubernetes 通过将
    pod 分为三个服务质量（QoS）类别来实现这一点：
- en: '`BestEffort` (the lowest priority)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BestEffort`（最低优先级）'
- en: '`Burstable`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Burstable`'
- en: '`Guaranteed` (the highest)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Guaranteed`（最高）'
- en: 14.3.1\. Defining the QoS class for a pod
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 14.3.1\. 为 pod 定义 QoS 类
- en: You might expect these classes to be assignable to pods through a separate field
    in the manifest, but they aren’t. The QoS class is derived from the combination
    of resource requests and limits for the pod’s containers. Here’s how.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能期望这些类别可以通过清单中的单独字段来分配，但实际上并非如此。QoS 类别是从 pod 容器的资源请求和限制的组合中派生出来的。以下是具体方法。
- en: Assigning a pod to the BestEffort class
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 将 pod 分配到 BestEffort 类
- en: The lowest priority QoS class is the `BestEffort` class. It’s assigned to pods
    that don’t have any requests or limits set at all (in any of their containers).
    This is the QoS class that has been assigned to all the pods you created in previous
    chapters. Containers running in these pods have had no resource guarantees whatsoever.
    In the worst case, they may get almost no CPU time at all and will be the first
    ones killed when memory needs to be freed for other pods. But because a `BestEffort`
    pod has no memory limits set, its containers may use as much memory as they want,
    if enough memory is available.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最低优先级的 QoS 类别是 `BestEffort` 类。它被分配给没有任何请求或限制设置的 pod（在任何容器中）。这是在前面章节中创建的所有 pod
    分配的 QoS 类别。在这些 pod 中运行的容器没有任何资源保证。在最坏的情况下，它们可能几乎得不到任何 CPU 时间，并且当需要为其他 pod 释放内存时，它们将是首先被杀死的。但是，由于
    `BestEffort` pod 没有设置内存限制，如果可用内存足够，其容器可以使用尽可能多的内存。
- en: Assigning a pod to the Guaranteed class
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 将 pod 分配到 Guaranteed 类
- en: 'On the other end of the spectrum is the `Guaranteed` QoS class. This class
    is given to pods whose containers’ requests are equal to the limits for all resources.
    For a pod’s class to be `Guaranteed`, three things need to be true:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一端是 `Guaranteed` QoS 类别。这个类别分配给那些容器请求等于所有资源限制的 pod。对于一个 pod 的类别要成为 `Guaranteed`，需要满足以下三个条件：
- en: Requests and limits need to be set for both CPU and memory.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要为 CPU 和内存设置请求和限制。
- en: They need to be set for each container.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要为每个容器设置这些。
- en: They need to be equal (the limit needs to match the request for each resource
    in each container).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们需要相等（限制需要与每个容器中每个资源的请求相匹配）。
- en: Because a container’s resource requests, if not set explicitly, default to the
    limits, specifying the limits for all resources (for each container in the pod)
    is enough for the pod to be `Guaranteed`. Containers in those pods get the requested
    amount of resources, but cannot consume additional ones (because their limits
    are no higher than their requests).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于容器的资源请求（如果没有明确设置），默认为限制，因此为所有资源（对于 pod 中的每个容器）指定限制就足够使 pod 成为 `Guaranteed`。这些
    pod 中的容器获得请求的资源量，但不能消耗额外的资源（因为它们的限制不高于它们的请求）。
- en: Assigning the Burstable QoS class to a pod
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Burstable QoS 类别分配给 pod
- en: In between `BestEffort` and `Guaranteed` is the `Burstable` QoS class. All other
    pods fall into this class. This includes single-container pods where the container’s
    limits don’t match its requests and all pods where at least one container has
    a resource request specified, but not the limit. It also includes pods where one
    container’s requests match their limits, but another container has no requests
    or limits specified. `Burstable` pods get the amount of resources they request,
    but are allowed to use additional resources (up to the limit) if needed.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '在 `BestEffort` 和 `Guaranteed` 之间是 `Burstable` QoS 类别。所有其他 pod 都属于这个类别。这包括容器限制与其请求不匹配的单容器
    pod，以及至少有一个容器指定了资源请求但没有指定限制的所有 pod。它还包括一个容器的请求与其限制相匹配，但另一个容器没有指定请求或限制的 pod。`Burstable`
    pod 会获得其请求的资源，但在需要时可以额外使用资源（最多达到限制）。 '
- en: Understanding how the relationship between requests and limits de- efines the
    QoS class
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 理解请求和限制之间的关系如何定义 QoS 类别
- en: All three QoS classes and their relationships with requests and limits are shown
    in [figure 14.4](#filepos1371056).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个 QoS 类别及其与请求和限制的关系都在[图 14.4](#filepos1371056)中展示。
- en: Figure 14.4\. Resource requests, limits and QoS classes
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4\. 资源请求、限制和 QoS 类别
- en: '![](images/00135.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00135.jpg)'
- en: Thinking about what QoS class a pod has can make your head spin, because it
    involves multiple containers, multiple resources, and all the possible relationships
    between requests and limits. It’s easier if you start by thinking about QoS at
    the container level (although QoS classes are a property of pods, not containers)
    and then derive the pod’s QoS class from the QoS classes of containers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 pod 的 QoS 类别可能会让你感到困惑，因为它涉及到多个容器、多个资源以及请求和限制之间所有可能的关系。如果你从容器级别的 QoS（尽管
    QoS 类别是 pod 的属性，而不是容器的属性）开始思考，然后从容器的 QoS 类别推导出 pod 的 QoS 类别，那就更容易理解了。
- en: Figuring out a container’s QoS class
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 确定容器的 QoS 类别
- en: '[Table 14.1](#filepos1372046) shows the QoS class based on how resource requests
    and limits are defined on a single container. For single-container pods, the QoS
    class applies to the pod as well.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 14.1](#filepos1372046)展示了基于单个容器上定义的资源请求和限制的 QoS 类别。对于单容器 pod，QoS 类别也适用于
    pod。'
- en: Table 14.1\. The QoS class of a single-container pod based on resource requests
    and limits
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14.1\. 基于资源请求和限制的单容器 pod 的 QoS 类别
- en: '| CPU requests vs. limits | Memory requests vs. limits | Container QoS class
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| CPU 请求与限制 | 内存请求与限制 | 容器 QoS 类别 |'
- en: '| None set | None set | BestEffort |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 未设置 | 未设置 | BestEffort |'
- en: '| None set | Requests < Limits | Burstable |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 未设置 | 请求 < 限制 | 可扩展 |'
- en: '| None set | Requests = Limits | Burstable |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 未设置 | 请求 = 限制 | 可扩展 |'
- en: '| Requests < Limits | None set | Burstable |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 请求 < 限制 | 未设置 | 可扩展 |'
- en: '| Requests < Limits | Requests < Limits | Burstable |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 请求 < 限制 | 请求 < 限制 | 可扩展 |'
- en: '| Requests < Limits | Requests = Limits | Burstable |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 请求 < 限制 | 请求 = 限制 | 可扩展 |'
- en: '| Requests = Limits | Requests = Limits | Guaranteed |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 请求 = 限制 | 请求 = 限制 | Guaranteed |'
- en: '|  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If only requests are set, but not limits, refer to the table rows where requests
    are less than the limits. If only limits are set, requests default to the limits,
    so refer to the rows where requests equal limits.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只设置了请求，但没有设置限制，请参考请求小于限制的表格行。如果只设置了限制，请求默认为限制值，因此请参考请求等于限制的行。
- en: '|  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Figuring out the QoS class of a pod with multiple containers
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 确定具有多个容器的 pod 的 QoS 类别
- en: For multi-container pods, if all the containers have the same QoS class, that’s
    also the pod’s QoS class. If at least one container has a different class, the
    pod’s QoS class is `Burstable`, regardless of what the container classes are.
    [Table 14.2](#filepos1374486) shows how a two-container pod’s QoS class relates
    to the classes of its two containers. You can easily extend this to pods with
    more than two containers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多容器 pod，如果所有容器具有相同的 QoS 类别，那么这也是 pod 的 QoS 类别。如果至少有一个容器具有不同的类别，那么 pod 的 QoS
    类别是 `Burstable`，无论容器的类别是什么。[表 14.2](#filepos1374486)展示了两个容器 pod 的 QoS 类别如何与其两个容器的类别相关。你可以轻松地将这个扩展到具有两个以上容器的
    pod。
- en: Table 14.2\. A Pod’s QoS class derived from the classes of its containers
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14.2\. 从其容器的类别推导出的 Pod 的 QoS 类别
- en: '| Container 1 QoS class | Container 2 QoS class | Pod’s QoS class |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 容器 1 QoS 类别 | 容器 2 QoS 类别 | Pod 的 QoS 类别 |'
- en: '| BestEffort | BestEffort | BestEffort |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| BestEffort | BestEffort | BestEffort |'
- en: '| BestEffort | Burstable | Burstable |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| BestEffort | 可扩展 | 可扩展 |'
- en: '| BestEffort | Guaranteed | Burstable |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| BestEffort | Guaranteed | 可扩展 |'
- en: '| Burstable | Burstable | Burstable |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展 | 可扩展 | 可扩展 |'
- en: '| Burstable | Guaranteed | Burstable |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Burstable | Guaranteed | Burstable |'
- en: '| Guaranteed | Guaranteed | Guaranteed |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Guaranteed | Guaranteed | Guaranteed |'
- en: '|  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A pod’s QoS class is shown when running `kubectl describe pod` and in the pod’s
    YAML/JSON manifest in the `status.qosClass` field.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行`kubectl describe pod`时，pod的QoS类别会显示出来，并在pod的YAML/JSON表示中的`status.qosClass`字段中。
- en: '|  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We’ve explained how QoS classes are determined, but we still need to look at
    how they determine which container gets killed in an overcommitted system.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解释了如何确定QoS类别，但我们还需要看看它们是如何在过载系统中确定哪个容器会被杀死的。
- en: 14.3.2\. Understanding which process gets killed when memory is low
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 14.3.2. 理解内存低时哪个进程会被杀死
- en: When the system is overcommitted, the QoS classes determine which container
    gets killed first so the freed resources can be given to higher priority pods.
    First in line to get killed are pods in the `BestEffort` class, followed by `Burstable`
    pods, and finally `Guaranteed` pods, which only get killed if system processes
    need memory.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统过载时，服务质量（QoS）类别决定了哪个容器首先被杀死，以便释放的资源可以分配给优先级更高的pod。首先被杀死的是`BestEffort`类别的pod，其次是`Burstable`
    pod，最后是`Guaranteed` pod，只有当系统进程需要内存时，`Guaranteed` pod才会被杀死。
- en: Understanding how QoS classes line up
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 理解QoS类别的排列方式
- en: Let’s look at the example shown in [figure 14.5](#filepos1377498). Imagine having
    two single-container pods, where the first one has the `BestEffort` QoS class,
    and the second one’s is `Burstable`. When the node’s whole memory is already maxed
    out and one of the processes on the node tries to allocate more memory, the system
    will need to kill one of the processes (perhaps even the process trying to allocate
    additional memory) to honor the allocation request. In this case, the process
    running in the `BestEffort` pod will always be killed before the one in the `Burstable`
    pod.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看[图14.5](#filepos1377498)中显示的示例。想象有两个单容器pod，第一个pod具有`BestEffort` QoS类别，第二个pod的类别是`Burstable`。当节点的全部内存已经达到最大值，并且节点上的某个进程尝试分配更多内存时，系统需要杀死一个进程（可能是尝试分配额外内存的进程）以遵守分配请求。在这种情况下，运行在`BestEffort`
    pod中的进程将始终在运行在`Burstable` pod中的进程之前被杀死。
- en: Figure 14.5\. Which pods get killed first
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5. 哪些pod先被杀死
- en: '![](images/00153.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00153.jpg)'
- en: Obviously, a `BestEffort` pod’s process will also be killed before any `Guaranteed`
    pods’ processes are killed. Likewise, a `Burstable` pod’s process will also be
    killed before that of a `Guaranteed` pod. But what happens if there are only two
    `Burstable` pod`s`? Clearly, the selection process needs to prefer one over the
    other.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在杀死任何`Guaranteed` pod的进程之前，`BestEffort` pod的进程也会被杀死。同样，在杀死`Guaranteed` pod之前，`Burstable`
    pod的进程也会被杀死。但如果只有两个`Burstable` pod呢？显然，选择过程需要优先考虑其中一个。
- en: Understanding how containers with the same QoS class are handled
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 理解具有相同QoS类别的容器是如何处理的
- en: Each running process has an OutOfMemory (OOM) score. The system selects the
    process to kill by comparing OOM scores of all the running processes. When memory
    needs to be freed, the process with the highest score gets killed.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 每个正在运行的过程都有一个内存不足（OOM）分数。系统通过比较所有运行进程的OOM分数来选择要杀死的进程。当需要释放内存时，分数最高的进程会被杀死。
- en: 'OOM scores are calculated from two things: the percentage of the available
    memory the process is consuming and a fixed OOM score adjustment, which is based
    on the pod’s QoS class and the container’s requested memory. When two single-container
    pods exist, both in the `Burstable` class, the system will kill the one using
    more of its requested memory than the other, percentage-wise. That’s why in [figure
    14.5](#filepos1377498), pod B, using 90% of its requested memory, gets killed
    before pod C, which is only using 70%, even though it’s using more megabytes of
    memory than pod B.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: OOM分数由两件事计算得出：进程消耗的可用内存百分比和一个固定的OOM分数调整，该调整基于pod的QoS类别和容器请求的内存。当存在两个单容器pod，且两者都属于`Burstable`类别时，系统将杀死使用其请求内存比例更高的那个。这就是为什么在[图14.5](#filepos1377498)中，使用90%请求内存的pod
    B在只使用70%的pod C之前被杀死，尽管pod B使用的内存兆字节比pod C多。
- en: This shows you need to be mindful of not only the relationship between requests
    and limits, but also of requests and the expected actual memory consumption.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明您需要留心的不只是请求和限制之间的关系，还要注意请求和预期实际内存消耗之间的关系。
- en: 14.4\. Setting default requests and limits for pods per namespace
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 14.4. 为每个命名空间中的pod设置默认请求和限制
- en: We’ve looked at how resource requests and limits can be set for each individual
    container. If you don’t set them, the container is at the mercy of all other containers
    that do specify resource requests and limits. It’s a good idea to set requests
    and limits on every container.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何为每个单独的容器设置资源请求和限制。如果你没有设置它们，容器就会受制于所有其他指定了资源请求和限制的容器。为每个容器设置请求和限制是一个好主意。
- en: 14.4.1\. Introducing the LimitRange resource
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 14.4.1\. 介绍 LimitRange 资源
- en: Instead of having to do this for every container, you can also do it by creating
    a Limit-Range resource. It allows you to specify (for each namespace) not only
    the minimum and maximum limit you can set on a container for each resource, but
    also the default resource requests for containers that don’t specify requests
    explicitly, as depicted in [figure 14.6](#filepos1380309).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为每个容器执行此操作外，您还可以通过创建 Limit-Range 资源来完成此操作。它允许您指定（对于每个命名空间）您可以为每个资源在容器上设置的最低和最高限制，以及未明确指定请求的容器的默认资源请求，如图
    14.6 所示。
- en: Figure 14.6\. A LimitRange is used for validation and defaulting pods.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6\. LimitRange 用于验证和默认 pod。
- en: '![](images/00170.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00170.jpg)'
- en: LimitRange resources are used by the LimitRanger Admission Control plugin (we
    explained what those plugins are in [chapter 11](index_split_087.html#filepos1036287)).
    When a pod manifest is posted to the API server, the LimitRanger plugin validates
    the pod spec. If validation fails, the manifest is rejected immediately. Because
    of this, a great use-case for LimitRange objects is to prevent users from creating
    pods that are bigger than any node in the cluster. Without such a LimitRange,
    the API server will gladly accept the pod, but then never schedule it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: LimitRange 资源由 LimitRanger 接受控制插件使用（我们在[第 11 章](index_split_087.html#filepos1036287)中解释了这些插件）。当一个
    pod 清单被提交到 API 服务器时，LimitRanger 插件会验证 pod 规范。如果验证失败，则立即拒绝清单。正因为如此，LimitRange 对象的一个很好的用途是防止用户创建比集群中任何节点都大的
    pod。如果没有这样的 LimitRange，API 服务器会欣然接受 pod，但随后永远不会调度它。
- en: The limits specified in a LimitRange resource apply to each individual pod/container
    or other kind of object created in the same namespace as the LimitRange object.
    They don’t limit the total amount of resources available across all the pods in
    the namespace. This is specified through ResourceQuota objects, which are explained
    in [section 14.5](index_split_110.html#filepos1392684).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: LimitRange 资源中指定的限制适用于与 LimitRange 对象在同一命名空间中创建的每个单独的 pod/容器或其他类型的对象。它们不会限制命名空间中所有
    pod 可用资源的总量。这通过 ResourceQuota 对象来指定，该对象在[第 14.5 节](index_split_110.html#filepos1392684)中进行了说明。
- en: 14.4.2\. Creating a LimitRange object
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 14.4.2\. 创建 LimitRange 对象
- en: Let’s look at a full example of a LimitRange and see what the individual properties
    do. The following listing shows the full definition of a LimitRange resource.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个完整的 LimitRange 示例，看看各个属性的作用。以下列表显示了 LimitRange 资源的完整定义。
- en: 'Listing 14.10\. A LimitRange resource: limits.yaml'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.10\. LimitRange 资源：limits.yaml
- en: '`apiVersion: v1 kind: LimitRange metadata:   name: example spec:   limits:
      - type: Pod` `1` `min:` `2` `cpu: 50m` `2` `memory: 5Mi` `2` `max:` `3` `cpu:
    1` `3` `memory: 1Gi` `3` `- type: Container` `4` `defaultRequest:` `5` `cpu: 100m`
    `5` `memory: 10Mi` `5` `default:` `6` `cpu: 200m` `6` `memory: 100Mi` `6` `min:`
    `7` `cpu: 50m` `7` `memory: 5Mi` `7` `max:` `7` `cpu: 1` `7` `memory: 1Gi` `7`
    `maxLimitRequestRatio:` `8` `cpu: 4` `8` `memory: 10` `8` `- type: PersistentVolumeClaim`
    `9` `min:` `9` `storage: 1Gi` `9` `max:` `9` `storage: 10Gi` `9`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: LimitRange metadata: name: example spec: limits: - type:
    Pod min: cpu: 50m memory: 5Mi max: cpu: 1 memory: 1Gi - type: Container defaultRequest:
    cpu: 100m memory: 10Mi default: cpu: 200m memory: 100Mi min: cpu: 50m memory:
    5Mi max: cpu: 1 memory: 1Gi maxLimitRequestRatio: cpu: 4 memory: 10 - type: PersistentVolumeClaim
    min: storage: 1Gi max: storage: 10Gi`'
- en: 1 Specifies the limits for a pod as a whole
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 指定整个 pod 的限制
- en: 2 Minimum CPU and memory all the pod’s containers can request in total
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 pod 的容器可以请求的总最小 CPU 和内存
- en: 3 Maximum CPU and memory all the pod’s containers can request (and limit)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 每个 pod 的容器可以请求（和限制）的最大 CPU 和内存
- en: 4 The container limits are specified below this line.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 容器限制在此行以下指定。
- en: 5 Default requests for CPU and memory that will be applied to containers that
    don’t specify them explicitly
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5 将应用于未明确指定它们的容器的 CPU 和内存的默认请求
- en: 6 Default limits for containers that don’t specify them
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 6 未指定限制的容器的默认限制
- en: 7 Minimum and maximum requests/limits that a container can have
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器可以拥有的最小和最大请求/限制
- en: 8 Maximum ratio between the limit and request for each resource
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个资源限制与请求之间的最大比率是8
- en: 9 A LimitRange can also set the minimum and maximum amount of storage a PVC
    can request.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9 LimitRange还可以设置PVC可以请求的最小和最大存储量。
- en: As you can see from the previous example, the minimum and maximum limits for
    a whole pod can be configured. They apply to the sum of all the pod’s containers’
    requests and limits.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，可以配置整个Pod的最小和最大限制。它们适用于Pod中所有容器的请求和限制的总和。
- en: Lower down, at the container level, you can set not only the minimum and maximum,
    but also default resource requests (`defaultRequest`) and default limits (`default`)
    that will be applied to each container that doesn’t specify them explicitly.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器级别，您不仅可以设置最小和最大值，还可以设置默认资源请求（`defaultRequest`）和默认限制（`default`），这些将应用于每个未明确指定的容器。
- en: Beside the min, max, and default values, you can even set the maximum ratio
    of limits vs. requests. The previous listing sets the CPU `maxLimitRequestRatio`
    to `4`, which means a container’s CPU limits will not be allowed to be more than
    four times greater than its CPU requests. A container requesting 200 millicores
    will not be accepted if its CPU limit is set to 801 millicores or higher. For
    memory, the maximum ratio is set to 10\.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 除了最小、最大和默认值之外，您甚至可以设置限制与请求之间的最大比率。前面的列表将CPU `maxLimitRequestRatio` 设置为 `4`，这意味着容器的CPU限制将不允许超过其CPU请求的四倍。如果CPU限制设置为801毫核或更高，则请求200毫核的容器将不被接受。对于内存，最大比率设置为10。
- en: In [chapter 6](index_split_055.html#filepos588298) we looked at PersistentVolumeClaims
    (PVC), which allow you to claim a certain amount of persistent storage similarly
    to how a pod’s containers claim CPU and memory. In the same way you’re limiting
    the minimum and maximum amount of CPU a container can request, you should also
    limit the amount of storage a single PVC can request. A LimitRange object allows
    you to do that as well, as you can see at the bottom of the example.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](index_split_055.html#filepos588298)中，我们探讨了PersistentVolumeClaims (PVC)，它允许您以类似于Pod的容器请求CPU和内存的方式请求一定量的持久存储。同样地，您应该限制单个PVC可以请求的存储量。LimitRange对象允许您这样做，如示例底部所示。
- en: The example shows a single LimitRange object containing limits for everything,
    but you could also split them into multiple objects if you prefer to have them
    organized per type (one for pod limits, another for container limits, and yet
    another for PVCs, for example). Limits from multiple LimitRange objects are all
    consolidated when validating a pod or PVC.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 示例显示了一个包含所有内容的单个LimitRange对象，但您也可以根据偏好将它们拆分为多个对象（例如，一个用于Pod限制，另一个用于容器限制，还有一个用于PVC）。在验证Pod或PVC时，来自多个LimitRange对象的所有限制都将合并。
- en: Because the validation (and defaults) configured in a LimitRange object is performed
    by the API server when it receives a new pod or PVC manifest, if you modify the
    limits afterwards, existing pods and PVCs will not be revalidated—the new limits
    will only apply to pods and PVCs created afterward.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LimitRange对象中配置的验证（和默认值）是在API服务器接收到新的Pod或PVC清单时执行的，因此如果您之后修改了限制，现有的Pod和PVC将不会被重新验证——新的限制将仅适用于之后创建的Pod和PVC。
- en: 14.4.3\. Enforcing the limits
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 14.4.3\. 执行限制
- en: With your limits in place, you can now try creating a pod that requests more
    CPU than allowed by the LimitRange. You’ll find the YAML for the pod in the code
    archive. The next listing only shows the part relevant to the discussion.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置了限制之后，您现在可以尝试创建一个请求比LimitRange允许的更多CPU的Pod。您可以在代码存档中找到Pod的YAML。下一个列表仅显示与讨论相关的部分。
- en: 'Listing 14.11\. A pod with CPU requests greater than the limit: limits-pod-too-big.yaml'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.11\. 一个CPU请求大于限制的Pod：limits-pod-too-big.yaml
- en: '`    resources:       requests:         cpu: 2`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`    资源:       请求:       cpu: 2`'
- en: 'The pod’s single container is requesting two CPUs, which is more than the maximum
    you set in the LimitRange earlier. Creating the pod yields the following result:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Pod的单个容器请求了两个CPU，这超过了您之前在LimitRange中设置的极限。创建Pod的结果如下：
- en: '`$ kubectl create -f limits-pod-too-big.yaml` `Error from server (Forbidden):
    error when creating "limits-pod-too-big.yaml": pods "too-big" is forbidden: [
      maximum cpu usage per Pod is 1, but request is 2.,   maximum cpu usage per Container
    is 1, but request is 2.]`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f limits-pod-too-big.yaml` `Error from server (Forbidden):
    error when creating "limits-pod-too-big.yaml": pods "too-big" is forbidden: [
      maximum cpu usage per Pod is 1, but request is 2.,   maximum cpu usage per Container
    is 1, but request is 2.]`'
- en: 'I’ve modified the output slightly to make it more legible. The nice thing about
    the error message from the server is that it lists all the reasons why the pod
    was rejected, not only the first one it encountered. As you can see, the pod was
    rejected for two reasons: you requested two CPUs for the container, but the maximum
    CPU limit for a container is one. Likewise, the pod as a whole requested two CPUs,
    but the maximum is one CPU (if this was a multi-container pod, even if each individual
    container requested less than the maximum amount of CPU, together they’d still
    need to request less than two CPUs to pass the maximum CPU for pods).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍微修改了输出，使其更易于阅读。服务器错误信息的好处是它列出了导致pods被拒绝的所有原因，而不仅仅是第一个遇到的原因。正如你所看到的，pods被拒绝了两个原因：你为容器请求了两个CPU，但容器的最大CPU限制是一个。同样，整个pods请求了两个CPU，但最大限制是一个CPU（如果这是一个多容器pods，即使每个容器请求的CPU少于最大量，它们加在一起仍然需要请求少于两个CPU才能通过pods的最大CPU限制）。
- en: 14.4.4\. Applying default resource requests and limits
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 14.4.4. 应用默认资源请求和限制
- en: 'Now let’s also see how default resource requests and limits are set on containers
    that don’t specify them. Deploy the `kubia-manual` pod from [chapter 3](index_split_028.html#filepos271328)
    again:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们也来看看默认资源请求和限制是如何设置在未指定它们的容器上的。再次部署`kubia-manual` pod，来自[第3章](index_split_028.html#filepos271328)：
- en: '`$ kubectl create -f ../Chapter03/kubia-manual.yaml` `pod "kubia-manual" created`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f ../Chapter03/kubia-manual.yaml` `pod "kubia-manual" created`'
- en: Before you set up your LimitRange object, all your pods were created without
    any resource requests or limits, but now the defaults are applied automatically
    when creating the pod. You can confirm this by describing the `kubia-manual` pod,
    as shown in the following listing.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置你的LimitRange对象之前，所有的pods都是没有资源请求或限制被创建的，但现在在创建pods时会自动应用默认值。你可以通过描述`kubia-manual`
    pod来确认这一点，如下所示。
- en: Listing 14.12\. Inspecting limits that were applied to a pod automatically
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.12. 检查自动应用到pods上的限制
- en: '`$ kubectl describe po kubia-manual` `Name:           kubia-manual ... Containers:
      kubia:     Limits:       cpu:      200m       memory:   100Mi     Requests:
          cpu:      100m       memory:   10Mi`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe po kubia-manual` `Name:           kubia-manual ... Containers:
      kubia:     Limits:       cpu:      200m       memory:   100Mi     Requests:
          cpu:      100m       memory:   10Mi`'
- en: The container’s requests and limits match the ones you specified in the LimitRange
    object. If you used a different LimitRange specification in another namespace,
    pods created in that namespace would obviously have different requests and limits.
    This allows admins to configure default, min, and max resources for pods per namespace.
    If namespaces are used to separate different teams or to separate development,
    QA, staging, and production pods running in the same Kubernetes cluster, using
    a different LimitRange in each namespace ensures large pods can only be created
    in certain namespaces, whereas others are constrained to smaller pods.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 容器的请求和限制与你在LimitRange对象中指定的相匹配。如果你在另一个命名空间中使用了不同的LimitRange规范，那么在该命名空间中创建的pods将显然具有不同的请求和限制。这允许管理员为每个命名空间配置pods的默认、最小和最大资源。如果命名空间被用来分隔不同的团队，或者用来分隔在同一Kubernetes集群中运行的开发、QA、预发布和生产pods，那么在每个命名空间中使用不同的LimitRange可以确保只能在某些命名空间中创建大型pods，而其他则被限制在较小的pods。
- en: But remember, the limits configured in a LimitRange only apply to each individual
    pod/container. It’s still possible to create many pods and eat up all the resources
    available in the cluster. LimitRanges don’t provide any protection from that.
    A Resource-Quota object, on the other hand, does. You’ll learn about them next.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，LimitRange中配置的限制仅适用于每个单独的pods/容器。仍然有可能创建许多pods并消耗掉集群中所有的资源。LimitRanges不能提供对此类情况的保护。另一方面，Resource-Quota对象可以。你将在下一部分学习它们。
- en: 14.5\. Limiting the total resources available in a namespace
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 14.5. 限制命名空间中可用的总资源
- en: As you’ve seen, LimitRanges only apply to individual pods, but cluster admins
    also need a way to limit the total amount of resources available in a namespace.
    This is achieved by creating a ResourceQuota object.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，LimitRanges仅适用于单个Pod，但集群管理员还需要一种方法来限制命名空间中可用的资源总量。这是通过创建资源配额对象来实现的。
- en: 14.5.1\. Introducing the ResourceQuota object
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 14.5.1\. 介绍资源配额对象
- en: In [chapter 10](index_split_079.html#filepos953352) we said that several Admission
    Control plugins running inside the API server verify whether the pod may be created
    or not. In the previous section, I said that the LimitRanger plugin enforces the
    policies configured in LimitRange resources. Similarly, the ResourceQuota Admission
    Control plugin checks whether the pod being created would cause the configured
    ResourceQuota to be exceeded. If that’s the case, the pod’s creation is rejected.
    Because resource quotas are enforced at pod creation time, a ResourceQuota object
    only affects pods created after the Resource-Quota object is created—creating
    it has no effect on existing pods.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](index_split_079.html#filepos953352)中，我们提到API服务器内部运行的几个准入控制插件会验证Pod是否可以创建。在前一节中，我说LimitRanger插件强制执行LimitRange资源中配置的策略。同样，资源配额准入控制插件会检查正在创建的Pod是否会超出配置的资源配额。如果是这种情况，Pod的创建将被拒绝。因为资源配额在Pod创建时生效，所以资源配额对象只会影响在资源配额对象创建之后创建的Pod——创建它对现有Pod没有影响。
- en: A ResourceQuota limits the amount of computational resources the pods and the
    amount of storage PersistentVolumeClaims in a namespace can consume. It can also
    limit the number of pods, claims, and other API objects users are allowed to create
    inside the namespace. Because you’ve mostly dealt with CPU and memory so far,
    let’s start by looking at how to specify quotas for them.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额（ResourceQuota）限制了命名空间中可以消耗的计算资源量以及持久卷声明（PersistentVolumeClaims）的存储量。它还可以限制用户在命名空间内可以创建的Pod、声明和其他API对象的数量。由于你到目前为止主要处理的是CPU和内存，让我们先看看如何为它们指定配额。
- en: Creating a ResourceQuota for CPU and memory
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 创建CPU和内存的资源配额
- en: The overall CPU and memory all the pods in a namespace are allowed to consume
    is defined by creating a ResourceQuota object as shown in the following listing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建资源配额对象，定义了命名空间中所有Pod允许消耗的总CPU和内存量，如下所示列表。
- en: 'Listing 14.13\. A ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.13\. CPU和内存的资源配额：quota-cpu-memory.yaml
- en: '`apiVersion: v1 kind: ResourceQuota metadata:   name: cpu-and-mem spec:   hard:
        requests.cpu: 400m     requests.memory: 200Mi     limits.cpu: 600m     limits.memory:
    500Mi`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ResourceQuota metadata:   name: cpu-and-mem spec:   hard:
        requests.cpu: 400m     requests.memory: 200Mi     limits.cpu: 600m     limits.memory:
    500Mi`'
- en: Instead of defining a single total for each resource, you define separate totals
    for requests and limits for both CPU and memory. You’ll notice the structure is
    a bit different, compared to that of a LimitRange. Here, both the requests and
    the limits for all resources are defined in a single place.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 与为每个资源定义单个总量不同，您为CPU和内存的请求和限制分别定义单独的总量。您会注意到结构与LimitRange的结构略有不同。在这里，所有资源的请求和限制都在一个地方定义。
- en: This ResourceQuota sets the maximum amount of CPU pods in the namespace can
    request to 400 millicores. The maximum total CPU limits in the namespace are set
    to 600 millicores. For memory, the maximum total requests are set to 200 MiB,
    whereas the limits are set to 500 MiB.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此资源配额将命名空间中Pod请求的最大CPU量设置为400毫核。命名空间中最大总CPU限制设置为600毫核。对于内存，最大总请求设置为200 MiB，而限制设置为500
    MiB。
- en: A ResourceQuota object applies to the namespace it’s created in, like a Limit-Range,
    but it applies to all the pods’ resource requests and limits in total and not
    to each individual pod or container separately, as shown in [figure 14.7](#filepos1395956).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额对象应用于其创建的命名空间，就像Limit-Range一样，但它应用于所有Pod的资源请求和限制的总量，而不是每个单独的Pod或容器，如图14.7所示。
- en: Figure 14.7\. LimitRanges apply to individual pods; ResourceQuotas apply to
    all pods in the namespace.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7\. LimitRanges应用于单个Pod；资源配额应用于命名空间中的所有Pod。
- en: '![](images/00189.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00189.jpg)'
- en: Inspecting the quota and quota usage
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 检查配额和配额使用情况
- en: After you post the ResourceQuota object to the API server, you can use the `kubectl
    describe` command to see how much of the quota is already used up, as shown in
    the following listing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在将资源配额对象发布到API服务器后，您可以使用`kubectl describe`命令查看已使用的配额量，如下所示。
- en: Listing 14.14\. Inspecting the ResourceQuota with `kubectl describe quota`
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.14\. 使用`kubectl describe quota`检查ResourceQuota
- en: '`$ kubectl describe quota` `Name:           cpu-and-mem Namespace:      default
    Resource        Used   Hard --------        ----   ---- limits.cpu      200m  
    600m limits.memory   100Mi  500Mi requests.cpu    100m   400m requests.memory
    10Mi   200Mi`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe quota` `Name: cpu-and-mem Namespace: default Resource Used
    Hard -------- ---- ---- limits.cpu 200m 600m limits.memory 100Mi 500Mi requests.cpu
    100m 400m requests.memory 10Mi 200Mi`'
- en: I only have the `kubia-manual` pod running, so the `Used` column matches its
    resource requests and limits. When I run additional pods, their requests and limits
    are added to the used amounts.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我只运行了`kubia-manual` Pod，所以`Used`列与它的资源请求和限制相匹配。当我运行额外的Pod时，它们的请求和限制会被加到已使用量中。
- en: Creating a LimitRange along with a ResourceQuota
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 与ResourceQuota一起创建LimitRange
- en: 'One caveat when creating a ResourceQuota is that you will also want to create
    a Limit-Range object alongside it. In your case, you have a LimitRange configured
    from the previous section, but if you didn’t have one, you couldn’t run the `kubia-manual`
    pod, because it doesn’t specify any resource requests or limits. Here’s what would
    happen in that case:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 创建ResourceQuota时有一个注意事项，那就是你也会想同时创建一个Limit-Range对象。在你的情况下，你已经从上一节配置了LimitRange，但如果你没有配置，你就无法运行`kubia-manual`
    Pod，因为它没有指定任何资源请求或限制。以下是这种情况会发生什么：
- en: '`$ kubectl create -f ../Chapter03/kubia-manual.yaml` `Error from server (Forbidden):
    error when creating "../Chapter03/kubia-      manual.yaml": pods "kubia-manual"
    is forbidden:` `failed quota: cpu-and-      mem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f ../Chapter03/kubia-manual.yaml` `错误来自服务器（禁止）：创建"../Chapter03/kubia-manual.yaml"时出错：pods
    "kubia-manual"被禁止：` `失败配额：cpu-and-mem：必须指定limits.cpu，limits.memory，requests.cpu，requests.memory`'
- en: When a quota for a specific resource (CPU or memory) is configured (request
    or limit), pods need to have the request or limit (respectively) set for that
    same resource; otherwise the API server will not accept the pod. That’s why having
    a LimitRange with defaults for those resources can make life a bit easier for
    people creating pods.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当为特定资源（CPU或内存）配置配额（请求或限制）时，Pod需要为该资源设置相应的请求或限制（分别）；否则API服务器将不接受该Pod。这就是为什么为这些资源设置默认值的LimitRange可以使创建Pod的人生活变得容易一些。
- en: 14.5.2\. Specifying a quota for persistent storage
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 14.5.2\. 指定持久存储的配额
- en: A ResourceQuota object can also limit the amount of persistent storage that
    can be claimed in the namespace, as shown in the following listing.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ResourceQuota对象还可以限制在命名空间中可以声明的持久存储量，如下面的列表所示。
- en: 'Listing 14.15\. A ResourceQuota for storage: quota-storage.yaml'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.15\. 存储资源的ResourceQuota：quota-storage.yaml
- en: '`apiVersion: v1 kind: ResourceQuota metadata:   name: storage spec:   hard:
        requests.storage: 500Gi` `1` `ssd.storageclass.storage.k8s.io/requests.storage:
    300Gi` `2` `standard.storageclass.storage.k8s.io/requests.storage: 1Ti`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ResourceQuota metadata: name: storage spec: hard: requests.storage:
    500Gi 1 ssd.storageclass.storage.k8s.io/requests.storage: 300Gi 2 standard.storageclass.storage.k8s.io/requests.storage:
    1Ti`'
- en: 1 The amount of storage claimable overall
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 总共可声明的存储量
- en: 2 The amount of claimable storage in StorageClass ssd
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 在StorageClass ssd中可声明的存储量
- en: In this example, the amount of storage all PersistentVolumeClaims in a namespace
    can request is limited to 500 GiB (by the `requests.storage` entry in the ResourceQuota
    object). But as you’ll remember from [chapter 6](index_split_055.html#filepos588298),
    PersistentVolumeClaims can request a dynamically provisioned PersistentVolume
    of a specific StorageClass. That’s why Kubernetes also makes it possible to define
    storage quotas for each StorageClass individually. The previous example limits
    the total amount of claimable SSD storage (designated by the `ssd` StorageClass)
    to 300 GiB. The less-performant HDD storage (StorageClass standard) is limited
    to 1 TiB.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，一个命名空间中所有PersistentVolumeClaims可以请求的存储量被限制为500 GiB（由ResourceQuota对象中的`requests.storage`条目限制）。但正如你从[第6章](index_split_055.html#filepos588298)中记得的那样，PersistentVolumeClaims可以请求特定StorageClass的动态预配PersistentVolume。这就是为什么Kubernetes还允许为每个StorageClass单独定义存储配额。前面的例子将可声明的总SSD存储量（由`ssd`
    StorageClass指定）限制为300 GiB。性能较低的HDD存储（StorageClass standard）限制为1 TiB。
- en: 14.5.3\. Limiting the number of objects that can be created
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 14.5.3\. 限制可以创建的对象数量
- en: A ResourceQuota can also be configured to limit the number of Pods, Replication-Controllers,
    Services, and other objects inside a single namespace. This allows the cluster
    admin to limit the number of objects users can create based on their payment plan,
    for example, and can also limit the number of public IPs or node ports Services
    can use.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ResourceQuota 也可以配置为限制单个命名空间内 Pods、Replication-Controllers、Services 和其他对象的数量。这允许集群管理员根据用户的付费计划等限制用户可以创建的对象数量，也可以限制公共
    IP 或节点端口 Services 可以使用的数量。
- en: The following listing shows what a ResourceQuota object that limits the number
    of objects may look like.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了限制对象数量的 ResourceQuota 对象可能的样子。
- en: 'Listing 14.16\. A ResourceQuota for max number of resources: quota-object-count.yaml'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.16\. 限制资源最大数量的 ResourceQuota：quota-object-count.yaml
- en: '`apiVersion: v1 kind: ResourceQuota metadata:   name: objects spec:   hard:
        pods: 10` `1` `replicationcontrollers: 5` `1` `secrets: 10` `1` `configmaps:
    10` `1` `persistentvolumeclaims: 4` `1` `services: 5` `2` `services.loadbalancers:
    1` `2` `services.nodeports: 2` `2` `ssd.storageclass.storage.k8s.io/persistentvolumeclaims:
    2` `3`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ResourceQuota metadata:   name: objects spec:   hard:
        pods: 10` `1` `replicationcontrollers: 5` `1` `secrets: 10` `1` `configmaps:
    10` `1` `persistentvolumeclaims: 4` `1` `services: 5` `2` `services.loadbalancers:
    1` `2` `services.nodeports: 2` `2` `ssd.storageclass.storage.k8s.io/persistentvolumeclaims:
    2` `3`'
- en: 1 Only 10 Pods, 5 ReplicationControllers, 10 Secrets, 10 ConfigMaps, and 4 PersistentVolumeClaims
    can be created in the namespace.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 在命名空间中可以创建最多 10 个 Pods、5 个 ReplicationControllers、10 个 Secrets、10 个 ConfigMaps
    和 4 个 PersistentVolumeClaims。
- en: 2 Five Services overall can be created, of which at most one can be a LoadBalancer
    Service and at most two can be NodePort Services.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 总共可以创建五个服务，其中最多一个可以是 LoadBalancer 服务，最多两个可以是 NodePort 服务。
- en: 3 Only two PVCs can claim storage with the ssd StorageClass.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 只有两个 PVC 可以使用 ssd StorageClass 声明存储。
- en: The ResourceQuota in this listing allows users to create at most 10 Pods in
    the namespace, regardless if they’re created manually or by a ReplicationController,
    ReplicaSet, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers
    to five. A maximum of five Services can be created, of which only one can be a
    `LoadBalancer`-type Service, and only two can be `NodePort` Services. Similar
    to how the maximum amount of requested storage can be specified per StorageClass,
    the number of PersistentVolumeClaims can also be limited per StorageClass.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在此列表中，ResourceQuota 允许用户在命名空间中创建最多 10 个 Pods，无论它们是手动创建还是由 ReplicationController、ReplicaSet、DaemonSet、Job
    等创建。它还限制了 ReplicationController 的数量为五个。最多可以创建五个服务，其中只有一个可以是 `LoadBalancer` 类型的服务，并且只有两个可以是
    `NodePort` 类型的服务。类似于可以指定每个 StorageClass 的最大请求存储量，也可以按 StorageClass 限制 PersistentVolumeClaims
    的数量。
- en: 'Object count quotas can currently be set for the following objects:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当前可以为以下对象设置对象计数配额：
- en: Pods
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods
- en: ReplicationControllers
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicationControllers
- en: Secrets
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Secrets
- en: ConfigMaps
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ConfigMaps
- en: PersistentVolumeClaims
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PersistentVolumeClaims
- en: Services (in general), and for two specific types of Services, such as `Load-Balancer`
    Services (`services.loadbalancers`) and `NodePort` Services (`services.nodeports`)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Services（一般），以及两种特定类型的 Services，例如 `Load-Balancer` Services (`services.loadbalancers`)
    和 `NodePort` Services (`services.nodeports`)
- en: Finally, you can even set an object count quota for ResourceQuota objects themselves.
    The number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses,
    and so on, cannot be limited yet (but this may have changed since the book was
    published, so please check the documentation for up-to-date information).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您甚至可以为 ResourceQuota 对象本身设置对象计数配额。其他对象（如 ReplicaSets、Jobs、Deployments、Ingresses
    等）的数量还不能限制（但自本书出版以来，这可能已经改变，所以请查阅文档以获取最新信息）。
- en: 14.5.4\. Specifying quotas for specific pod states and/or QoS classes
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 14.5.4\. 为特定 pod 状态和/或 QoS 类指定配额
- en: 'The quotas you’ve created so far have applied to all pods, regardless of their
    current state and QoS class. But quotas can also be limited to a set of quota
    scopes. Four scopes are currently available: `BestEffort`, `NotBestEffort`, `Terminating`,
    and `NotTerminating`.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 您迄今为止创建的配额已应用于所有 Pods，无论它们的当前状态和 QoS 类别如何。但是，配额也可以限制到一组配额范围。目前有四个范围可用：`BestEffort`、`NotBestEffort`、`Terminating`
    和 `NotTerminating`。
- en: The `BestEffort` and `NotBestEffort` scopes determine whether the quota applies
    to pods with the `BestEffort` QoS class or with one of the other two classes (that
    is, `Burstable` and `Guaranteed`).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`BestEffort` 和 `NotBestEffort` 范围决定了配额是否适用于具有 `BestEffort` QoS 类的 Pods 或其他两个类别（即
    `Burstable` 和 `Guaranteed`）的 Pods。'
- en: The other two scopes (`Terminating` and `NotTerminating`) don’t apply to pods
    that are (or aren’t) in the process of shutting down, as the name might lead you
    to believe. We haven’t talked about this, but you can specify how long each pod
    is allowed to run before it’s terminated and marked as `Failed`. This is done
    by setting the `active-Deadline-Seconds` field in the pod spec. This property
    defines the number of seconds a pod is allowed to be active on the node relative
    to its start time before it’s marked as `Failed` and then terminated. The `Terminating`
    quota scope applies to pods that have the `active-DeadlineSeconds` set, whereas
    the `Not-Terminating` applies to those that don’t.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 其他两个作用域（`Terminating`和`NotTerminating`）不适用于正在（或未在）关闭过程中的pods，正如名称可能让您所想的那样。我们还没有讨论这一点，但您可以指定每个pods在终止并标记为`Failed`之前允许运行的最长时间。这是通过在pods规范中设置`active-Deadline-Seconds`字段来完成的。此属性定义了pods在节点上相对于其启动时间允许活跃的秒数，在标记为`Failed`并终止之前。`Terminating`配额作用域适用于已设置`active-DeadlineSeconds`的pods，而`Not-Terminating`适用于未设置的pods。
- en: When creating a ResourceQuota, you can specify the scopes that it applies to.
    A pod must match all the specified scopes for the quota to apply to it. Additionally,
    what a quota can limit depends on the quota’s scope. `BestEffort` scope can only
    limit the number of pods, whereas the other three scopes can limit the number
    of pods, CPU/memory requests, and CPU/memory limits.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建ResourceQuota时，您可以指定它应用的作用域。一个pods必须匹配所有指定的作用域，配额才能应用于它。此外，配额可以限制的内容取决于配额的作用域。`BestEffort`作用域只能限制pods的数量，而其他三个作用域可以限制pods的数量、CPU/内存请求和CPU/内存限制。
- en: If, for example, you want the quota to apply only to `BestEffort`, `NotTerminating`
    pods, you can create the ResourceQuota object shown in the following listing.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您希望配额仅应用于`BestEffort`、`NotTerminating` pods，您可以创建以下列表中所示的ResourceQuota对象。
- en: 'Listing 14.17\. ResourceQuota for `BestEffort/NotTerminating` pods: quota-scoped.yaml'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.17\. `BestEffort/NotTerminating` pods的ResourceQuota：quota-scoped.yaml
- en: '`apiVersion: v1 kind: ResourceQuota metadata:   name: besteffort-notterminating-pods
    spec:   scopes:` `1` `- BestEffort` `1` `- NotTerminating` `1` `hard:     pods:
    4` `2`'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ResourceQuota metadata:   name: besteffort-notterminating-pods
    spec:   scopes:` `1` `- BestEffort` `1` `- NotTerminating` `1` `hard:     pods:
    4` `2`'
- en: 1 This quota only applies to pods that have the BestEffort QoS and don’t have
    an active deadline set.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 此配额仅适用于具有BestEffort QoS且未设置活动截止日期的pods。
- en: 2 Only four such pods can exist.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 只能存在四个这样的pods。
- en: This quota ensures that at most four pods exist with the `BestEffort` QoS class,
    which don’t have an active deadline. If the quota was targeting `NotBestEffort`
    pods instead, you could also specify `requests.cpu`, `requests.memory`, `limits.cpu`,
    and `limits.memory`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此配额确保最多存在四个具有`BestEffort` QoS类且没有活动截止日期的pods。如果配额针对的是`NotBestEffort` pods，您也可以指定`requests.cpu`、`requests.memory`、`limits.cpu`和`limits.memory`。
- en: '|  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before you move on to the next section of this chapter, please delete all the
    ResourceQuota and LimitRange resources you created. You won’t need them anymore
    and they may interfere with examples in the following chapters.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在您进入本章的下一节之前，请删除您创建的所有ResourceQuota和LimitRange资源。您将不再需要它们，并且它们可能会干扰以下章节中的示例。
- en: '|  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 14.6\. Monitoring pod resource usage
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 14.6\. 监控pod资源使用情况
- en: Properly setting resource requests and limits is crucial for getting the most
    out of your Kubernetes cluster. If requests are set too high, your cluster nodes
    will be underutilized and you’ll be throwing money away. If you set them too low,
    your apps will be CPU-starved or even killed by the OOM Killer. How do you find
    the sweet spot for requests and limits?
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 正确设置资源请求和限制对于充分利用您的Kubernetes集群至关重要。如果请求设置得太高，您的集群节点将得不到充分利用，您会浪费金钱。如果设置得太低，您的应用程序可能会因CPU不足而被杀死。您如何找到请求和限制的最佳平衡点？
- en: You find it by monitoring the actual resource usage of your containers under
    the expected load levels. Once the application is exposed to the public, you should
    keep monitoring it and adjust the resource requests and limits if required.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过监控容器在预期负载水平下的实际资源使用情况来找到它。一旦应用程序向公众开放，您应该继续监控它，并在必要时调整资源请求和限制。
- en: 14.6.1\. Collecting and retrieving actual resource usages
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 14.6.1\. 收集和检索实际资源使用情况
- en: How does one monitor apps running in Kubernetes? Luckily, the Kubelet itself
    already contains an agent called cAdvisor, which performs the basic collection
    of resource consumption data for both individual containers running on the node
    and the node as a whole. Gathering those statistics centrally for the whole cluster
    requires you to run an additional component called Heapster.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如何监控在 Kubernetes 中运行的应用程序？幸运的是，Kubelet 本身已经包含了一个名为 cAdvisor 的代理，它执行节点上运行的每个容器以及整个节点的资源消耗数据的基本收集。为了在整个集群中集中收集这些统计信息，您需要运行一个名为
    Heapster 的附加组件。
- en: Heapster runs as a pod on one of the nodes and is exposed through a regular
    Kubernetes Service, making it accessible at a stable IP address. It collects the
    data from all cAdvisors in the cluster and exposes it in a single location. [Figure
    14.8](#filepos1410686) shows the flow of the metrics data from the pods, through
    cAdvisor and finally into Heapster.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster 作为 pod 在节点上运行，并通过常规 Kubernetes 服务暴露，使其可以通过稳定的 IP 地址访问。它从集群中的所有 cAdvisors
    收集数据，并在单个位置暴露。![图 14.8](#filepos1410686) 展示了从 pod 到 cAdvisor，再到 Heapster 的指标数据流。
- en: Figure 14.8\. The flow of metrics data into Heapster
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.8\. 指标数据流入 Heapster 的流程]'
- en: '![](images/00010.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图片 00010.jpg](images/00010.jpg)'
- en: The arrows in the figure show how the metrics data flows. They don’t show which
    component connects to which to get the data. The pods (or the containers running
    therein) don’t know anything about cAdvisor, and cAdvisor doesn’t know anything
    about Heapster. It’s Heapster that connects to all the cAdvisors, and it’s the
    cAdvisors that collect the container and node usage data without having to talk
    to the processes running inside the pods’ containers.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的箭头显示了指标数据的流动方式。它们不显示哪个组件连接到哪个以获取数据。pod（或在其中运行的容器）对 cAdvisor 一无所知，cAdvisor
    也不知道 Heapster。是 Heapster 连接到所有 cAdvisors，而 cAdvisors 收集容器和节点使用数据，无需与 pod 容器内运行的进程交谈。
- en: Enabling Heapster
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 Heapster
- en: 'If you’re running a cluster in Google Kubernetes Engine, Heapster is enabled
    by default. If you’re using Minikube, it’s available as an add-on and can be enabled
    with the following command:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Google Kubernetes Engine 上运行集群，Heapster 默认启用。如果您使用 Minikube，它作为附加组件可用，可以使用以下命令启用：
- en: '`$ minikube addons enable heapster` `heapster was successfully enabled`'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ minikube addons enable heapster` `heapster was successfully enabled`'
- en: To run Heapster manually in other types of Kubernetes clusters, you can refer
    to instructions located at [https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要在其他类型的 Kubernetes 集群中手动运行 Heapster，您可以参考位于 [https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)
    的说明。
- en: After enabling Heapster, you’ll need to wait a few minutes for it to collect
    metrics before you can see resource usage statistics for your cluster, so be patient.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 Heapster 后，您需要等待几分钟，以便它收集指标，然后您才能看到集群的资源使用统计信息，所以请耐心等待。
- en: Displaying CPU and Memory usage for cluster nodes
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 显示集群节点的 CPU 和内存使用情况
- en: Running Heapster in your cluster makes it possible to obtain resource usages
    for nodes and individual pods through the `kubectl top` command. To see how much
    CPU and memory is being used on your nodes, you can run the command shown in the
    following listing.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的集群中运行 Heapster 可以通过 `kubectl top` 命令获取节点和单个 pod 的资源使用情况。要查看您的节点上使用了多少 CPU
    和内存，可以运行以下列表中的命令。
- en: Listing 14.18\. Actual CPU and memory usage of nodes
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.18\. 节点的实际 CPU 和内存使用情况
- en: '`$ kubectl top node` `NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
    minikube   170m         8%        556Mi           27%`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl top node` `NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
    minikube   170m         8%        556Mi           27%`'
- en: This shows the actual, current CPU and memory usage of all the pods running
    on the node, unlike the `kubectl describe node` command, which shows the amount
    of CPU and memory requests and limits instead of actual runtime usage data.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了节点上运行的所有 pod 的实际、当前 CPU 和内存使用情况，与 `kubectl describe node` 命令不同，后者显示的是 CPU
    和内存请求量以及限制量，而不是实际的运行时使用数据。
- en: Displaying CPU and Memory usage for individual pods
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 显示单个 pod 的 CPU 和内存使用情况
- en: To see how much each individual pod is using, you can use the `kubectl top pod`
    command, as shown in the following listing.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看每个 pod 使用了多少资源，可以使用 `kubectl top pod` 命令，如下所示。
- en: Listing 14.19\. Actual CPU and memory usages of pods
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.19\. pod 的实际 CPU 和内存使用情况
- en: '`$ kubectl top pod --all-namespaces` `NAMESPACE      NAME                            
    CPU(cores)   MEMORY(bytes) kube-system    influxdb-grafana-2r2w9           1m          
    32Mi kube-system    heapster-40j6d                   0m           18Mi default       
    kubia-3773182134-63bmb           0m           9Mi kube-system    kube-dns-v20-z0hq6              
    1m           11Mi kube-system    kubernetes-dashboard-r53mc       0m          
    14Mi kube-system    kube-addon-manager-minikube      7m           33Mi`'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl top pod --all-namespaces` `NAMESPACE      NAME                            
    CPU(cores)   MEMORY(bytes) kube-system    influxdb-grafana-2r2w9           1m          
    32Mi kube-system    heapster-40j6d                   0m           18Mi default       
    kubia-3773182134-63bmb           0m           9Mi kube-system    kube-dns-v20-z0hq6              
    1m           11Mi kube-system    kubernetes-dashboard-r53mc       0m          
    14Mi kube-system    kube-addon-manager-minikube      7m           33Mi`'
- en: 'The outputs of both these commands are fairly simple, so you probably don’t
    need me to explain them, but I do need to warn you about one thing. Sometimes
    the `top pod` command will refuse to show any metrics and instead print out an
    error like this:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个命令的输出相当简单，所以你可能不需要我解释，但我确实需要提醒你一件事。有时 `top pod` 命令会拒绝显示任何指标，而是打印出类似这样的错误：
- en: '`$ kubectl top pod` `W0312 22:12:58.021885   15126 top_pod.go:186] Metrics
    not available for pod default/kubia-3773182134-63bmb, age: 1h24m19.021873823s
    error: Metrics not available for pod default/kubia-3773182134-63bmb, age: 1h24m19.021873823s`'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl top pod` `W0312 22:12:58.021885   15126 top_pod.go:186] 指标对于 pod
    default/kubia-3773182134-63bmb 不可用，年龄：1h24m19.021873823s 错误：对于 pod default/kubia-3773182134-63bmb
    指标不可用，年龄：1h24m19.021873823s`'
- en: If this happens, don’t start looking for the cause of the error yet. Relax,
    wait a while, and rerun the command—it may take a few minutes, but the metrics
    should appear eventually. The `kubectl top` command gets the metrics from Heapster,
    which aggregates the data over a few minutes and doesn’t expose it immediately.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生这种情况，请不要立即寻找错误的根源。放松一下，稍等片刻，然后重新运行命令——可能需要几分钟，但指标最终应该会显示出来。`kubectl top`
    命令从 Heapster 获取指标，Heapster 会将数据聚合几分钟，并不会立即暴露出来。
- en: '|  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: To see resource usages across individual containers instead of pods, you can
    use the `--containers` option.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看跨各个容器的资源使用情况而不是 pod，可以使用 `--containers` 选项。
- en: '|  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 14.6.2\. Storing and analyzing historical resource consumption statistics
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 14.6.2\. 存储和分析历史资源消耗统计
- en: The `top` command only shows current resource usages—it doesn’t show you how
    much CPU or memory your pods consumed throughout the last hour, yesterday, or
    a week ago, for example. In fact, both cAdvisor and Heapster only hold resource
    usage data for a short window of time. If you want to analyze your pods’ resource
    consumption over longer time periods, you’ll need to run additional tools.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '`top` 命令只显示当前资源使用情况——它不会显示你的 pod 在过去一小时、昨天或一周前消耗了多少 CPU 或内存，例如。实际上，cAdvisor
    和 Heapster 只保留短时间窗口的资源使用数据。如果你想分析你的 pod 在更长的时间段内的资源消耗，你需要运行额外的工具。'
- en: When using Google Kubernetes Engine, you can monitor your cluster with Google
    Cloud Monitoring, but when you’re running your own local Kubernetes cluster (either
    through Minikube or other means), people usually use InfluxDB for storing statistics
    data and Grafana for visualizing and analyzing them.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Google Kubernetes Engine 时，你可以使用 Google Cloud Monitoring 监控你的集群，但当你运行自己的本地
    Kubernetes 集群（无论是通过 Minikube 还是其他方式）时，人们通常使用 InfluxDB 存储统计数据，并使用 Grafana 进行可视化和分析。
- en: Introducing InfluxDB and Grafana
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍 InfluxDB 和 Grafana
- en: InfluxDB is an open source time-series database ideal for storing application
    metrics and other monitoring data. Grafana, also open source, is an analytics
    and visualization suite with a nice-looking web console that allows you to visualize
    the data stored in InfluxDB and discover how your application’s resource usage
    behaves over time (an example showing three Grafana charts is shown in [figure
    14.9](#filepos1417307)).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: InfluxDB 是一个开源的时间序列数据库，非常适合存储应用程序指标和其他监控数据。Grafana，也是开源的，是一个具有美观网页控制台的分析和可视化套件，它允许你可视化存储在
    InfluxDB 中的数据，并发现你的应用程序资源使用随时间的变化情况（如图 14.9 所示的三个 Grafana 图表示例）。
- en: Figure 14.9\. Grafana dashboard showing CPU usage across the cluster
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9\. 显示集群 CPU 使用情况的 Grafana 仪表板
- en: '![](images/00029.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00029.jpg)'
- en: Running InfluxDB and Grafana in your cluster
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的集群中运行 InfluxDB 和 Grafana
- en: Both InfluxDB and Grafana can run as pods. Deploying them is straightforward.
    All the necessary manifests are available in the Heapster Git repository at [http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb](http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: InfluxDB和Grafana都可以作为Pod运行。部署它们很简单。所有必要的清单都可在Heapster Git仓库的[http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb](http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb)中找到。
- en: When using Minikube, you don’t even need to deploy them manually, because they’re
    deployed along with Heapster when you enable the Heapster add-on.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Minikube时，你甚至不需要手动部署它们，因为当你启用Heapster附加组件时，它们会与Heapster一起部署。
- en: Analyzing resource usage with Grafana
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Grafana分析资源使用情况
- en: 'To discover how much of each resource your pod requires over time, open the
    Grafana web console and explore the predefined dashboards. Generally, you can
    find out the URL of Grafana’s web console with `kubectl cluster-info`:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 要发现你的Pod随时间需要多少每种资源，请打开Grafana Web控制台并探索预定义的仪表板。通常，你可以使用`kubectl cluster-info`找到Grafana
    Web控制台的URL：
- en: '`$ kubectl cluster-info` `...` `monitoring-grafana` `is running at      https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-
         system/services/monitoring-grafana`'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl cluster-info` `...` `monitoring-grafana` `运行在 https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-
    system/services/monitoring-grafana`'
- en: 'When using Minikube, Grafana’s web console is exposed through a `NodePort`
    Service, so you can open it in your browser with the following command:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Minikube时，Grafana的Web控制台通过`NodePort`服务暴露，因此你可以使用以下命令在浏览器中打开它：
- en: '`$ minikube service monitoring-grafana -n kube-system` `Opening kubernetes
    service kube-system/monitoring-grafana in default      browser...`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ minikube service monitoring-grafana -n kube-system` `在默认浏览器中打开kubernetes服务kube-system/monitoring-grafana...`'
- en: 'A new browser window or tab will open and show the Grafana Home screen. On
    the right-hand side, you’ll see a list of dashboards containing two entries:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 将会打开一个新的浏览器窗口或标签页，显示Grafana的主屏幕。在右侧，你会看到一个包含两个条目的仪表板列表：
- en: Cluster
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群
- en: Pods
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods
- en: To see the resource usage statistics of the nodes, open the Cluster dashboard.
    There you’ll see several charts showing the overall cluster usage, usage by node,
    and the individual usage for CPU, memory, network, and filesystem. The charts
    will not only show the actual usage, but also the requests and limits for those
    resources (where they apply).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看节点的资源使用统计信息，请打开集群仪表板。在那里，你会看到几个图表，显示整体集群使用情况、按节点使用情况以及CPU、内存、网络和文件系统的单个使用情况。图表不仅会显示实际使用情况，还会显示那些资源的请求和限制（如果适用）。
- en: If you then switch over to the Pods dashboard, you can examine the resource
    usages for each individual pod, again with both requests and limits shown alongside
    the actual usage.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你切换到Pods仪表板，你可以检查每个单独Pod的资源使用情况，同样会显示请求和限制与实际使用情况并列。
- en: 'Initially, the charts show the statistics for the last 30 minutes, but you
    can zoom out and see the data for much longer time periods: days, months, or even
    years.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，图表显示过去30分钟的统计数据，但你可以放大并查看更长时间段的数据：几天、几个月，甚至几年。
- en: Using the information shown in the charts
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图表中显示的信息
- en: By looking at the charts, you can quickly see if the resource requests or limits
    you’ve set for your pods need to be raised or whether they can be lowered to allow
    more pods to fit on your nodes. Let’s look at an example. [Figure 14.10](#filepos1420805)
    shows the CPU and memory charts for a pod.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看图表，你可以快速了解为你的Pod设置的资源请求或限制是否需要提高，或者是否可以降低以允许更多的Pod适合你的节点。让我们看看一个例子。[图14.10](#filepos1420805)显示了Pod的CPU和内存图表。
- en: Figure 14.10\. CPU and memory usage chart for a pod
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10. Pod的CPU和内存使用图表
- en: '![](images/00049.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00049.jpg)'
- en: At the far right of the top chart, you can see the pod is using more CPU than
    was requested in the pod’s manifest. Although this isn’t problematic when this
    is the only pod running on the node, you should keep in mind that a pod is only
    guaranteed as much of a resource as it requests through resource requests. Your
    pod may be running fine now, but when other pods are deployed to the same node
    and start using the CPU, your pod’s CPU time may be throttled. Because of this,
    to ensure the pod can use as much CPU as it needs to at any time, you should raise
    the CPU resource request for the pod’s container.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部图表的最右侧，您可以看到Pod使用的CPU比Pod的清单中请求的要多。尽管当这是节点上唯一运行的Pod时，这并不成问题，但您应该记住，Pod只能保证通过资源请求请求的资源量。您的Pod现在可能运行良好，但当其他Pod部署到同一节点并开始使用CPU时，您的Pod的CPU时间可能会被限制。因此，为了确保Pod在任何时候都能使用它所需的尽可能多的CPU，您应该提高Pod容器的CPU资源请求。
- en: The bottom chart shows the pod’s memory usage and request. Here the situation
    is the exact opposite. The amount of memory the pod is using is well below what
    was requested in the pod’s spec. The requested memory is reserved for the pod
    and won’t be available to other pods. The unused memory is therefore wasted. You
    should decrease the pod’s memory request to make the memory available to other
    pods running on the node.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 底部图表显示了Pod的内存使用情况和请求。这里的情况正好相反。Pod使用的内存量远远低于Pod规范中请求的量。请求的内存为Pod保留，不会对其他Pod可用。因此，未使用的内存因此被浪费。您应该降低Pod的内存请求，以便将内存提供给节点上运行的其他Pod。
- en: 14.7\. Summary
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 14.7. 摘要
- en: This chapter has shown you that you need to consider your pod’s resource usage
    and configure both the resource requests and the limits for your pod to keep everything
    running smoothly. The key takeaways from this chapter are
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了您需要考虑您的Pod资源使用情况，并为您的Pod配置资源请求和限制，以保持一切运行顺畅。本章的关键要点是
- en: Specifying resource requests helps Kubernetes schedule pods across the cluster.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定资源请求有助于Kubernetes在集群中调度Pod。
- en: Specifying resource limits keeps pods from starving other pods of resources.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定资源限制可以防止Pod使其他Pod的资源匮乏。
- en: Unused CPU time is allocated based on containers’ CPU requests.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未使用的CPU时间是根据容器的CPU请求分配的。
- en: Containers never get killed if they try to use too much CPU, but they are killed
    if they try to use too much memory.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果容器尝试使用过多的CPU，它们不会被杀死，但如果它们尝试使用过多的内存，它们会被杀死。
- en: In an overcommitted system, containers also get killed to free memory for more
    important pods, based on the pods’ QoS classes and actual memory usage.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过度承诺的系统中，容器也会被杀死，以释放内存，为更重要的Pod提供内存，这基于Pod的QoS类别和实际内存使用情况。
- en: You can use LimitRange objects to define the minimum, maximum, and default resource
    requests and limits for individual pods.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用LimitRange对象来定义单个Pod的最小、最大和默认资源请求和限制。
- en: You can use ResourceQuota objects to limit the amount of resources available
    to all the pods in a namespace.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用ResourceQuota对象来限制命名空间中所有Pod可用的资源量。
- en: To know how high to set a pod’s resource requests and limits, you need to monitor
    how the pod uses resources over a long-enough time period.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要知道如何设置Pod的资源请求和限制，您需要监控Pod在足够长的时间内的资源使用情况。
- en: In the next chapter, you’ll see how these metrics can be used by Kubernetes
    to automatically scale your pods.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将看到这些指标如何被Kubernetes用来自动扩展您的Pod。
