- en: Chapter 5\. Accessing Web-Based Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章\. 访问基于 Web 的数据
- en: The internet is an incredible source of data; it is, arguably, the reason that
    data has become such a dominant part of our social, economic, political, and even
    creative lives. In [Chapter 4](ch04.html#chapter4), we focused our data wrangling
    efforts on the process of accessing and reformatting file-based data that had
    already been saved to our devices or to the cloud. At the same time, much of it
    came from the internet originally—whether it was downloaded from a website, like
    the unemployment data, or retrieved from a URL, like the Citi Bike data. Now that
    we have a handle on how to use Python to parse and transform a variety of file-based
    data formats, however, it’s time to look at what’s involved in collecting those
    files in the first place—especially when the data they contain is of the real-time,
    feed-based variety. To do this, we’re going to spend the bulk of this chapter
    learning how to get ahold of data made available through APIs—those *a*pplication
    *p*rogramming *i*nterfaces I mentioned early in [Chapter 4](ch04.html#chapter4).
    APIs are the primary (and sometimes only) way that we can access the data generated
    by real-time or on-demand services like social media platforms, streaming music,
    and search services—as well as many other private and public (e.g., government-generated)
    data sources.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网是一个不可思议的数据来源；可以说，这是数据成为我们社会、经济、政治甚至创意生活中如此主导的原因。在 [第 4 章](ch04.html#chapter4)
    中，我们专注于数据整理的过程，重点是访问和重新格式化已保存在我们设备或云端的基于文件的数据。与此同时，这些数据的大部分最初来自互联网 —— 无论是从网站下载的，如失业数据，还是从
    URL 检索的，如 Citi Bike 数据。然而，现在我们已经掌握了如何使用 Python 解析和转换各种基于文件的数据格式，是时候看看首先收集这些文件涉及哪些内容了
    —— 特别是当这些文件中包含的数据是实时的、基于 feed 的类型时。为此，我们将在本章的大部分时间内学习如何获取通过 API 提供的数据 —— 这些是我在
    [第 4 章](ch04.html#chapter4) 中早期提到的应用程序接口。API 是我们访问由实时或按需服务生成的数据的主要（有时是唯一）途径，例如社交媒体平台、流媒体音乐和搜索服务，以及许多其他私人和公共（例如政府生成的）数据来源。
- en: While the many benefits of APIs (see [“Why APIs?”](#why_apis) for a refresher)
    make them a popular resource for data-collecting companies to offer, there are
    significant costs and risks to doing so. For advertising-driven businesses like
    social media platforms, an outside product or project that is too comprehensive
    in its data collection is a profit risk. The ready availability of so much data
    about individuals has also significantly [increased privacy risks](https://dataprivacylab.org/projects/kanonymity/kanonymity.pdf).
    As a result, accessing data via many APIs requires registering with the data collector
    in advance, and even completing a code-based login or *authentication* process
    anytime you request data. At the same time, the data accessibility that APIs offer
    is a powerful tool for improving the transparency of government systems^([1](ch05.html#idm45143408861008))
    and accountability for private companies,^([2](ch05.html#idm45143408859632)) so
    the up-front work of creating an account and protecting any Python scripts you
    make that access API-based data is well worth the effort.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 API 的许多优点（参见 [“为什么使用 API？”](#why_apis) 进行复习）使它们成为数据收集公司提供的热门资源，但这样做也存在显著的成本和风险。对于像社交媒体平台这样以广告为驱动的企业来说，一个过于全面的外部产品或项目在数据收集方面是一种利润风险。关于个人的大量数据的即时可用性也显著增加了隐私风险。因此，通过许多
    API 访问数据通常需要事先向数据收集者注册，甚至在请求数据时完成基于代码的登录或 *身份验证* 过程。与此同时，API 提供的数据可访问性是改善政府系统透明度^([1](ch05.html#idm45143408861008))和私营公司责任^([2](ch05.html#idm45143408859632))的有力工具，因此创建帐户和保护访问基于
    API 的数据的任何 Python 脚本的前期工作是非常值得的。
- en: 'Over the course of this chapter, we’ll cover how to access a range of web-based,
    feed-type datasets via APIs, addressing everything from basic, no-login-required
    resources all the way to the multistep, highly protected APIs of social media
    platforms like Twitter. As we’ll see in [“Accessing Online XML and JSON”](#online_xml_and_json),
    the simpler end of this spectrum just involves using the Python *requests* library
    to download a web page already formatted as JSON or XML—all we need is the URL.
    In [“Specialized APIs: Adding Basic Authentication”](#basic_authentication), we’ll
    move on to the process of accessing data made available through the [Federal Reserve
    Economic Database (FRED)](https://fred.stlouisfed.org/docs/api/fred) API. This
    is the same data we looked at in Examples [4-12](ch04.html#xml_parsing) and [4-15](ch04.html#json_parsing),
    but rather than working with example files that I’ve provided, you’ll be programmatically
    downloading whatever data is most recent *whenever you run the script*.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的过程中，我们将介绍如何通过API访问一系列基于网络的、数据供稿类型的数据集，涵盖从基本的、无需登录的资源到社交媒体平台如Twitter的多步骤、高度保护的API。正如我们将在[“访问在线XML和JSON”](#online_xml_and_json)中看到的那样，这个光谱的简单端涉及使用Python的*requests*库下载已经格式化为JSON或XML的网页——我们只需要URL。在[“专用API：添加基本身份验证”](#basic_authentication)中，我们将继续讨论通过[Federal
    Reserve Economic Database (FRED)](https://fred.stlouisfed.org/docs/api/fred) API获取的数据访问过程。这与我们在示例[4-12](ch04.html#xml_parsing)和[4-15](ch04.html#json_parsing)中看到的数据相同，但与其使用我提供的示例文件不同，您将通过编程方式下载任何最新数据*每次运行脚本时*。
- en: 'This will require both creating a login on the FRED website as well as creating—and
    protecting—your own basic API “key” in order to retrieve data. Finally, in [“Specialized
    APIs: Working With OAuth”](#oauth_apis) we’ll cover the more complex API authentication
    process required for social media platforms like Twitter. Despite the degree of
    up-front work involved, learning how to programmatically interact with APIs like
    this has big payoffs—for the most part, you’ll be able to rerun these scripts
    at any time to retrieve the most up-to-date data these services offer.^([3](ch05.html#idm45143408848400))
    Of course, since not every data source we need offers an API, we’ll wrap up the
    chapter with [“Web Scraping: The Data Source of Last Resort”](#web_scraping) by
    explaining how we can use code to *responsibly* “scrape” data from websites with
    the *Beautiful Soup* Python library. Though in many cases these data-access tasks
    *could* be accomplished with a browser and mouse, you’ll quickly see how using
    Python helps lets us scale our data-retrieval efforts by making the process faster
    and more repeatable.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要在[FRED网站](https://fred.stlouisfed.org/docs/api/fred)上创建一个帐户，以及创建并保护您自己的基本API“密钥”，以便检索数据。最后，在[“专用API：使用OAuth工作”](#oauth_apis)中，我们将介绍像Twitter这样的社交媒体平台所需的更复杂的API身份验证过程。尽管需要大量的前期工作，但学习如何通过编程方式与这些API进行交互具有巨大的回报——在大多数情况下，您可以随时重新运行这些脚本，以获取这些服务提供的最新数据。^([3](ch05.html#idm45143408848400))
    当然，并非我们需要的每个数据源都提供API，因此我们将在[“网页抓取：最后的数据来源”](#web_scraping)中解释如何使用Python的*Beautiful
    Soup*库以*负责任的方式*“抓取”网站上的数据。尽管在许多情况下，这些数据访问任务可以通过浏览器和鼠标完成，但您很快就会看到，使用Python如何帮助我们通过使过程更快、更可重复来扩展我们的数据检索工作。
- en: Accessing Online XML and JSON
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问在线XML和JSON
- en: 'In [“Wrangling Feed-Type Data with Python”](ch04.html#wrangling_feed_data),
    we explored the process of accessing and transforming two common forms of web-based
    data: XML and JSON. What we didn’t address, however, was how to actually get those
    data files from the internet onto your computer. With the help of the versatile
    Python *requests* library, however, it only take a few lines of code to access
    and download that data without ever having to open a web browser.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“使用Python处理数据供稿类型数据”](ch04.html#wrangling_feed_data)中，我们探讨了访问和转换两种常见形式的基于网络的数据（XML和JSON）的过程。然而，我们没有解决的是如何将这些数据文件从互联网上下载到您的计算机上的问题。然而，借助多功能的Python
    *requests*库，仅需几行代码即可访问和下载这些数据，而无需打开网页浏览器。
- en: 'For the sake of comparison, let’s start by “manually” downloading two of the
    files we’ve used in previous examples: the BBC’s RSS feed of articles from [Example 4-13](ch04.html#bbc_example)
    and the Citi Bike JSON data mentioned in [“One Data Source, Two Ways”](ch04.html#one_data_two_ways).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行比较，让我们从“手动”下载我们在之前示例中使用的两个文件开始：[示例 4-13](ch04.html#bbc_example)中的BBC文章的RSS供稿和[“一个数据源，两种方式”](ch04.html#one_data_two_ways)中提到的Citi
    Bike JSON数据。
- en: 'For both of these data sources, the process is basically the same:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种数据源，处理过程基本相同：
- en: 'Visit the target URL; in this case, one of the following:'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问目标 URL；在这种情况下，选择以下之一：
- en: '[*http://feeds.bbci.co.uk/news/science_and_environment/rss.xml*](http://feeds.bbci.co.uk/news/science_and_environment/rss.xml)'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*http://feeds.bbci.co.uk/news/science_and_environment/rss.xml*](http://feeds.bbci.co.uk/news/science_and_environment/rss.xml)'
- en: '[*https://gbfs.citibikenyc.com/gbfs/en/station_status.json*](https://gbfs.citibikenyc.com/gbfs/en/station_status.json)'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://gbfs.citibikenyc.com/gbfs/en/station_status.json*](https://gbfs.citibikenyc.com/gbfs/en/station_status.json)'
- en: Context-click (also known as “right-click” or sometime “Ctrl+click,” depending
    on your system). From the menu that appears, simply choose “Save As” and save
    the file to the same folder where your Jupyter notebook or Python script is located.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文点击（也称为“右键单击”或有时“Ctrl+单击”，具体取决于您的系统）。从出现的菜单中简单地选择“另存为”，并将文件保存到您的 Jupyter 笔记本或
    Python 脚本所在的同一文件夹中。
- en: That’s it! Now you can run the scripts from [Example 4-13](ch04.html#bbc_example)
    on that updated XML file or paste the Citi Bike JSON data into [*https://jsonlint.com*](https://jsonlint.com)
    to see what it looks like when it’s properly formatted. Note that even though
    the BBC page looks almost like a “normal” website in your browser, true to its
    *.xml* file extension, it downloads as well-formatted XML.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在，您可以在更新的 XML 文件上运行 [示例 4-13](ch04.html#bbc_example) 中的脚本，或将 Citi Bike
    JSON 数据粘贴到 [*https://jsonlint.com*](https://jsonlint.com) 中，查看在正确格式化时其外观如何。请注意，尽管
    BBC 页面在浏览器中看起来几乎像是一个“普通”网站，但根据其 *.xml* 文件扩展名，它也会下载为格式良好的 XML。
- en: Now that we’ve seen how to do this part of the process by hand, let’s see what
    it takes to do the same thing in Python. To keep this short, the code in [Example 5-1](#data_download)
    will download and save *both* files, one after the other.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到如何手动完成这一过程的部分，让我们看看在 Python 中完成同样工作需要做些什么。为了简短起见，[示例 5-1](#data_download)
    中的代码将下载并保存 *两个* 文件，一个接着一个。
- en: Example 5-1\. data_download.py
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. data_download.py
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pretty simple, right? Apart from different filenames, the *.xml* and *.json*
    files produced by [Example 5-1](#data_download) are exactly the same as the ones
    we saved manually from the web. And once we have this script set up, of course,
    all we have to do to get the latest data is run it again and the new data will
    overwrite the earlier files.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单，对吧？除了不同的文件名外，[示例 5-1](#data_download)生成的 *.xml* 和 *.json* 文件与我们手动从网页保存的文件完全相同。一旦我们设置好了这个脚本，当然，要获取最新数据只需再次运行它，新数据将覆盖早期的文件。
- en: Introducing APIs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 API
- en: Up until this point, most of our data wrangling work has focused on data sources
    whose contents are almost entirely controlled by the data provider. In fact, while
    the contents of spreadsheet files, and documents—and even the web pages containing
    XML and JSON that we accessed just now in [Example 5-1](#data_download)—may change
    based on *when* we access them, we don’t really have any influence on *what* data
    they contain.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 直到这一点，我们大部分的数据整理工作都集中在数据源上，这些数据源的内容几乎完全由数据提供者控制。实际上，电子表格文件的内容，以及文档——甚至是刚才在 [示例 5-1](#data_download)
    中访问的包含 XML 和 JSON 的网页——可能根据我们访问它们的时间而变化，但我们并没有真正影响它们包含什么数据。
- en: At the same time, most of us are used to using the internet to get information
    that’s much more tailored to our needs. Often our first step when looking for
    information is to enter keywords or phrases into a search engine, and we expect
    to receive a list of highly customized “results” based (at least in part) on our
    chosen combination of terms. Sure, we can’t control what web pages are actually
    out there for our search to retrieve, but this process is so common—and so useful—for
    most of us that we rarely stop to think about what is happening behind the scenes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们大多数人习惯于使用互联网获取更符合我们需求的信息。当我们寻找信息时，通常第一步是在搜索引擎中输入关键词或短语，我们期望收到基于我们选择的组合的高度定制的“结果”列表。当然，我们无法控制实际存在哪些网页以供我们的搜索检索，但对于大多数人来说，这个过程如此常见和有用，以至于我们很少停下来考虑背后正在发生的事情。
- en: Despite their visually oriented interfaces, search engines are actually just
    a special instance of APIs. They are essentially just web pages that let you *interface*
    with a database containing information about websites on the internet, such as
    their URLs, titles, text, images, videos, and more. When you enter your search
    terms and hit Enter or Return, the search engine *queries* its database for web
    content that “matches” your search in some respect and then updates the web page
    you’re looking at to display those results in a list. Though the specialized APIs
    made available by social media platforms and other online services require us
    to authenticate *and* structure our searches in a very particular way, there are
    enough features shared between search engines and more specialized APIs that we
    can learn something useful about APIs by deconstructing a basic Google search.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们具有视觉导向的界面，但搜索引擎实际上只是 API 的一个特殊实例。它们本质上只是允许您与包含互联网上网站信息的数据库进行 *接口* 的网页，如它们的
    URL、标题、文本、图片、视频等。当您输入搜索词并按 Enter 或 Return 键时，搜索引擎会 *查询* 其数据库，以获取与您搜索有关的网页内容，并更新您正在查看的网页，以在列表中显示这些结果。尽管社交媒体平台和其他在线服务提供的专门
    API 需要我们以非常特定的方式进行身份验证 *并且* 结构化我们的搜索，但搜索引擎和更专业的 API 之间共享的功能足够多，以至于我们可以通过解构基本的
    Google 搜索来学习一些有用的 API 知识。
- en: 'Basic APIs: A Search Engine Example'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础 API：搜索引擎示例
- en: Though an internet search engine is probably the most straightforward form of
    API around, that’s not always obvious from the way we see them behave onscreen.
    For example, if you were to visit Google and search for “weather sebastopol,”
    you would probably see a page that looks something like [Figure 5-1](#google_search_results).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管互联网搜索引擎可能是最直接的 API 形式，但从屏幕上看到它们的行为并不总是显而易见。例如，如果您访问 Google 并搜索“weather sebastopol”，您可能会看到类似于
    [图 5-1](#google_search_results) 的页面。
- en: '![Sebastopol weather search results](assets/ppdw_0501.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Sebastopol weather search results](assets/ppdw_0501.png)'
- en: Figure 5-1\. Sample search results
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 样本搜索结果
- en: 'While the format of the search results is probably pretty familiar, right now
    let’s take a closer look at what’s happening in the URL bar. What you see will
    definitely be different from the [Figure 5-1](#google_search_results) screenshot,
    but it should contain at least some of the same information. Specifically, look
    through the text that now appears in *your* URL bar to find the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然搜索结果的格式可能非常熟悉，但现在让我们更仔细地查看 URL 栏中发生的事情。您看到的肯定与 [图 5-1](#google_search_results)
    的屏幕截图不同，但它应该至少包含部分相同的信息。具体来说，现在在 *您的* URL 栏中查找以下内容：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Found it? Great. Now without refreshing the page, change the text in the search
    box to “weather san francisco” and hit Enter. Once again look through the text
    in the URL to find:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 找到了吗？太好了。现在在不刷新页面的情况下，将搜索框中的文本更改为“weather san francisco”，然后按 Enter 键。再次查看 URL
    中的文本，找到：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, copy and paste the following into your URL bar and hit Enter:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，复制并粘贴以下内容到您的 URL 栏，然后按 Enter 键：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice anything? Hopefully, you’re seeing the same (or *almost* the same) search
    results when you type “weather san francisco” into Google’s search bar and hit
    Enter as when you directly visit the Google search URL with the key/value pair
    of `q=weather+san+francisco` appended (e.g., `https://www.google.com/search?q=weather+san+francisco`).
    That’s because `q=weather+san+francisco` is the part of the *query string* that
    delivers your actual search terms to Google’s database; everything else is just
    additional information Google tacks on for customization or tracking purposes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到了吗？希望当您在 Google 的搜索栏中输入“weather san francisco”并按 Enter 键时，您看到的搜索结果与直接访问 Google
    搜索 URL 并附加 `q=weather+san+francisco` 的情况几乎相同。这是因为 `q=weather+san+francisco` 是传送您实际搜索词到
    Google 数据库的 *查询字符串* 部分；其他所有内容只是 Google 为定制或跟踪目的附加的附加信息。
- en: 'While Google can (and will!) add whatever it wants to our search URL, *we*
    can also add other useful key/value pairs. For example, in [“Smart Searching for
    Specific Data Types”](ch04.html#smart_searching), we looked at searching for specific
    file types, such as *.xml*, by adding `filetype: .xml` to our search box query;
    we can do the same thing directly in the URL bar by adding the corresponding key/value
    pair of `as_filetype=xml` to our query string:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然Google可以（而且将会！）向我们的搜索URL添加任何它想要的内容，*我们*也可以通过添加其他有用的键值对来扩展其功能。例如，在[“智能搜索特定数据类型”](ch04.html#smart_searching)中，我们探讨了通过在搜索框查询中添加`filetype:
    .xml`来搜索特定文件类型，如*.xml*；同样，我们可以直接在URL栏中通过添加相应的键值对`as_filetype=xml`来完成相同的操作：'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Not only will this return results in the correct format, but notice that it
    updates the contents of the search box as well!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅将返回正确格式的结果，还请注意它也会更新搜索框的内容！
- en: The behavior of the Google search engine in this situation is almost identical
    to what we’ll see with more specialized APIs in the remainder of this chapter.
    Most APIs follow the general structure we’re seeing in this search example, where
    an *endpoint* (in this case `https://www.google.com/search`) is combined with
    one or more *query parameters* or *key/value pairs* (such as `as_filetype=xml`
    or `q=weather+san+francisco`), which comprise the *query string* that is appended
    after the question mark (`?`). A general overview of API endpoint and query string
    structure is shown in [Figure 5-2](#query_string_structure).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Google搜索引擎在这种情况下的行为几乎与我们在本章剩余部分将看到的更专业API相同。大多数API遵循我们在此搜索示例中看到的一般结构，其中*终端点*（在本例中为`https://www.google.com/search`）与一个或多个*查询参数*或*键值对*（例如`as_filetype=xml`或`q=weather+san+francisco`）结合，形成*查询字符串*，该字符串附加在表示查询字符串开始的问号（`?`）之后。API终端点和查询字符串结构的一般概述如[图 5-2](#query_string_structure)所示。
- en: '![Sebastopol weather search query, limited to 5 results](assets/ppdw_0502.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![塞巴斯托波尔天气搜索查询，限制为5个结果](assets/ppdw_0502.png)'
- en: Figure 5-2\. Basic query string structure
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 基本查询字符串结构
- en: 'While this structure is pretty universal, here are a couple of other useful
    tips about query string-based APIs:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种结构非常普遍，但以下是关于基于查询字符串的API的一些其他有用提示：
- en: Key/value pairs (such as `as_filetype=xml`, `num=5`, or even `q=weather+san+francisco`)
    can appear *in any order*, as long as they are added after the question mark (`?`)
    that indicates the start of the query string.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键值对（例如`as_filetype=xml`，`num=5`，甚至`q=weather+san+francisco`）可以*以任何顺序*出现，只要它们添加在表示查询字符串开始的问号（`?`）之后即可。
- en: The particular keys and values that are meaningful for a given API are determined
    by the API provider and can only be identified by reading the API documentation,
    or through experimentation (though this can present problems of its own). Anything
    appended to the query string that is not a recognized key or valid parameter value
    will probably be ignored.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定API而言，具体的键和值是由API提供者确定的，只有通过阅读API文档或通过实验（尽管这可能会带来自身的问题），才能识别出来。附加到查询字符串中的任何不是被识别键或有效参数值的内容都可能会被忽略。
- en: While these characteristics are common to almost all APIs, the vast majority
    of them will not allow you to access any data at all without first identifying
    (or *authenticating*) yourself by creating a login and providing unique, specialized
    “keys” to the API along with your queries. This part of the API process is what
    we’ll turn to next.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些特征几乎适用于所有API，但其中绝大多数在您创建登录并提供独特的专用“密钥”以及查询时，不会允许您访问任何数据，这些密钥需要首先进行身份验证（或*认证*）。这部分API流程是我们接下来将要讨论的内容。
- en: 'Specialized APIs: Adding Basic Authentication'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专用API：添加基本认证
- en: The first step in using most APIs is creating some kind of account with the
    API provider. Although many APIs allow *you* to use them for free, the process
    of compiling, storing, searching for, and returning data to you over the internet
    still presents risks and costs money, so providers want to track who is using
    their APIs and be able to cut off your access if they want to.^([4](ch05.html#idm45143408324000))
    This first part of the *authentication* process usually consists of creating an
    account and requesting an API “key” for yourself and/or each project, program,
    or “app” that you plan to have interact with the API. In a “basic” API authentication
    process, like the one we’ll go through now, once you’ve created your API key on
    the service provider’s website, all you need to do to retrieve data successfully
    is append your key to your data request just like any other query parameter, and
    you’re all set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大多数 API 的第一步是与 API 提供商创建某种类型的账户。尽管许多 API 允许*您*免费使用它们，但在互联网上编译、存储、搜索和返回数据的过程仍然存在风险并且需要花钱，因此提供商希望跟踪谁在使用他们的
    API，并且可以随时切断您的访问权限。^([4](ch05.html#idm45143408324000)) 这个身份验证过程的第一部分通常包括创建一个账户并为自己和/或每个项目、程序或“应用程序”请求一个
    API “密钥”。在像我们现在要进行的“基本” API 身份验证过程中，一旦您在服务提供商的网站上创建了您的 API 密钥，您只需像任何其他查询参数一样将其附加到您的数据请求中，即可成功检索数据。
- en: As an example, let’s get set up to programmatically access the unemployment
    data we worked with in [Example 4-15](ch04.html#json_parsing). We’ll start by
    making an account on the FRED website and requesting an API key. Once we have
    that, we can just append it to our query string and start downloading data!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，让我们开始设置以编程方式访问我们在[示例 4-15](ch04.html#json_parsing)中使用的失业数据。我们将首先在 FRED
    网站上创建一个账户并请求一个 API 密钥。一旦我们有了这个，我们就可以简单地将其附加到我们的查询字符串中并开始下载数据！
- en: Getting a FRED API Key
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 FRED API 密钥
- en: To create an account with the Federal Reserve Economic Database (FRED), visit
    [*https://fred.stlouisfed.org*](https://fred.stlouisfed.org) and click on My Account
    in the upper-righthand corner, as shown in [Figure 5-3](#fred_login).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要在美联储经济数据库（FRED）创建账户，请访问[*https://fred.stlouisfed.org*](https://fred.stlouisfed.org)，并点击右上角的“我的账户”，如图[5-3](#fred_login)所示。
- en: '![Federal Reserve Economic Database (FRED) homepage](assets/ppdw_0503.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![美联储经济数据库（FRED）主页](assets/ppdw_0503.png)'
- en: Figure 5-3\. FRED login link
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. FRED 登录链接
- en: Follow the directions in the pop-up, either creating an account with a new username
    and password or using your Google account to log in. Once your registration/login
    process is complete, clicking on the My Account link will open a drop-down menu
    that includes an option titled API Keys, as shown in [Figure 5-4](#fred_account_option).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 按照弹出窗口中的说明操作，可以创建一个带有新用户名和密码的账户或使用您的 Google 账户登录。一旦您的注册/登录过程完成，点击“我的账户”链接将打开一个下拉菜单，其中包括一个名为“API
    Keys”的选项，如图[5-4](#fred_account_option)所示。
- en: '![FRED account actions](assets/ppdw_0504.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![FRED 账户操作](assets/ppdw_0504.png)'
- en: Figure 5-4\. FRED account actions
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. FRED 账户操作
- en: Clicking that link will take you to a page where you can request one or more
    API keys using the Request API Key button. On the next page, you’ll be asked to
    provide a brief description of the application with which the API key will be
    used; this can just be a sentence or two. You’ll also need to read and agree to
    the Terms of Service by checking the provided box. Complete the process by clicking
    the Request API Key button.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 点击该链接将带您到一个页面，您可以使用“请求 API 密钥”按钮请求一个或多个 API 密钥。在接下来的页面上，您将被要求提供一个简要描述将使用 API
    密钥的应用程序；这可以只是一两个句子。您还需要通过勾选提供的框同意服务条款。点击“请求 API 密钥”按钮完成整个过程。
- en: If your request is successful (and it should be), you’ll be taken to an interim
    page that will display the key that’s been generated. If you leave that page,
    you can always just log in and visit [*https://research.stlouisfed.org/useraccount/apikeys*](https://research.stlouisfed.org/useraccount/apikeys)
    to see all of your available API keys.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的请求成功（应该会成功），您将被带到一个显示已生成密钥的中间页面。如果您离开了那个页面，您可以随时登录并访问[*https://research.stlouisfed.org/useraccount/apikeys*](https://research.stlouisfed.org/useraccount/apikeys)以查看您所有可用的
    API 密钥。
- en: Using Your API key to Request Data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用您的 API 密钥请求数据
- en: 'Now that you have an API key, let’s explore how to request the data we used
    in [Example 4-15](ch04.html#json_parsing). Start by trying to load the following
    URL in a browser:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了 API 密钥，让我们探讨如何请求我们在[示例 4-15](ch04.html#json_parsing)中使用的数据。首先尝试在浏览器中加载以下
    URL：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Even if you’re already logged in to FRED on that browser, you’ll see something
    like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您已在该浏览器上登录到 FRED，您也会看到如下内容：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is a pretty descriptive error message: it not only tells you that something
    went wrong, but it gives you some idea of how to fix it. Since you just created
    an API key, all you have to do is add it to your request as an additional parameter:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常描述性的错误消息：它不仅告诉您出了什么问题，还提示了如何修复它。由于您刚刚创建了一个 API 密钥，所以您只需将其作为附加参数添加到您的请求中：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'replacing `YOUR_API_KEY_HERE` with, of course, your API key. Loading that page
    in a browser should return something that looks something like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 用您的 API 密钥替换`YOUR_API_KEY_HERE`。在浏览器中加载该页面将返回类似以下的内容：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Pretty nifty, right? Now that you know how to use your API key to make data
    requests, it’s time to review how to both customize those requests *and* protect
    your API key when you use it in Python scripts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒，对吧？现在您知道如何使用 API 密钥进行数据请求，是时候复习如何*定制*这些请求和在使用 Python 脚本时保护您的 API 密钥了。
- en: Reading API Documentation
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阅读 API 文档
- en: As you can see from the preceding example, once we have an API key, we can load
    the latest data from the FRED database whenever we want. All we need to do is
    construct our query string and add our API key.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的示例所示，一旦我们拥有 API 密钥，我们就可以随时从 FRED 数据库加载最新数据。我们所需做的只是构建我们的查询字符串并添加我们的 API
    密钥。
- en: But how do we know what key/value pairs the FRED API will accept and what type
    of information they’ll return? The only really reliable way to do this is to read
    the API *documentation*, which should offer guidance and (hopefully) examples
    of how the API can be used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何知道 FRED API 将接受哪些键/值对以及它们将返回什么类型的信息呢？唯一真正可靠的方法是阅读 API *文档*，它应该提供使用指南和（希望的话）API
    的示例。
- en: Unfortunately, there’s no widely adopted standard for API documentation, which
    means that using a new API is almost always something of a trial-and-error process,
    especially if the documentation quality is poor or the provided examples don’t
    include the information you’re looking for. In fact, even *finding* the documentation
    for a particular API isn’t always straightforward, and often a web search is the
    simplest route.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，API 文档没有被广泛采用的标准，这意味着使用新的 API 几乎总是一种反复试验的过程，特别是如果文档质量不佳或提供的示例不包含您正在寻找的信息。事实上，甚至*找到*特定
    API 的文档并不总是一件直接的事情，通常通过网络搜索是最简单的途径。
- en: For example, getting to the FRED API documentation from the [FRED homepage](https://fred.stlouisfed.org)
    requires clicking on the Tools tab about halfway down the page, then selecting
    the Developer API link at the bottom right, which takes you to [*https://fred.stlouisfed.org/docs/api/fred*](https://fred.stlouisfed.org/docs/api/fred).
    By contrast, a web search for “fred api documentation” will take you to the same
    page, shown in [Figure 5-5](#fred_api_docs_main), directly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从[FRED 主页](https://fred.stlouisfed.org)获取到 FRED API 文档需要点击页面中部大约一半的工具选项卡，然后在页面右下角选择开发者
    API 链接，这将带您到[*https://fred.stlouisfed.org/docs/api/fred*](https://fred.stlouisfed.org/docs/api/fred)。相比之下，通过网络搜索“fred
    api documentation”将直接带您到相同的页面，如[图 5-5](#fred_api_docs_main)所示。
- en: '![FRED API documentation homepage](assets/ppdw_0505.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![FRED API 文档主页](assets/ppdw_0505.png)'
- en: Figure 5-5\. FRED API documentation page
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. FRED API 文档页面
- en: Unfortunately, the list of links on this page is actually a list of *endpoints*—different
    base URLs that you can use to request more specific information (recall that the
    *endpoint* is everything before the question mark (`?`), which separates it from
    the *query string*). In the preceding example, you used the endpoint `https://api.stlouisfed.org/fred/series/observations`
    and then paired it with the key/value pairs of `series_id=U6RATE`, `file_type=json`,
    and, of course, your API key in order to generate a response.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，此页面上的链接列表实际上是一组*端点*——不同的基本 URL，您可以使用它们来请求更具体的信息（请记住*端点*是在问号 (`?`) 之前的所有内容，它与*查询字符串*分隔开）。在前面的例子中，您使用了端点
    `https://api.stlouisfed.org/fred/series/observations`，然后配对了 `series_id=U6RATE`、`file_type=json`，当然，还有您的
    API 密钥，以生成响应。
- en: Scrolling down the page in [Figure 5-5](#fred_api_docs_main) and clicking on
    the documentation link labeled “fred/series/observations” will take you [*https://fred.stlouisfed.org/docs/api/fred/series_observations.html*](https://fred.stlouisfed.org/docs/api/fred/series_observations.html),
    which outlines all of the valid query keys (or *parameters*) for that particular
    endpoint, as well as the valid values those keys can have and some sample query
    URLs, as shown in [Figure 5-6](#fred_api_observations_docs).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-5](#fred_api_docs_main) 中滚动页面并点击标有“fred/series/observations”的文档链接，会将你带到
    [*https://fred.stlouisfed.org/docs/api/fred/series_observations.html*](https://fred.stlouisfed.org/docs/api/fred/series_observations.html)，该页面列出了该特定端点的所有有效查询键（或*参数*）以及这些键可以具有的有效值和一些示例查询
    URL，如 [图 5-6](#fred_api_observations_docs) 所示。
- en: '![FRED API ''observations'' endpoint documentation](assets/ppdw_0506.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![FRED API ''observations'' endpoint 文档](assets/ppdw_0506.png)'
- en: Figure 5-6\. FRED API *observations* endpoint documentation
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. FRED API *observations* 端点文档
- en: For example, you could limit the number of observations returned by using the
    `limit` parameter, or reverse the sort order of the returned results by adding
    `sort_order=desc`. You can also specify particular data formats (such as `file_type=xml`
    for XML output) or units (such as `units=pc1` to see the output as percent change
    from a year ago).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以通过使用 `limit` 参数来限制返回的观测数量，或者通过添加 `sort_order=desc` 来反转返回结果的排序顺序。你还可以指定特定的数据格式（例如
    `file_type=xml` 用于 XML 输出）或单位（例如 `units=pc1` 以查看输出作为与一年前相比的百分比变化）。
- en: Protecting Your API Key When Using Python
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在使用 Python 时保护你的 API 密钥
- en: As you may have already guessed, downloading data from FRED (or other, similar
    APIs) is as simple as replacing one of the URLs in [Example 5-1](#data_download)
    with your complete query, because the web page it generates is just another JSON
    file on the internet.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的那样，从 FRED（或其他类似的 API）下载数据就像用你完整的查询替换 [示例 5-1](#data_download) 中的其中一个
    URL 一样简单，因为它生成的网页只是互联网上的另一个 JSON 文件。
- en: 'At the same time, that query contains some especially sensitive information:
    your API key. Remember that as far as FRED (or any other API owner) is concerned,
    you are responsible for any activity on their platform that uses your API key.
    This means that while you *always* want to be documenting, saving, and versioning
    your code with something like Git, you *never* want your API keys or other credentials
    to end up in a file that others can access.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，该查询包含一些特别敏感的信息：你的 API 密钥。记住，就 FRED（或任何其他 API 所有者）而言，对于在其平台上使用你的 API 密钥进行的任何活动，你都要负责。这意味着虽然你*始终*希望用诸如
    Git 这样的工具记录、保存和版本化你的代码，但你*绝不*希望你的 API 密钥或其他凭据最终被其他人可以访问的文件所获取。
- en: Warning
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Properly protecting your API key takes some effort, and if you’re new to working
    with data, Python, or APIs (or all three), you may be tempted to skip the next
    couple of sections and just leave your API credentials inside files that could
    get uploaded to the internet.^([5](ch05.html#idm45143408221168)) Don’t! While
    right now you may be thinking, “Who’s ever going to bother looking at *my* work?”
    or “I’m just playing around anyway—what difference does it make?” there are two
    things you should know.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正确保护你的 API 密钥需要一些努力，如果你是第一次使用数据、Python 或 API（或三者兼有），你可能会被诱惑跳过接下来的几节，只是把你的 API
    凭据留在可能被上传到互联网的文件中。^([5](ch05.html#idm45143408221168)) 不要这么做！虽然现在你可能会想，“谁会有兴趣看*我的*工作？”或“我只是随便玩玩而已—有什么区别？”但你应该知道两件事。
- en: First, as with documentation, if you don’t deal with protecting your credentials
    correctly now, it will be *much* more difficult and time-consuming to do so later,
    in part because by then you’ll have forgotten what exactly is involved, and in
    part because *it may already be too late*. Second, while few of us feel that what
    we’re doing is “important” or visible enough that anyone *else* would bother looking
    at it, the reality is that bad actors don’t mind who their scapegoat is—and if
    you make it easy, they might choose you. The fallout, moreover, might not be limited
    to getting you kicked off of a data platform. In 2021, the former SolarWinds CEO
    claimed that the massive breach of thousands of high-security systems through
    compromises to the company’s software was made possible, in part, because of a
    weak password that was uploaded to a file on an intern’s personal GitHub account.^([6](ch05.html#idm45143408216624))
    In other words, even if you’re “just practicing,” you’re better off practicing
    good security in the first place.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，与文档一样，如果现在没有正确处理保护您的凭据，以后要做这件事情会*更加*困难和耗时，部分原因是到那时您可能已经忘记了具体涉及的内容，另一部分原因是*可能已经太晚*。其次，虽然我们中很少有人觉得自己正在做的事情“重要”或足够“可见”，以至于其他人*会*去查看它，但现实是，恶意行为者并不在乎他们的替罪羊是谁——如果您让它变得容易，他们可能会选择您。此外，后果可能不仅仅是使您被踢出数据平台。2021年，SolarWinds的前CEO声称，通过对公司软件的弱点，通过实习生个人GitHub账户上上传的一个弱密码文件，可能导致数千个高安全性系统的大规模入侵。^([6](ch05.html#idm45143408216624))
    换句话说，即使你只是“练习”，你最好一开始就做好安全保护。
- en: 'Protecting your API credentials is a two-part process:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 保护您的API凭据是一个两部分的过程：
- en: You need to separate your API key or other sensitive information from the rest
    of your code. We’ll do this by storing these credentials in a separate file that
    our main code only loads when the script is actually run.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要将API密钥或其他敏感信息与其余代码分开。我们将通过将这些凭据存储在单独的文件中，并且仅在实际运行脚本时主代码加载它们来实现这一点。
- en: You need a reliable way to ensure that as you are backing up your code using
    Git, for example, those credential files are *never* backed up to any online location.
    We’ll accomplish this by putting the word `credentials` in the name of any file
    that includes them and then using a `gitignore` file to make sure they don’t get
    uploaded to GitHub.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要一种可靠的方法来确保在使用Git进行代码备份时，例如，这些凭证文件*绝对不会*备份到任何在线位置。我们将通过在包含这些文件的任何文件名中加入`credentials`这个词，并使用`gitignore`文件来确保它们不会上传到GitHub。
- en: The simplest way to achieve both of these things consistently is to define a
    naming convention for any file that contains API keys or other sensitive login-related
    information. In this case, we’ll make sure that any such file has the word `credentials`
    somewhere in the filename. We’ll then make sure to create or update a special
    type of Git file known as a *.gitignore*, which stores rules for telling Git which
    files in our repo folder should *never* be committed to our repository and/or
    uploaded to GitHub. By including a rule for our “credentials” file to *.gitignore*,
    we guarantee that no files containing sensitive login information get uploaded
    to GitHub by accident.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这两个目标的最简单方法是为包含API密钥或其他敏感登录相关信息的任何文件定义命名约定。在这种情况下，我们将确保任何这类文件在文件名中都包含`credentials`这个词。然后，我们将确保创建或更新一种名为*.gitignore*的特殊类型的Git文件，该文件存储了关于告诉Git哪些文件不应*永远*提交到我们的存储库和/或上传到GitHub的规则。通过为我们的“凭证”文件添加到*.gitignore*的规则，我们保证不会意外上传包含敏感登录信息的任何文件到GitHub。
- en: Creating Your “Credentials” File
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建你的“凭证”文件
- en: Up until now, we’ve been putting all our code for a particular task—such as
    downloading or transforming a data file—into a single Python file or notebook.
    For the purposes of both security and reuse, however, when we’re working with
    APIs, it makes much more sense to separate our functional code from our credentials.
    Luckily, this process is very straightforward.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们一直将完成特定任务（如下载或转换数据文件）的所有代码放入单个Python文件或笔记本中。然而，出于安全性和重复使用的考虑，当我们使用API时，将功能代码与凭据分开更为合理。幸运的是，这个过程非常简单。
- en: First, create and save a new, empty Python file called *FRED_credentials.py*.
    For simplicity’s sake, go ahead and put this file in the same folder where you
    plan to put the Python code you’ll use to download data from FRED.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建并保存一个名为*FRED_credentials.py*的新的空白Python文件。为了简单起见，请将此文件放在计划用于从FRED下载数据的Python代码所在的同一文件夹中。
- en: Then, simply create a new variable and set its value to your own API key, as
    shown in [Example 5-2](#FRED_credentials_example).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，简单地创建一个新变量并将其值设置为你自己的 API 密钥，如 [示例 5-2](#FRED_credentials_example) 所示。
- en: Example 5-2\. Example FRED credentials file
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 示例 FRED 凭据文件
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now just save your file!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只需保存你的文件！
- en: Using Your Credentials in a Separate Script
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在单独的脚本中使用你的凭据
- en: Now that your API key exists as a variable in another file, you can import it
    into any file where you want to use it, using the same method we’ve used previously
    to import libraries created by others. [Example 5-3](#FRED_download_api_key_import)
    is a sample script for downloading the U6 unemployment data from FRED using the
    API key stored in my *FRED_credentials.py* file.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的 API 密钥作为另一个文件中的变量存在，你可以将其导入到任何你想要使用它的文件中，使用我们之前用来导入其他人创建的库的相同方法。 [示例 5-3](#FRED_download_api_key_import)
    是一个使用存储在我的 *FRED_credentials.py* 文件中的 API 密钥下载 FRED 的 U6 失业数据的示例脚本。
- en: Example 5-3\. FRED_API_example.py
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. FRED_API_example.py
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO1-1)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_accessing_web_based_data_CO1-1)'
- en: We can make our API available to this script by using the `from` keyword with
    the name of our credentials file (notice that we *don’t* include the *.py* extension
    here) and then telling it to `import` the variable that contains our API key.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `from` 关键字与我们的凭据文件的名称（注意，这里我们*不*包括 *.py* 扩展名）来使我们的 API 对这个脚本可用，然后告诉它`import`包含我们的
    API 密钥的变量。
- en: Now that we’ve succeeded in separating our API credentials from the main part
    of our code, we need to make sure that our credentials file doesn’t accidentally
    get backed up when we `git commit` our work and/or `git push` it to the internet.
    To do this simply and systematically, we’ll make use of a special type of file
    known as *.gitignore*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已成功将我们的 API 凭据与代码的主要部分分开，我们需要确保我们的凭据文件在我们`git commit`我们的工作和/或`git push`到互联网时不会意外被备份。为了简单而系统地做到这一点，我们将使用一种特殊类型的文件，称为
    *.gitignore*。
- en: Getting Started with .gitignore
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 .gitignore
- en: As the name suggests, a *.gitignore* file lets you specify certain types of
    files that—surprise, surprise!—you want Git to “ignore,” rather than track or
    back up. By creating (or modifying) the pattern-matching rules in the *.gitignore*
    file for a repository, we can predefine which types of files our repo will track
    or upload. While we could *theoretically* accomplish the same thing manually—by
    never using `git add` on files we don’t want to track—using a *.gitignore* file
    enforces this behavior^([8](ch05.html#idm45143408008560)) *and* prevents Git from
    “warning” us that we have untracked files every time we run `git status`. *Without*
    a *.gitignore* file, we would have to confirm which files we want to ignore every
    time we commit—which would quickly get tedious and easily lead to mistakes. All
    it would take is one hasty `git add -A` command to accidentally begin tracking
    our sensitive credentials file(s)—and getting things *out* of your Git history
    is much trickier than getting them in. Much better to avoid the whole problem
    with a little preparation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，*.gitignore* 文件让你指定某些类型的文件——惊喜，惊喜！——你希望 Git “忽略”，而不是跟踪或备份。通过创建（或修改）存储库中
    *.gitignore* 文件中的模式匹配规则，我们可以预定义我们的存储库将跟踪或上传哪些类型的文件。虽然我们*理论上*可以通过从不使用 `git add`
    来手动完成相同的操作——但使用 *.gitignore* 文件可以强制执行此行为^([8](ch05.html#idm45143408008560)) *并且*防止
    Git 每次运行 `git status` 时“警告”我们有未跟踪的文件。*没有* *.gitignore* 文件，我们将不得不确认每次提交时要忽略的文件——这会很快变得乏味，并很容易导致错误。只需一次匆忙的
    `git add -A` 命令就可能意外开始跟踪我们的敏感凭据文件——并且将文件*移出*你的 Git 历史比将其加入要困难得多。更好的办法是通过一点准备来避免整个问题。
- en: In other words, *.gitignore* files are our friend, letting us create general
    rules that prevent us from accidentally tracking files we don’t want to, and by
    making sure that Git only reports the status of files that we genuinely care about.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*.gitignore* 文件是我们的朋友，让我们创建通用规则，防止我们意外跟踪不想要的文件，并确保 Git 只报告我们真正关心的文件的状态。
- en: For the time being, we’ll create a new *.gitignore* file in the same folder/repository
    where our *FRED_credentials.py* file is, just to get a feel for how they work.^([9](ch05.html#idm45143407997552))
    To do this, we’ll start by opening up a new file in Atom (or you can add a new
    file directly in your GitHub repo) and saving it in the same folder as your *FRED_credentials.py*
    with the name *.gitignore* (be sure to start the filename with a dot (`.`)—that’s
    important!).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将在与我们的*FRED_credentials.py*文件相同的文件夹/存储库中创建一个新的*.gitignore*文件，只是为了感受一下它们是如何工作的。^([9](ch05.html#idm45143407997552))
    为此，我们将首先在Atom中打开一个新文件（或者您可以直接在GitHub存储库中添加一个新文件），并将其保存在与您的*FRED_credentials.py*相同的文件夹中，名称为*.gitignore*（确保文件名以点（`.`）开头——这很重要！）。
- en: 'Next, add the following lines to your file:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在您的文件中添加以下行：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As in Python, comments in *.gitignore* files are started with a hash (`#`) symbol,
    so the first line of this file is just descriptive. The contents of the second
    line (`**credentials*`) is a sort of *regular expression*—a special kind of pattern-matching
    system that lets us describe strings (including filenames) in the sort of generic
    way we might explain them to another person.^([10](ch05.html#idm45143407977504))
    In this case, the expression `**credentials*` translates to “a file anywhere in
    this repository that contains the word *credentials*.” By adding this line to
    our *.gitignore* file, we ensure that any file in this repository whose filename
    includes the word *credentials* will never be tracked or uploaded to GitHub.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与Python一样，*.gitignore*文件中的注释以井号（`#`）符号开头，因此此文件的第一行只是描述性的。第二行的内容（`**credentials*`）是一种*正则表达式*——一种特殊的模式匹配系统，它让我们可以以一种类似于向另一个人解释的方式描述字符串（包括文件名）。^([10](ch05.html#idm45143407977504))
    在这种情况下，表达式`**credentials*`转换为“此存储库中任何位置包含单词*credentials*的文件。”通过将此行添加到我们的*.gitignore*文件中，我们确保此存储库中任何文件名中包含单词*credentials*的文件都不会被跟踪或上传到GitHub。
- en: 'To see your *.gitignore* in action, save the file, and then in the command
    line, run:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看您的*.gitignore*文件的效果，请保存文件，然后在命令行中运行：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'While you should see the new file you created for the code in [Example 5-3](#FRED_download_api_key_import),
    you should *not* see your *FRED_credentials.py* file listed as “untracked.” If
    you want to be really sure that the files you intend to be ignored are, in fact,
    being ignored, you can also run:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您应该看到您为[示例 5-3](#FRED_download_api_key_import)中的代码创建的新文件，但*不*应该看到您的*FRED_credentials.py*文件列为“未跟踪”。如果您想确保您打算忽略的文件实际上正在被忽略，您还可以运行：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: which will show you *only* the files in your repository that are currently being
    ignored. Among them you’ll probably also see the *__pycache__* folder, which we
    also don’t need to back up.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将仅显示您的存储库中当前被忽略的文件。在其中，您可能还会看到*__pycache__*文件夹，我们也不需要备份它。
- en: 'Specialized APIs: Working With OAuth'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专用API：使用OAuth工作
- en: 'So far, we have everything we need to work with APIs that have what’s often
    described as a “basic” authentication process: we create an account with the API
    provider and are given a key that we append to our data request, just as we did
    in [Example 5-3](#FRED_download_api_key_import).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们拥有一切可以使用通常被描述为“基本”身份验证流程的API所需的东西：我们在API提供商那里创建一个账户，并获得一个我们将附加到我们的数据请求中的密钥，就像我们在[示例 5-3](#FRED_download_api_key_import)中所做的那样。
- en: 'While this process is very straightforward, it has some drawbacks. These days,
    APIs can do much more than just return data: they are also the way that apps post
    updates to a social media account or add items to your online calendar. To make
    that possible, they obviously need some type of access to your account—but of
    course you don’t want to be sharing your login credentials with apps and programs
    willy-nilly. If you did just give apps that information, the only way to later
    *stop* an app from accessing your account would be to change your username and
    password, and then you’d have to give your updated credentials to all the apps
    you still *want* to use in order for them to continue working…it gets messy and
    complicated, fast.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个过程非常简单直接，但它也有一些缺点。如今，API可以做的远不止返回数据：它们还是应用程序发布更新到社交媒体账户或向您的在线日历添加项目的方式。为了实现这一点，它们显然需要某种访问权限来访问您的账户——但当然您不想随意与应用程序和程序共享您的登录凭据。如果您只是向应用程序提供了这些信息，稍后要*停止*应用程序访问您的账户的唯一方法就是更改您的用户名和密码，然后您必须向所有您仍然*想要*使用的应用程序提供您更新后的凭据，以便它们继续工作……情况会变得混乱而复杂，很快就会变得混乱而复杂。
- en: 'The OAuth authentication workflow was designed to address these problems by
    providing a way to provide API access without passing around a bunch of usernames
    and passwords. In general, this is achieved by scripting a so-called *authorization
    loop*, which includes three basic steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: OAuth 认证工作流程旨在通过提供一种方式来提供 API 访问而不传递一堆用户名和密码来解决这些问题。总体上，这是通过编写所谓的 *授权循环* 来实现的，包括三个基本步骤：
- en: Obtaining and encoding your API key and “secret” (each of which is just a string
    that you get from the API provider—just as we did with the FRED API key).
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取和编码您的 API 密钥和“密钥”（每个都只是您从 API 提供商那里获取的字符串，就像我们在 FRED API 密钥中所做的那样）。
- en: Sending an encoded combination of those two strings as (yet another) “key” to
    a special “authorization” endpoint/URL.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个字符串的编码组合作为（又一个）“密钥”发送到特定的“授权”端点/URL。
- en: Receiving an *access token* (yet another string) from the authorization endpoint.
    The access token is what you actually then send along to the API’s data endpoint
    along with your query information.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从授权端点接收一个*访问令牌*（又一个字符串）。访问令牌是您实际发送到 API 数据端点的内容，以及您的查询信息。
- en: While this probably sounds convoluted, rest assured that, in practice, even
    this complex-sounding process is mostly about passing strings back and forth to
    and from certain URLs in a certain order. Yes, in the process we’ll need to do
    some “encoding” on them, but as you may have guessed, that part will be handled
    for us by a handy Python library.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这听起来可能很复杂，但请放心，在实践中，即使这个听起来复杂的过程也主要是在特定顺序中传递和传回某些 URL 的字符串。是的，在这个过程中，我们需要对它们进行一些“编码”，但正如你可能猜到的那样，这部分将由一个方便的
    Python 库为我们处理。
- en: 'Despite needing to complete an authorization loop and retrieve an access token,
    the process of interacting with even these more specialized APIs via Python is
    essentially the same as what we saw in [“Specialized APIs: Adding Basic Authentication”](#basic_authentication).
    We’ll create an account, request API credentials, and then create a file that
    both contains those credentials and does a little bit of preparatory work to them
    so we can use them in our main script. Then our main script will pull in those
    credentials and use them to request data from our target platform and write it
    to an output file.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管需要完成授权循环并获取访问令牌，但通过 Python 与这些更专门的 API 交互的过程与我们在[“专门 API：添加基本身份验证”](#basic_authentication)中看到的基本相同。我们将创建一个账号，请求
    API 凭据，然后创建一个文件，其中包含这些凭据，并做一些准备工作，以便我们可以在我们的主脚本中使用它们。然后我们的主脚本将导入这些凭据，并使用它们从目标平台请求数据并将其写入输出文件。
- en: For this example, we’ll be working with the Twitter API, but you’ll be able
    to use roughly the same process for other platforms (like Facebook) that use an
    OAuth approach. One thing we *won’t* do here is spend much time discussing how
    to structure specific queries, since the ins and outs of any given API could easily
    fill a book in and of itself! That said, once you have this authentication process
    down, you’ll have what you need to start experimenting with a whole range of APIs
    and can start practicing with them in order to access the data you want. Let’s
    get started!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用 Twitter API，但您可以对其他使用 OAuth 方法的平台（如 Facebook）使用大致相同的流程。在这里我们*不会*花费太多时间讨论如何结构化特定查询，因为任何给定
    API 的细节都可能填满一本书！尽管如此，一旦您掌握了这个认证过程，您将拥有开始尝试各种 API 并可以开始练习访问您想要的数据的所需内容。让我们开始吧！
- en: Applying for a Twitter Developer Account
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 申请 Twitter 开发者账号
- en: As it will be with (almost) every new API we want to use, our first step will
    be to request an API key from Twitter. Even if you already have a Twitter account,
    you’ll need to apply for “developer access,” which will take about 15 minutes
    (not counting the time for Twitter to review and/or approve), all told. Start
    by visiting [the Twitter Developer API “Apply for Access” page](https://developer.twitter.com/en/apply-for-access),
    and click the “Apply for a developer account” button. Once you’ve logged in, you’ll
    be prompted for more information about how you plan to use the API. For this exercise,
    you can select “Hobbyist” and “Exploring the API,” as shown in [Figure 5-7](#twitter_api_use_case).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们几乎每次想使用新的 API 一样，我们的第一步将是从 Twitter 请求一个 API 密钥。即使您已经有了 Twitter 账号，您也需要申请“开发者访问权限”，大约需要15分钟（不包括
    Twitter 审核和/或批准的时间）。首先访问 [Twitter 开发者 API “申请访问权限” 页面](https://developer.twitter.com/en/apply-for-access)，并点击“申请开发者帐户”按钮。登录后，系统将提示您提供有关您计划如何使用
    API 的更多信息。对于本次练习，您可以选择“业余爱好者”和“探索 API”，如 [图5-7](#twitter_api_use_case) 所示。
- en: '![Use case selection for the Twitter developer API](assets/ppdw_0507.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![对 Twitter 开发者 API 的用例选择](assets/ppdw_0507.png)'
- en: Figure 5-7\. Twitter developer API use case selection
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. Twitter 开发者 API 用例选择
- en: 'In the next step, you’ll be asked to provide a 200+ character explanation of
    what you plan to use the API for; here you can enter something like this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，您将被要求提供一个超过200个字符的说明，说明您计划使用API的目的；您可以输入类似以下内容：
- en: Using the Twitter API to learn more about doing data wrangling with Python.
    Interested in experimenting with OAuth loops and pulling different kinds of information
    from public Twitter feeds and conversations.
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 Twitter API 来学习如何使用 Python 进行数据整理。有兴趣尝试 OAuth 循环，并从公共 Twitter 资源和对话中获取不同类型的信息。
- en: Since our goal here is just to practice downloading data from Twitter using
    a Python script and an OAuth loop, you can toggle the answer to the four subsequent
    questions to “No,” as shown in [Figure 5-8](#twitter_api_explanation), though
    if you begin using the API in some other way, you will need to update these answers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这里的目标只是通过 Python 脚本和 OAuth 循环练习从 Twitter 下载数据，您可以将对四个后续问题的回答切换为“否”，如 [图5-8](#twitter_api_explanation)
    所示，但如果您开始以其他方式使用 API，则需要更新这些答案。
- en: '![Intended use selection for data from the Twitter developer API](assets/ppdw_0508.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![从 Twitter 开发者 API 中选择数据的预期使用情况](assets/ppdw_0508.png)'
- en: Figure 5-8\. Twitter developer API intended uses
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. Twitter 开发者 API 预期用途
- en: On the next two screens, you’ll review your previous selections and click a
    checkbox to acknowledge the Developer Agreement. You then click “Submit application,”
    which will trigger a verification email. If the email doesn’t arrive in your inbox
    within a few minutes, be sure to check your Spam and Trash. Once you locate it,
    click on the link in the email for your access to be confirmed!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个屏幕中，您将查看您之前的选择，并点击一个复选框以确认开发者协议。然后，点击“提交申请”，这将触发一个验证电子邮件。如果几分钟内未收到邮件，请务必检查您的垃圾邮件和已删除邮件。一旦找到邮件，请点击邮件中的链接以确认您的访问权限！
- en: Creating Your Twitter “App” and Credentials
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建您的 Twitter “App” 和凭据
- en: Once your developer access has been approved by Twitter, you can create a new
    “app” by logging in to your Twitter account and visiting [*https://developer.twitter.com/en/portal/projects-and-apps*](https://developer.twitter.com/en/portal/projects-and-apps).
    In the center of the page, click the Create Project button.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Twitter 批准了您的开发者访问权限，您可以通过登录到 Twitter 账户并访问 [*https://developer.twitter.com/en/portal/projects-and-apps*](https://developer.twitter.com/en/portal/projects-and-apps)
    来创建一个新的“应用程序”。在页面中央，点击“创建项目”按钮。
- en: 'Here you’ll be taken through a mini version of the process to apply for developer
    access: you’ll need to provide a name for your project, indicate how you intend
    to use the Twitter API, describe that purpose in words, and provide a name for
    the first app associated with that project, as shown in Figures [5-9](#twitter_project_name)
    through [5-12](#twitter_app_name).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里您将通过一个迷你版的开发者访问申请过程：您需要为项目提供一个名称，指明您打算如何使用 Twitter API，用文字描述该目的，并为与该项目关联的第一个应用程序命名，如
    [图5-9](#twitter_project_name) 至 [图5-12](#twitter_app_name) 所示。
- en: '![Twitter project name screen](assets/ppdw_0509.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter 项目名称屏幕](assets/ppdw_0509.png)'
- en: 'Figure 5-9\. Twitter project creation: project name'
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-9\. Twitter 项目创建：项目名称
- en: '![Twitter project purpose screen](assets/ppdw_0510.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter 项目目的屏幕](assets/ppdw_0510.png)'
- en: 'Figure 5-10\. Twitter project creation: project purpose'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-10\. Twitter 项目创建：项目目的
- en: '![Twitter project description screen](assets/ppdw_0511.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter 项目描述屏幕](assets/ppdw_0511.png)'
- en: 'Figure 5-11\. Twitter project creation: project description'
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-11\. Twitter 项目创建：项目描述
- en: '![Twitter app name](assets/ppdw_0512.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter 应用程序名称](assets/ppdw_0512.png)'
- en: 'Figure 5-12\. Twitter project creation: app name'
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-12\. Twitter 项目创建：应用程序名称
- en: Once you’ve added your app name, you’ll see a screen that shows your API key,
    API secret key, and Bearer token, as shown in [Figure 5-13](#twitter_keys_and_tokens).^([11](ch05.html#idm45143407870720))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您添加了您的应用程序名称，您将看到一个显示您的 API 密钥、API 密钥秘钥和 Bearer 令牌的屏幕，如[图 5-13](#twitter_keys_and_tokens)所示。^([11](ch05.html#idm45143407870720))
- en: '![API Keys and tokens screen](assets/ppdw_0513.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![API 密钥和令牌屏幕](assets/ppdw_0513.png)'
- en: Figure 5-13\. Twitter API keys and tokens screen
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-13\. Twitter API 密钥和令牌屏幕
- en: For security reasons, you’ll only be able to view your API key and API secret
    key *on this screen*, so we’re going to put them into a file for our Twitter credentials
    right away (note that in other places in the developer dashboard these are referred
    to as the “API Key” and “API Key Secret”—even big tech companies can have trouble
    with consistency!). Don’t worry, though! If you accidentally click away from this
    screen too soon, miscopy a value, or anything else, you can always go back to
    [your dashboard](https://developer.twitter.com/en/portal/dashboard) and click
    on the key icon next to your app, as shown in [Figure 5-14](#twitter_project_app_list).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于安全原因，您只能在*此屏幕*上查看您的 API 密钥和 API 密钥秘钥，因此我们将立即将它们放入我们的 Twitter 凭据文件中（请注意，在开发者仪表板的其他地方，它们被称为“API
    密钥”和“API 密钥秘钥” - 即使是大型科技公司也可能在一致性方面出现问题！）。但不要担心！如果您意外地太快从此屏幕点击离开，错误复制了值，或者发生其他任何事情，您都可以随时返回到[您的仪表板](https://developer.twitter.com/en/portal/dashboard)，并单击您的应用程序旁边的密钥图标，如[图 5-14](#twitter_project_app_list)所示。
- en: '![Twitter Dashboard with Apps](assets/ppdw_0514.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![带应用的 Twitter 仪表板](assets/ppdw_0514.png)'
- en: Figure 5-14\. Twitter dashboard with apps
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-14\. 带应用的 Twitter 仪表板
- en: Then, just click “Regenerate” under “Consumer Keys” to get a new API Key and
    API Key Secret, as shown in [Figure 5-15](#twitter_regenerate_keys).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需在“Consumer Keys”下点击“重新生成”即可获得新的 API 密钥和 API 密钥秘钥，如[图 5-15](#twitter_regenerate_keys)所示。
- en: Now that we know how to access the API Key and API Key Secret for our app, we
    need to put these into a new “credentials” file, similar to the one we created
    for our FRED API key. To do this, create a new file called *Twitter_credentials.py*
    and save it in the folder where you want to put your Python script for accessing
    Twitter data, as shown in [Example 5-4](#Twitter_credentials_example).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何访问我们应用程序的 API 密钥和 API 密钥秘钥，我们需要将它们放入一个新的“凭据”文件中，类似于我们为我们的 FRED API 密钥创建的文件。要做到这一点，请创建一个名为*Twitter_credentials.py*的新文件，并将其保存在您要放置用于访问
    Twitter 数据的 Python 脚本的文件夹中，如[示例 5-4](#Twitter_credentials_example)所示。
- en: Example 5-4\. Twitter_credentials.py
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. Twitter_credentials.py
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Twitter Regenerate Keys](assets/ppdw_0515.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter 重新生成密钥](assets/ppdw_0515.png)'
- en: Figure 5-15\. Twitter regenerate keys
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-15\. Twitter 重新生成密钥
- en: Warning
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Be sure to include the word `credentials` in the name of the file where you
    store your Twitter API Key and API Key Secret! Recall that in [“Getting Started
    with .gitignore”](#using_gitignore), we created a rule that ignores any file whose
    name includes the word `credentials` to make sure our API keys never get uploaded
    to GitHub by accident. So make sure to double-check the spelling in your filename!
    Just to be extra sure, you can always run:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在存储 Twitter API 密钥和 API 密钥秘钥的文件名中包含单词`credentials`！回想一下在[“使用 .gitignore 开始”](#using_gitignore)中，我们创建了一个规则，忽略任何文件名包含单词`credentials`的文件，以确保我们的
    API 密钥永远不会因疏忽上传到 GitHub。因此，请务必仔细检查您文件名中的拼写！为了确保额外的确定性，您总是可以运行：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: in the command line to confirm that all of your credentials files are indeed
    being ignored.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行中确认所有您的凭据文件确实被忽略。
- en: Encoding Your API Key and Secret
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码您的 API 密钥和秘钥
- en: So far, we haven’t needed to do things *too* differently than we did for the
    FRED API; we’ve just had to create two variables in our credentials file (`my_Twitter_key`
    and `my_Twitter_secret`) instead of one.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们与 FRED API 做的事情并没有*太*不同；我们只是需要在我们的凭据文件中创建两个变量（`my_Twitter_key`和`my_Twitter_secret`），而不是一个。
- en: Now, however, we need to do a little bit of work on these values to get them
    in the right format for the next step of the authentication process. While I won’t
    go too far into the details of what’s happening “under the hood” in these next
    few steps, just know that these coding and decoding steps are necessary for protecting
    the raw string values of your API Key and API Key Secret so that they can be safely
    sent over the internet.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，但是，我们需要对这些值进行一些工作，以便为身份验证过程的下一步获取它们正确的格式。虽然我不会深入研究在这些接下来的步骤中发生的“内部工作”细节，但请知道，这些编码和解码步骤对于保护API密钥和API密钥密钥的原始字符串值以便安全发送到互联网上是必要的。
- en: So, to our *Twitter_credentials.py* file we’re now going to add several lines
    of code so that the completed file looks like [Example 5-5](#complete_Twitter_credentials).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于我们的*Twitter_credentials.py*文件，我们现在将添加几行代码，以使完成的文件看起来像[Example 5-5](#complete_Twitter_credentials)。
- en: Example 5-5\. Twitter_credentials.py
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-5\. Twitter_credentials.py
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO2-1)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_accessing_web_based_data_CO2-1)'
- en: This library will let us transform our raw API Key and API Key Secret into the
    correct format for sending to the Twitter authorization endpoint.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 该库将允许我们将我们的原始API密钥和API密钥密钥转换为发送到Twitter授权端点的正确格式。
- en: Now that we’ve got our API Key and API Key Secret properly encoded, we can import
    *just* the `auth_ready_key` into the script we’ll use to actually specify and
    pull our data. After doing one request to the authorization endpoint to get our
    *access token*, we’ll finally(!) be ready retrieve some tweets!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经正确编码了API密钥和API密钥密钥，我们可以将*just*的`auth_ready_key`导入到我们将用来指定和拉取数据的脚本中。在进行一次请求以获取我们的*access
    token*到授权端点之后，我们终于(!)准备好检索一些推文了！
- en: Requesting an Access Token and Data from the Twitter API
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Twitter API请求访问令牌和数据
- en: 'As we did in [Example 5-3](#FRED_download_api_key_import), we’ll now create
    a Python file (or notebook) where we’ll do the next two steps of our Twitter data
    loading process:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[Example 5-3](#FRED_download_api_key_import)中所做的那样，我们现在将创建一个Python文件（或笔记本），在那里我们将执行我们的Twitter数据加载过程的下两个步骤：
- en: Requesting (and receiving) an access token or *bearer token* from Twitter
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求（和接收）从Twitter获取访问令牌或*bearer token*。
- en: Including that bearer token in a data request to Twitter and receiving the results
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该bearer token包含在发送给Twitter的数据请求中并接收结果
- en: 'Requesting an access token: get versus post'
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求访问令牌：get versus post
- en: Requesting an access or bearer token from Twitter is really just a matter of
    sending a well-formatted request to the *authorization endpoint*, which is [*https://api.twitter.com/oauth2/token*](https://api.twitter.com/oauth2/token).
    Instead of appending our `auth_ready_key` to the endpoint URL, however, we’ll
    use something called a *post* request (recall that in Examples [5-1](#data_download)
    and [5-3](#FRED_download_api_key_import), the `requests` recipe we used was called
    `get`).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从Twitter请求访问或bearer token实际上只是向*授权端点*发送格式良好的请求，该端点是[*https://api.twitter.com/oauth2/token*](https://api.twitter.com/oauth2/token)。但是，与将我们的`auth_ready_key`附加到端点URL不同，我们将使用称为*post*请求的东西（回想一下，在示例[5-1](#data_download)和[5-3](#FRED_download_api_key_import)中，我们使用的`requests`方法称为`get`）。
- en: A `post` request is important here in part because it offers some additional
    security over `get` requests^([12](ch05.html#idm45143407651104)) but mostly because
    `post` requests are effectively the standard when we’re asking an API to do something
    *beyond* simply returning data. So while in [Example 5-3](#FRED_download_api_key_import)
    the data we got back was unique to our query, the API would return that same data
    to *anyone* who submitted that same request. By contrast, when we use `post` to
    submit our `auth_ready_key`, the Twitter API will process our unique key and return
    a unique bearer token—so we use a `post` request.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用`post`请求是重要的部分，因为它在一定程度上提供了比`get`请求更高的安全性^([12](ch05.html#idm45143407651104))，但主要是因为当我们要求API执行*超出*简单返回数据的操作时，`post`请求实际上是标准。因此，当我们使用`post`提交我们的`auth_ready_key`时，Twitter
    API将处理我们的唯一密钥并返回一个唯一的bearer token。
- en: 'When building our `post` request in Python, we’ll need to create two `dict`
    objects: one that contains the request’s *headers*, which will contain both our
    `auth_ready_key` and some other information, and another that contains the request’s
    *data*, which in this case will specify that we’re asking for credentials. Then
    we’ll just pass these as parameters to the *requests* library’s `post` recipe,
    rather than sticking them on the end of the URL string, as shown in [Example 5-6](#twitter_api_access_token_request).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中构建我们的`post`请求时，我们需要创建两个`dict`对象：一个包含请求的*headers*，其中包含我们的`auth_ready_key`和一些其他信息，另一个包含请求的*data*，在这种情况下将指定我们正在请求凭证。然后，我们将这些作为参数传递给*requests*库的`post`方法，而不是像在[示例
    5-6](#twitter_api_access_token_request)中所示那样将它们粘贴在URL字符串的末尾。
- en: Example 5-6\. Twitter_data_download.py
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-6\. Twitter_data_download.py
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO3-1)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_accessing_web_based_data_CO3-1)'
- en: This `dict` contains the information the authorization endpoint wants in order
    to return an access token to us. This includes our encoded key and its data format.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`dict`包含了授权端点想要返回访问令牌所需的信息。这包括我们的编码密钥及其数据格式。
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO3-2)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_accessing_web_based_data_CO3-2)'
- en: The format of both the `auth_headers` and the `auth_data` objects was defined
    by the API provider.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`auth_headers`和`auth_data`对象的格式是由 API 提供者定义的。'
- en: Actually pretty straightforward, right? If everything went well (which we’ll
    confirm momentarily), we’ll just need to add a few lines of code to this script
    in order to use our access token to request some actual data about what’s happening
    on Twitter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上非常直接了当，对吧？如果一切顺利（我们马上会确认），我们只需在这个脚本中添加几行代码，就可以使用我们的访问令牌请求关于Twitter上发生的实际数据。
- en: 'Now that we have an access token (or *bearer token*) handy, we can go ahead
    and make a request for some tweets. For demonstration purposes, we’re going to
    keep our request simple: we’ll do a basic search request for recent tweets that
    contain the word *Python*, and ask to have a maximum of four tweets returned.
    In [Example 5-7](#twitter_api_data_request), we’re building on the script we started
    in [Example 5-6](#twitter_api_access_token_request), and we’ll structure and make
    this request, including our newly acquired bearer token in the header. Once the
    response arrives, we’ll write the data to a file. Since there is a *lot* in the
    JSON data we get back besides the text of the tweets, however, we’re also going
    to print out *just* the text of each tweet, just to give us confidence that we
    got correct (if sometimes unexpected) results ;-)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个访问令牌（或*Bearer 令牌*），我们可以继续请求一些推文。为了演示目的，我们将保持请求简单：我们将请求包含单词*Python*的最近推文的基本搜索请求，并要求最多返回四条推文。在[示例
    5-7](#twitter_api_data_request)中，我们在[示例 5-6](#twitter_api_access_token_request)中开始的脚本基础上构建并发出此请求，包括我们新获取的Bearer令牌在头部。一旦响应返回，我们将数据写入文件。然而，由于我们在返回的JSON数据中除了推文文本外还有很多内容，我们也会仅仅打印出每条推文的文本，以确保我们获得了正确（有时是意外的）结果
    ;-)
- en: Example 5-7\. Twitter_data_download.py, continued
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-7\. Twitter_data_download.py，继续
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO4-1)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_accessing_web_based_data_CO4-1)'
- en: In this case, our query (`q`) is `Python`, we’re looking for `recent` results,
    and we want a maximum of `4` tweets back. Remember that the keys and values we
    can include in this object are defined by the data provider.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的查询（`q`）是`Python`，我们正在寻找`recent`结果，并且我们希望最多返回`4`条推文。请记住，我们可以包含在此对象中的键和值由数据提供者定义。
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO4-2)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_accessing_web_based_data_CO4-2)'
- en: Because the response from Twitter is a JSON object, we have to use the built-in
    Python `str()` function to convert it to a string before we can write it to our
    file.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Twitter 返回的响应是一个 JSON 对象，所以在我们将其写入文件之前，我们必须使用内置的 Python `str()` 函数将其转换为字符串。
- en: '[![3](assets/3.png)](#co_accessing_web_based_data_CO4-3)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_accessing_web_based_data_CO4-3)'
- en: Because there is a *lot* of information in each result, we’re going to print
    out the text of each tweet returned, just to get a sense of what’s there. The
    `statuses` is the list of tweets in the JSON object, and the actual text of the
    tweets can be accessed with the key `text`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个结果中包含的信息都很多，我们将打印出每条返回的推文文本，以便了解其内容。`statuses`是 JSON 对象中的推文列表，推文的实际文本可以通过键`text`访问。
- en: Depending on how actively Twitter users have been posting about Python recently,
    you may see different results even if you run this script again in only a few
    minutes.^([13](ch05.html#idm45143407307360)) Of course you can change this search
    to contain any query term you want; just modify the value of the `search_params`
    variable as you like. To see all the possible parameters and their valid values,
    you can look through [the API documentation for this particular Twitter API endpoint](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Twitter用户最近对Python的活动程度，即使你只在几分钟内再次运行此脚本，你可能会看到不同的结果。^([13](ch05.html#idm45143407307360))
    当然，你可以更改此搜索以包含任何你想要的查询术语；只需根据需要修改`search_params`变量的值。要查看所有可能的参数及其有效值，你可以查看[此特定Twitter
    API端点的API文档](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets)。
- en: And that’s it! While there are a number of different APIs that Twitter makes
    available (others allow you to actually post to your own timeline or even someone
    else’s), for the purposes of accessing and wrangling data, what we’ve covered
    here should be enough to get you started with this and other similar APIs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样了！虽然Twitter提供了许多不同的API（其他API允许你实际上发布到你自己的时间轴甚至别人的时间轴），但是在访问和处理数据方面，我们在这里所讲述的内容足以让你开始使用这些以及其他类似的API了。
- en: API Ethics
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: API伦理
- en: Now that you know how to make API requests from services like Twitter (and others
    that use an OAuth process), you may be imagining all the cool things you can do
    with the data you can collect. Before you start writing dozens of scripts to track
    the conversations happening around your favorite topics online, however, it’s
    time to do both some practical and ethical reflection.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何从像Twitter这样的服务中发出API请求（以及其他使用OAuth流程的服务），你可能会想象你可以用收集到的数据做很多有趣的事情。然而，在你开始编写几十个脚本来跟踪你喜欢的在线话题讨论之前，现在是时候进行一些实际和伦理上的思考了。
- en: First, know that almost every API uses *rate-limiting* to restrict how many
    data requests you can make within a given time interval. On the particular API
    endpoint we used in [Example 5-7](#twitter_api_data_request), for example, you
    can make a maximum of 450 requests in a 15-minute time period, and each request
    can return a maximum of 100 tweets. If you exceed this, your data requests will
    probably fail to return data until Twitter determines that the next 15-minute
    window has begun.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，几乎每个API都使用*速率限制*来限制在给定时间间隔内可以进行多少数据请求。例如，在我们在[示例 5-7](#twitter_api_data_request)中使用的特定API端点上，你可以在15分钟的时间段内最多发出450个请求，每个请求最多可以返回100条推文。如果超过这个限制，你的数据请求可能会在Twitter确定下一个15分钟窗口开始之前无法返回数据。
- en: Second, while you probably didn’t read the Developer Agreement in detail (don’t
    worry, you’re not alone;^([14](ch05.html#idm45143407245520)) you can always find
    a [reference copy online](https://developer.twitter.com/en/developer-terms/agreement-and-policy)),
    it includes provisions that have important practical and ethical implications.
    For example, Twitter’s Developer Agreement *specifically* prohibits the practice
    of “Off-Twitter matching”—that is, combining Twitter data with other information
    about a user, unless that user has provided you the information directly or expressly
    provided their consent. It also contains rules about how you can store and display
    Twitter content that you may get from the API, and a whole host of other rules
    and restrictions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，虽然你可能没有详细阅读开发者协议（别担心，你并不孤单；^([14](ch05.html#idm45143407245520)) ），你总是可以在[在线参考副本](https://developer.twitter.com/en/developer-terms/agreement-and-policy)中找到相关内容。该协议包含一些具有重要实际和伦理意义的条款。例如，Twitter的开发者协议*明确*禁止“与Twitter外部的数据匹配”这一行为——也就是说，不得将Twitter数据与用户的其他信息结合，除非用户直接提供了这些信息或明确同意。协议还规定了您如何存储和展示从API获取的Twitter内容，以及一系列其他规则和限制。
- en: Whether or not those terms of service are legally binding,^([15](ch05.html#idm45143407239440))
    or truly ethical in and of themselves, remember that it is ultimately *your* responsibility
    to make sure that you are gathering, analyzing, storing, and sharing *any* data
    you use in an ethical way. That means taking into account the privacy and security
    of the people you may be using data about, as well as thinking about the implications
    of aggregating and sharing it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这些服务条款是否具有法律约束力，^([15](ch05.html#idm45143407239440))或者在本质上是否真正符合伦理，记住最终是*你*的责任确保你以伦理方式收集、分析、存储和共享*任何*数据。这意味着考虑到你可能使用的数据涉及的人的隐私和安全，以及思考聚合和分享的影响。
- en: Of course, if ethical issues were easy to identify and agree upon, we would
    live in a very different world. Because they aren’t, many organizations have well-defined
    review processes and oversight committees designed to help explore and (if possible)
    resolve ethical issues before, for example, research and data collection impacting
    humans ever begins. For my own part, I still find that a good place to start when
    trying to think through ethical issues is [the “Society of Professional Journalists’
    Code of Ethics”](https://spj.org/ethicscode.asp). While that document doesn’t
    cover every possible ethics situation in detail, it articulates some core principles
    that I think all data users would do well to consider when they are collecting,
    analyzing, and sharing information.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果伦理问题容易识别和达成一致，我们将生活在一个截然不同的世界中。因为它们不是，许多组织都设有明确定义的审查流程和监督委员会，旨在帮助探索并（如果可能的话）解决在例如研究和数据收集影响人类之前的伦理问题。对我个人来说，当试图思考伦理问题时，我仍然认为一个好的起点是["新闻记者职业道德准则"协会](https://spj.org/ethicscode.asp)。虽然这份文件并没有详细涵盖每一个可能的伦理情境，但它阐述了一些核心原则，我认为所有数据用户在收集、分析和分享信息时都应该考虑。
- en: In the end, however, the most important thing is that *whatever* choices you
    make, you’re ready to stand behind them. One of the great possibilities of data
    access and wrangling is the ability to uncover new information and generate new
    insights. Just as the skills for doing that are now entirely in your control,
    so is the responsibility for how you use them.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最重要的是，*无论*你做出什么选择，你都准备好支持它们。数据获取和整理的一大可能性是能够发现新信息并生成新的见解。就像现在完全掌握做到这一点的技能一样，使用它们的责任也完全掌握在你的手中。
- en: 'Web Scraping: The Data Source of Last Resort'
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络抓取：最后的数据来源
- en: While APIs are designed by companies and organizations with the specific intent
    of providing access to often rich, diverse datasets via the internet, there is
    still a whole lot of information online that only exists on form-based or highly
    formatted web pages as opposed to an API or even as a CSV or PDF. For situations
    like these, the only real solution is *web scraping*, which is the process of
    using code to programmatically retrieve the contents of a web page and systematically
    extract some amount of (usually) structured data from it.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然API是由公司和组织设计的，目的是通过互联网提供访问通常是丰富多样的数据集，但仍然有大量信息只存在于基于表单或高度格式化的网页上，而不是API甚至不是CSV或PDF。对于这类情况，唯一的真正解决方案就是*网络抓取*，这是使用代码以程序化方式检索网页内容，并系统地从中提取一定量（通常是结构化的）数据的过程。
- en: The reason why I refer to web scraping as “the data source of last resort” is
    that it’s both a technically and ethically complex process. Creating web scraping
    Python scripts almost always requires manually wading through a jumble of HTML
    code to locate the data you’re looking for, typically followed by a significant
    amount of trial and error to successfully separate the information you want from
    everything you don’t. It’s time-consuming, fiddly, and often frustrating. And
    if the web page changes even a little, you may have to start from scratch in order
    to make your script work with the updated page.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我将网络抓取称为“最后的数据来源”的原因是，这是一个技术上和伦理上都很复杂的过程。创建网络抓取的Python脚本几乎总是需要手动浏览杂乱的HTML代码，以定位你所寻找的数据，通常需要大量的试错才能成功地从中分离出你想要的信息。这是耗时的、琐碎的，而且经常令人沮丧。如果网页稍作改变，你可能需要从头开始，以使你的脚本与更新后的页面配合工作。
- en: Web scraping is also ethically complicated because, for a variety of reasons,
    many website owners don’t *want* you to scrape their pages. Poorly coded scraping
    programs can overwhelm the website, making it inaccessible to other users. Making
    lots of scripted data requests in quick succession can also drive up the website
    owner’s costs, because they have to pay their own service provider to return all
    that data in a brief period. As a result, many websites explicitly prohibit web
    scraping in their Terms of Service.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 网络抓取在伦理上也很复杂，因为出于各种原因，许多网站所有者*不希望*你抓取他们的页面。编写不良的抓取程序可能会使网站不可访问，从而影响其他用户。快速连续进行大量脚本数据请求也可能会增加网站所有者的成本，因为他们需要支付自己的服务提供商在短时间内返回所有数据。因此，许多网站在其服务条款中明确禁止网络抓取。
- en: At the same time, if important information—especially about powerful organizations
    or government agencies—is *only* available via a web page, then scraping may be
    your *only* option even if it goes against the Terms of Service. While it is *far*
    beyond the scope of this book to provide even pseudolegal advice on the subject,
    keep in mind that even if your scripts are written responsibly and there is a
    good, public-interest reason for your scraping activity, you may face sanction
    from the website owner (such as a “cease and desist” letter) or even legal action.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果重要信息——尤其是关于强大组织或政府机构的信息——*仅*通过网页可获取，那么即使违反了服务条款，抓取也可能是你*唯一*的选择。虽然本书的范围远远超出了在这个主题上提供任何伪法律建议的范围，请记住，即使你的脚本编写得负责任，并且有一个良好的公共利益原因支持你的抓取活动，你可能会面临来自网站所有者的制裁（如“停止和放弃”信函）甚至法律诉讼。
- en: Because of this, I strongly recommend that before you start down the road of
    writing *any* web scraping script you work through the excellent decision tree
    by Sophie Chou shown in [Figure 5-16](#scraping_decision_tree).^([16](ch05.html#idm45143407214288))
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我强烈建议，在你开始撰写*任何*网络抓取脚本之前，通过Sophie Chou展示的优秀决策树仔细思考一下。该决策树见[图5-16](#scraping_decision_tree)^([16](ch05.html#idm45143407214288))。
- en: '![Web scraping decision tree by Sophie Chou, for Storybench.org, 2016](assets/ppdw_0516.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![Sophie Chou为Storybench.org制作的网络抓取决策树，2016](assets/ppdw_0516.png)'
- en: Figure 5-16\. Web scraping decision tree (Sophie Chou for Storybench.org, 2016)
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-16\. 网络抓取决策树（Sophie Chou，Storybench.org，2016）
- en: Once you’ve determined that scraping is your only/best option, it’s time to
    get to work.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了抓取是你唯一/最佳选择，就该开始工作了。
- en: Carefully Scraping the MTA
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仔细地抓取MTA
- en: 'For this example, we’re going to use web scraping to download and extract data
    from a web page provided by New York City’s Metropolitan Transit Authority (MTA),
    which provides links to all of the turnstile data for the city’s subway stations,
    going back to 2010\. To ensure that we’re doing this as responsibly as possible,
    we’re going to make sure that *any* Python script we write:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个例子，我们将使用网络抓取来下载和提取纽约市地铁站提供的网页数据，时间跨度回溯到2010年。为了尽可能负责任地进行此操作，我们将确保我们编写的*任何*Python脚本：
- en: Identifies who we are and how to get in touch with us (in the example, we’ll
    include an email address).
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定了我们是谁以及如何联系我们（在示例中，我们将包括一个电子邮件地址）。
- en: Pauses between web page requests to make sure that we don’t overwhelm the server.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页请求之间暂停，以确保我们不会压倒服务器。
- en: In addition, we’ll structure and separate the parts of our script to make sure
    that we never download a particular web page more than absolutely necessary. To
    accomplish this, we’ll start by downloading and saving a copy of the web page
    that contains the links to the individual turnstile data files. Then we’ll write
    a separate script that goes through the *saved* version of the initial page and
    extracts the data we need. That way, any trial and error we go through in extracting
    data from the web page happens on our saved version, meaning it adds no additional
    load to the MTA’s server. Finally, we’ll write a third script that parses our
    file of extracted links and downloads the last four weeks of data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将结构化和分离脚本的各个部分，以确保我们只在绝对必要时下载特定网页。为此，我们将首先下载并保存包含个别旋转门数据文件链接的网页副本。然后，我们将编写一个单独的脚本，通过初始页面的*保存*版本提取我们需要的数据。这样，我们从网页中提取数据时的任何试错都会发生在我们保存的版本上，这意味着不会给MTA的服务器增加额外负载。最后，我们将编写第三个脚本，解析我们提取链接的文件，并下载过去四周的数据。
- en: Before we begin writing any code, let’s take a look at [the page we’re planning
    to scrape](http://web.mta.info/developers/turnstile.html). As you can see, the
    page is little more than a header and a long list of links, each of which will
    take you to a comma-separated *.txt* file. To load the last several weeks of these
    data files, our first step is to download a copy of this index-style page ([Example 5-8](#downloading_turnstile_index)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写任何代码之前，让我们先看看[我们打算抓取的页面](http://web.mta.info/developers/turnstile.html)。正如您所看到的，该页面只是一个标题和一个长长的链接列表，每个链接将带您转到一个逗号分隔的*.txt*文件。要加载这些数据文件的最近几周，我们的第一步是下载此索引样式页面的副本（示例 5-8）。
- en: Example 5-8\. MTA_turnstiles_index.py
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-8\. MTA_turnstiles_index.py
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO5-1)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_accessing_web_based_data_CO5-1)'
- en: Since we’re *not* using an API here, we want to proactively provide the website
    owner with information about who we are and how to contact us. In this case, we’re
    describing the browser it should treat our traffic as being from, along with our
    name and contact information. This is data that the website owner will be able
    to see in their server logs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这里*没有*使用 API，我们希望主动向网站所有者提供有关我们是谁以及如何联系我们的信息。在这种情况下，我们描述了浏览器应如何处理我们的流量，以及我们的名称和联系信息。这些数据网站所有者将能够在他们的服务器日志中看到。
- en: Now you’ll have a file called *MTA_turnstiles_index.html* in the same folder
    where your Python script was located. To see what it contains, you can just double-click
    on it, and it should open in your default web browser. Of course, because we only
    downloaded the raw code on the page and none of the extra files, images, and other
    materials that it would normally have access to on the web, it’s going to look
    a little wonky, probably something like what’s shown in [Figure 5-17](#local_copy_mta_turnstile_index).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将在与您的 Python 脚本相同的文件夹中有一个名为*MTA_turnstiles_index.html*的文件。要查看它包含的内容，您只需双击它，它应该会在您默认的网络浏览器中打开。当然，因为我们只下载了页面上的原始代码，而没有额外的文件、图片和其他材料，它看起来可能有些奇怪，可能与[图 5-17](#local_copy_mta_turnstile_index)所示的内容类似。
- en: Fortunately, that doesn’t matter at all, since what we’re after here is the
    list of links that’s stored with the page’s HTML. Before we worry about how to
    pull that data programmatically, however, we first need to find it within the
    page’s HTML code. To do this, we’re going to use our web browser’s *inspection
    tools*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这一点根本不重要，因为我们在这里追求的是存储在页面 HTML 中的链接列表。然而，在我们担心如何通过程序获取这些数据之前，我们首先需要在页面的
    HTML 代码中找到它。为此，我们将使用我们的网络浏览器的*检查工具*。
- en: '![Viewing our local copy of the MTA web page in a browser](assets/ppdw_0517.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![在浏览器中查看我们的 MTA 网页的本地副本](assets/ppdw_0517.png)'
- en: Figure 5-17\. Viewing our local copy of the MTA web page in a browser
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-17\. 在浏览器中查看我们的 MTA 网页的本地副本
- en: Using Browser Inspection Tools
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用浏览器检查工具
- en: With the local copy of the MTA turnstile data page open in a browser in front
    of you, scroll down until you can see the “Data Files” header, as shown in [Figure 5-17](#local_copy_mta_turnstile_index).
    To better target *just* the information we want on this web page with our Python
    script, we need to try to identify something unique about HTML code that surrounds
    it—this will make it easier for us to have our script zero in quickly on just
    the content we want. The easiest way to do this is by “inspecting” the code alongside
    the regular browser interface.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在您面前浏览器中打开 MTA 转门数据页面的本地副本后，向下滚动，直到您可以看到“数据文件”标题，如[图 5-17](#local_copy_mta_turnstile_index)所示。为了更精确地针对我们在这个网页上希望使用
    Python 脚本获取的*只有*信息，我们需要尝试识别一些围绕它的 HTML 代码中的独特内容——这将使我们的脚本更容易迅速地锁定我们想要的内容。最简单的方法是通过“检查”常规浏览器界面旁边的代码来做到这一点。
- en: To get started, put your mouse cursor over the “Data Files” text and context-click
    (also known as “right-click” or sometimes “Ctrl+click,” depending on your system).
    At the bottom of the menu that pops up, shown in [Figure 5-18](#MTA_context_click),
    choose “Inspect.”
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请将鼠标光标置于“数据文件”文本上，然后右键单击（也称为“右键单击”或有时“Ctrl+单击”，取决于您的系统）。在弹出的菜单底部，如[图 5-18](#MTA_context_click)所示，选择“检查”。
- en: '![The context menu on our local copy of the MTA web page](assets/ppdw_0518.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![我们的 MTA 网页本地副本上的上下文菜单](assets/ppdw_0518.png)'
- en: Figure 5-18\. The context menu on our local copy of the MTA web page
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-18\. 在浏览器中查看我们的 MTA 网页的本地副本上的上下文菜单
- en: While the precise location and shape of your particular browser’s inspection
    tools window will vary (this screenshot is from the Chrome browser), its contents
    will hopefully look at least somewhat similar to the image in [Figure 5-19](#inspection_window_highlights).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你特定浏览器的检查工具窗口的确切位置和形状会有所不同（此截图来自Chrome浏览器），但它的内容应该至少在某种程度上类似于[图 5-19](#inspection_window_highlights)中的图像。
- en: '![The inspection tools window with the Data Files header highlighted.](assets/ppdw_0519.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![突出显示“数据文件”标题的检查工具窗口。](assets/ppdw_0519.png)'
- en: Figure 5-19\. Inspection tools example
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-19\. 检查工具示例
- en: Wherever your own window appears (sometimes it is anchored to the side or bottom
    of your browser window, and, as in [Figure 5-19](#inspection_window_highlights),
    there are often multiple panes of information), the main thing we want to locate
    in the inspection tools window are the words “Data Files.” If you lost them (or
    never saw them in the first place!) once the window appeared, just move your mouse
    over those words on the web page and context-click to open the inspection tools
    window again.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的浏览器窗口显示在哪里（有时它会锚定在浏览器窗口的侧面或底部，并且如[图 5-19](#inspection_window_highlights)所示，通常有多个信息窗格），我们希望在检查工具窗口中找到的主要内容是“数据文件”这几个字。如果你在窗口出现后找不到它们（或者根本就没看见过它们！），只需将鼠标移动到网页上的这些字上并右键单击，以再次打开检查工具窗口。
- en: 'In this case, if you use your mouse to hover over the code in the inspection
    tools window that says:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果你用鼠标悬停在检查工具窗口中显示的代码上：
- en: '[PRE20]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: you should see the “Data Files” section of the web page highlighted in your
    browser window. Based on the area highlighted, it *appears* that this bit of code
    includes the entire list of links we’re interested in, which we can confirm by
    scrolling down in the inspection tools window. There we’ll see that all of the
    data links we want (which end in *.txt*) are indeed inside this `div` (notice
    how they are indented beneath it? That’s another instance of nesting at work!).
    Now, if we can confirm that class `span-84 last` only exists in one place on the
    web page, we have a good starting point for writing our Python script to extract
    the list of links.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能在浏览器窗口中看到网页的“数据文件”部分被突出显示。根据被突出显示的区域，*似乎*这段代码包含了我们感兴趣的所有链接列表，我们可以通过在检查工具窗口中向下滚动来确认这一点。在那里，我们将看到我们想要的所有数据链接（以*.txt*结尾）确实在这个`div`中（注意它们在其下缩进了吗？这是另一个嵌套工作的实例！）。现在，如果我们能确认类`span-84
    last`只存在于网页的一个地方，我们就有一个写Python脚本以提取链接列表的良好起点。
- en: 'The Python Web Scraping Solution: Beautiful Soup'
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python网页抓取解决方案：Beautiful Soup
- en: Before we begin writing our next Python script, let’s confirm that the `span-84
    last` class really is unique on the page we downloaded. The simplest way to do
    this is to first open the page in Atom (context-click on the filename instead
    of double-clicking, and choose Atom from the “Open with” menu option), which will
    show us the page’s code. Then do a regular “find” command (Ctrl+F or command+F)
    and search for `span-84 last`. As it turns out, even the `span-84` part only appears
    once in our file, so we can confine our Python script to looking for link information
    nested within that HTML tag.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写下一个Python脚本之前，让我们确认我们下载的页面上类`span-84 last`确实是唯一的。这样做的最简单方法是首先在Atom中打开页面（右键单击文件名而不是双击，并从“打开方式”菜单选项中选择Atom），这将显示页面的代码。然后执行常规的“查找”命令（Ctrl+F或Command+F）并搜索`span-84
    last`。事实证明，即使是`span-84`部分在我们的文件中也只出现了一次，所以我们可以将我们的Python脚本限制在查找该HTML标签内嵌的链接信息上。
- en: 'Now we’re ready to start writing the Python script that will extract the links
    from the web page. For this we’ll install and use the *Beautiful Soup* library,
    which is widely used for parsing the often messy markup we find on web pages.
    While *Beautiful Soup* has some functionality overlap with the *lxml* library
    that we used in [Example 4-12](ch04.html#xml_parsing), the main difference between
    them is that *Beautiful Soup* can handle parsing even less-than-perfectly-structured
    HTML and XML—which is what we invariably have to deal with on the web. In addition,
    *Beautiful Soup* lets us “grab” bits of markup by almost any feature we want—class
    name, tag type, or even attribute value—so it’s pretty much the go-to library
    for pulling data out of the “soup” of markup we often find online. You can read
    the [full documentation for the library](https://crummy.com/software/BeautifulSoup/bs4/doc),
    but the easiest way to install it will be the same process we’ve used for other
    libraries, by using `pip` on the command line:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备编写Python脚本，从网页中提取链接。为此，我们将安装并使用*Beautiful Soup*库，这是广泛用于解析网页上通常混乱标记的库。虽然*Beautiful
    Soup*在某些功能上与我们在[示例 4-12](ch04.html#xml_parsing)中使用的*lxml*库有所重叠，但它们之间的主要区别在于*Beautiful
    Soup*可以处理甚至不太完美结构的HTML和XML——这通常是我们在网页上不可避免要处理的内容。此外，*Beautiful Soup*允许我们根据几乎任何特征来“抓取”标记——类名、标签类型，甚至是属性值，因此它几乎成为了从网页标记“汤”中提取数据的首选库。你可以阅读关于该库的[完整文档](https://crummy.com/software/BeautifulSoup/bs4/doc)，但安装它的最简单方法与我们用于其他库的方法相同，即在命令行上使用`pip`：
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we’re ready to use Python to open the local copy of our web page and use
    the *Beautiful Soup* library to quickly grab all the links we need and write them
    to a simple *.csv* file, as shown in [Example 5-9](#parsing_the_soup).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用Python打开我们网页的本地副本，并使用*Beautiful Soup*库快速抓取我们需要的所有链接，并将它们写入一个简单的*.csv*文件，如[示例 5-9](#parsing_the_soup)所示。
- en: Example 5-9\. MTA_turnstiles_parsing.py
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-9\. MTA_turnstiles_parsing.py
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO6-1)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_accessing_web_based_data_CO6-1)'
- en: If we click one of the data links on the live copy of the web page, we see that
    the first part of the URL where the actual data file lives is `http://web.mta.info/developers/`,
    but each link only contains the latter half of the URL (in the format `data/nyct/turnstile/turnstile_YYMMDD.txt`).
    So that our download script has working links, we need to specify the “base” URL.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们点击网页上的一个数据链接，我们会发现实际数据文件位于URL的第一部分是`http://web.mta.info/developers/`，但每个链接只包含后半部分的URL（格式为`data/nyct/turnstile/turnstile_YYMMDD.txt`）。因此，为了使我们的下载脚本具有可用的链接，我们需要指定“基础”URL。
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO6-2)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_accessing_web_based_data_CO6-2)'
- en: Thanks to our work with the inspection tools, we can go straight to a `div`
    with the class `span-84 last` to start looking for the links we want. Note that
    because the word `class` has a special meaning in Python, Beautiful Soup appends
    an underscore to the end when we’re using it here (e.g., `class_`).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用检查工具的工作，我们可以直接定位到一个`class`为`span-84 last`的`div`来开始寻找我们需要的链接。请注意，在Python中`class`这个词有特殊含义，因此在此使用时Beautiful
    Soup会在其后附加下划线（例如，`class_`）。
- en: OK! What we should have now is a new file that contains a list of all the data
    links we’re interested in. Next we need to read that list in using a new script
    and download the files at those URLs. However, because we want to be careful not
    to overload the MTA website by downloading the files too quickly, we’re going
    to use the built-in Python *time* library to space out our requests by a second
    or two each. Also, we’ll be sure to only download the four files that we really
    want, rather than downloading everything just for the sake of it. To see how this
    second script is organized, take a look at [Example 5-10](#downloading_the_data).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 好了！现在我们应该有一个包含我们感兴趣的所有数据链接的新文件。接下来，我们需要使用一个新的脚本读取该列表，并下载这些URL中的文件。但是，由于我们不希望通过过快下载文件而过载MTA网站，我们将使用内置的Python
    *time*库来确保我们的请求每秒或每两秒之间有间隔。此外，我们将确保只下载我们真正需要的四个文件，而不是仅仅为了下载而下载。要了解这个第二个脚本的组织方式，请查看[示例 5-10](#downloading_the_data)。
- en: Example 5-10\. MTA_turnstiles_data_download.py
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-10\. MTA_turnstiles_data_download.py
- en: '[PRE23]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO7-1)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_accessing_web_based_data_CO7-1)'
- en: This library will let us “pause” our downloading script between data requests
    so that we don’t overload the MTA server with too many requests in too short a
    time period (and possibly get in trouble).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库将允许我们在数据请求之间“暂停”我们的下载脚本，以免在太短的时间内向MTA服务器发送过多请求（可能会惹上麻烦）。
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO7-2)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_accessing_web_based_data_CO7-2)'
- en: Here, we’re splitting the link URL on slashes, then taking the last item from
    the resulting list using [negative indexing](https://w3schools.com/python/gloss_python_string_negative_indexing.asp),
    which counts backward from the end of the string. This means that the item at
    position `-1` is the last item, which here is the *.txt* filename. This is the
    filename we’ll use for the local copy of the data that we save.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将链接URL根据斜杠分割，然后使用[negative indexing](https://w3schools.com/python/gloss_python_string_negative_indexing.asp)从结果列表中获取最后一项，负索引从字符串末尾开始计数。这意味着位置为`-1`的项是最后一项，这里是*.txt*文件名。这是我们将用于保存数据的本地副本的文件名。
- en: If everything has gone well, you should now have a new folder called *turnstile_data*,
    with the four most recent turnstile data files saved inside it. Pretty neat, right?
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您现在应该有一个名为*turnstile_data*的新文件夹，其中保存了最近的四个闸机数据文件。挺不错，对吧？
- en: Conclusion
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Now that we have explored the many ways to actually *get* the data we’re after
    and convert it into formats we can use, the next question is: what do we *do*
    with it all? Since the goal of all this data wrangling is to be able to answer
    questions and generate some insight about the world, we now need to move on from
    the process of *acquiring* data and start the process of assessing, improving,
    and analyzing it. To this end, in the next chapter we’ll work through a data quality
    evaluation of a public dataset, with an eye toward understanding both its possibilities
    and limitations and how our data wrangling work can help us make the most of it.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了多种实际获取数据并将其转换为可用格式的方法，接下来的问题是：我们应该如何处理这些数据？由于所有这些数据整理的目标是能够回答问题并生成一些关于世界的见解，所以我们现在需要从*获取*数据的过程转向评估、改进和分析数据的过程。为此，在下一章中，我们将对一个公共数据集进行数据质量评估，以便理解其可能性和局限性，以及我们的数据整理工作如何帮助我们充分利用它。
- en: ^([1](ch05.html#idm45143408861008-marker)) Like the [US Treasurey, for example](https://fiscaldata.treasury.gov/api-documentation).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#idm45143408861008-marker)) 就像[例如美国财政部](https://fiscaldata.treasury.gov/api-documentation)一样。
- en: ^([2](ch05.html#idm45143408859632-marker)) For two examples, see articles in
    [*The Markup*](https://themarkup.org/google-the-giant/2021/04/09/how-we-discovered-googles-social-justice-blocklist-for-youtube-ad-placements)
    and [*NPR*](https://npr.org/2021/08/04/1024791053/facebook-boots-nyu-disinformation-researchers-off-its-platform-and-critics-cry-f).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#idm45143408859632-marker)) 例如，请参阅[*The Markup*](https://themarkup.org/google-the-giant/2021/04/09/how-we-discovered-googles-social-justice-blocklist-for-youtube-ad-placements)和[*NPR*](https://npr.org/2021/08/04/1024791053/facebook-boots-nyu-disinformation-researchers-off-its-platform-and-critics-cry-f)上的文章。
- en: ^([3](ch05.html#idm45143408848400-marker)) This is also the first step to building
    your own “apps”!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.html#idm45143408848400-marker)) 这也是构建自己“应用程序”的第一步！
- en: ^([4](ch05.html#idm45143408324000-marker)) While this is probably most likely
    to happen if you make too many data requests in too short a time frame, most API
    providers can terminate your access to their API pretty much whenever they want,
    and for whatever reason they want.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.html#idm45143408324000-marker)) 虽然如果您在太短的时间内提出太多数据请求，这种情况可能发生得最有可能，但大多数API提供者可以在任何时候出于任何原因终止您对其API的访问。
- en: ^([5](ch05.html#idm45143408221168-marker)) For example, when you `git push`
    your code.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.html#idm45143408221168-marker)) 例如，当您`git push`您的代码时。
- en: ^([6](ch05.html#idm45143408216624-marker)) “Hackers Used SolarWinds’ Dominance
    against It in Sprawling Spy Campaign” by Raphael Satter, Christopher Bing, and
    Joseph Menn, [*https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8*](https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8);
    “Former SolarWinds CEO Blames Intern for *solarwinds123* Password Leak” by Brian
    Fung and Geneva Sands, [*https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern*](https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch05.html#idm45143408216624-marker)) “黑客利用 SolarWinds 的主导地位进行大规模间谍活动”
    作者 Raphael Satter、Christopher Bing 和 Joseph Menn，[*https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8*](https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8);
    “前 SolarWinds CEO 指责实习生泄露 *solarwinds123* 密码” 作者 Brian Fung 和 Geneva Sands，[*https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern*](https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern).
- en: ^([7](ch05.html#idm45143408103984-marker)) That’s why the program that makes
    Python run on your device is often called a Python *interpreter*—because it translates
    the code we humans write into bytecode that your device can actually understand.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch05.html#idm45143408103984-marker)) 这就是为什么使 Python 在您的设备上运行的程序通常被称为 Python
    *解释器* —— 因为它将我们人类编写的代码转换为您的设备实际可以理解的字节码。
- en: ^([8](ch05.html#idm45143408008560-marker)) Even if we run, say, `git add -A`
    or `git commit -a`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch05.html#idm45143408008560-marker)) 即使我们运行，比如 `git add -A` 或 `git commit
    -a`。
- en: ^([9](ch05.html#idm45143407997552-marker)) A repository can have different *.gitignore*
    files in different folders; for complete details, you can take a look at the [documentation](https://git-scm.com/docs/gitignore).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch05.html#idm45143407997552-marker)) 一个仓库可以在不同的文件夹中有不同的 *.gitignore* 文件；要获取完整的详情，您可以查看
    [文档](https://git-scm.com/docs/gitignore)。
- en: '^([10](ch05.html#idm45143407977504-marker)) We used this approach with the
    *glob* library in [Example 4-16](ch04.html#pdf_parsing) and will examine it in
    more detail in [“Regular Expressions: Supercharged String Matching”](ch07.html#regex_example).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch05.html#idm45143407977504-marker)) 我们在 [示例 4-16](ch04.html#pdf_parsing)
    中使用了 *glob* 库，并将在 [“正则表达式：强化的字符串匹配”](ch07.html#regex_example) 中详细讨论它。
- en: ^([11](ch05.html#idm45143407870720-marker)) These keys have since been replaced
    and will not work!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch05.html#idm45143407870720-marker)) 这些密钥已被替换，将不再起作用！
- en: ^([12](ch05.html#idm45143407651104-marker)) For example, the contents of a `post`
    request are not saved in the browser history the way that `get` requests are.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch05.html#idm45143407651104-marker)) 例如，`post` 请求的内容不会像 `get` 请求那样保存在浏览器历史记录中。
- en: ^([13](ch05.html#idm45143407307360-marker)) Remember that each time you run
    the script, you will also overwrite your output file, so it will only ever contain
    the most recent results.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch05.html#idm45143407307360-marker)) 请记住，每次运行脚本时，您也将覆盖输出文件，因此它只会包含最新的结果。
- en: '^([14](ch05.html#idm45143407245520-marker)) Aleecia M. McDonald and Lorrie
    Faith Cranor, “The Cost of Reading Privacy Policies,” *I/S: A Journal of Law and
    Policy for the Information Society* 4 (2008): 543, [*https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf*](https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf),
    and Jonathan A. Obar and Anne Oeldorf-Hirsch, “The Biggest Lie on the Internet:
    Ignoring the Privacy Policies and Terms of Service Policies of Social Networking
    Services,” *Information, Communication & Society* 23 no. 1 (2020): 128–147, [*https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465*](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '^([14](ch05.html#idm45143407245520-marker)) Aleecia M. McDonald 和 Lorrie Faith
    Cranor， “阅读隐私政策的成本”， *I/S: 信息社会法律与政策杂志* 4 (2008): 543， [*https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf*](https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf)，
    以及 Jonathan A. Obar 和 Anne Oeldorf-Hirsch，“互联网上最大的谎言：忽视社交网络服务的隐私政策和服务条款”， *信息、传播与社会*
    23 号 1 (2020): 128–147， [*https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465*](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465).'
- en: ^([15](ch05.html#idm45143407239440-marker)) Victoria D. Baranetsky, “Data Journalism
    and the Law,” *Tow Center for Digital Journalism* (2018) [*https://doi.org/10.7916/d8-15sw-fy51*](https://doi.org/10.7916/d8-15sw-fy51).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch05.html#idm45143407239440-marker)) Victoria D. Baranetsky，“数据新闻与法律”，
    *Tow Center for Digital Journalism* (2018) [*https://doi.org/10.7916/d8-15sw-fy51*](https://doi.org/10.7916/d8-15sw-fy51).
- en: '^([16](ch05.html#idm45143407214288-marker)) The accompanying blog post is also
    excellent: [*https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web*](https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch05.html#idm45143407214288-marker)) 伴随的博客文章也很棒：[*https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web*](https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web)。
