- en: 6 AutoML with a fully customized search space
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 使用完全自定义的搜索空间进行AutoML
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Customizing the entire AutoML search space without connecting AutoML blocks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不连接AutoML块的情况下自定义整个AutoML搜索空间
- en: Tuning autoencoder models for unsupervised learning tasks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整自动编码器模型以进行无监督学习任务
- en: Tuning shallow models with preprocessing pipelines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预处理管道调整浅层模型
- en: Controlling the AutoML process by customizing tuners
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过自定义调整器控制AutoML过程
- en: Joint tuning and selection among deep learning and shallow models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习和浅层模型之间进行联合调整和选择
- en: Hyperparameter tuning beyond Keras and scikit-learn models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整超出Keras和scikit-learn模型
- en: This chapter introduces customization of the entire AutoML search space in a
    layerwise fashion without wiring up AutoML blocks, giving you more flexibility
    in designing the search space for tuning unsupervised learning models and optimization
    algorithms. We introduce how to tune a shallow model with its preprocessing pipeline,
    including feature engineering steps. You will also learn how to control the model-training
    and evaluation process to conduct the joint tuning and selection of deep learning
    models and shallow models. This allows you to tune models with different training
    and evaluation procedures implemented with different ML libraries.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了在不连接AutoML块的情况下以分层方式自定义整个AutoML搜索空间，这为你设计调整无监督学习模型和优化算法的搜索空间提供了更大的灵活性。我们介绍了如何使用预处理管道调整浅层模型，包括特征工程步骤。你还将学习如何控制模型训练和评估过程，以进行深度学习模型和浅层模型的联合调整和选择。这允许你使用不同ML库实现的不同训练和评估程序来调整模型。
- en: 6.1 Customizing the search space in a layerwise fashion
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 以分层方式自定义搜索空间
- en: 'In chapter 5, you learned how to perform hyperparameter tuning and model selection
    by specifying the search space with AutoML blocks. You also know how to create
    your own AutoML blocks if the built-in blocks do not fit your needs. However,
    you may encounter some scenarios that are hard to address by wiring together AutoML
    blocks or where this simply isn’t the best approach, such as the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，你学习了如何通过指定搜索空间使用AutoML块进行超参数调整和模型选择。你还知道如果内置块不符合你的需求，如何创建自己的AutoML块。然而，你可能遇到一些难以通过连接AutoML块解决的场景，或者这种方法根本不是最佳选择，例如以下情况：
- en: '*Tuning models for tasks beyond classification and regression*—Although these
    are probably the most widely studied problems in ML, you may face tasks that lie
    outside these areas. They may not even be supervised learning tasks, where there
    are pre-existing responses for you to learn from.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调整超出分类和回归任务的模型*——尽管这些可能是机器学习中最广泛研究的问题，但你可能会遇到这些领域之外的任务。它们甚至可能不是监督学习任务，其中存在预先存在的响应供你学习。'
- en: '*Customizing the search space of the optimization algorithm (e.g., tuning the
    learning rate or batch size) and the loss function*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自定义优化算法的搜索空间（例如，调整学习率或批量大小）和损失函数*'
- en: Besides these scenarios, you may find it superfluous to create an AutoML pipeline
    by wiring together AutoML blocks if you need to customize all the blocks in the
    pipeline. Why not directly create the pipeline in one build() function, like you
    implement a neural network with TensorFlow Keras, rather than doing a two-step
    job (wrapping them up into different AutoKeras AutoML blocks and connecting them)?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些场景之外，如果你需要自定义管道中的所有块，可能觉得通过连接AutoML块来创建AutoML管道是多余的。为什么不直接在一个build()函数中创建管道，就像你使用TensorFlow
    Keras实现神经网络一样，而不是进行两步操作（将它们包装成不同的AutoKeras AutoML块并将它们连接起来）？
- en: In this section, you’ll see how to fully customize a search space in a layerwise
    fashion without wiring up AutoML blocks. This method may require more code, but
    it provides extra flexibility in customizing the search space for tuning tasks
    beyond supervised learning. If there are no suitable built-in AutoML blocks for
    you to use, this method can also reduce the burden of creating and wiring multiple
    custom blocks to define the search space. To achieve this, we’ll use KerasTuner,
    an AutoML library originally proposed for selecting and tuning TensorFlow Keras
    models and beyond. We used its hyperparameters module earlier in the previous
    two chapters to customize partial search space, and here we’ll use it to build
    a search space from scratch. We’ll start by tuning an MLP model for a regression
    task, which involves the tuning of optimization algorithms and data preprocessing
    method with custom search space. Then we’ll use KerasTuner to tune an autoencoder
    model for unsupervised learning tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解如何以分层的方式完全自定义搜索空间，而无需连接 AutoML 块。这种方法可能需要更多的代码，但它为定制超出监督学习之外的调整任务的搜索空间提供了额外的灵活性。如果您没有合适的内置
    AutoML 块可以使用，这种方法还可以减少创建和连接多个自定义块以定义搜索空间的负担。为了实现这一点，我们将使用 KerasTuner，这是一个最初为选择和调整
    TensorFlow Keras 模型以及更多内容而提出的 AutoML 库。我们在前两章中使用了它的超参数模块来定制部分搜索空间，在这里我们将使用它从头开始构建搜索空间。我们将从一个用于回归任务的
    MLP 模型开始调整，这涉及到使用自定义搜索空间调整优化算法和数据预处理方法。然后我们将使用 KerasTuner 来调整用于无监督学习任务的自动编码器模型。
- en: 6.1.1 Tuning an MLP for regression with KerasTuner
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 使用 KerasTuner 调整 MLP 的回归
- en: Let’s first work on the problem of tuning an MLP for regression. We’ll tune
    the number of units in the hidden layers first and gradually add more hyperparameters
    in the search space.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先解决调整 MLP 用于回归的问题。我们将首先调整隐藏层中的单元数量，并逐步在搜索空间中添加更多超参数。
- en: Implementing the search space for tuning an MLP model with KerasTuner is almost
    the same as building an MLP model with Keras. We need a build() function that
    can generate an MLP model and define the search space for the hyperparameters
    we’re concerned with, such as the number of units. You’ve seen how to define the
    search space with the hp container when customizing the AutoML blocks, and the
    process here is the same. However, because our purpose in this case is not to
    create an AutoML block and wire it together with other blocks to form the search
    space, we should create every component of the deep learning pipeline in one build()
    function. In other words, besides the network architecture, we also need to create
    the input node, set up the output regression layer, select the optimization algorithm,
    and compile the model for training. Listing 6.1 shows how to implement the build()
    function for tuning the number of units with KerasTuner. Except for the use of
    the hp container for defining the search space of the units hyperparameter, the
    rest is identical to building up a Keras model for regression, which you learned
    to do in chapter 3.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KerasTuner 调整 MLP 模型的搜索空间与使用 Keras 构建一个 MLP 模型几乎相同。我们需要一个 build() 函数，它可以生成一个
    MLP 模型并定义我们关心的超参数的搜索空间，例如单元数量。您已经看到如何在自定义 AutoML 块时使用 hp 容器来定义搜索空间，这里的流程是相同的。然而，由于我们在这个案例中的目的不是创建一个
    AutoML 块并将其与其他块连接起来以形成搜索空间，我们应该在一个 build() 函数中创建深度学习管道的每个组件。换句话说，除了网络架构之外，我们还需要创建输入节点，设置输出回归层，选择优化算法，并编译模型以进行训练。列表
    6.1 展示了如何使用 KerasTuner 实现调整单元数量的 build() 函数。除了使用 hp 容器来定义单元超参数的搜索空间之外，其余部分与构建用于回归的
    Keras 模型相同，这是您在第 3 章中学到的。
- en: Listing 6.1 Implementing the search space for tuning MLP units with KerasTuner
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 使用 KerasTuner 实现调整 MLP 单元的搜索空间
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Creates the build function, whose input is an hp container instance
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个 build 函数，其输入是一个 hp 容器实例
- en: ❷ Defines the search space for the number of units in the two hidden layers
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义两个隐藏层中单元数量的搜索空间
- en: ❸ Compiles the model with the Adam optimization algorithm and MSE loss, and
    calculates the MAE metric for the model
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 Adam 优化算法和 MSE 损失编译模型，并计算模型的 MAE 指标
- en: ❹ The returned model is a Keras model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回的模型是一个 Keras 模型。
- en: After defining the search space, we still need a search method to help conduct
    the search process. Recall from chapter 5 that when creating an AutoML pipeline,
    we wrap the blocks in an AutoModel object and then set its tuner argument (or
    use the default value, 'greedy') to select a search method for tuning. Here, because
    we do not have an AutoModel object, we directly choose a *tuner* from KerasTuner
    to do this. A tuner not only specifies a search method but also helps schedule
    the training and evaluation of the selected models in each search trial to ensure
    the sequential search process can proceed smoothly. We will discuss more about
    this later in this chapter. The name of the tuner accords with the search method
    we want to use. For example, we can use the random search method by choosing the
    RandomSearch tuner, as shown in listing 6.2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义搜索空间之后，我们仍然需要一个搜索方法来帮助执行搜索过程。回顾第 5 章的内容，在创建 AutoML 管道时，我们将模块封装在 AutoModel
    对象中，然后设置其 tuner 参数（或使用默认值，'greedy'）来选择一个搜索方法进行调优。在这里，因为我们没有 AutoModel 对象，所以我们直接从
    KerasTuner 中选择一个 *tuner* 来完成这个任务。一个调优器不仅指定了搜索方法，还帮助安排每个搜索试验中选定模型的训练和评估，以确保顺序搜索过程可以顺利执行。我们将在本章后面更详细地讨论这一点。调优器的名称与我们要使用的搜索方法相匹配。例如，我们可以通过选择
    RandomSearch 调优器来使用随机搜索方法，如列表 6.2 所示。
- en: We provide the search space (the build() function) to it during the initialization
    and set the number of models we want to explore during the search process (max_
    trial=5). The objective argument specifies the metric (or loss function) we want
    to use to compare the models and optimize the search method (for the random search
    method, there’s nothing to optimize). In this example, in each trial, the random
    tuner will build a model based on the randomly selected units value, train the
    model, and evaluate it. It will return the best model based on the objective we
    set, which is the mean absolute error (MAE) of the validation set (indicated by
    the val_ prefix). This validation set, if not specified, will be split out at
    random from the training set. We can specify how many times we want each model
    to be trained and evaluated, to reduce randomness in the evaluation process, by
    setting the executions_ per_trial argument.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化过程中，我们向其提供搜索空间（build() 函数），并设置在搜索过程中想要探索的模型数量（max_trial=5）。objective 参数指定了我们要用于比较模型和优化搜索方法的指标（或损失函数）（对于随机搜索方法，没有需要优化的内容）。在这个例子中，在每个试验中，随机调优器将基于随机选择的单元值构建一个模型，训练该模型并评估它。它将根据我们设定的目标返回最佳模型，即验证集的平均绝对误差（MAE）（由
    val_ 前缀表示）。如果没有指定验证集，它将随机从训练集中分割出来。我们可以通过设置 executions_per_trial 参数来指定每个模型想要训练和评估的次数，以减少评估过程中的随机性。
- en: Listing 6.2 Initializing the tuner
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 初始化调优器
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Initializes a random search tuner
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化随机搜索调优器
- en: ❷ Passes in the build_model function
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 传递 build_model 函数
- en: ❸ Sets the objective to optimize while selecting models
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在选择模型时设置优化目标
- en: ❹ The total number of different hyperparameter value sets to try
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 尝试的不同超参数值集的总数
- en: ❺ The number of runs for one hyperparameter value set
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 一个超参数值集的运行次数
- en: ❻ The directory in which to save the results
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保存结果的目录
- en: ❼ The name of the project
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 项目名称
- en: Before we start the search process, let’s print a summary of the search space
    using the summarize_search_space() method to make sure the tuner has received
    it as expected.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始搜索过程之前，让我们使用 summarize_search_space() 方法打印搜索空间的摘要，以确保调优器已按预期接收它。
- en: Listing 6.3 Printing the search space summary
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 打印搜索空间摘要
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Prints a summary of the search space
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印搜索空间摘要
- en: We can see that the created search space contains one hyperparameter (units).
    It is an integer that should be selected from the range 32 to 512, with a step
    size of 32.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到创建的搜索空间包含一个超参数（units）。它是一个整数，应该从 32 到 512 的范围内选择，步长为 32。
- en: Now let’s create a synthetic regression dataset and call the search() method
    of the tuner to start the search process. The search results are shown in listing
    6.4\. The arguments to the search() function control the training and evaluation
    of each model, corresponding to the arguments supported in the fit() method of
    a Keras model (tf.keras.Model.fit()). For example, in this case each selected
    MLP will be trained for one epoch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个合成回归数据集，并调用调优器的 search() 方法来启动搜索过程。搜索结果在列表 6.4 中显示。search() 函数的参数控制每个模型的训练和评估，对应于
    Keras 模型 fit() 方法支持的参数（tf.keras.Model.fit()）。例如，在这种情况下，每个选定的 MLP 将训练一个 epoch。
- en: Listing 6.4 Running the search
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 运行搜索
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Randomly creates a synthetic regression dataset with 20 features in each instance
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 随机创建一个具有每个实例20个特征的合成回归数据集
- en: ❷ Runs the search, which may take a while
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行搜索，这可能需要一段时间
- en: We can use the results_summary() function to print the five best models and
    their evaluation results, as shown in the next listing. The best model here has
    218 units in each dense layer. Its MAE on the validation set during the search
    process is 0.212.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用results_summary()函数打印五个最佳模型及其评估结果，如下一列表所示。这里的最佳模型在每个密集层中有218个单元。在搜索过程中，它在验证集上的MAE为0.212。
- en: Listing 6.5 Printing the five best models and the MAE of each
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 打印五个最佳模型及其每个模型的MAE
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After the search, we may want to export the best model to save it for future
    use. Doing this is straightforward, as listing 6.6 shows. The exported model is
    a Keras model, and we can print the architecture by calling the summary() function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索之后，我们可能想要导出最佳模型以保存它供将来使用。如列表6.6所示，这样做很简单。导出的模型是一个Keras模型，我们可以通过调用summary()函数来打印其架构。
- en: Listing 6.6 Summarizing and exporting the model
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 总结和导出模型
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Specifies that the function should return a list of two models
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定函数应返回两个模型的列表
- en: ❷ Gets the best model in the returned list
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从返回的列表中获取最佳模型
- en: ❸ Saves the model to disk
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将模型保存到磁盘
- en: ❹ Loads the model from disk
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从磁盘加载模型
- en: ❺ Uses the model for predictions
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用模型进行预测
- en: ❻ Prints a summary of the architecture of the model
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印模型架构的摘要
- en: Jointly tuning the optimization function
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 联合调整优化函数
- en: One benefit of customizing the whole pipeline in the build() function is that
    we can tune the optimization function. Because we need to compile the network
    ourselves to make sure it can be trained, we can take full control of which optimization
    method to use, as well as its hyperparameters, such as the learning rate. For
    example, we can choose from two widely used optimizers in deep learning, Adam
    and Adadelta, with the help of the hp.Choice() method. The learning rate of the
    selected optimization method can be sampled between 1e-5 and 0.1\. And because,
    practically, we often choose the learning rate on a logarithmic magnitude, such
    as 0.1 or 0.01, we can set the sampling argument to 'log' to assign equal probabilities
    to each order of magnitude range, as shown in the next code listing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在build()函数中自定义整个流程的一个好处是我们可以调整优化函数。因为我们需要自己编译网络以确保它可以被训练，我们可以完全控制使用哪种优化方法，以及其超参数，例如学习率。例如，我们可以通过hp.Choice()方法从深度学习中广泛使用的两个优化器中选择，Adam和Adadelta。所选优化方法的学习率可以在1e-5和0.1之间采样。而且因为实际上我们经常在对数尺度上选择学习率，例如0.1或0.01，我们可以将采样参数设置为'log'，以将等概率分配给每个数量级范围，如下一代码列表所示。
- en: Listing 6.7 Jointly tuning the units and optimization method
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 联合调整单元和优化方法
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Uniformly random sample on the logarithmic magnitude
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在对数尺度上进行均匀随机采样
- en: ❷ Defines the search space for the optimizer
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义优化器的搜索空间
- en: ❸ Compiles the model to set the loss
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编译模型以设置损失
- en: ❹ The metric we’re using is MAE.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们使用的指标是MAE。
- en: ❺ The function should return a Keras model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 函数应返回一个Keras模型。
- en: The search process is the same as in the previous example and will not be repeated
    here. Not surprisingly, we can tune the loss function in the same way, so we won’t
    elaborate on that further here.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索过程与之前的示例相同，这里不再重复。不出所料，我们可以以相同的方式调整损失函数，因此我们不会进一步详细说明。
- en: Tuning the model-training process
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 调整模型训练过程
- en: Besides the model-building process, you may also have some hyperparameters to
    tune in the model-training process. For example, you may want to tune the batch
    size of the data and whether to shuffle the data. You can specify these in the
    arguments of model.fit() of a Keras model. However, in the previous code examples,
    we define the search space only for building and compiling the model, not training
    the model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型构建过程之外，在模型训练过程中，你可能还需要调整一些超参数。例如，你可能想要调整数据的批量大小以及是否对数据进行洗牌。你可以在Keras模型的model.fit()方法的参数中指定这些。然而，在之前的代码示例中，我们只为构建和编译模型定义了搜索空间，而不是为训练模型定义。
- en: To tune the model-training process, we can use the HyperModel class, which provides
    an object-oriented style for defining the search space. You can override HyperModel.build(hp),
    which is the same as the build_model() function shown earlier. You need to override
    HyperModel.fit() to tune the model training. In the arguments of the method, you
    can access hp, the model we just built, and all the arguments passed to Tuner.search()
    in **kwargs. It should return the training history, which is the return value
    of model.fit(). The next listing is an example of defining a search space for
    tuning whether to shuffle the dataset and tuning the batch size in addition to
    the rest of the hyperparameters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要调整模型训练过程，我们可以使用 HyperModel 类，它提供了一种面向对象的方式来定义搜索空间。你可以覆盖 HyperModel.build(hp)，这与前面展示的
    build_model() 函数相同。你需要覆盖 HyperModel.fit() 来调整模型训练。在方法参数中，你可以访问 hp，我们刚刚构建的模型，以及传递给
    Tuner.search() 的所有 **kwargs。它应该返回训练历史，这是 model.fit() 的返回值。接下来的列表是一个定义搜索空间以调整是否打乱数据集以及调整批次大小，除了其他超参数的示例。
- en: Listing 6.8 Tuning the model-training process
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.8 调整模型训练过程
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Tunes the model training
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调整模型训练
- en: ❷ Returns the return value of model.fit()
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回 model.fit() 的返回值
- en: ❸ Tunes the batch size
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 调整批次大小
- en: ❹ Tunes whether to shuffle the dataset
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 调整是否打乱数据集
- en: Note that we did not tune the number of epochs in fit(—the model will be saved
    at its best epoch in terms of the objective value, which is the 'val_mae' here.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有在 fit() 中调整训练轮数（模型将在最佳轮次保存，以目标值为准，这里是指 'val_mae'）。
- en: So far we have learned how to define and tune the hyperparameters in the model-building,
    -compiling, and -fitting process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何在模型构建、编译和拟合过程中定义和调整超参数。
- en: Tuning the data preprocessing method
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 调整数据预处理方法
- en: Sometimes you may want to include some preprocessing steps in the workflow—for
    example, normalizing the features before feeding them into the neural network.
    In this section, we will first see how we can easily include the preprocessing
    steps in your model. Then, we will see how to tune the hyperparameters involved
    in these steps.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你可能想在工作流程中包含一些预处理步骤——例如，在将特征输入神经网络之前对特征进行归一化。在本节中，我们首先将看到如何轻松地将预处理步骤包含到你的模型中。然后，我们将看到如何调整涉及这些步骤的超参数。
- en: 'Instead of creating a normalization function the way you learned in chapter
    3, you can use the Keras preprocessing layer to do this. For example, you can
    stack a normalization layer before the dense layers, as shown in listing 6.9\.
    Note that there is special treatment for the preprocessing layer here: you call
    a layer adaptation function (layer.adapt()) on the full dataset. Neural networks
    are trained and evaluated on data batches, but the preprocessing method usually
    needs some statistics from the entire dataset, such as the means and variances
    of the features, to preprocess the batches. Calling the adaptation function will
    help the preprocessing layer gather these statistics.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与你在第 3 章中学到的创建归一化函数的方式不同，你可以使用 Keras 预处理层来完成这个任务。例如，你可以在密集层之前堆叠一个归一化层，如列表 6.9
    所示。请注意，这里的预处理层有特殊处理：你需要在整个数据集上调用一个层适配函数（layer.adapt()）。神经网络是在数据批次上训练和评估的，但预处理方法通常需要从整个数据集中获取一些统计信息，例如特征的均值和方差，以预处理批次。调用适配函数将帮助预处理层收集这些统计信息。
- en: Listing 6.9 Using a Keras normalization preprocessing layer
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.9 使用 Keras 归一化预处理层
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Initializes the normalization layer
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化归一化层
- en: ❷ Adapts the normalization layer to get the mean and variance of the data
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将归一化层适配到数据以获取均值和方差
- en: 'We have multiple ways of using the preprocessing layers. In the previous example,
    we put it into the Sequential model. You may also use it as a standalone step.
    You can use it to preprocess NumPy arrays. For NumPy arrays, you can just call
    the layers to get the processed data as shown in the following code sample:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种使用预处理层的方式。在之前的例子中，我们将其放入 Sequential 模型中。你也可以将其作为一个独立的步骤使用。你可以用它来预处理 NumPy
    数组。对于 NumPy 数组，你只需调用层即可获取处理后的数据，如下面的代码示例所示：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: However, your data may not always be in the format of NumPy arrays. It could
    be in the format of tf.data.Dataset, which is more generalized. It can be created
    from a small NumPy array or even large dataset streamed from a local or remote
    storage. The next code listing shows how to convert a NumPy array to tf.data.Dataset
    and normalize it with the layer we created by calling Dataset.map(layer).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你的数据可能并不总是以NumPy数组的格式存在。它可能是tf.data.Dataset的格式，这是一种更通用的格式。它可以由一个小NumPy数组或甚至是从本地或远程存储流式传输的大数据集创建。下面的代码示例展示了如何将NumPy数组转换为tf.data.Dataset，并使用我们通过调用Dataset.map(layer)创建的层进行归一化。
- en: Listing 6.10 Normalizing tf.data.Dataset
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10 归一化tf.data.Dataset
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we have learned how to use preprocessing layers for data preprocessing.
    Let’s see if we can have a Boolean hyperparameter to tune whether to use this
    normalization step.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何使用预处理层进行数据预处理。让我们看看我们是否可以有一个布尔超参数来调整是否使用这个归一化步骤。
- en: Because data preprocessing requires accessing the dataset, we will tune this
    step in HyperModel.fit(), which has the dataset in the arguments passed from Tuner.search(),
    as shown in the next listing. Instead of leaving these useful arguments in **kwargs,
    we will explicitly put x and y in the method signature this time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据预处理需要访问数据集，我们将在HyperModel.fit()中调整这一步，该函数具有从Tuner.search()传递的参数中的数据集，如下所示。这次，我们不会将这些有用的参数留在**kwargs中，而是会明确地将x和y放入方法签名中。
- en: Listing 6.11 Using a preprocessing layer in the search space
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.11 在搜索空间中使用预处理层
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Specifies whether to use a normalization layer
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定是否使用归一化层
- en: ❷ Replaces x with the normalized data
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用归一化后的数据替换x
- en: After implementing the search space, we can feed it to a tuner as we did with
    the build_model() function to search for a good model, as shown next.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现搜索空间后，我们可以将其输入到一个调优器中，就像我们对build_model()函数所做的那样，以搜索一个好的模型，如下所示。
- en: Listing 6.12 Searching in a space with a preprocessing layer
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 在具有预处理层的空间中进行搜索
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Passes the class instance to the tuner
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将类实例传递给调优器
- en: Having worked through this example, you may be wondering why, because the layerwise
    search space design can resolve all the problems that the blockwise search space
    design can with even more flexibility, we might ever want to build up an AutoML
    pipeline by wiring together AutoML blocks. The reason is, compared to layerwise
    search space design, using the built-in blocks of AutoKeras makes search space
    creation less laborious (especially if you aren’t well practiced in creating the
    search space in a layerwise fashion). The tradeoff between the implementation
    difficulty and flexibility will determine which search-space-creation method is
    best for you and the task at hand.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这个示例后，你可能想知道为什么，因为层状搜索空间设计可以解决块状搜索空间设计可以解决的问题，甚至具有更多的灵活性，我们可能还想通过连接AutoML块来构建AutoML管道。原因是，与层状搜索空间设计相比，使用AutoKeras的内置块可以使搜索空间创建不那么费力（特别是如果你不擅长以层状方式创建搜索空间）。实现难度和灵活性之间的权衡将决定哪种搜索空间创建方法最适合你和你手头的任务。
- en: 6.1.2 Tuning an autoencoder model for unsupervised learning
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 调整自动编码器模型以进行无监督学习
- en: All the examples we’ve worked on so far have been supervised learning tasks,
    but ML applications aren’t all about predicting pre-existing responses. In contrast
    to supervised learning, an ML paradigm called *unsupervised learning* aims to
    find hidden or undetected patterns in features without human supervision or involvement.
    A typical unsupervised learning task is *dimensionality reduction*. The goal is
    to learn a condensed representation (such as a vector with few elements) for the
    data instances, to remove the noisy or unimportant information in the data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所做的一切示例都是监督学习任务，但机器学习应用并不全是关于预测预存在的响应。与监督学习相反，一种称为*无监督学习*的机器学习范式旨在在没有人类监督或参与的情况下，在特征中找到隐藏或未检测到的模式。一个典型的无监督学习任务是*降维*。目标是学习数据实例的压缩表示（例如，具有少量元素的向量），以去除数据中的噪声或不重要信息。
- en: An *autoencoder* is a classic type of neural network to use for dimensionality
    reduction. It is often applied to images to learn a low-dimensional vector representation
    of each image. The architecture of an autoencoder is shown in figure 6.1.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动编码器*是一种经典的神经网络类型，用于降维。它通常应用于图像，以学习每个图像的低维向量表示。自动编码器的架构如图6.1所示。'
- en: '![06-01](../Images/06-01.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![06-01](../Images/06-01.png)'
- en: Figure 6.1 The architecture of an autoencoder
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 自动编码器的架构
- en: As you can see, the input and the output of the autoencoder are both images.
    The network consists of two parts, the *encoder* and the *decoder*. The encoder
    is used to condense the images to the low-dimensional vector representation, and
    the decoder tries to reconstruct the original image based on the encoded vector.
    The encoder and decoder often have an asymmetric structure. They both can use
    classic networks, such as MLPs or CNNs. For example, if each input instance is
    a vector of length 64, a simple encoder could be a single-layer MLP with 32 units
    in each dense layer. The decoder could also be a single-layer MLP with 64 units
    in the dense layer to decode each input to the original size.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，自动编码器的输入和输出都是图像。网络由两部分组成，即 *编码器* 和 *解码器*。编码器用于将图像压缩到低维向量表示，而解码器则试图根据编码向量重建原始图像。编码器和解码器通常具有不对称的结构。它们都可以使用经典网络，例如
    MLP 或 CNN。例如，如果每个输入实例是一个长度为 64 的向量，一个简单的编码器可以是一个单层 MLP，每个密集层有 32 个单元。解码器也可以是一个单层
    MLP，密集层有 64 个单元，以解码每个输入到原始大小。
- en: In this task, because we do not have pre-existing responses to guide the learning
    process, we use the original images themselves as the responses. During training,
    the difference between the reconstructed images and the original images will be
    the loss metric to help update the weights of the network. We can use a regression
    loss function like mean squared error (MSE) to measure the difference.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，因为我们没有预先存在的响应来指导学习过程，所以我们使用原始图像本身作为响应。在训练过程中，重建图像与原始图像之间的差异将是损失指标，以帮助更新网络的权重。我们可以使用回归损失函数，如均方误差（MSE）来衡量差异。
- en: In regression tasks, MSE measures the difference between the predicted value
    and the ground-truth value. In this image reconstruction task, the predicted value
    is the reconstructed image, whereas the ground-truth is the original image. If
    the neural network can successfully reconstruct the image, it means all the important
    information about the image is stored in the vector representation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归任务中，均方误差（MSE）衡量预测值和真实值之间的差异。在这个图像重建任务中，预测值是重建的图像，而真实值是原始图像。如果神经网络能够成功重建图像，这意味着图像的所有重要信息都存储在向量表示中。
- en: 'The learned vector representations have many potential uses: for example, they
    could be used to visualize the distribution of the images in 2-D or 3-D space,
    or we can use them to reconstruct the images to remove noise from the original
    versions.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的向量表示有许多潜在用途：例如，它们可以用来可视化图像在 2-D 或 3-D 空间中的分布，或者我们可以使用它们来重建图像，以从原始版本中去除噪声。
- en: Implementing an autoencoder with Keras
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras 实现自动编码器
- en: Let’s walk through an example to show how an autoencoder works. Then we’ll explore
    how to create the search space to tune it. The task we’ll work on here is to learn
    an autoencoder that can compress an image to a low-dimensional vector and then
    reconstruct the image.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来展示自动编码器是如何工作的。然后我们将探讨如何创建搜索空间来调整它。在这里，我们将要完成的工作是学习一个能够将图像压缩到低维向量并随后重建图像的自动编码器。
- en: We’ll use a benchmark deep learning dataset called Fashion MNIST, which consists
    of 60,000 images in the training dataset and 10,000 images in the testing dataset.
    All the images are grayscale, of size 28×28, and the contents of the images are
    all items of clothing. We can load the data via the Keras dataset API and normalize
    the pixel values of the images to the range 0 to 1 for ease of network optimization,
    as shown in the following listing.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为 Fashion MNIST 的基准深度学习数据集，该数据集包含训练数据集中的 60,000 张图像和测试数据集中的 10,000 张图像。所有图像都是灰度的，大小为
    28×28，图像内容都是服装。我们可以通过 Keras 数据集 API 加载数据，并将图像的像素值归一化到 0 到 1 的范围，以便于网络优化，如下面的列表所示。
- en: Listing 6.13 Loading the Fashion MNIST data
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.13 加载 Fashion MNIST 数据
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Normalizes the values in the images to the range [0, 1]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像中的值归一化到 [0, 1] 范围内
- en: You should already know how to implement the encoder and decoder separately,
    because they’re both conventional networks (CNNs or MLPs). The question is how
    to combine the two networks and compile them for the training and prediction.
    A common way to implement such a composite model is to override the tf.keras.Model
    class. When we subclass the Model class, we should define all the autoencoder
    layers in the __init__() function so that they can be created when an autoencoder
    instance is initialized. The next step is to implement the forward pass in a function
    named call. This function will take a batch of input images and output the reconstructed
    images. We often want to call the encoder individually to extract the representations
    it provides, so it’s a good idea to implement the forward passes of the encoder
    and decoder networks separately, in two methods. This is also the reason we want
    to implement the autoencoder by subclassing the Model class rather than grouping
    the layers in a tf.keras.Model or tf.keras.Sequential object.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经知道如何单独实现编码器和解码器，因为它们都是常规网络（CNN 或 MLP）。问题是如何将两个网络组合起来并编译以进行训练和预测。实现此类复合模型的一种常见方法是重写
    tf.keras.Model 类。当我们子类化 Model 类时，我们应该在 __init__() 函数中定义所有自动编码器层，以便在初始化自动编码器实例时创建它们。下一步是实现名为
    call 的函数中的正向传播。这个函数将接受一批输入图像并输出重构的图像。我们通常希望单独调用编码器以提取它提供的表示，因此单独实现编码器和解码器网络的前向传播是一个好主意。这也是我们为什么要通过子类化
    Model 类来实现自动编码器，而不是将层分组在 tf.keras.Model 或 tf.keras.Sequential 对象中的原因。
- en: Listing 6.14 shows the implementation of the autoencoder we use in this example.
    Both the encoder and decoder networks are single-layer MLP networks with the number
    of units specified by a customizable hyperparameter, latent_dim. The latent_dim
    hyperparameter also indicates the length of the encoded representation vector.
    During the forward pass, each image will be flattened into a vector of length
    784 and then encoded into a representation vector of length latent_dim. The decoder
    network will decode the representation to a vector of size 784 and reshape the
    image to the original size (28×28).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.14 展示了我们在本例中使用的自动编码器的实现。编码器和解码器网络都是单层 MLP 网络，其单元数量由可自定义的超参数 latent_dim
    指定。latent_dim 超参数还表示编码表示向量的长度。在正向传播过程中，每个图像将被展平成一个长度为 784 的向量，然后编码成一个长度为 latent_dim
    的表示向量。解码器网络将表示解码为一个大小为 784 的向量，并将图像重塑为原始大小（28×28）。
- en: Listing 6.14 Subclassing the Keras Model class to implement an autoencoder
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.14 通过子类化 Keras Model 类实现自动编码器
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Overrides the Model class
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重写 Model 类
- en: ❷ The initializer should create all the layer instances.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化器应创建所有层实例。
- en: ❸ The encoding layer should output the representation vector.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编码层应输出表示向量。
- en: ❹ The 28×28 images are flattened into a vector of length 784.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将 28×28 图像展平成一个长度为 784 的向量。
- en: ❺ Encodes the images with a fully connected layer
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用全连接层对图像进行编码
- en: ❻ Decodes the representation vector with a fully connected layer
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用全连接层解码表示向量
- en: ❼ Reshapes the images back to 28×28
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将图像重塑回 28×28
- en: ❽ The call function defines that the neural network should first encode the
    input and then decode it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ call 函数定义了神经网络应首先编码输入，然后解码它。
- en: Next, we’ll create an autoencoder model to encode the images to vectors of length
    64\. The model-compiling, -training, and -fitting process is the same as that
    for regression with an MLP. The only difference is that we use the images as both
    features and responses, as you can see here.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个自动编码器模型，将图像编码为长度为 64 的向量。模型编译、训练和拟合过程与 MLP 的回归相同。唯一的区别是我们使用图像作为特征和响应，如这里所示。
- en: Listing 6.15 Fitting the autoencoder model with Fashion MNIST
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.15 使用 Fashion MNIST 对自动编码器模型进行拟合
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Fixes the TensorFlow random seed
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置 TensorFlow 随机种子
- en: ❷ Fixes the NumPy random seed
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置 NumPy 随机种子
- en: ❸ Creates the autoencoder
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建自动编码器
- en: ❹ Compiles and fits the autoencoder—images serve as both features and target
    responses.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 编译和拟合自动编码器——图像既作为特征也作为目标响应。
- en: ❺ Evaluates the autoencoder on the test images
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在测试图像上评估自动编码器
- en: After the autoencoder is trained, we can use it to encode images as vectors
    by calling the encode() function we’ve implemented. As you can see in the following
    listing, the encoded vector of a test image is of length 64, as expected.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器训练完成后，我们可以通过调用我们实现的 encode() 函数来使用它将图像编码为向量。如以下列表所示，测试图像的编码向量长度为 64，正如预期的那样。
- en: Listing 6.16 Encoding the images with the autoencoder
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.16 使用自动编码器编码图像
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can also visualize the original and reconstructed images with the following
    code.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下代码可视化原始和重建的图像。
- en: Listing 6.17 Visualizing the reconstructed images
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.17 可视化重建的图像
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Gets the encoded and decoded test images—we could also call the call function
    to make it a one-step process.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取编码和解码的测试图像——我们也可以调用 call 函数使其成为一个单步过程。
- en: ❷ Controls the number and size of the displayed images
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 控制显示图像的数量和大小
- en: ❸ Displays the first 10 original images in the first row
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在第一行显示前 10 个原始图像
- en: ❹ Displays the first 10 reconstructed images in the second row
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在第二行显示前 10 个重建图像
- en: As figure 6.2 shows, the reconstructed images look very close to the original
    images, except for some minor blurring and a few missing details (e.g., the patterns
    on the shirts look blurry). This means that the 64-dimensional representations
    preserve almost all the information we need to reconstruct the original images.
    We can use this autoencoder to compress a large image dataset to save the memory.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 6.2 所示，重建的图像与原始图像非常接近，除了某些轻微的模糊和一些缺失的细节（例如，衬衫上的图案看起来模糊）。这意味着 64 维度的表示几乎保留了我们需要重建原始图像的所有信息。我们可以使用这个自动编码器来压缩大型图像数据集以节省内存。
- en: '![06-02](../Images/06-02.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![06-02](../Images/06-02.png)'
- en: Figure 6.2 Autoencoder reconstructed images
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 自动编码器重建的图像
- en: The next step is to create a search space to fine-tune the autoencoder with
    the help of KerasTuner.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个搜索空间，在 KerasTuner 的帮助下微调自动编码器。
- en: 6.2 Tuning the autoencoder model
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 调整自动编码器模型
- en: You already know how to define the search space of an MLP in a layerwise fashion
    by creating a model-building function. Because the autoencoder model is created
    in a class and the layers are initialized in the __init__() function of the class,
    we can directly set the search space for these layers when initializing them.
    To create the search space, we pass an instance of the hp container into the __init__()
    function. Its methods can be used to define the search space. For example, we
    can use hp.Int() to define a space for selecting the number of layers in the encoder
    (or decoder) and use hp.Choice() to tune how many units are in each layer, as
    shown in the next listing. Because the number of layers is undetermined before
    we create the autoencoder, we should loop through all the layers when implementing
    the forward pass through the encoding and decoding networks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道如何通过创建一个模型构建函数以分层方式定义 MLP 的搜索空间。因为自动编码器模型是在类中创建的，并且层是在类的 __init__() 函数中初始化的，所以我们可以直接在初始化时设置这些层的搜索空间。为了创建搜索空间，我们将
    hp 容器的一个实例传递给 __init__() 函数。它的方法可以用来定义搜索空间。例如，我们可以使用 hp.Int() 来定义编码器（或解码器）中层的数量选择空间，并使用
    hp.Choice() 来调整每层中的单元数量，如以下列表所示。因为在我们创建自动编码器之前层的数量是不确定的，所以在实现编码和解码网络的正向传递时，我们应该遍历所有层。
- en: Listing 6.18 Building a class to define the search space of the autoencoder
    model
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.18 建立一个类来定义自动编码器模型的搜索空间
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Adds hp to the initializer’s arguments
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 hp 添加到初始化器的参数中
- en: ❷ A list of encoding layers
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码层的列表
- en: ❸ Uses hp to decide on the number of encoding layers
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 hp 来决定编码层的数量
- en: ❹ Uses hp to choose the number of units for each encoding layer
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 hp 来选择每个编码层的单元数量
- en: ❺ A list of decoding layers
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 解码层的列表
- en: ❻ Uses hp to decide on the number of decoding layers
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用 hp 来决定解码层的数量
- en: ❼ Uses hp to choose the number of units for each decoding layer
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用 hp 来选择每个解码层的单元数量
- en: ❽ Takes a forward loop through the encoding layers
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 通过编码层进行正向循环
- en: ❾ Takes a forward loop through the decoding layers
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 通过解码层进行正向循环
- en: To follow the best practice for implementing Keras models, again, we should
    initialize all the layers in the __init__() function and use them in the call()
    function. With the hp container, we can get the values of all the hyperparameters
    needed to build the model and record them.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵循实现 Keras 模型的最佳实践，我们再次应该在 __init__() 函数中初始化所有层，并在 call() 函数中使用它们。使用 hp 容器，我们可以获取构建模型所需的所有超参数的值并将它们记录下来。
- en: The next steps follow the normal usage of KerasTuner. In listing 6.19, we create
    a build_model() function to return the autoencoder model we defined and then feed
    it into an initialized tuner to proceed with the search process. It’s worth noting
    that the initialization of the autoencoder requires an extra input (an hp container
    instance). We can also tune the optimization function jointly with the same hp
    container, as we did in the MLP example.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步遵循KerasTuner的正常使用方法。在列表6.19中，我们创建了一个build_model()函数来返回我们定义的自动编码器模型，并将其输入到一个初始化的调优器中，以继续搜索过程。值得注意的是，自动编码器的初始化需要一个额外的输入（一个hp容器实例）。我们还可以与相同的hp容器一起调整优化函数，就像我们在MLP示例中所做的那样。
- en: Listing 6.19 Running the search for the autoencoder
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.19 运行自动编码器的搜索
- en: '[PRE19]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Initializes the model and passes in the hp instance
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化模型并传入hp实例
- en: ❷ Compiles the model
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编译模型
- en: ❸ Returns the model
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回模型
- en: ❹ Clears the working directory before starting to remove any previous results
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在开始之前清除工作目录以移除任何先前结果
- en: ❺ The working directory
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 工作目录
- en: ❻ validation_data is required for evaluating the model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 验证数据是评估模型所必需的。
- en: The tuner conducts 10 trials, each of which trains an autoencoder for 10 epochs.
    We can select the best model and visualize the first 10 reconstructed images with
    the following code.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 调优器执行10次试验，每次训练一个自动编码器10个epoch。我们可以选择最佳模型，并使用以下代码可视化前10个重建图像。
- en: Listing 6.20 Evaluating the results
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.20 评估结果
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By tuning the hyperparameters in the autoencoder, we’ve made some of the reconstructed
    images perceptibly clearer, like the flip-flop shown in the ninth picture (see
    figure 6.3).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整自动编码器中的超参数，我们已经使一些重建图像在视觉上更加清晰，例如第九张图片中显示的翻转（见图6.3）。
- en: '![06-03](../Images/06-03.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![06-03](../Images/06-03.png)'
- en: Figure 6.3 The reconstructed images from the tuned autoencoder
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 调优后的自动编码器重建图像
- en: 6.3 Tuning shallow models with different search methods
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 使用不同搜索方法调整浅层模型
- en: 'In chapter 2, you learned how to tune a traditional (or shallow) ML model with
    grid search. Appendix B also covers several more complex examples of tuning multiple
    components in an ML pipeline. All of the examples leverage the built-in tuner
    (GridSearchCV) in the scikit-learn library. Now that you’ve seen the power of
    KerasTuner for selecting and tuning deep learning models, you may be wondering
    whether you can also tune shallow models with KerasTuner. Compared to tuning using
    GridSearchCV, doing this with KerasTuner has the following two key benefits:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，你学习了如何使用网格搜索调整传统（或浅层）机器学习模型。附录B还涵盖了在机器学习管道中调整多个组件的几个更复杂的示例。所有这些示例都利用了scikit-learn库中内置的调优器（GridSearchCV）。现在你已经看到了KerasTuner在选择和调整深度学习模型方面的强大功能，你可能想知道是否也可以使用KerasTuner调整浅层模型。与使用GridSearchCV相比，使用KerasTuner进行此操作有以下两个关键优势：
- en: It gives you a more straightforward way to perform model selection without tuning
    each model separately and comparing them manually. With the help of the conditional
    scope defined by the hyperparameter class, you can select between different shallow
    models the same way you did with deep learning models. We’ll introduce how to
    do this in the next example.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为你提供了一个更直接的方式来执行模型选择，无需分别调整每个模型并手动比较。借助由超参数类定义的条件范围，你可以以与深度学习模型相同的方式选择不同的浅层模型。我们将在下一个示例中介绍如何做到这一点。
- en: It gives you more search methods to select from. KerasTuner contains several
    advanced search methods. Selecting different ones can result in different tuning
    results, as you’ll see in the next example.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为你提供了更多可供选择的搜索方法。KerasTuner包含几种高级搜索方法。选择不同的方法可能会导致不同的调优结果，正如你将在下一个示例中看到的那样。
- en: In fact, it is also feasible to select from a pool of both deep learning models
    and shallow models. However, because deep learning models are often created and
    trained in a different way than shallow models (partly because of implementation
    differences in the various libraries, considering their diverse model architectures),
    training and evaluating them during the search process requires different treatment.
    You’ll learn how to create your own tuner with a personalized training and evaluation
    strategy to accommodate a broader search space in the next section. For now, we’ll
    start with an example that shows how we can select between different shallow models.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，从深度学习模型和浅层模型池中选择也是可行的。然而，由于深度学习模型通常以与浅层模型不同的方式创建和训练（部分原因是由于各种库在实现上的差异，考虑到它们不同的模型架构），在搜索过程中对它们的训练和评估需要不同的处理。您将在下一节中学习如何创建自己的调谐器，并使用个性化的训练和评估策略来适应更广泛的搜索空间。现在，我们将从一个示例开始，展示我们如何在不同浅层模型之间进行选择。
- en: 6.3.1 Selecting and tuning shallow models
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 选择和调整浅层模型
- en: In this section, we’ll work on an image classification problem, using the digits
    dataset that comes with the scikit-learn library. The dataset contains 1,797 8×8
    grayscale images of handwritten digits, from 0 to 9\. We can load the dataset
    with the built-in function load_digits() in the scikit-learn library and split
    out 20% of it to use as the test set, as shown in the following listing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理一个图像分类问题，使用scikit-learn库附带的手写数字数据集。该数据集包含1797个0到9的手写数字的8×8灰度图像。我们可以使用scikit-learn库中的内置函数load_digits()加载该数据集，并将其20%的数据分割出来作为测试集，如下所示。
- en: Listing 6.21 Loading the digits data
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.21 加载数字数据
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Loads the digits dataset
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数字数据集
- en: ❷ Separately stores the images and corresponding target digits
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分别存储图像及其相应的目标数字
- en: ❸ Reshapes images to vectors
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将图像重塑为向量
- en: ❹ Splits out 20% of the data to use as the final test set
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将20%的数据分割出来作为最终的测试集
- en: 'In appendix B, we introduce how to create an SVM model to classify images and
    tune its hyperparameters with grid search. You may also want to try out some different
    shallow classification models, such as decision trees, and select the best one
    among them. Similar to the selection of deep learning models, introduced in section
    6.1.2, you can use KerasTuner to select among different shallow models by setting
    a conditional hyperparameter for the model type. In listing 6.22, we create a
    search space to select between two models: SVM and random forest (see appendix
    B if you’re not familiar with the random forest model). The model selection is
    done by a hyperparameter named ''model_type''. In each trial of the search process,
    by choosing a specific ''model_type'' such as svm, narrow the search space down
    to the conditional scope of the selected model and create the corresponding model.
    The model selection can be conducted jointly with the hyperparameter tuning of
    each model.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录B中，我们介绍了如何创建一个SVM模型来分类图像，并使用网格搜索调整其超参数。您还可以尝试一些不同的浅层分类模型，如决策树，并从中选择最佳模型。类似于在第6.1.2节中介绍的深度学习模型的选择，您可以使用KerasTuner通过为模型类型设置条件超参数来在不同浅层模型之间进行选择。在列表6.22中，我们创建了一个搜索空间，用于在SVM和随机森林（如果您不熟悉随机森林模型，请参阅附录B）之间进行选择。模型选择是通过一个名为'model_type'的超参数来完成的。在搜索过程的每次试验中，通过选择特定的'model_type'，例如svm，将搜索空间缩小到所选模型的条件范围，并创建相应的模型。模型选择可以与每个模型的超参数调整一起进行。
- en: Listing 6.22 Creating a search space for shallow model selection
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.22 创建浅层模型选择的搜索空间
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Selects the classifier type
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择分类器类型
- en: ❷ Tunes the SVM classifier, if selected
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果选择，调整SVM分类器
- en: ❸ Tunes the random forest classifier, if selected
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果选择，调整随机森林分类器
- en: As we did in the previous section, we use random search for model selection
    and hyperparameter tuning. However, an important difference here is that we do
    not directly use the RandomSearch class but instead use a tuner class named SklearnTuner
    specifically designed for tuning scikit-learn models. The reason is that a tuner
    in KerasTuner controls the training and evaluation of the instantiated models
    during the search process. Because the scikit-learn models are trained and tested
    differently than the deep learning models implemented with TensorFlow Keras, we
    can use different tuners to accommodate the differences in model training. We
    will introduce how to build a general tuner capable of dealing with both cases
    in the next section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中所做的那样，我们使用随机搜索进行模型选择和超参数调优。然而，这里的一个重要区别是，我们并没有直接使用RandomSearch类，而是使用了一个名为SklearnTuner的特定调优器类，该类专门用于调优scikit-learn模型。原因是KerasTuner中的调优器在搜索过程中控制实例化模型的训练和评估。由于scikit-learn模型的训练和测试方式与使用TensorFlow
    Keras实现的深度学习模型不同，我们可以使用不同的调优器来适应模型训练中的差异。我们将在下一节中介绍如何构建一个通用的调优器，它能够处理这两种情况。
- en: Despite the differences in model training and evaluation, the method for selecting
    the hyperparameters is applicable for all the cases. It’s called an *oracle* in
    KerasTuner. It will decide on the hyperparameters to try out in each trial and
    take the evaluation results of the previously selected hyperparameters as inputs
    for its update if needed (see figure 6.4). Because the tuner touches only the
    hyperparameters and evaluation results, the differences in the evaluation process
    for different models (deep or shallow) do not affect it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型训练和评估存在差异，但选择超参数的方法适用于所有情况。在KerasTuner中，这被称为*Oracle*。它将决定在每个试验中尝试的超参数，并在需要时将先前选择超参数的评估结果作为其更新的输入（见图6.4）。由于调优器仅触及超参数和评估结果，不同模型（深度或浅层）的评估过程差异不会影响它。
- en: '![06-04](../Images/06-04.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![06-04](../Images/06-04.png)'
- en: Figure 6.4 The structure of a tuner and its oracle in KerasTuner
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 KerasTuner中调优器及其Oracle的结构
- en: To use the random search method to tune the models created with the scikit-learn
    library, we can create a SklearnTuner and set its oracle (search method) to RandomSearch
    (see listing 6.23). We can set the number of trials we want to perform during
    the search process with max_trials and set the objective of the search method,
    which is used to compare different models, using kt.Objective. The score objective
    we use here represents the evaluation accuracy score for each model. The max argument
    means the larger the score is, the better the model is. If the objective were
    the MSE, we would instead use 'min', because in that case, smaller scores are
    better. The evaluation strategy of the tuner, which is defined by the KFold module
    in scikit-learn, is set to three-fold evaluation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用随机搜索方法来调优使用scikit-learn库创建的模型，我们可以创建一个SklearnTuner，并将其Oracle（搜索方法）设置为RandomSearch（参见列表6.23）。我们可以设置在搜索过程中想要执行的最大试验次数，并使用kt.Objective设置搜索方法的目标准则，该准则用于比较不同的模型。我们在这里使用的评分目标代表每个模型的评估准确度分数。max参数意味着分数越大，模型越好。如果目标是MSE，我们将使用'min'，因为在那种情况下，分数越小越好。调优器的评估策略由scikit-learn中的KFold模块定义，设置为三折评估。
- en: Listing 6.23 Using random search to select and tune scikit-learn models
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.23 使用随机搜索选择和调优scikit-learn模型
- en: '[PRE23]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Creates a tuner for tuning scikit-learn models
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个用于调优scikit-learn模型的调优器
- en: ❷ Selects the random search method and specifies its arguments
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择随机搜索方法并指定其参数
- en: ❸ Passes the search space to the tuner
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将搜索空间传递给调优器
- en: ❹ Uses three-fold cross-validation to evaluate each model
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用三折交叉验证评估每个模型
- en: ❺ Overwrites the previous project if one existed
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果存在，则覆盖先前的项目
- en: ❻ Names this tuning project 'random_tuner'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将此调优项目命名为 'random_tuner'
- en: ❼ Conducts the search process by feeding training data to the tuner
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 通过向调优器提供训练数据来执行搜索过程
- en: Now let’s retrieve the best discovered model and evaluate it on the test data,
    as shown here.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们获取最佳发现的模型，并在测试数据上对其进行评估，如下所示。
- en: Listing 6.24 Viewing the results of using random search
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.24 查看使用随机搜索的结果
- en: '[PRE24]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Displays the random search results
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 显示随机搜索结果
- en: ❷ Retrieves the best model
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取最佳模型
- en: ❸ Retrains the best model on the whole training dataset
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在整个训练数据集上重新训练最佳模型
- en: ❹ Evaluates the best discovered model
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 评估最佳发现的模型
- en: The best model found in 30 trials is an SVM model using the RBF kernel with
    a regularization parameter of *C* = 2.24\. By retraining the best model on the
    whole training set, we arrive at a final testing accuracy of 95.83%.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在30次试验中找到的最佳模型是一个使用RBF核和正则化参数*C* = 2.24的SVM模型。通过在全部训练集上重新训练最佳模型，我们达到了最终的测试准确率95.83%。
- en: 6.3.2 Tuning a shallow model pipeline
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 调整浅层模型管道
- en: 'You may have multiple components in a pipeline and want to select and tune
    them jointly. For example, suppose you want to create a pipeline with two components:
    a PCA component to reduce the images’ dimensions and an SVM classifier to classify
    the preprocessed images. This can be done by stacking the components to form a
    sequential scikit-learn pipeline (see appendix B). You can then select a model
    and tune the other component in the pipeline at the same time, as shown in listing
    6.25.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能在管道中有多组件，并希望联合选择和调整它们。例如，假设您想创建一个包含两个组件的管道：一个用于降低图像维度的PCA组件和一个用于分类预处理图像的SVM分类器。这可以通过堆叠组件形成顺序的scikit-learn管道来完成（参见附录B）。然后您可以选择一个模型，同时调整管道中的其他组件，如下列6.25所示。
- en: Listing 6.25 Selecting and tuning a scikit-learn pipeline
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.25 选择和调整scikit-learn管道
- en: '[PRE25]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Selects the hyperparameters of PCA
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择PCA的超参数
- en: ❷ Selects the model type
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择模型类型
- en: ❸ Instantiates a scikit-learn pipeline with the selected hyperparameters
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用选定的超参数实例化scikit-learn管道
- en: ❹ Searches with the random search method
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用随机搜索方法进行搜索
- en: We evaluate the pipelines and retrieve the best one the same way we did in listing
    6.24, so we won’t elaborate on that further here.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与列表6.24相同的方式评估管道并检索最佳管道，因此我们在此不再详细说明。
- en: 6.3.3 Trying out different search methods
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 尝试不同的搜索方法
- en: As mentioned at the beginning of this section, a key benefit of using KerasTuner
    for tuning is that it enables you to easily switch between (or implement) different
    search methods for tuning shallow models. This can be done by changing the oracle
    to the one you prefer. For example, we can change the random search method to
    some more advanced methods, such as the *Bayesian optimization method*, as shown
    in the following listing. If you’re not familiar with this method, don’t worry
    about it for now; we’ll talk more about it in chapter 7.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节开头所述，使用KerasTuner进行调优的一个关键好处是，它使您能够轻松地在（或实现）不同的搜索方法之间切换。这可以通过更改到您偏好的oracle来实现。例如，我们可以将随机搜索方法更改为一些更高级的方法，例如*贝叶斯优化方法*，如下面的列表所示。如果您对这个方法不熟悉，现在不用担心；我们将在第7章中更多地讨论它。
- en: Listing 6.26 Using Bayesian optimization to tune scikit-learn models
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.26 使用贝叶斯优化调整scikit-learn模型
- en: '[PRE26]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Sets the oracle to BayesianOptimization
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将oracle设置为BayesianOptimization
- en: ❷ Searches with the Bayesian optimization method
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用贝叶斯优化方法进行搜索
- en: Different search methods often fit different search spaces. For example, the
    Bayesian optimization method is often more suitable for searching hyperparameters
    with continuous values. In practice, you can try out different methods and select
    the best discovered model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的搜索方法通常适合不同的搜索空间。例如，贝叶斯优化方法通常更适合搜索具有连续值的超参数。在实践中，您可以尝试不同的方法，并选择最佳发现的模型。
- en: 6.3.4 Automated feature engineering
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 自动特征工程
- en: In this section, we will introduce how to do automated feature engineering.
    Before introducing automated feature engineering, we will first see what feature
    engineering is. It is an important step in machine learning, which may boost the
    model’s performance, and it is especially effective for structured data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何进行自动特征工程。在介绍自动特征工程之前，我们首先将了解什么是特征工程。它是机器学习中的一个重要步骤，可能会提高模型的表现，并且对于结构化数据特别有效。
- en: For example, we may have a structured dataset, which is a table consisting of
    several columns as features and one column as the prediction target. Before directly
    inputting these features into a machine learning model, we could perform feature
    engineering, which consists of *feature generation* and *feature selection*. We
    could create more feature columns based on existing feature columns, which is
    called feature generation. We could also delete some unuseful features, which
    is called feature selection.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能有一个结构化数据集，它是一个包含多个列作为特征和一列作为预测目标的表格。在直接将这些特征输入到机器学习模型之前，我们可以进行特征工程，这包括*特征生成*和*特征选择*。我们可以根据现有的特征列创建更多的特征列，这被称为特征生成。我们也可以删除一些无用的特征，这被称为特征选择。
- en: To show how feature engineering exactly works, we again use the Titanic dataset,
    which we used in chapter 4\. The features of the dataset are the profiles of the
    passengers of the *Titanic*, and the prediction target is whether the passenger
    survived the accident. Download the dataset using the following code.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示特征工程是如何工作的，我们再次使用第4章中使用的泰坦尼克号数据集。数据集的特征是泰坦尼克号乘客的档案，预测目标是乘客是否在事故中幸存。使用以下代码下载数据集。
- en: Listing 6.27 Downloading the Titanic dataset
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.27 下载泰坦尼克号数据集
- en: '[PRE27]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: After downloading the CSV files, we can load them into pandas DataFrames with
    the read_csv() function as shown in the next code listing. We will pop out the
    target column from the DataFrame to use it separately.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 下载CSV文件后，我们可以使用read_csv()函数将它们加载到pandas DataFrame中，如下所示。我们将从DataFrame中弹出目标列以单独使用。
- en: Listing 6.28 Loading the downloaded CSV files with Pandas
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.28 使用Pandas加载下载的CSV文件
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Loads the CSV file into a pandas DataFrame
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将CSV文件加载到pandas DataFrame中
- en: ❷ Pops the target column as the y_train
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 弹出目标列作为y_train
- en: ❸ Converts the popped column from Series to DataFrame
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将弹出的列从Series转换为DataFrame
- en: ❹ Prints the first few lines of the data
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印数据的前几行
- en: The printed content of the training data is shown in figure 6.5.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的打印内容如图6.5所示。
- en: '![06-05](../Images/06-05.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![06-05](../Images/06-05.png)'
- en: Figure 6.5 The first few lines of the Titanic dataset
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 泰坦尼克号数据集的前几行
- en: As you can see, some of the features are categorical, and others are numerical.
    We will need to put them into different groups and use different encoding methods
    for them.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，一些特征是分类的，而其他特征是数值的。我们需要将它们放入不同的组，并为它们使用不同的编码方法。
- en: We set the age and fare they paid as numerical data. We will replace the missing
    values or NaN values with their median values. Then, we will normalize them to
    the range of 0 to 1.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将年龄和乘客支付的船票费用设置为数值数据。我们将用它们的中位数替换缺失值或NaN值。然后，我们将它们归一化到0到1的范围内。
- en: We put the number of siblings and spouses and the classes of the passengers
    as categorical features. Because they don’t have many different categories, we
    will use one-hot encoding to encode them. One-hot encoding is usually not used
    for categorical features with too many different values because it would create
    too many columns after the encoding.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将兄弟姐妹和配偶的数量以及乘客的等级作为分类特征。因为它们没有很多不同的类别，我们将使用独热编码来编码它们。独热编码通常不用于具有太多不同值的分类特征，因为这会在编码后创建太多的列。
- en: The rest of the features, like the gender of the passenger and whether they
    are on the deck, are also categorical features, but we will use ordinal encoding
    for them, which encodes the different string values to different integer values.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的特征，如乘客的性别和是否在甲板上，也是分类特征，但我们将对它们使用序数编码，将不同的字符串值编码为不同的整数值。
- en: For these two types of categorical features, before encoding, we will also need
    to replace the missing values with a constant value. We just use the string of
    “None” for convenience.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种类型的分类特征，在编码之前，我们还需要用常数值替换缺失值。我们只是为了方便使用“None”字符串。
- en: Here we use sklearn.pipeline.Pipeline to build a pipeline for each type of the
    columns for these transformations, the code of which is shown next.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用sklearn.pipeline.Pipeline为每种类型的列构建一个用于这些转换的管道，其代码如下所示。
- en: Listing 6.29 Building pipelines for cleaning and encoding the columns
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.29 构建用于清理和编码列的管道
- en: '[PRE29]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ The lists of names of the different types of columns
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不同类型列的名称列表
- en: ❷ The pipeline for numerical columns
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数字列的管道
- en: ❸ Replaces the missing values with the median
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用中位数替换缺失值
- en: ❹ Scales the values to the range of 0 to 1
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将值缩放到0到1的范围内
- en: ❺ The pipeline for one-hot encoding columns
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 独热编码列的管道
- en: ❻ Replaces the missing values with 'None'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 用'None'替换缺失值
- en: ❼ One-hot encodes the column. With handle_unknow='ignore', it would not raise
    an error during inference if it encountered any unknown values, whose encoding
    would be all zeros.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对列进行独热编码。使用handle_unknow='ignore'时，在推理过程中如果遇到任何未知值，它不会引发错误，其编码将为全零。
- en: ❽ The pipeline for ordinal encoding columns
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 原序编码列的管道
- en: ❾ Replaces the missing values with 'None'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 用'None'替换缺失值
- en: ❿ Encodes the values to integers. For unknown values, it will use -1.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 将值编码为整数。对于未知值，它将使用-1。
- en: So far, we have finished cleaning the data. After these pipelines, all the columns
    are numerical, either float or integer. The next step is feature generation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完成了数据的清理。在这些管道之后，所有列都是数值型的，无论是浮点数还是整数。下一步是特征生成。
- en: The first feature generation technique we will introduce is combining different
    categorical columns. You can think of it as simply concatenating the strings of
    two selected categorical columns. The concatenated string is the value for the
    new column generated. This technique may help the machine learning model discover
    some correlations between the two selected columns.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的第一种特征生成技术是组合不同的分类列。你可以将其视为简单地连接两个选定分类列的字符串。连接的字符串是新列的值。这种技术可能有助于机器学习模型发现两个选定列之间的某些相关性。
- en: 'For example, for the table shown in listing 6.30, the first column of the table
    contains only the values of A and B, whereas the second column of the table contains
    only values of 0 and 1. A new column generated using the technique described earlier
    would contain four different values: A0, A1, B0, B1. We can use an ordinal encoder
    to encode them to 0, 1, 2, 3.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于列表 6.30 中显示的表格，表格的第一列只包含 A 和 B 的值，而表格的第二列只包含 0 和 1 的值。使用前面描述的技术生成的新列将包含四个不同的值：A0、A1、B0、B1。我们可以使用序数编码器将它们编码为
    0、1、2、3。
- en: Listing 6.30 Generating a new column by combining existing columns
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.30 通过组合现有列生成新列
- en: '[PRE30]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We implemented this technique as a SklearnTransformer, which can be part of
    a pipeline, as shown in listing 6.31\. The fit() function should learn the information
    from the training data for the feature generation. Our fit() function generates
    the new column and fits the OrdinalEncoder. The transform() function should transform
    the data and return the transformed data. Our transform() function generates the
    new column and encodes it with the OrdinalEncoder.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此技术实现为一个 SklearnTransformer，它可以作为管道的一部分，如列表 6.31 所示。fit() 函数应该从训练数据中学习特征生成的信息。我们的
    fit() 函数生成新列并拟合 OrdinalEncoder。transform() 函数应该转换数据并返回转换后的数据。我们的 transform() 函数生成新列并用
    OrdinalEncoder 进行编码。
- en: Listing 6.31 Combining categorical features to generate a new feature
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.31 将分类特征组合以生成新特征
- en: '[PRE31]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Extends the two classes as required to implement a SklearnTransformer
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 按要求扩展两个类以实现 SklearnTransformer
- en: ❷ The initializer takes two column names.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化器接受两个列名。
- en: ❸ Prepares an OrdinalEncoder to encode the newly generated feature
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 准备一个 OrdinalEncoder 对新生成的特征进行编码
- en: ❹ Concatenates the columns to build a new column
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 将列连接起来构建新列
- en: ❺ Fits the encoder with the new column
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新列拟合编码器
- en: ❻ Concatenates the columns to build a new column
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 将列连接起来构建新列
- en: ❼ Uses the encoder to encode the new column
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用编码器对新的列进行编码
- en: With this CategoricalCombination, we can now easily generate a new column that
    combines the categorical data of two existing columns, as shown in listing 6.32.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个 CategoricalCombination，我们现在可以轻松地生成一个新列，该列结合了两个现有列的分类数据，如列表 6.32 所示。
- en: Listing 6.32 Generating a new feature with CategoricalCombination
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.32 使用 CategoricalCombination 生成新特征
- en: '[PRE32]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Prints the original columns
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 打印原始列
- en: ❷ Initializes the transformer
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化转换器
- en: ❸ Prints the newly generated column
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 打印新生成的列
- en: As shown in the outputs, the data we use and the values in the newly generated
    column are the same as what we used in the previous example.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，我们使用的数据和新生成列中的值与之前示例中使用的相同。
- en: The next technique to generate a new feature is using a numerical feature and
    a categorical feature to generate a new numerical feature. For example, given
    the table shown in listing 6.33, we first will need to divide the rows of the
    data into different groups. In this example, the data is divided into three groups,
    (A 1, A 1), (B 1, B 0), and (C 1, C -1). The rows in the same group have the same
    values for the first column. Second, we need to calculate the mean value of the
    numerical values in different groups. In this example, the three values we will
    get are 1 for group A, 0.5 for group B, and 0 for group C. The last step is to
    generate a new column identical to the categorical column and replace the categorical
    values with the corresponding mean value. It is like using the mean values as
    the encoding for the categorical values. A is replaced by 0.5, B is replaced by
    0.5, and C is replaced by 0. You can also see it as using the numerical feature
    to encode the categorical feature.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 生成新特征的下一个技术是使用数值特征和分类特征来生成一个新的数值特征。例如，给定列表 6.33 中的表格，我们首先需要将数据行分成不同的组。在这个例子中，数据被分成了三个组：(A
    1, A 1)，(B 1, B 0)，和 (C 1, C -1)。同一组中的行在第一列具有相同的值。其次，我们需要计算不同组中数值的平均值。在这个例子中，我们将得到三个值：组
    A 的值为 1，组 B 的值为 0.5，组 C 的值为 0。最后一步是生成一个与分类列相同的新列，并用相应的平均值替换分类值。这就像使用平均值作为分类值的编码。A
    被替换为 0.5，B 被替换为 0.5，C 被替换为 0。你也可以将其视为使用数值特征来编码分类特征。
- en: Listing 6.33 Generating a new column using numerical and categorical columns
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.33 使用数值和分类列生成新列
- en: '[PRE33]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: To implement this techinque, we implemented another SklearnTransformer as shown
    in listing 6.34\. The initializer also takes two column names, which are the column
    names of the categorical column and the numerical column. In the fit() function,
    we calculate the mean values of different groups divided according to the different
    categorical values. In the transform() function, we need to replace the categorical
    values with a numerical column and return the value.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一技术，我们实现了一个新的 SklearnTransformer，如列表 6.34 所示。初始化器也接受两个列名，分别是分类列和数值列的列名。在
    fit() 函数中，我们根据不同的分类值计算不同组的平均值。在 transform() 函数中，我们需要用数值列替换分类值并返回该值。
- en: Listing 6.34 Using numerical and categorical features to generate a new feature
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.34 使用数值和分类特征生成新特征
- en: '[PRE34]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Extends the required classes of a Transformer
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 扩展了 Transformer 所需的类
- en: ❷ The initializer takes two column names.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化器接受两个列名。
- en: ❸ The fit function groups the rows by the values in the categorical column and
    calculates the mean value of the numerical values in each group.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ fit 函数根据分类列中的值对行进行分组，并计算每个组中数值的平均值。
- en: ❹ Replaces the categorical values with the mean values
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将分类值替换为平均值
- en: With the MeanEncoder implemented, we can do another quick test to see if it
    works correctly as shown in the following listing. The data we use is the same
    as the example in listing 6.33.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: MeanEncoder 实现，我们可以进行另一个快速测试以查看它是否按预期工作，如下面的列表所示。我们使用的数据与列表 6.33 中的示例相同。
- en: Listing 6.35 Using numerical and categorical features to generate a new feature
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.35 使用数值和分类特征生成新特征
- en: '[PRE35]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Prepares some example data
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备一些示例数据
- en: ❷ Initializes the MeanEncoder
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化 MeanEncoder
- en: ❸ Transforms the data
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 转换数据
- en: As shown in the printed results, the newly generated column is the same as we
    expected in the previous example.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如打印结果所示，新生成的列与之前示例中预期的相同。
- en: Now we have all the modules we need for feature engineering. We will need to
    put them together into a single pipeline, including the feature encoding pipelines
    and the transformers for feature generation. We will use the SklearnColumnTransformer,
    which is just for transforming the features before input into the machine learning
    model. The code is shown in the next code listing. The ColumnTransformer accepts
    the argument of transformers as a list of tuples of three, a string as the name
    of the step, a Transformer or Pipeline instance, and a list of column names of
    the columns that will be used by the transformer.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所有需要的特征工程模块。我们需要将它们组合成一个单一的管道，包括特征编码管道和用于特征生成的转换器。我们将使用 SklearnColumnTransformer，它仅用于在输入机器学习模型之前转换特征。代码在下一个代码列表中显示。ColumnTransformer
    接受一个参数，即转换器的列表，为三个元组的列表，一个字符串作为步骤的名称，一个 Transformer 或 Pipeline 实例，以及一个列表，包含将被转换器使用的列名。
- en: Listing 6.36 Putting the feature encoding and generation together into a pipeline
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.36 将特征编码和生成合并到管道中
- en: '[PRE36]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ The preprocessing steps for different types of columns
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不同类型列的预处理步骤
- en: ❷ Combines the two categorical columns to generate a new column
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将两个分类列合并以生成新列
- en: ❸ Uses the age column to compute the mean values to encode the embark town column
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用年龄列计算平均值以对登船城镇列进行编码
- en: So far, we have done the feature generation part. The next step is the feature
    selection. Feature selection is usually using some metrics to evaluate each of
    the features, selecting the most useful ones for the task, and discarding the
    rest of the columns. A typical metric used for feature selection is called *mutual
    information*, which is an important concept in information theory. It measures
    the dependency between two variables. Each feature can be seen as a variable,
    and the values in the corresponding column can be seen as the samples of the variable.
    If the target column is highly dependent on a feature, the mutual information
    between these two columns would be high, which means it is a good column to keep.
    If two variables are independent of each other, the mutual information would be
    close to zero.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完成了特征生成部分。下一步是特征选择。特征选择通常使用一些指标来评估每个特征，选择对任务最有用的特征，并丢弃其余的列。用于特征选择的典型指标称为*互信息*，它是信息理论中的一个重要概念。它衡量两个变量之间的依赖性。每个特征可以看作是一个变量，相应列中的值可以看作是该变量的样本。如果目标列高度依赖于一个特征，这两个列之间的互信息就会很高，这意味着它是一个很好的保留列。如果两个变量相互独立，互信息就会接近于零。
- en: We implement this feature selection step using sklearn.feature_selection .SelectKBest,
    which can help us select the k best features base on a given metric. For example,
    to select the top eight features that have the highest mutual information with
    the target column, we can use SelectKBest(mutual_info_classif, k=8). It can also
    be one of the steps of a pipeline.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用sklearn.feature_selection.SelectKBest来实现这个特征选择步骤，它可以帮助我们根据给定的指标选择k个最佳特征。例如，为了选择与目标列具有最高互信息的八个顶级特征，我们可以使用SelectKBest(mutual_info_classif,
    k=8)。它也可以是管道步骤之一。
- en: With all these feature preprocessing, feature generation, and feature selection
    steps ready, we build a complete end-to-end pipeline as shown in listing 6.37,
    which uses a support vector machine as the final classification model implemented
    using sklearn.svm.SVC.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些特征预处理、特征生成和特征选择步骤准备就绪后，我们构建了一个完整的端到端管道，如列表6.37所示，它使用支持向量机作为最终分类模型，该模型是通过sklearn.svm.SVC实现的。
- en: Listing 6.37 Building the overall pipeline
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.37 构建整体管道
- en: '[PRE37]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Initializes the final end-to-end pipeline
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化最终的端到端管道
- en: ❷ The preprocessing and feature generation transformer
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预处理和特征生成转换器
- en: ❸ The feature selection step to select the top eight features with the highest
    mutual information with the target column
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 选择与目标列具有最高互信息的八个顶级特征的特征选择步骤
- en: ❹ The support vector machine classification model
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 支持向量机分类模型
- en: ❺ Fits the model using the training data
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用训练数据拟合模型
- en: With the pipeline trained with the training data, we can evaluate it using the
    testing data as shown in the following code.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据训练的管道，我们可以使用以下代码使用测试数据对其进行评估。
- en: Listing 6.38 Evaluating the pipeline using testing data
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.38 使用测试数据评估管道
- en: '[PRE38]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Predicts the testing data targets
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预测测试数据的目标
- en: ❷ Prints the accuracy score
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印准确度分数
- en: It shows the accuracy score as 0.74.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示准确度分数为0.74。
- en: We have shown an example of feature engineering. In this process, we could perform
    many parts differently, for example, the categorical columns we choose to combine,
    the numerical and categorical columns we choose to encode, or the number of columns
    to keep during the feature selection. If we define these decisions as hyperparameters,
    tuning these hyperparameters would be the process of automated feature engineering.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了一个特征工程示例。在这个过程中，我们可以执行许多不同的部分，例如，我们选择的组合分类列，我们选择的编码数值和分类列，或者在特征选择过程中要保留的列数。如果我们将这些决策定义为超参数，调整这些超参数的过程将是自动化特征工程的过程。
- en: We can define the hyperparameters in the following way. First, we generate all
    the possible combinations of two categorical columns and all the possible combinations
    of a numerical column and a categorical column. The code is shown here.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以下方式定义超参数。首先，我们生成两个分类列的所有可能组合以及数值列和分类列的所有可能组合。代码如下所示。
- en: Listing 6.39 Generating all possible combinations of the columns
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.39 生成所有可能的列组合
- en: '[PRE39]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ The list for all possible numerical and categorical column pairs
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所有可能的数值和分类列对的列表
- en: ❷ Iterates through the ordinal encoded categorical columns
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历有序编码的分类列
- en: ❸ Iterates through the numerical columns
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历数值列
- en: ❹ Adds the categorical and numerical column pairs to the list
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将分类和数值列对添加到列表中
- en: ❺ The list for all possible categorical column pairs
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 所有可能的分类列对的列表
- en: ❻ Iterates through all the ordinal encoded categorical columns
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 遍历所有有序编码的分类列
- en: ❼ Iterates through the rest of the ordinal encoded categorical columns
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 遍历剩余的有序编码分类列
- en: ❽ Adds the categorical column pairs to the list
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将分类列对添加到列表中
- en: Second, we will use a Boolean hyperparameter for each pair to control whether
    we generate a new feature using the two columns. The code for defining these hyperparameters
    would look like the following.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将为每一对使用布尔超参数来控制是否使用这两个列生成新特征。定义这些超参数的代码看起来如下所示。
- en: Listing 6.40 Using a Boolean hyperparameter for each of the pairs
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.40 使用每个对的布尔超参数
- en: '[PRE40]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Enumerates all the pairs with their indices
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 枚举所有对及其索引
- en: ❷ Uses a Boolean hyperparameter for each pair to control whether to generate
    the new feature
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用布尔超参数控制每个对是否生成新特征
- en: ❸ Adds the transformer tuple of three to the list, which will be used by a ColumnTransformer
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将三个转换器元组添加到列表中，该列表将被 ColumnTransformer 使用
- en: Now we are ready to put everything together to build the entire search space.
    The code is shown in listing 6.41\. First, as in the feature engineering example
    shown earlier, we build three pipelines to preprocess and encode the three types
    of features. Second, as in listing 6.40, we define the Boolean hyperparameters
    for each pair of categorical features for generating new features. We also do
    the same for each numerical and categorical feature pair. Finally, we define a
    hyperparameter for the number of columns to keep. Then, we put all these steps
    into a single overall pipeline and return it.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备将所有这些组合在一起以构建整个搜索空间。代码在列表 6.41 中展示。首先，正如前面展示的特征工程示例中，我们构建三个管道以预处理和编码三种类型的特征。其次，正如列表
    6.40 中所示，我们为每个分类特征的每一对定义布尔超参数以生成新特征。我们也为每个数值和分类特征对做同样的事情。最后，我们定义一个用于保留列数的超参数。然后，我们将所有这些步骤放入一个单独的整体管道中并返回它。
- en: Listing 6.41 The search space for automated feature engineering
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.41 自动特征工程的搜索空间
- en: '[PRE41]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Preprocessing pipeline for numerical features
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数值特征的预处理管道
- en: ❷ Preprocessing pipeline for categorical features to be one-hot encoded
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对要独热编码的分类特征进行预处理的管道
- en: ❸ Preprocessing pipeline for categorical features to be ordinally encoded
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对要有序编码的分类特征进行预处理的管道
- en: ❹ Puts the preprocessing pipelines into a list to be used by the ColumnTransformer
    later
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将预处理管道放入列表中，稍后由 ColumnTransformer 使用
- en: ❺ Enumerates the categorical feature pairs to define the Boolean hyperparameters
    to generate new features
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 枚举分类特征对以定义布尔超参数以生成新特征
- en: ❻ Enumerates the categorical and numerical feature pairs to define the Boolean
    hyperparameters to generate new features
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 枚举分类和数值特征对以定义布尔超参数以生成新特征
- en: ❼ Initializes the overall pipeline
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 初始化整体管道
- en: ❽ Initializes the ColumnTransformer as the first step of the pipeline to preprocess
    the data and generate new features
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将 ColumnTransformer 作为管道的第一步初始化以预处理数据和生成新特征
- en: ❾ Imputes the data again to avoid any missing values during feature generation
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 再次填充数据以避免在特征生成过程中出现任何缺失值
- en: ❿ Initializes the feature selector for selecting the top k features
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 初始化特征选择器以选择前 k 个特征
- en: ⓫ Uses mutual information as the metric for feature selection
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 使用互信息作为特征选择的度量
- en: ⓬ Defines a hyperparameter for the value of k
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 定义 k 值的超参数
- en: ⓭ Selects five features at least
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 至少选择五个特征
- en: ⓮ Selects all the features at most (13 features after preprocessing and encoding
    plus the newly generated features, which is the same as the number of transformers
    in the transformer list except for the three preprocessing pipelines)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 选择最多所有特征（预处理和编码后的13个特征加上新生成的特征，这与变换器列表中的变换器数量相同，除了三个预处理管道）
- en: To make sure the search space can build a model correctly, we can use the following
    code in listing 6.42 as a quick unit test.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保搜索空间能够正确构建模型，我们可以在列表6.42中使用以下代码作为快速单元测试。
- en: Listing 6.42 A quick unit test for the search space
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.42 搜索空间的快速单元测试
- en: '[PRE42]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Finally, we can start to search for the best model as shown here.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以开始搜索最佳模型，如下所示。
- en: Listing 6.43 Searching for the best automated feature engineering model
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.43 搜索最佳自动特征工程模型
- en: '[PRE43]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Uses the random search algorithm for searching
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用随机搜索算法进行搜索
- en: ❷ Uses accuracy as the metrics
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用准确率作为指标
- en: With automated feature engineering, we achieved a better accuracy score of 0.81\.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动特征工程，我们实现了更好的准确率得分，达到了0.81。
- en: 6.4 Controlling the AutoML process by customizing tuners
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 通过自定义调优器控制AutoML过程
- en: 'In this section, let’s delve into the tuner object and learn how to customize
    it to control the AutoML process and enable the tuning of models implemented with
    different libraries. Controlling the AutoML process means controlling the AutoML
    loop of several steps: instantiating an ML pipeline in each trial based on the
    hyperparameters selected by the search method (oracle), training and evaluating
    the performance of the pipeline, recording the evaluation results, and providing
    the results to update the oracle if needed (see figure 6.6). You’ve seen two types
    of tuners in the previous sections corresponding to the tuning of different models:
    a RandomSearch tuner for tuning the deep learning models (specifically implemented
    with TensorFlow Keras) with the random search method and the SklearnTuner for
    tuning the shallow models implemented with the scikit-learn library, in which
    you can select different search methods. The reason we choose different tuners
    is mainly because of the implementation difference of training and evaluating
    the deep learning and shallow models. This is quite a practical problem for doing
    AutoML because it’s hard to find a package that contains all the possible ML models
    you might want to use. Although KerasTuner provides a universal way of customizing
    the search space by defining the build() function with the help of the hp container,
    training, evaluating, saving, and loading these models may still require different
    treatments. This can be addressed by defining your own tuners.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们深入了解调优器对象，并学习如何自定义它以控制AutoML过程并启用使用不同库实现的模型的调优。控制AutoML过程意味着控制AutoML的几个步骤的循环：在每个试验中根据搜索方法（占卜者）选择的超参数实例化ML管道，训练和评估管道的性能，记录评估结果，并在需要时提供结果以更新占卜者（见图6.6）。您在前几节中已经看到了两种类型的调优器，对应于不同模型的调优：用于调优深度学习模型（特别是使用TensorFlow
    Keras实现）的随机搜索调优器，以及用于调优使用scikit-learn库实现的浅层模型的SklearnTuner，其中您可以选择不同的搜索方法。我们选择不同调优器的主要原因是因为深度学习和浅层模型的训练和评估实现上的差异。这对于进行AutoML来说是一个相当实际的问题，因为很难找到一个包含您可能想要使用的所有可能ML模型的包。尽管KerasTuner通过使用hp容器定义build()函数提供了一种自定义搜索空间的通用方法，但训练、评估、保存和加载这些模型可能仍然需要不同的处理。这可以通过定义自己的调优器来解决。
- en: '![06-06](../Images/06-06.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![06-06](../Images/06-06.png)'
- en: Figure 6.6 The search loop in the AutoML process
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 AutoML过程中的搜索循环
- en: 'In the rest of the section, we introduce how to customize the tuners for tuning
    scikit-learn models and TensorFlow Keras models, respectively, to get you familiar
    with basic tuner design. Following the two examples, you’ll learn how to design
    a tuner for jointly selecting and tuning deep learning and shallow models. You’ll
    also learn to tune models beyond the scikit-learn models and Keras models with
    an additional example: tuning a *gradient-boosted decision tree* (GBDT) model
    implemented with the LightGBM library.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，我们将介绍如何分别自定义调优器以调优scikit-learn模型和TensorFlow Keras模型，以便您熟悉基本调优器设计。在两个示例之后，您将学习如何设计一个调优器以联合选择和调优深度学习和浅层模型。您还将通过一个额外的示例学习如何调优模型，该模型是一个使用LightGBM库实现的*梯度提升决策树*（GBDT）模型。
- en: 6.4.1 Creating a tuner for tuning scikit-learn models
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 创建用于调优scikit-learn模型的调优器
- en: Let’s first learn to design a tuner to tune shallow models implemented with
    the scikit-learn library. We will work on the digit classification problem we
    used in section 6.2.1 and will use the same code for creating the search space
    and conducting the model selection and tuning. The only difference is that we
    customize our tuner rather than using the build-in scikit-learn model tuner (kt.tuners.SklearnTuner).
    The tuner, which we name ShallowTuner, should extend the tuner class (Tuner) in
    KerasTuner.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先学习如何设计一个用于调整使用 scikit-learn 库实现的浅层模型的调谐器。我们将处理在 6.2.1 节中使用的数字分类问题，并将使用相同的代码来创建搜索空间、执行模型选择和调谐。唯一的区别是我们自定义我们的调谐器，而不是使用内置的
    scikit-learn 模型调谐器（kt.tuners.SklearnTuner）。我们命名的调谐器 ShallowTuner 应该扩展 KerasTuner
    中的调谐器类（Tuner）。
- en: Overriding the Tuner class gives us full control over the search space, model
    building, training, saving, and evaluation process. Before getting into a real
    example, let’s first see a barebones example to get an idea of how it works. In
    listing 6.44, we try to find the value of x, which minimizes y=x*x+1, by defining
    x as a hyperparameter. Yes, by subclassing Tuner, or any subclass of Tuner, like
    RandomSearch, you can use KerasTuner as a black-box optimization tool for anything.
    To do this, we only need to override Tuner.run_trial(), define the hyperparameter,
    and return the objective function value. The returned value will be minimized
    by default. Tuner.run_trial() is just running the experiment once and returning
    the evaluation results.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖 Tuner 类让我们完全控制搜索空间、模型构建、训练、保存和评估过程。在进入实际示例之前，我们先来看一个基本示例，以了解其工作原理。在列表 6.44
    中，我们尝试通过将 x 定义为超参数来找到使 y=x*x+1 最小的 x 的值。是的，通过子类化 Tuner 或 Tuner 的任何子类，如 RandomSearch，你可以将
    KerasTuner 作为黑盒优化工具用于任何事物。为此，我们只需要覆盖 Tuner.run_trial()，定义超参数，并返回目标函数值。默认情况下，返回的值将被最小化。Tuner.run_trial()
    只是一次运行实验并返回评估结果。
- en: Listing 6.44 A barebones example for subclassing Tuner
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.44 一个用于子类化 Tuner 的基本示例
- en: '[PRE44]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Extends the RandomSearch class
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 扩展 RandomSearch 类
- en: ❷ Gets the HyperParameters object from trial
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从试验中获取超参数对象
- en: In this example, we did not use hypermodel. We did not specify an objective,
    either. These are all used by Tuner.run_trial(). If you don’t use them, there
    is no need to specify them. For the return value of Tuner.run_trial(), it supports
    different formats. It is more common to use a dictionary, which we will show in
    the next example.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们没有使用超模型。我们也没有指定一个目标。这些都是在 Tuner.run_trial() 中使用的。如果你不使用它们，就没有必要指定它们。对于
    Tuner.run_trial() 的返回值，它支持不同的格式。更常见的是使用字典，我们将在下一个示例中展示。
- en: 'However, when we implement the ShallowTuner, we have more functions to override
    because we want to save the models along the way and load the best model after
    the search. In general, the following five functions need to be implemented for
    customizing a tuner:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们实现 ShallowTuner 时，我们需要覆盖更多函数，因为我们希望在搜索过程中保存模型，并在搜索结束后加载最佳模型。一般来说，为了自定义调谐器，需要实现以下五个函数：
- en: '*An initialization function* (__init__())—Initializes the tuner by providing
    the search method (oracle) and the defined model-building function or class (hypermodel),
    which we’ve learned in the previous sections, to characterize the search space
    and build up the model in each trial. The oracle and hypermodel will be saved
    as the attributes of the tuner (self.oracle and self.hypermodel).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*初始化函数* (`__init__()`) — 通过提供搜索方法（占位符）和定义的模型构建函数或类（超模型），我们在前面的章节中学习到的，来初始化调谐器，以表征搜索空间并在每个试验中构建模型。占位符和超模型将被保存为调谐器的属性（self.oracle
    和 self.hypermodel）。'
- en: '*A search function* (search())—Starts the whole iterative AutoML search process
    when called. In each search iteration, it will first initiatiate an object named
    trial, which stores all the meta information in the current trial such as the
    hyperparameters selected by the search method and the status of the current trial
    to help track whether a trial is started or completed. Then the search function
    will call the core function introduced next to pursue the search process.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索函数* (`search()`) — 当被调用时，启动整个迭代式 AutoML 搜索过程。在每次搜索迭代中，它将首先初始化一个名为 trial
    的对象，该对象存储当前试验中的所有元信息，例如搜索方法选择的超参数和当前试验的状态，以帮助跟踪试验是否已开始或完成。然后搜索函数将调用下一个介绍的核心函数以追求搜索过程。'
- en: '*A core function* (run_trial())—Implements the single search loop we described
    in figure 6.6.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核心函数* (`run_trial()`) — 实现我们在图 6.6 中描述的单个搜索循环。'
- en: '*A save function* (save_model())—Saves the generated models.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*保存功能*（save_model()）—保存生成的模型。'
- en: '*A load function* (load_model())—Loads the models for retraining if needed
    after the search process is done.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加载功能*（load_model()）—在搜索过程完成后，如果需要重新训练，则加载模型。'
- en: The code of our ShallowTuner is shown in listing 6.45\. The initialization function
    and the search function can be ignored here because they only call the corresponding
    functions extended from the Tuner and do not have any specialized operations.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们ShallowTuner的代码显示在列表6.45中。初始化函数和搜索函数在此可以忽略，因为它们仅调用从Tuner扩展的相应函数，并且没有任何专门的操作。
- en: Listing 6.45 Customizing a tuner for tuning scikit-learn models
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.45 定制用于调整scikit-learn模型的tuner
- en: '[PRE45]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Initializes the tuner
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化tuner
- en: ❷ Performs the AutoML search process
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行AutoML搜索过程
- en: ❸ Builds, trains, and evaluates the model in the current trial
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在当前试验中构建、训练和评估模型
- en: ❹ Saves the model to disk
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将模型保存到磁盘
- en: ❺ Returns the evaluation result
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回评估结果
- en: ❻ Model-saving function with pickle package
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用pickle包的模型保存函数
- en: ❼ Model-loading function with pickle package
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用pickle包的模型加载函数
- en: Because the AutoML process is a loop process, the core function (run_trial())
    in the custom tuner is called repeatedly in the search function (implemented in
    the based Tuner). Its inputs contain the data for training and evaluating the
    models instantiated in it (X for training features, y for training responses,
    validation_data for testing data). A trial object contains all the hyperparameters
    returned by the oracle in the current trial and some metadata to help summarize
    the results, such as a randomly generated ID of this trial (trial.trial_id).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AutoML过程是一个循环过程，自定义tuner中的核心函数（run_trial()）在搜索函数（在基于Tuner的实现中）中被反复调用。其输入包含用于训练和评估在它中实例化的模型的训练和评估数据（X为训练特征，y为训练响应，validation_data为测试数据）。一个trial对象包含当前试验中占卜者返回的所有超参数以及一些帮助总结结果的元数据，例如随机生成的此试验的ID（trial.trial_id）。
- en: Digging into the run_trial() function, we can see that it first builds up the
    model based on the hyperparameters selected by the oracle. trial.hyperparameters
    is a hyperparameter container that helps create the current model. Then, the model
    is trained and evaluated. The model.score() function adopts the default evaluation
    criterion of the scikit-learn model. You can also implement your own evaluation
    method here, such as cross-validation. In this case, the validation data can also
    be removed from the arguments because the cross-validation will automatically
    split part of the training data (X and y) as validation data. The evaluation result
    (a dictionary with the metric names as the keys) is returned to update the oracle.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究run_trial()函数，我们可以看到它首先基于由占卜者选择的超参数构建模型。trial.hyperparameters是一个超参数容器，有助于创建当前模型。然后，模型被训练和评估。model.score()函数采用scikit-learn模型的默认评估标准。您也可以在此处实现自己的评估方法，例如交叉验证。在这种情况下，验证数据也可以从参数中移除，因为交叉验证将自动将部分训练数据（X和y）作为验证数据分割。评估结果（以度量名称为键的字典）返回以更新占卜者。
- en: The model is saved to disk for future usage by calling the save_model() function.
    This process strictly follows the search loop described in figure 6.6\. To help
    save and load the models discovered in the search process, we also need to implement
    the save_model() and load_model() functions. The save_model() function, called
    in the run_trial() function, takes the unique ID of a trial and the trained scikit-learn
    model as inputs and saves the model using the pickle package. The load_model()
    function is used after the search process is finished. It helps retrieve the best
    model in the disk. It takes a trial object as input (containing all the meta information
    of this trial such as the trial ID, hyperparameters, model accuracy) and returns
    the trained model selected in the corresponding trial.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用save_model()函数将模型保存到磁盘，以便将来使用。此过程严格遵循图6.6中描述的搜索循环。为了帮助保存和加载搜索过程中发现的模型，我们还需要实现save_model()和load_model()函数。在run_trial()函数中调用的save_model()函数以试验的唯一ID和训练的scikit-learn模型作为输入，并使用pickle包保存模型。load_model()函数在搜索过程完成后使用。它有助于从磁盘检索最佳模型。它以包含此试验的所有元信息（如试验ID、超参数、模型准确度）的试验对象作为输入，并返回在相应试验中选择的训练模型。
- en: We follow the same process introduced in section 6.2.1 to load the data and
    create the search space. Notably, we do an extra split on the training data to
    get the validation dataset because our custom tuner requires the input of validation
    data. We didn’t do this for the build-in scikit-learn tuner because it implements
    the cross-validation in the run_trial() function to evaluate each selected model.
    The search space here is still for joint model selection and hyperparameter tuning
    of the SVM model and random forest model. Code for conducting this AutoML task
    is shown in the following listing.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循第6.2.1节中介绍的相同过程来加载数据和创建搜索空间。值得注意的是，我们在训练数据上进行了额外的拆分以获得验证数据集，因为我们的自定义调谐器需要验证数据的输入。我们没有为内置的scikit-learn调谐器这样做，因为它在run_trial()函数中实现了交叉验证来评估每个选定的模型。这里的搜索空间仍然是用于SVM模型和随机森林模型的联合模型选择和超参数调整。执行此AutoML任务的代码如下所示。
- en: Listing 6.46 Tuning scikit-learn models for digits classification
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.46 调整scikit-learn模型以进行数字分类
- en: '[PRE46]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ Loads the digits dataset
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数字数据集
- en: ❷ Splits the dataset into training, validation, and test sets
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据集分为训练集、验证集和测试集
- en: ❸ Creates a search space for model selection and hyperparameter tuning
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为模型选择和超参数调整创建搜索空间
- en: ❹ Initializes the custom tuner
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化自定义调谐器
- en: ❺ Conducts the search process by feeding the training and validation datasets
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过提供训练集和验证集来执行搜索过程
- en: You can also use the custom tuner to tune the scikit-learn pipelines, like we
    did in section 6.2.2\. We won’t further elaborate on it here and leave it for
    you as an exercise.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用自定义调谐器来调整scikit-learn管道，就像我们在第6.2.2节中所做的那样。我们在这里不会进一步详细说明，将其留作您的练习。
- en: 6.4.2 Creating a tuner for tuning Keras models
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 创建用于调整Keras模型的调谐器
- en: 'In the second example, let’s create a custom tuner for tuning the deep learning
    models implemented with TensorFlow Keras. We’ve used a built-in tuner for tuning
    Keras models: the RandomSearch tuner, which is hardcoded with the random search
    method (oracle is the random search oracle and is not changeable). Now we create
    a custom tuner that can select between different search methods. Following the
    same steps we performed in the previous example, we create a tuner called DeepTuner
    by extending the base tuner class. As we’ve mentioned before, if the initialization
    function and the search function do not have any specialized operations compared
    to the base tuner ones, we can ignore them. Thus, we implement only three functions
    for the DeepTuner here: the run_trial(),save_model(), and load_model() functions.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，让我们创建一个自定义调谐器来调整使用TensorFlow Keras实现的深度学习模型。我们使用了一个内置的调谐器来调整Keras模型：随机搜索调谐器，它是硬编码的随机搜索方法（oracle是随机搜索oracle，不可更改）。现在我们创建一个可以选择不同搜索方法的自定义调谐器。遵循我们在上一个示例中执行的相同步骤，我们通过扩展基本调谐器类创建了一个名为DeepTuner的调谐器。正如我们之前提到的，如果初始化函数和搜索函数与基本调谐器相比没有任何专门的操作，我们可以忽略它们。因此，我们在这里只为DeepTuner实现了三个函数：run_trial()、save_model()和load_model()函数。
- en: Compared to the previous example, the major differences are how we evaluated
    the models in the run_trial() function and how we save and load these Keras models
    (see listing 6.47). Zooming into the run_trial() function, we can still build
    up the Keras model by calling the build() function of the hypermodel, the same
    as we did in the previous example. Then we call the fit() function of the instantiated
    Keras model to train it. Notably, training the deep learning models may require
    extra hyperparameters such as the batch size and number of epochs to help control
    the optimization algorithm. We can pass these hyperparameters in the **fit_kwargs
    arguments (as we will see later when calling the search() function of the tuner).
    Or, to be more automated, set the search space of it using the hyperparameter
    container (as we do for the batch_size in listing 6.47) to tune it along with
    other hyperparameters. After the model is trained, we can evaluate it with the
    validation data and use the evaluation results to update the oracle. Specifically,
    you may question how to update the oracle when we have multiple evaluation metrics.
    For example, by default, the evaluate() function of Keras will return the evaluation
    loss value and the classification accuracy in this example (assume that we’ve
    already created a search space of models for classifying the digits). A straightforward
    way of addressing this is to save the evaluation on all the metrics in a dictionary
    with the metric names as the keys and feed all of them to the oracle. When initializing
    the tuner, we can inform the tuner which specific metric we want to use to compare
    the models and update the oracle. As TensorFlow Keras provides methods for saving
    and loading the models, we can adopt these methods to implement the save_model()
    and load_model() functions.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个示例相比，主要区别在于我们在run_trial()函数中如何评估模型以及如何保存和加载这些Keras模型（参见列表6.47）。聚焦到run_trial()函数，我们仍然可以通过调用超模型的build()函数来构建Keras模型，就像我们在前一个示例中所做的那样。然后我们调用实例化Keras模型的fit()函数来训练它。值得注意的是，训练深度学习模型可能需要额外的超参数，如批大小和epoch数量，以帮助控制优化算法。我们可以通过**fit_kwargs参数传递这些超参数（正如我们将在调用调优器的search()函数时看到的那样）。或者，为了更自动化，可以使用超参数容器设置其搜索空间（就像我们在列表6.47中对batch_size所做的那样）以与其他超参数一起调整。模型训练完成后，我们可以使用验证数据对其进行评估，并使用评估结果来更新算子。具体来说，你可能想知道当我们有多个评估指标时如何更新算子。例如，默认情况下，Keras的evaluate()函数将返回评估损失值和分类准确率（在这个例子中，假设我们已经创建了一个用于分类数字的模型搜索空间）。解决这个问题的直接方法是将所有指标保存在一个字典中，其中指标名称作为键，并将所有这些指标都提供给算子。在初始化调优器时，我们可以通知调优器我们想要使用哪个特定指标来比较模型并更新算子。由于TensorFlow
    Keras提供了保存和加载模型的方法，我们可以采用这些方法来实现save_model()和load_model()函数。
- en: Listing 6.47 Customizing a tuner for tuning Keras models
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.47 为调整Keras模型自定义调优器
- en: '[PRE47]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ❶ Trains model with a tunable batch size
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用可调整的批大小训练模型
- en: ❷ Returns the evaluation results
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回评估结果
- en: Next, we use the custom tuner to tune the MLPs for the digit classification.
    As shown in listing 6.48, we create a search space of MLPs in a build_model()
    function and use it to initialize a DeepTuner object. As we can see, we select
    the oracle ourselves, which the RandomSearch tuner cannot do. The objective is
    specified as the classification accuracy, which will be the metric to compare
    the models and used in the run_trial() function to update the oracle. By calling
    the search function, we can execute the search process. The number of epochs will
    be passed from the search function to the run_trial() function (via the **fit_kwargs)
    to control the training epochs of each selected MLP.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用自定义调优器来调整用于数字分类的MLP。如列表6.48所示，我们在build_model()函数中创建一个MLP的搜索空间，并使用它来初始化一个DeepTuner对象。正如我们所见，我们自行选择了一个算子，这是随机搜索调优器无法做到的。目标被指定为分类准确率，这将是比较模型和用于在run_trial()函数中更新算子的指标。通过调用搜索函数，我们可以执行搜索过程。从搜索函数传递到run_trial()函数（通过**fit_kwargs）的epoch数量将用于控制每个选定的MLP的训练epoch数。
- en: Listing 6.48 Tuning MLPs with the custom tuner for digits classification
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.48 使用自定义调优器对数字分类的MLP进行调优
- en: '[PRE48]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ Creates a search space for tuning MLPs
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为调整MLP创建搜索空间
- en: ❷ Uses the classification accuracy as the objective for model comparison and
    oracle update
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用分类准确率作为模型比较和算子更新的目标
- en: ❸ Executes the search process
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行搜索过程
- en: From the two examples of tuner design, we can find that as long as we know how
    to train, evaluate, save, and load the models implemented with any ML library,
    we should be able to write a tuner that can control, select, and tune these models.
    By customizing the tuner, we can fully control the AutoML process and enable a
    broader search space, such as tuning the batch size and tuning models from different
    libraries. In the next two examples, we will further show the benefit of tuner
    design on enlarging the search space for tuning models in different libraries.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 从两个调优器设计的例子中，我们可以发现，只要我们知道如何训练、评估、保存和加载任何机器学习库实现的模型，我们就应该能够编写一个调优器来控制、选择和调整这些模型。通过定制调优器，我们可以完全控制自动化机器学习过程，并允许更广泛的搜索空间，例如调整批量大小和调整来自不同库的模型。在接下来的两个例子中，我们将进一步展示调优器设计在扩大不同库中调整模型搜索空间方面的好处。
- en: 6.4.3 Jointly tuning and selection among deep learning and shallow models
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 深度学习和浅层模型之间的联合调整和选择
- en: Although deep learning models have shown their prominence in many ML tasks recently,
    they’re not universally optimal solutions. In many cases, we don’t know in advance
    whether using deep learning models would beat the shallow models, especially when
    the dataset is small. You may face this situation at some point. The next example
    will show how to do joint model selection and tuning among the shallow models
    and deep learning models. We combine the search space used in the previous two
    examples into a unified search space. The search space contains three types of
    model structures—SVM, random forest, and MLP—each with their specified space of
    hyperparameters. To further clarify the search space hierarchy, we set up the
    conditional scope for the hyperparameters under each type of model as shown in
    the following listing.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习模型最近在许多机器学习任务中已经显示出其突出地位，但它们并不是普遍最优的解决方案。在许多情况下，我们事先不知道使用深度学习模型是否会优于浅层模型，尤其是在数据集较小的情况下。你可能会在某一点上遇到这种情况。下一个例子将展示如何在浅层模型和深度学习模型之间进行联合模型选择和调整。我们将前两个例子中使用的搜索空间合并为一个统一的搜索空间。该搜索空间包含三种类型的模型结构——支持向量机（SVM）、随机森林和多层感知器（MLP），每种都有其指定的超参数空间。为了进一步阐明搜索空间层次结构，我们为每种类型模型下的超参数设置了条件范围，如下所示。
- en: Listing 6.49 Creating a search space both deep and shallow models
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.49 创建既深又浅的模型搜索空间
- en: '[PRE49]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Selects whether to use shallow models or MLP
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择是否使用浅层模型或MLP
- en: ❷ Sets up the conditional hyperparameter scope for each type of model
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为每种类型的模型设置条件超参数范围
- en: Now it’s time to create the tuner. Following the previous two examples, a feasible
    idea is to merge the two tuners into one. Because the creation of deep learning
    models and shallow models is the same (by calling the build() function with the
    hyperparameters in each trial), we can set a model discriminator to judge whether
    a model created in each trial is a shallow model or deep model. Whenever we got
    a deep learning model (Keras model), we train, evaluate, and save it in the DeepTuner.
    On the contrary, if a shallow model (scikit-learn) is created, we’ll follow the
    steps implemented in the ShallowTuner. We implement the tuner in listing 6.50.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是创建调优器的时候了。遵循前两个例子，一个可行的方法是将两个调优器合并为一个。因为深度学习模型和浅层模型的创建方式相同（通过在每个试验中调用带有超参数的build()函数），我们可以设置一个模型判别器来判断在每个试验中创建的模型是浅层模型还是深度模型。每当得到深度学习模型（Keras模型）时，我们就在DeepTuner中进行训练、评估和保存。相反，如果创建的是浅层模型（scikit-learn），我们将遵循ShallowTuner中实现的步骤。我们在列表6.50中实现了调优器。
- en: As you can see, the run_trial(), save_model(), and load_model() functions all
    can decide whether or not the model is a Keras model. To ensure the oracle will
    receive the same type of evaluation for different models, we keep the classification
    accuracy of only the deep learning models. A tricky point here is that different
    models are saved and loaded in different ways. During training, we can directly
    save a model with its tailored saving method based on the model type. However,
    when loading a model, we do not have the model in advance to select the corresponding
    loading method. To address this problem, we predefine an attribute in the initialization
    function (trial_ id_to_type) to record the model type in each trial. It is a dictionary
    to map the trial ID to the corresponding model type (Keras or scikit-learn), so
    when loading the model, we can select the corresponding loading method based on
    the trial ID.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，run_trial()、save_model() 和 load_model() 函数都可以决定模型是否为 Keras 模型。为确保预言机对不同模型进行相同类型的评估，我们仅保留深度学习模型的分类准确率。这里的一个棘手点是，不同的模型以不同的方式保存和加载。在训练期间，我们可以直接使用基于模型类型的定制保存方法保存一个模型。然而，在加载模型时，我们没有模型来预先选择相应的加载方法。为了解决这个问题，我们在初始化函数（trial_id_to_type）中预先定义了一个属性来记录每个试验的模型类型。它是一个将试验
    ID 映射到相应模型类型（Keras 或 scikit-learn）的字典，因此当加载模型时，我们可以根据试验 ID 选择相应的加载方法。
- en: Listing 6.50 Customizing a tuner for tuning both deep and shallow models
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.50 自定义调优器以调优深度和浅层模型
- en: '[PRE50]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ Adds an attribute to record the type of model selected in each trial
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为每个试验添加一个属性以记录所选模型的类型
- en: ❷ Checks the model type for model training
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查模型训练的模型类型
- en: ❸ Retrieves the accuracy of only the Keras model
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 仅检索 Keras 模型的准确率
- en: ❹ Records the model type
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 记录模型类型
- en: ❺ Checks the model type of model loading
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 检查模型加载的模型类型
- en: The rest of the work is to instantiate the tuner and use it to explore the mixed
    search space of deep and shallow models (see listing 6.51). We select the random
    search method here and set the objective for comparing the models as accuracy,
    which aligns the one we specified in the run_trial() function of the tuner. We
    search for 30 trials, and the best model explored so far is an SVM classifier.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作就是实例化调优器，并使用它来探索深度和浅层模型的混合搜索空间（参见列表 6.51）。我们在这里选择随机搜索方法，并将比较模型的目标设置为准确率，这与我们在调优器的
    run_trial() 函数中指定的目标一致。我们进行 30 次试验，迄今为止探索的最佳模型是一个 SVM 分类器。
- en: Listing 6.51 Exploring the mixed search space with the custom tuner
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.51 使用自定义调优器探索混合搜索空间
- en: '[PRE51]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ❶ Sets the objective as the classification accuracy
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将目标设置为分类准确率
- en: ❷ Retrieves the best model
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检索最佳模型
- en: In the last example, we will work on tuning the models that are not implemented
    with TensorFlow Keras and scikit-learn APIs. This can help you generalize the
    AutoML techniques you’ve learned to a broader range of models and libraries you
    may want to use in practice.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个示例中，我们将对未使用 TensorFlow Keras 和 scikit-learn API 实现的模型进行调优。这可以帮助你将所学的 AutoML
    技术推广到更广泛的模型和库中，这些模型和库可能是你在实践中想要使用的。
- en: 6.4.4 Hyperparameter tuning beyond Keras and scikit-learn models
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 超参数调优超出 Keras 和 scikit-learn 模型
- en: 'As an example, we use the LightGBM library ([https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)),
    which is a gradient-boosting framework for tree-based learning algorithms. It
    contains several representative tree-based learning algorithms such as the GBDT
    algorithm and the random forest algorithm, which you may have seen before (see
    appendix B for more details on these algorithms). Learning how to tune these algorithms
    requires us to know how to apply them in advance. Specifically, you need to know
    how to instantiate the algorithm and use it to train a model. You should also
    know how to evaluate the learned model, save it, and load it back to make predictions
    when needed. We work on the California housing price-prediction task here and
    train a regression model with the GBDT algorithm (specified by the boosting_type
    argument) as shown in listing 6.52\. The algorithm will create and add decision
    trees to the final GBDT model sequentially. We train a GBDT model with an ensemble
    of at most 10 trees and 31 leaves in each tree. The learning rate here is a weighting
    factor for the corrections of the predictions given by new trees when added to
    the ensemble model. The best iteration here means the number of trees in the ensemble
    that achieves the best performance. We refer you to check the details of using
    this library from the official website: [http://mng.bz/q2jJ](http://mng.bz/q2jJ).
    We evaluate the trained model, save it, and reload it to check whether the model
    is saved correctly.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们使用 LightGBM 库（[https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)），这是一个基于树的梯度提升框架。它包含几个代表性的基于树的机器学习算法，如
    GBDT 算法和随机森林算法，你可能之前已经见过（更多关于这些算法的细节请见附录 B）。学习如何调整这些算法需要我们事先了解如何应用它们。具体来说，你需要知道如何实例化算法并使用它来训练模型。你还应该知道如何评估学习到的模型、保存它，并在需要时将其加载回来进行预测。在这里，我们针对加利福尼亚房价预测任务进行工作，并使用
    GBDT 算法（通过 boosting_type 参数指定）训练了一个回归模型，如列表 6.52 所示。该算法将按顺序创建并添加决策树到最终的 GBDT 模型中。我们训练了一个包含最多
    10 棵树和每棵树 31 个叶子的 GBDT 模型。这里的学习率是当新树添加到集成模型中时对预测校正的加权因子。这里的最佳迭代次数表示达到最佳性能的集成模型中的树的数量。我们建议您查阅官方网站以获取此库的详细使用信息：[http://mng.bz/q2jJ](http://mng.bz/q2jJ)。我们评估了训练好的模型，保存了它，并重新加载以检查模型是否正确保存。
- en: Listing 6.52 Applying the GBDT model in the LightGBM library for regression
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.52 在 LightGBM 库中应用 GBDT 模型进行回归
- en: '[PRE52]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: ❶ Installs and imports the LightGBM package
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 安装并导入 LightGBM 包
- en: ❷ Creates and fits a GBDT model
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建并拟合 GBDT 模型
- en: ❸ Evaluates the learned GBDT model
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 评估学习到的 GBDT 模型
- en: ❹ Saves, loads, and reevaluates the model
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 保存、加载和重新评估模型
- en: 'As shown next, the MSE of the GBDT model is around 0.75, and the model-saving
    and -loading methods could save the learned model in a .txt file successfully:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，GBDT 模型的均方误差（MSE）约为 0.75，模型保存和加载方法能够成功地将学习到的模型保存到 .txt 文件中：
- en: '[PRE53]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In listing 6.53, we try to tune the GBDT algorithm implemented with LightGBM
    with the AutoML technique we learned in the previous sections. Suppose we want
    to tune the maximum number of trees, the maximum number of leaves in each tree,
    and the learning rate of the GBDT algorithm. Because we know how to instantiate
    a single GBDT regressor, we can leverage the hp container and specify the search
    space of these concerned hyperparameters in a model-building function, the same
    as we’ve done for tuning the scikit-learn and Keras models.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.53 中，我们尝试使用之前章节中学到的 AutoML 技术调整 LightGBM 中实现的 GBDT 算法。假设我们想要调整最大树的数量、每棵树的最大叶子数以及
    GBDT 算法的学习率。因为我们知道如何实例化单个 GBDT 回归器，我们可以利用 hp 容器并在模型构建函数中指定这些相关超参数的搜索空间，就像我们为调整
    scikit-learn 和 Keras 模型所做的那样。
- en: Listing 6.53 Creating a search space for selecting and tuning LightGBM models
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.53 为选择和调整 LightGBM 模型创建搜索空间
- en: '[PRE54]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: ❶ Characterizes the search space of the concerned hyperparameters
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 描述相关超参数的搜索空间
- en: Following the method for customizing the tuner introduced in the previous examples,
    we create a LightGBMTuner, extending the base tuner in KerasTuner, and implement
    the three core functions for executing a search trial, saving the trained model,
    and loading the model from disk, respectively. From the code shown in listing
    6.54, we can see that except for several AutoML steps, including the model building
    with the selected hyperparameters and the oracle update, the code combining the
    three functions is the same as the code we implement a single GBDT algorithm with
    LightGBM for conducting the regression task.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前例中介绍的自定义调谐器的方法，我们创建了一个 LightGBMTuner，它扩展了 KerasTuner 的基本调谐器，并实现了执行搜索试验、保存训练模型和从磁盘加载模型这三个核心功能。从列表
    6.54 中的代码可以看出，除了包括使用所选超参数构建模型和或acle 更新在内的几个 AutoML 步骤外，结合这三个功能的代码与我们在 LightGBM
    中实现单个 GBDT 算法以进行回归任务的代码相同。
- en: Listing 6.54 Customizing a tuner for tuning LightGBM models
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.54 自定义用于调整 LightGBM 模型的调谐器
- en: '[PRE55]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: ❶ Builds and fits a GBDT model
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建并拟合一个 GBDT 模型
- en: ❷ Evaluates the learned GBDT model
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 评估学习到的 GBDT 模型
- en: ❸ Returns the model evaluation MSE
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回模型评估的均方误差 (MSE)
- en: ❹ Saves the learned GBDT model to disk
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将学习到的 GBDT 模型保存到磁盘
- en: We use the created model-building function and the custom tuner to tune the
    three hyperparameters in the GBDT algorithm implemented with the LightGBM library.
    By exploring the search space for 10 trials, the test MSE of the best-discovered
    model is largely reduced compared to the one from our initial model, which demonstrates
    the effectiveness of our tuning strategy (see listing 6.55). The hyperparameters
    of the best model can be printed out by calling the result_summary() function
    of the tuner, which is inherited from the base tuner.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用创建的模型构建函数和自定义调谐器来调整使用 LightGBM 库实现的 GBDT 算法中的三个超参数。通过进行 10 次试验来探索搜索空间，与初始模型相比，最佳发现的模型的测试
    MSE 大幅降低，这证明了我们的调整策略的有效性（见列表 6.55）。最佳模型的超参数可以通过调用从基本调谐器继承的 result_summary() 函数打印出来。
- en: Listing 6.55 Executing the hyperparameter tuning and evaluating the best model
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.55 执行超参数调整并评估最佳模型
- en: '[PRE56]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: ❶ Prints the hyperparameter and evaluation information of the best trial
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印最佳试验的超参数和评估信息
- en: You’ve now learned how to design the tuner to extend the search space to a broader
    range of models implemented with different libraries. Before ending this chapter,
    we want to point out several notes.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经学会了如何设计调谐器以扩展搜索空间到使用不同库实现的更广泛模型范围。在结束本章之前，我们想指出几个注意事项。
- en: Note
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Adopting KerasTuner requires you to know the implementation of creating the
    model. You also have to know how to train, evaluate, save, and load the model
    if you want to customize a tuner for tuning models not implemented with Keras
    or the scikit-learn library.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用 KerasTuner 需要知道模型的实现方式。如果您想为不使用 Keras 或 scikit-learn 库实现的模型自定义调谐器，您还必须知道如何训练、评估、保存和加载模型。
- en: We separate the model instantiation part in the build_model() function and the
    model-training part in the tuners. This separation is doable because of the support
    for the APIs of Keras and scikit-learn. However, some libraries may not allow
    the separation of model instantiation and training. In other words, you have to
    instantiate the model with the hyperparameters and fit the model with the training
    data in one sentence, such as the default training API of LightGBM ([http://mng.bz/7Wve](http://mng.bz/7Wve)).
    To accommodate this case, a straightforward way is to use the build_model() function
    to return the hyperparameters rather than the create model, and use these hyperparameters
    to instantiate the model in the run_trial() function of the tuner simultaneously,
    with the model training leveraging the API of the library covering the model you
    want to tune.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将 build_model() 函数中的模型实例化部分和调谐器中的模型训练部分分开。这种分离是可行的，因为得到了 Keras 和 scikit-learn
    API 的支持。然而，某些库可能不允许模型实例化和训练的分离。换句话说，您必须在一句话中用超参数实例化模型，并用训练数据拟合模型，例如 LightGBM 的默认训练
    API ([http://mng.bz/7Wve](http://mng.bz/7Wve))。为了适应这种情况，一种简单的方法是使用 build_model()
    函数返回超参数，而不是创建模型，并在调谐器的 run_trial() 函数中同时使用这些超参数实例化模型，利用覆盖您想要调整的模型的库的 API 进行模型训练。
- en: KerasTuner requires us to define the performance evaluation of each selected
    model in the tuner. Some AutoML toolkits define the model-training and -evaluation
    (objective) function outside the tuner and feed it into the tuner during the search
    process, such as Ray Tune ([https://docs.ray.io/en/latest/ tune/index.html](https://docs.ray.io/en/latest/tune/index.html))
    or Hyperopt ([https://github.com/hyperopt/hyperopt](https://github.com/hyperopt/hyperopt)).
    The general APIs are quite similar to the KerasTuner implementation, and you can
    learn more details from their official websites.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KerasTuner要求我们定义调优器中每个所选模型的性能评估。一些AutoML工具包在调优器外部定义模型训练和评估（目标）函数，并在搜索过程中将其输入到调优器中，例如Ray
    Tune ([https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html))
    或 Hyperopt ([https://github.com/hyperopt/hyperopt](https://github.com/hyperopt/hyperopt))。它们的通用API与KerasTuner实现相当相似，你可以从它们的官方网站了解更多详细信息。
- en: Though we often have to design a tailored tuner for tuning the models implemented
    with different libraries, the oracle is often universal and can be used in different
    tuners because the inputs and outputs of the oracle are always the numerical representations
    of the hyperparameters extracted in the hp container. We will introduce more details
    in the next chapter.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我们通常需要为使用不同库实现的模型设计定制的调优器，但算子通常是通用的，可以在不同的调优器中使用，因为算子的输入和输出总是从hp容器中提取的超参数的数值表示。我们将在下一章中介绍更多细节。
- en: Summary
  id: totrans-482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: To achieve extra flexibility of search space design for tuning and selecting
    deep learning models, you can create the entire search space in one build() function
    in a layerwise fashion. This way of model building is similar to creating a Keras
    model, except you should change the relevant hyperparameters to a space of feasible
    values.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了在调优和选择深度学习模型时获得额外的搜索空间设计灵活性，你可以通过分层的方式在一个`build()`函数中创建整个搜索空间。这种模型构建方式类似于创建Keras模型，除了你应该将相关超参数更改为可行值的空间。
- en: You can create the search space for tuning shallow models in the same way as
    deep learning models with KerasTuner. Multiple preprocessing methods and shallow
    models can be jointly selected and tuned by creating a scikit-learn pipeline.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用与深度学习模型相同的方式，在KerasTuner中为浅层模型创建搜索空间。可以通过创建scikit-learn管道来联合选择和调优多个预处理方法和浅层模型。
- en: A tuner contains a search method and organizes the training and evaluation of
    the selected pipelines during the search process. Different search methods can
    be chosen by changing the oracle in KerasTuner.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优器包含一个搜索方法，并在搜索过程中组织所选管道的训练和评估。可以通过更改KerasTuner中的算子来选择不同的搜索方法。
- en: For models with different training and evaluation strategies or implemented
    with different libraries, you may have to select a tuner with a suitable training
    and evaluation strategy or customize your own tuner.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有不同训练和评估策略或使用不同库实现的模型，你可能需要选择一个具有合适训练和评估策略的调优器，或者自定义自己的调优器。
- en: 'A typical case for customizing a tuner requires you to specify three functions:
    a run_trial() function to process a single AutoML loop (including the model instantiation,
    training, evaluation, and saving; and the oracle update), a model-saving function,
    and a model-loading function.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制调优器的典型情况需要你指定三个函数：一个`run_trial()`函数用于处理单个AutoML循环（包括模型实例化、训练、评估、保存以及算子更新），一个模型保存函数和一个模型加载函数。
