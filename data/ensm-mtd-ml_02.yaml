- en: '1 Ensemble methods: Hype or hallelujah?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 集成方法：炒作还是赞歌？
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Defining and framing the ensemble learning problem
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义和构建集成学习问题
- en: Motivating the need for ensembles in different applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同应用中阐述对集成方法的需求
- en: Understanding how ensembles handle fit versus complexity
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解集成方法如何处理拟合与复杂度
- en: Implementing our first ensemble with ensemble diversity and model aggregation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现我们的第一个集成：集成多样性和模型聚合
- en: In October 2006, Netflix announced a $1 million prize for the team that could
    improve movie recommendations by 10% via Netflix’s own proprietary recommendation
    system, CineMatch. The Netflix Grand Prize was one of the first-ever open data
    science competitions and attracted tens of thousands of teams.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年10月，Netflix宣布了一项100万美元的奖金，奖励能够通过Netflix自有的专有推荐系统CineMatch将电影推荐准确率提高10%的团队。Netflix大奖赛是历史上第一个开放数据科学竞赛，吸引了成千上万的团队。
- en: The training set consisted of 100 million ratings that 480,000 users had given
    to 17,000 movies. Within three weeks, 40 teams had already beaten CineMatch’s
    results. By September 2007, more than 40,000 teams had entered the contest, and
    a team from AT&T Labs took the 2007 Progress Prize by improving upon CineMatch
    by 8.42%.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集由480,000名用户对17,000部电影给出的1亿条评分组成。在短短三周内，已有40个团队击败了CineMatch的结果。到2007年9月，已有超过40,000个团队参加了比赛，AT&T实验室的团队通过将CineMatch的准确率提高了8.42%，赢得了2007年的进步奖。
- en: As the competition progressed with the 10% mark remaining elusive, a curious
    phenomenon emerged among the competitors. Teams began to collaborate and share
    knowledge about effective feature engineering, algorithms, and techniques. Inevitably,
    they began combining their models, blending individual approaches into powerful
    and sophisticated ensembles of many models. These ensembles combined the best
    of various diverse models and features, and they proved to be far more effective
    than any individual model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着竞赛的进行，10%的目标仍然遥不可及，竞争者中出现了一种奇特的现象。团队开始合作，分享关于有效特征工程、算法和技术方面的知识。不可避免地，他们开始结合自己的模型，将个别方法融合成许多模型的强大而复杂的集成。这些集成结合了各种不同模型和特征的最佳之处，并且证明比任何单个模型都更有效。
- en: In June 2009, nearly two years after the contest began, BellKor’s Pragmatic
    Chaos, a merger of three different teams, edged out another merged team, The Ensemble
    (which was a merger of more than 30 teams!), to improve on the baseline by 10%
    and take the $1 million prize. Just “edged out” is a bit of an understatement
    as BellKor’s Pragmatic Chaos managed to submit their final models barely 20 minutes
    before The Ensemble got their models in ([http://mng.bz/K08O](https://shortener.manning.com/K08O)).
    In the end, both teams achieved a final performance improvement of 10.06%.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在比赛开始近两年后的2009年6月，BellKor的Pragmatic Chaos团队（由三个不同的团队合并而成）在另一个合并团队The Ensemble（由超过30个团队合并而成）之前，通过将基准提高了10%，赢得了100万美元的奖金。“勉强领先”这个说法有点低估了BellKor的Pragmatic
    Chaos团队，因为他们几乎在The Ensemble提交模型的前20分钟内提交了他们的最终模型（[http://mng.bz/K08O](https://shortener.manning.com/K08O)）。最终，两队都实现了10.06%的最终性能提升。
- en: 'While the Netflix competition captured the imagination of data scientists,
    machine learners, and casual data science enthusiasts worldwide, its lasting legacy
    has been to establish ensemble methods as a powerful way to build practical and
    robust models for large-scale, real-world applications. Among the individual algorithms
    used are several that have become staples of collaborative filtering and recommendation
    systems today: k-nearest neighbors, matrix factorization, and restricted Boltzmann
    machines. However, Andreas Töscher and Michael Jahrer of BigChaos, co-winners
    of the Netflix prize, summed up[¹](#pgfId-1142176) their keys to success:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Netflix竞赛吸引了全球数据科学家、机器学习者和普通数据科学爱好者的想象力，但其持久的影响在于确立了集成方法作为构建大规模、实际应用中强大且稳健模型的一种有效方式。其中使用的个别算法中，有几个已经成为今天协同过滤和推荐系统的基础：k近邻算法、矩阵分解和受限玻尔兹曼机。然而，BigChaos的Andreas
    Töscher和Michael Jahrer，Netflix大奖赛的共同获奖者，总结了[¹](#pgfId-1142176)他们成功的关键：
- en: 'During the nearly 3 years of the Netflix competition, there were two main factors
    which improved the overall accuracy: the quality of the individual algorithms
    and the ensemble idea. . . . The ensemble idea was part of the competition from
    the beginning and evolved over time. In the beginning, we used different models
    with different parametrization and a linear blending. . . . [Eventually] the linear
    blend was replaced by a nonlinear one.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在近3年的Netflix竞赛中，有两个主要因素提高了整体准确度：个别算法的质量和集成理念。……集成理念从一开始就是比赛的一部分，并随着时间的推移而发展。一开始，我们使用了不同参数化的不同模型，并采用线性混合。……[最终]线性混合被非线性混合所取代。
- en: In the years since, the use of ensemble methods has exploded, and they have
    emerged as a state-of-the-art technology for machine learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自那以后，集成方法的使用急剧增加，它们已经成为机器学习领域的一项尖端技术。
- en: 'The next two sections provide a gentle introduction to what ensemble methods
    are, why they work, and where they are applied. Then, we’ll look at a subtle but
    important challenge prevalent in all machine-learning algorithms: the *fit versus
    complexity tradeoff*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个部分将温和地介绍集成方法是什么，为什么它们有效，以及它们的应用领域。然后，我们将探讨所有机器学习算法普遍存在的微妙但重要的挑战：*拟合与复杂度之间的权衡*。
- en: Finally, we jump into training our very first ensemble method for a hands-on
    view of how ensemble methods overcome this fit versus complexity tradeoff and
    improve overall performance. Along the way, you’ll become familiar with several
    key terms that form the lexicon of ensemble methods and will be used throughout
    the book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将着手训练我们第一个集成方法，以便直观地了解集成方法是如何克服拟合与复杂度之间的权衡，并提高整体性能的。在这个过程中，你将熟悉到几个关键术语，这些术语构成了集成方法的词汇表，并在整本书中都会用到。
- en: '1.1 Ensemble methods: The wisdom of the crowds'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 集成方法：众人的智慧
- en: What exactly is an ensemble method? Let’s get an intuitive idea of ensemble
    methods and how they work by considering the allegorical case of Dr. Randy Forrest.
    We can then go on to frame the ensemble learning problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法究竟是什么？让我们通过考虑兰迪·福雷斯特博士的寓言案例来获得对集成方法及其工作原理的直观理解。然后，我们可以继续构建集成学习问题。
- en: Dr. Randy Forrest is a famed and successful diagnostician, much like his idol
    Dr. Gregory House of TV fame. His success, however, is due not only to his exceeding
    politeness (unlike his cynical and curmudgeonly idol) but also his rather unusual
    approach to diagnosis.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 兰迪·福雷斯特博士是一位著名且成功的诊断专家，就像他崇拜的电视名人格雷戈里·豪斯博士一样。然而，他的成功并不仅仅是因为他超越的礼貌（与他的愤世嫉俗且脾气暴躁的偶像不同），还因为他诊断方法上的相当不寻常。
- en: 'You see, Dr. Forrest works at a teaching hospital and commands the respect
    of a large number of doctors-in-training. Dr. Forrest has taken care to assemble
    a team with a *diversity of skills* (this is pretty important, and we’ll see why
    shortly). His residents excel at different specializations: one is good at cardiology
    (heart), another at pulmonology (lungs), yet another at neurology (nervous system),
    and so on. All in all, the group is a rather diversely skillful bunch, each with
    their own strengths.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你看，福雷斯特博士在一家教学医院工作，并受到众多实习医生的尊敬。福雷斯特博士特别注意组建了一个具有*技能多样性*的团队（这非常重要，我们很快就会看到原因）。他的住院医生在各个专业领域都很出色：一个擅长心脏病学（心脏），另一个擅长肺病学（肺部），还有一个擅长神经学（神经系统），等等。总的来说，这个团队是一个相当多样化的技能组合，每个人都拥有自己的优势。
- en: Every time Dr. Forrest gets a new case, he solicits the opinions of his residents
    and collects possible diagnoses from all of them (see figure 1.1). He then democratically
    selects the final diagnosis as the *most common one* from among all those proposed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每当福雷斯特博士接诊一个新病例时，他会征求住院医生的意见，并收集他们所有人的可能诊断（见图1.1）。然后，他民主地选择所有提出的诊断中最常见的一个作为最终的诊断。
- en: '![CH01_F01_Kunapuli](../Images/CH01_F01_Kunapuli.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F01_Kunapuli](../Images/CH01_F01_Kunapuli.png)'
- en: 'Figure 1.1 The diagnostic procedure followed by Dr. Randy Forrest every time
    he gets a new case is to ask all of his residents their opinions of the case.
    His residents offer their diagnoses: either the patient does or does not have
    cancer. Dr. Forrest then selects the majority answer as the final diagnosis put
    forth by his team.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 每次福雷斯特博士接诊新病例时，他都会询问所有住院医生对该病例的意见。他的住院医生会提供诊断：病人是否有癌症。然后，福雷斯特博士选择多数答案作为他团队提出的最终诊断。
- en: 'Dr. Forrest embodies a diagnostic *ensemble*: he aggregates his residents’
    diagnoses into a single diagnosis representative of the collective wisdom of his
    team. As it turns out, Dr. Forrest is right more often than any individual resident
    because he knows that his residents are pretty smart, and a large number of pretty
    smart residents are unlikely to *all* make the same mistake. Here, Dr. Forrest
    relies on the power of *model aggregating* or *model averaging*: he knows that
    the average answer is most likely going to be a good one.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 弗罗斯特博士体现了一个诊断集成：他将他的住院医生的诊断汇总成一个代表团队集体智慧的单一诊断。结果证明，弗罗斯特博士比任何单个住院医生都正确，因为他知道他的住院医生都很聪明，而且大量聪明的住院医生不太可能都犯同样的错误。在这里，弗罗斯特博士依赖于*模型聚合*或*模型平均化*的力量：他知道平均答案最有可能是一个好的答案。
- en: Still, how does Dr. Forrest know that *all* his residents aren’t wrong? He can’t
    know that for sure, of course. However, he has guarded against this undesirable
    outcome all the same. Remember that his residents all have diverse specializations.
    Because of their diverse backgrounds, training, specialization, and skills, it’s
    possible, but highly unlikely, that all his residents are wrong. Here, Dr. Forrest
    relies on the power of *ensemble diversity*, or the diversity of the individual
    components of his ensemble.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，弗罗斯特博士如何知道他的所有住院医生都没有错呢？当然，他不能确定这一点。然而，他仍然防范了这种不希望的结果。记住，他的住院医生都有不同的专业。由于他们不同的背景、培训、专业和技能，所有他的住院医生都可能是错的，但这可能性非常小。在这里，弗罗斯特博士依赖于*集成多样性*，即他集成中个体组件的多样性。
- en: Dr. Randy Forrest, of course, is an ensemble method, and his residents (who
    are in training) are the machine-learning algorithms that make up the ensemble.
    The secrets to his success, and indeed the success of ensemble methods as well,
    are
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，兰迪·弗罗斯特博士是一个集成方法，他的住院医生（正在接受培训）是构成集成的机器学习算法。他成功的关键，以及集成方法的成功，在于
- en: '*Ensemble diversity*—He has a variety of opinions to choose from.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成多样性*——他有各种各样的意见可供选择。'
- en: '*Model aggregation*—He can combine those opinions into a single final opinion.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型聚合*——他可以将这些意见合并成一个最终的看法。'
- en: 'Any collection of machine-learning algorithms can be used to build an ensemble,
    which is, literally, a group of machine learners. But why do they work? James
    Surowiecki, in *The Wisdom of Crowds*, describes human ensembles or wise crowds
    thus:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习算法的集合都可以用来构建一个集成，字面上讲，就是一个机器学习者的群体。但为什么它们会起作用呢？詹姆斯·苏罗维基在《群体的智慧》一书中这样描述人类集成或明智的群体：
- en: 'If you ask a large enough group of diverse and independent people to make a
    prediction or estimate a probability, the average of those answers will cancel
    out errors in individual estimation. Each person’s guess, you might say, has two
    components: information and errors. Subtract the errors, and you’re left with
    the information.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你让足够多、不同且独立的人群做出预测或估计概率，这些答案的平均值将抵消个别估计中的错误。可以说，每个人的猜测都有两个组成部分：信息和错误。减去错误，你剩下的是信息。
- en: 'This is also precisely the intuition behind ensembles of learners: it’s possible
    to build a wise machine-learning ensemble by aggregating individual learners.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这也正是学习者集成背后的直觉：通过聚合单个学习者，可以构建一个明智的机器学习集成。
- en: Ensemble methods
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法
- en: Formally, an *ensemble method* is a machine-learning algorithm that aims to
    improve predictive performance on a task by aggregating the predictions of multiple
    estimators or models. In this manner, an ensemble method learns a *meta-estimator*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，集成学习方法是一种机器学习算法，旨在通过聚合多个估计器或模型的预测来提高任务上的预测性能。以这种方式，集成学习方法学习一个*元估计器*。
- en: The key to success with ensemble methods is ensemble diversity, also known by
    alternate terms such as model complementarity or model orthogonality. Informally,
    ensemble diversity refers to the fact that individual ensemble components, or
    machine-learning models, are different from each other. Training such ensembles
    of diverse individual models is a key challenge in ensemble learning, and different
    ensemble methods achieve this in different ways.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法成功的关键是集成多样性，也被称为模型互补性或模型正交性的其他术语。非正式地说，集成多样性指的是集成组件或机器学习模型之间彼此不同的事实。在集成学习中训练这样的多样性个体模型集是一个关键挑战，不同的集成方法以不同的方式实现这一点。
- en: 1.2 Why you should care about ensemble learning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 为什么你应该关注集成学习
- en: What can you do with ensemble methods? Are they really just hype, or are they
    hallelujah? As we see in this section, they can be used to train and deploy robust
    and effective predictive models for many different applications.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用集成方法做什么？它们真的是炒作，还是真的值得赞美？正如我们在本节中看到的，它们可以用于训练和部署针对许多不同应用的稳健和有效的预测模型。
- en: One palpable success of ensemble methods is their domination of data science
    competitions (alongside deep learning), where they have been generally successful
    on different types of machine-learning tasks and application areas.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法的一个显著成功是它们在数据科学竞赛（与深度学习并驾齐驱）中的主导地位，在这些竞赛中，它们在多种机器学习任务和应用领域上通常都取得了成功。
- en: Anthony Goldbloom, CEO of Kaggle, revealed in 2015 that the three most successful
    algorithms for structured problems were XGBoost, random forest, and gradient boosting,
    all ensemble methods. Indeed, the most popular way to tackle data science competitions
    these days is to combine feature engineering with ensemble methods. Structured
    data is generally organized in tables, relational databases, and other formats
    most of us are familiar with, and ensemble methods have proven to be very successful
    on this type of data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle的首席执行官Anthony Goldbloom在2015年透露，对于结构化问题最成功的三个算法是XGBoost、随机森林和梯度提升，它们都是集成方法。确实，如今解决数据科学竞赛最流行的方式是将特征工程与集成方法相结合。结构化数据通常以表格、关系数据库和其他我们大多数人熟悉的格式组织，集成方法已经证明在这种类型的数据上非常成功。
- en: Unstructured data, in contrast, doesn’t always have a tabular structure. Images,
    audio, video, waveform, and text data are typically unstructured, and deep learning
    approaches—including automated feature generation—have been very successful on
    these types of data. While we focus on structured data for most of this book,
    ensemble methods can be combined with deep learning for unstructured problems
    as well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，非结构化数据并不总是具有表格结构。图像、音频、视频、波形和文本数据通常是未结构化的，深度学习方法——包括自动特征生成——在这些类型的数据上已经非常成功。虽然我们在这本书的大部分内容中关注结构化数据，但集成方法也可以与深度学习结合来解决非结构化问题。
- en: Beyond competitions, ensemble methods drive data science in several areas, including
    financial and business analytics, medicine and health care, cybersecurity, education,
    manufacturing, recommendation systems, entertainment, and many more.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了竞赛之外，集成方法在多个领域推动数据科学的发展，包括金融和商业分析、医学和医疗保健、网络安全、教育、制造业、推荐系统、娱乐等。
- en: In 2018, Olson et al.[²](#pgfId-1142227) conducted a comprehensive analysis
    of 14 popular machine-learning algorithms and their variants. They ranked each
    algorithm’s performance on 165 classification benchmark data sets. Their goal
    was to emulate the standard machine-learning pipeline to provide advice on how
    to select a machine-learning algorithm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，Olson等人[²](#pgfId-1142227)对14种流行的机器学习算法及其变体进行了全面分析。他们评估了每个算法在165个分类基准数据集上的性能。他们的目标是模拟标准的机器学习流程，以提供有关如何选择机器学习算法的建议。
- en: These comprehensive results are compiled into figure 1.2\. Each row shows how
    often one model outperforms other models across all 165 data sets. For example,
    XGBoost beats gradient boosting on 34 of 165 benchmark data sets (first row, second
    column), while gradient boosting beats XGBoost on 12 of 165 benchmark data sets
    (second row, first column). Their performance is very similar on the remaining
    119 of 165 data sets, meaning both models perform equally well on 119 data sets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些综合结果被汇总到图1.2中。每一行显示了在所有165个数据集中，一个模型相对于其他模型的表现频率。例如，XGBoost在165个基准数据集中有34个胜过梯度提升（第一行，第二列），而梯度提升在165个基准数据集中有12个胜过XGBoost（第二行，第一列）。在剩余的119个数据集中，它们的性能非常相似，这意味着两个模型在119个数据集上的表现相当。
- en: '![CH01_F02_Kunapuli](../Images/CH01_F02_Kunapuli.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F02_Kunapuli](../Images/CH01_F02_Kunapuli.png)'
- en: Figure 1.2 Which machine-learning algorithm should I use for my data set? The
    performance of several different machine-learning algorithms, relative to each
    other on 165 benchmark data sets, is shown here. The final trained models are
    ranked (top-to-bottom, left-to-right) based on their performance on all benchmark
    data sets in relation to all other methods. In their evaluation, Olson et al.
    consider two methods to have the same performance on a data set if their prediction
    accuracies are within 1% of each other. This figure was reproduced using the codebase
    and comprehensive experimental results compiled by the authors into a publicly
    available GitHub repository ([https://github.com/rhiever/sklearn-benchmarks](https://github.com/rhiever/sklearn-benchmarks))
    and includes the authors’ evaluation of XGBoost as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 我应该为我的数据集使用哪种机器学习算法？这里展示了几个不同的机器学习算法在 165 个基准数据集上的性能，相对于彼此。根据它们在所有基准数据集上的性能相对于所有其他方法的排名（从上到下，从左到右），最终训练好的模型被排序。在评估中，Olson
    等人认为，如果两种方法的预测准确率相差在 1% 以内，则认为它们在数据集上的性能相同。此图使用作者编译的代码库和综合实验结果重新生成，这些结果包含在公开可用的
    GitHub 仓库中（[https://github.com/rhiever/sklearn-benchmarks](https://github.com/rhiever/sklearn-benchmarks)），并包括作者对
    XGBoost 的评估。
- en: In contrast, XGBoost beats multinomial naïve Bayes (MNB) on 157 of 165 data
    sets (first row, last column), while MNB only beats XGBoost on 2 of 165 data sets
    (last row, first column) and can only match XGBoost on 6 of 165 data sets!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，XGBoost 在 165 个数据集中击败了多项式朴素贝叶斯（MNB）中的 157 个（第一行，最后一列），而 MNB 只在 165 个数据集中的
    2 个（最后一行，第一列）上击败了 XGBoost，并且只能在 165 个数据集中的 6 个上与 XGBoost 匹配！
- en: 'In general, ensemble methods (1: XGBoost, 2: gradient boosting, 3: Extra Trees,
    4: random forests, 8: AdaBoost) outperformed other methods handily. These results
    demonstrate exactly why ensemble methods (specifically, tree-based ensembles)
    are considered state of the art.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，集成方法（1：XGBoost，2：梯度提升，3：Extra Trees，4：随机森林，8：AdaBoost）在性能上明显优于其他方法。这些结果恰好说明了为什么集成方法（特别是基于树的集成方法）被认为是当前最先进的。
- en: If your goal is to develop state-of-the-art analytics from your data, or to
    eke out better performance and improve models you already have, this book is for
    you. If your goal is to start competing more effectively in data science competitions
    for fame and fortune or to just improve your data science skills, this book is
    also for you. If you’re excited about adding powerful ensemble methods to your
    machine-learning arsenal, this book is definitely for you.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的目标是开发从你的数据中提取最先进的分析，或者是为了提高性能并改进你已有的模型，这本书适合你。如果你的目标是为了在数据科学竞赛中更有效地竞争以获得名利或者只是提高你的数据科学技能，这本书也适合你。如果你对将强大的集成方法添加到你的机器学习工具箱中感到兴奋，这本书绝对适合你。
- en: 'To drive home this point, we’ll build our first ensemble method: a *simple
    model combination ensemble*. Before we do, let’s dive into the tradeoff between
    fit and complexity that most machine-learning methods have to grapple with, as
    it will help us understand why ensemble methods are so effective.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调这一点，我们将构建我们的第一个集成方法：一个*简单模型组合集成*。在我们这样做之前，让我们深入了解大多数机器学习方法必须处理的拟合和复杂性的权衡，这将帮助我们理解为什么集成方法如此有效。
- en: 1.3 Fit vs. complexity in individual models
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 单个模型中的拟合与复杂度
- en: 'In this section, we look at two popular machine-learning methods: decision
    trees and support vector machines (SVMs). As we do so, we’ll explore how their
    fitting and predictive behavior changes as they learn increasingly complex models.
    This section also serves as a refresher of the training and evaluation practices
    we usually follow during modeling.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨两种流行的机器学习方法：决策树和支持向量机（SVMs）。在探讨过程中，我们将了解它们在学习越来越复杂的模型时，其拟合和预测行为是如何变化的。本节还作为我们在建模过程中通常遵循的训练和评估实践的复习。
- en: Machine-learning tasks are typically
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习任务通常是
- en: '*Supervised learning tasks*—These have a data set of *labeled examples*, where
    data has been annotated. For example, in cancer diagnoses, each example will be
    an individual patient, with label/annotation “has cancer” or “does not have cancer.”
    Labels can be 0-1 (binary classification), categorical (multiclass classification),
    or continuous (regression).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监督学习任务*—这些任务有一个带有*标记示例*的数据集，其中数据已经被标注。例如，在癌症诊断中，每个示例将是一个单独的患者，带有标签/注释“有癌症”或“没有癌症”。标签可以是
    0-1（二元分类）、分类（多类分类）或连续（回归）。'
- en: '*Unsupervised learning tasks*—These have a data set of *unlabeled examples*,
    where the data lacks annotations. This includes tasks such as grouping examples
    together by some notion of “similarity” (clustering) or identifying anomalous
    data that doesn’t fit the expected pattern (anomaly detection).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督学习任务*——这些任务具有一个*未标记示例*的数据集，其中数据缺乏注释。这包括诸如通过某种“相似性”概念将示例分组在一起（聚类）或识别不符合预期模式的异常数据（异常检测）等任务。'
- en: We’ll create a simple, synthetically generated, supervised regression data set
    to illustrate the key challenge in training machine-learning models and to motivate
    the need for ensemble methods. With this data set, we’ll train increasingly complex
    machine-learning models that fit and eventually overfit the data during training.
    As we’ll see, overfitting during training doesn’t necessarily produce models that
    generalize better.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个简单、人工生成的、监督回归数据集，以说明训练机器学习模型的关键挑战，并激发对集成方法的需求。使用这个数据集，我们将训练越来越复杂的机器学习模型，这些模型在训练过程中拟合数据，并最终过度拟合数据。正如我们将看到的，训练过程中的过度拟合并不一定会产生泛化能力更好的模型。
- en: 1.3.1 Regression with decision trees
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 决策树回归
- en: One of the most popular machine-learning models is the decision tree,[³](#pgfId-1142270)
    which can be used for classification as well as regression tasks. A decision tree
    is made up of decision nodes and leaf nodes, and each decision node tests the
    current example for a specific condition.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的机器学习模型之一是决策树，[³](#pgfId-1142270)，它可以用于分类以及回归任务。决策树由决策节点和叶节点组成，每个决策节点测试当前示例的特定条件。
- en: For example, in figure 1.3, we use a decision-tree classifier for a binary classification
    task over a data set with two features, *x*[1] and *x*[2].The first node tests
    each input example to see if the second feature *x*[2] > 5 and then funnels the
    example to the right or left branch of the decision tree depending on the result.
    This continues until the input example reaches a leaf node; at this point, the
    prediction corresponding to the leaf node is returned. For classification tasks,
    the leaf value is a class label, whereas for regression tasks, the leaf returns
    a regression value.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图1.3中，我们使用决策树分类器对一个具有两个特征的数据集进行二元分类任务，这两个特征是*x*[1]和*x*[2]。第一个节点测试每个输入示例，看第二个特征*x*[2]是否大于5，然后根据结果将示例引导到决策树的右侧或左侧分支。这个过程一直持续到输入示例达到叶节点；在这个时候，返回与叶节点对应的预测。对于分类任务，叶值是一个类标签，而对于回归任务，叶节点返回一个回归值。
- en: '![CH01_F03_Kunapuli](../Images/CH01_F03_Kunapuli.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F03_Kunapuli](../Images/CH01_F03_Kunapuli.png)'
- en: Figure 1.3 Decision trees partition the feature space into axis-parallel rectangles.
    When used for classification, the tree checks for conditions on the features in
    the decision nodes, funneling the example to the left or right after each test.
    Ultimately, the example filters down to a leaf node, which will give its classification
    label. The partition of the feature space according to this decision tree is shown
    on the left.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 决策树将特征空间划分为轴平行的矩形。当用于分类时，树在决策节点上检查特征的条件，在每个测试后引导示例向左或向右。最终，示例过滤到一个叶节点，该节点将给出其分类标签。根据此决策树对特征空间的划分如图左侧所示。
- en: A decision tree of depth 1 is called a *decision stump* and is the simplest
    possible tree. A decision stump contains a single decision node and two leaf nodes.
    A *shallow decision tree* (say, depth 2 or 3) will have a small number of decision
    nodes and leaf nodes and is a simple model. Consequently, it can only represent
    simple functions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深度为1的决策树称为*决策桩*，是最简单的树。决策桩包含一个决策节点和两个叶节点。一个*浅决策树*（例如，深度为2或3）将具有少量决策节点和叶节点，是一个简单的模型。因此，它只能表示简单的函数。
- en: On the other hand, a deeper decision tree is a more complex model with many
    more decision nodes and leaf nodes. A deeper decision tree, thus, can represent
    richer and more complex functions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一个更深的决策树是一个更复杂的模型，具有更多的决策节点和叶节点。因此，一个更深的决策树可以表示更丰富和更复杂的函数。
- en: Fit vs. complexity in decision trees
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树中的拟合与复杂度
- en: We’ll explore such tradeoffs between model fit and representation complexity
    in the context of a synthetic data set called *Friedman-1*, originally created
    by Jerome Friedman in 1991 to explore how well his new multivariate adaptive regression
    splines (MARS) algorithm was fitting high-dimensional data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个名为*Friedman-1*的合成数据集的背景下探索模型拟合和表示复杂度之间的这种权衡，该数据集最初由杰罗姆·弗里德曼于1991年创建，用于探索他的新多元自适应回归样条(MARS)算法在拟合高维数据方面的表现。
- en: 'This data set was carefully generated to evaluate a regression method’s ability
    to only pick up true feature dependencies in the data set and ignore others. More
    specifically, the data set is generated to have 15 randomly generated features
    of which only the first 5 features are relevant to the target variable:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是精心生成的，用于评估回归方法仅能从数据集中提取真实特征依赖关系的能力，并忽略其他关系。更具体地说，数据集生成了15个随机生成的特征，其中只有前5个特征与目标变量相关：
- en: '![CH01_F03_Kunapuli-E01](../Images/CH01_F03_Kunapuli-E01.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F03_Kunapuli-E01](../Images/CH01_F03_Kunapuli-E01.png)'
- en: 'scikit-learn contains a built-in function that we can use to generate as much
    data in this scheme as possible:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 包含一个内置函数，我们可以使用它来生成尽可能多的数据：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Generates a data set with 500 examples
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成包含500个示例的数据集
- en: ❷ Each example will have 15 features.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每个示例将包含15个特征。
- en: ❸ Adds Gaussian noise to each label to make it more realistic
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对每个标签添加高斯噪声，使其更真实
- en: We’ll randomly split the data set into a training set (with 67% of the data)
    and a test set (with 33% of the data) in order to illustrate the effects of the
    complexity versus fit more clearly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随机将数据集分成训练集（包含67%的数据）和测试集（包含33%的数据），以更清楚地说明复杂度与拟合之间的效应。
- en: TIP During modeling, we often have to split the data into a training and a test
    set. How big should these sets be? If the fraction of the data that makes up the
    training set is too small, the model won’t have enough data to train. If the fraction
    of the data that makes up the test set is too small, there will be higher variation
    in our generalization estimates of how well the model performs on future data.
    A good rule of thumb for medium to large data sets (known as the Pareto principle)
    is to start with an 80%-20% train-test split. Another good rule for small data
    sets is to use the leave-one-out approach, where a single example is left out
    each time for evaluation, and the overall training and evaluation process is repeated
    for every example.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 在建模过程中，我们通常需要将数据分成训练集和测试集。这些集应该有多大？如果构成训练集的数据比例太小，模型将没有足够的数据进行训练。如果构成测试集的数据比例太小，我们关于模型在未来的数据上表现如何的泛化估计将会有更高的变化。对于中等或大型数据集（称为帕累托原则），一个好的经验法则是从80%-20%的训练-测试分割开始。对于小型数据集，另一个好的规则是使用留一法，每次评估时留出一个示例，并针对每个示例重复整体训练和评估过程。
- en: 'For different depths d = 1 to 10, we train a tree on the training set and evaluate
    it on the test set. When we look at the training errors and the test errors across
    different depths, we can identify the depth of the “best tree.” We characterize
    “best” in terms of an *evaluation metric*. For regression problems, there are
    several evaluation metrics: mean squared error (MSE), mean absolute deviation
    (MAD), coefficient of determination, and so on.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同的深度d = 1到10，我们在训练集上训练一棵树，并在测试集上评估它。当我们查看不同深度的训练误差和测试误差时，我们可以确定“最佳树”的深度。我们用*评估指标*来定义“最佳”。对于回归问题，有几个评估指标：均方误差(MSE)、平均绝对偏差(MAD)、决定系数等。
- en: We’ll use the coefficient of determination, also known as the *R*² *score*,
    which measures the proportion of the variance in the labels (*y*) that is predictable
    from the features (*x*).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用决定系数，也称为*R*² *得分*，它衡量标签(*y*)中可从特征(*x*)预测的方差比例。
- en: Coefficient of determination
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 决定系数
- en: 'The coefficient of determination (*R*²) is a measure of regression performance.
    *R*² is the proportion of variance in the true labels that is predictable from
    the features. *R*² depends on two quantities: (1) the total variance in the true
    labels, or *total sum of squares* (TSS); and (2) the MSE, or the *residual sum
    of squares* (RSS) between the true and predicted labels. We have *R*² = 1 - *RSS*
    / *TSS*. A perfect model will have zero prediction error, or *RSS* = 0 and its
    corresponding *R*² = 1\. Really good models have *R*² values close to 1\. A really
    bad model will have high prediction error and high RSS. This means that for really
    bad models, we can have negative *R*².'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 决定系数（*R*²）是回归性能的一个度量。*R*² 是从特征中可预测的真实标签方差的比例。*R*² 取决于两个量：1) 真实标签中的总方差，或称 *总平方和*（TSS）；2)
    真实标签与预测标签之间的均方误差（MSE），或称 *残差平方和*（RSS）。我们有 *R*² = 1 - *RSS* / *TSS*。一个完美的模型将没有预测误差，即
    *RSS* = 0，其对应的 *R*² = 1。真正好的模型其 *R*² 值接近 1。一个真正差的模型将会有高的预测误差和高 RSS。这意味着对于真正差的模型，我们可能会有负的
    *R*²。
- en: One last thing to note is that we are splitting the data into a training set
    and test set *randomly*, which means that it’s possible to get very lucky or very
    unlucky in our split. To avoid the influence of randomness, we repeat our experiment
    K = 5 times and average the results across the runs. Why 5? This choice is often
    somewhat arbitrary, and you’ll have to decide whether you want less variation
    in the test errors (large values of K) or less computation time (small values
    of K).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要注意的一点是，我们正在将数据随机分割为训练集和测试集，这意味着我们的分割可能会非常幸运或非常不幸。为了避免随机性的影响，我们重复实验 K = 5
    次，并在运行之间平均结果。为什么是 5？这个选择通常是相当任意的，你将不得不决定你是否想要测试误差的更小变化（K 的较大值）或更少的计算时间（K 的较小值）。
- en: 'The pseudocode for our experiment is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验伪代码如下：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following code snippet does precisely this, and then it plots the training
    and test scores. Rather than explicitly implement the preceding pseudocode, the
    following code uses the scikit-learn function sklearn.model_selection.ShuffleSplit
    to automatically split the data into five different training and test subsets,
    and it uses sklearn.model_selection.validation_curve to determine *R*² scores
    for varying decision tree depths:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段正是这样做的，然后它绘制了训练和测试分数。而不是明确实现前面的伪代码，以下代码使用 scikit-learn 函数 sklearn.model_selection.ShuffleSplit
    自动将数据分割成五个不同的训练集和测试集，并使用 sklearn.model_selection.validation_curve 确定不同决策树深度的 *R*²
    分数：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Sets up five different random splits of the data into train and test sets
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置了五种不同的数据随机分割为训练集和测试集
- en: ❷ For each split, trains decision trees of depths from 1 to 10 and then evaluates
    on the test set
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对于每个分割，训练深度从 1 到 10 的决策树，然后在测试集上进行评估
- en: Remember, our ultimate goal is to build a machine-learning model that *generalizes*
    well, that is, a model that performs well on *future, unseen data*. Our first
    instinct then, will be to train a model that achieves the smallest training error.
    Such models will typically be quite complex in order to fit as many training examples
    as possible. After all, a complex model will likely fit our training data well
    and have a small training error. It is natural to presume that a model that achieves
    the smallest training error should also generalize well in the future and predict
    unseen examples equally well.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的最终目标是构建一个能够很好地 *泛化* 的机器学习模型，也就是说，一个在 *未来未见数据* 上表现良好的模型。因此，我们的第一个本能将是训练一个达到最小训练误差的模型。这样的模型通常会很复杂，以便尽可能多地拟合训练示例。毕竟，一个复杂的模型可能会很好地拟合我们的训练数据并具有小的训练误差。自然而然地，我们会假设一个达到最小训练误差的模型在未来的泛化能力也应该很好，并且能够同样好地预测未见示例。
- en: Now, let’s look at the training and test scores in figure 1.4 to see if this
    is the case. Remember that an *R*² score close to 1 indicates a very good regression
    model, and scores further away from 1 indicate worse models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看图 1.4 中的训练和测试分数，看看这是否如此。记住，*R*² 分数接近 1 表示一个非常好的回归模型，而分数远离 1 表示模型更差。
- en: 'Deeper decision trees are more complex and have greater representational power,
    so it’s not surprising to see that deeper trees fit the training data better.
    This is clear from figure 1.4: as tree depth (model complexity) increases, the
    training score approaches *R*² = 1\. Thus, more complex models achieve better
    fits on the training data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 深度更大的决策树更复杂，具有更强的表达能力，因此看到深度更大的树更好地拟合训练数据并不奇怪。这从图 1.4 中很明显：随着树深度（模型复杂度）的增加，训练分数接近
    *R*² = 1。因此，更复杂的模型在训练数据上达到更好的拟合。
- en: '![CH01_F04_Kunapuli](../Images/CH01_F04_Kunapuli.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F04_Kunapuli](../Images/CH01_F04_Kunapuli.png)'
- en: Figure 1.4 Comparing decision trees of different depths on the Friedman-1 regression
    data set using R² as the evaluation metric. Higher R² scores mean that the model
    achieves lower error and fits the data better. An R² score close to 1 means that
    the model achieves nearly zero error. It’s possible to fit the training data nearly
    perfectly with very deep decision trees, but such overly complex models actually
    overfit the training data and don’t generalize well to future data, as evidenced
    by the test scores.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 使用 R² 作为评估指标，比较了不同深度的决策树在 Friedman-1 回归数据集上的表现。更高的 R² 分数意味着模型达到更低的误差并更好地拟合数据。一个接近
    1 的 R² 分数意味着模型达到几乎零误差。使用非常深的决策树几乎可以完美地拟合训练数据，但这样的过度复杂模型实际上过度拟合了训练数据，并且对未来数据的泛化能力不好，如测试分数所示。
- en: What is surprising, however, is that the *test R*² score doesn’t similarly keep
    increasing with complexity. In fact, beyond max_depth=4, test scores remain fairly
    consistent. This suggests that a tree of depth 8 might fit the training data better
    than a tree of depth 4, but both trees will perform roughly identically when they
    try to generalize and predict on new data!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，令人惊讶的是，*测试 R²* 分数并没有随着复杂性的增加而相应地持续增加。事实上，超过 max_depth=4 后，测试分数保持相当稳定。这表明深度为
    8 的树可能比深度为 4 的树更适合训练数据，但这两棵树在尝试泛化和对新数据进行预测时表现大致相同！
- en: 'As decision trees become deeper, they get more complex and achieve lower training
    errors. However, their ability to generalize to future data (estimated by test
    scores) doesn’t keep decreasing. This is a rather counterintuitive result: the
    model with the best fit on the training set isn’t necessarily the best model for
    predictions when deployed in the real world.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随着决策树的深度增加，它们变得更加复杂，并实现更低的训练误差。然而，它们对未来数据（通过测试分数估计）的泛化能力并没有持续下降。这是一个相当反直觉的结果：在训练集上拟合最好的模型不一定是在实际应用中用于预测的最佳模型。
- en: 'It’s tempting to argue that we got unlucky when we partitioned the training
    and test sets randomly. However, we ran our experiment with five different random
    partitions and averaged the results to avoid this. To be sure, however, let’s
    repeat this experiment with another well-known machine-learning method: support
    vector regression.[⁴](#pgfId-1142370)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会认为我们在随机划分训练集和测试集时运气不好。然而，我们用五个不同的随机划分进行了实验，并平均了结果以避免这种情况。为了确保，让我们用另一种众所周知的机器学习方法重复这个实验：支持向量回归。[⁴](#pgfId-1142370)
- en: 1.3.2 Regression with support vector machines
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 使用支持向量机进行回归
- en: Like decision trees, support vector machines (SVMs) are a great off-the-shelf
    baseline modeling approach, and most packages come with a robust implementation
    of SVMs. You may have used SVMs for classification, where it’s possible to learn
    nonlinear models of considerable complexity using kernels such as the radial basis
    function (RBF) kernel, or the polynomial kernel. SVMs have also been adapted for
    regression, and as in the classification case, they try to find a model that trades
    off between regularization and fit during training. Specifically, SVM training
    tries to find a model to minimize
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策树一样，支持向量机（SVMs）是一种很好的现成建模方法，大多数软件包都提供了 SVMs 的稳健实现。你可能已经使用过 SVMs 进行分类，在这种情况下，可以使用径向基函数（RBF）核或多项式核等核学习相当复杂的非线性模型。SVMs
    也已被用于回归，并且与分类情况一样，它们在训练过程中试图找到一个在正则化和拟合度之间进行权衡的模型。具体来说，SVM 训练试图找到一个模型来最小化
- en: '![CH01_F04_Kunapuli-E02](../Images/CH01_F04_Kunapuli-E02.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F04_Kunapuli-E02](../Images/CH01_F04_Kunapuli-E02.png)'
- en: 'The regularization term measures the flatness of the model: the more it is
    minimized, the more linear and less complex the learned model is. The loss term
    measures the fit to the training data through a *loss function* (typically, MSE):
    the more it is minimized, the better the fit to the training data. The *regularization*
    parameter *C* trades off between these two competing objectives:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项衡量模型的平坦度：它被最小化的程度越高，学习到的模型就越线性、越简单。损失项通过 *损失函数*（通常是均方误差 MSE）衡量对训练数据的拟合度：它被最小化的程度越高，对训练数据的拟合度就越好。*正则化*
    参数 *C* 在这两个相互竞争的目标之间进行权衡：
- en: A small value of *C* means the model will focus more on regularization and simplicity,
    and less on training error, which causes the model to have higher training error
    and *underfit*.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C* 的值较小意味着模型将更多地关注正则化和简单性，而较少关注训练误差，这导致模型具有更高的训练误差和 *欠拟合*。'
- en: A large value of *C* means the model will focus more on training error and learn
    more complex models, which causes the model to have lower training errors and
    possibly *overfit*.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*的值较大意味着模型将更多地关注训练错误，并学习更复杂的模型，这导致模型具有更低的训练错误率，并可能*过拟合*。'
- en: We can see the effect of increasing the value of *C* on the learned models in
    figure 1.5\. In particular, we can visualize the tradeoff between fit and complexity.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图1.5中看到增加*C*值对学习模型的影响。特别是，我们可以可视化拟合与复杂度之间的权衡。
- en: '![CH01_F05_Kunapuli](../Images/CH01_F05_Kunapuli.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F05_Kunapuli](../Images/CH01_F05_Kunapuli.png)'
- en: Figure 1.5 Support vector machine with an RBF kernel, with kernel parameter
    gamma = 0.75\. Small values of *C* result in more linear (flatter) and less complex
    models that underfit the data, while large values of *C* result in more nonlinear
    (curvier) and more complex models that overfit the data. Selecting a good value
    for *C* is critically important in training a good SVM model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 使用RBF核的支持向量机，核参数gamma = 0.75。小的*C*值导致更线性（更平坦）且复杂度更低的模型，这些模型欠拟合数据，而大的*C*值导致更非线性（更弯曲）且更复杂的模型，这些模型过拟合数据。选择合适的*C*值对于训练一个好的SVM模型至关重要。
- en: CAUTION SVMs identify support vectors, a smaller working set of training examples
    that the model depends on. Counting the number of support vectors isn’t an effective
    way to measure model complexity as small values of *C* restrict the model more,
    forcing it to use more support vectors in the final model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：SVMs识别支持向量，这是一个较小的训练示例集合，模型依赖于它。计算支持向量的数量并不是衡量模型复杂性的有效方法，因为小的*C*值会限制模型更多，迫使它在最终模型中使用更多的支持向量。
- en: Fit vs. complexity in support vector machines
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机中的拟合与复杂度
- en: 'Much like max_depth in DecisionTreeRegressor(), the parameter C in support
    vector regression, SVR(), can be tuned to obtain models with different behaviors.
    Again, we’re faced with the same question: which is the best model? To answer
    this, we can repeat the same experiment as with decision trees:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与DecisionTreeRegressor()中的max_depth类似，支持向量回归（SVR）中的参数C可以调整以获得具有不同行为的模型。同样，我们面临着相同的问题：哪个是最好的模型？为了回答这个问题，我们可以重复与决策树相同的实验：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this code snippet, we train an SVM with a three-degree polynomial kernel.
    We try seven values of *C*—10-3, 10-2, 10-1, 1, 10, 102, and 103—and visualize
    the train and test scores, as before, in figure 1.6.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们使用三阶多项式核训练了一个SVM。我们尝试了七个*C*的值——10^-3, 10^-2, 10^-1, 1, 10, 10^2,
    和 10^3——并像之前一样，在图1.6中可视化训练和测试分数。
- en: '![CH01_F06_Kunapuli](../Images/CH01_F06_Kunapuli.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F06_Kunapuli](../Images/CH01_F06_Kunapuli.png)'
- en: Figure 1.6 Comparing SVM regressors of different complexities on the Friedman-1
    regression data set using *R*² as the evaluation metric. As with decision trees,
    highly complex models (corresponding to higher *C* values) appear to achieve fantastic
    fit on the training data, but they don’t actually generalize as well. This means
    that as *C* increases, so does the possibility of overfitting.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 使用*R*²作为评估指标，比较了不同复杂度的SVM回归器在Friedman-1回归数据集上的表现。与决策树一样，高度复杂的模型（对应于更高的*C*值）似乎在训练数据上实现了惊人的拟合，但实际上泛化能力并不强。这意味着随着*C*的增加，过拟合的可能性也增加。
- en: 'Again, rather counterintuitively, the model with the best fit on the training
    set isn’t necessarily the best model for predictions when deployed in the real
    world. Every machine-learning algorithm, in fact, exhibits this behavior:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，出人意料的是，在训练集上拟合最好的模型并不一定是当部署到现实世界中的最佳预测模型。实际上，每个机器学习算法都表现出这种行为：
- en: Overly simple models tend to not fit the training data properly and tend to
    generalize poorly on future data; a model that is performing poorly on training
    and test data is *underfitting*.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度简单的模型往往不能正确拟合训练数据，并且对未来数据的泛化能力较差；一个在训练和测试数据上表现不佳的模型是*欠拟合*。
- en: Overly complex models can achieve very low training errors but tend to generalize
    poorly on future data too; a model that is performing very well on training data,
    but poorly on test data is *overfitting*.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度复杂的模型虽然可以达到非常低的训练错误率，但往往在未来的数据上泛化能力较差；一个在训练数据上表现良好，但在测试数据上表现不佳的模型是*过拟合*。
- en: The best models trade off between complexity and fit, sacrificing a little bit
    of each during training so that they can generalize most effectively when deployed.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳模型在复杂度和拟合之间进行权衡，在训练过程中牺牲一点每一项，以便在部署时能够最有效地泛化。
- en: As we’ll see in the next section, ensemble methods are an effective way of tackling
    the problem of fit versus complexity.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下一节中看到的，集成方法是解决拟合与复杂度问题的有效方法。
- en: The bias-variance tradeoff
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: What we’ve informally discussed so far as the fit versus complexity tradeoff
    is more formally known as the *bias-variance tradeoff*. The *bias* of a model
    is the error arising from the effect of modeling assumptions (such as a preference
    for simpler models). The *variance* of a model is the error arising from sensitivity
    to small variations in the data set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前非正式讨论的拟合与复杂度权衡问题更正式地被称为**偏差-方差权衡**。模型的**偏差**是指由建模假设（例如对简单模型的偏好）引起的误差。模型的**方差**是指由对数据集微小变化的敏感性引起的误差。
- en: Highly complex models (low bias) will overfit the data and be more sensitive
    to noise (high variance), while simpler models (high bias) will underfit the data
    and be less sensitive to noise (low variance). This tradeoff is inherent in every
    machine-learning algorithm. Ensemble methods seek to overcome this problem by
    combining several low-bias models to reduce their variance or combining several
    low-variance models to reduce their bias.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 高度复杂的模型（低偏差）会过度拟合数据并对噪声更敏感（高方差），而简单的模型（高偏差）会欠拟合数据并对噪声不敏感（低方差）。这种权衡是每个机器学习算法固有的。集成方法通过结合几个低偏差模型来减少它们的方差，或者结合几个低方差模型来减少它们的偏差，以克服这个问题。
- en: 1.4 Our first ensemble
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 我们的第一个集成
- en: 'In this section, we’ll overcome the fit versus complexity problems of individual
    models by training our first ensemble. Recall from the allegorical Dr. Forrest
    that an effective ensemble performs model aggregation on a set of component models,
    as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过训练我们的第一个集成来克服单个模型的拟合与复杂度问题。回想一下寓言中的福雷斯特博士，一个有效的集成在一系列组件模型上执行模型聚合，如下所示：
- en: We train a set of *base estimators* (also known as *base learners*) using diverse
    base-learning algorithms on the same data set. That is, we count on the significant
    variations in each learning algorithm to produce a diverse set of base estimators.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用不同的基础学习算法在相同的数据集上训练一组**基础估计器**（也称为**基础学习器**）。也就是说，我们依赖每个学习算法的显著差异来产生一组多样化的基础估计器。
- en: For a regression problem (e.g., the Friedman-1 data introduced in the previous
    section), the predictions of individual base estimators are continuous. We can
    aggregate the results into one final ensemble prediction by *simple averaging*
    of the individual predictions.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归问题（例如，前一小节中引入的Friedman-1数据），单个基础估计器的预测是连续的。我们可以通过**简单平均**单个预测结果来汇总结果，形成一个最终的集成预测。
- en: 'We use the following regression algorithms to produce base estimators from
    our data set: kernel ridge regression, support vector regression, decision-tree
    regression, k-nearest neighbor regression, Gaussian processes, and multilayer
    perceptrons (neural networks).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下回归算法从我们的数据集中生成基础估计器：核岭回归、支持向量回归、决策树回归、k最近邻回归、高斯过程和多层感知器（神经网络）。
- en: Once we have the trained models, we use each one to make individual predictions
    and then aggregate the individual predictions into a final prediction, as shown
    in figure 1.7.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了训练好的模型，我们使用每个模型进行单个预测，然后将单个预测汇总成一个最终的预测，如图1.7所示。
- en: '![CH01_F07_Kunapuli](../Images/CH01_F07_Kunapuli.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F07_Kunapuli](../Images/CH01_F07_Kunapuli.png)'
- en: 'Figure 1.7 Our first ensemble method ensembles the predictions of six different
    regression models by averaging them. This simple ensemble illustrates two key
    principles of ensembling: (1) model diversity, achieved in this case by using
    six different base machine-learning models; and (2) model aggregation, achieved
    in this case by simple averaging across predictions.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 我们的第一个集成方法通过平均六个不同的回归模型的预测来集成预测。这个简单的集成说明了集成两个关键原则：（1）模型多样性，在本例中通过使用六个不同的基础机器学习模型实现；（2）模型聚合，在本例中通过简单平均预测实现。
- en: The code for training individual base estimators is shown in the following listing.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 训练单个基础估计器的代码如下所示。
- en: Listing 1.1 Training diverse base estimators
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.1 训练多样化的基础估计器
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Generates a synthetic Friedman-1 data set with 500 examples and 15 features
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成包含500个示例和15个特征的合成Friedman-1数据集
- en: ❷ Splits into a training set (with 75% of the data) and a test set (with the
    remaining 25%)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据集分为训练集（包含75%的数据）和测试集（包含剩余的25%）
- en: ❸ Initializes hyperparameters of each individual base estimator
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化每个单个基础估计器的超参数
- en: ❹ Trains the individual base estimators
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练单个基础估计器
- en: We have now trained six diverse base estimators using six different base-learning
    algorithms. Given new data, we can aggregate the individual predictions into a
    final prediction as shown in the following listing.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经使用六种不同的基础学习算法训练了六个不同的基础估计器。给定新的数据，我们可以将单个预测聚合成一个最终的预测，如下面的列表所示。
- en: Listing 1.2 Aggregating base estimator predictions
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.2 聚合基础估计器的预测
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Initializes individual predictions
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化单个预测
- en: ❷ Makes individual predictions using the base estimators
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用基础估计器进行单个预测
- en: ❸ Aggregates (average) individual predictions
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 聚合（平均）单个预测
- en: One way to understand the benefits of ensembling is if we look at all possible
    combinations of models for predictions. That is, we look at the performance of
    one model at a time, then all possible ensembles of two models (there are 15 such
    combinations), then all possible ensembles of three models (there are 20 such
    combinations), and so on. For ensemble sizes 1 to 6, we plot the test set performances
    of all these ensemble combinations in figure 1.8.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 理解集成好处的一种方式是查看所有可能的模型预测组合。也就是说，我们一次查看一个模型的性能，然后查看所有可能的两个模型的集成（有15种这样的组合），然后查看所有可能的三个模型的集成（有20种组合），以此类推。对于集成大小为1到6的情况，我们在图1.8中绘制了所有这些集成组合的测试集性能。
- en: '![CH01_F08_Kunapuli](../Images/CH01_F08_Kunapuli.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F08_Kunapuli](../Images/CH01_F08_Kunapuli.png)'
- en: Figure 1.8 Prediction performance versus ensemble size. When the ensemble size
    is 1, we can see that the performances of individual models are rather diverse.
    When the size is 2, we average the results of different pairs of models (in this
    case, 15 ensembles). When 3, we average the results of 3 models at a time (in
    this case, 20 ensembles), and so on, until the size is 6, when we average the
    results of all 6 models into a single, grand ensemble.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 预测性能与集成大小的关系。当集成大小为1时，我们可以看到各个模型的性能相当多样。当大小为2时，我们平均不同对模型的结果（在这种情况下，15个集成）。当大小为3时，我们一次平均3个模型的结果（在这种情况下，20个集成），以此类推，直到大小为6，此时我们将所有6个模型的结果平均到一个单一的、庞大的集成中。
- en: As we aggregate more and more models, we see that the ensembles generalize increasingly
    better. The most striking result of our experiment, though, is that the performance
    of the ensemble of all six estimators is often better than the performances of
    each individual estimator.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们聚合越来越多的模型，我们看到集成泛化得越来越好。然而，我们实验中最引人注目的结果是，所有六个估计器的集成性能通常比每个单个估计器的性能都要好。
- en: Finally, what of fit versus complexity? It’s difficult to characterize the complexity
    of the ensemble, as different types of estimators in our ensemble have different
    complexities. However, we can characterize the *variance* of the ensemble.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，拟合与复杂性的关系如何？描述集成的复杂性是困难的，因为我们的集成中不同类型的估计器具有不同的复杂性。然而，我们可以描述集成的*方差*。
- en: Recall that variance of an estimator reflects its sensitivity to the data. A
    high variance estimator is highly sensitive and less robust, often because it’s
    overfitting. In figure 1.9, we show the variance of the ensembles from figure
    1.8, which is the width of the band.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，估计器的方差反映了其对数据的敏感性。高方差估计器非常敏感且鲁棒性较差，通常是因为它过度拟合。在图1.9中，我们显示了图1.8中集成的方差，即带宽的宽度。
- en: '![CH01_F09_Kunapuli](../Images/CH01_F09_Kunapuli.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F09_Kunapuli](../Images/CH01_F09_Kunapuli.png)'
- en: Figure 1.9 The mean performance of the ensemble combinations increases, showing
    that bigger ensembles perform better. The standard deviation (square root of the
    variance) of the performance of ensemble combinations decreases, showing that
    the overall variance decreases!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 集成组合的平均性能增加，表明更大的集成表现更好。性能组合的标准差（方差的平方根）降低，表明整体方差降低！
- en: 'As ensemble size increases, the variance of the ensemble decreases! This is
    a consequence of model aggregation or averaging. We know that averaging “smooths
    out the rough edges.” In the case of our ensemble, averaging individual predictions
    smooths out mistakes made by individual base estimators, replacing them instead
    with the wisdom of the ensemble: from many, one. The overall ensemble is more
    robust to mistakes and, unsurprisingly, generalizes better than any single base
    estimator.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 随着集成大小的增加，集成的方差减小！这是模型聚合或平均化的结果。我们知道平均“可以平滑粗糙的边缘。”在我们的集成中，平均单个预测可以平滑掉单个基础估计器所犯的错误，取而代之的是集成的智慧：从多到一。整体集成对错误的鲁棒性更强，并且不出所料，比任何单个基础估计器泛化得更好。
- en: Each component estimator in the ensemble is an individual, like one of Dr. Forrest’s
    residents, and each makes predictions based on its own experiences (introduced
    during learning). At prediction time, when we have six individuals, we’ll have
    six predictions, or six opinions. For “easy examples,” the individuals will mostly
    agree. For “difficult examples,” the individuals will differ among each other
    but, on average, are more likely to be closer to the correct answer.[⁵](#pgfId-1142507)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 集成中的每个组件估计器都是独立的，就像福雷斯特博士的住院医生之一一样，每个估计器都根据自己的经验（在学习过程中引入）进行预测。在预测时间，当我们有六个个体时，我们将有六个预测，或六个观点。对于“简单例子”，个体之间将大部分达成一致。对于“困难例子”，个体之间会有所不同，但平均而言，更可能接近正确答案。[⁵](#pgfId-1142507)
- en: In this simple scenario, we trained six “diverse” models by using six different
    learning algorithms. Ensemble diversity is critical to the success of the ensemble
    as it ensures that the individual estimators are different from each other and
    don’t all make the same mistakes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的场景中，我们通过使用六种不同的学习算法训练了六个“多样化”的模型。集成多样性对于集成成功至关重要，因为它确保了各个估计器彼此不同，并且不会犯相同的错误。
- en: As we’ll see over and over again in each chapter, different ensemble methods
    take different approaches to train diverse ensembles. Before we end this chapter,
    let’s take a look at a broad classification of various ensembling techniques,
    many of which will be covered in the next few chapters.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在每一章中都会反复看到的，不同的集成方法采用不同的方法来训练多样化的集成。在我们结束这一章之前，让我们来看看各种集成技术的广泛分类，其中许多将在接下来的几章中介绍。
- en: 1.5 Terminology and taxonomy for ensemble methods
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 集成方法的术语和分类法
- en: All ensembles are composed of individual machine-learning models called *base
    models*, *base learners*, or *base estimators* (these terms are used interchangeably
    throughout the book) and are trained using *base machine-learning algorithms*.
    Base models are often described in terms of their complexity. Base models that
    are sufficiently complex (e.g., a deep decision tree) and have “good” prediction
    performance (e.g., accuracy over 80% for a binary classification task) are typically
    known as strong learners or *strong models*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所有集成都是由称为*基础模型*、*基础学习器*或*基础估计器*（这些术语在本书中可以互换使用）的单独机器学习模型组成的，并且使用*基础机器学习算法*进行训练。基础模型通常用它们的复杂性来描述。足够复杂（例如，深层决策树）并且具有“良好”预测性能（例如，对于二元分类任务的准确率超过80%）的基础模型通常被称为强学习器或*强模型*。
- en: In contrast, base models that are pretty simple (e.g., a shallow decision tree)
    and achieve barely acceptable performance (e.g., accuracy around 51% for a binary
    classification task) are known as *weak learners* or *weak models*. More formally,
    a weak learner only has to do slightly better than random chance, or 50% for a
    binary classification task. As we’ll see shortly, ensemble methods use either
    weak learners or strong learners as base models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，那些相当简单（例如，浅层决策树）且仅能实现勉强可接受性能（例如，对于二元分类任务的准确率约为51%）的基础模型被称为*弱学习器*或*弱模型*。更正式地说，弱学习器只需比随机机会略好，或者对于二元分类任务来说，就是50%。正如我们很快就会看到的，集成方法要么使用弱学习器，要么使用强学习器作为基础模型。
- en: 'More broadly, ensemble methods can be classified into two types depending on
    how they are trained: *parallel* and *sequential ensembles*. This is the taxonomy
    we’ll adopt in this book as it gives us a neat way of grouping the vast number
    of ensemble methods out there (see figure 1.10).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，根据它们的训练方式，集成方法可以分为两种类型：*并行*和*顺序集成*。这是我们将在本书中采用的分类法，因为它为我们提供了一个整洁的方式来分组大量现有的集成方法（见图1.10）。
- en: '![CH01_F10_Kunapuli](../Images/CH01_F10_Kunapuli.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F10_Kunapuli](../Images/CH01_F10_Kunapuli.png)'
- en: Figure 1.10 A taxonomy of ensemble methods covered in this book
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 本书涵盖的集成方法分类
- en: 'Parallel ensemble methods, as the name suggests, train each component base
    model *independently* of the others, which means that they can be trained in parallel.
    Parallel ensembles are often constructed out of strong learners and can further
    be categorized into the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，并行集成方法独立于其他模型训练每个组件基础模型，这意味着它们可以并行训练。并行集成通常由强学习器构成，并且可以进一步分为以下几类：
- en: '*Homogeneous parallel ensembles*—All the base learners are of the same type
    (e.g., all decision trees) and trained using the same base-learning algorithm.
    Several well-known ensemble methods, such as bagging, random forests, and extremely
    randomized trees (Extra Trees), are parallel ensemble methods. These are covered
    in chapter 2.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同质并行集成*——所有基学习器都是同一类型（例如，所有决策树）并且使用相同的基学习算法进行训练。一些著名的集成方法，如bagging、随机森林和极端随机树（Extra
    Trees），都是并行集成方法。这些内容在第2章中有详细说明。'
- en: '*Heterogeneous parallel ensembles*—The base learners are trained using different
    base-learning algorithms. Meta-learning by stacking is a well-known exemplar of
    this type of ensembling technique. These are covered in chapter 3.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异质并行集成*——基学习器使用不同的基学习算法进行训练。通过堆叠进行元学习是这种集成技术的一个著名例子。这些内容在第3章中有详细说明。'
- en: 'Sequential ensemble methods, unlike parallel ensemble methods, exploit the
    dependence of base learners. More specifically, during training, sequential ensembles
    train a new base learner in such a manner that it minimizes mistakes made by the
    base learner trained in the previous step. These methods construct ensembles sequentially
    in stages and often use weak learners as base models. They can also be further
    categorized into the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与并行集成方法不同，顺序集成方法利用基学习器之间的依赖关系。更具体地说，在训练过程中，顺序集成以这种方式训练新的基学习器，即最小化前一步训练的基学习器所犯的错误。这些方法按阶段顺序构建集成，通常使用弱学习器作为基模型。它们还可以进一步分为以下几类：
- en: '*Adaptive boosting ensembles*—Also called vanilla boosting, these ensembles
    train successive base learners by reweighting examples adaptively to fix mistakes
    in previous iterations. AdaBoost, the granddaddy of all the boosting methods,
    is an example of this type of ensemble method. These are covered in chapter 4.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自适应提升集成*——也称为普通提升，这些集成通过自适应地重新加权示例来训练后续的基学习器，以纠正前一轮迭代中的错误。AdaBoost，所有提升方法的鼻祖，是这种集成方法的例子。这些内容在第4章中有详细说明。'
- en: '*Gradient-boosting ensembles*—These ensembles extend and generalize the idea
    of adaptive boosting and aim to mimic gradient descent, which is often used under
    the hood to actually train machine-learning models. Some of the most powerful
    modern ensemble learning packages implement some form of gradient boosting (LightGBM,
    chapter 5), Newton boosting (XGBoost, chapter 6), or ordered boosting (CatBoost,
    chapter 8).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度提升集成*——这些集成扩展并推广了自适应提升的想法，旨在模拟梯度下降，这是在底层实际训练机器学习模型时经常使用的。一些最强大的现代集成学习包实现了某种形式的梯度提升（LightGBM，第5章）、牛顿提升（XGBoost，第6章）或有序提升（CatBoost，第8章）。'
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Ensemble learning aims to improve predictive performance by training multiple
    models and combining them into a meta-estimator. The component models of an ensemble
    are called base estimators or base learners.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习方法旨在通过训练多个模型并将它们组合成一个元估计器来提高预测性能。集成中的组件模型被称为基估计器或基学习器。
- en: Ensemble methods use the power of “the wisdom of crowds,” which relies on the
    principle that the collective opinion of a group is more effective than any single
    individual in the group.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法利用“群体智慧”的力量，这一原则基于集体意见比群体中任何单个个体更有效的观点。
- en: Ensemble methods are widely used in several application areas, including financial
    and business analytics, medicine and health care, cybersecurity, education, manufacturing,
    recommendation systems, entertainment, and many more.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法在多个应用领域得到广泛应用，包括金融和商业分析、医疗保健、网络安全、教育、制造、推荐系统、娱乐等。
- en: Most machine-learning algorithms contend with a fit versus complexity (also
    called bias-variance) tradeoff, which affects their ability to generalize well
    to future data. Ensemble methods use multiple component models to overcome this
    tradeoff.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数机器学习算法都面临着拟合与复杂度（也称为偏差-方差）之间的权衡，这影响了它们对未来数据的泛化能力。集成方法使用多个组件模型来克服这种权衡。
- en: 'An effective ensemble requires two key ingredients: (1) ensemble diversity
    and (2) model aggregation for the final predictions.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有效的集成需要两个关键要素：(1) 集成多样性和(2) 对最终预测的模型聚合。
- en: '* * *'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) Andreas Töscher, Michael Jahrer, and Robert M. Bell, “The BigChaos Solution
    to the Netflix Grand Prize,” ([http://mng.bz/9V4r](http://mng.bz/9V4r)).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) 安德烈亚斯·托舍尔（Andreas Töscher）、迈克尔·亚雷尔（Michael Jahrer）和罗伯特·M·贝尔（Robert M. Bell），《Netflix大奖赛的BigChaos解决方案》([http://mng.bz/9V4r](http://mng.bz/9V4r))。
- en: '^(2.) Randal S. Olson, William La Cava, Zairah Mustahsan, Akshay Varik, and
    Jason H. Moore, *Data-driven Advice for Applying Machine Learning to Bioinformatics
    Problems*, Pacific Symposium on Machine Learning (2018); arXiv preprint: [https://arxiv.org/abs/1708.05070](https://arxiv.org/abs/1708.05070).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (2.) Randal S. Olson, William La Cava, Zairah Mustahsan, Akshay Varik, 和 Jason
    H. Moore，*将机器学习应用于生物信息学问题的数据驱动建议*，太平洋机器学习研讨会（2018）；arXiv预印本：[https://arxiv.org/abs/1708.05070](https://arxiv.org/abs/1708.05070)。
- en: ^(3.) For more details about learning with decision trees, see chapters 3 (classification)
    and 9 (regression) of *Machine Learning in Action* by Peter Harrington (Manning,
    2012).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: (3.) 关于使用决策树进行学习的更多细节，请参阅Peter Harrington所著的《机器学习实战》第3章（分类）和第9章（回归）（Manning,
    2012）。
- en: ^(4.) For more details on SVMs for classification, see chapter 6 of *Machine
    Learning in Action* by Peter Harrington (Manning, 2012). For SVMs for regression,
    see “A Tutorial on Support Vector Regression” by Alex J. Smola and Bernhard Scholköpf
    (*Statistics and Computing*, 2004), as well as the documentation pages of sklearn.SVM.SVR().
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (4.) 关于分类中的SVM的更多细节，请参阅Peter Harrington所著的《机器学习实战》第6章（Manning, 2012）。关于回归中的SVM，请参阅Alex
    J. Smola和Bernhard Scholköpf的《支持向量回归教程》(*Statistics and Computing*, 2004)，以及sklearn.SVM.SVR()的文档页面。
- en: '^(5.) There are cases when this breaks down. In the UK version of *Who Wants
    To Be A Millionaire?*, a contestant successfully made it as far as £125,000 (or
    about $160,000), when he was asked which novel begins with the words: “3 May.
    Bistritz. Left Munich at 8:35 PM.” After using the 50/50 lifeline, he was left
    with only two choices: *Tinker Tailor Soldier Spy and Dracula*. Knowing he could
    lose £93,000 if he got it wrong, he asked the studio audience. In response, 81%
    of the audience voted for *Tinker Tailor Soldier Spy*. The audience was overwhelmingly
    confident and—unfortunately for the contestant—overwhelmingly wrong. As you’ll
    see in the book, we look to avoid this situation by making certain assumptions
    about the “audience,” which, in our case, is the base estimators.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: (5.) 有时候这种情况会出现问题。在英国版的《谁想成为百万富翁？》中，一位参赛者成功晋级到125,000英镑（约合16万美元），当被问到哪本小说以“5月3日。比斯特里茨。晚上8:35离开慕尼黑。”开头时，他使用了50/50的生命线，只剩下两个选择：*《间谍之影》和《德古拉》*。知道如果答错可能会损失93,000英镑，他向现场观众求助。作为回应，81%的观众投票选择了*《间谍之影》*。观众们信心满满——不幸的是，他们错了。正如你在书中看到的，我们通过做出关于“观众”的某些假设来避免这种情况，在我们的案例中，这些假设是基于基础估计的。
