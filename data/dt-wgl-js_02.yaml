- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Acquisition, storage, and retrieval
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 获取、存储和检索
- en: '**This chapter covers**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Structuring data pipelines around a design pattern called the core data representation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 围绕称为核心数据表示的设计模式来构建数据管道
- en: Importing and exporting JSON and CSV data from text files and REST APIs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本文件和REST API导入和导出JSON和CSV数据
- en: Importing and exporting data with MySQL and MongoDB databases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MySQL和MongoDB数据库导入和导出数据
- en: Creating flexible pipelines to convert data between different formats
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建灵活的管道以在不同格式之间转换数据
- en: 'Chapter 3 covers a topic that’s crucial to the data-wrangling process: the
    ability to acquire data from somewhere and then store it locally so we can work
    with it efficiently and effectively.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章涵盖了数据整理过程中至关重要的一个主题：从某处获取数据并将其本地存储，以便我们能够高效有效地处理它。
- en: 'Initially, we must import our data from somewhere: this is *acquisition*. We’ll
    probably then export the data to a database to make it convenient to work with:
    this is *storage*. We might then export the data to various other formats for
    reporting, sharing, or backup. Ultimately, we must be able to access our data
    to work with it: this is *retrieval*.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，我们必须从某处导入我们的数据：这是*获取*。我们可能然后将数据导出到数据库，以便于处理：这是*存储*。我们可能然后将数据导出到各种其他格式，用于报告、共享或备份。最终，我们必须能够访问我们的数据以进行处理：这是*检索*。
- en: In chapter 1 we looked at an example of the data-wrangling process where data
    was imported from a MySQL database and exported to a MongoDB database. This is
    one possible scenario. How you work in any given situation depends on how the
    data is delivered to you, the requirements of your project, and the data formats
    and storage mechanisms that you choose to work with.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们查看了一个数据整理过程的示例，其中数据从MySQL数据库导入并导出到MongoDB数据库。这是一个可能的场景。你在任何特定情况下的工作方式取决于数据是如何交付给你的，你项目的需求，以及你选择与之一起工作的数据格式和存储机制。
- en: 'In this chapter, we discuss building a flexible data pipeline that can handle
    a variety of different formats and storage mechanisms. This is to show you the
    range of possibilities. In any real project, you probably wouldn’t work with a
    large variety of formats. You might, for example, work with only two or three
    of these data formats, but I believe it’s good to know all your options: after
    all, you never know what’s around the corner, and we need a process in place that
    can handle whatever kind of data might come your way.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论构建一个灵活的数据管道，它可以处理各种不同的格式和存储机制。这是为了展示各种可能性。在任何实际项目中，你可能不会处理大量的格式。例如，你可能只处理这些数据格式中的两到三种，但我认为了解所有选项是好的：毕竟，你永远不知道下一步会发生什么，我们需要一个能够处理可能出现的任何类型数据的流程。
- en: This chapter is basic—it’s about data pipeline fundamentals. As you read through
    it and try out these techniques, you might wonder how they scale to large amounts
    of data. The techniques presented in this chapter work with reasonably large data
    sets, but there does come a point where our data is so large that these techniques
    will start to break down. We’ll come back and discuss these issues of scale in
    chapters 7 and 8 when we come to working with large data sets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是基础——关于数据管道的基本知识。当你阅读并通过这些技术尝试时，你可能会想知道这些技术如何扩展到大量数据。本章中提出的技术可以处理合理大小的数据集，但确实存在一个点，我们的数据变得如此之大，这些技术将开始崩溃。我们将在第7章和第8章中回到这些问题，届时我们将处理大量数据集。
- en: 3.1 Building out your toolkit
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 构建您的工具包
- en: Through this chapter we’ll look at the tools you need to move data from place
    to place. We’ll use Node.js and various third-party libraries. [Table 3.1](#table3.1)
    lists the tools we’ll use.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们将探讨您需要将数据从一个地方移动到另一个地方的工具。我们将使用Node.js和各种第三方库。[表3.1](#table3.1)列出了我们将使用的工具。
- en: Please note that this is only the tip of the iceberg! These modules are installed
    through the Node.js package manager (npm) and are a tiny taste of the many tools
    within arm’s reach of any Node.js developer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这仅仅是冰山一角！这些模块是通过Node.js包管理器（npm）安装的，并且只是任何Node.js开发者可触及的众多工具中的一小部分。
- en: Table 3.1 Chapter 3 tools
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 第3章工具
- en: '| **Type** | **Data Source** | **Data Format** | **Tools** | **Methods** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **类型** | **数据源** | **数据格式** | **工具** | **方法** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Import | Text file | JSON | Node.js API | `fs.readFile,` `JSON.parse` |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 导入 | 文本文件 | JSON | Node.js API | `fs.readFile,` `JSON.parse` |'
- en: '|  |  | CSV | Node.js API, `PapaParse` | `fs.readFile` `Papa.parse` |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CSV | Node.js API, `PapaParse` | `fs.readFile` `Papa.parse` |'
- en: '|  | REST API | JSON | `request-promise` | `request.get` |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | REST API | JSON | `request-promise` | `request.get` |'
- en: '|  |  | CSV | `request-promise, PapaParse` | `request.get, Papa.parse` |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CSV | `request-promise, PapaParse` | `request.get, Papa.parse` |'
- en: '|  | Database | MongoDB | `promised-mongo` | `<database>.find` |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据库 | MongoDB | `promised-mongo` | `<database>.find` |'
- en: '|  |  | MySQL | `nodejs-mysql` | `<database>.exec` |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MySQL | `nodejs-mysql` | `<database>.exec` |'
- en: '| Export | Text file | JSON | Node.js API | `fs.writeFile,` `JSON.stringify`
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 导出 | 文本文件 | JSON | Node.js API | `fs.writeFile,` `JSON.stringify` |'
- en: '|  |  | CSV | Node.js API, `PapaParse` | `fs.writeFile,` `Papa.unparse` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CSV | Node.js API, `PapaParse` | `fs.writeFile,` `Papa.unparse` |'
- en: '|  | Database | MongoDB | `promised-mongo` | `<database>.insert` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据库 | MongoDB | `promised-mongo` | `<database>.insert` |'
- en: '|  |  | MySQL | `nodejs-mysql` | `<database>.exec` |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MySQL | `nodejs-mysql` | `<database>.exec` |'
- en: In this chapter, and indeed throughout the book, we’ll continue to build our
    toolkit. This is important because we’ll use it again and again on future projects.
    As we work through various examples, we’ll create a library of Node.js functions
    for working with data in JavaScript.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，实际上在整个书中，我们将继续构建我们的工具集。这很重要，因为我们将反复在未来的项目中使用它。随着我们通过各种示例进行工作，我们将创建一个Node.js函数库，用于在JavaScript中处理数据。
- en: 3.2 Getting the code and data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 获取代码和数据
- en: The data theme of this chapter is earthquakes with data downloaded from the
    United States Geological Survey website. Additional data was downloaded from the
    Seismi earthquake data visualization project. Please note that the Seismi website
    no longer appears to be operational.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的数据主题是地震，数据是从美国地质调查网站下载的。还从Seismi地震数据可视化项目下载了附加数据。请注意，Seismi网站似乎不再运行。
- en: The code and data for this chapter are available in the Chapter-3 repository
    in the Data Wrangling with JavaScript GitHub organization at [https://github.com/data-wrangling-with-javascript/chapter-3](https://github.com/data-wrangling-with-javascript/chapter-3).
    Please download the code and install the dependencies. Refer back to “Getting
    the code and data”*in chapter 2 if you need help with this.*
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和数据可在Data Wrangling with JavaScript GitHub组织中的第3章仓库[https://github.com/data-wrangling-with-javascript/chapter-3](https://github.com/data-wrangling-with-javascript/chapter-3)中找到。请下载代码并安装依赖项。如果您需要帮助，请参考“获取代码和数据”*在第2章中*。
- en: '*The Chapter-3 code repository and most others for this book are a bit different
    to what you saw in chapter 2\. They contain the code for each code listing in
    separate JavaScript files in the same directory, and they’re named according to
    the listing number, for example, listing_3.1.js, listing_3.3.js, and so on. You
    can install all third-party dependencies for all code listings at once by running
    `npm install` once in the root directory of the repository. The *toolkit* subdirectory
    contains the toolkit functions that we’ll create in this chapter.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3章代码仓库以及本书中的大多数其他代码仓库与第2章中您所看到的不同。它们将每个代码列表的代码分别放在同一目录下的单独JavaScript文件中，并且根据列表编号命名，例如，listing_3.1.js，listing_3.3.js，等等。您可以通过在仓库的根目录中运行一次`npm
    install`来一次性安装所有代码列表的所有第三方依赖项。**工具集**子目录包含我们在本章中创建的工具函数。'
- en: Later in this chapter we’ll work with databases. Database setup can be complicated,
    so to make things convenient, the GitHub repository for chapter 3 includes Vagrant
    scripts that boot up virtual machines complete with databases and example data.
    We’ll talk more about Vagrant later in the chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将处理数据库。数据库设置可能很复杂，因此为了方便起见，第3章的GitHub仓库包含了Vagrant脚本，这些脚本可以启动带有数据库和示例数据的虚拟机。我们将在本章的后面部分更多地讨论Vagrant。
- en: 3.3 The core data representation
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 核心数据表示
- en: I’d like to introduce you to the *core data representation* (CDR). This is a
    design pattern for structuring data pipelines. The CDR allows us to piece together
    flexible data pipelines from reusable code modules. With this design pattern,
    we can produce an almost infinite variety of data processing and conversion pipelines.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我想向您介绍**核心数据表示**（CDR）。这是一种用于结构化数据管道的设计模式。CDR允许我们使用可重用的代码模块灵活地构建数据管道。使用这种设计模式，我们可以产生几乎无限多样的数据处理和转换管道。
- en: The stages in our data pipeline use the CDR to communicate; you might say the
    CDR is the glue that binds together our data pipeline (see [figure 3.1](#figure3.1)).
    The CDR is a shared representation of our data, and its purpose is to allow our
    pipeline stages to communicate and be cleanly separated with no hard dependencies
    on each other. This separation is what allows us to build reusable code modules
    that we can then rearrange to create other data pipelines.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据处理管道中的阶段使用CDR进行通信；你可以说CDR是我们数据处理管道的粘合剂（见图3.1）。CDR是我们数据的共享表示，其目的是允许我们的管道阶段进行通信，并且能够干净地分离，彼此之间没有硬依赖。这种分离使我们能够构建可重用代码模块，然后我们可以重新排列它们来创建其他数据处理管道。
- en: '![c03_01.eps](Images/c03_01.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![c03_01.eps](Images/c03_01.png)'
- en: '[Figure 3.1](#figureanchor3.1) A data pipeline with stages that communicate
    through the core data representation'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.1](#figureanchor3.1) 通过核心数据表示进行通信的阶段数据处理管道'
- en: The separation of the stages also gives us flexibility—we can restructure our
    data pipeline by rearranging the stages or by adding and removing stages. These
    modifications are easily made because the stages are only dependent on the CDR,
    and they don’t require any particular sequence of preceding stages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段的分离也为我们提供了灵活性——我们可以通过重新排列阶段或添加和删除阶段来重构我们的数据处理管道。这些修改很容易进行，因为阶段只依赖于CDR，并且不需要任何特定的先前阶段顺序。
- en: In this chapter, we’ll use the CDR to bridge the gap between our import and
    export code. This allows us to piece together data conversion pipelines from reusable
    code modules. We can mix and match import and export code to build a pipeline
    that converts data from any one format to any other.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用CDR来弥合导入和导出代码之间的差距。这使我们能够从可重用代码模块中拼接数据转换管道。我们可以混合和匹配导入和导出代码，构建一个可以将数据从任何一种格式转换为另一种格式的管道。
- en: 3.3.1 The earthquakes website
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 地震网站
- en: Let’s start with an example to help understand the CDR. Let’s say that we’re
    maintaining a website that reports on global earthquake activity. The site collates
    data on the world’s earthquakes from various sources into a central location.
    It’s useful for researchers and concerned citizens to have one place from which
    to obtain news and data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个例子开始，以帮助理解CDR。假设我们正在维护一个报告全球地震活动的网站。该网站从各种来源收集世界各地的地震数据，并将其汇总到一个中心位置。对于研究人员和关心公民来说，有一个地方可以获取新闻和数据是非常有用的。
- en: Where does the data come from? Let’s say that our website must read data from
    a variety of different sources and in many different formats. Flexibility is key.
    We must accept data from other websites and organizations in whatever format they
    provide it. We also want to be a good data sharing citizen, so not only do we
    make the data available through web pages and visualizations, we also want to
    make the data available in various machine-readable formats. Put succinctly, we
    must both import and export a variety of formats into and out of our data pipeline.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据从哪里来？假设我们的网站必须从各种不同的来源和多种不同的格式读取数据。灵活性是关键。我们必须接受来自其他网站和组织的数据，无论它们以何种格式提供。我们还希望成为一个好的数据共享公民，因此我们不仅通过网页和可视化使数据可用，还希望以各种机器可读的格式提供数据。简而言之，我们必须将各种格式导入和导出到我们的数据处理管道中。
- en: Let’s look at the import and export of one particular data format. Say we’ve
    imported the data file earthquakes.csv to the CDR. It’s going to look like what’s
    shown in figures 3.2 and 3.3.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一种特定数据格式的导入和导出。假设我们已经将数据文件earthquakes.csv导入到CDR中。它将看起来如图3.2和图3.3所示。
- en: 'The CDR should be simple to understand: after all it’s just a JavaScript array
    of data. Each array element corresponds to a row in earthquakes.csv (as illustrated
    in [figure 3.2](#figure3.2)). Each array element contains a JavaScript object,
    or a record if you will, and each field corresponds to a column in earthquakes.csv
    (as illustrated in [figure 3.3](#figure3.3)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CDR应该很容易理解：毕竟它只是一个数据JavaScript数组。每个数组元素对应于earthquakes.csv中的一行（如图3.2所示）。每个数组元素包含一个JavaScript对象，或者你可以称之为记录，每个字段对应于earthquakes.csv中的一列（如图3.3所示）。
- en: '![c03_02.eps](Images/c03_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![c03_02.eps](Images/c03_02.png)'
- en: '[Figure 3.2](#figureanchor3.2) Elements in a JavaScript array correspond to
    rows in earthquakes.csv.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.2](#figureanchor3.2) JavaScript数组中的元素对应于earthquakes.csv中的行。'
- en: To create a data conversion pipeline, we must import from a data format and
    then export to another. As one example, let’s take earthquakes.csv and import
    it into a MongoDB earthquakes database. To do this, we’ll need code to import
    the data from the CSV file and then code to export the data to the MongoDB database.
    We’ll look at the code soon enough; for now, note in [figure 3.4](#figure3.4)
    how the data is fed from import to export using the core data representation that
    sits in the middle.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建数据转换管道，我们必须从一个数据格式导入，然后导出到另一个格式。作为一个例子，让我们以 earthquakes.csv 为例，将其导入 MongoDB
    的地震数据库。为此，我们需要导入 CSV 文件中的数据的代码，然后是导出数据到 MongoDB 数据库的代码。我们很快就会看到代码；现在，注意在 [图 3.4](#figure3.4)
    中数据是如何通过位于中间的核心数据表示从导入到导出的。
- en: We aren’t interested only in CSV files and MongoDB databases. I’ve mentioned
    those as a specific example that illustrates how the CDR can connect our code
    for importing and exporting. We’re maintaining the earthquakes website, and we
    need to accept and share data in any format!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅对 CSV 文件和 MongoDB 数据库感兴趣。我提到这些作为特定例子，以说明 CDR 如何连接我们的导入和导出代码。我们正在维护地震网站，我们需要接受和分享任何格式的数据！
- en: '![c03_03.eps](Images/c03_03.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![c03_03.eps](Images/c03_03.png)'
- en: '[Figure 3.3](#figureanchor3.3) Fields in JavaScript objects correspond to columns
    in earthquakes.csv.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.3](#figureanchor3.3) JavaScript 对象中的字段对应于 earthquakes.csv 中的列。'
- en: '![c03_04.eps](Images/c03_04.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![c03_04.eps](Images/c03_04.png)'
- en: '[Figure 3.4](#figureanchor3.4) Import and export code feeds through the core
    data representation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.4](#figureanchor3.4) 导入和导出代码流通过核心数据表示。'
- en: 3.3.2 Data formats covered
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 涵盖的数据格式
- en: '[Table 3.2](#table3.2) shows the range of data formats we’ll cover in this
    chapter. By the end, you’ll have learned the basics for importing and exporting
    each of these common data formats through the core data representation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3.2](#table3.2) 展示了本章我们将涵盖的数据格式范围。到结束时，你将学会导入和导出这些常见数据格式的基本方法。'
- en: Table 3.2 Data formats covered in chapter 3
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2 第 3 章涵盖的数据格式
- en: '| **Data Format** | **Data Source** | **Notes** |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| **数据格式** | **数据来源** | **说明** |'
- en: '| --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| JSON | Text file, REST API | The JSON format is built into JavaScript. It’s
    convenient and most REST APIs use it. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| JSON | 文本文件，REST API | JSON 格式内置在 JavaScript 中。方便且大多数 REST API 都使用它。|'
- en: '| CSV is a more compact format than JSON and is compatible with Excel. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| CSV 格式比 JSON 更紧凑，且与 Excel 兼容。|'
- en: '| MongoDB | Database | Flexible and convenient, schema-free database. Ideal
    when you don’t yet know the format of your data. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| MongoDB | 数据库 | 灵活方便，无模式数据库。在你还不了解数据格式时非常理想。|'
- en: '| MySQL | Database | Standard relational database. Mature, robust, and reliable.
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| MySQL | 数据库 | 标准关系型数据库。成熟、健壮、可靠。|'
- en: The main idea that I’d like to impart to you is that we can easily plug a variety
    of data formats into our workflow as and when we need them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要传达给你的主要思想是，我们可以很容易地将各种数据格式插入到我们的工作流程中，只要我们需要它们。
- en: In this book you learn a common, but necessarily limited, set of data formats,
    but it may not cover your favorite data format. For example, I’ve been asked about
    XML, Microsoft SQL, PostgreSQL, and Oracle. It’s not an aim for this book to cover
    every conceivable data source; that would quickly get boring, so instead we’ll
    focus on a representative and commonly used set of data formats.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你将学习一组常见但必要的有限数据格式，但这可能不会涵盖你最喜欢的数据格式。例如，有人问过我关于 XML、Microsoft SQL、PostgreSQL
    和 Oracle。本书的目标不是涵盖所有可能的数据源；那样会很快变得无聊，所以我们将专注于一组代表性且常用的数据格式。
- en: CSV is here because it’s so common in data analysis projects. JSON is here because
    it’s so common in JavaScript (and it’s so dang convenient). I use MongoDB to represent
    the NoSQL class of databases. And finally, I use MySQL to represent the SQL class
    of databases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 存在的原因是它在数据分析项目中非常常见。JSON 存在的原因是它在 JavaScript 中非常常见（而且非常方便）。我使用 MongoDB 来表示
    NoSQL 类型的数据库。最后，我使用 MySQL 来表示 SQL 类型的数据库。
- en: 3.3.3 Power and flexibility
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 力量和灵活性
- en: Have you understood the power of the CDR design pattern yet? Have a look in
    [figure 3.5](#figure3.5) at how the data formats fit together. Notice the range
    of data formats that can be imported into the CDR and then the range of data formats
    that can be exported from it. By wiring together modular import and export code
    (communicating using the CDR), we can now build a large variety of data conversion
    pipelines.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经理解了CDR设计模式的强大之处了吗？看看[图3.5](#figure3.5)中数据格式是如何相互配合的。注意可以导入到CDR中的数据格式范围以及可以从CDR导出的数据格式范围。通过连接模块化的导入和导出代码（使用CDR进行通信），我们现在可以构建各种数据转换管道。
- en: '![c03_05.eps](Images/c03_05.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![c03_05.eps](Images/c03_05.png)'
- en: '[Figure 3.5](#figureanchor3.5) Select from a variety of data formats to build
    a custom data conversion process.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.5](#figureanchor3.5) 从多种数据格式中选择以构建自定义数据转换过程。'
- en: Need to import JSON and export MongoDB? No problem, we can do that! How about
    importing from a REST API and exporting to CSV? We can do that as well! Using
    the CDR design pattern, we can easily stitch together whatever data conversion
    we need to import from any data format on the left ([figure 3.5](#figure3.5))
    and export to any on the right.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 需要导入JSON并导出MongoDB？没问题，我们可以做到！从REST API导入并导出到CSV呢？我们同样可以做到！使用CDR设计模式，我们可以轻松地将所需的数据转换连接起来，无论是从左侧的任何数据格式([图3.5](#figure3.5))导入，还是导出到右侧的任何格式。
- en: 3.4 Importing data
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 导入数据
- en: 'Let’s start by importing data to the CDR. We’ll first understand how to load
    data from text files and REST APIs. Both are commonly found in business and data
    science scenarios. After loading text data—either from text file or REST API—we
    need to parse, or interpret, it according to a particular data format. That’s
    usually JSON or CSV, two common text formats. We’ll finish by loading data from
    two different types of databases: MongoDB and MySQL.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从将数据导入CDR开始。我们首先将了解如何从文本文件和REST API加载数据。这两种方式在商业和数据科学场景中都很常见。在加载文本数据——无论是从文本文件还是REST
    API——之后，我们需要根据特定的数据格式对其进行解析或解释。这通常是JSON或CSV，两种常见的文本格式。最后，我们将从两种不同类型的数据库加载数据：MongoDB和MySQL。
- en: 3.4.1 Loading data from text files
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 从文本文件加载数据
- en: We’re starting with text files—probably the simplest data storage mechanism—they’re
    easy to understand and in common use. Through this section we learn to load text
    files into memory. Ultimately, we need to parse, or interpret, the data from the
    file depending on the data format, but let’s first focus on loading from a file,
    and we’ll come back to parsing after we also see how to load text data from a
    REST API.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从文本文件开始——这可能是最简单的数据存储机制——它们易于理解且普遍使用。在本节中，我们将学习如何将文本文件加载到内存中。最终，我们需要根据数据格式解析或解释文件中的数据，但首先让我们专注于从文件中加载，之后我们再回到解析的话题，届时我们也会看到如何从REST
    API加载文本数据。
- en: The general process of importing a text file to the core data representation
    is illustrated in [figure 3.6](#figure3.6). Toward the right of the diagram, notice
    the pathway branches; this is where we interpret the incoming data as a particular
    format and decode it to the CDR. For the moment, though, let’s load the text file
    into memory.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本文件导入核心数据表示的一般过程如图3.6所示。在图表的右侧，注意路径分支；这就是我们将传入的数据解释为特定格式并将其解码到CDR的地方。不过，目前我们先加载文本文件到内存中。
- en: In Node.js, we use the `fs.readFile` function to read the file’s content into
    memory. How we parse the file varies according to the data format, but reading
    the text file into memory is the same in each case, an example of which is shown
    in [listing 3.1](#listing3.1). You can run this code, and it will print the contents
    of the file earthquakes.csv to the console.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在Node.js中，我们使用`fs.readFile`函数将文件内容读入内存。解析文件的方式根据数据格式而有所不同，但将文本文件读入内存的方式在每种情况下都是相同的，一个例子如[列表3.1](#listing3.1)所示。你可以运行此代码，它将打印出文件earthquakes.csv的内容到控制台。
- en: '![c03_06.eps](Images/c03_06.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![c03_06.eps](Images/c03_06.png)'
- en: '[Figure 3.6](#figureanchor3.6) Importing a text file to the CDR'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.6](#figureanchor3.6) 将文本文件导入CDR'
- en: Listing 3.1 Reading a text file into memory (listing-3.1.js)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1 将文本文件读入内存（listing-3.1.js）
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 3.1](#listing3.1) is a basic example of loading a text file in Node.js,
    but for the convenience of managing the asynchronous operation, we’ll now wrap
    this in a promise. We’re going to need boilerplate code that we’ll use each time
    we load a text file. We’ll reuse this code many times throughout the book, so
    let’s turn it into a reusable toolkit function.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.1](#listing3.1) 是在 Node.js 中加载文本文件的基本示例，但为了方便管理异步操作，我们现在将这个操作封装在 Promise
    中。我们需要一些样板代码，我们将在每次加载文本文件时使用这些代码。我们将在整本书中多次重用这些代码，所以让我们将其转换成一个可重用的工具函数。'
- en: The following listing is the first function in your toolkit. It lives in a file
    that I called file.js, and this defines a Node.js code module called `file`. For
    the moment, it contains the single function called `read`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表是您工具箱中的第一个函数。它位于一个我称为 file.js 的文件中，并定义了一个名为 `file` 的 Node.js 代码模块。目前，它包含一个名为
    `read` 的单个函数。
- en: Listing 3.2 A promise-based function to read a text file (toolkit/file.js)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 一个基于 Promise 的函数来读取文本文件（toolkit/file.js）
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 3.3](#listing3.3) is an example of how we can use our new `read` function.
    The `file` module is required, and we can now call `file.read` to load earthquakes.csv
    into memory. You can run the code, and it prints the file’s content to the console.
    You should compare the code for listings 3.1 and 3.3\. This will help you understand
    the differences between callback- and promise-based asynchronous coding.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.3](#listing3.3) 是我们如何使用新的 `read` 函数的一个示例。需要引入 `file` 模块，现在我们可以调用 `file.read`
    来将 earthquakes.csv 加载到内存中。你可以运行代码，它将文件内容打印到控制台。你应该比较列表 3.1 和 3.3 的代码。这将帮助你理解基于回调和基于
    Promise 的异步编程之间的区别。'
- en: Listing 3.3 Loading a text file with the promise-based read function (listing-3.3.js)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 使用基于 Promise 的读取函数加载文本文件（listing-3.3.js）
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Loading data from a text file illustrates one way of getting text data into
    memory; now let’s look at another way.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本文件加载数据说明了将文本数据放入内存的一种方法；现在让我们看看另一种方法。
- en: 3.4.2 Loading data from a REST API
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 从 REST API 加载数据
- en: We can load data from text files, so now let’s look at loading data from a REST
    (REpresentational State Transfer) API using HTTP (HyperText Transfer Protocol).
    This is a common way to retrieve data over the internet from a website or web
    service. Here again we’ll look at loading the data into memory; then we’ll come
    back and see how to interpret the data according to its format.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从文本文件中加载数据，所以现在让我们看看如何使用 HTTP（超文本传输协议）从 REST（表示状态传输）API 加载数据。这是从网站或网络服务通过互联网获取数据的一种常见方式。在这里，我们同样会将数据加载到内存中；然后我们再回来看看如何根据其格式解释数据。
- en: The general process of importing data from a REST API is illustrated in [figure
    3.7](#figure3.7). To get data by HTTP, we use the third-party library request-promise.
    The Node.js API has built-in support for HTTP communication, but I like to use
    the higher-level request-promise library because it’s easier, more convenient,
    and it wraps the operation in a promise for us.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从 REST API 导入数据的一般过程在 [图 3.7](#figure3.7) 中展示。要通过 HTTP 获取数据，我们使用第三方库 request-promise。Node.js
    API 内置了对 HTTP 通信的支持，但我更喜欢使用更高层次的 request-promise 库，因为它更简单、更方便，并且它为我们封装了操作在 Promise
    中。
- en: '![c03_07.eps](Images/c03_07.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![c03_07.eps](Images/c03_07.png)'
- en: '[Figure 3.7](#figureanchor3.7) Importing data from a REST API to the CDR'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.7](#figureanchor3.7) 从 REST API 导入数据到 CDR'
- en: 'To retrieve data from a REST API, we need to install request-promise. If you’re
    following along with code from GitHub and did the `npm install` in the code repository,
    you already have this dependency installed. If you need to install it in a fresh
    Node.js project, you can do it like this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从 REST API 获取数据，我们需要安装 request-promise。如果你正在跟随 GitHub 上的代码进行操作，并在代码仓库中执行了 `npm
    install`，那么你已经安装了这个依赖项。如果你需要在新的 Node.js 项目中安装它，你可以这样做：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that we installed both `request-promise` and `request` because one depends
    on the other as a peer dependency.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们安装了 `request-promise` 和 `request`，因为前者作为依赖项依赖于后者。
- en: As an example, we’re going to pull data from [https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/significant_month.geojson](https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/significant_month.geojson).
    You should open that link now, and you’ll see what the JSON data looks like in
    your web browser.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们将从 [https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/significant_month.geojson](https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/significant_month.geojson)
    拉取数据。你现在应该打开这个链接，你会在你的网络浏览器中看到 JSON 数据的样子。
- en: The simplest possible code to retrieve data via HTTP GET is shown in the following
    listing using `request-promise`’s `request.get` function to make a request to
    the REST API. You can run this code, and the retrieved data is printed to the
    console so that you can check it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了使用 `request-promise` 的 `request.get` 函数通过 HTTP GET 获取数据的简单代码。你可以运行此代码，检索到的数据将打印到控制台，以便你可以进行检查。
- en: Listing 3.4 Retrieving data from a REST API (listing-3.4.js)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.4 从 REST API 获取数据（listing-3.4.js）
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 3.4.3 Parsing JSON text data
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 解析 JSON 文本数据
- en: Now that we can load text data into memory, either from a text file or from
    a REST API, we must decide how to decode the content. Working with raw text data
    can be painful, time-consuming, and error-prone; however, when we work with a
    common or standardized data format such as JSON or CSV, we have the advantage
    of using an existing library to import or export the data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够将文本数据加载到内存中，无论是从文本文件还是从 REST API，我们必须决定如何解码内容。处理原始文本数据可能会很痛苦、耗时且容易出错；然而，当我们处理一个通用或标准化的数据格式，如
    JSON 或 CSV 时，我们可以利用现有的库来导入或导出数据。
- en: JSON is the first data format we’ll parse from our text data. It’s one of the
    most common data formats you’ll encounter when working with JavaScript. It’s simple
    to understand and goes hand-in-hand with JavaScript. The tools you need for working
    with JSON are built into the JavaScript API, and that makes JSON a particularly
    appealing format for us.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 是我们将从文本数据中解析的第一个数据格式。它是你在使用 JavaScript 时会遇到的最常见的数据格式之一。它易于理解，与 JavaScript
    密不可分。用于处理 JSON 的工具内置在 JavaScript API 中，这使得 JSON 对我们来说是一个特别吸引人的格式。
- en: Parsing a JSON text file
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解析 JSON 文本文件
- en: Before we attempt to import our data file, it’s a good idea to open the file
    in a text editor and visually verify that the data is what we think it is. There’s
    no point trying to work with a data file that’s corrupted or has other problems,
    and we can easily and quickly check for this before we start coding. This won’t
    catch all conceivable issues, but you might be surprised how many data issues
    you can spot by first doing a simple visual check. [Figure 3.8](#figure3.8) shows
    earthquakes.json loaded in Notepad++ (a text editor that I use on my Windows PC).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试导入数据文件之前，打开文件在文本编辑器中并直观地验证数据是否是我们所期望的是个好主意。尝试处理一个损坏或有其他问题的数据文件是没有意义的，我们可以在开始编码之前轻松快速地检查这一点。这不会捕捉到所有可能的问题，但你可能会惊讶于通过首先进行简单的视觉检查就能发现多少数据问题。[图
    3.8](#figure3.8) 展示了在 Notepad++（我在 Windows PC 上使用的文本编辑器）中加载的 earthquakes.json。
- en: '![c03_08.png](Images/c03_08.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![c03_08.png](Images/c03_08.png)'
- en: '[Figure 3.8](#figureanchor3.8) Earthquakes.json viewed in Notepad++'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.8](#figureanchor3.8) 在 Notepad++ 中查看 Earthquakes.json'
- en: Let’s now import earthquakes.json to the core data representation. This is particularly
    easy using the tools provided by Node.js and the JavaScript API. The JSON format
    is a serialized JavaScript data structure, so it lines up in a direct way with
    the core data representation. To read the file, we use our toolkit function `file.read`.
    Then we use the built-in JavaScript function `JSON.parse` to decode the text data
    to the CDR. This process is illustrated in [figure 3.9](#figure3.9).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将 earthquakes.json 导入到核心数据表示中。使用 Node.js 和 JavaScript API 提供的工具来做这件事尤其简单。JSON
    格式是序列化的 JavaScript 数据结构，因此它与核心数据表示直接对应。为了读取文件，我们使用我们的工具函数 `file.read`。然后我们使用内置的
    JavaScript 函数 `JSON.parse` 将文本数据解码到 CDR。这个过程在 [图 3.9](#figure3.9) 中展示。
- en: The following listing is a new function to import a JSON file to the core data
    representation. We read the file content using our function `file.read` and then
    parse the JSON data using `JSON.parse`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表是一个新的函数，用于将 JSON 文件导入到核心数据表示中。我们使用我们的函数 `file.read` 读取文件内容，然后使用 `JSON.parse`
    解析 JSON 数据。
- en: Listing 3.5 A function to import a JSON text file (toolkit/importJsonFile.js)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 导入 JSON 文本文件的函数（toolkit/importJsonFile.js）
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![c03_09.eps](Images/c03_09.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![c03_09.eps](Images/c03_09.png)'
- en: '[Figure 3.9](#figureanchor3.9) Importing a JSON text file to the CDR'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.9](#figureanchor3.9) 将 JSON 文本文件导入到 CDR'
- en: The following listing shows how to use our new function to import earthquakes.json.
    You can run this code, and the decoded data prints it to the console so that we
    can visually verify that the data was parsed correctly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了如何使用我们的新函数导入 earthquakes.json。你可以运行此代码，解码后的数据将打印到控制台，以便我们可以直观地验证数据是否正确解析。
- en: Listing 3.6 Importing data from earthquakes.json (listing-3.6.js)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 从 earthquakes.json 导入数据（listing-3.6.js）
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parsing JSON data from a REST API
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从 REST API 解析 JSON 数据
- en: Importing JSON data from a REST API is similar to importing it from a text file.
    We need to change where the data is loaded from. Instead of using the `file.read`
    function, we can use our `request-promise` to load the data from a REST API. The
    following listing shows a new function for our toolkit that imports JSON data
    from a REST API.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从 REST API 导入 JSON 数据与从文本文件导入类似。我们需要更改数据加载的位置。而不是使用 `file.read` 函数，我们可以使用我们的
    `request-promise` 从 REST API 加载数据。以下列表显示了一个用于工具包的新函数，用于从 REST API 导入 JSON 数据。
- en: Listing 3.7 Importing JSON data from a REST API (toolkit/importJsonFromRestApi.js)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 从 REST API 导入 JSON 数据 (toolkit/importJsonFromRestApi.js)
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 3.8](#listing3.8) shows how to call `importJsonFromRestApi` to import
    data from the example REST API that was also used earlier in [listing 3.4](#listing3.4).
    This code is similar to [listing 3.6](#listing3.6), but rather than loading the
    data from a file, it loads it from the REST API. Run this code and you’ll see
    how it operates, grabbing the data and then printing the decoded JSON data to
    the console so you can check that it worked as expected.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.8](#listing3.8) 展示了如何调用 `importJsonFromRestApi` 从之前在 [列表 3.4](#listing3.4)
    中也使用过的示例 REST API 导入数据。此代码与 [列表 3.6](#listing3.6) 类似，但不是从文件加载数据，而是从 REST API 加载数据。运行此代码，您将看到它是如何操作的，它会抓取数据，然后将解码后的
    JSON 数据打印到控制台，以便您可以检查它是否按预期工作。'
- en: Listing 3.8 Importing earthquakes data from a REST API (listing-3.8.js)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.8 从 REST API 导入地震数据 (listing-3.8.js)
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note in [listing 3.8](#listing3.8) how the incoming data is reorganized to fit
    our idea of the CDR. The incoming JSON data isn’t structured exactly how we’d
    like it to be to fit, so we rewrite on the fly into a tabular format.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 [列表 3.8](#listing3.8) 中，如何将传入的数据重新组织以符合我们对 CDR 的想法。传入的 JSON 数据的结构与我们希望它符合的结构并不完全一致，因此我们即时将其重写为表格格式。
- en: 3.4.4 Parsing CSV text data
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 解析 CSV 文本数据
- en: The next format we’ll look at is the CSV (comma-separated values) format. This
    simple format is in common use in the data science community. It directly represents
    tabular data and is a more compact representation than JSON.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要查看的格式是 CSV（逗号分隔值）格式。这种简单的格式在数据科学社区中很常见。它直接表示表格数据，并且比 JSON 格式更紧凑。
- en: Unfortunately, the tools we need to parse CSV files aren’t included with Node.js
    or JavaScript, but we can easily get what we need from npm. In this case, we’re
    going to install a great third-party library for parsing CSV files called Papa
    Parse.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们需要解析 CSV 文件的工具并未包含在 Node.js 或 JavaScript 中，但我们可以很容易地从 npm 获取所需的内容。在这种情况下，我们将安装一个名为
    Papa Parse 的优秀第三方库来解析 CSV 文件。
- en: Parsing a CSV text file
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解析 CSV 文本文件
- en: As with JSON, we should first should check that the content of our CSV file
    is well formed and not corrupted. We could look at the CSV file in Notepad++,
    like when we looked at the JSON file, but it’s worth noting that a CSV file can
    also be loaded as a spreadsheet! [Figure 3.10](#figure3.10) shows earthquakes.csv
    loaded in Excel.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与 JSON 类似，我们首先应该检查 CSV 文件的内容是否格式良好且未损坏。我们可以像查看 JSON 文件时一样查看 CSV 文件，但值得注意的是，CSV
    文件也可以作为工作表加载！[图 3.10](#figure3.10) 显示了 earthquakes.csv 在 Excel 中的加载情况。
- en: '![c03_10.png](Images/c03_10.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![c03_10.png](Images/c03_10.png)'
- en: '[Figure 3.10](#figureanchor3.10) Earthquakes.csv loaded in Excel'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.10](#figureanchor3.10) 在 Excel 中加载的 Earthquakes.csv'
- en: You should note that CSV files can also be exported from regular Excel spreadsheets,
    and that means we can use all the power of Excel when working with CSV. I have
    found the CSV format to be useful when I need to exchange data with people who
    use Excel.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意，CSV 文件也可以从常规 Excel 工作表中导出，这意味着我们可以在处理 CSV 时使用 Excel 的所有功能。我发现 CSV 格式在需要与使用
    Excel 的人交换数据时非常有用。
- en: Let’s import our CSV file to the core data representation. This is a bit more
    difficult than with JSON, but only because we must install the third-party library
    Papa Parse to do the job of parsing the CSV data. Unlike JSON, the CSV format
    doesn’t directly line up with the CDR, so it needs to be restructured during the
    import process. Fortunately, Papa Parse takes care of that.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们的 CSV 文件导入到核心数据表示中。这比 JSON 更困难一些，但只是因为我们必须安装第三方库 Papa Parse 来完成解析 CSV
    数据的工作。与 JSON 不同，CSV 格式并不直接与 CDR 对齐，因此在导入过程中需要重新结构化。幸运的是，Papa Parse 会处理这一点。
- en: As with JSON, we start by reading the CSV text file into memory; after that,
    we use Papa Parse to decode the text data to the CDR. This process is illustrated
    in [figure 3.11](#figure3.11). You probably already know how a CSV file is structured,
    but in case you don’t, [figure 3.12](#figure3.12) shows the anatomy of a CSV file
    as viewed in Notepad++.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 与 JSON 类似，我们首先将 CSV 文本文件读取到内存中；之后，我们使用 Papa Parse 将文本数据解码为 CDR。这个过程在[图 3.11](#figure3.11)
    中有所说明。你可能已经知道 CSV 文件的结构，但如果你不知道，[图 3.12](#figure3.12) 展示了在 Notepad++ 中查看的 CSV
    文件结构。
- en: '![c03_11.eps](Images/c03_11.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![c03_11.eps](Images/c03_11.png)'
- en: '[Figure 3.11](#figureanchor3.11) Importing a CSV text file to the CDR'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.11](#figureanchor3.11) 将 CSV 文本文件导入到 CDR'
- en: '![c03_12.eps](Images/c03_12.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![c03_12.eps](Images/c03_12.png)'
- en: '[Figure 3.12](#figureanchor3.12) The anatomy of a CSV file'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.12](#figureanchor3.12) CSV 文件的结构'
- en: 'A CSV file is a plain old text file: each line of the file is a row of data.
    Each row is then divided into fields that are separated by commas, hence the name
    of the data format. There isn’t much more to this format than what I have just
    described.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件是一个普通的文本文件：文件的每一行都是一行数据。然后每一行被分割成字段，这些字段由逗号分隔，因此得名。这个格式除了我刚才描述的之外，没有更多内容。
- en: 'If you are working with the GitHub repository for this chapter and have done
    the `npm install`, you already have Papa Parse installed into the Node.js project.
    If not, you can install Papa Parse in a fresh Node.js project as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用本章的 GitHub 仓库并且已经执行了 `npm install`，那么你已经在 Node.js 项目中安装了 Papa Parse。如果没有，你可以在新的
    Node.js 项目中按照以下方式安装 Papa Parse：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The following listing is our next toolkit function; this one imports a CSV file
    to the core data representation. Again, we use our toolkit function `file.read`
    to load the file into memory; then we parse the CSV data using `papa.parse`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码示例是我们下一个工具包函数；这个函数将 CSV 文件导入到核心数据表示中。同样，我们使用我们的工具包函数 `file.read` 将文件加载到内存中；然后使用
    `papa.parse` 解析 CSV 数据。
- en: Listing 3.9 A function to import a CSV text file (toolkit/importCsvFile.js)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.9 导入 CSV 文本文件的函数（toolkit/importCsvFile.js）
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note the options used with Papa Parse. The `header` option makes Papa Parse
    recognize the first line of the CSV file as the header line that specifies the
    column names for the tabular data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意与 Papa Parse 一起使用的选项。`header` 选项使 Papa Parse 识别 CSV 文件的第一个行作为标题行，该行指定了表格数据的列名。
- en: The `dynamicTyping` option enables Papa Parse’s automatic type conversion. This
    selects a type for each field value, depending on what type the value *looks like*.
    This is needed because the CSV format, unlike JSON, has no special support for
    data types. Every field in CSV is just a string value, but Papa Parse will figure
    out the actual data types for us. This capability is convenient and works most
    of the time. Sometimes, though, it will choose the wrong type, or for some reason,
    you might want more control to be able to apply your own conventions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`dynamicTyping` 选项启用了 Papa Parse 的自动类型转换。这为每个字段值选择一个类型，取决于值的类型。这是必需的，因为 CSV
    格式与 JSON 不同，没有对数据类型的特殊支持。CSV 中的每个字段只是一个字符串值，但 Papa Parse 会为我们确定实际的数据类型。这个功能很方便，并且大多数时候都能正常工作。有时，尽管如此，它可能会选择错误的类型，或者出于某种原因，你可能想要更多的控制权，以便能够应用你自己的约定。'
- en: The following listing uses our new function to import earthquakes.csv. You can
    run this code listing, and you will see the decoded data printed to the console
    so that you can check that the import worked.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码示例使用我们新的函数导入 earthquakes.csv。你可以运行这个代码示例，你将看到解码后的数据打印到控制台，以便你可以检查导入是否成功。
- en: Listing 3.10 Importing data from earthquakes.csv (listing_3.10.js)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.10 从 earthquakes.csv 导入数据（listing_3.10.js）
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parsing CSV data from a REST API
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从 REST API 解析 CSV 数据
- en: With CSV, as with JSON, we also have the option of loading CSV from a text file
    or from a REST API. To do this, we replace `file.read` with `request-promise`
    to load the data from a REST API instead of from a text file. The following listing
    is a new function `importCsvFromRestApi` that does this, and we can use it to
    import CSV data from a REST API.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与 JSON 类似，使用 CSV，我们也有从文本文件或 REST API 加载 CSV 的选项。为此，我们将 `file.read` 替换为 `request-promise`
    以从 REST API 加载数据而不是从文本文件。下面的代码示例是一个新的函数 `importCsvFromRestApi`，它执行此操作，我们可以使用它从
    REST API 导入 CSV 数据。
- en: Listing 3.11 A function to import CSV data from a REST API (toolkit/importCsvFromRestApi.js)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.11 从 REST API 导入 CSV 数据的函数（toolkit/importCsvFromRestApi.js）
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 3.12](#listing3.12) uses the function `importCsvFromRestApi` to import
    CSV data from the REST API at [https://earthquake.usgs.gov/fdsnws/event/1/query.csv](https://earthquake.usgs.gov/fdsnws/event/1/query.csv).
    You can run the following code listing, and it will pull CSV data in over your
    network, decode it, and then print it to the console so you can check it.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.12](#listing3.12) 使用`importCsvFromRestApi`函数从[https://earthquake.usgs.gov/fdsnws/event/1/query.csv](https://earthquake.usgs.gov/fdsnws/event/1/query.csv)的REST
    API导入CSV数据。你可以运行以下代码列表，它将通过你的网络拉取CSV数据，对其进行解码，然后将其打印到控制台以便你可以检查。'
- en: Listing 3.12 Importing CSV data from a REST API (listing-3.12.js)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.12 从REST API导入CSV数据（listing-3.12.js）
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This brings us to the end of loading and parsing data from text files. Note
    that other data formats exist that we might need to load, but here we’ve only
    used two of the most common formats: CSV and JSON. In practice, you might also
    need to handle XML files, YAML files, and many more—but any new format you can
    think to add will plug into your data pipeline through the CDR.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们到达了从文本文件加载和解析数据的结束。请注意，存在其他可能需要加载的数据格式，但在这里我们只使用了两种最常见的格式：CSV和JSON。在实践中，你也可能需要处理XML文件、YAML文件等等——但任何你可以想到的新格式都可以通过CDR插入到你的数据管道中。
- en: We’ll return to text files in chapter 4 to learn how to deal with unusual text
    file formats using regular expressions, for those times when we must import custom
    or proprietary data formats.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第4章回到文本文件，学习如何使用正则表达式处理不寻常的文本文件格式，对于那些我们必须导入自定义或专有数据格式的情况。
- en: 3.4.5 Importing data from databases
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.5 从数据库导入数据
- en: 'Before we finish looking at importing data, we need to learn how to import
    from databases to the core data representation. Databases are, as you can imagine,
    important in the data-wrangling world. They’re often an integral part of our data
    pipeline and necessary to efficiently and effectively work with large amounts
    of data. Databases are generally accessed using a network protocol using a third-party
    access library, as shown in [figure 3.13](#figure3.13). Many database products
    are available, but here we’ll focus on two of the most common: MongoDB and MySQL.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成查看数据导入之前，我们需要学习如何从数据库导入到核心数据表示。正如你可以想象的，数据库在数据整理的世界中非常重要。它们通常是我们的数据管道的一个组成部分，并且对于有效地处理大量数据是必要的。数据库通常使用第三方访问库通过网络协议访问，如图[图3.13](#figure3.13)所示。许多数据库产品都是可用的，但在这里我们将关注两个最常见的：MongoDB和MySQL。
- en: '![c03_13.eps](Images/c03_13.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![c03_13.eps](Images/c03_13.png)'
- en: '[Figure 3.13](#figureanchor3.13) Importing from a database to the CDR'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.13](#figureanchor3.13) 从数据库导入到CDR'
- en: 3.4.6 Importing data from MongoDB
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.6 从MongoDB导入数据
- en: MongoDB is a prevalent NoSQL database, and it’s my preferred database because
    it offers a good mix of convenience, flexibility, and performance. MongoDB, being
    NoSQL, is schema-free. MongoDB doesn’t impose a fixed schema on your data, so
    we don’t need to predefine the structure of the database.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB是一个流行的NoSQL数据库，也是我首选的数据库，因为它提供了便利性、灵活性和性能的良好组合。作为NoSQL数据库，MongoDB是无模式的。MongoDB不会对你的数据施加固定的模式，因此我们不需要预先定义数据库的结构。
- en: I find this useful when working with data that I don’t yet understand. MongoDB
    means I can throw the data into the database and save questions about the structure
    for later. Using MongoDB doesn’t mean we have unstructured data—far from it; we
    can easily express structure in MongoDB, but it means that we don’t have to worry
    about defining that structure up front. As with any data importing job, we should
    first look at the data before writing the import code. [Figure 3.14](#figure3.14)
    shows the example earthquakes database viewed through Robomongo.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现当处理我还不太理解的数据时，这很有用。MongoDB意味着我可以将数据扔进数据库，并将关于结构的问题留到以后解决。使用MongoDB并不意味着我们拥有非结构化数据——远非如此；我们可以在MongoDB中轻松表达结构，但这意味着我们不需要担心在前面定义这种结构。就像任何数据导入工作一样，我们应该在编写导入代码之前先查看数据。[图3.14](#figure3.14)展示了通过Robomongo查看的示例地震数据库。
- en: '![c03_14.eps](Images/c03_14.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![c03_14.eps](Images/c03_14.png)'
- en: '[Figure 3.14](#figureanchor3.14) Viewing the earthquakes MongoDB database using
    Robomongo database viewer'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.14](#figureanchor3.14) 使用Robomongo数据库查看器查看地震MongoDB数据库'
- en: You have various ways to retrieve data from a MongoDB database. Here we’ll use
    `promised-mongo`, a third-party library that emulates the Mongo shell and provides
    an elegant promised-based API. We’re using `promised-mongo` here because it’s
    a slightly easier way to get started with MongoDB and it’s similar to the commands
    we can also use in the Mongo shell and in Robomongo. In chapter 8, when we come
    back to MongoDB, we’ll use the official MongoDB access library.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您有多种方式从MongoDB数据库中检索数据。在这里，我们将使用`promised-mongo`，这是一个第三方库，它模拟Mongo shell并提供了一个优雅的基于承诺的API。我们在这里使用`promised-mongo`是因为这是一种稍微容易一些的开始使用MongoDB的方法，并且它与我们在Mongo
    shell和Robomongo中也可以使用的命令类似。在第8章，当我们回到MongoDB时，我们将使用官方的MongoDB访问库。
- en: We use `promised-mongo` to import data from MongoDB to the core data representation
    as illustrated in [figure 3.15](#figure3.15). Note that unlike working with text
    files, no extra parsing step is necessary; the database access library takes care
    of that.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`promised-mongo`将数据从MongoDB导入到核心数据表示中，如图3.15所示。请注意，与处理文本文件不同，不需要额外的解析步骤；数据库访问库会处理这一点。
- en: 'If you’re using the GitHub repository and did the `npm install,` you already
    have `promised-mongo` installed. Otherwise, you can install it in a fresh Node.js
    project as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用GitHub存储库并执行了`npm install`，您已经安装了`promised-mongo`。否则，您可以在新的Node.js项目中按照以下方式安装它：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![c03_15.eps](Images/c03_15.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![c03_15.eps](Images/c03_15.png)'
- en: '[Figure 3.15](#figureanchor3.15) Importing from MongoDB earthquakes database
    to the CDR'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.15](#figureanchor3.15) 从MongoDB地震数据库导入到CDR'
- en: 'The MongoDB database is easy to install: you can find downloads and more information
    at [www.mongodb.com](http://www.mongodb.com). For your convenience, the GitHub
    repository for Chapter-3 contains a Vagrant script that you can use to boot a
    virtual machine with the MongoDB database already installed, complete with example
    earthquakes data. To use this, you need Vagrant and Virtual Box installed, which
    I explain in appendix C, “Getting started with Vagrant.”'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB数据库易于安装：您可以在[www.mongodb.com](http://www.mongodb.com)找到下载和更多信息。为了您的方便，第3章的GitHub存储库包含一个Vagrant脚本，您可以使用它启动一个已经安装了MongoDB数据库的虚拟机，其中包括示例地震数据。要使用此脚本，您需要安装Vagrant和Virtual
    Box，我在附录C，“开始使用Vagrant”中进行了说明。
- en: Vagrant allows you to create virtual machines that emulate production environments.
    I’ve used Vagrant so that you can quickly boot a machine with a database, and
    this gives you a convenient data source to try the example code in listings 3.13
    and 3.14\. If you don’t want to use Vagrant, but you do want to try out this code,
    then you’ll need to install MongoDB on your development PC and manually load the
    data into the database.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant允许您创建模拟生产环境的虚拟机。我使用Vagrant是为了让您能够快速启动一个带有数据库的机器，这为您提供了一个方便的数据源来尝试列表3.13和3.14中的示例代码。如果您不想使用Vagrant，但想尝试这段代码，那么您需要在您的开发PC上安装MongoDB，并手动将数据加载到数据库中。
- en: 'Once you have Vagrant and Virtual Box installed, you can boot the virtual machine
    as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您安装了Vagrant和Virtual Box，您可以按照以下方式启动虚拟机：
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The virtual machine will take time to prepare. When it completes, you’ll have
    a MongoDB database with earthquakes data ready to go. Vagrant has mapped the default
    MongoDB port 27017 to port 6000 on our local PC (assuming that port isn’t already
    in use). This means we can access the MongoDB database on our local PC at port
    6000 as if that’s where it was running (rather than on the virtual machine where
    it’s actually running).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机需要一些时间来准备。当它完成时，您将拥有一个包含地震数据的MongoDB数据库，可以立即使用。Vagrant已将默认的MongoDB端口27017映射到我们本地PC的6000端口（假设该端口尚未被占用）。这意味着我们可以像它实际在虚拟机上运行一样，在本地PC的6000端口访问MongoDB数据库（而不是在它实际运行的虚拟机上）。
- en: 'Once you’re finished with the MongoDB virtual machine, don’t forget to destroy
    it so it doesn’t continue to consume your system resources:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成MongoDB虚拟机的使用后，不要忘记销毁它，以免它继续消耗您的系统资源：
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The following listing is our next toolkit function. It uses the MongoDB `find`
    function to import data from a MongoDB collection to the core data representation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表是我们的下一个工具函数。它使用MongoDB的`find`函数将数据从MongoDB集合导入到核心数据表示中。
- en: Listing 3.13 A function to import data from a MongoDB collection (toolkit/importFromMongoDB.js)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.13 从MongoDB集合导入数据的函数（toolkit/importFromMongoDB.js）
- en: '[PRE19]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The following listing shows how to use the function to import data from the
    largest_earthquakes*collection. Run this code and it will retrieve the data from
    the database and print it to the console for you to check.*
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了如何使用函数从largest_earthquakes*集合导入数据。运行此代码将检索数据库中的数据并将其打印到控制台供您检查。
- en: '*Listing 3.14 Importing the largest earthquakes collection from MongoDB (listing-3.14.js)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表3.14 从MongoDB导入largest earthquakes集合（listing-3.14.js）'
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note in [listing 3.14](#listing3.14) how we connect to the MongoDB database
    using the connection string `localhost:6000/earthquakes`. This assumes that we’re
    connecting to a MongoDB database named *earthquakes* running on the Vagrant virtual
    machine and that the MongoDB database instance is mapped to port 6000 on the host
    PC.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在[列表3.14](#listing3.14)中，我们如何使用连接字符串`localhost:6000/earthquakes`连接到MongoDB数据库。这假设我们正在连接到运行在Vagrant虚拟机上的名为*earthquakes*的MongoDB数据库，并且MongoDB数据库实例映射到主机PC上的6000端口。
- en: You must change this connection string to connect to a different database. For
    example, if you installed MongoDB on your local PC (instead of using the Vagrant
    virtual machine), you’ll probably find that MongoDB is using its default port
    of 27017\. If that’s the case, you need to use the connection string `localhost:27017/earthquakes`.
    Considering that *localhost* and *27017* are defaults, you can even drop those
    parts and use `earthquakes` as your connection string.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须更改此连接字符串以连接到不同的数据库。例如，如果您在本地PC上安装了MongoDB（而不是使用Vagrant虚拟机），您可能会发现MongoDB正在使用其默认端口27017。如果是这种情况，您需要使用连接字符串`localhost:27017/earthquakes`。考虑到*localhost*和*27017*是默认值，您甚至可以省略这些部分，直接使用`earthquakes`作为您的连接字符串。
- en: 'You can also connect to a MongoDB database over the internet by providing a
    valid hostname in the connection string. For example, if you have an internet-accessible
    database available on a machine with host name *my_host.com*, then your connection
    string might look like this: `my_host.com:27017/my_database.`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以通过在连接字符串中提供有效的主机名来通过互联网连接到MongoDB数据库。例如，如果您有一个名为*my_host.com*的机器上的可访问互联网数据库，那么您的连接字符串可能看起来像这样：`my_host.com:27017/my_database.`。
- en: 3.4.7 Importing data from MySQL
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.7 从MySQL导入数据
- en: We couldn’t finish looking at data importing without looking at an SQL-style
    database. SQL is a mainstay of the business world, and much data is contained
    within SQL databases. Here we look at importing data from MySQL, a popular SQL
    database.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看数据导入之前，我们无法完成对数据的查看。SQL是商业世界的基石，大量数据包含在SQL数据库中。在这里，我们查看从MySQL导入数据，MySQL是一个流行的SQL数据库。
- en: As we’ve done in the other cases, before we get into the code, we should first
    look at the data in a database viewer. In [figure 3.16](#figure3.16) you can see
    the earthquakes database and largest_earthquakes collection through the HeidiSQL
    database viewer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在其他情况下所做的那样，在我们进入代码之前，我们应该首先查看数据库中的数据。在[图3.16](#figure3.16)中，您可以通过HeidiSQL数据库查看器看到地震数据库和largest_earthquakes集合。
- en: '![c03_16.tif](Images/c03_16.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![c03_16.tif](Images/c03_16.png)'
- en: '[Figure 3.16](#figureanchor3.16) Viewing the largest_earthquakes table using
    the HeidiSQL database viewer'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.16](#figureanchor3.16) 使用HeidiSQL数据库查看器查看largest_earthquakes表'
- en: To read data from the MySQL, we’ll use a third-party library called `nodejs-mysql`.
    [Figure 3.17](#figure3.17) illustrates the process of retrieving data from an
    earthquakes database and importing it to the core data representation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要从MySQL读取数据，我们将使用一个名为`nodejs-mysql`的第三方库。[图3.17](#figure3.17)说明了从地震数据库检索数据并将其导入核心数据表示的过程。
- en: 'If you’re using the GitHub repository and did the `npm install`, you already
    have `nodejs-mysql` installed. Otherwise, you can install it in a fresh Node.js
    project as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用GitHub仓库并执行了`npm install`，您已经安装了`nodejs-mysql`。否则，您可以在新的Node.js项目中按以下方式安装它：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: MySQL is a little more difficult to set up than MongoDB. After installation
    of MySQL and before you import data, you must define your schema, something that
    isn’t necessary with MongoDB. Downloads and instructions for installation of MySQL
    can be found at [http://www.mysql.com](http://www.mysql.com).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL的设置比MongoDB要复杂一些。在安装MySQL并在导入数据之前，您必须定义模式，这在MongoDB中是不必要的。MySQL的下载和安装说明可以在[http://www.mysql.com](http://www.mysql.com)找到。
- en: '![c03_17.eps](Images/c03_17.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![c03_17.eps](Images/c03_17.png)'
- en: '[Figure 3.17](#figureanchor3.17) Importing data from the SQL database to the
    CDR'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.17](#figureanchor3.17) 从SQL数据库导入数据到CDR'
- en: For your convenience, the GitHub repo for Chapter-3 contains another Vagrant
    script that will boot up a virtual machine with a MySQL database installed complete
    with an earthquakes database that you can use to try the code in listings 3.15
    and 3.16\. You’ll need Vagrant and Virtual Box installed, which you might already
    have installed from the earlier example with MongoDB.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了您的方便，第 3 章的 GitHub 仓库包含另一个 Vagrant 脚本，该脚本将启动一个带有已安装 MySQL 数据库的虚拟机，其中包含地震数据库，您可以使用它来尝试列表
    3.15 和 3.16 中的代码。您需要安装 Vagrant 和 Virtual Box，您可能已经从之前的 MongoDB 示例中安装了它们。
- en: 'Boot the virtual machine with the following command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启动虚拟机：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The virtual machine will take time to prepare. Once it has completed, you’ll
    have a MySQL database with an earthquakes database ready to use. Vagrant has mapped
    the default MySQL port 3306 to port 5000 on our local PC (assuming port 5000 isn’t
    already in use). You can access the MySQL database on your PC at port 5000 as
    if that’s where it was running (rather than on the virtual machine where it’s
    actually running).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机需要一些时间来准备。一旦完成，您将拥有一个包含地震数据库的 MySQL 数据库，可以开始使用。Vagrant 将默认的 MySQL 端口 3306
    映射到我们本地电脑的端口 5000（假设端口 5000 没有被占用）。您可以在端口 5000 上访问您的 PC 上的 MySQL 数据库，就像它在那里运行一样（而不是在实际上运行的虚拟机上）。
- en: 'Once you’re finished with the virtual machine, don’t forget to destroy it so
    it doesn’t continue to consume your system resources:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成虚拟机的使用，不要忘记销毁它，以免它继续消耗您的系统资源：
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For more information on setting up and working with Vagrant, please see appendix
    C.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有关设置和使用 Vagrant 的更多信息，请参阅附录 C。
- en: '[Listing 3.15](#listing3.15) defines the function `importFromMySql` with the
    simple code required to execute an SQL command against the earthquakes database
    and import data to the core data representation.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.15](#listing3.15) 定义了 `importFromMySql` 函数，其中包含执行 SQL 命令并导入数据到核心数据表示所需的简单代码。'
- en: Listing 3.15 A function to import data from a MySQL database (toolkit/importFromMySql.js)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.15 从 MySQL 数据库导入数据的函数（toolkit/importFromMySql.js）
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Listing 3.16](#listing3.16) shows how to use the `importFromMySql` function.
    It connects to the earthquakes database and imports data from the largest_earthquakes
    table. Run this code and it will retrieve the data from the MySQL database and
    print it to the console so that we can check it.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.16](#listing3.16) 展示了如何使用 `importFromMySql` 函数。它连接到地震数据库，并从 largest_earthquakes
    表中导入数据。运行此代码，它将从 MySQL 数据库检索数据并将其打印到控制台，以便我们可以进行检查。'
- en: Listing 3.16 Importing largest earthquakes table from MySQL (listing-3.16.js)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.16 从 MySQL 导入最大地震表（listing-3.16.js）
- en: '[PRE25]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 3.5 Exporting data
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 导出数据
- en: 'We’ve finished learning about importing data into memory. In the second half
    of this chapter, we look at the other side of the equation: *exporting data*.
    We’ll learn to export data from our data pipeline to various data formats and
    storage mechanisms. The same as when we were learning about importing: we’ll start
    with text files, and we’ll finish with the databases MongoDB and MySQL.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了关于将数据导入内存的学习。在本章的后半部分，我们将探讨等式的另一面：*导出数据*。我们将学习如何将数据从我们的数据管道导出到各种数据格式和存储机制。就像我们学习导入时一样，我们将从文本文件开始，并以
    MongoDB 和 MySQL 数据库结束。
- en: 3.5.1 You need data to export!
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 您需要导出数据！
- en: Through the code examples to import data, we printed the imported data to the
    console to check that everything worked as expected. Exporting is a little different.
    Before we can export data, we need example data to export!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导入数据的代码示例，我们将导入的数据打印到控制台以检查一切是否按预期工作。导出略有不同。在我们能够导出数据之前，我们需要导出的示例数据！
- en: For the rest of this chapter, we’ll use earthquakes.csv as our example data.
    The general pattern for the export code examples is shown in [figure 3.18](#figure3.18).
    First, we use the toolkit function `importCsvFile` that we created earlier to
    load earthquakes.csv to the CDR. This is followed by the remainder of the export
    process, which depends on the data format we’re exporting. The following listing
    shows the general export process in code. You can see that after importing earthquakes.csv,
    we have a blank slot where we can insert our export code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将使用 earthquakes.csv 作为示例数据。导出代码示例的一般模式在 [图 3.18](#figure3.18) 中展示。首先，我们使用之前创建的工具包函数
    `importCsvFile` 将 earthquakes.csv 加载到 CDR 中。随后是导出过程的其余部分，这取决于我们导出的数据格式。以下列表展示了代码中的通用导出过程。您可以看到，在导入
    earthquakes.csv 之后，我们有一个空白槽，可以插入我们的导出代码。
- en: Listing 3.17 General pattern for your data export example code
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.17 数据导出示例代码的一般模式（toolkit/file.js）
- en: '[PRE26]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 3.5.2 Exporting data to text files
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 将数据导出到文本文件
- en: 'Exporting to a text file starts with serialization of the data that we’re holding
    in the core data representation. We must start by choosing our data format: here
    we’ll export our data either as JSON or as CSV. Our data is serialized to text
    in memory. Then we use the Node.js function `fs.writeFile` to write the text data
    to the file system. This process is illustrated in [figure 3.19](#figure3.19).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据导出到文本文件的过程始于将我们持有的核心数据表示形式中的数据进行序列化。我们必须首先选择我们的数据格式：在这里，我们将数据导出为JSON或CSV格式。我们的数据在内存中以文本形式进行序列化。然后我们使用Node.js函数`fs.writeFile`将文本数据写入文件系统。这个过程在[图3.19](#figure3.19)中进行了说明。
- en: '![c03_18.eps](Images/c03_18.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![c03_18.eps](Images/c03_18.png)'
- en: '[Figure 3.18](#figureanchor3.18) General format of the data export examples'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.18](#figureanchor3.18) 数据导出示例的一般格式'
- en: '![c03_19.eps](Images/c03_19.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![c03_19.eps](Images/c03_19.png)'
- en: '[Figure 3.19](#figureanchor3.19) Exporting from the CDR to a text file'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.19](#figureanchor3.19) 从CDR导出到文本文件'
- en: As we did with Node’s `fs.readFile` function, we create a function that wraps
    `fs.writeFile` in a promise. We want to keep our file-related functions together,
    so let’s add the new `write` function to our existing file module as shown in
    the following listing.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在Node的`fs.readFile`函数中所做的那样，我们创建了一个函数，该函数将`fs.writeFile`包装在一个Promise中。我们希望将我们的文件相关函数放在一起，所以让我们将新的`write`函数添加到以下列表中现有的文件模块中。
- en: Listing 3.18 A promise-based function to write a text file (toolkit/file.js)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.18 基于Promise的函数用于写入文本文件（toolkit/file.js）
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We’re going to use our new toolkit function to write our data to JSON and CSV
    files in subsequent sections.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将使用我们新的工具包函数将数据写入JSON和CSV文件。
- en: 3.5.3 Exporting data to JSON text files
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 将数据导出到JSON文本文件
- en: To export from the CDR to JSON, use the built-in JavaScript function `JSON.stringify`.
    With our data serialized to text, we then write the text to earthquakes.json,
    as illustrated in [figure 3.20](#figure3.20). The following listing shows the
    new function `exportJsonFile` that exports our data to a JSON file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要从CDR导出到JSON，请使用内置的JavaScript函数`JSON.stringify`。将我们的数据序列化为文本后，然后按照[图3.20](#figure3.20)所示将其写入earthquakes.json文件。以下列表显示了新的函数`exportJsonFile`，该函数将我们的数据导出到JSON文件。
- en: Listing 3.19 A function to export data to a JSON text file (toolkit/exportJsonFile.js)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.19 导出数据到JSON文本文件的函数（toolkit/exportJsonFile.js）
- en: '[PRE28]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![c03_20.eps](Images/c03_20.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![c03_20.eps](Images/c03_20.png)'
- en: '[Figure 3.20](#figureanchor3.20) Exporting from the CDR to a JSON text file'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.20](#figureanchor3.20) 从CDR导出到JSON文本文件'
- en: The following listing uses the `exportJsonFile` function to export our data
    to a JSON file. You can run this code, and you will find that it produces a file
    in the *output* folder called earthquakes.json.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表使用`exportJsonFile`函数将我们的数据导出到JSON文件。您可以运行此代码，您会发现它在*output*文件夹中生成了一个名为earthquakes.json的文件。
- en: Listing 3.20 Exporting data to earthquakes.json (listing-3.20.js)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.20 将数据导出到earthquakes.json（listing-3.20.js）
- en: '[PRE29]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 3.5.4 Exporting data to CSV text files
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 将数据导出到CSV文本文件
- en: CSV exporting isn’t built into JavaScript, so again we turn to Papa Parse for
    this capability. This time we use the function `papa.unparse` to serialize our
    data to CSV text. We then write the data to earthquakes.csv using our `file.write`
    function. The process is illustrated in [figure 3.21](#figure3.21). The following
    listing shows our function `exportCsvFile` that exports data to a CSV file using
    `papa.unparse`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: CSV导出不是JavaScript内置的，所以我们再次转向Papa Parse来实现这一功能。这次我们使用函数`papa.unparse`将我们的数据序列化为CSV文本。然后我们使用`file.write`函数将数据写入earthquakes.csv。这个过程在[图3.21](#figure3.21)中进行了说明。以下列表显示了我们的函数`exportCsvFile`，该函数使用`papa.unparse`将数据导出到CSV文件。
- en: '![c03_21.eps](Images/c03_21.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![c03_21.eps](Images/c03_21.png)'
- en: '[Figure 3.21](#figureanchor3.21) Exporting from the CDR to a CSV text file'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.21](#figureanchor3.21) 从CDR导出到CSV文本文件'
- en: Listing 3.21 A function to export data to a CSV text file (toolkit/exportCsvFile.js)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.21 导出数据到CSV文本文件的函数（toolkit/exportCsvFile.js）
- en: '[PRE30]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Listing 3.22](#listing3.22) uses the `exportCsvFile` function to export our
    data to a CSV file. Run this code and it will produce the file earthquakes-export.csv
    in the output folder.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.22](#listing3.22) 使用`exportCsvFile`函数将我们的数据导出到CSV文件。运行此代码，它将在输出文件夹中生成名为earthquakes-export.csv的文件。'
- en: Listing 3.22 Exporting data to earthquakes.csv (listing-3.22.js)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.22 将数据导出到earthquakes.csv（listing-3.22.js）
- en: '[PRE31]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 3.5.5 Exporting data to a database
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.5 将数据导出到数据库
- en: Exporting our data to a database is necessary for us to work effectively with
    data. With a database, we can easily and efficiently retrieve filtered and sorted
    data whenever it’s needed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的数据导出到数据库对于我们有效地处理数据是必要的。有了数据库，我们可以在需要时轻松高效地检索过滤和排序后的数据。
- en: '[Figure 3.22](#figure3.22) shows the general process. The core data representation
    is fed into a database access library. Typically, the library interfaces with
    the database through the network to store the data.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.22](#figure3.22) 展示了整个过程。核心数据表示被输入到数据库访问库中。通常，库通过网络与数据库接口以存储数据。'
- en: '![c03_22.eps](Images/c03_22.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![c03_22.eps](Images/c03_22.png)'
- en: '[Figure 3.22](#figureanchor3.22) Exporting from the CDR to a database'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.22](#figureanchor3.22) 从 CDR 导出到数据库'
- en: 3.5.6 Exporting data to MongoDB
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.6 将数据导出到 MongoDB
- en: We can export data to MongoDB using the third-party library `promised-mongo`
    that we installed earlier. This is illustrated in [figure 3.23](#figure3.23).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前安装的第三方库 `promised-mongo` 将数据导出到 MongoDB。这在上面的 [图 3.23](#figure3.23)
    中展示。
- en: '![c03_23.eps](Images/c03_23.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![c03_23.eps](Images/c03_23.png)'
- en: '[Figure 3.23](#figureanchor3.23) Exporting from the CDR to a MongoDB database'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.23](#figureanchor3.23) 从 CDR 导出到 MongoDB 数据库'
- en: Your toolkit function to export to MongoDB, shown in the following listing,
    is the simplest yet. It’s almost not worth having a separate function for this,
    but I included it for completeness. For a particular database and collection,
    it calls the `insert` function to insert an array of records.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您的工具箱函数，用于将数据导出到 MongoDB，如以下列表所示，是最简单的一种。几乎不值得为这个单独创建一个函数，但我为了完整性而包含了它。对于特定的数据库和集合，它调用
    `insert` 函数来插入记录数组。
- en: Listing 3.23 A function to export data to MongoDB (toolkit/exportToMongoDB.js)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.23 导出数据到 MongoDB 的函数（toolkit/exportToMongoDB.js）
- en: '[PRE32]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A specific example is shown in [listing 3.24](#listing3.24). This code connects
    to a MongoDB instance that’s running on the Vagrant virtual machine. The database
    access port is mapped to port 6000 on our development PC. Example data is imported
    from earthquakes.csv; then we call our function `exportToMongoDB` and store the
    data in the MongoDB database. You can run this code, and it will create and populate
    a new collection in the database that’s named largest_earthquakes_export*.*
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 3.24](#listing3.24) 中展示了一个具体示例。这段代码连接到一个在 Vagrant 虚拟机上运行的 MongoDB 实例。数据库访问端口映射到我们的开发
    PC 上的 6000 端口。示例数据从 earthquakes.csv 导入；然后我们调用 `exportToMongoDB` 函数并将数据存储在 MongoDB
    数据库中。你可以运行这段代码，它将在数据库中创建并填充一个名为 largest_earthquakes_export*.* 的新集合。
- en: Listing 3.24 Exporting to the MongoDB largest_earthquakes collection (listing-3.24)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.24 将数据导出到 MongoDB 的 largest_earthquakes 集合（listing-3.24）
- en: '[PRE33]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 3.5.7 Exporting data to MySQL
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.7 将数据导出到 MySQL
- en: We can export data to MySQL using the third-party library `nodejs-mysql` that
    we installed earlier. This process is illustrated in [figure 3.24](#figure3.24).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前安装的第三方库 `nodejs-mysql` 将数据导出到 MySQL。这个过程在 [图 3.24](#figure3.24) 中展示。
- en: '![c03_24.eps](Images/c03_24.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![c03_24.eps](Images/c03_24.png)'
- en: '[Figure 3.24](#figureanchor3.24) Exporting from the CDR to a MySQL database'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.24](#figureanchor3.24) 从 CDR 导出到 MySQL 数据库'
- en: Our function to export to MySQL is shown in [listing 3.25](#listing3.25). This
    is a bit different from exporting to MongoDB. With MongoDB, we could insert a
    large collection of records with a single call to `insert`. We can’t do that with
    this library; instead, we must execute multiple SQL `insert` commands. Note how
    the JavaScript `reduce` function in the following listing is used to sequence
    these SQL commands one after the other.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据导出到 MySQL 的函数在 [列表 3.25](#listing3.25) 中展示。这与导出到 MongoDB 有所不同。对于 MongoDB，我们可以通过一次调用
    `insert` 来插入大量记录。我们无法使用这个库做到这一点；相反，我们必须执行多个 SQL `insert` 命令。注意以下列表中如何使用 JavaScript
    的 `reduce` 函数来按顺序执行这些 SQL 命令。
- en: Listing 3.25 A function to export data to MySQL (toolkit/exportToMySql.js)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.25 导出数据到 MySQL 的函数（toolkit/exportToMySql.js）
- en: '[PRE34]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Before inserting the data into our MySQL database, we need to create the database
    table. For me, this is one of the disadvantages of using SQL: we have to create
    tables and define our schema before we can insert data. This kind of preparation
    isn’t necessary with MongoDB.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据插入我们的 MySQL 数据库之前，我们需要创建数据库表。对我来说，这是使用 SQL 的一大缺点：在插入数据之前，我们必须创建表并定义我们的模式。这种准备并不需要
    MongoDB。
- en: The following listing shows the creation of a largest_earthquakes_export table
    in the MySQL database with a schema that matches the format of our example data.
    You must run this code to create the database schema for our data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了在 MySQL 数据库中创建一个与我们的示例数据格式匹配的 largest_earthquakes_export 表的过程。你必须运行此代码以创建我们数据的数据库模式。
- en: Listing 3.26 Creating the largest_earthquakes_export table in the MySQL database
    (listing-3.26.js)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.26 在 MySQL 数据库中创建 largest_earthquakes_export 表（listing-3.26.js）
- en: '[PRE35]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: After creating the database table, we can now export data to it. In the following
    listing, we import the example data from earthquakes.csv and then use our `exportToMySql`
    function to export it to the MySQL database. You can run this code, and it will
    populate the SQL table largest_earthquakes_export*with your data.*
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建数据库表之后，我们现在可以将其导出。在下面的列表中，我们导入 earthquakes.csv 的示例数据，然后使用我们的 `exportToMySql`
    函数将其导出到 MySQL 数据库。你可以运行此代码，它将用你的数据填充 SQL 表 largest_earthquakes_export*。
- en: '*Listing 3.27 Exporting to the MySQL largest_earthquakes table (listing-3.27.js)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 3.27 导出到 MySQL largest_earthquakes 表（listing-3.27.js）'
- en: '[PRE36]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We’ve now completed our journey through importing and exporting a variety of
    data formats. How can we use this experience? Well, now we can mix and match data
    formats, and we can build a large variety of different data pipelines.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了对各种数据格式导入和导出的旅程。我们如何利用这个经验呢？嗯，现在我们可以混合和匹配数据格式，我们可以构建大量不同类型的数据管道。
- en: 3.6 Building complete data conversions
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 构建完整的数据转换
- en: Figure shows a complete picture of a data conversion from a CSV file to a MongoDB
    database. We’ve already seen this kind of conversion in the section “Exporting
    data to MongoDB.” Note how the import code overlaps with the export code in the
    middle with the core data representation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图表展示了从 CSV 文件到 MongoDB 数据库的数据转换的完整视图。我们已经在“导出到 MongoDB”这一节中看到了这种转换。注意导入代码如何在中部与导出代码重叠，核心数据表示就在这里。
- en: Let’s take another look at the code for this conversion. The following listing
    clearly identifies the import and export components of the conversion. These are
    nicely defined as toolkit functions that we created earlier in this chapter.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次审视这个转换的代码。下面的列表清晰地标识了转换的导入和导出组件。这些组件被很好地定义为我们在本章早期创建的工具函数。
- en: Listing 3.28 Example data conversion from CSV file to MongoDB collection
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.28 从 CSV 文件到 MongoDB 集合的数据转换示例
- en: '[PRE37]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Hopefully, you’re starting to get a feel for how you can mix and match data
    formats and piece them together to build data pipelines.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在开始对如何混合和匹配数据格式并将它们拼接起来以构建数据管道有了感觉。
- en: 3.7 Expanding the process
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 扩展过程
- en: Let’s come back to the core data representation pattern. You can see in [listing
    3.28](#listing3.28) that you could easily replace the import and export functions
    there with functions for working with any other data format. This forms the pattern
    that allows you to build almost any data conversion that you can imagine.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到核心数据表示模式。你可以在[列表 3.28](#listing3.28)中看到，你可以轻松地用处理任何其他数据格式的函数替换那里的导入和导出函数。这形成了一个模式，允许你构建几乎你能想象到的任何数据转换。
- en: '![c03_25.eps](Images/c03_25.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![c03_25.eps](Images/c03_25.png)'
- en: Figure 3.25 An example data conversion, CSV to MongoDB
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 CSV 到 MongoDB 的数据转换示例
- en: You can now build data conversion for any of the formats we’ve covered so far.
    Look at [figure 3.26](#figure3.26). Pick an import format from the left. Pick
    an export format from the right. Then wire these together in JavaScript code that
    feeds the data through the CDR.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以为之前我们覆盖的任何格式构建数据转换。看看[图 3.26](#figure3.26)。从左侧选择一个导入格式。从右侧选择一个导出格式。然后在
    JavaScript 代码中将这些格式连接起来，通过 CDR 传输数据。
- en: '![c03_26.eps](Images/c03_26.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![c03_26.eps](Images/c03_26.png)'
- en: '[Figure 3.26](#figureanchor3.26) The core data representation design pattern
    is a recipe for constructing data conversion pipelines.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.26](#figureanchor3.26) 核心数据表示设计模式是构建数据转换管道的配方。'
- en: The core data representation pattern is extensible. You aren’t limited to the
    data formats presented in this chapter. You can bring your own data formats, either
    standard (such as XML or YAML) or even custom formats, and integrate them into
    your workflow.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 核心数据表示模式是可扩展的。你不仅限于本章中展示的数据格式。你可以引入自己的数据格式，无论是标准格式（如 XML 或 YAML）还是自定义格式，并将它们集成到你的工作流程中。
- en: The kind of data pipeline we’ve looked at so far is generalized in [figure 3.27](#figure3.27).
    We take input data in a format and pass it through code that can decode that format.
    At this point, the data resides in memory in the core data representation. Now
    we pass the CDR data through export code to get the data where it needs to be.
    I’m sure you can imagine how you could add new formats into the mix. As an example,
    let’s say that you create toolkit functions for importing and exporting XML. Now
    you’ve extended your ability to create data conversion pipelines—for example,
    XML to CSV, XML to MongoDB, MySQL to XML, and so on.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前查看的数据管道类型在[图3.27](#figure3.27)中进行了概括。我们以某种格式接收输入数据，并通过可以解码该格式的代码传递它。此时，数据位于内存中的核心数据表示中。现在我们通过导出代码将CDR数据传递出去，使其到达需要的位置。我相信你可以想象如何将新格式添加到混合中。例如，假设你创建了导入和导出XML的工具函数。现在你已经扩展了创建数据转换管道的能力——例如，XML到CSV，XML到MongoDB，MySQL到XML等等。
- en: '![c03_27.eps](Images/c03_27.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![c03_27.eps](Images/c03_27.png)'
- en: '[Figure 3.27](#figureanchor3.27) A general data conversion pipeline'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.27](#figureanchor3.27) 一个通用数据转换流程'
- en: In the coming chapters, we’ll build on the core data representation pattern.
    As you can see in [figure 3.28](#figure3.28), we’re going to stretch out the middle
    section of the conversion pipeline. This is where we’ll add stages to our data
    pipeline for data cleanup, transformation, and analysis. Each of these stages
    operates on the core data representation. Each stage takes the CDR data as input,
    does work on it, and then outputs the transformed CDR data, passing it onto the
    next stage.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将基于核心数据表示模式进行构建。正如你在[图3.28](#figureanchor3.28)中看到的，我们将扩展转换流程的中间部分。这就是我们将添加数据清理、转换和分析阶段到我们的数据管道的地方。每个阶段都在核心数据表示上操作。每个阶段将CDR数据作为输入，对其进行处理，然后输出转换后的CDR数据，传递给下一个阶段。
- en: '![c03_28.eps](Images/c03_28.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![c03_28.eps](Images/c03_28.png)'
- en: '[Figure 3.28](#figureanchor3.28) The basic data conversion pipeline expanded
    to include data cleanup and transformation stages'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.28](#figureanchor3.28) 扩展的基本数据转换流程，包括数据清理和转换阶段'
- en: Using the core data representation pattern allows us to create a total of 36
    different data conversions from the techniques we learned in this chapter. Thirty-six
    is the number of importers (6) multiplied by the number of exporters (6). Any
    new formats that we add to the mix only increase this number. Say that you add
    the XML format to the mix; now you have 49 different data conversions at your
    disposal!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 使用核心数据表示模式，我们可以从本章学到的技术中创建总共36种不同的数据转换。36是导入器数量（6）乘以导出器数量（6）。我们添加到混合中的任何新格式只会增加这个数字。比如说，你将XML格式添加到混合中；现在你有49种不同的数据转换可供使用！
- en: Acquisition, storage, and retrieval are fundamental for building data pipelines.
    Now that you’ve tackled these aspects of data wrangling, you can move onto more
    varied and advanced topics. You aren’t done with data import yet though, and in
    chapter 4 we’ll look into more advanced aspects of it, such as dealing with custom
    data, web scraping, and working with binary data.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 获取、存储和检索是构建数据管道的基本要素。现在你已经处理了数据整理的这些方面，你可以转向更多样化和高级的主题。然而，你还没有完成数据导入，在第4章中，我们将探讨其更高级的方面，例如处理自定义数据、网络爬虫和与二进制数据一起工作。
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You learned that you can wire together flexible data pipelines with code that
    feeds data through the core data representation.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你了解到，你可以通过代码将灵活的数据管道连接起来，这些代码将数据通过核心数据表示传递。
- en: You discovered how to import and export JSON and CSV text files.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你发现了如何导入和导出JSON和CSV文本文件。
- en: We discussed importing JSON and CSV data from a REST API via HTTP GET.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了通过HTTP GET从REST API导入JSON和CSV数据。
- en: You worked through examples on importing and exporting data with MongoDB and
    MySQL databases.***
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你通过示例学习了如何使用MongoDB和MySQL数据库导入和导出数据。
