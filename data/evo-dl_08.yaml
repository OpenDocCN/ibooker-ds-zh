- en: 6 Neuroevolution optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 神经进化优化
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How DL networks optimize or learn
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习网络如何优化或学习
- en: Replacing backpropagation training of neural networks with GAs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用遗传算法替换神经网络的反向传播训练
- en: Evolutionary optimization of neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的进化优化
- en: Employing evolutionary optimization to a Keras DL model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将进化优化应用于Keras深度学习模型
- en: Scaling up neuroevolution to tackle image class recognition tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经进化扩展到处理图像分类任务
- en: In the last chapter, we managed to get our feet wet by employing evolutionary
    algorithms for the purposes of optimizing DL network hyperparameters. We saw how
    using EA could improve the search for hyperparameters beyond simple random or
    grid search algorithms. Employing variations of EA, such as PSO, evolutionary
    strategy, and differential evolution, uncovered insights into methods used to
    search and for hyperparameter optimization (HPO).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过使用进化算法来优化深度学习网络的超参数来尝试涉水。我们看到了如何使用EA可以改进超参数搜索，超越简单的随机或网格搜索算法。采用EA的变体，如PSO（粒子群优化）、进化策略和差分进化，揭示了用于搜索和超参数优化（HPO）的方法。
- en: Evolutionary DL is a term we use to encompass all evolutionary methods employed
    to improve DL. More specifically, the term *neuroevolution* has been used to define
    specific optimization patterns applied to DL. One such pattern we looked at in
    the last chapter was the application of evolutionary algorithms to HPO.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 进化深度学习是我们用来涵盖所有用于改进深度学习的进化方法的术语。更具体地说，术语“神经进化”已被用来定义应用于深度学习的特定优化模式。我们在上一章中查看的一个模式是将进化算法应用于HPO。
- en: Neuroevolution encompasses techniques for HPO, parameter optimization (weight/
    parameter search), and network optimization. In this chapter, we dive into how
    evolutionary methods can be applied to optimize network parameters directly, thus
    eliminating the need to backpropagate errors or loss through a network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经进化包括用于HPO（超参数优化）、参数优化（权重/参数搜索）和网络优化的技术。在本章中，我们将深入了解进化方法如何直接应用于优化网络参数，从而消除通过网络反向传播错误或损失的需求。
- en: Neuroevolution is typically employed for the purpose of improving a single DL
    network model. There are other applications of evolution to DL that broaden search
    to more than one model. For now, though, let’s look at how to build a simple multilayer
    perceptron (MLP) with NumPy as a basis for neuroevolution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经进化通常用于改进单个深度学习网络模型。还有其他将进化应用于深度学习的方法，这些方法将搜索范围扩展到多个模型。然而，现在，让我们看看如何使用NumPy作为神经进化的基础来构建一个简单的多层感知器（MLP）。
- en: 6.1 Multilayered perceptron in NumPy
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 NumPy中的多层感知器
- en: Before we jump into neuroevolving a network’s parameters, let’s take a closer
    look at a basic DL system. The most basic of these is the multilayered perceptron
    written in NumPy. We don’t use any frameworks like Keras or PyTorch, so we can
    clearly visualize the internal processes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨神经进化网络参数之前，让我们更仔细地看看一个基本的深度学习系统。其中最基本的是用NumPy编写的多层感知器。我们不使用像Keras或PyTorch这样的框架，因此我们可以清楚地可视化内部过程。
- en: Figure 6.1 shows a simple MLP network. At the top of the figure, we can see
    how backpropagation works by pushing the calculated loss through the network.
    The bottom of the figure shows how neuroevolution optimization works by replacing
    each of the network’s weights/parameters with a value from a `gene` sequence.
    Effectively, we are performing an evolutionary search similar to the one for hyperparameters
    in the last chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1显示了简单的MLP网络。在图的上部，我们可以看到通过将计算损失推过网络来工作反向传播。图的底部显示了通过用“基因”序列中的值替换网络的每个权重/参数来工作的神经进化优化。实际上，我们正在进行类似于上一章中用于超参数的进化搜索。
- en: '![](../Images/CH06_F01_Lanham.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F01_Lanham.png)'
- en: Figure 6.1 Backpropagation vs. neuroevolution optimization
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 反向传播与神经进化优化
- en: Hopefully, if you have a solid background in DL, you already understand the
    MLP and its internal workings. However, to be complete, we review the structure
    of the MLP written with just NumPy. Then, we look at how this simple network trains
    across various sample classification problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 希望如果你在深度学习方面有扎实的背景，你已经理解了MLP及其内部工作原理。然而，为了全面起见，我们回顾了仅使用NumPy编写的MLP结构。然后，我们看看这个简单的网络如何在各种样本分类问题中进行训练。
- en: Open the EDL_6_1_MLP_NumPy.ipynb notebook in Colab. If you need assistance,
    refer to the appendix. Run all the cells by selecting Runtime > Run All from the
    menu.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_6_1_MLP_NumPy.ipynb笔记本。如果你需要帮助，请参阅附录。通过选择菜单中的“运行”>“运行所有”来运行所有单元格。
- en: Figure 6.2 shows the second cell and options you can select. Select the options
    as shown in the figure and then run all cells in the notebook again by selecting
    Runtime > Run All.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2显示了你可以选择的第二个单元格和选项。按照图中的选项选择，然后通过选择运行 > 运行所有来再次运行笔记本中的所有单元格。
- en: '![](../Images/CH06_F02_Lanham.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F02_Lanham.png)'
- en: Figure 6.2 Selecting problem dataset generation parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 选择问题数据集生成参数
- en: Handling errors
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 处理错误
- en: Generally, if you encounter an error when running a notebook, it is the result
    of duplicate code being run or code being run out of sync. The simplest fix is
    to factory reset the notebook (Runtime > Factory Reset Runtime) and then run the
    cells again.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你在运行笔记本时遇到错误，那是因为重复运行代码或代码运行不同步。最简单的修复方法是工厂重置笔记本（运行 > 工厂重置运行时），然后再次运行单元格。
- en: The code to generate the problem datasets is built using the sklearn `make`
    `datasets` family of functions. We won’t worry about the explicit code, instead
    focusing on the parameter options (see figure 6.2) in table 6.1.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成问题数据集的代码是使用sklearn的`make` `datasets`函数族构建的。我们不会关注具体的代码，而是专注于表6.1中的参数选项（见图6.2）。
- en: Table 6.1 Summary description of parameters and value ranges
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 参数和值范围的摘要描述
- en: '| Parameter | Description | Range |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 | 范围 |'
- en: '| --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `number_samples` | The number of sample data points | 100–1,000 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `number_samples` | 样本数据点的数量 | 100–1,000 |'
- en: '| `difficulty` | An arbitrary factor that increases problem difficulty | 1–5
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `difficulty` | 一个增加问题难度的任意因素 | 1–5 |'
- en: '| `problem` | Defines the problem dataset function used | classification =
    `make_classification`moons = `make_moons`circles = `make_circles`blobs = `make_blobs`Gaussian
    quantiles = `make_gaussian_quantiles` |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `problem` | 定义用于问题数据集的函数 | classification = `make_classification`moons =
    `make_moons`circles = `make_circles`blobs = `make_blobs`Gaussian quantiles = `make_gaussian_quantiles`
    |'
- en: '| `middle_layer` | Sets the number of nodes in the middle network layer | 5–25
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `middle_layer` | 设置中间网络层的节点数 | 5–25 |'
- en: '| `epochs` | The number of training iterations to run on the MLP | 1000–25000
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `epochs` | 在MLP上运行的训练迭代次数 | 1000–25000 |'
- en: Figure 6.3 shows examples from each of the dataset types at difficulty level
    1\. Go ahead and change the problem type to see variations of each dataset. The
    most difficult dataset for the simple MLP network is circles, but be sure to explore
    all of them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3显示了在难度级别1下每种数据集类型的示例。尝试更改问题类型，以查看每个数据集的变体。对于简单的MLP网络来说，最困难的数据集是圆形，但请确保探索所有数据集。
- en: '![](../Images/CH06_F03_Lanham.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F03_Lanham.png)'
- en: Figure 6.3 Examples of sample dataset types at difficulty level 1
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 难度级别1下样本数据集类型的示例
- en: As a baseline, we compare a simple logistic regression (classification) model
    from sklearn. Scroll down to the code shown in the following listing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基线，我们比较了来自sklearn的一个简单逻辑回归（分类）模型。向下滚动查看以下列表中显示的代码。
- en: 'Listing 6.1 EDL_6_1_MLP_NumPy.ipynb: Sklearn logistics regression'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1 EDL_6_1_MLP_NumPy.ipynb：Sklearn逻辑回归
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Fits the model to the data
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将模型拟合到数据
- en: ❷ Uses the helper function to show the predictions visual
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用辅助函数显示预测的可视化
- en: ❸ Generates a set of predictions
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成一组预测
- en: ❹ Evaluates the accuracy of predictions and then prints
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 评估预测的准确性并打印
- en: Figure 6.4 shows the output of calling the helper `show_predictions` function.
    This function plots a nice visual of how the model classifies the data. As you
    can see in the figure, the results are less than stellar.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4显示了调用辅助`show_predictions`函数的输出。此函数绘制了模型如何对数据进行分类的漂亮可视化。如图所示，结果并不十分出色。
- en: '![](../Images/CH06_F04_Lanham.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F04_Lanham.png)'
- en: Figure 6.4 Plot of a logistic regression model classifying the classification
    dataset
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4展示了逻辑回归模型对分类数据集进行分类的图
- en: Next, click on the Show Code link on the cell titled MLP in Python. Be sure
    to review the `init`, `forward`, `back_prop`, and `train` functions at your leisure.
    We don’t spend time looking at the code here; we use this simple example to demonstrate
    the different functions. This code will be reused in future projects but without
    the `back_prop` and `training` functions. The last code block in the notebook,
    shown in the following listing, creates the MLP network, trains it, and outputs
    a visualization of the results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击标题为“Python中的MLP”的单元格上的“显示代码”链接。务必在空闲时间查看`init`、`forward`、`back_prop`和`train`函数。我们在这里不会花时间查看代码；我们使用这个简单的例子来展示不同的函数。此代码将在未来的项目中重用，但不包括`back_prop`和`training`函数。笔记本中最后一个代码块，如下所示，创建MLP网络，对其进行训练，并输出结果的可视化。
- en: 'Listing 6.2 EDL_6_1_MLP_NumPy.ipynb: Create and train the network'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2 EDL_6_1_MLP_NumPy.ipynb：创建和训练网络
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Creates the MLP network
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建MLP网络
- en: ❷ Trains the network
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练网络
- en: ❸ Shows the results of the training
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 展示训练结果
- en: ❹ Prints out the model accuracy
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印出模型准确率
- en: Figure 6.5 shows the results of training the MLP network. As we can see in the
    rudimentary classification example, the results of using the MLP network are significantly
    better than the logistic regression model from sklearn. That is, in part, why
    neural networks and DL have become so successful. However, this simple network
    will still struggle to solve all the problem datasets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5展示了训练MLP网络的结果。正如我们在基本的分类示例中可以看到的，使用MLP网络的结果显著优于sklearn中的逻辑回归模型。这就是为什么神经网络和深度学习变得如此成功的一部分原因。然而，这个简单的网络仍然难以解决所有的问题数据集。
- en: '![](../Images/CH06_F05_Lanham.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F05_Lanham.png)'
- en: Figure 6.5 The results of training a simple MLP network on a problem dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 在问题数据集上训练简单MLP网络的成果
- en: Figure 6.6 shows the output of the MLP network trying to solve the circles and
    moons problem sets. As the figure shows, the accuracy peaks at 0.5, or 50%, for
    circles and 0.89, or 89%, for moons. We could, of course, look at more powerful
    optimizers, like Adam, but let’s consider another way. What if we used GAs, for
    instance, to find the optimal network weights, like in many of our previous examples?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6展示了MLP网络尝试解决圆和月亮问题集的输出。如图所示，圆的准确率峰值在0.5，即50%，而月亮的准确率在0.89，即89%。我们当然可以查看更强大的优化器，如Adam，但让我们考虑另一种方法。如果我们使用遗传算法，比如找到最优的网络权重，就像我们之前的许多例子一样，会怎样呢？
- en: '![](../Images/CH06_F06_Lanham.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F06_Lanham.png)'
- en: Figure 6.6 Results of training MLP on circles and moons problem datasets
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 在圆和月亮问题数据集上训练MLP的结果
- en: 6.1.1 Learning exercises
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 学习练习
- en: 'Use the following exercises to improve your knowledge:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高你的知识：
- en: Increase or decrease the number of samples in figure 6.2 and then rerun the
    notebook.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加或减少图6.2中的样本数量，然后重新运行笔记本。
- en: Change the problem type and difficulty in figure 6.2 and then rerun the notebook
    after every change. Keep the size of the model consistent.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改图6.2中的问题类型和难度，并在每次更改后重新运行笔记本。保持模型大小一致。
- en: Change the model parameters and middle layer in figure 6.2 and then rerun.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改图6.2中的模型参数和中层，然后重新运行。
- en: Now that we have a perceptron MLP model, we can move on to optimizing it with
    genetic algorithms in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了感知器MLP模型，我们可以在下一节中继续使用遗传算法来优化它。
- en: 6.2 Genetic algorithms as deep learning optimizers
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 遗传算法作为深度学习优化器
- en: With the stage set from the previous project, we can now move on to replacing
    the DL optimization method used in our MLP, from backpropagation to neuroevolution
    optimization. So instead of using any form of backpropagation of loss through
    optimizers like gradient descent or Adam, we rely entirely on GAs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个项目设置的基础上，我们现在可以继续将我们的MLP中使用的深度学习优化方法从反向传播更改为神经进化优化。因此，我们不再使用任何形式的通过优化器（如梯度下降或Adam）通过损失反向传播，而是完全依赖于遗传算法。
- en: The next project we look at uses the code from the last project as our base
    network model, and then we wrap the training optimization process with GAs from
    DEAP. A lot of this code should now feel quite familiar, so we only consider the
    highlights. If you need a refresher on setting up GA with DEAP, consider reviewing
    chapters 3–5.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要看的下一个项目使用上一个项目的代码作为我们的基础网络模型，然后我们用DEAP中的遗传算法包裹训练优化过程。因此，很多代码现在应该感觉非常熟悉，所以我们只考虑重点。如果你需要复习如何使用DEAP设置遗传算法，请考虑回顾第3-5章。
- en: Open notebook EDL_6_2_MLP_GA.ipynb in Colab. Refer to appendix A if you need
    assistance. Be sure to run all cells in the model by selecting Run > Run All from
    the menu.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开笔记本EDL_6_2_MLP_GA.ipynb。如需帮助，请参阅附录A。确保通过菜单选择运行 > 运行所有来运行模型中的所有单元格。
- en: We focus on the major changes by starting to look at the MLP network class block
    of code. This project uses the same MLP network model but replaces the `train`
    and `back_prop` functions with a new `set_parameters` function within the `Neural_Network`
    class, shown in listing 6.3.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过开始查看MLP网络代码块中的主要变化来关注主要变化。本项目使用相同的MLP网络模型，但将`Neural_Network`类中的`train`和`back_prop`函数替换为新的`set_parameters`函数，如列表6.3所示。
- en: This code loops through the list of parameters in the model, finds the size
    and shape, and then extracts a matching number of `genes` from the `individual`.
    Then, a new tensor is constructed and reshaped to match the original parameter/weight
    tensor. We subtract the original tensor from itself to zero it and maintain the
    reference and then add the new tensor. Effectively, we swap sections of the `individual`’s
    `gene` sequence into tensors, which we then replace as new weights within the
    model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码遍历模型中的参数列表，找到大小和形状，然后从 `individual` 中提取匹配数量的 `genes`。然后，构建一个新的张量并将其重塑以匹配原始参数/权重张量。我们从原始张量中减去自身以将其置零并保持引用，然后添加新的张量。实际上，我们将
    `individual` 的 `gene` 序列的部分交换到张量中，然后将其作为模型中的新权重替换。
- en: 'Listing 6.3 EDL_6_2_MLP_GA.ipynb: The `set_parameters` function'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 6.3 EDL_6_2_MLP_GA.ipynb: `set_parameters` 函数'
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Loops through the list of model weights/parameters
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历模型权重/参数的列表
- en: ❷ Gets the size of the parameter tensor and then extracts the set of genes
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取参数张量的大小，然后提取基因集
- en: ❸ Creates a new tensor and then reshapes it from the gene sequence
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个新的张量，然后从基因序列中重塑它
- en: ❹ Resets the tensor to zero and then adds a new tensor
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将张量重置为零，然后添加一个新的张量
- en: ❺ Updates the index position to the individual
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将索引位置更新为个体
- en: Be sure to note that the `train` and `back_prop` functions have been completely
    removed, thus preventing the network from performing any form of conventional
    backpropagation training. The `set_parameters` function sets the weights/parameters
    of the model and allows us to search for the values using GA. The next code listing
    we look at instantiates our network, sets all the parameters to 1.0, and then
    outputs the results shown in figure 6.7.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`train` 和 `back_prop` 函数已被完全删除，从而防止网络执行任何形式的传统反向传播训练。`set_parameters` 函数设置模型的权重/参数，并允许我们使用
    GA 搜索这些值。我们接下来要查看的代码实例化了我们的网络，将所有参数设置为 1.0，然后输出图 6.7 中显示的结果。
- en: 'Listing 6.4 EDL_6_2_MLP_GA.ipynb: Creating the network and setting the sample
    weights'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 6.4 EDL_6_2_MLP_GA.ipynb: 创建网络和设置样本权重'
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Creates the MLP network
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建 MLP 网络
- en: ❷ Calculates the number of model parameters, which equals the number of genes
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算模型参数的数量，该数量等于基因的数量
- en: ❸ Sets each model weight to 1
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将每个模型权重设置为 1
- en: ❹ Generates the predictions plot
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成预测图
- en: ❺ Calculates the accuracy and then prints
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算准确率然后打印
- en: '![](../Images/CH06_F07_Lanham.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F07_Lanham.png)'
- en: Figure 6.7 Network prediction with all weights set to 1 on the circles dataset
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 在圆圈数据集上所有权重设置为 1 的网络预测
- en: Figure 6.7 shows the output of the model predictions with all weights/parameters
    set to 1.0\. The DEAP code to set up the GA is shown in the following listing,
    but it should already be familiar by now.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 显示了模型预测的输出，其中所有权重/参数都设置为 1.0。设置 GA 的 DEAP 代码如下所示，但现在应该已经很熟悉了。
- en: 'Listing 6.5 EDL_6_2_MLP_GA.ipynb: DEAP `toolbox` setup'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 6.5 EDL_6_2_MLP_GA.ipynb: DEAP `toolbox` 设置'
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Creates a gene sequence of floats of length number_of_genes
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建长度为 number_of_genes 的浮点基因序列
- en: ❷ Sets the selection to a tournament with a size of 5
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将选择设置为大小为 5 的锦标赛
- en: ❸ Uses the Blend function for crossover
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用混合函数进行交叉
- en: ❹ Uses Gaussian mutation
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用高斯变异
- en: Likewise, we can review the `evaluate` function, as shown in the following listing.
    Notice how we return the inverse of the accuracy. This allows us to minimize the
    `fitness` and, thus, maximize the accuracy of the `individual` during evolution.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以回顾 `evaluate` 函数，如下所示。注意我们返回准确率的倒数。这允许我们在进化过程中最小化 `fitness`，从而最大化 `individual`
    的准确率。
- en: 'Listing 6.6 EDL_6_2_MLP_GA.ipynb: The `evaluate` function'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 6.6 EDL_6_2_MLP_GA.ipynb: `evaluate` 函数'
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Sets the model parameters based on individual genes
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据个体基因设置模型参数
- en: ❷ Evaluates the model predictions on the problem dataset
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在问题数据集上评估模型预测
- en: ❸ Returns the inverse calculated accuracy
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回计算出的准确率的倒数
- en: ❹ Registers the function with the toolbox
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将函数注册到工具箱中
- en: Finally, we can jump down to the code that evolves the `population` to optimize
    the model, as shown in listing 6.7\. As you might expect, we use the `eaSimple`
    function to train the `population` over a set of `generations`. Then, we output
    a sample `individual` from the last `generation`’s `population` and the current
    best `individual` as a comparison. At the end of the code, we check for an early
    stopping condition if the accuracy reaches some value. Checking for early stopping
    allows our code to break as soon as an acceptable solution is found.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以跳到进化模型以优化模型的代码，如列表6.7所示。正如你所期望的，我们使用`eaSimple`函数在一系列`代`中训练`种群`。然后，我们输出上一代`种群`中的一个样本`个体`和当前最佳`个体`作为比较。在代码的末尾，如果准确率达到某个值，我们检查早期停止条件。检查早期停止条件允许我们的代码一旦找到可接受的解决方案就立即中断。
- en: 'Listing 6.7 EDL_6_2_MLP_GA.ipynb: Evolving the model'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 EDL_6_2_MLP_GA.ipynb：进化模型
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Calls the evolution function to evolve the population
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调用进化函数以进化种群
- en: ❷ Shows the results of the last individual in the last generation
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 显示了上一代最后一个个体的结果
- en: ❸ Shows the results of the best individual
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 显示了最佳个体的结果
- en: ❹ Breaks if the early stopping condition is met
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果满足早期停止条件则中断
- en: Figure 6.8 shows an example of evolving the `population` to an `individual`
    that can solve the circles problem with 100% accuracy. This is quite impressive
    when you consider our MLP network using backpropagation could only attain 50%
    on this same problem.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8展示了将`种群`进化到一个可以以100%准确率解决圆问题的`个体`的例子。当你考虑到我们使用反向传播的MLP网络在这个问题上只能达到50%时，这相当令人印象深刻。
- en: '![](../Images/CH06_F08_Lanham.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F08_Lanham.png)'
- en: Figure 6.8 The evolution progression to solve the circle problem with GA
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 使用GA解决圆问题的进化过程
- en: Take some time to explore other problem datasets using GA and see how this method
    compares with simple backpropagation and gradient descent optimization. Again,
    there are more powerful optimizers, like Adam, we compare against later, but take
    the time to appreciate how well GA can optimize a simple MLP network.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用GA探索其他问题数据集，看看这种方法与简单的反向传播和梯度下降优化相比如何。再次强调，还有更强大的优化器，比如我们稍后比较的Adam，但请花时间欣赏GA如何优化一个简单的MLP网络。
- en: 6.2.1 Learning exercises
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 学习练习
- en: 'Use the following exercises to improve your neuroevolutionary knowledge:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高你的神经进化知识：
- en: Increase or decrease the number of samples and then rerun. Is it harder to converge
    the network parameters with fewer or greater samples?
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加或减少样本数量然后重新运行。使用更少或更多的样本收敛网络参数是否更难？
- en: Alter the `crossover` and `mutation` rates and the rerun. Can you improve the
    performance of the evolution for a given problem?
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`交叉`和`变异`率然后重新运行。你能提高给定问题的进化性能吗？
- en: Increase or decrease the size of the middle layer and then rerun. What effect
    does the network size have on evolution?
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加或减少中间层的大小，然后重新运行。网络大小对进化有什么影响？
- en: Of course, we also have more powerful evolutionary methods, like evolutionary
    strategies and differential evolution, that may perform better. We take the time
    to look at both more advanced evolutionary methods in the next section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还有更强大的进化方法，如进化策略和差分进化，可能表现更好。我们将在下一节花时间查看这两种更先进的进化方法。
- en: 6.3 Other evolutionary methods for neurooptimization
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 用于神经优化的其他进化方法
- en: In chapter 5, when we tuned hyperparameters, we saw some great results using
    other evolutionary methods, like evolutionary strategies and differential evolution.
    Having seen good results like these, it only makes sense to apply both ES and
    DE to the set of problems worked on in the last section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，当我们调整超参数时，我们看到了使用其他进化方法（如进化策略和差分进化）的一些很好的结果。看到这样的好结果，将ES和DE应用于上一节中解决的问题集是很有意义的。
- en: In this project, we apply ES and DE as neuroevolution optimizers. The two code
    examples are extensions of our last project and reside in separate notebooks.
    We jump back and forth between both notebooks and the last project to make comparisons.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将ES和DE作为神经进化优化器应用。这两个代码示例是上一个项目的扩展，位于单独的笔记本中。我们在两个笔记本和上一个项目之间来回跳转，以便进行比较。
- en: Open up EDL_6_3_MLP_ES.ipynb and EDL_6_3_MLP_DE.ipynb in Colab in two separate
    browser tabs. You may also want to keep the last project notebook, EDL_6_2_MLP_GA.ipynb,
    open as well. If you need assistance, see the appendix.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开两个独立的浏览器标签页中的EDL_6_3_MLP_ES.ipynb和EDL_6_3_MLP_DE.ipynb。你可能还想保持最后一个项目笔记本EDL_6_2_MLP_GA.ipynb的打开状态。如果你需要帮助，请参阅附录。
- en: Select the same problem, circles or moons, from the notebook’s Dataset Parameters
    cell. If you are not sure which problem to choose, refer to figure 6.2 and the
    corresponding table that explains the options in greater detail.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从笔记本的“数据集参数”单元格中选择相同的问题，圆或月亮。如果你不确定选择哪个问题，请参阅图6.2和相应的表格，以更详细地解释选项。
- en: Run all the cells of both notebooks via Runtime > Run All from the menu. Switch
    between both notebooks as they are running to see how each of the methods optimize
    the weights.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过菜单中的“运行”>“运行所有”来运行两个笔记本的所有单元格。在它们运行时在两个笔记本之间切换，以查看每种方法如何优化权重。
- en: Figure 6.9 shows examples of running the ES and DE notebooks to completion (a
    maximum of 1,000 `generations`) on the circles and moons problems. It can be especially
    interesting to note how DE and ES evolve the weights for each problem. Notice
    how the ES notebook evolves and the 2D visualization produces several straight
    edges. Not only is ES solid at solving these more difficult datasets, but it also
    has the potential to solve more difficult problems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9展示了在圆和月亮问题上的ES和DE笔记本运行到完成（最多1,000代）的示例。特别值得注意的是，DE和ES如何为每个问题进化权重。注意ES笔记本的进化以及2D可视化产生的几条直线。ES不仅擅长解决这些更难的数据集，而且还有解决更难问题的潜力。
- en: '![](../Images/CH06_F09_Lanham.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F09_Lanham.png)'
- en: Figure 6.9 ES vs. DE on the circle and moons problem datasets
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 ES与DE在圆和月亮问题数据集上的比较
- en: We have reviewed all the main code elements in both notebooks, so we won’t revisit
    any code here. However, go ahead and look through the structure of the code on
    your own to see how easy it is to convert from using GA to ES and DE. You can
    also go back and try other problems or adjust other settings in the Dataset Parameters
    cell to see how either ES or DE performs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经回顾了两个笔记本中的所有主要代码元素，所以在这里我们不会重新审视任何代码。然而，你可以自己查看代码的结构，看看从使用GA转换为ES和DE有多容易。你也可以回过头去尝试其他问题或调整数据集参数单元格中的其他设置，以查看ES或DE的表现。
- en: For the sample datasets showcased in this suite of projects, the simpler GA
    approach generally performs the best. While this may vary slightly, DE is certainly
    the less optimal choice, but ES has some definite potential. In later chapters,
    we revisit this choice between methods again to dive deeper into which is the
    best option.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本套项目中展示的样本数据集，简单的GA方法通常表现最好。虽然这可能会略有不同，但DE显然不是最佳选择，但ES有一些明确的潜力。在后面的章节中，我们再次回顾这种方法的选择，以深入了解哪种选项是最好的。
- en: 6.3.1 Learning exercises
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 学习练习
- en: 'Complete the following exercises to help improve your understanding:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下练习，以帮助提高你的理解：
- en: Find a class of problems in which ES performs better than DE, and vice versa.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到一个问题类别，其中ES的表现优于DE，反之亦然。
- en: Tune the various hyperparameter options, and then see what effect they have
    on either DE or ES notebooks.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整各种超参数选项，然后看看它们对DE或ES笔记本有什么影响。
- en: Play with the specific evolutionary methods hyperparameters—min and max strategy
    for ES, and pmin/pmax and smin/smax for DE.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩转特定的进化方法超参数——ES的最小和最大策略，以及DE的pmin/pmax和smin/smax。
- en: In this section, we looked at how other evolutionary methods can be employed
    for a simple NumPy network weight optimization. In the next section, we apply
    the same principle but this time to a DL framework such as Keras.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了如何使用其他进化方法来对简单的NumPy网络权重进行优化。在下一节中，我们将应用同样的原理，但这次是针对深度学习框架，例如Keras。
- en: 6.4 Applying neuroevolution optimization to Keras
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 将神经进化优化应用于Keras
- en: Admittedly, the MLP network we were making comparisons to in the previous projects
    was somewhat underpowered and limited. To make a valid comparison, we should “improve
    our game” and look at a more robust DL platform, like Keras. Keras, much like
    PyTorch and many other DL frameworks, provides a wide selection of advanced optimizers
    we can use out of the box.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在前一个项目中，我们用来进行比较的MLP网络功能相对较弱且有限。为了进行有效的比较，我们应该“提高我们的水平”并查看一个更健壮的深度学习平台，如Keras。Keras与PyTorch和其他许多深度学习框架类似，提供了一系列开箱即用的高级优化器。
- en: In the following project, we set up a Keras multilayered DL network to solve
    for the classification datasets. Not only does this provide a great comparison
    between using a robust and established optimizer, like Adam, but it also showcases
    how we can incorporate *neuroevolution optimization* (NO) into a Keras network.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下项目中，我们设置了一个 Keras 多层深度学习网络来解决分类数据集。这不仅提供了使用鲁棒且成熟的优化器（如 Adam）之间的良好比较，而且还展示了我们如何将
    *神经进化优化*（NO）集成到 Keras 网络中。
- en: Open the EDL_6_4_Keras_GA.ipynb notebook in Colab. Refer to the appendix if
    you need assistance. Hold off on running all the cells in the notebook, as we
    take this step by step.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中打开 EDL_6_4_Keras_GA.ipynb 笔记本。如需帮助，请参阅附录。暂不运行笔记本中的所有单元格，因为我们一步一步地进行。
- en: Locate and select the Keras model setup code cell, as shown in listing 6.8,
    and then run all the notebook’s previous cells by selecting Runtime > Run Before
    from the menu. The code creates a simple Keras model with an input, hidden, and
    output layer. The output is a single binary node; we use binary cross entropy
    to calculate loss. We also determine the number of trainable parameters of the
    model, since this also relates to the number of `genes` later.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 定位并选择如图6.8所示的 Keras 模型设置代码单元格，然后通过选择菜单中的“运行之前”来运行笔记本的所有前一个单元格。该代码创建了一个简单的 Keras
    模型，具有输入、隐藏和输出层。输出是一个单一的二进制节点；我们使用二元交叉熵来计算损失。我们还确定了模型的可训练参数数量，因为这也与后面的 `genes`
    数量有关。
- en: 'Listing 6.8 EDL_6_4_Keras_GA.ipynb: Setting up the Keras model'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8 EDL_6_4_Keras_GA.ipynb：设置 Keras 模型
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Creates a simple Keras Sequential model
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个简单的 Keras Sequential 模型
- en: ❷ Creates an optimizer of type Adam with a learning rate
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个具有学习率的 Adam 类型优化器
- en: ❸ Sets the loss to binary crossentropy
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将损失设置为二元交叉熵
- en: ❹ Uses accuracy metrics
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用准确性指标
- en: ❺ Prints a summary of the model and outputs trainable parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印模型的摘要并输出可训练参数
- en: Run the Keras set up cell; you will get the output shown in figure 6.10\. The
    output shows the model summary and an output of the number of parameters/weights
    per layer. The total number of trainable parameters is also printed at the bottom
    of the output. This is important because it represents the number of `genes` in
    an `individual`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Keras 设置单元格；你将得到图6.10所示的输出。输出显示了模型摘要以及每层的参数/权重数量。输出底部还打印了可训练参数的总数。这很重要，因为它代表了
    `individual` 中的 `genes` 数量。
- en: '![](../Images/CH06_F10_Lanham.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F10_Lanham.png)'
- en: Figure 6.10 Summary model output and count of parameters
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 模型总结输出和参数计数
- en: Go back to the Dataset Parameters cell, as shown in figure 6.2, and select a
    difficult problem, like moons or circles. This reruns the cell and generates a
    view of the problem dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到数据集参数单元格，如图6.2所示，并选择一个困难的问题，如月亮或圆圈。这会重新运行单元格并生成问题数据集的视图。
- en: 'Scroll down to the model training code cell, as shown in the following listing,
    and then run the cell. As part of this training code, we are using a helpful callback
    function: `PlotLossesKeras` from the `LiveLossPlot` module.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动到模型训练代码单元格，如下所示列表，然后运行单元格。作为这部分训练代码的一部分，我们使用了一个有用的回调函数：来自 `LiveLossPlot` 模块的
    `PlotLossesKeras`。
- en: 'Listing 6.9 EDL_6_4_Keras_GA.ipynb: Fitting the model'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9 EDL_6_4_Keras_GA.ipynb：拟合模型
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Trains the model on datasets for a number of epochs
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在数据集上训练模型多个时期
- en: ❷ Uses PlotLossesKeras to output progress plots
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 PlotLossesKeras 输出进度图
- en: ❸ Turns off noisy output
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 关闭噪声输出
- en: Run the training cell. You will get a similar output to that shown in figure
    6.11.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 运行训练单元格。你将得到与图6.11中所示类似的输出。
- en: '![](../Images/CH06_F11_Lanham.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F11_Lanham.png)'
- en: Figure 6.11 An example output from training a Keras model with an Adam optimizer
    over 50 epochs
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 使用 Adam 优化器在50个时期内训练 Keras 模型的示例输出
- en: Run the next couple of cells to evaluate the model’s accuracy and output the
    results. Figure 6.12 shows the output from the `show_predictions` helper method.
    The rainbow pattern is representative of the model’s output, which is a value
    from 0 to 1\. The class separation is the middle at 0.5, shown by the yellow band.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 运行接下来的几个单元格以评估模型的准确性并输出结果。图6.12显示了 `show_predictions` 辅助方法的输出。彩虹图案代表了模型的输出，是一个从0到1的值。类别的分离在中间的0.5处，由黄色带表示。
- en: '![](../Images/CH06_F12_Lanham.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F12_Lanham.png)'
- en: Figure 6.12 Output of model using `show_predictions`
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 使用 `show_predictions` 的模型输出
- en: Move to the next code cell, where there is a helper function that extracts an
    `individual`’s `genes` and inserts them into the weights/parameters of the Keras
    model. This code is quite like how we set model weights in the simple MLP network.
    It loops over the model layers and model weights, extracting a tensor of weights.
    From this information, we rebuild a tensor from the next section of `individual`
    weights and add it to list of tensors.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 移动到下一个代码单元格，其中有一个辅助函数，用于提取一个`个体`的`基因`并将它们插入到Keras模型的权重/参数中。这段代码与我们在简单MLP网络中设置模型权重的方式非常相似。它遍历模型层和模型权重，提取一个权重张量。从这个信息中，我们重新构建一个张量，从下一个部分的`个体`权重中提取，并将其添加到张量列表中。
- en: Finally, the model weights are set using the `set_weights` function, as shown
    in the following listing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`set_weights`函数设置模型权重，如下所示。
- en: 'Listing 6.10 EDL_6_4_Keras_GA.ipynb: Fitting the model'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10 EDL_6_4_Keras_GA.ipynb：拟合模型
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Loops through the model layers
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历模型层
- en: ❷ Loops through the layers’ weight tensors
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历层的权重张量
- en: ❸ Appends the new tensor to a list
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将新的张量追加到列表中
- en: ❹ Sets the weights of the model from the list of tensors
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从张量列表中设置模型的权重
- en: The next cell sets all the model weights to 1 and outputs the results using
    `show_ predictions`. Again, we follow the same procedure we used in the MLP project.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格将所有模型权重设置为1，并使用`show_predictions`输出结果。同样，我们遵循在MLP项目中使用的相同程序。
- en: The rest of the code is identical to the previous GA example, so go ahead run
    the rest of the cells by selecting Runtime > Run After from the menu. Just be
    sure you have selected a cell in which the code and previous cells have already
    been fully run. If you are not sure which cell was last run, you can also just
    run all the cells.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码与之前的GA示例相同，因此请从菜单中选择运行 > 运行后续单元格来运行其余的单元格。只需确保您已选择一个单元格，其中代码和之前的单元格已经完全运行。如果您不确定哪个单元格是最后运行的，您也可以简单地运行所有单元格。
- en: Figure 6.13 shows the output of running GA optimization using a Keras network.
    Notice how well the model is optimizing without using any DL optimizers. If you
    are an experienced Keras user, go ahead and swap out various other optimizers
    to see if any can beat the evolutionary optimizer.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13显示了使用Keras网络运行GA优化的输出。注意模型在没有使用任何深度学习优化器的情况下优化得有多好。如果你是经验丰富的Keras用户，你可以尝试替换各种其他优化器，看看是否有任何可以击败进化优化器的。
- en: '![](../Images/CH06_F13_Lanham.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH06_F13_Lanham.png)'
- en: Figure 6.13 The output of a Keras model optimized with GA on the circles problem
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 使用GA在圆形问题上优化的Keras模型的输出
- en: 6.4.1 Learning exercises
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 学习练习
- en: 'The following exercises are intended to show the limits of neuroevolution in
    Keras:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习旨在展示Keras中神经进化的局限性：
- en: Change the problem type to circles and then rerun the problem. Are the network-evolved
    weights able to solve the problem?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将问题类型更改为圆形，然后重新运行问题。网络进化的权重能否解决这个问题？
- en: Alter the Keras model in listing 6.8 and then rerun the notebook. What happens
    when you remove or add new layers to the model?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改列表6.8中的Keras模型，然后重新运行笔记本。当你从模型中移除或添加新层时会发生什么？
- en: Alter the network loss to use MSE rather than binary cross-entropy in listing
    6.8\. What effect does this have on the performance of the evolution and results?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表6.8中，将网络损失更改为使用均方误差（MSE）而不是二元交叉熵。这对进化和结果的表现有何影响？
- en: We now have a powerful new tool in our toolbelt—something that should surely
    benefit all DL, it seems. Unfortunately, there are some limits to this method
    and evolutionary search in general. We look at an example of those limits in the
    next section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的工具箱中有一个强大的新工具——这似乎肯定会对所有深度学习（DL）都有益。不幸的是，这种方法以及进化搜索在一般意义上都有一些局限性。我们将在下一节中查看这些局限性的一个例子。
- en: 6.5 Understanding the limits of evolutionary optimization
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 理解进化优化的局限性
- en: DL models have continuously exploded in size, from early models having hundreds
    of parameters to the latest transformers having billions. Optimizing or training
    these networks requires substantial computational resources, so trying to evaluate
    better ways will always be a priority. As such, we want to move away from toy
    datasets and look at more practical applications of evolutionary optimization.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的大小一直在爆炸式增长，从早期模型拥有数百个参数到最新的变换器拥有数十亿个参数。优化或训练这些网络需要大量的计算资源，因此尝试评估更好的方法将始终是一个优先事项。因此，我们希望从玩具数据集转向更实际的进化优化应用。
- en: In the next project, we move up slightly from toy datasets to a first-class
    example problem of classifying the Modified National Institute of Standards and
    Technology (MNIST) handwritten digits dataset. As part of your DL education, you
    have likely already used MNIST in some capacity. MNIST is often the first dataset
    we learn to build DL networks to classify.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个项目中，我们将从玩具数据集提升到一级示例问题，即对修改后的国家标准与技术研究院（MNIST）手写数字数据集进行分类。作为你的深度学习教育的一部分，你很可能已经以某种方式使用过MNIST。MNIST通常是我们的第一个数据集，我们用它来构建深度学习网络进行分类。
- en: Open the EDL_6_5_MNIST_GA.ipynb notebook in Colab. The appendix can help you
    if you need assistance in this task. Run the top two cells—the `pip install` and
    `import`—to set up the base of the notebook code. The next cell loads the MNIST
    dataset, normalizes the values, and puts them into training tenors `x` and `y`,
    as shown in the following listing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_6_5_MNIST_GA.ipynb笔记本。附录可以帮助你在需要时完成任务。运行笔记本顶部的两个单元格——`pip install`和`import`——以设置笔记本代码的基础。下一个单元格加载MNIST数据集，归一化值，并将它们放入训练术语`x`和`y`中，如下所示。
- en: 'Listing 6.11 EDL_6_5_MNIST_GA.ipynb: Loading the data'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.11 EDL_6_5_MNIST_GA.ipynb：加载数据
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Load the MNIST dataset to train and test.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载MNIST数据集进行训练和测试。
- en: ❷ Normalize the byte values to 0-1 floats.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将字节数值归一化到0-1浮点数。
- en: ❸ Plot an example image from the set.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制集合中的一个示例图像。
- en: ❹ Print out the corresponding label for the image.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印出图像对应的标签。
- en: '![](../Images/CH06_F14_Lanham.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F14_Lanham.png)'
- en: Figure 6.14 An example image from MNIST
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 MNIST的一个示例图像
- en: Figure 6.14 shows a sample output of a single digit from the dataset. The next
    cell has the model building code, so run that cell and the training code shown
    in listing 6.12\. This code trains the model and, again, uses module livelossplot’s
    `PlotLossesKeras` function to show real-time results. After that, the model accuracy
    is displayed and a class classification report is generated.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14显示了数据集中单个数字的样本输出。下一个单元格包含模型构建代码，因此运行该单元格和列表6.12中显示的训练代码。此代码训练模型，并再次使用livelossplot模块的`PlotLossesKeras`函数来显示实时结果。之后，显示模型准确率并生成类别分类报告。
- en: 'Listing 6.12 EDL_6_5_MNIST_GA.ipynb: Training the model'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 EDL_6_5_MNIST_GA.ipynb：训练模型
- en: '[PRE11]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Trains the model with data for epochs
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用数据训练模型
- en: ❷ Validates the model with test data
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用测试数据验证模型
- en: ❸ Plots accuracy and loss plots
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制准确率和损失图
- en: ❹ Performs a test prediction
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 执行测试预测
- en: ❺ Takes the highest prediction as a class
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将最高预测值作为类别
- en: ❻ Prints the classification report
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印分类报告
- en: Figure 6.15 shows the class classification report generated by the sklearn module
    `classification_report` function based on the test prediction results. As you
    can see, our network is clearly excellent at classifying the digits from all classes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15显示了基于测试预测结果的sklearn模块`classification_report`函数生成的类别分类报告。正如你所看到的，我们的网络在分类所有类别的数字方面明显非常出色。
- en: Select Runtime > Run After from the menu to run all the remaining cells in the
    notebook. Again, most of the code in this notebook is identical to our previous
    projects, so we won’t need to review it.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从菜单中选择运行 > 运行后，以运行笔记本中剩余的所有单元格。同样，这个笔记本中的大部分代码与我们的前一个项目相同，所以我们不需要对其进行审查。
- en: '![](../Images/CH06_F15_Lanham.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F15_Lanham.png)'
- en: Figure 6.15 Classification report of MNIST digits classification
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 MNIST数字分类的分类报告
- en: Figure 6.16 demonstrates sample output from the last cell performing the evolution.
    This figure shows the accuracy progression over time as well as the classification
    report. As you can see from this quick example, evolutionary optimization has
    critical limitations when it approaches larger models.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16展示了最后单元格执行进化的样本输出。此图显示了随着时间的推移准确率的进展以及分类报告。从这个快速示例中可以看出，当进化优化接近更大的模型时，它具有关键的限制。
- en: '![](../Images/CH06_F16_Lanham.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH06_F16_Lanham.png)'
- en: Figure 6.16 Example output from GA EO, showing poor results
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 展示了GA EO的示例输出，显示了较差的结果
- en: As seen in the last project, using evolutionary optimization/search for finding
    optimal network weights/parameters produces poor results, while the network yields
    up to 60% accuracy over a couple of hours of training, which is far better than
    random. However, accuracy results for each class are subpar and not acceptable.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一次项目所示，使用进化优化/搜索来寻找最佳网络权重/参数会产生较差的结果，而网络在几小时的训练后可以达到高达60%的准确率，这比随机要好得多。然而，每个类别的准确率结果都不理想，不能接受。
- en: 6.5.1 Learning exercises
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 学习练习
- en: 'These exercises are intended for advanced readers wanting to test the limits
    of neuroevolutionary weight/parameter optimization:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习是为希望测试神经进化权重/参数优化极限的高级读者准备的：
- en: Change the base Keras model by altering the network size and shape. Do you get
    better results with a smaller network?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过改变网络大小和形状来更改基础Keras模型。使用更小的网络您能得到更好的结果吗？
- en: Add convolutional layers and max pooling to the model. This can help reduce
    the total number of model parameters to evolve.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型添加卷积层和最大池化。这可以帮助减少需要进化的模型参数总数。
- en: Adapt the notebook code to wrap another model you have used or worked with in
    the past.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将笔记本代码调整以适应您过去使用或合作过的其他模型。
- en: Obviously, the results of the last project demonstrate that using evolutionary
    search for DL optimization won’t work on larger parameter models. That doesn’t
    mean this technique is totally without merit, as we discuss in later chapters.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，最后一个项目的结果表明，使用进化搜索进行深度学习优化在更大的参数模型上不会奏效。但这并不意味着这项技术完全没有价值，正如我们在后面的章节中讨论的那样。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A simple multilayer perceptron network can be developed using the NumPy library.
    Sklearn can be used to generate a variety of single-label classification datasets
    that demonstrate binary model classifications with a simple NumPy MLP network.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用NumPy库开发一个简单的多层感知器网络。Sklearn可用于生成各种单标签分类数据集，这些数据集通过简单的NumPy MLP网络展示了二元模型分类。
- en: DEAP and genetic algorithms can be used to find the weights/parameters of a
    simple DL network.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DEAP和遗传算法可用于寻找简单深度学习网络的权重/参数。
- en: DEAP with evolutionary strategies and differential evolution can be used to
    optimize weight/parameter search on a simple MLP network. Comparing both methods
    can be useful to evaluate the tool to use for various evolutionary optimization
    methods and various sample classifications on problem datasets.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合进化策略和微分演化的DEAP可用于优化简单多层感知器网络的权重/参数搜索。比较这两种方法可以有助于评估用于各种进化优化方法和各种样本分类在问题数据集上使用的工具。
- en: A Keras DL model can be adapted to use an evolutionary search for weight optimization
    instead of the traditional method of differential backpropagation.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras深度学习模型可以调整以使用进化搜索进行权重优化，而不是传统的微分反向传播方法。
- en: Evolutionary weight optimization can be successful in solving complex and undifferentiable
    problems.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化权重优化可以成功解决复杂且不可微分的难题。
- en: DL problems that use automatic differentiation and backpropagation are limited
    to solving continuous problems.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动微分和反向传播的深度学习问题仅限于解决连续问题。
- en: Evolutionary optimization can be used to solve discontinuous problems previously
    unsolvable with DL networks.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化优化可用于解决以前深度学习网络无法解决的离散问题。
- en: Evolutionary optimization becomes much less successful as the problem scales.
    Applying EO to more complex problems, such as image classification, is not typically
    successful.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着问题规模的扩大，进化优化变得不太成功。将EO应用于更复杂的问题，如图像分类，通常不会成功。
