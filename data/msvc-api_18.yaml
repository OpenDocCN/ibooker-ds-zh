- en: 14 Deploying microservice APIs with Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 使用 Kubernetes 部署微服务 API
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Creating a cluster with AWS’s Elastic Kubernetes Service (EKS)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS 的 Elastic Kubernetes Service (EKS) 创建集群
- en: Exposing services using the AWS Load Balancer Controller
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Load Balancer Controller 暴露服务
- en: Deploying services to a Kubernetes cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将服务部署到 Kubernetes 集群
- en: Managing secrets securely in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中安全地管理机密
- en: Deploying an Aurora Serverless database
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署 Aurora Serverless 数据库
- en: Kubernetes is an open source container orchestration framework, and it’s fast
    becoming a standard way for deploying and managing applications across platforms.
    You can deploy Kubernetes yourself to your own servers, or you can use a managed
    Kubernetes service. In either case, you’ll get a consistent interface to your
    services, which means moving across cloud providers becomes less disruptive for
    your operations. You can also deploy a Kubernetes cluster in your machine and
    run your tests locally in much the same way you’d do in the cloud.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个开源的容器编排框架，它正在迅速成为跨平台部署和管理应用程序的标准方式。您可以自己将 Kubernetes 部署到自己的服务器上，或者您可以使用托管
    Kubernetes 服务。在两种情况下，您都将获得对服务的统一接口，这意味着跨云提供商迁移对您的运营影响较小。您还可以在自己的机器上部署 Kubernetes
    集群，并以与云中相同的方式在本地运行测试。
- en: Run kubernetes locally with minikube You can run a Kubernetes cluster locally
    using minikube. Although we won’t cover it in this chapter, minikube is a great
    tool to get more familiar with Kubernetes. Check out the official documentation
    for minikube ([https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 minikube 在本地运行 kubernetes 您可以使用 minikube 在本地运行 Kubernetes 集群。尽管我们不会在本章中介绍它，但
    minikube 是一个很好的工具，可以帮助您更熟悉 Kubernetes。请查看 minikube 的官方文档（[https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/)）。
- en: Deploying Kubernetes yourself is a good exercise to get more familiar with the
    technology, but in practice most companies use a managed service. In this chapter,
    we’ll use a Kubernetes managed service to deploy our cluster. Plenty of vendors
    offer Kubernetes managed services. The major players are Google Cloud’s Google
    Kubernetes Engine (GKE), Azure’s Kubernetes Service (AKS), and AWS’s Elastic Kubernetes
    Service (EKS). All three services are very robust and offer similar features.[¹](#pgfId-1118083)
    In this chapter, we’ll use EKS, which is currently the most popular managed Kubernetes
    service.[²](#pgfId-1118087)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自己部署 Kubernetes 是熟悉这项技术的良好练习，但在实践中，大多数公司都使用托管服务。在本章中，我们将使用 Kubernetes 托管服务来部署我们的集群。许多供应商提供
    Kubernetes 托管服务。主要玩家包括 Google Cloud 的 Google Kubernetes Engine (GKE)、Azure 的 Kubernetes
    服务 (AKS) 和 AWS 的 Elastic Kubernetes Service (EKS)。这三个服务都非常稳健，并提供类似的功能。[¹](#pgfId-1118083)
    在本章中，我们将使用 EKS，它是目前最受欢迎的托管 Kubernetes 服务。[²](#pgfId-1118087)
- en: To illustrate how to deploy applications to a Kubernetes cluster, we’ll use
    the example of the orders service. We’ll also create an Aurora Serverless database,
    and we’ll see how to securely feed the database connection credentials to the
    service using Kubernetes secrets.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何将应用程序部署到 Kubernetes 集群，我们将以订单服务为例。我们还将创建一个 Aurora Serverless 数据库，并展示如何使用
    Kubernetes secrets 安全地将数据库连接凭证传递给服务。
- en: The chapter doesn’t assume previous knowledge of AWS or Kubernetes. I’ve made
    an effort to explain every Kubernetes and AWS concept in detail so that you can
    follow along with the examples, even if you have no previous experience with either
    technology. Entire books have been written on these topics, so this chapter is
    just an overview, and I provide references to other resources you can use to dive
    deeper into these matters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不假设您对 AWS 或 Kubernetes 有先前的知识。我已经努力详细解释了 Kubernetes 和 AWS 的每个概念，以便您即使没有这两种技术的先前经验，也能跟随示例。关于这些主题已经写出了整本书，所以本章只是一个概述，并提供了一些其他资源的引用，您可以使用这些资源深入了解这些话题。
- en: Before proceeding, please bear in mind that EKS and other AWS services used
    in this chapter are for-fee services, so this is the only chapter in the book
    that will cost you some money if you follow along with the examples. The base
    cost of a Kubernetes cluster in AWS EKS is $0.10 per hour, which amounts to $2.40
    per day and roughly $72 per month. If budget is an issue, my recommendation is
    to read the chapter first to get an understanding of what we’re doing and then
    try out the EKS examples afterward. If this is your first time working with EKS
    and Kubernetes, it may take you one or two days to work through the examples,
    so try to schedule this time to work on the examples. Section 14.9 describes how
    to delete the EKS cluster, and all the other resources created in this chapter,
    to make sure you don’t incur additional costs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请记住，本章中使用的 EKS 和其他 AWS 服务是付费服务，所以这是本书中唯一一个如果你跟随示例可能会让你花费一些金钱的章节。在 AWS
    EKS 中的 Kubernetes 集群的基础费用是每小时 $0.10，相当于每天 $2.40，大约每月 $72。如果预算是个问题，我的建议是先阅读本章，了解我们在做什么，然后尝试
    EKS 示例。如果你是第一次使用 EKS 和 Kubernetes，可能需要一两天的时间来处理示例，所以尽量安排时间来处理这些示例。第 14.9 节描述了如何删除
    EKS 集群以及本章创建的所有其他资源，以确保你不产生额外的费用。
- en: Without further ado, let’s get started! We’ll begin by setting up the environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们开始吧！我们将从设置环境开始。
- en: 14.1 Setting up the environment for this chapter
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 设置本章的环境
- en: In this section, we set up the environment so that you can follow along with
    the examples in the rest of the chapter. Even if you’re not planning to try out
    the examples, I recommend you take at least a quick look at this section to learn
    about the tools we’re going to use. This chapter is heavy in tooling, so here
    we install the most important dependencies, and in the coming sections you’ll
    find additional instructions for other tools.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们设置环境，以便你可以跟随本章其余部分的示例。即使你并不打算尝试这些示例，我也建议你至少快速浏览这一节，了解我们将要使用的工具。本章工具较多，因此在这里我们安装最重要的依赖项，在接下来的章节中，你将找到其他工具的额外说明。
- en: 'First, copy over the code from chapter 13 into a new folder called ch14 by
    running the following command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过运行以下命令将第 13 章的代码复制到一个名为 ch14 的新文件夹中：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`cd` into ch14, install the dependencies, and activate the virtual environment
    by running the following commands:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 ch14 目录，安装依赖项，并通过运行以下命令激活虚拟环境：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we’ll deploy to AWS, we need to be able to access AWS services programmatically.
    In chapter 13, we installed and configured the AWS CLI. If you haven’t done so,
    please go back to section 13.1 and follow the steps to install and configure the
    AWS CLI.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将部署到 AWS，我们需要能够以编程方式访问 AWS 服务。在第 13 章中，我们安装并配置了 AWS CLI。如果你还没有这样做，请回到 13.1
    节，按照步骤安装和配置 AWS CLI。
- en: You’re going to learn how to deploy services to Kubernetes, so you also need
    to install the Kubernetes CLI, known as kubectl. There’re different ways to install
    kubectl depending on the platform that you’re using, so please refer to the official
    documentation to see which option works best for you ([https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何将服务部署到 Kubernetes，因此你还需要安装 Kubernetes CLI，即 kubectl。根据你使用的平台，安装 kubectl
    有不同的方法，所以请参考官方文档以查看哪个选项最适合你 ([https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/))。
- en: Finally, in this chapter we will make heavy use of `jq`—a CLI tool that helps
    us parse and query JSON documents. `jq` is not strictly necessary to follow along
    with the examples in this chapter, but it does make everything easier, and if
    you haven’t used the tool before, I highly encourage you to learn about it. We’ll
    use `jq` mostly for filtering JSON payloads and retrieving specific properties
    from them. As with Kubernetes, there are different installation options depending
    on your platform, so please refer to the official documentation to find out which
    strategy is best for you ([https://stedolan.github.io/jq/download/](https://stedolan.github.io/jq/download/)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在本章中我们将大量使用 `jq`——一个帮助我们解析和查询 JSON 文档的 CLI 工具。`jq` 并非严格必要，以跟随本章中的示例，但它确实让一切变得更容易，如果你之前没有使用过这个工具，我强烈建议你了解它。我们将主要使用
    `jq` 来过滤 JSON 有效负载并从中检索特定属性。与 Kubernetes 一样，根据你的平台，有不同的安装选项，所以请参考官方文档以了解哪种策略最适合你
    ([https://stedolan.github.io/jq/download/](https://stedolan.github.io/jq/download/))。
- en: Now that our environment is ready, it’s deployment time! Before we create the
    cluster, the next section explains some of the main concepts related to Kubernetes
    to make sure you can follow the upcoming sections. If you have previous experience
    with Kubernetes, you can skip section 14.2.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了环境，是时候部署了！在我们创建集群之前，下一节将解释一些与Kubernetes相关的主要概念，以确保你能理解接下来的章节。如果你有Kubernetes的先前经验，可以跳过第14.2节。
- en: '14.2 How Kubernetes works: The “CliffsNotes” version'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 Kubernetes的工作原理：简化版
- en: So, what is Kubernetes? If you don’t have previous experience with Kubernetes
    or are still confused about how it works, this section offers a hyper-compressed
    introduction to its main components.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Kubernetes是什么？如果你没有Kubernetes的先前经验或者对其工作原理仍然感到困惑，本节提供了一个对Kubernetes主要组件的超压缩介绍。
- en: Kubernetes is an open source container orchestration tool. *Container orchestration*
    is the process of running containerized applications. In addition to container
    orchestration, Kubernetes also helps us automate deployments, and it handles graceful
    rollouts and rollbacks, scaling applications, and more.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个开源的容器编排工具。*容器编排*是指运行容器化应用的过程。除了容器编排，Kubernetes还帮助我们自动化部署，并处理优雅的滚动发布和回滚、应用扩展等更多功能。
- en: Figure 14.1 offers a high-level overview of the main components of a Kubernetes
    cluster. The core of a Kubernetes cluster is the control plane, a process that
    runs the Kubernetes API for our cluster, controls its state, and manages the available
    resources, among many other tasks. It’s also possible to install add-ons on the
    control plane, including specific DNS servers such as CoreDNS.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1提供了Kubernetes集群主要组件的高级概述。Kubernetes集群的核心是控制平面，这是一个运行我们集群的Kubernetes API、控制其状态并管理可用资源的过程，以及其他许多任务。在控制平面上也可以安装附加组件，包括特定的DNS服务器，如CoreDNS。
- en: '![](../Images/14-01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-01.png)'
- en: Figure 14.1 High-level architecture of a Kubernetes cluster showing how all
    components of a cluster come together.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 Kubernetes集群的高级架构图，展示了集群所有组件如何协同工作。
- en: DEFINITION The *Kubernetes control plane* is a process that runs the Kubernetes
    API and controls the state of the cluster and manages the available resources,
    scheduling, and many other tasks. For more information about the control plane,
    see chapters 11 ([http://mng.bz/yayE](http://mng.bz/yayE)) and 12 ([http://mng.bz/M0dm](http://mng.bz/M0dm))
    from *Core Kubernetes* by Jay Vyas and Chris Love (Manning, 2022).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** Kubernetes控制平面是一个运行Kubernetes API并控制集群状态以及管理可用资源、调度和其他许多任务的过程。有关控制平面的更多信息，请参阅Jay
    Vyas和Chris Love所著的《Core Kubernetes》的第11章([http://mng.bz/yayE](http://mng.bz/yayE))和第12章([http://mng.bz/M0dm](http://mng.bz/M0dm))（Manning,
    2022）。'
- en: 'The smallest unit of computing in Kubernetes is the *pod*: a wrapper around
    containers that can include one or more containers. The most common practice is
    to run one container per pod, and in this chapter, we deploy the orders service
    as a single container per pod.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中最小的计算单元是*pod*：围绕一个或多个容器的一个包装器。最常见的做法是每个*pod*运行一个容器，在本章中，我们将订单服务作为每个*pod*的单个容器进行部署。
- en: 'To deploy pods into the cluster, we use *workloads*. Kubernetes has four types
    of workloads: `Deployment`, `StatefulSet`, `DaemonSet`, and `Job/CronJob`. `Deployment`
    is the most common type of Kubernetes workload and is useful for running stateless
    distributed applications. `StatefulSet` is used for running distributed applications
    whose state needs to be synchronized. You use `DaemonSet` to define processes
    that should run on all or most of the nodes in the cluster, such as log collectors.
    `Job` and `CronJob` help us to define one-off processes or applications that need
    to be run on a schedule, such as once a day or once a week.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要将*pods*部署到集群中，我们使用*工作负载*。Kubernetes有四种类型的工作负载：`Deployment`、`StatefulSet`、`DaemonSet`和`Job/CronJob`。`Deployment`是Kubernetes中最常见的工作负载类型，适用于运行无状态分布式应用。`StatefulSet`用于运行需要同步状态的状态化分布式应用。使用`DaemonSet`来定义应在集群的所有或大多数节点上运行的进程，例如日志收集器。`Job`和`CronJob`帮助我们定义一次性进程或应用，这些进程或应用需要按计划运行，例如每天或每周一次。
- en: To deploy a microservice, we use either a `Deployment` or a `StatefulSet`. Since
    our services are all stateless, in this chapter we deploy the orders service as
    a `Deployment`. To manage the number of pods, deployments use the concept of a
    `ReplicaSet`, a process that maintains the desired number of pods in the cluster.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署微服务，我们使用 `Deployment` 或 `StatefulSet`。由于我们的服务都是无状态的，在本章中我们将订单服务作为 `Deployment`
    进行部署。为了管理 pod 的数量，部署使用 `ReplicaSet` 的概念，这是一个维护集群中所需 pod 数量的进程。
- en: 'Workloads are normally scoped within *namespaces*. In Kubernetes, namespaces
    are logical groupings of resources that allow us to isolate and scope our deployments.
    For example, we can create a namespace for each service in our platform. Namespaces
    make it easier to manage our deployments and to avoid name conflicts: the names
    of our resources must be unique within a namespace but don’t have to be across
    namespaces.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载通常在 *命名空间* 内进行范围划分。在 Kubernetes 中，命名空间是资源的逻辑分组，允许我们隔离和范围我们的部署。例如，我们可以在平台中的每个服务上创建一个命名空间。命名空间使得管理我们的部署和避免名称冲突变得更容易：我们的资源名称必须在命名空间内是唯一的，但不需要在命名空间之间是唯一的。
- en: To run our applications as web services, Kubernetes offers the concept of *services*—processes
    that manage the interfaces of our pods and enable communication between them.
    To expose our services through the internet, we use a *load balancer*, which sits
    in front of the Kubernetes cluster, and forwards traffic to the services based
    on ingress rules.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的应用程序作为网络服务运行，Kubernetes 提供了 *服务* 的概念——这些是管理我们 pod 接口并使它们之间能够通信的进程。为了通过互联网公开我们的服务，我们使用一个
    *负载均衡器*，它位于 Kubernetes 集群之前，并根据入口规则将流量转发到服务。
- en: The final piece of the Kubernetes system is the *node*, which represents the
    actual computing resources in which our services run. We define nodes as computing
    resources since they can be anything from physical servers to virtual machines.
    For example, when running a Kubernetes cluster in AWS, our nodes will be represented
    by EC2 machines.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 系统的最后一部分是 *节点*，它代表我们的服务运行的实际计算资源。我们将节点定义为计算资源，因为它们可以是物理服务器到虚拟机中的任何东西。例如，当在
    AWS 上运行 Kubernetes 集群时，我们的节点将由 EC2 机器表示。
- en: Now that we understand what the main parts of Kubernetes are, let’s create a
    cluster!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Kubernetes 的主要组成部分，让我们创建一个集群吧！
- en: 14.3 Creating a Kubernetes cluster with EKS
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 使用 EKS 创建 Kubernetes 集群
- en: In this section, you’ll learn how to create a Kubernetes cluster using the AWS
    EKS. We launch the Kubernetes cluster using eksctl, which is the recommended tool
    for managing Kubernetes in AWS.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何使用 AWS EKS 创建 Kubernetes 集群。我们使用 eksctl 启动 Kubernetes 集群，这是在 AWS
    中管理 Kubernetes 的推荐工具。
- en: eksctl is an open source tool created and maintained by Weaveworks. It uses
    CloudFormation behind the scenes to create and manage changes to our Kubernetes
    clusters. This is excellent news, because it means we can reuse the CloudFormation
    templates to replicate the same infrastructure across different environments.
    It also makes all our changes to the cluster visible through CloudFormation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: eksctl 是由 Weaveworks 创建和维护的一个开源工具。它使用 CloudFormation 在幕后创建和管理 Kubernetes 集群的变化。这是一个好消息，因为它意味着我们可以重用
    CloudFormation 模板来在不同环境中复制相同的架构。这也使得我们对集群的所有更改都通过 CloudFormation 可见。
- en: Definition *CloudFormation* is AWS’s infrastructure-as-code service. With CloudFormation,
    we can declare our resources in YAML or JSON files called *templates*. When we
    submit the templates to CloudFormation, AWS creates a CloudFormation *stack*,
    the collection of resources defined in the templates. CloudFormation templates
    shouldn’t contain sensitive information and can be committed in our code repositories,
    which makes changes to our infrastructure very visible and replicable across different
    environments.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 *CloudFormation* 是 AWS 的基础设施即代码服务。使用 CloudFormation，我们可以在称为 *模板* 的 YAML 或
    JSON 文件中声明我们的资源。当我们提交模板到 CloudFormation 时，AWS 创建一个 CloudFormation *堆栈*，即模板中定义的资源集合。CloudFormation
    模板不应包含敏感信息，并且可以提交到我们的代码仓库中，这使得我们对基础设施的更改非常可见，并且可以在不同的环境中复制。
- en: There are various ways to install eksctl depending on the platform you’re using,
    so please refer to the official documentation to find out which strategy works
    best for you ([https://github.com/weaveworks/eksctl](https://github.com/weaveworks/eksctl)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您使用的平台，安装 eksctl 有多种方法，因此请参阅官方文档以了解哪种策略最适合您（[https://github.com/weaveworks/eksctl](https://github.com/weaveworks/eksctl)）。
- en: To run the containers in the Kubernetes cluster, we use AWS Fargate. As you
    can see in figure 14.2, Fargate is AWS’s serverless container service that allows
    us to run containers in the cloud without having to provision servers. With AWS
    Fargate, you don’t need to worry about scaling your servers up or down, since
    Fargate takes care of that.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Kubernetes集群中运行容器，我们使用AWS Fargate。如图14.2所示，Fargate是AWS的无服务器容器服务，允许我们在云中运行容器而无需配置服务器。使用AWS
    Fargate，你无需担心服务器扩展的问题，因为Fargate会处理这些。
- en: '![](../Images/14-02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-02.png)'
- en: Figure 14.2 AWS Fargate automatically provisions the servers required to operate
    our Kubernetes cluster.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 AWS Fargate自动配置运行我们的Kubernetes集群所需的服务器。
- en: 'To create a Kubernetes cluster using eksctl, run the following command:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用eksctl创建Kubernetes集群，请运行以下命令：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The creation process takes approximately 30 minutes to complete. Let’s look
    at each flag in this command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 创建过程大约需要30分钟来完成。让我们看看这个命令中的每个标志：
- en: '`--name`—The name of the cluster. We’re calling the cluster `coffeemesh`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--name`—集群的名称。我们将集群命名为`coffeemesh`。'
- en: '`--region`—The AWS region where you want to deploy the cluster. This region
    should be the same you used to create the ECR repository in section 13.4.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--region`—你想要部署集群的AWS区域。这个区域应该与你在第13.4节中创建ECR仓库时使用的区域相同。'
- en: '`--fargate`—Creates a Fargate profile to schedule pods in the `default` and
    the `kube-system` namespaces. Fargate profiles are policies that determine which
    pods must be launched by Fargate.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--fargate`—创建一个Fargate配置文件，用于在`default`和`kube-system`命名空间中调度Pod。Fargate配置文件是确定哪些Pod必须由Fargate启动的策略。'
- en: '`--alb-ingress-access`—Enables access to the cluster through an Application
    Load Balancer.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--alb-ingress-access`—通过应用程序负载均衡器启用对集群的访问。'
- en: Figure 14.3 illustrates the architecture of the stack created by eksctl when
    launching the Kubernetes cluster. By default, eksctl creates a dedicated Virtual
    Private Cloud (VPC) for the cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3说明了eksctl在启动Kubernetes集群时创建的堆栈架构。默认情况下，eksctl为集群创建一个专用的虚拟私有云（VPC）。
- en: '![](../Images/14-03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-03.png)'
- en: Figure 14.3 eksctl creates a VPC with three public networks, three private networks,
    two CIDR reservations, and two VPC security groups. It also deploys the Kubernetes
    cluster within the VPC.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 eksctl创建了一个包含三个公共网络、三个私有网络、两个CIDR预留和两个VPC安全组的VPC。它还在VPC内部部署了Kubernetes集群。
- en: 'Kubernetes networking To make advanced use of Kubernetes, you need to understand
    how networking works in Kubernetes. To learn more about Kubernetes networking,
    check out *Networking and Kubernetes: A Layered Approach* by James Strong and
    Vallery Lancey (O’Reilly, 2021).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kubernetes网络要高级使用Kubernetes，你需要了解Kubernetes中的网络是如何工作的。要了解更多关于Kubernetes网络的信息，请查看James
    Strong和Vallery Lancey所著的《Networking and Kubernetes: A Layered Approach》（O’Reilly，2021年）。'
- en: It’s also possible to launch the cluster within an existing VPC by specifying
    the subnets in which you want to run the deployment. If launching within an existing
    VPC, you must make sure the VPC and the provided subnets are correctly configured
    for operating the Kubernetes cluster. See the eksctl documentation to learn about
    the networking requirements of a Kubernetes cluster ([https://eksctl.io/usage/vpc-networking/](https://eksctl.io/usage/vpc-networking/))
    and the official AWS documentation on the VPC networking requirements for a Kubernetes
    cluster ([http://mng.bz/aPRY](http://mng.bz/aPRY)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在现有的VPC内部启动集群，通过指定你想要在其中运行部署的子网来实现。如果在现有的VPC内部启动，你必须确保VPC和提供的子网已正确配置以运行Kubernetes集群。有关Kubernetes集群的网络要求，请参阅eksctl文档（[https://eksctl.io/usage/vpc-networking/](https://eksctl.io/usage/vpc-networking/)）和AWS关于Kubernetes集群VPC网络要求的官方文档（[http://mng.bz/aPRY](http://mng.bz/aPRY)）。
- en: 'As you can see in figure 14.3, eksctl creates six subnets by default: three
    public and three private, with their corresponding NAT gateways and routing tables.
    A *subnet* is a subset of the IP addresses available in a VPC. Public subnets
    are accessible through the internet, while private subnets are not. eksctl also
    creates two subnet CIDR reservations for internal use by Kubernetes, as well as
    two security groups; one of them allows communication between all nodes in the
    cluster, and the other allows communication between the control plane and the
    nodes.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如图14.3所示，eksctl默认创建六个子网：三个公共和三个私有，以及它们对应的NAT网关和路由表。*子网*是VPC中可用IP地址的子集。公共子网可以通过互联网访问，而私有子网则不行。eksctl还创建了两个用于Kubernetes内部使用的子网CIDR预留，以及两个安全组；其中一个允许集群中所有节点之间的通信，另一个允许控制平面与节点之间的通信。
- en: 'Definition *CIDR* stands for Classless Inter-Domain Routing, and it’s a notation
    used for representing ranges of IP addresses. CIDR notation includes an IP address
    followed by a slash and a decimal number, where the decimal number represents
    the range of addresses. For example, 255.255.255.255/32 represents a range for
    one address. To learn more about CIDR notation, see Wikipedia’s article: [https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*CIDR*定义的无类域间路由，是一种用于表示IP地址范围的表示法。CIDR表示法包括一个IP地址后跟一个斜杠和一个十进制数，其中十进制数表示地址范围。例如，255.255.255.255/32表示一个地址的范围。要了解更多关于CIDR表示法的信息，请参阅维基百科的文章：[https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing)。'
- en: 'Once we’ve created the cluster, we can configure kubectl to point to it, which
    will allow us to manage the cluster with the command line. Use the following command
    to point kubectl to the cluster:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了集群，我们就可以配置kubectl指向它，这将允许我们通过命令行管理集群。使用以下命令将kubectl指向集群：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we’re connected to the cluster, we can inspect its properties. For
    example, we can get a list of running nodes with the following command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经连接到集群，我们可以检查其属性。例如，我们可以使用以下命令获取正在运行的节点列表：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To get the list of pods running in the cluster, run the following command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取集群中运行的Pod列表，请运行以下命令：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are many more useful commands you can run to learn more about your cluster.
    Check out the official documentation about the Kubernetes CLI for additional commands
    and options ([https://kubernetes.io/docs/reference/kubectl/](https://kubernetes.io/docs/reference/kubectl/)).
    A good starting point is the kubectl cheat sheet ([https://kubernetes.io/docs/reference/kubectl/cheatsheet/](https://kubernetes.io/docs/reference/kubectl/cheatsheet/)).
    Now that our cluster is up and running, in the next section, we’ll create an IAM
    role for our Kubernetes service accounts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以运行许多其他有用的命令来了解更多关于您的集群的信息。查看Kubernetes CLI的官方文档以获取更多命令和选项（[https://kubernetes.io/docs/reference/kubectl/](https://kubernetes.io/docs/reference/kubectl/)）。一个好的起点是kubectl速查表（[https://kubernetes.io/docs/reference/kubectl/cheatsheet/](https://kubernetes.io/docs/reference/kubectl/cheatsheet/)）。现在我们的集群已经启动并运行，在下一节中，我们将为我们的Kubernetes服务账户创建一个IAM角色。
- en: 14.4 Using IAM roles for Kubernetes service accounts
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 为Kubernetes服务账户使用IAM角色
- en: Every process that runs in your Kubernetes cluster has an identity, and that
    identity is given by a *service account*. Service accounts determine the access
    privileges of a process within the cluster. Sometimes, our services need to interact
    with AWS resources using the AWS API. To give access to the AWS API, we need to
    create *IAM roles*—entities that give applications access to the AWS API—for our
    services. As you can see in figure 14.4, to link a Kubernetes service account
    to an IAM role, we use OpenID Connect (OIDC). By using OIDC, our pods can obtain
    temporary credentials to access the AWS API.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的Kubernetes集群中运行的每个进程都有一个身份，这个身份由一个*服务账户*提供。服务账户决定了进程在集群内的访问权限。有时，我们的服务需要使用AWS
    API与AWS资源进行交互。为了给AWS API提供访问权限，我们需要为我们的服务创建*IAM角色*——这些实体为应用程序提供访问AWS API的权限。如图14.4所示，要将Kubernetes服务账户链接到IAM角色，我们使用OpenID
    Connect (OIDC)。通过使用OIDC，我们的Pod可以获取临时凭证以访问AWS API。
- en: '![](../Images/14-04.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-04.png)'
- en: Figure 14.4 Pods can authenticate with an OIDC provider to assume an IAM role,
    which gives them access to the AWS API, and therefore gives them access to AWS
    services.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 Pod可以通过OIDC提供者进行身份验证以假定IAM角色，这使它们能够访问AWS API，因此能够访问AWS服务。
- en: 'To check if your cluster has an OIDC provider, run the following command, replacing
    `<cluster_name>` with the name of your cluster:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查您的集群是否有OIDC提供者，请运行以下命令，将`<cluster_name>`替换为您的集群名称：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You’ll get an output like the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下类似的输出：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this case, the ID of the cluster’s OIDC provider is `BE4E5EE7DCDF9FB198D06FC9883FF1BE`.
    Grab the OIDC provider’s ID and run the following command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，集群OIDC提供者的ID是`BE4E5EE7DCDF9FB198D06FC9883FF1BE`。获取OIDC提供者的ID并运行以下命令：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This command lists all the OIDC providers in your AWS account, and it uses
    `grep` to filter by the ID of your cluster’s OIDC provider. If you get a result,
    it means you already have an OIDC provider for your cluster. If you don’t get
    any output, it means you don’t have an OIDC provider, so let’s create one! To
    create an OIDC provider for your cluster, run the following command, replacing
    `<cluster_name>` with the name of your cluster:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令列出您AWS账户中的所有OIDC提供者，并使用`grep`根据您集群OIDC提供者的ID进行过滤。如果您得到结果，这意味着您已经为您的集群配置了OIDC提供者。如果您没有输出任何内容，这意味着您没有OIDC提供者，因此让我们创建一个！要为您的集群创建OIDC提供者，请运行以下命令，将`<cluster_name>`替换为您的集群名称：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That’s all it takes. Now we can link IAM roles to our service accounts! In the
    next section, we deploy a Kubernetes load balancer to enable external traffic
    to the cluster.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。现在我们可以将IAM角色链接到我们的服务账户！在下一节中，我们将部署一个Kubernetes负载均衡器以使集群能够接收外部流量。
- en: 14.5 Deploying a Kubernetes load balancer
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.5 部署Kubernetes负载均衡器
- en: Right now, our cluster is not accessible from outside of the VPC. If we deploy
    our applications, they’ll only get internal IPs and therefore won’t be accessible
    to the external world. To enable external access to the cluster, we need an *ingress
    controller*. As you can see in figure 14.5, an ingress controller accepts traffic
    from outside of the Kubernetes cluster and load balances it among our pods. To
    redirect traffic to specific pods, we create *ingress resources* for each service.
    The ingress controller takes care of managing ingress resources.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的集群无法从VPC外部访问。如果我们部署应用程序，它们将只获得内部IP，因此无法对外部世界访问。为了使集群能够从外部访问，我们需要一个*入口控制器*。如图14.5所示，入口控制器接受来自Kubernetes集群外部的流量，并在我们的Pod之间进行负载均衡。为了将流量重定向到特定的Pod，我们为每个服务创建*入口资源*。入口控制器负责管理入口资源。
- en: '![](../Images/14-05.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-05.png)'
- en: Figure 14.5 An ingress controller accepts traffic from outside of the Kubernetes
    cluster and forwards it to the pods according to rules defined by ingress resources.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5 入口控制器接受来自Kubernetes集群外部的流量，并根据入口资源定义的规则将其转发到Pod。
- en: In this section, we’ll deploy a Kubernetes ingress controller as an AWS Load
    Balancer Controller.[³](#pgfId-1118240) As you can see in figure 14.5, the AWS
    Load Balancer Controller deploys an AWS Application Load Balancer (ALB), which
    sits in front of our cluster, captures incoming traffic, and forwards it to our
    services. To forward traffic to our services, the ALB uses the concept of *target
    groups*—a rule for how traffic should be forwarded from the ALB to a specific
    resource. For example, we can have target groups based on IPs, services IDs, and
    other factors. The load balancer monitors the health of its registered targets
    and makes sure traffic is only redirected to healthy targets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将部署一个Kubernetes入口控制器作为AWS负载均衡器控制器。[³](#pgfId-1118240)如图14.5所示，AWS负载均衡器控制器部署了一个AWS应用程序负载均衡器（ALB），它位于我们的集群前面，捕获传入的流量并将其转发到我们的服务。为了将流量转发到我们的服务，ALB使用*目标组*的概念——这是从ALB到特定资源转发流量的规则。例如，我们可以根据IP、服务ID和其他因素设置目标组。负载均衡器监控其注册目标的状态，确保流量只被重定向到健康的目标。
- en: 'To install the AWS Load Balancer Controller, we need to have an OIDC provider
    in the cluster, so make sure you’ve gone through section 14.4 before proceeding.
    The first step to deploying an AWS Load Balancer Controller is to create an IAM
    policy that gives the controller access to the relevant AWS APIs. The open source
    community that maintains the AWS Load Balancer Controller project provides a sample
    of the policy that we need, so we simply need to fetch it:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装AWS负载均衡器控制器，我们需要在集群中有一个OIDC提供者，因此请确保您在继续之前已经阅读了第14.4节。部署AWS负载均衡器控制器的第一步是创建一个IAM策略，该策略允许控制器访问相关的AWS
    API。维护AWS负载均衡器控制器项目的开源社区提供了一个我们需要策略的样本，因此我们只需获取它：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After running this command, you’ll see a file called alb_controller_policy.json
    in your directory. Now we can create the IAM policy using this file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，你会在你的目录中看到一个名为 alb_controller_policy.json 的文件。现在我们可以使用此文件创建 IAM 策略：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next step is to create an IAM role associated to a Kubernetes service account
    for the load balancer with the following command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个 IAM 角色与 Kubernetes 服务账户关联，用于负载均衡器，命令如下：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This command creates a CloudFormation stack, which includes an IAM role associated
    with the policy we created earlier, as well as a service account named `alb-controller`
    within the `kube-system` namespace reserved for system components of the Kubernetes
    cluster.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建一个 CloudFormation 堆栈，其中包括与之前创建的策略关联的 IAM 角色，以及名为 `alb-controller` 的服务账户，位于为
    Kubernetes 集群的系统组件保留的 `kube-system` 命名空间内。
- en: Now we can install the Load Balancer Controller. We’ll use Helm to install the
    controller, a package manager for Kubernetes. If you don’t have Helm available
    in your machine, you need to install it. There are different strategies for installing
    Helm depending on your platform, so make sure you check out the documentation
    to see which option works best for you ([https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以安装负载均衡器控制器。我们将使用 Helm 来安装控制器，它是 Kubernetes 的包管理器。如果你在机器上没有安装 Helm，你需要安装它。根据你的平台，安装
    Helm 有不同的策略，所以请确保查看文档以了解哪种选项最适合你（[https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/))）。
- en: 'Once Helm is available on your machine, you need to update it by adding the
    EKS charts repository to your local `helm` (in Helm, packages are called *charts*).
    To add the EKS charts, run the following command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Helm 在你的机器上可用，你需要通过添加 EKS 图表存储库到你的本地 `helm`（在 Helm 中，包被称为 *charts*）来更新它。要添加
    EKS 图表，请运行以下命令：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let’s update `helm` to make sure we pick up the most recent updates to
    the charts:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更新 `helm`，以确保我们获取到最新的图表更新：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that `helm` is up to date, we can install the AWS Load Balancer Controller.
    To install the controller, we need to get hold of the ID of the VPC eksctl created
    when we launched the cluster. To find the VPC ID, run the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `helm` 已更新，我们可以安装 AWS 负载均衡器控制器。要安装控制器，我们需要获取在启动集群时 eksctl 创建的 VPC 的 ID。要找到
    VPC ID，请运行以下命令：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To run the previous command successfully, you need to have `jq` installed.
    Please refer to section 14.1 to learn how to install it. Now we can install the
    controller by running the following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功运行前面的命令，你需要安装 `jq`。请参阅第 14.1 节了解如何安装它。现在我们可以通过运行以下命令来安装控制器：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Since the controller is an internal Kubernetes component, we install it within
    the `kube-system` namespace. We make sure that the controller is installed for
    the `coffeemesh` cluster. We also instruct Helm not to create a new service account
    for the controller, and instead use the `alb-controller` service account we created
    earlier.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于控制器是 Kubernetes 的内部组件，我们在 `kube-system` 命名空间内安装它。我们确保控制器为 `coffeemesh` 集群安装。我们还指示
    Helm 不要为控制器创建新的服务账户，而是使用我们之前创建的 `alb-controller` 服务账户。
- en: 'It’ll take a few minutes until all the resources are created. To verify that
    the deployment went well, run the following command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所有资源创建需要几分钟时间。要验证部署是否成功，请运行以下命令：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You’ll know that the controller is up and running when the `READY` column shows
    2/2, which means the desired number of resources are up. Our cluster is now ready,
    so it’s time to deploy the orders service!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `READY` 列显示 2/2 时，你就知道控制器正在运行，这意味着所需的资源数量已经启动。我们的集群现在已准备就绪，是时候部署订单服务了！
- en: 14.6 Deploying microservices to the Kubernetes cluster
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.6 在 Kubernetes 集群中部署微服务
- en: Now that our Kubernetes cluster is ready, it’s time to start deploying our services!
    In this section, we walk through the steps required to deploy the orders service.
    You can follow the same steps to deploy other services of the CoffeeMesh platform.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的 Kubernetes 集群已准备就绪，是时候开始部署我们的服务了！在本节中，我们将介绍部署订单服务所需的步骤。你可以遵循相同的步骤来部署 CoffeeMesh
    平台的其他服务。
- en: '![](../Images/14-06.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-06.png)'
- en: Figure 14.6 To deploy a microservice, we create a new namespace, and within
    this namespace we deploy all the components needed to operate the microservice,
    such as a `Deployment` object and a `Service` object.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 要部署一个微服务，我们创建一个新的命名空间，并在该命名空间内部署所有运行微服务所需的组件，例如 `Deployment` 对象和 `Service`
    对象。
- en: 'As you can see in figure 14.6, we deploy the orders service to a new namespace
    called `orders-service`. This allows us to logically group and isolate all the
    resources required to operate the orders service. To create a new namespace, we
    run the following command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如图14.6所示，我们将订单服务部署到名为`orders-service`的新命名空间。这允许我们逻辑上分组和隔离操作订单服务所需的所有资源。要创建新的命名空间，请运行以下命令：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Since we’ll run the orders service in the new namespace, we also need to create
    a new Fargate profile configured to schedule jobs within the `orders-service`
    namespace. To create the new Fargate profile, run the following command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在新的命名空间中运行订单服务，因此我们还需要创建一个新的Fargate配置文件，该配置文件配置为在`orders-service`命名空间内调度作业。要创建新的Fargate配置文件，请运行以下命令：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With the `orders-service` namespace and the Fargate profile ready, we can deploy
    the orders service. To make the deployment, we take the following steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 随着`orders-service`命名空间和Fargate配置文件的准备就绪，我们可以部署订单服务。要执行部署，我们采取以下步骤：
- en: Create a deployment object for the orders service.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为订单服务创建一个部署对象。
- en: Create a service object.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个服务对象。
- en: Create an ingress resource to expose the service.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个ingress资源以公开服务。
- en: The following sections explain in detail how to proceed in each step.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节将详细说明如何在每个步骤中进行操作。
- en: 14.6.1 Creating a deployment object
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.6.1 创建部署对象
- en: Let’s begin by creating a deployment for the orders service using a service
    manifest file. As you can see in figure 14.7, deployments are Kubernetes objects
    that operate our pods and provision them with everything they need to run, including
    a Docker image and port configuration. Create a file named orders-service-deployment.yaml
    and copy the contents of listing 14.1 into it.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个用于订单服务的部署开始，使用服务清单文件。如图14.7所示，部署是Kubernetes对象，它们操作我们的Pod，并为它们提供运行所需的一切，包括Docker镜像和端口配置。创建一个名为orders-service-deployment.yaml的文件，并将列表14.1的内容复制到其中。
- en: '![](../Images/14-07.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-07.png)'
- en: Figure 14.7 A `Deployment` object provides necessary configuration for the pods,
    such as their Docker image and port configuration, and ensures we have the desired
    number of pods running.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7 `Deployment`对象为Pod提供必要的配置，例如它们的Docker镜像和端口配置，并确保我们运行所需的Pod数量。
- en: We use Kubernetes’ API version apps/v1 and declare this object a `Deployment`.
    In metadata, we name the deployment `orders-service`, we specify its namespace,
    and we add the label `app:` `orders-service`. *Labels* are custom identifiers
    for Kubernetes objects, and they can be used for monitoring, tracing, or scheduling
    tasks, among other uses.[⁴](#pgfId-1118329)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Kubernetes的API版本apps/v1，并声明此对象为`Deployment`。在元数据中，我们命名部署为`orders-service`，指定其命名空间，并添加标签`app:`
    `orders-service`。*标签*是Kubernetes对象的自定义标识符，可用于监控、跟踪或调度任务等多种用途。[⁴](#pgfId-1118329)
- en: In the `spec` section, we define a selector rule that matches pods with the
    label `app:` `orders-service`, which means this deployment will only operate pods
    with this label. We also declare that we’d like to run only one replica of the
    pod.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在`spec`部分，我们定义了一个选择器规则，该规则匹配带有标签`app:` `orders-service`的Pod，这意味着此部署将仅操作具有此标签的Pod。我们还声明我们只想运行一个Pod副本。
- en: Within the `spec.template` section, we define the pod operated by this deployment.
    We label the pod with the `app:` `orders-service` key-value pair in agreement
    with the deployment’s selector rule. Within the pod’s `spec` section, we declare
    the containers that belong in the pod. In this case, we want to run just one container,
    which is the orders service application. Within the definition of the orders service
    container, we specify the image that must be used to run the application with
    the port on which the application runs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在`spec.template`部分，我们定义由此部署操作的Pod。我们使用与部署选择器规则一致的`app:` `orders-service`键值对标记Pod。在Pod的`spec`部分，我们声明属于Pod的容器。在这种情况下，我们只想运行一个容器，即订单服务应用程序。在订单服务容器的定义中，我们指定运行应用程序必须使用的镜像以及应用程序运行的端口。
- en: Listing 14.1 Declaring a deployment manifest
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1 声明部署清单
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Version of the Kubernetes API used in this manifest
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ① 此清单中使用的Kubernetes API版本
- en: ② This manifest defines a Deployment object.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ② 此清单定义了一个Deployment对象。
- en: ③ The deployment’s name
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 部署的名称
- en: ④ The namespace within which the deployment must be located
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 部署必须位于的命名空间
- en: ⑤ A label for the deployment
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 部署的标签
- en: ⑥ The deployment’s specification
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 部署的规范
- en: ⑦ How many pods must be deployed
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 需要部署多少个Pod
- en: ⑧ A label selector for pods
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ Pod的标签选择器
- en: ⑨ Template for the pods
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ Pod的模板
- en: ⑩ A label for the pods
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ Pod的标签
- en: ⑪ Specification for the pods
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ Pod的规范
- en: ⑫ The pods’ image
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ Pod的镜像
- en: ⑬ The port on which the API runs
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ API运行的端口
- en: 'To create the deployment, we run the following command:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建部署，我们运行以下命令：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This command creates the deployment and launches the pods we defined in the
    manifest file. It’ll take a few seconds for the pods to become available. You
    can check their state with the following command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建部署并启动清单文件中定义的Pod。Pod变为可用可能需要几秒钟。您可以使用以下命令检查它们的状态：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The initial state of the pods will be `Pending`, and once they’re up and running
    their state will change to `Running`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Pod的初始状态将是`Pending`，一旦它们启动并运行，其状态将变为`Running`。
- en: What is a Kubernetes manifest file?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是Kubernetes清单文件？
- en: In Kubernetes, we can create objects using manifest files. Objects are resources
    such as namespaces, deployments, services, and so on. A manifest file is a YAML
    file that describes the properties of the object and its desired state. Using
    manifest files is convenient because they can be tracked in source control, which
    helps us trace changes to our infrastructure.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，我们可以使用清单文件创建对象。对象是资源，如命名空间、部署、服务等。清单文件是一个YAML文件，描述了对象的属性及其期望状态。使用清单文件很方便，因为它们可以在源控制中跟踪，这有助于我们跟踪对基础设施的更改。
- en: 'Each manifest file contains, at a minimum, the following properties:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每个清单文件至少包含以下属性：
- en: '`apiVersion`—The version of the Kubernetes API that we want to use. Each Kubernetes
    object has its own stable version. You can check the latest stable version of
    each object for your Kubernetes cluster by running the following command: `kubectl`
    `api-resources`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apiVersion`—我们想要使用的Kubernetes API版本。每个Kubernetes对象都有自己的稳定版本。您可以通过运行以下命令来检查您Kubernetes集群中每个对象的最新稳定版本：`kubectl
    api-resources`。'
- en: '`kind`—The kind of object that we’re creating. Possible values include `Service`,
    `Ingress`, and `Deployment`, among others.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind`—我们正在创建的对象类型。可能的值包括`Service`、`Ingress`和`Deployment`等。'
- en: '`metadata`—A collection of properties that provide identifying information
    about the object, such as its name, its namespace, and additional labels.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`—一组属性，提供有关对象标识信息的集合，例如其名称、其命名空间和附加标签。'
- en: '`spec`—The specification for the object. For example, if we’re creating a service,
    we use this section to specify the type of service we’re creating (e.g., `NodePort`)
    and selector rules.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec`—对象的规范。例如，如果我们正在创建服务，我们使用此部分来指定我们正在创建的服务类型（例如，`NodePort`）和选择规则。'
- en: 'To create an object from a manifest file, we use the `kubectl` `apply` command.
    For example, if we have a manifest file called deployment.yaml, we apply it using
    the following command:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要从清单文件创建对象，我们使用`kubectl apply`命令。例如，如果我们有一个名为deployment.yaml的清单文件，我们使用以下命令应用它：
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 14.6.2 Creating a service object
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.6.2 创建服务对象
- en: Now that our deployment is ready, we will create a service object for the orders
    service. As we learned in section 14.2, services are Kubernetes objects that allow
    us to expose our pods as networking services. As you can see in figure 14.8, a
    service object exposes our applications as web services and redirects traffic
    from the cluster to our pods on the specified ports. Create a file named orders-service.yaml,
    and copy into it the contents of listing 14.2, which shows how to configure a
    simple service manifest.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的部署已经就绪，我们将为订单服务创建一个服务对象。正如我们在第14.2节中学到的，服务是Kubernetes对象，允许我们将Pod作为网络服务暴露。如图14.8所示，服务对象将我们的应用程序作为Web服务暴露，并将流量从集群重定向到指定端口上的我们的Pod。创建一个名为orders-service.yaml的文件，并将列表14.2的内容复制进去，该列表显示了如何配置简单的服务清单。
- en: '![](../Images/14-08.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-08.png)'
- en: Figure 14.8 A service object redirects traffic from the cluster to the pods
    on the specified ports. In this example, incoming traffic to the cluster on port
    80 is redirected to port 8000 in the pods.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8 一个服务对象将集群中的流量重定向到指定端口上的Pod。在这个例子中，端口80的集群入站流量被重定向到Pod的端口8000。
- en: 'We use version v1 of the Kubernetes API to declare our service. In metadata,
    we specify that the service’s name is `orders-service` and that it’s to be launched
    within the `orders-service` namespace. We also add a label: `app:` `orders-service`.
    In the service’s `spec` section, we configure a `ClusterIP` type, which means
    the pod will only be accessible from within the cluster. There are other types
    of services in Kubernetes, such as `NodePort` and `LoadBalancer`. (To learn more
    about the types of services and when to use each type, see the sidebar, “Which
    type of Kubernetes service should I use?”)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Kubernetes API的v1版本来声明我们的服务。在元数据中，我们指定服务的名称为`orders-service`，并且要在`orders-service`命名空间内启动。我们还添加了一个标签：`app:`
    `orders-service`。在服务的`spec`部分，我们配置了`ClusterIP`类型，这意味着pod只能在集群内部访问。Kubernetes中还有其他类型的服务，例如`NodePort`和`LoadBalancer`。（要了解更多关于服务类型及其使用情况，请参阅侧边栏，“我应该使用哪种类型的Kubernetes服务？”）
- en: We also create a forwarding rule to redirect traffic from port 80 to port 8000,
    which is the port on which our containers run. Finally, we specify a selector
    for the `app:` `orders-service` label, which means this service will only operate
    pods with that label.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个转发规则，将来自80端口的流量重定向到8000端口，这是我们容器运行的端口。最后，我们指定了一个`app:` `orders-service`标签的选择器，这意味着此服务将仅操作带有该标签的pods。
- en: Listing 14.2 Declaring a service manifest
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.2 声明服务清单
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① This manifest defines a Service object.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ① 此清单定义了一个Service对象。
- en: ② This is a ClusterIP type of Service.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ② 这是一个ClusterIP类型的Service。
- en: ③ The service communicates over HTTP.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 服务通过HTTP进行通信。
- en: ④ The service must be mapped to port 80.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 服务必须映射到端口80。
- en: ⑤ The service runs internally on port 8000.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 服务在内部运行在端口8000上。
- en: 'To deploy this service, run the following command:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署此服务，请运行以下命令：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Which type of Kubernetes service should I use?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该使用哪种类型的Kubernetes服务？
- en: 'Kubernetes has four types of services. Here we discuss the features of each
    service type and their use cases:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes有四种类型的服务。在这里，我们讨论每种服务类型的特性和用例：
- en: '`ClusterIP`—Exposes services on the cluster’s internal IP and therefore makes
    them accessible only within the cluster'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClusterIP`—在集群的内部IP上暴露服务，因此只能在集群内部访问'
- en: '`NodePort`—Exposes services on the node’s external IP and therefore makes them
    available on the cluster’s network'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NodePort`—在节点的外部IP上暴露服务，因此使它们在集群网络上可用'
- en: '`LoadBalancer`—Exposes the service directly through a dedicated cloud load
    balancer'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LoadBalancer`—通过专用云负载均衡器直接暴露服务'
- en: '`ExternalName`—Exposes the service through an internal DNS record within the
    cluster'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExternalName`—通过集群内部的内部DNS记录暴露服务'
- en: Which of these types should you use? It depends on your needs. `NodePort` is
    useful if you want to be able to access your services externally on the IP of
    the node in which they’re running. The downside is the service uses the static
    port of the node, so you can only run one service per node. `ClusterIP` is useful
    if you’d rather access the service on the cluster’s IP. `ClusterIP` services are
    not directly reachable from outside the cluster, but you can expose them by creating
    ingress rules that forward traffic to them. `LoadBalancer` is useful if you’d
    like to use one cloud load balancer per service. Using a load balancer per service
    makes configuration somewhat simpler, as you won’t have to configure multiple
    ingress rules. However, load balancers are usually the most expensive components
    of your cluster, so if budget is a factor, you may not want to use this option.
    Finally, `ExternalName` is useful if you want to be able to access your services
    from within the cluster using custom domains.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 应该使用哪种类型？这取决于您的需求。`NodePort`如果您想能够从运行它们的节点的IP地址外部访问服务，那么它很有用。缺点是服务使用节点的静态端口，因此每个节点只能运行一个服务。`ClusterIP`如果您更愿意通过集群的IP访问服务，那么它很有用。`ClusterIP`服务不能从集群外部直接访问，但您可以通过创建将流量转发到它们的ingress规则来暴露它们。`LoadBalancer`如果您想为每个服务使用一个云负载均衡器，那么它很有用。使用每个服务的负载均衡器可以使配置稍微简单一些，因为您不需要配置多个ingress规则。然而，负载均衡器通常是集群中最昂贵的组件，所以如果预算是一个因素，您可能不想使用这个选项。最后，`ExternalName`如果您想能够使用自定义域名从集群内部访问服务，那么它很有用。
- en: 14.6.3 Exposing services with ingress objects
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.6.3 使用ingress对象暴露服务
- en: The final step is to expose the service through the internet. To expose the
    service, we need to create an ingress resource that routes traffic to the service.
    As you can see in figure 14.9, an ingress resource is a service that redirects
    HTTP traffic to the pods running in our Kubernetes cluster on the specified ports
    and URL paths. Create a file named orders-service-ingress.yaml and copy the content
    of listing 14.3 to it.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是通过互联网公开服务。为了公开服务，我们需要创建一个路由到服务的入口资源。如图 14.9 所示，入口资源是一个服务，它将 HTTP 流量重定向到我们在指定端口和
    URL 路径上运行的 Kubernetes 集群中的 pod。创建一个名为 orders-service-ingress.yaml 的文件，并将列表 14.3
    的内容复制到其中。
- en: '![](../Images/14-09.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14-09.png)'
- en: Figure 14.9 An ingress object allows us to redirect HTTP traffic on a specific
    port and URL path to a service object.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9 入口对象允许我们将特定端口和 URL 路径上的 HTTP 流量重定向到服务对象。
- en: In the ingress manifest, we use version networking.k8s.io/v1 of the Kubernetes
    API, and we declare the object as an `Ingress` type. In `metadata`, we name the
    ingress object `orders-service-ingress`, and we specify that it should be deployed
    within the `orders-service` namespace. We use annotations to bind the ingress
    object to the AWS Load Balancer we deployed in section 14.5\. Within the `spec`
    section, we define the forwarding rules of the ingress resource. We declare an
    HTTP rule that forwards all traffic under the `/orders` path to the orders service
    and additional rules to access the service’s API documentation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在入口清单中，我们使用 Kubernetes API 的 networking.k8s.io/v1 版本，并声明对象为 `Ingress` 类型。在 `metadata`
    中，我们命名入口对象为 `orders-service-ingress`，并指定它应在 `orders-service` 命名空间内部署。我们使用注解将入口对象绑定到我们在
    14.5 节中部署的 AWS 负载均衡器。在 `spec` 部分中，我们定义入口资源的转发规则。我们声明了一个 HTTP 规则，将所有 `/orders`
    路径下的流量转发到订单服务，以及访问服务 API 文档的附加规则。
- en: Listing 14.3 Declaring an ingress manifest
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14.3 声明入口清单
- en: '[PRE26]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① The manifest defines an Ingress object.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ① 清单定义了一个入口对象。
- en: ② AWS configuration for the Ingress
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ② 入口的 AWS 配置
- en: ③ The Ingress exposes an Application Load Balancer.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 入口暴露了一个应用程序负载均衡器。
- en: ④ Traffic is routed to the pods based on IP.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 根据 IP 地址将流量路由到 pod。
- en: ⑤ The Ingress is available to external connections.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 入口可用于外部连接。
- en: ⑥ Traffic forwarding rules
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 流量转发规则
- en: ⑦ A rule for the /orders URL path
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 针对 /orders URL 路径的规则
- en: ⑧ The rule applies to requests starting with the /orders prefix.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 该规则适用于以 /orders 前缀开始的请求。
- en: ⑨ The backend service that handles this traffic
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 处理此流量的后端服务
- en: ⑩ Traffic must be routed to the orders-service Service.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 流量必须路由到 orders-service 服务。
- en: ⑪ The orders-service Service is available on port 80.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ orders-service 服务在端口 80 上可用。
- en: 'To create this ingress resource, we run the following command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建此入口资源，我们运行以下命令：
- en: '[PRE27]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The orders API is now accessible. To call the API, we first need to find out
    the endpoint for the ingress rule we just created. Run the following command to
    get the details of the ingress resource:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 订单 API 现在可访问。要调用 API，我们首先需要找到我们刚刚创建的入口规则的端点。运行以下命令以获取入口资源的详细信息：
- en: '[PRE28]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The value under the `ADDRESS` field is the URL of the load balancer. You can
    also get hold of this value by running the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`ADDRESS` 字段下的值是负载均衡器的 URL。您也可以通过运行以下命令来获取此值：'
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can use this URL to call the orders service API. Since the database isn’t
    yet ready, the API itself won’t work, but we can access the API documentation:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此 URL 调用订单服务 API。由于数据库尚未就绪，API 本身将无法工作，但我们可以访问 API 文档：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: It may take some time for the load balancer to become available, and in the
    meantime `curl` won’t be able to resolve the host. If that happens, wait a few
    minutes and try again. To be able to interact with the API, we must set up a database,
    which will be the goal of our next section!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器变为可用可能需要一些时间，在此期间 `curl` 将无法解析主机。如果发生这种情况，请等待几分钟再试。为了能够与 API 交互，我们必须设置一个数据库，这将是我们下一节的目标！
- en: 14.7 Setting up a serverless database with AWS Aurora
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.7 使用 AWS Aurora 设置无服务器数据库
- en: 'The orders service is almost ready: the application is up and running, and
    we can access it through the internet. Only one component is missing: the database.
    We have multiple choices for setting up the database. We can set up the database
    as a deployment within our Kubernetes cluster with a mounted volume, or we can
    choose one of the many managed database services that cloud providers offer.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 订单服务几乎准备好了：应用程序正在运行，我们可以通过互联网访问它。唯一缺少的部分是数据库。我们有很多选择来设置数据库。我们可以在我们的Kubernetes集群中设置数据库作为部署，使用挂载的卷，或者我们可以选择云提供商提供的众多托管数据库服务之一。
- en: To keep it simple and cost-effective, in this section, we’ll set up an Aurora
    Serverless database in AWS—a powerful database engine that is cost-effective since
    you only pay for what you use and is very convenient since you don’t have to worry
    about managing or scaling the database.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单且成本效益，在本节中，我们将在AWS中设置一个Aurora Serverless数据库——这是一个强大的数据库引擎，由于你只需为使用的部分付费，因此具有成本效益，并且非常方便，因为你无需担心管理或扩展数据库。
- en: 14.7.1 Creating an Aurora Serverless database
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7.1 创建Aurora Serverless数据库
- en: 'We’ll launch our Aurora database within the Kubernetes cluster’s VPC. To be
    able to launch a database within an existing VPC, we need to create a *database
    subnet group*: a collection of subnets within the VPC. As we learned in section
    14.3, eksctl divides the Kubernetes cluster’s VPC into six subnets: three public
    and three private. The six subnets are distributed across three *availability
    zones* (data centers within an AWS region), with one public and one private subnet
    per availability zone.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Kubernetes集群的VPC内启动我们的Aurora数据库。为了能够在现有的VPC内启动数据库，我们需要创建一个*数据库子网组*：VPC内的一组子网。正如我们在第14.3节中学到的，eksctl将Kubernetes集群的VPC划分为六个子网：三个公共和三个私有。这六个子网分布在三个*可用区*（AWS区域内的数据中心）中，每个可用区有一个公共子网和一个私有子网。
- en: 'When choosing the subnets for our database subnet group, we need to consider
    the following constraints:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择数据库子网组的子网时，我们需要考虑以下约束条件：
- en: Aurora Serverless only supports one subnet per availability zone.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aurora Serverless在每个可用区只支持一个子网。
- en: When creating a database subnet group, the subnets must all be either private
    or public.[⁵](#pgfId-1118556)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建数据库子网组时，子网必须全部为私有或公共。[⁵](#pgfId-1118556)
- en: 'For security, it’s best practice to use private subnets in database subnet
    groups as it ensures that the database server is not accessible from outside of
    the VPC, which means external and unauthorized users are unable to connect to
    it directly. To find the list of private subnets in the VPC, we first need to
    obtain the ID of the Kubernetes cluster’s VPC with the following command:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，在数据库子网组中使用私有子网是最佳实践，因为它确保数据库服务器无法从VPC外部访问，这意味着外部和未经授权的用户无法直接连接到它。为了找到VPC中私有子网的列表，我们首先需要使用以下命令获取Kubernetes集群VPC的ID：
- en: '[PRE31]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then use the following command to get the IDs of the private subnets in the
    VPC:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下命令获取VPC中私有子网的ID：
- en: '[PRE32]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The previous command lists all the subnets in the Kubernetes cluster’s VPC,
    and it uses `jq` to filter the public subnets. Armed with all this information,
    we can now create the database subnet group using the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令列出了Kubernetes集群VPC中的所有子网，并使用`jq`过滤出公共子网。有了所有这些信息，我们现在可以使用以下命令创建数据库子网组：
- en: '[PRE33]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see in figure 14.10, this command creates a database subnet group
    named `coffeemesh-db-subnet-group`. When running the command, make sure you replace
    the `<subnet_id>` placeholders with the IDs of your private subnets. We’ll deploy
    our Aurora database within this database subnet group.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如图14.10所示，此命令创建了一个名为`coffeemesh-db-subnet-group`的数据库子网组。在运行命令时，请确保将`<subnet_id>`占位符替换为你的私有子网的ID。我们将在该数据库子网组内部署我们的Aurora数据库。
- en: '![](../Images/14-10.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图14-10](../Images/14-10.png)'
- en: Figure 14.10 We deploy an Aurora database within a database subnet group named
    `coffeemesh-db-subnet-group`. The database subnet group is created on top of the
    three private subnets of our VPC to prevent unauthorized access.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10 我们在名为`coffeemesh-db-subnet-group`的数据库子网组中部署了一个Aurora数据库。该数据库子网组是在我们的VPC的三个私有子网之上创建的，以防止未经授权的访问。
- en: 'Next, we need to create a *VPC security group*—a set of rules that define what
    incoming and outgoing traffic is allowed from the VPC—that allows traffic to the
    database so that our applications can connect to it. The following command creates
    a security group called `db-access`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个*VPC安全组*——一组规则，定义了从VPC进入和出去的允许流量，以便允许流量访问数据库，使我们的应用程序能够连接到它。以下命令创建了一个名为`db-access`的安全组：
- en: '[PRE34]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the previous command, replace `<vpc-id>` with the ID of your Kubernetes
    cluster’s VPC. The output from the previous command is the ID of the security
    group we just created. We’ll allow traffic from all IP addresses on PostgreSQL’s
    default port, which is 5432\. Since we’re going to deploy the database into private
    subnets, it’s okay to listen on all IPs, but for additional security, you may
    want to restrict the range of addresses to those of your pods. We use the following
    command to create an inbound traffic rule for our database access security group:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的命令中，将`<vpc-id>`替换为你的Kubernetes集群VPC的ID。之前命令的输出是我们刚刚创建的安全组的ID。我们将允许所有IP地址在PostgreSQL的默认端口（5432）上的流量。由于我们将数据库部署到私有子网中，监听所有IP是可行的，但为了额外的安全性，你可能想限制地址范围到你的Pods的地址。我们使用以下命令为我们的数据库访问安全组创建一个入站流量规则：
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In this command, replace `<db-security-group-id>` with the ID of your database
    access security group.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在此命令中，将`<db-security-group-id>`替换为你的数据库访问安全组的ID。
- en: 'Now that we have a database subnet group and a security group that allows our
    pods to connect to it, we can use the subnet group to launch an Aurora Serverless
    cluster within our VPC! Run the following command to launch an Aurora Serverless
    cluster:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个数据库子网组和允许我们的Pods连接到它的安全组，我们可以使用子网组在我们的VPC内启动一个Aurora无服务器集群！运行以下命令来启动Aurora无服务器集群：
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let’s take a close look at the command’s parameters:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看命令的参数：
- en: '`--db-cluster-identifier`—The name of the database cluster. We’re naming the
    cluster `coffeemesh-orders-db`.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--db-cluster-identifier`—数据库集群的名称。我们将集群命名为`coffeemesh-orders-db`。'
- en: '`--engine`—The database engine you want to use. We’re using a PostgreSQL-compatible
    engine, but you can also choose a MySQL-compatible engine if you prefer.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--engine`—你想要使用的数据库引擎。我们使用兼容PostgreSQL的引擎，但如果你更喜欢，也可以选择兼容MySQL的引擎。'
- en: '`--engine-version`—The version of the Aurora engine you want to use. We’re
    choosing version 10.14, which is the only version available for Aurora PostgreSQL
    serverless right now. See the AWS documentation to keep up to date with new versions
    ([http://mng.bz/gRyn](http://mng.bz/gRyn)).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--engine-version`—你想要使用的Aurora引擎版本。我们选择版本10.14，这是目前Aurora PostgreSQL无服务器可用的唯一版本。参见AWS文档以了解新版本信息([http://mng.bz/gRyn](http://mng.bz/gRyn))。'
- en: '`--engine-mode`—The database engine mode. We’re choosing serverless to keep
    the example simple and cost-effective.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--engine-mode`—数据库引擎模式。我们选择无服务器模式以保持示例简单且成本效益高。'
- en: '`--scaling-configuration`—The autoscaling configuration for the Aurora cluster.
    We configure the cluster with minimum Aurora capacity units (ACU) of 8 and a maximum
    of 64\. Each ACU provides approximately 2 GB of memory. We also configure the
    cluster to scale down to 0 ACUs automatically after 1,000 seconds without activity.[⁶](#pgfId-1118611)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--scaling-configuration`—Aurora集群的自动扩展配置。我们配置集群以最小Aurora容量单元（ACU）为8，最大为64。每个ACU提供大约2
    GB的内存。我们还配置集群在1,000秒无活动后自动缩小到0 ACU。[⁶](#pgfId-1118611)'
- en: '`--master-username`—The username of the database master user.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--master-username`—数据库主用户的用户名。'
- en: '`--master-user-password`—The password of the database master user.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--master-user-password`—数据库主用户的密码。'
- en: '`--vpc-security-group-ids`—The ID of the database access security group we
    created in the previous step.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--vpc-security-group-ids`—我们在上一步中创建的数据库访问安全组的ID。'
- en: '`--db-subnet-group`—The name of the database security group we created earlier.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--db-subnet-group`—我们之前创建的数据库安全组的名称。'
- en: After running this command, you’ll get a large JSON payload with details about
    the database. To connect to the database, we need the value of the `DBCluster.Endpoint`
    property of the payload, which represents the database’s hostname. We’ll use this
    value in the next sections to connect to the database.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，你将获得一个包含数据库详细信息的JSON有效负载。要连接到数据库，我们需要有效负载中`DBCluster.Endpoint`属性的值，它表示数据库的主机名。我们将在下一节中使用此值来连接到数据库。
- en: 14.7.2 Managing secrets in Kubernetes
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7.2 Kubernetes中的秘密管理
- en: To connect our services to the database, we need a secure way to pass the connection
    credentials. The native way to manage sensitive information in Kubernetes is using
    Kubernetes secrets. This way, we avoid having to expose sensitive information
    through the code or through our image builds. In this section, you’ll learn how
    to manage Kubernetes secrets securely.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的服务连接到数据库，我们需要一种安全的方式来传递连接凭据。在 Kubernetes 中管理敏感信息的原生方式是使用 Kubernetes 机密。这样，我们就避免了通过代码或通过我们的镜像构建暴露敏感信息。在本节中，你将学习如何安全地管理
    Kubernetes 机密。
- en: 'AWS EKS offers two secure ways to manage Kubernetes secrets: we can use the
    AWS Secrets & Configuration Provider for Kubernetes,[⁷](#pgfId-1118635) or we
    can use AWS Key Management Service (KMS) to secure our secrets with envelope encryption.
    In this section, we’ll use envelope encryption to protect our secrets.[⁸](#pgfId-1118640)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EKS 提供两种安全的方式来管理 Kubernetes 机密：我们可以使用 AWS Secrets & Configuration Provider
    for Kubernetes，[⁷](#pgfId-1118635)，或者我们可以使用 AWS 密钥管理服务（KMS）通过封装加密来保护我们的机密。在本节中，我们将使用封装加密来保护我们的机密。[⁸](#pgfId-1118640)
- en: As you can see in figure 14.11, envelope encryption is the practice of encrypting
    your data with a data encryption key (DEK) and encrypting the DEK with a key encryption
    key (KEK).[⁹](#pgfId-1118647) It sounds complicated, but it’s simple to use since
    AWS does the heavy lifting for us.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图 14.11 中看到的，封装加密是指使用数据加密密钥（DEK）加密你的数据，然后使用密钥加密密钥（KEK）加密 DEK 的做法。[⁹](#pgfId-1118647)
    这听起来很复杂，但由于 AWS 为我们承担了繁重的工作，所以使用起来很简单。
- en: '![](../Images/14-11.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.11](../Images/14-11.png)'
- en: Figure 14.11 Envelope encryption is the practice of encrypting data with a data
    encryption key (DEK) and encrypting the DEK with a key encryption key.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.11 封装加密是指使用数据加密密钥（DEK）加密数据，然后使用密钥加密密钥（KEK）加密 DEK 的做法。
- en: 'To use envelope encryption, first we need to generate an AWS KMS key. You can
    use the following command to create the key:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用封装加密，首先我们需要生成一个 AWS KMS 密钥。你可以使用以下命令来创建密钥：
- en: '[PRE37]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The output of this command is a payload with metadata about the newly created
    key. From this payload, we want to use the `KeyMetadata.Arn` property, which represents
    the key’s ARN, or Amazon Resource Name. The next step is to enable secrets encryption
    in our Kubernetes cluster using eksctl`:`
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令的输出是一个包含新创建密钥元数据的有效负载。从这个有效负载中，我们想要使用 `KeyMetadata.Arn` 属性，它表示密钥的 ARN，或 Amazon
    资源名称。下一步是在我们的 Kubernetes 集群中使用 eksctl 启用机密加密：`
- en: '[PRE38]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Make sure you replace `<key_arn>` with the ARN of your KMS key and `<aws_region>`
    with the region where you deployed the Kubernetes cluster. The operation triggered
    by the previous command can take up to 45 minutes to complete. The command runs
    until the cluster is created, so just wait until it finishes. Once it’s done,
    we can create Kubernetes secrets. Let’s create a secret that represents the database
    connection string. A database connection string has the following structure:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将 `<key_arn>` 替换为你的 KMS 密钥的 ARN，并将 `<aws_region>` 替换为你部署 Kubernetes 集群的区域。由前面的命令触发的操作可能需要最多
    45 分钟才能完成。该命令会一直运行，直到集群创建完成，所以只需等待它完成。一旦完成，我们就可以创建 Kubernetes 机密。让我们创建一个表示数据库连接字符串的机密。数据库连接字符串具有以下结构：
- en: '[PRE39]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let’s look at each component of the connection string:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看连接字符串的每个组成部分：
- en: '`engine`—The database engine, for example, `postgresql`.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`engine`—数据库引擎，例如，`postgresql`。'
- en: '`username`—The username we chose earlier when creating the database.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`username`—我们在创建数据库时选择的用户名。'
- en: '`password`—The password we chose earlier when creating the database.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`password`—我们在创建数据库时选择的密码。'
- en: '`hostname`—The database’s hostname, which we obtained in the previous section
    from the `DBCluster.Endpoint` property of the payload returned by the `aws` `rds`
    `create-db-cluster` command.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hostname`—数据库的主机名，这是我们上一节中从 `aws rds create-db-cluster` 命令返回的有效负载的 `DBCluster.Endpoint`
    属性中获得的。'
- en: '`port`—The port on which the database is running. Each database has its own
    default port, such as 5432 for PostgreSQL and 3306 for MySQL.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`port`—数据库运行在上的端口。每个数据库都有自己的默认端口，例如 PostgreSQL 的 5432 和 MySQL 的 3306。'
- en: '`database_name`—The name of the database we’re connecting to. In PostgreSQL,
    the default database is called `postgres`.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`database_name`—我们要连接到的数据库的名称。在 PostgreSQL 中，默认数据库的名称为 `postgres`。'
- en: 'For example, for a PostgreSQL database, a typical connection string looks like
    this:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于 PostgreSQL 数据库，一个典型的连接字符串看起来像这样：
- en: '[PRE40]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To store the database connection string as a Kubernetes secret, we run the
    following command:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据库连接字符串作为 Kubernetes 机密存储，我们运行以下命令：
- en: '[PRE41]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The previous command creates a secret object named `db-credentials` within
    the `orders-service` namespace. To get the details of this secret object, you
    can run the following command:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令在 `orders-service` 命名空间内创建了一个名为 `db-credentials` 的密钥对象。要获取此密钥对象的详细信息，您可以运行以下命令：
- en: '[PRE42]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The secrets are listed under the `data` property of the payload, and they’re
    Base64 encoded. To obtain their values, you can run the following command:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 密钥列在有效载荷的 `data` 属性下，并且它们是Base64编码的。要获取它们的值，您可以运行以下命令：
- en: '[PRE43]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: where `<DB_URL>` is the Base64 encoded value of the `DB_URL` key.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `<DB_URL>` 是 `DB_URL` 键的Base64编码值。
- en: To make the secret available to the orders service, we need to update the order
    service deployment to consume the secret and expose it as an environment variable.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使密钥对订单服务可用，我们需要更新订单服务部署以消费密钥并将其作为环境变量公开。
- en: Listing 14.4 Consuming secrets as environment variables in a deployment
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.4 在部署中作为环境变量消费密钥
- en: '[PRE44]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ① Environment configuration for the pods
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ① Pod的环境配置
- en: ② Configuration for identifying the secret
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ② 配置用于识别密钥
- en: ③ Environment must be loaded from the secret named db-credentials.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 环境必须从名为db-credentials的密钥中加载。
- en: 'Let’s apply the changes by running the following command:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下命令来应用这些更改：
- en: '[PRE45]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Our service can now connect to the database! We’re almost done. The final step
    is to apply the database migrations, which we’ll accomplish in the next section.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务现在可以连接到数据库了！我们几乎完成了。最后一步是应用数据库迁移，我们将在下一节中完成。
- en: 14.7.3 Running the database migrations and connecting our service to the database
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7.3 运行数据库迁移并将我们的服务连接到数据库
- en: Our database is up and running, and now we can connect the orders service with
    it. However, before we can create records and run queries, we must ensure the
    database has the expected schemas. As we saw in chapter 7, the process of creating
    the database schemas is called migration. Our application’s migrations are available
    under the migrations folder. In this section, we’ll run the migrations against
    the Aurora Serverless database.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据库正在运行，现在我们可以将其与订单服务连接起来。然而，在我们能够创建记录和运行查询之前，我们必须确保数据库具有预期的模式。正如我们在第7章中看到的，创建数据库模式的过程被称为迁移。我们的应用程序的迁移可以在migrations文件夹下找到。在本节中，我们将对Aurora
    Serverless数据库运行迁移。
- en: 'In the previous section, we deployed the Aurora database to our private subnets,
    which means we can’t access our database directly to run the migrations. We have
    two main options to connect to the database: connect through a bastion server
    or create a Kubernetes Job that applies the migrations. Since we’re working with
    Kubernetes and our cluster is already up and running, using a Kubernetes Job is
    a suitable option for us.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将Aurora数据库部署到了我们的私有子网中，这意味着我们无法直接访问数据库来运行迁移。我们有两个主要选项来连接到数据库：通过堡垒机服务器连接或创建一个应用迁移的Kubernetes
    Job。由于我们正在使用Kubernetes，并且我们的集群已经启动并运行，因此使用Kubernetes Job对我们来说是一个合适的选项。
- en: Definition A *bastion server* is a server that allows you to establish a secure
    connection with a private network. By connecting to the bastion server, you are
    able to access other servers within the private network.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 定义A *堡垒机服务器* 是一个允许您与私有网络建立安全连接的服务器。通过连接到堡垒机服务器，您能够访问私有网络中的其他服务器。
- en: To create the Kubernetes job, we first need to create a Docker image for running
    the database migrations. Create a file named migrations.dockerfile, and copy the
    contents of listing 14.5 into it. This Dockerfile installs both the production
    and the development dependencies and copies over the migrations and the Alembic
    configuration into the container. As we saw in chapter 7, we use Alembic to manage
    our database migrations. The command for this container is a one-off `alembic`
    `upgrade`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建Kubernetes作业，我们首先需要为运行数据库迁移创建一个Docker镜像。创建一个名为migrations.dockerfile的文件，并将列表14.5的内容复制到其中。此Dockerfile安装了生产环境和开发依赖项，并将迁移和Alembic配置复制到容器中。正如我们在第7章中看到的，我们使用Alembic来管理我们的数据库迁移。此容器的命令是单次执行的
    `alembic upgrade`。
- en: Listing 14.5 Dockerfile for the database migrations job
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.5 数据库迁移作业的Dockerfile
- en: '[PRE46]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ① We set the PYTHONPATH environment variable.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们设置了PYTHONPATH环境变量。
- en: 'To build the Docker image, run the following command:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 构建Docker镜像，请运行以下命令：
- en: '[PRE47]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We’re naming the image `coffeemesh-orders-migrations` and tagging it with version
    1.0\. Make sure you replace `<aws_account_id>` with your AWS account ID and `<aws_region>`
    with the region where you want to store your Docker builds. Before we push the
    image to the container registry, we need to create a repository:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将镜像命名为`coffeemesh-orders-migrations`并标记为版本1.0。确保你用你的AWS账户ID替换`<aws_account_id>`，并用你想要存储Docker构建的区域的名称替换`<aws_region>`。在我们将镜像推送到容器注册库之前，我们需要创建一个仓库：
- en: '[PRE48]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now let’s push the image to the container registry:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将镜像推送到容器注册库：
- en: '[PRE49]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'If your ECR credentials have expired, you can refresh them by running the following
    command again:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的ECR凭证已过期，你可以通过再次运行以下命令来刷新它们：
- en: '[PRE50]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Now that our image is ready, we need to create a Kubernetes Job object. We use
    a manifest file to create the Job. Create a file named orders-migrations-job.yaml
    and copy the contents of listing 14.6 into it. Listing 14.6 defines a Kubernetes
    object of type `Job` using the batch/v1 API. Just as we did in the previous section
    for the orders service, we expose the database connection string in the environment
    by loading the `db-credentials` secret using the `envFrom` property of the container’s
    definition. We also set the `ttlSecondsAfterFinished` parameter to 30 seconds,
    which controls how long the pod will last in the `orders-service` namespace once
    it’s finished the job.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了镜像，我们需要创建一个Kubernetes Job对象。我们使用清单文件来创建Job。创建一个名为orders-migrations-job.yaml的文件，并将列表14.6的内容复制进去。列表14.6使用batch/v1
    API定义了一个类型为`Job`的Kubernetes对象。就像我们在上一节为订单服务所做的那样，我们通过在容器的定义中使用`envFrom`属性加载`db-credentials`机密来在环境中公开数据库连接字符串。我们还设置了`ttlSecondsAfterFinished`参数为30秒，这控制了Pod在完成作业后将在`orders-service`命名空间中持续多长时间。
- en: Listing 14.6 Creating a database migrations job
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.6 创建数据库迁移Job
- en: '[PRE51]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ① The pod must be deleted 30 seconds after completing.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ① Pod必须在完成30秒后删除。
- en: 'Let’s create the Job by running the following command:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下命令来创建Job：
- en: '[PRE52]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'It’ll take a few seconds until the `job`’s pod is up and running. You can check
    its status by running the following command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 直到`job`的Pod启动并运行，可能需要几秒钟。你可以通过运行以下命令来检查其状态：
- en: '[PRE53]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Once the pod’s status is `Running` or `Completed`, you can check the `job`’s
    logs by running the following command:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Pod的状态为`Running`或`Completed`，你可以通过运行以下命令来检查`job`的日志：
- en: '[PRE54]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Watching the pod’s logs in this way is useful to check how the process is going
    and to spot any issues raised in its execution. Since the migration job is ephemeral
    and will be deleted after completion, make sure you check the logs while the process
    is running. Once the migrations job has completed, the database is finally ready
    to be used! We can finally interact with the orders service—the moment we’ve been
    waiting for! Our service is now ready for use. The next section explains one more
    change we need to make to finalize the deployment.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式查看Pod的日志对于检查进程的进展和发现执行中出现的任何问题很有用。由于迁移作业是短暂的，完成后会删除，确保你在进程运行时检查日志。一旦迁移作业完成，数据库最终就绪，可以使用了！我们终于可以与订单服务交互了——我们一直等待的时刻！我们的服务现在已准备好使用。下一节将解释我们需要进行的另一个更改以完成部署。
- en: 14.8 Updating the OpenAPI specification with the ALB’s hostname
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.8 使用ALB的主机名更新OpenAPI规范
- en: Now that our service is ready and the database is deployed and configured, it’s
    time to play around with the application! In chapters 2 and 6, we learned to interact
    with our APIs using a Swagger UI. To use the Swagger UI in our deployment, we
    need to update the API specification with the hostname of our Kubernetes cluster’s
    ALB. In this section, we update the order’s API specification, make a new deployment,
    and test it.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的服务已经就绪，数据库已部署并配置，是时候玩转应用程序了！在第2章和第6章，我们学习了如何使用Swagger UI与我们的API交互。为了在我们的部署中使用Swagger
    UI，我们需要更新API规范，以包含我们Kubernetes集群ALB的主机名。在本节中，我们更新了订单的API规范，创建了一个新的部署，并对其进行了测试。
- en: Listing 14.7 Adding the ALB’s hostname as a server
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.7 将ALB的主机名添加为服务器
- en: '[PRE55]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In listing 14.8, replace `<alb-hostname>` with the hostname of your own ALB.
    As we learned in section 14.6, you obtain the ALB’s hostname by running the following
    command:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表14.8中，将`<alb-hostname>`替换为你自己的ALB的主机名。正如我们在14.6节中学到的，你可以通过运行以下命令来获取ALB的主机名：
- en: '[PRE56]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now we need to rebuild our Docker image:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要重新构建我们的Docker镜像：
- en: '[PRE57]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then, we publish the new build to AWS ECR:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将新的构建发布到AWS ECR：
- en: '[PRE58]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Next, we need to update the orders service deployment manifest.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要更新订单服务部署清单。
- en: Listing 14.8 Declaring a deployment manifest
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.8 声明部署清单
- en: '[PRE59]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Finally, we apply the new deployment configuration by running the following
    command:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过运行以下命令应用新的部署配置：
- en: '[PRE60]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Monitor the rollout by running the following command:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令来监控部署：
- en: '[PRE61]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Once the old pod is terminated and the new one is up and running, load the
    order’s service Swagger UI by pasting the ALB’s hostname in a browser and visiting
    the `/docs/orders` page. You can play around with the API using the same approach
    you learned in chapters 2 and 6: creating orders, modifying them, and fetching
    their details from the server.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦旧节点终止，新节点启动并运行，通过在浏览器中粘贴 ALB 的主机名并访问 `/docs/orders` 页面来加载订单服务的 Swagger UI。你可以使用在
    2 章和 6 章中学到的方法来玩转 API：创建订单、修改它们，并从服务器获取它们的详细信息。
- en: And the journey is finally complete! If you’ve been able to follow up to this
    point and managed to get your Kubernetes cluster up and running, please accept
    my most sincere congratulations! You’ve made it! Figure 14.12 shows a high-level
    overview of the architecture you’ve deployed in this chapter.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 旅程终于完成了！如果你能够跟到这里，并且成功地将你的 Kubernetes 集群启动并运行，请接受我最真诚的祝贺！你已经成功了！图 14.12 展示了你本章部署的架构的高级概述。
- en: '![](../Images/14-12.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-12.png)'
- en: Figure 14.12 High-level overview of the architecture deployed in this chapter
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12 本章部署的架构高级概述
- en: The overview of Kubernetes in this chapter is a brief one, but it’s enough to
    get an understanding of how Kubernetes works, and it’s sufficient to get a cluster
    up and running in your production environment. If you work or intend to work with
    Kubernetes, I strongly encourage you to continue reading about this technology.
    You can check all the references I’ve cited in this chapter, to which I’d like
    to add Marko Lukša’s fundamental *Kubernetes in Action* (2nd ed., Manning, expected
    2023).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 本章对 Kubernetes 的概述是简要的，但足以了解 Kubernetes 的工作原理，并且足以在你的生产环境中启动并运行一个集群。如果你正在使用或打算使用
    Kubernetes，我强烈建议你继续阅读有关这项技术的信息。你可以查看我在本章中引用的所有参考文献，我想要补充的是 Marko Lukša 的基础书籍 *Kubernetes
    in Action*（第 2 版，Manning，预计 2023 年出版）。
- en: In the next section, we’ll delete all the resources we created during this chapter.
    Don’t miss it if you don’t want to be charged more than needed!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将删除本章中创建的所有资源。如果你不想支付不必要的费用，请不要错过！
- en: 14.9 Deleting the Kubernetes cluster
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.9 删除 Kubernetes 集群
- en: This section explains how to delete all the resources we created in this chapter.
    This step is crucial to make sure you don’t get billed for the Kubernetes cluster
    once you’ve finished working through the examples. As you can see in figure 14.13,
    we have dependency relationships among some of our resources. To successfully
    delete all resources, we must delete them in reverse order of their dependencies.
    For example, the database cluster depends on the database subnet group, which
    depends on the VPC subnets, which depend on the VPC. In this case, we’ll start
    by deleting the database cluster, and in the last step we’ll delete the VPC.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何删除本章中创建的所有资源。这一步至关重要，以确保你在完成示例工作后不会为 Kubernetes 集群付费。如图 14.13 所示，我们的一些资源之间存在依赖关系。为了成功删除所有资源，我们必须按照它们的依赖关系反向删除。例如，数据库集群依赖于数据库子网组，数据库子网组依赖于
    VPC 子网，VPC 子网依赖于 VPC。在这种情况下，我们将首先删除数据库集群，最后一步将删除 VPC。
- en: '![](../Images/14-13.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/14-13.png)'
- en: Figure 14.13 The resources in our stack have relationships of dependency. The
    direction of dependency is indicated by the direction of the arrows. To delete
    the resources, we start by deleting those that have no arrows pointing to them.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13 我们堆栈中的资源之间存在依赖关系。依赖关系的方向由箭头的方向表示。为了删除资源，我们首先删除那些没有箭头指向的资源。
- en: 'Let’s delete the database cluster with the following command:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令删除数据库集群：
- en: '[PRE62]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The `--skip-final-snapshot` flag instructs the command not to create a snapshot
    of the database before deletion. It takes a few minutes for the database to be
    deleted. Once it’s deleted, we can delete the database subnet group with the following
    command:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`--skip-final-snapshot` 标志指示命令在删除前不要创建数据库快照。删除数据库需要几分钟时间。一旦删除完成，我们可以使用以下命令删除数据库子网组：'
- en: '[PRE63]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, let’s delete the AWS Load Balancer Controller. Deleting the AWS Load
    Balancer Controller is a two-step process: first we uninstall the controller using
    `helm`, and then we delete the ALB that was created when we installed the controller.
    To delete the ALB we need its URL, so let’s fetch that value first (make sure
    you run this step before uninstalling with `helm`):'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们删除AWS负载均衡器控制器。删除AWS负载均衡器控制器是一个两步过程：首先我们使用`helm`卸载控制器，然后我们删除在安装控制器时创建的ALB。要删除ALB，我们需要它的URL，所以让我们首先获取这个值（确保在用`helm`卸载之前运行此步骤）：
- en: '[PRE64]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now let’s uninstall the controller with the following command:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令卸载控制器：
- en: '[PRE65]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'After running this command, we need to delete the ALB. To delete the ALB, we
    need to find its ARN. We’ll use the AWS CLI to list the load balancers in our
    account and filter them out by their DNS name. The following command fetches the
    ARN of the load balancer whose DNS name matches the ALB’s URL, which we obtained
    earlier:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，我们需要删除ALB。要删除ALB，我们需要找到它的ARN。我们将使用AWS CLI列出我们账户中的负载均衡器，并通过它们的DNS名称过滤它们。以下命令获取与ALB的URL匹配的负载均衡器的ARN，这是我们之前获得的：
- en: '[PRE66]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Make sure you replace `<load_balancer_url>` with your load balancer’s URL,
    which we obtained in an earlier step. This command gives us the load balancer’s
    ARN, which we can use to delete it:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将`<load_balancer_url>`替换为你的负载均衡器的URL，这是我们之前步骤中获得的。此命令为我们提供了负载均衡器的ARN，我们可以用它来删除它：
- en: '[PRE67]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now we can delete the Kubernetes cluster with following command:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令删除Kubernetes集群：
- en: '[PRE68]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finally, let’s delete the KMS key we created earlier to encrypt our Kubernetes
    secrets. To delete the key, we run the following command:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们删除我们之前创建的用于加密Kubernetes机密的KMS密钥。要删除密钥，我们运行以下命令：
- en: '[PRE69]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: where `<key_id>` is the ID of the key we created earlier.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`<key_id>`是我们之前创建的密钥的ID。
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Kubernetes is a container orchestration tool that’s becoming a standard for
    deploying microservices at scale. Using Kubernetes helps us to move across cloud
    providers while keeping a consistent interface to our services.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes是一个容器编排工具，它正在成为大规模部署微服务的标准。使用Kubernetes可以帮助我们在云提供商之间迁移，同时保持对服务的一致接口。
- en: The three major managed Kubernetes services are Google’s Kubernetes Engine (GKE),
    Azure’s Kubernetes Service (AKS), and AWS’s Elastic Kubernetes Service (EKS).
    In this chapter, we learned to deploy a Kubernetes cluster with EKS, which is
    the most widely adopted Kubernetes managed service.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个主要的托管Kubernetes服务是Google的Kubernetes Engine (GKE)、Azure的Kubernetes Service
    (AKS)和AWS的Elastic Kubernetes Service (EKS)。在本章中，我们学习了如何使用EKS部署Kubernetes集群，EKS是最广泛采用的Kubernetes托管服务。
- en: We can deploy a Kubernetes cluster in AWS using the console, CloudFormation,
    or the eksctl command-line tool. In this chapter, we used the eksctl CLI since
    it’s the AWS recommended way to manage a Kubernetes cluster.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用控制台、CloudFormation或eksctl命令行工具在AWS中部署一个Kubernetes集群。在本章中，我们使用了eksctl CLI，因为它是AWS推荐的方式来管理Kubernetes集群。
- en: To make our Kubernetes cluster reachable from the internet, we use an ingress
    controller such as the AWS Load Balancer Controller.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使我们的Kubernetes集群可以从互联网访问，我们使用一个ingress控制器，例如AWS负载均衡器控制器。
- en: 'To deploy a microservice to a Kubernetes cluster, we create the following resources:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将微服务部署到Kubernetes集群，我们创建以下资源：
- en: A `Deployment`, which manages the desired state of the pods, processes that
    run the Docker build
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Deployment`，它管理pods的期望状态，运行Docker构建的过程
- en: A `Service` that allows us to expose our application as a web service
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个允许我们将应用程序作为Web服务公开的`Service`
- en: An `Ingress` object bound to the ingress controller (the AWS Load Balancer Controller)
    that forwards traffic to the service
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个绑定到ingress控制器（AWS负载均衡器控制器）的`Ingress`对象，它将流量转发到服务
- en: Aurora Serverless is a powerful database engine and a convenient choice for
    microservices. With Aurora Serverless, you only pay for what you use, and you
    don’t need to worry about scaling the database, thereby reducing your costs and
    the time you spend managing it.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aurora Serverless是一个强大的数据库引擎，是微服务的便捷选择。使用Aurora Serverless，你只需为使用的部分付费，你不需要担心数据库的扩展，从而降低你的成本和管理时间。
- en: 'To securely feed sensitive configuration details to your applications in Kubernetes,
    we use Kubernetes secrets. With EKS, we have two strategies for managing Kubernetes
    secrets securely:'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了在Kubernetes中安全地将敏感配置细节提供给应用程序，我们使用Kubernetes机密。使用EKS，我们有两种策略来安全地管理Kubernetes机密：
- en: Using the AWS Secrets & Configuration Provider for Kubernetes
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS Secrets & Configuration Provider for Kubernetes
- en: Using Kubernetes secrets in combination with the AWS Key Managed Service
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中使用密钥管理与 AWS 密钥管理服务结合使用
- en: '* * *'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '¹ For a quick comparison between GKE, AKS, and EKS, see Alexander Postasnick,
    “AWS vs EKS vs GKE: Managed Kubernetes Services Compared,” June 9, 2021, [https://acloudguru.com/blog/engineering/aks-vs-eks-vs-gke-managed-kubernetes-services-compared](https://acloudguru.com/blog/engineering/aks-vs-eks-vs-gke-managed-kubernetes-services-compared).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '¹ 关于 GKE、AKS 和 EKS 的快速比较，请参阅 Alexander Postasnick 的文章“AWS vs EKS vs GKE: Managed
    Kubernetes Services Compared”，发布于 2021 年 6 月 9 日，[https://acloudguru.com/blog/engineering/aks-vs-eks-vs-gke-managed-kubernetes-services-compared](https://acloudguru.com/blog/engineering/aks-vs-eks-vs-gke-managed-kubernetes-services-compared)。'
- en: ² Flexera, “2022 State of the Cloud Report” (pp. 52–53), [https://info.flexera.com/CM-REPORT-State-of-the-Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ² Flexera，“2022 云状态报告”（第 52-53 页），[https://info.flexera.com/CM-REPORT-State-of-the-Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)。
- en: ³ The AWS Load Balancer Controller is an open source project hosted on GitHub
    ([https://github.com/kubernetes-sigs/aws-load-balancer-controller/](https://github.com/kubernetes-sigs/aws-load-balancer-controller/)).
    The project was originally created by Ticketmaster and CoreOS.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ³ AWS 负载均衡器控制器是一个托管在 GitHub 上的开源项目 ([https://github.com/kubernetes-sigs/aws-load-balancer-controller/](https://github.com/kubernetes-sigs/aws-load-balancer-controller/))。该项目最初由
    Ticketmaster 和 CoreOS 创建。
- en: ⁴ To learn more about labels and how to use them, see the official documentation,
    [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/),
    and Zane Hitchcox’s “matchLabels, Labels, and Selectors Explained in Detail, for
    Beginners,” *Medium* (July 15, 2018), [https://medium.com/@zwhitchcox/matchlabels-labels-and-selectors-explained-in-detail-for-beginners-d421bdd05362](https://medium.com/@zwhitchcox/matchlabels-labels-and-selectors-explained-in-detail-for-beginners-d421bdd05362).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 若想了解更多关于标签及其使用方法的信息，请参阅官方文档，[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)，以及
    Zane Hitchcox 的文章“matchLabels, Labels, and Selectors Explained in Detail, for
    Beginners”，发表于 *Medium*（2018 年 7 月 15 日），[https://medium.com/@zwhitchcox/matchlabels-labels-and-selectors-explained-in-detail-for-beginners-d421bdd05362](https://medium.com/@zwhitchcox/matchlabels-labels-and-selectors-explained-in-detail-for-beginners-d421bdd05362)。
- en: '⁵ For more information on this point, see the official AWS documentation: [https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html).'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 关于此点的更多信息，请参阅官方 AWS 文档：[https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html)。
- en: '⁶ See the official documentation for more information on how Aurora Serverless
    works and the autoscaling configuration parameters: [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html).'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶ 若想了解更多关于 Aurora Serverless 的工作原理和自动扩展配置参数的信息，请参阅官方文档：[https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html)。
- en: ⁷ You can learn more about this option through Tracy Pierce’s article “How to
    use AWS Secrets & Configuration Provider with Your Kubernetes Secrets Store CSI
    driver,” [https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/](https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷ 您可以通过 Tracy Pierce 的文章“如何使用 AWS Secrets & Configuration Provider 与您的 Kubernetes
    Secrets Store CSI 驱动程序一起使用”了解更多关于此选项的信息，[https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/](https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/)。
- en: ⁸ Managing Kubernetes securely is a big and important topic, and to learn more
    about, it you can check out Alex Soto Bueno and Andrew Block’s *Securing Kubernetes
    Secrets* (Manning, 2022), [https://livebook.manning.com/book/securing-kubernetes-secrets/chapter-4/v-3/point-13495-119-134-1](https://livebook.manning.com/book/securing-kubernetes-secrets/chapter-4/v-3/point-13495-119-134-1).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸ 安全管理 Kubernetes 是一个庞大且重要的主题，若想了解更多，可以查看 Alex Soto Bueno 和 Andrew Block 的著作
    *Securing Kubernetes Secrets*（Manning，2022），[https://livebook.manning.com/book/securing-kubernetes-secrets/chapter-4/v-3/point-13495-119-134-1](https://livebook.manning.com/book/securing-kubernetes-secrets/chapter-4/v-3/point-13495-119-134-1)。
- en: ⁹ Ibid.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ⁹ 同上。
