- en: '5 Data frame gymnastics: Joining and grouping'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 数据帧体操：连接和分组
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Joining two data frames together
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将两个数据帧连接在一起
- en: Selecting the right type of join for your use case
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合你用例的正确类型的连接
- en: Grouping data and understanding the `GroupedData` transitional object
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行分组和理解 `GroupedData` 转换对象
- en: Breaking the `GroupedData` with an aggregation method
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚合方法打破 `GroupedData`
- en: Filling `null` values in your data frame
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的数据帧中填充 `null` 值
- en: 'In chapter 4, we looked at how we can transform a data frame using selection,
    dropping, creation, renaming, reordering, and creating a summary of columns. Those
    operations constitute the foundation for working with a data frame in PySpark.
    In this chapter, I will complete the review of the most common operations you
    will perform on a data frame: linking or *joining* data frames, as well as grouping
    data (and performing operations on the `GroupedData` object). We conclude this
    chapter by wrapping our exploratory program into a single script we can submit,
    just like we performed in chapter 3\. The skills learned in this chapter complete
    the set of fundamental operations you will use in your day-to-day work transforming
    data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章中，我们探讨了如何通过选择、删除、创建、重命名、重新排序和创建列的摘要来转换数据帧。这些操作构成了在 PySpark 中处理数据帧的基础。在本章中，我将完成对你在数据帧上最常执行的操作的回顾：连接或
    *连接* 数据帧，以及分组数据（并在 `GroupedData` 对象上执行操作）。我们通过将我们的探索性程序封装成一个可以提交的单个脚本来完成本章，就像我们在第
    3 章中所做的那样。本章学到的技能完成了你在日常工作中转换数据时将使用的所有基本操作集。
- en: We use the same `logs` data frames that we left in chapter 4\. In practical
    steps, this chapter’s code enriches our table with the relevant information contained
    in the link tables and then summarizes it in relevant groups, using what can be
    considered a graduate version of the `describe()` method I show in chapter 4\.
    If you want to catch up with a minimal amount of fuss, I provide a `checkpoint.py`
    script in the `code/Ch04-05` directory.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与第 4 章中留下的相同的 `logs` 数据帧。在实际步骤中，本章的代码通过包含在链接表中的相关信息丰富我们的表格，然后使用可以被认为是第 4
    章中展示的 `describe()` 方法的进阶版本对其进行总结。如果你想要以最小的麻烦赶上进度，我在 `code/Ch04-05` 目录提供了一个 `checkpoint.py`
    脚本。
- en: '5.1 From many to one: Joining data'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 从多到一：连接数据
- en: When working with data, we’re most often working on one structure at a time.
    Thus far, we’ve explored the many ways we can slice, dice, and modify a data frame
    to fit our wildest desires. What happens when we need to link two sources? This
    section will introduce joins and how we can apply them when using a star schema
    setup or another set of tables where values match exactly.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理数据时，我们通常一次只处理一个结构。到目前为止，我们已经探索了多种方法来切割、切块和修改数据帧以满足我们的最大愿望。当我们需要连接两个来源时会发生什么？本节将介绍连接以及我们如何在使用星型模式设置或另一组值完全匹配的表中应用它们。
- en: Joining data frames is a common operation when working with related tables.
    If you’ve used other data-processing libraries, you might have seen the same operation
    called a *merge* or a *link*. Because there are multiple ways to perform joins,
    the next section sets a common vocabulary to avoid confusion and build understanding
    on solid ground.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理相关表时，连接数据帧是一个常见的操作。如果你使用过其他数据处理库，你可能见过相同的操作被称为 *合并* 或 *链接*。因为执行连接有多种方式，所以下一节设定了一个共同词汇表，以避免混淆并在坚实的基础上建立理解。
- en: 5.1.1 What’s what in the world of joins
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 连接世界中的“什么是什么”
- en: This section covers the core blueprint of joins. I introduce the general syntax
    for the `join()` method, as well as the different parameters. With this, you’ll
    recognize and know how to construct a basic join and be ready to undertake the
    subtlety of performing more specific join operations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了连接的核心蓝图。我介绍了 `join()` 方法的通用语法，以及不同的参数。有了这些，你将能够识别并知道如何构建一个基本的连接，并准备好执行更具体的连接操作。
- en: At the most basic level, a `join` operation is a way to take the data from one
    data frame and link it to another one according to a set of rules. To introduce
    the moving parts of a join, I provide a second table to be joined to our `logs`
    data frame in listing 5.1\. I use the same parameterization of the `SparkReader.csv`
    as used for the `logs` table to read our new `log_identifier` table. Once the
    table is ingested, I filter the data frame to keep only the primary channels,
    as per the data documentation. With this, we should be good to go.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本层面上，`join` 操作是一种将一个数据帧中的数据根据一组规则链接到另一个数据帧的方法。为了介绍连接的移动部件，我在列表 5.1 中提供了一个要连接到我们的
    `logs` 数据帧的第二个表。我使用与 `logs` 表相同的 `SparkReader.csv` 参数化来读取我们的新 `log_identifier`
    表。一旦表被摄取，我就过滤数据帧，只保留主要通道，如数据文档中所述。有了这个，我们应该可以开始了。
- en: 'Listing 5.1 Exploring our first link table: `log_identifier`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 探索我们的第一个链接表：`log_identifier`
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ This is the channel identifier.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是通道标识符。
- en: ❷ This is the channel key (which maps to our center table).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是通道键（它映射到我们的中心表）。
- en: '❸ This is a Boolean flag: Is the channel primary (1) or (0)? We want only the
    1s.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这是一个布尔标志：通道是主要通道（1）还是（0）？我们只想保留 1。
- en: We have two data frames, `logs` and `log_identifier`, each containing a set
    of columns. We are ready to start joining!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个数据帧，`logs` 和 `log_identifier`，每个数据帧都包含一组列。我们准备好开始连接操作了！
- en: 'The join operation has three major ingredients:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 连接操作有三个主要成分：
- en: Two tables, called a *left* and a *right* table, respectively
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个表，分别称为 *left* 和 *right* 表
- en: One or more *predicates*, which are the series of conditions that determine
    how records between the two tables are joined
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个 *predicates*，它们是一系列条件，决定了两个表之间的记录如何连接
- en: A *method* to indicate how we perform the join when the predicate succeeds and
    when it fails
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种指示在谓词成功和失败时如何执行连接的方法
- en: With these three ingredients, you can construct a join between two data frames
    in PySpark by filling the blueprint in listing 5.2 with the relevant keywords
    to accomplish the desired behavior. Every join operation in PySpark will follow
    the same blueprint. The next few sections will take each keyword and illustrate
    how they impact the end result.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这三个成分，您可以使用列表 5.2 中的蓝图，通过填写相关关键字来在 PySpark 中构建两个数据帧之间的连接，以实现所需的行为。PySpark
    中的每个连接操作都将遵循相同的蓝图。接下来的几节将分别介绍每个关键字，并说明它们如何影响最终结果。
- en: Listing 5.2 A bare-bone recipe for a join in PySpark
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 PySpark 中连接操作的裸骨配方
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 5.1.2 Knowing our left from our right
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 知道我们的左和右
- en: A join is performed on two tables at a time. In this section, we cover the `[LEFT]`
    and `[RIGHT]` blocks of listing 5.2\. Knowing which table is called left and which
    is called right is helpful when discussing join types, so we start with this useful
    vocabulary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一次在两个表上执行连接操作。在本节中，我们将介绍列表 5.2 中的 `[LEFT]` 和 `[RIGHT]` 块。了解哪个表被称为左表以及哪个被称为右表，在讨论连接类型时很有帮助，因此我们从这个有用的词汇表开始。
- en: Because of the SQL heritage in the data manipulation vocabulary, the two tables
    are named *left* and *right* tables. In PySpark, a neat way to remember which
    is which is to say that the left table is to the left of the `join()` method,
    whereas the right is to the right (inside the parentheses). Knowing which is which
    is very useful when choosing the join method. Unsurprisingly, there are a left
    and right join types (see section 5.1.4).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据操作词汇中的 SQL 遗产，两个表被命名为 *left* 和 *right* 表。在 PySpark 中，一个记住左右表的好方法是说左表位于 `join()`
    方法的左侧，而右表位于右侧（括号内）。知道哪个是哪个在选择连接方法时非常有用。不出所料，存在左连接和右连接类型（见 5.1.4 节）。
- en: Our tables are now identified, so we can update our join blueprint as in the
    next listing. We now need to steer our attention to the next parameter, the predicates.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的表现在已经被标识，因此我们可以更新我们的连接蓝图，如下一列表所示。我们现在需要将注意力转向下一个参数，即谓词。
- en: Listing 5.3 A bare-bone join in PySpark, with left and right tables filled in
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 PySpark 中裸骨连接，左表和右表已填写
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ logs is the left table . . .
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ logs 是左表 . . .
- en: ❷ . . . and log_identifier is the right table.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ . . . 以及 log_identifier 是右表。
- en: '5.1.3 The rules to a successful join: The predicates'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 成功连接的规则：谓词
- en: This section covers the `[PREDICATES]` block of the join blueprint, which is
    the cornerstone of determining what records from the left table will match the
    right table. Most predicates in join operations are simple, but they can grow
    significantly in complexity depending on the logic you want. I introduce the simplest
    and most common use cases first before graduating to more complex predicates.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了连接蓝图中的 `[PREDICATES]` 块，这是确定左表中的哪些记录将与右表匹配的基石。连接操作中的大多数谓词都很简单，但根据您想要的逻辑，它们可以显著增加复杂性。我首先介绍最简单和最常见的使用案例，然后再过渡到更复杂的谓词。
- en: 'The predicates of a PySpark join are rules between columns of the left and
    right data frames. A join is performed record-wise, where each record on the left
    data frame is compared (via the predicates) to each record on the right data frame.
    If the predicates return `True`, the join is a match and is a no-match if `False`.
    We can think of this like a two-way `where` (see chapter 2): you match the values
    from one table to the other, and the (Boolean) result of the predicate block determines
    if it’s a match.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 连接的谓词是左右数据帧列之间的规则。连接是按记录进行的，其中左数据帧上的每个记录（通过谓词）与右数据帧上的每个记录进行比较。如果谓词返回
    `True`，则连接匹配；如果返回 `False`，则不匹配。我们可以将其视为双向的 `where` 子句（见第2章）：您将一个表中的值与另一个表中的值匹配，谓词块（布尔值）的结果确定是否匹配。
- en: The best way to illustrate a predicate is to create a simple example and explore
    the results. For our two data frames, we will build the predicate `logs["LogServiceID"]`
    `==` `log_identifier["LogServiceID"]`. In plain English, this translates to “match
    the records from the `logs` data frame to the records from the `log_identifier`
    data frame when the value of their `LogServiceID` column is equal.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的说明谓词的方法是创建一个简单的示例并探索结果。对于我们的两个数据帧，我们将构建谓词 `logs["LogServiceID"]` `==` `log_identifier["LogServiceID"]`。用普通英语来说，这相当于“当它们的
    `LogServiceID` 列的值相等时，匹配 `logs` 数据帧中的记录与 `log_identifier` 数据帧中的记录”。
- en: 'I’ve taken a small sample of the data in both data frames and illustrated the
    result of applying the predicate in figure 5.1\. There are two important points
    to highlight:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经从两个数据帧中取了一个小样本，并在图5.1中说明了应用谓词的结果。有两个重要点需要强调：
- en: If one record in the left table resolves the predicate with more than one record
    in the right table (or vice versa), this record will be duplicated in the joined
    table.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果左表中的一条记录与右表中的多条记录（或反之亦然）解析谓词，则该记录将在连接表中重复。
- en: If one record in the left or right table does not resolve the predicate with
    any record in the other table, it will not be present in the resulting table *unless
    the join method (see section 5.1.4) specifies a protocol for failed predicates*.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果左表或右表中的一条记录与另一表中的任何记录都无法解析谓词，则它将不会出现在结果表中，除非连接方法（见第5.1.4节）指定了失败谓词的协议。
- en: '![](../Images/05-01.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-01.png)'
- en: 'Figure 5.1 A simple join predicate resolution between `logs` and `log_identifier`
    using `LogServiceID` in both tables and equality testing in the predicate. I show
    only the four successes in the result table. Our predicate is applied to a sample
    of our two tables: `3590` in the left table resolves the predicate twice, while
    `3417` on the left and `3883` on the right have no matches.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 使用 `LogServiceID` 在两个表中的简单连接谓词解析以及谓词中的等式测试。我只显示了结果表中的四个成功案例。我们的谓词应用于我们两个表的一个样本：左表中的
    `3590` 解析谓词两次，而左表中的 `3417` 和右表中的 `3883` 没有匹配。
- en: In our example, the `3590` record on the left is equal to the two corresponding
    records on the right, and we see two solved predicates with this number in our
    result set. On the other hand, the `3417` record does not match anything on the
    right, and therefore is not present in the result set. The same thing happens
    with the `3883` record in the right table.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，左表中的 `3590` 记录等于右表中的两条对应记录，我们在结果集中看到这个数字有两个解决的谓词。另一方面，左表中的 `3417` 记录与右表中没有任何匹配，因此它不在结果集中。右表中的
    `3883` 记录发生相同的情况。
- en: 'You are not limited to a single test in your predicate. You can use multiple
    conditions by separating them with Boolean operators such as `|` (or) or `&` (and).
    You can also use a different test than equality. Here are two examples and their
    plain English translation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您在谓词中不仅限于一个测试。您可以通过使用布尔运算符（如 `|`（或）或 `&`（与））将它们分开来使用多个条件。您还可以使用不同于等于的测试。以下有两个示例及其普通英语翻译：
- en: '`(logs["LogServiceID"]` `==` `log_identifier["LogServiceID"])` `&` `(logs["left_
    col"]` `<` `log_identifier["right_col"])`—This will only match the records that
    have the same `LogServiceID` on both sides *and* where the value of the `left_col`
    in the `logs` table is smaller than the value of the `right_col` in the `log_identifier`
    table.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(logs["LogServiceID"]` `==` `log_identifier["LogServiceID"])` `&` `(logs["left_col"]`
    `<` `log_identifier["right_col"])`—这只会匹配两边的`LogServiceID`相同，并且`logs`表中的`left_col`的值小于`log_identifier`表中的`right_col`的值的记录。'
- en: '`(logs["LogServiceID"]` `==` `log_identifier["LogServiceID"])` `|` `(logs["left_
    col"]` `>` `log_identifier["right_col"])`—This will only match the records that
    have the same `LogServiceID` on both sides *or* where the value of the `left_col`
    in the `logs` table is greater than the value of the `right_col` in the `log_identifier`
    table.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(logs["LogServiceID"]` `==` `log_identifier["LogServiceID"])` `|` `(logs["left_col"]`
    `>` `log_identifier["right_col"])`—这只会匹配两边的`LogServiceID`相同的记录，或者`logs`表中的`left_col`的值大于`log_identifier`表中的`right_col`的值。'
- en: You can make the operations as complicated as you want. I recommend wrapping
    each condition in parentheses to avoid worrying about operator precedence and
    to facilitate the reading.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使操作尽可能复杂。我建议将每个条件用括号括起来，以避免担心运算符优先级并便于阅读。
- en: Before adding our predicate to our join in progress, I want to note that PySpark
    provides a few predicate shortcuts to reduce the complexity of the code. If you
    have multiple `and` predicates (such as `(left["col1"]` `==` `right["colA"])`
    `&` `(left["col2"]` `>` `right["colB"])` `&` `(left["col3"]` `!=` `right["colC"])`),
    you can put them into a list, such as `[left["col1"]` `==` `right["colA"],` `left["col2"]`
    `>` `right["colB"],` `left["col3"]` `!=` `right["colC"]]`. This makes your intent
    more explicit and avoids counting parentheses for long chains of conditions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的谓词添加到正在进行的连接之前，我想指出PySpark提供了一些谓词快捷方式来减少代码的复杂性。如果你有多个`and`谓词（例如`(left["col1"]`
    `==` `right["colA"])` `&` `(left["col2"]` `>` `right["colB"])` `&` `(left["col3"]`
    `!=` `right["colC"])`），你可以将它们放入一个列表中，例如`[left["col1"]` `==` `right["colA"],` `left["col2"]`
    `>` `right["colB"],` `left["col3"]` `!=` `right["colC"]]`。这使得你的意图更加明确，并避免了在长链条件中计数括号。
- en: Finally, if you are performing an “equi-join,” where you are testing for equality
    between identically named columns, you can simply specify the name of the columns
    as a string or a list of strings as a predicate. In our case, it means that our
    predicate can only be `"LogServiceID"`. This is what I put in the following listing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你正在进行“等值连接”，即测试同名列之间的相等性，你可以简单地指定列名作为字符串或字符串列表作为谓词。在我们的例子中，这意味着我们的谓词只能是`"LogServiceID"`。这就是我在以下列表中放入的内容。
- en: Listing 5.4 A join in PySpark, with left and right tables and predicate
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4 PySpark中的连接，包含左右表和谓词
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The join method influences how you structure predicates, so section 5.1.5 revisits
    the whole join operation after we’re done with the ingredient-by-ingredient approach.
    The last parameter is the `how`, which completes our join operation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 连接方法影响你如何构建谓词，因此第5.1.5节在完成逐个成分的方法后，重新审视整个连接操作。最后一个参数是`how`，它完成了我们的连接操作。
- en: '5.1.4 How do you do it: The join method'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 如何做：连接方法
- en: The last ingredient of a successful join is the `how` parameter, which will
    indicate the join method. Most books explaining joins show Venn diagrams indicating
    how each join colors the different areas, but I find that is only useful as a
    reminder, not a teaching tool. I’ll review each type of join with the same tables
    we’ve used in figure 5.1 and give the result of the operation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 成功连接的最后一种成分是`how`参数，它将指示连接方法。大多数解释连接的书籍都会展示维恩图来表示每种连接如何着色不同的区域，但我发现这仅仅是一个提醒，而不是一个教学工具。我将使用与图5.1中相同的表格来回顾每种类型的连接，并给出操作的结果。
- en: 'A join method boils down to these two questions:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 连接方法归结为这两个问题：
- en: What happens when the return value of the predicates is `True`?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当谓词的返回值为`True`时会发生什么？
- en: What happens when the return value of the predicates is `False`?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当谓词的返回值为`False`时会发生什么？
- en: Classifying the join methods based on the answer to these questions is an easy
    way to remember them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些问题的答案对连接方法进行分类是记住它们的一个简单方法。
- en: Tip PySpark’s joins are essentially the same as SQL’s. If you are already comfortable
    with them, feel free to skip this section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 PySpark的连接本质上与SQL相同。如果你已经熟悉它们，可以自由跳过这一节。
- en: Inner join
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 内连接
- en: An inner join (`how="inner"`) is the most common join. PySpark will default
    to an inner join if you don’t pass a join method explicitly. It returns a record
    if the predicate is true and drops it if false. I consider an inner join the natural
    way to think of joins because they are very simple to reason about.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 内连接（`how="inner"`）是最常见的连接。如果你没有明确传递连接方法，PySpark将默认使用内连接。如果谓词为真，则返回记录；如果为假，则丢弃它。我认为内连接是考虑连接的自然方式，因为它们非常简单易懂。
- en: If we look at our tables, we have a table very similar to figure 5.1\. The record
    with the `LogServiceID` `==` `3590` on the left will be duplicated because it
    matches two records in the right table. The result is illustrated in figure 5.2\.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看我们的表，我们有一个与图5.1非常相似的表。左表上`LogServiceID`等于`3590`的记录将被重复，因为它与右表中的两个记录匹配。结果如图5.2所示。
- en: '![](../Images/05-02.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-02.png)'
- en: Figure 5.2 An inner join. Each successful predicate creates a joined record.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/05-02.png)'
- en: Left and right outer join
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 左和右外连接
- en: 'Left (`how="left"` or `how="left_outer"`) and right (`how="right"` or `how="right_
    outer"`), as displayed in figure 5.4, are like an inner join in that they generate
    a record for a successful predicate. The difference is what happens when the predicate
    is false:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 左（`how="left"`或`how="left_outer"`）和右（`how="right"`或`how="right_outer"`），如图5.4所示，与内连接类似，它们在成功谓词的情况下生成一个记录。区别在于当谓词为假时会发生什么：
- en: A left (also called a *left outer*) join will add the unmatched records from
    the left table in the joined table, filling the columns coming from the right
    table with `null`.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个左连接（也称为*左外连接*）将在连接表中添加来自左表的不匹配记录，并用`null`填充来自右表的列。
- en: A right (also called a *right outer*) join will add the unmatched records from
    the right in the joined table, filling the columns coming from the left table
    with `null`.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个右连接（也称为*右外连接*）将在连接表中添加右表中不匹配的记录，并用`null`填充来自左表的列。
- en: In practice, this means that your joined table is guaranteed to contain all
    the records of the table that feed the join (left or right). Visually, figure
    5.3 shows this. Although `3417` doesn’t satisfy the predicate, it is still present
    in the left joined table. The same happens with `3883` and the right table. Just
    like an inner join, if the predicate is successful more than once, the record
    will be duplicated.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着你的连接表保证包含所有为连接提供数据的表的记录（左或右）。直观上，图5.3显示了这一点。尽管`3417`不满足谓词，但它仍然存在于左连接表中。同样，`3883`和右表也是如此。就像内连接一样，如果谓词成功多次，记录将被重复。
- en: '![](../Images/05-03.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-03.png)'
- en: Figure 5.3 A left and right joined table. All the records of the direction table
    are present in the resulting table.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 左连接和右连接的表。结果表中包含方向表的所有记录。
- en: Left and right joins are very useful when you are not certain if the link table
    contains every key. You can then fill the `null` values (see listing 5.16) or
    process them knowing you didn’t drop any records.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不确定链接表是否包含所有键时，左连接和右连接非常有用。你可以填充`null`值（参见列表5.16）或处理它们，同时知道你没有删除任何记录。
- en: Full outer join
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 全外连接
- en: A full outer (`how="outer"`, `how="full"`, or `how="full_outer"`) join is simply
    the fusion of a left and right join. It will add the unmatched records from the
    left and the right table, padding with `null`. It serves a similar purpose to
    the left and right join but is not as popular since you’ll generally have one
    (and only one) anchor table where you want to preserve all records.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 全外连接（`how="outer"`，`how="full"`或`how="full_outer"`）连接只是左连接和右连接的融合。它将添加来自左表和右表的不匹配记录，并用`null`填充。它服务于与左连接和右连接类似的目的，但由于你通常只有一个（并且只有一个）锚表，你希望在其中保留所有记录，所以它不太受欢迎。
- en: '![](../Images/05-04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-04.png)'
- en: Figure 5.4 A left and right joined table. We can see all the records from both
    tables.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 左连接和右连接的表。我们可以看到两个表中的所有记录。
- en: Left semi-join and left anti-join
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 左半连接和左反连接
- en: The left semi-join and left anti-join are less popular but still quite useful
    nonetheless.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 左半连接和左反连接不太常见，但仍然非常有用。
- en: A left semi-join (`how="left_semi"`) is the same as an inner join, but keeps
    the columns in the left table. It also won’t duplicate the records in the left
    table if they fulfill the predicate with more than one record in the right table.
    Its main purpose is to filter records from a table based on a predicate that is
    depending on another table.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 左半连接 (`how="left_semi"`) 与内连接相同，但保留左表中的列。它也不会在左表中重复记录，如果它们与右表中的多个记录满足谓词。其主要目的是根据依赖于另一个表的谓词从表中过滤记录。
- en: A left anti-join (`how="left_anti"`) is the opposite of an inner join. It will
    keep only the records from the left table *that do not match the predicate with
    any record in the right table*. If a record from the left table matches a record
    from the right table, it gets dropped from the join operation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 左外反连接 (`how="left_anti"`) 是内连接的相反。它将仅保留来自左表的记录，这些记录与右表中的任何记录都不匹配。如果左表中的记录与右表中的记录匹配，它将从连接操作中删除。
- en: 'Our blueprint join is now finalized: we are going with an inner join since
    we want to keep only the records where the `LogServiceID` has additional information
    in our `log_identifier` table. Since our join is complete, I assign the result
    to a new variable: `logs_and_channels`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的设计图连接现在已最终确定：我们选择内连接，因为我们只想保留 `LogServiceID` 在我们的 `log_identifier` 表中有额外信息的记录。由于我们的连接已完成，我将结果分配给一个新变量：`logs_and_channels`。
- en: Listing 5.5 Our join in PySpark, with all the parameters filled in
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 PySpark 中的我们的连接，所有参数都已填写
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ I could have omitted the how parameter outright, since inner join is the default.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我本可以直接省略 how 参数，因为内连接是默认的。
- en: In this section, we reviewed the different join methods and their usage. The
    next section covers the innocuous but important aspect of column and data frame
    names when joining. It’ll provide a solution to the common problem of having identically
    named columns in both the left and right data frame.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了不同的连接方法及其用法。下一节将涵盖连接时列和数据帧名称的无害但重要方面。它将提供一个解决方案，以解决左表和右表都具有相同名称列的常见问题。
- en: 'Cross join: The nuclear option'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉连接：核选项
- en: A cross join (`how="cross"`) is the nuclear option. It returns a record for
    every record pair, regardless of the value the predicates return. In our data
    frame example, our `logs` table contains four records and our `logs_identifier`
    five records, so the cross join will contain 4 × 5 = 20 records. The result is
    illustrated in figure 5.5.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉连接 (`how="cross"`) 是核选项。它为每个记录对返回一个记录，无论谓词返回的值如何。在我们的数据帧示例中，我们的 `logs` 表包含四个记录，而
    `logs_identifier` 表包含五个记录，因此交叉连接将包含 4 × 5 = 20 个记录。结果如图 5.5 所示。
- en: '![](../Images/05-05.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-05.png)'
- en: Figure 5.5 A visual example of a cross join. Each record on the left is matched
    to every record on the right.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 交叉连接的视觉示例。左边的每个记录都与右边的每个记录相匹配。
- en: Cross joins are seldom the operation you want, but they are useful when you
    want a table that contains every possible combination.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉连接很少是您想要的操作，但它们在您想要包含每个可能组合的表时很有用。
- en: Tip PySpark also provides an explicit `crossJoin()` method that takes the right
    data frame as a parameter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 还提供了一个显式的 `crossJoin()` 方法，该方法将右侧数据帧作为参数。
- en: The science of joining in a distributed environment
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式环境中的连接科学
- en: When joining data in a distributed environment, “we don’t care about where data
    is” no longer works. To be able to process a comparison between records, the data
    needs to be on the same machine. If not, PySpark will move the data in an operation
    called a *shuffle*. As you can imagine, moving large amounts of data over the
    network is very slow, and we should aim to avoid this when possible.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境中连接数据时，“我们不在乎数据在哪里”不再适用。为了能够处理记录之间的比较，数据需要位于同一台机器上。如果不是这样，PySpark 将在称为
    *shuffle* 的操作中移动数据。正如你所想象的那样，在网络上移动大量数据非常慢，我们应该尽可能避免这种情况。
- en: This is one of the instances in which PySpark’s abstraction model shows some
    weakness. Since joins are such an important part of working with multiple data
    sources, I introduce the syntax here so we can get things rolling. Chapter 11
    discusses shuffles in greater detail.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '这就是 PySpark 的抽象模型显示出一些弱点的一个实例。由于连接是处理多个数据源的重要组成部分，我在这里介绍了语法，这样我们就可以开始操作了。第
    11 章将更详细地讨论 shuffle。 '
- en: 5.1.5 Naming conventions in the joining world
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.5 连接世界的命名约定
- en: This section covers how PySpark manages column and data frame names. While this
    applies beyond the join world, name clashing is most painful when you are trying
    to assemble many data frames into one. We cover how to prevent name clashing and
    how to treat it if you inherit an already mangled data frame.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了PySpark如何管理列和数据帧的名称。虽然这适用于连接世界之外的情况，但在尝试将多个数据帧组合成一个时，名称冲突是最痛苦的。我们介绍了如何防止名称冲突以及如果你继承了一个已经混乱的数据帧应该如何处理它。
- en: By default, PySpark will not allow two columns to be named the same. If you
    create a column with `withColumn()` using an existing column name, PySpark will
    overwrite (or shadow) the column. When joining data frames, the situation is a
    little more complicated, as displayed in the following listing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PySpark不允许两个列有相同的名称。如果你使用`withColumn()`创建一个具有现有列名称的列，PySpark将覆盖（或阴影）该列。当连接数据帧时，情况要复杂一些，如下面的列表所示。
- en: Listing 5.6 A join that generates two seemingly identically named columns
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.6 生成两个看似同名列的连接
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ This is one LogServiceID column . . .
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是一个LogServiceID列...
- en: ❷ . . . and this is another.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ ...这是另一个。
- en: '❸ PySpark doesn’t know which column we mean: is it LogServiceID or LogServiceID?'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PySpark不知道我们指的是哪个列：是LogServiceID还是LogServiceID？
- en: PySpark happily joins the two data frames but fails when we try to work with
    the ambiguous column. This is a common situation when working with data that follows
    the same convention for column naming. To solve this problem, in this section
    I show three methods, from the easiest to the most general.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark愉快地将两个数据帧连接起来，但当我们尝试处理有歧义的列时却失败了。这在处理遵循相同列命名约定的数据时是一个常见情况。为了解决这个问题，在本节中，我将展示三种方法，从最简单到最通用。
- en: First, when performing an equi-join, I prefer using the simplified syntax, since
    it takes care of removing the second instance of the predicate column. This only
    works when using an equality comparison, since the data is identical in both columns
    from the predicate, which prevents information loss. I show the code and schema
    of the resulting data frame when using a simplified equi-join in the next listing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在进行等值连接时，我更喜欢使用简化语法，因为它会处理移除谓词列的第二个实例。这仅在使用相等比较时有效，因为从谓词的两个列中的数据是相同的，这防止了信息丢失。我将在下一个列表中展示使用简化等值连接时的代码和模式。
- en: Listing 5.7 Using the simplified syntax for equi-joins
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.7 使用等值连接的简化语法
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '❶ No LogServiceID here: PySpark kept only the first referred column.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这里没有LogServiceID：PySpark只保留了第一个引用的列。
- en: The second approach relies on the fact that PySpark-joined data frames remember
    the origin of the columns. Because of this, we can refer to the `LogServiceID`
    columns using the same nomenclature as before (i.e., `log_identifier["LogServiceID"]`).
    We can then rename this column or delete it, and thus solve our issue. I use this
    approach in the following listing.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法依赖于PySpark连接的数据帧记住列的来源。正因为如此，我们可以使用与之前相同的命名法（即，`log_identifier["LogServiceID"]`）来引用`LogServiceID`列。然后我们可以重命名这个列或删除它，从而解决我们的问题。我在下面的列表中使用这种方法。
- en: Listing 5.8 Using the origin name of the column for unambiguous selection
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8 使用列的原始名称进行无歧义选择
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ By dropping one of the two duplicated columns, we can then use the name for
    the other without any problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过删除两个重复列中的一个，我们就可以无问题地使用另一个列的名称。
- en: The last approach is convenient if you use the `Column` object directly. PySpark
    will not resolve the origin name when you rely on `F.col()` to work with columns.
    To solve this in the most general way, we need to `alias()` our tables when performing
    the join, as shown in the following listing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种方法如果你直接使用`Column`对象时很方便。PySpark不会解析原始名称，当你依赖`F.col()`来处理列时。为了以最通用的方式解决这个问题，我们需要在执行连接时对表进行`alias()`，如下面的列表所示。
- en: Listing 5.9 Aliasing our tables to resolve the origin
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9 别名我们的表以解决来源问题
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Our logs table gets aliased as left.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的日志表被别名化为left。
- en: ❷ Our log_identifier gets aliased as right.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的log_identifier被别名化为right。
- en: ❸ F.col() will resolve left and right as a prefix for the column names.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ F.col()将作为列名称的前缀解析左右。
- en: All three approaches are valid. The first one works only in the case of equi-joins,
    but the two others are mostly interchangeable. PySpark gives you a lot of control
    over the structure and naming of your data frame but requires you to be explicit.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种方法都是有效的。第一种方法仅在等值连接的情况下有效，但其他两种方法基本上可以互换。PySpark给你提供了很多控制数据帧结构和命名的选项，但要求你必须明确。
- en: 'This section packed in a lot of information about joins, a very important tool
    when working with interrelated data frames. Although the possibilities are endless,
    the syntax is simple and easy to understand: `left.join(right` decides the first
    parameter. `on` decides if it’s a match. `how` indicates how to operate on match
    success and failures.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了关于连接的大量信息，这是处理相关数据框时一个非常重要的工具。尽管可能性是无限的，但语法简单易懂：`left.join(right)` 决定第一个参数。`on`
    决定是否匹配。`how` 指示如何在匹配成功和失败时操作。
- en: Now that the first join is done, we will link two additional tables to continue
    our data discovery and processing. The `CategoryID` table contains information
    about the types of programs, and the `ProgramClassID` table contains the data
    that allows us to pinpoint the commercials.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在第一个连接已经完成，我们将链接两个额外的表以继续我们的数据发现和处理。`CategoryID` 表包含有关程序类型的信息，而 `ProgramClassID`
    表包含允许我们定位广告的数据。
- en: 'This time, we are performing `left` joins since we are not entirely certain
    about the existence of the keys in the link table. In listing 5.10, we follow
    the same process as we did for the `log_identifier` table in one fell swoop:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们正在执行 `left` 连接，因为我们并不完全确定链接表中的键的存在。在列表 5.10 中，我们一次性遵循了我们为 `log_identifier`
    表所做的相同过程：
- en: We read the table using the `SparkReader.csv` and the same configuration as
    our other tables.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `SparkReader.csv` 和与其他表相同的配置读取表。
- en: We keep the relevant columns.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们保留相关列。
- en: We join the data to our `logs_and_channels` table, using PySpark’s method chaining.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 PySpark 的方法链将数据连接到我们的 `logs_and_channels` 表。
- en: Listing 5.10 Linking the category and program class tables using two left joins
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10 使用两个左连接链接类别和程序类别表
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ We’re aliasing the EnglishDescription column to remember what it maps to.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们正在将 EnglishDescription 列别名为记住它映射的内容。
- en: ❷ We’re also aliasing here, but for the program class.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们在这里也进行了别称，但针对的是程序类别。
- en: 'With our table nicely augmented, let’s move to our last step: summarizing the
    table using groupings.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的表得到很好的增强后，让我们进入最后一步：使用分组来总结表。
- en: Exercise 5.1
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.1
- en: Assume two tables, `left` and `right`, each containing a column named `my_column`.
    What is the result of this code?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有两个表，`left` 和 `right`，每个表都包含一个名为 `my_column` 的列。这段代码的结果是什么？
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Exercise 5.2
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.2
- en: Assume two data frames, `red` and `blue`. Which is the appropriate join to use
    in `red.join(blue,` `...)` if you want to join `red` and `blue` and keep all the
    records satisfying the predicate?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有两个数据框，`red` 和 `blue`。如果你想在 `red.join(blue, ...)` 中使用适当的连接来连接 `red` 和 `blue`
    并保留满足谓词的所有记录，应该使用哪种连接？
- en: a) Left
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: a) 左
- en: b) Right
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: b) 右
- en: c) Inner
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: c) 内部
- en: d) Theta
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: d) Theta
- en: e) Cross
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: e) 交叉
- en: Exercise 5.3
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.3
- en: Assume two data frames, `red` and `blue`. Which is the appropriate join to use
    in `red.join(blue,` `...)` if you want to join `red` and `blue` and keep all the
    records satisfying the predicate *and* the records in the `blue` table?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有两个数据框，`red` 和 `blue`。如果你想在 `red.join(blue, ...)` 中使用适当的连接来连接 `red` 和 `blue`
    并保留满足谓词的所有记录，应该使用哪种连接？
- en: a) Left
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: a) 左
- en: b) Right
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: b) 右
- en: c) Inner
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: c) 内部
- en: d) Theta
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: d) Theta
- en: e) Cross
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: e) 交叉
- en: 5.2 Summarizing the data via groupby and GroupedData
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 通过 groupby 和 GroupedData 总结数据
- en: When displaying data, especially large amounts of data, you’ll often summarize
    data using statistics as a first step. Chapter 4 showed how you can use `summary()`
    and `display()` to compute mean, min, max, and so on over the entire data frame.
    How about stretching our data frame a little by summarizing it according to column
    contents?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在显示数据时，尤其是大量数据时，你通常会首先使用统计方法来总结数据。第四章展示了如何使用 `summary()` 和 `display()` 计算整个数据框的均值、最小值、最大值等。那么，通过根据列内容总结来稍微扩展我们的数据框怎么样？
- en: 'This section covers how to summarize a data frame into more granular dimensions
    (versus the entire data frame) via the `groupby()` method. We already grouped
    our text data frame in 3; this section goes deeper into the specifics of grouping.
    Here, I introduce the `GroupedData` object and its usage. In practical terms,
    we’ll use `groupby()` to answer our original question: what are the channels with
    the greatest and least proportion of commercials? To answer this, we have to take
    each channel and sum the `duration_seconds` in two ways:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何通过 `groupby()` 方法将数据框总结到更细粒度的维度（而不是整个数据框）。我们已经在 3 中对文本数据框进行了分组；本节将更深入地探讨分组的细节。在这里，我介绍了
    `GroupedData` 对象及其用法。在实践中，我们将使用 `groupby()` 来回答我们最初的问题：哪些频道拥有最多和最少的广告比例？为了回答这个问题，我们必须对每个频道进行操作，以两种方式对
    `duration_seconds` 进行求和：
- en: One to get the number of seconds when the program is a commercial
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于获取程序为商业时的秒数
- en: One to get the number of seconds of total programming
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于获取总编程秒数
- en: Our plan, before we start summing, is to identify what is considered a commercial
    and what is not. The documentation doesn’t provide formal guidance on how to do
    so, so we’ll explore the data and draw our conclusion. Let’s group!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始求和之前，我们的计划是确定什么是商业，什么不是。文档没有提供正式的指导如何做到这一点，所以我们将探索数据并得出结论。让我们开始分组！
- en: 5.2.1 A simple groupby blueprint
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 一个简单的 `groupby` 蓝图
- en: In chapter 3, we performed a very simple `groupby()` to count the occurrences
    of each word. It was a simple example of grouping and counting records based on
    the words inside the (only) column. In this section, we expand on that simple
    example by grouping over many columns. I also introduce a more general notation
    than the `count()` we’ve used previously so that we can compute more than one
    summary function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们执行了一个非常简单的 `groupby()` 来计算每个单词出现的次数。这是一个基于（唯一的）列中的单词进行分组和计数记录的简单示例。在本节中，我们扩展了这个简单示例，通过在多个列上分组。我还引入了一个比我们之前使用的
    `count()` 更通用的表示法，以便我们可以计算多个汇总函数。
- en: Since you are already acquainted with the basic syntax of `groupby()`, this
    section starts by presenting a full code block that computes the total duration
    (in seconds) of the program class. In the next listing we perform the grouping,
    compute the aggregate function, and present the results in decreasing order.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你已经熟悉了 `groupby()` 的基本语法，本节首先展示一个完整的代码块，用于计算程序类总时长（以秒为单位）。在下一个列表中，我们执行分组，计算聚合函数，并以降序展示结果。
- en: Listing 5.11 Displaying the most popular types of programs
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11 显示最流行的程序类型
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This small program has a few new parts, so let’s review them one by one.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小程序有几个新的部分，让我们逐一回顾。
- en: '![](../Images/05-06.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-06.png)'
- en: Figure 5.6 The original data frame, with the focus on the columns we are grouping
    by
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 原始数据帧，聚焦于我们要分组的列
- en: 'Our group routing starts with the `groupby()` method on the data frame shown
    in figure 5.6\. A “grouped by” data frame is no longer a data frame; instead,
    it becomes a `GroupedData` object and is displayed in all its glory in listing
    5.12\. This object is a transitional object: you can’t really inspect it (there
    is no `.show()` method), and it’s waiting for further instructions to become showable
    again. Illustrated, it would look like the right-hand side of figure 5.7\. You
    have the key (or keys, if you `groupby()` multiple columns), and the rest of the
    columns are grouped inside some “cell” awaiting a summary function so that they
    can be promoted to a bona fide column again.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分组路由从图 5.6 所示的数据帧上的 `groupby()` 方法开始。一个“按分组”的数据帧不再是数据帧；相反，它变成了一个 `GroupedData`
    对象，并在列表 5.12 中完整展示。这个对象是一个过渡对象：你实际上无法检查它（没有 `.show()` 方法），它正在等待进一步的指令以再次变得可展示。图示的话，它看起来就像图
    5.7 的右侧。你有键（或键，如果你 `groupby()` 多个列），其余的列被分组在某个“单元格”中，等待汇总函数，以便它们可以再次提升为真正的列。
- en: '![](../Images/05-07.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05-07.png)'
- en: Figure 5.7 The `GroupedData` object resulting from grouping
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 分组后的 `GroupedData` 对象
- en: Aggregating for the lazy
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为懒惰的人聚合
- en: '`agg()` also accepts a dictionary in the form `{column_name:` `aggregation_function}`
    where both are string. Because of this, we can rewrite listing 5.11 like so:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`agg()` 也接受一个字典，形式为 `{column_name: aggregation_function}`，其中两者都是字符串。正因为如此，我们可以像这样重写列表
    5.11：'
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This makes rapid prototyping very easy (you can, just like with column objects,
    use the `"*"` to refer to all columns). I personally don’t like this approach
    for most cases since you don’t get to alias your columns when creating them. I
    am including it since you will see it when reading other people’s code.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得快速原型设计变得非常容易（你可以像使用列对象一样，使用 `"*"` 来引用所有列）。我个人不喜欢这种方法，因为在创建时你无法为列创建别名。我包括它是因为你将在阅读其他人的代码时看到它。
- en: Listing 5.12 A `GroupedData` object representation
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.12 `GroupedData` 对象表示
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In chapter 3, we brought back the `GroupedData` into a data frame by using the
    `count()` method, which returns the count of each group. There are a few others,
    such as `min()`, `max()`, `mean()`, or `sum()`. We could have used the `sum()`
    method directly, but we wouldn’t have had the option of aliasing the resulting
    column and would have gotten stuck with `sum(duration_seconds)` for a name. Instead,
    we use the oddly named `agg()`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们通过使用`count()`方法将`GroupedData`重新引入数据框，该方法返回每个组的计数。还有一些其他方法，例如`min()`、`max()`、`mean()`或`sum()`。我们本可以直接使用`sum()`方法，但我们就无法选择给结果列起别名，并且会固定使用`sum(duration_seconds)`作为名称。相反，我们使用了奇怪命名的`agg()`。
- en: 'The `agg()` method, for aggregate (or aggregation), will take one or more *aggregate
    functions* from the `pyspark.sql.functions` module we all know and love, and apply
    them on each group of the `GroupedData` object. In figure 5.8, I start on the
    left with our `GroupedData` object. Calling `agg()` with an appropriate aggregate
    function pulls the column from the group cell, extracts the values, and performs
    the function, yielding the answer. Compared to using the `sum()` function on the
    `groupby` object, `agg()` trades a few keystrokes for two main advantages:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`agg()`方法（用于聚合或聚合）将从一个我们都熟悉并喜爱的`pyspark.sql.functions`模块中选择一个或多个聚合函数，并将它们应用于`GroupedData`对象的每个组。在图5.8中，我从左边的`GroupedData`对象开始。使用适当的聚合函数调用`agg()`从组单元格中提取列，提取值并执行函数，得到答案。与在`groupby`对象上使用`sum()`函数相比，`agg()`以几个按键的代价换取两个主要优势：'
- en: '`agg()` takes an arbitrary number of aggregate functions, unlike using a summary
    method directly. You can’t chain multiple functions on `GroupedData` objects:
    the first one will transform it into a data frame, and the second one will fail.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`agg()`可以接受任意数量的聚合函数，与直接使用摘要方法不同。你无法在`GroupedData`对象上链式调用多个函数：第一个会将其转换为数据框，第二个会失败。'
- en: You can alias resulting columns so that you control their name and improve the
    robustness of your code.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以给结果列起别名，从而控制它们的名称并提高你代码的健壮性。
- en: '![](../Images/05-08.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05-08.png)'
- en: 'Figure 5.8 A data frame arising from the application of the `agg()` method
    (aggregate function: `F.sum()` on `Duration_seconds`)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 应用`agg()`方法（聚合函数：`F.sum()`在`Duration_seconds`上）产生的一个数据框
- en: After the application of the aggregate function on our `GroupedData` object,
    we again have a data frame. We can then use the `orderBy()` method to order the
    data by decreasing order of `duration_total`, our newly created column. We finish
    by showing 100 rows, which is more than what the data frame contains, so it shows
    everything.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在对`GroupedData`对象应用聚合函数之后，我们再次得到一个数据框。然后我们可以使用`orderBy()`方法按`duration_total`（我们新创建的列）的降序对数据进行排序。最后，我们显示100行，这比数据框包含的行数多，因此显示了所有内容。
- en: Let’s select our commercials. Table 5.1 shows my picks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择我们的商业广告。表5.1显示了我的选择。
- en: Table 5.1 The types of programs we’ll consider as commercials
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 我们将考虑作为商业广告的节目类型
- en: '| ProgramClassCD | ProgramClass_Description | duration_total |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| ProgramClassCD | 程序类别描述 | duration_total |'
- en: '| COM | COMMERCIAL MESSAGE | 106810189 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| COM | 商业信息 | 106810189 |'
- en: '| PRC | PROMOTION OF UPCOMING CANADIAN PROGRAM | 27017583 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| PRC | 推广即将到来的加拿大节目 | 27017583 |'
- en: '| PGI | PROGRAM INFOMERCIAL | 23196392 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| PGI | 节目信息广告 | 23196392 |'
- en: '| PRO | PROMOTION OF NON-CANADIAN PROGRAM | 10213461 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| PRO | 推广非加拿大节目 | 10213461 |'
- en: '| LOC | LOCAL ADVERTISING | 483042 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| LOC | 本地广告 | 483042 |'
- en: '| SPO | SPONSORSHIP MESSAGE | 45257 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| SPO | 赞助信息 | 45257 |'
- en: '| MER | MERCHANDISING | 40695 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| MER | 营销 | 40695 |'
- en: '| SOL | SOLICITATION MESSAGE | 7808 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| SOL | 请求信息 | 7808 |'
- en: Now that we’ve done the hard job of identifying our commercial codes, we can
    start counting. The next section covers how we can flex the aggregation using
    custom column definitions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了识别我们的商业代码的艰难工作，我们可以开始计数。下一节将介绍我们如何使用自定义列定义来灵活使用聚合。
- en: agg() is not the only player in town
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`agg()`不是市场上唯一的参与者'
- en: 'You can also use `groupby()`, with the `apply()` (Spark 2.3+) and `applyInPandas()`
    (Spark 3.0+) method, in the creatively named *split-apply-combine* pattern. We
    explore this powerful tool in chapter 9\. Other less-used (but still useful) methods
    are also available. Check out the API documentation for the methods over the `GroupedData`
    object: [http://mng.bz/aDoJ](http://mng.bz/aDoJ).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`groupby()`，结合`apply()`（Spark 2.3+）和`applyInPandas()`（Spark 3.0+）方法，在名为“split-apply-combine”的创新模式中使用。我们将在第9章中探讨这个强大的工具。其他较少使用（但仍然有用）的方法也可用。查看`GroupedData`对象方法的API文档：[http://mng.bz/aDoJ](http://mng.bz/aDoJ)。
- en: '5.2.2 A column is a column: Using agg() with custom column definitions'
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 列就是列：使用 agg() 和自定义列定义
- en: When grouping and aggregating columns in PySpark, we have all the power of the
    `Column` object at our fingertips. This means that we can group by and aggregate
    on custom columns! For this section, we will start by building a definition of
    `duration_ commercial`, which takes the duration of a program only if it is a
    commercial, and use this in our `agg()` statement to seamlessly compute both the
    total duration and the commercial duration.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 PySpark 中对列进行分组和聚合时，我们手头上有 `Column` 对象的全部功能。这意味着我们可以根据自定义列进行分组和聚合！在本节中，我们将首先定义
    `duration_ commercial`，它只取商业节目的时长，并在我们的 `agg()` 语句中使用它来无缝地计算总时长和商业时长。
- en: If we encode the contents of table 5.1 into a PySpark definition, it gives us
    the next listing.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将表 5.1 的内容编码成 PySpark 定义，它将给出下一个列表。
- en: Listing 5.13 Computing only the commercial time for each program in our table
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.13 计算表中每个节目的商业时间
- en: '[PRE14]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: I think that the best way to describe the code this time is to literally translate
    it into plain English.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为描述这次代码的最佳方式是将它直接翻译成普通的英语。
- en: '***When** the field of the column `ProgramClass`, **trimmed** of spaces at
    the beginning and end of the field, **is in** our list of commercial codes, then
    take the value of the field in the column `duration_seconds`. **Otherwise**, use
    zero as a value*.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**当**列 `ProgramClass` 的字段**去除**字段开头和结尾的空格**在**我们的商业代码列表中时，则取列 `duration_seconds`
    中字段的值。**否则**，使用零作为值*。'
- en: 'The blueprint of the `F.when()` function is as follows. It is possible to chain
    multiple `when()` if we have more than one condition and to omit the `otherwise()`
    if we’re okay with having `null` values when none of the tests are positive:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`F.when()` 函数的蓝图如下。如果我们有多个条件，我们可以链式调用多个 `when()`，如果我们可以接受在所有测试都不为正时出现 `null`
    值，则可以省略 `otherwise()`：'
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We now have a column ready to use. While we could create the column before grouping
    by, using `withColumn()`, let’s take it up a notch and use our definition in the
    `agg()` clause directly. The following listing does just that, and at the same
    time, gives us our answer!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个可以使用的列。虽然我们可以在分组之前使用 `withColumn()` 创建这个列，但让我们更进一步，直接在 `agg()` 子句中使用我们的定义。下面的列表就是这样做的，同时，它也给出了我们的答案！
- en: Listing 5.14 Using our new column into `agg()` to compute our final answer
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.14 将我们的新列用于 `agg()` 来计算最终答案
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '❶ A column is a column: our F.when() function returns a column object that
    can be used in F.sum().'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列就是列：我们的 F.when() 函数返回一个可以用于 F.sum() 的列对象。
- en: 'Wait a moment—the commercial ratio of some channels is 1.0; are some channels
    *only commercials*? If we look at the total duration, we can see that some channels
    don’t broadcast much. Since one day is 86,400 seconds (24 × 60 × 60), we see that
    `HPITV` only has 403 seconds of programming in our data frame. I am not too concerned
    about this right now, but we always have the option to `filter()` our way out
    and remove the channels that broadcast very little (see chapter 2). Still, we
    accomplished our goal: we identified the channels with the most commercials. We
    finish this chapter with one last task: processing those `null` values.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下——一些频道的商业比率为 1.0；是否有些频道*只播放广告*？如果我们看总时长，我们可以看到一些频道播出的内容不多。由于一天有 86,400 秒（24
    × 60 × 60），我们看到 `HPITV` 在我们的数据框中只有 403 秒的节目。我现在不太关心这一点，但我们始终有选择 `filter()` 我们的方式出去并删除播出很少的频道（见第
    2 章）。尽管如此，我们达成了目标：我们确定了广告最多的频道。我们以处理那些 `null` 值的最后任务结束这一章。
- en: '5.3 Taking care of null values: Drop and fill'
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 处理空值：删除和填充
- en: '`null` values represent the absence of value. I think this is a great oxymoron:
    a value for no value? Philosophy aside, we have some `null`s in our result set,
    and I would like them gone. This section covers the two easiest ways to deal with
    `null` values in a data frame: you can either `dropna()` the record containing
    them or `fillna()` the `null` with a value. In this section, we explore both options
    to see which is best for our analysis.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`null` 值表示值的缺失。我认为这是一个很好的矛盾：没有值的值？抛开哲学不谈，我们的结果集中有一些 `null`，我希望它们消失。本节介绍了在数据框中处理
    `null` 值的两种最简单的方法：你可以删除包含它们的记录，或者用值填充 `null`。在本节中，我们探讨了这两种选项，看看哪一种最适合我们的分析。'
- en: '5.3.1 Dropping it like it’s hot: Using dropna() to remove records with null
    values'
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 热火朝天般地删除：使用 dropna() 删除包含空值的记录
- en: Our first option is to plainly ignore the records that have `null` values. In
    this section, I cover the different ways to use the `dropna()` method to drop
    records based on the presence of `null` values.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一种选择是简单地忽略具有`null`值的记录。在本节中，我将介绍使用`dropna()`方法根据`null`值的存在来删除记录的不同方式。
- en: '`dropna()` is pretty easy to use. This data frame method takes three parameters:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`dropna()`函数非常容易使用。这个数据帧方法接受三个参数：'
- en: '`how`, which can take the value `any` or `all`. If `any` is selected, PySpark
    will drop records where *at least one* of the fields is `null`. In the case of
    `all`, only the records where all fields are `null` will be removed. By default,
    PySpark will take the `any` mode.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`how`可以取`any`或`all`的值。如果选择`any`，PySpark将删除至少有一个字段是`null`的记录。在`all`的情况下，只有所有字段都是`null`的记录将被删除。默认情况下，PySpark将采用`any`模式。'
- en: '`thresh` takes an integer value. If set (its default is `None`), PySpark will
    ignore the `how` parameter and only drop the records with less than thresh non-`null`
    values.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thresh`接受一个整数值。如果设置（默认值为`None`），PySpark将忽略`how`参数，并且只删除小于`thresh`个非`null`值的记录。'
- en: '`subset` will take an optional list of columns that `dropna()` will use to
    make its decision.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subset`将接受一个可选的列列表，`dropna()`将使用它来做出决定。'
- en: In our case, we want to keep only the records that have a `commercial_ratio`
    and that are non-`null`. We just have to pass our column to the `subset` parameter,
    like in the next listing.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们只想保留具有`commercial_ratio`且非`null`的记录。我们只需将我们的列传递给`subset`参数，就像下一个列表中那样。
- en: Listing 5.15 Dropping only the records that have a `null` `commercial_ratio`
    value
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.15 仅删除具有`null` `commercial_ratio`值的记录
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This option is legitimate, but it removes some records from our data frame.
    What if we want to keep everything? The next section covers how to replace the
    `null` values with something else.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选项是合法的，但它会从我们的数据帧中删除一些记录。如果我们想保留所有内容怎么办？下一节将介绍如何用其他东西替换`null`值。
- en: 5.3.2 Filling values to our heart’s content using fillna()
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 使用`fillna()`随心所欲地填充值
- en: The yin to `dropna()`’s yang is to provide a default value to the `null` values.
    This section covers the `fillna()` method to replace `null` values.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`dropna()`的阴对应于提供默认值给`null`值。本节介绍了`fillna()`方法来替换`null`值。'
- en: '`fillna()` is even simpler than `dropna()`. This data frame method takes two
    parameters:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`fillna()`甚至比`dropna()`更简单。这个数据帧方法接受两个参数：'
- en: The `value`, which is a Python int, float, string, or bool. PySpark will only
    fill the compatible columns; for instance, if we were to `fillna("zero")`, our
    `commercial_ratio`, being a double, would not be filled.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value`是一个Python整型、浮点型、字符串或布尔型。PySpark只会填充兼容的列；例如，如果我们使用`fillna("zero")`，由于`commercial_ratio`是一个双精度浮点数，它将不会被填充。'
- en: The same `subset` parameter we encountered in `dropna()`. We can limit the scope
    of our filling to only the columns we want.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与我们在`dropna()`中遇到的相同的`subset`参数。我们可以限制填充的范围，仅限于我们想要的列。
- en: In concrete terms, a `null` value in any of our numerical columns means that
    the value should be zero, so the next listing fills the `null` values with zero.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们任何数值列中的`null`值意味着该值应该是零，所以下一个列表将使用零填充`null`值。
- en: Listing 5.16 Filling our numerical records with zero using the `fillna()` method
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.16 使用`fillna()`方法用零填充我们的数值记录
- en: '[PRE18]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ We have the two additional records that listing 5.15 dropped.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们有两个额外的记录，其中5.15被删除了。
- en: The return of the dict
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 字典的返回值
- en: 'You can also pass a dict to the `fillna()` method, with the column names as
    key and the values as dict values. If we were to use this method for our filling,
    the code would be like the following code:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将一个字典传递给`fillna()`方法，其中列名作为键，值作为字典值。如果我们使用这种方法进行填充，代码将如下所示：
- en: '[PRE19]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Just like with `agg()`, I prefer avoiding the dict approach because I find it
    less readable. In this case, you can chain multiple `fillna()` to achieve the
    same result with better readability.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`agg()`一样，我更喜欢避免使用字典方法，因为我觉得它不太易读。在这种情况下，你可以链式调用多个`fillna()`以获得更好的可读性并达到相同的结果。
- en: Our program is now devoid of `null` values, and we have a full list of channels
    and their associated ratio of commercial programming. I think it’s time for a
    complete wrap-up of our program and to summarize what we’ve covered in this chapter.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的程序现在没有`null`值，我们有一个完整的频道列表及其相关的商业节目比率。我认为现在是时候全面总结我们的程序，并总结我们在本章中涵盖的内容。
- en: 5.4 What was our question again? Our end-to-end program
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 我们的问题是什么？我们的端到端程序
- en: At the beginning of the chapter, we gave ourselves an anchor question to start
    exploring the data and uncover some insights. Throughout the chapter, we’ve assembled
    a cohesive data set containing the relevant information needed to identify commercial
    programs and ranked the channels based on how much of their programming is commercial.
    In listing 5.17, I’ve assembled all the relevant code blocks introduced in the
    chapter into a single program you can `spark-submit`. The code is also available
    in the book’s repository under `code/Ch05/commercials.py`. The end-of-chapter
    exercises also use this code.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们为自己设定了一个锚定问题，以开始探索数据并揭示一些见解。在整个章节中，我们收集了一个包含识别商业节目所需的相关信息的连贯数据集，并根据它们的节目中有多少是商业的来对频道进行排名。在列表5.17中，我将本章中引入的所有相关代码块组合成一个可以`spark-submit`的单个程序。代码也可在本书的仓库中找到，位于`code/Ch05/commercials.py`目录下。章节末尾的练习也使用了此代码。
- en: Not counting data ingestion, comments, or docstring, our code is a rather small
    hundred or so lines of code. We could play code golf (trying to shrink the number
    of characters as much as we can), but I think we’ve struck a good balance between
    terseness and ease of reading. Once again, we haven’t paid much attention to the
    distributed nature of PySpark. Instead, we took a very descriptive view of our
    problem and translated it into code via PySpark’s powerful data frame abstraction
    and rich function ecosystems.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 不计算数据摄取、注释或docstring，我们的代码是一百行左右的代码。我们可以玩代码高尔夫（尝试尽可能减少字符数），但我认为我们在简洁性和易读性之间找到了一个很好的平衡。再次强调，我们没有过多关注PySpark的分布式特性。相反，我们非常具体地看待我们的问题，并通过PySpark强大的数据框抽象和丰富的函数生态系统将其转化为代码。
- en: This chapter is the last chapter of the first part of the book. You are now
    familiar with the PySpark ecosystem and how you can use its main data structure,
    the data frame, to ingest and manipulate two very common sources of data, textual
    and tabular. You know a variety and method and functions that can be applied to
    data frames and columns, and can apply those to your own data problem. You can
    also leverage the documentation provided through the PySpark docstrings, straight
    from the PySpark shell.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是本书第一部分的最后一章。你现在已经熟悉了PySpark生态系统以及如何使用其主要数据结构——数据框，来摄取和处理两种非常常见的数据源，文本和表格。你了解可以应用于数据框和列的多种方法和函数，并且可以将它们应用于自己的数据问题。你还可以利用PySpark
    docstrings提供的文档，直接从PySpark shell中访问。
- en: There is a lot more you can get from the plain data manipulation portion of
    the book. Because of this, I recommend taking the time to review the PySpark online
    API and become proficient in navigating its structure. Now that you have a solid
    understanding of the data model and how to structure simple data manipulation
    programs, adding new functions to your PySpark quiver will be easy.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本书的数据操作部分获得更多内容。因此，我建议花时间回顾PySpark在线API，并熟练掌握其结构的导航。现在您已经对数据模型和如何构建简单的数据操作程序有了坚实的理解，向您的PySpark工具箱中添加新功能将变得容易。
- en: 'The second part of the book builds heavily on what you’ve learned so far:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 书的第二部分在很大程度上建立在您已经学到的知识之上：
- en: We dig deeper into PySpark’s data model and find opportunities to refine our
    code. We will also look at PySpark’s column types, how they bridge to Python’s
    types, and how to use them to improve the reliability of our code.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们深入PySpark的数据模型，寻找改进代码的机会。我们还将探讨PySpark的列类型，它们如何与Python的类型桥接，以及如何使用它们来提高代码的可靠性。
- en: We go beyond two-dimensional data frames with complex data types, such as the
    array, the map, and the struct, by ingesting hierarchical data.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过摄取层次数据，我们超越了二维数据框，使用复杂的数据类型，如数组、映射和结构。
- en: We look at how PySpark modernizes SQL, an influential language for tabular data
    manipulation, and how you can blend SQL and Python in a single program.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探讨PySpark如何使SQL现代化，SQL是一种用于表格数据操作的有影响力的语言，以及如何在单个程序中结合SQL和Python。
- en: We look at promoting pure Python code to run in the Spark-distributed environment.
    We formally introduce a lower-level structure, the resilient distributed dataset
    (RDD) and its row-major model. We also look at UDFs and pandas UDFs as a way to
    augment the functionality of the data frame.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探讨将纯Python代码提升到在Spark分布式环境中运行。我们正式引入一个较低级别的结构，即弹性分布式数据集（RDD）及其行主模型。我们还探讨了UDFs和pandas
    UDFs作为增强数据框功能的一种方式。
- en: Listing 5.17 Our full program, ordering channels by decreasing proportion of
    commercials
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.17 我们完整的程序，按商业节目比例降序排列频道
- en: '[PRE20]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'PySpark implements seven join functionalities, using the common “what?,” “on
    what?,” and “how?” questions: cross, inner, left, right, full, left semi and left
    anti. Choosing the appropriate join method depends on how to process the records
    that resolve the predicates and those that do not.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 实现了七种连接功能，使用常见的“是什么？”，“基于什么？”和“如何？”问题：交叉，内部，左，右，全，左半和左反。选择适当的连接方法取决于如何处理解决谓词的记录以及未解决谓词的记录。
- en: PySpark keeps lineage information when joining data frames. Using this information,
    we can avoid column naming clashes.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 在连接数据帧时保留 lineage 信息。利用这些信息，我们可以避免列命名冲突。
- en: You can group similar values using the `groupby()` method on a data frame. The
    method takes a number of column objects or strings representing columns and returns
    a `GroupedData` object.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用数据帧上的 `groupby()` 方法对相似值进行分组。该方法接受多个列对象或表示列的字符串，并返回一个 `GroupedData` 对象。
- en: '`GroupedData` objects are transitional structures. They contain two types of
    columns: the *key* columns, which are the one you “grouped by” with, and the group
    cell, which is a container for all the other columns. The most common way to return
    to a data frame is to summarize the values in the column via the `agg()` function
    or via one of the direct aggregation methods, such as `count()` or `min()`.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GroupedData` 对象是过渡结构。它们包含两种类型的列：*键*列，这是你“按此分组”的列，以及组单元格，它是一个包含所有其他列的容器。返回数据帧的最常见方式是通过
    `agg()` 函数或通过直接聚合方法（如 `count()` 或 `min()`）对列中的值进行汇总。'
- en: You can drop records containing `null` values using `dropna()` or replace them
    with another value with the `fillna()` method.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用 `dropna()` 删除包含 `null` 值的记录，或者使用 `fillna()` 方法用另一个值替换它们。
- en: Additional exercises
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外练习
- en: Exercise 5.4
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.4
- en: 'Write PySpark code that will return the result of the following code block
    without using a left anti-join:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 编写 PySpark 代码，在不使用左反连接的情况下返回以下代码块的结果：
- en: '[PRE21]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Exercise 5.5
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.5
- en: 'Using the data from the `data/broadcast_logs/Call_Signs.csv` (careful: the
    delimiter here is the comma, not the pipe!), add the `Undertaking_Name` to our
    final table to display a human-readable description of the channel.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `data/broadcast_logs/Call_Signs.csv`（注意：这里的分隔符是逗号，而不是管道！）中的数据，将 `Undertaking_Name`
    添加到我们的最终表中，以显示频道的可读描述。
- en: Exercise 5.6
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.6
- en: The government of Canada is asking for your analysis, but they’d like the `PRC`
    to be weighted differently. They’d like each `PRC` second to be considered 0.75
    commercial seconds. Modify the program to account for this change.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大政府要求你进行分析，但他们希望对 `PRC` 进行不同的加权。他们希望每个 `PRC` 第二被考虑为 0.75 个商业秒。修改程序以考虑这一变化。
- en: Exercise 5.7
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.7
- en: 'On the data frame returned from `commercials.py`, return the number of channels
    in each bucket based on their `commercial_ratio`. (Hint: look at the documentation
    for `round` on how to round a value.)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `commercials.py` 返回的数据帧上，根据它们的 `commercial_ratio` 返回每个桶中的频道数。（提示：查看 `round`
    的文档了解如何四舍五入值。）
- en: '| commercial_ratio | number_of_channels |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| commercial_ratio | number_of_channels |'
- en: '| 1.0 |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 |  |'
- en: '| 0.9 |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 |  |'
- en: '| 0.8 |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 0.8 |  |'
- en: '| ... |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| ... |  |'
- en: '| 0.1 |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 |  |'
- en: '| 0.0 |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 0.0 |  |'
