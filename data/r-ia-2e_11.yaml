- en: 8 Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 回归
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Fitting and interpreting linear models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和解释线性模型
- en: Evaluating model assumptions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型假设
- en: Selecting among competing models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在竞争模型中选择
- en: In many ways, regression analysis lives at the heart of statistics. It’s a broad
    term for a set of methodologies used to predict a response variable (also called
    a dependent, criterion, or outcome variable) from one or more predictor variables
    (also called independent or explanatory variables). In general, regression analysis
    can be used to identify the explanatory variables that are related to a response
    variable, to describe the form of the relationships involved, and to provide an
    equation for predicting the response variable from the explanatory variables.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，回归分析是统计学的核心。这是一个广泛的术语，用于描述一组用于从一个或多个预测变量（也称为独立或解释变量）预测响应变量（也称为依赖、标准或结果变量）的方法。一般来说，回归分析可以用来识别与响应变量相关的解释变量，描述涉及的关系形式，并提供一个从解释变量预测响应变量的方程。
- en: For example, an exercise physiologist might use regression analysis to develop
    an equation for predicting the expected number of calories a person will burn
    while exercising on a treadmill. The response variable is the number of calories
    burned (calculated from the amount of oxygen consumed), and the predictor variables
    might include duration of exercise (minutes), percentage of time spent at their
    target heart rate, average speed (mph), age (years), gender, and body mass index
    (BMI).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一位运动生理学家可能会使用回归分析来开发一个预测人在跑步机上锻炼时预期燃烧的卡路里数量的方程。响应变量是燃烧的卡路里（从消耗的氧气量计算得出），预测变量可能包括锻炼持续时间（分钟）、在目标心率下花费的时间百分比、平均速度（英里/小时）、年龄（年）、性别和体质指数（BMI）。
- en: 'From a theoretical point of view, the analysis will help answer such questions
    as these:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论的角度来看，分析将有助于回答以下问题：
- en: What’s the relationship between exercise duration and calories burned? Is it
    linear or curvilinear? For example, does exercise have less impact on the number
    of calories burned after a certain point?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锻炼持续时间与燃烧的卡路里之间有什么关系？它是线性的还是曲线的？例如，在某个点之后，锻炼对燃烧的卡路里的影响是否会减少？
- en: How does effort (the percentage of time at the target heart rate, the average
    walking speed) factor in?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 努力（在目标心率的时间百分比，平均行走速度）如何影响？
- en: Are these relationships the same for young and old, male and female, heavy and
    slim?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些关系对年轻人和老年人、男性和女性、胖人和瘦人是否相同？
- en: 'From a practical point of view, the analysis will help answer such questions
    as these:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际的角度来看，分析将有助于回答以下问题：
- en: How many calories can a 30-year-old man with a BMI of 28.7 expect to burn if
    he walks for 45 minutes at an average speed of 4 miles per hour and stays within
    his target heart rate 80% of the time?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个BMI为28.7的30岁男性，如果他以每小时4英里的平均速度行走45分钟，并且80%的时间保持在目标心率范围内，他可以预期燃烧多少卡路里？
- en: What’s the minimum number of variables you need to collect to accurately predict
    the number of calories a person will burn while walking?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要收集多少变量才能准确预测一个人在行走时燃烧的卡路里数量？
- en: How accurate will your prediction tend to be?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的预测将有多准确？
- en: Because regression analysis plays such a central role in modern statistics,
    we’ll cover it in some depth in this chapter. First, we’ll look at how to fit
    and interpret regression models. Next, we’ll review a set of techniques for identifying
    potential problems with these models and how to deal with them. Third, we’ll explore
    the issue of variable selection. Of all the potential predictor variables available,
    how do you decide which ones to include in your final model? Fourth, we’ll address
    the question of generalizability. How well will your model work when you apply
    it in the real world? Finally, we’ll consider relative importance. Of all the
    predictors in your model, which is the most important, the second most important,
    and the least important?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于回归分析在现代统计学中扮演着如此核心的角色，我们将在本章中对其进行深入探讨。首先，我们将探讨如何配置和解释回归模型。接下来，我们将回顾一系列用于识别这些模型潜在问题及其解决方法的技术。第三，我们将探讨变量选择的问题。在所有潜在的预测变量中，你如何决定哪些变量应该包含在你的最终模型中？第四，我们将讨论一般化的问题。当你将模型应用于现实世界时，它将如何表现？最后，我们将考虑相对重要性。在你的模型中，哪个预测变量最重要，第二个最重要的，以及最不重要的？
- en: As you can see, we’re covering a lot of ground. Effective regression analysis
    is an interactive, holistic process with many steps, and it involves more than
    a little skill. Rather than break it up into multiple chapters, I’ve opted to
    present this topic in a single chapter to capture this flavor. As a result, this
    will be the longest and most involved chapter in the book. Stick with it to the
    end, and you’ll have all the tools you need to tackle a wide variety of research
    questions. I promise!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们覆盖了大量的内容。有效的回归分析是一个交互式、整体的过程，包含许多步骤，并且需要相当多的技巧。而不是将其拆分成多个章节，我选择在一个章节中呈现这个主题，以捕捉这种风味。因此，这将是有史以来最长和最复杂的章节。坚持到最后，您将拥有解决各种研究问题所需的所有工具。我保证！
- en: 8.1 The many faces of regression
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 回归的多种面貌
- en: The term *regression* can be confusing because there are so many specialized
    varieties (see table 8.1). In addition, R has powerful and comprehensive features
    for fitting regression models, and the abundance of options can be confusing.
    For example, in 2005, Vito Ricci created a list of more than 205 functions in
    R that are used to generate regression analyses ([http://mng.bz/NJhu](http://mng.bz/NJhu)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“回归”可能会令人困惑，因为存在许多专门的品种（见表8.1）。此外，R具有强大的全面功能来拟合回归模型，众多的选项可能会令人困惑。例如，在2005年，Vito
    Ricci创建了一个包含超过205个R函数的列表，这些函数用于生成回归分析（[http://mng.bz/NJhu](http://mng.bz/NJhu)）。
- en: Table 8.1 Varieties of regression analysis
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 回归分析种类
- en: '| Type of regression | Typical use |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 回归类型 | 典型用途 |'
- en: '| Simple linear | Predicting a quantitative response variable from a quantitative
    explanatory variable |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 简单线性 | 从一个定量解释变量预测定量响应变量 |'
- en: '| Polynomial | Predicting a quantitative response variable from a quantitative
    explanatory variable, where the relationship is modeled as an nth order polynomial
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 多项式 | 从一个定量解释变量预测定量响应变量，其中关系被建模为n次多项式 |'
- en: '| Multiple linear | Predicting a quantitative response variable from two or
    more explanatory variables |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 多元线性 | 从两个或更多解释变量预测定量响应变量 |'
- en: '| Multilevel | Predicting a response variable from data that have a hierarchical
    structure (for example, students within classrooms within schools). Also called
    hierarchical, nested, or mixed models. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 多层 | 从具有层次结构的数据预测响应变量（例如，学校内的班级内的学生）。也称为层次、嵌套或混合模型。 |'
- en: '| Multivariate | Predicting more than one response variable from one or more
    explanatory variables |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 多变量 | 从一个或多个解释变量预测多个响应变量 |'
- en: '| Logistic | Predicting a categorical response variable from one or more explanatory
    variables |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑 | 从一个或多个解释变量预测分类响应变量 |'
- en: '| Poisson | Predicting a response variable representing counts from one or
    more explanatory variables |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 泊松 | 从一个或多个解释变量预测表示计数的响应变量 |'
- en: '| Cox proportional hazards | Predicting time to an event (death, failure, relapse)
    from one or more explanatory variables |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Cox比例风险 | 从一个或多个解释变量预测事件发生的时间（死亡、故障、复发） |'
- en: '| Time-series | Modeling time-series data with correlated errors |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 时间序列 | 使用相关误差对时间序列数据进行建模 |'
- en: '| Nonlinear | Predicting a quantitative response variable from one or more
    explanatory variables, where the form of the model is nonlinear |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 非线性 | 从一个或多个解释变量预测定量响应变量，其中模型的形式是非线性的 |'
- en: '| Nonparametric | Predicting a quantitative response variable from one or more
    explanatory variables, where the form of the model is derived from the data and
    not specified a priori |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 非参数 | 从一个或多个解释变量预测定量响应变量，其中模型的形式是从数据中推导出来的，而不是事先指定的 |'
- en: '| Robust | Predicting a quantitative response variable from one or more explanatory
    variables using an approach that’s resistant to the effect of influential observations
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 鲁棒 | 使用一种对有影响观测值效应具有抵抗力的方法从一个或多个解释变量预测定量响应变量 |'
- en: In this chapter, we’ll focus on regression methods that fall under the rubric
    of ordinary least squares (OLS) regression, including simple linear regression,
    polynomial regression, and multiple linear regression. Chapter 13 will cover other
    types of regression models, including logistic regression and Poisson regression.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注属于普通最小二乘法（OLS）回归范畴的回归方法，包括简单线性回归、多项式回归和多元线性回归。第13章将涵盖其他类型的回归模型，包括逻辑回归和泊松回归。
- en: 8.1.1 Scenarios for using OLS regression
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 使用OLS回归的场景
- en: In OLS regression, a quantitative dependent variable is predicted from a weighted
    sum of predictor variables, where the weights are parameters estimated from the
    data. Let’s take a look at a concrete example (no pun intended), loosely adapted
    from Fwa (2006).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在OLS回归中，定量因变量是从预测变量的加权和中预测出来的，其中权重是从数据中估计的参数。让我们看看一个具体的例子（无意中提到），这个例子是从Fwa（2006年）那里松散改编的。
- en: An engineer wants to identify the most important factors related to bridge deterioration
    (such as age, traffic volume, bridge design, construction materials and methods,
    construction quality, and weather conditions) and determine the mathematical form
    of these relationships. She collects data on each of these variables from a representative
    sample of bridges and models the data using OLS regression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师希望识别与桥梁退化（如年龄、交通量、桥梁设计、建筑材料和方法、施工质量和天气条件）相关的最重要因素，并确定这些关系的数学形式。她从代表性桥梁样本中收集了这些变量的数据，并使用OLS回归模型对这些数据进行建模。
- en: The approach is highly interactive. She fits a series of models, checks their
    compliance with underlying statistical assumptions, explores any unexpected or
    aberrant findings, and finally chooses the “best” model from among many possible
    models. If successful, the results will help her to
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法高度互动。她拟合了一系列模型，检查它们是否符合潜在统计假设，探索任何意外或异常发现，并最终从众多可能模型中选择“最佳”模型。如果成功，结果将帮助她
- en: Focus on important variables by determining which of the many collected variables
    are useful in predicting bridge deterioration, along with their relative importance.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过确定众多收集到的变量中哪些对预测桥梁退化有用，以及它们的相对重要性，来关注重要变量。
- en: Look for bridges that are likely to be in trouble by providing an equation that
    can be used to predict bridge deterioration for new cases (where the values of
    the predictor variables are known, but the degree of bridge deterioration isn’t).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供一个可以用于预测新案例（预测变量的值已知，但桥梁退化的程度未知）的桥梁退化预测方程，寻找可能陷入麻烦的桥梁。
- en: Take advantage of serendipity by identifying unusual bridges. If she finds that
    some bridges deteriorate much faster or slower than predicted by the model, a
    study of these outliers may yield important findings that could help her understand
    the mechanisms involved in bridge deterioration.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过识别不寻常的桥梁来利用偶然性。如果她发现某些桥梁的退化速度比模型预测的要快或慢得多，对这些异常值的研究可能会得出重要的发现，这有助于她理解桥梁退化的机制。
- en: 'Bridges may hold no interest for you. I’m a clinical psychologist and statistician,
    and I know next to nothing about civil engineering. But the general principles
    apply to an amazingly wide selection of problems in the physical, biological,
    and social sciences. Each of the following questions could also be addressed using
    an OLS approach:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 桥梁可能对你没有吸引力。我是一个临床心理学家和统计学家，我对土木工程知之甚少。但一般原则适用于物理、生物和社会科学中惊人的广泛问题。以下每个问题也可以使用OLS方法来解决：
- en: What’s the relationship between surface stream salinity and paved road surface
    area (Montgomery, 2007)?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地表径流盐度和铺砌道路表面积之间的关系是什么？（Montgomery，2007年）
- en: What aspects of a user’s experience contribute to the overuse of massively multiplayer
    online role-playing games (MMORPGs) (Hsu, Wen, and Wu, 2009)?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户体验的哪些方面导致了大型多人在线角色扮演游戏（MMORPGs）的过度使用？（Hsu，Wen和Wu，2009年）
- en: Which qualities of an educational environment are most strongly related to higher
    student achievement scores?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些教育环境的质量与更高的学生成绩分数最密切相关？
- en: What’s the form of the relationship between blood pressure, salt intake, and
    age? Is it the same for men and women?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 血压、盐摄入量和年龄之间的关系形式是什么？这对男性和女性是否相同？
- en: What’s the impact of stadiums and professional sports on metropolitan area development
    (Baade and Dye, 1990)?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体育场和专业运动对大都市区发展的影响是什么？（Baade和Dye，1990年）
- en: What factors account for interstate differences in the price of beer (Culbertson
    and Bradford, 1991)? (That one got your attention!)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么因素导致了州际间啤酒价格的差异？（Culbertson和Bradford，1991年）（这个问题引起了你的注意！）
- en: Our primary limitation is our ability to formulate an interesting question,
    devise a useful response variable to measure, and gather appropriate data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要限制是我们提出有趣问题、设计有用的响应变量来衡量以及收集适当数据的能力。
- en: 8.1.2 What you need to know
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 你需要知道什么
- en: For the remainder of this chapter, I’ll describe how to use R functions to fit
    OLS regression models, evaluate the fit, test assumptions, and select among competing
    models. I assume you’ve had exposure to least squares regression as typically
    taught in a second-semester undergraduate statistics course. But I’ve made efforts
    to keep the mathematical notation to a minimum and focus on practical rather than
    theoretical issues. Several excellent texts cover the statistical material outlined
    in this chapter. My favorites are John Fox’s *Applied Regression Analysis and
    Generalized Linear Models* (2008) (for theory) and *An R and S-Plus Companion
    to Applied Regression* (2002) (for application). They both served as major sources
    for this chapter. Licht (1995) provides a good nontechnical overview.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我将描述如何使用 R 函数来拟合 OLS 回归模型，评估拟合度，检验假设，并在竞争模型之间进行选择。我假设你已经接触过通常在大学二年级统计学课程中教授的最小二乘回归。但我已经努力将数学符号保持在最低限度，并关注实际问题而不是理论问题。有几本优秀的教材涵盖了本章概述的统计材料。我最喜欢的是
    John Fox 的 *应用回归分析和广义线性模型*（2008 年版）（用于理论）和 *R 和 S-Plus 应用回归指南*（2002 年版）（用于应用）。它们都为本章的主要来源。Licht（1995
    年）提供了一个很好的非技术性概述。
- en: 8.2 OLS regression
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 OLS 回归
- en: 'For most of this chapter, we’ll be predicting the response variable from a
    set of predictor variables (also called *regressing* the response variable on
    the predictor variables—hence the name) using OLS. OLS regression fits models
    of the form where *n* is the number of observations and *`k`* is the number of
    predictor variables. (Although I’ve tried to keep equations out of these discussions,
    this is one of the few places where it simplifies things.) In this equation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的大部分内容中，我们将使用 OLS 从一组预测变量（也称为将响应变量回归到预测变量——因此得名）预测响应变量。OLS 回归拟合形式为，其中 *n*
    是观测数，*`k`* 是预测变量的数量。（尽管我已经尽力将方程式排除在这些讨论之外，但这确实是简化事情的地方之一。）在这个方程中：
- en: '![](Images/CH08_F00_Kabacoff3-EQ01.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F00_Kabacoff3-EQ01.png)'
- en: '![](Images/CH08_F00_Kabacoff3-EQ02.png)is the predicted value of the dependent
    variable for observation *i* (specifically, it’s the estimated mean of the *Y*
    distribution, conditional on the set of predictor values).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](Images/CH08_F00_Kabacoff3-EQ02.png)是第 *i* 个观测值的因变量预测值（具体来说，它是基于预测值集合的 *Y*
    分布的估计均值）。'
- en: '![](Images/CH08_F00_Kabacoff3-EQ03.png)is the *j*th predictor value for the
    *i*th observation.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](Images/CH08_F00_Kabacoff3-EQ03.png)是第 *i* 个观测值的第 *j* 个预测值。'
- en: '![](Images/CH08_F00_Kabacoff3-EQ04.png)is the intercept (the predicted value
    of *Y* when all the predictor variables equal zero).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](Images/CH08_F00_Kabacoff3-EQ04.png)是截距（当所有预测变量都等于零时 *Y* 的预测值）。'
- en: '![](Images/CH08_F00_Kabacoff3-EQ05.png)is the regression coefficient for the
    *j*th predictor (slope representing the change in *Y* for a unit change in *X[j]*).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](Images/CH08_F00_Kabacoff3-EQ05.png)是第 *j* 个预测值的回归系数（表示 *X[j]* 单位变化时 *Y*
    的变化）。'
- en: 'Our goal is to select model parameters (intercept and slopes) that minimize
    the difference between actual response values and those predicted by the model.
    Specifically, model parameters are selected to minimize the sum of squared residuals:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是选择模型参数（截距和斜率）以最小化实际响应值与模型预测值之间的差异。具体来说，模型参数的选择是为了最小化残差平方和：
- en: '![](Images/CH08_F00_Kabacoff3-EQ06.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F00_Kabacoff3-EQ06.png)'
- en: 'To properly interpret the coefficients of the OLS model, you must satisfy a
    number of statistical assumptions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正确解释 OLS 模型的系数，你必须满足一系列统计假设：
- en: '*Normality*—For fixed values of the independent variables, the dependent variable
    is normally distributed.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正态性*—对于固定的自变量值，因变量呈正态分布。'
- en: '*Independence*—The *Y[i]* values are independent of each other.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*独立性*—*Y[i]* 值彼此独立。'
- en: '*Linearity*—The dependent variable is linearly related to the independent variables.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性关系*—因变量与自变量呈线性关系。'
- en: '*Homoscedasticity*—The variance of the dependent variable doesn’t vary with
    the levels of the independent variables. (I could call this constant variance,
    but saying homoscedasticity makes me feel smarter.)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同方差性*—因变量的方差不随独立变量水平的改变而改变。（我本可以称之为常数方差，但使用同方差性这个词让我感觉更聪明。）'
- en: If you violate these assumptions, your statistical significance tests and confidence
    intervals may not be accurate. Note that OLS regression also assumes that the
    independent variables are fixed and measured without error, but this assumption
    is typically relaxed in practice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果违反这些假设，你的统计显著性检验和置信区间可能不准确。请注意，OLS 回归还假设独立变量是固定的且无误差测量，但在实践中通常放宽此假设。
- en: 8.2.1 Fitting regression models with lm()
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 使用 lm() 拟合回归模型
- en: In R, the basic function for fitting a linear model is `lm()`. The format is
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中，拟合线性模型的基本函数是 `lm()`。其格式为
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where *`formula`* describes the model to be fit and *`data`* is the data frame
    containing the data to be used in fitting the model. The resulting object (`myfit`,
    in this case) is a list that contains extensive information about the fitted model.
    The formula is typically written as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *`formula`* 描述要拟合的模型，而 *`data`* 是包含用于拟合模型的数据的数据框。结果对象（在本例中为 `myfit`）是一个包含有关拟合模型大量信息的列表。公式通常写作
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where the ~ separates the response variable on the left from the predictor variables
    on the right, and the predictor variables are separated by + signs. Other symbols
    can be used to modify the formula in various ways (see table 8.2).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ~ 将左侧的响应变量与右侧的预测变量分开，预测变量由加号分隔。其他符号可以以各种方式修改公式（见表 8.2）。
- en: Table 8.2 Symbols commonly used in R formulas
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2 R 公式中常用符号
- en: '| Symbol | Usage |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 用法 |'
- en: '| `~` | Separates response variables on the left from the explanatory variables
    on the right. For example, a prediction of `y` from `x`, `z`, and w would be coded
    `y` `~` `x` `+` `z` `+` `w`. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `~` | 将左侧的响应变量与右侧的解释变量分开。例如，从 `x`、`z` 和 `w` 预测 `y` 的代码为 `y` `~` `x` `+` `z`
    `+` `w`。|'
- en: '| `+` | Separates predictor variables |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `+` | 分隔预测变量 |'
- en: '| `:` | Denotes an interaction between predictor variables. A prediction of
    `y` from `x`, `z`, and the interaction between `x` and `z` would be coded `y`
    `~` `x` `+` `z` `+` `x:z`. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `:` | 表示预测变量之间的交互。从 `x`、`z` 和 `x` 与 `z` 之间的交互预测 `y` 的代码为 `y` `~` `x` `+`
    `z` `+` `x:z`。|'
- en: '| `*` | A shortcut for denoting all possible interactions. The code `y` `~`
    `x` `*` `z` `*` w expands to `y` `~` `x` `+` `z` `+` `w` `+` `x:z` `+` `x:w` `+`
    `z:w` `+` `x:z:w`. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `*` | 表示所有可能交互的快捷方式。代码 `y` `~` `x` `*` `z` `*` `w` 展开为 `y` `~` `x` `+` `z`
    `+` `w` `+` `x:z` `+` `x:w` `+` `z:w` `+` `x:z:w`。|'
- en: '| `^` | Denotes interactions up to a specified degree. The code `y` `~` `(x`
    `+` `z` `+` `w)^2` expands to `y` `~` `x` `+` `z` `+` `w` `+` `x:z` `+` `x:w`
    `+` `z:w`. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `^` | 表示直到指定程度的交互。代码 `y` `~` `(x` `+` `z` `+` `w)^2` 展开为 `y` `~` `x` `+`
    `z` `+` `w` `+` `x:z` `+` `x:w` `+` `z:w`。|'
- en: '| `.` | A placeholder for all other variables in the data frame except the
    dependent variable. For example, if a data frame contained the variables `x`,
    `y`, `z`, and `w`, then the code `y ~`. would expand to `y` `~` `x` `+` `z` `+`
    `w`. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `.` | 数据框中除因变量外的所有其他变量的占位符。例如，如果数据框包含变量 `x`、`y`、`z` 和 `w`，则代码 `y ~` 会展开为
    `y` `~` `x` `+` `z` `+` `w`。|'
- en: '| `-` | A minus sign removes a variable from the equation. For example, `y`
    `~` `(x` `+` `z` `+` `w)^2` `–` `x:w` expands to `y` `~` `x` `+` `z` `+` `w` `+`
    `x:z` `+` `z:w`. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `-` | 减号从方程中删除变量。例如，`y` `~` `(x` `+` `z` `+` `w)^2` `–` `x:w` 展开为 `y` `~`
    `x` `+` `z` `+` `w` `+` `x:z` `+` `z:w`。|'
- en: '| `-1` | Suppresses the intercept. For example, the formula `y` `~` `x` `-1`
    fits a regression of `y` on `x` and forces the line through the origin at `x=0`.
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `-1` | 抑制截距。例如，公式 `y` `~` `x` `-1` 拟合 `y` 对 `x` 的回归，并强制直线通过原点 `x=0`。|'
- en: '| `I()` | Elements within the parentheses are interpreted arithmetically. For
    example, `y` `~` `x` `+` `(z +` `w)^2` expands to `y` `~` `x` `+` `z` `+` `w`
    `+` `z:w`. In contrast, the code `y` `~` `x` `+` `I((z` `+` `w)^2)` expands to
    `y` `~` `x` `+` `h`, where h is a new variable created by squaring the sum of
    `z` and `w`. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `I()` | 括号内的元素按算术方式解释。例如，`y` `~` `x` `+` `(z +` `w)^2` 展开为 `y` `~` `x` `+`
    `z` `+` `w` `+` `z:w`。相比之下，代码 `y` `~` `x` `+` `I((z` `+` `w)^2)` 展开为 `y` `~` `x`
    `+` `h`，其中 h 是通过平方 `z` 和 `w` 的和创建的新变量。|'
- en: '| `function` | Mathematical functions can be used in formulas. For example,
    `log(y)` `~` `x` `+` `z` `+` `w` predicts `log(y)` from `x`, `z`, and `w`. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `function` | 公式中可以使用数学函数。例如，`log(y)` `~` `x` `+` `z` `+` `w` 从 `x`、`z` 和
    `w` 预测 `log(y)`。|'
- en: In addition to `lm()`, table 8.3 lists several functions that are useful when
    generating a simple or multiple regression analysis. Each of these functions is
    applied to the object returned by `lm()`to generate additional information based
    on that fitted model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`lm()`之外，表8.3还列出了在生成简单或多元回归分析时有用的几个函数。这些函数中的每一个都是应用于`lm()`返回的对象，以基于该拟合模型生成更多信息。
- en: Table 8.3 Other functions that are useful when fitting linear models
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.3 在拟合线性模型时有用的其他函数
- en: '| Function | Action |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 操作 |'
- en: '| `summary()` | Displays detailed results for the fitted model |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `summary()` | 显示拟合模型的详细结果 |'
- en: '| `coefficients()` | Lists the model parameters (intercept and slopes) for
    the fitted model |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `coefficients()` | 列出拟合模型的模型参数（截距和斜率） |'
- en: '| `confint()` | Provides confidence intervals for the model parameters (95%
    by default) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `confint()` | 提供模型参数的置信区间（默认为95%） |'
- en: '| `fitted()` | Lists the predicted values in a fitted model |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `fitted()` | 列出拟合模型中的预测值 |'
- en: '| `residuals()` | Lists the residual values in a fitted model |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `residuals()` | 列出拟合模型中的残差值 |'
- en: '| `anova()` | Generates an ANOVA table for a fitted model or an ANOVA table
    comparing two or more fitted models |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `anova()` | 为拟合模型生成ANOVA表或比较两个或多个拟合模型的ANOVA表 |'
- en: '| `vcov()` | Lists the covariance matrix for model parameters |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `vcov()` | 列出模型参数的协方差矩阵 |'
- en: '| `AIC()` | Prints Akaike’s Information Criterion |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `AIC()` | 打印赤池信息准则 |'
- en: '| `plot()` | Generates diagnostic plots for evaluating the fit of a model |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `plot()` | 生成用于评估模型拟合的诊断图 |'
- en: '| `predict()` | Uses a fitted model to predict response values for a new dataset
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `predict()` | 使用拟合模型对新数据集的响应值进行预测 |'
- en: When the regression model contains one dependent variable and one independent
    variable, the approach is called *simple linear regression*. When there’s one
    predictor variable but powers of the variable are included (for example, `X`,
    `X²`, `X³`), it’s called *polynomial regression*. When there’s more than one predictor
    variable, it’s called multiple linear regression. We’ll start with an example
    of simple linear regression, progress to examples of polynomial and multiple linear
    regression, and end with an example of multiple regression that includes an interaction
    among the predictors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当回归模型包含一个因变量和一个自变量时，这种方法被称为*简单线性回归*。当只有一个预测变量但包含变量的幂（例如，`X`，`X²`，`X³`）时，它被称为*多项式回归*。当有多个预测变量时，它被称为多元线性回归。我们将从一个简单线性回归的例子开始，然后过渡到多项式和多元线性回归的例子，最后以一个包含预测变量之间交互作用的多元回归例子结束。
- en: 8.2.2 Simple linear regression
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 简单线性回归
- en: Let’s look at the functions in table 8.3 through a simple regression example.
    The dataset `women` in the base installation provides the height and weight for
    a set of 15 women ages 30 to 39\. Suppose you want to predict weight from height.
    Having an equation for predicting weight from height can help you identify overweight
    or underweight individuals. The following listing provides the analysis, and figure
    8.1 shows the resulting graph.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的回归示例来查看表8.3中的函数。基础安装中的`women`数据集提供了15名30至39岁女性的身高和体重。假设你想根据身高预测体重。拥有从身高预测体重的方程可以帮助你识别超重或体重不足的个人。以下列表提供了分析，图8.1显示了结果图。
- en: Listing 8.1 Simple linear regression
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1 简单线性回归
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](Images/CH08_F01_Kabacoff3.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F01_Kabacoff3.png)'
- en: Figure 8.1 Scatter plot with regression line for weight predicted from height
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 从身高预测体重的散点图和回归线
- en: From the output, you see that the prediction equation is
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，你可以看到预测方程是
- en: '*weight* = −87.52 + 3.45×*height*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*体重* = −87.52 + 3.45×*身高*'
- en: Because a height of 0 is impossible, you wouldn’t try to give a physical interpretation
    to the intercept. It merely becomes an adjustment constant. From the `Pr(>|t|)`
    column, you see that the regression coefficient (3.45) is significantly different
    from zero (p < 0.001) and indicates that there’s an expected increase of 3.45
    pounds of weight for every 1 inch increase in height. The multiple R-squared (0.991)
    indicates that the model accounts for 99.1% of the variance in weights. The multiple
    R-squared is also the squared correlation between the actual and predicted value
    (that is, *R*² = *r[ŷv]*). The residual standard error (1.53 pounds) can be thought
    of as the average error in predicting weight from height using this model. The
    F-statistic tests whether the predictor variables, taken together, predict the
    response variable above chance levels. Because there’s only one predictor variable
    in simple regression, in this example, the F-test is equivalent to the t-test
    for the regression coefficient for height.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于身高为 0 是不可能的，你不会尝试对截距给出物理解释。它仅仅成为一个调整常数。从 `Pr(>|t|)` 列中，你可以看到回归系数（3.45）与零显著不同（p
    < 0.001），这表明每增加 1 英寸身高，预期体重将增加 3.45 磅。多重 R 平方（0.991）表明该模型解释了体重变化的 99.1%。多重 R 平方也是实际值和预测值之间的相关系数的平方（即
    *R*² = *r[ŷv]*）。残差标准误差（1.53 磅）可以被视为使用此模型从身高预测体重的平均误差。F 统计量测试预测变量是否一起预测响应变量高于随机水平。由于简单回归中只有一个预测变量，在这个例子中，F
    测试等同于对高度回归系数的 t 测试。
- en: For demonstration purposes, we’ve printed out the actual, predicted, and residual
    values. Evidently, the largest residuals occur for low and high heights, which
    can also be seen in the plot (figure 8.1).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们打印出了实际值、预测值和残差值。显然，最大残差出现在身高低和高的地方，这也可以在图中（图 8.1）看到。
- en: The plot suggests that you might be able to improve on the prediction by using
    a line with one bend. For example, a model of the form
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图表明，你可能可以通过使用一条有弯曲的线来提高预测。例如，形式为
- en: '![](Images/CH08_F01_Kabacoff3-EQ09.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F01_Kabacoff3-EQ09.png)'
- en: may provide a better fit to the data. Polynomial regression allows you to predict
    a response variable from an explanatory variable, where the form of the relationship
    is an *n*th-degree polynomial.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会更好地拟合数据。多项式回归允许你从一个解释变量预测响应变量，其中关系的形式是 *n* 次多项式。
- en: 8.2.3 Polynomial regression
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 多项式回归
- en: The plot in figure 8.1 suggests that you might be able to improve your prediction
    using a regression with a quadratic term (that is, *X*²). You can fit a quadratic
    equation using the statement
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 中的图表明，你可能可以通过使用带有二次项的回归（即 *X*²）来提高你的预测。你可以使用以下语句拟合二次方程
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The new term `I(height^2)` requires explanation. `height^2` adds a height-squared
    term to the prediction equation. The `I()` `function` treats the contents within
    the parentheses as an R expression. You need this because the `^` operator has
    a special meaning in formulas that you don’t want to invoke here (see table 8.2).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 新术语 `I(height^2)` 需要解释。`height^2` 将一个高度平方项添加到预测方程中。`I()` 函数将括号内的内容视为一个 R 表达式。你需要这样做，因为
    `^` 运算符在公式中有特殊含义，你不想在这里调用它（见表 8.2）。
- en: The following listing shows the results of fitting the quadratic equation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了拟合二次方程的结果。
- en: Listing 8.2 Polynomial regression
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 多项式回归
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From this new analysis, the prediction equation is
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个新的分析中，预测方程是
- en: '*weight* = 261.88 − 7.35×*height* + 0.083×*height*²'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*weight* = 261.88 − 7.35×*height* + 0.083×*height*²'
- en: and both regression coefficients are significant at the p < 0.0001 level. The
    amount of variance accounted for has increased to 99.9%. The significance of the
    squared term (t = 13.89, p < .001) suggests that inclusion of the quadratic term
    improves the model fit. If you look at the plot of `fit2` (figure 8.2), you can
    see that the curve does indeed provide a better fit.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 并且两个回归系数在 p < 0.0001 的水平上都是显著的。解释的方差量增加到了 99.9%。平方项的显著性（t = 13.89，p < .001）表明包含二次项提高了模型拟合度。如果你看
    `fit2` 的图（图 8.2），你可以看到曲线确实提供了更好的拟合。
- en: '![](Images/CH08_F01_Kabacoff3.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F01_Kabacoff3.png)'
- en: Figure 8.2 Quadratic regression for weight predicted by height
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 根据身高预测的重量二次回归
- en: Linear vs. nonlinear models
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型与非线性模型
- en: Note that this polynomial equation still fits under the rubric of linear regression.
    It’s linear because the equation involves a weighted sum of predictor variables
    (height and height-squared in this case). Even a model such as
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个多项式方程仍然属于线性回归的范畴。它是线性的，因为方程涉及预测变量的加权总和（在这个例子中是身高和身高的平方）。即使是如下模型
- en: '![](Images/CH08_F02_EQSB_01.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F02_EQSB_01.png)'
- en: would be considered a linear model (in terms of the parameters) and fit with
    the formula
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 被认为是参数意义上的线性模型，并使用以下公式进行拟合
- en: '![](Images/CH08_F02_EQSB_02.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F02_EQSB_02.png)'
- en: 'In contrast, here’s an example of a truly nonlinear model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，这里有一个真正非线性模型的例子：
- en: '![](Images/CH08_F02_EQSB_03.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F02_EQSB_03.png)'
- en: Nonlinear models of this form can be fit with the `nls()` function.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式的非线性模型可以用 `nls()` 函数拟合。
- en: In general, an *n*th-degree polynomial produces a curve with *n* – 1 bends.
    To fit a cubic polynomial, you’d use
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个 *n* 次方的多项式会产生一个有 *n* - 1 个弯曲的曲线。要拟合一个三次多项式，你会使用
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Although higher polynomials are possible, I’ve rarely found that terms higher
    than cubic are necessary.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能存在更高次的多项式，但我很少发现超过三次方的项是必要的。
- en: 8.2.4 Multiple linear regression
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.4 多元线性回归
- en: When there’s more than one predictor variable, simple linear regression becomes
    multiple linear regression, and the analysis grows more involved. Technically,
    polynomial regression is a special case of multiple regression. Quadratic regression
    has two predictors (*X* and *X*²), and cubic regression has three predictors (*X*,
    *X*², and *X*³). Let’s look at a more general example.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在多个预测变量时，简单线性回归变为多元线性回归，分析变得更加复杂。从技术上讲，多项式回归是多元回归的一个特例。二次回归有两个预测变量（*X* 和 *X*²），三次回归有三个预测变量（*X*，*X*²
    和 *X*³）。让我们看一个更一般的例子。
- en: We’ll use the `state.x77` dataset in the base package for this example. Suppose
    you want to explore the relationship between a state’s murder rate and other characteristics
    of the state, including population, illiteracy rate, average income, and frost
    levels (mean the number of days below freezing).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用基础包中的 `state.x77` 数据集来演示这个例子。假设你想探索一个州的谋杀率与其他特征之间的关系，包括人口、文盲率、平均收入和霜冻水平（低于冰点的天数平均值）。
- en: 'Because the `lm()` function requires a data frame (and the `state.x77` dataset
    is contained in a matrix), you can simplify your life with the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `lm()` 函数需要一个数据框（并且 `state.x77` 数据集包含在一个矩阵中），你可以用以下代码简化你的生活：
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code creates a data frame called `states` that contains the variables you’re
    interested in. You’ll use this new data frame for the remainder of the chapter.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建了一个名为 `states` 的数据框，其中包含你感兴趣的变量。你将在本章的剩余部分使用这个新的数据框。
- en: A good first step in multiple regression is to examine the relationships among
    the variables two at a time. The bivariate correlations are provided by the `cor()`
    function, and scatter plots are generated from the `scatterplotMatrix()` function
    in the `car` package (see the following listing and figure 8.3).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在多元回归中，一个好的第一步是检查变量之间的关系，一次检查两个变量。双变量相关系数由 `cor()` 函数提供，散点图由 `car` 包中的 `scatterplotMatrix()`
    函数生成（见以下列表和图8.3）。
- en: Listing 8.3 Examining bivariate relationships
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 检查双变量关系
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](Images/CH08_F03_Kabacoff3.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F03_Kabacoff3.png)'
- en: Figure 8.3 Scatter plot matrix of dependent and independent variables for the
    `states` data, including linear and smoothed fits, and marginal distributions
    (kernel density plots and rug plots)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 `states` 数据的因变量和自变量的散点矩阵，包括线性拟合和平滑拟合，以及边缘分布（核密度图和地毯图）
- en: By default, the `scatterplotMatrix()` function provides scatter plots of the
    variables with each other in the off-diagonals and superimposes smoothed (loess)
    and linear fit lines on these plots. The principal diagonal contains density and
    rug plots for each variable. The smoothed lines are suppressed with the argument
    `smooth=FALSE`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`scatterplotMatrix()` 函数在非对角线位置提供变量的散点图，并在这些图上叠加平滑（loess）和线性拟合线。主对角线包含每个变量的密度和地毯图。通过
    `smooth=FALSE` 参数抑制平滑线。
- en: You can see that the murder rate may be bimodal and that each of the predictor
    variables is skewed to some extent. Murder rates rise with population and illiteracy,
    and they fall with higher income levels and frost. At the same time, colder states
    have lower illiteracy rates, lower population, and higher incomes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到谋杀率可能是双峰的，并且每个预测变量都有一定程度上的偏斜。谋杀率随着人口和文盲率的增加而上升，随着收入水平和霜冻的减少而下降。同时，较冷州的文盲率较低，人口较少，收入较高。
- en: Now let’s fit the multiple regression model with the `lm()` function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 `lm()` 函数拟合多元回归模型。
- en: Listing 8.4 Multiple linear regression
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 多元线性回归
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When there’s more than one predictor variable, the regression coefficients indicate
    the increase in the dependent variable for a unit change in a predictor variable,
    holding all other predictor variables constant. For example, the regression coefficient
    for `Illiteracy` is 4.14, suggesting that an increase of 1% in illiteracy is associated
    with a 4.14% increase in the murder rate, controlling for population, income,
    and temperature. The coefficient is significantly different from zero at the p
    < .0001 level. On the other hand, the coefficient for `Frost` isn’t significantly
    different from zero (p = 0.954), suggesting that `Frost` and `Murder` aren’t linearly
    related when controlling for the other predictor variables. Taken together, the
    predictor variables account for 57% of the variance in murder rates across states.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在多个预测变量时，回归系数表示在保持所有其他预测变量不变的情况下，预测变量单位变化导致的因变量增加。例如，`Illiteracy` 的回归系数为 4.14，这意味着文盲率增加
    1% 与谋杀率增加 4.14% 相关，在控制人口、收入和温度的情况下。该系数在 p < .0001 的水平上与零有显著差异。另一方面，`Frost` 的系数与零没有显著差异（p
    = 0.954），这意味着在控制其他预测变量的情况下，`Frost` 和 `Murder` 之间没有线性关系。综合来看，预测变量解释了各州谋杀率变异的 57%。
- en: Up to this point, we’ve assumed that the predictor variables don’t interact.
    In the next section, we’ll consider a case in which they do.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设预测变量之间没有交互作用。在下一节中，我们将考虑它们确实存在交互作用的案例。
- en: 8.2.5 Multiple linear regression with interactions
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.5 带有交互作用的多元线性回归
- en: Some of the most interesting research findings are those involving interactions
    among predictor variables. Consider the automobile data in the `mtcars` data frame.
    Let’s say that you’re interested in the impact of automobile weight and horsepower
    on mileage. You could fit a regression model that includes both predictors, along
    with their interaction, as shown in the next listing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最有趣的研究发现之一涉及预测变量之间的交互作用。考虑 `mtcars` 数据框中的汽车数据。假设您对汽车重量和马力对里程的影响感兴趣。您可以拟合一个包含这两个预测变量及其交互作用的回归模型，如下一列表所示。
- en: Listing 8.5 Multiple linear regression with a significant interaction term
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 带有显著交互项的多元线性回归
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see from the `Pr(>|t|)` column that the interaction between horsepower
    and car weight is significant. What does this mean? A significant interaction
    between two predictor variables tells you that the relationship between one predictor
    and the response variable depends on the level of the other predictor. Here it
    means the relationship between miles per gallon and horsepower varies by car weight.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 `Pr(>|t|)` 列中看到，马力和汽车重量之间的交互作用是显著的。这意味着什么？两个预测变量之间的显著交互作用表明，一个预测变量与响应变量之间的关系取决于另一个预测变量的水平。在这里，这意味着每加仑行驶里程与马力之间的关系取决于汽车重量。
- en: The model for predicting `mpg` is `mpg` = 49.81 – 0.12 × `hp` – 8.22 × `wt`
    + 0.03 × `hp` × `wt`. To interpret the interaction, you can plug in various values
    of `wt` and simplify the equation. For example, you can try the mean of `wt` (3.2)
    and one standard deviation below and above the mean (2.2 and 4.2, respectively).
    For `wt`=2.2, the equation simplifies to `mpg` = 49.81 – 0.12 × `hp` – 8.22 ×
    (2.2) + 0.03 × `hp` × (2.2) = 31.41 – 0.06 × `hp`. For `wt`=3.2, this becomes
    `mpg` = 23.37 – 0.03 × `hp`. Finally, for `wt`=4.2 the equation becomes `mpg`
    = 15.33 – 0.003 × `hp`. You see that as weight increases (2.2, 3.2, 4.2), the
    expected change in `mpg` from a unit increase in `hp` decreases (0.06, 0.03, 0.003).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 预测`mpg`的模型是`mpg` = 49.81 – 0.12 × `hp` – 8.22 × `wt` + 0.03 × `hp` × `wt`。为了解释交互作用，你可以插入不同的`wt`值并简化方程。例如，你可以尝试`wt`的均值（3.2）以及均值上下一个标准差（分别为2.2和4.2）。对于`wt`=2.2，方程简化为`mpg`
    = 49.81 – 0.12 × `hp` – 8.22 × (2.2) + 0.03 × `hp` × (2.2) = 31.41 – 0.06 × `hp`。对于`wt`=3.2，这变为`mpg`
    = 23.37 – 0.03 × `hp`。最后，对于`wt`=4.2，方程变为`mpg` = 15.33 – 0.003 × `hp`。你可以看到，随着重量增加（2.2，3.2，4.2），从单位增加`hp`带来的`mpg`的预期变化减少（0.06，0.03，0.003）。
- en: You can visualize interactions using the `effect()` function in the `effects`
    package. The format is
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`effects`包中的`effect()`函数来可视化交互作用。格式是
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: where *`term`* is the quoted model term to plot, *`mod`* is the fitted model
    returned by `lm()`, and *`xlevels`* is a list specifying the variables to be set
    to constant values and the values to employ. The `multiline=TRUE` option superimposes
    the lines being plotted, and the `lines` option specifies the line type for each
    line (where 1 = solid, 2 = dashed, and 3 = dotted, etc.). For the previous model,
    this becomes
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*`term`*是要绘制的引用模型项，*`mod`*是`lm()`返回的拟合模型，*`xlevels`*是一个指定要设置为常数值的变量及其值的列表。`multiline=TRUE`选项将叠加正在绘制的线，而`lines`选项指定每条线的类型（其中1
    = 实线，2 = 虚线，3 = 点线等）。对于前面的模型，这变为
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Figure 8.4 displays the resulting graph.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4显示了生成的图形。
- en: You can see from this graph that as the weight of the car increases, the relationship
    between horsepower and miles per gallon weakens. For `wt=4.2`, the line is almost
    horizontal, indicating that as `hp` increases, `mpg` doesn’t change.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中你可以看到，随着汽车重量的增加，马力与每加仑英里数之间的关系减弱。对于`wt=4.2`，线几乎水平，这表明随着`hp`的增加，`mpg`不会改变。
- en: Unfortunately, fitting the model is only the first step in the analysis. Once
    you fit a regression model, you need to evaluate whether you’ve met the statistical
    assumptions underlying your approach before you can have confidence in the inferences
    you draw. This is the topic of the next section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，拟合模型只是分析的第一步。一旦你拟合了一个回归模型，在你可以对所得到的推断有信心之前，你需要评估你是否已经满足了你方法背后的统计假设。这是下一节的主题。
- en: '![](Images/CH08_F04_Kabacoff3.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F04_Kabacoff3.png)'
- en: Figure 8.4 Interaction plot for `hp*wt`. This plot displays the relationship
    between `mpg` and `hp` at three values of `wt`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 `hp*wt`的交互作用图。此图显示了在三个`wt`值下`mpg`与`hp`之间的关系。
- en: 8.3 Regression diagnostics
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 回归诊断
- en: In the previous section, you used the `lm()` function to fit an OLS regression
    model and the `summary()` function to obtain the model parameters and summary
    statistics. Unfortunately, nothing in this printout tells you whether the model
    you’ve fit is appropriate. Your confidence in inferences about regression parameters
    depends on the degree to which you’ve met the statistical assumptions of the OLS
    model. Although the `summary()` function in listing 8.4 describes the model, it
    provides no information concerning the degree to which you’ve satisfied the statistical
    assumptions underlying the model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你使用了`lm()`函数来拟合一个OLS回归模型，并使用`summary()`函数来获取模型参数和摘要统计信息。不幸的是，这个打印输出中没有任何东西告诉你你拟合的模型是否合适。你对回归参数推断的信心取决于你满足OLS模型统计假设的程度。尽管列表8.4中的`summary()`函数描述了模型，但它没有提供有关满足模型统计假设程度的信息。
- en: Why is this important? Irregularities in the data or misspecifications of the
    relationships between the predictors and the response variable can lead you to
    settle on a model that’s wildly inaccurate. On the one hand, you may conclude
    that a predictor and a response variable are unrelated when, in fact, they are.
    On the other hand, you may conclude that a predictor and a response variable are
    related when, in fact, they aren’t. You may also end up with a model that makes
    poor predictions when applied in real-world settings, with significant and unnecessary
    error.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这为什么很重要？数据中的不规则性或预测变量与响应变量之间关系的误指定可能导致你选择一个极其不准确的模型。一方面，你可能会得出结论，预测变量和响应变量之间没有关系，而实际上它们是有关系的。另一方面，你可能会得出结论，预测变量和响应变量之间有关系，而实际上它们没有关系。你也可能得到一个在现实世界应用时预测效果不佳的模型，伴随着重大且不必要的误差。
- en: 'Let’s look at the output from the `confint()` function applied to the `states`
    multiple regression problem in section 8.2.4:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看应用于第8.2.4节中`states`多重回归问题的`confint()`函数的输出：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The results suggest that you can be 95% confident that the interval [2.38, 5.90]
    contains the true change in murder rate for a 1% change in illiteracy rate. Additionally,
    because the confidence interval for `Frost` contains 0, you can conclude that
    a change in temperature is unrelated to murder rate, holding the other variables
    constant. But your faith in these results is only as strong as the evidence you
    have that your data satisfies the statistical assumptions underlying the model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，你可以有95%的信心认为，在文盲率上升1%的情况下，区间[2.38, 5.90]包含了谋杀率的真实变化。此外，由于`Frost`的置信区间包含0，你可以得出结论，温度的变化与谋杀率无关，其他变量保持不变。但你对这些结果的信心强度仅取决于你拥有的证据，证明你的数据满足模型背后的统计假设。
- en: A set of techniques called *regression diagnostics* provides the necessary tools
    for evaluating the appropriateness of the regression model and can help you to
    uncover and correct problems. We’ll start with a standard approach that uses functions
    that come with R’s base installation. Then we’ll look at newer, improved methods
    available through the `car` package.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一套称为*回归诊断*的技术提供了评估回归模型适当性的必要工具，可以帮助你发现并纠正问题。我们将从使用R的基础安装中提供的函数的标准方法开始。然后我们将探讨通过`car`包提供的更新、改进的方法。
- en: 8.3.1 A typical approach
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 一种典型的方法
- en: R’s base installation provides numerous methods for evaluating the statistical
    assumptions in a regression analysis. The most common approach is to apply the
    `plot()` function to the object returned by `lm()`. Doing so produces four graphs
    that are useful for evaluating the model fit. Applying this approach to the simple
    linear regression example
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: R的基础安装提供了许多评估回归分析中统计假设的方法。最常见的方法是将`plot()`函数应用于`lm()`函数返回的对象。这样做会产生四个图表，这些图表对于评估模型拟合很有用。将这种方法应用于简单的线性回归示例
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: produces the graphs shown in figure 8.5\. The `par(mfrow=c(2,2))` statement
    is used to combine the four plots produced by the `plot()` function into one large
    two-by-two graph. The second `par()` function returns you to single graphs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 产生了图8.5所示的图表。`par(mfrow=c(2,2))`语句用于将`plot()`函数产生的四个图表组合成一个大的两行两列图表。第二个`par()`函数将你带回到单个图表。
- en: '![](Images/CH08_F05_Kabacoff3.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F05_Kabacoff3.png)'
- en: Figure 8.5 Diagnostic plots for the regression of weight on height
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5关于体重对身高的回归诊断图
- en: 'To understand these graphs, consider the assumptions of OLS regression:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这些图表，请考虑OLS回归的假设：
- en: '*Normality*—If the dependent variable is normally distributed for a fixed set
    of predictor values, then the residual values should be normally distributed with
    a mean of 0\. The normal Q-Q plot (upper right) is a probability plot of the standardized
    residuals against the values that would be expected under normality. If you’ve
    met the normality assumption, the points on this graph should fall on the straight
    45-degree line. Because they don’t, you’ve clearly violated the normality assumption.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正态性*——如果对于一组固定的预测值，因变量是正态分布的，那么残差值应该以0为均值正态分布。正态Q-Q图（右上角）是标准化残差与在正态性下预期的值之间的概率图。如果你已经满足了正态性假设，那么这个图上的点应该落在45度直线上。因为它们没有，所以你显然违反了正态性假设。'
- en: '*Independence*—You can’t tell if the dependent variable values are independent
    of these plots. You have to use your understanding of how the data was collected.
    There’s no a priori reason to believe that one woman’s weight influences another
    woman’s weight. If you found out that the data were sampled from families, you
    might have to adjust your assumption of independence.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*独立性*——你不能从这些图中判断依赖变量值是否独立。你必须使用你对数据收集方式的理解。没有先验理由相信一个女性的体重会影响另一个女性的体重。如果你发现数据是从家庭中抽取的，你可能需要调整你的独立性假设。'
- en: '*Linearity*—If the dependent variable is linearly related to the independent
    variables, there should be no systematic relationship between the residuals and
    the predicted (that is, fitted) values. In other words, the model should capture
    all the systematic variance present in the data, leaving nothing but random noise.
    In the residuals-versus-fitted graph (upper left), you see clear evidence of a
    curved relationship, which suggests that you may want to add a quadratic term
    to the regression.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性关系*——如果依赖变量与独立变量呈线性关系，那么残差与预测值（即拟合值）之间不应存在系统性关系。换句话说，模型应该捕捉数据中存在的所有系统性方差，留下只有随机噪声。在残差与拟合图（左上角）中，你看到明显的曲线关系，这表明你可能需要在回归中添加一个二次项。'
- en: '*Homoscedasticity*—If you’ve met the constant variance assumption, the points
    in the scale-location graph (bottom left) should be a random band around a horizontal
    line. You seem to meet this assumption.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同方差性*——如果你已经满足了常数方差假设，那么在尺度-位置图（左下角）中的点应该是在水平线周围的一个随机带。你似乎满足了这一假设。'
- en: Finally, the residuals-versus-leverage graph (bottom right) provides information
    about individual observations that you may wish to attend to. The graph identifies
    outliers, high-leverage points, and influential observations. Specifically,
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，残差与杠杆图（右下角）提供了关于你可能希望关注的单个观测值的信息。该图识别了异常值、高杠杆点和有影响力的观测值。具体来说，
- en: An outlier is an observation that isn’t predicted well by the fitted regression
    model (that is, has a large positive or negative residual).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值是指那些拟合回归模型预测不佳的观测值（即具有较大的正或负残差）。
- en: An observation with a high leverage value has an unusual combination of predictor
    values. That is, it’s an outlier in the predictor space. The dependent variable
    value isn’t used to calculate an observation’s leverage.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有高杠杆值的观测值具有不寻常的预测值组合。也就是说，它在预测空间中是一个异常值。依赖变量值不用于计算观测值的杠杆值。
- en: An influential observation is an observation that has a disproportionate impact
    on the determination of the model parameters. Influential observations are identified
    using a statistic called Cook’s distance, or Cook’s D.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有影响力的观测值是指对模型参数确定有不成比例影响的观测值。有影响力的观测值是通过称为Cook距离或Cook D的统计量来识别的。
- en: To be honest, I find the residuals-versus-leverage plot difficult to read and
    not useful. You’ll see better representations of this information in later sections.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 老实说，我发现残差与杠杆图难以阅读且没有用。你将在后面的章节中看到这个信息的更好表示。
- en: Although these standard diagnostic plots are helpful, better tools are now available
    in R, and I recommend their use over the `plot(fit)` approach.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些标准诊断图很有帮助，但现在R中已经有了更好的工具，我推荐使用它们而不是`plot(fit)`方法。
- en: 8.3.2 An enhanced approach
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2一种改进的方法
- en: The `car` package provides a number of functions that significantly enhance
    your ability to fit and evaluate regression models (see table 8.4).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`car`包提供了一些函数，这些函数可以显著提高你拟合和评估回归模型的能力（见表8.4）。'
- en: Table 8.4 Useful functions for regression diagnostics (`car` package)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.4回归诊断的有用函数（`car`包）
- en: '| Function | Purpose |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 目的 |'
- en: '| `qqPlot()` | Quantile comparisons plot |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `qqPlot()` | 分位数比较图 |'
- en: '| `durbinWatsonTest()` | Durbin–Watson test for autocorrelated errors |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `durbinWatsonTest()` | 自相关误差的Durbin-Watson测试 |'
- en: '| `crPlots()` | Component plus residual plots |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `crPlots()` | 组成加残差图 |'
- en: '| `ncvTest()` | Score test for nonconstant error variance |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| `ncvTest()` | 非常数误差方差得分测试'
- en: '| `spreadLevelPlot()` | Spread-level plots |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| `spreadLevelPlot()` | 扩散水平图 |'
- en: '| `outlierTest()` | Bonferroni outlier test |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| `outlierTest()` | 博费里尼异常值测试 |'
- en: '| `avPlots()` | Added variable plots |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| `avPlots()` | 添加变量图 |'
- en: '| `influencePlot()` | Regression influence plots |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| `influencePlot()` | 回归影响图 |'
- en: '| `vif()` | Variance inflation factors |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `vif()` | 方差膨胀因子 |'
- en: Let’s look at each in turn by applying them to our multiple regression example.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看它们，通过将它们应用于我们的多元回归示例来应用它们。
- en: Normality
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正态性
- en: 'The `qqPlot()` function provides a more accurate method of assessing the normality
    assumption than that provided by the `plot()` function in the base package. It
    plots the studentized residuals (also called *studentized deleted residuals* or
    *jackknifed residuals*) against a t distribution with *n* – *p* – 1 degrees of
    freedom, where *n* is the sample size and *p* is the number of regression parameters
    (including the intercept). The code follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`qqPlot()` 函数提供了比基础包中 `plot()` 函数提供的方法更准确的方法来评估正态性假设。它将学生化残差（也称为 *学生化删除残差*
    或 *刀切残差*）与具有 *n* – *p* – 1 个自由度的 t 分布进行比较，其中 *n* 是样本大小，*p* 是回归参数的数量（包括截距）。代码如下：'
- en: '[PRE14]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `qqPlot()` function generates the probability plot displayed in figure 8.6\.
    The option `id=list(method="identify")` makes the plot interactive—after the graph
    is drawn, mouse clicks on points in the graph will label them with values specified
    in the `labels` option of the function. Pressing the Esc key or the Finish button
    in the upper-right corner of the graph turns off this interactive mode. Here,
    I identified Nevada. When `simulate=TRUE`, a 95% confidence envelope is produced
    using a parametric bootstrap. (Bootstrap methods are considered in chapter 12.)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`qqPlot()` 函数生成了图 8.6 所示的概率图。选项 `id=list(method="identify")` 使得图形交互式——在图形绘制后，鼠标点击图中的点会使用函数
    `labels` 选项中指定的值对其进行标记。按下 Esc 键或图形右上角的完成按钮可以关闭这种交互模式。在这里，我识别了内华达州。当 `simulate=TRUE`
    时，使用参数化自助法生成 95% 置信区间。（自助法在第 12 章中讨论。）'
- en: '![](Images/CH08_F06_Kabacoff3.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH08_F06_Kabacoff3.png)'
- en: Figure 8.6 Q-Q plot for studentized residuals
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 学生化残差 Q-Q 图
- en: With the exception of Nevada, all the points fall close to the line and are
    within the confidence envelope, suggesting that you’ve met the normality assumption
    fairly well. But you should definitely look at Nevada. It has a large positive
    residual (actual minus predicted), indicating that the model underestimates the
    murder rate in this state. Specifically,
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内华达州之外，所有点都接近直线，并且位于置信区间内，这表明你相当好地满足了正态性假设。但你应该肯定地看看内华达州。它有一个大的正残差（实际值减去预测值），表明该模型低估了该州的谋杀率。具体来说，
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here you see that the murder rate is 11.5%, but the model predicts a 3.9% murder
    rate. The question you need to ask is, “Why does Nevada have a higher murder rate
    than predicted from population, income, illiteracy, and temperature?” Anyone (who
    hasn’t seen *Casino*) want to guess?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到谋杀率是 11.5%，但模型预测的谋杀率是 3.9%。你需要问的问题是，“为什么内华达州的谋杀率比根据人口、收入、文盲率和温度预测的要高？”任何（没有看过
    *Casino*）的人想猜一猜吗？
- en: Independence of errors
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 误差独立性
- en: 'As noted earlier, the best way to assess whether the dependent variable values
    (and thus the residuals) are independent is from your knowledge of how the data
    were collected. For example, time-series data often display autocorrelation—observations
    collected closer in time are more correlated with each other than with observations
    distant in time. The `car` package provides a function for the Durbin–Watson test
    to detect such serially correlated errors. You can apply the Durbin–Watson test
    to the multiple regression problem with the following code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，评估因变量值（以及因此残差）是否独立的最佳方式是依据你对数据收集方式的了解。例如，时间序列数据通常表现出自相关性——时间上更接近的观测值彼此之间比与时间上较远的观测值更相关。`car`
    包提供了一个用于检测此类序列相关错误的 Durbin-Watson 测试函数。你可以使用以下代码将 Durbin-Watson 测试应用于多重回归问题：
- en: '[PRE16]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The nonsignificant p-value (p = 0.282) suggests a lack of autocorrelation and,
    conversely, an independence of errors. The lag value (1 in this case) indicates
    that each observation is being compared with the one next to it in the dataset.
    Although appropriate for time-dependent data, the test is less applicable for
    data that isn’t clustered in this fashion. Note that the `durbinWatsonTest()`
    function uses bootstrapping (see chapter 12) to derive p-values. Unless you add
    the option `simulate=FALSE`, you’ll get a slightly different value each time you
    run the test.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 非显著 p 值（p = 0.282）表明不存在自相关性，反之，误差是独立的。滞后值（本例中为 1）表示每个观测值正在与数据集中相邻的观测值进行比较。尽管适用于时间依赖性数据，但对于这种方式的集群数据，该测试的适用性较低。请注意，`durbinWatsonTest()`
    函数使用自助法（见第 12 章）来推导 p 值。除非你添加选项 `simulate=FALSE`，否则每次运行测试时都会得到一个略有不同的值。
- en: Linearity
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 线性
- en: You can look for evidence of nonlinearity in the relationship between the dependent
    variable and the independent variables by using component-plus-residual plots
    (also known as *partial residual plots*). The plot is produced by the `crPlots()`
    function in the `car` package. You’re looking for any systematic departure from
    the linear model that you’ve specified.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用成分加残差图（也称为*部分残差图*）来寻找依赖变量与独立变量之间关系的非线性证据。该图由`car`包中的`crPlots()`函数生成。你正在寻找任何与指定的线性模型有系统的偏离。
- en: To create a component-plus-residual plot for variable *k*, you plot the points
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要为变量*k*创建一个成分加残差图，你绘制以下点
- en: '![](Images/CH08_F02_Kabacoff3-EQ11a.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F02_Kabacoff3-EQ11a.png)'
- en: 'where the residuals are based on the full model (containing all the predictors),
    and *i* = 1 ... *n*. The straight line in each graph is given by ![](Images/CH08_F02_Kabacoff3-EQ12a.png).
    A loess line (a smoothed nonparametric fit line) is also provided for each plot.
    Chapter 11 describes loess lines. The code to produce these plots is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中残差基于完整模型（包含所有预测因子），且*i* = 1 ... *n*。每个图中直线由![](Images/CH08_F02_Kabacoff3-EQ12a.png)给出。每个图还提供了一个局部加权回归线（loess线，一种平滑的非参数拟合线）。第11章将描述loess线。生成这些图的代码如下：
- en: '[PRE17]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Figure 8.7 provides the resulting plots. Nonlinearity in any of these plots
    suggests that you may not have adequately modeled the functional form of that
    predictor in the regression. If so, you may need to add curvilinear components
    such as polynomial terms, transform one or more variables (for example, use `log(X)`
    instead of `X)`, or abandon linear regression in favor of some other regression
    variant. Transformations are discussed later in this chapter.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7提供了结果图。这些图中的任何非线性都表明，你可能没有充分地模拟该预测因子的函数形式在回归中。如果是这样，你可能需要添加曲线成分，如多项式项，变换一个或多个变量（例如，使用`log(X)`而不是`X`），或者放弃线性回归，转而使用其他回归变体。变换将在本章后面讨论。
- en: '![](Images/CH08_F07_Kabacoff3.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F07_Kabacoff3.png)'
- en: Figure 8.7 Component-plus-residual plots for the regression of murder rate on
    state characteristics
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7回归中谋杀率与州特征成分加残差图
- en: The component-plus-residual plots confirm that you’ve met the linearity assumption.
    The form of the linear model seems to be appropriate for this dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 成分加残差图确认你已经满足了线性假设。线性模型的形态似乎适用于这个数据集。
- en: Homoscedasticity
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 同方差性
- en: The `car` package also provides two useful functions for identifying nonconstant
    error variance. The `ncvTest()` function produces a score test of the hypothesis
    of constant error variance against the alternative that the error variance changes
    with the level of the fitted values. A significant result suggests heteroscedasticity
    (nonconstant error variance).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`car`包还提供了两个用于识别非恒定误差方差的实用函数。`ncvTest()`函数产生一个关于恒定误差方差假设的得分检验，与误差方差随拟合值水平变化的备择假设相对。显著结果表明异方差性（非恒定误差方差）。'
- en: The `spreadLevelPlot()` function creates a scatter plot of the absolute standardized
    residuals versus the fitted values and superimposes a line of best fit. Both functions
    are demonstrated in the next listing.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`spreadLevelPlot()`函数创建一个绝对标准化残差与拟合值的散点图，并叠加最佳拟合线。这两个函数将在下一列表中演示。'
- en: Listing 8.6 Assessing homoscedasticity
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6评估同方差性
- en: '[PRE18]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The score test is nonsignificant (p = 0.19), suggesting that you’ve met the
    constant variance assumption. You can also see this in the spread-level plot (figure
    8.8). The points form a random horizontal band around a horizontal line of best
    fit. If you’d violated the assumption, you’d expect to see a nonhorizontal line.
    The suggested power transformation in listing 8.6 is the suggested power p (*Y*^p)
    that would stabilize the nonconstant error variance. For example, if the plot
    showed a nonhorizontal trend and the suggested power transformation was 0.5, then
    using √*Y* rather than *Y* in the regression equation might lead to a model that
    satisfied homoscedasticity. If the suggested power was 0, you’d use a log transformation.
    In the current example, there’s no evidence of heteroscedasticity, and the suggested
    power is close to 1 (no transformation required).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 分数检验是非显著的（p = 0.19），这表明您已经满足了常数方差假设。您也可以在散点图水平（图8.8）中看到这一点。点围绕最佳拟合线的水平线形成一个随机的水平带。如果您违反了这一假设，您会期望看到一条非水平线。列表8.6中建议的幂变换是建议的幂p
    (*Y*^p)，它将稳定非常数误差方差。例如，如果图显示了非水平趋势，并且建议的幂变换是0.5，那么在回归方程中使用√*Y*而不是*Y*可能会导致满足同方差性的模型。如果建议的幂是0，您将使用对数变换。在当前示例中，没有异方差性的证据，建议的幂接近1（不需要变换）。
- en: '![](Images/CH08_F08_Kabacoff3.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F08_Kabacoff3.png)'
- en: Figure 8.8 Spread-level plot for assessing constant error variance
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 用于评估常数误差方差的散点图水平
- en: 8.3.3 Multicollinearity
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 多重共线性
- en: Before leaving this section on regression diagnostics, let’s focus on a problem
    that’s not directly related to statistical assumptions but is important in helping
    you interpret multiple regression results. Imagine you’re conducting a study of
    grip strength. Your independent variables include date of birth (DOB) and age.
    You regress grip strength on DOB and age and find a significant overall F-test
    at p < .001. But when you look at the individual regression coefficients for DOB
    and age, you find that they’re both nonsignificant (that is, there’s no evidence
    that either is related to grip strength). What happened?
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开回归诊断这一节之前，让我们关注一个与统计假设无直接关系但有助于您解释多元回归结果的问题。想象一下，您正在进行一项关于握力的研究。您的自变量包括出生日期（DOB）和年龄。您将握力对DOB和年龄进行回归，并发现F检验在p
    < .001时具有显著的整体效果。但当您查看DOB和年龄的个体回归系数时，您发现它们都是非显著的（也就是说，没有证据表明它们与握力相关）。发生了什么？
- en: The problem is that DOB and age are perfectly correlated within rounding error.
    A regression coefficient measures the impact of one predictor variable on the
    response variable, holding all other predictor variables constant. This amounts
    to looking at the relationship of grip strength and age, holding age constant.
    The problem is called multicollinearity. It leads to large confidence intervals
    for model parameters and makes the interpretation of individual coefficients difficult.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于DOB和年龄在舍入误差范围内完全相关。回归系数衡量一个预测变量对响应变量的影响，同时保持所有其他预测变量不变。这相当于在保持年龄不变的情况下观察握力和年龄的关系。这个问题被称为多重共线性。它导致模型参数的置信区间很大，使得对单个系数的解释变得困难。
- en: Multicollinearity can be detected using a statistic called the variance inflation
    factor (VIF). For any predictor variable, the square root of the VIF indicates
    the degree to which the confidence interval for that variable’s regression parameter
    is expanded relative to a model with uncorrelated predictors (hence the name).
    VIF values are provided by the `vif()` function in the `car` package. As a general
    rule, a VIF > 10 indicates a multicollinearity problem. The following listing
    provides the code. The results indicate that multicollinearity isn’t a problem
    with these predictor variables.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用一个称为方差膨胀因子（VIF）的统计量来检测多重共线性。对于任何预测变量，VIF的平方根表示该变量的回归参数置信区间相对于无相关预测变量的模型扩展的程度（因此得名）。VIF值由`car`包中的`vif()`函数提供。一般来说，VIF
    > 10表示存在多重共线性问题。以下列表提供了代码。结果表明，这些预测变量不存在多重共线性问题。
- en: Listing 8.7 Evaluating multicollinearity
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7 评估多重共线性
- en: '[PRE19]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 8.4 Unusual observations
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 不寻常的观测值
- en: A comprehensive regression analysis will also include a screening for unusual
    observations—outliers, high-leverage observations, and influential observations.
    These data points warrant further investigation, either because they’re different
    from other observations in some way or because they exert a disproportionate amount
    of influence on the results. Let’s look at each in turn.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 综合回归分析还将包括对异常观测值的筛选——异常值、高杠杆观测值和有影响力的观测值。这些数据点需要进一步调查，要么是因为它们在某种程度上与其他观测值不同，要么是因为它们对结果产生了不成比例的影响。让我们逐一查看。
- en: 8.4.1 Outliers
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 异常值
- en: Outliers are observations that aren’t predicted well by the model. They have
    unusually large positive or negative residuals(*Y[i]* − *Ŷ[i]*). Positive residuals
    indicate that the model is underestimating the response value, whereas negative
    residuals indicate an overestimation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是模型预测不佳的观测值。它们具有异常大的正或负残差（*Y[i]* − *Ŷ[i]*）。正残差表示模型低估了响应值，而负残差表示高估。
- en: You’ve already seen one way to identify outliers. Points in the Q-Q plot of
    figure 8.6 that lie outside the confidence band are considered outliers. A rough
    rule of thumb is that standardized residuals that are larger than 2 or less than
    –2 are worth attention.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了一种识别异常值的方法。图8.6的Q-Q图中位于置信带之外的点被认为是异常值。一个粗略的规则是，标准化残差大于2或小于-2的值得注意。
- en: 'The `car` package also provides a statistical test for outliers. The `outlierTest()`
    function reports the Bonferroni adjusted p-value for the largest absolute studentized
    residual:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`car` 包还提供了一种用于异常值的统计测试。`outlierTest()` 函数报告最大绝对学生化残差的Bonferroni调整后的p值：'
- en: '[PRE20]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, you see that Nevada is identified as an outlier (p = 0.048). Note that
    this function tests the single largest (positive or negative) residual for significance
    as an outlier. If it isn’t significant, there are no outliers in the dataset.
    If it’s significant, you must delete it and rerun the test to see if others are
    present.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到内华达州被识别为异常值（p = 0.048）。请注意，此函数测试单个最大（正或负）残差是否作为异常值具有显著性。如果不显著，数据集中没有异常值。如果显著，你必须删除它并重新运行测试，以查看是否存在其他异常值。
- en: 8.4.2 High-leverage points
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 高杠杆点
- en: Observations that have high leverage are outliers with regard to the other predictors.
    In other words, they have an unusual combination of predictor values. The response
    value isn’t involved in determining leverage.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 具有高杠杆作用的观测值是相对于其他预测因子的异常值。换句话说，它们具有不寻常的预测值组合。响应值不涉及确定杠杆。
- en: 'Observations with high leverage are identified through the *hat statistic*.
    For a given dataset, the average hat value is *p/n*, where *p* is the number of
    parameters estimated in the model (including the intercept) and *n* is the sample
    size. Roughly speaking, an observation with a hat value greater than two or three
    times the average hat value should be examined. The code that follows plots the
    `hat` values:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通过帽子统计量识别具有高杠杆作用的观测值。对于给定的数据集，平均帽子值是 *p/n*，其中 *p* 是模型中估计的参数数量（包括截距）和 *n* 是样本大小。粗略地说，帽子值大于平均帽子值两倍或三倍的观测值应该进行检查。下面的代码绘制了
    `hat` 值：
- en: '[PRE21]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Figure 8.9 shows the resulting graph.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9显示了结果图。
- en: Horizontal lines are drawn at two and three times the average hat value. The
    locator function places the graph in interactive mode. Clicking points of interest
    labels them until the user presses Esc or the Finish button in the upper-right
    corner of the graph.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在平均帽子值的两倍和三倍处画水平线。定位函数将图形置于交互模式。点击感兴趣的点，直到用户按下Esc键或图形右上角的完成按钮。
- en: '![](Images/CH08_F09_Kabacoff3.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F09_Kabacoff3.png)'
- en: Figure 8.9 Index plot of hat values for assessing observations with high leverage
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9用于评估具有高杠杆作用的观测值的帽子值索引图
- en: Here you see that Alaska and California are particularly unusual when it comes
    to their predictor values. Alaska has a much higher income than other states,
    while having a lower population and temperature. California has a much higher
    population than other states, while having a higher income and higher temperature.
    These states are atypical compared with the other 48 observations.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到阿拉斯加和加利福尼亚在预测值方面特别不寻常。阿拉斯加的收入比其他州高得多，而人口和温度较低。加利福尼亚的人口比其他州高得多，而收入和温度也更高。与其他48个观测值相比，这些州是不典型的。
- en: High-leverage observations may or may not be influential observations. That
    will depend on whether they’re also outliers.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 高杠杆观察值可能是有影响力的观察值，也可能不是。这取决于它们是否也是异常值。
- en: 8.4.3 Influential observations
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 有影响力的观察值
- en: '*Influential* observations have a disproportionate impact on the values of
    the model parameters. Imagine finding that your model changes dramatically with
    the removal of a single observation. It’s this concern that leads you to examine
    your data for influential points.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*有影响力的*观察对模型参数的值有不成比例的影响。想象一下，如果你发现你的模型在移除单个观察值后发生了显著变化。正是这种担忧促使你检查你的数据以寻找有影响力的点。'
- en: 'There are two methods for identifying influential observations: Cook’s distance
    (or D statistic) and added variable plots. Roughly speaking, Cook’s D values greater
    than 4/(*n* – *k* – 1), where *n* is the sample size and *k* is the number of
    predictor variables, indicate influential observations. You can create a Cook’s
    D plot (figure 8.10) with the following code:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 识别有影响力的观察值有两种方法：Cook距离（或D统计量）和添加变量图。粗略地说，Cook的D值大于4/(*n* – *k* – 1)，其中*n*是样本大小，*k*是预测变量的数量，表明有影响力的观察值。你可以使用以下代码创建Cook的D图（图8.10）：
- en: '[PRE22]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](Images/CH08_F10_Kabacoff3.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH08_F10_Kabacoff3.png)'
- en: Figure 8.10 Cook’s D plot for identifying influential observations
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 Cook’s D图用于识别有影响力的观察值
- en: The graph identifies Alaska, Hawaii, and Nevada as influential observations.
    Deleting these states will have a notable impact on the values of the intercept
    and slopes in the regression model. Note that although it’s useful to cast a wide
    net when searching for influential observations, I tend to find a cutoff of 1
    more generally useful than 4/(*n* – *k* – 1). Given a criterion of D = 1, none
    of the observations in the dataset would appear to be influential.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表识别出阿拉斯加、夏威夷和内华达是有影响力的观察值。删除这些州将对回归模型中截距和斜率的值产生显著影响。请注意，尽管在寻找有影响力的观察值时撒网广泛是有用的，但我通常发现1的截止点比4/(*n*
    – *k* – 1)更普遍有用。给定D = 1的标准，数据集中的观察值似乎都不会显得有影响力。
- en: Cook’s D plots can help identify influential observations, but they don’t provide
    information about how these observations affect the model. Added-variable plots
    can help in this regard. For one response variable and *k* predictor variables,
    you’d create *k* added-variable plots as follows.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Cook’s D图可以帮助识别有影响力的观察值，但它们不提供有关这些观察值如何影响模型的信息。添加变量图在这方面有所帮助。对于单个响应变量和*k*个预测变量，你会创建*k*个添加变量图，如下所示。
- en: 'For each predictor *Xk*, plot the residuals from regressing the response variable
    on the other *k* – 1 predictors versus the residuals from regressing *Xk* on the
    other *k* – 1 predictors. Added-variable plots can be created using the `avPlots()`
    function in the `car` package:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个预测变量*Xk*，绘制响应变量对其他*k* – 1个预测变量的回归残差与*Xk*对其他*k* – 1个预测变量的回归残差之间的图。可以使用`car`包中的`avPlots()`函数创建添加变量图：
- en: '[PRE23]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Figure 8.11 provides the resulting graphs. The graphs are produced one at a
    time, and users can click points to identify them. Press Esc or the Finish button
    on the upper-right corner of the graph to move to the next plot. Here, I’ve identified
    Alaska in the bottom-left plot.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11提供了相应的图表。图表一个接一个地生成，用户可以点击点来识别它们。按Esc键或图表右上角的完成按钮以移动到下一个图表。在这里，我在左下角的图表中识别了阿拉斯加。
- en: '![](Images/CH08_F11_Kabacoff3.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH08_F11_Kabacoff3.png)'
- en: Figure 8.11 Added-variable plots for assessing the impact of influential observations
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11评估有影响力的观察值影响的添加变量图
- en: The straight line in each plot is the actual regression coefficient for that
    predictor variable. You can see the impact of influential observations by imagining
    how the line would change if the point representing that observation was deleted.
    For example, look at the graph of Murder | Others versus Income | Others in the
    lower-left corner. You can see that eliminating the point labeled Alaska would
    move the line in a negative direction. In fact, deleting Alaska changes the regression
    coefficient for Income from positive (.00006) to negative (–.00085).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图中的直线是该预测变量的实际回归系数。你可以通过想象如果删除代表该观察值的点，这条线会如何变化来看到有影响力的观察值的影响。例如，看看左下角的Murder
    | Others与Income | Others的图表。你可以看到，消除标记为阿拉斯加的点会将线向负方向移动。实际上，删除阿拉斯加将收入回归系数从正的（.00006）变为负的（–.00085）。
- en: 'You can combine the information from outlier, leverage, and influence plots
    into one highly informative plot using the `influencePlot()` function from the
    `car` package:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `car` 包中的 `influencePlot()` 函数将异常值、杠杆和影响力图的信息合并到一个高度信息化的图中：
- en: '[PRE24]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The resulting plot (figure 8.12) identifies observations that are particularly
    noteworthy. In particular, it shows that Nevada and Rhode Island are outliers,
    California, and Hawaii have high leverage, and Nevada and Alaska are influential
    observations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图（图8.12）识别了特别值得注意的观测值。特别是，它显示内华达州和罗德岛是异常值，加利福尼亚州和夏威夷有高杠杆作用，内华达州和阿拉斯加是具有影响力的观测值。
- en: Replacing `id="noteworthy"` with `id=list(method="identify")` allows you to
    identify points interactively with mouse clicks (ending with ESC or pressing the
    Finish button).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `id="noteworthy"` 替换为 `id=list(method="identify")` 允许您通过鼠标点击交互式地识别点（结束于ESC或按下完成按钮）。
- en: '![](Images/CH08_F12_Kabacoff3.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F12_Kabacoff3.png)'
- en: Figure 8.12 Influence plot. States above +2 or below –2 on the vertical axis
    are considered outliers. States above 0.2 or 0.3 on the horizontal axis have high
    leverage (unusual combinations of predictor values). Circle size is proportional
    to influence. Observations depicted by large circles may have disproportionate
    influence on the parameter estimates of the model.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 影响图。垂直轴上+2或-2以上的州被认为是异常值。水平轴上0.2或0.3以上的州具有高杠杆作用（预测值的不寻常组合）。圆圈大小与影响力成正比。由大圆圈表示的观测值可能对模型的参数估计有不成比例的影响。
- en: 8.5 Corrective measures
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 纠正措施
- en: 'Having spent the last 16 pages learning about regression diagnostics, you may
    ask, “What do you do if you identify problems?” There are four approaches to dealing
    with violations of regression assumptions:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去16页中学习了回归诊断之后，您可能会问，“如果你发现了问题，你会怎么做？”处理回归假设违反有四种方法：
- en: Deleting observations
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除观测值
- en: Transforming variables
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量转换
- en: Adding or deleting variables
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加或删除变量
- en: Using another regression approach
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用另一种回归方法
- en: Let’s look at each in turn.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一看看。
- en: 8.5.1 Deleting observations
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 删除观测值
- en: Deleting outliers can often improve a dataset’s fit to the normality assumption.
    Influential observations are often deleted as well, because they have an inordinate
    impact on the results. The largest outlier or influential observation is deleted,
    and the model is refit. If there are still outliers or influential observations,
    the process is repeated until an acceptable fit is obtained.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 删除异常值可以经常改善数据集对正态性假设的拟合。具有影响力的观测值通常也会被删除，因为它们对结果有不成比例的影响。删除最大的异常值或具有影响力的观测值，并重新拟合模型。如果仍然存在异常值或具有影响力的观测值，则重复此过程，直到获得可接受的拟合。
- en: Again, I urge caution when considering the deletion of observations. Sometimes
    you can determine that the observation is an outlier because of data errors in
    recording, or because a protocol wasn’t followed, or because a test subject misunderstood
    instructions. In these cases, deleting the offending observation seems perfectly
    reasonable.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我强烈建议在考虑删除观测值时要谨慎。有时您可以确定观测值是异常值，因为记录数据时的错误，或者因为未遵循协议，或者因为测试对象误解了指示。在这些情况下，删除有问题的观测值似乎是完全合理的。
- en: In other cases, the unusual observation may be the most interesting thing about
    the data you’ve collected. Uncovering why an observation differs from the rest
    can add great insight to the topic at hand and to other topics you might not have
    thought of. Some of our greatest advances have come from the serendipity of noticing
    that something doesn’t fit our preconceptions (pardon the hyperbole).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，异常观测值可能是您收集的数据中最有趣的事情。揭示为什么一个观测值与其他观测值不同可以为当前的主题以及您可能没有考虑到的其他主题提供深刻的见解。我们的一些最大进步来自于偶然注意到某些事情不符合我们的先入之见（请原谅我的夸张）。
- en: 8.5.2 Transforming variables
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.2 变量转换
- en: When models don’t meet the normality, linearity, or homoscedasticity assumptions,
    transforming one or more variables can often improve or correct the situation.
    Transformations typically involve replacing a variable *Y* with *Y^λ*. Table 8.5
    gives common values of *λ* and their interpretations. If *Y* is a proportion,
    a logit transformation `[loge (Y/1-Y)]` is often used. When *Y* is highly skewed,
    a log transformation is often helpful.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型不符合正态性、线性或同方差性假设时，转换一个或多个变量通常可以改善或纠正这种情况。转换通常涉及将变量 *Y* 替换为 *Y^λ*。表8.5给出了常见的
    *λ* 值及其解释。如果 *Y* 是一个比例，则通常使用对数变换 `[loge (Y/1-Y)]`。当 *Y* 极度偏斜时，对数变换通常很有帮助。
- en: Table 8.5 Common transformations
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.5 常见转换
- en: '| *λ* | -2 | -1 | -0.5 | 0 | 0.5 | 1 | 2 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| *λ* | -2 | -1 | -0.5 | 0 | 0.5 | 1 | 2 |'
- en: '| Transformation | 1/*Y*² | 1/*Y* | 1/√*Y* | log(*Y*) | √*Y* | None | *Y*²
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 1/*Y*² | 1/*Y* | 1/√*Y* | log(*Y*) | √*Y* | 无 | *Y*² |'
- en: When the model violates the normality assumption, you typically attempt a transformation
    of the response variable. You can use the `powerTransform()` function in the `car`
    package to generate a maximum-likelihood estimation of the power *λ* most likely
    to normalize the variable *X^λ*. The resulting transformation is called a Box–Cox
    transformation. In the next listing, this is applied to the `states` data.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型违反了正态性假设时，你通常会尝试对响应变量进行转换。你可以使用 `car` 包中的 `powerTransform()` 函数来生成最可能使变量
    *X^λ* 正态化的幂 *λ* 的最大似然估计。这种转换称为 Box-Cox 转换。在下一个列表中，这种转换应用于 `states` 数据。
- en: Listing 8.8 Box–Cox transformation to normality
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.8 Box-Cox 转换到正态性
- en: '[PRE25]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The results suggest that you can normalize the variable `Murder` by replacing
    it with `Murder0.6`. Because 0.6 is close to 0.5, you could try a square-root
    transformation to improve the model’s fit to normality. But in this case, the
    hypothesis that *λ* = 1 can’t be rejected (p = 0.145), so there’s no strong evidence
    that a transformation is needed in this case. This is consistent with the results
    of the Q-Q plot in figure 8.9.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，你可以通过将其替换为 `Murder0.6` 来标准化变量 `Murder`。因为 0.6 接近 0.5，你可以尝试平方根转换来改善模型对正态性的拟合。但在这个情况下，*λ*
    = 1 的假设不能被拒绝（p = 0.145），因此没有强有力的证据表明在这种情况下需要转换。这与图 8.9 中的 Q-Q 图的结果一致。
- en: Interpreting a log transformation
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 解释对数转换
- en: Log transformations are often used to make highly skewed distributions less
    skewed. For example, the variable `income` is often right skewed, with more individuals
    at the lower end of the scale and a few individuals with very high incomes. How
    do we interpret regression coefficients when the response variable has been log
    transformed?
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对数转换通常用于使高度偏斜的分布变得不那么偏斜。例如，变量 `income` 通常右偏斜，有更多的人位于刻度较低的一端，而少数人收入非常高。当响应变量已被对数转换时，我们如何解释回归系数？
- en: We normally interpret the regression coefficient for *X* as the expected change
    in *Y* for a unit change in *X*. Consider the model *Y* = 3 + 0.6*X*. We would
    predict a 0.6 increase in *Y* for a one-unit increase in *X*. Similarly, a 10-unit
    change in *X* would be associated with a 0.6(10) or 6-point change in *Y*.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将 *X* 的回归系数解释为 *Y* 在 *X* 单位变化时的预期变化。考虑模型 *Y* = 3 + 0.6*X*。我们预测当 *X* 增加1个单位时，*Y*
    将增加0.6。同样，*X* 增加10个单位将与 *Y* 增加0.6(10)或6个点的变化相关联。
- en: However, if the model is loge(*Y*) = 3 + 0.6*X*, then a one unit change in *X*
    multiplies the expected value of *Y* by *e*^(0.6) = 1.06. Thus, a one-unit increase
    in *X* would predict a 6% increase in *Y*. A 10-unit increase in *X* would multiply
    the expected valued of *Y* by *e*^(0.6(10)) = 1.82. Thus, a 10-unit increase in
    *X* would predict an 82% increase in *Y*.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果模型是 loge(*Y*) = 3 + 0.6*X*，那么 *X* 的一个单位变化会使 *Y* 的预期值乘以 *e*^(0.6) = 1.06。因此，*X*
    增加1个单位将预测 *Y* 增加6%。*X* 增加10个单位将使 *Y* 的预期值乘以 *e*^(0.6(10)) = 1.82。因此，*X* 增加10个单位将预测
    *Y* 增加82%。
- en: To learn more about interpreting log transformations in linear regression, see
    Kenneth Benoit's excellent guide ([https://kenbenoit.net/assets/courses/ME104/logmodels2.pdf](https://kenbenoit.net/assets/courses/ME104/logmodels2.pdf)).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于线性回归中解释对数转换的信息，请参阅 Kenneth Benoit 的优秀指南 ([https://kenbenoit.net/assets/courses/ME104/logmodels2.pdf](https://kenbenoit.net/assets/courses/ME104/logmodels2.pdf))。
- en: 'When the assumption of linearity is violated, a transformation of the predictor
    variables can often help. The `boxTidwell()` function in the `car` package can
    be used to generate maximum-likelihood estimates of predictor powers that can
    improve linearity. An example of applying the Box–Tidwell transformations to a
    model that predicts state murder rates from their population and illiteracy rates
    follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当线性假设被违反时，转换预测变量通常有助于解决问题。`car` 包中的 `boxTidwell()` 函数可以用来生成预测变量幂的最大似然估计，这可以改善线性。以下是一个将
    Box-Tidwell 转换应用于从人口和文盲率预测州谋杀率的模型的例子：
- en: '[PRE26]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The results suggest trying the transformations `Population`^(0.87) and `Population`^(1.36)
    to achieve greater linearity. But the score tests for `Population` (p = .75) and
    `Illiteracy` (p = .54) suggest that neither variable needs to be transformed.
    Again, these results are consistent with the component-plus-residual plots in
    figure 8.7.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明尝试变换`Population`^(0.87)和`Population`^(1.36)以实现更大的线性。但`Population`（p = .75）和`Illiteracy`（p
    = .54）的得分检验表明，这两个变量都不需要变换。再次强调，这些结果与图8.7中的成分加残差图一致。
- en: Finally, transformations of the response variable can help in situations of
    heteroscedasticity (nonconstant error variance). You saw in listing 8.8 that the
    `spreadLevel Plot()` function in the `car` package offers a power transformation
    for improving homoscedasticity. Again, in the case of the `states` example, the
    constant error- variance assumption is met, and no transformation is necessary.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，响应变量的变换可以帮助处理异方差性（非恒定误差方差）的情况。您在列表8.8中看到，`car`包中的`spreadLevel Plot()`函数提供了一个用于提高同方差性的幂变换。同样，在`states`示例中，常数误差方差假设得到满足，因此不需要进行变换。
- en: A caution concerning transformations
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 关于变换的注意事项
- en: 'There’s an old joke in statistics: if you can’t prove A, prove B and pretend
    it was A. (For statisticians, that’s pretty funny.) The relevance here is that
    if you transform your variables, your interpretations must be based on the transformed
    variables, not the original variables. If the transformation makes sense, such
    as the log of income or the inverse of distance, the interpretation is easier.
    But how do you interpret the relationship between the frequency of suicidal ideation
    and the cube root of depression? If a transformation doesn’t make sense, you should
    avoid it.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学中有一个古老的笑话：如果你不能证明A，就证明B，然后假装它是A。（对于统计学家来说，这相当有趣。）这里的相关性在于，如果你变换了变量，你的解释必须基于变换后的变量，而不是原始变量。如果变换有意义，例如收入的对数或距离的倒数，解释就更容易。但你怎么解释自杀意念频率与抑郁立方根之间的关系？如果变换没有意义，你应该避免它。
- en: 8.5.3 Adding or deleting variables
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.3 添加或删除变量
- en: Changing the variables in a model will impact the fit of the model. Sometimes,
    adding an important variable will correct many of the problems that we’ve discussed.
    Deleting a troublesome variable can do the same thing.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 改变模型中的变量将影响模型的拟合度。有时，添加一个重要变量可以纠正我们讨论过的许多问题。删除一个麻烦的变量也可以达到同样的效果。
- en: Deleting variables is particularly important for dealing with multicollinearity.
    If your only goal is to make predictions, then multicollinearity isn’t a problem.
    But if you want to make interpretations about individual predictor variables,
    then you must deal with it. The most common approach is to delete one of the variables
    involved in the multicollinearity (that is, one of the variables with a VIF >
    10). An alternative is to use lasso or ridge regression, variants of multiple
    regression designed to deal with multicollinearity situations.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 删除变量对于处理多重共线性尤为重要。如果你的唯一目标是进行预测，那么多重共线性不是一个问题。但如果你想要对个体预测变量进行解释，那么你必须处理它。最常见的方法是删除一个参与多重共线性的变量（即VIF
    > 10的变量之一）。另一种选择是使用lasso或ridge回归，这些是旨在处理多重共线性情况的多重回归的变体。
- en: 8.5.4 Trying a different approach
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.4 尝试不同的方法
- en: As you’ve just seen, one approach to dealing with multicollinearity is to fit
    a different type of model (ridge or lasso regression, in this case). If there
    are outliers and/or influential observations, you can fit a robust regression,
    model rather than an OLS regression. If you’ve violated the normality assumption,
    you can fit a nonparametric regression model. If there’s significant nonlinearity,
    you can try a nonlinear regression model. If you’ve violated the assumptions of
    independence of errors, you can fit a model that specifically takes the error
    structure into account, such as time-series models or multilevel regression models.
    Finally, you can turn to generalized linear models to fit a wide range of models
    in situations where the assumptions of OLS regression don’t hold.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您刚才看到的，处理多重共线性的一种方法是通过拟合不同类型的模型（在本例中为岭回归或lasso回归）。如果有异常值和/或影响性观测值，您可以拟合一个稳健回归模型，而不是OLS回归。如果您违反了正态性假设，您可以拟合一个非参数回归模型。如果存在显著的非线性，您可以尝试非线性回归模型。如果您违反了误差独立性的假设，您可以拟合一个特别考虑误差结构的模型，例如时间序列模型或多级回归模型。最后，您可以考虑广义线性模型，以拟合在OLS回归假设不成立的情况下广泛的各种模型。
- en: We’ll discuss some of these alternative approaches in chapter 13\. The decision
    of when to try to improve the fit of an OLS regression model and when to try a
    different approach is complex. It’s typically based on knowledge of the subject
    matter and an assessment of which approach will provide the best result.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第13章讨论这些替代方法中的一些。何时尝试改进OLS回归模型的拟合度，何时尝试不同的方法，这是一个复杂的问题。这通常基于对主题知识的了解和对哪种方法将提供最佳结果的评估。
- en: Speaking of best results, let’s turn now to the problem of deciding which predictor
    variables to include in a regression model.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 说到最佳结果，现在让我们转向决定将哪些预测变量包含在回归模型中的问题。
- en: 8.6 Selecting the “best” regression model
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 选择“最佳”回归模型
- en: When developing a regression equation, you’re implicitly faced with a selection
    of many possible models. Should you include all the variables under study, or
    drop ones that don’t make a significant contribution to prediction? Should you
    add polynomial and/or interaction terms to improve the fit? The selection of a
    final regression model always involves a compromise between predictive accuracy
    (a model that fits the data as well as possible) and parsimony (a simple and replicable
    model). All things being equal, if you have two models with approximately equal
    predictive accuracy, you favor the simpler one. This section describes methods
    for choosing among competing models. The word “best” is in quotation marks because
    there’s no single criterion you can use to make the decision. The final decision
    requires judgment on the part of the investigator. (Think of it as job security.)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发回归方程时，你隐含地面临从许多可能的模型中进行选择的问题。你应该包含所有研究变量，还是删除对预测没有显著贡献的变量？你应该添加多项式和/或交互项来提高拟合度？最终回归模型的选择总是涉及预测精度（尽可能好地拟合数据的模型）和简洁性（简单且可复制的模型）之间的折衷。在所有条件相同的情况下，如果你有两个具有大致相等预测精度的模型，你更倾向于选择更简单的一个。本节描述了在竞争模型之间进行选择的方法。单词“最佳”用引号括起来，因为没有单一的标准可以用来做出决定。最终的决定需要调查者的判断。（把它想象成职业保障。）
- en: 8.6.1 Comparing models
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.1 比较模型
- en: You can compare the fit of two nested models using the `anova()` function in
    the base installation. A nested model is one whose terms are completely included
    in the other model. In the `states` multiple regression model, you found that
    the regression coefficients for `Income` and `Frost` were nonsignificant. You
    can test whether a model without these two variables predicts as well as one that
    includes them (see the following listing).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用基础安装中的`anova()`函数比较两个嵌套模型的拟合度。嵌套模型是指其项完全包含在另一个模型中的模型。在`states`多元回归模型中，你发现`Income`和`Frost`的回归系数不显著。你可以测试一个不包含这两个变量的模型是否与包含它们的模型一样好（见以下列表）。
- en: Listing 8.9 Comparing nested models using the `anova()` function
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.9 使用`anova()`函数比较嵌套模型
- en: '[PRE27]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, model 1 is nested within model 2\. The `anova()` function provides a simultaneous
    test that `Income` and `Frost` add to linear prediction above and beyond `Population`
    and `Illiteracy`. Because the test is nonsignificant (p = .994), you conclude
    that they don’t add to the linear prediction, and you’re justified in dropping
    them from your model.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型1嵌套在模型2中。`anova()`函数提供了一个同时测试，即`Income`和`Frost`是否在`Population`和`Illiteracy`之上增加了线性预测。由于测试不显著（p
    = .994），你得出结论，它们没有增加线性预测，因此你有理由从模型中删除它们。
- en: The Akaike Information Criterion (AIC) provides another method for comparing
    models. The index takes into account a model’s statistical fit and the number
    of parameters needed to achieve this fit. Models with smaller AIC values—indicating
    adequate fit with fewer parameters—are preferred. The criterion is provided by
    the `AIC()` function (see the following listing).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 阿卡伊克信息准则（AIC）为比较模型提供了另一种方法。该指数考虑了模型的统计拟合度和实现这种拟合所需的参数数量。具有较小AIC值的模型——表示使用较少参数即可达到适当的拟合——更受欢迎。该准则由`AIC()`函数提供（见以下列表）。
- en: Listing 8.10 Comparing models with the AIC
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.10 使用AIC比较模型
- en: '[PRE28]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The AIC values suggest that the model without `Income` and `Frost` is the better
    model. Note that although the ANOVA approach requires nested models, the AIC approach
    doesn’t.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: AIC值表明，没有`Income`和`Frost`的模型是更好的模型。请注意，尽管ANOVA方法需要嵌套模型，但AIC方法不需要。
- en: Comparing two models is relatively straightforward, but what do you do when
    there are 4, or 10, or 100 possible models to consider? That’s the topic of the
    next section.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个模型相对简单，但当有4个、10个或100个可能的模型需要考虑时，你该怎么办？这就是下一节的主题。
- en: 8.6.2 Variable selection
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.2 变量选择
- en: Two popular approaches to selecting a final set of predictor variables from
    a larger pool of candidate variables are stepwise methods and all-subsets regression.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 从更大的候选变量池中选择最终预测变量集的两种流行方法是逐步方法和所有子集回归。
- en: Stepwise regression
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步回归
- en: In stepwise selection, variables are added to or deleted from a model one at
    a time until some stopping criterion is reached. For example, in *forward stepwise*
    regression, you add predictor variables to the model one at a time, stopping when
    the addition of variables would no longer improve the model. In *backward stepwise*
    regression, you start with a model that includes all predictor variables, and
    then you delete them one at a time until removing variables would degrade the
    quality of the model. In *stepwise stepwise* regression (usually called stepwise
    to avoid sounding silly), you combine the forward and backward *stepwise* approaches.
    Variables are entered one at a time, but at each step, the variables in the model
    are reevaluated, and those that don’t contribute to the model are deleted. A predictor
    variable may be added to, and deleted from, a model several times before a final
    solution is reached.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在逐步选择中，变量一次添加到或从模型中删除，直到达到某个停止标准。例如，在*正向逐步*回归中，你一次添加一个预测变量到模型中，直到添加变量不再提高模型质量为止。在*反向逐步*回归中，你从一个包含所有预测变量的模型开始，然后逐个删除它们，直到删除变量会降低模型质量为止。在*逐步逐步*回归（通常称为逐步以避免听起来愚蠢）中，你结合了正向和反向*逐步*方法。变量一次添加到模型中，但在每一步，模型中的变量都会重新评估，并且那些对模型没有贡献的变量会被删除。预测变量可能在最终解决方案达到之前被多次添加到和从模型中删除。
- en: The implementation of stepwise regression methods varies by the criteria used
    to enter or remove variables. The `step()` function in base R performs stepwise
    model selection (forward, backward, or stepwise) using an AIC criterion. The next
    listing applies backward stepwise regression to the multiple regression problem.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤回归方法的实现因用于进入或删除变量的标准而异。基础R中的`step()`函数使用AIC标准执行逐步模型选择（正向、反向或逐步）。下面的列表将反向逐步回归应用于多元回归问题。
- en: Listing 8.11 Backward stepwise regression
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.11 反向逐步回归
- en: '[PRE29]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You start with all four predictors in the model. For each step, the AIC column
    provides the model AIC resulting from the deletion of the variable listed in that
    row. The AIC value for `<none>` is the model AIC if no variables are removed.
    In the first step, `Frost` is removed, decreasing the AIC from 97.75 to 95.75\.
    In the second step, `Income` is removed, decreasing the AIC to 93.76\. Deleting
    any more variables would increase the AIC, so the process stops.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 你从模型中的所有四个预测变量开始。对于每一步，AIC列提供了删除该行中列出的变量后得到的模型AIC。`<none>`的AIC值是如果没有变量被删除的模型AIC。在第一步中，`Frost`被删除，AIC从97.75降至95.75。在第二步中，`Income`被删除，AIC降至93.76。删除更多变量会增加AIC，因此过程停止。
- en: Stepwise regression is controversial. Although it may find a good model, there’s
    no guarantee that it will find the “best” model because not every possible model
    is evaluated. An approach that attempts to overcome this limitation is `all subsets
    regression`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步回归是有争议的。尽管它可能找到一个好的模型，但无法保证它会找到“最佳”模型，因为并非每个可能模型都被评估。试图克服这一局限性的方法是“所有子集回归”。
- en: All subsets regression
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子集回归
- en: In all subsets regression, every possible model is inspected. The analyst can
    choose to have all possible results displayed or ask for the `nbest` models of
    each subset size (one predictor, two predictors, and so on). For example, if `nbest=2`,
    the two best one-predictor models are displayed, followed by the two best two-predictor
    models, followed by the two best three-predictor models, up to a model with all
    predictors.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有子集回归中，检查了每个可能模型。分析师可以选择显示所有可能的结果，或者要求显示每个子集大小的`nbest`模型（一个预测变量、两个预测变量，等等）。例如，如果`nbest=2`，则显示两个最佳的单预测变量模型，然后是两个最佳的二预测变量模型，然后是两个最佳的三预测变量模型，直到包含所有预测变量的模型。
- en: All subsets regression is performed using the `regsubsets()` function from the
    `leaps` package. You can choose the R-squared, Adjusted R-squared, or Mallows
    Cp statistic as your criterion for reporting “best” models.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子集回归是通过`leaps`包中的`regsubsets()`函数执行的。你可以选择R-squared、调整后的R-squared或Mallows
    Cp统计量作为报告“最佳”模型的准则。
- en: As you’ve seen, R-squared is the amount of variance accounted for in the response
    variable by the predictors variables. Adjusted R-squared is similar but takes
    into account the number of parameters in the model. R-squared always increases
    with the addition of predictors. When the number of predictors is large compared
    to the sample size, this can lead to significant overfitting. The adjusted R-squared
    is an attempt to provide a more honest estimate of the population R-squared—one
    that’s less likely to take advantage of chance variation in the data.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，R-squared是预测变量在响应变量中解释的方差量。调整后的R-squared与此类似，但考虑了模型中的参数数量。随着预测变量的增加，R-squared总是增加。当预测变量的数量相对于样本量很大时，这可能导致显著的过拟合。调整后的R-squared试图提供一个更诚实的总体R-squared估计——一个不太可能利用数据中偶然变化的估计。
- en: In the following listing, we’ll apply all subsets regression to the `states`
    data. The `leaps` package presents the results in a plot, but I've found that
    many people are confused by this graph. The code below presents the same results
    in the form of a table, which I believe will be easier to understand.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们将对所有子集回归应用于`states`数据。`leaps`包以图表的形式展示结果，但我发现很多人对此图表感到困惑。下面的代码以表格的形式展示了相同的结果，我认为这将更容易理解。
- en: Listing 8.12 All subsets regression
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.12 所有子集回归
- en: '[PRE30]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Each line of the table represents a model. The first column indicates the number
    of predictors in the model. The second column is the scale (adjusted R-squared
    in this case) used to describe each model''s fit, and rows are sorted by this
    scale. (Note: Other scale values can be used in place of `adjr2`. See `?regsubsets`
    for a list of options.) The ones and zeros in the row indicate which variables
    are included or excluded from the model.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中的每一行代表一个模型。第一列表示模型中的预测变量数量。第二列是用于描述每个模型拟合度的尺度（在本例中为调整后的R-squared），行按此尺度排序。（注意：可以使用其他尺度值代替`adjr2`。有关选项列表，请参阅`?regsubsets`。）行中的1和0表示哪些变量被包含或排除在模型之外。
- en: For example, a model based on the single predictor `Income` has an adjusted
    R-square of 0.033\. A model with the predictors `Population`, `Illiteracy`, and
    `Income` has an adjusted R-square of 0.539\. In contrast, a model using the predictors
    `Population` and `Illiteracy` alone has an adjusted R-square of 0.548\. Here you
    see that a model with fewer predictors actually has a larger adjusted R-square
    (something that can’t happen with an unadjusted R-square). The table suggests
    that the two-predictor model (`Population` and `Illiteracy`) is the best.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，基于单一预测变量`Income`的模型具有0.033的调整后的R-square。具有预测变量`Population`、`Illiteracy`和`Income`的模型具有0.539的调整后的R-square。相比之下，仅使用预测变量`Population`和`Illiteracy`的模型具有0.548的调整后的R-square。在这里，你可以看到具有较少预测变量的模型实际上具有更大的调整后的R-square（这是未经调整的R-square不可能发生的情况）。表格表明，双预测变量模型（`Population`和`Illiteracy`）是最好的。
- en: In most instances, all subsets regression is preferable to stepwise regression
    because more models are considered. But when the number of predictors is large,
    the procedure can require significant computing time. In general, automated variable-selection
    methods should be seen as an aid rather than a directing force in model selection.
    A well-fitting model that doesn’t make sense doesn’t help you. Ultimately, it’s
    your knowledge of the subject matter that should guide you.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，所有子集回归比逐步回归更可取，因为考虑了更多的模型。但当预测变量的数量很大时，该过程可能需要大量的计算时间。一般来说，自动变量选择方法应被视为模型选择的辅助工具，而不是主导力量。一个拟合良好但无意义的模型并不能帮助你。最终，你应该根据你对主题知识的了解来指导自己。
- en: 8.7 Taking the analysis further
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 深入分析
- en: We’ll end our discussion of regression by considering methods for assessing
    model generalizability and predictor relative importance.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过考虑评估模型泛化能力和预测变量相对重要性的方法来结束对回归的讨论。
- en: 8.7.1 Cross-validation
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1 交叉验证
- en: In the previous section, we examined methods for selecting the variables to
    include in a regression equation. When description is your primary goal, the selection
    and interpretation of a regression model signal the end of your labor. But when
    your goal is prediction, you can justifiably ask, “How well will this equation
    perform in the real world?”
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了选择回归方程中包含的变量的方法。当描述是你的主要目标时，选择和解释回归模型标志着你工作的结束。但是，当你的目标是预测时，你可以合理地询问，“这个方程在现实世界中表现如何？”
- en: By definition, regression techniques obtain model parameters that are optimal
    for a given set of data. In OLS regression, the model parameters are selected
    to minimize the sum of squared errors of prediction (residuals) and, conversely,
    maximize the amount of variance accounted for in the response variable (R-squared).
    Because the equation has been optimized for the given set of data, it's unlikely
    perform as well with a new set of data.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，回归技术获得的是针对给定数据集最优的模型参数。在OLS回归中，模型参数被选择以最小化预测（残差）的平方误差之和，反之，最大化响应变量（R平方）中解释的方差量。因为方程已经针对给定数据集进行了优化，所以它不太可能在新数据集上表现良好。
- en: We began this chapter with an example involving a research physiologist who
    wanted to predict the number of calories an individual will burn from the duration
    and intensity of their exercise, age, gender, and BMI. If you fit an OLS regression
    equation to this data, you’ll obtain model parameters that uniquely maximize the
    R-squared for this particular set of observations. But our researcher wants to
    use this equation to predict the calories burned by individuals in general, not
    only those in the original study. You know that the equation won’t perform as
    well with a new sample of observations, but how much will you lose? Cross-validation
    is a useful method for evaluating the generalizability of a regression equation.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本章以一个研究生理学家为例开始，他想要预测个体从运动持续时间、强度、年龄、性别和BMI中燃烧的卡路里数量。如果你将OLS回归方程拟合到这些数据上，你会得到一组独特的模型参数，这些参数最大化了这一特定观察集的R平方。但我们的研究人员想要使用这个方程来预测一般个体的卡路里消耗，而不仅仅是原始研究中的个体。你知道这个方程在新观察样本中表现不会那么好，但你会损失多少？交叉验证是评估回归方程泛化能力的一种有用方法。
- en: In cross-validation, a portion of the data is selected as the training sample,
    and a portion is selected as the holdout sample. A regression equation is developed
    on the training sample and then applied to the holdout sample. Because the holdout
    sample wasn’t involved in the selection of the model parameters, the performance
    on this sample is a more accurate estimate of the operating characteristics of
    the model with new data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中，一部分数据被选为训练样本，另一部分被选为保留样本。在训练样本上开发回归方程，然后将其应用于保留样本。因为保留样本没有参与模型参数的选择，所以在这个样本上的表现是对模型在新数据上运行特性的更准确估计。
- en: In k-fold cross-validation, the sample is divided into *k* subsamples. Each
    of the *k* subsamples serves as a holdout group, and the combined observations
    from the remaining *k* – 1 subsamples serve as the training group. The performance
    for the *k* prediction equations applied to the *k* holdout samples is recorded
    and then averaged. (When *k* equals *n*, the total number of observations, this
    approach is called *jackknifing*.)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，样本被分为*k*个子样本。每个*k*个子样本都作为保留组，而从剩余*k* - 1个子样本中合并的观察值作为训练组。记录应用于*k*个保留样本的*k*个预测方程的性能，然后取平均值。（当*k*等于*n*，即观察值的总数时，这种方法称为“刀切法”。）
- en: You can perform k-fold cross-validation using the `crossval()` function in the
    `bootstrap` package. The following listing provides a function (called `shrinkage()`)
    for cross-validating a model’s R-square statistic using k-fold cross-validation.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`bootstrap`包中的`crossval()`函数执行k折交叉验证。以下列表提供了一个函数（称为`shrinkage()`），用于使用k折交叉验证交叉验证模型的R平方统计量。
- en: Listing 8.13 Function for k-fold cross-validated R-square
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.13：k折交叉验证R平方函数
- en: '[PRE31]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using this listing, you define your functions, create a matrix of predictor
    and predicted values, get the raw R-squared and residual standard error, and get
    the cross-validated R-squared and residual standard error. (Chapter 12 covers
    bootstrapping in detail.)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个列表，你可以定义你的函数，创建一个预测值和预测值的矩阵，获取原始的R平方和残差标准误差，以及获取交叉验证的R平方和残差标准误差。（第12章详细介绍了自助法。）
- en: 'The `shrinkage()` function is then used to perform a 10-fold cross-validation
    with the `states` data, using a model with all four predictor variables:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`shrinkage()`函数对`states`数据进行10折交叉验证，使用包含所有四个预测变量的模型：
- en: '[PRE32]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can see that the R-square based on the sample (0.567) is overly optimistic.
    A better estimate of the amount of variance in murder rates that this model will
    account for with new data is the cross-validated R-square (0.356). (Note that
    observations are assigned to the *k* groups randomly, so a random number seed
    is provided to make the results reproducible.)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，基于样本的R-square（0.567）过于乐观。对于这个模型将用新数据解释的谋杀率变化量的更好估计是交叉验证的R-square（0.356）。（注意，观测值是随机分配到k个组中的，因此提供了一个随机数种子以使结果可重复。）
- en: 'You could use cross-validation in variable selection by choosing a model that
    demonstrates better generalizability. For example, a model with two predictors
    (Population and `Illiteracy`) shows less R-square shrinkage than the full model:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在变量选择中使用交叉验证，通过选择一个表现出更好泛化能力的模型。例如，具有两个预测变量（人口和`Illiteracy`）的模型比完整模型具有更小的R-square缩减：
- en: '[PRE33]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This may make the two-predictor model a more attractive alternative.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能使得双预测器模型成为一个更有吸引力的替代方案。
- en: All other things being equal, a regression equation that’s based on a larger
    training sample and one that’s more representative of the population of interest
    will cross-validate better. You’ll get less R-squared shrinkage and make more
    accurate predictions.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有其他条件相同的情况下，基于更大训练样本且更能代表目标人群的回归方程将具有更好的交叉验证效果。你将获得更小的R-squared缩减，并做出更准确的预测。
- en: 8.7.2 Relative importance
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2 相对重要性
- en: Up to this point in the chapter, we’ve been asking, “Which variables are useful
    for predicting the outcome?” But often your real interest is, “Which variables
    are most important in predicting the outcome?” You implicitly want to rank-order
    the predictors in terms of relative importance. There may be practical grounds
    for asking the second question. For example, if you could rank-order leadership
    practices by their relative importance for organizational success, you could help
    managers focus on the behaviors they most need to develop.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们一直在问，“哪些变量对预测结果有用？”但通常你的真正兴趣是，“哪些变量在预测结果中最为重要？”你隐含地希望根据相对重要性对预测变量进行排序。提出第二个问题可能有实际依据。例如，如果你可以根据相对重要性对领导实践进行排序，以组织成功为标准，你就可以帮助管理者专注于他们最需要发展的行为。
- en: If predictor variables were uncorrelated, this would be simple. You would rank-order
    the predictor variables by their correlation with the response variable. In most
    cases, though, the predictors are correlated with each other, and this complicates
    the task significantly.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果预测变量不相关，这将很简单。你会根据预测变量与响应变量的相关性对预测变量进行排序。然而，在大多数情况下，预测变量彼此相关，这显著增加了任务的复杂性。
- en: 'Many attempts have been made to develop a means for assessing the relative
    importance of predictors. The simplest is to compare standardized regression coefficients,
    which describe the expected change in the response variable (expressed in standard
    deviation units) for a standard deviation change in a predictor variable, holding
    the other predictor variables constant. You can obtain the standardized regression
    coefficients in R by standardizing each of the variables in your dataset to a
    mean of 0 and standard deviation of 1 using the `scale()` function, before submitting
    the dataset to a regression analysis. (Note that because the `scale()` function
    returns a matrix and the `lm()` function requires a data frame, you convert between
    the two in an intermediate step.) The code and results for the multiple regression
    problem are shown here:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 已经尝试了许多方法来开发一种评估预测变量相对重要性的手段。最简单的方法是比较标准化回归系数，它描述了预测变量标准差变化时响应变量的预期变化（以标准差单位表示），同时保持其他预测变量不变。你可以在R中使用`scale()`函数将你的数据集中的每个变量标准化到均值为0和标准差为1，然后将数据集提交给回归分析之前获得标准化回归系数。（注意，因为`scale()`函数返回一个矩阵，而`lm()`函数需要一个数据框，你需要在中间步骤中进行转换。）多回归问题的代码和结果如下所示：
- en: '[PRE34]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here you see that a one-standard-deviation increase in illiteracy rate yields
    a 0.68 standard deviation increase in murder rate when controlling for population,
    income, and temperature. Using standardized regression coefficients as your guide,
    `Illiteracy` is the most important predictor and `Frost` is the least.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到在控制人口、收入和温度的情况下，文盲率的每增加一个标准差会导致谋杀率增加 0.68 个标准差。使用标准化回归系数作为指导，`文盲率`
    是最重要的预测因子，而 `霜冻` 是最不重要的。
- en: Many other attempts have been made at quantifying relative importance, which
    can be thought of as the contribution each predictor makes to R-square, both alone
    and in combination with other predictors. Several possible approaches to relative
    importance are captured in the `relaimpo` package written by Ulrike Grömping ([http://mng.bz/KDYF](http://mng.bz/KDYF)).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 已经尝试了许多其他方法来量化相对重要性，这可以被认为是每个预测因子对 R-square 的贡献，无论是单独还是与其他预测因子结合。几种可能的相对重要性方法被
    Ulrike Grömping 编写的 `relaimpo` 包所捕捉（[http://mng.bz/KDYF](http://mng.bz/KDYF)）。
- en: A new method called relative weights shows significant promise. The method closely
    approximates the average increase in R-square obtained by adding a predictor variable
    across all possible submodels (Johnson, 2004; Johnson and LeBreton, 2004; LeBreton
    and Tonidandel, 2008). A function for generating relative weights is provided
    in the next listing.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为相对权重的新的方法显示出显著的潜力。该方法紧密地近似了通过添加预测变量到所有可能的子模型中获得的平均 R-square 增加量（Johnson，2004；Johnson
    和 LeBreton，2004；LeBreton 和 Tonidandel，2008）。下一个列表中提供了一个生成相对权重的函数。
- en: Listing 8.14 `relweights()` for calculating relative importance of predictors
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.14 `relweights()` 用于计算预测因子的相对重要性
- en: '[PRE35]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note The code in listing 8.16 is adapted from an SPSS program Dr. Johnson has
    generously provided. See Johnson, 2000, for an explanation of how the relative
    weights are derived.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表 8.16 中的代码是从 Johnson 博士慷慨提供的 SPSS 程序中改编的。有关相对权重的推导解释，请参阅 Johnson，2000。
- en: In the next listing, the `relweights()` function is applied to the `states`
    data with murder rate predicted by the population, illiteracy, income, and temperature.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，`relweights()` 函数被应用于 `states` 数据，其中谋杀率由人口、文盲率、收入和温度预测。
- en: Listing 8.15 Applying the `relweights()` function
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.15 应用 `relweights()` 函数
- en: '[PRE36]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You can see from the resulting plot (figure 8.13) that the total amount of variance
    accounted for by the model (R-square = 0.567) has been divided among the predictor
    variables. `Illiteracy` accounts for 59% of the R-square, `Frost` accounts for
    20.79%, and so forth. Based on the method of relative weights, `Illiteracy` has
    the greatest relative importance, followed by `Frost`, `Population`, and `Income`,
    in that order.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果图（图 8.13）中可以看出，模型解释的总方差（R-square = 0.567）被分配给了预测变量。`文盲率` 解释了 59% 的 R-square，`霜冻`
    解释了 20.79%，等等。根据相对权重方法，`文盲率` 具有最大的相对重要性，其次是 `霜冻`、`人口` 和 `收入`，顺序如下。
- en: '![](Images/CH08_F13_Kabacoff3.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F13_Kabacoff3.png)'
- en: Figure 8.13 Dot chart of relative weights for the `states` multiple regression
    problem. Larger weights indicate relatively more important predictors. For example,
    `Illiteracy` accounts for 59% of the total explained variance (0.567), whereas
    `Income` only accounts for 5.49%. Thus, `Illiteracy` has greater relative importance
    than `Income` in this model.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 `states` 多元回归问题的相对权重点图。较大的权重表示相对更重要的预测因子。例如，`文盲率` 解释了总解释方差（0.567）的 59%，而
    `收入` 只解释了 5.49%。因此，在这个模型中，`文盲率` 的相对重要性大于 `收入`。
- en: Relative importance measures (and, in particular, the method of relative weights)
    have wide applicability. They come much closer to our intuitive conception of
    relative importance than standardized regression coefficients do, and I expect
    to see their use increase dramatically in coming years.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 相对重要性度量（特别是相对权重方法）具有广泛的应用。它们比标准化回归系数更接近我们对于相对重要性的直观理解，我预计在未来的几年里它们的使用将会显著增加。
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Regression analysis is a highly interactive and iterative approach that involves
    fitting models, assessing their fit to statistical assumptions, modifying both
    the data and the models, and refitting to arrive at a final result.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归分析是一种高度交互和迭代的分析方法，涉及拟合模型、评估其与统计假设的拟合度、修改数据和模型，并重新拟合以达到最终结果。
- en: Regression diagnostics are used to assess the data’s fit to statistical assumptions
    and select methods for modifying the model or the data to meet these assumptions
    more closely.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归诊断用于评估数据与统计假设的拟合程度，并选择修改模型或数据以更接近这些假设的方法。
- en: Numerous methods are available for selecting the variables to include in a final
    regression model, including the use of significance tests, fit statistics, and
    automated solutions such as stepwise and all subsets regression.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于选择最终回归模型中包含的变量的方法有很多，包括使用显著性检验、拟合统计量以及自动化解决方案，如逐步回归和所有子集回归。
- en: Cross-validation can be used to evaluate a predictive model's likely performance
    on new samples of data.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证可用于评估预测模型在新数据样本上的可能性能。
- en: 'The method of relative weights can be used to address the thorny problem of
    variable importance: identifying which variables are the most important for predicting
    an outcome.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对权重法可用于解决变量重要性这一棘手问题：确定哪些变量对于预测结果最为重要。
