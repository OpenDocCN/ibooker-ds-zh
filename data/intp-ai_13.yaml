- en: 9 Path to explainable AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 可解释人工智能之路
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A recap of interpretability techniques learned in this book
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结本书中学到的可解释性技术
- en: Understanding the properties of an explainable AI system
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解可解释人工智能系统的特性
- en: Common questions asked of an explainable AI system and applying interpretability
    techniques to answer them
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的问题以及对可解释人工智能系统应用可解释性技术来回答这些问题
- en: Using counterfactual examples to come up with contrastive explanations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反事实示例来提出对比性解释
- en: We are now approaching the end of our journey through the world of interpretable
    AI. Figure 9.1 provides a map of this journey. Let’s take a moment to reflect
    on and to summarize what we have learned. Interpretability is all about understanding
    the cause and effect within an AI system. It is the degree to which we can consistently
    estimate what the underlying models in the AI system will predict given an input,
    understand how the models came up with the prediction, understand how the prediction
    changes with modifications to the input or algorithmic parameters, and finally
    understand when the models have made a mistake. Interpretability is becoming increasingly
    important because machine learning models are proliferating in various industries
    such as finance, healthcare, technology, and legal, to name a few. Decisions made
    by such models require transparency and fairness. The techniques that we have
    learned in this book are powerful tools to improve transparency and ensure fairness.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正接近通过可解释人工智能世界的旅程的终点。图9.1提供了这次旅程的地图。让我们花点时间来反思和总结我们所学的。可解释性全部关于理解人工智能系统中的因果关系。这是我们在给定输入的情况下，能够持续估计人工智能系统中的底层模型将预测什么，理解模型是如何得出预测的，理解预测如何随着输入或算法参数的修改而变化，以及最终理解模型何时犯错的程度。由于机器学习模型在金融、医疗保健、技术和法律等各个行业中日益增多，可解释性正变得越来越重要。这些模型所做的决策需要透明度和公平性。本书中学到的技术是提高透明度和确保公平性的强大工具。
- en: We looked at two broad classes of machine learning models in this book—white-box
    and black-box models—that fall on the spectrum of interpretability and predictive
    power. White-box models are inherently transparent and are straightforward to
    interpret. However, they have low to medium predictive power. We specifically
    focused on linear regression, logistic regression, decision trees, and generalized
    additive models (GAMs) and learned how to interpret them by understanding the
    internals of the model. Black-box models are inherently opaque and are harder
    to interpret, but they offer much higher predictive power. We focused most of
    our attention in this book on interpreting black-box models such as tree ensembles
    and neural networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们研究了两种广泛的机器学习模型类别——白盒模型和黑盒模型，它们位于可解释性和预测能力的光谱上。白盒模型本质上是透明的，易于解释。然而，它们的预测能力较低到中等。我们特别关注线性回归、逻辑回归、决策树和广义加性模型（GAMs），并通过理解模型的内部结构来学习如何解释它们。黑盒模型本质上是不可透见的，难以解释，但提供了更高的预测能力。在本书中，我们主要关注解释黑盒模型，如树集成和神经网络。
- en: '![](../Images/CH09_F01_Thampi.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F01_Thampi.png)'
- en: Figure 9.1 Map of our journey through the world of interpretable AI
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 通过可解释人工智能世界的旅程地图
- en: We have two ways of interpreting black-box models. One is to interpret model
    processing—that is, understand how the model processes the inputs and arrives
    at the final prediction. The other way, interpreting model representations, is
    applicable only to deep neural networks. To interpret model processing, we learned
    about post hoc model-agnostic methods such as partial dependence plots (PDPs)
    and feature interaction plots to understand the global effects of the input features
    on the model’s predictions. We also learned about post hoc model-agnostic methods
    that are local in scope, such as local interpretable model-agnostic explanations
    (LIME), SHapley Additive exPlanations (SHAP), and anchors. We can use these methods
    to explain how the model arrived at individual predictions. We also used visual
    attribution methods, such as saliency maps, to understand what input features
    or image pixels were important for neural networks used for visual tasks. To interpret
    model representations, we learned how to dissect neural networks and understand
    what representations of the data are learned by the intermediate or hidden layers
    in the network. We also learned how to visualize high-dimensional representations
    learned by the model using techniques like principal component analysis (PCA)
    and t-distributed stochastic neighbor embedding (t-SNE).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种解释黑盒模型的方法。一种是对模型处理过程进行解释——即理解模型如何处理输入并得出最终预测。另一种方法是解释模型表示，这仅适用于深度神经网络。为了解释模型处理过程，我们学习了诸如局部依赖性图（PDPs）和特征交互图等后验模型无关方法，以理解输入特征对模型预测的全球影响。我们还学习了局部范围内的后验模型无关方法，例如局部可解释模型无关解释（LIME）、SHapley
    加性解释（SHAP）和锚点。我们可以使用这些方法来解释模型如何得出个别预测。我们还使用了视觉归因方法，如显著性图，以理解对视觉任务使用的神经网络中哪些输入特征或图像像素是重要的。为了解释模型表示，我们学习了如何剖析神经网络并理解网络中中间或隐藏层学习的数据表示。我们还学习了如何使用主成分分析（PCA）和t分布随机邻域嵌入（t-SNE）等技术来可视化模型学习的高维表示。
- en: We finally focused on the topic of fairness and learned various fairness notions
    and how to make use of interpretability techniques to measure fairness. We also
    learned how to mitigate fairness using various preprocessing techniques, such
    as fairness through unawareness and an iterative label bias correction technique.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终聚焦于公平性的话题，学习了各种公平性概念以及如何利用可解释性技术来衡量公平性。我们还学习了如何通过各种预处理技术来缓解公平性问题，例如通过无意识实现的公平性和迭代标签偏差校正技术。
- en: In this book, we made a strong distinction between interpretability and explainability.
    Interpretability is mainly about answering the *how* question—*how* does the model
    work and *how* did it arrive at a prediction? Explainability goes beyond interpretability
    in that it helps us answer the *why* question—*why* did the model make one prediction
    as opposed to another? Interpretability is mostly discernible by experts who are
    building, deploying, or using the AI system, and these techniques are building
    blocks that will help you get to explainability. We will focus on the path to
    explainable AI in this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们明确区分了可解释性和可解释性。可解释性主要关于回答“如何”的问题——模型是如何工作的，以及它是如何得出预测的？可解释性超越了可解释性，因为它帮助我们回答“为什么”的问题——为什么模型做出了一个预测而不是另一个？可解释性主要是由构建、部署或使用AI系统的专家所识别的，这些技术是帮助你达到可解释性的基石。我们将专注于本章中可解释人工智能的路径。
- en: 9.1 Explainable AI
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 可解释人工智能
- en: Let’s look at a concrete example of an explainable AI system and what is expected
    of it. We will use the same example from chapter 8 of predicting the income of
    adults in the United States. Given a set of input features such as education,
    occupation, age, gender, and race, let’s assume that we have trained a model that
    predicts whether an adult earns more than $50,000 per year. After applying the
    interpretability techniques learned in this book, let’s assume that we can now
    deploy this model as a service. This service could be used by the public to determine
    how much they can earn given their features as input. An explainable AI system
    should provide functionality for the users of this system to question the predictions
    made by the model and to challenge the decisions made because of those predictions.
    This is illustrated in figure 9.2, where the functionality of providing an explanation
    to the user is built into the explanation agent. The users can ask the agent various
    questions regarding the predictions made by the model, and the onus is on the
    agent to provide meaningful answers. One possible question that the user could
    ask, as illustrated in figure 9.2, is why the model predicted that their salary
    would be less than $50K.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个可解释人工智能系统的具体例子以及对其的期望。我们将使用第8章中预测美国成年人收入的相同例子。给定一组输入特征，如教育、职业、年龄、性别和种族，假设我们已经训练了一个模型，该模型预测成年人每年是否收入超过50,000美元。在应用本书中学到的可解释性技术后，假设我们现在可以将此模型作为服务部署。此服务可以被公众使用，根据他们的特征作为输入来确定他们可以赚多少钱。一个可解释的人工智能系统应该为系统用户提供质疑模型做出的预测和挑战因这些预测而做出的决策的功能。这如图9.2所示，其中向用户提供解释的功能已内置到解释代理中。用户可以向代理提出关于模型做出的预测的各种问题，代理有责任提供有意义的答案。如图9.2所示，用户可能提出的一个问题就是为什么模型预测他们的薪水会低于50K。
- en: '![](../Images/CH09_F02_Thampi.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F02_Thampi.png)'
- en: Figure 9.2 An illustration of an agent explaining the prediction made by a model
    to the user of the system
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 一个代理向系统用户解释模型预测的示意图
- en: The question asked by the user in figure 9.2, for illustration purposes, is
    focused on understanding how various feature values influence the model prediction.
    This is just one type of question that could be asked of the system. Table 9.1
    shows a few broad classes of questions that we can ask of the system and techniques
    that we have learned in this book that could be applied for such questions. As
    can be seen in the table, we are well equipped to answer questions on how the
    model works, what features are important, how the model arrived at a prediction
    for a specific case, and whether the model is fair and unbiased. As highlighted
    earlier, we are not well equipped to answer the why question and will briefly
    touch upon that in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 用户在图9.2中提出的问题，为了说明目的，关注的是理解各种特征值如何影响模型预测。这只是可能向系统提出的问题之一。表9.1展示了我们可以向系统提出的一些广泛类别的问题以及我们在本书中学到的可用于此类问题的技术。如表所示，我们已准备好回答关于模型如何工作、哪些特征很重要、模型如何对特定案例做出预测以及模型是否公平无偏的问题。正如之前所强调的，我们并不擅长回答“为什么”的问题，将在本章简要涉及这一点。
- en: Table 9.1 Question types and explanation methods
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 问题类型和解释方法
- en: '| Category of methods | Question types | Explanation methods |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 方法类别 | 问题类型 | 解释方法 |'
- en: '| Explain the model |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 解释模型 |'
- en: '*How* does the model work?'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*是如何工作的？'
- en: '*What* features or inputs are the most important for the model?'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哪些*特征或输入对模型来说是最重要的？'
- en: '|'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Model-dependent descriptions. (This book provides good descriptions on how various
    broad classes of models, both white-box and black-box, work.)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型依赖描述。（本书提供了关于各种广泛类别的模型（包括白盒和黑盒）如何工作的良好描述。）
- en: Global feature importance (chapters 2 and 3).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局特征重要性（第2章和第3章）。
- en: Model representations (chapters 6 and 7).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型表示（第6章和第7章）。
- en: '|'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Explain a prediction |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 解释预测 |'
- en: '*How* did the model arrive at this prediction for my case?'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*是如何对我的案例做出这个预测的？'
- en: '|'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Local feature importance (chapter 4).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地特征重要性（第4章）。
- en: Visual attribution methods (chapter 5).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉归因方法（第5章）。
- en: '|'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Fairness |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 公平性 |'
- en: '*How* does the model treat people from a certain protected group?'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*是如何对待某个特定受保护群体的成员的？'
- en: '*Is* the model biased against a group that I belong to?'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*是否对我的所属群体存在偏见？'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Fairness notions and measurements (chapter 8).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性概念和度量（第8章）。
- en: '|'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Contrastive or counterfactual |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 对比或反事实 |'
- en: '*Why* did the model predict this outcome for me?'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么*模型会为我预测这个结果？'
- en: '*Why* not another outcome?'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么*不是另一种结果？'
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Counterfactual explanations (to be discussed in this chapter).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反事实解释（本章将讨论）。
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Although the interpretability techniques that we have learned in this book will
    help us come up with answers to most of the questions highlighted in table 9.1,
    there is more that goes into providing the answer or explanation to the user.
    We need to know what information is relevant to the question being asked, how
    much information to provide in the explanation, and how the user receives or understands
    explanations (i.e., their background). A whole field, called explainable AI (XAI),
    is dedicated to solving this problem. The scope of XAI, as shown in figure 9.3,
    is not just artificial intelligence, of which machine learning is a specific subfield,
    but also looks to other fields such as human-computer interaction (HCI) and social
    science.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本书中学到的可解释性技术将帮助我们回答表9.1中突出的大部分问题，但提供答案或解释给用户的过程远不止于此。我们需要知道与所提问题相关的信息是什么，解释中需要提供多少信息，以及用户如何接收或理解解释（即他们的背景）。一个名为可解释人工智能（XAI）的整个领域致力于解决这个问题。如图9.3所示，XAI的范围不仅限于人工智能，其中机器学习是一个特定的子领域，而且还涉及其他领域，如人机交互（HCI）和社会科学。
- en: '![](../Images/CH09_F03_Thampi.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F03_Thampi.png)'
- en: Figure 9.3 Scope of explainable AI (XAI)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 可解释人工智能（XAI）的范围
- en: 'Tim Miller published an important research paper (available at [https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf))
    on insights from social sciences that are relevant to XAI. The following are key
    findings in this paper:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Tim Miller发表了一篇重要的研究论文（可在[https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf)找到），这篇论文探讨了与社会科学相关的见解，这些见解与XAI相关。以下是该论文中的关键发现：
- en: '*Explanations are contrastive*—People usually do not just ask why the model
    predicted a specific outcome but rather why not another outcome. This is highlighted
    as the contrastive or counterfactual explanation method in table 9.1, and we will
    briefly discuss this in the next section.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解释是对比性的*——人们通常不仅仅询问模型为什么预测了特定的结果，而是为什么不是另一种结果。这在表9.1中被突出显示为对比性或反事实解释方法，我们将在下一节简要讨论这一点。'
- en: '*Explanations are usually selected in a biased way*—If a lot of explanations
    or causes for a prediction are provided to the user, then the user typically selects
    only one or two and the selection is usually biased. It is, therefore, important
    to know how much information to provide and what information is most relevant
    for the explanation.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解释通常是以偏概全的方式选择的*——如果向用户提供了许多解释或预测的原因，那么用户通常只会选择一两个，而且选择通常是偏颇的。因此，了解提供多少信息以及哪些信息对解释最相关是很重要的。'
- en: '*Explanations are social*—The transfer of information from the AI system to
    the user must be interactive and in the form of a conversation. It is, therefore,
    important to have an explanation agent, as illustrated in figure 9.2, that can
    comprehend questions and provide meaningful answers. The user must be at the center
    of this interaction, and it is important to look to the field of HCI to build
    such a system.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解释是社会的*——从AI系统到用户的 信息传递必须是互动的，并以对话的形式进行。因此，拥有一个如图9.2所示的解释代理，它能够理解问题并提供有意义的答案，是非常重要的。用户必须处于这个互动的中心，并且重要的是要关注HCI领域来构建这样的系统。'
- en: In the following section, we will specifically look at a technique that can
    be used to provide contrastive or counterfactual explanations, that is, answer
    the why and why not questions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将具体探讨一种可以用来提供对比性或反事实解释的技术，即回答为什么和为什么不的问题。
- en: 9.2 Counterfactual explanations
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 反事实解释
- en: Counterfactual explanations (also known as contrastive explanations) can be
    used to explain why a model predicted a given value as opposed to another. Let’s
    look at a concrete example. We will use the adult income prediction model, which
    is a binary classification problem, and focus on just two input features—age and
    education—for ease of visualization.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实解释（也称为对比性解释）可以用来解释为什么模型预测了某个值而不是另一个值。让我们看看一个具体的例子。我们将使用成人收入预测模型，这是一个二元分类问题，并且为了便于可视化，我们只关注两个输入特征——年龄和教育。
- en: '![](../Images/CH09_F04_Thampi.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F04_Thampi.png)'
- en: Figure 9.4 An illustration of counterfactual examples
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 反事实示例的说明
- en: 'These two features are shown in figure 9.4 as a two-dimensional plane. The
    decision boundary for the adult income model is also shown as a curve on the plane
    that separates the bottom part from the top part. For adults in the bottom part
    of the plane, the model predicts an income that is less than or equal to $50K,
    and for adults in the top part of the plane, the model predicts an income of greater
    than $50K. Let’s assume that we have an adult who provides inputs to the system
    to predict how much income they will earn. This is labeled “Original Input” in
    figure 9.4\. This adult has a high school education, and let’s assume that the
    age is 30 (this is irrelevant for this example). Because this input falls below
    the decision boundary, the model will predict that the adult will earn an income
    that is less than $50K. The user then poses the question: why is my income less
    than $50K and not greater than $50K?'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个特征在图9.4中显示为一个二维平面。成人收入模型的决策边界也显示为平面上的一条曲线，将平面下部分与上部分分开。对于平面下部分的成年人，模型预测的收入小于或等于$50K，而对于平面上部分的成年人，模型预测的收入大于$50K。假设我们有一个成年人向系统提供输入以预测他们将获得多少收入。这在图9.4中标记为“原始输入”。这位成年人受过高中教育，假设年龄为30岁（在这个例子中这不相关）。因为这个输入低于决策边界，模型将预测这位成年人将获得小于$50K的收入。然后用户提出问题：为什么我的收入小于$50K而不是大于$50K？
- en: A counterfactual or contrastive explanation will provide examples, where in
    a counterfactual world, if that user satisfied certain criteria, then it will
    result in their desired outcome—earn an income greater than $50K. The counterfactual
    examples are marked in figure 9.4\. They show that if the user’s education level
    was higher—bachelor’s, master’s, or doctorate—then they would have a higher chance
    of earning more than $50K.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实或对比解释将提供示例，在一个反事实世界中，如果该用户满足某些标准，那么它将导致他们获得期望的结果——获得大于$50K的收入。反事实示例在图9.4中标记。它们表明，如果用户的受教育程度更高——学士、硕士或博士——那么他们获得超过$50K收入的机会就更高。
- en: 'How do we generate these counterfactual examples? The whole process, described
    in figure 9.5, consists of an explainer that takes the following as input:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何生成这些反事实示例？整个过程，如图9.5所示，包括一个解释者，它将以下内容作为输入：
- en: '*Original input*—The input provided by the user'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原始输入*—用户提供的输入'
- en: '*Desired outcome*—The outcome desired by the user'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*期望结果*—用户期望得到的结果'
- en: '*Counterfactual example count*—The number of counterfactual examples to show
    in the explanation'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反事实示例数量*—在解释中要展示的反事实示例数量'
- en: '*Model*—The model used for prediction to obtain the predictions for the counterfactual
    examples'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*—用于预测以获得反事实示例预测的模型'
- en: '![](../Images/CH09_F05_Thampi.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F05_Thampi.png)'
- en: Figure 9.5 Counterfactual generation process
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 反事实生成过程
- en: 'The explainer then runs an algorithm to generate the counterfactual examples.
    It is essentially an optimization problem of finding counterfactual examples such
    that the following criteria are met:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 解释者随后运行一个算法来生成反事实示例。这本质上是一个寻找反事实示例的优化问题，以满足以下标准：
- en: The model output for the counterfactual example is as close to the desired outcome
    as possible.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反事实示例的模型输出尽可能接近期望结果。
- en: The counterfactual example is also close to the original input in the feature
    space, that is, the values of a minimum set of high-value features are changed
    to obtain the desired outcome.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反事实示例在特征空间中也接近原始输入，即，通过更改一组高价值特征的最小值来获得期望结果。
- en: 'In this chapter, we will focus on one popular technique called diverse counterfactual
    explanations (DiCE) to generate the counterfactual explanations. In DiCE, the
    optimization problem is formulated as we did earlier. Features are perturbed in
    such a way that they are diverse and feasible to change, and the desired outcome
    of the user is attained. The mathematical details are beyond the scope of this
    book, but let’s use the DiCE library to generate counterfactual explanations for
    the adult income prediction problem. The library can be installed as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注一种称为多样化反事实解释（DiCE）的流行技术来生成反事实解释。在DiCE中，优化问题被表述为我们之前所做的那样。特征以某种方式被扰动，以便它们多样化且可行更改，并达到用户的期望结果。数学细节超出了本书的范围，但让我们使用DiCE库为成人收入预测问题生成反事实解释。库可以按照以下方式安装：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following code snippet shows how to load the data and prepare it in a way
    that the DiCE explainer can process:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何加载数据并将其准备成DiCE解释器可以处理的形式：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Imports the DiCE library
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入DiCE库
- en: ② Imports the helpers module in the DiCE library
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在DiCE库中导入辅助模块
- en: ③ Uses the helpers module provided by DiCE to load the adult income dataset
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用DiCE提供的辅助模块加载成人收入数据集
- en: ④ Prepares the data for the DiCE explainer; sets the DataFrame argument in the
    Data class to the preloaded adult income dataset
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为DiCE解释器准备数据；在Data类中将DataFrame参数设置为预加载数的成人收入数据集
- en: ⑤ Sets the continuous_features argument in the Data class to the list of columns
    in the DataFrame that are continuous features
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 在Data类中将continuous_features参数设置为DataFrame中连续特征的列列表
- en: ⑥ Sets the outcome_name as the name of the column in the DataFrame that contains
    the target variable
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将outcome_name设置为DataFrame中包含目标变量的列名
- en: 'The next step is to train the model to predict the adult income. Because we
    have already done this in chapter 8 using the random forest model, we will not
    show the code for that here. Once you have trained the model, we are now ready
    to initialize the DiCE explainer, which we can do using the following code snippet:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是训练模型以预测成人收入。因为我们已经在第8章中使用随机森林模型完成了这项工作，所以这里不会展示相应的代码。一旦模型训练完成，我们现在就可以初始化DiCE解释器，可以使用以下代码片段来完成：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Initializes the DiCE Model class by setting the model argument to the trained
    adult income model
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ① 通过将model参数设置为训练好的成人收入模型初始化DiCE Model类
- en: ② Also sets the back-end argument in the Model class to "sklearn" because the
    model was a RandomForestClassifier provided by the Scikit-Learn library
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ② 同时将Model类中的back-end参数设置为"sklearn"，因为模型是Scikit-Learn库提供的RandomForestClassifier
- en: ③ Initializes the DiCE explainer by passing the DiCE data and model
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过传递DiCE数据和模型初始化DiCE解释器
- en: ④ Also sets the method to "random" in the DiCE explainer
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 同时在DiCE解释器中将方法设置为"random"
- en: 'Once we have initialized the DiCE explainer, we can generate the counterfactual
    examples using the next code snippet. The function essentially takes in as input
    the original input, the number of counterfactual examples, and the desired outcome.
    For the input picked here, the model predicts a low income (i.e., <$50K) and the
    desired outcome for the user is a high income (i.e., >$50K):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化DiCE解释器后，我们可以使用下一个代码片段生成反事实示例。该函数本质上接受原始输入、反事实示例的数量和期望结果作为输入。对于这里选择的输入，模型预测的是低收入（即<
    $50K），而用户的期望结果是高收入（即> $50K）：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Picks an input for which to generate counterfactuals
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ① 选择一个用于生成反事实的输入
- en: ② Uses the DiCE explainer to generate counterfactuals for that input
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用DiCE解释器为该输入生成反事实
- en: ③ Also sets the total_CFs argument to the number of counterfactual examples
    to generate
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 同时将total_CFs参数设置为要生成的反事实示例的数量
- en: ④ In addition, sets the desired_class argument to the desired outcome for the
    counterfactual examples
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 此外，将desired_class参数设置为反事实示例的期望结果
- en: ⑤ Visualizes the counterfactual examples as a Pandas DataFrame
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将反事实示例可视化为一个Pandas DataFrame
- en: The output of this code snippet will print the counterfactual examples as a
    Pandas DataFrame. This output has been reformatted as a table and is shown in
    figure 9.6\. We can see in figure 9.6 that the key contributor for why the model
    predicted a low income was the education level. If the education level was higher—doctorate,
    master’s, or professional school—then there is a higher chance for the user to
    earn the desired outcome. The features that are not changed are shown as “--”
    in the figure.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段的输出将打印出反事实示例，作为一个Pandas DataFrame。此输出已被重新格式化为表格，并显示在图9.6中。从图9.6中我们可以看到，模型预测低收入的贡献关键因素是教育水平。如果教育水平更高——博士、硕士或专业学校——那么用户获得期望结果的可能性就更高。未更改的特征在图中显示为“--”。
- en: '![](../Images/CH09_F06_Thampi.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F06_Thampi.png)'
- en: Figure 9.6 Output of the DiCE counterfactual explainer
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 DiCE反事实解释器的输出
- en: We can also use the DiCE counterfactual explainer for regression models. For
    classi-fication, we specified the desired outcome by setting the `desired_class`
    parameter in the `generate_counterfactuals` function when generating the counter-factual
    examples. For regression, we must instead set a different parameter in the same
    function, called `desired_range`, to a range of possible values that is desired
    for the model prediction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用DiCE反事实解释器来解释回归模型。对于分类，我们在生成反事实示例时通过在`generate_counterfactuals`函数中设置`desired_class`参数来指定所需的输出。对于回归，我们必须在同一个函数中设置一个不同的参数，称为`desired_range`，将其设置为模型预测所需的可能值的范围。
- en: Counterfactual examples are a great way of providing explanations that are contrastive.
    A counterfactual explanation of the form, “The model prediction was P because
    features X, Y, and Z had values A, B, and C, but if feature X had values D or
    E, then the model would have predicted a different outcome, Q,” is more causally
    informative and helps us understand why the model predicted a certain outcome
    as opposed to another. As mentioned earlier, more goes into providing a good explanation
    to the user of an AI system. XAI is an intersection of multiple fields such as
    AI, social sciences, and HCI and is a very active area of research. It is beyond
    the scope of this book, but the techniques that you have learned should provide
    you with a solid foundation, especially in the AI domain, to venture into the
    world of XAI.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实示例是提供对比性解释的绝佳方式。一种形式的反事实解释是，“模型预测P是因为特征X、Y和Z的值分别是A、B和C，但如果特征X的值是D或E，那么模型将预测不同的结果Q，”这种解释更具因果信息，有助于我们理解为什么模型预测了某个结果而不是另一个。如前所述，向AI系统的用户提供良好的解释需要更多的内容。XAI是AI、社会科学和HCI等多个领域的交汇点，是一个非常活跃的研究领域。这超出了本书的范围，但你所学到的技术应该为你提供一个坚实的基础，特别是在AI领域，以便进入XAI的世界。
- en: This brings us to the end of the book. With a wide range of interpretability
    techniques in your toolkit, you are well equipped to understand how complex machine
    learning models work and how they arrive at a prediction. You can use this to
    debug and improve the performance of models. You can also use it to increase transparency
    and build fair and unbiased models. This book should also pave the way for you
    to build explainable AI systems. You should have a solid foundation to learn more
    about this very active area of research. Happy building and learning!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的内容到此结束。在你的工具箱中拥有广泛的可解释性技术，你已准备好理解复杂机器学习模型是如何工作的以及它们是如何得出预测的。你可以利用这些技术来调试和提升模型的性能。你也可以利用它们来增加透明度，构建公平且无偏见的模型。这本书还应为你构建可解释的人工智能系统铺平道路。你应该有一个坚实的基础来学习这个非常活跃的研究领域。祝你在构建和学习中快乐！
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Interpretability is all about understanding how the underlying models in an
    AI system come up with predictions, understanding how the predictions change with
    modifications to the input or algorithmic parameters, and understanding when the
    models have made a mistake.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性主要关乎理解AI系统中的底层模型是如何得出预测的，理解预测如何随着输入或算法参数的修改而变化，以及理解模型何时犯了错误。
- en: Explainability goes beyond interpretability in that it helps to answer the *why*
    question—*why* did the model make a specific prediction as opposed to another?
    Interpretability is mostly discernible by experts who are building, deploying,
    or using the AI system, and these techniques are building blocks that will help
    you get to explainability.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性超越了可解释性，因为它有助于回答“为什么”的问题——为什么模型做出了特定的预测而不是另一个？可解释性主要是由构建、部署或使用AI系统的专家所感知的，而这些技术是帮助你达到可解释性的基石。
- en: The scope of explainable AI is not just artificial intelligence, of which machine
    learning is a specific subfield, but also looks to other fields such as human-computer
    interaction (HCI) and the social sciences.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释人工智能的范围不仅限于人工智能，其中机器学习是一个特定的子领域，而且还涉及其他领域，如人机交互（HCI）和社会科学。
- en: 'From the social sciences, the following three key findings are relevant for
    explainability:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从社会科学领域来看，以下三个关键发现与可解释性相关：
- en: Explanations are usually contrastive—people usually do not just ask *why* the
    model predicted a specific outcome but, rather, *why not* another outcome. Counterfactual
    explanations can be used to answer these types of questions.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释通常是对比性的——人们通常不会仅仅询问模型为什么预测了特定的结果，而是更想知道为什么不是另一个结果。反事实解释可以用来回答这类问题。
- en: Explanations are usually selected in a biased way. It is important to know how
    much information to provide and what information is most relevant for the explanation.
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释通常是以偏概全的。了解提供多少信息以及哪些信息对解释最为相关是很重要的。
- en: Explanations are social. The transfer of information from the AI system to the
    user must be in the form of a conversation or interactive. It is important to
    look to the field of HCI to build such a system.
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释是社会的。从AI系统到用户的 信息传递必须以对话或交互的形式进行。参考人机交互（HCI）领域来构建这样的系统是很重要的。
