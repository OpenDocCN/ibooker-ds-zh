- en: '3 Service pipelines: Building cloud-native applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 个服务管道：构建云原生应用程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Discovering the components for delivering cloud-native applications
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现交付云原生应用程序的组件
- en: Learning the advantages of creating and standardizing service pipelines
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习创建和标准化服务管道的优势
- en: Using Tekton, Dagger, and GitHub Actions to build cloud-native applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Tekton、Dagger 和 GitHub Actions 构建云原生应用程序
- en: In the previous chapter, you installed and interacted with a simple distributed
    Conference application composed of four services. This chapter covers what it
    takes to continuously deliver each component using the *pipeline* concept as a
    delivery mechanism. This chapter describes and shows in practice how each of these
    services can be built, packaged, released, and published so they can run in your
    organization’s environments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你安装并交互了一个由四个服务组成的简单分布式会议应用程序。本章将介绍如何使用 *管道* 作为交付机制来持续交付每个组件。本章描述并展示了如何构建、打包、发布和发布这些服务，以便它们可以在你的组织环境中运行。
- en: 'This chapter introduces the concept of *service pipelines*. The service pipeline
    takes all the steps to build your software from source code until the artifacts
    are ready to run. This chapter is divided into two main sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 *服务管道* 的概念。服务管道包括从源代码构建软件到工件准备运行的所有步骤。本章分为两个主要部分：
- en: What does it take to deliver a cloud-native applications continuously?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续交付云原生应用程序需要哪些条件？
- en: Service pipelines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务管道
- en: What is a service pipeline?
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是服务管道？
- en: 'Service pipelines in action using:'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下方式实现服务管道：
- en: Tekton, a Kubernetes native pipeline engine
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tekton，一个 Kubernetes 原生管道引擎
- en: Dagger to code your pipelines, and then run everywhere
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dagger 编写你的管道，然后在任何地方运行
- en: Should I use Tekton, Dagger, or GitHub Actions?
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该使用 Tekton、Dagger 还是 GitHub Actions？
- en: 3.1 What does it take to deliver cloud-native applications continuously?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 连续交付云原生应用程序需要哪些条件？
- en: When working with Kubernetes, teams are now responsible for more moving pieces
    and tasks involving containers and how to run them in Kubernetes. These extra
    tasks don’t come for free. Teams must learn to automate and optimize the steps
    required to keep each service running. Tasks that were the responsibility of the
    operations teams are now becoming more and more the responsibility of the teams
    in charge of developing each of the individual services. New tools and new approaches
    give developers the power to develop, run, and maintain the services they produce.
    The tools that we will look at in this chapter are designed to automate all the
    tasks involved to go from source code to a service that is up and running inside
    a Kubernetes cluster. This chapter describes the mechanisms to deliver software
    components (our application services) to multiple environments where these services
    will run. But before jumping into the tools, let’s take a quick look at the challenges
    that we are facing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当与 Kubernetes 一起工作时，团队现在负责更多的移动部件和涉及容器以及如何在 Kubernetes 中运行的任务。这些额外任务并非免费提供。团队必须学会自动化和优化保持每个服务运行所需的步骤。原本由运维团队负责的任务现在越来越多地成为负责每个单独服务的团队的责任。新的工具和新的方法赋予开发者开发、运行和维护他们所产生服务的权力。本章我们将探讨的工具旨在自动化从源代码到在
    Kubernetes 集群内部署并运行的服务所需的所有任务。本章描述了将软件组件（我们的应用程序服务）交付到多个环境的机制。但在深入研究工具之前，让我们快速看一下我们面临的挑战。
- en: 'Building and delivering cloud-native applications present significant challenges
    that teams must tackle:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和交付云原生应用程序面临着显著的挑战，团队必须应对：
- en: '*Dealing with different team interactions* *when building different pieces
    of the application:* This requires coordination between teams and ensuring that
    services are designed so that the team responsible for a service is not blocking
    other teams’ progress or their ability to keep improving their services.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在构建应用程序的不同部分时处理不同的团队互动：* 这需要团队之间的协调，并确保服务被设计成负责某个服务的团队不会阻碍其他团队的进度或他们改进服务的能力。'
- en: '*We need to* *support upgrading a service without breaking or stopping all
    the other running services:* If we want to achieve continuous delivery, services
    should be upgraded independently without the fear of bringing down the entire
    application. This teams to think about how backward compatible the new version
    is and whether the new version can run alongside the old version to avoid big
    bang upgrades.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们需要支持在不中断或停止所有其他运行服务的情况下升级服务：* 如果我们想要实现持续交付，服务应该能够独立升级，而不必担心整个应用程序会崩溃。这需要我们考虑新版本的后向兼容性，以及新版本是否可以与旧版本并行运行，以避免大爆炸式升级。'
- en: '*Storing and publishing several artifacts per service that can be accessed/downloaded
    from different environments, which might be in different regions:* If we are working
    in a cloud environment, all servers are remote, and all produced artifacts need
    to be accessible for each of these servers to fetch. If you are working on an
    on-premise setup, all the repositories for storing these artifacts must be provisioned,
    configured, and maintained in-house.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*存储和发布每个服务所需的多件工件，这些工件可以从不同的环境访问/下载，这些环境可能位于不同的地区：* 如果我们在云环境中工作，所有服务器都是远程的，所有产生的工件都需要对每个服务器都是可访问的，以便它们可以检索。如果你在本地环境中工作，存储这些工件的所有仓库都必须内部配置、配置和维护。'
- en: '*Managing and provisioning different environments for various purposes such
    as development, testing, Q&A, and production:* If you want to speed up your development
    and testing initiatives, developers and teams should be able to provision these
    environments on demand. Having environments configured as close as possible to
    the real production environment will save you a lot of time catching errors before
    they hit your live users.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理和配置不同环境以满足各种目的，如开发、测试、Q&A和生产：* 如果你想加快你的开发和测试工作，开发者和团队应该能够按需配置这些环境。将环境配置得尽可能接近真实的生产环境，将节省你大量时间，在错误影响到你的实际用户之前捕捉到它们。'
- en: As we saw in the previous chapter, the main paradigm shift when working with
    cloud-native applications is that our application has no single code base. Teams
    can work independently on their services, but this requires new approaches to
    compensate for the complexities of working with a distributed system. If teams
    worry and waste time every time a new service needs to be added to the system,
    we are doing things wrong. End-to-end automation is necessary for teams to feel
    comfortable adding or refactoring services. This automation is usually performed
    by what is commonly known as pipelines. As shown in figure 3.1, these pipelines
    describe what needs to be done to build and run our services, and usually, they
    can be executed without human intervention.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，与云原生应用程序一起工作时，主要的范式转变是我们的应用程序没有单一的代码库。团队可以独立地在其服务上工作，但这需要新的方法来弥补与分布式系统一起工作的复杂性。如果每次需要向系统中添加新服务时，团队都会感到担忧并浪费时间，那么我们就是在做错事。端到端自动化对于团队来说，是舒适地添加或重构服务所必需的。这种自动化通常由通常被称为管道的机制执行。如图3.1所示，这些管道描述了构建和运行我们的服务需要做什么，通常它们可以在没有人为干预的情况下执行。
- en: '![](../../OEBPS/Images/03-01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1](../../OEBPS/Images/03-01.png)'
- en: Figure 3.1 We use the concept of a pipeline to transform source code into an
    artifact that can run inside an environment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 我们使用管道的概念将源代码转换成一个可以在环境中运行的工件。
- en: You can even have pipelines to automate the creation of a new service or add
    new users to your identity management solution. But what are these pipelines doing
    exactly? Do we need to create our pipelines from scratch? How do we implement
    these pipelines in our projects? Do we need one or more pipelines to achieve this?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以拥有管道来自动化新服务的创建或添加新用户到你的身份管理解决方案。但这些管道究竟在做什么？我们需要从头开始创建我们的管道吗？我们如何在项目中实现这些管道？我们需要一个或多个管道来实现这一点？
- en: Section 3.2 is focused on using pipelines to build solutions that can be copied,
    shared, and executed multiple times to produce the same results. Pipelines can
    be created for different purposes, and it is common to define them as a set of
    steps (one after the other in sequence) that produce a set of expected outputs.
    Based on these outputs, these pipelines can be classified into different groups.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第3.2节专注于使用管道构建可以复制、共享和多次执行以产生相同结果的解决方案。可以为不同的目的创建管道，通常将它们定义为一组步骤（按顺序依次执行）以产生一组预期的输出。基于这些输出，可以将这些管道分类到不同的组中。
- en: Most pipeline tools allow you to define pipelines as a collection of tasks (also
    known as steps or jobs) that will run a specific job or script to perform a concrete
    action. These steps can be anything, from running tests, copying code from one
    place to another, deploying software, provisioning virtual machines, creating
    users, etc.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数管道工具允许您将管道定义为一系列任务（也称为步骤或作业），这些任务将运行特定的作业或脚本以执行具体操作。这些步骤可以是任何事情，从运行测试、将代码从一个地方复制到另一个地方、部署软件、提供虚拟机、创建用户等。
- en: Pipeline definitions can be executed by a component known as the *pipeline engine*,
    which is responsible for picking up the pipeline definition to create a new pipeline
    instance that runs each task. The tasks will be executed one after the other in
    sequence, and each task execution might generate data that can be shared with
    the following task. If there is an error in any of the steps involved with the
    pipeline, the pipeline stops, and the pipeline state will be marked as an error
    (failed). If there are no errors, the pipeline execution (also known as pipeline
    instance) can be marked as successful. Depending on the pipeline definition and
    whether the execution was successful, we should verify that the expected outputs
    were generated or produced.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 管道定义可以由一个称为*管道引擎*的组件执行，该组件负责拾取管道定义以创建一个新的管道实例，该实例运行每个任务。任务将按顺序依次执行，每个任务的执行可能会生成可以与后续任务共享的数据。如果管道中涉及任何步骤出现错误，则管道停止，并将管道状态标记为错误（失败）。如果没有错误，则管道执行（也称为管道实例）可以标记为成功。根据管道定义和执行是否成功，我们应该验证是否生成了预期的输出或产生了输出。
- en: In figure 3.2, we can see the pipeline engine picking up our pipeline definition
    and creating different instances that can be parameterized differently for different
    outputs. For example, Pipeline Instance 1 finished correctly, while Pipeline Instance
    2 failed to execute all the tasks included in the definition. Pipeline Instance
    3, in this case, is still running.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.2中，我们可以看到管道引擎正在拾取我们的管道定义并创建不同的实例，这些实例可以根据不同的输出进行不同的参数化。例如，管道实例1正确完成，而管道实例2未能执行定义中包含的所有任务。在这种情况下，管道实例3仍在运行。
- en: '![](../../OEBPS/Images/03-02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2](../../OEBPS/Images/03-02.png)'
- en: Figure 3.2 A pipeline definition can be instantiated by a pipeline engine multiple
    times, and it describes what needs to be done. The pipeline engine creates pipeline
    Instances, which run the tasks included in the pipeline definition. These pipeline
    instances can fail or run for longer periods of time depending on the tasks that
    they are performing. As a user, you can always ask the pipeline engine the status
    of a particular pipeline instance and its tasks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 管道定义可以被管道引擎多次实例化，它描述了需要完成的工作。管道引擎创建管道实例，运行管道定义中包含的任务。这些管道实例可能会失败或运行更长时间，具体取决于它们执行的任务。作为用户，您始终可以询问管道引擎特定管道实例及其任务的状态。
- en: As expected, with these pipeline definitions we can create loads of different
    automation solutions, and it is common to find tools that build more specific
    solutions on top of a pipeline engine or even hide the complexity of dealing with
    a pipeline engine to simplify the user experience. In the following sections,
    we will look for examples of different tools, some more low-level and flexible,
    and some higher level, more opinionated, and designed to solve a very concrete
    scenario.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，使用这些管道定义，我们可以创建大量的不同自动化解决方案，并且常见的是找到在管道引擎之上构建更具体解决方案的工具，甚至隐藏处理管道引擎的复杂性以简化用户体验。在接下来的章节中，我们将寻找不同工具的例子，一些更底层和灵活，一些更高级，更有观点，旨在解决一个非常具体的场景。
- en: 'But how do these concepts and tools apply to delivering cloud-native applications?
    For cloud-native applications, we have very concrete expectations about how to
    build, package, release, and publish our software components (services) and where
    these should be deployed. In the context of delivering cloud-native applications,
    we can define two main kinds of pipelines:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些概念和工具如何应用于交付云原生应用程序呢？对于云原生应用程序，我们对如何构建、打包、发布和发布我们的软件组件（服务）以及它们应该部署在哪里有非常具体的要求。在交付云原生应用程序的上下文中，我们可以定义两种主要的管道类型：
- en: '*Service pipelines:* These take care of the building, unit testing, packaging,
    and distributing (usually to an artifact repository) of our software artifacts.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务管道*：这些负责构建、单元测试、打包和分发（通常到工件存储库）我们的软件工件。'
- en: '*Environment pipelines:* These take care of deploying and updating all the
    services in a given environment, such as staging, testing, production, etc., usually
    consuming what needs to be deployed from a source of truth.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*环境管道*：这些负责部署和更新给定环境中的所有服务，例如预发布、测试、生产等，通常从事实来源消费需要部署的内容。'
- en: Chapter 3 focuses on service pipelines, while chapter 4 focuses on tools that
    help us to define environment pipelines using a more declarative approach known
    as GitOps.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第三章侧重于服务管道，而第四章侧重于帮助我们使用称为GitOps的更声明式方法定义环境管道的工具。
- en: By separating the build process (service pipeline) and the deployment process
    (environment pipeline), we give more control to the teams responsible for promoting
    new versions in front of our customers. Service and environment pipelines are
    executed on top of different resources and with different expectations. The following
    section goes into more detail about the steps that we commonly define in our service
    pipelines. Chapter 4 covers what is expected of environment pipelines.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将构建过程（服务管道）和部署过程（环境管道）分开，我们给予负责在客户面前推广新版本的团队更多的控制权。服务管道和环境管道在不同的资源上执行，并且有不同的期望。下一节将详细介绍我们在服务管道中通常定义的步骤。第四章涵盖了环境管道的期望内容。
- en: 3.2 Service pipelines
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 服务管道
- en: A service pipeline defines and executes all the steps required to build, package,
    and distribute a service artifact so it can be deployed into an environment. A
    service pipeline is not responsible for deploying the newly created service artifact,
    but it can be responsible for notifying interested parties that there is a new
    version available for the service.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 服务管道定义并执行构建、打包和分发服务工件所需的所有步骤，以便将其部署到环境中。服务管道不负责部署新创建的服务工件，但它可以负责通知感兴趣的各方，服务有新的版本可用。
- en: You can share the same pipeline definition for different services if you standardize
    how your services must be built, packaged, and released. Try to avoid pushing
    each of your teams to define a completely different pipeline for each service,
    because they will probably reinvent something that has already been defined, tested,
    and improved by other teams. A considerable amount of tasks need to be performed,
    and a set of conventions that, when followed, can reduce the time required to
    perform the whole process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您标准化服务的构建、打包和发布方式，就可以为不同的服务共享相同的管道定义。尽量避免让每个团队为每个服务定义一个完全不同的管道，因为他们可能会重新发明已经被其他团队定义、测试和改进过的东西。需要执行相当多的任务，并且有一套约定，遵循这些约定可以减少完成整个流程所需的时间。
- en: The name *service pipeline* refers to the fact that each of our application’s
    services will have a pipeline that describes the tasks required for that particular
    service. If the services are similar and they are using a similar technology stack,
    it makes sense for the pipelines to look quite similar. One of the main objectives
    of these service pipelines is to contain enough detail to run without any human
    intervention, automating all the tasks in the pipeline end to end.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “服务管道”这个名字指的是我们应用程序的每个服务都将有一个管道，描述了该特定服务所需的任务。如果服务相似并且它们使用相似的技术堆栈，那么管道看起来相当相似是有意义的。这些服务管道的主要目标之一是包含足够的细节，以便在没有人工干预的情况下运行，自动化管道中的所有任务。
- en: Service pipelines can be used as a mechanism to improve the communication between
    the development team that is creating a service and the operations team that is
    running that service in production. Development teams expect these pipelines to
    run and notify them if there is any problem with the code they are trying to build.
    If there are no errors, they will expect one or more artifacts to be produced
    as part of the pipeline execution. Operations teams can add all the checks to
    these pipelines to ensure the produced artifacts are production ready. These checks
    can include policy and conformance checks, signing, security scanning, and other
    requirements that validate that the produced artifacts are up to the standards
    expected to run in the production environment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 服务管道可以用作一种机制，以改善创建服务的开发团队和在生产环境中运行该服务的运维团队之间的沟通。开发团队期望这些管道能够运行，并在他们尝试构建的代码出现任何问题时通知他们。如果没有错误，他们期望作为管道执行的一部分生成一个或多个工件。运维团队可以向这些管道添加所有检查，以确保生成的工件已准备好投入生产。这些检查可以包括策略和合规性检查、签名、安全扫描以及其他要求，以验证生成的工件符合预期在生产环境中运行的标准。
- en: Note It is tempting to think about creating a single pipeline for the entire
    application (collection of services), as we did with monolith applications. However,
    that defeats the purpose of independently updating each service at its own pace.
    You should avoid situations with a single pipeline defined for a set of services,
    because it will block your ability to release services independently.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：可能会诱使人们考虑为整个应用程序（服务集合）创建一个单一的管道，就像我们处理单体应用程序那样。然而，这样做违背了独立按各自节奏更新每个服务的初衷。你应该避免为一系列服务定义单一管道的情况，因为这会阻碍你独立发布服务的能力。
- en: 3.3 Conventions that will save you time
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 节省你时间的约定
- en: 'Service pipelines can be more opinionated on their structure and reach. By
    following some of these strong opinions and conventions, you can avoid pushing
    your teams to define every little detail and discover these conventions by trial
    and error. The following approaches have been proven to work:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 服务管道可以在结构和范围上更加具有意见性。通过遵循这些强烈的意见和约定，你可以避免让你的团队定义每一个细节，并通过试错来发现这些约定。以下方法已被证明是有效的：
- en: '*Trunk-based development:* The idea here is to ensure that what you have in
    the *main* branch of your source code repository is always ready to be released.
    You don’t merge changes that break this branch’s build and release process. You
    only merge if the changes you are merging are ready to be released. This approach
    also includes using feature branches, which allow developers to work on features
    without breaking the main branch. When the features are done and tested, developers
    can send pull requests (change requests) for other developers to review and merge.
    This also means that when you merge something to the main branch, you can automatically
    create a new release of your service (and all the related artifacts). This creates
    a continuous stream of releases generated after each new feature is merged into
    the main branch. Because each release is consistent and has been tested, you can
    then deploy this new release to an environment that contains all the other services
    of your application. This approach enables the team behind the service to move
    forward and keep releasing without worrying about other services.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主干开发:* 这里的想法是确保你源代码仓库的*主分支*中的内容始终准备好发布。你不合并会破坏此分支构建和发布过程的更改。只有当你合并的更改准备好发布时，你才进行合并。这种方法还包括使用功能分支，允许开发者在不破坏主分支的情况下工作。当功能完成并经过测试后，开发者可以向其他开发者发送拉取请求（变更请求）以供审查和合并。这也意味着当你将某些内容合并到主分支时，你可以自动创建服务（以及所有相关工件）的新版本。这创建了一个连续的发布流，每次新功能合并到主分支后都会生成新的发布。因为每个发布都是一致的并且已经过测试，所以你可以将这个新版本部署到一个包含你应用程序中所有其他服务的环境中。这种方法使得服务背后的团队能够继续前进并持续发布，而无需担心其他服务。'
- en: '*Source code and configuration management:* There are different approaches
    to dealing with software and the configuration needed to run the software we are
    producing. When we talk about services and distributed applications, there are
    two different schools of thought:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*源代码和配置管理:* 处理软件及其运行所需配置的方法有很多种。当我们谈论服务和分布式应用程序时，存在两种不同的思想流派：'
- en: '*One service/one repository/one pipeline:* You keep your service source code
    and all the configurations that need to be built, packaged, released, and deployed
    in the same repository. This allows the team behind the service to push changes
    at any pace they want without worrying about other services’ source code. It is
    a common practice to have the source code in the same repository where you have
    the `Dockerfile` describing how the Docker image should be created and the Kubernetes
    manifest required to deploy the service into a Kubernetes cluster. These configurations
    should include the pipeline definition that will be used to build and package
    your service.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个服务/一个仓库/一个管道：* 您将服务的源代码以及所有需要构建、打包、发布和部署的配置保存在同一个仓库中。这使得服务背后的团队能够以他们想要的任何速度推送更改，而不用担心其他服务的源代码。通常的做法是将源代码保存在包含描述如何创建
    Docker 镜像的 `Dockerfile` 和部署服务到 Kubernetes 集群所需的 Kubernetes 清单的同一个仓库中。这些配置应包括用于构建和打包您的服务的管道定义。'
- en: '*Mono repository*: Alternatively, use a mono repository approach where a single
    repository is used, and different pipelines are configured for different directories
    inside the repository. While this approach can work, you need to ensure that your
    teams are not blocking each other by waiting for each other’s pull requests to
    merge.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*单仓库：* 或者，采用单仓库方法，其中使用单个仓库，并为仓库内的不同目录配置不同的管道。虽然这种方法可以工作，但您需要确保您的团队不会因为等待彼此的拉取请求合并而相互阻塞。'
- en: '*Consumer-driven contract testing:* Your service uses contracts to run tests
    against other services. Unit testing an individual service shouldn’t require having
    other services up and running. By creating consumer-driven contracts, each service
    can test its functionality against other APIs. If any downstream service is released,
    a new contract is shared with all the upstream services so they can run their
    tests against the new version.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*消费者驱动的合同测试：* 您的服务使用合同对其他服务进行测试。对单个服务的单元测试不应需要其他服务正在运行。通过创建消费者驱动的合同，每个服务都可以对其功能进行其他
    API 的测试。如果任何下游服务发布，新的合同将与所有上游服务共享，以便它们可以对其新版本进行测试。'
- en: 'There are two books that I strongly recommend:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐以下两本书：
- en: '*Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment
    Automation* by Jez Humble and David Farley (Addison-Wesley Professional, 2010)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '《持续交付：通过构建、测试和部署自动化实现可靠的软件发布》（Continuous Delivery: Reliable Software Releases
    through Build, Test, and Deployment Automation）作者：Jez Humble 和 David Farley（Addison-Wesley
    Professional，2010年）'
- en: '*Grokking Continuous Delivery* by Christie Wilson (Manning Publications, 2022)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《精通持续交付》（Grokking Continuous Delivery）作者：Christie Wilson（Manning Publications，2022年）
- en: 'Most of the tools mentioned in these books allow you to implement these practices
    for efficient delivery. If we take these practices and conventions into account,
    we can define the responsibility of a service pipeline as follows: *A service
    pipeline transforms source code to one or a set of artifacts that can be deployed
    in an environment.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些书中提到的大多数工具都允许您实现这些实践以提高交付效率。如果我们考虑这些实践和惯例，我们可以将服务管道的职责定义为如下：*服务管道将源代码转换为一个或多个可以在环境中部署的工件。*
- en: 3.4 Service pipeline structure
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 服务管道结构
- en: 'With this definition in mind, let’s take a look at what tasks are included
    in service pipelines for cloud-native applications that will run on Kubernetes:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这个定义，让我们看看在 Kubernetes 上运行的云原生应用的服务管道中包含哪些任务：
- en: '*Register to receive notifications* *about changes in the source code repository
    main branch:* (Source version control system, nowadays a Git repository.) If the
    source code changes, we need to create a new release. We create a new release
    by triggering the service pipeline. This is usually implemented using webhooks
    or a pull-based mechanism that checks if new changes were submitted.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注册以接收有关源代码仓库主分支更改的通知：*（源代码版本控制系统，如今通常是 Git 仓库。）如果源代码发生变化，我们需要创建一个新的版本。我们通过触发服务管道来创建新版本。这通常是通过
    Webhook 或基于拉取的机制实现的，该机制检查是否有新更改提交。'
- en: '*Clone the source code from the repository:* To build the service, we need
    to clone the source code into a machine that has the tools to build/compile the
    source code into a binary format that can be executed.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从仓库克隆源代码：* 为了构建服务，我们需要将源代码克隆到一台具有构建/编译源代码为可执行二进制格式的工具的机器上。'
- en: '*Create a new tag for the new version to be released:* Based on trunk-based
    development, a new release can be created every time a change happens. This will
    help us to understand what is being deployed and what changes were included in
    each new release.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为即将发布的版本创建一个新的标签:* 基于主干开发，每次发生更改时都可以创建一个新的发布版本。这将帮助我们了解正在部署的内容以及每个新版本中包含的更改。'
- en: '*Build and test the source code:*'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建和测试源代码:*'
- en: As part of the build process, most projects will execute unit tests and break
    the build if there are any failures.
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为构建过程的一部分，大多数项目都会执行单元测试，如果出现任何失败，则会中断构建。
- en: Depending on our technology stack, we will need tools available for this step,
    for example, compilers, dependencies, linters (static source code analyzers),
    etc.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据我们的技术栈，我们需要在这个步骤中可用的工具，例如，编译器、依赖项、linters（静态源代码分析器）等。
- en: Tools like CodeCov, which measures how much of the code is being covered by
    tests, are used to block changes from being merged if a coverage threshold is
    not met.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CodeCov等工具，它测量代码被测试覆盖的程度，如果未达到覆盖率阈值，则用于阻止更改合并。
- en: Security scanners are also used to evaluate vulnerabilities on our application
    dependencies. If a new CVE (Common Vulnerabilities and Exposures) is found, the
    change can be blocked too.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全扫描器也用于评估我们的应用程序依赖项中的漏洞。如果发现新的CVE（常见漏洞与暴露），更改也可以被阻止。
- en: '*Publish the binary artifacts into an artifact repository:* We need to make
    sure that these binaries are available for other systems to consume, including
    the next steps in the pipeline. This step involves copying the binary artifact
    to a different location over the network. This artifact will share the same version
    of the tag created in the repository, providing us with traceability from the
    binary to the source code used to produce it.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将二进制工件发布到工件存储库:* 我们需要确保这些二进制文件可供其他系统消费，包括管道中的下一步。这一步涉及通过网络将二进制工件复制到不同的位置。这个工件将与在仓库中创建的标签具有相同的版本，为我们提供了从二进制文件到用于生成它的源代码的可追溯性。'
- en: '*Building a container image:* If we are building cloud-native services, we
    must build a container image. The most common way of doing this today is using
    Docker or other container alternatives. This step requires the source code repository
    to have, for example, a `Dockerfile` defining how this container image needs to
    be built and the mechanism to build (builder) the container image. Some tools
    like CNCF Buildpacks ([https://buildpacks.io](https://buildpacks.io)) save us
    from having a `Dockerfile` and can automate the container-building process. Having
    the right tools for the job is essential, because multiple container images might
    need to be generated for different platforms. For a released service, we might
    have more than one container image, for example, one for `amd64` and one for `arm64`
    platforms. All the examples in this book are built for these two platforms.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*构建容器镜像:* 如果我们正在构建云原生服务，我们必须构建一个容器镜像。目前最常见的方法是使用Docker或其他容器替代品。这一步需要源代码仓库中有一个`Dockerfile`，定义如何构建这个容器镜像以及构建（构建器）容器镜像的机制。一些工具，如CNCF
    Buildpacks ([https://buildpacks.io](https://buildpacks.io))，可以让我们避免使用`Dockerfile`并自动化容器构建过程。拥有适合工作的正确工具是至关重要的，因为可能需要为不同的平台生成多个容器镜像。对于一个发布的服务，我们可能有多个容器镜像，例如，一个用于`amd64`平台，另一个用于`arm64`平台。本书中的所有示例都是为这两个平台构建的。'
- en: '*Publish the container image into a container registry:* In the same way that
    we published the binary artifacts generated when building our service source code,
    we need to publish our container image into a centralized location where others
    can access it. This container image will have the same version as the tag created
    in the repository and the binary published. This helps us see which source code
    will run when you run the container image.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将容器镜像发布到容器注册库:* 与我们发布构建服务源代码时生成的二进制工件的方式相同，我们需要将我们的容器镜像发布到一个集中位置，以便其他人可以访问它。这个容器镜像将与在仓库中创建的标签和发布的二进制文件具有相同的版本。这有助于我们了解运行容器镜像时将运行哪个源代码。'
- en: '*Lint, verify, and optionally package YAML files* *for Kubernetes deployments
    (Helm can be used here):* If you are running these containers inside Kubernetes,
    you need to manage, store, and version a Kubernetes manifest that defines how
    the containers are going to be deployed into a Kubernetes cluster. If you use
    a package manager such as Helm, you can version the package with the same version
    used for the binaries and the container image. My rule for packaging YAML files
    goes as follows: “If you have enough people trying to install your services (open-source
    project or very large globally distributed organization), you might want to package
    and version your YAML files. If you only have a few teams and environments to
    handle, you can probably distribute the YAML files without using a packaging tool.”'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检查、验证并可选地打包用于Kubernetes部署的YAML文件* *(此处可以使用Helm)：* 如果你在这些容器内部署Kubernetes，你需要管理、存储和版本化一个Kubernetes清单，该清单定义了容器将如何部署到Kubernetes集群中。如果你使用包管理器如Helm，你可以使用与二进制文件和容器镜像相同的版本对包进行版本控制。我打包YAML文件的规则如下：“如果你有足够多的人尝试安装你的服务（开源项目或非常大的全球分布式组织），你可能想要打包和版本化你的YAML文件。如果你只有少数团队和环境要处理，你很可能可以在不使用打包工具的情况下分发YAML文件。”'
- en: '*(Optional) Publish these Kubernetes* *manifests to a centralized location:*
    If you are using Helm, it makes sense to push these Helm packages (called Charts)
    to a centralized location. This will allow other tools to fetch these Charts so
    they can be deployed in any number of Kubernetes clusters. As we saw in chapter
    2, these Helm Charts can now be distributed as OCI container images to a container
    registry.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(可选) 将这些Kubernetes* *清单发布到集中位置:* 如果你使用Helm，将Helm包（称为图表）推送到集中位置是有意义的。这将允许其他工具检索这些图表，以便它们可以在任意数量的Kubernetes集群中部署。正如我们在第2章中看到的，这些Helm图表现在可以作为OCI容器镜像分发到容器注册库。'
- en: '*Notify interested parties about the new version of the service:* If we are
    trying to automate from a source to a service running, the service pipeline can
    send notifications to all the interested services that might be waiting for new
    versions to be deployed. These notifications can be pull requests to other repositories,
    events to an event bus, emails to the teams interested in these releases, etc.
    A pull-based approach can also work, where an agent constantly monitors the artifact
    repository (or container registry) to see if new versions are available for a
    given artifact.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通知相关方关于服务新版本的更新:* 如果我们尝试从源自动化到运行中的服务，服务管道可以向所有可能等待新版本部署的相关服务发送通知。这些通知可以是向其他存储库的拉取请求、事件总线的事件、发送给对这些发布感兴趣团队的电子邮件等。基于拉取的方法也可以工作，其中代理持续监控工件存储库（或容器注册库）以查看是否存在给定工件的新版本。'
- en: Figure 3.3 shows the steps described in the previous bullet points as a sequence
    of steps. Most pipeline tools will have a visual representation that allows you
    to see which steps will be executed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3展示了前述要点描述的步骤作为一系列步骤。大多数管道工具都将有一个可视化表示，允许你看到哪些步骤将被执行。
- en: '![](../../OEBPS/Images/03-03.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/03-03.png)'
- en: Figure 3.3 Service pipelines automate all the steps required to produce artifacts
    that can run in multiple environments. Service pipelines are often triggered by
    changes in source code, but are not in charge of deploying the created artifacts
    in a specific environment. They can notify other components about these new versions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3显示服务管道自动化了在多个环境中运行所需的所有步骤。服务管道通常由源代码的变化触发，但并不负责在特定环境中部署创建的工件。它们可以通知其他组件关于这些新版本的信息。
- en: The outcome of this pipeline is a set of artifacts that can be deployed to an
    environment to have the service up and running. The service needs to be built
    and packaged in a way that does not depend on any specific environment. The service
    can depend on other services to work in the environment, such as infrastructural
    components like databases, message brokers, or other downstream services.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道的输出是一组可以部署到环境中以使服务运行起来的工件。服务需要以不依赖于任何特定环境的方式构建和打包。服务可以依赖于其他服务在环境中工作，例如数据库、消息代理或其他下游服务。
- en: 'No matter the tool that you choose to use to implement these pipelines, you
    should be looking at the following characteristics:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种工具来实现这些管道，你应该关注以下特性：
- en: Pipelines run automatically based on changes (if you follow trunk-based development,
    one pipeline instance is created for every change in the repository’s main branch).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道根据更改自动运行（如果您遵循基于主干的开发，则为主分支的每个更改创建一个管道实例）。
- en: Pipelines executions will notify about the success or failure state with clear
    messages. This includes having easy ways to find, for example, the why and where
    the pipeline failed or how much time it takes to execute each step.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道执行将通知关于成功或失败状态，并带有清晰的消息。这包括有简单的方法来查找，例如，管道失败的原因和位置，以及执行每个步骤所需的时间。
- en: Each pipeline execution has a unique `id` that we can use to access the log
    and the parameters that were used to run the pipeline, so we can reproduce the
    setup that was used to troubleshoot problems. Using this unique `id`, we can also
    access the logs created by all the steps in the pipeline. By looking at the pipeline
    execution, we should also be able to find all the produced artifacts and where
    those were published.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个管道执行都有一个唯一的 `id`，我们可以使用它来访问日志和运行管道时使用的参数，这样我们就可以重现用于解决问题的设置。使用这个唯一的 `id`，我们还可以访问管道中所有步骤创建的日志。通过查看管道执行，我们也应该能够找到所有生成的工件以及它们发布的位置。
- en: Pipelines can also be triggered manually and configured with different parameters
    for special situations. For example, to test a work-in-progress feature branch.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道也可以手动触发，并针对特殊情况配置不同的参数。例如，测试一个正在开发中的功能分支。
- en: Let’s now deep dive into the concrete details of what a service pipeline will
    look like in real life.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解服务管道在现实生活中的具体细节。
- en: 3.4.1 Service pipeline in real life
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 生活中的服务管道
- en: 'In real life, a service pipeline will run every time you merge changes to the
    main branch of your repository. This is how it should work if you follow a trunk-based
    development approach:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，服务管道将在您将更改合并到存储库主分支的每次都运行。如果您遵循基于主干的开发方法，它应该这样工作：
- en: When you merge changes to your main branch, this service pipeline runs and creates
    a set of artifacts using the latest code base. If the service pipeline succeeds,
    our artifacts will be releasable. We want to ensure that our main branch is always
    in a releasable state, so the service pipeline that runs on top of the main branch
    must always succeed. If, for some reason, this pipeline is failing, the team behind
    the service needs to switch the focus to fixing the problem as soon as possible.
    In other words, teams shouldn’t merge code into your main branch that breaks its
    service pipeline. We must also run a pipeline in our feature branches to do that.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您将更改合并到主分支时，此服务管道将运行并使用最新的代码库创建一系列工件。如果服务管道成功，我们的工件将可发布。我们希望确保我们的主分支始终处于可发布状态，因此运行在主分支之上的服务管道必须始终成功。如果由于某种原因，此管道失败，负责服务的团队需要尽快将重点转向解决问题。换句话说，团队不应该将破坏其服务管道的代码合并到主分支中。我们还必须在我们的功能分支中运行管道来完成这项工作。
- en: For each of your feature branches, a very similar pipeline should run to verify
    that the changes in the branch can be built, tested, and released against the
    main branch. In modern environments, the concept of GitHub pull requests is used
    to run these pipelines to make sure that before merging any pull request, a pipeline
    validates the changes.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于您的每个功能分支，应该运行一个非常相似的管道来验证分支中的更改是否可以构建、测试和与主分支一起发布。在现代环境中，使用 GitHub 拉取请求的概念来运行这些管道，以确保在合并任何拉取请求之前，管道验证了更改。
- en: It is common that after merging a set of features to the main branch, and because
    we know that the main branch is releasable at all times, the team in charge of
    the service decides to tag a new release. In Git, a new tag (a pointer to a specific
    commit) is created based on the main branch. The tag name is commonly used to
    represent the version of the artifact that the pipeline will create.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将一组功能合并到主分支后，由于我们知道主分支始终可以发布，负责服务的团队决定标记一个新的发布版本。在 Git 中，基于主分支创建一个新的标签（指向特定提交的指针）。标签名称通常用于表示管道将创建的工件版本。
- en: Figure 3.4 shows the pipelines configured for the main branch and a generic
    pipeline to validate feature branches only when pull requests are created. Multiple
    instances of these pipelines can be triggered to validate new changes continuously.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 显示了为主分支配置的管道和一个仅当创建拉取请求时才验证功能分支的通用管道。可以触发这些管道的多个实例来持续验证新更改。
- en: '![](../../OEBPS/Images/03-04.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4](../../OEBPS/Images/03-04.png)'
- en: Figure 3.4 Service pipelines for main branch and feature branches
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 主分支和功能分支的服务管道
- en: 'The service pipeline shown in figure 3.4 represents the most common steps you
    must execute every time you merge something to the main branch. Still, there are
    also some variations of this pipeline that you might need to run under different
    circumstances. Different events can kick off a pipeline execution, and we can
    have slightly different pipelines for different purposes, such as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4中所示的服务管道代表了每次将内容合并到主分支时必须执行的常见步骤。尽管如此，还有一些在此管道基础上进行的变体，您可能需要在不同的环境下运行。不同的事件可以触发管道执行，我们可以为不同的目的拥有略微不同的管道，例如：
- en: '*Validate a change in a feature branch:* This pipeline can execute the same
    steps as the pipeline in the main branch, but the artifacts generated should include
    the branch name, maybe as a version or as part of the artifact name. Running a
    pipeline after every change might be too expensive and not needed all the time,
    so you should decide based on your needs.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证功能分支中的变更:* 此管道可以执行与主分支中相同的步骤，但生成的工件应包括分支名称，可能作为版本号或作为工件名称的一部分。每次变更后运行管道可能成本太高且不是每次都需要，因此您应根据需要做出决定。'
- en: '*Validate a pull request (PR)/change request:* The pipeline will validate that
    the pull request/change request changes are valid and that artifacts can be produced
    with the recent changes. Usually, the result of the pipeline can be notified back
    to the user in charge of merging the PR and also block the merging options if
    the pipeline is failing. This pipeline is used to validate that whatever is merged
    into the main branch is valid and can be released. Validating pull requests and
    change requests can be an excellent option to avoid running pipelines for every
    change in the feature branches. When the developer(s) is ready to get feedback
    from the build system, it can create a PR that will trigger the pipeline. The
    pipeline would be retriggered if developers made changes on top of the pull request.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证拉取请求（PR）/变更请求:* 管道将验证拉取请求/变更请求的更改是否有效，并且可以使用最近的更改生成工件。通常，管道的结果可以通知给负责合并PR的用户，如果管道失败，还可以阻止合并选项。此管道用于验证合并到主分支的内容是否有效并可发布。验证拉取请求和变更请求可以是一个很好的选项，以避免在功能分支的每次变更时运行管道。当开发者准备好从构建系统获取反馈时，它可以创建一个将触发管道的PR。如果开发者在PR之上进行了更改，则管道将被重新触发。'
- en: Despite minor differences and optimizations that can be added to these pipelines,
    the behavior and produced artifacts are mostly the same. These conventions and
    approaches rely on the pipelines executing enough tests to validate that the produced
    service can be deployed to an environment.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以添加一些细微的差异和优化到这些管道中，但行为和生成的工件基本上是相同的。这些约定和方法依赖于执行足够的测试以验证生成的服务可以部署到环境中。
- en: 3.4.2 Service pipeline requirements
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 服务管道要求
- en: This section covers the infrastructural requirements for service pipelines to
    work and the contents of the source repository required for the pipeline to do
    its work.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了服务管道工作的基础设施要求以及管道执行工作所需的源存储库内容。
- en: 'Let’s start with the infrastructural requirements that a service pipeline needs
    to work:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从服务管道需要工作的基础设施要求开始：
- en: '*Webhooks for source code change notifications:* First, it needs access to
    register webhooks to the Git repository with the service’s source code, so a pipeline
    instance can be created when a new change is merged into the main branch.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*源代码变更通知的Webhooks:* 首先，它需要访问权限来将Webhooks注册到包含服务源代码的Git仓库中，以便在新的更改合并到主分支时创建一个管道实例。'
- en: '*Artifact repository available and valid credentials to push the binary artifacts:*
    Once the source code is built, we need to push the newly created artifact to the
    artifact repository where all artifacts are stored. This requires configuring
    an artifact repository with valid credentials to push new artifacts.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工件存储库可用以及推送二进制工件的有效凭证:* 一旦构建了源代码，我们需要将新创建的工件推送到存储所有工件的工件存储库。这需要配置一个具有有效凭证的工件存储库以推送新工件。'
- en: '*Container registry and valid credentials to push new container images:* In
    the same way as we need to push binary artifacts, we need to distribute our docker
    containers, so Kubernetes clusters can fetch the images when we want to provision
    a new instance of a service. A container registry with valid credentials is needed
    to accomplish this step.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*容器注册库和有效的凭证以推送新的容器镜像:* 就像我们需要推送二进制工件一样，我们还需要分发我们的Docker容器，以便Kubernetes集群在我们想要部署服务的新实例时可以获取到镜像。需要一个带有有效凭证的容器注册库来完成这一步骤。'
- en: '*Helm Chart repository and valid credentials:* Kubernetes manifest can be packaged
    and distributed as Helm Charts. If you are using Helm, you must have a Helm Chart
    repository and valid credentials to push these packages.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Helm图表存储库和有效的凭证:* Kubernetes清单可以打包并作为Helm图表分发。如果您使用Helm，您必须有一个Helm图表存储库和有效的凭证来推送这些包。'
- en: Figure 3.5 shows the most common external systems a pipeline instance will interact
    with. From a Git repository to artifact repositories and container registries,
    the team maintaining these pipelines must ensure that the right credentials are
    in place and that these components are reachable (from a network perspective)
    from where the pipelines are running.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5显示了管道实例将与之交互的最常见外部系统。从Git仓库到工件存储库和容器注册库，维护这些管道的团队必须确保正确的凭证到位，并且这些组件可以从管道运行的位置（从网络角度来看）访问。
- en: '![](../../OEBPS/Images/03-05.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5](../../OEBPS/Images/03-05.png)'
- en: Figure 3.5 Running pipelines requires a lot of infrastructure to be in place.
    This includes maintaining services and repositories, creating users and credentials,
    and making sure that these services (repositories) are accessible from remote
    locations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 运行管道需要大量的基础设施就绪。这包括维护服务和存储库、创建用户和凭证，并确保这些服务（存储库）可以从远程位置访问。
- en: For service pipelines to do their job, the repository containing the service’s
    source code also needs to have a `Dockerfile` or the ways to produce a container
    image and the necessary Kubernetes manifest to deploy the service into Kubernetes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让服务管道完成其工作，包含服务源代码的存储库也需要有一个`Dockerfile`或生成容器镜像的方式以及必要的Kubernetes清单，以便将服务部署到Kubernetes中。
- en: Figure 3.6 shows a possible directory layout of our service source code repository,
    which includes the source (src) directory containing all the files that will be
    compiled into binary format. The `Dockerfile` is used to build our container image
    for the service, and the Helm Chart directory contains all the files to create
    a Helm chart that can be distributed to install the service into a Kubernetes
    cluster. You can decide between having a Helm Chart per service or a single Helm
    Chart for all the application services.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6显示了我们的服务源代码存储库的可能目录布局，其中包含所有将被编译成二进制格式的文件所在的源（src）目录。`Dockerfile`用于构建服务的容器镜像，Helm图表目录包含创建可以分发以安装到Kubernetes集群的Helm图表所需的所有文件。您可以选择为每个服务创建一个Helm图表，或者为所有应用程序服务创建一个单独的Helm图表。
- en: Figure 3.6 shows the service layout, including the Helm Chart definition. This
    can help to package and distribute services independently. If we include everything
    needed to build, package, and run our service into a Kubernetes cluster, the service
    pipeline needs to run after every change in the main branch to create a new service
    release.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6显示了服务布局，包括Helm图表定义。这有助于独立打包和分发服务。如果我们把构建、打包和运行我们的服务所需的所有内容都包含到一个Kubernetes集群中，服务管道需要在主分支的每次更改后运行，以创建新的服务版本。
- en: '![](../../OEBPS/Images/03-06.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6](../../OEBPS/Images/03-06.png)'
- en: Figure 3.6 The service source code repository needs to have all the configurations
    for the service pipeline to work.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 服务源代码存储库需要所有配置，以便服务管道能够工作。
- en: In summary, service pipelines are responsible for building our source and related
    artifacts to be deployed into an environment. As mentioned, service pipelines
    are not responsible for deploying the produced service into a live environment.
    The environment pipeline’s responsibility is covered in the next chapter.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，服务管道负责构建我们的源和相关工件以部署到环境中。如前所述，服务管道不负责将生成的服务部署到实际环境中。环境管道的责任将在下一章中介绍。
- en: 3.4.3 Opinions, limitations, and compromises around service pipelines
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 关于服务管道的意见、限制和妥协
- en: 'No “one size fits all” solution exists for creating our service pipelines.
    In real life, you must make compromises depending on your requirements. Before
    looking at tools like Tekton, Dagger, and GitHub Actions, let me quickly touch
    on some practical aspects I’ve seen teams fighting with. Here is a short and non-comprehensive
    list of things to consider when designing your service pipelines:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建服务管道方面，没有“一刀切”的解决方案。在现实生活中，您必须根据您的需求做出妥协。在查看Tekton、Dagger和GitHub Actions等工具之前，让我快速谈谈一些我看到团队在斗争中的实际方面。以下是在设计您的服务管道时需要考虑的一些简短且非详尽的清单：
- en: '*Avoid strict rules and opinions to define where service pipelines start and
    end:* For example, your services might not need to be packaged as Helm Charts,
    as mentioned in the previous sections. If there are not enough cases when you
    want to install an isolated service—for example, your service depends heavily
    on other services—removing that step from the service pipeline and the chart definition
    from the service repository might make a lot of sense.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*避免制定严格的规则和观点来定义服务管道的起点和终点：* 例如，您的服务可能不需要像前几节中提到的那样打包成Helm图表。如果没有足够的案例表明您想要安装一个隔离的服务——例如，您的服务严重依赖于其他服务——那么从服务管道和图表定义中移除这一步骤可能很有意义。'
- en: '*Understand the lifecycle* *of your components and artifacts:* Depending on
    how often services change and their dependencies, service pipelines can be linked
    together to build a set of services together. Mapping these relationships and
    understanding the needs of the teams operating these services will give you the
    right granularity to create your service pipelines. For example, you can enable
    your teams to keep releasing new container images for new versions of the services
    that they are working on, but a different team controls the cadence and release
    of the Helm Charts that bundle all the application services together.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*了解您组件和工件的生命周期：* 根据服务变更的频率及其依赖关系，服务管道可以相互链接，共同构建一组服务。映射这些关系并理解操作这些服务的团队的需求，将为您提供创建服务管道的正确粒度。例如，您可以允许您的团队持续发布新版本的服务的新容器镜像，但不同的团队控制着将所有应用程序服务捆绑在一起的Helm图表的节奏和发布。'
- en: '*Find what works best for your organization:* Optimize end-to-end automation
    based on business priorities. If a critical service is causing delays to be released
    and deployed, focus on having the service pipeline ready and fully functional
    before trying to cover other services. There is no point in creating generic solutions
    that might take a while to figure out that your organization suffers 80% of the
    cases with a single service.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*找到最适合您组织的方案：* 根据业务优先级优化端到端自动化。如果一个关键服务导致发布和部署延迟，请在尝试覆盖其他服务之前，确保服务管道已准备就绪并完全可用。创建通用的解决方案是没有意义的，因为这些方案可能需要一段时间才能发现您的组织在80%的情况下都只使用单一服务。'
- en: '*Do not create unnecessary steps until they are required:* I’ve heavily mentioned
    tools like Helm in this book to package and distribute Kubernetes manifest, but
    I am not suggesting that is the way to go. I’ve used Helm as an example tool that
    is widely adopted, but you might be in a situation where you don’t need to package
    your Kubernetes manifest for distribution. Your service pipeline shouldn’t have
    that step if that’s the case. If the need arises later, you can extend your service
    pipelines to include more steps.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不要创建不必要的步骤，直到它们是必需的：* 我在这本书中多次提到Helm这样的工具来打包和分发Kubernetes清单，但我并不是建议这就是正确的方法。我使用Helm作为一个广泛采用的示例工具，但您可能处于不需要打包您的Kubernetes清单进行分发的情况。如果这种情况发生，您的服务管道不应该包含这一步骤。如果以后需要，您可以扩展您的服务管道以包含更多步骤。'
- en: Let’s now jump to see some tools in this space.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看这个领域的一些工具。
- en: 3.5 Service pipelines in action
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 服务管道的实际应用
- en: There are several pipeline engines out there, even fully managed services like
    GitHub Actions ([https://github.com/features/actions](https://github.com/features/actions))
    and several well-known CI (continuous integration) managed services that will
    provide loads of integrations for you to build and package your application’s
    services.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 目前市面上有几种管道引擎，甚至包括像GitHub Actions（[https://github.com/features/actions](https://github.com/features/actions)）这样的完全托管服务以及几个知名的CI（持续集成）托管服务，它们将为您提供大量集成，以便您构建和打包应用程序的服务。
- en: 'In the following sections, we will examine two projects: Tekton and Dagger.
    These projects provide you with the tools to work with cloud-native applications
    and, as we will see in chapter 6, enable platform teams to package, distribute,
    and reuse the organization’s specific knowledge built over time. Tekton ([https://tekton.dev](https://tekton.dev))
    was designed as a pipeline engine for Kubernetes. Because Tekton is a generic
    pipeline engine, you can create any pipeline with it. On the other hand, a much
    newer project called Dagger ([https://dagger.io](https://dagger.io)) was designed
    to run everywhere. We will contrast Tekton and Dagger with GitHub actions.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将考察两个项目：Tekton 和 Dagger。这些项目为你提供了与云原生应用程序一起工作的工具，正如我们将在第 6 章中看到的，它们使平台团队能够打包、分发和重用组织在一段时间内构建的特定知识。Tekton（[https://tekton.dev](https://tekton.dev)）被设计为
    Kubernetes 的管道引擎。因为 Tekton 是一个通用的管道引擎，你可以用它创建任何管道。另一方面，一个名为 Dagger（[https://dagger.io](https://dagger.io)）的更新的项目被设计为可以在任何地方运行。我们将通过
    GitHub Actions 对比 Tekton 和 Dagger。
- en: 3.5.1 Tekton in action
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 Tekton 应用实例
- en: Tekton was initially created as part of the Knative project ([https://knative.dev](https://knative.dev))
    from Google. (We will look more into Knative in chapter 8). Tekton was initially
    called Knative Build, and later separated from Knative to be an independent project.
    Tekton’s main characteristic is that it is a cloud-native pipeline engine designed
    for Kubernetes. This section will look into how to use Tekton to define service
    pipelines.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Tekton 最初是由 Google 的 Knative 项目（[https://knative.dev](https://knative.dev)）的一部分创建的。（我们将在第
    8 章中更深入地了解 Knative）。Tekton 最初被称为 Knative Build，后来从 Knative 中分离出来成为一个独立的项目。Tekton
    的主要特点是它是一个为 Kubernetes 设计的云原生管道引擎。本节将探讨如何使用 Tekton 定义服务管道。
- en: 'In Tekton, you have two main concepts: tasks and pipelines. In Tekton, the
    pipeline engine is a set of components that understand how to execute `Tasks`
    and `Pipelines` Kubernetes resources. Tekton, like most of the Kubernetes projects
    covered in this book, can be installed into your Kubernetes cluster. I strongly
    recommend you check their official documentation page, which explains the value
    of using a tool like Tekton at [https://tekton.dev/docs/concepts/overview/](https://tekton.dev/docs/concepts/overview/).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Tekton 中，你有两个主要概念：任务和管道。在 Tekton 中，管道引擎是一组理解如何执行 `Tasks` 和 `Pipelines` Kubernetes
    资源组件。Tekton，就像本书中涵盖的大多数 Kubernetes 项目一样，可以安装到你的 Kubernetes 集群中。我强烈建议你查看他们的官方文档页面，该页面解释了使用像
    Tekton 这样的工具的价值，网址为 [https://tekton.dev/docs/concepts/overview/](https://tekton.dev/docs/concepts/overview/)。
- en: Note I’ve included a set of step-by-step tutorials in this repository. You can
    start by looking at how to install Tekton in your cluster and the `tekton/hello-world/`
    example at [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我在这个存储库中包含了一系列逐步教程。你可以从查看如何在你的集群中安装 Tekton 以及 `tekton/hello-world/` 示例开始，该示例位于
    [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton)。
- en: When you install Tekton, you install a set of custom resource definitions, which
    are extensions to the Kubernetes APIs, that define tasks and pipelines. Tekton
    also installs the pipeline engine that knows how to deal with `Tasks` and `Pipelines`
    resources. Notice that after installing Tekton, you can also install the Tekton
    Dashboard and the `tkn` command-line interface tool.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当你安装 Tekton 时，你安装了一套自定义资源定义，这是 Kubernetes API 的扩展，用于定义任务和管道。Tekton 还安装了知道如何处理
    `Tasks` 和 `Pipelines` 资源的管道引擎。请注意，在安装 Tekton 之后，你还可以安装 Tekton Dashboard 和 `tkn`
    命令行界面工具。
- en: Once you install the Tekton release, you will see a new namespace called `tekton-pipelines`,
    which contains the pipeline controller (the pipeline engine), and the pipeline
    webhook listener, which is used to listen for events coming from external sources,
    such as git repositories.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 Tekton 版本，你将看到一个名为 `tekton-pipelines` 的新命名空间，其中包含管道控制器（管道引擎）和管道 webhook
    监听器，后者用于监听来自外部源的事件，例如 Git 仓库。
- en: A task in Tekton will look like a normal Kubernetes resource, as shown in listing
    3.1.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Tekton 中的任务看起来就像一个普通的 Kubernetes 资源，如列表 3.1 所示。
- en: Listing 3.1 Simple Tekton task definition
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 简单的 Tekton 任务定义
- en: '[PRE0]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① The name of the resource defined in metadata.name represents the task definition
    name.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ① 元数据中定义的资源名称代表任务定义名称。
- en: ② We can use the params section to define which parameters can be configured
    for our task definition.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们可以使用 `params` 部分来定义可以为我们的任务定义配置哪些参数。
- en: ③ The Docker image called Ubuntu is going to be used for this task.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 这个任务将使用名为Ubuntu的Docker镜像。
- en: ④ The command arguments (args) in this case are just a “Hello World” string;
    notice that you can send a list of arguments for more complex commands.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在这种情况下，命令参数（args）只是一个“Hello World”字符串；请注意，你可以为更复杂的命令发送参数列表。
- en: '⑤ The command arguments (args) in this case are just a “Hello World: $(params.name)”
    string, which will use the Task parameter.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '⑤ 在这种情况下，命令参数（args）只是一个“Hello World: $(params.name)”字符串，它将使用任务参数。'
- en: 'You can find the task definition in this repository, alongside a step-by-step
    tutorial to run it in your cluster: [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-task.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-task.yaml).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个存储库中找到任务定义，以及一个逐步教程，教你如何在你的集群中运行它：[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-task.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-task.yaml)。
- en: Derived from this example, you can create a task for whatever you want, because
    you have the flexibility to define which container to use and which commands to
    run. Once you have the task definition, you need to make that available to Tekton
    by applying this file to the cluster with `kubectl apply -f task.yaml`. By applying
    the file into Kubernetes, we are only making the definition available to the Tekton
    components in the cluster, but the task will not run.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中，你可以为任何你想要的东西创建一个任务，因为你有权定义使用哪个容器以及运行哪些命令。一旦你有任务定义，你需要通过将此文件应用到集群中（`kubectl
    apply -f task.yaml`）来使它对Tekton可用。通过将文件应用到Kubernetes中，我们只是在集群中使定义对Tekton组件可用，但任务不会运行。
- en: If you want to run this task, a task can be executed multiple times. Tekton
    requires you to create a TaskRun resource like the following listing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要运行这个任务，一个任务可以被多次执行。Tekton要求你创建一个类似于以下列表的TaskRun资源。
- en: Listing 3.2 A task run represents an instance of our task definition
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2 任务运行表示任务定义的一个实例
- en: '[PRE1]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① We can define specific parameter values for this TaskRun.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们可以为这个TaskRun定义特定的参数值。
- en: ② We need to reference the name of the task definition that we want to run.
    Notice that this name is unique per task resource that we define.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们需要引用我们想要运行的任务定义的名称。请注意，这个名称对于每个我们定义的任务资源都是唯一的。
- en: The TaskRun resource can be found at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/task-run.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/task-run.yaml).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: TaskRun资源可以在[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/task-run.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/task-run.yaml)找到。
- en: If you apply this TaskRun to the cluster (`kubectl apply -f taskrun.yaml`),
    the pipeline engine will execute this task. You can take a look at the Tekton
    task in action by looking at the TaskRun resources in listing 3.3.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这个TaskRun应用到集群中（`kubectl apply -f taskrun.yaml`），管道引擎将执行这个任务。你可以通过查看列表3.3中的TaskRun资源来查看Tekton任务的实际操作。
- en: Listing 3.3 Get all TaskRun instances
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.3 获取所有TaskRun实例
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you list all the running pods, you will notice that each task creates a pod,
    as shown in listing 3.4.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你列出所有正在运行的Pod，你会注意到每个任务都会创建一个Pod，如列表3.4所示。
- en: Listing 3.4 List all the pods associated to TaskRuns
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 列出与TaskRuns关联的所有Pod
- en: '[PRE3]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And because you have a pod, you can tail the logs to see what the task is doing
    as in listing 3.5.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你有一个Pod，你可以使用Pod名称来查看任务正在做什么，如列表3.5所示。
- en: Listing 3.5 Accessing the TaskRun logs using the pod name
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 使用Pod名称访问TaskRun日志
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You just executed your first Tekton TaskRun. Congrats! But a single task is
    not interesting at all. If we can sequence multiple tasks together, we can create
    our service pipelines. Let’s look at how we can build Tekton pipelines from this
    simple task example.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚执行了你的第一个Tekton TaskRun。恭喜！但单个任务根本不有趣。如果我们能将多个任务串联起来，我们就可以创建我们的服务管道。让我们看看如何从这个简单的任务示例构建Tekton管道。
- en: 3.5.2 Pipelines in Tekton
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 Tekton中的管道
- en: A task can be helpful, but Tekton becomes interesting when you create sequences
    of these tasks using pipelines.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个任务可能很有用，但当你使用管道创建这些任务的序列时，Tekton才变得有趣。
- en: A pipeline is a collection of these tasks in a concrete sequence. The following
    pipeline uses the task definition that we defined earlier. It prints a message,
    fetches a file from a URL, and then reads its content, which is forwarded to our
    Hello World task, which prints a message.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 管道是一系列按具体顺序排列的任务。以下管道使用了我们之前定义的任务定义。它打印一条消息，从URL获取一个文件，然后读取其内容，该内容被转发到我们的Hello
    World任务，该任务打印一条消息。
- en: Figure 3.7 shows a simple Tekton pipeline comprising three Tekton tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7显示了一个包含三个Tekton任务的简单Tekton管道。
- en: '![](../../OEBPS/Images/03-07.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-07.png)'
- en: Figure 3.7 Simple Tekton pipeline using our Hello World task
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 使用我们的Hello World任务的简单Tekton管道
- en: In this simple pipeline, we are using an existing task definition (`wget`) from
    the Tekton Hub, which is a community repository hosting generic tasks, and then
    we are defining the `cat` task inline inside the pipeline to showcase Tekton flexibility,
    to finally use the `Hello World` task that we defined in the previous section.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的管道中，我们使用了一个来自Tekton Hub的现有任务定义（`wget`），Tekton Hub是一个社区仓库，托管通用任务，然后我们在管道内定义了`cat`任务，以展示Tekton的灵活性，最后使用我们在上一节中定义的`Hello
    World`任务。
- en: Let’s look at a simple service pipeline defined in Tekton (`hello-world-pipeline.yaml`).
    Don’t be scared. This is a lot of YAML, I warned you. See listing 3.6.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Tekton中定义的一个简单服务管道（`hello-world-pipeline.yaml`）。别害怕。这是一大堆YAML，我警告过您。参见列表3.6。
- en: Listing 3.6 Pipeline definition
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.6 管道定义
- en: '[PRE5]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Pipeline resources can define an array of results that are expected from the
    pipeline when they are executed. Tasks
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ① 管道资源可以定义在执行时预期的结果数组。任务
- en: ② In the same way as tasks, we can define which parameters can be set by the
    user when running this pipeline. These pipeline parameters can be forwarded to
    individual tasks if needed. can set these results values when they are executed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ② 与任务一样，我们可以定义在运行此管道时用户可以设置的参数。如果需要，这些管道参数可以转发到单个任务。可以在它们执行时设置这些结果值。
- en: ③ Pipelines and tasks allow the use of Tekton Workspaces to store persistent
    information. This can be used to share information between tasks. As each task
    is executed in its container, using persistent storage to share information is
    easy to set up.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 管道和任务允许使用Tekton Workspaces来存储持久信息。这可以用于在任务之间共享信息。由于每个任务都在其容器中执行，因此使用持久存储来共享信息很容易设置。
- en: ④ We use a task reference to a task we didn’t create. We need to make sure to
    install this task definition before creating a PipelineRun for this pipeline.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们使用对未创建的任务的任务引用。我们需要确保在为该管道创建PipelineRun之前安装此任务定义。
- en: ⑤ We can define tasks inline the pipeline if we want to. This makes the pipeline
    file more complicated, but sometimes it is useful to have a task that just glues
    other tasks together, as is shown in this case. The only purpose of this task
    is to read the content of the downloaded file and make it available as a String
    for our Hello World task that doesn’t accept a file.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 如果我们想的话，可以在管道内定义任务。这使得管道文件更复杂，但有时有一个仅仅将其他任务粘合在一起的任务是有用的，就像在这个例子中一样。这个任务的唯一目的是读取下载文件的内容，并将其作为字符串提供给我们的Hello
    World任务，该任务不接受文件。
- en: ⑥ This also requires installing the “hello-world-task” definition in the cluster.
    Remember that you can always run “kubectl get tasks” to see which tasks are available.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 这也要求在集群中安装“hello-world-task”定义。请记住，您始终可以运行“kubectl get tasks”来查看哪些任务可用。
- en: ⑦ We can use Tekton’s powerful templating mechanism to provide the value for
    Hello World task. We are using a reference to the “cat” task results.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 我们可以使用Tekton强大的模板机制为Hello World任务提供值。我们使用对“cat”任务结果的引用。
- en: You can find the full pipeline definition at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-pipeline.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-pipeline.yaml).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-pipeline.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/hello-world-pipeline.yaml)找到完整的管道定义。
- en: 'Before applying the pipeline definition, you need to install the `wget` Tekton
    task that was created and maintained by the Tekton community:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用管道定义之前，您需要安装由Tekton社区创建和维护的`wget` Tekton任务：
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once again, you must apply this pipeline resource to your cluster for Tekton
    to know about: `kubectl apply -f hello-world-pipeline.yaml`.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，您必须将此管道资源应用到您的集群中，以便Tekton能够识别：`kubectl apply -f hello-world-pipeline.yaml`。
- en: As you can see in the pipeline definition, the `spec.tasks` field contains an
    array of tasks. These tasks need to be already deployed into the cluster, and
    the pipeline definition defines the sequence in which these tasks will be executed.
    These task references can be your tasks, or as in the example, they can come from
    the Tekton catalog, a repository containing community-maintained task definitions
    that you can reuse.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在管道定义中所见，`spec.tasks`字段包含一个任务数组。这些任务需要已经部署到集群中，并且管道定义定义了这些任务将执行的顺序。这些任务引用可以是你的任务，或者如示例所示，它们可以来自Tekton目录，这是一个包含社区维护的任务定义的存储库，你可以重用它。
- en: In the same way, because tasks need TaskRuns for the executions, you will need
    to create a PipelineRun for every time you want to execute your pipeline, as shown
    in the following listing.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，因为任务需要TaskRuns来执行，所以每次你想执行你的管道时，你都需要创建一个PipelineRun，如下面的列表所示。
- en: Listing 3.7 PipelineRun represent an instance (execution) of our pipelines
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.7 PipelineRun代表我们管道的一个实例（执行）
- en: '[PRE7]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① When we create a PipelineRun, we need to bound the workspaces defined in the
    pipeline definition to real storage. In this case a VolumeClaim is created requesting
    1 Mb of storage for the PipelineRun to use.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ① 当我们创建一个PipelineRun时，我们需要将管道定义中定义的工作区绑定到实际的存储。在这种情况下，创建了一个VolumeClaim，请求为PipelineRun使用1
    Mb的存储。
- en: ② The pipeline parameter “url” can be any URL that you want as soon because
    it is accessible from the PipelineRun context (meaning that it can reach the URL,
    and it is not behind a firewall).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ② 管道参数“url”可以是任何你想要的URL，因为它可以从PipelineRun上下文中访问（这意味着它可以访问该URL，并且它不在防火墙后面）。
- en: ③ As with tasks, we need to provide the name of the pipeline definition that
    we want to use for this PipelineRun.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 与任务一样，我们需要提供我们想要用于此PipelineRun的管道定义的名称。
- en: You can find the PipelineRun resource at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/pipeline-run.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/pipeline-run.yaml).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/pipeline-run.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/pipeline-run.yaml)找到PipelineRun资源。
- en: When you apply this file to the cluster `kubectl apply -f pipeline-run.yaml`,
    Tekton will execute the pipeline by running all the tasks defined in the pipeline
    definition. When running this pipeline, Tekton will create one pod per task and
    three TaskRun resources. A pipeline is just orchestrating tasks, or in other words
    creating TaskRuns.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将此文件应用到集群中`kubectl apply -f pipeline-run.yaml`时，Tekton将通过运行管道定义中定义的所有任务来执行管道。在运行此管道时，Tekton将为每个任务创建一个pod和三个TaskRun资源。管道只是编排任务，换句话说，就是创建TaskRuns。
- en: To check that the TaskRuns were created and that the pipeline executed successfully,
    see listing 3.8.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查TaskRuns是否已创建并且管道已成功执行，请参阅列表3.8。
- en: Listing 3.8 Getting task runs from the pipeline execution
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8 从管道执行中获取任务运行
- en: '[PRE8]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For each TaskRun, Tekton created a pod (listing 3.9).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个TaskRun，Tekton会创建一个pod（见3.9列表）。
- en: Listing 3.9 Checking that all TaskRuns belonging to the pipeline have finished
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.9 检查属于管道的所有TaskRun是否已完成
- en: '[PRE9]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Review the logs from the `hello-world-pipeline-run-1-hello-world-pod` to see
    what the task printed, as shown in listing 3.10.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 查看来自`hello-world-pipeline-run-1-hello-world-pod`的日志，以查看任务打印了什么，如列表3.10所示。
- en: Listing 3.10 Getting the logs from the last task
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.10 获取最后一个任务的日志
- en: '[PRE10]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can always look at Tasks, TaskRuns, Pipelines, and PipelineRuns in the
    Tekton dashboard. To access the Tekton dashboard, if you installed it in your
    cluster, you need to first run:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以始终在Tekton仪表板中查看Tasks、TaskRuns、Pipelines和PipelineRuns。要访问Tekton仪表板，如果你在集群中安装了它，你首先需要运行：
- en: '[PRE11]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Figure 3.8 shows the Tekton dashboard user interface, where we can explore our
    task and pipeline definitions as well as trigger new task and pipeline runs and
    explore the logs that each task outputs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8显示了Tekton仪表板用户界面，在那里我们可以探索我们的任务和管道定义，以及触发新的任务和管道运行并探索每个任务输出的日志。
- en: '![](../../OEBPS/Images/03-08.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-08.png)'
- en: Figure 3.8 Our PipelineRun execution in the Tekton dashboard
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 Tekton仪表板中的我们的PipelineRun执行
- en: 'If required, you can find a step-by-step tutorial on how to install Tekton
    in your Kubernetes cluster and how to run the service pipeline at the following
    repository: [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/README.md).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，你可以在以下存储库中找到如何在你的 Kubernetes 集群中安装 Tekton 以及如何运行服务流水线的分步教程：[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/tekton/hello-world/README.md)。
- en: 'At the end of the tutorial, you will find links to more complex pipelines I’ve
    defined for each Conference application service. These pipelines are more complex
    because they require access to external services, credentials to publish artifacts
    and container images, and the rights to do some privileged actions inside the
    cluster. Check this section of the tutorial if you are interested in more details:
    [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton#tekton-for-service-pipelines](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton#tekton-for-service-pipelines).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在教程的最后，你可以找到链接到我为每个会议应用服务定义的更复杂的流水线的链接。这些流水线更复杂，因为它们需要访问外部服务、发布工件和容器镜像的凭证，以及在集群内部执行一些特权操作的权限。如果你对这个部分感兴趣，请查看教程的此部分：[https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton#tekton-for-service-pipelines](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3/tekton#tekton-for-service-pipelines)。
- en: 3.5.3 Tekton advantages and extras
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 Tekton 优点和附加功能
- en: 'As we have seen, Tekton is super flexible and allows you to create advanced
    pipelines, and it includes other features, such as:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Tekton 非常灵活，允许你创建高级流水线，并且它还包括其他功能，例如：
- en: Input and output mappings to share data between tasks
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出映射，用于在任务之间共享数据
- en: Event triggers that allow you to listen for events that will trigger pipelines
    or tasks
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件触发器，允许你监听将触发流水线或任务的事件
- en: A command-line tool to easily interact with tasks and pipelines from your terminal
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个命令行工具，可以轻松地从终端与任务和流水线交互
- en: A simple dashboard to monitor your pipelines and task executions (figure 3.9)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的仪表板，用于监控你的流水线和任务执行（图 3.9）
- en: '![](../../OEBPS/Images/03-09.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9](../../OEBPS/Images/03-09.png)'
- en: Figure 3.9 Tekton dashboard—a user interface for monitoring your pipelines
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 Tekton 仪表板——用于监控你的流水线的用户界面
- en: Figure 3.9 shows the community-driven Tekton dashboard, which you can use to
    visualize the execution of your pipelines. Remember that because Tekton was built
    to work on top of Kubernetes, you can monitor your pipelines using `kubectl` as
    with any other Kubernetes resource. Still, nothing beats a user interface for
    less technical users.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 展示了由社区驱动的 Tekton 仪表板，你可以使用它来可视化你的流水线执行。记住，因为 Tekton 是构建在 Kubernetes 之上的，你可以像监控其他
    Kubernetes 资源一样使用 `kubectl` 来监控你的流水线。然而，对于不太懂技术的用户来说，没有什么能比用户界面更好的了。
- en: But now, if you want to implement a service pipeline with Tekton, you will spend
    quite a bit of time defining tasks, the pipeline, how to map inputs and outputs,
    defining the right events listener for your Git repositories, and then going more
    low-level into defining which docker images you will use for each task. Creating
    and maintaining these pipelines and their associated resources can become a full-time
    job, and for that, Tekton launched an initiative to define a catalog where tasks
    (pipelines and resources are planned for future releases) can be shared. The Tekton
    catalog is available at [https://github.com/tektoncd/catalog](https://github.com/tektoncd/catalog).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，如果你想要使用 Tekton 实现一个服务流水线，你将花费相当多的时间来定义任务、流水线、如何映射输入和输出、定义适合你的 Git 仓库的正确事件监听器，然后更深入地定义每个任务将使用的
    Docker 镜像。创建和维护这些流水线及其相关资源可能变成一份全职工作，为此，Tekton 启动了一个项目来定义一个目录，其中可以共享任务（为未来的发布计划了流水线和资源）。Tekton
    目录可在 [https://github.com/tektoncd/catalog](https://github.com/tektoncd/catalog)
    找到。
- en: With the help of the Tekton catalog, we can create pipelines that reference
    tasks defined in the catalog. In the previous section, we used the `wget` task
    downloaded from this catalog; you can find a full description of the `wget` task
    at [https://hub.tekton.dev/tekton/task/wget](https://hub.tekton.dev/tekton/task/wget).
    Hence, we don’t need to worry about defining them. You can also visit [https://hub.tekton.dev](https://hub.tekton.dev),
    which allows you to search for task definitions and provides detailed documentation
    about installing and using these tasks in your pipelines (figure 3.10).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Tekton 目录的帮助下，我们可以创建引用目录中定义的任务的管道。在前一节中，我们使用了从该目录下载的 `wget` 任务；您可以在 [https://hub.tekton.dev/tekton/task/wget](https://hub.tekton.dev/tekton/task/wget)
    找到 `wget` 任务的完整描述。因此，我们不需要担心定义它们。您还可以访问 [https://hub.tekton.dev](https://hub.tekton.dev)，它允许您搜索任务定义，并提供有关在管道中安装和使用这些任务的详细文档（图
    3.10）。
- en: 'Tekton Hub and the Tekton catalog allow you to reuse tasks and pipelines created
    by a large community of users and companies. I strongly recommend you check out
    the Tekton Overview page, which summarizes the advantages of using Tekton, including
    who should use Tekton and why: [https://tekton.dev/docs/concepts/overview/](https://tekton.dev/docs/concepts/overview/).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Tekton Hub 和 Tekton 目录允许您重用大量用户和公司创建的任务和管道。我强烈建议您查看 Tekton 概述页面，该页面总结了使用 Tekton
    的优势，包括谁应该使用 Tekton 以及原因：[https://tekton.dev/docs/concepts/overview/](https://tekton.dev/docs/concepts/overview/)。
- en: '![](../../OEBPS/Images/03-10.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/03-10.png)'
- en: Figure 3.10 Tekton Hub is a portal to share and reuse tasks and pipeline definitions
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 Tekton Hub 是一个共享和重用任务和管道定义的门户
- en: 'Tekton is quite a mature project in the cloud-native space, but it also presents
    some challenges:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Tekton 是云原生空间中相当成熟的项目，但也带来了一些挑战：
- en: You need to install and maintain Tekton running inside a Kubernetes cluster.
    You don’t want your pipelines running right beside your application workloads,
    so you might need a separate cluster.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要在 Kubernetes 集群内部署并维护 Tekton。您不希望您的管道直接运行在应用程序工作负载旁边，因此您可能需要一个单独的集群。
- en: There is no easy way to run a Tekton pipeline locally. For development purposes,
    you rely on having access to a Kubernetes cluster to run a pipeline manually.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地运行 Tekton 管道没有简单的方法。出于开发目的，您需要能够访问 Kubernetes 集群以手动运行管道。
- en: You need to know Kubernetes to define and create tasks and pipelines.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要了解 Kubernetes 来定义和创建任务和管道。
- en: While Tekton provides some conditional logic, it is limited by what you can
    do in YAML and using a declarative approach of Kubernetes.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 Tekton 提供了一些条件逻辑，但它受到 YAML 中可以执行的操作以及 Kubernetes 的声明式方法的限制。
- en: We will now jump into a project called Dagger that was created to mitigate some
    of these points, not to replace Tekton but to provide a different approach to
    solving everyday challenges when building complex pipelines.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将跳入一个名为 Dagger 的项目，该项目旨在缓解一些这些问题，不是为了取代 Tekton，而是为了提供解决日常挑战的不同方法，当构建复杂管道时。
- en: 3.5.4 Dagger in action
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 Dagger 在行动
- en: 'Dagger ([https://dagger.io](https://dagger.io)) was born with one objective:
    “to enable developers to build pipelines using their favorite programming language
    that they can run everywhere.” Dagger only relies on a container runtime to run
    pipelines that can be defined using code that every developer can write. Dagger
    currently supports Go, Python, TypeScript, and JavaScript SDKs, but the team behind
    Dagger is quickly expanding to new languages.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Dagger ([https://dagger.io](https://dagger.io)) 的诞生有一个目标：“让开发者能够使用他们喜欢的编程语言构建管道，并且可以在任何地方运行。”Dagger
    只依赖于容器运行时来运行可以使用代码定义的管道，而任何开发者都可以编写这些代码。Dagger 目前支持 Go、Python、TypeScript 和 JavaScript
    SDKs，但 Dagger 背后的团队正在快速扩展到新的语言。
- en: Dagger is not focused on Kubernetes only. Platform teams must ensure that while
    teams use Kubernetes’ powerful and declarative nature, also development teams
    can be productive and use the appropriate tool for the job. This short section
    will examine how Dagger compares with Tekton, where it can fit better, and where
    it can complement other tools.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Dagger 并不专注于 Kubernetes。平台团队必须确保，在团队使用 Kubernetes 强大且声明式特性的同时，开发团队也能高效工作并使用适合工作的适当工具。本节将探讨
    Dagger 与 Tekton 的比较，它更适合的地方，以及它如何补充其他工具。
- en: 'If you are interested in getting started with Dagger, you can check these resources:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想开始使用 Dagger，您可以查看这些资源：
- en: '*Dagger docs*: [https://docs.dagger.io](https://docs.dagger.io)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dagger 文档*: [https://docs.dagger.io](https://docs.dagger.io)'
- en: '*Dagger Quickstart*: [https://docs.dagger.io/648215/quickstart/](https://docs.dagger.io/648215/quickstart/)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dagger 快速入门*: [https://docs.dagger.io/648215/quickstart/](https://docs.dagger.io/648215/quickstart/)'
- en: '*Dagger GraphQL playground*: [https://play.dagger.cloud](https://play.dagger.cloud)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dagger GraphQL playground*: [https://play.dagger.cloud](https://play.dagger.cloud)'
- en: Dagger, like Tekton, also has a pipeline engine, but this engine can work both
    locally and remotely, providing a unified runtime across environments. Dagger
    doesn’t directly integrate with Kubernetes. This means that there are no Kubernetes
    CRDs or YAML involved. This can be important depending on the skills and preferences
    of the teams in charge of creating and maintaining these pipelines.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Dagger，就像 Tekton 一样，也有一个管道引擎，但这个引擎可以在本地和远程工作，为不同环境提供统一的运行时。Dagger 不直接与 Kubernetes
    集成。这意味着没有 Kubernetes CRDs 或 YAML 涉及。这取决于负责创建和维护这些管道的团队的技术和偏好，可能很重要。
- en: In Dagger, we define pipelines by writing code. Because pipelines are just code,
    these pipelines can be distributed using any code packaging tools. For example,
    if our pipelines are written in Go, we can use Go modules to import pipelines
    or tasks written by other teams. If we use Java, we can use Maven or Gradle to
    package and distribute our pipeline libraries to promote reuse.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dagger 中，我们通过编写代码来定义管道。因为管道只是代码，所以这些管道可以使用任何代码打包工具进行分发。例如，如果我们的管道是用 Go 编写的，我们可以使用
    Go 模块导入其他团队编写的管道或任务。如果我们使用 Java，我们可以使用 Maven 或 Gradle 打包和分发我们的管道库以促进重用。
- en: Figure 3.11 shows how development teams can write pipelines using the Dagger
    SDKs and then use the Dagger engine to execute these pipelines using any OCI Container
    Runtime such as Docker or PodMan. It doesn’t matter if you want to run your pipelines
    in your local development environment (your laptop with Docker for Mac or Windows),
    your continuous integration environment, or even inside Kubernetes. These pipelines
    will behave in the same way.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 展示了开发团队如何使用 Dagger SDKs 编写管道，然后使用 Dagger 引擎执行这些管道，使用任何 OCI 容器运行时，如 Docker
    或 PodMan。无论你是在本地开发环境（装有 Docker for Mac 或 Windows 的笔记本电脑）中运行管道，还是在持续集成环境中，甚至是在
    Kubernetes 集群内部，这些管道的行为都是相同的。
- en: '![](../../OEBPS/Images/03-11.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/03-11.png)'
- en: 'Figure 3.11 Using your preferred programming language and its tools to write
    pipelines (Source: dagger.io)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 使用你喜欢的编程语言及其工具编写管道（来源：dagger.io）
- en: The Dagger pipeline engine is then in charge of orchestrating the tasks defined
    in the pipelines and optimizing what is requested by the container runtime used
    to execute each task. A significant advantage of the Dagger pipeline engine is
    that it was designed from the ground up to optimize how pipelines run. Imagine
    that you are building tons of services multiple times a day. You will not only
    keep your CPUs hot, but the amount of traffic downloading artifacts, again and
    again, becomes expensive—more if you are running on top of a cloud provider, which
    charges you based on consumption.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Dagger 管道引擎负责协调管道中定义的任务，并优化每个任务使用的容器运行时请求的内容。Dagger 管道引擎的一个显著优势是它从一开始就被设计用来优化管道的运行方式。想象一下，如果你每天要构建成吨的服务多次，你不仅会保持你的
    CPU 热起来，而且下载工件所产生的流量，一次又一次地，变得昂贵——如果你在云服务提供商之上运行，他们根据消费量向你收费，那就更贵了。
- en: Dagger, similar to Tekton, uses containers to execute each task (step) in the
    pipeline. The pipeline engine optimizes the resource consumption by caching the
    results of previous executions, preventing you from re-executing tasks that were
    already executed using the same inputs. In addition, you can run the Dagger engine
    locally on your laptop/workstation or remotely, even inside a Kubernetes cluster.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Dagger，类似于 Tekton，使用容器来执行管道中的每个任务（步骤）。管道引擎通过缓存先前执行的结果来优化资源消耗，防止你重新执行已经使用相同输入执行的任务。此外，你可以在你的笔记本电脑/工作站上本地运行
    Dagger 引擎，甚至远程运行，甚至在 Kubernetes 集群内部。
- en: When I compare Dagger to something like Tekton, with my developer background,
    I tend to like the flexibility of coding pipelines using a programming language
    I am familiar with. For developers to create, version and share code is easy,
    because I don’t need to learn any new tools.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当我将 Dagger 与类似 Tekton 的东西进行比较时，凭借我的开发者背景，我倾向于喜欢使用我熟悉的编程语言来编写管道的灵活性。对于开发者来说，创建、版本控制和共享代码很容易，因为我不需要学习任何新工具。
- en: Instead of looking at a Hello World example, I wanted to show how a service
    pipeline would look in Dagger. So, let’s look at how a service pipeline is defined
    using the Dagger Go SDK. The following code snippet shows a service pipeline defining
    the main goals we want to execute for each service. Take a look at the `buildService`,
    `testService`, and `publishService` functions. These functions codify what it
    means to build, test, and publish each service. These functions use the Dagger
    client to execute actions inside containers that Dagger will orchestrate, as shown
    in listing 3.11.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想看一个 Hello World 的例子，而是想展示在 Dagger 中服务管道的样子。因此，让我们看看如何使用 Dagger Go SDK 定义服务管道。以下代码片段展示了定义每个服务想要执行的主要目标的管道。看看
    `buildService`、`testService` 和 `publishService` 函数。这些函数将构建、测试和发布每个服务的含义编码化。这些函数使用
    Dagger 客户端在 Dagger 将要编排的容器内执行操作，如列表 3.11 所示。
- en: Listing 3.11 Go application defining tasks using Dagger
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.11 使用 Dagger 定义任务的 Go 应用程序
- en: '[PRE12]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can find the `service-pipeline.go` definition at [https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/service-pipeline.go](https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/service-pipeline.go).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/service-pipeline.go](https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/service-pipeline.go)
    找到 `service-pipeline.go` 的定义。
- en: By running `go run service-pipeline.go build notifications-service` Dagger will
    use containers to build our Go application source code and then build a container
    ready to be pushed to a container registry. If you look at the `buildService`
    function in listing 3.12, you will notice that it builds our service source code,
    in this case, looping over a list of target platforms (amd64 and arm64) to produce
    binaries for each of them. Once the binaries are produced, a container is created
    using the Dagger client `client.Container` function. Because we are defining each
    step programmatically, we can also define what needs to be cached for subsequent
    builds (using `client.CacheVolume`).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行 `go run service-pipeline.go build notifications-service`，Dagger 将使用容器构建我们的
    Go 应用程序源代码，然后构建一个准备推送到容器注册库的容器。如果你查看列表 3.12 中的 `buildService` 函数，你会注意到它构建我们的服务源代码，在这种情况下，遍历目标平台列表（amd64
    和 arm64）以为每个平台生成二进制文件。一旦生成了二进制文件，就使用 Dagger 客户端的 `client.Container` 函数创建一个容器。因为我们是以编程方式定义每个步骤，所以我们还可以定义后续构建需要缓存的文件（使用
    `client.CacheVolume`）。
- en: 'Listing 3.12 Tasks: Go code that uses Dagger built-in functions'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.12 任务：使用 Dagger 内置函数的 Go 代码
- en: '[PRE13]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These pipelines are written in Go and build Go applications, but nothing stops
    you from building other languages and using the necessary tools. Each task is
    just a container. Dagger and the open-source community will create all the basic
    building blocks, but each organization has to create domain-specific libraries
    to integrate with third-party or in-house/legacy systems. By focusing on enabling
    developers, Dagger lets you choose the right tool(s) to create these integrations.
    There is no need to write plugins, just code that can be distributed as any other
    library.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这些管道是用 Go 编写的，并构建 Go 应用程序，但没有任何阻止你构建其他语言并使用必要工具的限制。每个任务只是一个容器。Dagger 和开源社区将创建所有基本构建块，但每个组织都必须创建特定领域的库以与第三方或内部/遗留系统集成。通过专注于使开发者能够选择合适的工具来创建这些集成，Dagger
    让你可以选择合适的工具来创建这些集成。无需编写插件，只需可以像其他库一样分发的代码即可。
- en: Try running the pipeline for one of the services or follow the step-by-step
    tutorial that you can find at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md).
    If you run the pipeline twice, the second run will be almost instant since most
    steps are cached.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行其中一个服务的管道，或者按照你可以在 [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md)
    找到的逐步教程进行操作。如果你运行管道两次，第二次运行几乎会立即完成，因为大多数步骤都是缓存的。
- en: In contrast with Tekton, we are running the Dagger pipeline locally, not in
    a Kubernetes cluster, which has advantages. For example, we don’t need a Kubernetes
    cluster to run and test this pipeline, and we don’t need to wait for remote feedback.
    Developers can run these pipelinesby using a local container runtime (like Docker
    or Podman), including integration tests, before pushing any changes to the Git
    repository. Having fast feedback allows them to go faster.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 与Tekton相比，我们是在本地而不是在Kubernetes集群上运行Dagger管道，这有一些优势。例如，我们不需要Kubernetes集群来运行和测试这个管道，我们也不需要等待远程反馈。开发人员可以在将任何更改推送到Git存储库之前，使用本地容器运行时（如Docker或Podman）运行这些管道，包括集成测试。快速的反馈使他们能够更快地工作。
- en: 'But now, how does this translate to a remote environment? What if we want to
    run this pipeline remotely on a Kubernetes cluster? The good news is that it works
    the same: it is just a remote Dagger pipeline engine that will execute our pipelines.
    No matter where this remote pipeline engine is, running inside Kubernetes or as
    a managed service, our pipeline behavior and the caching mechanisms provided by
    the pipeline engine will behave the same way. Figure 3.12 shows how the execution
    will go if we install the Dagger pipeline engine inside Kubernetes and run the
    same pipelines.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，这如何转化为远程环境？如果我们想在Kubernetes集群上远程运行这个管道怎么办？好消息是它的工作方式相同：它只是一个远程的Dagger管道引擎，将执行我们的管道。无论这个远程管道引擎在哪里，无论是运行在Kubernetes内部还是作为一个托管服务，我们的管道行为和管道引擎提供的缓存机制都将以相同的方式运行。图3.12显示了如果我们在Kubernetes内部安装Dagger管道引擎并运行相同的管道，执行将如何进行。
- en: '![](../../OEBPS/Images/03-12.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/03-12.png)'
- en: Figure 3.12 When configured against a remote Dagger Pipeline Engine, the Dagger
    SDK will collect and send the context for the pipeline to be executed remotely.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 当配置与远程Dagger Pipeline Engine时，Dagger SDK将收集并发送管道执行的上下文。
- en: When the Dagger Pipeline Engine is installed in a remote environment such as
    a Kubernetes Cluster, virtual machine, or any other computing resource, we can
    connect and run our pipelines against it. The Dagger Go SDK takes all the context
    needed from the local environment and sends it to the Dagger Pipeline Engine to
    execute the tasks remotely. We don’t need to worry about publishing our application
    source code online for the pipeline.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当Dagger Pipeline Engine安装在远程环境，如Kubernetes集群、虚拟机或任何其他计算资源中时，我们可以连接并运行我们的管道。Dagger
    Go SDK从本地环境获取所有必要的上下文，并将其发送到Dagger Pipeline Engine以远程执行任务。我们不需要担心将应用程序源代码在线发布以供管道使用。
- en: 'Check this step-by-step tutorial on how to run your Dagger pipelines on Kubernetes:
    [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md#running-your-pipelines-remotely-on-kubernetes](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md#running-your-pipelines-remotely-on-kubernetes).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这个分步教程，了解如何在Kubernetes上运行你的Dagger管道：[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md#running-your-pipelines-remotely-on-kubernetes](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-3/dagger/README.md#running-your-pipelines-remotely-on-kubernetes)。
- en: As you can see, Dagger will use persistent storage (Cache) to cache all the
    builds and tasks to optimize performance and reduce pipeline running times. The
    operations team in charge of deploying and running Dagger inside Kubernetes will
    need to track how much storage is needed based on the pipelines the organization
    is running.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Dagger将使用持久存储（缓存）来缓存所有构建和任务以优化性能并减少管道运行时间。负责在Kubernetes内部部署和运行Dagger的操作团队需要根据组织运行的管道来跟踪所需的存储量。
- en: 'In this short section, we have seen how to use Dagger to create our service
    pipelines. We have seen that Dagger is very different from Tekton: you don’t need
    to write your pipelines using YAML, you can write your pipelines in any supported
    programming language, you can run your pipelines locally or remotely using the
    same code, and you can distribute your pipelines using the same tools that you
    are using for your applications.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何使用Dagger创建我们的服务管道。我们了解到Dagger与Tekton非常不同：你不需要使用YAML编写你的管道，你可以使用任何支持的编程语言编写你的管道，你可以使用相同的代码在本地或远程运行你的管道，并且你可以使用为你的应用程序使用的相同工具来分发你的管道。
- en: From a Kubernetes point of view, when you use a tool like Dagger, you lose the
    Kubernetes native approach of managing your pipelines as you manage your other
    Kubernetes resources. I see the Dagger community expanding in that direction if
    they get enough feedback and requests for that.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Kubernetes 的角度来看，当你使用像 Dagger 这样的工具时，你会失去像管理其他 Kubernetes 资源一样管理你的流水线的 Kubernetes
    原生方法。如果他们得到足够的反馈和请求，我认为 Dagger 社区会向那个方向发展。
- en: From a platform engineering perspective, you can create and distribute complex
    pipelines (and tasks) for your teams to use and extend using tools they already
    know. These pipelines will run the same way no matter where they are executed,
    making it an extremely flexible solution. Platform teams can take this flexibility
    to decide where to run these pipelines more efficiently (based on costs and resources)
    without complicating developers’ lives, as they will always be able to run their
    pipelines locally for development purposes.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 从平台工程的角度来看，你可以创建和分发复杂的流水线（和任务）供你的团队使用和扩展，他们可以使用他们已经知道的工具。这些流水线无论在哪里执行都会以相同的方式运行，这使得它成为一个极其灵活的解决方案。平台团队可以利用这种灵活性来决定在哪里更有效地运行这些流水线（基于成本和资源），而不会复杂化开发者的生活，因为他们将始终能够在开发目的上本地运行他们的流水线。
- en: 3.5.5 Should I use Tekton, Dagger, or GitHub Actions?
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.5 我应该使用 Tekton、Dagger 还是 GitHub Actions？
- en: As you have seen, Tekton and Dagger provide us with the basic building blocks
    to construct unopinionated pipelines. In other words, we can use Tekton and Dagger
    to build service pipelines and almost every imaginable pipeline. With Tekton,
    we use the Kubernetes resource-based approach, scalability, and self-healing features.
    Using Kubernetes-native resources can be very helpful in integrating Tekton with
    other Kubernetes tools, such as managing and monitoring Kubernetes resources.
    Using the Kubernetes resource model, you can treat your Tekton pipelines and PipelineRuns
    as any other Kubernetes resource and reuse all the existing tooling.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，Tekton 和 Dagger 为我们提供了构建无特定观点流水线的基本构建块。换句话说，我们可以使用 Tekton 和 Dagger 来构建服务流水线和几乎所有可想象的流水线。使用
    Tekton，我们使用基于 Kubernetes 资源的方法、可扩展性和自我修复功能。使用 Kubernetes 原生资源可以帮助将 Tekton 与其他
    Kubernetes 工具（如管理和监控 Kubernetes 资源）集成。使用 Kubernetes 资源模型，你可以将你的 Tekton 流水线和 PipelineRuns
    视为任何其他 Kubernetes 资源，并重用所有现有的工具。
- en: With Dagger, we can define our pipelines using well-known programming languages
    and tools and run these pipelines everywhere (locally in our workstations in the
    same way as if we were running them remotely). This makes Tekton and Dagger perfect
    tools that platform builders can use to build more opinionated pipelines that
    development teams can use.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Dagger，我们可以使用众所周知的编程语言和工具来定义我们的流水线，并在任何地方运行这些流水线（在本地工作站上运行的方式与远程运行相同）。这使得
    Tekton 和 Dagger 成为平台构建者可以用来构建更多具有特定观点的流水线的完美工具，这些流水线可以被开发团队使用。
- en: On the other hand, you can use a managed service such as GitHub Actions. You
    can look at how the service pipelines are configured using GitHub actions for
    all the projects mentioned here. For example, you can check the service pipeline
    for the notifications service at [https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你可以使用像 GitHub Actions 这样的托管服务。你可以查看如何使用 GitHub Actions 配置这里提到的所有项目的服务流水线。例如，你可以检查通知服务的服务流水线，[https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml)。
- en: This GitHub Action pipeline uses `ko-build` to build the service and then pushes
    the new container image to Docker Hub. Notice that this pipeline doesn’t run any
    tests, and it uses a custom step ([https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml#L17](https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml#L17))
    to check if the code for the service was changed; only run the build and push
    to Docker Hub if there were changes to the service source code.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 GitHub Action 流水线使用 `ko-build` 构建服务，然后将新的容器镜像推送到 Docker Hub。请注意，这个流水线没有运行任何测试，它使用一个自定义步骤（[https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml#L17](https://github.com/salaboy/platforms-on-k8s/blob/main/.github/workflows/notifications-service-service-pipelines.yaml#L17)）来检查服务的代码是否已更改；只有当服务源代码有更改时，才运行构建并将镜像推送到
    Docker Hub。
- en: The advantage of using GitHub Actions is that you don’t need to maintain the
    infrastructure running them or pay for the machines that run these pipelines (if
    your volume is small enough). But if you are running loads of pipelines and these
    pipelines are data-intensive, GitHub Actions will be costly.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GitHub Actions 的优势是，你不需要维护运行它们的底层基础设施，或者为运行这些管道的机器付费（如果你的量足够小）。但是，如果你正在运行大量的管道，并且这些管道是数据密集型的，GitHub
    Actions 将会变得昂贵。
- en: For cost-related reasons or because you cannot run your pipelines in the cloud
    due to industry regulations, Tekton and Dagger shine in providing you with all
    the building blocks to compose and run complex pipelines. While Dagger is already
    focused on cost and runtime optimization, this is coming for Tekton and other
    pipeline engines.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于成本相关的原因，或者由于行业法规限制你无法在云中运行管道，Tekton 和 Dagger 在为你提供构建和运行复杂管道的所有构建块方面表现出色。虽然
    Dagger 已经专注于成本和运行时优化，但 Tekton 和其他管道引擎也将实现这一点。
- en: It is important to note that you can integrate Tekton and Dagger with GitHub.
    For example, use Tekton Triggers ([https://github.com/tektoncd/triggers/blob/main/docs/getting-started/README.md](https://github.com/tektoncd/triggers/blob/main/docs/getting-started/README.md))
    to react to commits into a GitHub repository. You can also run Dagger inside a
    GitHub Action, enabling developers to run the same pipeline locally executed in
    GitHub Actions, which cannot be done easily out of the box.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，你可以将 Tekton 和 Dagger 与 GitHub 集成。例如，使用 Tekton Triggers ([https://github.com/tektoncd/triggers/blob/main/docs/getting-started/README.md](https://github.com/tektoncd/triggers/blob/main/docs/getting-started/README.md))
    来响应 GitHub 仓库中的提交。你还可以在 GitHub Action 中运行 Dagger，使开发者能够运行在 GitHub Actions 中本地执行的相同管道，而这通常不容易实现。
- en: Now that we have our artifacts and configurations ready to be deployed to multiple
    environments, let’s look at what is commonly known as the GitOps approach for
    continuous deployment through environment pipelines.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了要部署到多个环境中的工件和配置，让我们看看通常被称为通过环境管道进行持续部署的 GitOps 方法。
- en: 3.6 Linking back to platform engineering
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 回到平台工程
- en: As part of your platform initiatives, you will need to help teams build their
    services in an automated way. Most of the time, a decision must be made to standardize
    how the services will be built and packaged across teams. If the platform team
    can provide a solution that is accessible to teams to try out locally or have
    the right environments to test before pushing changes to a Git repository, this
    will increase the velocity and feedback loop that these teams need to move with
    confidence. A separate setup might be needed to validate pull requests and alert
    teams if their repositories’ main branch is unreleasable.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 作为你的平台倡议的一部分，你需要帮助团队以自动化的方式构建他们的服务。大多数时候，必须做出决定，以标准化跨团队构建和打包服务的方式。如果平台团队能够提供一个团队可以尝试本地使用或测试的解决方案，并在将更改推送到
    Git 仓库之前拥有正确的环境，这将提高这些团队所需的移动速度和反馈循环，从而增强他们的信心。可能需要单独的设置来验证拉取请求，并在主分支不可发布时提醒团队。
- en: While GitHub Actions (and other managed services) are a popular solution, platform
    engineering teams might choose different tools or services based on their budgets
    and other platform-wide decisions (such as aligning with the Kubernetes APIs).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GitHub Actions（以及其他托管服务）是一个流行的解决方案，但平台工程团队可能会根据他们的预算和其他平台级决策（例如与 Kubernetes
    API 保持一致）选择不同的工具或服务。
- en: I’ve made conscious choices for this book’s demos and step-by-step tutorials
    ([https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3))
    that might differ greatly from your projects. First, because the complexity of
    the projects presented in this book is quite low, but also because to keep the
    resources organized and versioned to support future revisions, all the application
    service’s source code is kept under a simple directory structure. This decision
    to have all the service’s source code together in the same repository influences
    the shape of our service pipelines.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我为这本书的演示和分步教程（[https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-3)）做了有意识的选择，这些选择可能与你的项目大相径庭。首先，因为本书中展示的项目复杂度相当低，而且为了保持资源组织化和版本控制以支持未来的修订，所有应用程序服务的源代码都被保存在一个简单的目录结构下。将所有服务的源代码集中存储在同一存储库中的这一决定影响了我们的服务管道的形状。
- en: The service pipelines provided (both using Tekton and Dagger) receive as a parameter
    the directory of the repository that the user wants to build. If you set up webhooks
    to trigger pipelines on pull requests, you must filter where the changes are to
    see which service pipeline to run. This adds to the complexity of the entire setup.
    As suggested in previous sections, an alternative approach is to have one repository
    per service. This enables you to have custom service pipeline definitions per
    service (which can reuse generic tasks) and simple webhook definitions, as you
    know exactly what to run when changes are made. The main problem with having one
    repository per service is dealing with users and access, because adding new services
    will force you to create new repositories and ensure that developers have access
    to it.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的服务管道（无论是使用 Tekton 还是 Dagger）都将用户想要构建的存储库目录作为参数。如果你设置了触发管道的 webhooks，你必须过滤更改的位置，以确定要运行哪个服务管道。这增加了整个设置的复杂性。如前几节所建议，一种替代方法是每个服务有一个存储库。这使你能够为每个服务拥有定制的服务管道定义（可以重用通用任务）和简单的
    webhook 定义，因为你确切知道在更改时运行什么。拥有每个服务一个存储库的主要问题是处理用户和访问权限，因为添加新服务将迫使你创建新的存储库并确保开发者可以访问它。
- en: Another big decision the platform team will need to make concerning service
    pipelines is where they start and end. For the examples provided here, the service
    pipelines start when a change is submitted and end after publishing the container
    images for each service. Service Pipelines for the walking skeleton services don’t
    package and publish individual services Helm Charts. Figure 3.13 shows the responsibility
    of the service pipelines defined by the examples.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 平台团队还需要做出的另一个重大决策是关于服务管道的起点和终点。在本例中提供的示例中，服务管道从提交更改开始，在为每个服务发布容器镜像后结束。对于行走骨架服务的服务管道不会打包和发布单个服务的
    Helm 图表。图 3.13 显示了由示例定义的服务管道的责任。
- en: '![](../../OEBPS/Images/03-13.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13](../../OEBPS/Images/03-13.png)'
- en: Figure 3.13 The service pipelines and the application pipeline have different
    lifecycles.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 服务管道和应用管道有不同的生命周期。
- en: 'You need to ask yourself if having Helm Charts per service is a good idea or
    an overkill. You should have a clear understanding of who will consume these artifacts.
    Try answering questions to find a strategy that will work for your teams:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要问自己，为每个服务创建 Helm 图表是一个好主意还是过度设计。你应该清楚地了解谁将消费这些工件。尝试回答以下问题，以找到适合你团队的战略：
- en: Will you deploy your services individually, or will they always be deployed
    as a set?
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将单独部署服务，还是它们总是作为一个集合部署？
- en: How often does your services change? Do you have services that change more often?
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的服务多久会变化一次？你是否有一些变化更频繁的服务？
- en: How many teams are going to deploy these services?
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将有多少团队部署这些服务？
- en: Are you creating an artifact that an open-source community will consume with
    many users deploying the services individually?
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否在创建一个开源社区将消费的工件，许多用户将单独部署服务？
- en: For the examples provided for this chapter, a separate application-level pipeline
    is provided to package and publish the Conference application Helm Chart.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章提供的示例，提供了一个单独的应用级管道来打包和发布会议应用的 Helm 图表。
- en: 'The reason behind this decision was simple: every reader will install the application
    in a cluster, and I needed a simple way to enable that. If readers don’t want
    to use Helm to install the application in their clusters, they can export the
    output of running the `helm template` command and apply the output using `kubectl`.
    Another important factor behind that decision is the lifecycle of the Helm Chart
    and the application’s services. The shape of the application doesn’t change much.
    The Helm Chart definition might only change if we need to add or remove a service.
    The service’s code, however, changes a lot, and we want to enable the teams working
    on these services to keep adding changes to them.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策背后的原因很简单：每个读者都会在集群中安装应用程序，我需要一个简单的方法来实现这一点。如果读者不想在他们的集群中使用 Helm 安装应用程序，他们可以导出运行
    `helm template` 命令的输出，并使用 `kubectl` 应用该输出。那个决策背后的另一个重要因素是 Helm 图表和应用程序服务的生命周期。应用程序的形状变化不大。Helm
    图表定义可能只会改变，如果我们需要添加或删除服务。然而，服务的代码变化很大，我们希望让在这些服务上工作的团队能够继续向它们添加更改。
- en: Figure 3.14 shows two complementary approaches for service pipelines. The services
    running in the developer’s environment provide fast feedback loops, and those
    running remotely produce artifacts that teams will use to deploy the same application
    across different environments.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14显示了服务管道的两种互补方法。在开发人员环境中运行的服务提供快速的反馈循环，而在远程运行的服务生成团队将用于在不同环境中部署相同应用程序的工件。
- en: '![](../../OEBPS/Images/03-14.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/03-14.png)'
- en: Figure 3.14 Local vs. remote service pipelines
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 本地与远程服务管道
- en: Finally, none of the examples in this book provide configurations to tap into
    webhooks from the Git repositories besides those linked using GitHub actions.
    Pushing readers to get the right tokens and configuring this with multiple Git
    providers is not complex, but it would take me many pages to explain. Teams consuming
    these mechanisms wouldn’t need to worry about dealing with the credentials needed
    for your service pipelines. As a platform team, automating access to credentials
    for development (and other) teams to just connect to services is fundamental to
    speed up their workflows.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本书中的所有示例都没有提供配置来从Git仓库中访问webhooks，除了使用GitHub Actions链接的那些。推动读者获取正确的令牌并使用多个Git提供商配置这并不复杂，但解释它将占用我许多页面。消费这些机制的团队不需要担心处理服务管道所需的凭证。作为一个平台团队，自动化开发（和其他）团队访问凭证以连接到服务是加快他们工作流程的基本方法。
- en: Summary
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Service pipelines define how to go from source code to artifacts that can be
    deployed in multiple environments. Following trunk-based development and one service
    = one repository practices helps your teams standardize building and releasing
    software artifacts more efficiently.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务管道定义了如何从源代码到可以在多个环境中部署的工件的过程。遵循基于主干的开发和“一个服务 = 一个仓库”的实践有助于您的团队更高效地标准化构建和发布软件工件。
- en: You need to find what works for your teams and applications. There is no one-size-fits-all
    solution, and compromises must be made. How often do your application’s services
    change, and how do you deploy them into environments? Answering these questions
    can help you to define where your service pipelines start and end.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要找到适合您团队和应用程序的方法。没有一种适合所有情况的解决方案，必须做出妥协。您的应用程序的服务多久会发生变化，您又是如何将它们部署到环境中的？回答这些问题可以帮助您定义服务管道的起点和终点。
- en: Tekton is a pipeline engine designed for Kubernetes. You can use Tekton to design
    your custom pipelines and use all the shared tasks and pipelines openly available
    in the Tekton catalog. You can now install Tekton in your cluster and start creating
    pipelines.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tekton是一个为Kubernetes设计的管道引擎。您可以使用Tekton设计您自己的管道，并使用在Tekton目录中公开可用的所有共享任务和管道。您现在可以在您的集群中安装Tekton并开始创建管道。
- en: Dagger allows you to write and distribute pipelines using your favorite programming
    language. These pipelines can be executed in any environment, including your developer’s
    laptops.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dagger允许您使用您喜欢的编程语言编写和分发管道。这些管道可以在任何环境中执行，包括您的开发人员的笔记本电脑。
- en: Tools like GitHub Actions are very useful but can be expensive. Platform builders
    must look for tools that provide enough flexibility to build and distribute tasks
    that other teams can reuse and follow company guidelines. Enabling teams to run
    their pipelines locally is a big plus as it will improve their developer experience
    and their feedback times.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像GitHub Actions这样的工具非常有用，但可能很昂贵。平台构建者必须寻找提供足够灵活性的工具，以构建和分发其他团队可以重用并遵循公司指南的任务。允许团队在本地运行他们的管道是一个很大的加分项，因为它将改善他们的开发体验并缩短他们的反馈时间。
- en: If you followed the step-by-step tutorials, you gained hands-on experience in
    using Tekton and Dagger to create and run your service pipelines.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您遵循了逐步教程，您就获得了使用Tekton和Dagger创建和运行服务管道的实践经验。
