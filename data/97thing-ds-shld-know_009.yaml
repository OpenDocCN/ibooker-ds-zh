- en: Chapter 8\. Be Careful with “Decisions of the Heart”
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第八章。小心“心之决策”
- en: Hugh Watson
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 休·沃森
- en: '![](Images/Hugh_Watson.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Hugh_Watson.png)'
- en: Professor of MIS, Terry College of Business, University of Georgia
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治亚大学特里商学院MIS教授
- en: Today, companies and government organizations are increasingly using advanced
    analytics like deep learning to partially or fully automate decision making. Analytics
    is employed to make lending decisions, recommend probation or prison sentencing,
    screen job applicants, and more. While these algorithms can result in faster,
    cheaper, more efficient, and even fairer decision making, they are not without
    risk. Cathy O’Neil, in her influential book [*Weapons of Math Destruction*](https://weaponsofmathdestructionbook.com)
    (Crown), and others argue that algorithms can increase inequality, deny services
    and opportunities, and even threaten democracy.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，公司和政府组织越来越多地使用深度学习等先进分析技术来部分或完全自动化决策制定。分析技术被用来进行贷款决策、建议缓刑或监狱判决、筛选职位申请者等。虽然这些算法可以加快、降低成本、提高效率，甚至使决策更公平，但也不是没有风险。凯西·奥尼尔在她有影响力的著作《*数学毁灭之武器*》（Crown）中以及其他人的论述中指出，算法可能增加不平等，拒绝服务和机会，甚至威胁到民主。
- en: 'The conversation between the Tin Woodman and the Scarecrow in *The Wonderful
    Wizard of Oz* by L. Frank Baum provides an interesting perspective on the need
    for including “heart” as well as brains (i.e., algorithms) when automating decisions:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在L·弗兰克·鲍姆的《绿野仙踪》中，锡人和稻草人之间的对话提供了一个有趣的视角，说明了在自动化决策时需要包括“心”和“脑”（即算法）：
- en: “*I don’t know enough,*” replied the Scarecrow cheerfully. “*My head is stuffed
    with straw, you know, and that is why I am going to Oz to ask him for some brains.*”
    “*Oh, I see,*” said the Tin Woodman. “*But, after all, brains are not the best
    things in the world.*” “*Have you any?*” inquired the Scarecrow. “*No, my head
    is quite empty,*” answered the Woodman, “*but once I had brains, and a heart also;
    so, having tried them both, I should much rather have a heart.*”
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “*我不知道足够多，*”稻草人愉快地回答道。“*你知道，我的头是塞满稻草的，这就是我去奥兹要求他给我一些大脑的原因。*”“*哦，我明白了，*”锡人说。“*但是，毕竟，大脑并不是世界上最好的东西。*”“*你有吗？*”稻草人问。“*没有，我的头是完全空的，*”锡人回答道，“*但我曾经有过大脑，还有一颗心；所以，既然两者都试过了，我更愿意拥有一颗心。*”
- en: When decisions can significantly affect people’s lives, the decision-making
    process should include “heart” as well as brains. The applications that are developed
    should be free of bias and not unfairly discriminate against classes of people;
    comply with the increasingly complex set of laws and regulations; not damage a
    company’s brand; and allow individuals to opt out and/or obtain an explanation
    of why a decision was made and seek remediation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当决策可能严重影响到人们生活时，决策过程应该包括“心”和“脑”。开发的应用程序应当没有偏见，不会不公平地歧视某些人群；应当遵守日益复杂的法律法规；不会损害公司品牌；并且允许个人选择退出和/或获取决策原因的解释，并寻求补救措施。
- en: When building models, be careful not to introduce unintentional errors and biases,
    as can happen with a poor selection of model training/testing data. For example,
    a prescreening bias can occur when using data that has biases that were baked
    into previous processes (e.g., a college admissions model that uses only the data
    of students who were admitted in the past). You must know how to handle categorical
    data that has only a small percentage of observations in a category of importance.
    Also, you should be sure to constantly monitor a model’s performance for accuracy
    and how it affects different classes of people.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，要注意不要引入意外的错误和偏见，这可能是由于模型训练/测试数据选择不当造成的。例如，在使用过去只录取学生数据的学院入学模型时可能会出现预筛选偏见。您必须知道如何处理在重要类别中只有少量观察值的分类数据。此外，您还应确保持续监控模型的准确性及其对不同人群的影响。
- en: The EU’s GDPR, which took effect in May 2018, and the California Consumer Privacy
    Act (CCPA) of January 2020 both place limitations on how personal data can be
    used and shared. GDPR requires opt-in authorization to collect any personal data;
    any requests to use personal data must be specific and unambiguous; the collection
    and use of personal data must be for a specific, well-understood business purpose;
    and citizens have the right to have their personal data deleted (the so-called
    right to be forgotten). Article 22 of GDPR states that “[individuals] shall have
    the right not to be subject to a decision based solely on automated processing.”
    CCPA has similarities to GDPR, but with a focus on consumer privacy rights and
    company-required disclosures to customers. For example, companies must have a
    link on their websites titled “Do Not Sell My Personal Information.”
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟的GDPR于2018年5月生效，而加利福尼亚州消费者隐私法（CCPA）于2020年1月生效，两者都对个人数据的使用和共享设置了限制。GDPR要求必须通过选择授权来收集任何个人数据；任何使用个人数据的请求必须具体明确；个人数据的收集和使用必须是为了特定和清晰理解的业务目的；并且公民有权利让他们的个人数据被删除（所谓的被遗忘权）。GDPR的第22条规定：“[个人]应有权利不受仅基于自动处理的决定的影响。”CCPA与GDPR有相似之处，但侧重于消费者隐私权和公司向客户披露所需信息。例如，公司必须在其网站上有一个标题为“不出售我的个人信息”的链接。
- en: Some uses of algorithms are legal but are bad for business. An oft-cited example
    is Target using predictive modeling to identify women who are likely pregnant
    and then sending them pregnancy-related coupons. The blowback occurred when a
    16-year-old girl received such coupons and her father complained that the coupons
    promoted teenage pregnancy (later he learned that she was indeed pregnant). The
    story was told in the *New York Times*, *Fortune*, and other widely read publications
    and tarnished Target’s brand.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 某些算法的使用虽然合法，但对商业有害。一个经常被引用的例子是 Target 使用预测建模来识别可能怀孕的女性，然后发送与怀孕相关的优惠券。当一名16岁的女孩收到这样的优惠券并且她的父亲抱怨这些优惠券促进了未成年怀孕时，问题就出现了（后来他发现她确实怀孕了）。这个故事被《纽约时报》、《财富》和其他广为人知的出版物报道，并且给
    Target 的品牌带来了污点。
- en: People should be able to ask for and receive an explanation of why a decision
    was made. The US Public Policy Council of the Association for Computing Machinery
    (ACM) and the ACM Europe Policy Committee, working both separately and together,
    codified seven principles for ensuring that personal data and algorithms are used
    fairly. The fourth guiding principle is the need for explanation—the ability to
    communicate, when asked, an algorithm’s logic in human terms. Individuals should
    also be able to challenge an automated decision and/or learn what can be done
    to remediate it. This requirement can be challenging because of the “black-box”
    nature of some of the most powerful predictive models (e.g., deep learning) and
    can lead to the use of models that have slightly less predictive power but are
    more explainable in human terms (e.g., decision trees).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人们应该能够要求并收到关于为什么做出决定的解释。美国计算机协会（ACM）的公共政策委员会和ACM欧洲政策委员会分别和共同工作，制定了确保个人数据和算法公平使用的七项原则。第四项指导原则是解释的需要——即当被问及时，能够用人类术语传达算法的逻辑。个人还应该能够质疑自动化决策和/或了解可以采取什么措施加以补救。由于一些最强大的预测模型（例如深度学习）的“黑箱”特性，这一要求可能具有挑战性，并可能导致使用在人类术语中更可解释的、预测力略低的模型（例如决策树）。
- en: To meet the demand for the legal and ethical use of algorithms, especially those
    that involve decisions of the heart, data scientists need to adopt a broader perspective
    on what their responsibilities are, and companies need to expand their governance
    (e.g., people, committees, and processes) to include their legal staff and businesspeople
    who are in touch with the customer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足对算法合法和道德使用的需求，特别是涉及到涉及心理决策的算法，数据科学家需要采用更广泛的视角来审视他们的责任，并且公司需要扩展他们的治理（例如人员、委员会和流程），包括他们的法律人员和与客户接触的业务人员。
