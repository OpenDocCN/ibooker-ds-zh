- en: Chapter 10\. Exploring advanced methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. 探索高级方法
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Decision tree–based models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于决策树的模型
- en: Generalized additive models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义加性模型
- en: Support vector machines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: 'In [chapter 7](../Text/07.xhtml#ch07), you learned about linear methods for
    fitting predictive models. These models are the bread-and-butter methods of machine
    learning; they are easy to fit; they are small, portable, and efficient; they
    sometimes provide useful advice; and they can work well in a wide variety of situations.
    However, they also make strong assumptions about the world: namely, that the outcome
    is linearly related to all the inputs, and all the inputs contribute additively
    to the outcome. In this chapter, you will learn about methods that relax these
    assumptions.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](../Text/07.xhtml#ch07)中，你学习了拟合预测模型的线性方法。这些模型是机器学习的核心方法；它们易于拟合；它们小巧、便携、高效；它们有时会提供有用的建议；并且它们可以在各种情况下表现良好。然而，它们也对世界做出了强烈的假设：即结果与所有输入线性相关，并且所有输入都以加性方式对结果产生影响。在本章中，你将了解一些放宽这些假设的方法。
- en: '[Figure 10.1](../Text/10.xhtml#ch10fig01) represents our mental model for what
    we''ll do in this chapter: use R to master the science of building supervised
    machine learning models.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.1](../Text/10.xhtml#ch10fig01)代表了我们本章将要执行的心理模型：使用R掌握构建监督机器学习模型的科学。'
- en: Figure 10.1\. Mental model
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1\. 心理模型
- en: '![](Images/10fig01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/10fig01.jpg)'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to study the relationship between mortality rates and measures
    of a person’s health or fitness, including BMI (body mass index).*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你想研究死亡率与一个人的健康或体能指标之间的关系，包括BMI（身体质量指数）。*'
- en: '* * *'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Figure 10.2](../Text/10.xhtml#ch10fig02) shows the relationship between BMI
    and mortality hazard ratio for a population of older Thais over a four-year period.^([[1](../Text/10.xhtml#ch10fn1)])
    It shows that both high and low BMI are associated with higher mortality rates:
    the relationship between BMI and mortality is not linear. So a straightforward
    linear model to predict mortality rates based (partly) on BMI may not perform
    well.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.2](../Text/10.xhtml#ch10fig02)显示了在四年期间，泰国老年人群体的BMI与死亡率风险比之间的关系.^([[1](../Text/10.xhtml#ch10fn1)])
    它表明，无论是高BMI还是低BMI都与较高的死亡率相关：BMI与死亡率之间的关系不是线性的。因此，基于BMI（部分）预测死亡率的简单线性模型可能表现不佳。'
- en: ¹
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Data from Vapattanawong, et.al. “Obesity and mortality among older Thais: a
    four year follow up study,” *BMC Public Health*, 2010\. [https://doi.org/10.1186/1471-2458-10-604](https://doi.org/10.1186/1471-2458-10-604).'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Vapattanawong等人。“泰国老年人的肥胖与死亡率：四年随访研究”，*BMC公共健康*，2010。[https://doi.org/10.1186/1471-2458-10-604](https://doi.org/10.1186/1471-2458-10-604)。
- en: Figure 10.2\. Mortality rates of men and women as a function of body mass index
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2\. 男女死亡率与身体质量指数的关系
- en: '![](Images/10fig02_alt.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/10fig02_alt.jpg)'
- en: In addition, there may be *interactions* between BMI and other factors, like
    how active a person is. For example, for people who are highly active, BMI may
    affect mortality rates much less than for people who are sedentary. Some interactions,
    such as “if-then” relationships among variables, or multiplicative effects between
    variables, may not always be expressible in linear models.^([[2](../Text/10.xhtml#ch10fn2)])
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，BMI（身体质量指数）与其他因素之间可能存在*相互作用*，例如一个人的活跃程度。例如，对于非常活跃的人来说，BMI对死亡率的影响可能远小于对久坐不动的人的影响。一些相互作用，如变量之间的“如果-那么”关系或变量之间的乘法效应，可能并不总是可以用线性模型来表示.^([[2](../Text/10.xhtml#ch10fn2)])
- en: ²
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One can model interactions in linear models, but it must be done explicitly
    by the data scientist. Instead, we’ll focus on machine learning techniques, such
    as tree-based methods, that can learn at least certain types of interactions directly.
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在线性模型中可以建模相互作用，但这必须由数据科学家明确执行。相反，我们将专注于机器学习技术，例如基于树的方法，这些方法可以直接学习至少某些类型的相互作用。
- en: The machine learning techniques in this chapter use a variety of methods to
    address non-linearity, interactions, and other issues in modeling.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的机器学习技术使用各种方法来解决建模中的非线性、相互作用和其他问题。
- en: 10.1\. Tree-based methods
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1\. 基于树的方法
- en: 'You saw an example of a basic decision tree model in [chapter 1](../Text/01.xhtml#ch01)
    (reproduced in [figure 10.3](../Text/10.xhtml#ch10fig03)). Decision trees are
    useful for both classification and regression, and they are an attractive method
    for a number of reasons:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你在[第1章](../Text/01.xhtml#ch01)中看到了一个基本的决策树模型的例子（在[图10.3](../Text/10.xhtml#ch10fig03)中重现）。决策树对分类和回归都很有用，并且由于以下原因，它们是一种吸引人的方法：
- en: They take any type of datums, numerical or categorical, without any distributional
    assumptions and without preprocessing.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以接受任何类型的数据，无论是数值型还是分类型，而不需要任何分布假设和预处理。
- en: Most implementations (in particular, R) handle missing data; the method is also
    robust to redundant and non-linear data.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数实现（特别是R）处理缺失数据；该方法对冗余和非线性数据也具有鲁棒性。
- en: The algorithm is easy to use, and the output (the tree) is relatively easy to
    understand.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法易于使用，输出（树）相对容易理解。
- en: 'They naturally express certain kinds of interactions among the input variables:
    those of the form “IF x is true AND y is true, THEN....”'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们自然地表达了输入变量之间的一些交互：形式为“如果x为真且y为真，那么……”
- en: Once the model is fit, scoring is fast.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型拟合完成，评分就很快。
- en: Figure 10.3\. Example decision tree (from [chapter 1](../Text/01.xhtml#ch01))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3。示例决策树（来自[第1章](../Text/01.xhtml#ch01))
- en: '![](Images/10fig03_alt.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig03_alt.jpg)'
- en: 'On the other hand, decision trees do have some drawbacks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，决策树也有一些缺点：
- en: They have a tendency to overfit, especially without pruning.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们有过度拟合的倾向，尤其是在没有剪枝的情况下。
- en: 'They have high training variance: samples drawn from the same population can
    produce trees with different structures and different prediction accuracy.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有很高的训练方差：从同一总体中抽取的样本可以产生具有不同结构和不同预测准确性的树。
- en: Simple decision trees are not as reliable as the other tree-based ensemble methods
    we'll discuss in this chapter.^([[3](../Text/10.xhtml#ch10fn3)])
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单决策树不如本章我们将讨论的其他基于树的集成方法可靠。（^[[3](../Text/10.xhtml#ch10fn3)])
- en: ³
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Lim, Loh, and Shih, “A Comparison of Prediction Accuracy, Complexity, and
    Training Time of Thirtythree Old and New Classification Algorithms,” *Machine
    Learning*, 2000\. 40, 203–229; online at [http://mng.bz/qX06](http://mng.bz/qX06).
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Lim、Loh和Shih，“三十三种旧分类算法与新分类算法的预测准确性、复杂性和训练时间比较”，*机器学习*，2000。40，203–229；在线[http://mng.bz/qX06](http://mng.bz/qX06)。
- en: For these reasons, we don’t emphasize the use of basic decision trees in this
    book. However, there are a number of techniques to fix these weaknesses that lead
    to state-of-the-art, useful, and performant modeling algorithms. We’ll discuss
    some of these techniques in this section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，我们在这本书中不强调基本决策树的使用。然而，有许多技术可以修复这些弱点，从而产生最先进、有用且性能良好的建模算法。我们将在本节中讨论一些这些技术。
- en: 10.1.1\. A basic decision tree
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1。一个基本的决策树
- en: To motivate the discussion of tree-based methods, we’ll return to an example
    that we used in [chapter 6](../Text/06.xhtml#ch06) and build a basic decision
    tree.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激发对基于树的讨论，我们将回到第6章中使用的示例，并构建一个基本的决策树。
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to classify email into spam (email you do not want) and non-spam
    (email you want).*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你想要将电子邮件分类为垃圾邮件（你不想收到的邮件）和非垃圾邮件（你想要的邮件）。*'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'For this example, you’ll again use the Spambase dataset. The dataset consists
    of about 4,600 documents and 57 features that describe the frequency of certain
    keywords and characters. Here''s the process:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，你将再次使用Spambase数据集。该数据集包含大约4,600个文档和57个特征，这些特征描述了某些关键词和字符的频率。以下是过程：
- en: First, you’ll train a decision tree to estimate the probability that a given
    document is spam.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，你将训练一个决策树来估计给定文档是垃圾邮件的概率。
- en: Next, you’ll evaluate the tree’s performance according to several performance
    measures, including accuracy, F1, and deviance (all discussed in [chapter 7](../Text/07.xhtml#ch07)).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，你将根据几个性能指标评估树的表现，包括准确率、F1和偏差（所有这些都在[第7章](../Text/07.xhtml#ch07)中讨论过）。
- en: Recall from discussions in [chapters 6](../Text/06.xhtml#ch06) and [7](../Text/07.xhtml#ch07)
    that we want accuracy and F1 to be high, and deviance (which is similar to variance)
    to be low.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下第6章和第7章的讨论，我们希望准确性和F1值要高，而偏差（与方差类似）要低。
- en: First, let’s load the data. As you did in [section 6.2](../Text/06.xhtml#ch06lev1sec2),
    download a copy of spamD.tsv from [https://github.com/WinVector/PDSwR2/raw/master/Spambase/spamD.tsv](https://github.com/WinVector/PDSwR2/raw/master/Spambase/spamD.tsv).
    Then, write a few convenience functions and train a decision tree, as in the following
    listing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载数据。就像你在[第6.2节](../Text/06.xhtml#ch06lev1sec2)中所做的那样，从[https://github.com/WinVector/PDSwR2/raw/master/Spambase/spamD.tsv](https://github.com/WinVector/PDSwR2/raw/master/Spambase/spamD.tsv)下载一份spamD.tsv的副本。然后，编写一些便利函数并训练一个决策树，如下所示。
- en: Listing 10.1\. Preparing Spambase data and evaluating a decision tree model
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1。准备Spambase数据和评估决策树模型
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Loads the data and splits into training (90% of data) and test (10% of data)
    sets
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据并将其分为训练集（90%的数据）和测试集（10%的数据）
- en: ❷ Uses all the features and does binary classification, where TRUE corresponds
    to spam documents
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用所有特征并执行二元分类，其中TRUE对应于垃圾邮件文档
- en: ❸ A function to calculate log likelihood (for calculating deviance)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一个用于计算对数似然（用于计算偏差）的函数
- en: '❹ A function to calculate and return various measures on the model: normalized
    deviance, prediction accuracy, and f1'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 一个用于计算并返回模型上各种度量（归一化偏差、预测准确性和f1）的函数
- en: ❺ Normalizes the deviance by the number of data points so we can compare the
    deviance across training and test sets
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过数据点的数量归一化偏差，以便我们可以比较训练集和测试集之间的偏差
- en: ❻ Converts the class probability estimator into a classifier by labeling documents
    that score greater than 0.5 as spam
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将类概率估计器转换为分类器，通过将评分大于0.5的文档标记为垃圾邮件
- en: ❻ Loads the rpart library and fits a decision tree model
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 加载rpart库并拟合决策树模型
- en: ❽ For plotting the tree
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 用于绘制树
- en: ❾ probabilities of the class “spam”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ “垃圾邮件”类的概率
- en: ❿ Evaluates the decision tree model against the training and test sets
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 评估决策树模型与训练集和测试集
- en: 'The resulting decision tree model is shown in [figure 10.4](../Text/10.xhtml#ch10fig04).
    The output of the two calls to `accuracyMeasures()` looks like the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的决策树模型显示在[图10.4](../Text/10.xhtml#ch10fig04)中。两次调用`accuracyMeasures()`的输出如下：
- en: Figure 10.4\. Decision tree model for spam filtering
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4\. 用于垃圾邮件过滤的决策树模型
- en: '![](Images/10fig04_alt.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig04_alt.jpg)'
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ A package to make nicely formatted ASCII tables
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个用于制作格式优美的ASCII表格的包
- en: ❷ Sets some options globally so we don't have to keep setting them in every
    call
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在全局设置一些选项，这样我们就不必在每次调用中都设置它们
- en: As expected, the accuracy and F1 scores both degrade on the test set, and the
    deviance increases.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，准确性和F1分数在测试集上都降低了，偏差增加了。
- en: 10.1.2\. Using bagging to improve prediction
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2\. 使用袋装提高预测
- en: One way to mitigate the shortcomings of decision tree models is by bootstrap
    aggregation, or *bagging*. In bagging, you draw bootstrap samples (random samples
    with replacement) from your data. From each sample, you build a decision tree
    model. The final model is the average of all the individual decision trees. This
    is shown in [figure 10.5](../Text/10.xhtml#ch10fig05).^([[4](../Text/10.xhtml#ch10fn4)])
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自助聚合，或称为**袋装**，可以减轻决策树模型的不足。在袋装中，你从数据中抽取自助样本（带有替换的随机样本）。从每个样本中，你构建一个决策树模型。最终模型是所有单个决策树的平均值。这显示在[图10.5](../Text/10.xhtml#ch10fig05).^([[4](../Text/10.xhtml#ch10fn4)])
- en: ⁴
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bagging, random forests, and gradient-boosted trees are variations of a general
    technique called *ensemble learning*. An ensemble model is composed of the combination
    of several smaller simple models (often small decision trees). Giovanni Seni and
    John Elder’s *Ensemble Methods in Data Mining* (Morgan & Claypool, 2010) is an
    excellent introduction to the general theory of ensemble learning.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 袋装、随机森林和梯度提升树是称为**集成学习**的一般技术的变体。一个集成模型由几个较小的简单模型（通常是小的决策树）的组合组成。Giovanni Seni和John
    Elder的《数据挖掘中的集成方法》（Morgan & Claypool，2010）是集成学习一般理论的优秀入门书籍。
- en: Figure 10.5\. Bagging decision trees
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5\. 袋装决策树
- en: '![](Images/10fig05_alt.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig05_alt.jpg)'
- en: 'To make this concrete, suppose that *x* is an input datum, *y*_*i*(*x*) is
    the output of the *i*th tree, *c*(*y*_1(*x*), *y*_2(*x*), ... *y*_*n*(*x*)) is
    the vector of individual outputs, and *y* is the output of the final model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点具体化，假设*x*是一个输入数据，*y*_*i*(*x*)是第*i*个树的输出，*c*(*y*_1(*x*), *y*_2(*x*), ...
    *y*_*n*(*x*))是单个输出的向量，*y*是最终模型的输出：
- en: 'For regression, or for estimating class probabilities, *y*(*x*) is the average
    of the scores returned by the individual trees: *y*(*x*) = mean(*c*(*y*_1(*x*),
    ... *y*_*n*(*x*))).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归，或用于估计类概率，*y*(*x*)是单个树返回的分数的平均值：*y*(*x*) = mean(*c*(*y*_1(*x*), ... *y*_*n*(*x*)))。
- en: For classification, the final model assigns the class that got the most votes
    from the individual trees.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类，最终模型将分配给单个树投票最多的类别。
- en: Bagging decision trees stabilizes the final model by lowering the variance;
    this improves the accuracy. A bagged ensemble of trees is also less likely to
    overfit the data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装决策树通过降低方差来稳定最终模型，这提高了准确性。一个树袋集成也较少可能过度拟合数据。
- en: Try bagging some tree models for the spam example.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试对垃圾邮件示例进行一些树模型的袋装处理。
- en: Listing 10.2\. Bagging decision trees
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.2\. 袋装决策树
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Uses bootstrap samples the same size as the training set, with 100 trees
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用与训练集相同大小的自助样本，有100棵树
- en: ❷ Builds the bootstrap samples by sampling the row indices of spamTrain with
    replacement. Each column of the matrix samples represents the row indices into
    spamTrain that comprise the bootstrap sample.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过替换抽样`spamTrain`的行索引来构建自助样本。矩阵的每一列样本表示构成自助样本的`spamTrain`中的行索引。
- en: '❸ Trains the individual decision trees and returns them in a list. Note: This
    step can take a few minutes.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练单个决策树并将它们以列表形式返回。注意：此步骤可能需要几分钟。
- en: ❹ predict.bag assumes the underlying classifier returns decision probabilities,
    not decisions. predict.bag takes the mean of the predictions of all the individual
    trees
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ `predict.bag`假设底层分类器返回决策概率，而不是决策。`predict.bag`取所有单个树的预测的平均值
- en: ❺ Evaluates the bagged decision trees against the training and test sets
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 评估袋装决策树对训练集和测试集的性能
- en: As you see, bagging improves accuracy and F1, and reduces deviance over both
    the training and test sets when compared to the single decision tree (you’ll see
    a direct comparison of the scores a little later on). There is also less degradation
    in the bagged model’s performance going from training to test than there is with
    the decision tree.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与单个决策树相比，袋装提高了准确性和F1分数，并减少了训练集和测试集的偏差。与决策树相比，从训练到测试的袋装模型性能下降较少。
- en: You can further improve prediction performance by going from bagging to random
    forests.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过从袋装到随机森林来进一步提高预测性能。
- en: '* * *'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Bagging classifiers**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**袋装分类器**'
- en: The proofs that bagging reduces variance are only valid for regression and for
    estimating class probabilities, not for classifiers (a model that only returns
    class membership, not class probabilities). Bagging a bad classifier can make
    it worse. So you definitely want to work over estimated class probabilities, if
    they’re at all available. But it can be shown that for CART trees (which is the
    decision tree implementation in R) under mild assumptions, bagging tends to increase
    classifier accuracy. See Clifton D. Sutton, “Classification and Regression Trees,
    Bagging, and Boosting,” *Handbook of Statistics, Vol. 24* (Elsevier, 2005) for
    more details.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装减少方差的证明仅适用于回归和估计类概率，不适用于分类器（仅返回类成员资格，不返回类概率）。对不良分类器进行袋装可能会使其变得更糟。因此，如果您能获得类概率估计，您肯定希望在此基础上工作。但可以证明，在R中的CART树（决策树实现）在轻微假设下，袋装往往会提高分类器的准确性。有关更多详细信息，请参阅Clifton
    D. Sutton的“分类和回归树、袋装和提升”，*《统计手册，第24卷》*（Elsevier，2005）。
- en: '* * *'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 10.1.3\. Using random forests to further improve prediction
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3\. 使用随机森林进一步提高预测
- en: In bagging, the trees are built using randomized datasets, but each tree is
    built by considering the exact same set of features. This means that all the individual
    trees are likely to use very similar sets of features (perhaps in a different
    order or with different split values). Hence, the individual trees will tend to
    be overly correlated with each other. If there are regions in feature space where
    one tree tends to make mistakes, then all the trees are likely to make mistakes
    there, too, diminishing our opportunity for correction. The random forest approach
    tries to decorrelate the trees by randomizing the set of variables that each tree
    is allowed to use.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装中，使用随机数据集构建树，但每棵树都是通过考虑完全相同的特征集来构建的。这意味着所有单个树很可能使用非常相似的特征集（可能是不同的顺序或不同的分割值）。因此，单个树之间可能会过度相关。如果特征空间中有某些区域一棵树倾向于出错，那么所有树也可能会在那里出错，从而减少了我们的纠正机会。随机森林方法试图通过随机化每棵树允许使用的变量集来解耦树。
- en: 'The process is shown in [figure 10.6](../Text/10.xhtml#ch10fig06). For each
    individual tree in the ensemble, the random forest method does the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在[图10.6](../Text/10.xhtml#ch10fig06)中显示。对于集成中的每棵单个树，随机森林方法执行以下操作：
- en: Draws a bootstrapped sample from the training data
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据中抽取一个自助样本
- en: For each sample, grows a decision tree, and at each node of the tree
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个样本，生长一棵决策树，并在树的每个节点
- en: Randomly draws a subset of `mtry` variables from the `p` total features that
    are available
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`p`个总特征中随机抽取一个子集`mtry`变量
- en: Picks the best variable and the best split from that set of `mtry` variables
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`mtry`变量集中选择最佳变量和最佳分割
- en: Continues until the tree is fully grown
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续生长直到树完全成熟
- en: Figure 10.6\. Growing a random forest
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6\. 增长随机森林
- en: '![](Images/10fig06_alt.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig06_alt.jpg)'
- en: The final ensemble of trees is then bagged to make the random forest predictions.
    This is quite involved, but fortunately all done by a single-line random forest
    call.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将最终的树集合进行袋装，以生成随机森林预测。这相当复杂，但幸运的是，所有这些操作都由单行随机森林调用完成。
- en: By default, the `randomForest()` function in R draws `mtry = p/3` variables
    at each node for regression trees, and `m = sqrt(p)` variables for classification
    trees. In theory, random forests aren’t terribly sensitive to the value of `mtry`.
    Smaller values will grow the trees faster; but if you have a very large number
    of variables to choose from, of which only a small fraction are actually useful,
    then using a larger `mtry` is better, since with a larger `mtry` you’re more likely
    to draw some useful variables at every step of the tree-growing procedure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，R中的`randomForest()`函数在每个节点上为回归树抽取`mtry = p/3`个变量，为分类树抽取`m = sqrt(p)`个变量。从理论上讲，随机森林对`mtry`值的敏感性并不高。较小的值会使树生长得更快；但如果你有很多变量可供选择，其中只有一小部分真正有用，那么使用较大的`mtry`更好，因为使用较大的`mtry`，你更有可能在树的生长过程中每一步都抽取到一些有用的变量。
- en: Continuing from the data in [section 10.1](../Text/10.xhtml#ch10lev1sec1), try
    building a spam model using random forests.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第10.1节](../Text/10.xhtml#ch10lev1sec1)中的数据继续，尝试使用随机森林构建一个垃圾邮件模型。
- en: Listing 10.3\. Using random forests
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.3\. 使用随机森林
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Loads the randomForest package
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载randomForest包
- en: ❷ Sets the pseudo-random seed to a known value to try to make the random forest
    run repeatable
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置伪随机种子为已知值，以尝试使随机森林运行可重复
- en: ❸ Calls the randomForest() function to build the model with explanatory variables
    as x and the category to be predicted as y
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 调用randomForest()函数，以解释变量作为x，预测类别作为y来构建模型
- en: ❹ Uses 100 trees to be compatible with our bagging example. The default is 500
    trees.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用100棵树以与我们的袋装示例兼容。默认为500棵树。
- en: ❺ Specifies that each node of a tree must have a minimum of 7 elements to be
    compatible with the default minimum node size that rpart() uses on this training
    set
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 指定树的每个节点必须至少有7个元素，以与rpart()在此训练集上使用的默认最小节点大小兼容
- en: ❻ Tells the algorithm to save information to be used for calculating variable
    importance (we’ll see this later)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 告诉算法将信息保存下来，用于计算变量重要性（我们稍后会看到）
- en: ❻ Reports the model quality
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 报告模型质量
- en: 'You can summarize the results for all three of the models you’ve looked at.
    First, on training:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以总结你所查看的所有三个模型的成果。首先，在训练数据上：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, on test:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在测试数据上：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The random forest model performed dramatically better than the other two models
    on both training and test.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型在训练和测试数据上都比其他两个模型表现得好得多。
- en: 'You can also look at performance change: the decrease in accuracy and F1 when
    going from training to test, and the corresponding increase in deviance.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以查看性能变化：从训练到测试时准确性和F1的下降，以及偏差的增加。
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The random forest’s model degraded about as much as a single decision tree
    when going from training to test data, and much more than the bagged model did.
    This is one of the drawbacks of random forest models: the tendency to overfit
    the training data. However, in this case, the random forest model was still the
    best performing.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当从训练数据到测试数据时，随机森林模型退化程度与单个决策树相当，并且比袋装模型要多得多。这是随机森林模型的一个缺点：过度拟合训练数据的倾向。然而，在这种情况下，随机森林模型仍然是表现最好的。
- en: '* * *'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Random forests can overfit!**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林可以过拟合！**'
- en: It’s lore among random forest proponents that “random forests don’t overfit.”
    In fact, they can. Hastie et al. back up this observation in their chapter on
    random forests in *The Elements of Statistical Learning* (Springer, 2011). Seeing
    virtually perfect prediction on training data and less-than-perfect performance
    on holdout data is characteristic of random forest models. So when using random
    forest, it’s extremely important to validate model performance on holdout data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林的支持者中，有一个传说：“随机森林不会过拟合。”实际上，它们可以。Hastie等人在这本《统计学习的要素》中关于随机森林的章节中支持了这个观察结果（Springer，2011年）。在训练数据上看到几乎完美的预测，而在保留数据上表现不佳，这是随机森林模型的特点。因此，在使用随机森林时，在保留数据上验证模型性能非常重要。
- en: '* * *'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Examining variable importance
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 检查变量重要性
- en: 'A useful feature of the `randomForest()` function is its variable importance
    calculation. Since the algorithm uses a large number of bootstrap samples, each
    data point `x` has a corresponding set of *out-of-bag samples*: those samples
    that don’t contain the point `x`. This is shown in [figure 10.7](../Text/10.xhtml#ch10fig07)
    for the data point `x1`. The out-of-bag samples can be used in a way similar to
    *N*-fold cross-validation, to estimate the accuracy of each tree in the ensemble.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`randomForest()`函数的一个有用特性是其变量重要性计算。由于该算法使用大量自助样本，每个数据点`x`都有一个相应的**袋外样本集**：那些不包含点`x`的样本。这在[图10.7](../Text/10.xhtml#ch10fig07)中对于数据点`x1`进行了展示。袋外样本可以像**N**-折交叉验证一样使用，以估计集成中每棵树的准确性。'
- en: Figure 10.7\. Out-of-bag samples for datum `x1`
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7. 数据点`x1`的袋外样本
- en: '![](Images/10fig07_alt.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图10.7的替代文本](Images/10fig07_alt.jpg)'
- en: To estimate the “importance” of a variable `v1`, the variable’s values are randomly
    permuted. Each tree is then evaluated against its out-of-bag samples, and the
    corresponding decrease in each tree’s accuracy is estimated. This is shown in
    [figure 10.8](../Text/10.xhtml#ch10fig08).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计变量`v1`的“重要性”，随机排列该变量的值。然后，评估每棵树对其袋外样本，并估计每棵树准确性的相应下降。这已在[图10.8](../Text/10.xhtml#ch10fig08)中展示。
- en: Figure 10.8\. Calculating variable importance of variable `v1`
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8. 计算变量`v1`的重要性
- en: '![](Images/10fig08_alt.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8的替代文本](Images/10fig08_alt.jpg)'
- en: If the average decrease over all the trees is large, then the variable is considered
    important—its value makes a big difference in predicting the outcome. If the average
    decrease is small, then the variable doesn’t make much difference to the outcome.
    The algorithm also measures the decrease in node purity that occurs from splitting
    on a permuted variable (how this variable affects the quality of the tree).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有树的平均下降幅度很大，则认为该变量很重要——其值对预测结果有很大影响。如果平均下降幅度很小，则该变量对结果影响不大。算法还测量了在排列变量（如何影响树的质量）上分割时节点纯度的下降。
- en: You can calculate the variable importance by setting `importance = TRUE` in
    the `randomForest()` call (as you did in [listing 10.3](../Text/10.xhtml#ch10ex03)),
    and then calling the functions `importance()` and `varImpPlot()`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在`randomForest()`调用中设置`importance = TRUE`来计算变量重要性（就像你在[列表10.3](../Text/10.xhtml#ch10ex03)中所做的那样），然后调用`importance()`和`varImpPlot()`函数。
- en: Listing 10.4\. `randomForest` variable importances
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4. `randomForest`变量重要性
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Calls importance() on the spam model
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在垃圾邮件模型上调用`importance()`
- en: ❷ The importance() function returns a matrix of importance measures (larger
    values = more important).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `importance()`函数返回一个重要性度量矩阵（值越大表示越重要）。
- en: ❸ Plots the variable importance as measured by accuracy change
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 以准确性变化衡量变量重要性的图
- en: The result of the `varImpPlot()` call is shown in [figure 10.9](../Text/10.xhtml#ch10fig09).
    According to the plot, the most important variable for determining if an email
    is spam is `char.freq.bang`, or the number of times an exclamation point appears
    in an email, which makes some intuitive sense. The next most important variable
    is `word.freq.remove`, or the number of times the word “remove” appears in the
    email.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`varImpPlot()`调用的结果在[图10.9](../Text/10.xhtml#ch10fig09)中展示。根据图表，确定一封电子邮件是否为垃圾邮件最重要的变量是`char.freq.bang`，即电子邮件中感叹号出现的次数，这在某种程度上是有直觉意义的。下一个最重要的变量是`word.freq.remove`，即电子邮件中“remove”一词出现的次数。'
- en: Figure 10.9\. Plot of the most important variables in the spam model, as measured
    by accuracy
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9. 垃圾邮件模型中最重要变量的准确性测量图
- en: '![](Images/10fig09_alt.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图10.9的替代文本](Images/10fig09_alt.jpg)'
- en: Knowing which variables are most important (or at least, which variables contribute
    the most to the structure of the underlying decision trees) can help you with
    variable reduction. This is useful not only for building smaller, faster trees,
    but also for choosing variables to be used by another modeling algorithm, if that’s
    desired. We can reduce the number of variables in this spam example from 57 to
    30 without affecting the quality of the final model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 了解哪些变量最重要（或者至少，哪些变量对底层决策树的结构贡献最大）可以帮助你进行变量减少。这不仅有助于构建更小、更快的树，还可以在选择用于其他建模算法的变量时发挥作用。在这个垃圾邮件示例中，我们可以将变量数量从57减少到30，而不会影响最终模型的质量。
- en: '* * *'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Variable screening as an initial screening**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量筛选作为初步筛选**'
- en: Data scientist Jeremy Howard (of Kaggle and fast.ai fame) is a big proponent
    of using an initial variable importance screen early in a data science project
    to eliminate variables that are not of interest and identify variables to discuss
    with business partners.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家Jeremy Howard（Kaggle和fast.ai的知名人士）是使用初始变量重要性筛选的强烈支持者，在数据科学项目的早期阶段消除不感兴趣的变量，并确定与业务伙伴讨论的变量。
- en: '* * *'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Listing 10.5\. Fitting with fewer variables
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.5\. 使用较少变量进行拟合
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Sorts the variables by their importance, as measured by accuracy change
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 按照准确度变化率对变量进行排序
- en: ❷ Builds a random forest model using only the 30 most important variables
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仅使用30个最重要的变量构建随机森林模型
- en: ❸ Compares the two random forest models on the test set
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在测试集上比较两个随机森林模型
- en: The smaller model performs just as well as the random forest model built using
    all 57 variables.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的模型与使用所有57个变量构建的随机森林模型表现相当。
- en: '* * *'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Random forest variable importance versus LIME**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林变量重要性与LIME**'
- en: Random forest variable importance measures how important individual variables
    are to the model’s *overall* prediction performance. They tell you which variables
    generally affect the model’s predictions the most, or which variables the model
    depends on the most.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林变量重要性衡量单个变量对模型整体预测性能的重要性。它们告诉你哪些变量通常对模型的预测影响最大，或者模型最依赖哪些变量。
- en: LIME variable importances (discussed in [section 6.3](../Text/06.xhtml#ch06lev1sec3))
    measure how much different variables affect the model’s prediction *on a specific
    example*. LIME explanations can help you determine if the model is using its variables
    appropriately, by explaining specific decisions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: LIME变量重要性（在第6.3节[section 6.3](../Text/06.xhtml#ch06lev1sec3)中讨论）衡量不同变量对模型在特定示例上的预测影响程度。LIME解释可以帮助你确定模型是否适当地使用了其变量，通过解释特定的决策。
- en: '* * *'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 10.1.4\. Gradient-boosted trees
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.4\. 梯度提升树
- en: '*Gradient boosting* is another ensemble method that improves the performance
    of decision trees. Rather than averaging the predictions of several trees together,
    as bagging and random forests do, gradient boosting tries to improve prediction
    performance by incrementally adding trees to an existing ensemble. The steps are
    as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度提升*是另一种集成方法，它通过逐步向现有集成中添加树来提高决策树的表现。与bagging和随机森林不同，梯度提升不是简单地平均多个树的预测，而是试图通过增量添加树来提高预测性能。步骤如下：'
- en: Use the current ensemble `TE` to make predictions on the training data.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前的集成`TE`对训练数据进行预测。
- en: Measure the residuals between the true outcomes and the predictions on the training
    data.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量真实结果与训练数据上的预测之间的残差。
- en: Fit a new tree `T_i` to the residuals. Add `T_i` to the ensemble `TE`.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新树`T_i`拟合到残差。将`T_i`添加到集成`TE`中。
- en: Continue until the residuals have vanished, or another stopping criterion is
    achieved.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续直到残差消失，或达到另一个停止标准。
- en: The procedure is sketched out in [figure 10.10](../Text/10.xhtml#ch10fig10).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在[图10.10](../Text/10.xhtml#ch10fig10)中进行了概述。
- en: Figure 10.10\. Building up a gradient-boosted tree model
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10\. 构建梯度提升树模型
- en: '![](Images/10fig10_alt.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig10_alt.jpg)'
- en: Gradient-boosted trees can also overfit, because at some point the residuals
    are just random noise. To mitigate overfitting, most implementations of gradient
    boosting provide cross-validation methods to help determine when to stop adding
    trees to the ensemble.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树也可能过拟合，因为某个时刻残差只是随机噪声。为了减轻过拟合，大多数梯度提升的实现都提供了交叉验证方法，以帮助确定何时停止向集成中添加树。
- en: You saw examples of gradient boosting when we discussed LIME in [section 6.3](../Text/06.xhtml#ch06lev1sec3),
    where you fit the gradient-boosted tree models using the `xgboost` package. In
    this section, we’ll go over the modeling code that you used in [section 6.3](../Text/06.xhtml#ch06lev1sec3)
    in more detail.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第6.3节[section 6.3](../Text/06.xhtml#ch06lev1sec3)中讨论LIME时，你看到了梯度提升的例子，其中使用`xgboost`包拟合了梯度提升树模型。在本节中，我们将更详细地介绍你在第6.3节[section
    6.3](../Text/06.xhtml#ch06lev1sec3)中使用的建模代码。
- en: The iris example
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花示例
- en: Let’s start with a small example.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个小例子开始。
- en: '* * *'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you have a dataset of petal and sepal measurements for three varieties
    of iris. The object is to predict whether a given iris is a setosa based on its
    petal and sepal dimensions.*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你有一个包含三种鸢尾花品种花瓣和萼片尺寸的数据集。目标是根据花瓣和萼片的尺寸预测给定的鸢尾花是否为setosa品种。*'
- en: '* * *'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Listing 10.6\. Loading the iris data
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.6\. 加载鸢尾花数据
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ setosa is the positive class.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ setosa是正类。
- en: ❷ Splits the data into training and test (75%/25%)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据分为训练集和测试集（75%/25%）
- en: ❸ Creates the input matrix
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建输入矩阵
- en: Note that `xgboost` requires its input to be a numeric (no categorical variables)
    matrix, so in [listing 10.6](../Text/10.xhtml#ch10ex06), you take the input data
    from the training data frame and create an input matrix.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`xgboost`需要其输入为数值矩阵（没有分类变量），所以在[列表10.6](../Text/10.xhtml#ch10ex06)中，你从训练数据框中获取输入数据并创建一个输入矩阵。
- en: In [section 6.3](../Text/06.xhtml#ch06lev1sec3), you fit the iris model by using
    the preprovided convenience function `fit_iris_example()`; here we’ll explain
    the code from that function in detail. The first step is to run the cross-validation
    function `xgb.cv()` to determine the appropriate number of trees to use.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6.3节](../Text/06.xhtml#ch06lev1sec3)中，你使用预提供的便利函数`fit_iris_example()`拟合了鸢尾花模型；这里我们将详细解释该函数中的代码。第一步是运行交叉验证函数`xgb.cv()`以确定要使用的树的数量。
- en: Listing 10.7\. Cross-validating to determine model size
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.7. 交叉验证以确定模型大小
- en: '[PRE10]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The input matrix
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入矩阵
- en: ❷ The class labels, which must also be numeric (1 for setosa, 0 for not setosa)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 类标签也必须是数字（1代表setosa，0代表非setosa）
- en: ❸ Uses the objective “binary:logistic” for binary classification, “reg:linear”
    for regression
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用“binary:logistic”目标函数进行二分类，使用“reg:linear”进行回归
- en: ❹ Uses 5-fold cross-validation
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用5折交叉验证
- en: ❺ Builds an ensemble of 100 trees
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 构建包含100棵树的集成
- en: ❻ Prints a message every 10th iteration (use verbose = FALSE for no messages)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 每10次迭代打印一条消息（使用verbose = FALSE不打印消息）
- en: ❻ Uses minimum cross-validated logloss (related to deviance) to pick the optimum
    number of trees. For regression, uses metrics = “rmse”.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用最小交叉验证对数损失（与偏差相关）来选择最佳树的数量。对于回归，使用指标“rmse”。
- en: ❽ Gets the performance log
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 获取性能日志
- en: ❾ evalframe records the training and cross-validated logloss as a function of
    the number of trees.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ `evalframe`记录了训练和交叉验证对数损失作为树数量的函数。
- en: ❿ Finds the number of trees that gave the minimum cross-validated logloss
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 找到给出最小交叉验证对数损失的树的数量
- en: '[Figure 10.11](../Text/10.xhtml#ch10fig11) shows the cross-validated log loss
    as a function of the number of trees. In this case, `xgb.cv()` estimated that
    18 trees gave the best model. Once you know the number of trees to use, you can
    call `xgboost()` to fit the appropriate model.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.11](../Text/10.xhtml#ch10fig11)显示了交叉验证对数损失作为树数量的函数。在这种情况下，`xgb.cv()`估计18棵树给出了最佳模型。一旦你知道要使用的树的数量，你就可以调用`xgboost()`来拟合适当的模型。'
- en: Figure 10.11\. Cross-validated log loss as a function of ensemble size
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11. 验证集交叉验证对数损失作为集成大小的函数
- en: '![](Images/10fig11_alt.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig11_alt.jpg)'
- en: Listing 10.8\. Fitting an `xgboost` model
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.8. 拟合`xgboost`模型
- en: '[PRE11]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Creates the input matrix for the test data
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为测试数据创建输入矩阵
- en: ❷ Makes predictions
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 进行预测
- en: 'The model predicts perfectly on the holdout data, because this is an easy problem.
    Now that you are familiar with the steps, you can try `xgboost` on a harder problem:
    the movie review classification problem from [section 6.3.3](../Text/06.xhtml#ch06lev2sec12).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在保留数据上预测完美，因为这是一个简单的问题。现在你已经熟悉了步骤，你可以尝试在更难的问题上使用`xgboost`：[第6.3.3节](../Text/06.xhtml#ch06lev2sec12)中的电影评论分类问题。
- en: Gradient boosting for text classification
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的梯度提升
- en: '* * *'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*For this example, you will classify movie reviews from the Internet Movie
    Database (IMDB). The task is to identify positive reviews.*'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于这个例子，你将使用来自互联网电影数据库（IMDB）的电影评论进行分类。任务是识别正面评论。*'
- en: '* * *'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'As you did in [section 6.3.3](../Text/06.xhtml#ch06lev2sec12), you’ll use the
    training and test data, `IMDBtrain.RDS` and `IMDBtest.RDS`, found at [https://github.com/WinVector/PDSwR2/tree/master/IMDB](https://github.com/WinVector/PDSwR2/tree/master/IMDB).
    Each `RDS` object is a list with two elements: a character vector representing
    25,000 reviews, and a vector of numeric labels where 1 means a positive review
    and 0 a negative review.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[第6.3.3节](../Text/06.xhtml#ch06lev2sec12)中所做的那样，你将使用训练和测试数据，`IMDBtrain.RDS`和`IMDBtest.RDS`，可以在[https://github.com/WinVector/PDSwR2/tree/master/IMDB](https://github.com/WinVector/PDSwR2/tree/master/IMDB)找到。每个`RDS`对象是一个包含两个元素的列表：一个表示25,000条评论的字符向量，以及一个表示标签的数值向量，其中1表示正面评论，0表示负面评论。
- en: 'First, load the training data:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载训练数据：
- en: '[PRE12]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You have to convert the textual input data to a numeric representation. As
    in [section 6.3.3](../Text/06.xhtml#ch06lev2sec12), you’ll convert the training
    data into a document-term matrix, implemented as a sparse matrix of class `dgCMatrix`.
    The convenience functions to do this conversion are in [https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R).
    Next, you’ll create the vocabulary of terms in the corpus, and then make the document-term
    matrix for the training data:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须将文本输入数据转换为数值表示。正如[第6.3.3节](../Text/06.xhtml#ch06lev2sec12)中所述，你将训练数据转换为文档-词矩阵，实现为`dgCMatrix`类的稀疏矩阵。用于执行此转换的便利函数在[https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R](https://github.com/WinVector/PDSwR2/tree/master/IMDB/lime_imdb_example.R)中。接下来，你将创建语料库中的术语词汇表，然后为训练数据创建文档-词矩阵：
- en: '[PRE13]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first step to fit the model is to determine the number of trees to use.
    This may take a while.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 配置模型的第一步是确定要使用的树的数量。这可能需要一些时间。
- en: '[PRE14]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Stop early if performance doesn’t improve for 20 rounds.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果20轮内性能没有提高，则提前停止。
- en: 'Then fit the model and evaluate it:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后拟合模型并评估它：
- en: '[PRE15]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Loads the test data and converts it to a document-term matrix
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载测试数据并将其转换为文档-词矩阵
- en: As with random forests, this gradient-boosted model gives near-perfect performance
    on training data, and less-than-perfect, but still decent performance on holdout
    data. Even though the cross-validation step suggested 319 trees, you may want
    to examine `evalframe` (as you did in the iris example), and experiment with different
    numbers of trees, to see if that reduces the overfitting.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林一样，这个梯度提升模型在训练数据上给出了近乎完美的性能，在保留数据上则略逊一筹，但仍然相当不错。尽管交叉验证步骤建议使用319棵树，你可能还想检查`evalframe`（就像你在鸢尾花示例中所做的那样），并尝试不同的树的数量，看看是否可以减少过拟合。
- en: '* * *'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Gradient boosting models vs. random forests**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升模型与随机森林**'
- en: In our own work, we’ve found that gradient boosting models tend to outperform
    random forests on most problems where we’ve tried both. However, there are occasionally
    situations where gradient boosting models perform poorly, while random forest
    models give acceptable performance. Your experiences may be different. In any
    case, it’s a good idea to keep both methods in your arsenal.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们自己的工作中，我们发现梯度提升模型在大多数我们已经尝试过的问题上往往优于随机森林。然而，偶尔会有梯度提升模型表现不佳，而随机森林模型给出可接受性能的情况。你的经验可能不同。无论如何，保留这两种方法在你的工具箱中是个好主意。
- en: '* * *'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Using xgboost with categorical variables
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用xgboost处理分类变量
- en: In the iris example, all the input variables were numeric; in the movie review
    example, you converted the unstructured text input into a structured, numeric
    matrix representation. In many situations, you will have structured input data
    with categorical levels, as in the following example.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在鸢尾花示例中，所有输入变量都是数值型的；在电影评论示例中，你将非结构化文本输入转换为结构化、数值矩阵表示。在许多情况下，你将拥有具有分类级别的结构化输入数据，如下例所示。
- en: '* * *'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to predict a newborn’s birth weight as a function of several
    variables, both numeric and categorical, using* `xgboost`*.*'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你想使用`xgboost`预测新生儿的出生体重作为几个变量的函数，这些变量既有数值型也有分类型。* '
- en: '* * *'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The data for this example is from the 2010 CDC natality dataset; it is similar
    to the data that you used in [chapter 7](../Text/07.xhtml#ch07) for predicting
    at-risk births.^([[5](../Text/10.xhtml#ch10fn5)])
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中的数据来自2010年CDC出生数据集；它与你在[第7章](../Text/07.xhtml#ch07)中用于预测风险出生的数据相似。[^([[5](../Text/10.xhtml#ch10fn5))]
- en: ⁵
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The dataset can be found at [https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData](https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData).
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据集可以在[https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData](https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData)找到。
- en: Listing 10.9\. Loading the natality data
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.9\. 加载出生数据
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Splits the data into training and test sets
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据分为训练集和测试集
- en: ❷ Uses all the variables in the model. DBWT (baby's birth weight) is the value
    to be predicted, and ORIGRANDGROUP is the grouping variable.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用模型中的所有变量。DBWT（婴儿出生体重）是要预测的值，ORIGRANDGROUP是分组变量。
- en: As you can see, the input data has numerical variables, logical variables, and
    categorical (factor) variables. If you want to use `xgboost()` to fit a gradient-boosted
    model for a baby’s birth weight using all of these variables, you must convert
    the input to all-numerical data. There are several ways to do this, including
    the base R `model .matrix()` function. We recommend using `vtreat`, as you did
    in [chapter 8](../Text/08.xhtml#ch08).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，输入数据包含数值变量、逻辑变量和分类（因子）变量。如果你想使用 `xgboost()` 来拟合一个使用所有这些变量的梯度提升模型以预测婴儿的出生体重，你必须将输入转换为全数值数据。有几种方法可以实现这一点，包括基础
    R 的 `model.matrix()` 函数。我们推荐使用 `vtreat`，正如你在第 8 章中所做的那样。
- en: 'For this scenario, there are three ways you can use `vtreat`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个场景，你可以使用 `vtreat` 的三种方法：
- en: 'Split the data into three sets: calibration/train/test. Use the calibration
    set with `designTreatmentsN()` to create the treatment plan; `prepare()` the training
    set to fit the `xgboost` model; and then `prepare()` the test set to validate
    the model. This is a good option when you have a large training set with either
    complex variables (categorical variables that take on a large number of possible
    levels), or a large number of categorical variables. It is also a good option
    if you want to prune some of the variables before fitting the model (using significance
    pruning—see [section 8.4.2](../Text/08.xhtml#ch08lev2sec6)).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分为三个集合：校准/训练/测试。使用 `designTreatmentsN()` 与校准集一起创建治疗方案；使用 `prepare()` 准备训练集以拟合
    `xgboost` 模型；然后使用 `prepare()` 准备测试集以验证模型。当你拥有一个包含复杂变量（具有大量可能级别的分类变量）或大量分类变量的大型训练集时，这是一个不错的选择。如果你希望在拟合模型之前剪枝一些变量（使用显著性剪枝——参见
    [第 8.4.2 节](../Text/08.xhtml#ch08lev2sec6)），这也是一个好的选择。
- en: Split the data into train/test (as we did here). Use `mkCrossFrameNExperiment()`
    to create the treatment plan and a cross-frame for training the `xgboost` model;
    `prepare()` the test set to validate the model. This is a good option when you
    don’t have enough training data to split into three groups, but you have complex
    variables or a large number of categorical variables, and/or you want to prune
    some of the variables before fitting the model.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分为训练/测试集（正如我们在这里所做的那样）。使用 `mkCrossFrameNExperiment()` 创建治疗方案和交叉帧以训练 `xgboost`
    模型；使用 `prepare()` 准备测试集以验证模型。当你没有足够的训练数据来分为三组，但你拥有复杂变量或大量分类变量，以及/或者你希望在拟合模型之前剪枝一些变量时，这是一个不错的选择。
- en: Split the data into train/test. Use `designTreatmentsZ()` to create a treatment
    plan that manages missing values and converts categorical variables to indicator
    variables. `prepare()` both the training and test sets to create purely numeric
    input. This solution is quite similar to calling `model.matrix()`, with the added
    advantage that it manages missing values, and also gracefully handles situations
    where some categorical levels show up in either training or test, but not both.
    It’s a good solution when you only have a few categorical variables, and none
    of the variables are too complex.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分为训练/测试集。使用 `designTreatmentsZ()` 创建一个处理计划，该计划管理缺失值并将分类变量转换为指示变量。使用 `prepare()`
    准备训练集和测试集以创建纯数值输入。这个解决方案与调用 `model.matrix()` 类似，但增加了管理缺失值和优雅地处理某些分类级别只在训练或测试中出现，而不在两者中都出现的情况的优势。当你只有少量分类变量，且没有任何变量过于复杂时，这是一个很好的解决方案。
- en: Since in this scenario there are only two categorical variables, and none of
    them are too complex (`GESTREC3` takes on two values, and `DPLURAL` takes on three),
    you can use the third option.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在这个场景中只有两个分类变量，而且它们都不太复杂（`GESTREC3` 有两个值，`DPLURAL` 有三个值），你可以使用第三种方法。
- en: Listing 10.10\. Using `vtreat` to prepare data for `xgboost`
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.10\. 使用 `vtreat` 准备 `xgboost` 的数据
- en: '[PRE17]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Creates the treatment plan
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建治疗方案
- en: ❷ Creates clean numeric variables (“clean”), missingness indicators (“isBad”),
    indicator variables (“lev”), but not catP (prevalence) variables
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建干净的数值变量（“clean”）、缺失值指示器（“isBad”）、指示变量（“lev”），但不包括 catP（患病率）变量
- en: ❸ Prepares the training data
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 准备训练数据
- en: Note that `train_treated` is purely numerical, with no missing values, and it
    doesn’t contain the outcome column, so it’s safe to use with `xgboost` (though
    you must convert it to a matrix first). To demonstrate this, the following listing
    directly fits a gradient-boosted model with 50 trees to the prepared training
    data (no cross-validation to pick the best size), and then applies the model to
    the prepared test data. This is just for demonstration purposes; normally you
    would want to call `xgb.cv()` to pick an appropriate number of trees first.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`train_treated`完全是数值的，没有缺失值，并且不包含结果列，因此可以安全地与`xgboost`一起使用（尽管你必须首先将其转换为矩阵）。为了演示这一点，以下列表直接将50棵树的梯度提升模型拟合到准备好的训练数据（没有交叉验证来选择最佳大小），然后将模型应用于准备好的测试数据。这只是为了演示目的；通常你想要先调用`xgb.cv()`来选择合适的树的数量。
- en: Listing 10.11\. Fitting and applying an `xgboost` model for birth weight
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.11\. 对出生体重进行`xgboost`模型的拟合和应用
- en: '[PRE18]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '* * *'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Exercise: Try to use xgboost to solve the birth weight problem.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 练习：尝试使用xgboost来解决出生体重问题。
- en: '*Try `xgboost` to predict `DBWT`, that is, set up the data and run the preceding
    code.*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*尝试使用`xgboost`来预测`DBWT`，即设置数据和运行前面的代码。*'
- en: '* * *'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Bagging, random forests, and gradient boosting are after-the-fact improvements
    you can try in order to improve decision tree models. In the next section, you’ll
    work with generalized additive models, which use a different method to represent
    non-linear relationships between inputs and outputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging、随机森林和梯度提升是事后可以尝试的改进，以提高决策树模型。在下一节中，你将使用广义加性模型，它使用不同的方法来表示输入和输出之间的非线性关系。
- en: 10.1.5\. Tree-based model takeaways
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.5\. 基于树的模型要点
- en: 'Here''s what you should remember about tree-based models:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于树的模型，你应该记住以下几点：
- en: Trees are useful for modeling data with non-linear relationships between the
    input and the output, and potential interactions among variables.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树对于建模输入和输出之间具有非线性关系的数据以及变量之间的潜在交互是有用的。
- en: Tree-based ensembles generally have better performance than basic decision tree
    models.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于树的集成通常比基本的决策树模型有更好的性能。
- en: Bagging stabilizes decision trees and improves accuracy by reducing variance.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging通过减少方差来稳定决策树并提高准确性。
- en: Both random forests and gradient-boosted trees may have a tendency to overfit
    on training data. Be sure to evaluate the models on holdout data to get a better
    estimate of model performance.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林和梯度提升树都可能倾向于在训练数据上过拟合。确保在保留数据上评估模型，以获得更好的模型性能估计。
- en: 10.2\. Using generalized additive models (GAMs) to learn non-monotone relationships
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2\. 使用广义加性模型（GAMs）来学习非单调关系
- en: In [chapter 7](../Text/07.xhtml#ch07), you used linear regression to model and
    predict quantitative output, and logistic regression to predict class probabilities.
    Linear and logistic regression models are powerful tools, especially when you
    want to understand the relationship between the input variables and the output.
    They’re robust to correlated variables (when regularized), and logistic regression
    preserves the marginal probabilities of the data. The primary shortcoming of both
    these models is that they assume that the relationship between the inputs and
    the output is monotone. That is, if more is good, than much more is always better.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](../Text/07.xhtml#ch07)中，你使用了线性回归来建模和预测定量输出，以及逻辑回归来预测类别概率。线性回归和逻辑回归模型是强大的工具，特别是当你想了解输入变量和输出变量之间的关系时。它们对相关变量（当正则化时）具有鲁棒性，逻辑回归保留了数据的边缘概率。这两个模型的主要缺点是它们假设输入和输出之间的关系是单调的。也就是说，如果更多是好的，那么更多总是更好的。
- en: 'But what if the actual relationship is non-monotone? Consider the BMI example
    that you saw at the beginning of the chapter. For underweight adults, increasing
    BMI can lower mortality. But there’s a limit: at some point a higher BMI is bad,
    and mortality will increase as BMI increases. Linear and logistic regression miss
    this distinction. For the data that we are working with, as [figure 10.12](../Text/10.xhtml#ch10fig12)
    shows, a linear model would predict that mortality always decreases as BMI increases.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果实际关系是非单调的呢？考虑一下你在本章开头看到的BMI例子。对于体重不足的成年人，增加BMI可以降低死亡率。但有一个限度：在某个点上，更高的BMI会变得有害，随着BMI的增加，死亡率也会上升。线性回归和逻辑回归无法捕捉到这种区别。对于我们正在处理的数据，如图10.12所示，线性模型会预测死亡率总是随着BMI的增加而降低。
- en: 'Figure 10.12\. The effect of BMI on mortality: linear model vs. GAM'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12\. BMI对死亡率的影响：线性模型与GAM
- en: '![](Images/10fig12_alt.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig12_alt.jpg)'
- en: '*Generalized additive models* (GAMs) are a way to model non-monotone responses
    within the framework of a linear or logistic model (or any other generalized linear
    model). In the mortality example, GAM would try to find a good “u-shaped” function
    of BMI, `s(BMI)`, that describes the relationship between BMI and mortality, as
    shown in [figure 10.12](../Text/10.xhtml#ch10fig12). GAM would then fit a function
    to predict mortality in terms of `s(BMI)`.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '*广义加性模型*（GAMs）是在线性或逻辑模型（或任何其他广义线性模型）框架内对非单调响应进行建模的一种方法。在死亡率示例中，GAM 将尝试找到一个好的“U形”函数
    `s(BMI)`，来描述 BMI 与死亡率之间的关系，如图 [图10.12](../Text/10.xhtml#ch10fig12) 所示。GAM 然后将拟合一个函数来根据
    `s(BMI)` 预测死亡率。'
- en: 10.2.1\. Understanding GAMs
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1\. 理解 GAMs
- en: Recall that if `y[i]` is the numeric quantity you want to predict, and `x[i,
    ]` is a row of inputs that corresponds to output `y[i]`, then linear regression
    finds a function `f(x)` such that
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，如果 `y[i]` 是你想要预测的数值量，而 `x[i, ]` 是对应于输出 `y[i]` 的输入行，那么线性回归找到一个函数 `f(x)`，使得
- en: '[PRE19]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And `f(x[i, ])` is as close to `y[i]` as possible.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 `f(x[i, ])` 尽可能接近 `y[i]`。
- en: In its simplest form, a GAM model relaxes the linearity constraint and finds
    a set of functions `s_i()` (and a constant term `a0`) such that
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单形式中，GAM 模型放宽了线性约束，并找到一组函数 `s_i()`（以及一个常数项 `a0`），使得
- en: '[PRE20]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We also want `f(x[i, ])` to be as close to `y[i]` as possible. The functions
    `s_i()` are smooth curve fits that are built up from polynomials. The curves are
    called *splines* and are designed to pass as closely as possible through the data
    without being too “wiggly” (without overfitting). An example of a spline fit is
    shown in [figure 10.13](../Text/10.xhtml#ch10fig13).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也希望 `f(x[i, ])` 尽可能接近 `y[i]`。函数 `s_i()` 是由多项式构建的平滑曲线拟合。这些曲线被称为 *样条*，它们被设计成尽可能接近数据点而不太“扭曲”（不过度拟合）。一个样条拟合的例子在
    [图10.13](../Text/10.xhtml#ch10fig13) 中展示。
- en: Figure 10.13\. A spline that has been fit through a series of points
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13\. 通过一系列点拟合的样条
- en: '![](Images/10fig13_alt.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig13_alt.jpg)'
- en: Let’s work on a concrete example.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子。
- en: 10.2.2\. A one-dimensional regression example
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2\. 一维回归示例
- en: First, consider this toy example.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，考虑这个玩具示例。
- en: '* * *'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to fit a model to data where the response* `y` *is a noisy
    non-linear function of the input variable* `x` *(in fact, it’s the function shown
    in [figure 10.13](../Text/10.xhtml#ch10fig13)).*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你想要将模型拟合到数据中，其中响应* `y` *是输入变量* `x` *的噪声非线性函数*（实际上，它是 [图10.13](../Text/10.xhtml#ch10fig13)
    中显示的函数）。*'
- en: '* * *'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As usual, we’ll split the data into training and test sets.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们将数据分为训练集和测试集。
- en: Listing 10.12\. Preparing an artificial problem
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.12\. 准备一个人工问题
- en: '[PRE21]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Given that the data is from the non-linear functions `sin()` and `cos()`, there
    shouldn’t be a good linear fit from `x` to `y`. We’ll start by building a (poor)
    linear regression.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据来自非线性函数 `sin()` 和 `cos()`，从 `x` 到 `y` 不应该有一个好的线性拟合。我们将首先构建一个（较差的）线性回归。
- en: Listing 10.13\. Applying linear regression to the artificial example
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.13\. 将线性回归应用于人工示例
- en: '[PRE22]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ A convenience function for calculating root mean squared error (RMSE ) from
    a vector of residuals
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算残差向量中均方根误差（RMSE）的便利函数
- en: ❷ Calculates the RMSE of this model on the training data
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算此模型在训练数据上的 RMSE
- en: ❸ Plots y versus prediction
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制 y 与预测值的对比图
- en: 'The resulting model’s predictions are plotted versus true response in [figure
    10.14](../Text/10.xhtml#ch10fig14). As expected, it’s a very poor fit, with an
    R-squared of about 0.04\. In particular, the errors are *not homoscedastic*: there
    are regions where the model systematically underpredicts and regions where it
    systematically overpredicts. If the relationship between `x` and `y` were truly
    linear (with independent noise), then the errors would be *homoscedastic*: the
    errors would be evenly distributed (mean 0) around the predicted value everywhere.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型的预测与真实响应在 [图10.14](../Text/10.xhtml#ch10fig14) 中绘制。正如预期的那样，这是一个非常差的拟合，R-squared
    约为 0.04。特别是，误差 *不是同方差*：存在模型系统性地低估和系统性地高估的区域。如果 `x` 和 `y` 之间的关系确实是线性的（具有独立的噪声），那么误差将是
    *同方差*：误差将在预测值周围均匀分布（均值为 0）。
- en: Figure 10.14\. Linear model’s predictions vs. actual response. The solid line
    is the line of perfect prediction (`prediction == actual`).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14\. 线性模型的预测与实际响应。实线是完美预测的线（预测 == 实际）。
- en: '![](Images/10fig14_alt.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig14_alt.jpg)'
- en: Now try finding a non-linear model that maps `x` to `y`. We’ll use the function
    `gam()` in the package `mgcv`.^([[6](../Text/10.xhtml#ch10fn6)]) When using `gam()`,
    you can model variables as either linear or non-linear. You model a variable `x`
    as non-linear by wrapping it in the `s()` notation. In this example, rather than
    using the formula `y ~ x` to describe the model, you’d use the formula `y ~ s(x)`.
    Then `gam()` will search for the spline `s()` that best describes the relationship
    between `x` and `y`, as shown in [listing 10.14](../Text/10.xhtml#ch10ex14). Only
    terms surrounded by `s()` get the GAM/spline treatment.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试找到一个将 `x` 映射到 `y` 的非线性模型。我们将使用 `mgcv` 包中的 `gam()` 函数。^([[6](../Text/10.xhtml#ch10fn6)])
    当使用 `gam()` 函数时，你可以将变量建模为线性或非线性。通过将变量 `x` 包裹在 `s()` 符号中，你可以将其建模为非线性。在这个例子中，你不会使用公式
    `y ~ x` 来描述模型，而是使用公式 `y ~ s(x)`。然后 `gam()` 将搜索最佳样条 `s()` 来描述 `x` 和 `y` 之间的关系，如
    [代码列表 10.14](../Text/10.xhtml#ch10ex14) 所示。只有被 `s()` 包围的项才会接受 GAM/样条处理。
- en: ⁶
  id: totrans-302
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-303
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There’s an older package called `gam`, written by Hastie and Tibshirani, the
    inventors of GAMs. The `gam` package works fine. But it’s incompatible with the
    `mgcv` package, which `ggplot` already loads. Since we’re using `ggplot` for plotting,
    we’ll use `mgcv` for our examples.
  id: totrans-304
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有一个名为 `gam` 的旧包，由 GAM 的发明者 Hastie 和 Tibshirani 编写。`gam` 包运行良好。但它与 `mgcv` 包不兼容，而
    `ggplot` 已经加载了 `mgcv` 包。由于我们使用 `ggplot` 进行绘图，我们将使用 `mgcv` 作为我们的示例。
- en: Listing 10.14\. Applying GAM to the artificial example
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.14\. 将 GAM 应用于人工示例
- en: '[PRE23]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Loads the mgcv package
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载 mgcv 包
- en: ❷ Builds the model, specifying that x should be treated as a non-linear variable
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建模型，指定 x 应被视为非线性变量
- en: ❸ The converged parameter tells you if the algorithm converged. You should only
    trust the output if this is TRUE.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 收敛参数告诉你算法是否收敛。只有当此值为 TRUE 时，你才应该相信输出结果。
- en: ❹ Setting family = gaussian and link = identity tells you that the model was
    treated with the same distribution assumptions as a standard linear regression.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置 family = gaussian 和 link = identity 告诉你模型被处理为与标准线性回归相同的分布假设。
- en: ❺ The parametric coefficients are the linear terms (in this example, only the
    constant term). This section of the summary tells you which linear terms were
    significantly different from 0.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 参数系数是线性项（在这个例子中，只有常数项）。本节总结告诉你哪些线性项与 0 显著不同。
- en: ❻ The smooth terms are the non-linear terms. This section of the summary tells
    you which non-linear terms were significantly different from 0\. It also tells
    you the effective degrees of freedom (edf) used to build each smooth term. An
    edf near 1 indicates that the variable has an approximately linear relationship
    to the output.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 平滑项是非线性项。本节总结告诉你哪些非线性项与 0 显著不同。它还告诉你构建每个平滑项使用的有效自由度（edf）。edf 接近 1 表示变量与输出有大约线性关系。
- en: ❻ R-sq.(adj) is the adjusted R-squared. “Deviance explained” is the raw R-squared
    (0.834).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ R-sq.(adj) 是调整后的 R 平方。 “Deviance explained” 是原始 R 平方（0.834）。
- en: ❽ Calculates the RMSE of this model on the training data
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 计算此模型在训练数据上的 RMSE
- en: ❾ Plots y versus prediction
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 绘制 y 与预测值的对比图
- en: 'The resulting model’s predictions are plotted versus true response in [figure
    10.15](../Text/10.xhtml#ch10fig15). This fit is much better: the model explains
    over 80% of the variance (R-squared of 0.83), and the root mean squared error
    (RMSE) over the training data is less than half the RMSE of the linear model.
    Note that the points in [figure 10.15](../Text/10.xhtml#ch10fig15) are distributed
    more or less evenly around the line of perfect prediction. The GAM has been fit
    to be homoscedastic, and any given prediction is as likely to be an overprediction
    as an underprediction.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型的预测值与真实响应值在 [图 10.15](../Text/10.xhtml#ch10fig15) 中进行对比。这个拟合效果要好得多：模型解释了超过
    80% 的方差（R 平方为 0.83），并且在训练数据上的均方根误差（RMSE）小于线性模型的 RMSE 的一半。注意，[图 10.15](../Text/10.xhtml#ch10fig15)
    中的点在大约均匀地分布在完美预测线的周围。GAM 被拟合为同方差，任何给定的预测值作为高估或低估的可能性相同。
- en: Figure 10.15\. GAM’s predictions vs. actual response. The solid line is the
    theoretical line of perfect prediction (`prediction == actual`).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15\. GAM 的预测值与实际响应值。实线是理论上的完美预测线（预测值等于实际值）。
- en: '![](Images/10fig15_alt.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig15_alt.jpg)'
- en: '* * *'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Modeling linear relationships using gam()**'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 gam() 建模线性关系**'
- en: By default, `gam()` will perform standard linear regression. If you were to
    call `gam()` with the formula `y ~ x`, you’d get the same model that you got using
    `lm()`. More generally, the call `gam(y ~ x1 + s(x2), data=...)` would model the
    variable `x1` as having a linear relationship with `y`, and try to fit the best
    possible smooth curve to model the relationship between `x2` and `y`. Of course,
    the best smooth curve could be a straight line, so if you’re not sure whether
    the relationship between `x` and `y` is linear, you can use `s(x)`. If you see
    that the coefficient has an `edf` (effective degrees of freedom—see the model
    summary in [listing 10.14](../Text/10.xhtml#ch10ex14)) of about 1, then you can
    try refitting the variable as a linear term.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`gam()`将执行标准线性回归。如果你用公式`y ~ x`调用`gam()`，你会得到与使用`lm()`相同的结果。更普遍地，调用`gam(y
    ~ x1 + s(x2), data=...)`将变量`x1`建模为与`y`有线性关系，并尝试拟合最佳可能的平滑曲线来建模`x2`与`y`之间的关系。当然，最佳平滑曲线可能是一条直线，所以如果你不确定`x`与`y`之间的关系是否线性，你可以使用`s(x)`。如果你看到系数有一个`edf`（有效自由度——参见模型摘要[列表10.14](../Text/10.xhtml#ch10ex14)），那么你可以尝试将变量重新拟合为线性项。
- en: '* * *'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The use of splines gives GAMs a richer model space to choose from; this increased
    flexibility brings a higher risk of overfitting. You should also check the models’
    performances on the test data.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 使用样条曲线为GAM提供了更丰富的模型空间来选择；这种增加的灵活性带来了更高的过拟合风险。你还应该检查模型在测试数据上的性能。
- en: Listing 10.15\. Comparing linear regression and GAM performance
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.15\. 比较线性回归和GAM性能
- en: '[PRE24]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Gets predictions from both models on the test data. The function transform()
    is a base R version of dplyr::mutate().
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从两个模型在测试数据上获取预测值。函数`transform()`是dplyr::mutate()的基础R版本。
- en: ❷ Calculates the residuals
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算残差
- en: ❸ Compares the RMSE of both models on the test data
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在测试数据上比较两个模型的均方根误差（RMSE）
- en: ❹ Compares the R-squared of both models on the test data, using the sigr package
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用sigr包在测试数据上比较两个模型的R平方
- en: 'The GAM performed similarly on both training and test sets: RMSE of 1.40 on
    test versus 1.45 on training; R-squared of 0.78 on test versus 0.83 on training.
    So there’s likely no overfit.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: GAM在训练集和测试集上的表现相似：测试集上的RMSE为1.40，训练集上的RMSE为1.45；测试集上的R平方为0.78，训练集上的R平方为0.83。所以很可能没有过拟合。
- en: 10.2.3\. Extracting the non-linear relationships
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3\. 提取非线性关系
- en: Once you fit a GAM, you’ll probably be interested in what the `s()` functions
    look like. Calling `plot()` on a GAM will give you a plot for each `s()` curve,
    so you can visualize non-linearities. In our example, `plot(gam_model)` produces
    the top curve in [figure 10.16](../Text/10.xhtml#ch10fig16).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拟合了一个广义可加模型（GAM），你可能会对`s()`函数的形状感兴趣。在GAM上调用`plot()`将为你提供每个`s()`曲线的图表，这样你可以可视化非线性。在我们的例子中，`plot(gam_model)`生成了[图10.16](../Text/10.xhtml#ch10fig16)中的顶部曲线。
- en: 'Figure 10.16\. Top: The non-linear function `s(PWGT)` discovered by `gam()`,
    as output by `plot(gam_model)`. Bottom: The same spline superimposed over the
    training data.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16\. 顶部：`gam()`发现的非线性函数`s(PWGT)`，由`plot(gam_model)`输出。底部：相同的样条曲线叠加在训练数据上。
- en: '![](Images/10fig16_alt.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig16_alt.jpg)'
- en: The shape of the curve is quite similar to the scatter plot we saw in [figure
    10.13](../Text/10.xhtml#ch10fig13) (which is reproduced as the lower half of [figure
    10.16](../Text/10.xhtml#ch10fig16)). In fact, the spline that’s superimposed on
    the scatter plot in [figure 10.13](../Text/10.xhtml#ch10fig13) is the same curve.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线的形状与我们看到的[图10.13](../Text/10.xhtml#ch10fig13)（作为[图10.16](../Text/10.xhtml#ch10fig16)的下半部分重现）非常相似。实际上，[图10.13](../Text/10.xhtml#ch10fig13)中散点图上叠加的样条曲线与同一曲线相同。
- en: You can extract the data points that were used to make this graph by using the
    `predict()` function with the argument `type = "terms"`. This produces a matrix
    where the *i*th column represents `s(x[,i])`. The following listing demonstrates
    how to reproduce the lower plot in [figure 10.16](../Text/10.xhtml#ch10fig16).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`predict()`函数并带有`type = "terms"`参数来提取用于制作此图的点。这将产生一个矩阵，其中第*i*列代表`s(x[,i])`。以下列表演示了如何重现[图10.16](../Text/10.xhtml#ch10fig16)中的下部分图。
- en: Listing 10.16\. Extracting a learned spline from a GAM
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.16\. 从GAM中提取学习样条
- en: '[PRE25]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that you’ve worked through a simple example, you are ready to try a more
    realistic example with more variables.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了一个简单的例子，你就可以尝试一个包含更多变量的更现实的例子了。
- en: 10.2.4\. Using GAM on actual data
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4\. 在实际数据上使用GAM
- en: '* * *'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to predict a newborn baby’s weight (*`DBWT`*) from a number
    of variables:*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设你想根据多个变量预测一个新生儿的体重（*`DBWT`*）：*'
- en: '*Mother’s weight (*`PWGT`*)*'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*母亲的体重（`PWGT`*）'
- en: '*Mother’s pregnancy weight gain (*`WTGAIN`*)*'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*母亲的孕期体重增加（`WTGAIN`*）'
- en: '*Mother’s age (*`MAGER`*)*'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*母亲的年龄（`MAGER`*）'
- en: '*The number of prenatal medical visits (*`UPREVIS`*)*'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*产前医疗访问次数（`UPREVIS`*）'
- en: '* * *'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For this example, you’ll use data from the 2010 CDC natality dataset that you
    used in [section 7.2](../Text/07.xhtml#ch07lev1sec2) (though this is not the risk
    data used in that chapter).^([[7](../Text/10.xhtml#ch10fn7)]) Note that we’ve
    chosen this example to highlight the mechanisms of `gam()`, not to find the best
    model for birth weight. Adding other variables beyond the four we’ve chosen will
    improve the fit, but obscure the exposition.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，你将使用2010年CDC出生数据集中你用过的数据[第7.2节](../Text/07.xhtml#ch07lev1sec2)（尽管这不是该章节中使用的风险数据）。^([[7](../Text/10.xhtml#ch10fn7)）请注意，我们选择这个例子是为了突出`gam()`的机制，而不是为了找到最佳出生体重模型。除了我们选择的四个变量之外，添加其他变量将提高拟合度，但会模糊说明。
- en: ⁷
  id: totrans-350
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-351
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The dataset can be found at [https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData](https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData).
    A script for preparing the dataset from the original CDC extract can be found
    at [https://github.com/WinVector/PDSwR2/blob/master/CDC/prepBirthWeightData.R](https://github.com/WinVector/PDSwR2/blob/master/CDC/prepBirthWeightData.R).
  id: totrans-352
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据集可在[https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData](https://github.com/WinVector/PDSwR2/blob/master/CDC/NatalBirthData.rData)找到。从原始CDC提取准备数据集的脚本可在[https://github.com/WinVector/PDSwR2/blob/master/CDC/prepBirthWeightData.R](https://github.com/WinVector/PDSwR2/blob/master/CDC/prepBirthWeightData.R)找到。
- en: In the next listing, you’ll fit a linear model and a GAM, and compare.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，你将拟合一个线性模型和一个GAM，并进行比较。
- en: Listing 10.17\. Applying linear regression (with and without GAM) to health
    data
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.17\. 将线性回归（带GAM和不带GAM）应用于健康数据
- en: '[PRE26]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Builds a linear model with four variables
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用四个变量构建线性模型
- en: ❷ The model explains about 6.6% of the variance; all coefficients are significantly
    different from 0.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 该模型解释了大约6.6%的方差；所有系数均与0有显著差异。
- en: ❸ Builds a GAM with the same variables
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用相同的变量构建GAM
- en: ❹ Verifies that the model has converged
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 验证模型已收敛
- en: ❺ The model explains a little over 9% of the variance; all variables have a
    non-linear effect significantly different from 0.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 模型解释了略超过9%的方差；所有变量都有与0显著不同的非线性效应。
- en: The GAM has improved the fit, and all four variables seem to have a non-linear
    relationship with birth weight, as evidenced by `edf`s all greater than 1\. You
    could use `plot(gammodel)` to examine the shape of the `s()` functions; instead,
    let's compare them with a direct smoothing curve of each variable against mother’s
    weight.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: GAM提高了拟合度，所有四个变量似乎与出生体重有非线性关系，正如`edf`值都大于1所证明的。你可以使用`plot(gammodel)`来检查`s()`函数的形状；相反，让我们将它们与每个变量对母亲体重的直接平滑曲线进行比较。
- en: Listing 10.18\. Plotting GAM results
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.18\. 绘制GAM结果
- en: '[PRE27]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Gets the matrix of s() functions
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取`s()`函数的矩阵
- en: ❷ Binds in the birth weight (DBWT)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绑定出生体重（`DBWT`）
- en: ❸ Shifts all the columns to be zero mean (to make comparisons easy); converts
    to a data frame
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将所有列移至零均值（以便比较）；转换为数据框
- en: ❹ Makes the column names reference-friendly (s(PWGT) is converted to sPWGT,
    etc.)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使列名便于引用（s(PWGT)转换为sPWGT等）
- en: ❺ Binds in the input variables
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 绑定输入变量
- en: ❻ Compares the spline s(PWGT) to the smoothed curve of DBWT (baby’s weight)
    as a function of mother’s weight (PWGT)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将样条s(PWGT)与DBWT（婴儿体重）作为母亲体重（`PWGT`）的函数的平滑曲线进行比较
- en: ❻ Repeats for the remaining variables (omitted for brevity)
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对剩余变量重复操作（为简洁起见省略）
- en: '[Figure 10.17](../Text/10.xhtml#ch10fig17) shows the `s()` splines learned
    by `gam()` as the dotted curves. These splines are `gam()`’s estimate of the (joint)
    relationship between each variable and the outcome, `DBWT`. The sum of the splines
    (plus an offset) is the model’s best estimate of `DBWT` as a function of the input
    variables.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.17](../Text/10.xhtml#ch10fig17)显示了`gam()`学习的`s()`样条曲线，这些曲线以虚线表示。这些样条曲线是`gam()`对每个变量与结果（`DBWT`）之间（联合）关系的估计。样条的总和（加上偏移量）是模型对`DBWT`作为输入变量的函数的最佳估计。'
- en: Figure 10.17\. Smoothing curves of each of the four input variables plotted
    against birth weight, compared with the splines discovered by `gam()`. All curves
    have been shifted to be zero mean for comparison of shape.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17\. 四个输入变量对出生体重的平滑曲线，与`gam()`发现的样条进行比较。所有曲线都已移至零均值，以便比较形状。
- en: '![](Images/10fig17_alt.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig17_alt.jpg)'
- en: The figure also shows the smoothing curves that directly relate each variable
    to `DBWT`. The smooth curves in each case are similar to the corresponding `s()`
    in shape, and non-linear for all the variables. The differences in shape are because
    the splines are fit jointly (which is more useful for modeling), and the smoothing
    curves are merely calculated one at a time.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图表还显示了直接与`DBWT`相关的平滑曲线。每个案例中的平滑曲线在形状上类似于相应的`s()`，并且对所有变量都是非线性的。形状上的差异是因为样条曲线是联合拟合的（这对建模更有用），而平滑曲线是单独计算的。
- en: As usual, you should check for overfit with holdout data.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，您应该使用保留数据来检查过拟合。
- en: Listing 10.19\. Checking GAM model performance on holdout data
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.19\. 在保留数据上检查GAM模型性能
- en: '[PRE28]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Gets predictions from both models on test data
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取测试数据上两个模型的预测
- en: ❷ Gets the residuals
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取残差
- en: ❸ Compares the RMSE of both models on the test data
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在测试数据上比较了两个模型的RMSE
- en: ❹ Compares the R-squared of both models on the test data, using sigr
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在测试数据上比较了两个模型的R-squared值，使用sigr
- en: The performance of the linear model and the GAM were similar on the test set,
    as they were on the training set, so in this example, there’s no substantial overfit.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上，线性模型和GAM的性能相似，与训练集上相似，因此在这个例子中，没有实质性的过拟合。
- en: 10.2.5\. Using GAM for logistic regression
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.5\. 使用GAM进行逻辑回归
- en: The `gam()` function can be used for logistic regression as well.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '`gam()`函数也可以用于逻辑回归。'
- en: '* * *'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*Suppose you want to predict when a baby will be born underweight (defined
    as* `DBWT < 2000`*), using the same input variables as the previous scenario.*'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设您想预测婴儿何时会出生体重不足（定义为`DBWT < 2000`*），使用与先前场景相同的输入变量。*'
- en: '* * *'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The logistic regression call to do this is shown in the following listing.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作的逻辑回归调用在以下列表中显示。
- en: Listing 10.20\. GLM logistic regression
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.20\. GLM逻辑回归
- en: '[PRE29]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The corresponding call to `gam()` also specifies the binomial family with the
    `logit` link.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的`gam()`调用还指定了二项式族和`logit`连接。
- en: Listing 10.21\. GAM logistic regression
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.21\. GAM逻辑回归
- en: '[PRE30]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Note the large p-value associated with mother’s weight (PGWT). That means
    that there’s no statistical proof that the mother’s weight (PWGT) has a significant
    effect on the outcome.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意与母亲体重（PGWT）相关的大p值。这意味着没有统计证据表明母亲的体重（PWGT）对结果有显著影响。
- en: '❷ “Deviance explained” is the pseudo R-squared: 1 - (deviance/null.deviance).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ “偏差解释”是伪R-squared：1 - (偏差/空偏差)。
- en: As with the standard logistic regression call, we recover the class probabilities
    with the call `predict(glogmodel, newdata = train, type = "response")`. Again,
    these models are coming out with low quality, and in practice we would look for
    more explanatory variables to build better screening models.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准逻辑回归调用一样，我们通过调用`predict(glogmodel, newdata = train, type = "response")`恢复类别概率。同样，这些模型的质量较低，在实践中，我们会寻找更多解释变量来构建更好的筛选模型。
- en: 10.2.6\. GAM takeaways
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.6\. GAM要点
- en: 'Here’s what you should remember about GAMs:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GAMs，您应该记住以下几点：
- en: GAMs let you represent non-linear and non-monotonic relationships between variables
    and outcome in a linear or logistic regression framework.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAMs让您能够在线性或逻辑回归框架中表示变量和结果之间的非线性和非单调关系。
- en: In the `mgcv` package, you can extract the discovered relationship from the
    GAM model using the `predict()` function with the `type = "terms"` parameter.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`mgcv`包中，您可以使用`predict()`函数和`type = "terms"`参数从GAM模型中提取发现的关系。
- en: 'You can evaluate the GAM with the same measures you’d use for standard linear
    or logistic regression: residuals, deviance, R-squared, and pseudo R-squared.
    The `gam()` summary also gives you an indication of which variables have a significant
    effect on the model.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用与标准线性或逻辑回归相同的指标来评估GAM：残差、偏差、R-squared和伪R-squared。`gam()`摘要还提供了哪些变量对模型有显著影响的指示。
- en: Because GAMs have increased complexity compared to standard linear or logistic
    regression models, there’s more risk of overfit.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于与标准线性或逻辑回归模型相比，GAMs具有更高的复杂性，因此存在更大的过拟合风险。
- en: GAMs extend linear methods (and generalized linear methods) by allowing variables
    to have non-linear (or even non-monotone) effects on outcome. Another approach
    is to form new variables from non-linear *combinations* of existing variables.
    The data scientist can do this by hand, by adding interactions or new synthetic
    variables, or it can be done mechanically, by support vector machines (SVMs),
    as shown in the next section. The hope is that with access to enough of these
    new variables, your modeling problem becomes easier.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: GAMs通过允许变量对结果产生非线性（甚至非单调）影响来扩展线性方法（以及广义线性方法）。另一种方法是通过对现有变量的非线性*组合*形成新的变量。数据科学家可以通过手动添加交互作用或新的合成变量来完成这项工作，或者可以通过支持向量机（SVMs）机械地完成，如下一节所示。希望有了足够多的这些新变量，你的建模问题就会变得更容易。
- en: 'In the next section, we’ll work with two of the most popular ways to add and
    manage new variables: *kernel methods* and *support vector machines*.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨两种最流行的方法来添加和管理新变量：*核方法*和*支持向量机*。
- en: 10.3\. Solving “inseparable” problems using support vector machines
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3. 使用支持向量机解决“不可分”问题
- en: 'Some classification problems are called *inseparable*: instances of one class,
    A, are inside regions bounded by another class, B, so that class A can’t be separated
    from class B by a flat boundary. For example, in [figure 10.18](../Text/10.xhtml#ch10fig18),
    we see a number of o’s inside a triangle defined by x’s (and we also see the data
    converted to a nice separable arrangement by a so-called kernel function `phi()`).
    The original arrangement on the left side is *linearly inseparable*: there is
    no hyperplane that separates the x’s from the o’s. Hence, it would be impossible
    for linear methods to completely separate the two classes. We could use the tree-based
    methods demonstrated in [section 10.1](../Text/10.xhtml#ch10lev1sec1) to fit a
    classifier, or we could use a technique called *kernel methods*. In this section,
    we will use SVMs and kernel methods to build good classifiers on linearly inseparable
    data.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分类问题被称为*不可分*：类A的实例位于由类B定义的区域内，因此类A不能通过一个平坦的边界与类B分开。例如，在[图10.18](../Text/10.xhtml#ch10fig18)中，我们看到许多o位于由x定义的三角形内部（我们也看到数据通过所谓的核函数`phi()`转换成了一个很好的可分离排列）。左侧的原始排列是*线性不可分*：没有超平面可以将x与o分开。因此，线性方法完全分离这两个类是不可能的。我们可以使用[第10.1节](../Text/10.xhtml#ch10lev1sec1)中展示的基于树的模型来拟合分类器，或者我们可以使用一种称为*核方法*的技术。在本节中，我们将使用SVMs和核方法在线性不可分数据上构建良好的分类器。
- en: Figure 10.18\. Notional illustration of a kernel transform (based on Cristianini
    and Shawe-Taylor, 2000)
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18. 核变换的概念图（基于Cristianini和Shawe-Taylor，2000）
- en: '![](Images/10fig18_alt.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig18_alt.jpg)'
- en: '* * *'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Having more than one way to do things**'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**有多种方法可以完成事情**'
- en: 'At this point, we have seen a number of advanced methods that give us more
    than one way to handle complex problems. For example: random forests, boosting,
    and SVMs can all introduce variable interactions to solve problems. It would be
    nice if there were always one obvious best method. However, each of these methodologies
    can dominate for different problems. So there is no one best method.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经看到了许多高级方法，它们为我们提供了处理复杂问题的多种方式。例如：随机森林、提升和SVMs都可以引入变量交互来解决问题。如果总有一种明显最好的方法那会很好。然而，每种方法对于不同的问题都可能占主导地位。因此，没有一种最好的方法。
- en: Our advice is try simple methods such as linear and logistic regression first.
    Then bring in and try advanced methods such as GAMs (which can handle single-variable
    reshaping), tree-based methods (which can handle many-variable interactions),
    and SVMs (which can handle many-variable reshapings) to address modeling issues.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的建议是首先尝试简单的方法，如线性回归和逻辑回归。然后引入并尝试高级方法，如GAMs（可以处理单变量重塑）、基于树的模型（可以处理多变量交互）和SVMs（可以处理多变量重塑）来解决建模问题。
- en: '* * *'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 10.3.1\. Using an SVM to solve a problem
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1. 使用SVM解决问题
- en: Let’s start with an example adapted from R’s `kernlab` library documentation.
    Learning to separate two spirals is a famous “impossible” problem that cannot
    be solved by linear methods (though it is solvable by spectral clustering, kernel
    methods, SVMs, and deep learning or deep neural nets).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从R的`kernlab`库文档中改编的一个例子开始。学习分离两个螺旋是一个著名的“不可能”问题，线性方法无法解决（尽管可以通过谱聚类、核方法、SVMs、深度学习或深度神经网络来解决）。
- en: '* * *'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Example
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '*[Figure 10.19](../Text/10.xhtml#ch10fig19) shows two spirals, one within the
    other. Your task is to build a decision procedure that cuts up the plane such
    that the 1-labeled examples are in one region and the 2-labeled examples are in
    the complimentary region.*^([[8](../Text/10.xhtml#ch10fn8)])'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '*[图10.19](../Text/10.xhtml#ch10fig19)显示了两个螺旋，一个在另一个内部。你的任务是构建一个决策过程，将平面切割成两个区域，使得1标记的例子在一个区域，2标记的例子在互补区域。*^([[8](../Text/10.xhtml#ch10fn8)])'
- en: ⁸
  id: totrans-420
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-421
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See K. J. Lang and M. J. Witbrock, “Learning to tell two spirals apart” in Proceedings
    of the 1988 Connectionist Models Summer School, D. Touretzky, G. Hinton, and T.
    Sejnowski (eds), Morgan Kaufmann, 1988 (pp. 52–59).
  id: totrans-422
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见K. J. Lang和M. J. Witbrock在1988年连接主义模型夏季学校的论文“学会区分两个螺旋”，D. Touretzky, G. Hinton,
    和T. Sejnowski（编），Morgan Kaufmann，1988（第52-59页）。
- en: '* * *'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Support vector machines excel at learning concepts of the form “examples that
    are near each other should be given the same classification.” To use the SVM technique,
    the user must choose a kernel (to control what is considered "near" or "far"),
    and pick a value for a hyperparameter called `C` or `nu` (to try to control model
    complexity).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机在学习和“相邻的例子应该给予相同的分类”形式的概念方面表现出色。为了使用SVM技术，用户必须选择一个核（以控制“近”或“远”的定义），并选择一个超参数`C`或`nu`的值（以尝试控制模型复杂度）。
- en: Spiral example
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 螺旋示例
- en: '[Listing 10.22](../Text/10.xhtml#ch10ex22) shows the recovery and labeling
    of the two spirals shown in [figure 10.19](../Text/10.xhtml#ch10fig19). You will
    use the labeled data for the example task: given the labeled data, recover the
    1 versus 2 regions by supervised machine learning.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.22](../Text/10.xhtml#ch10ex22) 展示了[图10.19](../Text/10.xhtml#ch10fig19)中显示的两个螺旋的恢复和标记。你将使用标记的数据进行示例任务：给定标记数据，通过监督机器学习恢复1与2的区域。'
- en: Figure 10.19\. The spiral counterexample
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19\. 螺旋反例
- en: '![](Images/10fig19_alt.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig19_alt.jpg)'
- en: Listing 10.22\. Setting up the spirals data as a classification problem
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.22\. 将螺旋数据设置为分类问题
- en: '[PRE31]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Loads the kernlab kernel and SVM package and then asks that the included example
    spirals be made available
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载kernlab核和SVM包，并要求提供包含的示例螺旋
- en: ❷ Uses kernlab’s spectral clustering routine to identify the two different spirals
    in the example dataset
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用kernlab的谱聚类程序识别示例数据集中的两个不同螺旋
- en: ❸ Combines the spiral coordinates and the spiral label into a data frame
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将螺旋坐标和螺旋标签合并到一个数据框中
- en: ❹ Plots the spirals with class labels
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制带有类别标签的螺旋
- en: '[Figure 10.19](../Text/10.xhtml#ch10fig19) shows the labeled spiral dataset.
    Two classes (represented by digits) of data are arranged in two interwoven spirals.
    This dataset is difficult for methods that don’t have a rich enough concept space
    (perceptrons, shallow neural nets) and easy for more-sophisticated learners that
    can introduce the right new features. Support vector machines, with the right
    kernel, are a way to introduce new composite features in order to solve the problem.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.19](../Text/10.xhtml#ch10fig19) 展示了标记的螺旋数据集。两种数据类别（用数字表示）被安排在两个交织的螺旋中。这个数据集对于没有足够丰富概念空间（感知器、浅层神经网络）的方法来说很难，但对于可以引入正确新特征的更复杂的学习者来说很容易。使用正确核的支持向量机是一种引入新组合特征以解决问题的方式。'
- en: Support vector machines with an oversimple kernel
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 使用过简单核的支持向量机
- en: Support vector machines are powerful, but without the correct kernel, they have
    difficulty with some concepts (such as the spiral example). [Listing 10.23](../Text/10.xhtml#ch10ex23)
    shows a failed attempt to learn the spiral concept with an SVM using the identity
    or dot-product (linear) kernel. The linear kernel does no transformations on the
    data; it can work for some applications, but in this case it does not give us
    the data-separating properties we want.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机功能强大，但没有正确的核，它们在处理某些概念（如螺旋示例）时会有困难。[列表10.23](../Text/10.xhtml#ch10ex23)
    展示了使用恒等或点积（线性）核尝试使用SVM学习螺旋概念的失败尝试。线性核不对数据进行变换；它可以适用于某些应用，但在这个情况下，它没有给我们想要的数据分离特性。
- en: Listing 10.23\. SVM with a poor choice of kernel
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.23\. 使用核选择不当的SVM
- en: '[PRE32]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Prepares to try to learn spiral class label from coordinates using an SVM
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备使用SVM从坐标中学习螺旋类别标签
- en: ❷ Builds the support vector model using a vanilladot kernel (not a very good
    kernel)
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用vanilladot核（不是一个很好的核）构建支持向量模型
- en: ❸ Uses the model to predict class on held-out data
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用模型对保留数据预测类别
- en: ❹ Calls the model on a grid of points to generate background shading indicating
    the learned concept
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在点网格上调用模型以生成表示学习概念的背景阴影
- en: ❺ Plots the predictions on top of a grey copy of all the data so we can see
    if predictions agree with the original markings
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在所有数据的灰色副本上绘制预测，以便我们可以看到预测是否与原始标记一致
- en: This attempt results in [figure 10.20](../Text/10.xhtml#ch10fig20). The figure
    shows the total dataset in a small font and the SVM classifications of the test
    dataset in large text. It also indicates the learned concept by shading. The SVM
    didn’t produce a good model with the identity kernel, as it was forced to pick
    a linear separator. In the next section, you’ll repeat the process with the Gaussian
    radial kernel and get a much better result.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 这次尝试的结果是[图10.20](../Text/10.xhtml#ch10fig20)。图中以小字体显示了整个数据集，以大字体显示了测试数据集的SVM分类。它还通过阴影指示了学习到的概念。SVM没有使用标识核产生一个好的模型，因为它被迫选择线性分离器。在下文中，你将使用高斯径向核重复这个过程，并得到更好的结果。
- en: Figure 10.20\. Identity kernel failing to learn the spiral concept
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.20\. 标识核无法学习螺旋概念
- en: '![](Images/10fig20_alt.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig20_alt.jpg)'
- en: Support vector machines with a good kernel
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 具有良好核的支持向量机
- en: In [listing 10.24](../Text/10.xhtml#ch10ex24), you’ll repeat the SVM fitting
    process, but this time specifying the Gaussian or radial kernel. [Figure 10.21](../Text/10.xhtml#ch10fig21)
    again plots the SVM test classifications in black (with the entire dataset in
    a smaller font). Note that this time the algorithm correctly learned the actual
    spiral concept, as indicated by the shading.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.24](../Text/10.xhtml#ch10ex24)中，你将重复SVM拟合过程，但这次指定高斯或径向核。[图10.21](../Text/10.xhtml#ch10fig21)再次以黑色（整个数据集以较小的字体显示）绘制了SVM测试分类。注意这次算法正确地学习了实际的螺旋概念，如阴影所示。
- en: Figure 10.21\. Radial kernel successfully learning the spiral concept
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.21\. 径向核成功学习螺旋概念
- en: '![](Images/10fig21_alt.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig21_alt.jpg)'
- en: Listing 10.24\. SVM with a good choice of kernel
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.24\. 选择良好核的SVM
- en: '[PRE33]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ This time uses the “radial” or Gaussian kernel, which is a nice geometric
    distance measure
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这次使用“径向”或高斯核，这是一个很好的几何距离度量
- en: '* * *'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Exercise: Try to use xgboost to solve the spirals problem.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 练习：尝试使用xgboost解决螺旋问题。
- en: '*As we stated, some methods work better on some problems than others. Try to
    use the* `xgboost` *package to solve the spirals problem. Do you find the* `xgboost`
    *results to be better or worse than the SVM results? (A worked version of this
    example can be found here: [https://github.com/WinVector/PDSwR2/tree/master/Spirals](https://github.com/WinVector/PDSwR2/tree/master/Spirals).)*'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '*正如我们所说的，一些方法在某些问题上比其他方法更有效。尝试使用* `xgboost` *包解决螺旋问题。你发现`xgboost`的结果比SVM的结果更好还是更差？（这个示例的工作版本可以在以下位置找到：[https://github.com/WinVector/PDSwR2/tree/master/Spirals](https://github.com/WinVector/PDSwR2/tree/master/Spirals)。)*'
- en: '* * *'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 10.3.2\. Understanding support vector machines
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2\. 理解支持向量机
- en: 'An SVM is often portrayed as a magic machine that makes classification easier.^([[9](../Text/10.xhtml#ch10fn9)])
    To dispel the awe and be able to use support vector methods with confidence, we
    need to take some time to learn their principles and how they work. The intuition
    is this: *SVMs with the radial kernel are very good nearest-neighbor-style classifiers.*'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机通常被描绘成一个使分类变得更容易的魔法机器。[9](../Text/10.xhtml#ch10fn9) 为了消除敬畏并能够自信地使用支持向量方法，我们需要花些时间学习它们的原则和它们是如何工作的。直觉是这样的：*具有径向核的支持向量机是非常好的近邻式分类器*。
- en: ⁹
  id: totrans-461
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-462
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Support vector machines can also be used for regression, but we will not cover
    that here.
  id: totrans-463
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 支持向量机也可以用于回归，但这里我们不会涉及。
- en: In [figure 10.22](../Text/10.xhtml#ch10fig22), in the “real space” (on the left),
    the data is separated by a non-linear boundary. When the data is lifted into the
    higher-dimensional kernel space (on the right), the lifted points are separated
    by a hyperplane. Let’s call the normal to that hyperplane `w` and the offset from
    the origin `b` (not shown).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图10.22](../Text/10.xhtml#ch10fig22)中，在“真实空间”（左侧），数据通过非线性边界分离。当数据提升到更高维的核空间（右侧）时，提升的点通过超平面分离。让我们称该超平面的法线为`w`，从原点偏移为`b`（未显示）。
- en: Figure 10.22\. Notional illustration of SVM
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.22\. SVM的概念性说明
- en: '![](Images/10fig22_alt.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/10fig22_alt.jpg)'
- en: An SVM finds a linear decision function (determined by parameters `w` and `b`),
    where for a given example `x` the machine decides `x` is in the class if
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: SVM找到一个线性决策函数（由参数`w`和`b`确定），对于给定的示例`x`，机器决定`x`属于该类，如果
- en: '[PRE34]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: for some `w` and `b`, and not in the class otherwise. The model is completely
    determined by the function `phi()`, the vector `w`, and the scalar offset `b`.
    The idea is that `phi()` lifts or reshapes the data into a nicer space (where
    things are linearly separable), and then the SVM finds a linear boundary separating
    the two data classes in this new space (represented by `w` and `b`). This linear
    boundary in the lifted space can be pulled back as a general curved boundary in
    the original space. The principle is sketched out in [figure 10.22](../Text/10.xhtml#ch10fig22).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些`w`和`b`，否则不在该类中。该模型完全由函数`phi()`、向量`w`和标量偏移`b`确定。其想法是`phi()`将数据提升或重塑到更合适的空间（其中事物是线性可分的），然后SVM在这个新空间中找到分离两个数据类的线性边界（由`w`和`b`表示）。这个提升空间中的线性边界可以拉回到原始空间中的通用曲线边界。这个原理在[图10.22](../Text/10.xhtml#ch10fig22)中进行了概述。
- en: The support vector training operation finds `w` and `b`. There are variations
    on the SVM that make decisions between more than two classes, perform scoring/regression,
    and detect novelty. But we’ll discuss only the SVMs for simple classification.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量训练操作找到`w`和`b`。SVM有变体可以在多于两个类别之间做出决策，执行评分/回归，并检测新颖性。但我们将只讨论用于简单分类的SVM。
- en: As a user of SVMs, you don’t immediately need to know how the training procedure
    works; that’s what the software does for you. But you do need to have some notion
    of what it’s trying to do. The model `w,b` is ideally picked so that
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 作为SVM的用户，你不必立即了解训练过程是如何工作的；这是软件为你做的。但你确实需要有一些关于它试图做什么的概念。模型`w,b`理想地选择，以便
- en: '[PRE35]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: for all training `x`s that were in the class, and
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有在类中的训练`x`。
- en: '[PRE36]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: for all training examples not in the class.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有不在该类中的训练示例。
- en: The data is called *separable* if `u > v`. The size of the separation is `(u
    - v) / sqrt(w %*% w)`, and is called the *margin*. The goal of the SVM optimizer
    is to maximize the margin. A large margin can actually ensure good behavior on
    future data (good generalization performance). In practice, real data isn’t always
    separable even in the presence of a kernel. To work around this, most SVM implementations
    implement the so-called *soft margin* optimization goal.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`u > v`，则数据被称为*可分*。分离的大小是`(u - v) / sqrt(w %*% w)`，称为*间隔*。SVM优化器的目标是最大化间隔。实际上，较大的间隔可以确保对未来数据的良好行为（良好的泛化性能）。在实践中，即使存在核，真实数据也不总是可分的。为了解决这个问题，大多数SVM实现都实现了所谓的*软间隔*优化目标。
- en: 'A soft margin optimizer adds additional error terms that are used to allow
    a limited fraction of the training examples to be on the wrong side of the decision
    surface.^([[10](../Text/10.xhtml#ch10fn10)]) The model doesn’t actually perform
    well on the altered training examples, but trades the error on these examples
    against increased margin on the remaining training examples. For most implementations,
    the model hyperparameter `C` or `nu` determines the trade-off between margin width
    for the remaining data and how much data is pushed around to achieve the margin.
    We will use the `nu` hyperparameter. `nu` takes settings between zero and one;
    lower values allow fewer training misclassifications, favoring more-complex models
    (more support vectors).^([[11](../Text/10.xhtml#ch10fn11)]) For our example, we
    will just use the function default value: `0.5`.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 软间隔优化器添加额外的误差项，这些误差项用于允许有限比例的训练示例位于决策表面的错误一侧。[10](../Text/10.xhtml#ch10fn10)]
    模型实际上并不擅长处理这些修改后的训练示例，但它将这些示例上的误差与剩余训练示例上的间隔增加进行权衡。对于大多数实现，模型超参数`C`或`nu`决定了剩余数据的间隔宽度和为了实现间隔而推来推去的数据量之间的权衡。我们将使用`nu`超参数。`nu`的设置在零和一之间；较低的值允许更少的训练错误分类，有利于更复杂的模型（更多的支持向量）。[11](../Text/10.xhtml#ch10fn11)]
    对于我们的示例，我们将只使用函数默认值：`0.5`。
- en: ^(10)
  id: totrans-478
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([10](https://example.org))
- en: ''
  id: totrans-479
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A common type of dataset that is inseparable under any kernel is a dataset where
    there are at least two examples belonging to different outcome classes with the
    exact same values for all input or `x` variables. The original “hard margin” SVM
    couldn’t deal with this sort of data and was for that reason not considered to
    be practical.
  id: totrans-480
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在任何核下都不可分的一种常见数据集是至少有两个示例属于不同结果类，并且所有输入或`x`变量的值都完全相同的数据集。原始的“硬间隔”SVM无法处理这类数据，因此被认为不实用。
- en: ^(11)
  id: totrans-481
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([11](https://example.org))
- en: ''
  id: totrans-482
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For more details on SVMs, we recommend Cristianini and Shawe-Taylor’s *An Introduction
    to Support Vector Machines and Other Kernel-based Learning Methods*, Cambridge
    University Press, 2000.
  id: totrans-483
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于 SVM 的更多详细信息，我们推荐 Cristianini 和 Shawe-Taylor 的 *支持向量机及其他基于核的学习方法导论*，剑桥大学出版社，2000年。
- en: 10.3.3\. Understanding kernel functions
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3\. 理解核函数
- en: The SVM picks which data is unimportant (left out) and which is very important
    (used as support vectors). But the reshaping of the problem to make the data separable
    is actually performed by what are called *kernel methods* or *kernel functions*.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 选择哪些数据是不重要的（被排除在外）以及哪些数据非常重要（用作支持向量）。但实际上，将问题重塑以使数据可分是由所谓的 *核方法* 或 *核函数*
    来执行的。
- en: '[Figure 10.22](../Text/10.xhtml#ch10fig22) illustrates^([[12](../Text/10.xhtml#ch10fn12)])
    what we hope for from a good kernel: our data being pushed around so it’s easier
    to sort or classify. By using a kernel transformation, we move to a situation
    where the distinction we’re trying to learn is representable by a linear separator
    of our transformed data.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.22](../Text/10.xhtml#ch10fig22) 展示了^([[12](../Text/10.xhtml#ch10fn12)])
    我们希望从好的核函数中得到的结果：我们的数据被推来推去，使其更容易排序或分类。通过使用核变换，我们进入一个这样的状态，即我们试图学习的区别可以通过我们变换后的数据的线性分离器来表示。'
- en: ^(12)
  id: totrans-487
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-488
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cristianini and Shawe-Taylor, *An Introduction to Support Vector Machines and
    Other Kernel-based Learning Methods*.
  id: totrans-489
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Cristianini 和 Shawe-Taylor，*支持向量机及其他基于核的学习方法导论*。
- en: To begin to understand SVMs, we need to take a quick look at the common math
    and terminology that a user of SVMs and kernel methods should be conversant with.
    First is the notion of a kernel function, which is used to implement the `phi()`
    we saw reshaping space.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始理解 SVM，我们需要快速查看 SVM 和核方法用户应该熟悉的常见数学和术语。首先是核函数的概念，它用于实现我们看到的重塑空间的 `phi()`。
- en: Formal definition of a kernel function
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数的正式定义
- en: In our application, a kernel is a function with a very specific definition.
    Let `u` and `v` be any pair of variables. `u` and `v` are typically vectors of
    input or independent variables (possibly taken from two rows of a dataset). A
    function `k(,)` that maps pairs `(u,v)` to numbers is called a *kernel function*
    if and only if there is some function `phi()` mapping `(u,v)`s to a vector space
    such that `k(u,v) = phi(u) %*% phi(v)` for all `u,v`.^([[13](../Text/10.xhtml#ch10fn13)])
    We’ll informally call the expression `k(u,v) = phi(u) %*% phi(v)` the *Mercer
    expansion of the kernel* (in reference to Mercer’s theorem; see [http://mng.bz/xFD2](http://mng.bz/xFD2))
    and consider `phi()` the certificate that tells us `k(,)` is a good kernel. This
    is much easier to understand from a concrete example. In the following listing,
    we show an equivalent `phi()` / `k(,)` pair.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用中，核是一个具有非常特定定义的函数。令 `u` 和 `v` 为任意一对变量。`u` 和 `v` 通常是从数据集的两行中选取的输入或独立变量的向量（可能是）。一个将
    `(u,v)` 对映射到数字的函数 `k(,)` 被称为 *核函数*，当且仅当存在一个将 `(u,v)` 映射到向量空间的函数 `phi()`，使得对于所有
    `u,v`，有 `k(u,v) = phi(u) %*% phi(v)`。^([[13](../Text/10.xhtml#ch10fn13)]) 我们将非正式地将表达式
    `k(u,v) = phi(u) %*% phi(v)` 称为核的 *Mercer 展开*（参考Mercer定理；见 [http://mng.bz/xFD2](http://mng.bz/xFD2)），并将
    `phi()` 视为告诉我们 `k(,)` 是一个好的核的证明。这可以通过具体的例子更容易理解。在下面的列表中，我们展示了等价的 `phi()` / `k(,)`
    对。
- en: ^(13)
  id: totrans-493
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(13)
- en: ''
  id: totrans-494
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`%*%` is R’s notation for dot product or inner product; see `help(''%*%'')`
    for details. Note that `phi()` is allowed to map to very large (and even infinite)
    vector spaces.'
  id: totrans-495
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`%*%` 是 R 的点积或内积的表示；有关详细信息，请参阅 `help(''%*%'')`。请注意，`phi()` 可以映射到非常大的（甚至无限的）向量空间。'
- en: Listing 10.25\. An artificial kernel example
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.25\. 一个人工核函数示例
- en: '[PRE37]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Defines a function of two vector variables (both two dimensional) as the sum
    of various products of terms
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个关于两个向量变量（都是二维的）的函数，作为各种项乘积的总和
- en: ❷ Defines a function of a single vector variable that returns a vector containing
    the original entries plus all products of entries
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个关于单个向量变量的函数，该函数返回一个包含原始条目以及所有条目乘积的向量
- en: ❸ Example evaluation of k (,)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ k(,) 的示例评估
- en: ❹ Confirms phi() agrees with k(,). phi() is the certificate that shows k(,)
    is in fact a kernel.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 确认 phi() 与 k(,) 一致。phi() 是一个证明，表明 k(,) 实际上是一个核函数。
- en: Most kernel methods use the function `k(,)` directly and only use properties
    of `k(,)` guaranteed by the matching `phi()` to ensure method correctness. The
    `k(,)` function is usually quicker to compute than the notional function `phi()`.
    A simple example of this is what we’ll call the *dot-product similarity* of documents.
    The dot-product document similarity is defined as the dot product of two vectors
    where each vector is derived from a document by building a huge vector of indicators,
    one for each possible feature. For instance, if the features you’re considering
    are word pairs, then for every pair of words in a given dictionary, the document
    gets a feature of 1 if the pair occurs as a consecutive utterance in the document
    and 0 if not. This method is the `phi()`, but in practice we never use the `phi()`
    procedure. Instead, when comparing two documents, each consecutive pair of words
    in one document is generated and a bit of score is added if this pair is both
    in the dictionary and found consecutively in the other document. For moderate-sized
    documents and large dictionaries, this direct `k(,)` implementation is vastly
    more efficient than the `phi()` implementation.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数核方法直接使用函数 `k(,)`，并且只使用由匹配的 `phi()` 保证的 `k(,)` 的性质来确保方法正确性。`k(,)` 函数通常比理论上的函数
    `phi()` 更快计算。一个简单的例子是我们所说的文档的 *点积相似性*。点积文档相似性定义为两个向量的点积，其中每个向量都是通过构建一个由指示器组成的大向量从文档中导出的，每个指示器对应一个可能的特征。例如，如果你考虑的特征是词对，那么对于给定字典中的每一对单词，如果这对单词在文档中作为连续的表述出现，则文档获得特征值
    1，如果没有出现，则获得 0。这种方法是 `phi()`，但在实践中我们从不使用 `phi()` 程序。相反，当比较两个文档时，一个文档中连续的每一对单词都会生成，如果这对单词在字典中并且也在另一个文档中连续出现，则会添加一些分数。对于中等大小的文档和大型字典，这种直接的
    `k(,)` 实现比 `phi()` 实现效率高得多。
- en: The support vectors
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量
- en: 'The support vector machine gets its name from how the vector `w` is usually
    represented: as a linear combination of training examples—the support vectors.
    Recall we said in [section 10.3.3](../Text/10.xhtml#ch10lev2sec14) that the function
    `phi()` is allowed, in principle, to map into a very large or even infinite vector
    space. This means it may not be possible to directly write down `w`.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机得名于向量 `w` 通常的表示方式：作为训练示例的线性组合——支持向量。回想一下，我们在 [第 10.3.3 节](../Text/10.xhtml#ch10lev2sec14)
    中提到，函数 `phi()` 原则上可以映射到一个非常大的或甚至是无限的向量空间。这意味着可能无法直接写出 `w`。
- en: 'Support vector machines work around the “can’t write down `w`” issue by restricting
    to `w`s that are in principle a sum of `phi()` terms as shown here:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机通过限制 `w` 为原则上为 `phi()` 项之和来绕过“无法写出 `w`”的问题，如下所示：
- en: '[PRE38]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The vectors `s1, ..., sm` are actually `m` training examples and are called
    the support vectors. The preceding formulation helps because such sums are (with
    some math) equivalent to sums of `k( ,x)` kernel terms of the form we show next:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 `s1, ..., sm` 实际上是 `m` 个训练示例，被称为支持向量。前面的公式之所以有用，是因为这样的和（通过一些数学运算）等同于我们接下来展示的形式的
    `k( ,x)` 内核项的和：
- en: '[PRE39]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The right side is a quantity we can compute.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 右边是一个我们可以计算的数量。
- en: The work of the support vector training algorithm is to pick the vectors `s1,
    ..., sm`, the scalars `a1, ..., am`, and the offset `b`. All of this is called
    “the kernel trick.”
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量训练算法的工作是选择向量 `s1, ..., sm`，标量 `a1, ..., am`，以及偏移 `b`。所有这些都称为“核技巧”。
- en: '* * *'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: What to remember about a support vector model
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 关于支持向量模型需要记住的内容
- en: 'A support vector model consists of these things:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量模型由以下这些组成：
- en: A *kernel* `phi()` that reshapes space (chosen by the user)
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *核* `phi()`，它重塑空间（由用户选择）
- en: A subset of training data examples, called the *support vectors* (chosen by
    the SVM algorithm)
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组称为 *支持向量* 的训练数据示例（由 SVM 算法选择）
- en: A set of scalars `a1, ..., am` that specify what linear combination of the support
    vectors define the separating surface (chosen by the SVM algorithm)
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组标量 `a1, ..., am`，这些标量指定了支持向量定义的分离表面的线性组合（由 SVM 算法选择）
- en: A scalar threshold `b` we compare to (chosen by the SVM algorithm)
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个与之一比较的标量阈值 `b`（由 SVM 算法选择）
- en: '* * *'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The reason why the data scientist must be aware of the support vectors is that
    they’re stored in the support vector model. For example, with too complex a model,
    there can be a very large number of support vectors, causing the model to be large
    and expensive to evaluate. In the worst case, the number of support vectors in
    the model can be almost as large as the number of training examples, making support
    vector model evaluation potentially as expensive as nearest-neighbor evaluation,
    and increasing the risk of overfit. The user picks a good number of support vectors
    by picking a good value of `C` or `nu` through cross-validation.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家必须了解支持向量的原因在于它们存储在支持向量模型中。例如，如果模型过于复杂，可能会有非常多的支持向量，导致模型变得很大且评估成本高昂。在最坏的情况下，模型中的支持向量数量几乎可以与训练示例的数量相当，使得支持向量模型的评估可能像最近邻评估一样昂贵，并增加了过拟合的风险。用户通过交叉验证选择一个良好的
    `C` 或 `nu` 值来选择一个良好的支持向量数量。
- en: '* * *'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Exercise: Try different values of nu on the spirals problem.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 练习：在螺旋问题中尝试不同的 `nu` 值。
- en: '*`nu` is the important hyperparameter for SVMs. Ideally, we should cross-validate
    for a good value of* `nu`*. Instead of full cross-validation, just try a few values
    of* `nu` *to get the landscape. (We have a worked solution here: [https://github.com/WinVector/PDSwR2/tree/master/Spirals](https://github.com/WinVector/PDSwR2/tree/master/Spirals).)*'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '*`nu` 是 SVM 的重要超参数。理想情况下，我们应该对 `nu` 的一个良好值进行交叉验证。而不是进行完整的交叉验证，只需尝试几个 `nu` 的值来获取景观。（我们在这里有一个解决方案：[https://github.com/WinVector/PDSwR2/tree/master/Spirals](https://github.com/WinVector/PDSwR2/tree/master/Spirals)）。*'
- en: '* * *'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 10.3.4\. Support vector machine and kernel methods takeaways
  id: totrans-524
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.4\. 支持向量机和核方法要点
- en: 'Here’s what you should remember from this section:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你应该记住的这部分内容：
- en: Support vector machines are a kernel-based classification approach where a complex
    separating surface is parameterized in terms of a (possibly very large) subset
    of the training examples (called the support vectors).
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机是一种基于核的分类方法，其中复杂的分离表面是通过训练示例的（可能非常大的）子集（称为支持向量）来参数化的。
- en: The goal of “the kernel trick” is to lift the data into a space where the data
    is separable, or where linear methods can be used directly. Support vector machines
    and kernel methods work best when the problem has a moderate number of variables
    and the data scientist suspects that the relation to be modeled is a non-linear
    combination of variable effects.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “核技巧”的目标是将数据提升到一个数据可分的空间，或者可以直接使用线性方法的空间。当问题具有适中的变量数量，并且数据科学家怀疑要建模的关系是变量效应的非线性组合时，支持向量机和核方法效果最佳。
- en: Summary
  id: totrans-528
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we demonstrated some advanced methods to fix specific issues
    with basic modeling approaches: modeling variance, modeling bias, issues with
    non-linearity, and issues with variable interactions. An important additional
    family of methods we wish we had time to touch on is *deep learning*, the improved
    modern treatment of neural nets. Fortunately there is already a good book we can
    recommend on this topic: *Deep Learning with R*, by François Chollet with J. J.
    Allaire, Manning, 2018.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了处理基本建模方法特定问题的某些高级方法：建模方差、建模偏差、非线性问题和变量交互问题。我们希望有时间触及的一个重要附加方法系列是 *深度学习*，这是神经网络现代改进处理。幸运的是，已经有了一本关于这个主题的好书可以推荐：*使用
    R 进行深度学习*，作者为 François Chollet 和 J. J. Allaire，Manning，2018 年。
- en: You should understand that you bring in advanced methods and techniques to fix
    specific modeling problems, not because they have exotic names or exciting histories.
    We also feel you should at least try to find an existing technique to fix a problem
    you suspect is hiding in your data *before* building your own custom technique;
    often the existing technique already incorporates a lot of tuning and wisdom.
    Which method is best depends on the data, and there are many advanced methods
    to try. Advanced methods can help fix overfit, variable interactions, non-additive
    relations, and unbalanced distributions, but not lack of features or data.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该理解，你引入高级方法和技术来解决特定的建模问题，并不是因为它们有异国情调的名字或激动人心的历史。我们也认为，在构建自己的定制技术之前，你应该至少尝试找到一种现有的技术来解决你怀疑隐藏在数据中的问题；通常，现有的技术已经包含了大量的调整和智慧。哪种方法最好取决于数据，并且有众多高级方法可以尝试。高级方法可以帮助解决过拟合、变量交互、非加性关系和不平衡分布，但不能解决特征或数据不足的问题。
- en: Finally, the goal of learning the theory of advanced techniques is not to be
    able to recite the steps of the common implementations, but to know when the techniques
    apply and what trade-offs they represent. The data scientist needs to supply thought
    and judgment and realize that the platform can supply implementations.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，学习高级技术理论的目标不是能够背诵常见实现的步骤，而是要知道何时应用这些技术以及它们代表了什么权衡。数据科学家需要提供思考和判断，并认识到平台可以提供实现。
- en: In this chapter you have learned
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了
- en: How to bag decision trees to stabilize their models and improve prediction performance
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过捆绑决策树来稳定模型并提高预测性能
- en: How to further improve decision-tree-based models by using random forests or
    gradient boosting
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过使用随机森林或梯度提升进一步改进基于决策树的模型
- en: How to use random forest variable importances to help with variable selection
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用随机森林变量重要性来帮助进行变量选择
- en: How to use generalized additive models to better model non-linear relationships
    between inputs and outputs in the context of linear and logistic regression
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用广义加性模型在线性回归和逻辑回归的背景下更好地建模输入和输出之间的非线性关系
- en: How to use support vector machines with the Gaussian kernel to model classification
    tasks with complex decision surfaces, especially nearest-neighbor-style tasks.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用高斯核支持向量机来对具有复杂决策表面的分类任务进行建模，特别是最近邻风格的任务。
- en: The actual point of a modeling project is to deliver results for production
    deployment and to present useful documentation and evaluations to your partners.
    The next part of this book will address best practices for delivering your results.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 建模项目的实际目的是为生产部署提供结果，并向您的合作伙伴提供有用的文档和评估。本书的下一部分将介绍交付您结果的最佳实践。
