- en: 8 Bayesian neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 贝叶斯神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Two approaches to fit Bayesian neural networks (BNNs)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适配贝叶斯神经网络（BNNs）的两种方法
- en: The variational inference (VI) approximation for BNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络（BNNs）的变分推断（VI）近似
- en: The Monte Carlo (MC) dropout approximation for BNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络（BNNs）的蒙特卡洛（MC）dropout近似
- en: TensorFlow Probability (TFP) variational layers to build VI-based BNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Probability（TFP）的变分层来构建基于VI的BNNs
- en: Using Keras to implement MC dropout in BNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras在BNNs中实现MC dropout
- en: '![](../Images/8-unnumb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-unnumb.png)'
- en: 'In this chapter, you learn about two efficient approximation methods that allow
    you to use a Bayesian approach for probabilistic DL models: variational inference
    (VI) and Monte Carlo dropout (also known as MC dropout). When setting up a Bayesian
    DL model, you combine Bayesian statistics with DL. (In the figure at the beginning
    of this chapter, you see a combination portrait of Reverend Thomas Bayes, the
    founder of Bayesian Statistics, and Geoffrey Hinton, the leader and one of the
    godfathers of DL.) With these approximation methods, fitting Bayesian DL models
    with many parameters becomes feasible. As discussed in chapter 7, the Bayesian
    method also takes care of the epistemic uncertainty that’s not included in non-Bayesian
    probabilistic DL models.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解两种高效的近似方法，这些方法允许你使用贝叶斯方法对概率深度学习模型进行建模：变分推断（VI）和蒙特卡洛dropout（也称为MC dropout）。在设置贝叶斯深度学习模型时，你将贝叶斯统计与深度学习相结合。（在本章开头的图中，你可以看到贝叶斯统计学的创始人托马斯·贝叶斯牧师和深度学习领袖及奠基人之一杰弗里·辛顿的合成肖像。）有了这些近似方法，用许多参数拟合贝叶斯深度学习模型变得可行。正如第7章所讨论的，贝叶斯方法还处理了非贝叶斯概率深度学习模型中未包含的认识论不确定性。
- en: In chapters 4, 5, and 6, you worked with deep probabilistic classification and
    regression models. These models capture the data-inherent (aleatoric) uncertainty
    by predicting a whole distribution for the outcome. In chapter 7, you learned
    about an additional uncertainty, called epistemic uncertainty, that captures the
    parameter uncertainty of the outcome distribution. This epistemic uncertainty
    becomes essential when you use a DL model to make predictions in new situations
    (recall the elephant in the room). If you can’t see the elephant, you’d like to
    know at least that something is wrong.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章、第5章和第6章中，你使用深度概率分类和回归模型进行了工作。这些模型通过预测结果的整体分布来捕捉数据固有的（随机）不确定性。在第7章中，你学习了另一种称为认识论不确定性的额外不确定性，它捕捉了结果分布的参数不确定性。当使用深度学习模型在新情况下进行预测时（回想一下房间里的大象），这种认识论不确定性变得至关重要。如果你看不到大象，你至少希望知道至少有什么地方不对劲。
- en: You also saw in the last chapter that the fitting of Bayesian probabilistic
    models lets you quantify parameter uncertainty and, moreover, gives better predictions
    (lower negative log-likelihood, or NLL), especially in situations with few training
    data. Unfortunately, the Bayesian approach soon becomes slow and, essentially,
    impossible if you turn from small toy examples to real-world DL tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你也看到了上一章，适配贝叶斯概率模型让你可以量化参数不确定性，而且更重要的是，它提供了更好的预测（负对数似然或NLL更低），尤其是在训练数据较少的情况下。不幸的是，如果你从小的玩具例子转向现实世界的深度学习任务，贝叶斯方法很快就会变得缓慢，实际上变得不可能。
- en: In this chapter, you’ll learn some approximation methods that allow fitting
    Bayesian variants of probabilistic DL models. This gives you a tool to detect
    if the predicted outcome distributions are uncertain. Bayesian DL models have
    the advantage that they can detect novel situations by expressing a larger epistemic
    uncertainty, which leads to a larger outcome uncertainty. You’ll see that Bayesian
    DL regression models report greater uncertainties in extrapolation regimes, and
    Bayesian DL classification models raise the uncertainty flag for novel classes,
    indicating that their predictions aren’t reliable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习一些近似方法，这些方法允许拟合概率深度学习模型的贝叶斯变体。这为你提供了一个工具，可以检测预测结果分布是否不确定。贝叶斯深度学习模型的优势在于，它们可以通过表达更大的认识论不确定性来检测新情况，这导致结果不确定性更大。你会发现，贝叶斯深度学习回归模型在外推状态下报告更大的不确定性，而贝叶斯深度学习分类模型对新型类别提高不确定性标志，表明它们的预测不可靠。
- en: 8.1 Bayesian neural networks (BNNs)
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 贝叶斯神经网络（BNNs）
- en: 'Let’s apply the Bayesian approach described in chapter 7 to neural networks
    (NNs). Figure 8.1 (on the left) shows the mother of all Bayesian networks: Bayesian
    linear regression. Compared to standard probabilistic linear regression, the weights
    aren’t fixed but follow a distribution *P*(*θ*|*D*). There’s absolutely no reason
    that we can’t continue to use distributions instead of single weights for an NN.
    Figure 8.1 (on the right) also shows such a simple BNN. It turns out that solving
    a deep BNN isn’t so easy, but TensorFlow Probability (TFP) provides you with the
    right tools.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将第7章中描述的贝叶斯方法应用于神经网络（NN）。图8.1（左侧）展示了所有贝叶斯网络之母：贝叶斯线性回归。与标准概率线性回归相比，权重不是固定的，而是遵循一个分布
    *P*(*θ*|*D*）。我们完全没有理由不能继续使用分布而不是单个权重来构建一个神经网络。图8.1（右侧）也展示了一个这样的简单贝叶斯神经网络（BNN）。实际上，解决一个深度贝叶斯神经网络并不那么容易，但TensorFlow
    Probability（TFP）为你提供了正确的工具。
- en: '![](../Images/8-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-1.png)'
- en: Figure 8.1 A graphical representation of a Bayesian simple linear regression
    problem by a simple NN without a hidden layer and only one output node. This provides
    an estimate for the expected value of the outcome (on the left). In a Bayesian
    variant of the NN for linear regression, distributions replace the slope (a) and
    intercept (b). This is also possible for deep networks, yielding a Bayesian neural
    network (BNN). A simple example of such a network is shown on the right.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 通过一个没有隐藏层且只有一个输出节点的简单神经网络，展示了贝叶斯简单线性回归问题的图形表示。这提供了对结果期望值的估计（左侧）。在贝叶斯神经网络线性回归的变体中，分布代替了斜率（a）和截距（b）。这同样适用于深度网络，从而产生贝叶斯神经网络（BNN）。这样的网络的一个简单例子在右侧展示。
- en: 'In section 7.3, we used an analytical solution to solve a simple linear regression
    problem. This solution was only possible with the further restriction that the
    parameter *σ* *x*, describing the spread of the data, doesn’t depend on *x*, but
    must be known in advance. This analytical approach isn’t applicable for BNNs with
    hidden layers because those are too complex. You might wonder if we should again
    revert to the Bayes for hacker’s approach from chapter 7 (section 7.2). In principle,
    that approach also works for deeper BNNs. However, again speed is the problem:
    it would take too long if we’d go straight from Bayesian linear regression with
    two parameters to NNs with 50 million weights.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7.3节中，我们使用解析方法来解决一个简单的线性回归问题。这个解决方案仅当参数 *σ* *x*，描述数据的分布，不依赖于 *x*，且必须预先知道时才可行。这种解析方法不适用于具有隐藏层的贝叶斯神经网络，因为它们太复杂了。你可能想知道我们是否应该再次回到第7章（第7.2节）中提到的贝叶斯黑客方法。原则上，这种方法也适用于更深的贝叶斯神经网络。然而，速度仍然是问题：如果我们直接从具有两个参数的贝叶斯线性回归到具有5000万个权重的神经网络，这将花费太长时间。
- en: To see this, recall that for the brute-force approach in chapter 7, we evaluated
    the variable a at nbins = 30 values. We also evaluated the variable *b* at nbins
    = 30 values. All together, we evaluated the variables a and *b* at *nbins*² =
    900 different combinations. A brute-force approach for a network with 50 million
    parameters would need to evaluate the posterior at `nbins^50` `million` different
    combinations. Let’s be satisfied with only `nbins` `=` `10` . Then we have 10^(50,000,00)
    evaluations. If you could do one billion evaluations per second, that would still
    take you 10^(50,000,00) / 10⁹ = 10^(49,999,991) seconds. Even for a small network
    with 100 weights, that would take 10^(100) /10⁹ = 10^(91) seconds (see the following
    fun fact).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，回顾第7章中提到的暴力方法，我们评估了变量a在nbins = 30个值。我们还评估了变量 *b* 在nbins = 30个值。总共，我们在
    *nbins*² = 900种不同的组合中评估了变量a和 *b*。对于一个有5000万个参数的网络，暴力方法需要评估后验概率在 `nbins^50` `million`
    种不同的组合。让我们满足于只有 `nbins` `=` `10`。那么我们就有 10^(50,000,00) 次评估。如果你能每秒进行10亿次评估，那也需要
    10^(50,000,00) / 10⁹ = 10^(49,999,991) 秒。即使是对于一个小型网络，有100个权重，也需要 10^(100) /10⁹
    = 10^(91) 秒（见以下有趣的事实）。
- en: 'Fun fact Go to [https://www.wolframalpha.com/](https://www.wolframalpha.com/)
    and type 10^(91) seconds into the search field. Solution: It takes 3.169 ⋅ 10^(83)
    years to evaluate all grid points--about 10 billion times longer than the age
    of the universe!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实 前往 [https://www.wolframalpha.com/](https://www.wolframalpha.com/) 并在搜索框中输入
    10^(91) seconds。解决方案：评估所有网格点需要 3.169 ⋅ 10^(83) 年——大约是宇宙年龄的10亿倍！
- en: Neither analytical methods nor the brute-force approach does the trick for BNNs.
    What’s next? There’s an approach called Markov Chain Monte Carlo(MCMC for short).
    This approach samples parameter values more efficiently compared to the brute-force
    method. The first MCMC algorithm was the Metropolis-Hastings algorithm developed
    in the 1950s and 1970s. The world’s largest technical professional organization,
    the prestigious IEEE (Institute for Electrical and Electronic Engineers), ranks
    this approach among the ten most influential algorithms for science and engineering
    developed in the 20th century (see [http://mng.bz/vxdp](http://mng.bz/vxdp)).
    It has the advantage that, given enough computations, it’s exact. It also works
    for small problems, say 10 to 100 variables (or weights in our language), but
    not for larger networks such as DL models with typically millions of weights.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于贝叶斯神经网络，既不是解析方法也不是蛮力方法能解决问题。接下来是什么？有一个叫做马尔可夫链蒙特卡洛（MCMC，简称）的方法。这种方法比蛮力方法更有效地采样参数值。第一个MCMC算法是20世纪50年代和70年代开发的Metropolis-Hastings算法。世界上最大的技术专业组织，享有盛誉的IEEE（电气和电子工程师协会），将这种方法列为20世纪科学和工程领域十大最具影响力的算法之一（见[http://mng.bz/vxdp](http://mng.bz/vxdp)）。它具有这样的优势，即如果计算足够，它是精确的。它也适用于小问题，比如10到100个变量（或在我们的语言中是权重），但不适用于具有数百万个权重的更大网络，如深度学习模型。
- en: 'Can we do something else to get a reasonable time to obtain an approximation
    for the normalized posterior? There are two approaches: one is the variational
    inference (VI) Bayes; the other is Monte Carlo (MC) dropout. The variational Bayes
    approach is welded into TFP, providing Keras layers to do the VI. MC dropout is
    a simple approach that can be done in Keras as well.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做些什么来获得一个合理的时间来获得归一化后验的近似？有两种方法：一种是变分推断（VI）贝叶斯；另一种是蒙特卡洛（MC）dropout。变分贝叶斯方法被整合到TFP中，提供了Keras层来进行VI。MC
    dropout是一种简单的方法，也可以在Keras中实现。
- en: 8.2 Variational inference (VI) as an approximative Bayes approach
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 变分推断（VI）作为近似贝叶斯方法
- en: In situations where you can’t determine the analytical solution for a Bayesian
    NN or use the MCMC methods, you need to use techniques to approximate the Bayesian
    model. In this section, you learn about such an approximation method--variational
    inference (VI). In section 8.2.1, we give a detailed derivation of the VI approximation.
    In section 8.2.2, we use the VI approximation for a simple linear regression example.
    Because we have the analytical solution for this example (see section 7.3.3),
    we can judge the quality of the VI approximation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在无法确定贝叶斯神经网络的分析解或使用MCMC方法的情况下，你需要使用技术来近似贝叶斯模型。在本节中，你将了解这种近似方法——变分推断（VI）。在第8.2.1节中，我们详细推导了VI近似。在第8.2.2节中，我们使用VI近似来处理一个简单的线性回归示例。因为我们有这个示例的分析解（见第7.3.3节），我们可以判断VI近似的质量。
- en: 'You can use the VI approximation approach for all kinds of DL models. In section
    8.5, you use it for two case studies: one for a regression problem and one for
    a classification problem. To understand the basic ideas of this approach and to
    be able to compare it with some exact methods, we demonstrate the approach on
    the same simple linear regression problem from chapter 7\. You’re encouraged to
    follow the notebook while reading the text.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用VI近似方法来处理各种深度学习模型。在第8.5节中，你用它来处理两个案例研究：一个用于回归问题，另一个用于分类问题。为了理解这种方法的基本思想，并能够将其与一些精确方法进行比较，我们在第7章的相同简单线性回归问题上展示了这种方法。鼓励你在阅读文本的同时跟随笔记本。
- en: '| ![](../Images/computer-icon.png) | Hands-on timeOpen [http://mng.bz/4A5R](http://mng.bz/4A5R)
    and follow the notebook as you read the text. This notebook is about linear regression
    in the Bayesian way. It shows the analytical approach, VI, and also how to use
    TFP. Try to understand what happens in the notebook. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间打开[http://mng.bz/4A5R](http://mng.bz/4A5R)，在阅读文本的同时跟随笔记本。这个笔记本是关于贝叶斯方式的线性回归。它展示了解析方法、VI，以及如何使用TFP。尝试理解笔记本中发生的事情。'
- en: The main idea of the Bayes approach in DL is that with BNNs, each weight is
    replaced by a distribution. Normally, this is quite a complicated distribution,
    and this distribution isn’t independent among different weights. The idea behind
    the VI Bayes method is that the complicated posterior distributions of the weights
    are approximated by a simple distribution called variational distribution.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中贝叶斯方法的核心理念是，在贝叶斯神经网络（BNNs）中，每个权重都被一个分布所替代。通常，这是一个相当复杂的分布，并且这些分布在不同权重之间不是独立的。VI贝叶斯方法背后的想法是，通过一个简单的分布，称为变分分布，来近似复杂的权重后验分布。
- en: 'Often one uses Gaussians as parametric distributions; this is also done by
    default when using TFP. The Gaussian variational distribution is defined by two
    parameters: the mean and the variance. Instead of learning a single weight value
    w , the network has to learn the two parameters of weight distribution: *w**[μ]*
    for the mean of the Gauss and *w**[σ]* for the spread of the Gauss (see figure
    8.2). Besides the type of the variational distribution that is used to approximate
    the posterior, we also need to define a prior distribution. A common choice is
    to use the standard normal, *N*(0, 1), as the prior.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通常人们使用高斯作为参数分布；当使用TFP时，这也默认使用。高斯变分分布由两个参数定义：均值和方差。网络不是学习单个权重值w，而是必须学习权重分布的两个参数：高斯的均值*w**[μ]*和方差*w**[σ]*（见图8.2）。除了用于近似后验的变分分布的类型外，我们还需要定义一个先验分布。一个常见的选择是将标准正态分布*N*(0,
    1)作为先验。
- en: '![](../Images/8-2.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-2.png)'
- en: Figure 8.2 A Bayesian network with two hidden layers. Instead of fixed weights,
    the weights now follow a distribution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 一个具有两个隐藏层的贝叶斯网络。这里的权重现在遵循一个分布，而不是固定的权重。
- en: Sections 8.2.1 and 8.2.2 are somewhat involved but give you some insights into
    the derivation of the VI approximation. You can skip those sections if you’re
    not interested in the derivations. In section 8.3, you learn how to use the TFP
    implementation of the VI method.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第8.2.1节和第8.2.2节内容较为复杂，但能为你提供一些关于VI近似推导的见解。如果你对推导不感兴趣，可以跳过这些章节。在第8.3节中，你将学习如何使用TFP的VI方法实现。
- en: 8.2.1 Looking under the hood of VI*
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 查看VI的内部机制*
- en: VI has been used in DL since the invention of the variational autoencoder in
    late 2013 by Kingma and Welling from Amsterdam University. The implementation
    of VI as we use it here (referred to as the Bayes by Backprop algorithm) was introduced
    in the paper, “Weight Uncertainty in Neural Networks,” from the Google DeepMind
    scientists Blundell and colleagues ([https://arxiv.org/abs/1505.05424](https://arxiv.org/abs/1505.05424)).
    TFP effectively integrates this approach as you’ll see later. But let’s understand
    the inner mechanics of the VI principle first. In figure 8.3, you see a sketch
    of that principle.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: VI自2013年底阿姆斯特丹大学的Kingma和Welling发明变分自动编码器以来，在深度学习（DL）中得到了应用。我们这里使用的VI实现（称为反向传播的贝叶斯算法）在Google
    DeepMind科学家Blundell及其同事的论文“神经网络中的权重不确定性”中提出（[https://arxiv.org/abs/1505.05424](https://arxiv.org/abs/1505.05424)）。TFP有效地整合了这一方法，你稍后将会看到。但首先让我们理解VI原理的内部机制。在图8.3中，你可以看到该原理的草图。
- en: '![](../Images/8-3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-3.png)'
- en: Figure 8.3 The principle idea of variational inference (VI). The larger region
    on the left depicts the space of all possible distributions, and the dot in the
    upper left represents the posterior *P*(*θ*[1] |*D*), corresponding to the dotted
    density in the right panel. In the left panel, the inner region depicts the space
    of possible variational distributions *q**[λ]*(*θ*[1]). The optimized variational
    distribution *P*(*θ*[1] |*D*), illustrated by the point in the inner loop, corresponds
    to the solid density displayed in the right panel, which has the smallest distance
    to the posterior as shown by the dotted line.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 变分推断（VI）的原理。左侧较大的区域描述了所有可能的分布空间，左上角的点代表后验*P*(*θ*[1] |*D*)，对应于右侧面板中的虚线密度。在左侧面板中，内部区域描述了可能的变分分布*q**[λ]*(*θ*[1])的空间。通过内部循环中的点表示的优化变分分布*P*(*θ*[1]
    |*D*)，对应于右侧面板中显示的实线密度，其与后验的最小距离由虚线表示。
- en: The actual normalized posterior distribution, *P*(*θ*|*D*), is inaccessible.
    The reason is that the integral, which needs to be solved to ensure that the posterior
    is normalized (see section 7.3.2), is high-dimensional. As discussed in section
    8.1, we can’t use the brute-force approximation because it’s far too slow. Further,
    such high-dimensional integrals are also too complex to be solved analytically.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的标准化后验分布，*P*(*θ*|*D*)，是无法访问的。原因是需要解决的积分，以确保后验分布是归一化的（参见第7.3.2节），是高维的。如第8.1节所述，我们无法使用暴力近似，因为它太慢了。此外，这样的高维积分也太复杂，无法解析求解。
- en: To get a feeling for the meaning of the different parameters in figure 8.3,
    let’s assume a deep BNN. The parameter *θ* replaces the weights of the non-Bayesian
    variant of the NN. The parameter *θ* in a Bayesian network isn’t fixed but follows
    a distribution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解图8.3中不同参数的意义，让我们假设一个深度贝叶斯神经网络。参数 *θ* 替代了非贝叶斯神经网络变体的权重。贝叶斯网络中的参数 *θ* 不是固定的，而是遵循一个分布。
- en: 'The left panel in figure 8.3 shows the abstract space of possible distributions,
    and the dot in the upper left represents the posterior *P*(*θ*|*D*). Instead of
    determining the posterior directly, we approximate it with a simple, variational
    distribution, *q**[λ]*(*θ*) such as a Gaussian (see the bell-shaped density in
    the right panel of figure 8.3). There are infinitely many Gaussians out there,
    but these make up only a subgroup of all possible distributions. (In the left
    panel of figure 8.3, it’s marked as the small region labeled with *q**[λ]*(*θ*).)
    The job of VI is to tune the variational parameter *λ* so that *q**[λ]*(*θ*[1])
    gets as close as possible to the true posterior *P*(*θ*|*D*). The right side of
    the figure shows this situation again for a single *θ* 1\. The 1D posterior distribution
    is approximated by a 1D Gaussian variational distribution. For each Gaussian,
    you have two parameters: *λ* = (*μ* , *σ*). These are called variational parameters.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3的左侧面板显示了可能分布的抽象空间，左上角的点代表后验 *P*(*θ*|*D*)。我们不是直接确定后验，而是用一个简单、变分的分布，*q**[λ]*(*θ*)，如高斯分布（参见图8.3右侧面板中的钟形密度）来近似它。有无数个高斯分布，但它们只是所有可能分布的一个子集。（在图8.3的左侧面板中，它被标记为带有
    *q**[λ]*(*θ*) 标签的小区域。）VI的工作是调整变分参数 *λ*，使得 *q**[λ]*(*θ*[1]) 尽可能接近真实后验 *P*(*θ*|*D*)。图的右侧再次显示了这种情况，针对单个
    *θ* 1。1维后验分布被一个1维高斯变分分布近似。对于每个高斯分布，你有两个参数：*λ* = (*μ* , *σ*)。这些被称为变分参数。
- en: 'You want the variational distribution to be as close to the real posterior
    as possible, or in more mathematical terms: minimize the distance between the
    nice distribution and the real one. You can fiddle with the shape of the nice
    distribution by manipulating the variational parameters *λ* = (*μ* , *σ*).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望变分分布尽可能接近真实后验分布，或者用更数学的说法：最小化良好分布与真实分布之间的距离。你可以通过操作变分参数 *λ* = (*μ* , *σ*)
    来调整良好分布的形状。
- en: It might be a good idea to recap all the terms. Table 8.1 gives the important
    terms for VI. You should know all, except for the one in the last row. We introduce
    the parameter w in the last row in section 8.2.2.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有必要回顾所有术语。表8.1给出了VI的重要术语。你应该了解所有术语，除了最后一行。我们在第8.2.2节中介绍了最后一行的参数w。
- en: Table 8.1 Different terms used in VI
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 VI中使用的不同术语
- en: '| Term | In the simple example | Name | Remarks |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Term | In the simple example | Name | Remarks |'
- en: '| *θ* | *θ* = (*a, b*) = (slope, intercept) | Parameters | *θ*(theta) isn’t
    fixed but follows a distribution.In the non-Bayes case, *θ* is fixed and is identical
    to the tunable parameters w of the network. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| *θ* | *θ* = (*a, b*) = (slope, intercept) | Parameters | *θ*(theta) isn’t
    fixed but follows a distribution.In the non-Bayes case, *θ* is fixed and is identical
    to the tunable parameters w of the network. |'
- en: '| *P*(*θ*&#124;*D*) | *P*(*a*&#124;*D*) | Posterior distribution | Usually
    not tractable. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| *P*(*θ*&#124;*D*) | *P*(*a*&#124;*D*) | Posterior distribution | Usually
    not tractable. |'
- en: '| *q**[λ]*(*θ*) | *N*(*a* ; *μ**[a]* , *σ**[a]*)*N*(*b* ; *μ**[b]* , *σ**[b]*)
    | Variational approximation | Tractable functions, such as an independent Gaussian
    for each *θ* i. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| *q**[λ]*(*θ*) | *N*(*a* ; *μ**[a]* , *σ**[a]*)*N*(*b* ; *μ**[b]* , *σ**[b]*)
    | Variational approximation | Tractable functions, such as an independent Gaussian
    for each *θ* i. |'
- en: '| *λ* | *λ* = (*μ**[a]* , *σ**[a]* , *μ**[b]* , *σ**[b]*) | Variational parameters
    | Parameters of the variational distribution that approximate the posterior distribution.
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| *λ* | *λ* = (*μ**[a]* , *σ**[a]* , *μ**[b]* , *σ**[b]*) | Variational parameters
    | Parameters of the variational distribution that approximate the posterior distribution.
    |'
- en: '| *ω* | *w* = (*w*[0] ,*w*[1] , *w*[2] , *w*[3])*λ* = (*w*[0] , *sp*(*w*[1]),
    *w*[2] , *sp*(*w*[3])) | Tunable parameters | Optimized parameters in the Bayesian
    network. The shortcut sp(w1) is for the `softplus` function, resulting in positive
    parameters as needed for the standard deviation of a Gaussian. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| *ω* | *w* = (*w*[0] ,*w*[1] , *w*[2] , *w*[3])*λ* = (*w*[0] , *sp*(*w*[1]),
    *w*[2] , *sp*(*w*[3])) | 可调参数 | 贝叶斯网络中的优化参数。快捷方式 sp(w1) 是指 `softplus` 函数，以产生正参数，这是高斯标准差所需的。|'
- en: 'But what measure can you use to describe the similarity or divergence between
    two distributions? And, maybe even more important, how can you measure the divergence
    to a posterior that you don’t know? Well, the Kullback-Leibler (KL) divergence
    is the answer to both questions. You already encountered KL divergence in section
    4.2\. Let’s use it and write the formula for the KL divergence between *P*(*θ*|*D*)
    and *q**[λ]*(*θ*). If you remember section 4.2, the KL divergence isn’t symmetrical.
    If you’re lucky and choose the right order, KL [*q**[λ]*(*θ*)||*P*(*θ*|*D*)],
    the unknown posterior drops out, and the following expression is all you need
    to optimize:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可以使用什么度量来描述两个分布之间的相似性或差异？也许更重要的是，你如何测量到一个你不知道的后验分布的差异？嗯，Kullback-Leibler (KL)
    散度是这两个问题的答案。你已经在第 4.2 节中遇到了 KL 散度。让我们使用它，并写出 *P*(*θ*|*D*) 和 *q**[λ]*(*θ*) 之间的
    KL 散度的公式。如果你记得第 4.2 节，KL 散度是不对称的。如果你幸运并且选择了正确的顺序，KL [*q**[λ]*(*θ*)||*P*(*θ*|*D*)]，未知的后验分布就会消失，你需要的表达式如下：
- en: '*λ*^* = argmin {KL [*q**[λ]*(*θ*) || *P*(*θ*)] − *E**[θ∼ qλ]* ) [log(*P*(*D*|*θ*)]}'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*^* = argmin {KL [*q**[λ]*(*θ*) || *P*(*θ*)] − *E**[θ∼ qλ]* ) [log(*P*(*D*|*θ*)]}'
- en: '*λ*^* = argmin {KL [*q**[λ]*(*θ*) || *P*(*θ*)] − *E**[θ∼ qλ]* ) [log(*P*(*D*|*θ*)]}
    Equation 8.1'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*^* = argmin {KL [*q**[λ]*(*θ*) || *P*(*θ*)] − *E**[θ∼ qλ]* ) [log(*P*(*D*|*θ*)]}
    方程 8.1'
- en: If you did it differently and started with KL [*P*(*θ*|*D*)|| *q**[λ]*(*θ*)],
    you won’t get a usable expression for equation 8.1\. (In the sidebar, you’ll find
    the derivation of the expression for equation 8.1). This equation looks a bit
    scarier than it actually is. Let’s have a closer look at the two terms in equation
    8.1.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以不同的方式开始，并从 KL [*P*(*θ*|*D*)|| *q**[λ]*(*θ*)] 开始，你将无法得到方程 8.1 的可用表达式。（在侧边栏中，你可以找到方程
    8.1 的推导）。这个方程看起来比实际要可怕一些。让我们更仔细地看看方程 8.1 中的两个项。
- en: Derivation of the optimization equation
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 优化方程的推导
- en: Let’s derive equation 8.1\. It’s a bit of calculus, so feel free to skip this
    if it isn’t your cup of tea. On the other hand, it could be fun, so let’s get
    started.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导方程 8.1。这涉及到一点微积分，所以如果你不喜欢这个，可以自由跳过。另一方面，这可能会很有趣，所以让我们开始吧。
- en: We start with the KL divergence between the variational approximation *q**[λ]*(*θ*)
    and the true posterior *P*(*θ*|*D*). The strange thing is that we don’t know the
    true posterior. But, as it turns out in a few lines, if we calculate KL [*q**[λ]*(*θ*)||
    *P*(*θ*|*D*)] and not KL [*P*(*θ*|*D*)|| *q**[λ]*(*θ*)] , the true posterior drops
    out and only the KL divergence you need to compute is between the variational
    distribution and the prior distribution that’s known.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从变分近似 *q**[λ]*(*θ*) 和真实后验 *P*(*θ*|*D*) 之间的 KL 散度开始。奇怪的是，我们不知道真实后验。但是，正如几行之后所显示的，如果我们计算
    KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] 而不是 KL [*P*(*θ*|*D*)|| *q**[λ]*(*θ*)]，真实后验就会消失，你只需要计算的是变分分布和已知先验分布之间的
    KL 散度。
- en: 'Let’s write the definition of the KL divergence and indicate the data with
    D. If you can’t remember how to write the KL divergence of, say, two functions
    f and g, maybe this rule of thumb will help: it’s “back alone down,” meaning that
    the second function g in KL [*f*(*θ*)||*g*(*θ*)] will be alone and in the denominator.
    The following definition of the KL divergence shouldn’t surprise you too much:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写出 KL 散度的定义，并用 D 表示数据。如果你记不起如何写出两个函数 f 和 g 的 KL 散度，也许这个经验法则会帮到你：它是“向后单独向下”，意味着
    KL [*f*(*θ*)||*g*(*θ*)] 中的第二个函数 g 将单独位于分母中。以下 KL 散度的定义不应该让你感到太惊讶：
- en: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = ∫ *q**[λ]*(*θ*) log( *q**[λ]*(*θ*) / *P*(*θ*|*D*)
    ) *dθ*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = ∫ *q**[λ]*(*θ*) log( *q**[λ]*(*θ*) / *P*(*θ*|*D*)
    ) *dθ*
- en: '*P*(*θ*|*D*) is the second function (“in the back”) and, thus, only appears
    once (“alone”) in the integral and, further, it is in the denominator (“down”).
    (you could also look up the definition of the KL divergence.) Now, there are some
    algebraic manipulations ahead. Feel free to follow the steps using a pen and paper.
    The first thing we do is to use the definition of the conditional probability,
    *P*(*θ*|*D*) = *P*(*θ*|*D*) / *P*(*D*):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) 是第二个函数（“在后面”），因此它只在积分中单独出现一次（“单独”），并且它位于分母（“下面”）。（你也可以查阅KL散度的定义。）现在，有一些代数操作。请随意使用笔和纸跟随步骤。我们首先使用条件概率的定义，*P*(*θ*|*D*)
    = *P*(*θ*|*D*) / *P*(*D*)：'
- en: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = ∫ *q**[λ]*(*θ*) log( *q**[λ]*(*θ*) / (*P*(*θ*|*D*)
    / *P*(*D*) ) ) *dθ*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = ∫ *q**[λ]*(*θ*) log( *q**[λ]*(*θ*) / (*P*(*θ*|*D*)
    / *P*(*D*) ) ) *dθ*
- en: 'Then we use the calculation rules of the logarithm log(*A* ⋅ *B*) = log(*A*)
    + log(*B*) and log(*B* /*A*) = −log(*A* /*B*) to split the integral into two parts:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用对数运算规则 log(*A* ⋅ *B*) = log(*A*) + log(*B*) 和 log(*B* /*A*) = −log(*A*
    /*B*) 将积分分成两部分：
- en: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = ∫ *q**[λ]*(*θ*) log *P*(*D*) *dθ* − ∫ *q**[λ]*(*θ*)
    log( *P*(*θ*|*D*) / *q**[λ]*(*θ*) ) *dθ*
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = ∫ *q**[λ]*(*θ*) log *P*(*D*) *dθ* − ∫ *q**[λ]*(*θ*)
    log( *P*(*θ*|*D*) / *q**[λ]*(*θ*) ) *dθ*
- en: 'Because log *P*(*D*) doesn’t depend on *θ* , we can put it before the integral:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 log *P*(*D*) 不依赖于 *θ*，我们可以将其放在积分之前：
- en: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = log *P*(*D*) ⋅ ∫ *q**[λ]*(*θ*) *dθ* − ∫
    *q**[λ]*(*θ*) log( *P*(*θ*|*D*)/ *q**[λ]*(*θ*) ) *dθ*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = log *P*(*D*) ⋅ ∫ *q**[λ]*(*θ*) *dθ* − ∫
    *q**[λ]*(*θ*) log( *P*(*θ*|*D*)/ *q**[λ]*(*θ*) ) *dθ*
- en: 'And because *q**[λ]*(*θ*) is a probability density and for all probability
    densities the integral is one, we have ∫ *q**[λ]*(*θ*) *dθ* = 1 :'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 *q**[λ]*(*θ*) 是概率密度，对于所有概率密度，积分等于1，因此我们有 ∫ *q**[λ]*(*θ*) *dθ* = 1 :'
- en: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = log *P*(*D*) − ∫ *q**[λ]*(*θ*) log( *P*(*θ*|*D*)
    / *q**[λ]*(*θ*) ) *dθ*
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: KL [*q**[λ]*(*θ*)|| *P*(*θ*|*D*)] = log *P*(*D*) − ∫ *q**[λ]*(*θ*) log( *P*(*θ*|*D*)
    / *q**[λ]*(*θ*) ) *dθ*
- en: 'The first term doesn’t depend on the variational parameter *λ* ; therefore,
    all you need to minimize is −∫ *q**[λ]*(*θ*) log( *P*(*θ*|*D*) / *q**[λ]*(*θ*)
    ) *dθ* . The optimal value *λ* is thus:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项不依赖于变分参数 *λ*；因此，你需要最小化的只是 −∫ *q**[λ]*(*θ*) log( *P*(*θ*|*D*) / *q**[λ]*(*θ*)
    ) *dθ*。因此，最优值 *λ* 是：
- en: '*λ*^* = argmin {− ∫ *q**[λ]*(*θ*) log( *P*(*θ*|*D*)) / *q**[λ]*(*θ*) ) *dθ*
    }'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*^* = argmin {− ∫ *q**[λ]*(*θ*) log( *P*(*θ*|*D*)) / *q**[λ]*(*θ*) ) *dθ*
    }'
- en: Now, let’s arrange it into the form of equation 8.1 with *P*(*θ*|*D*) = *P*(*D*|*θ*)
    ⋅ *P*(*θ*)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将其整理成方程8.1的形式，其中 *P*(*θ*|*D*) = *P*(*D*|*θ*) ⋅ *P*(*θ*)
- en: '*λ*^* = argmin{− ∫ *q**[λ]*(*θ*) log(*P*(*D*|*θ*) ⋅ *P*(*θ*) / *q**[λ]*(*θ*)
    ) *dθ* }'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*^* = argmin{− ∫ *q**[λ]*(*θ*) log(*P*(*D*|*θ*) ⋅ *P*(*θ*) / *q**[λ]*(*θ*)
    ) *dθ* }'
- en: 'and with the calculus rules for the logarithm:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 并且使用对数运算规则：
- en: '*λ*^* = argmin {∫ *q**[λ]*(*θ*) log( *q**[λ]*(*θ*) / *P*(*θ*) ) dθ − ∫ *q**[λ]*(*θ*)
    ⋅ log *P*(*D*|*θ*) *dθ* }'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*^* = argmin {∫ *q**[λ]*(*θ*) log( *q**[λ]*(*θ*) / *P*(*θ*) ) dθ − ∫ *q**[λ]*(*θ*)
    ⋅ log *P*(*D*|*θ*) *dθ* }'
- en: 'The first term is the definition of the KL divergence between the variational
    and the prior distribution: KL [*q**[λ]*(*θ*)|| *P*(*θ*)] (remember “back alone
    down”). The second term is the definition of the expectation of the function log
    *P*(*D*|*θ*). So finally, we have'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项是变分分布与先验分布之间的KL散度的定义：KL [*q**[λ]*(*θ*)|| *P*(*θ*)]（记住“向下回溯”）。第二项是函数 log *P*(*D*|*θ*)
    的期望的定义。所以最终我们有
- en: '*λ*^* = argmin {KL[*q**[λ]*(*θ*)|| *P*(*θ*)] − *E**[θ∼q[λ]]* [log(*P*(*D*|*θ*)]}'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*^* = argmin {KL[*q**[λ]*(*θ*)|| *P*(*θ*)] − *E**[θ∼q[λ]]* [log(*P*(*D*|*θ*)]}'
- en: and are done with deriving the expression in equation 8.1\. Wasn’t so hard,
    was it?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 并且完成了方程8.1表达式的推导。这并不难，对吧？
- en: Because we want to minimize the equation 8.1, the first term needs to get as
    small as possible. It’s again a KL divergence, but this time, it’s between the
    variational approximation *q**[λ]*(*θ*) and the prior *P*(*θ*). Because the KL
    divergence is (kind of) a distance, this term wants the approximate distribution
    *q**[λ]*(*θ*) to be as close to the prior distribution *P*(*θ*) as possible. In
    BNN, the prior is usually chosen to be around zero, so this term ensures that
    the distribution *q**[λ]*(*θ*) is centered at small values. For this reason, we
    also call the first term a regularizer. It favors *θ* distributions that are centered
    at zero. Choosing a narrow prior around values far away from zero might lead to
    poor performance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想最小化方程8.1，第一个项需要尽可能小。它又是KL散度，但这次，它是变分近似*q**[λ]*(*θ*)和先验*P*(*θ*)之间的。因为KL散度（某种程度上）是一个距离，这个项希望近似分布*q**[λ]*(*θ*)尽可能接近先验分布*P*(*θ*)。在贝叶斯神经网络中，先验通常选择为零附近，所以这个项确保了分布*q**[λ]*(*θ*)以小值为中心。因此，我们也称第一个项为正则化项。它倾向于以零为中心的*θ*分布。选择远离零的狭窄先验可能会导致性能不佳。
- en: The second term, *E**[θ∼q[λ]]* [log(*P*(*D*|*θ*)], is an old friend. It calculates
    the expected value of log(*P*(*D*|*θ*), given the parameter *θ* . The parameter
    *θ* is distributed according to the approximative variational distribution *λ*
    , which is determined by the variational parameter *λ* . But after all, *E**[θ∼q[λ]]*
    [log(*P*(*D*|*θ*)] is an expectation over log(*P*(*D*|*θ*)). Do you recognize
    this dear friend? Well, maybe, look closer. The expectation is the same as the
    mean if the number of draws goes to infinity. But the mean is sometimes easier
    to understand than the expectation, so we go with the mean and sample the parameter
    *θ* from *q**[λ]* for the network.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个项，*E**[θ∼q[λ]]* [log(*P*(*D*|*θ*)]，是一个老朋友。它计算给定参数*θ*的对数(*P*(*D*|*θ*))的期望值。参数*θ*根据变分分布*q**[λ]*(*θ*)分布，该分布由变分参数*λ*确定。但无论如何，*E**[θ∼q[λ]]*
    [log(*P*(*D*|*θ*)]是对对数(*P*(*D*|*θ*))的期望。你认识这个亲爱的朋友吗？好吧，也许，更仔细地看看。如果抽取次数趋于无穷大，期望就等于均值。但均值有时比期望更容易理解，所以我们选择均值，并从*q**[λ]*中抽取参数*θ*用于网络。
- en: Now, look at the right side of figure 8.3\. If you draw from the distribution
    (for example, for *θ* 1), you get *θ* 1 = 2\. Or, in our linear regression example,
    you randomly pick a and *b* from *q**[λ]*(*a, b*). Then, given *θ* = (*a, b*),
    you calculate the log probability that we observe in the data D. In the linear
    regression example, where we assumed that the data is distributed like a Gaussian
    around the mean *μ* = *a* ⋅ *x*[1] + *b* with fixed standard deviation *σ* , you
    get
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看图8.3的右侧。如果你从分布中抽取（例如，对于*θ* 1），你会得到*θ* 1 = 2。或者，在我们的线性回归例子中，你从*q**[λ]*(*a,
    b*)中随机选择a和*b*。然后，给定*θ* = (*a, b*)，你计算在数据D中观察到的对数概率。在假设数据像高斯分布围绕均值*μ* = *a* ⋅ *x*[1]
    + *b*且具有固定标准差*σ*的线性回归例子中，你会得到
- en: '![](../Images/8-3_E01.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图8-3_E01](../Images/8-3_E01.png)'
- en: Oh, hello. Welcome back, my dear friend log-likelihood! How did I not see you
    in the first place! Let’s nail it down. The second term (including the minus),
    −*E**[θ∼q[λ]]* [log(*P*(*D*|*θ*)], is the averaged NLL, which we as always like
    to minimize. The average is taken over different values of *θ* , which are drawn
    from *q**[λ]*(*θ*).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，你好。欢迎回来，我亲爱的朋友对数似然！我一开始怎么没看到你呢！让我们把它确定下来。第二个项（包括减号），−*E**[θ∼q[λ]]* [log(*P*(*D*|*θ*)]，是平均NLL，我们总是喜欢最小化它。平均是在*θ*的不同值上进行的，这些值是从*q**[λ]*(*θ*)中抽取的。
- en: To wrap up, in equation 8.1, we want the NLL averaged according to the probability
    of *θ* to be minimal, with the restriction that the variational distribution of
    the parameter *θ* isn’t too far away from the prior *P*(*θ*).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在方程8.1中，我们希望根据*θ*的概率平均的NLL最小，同时限制参数*θ*的变分分布不要离先验*P*(*θ*)太远。
- en: 8.2.2 Applying VI to the toy problem*
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 将VI应用于玩具问题
- en: Congratulations on not skipping to section 8.3! We still have a bit of math
    to digest in this section. We now apply the VI approach to our toy problem with
    Bayesian regression. You already saw applications of Bayesian regression in section
    7.2.1 (solved via the hacker’s approach and brute force) and section 7.3.3 (solved
    analytically). As a reminder, the Bayesian variant of a probabilistic model for
    simple linear regression is given by *P*(*y*|*x*, (*a*, *b*)) = *N*(*y* ; *μ*
    = *a* ⋅ *x* + *b* , *σ* = 3). We assume, as before, that the standard deviation
    *σ* , capturing the aleatoric uncertainty, is known. Don’t bend your head as to
    why we set *σ* = 3; we just choose it so that the plot looks nice.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你没有跳过到第8.3节！在这个部分我们还有一些数学内容需要消化。我们现在将变分推断（VI）方法应用于我们的玩具问题，即贝叶斯回归。你已经在第7.2.1节（通过黑客方法和暴力解决）和第7.3.3节（通过解析解决）中看到了贝叶斯回归的应用。作为提醒，简单线性回归的概率模型贝叶斯变体为
    *P*(*y*|*x*, (*a*, *b*)) = *N*(*y* ; *μ* = *a* ⋅ *x* + *b* , *σ* = 3)。我们假设，和之前一样，标准差
    *σ* ，它捕捉了随机不确定性，是已知的。不要纠结于为什么我们设 *σ* = 3；我们只是选择它以便图表看起来更美观。
- en: In the Bayesian variant, we first need to define our priors for the two model
    parameters (*θ* = (*a, b*)) with the slope a and the intercept b. As before, we
    choose the normally distributed *N*(0, 1). Then, we define the variational distribution
    *q**[λ]*(*θ*), which is tuned to approximate the posterior *P*(*θ*|*D*). In principle,
    the variational distribution could be a complex object, but here we keep things
    simple and choose two independent Gaussians.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯变体中，我们首先需要为两个模型参数（*θ* = (*a, b*)）定义先验分布，其中斜率 *a* 和截距 *b*。和之前一样，我们选择正态分布
    *N*(0, 1)。然后，我们定义变分分布 *q**[λ]*(*θ*)，该分布被调整以近似后验 *P*(*θ*|*D*)。原则上，变分分布可能是一个复杂对象，但在这里我们保持简单，选择两个独立的正态分布。
- en: 'The slope parameter a (*a* ∼ *N*(*μ**[a]* , *σ**[a]*)) is drawn from the first
    Gaussian, and the parameter *b* (*b* ∼ *N*(*μ**[b]* , *σ**[b]*)) is drawn from
    the second. This leaves us with four variational parameters, *λ* = (*μ**[a]* ,
    *σ**[a]* , *μ**[b]* , *σ**[b]*), that we determine via optimization. We use stochastic
    gradient descent to optimize the vector *w* = (*μ**[a]* , *w*[1] , *μ**[b]* ,
    *w*[3]). The scale parameters, *σ* a and *σ* b, need to be positive, and we don’t
    want to restrict the values of w1 and w3\. Therefore, we feed w1 and w3 through
    a `softplus` function as we did in section 5.3.2\. In listing 8.1, you can see
    the corresponding code: `sigma_a` `= tf.math.softplus(w[1])` and `sigma_b = tf.math.softplus(w[3])`
    .'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率参数 *a* (*a* ∼ *N*(*μ**[a]* , *σ**[a]*)) 从第一个高斯分布中抽取，参数 *b* (*b* ∼ *N*(*μ**[b]*
    , *σ**[b]*)) 从第二个高斯分布中抽取。这使我们剩下四个变分参数，*λ* = (*μ**[a]* , *σ**[a]* , *μ**[b]* ,
    *σ**[b]*)，我们通过优化来确定这些参数。我们使用随机梯度下降法来优化向量 *w* = (*μ**[a]* , *w*[1] , *μ**[b]* ,
    *w*[3])。尺度参数 *σ* a 和 *σ* b 需要是正数，我们不希望限制 w1 和 w3 的值。因此，我们像在第5.3.2节中做的那样，将 w1 和
    w3 通过 `softplus` 函数传递。在列表8.1中，你可以看到相应的代码：`sigma_a` `= tf.math.softplus(w[1])`
    和 `sigma_b = tf.math.softplus(w[3])` 。
- en: Figure 8.4 shows the network. It’s a small network because we want to compare
    it with the brute-force method in section 7.2.1.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4显示了网络。它是一个小网络，因为我们想将它与第7.2.1节中的暴力方法进行比较。
- en: '![](../Images/8-4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-4.png)'
- en: Figure 8.4 The simple linear regression model. On the left is a non-Bayesian
    NN and on the right, a Bayesian model with variational inference (VI) approximation.
    The weights, *a* and *b*, in the VI model are replaced by Gaussian distributions,
    parameterized by the variational parameters *λ* = (*μ**[a]* , *σ**[a]* , *μ**[b]*
    , *σ**[b]*).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 简单线性回归模型。左边是一个非贝叶斯神经网络，右边是一个具有变分推断（VI）近似的贝叶斯模型。在VI模型中，权重 *a* 和 *b* 被高斯分布所取代，这些分布由变分参数
    *λ* = (*μ**[a]* , *σ**[a]* , *μ**[b]* , *σ**[b]*）参数化。
- en: 'The task is to tune the variational parameters to minimize equation 8.1\. We
    determine the parameters *w* = (*μ**[a]* , *w*[1] , *μ**[b]* , *w*[3]) by using
    gradient descent. But before we start coding and minimizing equation 8.1, let’s
    take a closer look at the loss function in this equation to get a better understanding
    of what’s going on during minimization:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是调整变分参数以最小化方程8.1。我们通过梯度下降法确定参数 *w* = (*μ**[a]* , *w*[1] , *μ**[b]* , *w*[3])。但在我们开始编码并最小化方程8.1之前，让我们更仔细地看看这个方程中的损失函数，以便更好地理解最小化过程中的情况：
- en: loss[VI] = loss[KL] + loss[NLL] = KL [*q**[λ]*(*θ*)|| *P*(*θ*)] − *E**[θ]**[∼]**[qλ]*
    [log(*P*(*D*|*θ*)] Equation 8.2
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: loss[VI] = loss[KL] + loss[NLL] = KL [*q**[λ]*(*θ*)|| *P*(*θ*)] − *E**[θ]**[∼]**[qλ]*
    [log(*P*(*D*|*θ*)] 方程8.2
- en: Because we use Gaussians for the variational approximations *q**[λ]*(*a*) and
    *q**[λ]*(*b*) and also a standard normal *N*(0, 1) Gaussian for the prior, the
    KL divergence between a variational Gaussian *N*(*μ* , *σ*) and the prior *N*(0,
    1) can be calculated analytically. (We skip the derivation because it’s quite
    tedious without adding much insight.) From this, we get
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在变分近似*q**[λ]*(*a*)和*q**[λ]*(*b*)以及先验中使用了标准正态*N*(0, 1)高斯，所以变分高斯*N*(*μ* ,
    *σ*)和先验*N*(0, 1)之间的KL散度可以解析地计算。（我们省略了推导过程，因为它相当繁琐，而且没有增加太多洞察。）从这一点，我们得到
- en: loss[KL] = KL [*q**[λ]*(*w*)||*P*(*w*)] = KL [*N*(*μ* ,*σ*)||*N*(0, 1) = −1/2(1
    + log(*σ*²) − *μ*² − *σ*²) Equation 8.3
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 损失[KL] = KL [*q**[λ]*(*w*)||*P*(*w*)] = KL [*N*(*μ* ,*σ*)||*N*(0, 1) = −1/2(1
    + log(*σ*²) − *μ*² − *σ*²) 方程式8.3
- en: Don’t trust this? Have a look at the notebook after the VI section, where you
    can verify this numerically.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不相信这个？看看VI部分之后的笔记本，你可以在那里通过数值验证这一点。
- en: 'For the second loss[NLL] term in equation 8.2, we need to calculate the expectation
    of the NLL: *E**[θ]**[∼]**[qλ]* [log(*P*(*D*|*θ*)]. This time we aren’t so lucky.
    It’s not possible to calculate this term in closed form. We therefore approximate
    the expected value with the empirical mean by averaging over −log(*P*(*D*|*θ*)
    for a different *θ* , which we can sample from *θ* ∼ *q**[λ]* . But how many samples
    of *θ* are required? Well, it turns out that a single sample is usually enough
    (we’ll come back to this point later).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于方程式8.2中的第二个损失[NLL]项，我们需要计算NLL的期望值：*E**[θ]**[∼]**[qλ]* [log(*P*(*D*|*θ*)]。这次我们不太幸运。这个项不能以封闭形式计算。因此，我们通过平均不同的*θ*的-log(*P*(*D*|*θ*)来近似期望值，这些*θ*可以从*θ*
    ∼ *q**[λ]*中采样。但需要多少个*θ*样本呢？好吧，结果证明，通常一个样本就足够了（我们稍后会回到这个点）。
- en: 'To digest it a bit, let’s look at the code in listing 8.1 and imagine a forward
    pass through the NN shown in figure 8.4, including the evaluation of the loss
    in the training set. Let’s start with a vector w that holds the four fixed values:
    *w* = (*μ**[a]* , *w*[1] , *μ**[b]* , *w*[3]) . These values control the variational
    parameters (see the calculations of `sigma_a` , `mu_a` , `sigma_sig` , and `mu_sig`
    in listing 8.1) via *λ* = (*μ**[a]* , *sp*(*w*[1]), *μ**[b]* , *sp*(*w*[3])),
    where sp is the softplus. The variational distributions *N*(*μ**[a]* , *σ**[a]*)
    and *N*(*μ**[b]* , *σ**[b]*) are now fixed, and we can calculate the regularizing
    loss component (see the first component in equation 8.2) via equation 8.3\. As
    a starting point, we choose for all four parameters 0.1, yielding `loss_kl = -0.5`(see
    listing 8.1). Next, we calculate the NLL part of the loss function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们看看列表8.1中的代码，并想象一下通过图8.4所示的NN进行的前向传递，包括在训练集中评估损失。让我们从一个包含四个固定值的向量w开始：*w*
    = (*μ**[a]* , *w*[1] , *μ**[b]* , *w*[3])。这些值通过*λ* = (*μ**[a]* , *sp*(*w*[1]),
    *μ**[b]* , *sp*(*w*[3]))控制变分参数（参见列表8.1中`sigma_a`、`mu_a`、`sigma_sig`和`mu_sig`的计算），其中sp是softplus。变分分布*N*(*μ**[a]*
    , *σ**[a]*)和*N*(*μ**[b]* , *σ**[b]*)现在是固定的，我们可以通过方程式8.3计算正则化损失成分（参见方程式8.2中的第一个成分）。作为一个起点，我们为所有四个参数选择0.1，得到`loss_kl
    = -0.5`（参见列表8.1）。接下来，我们计算损失函数的NLL部分：
- en: '![](../Images/8-4_E01.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-4_E01.png)'
- en: 'where *N*(*y* ; *μ* , *σ*) is the density function of a Normal distribution.
    To approximate this NLL term, we sample a single value for a, b. Using this sample,
    we’re now fixed in a non-Bayesian NN, and you can calculate the NLL as usual (see
    listing 8.1). First, you choose the appropriate TFP distribution for the outcome:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N*(*y* ; *μ* , *σ*) 是正态分布的密度函数。为了近似这个NLL项，我们对a、b采样一个值。使用这个样本，我们现在固定在一个非贝叶斯NN中，你可以像往常一样计算NLL（参见列表8.1）。首先，你选择适当的TFP分布来表示结果：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, having the correct distribution, you calculate the NLL by summing overall
    training examples via:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，有了正确的分布，你通过以下方式计算NLL，即通过求和所有训练示例：
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We add the two loss components (`loss_kl` and `loss_nll`) together to get the
    final loss. Are we done? Almost. We have the power of TensorFlow and can calculate
    the derivations of the loss with respect to (w.r.t.) *w* = (*μ**[a]* , *w*[1]
    , *μ**[b]* , *w*[3]) and update these. But reality bites, and there’s a subtle
    problem.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将两个损失成分（`loss_kl`和`loss_nll`）相加以得到最终的损失。我们完成了吗？几乎。我们有TensorFlow的力量，可以计算损失相对于(w.r.t.)
    *w* = (*μ**[a]* , *w*[1] , *μ**[b]* , *w*[3])的导数，并更新这些值。但现实很残酷，存在一个微妙的问题。
- en: Let’s say we want to calculate the derivative of the loss w.r.t. the weight
    *μ* *a* = w[0], which gives the mean of the distribution of the slope *a* ~ *N*(*μ**[a]*
    , *σ**[a]*). In figure 8.5, on the left, you see the relevant part of the computational
    graph for sampling the slope parameter a from its variational distribution *N*(*μ**[a]*
    , *σ**[a]*). The part of the graph for the parameter *b* is analog. You remember
    from chapter 3 that you have to calculate the local gradient of the output w.r.t.
    the input. Easy. Just calculate the derivative of the density of *N*(*μ**[a]*
    , *σ**[a]*) w.r.t. *μ**[a]* . Wait--a is sampled from a Gaussian. How do we calculate
    the derivative through a sampled variable? This isn’t possible because the value
    of a is random and we don’t know at which position to take the derivative of the
    normal density.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要计算损失关于权重 *μ* *a* = w[0] 的导数，这给出了斜率 *a* ~ *N*(*μ**[a]* , *σ**[a]*) 分布的均值。在图
    8.5 的左侧，你可以看到从其变分分布 *N*(*μ**[a]* , *σ**[a]*) 中采样斜率参数 a 的相关计算图的部分。参数 *b* 的图部分类似。你还记得从第
    3 章中，你必须计算输出 w 关于输入的局部梯度。很简单。只需计算 *N*(*μ**[a]* , *σ**[a]*) 的密度关于 *μ**[a]* 的导数。等等--a
    是从高斯分布中采样的。我们如何通过采样变量来计算导数？这是不可能的，因为 a 的值是随机的，我们不知道在哪个位置取正态密度的导数。
- en: '![](../Images/8-5.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-5.png)'
- en: Figure 8.5 The reparameterization trick. Because backpropagation (here shown
    for *∂* /*∂μ**[a]*) doesn’t work through a random variable such as *a* ~ *N*(*μ**[a]*
    , *σ**[a]*) (left plot), we use the reparameterization trick (right plot). Instead
    of sampling a from *a* ~ *N*(*μ**[a]* , *σ**[a]*), we calculate a as *a* = *μ**[a]*
    + *σ**[a]* ⋅ ϵ with ϵ sampled from a standard Normal distribution ϵ ∼ *N*(0, 1)
    that has no tunable parameters. In the end, when using the reparameterization
    trick (right), a is again Normal distributed according to *a* ~ *N*(*μ**[a]* ,
    *σ**[a]*), but we can backpropagate through to get *∂* /*∂μ**[a]* and *∂* /*∂σ**[a]*
    because the random variable ϵ ∼ *N*(0, 1) doesn’t need to be updated.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 重参数化技巧。因为反向传播（此处显示为 *∂* /*∂μ**[a]*）不能通过随机变量如 *a* ~ *N*(*μ**[a]* , *σ**[a]*)（左图）进行，所以我们使用重参数化技巧（右图）。我们不是从
    *a* ~ *N*(*μ**[a]* , *σ**[a]*) 中采样 a，而是计算 a = *μ**[a]* + *σ**[a]* ⋅ ϵ，其中 ϵ 是从标准正态分布采样的，即
    ϵ ∼ *N*(0, 1)，它没有可调参数。最终，当使用重参数化技巧（右）时，a 再次根据 *a* ~ *N*(*μ**[a]* , *σ**[a]*) 正态分布，但我们可以反向传播以获得
    *∂* /*∂μ**[a]* 和 *∂* /*∂σ**[a]*，因为随机变量 ϵ ∼ *N*(0, 1) 不需要更新。
- en: We can’t calculate the derivative of *a* ~ *N*(*μ**[a]* , *σ**[a]*) w.r.t. *σ**[a]*
    or *μ**[a]* . Is all lost? In 2013, Kingma and Welling found the solution for
    that dilemma, but many others independently found that solution as well. Instead
    of sampling *a* ~ *N*(*μ**[a]* , *σ**[a]*), you can calculate *a**[rep]* = *μ**[a]*
    + *σ**[a]* ⋅ ϵ and then sample ϵ ∼ *N*(0, 1). You can check in the notebook [http://mng.bz/4A5R](http://mng.bz/4A5R)
    that *a**[rep]* ∼ *N*(*μ**[a]* , *σ**[a]*) or *μ**[a]*. (We do not need a backpropagation
    w.r.t. ε.) Figure 8.5 shows the reparameterization on the right. Now we’re done,
    and we have a working solution to calculate the gradients. This listing shows
    the complete code.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法计算 *a* ~ *N*(*μ**[a]* , *σ**[a]*) 关于 *σ**[a]* 或 *μ**[a]* 的导数。一切都没了？在 2013
    年，Kingma 和 Welling 找到了这个困境的解决方案，但许多人也独立地找到了这个解决方案。与其从 *a* ~ *N*(*μ**[a]* , *σ**[a]*)
    中采样，你可以计算 *a**[rep]* = *μ**[a]* + *σ**[a]* ⋅ ϵ，然后采样 ϵ ∼ *N*(0, 1)。你可以在笔记本 [http://mng.bz/4A5R](http://mng.bz/4A5R)
    中检查 *a**[rep]* ∼ *N*(*μ**[a]* , *σ**[a]*) 或 *μ**[a]*。 (我们不需要关于 ε 的反向传播。)图 8.5
    显示了右侧的重参数化。现在我们完成了，我们有一个可以计算梯度的有效解决方案。这个列表显示了完整的代码。
- en: Listing 8.1 Using VI for the simple linear regression example (full code)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 使用变分推断 (VI) 对简单线性回归示例进行计算（完整代码）
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The initial condition of the vector w
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向量 w 的初始条件
- en: ❷ The noise term, needed for the variational trick
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 噪声项，用于变分技巧
- en: ❸ Controls the center of parameter a
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 控制参数 a 的中心
- en: ❹ Controls the spread of parameter a
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 控制参数 a 的分布范围
- en: ❺ Controls the center of parameter b
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 控制参数 b 的中心
- en: ❻ Controls the spread of parameter b
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 控制参数 b 的分布范围
- en: ❼ KL divergence with Gaussian priors
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ KL 散度与高斯先验
- en: ❽ Samples a ~ *N*(mu_a, sigma_a) with the reparameterization trick
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用重参数化技巧采样 a ~ *N*(mu_a, sigma_a)
- en: ❾ Samples *b* ~ *N*(mu_b, sigma_b)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 样本 *b* ~ *N*(mu_b, sigma_b)
- en: ❿ Calculates the NLL
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 计算负对数似然 (NLL)
- en: ⓫ Gradient descent
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 梯度下降
- en: In figure 8.6, you can see how the parameters *μ* a and *μ* *b* converge during
    the training of the Bayesian model via the approximation VI method to the same
    values that we got analytically without using approximations (see section 7.3.3).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.6中，你可以看到参数 *μ* a 和 *μ* *b* 在通过变分推断方法近似贝叶斯模型训练过程中的收敛情况，这些值与我们没有使用近似方法解析得到的值非常接近（参见第7.3.3节）。
- en: '![](../Images/8-6.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-6.png)'
- en: Figure 8.6 The variational parameters *μ* a (upper curve) and *μ* *b* (lower
    curve) during several epochs converging close to the analytical solution
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 在几个周期中收敛到解析解的变分参数 *μ* a（上曲线）和 *μ* *b*（下曲线）
- en: 'Let’s recap how the estimation of the variational parameters was done in our
    simple regression example. The estimation minimized the loss that was derived
    for the VI method expression (equation 8.2, shown here again):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下在简单回归示例中如何估计变分参数。估计过程最小化了为变分推断方法表达式（方程8.2，此处再次展示）推导出的损失：
- en: loss[VI] = loss[KL] + loss[NLL] = KL [*q**[λ]*(*θ*)|| *P*(*θ*)] − *E**[θ]**[∼]**[qλ]*
    [log(*P*(*D*|*θ*)] Equation 8.2 (repetate*D*)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: loss[VI] = loss[KL] + loss[NLL] = KL [q**[λ]*(*θ*)|| *P*(*θ*)] − *E**[θ]**[∼]**[qλ]*
    [log(*P*(*D*|*θ*)] 方程8.2 (重复*D*)
- en: We used gradient descent to optimize the variational parameters *λ* = (*μ**[a]*
    , *sp*(*w*[1]), *μ**[b]* , *sp*(*w*[3])). The expectation *E**[θ]**[∼]**[qλ]*
    [log(*P*(*D*|*θ*)] can be approximated by the average of different log(*P*(*D*|*θ*)
    values, each corresponding to a different *θ* sampled from q*λ* . In principle,
    an infinite number of different *θ* ’s must be drawn from the distribution q*λ*
    and averaged to perfectly reproduce the expectation. This is the law of large
    numbers. However, in a practical sense, only a single draw of *θ* is done; the
    resulting log(*P*(*D*|*θ*) is then taken as an approximation for *E**[θ]**[∼]**[qλ]*
    [log(*P*(*D*|*θ*)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用梯度下降来优化变分参数 *λ* = (*μ**[a]* , *sp*(*w*[1]), *μ**[b]* , *sp*(*w*[3])). 期望
    *E**[θ]**[∼]**[qλ]* [log(*P*(*D*|*θ*)] 可以通过不同 log(*P*(*D*|*θ*) 值的平均值来近似，每个值对应于从
    q*λ* 中采样的不同 *θ*。原则上，必须从分布 q*λ* 中抽取无限多个不同的 *θ* 并取平均值，以完美地再现期望。这是大数定律。然而，在实际意义上，只进行一次
    *θ* 的抽样；然后，将得到的 log(*P*(*D*|*θ*) 作为 *E**[θ]**[∼]**[qλ]* [log(*P*(*D*|*θ*)] 的近似。
- en: In our example, we drew a single realization *θ* = (*a, b*). Then we took the
    gradient of equation 8.1 w.r.t. the variational parameters (location, *μ* a, *μ*
    *b* , and scale, *σ**[a]* , *σ**[b]* , of the distributions *a* ∼ *N*(*μ**[a]*
    , *σ**[a]*) and *a* ∼ *N*(*μ**[b]* , *σ**[b]*). The gradient didn’t point to the
    right direction of the steepest descent. Only the average of an infinite number
    of gradients would do so. Does this algorithm make sense? Let’s see if a single
    draw still makes sense.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们绘制了一个单个实现 *θ* = (*a, b*)。然后我们取方程8.1关于变分参数（位置，*μ* a，*μ* *b*，以及分布 *a*
    ∼ *N*(*μ**[a]* , *σ**[a]*) 和 *a* ∼ *N*(*μ**[b]* , *σ**[b]*) 的尺度，*σ**[a]* ，*σ**[b]*）的梯度。梯度没有指向最速下降的正确方向。只有无限多个梯度的平均值才能做到这一点。这个算法有意义吗？让我们看看单个抽样是否仍然有意义。
- en: '![](../Images/8-7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-7.png)'
- en: Figure 8.7 Training the variational parameters. To train a BNN means to learn
    the distributions for *θ* = (*a, b*). The simplest BNN (shown in figure 8.4) has
    only two parameters, *a* and *b*, for which with VI, a Gaussian distribution is
    assumed:*a* ∼ *N*(*μ**[a]* , *σ**[a]*), *a* ∼ *N*(*μ**[b]* , *σ**[b]*).. Here
    the location parameters *μ* a and *μ* *b* are shown during several training epochs.
    The dot is the current value of the variational parameters ( *μ* a, *μ* b) at
    a given epoch. Also shown is the negative gradient (the line pointing away from
    the dot). [https://youtu.be/MC_5Ne3Dj6g](https://youtu.be/MC_5Ne3Dj6g) provides
    an animated version.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 训练变分参数。训练一个贝叶斯神经网络意味着学习 *θ* = (*a, b*) 的分布。最简单的贝叶斯神经网络（如图8.4所示）只有两个参数，*a*
    和 *b*，对于这些参数，使用变分推断，假设高斯分布：*a* ∼ *N*(*μ**[a]* , *σ**[a]*), *a* ∼ *N*(*μ**[b]*
    , *σ**[b]*)。在这里，位置参数 *μ* a 和 *μ* *b* 在几个训练周期中显示出来。点表示给定周期中变分参数（ *μ* a, *μ* b）的当前值。还显示了负梯度（从点指向的线）。[https://youtu.be/MC_5Ne3Dj6g](https://youtu.be/MC_5Ne3Dj6g)
    提供了一个动画版本。
- en: Although the gradient is noisy, it still finds its way to the minimum. Calculating
    a more accurate gradient by summing over many iterations isn’t necessary and would
    be a waste of computations. The direction would be more accurate, however, having
    less fluctuation in the arrow (figure 8.7 and go watch the animation), but a rough
    and fast estimate is also fine and much quicker to calculate. The trick of replacing
    the expectation with only one evaluation is also used for different occasions
    in DL research like reinforcement learning or variational autoencoders.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然梯度是有噪声的，但它仍然找到了最小值。通过在许多迭代中求和来计算更精确的梯度是不必要的，而且会浪费计算资源。然而，方向会更准确，箭头（如图8.7和观看动画）的波动会更小，但一个粗糙且快速的估计也是可以接受的，并且计算得更快。用仅一次评估来替换期望的技巧也用于DL研究中的不同场合，如强化学习或变分自动编码器。
- en: 8.3 Variational inference with TensorFlow Probability
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 使用TensorFlow Probability进行变分推断
- en: TFP has several methods to build VI BNNs, so let’s first see how simple it is
    to build a VI network in TFP. There is a class, `tfp.layers.DenseReparameterization`
    , that you can use like a standard Keras layer, stacking one layer after another
    to build a fully connected Bayesian network. Listing 8.2 shows the code for the
    network shown in figure 8.2\. Can you guess how many parameters this Bayesian
    network has? Note that the bias terms have no variational distribution and, thus,
    only one weight.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: TFP有几种方法来构建VI BNNs，所以让我们首先看看在TFP中构建VI网络有多简单。有一个类，`tfp.layers.DenseReparameterization`，你可以像使用标准的Keras层一样使用它，一层接一层地堆叠来构建一个完全连接的贝叶斯网络。列表8.2显示了图8.2中所示网络的代码。你能猜出这个贝叶斯网络有多少个参数吗？请注意，偏差项没有变分分布，因此只有一个权重。
- en: Listing 8.2 Setting up a VI network with three layers
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2 设置具有三层结构的VI网络
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the default setting, `DenseReparameterization` includes the bias as a fixed
    parameter and not as a distribution. Therefore, the bias (edge from 1 to the output
    in figure 8.4) carries only one parameter. The node from *x* to the output is
    modeled with a Gaussian requiring two parameters (one for the center and one for
    the scale), so the first layer has three parameters to learn. Similarly, the second
    layer has two bias terms and two edges resulting in 2 + 2 ⋅ 2 = 6 parameters.
    Finally, the last layer has three biases and six edges resulting in 3 + 2 ⋅ 6
    = 15 parameters. In total, this network has 24 parameters. Going Bayesian roughly
    requires two times the number of parameters compared to a standard NN. As a little
    exercise, copy the code to a notebook and call it model.summary().
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认设置下，`DenseReparameterization` 将偏差作为一个固定参数而不是分布。因此，偏差（如图8.4中的从1到输出的边缘）只携带一个参数。从
    *x* 到输出的节点使用高斯分布建模，需要两个参数（一个用于中心，一个用于尺度），因此第一层有三个参数需要学习。同样，第二层有两个偏差项和两个边缘，导致 2
    + 2 ⋅ 2 = 6 个参数。最后，最后一层有三个偏差和六个边缘，导致 3 + 2 ⋅ 6 = 15 个参数。总共，这个网络有24个参数。采用贝叶斯方法大约需要比标准神经网络多两倍的参数数量。作为一个小练习，将代码复制到笔记本中，并命名为
    model.summary()。
- en: TFP is also quite flexible. Let’s rebuild the linear regression problem from
    section 7.3.3, in which we replace the slope a and intercept by distributions,
    and *σ* is assumed to be known. To be compatible with TFP, we need to translate
    the model into a variational Bayes network (see figure 8.4). To fully define our
    network, we also need to state our priors for the two variables *a* and *b*. We
    choose the normally distributed *N*(0,1).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: TFP也非常灵活。让我们重新构建7.3.3节中的线性回归问题，其中我们将斜率a和截距替换为分布，并且假设 *σ* 是已知的。为了与TFP兼容，我们需要将模型转换为变分贝叶斯网络（见图8.4）。为了完全定义我们的网络，我们还需要声明两个变量
    *a* 和 *b* 的先验分布。我们选择正态分布 *N*(0,1)。
- en: 'You’ll find the code for the network in listing 8.3\. In contrast to TFP’s
    default setting, we want the bias to also be a distribution. We thus need to overwrite
    it by setting `bias_prior_fn=normal` and `bias_posterior_fn=normal` in the constructor
    of the `DenseReparameterization` layer. Further, there is currently the following
    oddity (not to call it a bug) in TFP layers. This loss (shown in equation 8.1)
    consists of the usual NLL and an additional term called KL divergence. Normally,
    you take KL divergence and the NLL using the sum of overall training data (see
    listing 8.3). In DL, however, it’s quite common to calculate the mean NLL per
    training example (the summed NLL divided by the number of training examples).
    This is fine. We just need the minimum of the loss function, which doesn’t change
    if we divide the loss by a constant. (We also did this in all examples up to now.)
    But now, the KL divergence needs to be transformed to the mean per our training
    example as well, and this is currently not the case in TFP. Digging deeper into
    TFP’s documentation, you find the somewhat cryptic statement in `DenseReparameterization`([http://mng.bz/Qyd6](http://mng.bz/Qyd6))
    and also in the corresponding convolutional layers (`Convolution1DReparameterization`
    , `Convolution2DReparameterization` , and `Convolution3DReparameterization`):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在列表8.3中找到网络的代码。与TFP的默认设置相反，我们希望偏差也是一个分布。因此，我们需要通过在`DenseReparameterization`层的构造函数中设置`bias_prior_fn=normal`和`bias_posterior_fn=normal`来覆盖它。此外，TFP层目前存在以下奇特之处（暂不称之为错误）。这个损失（如方程8.1所示）由通常的NLL和一个称为KL散度的附加项组成。通常，你使用整体训练数据的总和来取KL散度和NLL（参见列表8.3）。然而，在深度学习中，计算每个训练示例的平均NLL（总和NLL除以训练示例的数量）是非常常见的。这是可以的。我们只需要损失函数的最小值，这不会因为除以一个常数而改变。（我们到目前为止的所有示例中都这样做过。）但现在，KL散度也需要转换为我们训练示例的平均值，而TFP目前并没有这样做。深入挖掘TFP的文档，你会在`DenseReparameterization`([http://mng.bz/Qyd6](http://mng.bz/Qyd6))以及相应的卷积层（`Convolution1DReparameterization`、`Convolution2DReparameterization`和`Convolution3DReparameterization`）中找到一些有些神秘的声明：
- en: When doing minibatch stochastic optimization, make sure to scale this loss such
    that it is applied just once per epoch (e.g., if kl is the sum of losses for each
    element of the batch, you should pass kl / num_examples_per_epoch to your optimizer).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行小批量随机优化时，确保将此损失缩放，以便每个epoch只应用一次（例如，如果kl是批中每个元素的损失总和，你应该将kl / num_examples_per_epoch传递给你的优化器）。
- en: 'To address this, one needs to also divide the KL term by the number of training
    data (`num`). This can be done via the following lines of code (the next listing
    shows this fix):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，需要将KL项也除以训练数据数量（`num`）。可以通过以下代码行来完成（下一个列表显示了这种修复）：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Listing 8.3 Coding our simple network from figure 8.4
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3：从图8.4编码我们的简单网络
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The usual NLL loss for a Gaussian distribution with fixed variance
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于具有固定方差的高斯分布，通常的NLL损失
- en: ❷ Rescales KL divergence term (kind of a bug fix for TFP)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重缩放KL散度项（对TFP的一种错误修复）
- en: ❸ TFP usually doesn’t assume distributions on the bias; we overwrite this here.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ TFP通常不假设偏差上有分布；我们在这里覆盖了这一点。
- en: TFP also offers layers to build convolutional BNNs using VI. For the usual 2D
    convolutions, these layers are called `Convolution2DReparameterization`. There
    are also 1D and 3D variants named `Convolution1DReparameterization` and `Convolution3DReparameterization`.
    Further, for the dense and convolutional BNNs, there are some special classes
    and some advanced VI methods in TFP. Most notably, `DenseFlipout` can be used
    as a drop-in replacement for `DenseReparameterization`. `DenseFlipout` uses a
    trick that speeds up learning. The flip-out trick is also available for convolutions
    (see, for example, `Convolution2DFlipout`). The flip-out method is described in
    detail in the paper by *y*. Wen, P. Vicol, et al., at [https://arxiv.org/abs/1803.04386](https://arxiv.org/abs/1803.04386)
    . In the notebook [http://mng.bz/MdmQ](http://mng.bz/MdmQ) , you use these layers
    to build a BNN for the CIFAR-10 data set (see figure 8.9).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: TFP还提供了用于构建使用变分推断（VI）的卷积贝叶斯神经网络（BNN）的层。对于常规的2D卷积，这些层被称为`Convolution2DReparameterization`。还有1D和3D变体，分别命名为`Convolution1DReparameterization`和`Convolution3DReparameterization`。此外，对于密集和卷积BNN，TFP中还有一些特殊类和一些高级VI方法。最值得注意的是，`DenseFlipout`可以用作`DenseReparameterization`的替代品。`DenseFlipout`使用一种加快学习的技巧。翻转技巧也适用于卷积（例如，参见`Convolution2DFlipout`）。翻转方法在Wen,
    P. Vicol等人发表的论文中有详细描述，该论文可在[https://arxiv.org/abs/1803.04386](https://arxiv.org/abs/1803.04386)找到。在笔记本[http://mng.bz/MdmQ](http://mng.bz/MdmQ)中，你使用这些层来构建CIFAR-10数据集的BNN（见图8.9）。
- en: 8.4 MC dropout as an approximate Bayes approach
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 MC dropout 作为一种近似贝叶斯方法
- en: In section 8.3, you took a first glimpse at a BNN approximated by VI. VI allows
    you to fit a Bayesian DL model by learning an approximative posterior distribution
    for each weight. The default in TFP is to approximate the posterior by a Gaussian.
    These BNNs have twice as many parameters compared to their non-Bayesian versions,
    because each weight is replaced by a Gaussian weight distribution that’s defined
    by two parameters (mean and standard deviation). It’s great that VI lets us fit
    a BNN despite its huge number of parameters. But it would be even nicer if the
    number of parameters wouldn’t double when going from a non-Bayesian NN to its
    Bayesian variant. Fortunately, that’s possible with an easy method called MC dropout.
    (MC stands for Monte Carlo and insinuates that a random process is involved as
    in a Monte Carlo casino.) In 2015, a PhD student, Yarin Gal, was able to show
    that the dropout method was similar to VI, allowing us to approximate a BNN. But
    before turning to a dropout as a Bayesian method, let’s see how the dropout was
    introduced.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8.3节中，您首次了解了通过变分推断（VI）近似的一个贝叶斯神经网络（BNN）。VI允许您通过学习每个权重的近似后验分布来拟合贝叶斯深度学习模型。在TFP中，默认使用高斯分布来近似后验。与它们的非贝叶斯版本相比，这些BNN具有两倍多的参数，因为每个权重都被一个由两个参数（均值和标准差）定义的高斯权重分布所取代。VI让我们能够拟合一个具有大量参数的BNN是非常棒的。但如果从非贝叶斯神经网络到其贝叶斯变体时参数数量不会加倍那就更好了。幸运的是，使用一个名为MC
    dropout的简单方法就可以实现这一点。（MC代表蒙特卡洛，暗示其中涉及一个随机过程，就像蒙特卡洛赌场一样。）2015年，一名博士生Yarin Gal能够证明dropout方法与VI相似，允许我们近似BNN。但在转向dropout作为贝叶斯方法之前，让我们看看dropout是如何被引入的。
- en: 8.4.1 Classical dropout used during training
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 训练过程中使用的经典dropout
- en: Dropout during training was introduced as a simple way to prevent an NN from
    overfitting. (This was even the title of the paper when Srivastava et al., presented
    the method in 2014.) How does it work? When doing dropout during training, you
    set some randomly picked neurons in the NN to zero. You do that in each update
    run. Because you actually drop neurons, the weights of all connections that start
    from the dropped neuron are simultaneously dropped (see figure 8.8).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的dropout被引入作为一种防止神经网络过拟合的简单方法。（这甚至是Srivastava等人于2014年介绍该方法时的论文标题。）它是如何工作的？在训练过程中进行dropout时，您将神经网络中随机选择的某些神经元设置为0。您在每个更新运行中都这样做。因为您实际上丢弃了神经元，所以从丢弃的神经元起始的所有连接的权重同时被丢弃（见图8.8）。
- en: '![](../Images/8-8.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-8.png)'
- en: 'Figure 8.8 Three NNs: a) shows the full NN with all neurons, and b) and c)
    show two versions of a thinned NN where some neurons are dropped. Dropping neurons
    is the same as setting all connections that start from these neurons to zero.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 三个神经网络：a)显示了包含所有神经元的完整神经网络，b)和c)显示了两种变薄的神经网络版本，其中一些神经元被丢弃。丢弃神经元等同于将这些神经元起始的所有连接设置为0。
- en: In Keras, this can easily be done by adding a dropout layer after a weight layer
    and giving the dropout the probability p * (here we use p * to indicate that this
    is an MC drop probability, which sets the weight to zero) as an argument (see
    the following listing). During training, the dropout is often only used in the
    fully connected layers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，这可以通过在权重层之后添加一个dropout层并给dropout提供一个概率p（在这里我们使用p *来表示这是一个MC drop概率，它将权重设置为0）作为参数来实现（见以下列表）。在训练过程中，dropout通常仅在全连接层中使用。
- en: Listing 8.4 Defining and training a classification CNN with dropout layers
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 定义和训练带有dropout层的分类卷积神经网络
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Dropout layer that sets each neuron of the former weight layer to 0 with a
    probability of 0.5
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以0.5的概率将前一层中每个神经元设置为0的dropout层
- en: Before we discuss the dropout method in more detail, you can easily convince
    yourself that dropout successfully fights overfitting by working through the following
    notebook. In the notebook, you develop a classification convolutional neural network
    (CNN) for the CIFAR-10 data set that has 50,000 images and 10 classes (see figure
    8.9).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们更详细地讨论dropout方法之前，您可以通过以下笔记本轻松地让自己相信dropout成功地防止了过拟合。在笔记本中，您为具有50,000张图像和10个类别的CIFAR-10数据集开发了一个分类卷积神经网络（CNN）（见图8.9）。
- en: '![](../Images/8-9.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-9.png)'
- en: Figure 8.9 Example images for the ten classes in the CIFAR-10 data set
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 CIFAR-10数据集中十个类别的示例图像
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/XP29](http://mng.bz/XP29)
    . This notebook uses (classical) dropout to fight overfitting while training a
    DL model for the CIFAR-10 classification.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/XP29](http://mng.bz/XP29)
    。这个笔记本使用（经典）dropout来对抗在训练用于 CIFAR-10 分类深度学习模型时的过拟合。'
- en: Check that the loss curves of the training data are valid with and without dropout
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查训练数据的损失曲线是否在带和不带dropout的情况下有效
- en: Check the accuracy with and without dropout
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查带和不带dropout的准确度
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In the notebook [http://mng.bz/4A5R](http://mng.bz/4A5R) , you saw that the
    dropout during training prevents overfitting and even improves the accuracy. For
    this exercise, see figure 8.10, where the results of the notebook are displayed.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本 [http://mng.bz/4A5R](http://mng.bz/4A5R) 中，你看到训练过程中的dropout可以防止过拟合，甚至可以提高准确度。为此练习，请看图8.10，其中显示了笔记本的结果。
- en: '![](../Images/8-10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![8-10.png]'
- en: Figure 8.10 CIFAR-10 results achieved with and without using dropout during
    training. During testing, the weights are fixed (no MC dropout). When using dropout,
    the accuracy on the validation data is higher (left panel), and the distance between
    validation and trained loss is much smaller (right panel). This indicates that
    dropout prevents overfitting.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10展示了在训练过程中使用和不使用dropout时在CIFAR-10上实现的成果。在测试时，权重是固定的（没有MC dropout）。当使用dropout时，验证数据的准确度更高（左面板），并且验证损失与训练损失之间的距离要小得多（右面板）。这表明dropout可以防止过拟合。
- en: Let’s see how dropout during training works. In each training loop, you apply
    dropout and get another thinned version of the NN (see figure 8.10). Why does
    dropout help to prevent overfitting? One reason is that you train many thinned
    NN versions, which have fewer parameters than the full NN. In each step, you only
    update the weights that weren’t dropped. Overall, you train not just a single
    version of the network but an ensemble of thinned versions of the NN that share
    weights. Another reason is that fewer complex features are learned when using
    dropout. Because dropout forces the NN to cope with missing information, it yields
    more robust and independent features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练过程中的dropout是如何工作的。在每个训练循环中，你应用dropout并获得NN的另一个更薄版本（见图8.10）。为什么dropout有助于防止过拟合？一个原因是你训练了许多更薄的NN版本，这些版本比完整神经网络有更少的参数。在每一步中，你只更新未被丢弃的权重。总的来说，你不仅训练了一个网络的单个版本，而且训练了一个共享权重的NN薄版本集合。另一个原因是使用dropout时学习的复杂特征更少。因为dropout迫使神经网络处理缺失信息，它产生了更稳健和独立的特征。
- en: 'How do you use a dropout-trained NN during testing? It’s simple. You go back
    to the full NN with fixed weights. There’s only one detail: you need to downweigh
    the learned weight values to *w*^* = *p*^* ⋅ *w* . This downweighting accounts
    for the fact that during training each neuron gets, on average, p * less inputs
    than in the full NN. The connections are, thus, stronger than these would be if
    no dropout is applied. During the application phase, there’s no dropout and, hence,
    you downweigh the too strong weights by multiplying those with p *. Fortunately,
    you don’t have to adjust the weights manually; Keras takes care of that. When
    using a dropout-trained NN during testing and applying the predict command on
    new input, Keras does the correct thing.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在测试时使用经过dropout训练的神经网络？很简单。你回到具有固定权重的完整神经网络。只有一个细节：你需要将学习到的权重值按 *w*^* = *p*^*
    ⋅ *w* 的方式降低权重。这种降低权重的做法是为了考虑到在训练过程中，每个神经元平均接收到的输入比完整神经网络少 p *。因此，连接比没有应用dropout时更强。在应用阶段，没有dropout，因此你通过乘以
    p * 来降低过强的权重。幸运的是，你不需要手动调整权重；Keras 会处理这个问题。当在测试时使用经过dropout训练的神经网络并应用预测命令对新输入进行操作时，Keras
    会做正确的事情。
- en: 8.4.2 MC dropout used during train and test times
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 训练和测试时使用的 MC dropout
- en: As you saw in section 8.4.1, dropout during training can easily enhance the
    prediction performance. For this reason, it quickly became popular in the DL community
    and is still widely used. But dropout has even more to offer. Turn on dropout
    during testing and you can use it as a BNN! Let’s see how this works.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在8.4.1节中看到的，训练过程中的dropout可以轻易地增强预测性能。因此，它很快在深度学习社区中变得流行，并且仍然被广泛使用。但dropout还有更多可以提供的。在测试时开启dropout，你可以将其用作贝叶斯神经网络！让我们看看这是如何工作的。
- en: 'In a BNN, a distribution of weights replaces each weight. With VI, you used
    a Gaussian weight distribution with two parameters (mean and standard deviation).
    When using dropout, you also have a weight distribution, but now the weight distribution
    is simpler and essentially consists of only two values: 0 or w (see figure 8.11).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯神经网络（BNN）中，每个权重都被一个分布所代替。使用变分推断（VI）时，你使用了一个具有两个参数（均值和标准差）的高斯权重分布。当使用dropout时，你也有一个权重分布，但现在权重分布更简单，本质上只包含两个值：0或w（见图8.11）。
- en: 'The dropout probability p * isn’t a parameter but is fixed when you define
    the NN (for example, *p*^* = 0.3). The dropout probability is a tuning parameter:
    if you don’t get good results when using *p*^* = 0.3 , you can try it with another
    dropout rate. The only parameter of the weight distribution in the MC dropout
    method is the value w (see figure 8.11 and, for details, see the MC dropout paper
    by Yarin Gal and Zoubin Ghahramani at [https://arxiv.org/abs/1506.02157](https://arxiv.org/abs/1506.02157)).
    Learning the parameter w works as usual. You turn on dropout during training and
    use the usual NLL loss function, which you minimize by tuning the weights via
    stochastic gradient descent (SG*D*).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: dropout概率p *不是一个参数，而是在定义神经网络时固定的（例如，*p*^* = 0.3）。dropout概率是一个调整参数：如果你使用*p*^*
    = 0.3时没有得到好的结果，你可以尝试另一个dropout率。MC dropout方法中权重分布的唯一参数是w的值（见图8.11，详情请参阅Yarin Gal和Zoubin
    Ghahramani在[https://arxiv.org/abs/1506.02157](https://arxiv.org/abs/1506.02157)上发表的MC
    dropout论文）。学习参数w的过程与通常一样。你在训练时开启dropout，并使用常规的NLL损失函数，通过调整权重来最小化它，这通常是通过随机梯度下降（SG*D*）来实现的。
- en: As a side remark, in Gal’s dropout paper, the loss consists of the NLL and an
    additional regularization term that penalizes large weights. We’ll skip this regularization
    term as most practitioners in the field do. It’s simply not needed in practice.
    In Gal’s framework, dropout is similar to the VI inference fitting a BNN method
    from section 8.2, but this time, the distribution from figure 8.11 is taken instead
    of the Gaussian usually used in the VI method. By updating the w values, you actually
    learn the weight distributions that approximate the weight posteriors (see figure
    8.11).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作为附带说明，在Gal的dropout论文中，损失包括负对数似然（NLL）和一个额外的正则化项，该正则化项惩罚大的权重。我们将跳过这个正则化项，因为该领域的多数实践者都不需要它。在实践中，这根本不是必需的。在Gal的框架中，dropout类似于8.2节中提到的从变分推断（VI）拟合贝叶斯神经网络（BNN）的方法，但这次使用的是图8.11中的分布，而不是VI方法中通常使用的高斯分布。通过更新w值，你实际上学习的是近似权重后验的权重分布（见图8.11）。
- en: '![](../Images/8-11.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-11.png)'
- en: Figure 8.11 The (simplifie*D*) weight distribution with MC dropout. The dropout
    probability p* is fixed when defining the NN. The only parameter in this distribution
    is the value of w.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 使用MC dropout的（简化**D**）权重分布。在定义神经网络时，dropout概率p*是固定的。这个分布中唯一的参数是w的值。
- en: How do you use the dropout-trained NN as a BNN? As for all Bayesian models,
    you get the predictive distribution by averaging over the weight distribution
    (see equation 7.6).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何使用dropout训练的神经网络作为BNN？对于所有贝叶斯模型，你通过在权重分布上取平均来获得预测分布（参见方程7.6）。
- en: '*p* ( *y* | *x**[test]* , *D* ) = ∑*[i]* *p* ( *y* | *x**[test]* , *w* *[i]*
    ) ⋅ *p* ( *w**[i]* | *D* ) Equation 8.4'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* ( *y* | *x**[test]* , *D* ) = ∑*[i]* *p* ( *y* | *x**[test]* , *w* *[i]*
    ) ⋅ *p* ( *w**[i]* | *D* ) 方程式8.4'
- en: 'To get the predictive distribution in a dropout-trained NN, you turn on dropout
    also during test time. Then, with the same input, you can make several predictions
    where each prediction results from another dropout variant of the NN. The predictions
    you get in this manner approximate the predictive distribution in equation 8.4
    as you will see in the next couple of lines. More precisely, you predict for the
    same input *x**[test]* T-times a conditional probability distribution (CPD) *p*
    ( *y* | *x**[test]* , *w* *[i]* ) . For each prediction, you get different CPDs,
    (*p* ( *y* | *x* , *w* *[i]* )), corresponding to a sampled weight constellation
    wi (see the following case studies). It’s called MC dropout because for each prediction,
    you make a forward pass through another thinned version of the NN, resulting from
    randomly dropping neurons. You then can combine the dropout predictions to a Bayesian
    predictive distribution:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 dropout 训练的神经网络中获得预测分布，你需要在测试时间也开启 dropout。然后，使用相同的输入，你可以做出几个预测，每个预测都来自神经网络的不同
    dropout 变体。你以这种方式得到的预测将接近方程 8.4 中的预测分布，正如你将在下一行看到的那样。更精确地说，你对相同的输入 *x**[test]*
    进行 T 次条件概率分布 (CPD) *p* ( *y* | *x**[test]* , *w* *[i]* ) 的预测。对于每个预测，你得到不同的 CPDs，(*p*
    ( *y* | *x* , *w* *[i]* )), 对应于一个采样的权重星座 wi（参见以下案例研究）。这被称为 MC dropout，因为对于每个预测，你通过另一个随机丢弃神经元的稀疏版本的前向传递来进行预测。然后，你可以将
    dropout 预测组合成一个贝叶斯预测分布：
- en: '![](../Images/8-11_E02.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-11_E02.png)'
- en: This is an empirical approximation to equation 8.4\. The resulting predictive
    distribution captures the epistemic and the aleatoric uncertainties.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对方程 8.4 的经验近似。得到的预测分布捕捉了认知不确定性和随机不确定性。
- en: To use MC dropout in Keras, there are two options. In the first option, you
    simply create your network by setting the training phase to true in the model
    definition. A more elegant way is to make it choosable, if dropout is used during
    test time or not. The next listing shows how to do so in Keras (and see the notebook
    [http://mng.bz/MdmQ](http://mng.bz/MdmQ)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Keras 中使用 MC dropout，有两种选择。在第一种选择中，你只需通过在模型定义中将训练阶段设置为 true 来创建你的网络。更优雅的方法是使其可选择的，即在测试时间是否使用
    dropout。下一个列表显示了如何在 Keras 中这样做（并参见笔记本 [http://mng.bz/MdmQ](http://mng.bz/MdmQ)）。
- en: Listing 8.5 Getting MC dropout predictions
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 获取 MC dropout 预测
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Defines a function using the model input and learning phase as input and returning
    the model output. What’s important is the learning phase. When set to 0, all weights
    are fixed and adjusted by the dropout p* during test time; when set to 1, dropout
    is used at test time.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个函数，该函数使用模型输入和学习阶段作为输入，并返回模型输出。重要的是学习阶段。当设置为 0 时，所有权重都固定，并在测试时间通过 dropout
    p* 调整；当设置为 1 时，在测试时间使用 dropout。
- en: ❷ Defines the number of dropout predictions
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义 dropout 预测的数量
- en: ❸ Each of the T predictions
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个 T 次预测
- en: ❹ Each of the T predictions for the same input are different because they’re
    determined from a different thinned NN version, corresponding to a different dropout
    run.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对于相同输入的每个 T 次预测都是不同的，因为它们是从不同的稀疏神经网络版本中确定的，对应于不同的 dropout 运行。
- en: 8.5 Case studies
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 案例研究
- en: Let’s return to our original problem that led us to BNNs. Recall the example
    in section 7.1, where we couldn’t find the elephant in the room because such images
    were not part of the training set. Let’s see if Bayesian modeling can help to
    express appropriate uncertainties when encountering new situations that aren’t
    present in the training data (see figure 8.12, which we repeat from chapter 7
    here).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们最初的问题，这个问题引导我们研究贝叶斯神经网络。回想一下第 7.1 节中的例子，在那里我们因为这样的图像不是训练集的一部分，所以找不到房间里的大象。让我们看看贝叶斯建模是否可以帮助在遇到训练数据中不存在的新情况时表达适当的不确定性（参见图
    8.12，我们在此处重复第 7 章中的内容）。
- en: '![](../Images/8-12.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-12.png)'
- en: Figure 8.12 Bad cases of DL. The elephant isn’t seen by the high performant
    VGG16 CNN trained on ImageNet data. The five highest ranked class predictions
    are horse_cart, shopping_cart, palace, streetcar, and gondola--the elephant isn’t
    found! In the regression problem, there’s zero uncertainty in the regions where
    there’s no data (see under Extrapolation on the right and also figure 4.18).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 深度学习中的不良案例。在 ImageNet 数据上训练的高性能 VGG16 CNN 没有看到大象。五个最高排名的类别预测是 horse_cart、shopping_cart、palace、streetcar
    和 gondola——大象没有被找到！在回归问题中，在无数据区域没有不确定性（参见右侧的 Extrapolation 以及图 4.18）。
- en: 8.5.1 Regression case study on extrapolation
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 外推回归案例研究
- en: 'Let’s first tackle the regression task in a case study (see the right side
    of figure 8.12) and compare the two methods: the VI network from section 8.4 and
    the Monte Carlo approximation from the last section. We also include a non-Bayesian
    approach in the example. You can use the same synthetic sinus data that you used
    in sections 4.3.2 and 4.3.3 to fit a non-linear regression model with a flexible
    variance. To do it yourself, work through the following notebook.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先通过一个案例研究来处理回归任务（见图8.12的右侧），并比较这两种方法：第8.4节中的VI网络和上一节中的蒙特卡洛近似。我们还在例子中包含了一种非贝叶斯方法。你可以使用与第4.3.2节和第4.3.3节中相同的合成正弦数据来拟合一个具有灵活方差的非线性回归模型。要自己完成这项工作，请阅读以下笔记本。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/yyxp](http://mng.bz/yyxp)
    . In this notebook, you investigate the advantages BNNs can offer in a regression
    task for extrapolation. You use synthetic data to fit different probabilistic
    NN:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![电脑图标](../Images/computer-icon.png) | 实践时间 打开[http://mng.bz/yyxp](http://mng.bz/yyxp)。在这个笔记本中，你研究贝叶斯神经网络在回归任务中为外推提供的优势。你使用合成数据来拟合不同的概率神经网络：'
- en: You fit a non-Bayesian NN.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你拟合了一个非贝叶斯神经网络。
- en: You fit two Bayesian NN, one via VI and one via dropout.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你拟合了两个贝叶斯神经网络，一个通过变分推断（VI），另一个通过dropout。
- en: You investigate the uncertainties expressed by the NN.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你研究了神经网络表达的不确定性。
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: As validation data, you use 400 examples with *x* values ranging between -10
    and 30 that are spaced in steps of 0.1\. This range doesn’t only cover the range
    of training data, where you expect the model to estimate the parameters with low
    uncertainty, but also covers a range beyond the training data, where you expect
    a higher epistemic uncertainty. After setting up the case study in this manner,
    you can check whether the models are able to express an increased uncertainty
    when getting into the extrapolation range (lower than -3 or larger than 30). The
    different probabilistic NN models yield quite different results as you’ll see.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 作为验证数据，你使用了400个例子，*x*的值在-10到30之间，以0.1的步长间隔。这个范围不仅覆盖了训练数据的范围，你期望模型以低不确定性估计参数，而且还覆盖了训练数据范围之外的区域，你期望有更高的认知不确定性。以这种方式设置案例研究后，你可以检查模型是否能够在进入外推范围（小于-3或大于30）时表达增加的不确定性。不同的概率神经网络模型会产生相当不同的结果，正如你将看到的。
- en: For comparing the three approaches, let’s look at the predictive distribution
    for the outcome. You saw in section 7.3 that you can compute the predictive distribution
    for a Bayesian model via equation 8.5\. (This is the same formula as found in
    equation 7.6, but we call the parameters in NNs usually w instead of *θ* .)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较三种方法，让我们看看结果预测分布。你在第7.3节中看到，你可以通过方程8.5来计算贝叶斯模型的预测分布。（这与方程7.6中的公式相同，但我们通常在神经网络中用w代替*θ*。）
- en: '*P*(*y* | *x**[test]* , *D*) = ∫*[θ]* *P*(*y* | *x**[test]* , *θ*) ⋅ *P*(*θ*|*D*)
    *dθ* Equation 8.5'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*y* | *x**[test]* , *D*) = ∫*[θ]* *P*(*y* | *x**[test]* , *θ*) ⋅ *P*(*θ*|*D*)
    *dθ* 方程8.5'
- en: This equation tells us to average overall possible weight configurations w of
    a network. We approximate this integral by taking the samples *y* ~ *P*(*y* |
    *x**[test]* , *D*) for the different configurations (w) of the network. Note that,
    with sampling, you don’t need to care for the weighting with the posterior *P*(*θ*|*D*).
    The reason is that you automatically sample more often those w that have higher
    posterior probabilities *P*(*θ*|*D*).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程告诉我们需要对网络可能的总权重配置w进行平均。我们通过从网络的不同配置（w）中抽取样本 *y* ~ *P*(*y* | *x**[test]*
    , *D*) 来近似这个积分。注意，在采样过程中，你不需要关心后验权重 *P*(*θ*|*D*)。原因是你会自动更多地采样那些具有更高后验概率 *P*(*θ*|*D*)
    的w。
- en: Let’s take a closer look at how to determine the outcome distribution of *P*(*y*|*x**[i]*
    , *D*) for a given input *x**[i]* . To investigate the outcome distribution, we
    need to look at the trained models and draw T samples from the networks. You’ll
    see that there are some differences in the three different probabilistic NN models.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看如何确定给定输入 *x**[i]* 的 *P*(*y*|*x**[i]* , *D*) 的结果分布。为了研究结果分布，我们需要查看训练好的模型并从网络中抽取T个样本。你会发现三个不同的概率神经网络模型之间有一些差异。
- en: '![](../Images/8-13.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-13.png)'
- en: Figure 8.13 Sketch of the sampling procedure of three models used in the regression
    case study. In the last column (on the right), one edge of the network is picked,
    showing its distribution. The remaining columns show realizations of different
    networks for T different runs. In each run, the input *x**[i]* is the same, and
    the values of the edges are sampled according to their distributions. In the upper
    row, you see the non-Bayesian approach, where all realized NN are the same. In
    the middle, you see the VI-Bayesian approach, where the values of the edges are
    sampled from Gaussians. On the bottom right, the MC dropout approach, where the
    values of edges come from binary distributions. For animated versions, see [https://youtu.be/mQrUcUoT2k4](https://youtu.be/mQrUcUoT2k4)(for
    VI); [https://youtu.be/0-oyDeR9HrE](https://youtu.be/0-oyDeR9HrE)(for MC dropout);
    and [https://youtu.be/FO5avm3XT4g](https://youtu.be/FO5avm3XT4g)(for non-Bayesian).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13展示了回归案例研究中使用的三个模型的采样过程草图。在最后一列（右边），选择网络的一条边，显示其分布。其余列显示了T次不同运行中不同网络的实现。在每次运行中，输入*x**[i]*是相同的，边的值根据它们的分布进行采样。在上行中，你看到的是非贝叶斯方法，其中所有实现的神经网络都是相同的。在中行，你看到的是VI-贝叶斯方法，其中边的值是从高斯分布中采样的。在右下角，是MC
    dropout方法，其中边的值来自二元分布。对于动画版本，请参阅[https://youtu.be/mQrUcUoT2k4](https://youtu.be/mQrUcUoT2k4)（对于VI）；[https://youtu.be/0-oyDeR9HrE](https://youtu.be/0-oyDeR9HrE)（对于MC
    dropout）；以及[https://youtu.be/FO5avm3XT4g](https://youtu.be/FO5avm3XT4g)（对于非贝叶斯）。
- en: 'In the non-Bayesian NN, you have a fixed weight at each connection c. To be
    consistent with the Bayesian terminology, we call this value *θ* c . We want to
    represent this by a probability distribution and so we assign the probability
    1 to the fixed weight *θ* c (see the upper left plot in figure 8.13). Sampling
    from these probability distributions (you have one distribution per connection)
    always yields identical NNs (see the first row, first through third columns in
    figure 8.13). Therefore, the NN yields in each of the T runs for the same input
    *x*, the same parameters for the Gaussian *N*(*y* ; *μ**[x, w[t]]* , *σ**[x, w[t]]*)
    = *N*(*y* ; *μ**[x, w]* , *σ**[x, w]*) (see table 8.2). To get the empirical outcome
    distribution, you can sample from this Gaussian: *y* ∼ *N*(*μ**[x, w]* , *σ**[x,
    w]*). In table 8.2, because the quantities *μ**[x, w]* , and *σ**[x, w]* are the
    same in all rows, the predicted outcome distribution at position *x* is in all
    runs *P*(*y*|*x*,*w*) = *N*(*y* ; *μ**[x]* , *σ**[x]*). In this case, we know
    that the outcome distribution is *N*(*y* ; *μ**[x]* , *σ**[x]*). To do the same
    as for the Bayesian approaches, we still sample one value per run and so get T
    values from the outcome distribution for each *x* position (see the left plot
    in the upper panel in figure 8.14).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在非贝叶斯神经网络中，每个连接c都有一个固定的权重。为了与贝叶斯术语保持一致，我们称这个值为*θ* c。我们希望用一个概率分布来表示这个值，因此我们赋予固定权重*θ*
    c的概率为1（见图8.13左上角的图）。从这些概率分布（每个连接有一个分布）中采样总是得到相同的神经网络（见图8.13的第一行，第一列到第三列）。因此，对于每个T次运行，对于相同的输入*x*，神经网络得到相同的参数对于高斯*N*(*y*
    ; *μ**[x, w[t]]* , *σ**[x, w[t]]*) = *N*(*y* ; *μ**[x, w]* , *σ**[x, w]*)（见表8.2）。为了得到经验结果分布，你可以从这个高斯分布中采样：*y*
    ∼ *N*(*μ**[x, w]* , *σ**[x, w]*)。在表8.2中，因为*μ**[x, w]*和*σ**[x, w]*在所有行中都是相同的，所以在所有运行中，位置*x*处的预测结果分布*P*(*y*|*x*,*w*)
    = *N*(*y* ; *μ**[x]* , *σ**[x]*)。在这种情况下，我们知道结果分布是*N*(*y* ; *μ**[x]* , *σ**[x]*)。为了与贝叶斯方法做同样的事情，我们仍然每次运行采样一个值，因此对于每个*x*位置从结果分布中得到T个值（见图8.14上面板左边的图）。
- en: Table 8.2 Predicted conditional probability distributions (CPDs) from a Bayesian
    NN that was trained via VI for the sinus regression task. Each row corresponds
    to one out of T predictions, yielding for all 400 *x* values, a Gaussian CPD of
    *N* ( *μ**[x]* , *σ**[x]* ) . *N*(*μ* *x*, *σ* *x*) from which it’s possible to
    sample.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2展示了通过变分推断（VI）训练的贝叶斯神经网络在正弦回归任务中预测的条件概率分布（CPDs）。每一行对应T个预测中的一个，对于所有400个*x*值，都得到一个高斯CPD
    *N* ( *μ**[x]* , *σ**[x]* )。*N*(*μ* *x*, *σ* *x*)可以从中采样。
- en: '| **predict_no** | *x*[1] = −10 | *x*[2] = −9.9 | **. . .** | *x*[400] = 30
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **预测编号** | *x*[1] = −10 | *x*[2] = −9.9 | **. . .** | *x*[400] = 30 |'
- en: '| 1 | *y* ∼ *N* ( *μ* *[x]*[1] ,*[w]*[1] , *σ* *[x]*[1] ,*[w]*[1] | *y* ∼ *N*
    ( *μ* *[x]*[2] ,*[w]*[1] , *σ* *[x]*[2] ,*[w]*[1] | **. . .** | *y* ∼ *N* ( *μ**[x]*[400]
    , *[w]*[1] , *σ**[x]*[400] , *[w]*[1] ) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 1 | *y* ∼ *N* ( *μ* *[x]*[1] ,*[w]*[1] , *σ* *[x]*[1] ,*[w]*[1] | *y* ∼ *N*
    ( *μ* *[x]*[2] ,*[w]*[1] , *σ* *[x]*[2] ,*[w]*[1] | **. . .** | *y* ∼ *N* ( *μ**[x]*[400]
    , *[w]*[1] , *σ**[x]*[400] , *[w]*[1] ) |'
- en: '| 2 | *y* ∼ *N* ( *μ**[x]*[1] *[w]*[2] , *σ**[x]*[1] ,*[w]*[2] ) | *y* ∼ *N*
    ( *μ**[x]*[2] ,*[w]*[2] , *σ**[x]*[2] ,*[w]*[2] ) | **. . .** | *y* ∼ *N* ( *μ**[x]*[400]
    ,*[w]*[2] , *σ**[x]*[400] ,*[w]*[2] ) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 2 | *y* ∼ *N* ( *μ**[x]*[1] *[w]*[2] , *σ**[x]*[1] ,*[w]*[2] ) | *y* ∼ *N*
    ( *μ**[x]*[2] ,*[w]*[2] , *σ**[x]*[2] ,*[w]*[2] ) | **. . .** | *y* ∼ *N* ( *μ**[x]*[400]
    ,*[w]*[2] , *σ**[x]*[400] ,*[w]*[2] ) |'
- en: '| **. . .** | **. . .** | **. . .** | **. . .** | **. . .** |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **. . .** | **. . .** | **. . .** | **. . .** | **. . .** |'
- en: '| T | *y* ∼ *N* ( *μ**[x]*[1] ,*[w]*[T] , *σ**[x]*[1] *[w]*[T] ) | *y* ∼ *N*
    ( *μ**[x]*[2] ,*[w]*[T] , *σ**[x]*[2] ,*[w]*[T] ) | **. . .** | *y* ∼ *N* ( *μ**[x]*[400]
    ,*[w]*[T] , *σ**[x]*[400] ,*[w]*[T] ) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| T | *y* ∼ *N* ( *μ**[x]*[1] ,*[w]*[T] , *σ**[x]*[1] *[w]*[T] ) | *y* ∼ *N*
    ( *μ**[x]*[2] ,*[w]*[T] , *σ**[x]*[2] ,*[w]*[T] ) | **. . .** | *y* ∼ *N* ( *μ**[x]*[400]
    ,*[w]*[T] , *σ**[x]*[400] ,*[w]*[T] ) |'
- en: 'A Bayesian VI NN replaces the fixed weight *θ**[c]* with a Gaussian distribution
    with the mean *μ**[c]* and a standard deviation *σ**[c]*(see the middle row, in
    the rightmost column, in figure 8.13). During test time, you sample T-times from
    these weight distributions, always getting slightly different values for the connections
    (see the second row, first to third columns in figure 8.13, where the thickness
    of the connections indicate whether the sampled value is larger or smaller than
    the mean of the weight distribution). The NN, therefore, yields in each of the
    T runs slightly different parameters of the Gaussian *N* ( *μ**[x]* ,*[w]*[t]
    , *σ**[x]* ,*[w]*[t] ) , which we collect in the T rows of table 8.2\. To get
    the empirical outcome distribution, you can sample from all of these determined
    Gaussians: *y* ∼ *N* ( *μ**[x]* ,*[w]*[t] , *σ**[x]* ,*[w]*[t] ) . If you sample
    one value per run, you get T outcome values at each *x* position (see figure 8.14,
    middle plot in the upper panel).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯变分推理神经网络将固定的权重 *θ**[c]* 替换为具有均值 *μ**[c]* 和标准差 *σ**[c]* 的高斯分布（见图 8.13 中间行，最右侧列）。在测试时间，你从这些权重分布中采样
    T 次，总是得到连接的略微不同的值（见图 8.13 的第二行，第一到第三列，其中连接的粗细表示采样值是否大于权重分布的均值）。因此，神经网络在 T 次运行中产生略微不同的高斯
    *N*（ *μ**[x]* ,*[w]*[t] , *σ**[x]* ,*[w]*[t] ）参数，我们将这些参数收集在表 8.2 的 T 行中。为了得到经验结果分布，你可以从所有这些确定的高斯分布中采样：*y*
    ∼ *N* ( *μ**[x]* ,*[w]*[t] , *σ**[x]* ,*[w]*[t] )。如果你每次运行采样一个值，你将在每个 *x* 位置得到
    T 个结果值（见图 8.14，上面板中间图）。
- en: '![](../Images/8-14.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-14.png)'
- en: 'Figure 8.14 Predictive distributions. The solid lines in the top row show five
    samples from the outcome distribution for the three models (classic NN, VI Bayes
    NN, and dropout Bayes NN). The second row shows summary statistics: the solid
    line represents the mean, and the lower and upper dashed lines depict the upper
    and lower borders of the 95% prediction interval.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 预测分布。顶行中的实线显示了三个模型（经典神经网络、变分贝叶斯神经网络和dropout贝叶斯神经网络）的结果分布的五个样本。第二行显示了汇总统计：实线代表均值，上下虚线表示
    95% 预测区间的上下边界。
- en: 'The Bayesian MC dropout NN replaces each fixed weight *θ* c with a binary distribution
    (see the third row in the last column of figure 8.13). During test time, you sample
    T-times from these weight distributions, always getting either zero or the value
    wc for a connection (see the third row, first through third column of figure 8.13).
    To get the empirical outcome distribution, you can sample from all of these determined
    Gaussians: *y* ∼ *N* ( *μ**[x]* ,*[w]* , *σ**[x]* ,*[w]* )(see figure 8.14, right
    plot in the upper panel).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯蒙特卡洛dropout神经网络将每个固定的权重 *θ* c 替换为二进制分布（见图 8.13 最后一列的第三行）。在测试时间，你从这些权重分布中采样
    T 次，总是得到连接的零或 wc 的值（见图 8.13 的第三行，第一到第三列）。为了得到经验结果分布，你可以从所有这些确定的高斯分布中采样：*y* ∼ *N*
    ( *μ**[x]* ,*[w]* , *σ**[x]* ,*[w]* )（见图 8.14，上面板右侧图）。
- en: Let’s use the results from the T predictions to first explore the outcome uncertainty
    for all three models. Figure 8.14 displays the results for the different methods.
    In the first row of figure 8.14, for the first 5 of the T rows in table 8.2, a
    line is drawn to connect the different sampled outcome values *y* at the different
    *x* positions. In the second row, all the T results are summarized by the mean
    value (solid line) and the 95% prediction interval (Dashed lines). The lower dashed
    line corresponds to the 2.5% percentile and the upper to the 97.5% percentile.
    For the non-Bayesian approach, you sample always from the same Gaussian *N*(*μ*
    *x*, *σ* *x*). Therefore, you could, in principle, compute the dashed lines for
    this Gaussian distribution as *y* = *μ**[x]* ± 1.96 ⋅ *σ**[x]* . This isn’t the
    case for the Bayesian approaches, where we don’t know the analytical form of the
    outcome distribution and, therefore, have to sample and then calculate the percentiles
    from the samples.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用T个预测的结果，首先探索所有三个模型的输出不确定性。图8.14显示了不同方法的结果。在图8.14的第一行中，对于表8.2中T行的前5行，画了一条线连接不同位置的不同的采样输出值
    *y*。在第二行中，所有T个结果通过平均值（实线）和95%预测区间（虚线）进行总结。下方的虚线对应于2.5%分位数，上方的对应于97.5%分位数。对于非贝叶斯方法，你总是从相同的高斯分布
    *N*(*μ* *x*, *σ* *x*) 中采样。因此，原则上，你可以计算这个高斯分布的虚线为 *y* = *μ**[x]* ± 1.96 ⋅ *σ**[x]*。对于贝叶斯方法，情况并非如此，因为我们不知道输出分布的解析形式，因此必须从样本中采样并计算分位数。
- en: To wrap up, let’s take a final look at the last row of figure 8.14 and compare
    the Bayes with the non-Bayes approaches. The central line indicates the mean position
    of your value *y* given the data. In all approaches, it follows the data. The
    dashed line indicates the regions in which we expect 95% of the data. In the region
    where we have training data, all approaches yield similar results. The uncertainty
    captured by the spread of the CPD is large in regions where the data spread is
    also large. All models are, thus, able to model the aleatoric uncertainty. When
    we leave the region where we have data and go into the extrapolation region, the
    non-Bayesian approach fails. It assumes 95% of the data in an unrealistically
    narrow region. Total disaster, so sad! The Bayesian approaches, however, know
    when they don’t know and express their uncertainties when leaving known grounds.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，让我们最后看一下图8.14的最后一行，比较贝叶斯与非贝叶斯方法。中心线表示在给定数据的情况下，你的值 *y* 的均值位置。在所有方法中，它都遵循数据。虚线表示我们预期95%数据所在的区域。在我们有训练数据的区域，所有方法都产生相似的结果。在数据分布也大的区域，CPD的分布范围捕捉到的不确定性很大。因此，所有模型都能够模拟随机不确定性。当我们离开有数据的区域进入外推区域时，非贝叶斯方法失败了。它在一个不切实际很窄的区域假设了95%的数据。总是一场灾难，真是太遗憾了！然而，贝叶斯方法知道它们不知道什么，并在离开已知领域时表达它们的不确定性。
- en: 8.5.2 Classification case study with novel classes
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.2 带有新类别的分类案例研究
- en: Let’s revisit the elephant in the room problem, where you face a classification
    task. When presenting a new input image to a trained classification NN, you get
    for each class seen during training a predicted probability. To predict the elephant
    image (see figure 8.12), we used the VGG16 CNN trained on ImageNet in section
    7.1\. ImageNet has 1,000 classes including different kinds of elephants. But the
    VGG16 network wasn’t able to find the elephant in the room, meaning the elephant
    wasn’t among the top five classes. A plausible explanation is that elephant images
    in the training data set never included an elephant in a room. With such an image
    (see figure 8.12, left panel), we ask the NN model to leave known grounds and
    to do extrapolation. Would it help to use a BNN? A BNN would probably not see
    the elephant either, but it should be better able to express its uncertainty.
    Unfortunately, you can’t give it a go because the ImageNet data set is huge, and
    you need several days on a strong GPU machine to train a Bayesian version of the
    VGG16 CNN.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视房间里的大象问题，其中你面临一个分类任务。当将一个新的输入图像呈现给训练好的分类神经网络时，对于训练过程中看到的每个类别，你都会得到一个预测概率。为了预测大象图像（见图8.12），我们在7.1节中使用了在ImageNet上训练的VGG16卷积神经网络。ImageNet有1,000个类别，包括不同种类的大象。但VGG16网络无法在房间里找到大象，这意味着大象不在前五个类别中。一个合理的解释是，训练数据集中的大象图像从未包括房间里的大象。对于这样的图像（见图8.12的左侧面板），我们要求神经网络模型离开已知领域并进行外推。使用贝叶斯神经网络会有帮助吗？贝叶斯神经网络可能也不会看到大象，但它应该能够更好地表达其不确定性。不幸的是，你无法尝试，因为ImageNet数据集非常大，你需要在强大的GPU机器上花费几天时间来训练VGG16卷积神经网络的贝叶斯版本。
- en: Let’s do it in a smaller way so that you can do the experiment yourself in notebook
    [http://mng.bz/MdmQ](http://mng.bz/MdmQ) . You can work with the CIFAR-10 data
    set that has only 50,000 images and 10 classes (see figure 8.15).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在更小的规模上做，这样您就可以在笔记本中自己进行实验[http://mng.bz/MdmQ](http://mng.bz/MdmQ)。您可以使用只有50,000张图像和10个类别的CIFAR-10数据集（见图8.15）。
- en: '![](../Images/8-15.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-15.png)'
- en: Figure 8.15 Example images for the ten classes in the CIFAR-10 data set (same
    as figure 8.9)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 CIFAR-10数据集中十个类别的示例图像（与图8.9相同）
- en: You can use a part of the CIFAR-10 data set to train a Bayesian CNN and do some
    experiments with it. But how do you design an experiment that recognizes if the
    NN model expresses uncertainty when leaving known grounds? Let’s go to the extreme
    and see what happens when you provide a trained CNN with an image from a class
    that isn’t part of the training data. For that, you can train a CNN on only nine
    classes; for example, without the class horse (remove all images showing horses
    from the training data). When presenting a horse image to the trained NN, it estimates
    probabilities for the classes on which it was trained. All of these classes are
    wrong, but the NN still can’t assign a zero probability to all classes because
    the output needs to add up to one, which is enforced using the softmax layer.
    Is it possible to find out if you can trust the classification based on the predicted
    distribution using a Bayesian approach? You can do the experiment yourself in
    the following notebook.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用CIFAR-10数据集的一部分来训练一个贝叶斯CNN并对其进行一些实验。但您如何设计一个实验来识别NN模型在离开已知领域时是否表达不确定性？让我们走向极端，看看当您向训练好的CNN提供不属于训练数据的类别的图像时会发生什么。为此，您可以在仅九个类别上训练一个CNN；例如，不包含马这个类别（从训练数据中移除所有显示马的图像）。当向训练好的NN展示马图像时，它估计了它在训练时训练的类别的概率。所有这些类别都是错误的，但NN仍然不能将所有类别的概率分配为零，因为输出需要加起来等于一，这是通过softmax层强制执行的。您是否可以通过贝叶斯方法找出您是否可以信任基于预测分布的分类？您可以在下面的笔记本中自己进行实验。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/MdmQ](http://mng.bz/MdmQ)
    . In this classification case study with novel classes notebook, you investigate
    the advantages BNNs can offer in a classification task. You use training data
    from 9 of the 10 classes in the CIFAR-10 data set to fit different probabilistic
    NNs:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![电脑图标](../Images/computer-icon.png) | 实践时间 打开[http://mng.bz/MdmQ](http://mng.bz/MdmQ)。在这个具有新类别的分类案例研究中，您研究BNN在分类任务中可以提供哪些优势。您使用CIFAR-10数据集中10个类别中的9个类别的训练数据来拟合不同的概率性神经网络：'
- en: You fit a non-Bayesian NN.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您拟合了一个非贝叶斯神经网络。
- en: You fit two BNNs, one via VI and one via MC dropout.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您拟合了两个贝叶斯神经网络（BNN），一个通过变分推断（VI）拟合，另一个通过MC dropout拟合。
- en: You compare the performance achieved with the different NNs.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您比较了使用不同神经网络所达到的性能。
- en: You investigate the uncertainties expressed by the NNs.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您研究NN表达的不确定性。
- en: You use the uncertainties to detect novel classes.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您使用不确定性来检测新的类别。
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'During test time, a traditional probabilistic CNN for classification yields
    for each input image a multinomial probability distribution (MN in the equations).
    In our case, one that’s fitted with nine outcome classes (see figure 8.16). The
    probabilities of the k classes provide the parameters for this multinomial distribution:
    MN(*p*[1] , *p*[2] ,⋯, *p**[k]*).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时间，一个传统的用于分类的概率性卷积神经网络（CNN）为每个输入图像提供一个多项式概率分布（MN在方程中）。在我们的情况下，一个拟合了九个结果类别的多项式分布（见图8.16）。k个类别的概率为这个多项式分布提供了参数：MN(*p*[1]
    , *p*[2] ,⋯, *p**[k]*).
- en: '![](../Images/8-16.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-16.png)'
- en: 'Figure 8.16 Multinomial distribution with nine classes: MN( *p*[1] , *p*[2]
    , *p*[3] , *p*[4] , *p*[5] , *p*[6] , *p*[7] , *p*[8] , *p*[9] )'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 九个类别的多项式分布：MN( *p*[1] , *p*[2] , *p*[3] , *p*[4] , *p*[5] , *p*[6] , *p*[7]
    , *p*[8] , *p*[9] )
- en: 'As in the regression case, let’s look at the predicted CPDs of three different
    probabilistic NN models first. In the non-Bayesian NN, you have fixed weights,
    and for one input image, you get one multinomial CPD: CPD *p* ( *y* | *x* , *w*
    ) = MN( *p*[1] ( *x* , *w* ), ..., *p*[9] ( *x* , *w* )(see table 8.3). If you
    predict the same image T times, you always get the same results. All rows of the
    corresponding table 8.3 are the same.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 与回归情况一样，让我们首先看看三个不同的概率性神经网络模型的预测CPD。在非贝叶斯神经网络中，您有固定的权重，对于一张输入图像，您得到一个多项式CPD：CPD
    *p* ( *y* | *x* , *w* ) = MN( *p*[1] ( *x* , *w* ), ..., *p*[9] ( *x* , *w* )（见表8.3）。如果您预测同一图像T次，您总是得到相同的结果。表8.3的每一行都是相同的。
- en: Table 8.3 Predictive distribution of a probabilistic CNN for the CIFAR-10 classification
    task trained with 9 out of 10 classes. Each prediction is a multinomial CPD with
    9 parameters (probability for class 1, 2, . . ., 9).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.3 使用9个类别中的9个类别训练的CIFAR-10分类任务的概率CNN的预测分布。每个预测是一个具有9个参数（类别1、2、……、9的概率）的多项式CPD。
- en: '| **predict_no** | **Image** *ξ*[1] **with known class** | **Image** *ξ*[2]
    **with unknown class** |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| **predict_no** | **Image** *ξ*[1] **with known class** | **Image** *ξ*[2]
    **with unknown class** |'
- en: '| 1 | *y* ∼ MN(*p*[1](*x*[1] , *w*[1]),..., *p*[9](*x*[1] , *w*[1])) | *y*
    ∼ MN(*p*[1](*x*[2] , *w*[1]),..., *p*[9](*x*[2] , *w*[1])) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 1 | *y* ∼ MN(*p*[1](*x*[1] , *w*[1]),..., *p*[9](*x*[1] , *w*[1])) | *y*
    ∼ MN(*p*[1](*x*[2] , *w*[1]),..., *p*[9](*x*[2] , *w*[1])) |'
- en: '| 2 | *y* ∼ MN(*p*[1](*x*[1] , *w*[2]),..., *p*[9](*x*[1] , *w*[2])) | *y*
    ∼ MN(*p*[1](*x*[2] , *w*[2]),..., *p*[9](*x*[2] , *w*[2])) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 2 | *y* ∼ MN(*p*[1](*x*[1] , *w*[2]),..., *p*[9](*x*[1] , *w*[2])) | *y*
    ∼ MN(*p*[1](*x*[2] , *w*[2]),..., *p*[9](*x*[2] , *w*[2])) |'
- en: '| . . . | . . . | . . . |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| . . . | . . . | . . . |'
- en: '| T | *y* ∼ MN(*p*[1](*x*[1] , *w**[T]*),..., *p*[9](*x*[1] , *w**[T]*)) |
    *y* ∼ MN(*p*[1](*x*[2] , *w**[T]* ),..., *p*[9](*x*[2] , *w**[T]*)) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| T | *y* ∼ MN(*p*[1](*x*[1] , *w**[T]*),..., *p*[9](*x*[1] , *w**[T]*)) |
    *y* ∼ MN(*p*[1](*x*[2] , *w**[T]* ),..., *p*[9](*x*[2] , *w**[T]*)) |'
- en: 'In a BNN fitted with VI, the fixed weights are replaced with Gaussian distributions.
    During test time, you sample from these weighted distributions by predicting the
    same input image not only once, but T times. For each prediction, you get one
    multinomial CPD: *P*(*x*, *w**[t]*) = MN(*p*[1](*x*, *w**[t]*),..., *p*[9](*x*,
    *w**[t]*)). Each time you predict the image, you get a different CPD (*P*(*y*|*x*,
    *w**[t]*)), corresponding to the sampled weight constellation wt. This means all
    rows of the corresponding table 8.3 are different.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用变分推断（VI）拟合的贝叶斯神经网络中，固定权重被替换为高斯分布。在测试时间，你通过预测相同的输入图像T次来从这些加权分布中进行采样。对于每次预测，你得到一个多项式CPD：*P*(*x*,
    *w**[t]*) = MN(*p*[1](*x*, *w**[t]*),..., *p*[9](*x*, *w**[t]*)). 每次预测图像时，你得到一个不同的CPD
    (*P*(*y*|*x*, *w**[t]*))，对应于采样的权重星座wt。这意味着对应表8.3的所有行都是不同的。
- en: 'In the BNN fitted with MC dropout, you replace the fixed weights with binary
    distributions. From here on this reads like the VI case. For each prediction,
    you get one multinomial CPD: *P*(*x*, *w*) = MN(*p*[1](*x*, *w*),..., *p*[9](*x*,
    *w*)). Each time you predict the image, you get a different CPD *P*(*y*|*x*, *w**[t]*)
    corresponding to the sampled weight constellation wi. Meaning all rows of the
    corresponding table 8.3 are different.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用MC dropout拟合的贝叶斯神经网络中，你用二进制分布替换固定权重。从现在起，这就像VI的情况一样。对于每次预测，你得到一个多项式CPD：*P*(*x*,
    *w*) = MN(*p*[1](*x*, *w*),..., *p*[9](*x*, *w*)). 每次预测图像时，你得到一个不同的CPD *P*(*y*|*x*,
    *w**[t]*)，对应于采样的权重星座wi。这意味着对应表8.3的所有行都是不同的。
- en: Summarizing and visualizing uncertainty in classification models
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 总结和可视化分类模型中的不确定性
- en: Plotting the predicted probabilities of one row in table 8.3 (parameters of
    the multinomial CPD) yields the plots in the second, third, and fourth rows of
    figure 8.17\. The aleatoric uncertainty is expressed in the distribution across
    the classes, which is zero if one class gets a probability of one. The epistemic
    uncertainty is expressed in the spread of the predicted probabilities of one class,
    which is zero if the spread is zero. The non-Bayesian NN can’t express epistemic
    uncertainty (you can’t get different predictions for the same image), but the
    BNN can.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制表8.3（多项式CPD的参数）中一行的预测概率，可以得到图8.17的第二、第三和第四行的图。随机不确定性在类别的分布中表示，如果一个类别得到一个概率为1，则该分布为零。认知不确定性在某一类预测概率的分布中表示，如果分布为零，则认知不确定性为零。非贝叶斯神经网络无法表示认知不确定性（你不能为同一图像得到不同的预测），但贝叶斯神经网络可以。
- en: Let’s look at the results of the network on a known class for two examples first.
    In the left panel of figure 8.17, you can see that all networks correctly classify
    the image of an airplane. In the two BNNs, you can see little variation in the
    plots, indicating that the classification is quite certain. The MC dropout Bayesian
    CNN shows a little more uncertainty than the VI CNN and also assigns some probability
    to the class bird. This is understandable when looking at the image.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看两个已知类别的网络结果。在图8.17的左侧面板中，你可以看到所有网络正确地分类了飞机的图像。在两个贝叶斯神经网络中，你可以看到图中几乎没有变化，这表明分类相当确定。MC
    dropout贝叶斯CNN比VI CNN显示出更多的不确定性，并且也分配了一些概率给鸟类类别。当查看图像时，这是可以理解的。
- en: If we go to the unknown class, all predictions in figure 8.17 are wrong. But
    you can see that the BNNs can better express their uncertainty. Of course, the
    Bayesian networks also predict a wrong class in all of their T runs. But what
    we see for each of the T runs is that the distributions vary quite a bit. When
    comparing the VI and MC dropout methods, again MC dropout shows more variation.
    Also, the shapes of the predicted probability distribution look different for
    MC dropout and VI. In the case of VI, the distributions look quite bell-shaped.
    This might be due to the Gaussian weight distributions used in VI. MC dropout,
    relying on Bernoulli distributed weights, yields richer predictive distribution
    shapes. Let’s now try to use a 9-dimensional predictive distribution to quantify
    the uncertainty of the prediction (see table 8.3 and figure 8.17).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们转向未知类别，图8.17中的所有预测都是错误的。但你可以看到，贝叶斯神经网络能更好地表达它们的确定性。当然，贝叶斯网络在其T次运行中也会预测一个错误的类别。但我们在每次T次运行中看到的是，分布变化很大。在比较VI和MC
    dropout方法时，再次显示MC dropout有更多的变化。此外，MC dropout和VI预测的概率分布形状看起来也不同。在VI的情况下，分布看起来非常钟形。这可能是因为VI中使用的高斯权重分布。依赖于伯努利分布权重的MC
    dropout产生了更丰富的预测分布形状。现在让我们尝试使用9维预测分布来量化预测的不确定性（参见表8.3和图8.17）。
- en: '![](../Images/8-17.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-17.png)'
- en: 'Figure 8.17 Upper panel: images presented to train CNNs included an image from
    the known class airplane (left) and an image from the unknown class horse (right).
    Second row of plots: corresponding predictive distributions resulting from non-Bayesian
    NN. Third row of plots: corresponding predictive distributions resulting from
    BNN via VI. Fourth row of plots: corresponding predictive distributions resulting
    from BNN via MC dropout.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 上半部分：提供给训练CNN的图像包括来自已知类别飞机的图像（左）和来自未知类别马匹的图像（右）。第二行图：非贝叶斯神经网络产生的相应预测分布。第三行图：通过VI产生的贝叶斯神经网络的相应预测分布。第四行图：通过MC
    dropout产生的贝叶斯神经网络的相应预测分布。
- en: Uncertainty measures in a non-Bayesian classification NN
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 非贝叶斯分类神经网络中的不确定性度量
- en: 'In the case of a traditional, non-Bayesian NN, you get for one image, one CPD(see
    the second row in figure 8.17). You classify the image to the class with the highest
    probability: *p**[pred]* = max( *p**[k]* ) . The CPD only expresses the aleatoric
    uncertainty, which would be zero if one class gets a probability of one and all
    other classes get a probability of zero. You can use *p**[pred]* as a measure
    for certainty or −log( *p**[pred]* ) as a measure for uncertainty, which is the
    well-known NLL:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统、非贝叶斯神经网络的情况下，对于一张图像，你得到一个CPD（参见图8.17的第二行）。你将图像分类到概率最高的类别：*p**[pred]* = max(
    *p**[k]* )。CPD仅表达随机不确定性，如果某个类别的概率为1而其他所有类别的概率为0，则这种不确定性将为零。你可以使用 *p**[pred]* 作为确定性的度量，或者使用
    −log( *p**[pred]* ) 作为不确定性的度量，这就是众所周知的NLL：
- en: NLL = −log( *p**[pred]* )
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: NLL = −log( *p**[pred]* )
- en: 'Another frequently used measure for the aleatoric uncertainty (using not only
    the probability of the predicted class) is entropy, which you already encountered
    in chapter 4\. There’s no epistemic uncertainty when working with a non-Bayesian
    NN. Here’s the formula:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用于随机不确定性的度量（不仅使用预测类别的概率）是熵，这在第4章中你已经遇到过。在使用非贝叶斯神经网络时，没有认知不确定性。以下是公式：
- en: 'Entropy: *H* = −∑⁹*[k]*[=1] *p**[k]* ⋅ log( *p**[k]* )'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 熵：*H* = −∑⁹*[k]*[=1] *p**[k]* ⋅ log( *p**[k]* )
- en: Uncertainty measures in a Bayesian classification NN
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类神经网络中的不确定性度量
- en: 'For each image, you predict T multinomial CPDs: MN(*p*[1](*x*, *w**[t]*),...,
    *p*[9](*x*, *w**[t]*)) (see table 8.3 and the third and fourth rows in figure
    8.17). For each class k, you can determine the mean probability *p*^**[k]* = 1/T
    ∑*^T**[t=1]* *p*^**[kt]* . You classify the image to the class with the highest
    mean probability *p*^**[pred]* = max( *p*^**[k]* ) .'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每张图像，你预测T个多项式CPD：MN(*p*[1](*x*, *w**[t]*),..., *p*[9](*x*, *w**[t]*))（参见表8.3和图8.17的第三行和第四行）。对于每个类别k，你可以确定平均概率
    *p*^**[k]* = 1/T ∑*^T**[t=1]* *p*^**[kt]*。你将图像分类到平均概率最高的类别 *p*^**[pred]* = max(
    *p*^**[k]* )。
- en: 'In the literature, there’s no consensus on how to best quantify the uncertainty
    that captures both epistemic and aleatoric contributions; in fact, that’s still
    an open research question (at least if you have more than two classes). Note that
    this mean probability already captures a part of the epistemic uncertainty because
    it’s determined from all T predictions. Averaging causes a shift to less extreme
    probabilities away from one or zero. You could use −log( *p*^**[pred]* ) as the
    uncertainty:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，关于如何最好地量化同时捕捉认知和随机贡献的不确定性没有共识；实际上，这仍然是一个开放的研究问题（至少如果你有超过两个类别）。请注意，这个平均概率已经捕捉到了一部分认知不确定性，因为它是由所有T个预测确定的。平均化导致概率向远离一或零的极端概率偏移。你可以使用
    −log( *p*^**[pred]* ) 作为不确定性：
- en: NLL^* = −log( *p*^**[pred]* )
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: NLL^* = −log( *p*^**[pred]* )
- en: 'The entropy based on the mean probability values, p*k, averaged over all T
    runs is an even better-established uncertainty measure. But also, you can use
    the total variance (sum of the variances for the individual classes) of the multidimensional
    probability distribution to quantify the uncertainty:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基于平均概率值p*k，在所有T次运行中平均得到的熵是一个更成熟的不确定性度量。但同样，你也可以使用多维概率分布的总方差（各个类别的方差的和）来量化不确定性：
- en: 'Entropy*: *H*^* = −∑⁹*[k]*[=1] *p*^**[k]* ⋅ log( *p*^**[k]* )'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '熵*: *H*^* = −∑⁹*[k]*[=1] *p*^**[k]* ⋅ log( *p*^**[k]* )'
- en: 'Total variance: *V**[tot]* = ∑⁹*[k]*[=1] var( *p**[k]* ) = ∑⁹*[k]*[=1] ∑*^T**[t=1]*
    ( *p**[kt]* − *p*^**[k]* )²'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总方差：*V**[tot]* = ∑⁹*[k]*[=1] var( *p**[k]* ) = ∑⁹*[k]*[=1] ∑*^T**[t=1]* ( *p**[kt]*
    − *p*^**[k]* )²
- en: Using uncertainty measures to filter out potentially wrong classified images
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不确定性度量来过滤掉可能被错误分类的图像
- en: Let’s see if these uncertainty measures can help us improve the prediction performance.
    You saw at the end of chapter 7 that the prediction performance in regression,
    quantified by the test NLL, is indeed better for a BNN than for a non-BNN, at
    least for the investigated simple linear regression task. But now, let’s turn
    to classification. We could again look at the test NLL, but here we want to focus
    on another question. We want to check if we can identify uncertain examples and
    if removing those from test samples can enhance the accuracy. The idea is that
    the images of unknown classes should especially show high uncertainties. Hence,
    the accuracy can be improved if we manage to filter out these wrongfully classified
    images.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些不确定性度量是否可以帮助我们提高预测性能。你在第7章的结尾看到，回归中的预测性能，通过测试NLL来衡量，对于贝叶斯神经网络（BNN）确实比非贝叶斯神经网络（non-BNN）要好，至少对于所研究的简单线性回归任务是这样。但现在，让我们转向分类。我们又可以再次查看测试NLL，但在这里我们想要关注另一个问题。我们想要检查是否可以识别不确定的示例，并且如果从测试样本中移除这些示例可以增强准确率。想法是未知类别的图像应该特别显示出高不确定性。因此，如果我们能够过滤掉这些错误分类的图像，就可以提高准确率。
- en: 'To check if we can identify wrongfully classified images, you can do a filter
    experiment (see [http://mng.bz/MdmQ](http://mng.bz/MdmQ)). For this, you pick
    one of the uncertainty measures and then rank the classified images according
    to this measure. You start with classifying test images with the lowest uncertainty
    and determine the achieved accuracies with the three CNN variants. Then you add
    images in the order of their uncertainty (smallest uncertainties first) and determine
    after each added image, again, the resulting accuracies. You can see the results
    in figure 8.18\. After adding the image with the highest uncertainty, all test
    samples are taken into account, achieving the following accuracies: 58% for the
    non-Bayesian CNN, 62% for the VI Bayesian CNN, and 63% for the MC dropout Bayesian
    CNN. This sounds quite bad, but keep in mind that 10% of all test samples come
    from the unknown class that must, by definition, lead to wrong classifications.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查我们是否可以识别被错误分类的图像，你可以进行一个过滤实验（参见[http://mng.bz/MdmQ](http://mng.bz/MdmQ)）。为此，你选择一个不确定性度量，然后根据这个度量对分类图像进行排序。你从具有最低不确定性的测试图像开始分类，并确定三个CNN变体所达到的准确率。然后，你按照不确定性的顺序添加图像（首先是具有最小不确定性的图像），并在添加每张图像后，再次确定所得到的准确率。你可以在图8.18中看到结果。在添加具有最高不确定性的图像后，考虑所有测试样本，达到以下准确率：非贝叶斯CNN为58%，VI贝叶斯CNN为62%，MC
    dropout贝叶斯CNN为63%。这听起来相当糟糕，但请记住，所有测试样本中有10%来自必须按定义导致错误分类的未知类别。
- en: '![](../Images/8-18.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-18.png)'
- en: Figure 8.18 Accuracy decreases if more and more images that have higher and
    higher uncertainties are taken into account. The highest accuracy in each plot
    corresponds to the accuracy when only the most certain test image is taken into
    account (100% accuracy). Then images are added with increasing uncertainty measures.
    The solid curves correspond to the non-Bayesian CNN, the dotted curves to the
    MC dropout Bayesian CNN, and the dashed curve to the VI Bayesian CNN. The left
    column shows the curves when the 5,000 thousand most certain images are taken
    into account; the right column shows the curve for the entire test data set of
    10,000 images. In the last row, only the Bayesian CNNs can be used for filtering
    because in a non-Bayesian CNN, no variance can be computed.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 如果考虑越来越多的具有更高不确定性的图像，准确性会降低。每个图表中的最高准确性对应于只考虑最确定的测试图像时的准确性（100%准确性）。然后，随着不确定性的增加，添加图像。实线曲线对应于非贝叶斯CNN，点线曲线对应于MC
    dropout贝叶斯CNN，虚线曲线对应于VI贝叶斯CNN。左侧列显示了考虑5,000个最确定图像时的曲线；右侧列显示了10,000个图像的整个测试数据集的曲线。在最后一行，只能使用贝叶斯CNN进行过滤，因为在非贝叶斯CNN中无法计算方差。
- en: When restricting the test sample only to known classes, you achieve test accuracies
    of 65% for the non-Bayesian CNN, 69% for the VI Bayesian CNN, and 70% for the
    MC dropout Bayesian CNN (see [http://mng.bz/MdmQ](http://mng.bz/MdmQ)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 当将测试样本仅限制在已知类别时，非贝叶斯CNN的测试准确率为65%，VI贝叶斯CNN为69%，MC dropout贝叶斯CNN为70%（见[http://mng.bz/MdmQ](http://mng.bz/MdmQ)）。
- en: 'Let’s come back to the question: Are the uncertainties from BNNs more appropriate
    to identify potentially wrong classifications? The answer is yes! You can see
    that in figure 8.18, where the accuracy of the non-Bayesian CNN could be improved
    by turning to a Bayesian variant. Also, between the VI and MC dropout Bayesian
    CNNs, you can see a performance difference. MC dropout clearly outperforms VI.
    This is probably because the VI works with unimodal Gaussian weight distributions
    and MC dropout with Bernoulli distributions. You already saw in figure 8.17 that
    VI tends to yield bell-shaped and quite narrow predictive distributions, while
    MC dropout yields for the same example broader and asymmetric distributions, which
    partly look almost bimodal (Distribution with two peaks). We hypothesize that
    in the near future, VI will achieve the same or better performance as MC dropout
    when it becomes possible to easily work in VI with more complex distributions
    instead of Gaussian. The switch from non-Bayesian to Bayesian NNs is easy to do,
    regardless if you choose to do VI or MC dropout, and allows you to flag potentially
    wrong classifications and achieve better prediction performance in regression
    and classification.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到问题：BNN的不确定性是否更适合识别潜在的错误分类？答案是肯定的！您可以在图8.18中看到，非贝叶斯CNN的准确性可以通过转向贝叶斯变体来提高。此外，在VI和MC
    dropout贝叶斯CNN之间，您可以看到性能差异。MC dropout明显优于VI。这可能是因为VI与单峰高斯权重分布一起工作，而MC dropout与伯努利分布一起工作。您已经在图8.17中看到，VI倾向于产生钟形且相当窄的预测分布，而MC
    dropout对于相同的示例产生更宽且不对称的分布，部分看起来几乎是双峰的（具有两个峰值的分布）。我们假设，在不久的将来，当可以轻松地在更复杂的分布而不是高斯分布中工作VI时，VI将实现与MC
    dropout相同或更好的性能。从非贝叶斯到贝叶斯神经网络的切换很容易完成，无论您选择进行VI还是MC dropout，都可以标记潜在的错误分类，并在回归和分类中实现更好的预测性能。
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Standard neural networks (NNs) fail to express their uncertainty. They can’t
    talk about the elephant in the room.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准神经网络（NN）无法表达它们的不确定性。它们无法谈论房间里的大象。
- en: Bayesian neural networks (BNNs) can express their uncertainty.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络（BNN）可以表达它们的不确定性。
- en: BNNs often yield better performance than their non-Bayesian variants.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络（BNN）通常比它们的非贝叶斯变体表现更好。
- en: Novel classes can be better identified with BNNs, which combine epistemic and
    aleatoric uncertainties compared to standard NNs.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与标准神经网络（NN）相比，贝叶斯神经网络（BNN）可以更好地识别新颖类别，因为它结合了认知和随机不确定性。
- en: Variational inference (VI) and Monte Carlo dropout (MC dropout) are approximation
    methods that allow you to fit deep BNNs.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分推断（VI）和蒙特卡洛dropout（MC dropout）是近似方法，允许您拟合深度贝叶斯神经网络（BNN）。
- en: TensorFlow Probability (TFP) provides easy-to-use layers for fitting a BNN via
    VI.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Probability (TFP) 提供了通过变分推断（VI）拟合贝叶斯神经网络（BNN）的易于使用的层。
- en: MC dropout can be used in Keras for fitting BNNs.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MC dropout可用于Keras中拟合BNN。
