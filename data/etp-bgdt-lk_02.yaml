- en: Chapter 2\. Historical Perspective
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。历史视角
- en: 'Data has been around for a very long time. First, it was visual: paintings
    on cave walls, mounds arranged a certain way. Once humans developed systems of
    writing, they used these to keep track of things: many ancient clay tablets and
    manuscripts seem to contain inventories, ledgers, and bills of sale and debt.
    Later, more general data was collected and published in almanacs and encyclopedias:
    records of the longest rivers, tallest mountains, deepest lakes, most populous
    countries, average rainfall, highest and lowest temperatures, and so on. We seem
    to have an endless fascination with measuring and counting, comparing and tracking.
    Traditionally, this measuring and counting process was laborious and manual, so
    we invented machines to help us with it. Eventually, those machines evolved into
    modern computers.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已存在很长时间。首先是视觉形式：洞穴壁画、特定排列的土堆。一旦人类发展出书写系统，他们开始用它们来记录事物：许多古代泥板和手稿似乎包含清单、账簿、买卖和债务的记录。后来，更多的通用数据被收集并出版在年鉴和百科全书中：最长的河流、最高的山脉、最深的湖泊、人口最多的国家、平均降雨量、最高和最低温度等等。我们似乎对测量、计数、比较和追踪有着无尽的迷恋。传统上，这种测量和计数过程是费时且手动的，因此我们发明了机器来帮助我们。最终，这些机器演变成了现代计算机。
- en: Very early on, it became obvious that computers had a capacity to count, measure,
    and store information that far exceeded a human’s. However, computers were also
    great at other things, like applying logic and executing business processes. Most
    of the focus in the early days of computers was on programs and logic. Data was
    considered an artifact of programs, something that could be accessed and made
    sense of only by the programs that had originally stored it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 很早就显而易见，计算机具有远远超出人类的计数、测量和存储信息的能力。然而，计算机在其他方面也非常擅长，比如应用逻辑和执行业务流程。在计算机早期，大部分关注点都集中在程序和逻辑上。数据被认为是程序的产物，只有原始存储它的程序才能访问并理解它。
- en: To make data humanly accessible, programmers developed *reports* that packaged
    data into a human-readable form. If an analyst wanted to look at data in a different
    way, they had to make a request and wait for developers to create a new report.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据对人类更易访问，程序员开发了*报告*，将数据打包成人类可读的形式。如果分析师想以不同方式查看数据，他们必须提出请求并等待开发人员创建新的报告。
- en: The Drive for Self-Service Data—The Birth of Databases
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自助数据驱动力——数据库的诞生
- en: The first step in the self-service data revolution was the spreadsheet. It allowed
    non-developers to work with data directly. For the first time, the analysts were
    able to work with the data themselves and manipulate it into the shapes they desired.
    Once that genie was out of the bottle, there was no putting it back. But while
    spreadsheets quickly became the most common decision support tool, they did not
    scale beyond small amounts of data and could address only a small subset of the
    problems that analysts wanted to address.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自助数据革命的第一步是电子表格。它允许非开发人员直接处理数据。分析师第一次能够自行处理数据并将其操纵成他们想要的形状。一旦这只神灯被释放出来，就无法将其收回。但尽管电子表格迅速成为最常见的决策支持工具，它们无法扩展到大量数据，并且只能解决分析师希望解决的问题的一小部分。
- en: Meanwhile, companies began to realize that the data, not the applications, was
    the crown jewel. Losing data meant the business would come to a grinding halt.
    Data had to be carefully managed, checked for consistency, and backed up. Instead
    of each program having to develop these capabilities itself, they were extracted
    and provided by a new class of systems called *database management systems* (DBMSs).
    These systems did not contain programming logic and existed purely to manage the
    data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，公司开始意识到数据而非应用程序才是最宝贵的资产。丢失数据意味着业务将停滞不前。数据必须被精心管理，确保一致性，并进行备份。不再让每个程序都自行开发这些能力，而是通过一种新的系统类别提取和提供，这类系统称为*数据库管理系统*（DBMS）。这些系统不包含编程逻辑，专注于数据管理。
- en: Early systems were still tied very closely to applications and required application
    logic to make sense of the data, but eventually, the separation between data and
    application became more institutionalized—especially with the advent of relational
    databases.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 早期系统仍然与应用程序紧密耦合，并需要应用逻辑来理解数据，但随着关系数据库的出现，数据和应用程序之间的分离逐渐得到制度化。
- en: Relational database management systems (RDBMSs) allow the users to describe
    the data explicitly to the database. Users create a *schema*—a human-readable
    collection of tables and fields. Instead of having to always go through the programs
    to get to the data, the users of RDBMSs were able to query data directly. Eventually,
    a somewhat standard language called Structured Query Language (SQL) emerged and
    became the *lingua franca* of databases. Using this language, the users could
    write their own queries and do their own analysis of data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库管理系统（RDBMSs）允许用户明确描述数据到数据库中。用户创建一个*模式*—一组可供人类阅读的表格和字段。而不是总是通过程序获取数据，RDBMSs
    的用户能够直接查询数据。最终，一种称为结构化查询语言（SQL）的标准语言出现，并成为数据库的*通用语言*。使用这种语言，用户可以编写自己的查询并对数据进行分析。
- en: Even though it was now possible to analyze the data used by applications directly,
    most database schemas were still designed to support applications. Because writing
    or reading data to and from disk was orders of magnitude slower than processing
    it in memory, a schema design technique called *normalization* broke data into
    the smallest possible chunks to ensure that each database update could write as
    little data as possible. This worked well for updates and queries that retrieved
    specific pieces of data, such as information about a single customer, but was
    extremely inefficient for doing large-scale analytics, such as looking at all
    the activity of a customer, that required a number of tables to be joined together.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然现在可以直接分析应用程序使用的数据，但大多数数据库模式仍然设计用于支持应用程序。由于从磁盘读写数据比在内存中处理数据慢几个数量级，一种称为*规范化*的模式设计技术将数据分解为尽可能小的块，以确保每个数据库更新尽可能少地写入数据。这对于更新和检索特定数据片段（例如单个客户的信息）效果良好，但在进行大规模分析（例如查看客户所有活动）时效率极低，因为需要连接多个表。
- en: Many books have been written on both relational database theory and schema design,
    so I will just cover the key concepts of relations, primary and foreign keys,
    and normalization here. Relational databases contain tables that have columns
    and rows. Imagine trying to store all the information you have about your customers
    in a table—you would have columns for different customer attributes like name,
    address, age, gender, and so on. Now imagine that you also want to keep track
    of the orders that each customer places. You could add new columns for order number,
    date, amount, and other order attributes. If you only had one order per customer,
    you would have one row for each customer and their order. But what if customers
    placed multiple orders? Would you now have a row for each order? That would mean
    that all the customer data would be replicated for each order, as illustrated
    in [Table 2-1](#the_customer_orders_table). If a customer had a thousand orders,
    their data would be replicated a thousand times. Worse, if their information were
    to change—if a customer were to move or change their name after getting married,
    for example—you would have to update each of those thousand records. Clearly,
    this is not a very efficient approach.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于关系型数据库理论和模式设计已经写了很多书籍，所以我在这里只介绍关系、主键和外键以及规范化的关键概念。关系数据库包含具有列和行的表格。想象一下，试图将您对客户的所有信息存储在一个表格中—您将有不同客户属性的列，如姓名、地址、年龄、性别等等。现在想象一下，您还想跟踪每位客户的订单。您可以添加新的列用于订单号、日期、金额和其他订单属性。如果每位客户只有一个订单，您将为每位客户及其订单拥有一行。但如果客户下了多个订单呢？您现在是否为每个订单都有一行？这意味着所有客户数据将为每个订单复制一次，如在[表
    2-1](#the_customer_orders_table)所示。如果一个客户有一千个订单，他们的数据将被复制一千次。更糟糕的是，如果他们的信息发生变化—例如客户搬家或结婚后更改姓名，您将不得不更新这一千条记录。显然，这不是一种非常有效的方法。
- en: Table 2-1\. *The Customer_Orders table*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. *客户订单表*
- en: '| **Name** | **Gender** | **Marital_ Status** | **Zip_Code** | **Order_ Number**
    | **Amount** | **Date** |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **姓名** | **性别** | **婚姻状况** | **邮政编码** | **订单号码** | **金额** | **日期** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Mary Ng | F | Married | 94301 | 2123123 | 987.19 | 7/12/18 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 玛丽·吴 | 女 | 已婚 | 94301 | 2123123 | 987.19 | 7/12/18 |'
- en: '| Mary Ng | F | Married | 94301 | 2221212 | 12.20 | 9/2/18 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 玛丽·吴 | 女 | 已婚 | 94301 | 2221212 | 12.20 | 9/2/18 |'
- en: '| Mary Ng | F | Married | 94301 | 2899821 | 5680.19 | 10/15/18 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 玛丽·吴 | 女 | 已婚 | 94301 | 2899821 | 5680.19 | 10/15/18 |'
- en: '| Tom Jones | M | Single | 93443 | 2344332 | 1500.00 | 9/12/18 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 汤姆·琼斯 | 男 | 单身 | 93443 | 2344332 | 1500.00 | 9/12/18 |'
- en: The solution to this problem applied in relational databases is called *normalization*.
    It is a way to break tables into smaller tables in order to avoid repeating information.
    For example, we would store customer information in one table and all the orders
    for all the customers in another table. Then, in order to identify which orders
    belong to which customers, we would generate a *key*—for example, `Customer_ID`—and
    have it as part of both the `Customers` and `Orders` tables, such that each order
    will contain the `Customer_ID` value that corresponds to the `Customer_ID` value
    in the record of the customer that placed the order. The `Customer_ID` column
    in the `Customers` table ([Table 2-2](#customers_table)) will be called the *primary
    key* because it uniquely identifies the customer, and the `Customer_ID` column
    in the `Orders` table ([Table 2-3](#orders_table)) will be called the *foreign
    key* because it refers to the `Customer_ID` column in the `Customers` table. Primary
    keys are expected to be unique, so, for example, the customer IDs in the `Customers`
    table would uniquely identify each customer, while foreign keys are expected to
    be a proper subset of primary keys. If we have a `Customer_ID` value in the `Orders`
    table that does not correspond to any `Customer_ID` value in the `Customers` table,
    we have what is called an *orphaned* foreign key and will not be able to tell
    which customer placed that order. This correspondence between primary and foreign
    keys is called *referential integrity*. Note that by breaking the data into separate
    `Customers` and `Orders` tables, we store customer information exactly once in
    the `Customers` table regardless of how many orders each customer places.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系数据库中解决此问题的解决方案称为*规范化*。这是一种将表分解为较小表以避免重复信息的方法。例如，我们可以将客户信息存储在一张表中，将所有客户的所有订单存储在另一张表中。然后，为了确定哪些订单属于哪些客户，我们会生成一个*键*，比如`顾客_ID`，并将其作为`顾客`表和`订单`表的一部分，使得每个订单都包含对应于`顾客`表中记录的`顾客_ID`值的`顾客_ID`值。`顾客`表中的`顾客_ID`列([Table 2-2](#customers_table))将被称为*主键*，因为它唯一标识客户，而`订单`表中的`顾客_ID`列([Table 2-3](#orders_table))将被称为*外键*，因为它引用了`顾客`表中的`顾客_ID`列。主键预期是唯一的，例如，`顾客`表中的顾客ID将唯一标识每个客户，而外键预期是主键的一个适当子集。如果我们在`订单`表中有一个不对应于`顾客`表中任何`顾客_ID`值的`顾客_ID`值，我们就有了所谓的*孤立的*外键，将无法确定哪位客户下了该订单。主键和外键之间的这种对应关系称为*参照完整性*。请注意，通过将数据分解为单独的`顾客`和`订单`表，我们可以在`顾客`表中仅存储一次客户信息，而不管每个客户下了多少订单。
- en: Table 2-2\. *Customers table*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2-2\. *顾客表*
- en: '| **Customer_ID** | **Name** | **Gender** | **Marital_Status** | **Zip_Code**
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **顾客_ID** | **姓名** | **性别** | **婚姻状况** | **邮政编码** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 112211 | Mary Ng | F | Married | 94301 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 112211 | Mary Ng | F | 已婚 | 94301 |'
- en: '| 299821 | Tom Jones | M | Single | 93443 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 299821 | Tom Jones | M | 单身 | 93443 |'
- en: Table 2-3\. *Orders table*
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2-3\. *订单表*
- en: '| **Customer_ID** | **Order_Number** | **Amount** | **Date** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **顾客_ID** | **订单号** | **金额** | **日期** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 112211 | 2123123 | 987.19 | 7/12/18 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 112211 | 2123123 | 987.19 | 7/12/18 |'
- en: '| 112211 | 2221212 | 12.20 | 9/2/18 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 112211 | 2221212 | 12.20 | 9/2/18 |'
- en: '| 112211 | 2899821 | 56.80.19 | 10/15/18 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 112211 | 2899821 | 56.80.19 | 10/15/18 |'
- en: '| 299821 | 2344332 | 1500.00 | 9/12/18 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 299821 | 2344332 | 1500.00 | 9/12/18 |'
- en: '| 299821 | 2554322 | 11.99 | 9/13/18 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 299821 | 2554322 | 11.99 | 9/13/18 |'
- en: 'In order to determine, say, how many orders are placed by married customers
    compared to unmarried customers, the query would have to combine data from the
    `Orders` and `Customers` tables by performing a SQL operation called a *join*.
    It would look something like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定例如，已婚客户与未婚客户下了多少订单，查询将必须通过执行称为*联接*的SQL操作从`订单`和`顾客`表中组合数据。它看起来会像这样：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This query would return the total of orders for each marital status by joining
    the two tables together, as illustrated in [Table 2-4](#total_orders_for_married_and_single_cust).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询将返回每种婚姻状况的订单总数，通过将两个表联接在一起，如[Table 2-4](#total_orders_for_married_and_single_cust)所示。
- en: Table 2-4\. *Total orders for married and single customers*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2-4\. *已婚和单身顾客的总订单数*
- en: '| **Marital_Status** | **Total_Sales** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **婚姻状况** | **总销售额** |'
- en: '| --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Married | 2,221,222.12 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 已婚 | 2,221,222.12 |'
- en: '| Single | 102,221,222.18 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 单身 | 102,221,222.18 |'
- en: While joins are very powerful and flexible, they are also computationally expensive.
    For larger systems that normalize data into dozens or even hundreds of tables,
    doing all the joins required for every query can bring a highly normalized operational
    database to its knees. To help with that problem, a new solution was developed.
    The idea was to completely separate data from applications and, in fact, to combine
    data from multiple applications in one system and use that system for analytics.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然连接非常强大和灵活，但计算成本高昂。对于将数据规范化为数十甚至数百个表的较大系统而言，为每个查询执行所需的所有连接可能会使高度规范化的操作性数据库陷入困境。为了解决这个问题，开发出了一个新的解决方案。其思想是完全将数据与应用程序分离，并实际上将多个应用程序的数据合并到一个系统中，并将该系统用于分析。
- en: The Analytics Imperative—The Birth of Data Warehousing
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析的必要性——数据仓库的诞生
- en: The original vision was to create a “warehouse” that would store all the data
    and thus all the history about an enterprise, and make it available for analytics.
    In 1990 Walmart created its now-famous data warehouse that helped it to take over
    the retail world by managing its logistics, and kicked off the analytics gold
    rush. Every enterprise soon realized that it could get tremendous value from its
    data and, hopefully, use it to crush its competition. Equally importantly, enterprises
    realized that if they did not invest in analytics, their competitors might crush
    them. Suddenly, everyone was building a data warehouse. Unfortunately, as with
    many multi-year, multi-million-dollar projects driven by fear and hope rather
    than sound use cases and business needs, many of these projects were spectacular
    and well-publicized failures.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的愿景是创建一个“仓库”，用于存储企业的所有数据和历史信息，并使其可供分析使用。1990年，沃尔玛创建了其现今著名的数据仓库，通过管理物流，帮助其主导零售界，并引发了分析黄金时代。很快，每个企业都意识到可以从数据中获得巨大的价值，并希望利用它来击败竞争对手。同样重要的是，企业们意识到，如果不投资于分析，竞争对手可能会压垮它们。突然之间，每个人都在建立数据仓库。不幸的是，与许多由恐惧和希望驱动而非充分的用例和业务需求推动的跨数年、数百万美元项目一样，这些项目中许多都是惊人的、广为人知的失败案例。
- en: 'Fortunately, the industry learned from these failures and continued to innovate
    and improve. Various specialized techniques were developed to optimize these analytical
    platforms for specific use cases and solve the problem of efficiently storing
    and analyzing large quantities of data: breaking large data warehouses into data
    marts, inventing appliances that exploit hardware optimizations to process queries,
    and using columnar stores and in-memory databases. Over time, a large ecosystem
    of tools was developed to create and manage data warehouses, to manage data quality,
    and to keep track of data models and metadata. Some of the more prominent technologies
    include:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，行业从这些失败中吸取了教训，并继续创新和改进。为了优化这些分析平台，针对特定用例开发了各种专业技术，解决了高效存储和分析大量数据的问题：将大型数据仓库拆分为数据集市，发明利用硬件优化处理查询的设备，并使用列存储和内存数据库。随着时间的推移，开发了大量工具生态系统，用于创建和管理数据仓库、管理数据质量，并跟踪数据模型和元数据。一些较为突出的技术包括：
- en: Extract, transform, load (ETL) and extract, load, transform (ELT) tools
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽取、转换、加载（ETL）和抽取、加载、转换（ELT）工具
- en: Data quality (DQ) and profiling tools
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量（DQ）和分析工具
- en: Data modeling tools
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据建模工具
- en: Business glossaries
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务词汇表
- en: Metadata repositories
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据存储库
- en: Data governance tools
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据治理工具
- en: Master data management (MDM) systems
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主数据管理（MDM）系统
- en: Enterprise information integration (EII), data federation, and data virtualization
    tools
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业信息集成（EII）、数据联合和数据虚拟化工具
- en: 'In addition, tools were developed to help create reports and analytics, including:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还开发了用于创建报告和分析的工具，包括：
- en: Reporting tools
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告工具
- en: Online analytical processing (OLAP) tools
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线分析处理（OLAP）工具
- en: Business intelligence (BI) tools
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业智能（BI）工具
- en: Data visualization tools
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化工具
- en: Advanced analytic tools
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级分析工具
- en: We’ll look at some of these in the following section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看看其中一些工具。
- en: The Data Warehouse Ecosystem
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据仓库生态系统
- en: '[Figure 2-1](#data_flow_in_the_data_warehousing_ecosys) illustrates the flow
    of data through the data warehouse ecosystem. Subsequent sections will explore
    the function of and data flow in each component. These tools are not the subject
    of this book, but must be understood and put into context so that you know what
    data processes are in place today in most organizations, and what functions we
    are trying to reproduce or replace with a data lake.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-1](#data_flow_in_the_data_warehousing_ecosys)说明了数据通过数据仓库生态系统的流动。后续章节将探讨每个组件的功能和数据流。这些工具不是本书的主题，但必须理解并置于上下文中，以便了解今天大多数组织中的数据处理流程及我们试图用数据湖复制或替代的功能。'
- en: '![Data flow in the data warehousing ecosystem](Images/ebdl_0201.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![数据仓库生态系统中的数据流](Images/ebdl_0201.png)'
- en: Figure 2-1\. Data flow in the data warehousing ecosystem
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1. 数据仓库生态系统中的数据流
- en: 'In addition to a data flow, the data warehouse ecosystem has a rich metadata
    flow as well as a number of metadata-specific tools, as illustrated in [Figure 2-2](#metadata_flow_in_the_data_warehouse_ecos).
    Subsequent chapters will describe the metadata flow between various tools. The
    two end user-facing components in the ecosystem are the ones at the top of the
    diagram: the business glossary and various reporting tools. A wide swath of IT
    staff—ETL developers, data and system architects, data modelers, data stewards,
    report and BI developers, and database administrators—use the rest of the tools
    to make sure the end users get their reports and analytics. Note that I am not
    including general administration, backup, management, and other tools that are
    not specific to data warehouses and am simplifying some of the components—for
    example, I include data profiling in DQ, lineage in ETL, and so on.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据流之外，数据仓库生态系统还有丰富的元数据流，以及许多特定于元数据的工具，如[图2-2](#metadata_flow_in_the_data_warehouse_ecos)所示。后续章节将描述各种工具之间的元数据流。生态系统中面向最终用户的两个组件位于图表顶部：业务词汇表和各种报表工具。广泛的IT人员群体——ETL开发人员、数据和系统架构师、数据建模者、数据管理员、报表和BI开发人员以及数据库管理员——使用其余工具确保最终用户获取其报告和分析数据。请注意，我没有包括一般的管理、备份、管理和其他与数据仓库无关的工具，并简化了一些组件——例如，我将数据剖析归入DQ，ETL中的血统等等。
- en: '![Metadata flow in the data warehouse ecosystem](Images/ebdl_0202.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![数据仓库生态系统中的元数据流](Images/ebdl_0202.png)'
- en: Figure 2-2\. Metadata flow in the data warehouse ecosystem
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2. 数据仓库生态系统中的元数据流
- en: Storing and Querying the Data
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储与查询
- en: 'The database is at the heart of a data warehouse. Usually, it is a relational
    database optimized for analytics-type processing: large, long queries; aggregation;
    and multi-table joins. The database is usually heavily indexed and tuned to ensure
    optimal performance for the most common queries.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库是数据仓库的核心。通常，它是为分析型处理而优化的关系型数据库：大型、长时间查询；聚合；以及多表连接。数据库通常经过大量索引和调优，以确保对最常见的查询有最佳的性能。
- en: Dimensional modeling and star schemas
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度建模和星型模式
- en: When relational databases are used to support operational systems and applications,
    data is usually stored in highly normalized data models. Normalized data models
    attempt to create tables with minimum redundancy and the smallest possible number
    of fields; this makes updates very fast.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当关系型数据库用于支持操作系统和应用程序时，数据通常存储在高度规范化的数据模型中。规范化的数据模型试图创建最小冗余和最少字段的表；这使得更新非常迅速。
- en: For instance, a table representing sales might contain very little information
    except some generated keys for product, buyer, retail location, and the like.
    In order to find useful information, such as the city corresponding to a retail
    location, one has to join the table to another in an expensive computation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，表示销售的表可能除了一些用于产品、买家、零售位置等生成的键之外几乎不包含任何信息。为了找到有用的信息，例如与零售位置对应的城市，必须将该表与另一表进行昂贵的计算。
- en: On the other hand, most data warehouses favor *denormalized* data models, where
    each table contains as many related attributes as possible.  In this way, all
    the information can be processed by a single pass through the data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，大多数数据仓库更青睐*去规范化*的数据模型，其中每个表尽可能包含多个相关属性。这样，所有信息可以通过一次数据遍历进行处理。
- en: Next, because data warehouses typically contain data from many sources and applications,
    each with its own schema, data from these sources has to be normalized to convert
    it to a single schema. A popular data model used by data warehouses is the *star
    schema*, introduced by Ralph Kimball and Margy Ross in 1996 in the first edition
    of *The Data Warehouse Toolkit* (Wiley). This schema consists of a set of *dimension*
    and *fact* tables.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，因为数据仓库通常包含来自许多来源和应用程序的数据，每个都有自己的架构，因此需要将这些来源的数据规范化，以转换为单一模式。数据仓库常用的数据模型是由Ralph
    Kimball和Margy Ross在1996年的《数据仓库工具包》第一版中引入的*星型模式*。该模式包括一组*维度*和*事实*表。
- en: 'The dimension tables represent the entities being analyzed: in a sales context,
    there might be a customer dimension table containing all the attributes of a customer
    (name, address, etc.), a time dimension table with all the attributes of time
    (date, fiscal year, etc.), and a product dimension table with all the attributes
    of a product (make, model, price, etc.).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 维度表代表正在分析的实体：在销售背景下，可能存在包含客户所有属性（姓名、地址等）的客户维度表，包含所有时间属性（日期、财政年度等）的时间维度表，以及包含所有产品属性（制造商、型号、价格等）的产品维度表。
- en: The fact table contains all the activities that involve the dimensions. For
    example, a transaction fact table would have a record for every line item in every
    order. The record would contain the customer key from the customer dimension table
    for the customer who placed the order, the time key from the time dimension table
    for when the order was placed, and the product key from the product dimension
    table for the product being ordered, as well as the attributes of the transaction
    itself (order and line item ID, quantity, price paid, etc.). The structure of
    the tables is symbolically represented by [Figure 2-3](#tables_in_a_simple_star_schema).
    With data organized into a star schema, even general-purpose relational databases
    such as Oracle, IBM DB2, and Microsoft SQL Server can achieve reasonable performance,
    and many now include specialized query optimizations for handling star schema–like
    joins.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 事实表包含涉及维度的所有活动。例如，交易事实表中每个订单的每一行都会有一条记录。记录中包含来自客户维度表的客户关键字，该客户下订单的时间维度表的时间关键字，以及产品维度表中所订购产品的产品关键字，以及交易本身的属性（订单和行项目ID、数量、付款价格等）。表的结构通过[图 2-3](#tables_in_a_simple_star_schema)进行符号表示。将数据组织成星型模式后，即使是如Oracle、IBM
    DB2和Microsoft SQL Server等通用关系数据库也能够实现合理的性能，而且现在许多数据库还包括了处理星型模式连接的专门查询优化。
- en: '![Tables in a simple star schema](Images/ebdl_0203.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![简单星型模式中的表](Images/ebdl_0203.png)'
- en: Figure 2-3\. Tables in a simple star schema
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 简单星型模式中的表
- en: Slowly changing dimensions
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓慢变化维度
- en: 'To allow accurate data analysis, it’s necessary to keep track of a person’s
    state over time. This ensures that each transaction corresponds to the person’s
    state at the time of the transaction.  Since the person’s state does not change
    very often, a special construct has been developed to represent those changes:
    *slowly changing dimensions*. This concept was also introduced by Kimball and
    Ross in *The Data Warehouse Toolkit*.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行准确的数据分析，有必要随时间追踪个人的状态。这确保了每个交易都对应于交易时个人的状态。由于个人的状态不经常变化，因此开发了一种特殊的结构来表示这些变化：*缓慢变化维度*。这一概念也由Kimball和Ross在《数据仓库工具包》中引入。
- en: The goal of a slowly changing dimension is to keep track of a dimension entity’s
    (e.g., person’s) state over time, so that the transactions (or facts) corresponding
    to the entity’s state reflect that state over time, thus making analysis more
    accurate in the long term.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 缓慢变化维度的目标是随时间跟踪维度实体（例如个人）的状态，以便与实体状态相对应的交易（或事实）反映出长期分析中的状态，从而使分析更加准确。
- en: This section illustrates the basic concept by describing the most common type
    of slowly changing dimension, which tracks historical data by creating multiple
    records. This is called a type 2 dimension.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过描述最常见的缓慢变化维度类型来阐述基本概念，该类型通过创建多条记录来跟踪历史数据。这被称为类型 2 维度。
- en: Let’s say we have a store that has kept track of customer purchases along with
    customer demographics. For our example, a hypothetical Mary is a single person
    who has been shopping at the store for five years. After five years, Mary gets
    married and becomes a homeowner; two years later she becomes a parent. She continues
    shopping at the store.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个商店，该商店记录了客户购买及其人口统计信息。在我们的示例中，假设玛丽是一个单身人士，在该商店购物已有五年时间。五年后，玛丽结婚成为房主；两年后成为父母。她继续在该店购物。
- en: In [Figure 2-4](#shopping_data_with_slowly_changing_dimen), Mary’s purchases
    for years 1–5 reflect those that might be made by a single person; for years 5–7
    they reflect those of a homeowner; and for the subsequent years those of a new
    parent.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2-4](#shopping_data_with_slowly_changing_dimen)中，玛丽年份1-5的购买反映了可能由单个人完成的购买；年份5-7反映了一个房主的购买；随后的年份反映了一个新父母的购买。
- en: '![Shopping data with slowly changing dimensions](Images/ebdl_0204.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![具有慢变化维度的购物数据](Images/ebdl_0204.png)'
- en: Figure 2-4\. Shopping data with slowly changing dimensions
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 具有慢变化维度的购物数据
- en: Without slowly changing dimensions, we would have one single record for Mary
    in our customer table that reflects her current state as a parent (see [Figure 2-5](#shopping_data_without_slowly_changing_di)).
    So, if we were to analyze how many people with children spend money on expensive
    travel or sports gear, we would mistakenly attribute her purchases from years
    1–7 (as a person without children) to her current category.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有慢变化维度，我们在客户表中只会有一个玛丽的单一记录，反映她当前作为一个家长的状态（见[图 2-5](#shopping_data_without_slowly_changing_di)）。因此，如果我们分析有多少有孩子的人在昂贵的旅行或运动装备上花了多少钱，我们会错误地将她年份1-7的购买（作为一个没有孩子的人）归因于她当前的类别。
- en: '![Shopping data without slowly changing dimensions](Images/ebdl_0205.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![没有慢变化维度的购物数据](Images/ebdl_0205.png)'
- en: Figure 2-5\. Shopping data without slowly changing dimensions
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 没有慢变化维度的购物数据
- en: However, with the help of slowly changing dimensions, designed to capture the
    state of the person at the time of the transaction, the customer table will have
    a different record for Mary for each change in her state, as shown in [Figure 2-6](#every_change_in_status_creates_a_new_rec).
    So, when Mary’s purchases are analyzed, they will be attributed to a person with
    the correct state.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，借助专门捕获交易时个人状态的慢变化维度的帮助，客户表将对玛丽的每次状态变化都有不同的记录，如[图 2-6](#every_change_in_status_creates_a_new_rec)所示。因此，当分析玛丽的购买时，它们将被归因于具有正确状态的人。
- en: '![Every change in status creates a new record for Mary in the customer dimension
    table, with fields including the start and end date of the record’s validity](Images/ebdl_0206.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![每次状态变化都会在客户维度表中为玛丽创建一个新记录，字段包括记录有效性的开始和结束日期](Images/ebdl_0206.png)'
- en: Figure 2-6\. Every change in status creates a new record for Mary in the customer
    dimension table, with fields including the start and end date of the record’s
    validity
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 每次状态变化都会在客户维度表中为玛丽创建一个新记录，字段包括记录有效性的开始和结束日期
- en: Because slowly changing dimensions add so much complexity to both ETL jobs and
    analytic queries, only the history for the most critical attributes (such as family
    status in the previous example) are tracked. That means that if one of the untracked
    attributes becomes critical, the history of its evolution will not be available.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于慢变化维度给ETL作业和分析查询增加了如此多的复杂性，只有最关键属性的历史（例如前面示例中的家庭状态）被跟踪。这意味着如果未跟踪的属性之一变得关键，其演变历史将不可用。
- en: Massively parallel processing (MPP) systems
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大规模并行处理（MPP）系统
- en: An alternative to using star schemas is to use a cluster of massively parallel
    computers that appear to the end user or BI tool as a single database. Teradata
    quickly became the database of choice for the largest data warehouses after developing
    such MPP technology. By using proprietary hardware, software, and networking protocols,
    Teradata data warehouses were able to achieve a scalability unmatched in the industry
    prior to the advent of Hadoop. Since Teradata was able to operate computers in
    parallel, it did not require the users to model the data a certain way. Instead,
    it relied on its query optimizer to execute complex queries in the most efficient
    way possible.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大量并行计算机集群作为单个数据库的替代方法，这些计算机对最终用户或BI工具呈现为单个数据库。在开发了这种MPP技术后，Teradata迅速成为最大数据仓库的首选数据库。通过使用专有的硬件、软件和网络协议，Teradata数据仓库能够实现业界无与伦比的可扩展性，这一优势在Hadoop出现之前是无法匹敌的。由于Teradata能够并行操作计算机，因此不需要用户按照特定方式对数据建模。相反，它依赖于其查询优化器以尽可能高效的方式执行复杂查询。
- en: Data warehouse (DW) appliances
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库（DW）设备
- en: DW appliances try to solve the problem of taking a high-performance database
    running on proprietary hardware and software and making it easier to deploy and
    manage than a standard off-the-shelf database. IBM Netezza is a good example of
    such a DW appliance. While not as scalable as MPP systems such as Teradata and
    IBM DB2, these appliances are much easier to deploy and tune and can address the
    needs of the majority of data warehouses and data marts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: DW设备试图解决的问题是将运行在专有硬件和软件上的高性能数据库，比标准的现成数据库更容易部署和管理。IBM Netezza就是这种DW设备的一个很好的例子。虽然不如诸如Teradata和IBM
    DB2等MPP系统可扩展，但这些设备更容易部署和调优，并且可以满足大多数数据仓库和数据集市的需求。
- en: Columnar stores
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列存储
- en: Relational databases model data as tables with rows (sometimes called records)
    and columns (sometimes called fields). For example, suppose you have a `Customers`
    table with 300 columns each containing a piece of data about a customer, such
    as `Name`, `Address`, `Age`, `Date_Of_First_Purchase`, and so on. Traditional
    relational databases store each row of data together, so all the information about
    the first customer is stored, then all the information about the second customer,
    and so on. In order to store 300 attributes with an average size of, say, 5 bytes/attribute,
    the database would need to use 1,500 bytes, or roughly 1.5 KB, for each user.
    If there were a million customers, the database would need at least 1.5 TB of
    storage to store all the customer data (in reality, it would need even more storage
    because the records won’t fit neatly into disk blocks, and because there are underlying
    data structures and indexes that take up space as well). If the user wanted to
    find out how many customers were under the age of 30, the database would have
    to read each record in the table—in other words, it would have to read all 1.5
    TB.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关系数据库将数据建模为包含行（有时称为记录）和列（有时称为字段）的表。例如，假设您有一个`Customers`表，其中有300列，每列包含有关客户的数据，例如`姓名`，`地址`，`年龄`，`第一次购买日期`等。传统的关系数据库将每行数据存储在一起，因此所有关于第一个客户的信息都被存储，然后是所有关于第二个客户的信息，依此类推。为了存储300个属性，每个属性的平均大小为5字节，数据库需要使用1,500字节，或者大约1.5
    KB，来存储每个用户。如果有一百万个客户，数据库至少需要1.5 TB的存储空间来存储所有客户数据（实际上，需要更多的存储空间，因为记录不会整齐地适配到磁盘块中，并且还有底层数据结构和索引占用的空间）。如果用户想要知道年龄小于30岁的客户数量，数据库将不得不读取表中的每条记录，换句话说，它将不得不读取所有的1.5
    TB。
- en: A *columnar* database would store all the data for each column together instead
    of storing all the data for each row together. For example, it would store the
    age for each customer together with a record identifier that indicates which customer
    record this value belongs to in the same storage blocks. If age takes 2 bytes
    and a record identifier is, say, 6 bytes, the database would need 8 bytes per
    field, or 8 GB for a million customers. Since it would have to read only 8 GB
    to answer the question of how many customers are under 30, it would be able to
    perform this query about 200 times faster. Of course, this improves performance
    only for queries requiring a few columns. If the query wanted to return all the
    information (all 300 columns) for a single user, a row-oriented database would
    just have to read a single block, while a columnar database would have to read
    300 blocks. In other words, columnar databases are employed for very specific
    query patterns, unlike the relational model, which is more general-purpose. Some
    well-known vertical databases include Sybase IQ and Vertica.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*列存储*数据库将每列的所有数据一起存储，而不是将每行的所有数据一起存储。例如，它会将每位客户的年龄与记录标识符一起存储在相同的存储块中，指示这个值属于哪个客户记录。如果年龄占用2个字节，记录标识符占用6个字节，那么数据库每个字段需要8个字节，或者对于一百万个客户需要8
    GB。由于只需读取8 GB即可回答“30岁以下客户有多少”的问题，它能够比原来快约200倍。当然，这仅对于需要少数列的查询性能有所改进。如果查询想要返回单个用户的所有信息（全部300列），行存储数据库只需读取一个块，而列存储数据库则需要读取300个块。换句话说，列存储数据库用于非常特定的查询模式，而关系模型则更通用。一些知名的竖直数据库包括Sybase
    IQ和Vertica。'
- en: In-memory databases
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存数据库
- en: Although traditionally memory access was many orders of magnitude faster than
    disk, it was also more expensive. Hence, much of database development focused
    on optimizing disk access. As my Stanford database professor Gio Wiederhold was
    fond of repeating, “Any database engineer worth his salt always counts block reads.”
    A lot of work went into minimizing those block reads by optimizing disk access,
    caching and pre-caching, creating indexes to reduce the number of block reads,
    and so on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统上内存访问速度比磁盘快几个数量级，但也更昂贵。因此，数据库开发的大部分工作集中在优化磁盘访问上。正如我的斯坦福数据库教授Gio Wiederhold喜欢重复的那样，“任何一位合格的数据库工程师都会计算块读取次数。”大量工作投入到通过优化磁盘访问、缓存和预缓存、创建索引以减少块读取次数等来最小化这些块读取次数。
- en: As the prices for memory fell and it became practical to store larger amounts
    of data in memory, the first database systems designed to keep and process data
    in memory came along. TimesTen was one of those pioneers; as the name suggests,
    it attempted to achieve 10 times the performance of traditional disk-based systems
    by focusing on storing and processing data in memory. Recently, there has been
    a renewed push for in-memory databases from vendors like SAP with its HANA system
    and the Apache Spark project, among many other efforts.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随着内存价格下降并且在内存中存储更大量数据变得实际可行，第一批设计用于在内存中保留和处理数据的数据库系统应运而生。TimesTen就是其中的先驱之一；正如其名称所示，它试图通过专注于在内存中存储和处理数据来实现传统基于磁盘系统10倍的性能。最近，像SAP的HANA系统和Apache
    Spark项目等厂商也再次推动内存数据库的发展。
- en: Loading the Data—Data Integration Tools
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据——数据集成工具
- en: One important thing to keep in mind is that the data in a data warehouse is
    loaded from applications and operational systems. Therefore, the first order of
    business is loading the data warehouse with data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的事情要记住是数据仓库中的数据是从应用程序和操作系统加载的。因此，首要任务是用数据加载数据仓库。
- en: There are various approaches, tools, and techniques for this.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种方法、工具和技术可以实现这一点。
- en: ETL
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ETL
- en: ETL technology has been around for more than 20 years. Most modern ETL tools
    were developed in the mid- to late 1990s as part of the data warehousing movement.
    When relational databases were used to support operational systems and applications,
    data was usually stored in highly normalized data models. We won’t go into too
    many details that are available in most relational database books, but normalized
    data models attempt to create tables with a minimum amount of redundancy and the
    smallest possible number of fields, so the updates are very fast. On the other
    hand, as we saw in [“Dimensional modeling and star schemas”](#dimensional_modeling_and_star_schemas),
    most data warehouses favor denormalized data models where each table contains
    as many related attributes as possible, so all the information can be processed
    by a single pass through the data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ETL技术已经存在20多年了。大多数现代ETL工具是作为数据仓库运动的一部分在1990年代中期至晚期开发的。当关系数据库用于支持操作系统和应用程序时，数据通常存储在高度归一化的数据模型中。我们不会详细讨论这些细节，这些内容在大多数关系数据库书籍中都有描述，但归一化数据模型试图创建具有最小冗余量和最少字段数量的表，因此更新非常快速。另一方面，正如我们在[“维度建模和星型模式”](#dimensional_modeling_and_star_schemas)中看到的那样，大多数数据仓库倾向于非归一化数据模型，其中每个表包含尽可能多的相关属性，因此所有信息可以通过数据的单次通过来处理。
- en: Data that comes from an operational system may contain customer information
    in several tables with different formats and representations. It would be the
    job of an ETL tool to convert the various representations into a common customer
    dimension, like the one we saw in [Figure 2-3](#tables_in_a_simple_star_schema),
    and to make sure that the same customer record from different systems is used
    to create or update a single customer record in the customer dimension ([Figure 2-7](#an_etl_tool_extracts_data_from_multiple)).
    Such a dimension is called a *conforming dimension*, because the customer dimension
    would make all incoming data conform to a single format and the same customer
    would be identified across the various records in different systems. In [Figure 2-7](#an_etl_tool_extracts_data_from_multiple),
    the operational system is using two tables to store customer data. Furthermore,
    these tables have different representations of the customer information. For example,
    first and last names are in different fields, there is a date of birth instead
    of an age, and multiple addresses are kept for each customer. The job of the ETL
    tool is to convert this representation into the representation expected by the
    data warehouse’s customer dimension table by concatenating names into one field,
    calculating age from date of birth, and choosing the best address and concatenating
    it into a single string.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 来自操作系统的数据可能包含不同格式和表示的多个表中的客户信息。ETL工具的工作是将各种表示转换为一个共同的客户维度，就像我们在[图 2-3](#tables_in_a_simple_star_schema)中看到的那样，并确保来自不同系统的同一客户记录用于创建或更新客户维度中的单个客户记录（[图 2-7](#an_etl_tool_extracts_data_from_multiple)）。这样的维度称为*符合维度*，因为客户维度将所有传入的数据符合到一个单一的格式，并且相同的客户将在不同系统中的各种记录中被识别。在[图 2-7](#an_etl_tool_extracts_data_from_multiple)中，操作系统使用两个表存储客户数据。此外，这些表有不同的客户信息表示。例如，名字和姓氏在不同的字段中，有出生日期而不是年龄，并且每个客户保存多个地址。ETL工具的任务是通过将名字连接到一个字段中，从出生日期计算年龄，并选择最佳地址并将其连接成一个字符串来将此表示转换为数据仓库客户维度表期望的表示。
- en: '![An ETL tool extracts data from multiple tables to create the customer dimension](Images/ebdl_0207.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![ETL工具从多个表中提取数据以创建客户维度](Images/ebdl_0207.png)'
- en: Figure 2-7\. An ETL tool extracts data from multiple tables to create the customer
    dimension
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7\. ETL工具从多个表中提取数据以创建客户维度
- en: Furthermore, because data warehouses typically contain data that comes from
    many different sources and applications, each with its own schema, data from each
    of these sources has to be normalized and converted to the single schema.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于数据仓库通常包含来自许多不同源和应用程序的数据，每个都有其自己的模式，因此来自这些源的数据必须被归一化并转换为单一模式。
- en: ETL versus ELT
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ETL与ELT
- en: For many years, Teradata and other high-end database vendors encouraged their
    customers to use their database engines to do the required transformations instead
    of leaving this to ETL tools. They argued that only highly scalable systems like
    theirs could handle the volume and complexity of loading their data warehouses.
    This processing is called ELT (extract, load, transform). In other words, the
    data gets loaded into the data warehouse as is and then converted into the right
    representation using the database engine ([Figure 2-8](#comparison_of_etl_and_elt)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，Teradata和其他高端数据库供应商鼓励其客户使用其数据库引擎来执行所需的转换，而不是将其留给ETL工具处理。他们认为只有像他们的这样高度可扩展的系统才能处理其数据仓库的体积和复杂性。这个处理过程被称为ELT（提取、加载、转换）。换句话说，数据以其原样加载到数据仓库中，然后使用数据库引擎将其转换为正确的表示形式（[图2-8](#comparison_of_etl_and_elt)）。
- en: '![Comparison of ETL and ELT](Images/ebdl_0208.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![ETL与ELT的比较](Images/ebdl_0208.png)'
- en: Figure 2-8\. Comparison of ETL and ELT
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8\. ETL与ELT的比较
- en: Federation, EII, and data virtualization tools
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联合、EII和数据虚拟化工具
- en: 'When data comes from multiple systems, the data warehouse approach is to bring
    it all together in one place, integrate it into a single conforming schema, and
    then use it for analytics queries. An alternative approach is to create a logical
    or virtual schema across multiple systems and then issue queries against that
    virtual schema. This approach goes by many names, the most common being federation,
    enterprise information integration (EII), and data virtualization. The main scenarios
    where this approach is more appropriate than using a data warehouse are:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据来自多个系统时，数据仓库的方法是将所有数据汇总到一个地方，将其集成到单一的符合模式中，然后用于分析查询。另一种方法是在多个系统之间创建逻辑或虚拟模式，然后针对该虚拟模式发出查询。这种方法有许多名称，最常见的是联合、企业信息集成（EII）和数据虚拟化。比使用数据仓库更适合的主要情况包括：
- en: When data must be kept fresh in the face of changes. Because these tools execute
    queries against original sources, the results are always up to date, while data
    warehouses usually have a lag depending on how often they are refreshed.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当面对数据必须保持更新时。因为这些工具针对原始源执行查询，所以结果始终是最新的，而数据仓库通常有一定的滞后取决于刷新频率。
- en: When data access is infrequent. Building very expensive data warehouses for
    data that may only be used once a year or even less is not cost-effective.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据访问不频繁时。为可能只会被使用一年甚至更少的数据建立非常昂贵的数据仓库并不划算。
- en: When compliance and data residency clauses may constrain data from being copied
    from one source location to a target destination.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当合规性和数据驻留条款可能限制数据从一个源位置复制到目标位置时。
- en: 'On the other hand, this approach has several important drawbacks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这种方法有几个重要的缺点：
- en: Labor-intensive manual process
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 劳动密集型的手动过程
- en: Virtual tables must be manually defined across disparate systems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟表必须手动定义在不同系统之间。
- en: Schema and logic changes
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 架构和逻辑变更
- en: Although a schema change can cause ETL jobs loading a data warehouse to break,
    it will affect only the latest data, and the bulk of the data will still be available
    for analysis. With data virtualization tools, a schema change can break the queries
    and make all data unavailable until the queries are fixed.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管架构更改可能导致加载数据仓库的ETL作业中断，但仅会影响最新数据，大部分数据仍然可用于分析。使用数据虚拟化工具时，架构更改可能会导致查询中断，并使所有数据在修复查询之前都不可用。
- en: Performance
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 性能
- en: Some queries that span multiple systems (called *federated queries*) have significant
    performance challenges. For example, complex multi-table joins and correlated
    subqueries across multiple databases can take a lot longer to execute than when
    all the tables are in the same database. Furthermore, while a data warehouse can
    be optimized for analytics with additional indexes and tuning, the operational
    source systems usually cannot be optimized for analytic queries without slowing
    down the operations that they were designed for.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 某些涉及多个系统的查询（称为*联合查询*）存在显著的性能挑战。例如，复杂的多表连接和跨多个数据库的相关子查询执行时间比所有表都在同一数据库时要长得多。此外，虽然数据仓库可以通过额外的索引和调整进行分析优化，但操作源系统通常不能在不减慢其设计用途的操作的情况下优化分析查询。
- en: Frequency
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 频率
- en: Each query is effectively executing a full integration job every time it runs.
    Therefore, if there are a lot of queries against the virtual schema, it becomes
    much more advantageous to extract the data once, store it in a data warehouse,
    and query it there. Doing so substantially reduces the load on the source systems
    and is far more computationally effective than reading and integrating the source
    tables again and again for each query. Some data virtualization tools mitigate
    the waste by caching data in some staging area, but in general, if the access
    frequency is very high and data freshness is not critical, a data warehouse may
    be a better choice.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个查询在每次运行时都有效执行完整的集成作业。因此，如果对虚拟模式有大量查询，一次性提取数据并将其存储在数据仓库中，然后在那里进行查询会更有利。这样做大大减少了源系统的负载，并且在计算上比每次读取和集成源表要更有效。一些数据虚拟化工具通过在某些临时区域缓存数据来减少浪费，但总体而言，如果访问频率非常高且数据新鲜度不是关键问题，数据仓库可能是更好的选择。
- en: As data volume and variety keep growing, data virtualization tools try to keep
    up by improving query optimization and adding in-memory processing and caching.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量和多样性的持续增长，数据虚拟化工具通过改进查询优化和增加内存处理和缓存来跟上发展。
- en: '[Figure 2-9](#comparison_of_data_warehouse_and_virtual) illustrates a data
    warehouse approach versus a data virtualization approach. In the top diagram,
    the two tables from different databases are combined during the ETL process and
    the result is persisted as a table in the data warehouse. All queries are then
    executed against this table. In the bottom diagram, a virtual view is created
    through data virtualization while the data physically remains in the original
    databases.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-9](#comparison_of_data_warehouse_and_virtual)说明了数据仓库方法与数据虚拟化方法的比较。在顶部图表中，在ETL过程中将来自不同数据库的两个表合并，并将结果持久化为数据仓库中的一个表。然后，所有查询都针对这个表执行。在底部图表中，通过数据虚拟化创建虚拟视图，而数据仍然物理上存储在原始数据库中。'
- en: '![Comparison of data warehouse and virtualization approaches](Images/ebdl_0209.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![数据仓库与虚拟化方法的比较](Images/ebdl_0209.png)'
- en: Figure 2-9\. Comparison of data warehouse and virtualization approaches
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 数据仓库与虚拟化方法的比较
- en: Organizing and Managing the Data
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织和管理数据
- en: The size and complexity of data warehouses has led to the development of a wide
    range of tools to organize them, check the quality of the data, and govern access.
    This final section explains the purpose and basic operation of these tools.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库的规模和复杂性促使开发了多种工具来组织它们、检查数据质量并管理访问权限。本节最后解释了这些工具的目的和基本操作。
- en: Data quality tools
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据质量工具
- en: Data quality is a mature discipline within data management. It involves defining
    quality rules, applying those rules to data to detect violations—often called
    exceptions—and fixing those exceptions. Data quality is a big topic, and entire
    books have been written about it, so this section provides just a quick summary
    designed to get the general approach across.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量是数据管理中的一门成熟学科。它涉及定义质量规则，将这些规则应用于数据以检测违规（通常称为异常），并修复这些异常。数据质量是一个广泛的话题，已经有很多书籍专门讨论这个问题，因此本节仅提供一个旨在传达一般方法的快速总结。
- en: 'Data quality rules come in many shapes and sizes, but can generally be broken
    into several broad categories:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量规则有多种形式和大小，但通常可以分为几个主要类别：
- en: Scalar
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 标量
- en: Applied to a specific value. For example, `Name` is a required field and should
    have a value; `Salary` should be a number; `Age` should be between 0 and 150.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于特定值。例如，`Name`是必填字段并且应该有值；`Salary`应该是一个数字；`Age`应该在0到150之间。
- en: Field level
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 字段级别
- en: Applied to all the values in a field. The most common examples have to do with
    field uniqueness (for example, `Customer_ID` should be unique) and field density
    (for example, `Name` cannot be empty), but there may be other rules, such as that
    `Income` should fall in the range of *X* to *Y*. While some of these rules, like
    that of density, may seem redundant—for example, `Name` not being empty can be
    expressed as a scalar test—the advantage of doing it at the field level is that
    we can provide tolerances; for instance, we can tolerate up to 10% of customer
    names being empty.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于字段中的所有值。最常见的例子与字段的唯一性有关（例如，`Customer_ID`应该是唯一的）和字段的密度（例如，`Name`不能为空），但也可能存在其他规则，比如`Income`应该在*X*到*Y*的范围内。虽然其中一些规则，如密度规则，可能看起来是多余的，例如，`Name`不为空可以通过标量测试来表达，但在字段级别执行的优势在于我们可以提供公差；例如，我们可以容忍高达10%的客户姓名为空。
- en: Record level
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 记录级别
- en: Applied to all the fields in a single record. For example, we can specify that
    if the `US_CITIZEN` field is `True` then the `Social_Security_Number` field should
    not be empty, or that a root element in the `Orders` record in a JSON file should
    have exactly three children.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于单个记录的所有字段。例如，我们可以指定如果 `US_CITIZEN` 字段为 `True`，则 `Social_Security_Number`
    字段不应为空，或者 JSON 文件中 `Orders` 记录中的根元素应有确切的三个子元素。
- en: Data set (table/file) level
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集（表/文件）级别
- en: Applied to the entire data set. These are not common and usually involve the
    number of records. For example, a data set containing sensor data should have
    at least one event per sensor per hour.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于整个数据集。这些情况并不常见，通常涉及记录数量。例如，包含传感器数据的数据集应每小时至少有一个事件。
- en: Cross–data set level
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 跨数据集级别
- en: 'Applied across data sets. Referential integrity rules are very common in relational
    systems. They basically state that a primary key should be unique and that a foreign
    key field should not have any value that does not exist in the primary key field:
    `select count(distinct order_id) from orders where fulfilled = 1` should be the
    same as `select count(distinct order_id) from shipments`, or the number of rows
    in file 1 should be smaller than or equal to the number of rows in file 2.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于数据集。在关系系统中，参照完整性规则非常常见。它们基本上规定主键应该是唯一的，并且外键字段不应该有任何不存在于主键字段中的值：`select count(distinct
    order_id) from orders where fulfilled = 1` 应该与 `select count(distinct order_id)
    from shipments` 相同，或者文件1中的行数应该小于或等于文件2中的行数。
- en: Some of the data quality rules can be fixed programmatically, while others require
    manual intervention. For example, a missing customer name may be looked up in
    a master customer list or a missing gender can be deduced from the salutation
    or first name. On the other hand, sometimes there is no way of programmatically
    fixing data quality problems. In such cases, data needs to either be fixed manually
    or be fixed differently depending on the project it is being used for. For example,
    if a transaction for an account number is missing a digit, the analyst curating
    the data may need to manually search through the accounts to see which one(s)
    it might match, and then look at the account history to see whether there is a
    transaction for that amount on that date. If customer income information is missing
    and the analyst has no way to get it, they may decide to take out the records
    with missing income values, to treat the income as 0, or to replace the missing
    income values with an average income, depending on the type of project they are
    working on.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据质量规则可以通过程序固定，而另一些则需要手动干预。例如，一个缺失的客户姓名可以在主客户列表中查找，或者缺失的性别可以从称谓或名字中推断出来。另一方面，有时无法通过程序修复数据质量问题。在这种情况下，数据需要手动修复，或者根据正在处理的项目以不同方式进行修复。例如，如果帐户号码的交易缺少一位数字，负责筛选数据的分析师可能需要手动搜索帐户，查看可能匹配的帐户，并查看帐户历史记录以确定该日期的该金额是否有交易。如果客户收入信息丢失且分析师无法获取，则可能决定删除缺失收入值的记录，将收入视为0，或者用平均收入替换缺失的收入值，具体取决于他们正在处理的项目类型。
- en: In the absence of data quality rules, *data profiling* is a technique of automatically
    gathering statistics about data to then ascertain its quality. Profiling tools
    usually read all the data and, for each field, keep track of how many values are
    of which type (string versus number versus date), how many values are empty (`NULL`s),
    the minimum and maximum values, as well as the most frequent values for each field
    and some other statistics depending on the field. The advantages of using profiling
    are that it does not require the design of any quality rules, and that analysts
    can use the results to ascertain the quality of the data with regard to the specific
    project they are working on. For example, if the `Age` field is mostly empty but
    is not needed for a specific project, the data set may have acceptable quality
    levels. While pretty much all data quality tools include profiling, profiling
    is also used by a variety of other tools, for everything from data prep to data
    discovery.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有数据质量规则的情况下，*数据剖析*是一种自动收集关于数据统计信息的技术，然后确定其质量的技术。剖析工具通常会读取所有数据，并针对每个字段跟踪哪些类型的值有多少（字符串、数字、日期），有多少空值（`NULL`），最小和最大值，以及每个字段的最频繁值和其他统计信息取决于字段类型。使用剖析的优点在于它不需要设计任何质量规则，并且分析师可以使用结果来确定数据在他们正在处理的具体项目中的质量。例如，如果`Age`字段大部分为空，但对于特定项目不需要，数据集可能具有可接受的质量水平。尽管几乎所有数据质量工具都包括剖析，但剖析也被各种其他工具使用，从数据准备到数据发现等各个方面。
- en: Popular profiling and data quality tools include IBM Information Analyzer, Informatica
    DQ, SAS DataFlux, and many others.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的剖析和数据质量工具包括IBM信息分析器，Informatica DQ，SAS DataFlux等等。
- en: MDM systems
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MDM系统
- en: 'A special class of data quality tools called master data management systems
    are used to create *master lists* of various entities: primarily customers, but
    also products (these are called product information management, or PIM, systems),
    suppliers, and many others. These are very sophisticated systems that take data
    from one or more systems, harmonize the data to a common schema and representation
    (units of measure, codes, etc.), and perform what’s called *entity resolution*:
    finding multiple records that apply to the same entity. For example, some systems
    have multiple records for the same customer because of duplicate data entry, acquisitions
    (one customer firm bought another and they became a single customer), human error,
    or a variety of other reasons. Furthermore, different systems may use different
    identities for the customers—one may use tax IDs, while another relies on name
    and address and a third uses account numbers. Reconciling all of these is the
    job of an MDM system.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特殊类别的数据质量工具称为主数据管理系统，用于创建各种实体的*主列表*：主要是客户，但也包括产品（称为产品信息管理或PIM系统），供应商等等。这些是非常复杂的系统，它们从一个或多个系统获取数据，将数据协调到一个通用的架构和表示（计量单位，代码等），并执行所谓的*实体解析*：找到适用于同一实体的多个记录。例如，一些系统可能有同一客户的多条记录，因为重复数据输入，收购（一个客户公司收购另一个公司，它们成为一个单一客户），人为错误或其他多种原因。此外，不同的系统可能使用不同的标识客户的方法——一个可能使用税号，另一个依赖于姓名和地址，第三个使用帐号号码。调和所有这些是MDM系统的工作。
- en: 'Once the records for the same entity have been identified, often it’s found
    that they contain conflicting information—addresses are different, names are spelled
    a little differently, and so forth. So, another task of the MDM system is to fix
    these conflicts, either automatically or by triggering a manual intervention,
    to create the *golden record*: the one correct record for an entity that everyone
    should use.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦识别出同一实体的记录，通常会发现它们包含冲突信息——地址不同，姓名拼写略有不同等等。因此，MDM系统的另一个任务是修复这些冲突，可以自动进行或触发手动干预，以创建*黄金记录*：每个实体应使用的正确记录。
- en: MDM suppliers include traditional vendors such as IBM, Oracle, and Informatica,
    and some next-generation vendors such as Tamr that provide machine learning capabilities
    to automate the process.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: MDM供应商包括传统供应商如IBM，Oracle和Informatica，以及一些提供机器学习能力来自动化过程的下一代供应商，如Tamr。
- en: Data modeling tools
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据建模工具
- en: Data modeling tools are used to create a relational schema. While theoretically
    data modelers can use tools such as Erwin and IBM InfoSphere Data Architect to
    create physical, logical, and semantic models, in practice, most of the time these
    tools are used to create an entity relationship model of the data with primary
    and foreign keys (also called referential integrity constraints).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模工具用于创建关系模式。理论上，数据建模师可以使用诸如Erwin和IBM InfoSphere Data Architect之类的工具来创建物理、逻辑和语义模型，但在实践中，大多数时候这些工具用于创建数据的实体关系模型，包含主键和外键（也称为参照完整性约束）。
- en: 'Schema design is a very important activity for operational databases. Well-designed
    schemas improve database performance, while poorly designed ones slow it down—sometimes
    quite dramatically. A schema has to be designed with usage in mind: if it’s operational,
    it must be well normalized and optimized for many small transactions; if it’s
    for data warehousing, dimensional design should be used to optimize for analytical
    queries. The schema designer must also consider understandability and extensibility.
    Schemas change, and well-designed schemas are usually easy to change by adding
    new columns, while poorly designed ones frequently require expensive rearchitecting.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 模式设计是运营数据库中非常重要的活动。设计良好的模式可以提高数据库性能，而设计不良的模式则会减慢速度——有时会显著减慢。模式必须根据使用情况设计：如果是运营模式，它必须良好规范化，并优化以处理许多小事务；如果是数据仓库模式，则应使用维度设计来优化分析查询。模式设计师还必须考虑可理解性和可扩展性。模式会改变，设计良好的模式通常很容易通过添加新列来改变，而设计不良的模式则经常需要昂贵的重构。
- en: We discussed referential integrity and normalization earlier in this chapter.
    Because it is such a core concept, all relational databases provide facilities
    to enforce referential integrity. Unfortunately, to do that, every time a new
    order is added to the `Orders` table in our previous example, the database has
    to check the `Customers` table to make sure the `Customer_ID` in the `Orders`
    table exists in the `Customers` table and, if the value is not there, abort or
    reject the transaction. This adds significant performance overhead to `Orders`
    table updates. It also complicates all the applications that process orders, because
    they need a way to deal with such rejected transactions. In practice, I haven’t
    seen any production databases that enforce referential integrity. Instead, the
    information about primary and foreign keys is kept in data modeling tools and
    data quality tools are used to check referential integrity.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早些时候，我们讨论了参照完整性和规范化。由于它是一个核心概念，所有关系数据库都提供了执行参照完整性的设施。不幸的是，为了做到这一点，每当在我们之前的示例中的`Orders`表中添加新订单时，数据库必须检查`Customers`表，以确保`Orders`表中的`Customer_ID`存在于`Customers`表中，如果值不存在，就中止或拒绝事务。这给`Orders`表的更新增加了显著的性能开销。这也使得处理订单的所有应用程序变得复杂，因为它们需要一种处理此类拒绝事务的方法。在实践中，我没有见过任何执行参照完整性的生产数据库。相反，关于主键和外键的信息保存在数据建模工具中，数据质量工具用于检查参照完整性。
- en: Metadata repositories
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元数据存储库
- en: 'Metadata repositories contain technical metadata (data about data) across the
    data assets. The metadata is collected manually or by integrating with various
    other tools, such as ETL tools, BI tools, and so forth. There are three main use
    cases for metadata repositories:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储库包含跨数据资产的技术元数据（关于数据的数据）。元数据是手动收集的或通过与各种其他工具（如ETL工具、BI工具等）集成来收集的。元数据存储库有三种主要用例：
- en: Finding data assets
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找数据资产
- en: For example, a data architect may want to know which tables in which databases
    contain a `Customer_ID`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数据架构师可能想知道哪些数据库中的哪些表包含`Customer_ID`。
- en: Tracking lineage (provenance)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪血统   跟踪血统（来源）
- en: Many regulations require enterprises to document the lineage of data assets—in
    other words, where the data for those assets came from and how it was generated
    or transformed.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 许多法规要求企业记录数据资产的血统——换句话说，这些资产的数据来源以及它是如何生成或转换的。
- en: Impact analysis
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 影响分析
- en: If developers are making changes in a complex ecosystem, there is always a danger
    of breaking something. Impact analysis allows developers to see all the data assets
    that rely on a particular field or integration job before making a change.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果开发人员在一个复杂的生态系统中进行更改，总是有破坏某些东西的危险。影响分析允许开发人员在进行更改之前，看到所有依赖于特定字段或集成作业的数据资产。
- en: Metadata repository vendors include IBM, Informatica, ASG Rochade, and many
    others. However, these are quickly being supplanted by a new class of products
    called *data catalogs*, covered in [Chapter 8](ch08.xhtml#cataloging_the_data_lake).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储库供应商包括IBM、Informatica、ASG Rochade等许多其他公司。然而，这些供应商正在迅速被称为*数据目录*的新产品类别所取代，该类产品在第8章中有详细介绍
    [Chapter 8](ch08.xhtml#cataloging_the_data_lake)。
- en: Data governance tools
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据治理工具
- en: Data governance tools record, document, and sometimes manage governance policies.
    The tools usually define who the *data steward* is for each data asset. Data stewards
    are responsible for making sure the data assets are correct, documenting their
    purpose and lineage, and defining access and lifecycle management policies for
    them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理工具记录、记录，有时管理治理政策。这些工具通常定义每个数据资产的*数据监护人*是谁。数据监护人负责确保数据资产正确，记录其目的和血统，并为它们定义访问和生命周期管理策略。
- en: 'In some companies, data steward can be a full-time, dedicated role. In other
    companies, the role may be assigned to someone with direct business responsibilities
    related to the data. The organizational structure also varies: some data stewards
    belong to a formal data governance organization, often managed by a chief data
    officer (CDO), whereas others belong to functional teams or business units or,
    more rarely, to the IT department. For example, a data steward for sales data
    may be a member of the sales ops team.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些公司中，数据监护人可以是一个全职、专职的角色。在其他公司中，这一角色可能分配给直接与数据相关的业务责任人员。组织结构也各不相同：一些数据监护人属于正式的数据治理组织，通常由首席数据官（CDO）管理，而其他人则属于功能团队、业务单位或者更少见的IT部门。例如，销售数据的数据监护人可能是销售运营团队的成员。
- en: Data stewardship is often complex and cross-functional. For example, each sales
    group may have its own customer relationship management (CRM) system and its own
    data steward, while a data warehouse that combines all the sales and customer
    data from all systems may have its own data steward too. The most important function
    of the data governance tool is to identify who is responsible for what, so they
    can be consulted and can authorize access and other data policies.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据监护常常是复杂且跨职能的。例如，每个销售组可能有自己的客户关系管理（CRM）系统和自己的数据监护人，而将所有系统的销售和客户数据汇总到一个数据仓库中可能也会有自己的数据监护人。数据治理工具最重要的功能是识别谁对什么负责，以便可以咨询他们并授权访问和其他数据政策。
- en: 'Once ownership has been documented, the next step in rolling out a data governance
    program is to document data governance policies. Broadly speaking, these usually
    include the following aspects:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有权已记录，推出数据治理程序的下一步是记录数据治理政策。广义上讲，这些政策通常包括以下几个方面：
- en: Access control and sensitive data regulatory compliance
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制和敏感数据法规合规性
- en: Who can see what. This is particularly important for sensitive data and for
    complying with regulations that address sensitive data. For example, the credit
    card industry has Payment Card Industry (PCI) regulations that define how sensitive
    credit card data should be handled, the medical industry in the US has government
    regulations called the Health Insurance Portability and Accountability Act (HIPAA),
    and any company that has any European customers must comply with a new regulation
    called the General Data Protection Regulation (GDPR).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 谁可以看到什么。对于敏感数据和符合相关法规的情况尤为重要。例如，信用卡行业有定义如何处理敏感信用卡数据的支付卡行业（PCI）法规，美国医疗行业有称为《健康保险便携性和责任法案》（HIPAA）的政府法规，以及任何拥有欧洲客户的公司必须遵守的新法规《通用数据保护条例》（GDPR）。
- en: Documentation or metadata management
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 文档管理或元数据管理
- en: What has to be documented about each data set, usually including lineage and,
    again, regulatory compliance. For the financial industry, the Basel III compliance
    requirement is documented in rule BCBS 239, requiring detailed lineage to be maintained
    for all financial results reported by a company.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集需要记录的内容，通常包括数据血统和再次强调的法规合规性。对于金融行业，Basel III合规要求在规则BCBS 239中有详细记录，要求公司报告的所有财务结果都要维护详细的血统。
- en: Data lifecycle management
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生命周期管理
- en: Retention policies, backup policies, and so on.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 保留政策、备份政策等。
- en: Data quality management
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量管理
- en: The acceptable levels of quality and what data quality rules to use.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 可接受的质量水平及应使用的数据质量规则。
- en: The business glossary
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 业务词汇表
- en: 'The various terms represented by the data. The glossary organizes and documents
    these terms: it usually contains the official names and descriptions for each
    term and their data representations (for example, the “profit” term may describe
    how profit is calculated, while the “customer status” term may describe a list
    of legal statuses and how these statuses are assigned).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数据代表的各种术语。术语表组织和记录这些术语：通常包含每个术语的官方名称和描述以及它们的数据表示（例如，“利润”术语可能描述利润如何计算，而“客户状态”术语可能描述法律状态列表及其如何分配）。
- en: Consuming the Data
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据的消费
- en: Once data is loaded and made available, the analysts can use it to produce reports,
    run ad hoc analytics, and create dashboards. There are a plethora of such tools
    available, including many open source and free products.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据加载并可用，分析师可以用它来生成报告，运行特别分析，并创建仪表板。有大量此类工具可用，包括许多开源和免费产品。
- en: Historically, these tools used to be separated into reporting tools such as
    Crystal Reports and Jasper Reports, which produced print-ready reports; BI tools
    such as Business Objects and Cognos, which created ad hoc reports and charts;
    and OLAP tools such as those that created in-memory cubes and allowed the users
    to “slice and dice” or analyze data along various dimensions. These cubes were
    built either in memory (e.g., ArborSoft/Hyperion) or on demand from a relational
    database (also called ROLAP; e.g., MicroStrategy). Eventually, most of these capabilities
    became consolidated into one tool, so these days a tool like MicroStrategy provides
    all of them.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，这些工具曾分为报告工具（如Crystal Reports和Jasper Reports，生成可打印报告）、BI工具（如Business Objects和Cognos，创建特别报告和图表）以及OLAP工具（如创建内存立方体并允许用户“切片和切块”或分析各种维度的工具）。这些立方体可以在内存中构建（例如ArborSoft/Hyperion）或按需从关系数据库构建（也称为ROLAP；例如MicroStrategy）。最终，大多数这些功能都整合到一个工具中，因此像MicroStrategy这样的工具现在提供所有这些功能。
- en: The first generation of these tools were designed for developers to create reports,
    dashboards, or OLAP cubes and let analysts work with these artifacts. In the 2000s,
    a new generation of products such as Tableau and Qlik came to prominence by providing
    analysts with simple tools that allowed them to work directly with data tables
    and files without having to wait for IT to code up a report. This ushered in the
    era of self-service analytics that we will cover extensively in [Chapter 6](ch06.xhtml#optimizing_for_self-service).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具的第一代设计供开发人员创建报告、仪表板或OLAP立方体，并让分析师使用这些工件。在2000年代，新一代产品如Tableau和Qlik凭借提供分析师简单工具的方式，让他们直接处理数据表和文件，而无需等待IT编写报告，开始占据主导地位。这引领了我们将在[第6章](ch06.xhtml#optimizing_for_self-service)中广泛探讨的自助式分析时代。
- en: Advanced analytics
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级分析
- en: Advanced analytics have been around for years in many industries. From engineering
    to insurance to finance, statistical models and predictive models have been used
    to measure risk and simulate real-life scenarios. Many of the natural sciences,
    from anthropology to physics, employ statistics to measure, extrapolate, and predict.
    Wall Street quants have been building automated trading models for decades. Insurance
    actuaries have been modeling risk and probabilities for over a century. An entire
    segment of computer science called data mining has been around for over 20 years.
    A multi-billion-dollar industry has grown up around providing specialized tools
    for statistics and advanced analytics, including such vendors as SAS, MATLAB,
    SPSS (now part of IBM), and many others.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，高级分析已在许多行业中存在。从工程到保险再到金融，统计模型和预测模型被用来衡量风险和模拟现实场景。许多自然科学领域，从人类学到物理学，都使用统计学来衡量、推断和预测。华尔街的量化分析师几十年来一直在构建自动化交易模型。保险精算师已经模拟风险和概率超过一个世纪。一个名为数据挖掘的计算机科学分支已经存在超过20年。围绕提供统计和高级分析专业工具的数十亿美元产业已经蓬勃发展，包括诸如SAS、MATLAB、SPSS（现在是IBM的一部分）等供应商。
- en: Traditionally the realm of statisticians and scientists, advanced analytics
    have slowly been making their way into the mainstream. Popular consumer packages
    such as Excel now include basic statistical functions like regular regression.
    More importantly, many consumer-facing applications showcase predictive or statistical
    models. From real estate websites like [Zillow.com](http://zillow.com) that use
    statistical models to calculate the value of a home to credit scoring applications
    and money management packages that model savings and retirement incomes, people
    are increasingly encountering the results of predictive analytics in their daily
    lives and are beginning to wonder how they can incorporate them into their business
    lives.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，高级分析主要是统计学家和科学家的领域，但现在正在逐渐走向主流。像Excel这样的流行消费者软件现在已经包括了基本的统计功能，比如常规回归。更重要的是，许多面向消费者的应用展示了预测或统计模型。从像[Zillow.com](http://zillow.com)这样使用统计模型计算房屋价值的房地产网站，到信用评分应用和金融管理软件模拟储蓄和退休收入，人们在日常生活中越来越多地遇到预测分析的结果，并开始思考如何将其应用于他们的商业生活中。
- en: There is increasing talk of “citizen data scientists”—basically business analysts
    who apply advanced analytics to address their problems without hiring statisticians
    and professional data scientists. Just as programming is now taught in many high
    schools and most analysts are comfortable using Excel macros, SQL queries, and
    even simple scripting languages, advanced analytics are slowly but surely making
    their way from “advanced” to commonplace.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多关于“公民数据科学家”的讨论——基本上是指商业分析师在解决问题时应用高级分析，而不需要聘请统计学家和专业数据科学家。就像现在很多高中都教授编程一样，大多数分析师已经习惯使用Excel宏、SQL查询，甚至简单的脚本语言，高级分析正在逐步从“高级”走向普及。
- en: Conclusion
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This brief history of data and data management techniques has brought us to
    the early 2010s. In the next chapter, we will cover the big data phenomenon and
    the disruption that it caused in data management practices.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这段关于数据和数据管理技术的简要历史将我们带到了2010年代初期。在下一章中，我们将讨论大数据现象及其对数据管理实践所带来的颠覆。
