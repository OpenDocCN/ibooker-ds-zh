- en: Part 1\. Introduction to GANs and generative modeling
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分\. GANs和生成建模简介
- en: '[Part 1](#part01) introduces the world of Generative Adversarial Networks (GANs)
    and walks through implementations of the most canonical GAN variants:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一部分](#part01)介绍了生成对抗网络（GANs）的世界，并介绍了最经典的GAN变体的实现：'
- en: In [chapter 1](../Text/kindle_split_010.xhtml#ch01), you will learn the basics
    of GANs and develop an intuitive understanding of how they work.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第1章](../Text/kindle_split_010.xhtml#ch01)中，您将学习GAN的基础知识，并发展对它们如何工作的直观理解。
- en: In [chapter 2](../Text/kindle_split_011.xhtml#ch02), we will switch gears a
    little and look at autoencoders, so you can get a more holistic understanding
    of generative modeling. Autoencoders are some of the most important theoretical
    and practical precursors to GANs and continue to be widely used to this day.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第2章](../Text/kindle_split_011.xhtml#ch02)中，我们将稍微转换一下方向，研究自动编码器，以便您能够更全面地理解生成建模。自动编码器是GANs最重要的理论和实践先驱之一，并且至今仍被广泛使用。
- en: '[Chapter 3](../Text/kindle_split_012.xhtml#ch03) starts where [chapter 1](../Text/kindle_split_010.xhtml#ch01)
    left off and dives deeper into the theory underlying GANs and adversarial learning.
    In this chapter, you will also implement and train your first, fully functional
    GAN.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3章](../Text/kindle_split_012.xhtml#ch03) 从[第1章](../Text/kindle_split_010.xhtml#ch01)结束的地方开始，深入探讨了GAN和对抗学习背后的理论。在这一章中，您还将实现并训练您的第一个、完全功能的GAN。'
- en: '[Chapter 4](../Text/kindle_split_013.xhtml#ch04) continues your learning journey
    by exploring the Deep Convolutional GAN (DCGAN). This innovation on top of the
    original GAN uses convolutional neural networks to improve the quality of the
    generated images.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4章](../Text/kindle_split_013.xhtml#ch04) 通过探索深度卷积生成对抗网络（DCGAN）继续您的学习之旅。这一在原始GAN之上的创新使用了卷积神经网络来提高生成图像的质量。'
- en: Chapter 1\. Introduction to GANs
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章\. GANs简介
- en: '*This chapter covers*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: An overview of Generative Adversarial Networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络概述
- en: What makes this class of machine learning algorithms special
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使这一类机器学习算法特殊的是
- en: Some of the exciting GAN applications that this book covers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书涵盖的一些令人兴奋的GAN应用
- en: The notion of whether machines can think is older than the computer itself.
    In 1950, the famed mathematician, logician, and computer scientist Alan Turing—perhaps
    best known for his role in decoding the Nazi wartime enciphering machine, Enigma—penned
    a paper that would immortalize his name for generations to come, “Computing Machinery
    and Intelligence.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器能否思考的观念比计算机本身还要古老。在1950年，著名的数学家、逻辑学家和计算机科学家艾伦·图灵——他因在解码纳粹战争加密机恩尼格玛中的角色而闻名——撰写了一篇将使他的名字永垂不朽的论文，“计算机与智能。”
- en: 'In the paper, Turing proposed a test he called the *imitation game*, better
    known today as the *Turing test*. In this hypothetical scenario, an unknowing
    observer talks with two counterparts behind a closed door: one, a fellow human;
    the other, a computer. Turing reasons that if the observer is unable to tell which
    is the person and which is the machine, the computer passed the test and must
    be deemed intelligent.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，图灵提出了一种他称之为“模仿游戏”的测试，今天更广为人知的是“图灵测试”。在这个假设的场景中，一个不知情观察者与两个在封闭门后的人交谈：一个，是人类；另一个，是计算机。图灵认为，如果观察者无法分辨出哪个人和哪台机器，那么计算机通过了测试，必须被认为是有智能的。
- en: Anyone who has attempted to engage in a dialogue with an automated chatbot or
    a voice-powered intelligent assistant knows that computers have a long way to
    go to pass this deceptively simple test. However, in other tasks, computers have
    not only matched human performance but also surpassed it—even in areas that were
    until recently considered out of reach for even the smartest algorithms, such
    as superhumanly accurate face recognition or mastering the game of Go.^([[1](#ch01fn01)])
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何尝试与自动化聊天机器人或语音智能助手进行对话的人都知道，计算机还有很长的路要走才能通过这个看似简单的测试。然而，在其他任务中，计算机不仅与人类的表现持平，甚至超越了人类——即使在最近还被认为连最聪明的算法都无法触及的领域，如超人类准确的面部识别或掌握围棋游戏。^([[1](#ch01fn01)])
- en: ¹
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Surpassing Human-Level Face Verification Performance on LFW with GaussianFace,”
    by Chaochao Lu and Xiaoou Tang, 2014, [https://arXiv.org/abs/1404.3840](https://arXiv.org/abs/1404.3840).
    See also the *New York Times* article “Google’s AlphaGo Defeats Chinese Go Master
    in Win for A.I.,” by Paul Mozur, 2017, [http://mng.bz/07WJ](http://mng.bz/07WJ).
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见“在LFW上超越人类水平的面部验证性能：基于GaussianFace”，作者：Chaochao Lu 和 Xiaoou Tang，2014年，[https://arXiv.org/abs/1404.3840](https://arXiv.org/abs/1404.3840)。另见《纽约时报》文章“谷歌的AlphaGo战胜中国围棋大师，人工智能取得胜利”，作者：Paul
    Mozur，2017年，[http://mng.bz/07WJ](http://mng.bz/07WJ)。
- en: Machine learning algorithms are great at recognizing patterns in existing data
    and using that insight for tasks such as *classification* (assigning the correct
    category to an example) and *regression* (estimating a numerical value based on
    a variety of inputs). When asked to generate new data, however, computers have
    struggled. An algorithm can defeat a chess grandmaster, estimate stock price movements,
    and classify whether a credit card transaction is likely to be fraudulent. In
    contrast, any attempt at making small talk with Amazon’s Alexa or Apple’s Siri
    is doomed. Indeed, humanity’s most basic and essential capacities—including a
    convivial conversation or the crafting of an original creation—can leave even
    the most sophisticated supercomputers in digital spasms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法擅长在现有数据中识别模式，并利用这些洞察力来完成诸如 *分类*（将正确的类别分配给示例）和 *回归*（根据多种输入估计数值）等任务。然而，当要求生成新数据时，计算机却遇到了困难。一个算法可以击败国际象棋大师，预测股价走势，或判断信用卡交易是否可能欺诈。相比之下，与亚马逊的Alexa或苹果的Siri进行闲聊的任何尝试都是注定要失败的。的确，人类最基本和最本质的能力——包括愉快的交谈或创作原创作品——甚至可以让最复杂的超级计算机陷入数字痉挛。
- en: This all changed in 2014 when Ian Goodfellow, then a PhD student at the University
    of Montreal, invented Generative Adversarial Networks (GANs). This technique has
    enabled computers to generate realistic data by using not one, but two, separate
    neural networks. GANs were not the first computer program used to generate data,
    but their results and versatility set them apart from all the rest. GANs have
    achieved remarkable results that had long been considered virtually impossible
    for artificial systems, such as the ability to generate fake images with real-world-like
    quality, turn a scribble into a photograph-like image, or turn video footage of
    a horse into a running zebra—all without the need for vast troves of painstakingly
    labeled training data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切都在2014年发生了变化，当时蒙特利尔大学的博士生Ian Goodfellow发明了生成对抗网络（GANs）。这项技术通过使用不是单个而是两个独立的神经网络，使计算机能够生成逼真的数据。GANs并不是第一个用于生成数据的计算机程序，但它们的结果和多功能性使它们与其他所有程序都不同。GANs取得了长期被认为对人工系统几乎不可能的显著成果，例如生成具有真实世界质量的假图像，将涂鸦变成类似照片的图像，或将马的视频变成奔跑的斑马——所有这些都不需要大量精心标注的训练数据。
- en: A telling example of how far machine data generation has been able to advance
    thanks to GANs is the synthesis of human faces, illustrated in [figure 1.1](#ch01fig01).
    As recently as 2014, when GANs were invented, the best that machines could produce
    was a blurred countenance—and even that was celebrated as a groundbreaking success.
    By 2017, just three years later, advances in GANs enabled computers to synthesize
    fake faces whose quality rivals high-resolution portrait photographs. In this
    book, we look under the hood of the algorithm that made all this possible.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）如何使机器数据生成能力得到如此大的提升的一个令人信服的例子是合成人类面部，如图1.1所示。就在GANs被发明的那年，即2014年，机器能产生的最好的结果是一个模糊的面孔——甚至那也被庆祝为划时代的成功。到了2017年，仅仅三年后，GANs的进步使计算机能够合成与高分辨率肖像照片相媲美的假面孔。在这本书中，我们将揭开使这一切成为可能的算法的面纱。
- en: Figure 1.1\. Progress in human face generation
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1. 人类面部生成进展
- en: '![](../Images/01fig01_alt.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01fig01_alt.jpg)'
- en: '(Source: “The Malicious Use of Artificial Intelligence: Forecasting, Prevention,
    and Mitigation,” by Miles Brundage et al., 2018, [https://arxiv.org/abs/1802.07228](https://arxiv.org/abs/1802.07228).)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：“人工智能的恶意使用：预测、预防和缓解”，作者：Miles Brundage 等，2018年，[https://arxiv.org/abs/1802.07228](https://arxiv.org/abs/1802.07228).)
- en: 1.1\. What are Generative Adversarial Networks?
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 什么是生成对抗网络？
- en: '*Generative Adversarial Networks (GANs)* are a class of machine learning techniques
    that consist of two simultaneously trained models: one (the *Generator*) trained
    to generate fake data, and the other (the *Discriminator*) trained to discern
    the fake data from real examples.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成对抗网络（GANs）*是一类机器学习技术，它由两个同时训练的模型组成：一个（*生成器*）被训练生成假数据，另一个（*判别器*）被训练从真实示例中辨别假数据。'
- en: 'The word *generative* indicates the overall purpose of the model: creating
    new data. The data that a GAN will learn to generate depends on the choice of
    the training set. For example, if we want a GAN to synthesize images that look
    like Leonardo da Vinci’s, we would use a training dataset of da Vinci’s artwork.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 词语*生成*表明了该模型的整体目的：创建新的数据。GAN将学习生成的数据取决于训练集的选择。例如，如果我们想让GAN合成看起来像达芬奇的作品的图像，我们会使用达芬奇艺术作品的训练数据集。
- en: 'The term *adversarial* points to the game-like, competitive dynamic between
    the two models that constitute the GAN framework: the Generator and the Discriminator.
    The Generator’s goal is to create examples that are indistinguishable from the
    real data in the training set. In our example, this means producing paintings
    that look just like da Vinci’s. The Discriminator’s objective is to distinguish
    the fake examples produced by the Generator from the real examples coming from
    the training dataset. In our example, the Discriminator plays the role of an art
    expert assessing the authenticity of paintings believed to be da Vinci’s. The
    two networks are continually trying to outwit each other: the better the Generator
    gets at creating convincing data, the better the Discriminator needs to be at
    distinguishing real examples from the fake ones.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*对抗*指向构成GAN框架的两个模型之间的游戏般、竞争性的动态：生成器和判别器。生成器的目标是创建与训练集中真实数据不可区分的示例。在我们的例子中，这意味着制作看起来就像达芬奇的作品的画作。判别器的目标是区分生成器产生的假示例和来自训练数据集的真实示例。在我们的例子中，判别器扮演了一个评估被认为是达芬奇作品的画作真实性的艺术专家的角色。这两个网络不断地试图欺骗对方：生成器在创建令人信服的数据方面越擅长，判别器在区分真实示例和假示例方面就需要越出色。
- en: 'Finally, the word *networks* indicates the class of machine learning models
    most commonly used to represent the Generator and the Discriminator: neural networks.
    Depending on the complexity of the GAN implementation, these can range from simple
    feed-forward neural networks (as you’ll see in [chapter 3](../Text/kindle_split_012.xhtml#ch03))
    to convolutional neural networks (as you’ll see in [chapter 4](../Text/kindle_split_013.xhtml#ch04))
    or even more complex variants, such as the U-Net (as you’ll see in [chapter 9](../Text/kindle_split_019.xhtml#ch09)).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，词语*网络*表明了最常用来表示生成器和判别器的机器学习模型类别：神经网络。根据GAN实现的复杂性，这些网络可以从简单的前馈神经网络（你将在第3章[chapter
    3](../Text/kindle_split_012.xhtml#ch03)中看到）到卷积神经网络（你将在第4章[chapter 4](../Text/kindle_split_013.xhtml#ch04)中看到）或甚至更复杂的变体，例如U-Net（你将在第9章[chapter
    9](../Text/kindle_split_019.xhtml#ch09)中看到）。
- en: 1.2\. How do GANs work?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. GAN是如何工作的？
- en: 'The mathematics underpinning GANs are complex (as you’ll explore in later chapters,
    especially [chapters 3](../Text/kindle_split_012.xhtml#ch03) and [5](../Text/kindle_split_015.xhtml#ch05));
    fortunately, many real-world analogies can make GANs easier to understand. Previously,
    we discussed the example of an art forger (the Generator) trying to fool an art
    expert (the Discriminator). The more convincing the fake paintings the forger
    makes, the better the art expert must be at determining their authenticity. This
    is true in the reverse situation as well: the better the art expert is at telling
    whether a particular painting is genuine, the more the forger must improve to
    avoid being caught red-handed.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 支撑GAN的数学原理相当复杂（你将在后面的章节中了解到，特别是第3章[chapters 3](../Text/kindle_split_012.xhtml#ch03)和第5章[5](../Text/kindle_split_015.xhtml#ch05)）；幸运的是，许多现实世界的类比可以使GAN更容易理解。之前，我们讨论了艺术伪造者（生成器）试图欺骗艺术专家（判别器）的例子。伪造者制作的假画越令人信服，艺术专家在确定其真实性方面的能力就必须越强。在相反的情况下也是如此：艺术专家在判断某幅画是否为真时的能力越强，伪造者就必须越改进以避免被当场抓获。
- en: Another metaphor often used to describe GANs—one that Ian Goodfellow himself
    likes to use—is that of a criminal (the Generator) who forges money, and a detective
    (the Discriminator) who tries to catch him. The more authentic-looking the counterfeit
    bills become, the better the detective must be at detecting them, and vice versa.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用来描述生成对抗网络（GANs）的隐喻——伊恩·古德费勒本人也喜欢使用的隐喻——是一个罪犯（生成器）伪造货币，和一个侦探（判别器）试图抓住他。伪造的钞票看起来越逼真，侦探在检测它们时就必须越好，反之亦然。
- en: In more technical terms, the Generator’s goal is to produce examples that capture
    the characteristics of the training dataset, so much so that the samples it generates
    look indistinguishable from the training data. The Generator can be thought of
    as an object recognition model in reverse. *Object recognition algorithms* learn
    the patterns in images to discern an image’s content. Instead of recognizing the
    patterns, the Generator learns to create them essentially from scratch; indeed,
    the input into the Generator is often no more than a vector of random numbers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在更技术性的术语中，生成器的目标是生成能够捕捉训练数据集特征的示例，以至于它生成的样本看起来与训练数据无法区分。生成器可以被视为一个反向的对象识别模型。*对象识别算法*通过学习图像中的模式来识别图像的内容。生成器不是识别模式，而是从零开始学习创建它们；实际上，生成器的输入通常只是一个随机数字的向量。
- en: The Generator learns through the feedback it receives from the Discriminator’s
    classifications. The Discriminator’s goal is to determine whether a particular
    example is real (coming from the training dataset) or fake (created by the Generator).
    Accordingly, each time the Discriminator is fooled into classifying a fake image
    as real, the Generator knows it did something well. Conversely, each time the
    Discriminator correctly rejects a Generator-produced image as fake, the Generator
    receives the feedback that it needs to improve.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器通过从判别器的分类中获得的反馈来学习。判别器的目标是确定一个特定示例是真实的（来自训练数据集）还是伪造的（由生成器创建）。相应地，每次判别器被欺骗将伪造图像分类为真实时，生成器就知道它做得很好。相反，每次判别器正确地拒绝生成器生成的图像为伪造时，生成器就会收到需要改进的反馈。
- en: The Discriminator continues to improve as well. Like any classifier, it learns
    from how far its predictions are from the true labels (real or fake). So, as the
    Generator gets better at producing realistic-looking data, the Discriminator gets
    better at telling fake data from the real, and both networks continue to improve
    simultaneously.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器也在不断改进。像任何分类器一样，它通过学习其预测与真实标签（真实或伪造）之间的距离来学习。因此，随着生成器在生成看起来逼真的数据方面变得更好，判别器在区分伪造数据和真实数据方面也变得更好，并且两个网络继续同时改进。
- en: '[Table 1.1](#ch01table01) summarizes the key takeaways about the two GAN subnetworks.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1.1](#ch01table01)总结了关于两个GAN子网络的关键要点。'
- en: Table 1.1\. Generator and Discriminator subnetworks
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.1. 生成器和判别器子网络
- en: '|   | Generator | Discriminator |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|   | 生成器 | 判别器 |'
- en: '| --- | --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Input | A vector of random numbers | The Discriminator receives input from
    two sources:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '| 输入 | 一个随机数字的向量 | 判别器从两个来源接收输入：'
- en: Real examples coming from the training dataset
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自训练数据集的真实示例
- en: Fake examples coming from the Generator
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自生成器的伪造示例
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Output | Fake examples that strive to be as convincing as possible | Predicted
    probability that the input example is real |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 努力尽可能令人信服的伪造示例 | 预测输入示例是真实的概率 |'
- en: '| Goal | Generate fake data that is indistinguishable from members of the training
    dataset | Distinguish between the fake examples coming from the Generator and
    the real examples coming from the training dataset |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 生成与训练数据集成员无法区分的伪造数据 | 区分来自生成器的伪造示例和来自训练数据集的真实示例 |'
- en: 1.3\. GANs in action
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3. GANs在行动中
- en: Now that you have a high-level understanding of GANs and their constituent networks,
    let’s take a closer look at the system in action. Imagine that our goal is to
    teach a GAN to produce realistic-looking handwritten digits. (You’ll learn to
    implement such a model in [chapter 3](../Text/kindle_split_012.xhtml#ch03) and
    expand on it in [chapter 4](../Text/kindle_split_013.xhtml#ch04).) [Figure 1.2](#ch01fig02)
    illustrates the core GAN architecture.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对GAN及其组成网络有了高层次的理解，让我们更仔细地看看系统在实际中的应用。想象一下，我们的目标是教会一个GAN生成看起来逼真的手写数字。（你将在第3章（../Text/kindle_split_012.xhtml#ch03）中学习实现这样的模型，并在第4章（../Text/kindle_split_013.xhtml#ch04）中对其进行扩展。）[图1.2](#ch01fig02)说明了核心GAN架构。
- en: Figure 1.2\. The two GAN subnetworks, their inputs and outputs, and their interactions
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2。两个GAN子网络、它们的输入和输出以及它们的交互
- en: '![](../Images/01fig02_alt.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01fig02_alt.jpg)'
- en: 'Let’s walk through the details of the diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下图中的内容：
- en: '***Training dataset—*** The dataset of real examples that we want the Generator
    to learn to emulate with near-perfect quality. In this case, the dataset consists
    of images of handwritten digits. This dataset serves as input (*x*) to the Discriminator
    network.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***训练数据集—*** 我们希望生成器能够以近乎完美的质量学习的真实例数据集。在这种情况下，数据集由手写数字的图像组成。这个数据集作为输入 (*x*)
    传递给判别器网络。'
- en: '***Random noise vector—*** The raw input (*z*) to the Generator network. This
    input is a vector of random numbers that the Generator uses as a starting point
    for synthesizing fake examples.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***随机噪声向量—*** 生成器网络的原始输入 (*z*)。这个输入是一个随机数字向量，生成器使用它作为合成假例的起点。'
- en: '***Generator network—*** The Generator takes in a vector of random numbers
    (*z*) as input and outputs fake examples (*x**). Its goal is to make the fake
    examples it produces indistinguishable from the real examples in the training
    dataset.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***生成器网络—*** 生成器以随机数字向量 (*z*) 作为输入，并输出假例 (*x*)。其目标是使其产生的假例与训练数据集中的真实例子的区别不可辨。'
- en: '***Discriminator network—*** The Discriminator takes as input either a real
    example (*x*) coming from the training set or a fake example (*x**) produced by
    the Generator. For each example, the Discriminator determines and outputs the
    probability of whether the example is real.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***判别器网络—*** 判别器以来自训练集的真实例 (*x*) 或生成器产生的假例 (*x*) 作为输入。对于每个例子，判别器确定并输出该例子为真实的概率。'
- en: '***Iterative training/tuning—*** For each of the Discriminator’s predictions,
    we determine how good it is—much as we would for a regular classifier—and use
    the results to iteratively tune the Discriminator and the Generator networks through
    backpropagation:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***迭代训练/调整—*** 对于判别器的每个预测，我们确定其好坏——就像我们会对一个常规分类器做的那样——并使用结果通过反向传播迭代调整判别器和生成器网络：'
- en: 'The Discriminator’s weights and biases are updated to maximize its classification
    accuracy (maximizing the probability of correct prediction: *x* as real and *x**
    as fake).'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新判别器的权重和偏差，以最大化其分类准确率（最大化正确预测的概率：*x* 为真实，*x* 为假）。
- en: The Generator’s weights and biases are updated to maximize the probability that
    the Discriminator misclassifies *x** as real.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新生成器的权重和偏差，以最大化判别器将 *x* 错误分类为真实例的概率。
- en: 1.3.1\. GAN training
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1. GAN训练
- en: 'Learning about the purpose of the various GAN components may feel like looking
    at a snapshot of an engine: it cannot be understood fully until we see it in motion.
    That’s what this section is all about. First, we present the GAN training algorithm;
    then, we illustrate the training process so you can see the architecture diagram
    in action.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 了解各种GAN组件的目的是像看发动机的快照一样：除非我们看到它在运动中，否则无法完全理解。这正是本节的内容。首先，我们介绍GAN训练算法；然后，我们展示训练过程，以便您可以看到架构图的实际应用。
- en: '|  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**GAN training algorithm**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**GAN训练算法**'
- en: '*For* each training iteration *do*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于*每个训练迭代*执行*'
- en: 'Train the Discriminator:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器：
- en: Take a random real example *x* from the training dataset.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中随机取一个真实例 *x*。
- en: Get a new random noise vector *z* and, using the Generator network, synthesize
    a fake example *x**.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个新的随机噪声向量 *z*，并使用生成器网络合成一个假例 *x*。
- en: Use the Discriminator network to classify *x* and *x**.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用判别器网络对 *x* 和 *x* 进行分类。
- en: Compute the classification errors and backpropagate the total error to update
    the Discriminator’s trainable parameters, seeking to *minimize* the classification
    errors.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分类错误，并将总错误反向传播以更新判别器的可训练参数，寻求*最小化*分类错误。
- en: 'Train the Generator:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器：
- en: Get a new random noise vector *z* and, using the Generator network, synthesize
    a fake example *x**.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个新的随机噪声向量 *z*，并使用生成器网络合成一个假例 *x*。
- en: Use the Discriminator network to classify *x**.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用判别器网络对 *x* 进行分类。
- en: Compute the classification error and backpropagate the error to update the Generator’s
    trainable parameters, seeking to *maximize* the Discriminator’s error.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分类错误，并将错误反向传播以更新生成器的可训练参数，寻求*最大化*判别器的错误。
- en: '*End for*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*结束for*'
- en: '|  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: GAN training visualized
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GAN训练可视化
- en: '[Figure 1.3](#ch01fig03) illustrates the GAN training algorithm. The letters
    in the diagram refer to the list of steps in the GAN training algorithm.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1.3](#ch01fig03) 展示了 GAN 训练算法。图中的字母代表 GAN 训练算法步骤列表。'
- en: Figure 1.3\. The GAN training algorithm has two main parts. These two parts,
    Discriminator training and Generator training, depict the same GAN network at
    different time snapshots in the corresponding stages of the training process.
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3。GAN 训练算法有两个主要部分。这两个部分，判别器训练和生成器训练，描绘了训练过程中相应阶段的同一 GAN 网络在不同时间快照下的情况。
- en: '![](../Images/01fig03_alt.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01fig03_alt.jpg)'
- en: '|  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Subdiagram legend**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**子图图例**'
- en: 'Train the Discriminator:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器：
- en: '![](../Images/f0010-01_alt.jpg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/f0010-01_alt.jpg)'
- en: Take a random real example *x* from the training dataset.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中随机取一个真实例子 *x*。
- en: Get a new random noise vector *z* and, using the Generator network, synthesize
    a fake example *x**.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个新的随机噪声向量 *z*，并使用生成器网络合成一个假例 *x**。
- en: Use the Discriminator network to classify *x* and *x**.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用判别器网络对 *x* 和 *x** 进行分类。
- en: Compute the classification errors and backpropagate the total error to update
    the Discriminator weights and biases, seeking to *minimize* the classification
    errors.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分类误差并将总误差反向传播以更新判别器的权重和偏差，寻求*最小化*分类误差。
- en: 'Train the Generator:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器：
- en: '![](../Images/f0010-02_alt.jpg)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/f0010-02_alt.jpg)'
- en: Get a new random noise vector *z* and, using the Generator network, synthesize
    a fake example *x**.
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个新的随机噪声向量 *z*，并使用生成器网络合成一个假例 *x**。
- en: Use the Discriminator network to classify *x**.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用判别器网络对 *x** 进行分类。
- en: Compute the classification error and backpropagate the error to update the Generator
    weights and biases, seeking to *maximize* the Discriminator’s error.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分类误差并将误差反向传播以更新生成器的权重和偏差，寻求*最大化*判别器的误差。
- en: '|  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 1.3.2\. Reaching equilibrium
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.2. 达到均衡
- en: 'You may wonder when the GAN training loop is meant to stop. More precisely,
    how do we know when a GAN is fully trained so that we can determine the appropriate
    number of training iterations? With a regular neural network, we usually have
    a clear objective to achieve and measure. For example, when training a classifier,
    we measure the classification error on the training and validation sets, and we
    stop the process when the validation error starts getting worse (to avoid overfitting).
    In a GAN, the two networks have competing objectives: when one network gets better,
    the other gets worse. How do we determine when to stop?'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道 GAN 训练循环何时应该停止。更确切地说，我们如何知道 GAN 已经完全训练好，以便我们可以确定适当的训练迭代次数？对于常规神经网络，我们通常有一个明确的目标要实现和衡量。例如，当训练一个分类器时，我们衡量训练集和验证集上的分类误差，并在验证误差开始变差时停止过程（以避免过拟合）。在
    GAN 中，两个网络有竞争目标：当一个网络变得更好时，另一个网络会变得更差。我们如何确定何时停止？
- en: Those familiar with game theory may recognize this setup as a *zero-sum game*—a
    situation in which one player’s gains equal the other player’s losses. When one
    player improves by a certain amount, the other player worsens by the same amount.
    All zero-sum games have a *Nash equilibrium*, a point at which neither player
    can improve their situation or payoff by changing their actions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉博弈论的人可能会认出这种设置是一个*零和博弈*——一种一个玩家的收益等于另一个玩家损失的情况。当一个玩家提高一定量时，另一个玩家会以相同的量变差。所有零和博弈都有一个*纳什均衡*，即一个点，在这个点上，任何玩家都不能通过改变自己的行动来改善自己的情况或收益。
- en: 'GAN reaches Nash equilibrium when the following conditions are met:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下条件时，GAN 达到纳什均衡：
- en: The Generator produces fake examples that are indistinguishable from the real
    data in the training dataset.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器产生的假例在训练数据集中与真实数据无法区分。
- en: The Discriminator can at best randomly guess whether a particular example is
    real or fake (that is, make a 50/50 guess whether an example is real).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器最多只能随机猜测一个特定例子是真实还是假（即，以50/50的概率猜测一个例子是真实的）。
- en: '|  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Nash equilibrium is named after the American economist and mathematician John
    Forbes Nash Jr., whose life story and career were captured in the biography titled
    *A Beautiful Mind* and inspired the eponymous film.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 纳什均衡是以美国经济学家和数学家约翰·福布斯·纳什（John Forbes Nash Jr.）的名字命名的，他的生平和事业在传记《美丽心灵》（A Beautiful
    Mind）中被捕捉，并启发了同名的电影。
- en: '|  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let us convince you of why this is the case. When each of the fake examples
    (*x**) is truly indistinguishable from the real examples (*x*) coming from the
    training dataset, there is nothing the Discriminator can use to tell them apart
    from one another. Because half of the examples it receives are real and half are
    fake, the best the Discriminator can do is to flip a coin and classify each example
    as real or fake with 50% probability.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来说服你为什么这是真的。当每个假例子（*x*）与训练数据集中来的真实例子（*x*）真正无法区分时，判别器无法利用任何东西来区分它们。因为它收到的例子中有一半是真实的，一半是假的，判别器能做的最好的事情就是掷硬币，以50%的概率将每个例子分类为真实或假。
- en: The Generator is likewise at a point where it has nothing to gain from further
    tuning. Because the examples it produces are already indistinguishable from the
    real ones, even a tiny change to the process it uses to turn the random noise
    vector (*z*) into a fake example (*x**) may give the Discriminator a cue for how
    to discern the fake example from the real data, making the Generator worse off.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器也处于一个点，它从进一步的调整中无法获得任何收益。因为它产生的例子已经与真实例子无法区分，即使是对它将随机噪声向量（*z*）转换为假例子（*x*）的过程进行微小的改变，也可能给判别器提供如何从真实数据中辨别假例子的线索，从而使生成器变得更糟。
- en: With equilibrium achieved, GAN is said to have *converged*. Here is when it
    gets tricky. In practice, it is nearly impossible to find the Nash equilibrium
    for GANs because of the immense complexities involved in reaching convergence
    in nonconvex games (more on convergence in later chapters, particularly [chapter
    5](../Text/kindle_split_015.xhtml#ch05)). Indeed, GAN convergence remains one
    of the most important open questions in GAN research.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当达到平衡时，人们说GAN已经*收敛*。这时事情变得复杂。在实践中，由于在非凸游戏中达到收敛的复杂性巨大，因此几乎不可能找到GAN的纳什均衡（关于收敛的更多内容将在后续章节中介绍，尤其是[第5章](../Text/kindle_split_015.xhtml#ch05)）。确实，GAN的收敛仍然是GAN研究中最重要的未解问题之一。
- en: Fortunately, this has not impeded GAN research or the many innovative applications
    of generative adversarial learning. Even in the absence of rigorous mathematical
    guarantees, GANs have achieved remarkable empirical results. This book covers
    a selection of the most impactful ones, and the following section previews some
    of them.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这并没有阻碍GAN研究或生成对抗学习的许多创新应用。即使在没有严格的数学保证的情况下，GAN也取得了显著的经验性成果。本书涵盖了其中最具影响力的选择，下一节将预览其中的一些。
- en: 1.4\. Why study GANs?
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4. 为什么研究GAN？
- en: Since their invention, GANs have been hailed by academics and industry experts
    as one of the most consequential innovations in deep learning. Yann LeCun, the
    director of AI research at Facebook, went so far as to say that GANs and their
    variations are “the coolest idea in deep learning in the last 20 years.”^([[2](#ch01fn02)])
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 自从它们的发明以来，GAN们被学术界和行业专家誉为深度学习中最有影响力的创新之一。Facebook人工智能研究总监Yann LeCun甚至说，GAN及其变体是“过去20年来深度学习中最酷的想法”。^([[2](#ch01fn02)])
- en: ²
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Google’s Dueling Neural Networks Spar to Get Smarter,” by Cade Metz, *Wired*,
    2017, [http://mng.bz/KE1X](http://mng.bz/KE1X).
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Cade Metz在*Wired*杂志2017年发表的“Google的对抗神经网络对决以变得更聪明”，[http://mng.bz/KE1X](http://mng.bz/KE1X)。
- en: The excitement is well justified. Unlike other advancements in machine learning
    that may be household names among researchers but would elicit no more than a
    quizzical look from anyone else, GANs have captured the imagination of researchers
    and the wider public alike. They have been covered by the *New York Times*, the
    BBC, *Scientific American*, and many other prominent media outlets. Indeed, it
    was one of those exciting GAN results that probably drove you to buy this book
    in the first place. (Right?)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种兴奋是有充分理由的。与其他在机器学习领域可能家喻户晓的进步不同，GAN们既吸引了研究人员的想象力，也吸引了更广泛的公众。它们被《纽约时报》、《BBC》、《科学美国人》和其他许多知名媒体所报道。确实，正是那些令人兴奋的GAN结果可能促使你最初购买这本书。（对吗？）
- en: Perhaps most notable is the capacity of GANs to create hyperrealistic imagery.
    None of the faces in [figure 1.4](#ch01fig04) belongs to a real human; they are
    all fake, showcasing GANs’ ability to synthesize images with photorealistic quality.
    The faces were produced using Progressive GANs, a technique covered in [chapter
    6](../Text/kindle_split_016.xhtml#ch06).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最值得注意的是，GAN能够创建超逼真的图像。[图1.4](#ch01fig04)中的所有面孔都不属于真实人类；它们都是假的，展示了GAN合成具有照片级质量图像的能力。这些面孔是使用渐进式GAN生成的，这一技术在[第6章](../Text/kindle_split_016.xhtml#ch06)中有介绍。
- en: Figure 1.4\. These photorealistic but fake human faces were synthesized by a
    Progressive GAN trained on high-resolution portrait photos of celebrities.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4。这些逼真但虚假的人类面孔是由训练在名人高清肖像照片上的渐进式GAN合成的。
- en: '![](../Images/01fig04_alt.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01fig04_alt.jpg)'
- en: '(Source: “Progressive Growing of GANs for Improved Quality, Stability, and
    Variation,” by Tero Karras et al., 2017, [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196).)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“通过渐进式增长GAN以改善质量、稳定性和多样性”，由Tero Karras等人撰写的，发表于2017年，[https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)。）
- en: Another remarkable GAN achievement is *image-to-image translation*. Similarly
    to the way a sentence can be translated from, say, Chinese to Spanish, GANs can
    translate an image from one domain to another. As shown in [figure 1.5](#ch01fig05),
    GANs can turn an image of a horse into an image of zebra (and back!), and a photo
    into a Monet-like painting—all with virtually no supervision and no labels whatsoever.
    The GAN variant that made this possible is called *CycleGAN*; you’ll learn all
    about it in [chapter 9](../Text/kindle_split_019.xhtml#ch09).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项引人注目的GAN成就是*图像到图像的翻译*。类似于一个句子可以从中文翻译成西班牙语，GANs可以将一个图像从一种领域转换到另一种领域。如图1.5所示，GANs可以将马的图像转换成斑马的图像（反之亦然），以及将照片转换成类似莫奈的画作——这一切几乎无需监督和任何标签。使这一切成为可能的GAN变体被称为*CycleGAN*；你将在[第9章](../Text/kindle_split_019.xhtml#ch09)中详细了解它。
- en: Figure 1.5\. By using a GAN variant called CycleGAN, we can turn a Monet painting
    into a photograph or turn an image of a zebra into a depiction of a horse, and
    vice versa.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5。通过使用名为CycleGAN的GAN变体，我们可以将莫奈的画作变成照片，或将斑马的图像变成马的描绘，反之亦然。
- en: '![](../Images/01fig05_alt.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01fig05_alt.jpg)'
- en: '(Source: See “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial
    Networks,” by Jun-Yan Zhu et al., 2017, [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593).)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：参见Jun-Yan Zhu等人撰写的《使用循环一致对抗网络进行无配对图像到图像翻译》，发表于2017年，[https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)。）
- en: 'The more practically minded GAN use cases are just as fascinating. The online
    giant Amazon is experimenting with harnessing GANs for fashion recommendations:
    by analyzing countless outfits, the system learns to produce new items matching
    any given style.^([[3](#ch01fn03)]) In medical research, GANs are used to augment
    datasets with synthetic examples to improve diagnostic accuracy.^([[4](#ch01fn04)])
    In [chapter 11](../Text/kindle_split_022.xhtml#ch11)—after you’ve mastered the
    ins and outs of training GANs and their variants—you’ll explore both of these
    applications in detail.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 更注重实际应用的GAN用例同样引人入胜。在线巨头亚马逊正在尝试利用GANs进行时尚推荐：通过分析无数套装，系统学会生产符合任何给定风格的全新商品.^([[3](#ch01fn03)])在医学研究中，GANs被用于通过合成示例增强数据集，以提高诊断准确性.^([[4](#ch01fn04)])在[第11章](../Text/kindle_split_022.xhtml#ch11)—在你掌握了训练GANs及其变体的所有细节之后—你将详细探讨这两个应用。
- en: ³
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Amazon Has Developed an AI Fashion Designer,” by Will Knight, *MIT Technology
    Review*, 2017, [http://mng.bz/9wOj](http://mng.bz/9wOj).
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见威尔·奈特撰写的《亚马逊已开发人工智能时尚设计师》，发表于2017年《麻省理工学院技术评论》，[http://mng.bz/9wOj](http://mng.bz/9wOj)。
- en: ⁴
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Synthetic Data Augmentation Using GAN for Improved Liver Lesion Classification,”
    by Maayan Frid-Adar et al., 2018, [https://arxiv.org/abs/1801.02385](https://arxiv.org/abs/1801.02385).
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Maayan Frid-Adar等人撰写的《使用GAN进行合成数据增强以改进肝脏病变分类》，发表于2018年，[https://arxiv.org/abs/1801.02385](https://arxiv.org/abs/1801.02385)。
- en: GANs are also seen as an important stepping stone toward achieving *artificial
    general intelligence*,^([[5](#ch01fn05)]) an artificial system capable of matching
    human cognitive capacity to acquire expertise in virtually any domain—from motor
    skills involved in walking, to language, to creative skills needed to compose
    sonnets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GANs也被视为实现*通用人工智能*的重要基石，^([[5](#ch01fn05)])这是一种能够匹配人类认知能力，在几乎任何领域获得专业知识的人工系统——从行走中涉及的肌肉技能，到语言，再到创作十四行诗所需的创造性技能。
- en: ⁵
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “OpenAI Founder: Short-Term AGI Is a Serious Possibility,” by Tony Peng,
    Synced, 2018, [http://mng.bz/j5Oa](http://mng.bz/j5Oa). See also “A Path to Unsupervised
    Learning Through Adversarial Networks,” by Soumith Chintala, f Code, 2016, [http://mng.bz/WOag](http://mng.bz/WOag).'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见托尼·彭撰写的《OpenAI创始人：短期AGI是一个严肃的可能性》，发表于2018年Synced，[http://mng.bz/j5Oa](http://mng.bz/j5Oa)。另见Soumith
    Chintala撰写的《通过对抗网络实现无监督学习之路》，发表于2016年f Code，[http://mng.bz/WOag](http://mng.bz/WOag)。
- en: 'But with the ability to generate new data and imagery, GANs also have the capacity
    to be dangerous. Much has been discussed about the spread and dangers of fake
    news, but the potential of GANs to create credible fake footage is disturbing.
    At the end of an aptly titled 2018 piece about GANs—“How an A.I. ‘Cat-and-Mouse
    Game’ Generates Believable Fake Photos”—the *New York Times* journalists Cade
    Metz and Keith Collins discuss the worrying prospect of GANs being exploited to
    create and spread convincing misinformation, including fake video footage of statements
    by world leaders. Martin Giles, the San Francisco bureau chief of *MIT Technology
    Review*, echoes their concern and mentions another potential risk in his 2018
    article “The GANfather: The Man Who’s Given Machines the Gift of Imagination”:
    in the hands of skilled hackers, GANs can be used to intuit and exploit system
    vulnerabilities at an unprecedented scale. These concerns are what motivated us
    to discuss the ethical considerations of GANs in [chapter 12](../Text/kindle_split_023.xhtml#ch12).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于能够生成新的数据和图像，GANs也具有潜在的危险性。关于虚假新闻的传播和危害已经讨论了很多，但GANs创建可信虚假视频片段的潜力令人不安。在2018年一篇关于GANs的恰当标题的文章《如何通过人工智能的“猫鼠游戏”生成可信的虚假照片》的结尾，*《纽约时报》*的记者凯德·梅茨和基思·柯林斯讨论了GANs被利用来创建和传播令人信服的错误信息的令人担忧的前景，包括世界领导人的虚假视频片段。*《麻省理工学院技术评论》*旧金山分社社长马丁·吉尔斯在2018年的文章《GAN之父：赋予机器想象力的男人》中重申了他们的担忧，并提到了另一个潜在风险：在熟练黑客的手中，GANs可以以前所未有的规模直观地利用系统漏洞。正是这些担忧促使我们在第12章中讨论GANs的伦理考量。
- en: 'GANs can do much good for the world, but all technological innovations have
    misuses. Here the philosophy has to be one of awareness: because it is impossible
    to “uninvent” a technique, it is crucial to make sure people like you are aware
    of this technique’s rapid emergence and its substantial potential.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: GANs（生成对抗网络）可以为世界带来许多好处，但所有技术创新都存在滥用风险。这里必须有一种意识哲学：因为无法“取消发明”一项技术，所以确保像你这样的人意识到这项技术的快速出现及其巨大潜力至关重要。
- en: In this book, we are only able to scratch the surface of what is possible with
    GANs. However, we hope that this book will provide you with the necessary theoretical
    knowledge and practical skills to continue exploring any facet of this field that
    you find most interesting.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们只能触及GANs可能性的表面。然而，我们希望这本书能为你提供必要的理论知识与实践技能，以便继续探索这个领域中最吸引你的任何方面。
- en: So, without further ado, let’s dive in!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无需多言，让我们深入探讨吧！
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'GANs are a deep learning technique that uses a competitive dynamic between
    two neural networks to synthesize realistic data samples, such as fake photorealistic
    imagery. The two networks that constitute a GAN are as follows:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs是一种深度学习技术，它利用两个神经网络之间的竞争动态来合成真实的数据样本，例如虚假的逼真图像。构成GAN的两个网络如下：
- en: The Generator, whose goal is to fool the Discriminator by producing data indistinguishable
    from the training dataset
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器，其目标是通过产生与训练数据集不可区分的数据来欺骗判别器
- en: The Discriminator, whose goal is to correctly distinguish between real data
    coming from the training dataset and the fake data produced by the Generator
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器，其目标是正确区分来自训练数据集的真实数据和生成器产生的虚假数据
- en: GANs have extensive applications across many different sectors, such as fashion,
    medicine, and cybersecurity.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs在许多不同领域都有广泛的应用，例如时尚、医学和网络安全。
- en: Chapter 2\. Intro to generative modeling with autoencoders
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二章\. 使用自编码器进行生成建模简介
- en: '*This chapter covers*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Encoding data into a latent space (dimensionality reduction) and subsequent
    dimensionality expansion
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据编码到潜在空间（降维）以及随后的维度扩展
- en: Understanding the challenges of generative modeling in the context of a variational
    autoencoder
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在变分自编码器的背景下理解生成建模的挑战
- en: Generating handwritten digits by using Keras and autoencoders
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras和自编码器生成手写数字
- en: Understanding the limitations of autoencoders and motivations for GANs
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自编码器的局限性以及GANs的动机
- en: '*I dedicate this chapter to my grandmother, Aurelie Langrova, who passed away
    as we were finishing the work on it. She will be missed dearly.*'
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我将这一章献给我的祖母，奥雷利·朗罗瓦，她在我们完成这项工作的时候去世了。我们将非常怀念她。*'
- en: ''
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Jakub*'
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*雅库布*'
- en: 'You might be wondering why we chose to include this chapter in the book. There
    are three core reasons:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们选择将这一章包含在书中。有三个核心原因：
- en: '*Generative models are a new area for most.* Most people who come across machine
    learning typically become exposed to classification tasks in machine learning
    first and more extensively—perhaps because they tend to be more straightforward.
    Generative modeling, through which we are trying to produce a new example that
    looks realistic, is therefore less understood. So we decided to include a chapter
    that covers generative modeling in an easier setting before delving into GANs,
    especially given the wealth of resources and research on autoencoders—GANs’ closest
    precursor. But if you want to dive straight into the new and exciting bits, feel
    free to skip this chapter.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成模型对于大多数人来说是一个新的领域。* 大多数接触到机器学习的人通常首先接触到机器学习中的分类任务，并且接触得更为广泛——也许是因为它们通常更直接。因此，通过生成模型我们试图产生一个看起来逼真的新示例，这个领域理解得较少。因此，我们决定在深入探讨生成对抗网络（GANs）之前，先包括一个涵盖生成模型章节，特别是在考虑到自动编码器——GANs最接近的前驱——的资源和研究丰富的情况下。但如果你想要直接进入新而激动人心的部分，请随意跳过这一章。'
- en: '*Generative models are very challenging.* Because generative modeling has been
    underrepresented, most people are unaware of what a typical model looks like and
    its challenges. Although autoencoders are in many ways closer to the models that
    are most commonly taught (such as an explicit objective function, as we will discuss
    later), they still present many challenges that GANs face—such as how difficult
    it is to evaluate sample quality. [Chapter 5](../Text/kindle_split_015.xhtml#ch05)
    covers this in more depth.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成模型非常具有挑战性。* 由于生成模型没有得到充分的表现，大多数人不知道典型模型的样子及其挑战。尽管自动编码器在许多方面更接近于最常教授的模型（例如，我们将在后面讨论的显式目标函数），但它们仍然面临着GANs面临许多挑战——例如评估样本质量有多困难。[第5章](../Text/kindle_split_015.xhtml#ch05)更深入地讨论了这一点。'
- en: '*Generative models are an important part of the literature today.* Autoencoders
    themselves have their own uses, as we discuss in this chapter. They are also still
    an active area of research, even state of the art in some areas, and are used
    explicitly by many GAN architectures. Other GAN architectures use them as implicit
    inspiration or a mental model—such as CycleGAN, covered in [chapter 9](../Text/kindle_split_019.xhtml#ch09).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成模型是当今文献的重要组成部分。* 自动编码器本身也有其用途，正如我们在本章中讨论的那样。它们也是一个活跃的研究领域，甚至在某些领域处于最前沿，并被许多GAN架构明确使用。其他GAN架构将它们用作隐含的灵感或心理模型——例如在第9章中介绍的CycleGAN。'
- en: 2.1\. Introduction to generative modeling
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 生成模型简介
- en: You should be familiar with how deep learning takes raw pixels and turns them
    into, for example, class predictions. For example, we can take three matrixes
    that contain pixels of an image (one for each color channel) and pass them through
    a system of transformations to get a single number at the end. But what if we
    want to go in the opposite direction?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该熟悉深度学习如何将原始像素转换为，例如，类别预测。例如，我们可以取包含图像像素的三个矩阵（每个颜色通道一个）并通过一个变换系统来获得最后的单个数字。但如果我们想反方向进行呢？
- en: We start with a prescription of what we want to produce and get the image at
    the other end of the transformations. That is *generative modeling* in its simplest,
    most informal form; we add more depth throughout the book.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从确定我们想要生产的内容开始，并在变换的另一端获得图像。这就是最简单、最非正式形式的*生成模型*；我们在整本书中会添加更多深度。
- en: A bit more formally, we take a certain prescription (*z*)—for this simple case,
    let’s say it is a number between 0 and 9—and try to arrive at a generated sample
    (*x**). Ideally, this *x** would look as realistic as another real sample, *x*.
    The prescription, *z*, lives in a *latent space* and serves as an inspiration
    so that we do not always get the same output, *x**. This latent space is a learned
    representation—hopefully meaningful to people in ways we think of it (“disentangled”).
    Different models will learn a different latent representation of the same data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式一点，我们取一个特定的规定（*z*）——对于这个简单案例，让我们假设它是一个介于0和9之间的数字——并试图得到一个生成的样本（*x*）。理想情况下，这个*x*看起来应该和另一个真实样本*x*一样逼真。这个规定*z*存在于一个*潜在空间*中，并作为一个灵感，以确保我们不会总是得到相同的输出*x*。这个潜在空间是一个学习到的表示——希望以我们思考的方式对人们有意义（“解耦”）。不同的模型将学习到相同数据的不同的潜在表示。
- en: The random noise vector we saw in [chapter 1](../Text/kindle_split_010.xhtml#ch01)
    is often referred to as a *sample from the latent space*. Latent space is a simpler,
    hidden representation of a data point. In our context, it is denoted by *z*, and
    *simpler* just means lower-dimensional—for example, a vector or array of 100 numbers
    rather than the 768 that is the dimensionality of the samples we will use. In
    many ways, a good latent representation of a data point will allow you to group
    things that are similar in this space. We will get to what *latent* means in the
    context of an autoencoder in [figure 2.3](#ch02fig03) and show you how this affects
    our generated samples in [figures 2.6](#ch02fig06) and [2.7](#ch02fig07), but
    before we can do that, we’ll describe how autoencoders function.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](../Text/kindle_split_010.xhtml#ch01)中看到的随机噪声向量通常被称为*潜在空间的样本*。潜在空间是数据点的简单、隐藏表示。在我们的上下文中，它表示为
    *z*，而*简单*只是意味着低维的——例如，一个包含100个数字的向量或数组，而不是我们将使用的样本的768维。在许多方面，一个好的潜在表示将允许你在这个空间中将相似的事物分组。我们将在[图2.3](#ch02fig03)中解释自编码器上下文中的*潜在*意味着什么，并展示它如何影响我们在[图2.6](#ch02fig06)和[2.7](#ch02fig07)中生成的样本，但在我们能够做到这一点之前，我们将描述自编码器是如何工作的。
- en: 2.2\. How do autoencoders function on a high level?
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 自编码器在高级层面上是如何工作的？
- en: 'As their name suggests, *autoencoders* help us encode data, well, automatically.
    Autoencoders are composed of two parts: encoder and decoder. For the purposes
    of this explanation, let’s consider one use case: compression.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如他们的名字所暗示的，*自编码器*帮助我们自动地编码数据。自编码器由两部分组成：编码器和解码器。为了解释的目的，让我们考虑一个用例：压缩。
- en: Imagine that you are writing a letter to your grandparents about your career
    as a machine learning engineer. You have only one page to explain everything that
    you do so that they understand, given their knowledge and beliefs about the world.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在给你的祖父母写一封信，关于你作为机器学习工程师的职业。你只有一页纸来解释你所做的一切，以便他们能够理解，考虑到他们对世界的知识和信念。
- en: Now imagine that your grandparents suffer from acute amnesia and do not remember
    what you do at all. This already feels a lot harder, doesn’t it? This may be because
    now you have to explain *all the terminology*. For example, they can still read
    and understand basic things in your letter, such as your description of what your
    cat did, but the notion of a machine learning engineer might be alien to them.
    In other words, their learned transformations from latent space *z* into *x**
    has been (almost) randomly initialized. You have to first retrain these mental
    structures in their heads before you can explain. You have to train their autoencoder
    by passing in concepts *x* and seeing whether they manage to reproduce them (*x**)
    back to you in a meaningful way. That way, you can measure their error, called
    the *reconstruction loss* (|| *x* – *x** ||).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，如果你的祖父母患有急性健忘症，完全记不起你的一切。这感觉是不是更难了？这可能是因为现在你必须解释*所有术语*。例如，他们仍然可以阅读并理解你信中的基本内容，比如你描述你的猫做了什么，但机器学习工程师的概念可能对他们来说是陌生的。换句话说，他们从潜在空间
    *z* 到 *x* 的学习转换已经被（几乎）随机初始化。在你能够解释之前，你必须首先在他们的大脑中重新训练这些心理结构。你必须通过传递概念 *x* 并观察他们是否能够以有意义的方式将它们（*x*）重新呈现给你来训练他们的自编码器。这样，你可以测量他们的错误，称为*重建损失*（||
    *x* – *x** ||）。
- en: 'Implicitly, we compress data—or information—every day so we do not spend ages
    explaining known concepts. Human communication is full of autoencoders, but they
    are context-dependent: what we explain to our grandparents, we do not have to
    explain to our engineering colleagues, such as what a machine learning model is.
    So some human latent spaces are more appropriate than others, depending on the
    context. We can just jump to the succinct representation that their autoencoder
    will already understand.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 暗示地，我们每天都在压缩数据或信息，这样我们就不必花费大量时间解释已知的概念。人类的交流充满了自编码器，但它们是上下文相关的：我们对我们祖父母的解释，我们不必对我们工程同事解释，比如机器学习模型是什么。因此，某些人类潜在空间比其他潜在空间更适合，这取决于上下文。我们可以直接跳到他们自编码器已经理解的简洁表示。
- en: We can compress, because it is useful to simplify certain recurring concepts
    into abstractions that we have agreed on—for example, a job title. Autoencoders
    can systematically and automatically uncover these information-efficient patterns,
    define them, and use them as shortcuts to increase the information throughput.
    As a result, we need to transmit only the *z*, which is typically much lower-dimensional,
    thereby saving us bandwidth.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以压缩，因为将某些重复的概念简化为我们已经同意的抽象（例如，职位名称）是有用的。自动编码器可以系统地、自动地发现这些信息效率模式，定义它们，并将它们用作捷径来增加信息吞吐量。因此，我们只需要传输*z*，它通常是低维的，从而节省我们带宽。
- en: From an information theory point of view, you are trying to pass as much information
    through the “information bottleneck” (your letter or spoken communication) as
    possible without sacrificing too much of the understanding. You can almost imagine
    this as a secret shortcut that only you and your family understand but that has
    been optimized for the topics you frequently discussed.^([[1](#ch02fn01)]) For
    simplicity and to focus on compression, we chose to ignore the fact that words
    are an explicit model, although most words also have tremendous context-dependent
    complexity behind them.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从信息论的角度来看，你试图尽可能多地通过“信息瓶颈”（你的信件或口头交流）传递信息，而不牺牲太多的理解。你几乎可以想象这是一个只有你和你的家人理解的秘密捷径，但它已经针对你经常讨论的主题进行了优化。[^([1](#ch02fn01))]
    为了简单起见，并专注于压缩，我们选择忽略这样一个事实，即单词是一个明确的模型，尽管大多数单词背后也有巨大的上下文相关复杂性。
- en: ¹
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, the Rothschilds, a famous European financier family, did this in their
    letters, which is why they were so successful in finance.
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实际上，著名的欧洲金融家家族罗思柴尔德家族在其信件中就是这样做的，这也是他们在金融上如此成功的原因。
- en: '|  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: The *latent space* is the hidden representation of the data. Rather than expressing
    words or images (for example, *machine learning engineer* in our example, or JPEG
    codec for images) in their uncompressed versions, an autoencoder compresses and
    clusters them based on its understanding of the data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*潜在空间*是数据的隐藏表示。与表达单词或图像（例如，我们例子中的“机器学习工程师”，或图像的JPEG编解码器）的未压缩版本不同，自动编码器根据其对数据的理解压缩和聚类它们。'
- en: '|  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 2.3\. What are autoencoders to GANs?
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 自动编码器对GANs是什么？
- en: One of the key distinctions with autoencoders is that we end-to-end train the
    whole network with one loss function, whereas GANs have distinct loss functions
    for the Generator and the Discriminator. Let’s now look at the context in which
    autoencoders sit compared to GANs. As you can see in [figure 2.1](#ch02fig01),
    both are generative models that are subsets of artificial intelligence (AI) and
    machine learning (ML). In the case of autoencoders (or their variational alternative,
    VAEs), we have an explicitly written function that we are trying to optimize (a
    cost function); but in the case of GANs (as you will learn), we do not have an
    explicit metric as simple as mean squared error, accuracy, or area under the ROC
    curve to optimize.^([[2](#ch02fn02)]) GANs instead have two competing objectives
    that cannot be written in one function.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与自动编码器相比，一个关键的区别是我们使用一个损失函数从头到尾训练整个网络，而生成对抗网络（GANs）对生成器和判别器有各自不同的损失函数。现在让我们看看自动编码器相对于生成对抗网络所处的上下文。如图2.1所示，两者都是人工智能（AI）和机器学习（ML）的生成模型的一部分。在自动编码器（或它们的变分替代品，VAEs）的情况下，我们有一个明确写出的函数，我们正在尝试优化（一个成本函数）；但在GANs的情况下（你将了解到），我们没有像均方误差、准确率或ROC曲线下的面积那样简单的显式指标来优化。[^([2](#ch02fn02))]
    GANs相反，有两个相互竞争的目标，这些目标不能写在一个函数中。
- en: ²
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A *cost function* (also known as a *loss function* or *objective function*)
    is what we are trying to optimize/minimize for. In statistics, for example, this
    would be the root mean squared error (RMSE). The *root mean squared error (RMSE)*
    is a mathematical function that gives an error by taking the root of the square
    of the difference between the true value of an example and our prediction.In statistics,
    we typically want to evaluate a classifier across several combinations of false
    positives and negatives. The *area under the curve (AUC)* helps us do that. For
    more details, Wikipedia has an excellent explanation, as this concept is beyond
    the scope of this book.
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*代价函数*（也称为*损失函数*或*目标函数*）是我们试图优化/最小化的。例如，在统计学中，这将是均方根误差（RMSE）。*均方根误差（RMSE）*是一个数学函数，通过取真实值与我们的预测之间的差异的平方根来给出误差。在统计学中，我们通常希望评估分类器在多个假阳性和假阴性组合上的表现。*曲线下面积（AUC）*帮助我们做到这一点。有关更多详细信息，维基百科有出色的解释，因为这一概念超出了本书的范围。'
- en: Figure 2.1\. Placing GANs and autoencoders in the AI landscape. Different researchers
    might draw this differently, but we will leave this argument to academics.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1\. 将 GAN 和自动编码器置于 AI 生态图中。不同的研究人员可能会有不同的绘制方式，但我们将把这个争论留给学者们。
- en: '![](../Images/02fig01_alt.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02fig01_alt.jpg)'
- en: 2.4\. What is an autoencoder made of?
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 自动编码器由什么组成？
- en: 'As we look at the structure of an autoencoder, we’ll use images as an example,
    but this structure also applies in other cases (for instance, language, as in
    our example about the letter to your grandparents). Like many advancements in
    machine learning, the high-level idea of autoencoders is intuitive and follows
    these simple steps, illustrated in [figure 2.2](#ch02fig02):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察自动编码器的结构时，我们将使用图像作为例子，但这种结构也适用于其他情况（例如，语言，如我们关于给祖父母写信的例子）。像机器学习中的许多进步一样，自动编码器的高级思想是直观的，并遵循以下简单步骤，如图2.2所示：
- en: 'Encoder network: We take a representation *x* (for example, an image) and then
    reduce the dimension from *y* to *z* by using a learned encoder (typically, a
    one- or many-layer neural network).'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码网络：我们取一个表示 *x*（例如，一个图像），然后使用学习到的编码器（通常是一个单层或多层神经网络）将维度从 *y* 减少到 *z*。
- en: 'Figure 2.2\. Using an autoencoder in our letter example follows these steps:
    (1) You compress all the things you know about a machine learning engineer, and
    then (2) compose that to the latent space (letter to your grandmother). When she,
    using her understanding of words as a decoder (3), reconstructs a (lossy) version
    of what that means, you get out a representation of an idea in the same space
    (in your grandmother’s head) as the original input, which was your thoughts.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2\. 在我们的信件例子中使用自动编码器遵循以下步骤：（1）压缩你关于机器学习工程师的所有知识，然后（2）将其组合到潜在空间（给祖母的信）。当她，使用她对单词的理解作为解码器（3），重建一个（有损）版本的意义时，你得到一个在相同空间（在你祖母的头脑中）的原输入表示，即你的思想。
- en: '![](../Images/02fig02_alt.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/02fig02_alt.jpg)'
- en: 'Latent space (*z*): As we train, here we try to establish the latent space
    to have some meaning. Latent space is typically a representation of a smaller
    dimension and acts as an intermediate step. In this representation of our data,
    the autoencoder is trying to “organize its thoughts.”'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '潜在空间 (*z*): 随着我们的训练，我们试图建立具有某些意义的潜在空间。潜在空间通常是较小维度的表示，并作为中间步骤。在我们的数据表示中，自动编码器试图“组织其思想”。'
- en: 'Decoder network: We reconstruct the original object into the original dimension
    by using the decoder. This is typically done by a neural network that is a mirror
    image of the encoder. This is the step from *z* to *x**. We apply the reverse
    process of the encoding to get back, for example, a 784 pixel-values long reconstructed
    vector (of a 28 × 28 image) from the 256 pixel-values long vector of the latent
    space.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码网络：我们使用解码器将原始对象重建到原始维度。这通常是通过一个与编码器镜像的神经网络来完成的。这是从 *z* 到 *x** 的步骤。我们应用编码过程的逆过程，例如，从潜在空间的256像素值长向量中获取一个784像素值长的重建向量（一个28
    × 28图像）。
- en: 'Here’s an example of autoencoder training:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个自动编码器训练的例子：
- en: We take images *x* and feed them through the autoencoder.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图像 *x* 输入到自动编码器中。
- en: We get out *x**, reconstruction of the images.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们得到 *x**，图像的重建。
- en: We measure the reconstruction loss—the difference between *x* and *x**.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们测量重建损失——即 *x* 和 *x** 之间的差异。
- en: This is done using a distance (for example, mean average error) between the
    pixels of *x* and *x**.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是通过计算 *x* 和 *x** 像素之间的距离（例如，平均绝对误差）来完成的。
- en: This gives us an explicit objective function (|| *x* –*x** ||) to optimize via
    a version of gradient descent.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这为我们提供了一个显式的目标函数（|| *x* –*x** ||），我们可以通过梯度下降的一种版本来优化它。
- en: So we are trying to find the parameters of the encoder and the decoder that
    would minimize the reconstruction loss that we update by using gradient descent.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们正在尝试找到编码器和解码器的参数，这些参数将最小化我们通过使用梯度下降更新的重建损失。
- en: And that’s it! We’re done. Now you may be wondering why this is useful or important.
    You’d be surprised!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们完成了。现在你可能想知道这有什么用或为什么重要。你会感到惊讶！
- en: 2.5\. Usage of autoencoders
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 自动编码器的使用
- en: 'Despite their simplicity, there are many reasons to care about autoencoders:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们很简单，但有许多原因需要关注自动编码器：
- en: First of all, we get compression for free! This is because the intermediate
    step (2) from [figure 2.2](#ch02fig02) becomes an intelligently reduced image
    or object at the dimensionality of the latent space. Note that in theory, this
    can be orders of magnitude less than the original input. It obviously is not lossless,
    but we are free to use this side effect, if we wish.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们免费获得压缩！这是因为 [图 2.2](#ch02fig02) 中的中间步骤（2）变成了一个在潜在空间维度上智能减少的图像或对象。请注意，在理论上，这可以比原始输入小几个数量级。这显然不是无损的，但我们有权使用这个副作用，如果我们愿意的话。
- en: Still using the latent space, we can think of many practical applications, such
    as a *one-class classifier* (an anomaly-detection algorithm), where we can see
    the items in a reduced, more quickly searchable latent space to check for similarity
    with the target class. This can work in search (information retrieval) or anomaly-detection
    settings (comparing closeness in the latent space).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仍然使用潜在空间，我们可以想到许多实际应用，例如一个 *单类分类器*（一个异常检测算法），其中我们可以看到在减少的、更易于搜索的潜在空间中的项目，以检查与目标类的相似性。这可以在搜索（信息检索）或异常检测设置（在潜在空间中比较接近度）中工作。
- en: Another use case is data denoising or colorization of black-and-white images.^([[3](#ch02fn03)])
    For example, if we have an old photo or video or a very noisy one—say, World War
    II images—we can make them less noisy and add color back in. Hence the similarity
    to GANs, which also tend to excel at these types of applications.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个用例是数据去噪或黑白图像的着色.^([[3](#ch02fn03)]) 例如，如果我们有一张旧照片或视频，或者非常嘈杂的——比如说，二战图像——我们可以使它们变得更少噪音，并重新添加颜色。因此，这与
    GANs 的相似性，GANs 也擅长这些类型的应用。
- en: ³
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: For more information on coloring black-and-white images, see Emil Wallner’s
    “Coloring Greyscale Images,” on GitHub ([http://mng.bz/6jWy](http://mng.bz/6jWy)).
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于着色黑白图像的更多信息，请参阅 Emil Wallner 的“着色灰度图像”，GitHub 上有 ([http://mng.bz/6jWy](http://mng.bz/6jWy))。
- en: Some GANs architectures—such as BEGAN^([[4](#ch02fn04)])—use autoencoders as
    part of their architecture to help them stabilize their training, which is critically
    important, as you will discover later.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些 GAN 架构——例如 BEGAN^([[4](#ch02fn04)])——使用自动编码器作为其架构的一部分，以帮助它们稳定训练，这在后面你会发现是至关重要的。
- en: ⁴
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: BEGAN is an acronym for Boundary Equilibrium Generative Adversarial Networks.
    This interesting GAN architecture was one of the first to use an autoencoder as
    part of the setup.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BEGAN 是边界均衡生成对抗网络（Boundary Equilibrium Generative Adversarial Networks）的缩写。这个有趣的
    GAN 架构是第一个将自动编码器作为设置一部分使用的。
- en: Training of these autoencoders does not require labeled data. We will get to
    this and why unsupervised learning is so important in the next section. This makes
    our lives a lot easier, because it is only self-training and does not require
    us to look for labels.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些自动编码器的训练不需要标记数据。我们将在下一节中了解到这一点以及为什么无监督学习如此重要。这使得我们的生活变得更加容易，因为这只是自我训练，不需要我们寻找标签。
- en: Last, but definitely not least, we can use autoencoders to generate new images.
    Autoencoders have been applied to anything from digits to faces to bedrooms, but
    usually the higher the resolution of the image, the worse the performance, as
    the output tends to look blurry. But for the MNIST dataset—as you will discover
    later—and other low-resolution images, autoencoders work great; you’ll see what
    the code looks like in just a moment!
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，但绝对不是最不重要的，我们可以使用自动编码器来生成新的图像。自动编码器已经应用于从数字到面孔到卧室的任何东西，但通常图像的分辨率越高，性能越差，因为输出往往看起来模糊。但对于
    MNIST 数据集——正如你稍后会发现的那样——和其他低分辨率图像，自动编码器工作得很好；你很快就会看到代码的样子！
- en: '|  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Definition
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: The *Modified National Institute of Standards and Technology (MNIST)* database
    is a dataset of handwritten digits. Wikipedia has a great overview of this extremely
    popular dataset used in computer vision literature.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*修改后的国家标准与技术研究院（MNIST）*数据库是一组手写数字数据集。维基百科对这个在计算机视觉文献中极其流行的数据集有很好的概述。'
- en: '|  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: So all of these things can be done just because we found a new representation
    of the data we already had. This representation is useful because it brings out
    the core information, which is natively compressed, but it’s also easier to manipulate
    or generate new data based on the latent representation!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 所以所有这些事情都可以做到，仅仅因为我们找到了我们已有数据的新表示形式。这种表示形式很有用，因为它揭示了核心信息，这些信息是原生压缩的，但基于潜在表示来操作或生成新数据也更加容易！
- en: 2.6\. Unsupervised learning
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6\. 无监督学习
- en: In the previous chapter, we already talked about unsupervised learning without
    using the term. In this section, we’ll take a closer look.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经讨论了无监督学习，但没有使用这个术语。在本节中，我们将更深入地探讨。
- en: '|  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Definition
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: '*Unsupervised learning* is a type of machine learning in which we learn from
    the data itself without additional labels as to what this data means. Clustering,
    for example, is unsupervised—because we are just trying to discover the underlying
    structure of the data; but anomaly detection is usually supervised, as we need
    human-labeled anomalies.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习*是一种机器学习方法，我们从中学习数据本身，而不需要额外的标签来解释这些数据的意义。例如，聚类是无监督的——因为我们只是试图发现数据的潜在结构；但异常检测通常是监督的，因为我们需要人类标记的异常。'
- en: '|  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'In this chapter, you will learn why unsupervised machine learning is different:
    we can use any data without having to label it for a *specific* purpose. We can
    throw in all images from the internet without having to annotate the data about
    the purpose of each sample, for *each representation* that we might care about.
    For example: Is there a dog in this picture? A car?'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解为什么无监督机器学习是不同的：我们可以使用任何数据，而无需为特定目的对其进行标记。我们可以随意加入来自互联网的所有图像，而无需注释每个样本的目的，对于我们可能关心的每种表示。例如：这张图片里有没有狗？有没有车？
- en: In supervised learning, on the other hand, if you don’t have labels for that
    exact task, (almost) all of your labels could be unusable. If you’re trying to
    make a classifier that would classify cars from Google Street View, but you do
    not have labels of those images for animals as well, training a classifier that
    would classify animals with the same dataset would be basically impossible. Even
    if the animals frequently feature in these samples, you would need to go back
    and ask your labelers to relabel the same Google Street View dataset for animals.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在监督学习中，如果你没有该特定任务的标签，(几乎)所有的标签都可能无法使用。如果你试图制作一个能够从谷歌街景中分类汽车的分类器，但你没有这些图像的动物标签，使用同一数据集训练一个能够用同一数据集分类动物的分类器基本上是不可能的。即使动物经常出现在这些样本中，你也需要回去要求你的标记者重新标记谷歌街景数据集中的动物。
- en: 'In essence, we need to think about the application of the data before we know
    the use case, which is difficult! But for a lot of compression-type tasks, you
    always have labeled data: your data. Some researchers, such as François Chollet
    (research scientist at Google and author of Keras), call this type of machine
    learning *self-supervised*. For much of this book, our only labels will be either
    the examples themselves or any other examples from the dataset.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，在我们知道用例之前，我们需要考虑数据的应用，这是困难的！但对于许多压缩类型任务，你总是有标记数据：你的数据。一些研究人员，如弗朗索瓦·肖莱特（Google的研究科学家和Keras的作者），将这种类型的机器学习称为*自监督*。在这本书的大部分内容中，我们唯一的标签将是示例本身或数据集中的任何其他示例。
- en: 'Since our training data also acts as our labels, training many of these algorithms
    becomes far easier from one crucial perspective: we now have lots more data to
    work with, and we do not need to wait weeks and pay millions for enough labeled
    data.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的训练数据也充当我们的标签，从这一关键角度来看，训练许多这些算法变得容易得多：我们现在有更多的数据可以工作，而且我们不需要等待数周并支付数百万美元来获取足够的标记数据。
- en: 2.6.1\. New take on an old idea
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.1\. 对旧想法的新看法
- en: Autoencoders themselves are a fairly old idea—at least when you look at the
    age of machine learning as a field. But seeing as everyone is working on *something*
    deep today, it should surprise exactly no one that people have successfully applied
    deep learning as part of both encoder and decoder.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器本身是一个相当古老的想法——至少当你从机器学习作为一个领域的角度来看它的年龄时。但是鉴于今天每个人都在研究“某种”深度学习，人们成功地将深度学习应用于编码器和解码器的一部分，这应该不会让任何人感到惊讶。
- en: 'An autoencoder is composed of two neural networks: an encoder and a decoder.
    In our case, both have activation functions,^([[5](#ch02fn05)]) and we will be
    using just one intermediate layer for each. This means we have two weight matrices
    in each network—one from input to intermediate and then one from intermediate
    to latent. Then again, we have one from latent to different intermediate and then
    one from intermediate to output. If we had just one weight matrix in each, our
    procedure would resemble a well-established analytical technique called *principal
    component analysis (PCA)*. If you have a background in linear algebra, you should
    be in broadly familiar territory.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器由两个神经网络组成：一个编码器和一个解码器。在我们的例子中，两者都有激活函数，^([[5](#ch02fn05)])，我们将为每个网络使用一个中间层。这意味着每个网络中有两个权重矩阵——一个从输入到中间层，然后一个从中间层到潜在层。然后，我们还有一个从潜在层到不同的中间层，然后一个从中间层到输出层。如果我们每个网络只有一个权重矩阵，我们的过程将类似于一个已建立的统计分析技术，称为“主成分分析（PCA）”。如果你有线性代数的背景，你应该对这一领域有广泛的了解。
- en: ⁵
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We feed any output from an earlier layer’s computation through an *activation
    function* before passing it to the next one. Frequently, people pick a rectified
    linear unit (ReLU)—which is defined as *max(0, x)*. We don’t go into too much
    depth on activation functions, because they alone could be a subject of a lengthy
    blog post.
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在将任何来自早期层计算的输出通过一个*激活函数*传递到下一个层之前，将其传递给下一个层。人们经常选择一个整流线性单元（ReLU）——定义为*max(0,
    x)*。我们不会深入探讨激活函数，因为它们本身就可以是一个长篇博客文章的主题。
- en: '|  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Some technical differences exist in how the solutions are learned—for example,
    PCA is numerically deterministic, whereas autoencoders are typically trained with
    a stochastic optimizer. There are also differences in the final form of the solution.
    But we’re not going to give you a lecture about how one of them gives you an orthonormal
    basis and how fundamentally they still span the same vector space—though if you
    happen to know what that means, then more power to you.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习解决方案的方式上存在一些技术差异——例如，PCA是数值确定的，而自编码器通常使用随机优化器进行训练。解决方案的最终形式也存在差异。但我们不会给你上一堂关于其中一个如何给出正交基以及它们如何本质上仍然覆盖相同的向量空间的长篇大论——尽管如果你碰巧知道这意味着什么，那么恭喜你。
- en: '|  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 2.6.2\. Generation using an autoencoder
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.2\. 使用自编码器进行生成
- en: At the beginning of this chapter, we said that autoencoders can be used to generate
    data. Some of you who are really keen may have been thinking about the use of
    the latent space and whether it can be repurposed for something else . . . and
    it totally can! (If you got this right, you can give yourself an official, approved
    self-five!)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们提到自编码器可以用来生成数据。一些真正热衷于此的人可能已经在思考潜在空间的使用以及它是否可以被重新用于其他目的……这完全可能！（如果你答对了，你可以给自己一个官方的、认可的自我点赞！）
- en: But you probably didn’t buy this book to look silly, so let’s get to the point.
    If we go back to the example with your grandparents and apply a slightly different
    lens, using autoencoders as a generative model might start to make sense. For
    example, imagine that your idea of what a *job* is becomes the input to the decoder
    network. Think of the word *job* written down on the piece of paper as the latent
    space input, and the idea of a job in your grandparents’ head as the output.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能不会买这本书来让自己看起来很傻，所以我们直接进入正题。如果我们回到你祖父母的例子，并使用一个稍微不同的视角，使用自编码器作为生成模型可能开始变得有意义。例如，想象一下你对“工作”这个概念的理解成为解码网络的输入。将“工作”这个词写在纸上的想法视为潜在空间输入，而你祖父母头脑中的工作概念作为输出。
- en: In this case, we see that the latent space encoding (a written word, combined
    with your grandparents’ ability to read and understand concepts) becomes a generative
    model that generates an idea in their heads. The written letter acts as an inspiration
    or some sort of latent vector, and the output—the ideas—are in the same high-dimensional
    space as the original input. Your grandparents’ ideas are as complex—albeit slightly
    different—as yours.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们看到潜在空间编码（一个书面词，结合你祖父母阅读和理解概念的能力）变成了一种在他们的脑海中生成想法的生成模型。书面字母充当灵感或某种潜在向量，而输出——想法——与原始输入处于相同的高维空间。你祖父母的想法和你的一样复杂——尽管略有不同。
- en: Now let’s switch back to the domain of images. We train our autoencoder on a
    set of images. So we tune the parameters of the encoder and the decoder to find
    appropriate parameters for the two networks. We also get a sense for the way the
    examples are represented in the latent space. For generation, we cut off the encoder
    part and use only the latent space and the decoder. [Figure 2.3](#ch02fig03) shows
    a schematic of the generation process.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到图像领域。我们在一组图像上训练我们的自动编码器。因此，我们调整编码器和解码器的参数，以找到两个网络适当的参数。我们还对例子在潜在空间中的表示方式有了感觉。对于生成，我们切断编码器部分，只使用潜在空间和解码器。[图2.3](#ch02fig03)显示了生成过程的示意图。
- en: Figure 2.3\. Because we know from training where our examples get placed in
    the latent space, we can easily generate examples similar to the ones that the
    model has seen. Even if not, we can easily iterate or grid-search through the
    latent space to determine the kinds of representations that our model can generate.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3. 因为我们知道从训练中我们的例子在潜在空间中的位置，我们可以轻松地生成与模型所见相似的例子。即使不是，我们也可以轻松地在潜在空间中进行迭代或网格搜索，以确定我们的模型可以生成的表示类型。
- en: '![](../Images/02fig03_alt.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02fig03_alt.jpg)'
- en: (Image adapted from Mat Leonard’s simple autoencoder project on GitHub, [http://mng.bz/oNXM](http://mng.bz/oNXM).)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: （图片改编自GitHub上Mat Leonard的简单自动编码器项目，[http://mng.bz/oNXM](http://mng.bz/oNXM)。）
- en: 2.6.3\. Variational autoencoder
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.3. 变分自动编码器
- en: 'You may be wondering: what is the difference between a variational autoencoder
    and a “regular” one? It all has to do with the magical latent space. In the case
    of a variational autoencoder, we choose to represent the latent space as a distribution
    with a learned mean and standard deviation rather than just a set of numbers.
    Typically, we choose multivariate Gaussian, but exactly what that is or why we
    choose this distribution over another is not that important right now. If you
    would like a refresher on what that might look like, take a look at [figure 2.5](#ch02fig05).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道：变分自动编码器和“常规”自动编码器之间的区别是什么？这所有的一切都与神奇的潜在空间有关。在变分自动编码器的情况下，我们选择将潜在空间表示为一个具有学习到的均值和标准差的分布，而不是仅仅是一组数字。通常，我们选择多元高斯分布，但具体是什么或者为什么我们选择这种分布而不是其他分布现在并不重要。如果你想回顾一下这可能是什么样子，请查看[图2.5](#ch02fig05)。
- en: As the more statistically inclined of you may have realized at this point, the
    variational autoencoder is a technique based on Bayesian machine learning. In
    practice, this means we have to learn the distribution, which adds further constraints.
    In other words, frequentist autoencoders would try to learn the latent space as
    an array of numbers, but Bayesian—for example, variational—autoencoders would
    try to find the right parameters defining a distribution.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如此一来，那些更倾向于统计学的你们可能已经意识到，变分自动编码器是一种基于贝叶斯机器学习的技术。在实践中，这意味着我们必须学习分布，这增加了进一步的约束。换句话说，频率派自动编码器会试图将潜在空间学习为一个数字数组，但贝叶斯——例如，变分——自动编码器会试图找到定义分布的正确参数。
- en: We then sample from the latent distribution and get some numbers. We feed these
    numbers through the decoder. We get back an example that looks like something
    from the original dataset, except it has been newly created by the model. Ta-da!
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后从潜在分布中进行采样并得到一些数字。我们将这些数字通过解码器。我们得到一个看起来像是原始数据集中的一些东西的例子，但它是由模型新创建的。哇塞！
- en: 2.7\. Code is life
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7. 代码即生命
- en: In this book, we use a popular, deep learning, high-level API called *Keras*.
    We highly suggest that you familiarize yourself with it. If you are not already
    comfortable with it, plenty of good free resources are available online, including
    outlets such as Towards Data Science ([http://towardsdatascience.com](http://towardsdatascience.com)),
    where we frequently contribute. If you want to learn more about Keras from a book,
    several good resources exist, including another great Manning book, *Deep Learning
    with Python* by François Chollet—the author and creator of Keras.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用了一个流行的、深度学习的高级API，称为*Keras*。我们强烈建议你熟悉它。如果你还不熟悉它，网上有大量的免费资源，包括像Towards
    Data Science ([http://towardsdatascience.com](http://towardsdatascience.com))这样的平台，我们经常在上面贡献。如果你想从书籍中了解更多关于Keras的信息，存在一些很好的资源，包括另一本优秀的Manning书籍，由Keras的作者和创建者François
    Chollet所著的*用Python进行深度学习*。
- en: Keras is a high-level API for several deep learning frameworks—TensorFlow, Microsoft
    Cognitive Toolkit (CNTK), and Theano. It is easy to use and allows you to work
    on a much higher level of abstraction, so you can focus on the concepts rather
    than recording every standard block of multiplication, biasing, activation, and
    then pooling^([[6](#ch02fn06)]) or having to worry about variable scopes too much.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是几个深度学习框架（TensorFlow、Microsoft Cognitive Toolkit (CNTK)和Theano）的高级API。它易于使用，并允许你在更高的抽象级别上工作，因此你可以专注于概念，而不是记录每个标准的乘法、偏差、激活和池化^([[6](#ch02fn06)])或过多地担心变量作用域。
- en: ⁶
  id: totrans-246
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-247
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A *pooling block* is an operation on a layer that allows us to pool several
    inputs into fewer—for example, having a matrix of four numbers and getting the
    maximum value as a single number. This is a common operation in computer vision
    to reduce complexity.
  id: totrans-248
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*池化块*是对一层进行的操作，允许我们将多个输入合并为更少的输入——例如，有一个包含四个数字的矩阵，并得到一个最大值作为单个数字。这是计算机视觉中常用的操作，用于降低复杂性。'
- en: To show the true power of Keras and how it simplifies the process of writing
    a neural network, we will look at the variational autoencoder example in its simplest
    form.^([[7](#ch02fn07)]) In this tutorial, we use the *functional API* that Keras
    has for a more function-oriented approach to writing deep learning code, but we
    will show you the sequential API (the other way) in later tutorials as things
    get more difficult.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示Keras的真实力量以及它是如何简化编写神经网络的过程，我们将查看其最简单的变分自动编码器示例^([[7](#ch02fn07)])。在本教程中，我们使用Keras的*功能性API*，以更面向函数的方法编写深度学习代码，但在后续教程中，我们将展示序列API（另一种方法），因为事情变得更加复杂。
- en: ⁷
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This example was highly modified by the authors for simplicity, from [http://mng.bz/nQ4K](http://mng.bz/nQ4K).
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作者为了简化，对示例进行了高度修改，来自[http://mng.bz/nQ4K](http://mng.bz/nQ4K)。
- en: The goal of this exercise is to generate handwritten digits based on the latent
    space. We are going to create an object, `generator` or `decoder`, that can use
    the `predict()` method to generate new examples of handwritten digits, given an
    input seed, which is just the latent space vector. And of course, we have to use
    MNIST because we wouldn’t want anyone getting any ideas that there could be other
    datasets out there; see [figure 2.4](#ch02fig04).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是根据潜在空间生成手写数字。我们将创建一个对象，`generator`或`decoder`，它可以使用`predict()`方法根据输入种子生成新的手写数字示例，其中输入种子只是潜在空间向量。当然，我们必须使用MNIST，因为我们不希望任何人产生其他数据集可能存在的想法；参见[图2.4](#ch02fig04)。
- en: Figure 2.4\. How computer vision researchers think. Enough said.
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4。计算机视觉研究人员是如何思考的。无需多言。
- en: '![](../Images/02fig04_alt.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02fig04_alt.jpg)'
- en: '(Source: Artificial Intelligence Memes for Artificial Intelligence Teens on
    Facebook, [http://mng.bz/vNjM](http://mng.bz/vNjM).)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Facebook上的人工智能青少年的人工智能迷因，[http://mng.bz/vNjM](http://mng.bz/vNjM)。）
- en: In our code, we first have to import all dependencies, as shown in the following
    listing. For reference, this code was checked with Keras as late as 2.2.4 and
    TensorFlow as late as 1.12.0.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，我们首先必须导入所有依赖项，如下所示列表。为了参考，此代码已与Keras的最新版本2.2.4和TensorFlow的最新版本1.12.0进行了检查。
- en: Listing 2.1\. Standard imports
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1。标准导入
- en: '[PRE0]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next step is to set global variables and hyperparameters, as shown in [listing
    2.2](#ch02ex02). They should all be familiar: the original dimensions are 28 ×
    28, which is the standard size. We then flatten the images from the MNIST dataset,
    to get a vector of 784 (28 × 28) dimensions. And we will also have a single intermediate
    layer of, say, 256 nodes. But do experiment with other sizes; that’s why it’s
    a hyperparameter!'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置全局变量和超参数，如[代码清单2.2](#ch02ex02)所示。它们都应该很熟悉：原始维度是28 × 28，这是标准尺寸。然后我们将MNIST数据集中的图像展平，得到一个784维（28
    × 28）的向量。我们还将有一个单一的中间层，例如256个节点。但请尝试其他尺寸；这就是为什么它是超参数的原因！
- en: Listing 2.2\. Setting hyperparameters
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单2.2\. 设置超参数
- en: '[PRE1]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* Height × width of MNIST image**'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* MNIST图像的高度 × 宽度**'
- en: '***2* Number of epochs**'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 训练的轮数**'
- en: In [listing 2.3](#ch02ex03), we start constructing the encoder. To achieve this,
    we use the functional API from Keras.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在[代码清单2.3](#ch02ex03)中，我们开始构建编码器。为了实现这一点，我们使用Keras的功能API。
- en: '|  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The *functional API* uses lambda functions in Python to return constructors
    for another function, which takes another input, producing the final result.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*功能性API*使用Python中的lambda函数来返回另一个函数的构造函数，该函数接受另一个输入，产生最终结果。'
- en: '|  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The short version is that we will simply declare each layer, mentioning the
    previous input as a *second group of arguments* after the regular arguments. For
    example, the layer `h` takes `x` as an input. At the end, when we compile the
    model and indicate where it starts (*x*) and where it ends ([`z_mean`, `z_log_var`
    and `z`]), Keras will understand how the starting input and the final list output
    are linked together. Remember from the diagrams that `z` is our latent space,
    which in this case is a normal distribution defined by mean and variance. Let’s
    now define the encoder.^([[8](#ch02fn08)])
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们将简单地声明每一层，在常规参数之后提到前一个输入作为*第二个参数组*。例如，层`h`以`x`作为输入。最后，当我们编译模型并指出它开始的位置（*x*）和结束的位置（`[z_mean`,
    `z_log_var`和`z`）时，Keras将理解起始输入和最终输出列表是如何联系在一起的。记住从图中，`z`是我们的潜在空间，在这种情况下，它是由均值和方差定义的正态分布。现在让我们定义编码器。[^8](#ch02fn08)
- en: ⁸
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This idea is inspired by Branko Blagojevic in our book forums. Thank you for
    this suggestion.
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个想法是从我们书论坛中的Branko Blagojevic那里得到的灵感。感谢这个建议。
- en: Listing 2.3\. Creating the encoder
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单2.3\. 创建编码器
- en: '[PRE2]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Input to our encoder**'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 编码器的输入**'
- en: '***2* Intermediate layer**'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 中间层**'
- en: '***3* Defines the mean of the latent space**'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 定义潜在空间的均值**'
- en: '***4* Defines the log variance of the latent space**'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 定义潜在空间的对数方差**'
- en: '***5* Note that output_shape isn’t necessary with the TensorFlow backend.**'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 注意，在TensorFlow后端中，output_shape不是必需的。**'
- en: '***6* Defines the encoder as a Keras model**'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将编码器定义为Keras模型**'
- en: 'Now comes the tricky part, where we sample from the latent space and then feed
    this information through to the decoder. But think for a bit how `z_mean` and
    `z_log_var` are connected: they are both connected to `h` with a dense layer of
    two nodes, which are the defining characteristics of a normal distribution: mean
    and variance. The preceding sampling function is implemented as shown in the following
    listing.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有点棘手的部分，我们从潜在空间中进行采样，然后将这些信息传递到解码器。但稍微思考一下`z_mean`和`z_log_var`是如何连接的：它们都通过一个有两个节点的密集层连接到`h`，这是正态分布的定义特征：均值和方差。前面的采样函数实现如下所示。
- en: Listing 2.4\. Creating the sampling helper function
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单2.4\. 创建采样辅助函数
- en: '[PRE3]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In other words, we learn the mean (μ) and the variance (`μ`). This overall implementation,
    where we have one (`ω`) connected through a sampling function as well as `z_mean`
    and `z_log_var`, allows us to both train and subsequently sample efficiently to
    get some neat-looking figures at the end. During generation, we sample from this
    distribution according to these learned parameters, and then we feed these values
    through the decoder to get the output, as you will see in the figures later. For
    those of you who are a bit rusty on distributions—or probability density functions
    in this case—we have included several examples of unimodal two-dimensional Gaussians
    in [figure 2.5](#ch02fig05).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们学习均值（μ）和方差（`μ`）。这种整体实现，其中我们有一个（`ω`）通过采样函数以及`z_mean`和`z_log_var`连接，使我们能够既进行训练，又能够高效地采样以在最后得到一些看起来很漂亮的图像。在生成过程中，我们根据这些学习到的参数从这个分布中进行采样，然后将这些值通过解码器传递以获得输出，正如你将在后面的图中看到的那样。对于那些对分布——或者在这个案例中是概率密度函数——有点生疏的人，我们在[图2.5](#ch02fig05)中包含了几个单峰二维高斯分布的例子。
- en: Figure 2.5\. As a reminder of what a multivariate (2D) distribution looks like,
    we’ve plotted probability density functions of bivariate (2D) Gaussians. They
    are uncorrelated 2D normal distributions, except with different variances. (a)
    has a variance of 0.5, (b) of 1, and (c) of 2\. (d), (e), and (f) are the exact
    same distributions as (a), (b), and (c), respectively, but plotted with a set
    z-axis limit at 0.7\. Intuitively, this is just a function that for each point
    says how likely it is to occur. So (a) and (d) are much more concentrated, whereas
    (c) and (f) are making it possible for values far away from the origin (0,0) to
    occur, but each given value is not as likely.
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5。为了提醒大家什么是多元（2D）分布的样子，我们绘制了二元（2D）高斯分布的概率密度函数。它们是不相关的2D正态分布，只是方差不同。（a）的方差为0.5，（b）为1，（c）为2。（d）、（e）和（f）分别与（a）、（b）和（c）完全相同的分布，但以0.7为z轴限制进行绘制。直观上，这只是一个对于每个点说明其发生的可能性的函数。所以（a）和（d）更加集中，而（c）和（f）则使得远离原点（0,0）的值出现成为可能，但每个给定值出现的可能性并不高。
- en: '![](../Images/02fig05_alt.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02fig05_alt.jpg)'
- en: Now that you understand what defines our latent space and what these distributions
    look like, we’ll write the decoder. In this case, we write the layers as variables
    first so we can reuse them later for the generation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了定义我们的潜在空间以及这些分布的样子，我们将编写解码器。在这种情况下，我们首先将层作为变量编写，这样我们可以在生成时重用它们。
- en: Listing 2.5\. Writing the decoder
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5。编写解码器
- en: '[PRE4]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* Input to the decoder**'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 解码器的输入**'
- en: '***2* Takes the latent space to the intermediate dimension**'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将潜在空间转换为中间维度**'
- en: '***3* Gets the mean from the original dimension**'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 从原始维度获取均值**'
- en: '***4* Defines the decoder as a Keras model**'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将解码器定义为Keras模型**'
- en: We can now combine the encoder and the decoder into a single VAE model.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将编码器和解码器组合成一个单一的VAE模型。
- en: Listing 2.6\. Combining the model
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6。组合模型
- en: '[PRE5]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '***1* Grabs the output. Recall that we need to grab the third element, our
    sampling z.**'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取输出。回想一下，我们需要获取第三个元素，我们的采样z。**'
- en: '***2* Links the input and the overall output**'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 连接输入和整体输出**'
- en: '***3* Prints out what the overall model looks like**'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 打印出整体模型的样子**'
- en: 'Next, we get to the more familiar parts of machine learning: defining a loss
    function so our autoencoder can train.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入机器学习的更熟悉的部分：定义损失函数，以便我们的自动编码器可以训练。
- en: Listing 2.7\. Defining our loss function
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.7。定义我们的损失函数
- en: '[PRE6]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1* Finally compiles our model**'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 最后编译我们的模型**'
- en: 'Here you can see where using binary cross-entropy and KL divergence add together
    to form overall loss. *KL divergence* measures the difference between distributions;
    imagine the two blobs from [figure 2.5](#ch02fig05) and then measuring the volume
    of overlap. Binary cross-entropy is one of the common loss functions for two-class
    classification: here we simply compare each grayscale pixel value of `x` to the
    value in `x_decoded_mean`, which is the reconstruction we were talking about earlier.
    If you are still confused about this paragraph after the following definition,
    [chapter 5](../Text/kindle_split_015.xhtml#ch05) provides more details on measuring
    differences between distributions.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到使用二元交叉熵和KL散度相加形成整体损失的地方。*KL散度*衡量分布之间的差异；想象一下[图2.5](#ch02fig05)中的两个团块，然后测量重叠的体积。二元交叉熵是两分类中常见的损失函数之一：在这里，我们简单地比较`x`的每个灰度像素值与`x_decoded_mean`中的值，这就是我们之前提到的重建。如果你在以下定义之后仍然对这个段落感到困惑，[第5章](../Text/kindle_split_015.xhtml#ch05)提供了关于测量分布之间差异的更多细节。
- en: '|  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: For those interested in more detail and who are familiar with information theory,
    the *Kullback–Leibler divergence (KL divergence)*, aka *relative entropy*, is
    the difference between cross-entropy of two distributions and their own entropy.
    For everyone else, imagine drawing out the two distributions, and wherever they
    do not overlap will be an area proportional to the KL divergence.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对细节感兴趣且熟悉信息理论的人来说，*库尔巴克-莱布勒散度（KL散度）*，也称为*相对熵*，是两个分布的交叉熵与它们自身的熵之间的差异。对于其他人来说，想象画出这两个分布，它们不重叠的部分将是一个与KL散度成比例的区域。
- en: '|  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Then we define the model to start at `x` and end at `x_decoded_mean`. The model
    is compiled using RMSprop, but we could use Adam or vanilla stochastic gradient
    descent (SGD). As with any deep learning system, we are using backpropagated errors
    to navigate the parameter space. We are always using some type of gradient descent,
    but in general, people rarely try any other than the three mentioned here: Adam,
    SGD, or RMSprop.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义模型从`x`开始，到`x_decoded_mean`结束。模型使用RMSprop编译，但我们可以使用Adam或普通的随机梯度下降（SGD）。与任何深度学习系统一样，我们使用反向传播的错误来导航参数空间。我们始终使用某种类型的梯度下降，但通常人们很少尝试除了这里提到的三种以外的其他方法：Adam、SGD或RMSprop。
- en: '|  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: '*Stochastic gradient descent (SGD)* is an optimization technique that allows
    us to train complex models by figuring out the contribution of any given weight
    to an error and updating this weight (no update if the prediction is 100% correct).
    We recommend brushing up on this in, for example, *Deep Learning with Python*.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机梯度下降（SGD）*是一种优化技术，它允许我们通过确定任何给定权重对错误的贡献并更新此权重（如果预测100%正确则不更新）来训练复杂模型。我们建议在例如《Python深度学习》中复习这一内容。'
- en: '|  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We train the model by using the standard procedure of train-test split and input
    normalization.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用标准的训练-测试分割和输入归一化程序来训练模型。
- en: Listing 2.8\. Creating the train/test split
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.8\. 创建训练/测试分割
- en: '[PRE7]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We normalize the data and reshape the train set and test set to be one 784-digit-long
    array per example instead of a 28 × 28 matrix.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据进行归一化，并将训练集和测试集重塑为每个示例一个784位长的数组，而不是一个28×28的矩阵。
- en: 'Then we apply the `fit` function, using shuffling to get a realistic (nonordered)
    dataset. We also use validation data to monitor progress as we train:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应用`fit`函数，通过打乱数据来获取一个真实的（非有序）数据集。我们还在训练过程中使用验证数据来监控进度：
- en: '[PRE8]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We’re done!
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了！
- en: The full version of the code provides a fun visualization of the latent space;
    however, for that, look into the accompanying Jupyter/Google Colaboratory notebook.
    Now we get to kick back, relax, and watch those pretty progress bars. After we
    are done, we can even take a look at what the values of the latent space look
    like on a 2D plane, as shown in [figure 2.6](#ch02fig06).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的完整版本提供了一个有趣的潜在空间可视化；然而，为了查看它，请查看附带的Jupyter/Google Colaboratory笔记本。现在我们可以放松，观看那些漂亮的进度条。完成后，我们甚至可以查看潜在空间在二维平面上的值，如图[图2.6](#ch02fig06)所示。
- en: Figure 2.6\. 2D projection of all the points from the test set into the latent
    space and their class. In this figure, we display the 2D latent space onto the
    graph. We then map out the classes of these generated examples and color them
    accordingly, as per the legend on the right. Here we can see that the classes
    tend to be neatly grouped together, which tells us that this is a good representation.
    A color version is available in the GitHub repository for this book.
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '图2.6\. 将测试集中的所有点及其类别投影到潜在空间中的二维投影。在这个图中，我们将二维潜在空间显示在图上。然后我们绘制这些生成示例的类别，并按照右侧图例中的颜色进行着色。在这里，我们可以看到类别往往整齐地分组在一起，这告诉我们这是一个好的表示。本书的GitHub仓库中有彩色版本。 '
- en: '![](../Images/02fig06.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02fig06.jpg)'
- en: We can also compute the values at fixed increments of a latent space grid to
    take a look at the generated output. For example, going from 0.05 to 0.95 in 0.15
    linear increments across both dimensions gives us the visualization in [figure
    2.7](#ch02fig07). Remember that we’re using a bivariate Gaussian in this case,
    giving us two axes to iterate over. Again, for the code for this visualization,
    look at the full Jupyter/Google Colab notebook.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算潜在空间网格上固定增量处的值，以查看生成的输出。例如，从0.05到0.95，在两个维度上以0.15的线性增量进行，我们得到了[图2.7](#ch02fig07)中的可视化。记住，在这种情况下，我们使用的是双变量高斯分布，给我们两个轴来迭代。再次提醒，对于这个可视化的代码，请查看完整的Jupyter/Google
    Colab笔记本。
- en: Figure 2.7\. We map out the values of a subset of the latent space on a grid
    and pass each of those latent space values through the generator to produce this
    figure. This gives us a sense of how much the resulting picture changes as we
    vary z.
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7\. 我们在网格上绘制潜在空间子集的值，并将每个潜在空间值通过生成器传递以生成此图。这让我们对随着z的变化，最终图片的变化程度有一个感觉。
- en: '![](../Images/02fig07_alt.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02fig07_alt.jpg)'
- en: 2.8\. Why did we try aGAN?
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8\. 为什么我们尝试了aGAN？
- en: It would seem that the book could almost stop at this point. After all, we have
    successfully generated images of MNIST, and that will be our test case for several
    examples. So before you call it quits, let us explain our motivation for the chapters
    to come.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这本书几乎可以在这里结束。毕竟，我们已经成功地生成了MNIST的图像，这将成为我们几个示例的测试案例。所以在你放弃之前，让我们解释一下接下来章节的动机。
- en: To appreciate the challenges, imagine that we have a simple one-dimensional
    bimodal distribution—as pictured in [figure 2.8](#ch02fig08). (As before, just
    think of it as a simple mathematical function that is bounded between 0 and 1
    and that represents probability at any given point. The higher the value of the
    function, the more points we sampled at that exact point before.)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解挑战，想象我们有一个简单的一维双峰分布——如图2.8所示。（就像之前一样，把它想象成一个简单的数学函数，它在0和1之间有界，代表任何给定点的概率。函数的值越高，我们在该确切点之前采样的点就越多。）
- en: Figure 2.8\. Maximum likelihood, point estimates, and true distributions. The
    gray (theoretical) distribution is bimodal rather than having a single mode. But
    because we have assumed this, our model is catastrophically wrong. Alternatively,
    we can get mode collapse, which is worth keeping in mind for [chapter 5](../Text/kindle_split_015.xhtml#ch05).
    This is especially true when we are using flavors of the KL, such as the VAE or
    early GANs.
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8. 最大似然估计、点估计和真实分布。灰色（理论）分布是双峰的，而不是单峰。但由于我们假设了这一点，我们的模型是灾难性的错误。或者，我们可能会遇到模式坍塌，这在第5章中值得记住。[第5章](../Text/kindle_split_015.xhtml#ch05)。这在我们使用KL散度的变体，如VAE或早期GANs时尤其如此。
- en: '![](../Images/02fig08_alt.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02fig08_alt.jpg)'
- en: Suppose we draw a bunch of samples from this true distribution, but we do not
    know the underlying model. We are now trying to infer what distribution generated
    these samples, but for some reason we assume that the true distribution is a simple
    Gaussian and we just need to estimate the mean and variance. But because we did
    not specify the model correctly (in this case, we put in wrong assumptions about
    the modality of these samples), we get into loads of trouble. For example, if
    we apply a traditional statistical technique called *maximum likelihood estimation*
    to estimate this distribution as unimodal—in some ways, that is what VAE is trying
    to do—we get out the wrong estimate. Because we have misspecified the model,^([[9](#ch02fn09)])
    it will estimate a normal distribution around the average of the two distributions—called
    the *point estimate*. Maximum likelihood is a technique that does not know and
    cannot figure out that there are two distinct distributions. So to minimize the
    error, it creates a “fat-tailed” normal around the point estimate. Here, it can
    seem trivial, but always remember, we are trying to specify models in very high-dimensional
    spaces, which is not easy!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从这个真实分布中抽取了一组样本，但我们不知道底层模型。我们现在试图推断生成这些样本的分布，但出于某种原因，我们假设真实分布是一个简单的高斯分布，我们只需要估计均值和方差。但由于我们没有正确指定模型（在这种情况下，我们对样本的模态做出了错误的假设），我们陷入了麻烦。例如，如果我们应用一种传统的统计技术，称为*最大似然估计*来估计这个分布为单峰——在某种程度上，这就是VAE试图做的——我们会得到错误的估计。因为我们没有正确指定模型，^([[9](#ch02fn09)])它将估计围绕两个分布平均值的正态分布——称为*点估计*。最大似然估计是一种不知道也无法弄清楚存在两个不同分布的技术。因此，为了最小化误差，它围绕点估计创建了一个“胖尾”正态分布。在这里，这似乎很微不足道，但请始终记住，我们正在尝试在非常高的维空间中指定模型，这并不容易！
- en: ⁹
  id: totrans-334
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-335
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See *Pattern Recognition and Machine Learning*, by Christopher Bishop (Springer,
    2011).
  id: totrans-336
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Christopher Bishop的《模式识别与机器学习》（Springer，2011年）。
- en: '|  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: '*Bimodal* means having two peaks, or modes. This notion will be useful in [chapter
    5](../Text/kindle_split_015.xhtml#ch05). In this case, we made the overall distribution
    to be composed of two normals with means of 0 and 5.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '*双峰*意味着有两个峰值或模态。这个概念将在第5章中很有用。在这种情况下，我们将整体分布由两个均值为0和5的正态分布组成。'
- en: '|  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Interestingly, the point estimate will also be wrong and can even live in an
    area where there is no actual data sampled from the true distribution. When you
    look at the samples (black crosses), no real samples occur where we have estimated
    our mean. This is, again, quite troubling. To tie it back to the autoencoder,
    see how in [figure 2.6](#ch02fig06) we learned 2D normal distribution in the latent
    space centered around the origin? But what if we had thrown images of celebrity
    faces into the training data? We would no longer have an easy center to estimate,
    because the two data distributions would have more modalities than we thought
    we would have. As a result, even around the center of the distribution, the VAE
    could produce odd hybrids of the two datasets, because the VAE would try to somehow
    separate the two datasets.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，点估计也可能出错，甚至可能位于一个没有任何实际数据样本的真实分布区域。当你查看样本（黑色交叉点）时，我们估计均值的地方并没有出现任何真实样本。这再次是非常令人不安的。为了将其与自编码器联系起来，看看在[图2.6](#ch02fig06)中我们是如何在以原点为中心的潜在空间中学习2D正态分布的？但如果我们把名人脸的图像扔进训练数据中呢？我们就不再有一个容易估计的中心了，因为两个数据分布的峰态比我们想象的要多。因此，即使在分布的中心附近，VAE也可能产生两个数据集的奇怪混合体，因为VAE会试图以某种方式分离这两个数据集。
- en: So far, we have discussed only the hypothetical impact of a statistical mistake.
    To connect this aspect all the way to autoencoder-generated images, we should
    think about what our Gaussian latent space *z* allows us to do. The VAE uses the
    Gaussian as a way to build representations of the data it sees. But because Gaussians
    have 99.7% of the probability mass within three standard deviations of the middle,
    the VAE will also opt for the safe middle. Because VAEs are, in a way, trying
    to come up directly with the underlying model based on Gaussians, but the reality
    can be pretty complex, VAEs do not scale up as well as GANs, which can pick up
    “scenarios.”
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了统计错误假设的影响。为了将这一方面完全与自编码器生成的图像联系起来，我们应该考虑我们的高斯潜在空间*z*允许我们做什么。VAE使用高斯作为构建它所看到的数据表示的方式。但由于高斯有99.7%的概率质量在中间三个标准差内，VAE也会选择安全的中庸之道。因为VAEs在某种程度上试图直接基于高斯提出底层模型，但现实可能相当复杂，VAEs的扩展性不如GANs，GANs可以捕捉“场景”。
- en: You can see what happens when your VAE opts for the “safe middle” in [figure
    2.9](#ch02fig09). On the CelebA dataset, which features aligned and cropped celebrity
    faces, the VAE models the consistently present facial features well, such as eyes
    or mouth, but makes mistakes in the background.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图2.9](#ch02fig09)中看到当你的VAE选择“安全的中庸之道”时会发生什么。在CelebA数据集上，该数据集具有对齐和裁剪的名人脸特征，VAE很好地模拟了持续存在的面部特征，如眼睛或嘴巴，但在背景上犯错误。
- en: Figure 2.9\. In these images of fake celebrity faces generated by a VAE, the
    edges are quite blurry and blend into the background. This is because the CelebA
    dataset has centered and aligned images with consistent features around eyes and
    mouth, but the backgrounds tend to vary. The VAE picks the safe path and makes
    the background blurry by choosing a “safe” pixel value, which minimizes the loss,
    but does not provide good images.
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9。在这些由VAE生成的假名人脸图像中，边缘相当模糊，并融入背景中。这是因为CelebA数据集具有围绕眼睛和嘴巴周围特征一致的中心和校准图像，但背景往往变化。VAE选择了安全的路径，通过选择一个“安全”的像素值来使背景模糊，这最小化了损失，但并没有提供好的图像。
- en: '![](../Images/02fig09_alt.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图2.9的替代图片](../Images/02fig09_alt.jpg)'
- en: '(Source: VAE-TensorFlow by Zhenliang He, GitHub, [https://github.com/LynnHo/VAE-Tensorflow](https://github.com/LynnHo/VAE-Tensorflow).)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Zhenliang He的VAE-TensorFlow，GitHub，[https://github.com/LynnHo/VAE-Tensorflow](https://github.com/LynnHo/VAE-Tensorflow)）
- en: On the other hand, GANs have an implicit and hard-to-analyze understanding of
    the real data distribution. As you will discover in [chapter 5](../Text/kindle_split_015.xhtml#ch05),
    VAEs live in the directly estimated maximum likelihood model family.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GANs对真实数据分布有一个隐含且难以分析的理解。正如你将在[第5章](../Text/kindle_split_015.xhtml#ch05)中发现的那样，VAEs生活在直接估计的最大似然模型家族中。
- en: This section hopefully made you comfortable with thinking about the distributions
    of the target data and how these distributional implications manifest themselves
    in our training process. We will look into these assumptions much more in [chapter
    10](../Text/kindle_split_021.xhtml#ch10), where the model has assumed how to fill
    in the distributions and that becomes a problem that adversarial examples will
    be able to exploit to make our machine learning models fail.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分希望让你对思考目标数据的分布以及这些分布性影响如何在我们的训练过程中体现自己感到舒适。我们将在第10章（../Text/kindle_split_021.xhtml#ch10）中更深入地探讨这些假设，在那里模型假设如何填充分布，而这成为了一个对抗性示例能够利用来使我们的机器学习模型失败的问题。
- en: Summary
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Autoencoders on a high level are composed of an encoder, a latent space, and
    a decoder. An autoencoder is trained by using a common objective function that
    measures the distance between the reproduced and original data.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高层次上，自动编码器由编码器、潜在空间和解码器组成。自动编码器通过使用一个常见的目标函数来训练，该函数衡量重生产的数据与原始数据之间的距离。
- en: Autoencoders have many applications and can also be used as a generative model.
    In practice, this tends not to be their primary use because other methods, especially
    GANs, are better at the generative task.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器有许多应用，也可以用作生成模型。在实践中，这通常不是它们的主要用途，因为其他方法，尤其是GANs，在生成任务上表现得更好。
- en: We can use Keras (a high-level API for TensorFlow) to write a simple variational
    autoencoder that produces handwritten digits.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用Keras（TensorFlow的高级API）编写一个简单的变分自动编码器，它可以生成手写数字。
- en: VAEs have limitations that motivate us to move on to GANs.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEs有一些局限性，这促使我们转向GANs。
- en: 'Chapter 3\. Your first GAN: Generating handwritten digits'
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章。你的第一个GAN：生成手写数字
- en: '*This chapter covers*'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Exploring the theory behind GANs and adversarial training
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索GANs和对抗性训练背后的理论
- en: Understanding how GANs differ from conventional neural networks
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解GANs与常规神经网络的不同
- en: Implementing a GAN in Keras, and training it to generate handwritten digits
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中实现GAN，并训练它生成手写数字
- en: In this chapter, we explore the foundational theory behind GANs. We introduce
    the commonly used mathematical notation you may encounter if you choose to dive
    deeper into this field, perhaps by reading a more theoretically focused publication
    or even one of the many academic papers on this topic. This chapter also provides
    background knowledge for the more advanced chapters, particularly [chapter 5](../Text/kindle_split_015.xhtml#ch05).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了GANs背后的基础理论。我们介绍了如果你选择深入研究这个领域，可能会遇到的常用数学符号，这可能通过阅读更理论化的出版物或甚至许多关于这个主题的学术论文来实现。本章还为更高级的章节提供了背景知识，特别是[第5章](../Text/kindle_split_015.xhtml#ch05)。
- en: From a strictly practical standpoint, however, you don’t have to worry about
    many of these formalisms—much as you don’t need to know how an internal combustion
    engine works to drive a car. Machine learning libraries such as Keras and TensorFlow
    abstract the underlying mathematics away from us and neatly package them into
    importable lines of code.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 从严格实用的角度来看，然而，你不必担心许多这些形式化——就像你不需要知道内燃机的工作原理就能开车一样。像Keras和TensorFlow这样的机器学习库将底层数学抽象化，并将它们巧妙地打包成可导入的代码行。
- en: This will be a recurring theme throughout this book; it is also true for machine
    learning and deep learning in general. So, if you are someone who prefers to dive
    straight into practice, feel free to skim through the theory section and skip
    ahead to the coding tutorial.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是贯穿整本书的一个反复出现的主题；这对于机器学习和深度学习来说也是正确的。所以，如果你是那种喜欢直接进入实践的人，你可以自由地浏览理论部分，然后跳到编码教程。
- en: '3.1\. Foundations of GANs: Adversarial training'
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. GANs的基础：对抗性训练
- en: Formally, the Generator and the Discriminator are represented by differentiable
    functions, such as neural networks, each with its own cost function. The two networks
    are trained by backpropagation by using the Discriminator’s loss. The Discriminator
    strives to minimize the loss for both the real and the fake examples, while the
    Generator tries to maximize the Discriminator’s loss for the fake examples it
    produces.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，生成器和判别器由不同的可微函数表示，例如神经网络，每个都有自己的成本函数。这两个网络通过使用判别器的损失进行反向传播来训练。判别器努力最小化真实和虚假示例的损失，而生成器则试图最大化它产生的虚假示例的判别器损失。
- en: This dynamic is summarized in [figure 3.1](#ch03fig01). It is a more general
    version of the diagram from [chapter 1](../Text/kindle_split_010.xhtml#ch01),
    where we first explained what GANs are and how they work. Instead of the concrete
    example of handwritten digits, in this diagram, we have a general training dataset
    which, in theory, could be anything.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这种动态在[图3.1](#ch03fig01)中得到了总结。这是[第1章](../Text/kindle_split_010.xhtml#ch01)中图表的更通用版本，在那里我们首先解释了GAN是什么以及它们是如何工作的。在这个图表中，我们没有使用手写数字的具体示例，而是一个通用的训练数据集，在理论上可以是任何东西。
- en: Figure 3.1\. In this GAN architecture diagram, both the Generator and the Discriminator
    are trained using the Discriminator’s loss. The Discriminator strives to minimize
    the loss; the Generator seeks to maximize the loss for the fake examples it produces.
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1。在这个GAN架构图中，生成器和判别器都使用判别器的损失进行训练。判别器力求最小化损失；生成器寻求最大化它产生的假例的损失。
- en: '![](../Images/03fig01_alt.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fig01_alt.jpg)'
- en: Importantly, the training dataset determines the kind of examples the Generator
    will learn to emulate. If, for instance, our goal is to produce realistic-looking
    images of cats, we would supply our GAN with a dataset of cat images.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，训练数据集决定了生成器将学习模仿的示例类型。例如，如果我们目标是生成看起来逼真的猫的图像，我们会向我们的GAN提供一个猫的图像数据集。
- en: 'In more technical terms, the Generator’s goal is to produce examples that capture
    the data distribution of the training dataset.^([[1](#ch03fn01)]) Recall that
    to a computer, an image is just a matrix of values: two-dimensional for grayscale
    and three-dimensional for color (RGB) images. When rendered onscreen, the pixel
    values within these matrices manifest all the visual elements of an image—lines,
    edges, contours, and so forth. These values follow a complex distribution across
    each image in a dataset; after all, if no distribution is followed, an image will
    be no more than random noise. Object recognition models learn the patterns in
    images to discern an image’s content. The Generator can be thought of as the reverse
    of the process: rather than recognizing these patterns, it learns to synthesize
    them.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在更技术性的术语中，生成器的目标是生成能够捕捉训练数据集数据分布的示例。^([[1](#ch03fn01)]) 请记住，对计算机来说，图像只是一个值矩阵：灰度图像是二维的，RGB图像是三维的。当在屏幕上渲染时，这些矩阵内的像素值表现出图像的所有视觉元素——线条、边缘、轮廓等等。这些值在数据集中的每个图像中遵循复杂的分布；毕竟，如果没有遵循分布，图像将不会比随机噪声更多。物体识别模型学习图像中的模式以辨别图像的内容。生成器可以被视为这一过程的逆过程：它不是识别这些模式，而是学习合成它们。
- en: ¹
  id: totrans-369
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-370
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Generative Adversarial Networks,” by Ian J. Goodfellow et al., 2014, [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
  id: totrans-371
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Ian J. Goodfellow 等人撰写的《生成对抗网络》，2014年，[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)。
- en: 3.1.1\. Cost functions
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 成本函数
- en: 'Following the standard notation, let *J*^((*G*)) denote the Generator’s cost
    function and *J*^((*D*)) the Discriminator’s cost function. The trainable parameters
    (weights and biases) of the two networks are represented by the Greek letter theta:
    *θ*^((*G*)) for the Generator and *θ*^((*D*)) for the Discriminator.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 按照标准符号，用 *J*^((*G*)) 表示生成器的成本函数，用 *J*^((*D*)) 表示判别器的成本函数。两个网络的训练参数（权重和偏置）由希腊字母
    theta 表示：*θ*^((*G*)) 用于生成器，*θ*^((*D*)) 用于判别器。
- en: GANs differ from conventional neural networks in two key respects. First, the
    cost function, *J*, of a traditional neural network is defined exclusively in
    terms of its own trainable parameters, *θ*. Mathematically, this is expressed
    as *J*(*θ*). In contrast, GANs consist of two networks whose cost functions are
    dependent on *both* of the networks’ parameters. That is, the Generator’s cost
    function is *J*^((*G*))(*θ*^((*G*)), *θ*^((*D*))), and the Discriminator’s cost
    function is *J*^((*D*))(*θ*^((*G*)), *θ*^((*D*))).^([[2](#ch03fn02)])
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: GAN与传统的神经网络在两个关键方面有所不同。首先，传统神经网络的成本函数 *J* 仅以它自己的可训练参数 *θ* 为定义。数学上，这表示为 *J*(*θ*)。相比之下，GAN由两个网络组成，其成本函数依赖于两个网络的所有参数。也就是说，生成器的成本函数是
    *J*^((*G*))(*θ*^((*G*)), *θ*^((*D*)))，判别器的成本函数是 *J*^((*D*))(*θ*^((*G*)), *θ*^((*D*)))。^([[2](#ch03fn02)])
- en: ²
  id: totrans-375
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-376
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “NIPS 2016 Tutorial: Generative Adversarial Networks,” by Ian Goodfellow,
    2016, [https://arxiv.org/abs/1701.00160](https://arxiv.org/abs/1701.00160).'
  id: totrans-377
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Ian Goodfellow 撰写的《NIPS 2016教程：生成对抗网络》，2016年，[https://arxiv.org/abs/1701.00160](https://arxiv.org/abs/1701.00160)。
- en: The second (related) difference is that a traditional neural network can tune
    *all* its parameters, *θ*, during the training process. In a GAN, each network
    can tune only its own weights and biases. The Generator can tune only *θ*^((*G*)),
    and the Discriminator can tune only *θ*^((*D*)) during training. Accordingly,
    each network has control over only a part of what determines its loss.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个（相关）的区别是，传统的神经网络可以在训练过程中调整其所有参数，即 *θ*。在 GAN 中，每个网络只能调整自己的权重和偏差。生成器只能调整 *θ*^((*G*))，判别器只能调整
    *θ*^((*D*))。因此，每个网络只控制决定其损失的部分。
- en: To make this a little less abstract, consider the following analogy. Imagine
    we are choosing which route to drive home from work. If there is no traffic, the
    fastest option is the highway. During rush hour, however, we may be better off
    taking one of the side roads. Despite being longer and windier, they might get
    us home faster when the highway is all clogged up with traffic.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个问题更加具体，考虑以下类比。想象一下，我们正在选择从工作中回家的路线。如果没有交通，最快的选项是高速公路。然而，在高峰时段，我们可能更倾向于选择一条侧路。尽管这些路更长、更曲折，但在高速公路交通拥堵时，它们可能更快地带我们回家。
- en: Let’s phrase it as a math problem. Let *J* be our cost function, defined as
    the amount of time it takes us to get home. Our goal is to minimize *J*. For simplicity,
    let’s assume we have a set time to leave the office, so we cannot leave early
    to get ahead of rush hour or stay late to avoid it. The only parameter, *θ*, we
    can change is our route.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这个问题表述为一个数学问题。设 *J* 为我们的成本函数，定义为我们回家所需的时间。我们的目标是使 *J* 最小化。为了简化，我们假设我们有一个固定的时间离开办公室，因此我们无法提前离开以避开高峰时段，或者延迟离开以避免高峰时段。我们唯一可以改变的是我们的路线，即
    *θ*。
- en: 'If ours were the only car on the road, our cost would be similar to a regular
    neural network’s: it would depend only on the route, and it would be entirely
    within our power to optimize, *J*(*θ*). However, as soon as we introduce other
    drivers into the equation, the situation gets more complicated. Suddenly, the
    time it will take us to get home depends not only on our decisions but also on
    other drivers’ course of action, *J*(*θ*^((*us*)),*θ*^((*other drivers*))). Much
    like the Generator and Discriminator networks, our “cost function” will depend
    on an interplay of factors, some of which are under our control and others of
    which are not.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们路上只有一辆车，我们的成本将与常规神经网络类似：它将仅取决于路线，并且我们完全有能力对其进行优化，即 *J*(*θ*)。然而，一旦我们将其他司机纳入方程，情况就会变得更加复杂。突然之间，我们回家所需的时间不仅取决于我们的决定，还取决于其他司机的行动，即
    *J*(*θ*^((*us*)),*θ*^((*other drivers*)))。就像生成器和判别器网络一样，我们的“成本函数”将取决于一系列因素，其中一些在我们控制之下，而另一些则不在。
- en: 3.1.2\. Training process
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2。训练过程
- en: The two differences we’ve described have far-reaching implications on the GAN
    training process. The training of a traditional neural network is an optimization
    problem. We seek to minimize the cost function by finding a set of parameters
    such that moving to any neighboring point in the parameter space would increase
    the cost. This could be either a local or a global minimum in the parameter space,
    as determined by the cost function we are seeking to minimize. [Figure 3.2](#ch03fig02)
    illustrates the optimization process of minimizing a cost function.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所描述的两个差异对 GAN 训练过程有着深远的影响。传统神经网络的训练是一个优化问题。我们试图通过找到一组参数来最小化成本函数，使得移动到参数空间中的任何相邻点都会增加成本。这可能是参数空间中的局部或全局最小值，这取决于我们试图最小化的成本函数。[图
    3.2](#ch03fig02) 说明了最小化成本函数的优化过程。
- en: Figure 3.2\. The bowl-shaped mesh represents the loss J in the parameter space
    θ[1] and θ[2]. The black dotted line illustrates the minimization of the loss
    in the parameter space through optimization.
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.2。碗状的网格表示参数空间 θ[1] 和 θ[2] 中的损失 J。黑色虚线说明了通过优化在参数空间中损失的最小化过程。
- en: '![](../Images/03fig02.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fig02.jpg)'
- en: '(Source: “Adversarial Machine Learning” by Ian Goodfellow, ICLR Keynote, 2019,
    [www.iangoodfellow.com/slides/2019-05-07.pdf](http://www.iangoodfellow.com/slides/2019-05-07.pdf).)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“对抗机器学习”由 Ian Goodfellow 撰写，ICLR 主题演讲，2019，[www.iangoodfellow.com/slides/2019-05-07.pdf](http://www.iangoodfellow.com/slides/2019-05-07.pdf)。）
- en: Because the Generator and Discriminator can tune only their own parameters and
    not each other’s, GAN training can be better described as a game, rather than
    optimization.^([[3](#ch03fn03)]) The players in this game are the two networks
    that the GAN comprises.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成器和判别器只能调整自己的参数而不能调整对方的参数，GAN 训练可以更好地描述为一场游戏，而不是优化。（参考文献 [[3](#ch03fn03)]）这个游戏中的玩家是
    GAN 包含的两个网络。
- en: ³
  id: totrans-388
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-389
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-390
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: Recall from [chapter 1](../Text/kindle_split_010.xhtml#ch01) that GAN training
    ends when the two networks reach Nash equilibrium, a point in a game at which
    neither player can improve their situation by changing their strategy. Mathematically,
    this occurs when the Generator cost *J*^((*G*))(*θ*^((*G*)), *θ*^((*D*))) is minimized
    with respect to the Generator’s trainable parameters *θ*^((*G*)) and, simultaneously,
    the Discriminator cost *J*^((*D*))(*θ*^((*G*)), *θ*^((*D*))) is minimized with
    respect to the parameters under this network’s control, *θ*^((*D*)).^([[4](#ch03fn04)])
    [Figure 3.3](#ch03fig03) illustrates the setup of a two-player zero-sum game and
    the process of reaching Nash equilibrium.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下 [第 1 章](../Text/kindle_split_010.xhtml#ch01)，GAN 训练在两个网络达到纳什均衡时结束，这是游戏中一个点，此时任何玩家都不能通过改变策略来改善自己的情况。从数学上来说，这发生在生成器成本
    *J*^((*G*))(*θ*^((*G*)), *θ*^((*D*))) 在生成器的可训练参数 *θ*^((*G*)) 上最小化，同时，判别器成本 *J*^((*D*))(*θ*^((*G*)),
    *θ*^((*D*))) 在该网络控制的参数上最小化。（参考文献 [[4](#ch03fn04)]）[图 3.3](#ch03fig03) 展示了两人零和游戏的设置以及达到纳什均衡的过程。
- en: ⁴
  id: totrans-392
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-393
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-394
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: 'Figure 3.3\. Player 1 (left) seeks to minimize V by tuning θ[1]. Player 2 (middle)
    seeks to minimize –V (maximize V) by tuning θ[2]. The saddle-shaped mesh (right)
    shows the combined loss in the parameter space V(θ[1], θ[2]). The dotted line
    shows the convergence to Nash equilibrium at the center of the saddle. (Source:
    Goodfellow, 2019, [www.iangoodfellow.com/slides/2019-05-07.pdf](http://www.iangoodfellow.com/slides/2019-05-07.pdf).)'
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3。玩家 1（左侧）通过调整 θ[1] 来最小化 V。玩家 2（中间）通过调整 θ[2] 来最小化 –V（最大化 V）。鞍形网格（右侧）显示了参数空间
    V(θ[1], θ[2]) 中的总损失。虚线显示了收敛到鞍形中心纳什均衡的过程。（来源：Goodfellow，2019，[www.iangoodfellow.com/slides/2019-05-07.pdf](http://www.iangoodfellow.com/slides/2019-05-07.pdf)。）
- en: '![](../Images/03fig03_alt.jpg)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fig03_alt.jpg)'
- en: Coming back to our analogy, Nash equilibrium would occur when every route home
    takes exactly the same amount of time—for us and all other drivers we may encounter
    on the way. Any faster route would be offset by a proportional increase in traffic,
    slowing everyone down just the right amount. As you may imagine, this state is
    virtually unattainable in real life. Even with tools like Google Maps that provide
    real-time traffic updates, it is often impossible to perfectly evaluate the optimal
    path home.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的类比，当每条回家的路线所需时间完全相同——对我们以及我们可能遇到的任何其他司机来说——纳什均衡就会发生。任何更快的路线都会因为交通量的相应增加而被抵消，从而恰好减慢每个人的速度。正如你可能想象的那样，这种状态在现实生活中几乎无法实现。即使有像
    Google Maps 这样的工具提供实时交通更新，也往往无法完美地评估回家的最佳路线。
- en: The same is true in the high-dimensional, nonconvex world of training GANs.
    Even small 28 × 28-pixel grayscale images like the ones in the MNIST dataset have
    28 × 28 = 784 dimensions. If they were colored (RGB), their dimensionality would
    increase threefold, to 2,352\. Capturing this distribution across all images in
    the training dataset is extremely difficult, especially when the best approach
    to learn is from an adversary (the Discriminator).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 GANs 的高维非凸世界中，情况也是如此。即使是 MNIST 数据集中像那样的 28 × 28 像素的灰度图像也有 28 × 28 = 784
    维。如果它们是彩色的（RGB），它们的维度将增加三倍，达到 2,352。捕捉训练数据集中所有图像的这种分布极为困难，尤其是在最佳学习方法是从对手（判别器）那里学习的情况下。
- en: Training GANs successfully requires trial and error, and although there are
    best practices, it remains as much an art as it is a science. [Chapter 5](../Text/kindle_split_015.xhtml#ch05)
    revisits the topic of GAN convergence in more detail. For now, you can rest assured
    that the situation is not as bad as it may sound. As we previewed in [chapter
    1](../Text/kindle_split_010.xhtml#ch01), and as you will see throughout this book,
    neither the enormous complexities in approximating the generative distribution
    nor our lack of complete understanding of what conditions make GANs converge has
    impeded GANs’ practical usability and their ability to generate realistic data
    samples.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 成功训练 GAN 需要反复试验，尽管有最佳实践，但它仍然既是一门艺术，也是一门科学。[第 5 章](../Text/kindle_split_015.xhtml#ch05)
    更详细地回顾了 GAN 收敛性的问题。现在，你可以放心，情况并没有听起来那么糟糕。正如我们在 [第 1 章](../Text/kindle_split_010.xhtml#ch01)
    中预览的那样，以及你将在本书的其余部分看到的那样，近似生成分布的巨大复杂性以及我们对使 GAN 收敛的条件缺乏完全理解，都没有阻碍 GAN 的实际可用性和生成逼真数据样本的能力。
- en: 3.2\. The Generator and the Discriminator
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 生成器和判别器
- en: Let’s recap what you’ve learned by introducing more notation. The Generator
    (*G*) takes in a random noise vector *z* and produces a fake example *x**. Mathematically,
    *G*(*z*) = *x**. The Discriminator (*D*) is presented either with a real example
    *x* or with a fake example *x**; for each input, it outputs a value between 0
    and 1 indicating the probability that the input is real. [Figure 3.4](#ch03fig04)
    depicts the GAN architecture by using the terminology and notation we just presented.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过引入更多符号来回顾你所学的内容。生成器 (*G*) 接收一个随机噪声向量 *z* 并生成一个伪造示例 *x**。从数学上讲，*G*(*z*)
    = *x**。判别器 (*D*) 被提供真实示例 *x* 或伪造示例 *x**；对于每个输入，它输出一个介于 0 和 1 之间的值，表示输入是真实的概率。[图
    3.4](#ch03fig04) 使用我们刚刚提出的术语和符号描述了 GAN 架构。
- en: 'Figure 3.4\. The Generator network G transforms the random vector z into a
    fake example x*: G(z) = x*. The Discriminator network D outputs a classification
    of whether the input example is real. For the real examples x, the Discriminator
    strives to output values as close to 1 as possible. For the fake examples x*,
    the Discriminator strives to output values as close to 0 as possible. In contrast,
    the Generator wants D(x*) to be as close as possible to 1, indicating that the
    Discriminator was fooled into classifying a fake example as real.'
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.4\. 生成器网络 G 将随机向量 z 转换为伪造示例 x*：G(z) = x*。判别器网络 D 输出对输入示例是否为真实的分类。对于真实示例
    x，判别器努力输出尽可能接近 1 的值。对于伪造示例 x*，判别器努力输出尽可能接近 0 的值。相反，生成器希望 D(x*) 尽可能接近 1，这表明判别器被欺骗，将伪造示例分类为真实。
- en: '![](../Images/03fig04_alt.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fig04_alt.jpg)'
- en: 3.2.1\. Conflicting objectives
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 冲突目标
- en: The Discriminator’s goal is to be as accurate as possible. For the real examples
    *x*, *D*(*x*) seeks to be as close as possible to 1 (label for the positive class).
    For fake examples *x**, *D*(*x**) strives to be as close as possible to 0 (label
    for the negative class).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的目标是尽可能准确。对于真实示例 *x*，*D*(*x*) 努力使其尽可能接近 1（正类的标签）。对于伪造示例 *x**，*D*(*x**) 努力使其尽可能接近
    0（负类的标签）。
- en: The Generator’s goal is the opposite. It seeks to fool the Discriminator by
    producing fake examples *x** that are indistinguishable from the real data in
    the training dataset. Mathematically, the Generator strives to produce fake examples
    *x** such that *D*(*x**) is as close to 1 as possible.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的目标是相反的。它通过产生与训练数据集中的真实数据不可区分的伪造示例 *x** 来欺骗判别器。从数学上讲，生成器努力产生伪造示例 *x**，使得
    *D*(*x**) 尽可能接近 1。
- en: 3.2.2\. Confusion matrix
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 混淆矩阵
- en: 'The Discriminator’s classifications can be expressed in terms of a confusion
    matrix, a tabular representation of all the possible outcomes in binary classification.
    In the case of the Discriminator, these are as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的分类可以用混淆矩阵来表示，它是二进制分类中所有可能结果的表格表示。对于判别器，这些如下：
- en: '*True positive*—Real example correctly classified as real; *D*(*x*) ≈ 1'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阳性*—将真实示例正确分类为真实；*D*(*x*) ≈ 1'
- en: '*False negative*—Real example incorrectly classified as fake; *D*(*x*) ≈ 0'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阴性*—将真实示例错误地分类为伪造；*D*(*x*) ≈ 0'
- en: '*True negative*—Fake example correctly classified as fake; *D*(*x**) ≈ 0'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阴性*—将伪造示例正确分类为伪造；*D*(*x**) ≈ 0'
- en: '*False positive*—Fake example incorrectly classified as real; *D*(*x**) ≈ 1'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阳性*—将伪造示例错误地分类为真实；*D*(*x**) ≈ 1'
- en: '[Table 3.1](#ch03table01) presents these outcomes.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3.1](#ch03table01) 展示了这些结果。'
- en: Table 3.1\. Confusion matrix of Discriminator outcomes
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1. 判别器结果的混淆矩阵
- en: '| Input | Discriminator output |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 判别器输出 |'
- en: '| --- | --- |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *Close to 1 (real)* | *Close to 0 (fake)* |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| *接近1（真实）* | *接近0（假）* |'
- en: '| --- | --- |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ***Real (x)*** | True positive | False negative |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| ***真实 (x)*** | 真阳性 | 假阴性 |'
- en: '| ***Fake (x*)*** | False positive | True negative |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| ***假 (x*)*** | 假阳性 | 真阴性 |'
- en: Using the confusion matrix terminology, the Discriminator is trying to maximize
    true positive and true negative classifications or, equivalently, minimize false
    positive and false negative classifications. In contrast, the Generator’s goal
    is to maximize the Discriminator’s false positive classifications—these are the
    instances in which the Generator successfully fools the Discriminator into believing
    a fake example is real. The Generator is not concerned with how well the Discriminator
    classifies the real examples; it cares only about the Discriminator’s classifications
    of the fake data samples.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混淆矩阵术语，判别器试图最大化真阳性和真阴性分类，或者等价地，最小化假阳性和假阴性分类。相比之下，生成器的目标是最大化判别器的假阳性分类——这些是生成器成功欺骗判别器相信假例是真实例子的实例。生成器并不关心判别器对真实例子的分类效果如何；它只关心判别器对假数据样本的分类。
- en: 3.3\. GAN training algorithm
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. GAN训练算法
- en: Let’s revisit the GAN training algorithm from [chapter 1](../Text/kindle_split_010.xhtml#ch01)
    and formalize it by using the notation introduced in this chapter. Unlike the
    algorithm in [chapter 1](../Text/kindle_split_010.xhtml#ch01), this one uses mini-batches
    rather than one example at a time.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下第1章中介绍的GAN训练算法，并使用本章引入的符号对其进行形式化。与第1章中的算法不同，这个算法使用的是小批量而不是逐个例子。
- en: '|  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**GAN training algorithm**'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**GAN训练算法**'
- en: '*For* each training iteration *do*'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于*每个训练迭代*执行*'
- en: 'Train the Discriminator:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器：
- en: 'Take a random mini-batch of real examples: *x*.'
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个随机真实例子的批量：x。
- en: 'Take a mini-batch of random noise vectors *z* and generate a mini-batch of
    fake examples: *G*(*z*) = *x**.'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个随机噪声向量z的小批量，并生成一个假例的小批量：G(z) = x*。
- en: Compute the classification losses for *D*(*x*) and *D*(*x**), and backpropagate
    the total error to update *θ*^((*D*)) to *minimize* the classification loss.
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算D(x)和D(x*)的分类损失，并将总误差反向传播以更新θ^((D))以最小化分类损失。
- en: 'Train the Generator:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器：
- en: 'Take a mini-batch of random noise vectors *z* and generate a mini-batch of
    fake examples: *G*(*z*) = *x**.'
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个随机噪声向量z的小批量，并生成一个假例的小批量：G(z) = x*。
- en: Compute the classification loss for *D*(*x**), and backpropagate the loss to
    update *θ*^((*G*)) to *maximize* the classification loss.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算D(x*)的分类损失，并将损失反向传播以更新θ^((G))以最大化分类损失。
- en: '*End for*'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '*结束for*'
- en: '|  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Notice that in step 1, the Generator’s parameters are kept intact while we train
    the Discriminator. Similarly, in step 2, we keep the Discriminator’s parameters
    fixed while the Generator is trained. The reason we allow updates only to the
    weights and biases of the network being trained is to isolate all changes to only
    the parameters that are under the network’s control. This ensures that each network
    gets relevant signals about the updates to make, without interference from the
    other’s updates. You can almost think of it as two players taking turns.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在第1步中，我们在训练判别器的同时保持生成器的参数不变。同样，在第2步中，我们保持判别器的参数固定，同时训练生成器。我们只允许更新正在训练的网络的权重和偏置，是为了将所有变化仅限于网络控制的参数。这确保了每个网络都能获得关于要进行的更新的相关信号，而不会受到其他网络更新的干扰。你几乎可以把它想象成两个玩家轮流进行。
- en: Of course, you can imagine a scenario in which each player merely undoes the
    other’s progress, so not even a turn-based game is guaranteed to yield a useful
    outcome. (Have we said yet that GANs are notoriously tricky to train?) More on
    this in [chapter 5](../Text/kindle_split_015.xhtml#ch05), where we also discuss
    techniques to maximize our chances of success.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以想象一个场景，其中每个玩家只是抵消了对方的进展，因此即使是回合制游戏也不能保证产生有用的结果。（我们是否已经说过GAN的训练非常困难？）更多内容请见第5章，其中我们还将讨论提高成功机会的技术。
- en: That’s it for theory, for the time being. Let’s now put what we learned into
    practice and implement our first GAN.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 理论部分就到这里，暂时告一段落。现在让我们将所学应用到实践中，并实现我们的第一个GAN。
- en: '3.4\. Tutorial: Generating handwritten digits'
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. 指南：生成手写数字
- en: In this tutorial, we will implement a GAN that learns to produce realistic-looking
    handwritten digits. We will use the Python neural network library Keras with a
    TensorFlow backend. [Figure 3.5](#ch03fig05) shows a high-level architecture of
    the GAN we will implement.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将实现一个GAN，该GAN学习生成看起来逼真的手写数字。我们将使用Python神经网络库Keras和TensorFlow后端。![图3.5](../Images/03fig05_alt.jpg)
- en: 'Figure 3.5\. Over the course of the training iterations, the Generator learns
    to turn random noise input into images that look like members of the training
    data: the MNIST dataset of handwritten digits. Simultaneously, the Discriminator
    learns to distinguish the fake images produced by the Generator from the genuine
    ones coming from the training dataset.'
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5\. 在训练迭代过程中，生成器学习将随机噪声输入转换为看起来像训练数据成员的图像：手写数字的MNIST数据集。同时，判别器学习区分生成器产生的假图像和来自训练数据集的真实图像。
- en: '![](../Images/03fig05_alt.jpg)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fig05_alt.jpg)'
- en: Much of the code used in this tutorial—especially the boilerplate code used
    in the training loop—was adapted from the open source GitHub repository of GAN
    implementations in Keras, *Keras-GAN*, created by Erik Linder-Norén ([https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)).
    The repository also includes several advanced GAN variants, some of which will
    be covered later in this book. We revised and simplified the implementation considerably,
    in terms of both code and network architecture, and we renamed variables so that
    they are consistent with the notation used in this book.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中使用的很大一部分代码——特别是训练循环中使用的样板代码——是从Keras中开源的GAN实现GitHub仓库（*Keras-GAN*）改编的，由Erik
    Linder-Norén创建（[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)）。该仓库还包括几个高级GAN变体，其中一些将在本书的后续章节中介绍。我们对代码和网络架构进行了相当大的修订和简化，并重命名了变量，以便它们与本书中使用的符号一致。
- en: A Jupyter notebook with the full implementation, including added visualizations
    of the training progress, is available on the book’s website at [www.manning.com/books/gans-in-action](http://www.manning.com/books/gans-in-action)
    and in the GitHub repository for this book at [https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action)
    under the chapter-3 folder. The code was tested with Python 3.6.0, Keras 2.1.6,
    and TensorFlow 1.8.0.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含完整实现（包括添加的训练进度可视化）的Jupyter笔记本可在本书的网站上找到，网址为[www.manning.com/books/gans-in-action](http://www.manning.com/books/gans-in-action)，以及本书GitHub仓库的[https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action)下的第3章文件夹中。代码已在Python
    3.6.0、Keras 2.1.6和TensorFlow 1.8.0上进行了测试。
- en: 3.4.1\. Importing modules and specifying model input dimensions
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 导入模块和指定模型输入维度
- en: First, we import all the packages and libraries needed to run the model. Notice
    we also import the MNIST dataset of handwritten digits directly from `keras.datasets`.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入运行模型所需的所有包和库。注意我们直接从`keras.datasets`导入手写数字的MNIST数据集。
- en: Listing 3.1\. Import statements
  id: totrans-447
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 导入语句
- en: '[PRE9]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Second, we specify the input dimensions of our model and dataset. Each image
    in MNIST is 28 × 28 pixels with a single channel (because the images are grayscale).
    The variable `z_dim` sets the size of the noise vector, *z*.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们指定模型和数据集的输入维度。MNIST中的每张图像都是28 × 28像素的单通道（因为图像是灰度的）。变量`z_dim`设置噪声向量*z*的大小。
- en: Listing 3.2\. Model input dimensions
  id: totrans-450
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2\. 模型输入维度
- en: '[PRE10]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1* Input image dimensions**'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入图像维度**'
- en: '***2* Size of the noise vector, used as input to the Generator**'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 噪声向量的大小，用作生成器的输入**'
- en: Next, we implement the Generator and the Discriminator networks.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现生成器和判别器网络。
- en: 3.4.2\. Implementing the Generator
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 实现生成器
- en: For simplicity, the Generator is a neural network with only a single hidden
    layer. It takes in *z* as input and produces a 28 × 28 × 1 image. In the hidden
    layer, we use the *Leaky ReLU* activation function. Unlike a regular ReLU function,
    which maps any negative input to 0, Leaky ReLU allows a small positive gradient.
    This prevents gradients from dying out during training, which tends to yield better
    training outcomes.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，生成器是一个只有一个隐藏层的神经网络。它以*z*为输入，并生成一个28 × 28 × 1的图像。在隐藏层中，我们使用*Leaky ReLU*激活函数。与将任何负输入映射到0的常规ReLU函数不同，Leaky
    ReLU允许一个小的正梯度。这防止了在训练过程中梯度消失，这往往会产生更好的训练结果。
- en: At the output layer, we employ the *tanh* activation function, which scales
    the output values to the range [–1, 1]. The reason for using *tanh* (as opposed
    to, say, *sigmoid*, which would output values in the more typical 0 to 1 range)
    is that *tanh* tends to produce crisper images.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出层，我们使用*tanh*激活函数，将输出值缩放到[–1, 1]的范围。使用*tanh*（而不是，比如说，输出值在更典型的0到1范围内的*sigmoid*）的原因是*tanh*倾向于产生更清晰的图像。
- en: The following listing implements the Generator.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表实现了生成器。
- en: Listing 3.3\. Generator
  id: totrans-459
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3. 生成器
- en: '[PRE11]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***1* Fully connected layer**'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 全连接层**'
- en: '***2* Leaky ReLU activation**'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* Leaky ReLU激活**'
- en: '***3* Output layer with tanh activation**'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 带有tanh激活函数的输出层**'
- en: '***4* Reshapes the Generator output to image dimensions**'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将生成器输出重塑为图像维度**'
- en: 3.4.3\. Implementing the Discriminator
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3. 实现判别器
- en: The Discriminator takes in a 28 × 28 × 1 image and outputs a probability indicating
    whether the input is deemed real rather than fake. The Discriminator is represented
    by a two-layer neural network, with 128 hidden units and a *Leaky ReLU* activation
    function at the hidden layer.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器接收一个28 × 28 × 1的图像，并输出一个概率，表示输入被认为是真实还是虚假。判别器由一个两层神经网络表示，有128个隐藏单元，在隐藏层使用*Leaky
    ReLU*激活函数。
- en: For simplicity, our Discriminator network looks almost identical to the Generator.
    This does not have to be the case; indeed, in most GAN implementations, the Generator
    and Discriminator network architectures vary greatly in both size and complexity.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们的判别器网络看起来几乎与生成器相同。这并不一定如此；实际上，在大多数GAN实现中，生成器和判别器网络架构在大小和复杂性上都有很大的差异。
- en: Notice that unlike for the Generator, in the following listing we apply the
    *sigmoid* activation function at the Discriminator’s output layer. This ensures
    that our output value will be between 0 and 1, so it can be interpreted as the
    probability the Generator assigns that the input is real.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与生成器不同，在下面的列表中，我们在判别器的输出层应用了*sigmoid*激活函数。这确保了我们的输出值将在0到1之间，因此它可以被解释为生成器分配给输入为真实的概率。
- en: Listing 3.4\. Discriminator
  id: totrans-469
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.4. 判别器
- en: '[PRE12]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1* Flattens the input image**'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将输入图像展平**'
- en: '***2* Fully connected layer**'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 全连接层**'
- en: '***3* Leaky ReLU activation**'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* Leaky ReLU激活**'
- en: '***4* Output layer with sigmoid activation**'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 带有sigmoid激活函数的输出层**'
- en: 3.4.4\. Building the model
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4. 构建模型
- en: In [listing 3.5](#ch03ex05), we build and compile the Generator and Discriminator
    models implemented previously. Notice that in the combined model used to train
    the Generator, we keep the Discriminator parameters fixed by setting `discriminator.trainable`
    to `False`. Also note that the combined model, in which the Discriminator is set
    to untrainable, is used to train the Generator only. The Discriminator is trained
    as an independently compiled model. (This will become apparent when we review
    the training loop.)
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表3.5](#ch03ex05)中，我们构建并编译了之前实现的生成器和判别器模型。注意，在用于训练生成器的组合模型中，我们通过将`discriminator.trainable`设置为`False`来固定判别器的参数。此外，请注意，在判别器设置为不可训练的组合模型中，仅用于训练生成器。判别器作为一个独立编译的模型进行训练。（当我们回顾训练循环时，这一点将变得明显。）
- en: We use binary cross-entropy as the loss function we are seeking to minimize
    during training. *Binary cross-entropy* is a measure of the difference between
    computed probabilities and actual probabilities for predictions with only two
    possible classes. The greater the cross-entropy loss, the further away our predictions
    are from the true labels.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用二元交叉熵作为我们在训练期间寻求最小化的损失函数。*二元交叉熵*是计算概率与实际概率之间的差异的度量，对于只有两个可能类别的预测。交叉熵损失越大，我们的预测与真实标签的距离就越远。
- en: To optimize each network, we use the *Adam optimization algorithm*. This algorithm,
    whose name is derived from *adaptive moment estimation*, is an advanced gradient-descent-based
    optimizer. The inner workings of this algorithm are beyond the scope of this book,
    but it suffices to say that Adam has become the go-to optimizer for most GAN implementations
    thanks to its often superior performance.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化每个网络，我们使用*Adam优化算法*。这个算法的名字来源于*自适应矩估计*，是一个基于高级梯度下降的优化器。这个算法的内部工作原理超出了本书的范围，但可以简单地说，Adam由于其通常优越的性能，已经成为大多数GAN实现的首选优化器。
- en: Listing 3.5\. Building and compiling the GAN
  id: totrans-479
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5. 构建和编译GAN
- en: '[PRE13]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1* Combined Generator + Discriminator model**'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 结合生成器 + 判别器模型**'
- en: '***2* Builds and compiles the Discriminator**'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 构建和编译判别器**'
- en: '***3* Builds the Generator**'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 构建生成器**'
- en: '***4* Keeps Discriminator’s parameters constant for Generator training**'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在生成器训练期间保持判别器参数不变**'
- en: '***5* Builds and compiles GAN model with fixed Discriminator to train the Generator**'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 使用固定的判别器构建和编译GAN模型以训练生成器**'
- en: 3.4.5\. Training
  id: totrans-486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.5\. 训练
- en: The training code in [listing 3.6](#ch03ex06) implements the GAN training algorithm.
    We get a random mini-batch of MNIST images as real examples and generate a mini-batch
    of fake images from random noise vectors *z*. We then use those to train the Discriminator
    network while keeping the Generator’s parameters constant. Next, we generate a
    mini-batch of fake images and use those to train the Generator network while keeping
    the Discriminator’s parameters fixed. We repeat this for each iteration.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表3.6](#ch03ex06)中的训练代码实现了GAN训练算法。我们获取一个随机的MNIST图像小批量作为真实示例，并从随机噪声向量*z*生成一个假图像小批量。然后我们使用这些图像来训练判别器网络，同时保持生成器参数不变。接下来，我们生成一个假图像小批量，并使用这些图像来训练生成器网络，同时保持判别器参数固定。我们重复这个过程，直到每个迭代。'
- en: 'We use one-hot-encoded labels: 1 for real images and 0 for fake ones. To generate
    *z*, we sample from the standard normal distribution (a bell curve with 0 mean
    and a standard deviation of 1). The Discriminator is trained to assign *fake*
    labels to the fake images and *real* labels to real images. The Generator is trained
    such that the Discriminator assigns *real* labels to the fake examples it produces.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用one-hot编码的标签：1表示真实图像，0表示假图像。为了生成*z*，我们从标准正态分布（均值为0，标准差为1的钟形曲线）中采样。判别器被训练来将*假*标签分配给假图像，将*真实*标签分配给真实图像。生成器被训练，使得判别器将其产生的假示例分配为*真实*标签。
- en: Notice that we are rescaling the real images in the training dataset from –1
    to 1\. As you saw in the preceding example, the Generator uses the *tanh* activation
    function at the output layer, so the fake images will be in the range (–1, 1).
    Accordingly, we have to rescale all the Discriminator’s inputs to the same range.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在训练数据集中将真实图像从-1缩放到1。正如前一个示例中看到的，生成器在输出层使用*tanh*激活函数，因此假图像将在范围（-1，1）内。因此，我们必须将判别器所有输入的缩放范围调整为相同。
- en: Listing 3.6\. GAN training loop
  id: totrans-490
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.6\. GAN训练循环
- en: '[PRE14]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1* Loads the MNIST dataset**'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 加载MNIST数据集**'
- en: '***2* Rescales [0, 255] grayscale pixel values to [–1, 1]**'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将[0, 255]灰度像素值缩放到[–1, 1]**'
- en: '***3* Labels for real images: all 1s**'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 真实图像的标签：全部为1**'
- en: '***4* Labels for fake images: all 0s**'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 假图像的标签：全部为0**'
- en: '***5* Gets a random batch of real images**'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取一个随机批量的真实图像**'
- en: '***6* Generates a batch of fake images**'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 生成一批假图像**'
- en: '***7* Trains the Discriminator**'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 训练判别器**'
- en: '***8* Generates a batch of fake images**'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 生成一批假图像**'
- en: '***9* Trains the Generator**'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 训练生成器**'
- en: '***10* Saves losses and accuracies so they can be plotted after training**'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 保存损失和准确率，以便在训练后绘制**'
- en: '***11* Outputs training progress**'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 输出训练进度**'
- en: '***12* Outputs a sample of generated images**'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12* 输出生成的图像样本**'
- en: 3.4.6\. Outputting sample images
  id: totrans-504
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.6\. 输出样本图像
- en: In the Generator training code, you may notice an invocation of the `sample_images()function`.
    This function gets called every `sample_interval` iterations and outputs a 4 ×
    4 grid of images synthesized by the Generator in the given iteration. After we
    run our model, we will use these images to inspect interim and final outputs.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器训练代码中，你可能注意到对`sample_images()`函数的调用。这个函数在每个`sample_interval`迭代中被调用，并输出一个由生成器在给定迭代中合成的4
    × 4图像网格。运行我们的模型后，我们将使用这些图像来检查中间和最终输出。
- en: Listing 3.7\. Displaying generated images
  id: totrans-506
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.7\. 显示生成的图像
- en: '[PRE15]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* Sample random noise**'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 生成随机噪声样本**'
- en: '***2* Generates images from random noise**'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从随机噪声生成图像**'
- en: '***2* Rescales image pixel values to [0, 1]**'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将图像像素值缩放到[0, 1]**'
- en: '***4* Sets image grid**'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 设置图像网格**'
- en: '***5* Outputs a grid of images**'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 输出图像网格**'
- en: 3.4.7\. Running the model
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.7\. 运行模型
- en: That brings us to the final step, shown in [listing 3.8](#ch03ex08). We set
    the training hyperparameters—the number of iterations and the batch size—and train
    the model. There is no tried-and-true method to determine the right number of
    iterations or the right batch size; we determine them experimentally through trial
    and error as we observe the training progress.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带我们进入最后一步，如[列表3.8](#ch03ex08)所示。我们设置训练超参数——迭代次数和批量大小——并训练模型。没有一成不变的方法来确定正确的迭代次数或正确的批量大小；我们通过观察训练进度，通过试错实验来确定它们。
- en: 'That said, there are important practical constraints to these numbers: each
    mini-batch must be small enough to fit inside the processing memory (typical batch
    sizes people use are powers of 2: 32, 64, 128, 256, and 512). The number of iterations
    also has a practical constraint: the more iterations we have, the longer the training
    process takes. With complex deep learning models like GANs, this can get out of
    hand quickly, even with significant computing power.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然如此，这些数字有一些重要的实际限制：每个小批量必须足够小，以便适合处理内存（人们通常使用的批量大小是2的幂：32、64、128、256和512）。迭代次数也有实际限制：我们拥有的迭代次数越多，训练过程就越长。对于像GAN这样的复杂深度学习模型，这可能会迅速失控，即使有显著的计算能力。
- en: To determine the right number of iterations, we monitor the training loss and
    set the iteration number around the point when the loss plateaus, indicating that
    we are getting little to no incremental improvement from further training. (Because
    this is a generative model, overfitting is as much a concern as it is for supervised
    learning algorithms.)
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定合适的迭代次数，我们监控训练损失，并将迭代次数设置在损失平台期附近，这表明我们通过进一步训练获得的增量改进很小或没有。（因为这是一个生成模型，过拟合与监督学习算法一样是一个担忧。）
- en: Listing 3.8\. Running the model
  id: totrans-517
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8。运行模型
- en: '[PRE16]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1* Sets hyperparameters**'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 设置超参数**'
- en: '***2* Trains the GAN for the specified number of iterations**'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 训练GAN指定次数的迭代**'
- en: 3.4.8\. Inspecting the results
  id: totrans-521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.8。检查结果
- en: '[Figure 3.6](#ch03fig06) shows example images produced by the Generator over
    the course of training iterations, from earliest to latest.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.6](#ch03fig06)展示了生成器在训练迭代过程中产生的示例图像，从最早到最新。'
- en: 'Figure 3.6\. Starting from what looks to be no more than random noise, the
    Generator gradually learns to emulate the features of the training dataset: in
    our case, images of handwritten digits.'
  id: totrans-523
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6。从看起来只是随机噪声开始，生成器逐渐学会模拟训练数据集的特征：在我们的案例中，是手写数字的图像。
- en: '![](../Images/03fig06.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03fig06.jpg)'
- en: As you can see, the Generator starts out by producing little more than random
    noise. Over the course of the training iterations, it gets better and better at
    emulating the features of the training data. Each time the Discriminator rejects
    a generated image as false or accepts one as real, the Generator improves a little.
    [Figure 3.7](#ch03fig07) shows examples of images the Generator can synthesize
    after it is fully trained.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，生成器最初产生的只是随机噪声。在训练迭代过程中，它逐渐变得越来越好地模拟训练数据的特征。每当判别器拒绝一个生成的图像为假或接受一个图像为真时，生成器都会有所改进。[图3.7](#ch03fig07)展示了生成器在完全训练后可以合成的图像示例。
- en: Figure 3.7\. Although far from perfect, our simple two-layer Generator learned
    to produce realistic-looking numerals, such as 9 and 1.
  id: totrans-526
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7。尽管远非完美，我们简单的两层生成器学会了生成看起来逼真的数字，例如9和1。
- en: '![](../Images/03fig07.jpg)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03fig07.jpg)'
- en: For comparison, [figure 3.8](#ch03fig08) shows a randomly selected sample of
    real images from the MNIST dataset.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，[图3.8](#ch03fig08)显示了从MNIST数据集中随机选择的真实图像样本。
- en: Figure 3.8\. Example of real handwritten digits from the MNIST dataset used
    to train our GAN. Although the Generator made impressive progress toward emulating
    the training data, the difference between the numerals it produces and the real,
    human-written numerals remains clear.
  id: totrans-529
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8。用于训练我们的GAN的MNIST数据集中真实手写数字的示例。尽管生成器在模拟训练数据方面取得了显著的进步，但它产生的数字与真实人类书写的数字之间的差异仍然明显。
- en: '![](../Images/03fig08.jpg)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03fig08.jpg)'
- en: 3.5\. Conclusion
  id: totrans-531
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5。结论
- en: 'Although the images our GAN generated are far from perfect, many of them are
    easily recognizable as real numerals—an impressive achievement, given that we
    used only a simple two-layer network architecture for both the Generator and the
    Discriminator. In the following chapter, you will learn how to improve the quality
    of the generated images by using a more complex and powerful neural network architecture
    for the Generator and Discriminator: convolutional neural networks.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们GAN生成的图像远非完美，但其中许多很容易被识别为真实的数字——这是一个令人印象深刻的成就，考虑到我们只使用了简单的两层网络架构来构建生成器和判别器。在下一章中，您将学习如何通过为生成器和判别器使用更复杂、更强大的神经网络架构来提高生成图像的质量：卷积神经网络。
- en: Summary
  id: totrans-533
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'GANs consist of two networks: the Generator (*G*) and the Discriminator (*D*),
    each with its own loss function: *J*^((*G*))(*θ*^((*G*)), *θ*^((*D*))) and *J*^((*D*))(*θ*^((*G*)),
    *θ*^((*D*))), respectively.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN由两个网络组成：生成器（*G*）和判别器（*D*），每个网络都有自己的损失函数：*J*^((*G*))(*θ*^((*G*)), *θ*^((*D*)))
    和 *J*^((*D*))(*θ*^((*G*)), *θ*^((*D*)))，分别。
- en: 'During training, the Generator and the Discriminator can tune only their own
    parameters: *θ*^((*G*)) and *θ*^((*D*)), respectively.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，生成器和判别器只能调整自己的参数：*θ*^((*G*)) 和 *θ*^((*D*))，分别。
- en: The two GAN networks are trained simultaneously via a game-like dynamic. The
    Generator seeks to maximize the Discriminator’s false-positive classifications
    (classifying a generated image as real), while the Discriminator seeks to minimize
    its false-positive and false-negative classifications.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过类似游戏的动态同时训练两个GAN网络。生成器试图最大化判别器的假阳性分类（将生成的图像分类为真实），而判别器试图最小化其假阳性和假阴性分类。
- en: Chapter 4\. Deep Convolutional GAN
  id: totrans-537
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章 深度卷积生成对抗网络
- en: '*This chapter covers*'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding key concepts behind convolutional neural networks
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积神经网络背后的关键概念
- en: Using batch normalization
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批归一化
- en: Implementing Deep Convolutional GAN, an advanced GAN architecture
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现深度卷积生成对抗网络（Deep Convolutional GAN），一种高级GAN架构
- en: In the previous chapter, we implemented a GAN whose Generator and Discriminator
    were simple feed-forward neural networks with a single hidden layer. Despite this
    simplicity, many of the images of handwritten digits that the GAN’s Generator
    produced after being fully trained were remarkably convincing. Even the ones that
    were not recognizable as human-written numerals had many of the hallmarks of handwritten
    symbols, such as discernible line edges and shapes—especially when compared to
    the random noise used as the Generator’s raw input.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们实现了一个GAN，其生成器和判别器都是具有单个隐藏层的简单前馈神经网络。尽管这种架构很简单，但经过充分训练后，GAN的生成器产生的许多手写数字图像都令人信服。即使那些不能被识别为人类书写的数字，也具有许多手写符号的特征，如可辨别的线条边缘和形状——尤其是与用作生成器原始输入的随机噪声相比。
- en: 'Imagine what we could accomplish with more powerful network architecture. In
    this chapter, we will do just that: instead of simple two-layer feed-forward networks,
    both our Generator and Discriminator will be implemented as convolutional neural
    networks (CNNs, or ConvNets). The resulting GAN architecture is known as *Deep
    Convolutional GAN*, or *DCGAN* for short.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果我们使用更强大的网络架构能取得什么样的成果。在本章中，我们将做到这一点：我们的生成器和判别器都将实现为卷积神经网络（CNNs，或ConvNets），而不是简单的两层前馈网络。这种GAN架构被称为*深度卷积生成对抗网络*，或简称*DCGAN*。
- en: 'Before delving into the nitty-gritty of the DCGAN implementation, we will review
    the key concepts underlying ConvNets, review the history behind the discovery
    of the DCGAN, and cover one of the key breakthroughs that made complex architectures
    like DCGAN possible in practice: batch normalization.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨DCGAN实现细节之前，我们将回顾构成卷积神经网络（ConvNets）的关键概念，回顾DCGAN发现背后的历史，并介绍一个使DCGAN等复杂架构在实践上成为可能的关键突破：批归一化。
- en: 4.1\. Convolutional neural networks
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 卷积神经网络
- en: We expect that you’ve already been exposed to convolutional networks; that said,
    if this technique is new to you, don’t worry. In this section, we review all the
    key concepts you need for this chapter and the rest of this book.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计你已经接触过卷积网络；话虽如此，如果这项技术对你来说是新的，请不要担心。在本节中，我们将回顾本章以及本书其余部分所需的所有关键概念。
- en: 4.1.1\. Convolutional filters
  id: totrans-547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 卷积滤波器
- en: Unlike a regular feed-forward neural network whose neurons are arranged in flat,
    fully connected layers, layers in a ConvNet are arranged in three dimensions (width
    × height × depth). Convolutions are performed by sliding one or more *filters*
    over the input layer. Each filter has a relatively small receptive field (width
    × height) but always extends through the entire depth of the input volume.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经元排列在平坦、全连接层的常规前馈神经网络不同，ConvNet的层排列在三个维度（宽度×高度×深度）中。通过在输入层上滑动一个或多个*滤波器*来执行卷积。每个滤波器具有相对较小的感受野（宽度×高度），但总是延伸到整个输入体积的深度。
- en: 'At every step as it slides across the input, each filter outputs a single activation
    value: the dot product between the input values and the filter entries. This process
    results in a two-dimensional activation map for each filter. The activation maps
    produced by each filter are then stacked on top of one another to produce a three-dimensional
    output layer; the output depth is equal to the number of filters used.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 在滑动输入的每一步，每个过滤器输出一个激活值：输入值和过滤器条目之间的点积。这个过程为每个过滤器产生一个二维的激活图。每个过滤器产生的激活图随后堆叠在一起，形成一个三维的输出层；输出深度等于使用的过滤器数量。
- en: 4.1.2\. Parameter sharing
  id: totrans-550
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 参数共享
- en: Importantly, filter parameters are shared by all the input values to the given
    filter. This has both intuitive and practical advantages. Intuitively, parameter
    sharing allows us to efficiently learn visual features and shapes (such as lines
    and edges) regardless of where they are located in the input image. From a practical
    perspective, parameter sharing drastically reduces the number of trainable parameters.
    This decreases the risk of overfitting and allows this technique to scale up to
    higher-resolution images without a corresponding exponential increase in trainable
    parameters, as would be the case with a traditional, fully connected network.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，过滤器参数被给定过滤器的所有输入值共享。这既有直观的优势，也有实际的优势。直观上，参数共享使我们能够高效地学习视觉特征和形状（如线条和边缘），无论它们在输入图像中的位置如何。从实际角度来看，参数共享极大地减少了可训练参数的数量。这降低了过拟合的风险，并允许这种技术扩展到更高分辨率的图像，而无需相应地指数级增加可训练参数，正如传统全连接网络那样。
- en: 4.1.3\. ConvNets visualized
  id: totrans-552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 可视化的 ConvNets
- en: If all this sounds confusing, let’s make these concepts a little less abstract
    by visualizing them. Diagrams make everything easier to understand for most people
    (us included!). [Figure 4.1](#ch04fig01) shows a single convolution operation;
    [figure 4.2](#ch04fig02) illustrates the convolution operation in the context
    of the input and output layers in a ConvNet.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一切听起来很复杂，让我们通过可视化来使这些概念更具体一些。图表对大多数人（包括我们）来说都更容易理解！[图 4.1](#ch04fig01) 显示了一个单个卷积操作；[图
    4.2](#ch04fig02) 在 ConvNet 的输入和输出层上下文中说明了卷积操作。
- en: Figure 4.1\. A 3 × 3 convolutional filter as it slides over a 5 × 5 input—left
    to right, top to bottom. At each step, the filter moves by two strides; accordingly,
    it makes a total of four steps, resulting in a 2 × 2 activation map. Notice how
    at each step, the entire filter produces a single activation value.
  id: totrans-554
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.1\. 一个 3 × 3 卷积过滤器在 5 × 5 输入上滑动——从左到右，从上到下。在每一步，过滤器移动两个步长；因此，它总共移动了四步，产生一个
    2 × 2 的激活图。注意在每个步骤中，整个过滤器只产生一个激活值。
- en: '![](../Images/04fig01_alt.jpg)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04fig01_alt.jpg)'
- en: '(Source: “A Guide to Convolution Arithmetic for Deep Learning,” by Vincent
    Dumoulin and Francesco Visin, 2016, [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285).)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“深度学习卷积算术指南”，作者 Vincent Dumoulin 和 Francesco Visin，2016，[https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)。）
- en: '[Figure 4.1](#ch04fig01) depicts the convolution operation for a single filter
    over a two-dimensional input. In practice, the input volume is usually three-dimensional,
    and we use several stacked filters. The underlying mechanics, however, remain
    the same: each filter produces a single value per step, regardless of the depth
    of the input volume. The number of filters we use determines the depth of the
    output volume, as their resulting activation maps are stacked on top of one another.
    All this is illustrated in [figure 4.2](#ch04fig02).'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.1](#ch04fig01) 描述了单个过滤器在二维输入上的卷积操作。在实践中，输入体积通常是三维的，我们使用几个堆叠的过滤器。然而，基本原理保持不变：每个过滤器每一步只产生一个值，无论输入体积的深度如何。我们使用的过滤器数量决定了输出体积的深度，因为它们的激活图是堆叠在一起的。所有这些都在
    [图 4.2](#ch04fig02) 中得到了说明。'
- en: Figure 4.2\. An activation value for a single convolutional step within the
    context of the activation map (feature map) and the input and output volumes.
    Notice that the ConvNet filter extends through the full depth of the input volume
    and that the depth of the output volume is determined by stacking together activation
    maps.
  id: totrans-558
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.2\. 在激活图（特征图）和输入、输出体积的上下文中，单个卷积步骤的激活值。请注意，ConvNet 过滤器贯穿整个输入体积的深度，输出体积的深度由堆叠在一起的激活图决定。
- en: '![](../Images/04fig02_alt.jpg)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04fig02_alt.jpg)'
- en: '(Source: “Convolutional Neural Network,” by Nameer Hirschkind et al., Brilliant.org,
    retrieved November 1, 2018, [http://mng.bz/8zJK](http://mng.bz/8zJK).)'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：“卷积神经网络”，作者：Nameer Hirschkind 等，Brilliant.org，2018年11月1日检索，[http://mng.bz/8zJK](http://mng.bz/8zJK).)
- en: '|  |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-562
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: If you would like to dive deeper into convolutional networks and the underlying
    concepts, we recommend reading the relevant chapters in François Chollet’s *Deep
    Learning with Python* (Manning, 2017), which provides an outstanding, hands-on
    introduction to all the key concepts and techniques in deep learning, including
    ConvNets. For those with a more academic bent, a great resource is Andrej Karpathy’s
    excellent lecture notes from his Stanford University class on Convolutional Neural
    Networks for Visual Recognition ([http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)).
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解卷积网络及其背后的概念，我们建议阅读François Chollet的《Python深度学习》（Manning，2017年），该书提供了对深度学习所有关键概念和技术的卓越、实用的介绍，包括卷积网络。对于那些更倾向于学术研究的人来说，Andrej
    Karpathy在斯坦福大学关于视觉识别卷积神经网络的优秀讲义是一个很好的资源([http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/))).
- en: '|  |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 4.2\. Brief history of the DCGAN
  id: totrans-565
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. DCGAN的简要历史
- en: Introduced in 2016 by Alec Radford, Luke Metz, and Soumith Chintala, DCGAN marked
    one of the most important early innovations in GANs since the technique’s inception
    two years earlier.^([[1](#ch04fn01)]) This was not the first time a group of researchers
    tried harnessing ConvNets for use in GANs, but it was the first time they succeeded
    at incorporating ConvNets directly into a full-scale GAN model.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN由Alec Radford、Luke Metz和Soumith Chintala于2016年提出，是GAN技术自两年前诞生以来最重要的早期创新之一。^([[1](#ch04fn01)])
    这并不是第一次有研究团队尝试利用卷积神经网络（ConvNets）在GAN中的应用，但这是他们第一次成功地将ConvNets直接整合到全规模的GAN模型中。
- en: ¹
  id: totrans-567
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-568
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Unsupervised Representation Learning with Deep Convolutional Generative
    Adversarial Networks,” by Alec Radford et al., 2015, [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434).
  id: totrans-569
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见“Alec Radford等人，《无监督表示学习与深度卷积生成对抗网络》，2015年，[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434).”
- en: The use of ConvNets exacerbates many of the difficulties plaguing GAN training,
    including instability and gradient saturation. Indeed, these challenges proved
    so daunting that some researchers resorted to alternative approaches, such as
    the LAPGAN, which uses a cascade of convolutional networks within a Laplacian
    pyramid, with a separate ConvNet being trained at each level using the GAN framework.^([[2](#ch04fn02)])
    If none of this makes sense to you, don’t worry. Superseded by superior methods,
    LAPGAN has been largely relegated to the dustbin of history, so it is not important
    to understand its internals.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ConvNets加剧了困扰GAN训练的许多困难，包括不稳定性和梯度饱和。事实上，这些挑战如此艰巨，以至于一些研究人员求助于替代方法，如LAPGAN，它使用拉普拉斯金字塔内的级联卷积网络，并在每个级别上使用GAN框架单独训练一个ConvNet。^([[2](#ch04fn02)])
    如果这些内容对你来说难以理解，请不要担心。随着更优越方法的取代，LAPGAN已被很大程度上归入历史，因此了解其内部机制并不重要。
- en: ²
  id: totrans-571
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-572
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Deep Generative Image Models Using a Laplacian Pyramid of Adversarial Networks,”
    by Emily Denton et al., 2015, [https://arxiv.org/abs/1506.05751](https://arxiv.org/abs/1506.05751).
  id: totrans-573
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见“Emily Denton等人，《使用拉普拉斯金字塔的对抗网络进行深度生成图像模型》，2015年，[https://arxiv.org/abs/1506.05751](https://arxiv.org/abs/1506.05751).”
- en: Although inelegant, complex, and computationally taxing, LAPGAN yielded the
    highest-quality images to date at the time of its publication, with fourfold improvement
    over the original GAN (40% versus 10% of generated images mistaken for real by
    human evaluators). As such, LAPGAN demonstrated the enormous potential of marrying
    GANs with ConvNets.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LAPGAN在发布时复杂、计算量大，但它在当时提供了最高质量的图像，比原始GAN（40%的人认为生成的图像是真实的，而原始GAN只有10%）提高了四倍。因此，LAPGAN展示了将GAN与ConvNets结合的巨大潜力。
- en: With DCGAN, Radford and his collaborators introduced techniques and optimizations
    that allowed ConvNets to scale up to the full GAN framework without the need to
    modify the underlying GAN architecture and without reducing GAN to a subroutine
    of a more complex model framework, like LAPGAN. One of the key techniques Radford
    et al. used is batch normalization, which helps stabilize the training process
    by normalizing inputs at each layer where it is applied. Let’s take a closer look
    at what batch normalization is and how it works.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在DCGAN中，Radford及其合作者引入了技术和优化方法，使得卷积神经网络（ConvNets）能够扩展到完整的GAN框架，而无需修改底层GAN架构，也不需要将GAN简化为更复杂模型框架（如LAPGAN）的子例程。Radford等人使用的关键技术之一是批量归一化（batch
    normalization），它通过在每个应用层对输入进行归一化来帮助稳定训练过程。让我们更详细地了解一下批量归一化是什么以及它是如何工作的。
- en: 4.3\. Batch normalization
  id: totrans-576
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 批量归一化
- en: '*Batch normalization* was introduced by Google scientists Sergey Ioffe and
    Christian Szegedy in 2015.^([[3](#ch04fn03)]) Their insight was as simple as it
    was groundbreaking. Just as we normalize network inputs, they proposed to normalize
    the inputs to each layer, for each training mini-batch as it flows through the
    network.'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量归一化**是由Google科学家Sergey Ioffe和Christian Szegedy于2015年提出的。[^([[3](#ch04fn03)])他们的洞察既简单又具有开创性。正如我们归一化网络输入一样，他们提出了对每个层在每个训练小批量通过网络流动时进行归一化的建议。'
- en: ³
  id: totrans-578
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-579
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift,” by Sergey Ioffe and Christian Szegedy, 2015, [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  id: totrans-580
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参见Sergey Ioffe和Christian Szegedy于2015年发表的论文“Batch Normalization: Accelerating
    Deep Network Training by Reducing Internal Covariate Shift”，[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)。'
- en: 4.3.1\. Understanding normalization
  id: totrans-581
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 理解归一化
- en: 'It helps to remind ourselves what normalization is and why we bother normalizing
    the input feature values in the first place. *Normalization* is the scaling of
    data so that it has zero mean and unit variance. This is accomplished by taking
    each data point *x*, subtracting the mean *μ*, and dividing the result by the
    standard deviation, *σ*, as shown in [equation 4.1](#ch04equ01):'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 它有助于提醒我们归一化的概念以及为什么我们最初要归一化输入特征值。**归一化**是指对数据进行缩放，使其具有零均值和单位方差。这是通过取每个数据点 *x*，减去均值
    *μ*，然后将结果除以标准差 *σ* 来实现的，如[方程式4.1](#ch04equ01)所示：
- en: equation 4.1\.
  id: totrans-583
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: equation 4.1\.
- en: '![](../Images/04equ01.jpg)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04equ01.jpg)'
- en: 'Normalization has several advantages. Perhaps most important, it makes comparisons
    between features with vastly different scales easier and, by extension, makes
    the training process less sensitive to the scale of the features. Consider the
    following (rather contrived) example. Imagine we are trying to predict the monthly
    expenditures of a family based on two features: the family’s annual income and
    the family size. We would expect that, in general, the more a family earns, the
    more they spend; and the bigger a family is, the more they spend.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化具有几个优点。也许最重要的是，它使得具有不同比例的特征之间的比较变得更容易，并且通过扩展，使得训练过程对特征的比例不那么敏感。考虑以下（相当牵强的）例子。假设我们正在尝试根据两个特征来预测一个家庭的月支出：家庭的年收入和家庭规模。我们预计，一般来说，一个家庭赚得越多，他们花费的就越多；家庭规模越大，他们花费的就越多。
- en: However, the scales of these features are vastly different—an extra $10 in annual
    income probably wouldn’t influence how much a family spends, but an additional
    10 members would likely wreak havoc on any family’s budget. Normalization solves
    this problem by scaling each feature value onto a standardized scale, such that
    each data point is expressed not as its face value but as a relative “score” indicating
    how many standard deviations the given data point is from the mean.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些特征的比例差异很大——年收入额外增加10万可能不会影响一个家庭的花费，但额外增加10个家庭成员可能会对任何家庭预算造成破坏。归一化通过将每个特征值缩放到标准尺度上，使得每个数据点不是以其面值表示，而是作为一个相对“分数”，表示给定数据点与均值的多少个标准差。
- en: The insight behind batch normalization is that normalizing inputs alone may
    not go far enough when dealing with deep neural networks with many layers. As
    the input values flow through the network, from one layer to the next, they are
    scaled by the trainable parameters in each of those layers. And as the parameters
    get tuned by backpropagation, the distribution of each layer’s inputs is prone
    to change in subsequent training iterations, which destabilizes the learning process.
    In academia, this problem is known as *covariate shift*. Batch normalization solves
    it by scaling values in each mini-batch by the mean and variance of that mini-batch.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化的背后理念是，仅对输入进行归一化在处理具有许多层的深度神经网络时可能还不够。随着输入值通过网络流动，从一层到下一层，它们会被每一层中的可训练参数所缩放。当参数通过反向传播进行调整时，每一层输入的分布容易在后续训练迭代中发生变化，这会破坏学习过程。在学术界，这个问题被称为
    *协变量偏移*。批归一化通过按每个迷你批的均值和方差缩放每个迷你批的值来解决它。
- en: 4.3.2\. Computing batch normalization
  id: totrans-588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 批归一化计算
- en: The way batch normalization is computed differs in several respects from the
    simple normalization equation we presented earlier. This section walks through
    it step by step.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化的计算方式与我们在前面提出的简单归一化方程在几个方面有所不同。本节将逐步介绍它。
- en: 'Let *μ[B]* be the mean of the mini-batch *B*, and σ*[B]*² be the variance (mean
    squared deviation) of the mini-batch *B*. The normalized value ![](../Images/xbar.jpg)
    is computed as shown in [equation 4.2](#ch04equ02):'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *μ[B]* 为迷你批 *B* 的均值，σ*[B]*² 为迷你批 *B* 的方差（均方偏差）。标准化值 ![](../Images/xbar.jpg)
    的计算方法如下所示 [方程式 4.2](#ch04equ02)：
- en: equation 4.2\.
  id: totrans-591
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4.2\.
- en: '![](../Images/04equ02.jpg)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04equ02.jpg)'
- en: The term *ϵ* (epsilon) is added for numerical stability, primarily to avoid
    division by zero. It is set to a small positive constant value, such as 0.001.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 *ϵ*（epsilon）是为了数值稳定性而添加的，主要是为了避免除以零。它被设置为一个小正的常数值，例如 0.001。
- en: In batch normalization, we do not use these normalized values directly. Instead,
    we multiply them by *γ* (gamma) and add *β* (beta) before passing them as inputs
    to the next layer; see [equation 4.3](#ch04equ03).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 在批归一化中，我们不会直接使用这些标准化值。相反，我们在将它们作为输入传递给下一层之前，将它们乘以 *γ*（伽马）并加上 *β*（贝塔）；参见 [方程式
    4.3](#ch04equ03)。
- en: equation 4.3\.
  id: totrans-595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4.3\.
- en: '![](../Images/04equ03.jpg)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04equ03.jpg)'
- en: Importantly, the terms *γ* and *β* are trainable parameters, which—just like
    weights and biases—are tuned during network training. The reason for this is that
    it may be beneficial for the intermediate input values to be standardized around
    a mean other than 0 and have a variance other than 1\. Because *γ* and *β* are
    trainable, the network can learn what values work best.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，术语 *γ* 和 *β* 是可训练参数，它们就像权重和偏差一样，在网络训练过程中进行调整。这样做的原因是，中间输入值可能有益于围绕非零均值和具有非一方差进行标准化。因为
    *γ* 和 *β* 是可训练的，网络可以学习哪些值最有效。
- en: Fortunately for us, we don’t have to worry about any of this. The Keras function
    `keras.layers.BatchNormalization` handles all the mini-batch computations and
    updates behind the scenes for us.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不必担心这些。Keras 函数 `keras.layers.BatchNormalization` 会为我们处理所有后台的迷你批计算和更新。
- en: Batch normalization limits the amount by which updating the parameters in the
    previous layers can affect the distribution of inputs received by the current
    layer. This decreases any unwanted interdependence between parameters across layers,
    which helps speed up the network training process and increase its robustness,
    especially when it comes to network parameter initialization.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化限制了更新前一层参数对当前层接收到的输入分布的影响程度。这减少了层间参数之间的任何不希望有的相互依赖性，有助于加快网络训练过程并提高其鲁棒性，尤其是在网络参数初始化方面。
- en: Batch normalization has proven essential to the viability of many deep learning
    architectures, including the DCGAN, which you will see in action in the following
    tutorial.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化已被证明对于许多深度学习架构的可行性至关重要，包括你将在以下教程中看到的DCGAN。
- en: '4.4\. Tutorial: Generating handwritten digits with DCGAN'
  id: totrans-601
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 教程：使用DCGAN生成手写数字
- en: In this tutorial, we will revisit the MNIST dataset of handwritten digits from
    [chapter 3](../Text/kindle_split_012.xhtml#ch03). This time, however, we will
    use the DCGAN architecture and represent both the Generator and the Discriminator
    as convolutional networks, as shown in [figure 4.3](#ch04fig03). Besides this
    change, the rest of the network architecture remains unchanged. At the end of
    the tutorial, we will compare the quality of the handwritten numerals produced
    by the two GANs (traditional versus DCGAN) so you can see the improvement made
    possible by the use of a more advanced network architecture.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将回顾来自[第 3 章](../Text/kindle_split_012.xhtml#ch03)的手写数字 MNIST 数据集。然而，这一次，我们将使用
    DCGAN 架构，并将生成器和判别器都表示为卷积网络，如图 4.3 所示。[![图 4.3](../Images/04fig03_alt.jpg)](#ch04fig03)。除了这个变化之外，网络架构的其他部分保持不变。在教程结束时，我们将比较两个
    GAN（传统与 DCGAN）生成的手写数字的质量，以便您可以看到使用更先进的网络架构所能实现的改进。
- en: Figure 4.3\. The overall model architecture for this chapter’s tutorial is the
    same as the GAN we implemented in [chapter 3](../Text/kindle_split_012.xhtml#ch03).
    The only differences (not visible on this high-level diagram) are the internal
    representations of the Generator and Discriminator networks (the insides of the
    Generator and Discriminator boxes). These networks are covered in detail later
    in this tutorial.
  id: totrans-603
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3\. 本章教程的整体模型架构与我们在[第 3 章](../Text/kindle_split_012.xhtml#ch03)中实现的 GAN
    相同。唯一的区别（在此高级图表中不可见）是生成器和判别器网络的内部表示（生成器和判别器框的内部）。这些网络将在本教程的后面部分详细说明。
- en: '![](../Images/04fig03_alt.jpg)'
  id: totrans-604
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3](../Images/04fig03_alt.jpg)'
- en: As in [chapter 3](../Text/kindle_split_012.xhtml#ch03), much of the code in
    this tutorial was adapted from Erik Linder-Norén’s open source GitHub repository
    of GAN models in Keras ([https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)),
    with numerous modifications and improvements spanning both the implementation
    details and network architectures. A Jupyter notebook with the full implementation,
    including added visualizations of the training progress, is available in the GitHub
    repository for this book at [https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action),
    under the chapter-4 folder. The code was tested with Python 3.6.0, Keras 2.1.6,
    and TensorFlow 1.8.0\. To speed up the training time, it is recommended to run
    the model on a GPU.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[第 3 章](../Text/kindle_split_012.xhtml#ch03)中所述，本教程中的大部分代码都是从 Erik Linder-Norén
    的开源 GitHub 仓库中改编的，该仓库包含 Keras 中的 GAN 模型([https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN))，经过许多修改和改进，包括实现细节和网络架构。包含完整实现以及添加的训练进度可视化的
    Jupyter 笔记本可在本书的 GitHub 仓库[https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action)下的第
    4 章文件夹中找到。代码已在 Python 3.6.0、Keras 2.1.6 和 TensorFlow 1.8.0 上进行了测试。为了加快训练时间，建议在
    GPU 上运行模型。
- en: 4.4.1\. Importing modules and specifying model input dimensions
  id: totrans-606
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 导入模块和指定模型输入维度
- en: First, we import all the packages, modules, and libraries we need to train and
    run the model. Just as in [chapter 3](../Text/kindle_split_012.xhtml#ch03), the
    MNIST dataset of handwritten digits is imported directly from `keras.datasets`.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所有需要用于训练和运行模型的包、模块和库。正如[第 3 章](../Text/kindle_split_012.xhtml#ch03)中所述，MNIST
    手写数字数据集直接从 `keras.datasets` 导入。
- en: Listing 4.1\. Import statements
  id: totrans-608
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.1\. 导入语句
- en: '[PRE17]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We also specify the model input dimensions: the image shape and the length
    of the noise vector *z*.'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指定了模型输入维度：图像形状和噪声向量 *z* 的长度。
- en: Listing 4.2\. Model input dimensions
  id: totrans-611
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2\. 模型输入维度
- en: '[PRE18]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1* Input image dimensions**'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入图像维度**'
- en: '***2* Size of the noise vector, used as input to the Generator**'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 噪声向量的尺寸，用作生成器的输入**'
- en: 4.4.2\. Implementing the Generator
  id: totrans-615
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 实现生成器
- en: 'ConvNets have traditionally been used for image classification tasks, in which
    the network takes in an image with the dimensions *height* × *width* × *number
    of color channels* as input and—through a series of convolutional layers—outputs
    a single vector of class scores, with the dimensions 1 × *n*, where *n* is the
    number of class labels. To generate an image by using the ConvNet architecture,
    we reverse the process: instead of taking an image and processing it into a vector,
    we take a vector and up-size it to an image.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNets）传统上用于图像分类任务，其中网络接收一个具有*高度* × *宽度* × *颜色通道数*维度的图像作为输入，并通过一系列卷积层输出一个单维度的类别得分向量，其维度为1
    × *n*，其中*n*是类别标签的数量。要使用卷积神经网络架构生成图像，我们反转这个过程：不是将图像处理成向量，而是将向量上采样成图像。
- en: 'Key to this process is the *transposed convolution*. Recall that regular convolution
    is typically used to reduce input width and height while increasing its depth.
    Transposed convolution goes in the reverse direction: it is used to increase the
    width and height while reducing depth, as you can see in the Generator network
    diagram in [figure 4.4](#ch04fig04).'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的关键是*转置卷积*。回想一下，常规卷积通常用于在增加深度的同时减少输入的宽度和高度。转置卷积则相反：它用于在减少深度的同时增加宽度和高度，正如您可以在[图4.4](#ch04fig04)的生成器网络图中看到的。
- en: Figure 4.4\. The Generator takes in a random noise vector as input and produces
    a 28 × 28 × 1 image. It does so by multiple layers of transposed convolutions.
    Between the convolutional layers, we apply batch normalization to stabilize the
    training process. (Image is not to scale.)
  id: totrans-618
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4。生成器接收一个随机噪声向量作为输入，并生成一个28 × 28 × 1的图像。它是通过多层转置卷积来实现的。在卷积层之间，我们应用批量归一化以稳定训练过程。（图像未按比例缩放。）
- en: '![](../Images/04fig04_alt.jpg)'
  id: totrans-619
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04fig04_alt.jpg)'
- en: The Generator starts with a noise vector *z*. Using a fully connected layer,
    we reshape the vector into a three-dimensional hidden layer with a small base
    (width × height) and large depth. Using transposed convolutions, the input is
    progressively reshaped such that its base grows while its depth decreases until
    we reach the final layer with the shape of the image we are seeking to synthesize,
    28 × 28 × 1\. After each transposed convolution layer, we apply batch normalization
    and the *Leaky ReLU* activation function. At the final layer, we do not apply
    batch normalization and, instead of ReLU, we use the *tanh* activation function.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器从噪声向量*z*开始。使用全连接层，我们将向量重构成一个具有小基础（宽度×高度）和大深度的三维隐藏层。使用转置卷积，输入逐渐被重塑，使其基础增长而深度减少，直到我们达到最终层，其形状是我们想要合成的图像，28
    × 28 × 1。在每个转置卷积层之后，我们应用批量归一化和*Leaky ReLU*激活函数。在最终层，我们不应用批量归一化，并且，而不是ReLU，我们使用*tanh*激活函数。
- en: 'Putting all the steps together, we do the following:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有步骤组合起来，我们执行以下操作：
- en: Take a random noise vector and reshape it into a 7 × 7 × 256 tensor through
    a fully connected layer.
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个随机噪声向量通过全连接层重构成一个7 × 7 × 256张量。
- en: Use transposed convolution, transforming the 7 × 7 × 256 tensor into a 14 ×
    14 × 128 tensor.
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用转置卷积，将7 × 7 × 256张量转换为14 × 14 × 128张量。
- en: Apply batch normalization and the *Leaky ReLU* activation function.
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用批量归一化和*Leaky ReLU*激活函数。
- en: Use transposed convolution, transforming the 14 × 14 × 128 tensor into a 14
    × 14 × 64 tensor. Notice that the width and height dimensions remain unchanged;
    this is accomplished by setting the stride parameter in `Conv2DTranspose` to 1.
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用转置卷积，将14 × 14 × 128张量转换为14 × 14 × 64张量。请注意，宽度和高度维度保持不变；这是通过在`Conv2DTranspose`中将步长参数设置为1来实现的。
- en: Apply batch normalization and the *Leaky ReLU* activation function.
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用批量归一化和*Leaky ReLU*激活函数。
- en: Use transposed convolution, transforming the 14 × 14 × 64 tensor into the output
    image size, 28 × 28 × 1.
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用转置卷积，将14 × 14 × 64张量转换为输出图像大小，28 × 28 × 1。
- en: Apply the *tanh* activation function.
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用*tanh*激活函数。
- en: The following listing shows what the Generator network looks like when implemented
    in Keras.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了在Keras中实现时生成器网络的外观。
- en: Listing 4.3\. DCGAN Generator
  id: totrans-630
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.3。DCGAN生成器
- en: '[PRE19]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1* Reshapes input into 7 × 7 × 256 tensor via a fully connected layer**'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 通过全连接层将输入重构成7 × 7 × 256张量**'
- en: '***2* Transposed convolution layer, from 7 × 7 × 256 into 14 × 14 × 128 tensor**'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 转置卷积层，从7 × 7 × 256张量转换为14 × 14 × 128张量**'
- en: '***2* Batch normalization**'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 批量归一化**'
- en: '***4* Leaky ReLU activation**'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* Leaky ReLU激活**'
- en: '***5* Transposed convolution layer, from 14 × 14 × 128 to 14 × 14 × 64 tensor**'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5* 转置卷积层，从 14 × 14 × 128 转换为 14 × 14 × 64 张量**'
- en: '***6* Transposed convolution layer, from 14 × 14 × 64 to 28 × 28 × 1 tensor**'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6* 转置卷积层，从 14 × 14 × 64 转换为 28 × 28 × 1 张量**'
- en: '***7* Output layer with tanh activation**'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**7* 输出层使用 tanh 激活**'
- en: 4.4.3\. Implementing the Discriminator
  id: totrans-639
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 实现判别器
- en: 'The Discriminator is a ConvNet of the familiar kind, one that takes in an image
    and outputs a prediction vector: in this case, a binary classification indicating
    whether the input image was deemed to be real rather than fake. [Figure 4.5](#ch04fig05)
    depicts the Discriminator network we will implement.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器是一个熟悉的卷积神经网络，它接收一个图像并输出一个预测向量：在这种情况下，是一个二元分类，指示输入图像被认为是真实的而不是伪造的。[图 4.5](#ch04fig05)
    展示了我们将要实现的判别器网络。
- en: Figure 4.5\. The Discriminator takes in a 28 × 28 × 1 image as input, applies
    several convolutional layers, and—using the *sigmoid* activation function σ—outputs
    a probability that the input image is real rather than fake. Between the convolutional
    layers, we apply batch normalization to stabilize the training process. (Image
    is not to scale.)
  id: totrans-641
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.5\. 判别器接收一个 28 × 28 × 1 的图像作为输入，应用多个卷积层，并使用 *sigmoid* 激活函数 σ 输出一个概率，表明输入图像是真实的而不是伪造的。在卷积层之间，我们应用批标准化以稳定训练过程。（图像未按比例缩放。）
- en: '![](../Images/04fig05_alt.jpg)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04fig05_alt.jpg)'
- en: The input to the Discriminator is a 28 × 28 × 1 image. By applying convolutions,
    the image is transformed such that its base (width × height) gets progressively
    smaller and its depth gets progressively deeper. On all convolutional layers,
    we apply the *Leaky ReLU* activation function. Batch normalization is used on
    all convolutional layers except the first. For output, we use a fully connected
    layer and the *sigmoid* activation function.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的输入是一个 28 × 28 × 1 的图像。通过应用卷积，图像被转换，使其基础（宽度 × 高度）逐渐减小，其深度逐渐增加。在所有卷积层上，我们应用
    *Leaky ReLU* 激活函数。批标准化用于所有卷积层，除了第一层。对于输出，我们使用全连接层和 *sigmoid* 激活函数。
- en: 'Putting all the steps together, we do the following:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有步骤组合起来，我们执行以下操作：
- en: Use a convolutional layer to transform a 28 × 28 × 1 input image into a 14 ×
    14 × 32 tensor.
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用卷积层将一个 28 × 28 × 1 的输入图像转换为 14 × 14 × 32 张量。
- en: Apply the *Leaky ReLU* activation function.
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 *Leaky ReLU* 激活函数。
- en: Use a convolutional layer, transforming the 14 × 14 × 32 tensor into a 7 × 7
    × 64 tensor.
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用卷积层，将 14 × 14 × 32 张量转换为 7 × 7 × 64 张量。
- en: Apply batch normalization and the *Leaky ReLU* activation function.
  id: totrans-648
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用批标准化和 *Leaky ReLU* 激活函数。
- en: Use a convolutional layer, transforming the 7 × 7 × 64 tensor into a 3 × 3 ×
    128 tensor.
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用卷积层，将 7 × 7 × 64 张量转换为 3 × 3 × 128 张量。
- en: Apply batch normalization and the *Leaky ReLU* activation function.
  id: totrans-650
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用批标准化和 *Leaky ReLU* 激活函数。
- en: Flatten the 3 × 3 × 128 tensor into a vector of size 3 × 3 × 128 = 1152.
  id: totrans-651
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 3 × 3 × 128 张量展平为大小为 3 × 3 × 128 = 1152 的向量。
- en: Use a fully connected layer feeding into the *sigmoid* activation function to
    compute the probability of whether the input image is real.
  id: totrans-652
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个全连接层，并输入到 *sigmoid* 激活函数，以计算输入图像是否为真实的概率。
- en: The following listing is a Keras implementation of the Discriminator model.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表是判别器模型的 Keras 实现。
- en: Listing 4.4\. DCGAN Discriminator
  id: totrans-654
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.4\. DCGAN 判别器
- en: '[PRE20]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1* Convolutional layer, from 28 × 28 × 1 into 14 × 14 × 32 tensor**'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 卷积层，从 28 × 28 × 1 转换为 14 × 14 × 32 张量**'
- en: '***2* Leaky ReLU activation**'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* Leaky ReLU 激活**'
- en: '***3* Convolutional layer, from 14 × 14 × 32 into 7 × 7 × 64 tensor**'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3* 卷积层，从 14 × 14 × 32 转换为 7 × 7 × 64 张量**'
- en: '***4* Batch normalization**'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4* 批标准化**'
- en: '***5* Leaky ReLU activation**'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5* Leaky ReLU 激活**'
- en: '***6* Convolutional layer, from 7 × 7 × 64 tensor into 3 × 3 × 128 tensor**'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6* 卷积层，从 7 × 7 × 64 张量转换为 3 × 3 × 128 张量**'
- en: '***7* Batch normalization**'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**7* 批标准化**'
- en: '***8* Leaky ReLU activation**'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8* Leaky ReLU 激活**'
- en: '***9* Output layer with sigmoid activation**'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**9* 输出层使用 sigmoid 激活**'
- en: 4.4.4\. Building and running the DCGAN
  id: totrans-665
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4\. 构建 和 运行 DCGAN
- en: Aside from the network architectures used for the Generator and the Discriminator,
    the rest of the DCGAN network setup and implementation is the same as the one
    we used for the simple GAN in [chapter 3](../Text/kindle_split_012.xhtml#ch03).
    This underscores the versatility of the GAN architecture. [Listing 4.5](#ch04ex05)
    code builds the model, and [listing 4.6](#ch04ex06) trains the model.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于生成器和判别器的网络架构外，DCGAN 网络的其余设置和实现与我们在第 3 章（[chapter 3](../Text/kindle_split_012.xhtml#ch03)）中使用的简单
    GAN 相同。这突出了 GAN 架构的通用性。[列表 4.5](#ch04ex05) 代码构建模型，[列表 4.6](#ch04ex06) 训练模型。
- en: Listing 4.5\. Building and compiling the DCGAN
  id: totrans-667
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.5\. 构建 和 编译 DCGAN
- en: '[PRE21]'
  id: totrans-668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1* Combined Generator + Discriminator model**'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 结合生成器和判别器模型**'
- en: '***2* Builds and compiles the Discriminator**'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 构建 和 编译 判别器**'
- en: '***2* Builds the Generator**'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 构建 生成器**'
- en: '***4* Keeps Discriminator’s parameters constant for Generator training**'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在生成器训练期间保持判别器参数不变**'
- en: '***5* Builds and compiles GAN model with fixed Discriminator to train the Generator**'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 使用固定的判别器构建和编译 GAN 模型以训练生成器**'
- en: Listing 4.6\. DCGAN training loop
  id: totrans-674
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6\. DCGAN 训练循环
- en: '[PRE22]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1* Loads the MNIST dataset**'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 加载 MNIST 数据集**'
- en: '***2* Rescales [0, 255] grayscale pixel values to [–1, 1]**'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将 [0, 255] 灰度像素值缩放到 [–1, 1]**'
- en: '***3* Labels for real images: all 1s**'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 真实图像的标签：全部为 1**'
- en: '***4* Labels for fake images: all 0s**'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 假图像的标签：全部为 0**'
- en: '***5* Gets a random batch of real images**'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取一批随机真实图像**'
- en: '***6* Generates a batch of fake images**'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 生成一批假图像**'
- en: '***7* Trains the Discriminator**'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 训练判别器**'
- en: '***8* Generates a batch of fake images**'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 生成一批假图像**'
- en: '***9* Trains the Generator**'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 训练生成器**'
- en: '***10* Saves losses and accuracies so they can be plotted after training**'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 保存损失和准确率，以便在训练后绘制**'
- en: '***11* Outputs training progress**'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 输出训练进度**'
- en: '***12* Outputs a sample generated image**'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12* 输出一个样本生成的图像**'
- en: For completeness, we are also including the `sample_images()` function in the
    following listing. Recall from [chapter 3](../Text/kindle_split_012.xhtml#ch03)
    that this function outputs a 4 × 4 grid of images synthesized by the Generator
    in a given training iteration.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们还在以下列表中包括了 `sample_images()` 函数。回想一下，在第 3 章（[chapter 3](../Text/kindle_split_012.xhtml#ch03)）中，此函数输出一个由生成器在给定训练迭代中合成的
    4 × 4 图像网格。
- en: Listing 4.7\. Displaying generated images
  id: totrans-689
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.7\. 显示生成的图像
- en: '[PRE23]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1* Sample random noise**'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 生成随机噪声样本**'
- en: '***2* Generates images from random noise**'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从随机噪声生成图像**'
- en: '***3* Rescales image pixel values to [0, 1]**'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将图像像素值缩放到 [0, 1]**'
- en: '***4* Sets image grid**'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 设置图像网格**'
- en: '***5* Outputs a grid of images**'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 输出一个图像网格**'
- en: Next, the following code is used to run the model.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，以下代码用于运行模型。
- en: Listing 4.8\. Running the model
  id: totrans-697
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.8\. 运行模型
- en: '[PRE24]'
  id: totrans-698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* Sets hyperparameters**'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 设置超参数**'
- en: '***2* Trains the DCGAN for the specified number of iterations**'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 训练 DCGAN 指定次数的迭代**'
- en: 4.4.5\. Model output
  id: totrans-701
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.5\. 模型输出
- en: '[Figure 4.6](#ch04fig06) shows a sample of handwritten digits produced by the
    Generator after the DCGAN is fully trained. For a side-by-side comparison, [figure
    4.7](#ch04fig07) shows a sample of digits produced by the GAN from [chapter 3](../Text/kindle_split_012.xhtml#ch03),
    and [figure 4.8](#ch04fig08) shows a sample of real handwritten numerals from
    the MNIST dataset.'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.6](#ch04fig06) 展示了 DCGAN 完全训练后生成器生成的手写数字样本。为了对比，[图 4.7](#ch04fig07) 展示了第
    3 章（[chapter 3](../Text/kindle_split_012.xhtml#ch03)）中 GAN 生成的数字样本，[图 4.8](#ch04fig08)
    展示了 MNIST 数据集中的真实手写数字样本。'
- en: Figure 4.6\. A sample of handwritten digits generated by a fully trained DCGAN
  id: totrans-703
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6\. 一个由完全训练的 DCGAN 生成的手写数字样本
- en: '![](../Images/04fig06.jpg)'
  id: totrans-704
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04fig06.jpg)'
- en: Figure 4.7\. A sample of handwritten digits generated by the GAN implemented
    in [chapter 3](../Text/kindle_split_012.xhtml#ch03)
  id: totrans-705
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.7\. 由第 3 章（[chapter 3](../Text/kindle_split_012.xhtml#ch03)）中实现的 GAN 生成的手写数字样本
- en: '![](../Images/04fig07.jpg)'
  id: totrans-706
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04fig07.jpg)'
- en: Figure 4.8\. A randomly generated grid of real handwritten digits from the MNIST
    dataset used to train our DCGAN. Unlike the images produced by the simple GAN
    we implemented in [chapter 3](../Text/kindle_split_012.xhtml#ch03), many of the
    handwritten digits produced by the fully trained DCGAN are essentially indistinguishable
    from the training data.
  id: totrans-707
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.8\. 从 MNIST 数据集中随机生成的真实手写数字网格，用于训练我们的 DCGAN。与我们在第 3 章（[chapter 3](../Text/kindle_split_012.xhtml#ch03)）中实现的简单
    GAN 生成的图像不同，许多由完全训练的 DCGAN 生成的手写数字基本上与训练数据无法区分。
- en: '![](../Images/04fig08.jpg)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04fig08.jpg)'
- en: As evidenced by the preceding figures, all the extra work we put into implementing
    DCGAN paid off handsomely. Many of the images of handwritten digits that the network
    produces after being fully trained are virtually indistinguishable from the ones
    written by a human hand.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所示，我们为实施DCGAN所投入的额外工作得到了丰厚的回报。网络在完全训练后产生的许多手写数字图像几乎与人类手写的无法区分。
- en: 4.5\. Conclusion
  id: totrans-710
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 结论
- en: DCGAN demonstrates the versatility of the GAN framework. In theory, the Discriminator
    and Generator can be represented by any differentiable function, even one as complex
    as a multilayer convolutional network. However, DCGAN also demonstrates that there
    are significant hurdles to making more complex implementations work in practice.
    Without breakthroughs such as batch normalization, DCGAN would fail to train properly.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN展示了GAN框架的通用性。从理论上讲，判别器和生成器可以表示为任何可微分的函数，甚至是一个多层卷积网络这样复杂的函数。然而，DCGAN也表明，在实际中实现更复杂的实现存在重大障碍。如果没有批标准化这样的突破，DCGAN将无法正确训练。
- en: In the following chapter, we will explore some of the theoretical and practical
    limitations that make GAN training so challenging as well as the approaches to
    overcome them.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一些理论上的和实践上的限制，这些限制使得GAN的训练变得如此具有挑战性，以及克服这些限制的方法。
- en: Summary
  id: totrans-713
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Convolutional neural networks (ConvNets) use one or more convolutional filters
    that slide over the input volume. At each step as it slides over the input, a
    filter uses a single set of parameters to produce a single activation value. Together,
    all the activation values from all the filters produce the output layer.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNets）使用一个或多个卷积滤波器在输入体积上滑动。在每次滑动输入的过程中，滤波器使用单一组参数来产生一个激活值。所有滤波器产生的激活值共同构成了输出层。
- en: Batch normalization is a method that reduces the covariate shift (variations
    in input value distributions between layers during training) in neural networks
    by normalizing the output of each layer before it is passed as input to the next
    layer.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化是一种通过在传递给下一层之前对每一层的输出进行归一化，以减少神经网络中协变量偏移（训练过程中输入值分布在不同层之间的变化）的方法。
- en: Deep Convolutional GAN (DCGAN) is a Generative Adversarial Network with convolutional
    neural networks as its Generator and Discriminator. This architecture achieves
    superior performance in image-processing tasks, including handwritten digit generation,
    which we implemented in a code tutorial.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积生成对抗网络（DCGAN）是一种生成对抗网络，其生成器和判别器均采用卷积神经网络。这种架构在图像处理任务中表现出色，包括手写数字生成，我们在代码教程中实现了这一功能。
