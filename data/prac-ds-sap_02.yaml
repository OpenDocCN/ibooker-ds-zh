- en: Chapter 2\. Data Science for SAP Professionals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 数据科学与SAP专业人员
- en: Note
  id: totrans-1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re a data scientist, you may not need much of the information in this
    chapter. We’re trying to get SAP professionals up to speed on things that you
    probably already know.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名数据科学家，你可能不需要本章节中的大部分信息。我们试图让SAP专业人员迅速掌握你可能已经了解的内容。
- en: As a SAP business analyst, Fred is always looking for process improvements.
    That’s his job, and he is good at it. He’s heard a lot of buzz about data science,
    but to him, it is just that...buzz. Data science is creating the self-driving
    car, beating world champions at Go, and translating languages. Fred works at a
    US manufacturer, and data science has no real relevance to him.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名SAP业务分析师，Fred始终在寻找流程改进的方法。那是他的工作，他做得很好。他听说过很多关于数据科学的信息，但对他来说，那只是...信息噪音。数据科学是创造自动驾驶汽车、击败围棋世界冠军和翻译语言。Fred在一家美国制造商工作，数据科学对他来说并没有真正的相关性。
- en: Or does it?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 或者呢？
- en: If Fred knew the basic concepts around data science, he would understand how
    it could be leveraged to provide business value. He recently worked with the product
    development team, which is looking to IT for help in streamlining their processes.
    They have lots of unorganized data. They present Fred with an idea, a dashboard
    to help them track their process. When Fred evaluates the project his first response
    is to put the data in a SQL database. Once there he can use a presentation tool
    like PowerBI to create a dashboard. It is a solution that everyone likes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Fred了解数据科学的基本概念，他就会明白如何利用它为业务提供价值。最近，他与产品开发团队合作，后者正在寻求IT的帮助来简化他们的流程。他们有大量未组织的数据。他们向Fred提出了一个想法，一个仪表板来帮助他们追踪他们的流程。当Fred评估项目时，他的第一个反应是将数据放入SQL数据库中。一旦放在那里，他可以使用像PowerBI这样的演示工具创建一个仪表板。这是一个每个人都喜欢的解决方案。
- en: Fred doesn’t know the basics of data science. There are features in this data
    that might help the company make better, data-driven decisions. If he knew the
    basic concepts of regression and clustering, he would see it. He would know that
    he could do more with this business data than the project team requested.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Fred并不了解数据科学的基础知识。这些数据中的特征可能帮助公司做出更好的、基于数据的决策。如果他了解回归和聚类的基本概念，他就会明白。他会知道他可以做更多，而不仅仅是项目团队要求的那些业务数据。
- en: 'Therein lies the point of this chapter. We’re not trying to create data scientists.
    We are trying to get business analysts to think a little like a data scientist;
    we’re trying to create *citizen data scientists*. These are business analysts
    and professionals who understand enough about data science to ask questions about
    how it can be applied to their data (in particular, useful to their *SAP* data).
    To do that, we need to introduce the fundamentals of data science, including the
    different types of learning models: machine learning and neural networks.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是本章的重点所在。我们不是要培养数据科学家。我们试图让业务分析师像数据科学家一样思考一点；我们试图创建*公民数据科学家*。这些是理解数据科学足够多的业务分析师和专业人士，他们能够提出关于如何将数据科学应用于他们的数据（特别是对他们的*SAP*数据有用的）的问题。为此，我们需要介绍数据科学的基础知识，包括不同类型的学习模型：机器学习和神经网络。
- en: What follows is a rabbit race through the subject that will leave you with,
    at the very least, enough information to think about business processes in a slightly
    different way...in a data science way. Ideally you can think about your projects
    and data and say to your data scientist or developer, “Maybe a classification
    algorithm like Naive Bayes might work on this.” Imagine the jaws that will drop
    to that response!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来将会对该主题进行一场快速浏览，至少会给你留下足够的信息，让你以稍微不同的方式思考业务流程...以数据科学的方式。理想情况下，你可以考虑你的项目和数据，并对你的数据科学家或开发者说：“也许朴素贝叶斯这样的分类算法在这个问题上可能有效。”想象一下这种回应将引起的震惊！
- en: This is a conceptual chapter that provides an overview of the main data science
    concepts, and as such we will not discuss tactical ideas such as exploratory data
    analysis (EDA) or data preparation. We’ve covered the topics we feel are most
    relevant, but one could easily argue that we left out things of importance, such
    as automated machine learning (autoML) and ensemble methods; however, we had to
    draw a line in the sand somewhere to keep this chapter manageable. Nonetheless,
    we will later take a look at tactical concepts such as EDA (discussed in [Chapter 4](ch04.html#exploratory_data_analysis_with_r)),
    so stay tuned.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个概念性的章节，提供了主要数据科学概念的概述，因此我们不会讨论诸如探索性数据分析（EDA）或数据准备等具体的想法。我们已经涵盖了我们认为最相关的主题，但可以轻松地争论我们忽略了重要的事物，例如自动化机器学习（autoML）和集成方法；然而，我们必须在某个地方划定界限，以便保持本章的可管理性。尽管如此，我们稍后将讨论诸如EDA（在[第4章](ch04.html#exploratory_data_analysis_with_r)中讨论）等具体概念，所以请继续关注。
- en: Machine Learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: The syntax in data science can be confusing and overlapping. Deep learning is
    a component of machine learning by definition, but we refer to deep learning as
    those models that use more complex neural networks. Deep learning requires more
    computing power, more time, and more data to be successful. Often, simpler machine
    learning models perform equally, and sometimes better. Don’t overlook them in
    the face of shiny and fancy neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的语法可能令人困惑且重叠。根据定义，深度学习是机器学习的一个组成部分，但我们指的深度学习是使用更复杂的神经网络模型。深度学习需要更多的计算能力、时间和数据才能取得成功。通常情况下，更简单的机器学习模型表现同样甚至更好。在光鲜和花哨的神经网络面前，不要忽视它们。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Most data scientists spend the majority of their time finding, cleaning, and
    organizing huge amounts of data. Some estimates say that [data scientists spend
    80% of their time](http://bit.ly/2NBXPTJ) on this unrewarding task. We have good
    news for the data scientist looking at SAP data. SAP is an ERP system. The millions
    of rows of business data are already in a relational database. While this does
    not end the need to do some cleaning and reorganizing, it does reduce that effort.
    We will show how to find and extract this data, but often there is very little
    cleaning or organizing needed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学家的时间主要用于查找、清理和组织大量数据。一些估计称，[数据科学家花费80%的时间](http://bit.ly/2NBXPTJ)在这个无趣的任务上。对于查看SAP数据的数据科学家，我们有好消息。SAP是一个ERP系统。数百万行的业务数据已经在关系数据库中。虽然这并没有结束进行一些清理和重新组织的必要性，但确实减少了这些工作的努力。我们将展示如何查找和提取这些数据，但通常情况下，几乎不需要进行清理或组织。
- en: 'Machine learning falls roughly into four categories:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习大致可以分为四类：
- en: Supervised
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Semi-supervised
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Reinforcement
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Tip
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Deep learning includes these categories as well. It is considered a subset of
    machine learning. For the purposes of this book, here we refer to machine learning
    and not the subset of deep learning. We will present deep learning a little later.
    There is a lot of overlap and confusion in the terminology. If you follow news
    about machine learning, you’ll see that no two people on Earth are using the same
    terminology in the same way—so don’t feel bad about getting confused.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习也包括这些类别。它被认为是机器学习的一个子集。对于本书的目的，我们在这里提到机器学习而不是深度学习的子集。我们稍后会介绍深度学习。术语中存在大量的重叠和混淆。如果你关注机器学习的新闻，你会发现地球上没有两个人使用相同的术语方式——所以不要因为感到困惑而感到难过。
- en: Supervised Machine Learning
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习机器学习
- en: Supervised machine learning is done on labeled data. It works well on classification,
    which is a method to classify or predict categorical labels for a set of data.
    In marketing, for instance, it may be determining the customer who will buy a
    product. Supervised machine learning also works well on prediction. Prediction
    is a method to determine a numerical value from a set of data. Using the same
    analogy as for classification, in marketing it may be used to try and determine
    how much a customer will spend. For example, the well-known Iris dataset includes
    information about the petal length, petal width, sepal length, and sepal width
    of 150 iris flowers, and identifies their species. Once we train a model against
    this data, it can accurately predict the species of a new iris flower, given its
    sepal and petal data. Let’s take a closer look at some of the different types
    of supervised machine learning models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是在有标记的数据上进行的。它在分类方面表现出色，这是一种为一组数据分类或预测分类标签的方法。例如，在营销中，可以确定购买产品的客户。监督学习还在预测方面表现良好。预测是从一组数据中确定数值的方法。以与分类相同的类比，例如在营销中，可以用来尝试确定客户可能花费多少。例如，著名的鸢尾花数据集包含有关150朵鸢尾花的花瓣长度、花瓣宽度、萼片长度和萼片宽度的信息，并确定它们的物种。一旦我们对这些数据进行模型训练，它就能准确预测一朵新鸢尾花的物种，只要给出其萼片和花瓣数据。让我们更详细地了解一些不同类型的监督学习模型。
- en: Linear regression
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear regression is an approach to modeling the relationship between a dependent
    variable and one or more explanatory variables. The relationship between a home’s
    value and its square footage is a good example ([Figure 2-1](#fig0201)). If you
    have several home values and their respective square footage you could surmise
    the value of an unknown home if you know its size. Granted, there’s more to a
    home’s value than that, but you get the point.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种建模因变量与一个或多个解释变量之间关系的方法。一个房屋价值与其面积的关系是一个很好的例子（[图 2-1](#fig0201)）。如果你有几个房屋的价值和它们各自的平方英尺，你可以推测出一个未知房屋的价值，只要知道它的大小。当然，一个房屋的价值不仅仅取决于这些因素，但你明白我的意思。
- en: '![](assets/pdss_0201.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0201.png)'
- en: Figure 2-1\. Linear regression of housing prices by square footage
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 房屋价格与平方英尺的线性回归
- en: Logistic regression
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression, like linear regression, uses the same basic formula. However,
    logistic regression is categorical while linear is continuous. Using the same
    home value example, linear regression would be used to determine the home value,
    whereas logistic regression could be used to determine if it would sell.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归与线性回归类似，使用相同的基本公式。然而，逻辑回归是分类的，而线性回归是连续的。以同样的房屋价值例子，线性回归用于确定房屋的价值，而逻辑回归可以用于确定是否会卖出。
- en: Decision trees
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are a type of model that simply asks questions and makes decisions.
    The nodes of the decision tree ask questions that lead to either other nodes,
    or to end nodes (leaves) which are classifications or predictions ([Figure 2-2](#fig0202)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种简单地提出问题并做出决策的模型。决策树的节点提出问题，导致要么到其他节点，要么到末端节点（叶子），这些末端节点是分类或预测（[图 2-2](#fig0202)）。
- en: '![](assets/pdss_0202.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0202.png)'
- en: Figure 2-2\. Decision tree for eating a cookie
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 吃饼干的决策树
- en: Random forest
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'Random forests are groups of decision trees that help solve one of the biggest
    problem of decision trees: overfitting ([Figure 2-3](#fig0203)). Overfitting a
    model means that it is very good at solving problems it knows, but when introduced
    to new data it will fall short. Think of it as training yourself to be a world-class
    Formula One driver—but never learning to park.^([1](ch02.html#idm46252721295272))'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一组决策树，帮助解决决策树的一个最大问题：过度拟合（[图 2-3](#fig0203)）。过度拟合模型意味着它非常擅长解决它已知的问题，但当引入新数据时，它会表现不佳。可以将其视为训练自己成为世界一流的一级方程式赛车手，但从未学会停车。^([1](ch02.html#idm46252721295272))
- en: '![](assets/pdss_0203.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0203.png)'
- en: Figure 2-3\. Random forest
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 随机森林
- en: Unsupervised Machine Learning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督机器学习
- en: Unsupervised machine learning, as you may have guessed, does not have labeled
    data. That is, you have a pile of data, but you do not know the output label.
    For example, you have a set of voting records with age, sex, income, occupation,
    and other features. What you do not know is how they relate. Let’s take a look
    at some of the different types of unsupervised machine learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的那样，无监督机器学习没有标记的数据。也就是说，你有一堆数据，但你不知道输出标签。例如，你有一组包含年龄、性别、收入、职业和其他特征的选民记录。你不知道的是它们之间的关系。让我们看一下一些不同类型的无监督机器学习。
- en: '*k*-means clustering'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '*k*-均值聚类'
- en: '*k*-means clustering takes data and groups it into a given set of points ([Figure 2-4](#fig0204)).
    An example would be to segment or cluster a group of customers into groups representing
    their buying frequency. One way it does this by grouping them with the nearest
    mean value. It also works on words if you use a non-Euclidean^([2](ch02.html#idm46252721278024))
    distance, such as Levenshtein. We will go more into this in [Chapter 7](ch07.html#clustering_and_segmentation_in_r).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值聚类将数据分组到给定的点集中（[图 2-4](#fig0204)）。一个例子是将一组客户分组或聚类到代表他们购买频率的组中。它通过将它们与最近的均值值进行分组来完成此操作。如果使用非欧几里德距离，如Levenshtein，也适用于单词。我们将在第7章中详细介绍这一点（[ch07.html#clustering_and_segmentation_in_r](ch07.html#clustering_and_segmentation_in_r)）。'
- en: '![](assets/pdss_0204.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0204.png)'
- en: Figure 2-4\. Clustering
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 聚类
- en: Naive Bayes
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Naive Bayes is not a single algorithm but a collection of classification algorithms
    within the *Bayes’ theorem* family ([Figure 2-5](#fig0205)). The common concept
    is that every feature of the data is classified as independent of every other
    feature. For example, a car has a hood, a trunk, wheels, and seats. Naive Bayes
    sees all of these as independent contributors to the probability the object is
    a car. Naive Bayes is extremely fast and is often the first classifier tried for
    machine learning tasks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯不是单一的算法，而是贝叶斯定理家族中的一系列分类算法（[图 2-5](#fig0205)）。其共同概念是数据的每个特征都被视为独立于其他特征。例如，一辆汽车有车盖、后备箱、轮子和座位。朴素贝叶斯将这些都视为独立的贡献因素，判断这个对象是一辆汽车的概率。朴素贝叶斯非常快速，通常是机器学习任务中首选的分类器。
- en: '![](assets/pdss_0205.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0205.png)'
- en: Figure 2-5\. Bayes’ theorem
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 贝叶斯定理
- en: 'Here are the terms of Bayes’ theorem, in plain language:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是贝叶斯定理的术语，用简单的语言来说：
- en: P(c | x)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: P(c | x)
- en: The probability the hypothesis (c) is true given the data (x).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设成立（c）的概率，给定数据（x）。
- en: P(x | c)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: P(x | c)
- en: The probability of the data (x) if the hypothesis (c) is true.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果假设成立（c），数据的概率（x）。
- en: P(c)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: P(c)
- en: The probability the hypothesis (c) is true regardless of the data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设成立的概率（c），无论数据如何。
- en: P(x)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: P(x)
- en: The probability of the data (x) regardless of the data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的概率（x），无论数据如何。
- en: This is a common explanation of Bayes; it’s found everywhere. However, it’s
    a bit tricky to understand so let’s simplify.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝叶斯的一个常见解释；它无处不在。然而，这有点难以理解，所以让我们简化一下。
- en: 'There is a very common and intuitive explanation of Bayes using breast cancer
    as an example. Consider this scenario: a patient goes to the doctor for a checkup
    and the results of a mammogram come back abnormal. What are the odds the patient
    has cancer? You might intuitively think that cancer must be present because of
    the test results, but applying Bayes to the situation shows something different.
    Let’s take a look.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个非常常见和直观的解释贝叶斯的例子，使用乳腺癌作为例子。考虑这种情况：患者去医生那里检查，乳房X线照片结果异常。那么患者患有癌症的可能性有多大？你可能直觉地认为癌症一定存在，因为测试结果，但将贝叶斯应用到这种情况显示了不同的结果。让我们来看一下。
- en: Consider these statistics:^([3](ch02.html#idm46252721257816))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这些统计数据:^([3](ch02.html#idm46252721257816))
- en: 1% of women age 40 who participate in routine screenings have breast cancer.
    99% do not.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参加例行筛查的40岁女性中，有1%患乳腺癌，99%没有。
- en: 80% of mammograms will detect cancer when present and 20% miss it.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当乳房癌存在时，80%的乳房X线照片将检测到癌症，而20%未能检测到。
- en: 9.5% of mammograms return a false positive; they detect cancer when it is not
    there. Meaning 89.5% do not detect cancer and it is not there (true negative).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9.5%的乳房X线照片返回假阳性；它们检测到了实际上不存在的癌症。这意味着89.5%未能检测到癌症，实际上也没有癌症（真阴性）。
- en: The probability of the event is the event divided by all possibilities.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件的概率是事件发生的次数除以所有可能性。
- en: '*P(c|x)* = .01 * .8 / (.99 * .095) + (.01 * .8) = .0776'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*P(c|x)* = .01 * .8 / (.99 * .095) + (.01 * .8) = .0776'
- en: Intuitively you hear that the mammogram is 80% accurate, so a positive result
    would mean you have an 80% chance of having cancer. But the truth is...you only
    have a 7.8% chance even if you get a positive result.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上听到乳腺X光检查的准确率为80%，因此如果结果呈阳性，则表示你有80%的患癌症的几率。但事实是……即使结果呈阳性，你只有7.8%的概率患癌。
- en: Hierarchical clustering
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层聚类
- en: Hierarchical clustering is a method of grouping results into a dendrogram, or
    tree ([Figure 2-6](#fig0206)). If it starts from many clusters and moves to one
    it is called *divisive*. If it starts from one cluster and moves to many clusters
    it is *agglomerative*. A divisive method partitions a given cluster by computing
    the greatest difference (or distance) between two of its features. An agglomerative
    method does the opposite. It computes the differences between all clusters and
    combines the two with the least common distances between their features. They
    both continue until they are either out of data or the dendrogram splits the predefined
    number of times. We will go into more detail in [Chapter 7](ch07.html#clustering_and_segmentation_in_r).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分层聚类是一种将结果分组成树状图或树状图的方法（[图 2-6](#fig0206)）。如果从许多集群开始并移动到一个集群，则称为*分裂*。如果从一个集群开始并移动到多个集群，则称为*聚合*。分裂方法通过计算其特征之间的最大差异（或距离）来分区给定的集群。聚合方法则相反。它计算所有集群之间的差异，并组合具有最少公共特征距离的两个集群。它们都会继续进行，直到数据用尽或树状图达到预定义的分裂次数。我们将在[第 7
    章](ch07.html#clustering_and_segmentation_in_r)中更详细地讨论。
- en: '![](assets/pdss_0206.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0206.png)'
- en: Figure 2-6\. Agglomerative and divisive hierarchical clustering
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 聚合和分割层次聚类
- en: Semi-Supervised Machine Learning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督机器学习
- en: Semi-supervised machine learning is a combination of supervised and unsupervised
    learning. In this scenario you have a lot of data but not all of it is labeled.
    Consider the scenario for fraud detection. Credit card companies and banks have
    huge amounts of transaction data, some of which has been properly labeled as fraudulent.
    However, they do not know of all the fraudulent transactions. Ideally, they would
    properly label all of the fraudulent transactions manually. However, this process
    is not practical and would take far too much time and effort. There exists a small
    set of labeled data and a very large set of unlabeled data. In semi-supervised
    learning one common technique is called *pseudo-labeling*. In this process the
    labeled data is modeled using traditional supervised learning methods. Once the
    model is built and tuned, the unlabeled data is fed into the model and *labeled*.
    Finally, the labeled data and the newly pseudo-labeled data is used to train the
    model again ([Figure 2-7](#fig0207)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督机器学习是监督学习和无监督学习的结合。在这种情况下，您有大量数据，但并非所有数据都已标记。考虑欺诈检测的情况。信用卡公司和银行拥有大量的交易数据，其中一些已正确标记为欺诈。然而，并不是所有的欺诈交易都被发现。理想情况下，他们会手动正确标记所有的欺诈交易。然而，这个过程是不切实际的，需要太多时间和精力。已有一小部分标记数据和非常大量的未标记数据。在半监督学习中，一个常见的技术称为*伪标记*。在这个过程中，使用传统的监督学习方法对标记数据进行建模。一旦模型建立并调整好，将未标记数据输入模型并进行*标记*。最后，使用已标记数据和新伪标记的数据再次训练模型（[图 2-7](#fig0207)）。
- en: '![](assets/pdss_0207.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0207.png)'
- en: Figure 2-7\. Pseudo-labeling for semi-supervised learning
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 用于半监督学习的伪标记
- en: Reinforcement Machine Learning
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化机器学习
- en: Reinforcement machine learning is when you train a model to make decisions based
    on trial and error. This model interacts with its environment by learning from
    past successes and failures. It then determines a course of action for the next
    attempt or iteration. It works on the premise of maximizing a reward. The most
    common example of this is training a machine to play a game. Let’s take a closer
    look at some of the different types of reinforcement learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 强化机器学习是指训练模型根据试错来做决策。该模型通过从过去的成功和失败中学习来与环境进行交互。然后，它确定下一次尝试或迭代的行动方案。它的工作原理是最大化奖励。最常见的例子是训练机器玩游戏。让我们更详细地看看几种不同类型的强化学习。
- en: Hidden Markov models
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: Hidden Markov models (HMMs) are a series of observable *emissions*. These are
    the results of a given state that a model passed through to make those emissions.
    This is a bit confusing so let us clarify. In a HMM you cannot directly observe
    the state, but you can observe the results of those states. You work in an office
    without windows and you cannot see the weather outside. You can see what people
    are wearing when they show up to the office. Say 75% of people are carrying umbrellas...you
    can surmise that it’s raining outside. HMMs are popular ways to identify sequences
    and time series. They do not look at the true state; rather, they look at the
    emissions from the true states. The simplest models assume that each observation
    is independent of the next. However, HMMs assume a relationship between the observations.
    As another example, a series of data is observed for weather. That data has features
    in it like barometric pressure, temperature, and day of the year. The corresponding
    *emission* data has the binary feature of “not cloudy” or “cloudy.” Observing
    many days in succession, the model predicts the state of the weather not only
    on today’s observable features, but on the previous days’ features. HMMs attempt
    to identify the most likely underlying unknown sequence to explain the observed
    sequence.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏马尔可夫模型（HMMs）是一系列可观察的*发射*。这些是模型通过的给定状态的结果，用来进行这些发射。这有点令人困惑，所以让我们澄清一下。在HMM中，你不能直接观察到状态，但你可以观察到那些状态的结果。你在一个没有窗户的办公室工作，看不到外面的天气。你可以看到人们来办公室时穿着什么。比如说75%的人带着雨伞...你可以推测外面正在下雨。HMMs是识别序列和时间序列的流行方法。它们不看真实的状态；相反，它们看真实状态的发射。最简单的模型假设每个观察是独立的。然而，HMMs假设观察之间存在关系。再举个例子，一系列数据被观察到了天气。这些数据中包含了气压、温度和一年中的日期等特征。相应的*发射*数据具有“不多云”或“多云”的二元特征。观察多天的连续性，该模型不仅预测今天的可观察特征的天气状态，还预测了前几天的特征的天气状态。HMMs试图识别最有可能的潜在未知序列，以解释观察到的序列。
- en: 'The concept is a bit tricky so let’s use another example. Say you’re wanting
    to use a HMM to determine if there is going to be an increase or decrease in the
    number of purchase orders placed at your company for widgets. SAP has a history
    of purchase order data with timestamps. It also has other *states* that might
    influence when widgets are purchased. There are sales orders, time of year (seasonality),
    warehouse inventory levels, and production orders. Each of these could be used
    by the HMM. Think of it in this way: “past behavior predicts future behavior.”'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念有点棘手，所以让我们用另一个例子来说明。比如你想用HMM来确定公司的订单是否增加或减少购买widgets。SAP有订单数据的历史记录和时间戳。它还有其他*状态*可能会影响widgets购买的时间。有销售订单、年度时间（季节性）、仓库库存水平和生产订单。每一个这些都可以被HMM使用。可以这样理解：“过去的行为预测未来的行为。”
- en: Q-learning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning is a value-based reinforcement learning algorithm. It is based on
    the *quality* of an action. Q-learning goes through steps where it learns to optimize
    its outcome ([Figure 2-8](#fig0208)). In a way, it builds a secret cheat sheet
    of how it should behave. In the example of game play, it takes an action, evaluates
    that action, updates its cheat sheet with whether it was good or not, and then
    tries again. It iterates on this incredibly fast.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning是一种基于价值的强化学习算法。它基于一个行动的*质量*。Q-learning通过学习优化其结果的步骤进行。在游戏玩法的例子中，它执行一个动作，评估该动作，更新其行为秘笈（cheat
    sheet）表明该动作的好坏，然后再次尝试。它以极快的速度迭代这个过程。
- en: '![](assets/pdss_0208.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0208.png)'
- en: Figure 2-8\. Q-learning steps
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-8\. Q-learning步骤
- en: A common illustration is to imagine a game where you are a dog and you must
    find the pile of bones. Every step you take costs one bone. If you run into that
    pesky cat you lose 10 bones and die ([Figure 2-9](#fig0209)). The goal is to maximize
    the number of bones.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的插图是想象一个游戏，你是一只狗，你必须找到骨头堆。每走一步花费一根骨头。如果你遇到那只讨厌的猫，你会失去10根骨头并且死亡（[Figure 2-9](#fig0209)）。目标是最大化骨头数量。
- en: '![](assets/pdss_0209.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0209.png)'
- en: Figure 2-9\. Q-learning dog optimizes for most bones
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-9\. Q-learning狗最大化优化骨头
- en: It may seem like a simple game to us, but a computer doesn’t know how to start.
    So first it goes down and gets two bones. Yaaaah! Man, that was a good move. It
    records that and takes a step to the right. Damn that cat...game over. It updates
    the cheat sheet with that information. Next time it takes a right step first,
    then another right, and then it only has the option of down. Yes—a motherlode
    of bones!! Remember there is a –1 bone price per step. The result is –1 +2 –1
    –1 +1 –1 –1 +10 = 8\. It logs the results and tries again. This time it takes
    a right because it knows there is a +1 there. It takes another right and then
    a down to hit the motherlode. The result is –1 +1 –1 –1 +10 = 8\. Both paths are
    equally as valuable, but if there is a bonus or limit on the number of steps option
    2 wins.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，这似乎是一个简单的游戏，但计算机不知道该如何开始。所以它首先下去拿了两根骨头。耶！哥们，这一步走得好。它记录下来并向右走了一步。该死的猫……游戏结束。它更新了作弊表格的信息。下次它先向右走一步，然后再向右走一步，然后只有向下的选项。是的——骨头的“金矿”！记住每走一步要付出-1个骨头的代价。结果是-1
    +2 -1 -1 +1 -1 -1 +10 = 8。它记录下结果并再试一次。这次它向右走，因为知道那里有+1。它再向右走一步，然后向下找到“金矿”。结果是-1
    +1 -1 -1 +10 = 8。两条路径同样有价值，但如果有奖励或步数限制，第二个选项胜出。
- en: You may be thinking, “Pretty cool, but how would this apply to anything but
    games?” Take the image of the bulldog finding the path to the motherlode. Now
    imagine it is a simple warehouse...expand it greatly ([Figure 2-10](#fig0210)).
    Reinforcement learning could reduce transit time for picking, packing, and stocking
    as well as optimizing space utilization. It is more complex, but fundamentally
    the same as the dog and bones game.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，“挺酷的，但这与游戏之外的应用有何关系？” 想象一下斗牛犬找到主要金矿的情景。现在把它想象成一个简单的仓库……大幅扩展它（[图 2-10](#fig0210)）。强化学习可以减少拣选、打包和库存的运输时间，同时优化空间利用率。虽然更复杂，但基本上与狗和骨头的游戏相同。
- en: '![](assets/pdss_0210.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0210.png)'
- en: Figure 2-10\. This warehouse is more complex than a dog finding a bone, but
    pathfinding through reinforcement learning works here, too
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 这个仓库比狗找骨头更复杂，但通过强化学习的路径规划同样适用于这里
- en: Neural Networks
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: Both of us authors have been programming for many years and have experienced
    some wonderful “wow” moments along the way. Greg learned to program using Basic
    on the Apple IIe. He had been programming for about a year before learning the
    PEEK, POKE, and CALL commands. The first time he used these evocations and ran
    his program, he sat back and thought, “Wow!”; he’s been programming in one form
    or another ever since. Greg and Paul both had that feeling when they wrote their
    first few deep learning programs. “Wow!” is all we could say.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们两个作者都从事编程多年，并在这过程中体验过一些美妙的“哇”时刻。格雷格通过Apple IIe上的Basic学会了编程。在学会使用PEEK、POKE和CALL命令之前，他已经编程了大约一年。第一次使用这些命令并运行他的程序时，他坐下来想：“哇！”；从那以后，他一直以某种形式进行编程。当格雷格和保罗编写他们的前几个深度学习程序时，他们都有这种感觉。“哇！”是我们能说的全部。
- en: Let’s talk about deep learning and what that term means.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈深度学习及其含义。
- en: Traditional programming follows a tale of straightforward, predefined logic.
    IF this THEN perform that action 10 times. It’s so powerful that we can simulate
    beautiful scenery and create games that transport us to magic, imaginary realms.
    But it makes tasks such as language translation near impossible. Imagine the program
    it would take to translate English to Korean. That program would need to have
    conditions for words, phrases, negations, syntax, vernacular, punctuation, and
    on and on, ad infinitum. Imagine nesting all that in linear logic. Along comes
    machine learning. Now you input a set of English texts and their translated Korean
    equal. You train the model by showing it the input and the expected output. The
    more data you have, the more you can train your model. Finally, you input a set
    of English texts that do not have a Korean translation and kazam! It performs
    the translation as it has learned.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 传统编程遵循简单直接的逻辑故事。如果这样，那么执行那个动作10次。它如此强大，我们可以模拟美丽的风景并创建让我们陶醉的游戏，带我们进入魔法般的、想象的境界。但这使得像语言翻译这样的任务几乎不可能。想象一下需要翻译英语到韩语的程序。这个程序需要考虑词语、短语、否定、语法、本土用语、标点符号等等，无穷尽的条件。想象将所有这些嵌套在线性逻辑中。机器学习出现了。现在你输入一组英文文本及其翻译成韩文的等价文本。你通过展示输入和期望输出来训练模型。你拥有的数据越多，你就能训练模型得越好。最后，你输入一组没有韩文翻译的英文文本，哇！它像学会了一样进行翻译。
- en: That is remarkable in itself, but it gets better. Google built a [deep learning
    algorithm](https://tcrn.ch/2Pcz2YJ) in 2016 that translated from English to Korean,
    Korean to English, Japanese to English, and English to Japanese. Pretty incredible
    by itself—but that’s not the amazing part. The network was able to translate from
    Japanese to Korean and Korean to Japanese [*without first translating through
    English*](https://tcrn.ch/2Lc0KAM). Let that sink in. What is happening in the
    network to allow for such a thing to happen? The network learned a *metalanguage*—a
    type of linguistic mapping that transcended simple one-to-one language translation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这本身就很了不起，但更好的是，谷歌在2016年构建了一个[深度学习算法](https://tcrn.ch/2Pcz2YJ)，能够从英语翻译成韩语、韩语翻译成英语、日语翻译成英语以及英语翻译成日语。这本身就非常了不起——但这不是令人惊奇的部分。该网络能够直接从日语翻译成韩语和韩语翻译成日语，[*而无需先经过英语*](https://tcrn.ch/2Lc0KAM)。想一想这是如何发生的？网络内部发生了什么，使得这种事情成为可能？该网络学会了一种*元语言*——一种超越简单一对一语言翻译的语言映射。
- en: When translating from Japanese to Korean one would expect the model to go through
    the English first (the curved lines); see [Figure 2-11](#fig0211). After all,
    the model was not trained to go from Japanese to Korean. However, the model did
    not do this. It went directly from Japanese to Korean (the dotted line). Amazing!
    Kind of spooky actually.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当从日语翻译成韩语时，人们期望模型首先经过英语（弯曲的线路）；见 [图2-11](#fig0211)。然而，模型并没有这样做。它直接从日语到韩语（虚线路）。令人惊讶！实际上有点令人毛骨悚然。
- en: '![](assets/pdss_0211.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0211.png)'
- en: Figure 2-11\. Google’s language translator
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11\. 谷歌的语言翻译器
- en: Google’s language translator is a neural network in action. Let’s take a look
    at a few basic neural network architectures. This is a gentle introduction to
    neural networks and deep learning. We hope it piques your curiosity enough for
    you to want to take a deeper dive. At its foundation, a neural network is a series
    of interconnected processing modules that work together to take inputs and solve
    for a given output. They are inspired by the way neurons and synapses in the brain
    process information. They have been instrumental in solving problems ranging from
    image classification^([4](ch02.html#idm46252724893688)) to language translation.
    We will go into more depth on this in [Chapter 9](ch09.html#natural_language_processing_with_the_goo).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的语言翻译器是一个正在运行的神经网络。让我们来看几种基本的神经网络架构。这是神经网络和深度学习的初步介绍。我们希望这能激发你的好奇心，让你有兴趣深入了解。神经网络的基础是一系列相互连接的处理模块，它们一起接收输入并求解给定的输出。它们受到大脑中神经元和突触处理信息的启发。它们在解决从图像分类^([4](ch02.html#idm46252724893688))到语言翻译等问题上发挥了重要作用。我们将在第9章中进一步深入探讨这一点
    [Chapter 9](ch09.html#natural_language_processing_with_the_goo)。
- en: 'There are three basic layers to a neural network:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有三个基本层：
- en: The input layer
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层
- en: This is where the data is input into the network.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据输入到网络的地方。
- en: The hidden layer(s)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层
- en: This layer performs basic computation and then transfers weights to the next
    layer. The next layer can be another hidden layer or the output layer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层执行基本计算，然后将权重传递到下一层。下一层可以是另一个隐藏层或输出层。
- en: The output layer
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层
- en: This is the end of the network and where the model outputs results.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是网络的结束，模型输出结果的地方。
- en: Neural networks have six foundational concepts, as described in the following
    sections.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有六个基本概念，如下节所述。
- en: Feed-forward propagation
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传播
- en: Data (weights and biases) flows forward through the network from the input layer
    through various hidden layers and finally to the output layer ([Figure 2-12](#fig0212)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据（权重和偏差）从输入层经过各个隐藏层向前流动，最终到达输出层（[图2-12](#fig0212)）。
- en: '![](assets/pdss_0212.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0212.png)'
- en: Figure 2-12\. Feed-forward propagation
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-12\. 前向传播
- en: Backward propagation
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: After data is fed forward through the network, the error (desired value minus
    the obtained value) is fed backward through the network to adjust the weights
    and biases with the aim of reducing the error ([Figure 2-13](#fig0213)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据经过网络前向传播后，误差（期望值减去获得值）通过网络反向传播以调整权重和偏差，旨在减少误差（[图2-13](#fig0213)）。
- en: '![](assets/pdss_0213.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0213.png)'
- en: Figure 2-13\. Backward propagation
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-13\. 反向传播
- en: />
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \/>
- en: Gradient descent
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: An optimization function that attempts to find the minimum value of a function.
    Another way of saying it is that gradient descent has the goal of minimizing the
    cost function as much as possible ([Figure 2-14](#fig0214)). When this is achieved,
    the network is optimized. A common analogy is a man walking down a mountain. Every
    step he takes he wants to head in a downward direction until he reaches the lowest
    possible point; it is here where the cost function is at a minimal. When this
    is achieved the model has the highest accuracy.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一种优化函数，尝试找到函数的最小值。另一种说法是，梯度下降的目标是尽可能地最小化成本函数（见[图 2-14](#fig0214)）。当这一目标达成时，网络被优化了。一个常见的类比是一个人走下山。他每走一步都希望朝向下降的方向前进，直到达到最低点；这就是成本函数最小化的地方。当这一目标达成时，模型的准确性最高。
- en: '![](assets/pdss_0214.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0214.png)'
- en: Figure 2-14\. Gradient descent
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-14\. 梯度下降
- en: Learning rate
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率（Learning rate）
- en: The learning rate is the size of the steps we take to achieve the minimum of
    gradient descent (bottom of the mountain). If the learning rate is too large,
    it will pass the minimum and potentially spin out of control. If it is too small,
    the process takes far too long ([Figure 2-15](#fig0215)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是我们为了实现梯度下降的最小值（山底）而采取的步骤大小。如果学习率太大，会超过最小值并潜在地失控。如果太小，过程会花费太长时间（见[图 2-15](#fig0215)）。
- en: '![](assets/pdss_0215.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0215.png)'
- en: Figure 2-15\. Learning rates
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-15\. 学习率
- en: Neuron
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元
- en: A neuron is the foundation of a neural network. It takes an input, or inputs,
    applies a function to those inputs and renders an output. It is loosely based
    on the human neuron ([Figure 2-16](#fig0216)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是神经网络的基础。它接收一个或多个输入，对这些输入应用一个函数，并生成一个输出。它在某种程度上基于人类神经元（见[图 2-16](#fig0216)）。
- en: '![](assets/pdss_0216.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0216.png)'
- en: Figure 2-16\. Neuron
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-16\. 神经元
- en: Functions
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数
- en: 'A function is a mathematical equation within a neuron that takes the input
    values and decides whether it should activate (or fire). There are many activation
    functions, but these are the are most common in neural networks:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 函数是神经元内的数学方程，它接收输入值并决定是否应该激活（或发出）。有许多激活函数，但这些是神经网络中最常见的：
- en: Sigmoid
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Takes the input value and puts it in a range from 0 to 1 ([Figure 2-17](#fig0217)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入值置于0到1的范围内（见[图 2-17](#fig0217)）。
- en: '![](assets/pdss_0217.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0217.png)'
- en: Figure 2-17\. Sigmoid
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-17\. Sigmoid
- en: Tanh
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh
- en: Takes the input value and puts it in the range of –1 to 1 ([Figure 2-18](#fig0218)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入值置于-1到1的范围内（见[图 2-18](#fig0218)）。
- en: '![](assets/pdss_0218.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0218.png)'
- en: Figure 2-18\. Tanh
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-18\. Tanh
- en: ReLU
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU
- en: Rectified Linear Unit takes the input value and puts it in the range of 0 to
    infinity. It makes all negative values 0 ([Figure 2-19](#fig0219)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元（Rectified Linear Unit）将输入值置于0到无穷大的范围内。它使所有负值为0（见[图 2-19](#fig0219)）。
- en: '![](assets/pdss_0219.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0219.png)'
- en: Figure 2-19\. Rectified Linear Unit
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-19\. 渗漏修正线性单元
- en: Leaky ReLU
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: Takes an input value and puts the range from a very small negative value to
    infinity ([Figure 2-20](#fig0220)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入值置于非常小的负值到无穷大的范围内（见[图 2-20](#fig0220)）。
- en: '![](assets/pdss_0220.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0220.png)'
- en: Figure 2-20\. Leaky Rectified Linear Unit
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-20\. 渗漏修正线性单元
- en: Softmax
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax
- en: Takes the inputs and predicts a result over a certain set of possibilities.
    For instance, in digit recognition the softmax function returns a result of 10
    possibilities (0-9) with probabilities for each. If you have five different sodas,
    it would return five possibilities with probabilities for each ([Figure 2-21](#fig0221)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接收输入并在一定的可能性集合上预测结果。例如，在数字识别中，Softmax 函数返回10个可能性（0-9），每个可能性都有概率。如果你有五种不同的汽水，它将返回五种可能性，每种可能性都有相应的概率（见[图
    2-21](#fig0221)）。
- en: '![](assets/pdss_0221.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0221.png)'
- en: Figure 2-21\. Softmax Function
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-21\. Softmax 函数
- en: Note
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: ReLUs have the problem of “dying”—getting stuck on the negative side and always
    outputting a value of 0\. Using Leaky ReLUs with their slight negative slope can
    remedy the problem, as can lowering the learning rate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数存在“死亡”的问题——它会陷入负值侧并始终输出值为0。使用具有轻微负斜率的渗漏修正线性单元或降低学习率可以解决这个问题。
- en: As business analysts we recommend taking a high-level view of machine learning
    and, in particular, neural networks. You can go down many rabbit holes here trying
    to understand the exact difference between sigmoid or tanh, or how exactly to
    determine gradient descent. You can dig into the math of this to such an extent
    you could write many doctoral theses on it. Our goal with this overview is to
    impart to SAP business analysts the sheer depth of this beautiful science. Furthermore,
    a basic understanding of this science will allow you to leverage it for real-world
    results.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 作为业务分析师，我们建议对机器学习，特别是神经网络，采取高层次的视角。在这里，你可以深入研究S型或双曲正切函数的确切差异，或者如何确定梯度下降的确切方法。你可以深入数学领域，甚至可以撰写许多博士论文。我们在这个概述中的目标是向SAP业务分析师传达这门美妙科学的深度。此外，对这门科学的基本理解将使你能够利用它获得实际结果。
- en: Now that we have some of the fundamentals, what are some of the basic neural
    networks we see in practice today?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们掌握了一些基本概念，那么今天我们实际上看到的一些基本神经网络是什么呢？
- en: Single layer perceptron
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单层感知器
- en: 'A *Single layer perceptron* is the simplest form of a neural network ([Figure 2-22](#fig0222)).
    It has no hidden layers. It has only an input and output layer. You might think
    that diagram has two layers, but the input layer is not considered a layer because
    it does no computation. A single layer perceptron receives multiple input signals,
    sums them, and if the value is above a predetermined threshold it fires. Because
    they either have a value or not, they are only capable of discerning between two
    linearly separable classes. What’s the big deal? In themselves, the single layer
    perceptron is quite limited. However, they comprise other neural networks. Imagine:
    the average human brain has 100 billion neurons. Each neuron has a simple function,
    as simple as this single layer perceptron. It is the concert of these neurons
    in our brains that makes the music of who we are.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*单层感知器* 是神经网络的最简单形式（[图 2-22](#fig0222)）。它没有隐藏层。它只有一个输入和输出层。你可能认为图表有两层，但是输入层不被认为是一层，因为它不进行计算。单层感知器接收多个输入信号，对它们求和，如果值高于预定阈值就发射。因为它们只有值或没有，它们只能区分两个线性可分的类别。这有什么了不起的？单层感知器本身相当有限。然而，它们构成其他神经网络。想象一下：普通人的大脑有1000亿个神经元。每个神经元的功能很简单，就像这个单层感知器一样。正是我们大脑中这些神经元的合奏构成了我们的音乐。'
- en: '![](assets/pdss_0222.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0222.png)'
- en: Figure 2-22\. Single layer perceptron
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-22\. 单层感知器
- en: Multilayer perceptron
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: A *Multilayer perceptron* is composed of multiple layers ([Figure 2-23](#fig0223)).
    They are normally interconnected. Nodes in the first hidden layer connect to the
    nodes in the input layer. A bias node can be added in the hidden layer that is
    not connected to the input layer. Bias nodes increase flexibility of the network
    to fit the data and their value is normally set to 1\. In more advanced neural
    networks the process of *batch normalization* performs this function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*多层感知器* 由多个层组成（[图 2-23](#fig0223)）。它们通常是互相连接的。第一个隐藏层中的节点连接到输入层中的节点。偏置节点可以添加在不连接输入层的隐藏层中。偏置节点增加了网络适应数据的灵活性，它们的值通常设为1。在更高级的神经网络中，*批量归一化*
    进行这个功能。'
- en: '![](assets/pdss_0223.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0223.png)'
- en: Figure 2-23\. Multilayer perceptron
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-23\. 多层感知器
- en: Convolutional network
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积网络
- en: A *convolutional neural network* (CNN) is a multilayer network that passes weights
    and biases back and forth through the layers. CNNs assume that the inputs are
    images and therefore there are special layers and encoding to these networks.
    Why not use a multilayer perceptron for image classification? Well, image data
    is big...it would not scale well. CNNs use three-dimensional tensors composed
    of width, height, and depth as their input.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络*（CNN）是通过层间传递权重和偏置的多层网络。CNN假设输入是图像，因此这些网络有特殊的层和编码。为什么不用多层感知器进行图像分类呢？嗯，图像数据很大……它不会很好地扩展。CNN使用三维张量作为它们的输入，包括宽度、高度和深度。'
- en: '![](assets/pdss_0224.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0224.png)'
- en: Figure 2-24\. Convolutional neural network layers
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-24\. 卷积神经网络层
- en: 'There are three unique layers to a CNN:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: CNN有三个独特的层：
- en: Convolutional Layer
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层
- en: The primary purpose is to extract features from the input. Every image is a
    matrix of pixel values, which are converted to features using a *filter* which
    slides over the image and computes a dot product.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 主要目的是从输入中提取特征。每个图像都是像素值矩阵，通过滑动在图像上的*滤波器*计算点积来转换为特征。
- en: Pooling Layer
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层
- en: This layer is also sometimes called downsampling or subsampling. It reduces
    the dimensionality of the features presented by the convolutional layer by using
    either Max, Average, or Sum values.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有时也称为降采样或子采样。它通过使用最大值、平均值或总和值来减少由卷积层呈现的特征的维度。
- en: Fully Connected Layer
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层
- en: Similar to a multilayer perceptron that uses a SoftMax activation function to
    deliver to the output layer a probability distribution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于使用SoftMax激活函数的多层感知器，将概率分布传递到输出层。
- en: CNNs can become very complex. Check out Google’s Inception model, shown in [Figure 2-25](#fig0225).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: CNN可以变得非常复杂。查看谷歌的Inception模型，如[图2-25](#fig0225)所示。
- en: '![](assets/pdss_0225.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0225.png)'
- en: Figure 2-25\. Google’s Inception model
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-25\. 谷歌的Inception模型
- en: Note
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: The field of neural networks is undergoing rapid and exciting change. Brilliant
    minds are working ardently to push this field forward incredibly fast. Along come
    researchers Sara Sabour, Nicholas Frost, and Geoffrey Hinton with a proposal called
    [CapsNets](http://bit.ly/2lQr9dl) (Capsule Networks). (Hinton is an icon in this
    field; when his name is on a paper...you read it.) In a multilayer neural network
    you add more and more layers depending on your needs. In a CapsNet you add a neural
    network inside another layer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络领域正在经历快速且令人兴奋的变化。杰出的思想家们正努力推动这一领域的快速发展。随着研究人员萨拉·萨布尔、尼古拉斯·弗罗斯特和杰弗里·辛顿提出了[胶囊网络](http://bit.ly/2lQr9dl)（Capsule
    Networks）的提议，这一变化尤为明显。（辛顿在这一领域是一个偶像级人物；只要他的名字出现在一篇论文上...你就会去读它。）在多层神经网络中，根据你的需求不断添加更多层次。在CapsNet中，你在另一层中添加一个神经网络。
- en: As Hinton says, “The pooling operation used in convolutional neural networks
    is a big mistake and the fact that it works so well is a disaster.”
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如辛顿所说，“卷积神经网络中使用的池化操作是一个大错误，它能够如此出色地工作，这是一场灾难。”
- en: What makes capsule networks so exciting is they, like our own image processing,
    do not take into account the orientation of the image. When a child looks at a
    dog, the orientation of the dog does not affect his/her perception of the image.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 令胶囊网络如此令人兴奋的原因在于它们，就像我们自己的图像处理一样，不考虑图像的方向。当一个孩子看着一只狗时，狗的方向并不会影响他/她对图像的感知。
- en: CapsNets are too new at this time, but if they continue to gain traction we
    will discuss them more fully in future editions of this book.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: CapsNets在目前仍然很新，但如果它们继续受到关注，我们将在未来的版本中更全面地讨论它们。
- en: Recursive neural network
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: A *recursive neural network* is a multilayer network that leverages time-series
    or sequential data. They perform very well and are often the go-to model for natural
    language processing (NLP) tasks and time-series data. We will see them in action
    in the chapter *Language and Text Processing*. In our other neural networks, once
    data is passed to the next layer the previous layer is forgotten. However, when
    trying to make predictions along a sequence of data it is important to *remember*
    what came before it. These networks are *recurrent* in that they double back and
    look at the previous input or inputs. In a sense, they have a memory.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*递归神经网络*是利用时间序列或顺序数据的多层网络。它们表现非常出色，通常是自然语言处理（NLP）任务和时间序列数据的首选模型。我们将在*语言与文本处理*章节中看到它们的运行。在其他神经网络中，一旦数据传递到下一层，前一层就被遗忘了。然而，在试图对数据序列进行预测时，*记住*之前发生过的事情是很重要的。这些网络是*递归*的，它们会回溯并查看之前的输入或输入。从某种意义上说，它们具有记忆功能。'
- en: The arrows circling back show the recurrence in the RNN ([Figure 2-26](#fig0226)).
    As you can see, this recurrence is very short; it only circles back on the same
    layer. In essence, it has only a short-term memory. This problem is overcome by
    introducing to the network a long short-term memory (LSTM).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 环绕返回的箭头显示了RNN中的循环（见[图2-26](#fig0226)）。正如你所见，这种循环非常短暂；它仅在同一层上循环返回。从本质上讲，它只有短期记忆。引入长短期记忆（LSTM）网络克服了这一问题。
- en: 'LSTMs allow the network to learn over a long period of time. They have three
    gates: input, output, and forget. The input gate determines what data is let in.
    The output gate determines what data is let out. Finally, the forget gate decides
    what data should be forgotten. Their architecture can be difficult for the beginner
    so suffice it to say that LSTMs allow the network to remember over a long period.
    If you are interested in a deeper dive into them, read [this blog](http://bit.ly/2kjzC8A).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM允许网络在长时间内学习。它们有三个门：输入门、输出门和遗忘门。输入门确定允许进入的数据。输出门确定允许输出的数据。最后，遗忘门决定应该遗忘哪些数据。它们的架构对初学者来说可能很难理解，但可以说LSTM允许网络在长时间内记住。如果您对它们更深入地了解感兴趣，请阅读[这篇博客](http://bit.ly/2kjzC8A)。
- en: '![](assets/pdss_0226.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图2-26](assets/pdss_0226.png)'
- en: Figure 2-26\. Feed forward and recurrent networks
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-26。前馈和递归网络
- en: Temporal networks
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间网络
- en: A *temporal convolutional network* (TCN) is a multilayer network that has the
    advantages of a convolutional network while also considering placement and position.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*时间卷积网络*（TCN）是一个多层网络，具有卷积网络的优点，同时也考虑位置和摆放。'
- en: Convolutional networks are generally very good at image recognition and language
    classification. They do not however, care about placement. For instance, a CNN
    wants to know if the image contains a tail, a brown button nose, and floppy ears.
    Then it classifies that image as a dog. It does not care about the positioning
    of the image. In language classification, a CNN wants to know the presence of
    certain keywords that will indicate if it is looking at a legal document, a comic
    book, or a Hemingway novel. The position, again, does not really matter. What
    if you want to work on data in which position and placement *is* important, such
    as time-series data? Time-series data is simply a dataset on a timeline with date
    and/or timestamps. As we mentioned earlier, the industry go-to model for such
    tasks is the RNN. However, like many things in data science, that model has recently
    been unseated...by the mighty TCN.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络通常非常擅长图像识别和语言分类。然而，它们并不关心位置。例如，卷积神经网络想知道图像中是否有尾巴、棕色的鼻子和垂耳。然后将该图像分类为狗。它并不关心图像的位置。在语言分类中，卷积神经网络想知道是否存在某些关键词，这些关键词会提示它是否在查看法律文件、漫画书或海明威的小说。同样，位置并不真正重要。但是如果您想处理位置和摆放很重要的数据，比如时间序列数据呢？时间序列数据就是一个带有日期和/或时间戳的时间轴上的数据集。正如我们之前提到的，处理这类任务的行业首选模型是循环神经网络（RNN）。然而，像许多数据科学中的事物一样，这个模型最近被强大的TCN取代了。
- en: Compared to RNNs, TCNs have the advantage of being computationally less expensive
    and using a simpler architecture. RNNs need resources, the LSTM layers, to *remember*.
    TCNs use input steps that map to outputs that are used in the next layer of the
    input ([Figure 2-27](#fig0227)). Instead of using recurrence, they use the results
    of one layer to feed the next layer.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于RNN，TCN在计算上更加经济实惠，并且使用更简单的架构。RNN需要资源，如LSTM层来*记住*。TCN使用映射到下一层输入的输出步骤（参见[图2-27](#fig0227)）。它们不使用递归，而是使用一个层的结果来馈送下一个层。
- en: '![](assets/pdss_0227.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图2-27](assets/pdss_0227.png)'
- en: Figure 2-27\. Temporal convolutional network
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-27。时间卷积网络
- en: In [Chapter 6](ch06.html#predictive_analytics_in_r_and_python), we do a simple
    sales forecast. TCNs seem like the proper model to use for such a task; we will
    attempt to use it to forecast the sales for a particular product from an SAP system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.html#predictive_analytics_in_r_and_python)中，我们进行了一个简单的销售预测。TCN似乎是用于此类任务的合适模型；我们将尝试使用它来预测从SAP系统中某个特定产品的销售情况。
- en: Autoencoder
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'The Autoencoder is a feed-forward-only neural network with a deceptively simple
    definition. It is a network that takes input data and tries to copy it as the
    output. It is comprised of two parts:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一个仅具有前向传递的神经网络，其定义看似简单。它是一个接受输入数据并试图将其复制为输出的网络。它由两部分组成：
- en: Encoder
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器
- en: Deconstructs the input data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 拆解输入数据。
- en: Decoder
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器
- en: Reconstructs the data for output.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 重构数据以供输出。
- en: The most common use for this type of network is image denoising and image generation.
    The real value in the autoencoder is not the output, which is the case for our
    other neural networks. The real value is in the representation that the neural
    network has of the output in the compressed data. To clarify this further, the
    model at its most compressed has learned the salient features of the object. Let’s
    say it is looking at the image of a dog. The salient features are ears, eyes,
    mouth, snout, dog-like nose, and so on. If the model compresses too far it may
    think the only salient features are eyes and won’t be able to tell the difference
    between a dog and any other animal. If the model is not compressed enough such
    that it recognizes too many features (such as coloring and facial shape) it will
    know only one type of dog. The trick in this model is knowing the balance. As
    a recap, the neural network is optimized not when the output is closer to the
    input, but when the output still represents the key features of the input and
    the data is *compressed* as much as possible.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的网络最常见的用途是图像去噪和图像生成。自编码器的真正价值不在于输出，这在我们其他神经网络的情况下是如此。真正的价值在于神经网络对压缩数据输出的表征。为了进一步澄清这一点，模型在最压缩的情况下已经学会了对象的显著特征。比如说，它正在观察一张狗的图片。显著特征包括耳朵、眼睛、嘴巴、吻、类似狗的鼻子等等。如果模型压缩得太远，可能会认为唯一显著的特征是眼睛，无法区分狗和其他动物。如果模型压缩得不够，识别了太多特征（比如颜色和面部形状），它只会知道一种类型的狗。这种模型的技巧在于知道平衡点。总结一下，神经网络的优化不是在输出接近输入时，而是在输出仍然代表输入的关键特征且数据尽可能*压缩*的时候。
- en: Note
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A key concept with Autoencoders is that the output dimension must be smaller
    than the input dimension for the network to learn the most relevant features.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的一个关键概念是，输出维度必须小于输入维度，以便网络学习最相关的特征。
- en: '![](assets/pdss_0228.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0228.png)'
- en: Figure 2-28\. Autoencoder
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-28\. 自编码器
- en: Autoencoders are typically used for reducing the dimensionality of the data
    and feature learning. They are commonly part of another neural network where it
    helps reduce feature dimensionality.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通常用于减少数据的维度和特征学习。它们通常是另一个神经网络的一部分，帮助减少特征的维度。
- en: Generative adversarial network
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: A generative adversarial network (GAN) is a neural network architecture where
    two networks, to put it frankly, fight. Hence the term *adversarial*. The two
    networks are referred to as the Generator and the Discriminator. Imagine this
    commonly used scenario. The GAN wants to make fake money. The generator creates
    a bill and sends it to the discriminator for testing. Well, the discriminator
    knows what bills look like because it has learned from a set of real-world images.
    The generator’s first attempt is woeful, it fails and it gets feedback on its
    failure. Then it tries again, and again, and again until it is able to produce
    a bill that the discriminator thinks is real. Then it is the discriminator’s turn
    to learn. It finds out that it was wrong and learns not to accept that fake bill
    again. This bickering goes back and forth until a point where the networks fairly
    evenly fail and succeed...a point where no more learning is happening on either
    side.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）是一种神经网络架构，其中有两个网络，坦率地说，它们相互对抗。因此有*对抗*一词。这两个网络分别称为生成器和判别器。想象一下这种常见的场景。GAN想制造假钱。生成器制作一张钞票并将其发送给判别器进行测试。嗯，判别器知道真钞看起来是什么样的，因为它已经从一组真实世界的图像中学习了。生成器的第一次尝试是可悲的，它失败了并且得到了失败的反馈。然后它再试一次，再试一次，直到能够生成一个让判别器认为是真实的钞票。然后轮到判别器学习了。它发现自己错了，并学会不再接受那张假钞票。这种争吵来回进行，直到网络在失败和成功上达到了一个平衡点......即不再有学习发生的点。
- en: '![](assets/pdss_0229.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pdss_0229.png)'
- en: Figure 2-29\. Generative adversarial network
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-29\. 生成对抗网络
- en: />
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: />
- en: You may wonder how a network like this would be used. Well, these networks like
    to mimic data. Therefore they have been taught to mimic art, music, images, and
    even poetry. They can be taught to merge concepts in images. For instance, you
    train the network on images of men wearing hats, women wearing no hats and ask
    the GAN to generate images of women wearing hats, and it does a pretty good job.
    Sounds nifty, but what is the use in our business scenarios? Well, GANs have been
    used to detect anomalies in data and also to generate training data for other
    networks when a limited amount is available. In the introduction to neural networks
    we provide here, we would be remiss to not mention GANs. However, we admit it
    is harder to apply them to business applications. Presenting them here is illustrative
    of our goal of creating a type of *Citizen Data Scientist* within the SAP business
    analyst community. Keep in mind all the concepts, including GANs, and perhaps
    you will identify a business scenario where a GAN could be employed.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道这样的网络会如何使用。嗯，这些网络喜欢模仿数据。因此，它们已经被教会模仿艺术、音乐、图像，甚至诗歌。它们可以被教会合并图像中的概念。例如，你训练网络使用戴帽男士的图像，不戴帽女士的图像，并要求GAN生成戴帽女士的图像，它会做得相当不错。听起来很棒，但在我们的业务场景中有什么用呢？嗯，GAN已被用于检测数据中的异常，也用于为其他网络生成训练数据，当数据量有限时。在这里我们提供的神经网络介绍中，我们不提到GAN是不完整的。但是，我们承认将它们应用于业务应用程序要困难得多。在此介绍它们只是为了说明我们在SAP业务分析师社区中创建一种*公民数据科学家*的目标。请记住，包括GAN在内的所有概念，也许你能找到一个能够应用GAN的业务场景。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: If this was your first introduction to data science concepts, we understand
    it was a lot to take in. If you are an experienced data scientist you may have
    asked questions such as “Where is XGBoost?” or “Why not AutoML?” Remember our
    main intent, we want to get business analysts to think a little like data scientists.
    The creation of citizen data scientists if you will. There are many other areas
    of data science that we did not cover in this chapter but will address later such
    as exploratory data analysis and data visualization. Business analysts, we hope
    that you found in this chapter ideas that will get you thinking about your own
    data—and in particular for this book, your SAP data. In the following chapters
    we will go into detailed business scenarios using SAP data and the concepts we
    introduced in this chapter.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你第一次接触数据科学概念，我们理解这可能有些难以接受。如果你是一位经验丰富的数据科学家，你可能会问：“XGBoost在哪里？”或者“为什么不用AutoML？”请记住我们的主要意图，我们希望业务分析师能多少像数据科学家思考一下。我们在本章中没有涵盖的数据科学的其他领域，如探索性数据分析和数据可视化，将在后面进行讨论。业务分析师们，我们希望你在本章中找到一些能让你思考你自己数据的想法，特别是本书中的SAP数据。在接下来的章节中，我们将使用SAP数据和我们在本章介绍的概念来进行详细的业务场景讨论。
- en: ^([1](ch02.html#idm46252721295272-marker)) The authors recommend learning to
    park *before* Formula One racing, but we did not analyze this using any of the
    techniques in this book. So who knows? Maybe it *is* better to be an Formula One
    driver but not learn to park! More data is needed.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#idm46252721295272-marker)) 作者建议在学习参与F1赛车之前先学会停车，但我们没有使用本书中的任何技术进行分析。那么谁知道呢？也许成为F1赛车手但不学会停车更好！需要更多数据来证明。
- en: ^([2](ch02.html#idm46252721278024-marker)) Euclidean distance is simply the
    ordinary straight-line distance between two points, either on a plane or in three-dimensional
    space. Why say “straight-line” when you can say “Euclidean distance” and sound
    scholarly? Bonus points if you have a pipe or tweed jacket.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#idm46252721278024-marker)) 欧氏距离简单来说就是两点之间的普通直线距离，可以是平面上的或者三维空间中的。为什么说“直线距离”而不说“欧氏距离”听起来更学术呢？如果你有烟斗或羊毛衫夹克，额外加分。
- en: ^([3](ch02.html#idm46252721257816-marker)) A much more detailed explanation
    of this scenario can be found at [*http://yudkowsky.net/rational/bayes#content*](http://yudkowsky.net/rational/bayes#content).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#idm46252721257816-marker)) 这种情况的更详细解释可以在[*http://yudkowsky.net/rational/bayes#content*](http://yudkowsky.net/rational/bayes#content)找到。
- en: ^([4](ch02.html#idm46252724893688-marker)) Image classification refers to the
    process of extracting information from an image and classifying it; for example,
    to identify when a picture is of a [Chihuahua or a blueberry muffin](http://bit.ly/2U6fpAt).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#idm46252724893688-marker)) 图像分类是指从图像中提取信息并对其进行分类的过程；例如，识别一张图片是[吉娃娃还是蓝莓松饼](http://bit.ly/2U6fpAt)。
