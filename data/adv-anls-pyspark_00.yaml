- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序言
- en: Apache Spark’s long lineage of predecessors, from MPI (message passing interface)
    to MapReduce, made it possible to write programs that take advantage of massive
    resources while abstracting away the nitty-gritty details of distributed systems.
    As much as data processing needs have motivated the development of these frameworks,
    in a way the field of big data has become so related to them that its scope is
    defined by what these frameworks can handle. Spark’s original promise was to take
    this a little further—to make writing distributed programs feel like writing regular
    programs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的悠久前身，从 MPI（消息传递接口）到 MapReduce，使得编写能够利用大量资源并抽象掉分布式系统细节的程序成为可能。正如数据处理需求推动了这些框架的发展一样，在某种程度上，大数据领域与它们密切相关，其范围由这些框架能够处理的内容定义。Spark
    最初的承诺是将这一点推向更远——使编写分布式程序感觉像编写常规程序一样。
- en: The rise in Spark’s popularity coincided with that of the Python data (PyData)
    ecosystem. So it makes sense that Spark’s Python API—PySpark—has significantly
    grown in popularity over the last few years. Although the PyData ecosystem has
    recently sprung up some distributed programming options, Apache Spark remains
    one of the most popular choices for working with large datasets across industries
    and domains. Thanks to recent efforts to integrate PySpark with the other PyData
    tools, learning the framework can help you boost your productivity significantly
    as a data science practitioner.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的普及与 Python 数据（PyData）生态系统的普及同时发生。因此，Spark 的 Python API——PySpark，在过去几年中显著增长是合情合理的。尽管
    PyData 生态系统最近推出了一些分布式编程选项，但 Apache Spark 仍然是跨行业和领域处理大型数据集的最受欢迎选择之一。由于最近努力将 PySpark
    与其他 PyData 工具集成，学习这一框架可以显著提高数据科学从业者的生产力。
- en: 'We think that the best way to teach data science is by example. To that end,
    we have put together a book of applications, trying to touch on the interactions
    between the most common algorithms, datasets, and design patterns in large-scale
    analytics. This book isn’t meant to be read cover to cover: page to a chapter
    that looks like something you’re trying to accomplish, or that simply ignites
    your interest, and start there.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为教授数据科学的最佳方式是通过示例。为此，我们编写了一本应用程序书籍，试图涵盖大规模分析中最常见的算法、数据集和设计模式之间的互动。本书并非用于全书阅读：请翻到一个看起来您想要完成的任务或仅仅激发您兴趣的章节或页面，从那里开始阅读。
- en: Why Did We Write This Book Now?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们现在写这本书？
- en: Apache Spark experienced a major version upgrade in 2020—version 3.0\. One of
    the biggest improvements was the introduction of Spark Adaptive Execution. This
    feature takes away a big portion of the complexity around tuning and optimization.
    We do not refer to it in the book because it’s turned on by default in Spark 3.2
    and later versions, and so you automatically get the benefits.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 在 2020 年经历了一次重大的版本升级——版本 3.0。其中最大的改进之一是引入了 Spark 自适应执行。这一特性大大简化了调优和优化的复杂性。尽管在书中我们没有提及它，因为在
    Spark 3.2 及更高版本中，默认已启用，因此您将自动获得其好处。
- en: The ecosystem changes, combined with Spark’s latest major release, make this
    edition a timely one. Unlike previous editions of *Advanced Analytics with Spark*,
    which chose Scala, we will use Python. We’ll cover best practices and integrate
    with the wider Python data science ecosystem when appropriate. All chapters have
    been updated to use the latest PySpark API. Two new chapters have been added and
    multiple chapters have undergone major rewrites. We will not cover Spark’s streaming
    and graph libraries. With Spark in a new era of maturity and stability, we hope
    that these changes will preserve the book as a useful resource on analytics for
    years to come.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 生态系统的变化，加上 Spark 的最新主要发布，使得本版及时而至。与之前版本的《使用 Scala 进行高级分析与 Spark》不同，我们将使用 Python。我们将涵盖最佳实践，并在适当时与更广泛的
    Python 数据科学生态系统集成。所有章节均已更新以使用最新的 PySpark API。新增了两个章节，并对多个章节进行了重大改写。我们不会涵盖 Spark
    的流处理和图形库。随着 Spark 进入一个新的成熟和稳定的时代，我们希望这些变化能使本书作为多年来分析的有用资源得以保留。
- en: How This Book Is Organized
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书的组织方式
- en: '[Chapter 1](ch01.xhtml#Introduction) places Spark and PySpark within the wider
    context of data science and big data analytics. After that, each chapter comprises
    a self-contained analysis using PySpark. [Chapter 2](ch02.xhtml#introduction_to_data_anlysis_with_pyspark)
    introduces the basics of data processing in PySpark and Python through a use case
    in data cleansing. The next few chapters delve into the meat and potatoes of machine
    learning with Spark, applying some of the most common algorithms in canonical
    applications. The remaining chapters are a bit more of a grab bag and apply Spark
    in slightly more exotic applications—for example, querying Wikipedia through latent
    semantic relationships in the text, analyzing genomics data, and identifying similar
    images.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](ch01.xhtml#Introduction)将Spark和PySpark置于数据科学和大数据分析的更广泛背景下。之后，每一章都包括使用PySpark的独立分析。[第2章](ch02.xhtml#introduction_to_data_anlysis_with_pyspark)通过数据清洗的用例介绍了PySpark和Python中的数据处理基础知识。接下来的几章深入探讨了使用Spark进行机器学习的核心内容，应用了一些最常见的算法于经典应用场景中。其余章节则更多地涉及稍微不同寻常的应用，例如通过文本中的潜在语义关系查询维基百科、分析基因组数据和识别相似图像。'
- en: This book is not about PySpark’s merits and disadvantages. There are a few other
    things that it is not about either. It introduces the Spark programming model
    and basics of Spark’s Python API, PySpark. However, it does not attempt to be
    a Spark reference or provide a comprehensive guide to all Spark’s nooks and crannies.
    It does not try to be a machine learning, statistics, or linear algebra reference,
    although many of the chapters provide some background on these before using them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涉及PySpark的优缺点。它也不涉及其他几件事。它介绍了Spark编程模型以及Spark的Python API PySpark的基础知识。然而，它不试图成为Spark的参考书或提供所有Spark细节的全面指南。它也不试图成为机器学习、统计学或线性代数的参考书，尽管许多章节在使用它们之前提供了一些背景。
- en: 'Instead, this book will help the reader get a feel for what it’s like to use
    PySpark for complex analytics on large datasets by covering the entire pipeline:
    not just building and evaluating models, but also cleansing, preprocessing, and
    exploring data, with attention paid to turning results into production applications.
    We believe that the best way to teach this is by example.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这本书将帮助读者体会使用PySpark进行大型数据集上复杂分析的感觉，覆盖整个流程：不仅仅是建模和评估，还包括数据清洗、预处理和数据探索，特别关注将结果转化为生产应用程序。我们认为通过示例来教授这些是最好的方式。
- en: 'Here are examples of some tasks that will be tackled in this book:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是本书将解决的一些任务的示例：
- en: Predicting forest cover
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 预测森林覆盖
- en: We predict type of forest cover using relevant features like location and soil
    type by using decision trees (see [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用决策树（见[第4章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)）预测森林覆盖类型，使用相关特征如位置和土壤类型。
- en: Querying Wikipedia for similar entries
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查询维基百科以查找类似条目
- en: We identify relationships between entries and query the Wikipedia corpus by
    using NLP (natural language processing) techniques (see [Chapter 6](ch06.xhtml#understanding_wikipedia_with_lda_and_spark_nlp)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用自然语言处理（NLP）技术（见[第6章](ch06.xhtml#understanding_wikipedia_with_lda_and_spark_nlp)）识别条目之间的关系并查询维基百科语料库。
- en: Understanding utilization of New York cabs
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 理解纽约出租车的利用率
- en: We compute average taxi waiting time as a function of location by performing
    temporal and geospatial analysis (see [Chapter 7](ch07.xhtml#geospatial_and_temporal_data_analysis_on_taxi_trip_data)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过执行时间和地理空间分析（见[第7章](ch07.xhtml#geospatial_and_temporal_data_analysis_on_taxi_trip_data)）计算出租车等待时间的平均值作为位置的函数。
- en: Reduce risk for an investment portfolio
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为投资组合减少风险
- en: We estimate financial risk for an investment portfolio using the Monte Carlo
    simulation (see [Chapter 9](ch09.xhtml#analyzing_genomics_data_and_and_the_bdg_project)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用蒙特卡洛模拟（见[第9章](ch09.xhtml#analyzing_genomics_data_and_and_the_bdg_project)）为投资组合估计金融风险。
- en: When possible, we attempt not to just provide a “solution,” but to demonstrate
    the full data science workflow, with all of its iterations, dead ends, and restarts.
    This book will be useful for getting more comfortable with Python, Spark, and
    machine learning and data analysis. However, these are in service of a larger
    goal, and we hope that most of all this book will teach you how to approach tasks
    like those described earlier. Each chapter, in about 20 measly pages, will try
    to get as close as possible to demonstrating how to build one piece of these data
    applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，我们尝试不仅仅提供“解决方案”，而是演示完整的数据科学工作流程，包括所有的迭代、死胡同和重新启动。这本书将有助于您更熟悉Python、Spark以及机器学习和数据分析。然而，这些都是为了更大的目标服务，我们希望这本书最重要的是教会您如何处理类似前述任务。每一章，大约20页的内容，都会尽可能接近演示如何构建这些数据应用的一个部分。
- en: Conventions Used in This Book
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: '*Italic*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、网址、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`常数宽度`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序清单，以及段落中引用程序元素，如变量或函数名、数据库、数据类型、环境变量、语句和关键字。
- en: '**`Constant width bold`**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**`常数宽度粗体`**'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 显示用户应按字面意义键入的命令或其他文本。
- en: '*`Constant width italic`*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*`常数宽度斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应使用用户提供的值或由上下文确定的值替换的文本。
- en: This element signifies a tip or suggestion.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: This element signifies a general note.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般说明。
- en: This element indicates a warning or caution.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示警告或注意事项。
- en: Using Code Examples
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://github.com/sryza/aas*](https://github.com/sryza/aas).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可以下载补充材料（代码示例、练习等）位于[*https://github.com/sryza/aas*](https://github.com/sryza/aas)。
- en: If you have a technical question or a problem using the code examples, please
    send email to [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在使用代码示例时遇到技术问题或问题，请发送电子邮件至[*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目的是帮助您完成工作。一般情况下，如果本书提供了示例代码，您可以在自己的程序和文档中使用它。除非您要复制大量代码，否则无需事先联系我们请求许可。例如，编写一个使用本书多个代码片段的程序并不需要许可。销售或分发O’Reilly图书中的示例代码则需要许可。如果通过引用本书回答问题并引用示例代码，也不需要许可。将本书中大量示例代码整合到产品文档中，则需要许可。
- en: 'We appreciate, but do not require, attribution. An attribution usually includes
    the title, author, publisher, and ISBN. For example: “*Advanced Analytics with
    PySpark* by Akash Tandon, Sandy Ryza, Uri Laserson, Sean Owen, and Josh Wills
    (O’Reilly). Copyright 2022 Akash Tandon, 978-1-098-10365-1.”'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢您，但不要求您进行署名。通常的署名包括标题、作者、出版社和ISBN。例如：“*使用PySpark进行高级分析* 由Akash Tandon、Sandy
    Ryza、Uri Laserson、Sean Owen和Josh Wills（O’Reilly）编写。版权2022 Akash Tandon，978-1-098-10365-1。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为使用示例代码超出了合理使用或以上授权，请随时通过[*permissions@oreilly.com*](mailto:permissions@oreilly.com)联系我们。
- en: O’Reilly Online Learning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly在线学习
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 40多年来，[*O’Reilly Media*](https://oreilly.com)一直为公司提供技术和业务培训、知识和见解，帮助它们取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly的在线学习平台为您提供按需访问实时培训课程、深入学习路径、交互式编码环境以及来自O’Reilly和其他200多个出版商的广泛文本和视频的机会。有关更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: How to Contact Us
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本书的评论和问题，请联系出版社：
- en: O’Reilly Media, Inc.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol, CA 95472
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-998-9938（美国或加拿大）
- en: 707-829-0515 (international or local)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0515（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/adv-analytics-pyspark*](https://oreil.ly/adv-analytics-pyspark).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书创建了一个网页，列出勘误、示例和任何额外信息。您可以访问[*https://oreil.ly/adv-analytics-pyspark*](https://oreil.ly/adv-analytics-pyspark)获取此页面。
- en: Email [*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com) to comment
    or ask technical questions about this book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件[*bookquestions@oreilly.com*](mailto:bookquestions@oreilly.com)以评论或询问有关本书的技术问题。
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有关我们的书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在LinkedIn上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)
- en: 'Follow us on Twitter: [*https://twitter.com/oreillymedia*](https://twitter.com/oreillymedia)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们：[*https://twitter.com/oreillymedia*](https://twitter.com/oreillymedia)
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上观看我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: It goes without saying that you wouldn’t be reading this book if it were not
    for the existence of Apache Spark and MLlib. We all owe thanks to the team that
    has built and open sourced it and the hundreds of contributors who have added
    to it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，如果没有Apache Spark和MLlib的存在，您将不会阅读本书。我们都要感谢构建和开源它的团队以及为其增加功能的数百名贡献者。
- en: 'We would like to thank everyone who spent a great deal of time reviewing the
    content of the previous editions of the book with expert eyes: Michael Bernico,
    Adam Breindel, Ian Buss, Parviz Deyhim, Jeremy Freeman, Chris Fregly, Debashish
    Ghosh, Juliet Hougland, Jonathan Keebler, Nisha Muktewar, Frank Nothaft, Nick
    Pentreath, Kostas Sakellis, Tom White, Marcelo Vanzin, and Juliet Hougland again.
    Thanks all! We owe you one. This has greatly improved the structure and quality
    of the result.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢所有花费大量时间审查本书前版本内容的专家：Michael Bernico，Adam Breindel，Ian Buss，Parviz Deyhim，Jeremy
    Freeman，Chris Fregly，Debashish Ghosh，Juliet Hougland，Jonathan Keebler，Nisha Muktewar，Frank
    Nothaft，Nick Pentreath，Kostas Sakellis，Tom White，Marcelo Vanzin和再次感谢Juliet Hougland。谢谢大家！我们欠你们一份感谢。这极大地改进了结果的结构和质量。
- en: Sandy also would like to thank Jordan Pinkus and Richard Wang for helping with
    some of the theory behind the risk chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Sandy还要感谢Jordan Pinkus和Richard Wang对风险章节理论的帮助。
- en: Thanks to Jeff Bleiel and O’Reilly for the experience and great support in getting
    this book published and into your hands.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢Jeff Bleiel和O’Reilly为出版这本书和将其送入您手中提供的经验和大力支持。
