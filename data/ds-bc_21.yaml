- en: 17 Case study 4 solution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17 案例研究4解决方案
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Parsing text from HTML
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从HTML中解析文本
- en: Computing text similarities
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算文本相似度
- en: Clustering and exploring large text datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类和探索大型文本数据集
- en: 'We have downloaded thousands of job postings by searching on this book’s table
    of contents for case studies 1 through 4 (see the problem statement for details).
    Besides the downloaded postings, we also have at our disposal two text files:
    resume.txt and table_of_contents.txt. The first file contains a resume draft,
    and the second contains the truncated table of contents used to query for job
    listing results. Our goal is to extract common data science skills from the downloaded
    job postings. Then we’ll compare these skills to our resume to determine which
    skills are missing. We will do so as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在这本书的目录中搜索案例研究1至4（详细信息请参阅问题陈述）来下载了数千份职位发布。除了下载的发布内容外，我们还拥有两个文本文件：resume.txt和table_of_contents.txt。第一个文件包含一份简历草案，第二个文件包含用于查询职位列表结果的截断目录。我们的目标是提取下载的职位发布中的常见数据科学技能。然后我们将这些技能与我们的简历进行比较，以确定缺少哪些技能。我们将按以下步骤进行：
- en: Parse all text from the downloaded HTML files.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析下载的HTML文件中的所有文本。
- en: Explore the parsed output to learn how job skills are described in online postings.
    We’ll pay particular attention to whether certain HTML tags are more associated
    with skill descriptions.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索解析后的输出，了解在线发布中如何描述工作技能。我们将特别注意某些HTML标签是否与技能描述更相关。
- en: Attempt to filter any irrelevant job postings from our dataset.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试从我们的数据集中过滤掉任何不相关的职位发布。
- en: Cluster job skills based on text similarity.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据文本相似性对工作技能进行聚类。
- en: Visualize the clusters using word clouds.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用词云可视化聚类。
- en: Adjust clustering parameters, if necessary, to improve the visualized output.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有必要，调整聚类参数以改善可视化输出。
- en: Compare the clustered skills to our resume to uncover missing skills.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将聚类技能与我们的简历进行比较，以揭示缺失的技能。
- en: Warning Spoiler alert! The solution to case study 4 is about to be revealed.
    We strongly encourage you to try to solve the problem before reading the solution.
    The original problem statement is available for reference at the beginning of
    the case study.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 揭示警告！案例研究4的解决方案即将揭晓。我们强烈建议你在阅读解决方案之前尝试解决问题。原始问题陈述可在案例研究开始处参考。
- en: 17.1 Extracting skill requirements from job posting data
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 从职位发布数据中提取技能要求
- en: We begin by loading all the HTML files in the job_postings directory. We store
    the contents of these files in an `html_contents` list.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载job_postings目录中的所有HTML文件。我们将这些文件的 内容存储在`html_contents`列表中。
- en: Warning Be sure to manually unzip the compressed job_postings.zip directory
    before executing the following code.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 确保在执行以下代码之前手动解压缩压缩文件job_postings.zip目录。
- en: Listing 17.1 Loading HTML files
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.1 加载HTML文件
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ We use the Python 3 glob module to obtain filenames with HTML extensions in
    the job_postings directory. These filenames are sorted to maintain output consistency
    across readers’ personal machines. This ensures that the first two sampled files
    remain the same for all the readers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用Python 3的glob模块从job_postings目录中获取HTML扩展名的文件名。这些文件名按顺序排序，以确保所有读者在不同个人机器上的输出一致性。这确保了前两个样本文件对所有读者都是相同的。
- en: Each of our 1,458 HTML files can be parsed using Beautiful Soup. Let’s execute
    the parsing and store the parsed results in a `soup_objects` list. We also confirm
    that each parsed HTML file contains a title and a body.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有1,458个HTML文件都可以使用Beautiful Soup进行解析。让我们执行解析并将解析结果存储在`soup_objects`列表中。我们还确认每个解析的HTML文件都包含一个标题和一个正文。
- en: Listing 17.2 Parsing HTML files
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.2 解析HTML文件
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Each parsed HTML file contains a title and a body. Are there any duplicates
    across the titles or bodies of these files? We can find out by storing all title
    text and body text in two columns in a Pandas table. Calling the Pandas `describe`
    method will reveal the presence of any duplicates in the text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解析的HTML文件都包含一个标题和一个正文。这些文件的标题或正文之间是否有重复？我们可以通过在Pandas表中存储所有标题文本和正文文本在两列中，来找出答案。调用Pandas的`describe`方法将揭示文本中是否存在重复。
- en: Listing 17.3 Checking title and body texts for duplicates
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.3 检查标题和正文文本是否存在重复
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '1,364 of the 1,458 titles are unique. The remaining 94 titles are duplicates.
    The most common title is repeated 13 times: it is for a data scientist position
    in New York. We can easily verify that all the duplicate titles correspond to
    unique body text. All 1,458 bodies are unique, so none of the job postings occur
    more than once, even if some postings share a common generic title.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在1,458个标题中，有1,364个是唯一的。剩余的94个标题是重复的。最常见的标题重复了13次：它是一个位于纽约的数据科学家职位。我们可以轻松验证所有重复的标题都对应着独特的正文内容。所有1,458个正文都是唯一的，所以没有任何职位发布重复出现，即使有些发布共享了一个通用的标题。
- en: We’ve confirmed that no duplicates are present in the HTML. Now, let’s explore
    the HTML content in more detail. The goal of our exploration is to determine how
    job skills are described in the HTML.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确认HTML中没有重复项。现在，让我们更详细地探索HTML内容。我们探索的目标是确定在HTML中如何描述职位技能。
- en: 17.1.1 Exploring the HTML for skill descriptions
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.1.1 探索技能描述的HTML
- en: We start our exploration by rendering the HTML at index 0 of `html_contents`
    (figure 17.1).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过渲染`html_contents`中的第0个HTML来开始我们的探索（图17.1）。
- en: Listing 17.4 Rendering the HTML of the first job posting
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.4 渲染第一个职位发布的HTML
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/17-01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17-01.png)'
- en: Figure 17.1 The rendered HTML for the first job posting. The initial paragraph
    summarizes the data science job. The paragraph is followed by lists of bullet
    points, each containing a skill that is required to get the job.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1 第一个职位发布的渲染HTML。首段总结了数据科学职位。该段落后面是包含获得该职位所需技能的要点列表。
- en: 'The rendered job posting is for a data science position. The posting starts
    with a brief position overview, from which we learn that the job entails drawing
    insights from government data. The various required skills include model building,
    statistics, and visualization. These skills are further elaborated in the two
    bolded subsections: Responsibilities and Qualifications. Each subsection is composed
    of multiple single-sentence bullet points. The bullets are varied in their content:
    responsibilities include statistical method usage (bullet 1), future trend discovery
    (bullet 5), spatial analysis of geographic data (bullet 6), and aesthetically
    appealing visualization (bullet 7). Additionally, the bulleted qualifications
    cover computer languages such as R or Python (bullet 1), visualization tools such
    as Matplotlib (bullet 2), machine learning techniques including clustering (bullet
    3), and knowledge of advanced statistical concepts (bullet 4).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染的职位发布是一个数据科学职位。发布从简短的工作概述开始，从中我们了解到该工作涉及从政府数据中提取见解。所需的各种技能包括模型构建、统计学和可视化。这些技能在两个加粗的小节中得到进一步阐述：职责和资格。每个小节由多个单句要点组成。要点的内容各异：职责包括统计方法的使用（要点1）、未来趋势的发现（要点5）、地理数据的空间分析（要点6）和美观的视觉呈现（要点7）。此外，要点资格涵盖了诸如R或Python等计算机语言（要点1）、Matplotlib等可视化工具（要点2）、包括聚类在内的机器学习技术（要点3）以及高级统计概念的掌握（要点4）。
- en: 'It’s worth noting that the qualifications are not that different from the responsibilities.
    Yes, the qualifications focus on tools and concepts, while the responsibilities
    are more attuned to actions on the jobs; but in a way, their bullet points are
    interchangeable. Each bullet describes a skill that an applicant must have to
    perform well at the job. Thus, we can subdivide `html_contents[0]` into two conceptually
    different parts:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，资格与职责并没有太大的区别。是的，资格侧重于工具和概念，而职责则更贴近工作上的行动；但某种程度上，它们的要点是可以互换的。每个要点描述了申请者必须具备的技能，以便在岗位上表现良好。因此，我们可以将`html_contents[0]`划分为两个概念上不同的部分：
- en: An initial job summary
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个初始的职位摘要
- en: A list of bulleted skills required to get the job
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得该职位所需技能的要点列表
- en: Is the next job posting structured in a similar manner? Let’s find out by rendering
    `html_contents[1]` (figure 17.2).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个职位发布是否以类似的方式结构化？让我们通过渲染`html_contents[1]`（图17.2）来找出答案。
- en: Listing 17.5 Rendering the HTML of the second job posting
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.5 渲染第二个职位发布的HTML
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/17-02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17-02.png)'
- en: Figure 17.2 The rendered HTML for the second job posting. As in the first posting,
    the initial paragraph summarizes the data science job, and a list of bullet points
    describes the skills required to get the job.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2 第二个职位发布的渲染HTML。与第一个发布一样，首段总结了数据科学职位，然后是一个描述获得该职位所需技能的要点列表。
- en: 'The job posting is for a data science position in an AI marketing company.
    The structure of the posting is similar to that of `html_contents[0]`: the job
    is summarized in the post’s initial paragraph, and then the required skills are
    presented in bullet points. These bulleted skills are varied in terms of technical
    requirements and details. For example, the fourth bullet from the bottom calls
    for expertise in the Python data science stack (NumPy, SciPy, Pandas, scikit-learn),
    the next bullet requires a track record of solving difficult real-world business
    problems, and the final bullet calls for excellent written and verbal communication
    skills. These three bulleted skills are very different. The difference is intentional—the
    author of the posting is emphasizing the diverse requirements needed to obtain
    the job. Thus, the bullet points in `html_ contents[0]` and `html_contents[1]`
    serve a singular purpose: they offer us brief, sentence-length descriptions of
    unique skills required for each position.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这则职位招聘的是一家 AI 营销公司的数据科学职位。帖子的结构类似于 `html_contents[0]`：职位在帖子的第一段中总结，然后以项目符号的形式展示所需技能。这些项目符号的技能在技术要求和细节方面各不相同。例如，从底部起的第四个项目符号要求具备
    Python 数据科学栈（NumPy、SciPy、Pandas、scikit-learn）的专业知识，下一个项目符号要求有解决困难现实世界商业问题的记录，最后一个项目符号要求有优秀的书面和口头沟通技能。这三个项目符号技能非常不同。这种差异是有意的——帖子的作者在强调获得这份工作所需的多样化要求。因此，`html_contents[0]`
    和 `html_contents[1]` 中的项目符号只有一个目的：它们为我们提供了每个职位所需独特技能的简短、句子长度的描述。
- en: Do these types of bulleted skill descriptions appear in other job posts? Let’s
    find out. First we’ll extract the bullets from each of our parsed HTML files.
    As a reminder, a bullet point is represented by the HTML tag `<li>`. Any bulleted
    file contains multiple such tags; thus, we can extract a list of bullet points
    from a `soup` object by calling `soup.find_all('li')`. Next, we’ll iterate over
    our `soup_objects` list and extract all bullets from each element of that list.
    We store these results in a `Bullets` column in our existing `df_jobs` table.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的带项目符号的技能描述是否出现在其他职位帖子中？让我们来了解一下。首先，我们将从每个解析的 HTML 文件中提取项目符号。提醒一下，项目符号由
    HTML 标签 `<li>` 表示。任何带项目符号的文件都包含多个此类标签；因此，我们可以通过调用 `soup.find_all('li')` 从 `soup`
    对象中提取项目符号列表。接下来，我们将遍历我们的 `soup_objects` 列表，并从该列表的每个元素中提取所有项目符号。我们将这些结果存储在我们现有的
    `df_jobs` 表的 `Bullets` 列中。
- en: Listing 17.6 Extracting bullets from the HTML
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.6 从 HTML 中提取项目符号
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Strips the line break from each bullet to avoid printing the line breaks in
    our later investigations
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从每个项目符号中删除换行符，以避免在我们后续的调查中打印换行符
- en: The bullets in each job posting are stored in `df_jobs.Bullets`. However, it
    is possible that some (or most) of the postings don’t include any bullets. What
    percentage of job postings actually contain bulleted text? We need to find out!
    If that percentage is too low, further bullet analysis is not worth our time.
    Let’s measure the percentage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个职位帖子中的项目符号存储在 `df_jobs.Bullets` 中。然而，可能有一些（或大多数）帖子没有包含任何项目符号。有多少百分比的工作帖子实际上包含带项目符号的文本？我们需要找出！如果这个百分比太低，进一步的子弹分析不值得我们花费时间。让我们来测量这个百分比。
- en: Listing 17.7 Measuring the percent of bulleted postings
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.7 测量带项目符号的帖子百分比
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 90% of the job postings contain bullets. Do all (or most) of these bullets focus
    on skills? We currently don’t know. However, we can better gauge the contents
    of the bullet points by printing the top-ranked words in their text. We can rank
    these words by occurrence count; alternatively, we can carry out the ranking using
    term frequency-inverse document frequency (TFIDF) values rather than raw counts.
    As discussed in section 15, such TFIDF rankings are less likely to contain irrelevant
    words.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 90% 的工作帖子包含项目符号。所有（或大多数）这些项目符号是否都集中在技能上？我们目前还不知道。然而，我们可以通过打印这些项目符号文本中的排名最高的单词来更好地衡量其内容。我们可以通过出现次数来对这些单词进行排名；或者，我们可以使用术语频率-逆文档频率（TFIDF）值而不是原始计数来进行排名。如第
    15 节所述，这种 TFIDF 排名不太可能包含无关的单词。
- en: 'Next, we rank the words using summed TFIDF values. First we compute a TFIDF
    matrix in which rows correspond to individual bullets. Then we sum across the
    rows of the matrix: these sums are used to rank the words, which correspond to
    matrix columns. Finally, we check the top five ranked words for skill-related
    terminology.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用求和的 TFIDF 值对单词进行排名。首先，我们计算一个 TFIDF 矩阵，其中行对应于单个项目符号。然后，我们在矩阵的行上求和：这些和用于对单词进行排名，这些单词对应于矩阵的列。最后，我们检查排名前五的单词，以检查与技能相关的术语。
- en: Listing 17.8 Examining the top-ranked words in the HTML bullets
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.8 检查HTML项目符号中的顶级单词
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Returns a sorted Pandas table of top-ranked words
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回一个按排名排序的Pandas表
- en: ❷ Words are sorted based on the summed TFIDF values across rows in tfidf_matrix.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 单词是根据tfidf_matrix中每行的TFIDF值总和进行排序的。
- en: Terms such as *skills* and *ability* appear among the top five bulleted words.
    There’s reasonable evidence that the bullets correspond to individual job skills.
    How do these bulleted words compare to the remaining words in each job posting?
    Let’s find out. We iterate over the body of each posting and delete any bulleted
    lists using Beautiful Soup’s `decompose` method. Then we extract the remaining
    body text and store it in a `non_bullets` list. Finally, we apply our `rank_words`
    function to that list and display the top five non-bullet words.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*技能*和*能力*等术语出现在前五个项目符号中。有合理的证据表明，项目符号与个人的工作技能相对应。这些项目符号与每个职位帖子中的其他单词如何比较？让我们找出答案。我们遍历每个帖子的正文，并使用Beautiful
    Soup的`decompose`方法删除任何项目符号列表。然后我们提取剩余的正文文本，并将其存储在`non_bullets`列表中。最后，我们将我们的`rank_words`函数应用于该列表，并显示前五个非项目符号单词。'
- en: Listing 17.9 Examining the top-ranked words in the HTML bodies
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.9 检查HTML正文中的顶级单词
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Alternatively, calling body.find_all('ul') will achieve the same result.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 另一种方法是调用body.find_all('ul')将得到相同的结果。
- en: 'The words *skills* and *ability* are no longer present in the ranked output.
    They have been replaced by the words *business* and *team*. Thus, the non-bulleted
    text appears to be less skill oriented than the bullet contents. However, it’s
    still interesting to note that certain top-ranked words are shared between `bullets`
    and `non_bullets`: these words are *data*, *experience*, and *work*. Strangely,
    the words *scientist* and *science* are missing from the list. Do some posts pertain
    to data-driven jobs that aren’t directly data science jobs? Let’s actively explore
    this possibility. We start by iterating over all the titles across all jobs and
    checking if each title mentions a data science position. Then we measure the percentage
    of jobs where the terms *data science* and *data scientist* are missing from the
    titles. Finally, we print a sample of 10 such titles for evaluation purposes.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*技能*和*能力*这两个词不再出现在排名输出中。它们已被*商业*和*团队*这两个词所取代。因此，非项目符号文本似乎比项目符号内容更不侧重于技能。然而，仍然值得注意的是，某些顶级排名的单词在`bullets`和`non_bullets`之间共享：这些单词是*数据*、*经验*和*工作*。奇怪的是，*科学家*和*科学*这两个词在列表中缺失。是否有帖子涉及数据驱动的工作，而这些工作并非直接的数据科学工作？让我们积极探索这个可能性。我们首先遍历所有职位的所有标题，并检查每个标题是否提到了数据科学职位。然后我们测量缺少*数据科学*和*数据科学家*这两个词的职位的百分比。最后，为了评估目的，我们打印出10个这样的标题样本。'
- en: Note As discussed in section 11, we match our terms to the title text using
    regular expressions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如第11节所述，我们使用正则表达式将我们的术语与标题文本进行匹配。
- en: Listing 17.10 Checking titles for references to data science positions
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.10 检查标题是否有提及数据科学职位
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The Pandas str.contains methods can match a regular expression to column text.
    Passing case=False ensures that the match is not case sensitive.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Pandas的str.contains方法可以将正则表达式匹配到列文本。传递case=False确保匹配不区分大小写。
- en: 'Nearly 65% of the posting titles do not mention a data science position. However,
    from our sampled output, we can glean the alternative language that can be used
    to describe a data science job. A posting may call for a *data specialist*, a
    *data analyst*, or a *scientific programmer*. Furthermore, certain job postings
    are for research internships, which we can assume are data-centric. But not all
    sampled jobs are fully relevant: multiple postings are for management positions,
    which don’t align with our immediate career goals. Management is a separate career
    track that requires its own unique set of skills. We should consider excluding
    management positions from our analysis.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎65%的帖子标题没有提及数据科学职位。然而，从我们的样本输出中，我们可以了解到可以用来描述数据科学工作的替代语言。一个帖子可能要求一个*数据专家*、一个*数据分析师*或一个*科学程序员*。此外，某些职位帖子是为研究实习生的，我们可以假设这些实习是数据驱动的。但并非所有样本工作都与我们的直接职业目标完全相关：多个帖子是为管理职位的，这与我们的直接职业目标不符。管理是一个独立的职业路径，需要其独特的技能集。我们应该考虑从我们的分析中排除管理职位。
- en: More troublingly, the first posting on the list is for a *Patient Care Assistant*
    or *PCA*. Clearly, this posting has been crawled erroneously. Perhaps the crawling
    algorithm confused the job title with the PCA data-reduction technique. The erroneous
    posting contains skills that we lack and also have no interest in obtaining. These
    irrelevant skills pose a danger to our analysis and will serve as a source of
    noise if not removed. We can illustrate this danger by printing the first five
    bullets of `df_non_ds_jobs[0]`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更令人不安的是，列表中的第一份发布信息是关于一名*患者护理助理*或*PCA*。显然，这份发布信息被错误地抓取了。也许抓取算法将职位名称与PCA数据缩减技术混淆了。这份错误发布的信息包含我们缺乏且没有兴趣获得的技能。这些无关的技能对我们的分析构成了危险，如果不删除，将成为噪声的来源。我们可以通过打印`df_non_ds_jobs[0]`的前五个项目符号来说明这种危险。
- en: Listing 17.11 Sampling bullets from a non-data science job
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.11 从非数据科学职位中采样项目符号
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We are data scientists; our primary objective isn’t patient care (index 0) or
    nursing equipment maintenance (index 4). We need to delete these skills from our
    dataset, but how? One approach is to use text similarity. We could compare the
    postings to our resume and delete the jobs that don’t align with our resume content.
    Also, we should consider comparing the postings with this book’s table of contents
    for added signal. Basically, we should evaluate the relevance of each job relative
    to both the resume and book material; this would allow us to filter the extraneous
    postings and retain only the most relevant jobs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是数据科学家；我们的主要目标不是患者护理（索引 0）或护理设备维护（索引 4）。我们需要从我们的数据集中删除这些技能，但如何操作呢？一种方法是使用文本相似度。我们可以将职位发布信息与我们的简历进行比较，并删除与简历内容不一致的工作。此外，我们还应该考虑将职位发布信息与本书的目录进行比较，以增加信号。基本上，我们应该评估每个职位相对于简历和书籍材料的相关性；这将使我们能够过滤掉无关的发布信息，仅保留最相关的职位。
- en: 'Alternatively, we could consider filtering the individual skills contained
    in the bullet points. Basically, we’d rank the individual bullet points rather
    than individual jobs. But there’s a problem with this second approach. Imagine
    if we filter out all bullets that don’t align with our resume or book material:
    the bullets that remain will cover skills that we already possess. This is counter
    to our goal of uncovering our missing skills using relevant data science postings.
    Instead, we should accomplish our goal as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以考虑过滤掉项目符号中包含的个别技能。基本上，我们会给个别项目符号排名，而不是个别工作。但第二种方法存在一个问题。想象一下，如果我们过滤掉所有与我们的简历或书籍材料不一致的项目符号：剩下的项目符号将涵盖我们已掌握的技能。这与我们的目标相悖，即通过相关数据科学发布信息来揭示我们缺失的技能。相反，我们应该按照以下方式完成我们的目标：
- en: Obtain relevant job postings that partially match our existing skill set.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取与我们的现有技能集部分匹配的相关职位发布信息。
- en: Examine which bullet points in these postings are missing from our existing
    skill set.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查这些发布信息中哪些项目符号缺少在我们的现有技能集中。
- en: With this strategy in mind, we’ll now filter the jobs by relevance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 思考这个策略，我们现在将根据相关性过滤工作。
- en: 17.2 Filtering jobs by relevance
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 通过相关性过滤工作
- en: Our goal is to evaluate job relevance using text similarity. We want to compare
    the text in each posting to our resume and/or the book’s table of contents. In
    preparation, let’s store our resume in a `resume` string.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是使用文本相似度来评估职位的相关性。我们希望将每份发布信息中的文本与我们的简历和/或本书的目录进行比较。为此，让我们将我们的简历存储在一个`resume`字符串中。
- en: Listing 17.12 Loading the resume
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.12 加载简历
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this same manner, we can store the table of contents in a `table_of_contents`
    string.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，我们可以将目录存储在一个`table_of_contents`字符串中。
- en: Listing 17.13 Loading the table of contents
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.13 加载目录
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Together, `resume` and `table_of_contents` summarize our existing skill set.
    Let’s concatenate these skills into a single `existing_skills` string.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`resume`和`table_of_contents`一起总结了我们的现有技能集。让我们将这些技能连接成一个单一的`existing_skills`字符串。'
- en: Listing 17.14 Combining skills into a single string
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.14 将技能合并成一个字符串
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our task is to compute the text similarity between each job posting and our
    existing skills. In other words, we want to compute all similarities between `df_jobs.Body`
    and `existing_skills.` This computation first requires that we vectorize all texts.
    We need to vectorize `df_jobs.Body` together with `existing_skills` to ensure
    that all vectors share the same vocabulary. Next, we combine our job posts and
    our skill string into a single list of texts and vectorize these texts using scikit-learn’s
    `TfidfVectorizer` implementation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是计算每个职位发布与我们的现有技能之间的文本相似度。换句话说，我们想要计算`df_jobs.Body`和`existing_skills.`之间的所有相似度。这种计算首先需要将所有文本向量化。我们需要将`df_jobs.Body`和`existing_skills`一起向量化，以确保所有向量共享相同的词汇表。接下来，我们将我们的职位发布和技能字符串合并成一个单独的文本列表，并使用scikit-learn的`TfidfVectorizer`实现向量化这些文本。
- en: Listing 17.15 Vectorizing our skills and the job-posting data
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.15 向量化我们的技能和职位发布数据
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Our vectorized texts are stored in a matrix format in `tfidf_matrix`. The final
    matrix row (`tfidf_matrix[-1]`) corresponds to our existing skill set, and all
    the other rows in `(tfidf_matrix[:-1])` correspond to the job postings. Thus,
    we can easily compute the cosine similarities between the job postings and `existing_skills`.
    We simply need to execute `tfdf_matrix[:-1] @ tfidf_matrix[-1]`: this matrix-vector
    product returns an array of cosine similarities. Listing 17.16 computes that `cosine_similarities`
    array.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向量化后的文本存储在`tfidf_matrix`矩阵格式中。最终的矩阵行（`tfidf_matrix[-1]`）对应于我们的现有技能集，而`(tfidf_matrix[:-1])`中的所有其他行对应于职位发布。因此，我们可以轻松地计算职位发布与`existing_skills`之间的余弦相似度。我们只需执行`tfidf_matrix[:-1]
    @ tfidf_matrix[-1]`：这个矩阵-向量积返回一个余弦相似度数组。列表17.16计算了`cosine_similarities`数组。
- en: Note You may wonder if it’s worthwhile to visualize the distribution of rankings.
    The answer is yes! We will plot this distribution shortly to obtain valuable insights,
    but first we want to carry out a simple sanity check by printing the top-ranking
    job titles. Doing so will confirm that our hypothesis is correct and all the printed
    jobs are relevant.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能想知道可视化排名分布是否有价值。答案是肯定的！我们很快就会绘制这个分布以获得有价值的见解，但首先我们想要通过打印排名最高的职位标题来进行简单的合理性检查。这样做将确认我们的假设是正确的，并且所有打印的职位都是相关的。
- en: Listing 17.16 Computing skill-based cosine similarities
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.16 计算基于技能的余弦相似度
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The cosine similarities capture the text overlap between our existing skills
    and the posted jobs. Jobs with greater overlap are more relevant, and jobs with
    lesser overlap are less relevant. Thus, we can use cosine similarities to rank
    jobs by relevance. Let’s carry out the ranking. First, we need to store the cosine
    similarities in a `Relevance` column of `df_jobs`. Then, we sort the table by
    `df_jobs.Relevance` in descending order. Finally, we print the 20 least relevant
    job titles in the sorted table and confirm whether these low-ranking jobs have
    anything to do with data science.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度捕捉了我们现有技能与发布的职位之间的文本重叠。重叠度更高的职位更相关，重叠度较低的职位则不太相关。因此，我们可以使用余弦相似度来按相关性对职位进行排序。让我们进行排序。首先，我们需要将余弦相似度存储在`df_jobs`的`Relevance`列中。然后，我们按`df_jobs.Relevance`降序排列表格。最后，我们打印出排序表中排名最低的20个职位标题，并确认这些排名靠后的职位是否与数据科学有关。
- en: Listing 17.17 Printing the 20 least relevant jobs
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.17 打印出20个最不相关的职位
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Most of the printed jobs are completely irrelevant. Various extraneous employment
    opportunities include *Leadership and Advocacy Coordinator*, *Financial Consultant*,
    *RN* (registered nurse), and *Scorekeeper*. One of the job titles even reads *Page
    Not Found*, which indicates that the web page was not downloaded correctly. However,
    a few of the jobs are related to data science: for instance, one calls for a *Part-time
    instructor of Statistics and Data Science and Machine Learning*. Still, this job
    is not exactly what we’re looking for. After all, our immediate goal is to practice
    data science, not teach it. We can discard the 20 lowest-ranking jobs in the sorted
    table. Now, for comparison’s sake, let’s print the 20 most relevant job titles
    in `sorted_ds_jobs`.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出的大多数职位都完全不相关。各种无关的就业机会包括*领导倡导协调员*、*财务顾问*、*注册护士*（RN）和*记分员*。甚至有一个职位标题是*页面未找到*，这表明网页没有正确下载。然而，其中一些职位与数据科学相关：例如，有一个职位要求招聘*统计学和数据科学及机器学习兼职讲师*。尽管如此，这个职位并不是我们真正想要的。毕竟，我们的直接目标是练习数据科学，而不是教授它。我们可以丢弃排序表中排名最低的20个职位。现在，为了进行比较，让我们打印出`sorted_ds_jobs`中排名最高的20个职位标题。
- en: Listing 17.18 Printing the 20 most relevant jobs
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.18 打印出20个最相关的职位
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Almost all of the printed job titles are for data science jobs. Some jobs, such
    as *Chief Data Officer*, probably lie beyond our existing level of expertise,
    but the top-ranking jobs appear to be quite relevant to our data science career.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有打印的职位名称都是数据科学职位。一些职位，如 *首席数据官*，可能超出了我们现有的专业知识水平，但排名最高的职位似乎与我们数据科学职业非常相关。
- en: Note Based on its title, the position of *Chief Data Officer* appears to be
    a management position. As we stated earlier, a management position requires its
    own separate set of skills. However, if we output the job posting’s body (`sorted_df_jobs.iloc[0].Body`),
    we immediately discover that the job isn’t a management job at all! The company
    is simply searching for a highly experienced data scientist to cover all its data
    science needs. Sometimes job titles can be deceiving; glancing briefly over a
    title cannot fully substitute for carefully reading the body text.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注：根据其标题，首席数据官（*Chief Data Officer*）的位置看起来是一个管理职位。正如我们之前所述，管理职位需要一套独特的技能。然而，如果我们输出职位描述的主体（`sorted_df_jobs.iloc[0].Body`），我们会立即发现这份工作根本不是管理职位！公司只是在寻找一位经验丰富的数据科学家来满足其所有的数据科学需求。有时职位名称可能会误导人；仅仅浏览标题并不能完全替代仔细阅读正文。
- en: Clearly, when `df_jobs.Relevance` is high, the associated job postings are relevant.
    As `df_jobs.Relevance` decreases, the associated jobs become less relevant. Thus,
    we can presume that there exists some `df_jobs.Relevance` cutoff that separates
    the relevant jobs from the non-relevant jobs. Let’s try to identify that cutoff.
    We start by visualizing the shape of the sorted relevance distribution relative
    to rank. In other words, we plot `range(df_jobs.shape[0])` versus `sorted_df_jobs.Relevance`
    (figure 17.3). In the plot, we expect to see a relevance curve that’s continuously
    decreasing; any sudden decreases of relevance in the curve indicate a separation
    between relevant and non-relevant jobs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，当 `df_jobs.Relevance` 较高时，相关的职位发布是相关的。随着 `df_jobs.Relevance` 的降低，相关的职位变得不那么相关。因此，我们可以假设存在某个
    `df_jobs.Relevance` 截止值，将相关职位与非相关职位分开。让我们尝试识别这个截止值。我们首先可视化排序后的相关性分布相对于排名的形状。换句话说，我们绘制
    `range(df_jobs.shape[0])` 与 `sorted_df_jobs.Relevance` 的关系图（图 17.3）。在图表中，我们期望看到一个持续下降的相关性曲线；曲线中任何突然的相关性下降都表明相关职位与非相关职位之间的分离。
- en: Listing 17.19 Plotting job ranking vs. relevance
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.19 绘制职位排名与相关性的关系
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/17-03.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-03.png)'
- en: Figure 17.3 Ranked job posting indices plotted vs. relevance. Lower indices
    indicate higher relevance. The relevance is equal to the cosine similarity between
    each job and `existing_skills`. This relevance drops rapidly at an index of approximately
    60.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3 职位发布排名索引与相关性的关系图。较低的索引表示较高的相关性。相关性等于每个职位与 `existing_skills` 之间的余弦相似度。相关性在约
    60 的索引处迅速下降。
- en: Our relevance curve resembles a K-means elbow plot. Initially, the relevance
    drops rapidly. Then, at an x-value of approximately 60, the curve begins to level
    off. Let’s emphasize this transition by drawing a vertical line through the x-position
    of 60 in our plot (figure 17.4).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的相关性曲线类似于 K-means 肘部图。最初，相关性迅速下降。然后，在 x 值约为 60 时，曲线开始平缓。让我们通过在图表中 x 位置 60
    处画一条垂直线来强调这一转变（图 17.4）。
- en: Listing 17.20 Adding a cutoff to the relevance plot
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.20 在相关性图中添加截止值
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/17-04.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-04.png)'
- en: Figure 17.4 Ranked job posting indices are plotted vs. relevance. A vertical
    cutoff of 60 is also included in the plot. Indices below 60 correspond to much
    higher relevance values.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.4 职位发布排名索引与相关性的关系图。图中还包括了 60 的垂直截止值。低于 60 的索引对应于更高的相关性值。
- en: 'Our plot implies that the first 60 jobs are noticeably more relevant than all
    subsequent jobs. We’ll now probe this implication. As we’ve already seen, the
    first 20 jobs are highly relevant. Based on our hypothesis, jobs 40 through 60
    should be highly relevant as well. Next, we print `sorted_ds_jobs[40: 60].Title`
    for evaluation purposes.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的图表表明，前 60 个职位与所有后续职位相比明显更具相关性。现在，我们将探究这一暗示。正如我们已经看到的，前 20 个职位高度相关。根据我们的假设，40
    到 60 之间的职位也应该高度相关。接下来，为了评估目的，我们打印 `sorted_ds_jobs[40: 60].Title`。'
- en: Listing 17.21 Printing jobs below the relevance cutoff
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.21 打印低于相关性截止值的职位
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Almost all of the printed jobs are for data scientist/analyst positions. The
    only outlier is a posting for an epidemiologist, which probably appeared due to
    our stated experience of tracking disease epidemics. The outlier notwithstanding,
    the remaining jobs are highly relevant. Implicitly, the relevance should decrease
    when we print the next 20 job titles since they lie beyond the bounds of index
    60\. Let’s verify if this is the case.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有发布的职位都是数据科学家/分析师职位。唯一的异常是一个流行病学家职位，这可能是由于我们声明的跟踪疾病流行的经验。尽管有异常，剩余的工作相关性仍然很高。隐含地，当我们打印下一个
    20 个工作标题时，相关性应该会降低，因为它们超出了索引 60 的范围。让我们验证一下这是否属实。
- en: Listing 17.22 Printing jobs beyond the relevance cutoff
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.22 打印超出相关性截止值的工作
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'A few of the job titles for postings 60 through 80 are noticeably less relevant.
    Some jobs are management positions, and one is a health science specialist position.
    Nonetheless, a majority of the jobs refer to data science/analyst roles outside
    the scope of health science or management. We can quickly quantify this observation
    using regular expressions. We define a `percent_relevant_titles` function, which
    returns the percent of non-management data science and analysis jobs in a data
    frame slice. Then we apply that function to `sorted_df_jobs[60: 80]`. The output
    gives us a very simple alternative measure of relevance based on job post titles.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '60 到 80 号发布的几个工作标题的相关性明显较低。有些工作是管理职位，还有一个是健康科学专家职位。尽管如此，大多数工作都涉及健康科学或管理范围之外的数据科学/分析师角色。我们可以快速使用正则表达式量化这一观察结果。我们定义了一个
    `percent_relevant_titles` 函数，它返回数据帧切片中非管理数据科学和分析工作的百分比。然后我们将该函数应用于 `sorted_df_jobs[60:
    80]`。输出给出了基于工作发布标题的非常简单的相关性替代度量。'
- en: Listing 17.23 Measuring title relevance in a subset of jobs
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.23 在工作子集中测量标题相关性
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Matches relevant job titles that mention data science/analyst positions
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 匹配提及数据科学/分析师职位的相关工作标题
- en: ❷ Matches irrelevant job titles that mention management positions
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 匹配提及管理职位的不相关工作标题
- en: ❸ Counts the number of non-management data science/analyst title matches
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算非管理数据科学/分析师标题匹配的数量
- en: 'Approximately two-thirds of the job titles in `sorted_df_jobs[60: 80]` are
    relevant. Although the job relevance has decreased beyond index 60, more than
    50% of the titles still refer to data science jobs. Perhaps that percentage will
    drop if we sample the next 20 jobs across an index range of 80 to 100\. Let’s
    check.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`sorted_df_jobs[60: 80]` 中的工作标题中有大约三分之二的相关性。尽管工作相关性在索引 60 之后有所下降，但仍有超过 50%
    的标题指的是数据科学工作。如果我们从索引范围 80 到 100 样本下一个 20 个工作，这个百分比可能会下降。让我们检查一下。'
- en: Listing 17.24 Measuring title relevance in the next subset of jobs
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.24 在下一个工作子集测量标题相关性
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Nope! The data science title percentage rose to 80%. At what point will the
    percentage drop below 50%? We can easily find out! Let’s iterate over `sorted_df_jobs[i:
    i + 20]` for all values of `i`. At every iteration, we compute the relevance percentage.
    Then we plot all the percentages (figure 17.5). We also plot a horizontal line
    at 50% to allow us to determine the index at which relevant job titles fall into
    the minority.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '没错！数据科学标题的百分比上升到 80%。百分比将在何时降至 50% 以下？我们可以轻松地找出答案！让我们遍历 `sorted_df_jobs[i:
    i + 20]` 的所有 `i` 值。在每次迭代中，我们计算相关性百分比。然后我们绘制所有百分比（图 17.5）。我们还绘制了一条 50% 的水平线，以便我们确定相关工作标题落入少数的索引。'
- en: Listing 17.25 Plotting percent relevance across all title samples
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.25 绘制所有标题样本的相关性百分比
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ The function runs percent_relevant_titles across each consecutive slice of
    index_range jobs. Next, all the percentages are plotted. The index_range parameter
    is preset to 20\. Later, we adjust that parameter value.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 函数在索引范围的工作的每个连续切片上运行 `percent_relevant_titles`。接下来，绘制所有百分比。`index_range` 参数预设为
    20。稍后，我们将调整该参数值。
- en: '❷ Analyzes sorted_df_jobs[i: i + index_range], where i ranges from 0 to the
    total posting count minus the index range'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ 分析 `sorted_df_jobs[i: i + index_range]`，其中 i 的范围从 0 到总发布数减去索引范围'
- en: '![](../Images/17-05.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-05.png)'
- en: Figure 17.5 Ranked job posting indices plotted vs. title relevance. Title relevance
    is equal to the percent of data science titles across 20 consecutive job postings
    (starting at some index). A horizontal line demarcates 50% relevance. The relevance
    drops below 50% at an index of approximately 700.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5 按标题相关性排序的工作发布索引与标题相关性对比。标题相关性等于 20 个连续工作发布中数据科学标题的百分比（从某个索引开始）。一条水平线标志着
    50% 的相关性。相关性在约 700 个索引处降至 50% 以下。
- en: The plot fluctuates with a high degree of variance. But despite the fluctuations,
    we can observe that the relevant data science titles drop below 50% at an index
    of around 700\. Of course, it’s possible that the cutoff of 700 is merely an artifact
    of our chosen index range. Will the cutoff still be present if we double our index
    range? We’ll find out by running `relevant_title_plot(index_range=40)` (figure
    17.6). We also plot a vertical line at index 700 to confirm that the percentage
    drops below 50% beyond that line.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图表波动幅度很大。但尽管有波动，我们可以观察到相关的数据科学标题在约700的索引处下降到50%以下。当然，700的截止点可能只是我们选择的索引范围的一个偶然现象。如果我们将索引范围加倍，截止点还会存在吗？我们将通过运行`relevant_title_plot(index_range=40)`（图17.6）来找出答案。我们还绘制了一条垂直线在索引700处，以确认百分比在超过该线的地方下降到50%以下。
- en: Listing 17.26 Plotting percent relevance across an increased index range
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.26 在增加的索引范围内绘制百分比相关性
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Our updated plot continues to drop below 50% at an index cutoff of 700.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新的图表继续在索引截止点700以下下降到50%以下。
- en: '![](../Images/17-06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-06.png)'
- en: Figure 17.6 Ranked job-posting indices plotted vs. title relevance. Title relevance
    is equal to the percent of data science titles across 40 consecutive job postings
    (starting at some index). A horizontal line demarcates 50% relevance, and a vertical
    line demarcates an index of 700\. Below that line, relevance drops to less than
    50%.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6 职位发布索引排名与标题相关性的对比。标题相关性等于40个连续职位发布中数据科学标题的百分比（从某个索引开始）。一条水平线表示50%的相关性，一条垂直线表示索引700。在这条线以下，相关性下降到50%以下。
- en: Note There’s more than one way to approximate that cutoff. Suppose, for instance,
    that we simplify our regex to `r'Data (Science|Scientist)'`. We thus ignore all
    mentions of analysts or managers. Also, suppose we eliminate the use of index
    ranges and instead count the total number of data science titles appearing below
    each index. If we plot these simple results, we see a curve that levels off at
    an index of 700\. Despite our simplifications, we achieve very similar results.
    In data science, there’s frequently more than one path toward an insightful observation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：近似该截止点的方法不止一种。例如，我们可以简化我们的正则表达式为`r'Data (Science|Scientist)'`。这样我们就忽略了所有关于分析师或经理的提及。此外，假设我们消除使用索引范围，而是计算每个索引以下出现的数据科学标题的总数。如果我们绘制这些简单结果，我们会看到一条在索引700处趋于平稳的曲线。尽管我们的简化，我们仍然取得了非常相似的结果。在数据科学中，通常有不止一条通往有洞察力的观察的路径。
- en: 'At this point, we face a choice between two relevance cutoffs. Our first cutoff,
    at index 60, is highly precise: most jobs below that cutoff are data science positions.
    However, the cutoff has limited recall: hundreds of data science jobs appear beyond
    an index of 60\. Meanwhile, our second cutoff of 700 captures many more data science
    positions, but some irrelevant jobs also appear below the cutoff range. There’s
    almost a 12-fold difference between the two relevance cutoffs. So, which cutoff
    do we choose? Do we prefer higher precision or higher recall? If we choose higher
    recall, will the noise hurt our analysis? If we choose higher precision, will
    the limited diversity of seen skills render our analysis incomplete? These are
    all important questions. Unfortunately, there’s no immediate right answer. Higher
    precision at the expense of recall could potentially hurt us, and vice versa.
    What should we do?'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们面临两个相关性截止点的选择。我们的第一个截止点，在索引60处，非常精确：大多数低于该截止点的职位都是数据科学职位。然而，该截止点的召回率有限：数百个数据科学职位出现在索引60以上。同时，我们的第二个截止点700捕捉到了更多的数据科学职位，但也有一些不相关的职位出现在截止点范围内。两个相关性截止点之间几乎有12倍的区别。那么，我们应该选择哪个截止点？我们更倾向于更高的精确度还是更高的召回率？如果我们选择更高的召回率，噪声会损害我们的分析吗？如果我们选择更高的精确度，看到的技能的有限多样性会使我们的分析不完整吗？这些都是重要的问题。不幸的是，没有立即正确的答案。以牺牲召回率为代价的更高精确度可能会对我们造成伤害，反之亦然。我们应该怎么做？
- en: How about trying both cutoffs? That way, we can compare the trade-offs and benefits
    of each! First, we’ll cluster the skill sets from job postings below an index
    of 60\. Then, we’ll repeat our analysis for job postings below an index of 700\.
    Finally, we’ll integrate these two different analyses into a single, coherent
    conclusion.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么尝试两个截止点怎么样？这样，我们可以比较每个截止点的权衡和好处！首先，我们将对索引60以下的职位发布中的技能集进行聚类。然后，我们将对索引700以下的职位发布重复我们的分析。最后，我们将将这些不同的分析整合成一个单一、连贯的结论。
- en: 17.3 Clustering skills in relevant job postings
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 相关职位发布中的聚类技能
- en: 'Our aim is to cluster the skills in the 60 most relevant job postings. The
    skills in each posting are diverse and partially represented by bullet points.
    Thus we face a choice:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是对60个最相关的工作帖子中的技能进行聚类。每个帖子中的技能多种多样，部分由项目点表示。因此，我们面临一个选择：
- en: Cluster the 60 texts in `sorted_df_jobs[:60].Body`.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`sorted_df_jobs[:60].Body`中的60个文本进行聚类。
- en: Cluster the hundreds of individual bullet points in `sorted_df_jobs[:60] .Bullets`.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`sorted_df_jobs[:60].Bullets`中的数百个单独的项目点进行聚类。
- en: 'The second option is preferable for the following reasons:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择更可取，以下是一些原因：
- en: Our stated aim is to identify missing skills. The bullet points focus more on
    individual skills than the heterogeneous body of each posting.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的目标是识别缺失的技能。项目点更多地关注每个帖子中的个人技能，而不是每个帖子的异质体。
- en: The short bullet points are easy to print and read. This is not the case for
    the larger postings. Thus, clustering by bullets allows us to examine each cluster
    by outputting a sample of the clustered bullet text.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短的项目点易于打印和阅读。对于较大的帖子来说并非如此。因此，通过项目点进行聚类使我们能够通过输出聚类项目文本的样本来检查每个聚类。
- en: We’ll cluster the scraped bullets. We start by storing `sorted_df_jobs[:60].Bullets`
    in a single list.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将聚类抓取的项目点。我们首先将`sorted_df_jobs[:60].Bullets`存储在一个单独的列表中。
- en: Listing 17.27 Obtaining bullets from the 60 most relevant jobs
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.27 从60个最相关的职位中获取项目点
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How many bullets are in the list? Are any of the bullets duplicated? We can
    check by loading `total_bullets` into a Pandas table and then applying the `describe`
    method.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中有多少个项目点？是否有重复的项目点？我们可以通过将`total_bullets`加载到Pandas表中并应用`describe`方法来检查。
- en: Listing 17.28 Summarizing basic bullet statistics
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.28 总结基本的项目点统计信息
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The list contains 1,091 bullets. However, only 900 are unique—the remaining
    91 bullets are duplicates. The most frequent duplicate is mentioned nine times.
    If we don’t deal with this issue, it could affect our clustering. We should remove
    all duplicate texts before proceeding with our analysis.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中包含1,091个项目点。然而，只有900个是唯一的——剩余的91个项目点是重复的。最频繁的重复项被提及了九次。如果我们不处理这个问题，它可能会影响我们的聚类。在进行分析之前，我们应该移除所有重复文本。
- en: Note Where do the duplicates originate? We can find out by tracing back a few
    duplicates to their original job posts. For brevity’s sake, this analysis is not
    included in the book. However, you’re encouraged to try it yourself. The output
    shows how certain companies reuse job templates for different jobs. Each template
    is modified for each position, but certain repeated bullet points remain. These
    repeated bullet points could bias our clusters toward company-specific skills
    and thus should be removed from `total_bullets`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些重复项从何而来？我们可以通过追踪几个重复项到它们原始的工作帖子中找到答案。为了简洁起见，这项分析不包括在书中。然而，我们鼓励你自己尝试。输出显示了某些公司如何为不同的工作重复使用工作模板。每个模板都会为每个职位进行修改，但某些重复的项目点仍然保留。这些重复的项目点可能会使我们的聚类偏向于特定公司的技能，因此应该从`total_bullets`中移除。
- en: Next, we filter empty strings and duplicates from our bullet list. Then we vectorize
    the list using a TFIDF vectorizer.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从我们的项目点列表中过滤掉空字符串和重复项。然后我们使用TFIDF向量器对列表进行向量化。
- en: Listing 17.29 Removing duplicates and vectorizing the bullets
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.29 移除重复项并对项目点进行向量化
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Converts total_bullets into a set to remove the 91 duplicates. We sort that
    set to ensure consistent ordering (and thus consistent output). Alternatively,
    we can drop the duplicates directly from our Pandas table by running df_bullets.drop_duplicates(inplace=True).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将`total_bullets`转换为集合以移除91个重复项。我们对该集合进行排序以确保一致的顺序（从而保证一致的输出）。或者，我们可以通过运行`df_bullets.drop_duplicates(inplace=True)`直接从我们的Pandas表中删除重复项。
- en: 'We’ve vectorized our deduplicated bullet list. The resulting TFIDF matrix has
    900 rows and over 2,000 columns; it thus contains over 1.8 million elements. This
    matrix is too large for efficient clustering. Let’s dimensionally reduce the matrix
    using the procedure described in section 15: we’ll shrink the matrix to 100 dimensions
    with SVD, and then we’ll normalize the matrix.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将去重后的项目点列表进行了向量化。得到的TFIDF矩阵有900行和超过2000列；因此，它包含超过180万个元素。这个矩阵对于有效的聚类来说太大。让我们使用第15节中描述的程序对矩阵进行降维：我们将使用SVD将矩阵缩小到100维，然后对矩阵进行归一化处理。
- en: Listing 17.30 Dimensionally reducing the TFIDF matrix
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.30 对TFIDF矩阵进行降维
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Applies SVD to an inputted TFIDF matrix. The matrix is reduced to 100 dimensions,
    normalized, and returned.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对输入的TFIDF矩阵应用奇异值分解（SVD）。矩阵被降至100维，进行归一化处理并返回。
- en: We are nearly ready to cluster our normalized matrix using K-means. However,
    first we need to estimate *K*. Let’s generate an elbow plot using mini-batch K-means,
    which is optimized for speed (figure 17.7).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好使用K-means算法对标准化矩阵进行聚类了。然而，首先我们需要估计*K*值。让我们使用迷你批量的K-means算法生成一个肘部图，该算法针对速度进行了优化（图17.7）。
- en: Listing 17.31 Plotting an elbow curve using mini-batch K-means
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.31 使用迷你批量的K-means算法绘制肘部曲线
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Generates an elbow plot across an inputted data matrix using mini-batch K-means
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用迷你批量的K-means算法在输入数据矩阵上生成肘部图
- en: ❷ The number of clusters ranges from 1 to 60.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 聚类的数量从1到60不等。
- en: ❸ Plots grid lines to help us identify where the elbow lies on the x-axis
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制网格线以帮助我们确定肘部在x轴上的位置
- en: '![](../Images/17-07.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-07.png)'
- en: Figure 17.7 An elbow plot generated using mini-batch K-means, across *K* values
    ranging from 1 to 60\. The precise location of the elbow is difficult to determine.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7 使用迷你批量的K-means算法生成的肘部图，*K*值从1到60不等。肘部的确切位置难以确定。
- en: 'Our plotted curve decreases smoothly. The precise location of a bent elbow–shaped
    transition is difficult to spot: that curve drops rapidly at a *K* of 10 and then
    gradually bends into an elbow somewhere between a *K* of 10 and a *K* of 25\.
    Which *K* value should we choose? 10, 25, or some value in between, such as 15
    or 20? The right answer is not immediately clear. So why not try multiple values
    of *K*? Let’s cluster our data multiple times using *K* values of 10, 15, 20,
    and 25\. Then we’ll compare and contrast the results. If necessary, we’ll consider
    choosing a different *K* for clustering. We’ll start by grouping our job skills
    into 15 clusters.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制的曲线平滑下降。弯曲肘部形状的精确位置难以辨认：该曲线在*K*值为10时急剧下降，然后逐渐弯曲到10和25之间的某个位置。我们应该选择哪个*K*值？10，25，还是介于两者之间的某个值，比如15或20？正确的答案并不立即明朗。那么为什么不尝试多个*K*值呢？让我们使用10，15，20和25的*K*值多次聚类我们的数据。然后我们将比较和对比结果。如果需要，我们将考虑为聚类选择不同的*K*值。我们将首先将工作技能分组到15个聚类中。
- en: Note Our aim is to investigate outputs for four different values of *K*. The
    order in which we generate the outputs is completely arbitrary. In this book,
    we start with a *K* value of 15 because the resulting cluster count is not too
    large and not too small. This sets a nice baseline for the subsequent discussion
    of the outputs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们的目标是调查四个不同*K*值输出的结果。我们生成输出的顺序是完全随机的。在这本书中，我们从*K*值为15开始，因为生成的聚类数量既不太大也不太小。这为后续讨论输出结果提供了一个很好的基线。
- en: 17.3.1 Grouping the job skills into 15 clusters
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.1 将工作技能分组到15个聚类中
- en: We execute K-means using a *K* of 15\. Then we store the text indices and cluster
    IDs in a Pandas table. We also store the actual bullet text for easier accessibility.
    Finally, we utilize the Pandas `groupby` method to split the table by cluster.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用*K*值为15执行K-means算法。然后我们将文本索引和聚类ID存储在一个Pandas表中。我们还存储了实际的子弹文本，以便更容易访问。最后，我们利用Pandas的`groupby`方法按聚类分割表。
- en: Listing 17.32 Clustering bullets into 15 clusters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.32 将子弹聚类到15个聚类中
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Executes K-means clustering on the input shrunk_norm_matrix. The K parameter
    is preset to 15\. The function returns a list of Pandas tables, where each table
    represents a cluster. Clustered bullets are included in these tables; the bullets
    were passed in through an optional bullets parameter.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在输入的shrink_norm_matrix上执行K-means聚类。K参数预设为15。该函数返回一个Pandas表的列表，其中每个表代表一个聚类。聚类后的子弹包含在这些表中；子弹是通过可选的bullets参数传递的。
- en: ❷ Tracks each clustered bullet’s index in clusters, cluster ID, and text
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 跟踪每个聚类子弹的索引、聚类ID和文本
- en: Each of our text clusters is stored as a Pandas table in the `cluster_groups`
    list. We can visualize the clusters using word clouds. In section 15, we defined
    a custom `cluster_to_image` function for word cloud visualization. The function
    took as input a cluster-specific Pandas table and returned a word cloud image.
    Listing 17.33 redefines that function and applies it to `cluster_groups[0]` (figure
    17.8).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的每个文本聚类都存储在`cluster_groups`列表中的Pandas表中。我们可以使用词云可视化聚类。在第15节中，我们定义了一个自定义的`cluster_to_image`函数用于词云可视化。该函数接受一个特定聚类的Pandas表作为输入，并返回一个词云图像。列表17.33重新定义了该函数并将其应用于`cluster_groups[0]`（图17.8）。
- en: '![](../Images/17-08.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-08.png)'
- en: Figure 17.8 A word cloud generated for the cluster at index 0\. The language
    in the word cloud is a little vague. It appears to be describing a focused, data-oriented
    personality.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8 为索引0的聚类生成的词云。词云中的语言有些模糊。它似乎在描述一个专注、以数据为导向的性格。
- en: Note Why should we redefine the function? Well, in section 15, `cluster_to_image`
    depended on a fixed TFIDF matrix and vocabulary list. In our current analysis,
    these parameters are not fixed—both the matrix and vocabulary will shift as we
    adjust our relevancy index. Thus, we need to update the function to allow for
    more dynamic input.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为什么我们要重新定义函数？好吧，在第15节中，`cluster_to_image`依赖于一个固定的TFIDF矩阵和词汇表。在我们的当前分析中，这些参数不是固定的——矩阵和词汇表将随着我们调整相关性指数而变化。因此，我们需要更新函数以允许更动态的输入。
- en: Listing 17.33 Visualizing the first cluster
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.33 可视化第一个簇
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Takes as input a df_cluster table and returns a word cloud image for the top
    max_words words corresponding to the cluster. The words are obtained from the
    inputted vectorizer class. They are ranked by summing over the rows of the inputted
    tfidf_matrix. When we extend our job threshold from 60 to 700, both vectorizer
    and tfidf_matrix must be adjusted accordingly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以df_cluster表作为输入，并返回对应于簇的top max_words个单词的词云图像。单词来自输入的vectorizer类。它们通过输入的tfidf_matrix的行求和进行排序。当我们将工作阈值从60扩展到700时，vectorizer和tfidf_matrix都必须相应调整。
- en: ❷ Helper function to randomly assign one of five acceptable colors to each word
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 辅助函数，随机将五种可接受的颜色中的一种分配给每个单词
- en: The language in the word cloud seems to be describing someone focused and data
    oriented, but it’s a little vague. Perhaps we can learn more about the cluster
    by printing some sample bullets from `cluster_group[0]`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 词云中的语言似乎在描述一个专注且数据导向的人，但它有点模糊。也许我们可以通过打印`cluster_group[0]`的一些样本子弹来更多地了解这个簇。
- en: 'Note We’ll print a random sample of bullets. This should be sufficiently informative
    to understand the cluster. However, it’s worth emphasizing that not all bullets
    are equal: some bullets are closer to their K-means cluster centroid and therefore
    more representative of the cluster. Thus, we can optionally sort the bullets based
    on their distance to the cluster mean. In this book, we bypass bullet ranking
    for brevity’s sake, but you’re encouraged to try ranking the bullets on your own.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们将打印子弹的随机样本。这应该足够提供信息以理解簇。然而，值得强调的是，并非所有子弹都是平等的：一些子弹更接近它们的K-means簇质心，因此更能代表簇。因此，我们可以根据子弹到簇平均距离的远近进行排序。在这本书中，我们为了简洁起见跳过了子弹排名，但鼓励你自己尝试对子弹进行排名。
- en: Listing 17.34 Printing sample bullets from cluster 0
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.34 从簇0打印样本子弹
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Prints five random bullets from cluster_groups[cluster_id]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从cluster_groups[cluster_id]中打印五个随机子弹
- en: 'The printed bullets all use very similar language: they call for an employee
    who is detail oriented and data oriented. Linguistically, this cluster is legitimate.
    Unfortunately, it represents a skill that is difficult to grasp. Being detail
    oriented is a very general skill—it’s hard to quantify, demonstrate, and learn.
    Ideally, the other clusters will contain more concrete technical skills.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的子弹都使用非常相似的语言：它们呼吁招聘一个注重细节和数据导向的员工。从语言学的角度来看，这个簇是合法的。不幸的是，它代表了一种难以掌握的技能。注重细节是一个非常通用的技能——很难量化、展示和学习。理想情况下，其他簇将包含更多具体的技术技能。
- en: Let’s examine all 15 clusters simultaneously using word clouds. These word clouds
    are displayed across 15 subplots in a five-row-by-three-column grid (figure 17.9).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们同时使用词云来检查所有15个簇。这些词云在一个五行三列的网格中显示在15个子图中（图17.9）。
- en: Listing 17.35 Visualizing all 15 clusters
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.35 可视化所有15个簇
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Plots the word clouds for each cluster in cluster_groups. The word clouds
    are plotted in a num_rows by num_columns grid.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在cluster_groups中为每个簇绘制词云。词云绘制在一个num_rows行num_columns列的网格中。
- en: ❷ The **kwargs syntax allows us to pass additional parameters into the utilized
    cluster_to_image function. This way, we can modify both vectorizer and tfidf_matrix
    with ease.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ **kwargs语法允许我们将额外的参数传递给使用的cluster_to_image函数。这样，我们可以轻松地修改vectorizer和tfidf_matrix。
- en: '![](../Images/17-09.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-09.png)'
- en: Figure 17.9 15 word clouds visualized across 15 subplots. Each word cloud corresponds
    to one of 15 clusters. The subplot titles correspond to cluster IDs. Some clusters,
    like cluster 7, describe technical skills; others, like cluster 0, are less technical.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.9 在15个子图中可视化的15个词云。每个词云对应于15个簇中的一个。子图标题对应于簇ID。一些簇，如簇7，描述技术技能；其他簇，如簇0，则不太技术化。
- en: Our 15 skill clusters show a diverse collection of topics. Some of the clusters
    are highly technical. For instance, cluster 7 fixates on external data science
    libraries such as scikit-learn, Pandas, NumPy, Matplotlib, and SciPy. The scikit-learn
    library clearly dominates. Most of these libraries appear in our resume and have
    been discussed in this book. Let’s print a sample of bullets from cluster 7 and
    confirm their focus on data science libraries.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的15个技能集群展示了多样化的主题集合。其中一些集群非常技术性。例如，第7个集群专注于外部数据科学库，如scikit-learn、Pandas、NumPy、Matplotlib和SciPy。scikit-learn库明显占据主导地位。这些库中的大多数都出现在我们的简历中，并在本书中进行了讨论。让我们打印出第7个集群的一些样本项目符号，以确认它们对数据科学库的关注。
- en: Listing 17.36 Printing sample bullets from cluster 7
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.36 打印第7个集群的样本子弹
- en: '[PRE35]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Meanwhile, other clusters, like cluster 0, focus on nontechnical skills. These
    soft skills, covering business acumen, focus, strategy, communication, and collaboration,
    are clearly missing from our resume. Thus, on average, the nontechnical clusters
    should have a lower resume similarity. This line of thought leads to an interesting
    possibility: perhaps we can separate the technical clusters and soft-skill clusters
    using text similarity. The separation would allow us to more systematically examine
    each skill type. Let’s give this a shot! We’ll start by computing the cosine similarity
    between each bullet in `total_bullets` and our resume.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，其他集群，如第0个集群，专注于非技术技能。这些软技能，包括商业洞察力、专注力、策略、沟通和协作，在我们的简历中明显缺失。因此，平均而言，非技术集群的简历相似度应该较低。这种思考方式引出了一个有趣的可能性：也许我们可以通过文本相似度来区分技术集群和软技能集群。这种分离将使我们能够更系统地检查每种技能类型。让我们试试看！我们将从计算`total_bullets`中每个项目符号与我们的简历之间的余弦相似度开始。
- en: Note Why utilize just the resume rather than the combined resume and table of
    contents sorted in the `existing_skills` variable? Well, our end goal is to determine
    which skill clusters are missing from the resume. The direct similarity between
    the resume and each cluster could be useful in that regard. If the similarity
    is low, then clustered skills are not appropriately represented in the resume’s
    text.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 为什么只使用简历而不是将`existing_skills`变量中的合并简历和目录排序？嗯，我们的最终目标是确定哪些技能集群缺失在简历中。简历与每个集群之间的直接相似度在这方面可能很有用。如果相似度低，那么集群中的技能在简历文本中未得到适当体现。
- en: Listing 17.37 Computing similarities between the bullets and our resume
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.37 计算子弹与简历之间的相似度
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Computes the cosine similarities between the inputted bullet_texts and the
    resume variable
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算输入的bullet_texts与简历变量之间的余弦相似度
- en: Our `bullet_cosine_similarities` array contains the text similarities across
    all clustered bullets. For any given cluster, we can combine these cosine similarities
    into a score by taking their mean. According to our hypothesis, a technical cluster
    should have a higher mean similarity than a soft-skill similarity cluster. Let’s
    confirm if this is the case for the technical cluster 7 and the soft-skill cluster
    0.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`bullet_cosine_similarities`数组包含了所有集群子弹之间的文本相似度。对于任何给定的集群，我们可以通过取它们的平均值将这些余弦相似度组合成一个分数。根据我们的假设，技术集群的平均相似度应该比软技能相似度集群更高。让我们确认技术集群7和软技能集群0是否是这样的情况。
- en: Listing 17.38 Comparing mean resume similarities
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.38 比较平均简历相似度
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The technical cluster is 100 times more proximate to our resume than the soft-skill
    cluster. It appears that we’re on the right track! Let’s compute the average similarity
    for all 15 clusters. Then we’ll sort the clusters by their similarity score, in
    descending order. If our hypothesis is correct, technical clusters will appear
    first in the sorted results. We’ll be able to confirm by replotting the word cloud
    subplot grid. Listing 17.39 carries out the sorting and visualizes the sorted
    clusters (figure 17.10).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 技术集群与我们的简历的相似度是软技能集群的100倍。看起来我们正在正确的道路上！让我们计算所有15个集群的平均相似度。然后我们将按相似度分数降序排列这些集群。如果我们的假设正确，技术集群将首先出现在排序结果中。我们将通过重新绘制单词云子图网格来确认这一点。列表17.39执行了排序并可视化了排序后的集群（图17.10）。
- en: '![](../Images/17-10.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-10.png)'
- en: Figure 17.10 15 word clouds visualized across 15 subplots. Each word cloud corresponds
    to one of 15 clusters. The clusters are sorted by average resume similarity. The
    first two rows in the subplot grid correspond to more technical clusters.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.10 在15个子图中可视化的15个单词云。每个单词云对应于15个集群中的一个。集群按平均简历相似度排序。子图网格的前两行对应于更技术性的集群。
- en: Note We are about to sort the clusters by technical relevance. This isn’t necessarily
    required to complete the case study—it’s possible to examine each cluster individually,
    in unsorted order. However, by reordering the clusters, we can extract insights
    at a faster rate. Thus, sorting is a preferable way of simplifying our workflow.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们即将按技术相关性对集群进行排序。这并不是完成案例研究所必需的——可以单独、无序地检查每个集群。然而，通过重新排序集群，我们可以更快地提取见解。因此，排序是简化我们工作流程的一种更可取的方式。
- en: Listing 17.39 Sorting subplots by resume similarity
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.39 按简历相似度对子图进行排序
- en: '[PRE38]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Sorts the inputted cluster_groups array by their mean cosine similarity to
    the resume
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 按输入的 cluster_groups 数组与简历的平均余弦相似度进行排序
- en: Our hypothesis was right! The first two rows in the updated subplot clearly
    correspond to technical skills. Furthermore, these technical skills are now conveniently
    sorted based on their similarity to our resume. This allows us to systematically
    rank the skills from most similar (and thus represented by our resume) to least
    similar (and thus likely to be missing from our resume).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设是正确的！更新后的子图中的前两行明显对应着技术技能。此外，这些技术技能现在根据它们与我们的简历的相似度方便地排序。这使得我们可以系统地按相似度从高到低（因此由我们的简历表示）到低（因此可能缺少在我们的简历中）对技能进行排名。
- en: 17.3.2 Investigating the technical skill clusters
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.2 调查技术技能集群
- en: Let’s turn our attention to the six technical-skill clusters in the first two
    rows of the subplot grid. Next, we replot their associated word clouds in a two-row-by-three-column
    grid (figure 17.11). This technically focused visualization will allow us to expand
    the size of the word cloud. Later, we return to the remaining soft-skill word
    clouds in figure 17.10.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把注意力转向子图网格中前两行的六个技术技能集群。接下来，我们将在两行三列的网格中重新绘制它们相关的词云（图 17.11）。这种技术导向的可视化将允许我们扩大词云的大小。稍后，我们将回到图
    17.10 中剩余的软技能词云。
- en: Listing 17.40 Plotting just the first six technical clusters
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.40 仅绘制前六个技术集群
- en: '[PRE39]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](../Images/17-11.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-11.png)'
- en: 'Figure 17.11 Six word clouds associated with six technical-skill clusters.
    They are sorted by average resume similarity. The first four word clouds are informative:
    they focus on data science libraries, statistical analysis, Python programming,
    and machine learning. The remaining two word clouds are vague and uninformative.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.11 与六个技术技能集群相关的六个词云。它们按平均简历相似度排序。前四个词云很有信息量：它们专注于数据科学库、统计分析、Python 编程和机器学习。其余两个词云模糊且无信息。
- en: The first four technical-skill clusters in the grid plot are very informative.
    We’ll now examine these clusters one by one, starting with the grid’s upper-left
    quadrant. For brevity’s sake, we rely solely on the word clouds; their contents
    should be sufficient to grasp the skills represented by each cluster. However,
    if you wish to dive deeper into any cluster, feel free to sample the cluster’s
    bullet points.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 网格图中前四个技术技能集群非常具有信息量。现在，我们将逐一检查这些集群，从网格的左上角开始。为了简洁起见，我们仅依赖于词云；它们的内容应该足以理解每个集群所代表的技能。但是，如果您想深入了解任何集群，请随意查看集群的要点。
- en: 'The first four technical clusters can be described as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个技术集群可以描述如下：
- en: '*Cluster 7 (row 0, column 0)*—This data science library cluster has already
    been discussed. Libraries such as scikit-learn, NumPy, SciPy, and Pandas have
    been covered in this book.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集群 7（行 0，列 0）*——这个数据科学库集群已经讨论过了。本书涵盖了诸如 scikit-learn、NumPy、SciPy 和 Pandas
    等库。'
- en: 'The following two libraries have not been covered: TensorFlow and Keras. These
    are deep learning libraries used by AI practitioners to train complex, predictive
    models on high-powered hardware. The boundary between data science positions and
    AI positions is not always clear. Although deep learning knowledge is not usually
    a prerequisite, sometimes it will help you get a job. With this in mind, if you
    wish to study these libraries in more detail, check out *Machine Learning with
    TensorFlow* by Nishant Shukla (Manning, 2018, [www.manning.com/books/machine-learning-with-tensorflow](https://www.manning.com/books/machine-learning-with-tensorflow))
    or *Deep Learning with Python*, *Second Edition*, by François Chollet (Manning,
    2021, [www.manning.com/books/deep-learning-with-python-second-edition](https://www.manning.com/books/deep-learning-with-python-second-edition)).'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下两个库尚未介绍：TensorFlow 和 Keras。这些是 AI 实践者用于在高性能硬件上训练复杂、预测模型的深度学习库。数据科学职位和 AI 职位之间的界限并不总是清晰的。尽管深度学习知识通常不是先决条件，但有时它将有助于你找到工作。考虑到这一点，如果你希望更详细地学习这些库，请查看
    Nishant Shukla 的《使用 TensorFlow 进行机器学习》（Manning，2018，[www.manning.com/books/machine-learning-with-tensorflow](https://www.manning.com/books/machine-learning-with-tensorflow)）或
    François Chollet 的《使用 Python 进行深度学习》（第二版）（Manning，2021，[www.manning.com/books/deep-learning-with-python-second-edition](https://www.manning.com/books/deep-learning-with-python-second-edition)）。
- en: '*Cluster 14 (row 0, column 1)*—This cluster discusses statistical analysis,
    which is represented in our resume. Statistical methods were covered in case study
    2 of this book.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类 14（行 0，列 1）*—这个聚类讨论统计分析，这在我们的简历中有体现。本书的案例研究 2 中涵盖了统计方法。'
- en: '*Cluster 13 (row 1, column 0)*—This cluster focuses on programming language
    proficiency. Among the languages, Python clearly dominates. Given our experience
    with Python, why doesn’t this programming cluster rank higher? Well, it turns
    out Python is not mentioned anywhere in our resume! Yes, we refer to plenty of
    Python libraries, implying our familiarity with the language, but the Python skills
    that we’ve honed over the course of this book are not explicitly referenced. Perhaps
    we should update our resume by mentioning our Python skills.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类 13（行 1，列 0）*—这个聚类关注编程语言熟练度。在这些语言中，Python 明显占主导地位。鉴于我们对 Python 的经验，为什么这个编程聚类没有排名更高呢？好吧，结果发现
    Python 在我们的简历中根本没被提及！是的，我们提到了许多 Python 库，这暗示我们熟悉这种语言，但我们在本书过程中磨练的 Python 技能并没有明确地被引用。也许我们应该通过提及我们的
    Python 技能来更新我们的简历。'
- en: '*Cluster 10 (row 1, column 2)*—This cluster focuses on machine learning. The
    machine learning field encompasses a variety of data-driven prediction algorithms.
    Many of these algorithms are presented in the subsequent case study in this book;
    but until this case study has been completed, we cannot reference machine learning
    in our resume.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类 10（行 1，列 2）*—这个聚类关注机器学习。机器学习领域包括各种数据驱动预测算法。本书后续的案例研究中介绍了许多这些算法；但在完成这个案例研究之前，我们无法在我们的简历中引用机器学习。'
- en: As a side note, we should mention that clustering techniques are sometimes referred
    to as *unsupervised* machine learning algorithms. Thus, a reference to unsupervised
    techniques is permissible. But any reference to more general machine learning
    will give a false impression of our skills.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为一个旁注，我们应该提到，聚类技术有时被称为*无监督*机器学习算法。因此，提及无监督技术是可以接受的。但任何提及更广泛机器学习的说法都会给我们技能的印象造成误解。
- en: The final two technical-skill clusters are vague and uninformative. They mention
    numerous unrelated tools and analysis techniques. Listing 17.41 samples bullets
    from these clusters (8 and 1) to confirm a lack of pattern.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个技术技能聚类模糊且缺乏信息。它们提到了许多不相关的工具和分析技术。列表 17.41 从这些聚类（8 和 1）中抽取样本以确认缺乏模式。
- en: Note Both clusters mention databases. Database usage is a useful skill to have
    but not a major topic in either cluster. Later in this section, we encounter a
    database cluster, which arises when we increase the value of *K*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这两个聚类都提到了数据库。数据库的使用是一项有用的技能，但不是这两个聚类中的主要内容。在本节后面的内容中，我们将遇到一个数据库聚类，这是当我们增加
    *K* 的值时出现的。
- en: Listing 17.41 Printing sample bullets from clusters 8 and 1
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.41 打印来自聚类 8 和 1 的样本项目符号
- en: '[PRE40]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We’ve finished our analysis of the technical-skill clusters. Four of these clusters
    were relevant, and two were not. Now, let’s turn our attention to the remaining
    soft-skill clusters. We want to see if any relevant soft-skill clusters are present
    in the data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了对技术技能聚类的分析。其中四个聚类是相关的，而两个则不是。现在，让我们将注意力转向剩余的软技能聚类。我们想看看数据中是否存在任何相关的软技能聚类。
- en: 17.3.3 Investigating the soft-skill clusters
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.3 调查软技能簇
- en: We start by visualizing the remaining nine soft-skill clusters in a three-row-by-three-column
    grid (figure 17.12).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在一个三行三列的网格中可视化剩余的九个软技能簇（图 17.12）。
- en: Listing 17.42 Plotting the remaining nine soft-skill clusters
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.42 绘制剩余的九个软技能簇
- en: '[PRE41]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](../Images/17-12.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-12.png)'
- en: Figure 17.12 Nine word clouds associated with nine soft-skill clusters. They
    are sorted by average resume similarity. Most of the clusters are vague and uninformative,
    but the communication skills clusters in the first row are worth noting.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.12 与九个软技能簇相关的九个词云。它们按平均简历相似度排序。大多数簇都很模糊且没有信息量，但第一行的沟通技能簇值得关注。
- en: 'The remaining clusters appear much more ambiguous than the first four technical
    clusters. They are harder to interpret. For example, cluster 2 (row 2, column
    0) uses vague terms such as *work*, *team*, *research*, and *environment*. Cluster
    12 (row 1, column 0) is equally enigmatic, composed of terms such as *environment*,
    *working*, and *experience*. Furthermore, the output is made more complicated
    by clusters that do not represent true skills! For instance, cluster 3 (row 0,
    column 0) is composed not of skills but of temporal experience: it consists of
    bullets requiring a minimum number of years working in industry. Similarly, cluster
    6 (row 2, column 1) is not composed of skills; it represents educational constraints,
    requiring a quantitative degree to land an interview. We were slightly wrong in
    our assumptions—not all bullets represent true skills. We can confirm our error
    by sampling bullet points from clusters 6 and 3.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的簇看起来比前四个技术簇要模糊得多。它们更难解释。例如，簇 2（行 2，列 0）使用了诸如 *工作*、*团队*、*研究* 和 *环境* 这样的模糊术语。簇
    12（行 1，列 0）同样神秘莫测，由诸如 *环境*、*工作* 和 *经验* 这样的术语组成。此外，输出变得更加复杂，因为有些簇并不代表真正的技能！例如，簇
    3（行 0，列 0）由技能而非时间经验组成：它包括需要一定年限在行业工作的子弹。同样，簇 6（行 2，列 1）也不是由技能组成；它代表教育限制，要求有定量学位才能获得面试机会。我们在假设上略有错误——并非所有子弹都代表真正的技能。我们可以通过从簇
    6 和 3 中抽样子弹点来确认我们的错误。
- en: Listing 17.43 Printing sample bullets from clusters 6 and 3
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.43 打印来自簇 6 和 3 的样本子弹
- en: '[PRE42]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'One of our soft-skill clusters is very easy to interpret: cluster 5 (row 0,
    column 1) focuses on interpersonal communication skills, both written and verbal.
    Good communication skills are crucial in a data science career. The insights we
    extract from complex data must be carefully communicated to all stakeholders.
    The stakeholders will then take consequential actions based on the persuasiveness
    of our argument. If we are unable to communicate our results, all our hard work
    will come to nothing.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的软技能簇中有一个非常容易解释：簇 5（行 0，列 1）专注于人际沟通技能，包括书面和口头沟通。良好的沟通技巧在数据科学职业生涯中至关重要。我们从复杂数据中提取的见解必须仔细传达给所有利益相关者。利益相关者将根据我们论点的说服力采取重要行动。如果我们无法传达我们的结果，所有我们的辛勤工作都将化为乌有。
- en: Unfortunately, communication skills are not easy to learn. Simply reading a
    book is insufficient; practiced collaboration with other individuals is required.
    If you would like to broaden your communication abilities, you should consider
    interacting with other budding data scientists, either locally or remotely. Choose
    a data-driven project, and complete that project as part of a team. Then be sure
    to emphasize your honed communication skills in your resume.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，沟通技巧并不容易学习。仅仅读书是不够的；需要与其他个人进行实践合作。如果您想拓宽您的沟通能力，您应该考虑与本地或远程的其他新兴数据科学家互动。选择一个数据驱动型项目，并作为团队的一部分完成该项目。然后务必在您的简历中强调您磨炼的沟通技能。
- en: 17.3.4 Exploring clusters at alternative values of K
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.4 探索 K 的不同值下的簇
- en: 'K-means clustering gave us decent results when we set *K* to 15\. However,
    that parameter input was partially arbitrary since we couldn’t determine a perfectly
    optimal *K*. The arbitrary nature of our insights is a bit troubling: perhaps
    we just got lucky, and a different *K* would have yielded no insights at all.
    Or maybe we’ve missed critical clusters by choosing *K* incorrectly. The issue
    we need to probe is cluster consistency. How many of our insight-driving clusters
    will remain if we modify *K*? To find out, we’ll regenerate the clusters using
    alternative values of *K*. We begin by setting *K* to 25 and plotting the results
    in a five-row-by-five-column grid (figure 17.13). The subplots will be sorted
    based on cluster similarity to our resume.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 *K* 设置为 15 时，K-means 簇群算法给出了相当不错的结果。然而，这个参数输入部分是随机的，因为我们无法确定一个完美的最优 *K*。我们洞察力的任意性有点令人不安：也许我们只是运气好，不同的
    *K* 可能根本不会产生任何洞察。或者，也许我们选择了错误的 *K* 而错过了关键的簇。我们需要探究的问题是簇的一致性。如果我们修改 *K*，我们驱动的多少个洞察力簇将保持不变？为了找出答案，我们将使用不同的
    *K* 值重新生成簇。我们首先将 *K* 设置为 25，并在一个五行五列的网格中绘制结果（图 17.13）。子图将根据簇与我们简历的相似性进行排序。
- en: Listing 17.44 Visualizing 25 sorted clusters
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.44 可视化 25 个排序后的簇
- en: '[PRE43]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](../Images/17-13.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-13.png)'
- en: Figure 17.13 25 word clouds associated with 25 skill clusters. Our previously
    discussed skills remain present even after we’ve increased the value *K*. Additionally,
    we see new technical skills that are worth noting, including web service usage
    and familiarity with databases.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.13 与 25 个技能簇相关的 25 个词云。我们之前讨论的技能在我们增加 *K* 值后仍然存在。此外，我们还看到了一些值得注意的新技术技能，包括网络服务使用和对数据库的熟悉。
- en: 'Most of the previously observed clusters remain in the updated output. These
    include data science library usage (row 0, column 0), statistical analysis (row
    0, column 2), Python programming (row 0, column 1), machine learning (row 1, column
    2), and communication skills (row 2, column 0). Additionally, we gain three insightful
    technical skill clusters, which appear among the first two rows of the grid:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数之前观察到的簇在更新后的输出中仍然存在。这些包括数据科学库的使用（行 0，列 0）、统计分析（行 0，列 2）、Python 编程（行 0，列 1）、机器学习（行
    1，列 2）和沟通技巧（行 2，列 0）。此外，我们还获得了三个有洞察力的技术技能簇，它们出现在网格的前两行中：
- en: '*Cluster 8 (row 0, column 4)*—This cluster focuses on web services. These are
    tools that propagate communication between a client and a remote server. In most
    industrial data science settings, data is stored remotely on a server and can
    be transferred using custom APIs. In Python, these API protocols are commonly
    coded using the Django framework. For budding data scientists, some familiarity
    with these tools is preferable but not necessarily required. To learn more about
    web services and API transfers, see *Amazon Web Services in Action* by Michael
    Wittig and Andreas Wittig (Manning, 2018, [https://www.manning.com/books/amazon-web-services-in-action-second-edition](https://www.manning.com/books/amazon-web-services-in-action-second-edition))
    and *The Design of Web APIs* by Arnaud Lauret (Manning, 2019, [https://www.manning.com/books/the-design-of-web-apis](https://www.manning.com/books/the-design-of-web-apis)).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*簇 8（行 0，列 4）*—该簇专注于网络服务。这些是传播客户端与远程服务器之间通信的工具。在大多数工业数据科学设置中，数据存储在远程服务器上，可以通过自定义
    API 进行传输。在 Python 中，这些 API 协议通常使用 Django 框架进行编码。对于初学者数据科学家来说，对这些工具有所了解是首选，但并非必需。要了解更多关于网络服务和
    API 传输的信息，请参阅 Michael Wittig 和 Andreas Wittig 所著的 *《Amazon Web Services in Action》*（Manning，2018，[https://www.manning.com/books/amazon-web-services-in-action-second-edition](https://www.manning.com/books/amazon-web-services-in-action-second-edition)）以及
    Arnaud Lauret 所著的 *《Web APIs 的设计》*（Manning，2019，[https://www.manning.com/books/the-design-of-web-apis](https://www.manning.com/books/the-design-of-web-apis)）。'
- en: '*Cluster 23 (row 1, column 3)*—This cluster focuses on various types of databases.
    Large-scale structured data is commonly stored in relational databases and can
    be queried using Structured Query Language (SQL). However, not all databases are
    relational. Sometimes data is stored in alternative, unstructured databases, such
    as MongoDB. Data in an unstructured database can be queried using a NoSQL query
    language. Knowledge of the various database types can be quite useful in a data
    science career. If you would like to learn more about the subject, check out *Understanding
    Databases* (Manning, 2019, [www.manning.com/books/understanding-databases](https://www.manning.com/books/understanding-databases));
    and to learn more about MongoDB, see *MongoDB in Action, Second Edition* by Kyle
    Banker et al. (Manning, 2016, [www.manning.com/books/mongodb-in-action-second-edition](http://www.manning.com/books/mongodb-in-action-second-edition)).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类 23（第 1 行，第 3 列）*—这个聚类专注于各种类型的数据库。大规模结构化数据通常存储在关系型数据库中，可以使用结构化查询语言（SQL）进行查询。然而，并非所有数据库都是关系型的。有时数据存储在替代的、非结构化的数据库中，例如
    MongoDB。非结构化数据库中的数据可以使用 NoSQL 查询语言进行查询。了解各种数据库类型在数据科学职业生涯中可能非常有用。如果你想了解更多关于这个主题的信息，可以查看
    *《理解数据库》*（Manning，2019，[www.manning.com/books/understanding-databases](https://www.manning.com/books/understanding-databases)）；并且要了解更多关于
    MongoDB 的信息，可以查看 Kyle Banker 等人编写的 *《MongoDB 实战第二版》*（Manning，2016，[www.manning.com/books/mongodb-in-action-second-edition](http://www.manning.com/books/mongodb-in-action-second-edition)）。'
- en: '*Cluster 2 (row 1, column 1)*—This cluster focuses on non-Python visualization
    tools, such as Tableau and ggplot. Tableau is paid software provided by Salesforce
    and is commonly used by businesses that can afford the Salesforce contract; you
    can read more about it in *Practical Tableau* [by Ryan Sleeper ((O’Reilly, 2018,](http://mng.bz/Xrdv)
    http://mng.bz/Xrdv). ggplot is a data-visualization package for the statistical
    programming language R. Generally, Python data scientists are not expected to
    know R; but if you would like to familiarize yourself with the subject, see *Practical
    Data Science with R, Second Edition* by Nina Zumel and John Mount (Manning, 2019,
    [www.manning.com/books/practical-data-science-with-r-second-edition](http://www.manning.com/books/practical-data-science-with-r-second-edition)).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类 2（第 1 行，第 1 列）*—这个聚类专注于非 Python 可视化工具，例如 Tableau 和 ggplot。Tableau 是 Salesforce
    提供的付费软件，通常被能够承担 Salesforce 合同的企业使用；你可以在 *《实用 Tableau》* [由 Ryan Sleeper 编著 ((O’Reilly,
    2018,](http://mng.bz/Xrdv) http://mng.bz/Xrdv) 中了解更多信息。ggplot 是用于统计编程语言 R 的数据可视化包。一般来说，Python
    数据科学家不需要了解 R；但如果你想熟悉这个主题，可以查看 Nina Zumel 和 John Mount 编著的 *《实用数据科学 R 语言第二版》*（Manning，2019，[www.manning.com/books/practical-data-science-with-r-second-edition](http://www.manning.com/books/practical-data-science-with-r-second-edition)）。'
- en: Our plot also contains seven newly added clusters. These clusters contain mostly
    generic skills like problem solving (row 3, column 0) and teamwork (row 2, column
    3). Also, at least one of the new skill clusters doesn’t correspond to an actual
    skill (like the health insurance benefits cluster in row 3, column 4).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图表还包含七个新添加的聚类。这些聚类主要包含一些通用技能，如问题解决（第 3 行，第 0 列）和团队合作（第 2 行，第 3 列）。此外，至少有一个新的技能聚类不对应于实际技能（例如第
    3 行，第 4 列的健康保险福利聚类）。
- en: Increasing *K* from 15 to 25 retained all the previously observed insightful
    clusters and introduced several interesting new clusters. Will the stability of
    these clusters persist if we shift *K* to an intermediate value of 20? We find
    out next by plotting 20 sorted clusters in a four-row-by-five-column grid (figure
    17.14).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *K* 从 15 增加到 25 保留了之前观察到的所有有洞察力的聚类，并引入了几个有趣的新聚类。如果我们把 *K* 转移到中间值 20，这些聚类的稳定性会持续吗？我们将在下一部分通过在一个四行五列的网格中绘制
    20 个排序后的聚类来找出答案（图 17.14）。
- en: Listing 17.45 Visualizing 20 sorted clusters
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.45 可视化 20 个排序后的聚类
- en: '[PRE44]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](../Images/17-14.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17-14.png)'
- en: Figure 17.14 20 word clouds associated with 20 skill clusters. Most of our previously
    discussed skills remain, but the statistical analysis cluster is now missing from
    the output.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.14 与 20 个技能聚类相关的 20 个词云。我们之前讨论的大多数技能仍然存在，但现在统计分析聚类已从输出中消失。
- en: Most of our observed insightful clusters remain at `k=20`, including data science
    library usage (row 0, column 0), Python programming (row 0, column 3), machine
    learning (row 1, column 0), communication skills (row 1, column 4), web services
    (row 0, column 1), and database usage (row 0, column 4). However, the non-Python
    visualization cluster is gone. More troublingly, the statistical analysis cluster
    observed at *K* values of 15 and 25 is missing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的多数有见解的聚类在`k=20`时仍然存在，包括数据科学库的使用（第0行，第0列）、Python编程（第0行，第3列）、机器学习（第1行，第0列）、沟通技巧（第1行，第4列）、网络服务（第0行，第1列）和数据库使用（第0行，第4列）。然而，非Python可视化聚类已消失。更令人不安的是，在*K*值为15和25时观察到的统计分析聚类缺失。
- en: 'Note This statistical analysis cluster appears to have been replaced by a statistical
    algorithm cluster, which is positioned at row 0, column 2 of the grid. It is dominated
    by three terms: *algorithms*, *clustering*, and *regression*. Of course, by now,
    we’re intimately familiar with clustering. However, regression techniques are
    missing from our resume because we haven’t learned them yet. We will learn these
    techniques in case study 5 and can then add them to our resume.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这个统计分析聚类似乎已被一个统计算法聚类所取代，该聚类位于网格的第0行，第2列。它主要由三个术语组成：*算法*、*聚类*和*回归*。当然，到现在我们已经非常熟悉聚类了。然而，回归技术尚未出现在我们的简历上，因为我们还没有学习它们。我们将在案例研究5中学习这些技术，然后可以将它们添加到我们的简历中。
- en: A seemingly stable cluster has been eliminated. Unfortunately, such fluctuations
    are quite common. Text clustering is sensitive to parameter changes due to the
    complex nature of human language. Language topics can be interpreted in a multitude
    of ways, making it hard to find consistently perfect parameters. Clusters that
    appear under one set of parameters may disappear if these parameters are tweaked.
    If we cluster over just a single value of *K*, we risk missing out on useful insights.
    Thus, it’s preferable to visualize results over a range of *K* values during text
    analysis. With this in mind, let’s see what happens when we reduce *K* to 10 (figure
    17.15).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 一个看似稳定的聚类已被消除。不幸的是，这种波动相当常见。由于人类语言的复杂性质，文本聚类对参数变化很敏感。语言主题可以以多种方式解释，这使得找到一致完美的参数变得困难。在某一组参数下出现的聚类，如果这些参数被调整，可能会消失。如果我们只对*K*的一个值进行聚类，我们可能会错过有用的见解。因此，在文本分析期间，最好在一系列*K*值上可视化结果。考虑到这一点，让我们看看当我们将*K*减少到10时会发生什么（图17.15）。
- en: Listing 17.46 Visualizing 10 sorted clusters
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列出17.46可视化10个排序后的聚类
- en: '[PRE45]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](../Images/17-15.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-15.png)'
- en: Figure 17.15 10 word clouds associated with 10 skill clusters. Four of our previously
    discussed skills remain, despite the low value of *K*.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.15与10个技能聚类相关的10个词云。尽管*K*值较低，但我们之前讨论的四个技能仍然存在。
- en: 'The 10 visualized clusters are quite limited. Nonetheless, 4 of the 10 clusters
    contain critical skills we’ve observed previously: Python programming (row 0,
    column 0), machine learning (row 0, column 1), and communication skills (row 2,
    column 1). The statistical analysis cluster has also reappeared (row 1, column
    0). Surprisingly, some of our skill clusters are versatile and appear even when
    the *K* value is radically adjusted. Despite some stochasticity in our clustering,
    a level of consistency remains. Thus, the insights we observe aren’t just random
    outputs—they are tangible patterns that we’ve captured across complex, messy,
    real-world texts.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 10个可视化的聚类相当有限。尽管如此，10个聚类中有4个包含我们之前观察到的关键技能：Python编程（第0行，第0列）、机器学习（第0行，第1列）和沟通技巧（第2行，第1列）。统计分析聚类也再次出现（第1行，第0列）。令人惊讶的是，一些我们的技能聚类具有多功能性，甚至在*K*值大幅调整时也会出现。尽管我们的聚类存在一些随机性，但仍然保持了一定的一致性。因此，我们观察到的见解并不仅仅是随机输出——它们是我们从复杂、混乱的真实世界文本中捕捉到的可感知的模式。
- en: So far, our observations have been limited to the 60 most relevant job postings.
    However, as we’ve seen, there is some noise in that data subset. What will happen
    if we extend our analysis to the top 700 postings? Will our observations change
    or stay the same? Let’s find out.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的观察仅限于60个最相关的职位发布。然而，正如我们所看到的，该数据子集中存在一些噪声。如果我们将我们的分析扩展到前700个发布，会发生什么？我们的观察会改变还是保持不变？让我们来看看。
- en: 17.3.5 Analyzing the 700 most relevant postings
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.5 分析前700个最相关的发布
- en: 'We start by preparing `sorted_df_jobs[:700].Bullets` for clustering by doing
    the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过以下方式为聚类准备`sorted_df_jobs[:700].Bullets`：
- en: Extract all the bullets while removing duplicates.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取所有子弹，同时删除重复项。
- en: Vectorize the bullet texts.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量化子弹文本。
- en: Dimensionally reduce the vectorized texts, and normalize the resulting matrix.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对向量化文本进行降维，并对生成的矩阵进行归一化。
- en: Listing 17.47 Preparing `sorted_df_jobs[:700]` for clustering analysis
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.47 准备 `sorted_df_jobs[:700]` 用于聚类分析
- en: '[PRE46]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We’ve vectorized 10,194 bullet points. Now, we generate an elbow plot across
    the vectorized results. Based on previous observations, we don’t expect the elbow
    plot to be particularly informative, but we create the plot to stay consistent
    with our previous analyses (figure 17.16).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将 10,194 个子弹向量化。现在，我们生成一个基于向量化结果的肘部图。根据之前的观察，我们预计肘部图不会特别有信息量，但我们创建这个图以保持与之前分析的一致性（图
    17.16）。
- en: Listing 17.48 Plotting an elbow curve for 10,194 bullets
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.48 为 10,194 个子弹绘制肘部曲线
- en: '[PRE47]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](../Images/17-16.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-16.png)'
- en: Figure 17.16 An elbow plot generated using bullets from the top 700 most relevant
    postings. The precise location of the elbow is difficult to determine.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.16 使用前 700 个最相关帖子生成的肘部图。肘部的确切位置难以确定。
- en: As expected, the precise location of the elbow is not clear in the plot. The
    elbow is spread out between a *K* of 10 and 25\. We’ll deal with ambiguity by
    arbitrarily setting *K* to 20\. Let’s generate and visualize 20 clusters; if necessary,
    we’ll adjust *K* for comparative clustering (figure 17.17).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，在图中肘部的确切位置并不清晰。肘部在 10 和 25 之间的 *K* 值上分散。我们将通过任意地将 *K* 设置为 20 来处理这种歧义。让我们生成并可视化
    20 个聚类；如果需要，我们将调整 *K* 以进行比较聚类（图 17.17）。
- en: '![](../Images/17-17.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/17-17.png)'
- en: Figure 17.17 20 word clouds generated by clustering over 10,000 bullets. Despite
    the 10-fold increase in bullets, the observed skill clusters mostly remain the
    same.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.17 通过 10,000 个子弹进行聚类生成的 20 个词云。尽管子弹数量增加了 10 倍，但观察到的技能聚类大多保持不变。
- en: Warning As we discussed in section 15, the K-means output can vary across computers
    for large matrices containing 10,000-by-100 elements. Your local clustering results
    may differ from the output shown here, but you should be able to draw conclusions
    similar to those presented in this book.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 如我们在第 15 节中讨论的，对于包含 10,000 行 100 列的大矩阵，K-means 输出可能因计算机而异。您的本地聚类结果可能与此处显示的输出不同，但您应该能够得出与本书中提出的类似结论。
- en: Listing 17.49 Visualizing 20 sorted clusters for 10,194 bullets
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.49 可视化 10,194 个子弹的 20 个排序聚类
- en: '[PRE48]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ Recomputes bullet_cosine_similarities for sorting purposes
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重新计算用于排序的 bullet_cosine_similarities
- en: ❷ We need to pass the updated TFIDF matrix and vectorizer into our plotting
    function.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们需要将更新的 TFIDF 矩阵和向量器传递给我们的绘图函数。
- en: Our clustering output looks very similar to what we’ve seen before. The key
    insightful clusters we observed at 60 postings remain, including data science
    library usage (row 0, column 0), statistical analysis (row 0, column 1), Python
    programming (row 0, column 4), machine learning (row 0, column 3), and communication
    skills (row 1, column 3). Some subtle changes are present, but for the most part,
    the output is the same.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聚类输出看起来与我们之前看到的非常相似。我们观察到的关键洞察力聚类在 60 个帖子中仍然存在，包括数据科学库的使用（行 0，列 0）、统计分析（行
    0，列 1）、Python 编程（行 0，列 4）、机器学习（行 0，列 3）和沟通技巧（行 1，列 3）。有一些细微的变化，但大部分输出是相同的。
- en: Note One interesting change is the appearance of a generalized visualization
    cluster (row 0, column 2). This cluster encompasses a variety of visualization
    tools, including Matplotlib. Additionally, the freely available JavaScript library
    D3.js is mentioned in the cluster’s word cloud. The D3.js library is used by some
    data scientists to make interactive web visualizations. To learn more about the
    library, see *D3.js in Action, Second Edition* by Elijah Meeks (Manning, 2017,
    [www.manning.com/books/d3js-in-action-second-edition](https://www.manning.com/books/d3js-in-action-second-edition)).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 一个有趣的变化是出现了一个通用的可视化聚类（行 0，列 2）。这个聚类包括各种可视化工具，包括 Matplotlib。此外，在聚类的词云中提到了免费可用的
    JavaScript 库 D3.js。一些数据科学家使用 D3.js 库制作交互式网络可视化。要了解更多关于这个库的信息，请参阅 Elijah Meeks（Manning，2017）所著的《D3.js
    in Action, Second Edition》（[www.manning.com/books/d3js-in-action-second-edition](https://www.manning.com/books/d3js-in-action-second-edition)）。
- en: Certain skills consistently appear in the job postings. These skills are not
    very sensitive to our selected relevance threshold, so we can elucidate them even
    if our threshold remains uncertain.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 某些技能在职位发布中持续出现。这些技能对我们的所选相关性阈值不太敏感，因此即使我们的阈值不确定，我们也可以阐明它们。
- en: 17.4 Conclusion
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4 结论
- en: 'We’re ready to update the draft of our resume. First and foremost, we should
    emphasize our Python skills. A single line saying that we’re *proficient in Python*
    should be sufficient. Additionally, we want to assert our communication skills.
    How do we show that we’re good communicators? It’s tricky; simply stating that
    we can *clearly communicate complex results to different audiences* is not enough.
    Instead, we should describe a personal project in which we did the following:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好更新简历草案。首先，我们应该强调我们的Python技能。一句说我们“精通Python”就足够了。此外，我们想要强调我们的沟通技能。我们如何展示我们是优秀的沟通者？这很棘手；仅仅声明我们能够“清晰地与不同受众沟通复杂的结果”是不够的。相反，我们应该描述一个个人项目，其中我们做了以下事情：
- en: Collaborated with teammates on a difficult data problem
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与队友合作解决困难的数据问题
- en: Conveyed complex results, in oral or written form, to a nontechnical audience
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将复杂的结果，无论是口头还是书面形式，传达给非技术受众
- en: Note If you’ve experienced working on this type of project, you should definitely
    add it to your resume. Otherwise, you’re encouraged to pursue this type of project
    voluntarily. The skills you’ll gain will prove invaluable while also bettering
    your employment prospects.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你有参与此类项目的经验，你绝对应该将其添加到你的简历中。否则，我们鼓励你自愿追求此类项目。你将获得的技能将证明是无价的，同时也会改善你的就业前景。
- en: Furthermore, before we complete our resume, we need to address our remaining
    skill deficiencies. Machine learning experience is crucial to a successful data
    science career. We haven’t yet studied machine learning, but in the subsequent
    case study, we expand our machine learning skills. Then we’ll be able to proudly
    describe our machine learning abilities in our resume.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们完成简历之前，我们需要解决我们剩余的技能缺陷。机器学习经验对于成功的数据科学职业生涯至关重要。我们尚未学习机器学习，但在随后的案例研究中，我们将扩展我们的机器学习技能。然后我们就能在简历中自豪地描述我们的机器学习能力。
- en: Finally, it’s worthwhile to demonstrate some experience with tools to fetch
    and store remote data. These tools include databases and hosted web services.
    Their use is beyond the scope of this book, but they can be learned through independent
    study. Database and web services experience isn’t always required to get the job;
    nevertheless, some limited experience is always welcomed by potential employers.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，展示一些使用工具获取和存储远程数据的经验是值得的。这些工具包括数据库和托管网络服务。它们的使用超出了本书的范围，但可以通过独立学习来掌握。数据库和网络服务经验并非总是获得工作的必要条件；然而，一些有限的经验总是受到潜在雇主的欢迎。
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Text data should not be analyzed blindly. We should always sample and read some
    of the text before running any algorithms. This is especially true of HTML files,
    where tags can demarcate unique signals in the text. By rendering sampled job
    postings, we discovered that unique job skills are marked by bullet points in
    each HTML file. If we had blindly clustered the body of each file, our final results
    wouldn’t have been as informative.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据不应盲目分析。我们在运行任何算法之前，都应该采样并阅读一些文本。这在HTML文件中尤其如此，其中标签可以界定文本中的独特信号。通过渲染采样后的职位发布，我们发现独特的职位技能在每个HTML文件中都用项目符号标记。如果我们盲目地对每个文件的正文进行聚类，我们的最终结果就不会那么有信息量。
- en: 'Text clustering is hard. An ideal cluster count rarely exists because language
    is fluid, and so are boundaries between topics. But despite the uncertainty, certain
    topics consistently appear across multiple cluster counts. So, even if our elbow
    plot does not reveal the exact number of clusters, the situation is salvageable:
    sampling over multiple clustering parameters can reveal stable topics in the text.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本聚类很困难。理想的聚类数量很少存在，因为语言是流动的，主题之间的边界也是如此。但尽管存在不确定性，某些主题在多个聚类数量中始终出现。因此，即使我们的肘部图没有揭示确切的聚类数量，情况也是可以挽救的：对多个聚类参数进行采样可以揭示文本中的稳定主题。
- en: 'Choosing parameter values is not always easy. This issue extends well beyond
    mere clustering. When selecting our relevance cutoff, we were torn between two
    values: 60 and 700\. Neither value seemed much superior to the other, so we tried
    both! In data science, some problems don’t have an ideal threshold or parameter.
    However, we shouldn’t give up and ignore such problems. On the contrary, we should
    experiment. Scientists learn by exploring outputs across a range of parameter
    inputs. As data scientists, we can gain invaluable insights by tweaking and adjusting
    our parameters.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择参数值并不总是容易。这个问题远远超出了简单的聚类。当我们选择相关性截止值时，我们在两个值之间犹豫不决：60 和 700。这两个值似乎并没有一个明显优于另一个，所以我们尝试了两个！在数据科学中，一些问题并没有一个理想的阈值或参数。然而，我们不应该放弃并忽略这些问题。相反，我们应该进行实验。科学家通过探索一系列参数输入的输出来学习。作为数据科学家，我们可以通过调整和调整我们的参数来获得宝贵的见解。
