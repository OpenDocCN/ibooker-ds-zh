- en: Chapter 2\. Data Modeling for Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 数据建模与分析
- en: In today’s data-driven world, organizations rely more and more on data analytics
    to gain valuable insights and make informed decisions. Data modeling plays an
    imperative role in this process, providing a solid foundation for structuring
    and organizing data to support effective analysis. In addition, understanding
    the concepts of data modeling and normalization is essential to realizing the
    full potential of analytics and gaining actionable insights from complex datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今数据驱动的世界中，组织越来越依赖数据分析来获取宝贵的见解并做出明智的决策。数据建模在此过程中扮演了至关重要的角色，为结构化和组织数据提供了坚实的基础，以支持有效的分析。此外，理解数据建模和规范化的概念对于实现分析的全部潜力并从复杂数据集中获得可操作的见解至关重要。
- en: '*Data modeling* is about defining the structure, relationships, and attributes
    of data entities within a system. An essential aspect of data modeling is the
    normalization of the data. *Data normalization* is a technique for eliminating
    data redundancy and improving data integrity. It involves breaking data into logical
    units and organizing them into separate tables, which reduces data duplication
    and improves overall database efficiency. Normalization ensures that data is stored
    in a structured and consistent manner, which is critical for accurate analysis
    and reliable results.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据建模*是定义系统内数据实体的结构、关系和属性。数据建模的一个重要方面是数据的规范化。*数据规范化*是消除数据冗余并提高数据完整性的技术。它涉及将数据分解为逻辑单元，并将其组织成单独的表格，从而减少数据重复，并提高整体数据库效率。规范化确保数据以结构化和一致的方式存储，这对于准确的分析和可靠的结果至关重要。'
- en: Regarding analytics, data modeling provides a solid foundation for creating
    analytical models. Analysts can design effective models that capture relevant
    information and support the desired analytics objectives by understanding the
    relationships among entities and data structures. In other words, a well-designed
    data model enables analysts to perform complex queries, join tables, and aggregate
    data to produce meaningful insights.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析领域，数据建模为创建分析模型提供了坚实的基础。分析师可以通过理解实体和数据结构之间的关系来设计有效的模型，捕捉相关信息并支持所需的分析目标。换句话说，一个设计良好的数据模型使分析师能够执行复杂的查询，连接表格，并聚合数据以产生有意义的见解。
- en: Understanding data modeling and normalization is critical to practical data
    analysis. Analysts may have difficulty accessing and correctly interpreting data
    without an appropriate data model, which can lead to incorrect conclusions and
    ineffective decisions. In addition, a lack of normalization can lead to data anomalies,
    inconsistencies, and difficulty aggregating data, hindering the analysis process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据建模和规范化对于实际数据分析至关重要。分析师如果没有合适的数据模型，可能会难以访问和正确解释数据，从而导致错误的结论和无效的决策。此外，缺乏规范化可能导致数据异常、不一致性和难以聚合数据，阻碍分析过程。
- en: In this book, we highlight SQL and dbt as two core technologies to sustain an
    effective analytics engineering project, and this is also applicable to designing
    and implementing an effective data model. The reason behind this is that SQL equips
    users with the ability to define tables, manipulate data, and retrieve information
    through its robust query capabilities. Its unmatched flexibility and versatility
    position it as a great tool for constructing and sustaining data models, empowering
    users to articulate intricate relationships and effortlessly access specific data
    subsets.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将SQL和dbt作为两种核心技术突出展示，以支持有效的分析工程项目，这也适用于设计和实施有效的数据模型。背后的原因是，SQL赋予用户定义表格、操作数据并通过其强大的查询能力检索信息的能力。其无与伦比的灵活性和多功能性使其成为构建和维护数据模型的强大工具，赋予用户表达复杂关系和轻松访问特定数据子集的能力。
- en: 'Complementary to SQL, dbt plays a central role in this narrative, taking the
    art of data modeling to a whole new level. It serves as a comprehensive framework
    for constructing and orchestrating complex data pipelines. Within this framework,
    users can define transformation logic, apply essential business rules, and craft
    reusable modular code components known as *models*. It’s worth noting that dbt
    goes beyond standalone functionality: it seamlessly integrates with version control
    systems, making collaboration a breeze and ensuring that data models maintain
    consistency, auditability, and effortless reproducibility.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与SQL相辅相成，dbt在这一叙述中起着核心作用，将数据建模艺术推向全新高度。它作为一个全面的框架，用于构建和编排复杂的数据管道。在这个框架内，用户可以定义转换逻辑，应用关键业务规则，并编写可重复使用的模块化代码组件，称为*模型*。值得注意的是，dbt不仅限于独立功能：它与版本控制系统无缝集成，使协作变得轻松，并确保数据模型保持一致性、可审计性和易复现性。
- en: Another crucial aspect of SQL and dbt in data modeling is their emphasis on
    testing and documentation, although with some distinctions that are worth clarifying.
    In the context of data modeling, testing involves validating the data model’s
    accuracy, reliability, and adherence to business rules. While it’s important to
    note that dbt’s testing capabilities differ from traditional unit testing in software
    development, they serve a similar purpose. Instead of traditional unit tests,
    dbt offers validation queries that are comparable to what analysts are accustomed
    to running. These validation queries check data quality, data integrity, and adherence
    to defined rules, providing confidence in the model’s outputs. Furthermore, dbt
    excels in documentation, serving as a valuable resource for analysts and stakeholders
    alike. This documentation simplifies the understanding of the underlying logic
    and assumptions that drive the data model, which enhances transparency and fosters
    effective collaboration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: SQL和dbt在数据建模中的另一个关键方面是它们对测试和文档的重视，尽管有一些值得澄清的区别。在数据建模的背景下，测试涉及验证数据模型的准确性、可靠性和遵守业务规则。虽然需要注意的是，dbt的测试能力与软件开发中传统的单元测试有所不同，但它们的目的类似。dbt提供的验证查询不同于传统的单元测试，它们类似于分析人员习惯运行的查询。这些验证查询检查数据质量、数据完整性以及遵守定义的规则，为模型的输出提供信心。此外，dbt在文档编制方面表现出色，为分析人员和利益相关者提供了宝贵的资源。这些文档简化了理解驱动数据模型的基础逻辑和假设，增强了透明度并促进了有效的协作。
- en: Together, SQL and dbt empower data professionals to create robust, scalable,
    and maintainable data models that drive insightful analytics and informed decision
    making. By leveraging these tools, organizations can unlock the full potential
    of their data, fueling innovation and gaining a competitive advantage in today’s
    data-driven landscape. Combining both in the same data architecture and strategy
    brings significant advantages to data modeling.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: SQL和dbt共同赋能数据专业人士创建强大、可扩展和可维护的数据模型，推动洞察性分析和明智的决策。通过利用这些工具，组织可以释放数据的全部潜力，在当今数据驱动的景观中获得竞争优势。将这两者结合在同一数据架构和策略中，对数据建模带来了显著的优势。
- en: A Brief on Data Modeling
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据建模简介
- en: In the world of database design, creating a structured and organized environment
    is vital for storing, manipulating, and leveraging data effectively. Database
    modeling plays a relevant role in achieving this objective by providing the blueprint
    for representing a specific reality or a business and supporting its processes
    and rules.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库设计领域，创建结构化和有组织的环境对于有效存储、操作和利用数据至关重要。数据库建模通过提供代表特定现实或业务的蓝图，并支持其过程和规则，对实现这一目标起到了重要作用。
- en: However, before we dive deep into creating this blueprint, we should focus on
    comprehending the nuances of the business. Understanding the business’s operations,
    terminology, and processes is essential for creating accurate and meaningful data
    models. By gathering requirements through interviews, document analysis, and process
    studies, we gain insights into the business’s needs and data requirements. During
    this gathering process, we should focus on natural communication—written language.
    By expressing business facts in unambiguous sentences, we ensure that representations
    of the business are accurate and free of interpretation. Breaking complex sentences
    into simple structures with subjects, verbs, and direct objects helps concisely
    capture business realities.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们深入创建这个蓝图之前，我们应该专注于理解业务的微妙差别。理解业务的运营、术语和流程对于创建准确和有意义的数据模型至关重要。通过采访、文档分析和流程研究收集需求，我们深入了解业务的需求和数据要求。在这个收集过程中，我们应该专注于自然的沟通方式——书面语言。通过清晰的句子表达业务事实，确保业务的表达准确且没有歧义。将复杂的句子分解为主语、谓语和宾语的简单结构，有助于简洁地捕捉业务实际情况。
- en: In addition to these essential practices, it’s worth noting that experts in
    the field, like Lawrence Corr in his popular book *Agile Data Warehouse Design*
    (DecisionOne Press), advocate for further techniques such as whiteboarding and
    canvassing during this initial phase of data model design. These additional strategies
    can add nuance to the process, allowing for a more comprehensive exploration of
    business requirements and ensuring that the resulting data models align seamlessly
    with the business’s objectives and intricacies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本实践之外，还值得注意的是，像劳伦斯·科尔在他的畅销书《敏捷数据仓库设计》（DecisionOne Press）中提倡的进一步技术，如白板绘图和画布设计，在数据模型设计的初始阶段可以增加细节，允许更全面地探索业务需求，并确保最终的数据模型与业务目标和复杂性无缝对接。
- en: 'Once the understanding phase is complete, we move to the three basic steps
    of database modeling:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦理解阶段完成，我们将进入数据库建模的三个基本步骤：
- en: Conceptual phase
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念阶段
- en: Logical phase
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑阶段
- en: Physical phase
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理阶段
- en: These steps form a journey to creating a robust and well-organized database
    structure. Let’s use the example of a book publisher to illustrate the process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤构成了创建稳健且良好组织的数据库结构的旅程。让我们以图书出版商为例来说明这个过程。
- en: The Conceptual Phase of Modeling
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模的概念阶段
- en: The *conceptual phase* of database modeling requires several essential steps.
    First, it is necessary to identify the purpose and goals of the database and clarify
    the specific problems or requirements it needs to address. The next step is gathering
    requirements by interviewing stakeholders and subject matter experts to comprehensively
    understand the required data elements, relationships, and constraints. Entity
    analysis and definition follow, which involves identifying the key objects or
    concepts to be represented in the database and defining their attributes and relationships.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库建模的*概念阶段*需要几个关键步骤。首先，需要确定数据库的目的和目标，并明确它需要解决的具体问题或要求。接下来是通过采访利益相关者和主题专家来收集需求，全面了解所需的数据元素、关系和约束条件。随后进行实体分析和定义，这涉及识别要在数据库中表示的关键对象或概念，并定义它们的属性和关系。
- en: We begin with a light normalization as we design the initial sketches for the
    appearance of the database. This ensures integrity between the identified entities
    and relationships and minimizes redundancy by organizing entities and attributes
    around semantic structures that are conceptually related. Identifying keys, including
    primary and foreign keys, is critical to maintaining uniqueness and establishing
    relationships among tables.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在设计数据库外观的初始草图时，首先进行轻度规范化，以确保识别的实体和关系之间的完整性，并通过将实体和属性组织在概念上相关的语义结构周围来减少冗余。识别包括主键和外键在内的键是保持唯一性和在表之间建立关系的关键。
- en: These database designs are often created through diagrams, textual descriptions,
    or other methods that capture and effectively convey the design and concepts of
    the database. One of the most commonly used tools to visually represent database
    concepts is the entity-relationship diagram (ERD). The visual models created using
    an ERD serve as a diagrammatic representation that effectively describes the entities
    to be modeled, their relationships, and the cardinality of those relationships.
    By employing the ERD model, we can visually describe the database structure, including
    the entities as the main components, the connections or associations among entities,
    and the quantity or extent of the relationships.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据库设计通常通过图表、文字描述或其他捕捉和有效传达数据库设计和概念的方法创建。最常用的视觉表示数据库概念的工具之一是实体-关系图（ERD）。使用
    ERD 创建的视觉模型作为有效描述被建模实体、它们的关系以及这些关系的基数的图表表示。通过采用 ERD 模型，我们可以视觉描述数据库结构，包括实体作为主要组成部分、实体之间的连接或关联，以及关系的数量或程度。
- en: 'Let’s do a very simple conceptual design of a database. Imagine O’Reilly aims
    to track books and authors previously published, along with launch dates for new
    books yet to be published. We engage in a series of interviews with the publisher’s
    managers and start to understand exactly what data needs to be stored in the database.
    The main goal is to identify the entities involved, their relationships, and the
    attributes of each entity. Keep in mind this exercise is illustrative and is simplified
    on purpose. We identify three distinct entities in this sub-universe of book management:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来做一个非常简单的数据库概念设计。想象一下，O'Reilly 的目标是跟踪以前出版的书籍和作者，以及尚未出版的新书的发布日期。我们与出版商的经理进行一系列的访谈，并开始准确理解需要存储在数据库中的数据。主要目标是识别涉及的实体、它们之间的关系以及每个实体的属性。请记住，这个练习是说明性的，并且是有意简化的。我们在书籍管理的这个子领域中识别了三个不同的实体：
- en: Book
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍
- en: This entity represents a book published by O’Reilly. Attributes may include
    `book_id`, `title`, `publication_date`, `ISBN`, `price`, and a particular category.
    The interviewers said that one book may have only one category in this model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此实体代表 O'Reilly 出版的一本书。属性可能包括`book_id`、`title`、`publication_date`、`ISBN`、`price`，以及一个特定的类别。采访者表示，在这个模型中，一本书可能只有一个类别。
- en: Author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作者
- en: This entity represents an author who has written books for O’Reilly. Attributes
    may include `author_id`, `author_name`, `email`, and `bio`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此实体代表为 O'Reilly 写书的作者。属性可能包括`author_id`、`author_name`、`email`，以及`bio`。
- en: Category
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 类别
- en: This entity represents the book category and can contain attributes such as
    `category_id` as the unique identifier and `category_name`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此实体代表书籍类别，可以包含诸如`category_id`作为唯一标识符和`category_name`等属性。
- en: 'The next step would be to identify relationships among entities. In database
    design, a few types of relationships can exist among entities, and the type of
    relationship can be called the relationship’s *cardinality*. For example, in a
    one-to-one relationship, we could have a Book entity connected to an Author entity,
    where each book is associated with a single author, and vice versa. In a one-to-many
    relationship, consider a Category entity linked to a Book entity, where each book
    can belong to only one category, but each category can have multiple books. Contrarily,
    in a many-to-one relationship, think of a Publisher entity connected to a Book
    entity, where the same publisher publishes multiple books. Finally, in a many-to-many
    relationship, we could have a Book entity associated with a Reader entity, indicating
    that multiple readers can have multiple books in their possession. Continuing
    our exercise, we also identified two clear relationships:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是识别实体之间的关系。在数据库设计中，实体之间可以存在几种类型的关系，关系的类型可以称为关系的*基数*。例如，在一对一的关系中，我们可以有一个书籍实体连接到一个作者实体，其中每本书都与一个作者相关联，反之亦然。在一对多的关系中，考虑一个类别实体与一个书籍实体连接，其中每本书只能属于一个类别，但每个类别可以有多本书。相反，在多对一的关系中，想象一个出版商实体连接到一个书籍实体，同一个出版商出版多本书。最后，在多对多的关系中，我们可以有一个书籍实体与一个读者实体相关联，表示多个读者可能拥有多本书。继续我们的练习，我们还确定了两个明确的关系：
- en: Book-Category relationship
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 书-类别关系
- en: Establishes the connection between books and categories. A book can have one
    category, and a category can have multiple books. This relationship is represented
    as a one-to-many relationship.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 建立书籍和类别之间的连接。一本书可以有一个类别，一个类别可以有多本书。这种关系表示为一对多关系。
- en: Book-Author relationship
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍-作者关系
- en: Establishes the connection between books and authors. A book can have multiple
    authors, and an author can write multiple books. This relationship is represented
    as a many-to-many relationship. It’s in this relationship that a specific book’s
    publication happens.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 建立书籍和作者之间的关系。一本书可以有多位作者，一个作者可以写多本书。这种关系表示为多对多关系。在这个关系中，特定书籍的出版发生。
- en: When identifying relationships, using relationship names that represent the
    real interaction between entities is common. For example, instead of calling it
    Book-Category, we could say *Classifies* because the category classifies the book,
    or instead of Book-Author, we might say *Publishes* because the author has books
    published.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别关系时，通常使用能够代表实体间真实交互的关系名称。例如，我们可以称之为*分类*而不是Book-Category，因为类别对书籍进行分类，或者我们可以称之为*出版*而不是Book-Author，因为作者出版了书籍。
- en: Now that we have an idea of entities, attributes, and relationships, we have
    what is needed to design our database with an ERD. By doing so, we can visually
    represent the entities, relationships, and cardinality, as shown in [Figure 2-1](#ERDiagram).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对实体、属性和关系有了一个概念，我们可以使用ERD设计我们的数据库。通过这样做，我们可以直观地表示实体、关系和基数，如[图2-1](#ERDiagram)所示。
- en: '![ch03_er_01](assets/aesd_0201.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![ch03_er_01](assets/aesd_0201.png)'
- en: Figure 2-1\. ERD example for the books database
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 书籍数据库的ERD示例
- en: As we can observe, entities are represented as while rectangular boxes and represent
    real-world objects or concepts, such as Book or Author. Relationships are represented
    as diamonds and illustrate how entities are related.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，实体以白色矩形框表示，代表现实世界的对象或概念，如Book或Author。关系以菱形表示，说明实体之间的关系。
- en: Attributes are represented as shaded boxes and describe properties or characteristics
    of an entity. An example could be Name or Publish date. In addition, attributes
    can be classified as key attributes (underlined shaded boxes) that uniquely identify
    an entity or as nonkey attributes (nonunderlined shaded boxes) that provide additional
    information about an entity. More types of attributes exist when designing such
    diagrams, but we will stick with the basics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 属性表示为阴影框，描述实体的属性或特征。例如，名称或出版日期。此外，属性可以分类为关键属性（带有下划线的阴影框），这些属性唯一标识实体，或非关键属性（非下划线的阴影框），提供有关实体的额外信息。在设计这类图表时还存在更多类型的属性，但我们将坚持基础知识。
- en: Other components in an ERD include cardinality and participation constraints.
    Cardinality defines the number of instances in a relationship, usually represented
    with symbols such as 1, M, or N to indicate a one-to-one or one-to-many relationship,
    respectively. (N indicates that there is an undetermined number of relationships.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ERD中的其他组成部分包括基数和参与约束。基数定义关系中实例的数量，通常用符号1、M或N表示，表示一对一或一对多关系，分别表示未确定数量的关系（N表示）。
- en: The Logical Phase of Modeling
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模的逻辑阶段
- en: In the *logical phase* of modeling, the focus is on normalizing the data to
    eliminate redundancies, improve data integrity, and optimize query performance.
    The result is a normalized logical model that accurately reflects the relationships
    and dependencies among entities.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模的*逻辑阶段*，重点是将数据规范化，消除冗余，提高数据完整性，并优化查询性能。结果是一个规范化的逻辑模型，准确反映实体之间的关系和依赖。
- en: This phase can be divided into two steps. First, the restructuring of the Entity-Relationship
    schema focuses on optimizing the schema based on specific criteria. This step
    is not tied to any particular logical model. The second step translates the optimized
    ERD into a specific logical model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该阶段可分为两步。首先，实体关系模式的重构侧重于根据特定标准优化模式。此步骤与任何特定的逻辑模型无关。第二步将优化的ERD转换为特定的逻辑模型。
- en: Assuming we have decided to map the ERD to a relational database model (which
    will be our case)—as opposed to a document or graph database—each entity from
    the conceptual ERD exercise is represented as a table. The attributes of each
    entity become the columns of the respective table. The primary-key constraint
    is indicated for the primary-key columns of each table. Additionally, the many-to-many
    relationships are represented by separate junction tables that hold the foreign
    keys referencing the corresponding entities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已决定将ER图映射到关系数据库模型（这将是我们的情况），而不是文档或图形数据库，那么每个从概念性ER图练习中得到的实体将被表示为一个表。每个实体的属性将成为相应表的列。为每个表的主键列指示主键约束。此外，多对多关系由单独的连接表表示，这些表包含引用相应实体的外键。
- en: 'By translating the conceptual ERD exercise into a logical schema using the
    relational model, we establish a structured representation of the entities, their
    attributes, and their relationships. This logical schema can be a foundation for
    implementing the database in a specific database management system (DBMS) while
    remaining independent of any particular system. To achieve this translation effectively,
    all the normalization steps apply, but we would like to share an effective algorithm:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将概念性的ER图练习转换为使用关系模型的逻辑模式后，我们建立了实体、它们的属性以及它们之间关系的结构化表示。这个逻辑模式可以作为在特定数据库管理系统（DBMS）中实现数据库的基础，同时保持独立于任何特定系统。为了有效地进行这种转换，需要应用所有的规范化步骤，但我们愿意分享一个有效的算法：
- en: Entity *E* is converted to table *T*.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体*E*被转换为表*T*。
- en: The name of *E* becomes the name of *T*.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体*E*的名称成为表*T*的名称。
- en: The primary key of *E* becomes the primary key of *T*.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体*E*的主键成为表*T*的主键。
- en: Simple attributes of *E* become simple attributes of *T*.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体*E*的简单属性成为表*T*的简单属性。
- en: 'When it comes to relationships, we can also share a few steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到关系时，我们也可以分享一些步骤：
- en: N:1 relationships
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: N:1关系
- en: A foreign key is defined in table T1 that references the primary key of table
    T2\. This establishes the connection between the two tables, indicating the N:1
    relationship. The attributes (Attrs) associated with the relationship are mapped
    and included in table T1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在表T1中定义一个外键，引用表T2的主键。这建立了两个表之间的连接，指示了N:1的关系。与关系相关的属性（Attrs）被映射并包含在表T1中。
- en: N:N relationships
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: N:N关系
- en: A specific *cross-reference table* is created to represent the relationship
    REL. The primary key of REL is defined as the combination of the primary keys
    of both tables T1 and T2, which act as foreign keys in the cross-reference table.
    The attributes (Attrs) associated with the relationship are mapped and included
    in the cross-reference table.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个特定的*交叉参照表*来表示关系REL。REL的主键被定义为作为外键在交叉参照表中的两个表T1和T2的主键的组合。与关系相关的属性（Attrs）被映射并包含在交叉参照表中。
- en: Now let’s apply these rules to our previous conceptual model; see [Figure 2-2](#LogicalDiagram).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这些规则应用到我们先前的概念模型中；参见[图2-2](#LogicalDiagram)。
- en: '![ch03_logical_01](assets/aesd_0202.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![ch03_logical_01](assets/aesd_0202.png)'
- en: Figure 2-2\. Logical ERD example for the books database
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2：图书数据库的逻辑ERD示例
- en: In our example, we have a few entities that, as our algorithm suggests, are
    directly mapped to tables. Such is the case of Authors, Books, and Category.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，有一些实体，如我们的算法所建议的那样，直接映射为表。这种情况包括作者、书籍和类别。
- en: We identified a 1:N relationship between Books and Category where one book has
    one category, but one category has multiple books. To map this relationship, we
    create a foreign key in the `books` table to reference the corresponding category.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确定了图书和类别之间的1:N关系，其中一本书对应一个类别，但一个类别可以有多本书。为了映射这种关系，我们在`books`表中创建一个外键，引用相应的类别。
- en: We also have an N:N relationship. In this case, we must create a new table (cross-reference
    table) that stores the relationship. In our case, we create the Publishes tables
    for which the primary key becomes a composite between the related entities (Book
    ID and Author ID). At the same time, the attributes of the relationship become
    attributes of this cross-reference table.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个N:N的关系。在这种情况下，我们必须创建一个新的表（交叉参照表），用于存储这种关系。在我们的案例中，我们创建了一个名为Publishes的表，其主键成为相关实体（Book
    ID和Author ID）的复合键。与此同时，关系的属性成为这个交叉参照表的属性。
- en: The Physical Phase of Modeling
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模的物理阶段
- en: We are now ready to convert the normalized logical model into a physical database
    design in a step called the *physical phase*, or *physical model creation*. This
    step defines storage structures, indexing strategies, and data types to ensure
    efficient data storage and retrieval. While the logical model focuses on the conceptual
    representation, the physical model deals with the implementation details required
    for smooth data management.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备将归一化的逻辑模型转换为物理数据库设计，这一步称为*物理阶段*或*物理模型创建*。这一步定义了存储结构、索引策略和数据类型，以确保数据的高效存储和检索。而逻辑模型侧重于概念表示，物理模型则处理平滑数据管理所需的实施细节。
- en: In our case, let’s continue from the previous logical model and assume we would
    be working with a MySQL database engine. [Example 2-1](#reference_phisic_model)
    shows the physical model of the books database.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，让我们继续从前面的逻辑模型，并假设我们将使用MySQL数据库引擎。[示例 2-1](#reference_phisic_model)显示了图书数据库的物理模型。
- en: Example 2-1\. The books database in a physical model
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-1\. 物理模型中的图书数据库
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In [Example 2-1](#reference_phisic_model), we’ve created four tables: `category`,
    `books`, `authors`, and `publishes`. The physical design aspect fine-tunes the
    table structures, data types, and constraints to align with the MySQL database
    system.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 2-1](#reference_phisic_model)中，我们创建了四个表：`category`、`books`、`authors`和`publishes`。物理设计方面对表结构、数据类型和约束进行了微调，以与MySQL数据库系统对齐。
- en: For example, in the `category` table, we could specify the data type for the
    `c⁠a⁠t⁠e⁠g⁠o⁠r⁠y​_⁠i⁠d` column as `INT`, ensuring that it is suitable for storing
    integer values while defining it as a primary key, given that it identifies unique
    records on the table. Similarly, the `category_name` column could be defined as
    `VARCHAR(255)` to accommodate variable-length category names.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在`category`表中，我们可以将`c⁠a⁠t⁠e⁠g⁠o⁠r⁠y​_⁠i⁠d`列的数据类型指定为`INT`，确保其适合存储整数值，同时将其定义为主键，因为它标识表上的唯一记录。类似地，`category_name`列可以定义为`VARCHAR(255)`，以容纳可变长度的类别名称。
- en: In the `books` table, appropriate data types and lengths can be assigned to
    columns such as `book_id (INT)`, `ISBN (VARCHAR(13))`, `title (VARCHAR(50))`,
    and `summary (VARCHAR(255))`. Additionally, the `category_id` column can be configured
    as a foreign key referencing the `category_id` column in the `category` table.
    Note that each ISBN code is composed of 13-character-length strings. Thus, we
    don’t need bigger strings than that.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在`books`表中，可以为诸如`book_id (INT)`、`ISBN (VARCHAR(13))`、`title (VARCHAR(50))`和`summary
    (VARCHAR(255))`等列分配适当的数据类型和长度。此外，`category_id`列可以配置为引用`category`表中的`category_id`列的外键。注意，每个ISBN代码由长度为13个字符的字符串组成，因此我们不需要比这更大的字符串。
- en: Similarly, in the `authors` table, data types can be defined for columns such
    as `author_id (INT)`, `author_name (VARCHAR(255))`, and `date_birth (DATETIME)`,
    all respecting the expected domain of values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在`authors`表中，可以为诸如`author_id (INT)`、`author_name (VARCHAR(255))`和`date_birth
    (DATETIME)`等列定义数据类型，所有这些都符合预期的值域。
- en: In the `publishes` table, we highlight that we defined foreign-key constraints
    to establish the relationships between the `book_id` column in the `books` table
    and the `author_id` column in the `authors` table. At the same time, the foreign
    key is composed of the primary keys of the two tables it’s relating.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在`publishes`表中，我们强调我们定义了外键约束，以建立`books`表中的`book_id`列与`authors`表中的`author_id`列之间的关系。同时，外键由两个表的主键组成。
- en: After all these steps, we’ve successfully moved from requirements to concept
    to a logical relational model and finished with a practical implementation of
    the model in MySQL, thus building our database.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 经过所有这些步骤，我们已成功从需求到概念再到逻辑关系模型，并在MySQL中完成了模型的实际实现，从而构建了我们的数据库。
- en: The Data Normalization Process
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据归一化过程
- en: The data normalization technique consists of several steps, each aimed at organizing
    data into logical and efficient structures. [Example 2-2](#reference_normalisation_0)
    illustrates the `books` table containing a few relevant attributes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化技术由几个步骤组成，每个步骤旨在将数据组织成逻辑和高效的结构。[示例 2-2](#reference_normalisation_0)说明了包含几个相关属性的`books`表。
- en: Example 2-2\. `books` table to be normalized
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-2\. `books`表需要归一化
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first step in normalization, known as the *first normal form* (1NF), requires
    eliminating repeating groups by breaking the data into smaller atomic units. We’ll
    create a table called `authors` that includes the author ID and author’s name.
    The `books` table now references the author ID instead of storing the full name
    repeatedly, as shown in [Example 2-3](#reference_normalisation_1).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化的第一步，称为*第一范式*（1NF），要求通过将数据拆分为较小的原子单元来消除重复组。我们将创建一个名为`authors`的表，包含作者ID和作者姓名。现在，`books`表引用作者ID而不是重复存储全名，如[示例 2-3](#reference_normalisation_1)中所示。
- en: Example 2-3\. `books` table in 1NF
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-3\. 1NF中的`books`表
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Moving to the *second normal form* (2NF), we examine dependencies within the
    data. We observe that the publication year functionally depends on the book ID,
    while the genre depends on the author ID. To adhere to 2NF, we split the `books`
    table into three tables:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移到第二范式（2NF），我们检查数据内部的依赖关系。我们观察到出版年份功能上依赖于书籍ID，而流派依赖于作者ID。为了遵守2NF，我们将`books`表拆分为三个表：
- en: '`books`, containing book ID and title'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`books`，包含书籍ID和标题'
- en: '`authors`, with author ID and name'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`authors`，包含作者ID和姓名'
- en: '`bookDetails`, storing book ID, publication year, and genre'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bookDetails`，存储书籍ID、出版年份和流派'
- en: This ensures that each column depends solely on the primary key, as shown in
    [Example 2-4](#reference_normalisation_2).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保每一列仅依赖于主键，如[示例 2-4](#reference_normalisation_2)所示。
- en: Example 2-4\. `books` table in 2NF
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-4\. 2NF中的`books`表
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The *third normal form* (3NF) focuses on eliminating transitive dependencies.
    We realize that the genre can be derived from the book ID through the `bookDetails`
    table. To resolve this, we create a new table called `genres` with genre ID and
    genre name, and the `bookDetails` table now references the genre ID instead of
    storing the genre name directly ([Example 2-5](#reference_normalisation_3)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第三范式（3NF）侧重于消除传递依赖。我们意识到流派可以通过`bookDetails`表从书籍ID派生出来。为了解决这个问题，我们创建了一个名为`genres`的新表，包含流派ID和流派名称，而`bookDetails`表现在引用流派ID而不是直接存储流派名称（[示例 2-5](#reference_normalisation_3)）。
- en: Example 2-5\. `books` table in 3NF
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-5\. 3NF中的`books`表
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These resulting normalized structures (3NF) are often used in operational systems,
    also known as *online transaction processing systems* (OLTP), that are designed
    to efficiently process and store transactions and retrieve transaction data, such
    as customer orders, bank transactions, or even payroll. It’s important to highlight
    that we can apply further normalization steps if necessary, such as the fourth
    normal form (4NF) and fifth normal form (5NF), to address complex data dependencies
    and ensure even higher levels of data integrity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些产生的标准化结构（3NF）通常用于操作系统中，也称为*在线事务处理系统*（OLTP），旨在有效处理和存储交易，并检索交易数据，例如客户订单、银行交易甚至工资单。重要的是要强调，如果需要，我们可以应用进一步的规范化步骤，如第四范式（4NF）和第五范式（5NF），以解决复杂的数据依赖关系，并确保更高水平的数据完整性。
- en: Data normalization is crucial to achieve efficient processing and storage of
    individual transactions in an OLTP system. In this process, the data is divided
    into smaller, less redundant parts to achieve this goal, bringing several advantages
    to OLTP systems. Data normalization is known for its emphasis on reducing data
    redundancy and improving data integrity because data is organized into multiple
    tables, each serving a specific purpose. These tables are linked by primary and
    foreign keys to establish their relationships, ensuring that records in each table
    are unique and that the same field is not replicated in multiple tables, except
    for key fields or in-system fields such as ID or creation timestamps.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据规范化对于实现OLTP系统中个体事务的高效处理和存储至关重要。在这个过程中，数据被划分为较小、冗余较少的部分，以实现这一目标，并为OLTP系统带来多个优势。数据规范化以减少数据冗余和提高数据完整性而闻名，因为数据被组织成多个表，每个表都有特定的用途。这些表通过主键和外键进行链接以建立它们的关系，确保每个表中的记录是唯一的，并且同一字段不会在多个表中重复，除了关键字段或系统字段，如ID或创建时间戳。
- en: Another reason data normalization is relevant is that it enhances and maximizes
    performance. These normalized databases are designed to efficiently handle fast
    reads and writes by minimizing data redundancy and establishing well-defined relationships
    among tables so that the database can handle a large number of transactions with
    lightning-fast performance. This is important for transactional systems in which
    the timely execution of operations is critical.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化相关的另一个原因是它增强和最大化了性能。这些归一化数据库通过最小化数据冗余并在表之间建立明确定义的关系，设计成能够高效处理快速读写操作，从而可以处理大量的事务并具有闪电般的性能。这对于操作时间关键的事务系统非常重要。
- en: Last but not least, a normalized database focuses on storing only current data
    so that the database represents the most current information available. In a table
    that stores customer information, each record always reflects the customer’s up-to-date
    details, such as first name, phone number, and other relevant data, ensuring that
    the database accurately represents the current state of affairs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，归一化数据库专注于仅存储当前数据，以便数据库反映最新可用信息。在存储客户信息的表中，每条记录始终反映客户的最新详细信息，如名字、电话号码和其他相关数据，确保数据库准确反映当前事务状态。
- en: However, the paradigm is somewhat different when it comes to an analytics project
    or system. Often, users want to be able to retrieve the data they need without
    having to do a lot of linking, which is a natural consequence of a normalization
    process. While an OLTP system is optimized for write operations to avoid increased
    latency in live systems like a web application, users of analytics systems want
    read optimization to get their analytics data as quickly as possible. Unlike normalized
    transactional databases that store live data, analytics databases are expected
    to contain both real-time and non-real-time data and act as a historical archive
    for past data. And often, an analytics database is expected to contain data from
    multiple OLTP systems to provide an integrated view of business processes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉及到分析项目或系统时，范式有些不同。通常，用户希望能够在不进行大量连接的情况下检索所需的数据，这是归一化过程的自然结果。虽然OLTP系统经过优化，以避免在诸如Web应用等实时系统中出现延迟增加的写操作，但分析系统的用户希望进行读取优化，以尽快获取其分析数据。与存储实时数据的归一化事务性数据库不同，分析数据库预计包含实时和非实时数据，并充当过去数据的历史存档。而且，通常期望分析数据库包含来自多个OLTP系统的数据，以提供业务流程的集成视图。
- en: These differences are indeed critical to grasp as they underpin distinct requirements
    for data organization, retention, and utilization. However, it’s important to
    clarify that what we’ve just explored pertains primarily to the realm of normalization
    for performance optimization and adhering to best practices in OLTP database design.
    While this foundation is valuable, it represents just one facet of the broader
    landscape of analytics engineering.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这些差异至关重要，因为它们支撑着数据组织、保留和利用的不同要求。然而，重要的是澄清我们刚刚探讨的主要是规范化在性能优化和遵循OLTP数据库设计最佳实践方面的基础。虽然这一基础非常宝贵，但它只代表了分析工程更广阔景观中的一个方面。
- en: To provide a clearer roadmap, let’s establish that our journey begins with an
    exploration of this foundational type of data modeling, which forms the basis
    for OLTP systems. Following this, we will pivot toward a discussion of data modeling
    approaches optimized for OLAP environments. By making this distinction, we aim
    to provide a comprehensive understanding of both aspects of data modeling, setting
    the stage for a deeper dive into analytics engineering methodologies and their
    application in the subsequent sections.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更清晰的路线图，让我们确立我们的旅程从探索这种基础数据建模开始，这种建模形成了OLTP系统的基础。接下来，我们将转向讨论针对OLAP环境优化的数据建模方法。通过做出这种区分，我们旨在全面理解数据建模的两个方面，为深入探讨分析工程方法论及其在后续章节中的应用做好铺垫。
- en: Dimensional Data Modeling
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度数据建模
- en: Data modeling is a fundamental aspect of designing and organizing databases
    to store and manage data efficiently. As we previously discussed, it involves
    defining the structure, relationships, and attributes of the data entities within
    a system.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模是设计和组织数据库以高效存储和管理数据的基本方面。正如我们之前讨论的，它涉及在系统内定义数据实体的结构、关系和属性。
- en: One popular approach to data modeling is *dimensional modeling*, which focuses
    on modeling data to support analytics and reporting requirements. Dimensional
    modeling is particularly well suited for data warehousing and BI applications.
    It emphasizes the creation of dimensional models that consist of fact tables representing
    measurable data and dimension tables providing descriptive context. By using dimensional
    modeling techniques, such as star schemas and snowflake schemas, data can be organized
    in a way that simplifies complex queries and enables efficient data analysis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模的一种流行方法是*维度建模*，其重点在于为支持分析和报告需求而进行数据建模。维度建模特别适用于数据仓库和BI应用程序。它强调创建包含事实表和提供描述性上下文的维度表的维度模型。通过使用维度建模技术，如星型模式和雪花模式，数据可以以简化复杂查询并实现高效数据分析的方式进行组织。
- en: The relationship between data modeling and dimensional modeling lies in their
    complementary nature. Data modeling provides the foundation for capturing and
    structuring data, whereas dimensional modeling offers a specialized technique
    for modeling data to support analytical and reporting needs. Jointly, these approaches
    enable organizations to design robust and flexible databases that facilitate transactional
    processing and in-depth data analysis.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模与维度建模之间的关系在于它们的互补性质。数据建模为捕获和结构化数据提供基础，而维度建模则提供了一种专门的建模数据以支持分析和报告需求的技术。这些方法共同促使组织设计健壮且灵活的数据库，从而便于事务处理和深入数据分析。
- en: 'To understand dimensional modeling, we should first pay our respect to two
    individuals considered to be the fathers of data warehousing and dimensional modeling:
    Bill Inmon and Ralph Kimball, respectively. They are recognized as pioneers in
    the field of enterprise-wide information gathering, management, and analytics
    for decision support.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解维度建模，我们首先应该向被认为是数据仓库和维度建模之父的两位先驱致敬：分别是Bill Inmon和Ralph Kimball。他们被公认为企业级信息收集、管理和决策支持分析的先驱。
- en: They have contributed to a significant debate on the topic of data warehousing,
    each advocating for different philosophies and approaches. Inmon proposes the
    creation of a centralized data warehouse that encompasses the entire enterprise,
    aiming to generate a comprehensive BI system. On the other hand, Kimball suggests
    creating multiple smaller data marts focused on specific departments, enabling
    department-level analysis and reporting. Their divergent viewpoints result in
    contrasting design techniques and implementation strategies for data warehousing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 它们对数据仓库话题的重要讨论做出了贡献，每种方法都主张不同的哲学和方法。Inmon提议创建一个涵盖整个企业的集中式数据仓库，旨在生成全面的BI系统。另一方面，Kimball建议创建多个小型数据集市，专注于特定部门，从而实现部门级别的分析和报告。他们不同的观点导致了数据仓库的对比设计技术和实施策略。
- en: In addition to their differing approaches, Inmon and Kimball propose distinct
    methods for structuring data in the context of data warehousing. Inmon advocates
    for using the relational (ERD) model, specifically the third normal form (3NF),
    in the enterprise data warehouse. On the contrary, Kimball’s approach employs
    a multidimensional model in the dimensional data warehouse, utilizing star schemas
    and snowflakes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了他们不同的方法之外，Inmon和Kimball还提出了在数据仓库的数据结构化背景下使用不同的方法。Inmon主张在企业数据仓库中使用关系（ERD）模型，特别是第三范式（3NF）。相反，Kimball的方法在维度数据仓库中采用多维模型，利用星型模式和雪花模式。
- en: Inmon argues that structuring data in a relational model ensures enterprise-wide
    consistency. This consistency facilitates the creation of data marts in the dimensional
    model with relative ease. On the other hand, Kimball contends that organizing
    data in a dimensional model facilitates the information bus, allowing users to
    comprehend, analyze, aggregate, and explore data inconsistencies more effectively.
    Moreover, Kimball’s approach enables direct access to data from analytics systems.
    In contrast, Inmon’s approach restricts analytics systems from accessing data
    solely from the enterprise data warehouse, necessitating interaction with data
    marts for retrieval.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Inmon主张在关系模型中组织数据可确保企业范围内的一致性。这种一致性有助于在维度模型中相对轻松地创建数据集市。另一方面，Kimball认为在维度模型中组织数据有助于信息总线，允许用户更有效地理解、分析、聚合和探索数据不一致性。此外，Kimball的方法使得可以直接从分析系统访问数据。相比之下，Inmon的方法限制了分析系统仅从企业数据仓库访问数据，需要与数据集市互动来检索数据。
- en: Tip
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A *data mart* is a specific part of a data warehouse that is meant to fulfill
    the unique demands of a particular department or business unit.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据集市*是数据仓库的一个特定部分，旨在满足特定部门或业务单位的独特需求。'
- en: 'In the following sections, we will delve deep into three modeling techniques:
    star schema, snowflake modeling, and the emerging Data Vault. Data Vault, introduced
    by Dan Linstedt in 2000, has been gaining momentum in recent years. It follows
    a more normalized structure, which is not entirely aligned with Inmon’s approach
    but is similar.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨三种建模技术：星型模式、雪花模型和新兴的数据仓库。数据仓库，由Dan Linstedt于2000年推出，近年来已经获得了迅速发展。它遵循更规范化的结构，虽然与Inmon的方法并不完全一致，但相似。
- en: Modeling with the Star Schema
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用星型模式建模
- en: The *star schema* is a widely used modeling approach in relational data warehouses,
    especially for analysis and reporting purposes. It involves classifying tables
    as either dimension tables or fact tables to effectively organize and represent
    business units and related observations or events.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*星型模式*是关系数据仓库中广泛使用的建模方法，尤其适用于分析和报告目的。它将表格分类为维度表和事实表，以有效组织和表示业务单位及相关观察或事件。'
- en: Dimension tables are used to describe the business entities to be modeled. These
    entities can include various aspects such as products, people, places, and concepts,
    including time. In a star schema, you will typically find a date dimension table
    that provides a comprehensive set of dates for analysis. A dimension table usually
    consists of one or more key columns that serve as unique identifiers for each
    entity, as well as additional descriptive columns that provide further information
    about the entities.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 维度表用于描述要建模的业务实体。这些实体可以包括产品、人员、地点和概念等各个方面，包括时间。在星型模式中，您通常会找到一个日期维度表，为分析提供全面的日期集合。维度表通常包括一个或多个键列，这些列作为每个实体的唯一标识符，以及提供有关实体的进一步信息的附加描述列。
- en: Fact tables, on the other hand, store observations or events that occur in the
    business. These include sales orders, inventory levels, exchange rates, temperatures,
    and other measurable data. A fact table contains dimension key columns, which
    refer to the dimension tables, and numeric measurement columns. The dimension
    key columns determine the dimensionality of the fact table and specify which dimensions
    are included in the analysis. For example, a fact table that stores sales targets
    may contain dimension key columns for Date and ProductKey, indicating that the
    analysis includes dimensions related to time and products.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，事实表存储发生在业务中的观察或事件。这些包括销售订单、库存水平、汇率、温度和其他可测量数据。事实表包含维度键列，这些列指向维度表，以及数值测量列。维度键列确定事实表的维度，并指定分析中包含的维度。例如，存储销售目标的事实表可能包含日期和产品键的维度键列，表明分析包括与时间和产品相关的维度。
- en: The granularity of a fact table is determined by the values in its dimension
    key columns. If the Date column in a sales target fact tables stores values representing
    the first day of each month, for example, then the granularity of the table is
    at the Month/Product level. This means that the fact table captures sales target
    data at the monthly level, specific to each product.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 事实表的粒度由其维度键列中的值决定。例如，如果销售目标事实表中的日期列存储表示每个月第一天的值，那么表的粒度就在月份/产品级别。这意味着事实表捕获了每个产品的月度销售目标数据。
- en: By structuring data in a star schema with dimension tables representing business
    units and fact tables capturing observations or events, companies can efficiently
    perform complex analysis and gain meaningful insights. The star schema provides
    a clear and intuitive structure for querying and aggregating data, making it easier
    to analyze and understand the relationships among dimensions and facts within
    the dataset.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在星型模式中结构化数据，其中维度表表示业务单元，而事实表捕获观察或事件，公司可以有效地执行复杂分析并获得有意义的洞见。星型模式为查询和聚合数据提供了清晰和直观的结构，使得分析和理解数据集中维度与事实之间的关系变得更加容易。
- en: Returning to our `books` table, we will follow the modeling steps to develop
    a simple star schema model. The first step would be to identify the dimension
    tables. But first, let’s remember our base table in [Example 2-6](#reference_ss_0).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的`books`表，我们将遵循建模步骤来开发一个简单的星型模型。第一步将是识别维度表。但首先，让我们记住我们的基础表在[示例2-6](#reference_ss_0)中。
- en: Example 2-6\. Base table for our star schema
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-6\. 我们星型模型的基础表
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We should identify all the individual dimensions (attributes related to a particular
    business entity) in the `books` table and create separate dimension tables for
    each. In our example, and just as in the normalization steps, we identify three
    entities: books, authors, and genres. Let’s see the physical model with [Example 2-7](#reference_ss_1).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在`books`表中识别所有单独的维度（与特定业务实体相关的属性），并为每个创建单独的维度表。在我们的示例中，就像在规范化步骤中一样，我们识别了三个实体：books、authors和genres。让我们看看与[示例2-7](#reference_ss_1)相对应的物理模型。
- en: Example 2-7\. Dimension tables for our star schema
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-7\. 我们星型模型的维度表
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When it comes to naming dimension tables, it is recommended to use descriptive
    and intuitive names that reflect the entities they represent. For example, if
    we have a dimension table representing books, we could name it `dimBook` or simply
    `books`. Similarly, relevant and self-explanatory names like `dimAuthor` or `dimGenre`
    can be used for dimension tables representing authors, genres, or other entities.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当命名维度表时，建议使用描述性和直观的名称反映它们所代表的实体。例如，如果我们有一个表示书籍的维度表，我们可以命名为`dimBook`或简单地`books`。类似地，像`dimAuthor`或`dimGenre`这样的相关且自解释的名称可以用于代表作者、流派或其他实体的维度表。
- en: For the fact tables, it is advisable to use names that indicate the measurements
    or events being captured. For instance, if we have a fact table recording book
    sales, we could name it `factBookSales` or `salesFact`. These names indicate that
    the table contains data related to book sales.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于事实表，建议使用能够指示所捕获测量或事件的名称。例如，如果我们有一个记录书籍销售数据的事实表，我们可以命名为`factBookSales`或`salesFact`。这些名称表明该表包含与书籍销售相关的数据。
- en: We can now create a fact table called `factBookPublish`, as shown in [Example 2-8](#reference_ss_2),
    to capture publication data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个名为`factBookPublish`的事实表，如[示例2-8](#reference_ss_2)所示，来捕获出版数据。
- en: Example 2-8\. Fact table for our star schema
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-8\. 我们星型模型的事实表
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code creates a new fact table `factBookPublish` with columns representing
    the measurements or events related to the dimensions. In this case, it’s only
    the publication year. The foreign-key constraints establish the relationships
    between the fact table and the dimension tables.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建一个名为`factBookPublish`的新事实表，其列代表与维度相关的测量或事件。在这种情况下，只有出版年份。外键约束建立了事实表与维度表之间的关系。
- en: With the star schema model representing the books dataset, we now have a strong
    foundation for conducting various analytical operations and extracting valuable
    insights. The dimensional structure of the star schema allows for efficient and
    intuitive querying, enabling us to explore the data from different perspectives.
    Once we finish the modeling process, we should end up with a model similar to
    [Figure 2-3](#star_schema) that resembles a star, thus its name, star schema.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有了星型模型代表的书籍数据集，我们现在有了进行各种分析操作和提取宝贵洞见的坚实基础。星型模式的维度结构允许高效和直观地查询，使我们能够从不同角度探索数据。一旦我们完成建模过程，我们应该得到一个类似于[图2-3](#star_schema)的模型，它类似于星星，因此被称为星型模式。
- en: '![ch03_star_schema](assets/aesd_0203.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![ch03_star_schema](assets/aesd_0203.png)'
- en: Figure 2-3\. Star schema model
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 星型模型
- en: Using this model, we can now easily analyze book publications by applying filters
    such as genre, author, or publication year. For instance, we could quickly retrieve
    the total publications for a specific genre. By joining the dimension tables with
    the fact table, as represented in [Example 2-9](#reference_ss_3), we can effortlessly
    gain insights into the relationships among books, authors, genres, and sales.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种模型，我们现在可以通过应用诸如流派、作者或出版年份等筛选器轻松分析图书出版情况。例如，我们可以快速检索特定流派的总出版物数量。通过将维度表与事实表连接，如在[示例 2-9](#reference_ss_3)中所示，我们可以轻松地深入了解书籍、作者、流派和销售之间的关系。
- en: Example 2-9\. Retrieving data from a star schema
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-9\. 从星型模式检索数据
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, we used a `LEFT JOIN` when joining a fact table with a dimension
    table. This is quite common. It ensures that all the records from the fact table
    are included in the result, regardless of whether there is a matching record in
    the dimension table. This consideration is important because it acknowledges that
    not every fact record may necessarily have a corresponding entry in every dimension.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，当将事实表与维度表连接时，我们使用了`LEFT JOIN`。这是相当常见的。它确保包括结果中来自事实表的所有记录，无论维度表中是否存在匹配记录。这一考虑很重要，因为它承认并非每个事实记录必定在每个维度中都有对应的条目。
- en: By using a `LEFT JOIN`, you retain all the data from the fact table while enriching
    it with the relevant attributes from the dimension table. This allows you to perform
    analysis and aggregations based on various dimensions and explore the data from
    different perspectives. However, we must handle any missing correspondence. Thus
    we use the `COALESCE` operator, which is often used to set a default value like
    `-1` or `Not available`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`LEFT JOIN`，您可以保留事实表中的所有数据，同时使用维度表中的相关属性进行丰富。这样可以基于不同的维度进行分析和聚合，从不同的视角探索数据。然而，我们必须处理任何缺失的对应关系。因此，我们使用`COALESCE`运算符，通常用于设置默认值，如`-1`或`不可用`。
- en: A `LEFT JOIN` also allows for incremental dimension updates. If new records
    are added to the dimension table, the `LEFT JOIN` will still include the existing
    fact records, associating them with the available dimension data. This flexibility
    ensures that your analysis and reporting remain consistent even as your dimension
    data evolves over time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`LEFT JOIN`还允许增量维度更新。如果向维度表添加新记录，则`LEFT JOIN`仍将包括现有的事实记录，并将它们与可用的维度数据关联起来。这种灵活性确保您的分析和报告在维度数据随时间演变的同时保持一致。'
- en: Overall, the star schema’s simplicity and denormalized structure make it conducive
    to aggregations and summarizations. You can generate various reports, such as
    sales trends over time, best-selling genres, or revenue by author. Additionally,
    the star schema facilitates drill-down and roll-up operations, allowing you to
    drill into more detailed information or roll up to higher levels of aggregation
    for a comprehensive view of the data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，星型模式的简单性和去规范化结构使其非常适合进行聚合和汇总。您可以生成各种报告，例如随时间变化的销售趋势、畅销流派或按作者的收入。此外，星型模式还支持逐步深入和上卷操作，使您可以深入了解更详细的信息或上卷到更高层次的聚合，从而全面查看数据。
- en: This modeling technique also aligns seamlessly with integration into data visualization
    tools and BI platforms. By connecting your model to tools such as Tableau, Power
    BI, or Looker, you can craft visually captivating dashboards and interactive reports.
    These resources empower stakeholders to swiftly grasp insights and make data-driven
    decisions at a glance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种建模技术还能无缝对接数据可视化工具和BI平台。通过将您的模型连接到诸如Tableau、Power BI或Looker等工具，您可以创建视觉上吸引人的仪表板和交互式报告。这些资源使利益相关者能够迅速掌握见解，并在一瞥间做出数据驱动的决策。
- en: However, it’s worth noting that the preceding example doesn’t fully highlight
    the denormalization aspect championed by star schemas. For instance, if your dataset
    strictly adheres to a one-genre-per-book scenario, you have the opportunity to
    further streamline your model by consolidating the genre information directly
    within a unified `dimBooks` table, promoting denormalization and simplifying data
    access.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，上述示例并未充分突出星型模式所倡导的去规范化方面。例如，如果您的数据集严格遵循每本书一个流派的情况，您可以通过直接在统一的`dimBooks`表中
    consolida流派信息来进一步简化模型，促进去规范化并简化数据访问。
- en: Modeling with the Snowflake Schema
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用雪花模式建模
- en: In a *snowflake schema*, the data model is more normalized than in a star schema.
    It contains additional levels of normalization by splitting dimension tables into
    multiple contiguous tables. This allows for better data integrity and reduces
    data redundancy. For example, consider a snowflake schema for an ecommerce database.
    We have a dimension table `customers` that contains customer information such
    as ID, name, and address. In a snowflake schema, we could split this table into
    several contiguous tables.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在*雪花模式*中，数据模型比星型模式更归一化。它通过将维度表拆分为多个连续表来增加归一化级别。这有助于提高数据完整性并减少数据冗余。例如，考虑一个电子商务数据库的雪花模式。我们有一个维度表`customers`，包含客户信息，如ID、姓名和地址。在雪花模式中，我们可以将这个表拆分为多个连续的表。
- en: The `customers` table could be split into a `customers` table and a separate
    `addresses` table. The `customers` table would contain customer-specific attributes
    such as ID and the customer’s name. In contrast, the `addresses` table would contain
    address-related information such as ID and the customer’s street, city, and zip
    code. If several customers have the same address, we need to store the address
    information only once in the `addresses` table and link it to the respective customers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`customers`表可以拆分为`customers`表和单独的`addresses`表。`customers`表将包含特定于客户的属性，如ID和客户姓名。相比之下，`addresses`表将包含与地址相关的信息，如ID、客户的街道、城市和邮政编码。如果几个客户具有相同的地址，则我们只需在`addresses`表中存储一次地址信息，并将其链接到相应的客户即可。'
- en: To retrieve data from a snowflake schema, we usually need to perform multiple
    joins over the associated tables to get the desired information. For example,
    if we want to query the customer name and address, we must join the `customers`
    table with the `addresses` table on the ID page. While a snowflake schema provides
    better data integrity, it also requires more complex queries because of the additional
    links. However, this schema can be beneficial for large datasets and complex relationships
    because it provides better normalization and flexibility in data management.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要从雪花模式检索数据，通常需要在相关表上执行多次连接操作，以获取所需的信息。例如，如果我们想查询客户姓名和地址，我们必须将`customers`表与`addresses`表在ID页上进行连接。虽然雪花模式提供了更好的数据完整性，但由于额外的连接，它也需要更复杂的查询。然而，由于提供了更好的归一化和数据管理灵活性，因此这种模式对于大数据集和复杂关系可能是有益的。
- en: Both the star schema and snowflake schema are two common data warehouse schema
    designs. In a star schema, dimension tables are denormalized, meaning they contain
    redundant data. The star schema offers advantages such as more accessible design
    and implementation and more efficient querying due to fewer `JOIN` operations.
    However, it may require more storage space because of denormalized data and can
    be more challenging to update and troubleshoot.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 星型模式和雪花模式都是两种常见的数据仓库模式设计。在星型模式中，维度表是非归一化的，意味着它们包含冗余数据。星型模式提供了诸如更易于设计和实施、由于较少的`JOIN`操作而更高效的查询等优点。然而，由于冗余数据可能需要更多的存储空间，并且在更新和故障排除时可能更具挑战性。
- en: This is one of the reasons we often see hybrid models, in which companies model
    star schemas and often normalize a few dimensions for different optimization strategies.
    The choice depends heavily on your unique needs and requirements. A star schema
    could be the ideal choice if you prioritize simplicity and efficiency in a data
    warehouse solution. This schema offers easy implementation and efficient querying,
    making it suitable for straightforward data analysis tasks. However, a snowflake
    schema might be better if you anticipate frequent changes in your data requirements
    and require more flexibility since it allows easier adaptation to evolving data
    structures.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们经常看到混合模型的原因之一，公司在其中建模星型模式，并常常为不同的优化策略归一化少量维度。选择主要取决于您独特的需求和要求。如果您在数据仓库解决方案中优先考虑简单性和效率，星型模式可能是理想选择。这种模式提供了易于实施和高效的查询，适合简单的数据分析任务。然而，如果您预期数据需求经常变化，并且需要更多的灵活性，则雪花模式可能更好，因为它允许更容易地适应不断演变的数据结构。
- en: Imagine we have a dimension that represents the location of specific customers
    across the globe. One way to model it in a star schema would be to create a single
    dimension table with all the location hierarchies denormalized. [Example 2-10](#reference_snows_01)
    shows the `dimLocation` under a star schema paradigm.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们有一个表示全球特定客户位置的维度。在星型模式中建模它的一种方法是创建一个包含所有位置层次结构的单个维度表，使其反规范化。[示例 2-10](#reference_snows_01)
    展示了在星型模式范例下的 `dimLocation`。
- en: Example 2-10\. Star schema location dimension
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-10\. 星型模式位置维度
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Example 2-11](#reference_snows_02) models the location dimension following
    a snowflake schema.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 2-11](#reference_snows_02) 模型按照雪花模式处理位置维度。'
- en: Example 2-11\. Snowflake schema location dimension
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-11\. 雪花模式位置维度
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the snowflake schema example, the location dimension is split into four
    tables: `dimLocation`, `dimCity`, `dimState`, and `dimCountry`. The tables are
    connected using primary and foreign keys to establish relationships among them.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在雪花模式示例中，位置维度被分为四个表：`dimLocation`、`dimCity`、`dimState`和`dimCountry`。这些表使用主键和外键连接以建立它们之间的关系。
- en: One important topic is that although we have four tables to represent the location
    dimension, only the table with the highest hierarchy connects to the fact table
    (or fact tables) via its primary key. All the other hierarchy levels follow the
    lineage from highest to lowest granularity. [Figure 2-4](#snowflake) illustrates
    this case.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们有四个表来表示位置维度，但只有最高层次的表通过其主键与事实表（或事实表）连接是一个重要的主题。所有其他层次的层次结构都按照从最高到最低粒度的谱系进行遵循。[图 2-4](#snowflake)说明了这种情况。
- en: '![ch03_snowflake_02](assets/aesd_0204.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![ch03_snowflake_02](assets/aesd_0204.png)'
- en: Figure 2-4\. Snowflake schema model
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 雪花模式模型
- en: Modeling with Data Vault
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Data Vault进行建模
- en: '*Data Vault 2.0* is a modeling approach that does not fall under dimensional
    modeling but is still worth mentioning. Its approach combines 3NF elements and
    dimensional modeling to create a logical enterprise data warehouse. It is designed
    to handle various data types, including structured, semi-structured, and unstructured
    data, by providing flexible and scalable patterns. One of its most highlighted
    characteristics is that it focuses on building a modular and incremental Data
    Vault model that integrates raw data based on business keys. This approach ensures
    that the data warehouse can accommodate changing business requirements and evolving
    datasets.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*Data Vault 2.0* 是一种建模方法，不属于维度建模，但仍然值得一提。其方法结合了3NF元素和维度建模，以创建一个逻辑企业数据仓库。它旨在通过提供灵活和可扩展的模式来处理包括结构化、半结构化和非结构化数据在内的各种数据类型。其最突出的特点之一是专注于基于业务键构建模块化和增量的Data
    Vault模型。这种方法确保数据仓库能够适应不断变化的业务需求和不断发展的数据集。'
- en: 'Going deeper, this modeling technique provides a scalable and flexible data
    warehousing and analytics solution. It is designed to handle large data volumes,
    changing business requirements, and evolving data sources. Data Vault’s model
    consists of three main components: hubs, links, and satellites.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地说，这种建模技术提供了一个可扩展和灵活的数据仓库和分析解决方案。它旨在处理大数据量、不断变化的业务需求和不断发展的数据源。Data Vault的模型由三个主要组件组成：hub、link和satellite。
- en: '*Hubs* represent business entities and serve as a central point for storing
    unique identifiers called *business keys*. Each hub corresponds to a specific
    entity, such as customers, products, or locations. The hub table contains the
    business-key column along with any descriptive attributes related to the entity.
    By separating the business key from the descriptive attributes, Data Vault enables
    easy tracking of changes to the descriptive information without compromising the
    integrity of the business key.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*Hubs* 代表业务实体，并作为存储称为*业务键*的唯一标识符的中心点。每个Hub对应于特定的实体，如客户、产品或位置。Hub表包含业务键列以及与实体相关的任何描述性属性。通过将业务键与描述性属性分开，Data
    Vault能够轻松跟踪描述信息的变化，而不会损害业务键的完整性。'
- en: '*Links* capture the relationships among business entities. They are created
    to represent many-to-many relationships or complex associations. The link table
    contains foreign keys from the participating hubs that form a bridge between the
    linked entities. This approach allows for modeling complicated relationships without
    duplicating data or creating unnecessary complexity.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*链接*捕获业务实体之间的关系。它们被创建来表示多对多关系或复杂的关联。链接表包含参与的中心的外键，形成连接实体之间的桥梁。这种方法允许对复杂关系进行建模，而无需复制数据或创建不必要的复杂性。'
- en: '*Satellites* store the context-specific attributes related to the hubs and
    links. They contain additional descriptive information that is not part of the
    business key but provides valuable context about the entities. Satellites are
    associated with the corresponding hubs or links via foreign keys, which allows
    for the storage of time-varying data and the preservation of historical records.
    Multiple satellites can be associated with a hub or link, each capturing specific
    attributes for different points in time or different perspectives.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*卫星*存储与中心和链接相关的特定上下文属性。它们包含不属于业务键但提供有关实体的有价值上下文信息的附加描述信息。通过外键，卫星与相应的中心或链接相关联，允许存储时变数据并保留历史记录。可以将多个卫星与中心或链接关联，每个卫星捕获不同时间点或不同视角的特定属性。'
- en: Data Vault architecture promotes traceability, scalability, and auditability
    while providing a solid foundation for data integration, analytics, and data governance.
    Using hubs, links, and satellites, organizations can build a Data Vault that supports
    their analytical needs, adapt to changing business requirements, and maintain
    a reliable historical record of data changes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库架构推动可追溯性、可扩展性和审计性，同时为数据集成、分析和数据治理提供坚实的基础。通过中心（hubs）、链接（links）和卫星（satellites），组织可以构建支持其分析需求的数据仓库，适应不断变化的业务需求，并维护可靠的历史数据变更记录。
- en: Returning to our `books` table, let’s follow the three modeling steps to develop
    a simple Data Vault model. The first step is identifying the business keys and
    creating the corresponding hub and satellite tables. In this case, we have only
    one business entity, so links won’t be used. [Example 2-12](#reference_dv_0) shows
    the Data Vault modeling of the `books` table.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的`books`表，让我们按照三个建模步骤开发简单的数据仓库模型。第一步是识别业务键并创建相应的中心和卫星表。在这种情况下，我们只有一个业务实体，因此不会使用链接。[示例 2-12](#reference_dv_0)展示了`books`表的数据仓库建模。
- en: Example 2-12\. Modeling the `books` table with Data Vault 2.0
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-12\. 使用数据仓库2.0建模`books`表
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In Data Vault modeling, we begin to identify the business keys, which are unique
    identifiers for each entity. In this case, the primary key of the `books` table,
    `book_id`, serves as the business key.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据仓库建模中，我们首先要识别业务键，即每个实体的唯一标识符。在这种情况下，`books`表的主键`book_id`充当业务键。
- en: 'Now it’s time to model and create our first table: the hub table, which stores
    the unique business keys and their corresponding hash keys for stability. [Example 2-13](#reference_dv_1)
    creates the hub table.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候对我们的第一张表进行建模和创建了：中心表，它存储唯一的业务键及其对应的哈希键以确保稳定性。[示例 2-13](#reference_dv_1)创建了中心表。
- en: Example 2-13\. Hub creation
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-13\. 创建中心（hub）
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the hub table, we store the unique identifier for each book as the primary
    key (`bookKey`) and a hash key (`bookHashKey`) for stability. The `Title` column
    contains descriptive information about the book.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在中心表中，我们将每本书的唯一标识符存储为主键(`bookKey`)，并使用哈希键(`bookHashKey`)来保持稳定性。`Title`列包含关于书籍的描述信息。
- en: Next comes our satellite table, shown in [Example 2-14](#reference_dv_2), which
    captures additional book details and maintains the historical changes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的卫星表，显示在[示例 2-14](#reference_dv_2)，它捕获了额外的书籍详情并保持了历史更改。
- en: Example 2-14\. Satellite creation
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-14\. 创建卫星（satellite）
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By separating the core book information into the hub table and storing the historical
    details in the satellite table, we ensure that changes to attributes like the
    author, publication year, or genre can be captured over time without modifying
    existing records.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将核心图书信息分离到中心表中，并在卫星表中存储历史细节，我们确保可以随时间捕获诸如作者、出版年份或流派等属性的变化，而不会修改现有记录。
- en: In a Data Vault model, we may have additional tables, such as link tables to
    represent relationships among entities or other satellite tables to capture historical
    changes in specific attributes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据仓库模型中，我们可能会有额外的表，比如链接表来表示实体之间的关系，或者其他卫星表来捕获特定属性的历史变化。
- en: Monolith Data Modeling
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单片数据建模
- en: Until recently, the prevailing approach to data modeling revolved around the
    creation of extensive SQL scripts. In this conventional method, a single SQL file,
    often stretching over thousands of lines, encapsulated the entirety of the data
    modeling process. For a more sophisticated workflow, practitioners might have
    divided the file into multiple SQL scripts or stored procedures, which were then
    executed sequentially via Python scripts. To make the workflow more complex, these
    scripts typically remained largely unknown within the organization. Consequently,
    even if another individual wished to undertake data modeling in a similar fashion,
    they would invariably start from scratch, eschewing the opportunity to leverage
    preexisting work.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，数据建模的主流方法围绕创建大量的SQL脚本展开。在这种传统方法中，一个单独的SQL文件通常会延伸数千行，封装了整个数据建模过程。为了实现更复杂的工作流程，从业者们可能会将文件分割成多个SQL脚本或存储过程，然后通过Python脚本顺序执行。为了使工作流程更加复杂，这些脚本通常在组织内部鲜为人知。因此，即使另一个人希望以类似的方式进行数据建模，他们也会从头开始，错失利用现有工作的机会。
- en: This approach can aptly be described as a *monolithic* or traditional approach
    to data modeling, where each data consumer independently reconstructed their data
    transformations from the raw source data. Within this paradigm, several notable
    challenges persisted, including the absence of version control for scripts, the
    daunting task of managing dependencies between views, and the common practice
    of crafting new views or tables from raw data sources to the final reporting stage,
    damaging reusability. Moreover, the concept of idempotency was not uniformly applied
    to large tables, sometimes resulting in redundancy, and backfills—which, while
    common, often proved to be intricate and labor-intensive affairs.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以恰当地描述为数据建模的*单片*或传统方法，其中每个数据消费者都独立地从原始数据重构他们的数据转换。在这种范式内部，存在一些显著的挑战，包括脚本的版本控制缺失、管理视图之间依赖关系的艰巨任务，以及从原始数据源到最终报告阶段制作新视图或表的常见做法，损害了可重用性。此外，幂等性概念并不统一地应用于大表中，有时会导致冗余和回填操作，虽然这些操作很常见，但通常复杂且耗时。
- en: 'In today’s rapidly evolving world of data engineering, monolithic data models,
    particularly in the context of SQL transformations, pose a significant challenge
    that engineers grapple with. Consider the following scenario: you discover that
    something in your production system is broken, only to find that what initially
    appears to be a simple change has set off a chain reaction of errors that propagates
    throughout the entire infrastructure. This nightmarish scenario, characterized
    by highly interconnected systems and a minor alteration that acts as the catalyst
    for a cascading domino effect, is a hauntingly familiar problem for many data
    professionals.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今快速发展的数据工程世界中，特别是在SQL转换的背景下，单片数据模型构成了工程师们需要应对的重大挑战。考虑以下情景：你发现你的生产系统出了问题，最初看似简单的改变引发了一连串的错误，这些错误蔓延至整个基础设施。这个噩梦般的情景以高度互联的系统和微小的改动作为导火索，形成了连锁反应，对许多数据专业人士来说是一个令人不安但又熟悉的问题。
- en: The risks associated with monolithic data models are precisely what we seek
    to avoid when designing a data model. The last thing you want is tightly coupled
    data models that make debugging and implementing changes a daunting task, as each
    change can potentially disrupt the entire data pipeline. The lack of modularity
    hinders the flexibility, scalability, and maintainability that are critical in
    today’s data-driven landscape.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计数据模型时，我们要避免的正是与单片数据模型相关的风险。你最不希望的是紧密耦合的数据模型，这使得调试和实施变更变得艰巨，因为每一次变更都有可能干扰整个数据管道。缺乏模块化会妨碍在当今数据驱动的景观中至关重要的灵活性、可扩展性和可维护性。
- en: In a monolithic data model, all components are tightly interconnected, rendering
    the identification and isolation of problems a challenging endeavor. In essence,
    this traditional approach to designing data systems tends to unify the entire
    system into a single, although not always cohesive, unit.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在整体数据模型中，所有组件紧密相连，使得问题的识别和隔离成为一项具有挑战性的任务。本质上，这种传统的数据系统设计方法倾向于将整个系统统一为一个单一的单位，尽管并非始终内聚。
- en: This interconnectedness of the model means that seemingly unrelated changes
    can have unintended consequences that impact the entire system. This complexity
    not only makes troubleshooting more difficult but also increases the risk of introducing
    errors or overlooking critical dependencies. All data and functionality are so
    tightly integrated and interdependent that it becomes tough to modify or update
    any one part of the system without affecting the entire system.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的这种相互连接性意味着看似无关的变更可能会对整个系统产生意外后果。这种复杂性不仅使故障排除变得更加困难，还增加了引入错误或忽视关键依赖性的风险。所有数据和功能都如此紧密集成和相互依赖，以至于修改或更新系统的任何一个部分都变得非常困难。
- en: In addition, the lack of modularity in data models hinders the ability to adapt
    to changing business requirements. In a dynamic environment of data needs with
    sources constantly changing, a monolithic model becomes a bottleneck to progress.
    Incorporating new data sources, scaling infrastructure, or integrating new technologies
    and frameworks becomes increasingly challenging.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据模型缺乏模块化会阻碍适应不断变化的业务需求能力。在数据需求动态变化的环境中，整体模型成为进展的瓶颈。整合新数据源、扩展基础设施或集成新技术和框架变得日益具有挑战性。
- en: Also, maintenance and updates to a monolithic data model become time-consuming
    and resource-intensive undertakings. Each change carries more risk due to the
    complicated dependencies within the system. The fear of inadvertently breaking
    critical components leads to an overly cautious approach that slows development
    cycles and inhibits innovation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对整体数据模型的维护和更新变得耗时且资源密集。由于系统内复杂的依赖关系，每次变更都存在更高的风险。对于可能不经意地破坏关键组件的担忧导致过度谨慎的方法，从而减缓开发周期并抑制创新。
- en: The challenges posed by monolithic data models in today’s data engineering landscape
    are significant. The risks of interdependencies, lack of flexibility, and difficulties
    with maintenance and scalability necessitate a shift to modular data models. By
    adopting modularity, data engineers can achieve greater flexibility, robustness,
    and adaptability in their data infrastructure to manage the complexity of a rapidly
    evolving data ecosystem. By moving away from monolithic structures, organizations
    can realize the full potential of their data, drive innovation, and gain a competitive
    advantage in the data-driven world we live in.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今数据工程领域，整体数据模型带来的挑战是显著的。相互依赖性、缺乏灵活性以及在维护和扩展方面的困难，迫使我们转向模块化数据模型。通过采用模块化，数据工程师可以在其数据基础设施中实现更大的灵活性、鲁棒性和适应性，以应对快速演变的数据生态复杂性。通过摆脱整体结构，组织可以实现数据的全部潜力，在我们生活的数据驱动世界中推动创新，并获得竞争优势。
- en: dbt has been instrumental in taking a modular approach and overcoming the challenges
    of monolithic models. It allows us to improve maintainability, flexibility, and
    scalability by breaking the singular data model into individual modules, each
    with its own SQL code and dependencies. This modular structure allows us to work
    on individual modules independently, making it easier to develop, test, and debug
    specific parts of the data model. This eliminates the risk of unintended changes
    affecting the entire system, which makes introducing changes and updates safer.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: dbt 在采用模块化方法和克服整体模型挑战方面发挥了重要作用。通过将单一数据模型拆分为各自具有独立 SQL 代码和依赖项的模块，它使我们能够提高可维护性、灵活性和可扩展性。这种模块化结构使我们能够独立处理每个模块，从而更容易开发、测试和调试数据模型的特定部分。这消除了意外更改影响整个系统的风险，使引入变更和更新更加安全。
- en: This topic of modularity in the dbt will receive more attention in the coming
    subsections, and [Chapter 4](ch04.html#chapter_id_04) will dive into a comprehensive
    exploration of dbt.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: dbt 中模块化的主题将在接下来的子章节中得到更多关注，并且[第四章](ch04.html#chapter_id_04)将深入探讨 dbt 的全面探索。
- en: Building Modular Data Models
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模块化数据模型
- en: The previous example highlights how much dbt and data model modularization,
    in general, can contribute to a better data development process. However, why
    isn’t this a given for data engineers and scientists? The truth is that in the
    software development world, over the last few decades, engineers and architects
    have chosen new ways to employ modularization as a means to simplify their coding
    process. Instead of tackling one large piece of code at a time, modularization
    breaks the coding process into various steps. This method offers several advantages
    over alternative strategies.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子突显了dbt和数据模型模块化在改进数据开发过程中的贡献。然而，为何数据工程师和科学家没有将此视为必备技能？事实是，在过去几十年的软件开发世界中，工程师和架构师选择了新的方式来利用模块化简化编码过程。模块化将编码过程从处理一个大代码块变成多个步骤。这种方法相较于替代策略具有几个优势。
- en: One major advantage of modularization is its ability to enhance manageability.
    When developing a large software program, it can be challenging to stay focused
    on a single piece of coding. However, the job becomes more manageable by breaking
    it into individual tasks. This helps developers stay on track and prevents them
    from feeling overwhelmed by the project’s magnitude.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化的一个主要优势是增强了可管理性。在开发大型软件程序时，集中精力处理一个大块编码可能会具有挑战性。但通过将其分解为个别任务，工作变得更加可管理。这有助于开发者保持专注，并防止他们因项目的庞大而感到不知所措。
- en: Another advantage of modularization is its support for team programming. Instead
    of assigning a large job to a single programmer, it can be divided among a team.
    Each programmer is assigned specific tasks as part of the overall program. In
    the end, the work from all the programmers is combined to create the final program.
    This approach accelerates the development process and allows for specialization
    within the team.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化的另一个优点是支持团队编程。与将一个大任务交给单个程序员不同，可以将其分配给团队。每个程序员作为整体程序的一部分分配了特定的任务。最终，来自所有程序员的工作被结合起来创建最终程序。这种方法加速了开发过程，并允许团队内部的专业化。
- en: Modularization also contributes to improving code quality. Breaking the code
    into small parts and assigning responsibility to individual programmers enhances
    the quality of each section. When a programmer focuses on their assigned section
    without worrying about the entire program, they can ensure the flawlessness of
    their code. Consequently, the overall program is less likely to contain errors
    when all the parts are integrated.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化还有助于提升代码质量。将代码分解为小部分，并将责任分配给个别程序员，增强了每个部分的质量。当程序员专注于他们分配的部分，而不必担心整个程序时，他们可以确保代码的完美性。因此，当所有部分集成时，整体程序较少可能包含错误。
- en: Additionally, modularization enables the reuse of code modules that have already
    been proven to work effectively. By dividing the program into modules, the fundamental
    aspects are broken down. If a particular piece of code functions well for a specific
    task, there is no need to reinvent it. Instead, the same code can be reused, saving
    programmers time and effort. This can be repeated throughout the program whenever
    similar features are required, further streamlining development.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模块化还能够使已被证明有效的代码模块得以重复利用。通过将程序分解为模块，可以打破基本的方面。如果某段代码在特定任务中运行良好，就无需重新发明。相反，可以重复使用相同的代码，节省程序员的时间和精力。在需要类似特性时，可以在整个程序中重复此过程，进一步简化开发。
- en: Furthermore, modular code is highly organized, which enhances its readability.
    By organizing code based on tasks, programmers can easily find and reference specific
    sections based on their organization scheme. This improves collaboration among
    multiple developers, as they can follow the same organizational scheme and understand
    the code more efficiently.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模块化的代码高度组织，提升了其可读性。通过根据任务组织代码，程序员可以轻松找到并引用特定的部分，基于其组织方案。这提升了多个开发者之间的协作，因为他们可以遵循相同的组织方案，更高效地理解代码。
- en: All the advantages of modularization ultimately lead to improved reliability.
    Code that is easier to read, debug, maintain, and share operates with fewer errors.
    This becomes crucial when working on large projects with numerous developers who
    need to share code or interface with one another’s code in the future. Modularization
    enables the creation of complex software in a reliable manner.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模块化的优势最终都导致了改进的可靠性。代码更易阅读、调试、维护和共享，减少错误。在需要多名开发者共享代码或在未来与他人代码接口的大型项目中，这变得至关重要。模块化使得能够可靠地创建复杂软件。
- en: Although modularization is a must and a given in the software engineering world,
    in the data space, it has been left behind and picked up only in the last few
    years. The reason behind this is the need for more clarity between data architecture
    and software engineering. Yet, recently the industry has evolved into a blend
    of both worlds as the advantages mentioned before also apply to data analytics
    and engineering.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模块化在软件工程界是必须且已知的，但在数据领域，它在过去几年中才开始被重视。这背后的原因是需要在数据架构和软件工程之间更清晰的界定。然而，最近，行业已经演变成两者融合的形式，因为前述的优势也适用于数据分析和工程。
- en: Just as modularization simplifies the coding process, it can also streamline
    the design and development of data models. By breaking complex data structures
    into modular components, data engineers can better manage and manipulate data
    at various levels of granularity. This modular approach enables efficient data
    integration, scalability, and flexibility, allowing for easier updates, maintenance,
    and enhancements to the overall data architecture.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正如模块化简化了编码过程，它也可以简化数据模型的设计和开发。通过将复杂数据结构分解为模块化组件，数据工程师可以更好地管理和操作不同粒度的数据。这种模块化方法支持高效的数据集成、可扩展性和灵活性，使得对整体数据架构的更新、维护和增强更加容易。
- en: At the same time, modularization facilitates the reuse of data modules, ensuring
    consistency and accuracy across data models and reducing redundancy. Overall,
    modularization principles provide a solid foundation for effective data modeling
    and engineering, enhancing the organization, accessibility, and reliability of
    data systems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，模块化促进了数据模块的重用，确保数据模型的一致性和准确性，减少了冗余。总体而言，模块化原则为有效的数据建模和工程提供了坚实的基础，增强了数据系统的组织性、可访问性和可靠性。
- en: Thus, modular data modeling is a powerful technique for designing efficient
    and scalable data systems. Developers can build more robust and maintainable systems
    by breaking complex data structures into smaller reusable components. This is
    a powerful technique for designing efficient and scalable data systems, and both
    dbt and SQL provide efficient tools to help us implement this technique.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模块化数据建模是设计高效可扩展数据系统的强大技术。开发人员可以通过将复杂数据结构分解为可重用的小组件来构建更健壮和易维护的系统。这是设计高效可扩展数据系统的强大技术，而dbt和SQL都提供了有效的工具来帮助我们实施这一技术。
- en: 'In summary, the core principles of modular data modeling can be defined as
    follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：模块化数据建模的核心原则可以定义如下：
- en: Decomposition
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 分解
- en: Breaking the data model into smaller, more manageable components
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据模型分解为更小、更易管理的组件
- en: Abstraction
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象化
- en: Hiding the implementation details of the data model behind interfaces
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏数据模型实现细节的接口背后
- en: Reusability
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 可重用性
- en: Creating components that can be reused across multiple parts of the system
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 创建可以在系统多个部分重复使用的组件
- en: This kind of data modeling can be achieved using normalization, data warehousing,
    and data virtualization techniques. For example, using the normalization technique,
    the data is separated into tables based on its characteristics and relationships,
    leading to a modular data model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据建模可以通过标准化、数据仓库化和数据虚拟化技术来实现。例如，使用标准化技术，根据数据特征和关系将数据分离到表中，从而实现模块化数据模型。
- en: Another option is to leverage dbt because it helps to automate the process of
    creating a modular data model, providing several features that support the principles
    of modular data modeling. For example, dbt allows us to tackle decomposition by
    allowing us to split our data model into smaller reusable components, which provides
    a way to create reusable macros and modular model files. It also gives us a way
    to abstract the implementation details of the data model by providing a simple,
    consistent interface for working with data sources.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是利用dbt，因为它有助于自动化创建模块化数据模型的过程，提供支持模块化数据建模原则的多个功能。例如，dbt允许我们通过将数据模型拆分为更小的可重复使用组件来解决分解问题，从而提供了创建可重复使用的宏和模块化模型文件的方式。它还通过提供简单一致的数据源工作界面，抽象化数据模型的实现细节。
- en: Furthermore, dbt encourages reusability by providing a way to define and reuse
    common code across various models. Additionally, dbt helps improve maintainability
    by providing a way to test and document your data models. Finally, dbt allows
    you to optimize performance by defining and testing different materialization
    strategies for your models, which, in the end, allows you to fine-tune the performance
    of individual components of your data model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，dbt通过提供一种定义和重复使用各种模型中通用代码的方式，鼓励可重用性。此外，dbt通过提供一种测试和记录数据模型的方式，有助于提高可维护性。最后，dbt允许您通过定义和测试不同的模型实现策略来优化性能，从而最终允许您调整数据模型的各个组件的性能。
- en: However, it’s important to acknowledge that modularity also comes with potential
    drawbacks and risks. Integrated systems can often be better optimized than modular
    systems, whether it’s because of the minimization of data movement and memory
    usage or the ability for the database optimizer to improve SQL behind the scenes.
    Creating views to then create tables can sometimes result in suboptimal models.
    However, this trade-off is often worth it for the benefits of modularity. Modularity
    creates more files, which can mean more objects to own, govern, and potentially
    deprecate. Without a mature data governance strategy, this can lead to a proliferation
    of modular but unowned tables, which can be challenging to manage when issues
    arise.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须承认，模块化也带来潜在的缺点和风险。集成系统通常比模块化系统更易优化，无论是由于数据移动和内存使用的最小化，还是由于数据库优化器在幕后改进SQL的能力。创建视图然后创建表有时可能导致次优模型。然而，考虑到模块化的好处，这种权衡往往是值得的。模块化会创建更多文件，这可能意味着更多的对象需要拥有、治理和可能废弃。如果没有成熟的数据治理策略，这可能会导致模块化但未拥有的表的增多，在问题出现时管理起来可能会很具挑战性。
- en: Enabling Modular Data Models with dbt
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用dbt实现模块化数据模型
- en: As we previously highlighted, building modular data models is an essential aspect
    of developing a robust and maintainable data infrastructure. However, the process
    of managing and orchestrating these models can become complex as the project grows
    in size and complexity.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前强调的，构建模块化数据模型是开发健壮和可维护数据基础设施的重要方面。然而，随着项目规模和复杂性的增长，管理和协调这些模型的过程可能变得复杂起来。
- en: This is where a robust data transformation tool like dbt comes in. By combining
    the principles of modular data modeling with the features of dbt, we can easily
    unlock a whole new level of efficiency and scalability in our data infrastructure.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是强大的数据转换工具如dbt的用武之地。通过将模块化数据建模原则与dbt的功能结合，我们可以轻松地在我们的数据基础设施中实现全新的效率和可伸缩性水平。
- en: With the adoption of this modular approach, every data producer or consumer
    within an organization gains the ability to build upon the foundational data modeling
    work accomplished by others, eliminating the need to start from scratch with the
    source data on every occasion.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种模块化方法后，组织内的每个数据生产者或消费者都能够在其他人已完成的基础数据建模工作基础上进行扩展，无需在每次使用源数据时都从头开始。
- en: Upon integrating dbt into the data modeling framework, a shift in perspective
    occurs, transforming the concept of data models from a monolithic entity into
    a distinct component. Every individual contributor to a model starts identifying
    transformations that could be shared across various data models. These shared
    transformations are extracted and organized into foundational models, allowing
    for their efficient referencing in multiple contexts.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 将dbt集成到数据建模框架中后，会发生一种视角转变，将数据模型的概念从单片实体转变为独立组件。每个模型的单独贡献者开始识别可以在各种数据模型中共享的转换。这些共享的转换被提取并组织成基础模型，允许在多个上下文中高效地引用它们。
- en: As illustrated by [Figure 2-5](#dbtmodularity), using basic data models across
    multiple instances, rather than starting from scratch each time, simplifies the
    visualization of the DAG in data modeling. This modularized multilevel structure
    clarifies how the layers of data modeling logic build on one another and shows
    dependencies. However, it is essential to note that simply adopting a data modeling
    framework like dbt does not automatically ensure modular data models and an easy-to-understand
    DAG.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[图2-5](#dbtmodularity)所示，跨多个实例使用基本数据模型，而不是每次从头开始，简化了数据建模中DAG的可视化。这种模块化的多层结构阐明了数据建模逻辑层如何相互构建并显示依赖关系。但是，需要注意的是，仅仅采用像dbt这样的数据建模框架并不能自动确保模块化的数据模型和易于理解的DAG。
- en: '![images/ch03_modularity_dbt.png](assets/aesd_0205.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aesd_0205.png)'
- en: Figure 2-5\. dbt modularity
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. dbt模块化
- en: The structure of your DAG depends on your team’s data modeling ideas and thought
    processes, as well as the consistency with which they are expressed. To achieve
    modular data modeling, consider principles such as naming conventions, readability,
    and ease of debugging and optimization. These principles can be applied to various
    models in dbt, including staging models, intermediate models, and mart models,
    to improve modularity and maintain a well-structured DAG.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 您的DAG的结构取决于团队的数据建模理念和思维过程，以及表达这些理念的一致性。为了实现模块化数据建模，考虑诸如命名规范、可读性以及调试和优化的便利性等原则非常重要。这些原则可以应用于dbt中的各种模型，包括分段模型、中间模型和mart模型，以提高模块化并保持良好结构的DAG。
- en: 'Let’s start this journey toward leveraging dbt for modular data models by understanding
    how dbt enables model reusability via the Jinja syntax through the usage of referencing
    data models: `{{ ref() }}`.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解dbt如何通过使用引用数据模型的Jinja语法实现模型可重用性开始，从而开始利用dbt构建模块化数据模型的旅程：`{{ ref() }}`。
- en: Referencing data models
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引用数据模型
- en: By adopting dbt’s features, such as model referencing and Jinja syntax, data
    engineers and analysts can establish clear dependencies among models, enhance
    code reusability, and ensure the consistency and accuracy of their data pipelines.
    *Jinja*, in this context, is a templating language that allows dynamic and programmatic
    transformations within SQL code, offering a powerful tool for customizing and
    automating data transformations. This powerful combination of modularity and dbt’s
    capabilities empowers teams to build flexible and maintainable data models, accelerating
    the development process and enabling seamless collaboration among stakeholders.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用dbt的功能，如模型引用和Jinja语法，数据工程师和分析师可以建立模型之间清晰的依赖关系，增强代码的可重用性，并确保数据管道的一致性和准确性。在这种情况下，*Jinja*是一种模板语言，允许在SQL代码中进行动态和程序化的转换，为定制和自动化数据转换提供了强大的工具。这种模块化和dbt能力的强大结合使团队能够构建灵活和可维护的数据模型，加快开发过程，并促进利益相关者之间的无缝协作。
- en: To leverage the full capabilities of dbt and ensure accurate model building,
    it is crucial to employ model referencing using the `{{ ref() }}` syntax. By referencing
    models this way, dbt can automatically detect and establish dependencies among
    models based on upstream tables. This enables a smooth and reliable execution
    of the data transformation pipeline.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用dbt的全部功能并确保准确的模型构建，关键在于使用`{{ ref() }}`语法进行模型引用。通过这种方式引用模型，dbt可以自动检测并建立基于上游表的模型依赖关系。这样一来，数据转换管道的执行变得顺畅可靠。
- en: On the other hand, the `{{ source() }}` Jinja syntax should be used sparingly,
    typically limited to the initial selection of raw data from the database. It is
    important to avoid direct references to non-dbt-created tables because they can
    hinder the flexibility and modularity of the dbt workflow. Instead, the focus
    should be on establishing relationships among models by using the `{{ ref() }}`
    Jinja syntax, ensuring that changes in upstream tables are correctly propagated
    downstream, and maintaining a clear and coherent data transformation process.
    By sticking to these best practices, dbt enables efficient model management and
    promotes scalability and maintainability in the analytics workflow.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，应该节俭地使用`{{ source() }}` Jinja语法，通常仅限于从数据库中选择原始数据的初始阶段。避免直接引用非dbt创建的表是很重要的，因为这可能会影响dbt工作流的灵活性和模块化。相反，应重点建立模型之间的关系，通过使用`{{
    ref() }}` Jinja语法确保上游表的更改正确传播到下游，并保持清晰和连贯的数据转换过程。通过遵循这些最佳实践，dbt能够实现有效的模型管理，并在分析工作流中促进可伸缩性和可维护性。
- en: 'For example, suppose we have two models: orders and customers, where the `orders`
    table contains information about customer orders and the `customers` table stores
    customer details. We want to perform a join between these two tables to enrich
    the orders data with customer information ([Example 2-15](#reference_dbt_sql_statement)).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有两个模型：orders和customers，其中`orders`表包含有关客户订单的信息，而`customers`表存储客户详细信息。我们希望在这两个表之间执行联接，以用客户信息丰富订单数据（[示例 2-15](#reference_dbt_sql_statement)）。
- en: Example 2-15\. Referencing model
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-15\. 引用模型
- en: '[PRE14]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This example demonstrates referencing models in a SQL query by using the `ref()`
    function. The scenario involves two model files: *orders.sql* and *customers.sql*.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了在SQL查询中通过使用`ref()`函数引用模型。该场景涉及两个模型文件：*orders.sql*和*customers.sql*。
- en: In the *orders.sql* file, a `SELECT` statement is written to retrieve order
    information from the `orders` model. The `{{ ref('orders') }}` expression references
    the `orders` model, allowing the query to use the data defined in that model.
    The query joins the `orders` model with the `customers` model by using a `customer_id`
    column, retrieving additional customer information such as name and email.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在*orders.sql*文件中，编写了一个`SELECT`语句，用于从`orders`模型中检索订单信息。表达式`{{ ref('orders') }}`引用了`orders`模型，允许查询使用在该模型中定义的数据。查询通过使用`customer_id`列将`orders`模型与`customers`模型进行了连接，检索附加的客户信息，如姓名和电子邮件。
- en: In the *customers.sql* file, a `SELECT` statement is written to extract customer
    information from the `raw_customers` table. This model represents the raw customer
    data before any transformations.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在*customers.sql*文件中，编写了一个`SELECT`语句，用于从`raw_customers`表中提取客户信息。这个模型代表了在任何转换之前的原始客户数据。
- en: 'This referencing mechanism in dbt enables the creation of modular and interconnected
    models that build upon one another to generate meaningful insights and reports.
    To illustrate the need for it, let’s consider a practical example: imagine you’re
    dealing with a complex dataset, such as weekly product orders. Without a structured
    approach, managing this data can quickly become chaotic. You might end up with
    a tangled web of SQL queries, making it challenging to track dependencies, maintain
    code, and ensure data accuracy.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: dbt中的这种引用机制使得创建模块化和互连的模型成为可能，这些模型相互构建，以生成有意义的见解和报告。为了说明其必要性，让我们考虑一个实际的例子：想象一下，您正在处理一个复杂的数据集，例如每周的产品订单。没有结构化的方法，管理这些数据很快就会变得混乱。您可能最终得到一堆SQL查询的混乱网，这样就很难跟踪依赖关系、维护代码并确保数据的准确性。
- en: By organizing your data transformation process into distinct layers, from source
    to mart tables, you gain several benefits. This simplifies the data pipeline,
    making it more understandable and manageable. It also allows for incremental improvements,
    as each layer focuses on a specific transformation task. This structured approach
    enhances collaboration among data engineers and analysts, reduces errors, and
    ultimately leads to more reliable and insightful reports.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据转换过程组织成从源到数据仓库表的不同层，您将获得几个好处。这简化了数据管道，使其更易理解和管理。它还允许增量改进，因为每个层次都专注于特定的转换任务。这种结构化方法增强了数据工程师和分析师之间的协作，减少了错误，并最终产生了更可靠和见解深刻的报告。
- en: Staging data models
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分期数据模型
- en: The staging layer plays a crucial role in data modeling, as it serves as the
    basis for the modular construction of more complex data models. Each *staging
    model* corresponds to a source table with a 1:1 relationship to the original data
    source. Keeping staging models simple and minimizing transformations within this
    layer is important. Acceptable transformations include type conversion, column
    renaming, basic calculations (such as unit conversion), and categorization using
    conditional statements such as `CASE WHEN`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 分阶段层在数据建模中扮演着关键角色，因为它作为构建更复杂数据模型的模块化基础。每个*分阶段模型*对应于源表，与原始数据源之间存在一对一的关系。保持分阶段模型简单并在该层内尽量减少转换是很重要的。可接受的转换包括类型转换、列重命名、基本计算（如单位转换）以及使用条件语句（如`CASE
    WHEN`）进行分类。
- en: Staging models usually materialize as views to preserve data timeliness and
    optimize storage costs. This approach allows intermediate or mart models that
    reference the staging layer to access up-to-date data while saving space and cost.
    It is advisable to avoid joins in the staging layer to prevent redundant or duplicate
    computations. Join operations are better suited for subsequent layers where more
    complex relationships are established.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 分阶段模型通常被实现为视图，以保持数据的时效性并优化存储成本。这种方法允许中间或市场模型引用分阶段层以访问最新的数据，同时节省空间和成本。建议避免在分阶段层进行连接，以防止冗余或重复的计算。连接操作更适合在后续层次建立更复杂的关系时使用。
- en: Also, aggregations in the staging layer should be avoided because they can group
    and potentially limit access to valuable source data. The primary purpose of the
    staging layer is to create the basic building blocks for subsequent data models,
    providing flexibility and scalability in downstream transformations. Following
    these guidelines, the staging layer becomes a reliable and efficient starting
    point for building robust data models in a modular data architecture.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，应避免在分阶段层进行聚合，因为它们可能会分组并潜在地限制对宝贵源数据的访问。分阶段层的主要目的是为后续数据模型创建基本构建块，在模块化数据架构中提供灵活性和可伸缩性。遵循这些准则，分阶段层成为构建健壮数据模型的可靠和高效起点。
- en: Utilizing staging models in dbt allows us to adopt the Don’t Repeat Yourself
    (DRY) principle in our code. By following dbt’s modular and reusable structure,
    we aim to push any transformations that are consistently required for a specific
    component model as far upstream as possible. This approach helps us avoid duplicating
    code, which reduces complexity and computational overhead.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在dbt中利用分阶段模型允许我们在代码中采用“不重复你自己”（DRY）原则。通过遵循dbt的模块化和可重用的结构，我们旨在尽可能将对特定组件模型一致需要的任何转换推到最上游。这种方法帮助我们避免重复代码，从而降低复杂性和计算开销。
- en: For example, suppose we consistently need to convert monetary values from integers
    in cents to floats in dollars. In that case, performing the division and type
    casting early in the staging model is more efficient. This way, we can reference
    the transformed values downstream without repeating the same transformation multiple
    times. By leveraging staging models, we optimize code reuse and streamline the
    data transformation process in a scalable and efficient manner.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们始终需要将整数形式的美分转换为浮点形式的美元。在这种情况下，在分阶段模型中早期执行除法和类型转换更为高效。这样一来，我们可以在下游引用转换后的值，而无需多次重复相同的转换过程。通过利用分阶段模型，我们优化了代码复用，并以可扩展和高效的方式简化了数据转换过程。
- en: Let’s say we have a source table called `raw_books` that contains the raw books
    data. We now want to create a staging model called `stg_books` to transform and
    prepare the data before further processing. In our dbt project, we can create
    a new dbt model file named *stg_books.sql* and define the logic to generate the
    staging model, as shown in [Example 2-16](#reference_stg_model).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个名为`raw_books`的源表，其中包含原始书籍数据。现在我们想要创建一个名为`stg_books`的分阶段模型，以在进一步处理之前对数据进行转换和准备。在我们的dbt项目中，我们可以创建一个名为*stg_books.sql*的新dbt模型文件，并定义生成分阶段模型的逻辑，如[示例 2-16](#reference_stg_model)所示。
- en: Example 2-16\. Staging model
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-16\. 分阶段模型
- en: '[PRE15]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A staging model, like `stg_books` in this example, selects relevant columns
    from the `raw_books` table. It can include basic transformations such as renaming
    columns or converting data types. By creating a staging model, you separate the
    initial data transformation from the downstream processing. This ensures data
    quality, consistency, and compliance with standards before further use. Staging
    models serve as the foundation for more complex data models in your data pipeline’s
    intermediate and mart layers. They streamline transformations, maintain data integrity,
    and improve the reusability and modularity of your dbt project.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，像`stg_books`这样的分阶段模型从`raw_books`表中选择相关列。它可以包括基本的转换，如重命名列或转换数据类型。通过创建分阶段模型，您可以将初始数据转换与下游处理分开。这确保了数据质量、一致性，并符合标准，以便进一步使用。分阶段模型作为数据管道中间和市场层更复杂数据模型的基础。它们简化转换过程，维护数据完整性，并提高了您的
    dbt 项目的可重用性和模块化。
- en: Base data models
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础数据模型
- en: In dbt, the *base models* often serve as staging models, but they can also encompass
    additional transformation steps depending on your project’s specific needs. These
    models are typically designed to directly reference the raw data entered into
    your data warehouse, and they play a crucial role in the data transformation process.
    Once you have created your staging or base models, other models in your dbt project
    can reference them.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在 dbt 中，*基础模型*通常作为分阶段模型，但根据项目的特定需求，它们也可以包含额外的转换步骤。这些模型通常设计为直接引用输入到数据仓库中的原始数据，它们在数据转换过程中扮演着至关重要的角色。一旦您创建了分阶段或基础模型，您的
    dbt 项目中的其他模型可以引用它们。
- en: The change from “base” to “staging” models in the dbt documentation reflects
    a desire not to be restrained by the name “base,” which implies the first step
    in building a data model. The new terminology allows more flexibility in describing
    the role and purpose of these models within the dbt framework.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: dbt 文档中从“基础”模型更改为“分阶段”模型的变化反映了不受“基础”名称约束的愿望，该名称暗示建立数据模型的第一步。新的术语允许更灵活地描述这些模型在
    dbt 框架中的角色和目的。
- en: Intermediate data models
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中间数据模型
- en: The intermediate layer plays a crucial role in data modeling by combining the
    atomic building blocks from the staging layer to create more complex and meaningful
    models. These *intermediate models* represent constructs that hold significance
    for the business but are typically not directly exposed to end users through dashboards
    or applications.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层通过将分阶段层的原子构建模块组合起来，创建更复杂和有意义的模型，在数据建模中扮演着至关重要的角色。这些*中间模型*代表着对业务有意义的构造，但通常不会通过仪表板或应用程序直接向最终用户展示。
- en: To maintain separation and optimize performance, it is advisable to store intermediate
    models as ephemeral models. *Ephemeral models* are not created directly on the
    database or dataset, but rather their code is interpolated into the models that
    reference them as common table expressions (CTEs). However, sometimes materializing
    them as views is more suitable. Ephemeral models cannot be selected directly,
    which makes troubleshooting challenging. Additionally, macros invoked through
    `run-operation` cannot reference ephemeral models. Therefore, whether to materialize
    a particular intermediate model as ephemeral or as a view depends on the specific
    use case, but starting with ephemeral materialization is recommended.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持分离和优化性能，建议将中间模型存储为临时模型。*临时模型*不会直接在数据库或数据集上创建，而是它们的代码会插入到引用它们的模型中作为公共表达式（CTE）。然而，有时将它们材料化为视图更为合适。临时模型不能直接选择，这使得故障排除变得具有挑战性。此外，通过`run-operation`调用的宏不能引用临时模型。因此，将特定的中间模型作为临时模型还是视图材料化取决于具体的用例，但建议从临时材料化开始。
- en: If you choose to materialize intermediate models as views, it may be beneficial
    to place them in a custom schema outside the main schema defined in your dbt profile.
    This helps in organizing the models and managing permissions effectively.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择将中间模型材料化为视图，则将它们放置在 dbt 配置文件中定义的主模式之外的自定义模式可能会有所帮助。这有助于组织模型并有效地管理权限。
- en: The primary purpose of the intermediate layer is to bring together different
    entities and absorb complexity from the final mart models. These models facilitate
    readability and flexibility in the overall data model structure. It is important
    to consider the frequency of referencing an intermediate model in other models.
    Multiple models referencing the same intermediate model may indicate a design
    issue. In such cases, transforming the intermediate model into a macro could be
    a suitable solution to enhance modularity and maintain a cleaner design.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层的主要目的是汇集不同实体并吸收最终马特模型的复杂性。这些模型提升了整体数据模型结构的可读性和灵活性。重要的是要考虑在其他模型中引用中间模型的频率。多个模型引用同一中间模型可能表明存在设计问题。在这种情况下，将中间模型转换为宏可能是增强模块化和保持更清晰设计的合适解决方案。
- en: By effectively leveraging the intermediate layer, data models can be made more
    modular and manageable, ensuring the absorption of complexity while maintaining
    the readability and flexibility of the components.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过有效利用中间层，数据模型可以变得更加模块化和可管理，确保在吸收复杂性的同时保持组件的可读性和灵活性。
- en: Let’s say we have two staging models, `stg_books` and `stg_authors`, representing
    the book and author data, respectively. Now we want to create an intermediate
    model called `int_book_authors` that combines the relevant information from both
    staging models. In our dbt project, we can create a new dbt model file named *int_book_authors.sql*,
    as shown in [Example 2-17](#reference_int_model), and define the logic to generate
    the intermediate model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个分期模型，`stg_books`和`stg_authors`，分别代表书籍和作者数据。现在我们想要创建一个名为`int_book_authors`的中间模型，将来自两个分期模型的相关信息组合在一起。在我们的dbt项目中，可以创建一个名为*int_book_authors.sql*的新dbt模型文件，如[示例 2-17](#reference_int_model)所示，并定义生成中间模型的逻辑。
- en: Example 2-17\. Intermediate model
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-17\. 中间模型
- en: '[PRE16]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In [Example 2-17](#reference_int_model), the `int_book_authors` model references
    the staging models, `stg_books` and `stg_authors`, using the `{{ ref() }}` Jinja
    syntax. This ensures that dbt can infer the model dependencies correctly and build
    the intermediate model based on the upstream tables.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 2-17](#reference_int_model)中，`int_book_authors`模型使用`{{ ref() }}` Jinja
    语法引用了分期模型`stg_books`和`stg_authors`，这确保了dbt能够正确推断模型依赖关系，并基于上游表构建中间模型。
- en: Mart models
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 马特模型
- en: The top layer of the data pipeline consists of *mart models*, which are responsible
    for integrating and presenting business-defined entities to end users via dashboards
    or applications. These models combine all relevant data from multiple sources
    and transform it into a cohesive view.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的顶层由*马特模型*组成，负责通过仪表板或应用程序将业务定义的实体集成和呈现给最终用户。这些模型将来自多个来源的所有相关数据结合在一起，形成一个统一的视图。
- en: To ensure optimal performance, mart models are typically materialized as tables.
    Materializing the models enables faster execution of queries and better responsiveness
    in delivering results to end users. If the creation time or cost of materializing
    a table is an issue, configuration as an incremental model can be considered,
    allowing for efficient updates as new data is included.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保最佳性能，马特模型通常被物化为表格。物化模型能够加快查询执行速度，并在向最终用户提供结果时具备更好的响应性。如果物化表格的创建时间或成本成为问题，可以考虑配置为增量模型，允许随着包含新数据的加入而进行高效更新。
- en: Simplicity is key to mart models, and excessive joins should be avoided. If
    you need multiple joins in a mart model, rethink the design and consider restructuring
    the intermediate layer. By keeping mart models relatively simple, you can ensure
    efficient query execution and maintain the overall performance of your data pipeline.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性是马特模型的关键，应避免过多的连接。如果在马特模型中需要多个连接，请重新思考设计，并考虑重新构造中间层。通过保持马特模型相对简单，可以确保查询执行的高效性，并维护数据管道的整体性能。
- en: Let’s consider the example of a data mart for book publication analysis. We
    have an intermediate model called `int_book_authors` that contains the raw books
    data, including information about the authors of each book ([Example 2-18](#reference_mart_model)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来考虑一个数据马特（data mart）的例子，用于书籍出版分析。我们有一个名为`int_book_authors`的中间模型，包含原始书籍数据，包括每本书的作者信息（[示例 2-18](#reference_mart_model)）。
- en: Example 2-18\. Mart model
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-18\. 马特模型
- en: '[PRE17]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We start by setting the configuration for the model, specifying that it should
    be materialized as a table. The unique key is set to `author_id` to ensure uniqueness,
    and the sorting is done based on `author_id` as well.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先设置模型的配置，指定其应作为表物化。唯一键设置为`author_id`以确保唯一性，并且排序也基于`author_id`进行。
- en: Next, we use a CTE called `book_counts` to aggregate the book data. We select
    the `author_id` column and count the number of books associated with each author
    from the `stg_books` staging model. Finally, the `SELECT` statement retrieves
    the aggregated data from the `book_counts` CTE, returning the `author_id` and
    the corresponding count of books for each author. Given that it’s a materialized
    table, this model can be refreshed whenever needed to reflect any changes in the
    original data.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用名为`book_counts`的CTE来汇总图书数据。我们选择`author_id`列，并计算与每位作者相关联的图书数量，这些数据来自`stg_books`暂存模型。最后，`SELECT`语句从`book_counts`
    CTE检索汇总数据，返回每位作者的`author_id`及其对应的图书计数。由于这是一个物化表，可以根据需要随时刷新该模型，以反映原始数据的任何更改。
- en: Testing Your Data Models
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试您的数据模型
- en: Testing in dbt is a vital aspect of ensuring the accuracy and reliability of
    your data models and data sources. dbt provides a comprehensive testing framework
    that allows you to define and execute tests by using SQL queries. These tests
    are designed to identify rows or records that do not meet the specified assertion
    criteria rather than check specific conditions’ correctness.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在dbt中进行测试是确保数据模型和数据源准确性和可靠性的重要方面。dbt提供了一个全面的测试框架，允许您使用SQL查询定义和执行测试。这些测试旨在识别不符合指定断言条件的行或记录，而不是检查特定条件的正确性。
- en: 'dbt has two main types of tests: singular and generic. *Singular tests* are
    specific and targeted tests written as SQL statements and stored in separate SQL
    files. They allow you to test specific aspects of your data, such as checking
    for the absence of `NULL` values in a fact table or validating certain data transformations.
    With singular tests, we can leverage the power of Jinja to dynamically define
    assertions based on our data and business requirements. Let’s look at a singular
    test in dbt by analyzing [Example 2-19](#dbt_test_example).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: dbt有两种主要类型的测试：单独测试和通用测试。*单独测试*是具体而有针对性的测试，以SQL语句形式编写，并存储在单独的SQL文件中。它们允许您测试数据的特定方面，例如检查事实表中的`NULL`值的缺失或验证特定的数据转换。通过单独测试，我们可以利用Jinja的强大功能根据数据和业务需求动态定义断言。让我们通过分析[Example 2-19](#dbt_test_example)来看一个dbt中的单独测试。
- en: Example 2-19\. Singular test example in dbt
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 2-19\. dbt中的单独测试示例
- en: '[PRE18]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example, we define a single test called `not_null_columns` for the dbt
    model `my_model`. This test checks whether specific columns in the model contain
    `NULL` values. The `columns` parameter specifies the columns to check for `NULL`
    values. In this case, `column1` and `column2` are specified. If any of these columns
    contain `NULL` values, the test fails.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们为dbt模型`my_model`定义了一个名为`not_null_columns`的单独测试。此测试检查模型中特定列是否包含`NULL`值。`columns`参数指定要检查`NULL`值的列。在这种情况下，指定了`column1`和`column2`。如果任何这些列包含`NULL`值，则测试失败。
- en: '*Generic tests*, on the other hand, are more versatile and can be applied to
    multiple models or data sources. They are defined in dbt project files by using
    a special syntax. These tests allow us to define more comprehensive criteria for
    validating our data, such as checking data consistency among tables or ensuring
    the integrity of specific columns. Also, they provide a flexible and reusable
    way to define assertions that can be applied across dbt models. These tests are
    written and stored in YAML (*.yml*) files, which allows us to parameterize the
    queries and easily reuse them in various contexts. The parameterization of queries
    in generic tests enables you to adapt the tests to multiple scenarios quickly.
    For example, you can specify different column names or condition parameters when
    applying the generic test to different models or datasets.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*通用测试*则更加灵活，可以应用于多个模型或数据源。它们在dbt项目文件中通过特殊语法定义。这些测试允许我们定义更全面的条件来验证我们的数据，例如检查表之间的数据一致性或确保特定列的完整性。此外，它们提供了一种灵活且可重用的方式来定义断言，可以应用于dbt模型中。这些测试以YAML（*.yml*）文件形式编写和存储，允许我们对查询进行参数化，并在各种情境中轻松重用。通用测试中的查询参数化使您能够快速地将测试适应多种情况。例如，您可以在将通用测试应用于不同模型或数据集时指定不同的列名或条件参数。'
- en: Let’s look at one of these generic tests in [Example 2-20](#dbt_generic_test_example).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看[Example 2-20](#dbt_generic_test_example)中的其中一个通用测试。
- en: Example 2-20\. Generic test example in dbt
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 2-20\. dbt 中的通用测试示例
- en: '[PRE19]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this example, the generic test is defined with the name `non_negative_values`.
    Here, we can observe the columns to be tested and the assertion criteria for each
    column. The test checks whether the values in the `amount` and `quantity` columns
    are nonnegative. Generic tests allow you to write reusable test logic that can
    be applied to multiple models in your dbt project.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，通用测试被定义为`non_negative_values`。在这里，我们可以观察到要测试的列以及每列的断言标准。该测试检查`amount`和`quantity`列的值是否为非负数。通用测试允许您编写可重复使用的测试逻辑，可以应用于dbt项目中的多个模型。
- en: To reuse the generic test in multiple models, we can reference it in the tests
    section of each individual model’s YAML file, as presented in [Example 2-21](#dbt_reuse_test_example).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 要在多个模型中重用通用测试，我们可以在每个单独模型的YAML文件的测试部分引用它，如[Example 2-21](#dbt_reuse_test_example)所示。
- en: Example 2-21\. Reusing a generic test
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 2-21\. 重用通用测试
- en: '[PRE20]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this example, the model `my_model` is defined, and the `amount` and `quantity`
    columns are specified with the corresponding tests. The tests refer to the generic
    test `non_negative_values` from the namespace `my_project` (assuming `my_project`
    is the name of your dbt project).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，定义了模型`my_model`，并指定了`amount`和`quantity`列以及相应的测试。这些测试引用了来自命名空间`my_project`的通用测试`non_negative_values`（假设`my_project`是您的dbt项目名称）。
- en: By specifying the generic test in the `tests` section of each model, you can
    reuse the same test logic in multiple models. This approach ensures consistency
    of data validation and allows you to easily apply the generic test to specific
    columns in different models without duplicating the test logic.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在每个模型的`tests`部分指定通用测试，您可以在多个模型中重复使用相同的测试逻辑。此方法确保了数据验证的一致性，并允许您轻松地将通用测试应用于不同模型中的特定列，而无需复制测试逻辑。
- en: Note that you must ensure that the YAML file for the generic test is in the
    correct directory within your dbt project structure, and you may need to modify
    the test reference to match your project’s namespace and folder structure.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您必须确保通用测试的YAML文件位于dbt项目结构内正确的目录中，并且可能需要修改测试引用以匹配项目的命名空间和文件夹结构。
- en: Generating Data Documentation
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成数据文档
- en: Another integral component of proper data modeling is *documentation*. Specifically,
    ensuring that everyone in your organization, including business users, can easily
    understand and access metrics such as ARR (annual recurring revenue), NPS (net
    promoter score), or even MAU (monthly active users) is crucial for enabling data-driven
    decision making.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 正确数据建模的另一个关键组成部分是*文档*。具体而言，确保组织中的每个人，包括业务用户，都能轻松理解和访问诸如ARR（年度重复收入）、NPS（净推荐值）或者MAU（月活跃用户）等指标，对于推动数据驱动决策至关重要。
- en: By leveraging dbt’s features, we can document how metrics like these are defined
    and the specific source data they rely on. This documentation becomes a valuable
    resource anyone can access, fostering transparency and enabling self-service data
    exploration.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用dbt的功能，我们可以记录这些指标如何定义以及它们所依赖的具体源数据。这些文档成为任何人都可以访问的宝贵资源，促进透明度并实现自助数据探索。
- en: As we remove these semantics barriers and provide accessible documentation,
    dbt enables users at all levels of technical expertise to navigate and explore
    datasets, ensuring that valuable insights are available to a broader audience.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们消除这些语义障碍并提供易于访问的文档时，dbt使得所有技术水平的用户都能够浏览和探索数据集，确保宝贵的洞见对更广泛的受众可用。
- en: Let’s assume we have a dbt project with a model called *nps_metrics.sql*, which
    calculates the net promoter score. We can easily document this metric by using
    comments within the SQL file, enhanced with Markdown syntax, as shown in [Example 2-22](#dbt_doc_example).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个名为*nps_metrics.sql*的dbt项目模型，用于计算净推荐值。我们可以在SQL文件中使用Markdown语法通过注释轻松记录此指标，如[Example 2-22](#dbt_doc_example)所示。
- en: Example 2-22\. Documentation
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 2-22\. 文档
- en: '[PRE21]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this example, the comments provide essential details about the NPS metric.
    They specify the dependencies of the `nps_metrics` model, explain the calculation
    process, and mention the relevant tables involved in the query.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，注释提供了关于NPS指标的重要细节。它们指定了`nps_metrics`模型的依赖关系，解释了计算过程，并提到了查询涉及的相关表。
- en: After documenting the model, we can generate the documentation for our dbt project
    by using the dbt command-line interface (CLI) to run the following command ([Example 2-23](#dbt_doc_example_2)).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 完成模型文档后，我们可以通过使用 dbt 命令行界面（CLI）运行以下命令来为我们的 dbt 项目生成文档（[示例 2-23](#dbt_doc_example_2)）。
- en: Example 2-23\. Running documentation generation
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-23\. 运行文档生成
- en: '[PRE22]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Running the command generates HTML documentation for your entire dbt project,
    including the documented NPS metric. The generated documentation can be hosted
    and made accessible to users in your organization, enabling them to find and understand
    the NPS metric easily.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 运行该命令将为您的整个 dbt 项目生成 HTML 文档，包括已记录的 NPS 指标。生成的文档可以托管，并向您的组织用户提供访问权限，使他们可以轻松找到和理解
    NPS 指标。
- en: Debugging and Optimizing Data Models
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试和优化数据模型
- en: A valuable optimization suggestion for improving the performance of dbt is to
    analyze and optimize the queries themselves carefully. One approach is to leverage
    the capabilities of the query planner, such as the PostgreSQL (Postgres) query
    planner. Understanding the query planner will help you identify potential bottlenecks
    and inefficiencies in query execution.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 改进 dbt 性能的宝贵建议是仔细分析和优化查询本身。其中一种方法是利用查询规划器的功能，例如 PostgreSQL（Postgres）查询规划器。理解查询规划器将帮助您识别查询执行中的潜在瓶颈和低效性。
- en: Another effective optimization technique is deconstructing complex queries by
    breaking them into smaller components, such as CTEs. Depending on the complexity
    and nature of the operations involved, these CTEs can then be transformed into
    either views or tables. Simple queries involving light computations can be materialized
    as views, whereas more complex and computationally intensive queries can be materialized
    as tables. The dbt config block can be used to specify the desired materialization
    approach for each query.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有效的优化技术是通过将复杂查询分解为更小的组件（如CTE）来解构它们。根据操作的复杂性和性质，这些CTE可以转换为视图或表。简单的涉及轻量计算的查询可以作为视图物化，而复杂且计算密集型的查询可以作为表物化。dbt
    配置块可以用于指定每个查询所需的物化方法。
- en: Significant performance improvements can be achieved by selectively using an
    appropriate materialization technique. This can result in faster query execution
    times, reduce processing delays, and improve overall data modeling efficiency.
    In particular, the use of table materialization has shown impressive performance
    gains that can dramatically improve the speed, depending on the scenario.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择适当的物化技术可以实现显著的性能改进。这可以导致更快的查询执行时间，减少处理延迟，并提高整体数据建模效率。特别是，使用表物化已显示出令人印象深刻的性能提升，可以根据场景显著提高速度。
- en: Implementing these optimization recommendations will enable a leaner and more
    efficient dbt workflow. By optimizing queries and using appropriate materialization
    strategies, you can optimize the performance of your dbt models, resulting in
    better data processing and more efficient data transformations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些优化建议将使 dbt 工作流更精简和高效。通过优化查询并使用适当的物化策略，您可以优化 dbt 模型的性能，从而实现更好的数据处理和更高效的数据转换。
- en: Let’s look at the complex query in [Example 2-24](#dbt_complex_query).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 [示例 2-24](#dbt_complex_query) 中的复杂查询。
- en: Example 2-24\. Complex query 1
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-24\. 复杂查询 1
- en: '[PRE23]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This query involves joining tables, applying filters, and performing aggregations.
    Let’s deconstruct it into multiple CTEs before creating our final model ([Example 2-25](#dbt_complex_query_1)).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 该查询涉及表连接、应用过滤器和执行聚合。在创建最终模型之前，让我们将其解构为多个公共表达式（CTE）（[示例 2-25](#dbt_complex_query_1)）。
- en: Example 2-25\. Deconstructing complex query 1
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-25\. 解构复杂查询 1
- en: '[PRE24]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `join_query` CTE focuses on joining the required tables, while the `filter_query`
    CTE applies the filter condition to narrow down the rows. The `aggregate_query`
    CTE then performs the aggregation and applies the final filter condition.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`join_query` CTE 专注于连接所需的表，而 `filter_query` CTE 应用过滤条件以缩小行数。然后，`aggregate_query`
    CTE 执行聚合并应用最终的筛选条件。'
- en: By splitting the complex query into individual CTEs, you can simplify and organize
    the logic to optimize execution. This approach allows for better readability,
    maintainability, and potential performance improvements because the database engine
    can optimize the execution plan for each CTE. The final query retrieves the optimized
    results by selecting columns from the `aggregate_query` CTE.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将复杂查询拆分为单独的CTE，可以简化和组织逻辑以优化执行。这种方法允许更好的可读性、可维护性和潜在的性能改进，因为数据库引擎可以针对每个CTE优化执行计划。最终查询通过从`aggregate_query`
    CTE选择列来检索优化的结果。
- en: Let’s now explore the process of debugging materialized models in dbt. This
    can be a difficult task at first, as it requires thorough validation. One important
    aspect is ensuring that the data model appears as expected and the values match
    the non-materialized version.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索在dbt中调试物化模型的过程。起初这可能是一个困难的任务，因为它需要彻底的验证。一个重要的方面是确保数据模型看起来如预期，并且值与非物化版本匹配。
- en: To facilitate debugging and validation, it may be necessary to fully refresh
    the entire table and treat it as if it were not incremental. This can be accomplished
    with the `dbt run --full-refresh` command, which updates the table and runs the
    model as if it were being executed for the first time.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于调试和验证，可能需要完全刷新整个表，并将其视为非增量。这可以通过`dbt run --full-refresh`命令来完成，该命令会更新表并像第一次执行一样运行模型。
- en: In some cases, it may be helpful to perform a full update of the model and the
    incremental model in parallel during the first few days. This comparative approach
    allows validation of consistency between the two versions and minimizes the risk
    of future data discrepancies. This technique is particularly effective when working
    with a well-established, reliable data model in production, as it builds confidence
    in the changes that have been made. By comparing the updated and incremental models,
    we can ensure the accuracy of the changes and mitigate potential data-related
    issues.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，首几天同时对模型和增量模型进行全面更新可能会有所帮助。这种比较的方法允许验证两个版本之间的一致性，并减少未来数据差异的风险。当与生产中经过时间验证且可靠的数据模型一起工作时，这种技术尤为有效，因为它增强了对已做更改的信心。通过比较更新和增量模型，我们可以确保更改的准确性，并减轻潜在的与数据相关的问题。
- en: Consider an example scenario with a materialized dbt model that calculates monthly
    revenue based on transactional data. We want to debug and validate this model
    to ensure its accuracy. We start with the suspicion that the values generated
    by the materialized model may not match the expected results. To troubleshoot,
    we decide to fully refresh the table as if it were not incremental. Using the
    `dbt full-refresh` command, we trigger the process that updates the entire table
    and runs the model from scratch.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个示例场景，使用基于交易数据计算每月收入的物化dbt模型。我们希望调试和验证此模型以确保其准确性。我们开始怀疑物化模型生成的值可能与预期结果不符合。为了排除故障，我们决定完全刷新表格，就像它不是增量一样。使用`dbt
    full-refresh`命令，我们触发了更新整个表并从头运行模型的流程。
- en: In the first few days, we also run a parallel process to update the materialized
    and incremental models. This allows us to compare the results between the two
    versions and ensure they match. By checking the consistency of the updated model
    and the incremental model, we gain confidence in the accuracy of the changes made.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的几天，我们还会运行一个并行过程来更新物化和增量模型。这使我们能够比较两个版本之间的结果，并确保它们匹配。通过检查更新模型和增量模型的一致性，我们增强了对所做更改准确性的信心。
- en: For example, if we have a well-established revenue model that has been in production
    for a while and is considered reliable, comparing the updated and incremental
    models is even more meaningful. In this way, we can confirm that the changes made
    to the model have not caused any unintended discrepancies in the calculated revenue
    figures. Additionally, comprehensive testing is essential to ensure the accuracy
    and reliability of your data models. Implementing tests throughout your workflow
    can help identify issues early and provide valuable insights into the performance
    of your SQL queries.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个经过时间验证并被认为是可靠的运行已久的收入模型，那么比较更新和增量模型的结果就更有意义。通过这种方式，我们可以确认对模型的更改未导致计算的收入数字出现意外差异。此外，全面的测试对确保数据模型的准确性和可靠性至关重要。在整个工作流程中实施测试可以帮助及早发现问题，并为SQL查询的性能提供宝贵的见解。
- en: Note
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All these dbt functionalities, from building dbt models to testing and documentation,
    will be discussed and reinforced in Chapters [4](ch04.html#chapter_id_04) and
    [5](ch05.html#chapter_id_05).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些dbt功能，从构建dbt模型到测试和文档编制，将在第 [4](ch04.html#chapter_id_04) 章和第 [5](ch05.html#chapter_id_05)
    章中讨论和强化。
- en: Medallion Architecture Pattern
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中勋体架构模式
- en: Data warehouses have a rich history in decision support and BI, but they have
    limitations when it comes to handling unstructured, semi-structured, and high-variety
    data. At the same time, data lakes emerged as repositories for storing diverse
    data formats, but they lack critical features such as transaction support, data
    quality enforcement, and consistency.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库在决策支持和商业智能方面有着丰富的历史，但在处理非结构化、半结构化和高多样化数据时存在限制。与此同时，数据湖作为存储多样数据格式的仓库出现，但缺乏关键特性，如事务支持、数据质量强制执行和一致性。
- en: This has inhibited their ability to deliver on their promises and led to the
    loss of benefits associated with data warehouses. To meet the evolving needs of
    companies, a flexible and high-performance system is required to support diverse
    data applications like SQL analytics, real-time monitoring, data science, and
    machine learning. However, some recent advancements in AI focus on processing
    a wide range of data types, including semi-structured and unstructured data, which
    traditional data warehouses are not optimized for.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这阻碍了它们实现承诺的能力，并导致丧失与数据仓库相关的好处。为了满足企业不断发展的需求，需要一个灵活且高性能的系统，支持SQL分析、实时监控、数据科学和机器学习等多样的数据应用。然而，一些最新的人工智能进展侧重于处理广泛的数据类型，包括半结构化和非结构化数据，而传统的数据仓库并未为此进行优化。
- en: 'Consequently, organizations often use multiple systems, including data lakes,
    data warehouses, and specialized databases, which introduces complexity and delays
    due to data movement and copying between systems. As a natural consequence of
    the need to combine all these traditional systems into something that answers
    all the new market requirements, a new type of system is emerging: the data lakehouse.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，组织通常使用多个系统，包括数据湖、数据仓库和专用数据库，这增加了数据在系统之间移动和复制导致的复杂性和延迟。作为将所有这些传统系统整合成能够满足新市场需求的系统的自然结果，出现了一种新类型的系统：数据湖仓。
- en: The *data lakehouse* combines the strengths of both data lakes and data warehouses,
    implementing data structures and management features similar to warehouses directly
    on cost-effective cloud storage in open formats such as Apache Delta Lake, Iceberg,
    or Apache Hudi. These formats offer various advantages over traditional file formats
    like CSV and JSON. While CSV lacks typing for columns, JSON provides more flexible
    structures but inconsistent typing. Parquet, Apache Avro, and the ORC (optimized
    row columnar) file format improve on these by being column-oriented and more strongly
    typed but not ACID (atomicity, consistency, isolation, durability) compliant (except
    for ORC, in some cases). In contrast, Delta Lake, Iceberg, and Hudi enhance data
    storage by adding ACID compliance and the ability to serve as two-way data stores,
    enabling high throughput of modifications while supporting high volumes of analytical
    queries. These formats are particularly well suited for modern cloud-based data
    systems, unlike traditional formats like Parquet, which were initially designed
    for on-premises Hadoop-based systems.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据湖仓* 结合了数据湖和数据仓库的优势，通过在开放格式（如Apache Delta Lake、Iceberg或Apache Hudi）的成本效益云存储上实现类似仓库的数据结构和管理特性。这些格式相对于CSV和JSON等传统文件格式具有各种优势。虽然CSV缺乏列类型化，JSON提供更灵活的结构但类型不一致。Parquet、Apache
    Avro和ORC（优化的行列式文件格式）通过面向列和更强类型化来改进这些问题，但不符合ACID（原子性、一致性、隔离性、持久性）要求（在某些情况下除外，ORC符合ACID）。相反，Delta
    Lake、Iceberg和Hudi通过增加ACID合规性和作为双向数据存储的能力来增强数据存储，支持修改的高吞吐量同时支持大量分析查询。与最初为本地Hadoop系统设计的传统格式Parquet不同，这些格式特别适合现代基于云的数据系统。'
- en: Lakehouses offer key features such as transaction support for concurrent data
    reading and writing, schema enforcement and governance, direct BI tool support,
    decoupling of storage and computing for scalability, openness with standardized
    storage formats and APIs for efficient data access, support for diverse data types,
    and compatibility with various workloads including data science, machine learning,
    and SQL analytics. They also often provide end-to-end streaming capabilities,
    eliminating the need for separate systems for real-time data applications. Enterprise-grade
    lakehouse systems include features like security, access control, data governance,
    data discovery tools, and compliance with privacy regulations. Implementing a
    lakehouse enables organizations to consolidate these essential features into a
    single system shared by data engineers, analytics engineers, scientists, analysts,
    and even machine learning engineers, who can then collaborate to develop new data
    products.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 湖屋提供关键功能，如并发数据读写的事务支持、模式执行和治理、直接 BI 工具支持、为了可伸缩性而解耦存储和计算、开放式的标准化存储格式和 API，以便高效地访问数据、支持多种数据类型，并兼容包括数据科学、机器学习和
    SQL 分析在内的各种工作负载。它们还经常提供端到端流处理能力，消除了实时数据应用需要单独系统的需求。企业级湖屋系统包括安全性、访问控制、数据治理、数据发现工具，并符合隐私规定的合规性。实施湖屋使组织能够将这些基本功能整合到一个由数据工程师、分析工程师、科学家、分析师甚至机器学习工程师共享的单一系统中，从而协作开发新的数据产品。
- en: 'It is in the context of lakehouses and the new open formats that the *medallion
    architecture* emerges. In simple terms, this is a data modeling paradigm employed
    to strategically structure data within a lakehouse environment, aiming to iteratively
    enhance data quality as data transits different levels of iteration. This architectural
    framework often comprises three discernible tiers, the bronze, silver, and gold
    layers, each symbolizing ascending degrees of data refinement:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在湖屋和新开放格式的背景下，*勋章架构*应运而生。简而言之，这是一种数据建模范式，用于在湖屋环境中战略性地组织数据，旨在通过不同层次的迭代逐步提升数据质量。这种架构框架通常包括三个可辨识的层次，即青铜层、银层和金层，每一层代表数据精炼程度的逐级提升：
- en: Bronze layer
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 青铜层
- en: This serves as the initial destination for data from external source systems.
    The tables in this layer mirror the structures of the source system tables as
    they are, including any extra metadata columns to capture information like load
    date/time and process ID. This layer prioritizes efficient change data capture
    (CDC), maintaining a historical archive of the source data, ensuring data lineage,
    facilitating audits, and enabling reprocessing without rereading data from the
    source system.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这作为来自外部源系统数据的初始目标。此层中的表反映了源系统表的结构，包括任何额外的元数据列，以捕获加载日期/时间和处理 ID 等信息。此层优先考虑高效的变更数据捕获（CDC），保持源数据的历史存档，确保数据血统，便于审计，并支持重新处理，无需重新读取源系统的数据。
- en: Silver layer
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 银层
- en: Within the lakehouse architecture, this layer plays an essential function in
    consolidating and refining data sourced from the bronze layer. The silver layer
    creates a holistic view that encompasses key business entities, concepts, and
    transactions through processes such as matching, merging, conforming, and cleansing.
    This includes master customers, stores, nonduplicated transactions, and cross-reference
    tables. The silver layer serves as a comprehensive source for self-service analytics,
    empowering users with ad hoc reporting, advanced analytics, and machine learning
    capabilities. It is often observed that the silver layer can take the form of
    a 3NF data model, a star schema, a Data Vault, or even a snowflake. Similar to
    a traditional data warehouse, this is a valuable resource for anyone who leverages
    data to undertake projects and analyses aimed at solving business problems.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在湖屋架构中，这一层在整合和精炼从青铜层获取的数据方面发挥着重要作用。银层通过匹配、合并、符合和清洗等过程创建了一个全面的视图，涵盖了关键业务实体、概念和交易。这包括主客户、商店、非重复交易和交叉引用表。银层作为自助分析的全面数据源，赋予用户自由报告、高级分析和机器学习能力。通常观察到银层可以采用3NF数据模型、星型模式、数据仓库或者雪花模型。与传统数据仓库类似，这是任何利用数据解决业务问题的项目和分析的宝贵资源。
- en: Gold layer
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 金层
- en: This layer delivers valuable insights that address business questions. It aggregates
    data from the silver layer and serves it to BI ad hoc reporting tools and machine
    learning applications. This layer ensures reliability, improved performance, and
    ACID transactions for data lakes, while also unifying streaming and batch transactions
    on top of cloud data stores.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此层提供了解决业务问题的宝贵见解。它从银层聚合数据，并将其提供给BI临时报告工具和机器学习应用程序。该层确保了数据湖的可靠性、提高了性能，并为云数据存储提供了ACID事务，同时还统一了流处理和批处理事务。
- en: '[Figure 2-6](#MedallionArch) represents the medallion architecture in the context
    of a lakehouse and shows where dbt can support the creation of such systems.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-6](#MedallionArch) 描述了湖屋中奖牌架构的情境，并展示了dbt如何支持这种系统的创建。'
- en: '![ch04_medalion](assets/aesd_0206.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![ch04_medalion](assets/aesd_0206.png)'
- en: Figure 2-6\. Representation of medallion architecture and how it relates to
    dbt
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 奖牌架构的表现形式及其与dbt的关系
- en: Through the progression from the bronze to gold layers, data undergoes several
    steps, such as ingestion, cleansing, enhancement, and aggregation processes, delivering
    incalculable business insights. This approach represents a significant advancement
    over conventional data architectures, such as a data warehouse with a staging
    and dimensional model layer or even a sole data lake, often involving more file
    organization rather than creating proper semantic layers.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 从青铜层到黄金层的进展中，数据经历了摄取、清洗、增强和聚合等多个步骤，提供了无数的业务洞见。这种方法相对于传统的数据架构（如具有分段和维度模型层的数据仓库，甚至仅是一个数据湖，通常涉及更多的文件组织而不是创建适当的语义层），表示了一个重要的进步。
- en: Note
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释
- en: The medallion architecture doesn’t replace other dimensional modeling techniques.
    The structure of schemas and tables in each layer can vary based on the frequency
    and type of data updates, as well as the intended uses of the data. Instead, it
    guides how the data should be organized across three layers to enable a more modular
    data modeling approach.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 奖牌架构并不取代其他维度建模技术。每个层中模式和表的结构可以根据数据更新的频率和类型以及数据的预期使用而有所不同。相反，它指导数据应如何在三个层次之间组织，以实现更模块化的数据建模方法。
- en: It’s valuable for analytics engineers to understand the foundations of the medallion
    architecture and the concepts behind the lakehouse, because in some scenarios,
    this is where they may spend a significant amount of their time. This involvement
    could include modeling structures to be deployed in one of the layers of the medallion,
    leveraging an interface provided by open formats, or building transformation scripts
    (using tools like dbt, for example) to enable data progression across the layers
    of the architecture.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分析工程师来说，理解奖牌架构的基础和湖屋背后的概念是非常有价值的，因为在某些情况下，他们可能会花费大量时间在此处。这种参与可以包括在奖牌的某一层部署建模结构，利用开放格式提供的接口，或构建转换脚本（例如使用dbt等工具）以促进数据在架构各层之间的进展。
- en: However, it’s important to note that the significance of open formats and lakehouses
    can vary depending on the specific data architecture in use. For example, in architectures
    like Snowflake, data may primarily be ingested into native tables rather than
    open formats like Iceberg, making the understanding of lakehouses more of a nice-to-have
    rather than an essential requirement for analytics engineering.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，需要注意的是，开放格式和湖屋在特定数据架构使用情况下的重要性可能有所不同。例如，在像Snowflake这样的架构中，数据可能主要被摄取到本地表中，而不是像Iceberg这样的开放格式，这使得理解湖屋更多地成为一种不可或缺的要求，而不是分析工程的基本需求。
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Data modeling has evolved significantly in the field of analysis to suit diverse
    business insights and reporting requirements. The star schema provides a simple
    query method by having a central fact table surrounded by dimension tables. The
    snowflake schema allows deeper granularity by breaking down these dimensions further.
    In contrast, the Data Vault approach prioritizes flexibility to deal with environments
    where data sources change rapidly. The new medallion design combines all these
    models, forming a complete plan for various analytical needs.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析领域，数据建模已经在适应多样化的业务洞见和报告需求方面有了显著发展。星型模式通过将中心事实表环绕以维度表提供了简单的查询方法。雪花模式通过进一步分解这些维度提供了更深入的细粒度。相比之下，数据仓库方法优先考虑灵活性，以处理数据源快速变化的环境。新的奖牌设计结合了所有这些模型，形成了适应各种分析需求的完整计划。
- en: All the modeling advancements have been designed to tackle particular analytical
    issues. The central goal is to efficiently provide insights that can be acted
    upon, whether in the performance improvements of star and snowflake schemas or
    the versatility of the Data Vault. As the requirements for analytics become more
    complex, it is crucial to choose the correct modeling approach to not only make
    data available but also ensure it is meaningful and provides insights.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 所有建模进展都旨在解决特定的分析问题。其核心目标是高效提供可操作的洞察，无论是在星型和雪花模式的性能改进，还是数据仓库的多功能性。随着分析需求变得更加复杂，选择正确的建模方法不仅能使数据可用，还能确保数据具有意义并提供洞察。
- en: Analytics engineers use modeling structures such as star, snowflake, Data Vault,
    or medallion to create and maintain robust, scalable, and efficient data structures.
    Their work ensures the optimal organization of data, making it easily accessible
    and helpful to data analysts and scientists. Analytics engineers lay the foundation
    for accurate insights and informed decision making by creating coherent datasets
    from massive data streams through understanding and applying these models.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工程师使用建模结构，如星型、雪花、数据仓库或勋章，创建和维护稳健、可扩展和高效的数据结构。他们的工作确保数据的最佳组织，使其易于访问，并对数据分析师和科学家有所帮助。分析工程师通过理解并应用这些模型，从海量数据流中创建连贯的数据集，为准确的洞察和明智的决策奠定基础。
