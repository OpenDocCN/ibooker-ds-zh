- en: Chapter 15\. Time Series for Government
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章。政府时间序列
- en: 'Time series analysis is quite relevant and important for governmental applications
    for a number of reasons. First, governments both large and small, are the keepers
    of some of the most important time series data in the world, including the US
    jobs report, ocean temperature data (that is, global warming data), and local
    crime statistics. Second, governments by definition provide some of the most essential
    services we all rely on, and thus they need to be reasonably adept forecasters
    of demand if they don’t want to grossly overspend on, or understaff, those services.
    Thus, all aspects of time series are relevant to government purposes: storage,
    cleaning, exploration, and forecasting.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分析对于政府应用非常重要且相关，原因有很多。首先，无论大小，政府都是全球一些最重要时间序列数据的保管者，包括美国的就业报告、海洋温度数据（即全球变暖数据）和本地犯罪统计数据。其次，按定义，政府提供我们所有人依赖的一些最基本服务，因此，如果他们不希望在这些服务上大幅超支或者人手不足，他们就需要相当熟练地预测需求。因此，时间序列的所有方面对于政府目的都是相关的：存储、清理、探索和预测。
- en: 'As I mentioned back in [Chapter 2](ch02.html#finding_and_wrangling_time_series_data)
    when discussing “found” time series, a very high percentage of all government
    data can look a lot like time series data with some restructuring. Generally,
    most government data sets are the result of ongoing data collection rather than
    a single slice of time. However, government data sets can be daunting for a number
    of reasons:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在[第二章](ch02.html#finding_and_wrangling_time_series_data)中提到的，“找到”的时间序列数据在所有政府数据中所占比例非常高，只需进行一些重组即可看到。通常情况下，大多数政府数据集都是持续进行数据收集的结果，而不是时间的单一切片。然而，由于多种原因，政府数据集可能令人望而却步：
- en: Inconsistent recordkeeping (due to organizational constraints or political forces
    changing over time)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于组织约束或随着时间变化的政治力量而不一致的记录保持
- en: Opaque or confusing data practices
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不透明或令人困惑的数据实践
- en: Enormous data sets with relatively low information content
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型数据集，信息含量相对较低
- en: 'Nonetheless, it can be quite interesting to look at government data sets both
    for intellectual interest and for many practical purposes. In this chapter we
    explore a governmental data set that consists of all the [complaints made in New
    York City from 2010 to the present](https://perma.cc/BXF6-BZ4X) to a city-run
    hotline that can be reached by dialing 311\. Because the data set is continuously
    updated, the data seen in the book will likely differ from what you see when you
    download it; you will have even more information than I did when preparing this
    chapter. Nonetheless, the results should be fairly similar. In this chapter we
    will go through a few topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，审视政府数据集仍然可能非常有趣，无论是出于知识兴趣还是出于许多实际目的。在本章中，我们探讨了一个政府数据集，其中包括从2010年至今所有在[纽约市投诉](https://perma.cc/BXF6-BZ4X)的情况，该数据集是通过拨打311市政热线获取的。由于数据集持续更新，书中看到的数据可能与您下载时看到的数据不同；在我准备本章时，您将比我拥有更多信息。尽管如此，结果应该是相当相似的。在本章中，我们将讨论以下几个主题：
- en: Interesting sources of government data, including the one we will analyze
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政府数据的有趣来源，包括我们将要分析的那个
- en: Dealing with extremely large files of plain-text data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理极大的纯文本数据文件
- en: Online/rolling statistical analysis of large data sets and other options for
    analyzing data without keeping it all in memory
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不将所有数据保留在内存中的情况下，对大数据集进行在线/滚动统计分析以及其他分析选项
- en: Obtaining Governmental Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取政府数据
- en: Governmental data sets in the “found data” category can be a nightmare from
    a data consistency standpoint. These data sets, although they have a timestamp,
    are usually released for an open data initiative rather than for a specific time
    series purpose. Often there is little to no information available about the timestamping
    conventions of the data or other recording conventions. It can be difficult to
    confirm that the underlying recording practices were consistent.^([1](ch15.html#idm45576012501832))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在“找到数据”类别中的政府数据集在数据一致性的角度来看可能是一场噩梦。这些数据集虽然具有时间戳，但通常是为了开放数据倡议而发布，而不是为了特定的时间序列目的。通常几乎没有关于数据时间戳惯例或其他记录惯例的信息可用。确认底层记录实践是否一致可能会很困难。^([1](ch15.html#idm45576012501832))
- en: 'Nonetheless, if you are adventurous or keen to be the first to identify interesting
    temporal features in human behaviors related to governmental activities, you are
    lucky to live in the age of open government and open data. Many governments at
    all levels have made more efforts in recent years to make their time series data
    transparent to the public. Here are just a few examples of where you can obtain
    open governmental data with a time series component:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，如果您冒险或渴望成为首批发现与政府活动相关的人类行为中有趣时间特征的人，您会发现自己生活在开放政府和开放数据的时代。近年来，许多各级政府已经更加努力地使其时间序列数据对公众透明化。以下是一些您可以获取包含时间序列组件的开放政府数据的例子：
- en: '[Monthly hospital data](https://perma.cc/4TR3-84WA) from the United Kingdom’s
    National Health Services. This data set is surprisingly time series–aware: it
    includes having a tab named “MAR timeseries” and describes recording conventions
    and how they have evolved over time.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英国国家医疗服务的[月度医院数据](https://perma.cc/4TR3-84WA)。该数据集令人惊讶地具有时间序列意识：其中包括名为“MAR时间序列”的选项卡，并描述了记录约定及其随时间演变的方式。
- en: Jamaica’s open data portal also includes an appreciation and recognition of
    time series data, such as its timestamped [data set of Chikungunya cases](https://perma.cc/4RCP-VMY6)
    for 2014 and the [associated data report](https://perma.cc/QPR6-WNMJ), which includes
    an animation (i.e., a time series visualization) and an epidemic curve.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牙买加的开放数据门户还包括对时间序列数据的认识和认可，例如其时间戳的[2014年Chikungunya病例数据集](https://perma.cc/4RCP-VMY6)以及[相关数据报告](https://perma.cc/QPR6-WNMJ)，其中包括动画（即时间序列可视化）和流行曲线。
- en: Singapore’s [open data portal](https://perma.cc/N9W4-ZDM8) features extensive
    data sets and advertises the time series nature of some of that data by including
    two time series plots on its main page, as shown in [Figure 15-1](#fig-1501).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新加坡的[开放数据门户](https://perma.cc/N9W4-ZDM8)展示了大量数据集，并通过其主页上的两个时间序列图表来宣传某些数据的时间序列特性，如[图 15-1](#fig-1501)所示。
- en: '![](assets/ptsa_1501.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1501.png)'
- en: Figure 15-1\. Two of the four graphs on the main page of Singapore’s open data
    website (as accessed in spring 2019) are time series visualizations used to show
    important information about the country.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-1\. 新加坡开放数据网站主页上四幅图表中的两幅是用来展示有关该国重要信息的时间序列可视化。
- en: As you might notice, all my examples are from English-speaking areas, but of
    course they do not have a monopoly on the open data movement in government. For
    example, [the city of Paris](https://perma.cc/7V8Z-JZ4T), [the nation of Serbia](https://perma.cc/U3SQ-WF3C),
    and the [African Development Bank Group](https://perma.cc/7L6X-5B9F) all run open
    data websites.^([2](ch15.html#idm45576012488200))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，所有我的例子都来自讲英语的地区，但当然他们并不垄断政府开放数据运动。例如，[巴黎市](https://perma.cc/7V8Z-JZ4T)，[塞尔维亚国家](https://perma.cc/U3SQ-WF3C)，以及[African
    Development Bank Group](https://perma.cc/7L6X-5B9F)都在运行开放数据网站。^([2](ch15.html#idm45576012488200))
- en: In this chapter’s examples we are pulling our information from the New York
    City Open Data portal, selected because NYC is a large and interesting place that
    happens to be my home. We dive into their 311 hotline data set in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的例子中，我们从纽约市开放数据门户中获取信息，选择这里因为纽约市是一个大而有趣的地方，也是我的家。在下一节中，我们将深入研究他们的311热线数据集。
- en: Exploring Big Time Series Data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索大时间序列数据
- en: When data is sufficiently large, you will be unable to fit it in memory. How
    large data needs to be before you reach this limit will depend on the hardware
    you are using.^([3](ch15.html#idm45576012481048)) You will eventually need to
    understand how to iterate through your data, one manageable chunk at a time. For
    those familiar with deep learning, you have likely already done this, particularly
    if you work with image processing. In deep learning frameworks there are often
    Python iterators that wend their way through a data set with that data set stored
    in specified directories, each with many files.^([4](ch15.html#idm45576012479544))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量足够大时，您将无法将其全部装入内存。在达到此限制之前，数据需要多大取决于您使用的硬件。^([3](ch15.html#idm45576012481048))
    您最终需要理解如何逐个可管理的块迭代您的数据。对于那些熟悉深度学习的人来说，特别是涉及图像处理的人，您可能已经这样做过。在深度学习框架中，通常有Python迭代器，可以通过指定目录遍历数据集，每个目录中有许多文件。^([4](ch15.html#idm45576012479544))
- en: When I downloaded the 311 data set, it was over 3 gigabytes in CSV format. There
    was no way I would be able to open this on my computer, so my first idea was to
    use standard Unix operating system options, such as `head`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我下载了 311 数据集时，它以 CSV 格式超过了 3 GB。 我的计算机无法打开它，所以我的第一个想法是使用标准的 Unix 操作系统选项，比如
    `head`。
- en: 'Unfortunately, what printed out was already so large as to be unmanageable
    in a Unix command-line interface, at least for someone inexpert in Unix tools:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，打印出来的内容已经太大，无法在 Unix 命令行界面上管理，至少对于不熟悉 Unix 工具的人来说是这样：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: While the content is unwieldy, this view was sufficient to show that there were
    several timestamps as well as other interesting and ordered information, such
    as geographic coordinates. Clearly the data is very wide, so we need to be able
    to manipulate this information to get the columns we want.^([5](ch15.html#idm45576008967592))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管内容很难处理，但这个视图足以显示出有几个时间戳以及其他有趣且有序的信息，比如地理坐标。 显然数据非常广泛，因此我们需要能够操纵此信息以获取我们想要的列。
    ^([5](ch15.html#idm45576008967592))
- en: 'Even if you are new to Linux, you can easily learn about simple command line
    tools that can provide helpful information. We can get a line count of the CSV
    file so we have an idea of the scale we are looking at, namely how many data points
    we have. This is a one-liner:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您是 Linux 的新手，也可以轻松了解一些简单的命令行工具，这些工具可以提供有用的信息。 我们可以获取 CSV 文件的行数，以便了解我们正在查看的规模，即我们有多少数据点。
    这是一个一行命令：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can see that NYC has fielded about 20 million complaints to 311 since 2010\.
    That’s more than two complaints per resident.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到自 2010 年以来，纽约市已经接到了大约 2000 万起 311 投诉。 这超过了每位居民的两次投诉。
- en: 'Armed with this knowledge, we use R’s `data.table` knowing that its `fread()`
    function enables partial reading of files (seen in the [documentation](https://perma.cc/ZHN9-5HD3)
    when you read about the `nrows` and `skip` parameters^([6](ch15.html#idm45576043321224)))
    and that `data.table` is extremely performant when handling large data sets. We
    can use this to get initial information, as in the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些知识，我们使用 R 的 `data.table`，知道它的 `fread()` 函数可以部分读取文件（在阅读有关 `nrows` 和 `skip`
    参数的[文档](https://perma.cc/ZHN9-5HD3)时可以看到），并且 `data.table` 在处理大数据集时非常高效。 我们可以使用这个来获取初始信息，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Just from reading the first 10 lines we can already see the column names. For
    all the strengths I listed for NoSQL approaches to time series data, it can be
    nice in a large data set to know the column names from the outset. Of course there
    are workarounds for this with NoSQL data, but most require some effort on the
    part of the user rather than happening automatically.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅从阅读前 10 行，我们已经可以看到列名。 对于所有我列出的 NoSQL 处理时间序列数据的优点来说，在大数据集中，从一开始就知道列名可能是一件好事。
    当然，对于 NoSQL 数据，有解决方法，但大多数都需要用户付出一些努力，而不是自动发生。
- en: 'Several columns suggest useful information:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 几列表明了有用的信息：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These will likely be of character type before we convert, but once we do a
    conversion to a POSIXct type, we can see what the time spans between these dates
    look like:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在转换之前可能是字符类型，但一旦我们转换为 POSIXct 类型，我们就可以看到这些日期之间的时间跨度是什么样的：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the formatting string, we need to use a `%I` for the hour since it’s expressed
    only in 01–12 format, and a `%p` because the timestamps include an AM/PM designation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在格式化字符串中，我们需要使用 `%I` 表示小时，因为它只以 01-12 的格式表示，以及 `%p`，因为时间戳包含 AM/PM 的指示。
- en: 'To get a sense of how these dates tend to be spaced, particularly with respect
    to when a complaint is created versus closed out, let’s load in more rows and
    examine the distribution of what I’ll call the *complaint lifetime* (i.e., the
    span between creation and closing):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这些日期如何分布，特别是在投诉创建与关闭之间的时间，让我们加载更多行并检查所谓的*投诉生命周期*的分布（即创建与关闭之间的时间跨度）：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As we can see, it’s a wide distribution. More surprising than the yearlong wait
    times for some complaints to be closed out is the fact that some complaints have
    negative, even extremely negative, times between their creation and closing date.
    If the negative time were around –365 days (a year), we might imagine a data entry
    problem, but this seems less likely with numbers such as –75 days. This is a problem
    we’ll need to look into.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，这是一个广泛分布的情况。 一些投诉的闭环时间长达一年，令人惊讶的不仅是这些投诉的闭环时间长，有些投诉的闭环时间甚至是负数，甚至是极端负数。
    如果负数时间约为 -365 天（一年），我们可能会想象这是数据输入问题，但对于 -75 天这样的数字，这种情况似乎不太可能。 这是我们需要研究的问题。
- en: 'We can spot another problem by taking a range of the creation date:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过取创建日期的范围来发现另一个问题：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Given the size of this CSV file and the fact that it is supposed to be continuously
    updated, it is surprising that the first lines are not from the 2010 date that
    is supposed to mark the earliest data sets. We would have expected the CSV to
    be continuously appended to. More surprising still is the fact that 2019 dates
    are in the first rows and that 2014 and 2019 dates are both in the same first
    10,000 lines. This suggests that we cannot easily determine the date ordering
    of the data in the file. We can visualize the date distribution from one row to
    the next by performing a line plot of row index versus date, as we do in [Figure 15-2](#fig-1502).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这个CSV文件的大小以及它应该是持续更新的事实，令人惊讶的是前几行不是从标记了最早数据集的2010年开始的日期。我们本来期望CSV文件是持续追加的。更令人惊讶的是，2019年的日期出现在前几行，并且2014年和2019年的日期都在同一前10000行中。这表明我们无法轻易确定文件中数据的日期顺序。我们可以通过执行行索引与日期的线图来可视化从一行到下一行的日期分布，就像我们在[图15-2](#fig-1502)中所做的那样。
- en: '![](assets/ptsa_1502.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图15-2](assets/ptsa_1502.png)'
- en: Figure 15-2\. While most of the dates in the first 10,000 lines of code appear
    to fall in 2014, they also jump around as far forward as 2019—and often!
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-2。虽然代码的前10000行中的大部分日期似乎都在2014年，但它们也会向前跳到2019年——而且经常如此！
- en: There is no way to avoid the problem of acting. If we want to understand how
    behavior changes over time, we will have to confront the unordered data. But we
    have a few options.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 无法避免行动问题。如果我们想了解行为如何随时间变化，我们将不得不面对无序的数据。但我们有几个选择。
- en: Upsample and Aggregate the Data as We Iterate Through It
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在迭代过程中进行上采样和聚合数据
- en: One option is to upsample the data as we iterate through it to construct a condensed
    time series with aggregate statistics. We could choose a resolution and aggregation
    counts from the start of our analysis and then calculate these as we iterate through.
    Then we could sort our results at the end of our analysis. This would look something
    like having, say, a dictionary/list of all dates from 2010 to the present and
    then adding to the appropriate date for each row. This method would produce a
    list/dictionary with a relatively small number of entries, and we could then sort
    this at the end based on date.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是在迭代过程中对数据进行上采样，以构建具有聚合统计信息的压缩时间序列。我们可以从分析的开始选择分辨率和聚合计数，然后在迭代过程中计算这些内容。然后，我们可以在分析结束时对结果进行排序。这看起来就像是拥有从2010年到现在的所有日期的字典/列表，然后为每一行添加到适当的日期。这种方法将产生一个具有相对较少条目的列表/字典，然后我们可以基于日期在结束时对其进行排序。
- en: The upside of this is that it would be relatively straightforward to code and
    would be a way to combine data cleaning, exploration, and analysis into one exploratory
    step. The downside is that detailed data would still be lost in the morass of
    the unsorted file, so that if there was a specific time period of interest we
    would have to search the entire unsorted file to find all relevant entries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的好处是相对容易编码，并且是将数据清理、探索和分析结合到一个探索性步骤中的方法。缺点是详细数据仍然会在未排序的文件混乱中丢失，因此如果有特定的时间段感兴趣，我们将不得不搜索整个未排序的文件以找到所有相关条目。
- en: Since we have already done examples of upsampling in [Chapter 2](ch02.html#finding_and_wrangling_time_series_data),
    I leave this as an exercise for the reader.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在[第2章](ch02.html#finding_and_wrangling_time_series_data)中做过上采样的示例，我把这部分留给读者作为练习。
- en: Sort the Data
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数据进行排序
- en: Another option is to sort the data. This is a daunting task given the large
    size of the file and the relatively unordered dates from what we can observe.
    Even for the data within 2014 it does not seem as though the dates come in order.
    For this reason we have no indications that we can trust any slices of the data,
    so we should think of this as a pile of data in random order.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是对数据进行排序。考虑到文件的大尺寸和相对无序的日期，这是一项艰巨的任务，从我们观察到的情况来看。即使对于2014年的数据，日期似乎也不是按顺序排列的。因此，我们没有任何迹象表明我们可以信任数据的任何切片，因此我们应该将其视为随机顺序的数据堆叠。
- en: Sorting the full file would be extremely memory intensive, but there are two
    reasons it could be worthwhile. One is that we only need to sort once and then
    can save our results for whatever subsequent analysis we want to do. Second is
    that we can then have the full level of detail preserved so that if we spot specific
    time periods of interest in our analysis, we can examine the data in all its detail
    to understand what is happening.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对整个文件进行排序将消耗极大的内存，但有两个理由使得这样做是值得的。一个是我们只需要排序一次，然后可以保存结果，以供我们想要进行的任何后续分析。第二个是我们可以保留完整的详细级别，这样如果我们在分析中发现感兴趣的特定时间段，我们可以检查数据的所有细节，以了解正在发生的情况。
- en: 'Concretely, thinking about how we could accomplish this, we have a few options:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，思考我们如何完成这个任务，我们有几个选择：
- en: Linux has a [command-line tool](https://perma.cc/7SNE-TQ2T) for sorting.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux 有一个[命令行工具](https://perma.cc/7SNE-TQ2T)用于排序。
- en: Most databases can sort data, so we could transfer the data to a database and
    let the database take care of it.^([7](ch15.html#idm45576008488712))
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数数据库都可以对数据进行排序，所以我们可以将数据转移到数据库中，并让数据库来处理它。^([7](ch15.html#idm45576008488712))
- en: We could come up with our own sorting algorithm and implement it. We would need
    to formulate something that did not consume enormous amounts of memory. Odds are
    strongly against our effort matching what is available in prepackaged sorting
    options.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以想出自己的排序算法并实施它。我们需要制定一些不消耗大量内存的东西。很可能我们的努力几乎无法与预打包排序选项中可用的功能匹配。
- en: We choose to use the Linux command-line tool. While it may take some time to
    get this right, we will develop a new life skill as well as gaining access to
    a well-implemented and correct sort for this large file.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用 Linux 命令行工具。虽然可能需要一些时间来做到这一点，但我们将开发一个新的生活技能，并获得一个对于这个大文件而言实现良好且正确的排序。
- en: 'We begin by first creating a small test file to use:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个小的测试文件以便使用：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that this involves both the `head` (i.e., print out the start) and `tail`
    (i.e., print out the end) commands. `head` will include the first line of the
    file, which for this file provides the column names. If we include this and sort
    it along with the values, it will not preserve the column names as the top line
    in the file, so we cut it out before sorting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这涉及到`head`（即打印开头）和`tail`（即打印结尾）命令。`head` 将包括文件的第一行，对于这个文件来说，它提供了列名。如果我们将其包含并与数值一起排序，它将不会保留列名作为文件顶部行，因此我们在排序之前将其删除。
- en: 'If you are working on a Linux-based operating system, you can then apply the
    `sort` command as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用基于 Linux 的操作系统，那么可以如下应用`sort`命令：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case we identify the field separator and then indicate we want to sort
    by the second and third columns—that is, by the creation date and close date (which
    we only know from previously inspecting the file). We output this into a new file,
    as it wouldn’t be very helpful to have it output to standard out.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们确定字段分隔符，然后指示我们想要按第二和第三列排序，即按创建日期和关闭日期排序（这些信息我们只能从先前检查的文件中得知）。我们将其输出到一个新文件中，因为将其输出到标准输出中并不会有太大帮助。
- en: 'Now we can inspect the sorted file in R, but unfortunately we will find that
    this also does not return a sorted file. Go back a few pages to where we ran the
    `head` command on the CSV file, and you will see why. We sorted according to a
    date column (which would be processed like a string, not in a date-aware manner).
    However, the current formatting of the dates begins with the month, so we will
    end with a column of dates by month rather than sorted by overall time, as we
    see when we review the “sorted” CSV resulting from our previous command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在 R 中检查排序后的文件，但不幸的是我们会发现这也不会返回一个排序后的文件。回到我们在 CSV 文件上运行`head`命令的几页前，你就会明白为什么了。我们按日期列进行排序（这将被处理为字符串，而不是日期感知方式）。然而，当前日期的格式是从月份开始，所以我们最终会得到一个按月份而不是按整体时间排序的日期列，这是当我们审查由前面命令生成的“排序后”CSV时所见到的：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we can see, the sorting makes sense as a string sort rather than a date
    sort. In fact, this points to one of the virtues of using proper ISO date formatting,
    which is that it will still get the sort right when sorting as a string, unlike
    in the preceding format. This is an example of a very common problem with “found”
    time series data: the timestamping format available may not be the most conducive
    to time series analysis.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，排序作为字符串排序而不是日期排序是有道理的。事实上，这指出了使用正确的ISO日期格式的优点之一，即当作为字符串进行排序时，它仍将正确排序，与之前的格式不同。这是“现有”时间序列数据常见问题的一个示例：可用的时间戳格式可能不利于时间序列分析。
- en: We revisit the NYC Open Data interface to see whether there is a workaround
    to this formatting (see [Figure 15-3](#fig-1503)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新访问纽约市开放数据界面，看看是否有解决此格式问题的方法（见[图 15-3](#fig-1503)）。
- en: '![](assets/ptsa_1503.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1503.png)'
- en: Figure 15-3\. The NYC Open Data portal offers sorting via a web interface, which
    appears to be available to any column on this large data set. This is an impressive
    free resource when you think about the computational power required for that sort.
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-3\. 纽约市开放数据门户提供了通过网页界面进行排序的功能，这似乎适用于这个大数据集的任何列。考虑到排序所需的计算能力，这是一个令人印象深刻的免费资源。
- en: A glimpse at the web page tabular view of the data seems to concur with our
    CSV, in that the data does not appear to be sorted in time order. However, we
    see there is a sort option, so we apply this to the data set, which does indeed
    update the data after an understandable wait while the large data set is stored.
    This would be a great solution, except that, unfortunately the resulting CSV downloaded
    is still out of order.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据的网页表格视图的一瞥似乎与我们的CSV文件一致，即数据似乎没有按时间顺序排序。然而，我们看到有排序选项，因此我们将其应用于数据集，确实在存储大数据集时会有一个可以理解的等待时间后更新数据。这将是一个很好的解决方案，但不幸的是，下载得到的CSV文件仍然是无序的。
- en: There are a number of other solutions we could explore at this time. We could
    see whether using the Open Data’s API rather than the web interface provides more
    digestible dates or a way to ensure sorting. We could work with more Linux command-line
    tools, such as `awk`, to extract the different parts of the timestamp into different
    columns or into a single rearranged column with ISO formatting.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此时我们可以探索其他几种解决方案。我们可以查看使用开放数据的API而不是网页界面是否提供了更易处理的日期或确保排序的方法。我们可以使用更多的Linux命令行工具，比如`awk`，来提取时间戳的不同部分到不同的列或者合并到一个带有ISO格式的重新排列列中。
- en: 'Instead, we take a pared-back approach of seeing whether our available tools
    can handle this CSV if we read only certain columns. The first question that interests
    me about this data set is how the lag between the creation of a 311 complaint
    and the closing of that complaint may have varied over time. In this case, I hypothesize
    that I need only two columns: `CreatedDate` and `ClosedDate`. I will see whether
    just reading these two columns, which are a small portion of all the columns in
    terms of both count and character count (because some columns are quite long),
    is possible on my lightweight laptop. (I could also explore the lazy way of fixing
    the problem, which would be a hardware upgrade, either temporarily or permanently.)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们采用了简化的方法，看看我们现有的工具是否能够处理这个CSV，如果我们只读取特定的列。我对这个数据集感兴趣的第一个问题是创建311投诉与关闭该投诉之间的滞后时间可能随时间变化的情况。在这种情况下，我假设我只需要两列：`CreatedDate`和`ClosedDate`。我将看看是否可以只读取这两列，这在数量和字符计数方面都是所有列的一个小部分（因为某些列非常长），在我的轻便笔记本电脑上是可能的。（我还可以探索修复问题的懒惰方式，即进行硬件升级，无论是暂时的还是永久的。）
- en: 'So now we are able to read in all the rows of the data, and our subsequent
    analysis will be on the full data set rather than only the first 1,000 rows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够读取数据的所有行，并且我们随后的分析将基于整个数据集而不仅仅是前1000行：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This was done on a lightweight laptop manufactured in 2015, so chances are you
    can do it with whatever you use for work or home, too. Those new to “big data”
    may be surprised that 19 million rows is not a big deal, but this is often the
    case. We actually need only a small slice of the data to address a relevant time
    series question.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在2015年生产的一款轻便笔记本电脑上完成的，因此您可能也可以用于工作或家庭使用的设备进行操作。对于“大数据”新手来说，1900万行数据并不算什么大不了的事情，但实际情况常常如此。我们实际上只需要数据的一小部分来回答相关的时间序列问题。
- en: 'Once we look at the `LagTime` column, we notice some surprisingly incorrect
    numbers—numbering in tens of thousands of days, or negative days. We weed out
    these numbers and put a cap on the data driven partly by a distribution of a random
    sample of data points:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看`LagTime`列时，我们注意到一些令人惊讶的错误数字——数以万计的天数，甚至是负数。我们排除了这些数字，并在一定程度上根据数据随机样本的分布加以限制：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We discard data with negative lag times, as we lack documentation or domain
    knowledge to know what those are. We also reject data that we regard as having
    unrealistic or unhelpfully extreme values by discarding data where the lag time
    to close out the 311 complaint was more than 1,000 days.^([8](ch15.html#idm45576008105304))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们丢弃负滞后时间的数据，因为我们缺乏相关文档或领域知识来了解这些数据的含义。我们也会拒绝那些我们认为有不切实际或者极端不适的数值，例如关闭311投诉所需的滞后时间超过1000天的数据。^([8](ch15.html#idm45576008105304))
- en: Warning
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Use caution when you discard data. In this exercise, we discarded around 1.3%
    of the data due to lag times in resolving a 311 complaint that didn’t make sense,
    either because they were so long as to warrant an explanation we would not be
    able to get or because they were negative, implying a data entry error or some
    other issue beyond what we could likely discover from the data alone.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在丢弃数据时请谨慎。在这个练习中，由于处理311投诉所需的滞后时间过长，导致我们丢弃了约1.3%的数据，这些投诉可能因为太长而无法解释，或者因为是负数，暗示着数据录入错误或者其他我们单靠数据无法解决的问题。
- en: Given that our question relates to the overall distribution of the data, it’s
    unlikely that such a small portion of points would affect our analysis about a
    distributional question. However, in the real-world you would want to investigate
    these data points and the possible downstream effects on your analysis task. This
    is not time series–specific advice but just a matter of general practice.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的问题与数据整体分布有关，这些少数数据点不太可能影响我们关于分布问题的分析。然而，在实际应用中，您需要调查这些数据点及其可能对分析任务的后续影响。这并非时间序列特定的建议，而只是一般实践的问题。
- en: Now that we are able to hold all of the data of interest in memory at the same
    time, we could ask questions about the series holistically. However, the question
    of interest is whether and how the distribution of lag time may have changed over
    time. We could do this with a sliding or rolling window over the time series,
    but that is computationally taxing; we would have to do many related computations
    repeatedly as we slide a window over the data. Also we want to explore doing this
    in a way that could also work on a live stream, as we could imagine continuing
    this project onto current data as it comes in. It would be better not to have
    to store this multigigabyte data file indefinitely.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够同时将所有感兴趣的数据保留在内存中，我们可以全面地对序列提出问题。然而，我们感兴趣的问题是滞后时间的分布是否随时间变化而改变。我们可以通过时间序列的滑动或滚动窗口来实现这一点，但这是计算密集型的；当我们在数据上滑动窗口时，我们需要重复执行许多相关的计算。同时，我们也希望探索一种可以应用于实时数据流的方法，因为我们可以想象将这个项目持续到当前数据的情况。因此最好不要无限期地存储这个多GB的数据文件。
- en: Online Statistical Analysis of Time Series Data
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列数据的在线统计分析
- en: We are going to use a fairly straightforward online quantile estimation tool
    called the [*P-square algorithm*](https://perma.cc/G8LA-7738), with one slight
    modification to make it time-aware. The original algorithm assumed that there
    was a stable distribution from which quantiles were being inferred, but we want
    to account for the case of distributions that change over time. As with an exponentially
    weighted moving average, we add this time awareness by weighting earlier observations
    less, and we do it in the same way by introducing a factor to scale down the weights
    of the previous measurements each time a new measurement is available (see [Figure 15-4](#fig-1504)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为[*P-square算法*](https://perma.cc/G8LA-7738)的在线分位数估算工具，稍作修改以使其具有时间感知性。原始算法假定存在一个稳定的分布用于推断分位数，但我们希望考虑分布随时间变化的情况。与指数加权移动平均类似，我们通过减少早期观察的权重来引入时间感知性，并且每次有新的测量值时都以相同的方式引入一个因子来缩小以前测量的权重（参见[图15-4](#fig-1504)）。
- en: '![](assets/ptsa_1504.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1504.png)'
- en: Figure 15-4\. The computational structure of our online quantile estimation
    using the P-square algorithm. We maintain a series of markers indicating where
    we think the quantiles are and what the cumulative counts are for all data points
    less than or equal to each of those quantiles.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-4\. 我们使用P-square算法进行在线分位数估计的计算结构。我们维护一系列标记，指示我们认为分位数在哪里以及所有小于或等于每个分位数的数据点的累积计数。
- en: The algorithm requires a bit of recordkeeping, making it easier to implement
    in a more object-oriented programming language, so we’ll switch from R to Python.
    Note, however, that we will use the preprocessed data from R, as the `data.table`
    package has substantially better performance with that kind of big data than tools
    available in Python.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法需要一些记录，使得在更面向对象的编程语言中更容易实现，因此我们将从R切换到Python。但是，请注意，我们将使用R中预处理的数据，因为`data.table`包在处理大数据时比Python中的工具具有更好的性能。
- en: 'The version of the P-square algorithm we implement forms a histogram of the
    values. So, as we create a `PQuantile` object, we allocate a preset number of
    bins for our histogram counts, bin positions, and running tallies of observations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的P-square算法版本形成值的直方图。因此，当我们创建一个`PQuantile`对象时，为我们的直方图计数、箱位和观测的累计总和分配了预设数量的箱：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are two configurable parameters: the number of evenly spaced quantiles
    to estimate and the discounting factor for old observations.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个可配置参数：用于估计的均匀分布的分位数的数量和旧观测的折现因子。
- en: The other class members include a running total of the number of observations
    (this will be subject to time discounting by the configurable time discounting
    factor), the estimated quantiles, and the running count of observations less than
    or equal to a given quantile value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类成员包括观测数量的累计总和（这将根据可配置的时间折现因子进行时间折现），估计的分位数以及小于或等于给定分位数值的观测计数的累计总和。
- en: There is only one public function, whose role is to accept the next observation.
    When a new observation arrives, what happens is a result of how early in the series
    we are. For the first `self.b` values, the inputs are accepted and necessarily
    make up the estimation of the quantile. `self.q` is sorted so that its values
    reflect the quantiles.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个公共函数，其作用是接受下一个观测值。当有新的观测到来时，结果取决于序列的早期阶段。对于前`self.b`个值，接受这些输入并且必然构成分位数的估计。`self.q`被排序，使其值反映分位数。
- en: 'So, for example, imagine you input a `b = 5` for the number of desired quantile
    values and then input the sequence `2, 8, 1, 4, 3`. At the end of this sequence,
    `self.q` would be equal to `[1, 2, 3, 4, 8]`. `self.n`, the counts of values less
    than or equal to each of the quantiles, would be equal to `[1, 2, 3, 4, 5]`, the
    value it was already initialized to in `__init__`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您输入了`b = 5`，用于期望的分位数值，然后输入了序列`2, 8, 1, 4, 3`。在这个序列结束时，`self.q`将等于`[1, 2,
    3, 4, 8]`。`self.n`，小于或等于每个分位数的值的计数，将等于`[1, 2, 3, 4, 5]`，这是在`__init__`中初始化的值：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Things get interesting once you have more than `self.b` values. At this point,
    the code begins to make decisions as to how to combine values to estimate quantiles
    without keeping all the data points stored for repeated analysis. In this case,
    the P-square algorithm does this with what we call `self.next_obs2`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有超过`self.b`个值，事情就变得有趣起来。在这一点上，代码开始决定如何组合值以估计分位数，而无需保留所有数据点以进行重复分析。在这种情况下，P-square算法通过我们称之为`self.next_obs2`来执行此操作：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Ideally, the *i*th quantile value should be evenly spaced so that exactly *i/b
    ×* total observations are less than it. If this is not the case, the marker is
    moved one position to the left or right, and its associated quantile value is
    modified with a formula derived from the presumption of a locally parabolic shape
    of the histogram. That formula dictates the criteria indicated earlier for sizing
    the `d` variable to determine whether a particular quantile value and count have
    to be adjusted.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，第*i*个分位数值应均匀分布，以便恰好有*i/b ×* 总观测值小于它。如果不是这种情况，标记将向左或向右移动一个位置，并且其相关的分位数值将使用从直方图的局部抛物线形状假设推导出的公式进行修改。该公式根据局部抛物线形状的假设确定是否需要调整特定分位数值和计数的`d`变量的大小。
- en: 'If the value does have to be adjusted, there is another decision to make as
    to whether the parabolic or linear adjustment is appropriate. This is implemented
    in the following code. For more details, see the derivation in the [original paper](https://perma.cc/G8LA-7738).
    This paper is great because of the approachable mathematics used in the technique
    and also because it provides very clear instructions as to how to implement the
    method and then test your implementation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果值确实需要调整，那么另一个决定就是确定抛物线还是线性调整是否合适。这在以下代码中实现。更多细节请参见[原始论文](https://perma.cc/G8LA-7738)中的推导。这篇论文很棒，因为它使用的数学技术易于理解，同时还清楚地说明了如何实施该方法以及如何测试您的实施方法：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For perspective on the simplicity of this method, all the class code is listed
    here in one place:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种方法的简易性的视角，所有的班级代码都在这里列出：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have this time-oriented method, we should convince ourselves that
    it works reasonably well with a toy example. We first try sampling data points
    from one distribution and then abruptly changing to another. In each case, we
    are sampling for the 40th percentile, although based on the configuration of 10
    histogram points, we are maintaining a histogram that indicates the 0th, 10th,
    20th, …90th, 100th percentiles. This is helpful because it means we can have a
    fairly detailed description of the changing distribution. For this toy example,
    we focus on just the 40th percentile (`qt.q[4]`), which results in the plot in
    [Figure 15-5](#fig-1505):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这种时间导向的方法，我们应该确信它在玩具示例中运行得相当好。我们首先尝试从一个分布中抽样数据点，然后突然切换到另一个分布。在每种情况下，我们都在采样第40百分位数，尽管基于配置的10个直方图点，我们维护的直方图显示了0、10、20…90、100百分位数。这很有帮助，因为这意味着我们可以对变化的分布有相当详细的描述。对于这个玩具示例，我们只关注第40百分位数（`qt.q[4]`），这导致了[图 15-5](#fig-1505)中的绘图：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](assets/ptsa_1505.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1505.png)'
- en: Figure 15-5\. When we discount older measurements heavily (multiplying by a
    smaller discount factor), we more quickly see that the underlying distribution
    has changed.
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-5\. 当我们大幅度折扣旧测量数据（乘以较小的折扣因子）时，我们更快地看到底层分布已经发生变化。
- en: 'In contrast, we see a slower adoption to the changing quantile in the case
    of a larger discounting factor (see [Figure 15-6](#fig-1506)):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当我们对较大折扣因子的情况下较少折扣旧测量时，我们看到对于变化的分位数估计较慢（见[图 15-6](#fig-1506)）：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](assets/ptsa_1506.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1506.png)'
- en: Figure 15-6\. When we discount older measurements less (multiplying by a larger
    discount factor), our quantile estimation is slower to recognize that the underlying
    distribution has changed.
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-6\. 当我们对较老的测量数据的折扣较少（乘以较大的折扣因子）时，我们的分位数估计较慢地认识到底层分布已经发生变化。
- en: Now we apply this rolling quantile to a subset of our data (see [Figure 15-7](#fig-1507)).
    We do not do the entire data set, not because of the computational challenge,
    but because it was overly taxing for my workaday laptop to graph all the recorded
    quantiles!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这种滚动分位数应用于我们数据的一个子集（参见[图 15-7](#fig-1507)）。我们并不对整个数据集进行操作，不是因为计算上的挑战，而是因为将所有记录的分位数绘制到我的日常笔记本电脑上过于繁重！
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](assets/ptsa_1507.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_1507.png)'
- en: Figure 15-7\. The 90th, 70th, 50th, 30th, and 20th percentile values over time
    for the first 100,000 rows in the data set when sorted by closed date.
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-7\. 数据集中前100,000行按关闭日期排序时，时间内的第90、70、50、30和20百分位数值。由于按关闭日期排序，可能会发现许多迅速解决的311投诉集中在前期，这解释了数据集前部分中更小的分位数估计。^([9](ch15.html#idm45576005162488))
- en: '[Figure 15-7](#fig-1507) shows the 90th, 70th 50th, 30th, and 20th percentile
    values plotted over time for the first 100,000 rows in the data set when sorted
    by closed date. Because they were sorted by closed date, it is likely that many
    quickly resolved 311 complaints were front-loaded, which explains the much smaller
    quantile estimations in the first portion of the data set.^([9](ch15.html#idm45576005162488))'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-7](#fig-1507)显示了按关闭日期排序时数据集前100,000行的时间内第90、70、50、30和20百分位数值。由于按关闭日期排序，可能会发现许多迅速解决的311投诉集中在前期，这解释了数据集前部分中更小的分位数估计。^([9](ch15.html#idm45576005162488))'
- en: So did the distribution change? Visually it seems to, for a few reasons. One
    is left censoring, described shortly, which reflects how we sorted and selected
    data. The fact that we ordered the data by the `ClosedData` column, combined with
    the fact that this data set does not appear to have infinite lookback (that is,
    presumably 311 complaints made before a certain date did not make it into this
    system), makes the lag times we see at the beginning dates appear shorter. In
    other words, this apparent change over time is simply an artifact of our incomplete
    data (and an incomplete underlying data set) combined with our choice of sorting.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来分布有变化吗？从视觉上看，似乎有几个原因。其中一个是左截尾，稍后会简要描述，这反映了我们如何对数据进行排序和选择。我们按`ClosedData`列排序数据，并结合这个数据集似乎没有无限的回溯期（也就是说，假设在某个日期之前提交的
    311 投诉并未进入这个系统），这使得我们在开始日期看到的滞后时间似乎较短。换句话说，这种看似随时间变化的现象只是我们不完整的数据（以及不完整的底层数据集）的产物，再加上我们选择排序的结果。
- en: On the other hand, we can see features that nonetheless do suggest changes in
    distributions over time. There appear to be peaks and troughs in the quantile
    curve estimates, and we might even consider whether there may be periodic behavior
    in our curves, as there may be predictable times when the quantile values rise
    and fall due to exogenous organizational factors (perhaps a drive to close out
    complaints before the end of a month, or certain funding cycles that increase
    the number of workers available to close out complaints at certain times of the
    year).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以看到仍然有特征表明随时间分布发生变化。分位数曲线估计中出现高峰和低谷，我们甚至可以考虑我们的曲线中是否可能存在周期性行为，因为由于外生组织因素的原因，分位数值可能在可预测的时间上升和下降（也许是为了在月底之前结案投诉的推动，或者某些资金周期增加了特定时间可用于结案投诉的工人数量）。
- en: Given that we now have preliminary results, the best option would be to establish
    some significant dates (i.e., where do we see spikes or periodic behavior?) and
    try to cross-reference them against any institutional facts we can establish about
    the rhythm of work. We should also run simulations to assess how we think the
    left censoring should affect early quantile estimations under different scenarios.
    In this way we could develop better qualitative and quantitative understanding
    of the unknown aspects of our system, and this information would be extremely
    helpful for making a final determination as to whether the distribution of resolution
    times is evolving over time and, if so, how.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们现在有了初步结果，最好的选择是确定一些重要日期（即我们在哪些日期看到尖峰或周期性行为？），并尝试将它们与我们可以确认的工作节奏相关的任何制度事实进行交叉参考。我们还应该运行模拟，评估我们认为左截尾如何在不同情景下影响早期分位数估计。通过这种方式，我们可以更好地理解系统未知方面的定性和定量内容，这些信息对于最终确定解决时间分布是否随时间演变以及如何演变非常有帮助。
- en: Let’s say we had taken these steps—then what? We would need to look for a methodology
    to compare distributions for similarity difference where only the quantiles of
    the distribution rather than all the sample points were available. One way we
    could do this would be to run simulations/bootstrap the entire process. This would
    lead to an answer where we could fully articulate and control the assumptions
    that went into the model by coding our simulations. In fact, many statistical
    approaches to doing such comparisons also focus on bootstrap methods.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 比方说，如果我们采取了这些步骤——接下来呢？我们需要寻找一种方法来比较分布的相似性或差异性，只使用分布的分位数而不是所有样本点。我们可以通过运行模拟/引导整个过程来做到这一点。这将带来一个答案，我们可以通过编码我们的模拟来完全阐明和控制模型所依赖的假设。事实上，许多进行这种比较的统计方法也专注于引导法。
- en: Remaining Questions
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 余下的问题
- en: 'Our visualization suggests new queries. One relates to the possibility of cyclical
    or seasonal behavior. There seem to be periodic bumps in all the estimated quantiles.
    We might consider further investigating this, and we have a number of options
    for doing so:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可视化表明了新的查询需求。其中一个与周期性或季节性行为的可能性相关。所有估计的分位数都似乎有周期性的起伏。我们可以考虑进一步调查这一点，并有几种方法可以做到这一点：
- en: We could attempt to fit harmonics (sines and cosines) to these quantile curves
    and see whether a common periodicity emerged.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以尝试将谐波（正弦和余弦）拟合到这些分位数曲线上，看看是否会出现共同的周期性。
- en: We could model the quantile itself as an ARIMA or SARIMA process and look for
    evidence of seasonality. This would also entail preliminary steps such as exploring
    the ACF and PACF of the curves that we would model as time series.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将分位数本身建模为 ARIMA 或 SARIMA 过程，并寻找季节性的证据。这还需要进行初步步骤，如探索我们将作为时间序列建模的曲线的 ACF
    和 PACF。
- en: We could ask the agency running the 311 service for more information and see
    whether they recognize any periodic behavior induced by their organizational structure
    and operating procedures.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以向运行 311 服务的机构索取更多信息，看看他们是否认识到任何由其组织结构和运营程序引起的周期性行为。
- en: 'In addition to the periodic behavior, we can also see a jump in the estimated
    quantile values at just under the index = 70,000 location. Given that all the
    quantiles jumped, it seems unlikely that this is due to just a single or a handful
    of outliers. There are a few ways we might investigate this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了周期性行为之外，在索引约为70,000位置附近，我们还可以看到估计分位数值的跳跃。考虑到所有分位数都跳跃了，似乎不太可能仅由单个或少数异常值引起。我们可以通过几种方式进行调查：
- en: Go back to the raw data for this time period and see what features may suggest
    an explanation. Was there a surge in 311 complaints? Or a surge in a particular
    kind of complaint that tends to take longer to resolve?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回到这段时间内的原始数据，看看有哪些特征可能可以提供解释。是否有 311 投诉激增？或者是某种类型的投诉激增，这类投诉通常需要更长时间来解决？
- en: Alternately, we could revisit the raw data to back out the approximate date
    of this jump in the quantiles and cross-reference this against local news, preferably
    with the help of someone who could point us in the right direction. Assistance
    from someone at the agency, or someone savvy about city government, might be most
    valuable. It’s also possible, however, that this date could correspond to a massive
    event in NYC that would explain the jump, such as superstorm Sandy in 2012.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，我们可以重新审视原始数据，以确定分位数跳跃的大致日期，并与本地新闻进行交叉参考，最好是在某人的帮助下指引我们正确的方向。来自机构的某人的帮助，或者对城市政府了解深刻的人的帮助，可能是最有价值的。然而，也有可能，这个日期可能对应于纽约市的一个重大事件，例如2012年的超级飓风桑迪，这可以解释这个跳跃。
- en: Further Improvements
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步的改进
- en: 'We could make this an even more time-aware algorithm. Our modification to the
    P-square algorithm discounts prior observations, but it assumes that all observations
    are evenly spaced. This is manifested in the fact that the input of the next observation
    does not feature a timestamp, and the same discounting factor is always applied.
    We could craft a more flexible algorithm by using the change in timestamp to old
    information as compared to new information such that the discounting would depend
    on the change in time since the last update was measured. It would also be more
    accurate for our 311 data set. This is left as an exercise for the reader but
    only involves changing a few lines of code. Hint: the discounting factor should
    become a function of time.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使这个算法更加时间感知。我们对 P-square 算法的修改排除了先前的观察，但它假定所有观察值是均匀分布的。这体现在下一个观察值的输入中没有时间戳，并且始终应用相同的折扣因子。我们可以通过使用时间变化到旧信息相对于新信息的折扣来设计一个更灵活的算法，使折扣取决于自上次更新以来经过的时间变化。这对我们的
    311 数据集也会更加精确。这留给读者作为一个练习，但只涉及更改几行代码。提示：折扣因子应该成为时间的函数。
- en: We could also look into other ways of estimating quantiles over time, either
    with an online or window measurement. Because the importance of online data is
    growing—and especially for online big data—there is a variety of emerging research
    on this topic. Both statistical and machine learning approaches have dealt with
    this over the last years, and there are a good number of approachable academic
    papers that are accessible to practicing data scientists.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以探索其他通过时间估计分位数的方法，无论是在线还是窗口测量。由于在线数据的重要性越来越大，特别是对于在线大数据，关于这个主题有大量新兴研究。统计学和机器学习方法在过去几年已经处理了这个问题，并且有许多适合实际数据科学家的可接近的学术论文。
- en: More Resources
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多资源
- en: Ted Dunning and Otmar Ertal, [“Computing Extremely Accurate Quantiles Using
    t-Digests,”](https://perma.cc/Z2A6-H76H) research paper, 2019, https://perma.cc/Z2A6-H76H.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Ted Dunning 和 Otmar Ertal，[“使用 t-Digest 计算极其精确的分位数,”](https://perma.cc/Z2A6-H76H)
    研究论文，2019，https://perma.cc/Z2A6-H76H。
- en: The f-digest algorithm for extremely efficient and flexible computation of quantiles
    of online time series data is rapidly gaining traction as a leading technique
    for handling quantile estimation of online time series even for the case of nonstationary
    distributions. Implementations are available in a number of languages, including
    Python and high-performance C++ and Go variations. This approach is particularly
    useful because there is no need to decide in advance which quantiles interest
    you—rather, the entire distribution is modeled as a set of clusters from which
    you can infer any quantile you like.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: f-digest算法用于极高效且灵活地计算在线时间序列数据的分位数，正迅速成为处理非平稳分布情况下的在线时间序列分位数估计的领先技术。该方法在多种语言中都有实现，包括Python和高性能的C++以及Go变体。该方法特别有用，因为无需预先决定感兴趣的分位数，而是将整个分布建模为一组聚类，从中可以推断出任何你想要的分位数。
- en: 'Dana Draghicescu, Serge Guillas, and Wei Biao Wu, [“Quantile Curve Estimation
    and Visualization for Nonstationary Time Series,”](https://perma.cc/Z7T5-PSCB)
    *Journal of Computational and Graphical Statistics* 18, no. 1 (2009): 1–20, https://perma.cc/Z7T5-PSCB.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dana Draghicescu, Serge Guillas, 和 Wei Biao Wu, [“非平稳时间序列的分位曲线估计与可视化,”](https://perma.cc/Z7T5-PSCB)
    *计算与图形统计杂志* 18卷, 1期 (2009): 1–20, https://perma.cc/Z7T5-PSCB.'
- en: This article illustrates several nonparametric methods to model nonstationary
    distributions in time series quantile estimation. It is helpful because it covers
    real-world data, simulation, and nonstandard distributions (such as non-Gaussian
    distributions). There is also sample code (unusual for a statistics academic journal
    article), albeit behind a paywall.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了几种非参数方法来建模时间序列分位数估计中的非平稳分布。它之所以有用，是因为涵盖了真实世界数据、模拟数据和非标准分布（如非高斯分布）。此外，该文章提供了样例代码（对于统计学术期刊文章来说是不寻常的），尽管可能需要付费才能获取。
- en: András A. Benczúr, Levente Kocsis, and Róbert Pálovics, [“Online Machine Learning
    in Big Data Streams,”](https://perma.cc/9TTY-VQL3) research paper, 2018, https://perma.cc/9TTY-VQL3.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: András A. Benczúr, Levente Kocsis, 和 Róbert Pálovics, [“大数据流中的在线机器学习,”](https://perma.cc/9TTY-VQL3)
    研究论文，2018年，https://perma.cc/9TTY-VQL3.
- en: This reference discusses technical approaches to a variety of common machine
    learning tasks related to time series. Particularly interesting is the discussion
    of online data processing for many kinds of machine learning and technical tips
    about parallelization of online tasks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该参考资料讨论了与时间序列相关的各种常见机器学习任务的技术方法。特别有趣的是关于多种机器学习任务的在线数据处理的讨论，以及关于在线任务并行化的技术提示。
- en: Sanjay Dasgupta, [“Online and Streaming Algorithms for Clustering,”](https://perma.cc/V3XL-GPK2)
    lecture notes, Computer Science and Engineering, University of California San
    Diego, Spring 2008, https://perma.cc/V3XL-GPK2.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Sanjay Dasgupta, [“在线和流式聚类算法,”](https://perma.cc/V3XL-GPK2) 计算机科学与工程，加利福尼亚大学圣迭戈分校，2008年春季，https://perma.cc/V3XL-GPK2.
- en: While not specific to time series data, these lecture notes give a general overview
    of unsupervised clustering for online data. These notes are enough to get you
    started putting together potential solutions for a time series–specific application.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不专门针对时间序列数据，这些讲义总结了关于在线数据的无监督聚类的一般概述。这些讲义足以帮助您开始构建时间序列特定应用的潜在解决方案。
- en: Ruofeng Wen et al., [“A Multi-Horizon Quantile Recurrent Forecaster,”](https://perma.cc/22AE-N7F3)
    research paper, November 2017, https://perma.cc/22AE-N7F3.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Ruofeng Wen 等, [“多时域分位数递归预测器,”](https://perma.cc/22AE-N7F3) 研究论文，2017年11月，https://perma.cc/22AE-N7F3.
- en: This research paper from Amazon provides an example of using quantile information
    from data as a way to train recurrent neural networks effectively for time series
    forecasting. The researchers demonstrate effective training of a neural network
    that can produce probabilistic assessments rather than point estimates. This paper
    is a good illustration of another potential use case for quantile information,
    an underused resource in time series analysis.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊的这篇研究论文提供了一个例子，展示了如何有效地使用数据中的分位数信息来训练递归神经网络，以便进行时间序列预测。研究人员展示了如何有效地训练神经网络，使其能够生成概率评估而非点估计。本文很好地阐明了分位数信息的另一个潜在用例，这在时间序列分析中是一个未充分利用的资源。
- en: ^([1](ch15.html#idm45576012501832-marker)) Note that highly sought-after time
    series data, such as the US jobs report, is very time series–aware and meticulously
    cleaned and formatted. However, such data is already quite picked over and unlikely
    to furnish novel time series applications to the would-be researcher or entrepreneur.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch15.html#idm45576012501832-marker)) 请注意，像美国的就业报告等备受追捧的时间序列数据非常重视时间序列，经过精心清洗和格式化。然而，这类数据已经被充分利用，不太可能为即将成为研究人员或企业家的人提供新的时间序列应用。
- en: ^([2](ch15.html#idm45576012488200-marker)) Of course your access might depend
    on your linguistic capabilities (or those of a helpful colleague).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch15.html#idm45576012488200-marker)) 当然，您的访问权限可能取决于您的语言能力（或者一位乐意帮助的同事的语言能力）。
- en: ^([3](ch15.html#idm45576012481048-marker)) If your organization always scales
    up to solve the problem of not enough RAM, you’re doing it wrong.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch15.html#idm45576012481048-marker)) 如果您的组织总是通过扩展解决RAM不足的问题，那么您的做法是错误的。
- en: ^([4](ch15.html#idm45576012479544-marker)) For inspiration, read the [TensorFlow
    documentation on data sets and associated classes](https://www.tensorflow.org/guide/datasets).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch15.html#idm45576012479544-marker)) 作为灵感，阅读[TensorFlow关于数据集和相关类的文档](https://www.tensorflow.org/guide/datasets)。
- en: ^([5](ch15.html#idm45576008967592-marker)) Notice that someone expert in Unix
    systems could easily use `awk` to manipulate a CSV quite effectively on the command
    line or with a simple shell script. Often such tools are excellent options for
    big data because they are extremely efficient and well implemented, unlike, unfortunately,
    many commonly used data analysis tools in R and Python. If you run into problems
    with your favorite data processing tools, it can often be a good idea to learn
    some Unix command-line tools to supplement, especially in the case of big data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch15.html#idm45576008967592-marker)) 请注意，精通Unix系统的人可以轻松使用`awk`在命令行或简单的shell脚本中有效地操作CSV。通常，这些工具在处理大数据时非常高效且实现良好，与不幸的是，许多常用的R和Python数据分析工具相比。如果您在使用您喜爱的数据处理工具时遇到问题，学习一些Unix命令行工具来补充可能是个好主意，尤其是在处理大数据的情况下。
- en: ^([6](ch15.html#idm45576043321224-marker)) Note that Python’s Pandas offers
    [similar functionality](https://perma.cc/68EE-2ZZ9).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch15.html#idm45576043321224-marker)) 请注意，Python的Pandas提供了[类似的功能](https://perma.cc/68EE-2ZZ9)。
- en: ^([7](ch15.html#idm45576008488712-marker)) We’d have to pick one that offers
    this functionality—not all databases do, and especially not all time series databases,
    which tend to presume data comes into the database in chronological order.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch15.html#idm45576008488712-marker)) 我们必须选择一个能提供这种功能的数据库——并非所有数据库都能提供，尤其是不是所有时间序列数据库都能假定数据按时间顺序进入数据库。
- en: ^([8](ch15.html#idm45576008105304-marker)) While 1,000 days may seem surprisingly
    high, in fact I personally have made several 311 complaints that have not been
    resolved in as many days. I have been waiting years for NYC to replace the tree
    in front of my house after the old one died—every time I call 311 I am told they
    are working on it.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch15.html#idm45576008105304-marker)) 虽然1,000天可能看起来很长，但事实上，我个人已经等了几年，提交了多次311投诉，但仍未解决。树死后，我一直在等纽约市在我家门前替换树木——每次我打电话给311，他们都告诉我他们正在处理。
- en: ^([9](ch15.html#idm45576005162488-marker)) The 20th percentile is so small it
    can’t really be seen when plotted on the scale of the other quantiles presented
    here. It appears as a solid horizontal line at the base of the other distributions,
    but you could see it better by changing to log-scaling or by plotting it on its
    own.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch15.html#idm45576005162488-marker)) 第20百分位数非常小，在这里呈现的其他分位数尺度上几乎看不见。它看起来像其他分布的基础上的一条实线，但如果切换到对数尺度或单独绘制它，您就能更清楚地看到它。
