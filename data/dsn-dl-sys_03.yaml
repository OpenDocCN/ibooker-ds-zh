- en: 3 Model training service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 模型训练服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Designing principles for building a training service
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建训练服务的原则
- en: Explaining the deep learning training code pattern
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释深度学习训练代码模式
- en: Touring a sample training service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索示例训练服务
- en: Using an open source training service, such as Kubeflow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开源训练服务，例如Kubeflow
- en: Deciding when to use a public cloud training service
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定何时使用公共云训练服务
- en: The task of model training in machine learning is not the exclusive responsibility
    of researchers and data scientists. Yes, their work on training the algorithms
    is crucial because they define the model architecture and the training plan. But
    just like physicists need a software system to control the electron-positron collider
    to test their particle theories, data scientists need an effective software system
    to manage the expensive computation resources, such as GPU, CPU, and memory, to
    execute the training code. This system of managing compute resources and executing
    training code is known as the *model training service*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中模型训练的任务不是研究人员和数据科学家的专属责任。是的，他们在训练算法上的工作是至关重要的，因为他们定义了模型架构和训练计划。但就像物理学家需要软件系统来控制电子-正电子对撞机以测试他们的粒子理论一样，数据科学家需要有效的软件系统来管理昂贵的计算资源，如GPU、CPU和内存，以执行训练代码。这种管理计算资源和执行训练代码的系统被称为**模型训练服务**。
- en: Building a high-quality model depends not only on the training algorithm but
    also on the compute resources and the system that executes the training. A good
    training service can make model training much faster and more reliable and can
    also reduce the average model-building cost. When the dataset or model architecture
    is massive, using a training service to manage the distributed computation is
    your only option.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 构建高质量模型不仅取决于训练算法，还取决于执行训练的计算资源和系统。一个好的训练服务可以使模型训练更快、更可靠，并且还可以降低平均模型构建成本。当数据集或模型架构巨大时，使用训练服务来管理分布式计算是您的唯一选择。
- en: In this chapter, we first examine the training service’s value proposition and
    design principles, and then we meet our sample training service. This sample service
    not only shows you how to apply the design principles in practice but also teaches
    you how the training service interacts with arbitrary training code. Next, we
    introduce several open source training applications that you can use to set up
    your own training service quickly. We end with a discussion on when to use a public
    cloud training system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先检查训练服务的价值主张和设计原则，然后介绍我们的示例训练服务。这个示例服务不仅向您展示了如何在实践中应用设计原则，还向您展示了训练服务如何与任意训练代码交互。接下来，我们介绍了几种开源训练应用程序，您可以使用它们快速设置自己的训练服务。最后，我们讨论了何时使用公共云训练系统。
- en: This chapter focuses on designing and building effective training services *from
    a software engineer’s perspective, not a data scientist’s*. So we don’t expect
    you to be familiar with any deep learning theories or frameworks. Section 3.2,
    on the deep learning algorithm code pattern, is all the preparation you need to
    understand the training code in this chapter. The training code is not our main
    focus here; we wrote it only for demonstration purposes, so we have something
    on which to demonstrate the sample training service.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从软件工程师的角度，而不是数据科学家的角度，专注于设计和构建有效的训练服务。因此，我们并不期望您熟悉任何深度学习理论或框架。第3.2节关于深度学习算法代码模式，是您理解本章中训练代码所需的所有准备。训练代码不是我们这里的主要焦点；我们只编写它用于演示目的，这样我们就有东西可以用来演示示例训练服务。
- en: 'Model training topics often intimidate engineers. One common misunderstanding
    is that model training is all about training algorithms and research. By reading
    this chapter, I hope you will not only learn how to design and build training
    services but also absorb this message: the success of model training is built
    on two pillars, algorithms and system engineering. The model training activities
    in an organization cannot scale without a good training system. So we, as software
    engineers, have a lot to contribute to this field.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的话题往往令工程师感到畏惧。一个常见的误解是模型训练完全是关于训练算法和研究。通过阅读本章，我希望您不仅能够学习如何设计和构建训练服务，还能够吸收这个信息：模型训练的成功建立在两个支柱上，即算法和系统工程。没有良好的训练系统，组织的模型训练活动无法扩展。因此，作为软件工程师，我们有很多可以贡献这个领域。
- en: '3.1 Model training service: Design overview'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 模型训练服务：设计概述
- en: 'In an enterprise environment, there are two roles involved in deep learning
    model training: data scientists, who develop model training algorithms (in TensorFlow,
    PyTorch, or other frameworks), and platform engineers, who build and maintain
    the system that runs the model training code in remote and shared server farms.
    We call this system the model training service.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中，涉及深度学习模型训练有两个角色：数据科学家，他们开发模型训练算法（在TensorFlow、PyTorch或其他框架中），以及平台工程师，他们构建和维护运行模型训练代码的远程和共享服务器农场系统。我们将这个系统称为模型训练服务。
- en: A model training service works as a training infrastructure to execute the model
    training code (algorithm) in a dedicated environment; it handles both training
    job scheduling and compute resource management. Figure 3.1 shows a high-level
    workflow in which the model training service runs a model training code to produce
    a model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练服务作为一个训练基础设施，在专用环境中执行模型训练代码（算法）；它处理训练作业调度和计算资源管理。图3.1显示了模型训练服务运行模型训练代码以生成模型的高级工作流程。
- en: '![](../Images/03-01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-01.png)'
- en: Figure 3.1 A high-level workflow for executing model training via a training
    service. In step 1, the data scientist submits a training request with training
    code to the training service, which creates a job in the job queue. In step 2,
    the model training service allocates compute resources to execute the training
    job (training code). In step 3, the job produces a model when the training execution
    completes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 通过训练服务执行模型训练的高级工作流程。在第1步，数据科学家将训练代码提交给训练服务，服务在作业队列中创建一个作业。在第2步，模型训练服务分配计算资源以执行训练作业（训练代码）。在第3步，训练执行完成后，作业生成一个模型。
- en: The most common question asked about this component is why we would need to
    write a service to do model training. For many people, it seems much easier to
    write a simple bash script to execute the training code (algorithm) locally or
    remotely, such as with an Amazon Elastic Cloud Computing (Amazon EC2) instance.
    The rationale behind building a training service, however, goes beyond just launching
    a training computation. We will discuss it in detail in the next section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个组件最常见的问题是为什么我们需要编写一个服务来进行模型训练。对许多人来说，似乎编写一个简单的bash脚本来本地或远程执行训练代码（算法）要容易得多，例如使用亚马逊弹性云计算（Amazon
    EC2）实例。然而，构建训练服务的理由并不仅仅是为了启动训练计算。我们将在下一节中详细讨论。
- en: 3.1.1 Why use a service for model training?
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 为什么使用服务进行模型训练？
- en: 'Imagine you lead a data science team, and you need to assign the team’s precious
    compute resources wisely to the team members Alex, Bob, and Kevin. The computing
    resource needs to be allocated in a way that all team members can complete their
    model training tasks within a time limit and a budget. Figure 3.2 paints two approaches
    for allocating the compute resources: dedicated and shared.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你领导着一个数据科学团队，需要明智地将团队宝贵的计算资源分配给团队成员Alex、Bob和Kevin。计算资源需要以这种方式分配，即所有团队成员都能在时间限制和预算范围内完成他们的模型训练任务。图3.2展示了两种分配计算资源的方法：专用和共享。
- en: '![](../Images/03-02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-02.png)'
- en: 'Figure 3.2 Different compute resource allocation strategies: dedicated vs.
    shared'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 不同的计算资源分配策略：专用与共享
- en: The first option, dedicated, is to exclusively assign a powerful workstation
    to each member of the team. This is the simplest approach but clearly not an economic
    one because when Alex is not running his training code, his server sits idle and
    neither Bob nor Kevin can use it. So, in this approach, our budget is underutilized.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种选择，专用，是为团队中的每个成员独家分配一台强大的工作站。这是一种最简单的方法，但显然不是经济的方法，因为当Alex没有运行他的训练代码时，他的服务器处于闲置状态，Bob和Kevin也无法使用它。因此，在这种方法中，我们的预算没有得到充分利用。
- en: Another problem with the dedicated approach is that it cannot scale. When Alex
    wants to train a large model or a model with a large dataset, he will need multiple
    machines. And training machines are normally expensive; because of the complexity
    of the deep learning model architecture, even a decent size neural network requires
    a GPU with large memory. In this case, we must assign more dedicated servers to
    Alex, which exacerbates the inefficient resource allocation problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 专用方法的另一个问题是它无法扩展。当Alex想要训练一个大型模型或具有大量数据集的模型时，他需要多台机器。训练机器通常很昂贵；由于深度学习模型架构的复杂性，即使是相当大小的神经网络也需要具有大内存的GPU。在这种情况下，我们必须为Alex分配更多的专用服务器，这加剧了资源分配效率低下的问题。
- en: The second option, shared compute resources, is to build an elastic server group
    (the size of the group is adjustable) and share it with all members. This approach
    is obviously more economical because we use fewer servers to achieve the same
    result, which maximizes our resource utilization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择，共享计算资源，是构建一个可伸缩的服务器组（组的大小可调整）并与所有成员共享。这种方法显然更经济，因为我们使用更少的服务器就能达到相同的效果，从而最大化我们的资源利用率。
- en: It’s not a hard decision to choose a sharing strategy because it greatly reduces
    the cost of our training cluster. But the sharing approach requires proper management,
    such as queuing user requests if there is a sudden burst of training requests,
    babysitting each training execution and intervening (restarting or aborting) when
    necessary (training progress is stuck), and scaling up or scaling down our cluster
    according to the real-time system usage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 选择共享策略并不困难，因为它大大降低了我们训练集群的成本。但是，共享方法需要适当的管理，例如，在训练请求突然激增时排队用户请求，对每个训练执行进行监护并在必要时干预（重启或终止），以及根据实时系统使用情况调整集群的规模。
- en: Script vs. service
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本与服务的比较
- en: Now let’s revisit the previous script versus service discussion. In a model
    training context, *training script* refers to using shell scripts to orchestrate
    different training activities in a shared server cluster. A training service is
    a remote process that communicates over the network using HTTP (hypertext transfer
    protocol) or gRPC (gRPC Remote Procedure Call). As data scientists, Alex and Bob
    send training requests to the service, and the service orchestrates these requests
    and manages the training executions on the shared servers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下之前的脚本与服务的讨论。在模型训练的背景下，*训练脚本*指的是使用shell脚本在共享服务器集群中编排不同的训练活动。训练服务是一个通过网络使用HTTP（超文本传输协议）或gRPC（gRPC远程过程调用）进行通信的远程进程。作为数据科学家，亚历克斯和鲍勃向服务发送训练请求，服务编排这些请求并管理在共享服务器上的训练执行。
- en: The script approach may work for a single-person scenario but will prove difficult
    in a shared-resource environment. Besides executing training code, we need to
    take care of other important elements, such as setting up the environment, ensuring
    data compliance, and troubleshooting model performance. For example, environment
    setup requires that the library dependencies of the training framework and training
    code are installed properly on the training server before starting model training.
    Data compliance requires that the sensitive training data (user credit card numbers,
    payment records) is protected with restricted access. And performance troubleshooting
    requires that everything used in training, including dataset IDs and versions,
    training code versions, and hyperparameters, is tracked for model reproduction
    purposes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本方法可能适用于单个人场景，但在资源共享环境中将证明是困难的。除了执行训练代码外，我们还需要关注其他重要元素，例如设置环境、确保数据合规性以及排除模型性能故障。例如，环境设置要求在开始模型训练之前，在训练服务器上正确安装训练框架和训练代码的库依赖项。数据合规性要求对敏感的训练数据（如用户信用卡号码、支付记录）进行限制访问。性能故障排除要求跟踪训练中使用的所有内容，包括数据集ID和版本、训练代码版本和超参数，以实现模型复现。
- en: It’s hard to imagine addressing these requirements in shell scripts and having
    the model training executed in a reliable, repeatable, and scalable fashion. This
    is why most models trained in production nowadays are produced by thoughtfully
    designed model training services.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 很难想象在shell脚本中满足这些要求，并且以可靠、可重复和可扩展的方式执行模型训练。这就是为什么现在大多数在生产中训练的模型都是由精心设计的模型训练服务产生的。
- en: Benefits of having a model training service
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有模型训练服务的优势
- en: 'From the previous discussion, we can imagine a model training service’s value
    proposition as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的讨论中，我们可以想象模型训练服务的价值主张如下：
- en: Saturates computing resources and reduces model training costs
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 充分利用计算资源并降低模型训练成本
- en: Expedites model development by building models in a fast (more resources available)
    and reliable way
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过快速（资源更多）和可靠的方式构建模型，加速模型开发
- en: Enforces data compliance by executing training in a confined environment
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在受限环境中执行训练来强制执行数据合规性
- en: Facilitates model performance troubleshooting
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 促进模型性能故障排除
- en: 3.1.2 Training service design principles
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 训练服务设计原则
- en: Before we look at our sample training service, let’s look at the four design
    principles we can use to evaluate a model training system.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看示例训练服务之前，让我们看看我们可以用来评估模型训练系统的四个设计原则。
- en: 'Principle 1: Provides a unified API and is agnostic about actual training code'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 1：提供统一的 API，对实际的训练代码无偏见
- en: Having only one public API to train models with different kinds of training
    algorithms makes the training service easy to use. Whether it’s object detection
    training, voice recognition training, or text-intent classification training,
    we can use sample APIs to trigger the model training execution. Future algorithm
    performance A/B tests can also be easily implemented by having a single training
    API.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个公共 API 用于训练不同类型的训练算法，使得训练服务易于使用。无论是目标检测训练、语音识别训练还是文本意图分类训练，我们都可以使用示例 API
    触发模型训练执行。通过拥有单个训练 API，未来算法性能的 A/B 测试也可以轻松实现。
- en: Training code-agnostic means that the training service defines a clear mechanism
    or protocol for how it executes a training algorithm (code). It establishes, for
    instance, how the service passes in variables to the training code/process, how
    the training code obtains the training dataset, and where the trained model and
    metrics are uploaded. As long as training code follows this protocol, it doesn’t
    matter how it’s implemented, what its model architecture is, or which training
    libraries it uses.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码无偏见意味着训练服务定义了一个明确的机制或协议，用于如何执行训练算法（代码）。例如，它规定了服务如何将变量传递给训练代码/过程，训练代码如何获取训练数据集，以及训练模型和指标上传的位置。只要训练代码遵循此协议，其实现方式、模型架构或使用的训练库就无关紧要。
- en: 'Principle 2: Builds a model with high performance and low costs'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 2：构建高性能且低成本的模型
- en: A good training service should set cost-effectiveness as a priority. Cost-effectiveness
    can provide methods to shorten the model training execution time and improve the
    utilization rate of the compute resources. For instance, a modern training service
    can reduce time and hardware costs by supporting various distributed training
    methods, offering good job-schedule management to saturate the server farm, and
    alerting users when a training process goes off the original plan so it can be
    terminated early.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的训练服务应该将成本效益作为首要任务。成本效益可以提供缩短模型训练执行时间和提高计算资源利用率的方法。例如，一个现代训练服务可以通过支持各种分布式训练方法、提供良好的作业调度管理以充分利用服务器农场，以及在训练过程偏离原计划时提醒用户以便提前终止，从而减少时间和硬件成本。
- en: 'Principle 3: Supports model reproducibility'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 3：支持模型可重复性
- en: A service should produce the same model if given the same inputs. This is not
    only important for debugging and performance troubleshooting but also builds trustworthiness
    in the system. Remember, we will build business logic based on the model prediction
    result. We might, for instance, employ a classification model to predict a user’s
    credibility and then make loan-approval decisions based on it. We can’t trust
    the entire loan-approval application unless we can repeatedly produce models of
    the same quality.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给定相同的输入，服务应该生成相同的模型。这不仅对调试和性能故障排除很重要，而且有助于建立系统的可信度。记住，我们将基于模型预测结果构建业务逻辑。例如，我们可能会使用分类模型来预测用户的信誉，然后据此做出贷款批准决定。除非我们能够反复生成相同质量的模型，否则我们无法信任整个贷款批准应用程序。
- en: 'Principle 4: Supports robust, isolated, and elastic compute management'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 4：支持稳健、隔离和弹性的计算管理
- en: Modern deep learning models, such as language understanding models, take a long
    time to train (more than a week). If the training process is interrupted or gets
    aborted in the middle for some random OS failures, all the time and computation
    expenses are wasted. A matured training service should handle the training job
    robustness (failover, failure recovery), resource isolation, and elastic resource
    management (ability to adjust the number of resources), so it can make sure its
    training job execution will complete successfully in a variety of situations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习模型，如语言理解模型，训练时间较长（超过一周）。如果训练过程被中断或因某些随机的操作系统故障而提前终止，所有的时间和计算费用都将浪费。一个成熟的训练服务应该处理训练作业的稳健性（故障转移、故障恢复）、资源隔离和弹性资源管理（调整资源数量的能力），以确保在各种情况下其训练作业执行都能成功完成。
- en: After discussing all the important abstract concepts, let’s tackle how to design
    and build a model training service. In the next two sections, we will learn a
    general code pattern for deep learning code and an example of a model training
    service.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了所有重要的抽象概念之后，让我们来探讨如何设计和构建模型训练服务。在接下来的两个部分中，我们将学习深度学习代码的一般代码模式和模型训练服务的示例。
- en: 3.2 Deep learning training code pattern
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 深度学习训练代码模式
- en: Deep learning algorithms can be complicated and often intimidating for engineers.
    Fortunately, as software engineers designing the platform for deep learning systems,
    we don’t need to master these algorithms for our daily work. We do, however, need
    to be familiar with the general code pattern of these algorithms. With a high-level
    understanding of the model training code pattern, we can comfortably treat model
    training code as a black box. In this section, we’ll introduce you to the general
    pattern.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法可能对工程师来说很复杂，常常令人感到畏惧。幸运的是，作为设计深度学习系统平台的软件工程师，我们不需要掌握这些算法来处理我们的日常工作。然而，我们确实需要熟悉这些算法的一般代码模式。通过高层次地理解模型训练代码模式，我们可以轻松地将模型训练代码视为一个黑盒。在本节中，我们将向您介绍一般模式。
- en: 3.2.1 Model training workflow
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 模型训练工作流程
- en: In a nutshell, most deep learning models are trained through an iterative learning
    process. The process repeats the same set of computation steps in many iterations,
    and in every iteration, it tries to update the weights and biases of the neural
    network to get the algorithm output (prediction result) closer to the training
    targets in the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，大多数深度学习模型都是通过迭代学习过程进行训练的。这个过程在许多迭代中重复相同的计算步骤，并且在每次迭代中，它都试图更新神经网络的权重和偏差，以使算法输出（预测结果）更接近数据集中的训练目标。
- en: To measure how well a neural network models the given data and uses it to update
    the weights of the neural network to get better results, a loss function is defined
    to calculate the deviations of the algorithm output from the actual results. The
    output of the loss function is named LOSS.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量神经网络模型对给定数据的建模能力以及它如何使用这些数据来更新神经网络的权重以获得更好的结果，定义了一个损失函数来计算算法输出与实际结果之间的偏差。损失函数的输出被称为LOSS。
- en: So, you can see the entire iterative training process as a repeating effort
    to reduce the loss value. Eventually, when the loss value meets our training goal
    or it can’t be reduced any further, then the training completes. The training
    output is the neural network and its weights, but we generally refer to it simply
    as a model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以将整个迭代训练过程视为一个重复的努力，以减少损失值。最终，当损失值达到我们的训练目标或无法进一步降低时，训练完成。训练输出是神经网络及其权重，但通常我们简单地将其称为模型。
- en: '![](../Images/03-03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-03.png)'
- en: Figure 3.3 General steps of a deep learning model training workflow
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 深度学习模型训练工作流程的一般步骤
- en: 'Figure 3.3 illustrates the general model training steps. Because neural networks
    cannot load the entire dataset at once due to memory limitations, we usually regroup
    the dataset into small batches (mini-batches) before training begins. In step
    1, the mini-batch examples are fed to the neural network, and the network calculates
    the prediction result for each example. In step 2, we pass in the predicted results
    and the expected value (training labels) to the loss function to compute the loss
    value, which indicates the deviation between the current learning and the target
    data pattern. In step 3, a process called backpropagation calculates gradients
    for each of the neural network’s parameters with the loss value. These gradients
    are used to update the model parameters, so the model can get a better prediction
    accuracy in the next training loop. In step 4, The neural network’s parameters
    (weights and biases) are updated by a selected optimization algorithm, such as
    stochastic gradient descent and its variants. The gradients (from step 3) and
    learning rate are the input parameters for the optimization algorithm. The model
    accuracy is supposed to improve after this model update step. Finally, in step
    5, training completes and the network and its parameters are saved as the final
    model file. The training is completed under either of the two following conditions:
    finishing the expected training runs or reaching the expected model accuracy.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3展示了通用模型训练步骤。由于神经网络受限于内存，不能一次性加载整个数据集，因此在训练开始前，我们通常将数据集重新分组为小批量（迷你批量）。在步骤1中，迷你批量示例被输入到神经网络中，网络为每个示例计算预测结果。在步骤2中，我们将预测结果和期望值（训练标签）传递给损失函数，以计算损失值，该值表示当前学习与目标数据模式之间的偏差。在步骤3中，一个称为反向传播的过程计算神经网络每个参数的梯度，这些梯度用于更新模型参数，从而使模型在下一个训练循环中获得更好的预测精度。在步骤4中，神经网络的参数（权重和偏差）通过选定的优化算法（如随机梯度下降及其变体）进行更新。步骤3中的梯度和学习率是优化算法的输入参数。模型精度预计在模型更新步骤之后会提高。最后，在步骤5中，训练完成，网络及其参数被保存为最终的模型文件。训练在以下两种条件之一满足时完成：完成预期的训练运行或达到预期的模型精度。
- en: Although there are different types of model architectures, including recurrent
    neural networks (RNNs), convolutional neural networks (CNNs), and autoencoders,
    their model training processes all follow this same pattern; only the model network
    differs. Also, abstracting model training code to the previously repeated general
    steps is the foundation for running distributed training. This is because, no
    matter how the model architecture is different, we can train them in a common
    training strategy. We will discuss distributed training in detail in the next
    chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在不同类型的模型架构，包括循环神经网络（RNNs）、卷积神经网络（CNNs）和自编码器，但它们的模型训练过程都遵循相同的模式；只是模型网络不同。此外，将模型训练代码抽象为之前重复的一般步骤是运行分布式训练的基础。这是因为，无论模型架构如何不同，我们都可以使用共同的训练策略来训练它们。我们将在下一章详细讨论分布式训练。
- en: 3.2.2 Dockerize model training code as a black box
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 将模型训练代码Docker化作为黑盒
- en: With the previously discussed training pattern in mind, we can view deep learning
    training code as a black box. No matter what model architecture and training algorithm
    a training code implements, we can execute it the same way in a training service.
    To run the training code anywhere in the training cluster and create isolation
    for each training execution, we can pack the training code and its dependent libraries
    into a Docker image and run it as a container (figure 3.4).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑之前讨论的训练模式时，我们可以将深度学习训练代码视为黑盒。无论训练代码实现什么模型架构和训练算法，我们都可以在训练服务中以相同的方式执行它。为了在任何训练集群中运行训练代码并创建每个训练执行的隔离，我们可以将训练代码及其依赖库打包成一个Docker镜像，并以容器形式运行它（图3.4）。
- en: '![](../Images/03-04.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-04.png)'
- en: Figure 3.4 A training service launches a Docker container to perform model training
    instead of running the training code directly as a process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 训练服务启动一个Docker容器来执行模型训练，而不是直接作为进程运行训练代码。
- en: In figure 3.4, by Dockerizing the training code, the training service can execute
    a model training by simply launching a Docker container. Because the service is
    agnostic about what’s inside the container, the training service can execute all
    different code in this standard method. This is much simpler than letting the
    training service spawn a process to execute model training because the training
    service needs to set up the various environments and dependent packages for each
    training code. Another benefit of Dockerization is it decouples the training service
    and training code, which enables data scientists and platform engineers to focus
    on model algorithm development and training execution performance, respectively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.4中，通过将训练代码Docker化，训练服务可以通过简单地启动一个Docker容器来执行模型训练。因为服务对容器内部的内容一无所知，所以训练服务可以使用这种方法执行所有不同的代码。这比让训练服务生成一个进程来执行模型训练要简单得多，因为训练服务需要为每个训练代码设置各种环境和依赖包。Docker化的另一个好处是它解耦了训练服务和训练代码，这使得数据科学家和平台工程师可以分别专注于模型算法开发和训练执行性能。
- en: You may wonder how a training service communicates with training code if they
    are agnostic to each other. The key is to define a communication protocol; this
    protocol delineates which parameters and their data format a training service
    passes to training code. These parameters include dataset, hyperparameters, model
    saving location, metrics saving location, and more. We will see a concrete example
    in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，如果训练服务与训练代码之间是无关的，它们是如何进行通信的。关键在于定义一个通信协议；这个协议规定了训练服务传递给训练代码的哪些参数及其数据格式。这些参数包括数据集、超参数、模型保存位置、指标保存位置等。我们将在下一节看到一个具体的例子。
- en: 3.3 A sample model training service
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 一个示例模型训练服务
- en: As we now know, most deep learning training codes follow the same pattern (figure
    3.3), and they can be Dockerized and executed in a unified fashion (figure 3.4).
    Let’s take a closer look at a concrete example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，大多数深度学习训练代码遵循相同的模式（图3.3），并且可以以统一的方式Docker化并执行（图3.4）。让我们更仔细地看看一个具体的例子。
- en: To demonstrate the concept and design principles we introduced so far, we built
    a sample service that implements the basic production scenarios of model training—receiving
    a training request, launching a training execution in a Docker container, and
    tracking its execution progress. Although the scenarios are quite simple—a few
    hundred lines of code—they demonstrate the key concepts we discussed in previous
    sections, including using a unified API, Dockerized training code, and communication
    protocol between the training service and training container.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们迄今为止介绍的概念和设计原则，我们构建了一个示例服务，该服务实现了模型训练的基本生产场景——接收训练请求、在Docker容器中启动训练执行，并跟踪其执行进度。尽管场景非常简单——只有几百行代码——但它们展示了我们在前几节讨论的关键概念，包括使用统一的API、Docker化的训练代码以及训练服务与训练容器之间的通信协议。
- en: 'Note To show the key pieces clearly, the service is built in a slim fashion.
    Model training metadata (such as running jobs and waiting jobs) is tracked in
    memory instead of a database, and the training jobs are executed in the local
    Docker engine directly. By removing lots of intermediate layers, you will have
    a straight view of two key areas: training job management and the communication
    between the training service and training code (Docker container).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了清晰地展示关键部分，该服务以精简的方式构建。模型训练元数据（如正在运行的工作和等待的工作）在内存中跟踪，而不是数据库中，并且训练作业直接在本地Docker引擎中执行。通过移除许多中间层，你将直接看到两个关键区域：训练作业管理和训练服务与训练代码（Docker容器）之间的通信。
- en: 3.3.1 Play with the service
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 与服务互动
- en: Before we look at service design and implementation, let’s see how we can play
    with it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看服务设计和实现之前，让我们看看我们如何与之互动。
- en: Note Please follow the GitHub instructions to run this lab. We highlight only
    the major steps and key commands for how to run the sample service to avoid the
    lengthy pages of code and execution outputs, so the concept can be demonstrated
    clearly. To run this lab, follow the instructions in the “single trainer demo”
    doc (`training-service/single_trainer_demo.md`) in the orca3/MiniAutoML Git repository,
    which also captures the desired outputs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请按照GitHub的说明运行此实验。我们仅突出显示运行示例服务的主要步骤和关键命令，以避免代码和执行输出的冗长页面，从而清晰地展示概念。要运行此实验，请遵循orca3/MiniAutoML
    Git仓库中“single trainer demo”文档（`training-service/single_trainer_demo.md`）中的说明，该文档也捕获了期望的输出。
- en: 'First, we start the service with `scripts/ts-001-start-server.sh`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `scripts/ts-001-start-server.sh` 启动服务：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After launching the training service Docker container, we can send a gRPC request
    to kick off a model training execution (`scripts/ts-002-start-run.sh` `<dataset`
    `id>`). See a sample gRPC request as follows.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 启动训练服务 Docker 容器后，我们可以发送一个 gRPC 请求来启动模型训练执行（`scripts/ts-002-start-run.sh` `<dataset
    id>`）。以下是一个示例 gRPC 请求。
- en: 'Listing 3.1 Calling training service API: Submitting a training job'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 调用训练服务 API：提交训练作业
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Training algorithm; also the name of the training Docker image
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练算法；也是训练 Docker 镜像的名称
- en: ❷ Version hash of the training dataset
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练数据集的版本哈希
- en: ❸ Training hyperparameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练超参数
- en: 'Once the job is submitted successfully, we can use the returned job ID from
    the `train` API to query the progress of the training execution (`scripts/ts-003-check-run.sh`
    `<job` `id>`); see the following example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦作业成功提交，我们可以使用 `train` API 返回的作业 ID 来查询训练执行的进度（`scripts/ts-003-check-run.sh`
    `<job id>`）；以下是一个示例：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Uses the job ID returned by the train API
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 train API 返回的作业 ID
- en: As you can see, by calling two gRPC APIs, we can kick off deep learning training
    and track its progress. Now, let’s look at the design and implementation of this
    sample training service.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，通过调用两个 gRPC API，我们可以启动深度学习训练并跟踪其进度。现在，让我们看看这个示例训练服务的设计和实现。
- en: Note Check out appendix A if you encounter any problems. Scripts in section
    A.2 automate both dataset preparation and model training. If you want to see a
    working model training example, read the lab portion of that section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果在附录 A 中遇到任何问题，请查看附录 A。附录 A 中的脚本（A.2 节）自动化了数据集准备和模型训练。如果您想看到一个正在进行的模型训练示例，请阅读该节中的实验部分。
- en: 3.3.2 Service design overview
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 服务设计概述
- en: Let’s use Alex (a data scientist) and Tang (a developer) to show how the service
    functions. To use the training service to train a model, Alex needs to write training
    code (for example, a neural network algorithm) and build the code into a Docker
    image. This Docker image needs to be published to an artifact repository, so the
    training service can pull the image and run it as a container. Inside the Docker
    container, the training code will be executed by a bash script.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Alex（一位数据科学家）和 Tang（一位开发者）来展示服务功能。为了使用训练服务训练一个模型，Alex 需要编写训练代码（例如，神经网络算法）并将代码构建成一个
    Docker 镜像。这个 Docker 镜像需要发布到一个工件仓库中，以便训练服务可以拉取镜像并作为容器运行。在 Docker 容器内部，训练代码将通过一个
    bash 脚本执行。
- en: To provide an example, we wrote a sample intent classification training code
    in PyTorch, built the code into a Docker image, and pushed it to the Docker hub
    ([https://hub.docker.com/u/orca3](https://hub.docker.com/u/orca3)). We will explain
    it again in section 3.3.6.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个例子，我们编写了一个示例意图分类训练代码，使用 PyTorch 构建，并将其打包成一个 Docker 镜像，然后推送到 Docker Hub
    ([https://hub.docker.com/u/orca3](https://hub.docker.com/u/orca3))。我们将在 3.3.6
    节中再次解释它。
- en: 'note In real-world scenarios, the training Docker image creation, publication,
    and consumption are done automatically. A sample scenario could be as follows:
    step 1, Alex checks his training code into a Git repository; step 2, a preconfigured
    program—for example, a Jenkins pipeline—is triggered to build a Docker image from
    this repo; step 3, the pipeline also publishes the Docker image to a Docker image
    artifactory, for example, JFrog Artifactory; and step 4, Alex sends a training
    request, and then the training service pulls the training images from the artifactory
    and begins model training.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: note：在实际场景中，训练 Docker 镜像的创建、发布和消费是自动完成的。一个示例场景可能如下：步骤 1，Alex 将其训练代码提交到 Git 仓库；步骤
    2，一个预配置的程序（例如，Jenkins 管道）被触发，从这个仓库构建 Docker 镜像；步骤 3，该管道还将 Docker 镜像发布到 Docker
    镜像工件库，例如 JFrog Artifactory；步骤 4，Alex 发送训练请求，然后训练服务从工件库中拉取训练镜像并开始模型训练。
- en: 'When Alex finishes the training code development, he can start to use the service
    to run his training code. The entire workflow is as follows: step 1.a, Alex submits
    a training request to our sample training service. The request defines the training
    code—a Docker image and tag. When the training service receives a training request,
    it creates a job in the queue and returns the job ID to Alex for future job tracking;
    step 1.b, Alex can query the training service to get the training progress in
    realtime; step 2, the service launches a training job as a Docker container in
    the local Docker engine to execute the model training; and step 3, the training
    code in the Docker container uploads training metrics to the metadata store during
    training and the final model when training completes.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当Alex完成培训代码开发后，他可以开始使用该服务运行他的培训代码。整个工作流程如下：步骤1.a，Alex向我们的示例培训服务提交培训请求。请求定义了培训代码——Docker镜像和标签。当培训服务收到培训请求时，它在队列中创建一个作业并将作业ID返回给Alex以供未来作业跟踪；步骤1.b，Alex可以查询培训服务以实时获取培训进度；步骤2，服务在本地Docker引擎中以Docker容器的形式启动培训作业以执行模型训练；步骤3，在训练过程中，Docker容器将训练指标上传到元数据存储，并在训练完成后上传最终模型。
- en: Note Model evaluation is the step we did not mention in the aforementioned model
    training workflow. After the model is trained, Alex (data scientist) will look
    at the training metrics, reported by the training service, to validate the model's
    quality. To evaluate the model quality, Alex can check the prediction failure
    rate, gradients, and loss-value graphs. As model evaluation is usually a data
    scientist’s responsibility, we will not cover it in this book, but we will discuss
    in chapter 8 how model training metrics are collected and stored.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：模型评估是我们之前提到的模型训练工作流程中没有提到的步骤。模型训练完成后，Alex（数据科学家）将查看培训服务报告的训练指标，以验证模型的质量。为了评估模型质量，Alex可以检查预测失败率、梯度和损失值图表。由于模型评估通常是数据科学家的责任，我们不会在本章中涵盖它，但我们将讨论第8章中如何收集和存储模型训练指标。
- en: The entire training workflow is self-serve; Alex can manage the model training
    entirely by himself. Tang develops the training service and maintains the system,
    but the system is agnostic about the training code developed by Alex. Tang’s focus
    is not the model’s accuracy but the availability and efficiency of the system.
    See figure 3.5 for the user workflow we just described.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 整个培训工作流程是自助服务；Alex可以完全自行管理模型训练。Tang开发了培训服务并维护系统，但系统对Alex开发的培训代码是中立的。Tang的关注点不是模型的准确性，而是系统的可用性和效率。请参阅图3.5，了解我们刚才描述的用户工作流程。
- en: '![](../Images/03-05.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5](../Images/03-05.png)'
- en: 'Figure 3.5 A high-level service design and user workflow: user training requests
    are queued, and the Docker job tracker picks up jobs from the queue and launches
    Docker containers to run model training.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 高级服务设计和用户工作流程：用户培训请求被排队，Docker作业跟踪器从队列中提取作业并启动Docker容器以运行模型训练。
- en: 'Having seen the user workflow, let’s look at two key components: memory store
    and Docker job tracker. The memory store uses the following four data structures
    (maps) to organize requests (jobs): job queue, job launch list, running list,
    and finalized list. Each of these maps represents jobs in a different running
    status. We implement the job tracking store in memory just for simplicity; ideally,
    we should use a database.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了用户工作流程后，让我们看看两个关键组件：内存存储和Docker作业跟踪器。内存存储使用以下四个数据结构（映射）来组织请求（作业）：作业队列、作业启动列表、运行列表和完成列表。这些映射中的每一个都代表不同运行状态下的作业。我们仅为了简单起见在内存中实现了作业跟踪存储；理想情况下，我们应该使用数据库。
- en: The Docker job tracker handles the actual job execution in the Docker engine;
    it periodically monitors the job queue in the memory store. When there is capacity
    in the Docker engine, the tracker will launch a Docker container from the job
    queue and keep monitoring the container execution. In our example, we use the
    local Docker engine, so the service can run on your local. But it can be easily
    configured to a remote Docker engine as well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Docker作业跟踪器处理Docker引擎中的实际作业执行；它定期监控内存存储中的作业队列。当Docker引擎有容量时，跟踪器将从作业队列中启动一个Docker容器并持续监控容器执行。在我们的示例中，我们使用本地Docker引擎，因此服务可以在您的本地运行。但它也可以很容易地配置为远程Docker引擎。
- en: After launching a training container, based on the execution status, the Docker
    job tracker moves the job object from the job queue to other job lists, such as
    the job launching list, running list, and `finalizedJobs` list. In section 3.4.4,
    we will discuss this process in detail.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 启动训练容器后，根据执行状态，Docker作业跟踪器将作业对象从作业队列移动到其他作业列表，例如作业启动列表、运行列表和`finalizedJobs`列表。在3.4.4节中，我们将详细讨论此过程。
- en: Note Consider that a dataset will be split in a training container (at training
    time). It is valid to split datasets during dataset building or model training,
    and both processes have pros and cons. But either way, it will not affect the
    design of the training service significantly. For simplicity, we assume in this
    sample training service that the algorithm code will split the dataset into train,
    validate, and test subsets.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：考虑在训练容器（在训练时间）中数据集将被分割。在数据集构建或模型训练期间分割数据集是有效的，并且这两个过程都有优点和缺点。但无论如何，它不会显著影响训练服务的设计。为了简单起见，我们假设在这个示例训练服务中，算法代码将数据集分割成训练、验证和测试子集。
- en: 3.3.3 Training service API
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 训练服务API
- en: 'Having seen the overview, let’s dive into the public gRPC APIs (`grpc-contract/
    src/main/proto/training_service.proto`) to gain a deeper understanding of the
    service. There are two APIs in the training service: `Train` and `GetTrainingStatus`.
    The `Train` API is for submitting training requests, and the `GetTrainingStatus`
    API is for fetching the training execution status. See the API definition in the
    following listing.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了概述之后，让我们深入了解公共gRPC API（`grpc-contract/src/main/proto/training_service.proto`），以获得对服务的更深入理解。训练服务中有两个API：`Train`和`GetTrainingStatus`。`Train`
    API用于提交训练请求，而`GetTrainingStatus` API用于获取训练执行状态。请参见以下列表中的API定义。
- en: Listing 3.2 A model training service gRPC interface
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2 模型训练服务gRPC接口
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Defines the dataset, training algorithm, and extra parameters for the model-building
    request
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了模型构建请求的数据集、训练算法和额外参数
- en: 'From the gRPC interface in listing 3.2, to use the `Train` API, we need to
    provide the following information as `TrainingJobMetadata`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表3.2中的gRPC接口，要使用`Train` API，我们需要提供以下信息作为`TrainingJobMetadata`：
- en: '`dataset_id`—The dataset ID in the dataset management service'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_id`—数据集管理服务中的数据集ID'
- en: '`train_data_version_hash`—The hash version of the dataset that is used in training'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_data_version_hash`—用于训练的数据集的哈希版本'
- en: '`name`—Training job name'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`—训练作业名称'
- en: '`algorithm`—Specify which training algorithm to use to train the dataset. This
    algorithm string needs to be one of our predefined algorithms. Internally, the
    training service will find the Docker image associated with this algorithm to
    execute training.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`algorithm`—指定用于训练数据集的训练算法。此算法字符串需要是我们预定义算法之一。内部，训练服务将找到与该算法关联的Docker镜像以执行训练。'
- en: '`parameters`—Training hyperparameters that we pass directly to the training
    container, such as number of epochs, batch size, etc.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parameters`—直接传递给训练容器的训练超参数，例如epoch数量、批量大小等。'
- en: Once the `Train` API receives a training request, the service puts the request
    to the job queue and returns an ID (`job_id`) for the caller to reference the
    job. This `job_id` can be used with the `GetTrainingStatus` API to check the training
    status. Now that we have seen the API definitions, let’s look at their implementations
    in the next two sections.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`Train` API收到训练请求，服务将请求放入作业队列，并为调用者返回一个ID（`job_id`）以供引用作业。此`job_id`可以与`GetTrainingStatus`
    API一起使用来检查训练状态。现在我们已经看到了API定义，让我们在下一两个部分中查看它们的实现。
- en: 3.3.4 Launching a new training job
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 启动新的训练作业
- en: When a user calls the `Train` API, a training request is added to the job queue
    of the memory store, and then the Docker job tracker handles the actual job execution
    in another thread. This logic will be explained in the next three listings (3.3–3.5).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户调用`Train` API时，训练请求将被添加到内存存储的作业队列中，然后Docker作业跟踪器在另一个线程中处理实际的作业执行。此逻辑将在下一三个列表（3.3-3.5）中解释。
- en: Receiving training requests
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接收训练请求
- en: First, a new training request will be added to the job-waiting queue and will
    be assigned a job ID for future reference; see code (`training-service/src/main/`
    `java/org/orca3/miniAutoML/training/TrainingService.java`) as follows.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将一个新的训练请求添加到作业等待队列，并为将来引用分配一个作业ID；如下代码所示（`training-service/src/main/java/org/orca3/miniAutoML/training/TrainingService.java`）。
- en: Listing 3.3 Submitting training request implementation
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.3 提交训练请求实现
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Implements train API
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实现train API
- en: ❷ Enqueues the training request
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将训练请求入队
- en: ❸ Returns the job ID
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回作业ID
- en: ❹ Four different job lists to track job status
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 四个不同的作业列表以跟踪作业状态
- en: ❺ Generates job ID
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 生成作业ID
- en: ❻ Starts the job at the waiting queue
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在等待队列中启动作业
- en: Launching a training job (container)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 启动训练作业（容器）
- en: Once the job is in the waiting queue, the Docker job tracker will process it
    when there are enough system resources. Figure 3.6 shows the entire process. The
    Docker job tracker monitors the job-waiting queue and picks up the first available
    job when there is enough capacity in the local Docker engine (step 1 in figure
    3.6). Then the Docker job tracker executes the model training the job by launching
    a Docker container (step 2). After the container launches successfully, the tracker
    moves the job object from the job queue to the launching list queue (step 3).
    The code implementation for figure 3.6 (`training-service/src/main/java/org/orca3/miniAutoML/training/
    tracker/DockerTracker.java`) is highlighted in listing 3.4.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦作业进入等待队列，Docker作业跟踪器将在有足够的系统资源时处理它。图3.6显示了整个过程。Docker作业跟踪器监控作业等待队列，并在本地Docker引擎有足够容量时选择第一个可用的作业（图3.6中的步骤1）。然后，Docker作业跟踪器通过启动Docker容器来执行作业中的模型训练（步骤2）。容器成功启动后，跟踪器将作业对象从作业队列移动到启动列表队列（步骤3）。图3.6的代码实现（`training-service/src/main/java/org/orca3/miniAutoML/training/tracker/DockerTracker.java`）在代码列表3.4中突出显示。
- en: '![](../Images/03-06.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-06.png)'
- en: 'Figure 3.6 The training job launching workflow: the Docker job tracker launches
    training containers from the job queue when it has the capacity.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 训练作业启动工作流程：当Docker作业跟踪器有容量时，它会从作业队列启动训练容器。
- en: Listing 3.4 Launching a training container with DockerTracker
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 使用DockerTracker启动训练容器
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Checks the system’s capacity
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查系统容量
- en: ❷ Launches the training Docker container
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 启动训练Docker容器
- en: ❸ Converts training parameters into environment variables
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将训练参数转换为环境变量
- en: ❹ Builds the Docker launch command
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 构建Docker启动命令
- en: ❺ Sets the Docker image name; the value is from the algorithm name parameter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 设置Docker镜像名称；该值来自算法名称参数。
- en: ❻ Passes the training parameter to the Docker container as environment variables
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将训练参数作为环境变量传递给Docker容器
- en: ❼ Runs the Docker container
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 运行Docker容器
- en: It is important to note that the training parameters defined in the `train`
    API request are passed to the training container (training code) as environment
    variables by the `launch` function in code listing 3.4.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在`train` API请求中定义的训练参数是通过代码列表3.4中的`launch`函数作为环境变量传递给训练容器的（训练代码）。
- en: Tracking training progress
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪训练进度
- en: During the last step, the Docker job tracker continues tracking each job by
    monitoring its container’s execution status. When it detects a container status
    change, the job tracker moves the container’s job object to the corresponding
    job list in the memory store.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，Docker作业跟踪器通过监控其容器的执行状态来继续跟踪每个作业。当它检测到容器状态变化时，作业跟踪器将容器的作业对象移动到内存存储中相应的作业列表。
- en: The job tracker will query the Docker runtime to fetch the container’s status.
    For example, if a job’s Docker container starts running, the job tracker will
    detect the change and put the job in the “running job list”; if a job’s Docker
    container finishes, the job is then moved to the “finalized jobs list” by the
    job tracker. The job tracker will stop checking the job status once it’s placed
    on the “finalized jobs list,” which means the training is completed. Figure 3.7
    depicts this job tracking workflow. Code listing 3.5 highlights the implementation
    of this job tracking process.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 作业跟踪器将查询Docker运行时以获取容器的状态。例如，如果一个作业的Docker容器开始运行，作业跟踪器将检测到变化并将作业放入“运行中的作业列表”；如果一个作业的Docker容器完成，作业跟踪器将作业移动到“最终作业列表”。作业跟踪器一旦将作业放置在“最终作业列表”中，就会停止检查作业状态，这意味着训练已完成。图3.7描述了此作业跟踪工作流程。代码列表3.5突出了此作业跟踪过程的实现。
- en: '![](../Images/03-07.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-07.png)'
- en: Figure 3.7 The Docker job tracker monitors the Docker container execution status
    and updates the job queues.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 Docker作业跟踪器监控Docker容器执行状态并更新作业队列。
- en: Listing 3.5 DockerTracker monitoring Docker and updating the job status
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 DockerTracker监控Docker并更新作业状态
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Checks container status for all jobs in the launching job list
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查启动中的作业列表中所有作业的容器状态
- en: ❷ Queries the container’s execution status
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 查询容器的执行状态
- en: ❸ Checks the container status for all jobs in the running job list
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查运行中的作业列表中所有作业的容器状态
- en: 3.3.5 Updating and fetching job status
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.5 更新和检索作业状态
- en: 'Now that you have seen how a training request is executed in the training service,
    let’s move on to the last stop of the code tour: obtaining the training execution
    status. After launching a training job, we can query the `GetTrainingStatus` API
    to get the training status. As a reminder, we are reposting figure 3.5, presented
    here as figure 3.8, which shows the service’s high-level design, as follows.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了训练请求在训练服务中的执行方式，让我们继续代码之旅的最后一站：获取训练执行状态。在启动训练作业后，我们可以查询 `GetTrainingStatus`
    API 来获取训练状态。作为提醒，我们正在重新发布图 3.5，此处作为图 3.8，展示了服务的高级设计，如下所示。
- en: '![](../Images/03-08.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-08.png)'
- en: Figure 3.8 A high-level service design and user workflow
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 高级服务设计和用户工作流程
- en: Based on figure 3.8, we can see that obtaining the training status needs only
    one step, 1.b. Also, the latest status of a training job can be determined by
    finding which job list (in the memory store) contains the `jobId`. See the following
    code for querying the status of a training job/request (`training-service/src/main/java/org/orca3/miniAutoML/training/TrainingService.java`).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图 3.8，我们可以看到获取训练状态只需要一步，即 1.b。此外，可以通过找到包含 `jobId` 的作业列表（在内存存储中）来确定训练作业的最新状态。以下代码用于查询训练作业/请求的状态（`training-service/src/main/java/org/orca3/miniAutoML/training/TrainingService.java`）。
- en: Listing 3.6 Training status implementation
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 训练状态实现
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Searches the job in the finalized job list
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在最终确定的作业列表中搜索作业
- en: ❷ Searches the job in the launching job list
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在启动作业列表中搜索作业
- en: ❸ Searches the job in the running job list
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在运行中的作业列表中搜索作业
- en: ❹ The job is still in the waiting job queue.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 作业仍然在等待作业队列中。
- en: Because the Docker job tracker moves the job to the corresponding job list in
    real time, we can rely on using the job queue type to determine a training job
    status.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Docker 作业跟踪器会实时将作业移动到相应的作业列表中，因此我们可以依赖使用作业队列类型来确定训练作业状态。
- en: 3.3.6 The intent classification model training code
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.6 意图分类模型训练代码
- en: Until now, we have been working with the training service code. Now let’s look
    at the last piece, the model training code. Please do not be intimidated by the
    deep learning algorithms here. The purpose of this code example is to show you
    a concrete example of how a training service interacts with the model training
    code. Figure 3.9 draws the workflow of the sample intent classification training
    code.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理训练服务代码。现在让我们看看最后一部分，即模型训练代码。请不要被这里的深度学习算法吓倒。此代码示例的目的是向您展示训练服务如何与模型训练代码进行交互的具体示例。图
    3.9 绘制了示例意图分类训练代码的工作流程。
- en: '![](../Images/03-09.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-09.png)'
- en: Figure 3.9 The intent classification training code workflow first reads all
    the input parameters from environment variables and then downloads the dataset,
    processes it, and starts the training loop. At the end, it uploads the output
    model file.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 意图分类训练代码工作流程首先从环境变量中读取所有输入参数，然后下载数据集，处理它，并开始训练循环。最后，它上传输出模型文件。
- en: Our sample training code trains a three-layer neural network to perform intent
    classification. It first obtains all the input parameters from environment variables
    that are passed by our training service (see section 3.3.4). The input parameters
    include hyperparameters (epoch number, learning rate, etc.), dataset download
    settings (MinIO server address, dataset ID, version hash), and model upload settings.
    Next, the training code downloads and parses the dataset and starts the iterative
    learning process. In the last step, the code uploads the generated model and training
    metrics to the metadata store. The following code listing highlights the major
    steps mentioned previously (`train-service/text-classification/train.py` and `train-service/text-classification/Dockerfile`).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例训练代码训练一个三层神经网络以执行意图分类。它首先从环境变量中获取所有输入参数，这些参数是由我们的训练服务传递的（见第 3.3.4 节）。输入参数包括超参数（epoch
    数量、学习率等）、数据集下载设置（MinIO 服务器地址、数据集 ID、版本哈希）以及模型上传设置。接下来，训练代码下载并解析数据集，并开始迭代学习过程。最后一步，代码将生成的模型和训练指标上传到元数据存储。以下代码列表突出了之前提到的主要步骤（`train-service/text-classification/train.py`
    和 `train-service/text-classification/Dockerfile`）。
- en: Listing 3.7 Intent classification model training code and Docker file
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 意图分类模型训练代码和 Docker 文件
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note We hope our sample training code demonstrates how a deep learning training
    code follows a common pattern. With Dockerization and a clear protocol to pass
    in parameters, a training service can execute varieties of training code, regardless
    of the training framework or model architecture.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们希望我们的示例训练代码演示了深度学习训练代码遵循的共同模式。通过Docker化和清晰的参数传递协议，训练服务可以执行各种训练代码，无论训练框架或模型架构如何。
- en: 3.3.7 Training job management
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.7 训练作业管理
- en: 'In section 3.1.2, we mentioned that a good training service should address
    computation isolation and provide on-demand computing resources (principle 4).
    This isolation has two meanings: training process execution isolation and resource
    consumption isolation. Because we Dockerize the training process, the process
    execution isolation is guaranteed by the Docker engine. But we still have to handle
    the resource consumption isolation ourselves.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在3.1.2节中，我们提到，一个好的训练服务应该解决计算隔离问题并提供按需计算资源（原则4）。这种隔离有两个含义：训练过程执行隔离和资源消耗隔离。因为我们Docker化了训练过程，过程执行隔离由Docker引擎保证。但我们仍然需要自己处理资源消耗隔离。
- en: 'Imagine three users (A, B, and C) from different teams submitting training
    requests to our training service. If user A submits 100 training requests and
    then users B and C both submit one request each, user B’s and C’s requests will
    sit in the waiting job queue for a while until all of user A’s training requests
    complete. This is what happens when we treat the training cluster as a playing
    field for everyone: one heavy-use case will dominate the job scheduling and resource
    consumption.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 想象有三个来自不同团队的用户（A、B和C）向我们的训练服务提交训练请求。如果用户A提交了100个训练请求，然后用户B和C各自提交一个请求，用户B和C的请求将暂时停留在等待作业队列中，直到用户A的所有训练请求完成。这就是当我们把训练集群当作每个人的竞技场时会发生的情况：一个高负载用例将主导作业调度和资源消耗。
- en: To solve this resource competition problem, we need to set boundaries within
    the training cluster for different teams and users. We could create machine pools
    inside the training cluster to create resource consumption isolation. Each team
    or user can be assigned to a dedicated machine pool, each pool has its own GPU
    and machines, and the pool size depends on the project needs and training usage.
    Also, each machine pool can have a dedicated job queue, so the heavy users won’t
    affect other users. Figure 3.9 shows this approach at work.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这种资源竞争问题，我们需要在训练集群内为不同的团队和用户设置边界。我们可以在训练集群内创建机器池来创建资源消耗隔离。每个团队或用户可以被分配到一个专用的机器池，每个池都有自己的GPU和机器，池的大小取决于项目需求和训练使用情况。此外，每个机器池还可以有自己的专用作业队列，这样高负载用户就不会影响其他用户。图3.9展示了这种方法的应用。
- en: Note The resource segregation approach, like the server pools method we just
    mentioned, may not be efficient on the resource utilization front. For example,
    server pool A may be extremely busy, whereas server pool B may be idle. It is
    possible to define the size of each server pool in a range instead of a fixed
    number, such as a minimum of 5 servers and a maximum of 10 to improve resource
    utilization. Additional logic that either shuffles servers between pools or provisions
    new servers can then be applied.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与刚才提到的服务器池方法类似，资源隔离方法在资源利用率方面可能并不高效。例如，服务器池A可能非常繁忙，而服务器池B可能空闲。可以定义每个服务器池的大小为一个范围，而不是一个固定数字，例如最小5个服务器和最大10个，以提高资源利用率。然后可以应用额外的逻辑，要么在池之间重新分配服务器，要么提供新的服务器。
- en: The ideal approach for implementing figure 3.10 is to use Kubernetes. Kubernetes
    allows you to create multiple virtual clusters backed by the same physical cluster,
    which is called a namespace. Kubernetes namespace is a lightweight machine pool
    that consumes very few system resources.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 实现图3.10的理想方法是使用Kubernetes。Kubernetes允许你创建多个由同一物理集群支持的虚拟集群，这被称为命名空间。Kubernetes命名空间是一个轻量级的机器池，消耗非常少的系统资源。
- en: '![](../Images/03-10.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10](../Images/03-10.png)'
- en: Figure 3.10 Creating machine pools within the training cluster to set up the
    resource consumption boundary for different users.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 在训练集群内创建机器池，为不同用户设置资源消耗边界。
- en: If you are using Kubernetes to manage your service environment and your computing
    cluster, setting up such isolation is fairly easy. First, you create a namespace
    with a resource quota, such as the number of CPUs, memory size, and GPU counts;
    then, define the user and its namespace mapping in the training service.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用 Kubernetes 来管理您的服务环境和计算集群，设置此类隔离相当简单。首先，您创建一个带有资源配额的命名空间，例如 CPU 数量、内存大小和
    GPU 数量；然后，在训练服务中定义用户及其命名空间映射。
- en: Now, when a user submits a training request, the training service first finds
    the right namespace for the user by checking the user information from the request
    and then calls the Kubernetes API to place the training executable in the namespace.
    Because Kubernetes tracks the system usage in real time, it knows whether a namespace
    has enough capacity, and it will reject the job launch request if the namespace
    is fully loaded.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当用户提交训练请求时，训练服务首先通过检查请求中的用户信息来找到用户的正确命名空间，然后调用 Kubernetes API 将训练可执行文件放置在命名空间中。因为
    Kubernetes 实时跟踪系统使用情况，它知道命名空间是否有足够的容量，如果命名空间已满载，它将拒绝作业启动请求。
- en: As you can see, by using Kubernetes to manage training clusters, we can offload
    the resource capacity tracking and resource isolation management from the training
    service. This is one of the reasons why Kubernetes is a good choice for building
    training cluster management for deep learning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，通过使用 Kubernetes 来管理训练集群，我们可以将资源容量跟踪和资源隔离管理从训练服务中卸载。这是 Kubernetes 成为构建深度学习训练集群管理的好选择之一的原因。
- en: 3.3.8 Troubleshooting metrics
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.8 指标故障排除
- en: 'What we didn’t demo in this sample service is metrics. In general, metrics
    are measures of quantitative assessment commonly used for assessing, comparing,
    and tracking performance or production. For deep learning training specifically,
    we usually define two types of metrics: model training execution metrics and model
    performance metrics.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例服务中，我们没有演示的是指标。一般来说，指标是用于评估、比较和跟踪性能或生产的定量评估措施。对于深度学习训练而言，我们通常定义两种类型的指标：模型训练执行指标和模型性能指标。
- en: Model training execution metrics include resource saturation rate, training
    jobs’ execution availability, average training job execution time, and job failure
    rate. We check these metrics to make sure the training service is healthy and
    functioning and that our user’s daily activities are healthy. As an example, we
    expect service availability to exceed 99.99% and training job failure rate to
    be less than 0.1%.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练执行指标包括资源饱和率、训练作业的执行可用性、平均训练作业执行时间和作业失败率。我们检查这些指标以确保训练服务健康且运行正常，以及确保用户的日常活动健康。例如，我们期望服务可用性超过
    99.99%，训练作业失败率低于 0.1%。
- en: Model performance metrics measure the quality of the model learning. It includes
    a loss value and evaluation score for each training iteration (epoch) and the
    final model evaluation results, such as accuracy, precision, and F1 score.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能指标衡量模型学习的质量。它包括每个训练迭代（epoch）的损失值和评估分数，以及最终模型评估结果，如准确率、精确率和 F1 分数。
- en: For model performance–related metrics, we need to store the metrics in a more
    organized way, so we can use a unified method to search for information and compare
    performance between different training runs easily. We will discuss this in more
    detail in chapter 8.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与模型性能相关的指标，我们需要以更组织化的方式存储这些指标，这样我们就可以使用统一的方法来搜索信息，并轻松比较不同训练运行之间的性能。我们将在第 8
    章中更详细地讨论这一点。
- en: 3.3.9 Supporting new algorithm or new version
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.9 支持新算法或新版本
- en: Now let’s discuss how to onboard more training code to our sample training service.
    In the current implementation, we define a naive mapping between the user training
    request and the training code, using the `algorithm` variable in the request to
    find the training image. The underlying rule is the `algorithm` variable must
    equal a Docker image name; otherwise, the training service can’t find the right
    image to run model training.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何将更多训练代码集成到我们的示例训练服务中。在当前实现中，我们通过在请求中使用 `algorithm` 变量来查找训练镜像，定义了用户训练请求和训练代码之间的简单映射。其底层规则是
    `algorithm` 变量必须等于 Docker 镜像名称；否则，训练服务无法找到正确的镜像来运行模型训练。
- en: Use our intent classification training as an example. First, we need to Dockerize
    our intent training Python code to a Docker image and name it “intent-classification.”
    Then, when the user sends a training request with the parameter `algorithm='intent-classification'`,
    the Docker job tracker will use the algorithm name (intent-classification) to
    search local Docker repository for the “intent-classification” training image
    and run the image as a training container.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的意图分类训练为例。首先，我们需要将意图训练Python代码Docker化成一个Docker镜像，并命名为“intent-classification”。然后，当用户发送带有参数`algorithm='intent-classification'`的训练请求时，Docker作业跟踪器将使用算法名称（intent-classification）在本地Docker仓库中搜索“intent-classification”训练镜像，并作为训练容器运行该镜像。
- en: This approach is definitely oversimplified, but it exemplifies how we work with
    data scientists to define a formal contract for mapping user training requests
    to the actual training code. In practice, the training service should provide
    a set of APIs to allow data scientists to register training code in a self-serve
    manner.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法无疑是过于简化的，但它展示了我们如何与数据科学家合作，定义一个将用户训练请求映射到实际训练代码的正式合同。在实践中，训练服务应提供一组API，允许数据科学家以自助方式注册训练代码。
- en: One possible approach is to define an algorithm name and training code mapping
    in a database and add some API to manage this mapping. The proposed APIs can be
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法是在数据库中定义算法名称和训练代码映射，并添加一些API来管理这种映射。提出的API可以是
- en: '`createAlgorithmMapping(string` `algorithmName,` `string` `image,` `string`
    `version)`'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createAlgorithmMapping(string algorithmName, string image, string version)`'
- en: '`updateAlgorithmVersion(string` `algorithmName,` `string` `image,` `string`
    `version)`'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`updateAlgorithmVersion(string algorithmName, string image, string version)`'
- en: If data scientists want to add a new algorithm type, they would call the `createAlgorithmMapping`
    API to register the new training image with a new algorithm name into the training
    service. Our users just need to use this new algorithm name in the training request
    to kick off model training with this new algorithm.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据科学家想要添加一种新的算法类型，他们将通过调用`createAlgorithmMapping` API将新的训练镜像与新的算法名称注册到训练服务中。我们的用户只需在训练请求中使用这个新的算法名称，就可以启动使用这种新算法的模型训练。
- en: If data scientists want to release a newer version of an existing algorithm,
    they can call the `updateAlgorithmVersion` API to update the mapping. Our users
    will still have the same algorithm name (such as intent-classification) to send
    requests, but they won’t be aware that the training code has upgraded to a different
    version behind the scenes. Also, it is worth pointing out that the service’s public
    API won’t be affected by adding new training algorithms; only a new parameter
    value is used.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据科学家想要发布现有算法的新版本，他们可以通过调用`updateAlgorithmVersion` API来更新映射。我们的用户仍然可以使用相同的算法名称（如intent-classification）发送请求，但不会意识到训练代码在幕后已升级到不同版本。此外，值得注意的是，服务的公共API不会因添加新的训练算法而受到影响；只使用新的参数值。
- en: '3.4 Kubeflow training operators: An open source approach'
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 Kubeflow训练操作员：一种开源方法
- en: After seeing our sample training service, let’s look at an open source training
    service. In this section, we will discuss a set of open source training operators
    from the Kubeflow project. These training operators work out of the box and can
    be set up independently in any Kubernetes cluster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到我们的示例训练服务后，让我们看看一个开源训练服务。在本节中，我们将讨论来自Kubeflow项目的一组开源训练操作员。这些训练操作员即插即用，可以在任何Kubernetes集群中独立设置。
- en: Kubeflow is a mature, open source machine learning system built for production
    use cases. We briefly discuss it in appendix B.4 along with Amazon SageMaker and
    Google Vertex AI. We recommend Kubeflow training operators because they are well
    designed and offer high-quality training that’s scalable, distributable, and robust.
    We will first talk about the high-level system design and then discuss how to
    integrate these training operators into your own deep learning system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow是一个成熟的、开源的机器学习系统，专为生产用例构建。我们简要讨论它，在附录B.4中，包括Amazon SageMaker和Google
    Vertex AI。我们推荐Kubeflow训练操作员，因为它们设计良好，提供高质量、可扩展、可分发且健壮的训练。我们将首先讨论高级系统设计，然后讨论如何将这些训练操作员集成到您自己的深度学习系统中。
- en: What Is Kubeflow?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是Kubeflow？
- en: Kubeflow is an open source machine learning platform (originated from Google)
    for developing and deploying production-level machine learning models. You can
    view Kubeflow as the open source version of Amazon SageMaker, but it runs natively
    on Kubernetes, so it’s cloud agnostic. Kubeflow integrates a full list of machine
    learning features into one system—from notebooks and pipelines to training and
    serving.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow是一个开源机器学习平台（源自Google），用于开发和部署生产级机器学习模型。您可以将Kubeflow视为Amazon SageMaker的开源版本，但它原生运行在Kubernetes上，因此它是云无关的。Kubeflow将机器学习功能的完整列表集成到一个系统中——从笔记本和管道到训练和服务。
- en: I highly recommend that you pay attention to Kubeflow projects, even if you
    have no interest in using it. Kubeflow is a well-designed and fairly advanced
    deep learning platform; its feature list covers the entire machine learning life
    cycle. By reviewing its use cases, design, and code, you will gain a deep understanding
    of modern deep learning platforms.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐您关注Kubeflow项目，即使您对使用它没有兴趣。Kubeflow是一个设计精良且相当先进的深度学习平台；其功能列表涵盖了整个机器学习生命周期。通过审查其用例、设计和代码，您将深入了解现代深度学习平台。
- en: Also, because Kubeflow is built natively on top of Kubernetes, you can easily
    set up the whole system on your local or production environment. If you are not
    interested in borrowing the entire system, you can also port some of its components—such
    as training operators or hyperparameter optimization services—which can work by
    themselves in any Kubernetes environment out of the box.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于Kubeflow是原生构建在Kubernetes之上的，您可以在本地或生产环境中轻松设置整个系统。如果您对借用整个系统不感兴趣，您也可以移植其部分组件——例如训练操作员或超参数优化服务——这些组件可以在任何Kubernetes环境中直接运行。
- en: 3.4.1 Kubeflow training operators
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 Kubeflow训练操作员
- en: Kubeflow offers a set of training operators, such as TensorFlow operator, PyTorch
    operator, MXNet operator, and MPI operator. These operators cover all the major
    training frameworks. Each operator has the knowledge to launch and monitor the
    training code (container) written in a specific type of training framework.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow提供了一套训练操作员，例如TensorFlow操作员、PyTorch操作员、MXNet操作员和MPI操作员。这些操作员涵盖了所有主要的训练框架。每个操作员都有在特定类型的训练框架中启动和监控训练代码（容器）的知识。
- en: 'If you plan to run model training in Kubernetes cluster and want to set up
    your own training service to reduce the operation cost, Kubeflow training operators
    are the perfect choice. Here are the three reasons:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划在Kubernetes集群中运行模型训练，并想设置自己的训练服务以降低运营成本，Kubeflow训练操作员是完美的选择。以下是三个原因：
- en: '*Easy install and low maintenance*—Kubeflow operators work out of the box;
    you can make them work in your cluster by issuing a few lines of Kubernetes commands.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易于安装和低维护*——Kubeflow操作员直接运行；您可以通过发出几行Kubernetes命令使它们在您的集群中工作。'
- en: '*Compatible with most training algorithms and frameworks*—As long as you containerize
    your training code, you can use Kubeflow operators to execute it.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与大多数训练算法和框架兼容*——只要您将训练代码容器化，您就可以使用Kubeflow操作员来执行它。'
- en: '*Easy integration to existing systems*—Because Kubeflow training operators
    follow the Kubernetes operator design pattern, you can use Kubernetes’s declarative
    HTTP API to submit the training job request and check the job running status and
    result. You can also use RESTful queries to interact with these operators.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易于集成到现有系统*——因为Kubeflow训练操作员遵循Kubernetes操作员设计模式，您可以使用Kubernetes的声明性HTTP API提交训练作业请求并检查作业运行状态和结果。您还可以使用RESTful查询与这些操作员交互。'
- en: 3.4.2 Kubernetes operator/controller pattern
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 Kubernetes操作员/控制器模式
- en: Kubeflow training operators follow the Kubernetes operator (controller) design
    pattern. If we understand this pattern, running Kubeflow training operators and
    reading their source code is straightforward. Figure 3.11 shows the controller
    pattern design graph.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow训练操作员遵循Kubernetes操作员（控制器）设计模式。如果我们理解这个模式，运行Kubeflow训练操作员和阅读它们的源代码就变得简单直接。图3.11显示了控制器模式设计图。
- en: '![](../Images/03-11.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11](../Images/03-11.png)'
- en: Figure 3.11 The Kubernetes operator/controller pattern runs an infinite control
    loop that watches the actual state (on the right) and desired state (on the left)
    of certain Kubernetes resources and tries to move the actual state to the desired
    one.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 Kubernetes操作员/控制器模式运行一个无限控制循环，监视某些Kubernetes资源的实际状态（在右侧）和期望状态（在左侧），并试图将实际状态移动到期望状态。
- en: Everything in Kubernetes is built around resource objects and controllers. Kubernetes’
    resource objects such as Pods, Namespaces, and ConfigMaps are conceptual objects
    that persist entities (data structures) that represent the state (desired and
    current) of your cluster. A controller is a control loop that makes changes to
    the actual system resources to bring your cluster from the current state closer
    to the desired one, which is defined in resource objects.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的所有内容都是围绕资源对象和控制器构建的。Kubernetes 的资源对象，如 Pods、Namespaces 和 ConfigMaps，是概念性对象，它们持久化表示集群状态（期望和当前）的实体（数据结构）。控制器是一个控制循环，它通过更改实际系统资源来将您的集群从当前状态调整到期望状态，该状态在资源对象中定义。
- en: 'Note Kubernetes pods are the smallest deployable units of computing that you
    can create and manage in Kubernetes. Pods can be viewed as “logical hosts” that
    run one or more Docker containers. A detailed explanation of the Kubernetes concepts,
    such as Namespaces and ConfigMaps, can be found at the official website: [https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Kubernetes pods 是您可以在 Kubernetes 中创建和管理的最小的可部署计算单元。Pod 可以被视为运行一个或多个 Docker
    容器的“逻辑主机”。有关 Kubernetes 概念的详细解释，例如 Namespaces 和 ConfigMaps，可以在官方网站上找到：[https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/)。
- en: 'For example, when a user applies the Kubernetes command to create a pod, it
    will create a pod resource object (a data structure) in the cluster, which contains
    the desired states: two Docker containers and one disk volume. When the controller
    detects this new resource object, it will provision the actual resource in the
    cluster and run the two Docker containers and attach the disk. Next, it will update
    the pod resource object with the latest actual status. Users can query the Kubernetes
    API to get the updated information from this pod resource object. When the user
    deletes this pod resource object, the controller will remove the actual Docker
    containers because the desired state is changed to zero.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当用户应用 Kubernetes 命令来创建 pod 时，它将在集群中创建一个 pod 资源对象（数据结构），其中包含期望状态：两个 Docker
    容器和一块磁盘卷。当控制器检测到这个新的资源对象时，它将在集群中配置实际资源并运行两个 Docker 容器并附加磁盘。接下来，它将使用最新的实际状态更新 pod
    资源对象。用户可以通过查询 Kubernetes API 从这个 pod 资源对象中获取更新信息。当用户删除这个 pod 资源对象时，控制器将删除实际的 Docker
    容器，因为期望状态已更改为零。
- en: To extend Kubernetes easily, Kubernetes allows users to define customer resource
    definition (CRD) objects and register customized controllers to handle these CRD
    objects, which are called operators. If you want to learn more about controllers/
    operators, you can read the “Kubernetes/sample-controller” GitHub repository,
    which implements a simple controller for watching a CRD object. This sample controller
    code can help you to understand operator/controller patterns, and this understanding
    is very useful for reading the Kubeflow training operator source code.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松扩展 Kubernetes，Kubernetes 允许用户定义自定义资源定义（CRD）对象并注册自定义控制器来处理这些 CRD 对象，这些被称为操作符。如果您想了解更多关于控制器/操作符的信息，您可以阅读“Kubernetes/sample-controller”
    GitHub 仓库，它实现了一个简单的控制器来监视 CRD 对象。这个示例控制器代码可以帮助您理解操作符/控制器模式，这种理解对于阅读 Kubeflow 训练操作符源代码非常有用。
- en: Note The terms *controller* and *operator* are used interchangeably in this
    section.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本节中，术语“控制器”和“操作符”是互换使用的。
- en: 3.4.3 Kubeflow training operator design
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 Kubeflow 训练操作符设计
- en: Kubeflow training operators (TensorFlow operator, PyTorch operator, MPI operator)
    follow the Kubernetes operator design. Each training operator watches its own
    kind of customer resource definition object—such as `TFJob`, `PyTorchJob`, and
    `MPIJob`—and creates the actual Kubernetes resources to run the training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 训练操作符（TensorFlow 操作符、PyTorch 操作符、MPI 操作符）遵循 Kubernetes 操作符设计。每个训练操作符监视其自己的客户资源定义对象——例如
    `TFJob`、`PyTorchJob` 和 `MPIJob`——并创建实际的 Kubernetes 资源来运行训练。
- en: For example, the TensorFlow operator processes any `TFJob` CRD object generated
    in the cluster and creates the actual services/pods based on the `TFJob` spec.
    It synchronizes `TFJob` objects’ resource requests with the actual Kubernetes
    resources, such as services and pods, and continuously strives to make the observed
    state match the desired state. See a visual workflow in figure 3.12.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，TensorFlow 操作符处理集群中生成的任何 `TFJob` CRD 对象，并根据 `TFJob` 规范创建实际的服务/pod。它同步 `TFJob`
    对象的资源请求与实际的 Kubernetes 资源，如服务和 pod，并持续努力使观察到的状态与期望状态相匹配。请参见图 3.12 中的可视化工作流程。
- en: '![](../Images/03-12.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-12.png)'
- en: Figure 3.12 The Kubeflow training operator workflow. A user first creates a
    `TFJob` CRD object that defines a training request, and then the TensorFlow operator
    detects this object and creates actual pods to execute the TensorFlow training
    image. The TensorFlow operator also monitors the pod status and updates its status
    to the `TFJob` CRD object. The same workflow applies to the PyTorch operator.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 Kubeflow训练操作员工作流程。用户首先创建一个`TFJob` CRD对象来定义训练请求，然后TensorFlow操作员检测到该对象并创建实际的pod来执行TensorFlow训练镜像。TensorFlow操作员还监控pod状态并将其状态更新到`TFJob`
    CRD对象。相同的流程也适用于PyTorch操作员。
- en: Each operator can run training pods for its own type of training framework.
    For example, the TensorFlow operator knows how to set up a distributed training
    pod group for training code written in TensorFlow. The operator reads the user
    request from the CRD definition, creates training pods, and passes the correct
    environment variables and command-line arguments to each training pod/container.
    You can check out the `reconcileJobs` and `reconcilePods` functions in each operator’s
    code for further details.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 每个操作员都可以为其自己的训练框架运行训练pod。例如，TensorFlow操作员知道如何为用TensorFlow编写的训练代码设置分布式训练pod组。操作员从CRD定义中读取用户请求，创建训练pod，并将正确的环境变量和命令行参数传递给每个训练pod/容器。您可以在每个操作员的代码中查看`reconcileJobs`和`reconcilePods`函数以获取更多详细信息。
- en: Each Kubeflow operator also handles job queue management. Because Kubeflow operators
    follow the Kubernetes operator pattern and create Kubernetes resources at the
    pod level, the training pod failover is handled nicely. For example, when a pod
    fails unexpectedly, the current pod number becomes one less than the desired pod
    number. In this situation, the `reconcilePods` logic in the operator will create
    a new pod in the cluster to make sure the actual pod number is equal to the desired
    number defined in the CRD object, thus addressing failover.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Kubeflow操作员还处理作业队列管理。因为Kubeflow操作员遵循Kubernetes操作员模式，并在pod级别创建Kubernetes资源，所以训练pod故障转移得到了很好的处理。例如，当一个pod意外失败时，当前pod数量比期望的pod数量少一个。在这种情况下，操作员中的`reconcilePods`逻辑将在集群中创建一个新的pod，以确保实际pod数量等于在CRD对象中定义的期望数量，从而实现故障转移。
- en: Note At the time of writing this book, the TensorFlow operator was becoming
    the all-in-one Kubeflow operator. It aims to simplify running distributed or nondistributed
    TensorFlow/PyTorch/MXNet/XGBoost jobs on Kubernetes. No matter how it ends up,
    it will be built on top of the design we mention here but simply more convenient
    to use.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在撰写本书时，TensorFlow操作员正成为全能的Kubeflow操作员。它的目标是简化在Kubernetes上运行分布式或非分布式TensorFlow/PyTorch/MXNet/XGBoost作业。无论结果如何，它都将建立在我们在本节中提到的设计之上，但使用起来更加方便。
- en: 3.4.4 How to use Kubeflow training operators
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 如何使用Kubeflow训练操作员
- en: In this section, we will use the PyTorch operator as an example for training
    a PyTorch model in four steps. Because all Kubeflow training operators follow
    the same usage pattern, these steps are applicable to other operators as well.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将以PyTorch操作员为例，说明在四个步骤中训练PyTorch模型。因为所有Kubeflow训练操作员遵循相同的用法模式，所以这些步骤也适用于其他操作员。
- en: 'First, install the stand-alone PyTorch operator and `PyTorchJob` CRD in your
    Kubernetes cluster. You can find detailed instructions in the developer guide
    from the PyTorch operator Git repository. After installation, you can find a training
    operator pod running and a CRD definition created in your Kubernetes cluster.
    See the CRD query command as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在您的Kubernetes集群中安装独立的PyTorch操作员和`PyTorchJob` CRD。您可以在PyTorch操作员Git仓库的开发者指南中找到详细说明。安装后，您可以在Kubernetes集群中找到一个正在运行的训练操作员pod和一个创建的CRD定义。以下为CRD查询命令：
- en: '[PRE9]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Lists all CRD definitions
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列出所有CRD定义
- en: ❷ PyTorchJob CRD is created in Kubernetes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在Kubernetes中创建PyTorchJob CRD。
- en: Note The training operator installation can be confusing because the README
    suggests that you install the entire Kubeflow to run these operators, but this
    isn’t necessary. Each training operator can be installed individually, which is
    how we recommend handling it. Please check the development guide or the setup
    script at [https://github.com/kubeflow/pytorch-operator/blob/master/scripts/setup-pytorch-operator.sh](https://github.com/kubeflow/pytorch-operator/blob/master/scripts/setup-pytorch-operator.sh).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：训练操作符的安装可能会令人困惑，因为README建议您安装整个Kubeflow来运行这些操作符，但这并不是必要的。每个训练操作符都可以单独安装，这是我们推荐的处理方式。请检查开发指南或设置脚本[https://github.com/kubeflow/pytorch-operator/blob/master/scripts/setup-pytorch-operator.sh](https://github.com/kubeflow/pytorch-operator/blob/master/scripts/setup-pytorch-operator.sh)。
- en: Next, update your training container to read the parameter input from environment
    variables and command-line arguments. You can pass in these parameters later in
    the CRD object.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，更新您的训练容器以从环境变量和命令行参数读取参数输入。您可以在CRD对象中稍后传递这些参数。
- en: Third, create a `PyTorchJob` CRD object to define our training request. You
    can create this CRD object by first writing a YAML file (e.g., pytorchCRD.yaml)
    and then running `kubectl` `create` `-f` `pytorchCRD.yaml` in your Kubernetes
    cluster. The PT-operator will detect this newly created CRD object, put it into
    the controller’s job queue, and try to allocate resources (Kubernetes pods) to
    run the training. Listing 3.8 shows a sample `PyTorchJob` CRD.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，创建一个`PyTorchJob` CRD对象来定义我们的训练请求。您可以通过首先编写一个YAML文件（例如，pytorchCRD.yaml），然后在您的Kubernetes集群中运行`kubectl
    create -f pytorchCRD.yaml`来创建此CRD对象。PT操作符将检测这个新创建的CRD对象，将其放入控制器的作业队列中，并尝试分配资源（Kubernetes
    pods）来运行训练。列表3.8显示了一个示例`PyTorchJob` CRD。
- en: Listing 3.8 A sample PyTorch CRD object
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8：一个示例PyTorch CRD对象
- en: '[PRE10]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The name of the CRD
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ CRD的名称
- en: ❷ Trains job name
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练作业名称
- en: ❸ Defines training group specs
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义训练组规范
- en: ❹ Numbers of master pods
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 主Pod的数量
- en: ❺ Numbers of worker pods
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 工作Pod的数量
- en: ❻ Defines training container configuration
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义训练容器配置
- en: ❼ Defines environment variable for each training pod
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 为每个训练Pod定义环境变量
- en: ❽ Defines the command-line parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义命令行参数
- en: 'The last step is monitoring. You can obtain the training status by using the
    `kubectl` `get` `-o` `yaml` `pytorchjobs` command, which will list the details
    of all the `pytorchjobs` types of CRD objects. Because the controller of the PyTorch
    operator will continue updating the latest training information back to the CRD
    object, we can read the current status from it. For example, the following command
    will make the `PyTorchJob` type CRD object with the name equal to `pytorch-demo`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是监控。您可以通过使用`kubectl get -o yaml pytorchjobs`命令来获取训练状态，该命令将列出所有`pytorchjobs`类型的CRD对象的详细信息。因为PyTorch操作符的控制器将持续更新最新的训练信息回CRD对象，所以我们可以从它那里读取当前状态。例如，以下命令将创建一个名为`pytorch-demo`的`PyTorchJob`类型CRD对象：
- en: '[PRE11]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note In the previous sample, we used the Kubernetes command `kubectl` to interact
    with the PyTorch operator. But we could also send RESTful requests to the cluster’s
    Kubernetes API to create a training job CRD object and query its status. The newly
    created CRD object will then trigger training actions in the controller. This
    means Kubeflow training operators can be easily integrated into other systems.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在先前的示例中，我们使用了Kubernetes命令`kubectl`与PyTorch操作符进行交互。但我们也可以向集群的Kubernetes API发送RESTful请求来创建训练作业CRD对象并查询其状态。新创建的CRD对象将触发控制器中的训练操作。这意味着Kubeflow训练操作符可以轻松集成到其他系统中。
- en: 3.4.5 How to integrate these operators into an existing system
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.5 如何将这些操作符集成到现有系统中
- en: 'From section 3.4.3, we see that the operators’ CRD objects act as the gateway
    APIs to trigger training operations and the source of truth of the training status.
    Therefore, we can integrate these training operators into any system by building
    a web service as a wrapper on top of the operator CRD objects. This wrapper service
    has two responsibilities: first, it converts training requests in your system
    to the CRUD (create, read, update, and delete) operation on the CRD (training
    job) objects; second, it queries training status by reading the CRD objects. See
    the main workflow in figure 3.13.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 从3.4.3节中，我们看到操作符的CRD对象充当触发训练操作的网关API和训练状态的真相来源。因此，我们可以通过在操作符CRD对象之上构建一个网络服务来将这些训练操作符集成到任何系统中。这个包装服务有两个职责：首先，它将您系统中的训练请求转换为CRD（训练作业）对象的CRUD（创建、读取、更新和删除）操作；其次，它通过读取CRD对象来查询训练状态。请参见图3.13中的主要工作流程。
- en: '![](../Images/03-13.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 3-13](../Images/03-13.png)'
- en: Figure 3.13 Integrating Kubeflow training operators into an existing deep learning
    system as training backend. The wrapper service can transform training requests
    to CRD objects and fetch the training status from the CRD objects.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 将 Kubeflow 训练操作符集成到现有的深度学习系统作为训练后端。包装器服务可以将训练请求转换为 CRD 对象，并从 CRD 对象中获取训练状态。
- en: 'In figure 3.13, the front part of the existing system is untouched—for example,
    the front door website. At the computation backend, we changed the internal components
    and talked to the wrapper training service to execute model training. The wrapper
    service does three things: first, it manages the job queue; second, it translates
    the training request from the existing format to the Kubeflow training operators’
    CRD objects; and third, it fetches the training status from CRD objects. With
    this approach, by adding the wrapper service, we can adopt Kubeflow training operators
    easily as the training backend for any existing deep learning platform/systems.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 3-13 中，现有系统的前端部分保持不变——例如，前门网站。在计算后端，我们改变了内部组件，并与包装器训练服务交谈以执行模型训练。包装器服务做三件事：首先，它管理作业队列；其次，它将现有的训练请求格式转换为
    Kubeflow 训练操作符的 CRD 对象；第三，它从 CRD 对象中获取训练状态。通过这种方法，通过添加包装器服务，我们可以轻松地将 Kubeflow
    训练操作符作为任何现有深度学习平台/系统的训练后端采用。
- en: Building a production-quality training system from scratch requires a lot of
    effort. You need to know not only every nuance of different training frameworks
    but also how to handle the reliability and scalability challenges on the engineering
    side. Therefore, we highly recommend adopting Kubeflow training operators if you
    decide to run model training in Kubernetes. It’s an out-of-the-box solution and
    can be ported to an existing system easily.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始构建一个生产质量的训练系统需要大量的努力。你需要了解不同训练框架的每一个细微差别，以及如何处理工程方面的可靠性和可扩展性挑战。因此，如果你决定在
    Kubernetes 上运行模型训练，我们强烈建议采用 Kubeflow 训练操作符。这是一个现成的解决方案，可以轻松地移植到现有系统中。
- en: 3.5 When to use the public cloud
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 何时使用公有云
- en: Major public cloud vendors like Amazon, Google, and Microsoft provide their
    deep learning platforms such as Amazon SageMaker, Google Vertex AI, and Azure
    Machine Learning Studio out of the box. All these systems claim to offer fully
    managed services that support the entire machine learning workflow to train and
    deploy machine learning models quickly. In fact, they cover not only model training
    but also data processing and storage, versioning, troubleshooting, operating,
    and more.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的公有云供应商，如亚马逊、谷歌和微软，提供他们的深度学习平台，例如亚马逊 SageMaker、谷歌 Vertex AI 和 Azure Machine
    Learning Studio，这些平台都是现成的。所有这些系统都声称提供全面管理的服务，支持整个机器学习工作流程，以便快速训练和部署机器学习模型。实际上，它们不仅涵盖模型训练，还包括数据处理和存储、版本控制、故障排除、运营等方面。
- en: In this section, we’re not going to talk about which cloud solution is the best;
    instead, we want to share our thoughts on when to use them. When we propose building
    services inside our company, such as training services or hyperparameter tuning
    services, we often hear questions like “Can we use SageMaker? I heard they have
    a feature . . .” or “Can you build a wrapper on top of Google Vertex AI? I heard. . . .”
    These questions are sometimes valid and sometimes not. What you can afford really
    depends on the stage of your business.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们不会讨论哪种云解决方案是最好的；相反，我们想分享我们对何时使用它们的看法。当我们提出在公司内部构建服务，例如训练服务或超参数调整服务时，我们经常听到类似“我们可以使用
    SageMaker 吗？我听说他们有一个功能…….”或“你能在 Google Vertex AI 之上构建一个包装器吗？我听说……”的问题。这些问题有时是有效的，有时则不然。你能承担的费用实际上取决于你业务的阶段。
- en: 3.5.1 When to use a public cloud solution
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 何时使用公有云解决方案
- en: If you run a startup or want to validate your business idea quickly, using the
    public cloud AI platform is a good option. It handles all the underlying infrastructure
    management and provides a standard workflow for you to follow. As long as the
    predefined methods work for you, you can focus on developing your business logic,
    collecting data, and implementing model algorithms. The real benefit is the time
    saved on building your own infrastructure, so you can “fail early and learn fast.”
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运营一家初创公司或想快速验证你的商业想法，使用公有云 AI 平台是一个不错的选择。它处理所有底层基础设施管理，并提供一个标准的工作流程供你遵循。只要预定义的方法对你有效，你就可以专注于开发你的业务逻辑、收集数据和实现模型算法。真正的益处是节省了构建自己基础设施的时间，因此你可以“快速失败，快速学习”。
- en: Another reason to use the public cloud AI platforms is that you have only a
    few deep learning scenarios, and they fit the public cloud’s standard-use case
    well. In this event, it isn’t worth the resources to build a complicated deep
    learning system for just a few applications.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用公共云人工智能平台的另一个原因是，你只有少数深度学习场景，并且它们很好地符合公共云的标准用例。在这种情况下，为仅几个应用程序构建复杂的深度学习系统不值得投入资源。
- en: 3.5.2 When to build your own training service
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 何时构建自己的训练服务
- en: Now, let’s talk about situations when you need to build your own training approach.
    If you have any of the following five requirements for your system, building your
    own training service is the way to go.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈你需要构建自己的训练方法的情况。如果你对你的系统有以下五个要求，构建自己的训练服务是可行的。
- en: Being cloud agnostic
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 实现云无关性
- en: You can’t use Amazon SageMaker or Google Vertex AI platforms if you want your
    application to be cloud agnostic because these systems are cloud specific. Being
    cloud agnostic is important when your service stores customer data because some
    potential customers have specific requirements on which cloud they *don’t* want
    to put their data in. You want your application to have the capability of running
    on various cloud infrastructures seamlessly.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让你的应用程序实现云无关性，你不能使用亚马逊SageMaker或谷歌Vertex AI平台，因为这些系统是特定于云的。当你的服务存储客户数据时，云无关性很重要，因为一些潜在客户对他们在哪个云中*不*希望存储数据有具体要求。你希望你的应用程序能够在各种云基础设施上无缝运行。
- en: The common method of building a cloud-agnostic system on public clouds is to
    *only* use the foundation services, such as virtual machines (VM) and storage,
    and build your application logic on top of it. Using model training as an example,
    when using Amazon Web Services, we first set up a Kubernetes cluster (Amazon Elastic
    Kubernetes Service (Amazon EKS)) by using Amazon EC2 service to manage the computing
    resources and then build our own training service with the Kubernetes interfaces
    to launch training jobs. In this way, when we need to migrate to Google Cloud
    (GCP), we can simply apply our training service to the GCP Kubernetes cluster
    (Google Kubernetes Engine) instead of Amazon EKS, and most of the service remains
    unchanged.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在公共云上构建云无关系统的常见方法是只使用基础服务，例如虚拟机（VM）和存储，并在其上构建你的应用程序逻辑。以模型训练为例，当使用亚马逊网络服务（AWS）时，我们首先通过使用亚马逊弹性计算云（Amazon
    EC2）服务来管理计算资源，然后使用Kubernetes接口构建自己的训练服务以启动训练作业。这样，当我们需要迁移到谷歌云（GCP）时，我们可以简单地将自己的训练服务应用到GCP
    Kubernetes集群（谷歌Kubernetes引擎）上，而不是亚马逊EKS，并且大部分服务保持不变。
- en: Reducing infrastructure cost
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 降低基础设施成本
- en: Using the cloud provider’s AI platform will charge you premium dollars compared
    to operating on your own services. You may not care so much about your bill at
    the prototyping phase, but after the product is released, you certainly should.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云提供商的人工智能平台将比在自己的服务上操作收费更高。在原型设计阶段，你可能不太关心你的账单，但产品发布后，你当然应该关注。
- en: Using Amazon SageMaker as an example, at the time this book was written (2022),
    SageMaker charged $0.461 per hour for an m5.2xlarge type (eight virtual CPUs,
    32 GB memory) machine. If you launch an Amazon EC2 instance (VM) on this hardware
    spec directly, it charges $0.384 per hour. By building your own training service
    and operating on the Amazon EC2 instances directly, you save nearly 20% on average
    for model building. If a company has multiple teams doing model training on a
    daily basis, a self-built training system will give you an edge over your competitors.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以亚马逊SageMaker为例，在本书撰写之时（2022年），SageMaker对m5.2xlarge类型（八个虚拟CPU，32 GB内存）的机器每小时收费0.461美元。如果你直接在这个硬件规格上启动亚马逊EC2实例（虚拟机），它每小时收费0.384美元。通过构建自己的训练服务并在亚马逊EC2实例上直接运行，你可以在模型构建上平均节省近20%。如果一个公司有多支团队每天进行模型训练，一个自建的训练系统将使你比竞争对手更有优势。
- en: Customization
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 定制化
- en: Although the cloud AI platform gives you a lot of options for the workflow configuration,
    they are still black-box approaches. Because they are the one-for-all approach,
    these AI platforms focus on the most common scenarios. But there are always exceptions
    that you need to customize for your business; it won’t be a good experience when
    there aren’t many choices.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管云人工智能平台为你提供了许多工作流程配置的选项，但它们仍然是黑盒方法。因为它们是通用的方法，这些人工智能平台专注于最常见的场景。但总有例外，你需要为你的业务进行定制；如果没有很多选择，体验不会很好。
- en: Another problem for the cloud AI platform is it always has a delay in adopting
    new technologies. For example, you have to wait for the SageMaker team’s decision
    on whether to support a training method and when to support it, and sometimes
    that decision is not agreeable to you. Deep learning is a rapidly developing space.
    Building your own training service can help you to adopt the latest research and
    pivot quickly, which will give you an edge over the fierce competition.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 云AI平台的另一个问题是它总是对新技术的采用存在延迟。例如，您必须等待SageMaker团队决定是否支持某种训练方法以及何时支持它，有时这个决定可能不符合您的意愿。深度学习是一个快速发展的领域。建立自己的训练服务可以帮助您采用最新的研究成果并快速转型，这将使您在激烈竞争中占据优势。
- en: Passing compliance audits
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 通过合规审计
- en: To be qualified to run some businesses, you need to obtain certificates for
    compliance laws and regulations—for example, HIPAA (Healthcare Insurance Portability
    and Accountability Act) or CCPA (California Consumer Privacy Act). These certifications
    require that you provide evidence not only that your code meets these requirements
    but also that the infrastructure on which your application runs is compliant.
    If your application is built on Amazon SageMaker and Google Vertex AI platforms,
    they also need to be in compliance. As cloud vendors are a black box, running
    through compliance checklists and providing evidence is an unpleasant task.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有资格运营某些业务，您需要获得符合法律法规的证书——例如，HIPAA（医疗保健保险可携带性和问责制法案）或CCPA（加利福尼亚消费者隐私法案）。这些认证要求您不仅提供证据证明您的代码符合这些要求，而且证明您的应用程序运行的基础设施也符合要求。如果您的应用程序建立在Amazon
    SageMaker和Google Vertex AI平台上，它们也需要符合要求。由于云供应商是一个黑盒，运行合规清单并提供证据是一项令人不愉快的任务。
- en: Authentication and authorization
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 认证和授权
- en: Integrating authentication and authorization functionality into cloud AI platforms
    and in-house auth services (on-premises) requires a lot of effort. Many companies
    have their own version of auth services to authenticate and authorize user requests.
    If we adopt SageMaker as the AI platform and expose it to different internal services
    for various business purposes, bridging SageMaker auth management with the in-house
    user auth management services is not going to be easy. Instead, building on-premises
    training services is a lot easier because we can change our API freely and simply
    integrate it into existing auth services.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 将认证和授权功能集成到云AI平台和内部认证服务（本地）需要大量工作。许多公司都有自己的认证服务版本来验证和授权用户请求。如果我们采用SageMaker作为AI平台并将其暴露给不同内部服务以实现各种业务目的，将SageMaker认证管理与内部用户认证管理服务桥接将不会容易。相反，建立本地训练服务要容易得多，因为我们可以自由地更改我们的API并将其简单地集成到现有的认证服务中。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The primary goal of the training service is to manage the computing resources
    and training executions.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练服务的主要目标是管理计算资源和训练执行。
- en: 'A sophisticated training service follows four principles: it supports all kinds
    of model training code through a unified interface; it reduces training cost;
    it supports model reproducibility; and it has high scalability and availability
    and handles compute isolation.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个复杂的训练服务遵循四个原则：它通过统一的接口支持所有类型的模型训练代码；它降低训练成本；它支持模型可重复性；并且具有高可扩展性和可用性，并处理计算隔离。
- en: Understanding the general model training code pattern allows us to treat the
    code as a black box from the perspective of the training service.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解通用模型训练代码模式使我们能够从训练服务的角度将代码视为黑盒。
- en: Containerization is the key to using a generic method to handle the diversities
    of deep learning training methods and frameworks.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化是使用通用方法处理深度学习训练方法和框架多样性的关键。
- en: By Dockerizing training code and defining clear communication protocol, a training
    service can treat training code as a black box and execute the training on a single
    device or distributively. This also benefits data scientists because they can
    focus on model algorithm development without worrying about training execution.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Docker化训练代码和定义清晰的通信协议，训练服务可以将训练代码视为黑盒，并在单个设备或分布式上执行训练。这也使数据科学家受益，因为他们可以专注于模型算法开发，而无需担心训练执行。
- en: Kubeflow training operators are a set of Kubernetes-based open source training
    applications. These operators work out of the box, and they can be easily integrated
    into any existing systems as a model training backend. Kubeflow training operators
    support both distributed and nondistributed training.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow训练操作员是一组基于Kubernetes的开源训练应用程序。这些操作员无需额外配置即可直接使用，并且可以轻松集成到任何现有系统中作为模型训练的后端。Kubeflow训练操作员支持分布式和非分布式训练。
- en: Using public cloud training services can help to build deep learning applications
    quickly. On the other hand, building your own training services can reduce training
    operation costs, provide more customized options, and remain cloud agnostic.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用公共云训练服务可以帮助快速构建深度学习应用程序。另一方面，构建自己的训练服务可以降低训练操作成本，提供更多定制选项，并且保持对云的无依赖性。
