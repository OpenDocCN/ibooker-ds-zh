- en: 4 Distributed training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 分布式训练
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding data parallelism, model parallelism, and pipeline parallelism
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据并行、模型并行和流水线并行
- en: Using a sample training service that supports data parallel training in Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持在Kubernetes中执行数据并行训练的示例训练服务
- en: Training large models with multiple GPUs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个GPU训练大型模型
- en: 'One obvious trend in the deep learning research field is to improve model performance
    with larger datasets and bigger models with increasingly more complex architecture.
    But more data and bulkier models have consequences: they slow down the model training
    process as well as the model development process. As is often the case in computing,
    performance is pitted against speed. For example, it can cost several months to
    train a BERT (Bidirectional Encoder Representations from Transformers) natural
    language processing model with a single GPU.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习研究领域的一个明显趋势是，通过更大的数据集和更复杂的架构的更大模型来提高模型性能。但更多的数据和更庞大的模型有后果：它们会减慢模型训练过程以及模型开发过程。在计算领域，性能与速度往往是相矛盾的。例如，使用单个GPU训练BERT（来自Transformer的双向编码器表示）自然语言处理模型可能需要数月时间。
- en: To address the problem of ever-growing datasets and model parameter size, researchers
    have created various distributed training strategies. And major training frameworks,
    such as TensorFlow and PyTorch, provide SDKs that implement these training strategies.
    With the help of these training SDKs, data scientists can write training code
    that runs across multiple devices (CPU or GPU) and in parallel.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决数据集不断增长和模型参数规模的问题，研究人员已经创建了各种分布式训练策略。主要训练框架，如TensorFlow和PyTorch，提供了实现这些训练策略的SDK。借助这些训练SDK，数据科学家可以编写跨多个设备（CPU或GPU）并行运行的训练代码。
- en: In this chapter, we will explore how to support distributed training from a
    software engineer’s perspective. More specifically, we will see how to write a
    training service to execute different distributed training codes (developed by
    data scientists) in a group of machines.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从软件工程师的角度探讨如何支持分布式训练。更具体地说，我们将看到如何编写一个训练服务来执行一组机器上的不同分布式训练代码（由数据科学家开发）。
- en: After reading this chapter, you will have a holistic view of how distributed
    training can work from the perspectives of both a data scientist and a developer.
    You will know several distributed training strategies and distributed training
    code patterns, as well as how a training service facilitates different distributed
    training codes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，您将从数据科学家和开发者的角度全面了解分布式训练的工作方式。您将了解几种分布式训练策略和分布式训练代码模式，以及训练服务如何促进不同的分布式训练代码。
- en: 4.1 Types of distributed training methods
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 分布式训练方法的类型
- en: 'There are three major types of distributed training methods: model parallelism,
    data parallelism, and pipeline parallelism. *Model parallelism* is a strategy
    to split a neural network into several sequential subnetworks and run each subnetwork
    on different devices (GPU or CPU). In this way, we can train a large model with
    a group of GPUs.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练主要有三种方法：模型并行、数据并行和流水线并行。*模型并行*是一种将神经网络分割成几个顺序子网络并在不同设备（GPU或CPU）上运行每个子网络的策略。这样，我们可以使用一组GPU训练大型模型。
- en: '*Pipeline parallelism* is an advanced version of model parallelism. A major
    problem with model parallelism is that only one GPU is active during training;
    the others are idle. By dividing each training example batch into small microbatches,
    pipeline parallelism overlaps computations between layers to maximize GPU performance.
    This allows different GPUs to work on various microbatches at the same time. The
    GPUs’ training throughput and device utilization improve, resulting in a much
    faster model training speed than model parallelism.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*流水线并行*是模型并行的高级版本。模型并行的一个主要问题是，在训练过程中只有一个GPU处于活动状态；其他GPU处于空闲状态。通过将每个训练示例批次分割成小的微批次，流水线并行在层之间重叠计算以最大化GPU性能。这允许不同的GPU同时处理不同的微批次。GPU的训练吞吐量和设备利用率提高，从而比模型并行实现更快的模型训练速度。'
- en: '*Data parallelism* partitions the dataset into smaller pieces and lets each
    device train these subdatasets separately. Because each device now trains a smaller
    dataset, the training speed improves.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据并行*将数据集分割成更小的部分，并让每个设备分别训练这些子数据集。因为每个设备现在训练的是更小的数据集，所以训练速度提高了。'
- en: Converting a single-device training code to model parallelism or pipeline parallelism
    training requires lots of code changes, including splitting the neural network
    into multiple subnetworks, running subnetworks on different GPUs, and copying
    the subnetworks’ compute output on different GPUs. The sheer quantity, as well
    as the complexity of these changes, makes them problematic and hard to debug.
    Each model algorithm might have a dramatically different model architecture, so
    no standardized method exists for splitting a model for model parallelism or pipeline
    parallelism. Data scientists must build the code case by case.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将单个设备训练代码转换为模型并行或流水线并行训练需要大量的代码更改，包括将神经网络分成多个子网络，在不同的GPU上运行子网络，以及在不同GPU上复制子网络的计算输出。这些更改的数量以及复杂性使得它们变得有问题的同时难以调试。每个模型算法可能具有截然不同的模型架构，因此不存在分割模型以进行模型并行或流水线并行的标准化方法。数据科学家必须逐个构建代码。
- en: On the contrary, data parallelism requires only minimal code changes on a single-device
    training code. And there are standardized patterns for converting a nondistributed
    training code to data parallelism without changing the model algorithm or architecture.
    Also, data parallelism code is relatively easy to both understand and debug. These
    merits make data parallelism our primary choice for distributed training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，数据并行只需要在单个设备训练代码上进行最小的代码更改。而且，有标准化的模式将非分布式训练代码转换为数据并行，而无需更改模型算法或架构。此外，数据并行代码相对容易理解和调试。这些优点使数据并行成为我们分布式训练的首选。
- en: Although data parallelism has a lot of advantages, model parallelism and pipeline
    parallelism have their own strengths and uses as well. When you have large models
    that can’t fit into one GPU, for instance, they are the best-distributed solution.
    We will talk about them more in section 4.4.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据并行有很多优点，但模型并行和流水线并行也有它们自己的优势和用途。例如，当你有大型模型无法适应单个GPU时，它们是最好的分布式解决方案。我们将在第4.4节中进一步讨论它们。
- en: 4.2 Data parallelism
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 数据并行
- en: In this section, we will look at data parallelism theories and their parallel
    execution challenges, along with sample training codes in PyTorch, TensorFlow,
    and Horovod.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨数据并行理论及其并行执行挑战，以及PyTorch、TensorFlow和Horovod中的示例训练代码。
- en: 4.2.1 Understanding data parallelism
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 理解数据并行
- en: Data parallelism involves a group of training devices working together on a
    large dataset. By having each device process a subset of the dataset, we can greatly
    reduce the training time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行涉及一组训练设备共同在大型数据集上工作。通过让每个设备处理数据集的一个子集，我们可以大大减少训练时间。
- en: Synchronous data parallelism is the most adopted data parallelism method. It
    replicates the model network to every device in the training group, whether it
    is a GPU or CPU. The dataset is split into minibatches, and these batches are
    distributed across all devices (again, either CPU or GPU). The training steps
    occur simultaneously, using a different minibatch on each of the devices; therefore,
    the devices act as their own data partition. When calculating gradients to update
    the neural network, the algorithm calculates the final gradients by aggregating
    them from each device. Then it dispatches the aggregated gradients back to each
    device to update their local neural network. Although the training dataset on
    each device is different, the neural networks local to these devices are the same
    because they are updated by the same gradients in each training iteration. As
    a result, this process is called synchronous data parallelism.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 同步数据并行是最常用的数据并行方法。它将模型网络复制到训练组中的每个设备，无论是GPU还是CPU。数据集被分成小批量，这些批量被分配到所有设备上（再次强调，无论是CPU还是GPU）。训练步骤是同时发生的，每个设备使用不同的小批量；因此，设备充当它们自己的数据分区。在计算用于更新神经网络的梯度时，算法通过从每个设备聚合梯度来计算最终梯度。然后它将聚合的梯度派发回每个设备以更新它们本地的神经网络。尽管每个设备上的训练数据集不同，但由于在每个训练迭代中由相同的梯度更新，因此这些设备本地的神经网络是相同的。因此，这个过程被称为同步数据并行。
- en: You can visualize this process in figure 4.1\. The figure compares the process
    of deep learning training on a single GPU, in graph (a) on the left, with the
    setup for synchronous data parallel training using three GPUs, in graph (b) on
    the right.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图4.1中可视化这个过程。该图比较了在单个GPU上进行的深度学习训练过程（如图左边的图(a)所示），以及使用三个GPU进行同步数据并行训练的设置（如图右边的图(b)所示）。
- en: '![](../Images/04-01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-01.png)'
- en: Figure 4.1 A synchronous data parallelism concept graph. (a) Deep learning training
    on a single GPU. (b) Synchronous data parallel training with three GPUs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 同步数据并行概念图。（a）单个GPU上的深度学习训练。（b）使用三个GPU的同步数据并行训练。
- en: By comparing graphs (a) and (b), you can see that synchronous data parallelism
    introduces two extra steps compared with single-device training. The first extra
    step is to divide one training batch into three minibatches, so each device can
    work on its own minibatch. The second step is to synchronize the gradients aggregated
    from all the machines, so they are all operating with the same gradients when
    updating their local model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较图(a)和图(b)，您可以看到与单设备训练相比，同步数据并行引入了两个额外的步骤。第一个额外步骤是将一个训练批次分成三个小批次，以便每个设备可以处理自己的小批次。第二个步骤是同步从所有机器聚合的梯度，以确保在更新本地模型时，所有设备都使用相同的梯度。
- en: Note To aggregate gradients computed by different workers, you can use the algorithm
    all-reduce. This is a popular algorithm that independently combines arrays of
    data from all processes into a single array. In “Writing Distributed Applications
    with PyTorch” ([https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)),
    you can find an example of how PyTorch supports the all-reduce algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了聚合不同工作节点计算出的梯度，可以使用all-reduce算法。这是一个流行的算法，它独立地将所有进程的数据数组组合成一个单一数组。在“使用PyTorch编写分布式应用程序”([https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html))中，您可以找到一个PyTorch支持all-reduce算法的示例。
- en: From an implementation perspective, data parallelism requires minimal changes
    in a single-device model training process. Its main overhead is the added step
    of synchronizing the gradient aggregation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现角度来看，数据并行对单设备模型训练过程的影响最小。其主要开销是添加了同步梯度聚合的步骤。
- en: 'Model parameter updates: Synchronous vs. Asynchronous'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数更新：同步与异步
- en: 'There are two schools of thought on aggregating gradients across workers in
    data parallelism: synchronous updates and asynchronous updates. Let’s review how
    each of these works, along with their advantages and drawbacks, so you can choose
    for yourself:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行中，关于跨工作节点聚合梯度的两种观点：同步更新和异步更新。让我们回顾一下这些方法的工作原理，以及它们的优缺点，以便您可以自行选择：
- en: '*Synchronous model update*—As demonstrated in figure 4.1, a synchronous model
    update pauses the training iteration at the gradient sync step until all devices
    receive the aggregated gradients. Then it proceeds to the next step, updating
    the model parameters. In this way, all devices get the same gradient updates at
    the same time, thus ensuring that the model of each worker is on the same page
    in every training iteration. The problem with synchronous model updates is obvious:
    the training iteration is blocked while the gradients are being synchronized among
    the workers, so none of the workers can begin processing the next minibatch of
    data. If there are some slow machines or network problems, the entire distributed
    working group is stuck, and the faster workers are idle.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同步模型更新*——如图4.1所示，同步模型更新在梯度同步步骤暂停训练迭代，直到所有设备都接收到聚合的梯度。然后它继续到下一步，更新模型参数。这样，所有设备可以同时获得相同的梯度更新，从而确保每个工作节点的模型在每次训练迭代中都在同一页面上。同步模型更新的问题很明显：在梯度在工作节点之间同步时，训练迭代被阻塞，因此没有任何工作节点可以开始处理下一个数据小批次。如果有一些慢速机器或网络问题，整个分布式工作组都会陷入停滞，而较快的工人则会闲置。'
- en: '*Asynchronous model update*—In contrast, the asynchronous model update approach
    does not force each training device or worker to wait to receive gradients from
    the other devices. Instead, whenever a device has finished computing the gradients,
    it immediately updates its local model without checking other devices. Every device
    works independently, and although its gradients still need to be copied to every
    other device, synchronization of these updates is not necessary. The asynchronous
    method may seem very appealing; it’s simple and can run more training steps per
    minute than the synchronous method. A downside to the asynchronous method is that
    it takes a longer time to train and produces less-accurate models than the synchronous
    model update method.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异步模型更新*——相比之下，异步模型更新方法不会强迫每个训练设备或工作者等待从其他设备接收梯度。相反，每当一个设备完成计算梯度后，它立即更新其本地模型，而不检查其他设备。每个设备独立工作，尽管其梯度仍然需要复制到每个其他设备，但这些更新的同步不是必要的。异步方法可能看起来非常吸引人；它简单，并且每分钟可以运行比同步方法更多的训练步骤。异步方法的缺点是它需要更长的时间来训练，并且产生的模型比同步模型更新方法更不准确。'
- en: When we use the asynchronous method, gradients are calculated independently
    on different devices. Some machines run faster while others run slower; consequently,
    these gradients can be produced from different training iterations of each device.
    So there is no guarantee that the aggregated gradients will point in the optimal
    direction. For example, say the gradients from a slow machine are calculated from
    training iteration 5, while other, faster machines have already moved to training
    iteration 20\. When we aggregate the gradients from all the workers, the gradients
    from the lower iteration are applied to those from the higher iteration; this
    degrades the gradient quality.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用异步方法时，梯度在不同的设备上独立计算。有些机器运行得更快，而有些机器运行得更慢；因此，这些梯度可能来自每个设备的不同训练迭代。所以，无法保证聚合的梯度将指向最优方向。例如，假设来自慢速机器的梯度是从训练迭代5计算的，而其他更快机器已经移动到训练迭代20。当我们聚合来自所有工作者的梯度时，低迭代次数的梯度会被应用到高迭代次数的梯度上；这会降低梯度的质量。
- en: In addition, the asynchronous method often converges slowly and has a higher
    accuracy loss than the synchronous method. Thus, most data parallelism libraries
    today are doing synchronous model updates. In this chapter, when we mention data
    parallelism and its code implementation, we mean synchronous data parallelism.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，异步方法通常收敛速度较慢，并且比同步方法有更高的精度损失。因此，今天的大多数数据并行性库都在进行同步模型更新。在本章中，当我们提到数据并行性和其代码实现时，我们指的是同步数据并行性。
- en: Memory constraint for dataset and model
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集和模型的内存限制
- en: In deep learning, datasets and models consume the most memory of the compute
    instance during training. The training process will be terminated by an out-of-memory
    (OOM) error if the training data or neural network (model) exceeds the memory
    limits of the local device. Data parallelism is designed to improve training speed
    but not to solve memory constraint problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，数据集和模型在训练过程中消耗了计算实例的最多内存。如果训练数据或神经网络（模型）超过了本地设备的内存限制，训练过程将被内存溢出（OOM）错误终止。数据并行性旨在提高训练速度，但不是为了解决内存限制问题。
- en: For OOM caused by loading a dataset, we can reduce the batch size of the training
    data, so the training process loads a smaller amount of data into local memory
    in each training loop. In the data parallelism context, we need to make sure the
    minibatch training data can fit into the memory of every worker device.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由于加载数据集导致的内存溢出（OOM），我们可以减少训练数据的批量大小，这样在每次训练循环中，训练过程将加载更少的数据到本地内存中。在数据并行性的上下文中，我们需要确保小批量训练数据可以适应每个工作者设备的内存。
- en: For OOM caused by the model size, we need to adopt model parallelism or pipeline
    parallelism (see section 4.4). Data parallelism simply won’t work when the size
    of a neural network (model) exceeds the memory limits of a single device.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由于模型大小导致的内存溢出（OOM），我们需要采用模型并行性或流水线并行性（见第4.4节）。当神经网络（模型）的大小超过单个设备的内存限制时，数据并行性根本无法工作。
- en: 4.2.2 Multiworker training challenges
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 多工作者训练挑战
- en: Fault tolerance and bandwidth saturation are the two challenges we, as software
    developers, need to address when executing data parallelism code in the training
    service. Meeting these two challenges is critical to reducing operating costs
    and improving training performance for data parallelism–distributed training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 容错性和带宽饱和是我们作为软件开发者在执行训练服务中的数据并行代码时需要解决的两大挑战。解决这两个挑战对于降低运营成本和提高数据并行-分布式训练的训练性能至关重要。
- en: Fault tolerance
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 容错性
- en: We don’t want the entire distributed training group to fail just because one
    of the workers fails unexpectedly. This not only causes a problem for service
    availability but also increases our training cost because all other workers’ efforts
    are wasted if one fails.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望整个分布式训练组因为某个工作者意外失败而全部失败。这不仅会对服务可用性造成问题，还会增加我们的训练成本，因为如果其中一个工作者失败，其他所有工作者的努力都将白费。
- en: To improve fault tolerance, we can preserve the training state (i.e., the model
    parameters) of each training step in a remote filesystem for each worker. Then,
    if one worker fails or takes too long to complete one training iteration, we can
    restart that worker and load its most recent previous state.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高容错性，我们可以将每个工作者的每个训练步骤的训练状态（即模型参数）保存在远程文件系统中。然后，如果某个工作者失败或完成一个训练迭代的时间过长，我们可以重新启动该工作者并加载其最新的先前状态。
- en: Both TensorFlow and PyTorch frameworks have features to back up and restore.
    As training service developers, we can set up the remote disk or backup storage
    system and pass the access configuration to the training container. Then, during
    the training, the training code can use the external filesystem to backup or restore
    states.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和 PyTorch 框架都具备备份和恢复的功能。作为训练服务开发者，我们可以设置远程磁盘或备份存储系统，并将访问配置传递给训练容器。然后，在训练过程中，训练代码可以使用外部文件系统来备份或恢复状态。
- en: Bandwidth saturation
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽饱和
- en: Adding more GPUs and more machines to the distributed training group doesn’t
    always improve performance. Whether we use synchronous or asynchronous model updates,
    the algorithm must communicate the gradients or model parameters between the training
    workers at the end of each training iteration. The time spent on moving data in
    and out of GPU RAM and across the network will eventually outweigh the speedup
    obtained by splitting the training workload.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练组中添加更多 GPU 和更多机器并不总是能提高性能。无论我们使用同步还是异步模型更新，算法都必须在每个训练迭代的末尾在训练工作者之间传递梯度或模型参数。在
    GPU RAM 和网络之间移动数据所花费的时间最终会超过通过分割训练工作负载所获得的速度提升。
- en: Therefore, a cap exists for how many parallel instances can occur before data
    parallelism reaches its peak performance. This cap is determined by the number
    of model parameters and the density of the model (how many nonzero values are
    in model weights). If it’s a large, dense model with lots of parameters and gradients
    to transfer, its saturation is greater than either a smaller model or a large,
    sparse model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在数据并行达到峰值性能之前，可以发生的并行实例数量有一个上限。这个上限由模型参数的数量和模型的密度（模型权重中的非零值的数量）决定。如果是一个大型、密集的模型，具有大量参数和梯度需要传输，其饱和度将大于较小的模型或大型、稀疏的模型。
- en: There are some recommended parallel instance numbers, such as a 6× speedup on
    8 GPUs for neural machine translations and a 32× speedup on 50 GPUs for ImageNet
    models. But we need to determine the sweet spot with our own experiments because
    both GPU and model architectures evolve rapidly, and standard recommendations
    will quickly become outdated. As platform developers, besides choosing the perfect
    number of parallel workers, we have three additional methods for mitigating bandwidth
    saturation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些推荐的并行实例数量，例如神经机器翻译在 8 个 GPU 上实现 6 倍速度提升，对于 ImageNet 模型在 50 个 GPU 上实现 32 倍速度提升。但我们需要通过自己的实验来确定最佳点，因为
    GPU 和模型架构都在快速发展，标准建议很快就会过时。作为平台开发者，除了选择最佳的并行工作者数量外，我们还有三种额外的缓解带宽饱和的方法。
- en: First, we can group the parallel workers (i.e., the containers or pods) into
    fewer machines to reduce the network hops. For example, in Kubernetes, you can
    set `nodeSelector` with affinity and anti-affinity rules ([http://mng.bz/qo76](http://mng.bz/qo76))
    to provision training instances (Kubernetes pods) on a few selected servers that
    have a better network and more computational power.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以将并行工作进程（即容器或Pod）组合到更少的机器上，以减少网络跳数。例如，在Kubernetes中，你可以使用亲和力和反亲和力规则设置`nodeSelector`（[http://mng.bz/qo76](http://mng.bz/qo76)），在具有更好网络和更多计算能力的少数选定服务器上部署训练实例（Kubernetes
    Pod）。
- en: A second option is to always upgrade the training image to use the latest version
    of the training framework. Popular frameworks such as PyTorch, TensorFlow, and
    others are constantly evolving to reduce the data volume transferred within the
    network for distributed training. Pay attention to the release note and take advantage
    of these improvements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是始终升级训练镜像以使用最新的训练框架版本。像PyTorch、TensorFlow和其他流行的框架一样，它们不断进化以减少分布式训练中网络内部传输的数据量。请注意发布说明，并利用这些改进。
- en: Finally, don’t underestimate the gains that can result from doing small tweaks
    when initializing the distributed group. Consider using PyTorch, for example.
    The PyTorch data parallel library partitions the neural network parameter gradients
    into buckets and then sends the buckets around to the workers during the gradient
    synchronization step. The bucket size determines how much data is transferred
    between different devices at one time. So by choosing the right bucket size, we
    can determine a sweet spot between device saturation and network saturation, thus
    reaching the best training speed. The bucket size can be configured in the constructor
    of the PyTorch distributed data parallel (DDP) component ([http://mng.bz/7ZB7](http://mng.bz/7ZB7)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，不要低估在初始化分布式组时进行小调整可能带来的收益。例如，考虑使用PyTorch。PyTorch数据并行库将神经网络参数梯度分割成桶，然后在梯度同步步骤中将桶发送到工作进程。桶的大小决定了不同设备在某一时刻传输的数据量。因此，通过选择合适的桶大小，我们可以在设备饱和和网络饱和之间找到一个最佳点，从而实现最佳的训练速度。桶的大小可以在PyTorch分布式数据并行（DDP）组件的构造函数中配置（[http://mng.bz/7ZB7](http://mng.bz/7ZB7)）。
- en: 4.2.3 Writing distributed training (data parallelism) code for different training
    frameworks
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 为不同的训练框架编写分布式训练（数据并行）代码
- en: 'In this section, you will see some training code snippets for data parallel
    distributed training in three training frameworks: TensorFlow, PyTorch, and Horovod.
    Don’t worry if the code samples here are difficult to parse. The purpose is to
    experience how data scientists handle distributed training on their side. This
    will give you a sense of how training services enable distributed training.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将看到三个训练框架（TensorFlow、PyTorch和Horovod）中数据并行分布式训练的一些训练代码片段。如果这里的代码示例难以解析，请不要担心。目的是体验数据科学家如何处理他们自己的分布式训练。这将让您了解训练服务如何实现分布式训练。
- en: PyTorch
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch
- en: The PyTorch framework has a DDP library that implements data parallelism at
    the module level. The DDP wraps the model object so that it can run the object
    across multiple machines seamlessly. Its training processes can be placed on the
    same machine or across machines.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch框架有一个DDP库，它在模块级别实现数据并行。DDP包装模型对象，使其能够在多台机器上无缝运行。其训练进程可以放置在同一台机器上或跨机器。
- en: To convert a single device/process training code to a data parallel–distributed
    training code, we need to make the following two modifications. First, we must
    initialize the training group by allowing each training process to register itself
    with the master process. One of the processes claims to be the master while the
    others claim to be the workers. Each training process will be pending at this
    registration stage until all workers join the distributed group.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将单个设备/进程的训练代码转换为数据并行-分布式训练代码，我们需要进行以下两个修改。首先，我们必须通过允许每个训练进程向主进程注册自己来初始化训练组。其中一个进程声称自己是主进程，而其他进程声称自己是工作进程。每个训练进程将在这个注册阶段等待，直到所有工作进程加入分布式组。
- en: 'To register a process, we need to know the total number of training processes
    (`world_size`), a unique ID for this process (`rank`), and the master process’s
    address (define `MASTER_ADDR` and `MASTER_PORT` in environment variables). See
    the code sample as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要注册一个进程，我们需要知道训练进程的总数（`world_size`）、该进程的唯一ID（`rank`）以及主进程的地址（在环境变量中定义`MASTER_ADDR`和`MASTER_PORT`）。请参见以下代码示例：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Second, we use the DDP class to wrap the model object. The PyTorch DDP class
    will handle the distributed data communication, gradient aggregation, and local
    model parameter updates:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们使用DDP类来包装模型对象。PyTorch的DDP类将处理分布式数据通信、梯度聚合和本地模型参数更新：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The DDP wrapper takes care of the distributed training execution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ DDP包装器负责分布式训练的执行。
- en: For advanced use cases, the PyTorch library provides the API so you can implement
    your own gradient synchronization function at a lower level. You can check the
    details at the official tutorial, “Writing Distributed Applications with Pytorch”
    ([http://mng.bz/m27W](http://mng.bz/m27W)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高级用例，PyTorch库提供了API，允许您在较低级别实现自己的梯度同步函数。您可以在官方教程“使用Pytorch编写分布式应用程序”中查看详细信息，“Writing
    Distributed Applications with Pytorch”([http://mng.bz/m27W](http://mng.bz/m27W))。
- en: TensorFlow/Keras
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow/Keras
- en: 'TensorFlow supports distributed training in a very similar way to PyTorch;
    it first defines a distributed training strategy (such as `MultiWorkerMirroredStrategy`)
    and then initializes the model with this strategy. To let the strategy identify
    the workers in the distributed group, we need to define a `TF_CONFIG` environment
    variable in each training process. `TF_CONFIG` contains a worker’s unique ID and
    the addresses of all other workers in the group. See the code as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow支持与PyTorch非常相似的方式进行分布式训练；它首先定义一个分布式训练策略（例如`MultiWorkerMirroredStrategy`），然后使用此策略初始化模型。为了让策略识别分布式组中的工作者，我们需要在每个训练过程中定义一个`TF_CONFIG`环境变量。`TF_CONFIG`包含一个工作者的唯一ID和组中所有其他工作者的地址。以下代码示例：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Horovod
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod
- en: 'Horovod is a single-purpose distributed framework. Compared to TensorFlow and
    PyTorch, which can be used across a range of tasks, such as data processing, model
    training, and model serving, Horovod can only focus on one task: making distributed
    deep learning training fast and easy to use.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod是一个单一用途的分布式框架。与TensorFlow和PyTorch可以用于各种任务（如数据处理、模型训练和模型服务）相比，Horovod只能专注于一个任务：使分布式深度学习训练快速且易于使用。
- en: Horovod’s greatest advantage is that it works with different training frameworks,
    such as TensorFlow, Keras, PyTorch, and Apache MXNet. Therefore, we can configure
    our training cluster in one manner (the Horovod way) to run distributed training
    for PyTorch, TensorFlow, and other frameworks. Here, we only list two code snippets
    for using Horovod with TensorFlow and PyTorch, but you can check examples of other
    frameworks on Horovod’s website.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod最大的优势是它能与不同的训练框架协同工作，例如TensorFlow、Keras、PyTorch和Apache MXNet。因此，我们可以以Horovod的方式配置我们的训练集群，以运行PyTorch、TensorFlow和其他框架的分布式训练。在这里，我们只列出使用Horovod与TensorFlow和PyTorch相关的两个代码片段，但您可以在Horovod的网站上查看其他框架的示例。
- en: 'Let’s look at the TensorFlow example. To set up data parallelism–distributed
    training, first we initialize the Horovod training group, which will find other
    Horovod nodes in your cluster automatically. Next, we broadcast the rank 0’s (master
    worker’s) initial variable states to all other processes. This will ensure the
    consistent initialization of all workers. Then we wrap the gradient tape with
    distributed gradient tape, which will average gradients on all workers. The remaining
    code is simply normal TensorFlow training code. As such, please see the code that
    follows ([https://github.com/horovod/horovod/blob/master/examples](https://github.com/horovod/horovod/blob/master/examples)):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看TensorFlow的示例。为了设置数据并行——分布式训练，首先初始化Horovod训练组，它将自动找到集群中的其他Horovod节点。接下来，将rank
    0（主工作者）的初始变量状态广播到所有其他进程。这将确保所有工作者的初始化一致。然后，我们将分布式梯度带包装在梯度带上，这将平均所有工作者的梯度。剩余的代码仅仅是正常的TensorFlow训练代码。因此，请参阅以下代码（[https://github.com/horovod/horovod/blob/master/examples](https://github.com/horovod/horovod/blob/master/examples)）：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Initializes Horovod
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化Horovod
- en: ❷ Adjusts the number of steps based on the number of GPUs
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据GPU数量调整步数
- en: 'The following code is an example of using Horovod with PyTorch. Some PyTorch
    Horovod APIs are different than TensorFlow—for example, `hvd.DistributedOptimizer`
    versus `hvd.DistributedGradientTape`. But these APIs are from the same Horovod
    SDK and share the same interworker mechanism under the hood. Let’s look at the
    PyTorch code snippet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是使用Horovod与PyTorch的示例。一些PyTorch Horovod API与TensorFlow不同——例如，`hvd.DistributedOptimizer`与`hvd.DistributedGradientTape`。但这些API来自同一个Horovod
    SDK，并且在底层共享相同的跨工作者机制。让我们看看PyTorch代码片段：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although the model is defined in two different frameworks—TensorFlow 2 and PyTorch—we
    can see from these two code snippets that they use the same Horovod SDK to run
    distributed training. The benefit here is that we can use a standard method (the
    Horovod way) to set up the distributed worker group in our training cluster, and
    it can still function for the training code written in different training frameworks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型是在两个不同的框架中定义的——TensorFlow 2 和 PyTorch——但我们从这两个代码片段中可以看出，它们使用了相同的 Horovod
    SDK 来运行分布式训练。这里的优势在于，我们可以使用标准方法（Horovod 方法）在我们的训练集群中设置分布式工作组，并且它仍然适用于不同训练框架编写的训练代码。
- en: Two takeaways on training code
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关于代码训练的两个要点
- en: 'It’s fine if you are confused when reading those training code snippets. As
    a training service developer, you don’t need to write these pieces of code. We
    want to emphasize two points from this discussion:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读这些训练代码片段时感到困惑是正常的。作为一个训练服务开发者，你不需要编写这些代码。我们想从这次讨论中强调两点：
- en: Although the code samples in this section implement distributed training in
    different frameworks with different APIs, the code follows the same data parallelism
    paradigm described in section 4.2.1\. That is, the code always (1) sets up the
    communication group for each parallel training process and (2) configures the
    model object to aggregate gradients across all workers. So, as developers, we
    can use a unified method to set up and manage distributed training processes for
    different training frameworks.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然本节中的代码示例在不同的框架中使用不同的 API 实现了分布式训练，但代码遵循了第 4.2.1 节中描述的相同数据并行范式。也就是说，代码始终（1）为每个并行训练过程设置通信组，并（2）配置模型对象以聚合所有工作者的梯度。因此，作为开发者，我们可以使用统一的方法来设置和管理不同训练框架的分布式训练过程。
- en: The work of extending model training code from single-device training to data
    parallelism–distributed training is relatively trivial. Nowadays, the distributed
    training frameworks/SDKs are so powerful that we don’t need to implement every
    detail of data parallelism, such as the gradient synchronization that synchronizes
    the gradients across the network. The training frameworks and SDKs handle these
    processes so they run seamlessly. The distributed data parallel training code
    is almost identical to the single-device training code, except when configuring
    training groups.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型训练代码从单设备训练扩展到数据并行——分布式训练相对简单。如今，分布式训练框架/SDK 非常强大，我们不需要实现数据并行的每一个细节，例如同步网络中梯度的梯度同步。训练框架和
    SDK 处理这些过程，使它们运行得非常顺畅。分布式数据并行训练代码几乎与单设备训练代码相同，只是在配置训练组时有所不同。
- en: 4.2.4 Engineering effort in data parallel–distributed training
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 数据并行——分布式训练中的工程努力
- en: So what does the work look like for enabling data parallel–distributed training
    in production? First, it requires a joint engineering effort between data scientists
    and service developers. For their part, data scientists need to upgrade the single-device
    training code to run distributedly, using code like the snippets in the previous
    section. Meanwhile, service developers must enhance the training service to automatically
    set up distributed worker groups that allow distributed training to happen.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在生产环境中启用数据并行——分布式训练的工作是什么样的呢？首先，它需要数据科学家和服务开发者之间的联合工程努力。对于数据科学家来说，他们需要将单设备训练代码升级以支持分布式运行，使用前面章节中的代码片段。同时，服务开发者必须增强训练服务，以自动设置分布式工作组，允许进行分布式训练。
- en: To make the training service user friendly, the service should incorporate the
    setup details for different distributed training frameworks. Therefore, data scientists
    have to define only the number of parallel instances they need for the training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使训练服务用户友好，服务应包含不同分布式训练框架的设置细节。因此，数据科学家只需定义他们需要的并行实例数量即可进行训练。
- en: 'Let’s use TensorFlow distributed training as an example. From our discussion
    in section 4.2.3, the TensorFlow training code on each device must have `tf_config`
    (see the following example) as an environment variable. So the underlying TensorFlow-distributed
    library in the training process knows how to communicate with other training processes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 TensorFlow 分布式训练为例。根据我们在第 4.2.3 节的讨论，每个设备上的 TensorFlow 训练代码必须设置 `tf_config`（见以下示例）作为环境变量。这样，训练过程中的底层
    TensorFlow-distributed 库就知道如何与其他训练进程进行通信：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From a usability perspective, we can’t expect data scientists to figure out
    the setup value—server IP address and task indexes—for every distributed training
    process, especially if the entire training group is provisioned dynamically. A
    training service should automatically create the group of compute resources for
    a distributed training request, initialize the distributed training libraries
    with the correct IP addresses, and kick off the training process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从可用性的角度来看，我们不能期望数据科学家弄清楚每个分布式训练过程的设置值——服务器IP地址和任务索引，尤其是如果整个训练组是动态配置的。训练服务应该自动为分布式训练请求创建计算资源组，使用正确的IP地址初始化分布式训练库，并启动训练过程。
- en: Figure 4.2 is a conceptual diagram of a training service that supports distributed
    training. From the diagram, you can see that Alex, a data scientist, sends a training
    request to kick off a distributed training run. The service (built by Tang, the
    service developer) then spawns two worker machines and executes the training code
    distributedly. Besides preparing the training code, Alex can specify configurations
    for the training run, such as the number of parallel workers and the type of distributed
    training framework (TensorFlow, PyTorch, or Horovod).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2是一个支持分布式训练的训练服务的概念图。从图中可以看出，数据科学家Alex发送一个训练请求以启动分布式训练运行。然后服务（由服务开发者唐构建）启动两个工作节点并分布式执行训练代码。除了准备训练代码外，Alex还可以指定训练运行的配置，例如并行工作节点的数量和分布式训练框架的类型（TensorFlow、PyTorch或Horovod）。
- en: '![](../Images/04-02.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2](../Images/04-02.png)'
- en: Figure 4.2 An overview of a distributed training system
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 分布式训练系统概述
- en: Let’s take a slow walk through this diagram to better understand how the system
    is set up and who does what job. We see that Tang, as the engineer, needs to make
    three enhancements—numbered 1, 2, and 3 in figure 4.2—to change the training service
    from a single-device trainer (as we saw in chapter 3) to a data parallel–distributed
    trainer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们慢慢走过这个图，以便更好地理解系统的设置以及谁负责什么工作。我们看到，作为工程师的唐需要做出三项改进——如图4.2中编号1、2和3所示——以将训练服务从单设备训练器（正如我们在第3章中看到的）转变为数据并行分布式训练器。
- en: The first step is to update the training service to build a distributed training
    group on demand (at the runtime). When the service receives the request for distributed
    training, it allocates multiple workers from the training cluster for the training
    job and distributes the training code to each worker.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是更新训练服务，以便在运行时按需构建分布式训练组。当服务接收到分布式训练的请求时，它从训练集群中分配多个工作节点用于训练任务，并将训练代码分发到每个工作节点。
- en: The second step is to programmatically initialize each training process with
    the correct server IP, port number, and training process ID. This ensures that
    the distributed libraries (collectively known as the framework, such as TensorFlow)
    have enough information to set up interworker communication for the training group.
    As we saw in the previous section, the setup configuration varies for each distributed
    training framework. The training service should know how to set up interworker
    communication for various frameworks, so data scientists can focus only on the
    algorithm development and not worry about the infrastructure underneath.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是通过编程方式初始化每个训练过程，使用正确的服务器IP地址、端口号和训练过程ID。这确保了分布式库（统称为框架，如TensorFlow）有足够的信息来设置训练组的跨工作节点通信。正如我们在上一节中看到的，设置配置因每个分布式训练框架而异。训练服务应该知道如何为各种框架设置跨工作节点通信，这样数据科学家就可以只关注算法开发，而不必担心底层的基础设施。
- en: The third step is to provide remote storage to back up and restore each worker’s
    training state. In distributed training, if a single worker fails, the entire
    training group fails, and a great deal of computation is wasted. Thus, giving
    distributed training groups the capability to recover from a hardware failure
    or network problem is crucial. By providing remote storage and a backup API, the
    distributed training processes can save their training state (neural network)
    after each training iteration. When a training process fails in the middle of
    the training and can restore its previous state and start over, the entire training
    group continues.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是为每个工作者的训练状态提供远程存储以进行备份和恢复。在分布式训练中，如果单个工作者失败，整个训练组都会失败，并且会浪费大量的计算资源。因此，赋予分布式训练组从硬件故障或网络问题中恢复的能力至关重要。通过提供远程存储和备份API，分布式训练过程可以在每次训练迭代后保存其训练状态（神经网络）。当训练过程在训练过程中失败并能够恢复其先前状态并重新开始时，整个训练组将继续。
- en: 'Note If you want to learn more about data parallelism, you can start with the
    following two articles: a blog post from O’Reilly, “Distributed TensorFlow: Reduce
    both experimentation time and training time for neural networks by using many
    GPU servers,” by Jim Dowling ([www.oreilly.com/content/distributed-tensorflow/](http://www.oreilly.com/content/distributed-tensorflow/)),
    and a paper from Google Brain, “Revisiting Distributed Synchronous SGD,” by Chen
    et al.([https://arxiv.org/pdf/1604.00981.pdf](https://arxiv.org/pdf/1604.00981.pdf)).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您想了解更多关于数据并行的信息，可以从以下两篇文章开始：O’Reilly的一篇博客文章，“使用许多GPU服务器减少神经网络实验时间和训练时间”，作者Jim
    Dowling（[www.oreilly.com/content/distributed-tensorflow/](http://www.oreilly.com/content/distributed-tensorflow/))，以及Google
    Brain的一篇论文，“重新审视分布式同步SGD”，作者陈等（[https://arxiv.org/pdf/1604.00981.pdf](https://arxiv.org/pdf/1604.00981.pdf))。
- en: 4.3 A sample service supporting data parallel–distributed training
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 支持数据并行—分布式训练的示例服务
- en: In this section, we will extend the sample service introduced in the previous
    chapter (section 3.3) to support data parallelism–distributed training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将扩展前一章（第3.3节）中介绍的示例服务，以支持数据并行—分布式训练。
- en: 4.3.1 Service overview
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 服务概述
- en: Compared to the single-device training discussed in section 3.3, the user workflow
    remains the same. Alex, the data scientist, first builds the model training code
    and sends a training request to the training service. Then, the service runs the
    actual training and produces the model at the end.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与第3.3节中讨论的单设备训练相比，用户工作流程保持不变。数据科学家Alex首先构建模型训练代码，并向训练服务发送训练请求。然后，服务运行实际训练并在最后生成模型。
- en: However, there are some crucial differences. First, Alex upgrades the intent
    classification training code to enable it for both a single device and multiple
    devices. Second, Tang, the service developer, modifies the training service API
    to offer a new parameter, `PARALLEL_INSTANCES`. This parameter allows Alex to
    define the size of the worker group for his distributed training run.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些关键的区别。首先，Alex将意图分类训练代码升级，使其适用于单个设备和多个设备。其次，服务开发者Tang修改了训练服务API，以提供一个新参数`PARALLEL_INSTANCES`。此参数允许Alex为其分布式训练运行定义工作者组的大小。
- en: To manage a cluster of servers properly, we need help from Kubernetes. Kubernetes
    can save us a lot of effort on worker resource allocation and interworker communication.
    So we introduce a new component—the *Kubernetes job tracker*—to manage training
    jobs in Kubernetes. You can see the updated service design graph and user workflow
    in figure 4.3.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确管理服务器集群，我们需要Kubernetes的帮助。Kubernetes可以帮助我们在工作者资源分配和工作者间通信上节省大量精力。因此，我们引入了一个新的组件——*Kubernetes作业跟踪器*——以管理Kubernetes中的训练作业。您可以在图4.3中看到更新的服务设计图和用户工作流程。
- en: '![](../Images/04-03.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-03.png)'
- en: Figure 4.3 (a) The previous training service design introduced in figure 3.5;
    (b) the updated service design with distributed training support in Kubernetes
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3（a）图3.5中介绍的先前训练服务设计；（b）在Kubernetes中支持分布式训练的更新服务设计
- en: Figure 4.3 (a) repeats the training service’s system diagram we discussed in
    section 3.3, which uses a Docker job tracker to run the training jobs in the Docker
    engine. Figure 4.3 (b) visualizes the updated training service that now supports
    distributed training—including both Kubernetes and Docker engine backends. The
    Kubernetes job tracker is added to run training jobs in the Kubernetes cluster
    for distributed training jobs. This component executes training jobs by launching
    Kubernetes pods and monitors and updates the job-execution status in the memory
    store.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3（a）重复了我们在第3.3节中讨论的训练服务系统图，它使用Docker作业跟踪器在Docker引擎中运行训练作业。图4.3（b）可视化了一个更新的训练服务，现在支持分布式训练——包括Kubernetes和Docker引擎后端。Kubernetes作业跟踪器被添加到Kubernetes集群中运行分布式训练作业。该组件通过启动Kubernetes
    pods来执行训练作业，并在内存存储中监控和更新作业执行状态。
- en: We also made some changes to the intent classification PyTorch training code
    so it can run distributedly. We’ll review this shortly, in section 4.3.5.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对意图分类的PyTorch训练代码进行了一些修改，使其可以分布式运行。我们将在第4.3.5节中简要回顾。
- en: 'One great timesaver is that we don’t need to change the service API interface
    that we’ve already created (section 3.3.3). Our users can simply work the same
    API to train models in both Docker engines and Kubernetes clusters. This follows
    training service principle number one, which we introduced in chapter 3 (section
    3.1.2): using unified APIs and keeping them agnostic on the backend implementation.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个节省时间的好方法是我们不需要更改我们已经创建的服务API接口（第3.3.3节）。我们的用户可以简单地使用相同的API在Docker引擎和Kubernetes集群中训练模型。这遵循了我们在第3章（第3.1.2节）中介绍的训练服务原则之一：使用统一的API，并在后端实现上保持它们的无知。
- en: 4.3.2 Playing with the service
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 与服务互动
- en: 'First, let’s run the training service with the Kubernetes backend; see the
    commands as follows (`scripts/ts-001-start-server-kube.sh`):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用Kubernetes后端运行训练服务；请参阅以下命令（`scripts/ts-001-start-server-kube.sh`）：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Local Kubernetes config
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 本地Kubernetes配置
- en: Note This section contains only the main steps and key commands necessary to
    run the sample service. As a result, the concept can be demonstrated clearly without
    lengthy pages of code and execution output. Please follow the instructions in
    the “Distributed trainer training demo” ([github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md](http://github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md))
    document at the orca3/MiniAutoML git repository if you want to run the lab in
    this section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节仅包含运行示例服务所需的主要步骤和关键命令。因此，可以在不包含大量代码和执行输出的长页中清楚地展示概念。如果您想在本节中运行实验室，请遵循orca3/MiniAutoML
    git仓库中“分布式训练器训练演示”文档（[github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md](http://github.com/orca3/MiniAutoML/blob/main/training-service/distributed_trainer_demo.md)）中的说明。
- en: 'Once the training service container is running, we can submit a training gRPC
    request. Although the service is now running on the Kubernetes backend, the training
    API is still the same. Compared to the training request we sent to the Docker
    backend demo (see section 3.3.1), only one more parameter—`PARALLEL_INSTANCES=3`—is
    added in the request payload. This tells the training service to create a distributed
    training group with three workers to train the model. If we set this parameter
    to 1, it will be a single-device training request. See the following code snippet
    to submit a distributed training request with three parallel instances (`scripts/ts-004-start-parallel-run.sh`
    `1`):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练服务容器运行起来，我们就可以提交一个训练gRPC请求。尽管服务现在运行在Kubernetes后端，但训练API仍然是相同的。与我们在第3.3.1节中发送到Docker后端演示的训练请求相比，请求负载中只增加了一个额外的参数——`PARALLEL_INSTANCES=3`——这告诉训练服务创建一个包含三个工作节点的分布式训练组来训练模型。如果我们把这个参数设置为1，它将是一个单设备训练请求。请参阅以下代码片段以提交具有三个并行实例的分布式训练请求（`scripts/ts-004-start-parallel-run.sh`
    `1`）：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Requires a training group with three workers
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 需要一个包含三个工作节点的训练组
- en: 'To check the progress of the training execution, we can use the `GetTrainingStatus`
    API:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查训练执行的进度，我们可以使用`GetTrainingStatus` API：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Provides job ID to query status
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提供作业ID以查询状态
- en: 'Besides querying the training service API to obtain job-execution status, we
    can also check the training progress in Kubernetes. By using the Kubernetes command
    `kubectl` `get` `all`, we see three worker pods are created in the local Kubernetes
    environment. One is the master worker, and the other two are normal workers. A
    Kubernetes service object `intent-classification-1-master-service` is also created
    for the master worker/pod, which enables the network connectivity between master
    pods and worker pods. See the code snippet as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查询训练服务API以获取作业执行状态外，我们还可以在Kubernetes中检查训练进度。通过使用Kubernetes命令`kubectl get all`，我们看到在本地Kubernetes环境中创建了三个工作Pod。一个是主工作Pod，另外两个是普通工作Pod。还创建了一个名为`intent-classification-1-master-service`的Kubernetes服务对象，为主工作Pod/容器提供网络连接。如下是代码片段：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ One of the worker pods
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个工作Pod
- en: ❷ Mastering the training pod
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 掌握训练Pod
- en: ❸ The Kubernetes service for training pods communication
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练Pod通信的Kubernetes服务
- en: 4.3.3 Launching training jobs
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 启动训练作业
- en: Now, let’s look at the workflow for launching training jobs with the Kubernetes
    backend. When receiving a training request, the request will be added to the job
    queue. Meanwhile, the Kubernetes job tracker monitors the job queue. When the
    tracker finds jobs waiting and the system has available capacity, it will start
    to process these jobs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看使用Kubernetes后端启动训练作业的工作流程。当接收到训练请求时，请求将被添加到作业队列中。同时，Kubernetes作业跟踪器监控作业队列。当跟踪器发现等待的作业并且系统有可用容量时，它将开始处理这些作业。
- en: To launch a PyTorch-distributed training job, the tracker first creates the
    required numbers of Kubernetes pods. Each pod hosts one training process. The
    tracker also passes separate parameters to each pod, and it then moves the job
    from the job queue to the launching list (figure 4.4).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动PyTorch分布式训练作业，跟踪器首先创建所需数量的Kubernetes Pod。每个Pod托管一个训练过程。跟踪器还向每个Pod传递单独的参数，然后将作业从作业队列移动到启动列表（图4.4）。
- en: '![](../Images/04-04.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-04.png)'
- en: 'Figure 4.4 The workflow for launching a training job in Kubernetes: step 1,
    detects the waiting job in the job queue; step 2, creates Kubernetes pods to run
    training; and step 3, moves the job from the job queue to the launching list.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 Kubernetes中启动训练作业的工作流程：步骤1，检测作业队列中的等待作业；步骤2，创建Kubernetes Pod以运行训练；步骤3，将作业从作业队列移动到启动列表。
- en: In figure 4.4, the Kubernetes job tracker can handle both single-device training
    and distributed training. It creates one Kubernetes pod for single-device training
    and multiple pods for distributed training.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.4中，Kubernetes作业跟踪器可以处理单设备训练和分布式训练。它为单设备训练创建一个Kubernetes Pod，为分布式训练创建多个Pod。
- en: A Kubernetes job tracker runs one training pod similarly to a Docker job tracker.
    It wraps up all the user-defined parameters in the environment variables and passes
    them to the Kubernetes pod.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes作业跟踪器运行一个训练Pod的方式类似于Docker作业跟踪器。它将所有用户定义的参数封装在环境变量中，并将它们传递给Kubernetes
    Pod。
- en: 'To set up PyTorch distributed training with multiple pods, the service handles
    two more functions. First, it creates a Kubernetes service object to talk to the
    master pod. From the PyTorch distributed training algorithm section (4.2.3), we
    know that each PyTorch training process needs the IP address of the master process
    (pod) to initialize the distributed training group. For example, each PyTorch
    code needs to have the following code snippet before the training logic starts:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置具有多个Pod的PyTorch分布式训练，服务处理两个额外的功能。首先，它创建一个Kubernetes服务对象以与主Pod通信。从PyTorch分布式训练算法部分（4.2.3）中，我们知道每个PyTorch训练过程都需要主进程（Pod）的IP地址来初始化分布式训练组。例如，每个PyTorch代码在训练逻辑开始之前都需要以下代码片段：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Joins the current process to a distributed group by seeking the master pod
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过寻找主Pod将当前进程加入分布式组
- en: But in Kubernetes, a pod is an ephemeral resource, so we can’t rely on the pod
    IP address to locate a pod. Instead, we use the Kubernetes domain name service
    (DNS) as a permanent address to locate pods. Even if the pod is destroyed and
    recreated in a different node and the IP is different, we can always use the same
    DNS to reach it. So, to enable the training group’s initialization, we first create
    a Kubernetes service for the master pod and then pass the DNS to all worker pods
    as the master pod address.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但在Kubernetes中，Pod是一个短暂的资源，所以我们不能依赖于Pod IP地址来定位Pod。相反，我们使用Kubernetes域名服务（DNS）作为永久地址来定位Pod。即使Pod在不同的节点上被销毁和重新创建，IP地址不同，我们也可以始终使用相同的DNS来访问它。因此，为了使训练组的初始化成为可能，我们首先为主Pod创建一个Kubernetes服务，然后将DNS传递给所有工作Pod作为主Pod地址。
- en: 'Second, it passes four environment variables to each pod. The four variables
    required by each training pod are `WORLD_SIZE`, `RANK`, `MASTER_ADDR`, and `MASTER_PORT`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，它将四个环境变量传递给每个Pod。每个训练Pod所需的四个变量是`WORLD_SIZE`、`RANK`、`MASTER_ADDR`和`MASTER_PORT`：
- en: '`WORLD_SIZE` means the total number of pods of the training group, including
    master and workers.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WORLD_SIZE`表示训练组中Pod的总数，包括主Pod和工作Pod。'
- en: '`RANK` is the unique ID of one training process; the master process’s rank
    must be 0.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RANK`是一个训练进程的唯一ID；主进程的排名必须是0。'
- en: '`MASTER_ADDR` and `MASTER_PORT` define the host address and port number of
    the master process, so each worker can use them to reach the master pod.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MASTER_ADDR`和`MASTER_PORT`定义了主进程的主机地址和端口号，因此每个工作进程都可以使用它们来访问主Pod。'
- en: 'For example, when running distributed training with three instances, we create
    three pods (one master, two workers) with the following environment variables
    for each pod:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用三个实例运行分布式训练时，我们为每个Pod创建三个Pod（一个主Pod，两个工作Pod），每个Pod具有以下环境变量：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In light of all the explanations, let’s take a look at how the actual code is
    implemented. The following listing highlights how launching distributed training
    in Kubernetes is implemented.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所有这些解释，让我们看看实际的代码是如何实现的。以下列表突出了在Kubernetes中启动分布式训练的实现方式。
- en: Listing 4.1 Launching a distributed training job
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 启动分布式训练作业
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '❶ World size >1: indicates it’s a distributed training'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 世界大小 >1：表示这是分布式训练
- en: ❷ Creates a Kubernetes service and points to the master pod
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个Kubernetes服务并指向主Pod
- en: ❸ Sets the distributed training–related config as the environment variable
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将分布式训练相关的配置作为环境变量设置
- en: ❹ Defines pod configuration; passes in training parameters as environment variables
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义Pod配置；将训练参数作为环境变量传递
- en: ❺ Creates actual training pods
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建实际的训练Pod
- en: '**RANK** values do not neccesarily map to pods one to one'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**RANK**值不一定与Pod一一对应'
- en: '`RANK` is a tricky variable in distributed training. Please be aware that `RANK`
    is the unique ID of a training process, not a pod. A pod can run multiple training
    processes if it has multiple GPUs. In the example here, because we run one training
    process per pod, we assign a different `RANK` value to each pod.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`RANK`是分布式训练中的一个棘手变量。请注意，`RANK`是训练进程的唯一ID，而不是Pod。如果Pod有多个GPU，它可以运行多个训练进程。在这个例子中，因为我们每个Pod运行一个训练进程，所以我们为每个Pod分配不同的`RANK`值。'
- en: When we run multiple training processes in one pod, then we need to assign multiple
    `RANK` values to a pod. For example, when we run two processes in a pod, this
    pod needs two `RANK` values, one for each process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在一个Pod中运行多个训练进程时，我们需要为Pod分配多个`RANK`值。例如，当我们在一个Pod中运行两个进程时，这个Pod需要两个`RANK`值，每个进程一个。
- en: You may notice that the Kubernetes pods and services created in this sample
    are customized for PyTorch distributed training library. In fact, the sample service
    is not limited to PyTorch. To support training code written in other frameworks,
    such as TensorFlow 2, we can extend the Kubernetes job tracker to support the
    settings for TensorFlow distributed training.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，在这个示例中创建的Kubernetes Pod和服务是为PyTorch分布式训练库定制的。实际上，这个示例服务不仅限于PyTorch。为了支持用其他框架编写的训练代码，例如TensorFlow
    2，我们可以扩展Kubernetes作业跟踪器以支持TensorFlow分布式训练的设置。
- en: For example, we can collect all the IPs or DNSs of the worker pods, put them
    together, and broadcast them back to each worker pod. During the broadcasting,
    we set worker group information to the `TF_CONFIG` environment variable in every
    pod to start the distributed training group. The `TF_CONFIG` environment variable
    is a special requirement for the TensorFlow distributed library.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以收集所有工作者pod的IP或DNS，将它们放在一起，并将它们广播回每个工作者pod。在广播过程中，我们将工作者组信息设置为每个pod中的`TF_CONFIG`环境变量以启动分布式训练组。`TF_CONFIG`环境变量是TensorFlow分布式库的特殊要求。
- en: 4.3.4 Updating and fetching the job status
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 更新和检索作业状态
- en: After creating training pods, the Kubernetes job tracker will continue querying
    the pod execution status and move the job to other job lists when its status changes.
    For example, if the pod is created successfully and starts running, the tracker
    moves the job from the launching list to the running list. If the pod execution
    is completed, the tracker moves the job from the running list to the finalized
    jobs list. Figure 4.5 depicts this process.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建训练pod后，Kubernetes作业跟踪器将继续查询pod执行状态，并在其状态发生变化时将作业移动到其他作业列表。例如，如果pod创建成功并开始运行，跟踪器将作业从启动列表移动到运行列表。如果pod执行完成，跟踪器将作业从运行列表移动到最终作业列表。图4.5描述了此过程。
- en: '![](../Images/04-05.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5](../Images/04-05.png)'
- en: 'Figure 4.5 Track the Kubernetes training job status: step 1, obtains the jobs
    in the running list; step 2, queries the pod execution status of each of the jobs
    running in the Kubernetes cluster; and step 3, moves the job to the finalized
    job list if the pod execution is complete (success or failure).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 跟踪Kubernetes训练作业状态：步骤1，获取运行列表中的作业；步骤2，查询在Kubernetes集群中运行的每个作业的pod执行状态；步骤3，如果pod执行完成（成功或失败），则将作业移动到最终作业列表。
- en: When a user submits a job status query, the training service will search the
    job ID in all four job queues in the memory store and return the job object. Interestingly,
    although there are multiple training pods, we only need to check the status of
    the master pod to track the distributed training progress. This is because, for
    synchronous data parallel training, all workers have to sync with each other in
    every training cycle, so the master pod can represent the other worker pods.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提交作业状态查询时，训练服务将在内存存储中的所有四个作业队列中搜索作业ID，并返回作业对象。有趣的是，尽管有多个训练pod，但我们只需要检查主pod的状态来跟踪分布式训练进度。这是因为，对于同步数据并行训练，所有工作者在每个训练周期中都必须相互同步，因此主pod可以代表其他工作者pod。
- en: The code for querying and updating job execution status is very similar to the
    Docker job tracker that we see in section 3.3.5\. The only difference is that
    we query the Kubernetes cluster instead of the Docker engine to obtain the training
    status. We leave the code for you to explore; you can find it in the `updateContainerStatus`
    method of the `KubectlTracker` class.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 查询和更新作业执行状态的代码与我们在第3.3.5节中看到的Docker作业跟踪器非常相似。唯一的区别是我们查询Kubernetes集群而不是Docker引擎以获取训练状态。我们留给您探索的代码；您可以在`KubectlTracker`类的`updateContainerStatus`方法中找到它。
- en: 4.3.5 Converting the training code to run distributedly
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 将训练代码转换为分布式运行
- en: We made two changes to our intent classification training code (introduced in
    the previous chapter, section 3.3.6) to support both distributed mode and single-device
    mode.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对我们的意图分类训练代码（在上一章第3.3.6节中介绍）进行了两项更改，以支持分布式模式和单设备模式。
- en: 'First change: Initialize the training group'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个更改：初始化训练组
- en: We use the `WORLD_SIZE` environment variable to check whether the training code
    should run in distributed training. If the world size equals 1, then we use the
    same single-device training code that we saw in section 3.3.6.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`WORLD_SIZE`环境变量来检查训练代码是否应在分布式训练中运行。如果世界大小等于1，则我们使用与第3.3.6节中看到的相同的单设备训练代码。
- en: 'But if the value is greater than 1, we initialize the training process to join
    the distributed group. Please also notice that a unique `RANK` value for each
    pod is passed from the training service (Kubernetes job tracker), which is needed
    for distributed group initialization. After self-registering to the distributed
    group, we declare the model and data sampler to be distributed as well. See the
    following code for the changes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果值大于1，我们将初始化训练过程以加入分布式组。请注意，每个pod都有一个唯一的`RANK`值，该值从训练服务（Kubernetes作业跟踪器）传递过来，这是分布式组初始化所需的。在向分布式组自我注册后，我们声明模型和数据采样器也应分布式。以下代码显示了所做的更改：
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Second change: Only upload the final model from the master pod'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次更改：仅从主节点上传最终模型
- en: 'In the second change, we only allow the master pod (rank = 0) to upload the
    final model. This is to prevent each worker from uploading the same models multiple
    times:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次更改中，我们只允许主节点（rank = 0）上传最终模型。这是为了防止每个工作节点多次上传相同的模型：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Rank 0 is the master pod.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Rank 0 是主节点。
- en: 4.3.6 Improvements
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.6 改进
- en: If we continue the path to making this sample service production ready, we can
    follow the thoughts in section 4.2.2 to work on improving fault tolerance and
    reducing network bandwidth saturation. We can also extend the Kubernetes job tracker
    to support TensorFlow and Horovod distributed training. From a training service
    perspective, they are not very different because the configuration that the training
    service passes to the training code is very generic; this information is needed
    for all frameworks but with different names. As long as the protocol between the
    training service and the training code is clear and stable, we can still treat
    the training code as a black box, even in the distributed setting.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续将此示例服务推向生产，我们可以遵循第4.2.2节中的思路来提高容错性和减少网络带宽饱和。我们还可以扩展Kubernetes作业跟踪器以支持TensorFlow和Horovod分布式训练。从训练服务的角度来看，它们并没有很大不同，因为训练服务传递给训练代码的配置非常通用；这些信息对于所有框架都是必需的，但名称不同。只要训练服务和训练代码之间的协议清晰且稳定，我们仍然可以将训练代码视为黑盒，即使在分布式环境中也是如此。
- en: 4.4 Training large models that can’t load on one GPU
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 无法加载到单个GPU上的大型模型训练
- en: The neural network size (defined by the number of parameters) is growing rapidly
    in the research field, and we cannot ignore this trend. Using the ImageNet challenge
    as an example, the winner in 2014 (GoogleNet) had 4 million parameters; the winner
    in 2017 (Squeeze-and-Excitation Networks) had 145.8 million parameters; and the
    current leading approaches have more than 1 billion parameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究领域，神经网络的大小（由参数数量定义）正在迅速增长，我们不能忽视这一趋势。以ImageNet挑战赛为例，2014年的获胜者（GoogleNet）有400万个参数；2017年的获胜者（Squeeze-and-Excitation
    Networks）有1.458亿个参数；而当前领先的方法有超过10亿个参数。
- en: Although our neural network size grew nearly 300×, GPU memory has only increased
    4×. You will see cases more often in the future in which we can’t train a model
    because it can’t be loaded onto one GPU.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的神经网络大小增长了近300倍，但GPU内存仅增加了4倍。您未来将更频繁地看到我们无法训练一个模型，因为它无法加载到一个GPU上。
- en: In this section, we will discuss common strategies for training large models.
    Unlike the data parallelism strategy described in section 4.2, the method introduced
    here requires effortful work on training code.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论训练大型模型的常见策略。与第4.2节中描述的数据并行策略不同，这里介绍的方法需要对训练代码进行努力的工作。
- en: Note Although the methods introduced in this section are normally implemented
    by data scientists, we hope you can still follow them. Understanding the strategies
    behind these training techniques is very helpful for designing communication protocols
    between training services and training codes. It also provides insight into troubleshooting
    or fine-tuning the training performance in training service. To keep it simple,
    we will only describe algorithms at the concept level and focus on the necessary
    work from an engineering perspective.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：尽管本节中介绍的方法通常由数据科学家实现，但我们希望您仍然可以理解它们。了解这些训练技术背后的策略对于设计训练服务和训练代码之间的通信协议非常有帮助。它还提供了在训练服务中故障排除或微调训练性能的见解。为了简化，我们将在概念层面上描述算法，并侧重于从工程角度必要的任务。
- en: '4.4.1 Traditional methods: Memory saving'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 传统方法：内存节省
- en: Let’s say your data science team wants to train a model that can load to the
    largest GPU in your training cluster; for example, they want to train a 24 GB
    BERT model in a 10 GB memory GPU. There are several memory-saving techniques the
    team can use to train the model in this situation, including gradient accumulation
    and memory swap. This work is generally implemented by data scientists. As a platform
    developer, you just need to be aware of these options. We’ll describe them briefly,
    so you will know when to suggest each of their use.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据科学团队想要训练一个可以加载到训练集群中最大GPU的模型；例如，他们想在10GB内存的GPU上训练一个24GB的BERT模型。在这种情况下，团队可以使用几种内存节省技术来训练模型，包括梯度累积和内存交换。这项工作通常由数据科学家完成。作为平台开发者，您只需要了解这些选项。我们将简要描述它们，这样您就会知道何时建议使用它们。
- en: note There are several other memory-saving methods, such as OpenAI’s gradient
    checkpointing ([https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing))
    and NVIDIA’s vDNN ([https://arxiv.org/abs/1602.08124](https://arxiv.org/abs/1602.08124)),
    but because this book is not about deep learning algorithms, we will leave them
    for independent study.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：还有其他几种内存节省方法，例如OpenAI的梯度检查点（[https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing)）和NVIDIA的vDNN（[https://arxiv.org/abs/1602.08124](https://arxiv.org/abs/1602.08124)），但由于这本书不是关于深度学习算法的，我们将它们留作独立学习。
- en: Gradient accumulation
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积
- en: In deep learning training, the dataset is split into batches. In each training
    step, for loss calculation, gradient computation, and model parameter updating,
    we take the whole batch of examples (training data) into memory and handle the
    computations all at once.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习训练中，数据集被分割成批次。在每个训练步骤中，为了损失计算、梯度计算和模型参数更新，我们将整个批次的示例（训练数据）加载到内存中，并一次性处理计算。
- en: We can mitigate the memory pressure by reducing the batch size—for example,
    training 16 examples in a batch rather than 32 examples in a batch. But reducing
    batch size can cause the model to converge a lot more slowly. And this is where
    gradient accumulation can be helpful.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过减少批量大小来减轻内存压力——例如，将批量大小从32个示例减少到16个示例进行训练。但是，减少批量大小会导致模型收敛速度大大减慢。这正是梯度累积可以发挥作用的地方。
- en: Gradient accumulation cuts batch examples into configurable numbers of minibatches
    and then calculates the loss and gradients after each minibatch. But instead of
    updating the model parameters, it waits and accumulates the gradients over all
    the minibatches. And then, ultimately, it updates the model parameters based on
    the cumulative gradient.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积将批量示例分割成可配置数量的微批量，并在每个微批量之后计算损失和梯度。但是，它不是更新模型参数，而是等待并累积所有微批量的梯度。最终，它根据累积梯度更新模型参数。
- en: Let’s look at an example to see how this speeds up the process. Imagine that,
    because of GPU memory constraints, we can’t run training with a batch size of
    32\. With gradient accumulation, we can split each batch into four minibatches,
    each with a size of 8\. Because we accumulate the gradients for all four minibatches
    and only update the model after all four are complete, the process is almost equal
    to training with a batch size of 32\. The difference is that we only compute 8
    examples at a time in GPU instead of 32, so the cost is 4× *slower* than it would
    be with a batch of 32.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看看这种方法是如何加快过程的。想象一下，由于GPU内存限制，我们无法以32个示例的批量大小进行训练。使用梯度累积，我们可以将每个批次分割成四个微批量，每个微批量大小为8。因为我们累积了所有四个微批量的梯度，并且只有在所有四个都完成后才更新模型，所以这个过程几乎等同于使用32个示例的批量大小进行训练。区别在于我们每次只在GPU上计算8个示例，而不是32个，所以成本是32个示例批量的4倍*更慢*。
- en: Memory swap (GPU and CPU)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 内存交换（GPU和CPU）
- en: 'The memory swap method is very simple: it copies activations between CPU and
    GPU, back and forth. If you are unaccustomed to deep learning terms, think of
    *activation* as the computation output from each node of the neural network. The
    idea is to only keep the necessary data for the current computation step in GPU
    and swap the compute result out to CPU memory for future steps.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 内存交换方法非常简单：在CPU和GPU之间来回复制激活。如果你不熟悉深度学习术语，可以将*激活*视为神经网络每个节点的计算输出。想法是只保留当前计算步骤所需的必要数据在GPU上，并将计算结果交换到CPU内存中用于后续步骤。
- en: Building on this idea, a new relay-style execution technique called L2L (layer
    to layer) keeps only the executing layers and transit buffers on the GPU. The
    whole model and the optimizer—which holds the state—are stored in the CPU space.
    L2L can greatly increase the GPU throughput and allow us to develop large models
    on affordable devices. If you are interested in this method, you can check out
    the paper “Training Large Neural Networks with Constant Memory Using a New Execution
    Algorithm,” by Pudipeddi et al. ([https://arxiv.org/abs/2002.05645](https://arxiv.org/abs/2002.05645)),
    which also has a PyTorch implementation in GitHub.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个想法，一种新的中继式执行技术L2L（层到层）只在GPU上保留执行层和传输缓冲区。整个模型和优化器（包含状态）存储在CPU空间中。L2L可以大大提高GPU吞吐量，并允许我们在经济实惠的设备上开发大型模型。如果你对这个方法感兴趣，可以查看Pudipeddi等人撰写的论文“使用新的执行算法以恒定内存训练大型神经网络”，该论文可在GitHub上找到PyTorch实现（[https://arxiv.org/abs/2002.05645](https://arxiv.org/abs/2002.05645)）。
- en: 'Both gradient accumulation and memory swap are effective ways to train a large
    model on a smaller GPU. But, like most things, they come with a cost: they tend
    to slow down the training. Because of this drawback, we normally use them only
    for prototyping ideas.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度累积和内存交换都是在大型GPU上训练有效的方法。但，就像大多数事情一样，它们都有代价：它们往往会减慢训练速度。由于这个缺点，我们通常只将它们用于原型设计。
- en: 'To obtain workable training speeds, we really need to train models distributedly
    on multiple GPUs. So, in the next section, we will introduce a more production-like
    approach: pipeline parallelism. It can train a large model distributedly with
    impressive training speed.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得可工作的训练速度，我们真的需要在多个GPU上分布式训练模型。因此，在下一节中，我们将介绍一个更接近生产环境的方案：管道并行。它可以通过令人印象深刻的训练速度分布式训练大型模型。
- en: 4.4.2 Pipeline model parallelism
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 管道模型并行
- en: 'In section 4.2, we discussed the most commonly used distributed training method:
    data parallelism. This approach keeps a copy of the whole model on each device
    and partitions data into multiple devices. Then it aggregates the gradients and
    updates the model in each training step. The whole approach of data parallelism
    works well, as long as the entire model can be loaded into one GPU. As we see
    in this section, however, we are not always able to do this. And that is where
    pipeline parallelism can be useful. In this section, we will learn about pipeline
    parallelism, a training method that trains large models distributedly on multiple
    GPUs.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在4.2节中，我们讨论了最常用的分布式训练方法：数据并行。这种方法在每个设备上保留整个模型的副本，并将数据分割到多个设备上。然后在每个训练步骤中聚合梯度并更新模型。只要整个模型可以加载到一个GPU上，整个数据并行方法都工作得很好。然而，正如我们在这节中看到的，我们并不总是能够做到这一点。这就是管道并行可以发挥作用的地方。在本节中，我们将学习管道并行，这是一种在多个GPU上分布式训练大型模型的训练方法。
- en: To understand pipeline parallelism, let’s first take a brief look at model parallelism.
    This little detour will make the jump to pipeline parallelism easier.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解管道并行，我们先简要看看模型并行。这个小插曲会让过渡到管道并行变得更容易。
- en: Model parallelism
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行
- en: The idea of model parallelism is to split a neural network into smaller subnets
    and run each subnet on different GPUs. Figure 4.6 illustrates the model parallelism
    approach.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行的想法是将神经网络拆分成更小的子网络，并在不同的GPU上运行每个子网络。图4.6展示了模型并行的方法。
- en: '![](../Images/04-06.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6](../Images/04-06.png)'
- en: Figure 4.6 Split a four-layer, fully connected deep learning network into four
    subgroups; each group has one layer, and each subgroup runs on one GPU.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 将一个四层、全连接的深度学习网络拆分为四个子组；每个组有一个层，每个子组运行在一个GPU上。
- en: Figure 4.6 visualizes the model parallel process. It first converts a neural
    network (four layers) into four sub–neural networks (single layer) and then assigns
    each single-layer network a dedicated GPU. By doing so, we run a model distributedly
    on four GPUs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6可视化模型并行过程。它首先将一个神经网络（四层）转换为四个子-神经网络（单层），然后为每个单层网络分配一个专用的GPU。通过这样做，我们在四个GPU上分布式运行模型。
- en: The concept of model parallelism is straightforward, but the actual implementation
    can be tricky; it depends on the architecture of the network. To give you an idea,
    the following listing is a piece of dummy PyTorch code that makes a network run
    on two GPUs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行的概念很简单，但实际的实现可能很复杂；它取决于网络的架构。为了给你一个概念，以下列表是一段模拟PyTorch代码，它使网络在两个GPU上运行。
- en: Listing 4.2 A sample model parallelism implementation in PyTorch
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 PyTorch中一个示例模型并行实现
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see in listing 4.2, two subnetworks are initialized and assigned
    to two GPUs in the `__init__` function, and then they are connected in the `forward`
    function. Because of the variety of structures of deep learning networks, no general
    method (paradigm) exists to split the network. We must implement model parallelism
    case by case.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在列表4.2中看到的，两个子网络在`__init__`函数中被初始化并分配到两个GPU上，然后在`forward`函数中连接起来。由于深度学习网络结构的多样性，不存在通用的方法（范式）来分割网络。我们必须逐个实现模型并行。
- en: Another problem with model parallelism is its severe underutilization of GPU
    resources. Because all the devices in the training group have sequential dependency,
    only one device can work at a time, which wastes a lot of GPU cycles. Figure 4.7
    visualizes the GPU utilization situation for model parallel training with three
    GPUs.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行还存在另一个问题，即其对GPU资源的严重低利用率。因为训练组中的所有设备都有顺序依赖性，一次只能有一个设备工作，这浪费了很多GPU周期。图4.7可视化了使用三个GPU进行模型并行训练的GPU利用率情况。
- en: '![](../Images/04-07.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-07.png)'
- en: Figure 4.7 Model parallel training can have severely low GPU usage. In this
    approach, the network is split into three subnets and runs on three GPUs. Because
    of the sequential dependency among the three GPUs, each GPU is idle 66% of the
    training time.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 模型并行训练的GPU利用率可能会非常低。在这种方法中，网络被分成三个子网，并在三个GPU上运行。由于三个GPU之间的顺序依赖性，每个GPU在训练时间中有66%处于空闲状态。
- en: Let’s walk through this figure to see why GPU usage is so low. On the left,
    in figure 4.7 (a), we see the model parallel design. We split a model network
    into three subnetworks and let each subnetwork run on a different GPU. In each
    training iteration, when running the forward pass, we first compute subnet 1 and
    then subnet 2 and subnet 3; when running the backward pass, the gradient update
    happens reversely.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过这张图来了解为什么GPU利用率如此低。在左侧，图4.7（a）中，我们看到模型并行设计。我们将模型网络分成三个子网络，并让每个子网络在不同的GPU上运行。在每个训练迭代中，当运行正向传递时，我们首先计算子网1，然后是子网2和子网3；当运行反向传递时，梯度更新是相反的。
- en: 'In figure 4.7 (b), on the right, you can see the resource utilization of the
    three GPUs during the training. The time axis is divided into two parts: the forward
    pass and the backward pass. The forward pass means the computation of the model
    inference, from GPU 1 to GPU 2 and GPU3, whereas the backward pass means backpropagation
    for the model weights update, from GPU 3 to GPU 2 and GPU 1.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.7（b）的右侧，你可以看到训练过程中三个GPU的资源利用率。时间轴分为两部分：正向传递和反向传递。正向传递意味着模型推理的计算，从GPU 1到GPU
    2和GPU 3，而反向传递则意味着模型权重的反向传播，从GPU 3到GPU 2和GPU 1。
- en: If you look vertically at the time bar, regardless of whether it’s a forward
    pass or a backward pass, you see only one GPU active at a time. This is because
    of the sequential dependency between each subnet. For instance, in the forward
    pass, subnet 2 needs to wait for subnet 1’s output to fulfill its own forward
    calculation, so GPU 2 will be idle in the forward pass until the calculation on
    GPU 1 completes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你垂直观察时间条，无论它是正向传递还是反向传递，你一次只能看到一个GPU处于活动状态。这是因为每个子网之间的顺序依赖性。例如，在正向传递中，子网2需要等待子网1的输出来完成自己的正向计算，因此GPU
    2在正向传递中将会空闲，直到GPU 1上的计算完成。
- en: No matter how many GPUs you add, only one GPU can work at one time, which is
    a huge waste. This is when pipeline parallelism comes in handy. Pipeline parallelism
    makes model training more efficient by eliminating that waste and fully saturating
    the GPUs. Let’s see how it works.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你添加多少个GPU，一次只能有一个GPU在工作，这是一个巨大的浪费。这就是管道并行性发挥作用的时候。管道并行性通过消除这种浪费并完全饱和GPU来提高模型训练的效率。让我们看看它是如何工作的。
- en: Pipeline parallelism
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行性
- en: Pipeline parallelism is essentially an improved version of model parallelism.
    In addition to partitioning a network to different GPUs, it also divides each
    training example batch into small minibatches and overlaps computations of these
    minibatches between layers. By doing so, it keeps all GPUs busy most of the time,
    thus improving GPU utilization.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行性本质上是对模型并行的改进版本。除了将网络划分到不同的GPU上，它还将每个训练示例批次划分为小批，并在层之间重叠这些小批的计算。通过这样做，它使所有GPU大部分时间都处于忙碌状态，从而提高了GPU的利用率。
- en: 'There are two major implementations of this approach: PipeDream (Microsoft)
    and GPipe (Google). We use GPipe as the demo example here because it optimizes
    the gradients’ update in each training step and has better training throughput.
    You can find further details about GPipe from “GPipe: Easy scaling with micro-batch
    pipeline parallelism,” by Huang et al. ([https://arxiv.org/abs/1811.06965](https://arxiv.org/abs/1811.06965)).
    Let’s look, in figure 4.8, at how GPipe works at a high level.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '这种方法有两种主要实现：PipeDream（微软）和GPipe（谷歌）。在这里我们使用GPipe作为演示示例，因为它优化了每个训练步骤中的梯度更新，并且有更好的训练吞吐量。你可以从“GPipe:
    Easy scaling with micro-batch pipeline parallelism”这篇文章中找到关于GPipe的更多细节，作者为Huang等人（[https://arxiv.org/abs/1811.06965](https://arxiv.org/abs/1811.06965)）。让我们在图4.8中看看GPipe在高级别是如何工作的。'
- en: '![](../Images/04-08.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-08.png)'
- en: 'Figure 4.8 (a) An example neural network with sequential layers is partitioned
    across four accelerators. F[k] is the composite forward computation function of
    the *k*th cell. Bk is the backpropagation function, which depends on both B[k+1],
    from the upper layer, and F[k]. (b) The naive model parallelism strategy leads
    to severe under utilization due to the sequential dependency of the network. (c)
    Pipeline parallelism divides the input minibatch into smaller microbatches, enabling
    different accelerators to work on different microbatches simultaneously. Gradients
    are applied synchronously at the end. (Source: figure 2, “GPipe: Easy Scaling
    with Micro-Batch Pipeline Parallelism,” Huang et al., 2019, arXiv:1811.06965)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.8 (a) 一个具有顺序层的示例神经网络被分配到四个加速器上。F[k] 是第 *k* 个单元的复合前向计算函数。Bk 是反向传播函数，它依赖于来自上层
    B[k+1] 和 F[k]。 (b) 原始模型并行性策略由于网络的顺序依赖性而导致严重低效。 (c) 管道并行性将输入小批量划分为更小的微批量，使得不同的加速器可以同时处理不同的微批量。梯度在最后同步应用。(来源：图
    2，“GPipe: 使用微批量管道并行性轻松扩展”，黄等人，2019，arXiv:1811.06965)'
- en: Figure 4.8 (a) depicts a neural network made of four subnetworks; each subnetwork
    is loaded on one GPU. F means forward pass, B means backward pass, and F[k] and
    B[k] run on GPUk. The training sequence is first, forward pass, F[0] -> F[1] ->
    F[2] -> F[3], and second, backward pass, F[3] -> (B[3], F[2]) -> (B[2], F[2])
    -> (B[1], F[1]) -> B[0].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 (a) 描述了一个由四个子网络组成的神经网络；每个子网络加载在一个 GPU 上。F 表示前向传递，B 表示反向传递，F[k] 和 B[k]
    在 GPUk 上运行。训练顺序首先是前向传递，F[0] -> F[1] -> F[2] -> F[3]，然后是反向传递，F[3] -> (B[3], F[2])
    -> (B[2], F[2]) -> (B[1], F[1]) -> B[0]。
- en: Figure 4.8 (b) displays the training flow for naive model parallelism. We can
    see that the GPU is seriously underutilized; only one GPU is activated in the
    forward- and backward pass; thus, each GPU is idle 75% of the time.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 (b) 显示了原始模型并行性的训练流程。我们可以看到 GPU 严重未被充分利用；前向和反向传递中只有一个 GPU 被激活；因此，每个 GPU
    有 75% 的时间处于空闲状态。
- en: Figure 4.8 (c) shows the GPipe improvements in the sequence of training operations.
    GPipe first divides every training example batch into four equal microbatches,
    which are pipelined through the four GPUs. F[(0,2)] in the graph means forward
    pass computation at GPU 0 with minibatch 2\. During the backward pass, gradients
    for each microbatch are computed based on the same model parameters used for the
    forward pass. The key is that it doesn’t update model parameters immediately;
    instead, it accumulates all the gradients for each microbatch. At the end of each
    training batch, we use the accumulated gradients from all four microbatches to
    update the model parameters across all four GPUs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 (c) 展示了训练操作序列中的 GPipe 改进。GPipe 首先将每个训练示例批量划分为四个相等的微批量，这些微批量通过四个 GPU 进行管道化。图中的
    F[(0,2)] 表示 GPU 0 上的前向传递计算，小批量号为 2。在反向传递期间，每个微批量的梯度是基于用于前向传递的相同模型参数计算的。关键是它不会立即更新模型参数；相反，它累积每个微批量的所有梯度。在每个训练批量结束时，我们使用来自所有四个微批量的累积梯度来更新所有四个
    GPU 上的模型参数。
- en: 'By comparing figure 4.8 (b) and (c), we see the GPU utilization increase greatly;
    now each GPU is idle 47% of the time. Let’s see a code example using PyTorch GPipe
    implementation to train a transformer model on two GPUs (see following listing).
    To demo the idea clearly, we keep only the pipeline-related code and partition
    them into four parts. You can check out the tutorial “PyTorch: Training transformer
    models using pipeline parallelism,” by Pritam Damania, for the full code ([http://mng.bz/5mD8](http://mng.bz/5mD8)).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '通过比较图 4.8 (b) 和 (c)，我们看到 GPU 利用率显著提高；现在每个 GPU 有 47% 的时间处于空闲状态。让我们看看使用 PyTorch
    GPipe 实现训练转换器模型在两个 GPU 上的代码示例（见以下列表）。为了清楚地演示这个想法，我们只保留与管道相关的代码，并将它们分为四个部分。您可以查看
    Pritam Damania 的教程“PyTorch: 使用管道并行性训练转换器模型”，以获取完整代码([http://mng.bz/5mD8](http://mng.bz/5mD8))。'
- en: Listing 4.3 Training transformer models using pipeline parallelism
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 使用管道并行性训练转换器模型
- en: '[PRE16]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see from listing 4.3, pipeline parallelism code is much more complicated
    than distributed data parallelism. Besides setting up the communication group,
    we also need to consider how to divide our model network and transfer gradients
    and activation (model the subnetwork’s forward output) in interworker communication.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表 4.3 中我们可以看出，管道并行性代码比分布式数据并行性代码复杂得多。除了设置通信组之外，我们还需要考虑如何划分我们的模型网络，以及在跨工作进程通信中传输梯度和激活（对子网络的正向输出建模）。
- en: 4.4.3 How software engineers can support pipeline parallelism
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 软件工程师如何支持管道并行性
- en: You may have noticed that all the methods we talk about in this section are
    techniques for writing training code. Because data scientists normally write the
    training code, you may be wondering what we, as software developers, can do to
    support pipeline parallel training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，本节中我们讨论的所有方法都是编写训练代码的技术。因为数据科学家通常编写训练代码，你可能想知道作为软件开发者，我们能做些什么来支持管道并行训练。
- en: First, we can work on building the training service to automate the pipeline
    training execution and improve resource utilization (for example, always keeping
    the GPU busy). This automation includes matters like allocating worker resources,
    enabling interworker communication, and distributing the pipeline training code
    with corresponding initialized parameters to each worker (such as worker IP address,
    process ID, GPU ID, and worker group size).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以致力于构建训练服务来自动化管道训练执行并提高资源利用率（例如，始终保持GPU忙碌）。这种自动化包括诸如分配工作资源、启用工作间通信以及将带有相应初始化参数的管道训练代码分发到每个工作节点（如工作节点IP地址、进程ID、GPU
    ID和工作节点组大小）等事项。
- en: Second, we can alert the data scientist team about the new distributed training
    options. Sometimes the data scientist team isn’t aware of the new engineering
    methods that can improve the model training experience, so communication is key
    here. We can collaborate with members of the team and lead the conversation about
    experimenting with the pipeline parallelism method.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以提醒数据科学家团队关于新的分布式训练选项。有时数据科学家团队并不了解可以改善模型训练体验的新工程方法，因此沟通在这里至关重要。我们可以与团队成员合作，并引导关于实验管道并行方法对话。
- en: Third, we can work on improving the availability of model training. In section
    4.2.4, we discussed that distributed training is fragile; it requires every worker
    to perform consistently. If one worker fails, the entire training group fails,
    which is a huge waste of time and budget. The effort spent on training-process
    monitoring, failover, and failure recovery would be much appreciated by data scientists.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们可以致力于提高模型训练的可用性。在第4.2.4节中，我们讨论了分布式训练是脆弱的；它要求每个工作节点都能保持一致。如果一个工作节点失败，整个训练组都会失败，这将造成巨大的时间和预算浪费。数据科学家将非常感激在训练过程监控、故障转移和故障恢复上所付出的努力。
- en: Data parallelism or pipeline parallelism?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行或管道并行？
- en: 'Now we know that there are two major strategies for distributed training: data
    parallelism and pipeline parallelism. You might understand these concepts, but
    you might still be uncertain about when to use them.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，分布式训练有两种主要策略：数据并行和管道并行。你可能已经理解了这些概念，但你可能仍然不确定何时使用它们。
- en: We would suggest always starting with model training on a single machine. If
    you have a large dataset and the training takes a long time, then consider distributed
    training. We always prefer data parallelism over pipeline parallelism merely because
    data parallelism is simpler to implement and we can obtain results quicker. If
    the model is so big that it can’t load on one GPU, then pipeline parallelism is
    the right choice.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议始终从单机上的模型训练开始。如果你有一个大型数据集，并且训练需要很长时间，那么考虑分布式训练。我们总是优先选择数据并行而不是管道并行，仅仅因为数据并行更容易实现，我们可以更快地获得结果。如果模型太大，无法加载到一个GPU上，那么管道并行是正确的选择。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Distributed training has two schools of thought: data parallelism and model
    parallelism. Pipeline parallelism is an improved version of model parallelism.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式训练有两种主要思想：数据并行和模型并行。管道并行是模型并行的改进版本。
- en: If a model can be loaded into one GPU, data parallelism is the primary method
    to implement distributed training; it’s simple to use and provides great speed
    improvements.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个模型可以加载到一个GPU上，数据并行是实施分布式训练的主要方法；它使用简单，提供了极大的速度提升。
- en: Using Kubernetes to manage the computing cluster can greatly reduce the complexity
    of compute resource management.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes来管理计算集群可以大大降低计算资源管理的复杂性。
- en: Although each training framework (TensorFlow, PyTorch) offers different configurations
    and APIs to write distributed training code, their code pattern and execution
    workflow are very similar. Thus, a training service can support the various distributed
    training codes with a unified approach.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管每个训练框架（TensorFlow、PyTorch）都提供了不同的配置和API来编写分布式训练代码，但它们的代码模式和执行流程非常相似。因此，训练服务可以使用统一的方法支持各种分布式训练代码。
- en: After encapsulating the setup configuration of various training frameworks,
    a training service can still treat training code as a black box, even in the distributed
    training setting.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在封装了各种训练框架的设置配置之后，即使在分布式训练设置中，训练服务仍然可以将训练代码视为黑盒。
- en: To obtain data parallelism training progress/status, you only need to check
    the master worker because all workers are always in sync with each other. Also,
    to avoid saving duplicated models from all workers when their training jobs complete,
    you can set the training code to persist model and checkpoint files only when
    the code is executed by the master worker.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获取数据并行训练的进度/状态，你只需检查主工作节点，因为所有工作节点总是彼此同步。此外，为了避免在训练作业完成后从所有工作节点保存重复的模型，你可以设置训练代码仅在主工作节点执行时持久化模型和检查点文件。
- en: 'Horovod is a great distributed training framework. It offers a unified method
    to run distributed training for code written in various frameworks: PyTorch, TensorFlow,
    MXNet, and PySpark. If a training code uses Horovod to implement distributed training,
    a training service can use a single method (the Horovod method) to execute it,
    regardless of the training frame with which it’s written.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horovod是一个优秀的分布式训练框架。它为使用各种框架（PyTorch、TensorFlow、MXNet和PySpark）编写的代码提供了统一的分布式训练运行方法。如果训练代码使用Horovod来实现分布式训练，训练服务可以使用单一方法（Horovod方法）来执行它，无论其编写的是哪种训练框架。
- en: Availability, resilience, and failure recovery are important engineering concerns
    for distributed training.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性、弹性和故障恢复是分布式训练中的重要工程问题。
- en: 'There are two strategies for training a model that does not fit into one GPU:
    the memory-saving method and the model parallelism method.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于无法适应单个GPU的模型，有两种训练策略：内存节省方法和模型并行方法。
- en: The memory-saving method loads only a portion of the model or a small data batch
    to GPU at a time—for instance, gradient accumulation and memory swap. These methods
    are easy to implement but slow down the model training process.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存节省方法一次只将模型的一部分或小数据批次加载到GPU上——例如，梯度累积和内存交换。这些方法易于实现，但会减慢模型训练过程。
- en: The model parallelism method divides a large model into a group of sub–neural
    networks and distributes them onto multiple GPUs. The downside of this approach
    is low GPU utilization. To overcome that, pipeline model parallelism was invented.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型并行方法将大型模型划分为一组子-神经网络，并将它们分布到多个GPU上。这种方法的不利之处在于GPU利用率低。为了克服这一点，发明了流水线模型并行。
