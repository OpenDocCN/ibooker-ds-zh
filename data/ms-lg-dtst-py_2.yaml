- en: Part 3\.
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3部分。
- en: '[Part 3](#part03) explains how to bring the tools and techniques we’ve covered
    throughout this book into the cloud. We’ll cover the fundamentals of cloud computing,
    object storage in the cloud, and how to set up your own computing clusters in
    the cloud. Through hands-on examples, we’ll run the distributed computing frameworks
    covered in [Part 2](kindle_split_016.html#part02)—Hadoop and Spark—in the cloud.
    This part focuses on large data category 3: data that is too big for either storing
    or processing locally. Once you’ve mastered the content in this chapter, you’ll
    be able to tackle data of any size.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3部分](#part03)解释了如何将本书中涵盖的工具和技术带入云中。我们将涵盖云计算的基本知识、云中的对象存储以及如何在云中设置自己的计算集群。通过实际示例，我们将在云中运行第[2部分](kindle_split_016.html#part02)中涵盖的分布式计算框架——Hadoop和Spark。本部分专注于大型数据类别3：太大以至于无法在本地存储或处理的数据。一旦你掌握了本章的内容，你将能够处理任何大小的数据。'
- en: Chapter 11\. Large datasets in the cloud with Amazon Web Services and S3
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章\. 使用Amazon Web Services和S3在云中处理大型数据集
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding distributed object storage in the cloud
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解云中的分布式对象存储
- en: Using the AWS web interface to set up buckets and upload objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS网络界面设置存储桶和上传对象
- en: Working with the boto3 library to upload data to an S3 bucket
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用boto3库将数据上传到S3存储桶
- en: In [chapters 7](kindle_split_017.html#ch07)–[10](kindle_split_020.html#ch10),
    we saw the power of the distributed frameworks in Hadoop and Spark. These frameworks
    can take advantage of clusters of computers to parallelize massive data processing
    tasks and complete them in short order. Most of us, however, don’t have access
    to physical compute clusters.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[7](kindle_split_017.html#ch07)章至[10](kindle_split_020.html#ch10)章中，我们看到了Hadoop和Spark分布式框架的力量。这些框架可以利用计算机集群并行处理大量数据处理任务并在短时间内完成。然而，我们大多数人没有访问物理计算集群的权限。
- en: In contrast, we can all get access to compute clusters from cloud service providers
    such as Amazon, Microsoft, and Google. These cloud providers have platforms that
    we can use for storing and processing data, along with a variety of services that
    automate common tasks we may want to do. In this chapter, we’ll take the first
    step of analyzing big data in the cloud by uploading data to Amazon’s Simple Storage
    Service (S3). First, we’ll review the basics of S3; then we’ll create a bucket
    and upload an object using the browser-based AWS console; and finally we’ll upload
    several objects to a bucket with the boto3 software development kit.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们可以从亚马逊、微软和谷歌等云服务提供商那里获取计算集群的访问权限。这些云提供商拥有我们可以用于存储和处理数据的平台，以及一系列自动化我们可能想要执行的任务的服务。在本章中，我们将通过将数据上传到亚马逊的简单存储服务（S3）来迈出分析云中大数据的第一步。首先，我们将回顾S3的基本知识；然后我们将使用基于浏览器的AWS控制台创建存储桶并上传对象；最后，我们将使用boto3软件开发工具包将多个对象上传到存储桶。
- en: 11.1\. AWS Simple Storage Service—A solution for large datasets
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1\. AWS简单存储服务——大型数据集的解决方案
- en: 'Amazon Web Service’s Simple Storage Service, better known as S3, is a data
    storage service used to hold some of the largest datasets, such as the datasets
    of General Electric, NASA, Netflix, the UK Data Service, Yelp, and—of course—Amazon
    itself. S3 is the go-to service for large datasets for the following five reasons:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊网络服务的简单存储服务，更广为人知的是S3，是一种数据存储服务，用于存储一些最大的数据集，例如通用电气、NASA、Netflix、英国数据服务、Yelp以及当然还有亚马逊本身的数据集。S3是大型数据集的首选服务，以下五个原因：
- en: '***S3 has effectively unlimited storage capacity.*** We never have to worry
    about our dataset becoming too large.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***S3具有无限大的存储容量。*** 我们永远不必担心我们的数据集变得过大。'
- en: '***S3 is cloud-based.*** We can scale up and down quickly as necessary.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***S3是基于云的。*** 我们可以根据需要快速扩展或缩减。'
- en: '***S3 offers object storage.*** We can focus on organizing our data with metadata
    and store many different types of data.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***S3提供对象存储。*** 我们可以专注于使用元数据组织我们的数据，并存储许多不同类型的数据。'
- en: '***S3 is a managed service.*** Amazon Web Services takes care of a lot of the
    details for us, such as ensuring data availability and durability. They also take
    care of security patches and software updates.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***S3是一项托管服务。*** 亚马逊网络服务为我们处理了很多细节，例如确保数据可用性和持久性。他们还负责安全补丁和软件更新。'
- en: '***S3 supports versioning and life cycle policies.*** We can use them to update
    or archive our data as it ages.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***S3支持版本控制和生命周期策略。*** 我们可以利用它们来更新或归档随着时间推移的数据。'
- en: '|  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Cloud Options: AWS, Azure, and Google Cloud'
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 云选项：AWS、Azure和Google Cloud
- en: The three prominent cloud providers—Amazon (AWS), Microsoft (Azure), and Google
    (Google Cloud)—all offer a standard suite of core services. The core services
    include virtual machines for computing and object-based storage. In this chapter,
    I’ll go into detail on Amazon’s S3 service because AWS is the most popular of
    the cloud platforms. That said, the principles in this chapter apply to all the
    object storage systems of the three cloud providers. Indeed, we can use everything
    in this chapter and [chapter 12](kindle_split_023.html#ch12) on Microsoft Azure
    and Google Cloud.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 三大主要的云服务提供商——亚马逊(AWS)、微软(Azure)和谷歌(Google Cloud)——都提供了一套标准的核心服务。核心服务包括用于计算的虚拟机和基于对象的存储。在本章中，我将详细介绍亚马逊的S3服务，因为AWS是最受欢迎的云平台。话虽如此，本章中的原则适用于三大云提供商的所有对象存储系统。实际上，我们可以使用本章和[第12章](kindle_split_023.html#ch12)中关于微软Azure和谷歌云的所有内容。
- en: '|  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: In this section, I’ll go over the five advantages to using S3 and, in the process,
    explain what S3 is and how it works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍使用S3的五个优点，并在过程中解释S3是什么以及它是如何工作的。
- en: 11.1.1\. Limitless storage with S3
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.1\. 使用S3实现无限存储
- en: When a dataset becomes so big that we start to worry about where and how to
    store it, we know we’re dealing with a large dataset. For these situations, S3
    is always an option because it allows for effectively limitless (but potentially
    costly) storage ([figure 11.1](#ch11fig01)). In fact, S3 is such a good option
    for large datasets, AWS even has a service designed to help organizations migrate
    local petabyte-scale datasets into S3.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集变得如此之大，以至于我们开始担心存储的位置和方法时，我们知道我们正在处理一个大型数据集。对于这些情况，S3始终是一个选择，因为它提供了实际上无限的（但可能昂贵的）存储空间([图11.1](#ch11fig01))。事实上，S3对于大型数据集来说是一个如此好的选择，以至于AWS甚至有一个服务旨在帮助组织将本地PB级数据集迁移到S3。
- en: Figure 11.1\. Because AWS data centers are so large in proportion to the size
    of our data, S3 offers effectively limitless storage.
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1\. 由于AWS数据中心相对于我们数据的大小比例很大，S3提供了实际上无限的存储空间。
- en: '![](11fig01_alt.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig01_alt.jpg)'
- en: What makes S3 effectively limitless? Large data centers with lots of disk space.
    There’s no secret when it comes to cloud-based storage. AWS stores the data on
    disk volumes much like you would if you were to store it locally. What makes it
    so appealing for us is that instead of us buying the disk space and managing it
    ourselves, AWS is willing to rent it to us. And when we need more, AWS is willing
    to rent us more.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: S3为何能实现无限存储？这是因为拥有大量磁盘空间的大型数据中心。在基于云的存储方面，这并没有什么秘密。AWS将数据存储在磁盘卷上，就像您本地存储数据一样。对我们来说，它之所以如此吸引人，是因为我们不必购买磁盘空间并自行管理，AWS愿意将其租给我们。而且当我们需要更多时，AWS也愿意租给我们更多。
- en: 11.1.2\. Cloud-based storage for scalability
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.2\. 可扩展性的云存储
- en: Because S3 is a cloud-based storage service, we get scalability benefits that
    we wouldn’t get if we were storing the data ourselves. In the cloud, we never
    need to buy more physical storage devices, we only need to pay for more of the
    storage service. And we can purchase more of that service anytime we want and
    give it up anytime we want ([figure 11.2](#ch11fig02)). AWS refers to this as
    *elasticity*, and others refer to it as *scalability*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于S3是一种基于云的存储服务，我们获得了如果自己存储数据所无法获得的可扩展性优势。在云中，我们永远不需要购买更多的物理存储设备，我们只需要为更多的存储服务付费。而且我们可以在任何时候购买更多服务，也可以在任何时候放弃服务([图11.2](#ch11fig02))。AWS将这称为*弹性*，其他人则称之为*可扩展性*。
- en: Figure 11.2\. Cloud-based storage is useful when we need to be flexible, because
    the storage space is available to us on-demand as we have more and more data to
    store.
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2\. 当我们需要灵活性时，基于云的存储很有用，因为随着我们存储的数据越来越多，存储空间可以按需提供。
- en: '![](11fig02_alt.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig02_alt.jpg)'
- en: 'Consider the following scenario: you’re running a small survey company, and
    you’ve bought some storage to hold the survey data for your first few customers.
    This has a few drawbacks:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：您正在经营一家小型调查公司，您已经购买了一些存储空间来保存前几位客户的调查数据。这有几个缺点：
- en: You need to find a good way to estimate how much space the surveys will require.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要找到一种好方法来估算调查所需的存储空间。
- en: You need to pay for the storage space all at once.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要一次性支付存储空间费用。
- en: Because S3 is in the cloud, we can avoid both these problems. With S3, we pay
    for the storage we use for our data when we store it, and we can be confident
    that there will always be storage available when we’re ready to purchase it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于S3位于云中，我们可以避免这两个问题。使用S3，我们在存储数据时为使用的存储付费，并且可以确信，当我们准备购买时，总会有的存储空间可用。
- en: 'Now imagine that your first round of surveys went so well that a hospital has
    asked you to run a massive nationwide survey for them. You need to prepare to
    hold their data—and fast. You have a new challenge: you need to quickly find and
    set up a large data storage solution.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，你的第一轮调查进行得非常顺利，一家医院要求你为他们进行一次全国性的大规模调查。你需要准备保存他们的数据——而且要快。你面临一个新的挑战：你需要快速找到并设置一个大型数据存储解决方案。
- en: 'If you were storing your data in S3, the large data storage solution could
    be the same as your smaller data storage solution: it could all go in S3\. Because
    the service is limitless and available on demand, when there’s more data for us
    to store, we can pay more to store it.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据存储在S3中，大型数据存储解决方案可以与你的小型数据存储解决方案相同：所有数据都可以存放在S3中。因为这项服务是无限的，并且按需提供，当我们有更多数据需要存储时，我们可以支付更多费用来存储它。
- en: 11.1.3\. Objects for convenient heterogenous storage
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.3. 方便异构存储的对象
- en: Another advantage of S3 is that it follows the object storage paradigm. Object
    storage—as opposed to traditional file storage—is a storage pattern that focuses
    on the what of the data instead of the where. With traditional file storage, a
    file is referred to by its name and which directory it’s in. With object storage,
    we recognize objects by a unique identifier ([figure 11.3](#ch11fig03)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: S3的另一个优点是它遵循对象存储模式。对象存储——与传统的文件存储相对——是一种关注数据“是什么”而不是“在哪里”的存储模式。在传统的文件存储中，文件通过其名称和所在的目录来引用。在对象存储中，我们通过一个唯一的标识符来识别对象（[图11.3](#ch11fig03)）。
- en: Figure 11.3\. Object storage associates data with a unique identifier, which
    we can call to perform file operations on the object.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3。对象存储将数据与一个唯一的标识符关联，我们可以通过这个标识符来对对象执行文件操作。
- en: '![](11fig03_alt.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](11fig03_alt.jpg)'
- en: Because unique identifiers themselves are not usually enough to help humans
    keep track of their data, object storage supports arbitrary metadata. This means
    that we can tag our objects flexibly based on our needs. Do we need to tag data
    by day? By user or customer? By product or marketing campaign? By the tides or
    the moon? We can apply any tags we want. Additionally—though we won’t cover them
    in this book—querying tools are available for S3 that allow SQL-like querying
    of these metadata tags for metadata analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于唯一标识符本身通常不足以帮助人类跟踪他们的数据，对象存储支持任意元数据。这意味着我们可以根据我们的需求灵活地对对象进行标记。我们需要按天标记数据吗？按用户或客户？按产品或营销活动？按潮汐或月亮？我们可以应用任何我们想要的标签。此外——尽管我们不会在本书中介绍它们——S3上有可用的查询工具，允许对这些元数据标签进行类似SQL的查询，以进行元数据分析。
- en: Having unique identifiers as the approach to calling all our objects means that
    we can store heterogenous data in the same way. Say we’re running a social media
    platform, and our users are uploading pictures and videos to our website. We can
    store both of those file types in S3 and tag them with the same metadata even
    though they’re different types.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将唯一标识符作为调用所有对象的方法意味着我们可以以相同的方式存储异构数据。比如说我们正在运营一个社交媒体平台，我们的用户正在将图片和视频上传到我们的网站上。我们可以在S3中存储这两种文件类型，并使用相同的元数据对它们进行标记，尽管它们是不同类型的。
- en: 11.1.4\. Managed service for conveniently managing large datasets
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.4. 方便管理大型数据集的托管服务
- en: One problem that we’d run into if we were managing a large dataset ourselves
    is the day-to-day maintenance of the dataset. If we want our data to be highly
    available, we have to take steps to replicate the data in multiple storage environments
    while also setting up failovers, so that when our data is unavailable in one location,
    we can find it quickly in another. For large datasets, this is no trivial matter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们自己在管理大型数据集时遇到的一个问题是数据集的日常维护。如果我们希望我们的数据具有高度可用性，我们必须采取步骤在多个存储环境中复制数据，同时设置故障转移，以便当我们的数据在一个位置不可用时，我们可以在另一个位置快速找到它。对于大型数据集来说，这可不是一件小事。
- en: Because S3 is a managed service, Amazon Web Services handles all of the low-level
    implementation of our data and ensures high durability and availability. That
    means that we can expect our data to be available when we need it without having
    to think about it too much. This will free us up to do other things, like actually
    working on the large dataset we now have stored in S3.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于S3是一项托管服务，亚马逊网络服务处理我们数据的所有底层实现，并确保高可靠性和可用性。这意味着我们可以在需要时期待我们的数据可用，而无需过多考虑。这将使我们能够腾出时间来做其他事情，比如实际上处理我们现在存储在S3中的大量数据集。
- en: 11.1.5\. Life cycle policies for managing large datasets over time
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.5. 管理大型数据集的生命周期策略
- en: One of the issues we’ll have with large datasets—which we’ve already alluded
    to in this section—is that large datasets are growing datasets. Over time, our
    dataset grows larger and larger. That said, not all of the data in that large
    dataset stays relevant.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将面临的一个大问题是大型数据集——我们已经在本节中提到了这一点——大型数据集是增长的数据集。随着时间的推移，我们的数据集会越来越大。尽管如此，那个大型数据集中的所有数据并不都保持相关性。
- en: Image we’re a running subscription-based online video service. We want to store
    records of all the videos our users have watched so we can make recommendations
    to them about which other videos they may enjoy. That said, we may want to limit
    the recommendations we generate so that we’re only generating recommendations
    for currently subscribing users and only using data from the last year.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们是一家运行基于订阅的在线视频服务。我们希望存储用户观看的所有视频的记录，以便我们可以向他们推荐他们可能喜欢的其他视频。话虽如此，我们可能希望限制我们生成的推荐，以便我们只为当前订阅用户生成推荐，并且只使用过去一年的数据。
- en: One way to go about doing this would be to filter the data. We’ve used filter
    operations throughout the book—starting in [chapter 4](kindle_split_013.html#ch04)—and
    we’ve seen that they’re natural to implement with the Hadoop and Spark frameworks.
    Filtering still requires us to pay for the data to be available to us and pay
    to process it. Another option would be to archive data we know we don’t need,
    such as old log files that we wouldn’t regularly analyze.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的一种方法是对数据进行筛选。我们在整本书中使用了筛选操作——从[第4章](kindle_split_013.html#ch04)开始——我们已经看到它们与Hadoop和Spark框架的实现是自然的。筛选仍然需要我们为数据可用性付费，并为其处理付费。另一种选择是存档我们知道不需要的数据，例如我们不会定期分析的老日志文件。
- en: For this, S3 has a life cycle policy feature that we can use to make data that
    we’re unlikely to need less available and store it more cheaply. A standard approach
    ([figure 11.4](#ch11fig04)) is to
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此，S3有一个生命周期策略功能，我们可以使用它来减少我们不太可能需要的数据的可用性，并更便宜地存储它。一种标准的方法 ([图11.4](#ch11fig04))
    是
- en: start the data we have in S3 Standard
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从S3标准存储开始我们的数据
- en: then when we need it less, relegate it to S3 Infrequent Access
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后当我们不太需要它时，将其降级到S3不经常访问
- en: then when we’re ready to archive the data, move it to S3 Glacier
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后当我们准备存档数据时，将其移动到S3 Glacier
- en: Figure 11.4\. We can use the life cycle policy to ensure that old data we’re
    less likely to want to analyze costs us less, while still maintaining the same
    storage strategy.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4\. 我们可以使用生命周期策略来确保我们不太可能想要分析的老数据成本更低，同时仍然保持相同的存储策略。
- en: '![](11fig04_alt.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig04_alt.jpg)'
- en: The different storage formats all have different cost structures. [Table 11.1](#ch11table01)
    summarizes the differences between the storage classes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的存储格式都有不同的成本结构。[表11.1](#ch11table01) 总结了存储类别之间的差异。
- en: Table 11.1\. Three major S3 storage classes are available, depending on how
    often you need to access your data.
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.1\. 根据您需要访问数据频率的不同，S3提供了三种主要的存储类别。
- en: '| S3 storage class | Cost to store | Cost to use | Availability |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| S3存储类别 | 存储成本 | 使用成本 | 可用性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| S3 Standard | Low | Very low | Very high |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| S3标准存储 | 低 | 非常低 | 非常高 |'
- en: '| S3 Infrequent Access | Very Low | Low | Very high |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| S3不经常访问存储 | 非常低 | 低 | 非常高 |'
- en: '| S3 Glacier | Lowest | ^([[*](#ch11table01tn01)])Medium | Low |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| S3 Glacier | 最低 | ^([[*](#ch11table01tn01)])中等 | 低 |'
- en: ^*
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^*
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Includes the cost of moving an object from S3 Glacier to another S3 format before
    use
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 包括在使用前将对象从S3 Glacier移动到另一个S3格式的成本
- en: S3 Standard has the greatest storage cost but the lowest per-transaction cost,
    which is great when we have data we’ll be using a lot. S3 Infrequent Access has
    lower storage costs than S3 but greater transaction costs—storing data in this
    format is cost-effective when we’ll be accessing the data less, but we want it
    available for when we do need it. S3 Glacier has the lowest storage cost but must
    be elevated to another S3 type for us to use it. The time it takes to do this
    can be adjusted in the range of several minutes to several hours.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: S3标准存储的存储成本最高，但每笔交易的成本最低，当我们有大量数据需要使用时，这非常理想。S3不经常访问存储的成本低于S3，但交易成本更高——当我们不太可能访问数据，但希望数据可用时，以这种格式存储数据是成本效益的。S3
    Glacier的存储成本最低，但必须提升到另一种S3类型才能使用。完成这一过程所需的时间可以在几分钟到几小时之间调整。
- en: In general, using S3 Standard is fine. I recommend using S3 Infrequent Access
    and S3 Glacier only if you have specific needs. For example, if you know you’ll
    only need to analyze the data once each month, you could consider storing it in
    S3 Infrequent Access. If you need the data only for quarterly or annual analysis
    and can plan ahead, you may want to use S3 Glacier for cost savings.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用S3标准版就足够了。我建议只有在您有特定需求时才使用S3低频访问和S3冰川。例如，如果您知道您每个月只需要分析一次数据，您可以考虑将其存储在S3低频访问中。如果您只需要数据用于季度或年度分析，并且可以提前规划，您可能希望使用S3冰川以节省成本。
- en: 11.2\. Storing data in the cloud with S3
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 使用S3在云中存储数据
- en: S3 is a place we can store large datasets. In this section, we’ll go over two
    ways we can store that data by using
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: S3是我们可以存储大型数据集的地方。在本节中，我们将介绍两种我们可以使用的方法来存储这些数据
- en: a browser-based graphical interface
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于浏览器的图形界面
- en: the boto Amazon Web Services/Python Software Development Kit (SDK)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: boto Amazon Web Services/Python软件开发工具包（SDK）
- en: The browser-based interface is a convenient and user-friendly way to upload
    data and manage metadata. We can use the Python SDK library, boto, to harness
    the full power of Python and embed S3 actions in our scripts and software.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于浏览器的界面是上传数据和管理工作元数据的便捷且用户友好的方式。我们可以使用Python SDK库boto，利用Python的全部力量，并在我们的脚本和软件中嵌入S3操作。
- en: 11.2.1\. Storing data with S3 through the browser
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1\. 通过浏览器使用S3存储数据
- en: We’ll start with learning how to store data in S3 through the browser. The browser-based
    interface to S3 *buckets* offers some advantages over the programmatic SDK access
    we’ll look at later. In particular, the browser
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从学习如何通过浏览器在S3中存储数据开始。S3的基于浏览器的界面与我们将稍后查看的程序化SDK访问相比，提供了一些优势。特别是，浏览器
- en: provides visuals queues that aid in understanding the concepts of S3 storage
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供视觉队列，有助于理解S3存储的概念
- en: has wizards that enumerate the available options
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有向导列出可用的选项
- en: These advantages make the browser-based interface a good option for getting
    used to S3 storage.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优势使得基于浏览器的界面成为熟悉S3存储的一个好选择。
- en: 'Loading data into S3 is a two-step process:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到S3是一个两步过程：
- en: Set up a bucket—a place to store the data.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个存储桶——存储数据的地方。
- en: Upload an object—a piece of data to be stored.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传一个对象——要存储的数据。
- en: We’ll tackle these steps in order. First, we’ll set up a bucket and talk about
    the options available to us there; then we’ll upload an object and talk about
    object-level options.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按顺序处理这些步骤。首先，我们将设置一个存储桶，并讨论我们在这里可用的选项；然后我们将上传一个对象，并讨论对象级别的选项。
- en: Setting up buckets in S3
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在S3中设置存储桶
- en: Buckets are areas in S3 where we can store data. When we upload data to S3,
    we upload that data to a specific bucket. When the object is uploaded, it becomes
    accessible to only those who have access to the bucket. This makes buckets a great
    way to separate our data and control access to it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 存储桶是S3中的区域，我们可以在这里存储数据。当我们上传数据到S3时，我们将数据上传到特定的存储桶。当对象上传后，它只能被有权访问该存储桶的人访问。这使得存储桶成为分离我们的数据和控制对其访问的绝佳方式。
- en: '|  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Working on AWS
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在AWS上工作
- en: In this section through the rest of the book, the exercises involve using live
    Amazon Web Services resources. These services are a business for Amazon. To follow
    along, you’ll need to set up an AWS account with a credit card, debit card, or
    prepaid cash card. The resources needed for the examples in [chapters 11](#ch11)
    and [12](kindle_split_023.html#ch12) cost less than $5 as I’m writing this. To
    conserve cost, make sure you shut down all your compute resources when you no
    longer need them. Idle compute clusters can quickly raise the cost of using AWS.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节以及本书的其余部分，练习涉及使用真实的亚马逊网络服务资源。这些服务是亚马逊的业务。为了跟上进度，您需要使用信用卡、借记卡或预付现金卡设置一个AWS账户。我在编写此内容时，[第11章](#ch11)和[第12章](kindle_split_023.html#ch12)中的示例所需资源成本不到5美元。为了节省成本，确保您在不再需要时关闭所有计算资源。闲置的计算集群会迅速提高使用AWS的成本。
- en: '|  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: For example, imagine we’re an airline and we have an application that allows
    users to see where all our planes are flying at any given time. We might want
    to store full flight location logs in S3 so that we can access this data for future
    analysis. At the same time, we want to keep curious third parties—potentially
    our competitors—from downloading our data. Buckets and their privacy controls
    allow us to limit access to such data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们是一家航空公司，我们有一个允许用户查看我们所有飞机在任何给定时间飞往何处的应用程序。我们可能希望将完整的飞行位置日志存储在 S3 中，以便我们可以访问这些数据进行未来的分析。同时，我们希望防止好奇的第三方——可能是我们的竞争对手——下载我们的数据。桶及其隐私控制允许我们限制对这类数据的访问。
- en: To start setting up a bucket, we’ll need to navigate to the S3 page in AWS.
    We can find Amazon Web Services at [https://aws.amazon.com](https://aws.amazon.com).
    From there, you can click the Services drop-down menu in the upper left corner
    of the screen and select S3, which you can find under Storage. Additionally, we
    could search for S3 in the Services search ([figure 11.5](#ch11fig05)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始设置存储桶，我们需要导航到 AWS 中的 S3 页面。您可以在 [https://aws.amazon.com](https://aws.amazon.com)
    找到亚马逊网络服务。从那里，您可以在屏幕左上角点击服务下拉菜单并选择 S3，您可以在存储下找到它。此外，我们还可以在服务搜索中搜索 S3（[图 11.5](#ch11fig05)）。
- en: Figure 11.5\. To navigate to the S3 landing page, we can always use the Services
    navigation drop-down menu and either do a search or find S3 under Storage.
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.5\. 要导航到 S3 登录页面，我们始终可以使用服务导航下拉菜单，进行搜索或在存储下找到 S3。
- en: '![](11fig05_alt.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](11fig05_alt.jpg)'
- en: This will take us off of the main AWS landing page and onto the S3 landing page.
    This page will list the buckets we have once we have one or more buckets set up.
    For now, though, it offers us a search bar and a button that we can click to launch
    the S3 Create Bucket wizard ([figure 11.6](#ch11fig06)). Click that button.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带我们离开主要的 AWS 登录页面，进入 S3 登录页面。一旦我们设置了一个或多个桶，此页面将列出我们拥有的桶。目前，它提供了一个搜索栏和一个我们可以点击以启动
    S3 创建桶向导的按钮（[图 11.6](#ch11fig06)）。点击该按钮。
- en: Figure 11.6\. The S3 Create Bucket wizard and bucket search are available from
    the S3 landing page.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.6\. S3 创建桶向导和桶搜索都可在 S3 登录页面找到。
- en: '![](11fig06_alt.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](11fig06_alt.jpg)'
- en: Once we enter the Create Bucket wizard, it will walk us through the options
    for setting up an S3 bucket. The first two options we’ll face are deciding on
    a bucket name and selecting a region for our bucket. The name of our S3 bucket
    has several restrictions. The three major restrictions for S3 bucket names are
    that they
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进入创建桶向导，它将引导我们通过设置 S3 桶的选项。我们将面临的前两个选项是决定桶名并选择桶的区域。我们的 S3 桶名有几个限制。S3 桶名的三个主要限制是它们
- en: must be unique among all S3 bucket names (see [figure 11.7](#ch11fig07))
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须在所有 S3 桶名中是唯一的（见 [图 11.7](#ch11fig07)）
- en: can’t use capital letters or underscores
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不能使用大写字母或下划线
- en: must be between 3 and 63 characters
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须介于 3 到 63 个字符之间
- en: Figure 11.7\. The bucket wizard is helpful in selecting a bucket name, which
    must be unique across all S3 buckets, and a region.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.7\. 桶向导有助于选择桶名，该桶名必须在所有 S3 桶中是唯一的，并选择一个区域。
- en: '![](11fig07_alt.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](11fig07_alt.jpg)'
- en: A common way to name S3 buckets is to break bucket names into a series of labels.
    For example, wolohan.mastering.largedata could be a bucket for this book. That
    name consists of three labels—wolohan, mastering, and largedata—each of which
    is separated by a period. If I wanted to create a second bucket for a similar
    purpose, I could create wolohan.mastering.largedata2\. If I wanted to create a
    bucket for a book on small data, I could call it wolohan.mastering.smalldata.
    Another common approach is to use hyphens instead of periods.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 命名 S3 桶的一个常见方法是将其拆分为一系列标签。例如，wolohan.mastering.largedata 可以是这本书的桶。该名称由三个标签组成——wolohan、mastering
    和 largedata，每个标签之间由句点分隔。如果我想为类似目的创建第二个桶，我可以创建 wolohan.mastering.largedata2。如果我想为关于小数据的书籍创建一个桶，我可以将其命名为
    wolohan.mastering.smalldata。另一种常见的方法是使用连字符而不是句点。
- en: Additionally, on the first Create Bucket page, we need to select a region for
    our bucket. The region refers to the group of data centers where AWS will store
    the bucket’s data. In AWS parlance, a region is a group of availability zones,
    which are themselves data centers or groupings of data centers ([figure 11.8](#ch11fig08)).
    The availability zone level offers the lowest redundancy and fault tolerance (which
    is still quite good), with a region-level service offering more fault tolerance
    (which is great), and a multiregion setup offering the most fault tolerance (excellent).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在第一个创建存储桶页面，我们需要为我们的存储桶选择一个区域。区域指的是 AWS 将存储存储桶数据的数据中心组。在 AWS 术语中，一个区域是一组可用区，这些可用区本身是数据中心或数据中心的组合（[图
    11.8](#ch11fig08)）。可用区级别提供最低的冗余和容错能力（尽管仍然相当不错），而区域级别的服务提供更多的容错能力（非常好），多区域设置提供最大的容错能力（优秀）。
- en: Figure 11.8\. Regions and availability zones in AWS refer to the data centers
    that are used to run compute operations or store data. Moving from small scale
    (availability zone) up to large scale (multiple regions) improves fault tolerance.
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.8\. AWS 中的区域和可用区指的是用于运行计算操作或存储数据的数据中心。从小规模（可用区）到大规模（多个区域）的迁移提高了容错能力。
- en: '![](11fig08_alt.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig08_alt.jpg)'
- en: Managed services in AWS typically run at the region level. Services that we
    manage ourselves—such as basic compute and traditional block storage—are run at
    the availability zone level. We can replicate managed services and self-managed
    services across regions if we need the extra redundancy or need to make our application
    available to customers in a different part of the world. For our purposes, any
    region will do. Pick the one closest to you and click Next.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 中的托管服务通常在区域级别运行。我们自行管理的服务，如基本计算和传统块存储，在可用区级别运行。如果我们需要额外的冗余或需要将我们的应用程序提供给世界不同地区的客户，我们可以跨区域复制托管服务和自托管服务。就我们的目的而言，任何区域都适用。选择离您最近的一个，然后点击下一步。
- en: The next two screens in the S3 Create Bucket wizard allow us to select optional
    features and permissions for our buckets. In the Configure Options screen, the
    two options that I want to draw our attention to are the Versioning and Tags features
    ([figure 11.9](#ch11fig09)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 S3 创建存储桶向导的下一两个屏幕中，我们可以选择存储桶的可选功能和权限。在配置选项屏幕中，我想引起您注意的是版本控制和标签功能（[图 11.9](#ch11fig09)）。
- en: Figure 11.9\. Configure Options offers options for generating S3 buckets in
    the AWS browser wizard.
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.9\. 配置选项提供了在 AWS 浏览器向导中生成 S3 存储桶的选项。
- en: '![](11fig09_alt.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig09_alt.jpg)'
- en: S3 versioning is a very useful feature because it allows us to keep track of
    objects through time. For example, we can use S3 to store snapshots of a database
    all in a single object. That being said, with S3, we do pay to store every version
    of the object uploaded. If we upload four versions of an object at 10 MB, we’re
    paying for 40 MB or storage. If we store 100 versions of an object at 100 GB each,
    we’re paying for 10 TB of storage. Versioning is an important feature, but don’t
    get caught off-guard if you’re versioning large objects.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: S3 版本控制是一个非常实用的功能，因为它允许我们跟踪对象随时间的变化。例如，我们可以使用 S3 存储数据库快照的所有版本在一个单独的对象中。话虽如此，使用
    S3，我们确实需要为上传的对象的每个版本付费存储。如果我们上传一个 10 MB 的对象的四个版本，我们将为 40 MB 的存储付费。如果我们存储一个对象 100
    个版本，每个版本 100 GB，我们将为 10 TB 的存储付费。版本控制是一个重要的功能，但如果您正在对大型对象进行版本控制，不要感到措手不及。
- en: The Tags option for S3 buckets allows us to use arbitrary metadata to keep track
    of projects. For example, it may make sense for you to enter a tag for your S3
    bucket with a key of “project” and a value of “chapter-11.” You can add as many
    of these tags as you need for your project. For example, if you have a bucket
    for movies, you may want to add a key of “content” and a value of “movies.” Once
    you’re done adding tags, click through to the next screen.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: S3 存储桶的标签选项允许我们使用任意元数据来跟踪项目。例如，您可能希望为您的 S3 存储桶添加一个键为“project”且值为“chapter-11”的标签。您可以为您的项目添加所需的所有这些标签。例如，如果您有一个用于电影的存储桶，您可能希望添加一个键为“content”且值为“movies”的标签。完成标签添加后，点击进入下一屏幕。
- en: From the Set Permissions screen, we can set restrictions on the public access
    of our bucket. Public access refers to access that comes in directly from the
    public internet. For data analysis, assuming we want to do our analysis on AWS
    as I demonstrate next chapter, we won’t need this ([figure 11.10](#ch11fig10)).
    For other use cases, public access can be helpful. Amazon recommends limiting
    public access to S3 buckets as much as possible. Go ahead and block all public
    access for this bucket, click through the next two pages, and create the bucket.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从设置权限屏幕，我们可以对我们的桶的公开访问设置限制。公开访问是指直接从公共互联网进入的访问。对于数据分析，假设我们想在AWS上进行分析，就像我在下一章中演示的那样，我们不需要这个（[图11.10](#ch11fig10)）。对于其他用例，公开访问可能很有帮助。亚马逊建议尽可能限制S3桶的公开访问。请继续阻止此桶的所有公开访问，点击下一两个页面，并创建该桶。
- en: Figure 11.10\. Public access to S3 buckets is not generally necessary for analytics
    workflows.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.10\. 对于分析工作流程，通常不需要对S3桶进行公开访问。
- en: '![](11fig10_alt.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig10_alt.jpg)'
- en: 'At this point, we’ve created an S3 bucket. You should see a bucket show up
    on the main S3 landing page. Click on the bucket’s link, and you’ll be brought
    to a landing page specific to that bucket. As long as that bucket is empty, it
    will show you a landing page giving you three options ([figure 11.11](#ch11fig11)):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了一个S3桶。您应该在主S3登录页面上看到一个桶出现。点击该桶的链接，您将被带到该桶的特定登录页面。只要该桶为空，它将显示一个登录页面，为您提供三个选项（[图11.11](#ch11fig11)）：
- en: Upload an object
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传对象
- en: Set object properties
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置对象属性
- en: Set object permissions
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置对象权限
- en: Figure 11.11\. The main thing we’ll do with S3 buckets is upload objects to
    them.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.11\. 我们将主要使用S3桶上传对象。
- en: '![](11fig11_alt.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig11_alt.jpg)'
- en: Of these three, we’ll want to upload an object. If we click the blue Upload
    button in the top left corner, we’ll be brought to another wizard like the one
    we just went through. This wizard is for adding data to an S3 bucket.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三个中，我们将想要上传一个对象。如果我们点击左上角的蓝色“上传”按钮，我们将被带到另一个类似于我们刚刚经历的向导。这个向导是用来向S3桶添加数据的。
- en: 'The bucket Upload wizard ([figure 11.12](#ch11fig12)) allows us to upload a
    single file or multiple files. Go ahead and click Add Files and choose a file
    from your file system. The [chapter 11](#ch11) repository (you can find it at
    this link: [https://www.manning.com/downloads/1961](https://www.manning.com/downloads/1961))
    for this book includes several files for the programmatic example later in this
    chapter that you can use for this part.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 桶上传向导（[图11.12](#ch11fig12)）允许我们上传单个文件或多个文件。请继续点击“添加文件”并从您的文件系统中选择一个文件。本书的第11章存储库（您可以在以下链接中找到：[https://www.manning.com/downloads/1961](https://www.manning.com/downloads/1961)）包含本章后面程序示例中可以使用的几个文件。
- en: Figure 11.12\. The bucket Upload wizard allows us to upload files to an S3 bucket.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.12\. 桶上传向导允许我们将文件上传到S3桶。
- en: '![](11fig12_alt.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig12_alt.jpg)'
- en: Click through the next screen, selecting to use the bucket-level permissions,
    and you’ll be brought to a Set Properties screen ([figure 11.13](#ch11fig13)).
    On this screen, we can select the storage class of the object we’re uploading.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下一屏幕，选择使用桶级别权限，您将被带到设置属性屏幕（[图11.13](#ch11fig13)）。在这个屏幕上，我们可以选择我们上传的对象的存储类别。
- en: Figure 11.13\. The storage classes in S3 are all tailored to a different use
    case. The standard S3 storage class is appropriate for most use cases.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.13\. S3中的存储类别都是针对不同的用例量身定制的。标准的S3存储类别适用于大多数用例。
- en: '![](11fig13_alt.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig13_alt.jpg)'
- en: 'We covered three of these storage classes in [section 11.1.5](#ch11lev2sec5):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第11.1.5节](#ch11lev2sec5)中介绍了这三个存储类别：
- en: Standard storage is appropriate for most use cases.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准存储适用于大多数用例。
- en: Infrequent Access storage is for data we want to have available but won’t need
    often.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不经常访问存储适用于我们希望可用但不会经常需要的数据。
- en: Glacier storage is for data we want to keep but will need infrequently and about
    which we’ll have plenty of notice before we need it.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冰川存储适用于我们想要保留但不太可能频繁使用的数据，并且在我们需要它之前会有足够的通知。
- en: On this screen, we can see those three classes, plus several more. You’ll notice
    that AWS provides its own descriptions of when the different storage classes are
    useful. At the top of the page in the wizard, the link to the current S3 pricing
    will let us compare the costs of the different storage options. I recommend using
    S3 Standard for this upload and other uploads where the case for another class
    is not obvious.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个屏幕上，我们可以看到这三个类别，以及更多。你会注意到AWS提供了不同存储类别何时有用的描述。在向导页面顶部，当前S3定价的链接让我们可以比较不同存储选项的成本。我建议使用S3标准版进行此上传和其他不太明显需要其他类别的上传。
- en: 'Additionally, via this screen, we have the option of adding metadata tags to
    our object ([figure 11.14](#ch11fig14)). These tags are key-value pairs that can
    be anything we want. They can be helpful for storing our data. For me, I’m uploading
    the data file named 2014-01.json—which I know is a JSON file with data from January
    2014 in it. For that reason, I’ll give it three tags:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过这个屏幕，我们还有添加元数据标签到我们的对象的选项（[图11.14](#ch11fig14)）。这些标签可以是任何我们想要的键值对。它们可以帮助我们存储数据。对我来说，我正在上传名为2014-01.json的数据文件——我知道这是一个包含2014年1月数据的JSON文件。因此，我将给它三个标签：
- en: A header declaring the content type of the object
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个声明对象内容类型的标题
- en: A custom tag indicating the month of the object
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自定义标签，表示对象的月份
- en: A custom tag indicating the year of the object
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自定义标签，表示对象的年份
- en: Figure 11.14\. Adding metadata to S3 objects helps us find those objects later
    when we need to use them.
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.14。向S3对象添加元数据有助于我们在需要使用它们时找到这些对象。
- en: '![](11fig14_alt.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![11fig14_alt.jpg](11fig14_alt.jpg)'
- en: I can use these tags in the future to find the object among all of the objects
    that I upload to this bucket.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以用这些标签在未来从所有上传到这个存储桶的对象中找到这个对象。
- en: 'Once you’ve added the metadata you want, click through this screen, review
    your choices, and upload the object to your bucket. Now, when you’re on the landing
    page of your S3 bucket, you should see a listing of all the objects in that bucket.
    There should only be one object: the one you just uploaded. Click on that object,
    and you’ll be brought to an object page ([figure 11.15](#ch11fig15)).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你添加了你想要的元数据，点击通过这个屏幕，审查你的选择，并将对象上传到你的存储桶。现在，当你处于你的S3存储桶的登录页面时，你应该能看到该存储桶中所有对象的列表。应该只有一个对象：你刚刚上传的那个。点击该对象，你将被带到对象页面（[图11.15](#ch11fig15)）。
- en: Figure 11.15\. The S3 object page shows metadata about the object and lists
    actions—such as downloading the object or opening the object—that we can take.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.15。S3对象页面显示了对象元数据以及我们可采取的操作列表——例如下载对象或打开对象。
- en: '![](11fig15_alt.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![11fig15_alt.jpg](11fig15_alt.jpg)'
- en: The object page shows you properties of the object you just uploaded, including
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对象页面显示了您刚刚上传的对象的属性，包括
- en: the owner of the object
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象的所有者
- en: the date the object was last modified
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象最后修改的日期
- en: the storage class of the object
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象的存储类别
- en: the size of the object
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象的大小
- en: Additionally, options at the top of the page indicate actions we can take. Try
    to open the object using the open option, and you’ll be brought to an error page.
    Why is this happening?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，页面顶部的选项指示我们可以采取的操作。尝试使用打开选项打开对象，你将被带到错误页面。为什么会发生这种情况？
- en: We’re getting the error page because we’re attempting to access the object from
    our browser over the public internet, and we blocked public internet access to
    all the objects in our bucket. This is the same response that anyone else would
    see if they were trying to access our object. If we want to preview the JSON file,
    a convenient way to do that is on the Select From tab.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到错误页面是因为我们试图通过浏览器从公共互联网访问对象，而我们阻止了对存储桶中所有对象的公共互联网访问。如果其他人试图访问我们的对象，他们也会看到相同的响应。如果我们想预览JSON文件，一个方便的方法是在“选择从”标签页上。
- en: The Select From tab gives us options for querying our data ([figure 11.16](#ch11fig16)).
    If we select the JSON file format and JSON lines type, AWS will give us a preview
    of the document. We can also click through and use SQL-like expressions to query
    our document. For large files, this may be an effective way of preprocessing our
    data, although we also can use the map and filter techniques we have learned through
    this book.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: “选择从”标签页为我们提供了查询数据的选择（[图11.16](#ch11fig16)）。如果我们选择JSON文件格式和JSON行类型，AWS将为我们提供文档的预览。我们还可以点击并通过使用类似SQL的表达式来查询我们的文档。对于大文件，这可能是一种有效的数据预处理方法，尽管我们也可以使用本书中学到的映射和过滤技术。
- en: Figure 11.16\. We can use S3 Select to preview JSON, CSV, or Apache Parquet
    files that we’ve uploaded to S3\. S3 Select provides SQL-like access to data in
    all three formats.
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.16。我们可以使用 S3 Select 预览我们已上传到 S3 的 JSON、CSV 或 Apache Parquet 文件。S3 Select
    为所有三种格式的数据提供类似 SQL 的访问。
- en: '![](11fig16_alt.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig16_alt.jpg)'
- en: '|  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Parquet: A concise tabular data store**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**Parquet：简洁的表格数据存储**'
- en: 'In [figure 11.16](#ch11fig16), you’ll notice three file format options: CSV,
    JSON, and Parquet. The first two we’ve already used in this book. CSV is a simple,
    tabular data store, and JSON is a human-readable document store. Both are common
    in data interchange and are often used in the storage of distributed large datasets.
    Parquet is a Hadoop-native tabular data format.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 11.16](#ch11fig16) 中，你会注意到三个文件格式选项：CSV、JSON 和 Parquet。前两种我们已经在本书中使用过。CSV
    是一个简单的表格数据存储，而 JSON 是一个可读性强的文档存储。两者在数据交换中都很常见，并且经常用于分布式大型数据集的存储。Parquet 是一个 Hadoop
    原生的表格数据格式。
- en: Parquet uses clever metadata to improve the performance of map and reduce operations.
    Running a job on Parquet can take as little as 1/100th the time a comparable job
    on a CSV or JSON file would take. Additionally, Parquet supports efficient compression.
    As a result, it can be stored at a fraction of the cost of CSV or JSON.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 使用巧妙的元数据来提高 map 和 reduce 操作的性能。在 Parquet 上运行一个作业可能只需要与在 CSV 或 JSON 文件上运行类似作业所需时间的
    1/100。此外，Parquet 支持高效的压缩。因此，它可以以 CSV 或 JSON 成本的一小部分存储。
- en: 'These benefits make Parquet an excellent option for data that primarily needs
    to be read by a machine, such as for batch analytics operations. JSON and CSV
    remain good options for smaller data or data that’s likely to need some human
    inspection. For more on Parquet, see [chapter 7](kindle_split_017.html#ch07) of
    *Spark in Action*, Second Edition, by Jean-Georges Perrin: [http://mng.bz/eD7P](http://mng.bz/eD7P).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优点使 Parquet 成为需要主要由机器读取的数据的绝佳选择，例如用于批量分析操作。JSON 和 CSV 仍然是小型数据或可能需要一些人工检查的数据的好选择。有关
    Parquet 的更多信息，请参阅 Jean-Georges Perrin 所著的 *Spark in Action* 第二版中的第 7 章：[http://mng.bz/eD7P](http://mng.bz/eD7P)。
- en: '|  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Uploading objects manually is useful because it can be a good introduction or
    a reminder of all the options that are available to us. It does, however, require
    a lot of clicking. In the next section, we’ll look at how we can upload an object
    programmatically.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 手动上传对象是有用的，因为它可以是一个很好的介绍或提醒我们所有可用的选项。然而，它确实需要大量的点击。在下一节中，我们将探讨如何以编程方式上传对象。
- en: 11.2.2\. Programmatic access to S3 with Python and boto
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2. 使用 Python 和 boto 对 S3 进行编程访问
- en: Although the browser-based interface to S3 is nice, at times we want to upload
    objects to S3 without as much human involvement. For these situations, we can
    use one of the AWS SDKs. For Python, that would be the boto library.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于浏览器的 S3 界面很棒，但有时我们希望在尽可能少的人工参与下将对象上传到 S3。在这种情况下，我们可以使用 AWS SDK 之一。对于 Python，那就是
    boto 库。
- en: 'Boto is a library that provides Pythonic access to many of the AWS APIs, including
    the S3 API. We can use boto to write Python code—including all of the map and
    reduce perks we’ve used so far in this book—to upload objects to S3\. The current
    version of boto is boto3, and we can install it using `pip`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Boto 是一个库，它提供了对许多 AWS API 的 Pythonic 访问，包括 S3 API。我们可以使用 boto 来编写 Python 代码——包括我们在本书中迄今为止使用过的所有
    map 和 reduce 优点——以将对象上传到 S3。boto 的当前版本是 boto3，我们可以使用 `pip` 来安装它：
- en: '[PRE0]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We’ll be able to use boto to interact with AWS on our behalf. To do so, we need
    to give it authorization. This authorization comes in the form of an access key
    and an access key secret. To create these keys, we’ll have to go back to AWS in
    our browser. Specifically, we’ll want to go to our Identity Access and Management
    (IAM) console.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将能够使用 boto 代表我们与 AWS 进行交互。为此，我们需要授予它授权。这种授权以访问密钥和访问密钥秘密的形式出现。要创建这些密钥，我们将在浏览器中返回
    AWS。具体来说，我们希望进入我们的身份访问管理（IAM）控制台。
- en: A secure cloud with IAM
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有 IAM 的安全云
- en: Through the Amazon Web Services IAM interface, we can create accounts with different
    access permissions. For example, we may want to give our developers access to
    our compute resources but restrict the finance team to only the billing. This
    is a powerful tool—similar to user accounts on an operating system.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过亚马逊网络服务 IAM 接口，我们可以创建具有不同访问权限的账户。例如，我们可能希望让我们的开发人员访问我们的计算资源，但限制财务团队只能访问计费。这是一个强大的工具——类似于操作系统上的用户账户。
- en: By default in AWS, we operate as root. As you may know from working on a Unix
    system, root access gives us a lot of power, but it also can allow a malicious
    or ignorant actor to cause a lot of damage. For that reason, we want to limit
    the amount of time we spend at root. To do so, we’ll create separate IAM accounts
    to work from.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中默认情况下，我们以root身份操作。正如您可能从Unix系统的工作中知道的那样，root访问权限给我们提供了很多权力，但它也可能允许恶意或无知的行动者造成大量损害。因此，我们希望限制我们在root身份下花费的时间。为此，我们将创建单独的IAM账户来工作。
- en: 'Navigate to the Users tab of the IAM console by doing one of the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下方式导航到IAM控制台的“用户”选项卡：
- en: Clicking your name in the top right corner, then Security Credentials, then
    Users in the sidebar
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击右上角的您的名字，然后安全凭证，然后在侧边栏中选择用户
- en: Going to [https://console.aws.amazon.com/iam/home?#/users](https://console.aws.amazon.com/iam/home?#/users)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前往[https://console.aws.amazon.com/iam/home?#/users](https://console.aws.amazon.com/iam/home?#/users)
- en: From here, you’ll see a blank list of users. It’s blank because we haven’t created
    any IAM users yet. Up top, there will be a big blue Add User button. Click that
    button, and you’ll be brought to yet another AWS wizard ([figure 11.17](#ch11fig17)).
    This wizard will walk us through setting up an IAM user.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，您将看到一个空的用户列表。它是空的，因为我们还没有创建任何IAM用户。顶部将有一个大蓝色“添加用户”按钮。点击该按钮，您将被带到另一个AWS向导([图11.17](#ch11fig17))。此向导将引导我们设置IAM用户。
- en: Figure 11.17\. The AWS Create User wizard will help us create a user that will
    have programmatic-only access to our AWS resources.
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.17\. AWS创建用户向导将帮助我们创建一个只能以编程方式访问我们的AWS资源的用户。
- en: '![](11fig17_alt.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig17_alt.jpg)'
- en: 'In the first screen, give the user a User Name and check the box for Programmatic
    Access. This will provide that user credentials to use the Python SDK for AWS:
    boto. Don’t click the checkbox for AWS Management Console Access. Leaving it unchecked
    will prevent that user from accessing AWS over the web.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一页，为用户输入用户名并勾选程序访问权限的复选框。这将提供用户凭证以使用AWS的Python SDK：boto。不要勾选AWS管理控制台访问的复选框。不勾选它将防止该用户通过Web访问AWS。
- en: 'Click through to the second page, and you’ll be asked to set the user permissions
    ([figure 11.18](#ch11fig18)). This is where we decide what the user can and can’t
    do. We’ll want our user to be able to access and modify AWS resources necessary
    for working on large datasets. AWS refers to this type of user as a data scientist.
    To give the user we’re creating the permissions of a data scientist, do the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 点击到第二页，您将被要求设置用户权限([图11.18](#ch11fig18))。这就是我们决定用户可以做什么和不能做什么的地方。我们希望我们的用户能够访问和修改处理大型数据集所需的AWS资源。AWS将此类用户称为数据科学家。为了给创建的用户赋予数据科学家的权限，请执行以下操作：
- en: Click Attach Existing so we can see the AWS suggested permissions policies.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击“附加现有”，以便我们可以查看AWS建议的权限策略。
- en: Type DataScientist in the search bar and select the result that appears.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在搜索栏中输入DataScientist并选择出现的搜索结果。
- en: Figure 11.18\. Adding a data scientist policy to our new IAM user will allow
    the user to access the resources necessary for working with large datasets in
    the cloud, but nothing else.
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.18\. 向我们的新IAM用户添加数据科学家策略将允许用户访问在云中处理大型数据集所需的资源，但除此之外没有其他权限。
- en: '![](11fig18_alt.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](11fig18_alt.jpg)'
- en: Note that AWS has a variety of other permissions sets for other roles—such as
    system administrators, billing only, read only, and database administration only—that
    we can use to ensure folks only have access to what they need. See [table 11.2](#ch11table02)
    for more information.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，AWS为其他角色提供了各种其他权限集，例如系统管理员、仅限计费、仅限读取和仅限数据库管理，我们可以使用这些权限集来确保人们只能访问他们需要的资源。更多信息请参阅[表11.2](#ch11table02)。
- en: Table 11.2\. Useful AWS Security Policies and common situations in which you
    would assign them
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.2\. 有用的AWS安全策略和您可能会分配它们的常见情况
- en: '| AWS Security Policy | Use case |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| AWS安全策略 | 用例 |'
- en: '| --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AdministratorAccess | Individuals who need to be able to manage other users;
    start up and shut down all services |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 管理员访问 | 需要能够管理其他用户；启动和关闭所有服务 |'
- en: '| DataScientist | Users who are performing general data analytics tasks, requiring
    a mix of S3, EC2, and Elastic MapReduce services |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 数据科学家 | 执行通用数据分析任务的用户，需要使用S3、EC2和Elastic MapReduce服务的组合 |'
- en: '| AmazonElasticMapReduceRole | Users who need to use the Elastic MapReduce
    cluster-computing abilities of AWS |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| AmazonElasticMapReduceRole | 需要使用AWS的Elastic MapReduce集群计算能力的用户 |'
- en: '| AmazonS3FullAccess | Programs/scripts that need to both read and write data
    to AWS S3 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| AmazonS3FullAccess | 需要读取和写入 AWS S3 数据的程序/脚本 |'
- en: '| AmazonS3ReadOnlyAccess | Programs/scripts that only need to read data from
    AWS S3 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| AmazonS3ReadOnlyAccess | 仅需要从 AWS S3 读取数据的程序/脚本 |'
- en: '| PowerUserAccess | Users who need to access all features of all services but
    don’t need to manage other users |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| PowerUserAccess | 需要访问所有服务的所有功能但不需要管理其他用户的高级用户 |'
- en: Once you’re ready—click through the next two screens until you see a success
    message indicating the user has been created. On this page, you’ll see an option
    to download a .csv file. This file contains the user credentials for the user
    you just created, including the access key and the secret key we’ll need to programmatically
    access S3 through boto. Download this file and get it ready—we’re about to write
    some code.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦准备好——点击下一两个屏幕，直到看到一条成功消息，表明用户已创建。在这个页面上，您将看到一个下载 .csv 文件的选项。此文件包含您刚刚创建的用户凭证，包括我们需要通过
    boto 程序化访问 S3 所需的访问密钥和秘密密钥。下载此文件并准备好——我们即将编写一些代码。
- en: AWS scripting with the Python SDK boto3
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 Python SDK boto3 进行 AWS 脚本编写
- en: 'In the repository for this chapter, there’s data on car accidents. We’ll analyze
    this data in the cloud in the next chapter—but first, we need to load it up into
    S3 buckets. To do this, we’ll use the familiar map pattern that we first introduced
    in [chapter 2](kindle_split_011.html#ch02). For this map operation, we’ll need
    two things:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的存储库中，有关于车祸的数据。我们将在下一章在云中分析这些数据——但首先，我们需要将其加载到 S3 存储桶中。为此，我们将使用我们在第 2 章中首次介绍的熟悉的映射模式。对于这个映射操作，我们需要两样东西：
- en: A sequence of file paths indicating all the files we want to upload
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列文件路径，指示我们想要上传的所有文件
- en: A helper function that does the work of uploading those files
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个执行上传这些文件工作的辅助函数
- en: Let’s start with the helper function to get working with boto3\. Our map helper
    functions typically have taken one parameter, but for this map helper function,
    let’s design it to take two parameters. The first will be the path to the file
    we’re trying to upload, and the second will be the bucket to which we want to
    upload. This will let us reuse the function for other buckets if we’d like.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从辅助函数开始，以便开始使用 boto3。我们的映射辅助函数通常只接受一个参数，但为了这个映射辅助函数，让我们设计它以接受两个参数。第一个将是我们要上传的文件的路径，第二个是我们想要上传的存储桶。这将使我们能够为其他存储桶重用该函数。
- en: 'We’ll start by focusing on the first parameter of the function: the file path.
    Let’s take this file path and use the `os.path.split` function to extract the
    filename from the path. We’ll assign the file this name—the same name that it
    has on our local system—when we upload it to S3.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先关注函数的第一个参数：文件路径。让我们使用这个文件路径，并通过 `os.path.split` 函数从路径中提取文件名。当我们将其上传到 S3
    时，我们将赋予文件这个名称——与我们在本地系统上相同的名称。
- en: 'From here, we’re ready to create an AWS client instance. The client instance
    is a class that has methods representing actions we can take on AWS, such as uploading
    files to a bucket. This is available in boto as `.client`, and we’ll initialize
    it with three parameters:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们准备好创建一个 AWS 客户端实例。客户端实例是一个具有代表我们可以在 AWS 上执行的操作的方法的类，例如将文件上传到存储桶。这在 boto
    中作为 `.client` 提供，我们将使用三个参数对其进行初始化：
- en: The name of the service we want to use—in this case `"s3"`
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要使用的服务名称——在这种情况下是 `"s3"`
- en: The access key id for the DataScientist account we created
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建的数据科学家账户的访问密钥 ID
- en: The secret access key for the DataScientist account we created
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建的数据科学家账户的秘密访问密钥
- en: Importantly, we don’t want to pass these keys in as plain text. Doing so could
    potentially expose our account credentials if we upload our code to a code repository.
    Instead, we want to read them from environment variables. You can assign the access
    key and access secret to environment variables with the `export` command on a
    Unix machine or from the environment variables wizard on a PC.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们不想以纯文本形式传递这些密钥。如果我们将代码上传到代码仓库，这样做可能会暴露我们的账户凭证。相反，我们想要从环境变量中读取它们。您可以在
    Unix 机器上使用 `export` 命令或从 PC 上的环境变量向导中分配访问密钥和访问秘密。
- en: '[PRE1]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Credentials, AWS, and boto3**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**凭证、AWS 和 boto3**'
- en: 'You have several ways to establish your identity when using boto3\. The method
    I’ve chosen here balances ease and security. Two other popular options are to
    specify your access key and secret key in a credentials or configuration file,
    located at either `~/.aws/credentials` or `~/.aws/config`. Amazon provides information
    on how to set up those files in their AWS Command Line Interface documentation:
    [http://mng.bz/O9oo](http://mng.bz/O9oo).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 boto3 时，您有几种方式可以建立您的身份。我在这里选择的方法在易用性和安全性之间取得了平衡。另外两种流行的选择是将您的访问密钥和秘密密钥指定在位于
    `~/.aws/credentials` 或 `~/.aws/config` 的凭证或配置文件中。亚马逊在其 AWS 命令行界面文档中提供了如何设置这些文件的说明：[http://mng.bz/O9oo](http://mng.bz/O9oo)。
- en: 'An advantage of the credentials file is that you can specify multiple profiles—for
    example, for development and environments—and easily alternate between them when
    setting up a boto3 session. That’s beyond the scope of this chapter, but I encourage
    you to take a look at the boto3 configuration documentation for more information:
    [http://mng.bz/G4ER](http://mng.bz/G4ER).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 凭证文件的优点是您可以指定多个配置文件——例如，用于开发和环境——并在设置 boto3 会话时轻松地在它们之间切换。这超出了本章的范围，但我鼓励您查看
    boto3 配置文档以获取更多信息：[http://mng.bz/G4ER](http://mng.bz/G4ER)。
- en: '|  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Providing those three parameters returns a client that can take S3 actions
    on our behalf. This client has a method `.upload_file` that we can use to upload
    our files. We’ll also pass three parameters to the `.upload_file` method:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 提供这三个参数将返回一个可以代表我们执行 S3 操作的客户端。此客户端有一个 `.upload_file` 方法，我们可以使用它来上传我们的文件。我们还将向
    `.upload_file` 方法传递三个参数：
- en: The file path of the file we want to upload
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要上传的文件的文件路径
- en: The name of the bucket to which we want to upload
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要上传到的存储桶的名称
- en: The name of the file as we want it to show up on S3
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望文件在 S3 上显示的名称
- en: This method performs the upload to AWS and may return an HTTP response. Let’s
    take this response and return it, along with the file name, as the value of our
    function upon completion. We can see this helper function in full in the following
    listing.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法执行 AWS 上传并可能返回一个 HTTP 响应。让我们将此响应返回，并连同文件名一起，作为我们函数完成时的值。我们可以在下面的列表中看到完整的辅助函数。
- en: Listing 11.1\. A helper function to upload files to S3
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.1\. 用于上传文件到 S3 的辅助函数
- en: '[PRE2]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To use this function, we need to map it across a sequence of files. Use the
    `iglob` function, which we covered in [chapter 4](kindle_split_013.html#ch04),
    to assign a sequence of the files we’re interested in to a variable for that purpose.
    Once we have that sequence, we need to apply our `upload_file` function to each
    of the files, as shown in the following listing.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此函数，我们需要将其映射到一系列文件上。使用我们在第 4 章中介绍的 `iglob` 函数，将我们感兴趣的文件序列分配给一个变量。一旦我们有了这个序列，我们需要将我们的
    `upload_file` 函数应用到每个文件上，如下面的列表所示。
- en: Listing 11.2\. Uploading files from the filesystem to S3
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.2\. 从文件系统上传文件到 S3
- en: '[PRE3]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running this code will take some time, but it shouldn’t provide you with any
    clues of its completion in the terminal. Instead, navigate in the browser to the
    S3 bucket you created. Once there, you should see a bucket full of data files
    ready to analyze ([figure 11.19](#ch11fig19)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将花费一些时间，但它不应在终端中提供任何完成线索。相反，在浏览器中导航到您创建的 S3 存储桶。一旦到达那里，您应该会看到一个装满数据文件的存储桶，准备进行分析（[图
    11.19](#ch11fig19)）。
- en: Figure 11.19\. Your browser shows the traffic data files that have been uploaded
    to an AWS Simple Storage Service bucket.
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.19\. 您的浏览器显示了已上传到 AWS 简单存储服务存储桶中的流量数据文件。
- en: '![](11fig19_alt.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![11fig19_alt.jpg](11fig19_alt.jpg)'
- en: In the next chapter, we’ll use those files and this bucket to learn how to analyze
    large datasets in the cloud.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用这些文件和这个存储桶来学习如何在云中分析大型数据集。
- en: 11.3\. Exercises
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3\. 练习
- en: 11.3.1\. S3 Storage classes
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.1\. S3 存储类
- en: Which S3 storage class is best for each of the following three situations?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三种情况下哪种 S3 存储类最适合？
- en: Data we know we’ll only need rarely and with plenty of warning
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们知道我们很少需要并且有足够时间准备的数据
- en: Data we know we’ll only need a few times each month
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们知道我们每个月只需要使用几次的数据
- en: Data we’ll need access to regularly
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要定期访问的数据
- en: 11.3.2\. S3 storage region
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.2\. S3 存储区域
- en: AWS resources exist inside either availability zones or regions. Does highly
    durable S3 storage exist in an availability zone or across a region?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 资源存在于可用区或区域内部。高度耐用的 S3 存储是否存在于可用区或跨区域？
- en: 11.3.3\. Object storage
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.3\. 对象存储
- en: Which three elements are the integral components of object storage?
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储的三个基本组成部分是什么？
- en: Object, object name, object location
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象、对象名称、对象位置
- en: Object, object path, object color
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象、对象路径、对象颜色
- en: Object, object size, metadata
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象、对象大小、元数据
- en: Object, object ID, metadata
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象、对象ID、元数据
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Amazon Web Services Simple Storage Solution—known as S3—is a great option for
    large datasets we’ll want to operate on in the cloud because it’s effectively
    limitless in size.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊网络服务简单存储解决方案——通常称为S3——是我们希望在云中操作的大型数据集的绝佳选择，因为它在大小上实际上是无限的。
- en: S3 is also a managed service—AWS acts as a custodian for our data, and we can
    focus on getting value from it.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3也是一种托管服务——AWS充当我们数据的保管人，我们可以专注于从中获取价值。
- en: In S3, objects, which can be any data file we upload, are stored in buckets.
    We can assign metadata tags to both buckets and objects for organization.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在S3中，对象可以是任何我们上传的数据文件，它们存储在桶中。我们可以为桶和对象分配元数据标签以进行组织。
- en: We can store S3 objects in the Standard storage class, if we’ll access them
    frequently; an Infrequent Access storage class, if we’ll access them infrequently;
    and Glacier storage class for archiving.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们经常访问它们，我们可以将S3对象存储在标准存储类别中；如果不太经常访问，则存储在频繁访问存储类别中；对于存档，则使用冰川存储类别。
- en: We can create buckets and upload objects through the browser using AWS’s graphical
    interface. The interface shows us lots of options for every action we take.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过AWS的图形界面通过浏览器创建桶和上传对象。界面显示了我们执行每个操作时的大量选项。
- en: 'We also can upload objects through the Python software development kit for
    AWS: boto3.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以通过AWS的Python软件开发工具包boto3上传对象。
- en: Chapter 12\. MapReduce in the cloud with Amazon’s Elastic MapReduce
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12章\. 使用Amazon的弹性MapReduce在云中实现MapReduce
- en: '*This chapter covers*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Launching and configuring cloud compute clusters with Elastic MapReduce
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用弹性MapReduce启动和配置云计算集群
- en: Running Hadoop jobs in the cloud with mrjob
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用mrjob在云中运行Hadoop作业
- en: Distributed cloud machine learning with Spark
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行分布式云机器学习
- en: Throughout this book, we’ve been talking about the ability to scale code up.
    We started by looking at how to parallelize code locally; then we moved on to
    distributed computing frameworks; and finally, in [chapter 11](kindle_split_022.html#ch11),
    we introduced cloud computing technologies. In this chapter, we’ll look at techniques
    we can use to work with data of any scale. We’ll see how to take the Hadoop and
    Spark frameworks we covered in the middle of the book ([chapters 7](kindle_split_017.html#ch07)
    and [8](kindle_split_018.html#ch08) for Hadoop; [chapters 7](kindle_split_017.html#ch07),
    [9](kindle_split_019.html#ch09), and [10](kindle_split_020.html#ch10) for Spark)
    and bring them into the cloud with Amazon Elastic MapReduce. We’ll start by looking
    at how to bring Hadoop into the cloud with mrjob—a framework for Hadoop and Python
    that we introduced in [chapter 8](kindle_split_018.html#ch08). Then, we’ll look
    at bringing Spark and its machine learning capabilities into the cloud.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们一直在谈论扩展代码的能力。我们首先探讨了如何在本地并行化代码；然后转向分布式计算框架；最后，在第11章（kindle_split_022.html#ch11）中，我们介绍了云计算技术。在本章中，我们将探讨我们可以用来处理任何规模数据的技巧。我们将看到如何将我们在书中中间部分介绍过的Hadoop和Spark框架（Hadoop为第7章和第8章；Spark为第7章、第9章和第10章）带入云中，使用Amazon
    Elastic MapReduce。我们将首先探讨如何使用mrjob将Hadoop带入云中——这是我们在第8章中介绍的一个用于Hadoop和Python的框架。然后，我们将探讨将Spark及其机器学习功能带入云中。
- en: 12.1\. Running Hadoop on EMR with mrjob
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1\. 使用mrjob在EMR上运行Hadoop
- en: 'In [chapter 8](kindle_split_018.html#ch08), we reviewed two methods of working
    with Hadoop:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](kindle_split_018.html#ch08)中，我们回顾了两种与Hadoop一起工作的方法：
- en: '***Hadoop Streaming—*** Which uses Python scripts for its mappers and reducers'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***Hadoop Streaming—*** 它使用Python脚本来进行映射和归约'
- en: '***mrjob—*** Which we can use to do Hadoop jobs using only Python code'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***mrjob—*** 我们可以使用它仅使用Python代码来执行Hadoop作业'
- en: When we used both of these approaches, we focused on implementing the map and
    reduce style in Hadoop. With the techniques in [chapter 8](kindle_split_018.html#ch08),
    you could take advantage of a Hadoop cluster if you already had one available,
    but most people don’t. In this section, we’ll review running Hadoop jobs on Amazon
    Web Services’ Elastic MapReduce (EMR), a service we can use to create compute
    clusters whenever we need them.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用这两种方法时，我们专注于在Hadoop中实现映射和减少风格。通过第8章中的技术，如果你已经有一个可用的Hadoop集群，你可以利用它，但大多数人没有。在本节中，我们将回顾在亚马逊网络服务（AWS）的弹性映射和减少（EMR）上运行Hadoop作业，这是一个我们可以随时创建计算集群的服务。
- en: 12.1.1\. Convenient cloud clusters with EMR
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.1\. 使用EMR方便的云集群
- en: Hadoop clusters used to be reserved for only those who needed them often or
    could afford to have a large amount of computing resources laying around idle
    much of the time. This meant that 10 years ago, for the most part, only corporations
    and academic institutions had cluster computing. Now, with the cloud increasing
    in popularity, everyone can have access. One convenient way to get access to a
    compute cluster is Amazon’s Elastic MapReduce service.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，Hadoop集群只为那些经常需要它们或能够负担得起大部分时间闲置的大量计算资源的人所保留。这意味着10年前，大部分情况下，只有企业和学术机构才有集群计算。现在，随着云服务的普及，每个人都可以访问。获取计算集群的一种便捷方式是亚马逊的弹性映射和减少服务（Elastic
    MapReduce）。
- en: '|  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Other cloud compute services**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他云计算服务**'
- en: Amazon is not the only provider of cloud-based clustering computing services.
    Their two major competitors, Microsoft Azure and Google Cloud, both offer services
    you’ll find similar to Amazon Web Services. Microsoft’s Azure HDInsight service
    and Google’s Cloud Dataproc service both support Hadoop and Spark. That means
    you can use the knowledge from [chapters 7](kindle_split_017.html#ch07)–[10](kindle_split_020.html#ch10)
    with both of those services. mrjob, which we covered in [chapter 8](kindle_split_018.html#ch08)
    and go into more depth on in this section, also supports Google Cloud Dataproc.
    mrjob doesn’t support Azure HDInsight.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊并不是云基础集群计算服务的唯一提供商。他们的两个主要竞争对手，微软Azure和谷歌云，都提供与亚马逊网络服务类似的服务。微软的Azure HDInsight服务和谷歌的云数据处理服务（Cloud
    Dataproc）都支持Hadoop和Spark。这意味着你可以使用第7章到第10章的知识来使用这两项服务。在第8章中介绍并在本节中深入探讨的mrjob也支持谷歌云数据处理服务。mrjob不支持Azure
    HDInsight。
- en: In this chapter, we’ll use AWS because we’ll want to work with the resources
    we used in the previous chapter.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用AWS，因为我们希望使用上一章中使用的资源。
- en: '|  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Amazon Web Services’ EMR is a managed data cluster service. We specify general
    properties of the cluster, and AWS runs software that creates the cluster for
    us. When we’re done using the cluster, Amazon absorbs the compute resources back
    into its network. You can think of this like the S3 cloud storage. Because Amazon
    has so much compute power, we can ask for some whenever we want, and they’ll rent
    it to us. Then, when we don’t need it anymore, Amazon is happy to take it back.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊网络服务的EMR是一个托管数据集群服务。我们指定集群的一般属性，AWS运行软件为我们创建集群。当我们完成使用集群后，亚马逊将计算资源吸收回其网络。你可以将其想象成S3云存储。由于亚马逊拥有如此多的计算能力，我们可以随时请求一些资源，他们将会租给我们。然后，当我们不再需要时，亚马逊很乐意将其收回。
- en: With this setup, if we need to run a large data processing task once a month
    or even once a year, we don’t need to pay to maintain the cluster all month or
    all year. We can ask AWS to provide us the compute resources when we need to do
    the processing, then we can return the compute resources to Amazon when we’re
    done. If we need to do this work more often, or at irregular intervals, we can
    maintain a small cluster that can grow based on usage.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种设置，如果我们每月或每年只需要运行一次大型数据处理任务，我们就不需要在整个月或整个年内支付维护集群的费用。我们可以在需要处理时请求AWS提供计算资源，处理完毕后，我们可以将计算资源归还给亚马逊。如果我们需要更频繁地做这项工作，或者在不规则的时间间隔内进行，我们可以维护一个小型集群，该集群可以根据使用情况扩展。
- en: For example, say we run text analytics on all the comments posted about our
    products on Facebook, Twitter, and Instagram every six hours. We may maintain
    a small cluster all the time to reduce startup time and then have the cluster
    expand based on how much new data there is to analyze. If there are only a few
    thousand comments about our products, we might get by without expanding our small
    cluster. If something surprising happens—say an A-list celebrity is caught using
    a product of ours—and we have hundreds of thousands of comments to parse through,
    our cluster can automatically expand to accommodate the increased volume.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们每六小时运行一次对Facebook、Twitter和Instagram上关于我们产品评论的文本分析。我们可能始终维护一个小集群以减少启动时间，然后根据要分析的新数据量来扩展集群。如果我们产品只有几千条评论，我们可能不需要扩展我们的小集群。如果发生意外情况——比如说一位A级名人被发现使用我们的产品——并且我们有数十万条评论需要解析，我们的集群可以自动扩展以适应增加的流量。
- en: Importantly, too, the pricing model for Amazon’s EMR service is a per-compute-unit
    per-second charge. If we run 100 machines and our job finishes in 2 minutes, we’ll
    pay the same amount as if we processed it on one machine and it took us 200 minutes.
    That means there are no cost savings to doing things slowly. Amazon encourages
    us to parallelize our problems away. All three cloud providers—Microsoft, Google,
    and Amazon—price their managed compute services in this way, though prices vary
    by provider.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，亚马逊EMR服务的定价模式是按计算单元每秒计费。如果我们运行100台机器，并且我们的作业在2分钟内完成，我们将支付与我们在一台机器上处理并花费200分钟相同的费用。这意味着缓慢做事没有成本节约。亚马逊鼓励我们并行化我们的问题。所有三家云服务提供商——微软、谷歌和亚马逊——都以这种方式定价他们的托管计算服务，尽管价格因提供商而异。
- en: 12.1.2\. Starting EMR clusters with mrjob
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.2\. 使用mrjob启动EMR集群
- en: 'The easiest way to start an EMR cluster is by using a Python library we’re
    already familiar with: mrjob. Although we can—and did, in [chapter 8](kindle_split_018.html#ch08)—use
    mrjob locally, mrjob was designed to automate the procurement of EMR clusters.
    By writing a Hadoop job with EMR and specifying the right settings, we can quickly
    set up a Hadoop cluster in the cloud.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 启动EMR集群最简单的方法是使用我们已熟悉的Python库：mrjob。尽管我们可以——并且在[第8章](kindle_split_018.html#ch08)中这样做——在本地使用mrjob，但mrjob旨在自动化EMR集群的采购。通过编写带有EMR的Hadoop作业并指定正确的设置，我们可以在云中快速设置一个Hadoop集群。
- en: Because we’ll do machine learning with Spark later in this chapter ([section
    12.2](#ch12lev1sec2)), let’s do a bit of data analysis that will help us understand
    the files we uploaded to S3 in [chapter 11](kindle_split_022.html#ch11). Back
    in [chapter 11](kindle_split_022.html#ch11), we uploaded data about car accidents,
    including features such as the time of day and the number of vehicles involved.
    For our first Hadoop job on EMR, let’s write a MapReduce job that counts up the
    number of times crashes occurred with different numbers of vehicles ([figure 12.1](#ch12fig01)).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在本章后面将使用Spark进行机器学习（[第12.2节](#ch12lev1sec2)），所以让我们做一些数据分析，这将帮助我们理解我们在第11章上传到S3的文件。在[第11章](kindle_split_022.html#ch11)中，我们上传了关于交通事故的数据，包括诸如白天时间和涉及车辆数量等特征。对于我们在EMR上的第一个Hadoop作业，让我们编写一个MapReduce作业，计算不同数量车辆发生事故的次数（[图12.1](#ch12fig01)）。
- en: Figure 12.1\. We can use EMR to scale our MapReduce jobs up to any size. In
    this case, we’ll use EMR to analyze the number of car crashes with different numbers
    of vehicles involved.
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1\. 我们可以使用EMR将MapReduce作业扩展到任何规模。在这种情况下，我们将使用EMR来分析涉及不同数量车辆的交通事故数量。
- en: '![](12fig01_alt.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图片](12fig01_alt.jpg)'
- en: 'To do this, we’ll create a custom class that inherits from the main mrjob class.
    Then we’ll write two methods for that class:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们将创建一个继承自主要mrjob类的自定义类。然后我们将为该类编写两个方法：
- en: A `.mapper` that takes in the line and returns the number of vehicles involved
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`.mapper`，接收行并返回涉及的车辆数量
- en: A `.reducer` that groups the vehicles and sums the counts
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`.reducer`，用于将车辆分组并计算总数
- en: Because our data is stored in the JSON lines format, when we process each line
    with our `.mapper`, we’ll read it into a Python object using `json.loads`, as
    shown in [listing 12.1](#ch12ex01). From there, we can use dictionary notation
    to retrieve the number of vehicles involved in the crash. Yielding this value
    and a 1 in a `tuple` will put us in good shape to count the values up in our `.reducer`.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的数据存储在JSON行格式中，所以当我们使用`.mapper`处理每一行时，我们将使用`json.loads`将其读取到一个Python对象中，如[列表12.1](#ch12ex01)所示。从那里，我们可以使用字典符号来检索事故中涉及的车辆数量。在`tuple`中产生这个值和1将使我们能够在我们`.reducer`中向上计数值。
- en: Listing 12.1\. Counting crashes by number of vehicles with mrjob
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.1\. 使用mrjob按车辆数量计数碰撞
- en: '[PRE4]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In our `.reducer`, we use the standard counting reduction. The key stays the
    key, but the value becomes the sum of all the values. If we ran this locally,
    the result would be a sequence of keys and values printed to the terminal. The
    first value indicates the number of vehicles involved, and the second value indicates
    how many crashes involved that number of vehicles. If you have the data files
    locally, you can run the mrjob script and validate this for yourself.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`.reducer`中，我们使用标准的计数归约。键保持不变，但值变为所有值的总和。如果我们本地运行，结果将是一系列键和值打印到终端。第一个值表示涉及的车辆数量，第二个值表示涉及该数量车辆的碰撞数量。如果你有本地数据文件，你可以运行mrjob脚本并自行验证这一点。
- en: 'To run this in the cloud on EMR, we need to pass three additional parameters
    to our mrjob script on the command line, as shown in [listing 12.2](#ch12ex02):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 要在云上EMR上运行此操作，我们需要在命令行中向我们的mrjob脚本传递三个额外的参数，如[列表12.2](#ch12ex02)所示：
- en: The first parameter we need to specify is called the runner. This tells mrjob
    how to process our command. By default, it processes locally. To process on EMR,
    we will need to specify `-r emr`.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要指定的第一个参数称为runner。这告诉mrjob如何处理我们的命令。默认情况下，它会在本地处理。要在EMR上处理，我们需要指定`-r emr`。
- en: Next, we’ll need to provide a path to our input files. So far, we’ve been using
    blob syntax and pointing to where those files reside locally. Here, though, we
    have our data in S3\. That path will need to be our bucket.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要提供一个指向我们的输入文件的路径。到目前为止，我们一直在使用blob语法并指向这些文件在本地存储的位置。然而，我们的数据存储在S3中。这个路径将需要是我们的存储桶。
- en: Lastly, let’s specify a folder to which we’ll write our output. We can place
    it in the same bucket or in a separate bucket.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们指定一个文件夹，我们将把输出写入该文件夹。我们可以将其放在同一个存储桶中或单独的存储桶中。
- en: Listing 12.2\. Running Hadoop on EMR with mrjob
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.2\. 使用mrjob在EMR上运行Hadoop
- en: '[PRE5]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In addition to these variables, which define where the script will go, we need
    to provide our credentials. mrjob uses them to create a compute cluster on our
    behalf. To keep these credentials secret, mrjob insists that you have these variables
    exported to your local environment. This prevents you from exposing your credentials
    in plaintext in your source code. If you didn’t provide your credentials in [chapter
    11](kindle_split_022.html#ch11) when using boto3 to upload data to S3, the following
    listing shows how to do that for Mac and Linux. For Windows, search for “environment
    variables” and follow the wizard.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些定义脚本将去往何处的变量之外，我们还需要提供我们的凭证。mrjob使用它们代表我们创建计算集群。为了保持这些凭证的秘密性，mrjob坚持要求你将这些变量导出到你的本地环境中。这防止你在源代码中以明文形式暴露你的凭证。如果你在[第11章](kindle_split_022.html#ch11)中使用boto3上传数据到S3时没有提供凭证，以下列表显示了如何在Mac和Linux上执行此操作。对于Windows，搜索“环境变量”并按照向导操作。
- en: Listing 12.3\. Setting AWS credentials for mrjob
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.3\. 为mrjob设置AWS凭证
- en: '[PRE6]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once you have your environment variables set, you’ll be able to run mrjob on
    EMR with the command from [listing 12.2](#ch12ex02). By default, this command
    will spin up a small test cluster for you. For learning the tool, this small cluster
    is plenty. For bigger jobs, you’ll want to use more resources. A common way to
    use more resources is to use an mrjob config file. This file allows us to use
    YAML notation to specify the type of cluster we’d like.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了环境变量，你将能够使用[列表12.2](#ch12ex02)中的命令在EMR上运行mrjob。默认情况下，此命令将为你启动一个小型测试集群。对于学习工具，这个小集群已经足够了。对于更大的作业，你将需要使用更多资源。使用更多资源的一种常见方法是使用mrjob配置文件。此文件允许我们使用YAML符号来指定我们想要的集群类型。
- en: For example, if we wanted to
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想
- en: run our Hadoop job with 20 instances
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用20个实例运行我们的Hadoop作业
- en: have all those instances be m1.large
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让所有这些实例都是m1.large
- en: have those resources be in the us-west-1 region (Northern California)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让这些资源位于美国西部（北部加利福尼亚）地区
- en: tag those resources with a “project” tag that had a value of “Mastering Large
    Datasets”
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有值为“Mastering Large Datasets”的“项目”标签标记这些资源。
- en: we could specify all of that in the config file. We can see an example of a
    config file for just that in the following listing.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在配置文件中指定所有这些。以下列表中提供了一个配置文件示例。
- en: Listing 12.4\. An example configuration file for mrjob
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.4\. mrjob的示例配置文件
- en: '[PRE7]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Specifying settings in a configuration file makes it possible for us to use
    and reuse multiple settings. For example, it may be enough to use 20 instances
    to run a nightly extract-transform-load process. For the monthly executive report,
    though, we may need to use 100 instances. We can use two configuration files to
    allow us to save our parameters.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件中指定设置使我们能够使用和重用多个设置。例如，运行夜间提取-转换-加载过程可能只需要使用20个实例。然而，对于月度管理报告，我们可能需要使用100个实例。我们可以使用两个配置文件来允许我们保存我们的参数。
- en: We can pass those parameters to mrjob on the command line when we invoke it
    with Python using the `conf-path` parameter. The following listing shows an example
    of this action.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在使用Python调用mrjob时，通过命令行传递这些参数，使用`conf-path`参数。下面的列表显示了这一操作的示例。
- en: Listing 12.5\. Adding a config file to an mrjob call
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.5\. 向mrjob调用添加配置文件
- en: '[PRE8]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Once you’ve successfully run your Hadoop job on EMR with mrjob, we can open
    up the AWS console to see what happened.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你成功地在EMR上使用mrjob运行了Hadoop作业，我们就可以打开AWS控制台来查看发生了什么。
- en: 12.1.3\. The AWS EMR browser interface
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.3\. AWS EMR浏览器界面
- en: In [section 12.1.2](#ch12lev2sec2), we looked at how we can use AWS EMR with
    the mrjob tool. In this section, we’ll look at how we can use the browser interface
    to run Spark jobs. Just as AWS provides a browser-based interface to S3, the object
    storage system that we looked at in [chapter 11](kindle_split_022.html#ch11),
    AWS also provides a browser-based interface to EMR. You can access that interface
    by going to [https://console.aws.amazon.com/elasticmapreduce/home](https://console.aws.amazon.com/elasticmapreduce/home).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第12.1.2节](#ch12lev2sec2)中，我们探讨了如何使用AWS EMR与mrjob工具。在本节中，我们将探讨如何使用浏览器界面运行Spark作业。正如AWS为我们之前在[第11章](kindle_split_022.html#ch11)中查看的对象存储系统S3提供了基于浏览器的界面一样，AWS也为我们提供了基于浏览器的EMR界面。你可以通过访问[https://console.aws.amazon.com/elasticmapreduce/home](https://console.aws.amazon.com/elasticmapreduce/home)来访问该界面。
- en: If you ran the job from [section 12.1.2](#ch12lev2sec2) and that job completed
    successfully, you should see a task with the status “Terminated—All steps completed”
    ([figure 12.2](#ch12fig02)). If you see another message, the job may still be
    running, or there may have been an error.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从[第12.1.2节](#ch12lev2sec2)运行了作业并且作业成功完成，你应该会看到一个状态为“已终止——所有步骤完成”的任务（[图12.2](#ch12fig02)）。如果你看到其他消息，作业可能仍在运行，或者可能发生了错误。
- en: Figure 12.2\. The Amazon browser-based console provides a convenient overview
    of the status of our clusters, including their names, IDs, status, time started,
    and total uptime.
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2\. 亚马逊基于浏览器的控制台提供了我们集群状态的便捷概览，包括它们的名称、ID、状态、启动时间和总运行时间。
- en: '![](12fig02_alt.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![12fig02_alt.jpg](12fig02_alt.jpg)'
- en: Viewing cluster status from the AWS cluster console
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从AWS集群控制台查看集群状态
- en: Whatever you see after you run the job, click on the name of the cluster, and
    you’ll arrive at the cluster-specific console page. On this page, you’ll see the
    name and status of the instance along with other information about your cluster
    ([figure 12.3](#ch12fig03)).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你在运行作业后看到什么，点击集群名称，你将到达特定于该集群的控制台页面。在这个页面上，你会看到实例的名称和状态，以及其他关于你的集群的信息（[图12.3](#ch12fig03)）。
- en: Figure 12.3\. The cluster console shows information specific to that AWS cluster,
    such as the ID of the cluster and the number of machines in it. You can also use
    this console to modify the settings of running clusters.
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.3\. 集群控制台显示了特定于该AWS集群的信息，例如集群ID和其中的机器数量。你还可以使用此控制台来修改运行集群的设置。
- en: '![](12fig03_alt.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![12fig03_alt.jpg](12fig03_alt.jpg)'
- en: Click the Steps tab, and you’ll find a list of all the steps submitted to your
    cluster. In EMR, steps are tasks that we send to the cluster through the EMR API—either
    using the console, through an SDK like boto3, or through the command-line AWS
    tools. In our case, there should be only a single step ([figure 12.4](#ch12fig04)).
    AWS created this step when we ran mrjob with the EMR runner.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 点击步骤标签，你将找到提交给你的集群的所有步骤列表。在EMR中，步骤是我们通过EMR API发送到集群的任务——无论是通过控制台、使用boto3等SDK，还是通过命令行AWS工具。在我们的情况下，应该只有一个步骤（[图12.4](#ch12fig04)）。当我们使用EMR运行器运行mrjob时，AWS创建了这一步骤。
- en: Figure 12.4\. The step-specific detail screen of the EMR console shows information
    about tasks we’ve asked our cluster to work on.
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.4。EMR控制台的步骤特定详细信息屏幕显示了我们的集群需要工作的任务信息。
- en: '![](12fig04_alt.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig04_alt.jpg)'
- en: It’s possible to submit multiple steps to the same cluster. If we do that, the
    steps will run one after another. Each step will wait until all steps in front
    of it have finished before it begins. If we want to run multiple steps simultaneously,
    we can request multiple clusters at the same time from EMR.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将多个步骤提交到同一个集群。如果我们这样做，步骤将一个接一个地运行。每个步骤将在开始之前等待它前面的所有步骤完成。如果我们想要同时运行多个步骤，我们可以从EMR同时请求多个集群。
- en: This tab is useful because it provides convenient access to the logs for each
    step. If your EMR steps fail—which they inevitably will if you use the service
    enough—this page can be helpful in your debugging process. Additionally, when
    jobs fail, mrjob will parse through the logs created by the Hadoop job and attempt
    to provide you with a user-friendly diagnosis of what went wrong. Hadoop’s logs
    are Java logs, and Java error messages can require some getting used to. If you’re
    more familiar with Python than with Java, the mrjob diagnosis can be quite a benefit.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标签页很有用，因为它提供了方便地访问每个步骤的日志。如果你的EMR步骤失败——如果你足够频繁地使用该服务，这是不可避免的——这个页面可以在你的调试过程中提供帮助。此外，当作业失败时，mrjob会解析由Hadoop作业创建的日志，并尝试为你提供一个用户友好的错误诊断。Hadoop的日志是Java日志，Java错误信息可能需要一些适应。如果你比Java更熟悉Python，mrjob的诊断可以非常有帮助。
- en: Running lots of small EMR jobs
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行大量的小型EMR作业
- en: On this tab, you will also note the amount of time that was spent working each
    particular step. For example, in [figure 12.4](#ch12fig04) you can see that my
    crash counts script only spent 1 minute running. Compare that to [figure 12.3](#ch12fig03),
    where the cluster as a whole ran for 13 minutes. The remaining time was spent
    in setup and teardown. In the setup phase, the machines are procured and connected,
    and the necessary software is installed on them (such as Python, Java, and Hadoop).
    In the teardown phase, AWS returns the resources and produces logs.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个标签页上，你还会注意到每个特定步骤花费的时间量。例如，在[图12.4](#ch12fig04)中，你可以看到我的崩溃计数脚本只花费了1分钟运行。与[图12.3](#ch12fig03)相比，整个集群运行了13分钟。剩余的时间用于设置和拆除。在设置阶段，机器被采购并连接，并在其上安装必要的软件（如Python、Java和Hadoop）。在拆除阶段，AWS返回资源并生成日志。
- en: 'If you’ll be running lots of small tasks, that’s a use case where you may want
    to submit multiple steps to a single cluster. mrjob makes it easy for us to set
    up clusters with the `create-cluster` command. We can pass this command to our
    configuration file so the cluster behaves just like it would if we created it
    with a single job. Additionally, the cluster will keep running after our job has
    finished. When we do run a persistent cluster like this, we’ll usually want to
    specify a maximum number of hours it can be idle before it shuts down entirely:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将运行大量的小任务，这是一个你可能想要将多个步骤提交到单个集群的场景。mrjob使我们能够通过`create-cluster`命令轻松设置集群。我们可以将此命令传递到配置文件中，以便集群的行为就像我们用单个作业创建它一样。此外，集群将在我们的作业完成后继续运行。当我们运行这种持久集群时，我们通常会想要指定在完全关闭之前它可以空闲的最大小时数：
- en: '[PRE9]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This prevents us from paying for resources we don’t need.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以防止我们为不需要的资源付费。
- en: To submit jobs to an existing cluster, we’ll need to specify the cluster ID
    to which we want to submit our code. On the command line, the parameter is `--cluster-id`,
    and it should be followed by the ID of the cluster on which we want to run.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 要将作业提交到现有的集群，我们需要指定我们想要提交代码的集群ID。在命令行中，该参数是`--cluster-id`，并且应该跟在我们想要在其上运行的集群ID后面。
- en: Another parameter to note is `emr_action_on_failure` (in the config file) or
    `--emr-action-on-failure` (on the command line). These parameters specify what
    should happen to the cluster if our jobs fail. When we run a single step, this
    defaults to `TERMINATE_CLUSTER`. Having terminate on fail as the default means
    that if our job has any errors, our cluster will shut down. The two other options
    for `emr_action _on_failure` are `CANCEL_AND_WAIT` and `CONTINUE`.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的参数是`emr_action_on_failure`（在配置文件中）或`--emr-action-on-failure`（在命令行中）。这些参数指定了如果我们的作业失败，集群应该发生什么。当我们运行单个步骤时，默认为`TERMINATE_CLUSTER`。将失败终止作为默认设置意味着如果我们的作业有任何错误，我们的集群将关闭。`emr_action_on_failure`的另外两个选项是`CANCEL_AND_WAIT`和`CONTINUE`。
- en: '`CANCEL_AND_WAIT` tells the cluster to cancel the other steps that have been
    queued up and to hold off on doing anything. This is useful if your steps are
    related. For example, if you have three steps in an extract-transform-load workflow—one
    for each extract, transform, and load—you don’t want your load step running if
    your transform step hasn’t completed properly.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`CANCEL_AND_WAIT` 告诉集群取消已排队的其他步骤，并暂停执行任何操作。如果您的步骤相关，这很有用。例如，如果您有一个提取-转换-加载工作流程中的三个步骤——每个步骤分别用于提取、转换和加载——您不希望加载步骤在转换步骤未正确完成时运行。'
- en: '`CONTINUE` tells the cluster to go ahead and work the other steps. This is
    useful when the steps aren’t related; for example, if you’re running batch analytics.
    The results of one analytics step won’t necessarily impact the next step, so it’s
    fine to continue with our analytics jobs if we have errors in one of them. We
    use `CONTINUE` in the following listing.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`CONTINUE` 告诉集群继续执行其他步骤。当步骤不相关时，这很有用；例如，如果您正在运行批量分析。一个分析步骤的结果不一定会影响下一个步骤，因此，如果我们其中一个步骤有错误，我们仍然可以继续我们的分析作业。我们在以下列表中使用
    `CONTINUE`。'
- en: Listing 12.6\. Specifying cluster ID and failure behavior in an mrjob config
    file
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.6\. 在 mrjob 配置文件中指定集群 ID 和故障行为
- en: '[PRE10]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1* Specifying a cluster ID and an action on failure from the command line
    allows us to save time repeatedly setting up clusters to run fast jobs.**'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从命令行指定集群 ID 和故障时的操作可以让我们节省时间，反复设置集群以快速运行作业。**'
- en: Viewing our output in S3
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 S3 中查看我们的输出
- en: Let’s take a look at the output of our Hadoop job. When we called our mrjob
    script, we specified an output directory. This was a folder in an S3 bucket. Our
    output was written as objects to that bucket. If you navigate to that bucket in
    the browser, you should see a list of objects ([figure 12.5](#ch12fig05)).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的 Hadoop 作业的输出。当我们调用我们的 mrjob 脚本时，我们指定了一个输出目录。这是一个 S3 桶中的文件夹。我们的输出被写入到该桶中的对象。如果您在浏览器中导航到该桶，您应该会看到一个对象列表（[图
    12.5](#ch12fig05)）。
- en: Figure 12.5\. In your browser, the bucket lists the objects created as a result
    of our Hadoop process.
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.5\. 在您的浏览器中，该桶列出了我们的 Hadoop 处理结果创建的对象。
- en: '![](12fig05_alt.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig05_alt.jpg)'
- en: Each of these objects was created as a result of our crash counts Hadoop process
    and contains a part of the results. Each line of these files will have the same
    pattern as the output from our mrjob class’s `.reducer`. The first element on
    each line will be the number of vehicles involved in the crash, and the second
    element will be the number of times we saw a crash involving that number of vehicles.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象都是我们的 Hadoop 处理崩溃计数的结果，并包含结果的一部分。这些文件中的每一行都将与我们的 mrjob 类的 `.reducer` 输出具有相同的模式。每一行的第一个元素将是参与事故的车辆数量，第二个元素将是看到涉及该数量车辆的碰撞次数。
- en: If we don’t want to have our results stored to an S3 bucket, omitting the `--output-dir`
    parameter will instead print those values to our screen. Outputting the values
    to our screen can be useful in situations where we know we won’t need to use those
    results in future workflows through EMR. One example might be when we’re testing
    our job. We can run it with a few small instances and print the results locally
    for testing, then use many instances and save the results when we’ve validated
    that the job works.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不希望将结果存储到 S3 桶中，省略 `--output-dir` 参数将改为将那些值打印到我们的屏幕上。在知道我们不会在未来通过 EMR 使用那些结果的情况下，将值输出到屏幕上可能很有用。一个例子可能是当我们测试我们的作业时。我们可以用几个小实例运行它，并将结果本地打印出来以进行测试，然后在验证作业正常工作后使用许多实例并保存结果。
- en: In this section, we’ve reviewed how to submit a Hadoop job to a cloud-compute
    cluster using mrjob and Amazon Web Services’ EMR. Hadoop on EMR is excellent for
    large data processing workloads, such as batch analytics or extract-transform-load.
    In the next section, we’ll review using Spark on EMR.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们回顾了如何使用 mrjob 和亚马逊网络服务的 EMR 将 Hadoop 作业提交到云计算集群。EMR 上的 Hadoop 对于大型数据处理工作负载，如批量分析或提取-转换-加载，非常出色。在下一节中，我们将回顾在
    EMR 上使用 Spark。 '
- en: 12.2\. Machine learning in the cloud with Spark on EMR
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2\. 在 EMR 上使用 Spark 进行云中的机器学习
- en: When I introduced Hadoop and Spark in [chapter 7](kindle_split_017.html#ch07),
    I introduced both of them as frameworks for distributed computing. Hadoop is great
    for low-memory workloads and massive data. Spark is great for jobs that are harder
    to break down into map and reduce steps, and situations where we can afford higher
    memory machines. In this section, we’ll focus on how we can use Spark to train
    a machine learning model on large data in the cloud on EMR.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在[第7章](kindle_split_017.html#ch07)中介绍Hadoop和Spark时，我将它们都介绍为分布式计算的框架。Hadoop非常适合低内存工作负载和大量数据。Spark非常适合难以分解为map和reduce步骤的工作，以及我们可以负担更高内存机器的情况。在本节中，我们将关注如何使用Spark在EMR上对云中的大量数据进行机器学习模型的训练。
- en: 12.2.1\. Writing our machine learning model
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.1\. 编写我们的机器学习模型
- en: 'Before we can run our machine learner in the cloud, let’s start by building
    a model locally on some testing data. This will mirror a process we might perform
    if we were running machine learning algorithms on a truly large dataset:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以在云端运行我们的机器学习器之前，让我们首先在测试数据上本地构建一个模型。这将模拟如果我们在一个真正的大型数据集上运行机器学习算法时可能会执行的过程：
- en: Get a sample of the full dataset.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取整个数据集的样本。
- en: Train and evaluate a few models on that dataset.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在该数据集上训练和评估几个模型。
- en: Select some models to evaluate on the full dataset.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从中选择一些模型在完整数据集上进行评估。
- en: Train several models on the full dataset in the cloud.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在云端对整个数据集训练几个模型。
- en: This process has the virtue of making it possible for you to test lots of models
    quickly and cheaply on your local machine. And later, because we’re using scalable
    frameworks and a scalable computing style, we can bring the models we like into
    the cloud and test them on the full dataset ([figure 12.6](#ch12fig06)).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的好处是，它使你能够在本地机器上快速且低成本地测试大量模型。而且，由于我们使用的是可扩展的框架和可扩展的计算风格，我们可以将我们喜欢的模型带到云端，并在完整数据集上测试它们（[图12.6](#ch12fig06)）。
- en: Figure 12.6\. A common machine learning process for large datasets is to sample
    many models locally and then evaluate the best models in the cloud.
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.6\. 对于大型数据集，一个常见的机器学习过程是在本地采样许多模型，然后在云端评估最佳模型。
- en: '![](12fig06_alt.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig06_alt.jpg)'
- en: For this scenario, we’ll continue to work with the car crash data we uploaded
    in [chapter 11](kindle_split_022.html#ch11) and explored in the first section
    of this chapter.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个场景，我们将继续使用我们在[第11章](kindle_split_022.html#ch11)中上传并在本章第一部分探索的车祸数据。
- en: '|  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Scenario: Car Crash Analysis'
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景：车祸分析
- en: Root-cause analysis of car crashes is a key way governments and safety organizations
    make driving safer. We’ve been asked by one such organization to develop a machine
    learning model that can predict which conditions lead to crashes that involve
    several vehicles (three or more) and which conditions lead to crashes that involve
    only one vehicle.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 车祸的根本原因分析是政府和安全组织使驾驶更安全的关键方式。我们被这样一个组织要求开发一个机器学习模型，可以预测哪些条件会导致涉及多辆车（三辆或更多）的碰撞，以及哪些条件会导致只涉及一辆车的碰撞。
- en: '|  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If you worked through [chapter 10](kindle_split_020.html#ch10), where we learned
    about machine learning in Spark, you may want to try this part yourself as a challenge.
    We’ll use a naïve Bayes classifier as the machine learning model for this scenario.
    The naïve Bayes algorithm is a simple, probabilistic classifier that is often
    used for baseline assessments of the difficulty of machine learning problems,
    especially in text analytics. Problems where naïve Bayes algorithms perform poorly
    can be considered difficult, whereas problems where naïve Bayes algorithms perform
    well are easy.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经完成了[第10章](kindle_split_020.html#ch10)，其中我们学习了Spark中的机器学习，你可能想自己尝试这一部分作为挑战。我们将使用朴素贝叶斯分类器作为此场景的机器学习模型。朴素贝叶斯算法是一种简单、概率分类器，常用于对机器学习问题难度的基线评估，尤其是在文本分析中。朴素贝叶斯算法表现不佳的问题可以认为是困难的，而朴素贝叶斯算法表现良好的问题则相对容易。
- en: 'The first thing we need to do to run a naïve Bayes algorithm is the same as
    the first thing we did for decision trees in [chapter 10](kindle_split_020.html#ch10):
    we need to read in data. Our data is in the JSON lines format, so the best way
    to read in our data is to use the `.textFile` method of a `SparkContext` and then
    chain `.map` methods together to transform the data into a version ready for transformation
    into a Spark `DataFrame`. Spark `DataFrame`s are the required data format for
    Spark’s built-in machine learning libraries.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 运行朴素贝叶斯算法的第一步与第10章中决策树的第一步相同：我们需要读取数据。我们的数据是JSON行格式，因此读取数据的最佳方式是使用 `SparkContext`
    的 `.textFile` 方法，然后链式调用 `.map` 方法将数据转换成适合转换为Spark `DataFrame` 的版本。Spark `DataFrame`s
    是Spark内置机器学习库所需的数据格式。
- en: To transform the data from JSON lines into a sequence of Python objects, we
    first need to split the data into lines. We can do this with a `.flatMap` and
    a `.split` on all newline characters. We use `.flatMap` here instead of normal
    `.map` because our original sequence is a sequence of files. If we used a standard
    `.map`, we’d have a sequence of sequences resulting from each file being transformed
    into a sequence of lines. What we want is a single sequence of lines. The `.flatMap`
    method flattens our sequence of sequences into a single sequence. From here, we
    can map the `loads` function from the `JSON` module across all the lines. This
    method, which we’ve used a few times already, converts a JSON-formatted string
    into a Python object.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据从JSON行转换为Python对象的序列，我们首先需要将数据拆分为行。我们可以通过在所有换行符上使用 `.flatMap` 和 `.split`
    来完成此操作。我们在这里使用 `.flatMap` 而不是正常的 `.map`，因为我们的原始序列是一系列文件。如果我们使用标准的 `.map`，那么每个文件被转换成一系列行，我们会得到一系列序列。我们想要的只是一个行序列。`.flatMap`
    方法将我们的序列序列扁平化为一个单一的序列。从这里，我们可以将 `JSON` 模块中的 `loads` 函数映射到所有行上。这种方法我们已经使用过几次，它将JSON格式的字符串转换为Python对象。
- en: Additionally, we’ll want to improve the way the times are recorded. In the data,
    the times are recorded as raw times. That’s not useful because our machine learning
    model likely won’t have enough data to learn that 11:45 a.m. and 1:03 p.m. are
    closely related, but 3:45 p.m. and 5:03 p.m. likely have very different driving
    conditions (because of the beginning of evening commute traffic). [Listing 12.7](#ch12ex07)
    includes a small function that makes some sense of the times.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想改进记录时间的方式。在数据中，时间以原始时间记录。这没有用，因为我们的机器学习模型可能没有足够的数据来学习上午11:45和下午1:03是密切相关的，但下午3:45和下午5:03可能具有非常不同的驾驶条件（因为傍晚通勤交通的开始）。[列表12.7](#ch12ex07)
    包含一个使时间有意义的函数。
- en: 'Once we have the data in a sequence of Python objects, we also want to make
    two more cleanup transformations. First, we’ll want to group the crashes into
    three categories:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据放入Python对象的序列中，我们还想进行另外两个清理转换。首先，我们将想要将碰撞分为三类：
- en: Single-vehicle crashes
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单辆车碰撞
- en: Two-vehicle crashes
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双辆车碰撞
- en: Three-or-more-vehicle crashes
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三辆车或更多车辆的碰撞
- en: The number of crashes will be the target variable for our analysis. To do this
    grouping, we’ll need to write a helper function that transforms the `'Number of
    Vehicles Involved'` field, as shown in the following listing.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 碰撞数量将是我们的分析的目标变量。为了进行此分组，我们需要编写一个辅助函数来转换 `'涉及车辆数量'` 字段，如下所示。
- en: Listing 12.7\. Reading and cleaning crash data from JSON lines
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.7\. 从JSON行读取和清理碰撞数据
- en: '[PRE11]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: From this point, we’re ready to convert our `RDD` into a `DataFrame` and prepare
    our `DataFrame` for Spark’s machine learners. To transform the `RDD` into a `DataFrame`,
    we use the `.createDataFrame` method of our `SparkSession` ([figure 12.7](#ch12fig07)).
    `SparkSession` objects are central to Spark’s SQL, `DataFrame`, and machine learning
    capabilities and serve as a mirror to `SparkContext` objects for `RDDs`.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，我们准备将我们的 `RDD` 转换为 `DataFrame` 并为Spark的机器学习器准备我们的 `DataFrame`。要将 `RDD`
    转换为 `DataFrame`，我们使用 `SparkSession` 的 `.createDataFrame` 方法（[图12.7](#ch12fig07)）。`SparkSession`
    对象是Spark的SQL、`DataFrame` 和机器学习能力的核心，并作为 `SparkContext` 对象的 `RDDs` 的镜像。
- en: Figure 12.7\. We can use both the `SparkContext` and the `SparkSession` to take
    advantage of `RDD`s and `DataFrame`s. We’ll need to explicitly convert our `RDD`
    to a `DataFrame` when we want to use the `DataFrame` methods for machine learning.
  id: totrans-359
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.7\. 我们可以使用 `SparkContext` 和 `SparkSession` 来利用 `RDD`s 和 `DataFrame`s。当我们想要使用
    `DataFrame` 的机器学习方法时，我们需要显式地将我们的 `RDD` 转换为 `DataFrame`。
- en: '![](12fig07_alt.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig07_alt.jpg)'
- en: Once we have our data in a `DataFrame`, we need to use the `StringIndexer` to
    transform our variables into the indexed format that Spark expects ([listing 12.8](#ch12ex08)).
    We won’t go into the details of the indexer code here, because our focus for this
    section is on Spark and EMR. If you’d like a refresher, we originally discussed
    these concepts in [chapter 10](kindle_split_020.html#ch10), specifically [section
    10.2.2](kindle_split_020.html#ch10lev2sec4) with [listings 10.2](kindle_split_020.html#ch10ex02),
    [10.3](kindle_split_020.html#ch10ex03), and [10.4](kindle_split_020.html#ch10ex04).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的数据在`DataFrame`中，我们需要使用`StringIndexer`将我们的变量转换为Spark期望的索引格式（[列表12.8](#ch12ex08)）。我们不会在这里详细介绍索引器代码，因为本节的重点是Spark和EMR。如果你需要复习，我们最初在[第10章](kindle_split_020.html#ch10)中讨论了这些概念，特别是[第10.2.2节](kindle_split_020.html#ch10lev2sec4)以及[列表10.2](kindle_split_020.html#ch10ex02)、[10.3](kindle_split_020.html#ch10ex03)和[10.4](kindle_split_020.html#ch10ex04)。
- en: Listing 12.8\. Preparing the crashes `RDD` for machine learning
  id: totrans-362
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.8. 准备机器学习用的事故`RDD`
- en: '[PRE12]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With our `DataFrame` ready for machine learning, the last step is to set up
    the actual machine learning algorithm. As noted earlier, for this example we want
    to use a naïve Bayes algorithm. Like in our final example from [chapter 10](kindle_split_020.html#ch10),
    we’ll use cross-validation to assess model performance. As you might guess from
    the algorithm’s name (*naïve*), the naïve Bayes model has relatively few parameters
    compared to more sophisticated models, so we’ll only optimize a single parameter
    of the model: smoothing. The smoothing parameter refers to how much additive smoothing
    is used in the model. The additive smoothing process prevents zeros from dominating
    the model, instead treating zeros as *very small* numbers. Typical values are
    1/1000, 1/100, 1/10, and 1\. You can see the machine learning code in the following
    listing. You’ll notice a lot of similarities between this code and the code we
    wrote for our random forest classifier in [section 10.3.2](kindle_split_020.html#ch10lev2sec6).'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`DataFrame`已经准备好用于机器学习，最后一步是设置实际的机器学习算法。正如之前提到的，对于这个例子，我们想使用朴素贝叶斯算法。就像在[第10章](kindle_split_020.html#ch10)的最终示例中一样，我们将使用交叉验证来评估模型性能。从算法的名称（*朴素*）你可能已经猜到，与更复杂的模型相比，朴素贝叶斯模型具有相对较少的参数，所以我们只优化模型的单个参数：平滑度。平滑度参数指的是模型中使用的附加平滑度量。附加平滑过程防止零值主导模型，而是将零值视为*非常小的*数字。典型值是1/1000，1/100，1/10和1。你可以在下面的列表中看到机器学习代码。你会发现这段代码和我们在[第10.3.2节](kindle_split_020.html#ch10lev2sec6)中为随机森林分类器编写的代码有很多相似之处。
- en: Listing 12.9\. A naïve Bayes classifier for vehicles in crashes
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.9. 事故中车辆的朴素贝叶斯分类器
- en: '[PRE13]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One thing you may notice about this code that’s different from the code in [chapter
    10](kindle_split_020.html#ch10) is that we refer to our evaluation metric as F1
    instead of AUC. F1, like AUC, is a metric that assesses trade-offs between false
    positives and false negatives. It’s most prominently used in information retrieval
    and document classification. For our purposes, it’s enough to know that F1 scores
    can range between 0 and 1, with higher numbers being better.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，这段代码与[第10章](kindle_split_020.html#ch10)中的代码不同的一点是我们将评估指标称为F1而不是AUC。F1，就像AUC一样，是一个评估假阳性与假阴性之间权衡的指标。它在信息检索和文档分类中应用最为广泛。对于我们来说，知道F1分数可以在0到1之间变化，数值越高越好就足够了。
- en: You can run this code locally, pointing the `.textFiles` method and the `.bestModel.save`
    method to locations on your local machine. For the inputs to `.textFiles`, I recommend
    using a subset of the full crashes dataset. This will speed up the test process—the
    entire dataset will take several minutes to process on a single machine. For the
    output, you’re specifying a location where Spark will try to create a directory
    and store a description of the model. This should be a directory that doesn’t
    exist yet.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本地运行此代码，将`.textFiles`方法和`.bestModel.save`方法指向本地机器上的位置。对于`.textFiles`的输入，我建议使用完整事故数据集的一个子集。这将加快测试过程——整个数据集在单台机器上处理需要几分钟。对于输出，你指定了一个Spark将尝试创建目录并存储模型描述的位置。这应该是一个尚未存在的目录。
- en: '|  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Reminder: Spark-submit'
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提醒：Spark-submit
- en: Remember to run your Spark code with the `spark-submit` utility instead of Python.
    The `spark-submit` utility queues up a Spark job, which will run in parallel locally
    and simulate what would happen if you ran the program on an active cluster.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，要用`spark-submit`实用程序而不是Python运行你的Spark代码。`spark-submit`实用程序将Spark作业排队，它将在本地并行运行并模拟如果你在活动集群上运行程序会发生什么。
- en: '|  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 12.2.2\. Setting up an EMR cluster for Spark
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.2\. 为 Spark 设置 EMR 集群
- en: 'To run this machine learning job in the cloud, we’ll need a cluster on which
    to run our Spark job. Earlier in this section, we saw two ways to set up an EMR
    cluster programmatically using mrjob:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 要在云中运行这个机器学习作业，我们需要一个运行 Spark 作业的集群。在本节的前面部分，我们看到了两种使用 mrjob 以编程方式设置 EMR 集群的方法：
- en: We can set up single-step clusters by submitting a Hadoop job with the `-r emr`
    flag set.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过提交带有 `-r emr` 标志的 Hadoop 作业来设置单步集群。
- en: We can set up persistent clusters by running the `mrjob create-cluster` utility.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过运行 `mrjob create-cluster` 实用工具来设置持久集群。
- en: In this subsection, I’ll show you how to set up a Spark cluster with mrjob and
    introduce you to the EMR cluster wizard.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我将向您展示如何使用 mrjob 设置 Spark 集群，并介绍 EMR 集群向导。
- en: Setting up a Spark cluster with mrjob
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 mrjob 设置 Spark 集群
- en: Back in [section 12.1](#ch12lev1sec1), we set up EMR clusters using mrjob so
    that we could run our Hadoop jobs in the cloud. As part of this, we wrote an mrjob
    config file ([listing 12.4](#ch12ex04)). The mrjob config file was a declaration
    of what we wanted our cluster to look like, as shown in [listing 12.10](#ch12ex10).
    We can use that same approach to set up a Spark cluster. All we’ll need to do
    is specify a few extra options.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 12.1 节](#ch12lev1sec1) 中，我们使用 mrjob 设置了 EMR 集群，以便我们可以在云中运行我们的 Hadoop 作业。作为这部分内容的一部分，我们编写了一个
    mrjob 配置文件（[列表 12.4](#ch12ex04)）。mrjob 配置文件是我们希望我们的集群看起来像什么的声明，如 [列表 12.10](#ch12ex10)
    所示。我们可以使用相同的方法来设置一个 Spark 集群。我们只需要指定一些额外的选项。
- en: 'Listing 12.10\. Refresher: mrjob config for EMR'
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.10\. 复习：mrjob 配置用于 EMR
- en: '[PRE14]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that this configuration defines a cluster of 21 machines—20 workers and
    1 master. Those machines are of type m1.large and are using AMI version 5.24.0\.
    Additionally, we’ll be setting the cluster up in the us-west-1 region and tagging
    it with “project: Mastering Large Datasets.”'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，此配置定义了一个由 21 台机器组成的集群——20 个工作节点和 1 个主节点。这些机器的类型是 m1.large，并使用 AMI 版本 5.24.0。此外，我们将在
    us-west-1 区域设置集群，并标记为“project: Mastering Large Datasets”。'
- en: For our Spark cluster, the first thing we’ll need to do is change the instances
    we’re using to one that has more memory. Hadoop, as I’ve mentioned before, was
    designed to take advantage of low computing power environments. Spark has greater
    resource requirements. For Spark, the smallest instance type we can us is m1.xlarge.
    When running Spark jobs in production, we can achieve better performance by using
    the AWS C-series instances, which are compute optimized.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 Spark 集群，我们首先需要做的是更改我们使用的实例，以一个具有更多内存的实例。如我之前提到的，Hadoop 是为了利用低计算能力环境而设计的。Spark
    有更高的资源需求。对于 Spark，我们可以使用的最小实例类型是 m1.xlarge。当在生产环境中运行 Spark 作业时，我们可以通过使用 AWS C
    系列实例来获得更好的性能，这些实例是针对计算优化的。
- en: '|  |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**EC2 instance types and clusters**'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '**EC2 实例类型和集群**'
- en: 'We’ll want to know about three types of EC2 instances for cluster computing:
    M-series, C-series, and R-series. M-series instances are the default for cluster
    computing. These instances are solid, general-purpose instances. I recommend using
    them for Hadoop jobs, and for testing Spark jobs. AWS provides C-series instances
    for compute-heavy workloads, which includes Spark analytics. Batch Spark jobs
    are best run in production on C-series instances. Lastly, the R-series of instances
    is a high-memory series. We’ll want to use this series of instances if we’re dealing
    with streaming analytics.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解三种类型的 EC2 实例用于集群计算：M 系列、C 系列和 R 系列。M 系列实例是集群计算的默认选项。这些实例是坚固的通用实例。我建议将它们用于
    Hadoop 作业和 Spark 作业的测试。AWS 为计算密集型工作负载提供 C 系列实例，包括 Spark 分析。批处理 Spark 作业最好在生产环境中在
    C 系列实例上运行。最后，R 系列实例是一个高内存系列。如果我们处理流分析，我们将想要使用这个系列的实例。
- en: '|  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Next, we’ll need to tell mrjob that we want to use Spark. For this, mrjob provides
    an option called `bootstrap_spark`. This takes a boolean variable, so we’ll set
    that to `true`.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要告诉 mrjob 我们想要使用 Spark。为此，mrjob 提供了一个名为 `bootstrap_spark` 的选项。这个选项接受一个布尔变量，因此我们将它设置为
    `true`。
- en: Lastly, we’ll want to be able to access our instance over the command line through
    SSH. SSH is a utility we can use to log in to and run commands on remote servers.
    To set up the cluster so we can log in through SSH, we’ll need to specify an AWS
    .pem key pair.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望能够通过 SSH 命令行访问我们的实例。SSH 是一个我们可以用来登录远程服务器并运行命令的实用工具。为了设置集群以便我们可以通过 SSH
    登录，我们需要指定一个 AWS .pem 密钥对。
- en: If you haven’t set up an AWS EC2 key pair, you can create one using the AWS
    command line tool, which you installed along with boto3\. The command for that
    is `aws ec2 create-key-pair`. You’ll also want to set the mandatory `--key-name`
    option so that you can refer to your key.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有设置AWS EC2密钥对，您可以使用与boto3一起安装的AWS命令行工具创建一个。该命令是`aws ec2 create-key-pair`。您还想要设置强制性的`--key-name`选项，这样您就可以引用您的密钥。
- en: '[PRE15]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: AWS EC2 keys are region specific, and this command will create the key in your
    default region. If you’re not sure what your default region is, you can go to
    [https://console.aws.amazon.com](https://console.aws.amazon.com). The region will
    display as a parameter in the URL; for example, [http://mng.bz/ZeA5](http://mng.bz/ZeA5).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EC2密钥是区域特定的，此命令将在您的默认区域中创建密钥。如果您不确定您的默认区域是什么，您可以访问[https://console.aws.amazon.com](https://console.aws.amazon.com)。区域将显示为URL中的参数；例如，[http://mng.bz/ZeA5](http://mng.bz/ZeA5)。
- en: With that, we’ll have a configuration file ready to set up a Spark cluster.
    Our Spark mrjob configuration file looks like the following listing.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就有了设置Spark集群的配置文件。我们的Spark mrjob配置文件看起来如下所示。
- en: Listing 12.11\. mrjob configuration file for a Spark EMR cluster
  id: totrans-394
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.11\. Spark EMR集群的mrjob配置文件
- en: '[PRE16]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1* Tells EMR to install Spark on the cluster**'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 告诉EMR在集群上安装Spark**'
- en: '***2* Provides EMR the name of the key we’ll use for SSHing into the cluster**'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 为EMR提供我们将用于SSH到集群的密钥名称**'
- en: You can run this cluster with the `create-cluster` command. When you do, you
    should receive a JSON string as a response. You’ll also be able to go to the AWS
    EMR Console and see your cluster setting up. Once you’re satisfied that the cluster
    is running, feel free to shut it down. To do that, merely select the checkbox
    next the cluster and click the Terminate button at the top of the screen.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`create-cluster`命令运行此集群。当您这样做时，您应该会收到一个JSON字符串作为响应。您还可以访问AWS EMR控制台并查看您的集群设置。一旦您确认集群正在运行，您可以随时将其关闭。要这样做，只需选中集群旁边的复选框，然后点击屏幕顶部的终止按钮。
- en: The AWS EMR cluster wizard
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: AWS EMR集群向导
- en: 'In addition to setting up EMR clusters using mrjob, we can also do so using
    the AWS console. Like we saw in [chapter 11](kindle_split_022.html#ch11) with
    S3, the AWS console is a good way to see all of the options that we have when
    we use AWS. To get started, navigate to the EMR console main page: [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/).'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用mrjob设置EMR集群外，我们还可以使用AWS控制台来设置。就像我们在第11章中看到的那样，与S3一起使用，AWS控制台是查看我们使用AWS时所有选项的好方法。要开始，导航到EMR控制台主页面：[https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)。
- en: On this page, you should see a list of clusters, including the ones you may
    have created while running the Hadoop jobs and the one you created from the previous
    subsection on Spark and mrjob. At the top of this page, you should see a button
    inviting you to create a cluster. That button launches the AWS EMR cluster wizard.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个页面上，您应该会看到一系列集群列表，包括您在运行Hadoop作业时可能创建的集群以及您在Spark和mrjob的前一小节中创建的集群。在这个页面的顶部，您应该会看到一个按钮邀请您创建一个集群。该按钮启动AWS
    EMR集群向导。
- en: 'When you click it, you’ll immediately be brought to a quick setup page. On
    this page, there are four sets of options:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 当您点击它时，您将立即被带到快速设置页面。在这个页面上，有四组选项：
- en: General options that describe our cluster
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述我们的集群的常规选项
- en: Software options that tell AWS what we’ll be doing on the cluster
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 软件选项告诉AWS我们在集群上要做什么
- en: Hardware options that tell AWS which instances to reserve for us
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件选项告诉AWS为我们保留哪些实例
- en: Security options that tell AWS how we’ll be accessing the cluster
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安全选项告诉AWS我们将如何访问集群
- en: In the general options (General Configuration) section ([figure 12.8](#ch12fig08)),
    you’ll want to give your cluster a name you’ll recognize. There are two other
    options there defining the Logging behavior and Launch Mode—these are both fine
    by default. Next, in the software options (Software Configuration) (also [figure
    12.8](#ch12fig08)), you’ll want to use the latest EMR release and select the software
    configuration that contains Spark. This tells AWS to install Spark when it’s setting
    up our cluster.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规选项（常规配置）部分（[图12.8](#ch12fig08)），您会给您的集群起一个您能认出的名字。那里还有两个其他选项定义了日志行为和启动模式——这两个都是默认的。接下来，在软件选项（软件配置）（也[图12.8](#ch12fig08)），您会想要使用最新的EMR版本并选择包含Spark的软件配置。这告诉AWS在设置我们的集群时安装Spark。
- en: Figure 12.8\. The General Configuration section of the AWS EMR wizard lets you
    specify the Cluster Name and other configuration options.
  id: totrans-408
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.8\. AWS EMR向导的“常规配置”部分允许您指定集群名称和其他配置选项。
- en: '![](12fig08_alt.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![12fig08_alt.jpg](12fig08_alt.jpg)'
- en: Scroll down and you’ll see hardware and security configuration options ([figure
    12.9](#ch12fig09)). For the hardware options (Hardware Configuration), set Instance
    Type to m1.xlarge and the Number of Instances to 3\. If you change the number
    of instances, you’ll notice that the number of core nodes—the nodes that will
    run work on your cluster—is always one less than the total number of instances
    you’ve selected. This is because one instance always needs to serve as the master
    instance. Lastly, select your EC2 Key Pair from the drop-down menu. If you don’t
    see your key pair listed here, try changing availability zones using the drop-down
    menu in the top right corner of the screen.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动页面，你会看到硬件和安全配置选项（[图12.9](#ch12fig09)）。对于硬件选项（硬件配置），将实例类型设置为m1.xlarge，并将实例数量设置为3。如果你更改实例数量，你会注意到核心节点数——将在你的集群上运行工作的节点——总是比所选的总实例数少一个。这是因为总有一个实例需要作为主实例。最后，从下拉菜单中选择你的EC2密钥对。如果你在这里看不到你的密钥对，请尝试使用屏幕右上角的下拉菜单更改可用区域。
- en: Figure 12.9\. The hardware and security configuration options in the EMR setup
    wizard offer a simple GUI for launching a right-sized cluster.
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.9\. 在EMR设置向导中，硬件和安全配置选项提供了一个简单的GUI来启动一个合适大小的集群。
- en: '![](12fig09_alt.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![12fig09_alt.jpg](12fig09_alt.jpg)'
- en: If you proceeded from here to launch your cluster, you’d launch a cluster that
    is more or less the same as the cluster you launched using mrjob. Instead, though,
    go to the top of the page and select Go to Advanced Options. This will take you
    to a four-step wizard that shows you all of the options for an EMR cluster.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从这里继续启动你的集群，你将启动一个与使用mrjob启动的集群大致相同的集群。然而，相反，请转到页面顶部并选择“转到高级选项”。这将带您进入一个四步向导，显示EMR集群的所有选项。
- en: First, you’ll see a long list of software that is available to you, including
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你会看到一个可供你使用的软件长列表，包括
- en: Hadoop
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hadoop
- en: Spark
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark
- en: JupyterHub
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: JupyterHub
- en: Hive
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hive
- en: Pig
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pig
- en: TensorFlow
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: 'We’ve covered the first two pieces of software—Hadoop and Spark—in this book:
    Hadoop in [chapters 7](kindle_split_017.html#ch07) and [8](kindle_split_018.html#ch08),
    and Spark in [chapters 7](kindle_split_017.html#ch07), [9](kindle_split_019.html#ch09),
    and [10](kindle_split_020.html#ch10). Additionally, we’ve looked at both Hadoop
    and Spark in this chapter. Depending on your background, you may be familiar with
    the remaining four tools.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经介绍了前两种软件——Hadoop和Spark：Hadoop在第[7章](kindle_split_017.html#ch07)和[8章](kindle_split_018.html#ch08)，Spark在第[7章](kindle_split_017.html#ch07)、[9章](kindle_split_019.html#ch09)和[10章](kindle_split_020.html#ch10)。此外，本章我们还讨论了Hadoop和Spark。根据你的背景，你可能熟悉剩下的四个工具。
- en: JupyterHub is a cluster-ready version of the popular Jupyter Notebook software.
    Installing that software means you can run interactive Spark and Hadoop jobs from
    a notebook environment. This is a great tool for data analysts and data scientists.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: JupyterHub是流行的Jupyter Notebook软件的集群版。安装该软件意味着您可以从笔记本环境中运行交互式的Spark和Hadoop作业。这对于数据分析师和数据科学家来说是一个非常好的工具。
- en: Hive and Pig are similar tools that provide SQL or SQL-like interfaces to large
    datasets. Analysts can use Hive to compile SQL code to Hadoop MapReduce jobs.
    Likewise, we can use Pig to compile *Pig-latin* commands to run Hadoop MapReduce
    jobs. Both pieces of software are aimed at making large datasets accessible to
    traditional business analysts.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: Hive和Pig是类似工具，它们为大数据集提供了SQL或类似SQL的接口。分析师可以使用Hive将SQL代码编译成Hadoop MapReduce作业。同样，我们也可以使用Pig将*Pig-latin*命令编译成运行Hadoop
    MapReduce作业。这两款软件都旨在使大数据集对传统商业分析师可访问。
- en: The last of the four, TensorFlow, is a popular deep learning library. The library
    is used for many state-of-the-art implementations of deep learning. The ability
    to run TensorFlow on AWS can reduce training time dramatically, because it enables
    us to run jobs on GPU (Graphic Processing Unit—processors designed for fast arithmetic)
    clusters or TPUs (Tensor Processing Units—processors designed for deep learning)
    that would be cost-prohibitive if not in the cloud.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个是TensorFlow，这是一个流行的深度学习库。该库被用于许多最先进的深度学习实现。在AWS上运行TensorFlow的能力可以显著减少训练时间，因为它使我们能够在GPU（图形处理单元——为快速算术设计的处理器）集群或TPU（张量处理单元——为深度学习设计的处理器）上运行作业，如果没有云，这些作业将是成本高昂的。
- en: If you click through to the next page in the wizard, you’ll see detailed options
    for defining the hardware available to your cluster ([figure 12.10](#ch12fig10)).
    In particular on this page, pay attention to the instance groups at the bottom.
    You’ll notice that here you not only have the ability to define which type of
    instance you want, you also can see and set resource and pricing information for
    those instances. For example, we can see that the m3.xlarge instance that AWS
    suggests for us has 8 virtual cores and 15 GB of memory.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你点击向导中的下一页，你将看到定义集群可用硬件的详细选项（[图 12.10](#ch12fig10)）。特别是在这一页上，请注意底部的实例组。你会注意到，在这里你不仅能够定义你想要的实例类型，还可以查看和设置这些实例的资源定价信息。例如，我们可以看到
    AWS 建议我们使用的 m3.xlarge 实例具有 8 个虚拟核心和 15 GB 的内存。
- en: Figure 12.10\. The advanced hardware configuration options allow us to bid for
    spot instances and set up auto-scaling for our cluster.
  id: totrans-426
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.10\. 高级硬件配置选项使我们能够竞标临时实例并设置集群的自动扩展。
- en: '![](12fig10_alt.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![12fig10_alt.jpg](12fig10_alt.jpg)'
- en: Additionally, we can select either On-Demand or Spot pricing for our instances.
    Spot pricing is a short-term market rate price that we can use to save money on
    our compute jobs. When we enable spot pricing, Amazon gives us access to unused
    instances at a low rate. This low rate comes with some risk, though. If the spot
    price ever exceeds what we bid—such as when demand for AWS resources is high—Amazon
    may shut down our instances and lease them to another buyer. That said, if we’re
    running batch analytics jobs over night, this is often an excellent way to save
    money.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以为我们的实例选择按需或临时定价。临时定价是一种短期市场价格，我们可以用它来在计算作业上节省资金。当我们启用临时定价时，亚马逊以低廉的价格为我们提供未使用的实例。然而，这种低廉的价格伴随着一些风险。如果临时价格超过我们的出价——例如，当
    AWS 资源需求高时——亚马逊可能会关闭我们的实例并将它们租给另一个买家。话虽如此，如果我们正在夜间运行批量分析作业，这通常是一种节省资金的好方法。
- en: Lastly, in the Create Cluster—Advanced Options view, we can see the Auto Scaling
    option. When we turn Auto Scaling on, AWS will watch our resource usage and scale
    our cluster up or down as necessary. For example, if we’re running a big Spark
    job, the cluster may scale up to the maximum number of instances we’ve set. When
    that job finishes, the cluster will eventually scale down to the minimum until
    we’re ready to run another job.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在创建集群——高级选项视图中，我们可以看到自动扩展选项。当我们打开自动扩展时，AWS 将监视我们的资源使用情况，并根据需要扩展或缩小我们的集群。例如，如果我们正在运行一个大的
    Spark 作业，集群可能会扩展到我们设置的实例最大数量。当那个作业完成时，集群最终会缩小到最小，直到我们准备好运行另一个作业。
- en: Clicking through to the next two pages, you’ll be able to
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下一页，你将能够
- en: define logging settings
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义日志设置
- en: add free-form key-value tags to your cluster
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的集群添加自由格式的键值标签
- en: add an EC2 key pair for SSH access
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加 EC2 密钥对以进行 SSH 访问
- en: Once you’ve had a chance to look at all the options available to you on these
    pages, you can create a cluster using either the Quick or Advanced settings. If
    you preferred using mrjob, you also can relaunch your cluster with the `mrjob
    create-cluster` command from the previous subsection. You’ll need a running cluster,
    with Spark, with an attached EC2 key pair for the next subsection when we will
    SSH into our cluster.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有机会查看这些页面上的所有选项，你可以使用快速或高级设置创建一个集群。如果你更喜欢使用 mrjob，你也可以从上一个子节使用 `mrjob create-cluster`
    命令重新启动你的集群。在接下来的子节中，当我们通过 SSH 连接到我们的集群时，你需要一个运行中的集群，其中包含 Spark，并附加了 EC2 密钥对。
- en: '|  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You can run the examples in this chapter for less than $5 at the time of this
    writing.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在写作时以不到 5 美元的价格运行本章中的示例。
- en: '|  |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 12.2.3\. Running PySpark jobs from our cluster
  id: totrans-439
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.3\. 从我们的集群运行 PySpark 作业
- en: 'Once we have our cluster up and running, we’re almost ready to run our machine
    learning job. There are only five steps left:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的集群启动并运行，我们几乎准备好运行我们的机器学习作业了。只剩下五个步骤：
- en: Modifying our script for the cloud
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改我们的云脚本
- en: Adding our script to S3
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的脚本添加到 S3
- en: SSHing into the master node
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 SSH 连接到主节点
- en: Installing the required software
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所需的软件
- en: Configuring our Spark cluster to run Python3
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置我们的 Spark 集群以运行 Python3
- en: Running our Spark job
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行我们的 Spark 作业
- en: These steps will take us from having a local-only machine learning script to
    having run our machine learning job on the cloud. But first, we need to make two
    small modifications to our machine learning script.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将使我们从只有本地机器学习脚本的机器学习脚本，到在云上运行我们的机器学习作业。但首先，我们需要对我们的机器学习脚本进行两项小的修改。
- en: In the naïve Bayes script from earlier in this section, we used local file paths
    as inputs and outputs. That was important because we wanted to test the script
    locally. When we’re running the script in the cloud, we won’t have access to our
    local resources—only resources that are in the cloud. We’ll need to change those
    paths to point to cloud locations. Specifically, we’ll point both of them to S3
    locations.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节前面提到的朴素贝叶斯脚本中，我们使用了本地文件路径作为输入和输出。这很重要，因为我们想在本地上测试脚本。当我们云中运行脚本时，我们将无法访问本地资源——只有云中的资源。我们需要更改这些路径以指向云位置。具体来说，我们将它们都指向S3位置。
- en: To reference an S3 bucket, prepend S3:// to the name of your bucket. For example,
    if you had a bucket named my-favorite-S3-bucket, the path to that bucket would
    be S3://my-favorite-S3-bucket/. For S3 folders, you can add any word after the
    bucket name. Point the input path to target the bucket that contains your car
    crash data files and point the output path to target a folder in another bucket,
    as shown in the following listing. Once you’ve done that, save this as a new file.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 要引用一个S3存储桶，在你的存储桶名称前加上S3://。例如，如果你有一个名为my-favorite-S3-bucket的存储桶，该存储桶的路径将是S3://my-favorite-S3-bucket/。对于S3文件夹，你可以在存储桶名称后添加任何单词。将输入路径指向包含你的车祸数据文件的存储桶，将输出路径指向另一个存储桶中的文件夹，如下所示。完成这些后，将其保存为一个新的文件。
- en: Listing 12.12\. Cloud-ready paths for input and output
  id: totrans-450
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.12。输入和输出的云准备路径
- en: '[PRE17]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With these paths defined, we have a cloud-ready script. Unfortunately, we can’t
    access this script from the cloud if it’s on our local machine. We also need to
    move the script into the cloud. Again, we’ll use S3\. If you’re up for a challenge,
    you can try on your own to create a new bucket and upload the script there using
    either the AWS console or boto3\. Otherwise, you can follow the instructions from
    [section 11.2.1](kindle_split_022.html#ch11lev2sec6) to create a new bucket and
    upload your new script.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这些路径后，我们就拥有了一个适用于云的脚本。不幸的是，如果这个脚本在我们的本地机器上，我们就无法从云中访问它。我们还需要将脚本移动到云中。同样，我们将使用S3。如果你愿意接受挑战，你可以尝试自己创建一个新的存储桶并将脚本上传到那里，无论是使用AWS控制台还是boto3。否则，你可以按照[第11.2.1节](kindle_split_022.html#ch11lev2sec6)中的说明创建一个新的存储桶并上传你的新脚本。
- en: 'Once the script is uploaded, we’re ready to log in to our cluster. If you’re
    on a Mac or Linux machine, you’ll be able to use the built-in SSH utility. On
    Windows, you’ll need to download a terminal emulator that supports SSH: PuTTY
    is the conventional choice for this. You can download it here: [http://mng.bz/RP4D](http://mng.bz/RP4D).'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦脚本上传完毕，我们就准备好登录到我们的集群。如果你使用的是Mac或Linux机器，你将能够使用内置的SSH工具。在Windows上，你需要下载一个支持SSH的终端模拟器：PuTTY是这一领域的传统选择。你可以从这里下载它：[http://mng.bz/RP4D](http://mng.bz/RP4D)。
- en: Once you know you’ll be able to use SSH, head to the EMR console in your browser
    and find your running EMR cluster. At the top of the page you’ll see a path to
    the cluster ([figure 12.11](#ch12fig11)). This is the path you’ll SSH into.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道你可以使用SSH，请转到浏览器中的EMR控制台并找到你的运行中的EMR集群。在页面顶部，你会看到一个集群的路径（[图12.11](#ch12fig11)）。这就是你将SSH进入的路径。
- en: Figure 12.11\. You can find the address of the master node of your cluster at
    the top of the console page for your cluster.
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.11。你可以在你集群的控制台页面顶部找到你集群的主节点地址。
- en: '![](12fig11_alt.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![12fig11_alt.jpg](12fig11_alt.jpg)'
- en: 'To enter the cluster, open up your terminal or PuTTY and enter the address
    of the cluster. (AWS provides documentation for connecting using PuTTY at [http://mng.bz/2Jn9](http://mng.bz/2Jn9).)
    Additionally, you’ll need to identify yourself by pointing SSH to your key:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入集群，打开你的终端或PuTTY，并输入集群的地址。（AWS提供了使用PuTTY连接的文档，[http://mng.bz/2Jn9](http://mng.bz/2Jn9)。）此外，你还需要通过将SSH指向你的密钥来识别自己：
- en: '[PRE18]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If this goes through successfully, you should see EMR in ASCII art on your terminal
    screen ([figure 12.12](#ch12fig12)). This is how you’ll know you’re logged into
    the master node. From this screen, the entire cluster is in your control. You
    can install software and run scripts just like you would if you were at the command
    line of your local machine. For our machine learning script, we’ll need NumPy—a
    Python library for numerical processing. Let’s make sure it’s installed, and we’ll
    install the toolz package for good measure.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该能在你的终端屏幕上看到ASCII艺术的EMR（[图12.12](#ch12fig12)）。这将告诉你你已经登录到了主节点。从这个屏幕开始，整个集群都在你的控制之下。你可以像在本地机器的命令行一样安装软件和运行脚本。对于我们的机器学习脚本，我们需要NumPy——这是一个用于数值处理的Python库。让我们确保它已安装，并且为了保险起见，我们还将安装toolz包。
- en: Figure 12.12\. Some nice EMR ASCII art will greet you when you log in to an
    EMR cluster.
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.12\. 当你登录到EMR集群时，会看到一个漂亮的EMR ASCII艺术。
- en: '![](12fig12_alt.jpg)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig12_alt.jpg)'
- en: The way we install Python libraries doesn’t change whether we’re on our local
    machine or on a remote server; we use `pip`. Because we want to use Python3, however,
    we’ll need to install `pip3`, which doesn’t come installed by default. You can
    install `pip3` with the command `sudo yum install -y pip3`. You can then use `pip3`
    to install NumPy and toolz with the command `sudo pip3 install -y numpy toolz`.
    Additionally, you can install any other software you may wish to use. Installing
    software on the master node will replicate this install across all of the other
    nodes.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们安装Python库的方式不会因为我们在本地机器上还是在远程服务器上而改变；我们使用 `pip`。然而，因为我们想使用Python3，所以我们需要安装
    `pip3`，这通常不是默认安装的。你可以使用命令 `sudo yum install -y pip3` 来安装 `pip3`。然后你可以使用 `pip3`
    来使用命令 `sudo pip3 install -y numpy toolz` 安装NumPy和toolz。此外，你可以安装你可能希望使用的任何其他软件。在主节点上安装软件将在所有其他节点上复制此安装。
- en: At this point, if you want to test NumPy, toolz, or any other library, you can
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，如果你想测试NumPy、toolz或其他任何库，你可以
- en: call Python with the `python3` command
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `python3` 命令调用Python
- en: import the library you want to test
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入你想要测试的库
- en: run any Python code you’d like from the console
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从控制台运行任何你想要的Python代码
- en: Now our Python3 environment is set up to run our machine learning script. Unfortunately,
    Spark is still configured to run with legacy Python. Let’s configure our Spark
    environment so it runs Python3\. To do this, we’ll need to modify a shell file
    on the server. On the server, we can only use terminal-based text editors. The
    easiest of these editors to use is *nano*.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了Python3环境来运行我们的机器学习脚本。不幸的是，Spark仍然配置为使用遗留的Python。让我们配置我们的Spark环境，使其运行Python3。为此，我们需要修改服务器上的一个shell文件。在服务器上，我们只能使用基于终端的文本编辑器。这些编辑器中最容易使用的是
    *nano*。
- en: To open a file in nano, we type `nano` and then the file’s name. To open the
    file where the Spark environment variables are stored, we’ll type
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 要在nano中打开文件，我们输入 `nano` 然后文件名。要打开存储Spark环境变量的文件，我们将输入
- en: '[PRE19]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When you open this up, you’ll see a shell script. At the bottom of the script,
    modify the line that says `PYSPARK_PYTHON` so it reads `PYSPARK_PYTHON=python3`.
    When you’re done, you can save and exit the file by pressing the following keys
    (these will be the same on Mac, PC, and Linux):'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打开它时，你会看到一个shell脚本。在脚本的底部，修改显示 `PYSPARK_PYTHON` 的行，使其读取 `PYSPARK_PYTHON=python3`。当你完成时，你可以通过按以下键来保存并退出文件（这些键在Mac、PC和Linux上都是相同的）：
- en: '***Control-O—*** Begins to save the file'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***Control-O—*** 开始保存文件'
- en: '***Enter—*** Writes the file to the disk'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***Enter—*** 将文件写入磁盘'
- en: '***Control-X—*** Begins to exit nano'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***Control-X—*** 开始退出nano'
- en: '***Y—*** Tells nano that yes, you want to exit'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***Y—*** 告诉nano你想要退出'
- en: 'Finally, to put these changes into action, activate the Spark environment you
    just modified:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了使这些更改生效，激活你刚刚修改的Spark环境：
- en: '[PRE20]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now we’re ready to run our Spark machine learning script! You can run your PySpark
    script just like you did on your local machine. Remember that your script lives
    in an S3 bucket, and you’ll need to point PySpark to the file there.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好运行我们的Spark机器学习脚本了！你可以像在本地机器上一样运行你的PySpark脚本。记住，你的脚本位于S3存储桶中，你需要将PySpark指向那里的文件。
- en: '[PRE21]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When you run your script, you’ll see the standard output from Spark. It will
    tell you what it’s doing and give you progress on the task. Depending on how many
    instances you allocated to this cluster, this job may take a little or some time.
    When it’s finished, you’ll be able to go into the S3 bucket you targeted for your
    output and see a folder named nb-model. This folder contains the compressed description
    of the naïve Bayes model you trained.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行脚本时，你会看到Spark的标准输出。它会告诉你它在做什么，并给出任务的进度。根据你分配给这个集群的实例数量，这个作业可能需要一点时间或更长的时间。完成时，你将能够进入你为输出指定的S3存储桶，并看到一个名为nb-model的文件夹。这个文件夹包含你训练的朴素贝叶斯模型的压缩描述。
- en: 12.3\. Exercises
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3\. 练习
- en: 12.3.1\. R-series cluster
  id: totrans-481
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.1\. R系列集群
- en: Write an mrjob config file that you could use to start a cluster of five R-series
    instances.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个mrjob配置文件，你可以用它来启动五个R系列实例的集群。
- en: 12.3.2\. Back-to-back Hadoop jobs
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.2\. 连续运行Hadoop作业
- en: Configure an EMR cluster to be persistent and then execute two Hadoop MapReduce
    jobs on that cluster. For example, select only a few fields from JSON-line data
    with the first job, then transform that data into CSV format with another job.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 配置一个持久的 EMR 集群，然后在那个集群上执行两个 Hadoop MapReduce 任务。例如，第一个任务中仅选择 JSON 行数据的几个字段，然后使用另一个任务将那些数据转换成
    CSV 格式。
- en: 12.3.3\. Instance types
  id: totrans-485
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.3. 实例类型
- en: 'We use three major instance types in cluster compute workflows: M, C, and R.
    Which of these types is good for each of the following?'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在集群计算工作流中使用三种主要的实例类型：M、C 和 R。以下哪种类型适合以下每个情况？
- en: Streaming workflows
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式工作流
- en: Hadoop workflows
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 工作流
- en: Test workflows
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试工作流
- en: Spark workflows
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 工作流
- en: Summary
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Elastic MapReduce, known by the acronym EMR, is an AWS managed service we can
    use to quickly and conveniently obtain cluster computing capability.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性映射减少（Elastic MapReduce），简称 EMR，是我们可以使用的一种 AWS 管理服务，可以快速方便地获得集群计算能力。
- en: We can run Hadoop jobs on EMR with the mrjob library, which allows us to write
    distributed MapReduce and procure cluster computing in Python.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用 mrjob 库在 EMR 上运行 Hadoop 任务，这允许我们用 Python 编写分布式 MapReduce 并进行集群计算。
- en: We can use mrjob’s configuration files to describe what we want our clusters
    to look like, including which instances we’d like to use, where we’d like those
    instances to be located, and any tags we may want to add.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用 mrjob 的配置文件来描述我们希望我们的集群看起来像什么，包括我们希望使用哪些实例，我们希望这些实例位于何处，以及我们可能希望添加的任何标签。
- en: When running Hadoop on EMR, we can operate directly on data in S3, which facilitates
    petabyte-scale analytics and extract-transform-load operations.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在 EMR 上运行 Hadoop 时，我们可以直接在 S3 中的数据上操作，这促进了PB级分析以及提取-转换-加载操作。
- en: When we need to run advanced analytics and machine learning on large datasets,
    AWS EMR also supports Spark.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们需要在大数据集上运行高级分析和机器学习时，AWS EMR 也支持 Spark。
- en: Running Spark jobs on EMR requires more powerful instances than Hadoop jobs,
    which can increase cost. But for some workflows, Spark jobs will be faster than
    Hadoop jobs.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 EMR 上运行 Spark 任务需要比 Hadoop 任务更强大的实例，这可能会增加成本。但对于某些工作流，Spark 任务的运行速度将比 Hadoop
    任务快。
