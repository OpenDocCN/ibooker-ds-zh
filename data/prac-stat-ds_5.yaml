- en: Chapter 5\. Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章\. 分类
- en: 'Data scientists are often tasked with automating decisions for business problems.
    Is an email an attempt at phishing? Is a customer likely to churn? Is the web
    user likely to click on an advertisement? These are all *classification* problems,
    a form of *supervised learning* in which we first train a model on data where
    the outcome is known and then apply the model to data where the outcome is not
    known. Classification is perhaps the most important form of prediction: the goal
    is to predict whether a record is a 1 or a 0 (phishing/not-phishing, click/don’t
    click, churn/don’t churn), or in some cases, one of several categories (for example,
    Gmail’s filtering of your inbox into “primary,” “social,” “promotional,” or “forums”).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家经常被要求自动化解决业务问题的决策。一封电子邮件是否是钓鱼尝试？客户是否可能流失？网页用户是否可能点击广告？这些都是*分类*问题，一种*监督学习*形式，我们首先在已知结果的数据上训练模型，然后将模型应用于结果未知的数据。分类可能是最重要的预测形式：目标是预测记录是1还是0（钓鱼/非钓鱼，点击/不点击，流失/不流失），或者在某些情况下是多个类别之一（例如，Gmail将您的收件箱分类为“主要”，“社交”，“促销”或“论坛”）。
- en: 'Often, we need more than a simple binary classification: we want to know the
    predicted probability that a case belongs to a class. Rather than having a model
    simply assign a binary classification, most algorithms can return a probability
    score (propensity) of belonging to the class of interest. In fact, with logistic
    regression, the default output from *R* is on the log-odds scale, and this must
    be transformed to a propensity. In *Python*’s `scikit-learn`, logistic regression,
    like most classification methods, provides two prediction methods: `predict` (which
    returns the class) and `predict_proba` (which returns probabilities for each class).
    A sliding cutoff can then be used to convert the propensity score to a decision.
    The general approach is as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要的不仅仅是简单的二元分类：我们想知道案例属于某个类别的预测概率。与其让模型简单地分配一个二元分类不同，大多数算法可以返回属于感兴趣类别的概率分数（倾向性）。实际上，在逻辑回归中，默认输出是在对数几率（log-odds）尺度上，必须将其转换为倾向性。在*Python*的`scikit-learn`中，逻辑回归与大多数分类方法一样，提供两种预测方法：`predict`（返回类别）和`predict_proba`（返回每个类别的概率）。然后可以使用滑动截止点将倾向性分数转换为决策。一般的方法如下所示：
- en: Establish a cutoff probability for the class of interest, above which we consider
    a record as belonging to that class.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为感兴趣的类别建立一个截止概率，高于这个概率则认为记录属于该类别。
- en: Estimate (with any model) the probability that a record belongs to the class
    of interest.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计（使用任何模型）记录属于感兴趣类别的概率。
- en: If that probability is above the cutoff probability, assign the new record to
    the class of interest.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果该概率高于截止概率，则将新记录分配给感兴趣的类别。
- en: The higher the cutoff, the fewer the records predicted as 1—that is, as belonging
    to the class of interest. The lower the cutoff, the more the records predicted
    as 1.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 截止点越高，预测为1的记录越少，即认为属于感兴趣类别的记录越少。截止点越低，预测为1的记录越多。
- en: This chapter covers several key techniques for classification and estimating
    propensities; additional methods that can be used both for classification and
    for numerical prediction are described in the next chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了几种关键的分类技术和估计倾向性的方法；下一章描述了可以用于分类和数值预测的其他方法。
- en: Naive Bayes
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: 'The naive Bayes algorithm uses the probability of observing predictor values,
    given an outcome, to estimate what is really of interest: the probability of observing
    outcome *Y = i*, given a set of predictor values.^([1](ch05.xhtml#idm46522850239256))'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法使用观察到的预测器值的概率，给定一个结果，来估计真正感兴趣的内容：观察到结果*Y = i*的概率，给定一组预测器值。^([1](ch05.xhtml#idm46522850239256))
- en: 'To understand naive Bayesian classification, we can start out by imagining
    complete or exact Bayesian classification. For each record to be classified:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解朴素贝叶斯分类，我们可以从想象完全或精确的贝叶斯分类开始。对于要分类的每条记录：
- en: Find all the other records with the same predictor profile (i.e., where the
    predictor values are the same).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到具有相同预测配置文件的所有其他记录（即预测值相同的地方）。
- en: Determine what classes those records belong to and which class is most prevalent
    (i.e., probable).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定这些记录属于哪些类别，并且哪个类别最普遍（即最可能）。
- en: Assign that class to the new record.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配该类别给新记录。
- en: The preceding approach amounts to finding all the records in the sample that
    are exactly like the new record to be classified in the sense that all the predictor
    values are identical.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法相当于找到样本中与要分类的新记录完全相同的所有记录，即所有预测变量值都相同。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Predictor variables must be categorical (factor) variables in the standard naive
    Bayes algorithm. See [“Numeric Predictor Variables”](#NumericPredictors) for two
    workarounds for using continuous variables.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 标签预测变量必须是标准朴素贝叶斯算法中的分类（因子）变量。有关使用连续变量的两种解决方案，请参阅[“数值预测变量”](#NumericPredictors)。
- en: Why Exact Bayesian Classification Is Impractical
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么精确贝叶斯分类不切实际
- en: When the number of predictor variables exceeds a handful, many of the records
    to be classified will be without exact matches. Consider a model to predict voting
    on the basis of demographic variables. Even a sizable sample may not contain even
    a single match for a new record who is a male Hispanic with high income from the
    US Midwest who voted in the last election, did not vote in the prior election,
    has three daughters and one son, and is divorced. And this is with just eight
    variables, a small number for most classification problems. The addition of just
    a single new variable with five equally frequent categories reduces the probability
    of a match by a factor of 5.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测变量的数量超过几个时，将没有准确匹配的记录将要分类。考虑一个根据人口统计变量预测投票的模型。即使是一个相当大的样本，也可能不包含一个新记录的准确匹配，该记录是美国中西部高收入的男性拉丁裔，在上次选举中投票，在上次选举中没有投票，有三个女儿和一个儿子，离过婚。而且这只是八个变量，对于大多数分类问题来说是一个小数量。只需添加一个具有五个同等频率类别的新变量，就会将匹配概率降低
    5 倍。
- en: The Naive Solution
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素解决方案
- en: 'In the naive Bayes solution, we no longer restrict the probability calculation
    to those records that match the record to be classified. Instead, we use the entire
    data set. The naive Bayes modification is as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素贝叶斯解决方案中，我们不再将概率计算限制为与要分类的记录匹配的那些记录。相反，我们使用整个数据集。朴素贝叶斯修改如下：
- en: For a binary response *Y = i* (*i* = 0 or 1), estimate the individual conditional
    probabilities for each predictor <math alttext="upper P left-parenthesis upper
    X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> ; these are the probabilities that the predictor
    value is in the record when we observe *Y = i*. This probability is estimated
    by the proportion of *X[j]* values among the *Y = i* records in the training set.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于二元响应 *Y = i*（*i* = 0 或 1），估算每个预测变量 <math alttext="upper P left-parenthesis
    upper X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> 的条件概率；这些是观察 *Y = i* 时预测变量值在记录中的概率。该概率通过训练集中
    *Y = i* 记录中 *X[j]* 值的比例来估算。
- en: Multiply these probabilities by each other, and then by the proportion of records
    belonging to *Y = i*.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些概率相乘，然后乘以属于 *Y = i* 的记录的比例。
- en: Repeat steps 1 and 2 for all the classes.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为所有类别重复步骤 1 和 2。
- en: Estimate a probability for outcome *i* by taking the value calculated in step
    2 for class *i* and dividing it by the sum of such values for all classes.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将步骤 2 中为第 *i* 类计算的值除以所有类别的这种值的总和来估算结果 *i* 的概率。
- en: Assign the record to the class with the highest probability for this set of
    predictor values.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将记录分配给具有此预测变量集的最高概率的类别。
- en: 'This naive Bayes algorithm can also be stated as an equation for the probability
    of observing outcome *Y = i*, given a set of predictor values <math alttext="upper
    X 1 comma ellipsis comma upper X Subscript p Baseline"><mrow><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math>
    :'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该朴素贝叶斯算法也可以陈述为根据一组预测变量 <math alttext="upper X 1 comma ellipsis comma upper X
    Subscript p Baseline"><mrow><msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math> 观察结果 *Y = i* 的概率的方程：
- en: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo></math>
- en: 'Here is the full formula for calculating class probabilities using exact Bayes
    classification:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用精确贝叶斯分类计算类别概率的完整公式：
- en: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub>
    <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo stretchy="false">)</mo></mrow> <mrow><mi>P</mi> <mo stretchy="false">(</mo>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>X</mi>
    <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mo>+</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo>
    <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>…</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></mfrac></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub>
    <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo stretchy="false">)</mo></mrow> <mrow><mi>P</mi> <mo stretchy="false">(</mo>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>X</mi>
    <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mo>+</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo>
    <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>…</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></mfrac></math>
- en: 'Under the naive Bayes assumption of conditional independence, this equation
    changes into:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据条件独立的朴素贝叶斯假设，该方程变为：
- en: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo stretchy="false">)</mo></mrow>
    <mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>0</mn>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo>
    <mo>+</mo> <mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></mfrac></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo stretchy="false">)</mo></mrow>
    <mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>0</mn>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo>
    <mo>+</mo> <mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></mfrac></math>
- en: Why is this formula called “naive”? We have made a simplifying assumption that
    the *exact conditional probability* of a vector of predictor values, given observing
    an outcome, is sufficiently well estimated by the product of the individual conditional
    probabilities <math alttext="upper P left-parenthesis upper X Subscript j Baseline
    vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math>
    . In other words, in estimating <math alttext="upper P left-parenthesis upper
    X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> instead of <math alttext="upper P left-parenthesis
    upper X 1 comma upper X 2 comma ellipsis upper X Subscript p Baseline vertical-bar
    upper Y equals i right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <msub><mi>X</mi> <mi>p</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo>)</mo></mrow></math> , we are assuming <math alttext="upper X Subscript j"><msub><mi>X</mi>
    <mi>j</mi></msub></math> is *independent* of all the other predictor variables
    <math alttext="upper X Subscript k"><msub><mi>X</mi> <mi>k</mi></msub></math>
    for <math alttext="k not-equals j"><mrow><mi>k</mi> <mo>≠</mo> <mi>j</mi></mrow></math>
    .
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个公式被称为“朴素”？我们做出了一个简化的假设，即给定观察结果的预测变量向量的*精确条件概率*可以通过各个条件概率的乘积 <math alttext="upper
    P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub>
    <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math> 很好地估计。换句话说，在估计
    <math alttext="upper P left-parenthesis upper X Subscript j Baseline vertical-bar
    upper Y equals i right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math>
    而不是 <math alttext="upper P left-parenthesis upper X 1 comma upper X 2 comma ellipsis
    upper X Subscript p Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo>|</mo> <mi>Y</mi>
    <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math> 的情况下，我们假设 <math alttext="upper
    X Subscript j"><msub><mi>X</mi> <mi>j</mi></msub></math> 对于所有其他预测变量 <math alttext="upper
    X Subscript k"><msub><mi>X</mi> <mi>k</mi></msub></math> （对于 <math alttext="k
    not-equals j"><mrow><mi>k</mi> <mo>≠</mo> <mi>j</mi></mrow></math> ）是*独立*的。
- en: 'Several packages in *R* can be used to estimate a naive Bayes model. The following
    fits a model to the loan payment data using the `klaR` package:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*中有几个包可以用来估计朴素贝叶斯模型。以下是使用`klaR`包对贷款支付数据拟合模型的示例：'
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output from the model is the conditional probabilities <math alttext="upper
    P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub>
    <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math> .
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出是条件概率 <math alttext="upper P left-parenthesis upper X Subscript j Baseline
    vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math>
    。
- en: 'In *Python* we can use `sklearn.naive_bayes.MultinomialNB` from `scikit-learn`.
    We need to convert the categorical features to dummy variables before we fit the
    model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以使用`scikit-learn`中的`sklearn.naive_bayes.MultinomialNB`。在拟合模型之前，我们需要将分类特征转换为虚拟变量。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is possible to derive the conditional probabilities from the fitted model
    using the property `feature_log_prob_`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`feature_log_prob_`属性从拟合模型中导出条件概率。
- en: 'The model can be used to predict the outcome of a new loan. We use the last
    value of the data set for testing:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可用于预测新贷款的结果。我们使用数据集的最后一个值进行测试：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In *Python*, we get this value as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以这样获取该值：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this case, the model predicts a default (*R*):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型预测一个默认值（*R*）：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we discussed, `scikit-learn`’s classification models have two methods—`predict`,
    which returns the predicted class, and `predict_proba`, which returns the class
    probabilities:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，`scikit-learn`的分类模型有两种方法——`predict`返回预测的类别，`predict_proba`返回类别概率：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The prediction also returns a `posterior` estimate of the probability of default.
    The naive Bayesian classifier is known to produce *biased* estimates. However,
    where the goal is to *rank* records according to the probability that *Y* = 1,
    unbiased estimates of probability are not needed, and naive Bayes produces good
    results.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 预测还返回一个`posterior`概率的估计。朴素贝叶斯分类器因产生*偏倚*估计而闻名。然而，如果目标是根据*Y* = 1的概率对记录进行*排名*，则不需要无偏估计，朴素贝叶斯能够产生良好的结果。
- en: Numeric Predictor Variables
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值预测变量
- en: 'The Bayesian classifier works only with categorical predictors (e.g., with
    spam classification, where the presence or absence of words, phrases, characters,
    and so on lies at the heart of the predictive task). To apply naive Bayes to numerical
    predictors, one of two approaches must be taken:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类器仅适用于分类预测变量（例如垃圾邮件分类，其中词语、短语、字符等的存在与否是预测任务的核心）。要将朴素贝叶斯应用于数值预测变量，必须采用以下两种方法之一：
- en: Bin and convert the numerical predictors to categorical predictors and apply
    the algorithm of the previous section.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数值预测变量分箱并转换为分类预测变量，然后应用前一节的算法。
- en: Use a probability model—for example, the normal distribution (see [“Normal Distribution”](ch02.xhtml#NormalDist))—to
    estimate the conditional probability <math alttext="upper P left-parenthesis upper
    X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> .
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用概率模型（例如正态分布，参见[“正态分布”](ch02.xhtml#NormalDist)）来估计条件概率<math alttext="upper
    P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub>
    <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math> 。
- en: Caution
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When a predictor category is absent in the training data, the algorithm assigns
    *zero probability* to the outcome variable in new data, rather than simply ignoring
    this variable and using the information from other variables, as other methods
    might. Most implementations of Naive Bayes use a smoothing parameter (Laplace
    Smoothing) to prevent this.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据中缺少预测变量类别时，该算法会将新数据中的结果变量概率分配为*零概率*，而不是简单地忽略该变量并使用其他变量的信息，如其他方法可能会做的那样。大多数朴素贝叶斯的实现都使用平滑参数（拉普拉斯平滑）来防止这种情况发生。
- en: Further Reading
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*The Elements of Statistical Learning*, 2nd ed., by Trevor Hastie, Robert Tibshirani,
    and Jerome Friedman (Springer, 2009).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计学习要素*，第2版，作者特雷弗·哈斯蒂、罗伯特·蒂布什拉尼和杰罗姆·弗里德曼（Springer，2009）。'
- en: There is a full chapter on naive Bayes in *Data Mining for Business Analytics*
    by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth
    Lichtendahl (Wiley, 2007–2020, with editions for *R*, *Python*, Excel, and JMP).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在《商业分析数据挖掘》（Wiley，2007–2020，包括*R*、*Python*、Excel和JMP版本）一书中有一整章介绍朴素贝叶斯。该书的作者是加利特·舒梅利、彼得·布鲁斯、尼汀·帕特尔、彼得·格德克、因巴尔·雅哈夫和肯尼斯·利克滕达尔。
- en: Discriminant Analysis
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别分析
- en: '*Discriminant analysis* is the earliest statistical classifier; it was introduced
    by R. A. Fisher in 1936 in an article published in the *Annals of Eugenics* journal.^([2](ch05.xhtml#idm46522849542200))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*判别分析*是最早的统计分类器；它由R·A·费舍尔于1936年在《遗传学年刊》期刊上发表的一篇文章中介绍。^([2](ch05.xhtml#idm46522849542200))'
- en: While discriminant analysis encompasses several techniques, the most commonly
    used is *linear discriminant analysis*, or *LDA*. The original method proposed
    by Fisher was actually slightly different from LDA, but the mechanics are essentially
    the same. LDA is now less widely used with the advent of more sophisticated techniques,
    such as tree models and logistic regression.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然判别分析包括几种技术，但最常用的是*线性判别分析*，即*LDA*。费舍尔最初提出的方法实际上与LDA略有不同，但其原理基本相同。随着更复杂技术的出现，如树模型和逻辑回归，LDA的应用变得不那么广泛了。
- en: However, you may still encounter LDA in some applications, and it has links
    to other more widely used methods (such as principal components analysis; see
    [“Principal Components Analysis”](ch07.xhtml#PCA)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但在某些应用中仍可能会遇到LDA，并且它与其他更广泛使用的方法存在联系（例如主成分分析；参见[“主成分分析”](ch07.xhtml#PCA)）。
- en: Warning
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Linear discriminant analysis should not be confused with Latent Dirichlet Allocation,
    also referred to as LDA. Latent Dirichlet Allocation is used in text and natural
    language processing and is unrelated to linear discriminant analysis.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 线性判别分析不应与潜在狄利克雷分配（也称为LDA）混淆。潜在狄利克雷分配用于文本和自然语言处理，与线性判别分析无关。
- en: Covariance Matrix
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协方差矩阵
- en: 'To understand discriminant analysis, it is first necessary to introduce the
    concept of *covariance* between two or more variables. The covariance measures
    the relationship between two variables <math alttext="x"><mi>x</mi></math> and
    <math alttext="z"><mi>z</mi></math> . Denote the mean for each variable by <math
    alttext="x overbar"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>
    and <math alttext="z overbar"><mover accent="true"><mi>z</mi> <mo>¯</mo></mover></math>
    (see [“Mean”](ch01.xhtml#Mean)). The covariance <math alttext="s Subscript x comma
    z"><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub></math>
    between <math alttext="x"><mi>x</mi></math> and <math alttext="z"><mi>z</mi></math>
    is given by:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解判别分析，首先需要介绍两个或多个变量之间的 *协方差* 的概念。协方差衡量了变量 <math alttext="x"><mi>x</mi></math>
    和 <math alttext="z"><mi>z</mi></math> 之间的关系。用 <math alttext="x overbar"><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover></math> 和 <math alttext="z overbar"><mover
    accent="true"><mi>z</mi> <mo>¯</mo></mover></math> 表示每个变量的均值（参见 [“均值”](ch01.xhtml#Mean)）。变量
    <math alttext="x"><mi>x</mi></math> 和 <math alttext="z"><mi>z</mi></math> 的协方差
    <math alttext="s Subscript x comma z"><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub></math>
    定义如下：
- en: <math display="block"><mrow><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>z</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>z</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math>
- en: where *n* is the number of records (note that we divide by *n* – 1 instead of
    *n*; see [“Degrees of Freedom, and *n* or *n* – 1?”](ch01.xhtml#Nminus1)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 为记录数（注意我们除以 *n* – 1 而不是 *n*；参见 [“自由度，*n* 还是 *n* – 1？”](ch01.xhtml#Nminus1)）。
- en: 'As with the correlation coefficient (see [“Correlation”](ch01.xhtml#Correlations)),
    positive values indicate a positive relationship and negative values indicate
    a negative relationship. Correlation, however, is constrained to be between –1
    and 1, whereas covariance scale depends on the scale of the variables <math alttext="x"><mi>x</mi></math>
    and <math alttext="z"><mi>z</mi></math> . The *covariance matrix* <math alttext="normal
    upper Sigma"><mi>Σ</mi></math> for <math alttext="x"><mi>x</mi></math> and <math
    alttext="z"><mi>z</mi></math> consists of the individual variable variances, <math
    alttext="s Subscript x Superscript 2"><msubsup><mi>s</mi> <mi>x</mi> <mn>2</mn></msubsup></math>
    and <math alttext="s Subscript z Superscript 2"><msubsup><mi>s</mi> <mi>z</mi>
    <mn>2</mn></msubsup></math> , on the diagonal (where row and column are the same
    variable) and the covariances between variable pairs on the off-diagonals:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与相关系数类似（参见 [“相关性”](ch01.xhtml#Correlations)），正值表示正相关，负值表示负相关。但是，相关性的取值范围限制在
    –1 到 1 之间，而协方差的尺度依赖于变量 <math alttext="x"><mi>x</mi></math> 和 <math alttext="z"><mi>z</mi></math>
    的尺度。*协方差矩阵* <math alttext="normal upper Sigma"><mi>Σ</mi></math> 对于变量 <math alttext="x"><mi>x</mi></math>
    和 <math alttext="z"><mi>z</mi></math> 包括对角线上的各自变量方差，<math alttext="s Subscript
    x Superscript 2"><msubsup><mi>s</mi> <mi>x</mi> <mn>2</mn></msubsup></math> 和
    <math alttext="s Subscript z Superscript 2"><msubsup><mi>s</mi> <mi>z</mi> <mn>2</mn></msubsup></math>
    ，以及非对角线上的变量对之间的协方差：
- en: <math display="block"><mrow><mover accent="true"><mi>Σ</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msubsup><mi>s</mi> <mi>x</mi>
    <mn>2</mn></msubsup></mtd> <mtd><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>s</mi> <mrow><mi>z</mi><mo>,</mo><mi>x</mi></mrow></msub></mtd>
    <mtd><msubsup><mi>s</mi> <mi>z</mi> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>Σ</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msubsup><mi>s</mi> <mi>x</mi>
    <mn>2</mn></msubsup></mtd> <mtd><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>s</mi> <mrow><mi>z</mi><mo>,</mo><mi>x</mi></mrow></msub></mtd>
    <mtd><msubsup><mi>s</mi> <mi>z</mi> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
- en: Note
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Recall that the standard deviation is used to normalize a variable to a *z*-score;
    the covariance matrix is used in a multivariate extension of this standardization
    process. This is known as Mahalanobis distance (see [“Other Distance Metrics”](ch06.xhtml#Mahalanobis))
    and is related to the LDA function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，标准偏差用于将变量标准化为 *z*-分数；协方差矩阵用于这一标准化过程的多变量扩展。这称为马哈拉诺比斯距离（参见 [“其他距离度量”](ch06.xhtml#Mahalanobis)），与
    LDA 函数相关。
- en: Fisher’s Linear Discriminant
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 费歇尔线性判别
- en: 'For simplicity, let’s focus on a classification problem in which we want to
    predict a binary outcome *y* using just two continuous numeric variables <math
    alttext="left-parenthesis x comma z right-parenthesis"><mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></math> . Technically, discriminant analysis
    assumes the predictor variables are normally distributed continuous variables,
    but, in practice, the method works well even for nonextreme departures from normality,
    and for binary predictors. Fisher’s linear discriminant distinguishes variation
    *between* groups, on the one hand, from variation *within* groups on the other.
    Specifically, seeking to divide the records into two groups, linear discriminant
    analysis (LDA) focuses on maximizing the “between” sum of squares <math alttext="normal
    upper S normal upper S Subscript normal b normal e normal t normal w normal e
    normal e normal n"><msub><mi>SS</mi> <mi>between</mi></msub></math> (measuring
    the variation between the two groups) relative to the “within” sum of squares
    <math alttext="normal upper S normal upper S Subscript normal w normal i normal
    t normal h normal i normal n"><msub><mi>SS</mi> <mi>within</mi></msub></math>
    (measuring the within-group variation). In this case, the two groups correspond
    to the records <math alttext="left-parenthesis x 0 comma z 0 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>z</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></math> for which *y* = 0 and the records <math alttext="left-parenthesis
    x 1 comma z 1 right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>z</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math> for which
    *y* = 1. The method finds the linear combination <math alttext="w Subscript x
    Baseline x plus w Subscript z Baseline z"><mrow><msub><mi>w</mi> <mi>x</mi></msub>
    <mi>x</mi> <mo>+</mo> <msub><mi>w</mi> <mi>z</mi></msub> <mi>z</mi></mrow></math>
    that maximizes that sum of squares ratio:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 简单起见，让我们专注于一个分类问题，我们希望仅使用两个连续数值变量<math alttext="left-parenthesis x comma z right-parenthesis"><mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></math>来预测二进制结果*y*。技术上，判别分析假设预测变量是正态分布的连续变量，但实际上，即使与正态分布的极端偏差，该方法也表现良好，并且适用于二元预测变量。费舍尔线性判别法区分了组间变异与组内变异。具体而言，试图将记录分为两组，线性判别分析（LDA）专注于最大化“组间”平方和<math
    alttext="normal upper S normal upper S Subscript normal b normal e normal t normal
    w normal e normal e normal n"><msub><mi>SS</mi> <mi>between</mi></msub></math>（衡量两组之间的变异）相对于“组内”平方和<math
    alttext="normal upper S normal upper S Subscript normal w normal i normal t normal
    h normal i normal n"><msub><mi>SS</mi> <mi>within</mi></msub></math>（衡量组内变异）。在这种情况下，两组对应于*y*
    = 0的记录<math alttext="left-parenthesis x 0 comma z 0 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>z</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></math>和*y* = 1的记录<math alttext="left-parenthesis x 1 comma z
    1 right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>z</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math>。该方法找到了最大化这两个平方和比例的线性组合<math
    alttext="w Subscript x Baseline x plus w Subscript z Baseline z"><mrow><msub><mi>w</mi>
    <mi>x</mi></msub> <mi>x</mi> <mo>+</mo> <msub><mi>w</mi> <mi>z</mi></msub> <mi>z</mi></mrow></math>：
- en: <math display="block"><mfrac><msub><mi>SS</mi> <mi>between</mi></msub> <msub><mi>SS</mi>
    <mi>within</mi></msub></mfrac></math>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfrac><msub><mi>SS</mi> <mi>between</mi></msub> <msub><mi>SS</mi>
    <mi>within</mi></msub></mfrac></math>
- en: The between sum of squares is the squared distance between the two group means,
    and the within sum of squares is the spread around the means within each group,
    weighted by the covariance matrix. Intuitively, by maximizing the between sum
    of squares and minimizing the within sum of squares, this method yields the greatest
    separation between the two groups.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 组间平方和是两组均值之间的平方距离，组内平方和是每组内均值周围的加权协方差矩阵的扩展。直觉上，通过最大化组间平方和和最小化组内平方和，该方法实现了两组之间的最大分离。
- en: A Simple Example
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单示例
- en: 'The `MASS` package, associated with the book *Modern Applied Statistics with
    S* by W. N. Venables and B. D. Ripley (Springer, 1994), provides a function for
    LDA with *R*. The following applies this function to a sample of loan data using
    two predictor variables, `borrower_score` and `payment_inc_ratio`, and prints
    out the estimated linear discriminator weights:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`MASS`包与W. N. Venables和B. D. Ripley（Springer，1994）合著的书籍*Modern Applied Statistics
    with S*相关联，为*R*提供了LDA的函数。以下是将此函数应用于一组贷款数据样本，使用两个预测变量`borrower_score`和`payment_inc_ratio`，并输出估计的线性判别器权重：'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In *Python*, we can use `LinearDiscriminantAnalysis` from `sklearn.discriminant_analysis`.
    The `scalings_` property gives the estimated weights:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以使用`LinearDiscriminantAnalysis`来自`sklearn.discriminant_analysis`。`scalings_`属性给出了估计的权重：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using Discriminant Analysis for Feature Selection
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用判别分析进行特征选择
- en: If the predictor variables are normalized prior to running LDA, the discriminator
    weights are measures of variable importance, thus providing a computationally
    efficient method of feature selection.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行LDA之前对预测变量进行了标准化，那么判别权重就是变量重要性的度量，从而提供了一种计算效率高的特征选择方法。
- en: 'The `lda` function can predict the probability of “default” versus “paid off”:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`lda`函数可以预测“违约”与“已结清”的概率：'
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `predict_proba` method of the fitted model returns the probabilities for
    the “default” and “paid off” outcomes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合模型的`predict_proba`方法返回“违约”和“已结清”结果的概率：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A plot of the predictions helps illustrate how LDA works. Using the output
    from the `predict` function, a plot of the estimated probability of default is
    produced as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 预测图有助于说明线性判别分析（LDA）的工作原理。使用`predict`函数的输出，可以生成关于违约概率的估计的图，如下所示：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A similar graph is created in Python using this code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中使用以下代码创建类似的图：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The resulting plot is shown in [Figure 5-1](#LoanLDA). Data points on the left
    of the diagonal line are predicted to default (probability greater than 0.5).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图如[图5-1](#LoanLDA)所示。在对角线线左侧的数据点被预测为违约（概率大于0.5）。
- en: '![LDA prediction of loan default using two variables: a score of the borrower''s
    creditworthiness and the payment-to-income ratio. Data points on the left of the
    diagonal line are predicted to default (probability greater than 0.5).](Images/psd2_0501.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![使用两个变量（借款人信用评分和还款占收入比例）的LDA预测贷款违约情况。对角线线左侧的数据点被预测为违约（概率大于0.5）。](Images/psd2_0501.png)'
- en: 'Figure 5-1\. LDA prediction of loan default using two variables: a score of
    the borrower’s creditworthiness and the payment-to-income ratio'
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1. 使用两个变量（借款人信用评分和还款占收入比例）的LDA预测贷款违约情况
- en: Using the discriminant function weights, LDA splits the predictor space into
    two regions, as shown by the solid line. The predictions farther away from the
    line in both directions have a higher level of confidence (i.e., a probability
    further away from 0.5).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用判别函数权重，LDA将预测空间分成两个区域，如实线所示。在两个方向上距离该线更远的预测具有更高的置信度（即，概率远离0.5）。
- en: Extensions of Discriminant Analysis
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别分析的扩展
- en: 'More predictor variables: while the text and example in this section used just
    two predictor variables, LDA works just as well with more than two predictor variables.
    The only limiting factor is the number of records (estimating the covariance matrix
    requires a sufficient number of records per variable, which is typically not an
    issue in data science applications).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更多预测变量：虽然本节的文本和示例仅使用了两个预测变量，但是LDA同样适用于超过两个预测变量。唯一的限制因素是记录的数量（估计协方差矩阵需要每个变量足够数量的记录，在数据科学应用中通常不是问题）。
- en: There are other variants of discriminant analysis. The best known is quadratic
    discriminant analysis (QDA). Despite its name, QDA is still a linear discriminant
    function. The main difference is that in LDA, the covariance matrix is assumed
    to be the same for the two groups corresponding to *Y* = 0 and *Y* = 1. In QDA,
    the covariance matrix is allowed to be different for the two groups. In practice,
    the difference in most applications is not critical.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 判别分析还有其他变体。最知名的是二次判别分析（QDA）。尽管其名称如此，但QDA仍然是一个线性判别函数。主要区别在于，在LDA中，协方差矩阵被假定为对应于*Y*
    = 0和*Y* = 1的两组相同。在QDA中，允许两组的协方差矩阵不同。实际上，在大多数应用中，这种差异并不重要。
- en: Further Reading
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Both *The Elements of Statistical Learning*, 2nd ed., by Trevor Hastie, Robert
    Tibshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, *An
    Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor
    Hastie, and Robert Tibshirani (Springer, 2013), have a section on discriminant
    analysis.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trevor Hastie、Robert Tibshirani 和 Jerome Friedman（Springer，2009）的《统计学习的要素》第二版和
    Gareth James、Daniela Witten、Trevor Hastie 和 Robert Tibshirani（Springer，2013）的《统计学习简介》及其简化版都有一个关于判别分析的章节。
- en: '*Data Mining for Business Analytics* by Galit Shmueli, Peter Bruce, Nitin Patel,
    Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions
    for *R*, *Python*, Excel, and JMP) has a full chapter on discriminant analysis.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galit Shmueli、Peter Bruce、Nitin Patel、Peter Gedeck、Inbal Yahav 和 Kenneth Lichtendahl（Wiley，2007–2020，涵盖*R*、*Python*、Excel和JMP版本）的《商业分析的数据挖掘》有一个关于判别分析的完整章节。
- en: For historical interest, Fisher’s original article on the topic, “The Use of
    Multiple Measurements in Taxonomic Problems,” as published in 1936 in *Annals
    of Eugenics* (now called *Annals of Genetics*), can be found [online](https://oreil.ly/_TCR8).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于历史兴趣，费舍尔关于这个主题的原始文章，“在分类问题中使用多个测量值”，于1936年发表在*优生学年鉴*（现称为*遗传学年鉴*）中，可以在[网上](https://oreil.ly/_TCR8)找到。
- en: Logistic Regression
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is analogous to multiple linear regression (see [Chapter 4](ch04.xhtml#Regression)),
    except the outcome is binary. Various transformations are employed to convert
    the problem to one in which a linear model can be fit. Like discriminant analysis,
    and unlike *K*-Nearest Neighbor and naive Bayes, logistic regression is a structured
    model approach rather than a data-centric approach. Due to its fast computational
    speed and its output of a model that lends itself to rapid scoring of new data,
    it is a popular method.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归类似于多元线性回归（见[第四章](ch04.xhtml#Regression)），只是结果是二元的。 各种转换被用来将问题转换为可以拟合线性模型的问题。
    与 *K* 近邻和朴素贝叶斯不同，类别判别分析与逻辑回归类似，它是一种结构化模型方法，而不是数据中心方法。 由于其快速的计算速度和输出易于快速评分新数据的模型，它是一种流行的方法。
- en: Logistic Response Function and Logit
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑响应函数和对数几率
- en: The key ingredients for logistic regression are the *logistic response function*
    and the *logit*, in which we map a probability (which is on a 0–1 scale) to a
    more expansive scale suitable for linear modeling.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的关键要素是 *逻辑响应函数* 和 *对数几率*，其中我们将一个概率（在0-1范围内）映射到更广泛的范围，适用于线性建模。
- en: 'The first step is to think of the outcome variable not as a binary label but
    as the probability *p* that the label is a “1.” Naively, we might be tempted to
    model *p* as a linear function of the predictor variables:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将结果变量看作不是二元标签，而是标签为“1”的概率 *p*。 幼稚地说，我们可能会诱使将 *p* 建模为预测变量的线性函数：
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>β</mi> <mi>q</mi></msub> <msub><mi>x</mi>
    <mi>q</mi></msub></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>β</mi> <mi>q</mi></msub> <msub><mi>x</mi>
    <mi>q</mi></msub></mrow></math>
- en: However, fitting this model does not ensure that *p* will end up between 0 and
    1, as a probability must.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拟合这个模型并不能确保 *p* 会最终落在 0 和 1 之间，因为概率必须如此。
- en: 'Instead, we model *p* by applying a *logistic response* or *inverse logit*
    function to the predictors:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们通过将 *logistic response* 或 *inverse logit* 函数应用到预测变量上来对 *p* 进行建模：
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi>
    <mi>q</mi></msub> <msub><mi>x</mi> <mi>q</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi>
    <mi>q</mi></msub> <msub><mi>x</mi> <mi>q</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: This transform ensures that the *p* stays between 0 and 1.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换确保 *p* 保持在 0 和 1 之间。
- en: 'To get the exponential expression out of the denominator, we consider *odds*
    instead of probabilities. Odds, familiar to bettors everywhere, are the ratio
    of “successes” (1) to “nonsuccesses” (0). In terms of probabilities, odds are
    the probability of an event divided by the probability that the event will not
    occur. For example, if the probability that a horse will win is 0.5, the probability
    of “won’t win” is (1 – 0.5) = 0.5, and the odds are 1.0:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将指数表达式从分母中提取出来，我们考虑*赔率*而不是概率。 赔率，对于所有赌徒来说都很熟悉，是“成功”（1）与“失败”（0）的比率。 就概率而言，赔率是事件发生的概率除以事件不发生的概率。
    例如，如果一匹马获胜的概率为0.5，则“不会获胜”的概率为（1 - 0.5）= 0.5，而赔率为1.0：
- en: <math display="block"><mrow><mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math>
- en: 'We can obtain the probability from the odds using the inverse odds function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用逆赔率函数从赔率中获得概率：
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mi>Odds</mi> <mrow><mn>1</mn><mo>+</mo>
    <mi>Odds</mi></mrow></mfrac></mrow></math>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mi>Odds</mi> <mrow><mn>1</mn><mo>+</mo>
    <mi>Odds</mi></mrow></mfrac></mrow></math>
- en: 'We combine this with the logistic response function, shown earlier, to get:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个与前面显示的 logistic response function 结合起来，得到：
- en: <math display="block"><mrow><mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>e</mi> <mrow><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi> <mi>q</mi></msub>
    <msub><mi>x</mi> <mi>q</mi></msub></mrow></msup></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>e</mi> <mrow><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi> <mi>q</mi></msub>
    <msub><mi>x</mi> <mi>q</mi></msub></mrow></msup></mrow></math>
- en: 'Finally, taking the logarithm of both sides, we get an expression that involves
    a linear function of the predictors:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，两边取对数，我们得到涉及预测变量的线性函数的表达式：
- en: <math display="block"><mrow><mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>Odds</mi>
    <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>β</mi>
    <mi>q</mi></msub> <msub><mi>x</mi> <mi>q</mi></msub></mrow></math>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>Odds</mi>
    <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>β</mi>
    <mi>q</mi></msub> <msub><mi>x</mi> <mi>q</mi></msub></mrow></math>
- en: The *log-odds* function, also known as the *logit* function, maps the probability
    *p* from <math alttext="left-parenthesis 0 comma 1 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math> to any value <math alttext="left-parenthesis
    negative normal infinity comma plus normal infinity right-parenthesis"><mrow><mo>(</mo>
    <mo>-</mo> <mi>∞</mi> <mo>,</mo> <mo>+</mo> <mi>∞</mi> <mo>)</mo></mrow></math>
    —see [Figure 5-2](#LogitFun). The transformation circle is complete; we have used
    a linear model to predict a probability, which we can in turn map to a class label
    by applying a cutoff rule—any record with a probability greater than the cutoff
    is classified as a 1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*对数几率* 函数，也称为 *logit* 函数，将概率 *p* 从<math alttext="left-parenthesis 0 comma 1
    right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>映射到任何值<math
    alttext="left-parenthesis negative normal infinity comma plus normal infinity
    right-parenthesis"><mrow><mo>(</mo> <mo>-</mo> <mi>∞</mi> <mo>,</mo> <mo>+</mo>
    <mi>∞</mi> <mo>)</mo></mrow></math> —见[图 5-2](#LogitFun)。 转换过程完成；我们使用线性模型来预测概率，然后可以通过应用截断规则将其映射到类标签
    —— 任何概率大于截断的记录都被分类为 1。'
- en: '![Graph of the logit function that maps a probability to a scale suitable for
    a linear model](Images/psd2_0502.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![将概率映射到适合线性模型的比例的logit函数的图形](Images/psd2_0502.png)'
- en: Figure 5-2\. Graph of the logit function that maps a probability to a scale
    suitable for a linear model
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 将概率映射到适合线性模型比例的logit函数的图形
- en: Logistic Regression and the GLM
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归和GLM
- en: The response in the logistic regression formula is the log odds of a binary
    outcome of 1. We observe only the binary outcome, not the log odds, so special
    statistical methods are needed to fit the equation. Logistic regression is a special
    instance of a *generalized linear model* (GLM) developed to extend linear regression
    to other settings.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归公式中的响应是二元结果1的对数几率。我们只观察到二元结果，而不是对数几率，因此需要特殊的统计方法来拟合方程。逻辑回归是广义线性模型（GLM）的一个特例，旨在将线性回归扩展到其他设置中。
- en: 'In *R*, to fit a logistic regression, the `glm` function is used with the `family`
    parameter set to `binomial`. The following code fits a logistic regression to
    the personal loan data introduced in [“K-Nearest Neighbors”](ch06.xhtml#KNN):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在*R*中，要拟合逻辑回归，使用`glm`函数，并将`family`参数设置为`binomial`。以下代码将逻辑回归拟合到[“K-Nearest Neighbors”](ch06.xhtml#KNN)中介绍的个人贷款数据中：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The response is `outcome`, which takes a 0 if the loan is paid off and a 1 if
    the loan defaults. `purpose_` and `home_` are factor variables representing the
    purpose of the loan and the home ownership status. As in linear regression, a
    factor variable with *P* levels is represented with *P* – 1 columns. By default
    in *R*, the *reference* coding is used, and the levels are all compared to the
    reference level (see [“Factor Variables in Regression”](ch04.xhtml#FactorsRegression)).
    The reference levels for these factors are `credit_card` and `MORTGAGE`, respectively.
    The variable `borrower_score` is a score from 0 to 1 representing the creditworthiness
    of the borrower (from poor to excellent). This variable was created from several
    other variables using *K*-Nearest Neighbor—see [“KNN as a Feature Engine”](ch06.xhtml#KnnFeatureEngine).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 响应为`outcome`，如果贷款已还清则为0，如果贷款违约则为1。`purpose_`和`home_`是表示贷款目的和房屋所有权状态的因子变量。与线性回归一样，具有*P*个级别的因子变量用*P*
    – 1列表示。在*R*中，默认使用*reference*编码，并且所有级别都与参考级别进行比较（参见[“Factor Variables in Regression”](ch04.xhtml#FactorsRegression)）。这些因子的参考级别分别是`credit_card`和`MORTGAGE`。变量`borrower_score`是一个从0到1的分数，表示借款人的信用价值（从差到优）。此变量是使用多个其他变量通过*K*-最近邻算法创建的—参见[“KNN
    as a Feature Engine”](ch06.xhtml#KnnFeatureEngine)。
- en: 'In *Python*, we use the `scikit-learn` class `LogisticRegression` from `sklearn.linear_model`.
    The arguments `penalty` and `C` are used to prevent overfitting by L1 or L2 regularization.
    Regularization is switched on by default. In order to fit without regularization,
    we set `C` to a very large value. The `solver` argument selects the used minimizer;
    the method `liblinear` is the default:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们使用`sklearn.linear_model`中的`LogisticRegression`类来执行逻辑回归。参数`penalty`和`C`用于通过L1或L2正则化防止过拟合。默认情况下启用正则化。为了不使用正则化来拟合，我们将`C`设置为一个非常大的值。参数`solver`选择所使用的最小化器；`liblinear`方法是默认方法。
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In contrast to *R*, `scikit-learn` derives the classes from the unique values
    in `y` (*paid off* and *default*). Internally, the classes are ordered alphabetically.
    As this is the reverse order from the factors used in *R*, you will see that the
    coefficients are reversed. The `predict` method returns the class label and `predict_proba`
    returns the probabilities in the order available from the attribute `logit_reg.classes_`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与*R*不同，`scikit-learn`从`y`的唯一值（*paid off*和*default*）派生类。在内部，这些类按字母顺序排序。由于这与*R*中使用的因子顺序相反，您会发现系数是反向的。`predict`方法返回类标签，`predict_proba`按`logit_reg.classes_`属性中可用的顺序返回概率。
- en: Generalized Linear Models
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性模型
- en: 'Generalized linear models (GLMs) are characterized by two main components:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 广义线性模型（GLM）的两个主要组成部分：
- en: A probability distribution or family (binomial in the case of logistic regression)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率分布或家族（在逻辑回归的情况下为二项式）
- en: A link function—i.e., a transformation function that maps the response to the
    predictors (logit in the case of logistic regression)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接函数—即将响应映射到预测变量的转换函数（在逻辑回归的情况下为logit函数）
- en: Logistic regression is by far the most common form of GLM. A data scientist
    will encounter other types of GLMs. Sometimes a log link function is used instead
    of the logit; in practice, use of a log link is unlikely to lead to very different
    results for most applications. The Poisson distribution is commonly used to model
    count data (e.g., the number of times a user visits a web page in a certain amount
    of time). Other families include negative binomial and gamma, often used to model
    elapsed time (e.g., time to failure). In contrast to logistic regression, application
    of GLMs with these models is more nuanced and involves greater care. These are
    best avoided unless you are familiar with and understand the utility and pitfalls
    of these methods.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归远远是广义线性模型中最常见的形式。数据科学家会遇到其他类型的广义线性模型。有时候会使用对数链接函数而不是logit；在实践中，使用对数链接函数不太可能在大多数应用中导致非常不同的结果。泊松分布通常用于建模计数数据（例如用户在某段时间内访问网页的次数）。其他家族包括负二项分布和伽马分布，通常用于建模经过时间（例如失效时间）。与逻辑回归相比，使用这些模型的GLM的应用更加微妙，需要更多的注意。除非您熟悉并理解这些方法的效用和缺陷，最好避免使用这些模型。
- en: Predicted Values from Logistic Regression
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归预测值
- en: 'The predicted value from logistic regression is in terms of the log odds: <math
    alttext="ModifyingAbove upper Y With caret equals log left-parenthesis normal
    upper O normal d normal d normal s left-parenthesis upper Y equals 1 right-parenthesis
    right-parenthesis"><mrow><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mo>=</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi>
    <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> . The
    predicted probability is given by the logistic response function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从逻辑回归中得到的预测值是以对数几率的形式：<math alttext="ModifyingAbove upper Y With caret equals
    log left-parenthesis normal upper O normal d normal d normal s left-parenthesis
    upper Y equals 1 right-parenthesis right-parenthesis"><mrow><mover accent="true"><mi>Y</mi>
    <mo>^</mo></mover> <mo>=</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>Odds</mi>
    <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    。预测概率由逻辑响应函数给出：
- en: <math display="block"><mrow><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover></mrow></msup></mrow></mfrac></mrow></math>
- en: 'For example, look at the predictions from the model `logistic_model` in *R*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看一下*R*中模型`logistic_model`的预测结果：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In *Python*, we can convert the probabilities into a data frame and use the
    `describe` method to get these characteristics of the distribution:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以将概率转换为数据框，并使用`describe`方法获取分布的这些特征：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Converting these values to probabilities is a simple transform:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些值转换为概率是一个简单的变换：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The probabilities are directly available using the `predict_proba` methods
    in `scikit-learn`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接使用`scikit-learn`中的`predict_proba`方法获取概率：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: These are on a scale from 0 to 1 and don’t yet declare whether the predicted
    value is default or paid off. We could declare any value greater than 0.5 as default.
    In practice, a lower cutoff is often appropriate if the goal is to identify members
    of a rare class (see [“The Rare Class Problem”](#RareClassProblem)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据在0到1的范围内，尚未声明预测值是违约还是已偿还。我们可以将大于0.5的任何值声明为违约。在实际操作中，如果目标是识别罕见类别的成员，则通常适合使用较低的截断值（见[“罕见类问题”](#RareClassProblem)）。
- en: Interpreting the Coefficients and Odds Ratios
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释系数和比率比
- en: 'One advantage of logistic regression is that it produces a model that can be
    scored to new data rapidly, without recomputation. Another is the relative ease
    of interpretation of the model, as compared with other classification methods.
    The key conceptual idea is understanding an *odds ratio*. The odds ratio is easiest
    to understand for a binary factor variable *X*:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的一个优点是它能够快速对新数据进行评分，而无需重新计算。另一个优点是与其他分类方法相比，模型的解释相对容易。关键的概念是理解*比率比*。对于二元因子变量*X*，比率比最容易理解：
- en: <math display="block"><mrow><mtext>odds</mtext> <mtext>ratio</mtext> <mo>=</mo>
    <mfrac><mrow><mi>Odds</mi> <mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow>
    <mrow><mi>Odds</mi> <mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>0</mn><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>odds</mtext> <mtext>ratio</mtext> <mo>=</mo>
    <mfrac><mrow><mi>Odds</mi> <mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow>
    <mrow><mi>Odds</mi> <mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>0</mn><mo>)</mo></mrow></mfrac></mrow></math>
- en: This is interpreted as the odds that *Y* = 1 when *X* = 1 versus the odds that
    *Y* = 1 when *X* = 0. If the odds ratio is 2, then the odds that *Y* = 1 are two
    times higher when *X* = 1 versus when *X* = 0.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这被解释为*X* = 1时*Y* = 1的几率与*X* = 0时*Y* = 1的几率之比。如果比率比为2，则*X* = 1时*Y* = 1的几率是*X*
    = 0时的两倍。
- en: Why bother with an odds ratio rather than probabilities? We work with odds because
    the coefficient <math alttext="beta Subscript j"><msub><mi>β</mi> <mi>j</mi></msub></math>
    in the logistic regression is the log of the odds ratio for <math alttext="upper
    X Subscript j"><msub><mi>X</mi> <mi>j</mi></msub></math> .
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要关注比率比而不是概率？我们使用比率是因为逻辑回归中的系数<math alttext="beta Subscript j"><msub><mi>β</mi>
    <mi>j</mi></msub></math> 是<math alttext="upper X Subscript j"><msub><mi>X</mi>
    <mi>j</mi></msub></math> 的比率比的对数。
- en: An example will make this more explicit. For the model fit in [“Logistic Regression
    and the GLM”](#GLM), the regression coefficient for `purpose_small_business` is
    1.21526. This means that a loan to a small business compared to a loan to pay
    off credit card debt reduces the odds of defaulting versus being paid off by <math><mrow><mi>e</mi>
    <mi>x</mi> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>.</mo> <mn>21526</mn> <mo>)</mo>
    <mo>≈</mo> <mn>3</mn> <mo>.</mo> <mn>4</mn></mrow></math> . Clearly, loans for
    the purpose of creating or expanding a small business are considerably riskier
    than other types of loans.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子可以更清楚地说明。对于在[“逻辑回归与广义线性模型”](#GLM)中拟合的模型，`purpose_small_business` 的回归系数为1.21526。这意味着与用来偿还信用卡债务的贷款相比，向小企业贷款减少了违约的几率，相对于被偿还<math><mrow><mi>e</mi>
    <mi>x</mi> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>.</mo> <mn>21526</mn> <mo>)</mo>
    <mo>≈</mo> <mn>3</mn> <mo>.</mo> <mn>4</mn></mrow></math>。显然，用于创建或扩展小企业的贷款远比其他类型的贷款风险要高得多。
- en: '[Figure 5-3](#LogOddsRatio) shows the relationship between the odds ratio and
    the log-odds ratio for odds ratios greater than 1. Because the coefficients are
    on the log scale, an increase of 1 in the coefficient results in an increase of
    <math><mrow><mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>)</mo>
    <mo>≈</mo> <mn>2</mn> <mo>.</mo> <mn>72</mn></mrow></math> in the odds ratio.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-3](#LogOddsRatio) 显示了赔率比大于1时赔率比和对数赔率比之间的关系。由于系数在对数尺度上，系数增加1会导致赔率比增加<math><mrow><mi>e</mi>
    <mi>x</mi> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>)</mo> <mo>≈</mo> <mn>2</mn> <mo>.</mo>
    <mn>72</mn></mrow></math>。'
- en: '![The relationship between the odds ratio and the log-odds ratio](Images/psd2_0503.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![赔率比和对数赔率比之间的关系](Images/psd2_0503.png)'
- en: Figure 5-3\. The relationship between the odds ratio and the log-odds ratio
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3\. 赔率比和对数赔率比之间的关系
- en: 'Odds ratios for numeric variables *X* can be interpreted similarly: they measure
    the change in the odds ratio for a unit change in *X*. For example, the effect
    of increasing the payment-to-income ratio from, say, 5 to 6 increases the odds
    of the loan defaulting by a factor of <math><mrow><mi>e</mi> <mi>x</mi> <mi>p</mi>
    <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>08244</mn> <mo>)</mo> <mo>≈</mo> <mn>1</mn>
    <mo>.</mo> <mn>09</mn></mrow></math> . The variable `borrower_score` is a score
    on the borrowers’ creditworthiness and ranges from 0 (low) to 1 (high). The odds
    of the best borrowers relative to the worst borrowers defaulting on their loans
    is smaller by a factor of <math><mrow><mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo>
    <mo>-</mo> <mn>4</mn> <mo>.</mo> <mn>61264</mn> <mo>)</mo> <mo>≈</mo> <mn>0</mn>
    <mo>.</mo> <mn>01</mn></mrow></math> . In other words, the default risk from the
    borrowers with the poorest creditworthiness is 100 times greater than that of
    the best borrowers!'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 数值变量 *X* 的赔率比可以类似地解释：它们测量 *X* 单位变化对赔率比的影响。例如，将支付收入比率从5增加到6会使贷款违约的几率增加<math><mrow><mi>e</mi>
    <mi>x</mi> <mi>p</mi> <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>08244</mn> <mo>)</mo>
    <mo>≈</mo> <mn>1</mn> <mo>.</mo> <mn>09</mn></mrow></math>。变量 `borrower_score`
    是借款人信用评分，范围从0（低）到1（高）。与最差的借款人相比，最佳借款人违约贷款的几率较小，因为系数<math><mrow><mi>e</mi> <mi>x</mi>
    <mi>p</mi> <mo>(</mo> <mo>-</mo> <mn>4</mn> <mo>.</mo> <mn>61264</mn> <mo>)</mo>
    <mo>≈</mo> <mn>0</mn> <mo>.</mo> <mn>01</mn></mrow></math>。换句话说，最差信用评分借款人的违约风险是最佳借款人的100倍！
- en: 'Linear and Logistic Regression: Similarities and Differences'
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归和逻辑回归：相似与不同之处
- en: 'Linear regression and logistic regression share many commonalities. Both assume
    a parametric linear form relating the predictors with the response. Exploring
    and finding the best model are done in very similar ways. Extensions to the linear
    model, like the use of a spline transform of a predictor (see [“Splines”](ch04.xhtml#Splines)),
    are equally applicable in the logistic regression setting. Logistic regression
    differs in two fundamental ways:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归和逻辑回归有许多共同点。两者都假设一个参数化的线性形式来关联预测变量与响应变量。探索和找到最佳模型的方法非常相似。对线性模型的扩展，比如使用预测变量的样条变换（参见[“样条”](ch04.xhtml#Splines)），同样适用于逻辑回归设置中。逻辑回归在两个基本方面有所不同：
- en: The way the model is fit (least squares is not applicable)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型拟合方式（最小二乘法不适用）
- en: The nature and analysis of the residuals from the model
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型残差的性质和分析
- en: Fitting the model
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Linear regression is fit using least squares, and the quality of the fit is
    evaluated using RMSE and R-squared statistics. In logistic regression (unlike
    in linear regression), there is no closed-form solution, and the model must be
    fit using *maximum likelihood estimation* (MLE). Maximum likelihood estimation
    is a process that tries to find the model that is most likely to have produced
    the data we see. In the logistic regression equation, the response is not 0 or
    1 but rather an estimate of the log odds that the response is 1. The MLE finds
    the solution such that the estimated log odds best describes the observed outcome.
    The mechanics of the algorithm involve a quasi-Newton optimization that iterates
    between a scoring step (*Fisher’s scoring*), based on the current parameters,
    and an update to the parameters to improve the fit.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归使用最小二乘法进行拟合，拟合质量使用RMSE和R-squared统计量进行评估。在逻辑回归中（不同于线性回归），没有封闭形式的解，必须使用*最大似然估计*（MLE）拟合模型。最大似然估计是一个过程，试图找到最有可能产生我们所看到数据的模型。在逻辑回归方程中，响应不是0或1，而是响应为1的对数几率的估计。MLE找到的解决方案使得估计的对数几率最能描述观察到的结果。算法的机制涉及一种拟牛顿优化，该优化在评分步骤（*费舍尔评分*）和根据当前参数更新参数以改进拟合之间迭代。
- en: Fortunately, most practitioners don’t need to concern themselves with the details
    of the fitting algorithm since this is handled by the software. Most data scientists
    will not need to worry about the fitting method, other than understanding that
    it is a way to find a good model under certain assumptions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大多数从业者不需要关注拟合算法的细节，因为这是由软件处理的。大多数数据科学家不需要担心拟合方法，除了理解这是在某些假设下找到一个良好模型的方法。
- en: Handling Factor Variables
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理因子变量
- en: In logistic regression, factor variables should be coded as in linear regression;
    see [“Factor Variables in Regression”](ch04.xhtml#FactorsRegression). In *R* and
    other software, this is normally handled automatically, and generally reference
    encoding is used. All of the other classification methods covered in this chapter
    typically use the one hot encoder representation (see [“One Hot Encoder”](ch06.xhtml#OneHotEncoder)).
    In *Python*’s `scikit-learn`, it is easiest to use one hot encoding, which means
    that only *n – 1* of the resulting dummies can be used in the regression.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，因子变量应该像线性回归一样进行编码；参见[“回归中的因子变量”](ch04.xhtml#FactorsRegression)。在*R*和其他软件中，这通常是自动处理的，通常使用引用编码。本章覆盖的所有其他分类方法通常使用独热编码表示（参见[“独热编码器”](ch06.xhtml#OneHotEncoder)）。在*Python*的`scikit-learn`中，最容易使用独热编码，这意味着在回归中只能使用*n
    – 1*个结果虚拟变量。
- en: Assessing the Model
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Like other classification methods, logistic regression is assessed by how accurately
    the model classifies new data (see [“Evaluating Classification Models”](#EvaluatingModels)).
    As with linear regression, some additional standard statistical tools are available
    to examine and improve the model. Along with the estimated coefficients, *R* reports
    the standard error of the coefficients (SE), a *z*-value, and a p-value:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他分类方法一样，逻辑回归的评估方式是根据模型分类新数据的准确性（见[“评估分类模型”](#EvaluatingModels)）。与线性回归一样，还有一些额外的标准统计工具可用于检查和改进模型。除了估计的系数外，*R*报告系数的标准误（SE）、*z*-值和p值：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The package `statsmodels` has an implementation for generalized linear model
    (`GLM`) that provides similarly detailed information:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 包`statsmodels`提供了广义线性模型（`GLM`）的实现，提供类似详细信息：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Interpretation of the p-value comes with the same caveat as in regression and
    should be viewed more as a relative indicator of variable importance (see [“Assessing
    the Model”](ch04.xhtml#RMSE)) than as a formal measure of statistical significance.
    A logistic regression model, which has a binary response, does not have an associated
    RMSE or R-squared. Instead, a logistic regression model is typically evaluated
    using more general metrics for classification; see [“Evaluating Classification
    Models”](#EvaluatingModels).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: P值的解释与回归中的相同注意事项，应该更多地视为变量重要性的相对指标（见[“评估模型”](ch04.xhtml#RMSE)），而不是统计显著性的正式衡量。逻辑回归模型具有二元响应，没有关联的RMSE或R-squared。相反，逻辑回归模型通常使用更一般的分类指标进行评估；参见[“评估分类模型”](#EvaluatingModels)。
- en: 'Many other concepts for linear regression carry over to the logistic regression
    setting (and other GLMs). For example, you can use stepwise regression, fit interaction
    terms, or include spline terms. The same concerns regarding confounding and correlated
    variables apply to logistic regression (see [“Interpreting the Regression Equation”](ch04.xhtml#InterpretingRegression)).
    You can fit generalized additive models (see [“Generalized Additive Models”](ch04.xhtml#GAMS))
    using the `mgcv` package in *R*:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 许多线性回归的概念也适用于逻辑回归设置（及其他广义线性模型）。例如，您可以使用逐步回归、拟合交互项或包括样条项。与逻辑回归相关的混淆和相关变量的相同问题也适用于此（参见[“解释回归方程”](ch04.xhtml#InterpretingRegression)）。您可以使用`mgcv`包在*R*中拟合广义加性模型（参见[“广义加性模型”](ch04.xhtml#GAMS)）：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The formula interface of `statsmodels` also supports these extensions in *Python*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`statsmodels`的公式接口也支持*Python*中的这些扩展：'
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Analysis of residuals
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 残差分析
- en: 'One area where logistic regression differs from linear regression is in the
    analysis of the residuals. As in linear regression (see [Figure 4-9](ch04.xhtml#HousePartialResid)),
    it is straightforward to compute partial residuals in *R*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归与线性回归不同的一个领域是在残差分析中。与线性回归类似（见[图 4-9](ch04.xhtml#HousePartialResid)），在*R*中计算部分残差是直观的：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The resulting plot is displayed in [Figure 5-4](#LogisticPartialResidual). The
    estimated fit, shown by the line, goes between two sets of point clouds. The top
    cloud corresponds to a response of 1 (defaulted loans), and the bottom cloud corresponds
    to a response of 0 (loans paid off). This is very typical of residuals from a
    logistic regression since the output is binary. The prediction is measured as
    the logit (log of the odds ratio), which will always be some finite value. The
    actual value, an absolute 0 or 1, corresponds to an infinite logit, either positive
    or negative, so the residuals (which get added to the fitted value) will never
    equal 0\. Hence the plotted points lie in clouds either above or below the fitted
    line in the partial residual plot. Partial residuals in logistic regression, while
    less valuable than in regression, are still useful to confirm nonlinear behavior
    and identify highly influential records.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图 5-4](#LogisticPartialResidual)中。估计拟合线穿过两组点云。顶部点云对应响应为1（违约贷款），底部点云对应响应为0（偿还贷款）。这在逻辑回归的残差中非常典型，因为输出是二进制的。预测被测量为logit（几率的对数），这总是某个有限值。实际值是绝对的0或1，对应于无限的logit，无论是正还是负，因此残差（添加到拟合值）永远不会等于0。因此，在部分残差图中，绘制的点云要么位于拟合线的上方，要么位于下方。尽管在逻辑回归中，部分残差不如回归中有价值，但仍然有助于确认非线性行为和识别高度影响力的记录。
- en: There is currently no implementation of partial residuals in any of the major
    *Python* packages. We provide *Python* code to create the partial residual plot
    in the accompanying source code repository.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当前主要*Python*包中没有部分残差的实现。我们提供*Python*代码以创建配套源代码库中的部分残差图。
- en: '![Partial residuals from logistic regression](Images/psd2_0504.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归的部分残差](Images/psd2_0504.png)'
- en: Figure 5-4\. Partial residuals from logistic regression
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4。逻辑回归的部分残差
- en: Warning
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Some of the output from the `summary` function can effectively be ignored. The
    dispersion parameter does not apply to logistic regression and is there for other
    types of GLMs. The residual deviance and the number of scoring iterations are
    related to the maximum likelihood fitting method; see [“Maximum Likelihood Estimation”](#MLE).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 某些`summary`函数的输出可以有效地忽略。离散参数不适用于逻辑回归，而是适用于其他类型的广义线性模型。残差偏差和评分迭代次数与最大似然拟合方法有关；参见[“最大似然估计”](#MLE)。
- en: Further Reading
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: The standard reference on logistic regression is *Applied Logistic Regression*,
    3rd ed., by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant (Wiley, 2013).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归的标准参考书是David Hosmer、Stanley Lemeshow和Rodney Sturdivant的《应用逻辑回归》，第3版（Wiley,
    2013）。
- en: 'Also popular are two books by Joseph Hilbe: *Logistic Regression Models* (very
    comprehensive, 2017) and *Practical Guide to Logistic Regression* (compact, 2015),
    both from Chapman & Hall/CRC Press.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joseph Hilbe的两本书也很受欢迎：《逻辑回归模型》（非常全面，2017年）和《逻辑回归实用指南》（简洁，2015年），均出自Chapman &
    Hall/CRC Press。
- en: Both *The Elements of Statistical Learning*, 2nd ed., by Trevor Hastie, Robert
    Tibshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, *An
    Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor
    Hastie, and Robert Tibshirani (Springer, 2013), have a section on logistic regression.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计学习的要素*第二版（Trevor Hastie, Robert Tibshirani, 和 Jerome Friedman 著，Springer
    出版，2009 年）以及它的精简版本 *统计学习导论*（Gareth James, Daniela Witten, Trevor Hastie, 和 Robert
    Tibshirani 著，Springer 出版，2013 年）都有关于逻辑回归的章节。'
- en: '*Data Mining for Business Analytics* by Galit Shmueli, Peter Bruce, Nitin Patel,
    Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions
    for *R*, *Python*, Excel, and JMP) has a full chapter on logistic regression.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据挖掘与商业分析*（Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav,
    and Kenneth Lichtendahl 著，Wiley 出版，2007–2020 年，包括 *R*、*Python*、Excel 和 JMP 版本）一书有一整章讲述逻辑回归。'
- en: Evaluating Classification Models
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: It is common in predictive modeling to train a number of different models, apply
    each to a holdout sample, and assess their performance. Sometimes, after a number
    of models have been evaluated and tuned, and if there are enough data, a third
    holdout sample, not used previously, is used to estimate how the chosen model
    will perform with completely new data. Different disciplines and practitioners
    will also use the terms *validation* and *test* to refer to the holdout sample(s).
    Fundamentally, the assessment process attempts to learn which model produces the
    most accurate and useful predictions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测建模中，通常会训练多个不同的模型，将每个模型应用于留出样本，并评估其性能。有时，在评估和调整了多个模型之后，如果有足够的数据，会使用第三个未使用过的留出样本来估计所选模型在完全新数据上的表现。不同的学科和从业者也会使用术语
    *验证* 和 *测试* 来指代留出样本。基本上，评估过程试图了解哪个模型产生了最准确和有用的预测。
- en: 'A simple way to measure classification performance is to count the proportion
    of predictions that are correct, i.e., measure the *accuracy*. Accuracy is simply
    a measure of total error:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量分类性能的一种简单方法是计算预测正确的比例，即测量 *准确性*。准确性只是总误差的一种度量方式：
- en: <math display="block"><mrow><mtext>accuracy</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Negative</mi></mrow></mrow> <mrow><mi>Sample</mi> <mi>Size</mi></mrow></mfrac></mrow></math>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>accuracy</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Negative</mi></mrow></mrow> <mrow><mi>Sample</mi> <mi>Size</mi></mrow></mfrac></mrow></math>
- en: In most classification algorithms, each case is assigned an “estimated probability
    of being a 1.”^([3](ch05.xhtml#idm46522847087352)) The default decision point,
    or cutoff, is typically 0.50 or 50%. If the probability is above 0.5, the classification
    is “1”; otherwise it is “0.” An alternative default cutoff is the prevalent probability
    of 1s in the data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数分类算法中，每个案例被分配一个“估计为 1 的概率”。^([3](ch05.xhtml#idm46522847087352)) 默认决策点或截断通常为
    0.50 或 50%。如果概率高于 0.5，则分类为“1”；否则为“0”。另一个默认截断是数据中 1 的普遍概率。
- en: Confusion Matrix
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: At the heart of classification metrics is the *confusion matrix*. The confusion
    matrix is a table showing the number of correct and incorrect predictions categorized
    by type of response. Several packages are available in *R* and *Python* to compute
    a confusion matrix, but in the binary case, it is simple to compute one by hand.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 分类指标的核心是 *混淆矩阵*。混淆矩阵是一张表，显示了按响应类型分类的正确和错误预测的数量。*R* 和 *Python* 中有几个包可用于计算混淆矩阵，但在二元情况下，通过手动计算一个是简单的。
- en: 'To illustrate the confusion matrix, consider the `logistic_gam` model that
    was trained on a balanced data set with an equal number of defaulted and paid-off
    loans (see [Figure 5-4](#LogisticPartialResidual)). Following the usual conventions,
    *Y* = 1 corresponds to the event of interest (e.g., default), and *Y* = 0 corresponds
    to a negative (or usual) event (e.g., paid off). The following computes the confusion
    matrix for the `logistic_gam` model applied to the entire (unbalanced) training
    set in *R*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要说明混淆矩阵，请考虑在平衡数据集上训练的 `logistic_gam` 模型（参见 [Figure 5-4](#LogisticPartialResidual)）。按照通常的惯例，*Y*
    = 1 对应于感兴趣的事件（例如违约），而 *Y* = 0 对应于负面（或通常的）事件（例如已偿还）。以下计算了应用于整个（不平衡的）训练集中的 `logistic_gam`
    模型的混淆矩阵在 *R* 中：
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In *Python*:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The predicted outcomes are columns and the true outcomes are the rows. The diagonal
    elements of the matrix show the number of correct predictions, and the off-diagonal
    elements show the number of incorrect predictions. For example, 14,295 defaulted
    loans were correctly predicted as a default, but 8,376 defaulted loans were incorrectly
    predicted as paid off.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果为列，真实结果为行。矩阵的对角线元素显示了正确预测的数量，而非对角线元素显示了错误预测的数量。例如，14,295 笔违约贷款被正确预测为违约，但
    8,376 笔违约贷款被错误预测为已偿还。
- en: '[Figure 5-5](#ConfusionGraphic) shows the relationship between the confusion
    matrix for a binary response *Y* and different metrics (see [“Precision, Recall,
    and Specificity”](#PrecisonRecallSpecificity) for more on the metrics). As with
    the example for the loan data, the actual response is along the rows and the predicted
    response is along the columns. The diagonal boxes (upper left, lower right) show
    when the predictions <math alttext="ModifyingAbove upper Y With caret"><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover></math> correctly predict the response.
    One important metric not explicitly called out is the false positive *rate* (the
    mirror image of precision). When 1s are rare, the ratio of false positives to
    all predicted positives can be high, leading to the unintuitive situation in which
    a predicted 1 is most likely a 0. This problem plagues medical screening tests
    (e.g., mammograms) that are widely applied: due to the relative rarity of the
    condition, positive test results most likely do not mean breast cancer. This leads
    to much confusion in the public.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-5](#ConfusionGraphic) 展示了二元响应 *Y* 的混淆矩阵与不同指标之间的关系（有关指标的更多信息，请参见[“精度、召回率和特异性”](#PrecisonRecallSpecificity)）。与贷款数据示例一样，实际响应沿行，预测响应沿列。对角线框（左上角、右下角）显示了预测
    <math alttext="修改上方的上箭头 Y"><mover accent="true"><mi>Y</mi> <mo>^</mo></mover></math>
    正确预测响应的情况。一个未明确指出的重要指标是假阳率（精度的镜像）。当 1 很少见时，假阳率相对于所有预测的阳性可能会很高，导致出现这样的不直观情况，即预测的
    1 最有可能是 0。这个问题困扰着广泛应用的医学筛查测试（例如，乳房X光检查）：由于这种情况的相对罕见，阳性测试结果很可能不意味着患有乳腺癌。这导致公众非常困惑。'
- en: '![images/confusion-matrix-terms.png](Images/psd2_0505.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图像/confusion-matrix-terms.png](Images/psd2_0505.png)'
- en: Figure 5-5\. Confusion matrix for a binary response and various metrics
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 二元响应的混淆矩阵和各种指标
- en: Warning
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Here, we present the actual response along the rows and the predicted response
    along the columns, but it is not uncommon to see this reversed. A notable example
    is the popular `caret` package in *R*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将实际响应沿行呈现，预测响应沿列呈现，但将其反转也是很常见的。一个显着的例子是 *R* 中流行的 `caret` 包。
- en: The Rare Class Problem
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 罕见类别问题
- en: In many cases, there is an imbalance in the classes to be predicted, with one
    class much more prevalent than the other—for example, legitimate insurance claims
    versus fraudulent ones, or browsers versus purchasers at a website. The rare class
    (e.g., the fraudulent claims) is usually the class of more interest and is typically
    designated 1, in contrast to the more prevalent 0s. In the typical scenario, the
    1s are the more important case, in the sense that misclassifying them as 0s is
    costlier than misclassifying 0s as 1s. For example, correctly identifying a fraudulent
    insurance claim may save thousands of dollars. On the other hand, correctly identifying
    a nonfraudulent claim merely saves you the cost and effort of going through the
    claim by hand with a more careful review (which is what you would do if the claim
    were tagged as “fraudulent”).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，待预测的类别存在不平衡，其中一个类别比另一个类别更普遍——例如，合法的保险索赔与欺诈性索赔，或者在网站上浏览者与购买者之间的情况。罕见的类别（例如，欺诈性索赔）通常是更感兴趣的类别，并且通常被指定为
    1，与更普遍的 0 相对应。在典型情况下，1 是更重要的情况，因为将它们错误分类为 0 比将 0 错误分类为 1 更为昂贵。例如，正确识别欺诈性保险索赔可能会节省数千美元。另一方面，正确识别非欺诈性索赔仅仅节省了您手动进行更仔细审查的成本和精力（如果索赔被标记为“欺诈性”，那么您会这样做）。
- en: In such cases, unless the classes are easily separable, the most accurate classification
    model may be one that simply classifies everything as a 0. For example, if only
    0.1% of the browsers at a web store end up purchasing, a model that predicts that
    each browser will leave without purchasing will be 99.9% accurate. However, it
    will be useless. Instead, we would be happy with a model that is less accurate
    overall but is good at picking out the purchasers, even if it misclassifies some
    nonpurchasers along the way.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，除非类别很容易分开，否则最准确的分类模型可能是将所有东西简单地分类为 0。例如，如果网上商店的浏览者中只有 0.1% 最终购买，那么一个预测每个浏览者都会离开而不购买的模型将达到
    99.9% 的准确率。但是，它将是无用的。相反，我们会满意一个总体上不太准确但擅长识别购买者的模型，即使在此过程中它会错误地将一些非购买者分类错误。
- en: Precision, Recall, and Specificity
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精度、召回率和特异性
- en: 'Metrics other than pure accuracy—metrics that are more nuanced—are commonly
    used in evaluating classification models. Several of these have a long history
    in statistics—especially biostatistics, where they are used to describe the expected
    performance of diagnostic tests. The *precision* measures the accuracy of a predicted
    positive outcome (see [Figure 5-5](#ConfusionGraphic)):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了纯准确度之外的指标——更加细致的指标——在评估分类模型时通常使用。其中几个指标在统计学中有着悠久的历史，特别是在生物统计学中，它们用于描述诊断测试的预期性能。*精度*衡量了预测正面结果的准确性（见[图 5-5](#ConfusionGraphic)）：
- en: <math display="block"><mrow><mtext>precision</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Positive</mi></mrow></mrow></mfrac></mrow></math>
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>precision</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Positive</mi></mrow></mrow></mfrac></mrow></math>
- en: 'The *recall*, also known as *sensitivity*, measures the strength of the model
    to predict a positive outcome—the proportion of the 1s that it correctly identifies
    (see [Figure 5-5](#ConfusionGraphic)). The term *sensitivity* is used a lot in
    biostatistics and medical diagnostics, whereas *recall* is used more in the machine
    learning community. The definition of recall is:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*，也被称为*敏感性*，衡量了模型预测正面结果的能力——它正确识别的1的比例（见[图 5-5](#ConfusionGraphic)）。术语*敏感性*在生物统计学和医学诊断中经常使用，而*召回率*在机器学习社区中更为常见。召回率的定义是：'
- en: <math display="block"><mrow><mtext>recall</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Negative</mi></mrow></mrow></mfrac></mrow></math>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>recall</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Negative</mi></mrow></mrow></mfrac></mrow></math>
- en: 'Another metric used is *specificity*, which measures a model’s ability to predict
    a negative outcome:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使用的度量指标是*特异性*，它衡量了模型预测负面结果的能力：
- en: <math display="block"><mrow><mtext>specificity</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Negative</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Negative</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Positive</mi></mrow></mrow></mfrac></mrow></math>
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>specificity</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Negative</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Negative</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Positive</mi></mrow></mrow></mfrac></mrow></math>
- en: 'We can calculate the three metrics from `conf_mat` in *R*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从*R*中的`conf_mat`计算出三个指标：
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here is the equivalent code to calculate the metrics in *Python*:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是在*Python*中计算指标的等效代码：
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`scikit-learn` has a custom method `precision_recall_fscore_support` that calculates
    precision and recall/specificity all at once.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`具有一个自定义方法`precision_recall_fscore_support`，可以一次计算精度和召回率/特异性。'
- en: ROC Curve
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC曲线
- en: You can see that there is a trade-off between recall and specificity. Capturing
    more 1s generally means misclassifying more 0s as 1s. The ideal classifier would
    do an excellent job of classifying the 1s, without misclassifying more 0s as 1s.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在召回率和特异性之间存在一种权衡。捕捉更多的1通常意味着将更多的0误分类为1。理想的分类器应该能够优秀地分类1，而不会误将更多的0分类为1。
- en: 'The metric that captures this trade-off is the “Receiver Operating Characteristics”
    curve, usually referred to as the *ROC curve*. The ROC curve plots recall (sensitivity)
    on the y-axis against specificity on the x-axis.^([4](ch05.xhtml#idm46522846485448))
    The ROC curve shows the trade-off between recall and specificity as you change
    the cutoff to determine how to classify a record. Sensitivity (recall) is plotted
    on the y-axis, and you may encounter two forms in which the x-axis is labeled:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 描述这种权衡的度量指标是“接收者操作特征曲线”，通常称为*ROC曲线*。ROC曲线将召回率（敏感性）绘制在y轴上，特异性绘制在x轴上。^([4](ch05.xhtml#idm46522846485448))
    ROC曲线显示了随着更改分类记录的截断点来确定如何分类记录时，召回率和特异性之间的权衡。特异性在x轴上绘制，左侧为1，右侧为0。
- en: Specificity plotted on the x-axis, with 1 on the left and 0 on the right
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x轴以特异性为标尺，左侧为1，右侧为0
- en: 1-Specificity plotted on the x-axis, with 0 on the left and 1 on the right
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x轴以1-特异性为标尺，左侧为0，右侧为1
- en: 'The curve looks identical whichever way it is done. The process to compute
    the ROC curve is:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 无论以何种方式执行，曲线看起来都是相同的。计算ROC曲线的过程如下：
- en: Sort the records by the predicted probability of being a 1, starting with the
    most probable and ending with the least probable.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照被预测为1的概率对记录进行排序，从最可能的开始，以最不可能的结束。
- en: Compute the cumulative specificity and recall based on the sorted records.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据排序记录计算累积特异性和召回率。
- en: 'Computing the ROC curve in *R* is straightforward. The following code computes
    ROC for the loan data:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在*R*中计算ROC曲线很简单。以下代码计算了贷款数据的ROC：
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In *Python*, we can use the `scikit-learn` function `sklearn.metrics.roc_curve`
    to calculate the required information for the ROC curve. You can find similar
    packages for *R*, e.g., `ROCR`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以使用`scikit-learn`函数`sklearn.metrics.roc_curve`来计算ROC曲线所需的信息。你也可以找到类似的*R*包，例如`ROCR`：
- en: '[PRE28]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The result is shown in [Figure 5-6](#ROCLoan). The dotted diagonal line corresponds
    to a classifier no better than random chance. An extremely effective classifier
    (or, in medical situations, an extremely effective diagnostic test) will have
    an ROC that hugs the upper-left corner—it will correctly identify lots of 1s without
    misclassifying lots of 0s as 1s. For this model, if we want a classifier with
    a specificity of at least 50%, then the recall is about 75%.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图5-6](#ROCLoan)中。虚线对角线对应于不比随机机会更好的分类器。一个极其有效的分类器（或者在医学情况下，一个极其有效的诊断测试）将有一个ROC曲线，它贴近左上角——能够正确识别大量的1，而不会误将大量的0误分类为1。对于这个模型，如果我们希望分类器的特异性至少为50%，那么召回率约为75%。
- en: '![ROC curve for the loan data](Images/psd2_0506.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![贷款数据的ROC曲线](Images/psd2_0506.png)'
- en: Figure 5-6\. ROC curve for the loan data
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 贷款数据的ROC曲线
- en: Precision-Recall Curve
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线
- en: In addition to ROC curves, it can be illuminating to examine the [precision-recall
    (PR) curve](https://oreil.ly/_89Pr). PR curves are computed in a similar way except
    that the data is ordered from least to most probable and cumulative precision
    and recall statistics are computed. PR curves are especially useful in evaluating
    data with highly unbalanced outcomes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ROC曲线之外，检查[精确率-召回率（PR）曲线](https://oreil.ly/_89Pr)也是很有启发性的。PR曲线的计算方式类似，只是数据按概率从低到高排序，并计算累积的精确率和召回率统计数据。PR曲线在评估高度不平衡结果的数据时尤为有用。
- en: AUC
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AUC
- en: 'The ROC curve is a valuable graphical tool, but by itself doesn’t constitute
    a single measure for the performance of a classifier. The ROC curve can be used,
    however, to produce the area underneath the curve (AUC) metric. AUC is simply
    the total area under the ROC curve. The larger the value of AUC, the more effective
    the classifier. An AUC of 1 indicates a perfect classifier: it gets all the 1s
    correctly classified, and it doesn’t misclassify any 0s as 1s.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线是一个有价值的图形工具，但它本身并不构成分类器性能的单一度量标准。然而，ROC曲线可以用来生成曲线下面积（AUC）指标。AUC简单地是ROC曲线下的总面积。AUC值越大，分类器越有效。AUC为1表示一个完美的分类器：它正确分类所有的1，并且不会将任何0误分类为1。
- en: A completely ineffective classifier—the diagonal line—will have an AUC of 0.5.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全无效的分类器——对角线——其AUC为0.5。
- en: '[Figure 5-7](#AUCLoan) shows the area under the ROC curve for the loan model.
    The value of AUC can be computed by a numerical integration in *R*:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-7](#AUCLoan)展示了贷款模型的ROC曲线下面积。AUC的值可以通过在*R*中进行数值积分来计算：'
- en: '[PRE29]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In *Python*, we can either calculate the accuracy as shown for *R* or use `scikit-learn`’s
    function `sklearn.metrics.roc_auc_score`. You will need to provide the expected
    value as 0 or 1:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以像*R*中显示的那样计算准确率，或者使用`scikit-learn`的函数`sklearn.metrics.roc_auc_score`。您需要提供期望值为0或1：
- en: '[PRE30]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The model has an AUC of about 0.69, corresponding to a relatively weak classifier.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的AUC约为0.69，对应于一个相对较弱的分类器。
- en: '![Area under the ROC curve for the loan data](Images/psd2_0507.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![贷款数据的ROC曲线下面积](Images/psd2_0507.png)'
- en: Figure 5-7\. Area under the ROC curve for the loan data
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 贷款数据的ROC曲线下面积
- en: False Positive Rate Confusion
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假阳性率混淆
- en: False positive/negative rates are often confused or conflated with specificity
    or sensitivity (even in publications and software!). Sometimes the false positive
    rate is defined as the proportion of true negatives that test positive. In many
    cases (such as network intrusion detection), the term is used to refer to the
    proportion of positive signals that are true negatives.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性/假阴性率经常被与特异性或敏感性混淆或混合（即使在出版物和软件中也是如此！）。有时假阳性率被定义为测试为阳性的真阴性比例。在许多情况下（如网络入侵检测），该术语用于指代被正确分类为阴性的阳性信号的比例。
- en: Lift
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升度
- en: Using the AUC as a metric to evaluate a model is an improvement over simple
    accuracy, as it can assess how well a classifier handles the trade-off between
    overall accuracy and the need to identify the more important 1s. But it does not
    completely address the rare-case problem, where you need to lower the model’s
    probability cutoff below 0.5 to avoid having all records classified as 0\. In
    such cases, for a record to be classified as a 1, it might be sufficient to have
    a probability of 0.4, 0.3, or lower. In effect, we end up overidentifying 1s,
    reflecting their greater importance.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AUC作为评估模型的指标优于简单的准确度，因为它可以评估分类器在整体准确度和识别更重要的1的需要之间的平衡。但它并不能完全解决稀有案例问题，您需要将模型的概率截断值降低到0.5以下，以避免所有记录都被分类为0。在这种情况下，要使记录被分类为1，可能只需要概率为0.4、0.3或更低。实际上，我们最终过度识别1，反映了它们更重要的情况。
- en: Changing this cutoff will improve your chances of catching the 1s (at the cost
    of misclassifying more 0s as 1s). But what is the optimum cutoff?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 调整此截断值将提高捕捉1的机会（以误将更多的0误分类为1为代价）。但最佳截断值是多少？
- en: The concept of lift lets you defer answering that question. Instead, you consider
    the records in order of their predicted probability of being 1s. Say, of the top
    10% classified as 1s, how much better did the algorithm do, compared to the benchmark
    of simply picking blindly? If you can get 0.3% response in this top decile instead
    of the 0.1% you get overall by picking randomly, the algorithm is said to have
    a *lift* (also called *gains*) of 3 in the top decile. A lift chart (gains chart)
    quantifies this over the range of the data. It can be produced decile by decile,
    or continuously over the range of the data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 提升（lift）的概念允许您推迟回答这个问题。而是，您按其预测的概率顺序考虑记录。比如，对于被分类为1的前10%，算法相较于简单随机选择，表现如何？如果在这个前10%中，您能获得0.3%的响应，而不是整体随机选择的0.1%，则称算法在前10%中具有3的*提升*（也称为*增益*）。提升图（增益图）在数据范围内量化这一点。它可以逐十分位生成，也可以连续覆盖整个数据范围。
- en: To compute a lift chart, you first produce a *cumulative gains chart* that shows
    the recall on the y-axis and the total number of records on the x-axis. The *lift
    curve* is the ratio of the cumulative gains to the diagonal line corresponding
    to random selection. *Decile gains charts* are one of the oldest techniques in
    predictive modeling, dating from the days before internet commerce. They were
    particularly popular among direct mail professionals. Direct mail is an expensive
    method of advertising if applied indiscriminately, and advertisers used predictive
    models (quite simple ones, in the early days) to identify the potential customers
    with the likeliest prospect of payoff.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算提升图，首先生成显示召回率在y轴上和记录总数在x轴上的*累积增益图*。*提升曲线*是累积增益与对应随机选择的对角线的比率。*十分位增益图*是预测建模中最古老的技术之一，可以追溯到互联网商务之前的日子。它们特别受直邮专业人员欢迎。直邮是一种昂贵的广告方法，如果不加区分地应用，广告商使用预测模型（早期非常简单的模型）来识别可能有最高回报的潜在客户。
- en: Uplift
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升
- en: Sometimes the term *uplift* is used to mean the same thing as lift. An alternate
    meaning is used in a more restrictive setting, when an A/B test has been conducted
    and the treatment (A or B) is then used as a predictor variable in a predictive
    model. The uplift is the improvement in response predicted *for an individual
    case* with treatment A versus treatment B. This is determined by scoring the individual
    case first with the predictor set to A, and then again with the predictor toggled
    to B. Marketers and political campaign consultants use this method to determine
    which of two messaging treatments should be used with which customers or voters.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有时术语*提升*（uplift）用于与提升相同的意思。在更严格的设置中，当进行A/B测试并且治疗（A或B）随后用作预测模型中的预测变量时，采用了替代含义。提升是预测响应改善*针对个别案例*，使用治疗A与治疗B。这是通过首先使用设置为A的预测器对个别案例进行评分，然后再次切换到设置为B的预测器来确定的。营销人员和政治竞选顾问使用这种方法确定应向哪些客户或选民使用哪种消息处理。
- en: A lift curve lets you look at the consequences of setting different probability
    cutoffs for classifying records as 1s. It can be an intermediate step in settling
    on an appropriate cutoff level. For example, a tax authority might have only a
    certain amount of resources that it can spend on tax audits, and it wants to spend
    them on the likeliest tax cheats. With its resource constraint in mind, the authority
    would use a lift chart to estimate where to draw the line between tax returns
    selected for audit and those left alone.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 提升曲线允许您查看为将记录分类为1而设置不同概率截断的后果。这可以是确定适当截断水平的中间步骤。例如，税务机构可能只有一定数量的资源可以用于税务审核，并希望将其用于最有可能的税务违规者。考虑到其资源限制，该机构会使用提升图来估计在哪里划定税务审核的界限，哪些税务申报可以选择进行审核，哪些可以留下。
- en: Further Reading
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Evaluation and assessment are typically covered in the context of a particular
    model (e.g., *K*-Nearest Neighbors or decision trees); three books that handle
    the subject in its own chapter are:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 评估和评估通常在特定模型的背景下进行（例如*K*最近邻或决策树）；三本独立章节处理该主题的书籍是：
- en: '*Data Mining*, 3rd ed., by Ian Whitten, Eibe Frank, and Mark Hall (Morgan Kaufmann,
    2011).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据挖掘*，第3版，Ian Whitten、Eibe Frank和Mark Hall（Morgan Kaufmann，2011年）。'
- en: '*Modern Data Science with R* by Benjamin Baumer, Daniel Kaplan, and Nicholas
    Horton (Chapman & Hall/CRC Press, 2017).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*现代数据科学与R*，Benjamin Baumer、Daniel Kaplan和Nicholas Horton（Chapman & Hall/CRC
    Press，2017年）。'
- en: '*Data Mining for Business Analytics* by Galit Shmueli, Peter Bruce, Nitin Patel,
    Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions
    for *R*, *Python*, Excel, and JMP).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于商业分析的数据挖掘*，Galit Shmueli、Peter Bruce、Nitin Patel、Peter Gedeck、Inbal Yahav和Kenneth
    Lichtendahl（Wiley，2007-2020年，包括*R*、*Python*、Excel和JMP的版本）。'
- en: Strategies for Imbalanced Data
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不平衡数据的策略
- en: The previous section dealt with evaluation of classification models using metrics
    that go beyond simple accuracy and are suitable for imbalanced data—data in which
    the outcome of interest (purchase on a website, insurance fraud, etc.) is rare.
    In this section, we look at additional strategies that can improve predictive
    modeling performance with imbalanced data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节讨论了使用超出简单准确度的指标评估分类模型，适用于不平衡数据——关注的结果（例如网站上的购买行为、保险欺诈等）很少出现的数据。在本节中，我们将探讨可以改善不平衡数据预测建模性能的额外策略。
- en: Undersampling
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**欠采样**'
- en: If you have enough data, as is the case with the loan data, one solution is
    to *undersample* (or downsample) the prevalent class, so the data to be modeled
    is more balanced between 0s and 1s. The basic idea in undersampling is that the
    data for the dominant class has many redundant records. Dealing with a smaller,
    more balanced data set yields benefits in model performance, and it makes it easier
    to prepare the data and to explore and pilot models.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有足够的数据，就像贷款数据一样，一个解决方案是*欠采样*（或降采样）主导类别，使要建模的数据在0和1之间更加平衡。欠采样的基本思想是，主导类别的数据具有许多冗余记录。处理更小、更平衡的数据集可以提升模型性能，并使数据准备、探索和试验模型变得更容易。
- en: How much data is enough? It depends on the application, but in general, having
    tens of thousands of records for the less dominant class is enough. The more easily
    distinguishable the 1s are from the 0s, the less data needed.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量有多少才够？这取决于具体的应用场景，但通常来说，对于较少出现的类别，拥有数万条记录就足够了。1和0的区分越明显，需要的数据量就越少。
- en: 'The loan data analyzed in [“Logistic Regression”](#LogisticRegression) was
    based on a balanced training set: half of the loans were paid off, and the other
    half were in default. The predicted values were similar: half of the probabilities
    were less than 0.5, and half were greater than 0.5. In the full data set, only
    about 19% of the loans were in default, as shown in *R*:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“逻辑回归”](#LogisticRegression)分析的贷款数据基于一个平衡的训练集：一半的贷款已经偿还，另一半处于违约状态。预测值相似：一半的概率小于0.5，另一半大于0.5。在完整数据集中，只有约19%的贷款处于违约状态，如在
    *R* 中显示的那样：
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In *Python*:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中：
- en: '[PRE32]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'What happens if we use the full data set to train the model? Let’s see what
    this looks like in *R*:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用完整数据集来训练模型会发生什么？让我们看看在 *R* 中的情况：
- en: '[PRE33]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'And in *Python*:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 并在 *Python* 中：
- en: '[PRE34]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Only 0.39% of the loans are predicted to be in default, or less than 1/47 of
    the expected number.^([5](ch05.xhtml#idm46522845748808)) The loans that were paid
    off overwhelm the loans in default because the model is trained using all the
    data equally. Thinking about it intuitively, the presence of so many nondefaulting
    loans, coupled with the inevitable variability in predictor data, means that,
    even for a defaulting loan, the model is likely to find some nondefaulting loans
    that it is similar to, by chance. When a balanced sample was used, roughly 50%
    of the loans were predicted to be in default.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 只有0.39%的贷款预测会违约，即预期数量的1/47以下。^([5](ch05.xhtml#idm46522845748808)) 因为模型使用所有数据进行训练，因此已偿还贷款占据了主导地位，而未偿还贷款数量较少。从直觉上讲，即使是违约贷款，由于预测数据的不可避免的变化，模型也很可能会找到一些类似的未违约贷款。当使用平衡样本时，大约50%的贷款预测会违约。
- en: Oversampling and Up/Down Weighting
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过采样和上/下加权
- en: One criticism of the undersampling method is that it throws away data and is
    not using all the information at hand. If you have a relatively small data set,
    and the rarer class contains a few hundred or a few thousand records, then undersampling
    the dominant class has the risk of throwing out useful information. In this case,
    instead of downsampling the dominant case, you should oversample (upsample) the
    rarer class by drawing additional rows with replacement (bootstrapping).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对欠采样方法的一个批评是它丢弃了数据并未充分利用手头所有的信息。如果您拥有一个相对较小的数据集，并且罕见类包含几百或几千条记录，则降低主导类的样本量有可能丢失有用的信息。在这种情况下，您应该通过抽取带替换的额外行来过采样（上采样）罕见类，而不是降低主导情况的样本量。
- en: 'You can achieve a similar effect by weighting the data. Many classification
    algorithms take a weight argument that will allow you to up/down weight the data.
    For example, apply a weight vector to the loan data using the `weight` argument
    to `glm` in *R*:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对数据进行加权，您可以达到类似的效果。许多分类算法接受一个权重参数，允许您对数据进行上/下加权。例如，在*R*中使用`glm`的`weight`参数将权重向量应用于贷款数据：
- en: '[PRE35]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Most `scikit-learn` methods allow specifying weights in the `fit` function
    using the keyword argument `sample_weight`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数`scikit-learn`方法允许在`fit`函数中使用关键字参数`sample_weight`指定权重：
- en: '[PRE36]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The weights for loans that default are set to <math alttext="StartFraction 1
    Over p EndFraction"><mfrac><mn>1</mn> <mi>p</mi></mfrac></math> , where *p* is
    the probability of default. The nondefaulting loans have a weight of 1. The sums
    of the weights for the defaulting loans and nondefaulting loans are roughly equal.
    The mean of the predicted values is now about 58% instead of 0.39%.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于违约贷款，权重设置为<math alttext="StartFraction 1 Over p EndFraction"><mfrac><mn>1</mn>
    <mi>p</mi></mfrac></math> ，其中*p*是违约的概率。未违约贷款的权重为1。违约贷款和未违约贷款的权重总和大致相等。预测值的均值现在约为58%，而不是0.39%。
- en: Note that weighting provides an alternative to both upsampling the rarer class
    and downsampling the dominant class.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，加权提供了一个替代方案，既不需要过采样罕见类，也不需要降低主导类的样本量。
- en: Adapting the Loss Function
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整损失函数
- en: 'Many classification and regression algorithms optimize a certain criteria or
    *loss function*. For example, logistic regression attempts to minimize the deviance.
    In the literature, some propose to modify the loss function in order to avoid
    the problems caused by a rare class. In practice, this is hard to do: classification
    algorithms can be complex and difficult to modify. Weighting is an easy way to
    change the loss function, discounting errors for records with low weights in favor
    of records with higher weights.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分类和回归算法优化特定的标准或*损失函数*。例如，逻辑回归试图最小化偏差。在文献中，一些人建议修改损失函数以避免罕见类所引起的问题。实际操作中，这很难做到：分类算法可能非常复杂且难以修改。通过加权可以轻松改变损失函数，以便在记录权重较低的情况下减少误差，而不是记录权重较高的情况。
- en: Data Generation
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生成
- en: A variation of upsampling via bootstrapping (see [“Oversampling and Up/Down
    Weighting”](#UpDownWeighting)) is *data generation* by perturbing existing records
    to create new records. The intuition behind this idea is that since we observe
    only a limited set of instances, the algorithm doesn’t have a rich set of information
    to build classification “rules.” By creating new records that are similar but
    not identical to existing records, the algorithm has a chance to learn a more
    robust set of rules. This notion is similar in spirit to ensemble statistical
    models such as boosting and bagging (see [Chapter 6](ch06.xhtml#StatisticalML)).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入bootstrap的一种上采样变体（参见[“过采样和上/下加权”](#UpDownWeighting)），可以通过扰动现有记录来生成新记录。这个想法的直觉是，由于我们只观察到有限的一组实例，算法没有丰富的信息集来构建分类“规则”。通过创建与现有记录相似但不完全相同的新记录，算法有机会学习更健壮的规则集。这个概念在精神上与增强统计模型（如boosting和bagging，参见[第6章](ch06.xhtml#StatisticalML)）类似。
- en: The idea gained traction with the publication of the *SMOTE* algorithm, which
    stands for “Synthetic Minority Oversampling Technique.” The SMOTE algorithm finds
    a record that is similar to the record being upsampled (see [“K-Nearest Neighbors”](ch06.xhtml#KNN))
    and creates a synthetic record that is a randomly weighted average of the original
    record and the neighboring record, where the weight is generated separately for
    each predictor. The number of synthetic oversampled records created depends on
    the oversampling ratio required to bring the data set into approximate balance
    with respect to outcome classes.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 该想法随着*SMOTE*算法的发布而受到关注，它代表“合成少数过采样技术”。SMOTE算法找到与正在上采样的记录相似的记录（参见[“K最近邻”](ch06.xhtml#KNN)），并创建一个合成记录，该记录是原始记录和相邻记录的随机加权平均值，其中权重针对每个预测变量分别生成。创建的合成过采样记录数量取决于为使数据集在结果类别方面大致平衡所需的过采样比例。
- en: There are several implementations of SMOTE in *R*. The most comprehensive package
    for handling unbalanced data is `unbalanced`. It offers a variety of techniques,
    including a “Racing” algorithm to select the best method. However, the SMOTE algorithm
    is simple enough that it can be implemented directly in *R* using the `FNN` package.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在*R*中有几种SMOTE的实现。处理不平衡数据的最全面的包是`unbalanced`。它提供了各种技术，包括一种“Racing”算法来选择最佳方法。然而，SMOTE算法足够简单，可以直接在*R*中使用`FNN`包实现。
- en: The *Python* package `imbalanced-learn` implements a variety of methods with
    an API that is compatible with `scikit-learn`. It provides various methods for
    over- and undersampling and support for using these techniques with boosting and
    bagging classifiers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*包`imbalanced-learn`实现了与`scikit-learn`兼容的API，提供了各种过采样和欠采样方法，并支持将这些技术与boosting和bagging分类器一起使用。'
- en: Cost-Based Classification
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于成本的分类
- en: 'In practice, accuracy and AUC are a poor man’s way to choose a classification
    rule. Often, an estimated cost can be assigned to false positives versus false
    negatives, and it is more appropriate to incorporate these costs to determine
    the best cutoff when classifying 1s and 0s. For example, suppose the expected
    cost of a default of a new loan is <math alttext="upper C"><mi>C</mi></math> and
    the expected return from a paid-off loan is <math alttext="upper R"><mi>R</mi></math>
    . Then the expected return for that loan is:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，准确率和AUC是选择分类规则的一种不太精确的方法。通常，可以为假阳性与假阴性分配一个估计成本，并更适合于将这些成本合并以确定在分类1和0时的最佳截断点。例如，假设新贷款违约的预期成本为<math
    alttext="upper C"><mi>C</mi></math>，而已偿还贷款的预期收益为<math alttext="upper R"><mi>R</mi></math>。那么该贷款的预期收益为：
- en: <math display="block"><mrow><mtext>expected</mtext> <mtext>return</mtext> <mo>=</mo>
    <mi>P</mi> <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo>)</mo> <mo>×</mo> <mi>R</mi>
    <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo> <mo>×</mo>
    <mi>C</mi></mrow></math>
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>expected</mtext> <mtext>return</mtext> <mo>=</mo>
    <mi>P</mi> <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo>)</mo> <mo>×</mo> <mi>R</mi>
    <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo> <mo>×</mo>
    <mi>C</mi></mrow></math>
- en: Instead of simply labeling a loan as default or paid off, or determining the
    probability of default, it makes more sense to determine if the loan has a positive
    expected return. Predicted probability of default is an intermediate step, and
    it must be combined with the loan’s total value to determine expected profit,
    which is the ultimate planning metric of business. For example, a smaller value
    loan might be passed over in favor of a larger one with a slightly higher predicted
    default probability.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅将贷款简单标记为违约或已偿还，或者确定违约概率，更有意义的是确定贷款是否有正期望收益。预测的违约概率是一个中间步骤，必须与贷款的总价值结合起来来确定预期利润，这是业务的最终规划指标。例如，可能会选择放弃较小价值的贷款，而选择具有稍高预测违约概率的较大贷款。
- en: Exploring the Predictions
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索预测
- en: 'A single metric, such as AUC, cannot evaluate all aspects of the suitability
    of a model for a situation. [Figure 5-8](#ComparisonOfRules) displays the decision
    rules for four different models fit to the loan data using just two predictor
    variables: `borrower_score` and `payment_inc_ratio`. The models are linear discriminant
    analysis (LDA), logistic linear regression, logistic regression fit using a generalized
    additive model (GAM), and a tree model (see [“Tree Models”](ch06.xhtml#TreeModels)).
    The region to the upper left of the lines corresponds to a predicted default.
    LDA and logistic linear regression give nearly identical results in this case.
    The tree model produces the least regular rule, with two steps. Finally, the GAM
    fit of the logistic regression represents a compromise between the tree model
    and the linear model.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 单一度量，如AUC，不能评估模型在某种情况下的所有适用性方面。[图5-8](#ComparisonOfRules)展示了四种不同模型对贷款数据进行拟合时使用的决策规则，仅使用两个预测变量：`borrower_score`和`payment_inc_ratio`。这些模型包括线性判别分析（LDA）、逻辑线性回归、使用广义加性模型（GAM）拟合的逻辑回归以及树模型（参见[“树模型”](ch06.xhtml#TreeModels)）。在线性判别分析（LDA）和逻辑线性回归中，这些模型在本例中给出了几乎相同的结果。树模型产生了最不规则的规则，具有两个步骤。最后，逻辑回归的GAM拟合代表了树模型和线性模型之间的折衷。
- en: '![Comparison of the classification rules for four different methods. Logistic
    and LDA produce nearly identical, overlapping linear classifiers.](Images/psd2_0508.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![四种不同方法分类规则的比较图。Logistic和LDA产生几乎相同且重叠的线性分类器。](Images/psd2_0508.png)'
- en: Figure 5-8\. Comparison of the classification rules for four different methods
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 四种不同方法的分类规则比较
- en: It is not easy to visualize the prediction rules in higher dimensions or, in
    the case of the GAM and the tree model, even to generate the regions for such
    rules.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高维度中或在GAM和树模型的情况下，甚至为这些规则生成区域都不容易可视化预测规则。
- en: In any case, exploratory analysis of predicted values is always warranted.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，总是有必要对预测值进行探索性分析。
- en: Further Reading
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Tom Fawcett, author of *Data Science for Business*, has a [good article on imbalanced
    classes](https://oreil.ly/us2rd).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据科学与业务*的作者Tom Fawcett在[不平衡类问题](https://oreil.ly/us2rd)上有一篇很好的文章。'
- en: 'For more on SMOTE, see Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall,
    and W. Philip Kegelmeyer, [“SMOTE: Synthetic Minority Over-sampling Technique,”](https://oreil.ly/bwaIQ)
    *Journal of Artificial Intelligence Research* 16 (2002): 321–357.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欲了解更多关于SMOTE的信息，请参阅Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall和W. Philip
    Kegelmeyer在[“SMOTE：合成少数过采样技术”](https://oreil.ly/bwaIQ)中的论文，《人工智能研究杂志》第16卷（2002年）：321-357。
- en: Also see the Analytics Vidhya Content Team’s [“Practical Guide to Deal with
    Imbalanced Classification Problems in *R*,”](https://oreil.ly/gZUDs) March 28,
    2016.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也请参阅Analytics Vidhya内容团队的[“在*R*中处理不平衡分类问题的实用指南”](https://oreil.ly/gZUDs)，2016年3月28日。
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: Classification, the process of predicting which of two or more categories a
    record belongs to, is a fundamental tool of predictive analytics. Will a loan
    default (yes or no)? Will it prepay? Will a web visitor click on a link? Will
    they purchase something? Is an insurance claim fraudulent? Often in classification
    problems, one class is of primary interest (e.g., the fraudulent insurance claim),
    and in binary classification, this class is designated as a 1, with the other,
    more prevalent class being a 0\. Often, a key part of the process is estimating
    a *propensity score*, a probability of belonging to the class of interest. A common
    scenario is one in which the class of interest is relatively rare. When evaluating
    a classifier, there are a variety of model assessment metrics that go beyond simple
    accuracy; these are important in the rare-class situation, when classifying all
    records as 0s can yield high accuracy.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是预测记录属于两个或多个类别中的哪一个的过程，是预测分析的基本工具。贷款是否违约（是或否）？它是否预付？网络访问者是否会点击链接？他们是否会购买东西？保险索赔是否欺诈？在分类问题中，通常一个类别是主要关注的（例如欺诈性保险索赔），在二元分类中，这个类别被指定为1，而其他更普遍的类别为0。通常，过程的关键部分是估计*倾向分数*，即属于感兴趣类别的概率。常见的情况是感兴趣类别相对稀有。在评估分类器时，有许多模型评估指标超出简单的准确性；在罕见类情况下，将所有记录分类为0可能会产生高准确性。
- en: ^([1](ch05.xhtml#idm46522850239256-marker)) This and subsequent sections in
    this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck;
    used with permission.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm46522850239256-marker)) 本章及后续章节版权 © 2020 Datastats, LLC,
    Peter Bruce, Andrew Bruce, and Peter Gedeck；已获授权使用。
- en: ^([2](ch05.xhtml#idm46522849542200-marker)) It is certainly surprising that
    the first article on statistical classification was published in a journal devoted
    to eugenics. Indeed, there is a [disconcerting connection between the early development
    of statistics and eugenics](https://oreil.ly/eUJvR).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm46522849542200-marker)) 令人惊讶的是，第一篇关于统计分类的文章竟然发表在一本致力于优生学的期刊上。确实，统计学早期发展与优生学之间存在[令人不安的联系](https://oreil.ly/eUJvR)。
- en: ^([3](ch05.xhtml#idm46522847087352-marker)) Not all methods provide unbiased
    estimates of probability. In most cases, it is sufficient that the method provide
    a ranking equivalent to the rankings that would result from an unbiased probability
    estimate; the cutoff method is then functionally equivalent.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#idm46522847087352-marker)) 并非所有方法都能提供概率的无偏估计。在大多数情况下，只要方法提供与无偏概率估计产生相同排名的排名即可满足要求；然后截止方法在功能上是等效的。
- en: ^([4](ch05.xhtml#idm46522846485448-marker)) The ROC curve was first used during
    World War II to describe the performance of radar receiving stations, whose job
    was to correctly identify (classify) reflected radar signals and alert defense
    forces to incoming aircraft.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.xhtml#idm46522846485448-marker)) ROC 曲线首次用于描述二战期间雷达接收站的性能，其任务是正确识别（分类）反射的雷达信号并警示防御部队迎击来袭飞机。
- en: '^([5](ch05.xhtml#idm46522845748808-marker)) Due to differences in implementation,
    results in *Python* differ slightly: 1%, or about 1/18 of the expected number.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.xhtml#idm46522845748808-marker)) 由于实现方式的差异，*Python* 中的结果略有不同：1%，大约是预期数量的
    1/18。
