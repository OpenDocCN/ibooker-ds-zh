- en: Chapter 3\. Scraping Websites and Extracting Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。网站抓取和数据提取
- en: Often, it will happen that you visit a website and find the content interesting.
    If there are only a few pages, it’s possible to read everything on your own. But
    as soon as there is a considerable amount of content, reading everything on your
    own will not be possible.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 经常会发生这样的情况，你访问一个网站并发现内容很有趣。如果只有几页，可能可以自己阅读所有内容。但是一旦有大量内容，就不可能自己阅读所有内容了。
- en: To use the powerful text analytics blueprints described in this book, you have
    to acquire the content first. Most websites won’t have a “download all content”
    button, so we have to find a clever way to download (“scrape”) the pages.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本书描述的强大文本分析蓝图，首先必须获取内容。大多数网站不会有“下载所有内容”按钮，因此我们必须找到一个巧妙的方法来下载（“抓取”）页面。
- en: Usually we are mainly interested in the content part of each individual web
    page, less so in navigation, etc. As soon as we have the data locally available,
    we can use powerful extraction techniques to dissect the pages into elements such
    as title, content, and also some meta-information (publication date, author, and
    so on).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们主要对每个网页的内容部分感兴趣，对导航等不太感兴趣。一旦我们在本地有了数据，我们可以使用强大的提取技术来将页面分解为标题、内容以及一些元信息（发布日期、作者等）。
- en: What You’ll Learn and What We’ll Build
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你将学到的内容及我们将要构建的东西
- en: In this chapter, we will show you how to acquire HTML data from websites and
    use powerful tools to extract the content from these HTML files. We will show
    this with content from one specific data source, the Reuters news archive.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向你展示如何从网站获取 HTML 数据，并使用强大的工具从这些 HTML 文件中提取内容。我们将以一个特定数据源，路透社新闻存档中的内容为例。
- en: In the first step, we will download single HTML files and extract data from
    each one with different methods.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们将下载单个 HTML 文件，并使用不同的方法从每个文件中提取数据。
- en: Normally, you will not be interested in single pages. Therefore, we will build
    a blueprint solution. We will download and analyze a news archive page (which
    contains links to all articles). After completing this, we know the URLs of the
    referred documents. Then you can download the documents at the URLs and extract
    their content to a Pandas `DataFrame`.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通常你不会对单个页面感兴趣。因此，我们将建立一个蓝图解决方案。我们将下载并分析一个新闻存档页面（其中包含所有文章的链接）。完成后，我们将知道所引用文档的
    URL。然后你可以下载这些 URL 的文档并提取它们的内容到 Pandas 的`DataFrame`中。
- en: After studying this chapter, you will have a good overview of methods that download
    HTML and extract data. You will be familiar with the different extraction methods
    for content provided by Python. We will have seen a complete example for downloading
    and extracting data. For your own work, you will be able to select an appropriate
    framework. In this chapter, we will provide standard blueprints for extracting
    often-used elements that you can reuse.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 学习完本章后，你将对下载 HTML 和提取数据的方法有一个很好的概述。你将熟悉 Python 提供的不同内容提取方法。我们将看到一个完整的示例，用于下载和提取数据。对于你自己的工作，你将能够选择一个合适的框架。在本章中，我们将提供用于提取经常使用的元素的标准蓝图，你可以重复使用。
- en: Scraping and Data Extraction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取和数据提取
- en: Scraping websites is a complex process consisting of typically three different
    phases, as illustrated in [Figure 3-1](#outline_of_scraping_process).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 网站抓取是一个复杂的过程，通常包括三个不同阶段，如[图3-1](#outline_of_scraping_process)所示。
- en: '![](Images/btap_0301.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0301.jpg)'
- en: Figure 3-1\. Outline of scraping process.
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。抓取过程概述。
- en: In the first step, we have to generate all interesting URLs of a website. Afterward,
    we can use different tools to download the pages from the corresponding URLs.
    Finally, we will extract the “net” data from the downloaded pages; we can also
    use different strategies in this phase. Of course, it is crucial to permanently
    save extracted data. In this chapter, we use a Pandas `DataFrame` that offers
    a variety of persistence mechanisms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们必须生成网站所有有趣的 URL。然后，我们可以使用不同的工具从相应的 URL 下载页面。最后，我们将从下载的页面中提取“净”数据；在此阶段我们也可以使用不同的策略。当然，永久保存提取的数据是至关重要的。在本章中，我们使用
    Pandas 的`DataFrame`，它提供了各种持久化机制。
- en: Introducing the Reuters News Archive
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍路透社新闻存档
- en: Let’s assume we are interested in analyzing the current and past political situation
    and are looking for an appropriate dataset. We want to find some trends, uncover
    when a word or topic was introduced for the first time, and so on. For this, our
    aim is to convert the documents to a Pandas `DataFrame`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对分析当前和过去的政治局势感兴趣，并正在寻找适当的数据集。我们希望找到一些趋势，揭示一个词或主题何时首次引入等等。为此，我们的目标是将文档转换为一个
    Pandas `DataFrame`。
- en: Obviously, news headlines and articles are well suited as a database for these
    requirements. If possible, we should find an archive that goes back a few years,
    ideally even some decades.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，新闻头条和文章非常适合作为这些需求的数据库。如果可能的话，我们应该找到一个档案，可以追溯几年，甚至几十年。
- en: Some newspapers have such archives, but most of them will also have a certain
    political bias that we want to avoid if possible. We are looking for content that
    is as neutral as possible.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些报纸拥有这样的档案，但大多数也会有一定的政治偏见，如果可能的话，我们希望避免这种情况。我们正在寻找尽可能中立的内容。
- en: 'This is why we decided to use the Reuters news archive. Reuters is an international
    news organization and works as a news agency; in other words, it provides news
    to many different publications. It was founded more than a hundred years ago and
    has a lot of news articles in its archives. It’s a good source of content for
    many reasons:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们决定使用路透社新闻档案的原因。路透社是一家国际新闻组织，作为新闻机构运作；换句话说，它向许多不同的出版物提供新闻。它成立了一百多年，档案中有大量新闻文章。由于许多原因，它是内容的良好来源：
- en: It is politically neutral.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在政治上是中立的。
- en: It has a big archive of news.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它拥有大量的新闻档案。
- en: News articles are categorized in sections.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新闻文章被分类在不同的部分中。
- en: The focus is not on a specific region.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 焦点不在于特定的地区。
- en: Almost everybody will find some interesting headlines there.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎每个人都会在那里找到一些有趣的头条新闻。
- en: It has a liberal policy for downloading data.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对下载数据有宽松的政策。
- en: It is very well connected, and the website itself is fast.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的连接速度非常快。
- en: URL Generation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: URL生成
- en: For downloading content from the Reuters’ archive, we need to know the URLs
    of the content pages. After we know the URLs, the download itself is easy as there
    are powerful Python tools available to accomplish that.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要从路透社档案下载内容，我们需要知道内容页面的URL。一旦知道了URL，下载本身就很容易，因为有强大的Python工具可以实现这一点。
- en: At first sight it might seem easy to find URLs, but in practice it is often
    not so simple. The process is called *URL generation*, and in many crawling projects
    it is one of the most difficult tasks. We have to make sure that we do not systematically
    miss URLs; therefore, thinking carefully about the process in the beginning is
    crucial. Performed correctly, URL generation can also be a tremendous time-saver.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，找到网址似乎很容易，但实际上往往并非如此简单。这个过程称为*URL生成*，在许多爬行项目中，这是最困难的任务之一。我们必须确保不会系统性地错过网址；因此，在开始时仔细思考这个过程至关重要。如果正确执行，URL生成也可以节省大量时间。
- en: Before You Download
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在下载之前
- en: 'Be careful: sometimes downloading data is illegal. The rules and legal situation
    might depend on the country where the data is hosted and into which country it
    is downloaded. Often, websites have a page called “terms of use” or something
    similar that might be worth taking a look at.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有时下载数据是非法的。规则和法律情况可能取决于数据托管的国家及其下载到的国家。通常，网站会有一个名为“使用条款”或类似的页面，值得一看。
- en: 'If data is saved only temporarily, the same rules for search engines might
    apply. As search engines like Google cannot read and understand the terms of use
    of every single page they index, there is a really old protocol called the [robots
    exclusion standard](https://oreil.ly/IWysG). Websites using this have a file called *robots.txt* at
    the top level. This file can be downloaded and interpreted automatically. For
    single websites, it is also possible to read it manually and interpret the data.
    The rule of thumb is that if there is no `Disallow: *`, you should be allowed
    to download and (temporarily) save the content.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '如果数据只是临时保存，同样的规则也适用于搜索引擎。就像 Google 等搜索引擎无法阅读和理解它们索引的每一页的使用条款一样，有一个非常老的协议称为[robots排除标准](https://oreil.ly/IWysG)。使用这个的网站在顶级有一个名为*robots.txt*的文件。这个文件可以自动下载和解释。对于单个网站，也可以手动读取并解释数据。经验法则是，如果没有`Disallow:
    *`，你应该被允许下载和（暂时）保存内容。'
- en: 'There are many different possibilities:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的可能性：
- en: Crawling
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取
- en: Start on the home page (or a section) of the website and download all links
    on the same website. Crawling might take some time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从网站的主页（或某一部分）开始，下载同一网站上的所有链接。爬行可能需要一些时间。
- en: URL generators
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: URL 生成器
- en: Writing a URL generator is a slightly more sophisticated solution. This is most
    suitable for use on hierarchically organized content like forums, blogs, etc.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个 URL 生成器是一个稍微复杂一点的解决方案。这在像论坛、博客等分级组织内容的地方最为适用。
- en: Search engines
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎
- en: Ask search engines for specific URLs and download only these specific URLs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请求搜索引擎获取特定的 URL，并仅下载这些特定的 URL。
- en: Sitemaps
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 网站地图
- en: A standard called [*sitemap.xml*](https://oreil.ly/XANO0), which was originally
    conceived for search engines, is an interesting alternative. A file called *sitemap.xml*
    contains a list of all pages on a website (or references to sub-sitemaps). Contrary
    to *robots.txt*, the filename is not fixed and can sometimes be found in *robots.txt*
    itself. The best guess is to look for *sitemap.xml* on the top level of a website.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为[*sitemap.xml*](https://oreil.ly/XANO0)的标准，最初是为搜索引擎而设计的，是一个有趣的替代方案。一个名为
    *sitemap.xml* 的文件包含网站上所有页面的列表（或子站点的引用）。与 *robots.txt* 相反，文件名并非固定，有时可以在 *robots.txt*
    中找到。最好的猜测是在网站的顶级目录中查找 *sitemap.xml*。
- en: RSS
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RSS
- en: The [RSS format](https://oreil.ly/_aOOM) was originally conceived for newsfeeds
    and is still in wide use for subscribing to frequently changing content sources.
    It works via XML files and does not only contain URLs but also document titles
    and sometimes summaries of articles.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[RSS 格式](https://oreil.ly/_aOOM)最初是为新闻订阅而设计，并且仍然广泛用于订阅内容频繁变化的来源。它通过 XML 文件工作，不仅包含
    URL，还包括文档标题，有时还有文章摘要。'
- en: Specialized programs
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 专业程序
- en: Downloading data from social networks and similar content is often simplified
    by using specialized programs that are available on GitHub (such as [Facebook
    Chat Downloader](https://oreil.ly/ThyNf) for Facebook Chats, [Instaloader](https://oreil.ly/utGsC)
    for Instagram, and so on).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用在 GitHub 上可用的专门程序（如 [Facebook 聊天下载器](https://oreil.ly/ThyNf) 用于 Facebook
    聊天，[Instaloader](https://oreil.ly/utGsC) 用于 Instagram 等），简化从社交网络和类似内容的下载数据。
- en: In the following sections, we focus on *robots.txt*, *sitemaps.xml*, and RSS
    feeds. Later in the chapter, we show a multistage download that uses URL generators.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下各节中，我们将重点放在 *robots.txt*、*sitemaps.xml* 和 RSS 订阅上。本章稍后，我们将展示使用 URL 生成器的多阶段下载。
- en: 'Note: Use an API for Downloading Data If It’s Available'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意：如果有 API 可用，请使用 API 下载数据
- en: Instead of generating the URLs, downloading the content, and extracting it,
    using an API is much easier and more stable. You will find more information about
    that in [Chapter 2](ch02.xhtml#ch-api).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 API 而不是生成 URL、下载内容和提取内容，更简单且更稳定。关于此，您将在 [第 2 章](ch02.xhtml#ch-api) 中找到更多信息。
- en: 'Blueprint: Downloading and Interpreting robots.txt'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：下载和解释 robots.txt
- en: Finding the content on a website is often not so easy. To see the techniques
    mentioned earlier in action, we’ll take a look at the Reuters news archive. Of
    course, (almost) any other website will work in a similar fashion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在网站上找到内容通常并不那么容易。为了看到前面提到的技术实际操作，我们将查看 Reuters 新闻档案。当然，（几乎）任何其他网站都会以类似的方式工作。
- en: 'As discussed, [*robots.txt*](https://www.reuters.com/robots.txt) is a good
    starting point:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论的，[*robots.txt*](https://www.reuters.com/robots.txt) 是一个很好的起点：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Some user agents are not allowed to download anything, but the rest may do
    that. We can check that programmatically in Python:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有些用户代理程序不被允许下载任何内容，但其他用户可以这样做。我们可以用 Python 程序来检查这一点：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Out:`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Blueprint: Finding URLs from sitemap.xml'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：从 sitemap.xml 查找 URL
- en: Reuters is even nice enough to mention the URLs of the [sitemap for the news](https://reuters.com/sitemap_news_index.xml),
    which actually contains only a reference to [other sitemap files](https://www.reuters.com/sitemap_news_index1.xml).
    Let’s download that. An excerpt at the time of writing looks like this:^([1](ch03.xhtml#idm45634207731272))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Reuters 甚至友好地提到了新闻 [站点地图](https://reuters.com/sitemap_news_index.xml) 的 URL，实际上只包含对
    [其他站点地图文件](https://www.reuters.com/sitemap_news_index1.xml) 的引用。让我们下载它。撰写时的节选如下：^([1](ch03.xhtml#idm45634207731272))
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The most interesting part is the line with `<loc>`, as it contains the URL of
    the article. Filtering out all these `<loc>` lines leads to a list of URLs for
    news articles that can be downloaded afterward.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最有趣的部分是带有 `<loc>` 的行，因为它包含文章的 URL。过滤掉所有这些 `<loc>` 行将导致一个可以随后下载的新闻文章 URL 列表。
- en: As Python has an incredibly rich ecosystem of libraries, it’s not hard to find
    a sitemap parser. There are several available, such as [`ultimate-sitemap-parser`](https://oreil.ly/XgY9z).
    However, this parser downloads the whole sitemap hierarchy, which is a bit too
    sophisticated for us as we just want the URLs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Python拥有一个非常丰富的库生态系统，找到一个站点地图解析器并不难。有几种可用，比如[`ultimate-sitemap-parser`](https://oreil.ly/XgY9z)。然而，这种解析器下载整个站点地图层次结构，对于我们来说有点过于复杂，因为我们只需URL。
- en: It’s easy to convert *sitemap.xml* to an associative array (hash) that is called
    a `dict` in Python:^([2](ch03.xhtml#idm45634208553592))
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将*sitemap.xml*转换为Python中称为`dict`的关联数组（哈希）非常容易：^([2](ch03.xhtml#idm45634208553592))
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s check what is in the `dict` before actually downloading the files^([3](ch03.xhtml#idm45634212104584)):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在实际下载文件之前检查一下`dict`中有什么内容^([3](ch03.xhtml#idm45634212104584))：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Out:`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We will use this list of URLs in the following section and download their content.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中使用这些URL列表并下载它们的内容。
- en: 'Blueprint: Finding URLs from RSS'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：从RSS中找到URL
- en: As Reuters is a news website, it also offers access to its articles via an RSS
    feed. Several years ago, browsers would show an RSS icon next to the URL if you
    could subscribe to this source. While those days are gone, it is still not too
    difficult to find the URLs for RSS feeds. At the bottom of the website, we can
    see a line with navigation icons, as shown in [Figure 3-2](#part_of_the_reuters_website_which_links_to_the_rss_feed).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于路透社是一个新闻网站，它也通过RSS提供其文章的访问。几年前，如果你可以订阅这个源，浏览器会在URL旁边显示一个RSS图标。虽然那些日子已经过去，但现在仍然不难找到RSS源的URL。在网站底部，我们可以看到一行带有导航图标的内容，如[图3-2](#part_of_the_reuters_website_which_links_to_the_rss_feed)所示。
- en: '![](Images/btap_0302.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0302.jpg)'
- en: Figure 3-2\. Part of the Reuters website that links to the RSS feed.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2。链接到RSS源的路透社网站的一部分。
- en: The icon that looks like a WIFI indicator is the link to the RSS feeds page.
    Often (and sometimes more easily) this can be found by taking a look at the source
    code of the corresponding webpage and searching for *RSS*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像WIFI指示器的图标是指向RSS订阅页面的链接。通常（有时更容易），这可以通过查看相应网页的源代码并搜索*RSS*来找到。
- en: 'The world news RSS feed has the URL [*http://feeds.reuters.com/Reuters/worldNews*](http://feeds.reuters.com/Reuters/worldNews)^([4](ch03.xhtml#idm45634206884968))
    and can easily be parsed in Python, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 世界新闻的RSS源URL是[*http://feeds.reuters.com/Reuters/worldNews*](http://feeds.reuters.com/Reuters/worldNews)^([4](ch03.xhtml#idm45634206884968))，在Python中可以轻松解析如下：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The individual format of the RSS file might differ from site to site. However,
    most of the time we will find title and link as fields^([5](ch03.xhtml#idm45634206875096)):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RSS文件的具体格式可能因网站而异。然而，大多数情况下，我们会找到标题和链接作为字段^([5](ch03.xhtml#idm45634206875096))：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Out:`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In our case, we are more interested in the “real” URLs, which are contained
    in the `id` field:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们更感兴趣的是包含在`id`字段中的“真实”URL：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`Out:`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Great, we have found an alternative way to get a list of URLs that can be used
    when no *sitemap.xml* is available.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，我们找到了一种替代方法，可以在没有*sitemap.xml*的情况下获取URL列表。
- en: Sometimes you will still encounter so-called [*Atom feeds*](https://oreil.ly/Jcdgi), which
    basically offer the same information as RSS in a different format.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你仍会遇到所谓的[*Atom feeds*](https://oreil.ly/Jcdgi)，它们基本上以不同的格式提供与RSS相同的信息。
- en: If you wanted to implement a website monitoring tool, taking a periodic look
    at Reuters news (or other news sources) or RSS (or Atom) would be a good way to
    go ahead.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要实现一个网站监控工具，定期查看路透社新闻（或其他新闻源）或RSS（或Atom）是一个不错的方法。
- en: If you are interested in whole websites, looking for *sitemap.xml* is an excellent
    idea. Sometimes it might be difficult to find (hints might be in *robots.txt*),
    but it is almost always worth the extra effort to find it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对整个网站感兴趣，寻找*sitemap.xml*是一个绝佳的主意。有时可能会很难找到（提示可能在*robots.txt*中），但多数情况下额外努力去找它几乎总是值得的。
- en: If you cannot find *sitemap.xml* and you plan to regularly download content,
    going for RSS is a good second choice.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找不到*sitemap.xml*并且你计划定期下载内容，那么转向RSS是一个很好的第二选择。
- en: Whenever possible, try to avoid crawling websites for URLs. The process is largely
    uncontrollable, can take a long time, and might yield incomplete results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，尽量避免对URL进行爬取。这个过程很难控制，可能需要很长时间，并且可能得到不完整的结果。
- en: Downloading Data
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载数据
- en: At first sight, downloading data might seem like the most difficult and time-consuming
    part of the scraping process. Often, that’s not true as you can accomplish it
    in a highly standardized way.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，下载数据可能看起来是网页抓取过程中最困难和耗时的部分。通常情况下，并非如此，因为您可以以高度标准化的方式完成它。
- en: In this section, we show different methods for downloading data, both with Python
    libraries and external tools. Especially for big projects, using external programs
    has some advantages.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了使用Python库和外部工具下载数据的不同方法。特别是对于大型项目，使用外部程序具有一些优势。
- en: Compared to several years ago, the Internet is much faster today. Big websites
    have reacted to this development by using content-delivery networks, which can
    speed them up by orders of magnitude. This helps us a lot as the actual downloading
    process is not as slow as it used to be but is more or less limited by our own
    bandwidth.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与几年前相比，今天的互联网速度快得多。大型网站已经通过使用内容交付网络做出了反应，这可以将它们的速度提高几个数量级。这对我们非常有帮助，因为实际的下载过程并不像过去那样慢，而是更多地受限于我们自己的带宽。
- en: 'Blueprint: Downloading HTML Pages with Python'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用Python下载HTML页面
- en: 'To download HTML pages, it’s necessary to know the URLs. As we have seen, the
    URLs are contained in the sitemap. Let’s use this list to download the content:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载HTML页面，需要知道URL。正如我们所见，URL包含在站点地图中。让我们使用这个列表来下载内容：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Out:`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Depending on your Internet connection, it might take longer, but that was quite
    fast. Using the session abstraction, we make sure to have maximum speed by leveraging
    keep-alive, SSL session caching, and so on.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的互联网连接速度不同，可能需要更长时间，但下载速度相当快。通过使用会话抽象，我们通过利用保持活动状态、SSL会话缓存等来确保最大速度。
- en: Use Proper Error Handling When Downloading URLs
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在下载URL时使用适当的错误处理
- en: When downloading URLs, you are using a network protocol to communicate with
    remote servers. There are many kinds of errors that can happen, such as changed
    URLs, servers not responding, etc. The example just shows an error message; in
    real life, your solution should probably be more sophisticated.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下载URL时，您正在使用网络协议与远程服务器通信。可能会发生许多种错误，例如URL更改、服务器未响应等。这个例子只是显示了一个错误消息；在实际生活中，您的解决方案可能需要更加复杂。
- en: 'Blueprint: Downloading HTML Pages with wget'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用wget下载HTML页面
- en: A good tool for mass downloading pages is [wget](https://oreil.ly/wget), which
    is a command line tool available for almost all platforms. On Linux and macOS, `wget`
    should already be installed or can easily be installed using a package manager.
    On Windows, there is a port available at [*https://oreil.ly/2Nl0b*](https://oreil.ly/2Nl0b).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 用于大规模下载页面的好工具是[wget](https://oreil.ly/wget)，这是一个几乎所有平台都可用的命令行工具。在Linux和macOS上，`wget`应该已经安装或可以通过包管理器轻松安装。在Windows上，可以在[*https://oreil.ly/2Nl0b*](https://oreil.ly/2Nl0b)获取到一个端口。
- en: '`wget` supports lists of URLs for downloads and HTTP keep-alive. Normally,
    each HTTP request needs a separate TCP connection (or a Diffie-Hellman key exchange;
    see [“Tips for Efficient Downloads”](#ch3-tips-for-efficient-downloads)). The
    `-nc` option of `wget` will check whether files have already been downloaded.
    This way, we can avoid downloading content twice. We can now stop the process
    at any time and restart without losing data, which is important if a web server
    blocks us, our Internet connection goes down, etc. Let’s save the list of URLs
    from the last blueprint to a file and use that as a template for downloading:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`wget`支持URL列表进行下载和HTTP保持活动状态。通常，每个HTTP请求需要单独的TCP连接（或Diffie-Hellman密钥交换；参见[“高效下载的技巧”](#ch3-tips-for-efficient-downloads)）。`wget`的`-nc`选项将检查文件是否已经下载过。这样，我们可以避免重复下载内容。现在我们随时可以停止进程并重新启动而不会丢失数据，这在Web服务器阻止我们、互联网连接中断等情况下非常重要。让我们将上一个蓝图中的URL列表保存到文件中，并将其用作下载的模板：'
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now go to your command line (or a terminal tab in Jupyter) and call `wget`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在去你的命令行（或Jupyter中的终端标签）并调用`wget`：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `-i` option tells `wget` the list of URLs to download. It’s fun to see how
    `wget` skips the existing files (due to the `-nc` option) and how fast the downloading
    works.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`-i`选项告诉`wget`要下载的URL列表。看到`wget`由于`-nc`选项跳过现有文件（很有趣）以及下载速度的快慢。'
- en: '`wget` can also be used for recursively downloading websites with the option `-r`.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`wget`也可以用于递归下载网站，使用选项`-r`。'
- en: Danger of Lockout!
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 锁定的危险！
- en: Be careful, this might lead to long-running processes, and eventually you might
    get locked out of the website. It’s often a good idea to combine `-r` with `-l` (recursion
    level) when experimenting with recursive downloads.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要小心，这可能导致长时间运行的进程，最终可能导致您被锁定在网站外。在尝试递归下载时，将`-r`与`-l`（递归级别）结合使用通常是一个好主意。
- en: There are several different ways to download data. For a moderate number of
    pages (like a few hundred to a thousand), a download directly in a Python program
    is the standard way to go. We recommend the `requests` library, as it is easy
    to use.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的下载数据的方式。对于中等数量的页面（如几百到一千页），直接在Python程序中下载是标准的方式。我们推荐使用`requests`库，因为它易于使用。
- en: Downloading more than a few thousand pages normally works better in a multistage
    process by first generating a list of URLs and then downloading them externally
    via a dedicated program like `wget`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下载超过几千页通常最好通过多阶段过程来完成，首先生成URL列表，然后通过专用程序（如`wget`）在外部下载它们。
- en: Extracting Semistructured Data
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取半结构化数据
- en: In the following section, we will explore different methods to extract data
    from Reuters articles. We will start with using regular expressions and then turn
    to a full-fledged HTML parser.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨从路透社文章中提取数据的不同方法。我们将从使用正则表达式开始，然后转向完整的HTML解析器。
- en: 'Eventually we will be interested in the data of more than one article, but
    as a first step we will concentrate on a single one. Let’s take [“Banned in Boston:
    Without vaping, medical marijuana patients must adapt”](https://oreil.ly/jg0Jr)
    as our example.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们将对多篇文章的数据感兴趣，但作为第一步，我们将集中精力在一篇文章上。让我们以[“波士顿禁用：没有电子烟，医用大麻患者必须适应”](https://oreil.ly/jg0Jr)为例。
- en: 'Blueprint: Extracting Data with Regular Expressions'
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用正则表达式提取数据
- en: The browser will be one of the most important tools for dissecting the article.
    Start by opening the URL and using the View Source functionality. In the first
    step, we can see that the title is interesting. Taking a look at the HTML, the
    title is surrounded by both `<title>` and `<h1>`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器将是剖析文章的最重要工具之一。首先打开URL并使用“查看源代码”功能。在第一步中，我们可以看到标题很有趣。查看HTML，标题被`<title>`和`<h1>`包围。
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: HTML Code Changes Over Time
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HTML代码随时间变化
- en: The programs described in this section work with the HTML code that was current
    when the book was written. However, publishers are free to change their website
    structure anytime and even remove content. An alternative is to use the data from
    the [Wayback Machine](https://archive.org). The Reuters website is mirrored there,
    and snapshots are kept that preserve the layout and the HTML structure.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述的程序使用了在撰写本书时当前的HTML代码。但是，出版商可以随时更改其网站结构甚至删除内容。一个替代方法是使用来自[网络档案馆](https://archive.org)的数据。路透社网站在那里被镜像，快照被保留以保持布局和HTML结构。
- en: Also take a look at the GitHub archive of the book. If the layout has changed
    and the programs would not work anymore, alternative links (and sitemaps) will
    be provided there.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 还要查看该书的GitHub存档。如果布局发生了变化，并且程序无法再正常工作，那里将提供替代链接（和网站地图）。
- en: Programmatically, the extraction of the title can be achieved with regular expressions
    without using any other libraries. Let’s first download the article and save it
    to a local file called *us-health-vaping-marijuana-idUSKBN1WG4KT.html*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正则表达式，可以以编程方式提取标题而无需使用其他库。让我们首先下载文章并将其保存到名为*us-health-vaping-marijuana-idUSKBN1WG4KT.html*的本地文件中。
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A Python blueprint for extracting the title might look like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 提取标题的Python蓝图可能如下所示：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Out:`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `re` library is not fully integrated into Python string handling. In other
    words, it cannot be invoked as methods of string. As our HTML documents consist
    of many lines, we have to use `re.MULTILINE|re.DOTALL`. Sometimes cascaded calls
    to `re.search` are necessary, but they do make the code harder to read.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`re`库没有完全整合到Python字符串处理中。换句话说，它不能作为字符串的方法调用。由于我们的HTML文档由许多行组成，因此我们必须使用`re.MULTILINE|re.DOTALL`。有时需要级联调用`re.search`，但这确实使代码难以阅读。'
- en: It is crucial to use `re.search` and not `re.match` in Python, which is different
    than in many other programming languages. The latter tries to match the whole
    string, and as there is data before `<title>` and after `</title>`, it fails.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，使用`re.search`而不是`re.match`至关重要，这与许多其他编程语言不同。后者试图匹配整个字符串，并且由于在`<title>`之前和`</title>`之后有数据，它会失败。
- en: 'Blueprint: Using an HTML Parser for Extraction'
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用HTML解析器进行提取
- en: The article has more interesting parts that are tedious to extract with regular
    expressions. There’s text in the article, a publication date is associated with
    it, and the authors are named. This is much easier to accomplish with an HTML
    parser.^([6](ch03.xhtml#idm45634206104264)) Fortunately, with the Python package
    called [Beautiful Soup](https://oreil.ly/I2VJh), we have an extremely powerful
    library for handling this. If you don’t have Beautiful Soup installed, install
    it now with `pip install bs4` or `conda install bs4`. Beautiful Soup is tolerant
    and can also parse “bad” HTML that is often found on sloppily managed websites.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 文章还有更多有趣的部分，使用正则表达式提取起来很繁琐。在文章中有文本，与之相关的出版日期以及作者的名称。使用HTML解析器（^([6](ch03.xhtml#idm45634206104264))）可以更容易地实现这一点。幸运的是，Python包Beautiful
    Soup可以很好地处理这些任务。如果尚未安装Beautiful Soup，请使用`pip install bs4`或`conda install bs4`进行安装。Beautiful
    Soup很宽容，也可以解析通常在管理不善的网站上找到的“不良”HTML。
- en: The next sections make use of the fact that all articles have the same structure
    in the news archive. Fortunately, this is true for most big websites as the pages
    are not hand-crafted but rather generated by a content management system from
    a database.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节利用了新闻档案中所有文章具有相同的结构这一事实。幸运的是，这对大多数大型网站来说是真实的，因为这些页面不是手工制作的，而是由内容管理系统从数据库生成的。
- en: Extracting the title/headline
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取标题/头条
- en: Selecting content in Beautiful Soup uses so-called selectors that need to be
    given in the Python program. Finding them is a bit tricky, but there are structural
    approaches for that. Almost all modern browsers support a Web Inspector, which
    is useful for finding the CSS selectors. Open the Web Inspector in the browser
    (most commonly achieved by pressing F12) when the article is loaded, and click
    the Web Inspector icon, as shown in [Figure 3-3](#web_inspector_icon_in_chrome_browser).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在Beautiful Soup中选择内容使用所谓的选择器，在Python程序中需要提供这些选择器。找到它们有些棘手，但有结构化的方法可以解决。几乎所有现代浏览器都支持Web
    Inspector，用于查找CSS选择器。在加载文章时在浏览器中打开Web Inspector（通常按F12键即可），然后单击Web Inspector图标，如[图 3-3](#web_inspector_icon_in_chrome_browser)所示。
- en: '![](Images/btap_0303.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0303.jpg)'
- en: Figure 3-3\. Web Inspector icon in the Chrome browser.
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. Chrome浏览器中的Web Inspector图标。
- en: Hover over the headline and you will see the corresponding element highlighted,
    as shown in [Figure 3-4](#screenshot_of_the_chrome_browser_with_the_web_inspector).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 悬停在标题上，您将看到相应的元素突出显示，如[图 3-4](#screenshot_of_the_chrome_browser_with_the_web_inspector)所示。
- en: '![](Images/btap_0304.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0304.jpg)'
- en: Figure 3-4\. Chrome browser using the Web Inspector.
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 使用Web Inspector的Chrome浏览器。
- en: 'Clicking the headline to show it in the Web Inspector. It should look like
    this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 单击标题以在Web Inspector中显示它。它应该看起来像这样：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Using CSS notation,^([7](ch03.xhtml#idm45634206031320)) this element can be
    selected with `h1.ArticleHeader_headline`. Beautiful Soup understands that:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CSS表示法，^([7](ch03.xhtml#idm45634206031320))可以通过`h1.ArticleHeader_headline`选择此元素。Beautiful
    Soup理解到：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Out:`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Beautiful Soup makes it even easier and lets us use the tag names directly:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup使得这更加简单，让我们可以直接使用标签名：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`Out:`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Normally, the most interesting part of the previous HTML fragment is the real
    text without the HTML clutter around it. Beautiful Soup can extract that:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，前一个HTML片段中最有趣的部分是没有HTML混杂物围绕的真实文本。Beautiful Soup可以提取这部分内容：
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Out:`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that in contrast to the regular expression solution, unnecessary whitespaces
    have been stripped by Beautiful Soup.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与正则表达式解决方案相比，Beautiful Soup已经去除了不必要的空格。
- en: 'Unfortunately, that does not work as well for the title:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这对标题的效果不太好：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we would need to manually strip the data and eliminate the `- Reuters`
    suffix.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要手动剥离数据并消除`- Reuters`后缀。
- en: Extracting the article text
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取文章正文
- en: 'In a similar way to the previously described procedure for finding the headline
    selector, you can easily find the text content at the selector `div.StandardArticleBody_body`.
    When using `select`, Beautiful Soup returns a list. Often it is clear from the
    underlying HTML structure that the list consists of only one item or we are interested
    only in the first element. We can use the convenience method `select_one` here:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前描述的查找标题选择器的过程类似，您可以轻松找到选择器`div.StandardArticleBody_body`中的文本内容。在使用`select`时，Beautiful
    Soup返回一个列表。从底层的HTML结构可以明显看出，该列表仅包含一个项目，或者我们只对第一个元素感兴趣。在这里我们可以使用方便的方法`select_one`：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Extracting image captions
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取图像标题
- en: 'But wait, apart from the text, this part also contains images with captions
    that might be relevant separately. So again, use the Web Inspector to hover over
    the images and find the corresponding CSS selectors. All images are contained
    in `<figure>` elements, so let’s select them:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请注意，除了文本之外，此部分还包含带有可能单独相关的标题的图片。因此，再次使用 Web Inspector 悬停在图片上并找到相应的 CSS 选择器。所有图片都包含在
    `<figure>` 元素中，因此让我们选择它们：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`Out:`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Inspecting the result closely, this code contains only one image, whereas the
    browser displays many images. This is a pattern that can often be found in web
    pages. Code for the images is not in the page itself but is added later by client-side
    JavaScript. Technically this is possible, although it is not the best style. From
    a content perspective, it would be better if the image source were contained in
    the original server-generated page and made visible by CSS later. This would also
    help our extraction process.  Anyway, we are more interested in the caption of
    the image, so the correct selector would be to replace `img` with `figcaption`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细检查结果，此代码仅包含一个图像，而浏览器显示许多图像。这是在网页中经常可以找到的一种模式。图像的代码不在页面本身中，而是由客户端 JavaScript
    后添加。技术上这是可能的，尽管不是最佳的风格。从内容的角度来看，如果图像源包含在原始生成的服务器页面中，并通过 CSS 后来可见，那将更好。这也将有助于我们的提取过程。总之，我们更感兴趣的是图像的标题，因此正确的选择器应该是将
    `img` 替换为 `figcaption`。
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Out:`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Extracting the URL
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取 URL
- en: 'When downloading many HTML files, it is often difficult to find the original
    URLs of the files if they have not been saved separately. Moreover, URLs might
    change, and normally it is best to use the standard (called *canonical*) URL.
    Fortunately, there is an HTML tag called `<link rel="canonical">` that can be
    used for this purpose. The tag is not mandatory, but it is extremely common, as
    it is also taken into account by search engines and contributes to a good ranking:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载许多 HTML 文件时，如果它们未单独保存，通常很难找到文件的原始 URL。此外，URL 可能会更改，通常最好使用标准的（称为 *canonical*）URL。幸运的是，有一个
    HTML 标签称为 `<link rel="canonical">`，可以用于此目的。该标签不是强制性的，但它非常常见，因为搜索引擎也会考虑它，有助于良好的排名：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`Out:`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Extracting list information (authors)
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取列表信息（作者）
- en: Taking a look at the source code, the author of the article is mentioned in
    a `<meta name="Author">` tag.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 查看源代码，文章的作者在 `<meta name="Author">` 标签中提到。
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`Out:`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'However, this returns only one author. Reading the text, there is another author,
    which is unfortunately not contained in the meta-information of the page. Of course,
    it can be extracted again by selecting the elements in the browser and using the
    CSS selector:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只返回了一个作者。阅读文本，还有另一个作者，但不幸的是，这并未包含在页面的元信息中。当然，可以通过在浏览器中选择元素并使用 CSS 选择器再次提取：
- en: '[PRE39]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`Out:`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE40]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Extracting the author names is then straightforward:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提取作者姓名非常直接：
- en: '[PRE41]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Out:`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE42]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Semantic and nonsemantic content
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义和非语义内容
- en: In contrast to the previous examples, the `sel` selector is not *semantic*.
    Selection is performed based on layout-like classes. This works well for the moment
    but is likely to break if the layout is changed. Therefore, it’s a good idea to
    avoid these kinds of selections if the code is likely to be executed not only
    once or in a batch but should also run in the future.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的例子相比，`sel` 选择器不是 *语义* 的。选择是基于布局类似的类。目前这种方式效果不错，但如果布局改变，很可能会出现问题。因此，如果代码不仅仅是一次性或批处理运行，而且将来还应该运行，则最好避免这些类型的选择。
- en: Extracting text of links (section)
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取链接的文本（章节）
- en: 'The section is easy to extract. Using the Web Inspector again, we can find
    that the CSS selector is the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分很容易提取。再次使用 Web Inspector，我们可以找到以下 CSS 选择器：
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`Out:`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE44]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Extracting reading time
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取阅读时间
- en: 'Reading time can be found easily via the Web Inspector:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Web Inspector 可以轻松找到阅读时间：
- en: '[PRE45]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`Out:`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE46]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Extracting attributes (ID)
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取属性（ID）
- en: 'Having a primary key that uniquely identifies an article is helpful. The ID
    is also present in the URL, but there might be some heuristics and advanced splitting
    necessary to find it. Using the browser’s View Source functionality and searching
    for this ID, we see that it is the `id` attribute of the article container:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有唯一标识文章的主键是有帮助的。ID 也出现在 URL 中，但可能需要一些启发式和高级分割才能找到它。使用浏览器的查看源代码功能并搜索此 ID，我们看到它是文章容器的
    `id` 属性：
- en: '[PRE47]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`Out:`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE48]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Extracting attribution
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取归属信息
- en: 'Apart from the authors, the article carries more attributions. They can be
    found at the end of the text and reside in a special container:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作者外，文章还有更多的归属。它们可以在文本末尾找到，并放置在一个特殊的容器中：
- en: '[PRE49]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`Out:`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE50]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Extracting timestamp
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取时间戳
- en: 'For many statistical purposes, it is crucial to know the time that the article
    was posted. This is mentioned next to the section, but unfortunately it is constructed
    to be human-readable (like “3 days ago”). This can be parsed but is tedious. Knowing
    the real publishing time, the correct element can be found in the HTML head element:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多统计目的来说，知道文章发布的时间非常关键。这通常在部分旁边提到，但不幸的是它是以人类可读的方式构建的（如“3天前”）。这可以被解析，但很繁琐。知道真实的发布时间后，可以在HTML头元素中找到正确的元素：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`Out:`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE52]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'A string is already helpful (especially in this notation, as we will see later),
    but Python offers facilities to convert that to a datetime object easily:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字符串已经很有帮助了（特别是在后面我们将看到的表示法中），但Python提供了将其轻松转换为日期时间对象的功能：
- en: '[PRE53]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`Out:`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE54]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The same can be done for `modified_time` instead of `published_time`, if that
    is more relevant.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果更相关的话，也可以对`modified_time`而不是`published_time`执行相同操作。
- en: Use regular expressions only for crude extraction. An HTML parser is slower
    but much easier to use and more stable.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用正则表达式进行粗略提取。HTML解析器速度较慢，但使用起来更加简单且更稳定。
- en: Often, it makes sense to take a look at the semantic structure of the documents
    and use HTML tags that have semantic class names to find the value of structural
    elements. These tags have the advantage that they are the same over a large class
    of web pages. Extraction of their content therefore has to be implemented only
    once and can be reused.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，查看文档的语义结构并使用具有语义类名的HTML标签是有意义的，以找到结构元素的值。这些标签的优势在于它们在大量网页上都是相同的。因此，只需要实现一次其内容的提取，就可以重复使用。
- en: Apart from extremely simple cases, try to use an HTML parser whenever possible.
    Some standard structures that can be found in almost any HTML document are discussed
    in the following sidebar.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 除了极端简单的情况外，尽量在可能的情况下使用HTML解析器。以下侧栏讨论了几乎在任何HTML文档中都可以找到的一些标准结构。
- en: 'Blueprint: Spidering'
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：爬虫
- en: So far we have taken a look at how we can download web pages and extract the
    content using HTML parsing techniques. From a business perspective, looking at
    single pages is often not so interesting, butyou want to see the whole picture.
    For this, you need much more content.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过如何下载网页并使用HTML解析技术提取内容。从业务角度来看，查看单个页面通常并不那么有趣，但是你想要看到整体图片。为此，你需要更多的内容。
- en: Fortunately, our acquired knowledge can be combined to download content archives
    or whole websites. This is often a multistage process where you need to generate
    URLs first, download the content, find more URLs, and so on.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们掌握的知识可以结合起来下载内容档案或整个网站。这通常是一个多阶段的过程，首先需要生成URL，下载内容，找到更多URL，依此类推。
- en: This section explains one of these “spidering” examples in detail and creates
    a scalable blueprint that can be used for downloading thousands (or millions)
    of pages.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细解释了其中一个这样的“爬虫”示例，并创建了一个可扩展的蓝图，可用于下载成千上万（甚至百万）页。
- en: Introducing the Use Case
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入使用案例
- en: Parsing a single Reuters article is a nice exercise, but the Reuters archive
    is much larger and contains many articles.  It is also possible to use the techniques
    we have covered to parse a larger amount. Imagine that you want to download and
    extract, for example, a whole forum with user-generated content or a website with
    scientific articles. As mentioned previously, it is often most difficult to find
    the correct URLs of the articles.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 解析单个路透社文章是一个不错的练习，但路透社档案规模更大，包含许多文章。也可以使用我们已经涵盖的技术来解析更多内容。想象一下，你想要下载和提取，例如，一个带有用户生成内容的整个论坛或一个包含科学文章的网站。正如之前提到的，通常最难找到正确的文章URL。
- en: Not in this case, though. It would be possible to use *sitemap.xml*, but Reuters
    is generous enough to offer a dedicated archive page at [*https://www.reuters.com/news/archive*](https://www.reuters.com/news/archive/).
    A paging functionality is also available, so it’s possible to go backward in time.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 不过在这种情况下并非如此。可以使用*sitemap.xml*，但路透社慷慨地提供了一个专门的存档页面[*https://www.reuters.com/news/archive*](https://www.reuters.com/news/archive/)。还提供了分页功能，因此可以向前回溯时间。
- en: '[Figure 3-5](#flowchart_for_spidering_process) shows the steps for downloading
    part of the archive (called *spidering*). The process works as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](#flowchart_for_spidering_process) 展示了下载存档部分（称为*爬虫*）的步骤。该过程如下：'
- en: Define how many pages of the archive should be downloaded.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义应下载存档的页数。
- en: Download each page of the archive into a file called *page-000001.html*, *page-000002.html*,
    and so on for easier inspection. Skip this step if the file is already present.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将存档的每一页下载到名为*page-000001.html*、*page-000002.html*等文件中，以便更轻松地检查。如果文件已经存在，则跳过此步骤。
- en: For each *page-*.html* file, extract the URLs of the referenced articles.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个*page-*.html*文件，提取引用文章的URL。
- en: For each article URL, download the article into a local HTML file. Skip this
    step if the article file is already present.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文章的网址，将文章下载到本地的HTML文件中。如果文章文件已经存在，则跳过此步骤。
- en: For each article file, extract the content into a `dict` and combine these `dict`s
    into a Pandas `DataFrame`.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文章文件，提取内容到一个`dict`中，并将这些`dict`组合成一个Pandas的`DataFrame`。
- en: '![](Images/btap_0305.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0305.jpg)'
- en: Figure 3-5\. Flowchart for spidering process.
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 爬虫流程图。
- en: In a more generic approach, it might be necessary to create intermediate URLs
    in step 3 (if there is an overview page for years, months, etc.) before we finally
    arrive at the article URLs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在更通用的方法中，可能需要在步骤3中创建中间网址（如果有年份、月份等的概述页面）才能最终到达文章网址。
- en: The procedure is constructed in a way that each step can be run individually
    and downloads have to be performed only once. This has proven to be useful, especially
    when we have to extract a large number of articles/URLs, as a single missing download
    or malformed HTML page does not mean that the whole procedure including downloading
    has to be started again. Moreover, the process can be restarted anytime and downloads
    only data that has not yet been downloaded. This is called *idempotence* and is
    often a useful concept when interacting with “expensive” APIs.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的构造方式使得每个步骤都可以单独运行，并且只需要执行一次下载。这被证明非常有用，特别是当我们需要提取大量文章/网址时，因为单个缺失的下载或格式不正确的HTML页面并不意味着必须重新开始整个下载过程。此外，该过程随时可以重新启动，并且仅下载尚未下载的数据。这称为*幂等性*，在与“昂贵”的API交互时经常是一个有用的概念。
- en: 'The finished program looks like this:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的程序看起来是这样的：
- en: '[PRE55]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Having defined these functions, they can be invoked with parameters (which
    can easily be changed):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这些函数后，它们可以用参数调用（这些参数可以很容易地更改）：
- en: '[PRE56]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Error Handling and Production-Quality Software
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误处理和生产质量软件
- en: For simplicity, all example programs discussed in this chapter do not use error
    handling. For production software, however, you should use exception handling.
    As HTML can change frequently and pages might be incomplete, errors can happen
    at any time, so it is a good idea to use try/except generously and log the errors.
    If systematic errors occur, you should look for the root cause and eliminate it.
    If errors occur only sporadically or due to malformed HTML, you can probably ignore
    them, as they might also be due to server software.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，本章讨论的所有示例程序都不使用错误处理。然而，对于生产软件，应使用异常处理。由于HTML可能会经常变化且页面可能不完整，错误可能随时发生，因此大量使用try/except并记录错误是一个好主意。如果出现系统性错误，应找出根本原因并消除它。如果错误只是偶尔发生或由于格式不正确的HTML导致的，您可能可以忽略它们，因为这可能也是由服务器软件引起的。
- en: Using the download and save file mechanism described earlier, the extraction
    procedure can be restarted anytime or also be applied to certain problematic files
    separately. This is often a big advantage and helps to achieve a cleanly extracted
    dataset fast.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面描述的下载和保存文件机制，提取过程随时可以重新启动，也可以单独应用于某些有问题的文件。这通常是一个很大的优势，有助于快速获得干净的提取数据集。
- en: Generating URLs is often as difficult as extracting content and is frequently
    related to it. In many cases, this has to be repeated several times to download,
    for example, hierarchical content.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 生成URL通常与提取内容一样困难，并且经常与之相关。在许多情况下，这必须重复多次以下载例如分层内容。
- en: When you download data, always find a filename for each URL and save it to the
    filesystem. You will have to restart the process more often than you think. Not
    having to download everything over and over is immensely useful, especially during
    the development process.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载数据时，始终为每个网址找到一个文件名并保存到文件系统中。你将不得不比你想象中更频繁地重新启动这个过程。避免反复下载所有内容非常有用，特别是在开发过程中。
- en: If you have downloaded and extracted the data, you will probably want to persist
    it for later use. An easy way is to save it in individual JSON files. If you have
    many files, using a directory structure might be a good option. With an increasing
    number of pages, even this might not scale well, and it’s a better idea to use
    a database or another columnar data store.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经下载并提取了数据，您可能希望将其持久化以供以后使用。一种简单的方法是将其保存在单独的JSON文件中。如果您有很多文件，使用目录结构可能是一个不错的选择。随着页面数量的增加，即使这样也可能扩展性不佳，最好使用数据库或其他列存储数据存储解决方案。
- en: Density-Based Text Extraction
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密度基础文本提取
- en: Extracting structured data from HTML is not complicated, but it is tedious.
    If you want to extract data from a whole website, it is well worth the effort
    as you only have to implement the extraction for a limited number of page types.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从HTML中提取结构化数据并不复杂，但很繁琐。如果您想从整个网站提取数据，那么值得付出努力，因为您只需针对有限数量的页面类型实施提取。
- en: However, you may need to extract text from many different websites. Implementing
    the extraction for each of them does not scale well. There is some metadata that
    can be found easily, such as title, description, etc. But the text itself is not
    so easy to find.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您可能需要从许多不同的网站提取文本。针对每个网站实施提取并不具有良好的可扩展性。有一些元数据可以很容易找到，比如标题、描述等。但是文本本身并不那么容易找到。
- en: Taking a look at the information density, there are some heuristics that allow
    extraction of the text. The algorithm behind it measures the *density of information* and
    therefore automatically eliminates repeated information such as headers, navigation,
    footers, and so on. The implementation is not so simple but is fortunately available
    in a library called [`python-readability`](https://oreil.ly/AemZh). The name originates
    from a now-orphaned browser plugin called Readability, which was conceived to
    remove clutter from web pages and make them easily readable—exactly what is needed
    here. To get started, we must first install `python-readability` (**`pip install
    readability-lxml`**).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从信息密度的角度来看，有一些启发式方法允许提取文本。其背后的算法测量了*信息密度*，因此自动消除了重复信息，如标题、导航、页脚等。实施起来并不简单，但幸运的是在名为[`python-readability`](https://oreil.ly/AemZh)的库中已经提供了。其名称源自一个现在已经被废弃的浏览器插件Readability，它的构想是从网页中移除混乱内容，使其易于阅读，这正是这里所需要的。要开始使用，我们首先必须安装`python-readability`（**`pip
    install readability-lxml`**）。
- en: Extracting Reuters Content with Readability
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Readability提取路透社内容
- en: 'Let’s see how this works in the Reuters example. We keep the HTML we have downloaded,
    but of course you can also use a file or URL:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在路透社示例中是如何工作的。我们保留已下载的HTML，当然您也可以使用文件或URL：
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`Out:`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE58]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'As you can see, that was easy. The title can be extracted via the corresponding
    element. However, the library can do some additional tricks, such as finding the
    title or the summary of the page:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这很容易。可以通过相应元素提取标题。但是，该库还可以执行一些附加技巧，例如查找页面的标题或摘要：
- en: '[PRE59]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`Out:`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE60]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'That is already quite good. Let’s check how well it works for the actual content:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 那已经相当不错了。让我们来看看它在实际内容中的表现如何：
- en: '[PRE61]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`Out:`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE62]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The data still has some remaining HTML structure, which can be useful to keep
    because paragraphs are included. Of course, the body part can be extracted again
    with Beautiful Soup:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仍然保留了一些HTML结构，这对于包含段落的部分很有用。当然，可以再次使用Beautiful Soup提取正文部分：
- en: '[PRE63]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`Out:`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE64]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In this case, the results are excellent. In most of the cases, `python-readability`
    works reasonably well and removes the need to implement too many special cases.
    However, the cost of using this library is uncertainty. Will it always work in
    the expected way with the impossibility of extracting structured data such as
    timestamps, authors, and so on (although there might be other heuristics for that)?
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，结果是非常好的。在大多数情况下，`python-readability`表现得相当不错，并且避免了实施过多特殊情况的需要。然而，使用此库的成本是不确定性。它是否总是按预期方式工作，例如无法提取结构化数据（如时间戳、作者等）（尽管可能存在其他启发式方法）？
- en: Summary Density-Based Text Extraction
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要 密度基础文本提取
- en: Density-based text extraction is powerful when using both heuristics and statistical
    information about information distribution on an HTML page. You should keep in
    mind that the results are almost always worse when compared to implementing a
    specific extractor. However, if you need to extract content from many different
    page types or from an archive where you don’t have a fixed layout at all, it might
    well be worth it to go that way.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的文本提取在使用启发式和关于HTML页面信息分布的统计信息时非常强大。请记住，与实施特定提取器相比，结果几乎总是更差。但是，如果您需要从许多不同类型的页面或者您根本没有固定布局的存档中提取内容，那么这种方法可能是值得一试的。
- en: Performing a detailed quality assurance afterward is even more essential compared
    to the structured approach as both the heuristics and the statistics might sometimes
    go in the wrong direction.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 与结构化方法相比，执行详细的质量保证工作更加重要，因为启发式和统计方法有时可能会走错方向。
- en: All-in-One Approach
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一体化方法
- en: '[*Scrapy*](https://scrapy.org) is another Python package that offers an all-in-one
    approach to spidering and content extraction. The methods are similar to the ones
    described in the earlier sections, although Scrapy is more suited for downloading *whole* websites
    and not only parts of them.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Scrapy*](https://scrapy.org) 是另一个 Python 包，提供了一体化的爬虫和内容提取方法。其方法与前文描述的方法类似，尽管
    Scrapy 更适合下载整个网站，而不仅仅是其中的一部分。'
- en: The object-oriented, holistic approach of Scrapy is definitely nice, and the
    code is readable. However, it turns out to be quite difficult to restart spidering
    and extraction without having to download the whole website again.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 的面向对象、整体方法确实很好，并且代码易读。但是，重新启动爬虫和提取而不必重新下载整个网站确实非常困难。
- en: Compared to the approach described earlier, downloading must also happen in
    Python. For websites with a huge number of pages, HTTP keep-alive cannot be used,
    and gzip encoding is also difficult. Both can be easily integrated in the modular
    method by externalizing the downloads via tools such as wget.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 与前文描述的方法相比，下载还必须在 Python 中进行。对于页面数量庞大的网站，无法使用 HTTP keep-alive，并且 gzip 编码也很困难。这两者可以通过像 wget 这样的工具在模块化方法中轻松集成外部下载。
- en: 'Blueprint: Scraping the Reuters Archive with Scrapy'
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用 Scrapy 爬取路透社存档
- en: Let’s see how the download of the archive and the articles would look in Scrapy.
    Go ahead and install Scrapy (either via **`conda install scrapy`** or **`pip install
    scrapy`**).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 Scrapy 中下载存档和文章。请继续安装 Scrapy（可以通过 **`conda install scrapy`** 或 **`pip
    install scrapy`** 进行安装）。
- en: '[PRE65]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Scrapy works in an object-oriented way. For each so-called spider, a class needs
    to be implemented that is derived from `scrapy.Spider`. Scrapy adds a lot of debug
    output, which is reduced in the previous example by `logging.WARNING`. The base
    class automatically calls the parse function with the `start_urls`. This function
    extracts the links to the article and invokes `yield` with the function `parse_article`
    as a parameter. This function in turn extracts some attributes from the articles
    and yields them in a `dict`. Finally, the next page link is crawled, but we stop
    here before getting the second page.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 以面向对象的方式工作。对于每个所谓的 spider，都需要实现一个从 `scrapy.Spider` 派生的类。Scrapy 添加了大量的调试输出，在上一个示例中通过
    `logging.WARNING` 进行了减少。基类自动使用 `start_urls` 调用解析函数 `parse`。该函数提取文章的链接并调用 `parse_article`
    函数进行 `yield`。该函数又从文章中提取一些属性并以 `dict` 形式 `yield` 返回。最后，爬取下一页的链接，但在获取第二页之前停止。
- en: '`yield` has a double functionality in Scrapy. If a `dict` is yielded, it is
    added to the results. If a `Request` object is yielded, the object is fetched
    and gets parsed.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`yield` 在 Scrapy 中有双重功能。如果 `yield` 一个 `dict`，它将被添加到结果中。如果 `yield` 一个 `Request`
    对象，则会获取并解析该对象。'
- en: Scrapy and Jupyter
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrapy 和 Jupyter
- en: 'Scrapy is optimized for command-line usage, but it can also be invoked in a
    Jupyter notebook. Because of Scrapy’s usage of the (ancient) [Twisted environment](https://oreil.ly/j6HCm),
    the scraping cannot be restarted, so you have only one shot if you try it in the
    notebook (otherwise you have to restart the notebook):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 优化用于命令行使用，但也可以在 Jupyter 笔记本中调用。由于 Scrapy 使用了（古老的） [Twisted 环境](https://oreil.ly/j6HCm)，所以在笔记本中无法重新启动爬取，因此您只有一次机会（否则必须重新启动笔记本）：
- en: '[PRE66]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here are a few things worth mentioning:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些值得一提的事情：
- en: The all-in-one approach looks elegant and concise.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一体化方法看起来既优雅又简洁。
- en: As most of the coding is spent in extracting data in the articles, this code
    has to change frequently. For this, spidering has to be restarted (and if you
    are running the script in Jupyter, you also have to start the Jupyter notebook
    server), which tremendously increases turnaround times.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于大部分编码都花在提取文章数据上，这些代码必须经常更改。为此，必须重新启动爬虫（如果在 Jupyter 中运行脚本，则还必须启动 Jupyter 笔记本服务器），这极大地增加了周转时间。
- en: It’s nice that JSON can directly be produced. Be careful as the JSON file is
    appended, which can result in an invalid JSON if you don’t delete the file before
    starting the spidering process. This can be solved by using the so-called jl format
    (JSON lines), but it is a workaround.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON 可以直接生成是件好事。请注意，由于 JSON 文件是追加的，如果在启动爬虫进程之前不删除文件，可能会导致无效的 JSON。这可以通过使用所谓的
    jl 格式（JSON 行）来解决，但这只是一个变通方法。
- en: Scrapy has some nice ideas. In our day-to-day work, we do not use it, mainly
    because debugging is hard. If persistence of the HTML files is needed (which we
    strongly suggest), it loses lots of advantages. The object-oriented approach is
    useful and can be implemented outside of Scrapy without too much effort.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scrapy 提出了一些不错的想法。在我们的日常工作中，我们并不使用它，主要是因为调试很困难。如果需要持久化 HTML 文件（我们强烈建议这样做），它会失去很多优势。面向对象的方法很有用，可以在不用太多精力的情况下在
    Scrapy 之外实现。
- en: As Scrapy also uses CSS selectors for extracting HTML content, the basic technologies
    are the same as with the other approaches. There are considerable differences
    in the downloading method, though. Having Twisted as a backend creates some overhead
    and imposes a special programming model.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Scrapy 也使用 CSS 选择器来提取 HTML 内容，基本技术与其他方法相同。不过，在下载方法上有相当大的差异。由于 Twisted 作为后端，会产生一些额外开销，并施加特殊的编程模型。
- en: Decide carefully whether an all-in-one approach suits your project needs. For
    some websites, ready-made Scrapy spiders might already be available and can be
    reused.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑是否适合您项目需求的一体化方法。对于某些网站，可能已经有现成的 Scrapy 爬虫可供使用和重用。
- en: Possible Problems with Scraping
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取可能遇到的问题
- en: Before scraping content, it is always worthwhile to consider possible copyright
    and data protection issues.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬取内容之前，考虑可能的版权和数据保护问题总是值得的。
- en: More and more web applications are constructed using frameworks like [React](https://reactjs.org).
    They have only a single page, and data is transferred via an API. This often leads
    to websites not working without JavaScript. Sometimes there are specialized URLs
    constructed for search engines that are also useful for spidering. Usually, those
    can be found in *sitemap.xml*. You can try it by switching off JavaScript in your
    browser and then see whether the website still works.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的 Web 应用程序使用像 [React](https://reactjs.org) 这样的框架构建。它们只有一个单页面，并通过 API 传输数据。这通常导致禁用
    JavaScript 后网站无法工作。有时，专门为搜索引擎构建的特定 URL 对于爬取也很有用。通常可以在 *sitemap.xml* 中找到这些内容。您可以尝试在浏览器中关闭
    JavaScript，然后查看网站是否仍然可用。
- en: If JavaScript is needed, you can find requests on the Network tab by using the
    Web Inspector of the browser and clicking around the application. Sometimes, JSON
    is used to transfer the data, which makes extraction often much easier compared
    to HTML. However, the individual JSON URLs still have to be generated, and there
    might be additional parameters to avoid [cross-site request forgery (CSRF)](https://oreil.ly/_6O_Q).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要 JavaScript，可以通过使用浏览器的 Web Inspector 在 Network 标签中查找请求，并在应用程序中点击。有时，JSON
    用于数据传输，这使得与 HTML 相比提取通常更加容易。但是，仍然需要生成单独的 JSON URL，并可能有额外的参数以避免[跨站请求伪造（CSRF）](https://oreil.ly/_6O_Q)。
- en: Requests can become quite complicated, such as in the Facebook timeline, on
    Instagram, or on Twitter. Obviously, these websites try to keep their content
    for themselves and avoid spidering.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 请求可能会变得非常复杂，比如在 Facebook 时间线上，Instagram 或 Twitter 上。显然，这些网站试图保留他们的内容，避免被爬取。
- en: For complicated cases, it can be useful to “remote control” the browser by using [Selenium](https://oreil.ly/YssLD),
    a framework that was originally conceived for the automated testing of web applications,
    or a [headless browser](https://oreil.ly/CH2ZI).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂情况，通过使用 [Selenium](https://oreil.ly/YssLD)（一个最初用于自动化测试 Web 应用程序的框架）或者 [无头浏览器](https://oreil.ly/CH2ZI)
    可以“远程控制”浏览器可能很有用。
- en: Websites like Google try to detect automatic download attempts and start sending
    captchas. This can also happen with other websites. Most of the time this is bound
    to certain IP addresses. The website must then be “unlocked” with a normal browser,
    and the automatic requests should be sent with larger pauses between them.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 像Google这样的网站会尝试检测自动下载尝试并开始发送验证码。其他网站也可能会发生这种情况。大多数情况下，这与特定的IP地址绑定在一起。然后必须使用正常的浏览器“解锁”网站，并且自动请求之间应该发送较长的间隔。
- en: Another method to avoid content extraction is obfuscated HTML code where CSS
    classes have totally random names. If the names do not change, this is more work
    initially to find the correct selectors but should work automatically afterward.
    If the names change every day (for example), content extraction becomes extremely
    difficult.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 避免内容提取的另一种方法是使用混淆的HTML代码，其中CSS类的名称完全是随机的。如果名称不变，最初找到正确的选择器可能会更费力，但之后应该会自动运行。如果名称每天都更改（例如），内容提取将变得非常困难。
- en: Closing Remarks and Recommendation
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与建议
- en: Web scraping is a powerful and scalable technique to acquire content. The necessary
    Python infrastructure supports scraping projects in an excellent way. The combination
    of the requests library and Beautiful Soup is comfortable and works well for moderately
    large scraping jobs.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 网络抓取是一种强大且可扩展的获取内容的技术。必要的Python基础设施以非常出色的方式支持抓取项目。请求库和Beautiful Soup的组合很舒适，对于中等规模的抓取工作效果很好。
- en: As we have seen throughout the chapter, we can systematically split up large
    scraping projects into URL generation and downloading phases. If the number of
    documents becomes really big, external tools like `wget` might be more appropriate
    compared to requests. As soon as everything is downloaded, Beautiful Soup can
    be used to extract the content.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在整章中所看到的，我们可以将大型抓取项目系统地分解为URL生成和下载阶段。如果文档数量变得非常大，那么与请求相比，外部工具如`wget`可能更合适。一旦所有内容都被下载，就可以使用Beautiful
    Soup来提取内容。
- en: If you want to minimize waiting time, all stages can be run in parallel.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果想要最小化等待时间，所有阶段都可以并行运行。
- en: In any case, you should be aware of the legal aspects and behave as an “ethical
    scraper” by respecting the rules in *robots.txt*.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，你都应该了解法律方面的问题，并且要表现得像一个“道德的抓取者”，尊重*robots.txt*中的规则。
- en: ^([1](ch03.xhtml#idm45634207731272-marker)) Reuters is a news website and changes
    daily. Therefore, expect completely different results when running the code!
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm45634207731272-marker)) 路透社是一个新闻网站，每天都在变化。因此，运行代码时会得到完全不同的结果！
- en: ^([2](ch03.xhtml#idm45634208553592-marker)) You might have to install the package
    first with **`pip install xmltodict`**.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.xhtml#idm45634208553592-marker)) 你可能需要先用**`pip install xmltodict`**安装该包。
- en: ^([3](ch03.xhtml#idm45634212104584-marker)) Reuters is a news site, and the
    content is continually updated. Note that your results will definitely be different!
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.xhtml#idm45634212104584-marker)) 路透社是一个新闻网站，内容不断更新。请注意，你的结果肯定会有所不同！
- en: ^([4](ch03.xhtml#idm45634206884968-marker)) Just after the time of writing,
    Reuters stopped providing RSS feeds, which led to a public outcry. We hope that
    RSS feeds will be restored. The Jupyter notebook for this chapter [on GitHub](https://oreil.ly/Wamlu)
    uses an archived version of the RSS feed from the Internet archive.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.xhtml#idm45634206884968-marker)) 就在撰写本文的时候，路透社停止提供RSS源，引发了公众的强烈抗议。我们希望RSS源会得到恢复。本章的Jupyter笔记本
    [在GitHub上](https://oreil.ly/Wamlu) 使用了来自互联网档案馆的RSS源的存档版本。
- en: ^([5](ch03.xhtml#idm45634206875096-marker)) As stated previously, Reuters is
    a dynamically generated website, and your results will be different!
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch03.xhtml#idm45634206875096-marker)) 正如之前所述，路透社是一个动态生成的网站，你的结果会有所不同！
- en: ^([6](ch03.xhtml#idm45634206104264-marker)) HTML cannot be parsed with [regular
    expressions](https://oreil.ly/EeCjy).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch03.xhtml#idm45634206104264-marker)) HTML不能用[正则表达式](https://oreil.ly/EeCjy)解析。
- en: '^([7](ch03.xhtml#idm45634206031320-marker)) See *CSS: The Definitive Guide,
    4th Edition* by Eric A. Meyer and Estelle Weyl (O’Reilly, 2017)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch03.xhtml#idm45634206031320-marker)) 参见Eric A. Meyer和Estelle Weyl的*CSS：权威指南，第4版*（O'Reilly，2017）
