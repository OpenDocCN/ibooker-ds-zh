- en: Part 3\. Where to go from here
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三部分。从这里走向何方
- en: '[Part 3](#part03) explores a selection of practical use cases and other areas
    where you can apply what you’ve learned about GANs and their implementations in
    [Parts 1](../Text/kindle_split_009.xhtml#part01) and [2](../Text/kindle_split_014.xhtml#part02):'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第三部分](#part03) 探索了你可以应用你在[第一部分](../Text/kindle_split_009.xhtml#part01)和[第二部分](../Text/kindle_split_014.xhtml#part02)中学到的关于GANs及其实现的一些实际用例和其他领域：'
- en: '[Chapter 10](../Text/kindle_split_021.xhtml#ch10) discusses adversarial examples
    (means of intentionally deceiving classifiers into making mistakes), an area with
    great practical and theoretical importance.'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第10章](../Text/kindle_split_021.xhtml#ch10) 讨论了对抗样本（故意欺骗分类器犯错的手段），这是一个具有极大实践和理论重要性的领域。'
- en: '[Chapter 11](../Text/kindle_split_022.xhtml#ch11) explores practical applications
    of GANs in medicine and fashion, whose implementations use the GAN variants covered
    in this book.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第11章](../Text/kindle_split_022.xhtml#ch11)探讨了GANs在医学和时尚领域的实际应用，其实现使用了本书中涵盖的GAN变体。'
- en: '[Chapter 12](../Text/kindle_split_023.xhtml#ch12) outlines the ethical considerations
    of GANs and their applications. We also mention emerging GAN techniques for those
    interested in continuing to explore this field beyond this book.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第12章](../Text/kindle_split_023.xhtml#ch12)概述了GANs及其应用的伦理考量。我们还提到了那些对继续探索这一领域感兴趣的读者所关注的新兴GAN技术。'
- en: Chapter 10\. Adversarial examples
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章。对抗样本
- en: '*This chapter covers*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: A fascinating research area that precedes GANs and has an interwoven history
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个在GANs之前出现且有着交织历史的迷人研究领域
- en: Deep learning approaches in a computer vision setting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉环境中的深度学习方法
- en: Our own adversarial examples with real images and noise
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们自己的带有真实图像和噪声的对抗样本
- en: Over the course of this book, you have come to understand GANs as an intuitive
    concept. However, in 2014, GANs seemed like a massive leap of faith, especially
    for those unfamiliar with the emerging field of adversarial examples, including
    Ian Goodfellow’s and others’ work in this field.^([[1](#ch10fn01)]) This chapter
    dives into *adversarial examples*—specially constructed examples that make other
    classification algorithms fail catastrophically.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，你已经将生成对抗网络（GANs）作为一个直观的概念来理解。然而，在2014年，GANs似乎是一个巨大的信仰飞跃，尤其是对于那些不熟悉对抗样本这一新兴领域的人来说，包括伊恩·古德费洛（Ian
    Goodfellow）和其他人在这一领域的工作。[1](#ch10fn01) 这一章深入探讨了*对抗样本*——专门构建的样本，使其他分类算法失败。
- en: ¹
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Intriguing Properties of Neural Networks,” by Christian Szegedy et al.,
    2014, [https://arxiv.org/pdf/1312.6199.pdf](https://arxiv.org/pdf/1312.6199.pdf).
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Christian Szegedy 等人于2014年发表的“神经网络的有趣特性”，[https://arxiv.org/pdf/1312.6199.pdf](https://arxiv.org/pdf/1312.6199.pdf)。
- en: We also talk about their connections to GANs and how and why adversarial learning
    is still largely an unsolved problem in ML—an important but rarely discussed flaw
    of the current approaches. That is true even though adversarial examples have
    an important role to play in ML robustness, fairness, and (cyber)security.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了它们与生成对抗网络（GANs）的联系，以及为什么和如何对抗性学习在机器学习（ML）中仍然是一个未解决的问题——这是当前方法的一个重要但很少被讨论的缺陷。尽管对抗样本在机器学习的鲁棒性、公平性和（网络安全）中扮演着重要角色，但这仍然是事实。
- en: There is no denying we have made substantial progress in machine learning’s
    capacity to match and surpass human-level performance over the last five years—for
    example, in computer vision (CV) classification tasks or the ability to play games.^([[2](#ch10fn02)])
    However, looking only at metrics and ROC curves^([[3](#ch10fn03)]) is insufficient
    for us to understand (a) why neural networks make the decisions they do (how they
    work) and (b) what errors they are prone to making. This chapter touches on the
    first and dives into the second. Before we begin, it should be said that although
    this chapter deals almost exclusively with CV problems, adversarial examples have
    been identified in diverse areas such as text or even in humans.^([[4](#ch10fn04)])
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法否认，在过去五年中，我们在机器学习匹配甚至超越人类水平性能的能力上取得了实质性进展——例如，在计算机视觉（CV）分类任务或玩游戏的能力。[2](#ch10fn02)
    然而，仅仅关注指标和ROC曲线[3](#ch10fn03) 对于我们理解（a）神经网络做出决策的原因（它们是如何工作的）以及（b）它们容易犯的错误是不够的。这一章触及了第一个问题，并深入探讨了第二个问题。在我们开始之前，应该指出的是，尽管这一章几乎完全涉及CV问题，但对抗样本已经在文本甚至人类等多样化的领域中得到了识别。[4](#ch10fn04)]
- en: ²
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What constitutes human-level performance in vision-classification tasks is a
    complicated topic. However, at least in, for example, Dota 2 and Go, AI has beat
    human experts by a substantial margin.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 视觉分类任务中构成人类水平表现的因素是一个复杂的话题。然而，至少在例如Dota 2和围棋这样的游戏中，人工智能已经以相当大的优势击败了人类专家。
- en: ³
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A *receiver operating characteristic (ROC) curve* explains the trade-offs between
    false positives and negatives. We also encountered them in [chapter 2](../Text/kindle_split_011.xhtml#ch02).
    For more details, Wikipedia has an excellent explanation.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个 *接收者操作特征（ROC）曲线* 解释了假阳性和假阴性之间的权衡。我们也在第2章中遇到了它们。[http://arxiv.org/abs/1901.06796](http://arxiv.org/abs/1901.06796)。更多细节，维基百科有很好的解释。
- en: ⁴
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey,” by Wei Emma Zhang et al., 2019, [http://arxiv.org/abs/1901.06796](http://arxiv.org/abs/1901.06796).
    See also “Adversarial Examples That Fool Both Computer Vision and Time-Limited
    Humans,” by Gamaleldin F. Elsayed et al., 2018, [http://arxiv.org/abs/1802.08195](http://arxiv.org/abs/1802.08195).'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Wei Emma Zhang等人撰写的“自然语言处理中深度学习模型的对抗攻击：综述”，2019年，[http://arxiv.org/abs/1901.06796](http://arxiv.org/abs/1901.06796)。另见Gamaleldin
    F. Elsayed等人撰写的“欺骗计算机视觉和有限时间人类的对抗示例”，2018年，[http://arxiv.org/abs/1802.08195](http://arxiv.org/abs/1802.08195)。
- en: First of all, when we speak about neural networks’ performance, we frequently
    read that their error rate is lower than that of humans on the large ImageNet
    dataset. This often-cited statistic—which started more as an academic joke than
    anything else—belies the performance differences hidden underneath this average.
    While humans’ error rate tends to be driven mostly by their inability to distinguish
    between different breeds of dogs that appear prominently in this dataset, the
    machine learning failures are much more ominous. Upon further investigation, adversarial
    examples were born.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当我们谈论神经网络的性能时，我们经常读到它们的错误率在大型ImageNet数据集上低于人类。这个经常被引用的统计数据——它更多的是一个学术玩笑而不是其他——掩盖了隐藏在平均数背后的性能差异。虽然人类的错误率往往是由他们无法区分数据集中突出显示的不同品种的狗所驱动的，但机器学习失败的情况则更为严重。经过进一步调查，对抗样本应运而生。
- en: Unlike humans, CV algorithms struggle with problems that are very different
    in nature and can be close to the training data. Because the algorithm has to
    make predictions for every picture possible, it has to extrapolate between the
    isolated and far-apart individual instances it has seen in the training data,
    even if we have lots of them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类不同，CV算法在处理本质上非常不同且可能接近训练数据的问题上存在困难。因为算法必须对每张可能的图片做出预测，它必须在训练数据中看到的孤立且相距甚远的单个实例之间进行外推，即使我们有很多这样的实例。
- en: When we have trained networks such as Inception V3 and VGG-19, we have found
    an amazing way of making image classification work on a thin manifold around the
    training data. But when people tried to poke holes in the classification ability
    of these algorithms, they discovered a cosmic crater—current machine learning
    algorithms get easily fooled by even minor distortions. Virtually all major successful
    machine learning algorithms to date suffer from this flaw to some extent, and,
    indeed, some speculate that is why machine learning works at all.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练了Inception V3和VGG-19这样的网络时，我们发现了一种令人惊叹的方法，可以在训练数据周围的薄流形上实现图像分类。但当人们试图在这些算法的分类能力上找漏洞时，他们发现了一个宇宙般大小的陨坑——当前的机器学习算法很容易被微小的扭曲所欺骗。迄今为止，几乎所有主要的成功机器学习算法都或多或少地存在这一缺陷，而且确实，有些人推测这正是机器学习之所以有效的原因。
- en: '|  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In supervised settings, think of our training set. We have a training manifold—just
    a fancy word describing a high-dimensional distribution in which our examples
    live. For example, our 300 × 300 pixel images live in a 270,000 dimensional space
    (300 × 300 × 3 colors). That makes training very complicated.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督设置中，想想我们的训练集。我们有一个训练流形——这是一个描述我们例子所在的高维分布的术语。例如，我们的300 × 300像素图像生活在270,000维的空间中（300
    × 300 × 3种颜色）。这使得训练变得非常复杂。
- en: '|  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 10.1\. Context of adversarial examples
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1. 对抗样本的背景
- en: 'To start, we want to quickly touch on why we included this chapter toward the
    end of the book:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们想简要谈谈为什么我们把这一章放在书的末尾：
- en: With adversarial examples, we are typically trying to generate new examples
    that fool our existing systems to misclassify the input. We do this usually either
    as evil attackers or perhaps just as researchers to see how robustly our system
    will behave. Adversarial examples are about as closely a related topic to GANs
    as it gets, though important differences exist.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对抗性示例，我们通常试图生成新的示例来欺骗我们现有的系统，使其错误分类输入。我们通常要么作为邪恶的攻击者，要么作为研究人员来观察我们的系统将如何表现出鲁棒性。对抗性示例与生成对抗网络（GANs）的关系非常密切，尽管存在一些重要差异。
- en: This will give you a sense of why GANs can be so hard to train and why our existing
    systems are so fragile.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将让你了解为什么生成对抗网络（GANs）可能如此难以训练，以及为什么我们现有的系统如此脆弱。
- en: Adversarial examples allow for a different set of applications from GANs, and
    we hope to give you at least the basics of their capabilities.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性示例允许从生成对抗网络（GANs）中获得不同的一组应用，我们希望至少向您介绍它们的基本能力。
- en: 'In terms of applications, adversarial examples are interesting for several
    reasons:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用方面，对抗性示例有几个有趣的原因：
- en: As discussed, adversarial examples can be used for malicious purposes, so it
    is important to test for robustness in critical systems. What if an attacker could
    easily fool a facial-recognition system to gain access to your phone?
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如讨论的那样，对抗性示例可以用于恶意目的，因此在关键系统中进行鲁棒性测试很重要。如果攻击者能够轻易地欺骗面部识别系统以获取你的手机访问权限怎么办？
- en: They help us understand machine learning fairness—which is a topic of growing
    importance. We can use adversarially learned representations that are useful for
    classifications but do not allow an attacker to recover protected facts, as probably
    one of the best ways of ensuring that our ML is not discriminating against anyone.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们帮助我们理解机器学习公平性——这是一个日益重要的主题。我们可以使用对抗性学习得到的表示，这些表示对分类很有用，但不会让攻击者恢复受保护的事实，这可能是确保我们的机器学习不会歧视任何人的最佳方法之一。
- en: In a similar vein, we can use adversarial learning to protect the privacy of
    sensitive—perhaps medical or financial—information about individuals. In this
    case, we are simply focusing on information about individuals not being recoverable.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用对抗性学习来保护个人敏感信息（可能是医疗或财务信息）的隐私。在这种情况下，我们只是关注个人信息不可恢复的问题。
- en: As current research stands, learning about adversarial examples is the only
    way to start to understand adversarial defenses, as most papers begin with a description
    of the types of attacks they defend against and only then try to solve them. At
    the time of writing this book, no universal defenses work against all types of
    attack. But whether this is a good reason to study them depends on your view on
    adversarial examples. We decided not to cover defenses in detail—above the high-level
    ideas toward the end of this chapter—because anything beyond that is beyond the
    scope of this book.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据当前的研究，了解对抗性示例是开始理解对抗性防御的唯一方法，因为大多数论文都是从描述他们所防御的攻击类型开始的，然后才尝试解决这些问题。在撰写本书时，没有通用的防御方法可以抵御所有类型的攻击。但这是否是研究它们的好理由取决于你对对抗性示例的看法。我们决定不详细讨论防御——在本章末尾的高层次思想之上——因为超出本书的范围。
- en: 10.2\. Lies, damned lies, and distributions
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2. 谎言、该死的谎言和分布
- en: To truly understand adversarial examples, we must come back to the domain of
    CV classification tasks—partially to understand how difficult a task it is. Recall
    that to go from raw pixels to ultimately being able to classify sets of images
    is challenging.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正理解对抗性示例，我们必须回到计算机视觉分类任务的领域——部分是为了了解这项任务有多困难。回想一下，从原始像素到最终能够对图像集进行分类是一项挑战。
- en: This is in part because, in order to have a truly generalizable algorithm, we
    have to make sensible predictions on data nowhere near anything that we have seen
    in the training set. Moreover, the pixel-level differences between the image at
    hand and the closest image in the training set of the same class are large, even
    when we slightly change the angle at which the picture was taken.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是因为，为了有一个真正可泛化的算法，我们必须对训练集中没有看到过的数据进行合理的预测。此外，当前图像与同一类训练集中最接近的图像在像素级上的差异很大，即使我们稍微改变拍摄图片的角度也是如此。
- en: When we have our training set of 100,000 examples of 300 × 300 images in RGB
    space, we have to somehow deal with 270,000 dimensions. When we consider all *possible*
    images (not the ones that we actually observe, but the ones that *could* happen),
    the pixel value of each dimension is independent of the other dimensions, because
    we can always generate a valid picture by rolling a hypothetical 256-sided dice
    270,000 times. Therefore, we theoretically have 256^(270,000) examples (a number
    that is 650,225 digits long) at 8-bit color space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有300 × 300像素大小的RGB空间中10万个图像的培训集时，我们必须以某种方式处理270,000个维度。当我们考虑所有*可能*的图像（不是我们实际观察到的，而是可能发生的），每个维度的像素值与其他维度是独立的，因为我们总是可以通过掷一个假设的256面骰子270,000次来生成一个有效的图片。因此，在8位色彩空间中，我们理论上拥有256^(270,000)个示例（一个有650,225位的数字）。
- en: We would need a lot of examples to cover even 1% of this space. Of course, most
    of these images would not make any sense. Frequently, our training set is a lot
    sparser than that, so we need our algorithms to train using this relatively limited
    data to extrapolate even into regions they have not seen at all yet. This is because
    the algorithm most likely has seen nothing near what we have in the training set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要大量的示例来覆盖这个空间的1%。当然，这些图像中的大多数都没有意义。通常，我们的训练集比这稀疏得多，因此我们需要我们的算法使用这些相对有限的数据进行训练，以便外推到他们尚未看到的所有区域。这是因为算法最有可能看到的东西与我们训练集中的东西相差甚远。
- en: '|  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Having 100,000 examples is frequently cited as a minimum at which deep learning
    algorithms should really start to shine.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有10万个示例通常被认为是深度学习算法真正开始发光的最低标准。
- en: '|  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We understand that algorithms have to meaningfully generalize; they have to
    be able to meaningfully fill in the huge part of space where they have not seen
    any example. Computer vision algorithms work mostly because they can come up with
    good guesses for the vast swaths of missing probability, but their strength is
    also their greatest weakness.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解算法必须有意义地泛化；它们必须能够有意义地填补它们尚未看到任何示例的巨大空间。计算机视觉算法之所以有效，主要是因为它们可以为大量缺失的概率提供良好的猜测，但它们的优点也是它们的最大弱点。
- en: 10.3\. Use and abuse of training
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3\. 训练的使用与滥用
- en: In this section, we introduce two ways of thinking about adversarial examples—one
    from first principles and the other by analogy. The first way to think about adversarial
    examples is to start from the way machine learning classification is trained.
    Remember that these are networks with tens of millions of parameters. Throughout
    training, we update some of them so that the class matches the label as provided
    in the training set. We need to find just the right parameter updates, which is
    what the stochastic gradient descent (SGD) allows us to do.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍两种思考对抗样本的方法——一种是从第一原理出发，另一种是通过类比。思考对抗样本的第一种方法是从小型机器学习分类的训练方式开始。记住，这些网络有数百万个参数。在整个训练过程中，我们更新其中的一些参数，以便类别与训练集中提供的标签相匹配。我们需要找到恰到好处的参数更新，这正是随机梯度下降（SGD）允许我们做到的。
- en: Now think back to the simple classifier days, before you knew a lot about GANs.
    Here we have some sort of learnable classification function *f[θ]*(*x*) (for example,
    a deep neural network, or DNN), which is parametrized by *θ* (parameters of the
    DNN) and takes *x* (for example, an image) as input and produces a classification
    ![](../Images/ycap.jpg). At training time, we then take ![](../Images/ycap.jpg)
    and compare it with the true *y*, which is how we get our loss (*L*). We then
    update the parameters of *f[θ]*(*x*) such that the loss is minimized. [Equations
    10.1](#ch10equ01), [10.2](#ch10equ02), and [10.3](#ch10equ03) summarize.^([[5](#ch10fn05)])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回想一下在你知道很多关于GANs之前简单的分类器时代。这里我们有一种可学习的分类函数 *f[θ]*(*x*)（例如，深度神经网络，或DNN），它由
    *θ*（DNN的参数）参数化，并接受 *x*（例如，图像）作为输入，产生一个分类 ![](../Images/ycap.jpg)。在训练时，我们然后取 ![](../Images/ycap.jpg)
    并将其与真实的 *y* 进行比较，这就是我们如何得到损失 (*L*)。然后我们更新 *f[θ]*(*x*) 的参数，使得损失最小化。[方程式10.1](#ch10equ01)，[10.2](#ch10equ02)，和
    [10.3](#ch10equ03) 总结了这些。[^([[5](#ch10fn05)])
- en: ⁵
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Please remember, this is just a quick summary, and we have to skip over some
    details, so if you can point them out—great. If not, we suggest picking up a book
    such as *Deep Learning with Python* by François Chollet (Manning, 2017) to brush
    up on the specifics.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请记住，这只是一个快速总结，我们不得不跳过一些细节，所以如果你能指出它们——太好了。如果你不能，我们建议阅读像 François Chollet 的《用Python进行深度学习》（Manning,
    2017）这样的书籍，以复习具体细节。
- en: equation 10.1\.
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: equation 10.1\.
- en: '![](../Images/10equ01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10equ01.jpg)'
- en: equation 10.2\.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: equation 10.2\.
- en: '![](../Images/10equ02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10equ02.jpg)'
- en: equation 10.3\.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: equation 10.3。
- en: '![](../Images/10equ03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10equ03.jpg)'
- en: In essence, we have defined *prediction* as the output of the neural net after
    being fed an example ([equation 10.1](#ch10equ01)). *Loss* is some form of the
    difference between the true and predicted label ([equation 10.2](#ch10equ02)).
    The overall problem is then phrased as trying to minimize the difference between
    the true and predicted labels over the parameters of the DNN, which then constitute
    the prediction given an example ([equation 10.3](#ch10equ03)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们已经将*预测*定义为神经网络在输入一个示例后的输出([方程式 10.1](#ch10equ01))。*损失*是真实标签和预测标签之间的一种差异([方程式
    10.2](#ch10equ02))。因此，整体问题被表述为尝试最小化真实标签和预测标签之间的差异，这些差异构成了给定示例的预测，即DNN的参数([方程式
    10.3](#ch10equ03))。
- en: This is all working great, but how do we actually minimize our classification
    loss? How do we solve the optimization problem as phrased in [equation 10.3](#ch10equ03)?
    We usually use an SGD-based method to take batches of *x*; then we take the derivative
    of the loss function with respect to the current parameters (*θ[t]*) multiplied
    by our learning rate (α), which constitutes our new parameters (*θ[t]* [+ 1]).
    See [equation 10.4](#ch10equ04).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都进行得很好，但我们实际上如何最小化我们的分类损失？我们如何解决[方程式 10.3](#ch10equ03)中表述的优化问题？我们通常使用基于SGD的方法来获取批次的*x*；然后我们计算损失函数相对于当前参数（*θ[t]*)的导数，并将其乘以我们的学习率（α），这构成了我们的新参数（*θ[t]*
    [+ 1]）。参见[方程式 10.4](#ch10equ04)。
- en: equation 10.4\.
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: equation 10.4。
- en: '![](../Images/10equ04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10equ04.jpg)'
- en: 'This was the quickest introduction to deep learning you will ever find. But
    now that you have this context, think about whether this powerful tool (SGD) could
    be used for other purposes as well. For instance, what happens when we take a
    step *up* the loss space rather than *down*? Turns out, maximizing the error rather
    than minimizing it is much easier, but also important. And like many great discoveries,
    it started as a seeming bug that turned into a hack: what if we start updating
    the pixels rather than the weights? If we update them maliciously, adversarial
    examples happen.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你能找到的关于深度学习最快的介绍。但现在你有了这个背景，想想这个强大的工具（SGD）是否也可以用于其他目的。例如，当我们向上而不是向下移动损失空间时会发生什么？结果发现，最大化错误而不是最小化错误要容易得多，但也很重要。而且像许多伟大的发现一样，它开始时是一个看似的bug，后来变成了一个hack：如果我们开始更新像素而不是权重会怎样？如果我们恶意地更新它们，就会发生对抗样本。
- en: Some of you may be confused, about this quick recap of SGD, so let’s remind
    ourselves what a typical loss space could look like in [figure 10.1](#ch10fig01).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人对SGD的快速回顾可能感到困惑，所以让我们提醒自己一个典型的损失空间在[图 10.1](#ch10fig01)中可能看起来是什么样子。
- en: Figure 10.1\. In this typical loss space, remember, this is the type of loss
    value we can feasibly get with our deep learning algorithms. On the left, you
    have 2D contour lines of equal loss, and on the right, you have a 3D rendering
    of what a loss space may look like. Remember the mountaineering analogy from [chapter
    6](../Text/kindle_split_016.xhtml#ch06)?
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.1。记住，这是我们用深度学习算法可以实际得到的损失值类型。在左侧，你有等损失值的2D等高线，在右侧，你有损失空间的3D渲染。还记得[第6章](../Text/kindle_split_016.xhtml#ch06)中的登山类比吗？
- en: '![](../Images/10fig01_alt.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10fig01_alt.jpg)'
- en: '(Source: “Visualizing the Loss Landscape of Neural Nets,” by Tom Goldstein
    et al., 2018, [https://github.com/tomgoldstein/loss-landscape](https://github.com/tomgoldstein/loss-landscape).)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：“可视化神经网络损失景观”，作者Tom Goldstein等，2018年，[https://github.com/tomgoldstein/loss-landscape](https://github.com/tomgoldstein/loss-landscape)。)
- en: The second useful (though imperfect) mental model to think about adversarial
    examples is by analogy. You may think of adversarial examples as Conditional GANs
    like those we encountered in the preceding two chapters. With adversarial examples,
    we are conditioning on an entire image and trying to produce a *domain transferred*
    or similar image, except in a domain that fools the classifier. The “generator”
    can be a simple stochastic gradient ascent that simply adjusts the image to fool
    some other classifier.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑对抗样本的第二个有用的（尽管不完美）心智模型是通过类比。你可以将对抗样本视为类似于我们在前两章中遇到的那些条件生成对抗网络（Conditional
    GANs）。在对抗样本中，我们是在整个图像上进行条件化，并试图生成一个*域转换*或类似图像，但这个域能够欺骗分类器。“生成器”可以是一个简单的随机梯度上升算法，它只是调整图像以欺骗其他分类器。
- en: Whichever of the two ways makes sense to you, let’s now dive straight into adversarial
    examples and what they look like. They were discovered with an observation of
    how easy it is to misclassify these altered images. One of the first methods to
    achieve this is the *fast sign gradient method (FSGM)*, which is as simple as
    our previous description.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式对你来说更有意义，我们现在直接深入探讨对抗性示例及其外观。它们是通过观察如何容易错误分类这些修改后的图像而被发现的。实现这一目标的第一种方法是最简单的*快速符号梯度方法（FSGM）*，它就像我们之前的描述一样简单。
- en: You start with the gradient update ([equation 10.4](#ch10equ04)), look at the
    sign, and then make a small step in the opposite direction. In fact, frequently
    the images come out looking (almost) identical! A picture is worth a thousand
    words to show you how little noise is needed; see [figure 10.2](#ch10fig02).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你从梯度更新([方程10.4](#ch10equ04))开始，查看符号，然后在相反方向上迈出小一步。事实上，图像经常看起来（几乎）完全相同！一张图片胜过千言万语，向您展示所需的噪声有多少；请参阅[图10.2](#ch10fig02)。
- en: Figure 10.2\. A bit of noise makes a lot of difference. The picture in the middle
    has the noise (difference) applied to it (the picture to the right). Of course,
    the right picture is heavily amplified—approximately 300 times—and shifted so
    that it can create a meaningful image.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2\. 一点噪声就能产生很大的差异。中间的图片应用了噪声（差异），（右边的图片）。当然，右边的图片被大幅放大——大约300倍——并移动，以便它可以创建一个有意义的图像。
- en: '![](../Images/10fig02_alt.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10fig02_alt.jpg)'
- en: Now we run a ResNet-50 pretrained classifier on this unmodified vacation image
    and check the top three predictions, shown in [table 10.1](#ch10table01); drumroll,
    please.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在这个未修改的假日图像上运行一个预训练的ResNet-50分类器，并检查前三个预测结果，如[表10.1](#ch10table01)所示；请鼓掌。
- en: Table 10.1\. Original image predictions
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.1\. 原始图像预测
- en: '| Order | Class | Confidence |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 排序 | 类别 | 置信度 |'
- en: '| --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| First | mountain_tent | 0.6873 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 第一 | 山顶帐篷 | 0.6873 |'
- en: '| Second | promontory | 0.0736 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 第二 | 岩石突出部 | 0.0736 |'
- en: '| Third | valley | 0.0717 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 第三 | 山谷 | 0.0717 |'
- en: The top three are all sensible, with `mountain_tent` taking the top spot, as
    it should. [Table 10.2](#ch10table02) shows the adversarial image predictions.
    The top three miss `mountain_tent` completely, with some suggestions that at least
    match the outdoors, but even the modified image is clearly not a suspension bridge.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最前面的三个都是合理的，其中`mountain_tent`占据首位，正如它应该的那样。[表10.2](#ch10table02)显示了对抗性图像预测。前三个完全错过了`mountain_tent`，尽管有些建议至少与户外相符，但修改后的图像显然不是一座悬索桥。
- en: Table 10.2\. Adversarial image predictions
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.2\. 对抗性图像预测
- en: '| Order | Class | Confidence |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 排序 | 类别 | 置信度 |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| First | volcano | 0.5914 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 第一 | 火山 | 0.5914 |'
- en: '| Second | suspension_bridge | 0.1685 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 第二 | 悬索桥 | 0.1685 |'
- en: '| Third | valley | 0.0869 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 第三 | 山谷 | 0.0869 |'
- en: This is how much we can distort the prediction, with a budget of only approximately
    200 pixel values—the equivalent of taking a *single* almost-black pixel and turning
    it into an almost-white pixel—spread across the whole image.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们可以在仅约200个像素值（相当于将一个几乎黑色的像素变成几乎白色的像素）的预算内扭曲预测的程度——在整个图像上分散。
- en: A somewhat scary thing is how little code it takes to create this whole example.
    In this chapter, we’ll use an amazing library called `foolbox`, which provides
    many great convenience methods to create adversarial examples. Without further
    ado, let’s dive into it. We start with our well-known imports, plus `foolbox`,
    which is a library designed specifically to make adversarial attacks easier.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点令人害怕的是，创建整个示例所需的代码如此之少。在本章中，我们将使用一个名为`foolbox`的神奇库，它提供了许多方便的方法来创建对抗性示例。无需多言，让我们深入探讨。我们首先进行我们熟悉的导入，包括`foolbox`，这是一个专门设计来使对抗性攻击更简单的库。
- en: Listing 10.1\. Our trusty imports
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1\. 我们可靠的导入
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we define a convenience function to load in more images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个方便的函数来加载更多图像。
- en: Listing 10.2\. Helper function
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2\. 辅助函数
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we have to set Keras to register our model and download ResNet-50 from
    the Keras convenience function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须设置Keras以注册我们的模型并从Keras方便函数下载ResNet-50。
- en: Listing 10.3\. Creating [tables 10.1](#ch10table01) and [10.2](#ch10table02)
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3\. 创建[表10.1](#ch10table01)和[10.2](#ch10table02)
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Instantiates model**'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 实例化模型**'
- en: '***2* Creates the foolbox model object from the Keras model**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从Keras模型创建foolbox模型对象**'
- en: '***3* We make the image (1, 224, 224, 3) so that it fits ResNet-50, which expects
    images for predictions to be in batches.**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将图像调整为(1, 224, 224, 3)，以便它适合ResNet-50，因为ResNet-50期望预测图像为批量形式。**'
- en: '***4* We call predict and print the results.**'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 调用predict并打印结果。**'
- en: '***5* Gets the index of the highest number, as a label to be used later**'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取最高数字的索引，作为后续使用的标签**'
- en: '***6* ::-1 reverses the color channels, because Keras ResNet-50 expects BGR
    instead of RGB.**'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* ::-1反转颜色通道，因为Keras ResNet-50期望BGR而不是RGB。**'
- en: '***7* Creates the attack object, setting high misclassification criteria**'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 创建攻击对象，设置高误分类标准**'
- en: '***8* Applies attack on source image**'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 对源图像应用攻击**'
- en: '***9* Gets the new predictions on the adversarial image**'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 获取对抗图像的新预测**'
- en: That’s how easy it is to use these examples! Now you may be thinking, maybe
    that’s just ResNet-50 that suffers from these examples. Well, we have some bad
    news for you. ResNet not only proved to be the hardest classifier to break as
    we were testing various code setups for this chapter, but also is an uncontested
    winner on DAWNBench in every ImageNet category (which is the most challenging
    task in the CV category on DAWNBench), as shown in [figure 10.3](#ch10fig03).^([[6](#ch10fn06)])
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用这些例子有多简单！现在你可能正在想，也许这只是ResNet-50在这些例子中受到影响。好吧，我们有一些坏消息要告诉你。ResNet不仅在我们测试本章的各种代码设置时证明是最难被打破的分类器，而且在DAWNBench的每个ImageNet类别（这是DAWNBench中CV类别中最具挑战性的任务）中都是无可争议的赢家，如图10.3所示。[图10.3](#ch10fig03).^([[6](#ch10fn06)])
- en: ⁶
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Image Classification on ImageNet,” at DAWNBench, [https://dawn.cs.stanford.edu/benchmark/#imagenet](https://dawn.cs.stanford.edu/benchmark/#imagenet).
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅DAWNBench上的“Image Classification on ImageNet”，[https://dawn.cs.stanford.edu/benchmark/#imagenet](https://dawn.cs.stanford.edu/benchmark/#imagenet)。
- en: Figure 10.3\. DAWNBench is a great place to see the current state-of-the-art
    models and ResNet-50 dominance, at least as of early July 2019.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3\. DAWNBench是一个很好的地方，可以查看当前最先进的模型和ResNet-50的统治地位，至少截至2019年7月初。
- en: '![](../Images/10fig03_alt.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10fig03_alt.jpg)'
- en: But the biggest problem of adversarial examples is their pervasiveness. Adversarial
    examples generalize beyond deep learning and transfer to different ML techniques.
    If we generate an adversarial example against one technique, there is a reasonable
    chance it will work even on another model we are trying to attack, as illustrated
    in [figure 10.4](#ch10fig04).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但对抗样本的最大问题是它们的普遍性。对抗样本不仅限于深度学习，而且可以迁移到不同的机器学习技术。如果我们针对一种技术生成一个对抗样本，那么它在另一个我们试图攻击的模型上也能合理地工作，如图10.4所示。
- en: Figure 10.4\. The numbers here denote the percentage of adversarial examples
    crafted to fool the classifier in that row that also fooled that column’s classifier.
    The methods are deep neural networks (DNNs), logistic regression (LR), support-vector
    machine (SVM), decision trees (DT), nearest neighbors (kNN), and ensembles (Ens.).
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4\. 这里显示的数字表示为该行中用来欺骗分类器的对抗样本的百分比，同时也欺骗了该列的分类器。这些方法是深度神经网络（DNNs）、逻辑回归（LR）、支持向量机（SVM）、决策树（DT）、最近邻（kNN）和集成（Ens.）。
- en: '![](../Images/10fig04_alt.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10fig04_alt.jpg)'
- en: '(Source: “Transferability in Machine Learning: from Phenomena to Black-Box
    Attacks Using Adversarial Samples,” by Nicolas Papernot et al., 2016, [https://arxiv.org/pdf/1605.07277.pdf](https://arxiv.org/pdf/1605.07277.pdf).)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“机器学习中的可迁移性：使用对抗样本从现象到黑盒攻击”，Nicolas Papernot等人，2016年，[https://arxiv.org/pdf/1605.07277.pdf](https://arxiv.org/pdf/1605.07277.pdf)）
- en: 10.4\. Signal and the noise
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4\. 信号和噪声
- en: Worse yet, many of the adversarial examples are so easy to construct that we
    can just as easily fool the classifier by Gaussian noise that we can sample from
    `np.random.normal`. On the other hand—and to support our earlier point of ResNet-50
    being a fairly robust architecture—we will show you that other architectures suffer
    from this issue much more.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，许多对抗样本构建起来非常简单，我们可以用从`np.random.normal`中采样的高斯噪声同样容易地欺骗分类器。另一方面——为了支持我们之前关于ResNet-50是一个相当鲁棒的架构的观点——我们将向你展示其他架构对此问题的影响更大。
- en: '[Figure 10.5](#ch10fig05) shows the result of running ResNet-50 on pure Gaussian
    noise. However, we can use an adversarial attack on the noise itself to see how
    misclassified our image can get—rather quickly.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.5](#ch10fig05)显示了在纯高斯噪声上运行ResNet-50的结果。然而，我们可以通过对噪声本身进行对抗攻击来查看我们的图像可能被误分类到什么程度——相当快地。'
- en: Figure 10.5\. It is clear that we do not get a confident classification as a
    wrong class in most cases on just naively sampled noise. So that is plus points
    to ResNet-50\. On the left, we include the mean and variance we used so that you
    can see their impact.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5\. 很明显，在大多数情况下，我们不会在简单地采样的噪声上得到一个自信的分类作为错误类别。所以这是ResNet-50的加分项。在左侧，我们包括了我们所使用的均值和方差，以便你可以看到它们的影响。
- en: '![](../Images/10fig05_alt.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片10.5_替代](../Images/10fig05_alt.jpg)'
- en: In [listing 10.4](#ch10ex04), we’ll use a *projected gradient descent (PGD)
    attack*, illustrated in [figure 10.6](#ch10fig06). Although this is still a simple
    attack, it warrants a high-level explanation. Unlike with the previous attacks,
    we are now taking a step regardless of where it may lead us—even “invalid” pixel
    values—and then projecting back onto the feasible space. Now let’s apply the PGD
    attack onto our Gaussian noise in [figure 10.7](#ch10fig07) and run ResNet-50
    to see how we do.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.4](#ch10ex04)中，我们将使用一种*投影梯度下降（PGD）攻击*，如图[图10.6](#ch10fig06)所示。尽管这仍然是一种简单的攻击，但它值得进行高级解释。与之前的攻击不同，我们现在无论走向何方都会迈出一步——甚至“无效”的像素值——然后将其投影回可行空间。现在让我们将PGD攻击应用于[图10.7](#ch10fig07)中的高斯噪声，并运行ResNet-50以查看我们的表现。
- en: 'Figure 10.6\. Projected gradient descent takes a step in the optimal direction,
    wherever it may be, and then uses projection to find the nearest equivalent point
    in the set of points. In this case, we are trying to ensure that we still end
    up with a valid picture: we take an example x(k) and take the optimal step to
    y^((k + 1)) to then project it to a valid set of images as x^((k + 1)).'
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6\. 投影梯度下降在任意方向上迈出一步，然后使用投影找到点集中最近的等效点。在这种情况下，我们试图确保最终得到一个有效的图片：我们取一个示例x(k)，并对其采取最优步骤到y^((k
    + 1))，然后将其投影到一个有效的图像集x^((k + 1))。
- en: '![](../Images/10fig06.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片10.6](../Images/10fig06.jpg)'
- en: 'Figure 10.7\. When we run ResNet-50 on adversarial noise, we get a different
    story: most of the items are misclassified after applying a PGD attack—still a
    simple attack.'
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7\. 当我们在对抗噪声上运行ResNet-50时，我们得到一个不同的故事：在应用PGD攻击后，大多数项目都被错误分类——仍然是一个简单的攻击。
- en: '![](../Images/10fig07_alt.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片10.7_替代](../Images/10fig07_alt.jpg)'
- en: To demonstrate that most architectures are even worse, we’ll look into Inception
    V3—an architecture that has earned fame in the CV community. Indeed, this network
    has been deemed so reliable that we touched on it in [chapter 5](../Text/kindle_split_015.xhtml#ch05).
    In [figure 10.8](#ch10fig08), you can see that even something that gave birth
    to the inception score still fails on trivial examples. To dispel any doubts,
    Inception V3 is still one of the better pretrained networks out there and does
    have superhuman accuracy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明大多数架构甚至更差，我们将研究Inception V3——一个在CV社区中赢得声誉的架构。的确，这个网络被认为非常可靠，我们在[第5章](../Text/kindle_split_015.xhtml#ch05)中提到了它。在[图10.8](#ch10fig08)中，你可以看到即使是产生了
    inception score 的东西在简单的例子上仍然失败。为了消除任何疑虑，Inception V3仍然是最好的预训练网络之一，并且确实具有超人的准确率。
- en: Figure 10.8\. Inception V3 applied to Gaussian noise. Notice that we are not
    using any attacks; this noise is just sampled from the distribution.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8\. Inception V3应用于高斯噪声。请注意，我们没有使用任何攻击；这种噪声只是从分布中采样的。
- en: '![](../Images/10fig08_alt.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片10.8_替代](../Images/10fig08_alt.jpg)'
- en: '|  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: This was just regular Gaussian noise. You can see in the code for yourself that
    no adversarial step was applied. Sure, you could argue that the noise could have
    been preprocessed better. But even that is a massive adversarial weakness.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是普通的Gaussian噪声。你可以在代码中亲自看到没有应用任何对抗步骤。当然，你可以争论噪声可以预处理得更好。但即使是这一点也是一个巨大的对抗弱点。
- en: '|  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If you are anything like us, you are thinking, no way, I want to see for myself.
    Well, now we give you the code to reproduce those figures. Because the code for
    each is similar, we go through it only once and for next time promise DRYer code.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我们一样，你可能在想，不可能，我想亲自看看。好吧，现在我们给你代码来重现这些图。因为每个代码都很相似，我们只看一次，并承诺下次会有更DRY的代码。
- en: '|  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: For an explanation of *don’t repeat yourself (DRY)* code, see Wikipedia at [https://en.wikipedia.org/wiki/Don%27t_repeat_yourself](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 关于*不要重复自己（DRY）*代码的解释，请参阅维基百科[https://en.wikipedia.org/wiki/Don%27t_repeat_yourself](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)。
- en: '|  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 10.4\. Gaussian noise
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.4\. 高斯噪声
- en: '[PRE3]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* Lists of means and variances as floats**'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 以浮点数形式表示均值和方差列表**'
- en: '***2* The core function that renders [figure 10.8](#ch10fig08)**'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 生成[图10.8](#ch10fig08)的核心功能**'
- en: '***3* Sample noise for each mean and variance**'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为每个均值和方差采样噪声**'
- en: '***4* Only 0–255 pixel values permitted**'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 只允许0-255像素值**'
- en: '***5* Gets our first prediction**'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获得我们的第一个预测**'
- en: '***6* Gets the predicted class and confidence, respectively**'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 分别获取预测类别和置信度**'
- en: '***7* Sets up annotating code for [figure 10.8](#ch10fig08) and then adds the
    annotations and text**'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 设置为注释代码[图10.8](#ch10fig08)并添加注释和文本**'
- en: '***8* Division by 255 to convert [0, 255] to [0, 1]**'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 除以255将[0, 255]转换为[0, 1]**'
- en: '***9* The main for loop that allows us to insert subplots into the figure**'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 主循环允许我们将子图插入到图中**'
- en: 10.5\. Not all hope is lost
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5\. 并非所有希望都已破灭
- en: Some people now start to worry about the security implications of adversarial
    examples. However, it is important to keep this in a meaningful perspective of
    a hypothetical attacker. If the attacker can change every pixel slightly, why
    not change the whole image?^([[7](#ch10fn07)]) Why not just feed in another one
    that is completely different? Why does the passed-in example have to be imperceptibly—rather
    than visibly—different?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有些人开始担心对抗性示例的安全影响。然而，重要的是要从一个假设的攻击者的有意义的视角来看待这个问题。如果攻击者可以稍微改变每个像素，为什么不改变整个图像呢？^([[7](#ch10fn07)])
    为什么不输入一个完全不同的图像呢？为什么传入的示例必须是不易察觉的——而不是明显不同的？
- en: ⁷
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Motivating the Rules of the Game for Adversarial Example Research,” by
    Justin Gilmer et al., 2018, [http://arxiv.org/abs/1807.06732](http://arxiv.org/abs/1807.06732).
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Justin Gilmer等人撰写的“对抗性示例研究游戏规则激励”，2018年，[http://arxiv.org/abs/1807.06732](http://arxiv.org/abs/1807.06732)。
- en: Some people give the example of self-driving cars and adversarially perturbing
    stop signs. But if we can do that, why wouldn’t the attackers completely spray-paint
    over the stop signs or simply physically obscure the stop sign with a high speed-limit
    sign for a little while? Because these “traditional attacks,” unlike adversarial
    examples, will work 100% of the time, whereas an adversarial attack works only
    when it transfers well and manages to not get distorted by the preprocessing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人以自动驾驶汽车和对抗性地干扰停车标志为例。但如果我们可以做到这一点，为什么攻击者不会完全喷漆覆盖停车标志，或者简单地用高速限制标志暂时遮挡停车标志呢？因为这些“传统攻击”与对抗性示例不同，将始终100%有效，而对抗性攻击只有在它能够很好地转移并且没有被预处理扭曲时才会有效。
- en: This does not mean that when you have a mission-critical ML application, you
    can just ignore this problem. However, it most cases, adversarial attacks require
    far more effort than more commonplace vectors of attack, so bearing that in mind
    is worthwhile.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着当你有一个关键任务机器学习应用时，你可以忽略这个问题。然而，在大多数情况下，对抗性攻击需要比更常见的攻击向量更多的努力，所以考虑到这一点是值得的。
- en: Yet, as with most security implications, adversarial attacks also have adversarial
    defenses that attempt to defend against the many types of attacks. The attacks
    covered in this chapter have been some of the easier ones, but even simpler ones
    exist—such as drawing a single line through MNIST. Even that is sufficient to
    fool most classifiers.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与大多数安全影响一样，对抗性攻击也有对抗性防御，试图防御许多类型的攻击。本章中涵盖的攻击是一些较容易的，但甚至更简单的攻击也存在——例如在MNIST上画一条线。即使这样也足以欺骗大多数分类器。
- en: Adversarial defenses are an ever-evolving game, in which many good defenses
    are available against some types of attacks, but not all. The turnaround can be
    so quick that just three days after the submission deadline for ICLR 2018, seven
    of the eight proposed and examined defenses were broken.^([[8](#ch10fn08)])
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性防御是一场不断发展的游戏，其中对某些类型的攻击有许多好的防御措施，但并非所有。这种转变可能非常快，以至于在ICLR 2018的提交截止日期后的三天内，就有八项提议和检查的防御措施被破解了.^([[8](#ch10fn08)])
- en: ⁸
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ICLR is the *International Conference on Learning Representations*, one of the
    smaller but excellent machine learning conferences. See Anish Athalye on Twitter
    in 2018, [http://mng.bz/ad77](http://mng.bz/ad77). It should be noted that there
    were three more defenses unexamined by the author.
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ICLR是*国际学习表示会议*，这是一个较小但很棒的机器学习会议。请参阅2018年Anish Athalye在Twitter上的内容，[http://mng.bz/ad77](http://mng.bz/ad77)。需要注意的是，还有三个作者未检查的防御措施。
- en: 10.6\. Adversaries to GANs
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6\. GAN的对抗者
- en: 'To make the connection with GANs even clearer, imagine a system generating
    adversarial examples, and another one saying how good that example is—depending
    on whether the example managed to fool the system or not. Doesn’t that remind
    you of a Generator (adversary) and a Discriminator (classification algorithm)?
    These two algorithms are again competing: the adversary is trying to fool the
    classifier with slight perturbations of the image, and the classifier is trying
    to not get fooled. Indeed, a way to think of GANs is almost as ML-in-the-loop
    adversarial examples that eventually come up with images.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使与 GANs 的联系更加清晰，想象一个生成对抗性示例的系统，另一个系统则评估该示例的好坏——这取决于示例是否成功地欺骗了系统。这难道不让你想起了生成器（对手）和判别器（分类算法）吗？这两个算法再次竞争：对手试图通过轻微扰动图像来欺骗分类器，而分类器则试图不被欺骗。实际上，将
    GANs 视为几乎是在循环中进行的机器学习对抗性示例，最终生成图像。
- en: On the other hand, you can think of iterated adversarial attacks as if you took
    a GAN and, rather than specifying that the objective is to generate the most realistic
    examples, you specify that the objective is to generate examples that will fool
    the classifier. Of course, you have to always remember that important differences
    exist, and typically you have a fixed classifier in deployed systems. But that
    does not preclude us from using this idea in *adversarial training* in which some
    implementations even include a repeated retraining of the classifier based on
    the adversarial examples that fooled it. These techniques are then moving closer
    to a typical GANs setup.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你可以将迭代对抗攻击视为如果你取了一个 GAN，而不是指定目标是要生成最逼真的示例，而是指定目标是要生成能够欺骗分类器的示例。当然，你必须始终记住存在一些重要差异，通常在部署系统中你有一个固定的分类器。但这并不妨碍我们在对抗性训练中使用这个想法，其中一些实现甚至包括基于欺骗它的对抗性示例的重复重新训练分类器。这些技术正在逐渐接近典型的
    GANs 设置。
- en: To give you an example, let’s take a look at one technique that has held its
    ground for a while as a viable defense. In the *Robust Manifold Defense*, we take
    the following steps to defend against the adversarial examples:^([[9](#ch10fn09)])
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给您一个例子，让我们看看一种已经作为可行的防御方法存在了一段时间的技术。在 *鲁棒流形防御* 中，我们采取以下步骤来防御对抗性示例：^([[9](#ch10fn09)])
- en: ⁹
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “The Robust Manifold Defense: Adversarial Training Using Generative Models,”
    by Ajil Jalal et al., 2019, [https://arxiv.org/pdf/1712.09196.pdf](https://arxiv.org/pdf/1712.09196.pdf).'
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Ajil Jalal 等人于 2019 年发表的论文“《鲁棒流形防御：使用生成模型的对抗性训练》”，[https://arxiv.org/pdf/1712.09196.pdf](https://arxiv.org/pdf/1712.09196.pdf)。
- en: We take an image *x* (adversarial or regular) and
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们取一个图像 *x*（对抗性或常规）并
- en: Project it back to the latent space *z*.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其投影回潜在空间 *z*。
- en: Use the generator *G* to generate a similar example to *x*, called *x** by *G*(*z*).
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成器 *G* 生成与 *x* 相似的示例，称为 *x*，通过 *G*(*z*)。
- en: We use the classifier *C* to classify this example *C*(*x**), which generally
    already tends to misclassify way less than running the classification directly
    on *x*.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分类器 *C* 对此示例进行分类，即 *C*(*x*)，这通常已经比直接在 *x* 上运行分类错误率低得多。
- en: 'However, the authors of this defense find out that there are still *some* ambiguous
    cases in which the classifier does get fooled by minor perturbations. Still, we
    encourage you to check out their paper, as these cases tend to be unclear to humans
    as well, which is a sign of a robust model. To fix this, we apply *adversarial
    training* on the manifold: we get some of these adversarial cases into the training
    set so the classifier learns to distinguish those from the real training data.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该防御的作者发现，仍然存在一些模糊的情况，其中分类器会被轻微的扰动欺骗。尽管如此，我们鼓励您查看他们的论文，因为这些情况对人类来说也往往是不清晰的，这是鲁棒模型的一个标志。为了解决这个问题，我们在流形上应用了对抗性训练：我们将一些这些对抗性案例纳入训练集，以便分类器学会区分这些案例和真实训练数据。
- en: This paper demonstrates that using GANs can give us classifiers that do not
    completely break down after minor perturbations, even against some of the most
    sophisticated methods. Performance of the downstream classifier does drop as with
    most of these defenses, because our classifier now has to be trained to implicitly
    deal with these adversarial cases. But even despite this setback, it is not a
    universal defense.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文表明，使用 GANs 可以给我们提供在轻微扰动后不会完全崩溃的分类器，甚至对抗一些最复杂的方法。下游分类器的性能确实会下降，就像大多数这些防御一样，因为我们的分类器现在必须被训练来隐式地处理这些对抗性案例。但即使有这个挫折，它也不是一个通用的防御。
- en: Adversarial training, of course, has some interesting applications. For example,
    for a while, the best results—state of the art—in semi-supervised learning were
    achieved by using adversarial training.^([[10](#ch10fn10)]) This was subsequently
    challenged by GANs (remember [chapter 7](../Text/kindle_split_017.xhtml#ch07)?)
    and other approaches, but that does not mean that by the time you are reading
    these lines, adversarial training will not be the state of the art again.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对抗训练有一些有趣的应用。例如，在一段时间内，半监督学习中最优秀的结果——最先进的技术——是通过使用对抗训练实现的。[^([10](#ch10fn10))]
    这随后被 GANs（还记得第 7 章吗？）和其他方法所挑战，但这并不意味着在你阅读这些文字的时候，对抗训练不会再次成为最先进的技术。
- en: ^(10)
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([10](#ch10fn10))
- en: ''
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Virtual Adversarial Training: A Regularization Method for Supervised and
    Semi-Supervised Learning,” by Takeru Miyato et al., 2018, [https://arxiv.org/pdf/1704.03976.pdf](https://arxiv.org/pdf/1704.03976.pdf).'
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Takeru Miyato 等人于 2018 年发表的“虚拟对抗训练：监督和半监督学习的正则化方法”，[https://arxiv.org/pdf/1704.03976.pdf](https://arxiv.org/pdf/1704.03976.pdf)。
- en: Hopefully, this gave you another reason to study GANs and adversarial examples—partially
    because in mission-critical classification tasks, GANs may be the best defense
    going forward or because of other applications beyond the scope of this book.^([[11](#ch10fn11)])
    That is best left for a hypothetical *Adversarial Examples in Action*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这为你学习 GANs 和对抗样本提供了另一个理由——部分原因是因为在关键任务分类中，GANs 可能是未来的最佳防御手段，或者是因为本书范围之外的其它应用。[^([11](#ch10fn11))]
    这最好留给假设性的“对抗样本实战”。
- en: ^(11)
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([11](#ch10fn11))
- en: ''
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This was a hotly debated topic at ICLR 2019\. Though most of these conversations
    were informal, using (pseudo) invertible generative models as a way to classify
    “out-of-sample”ness of an image seems like a fruitful avenue.
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是在 ICLR 2019 上激烈争论的话题。尽管这些对话大多数是非正式的，但使用（伪）可逆生成模型作为分类图像“样本外”性的方法似乎是一条富有成效的途径。
- en: To sum up, we have laid out the notion of adversarial examples and made the
    connection to GANs even more specific. This is an underappreciated connection,
    but one that can solidify your understanding of this challenging subject. Furthermore,
    one of the defenses against adversarial examples are GANs themselves!^([[12](#ch10fn12)])
    So GANs also have the potential to solve this gap that likely led to their existence
    in the first place.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们阐述了对抗样本的概念，并将它与 GANs 的联系变得更加具体。这是一个被低估的联系，但可以巩固你对这个具有挑战性的主题的理解。此外，对抗样本的一种防御手段就是
    GANs 本身！[^([12](#ch10fn12))] 因此，GANs 也具有解决这一差距的潜力，这可能是它们最初存在的原因。
- en: ^(12)
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([12](#ch10fn12))
- en: ''
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Jalal et al., 2019, [https://arxiv.org/pdf/1712.09196.pdf](https://arxiv.org/pdf/1712.09196.pdf).
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Jalal 等人于 2019 年发表的文章，[https://arxiv.org/pdf/1712.09196.pdf](https://arxiv.org/pdf/1712.09196.pdf)。
- en: 10.7\. Conclusion
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.7. 结论
- en: Adversarial examples are an important field, because even commercial computer
    vision products suffered from this shortcoming and can still be easily fooled
    by academics.^([[13](#ch10fn13)]) Beyond security and machine learning explainability
    applications, many practical uses remain in fairness and robustness.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本是一个重要的领域，因为即使是商业计算机视觉产品也受到了这种不足的影响，并且仍然可以被学术界轻易欺骗。[^([13](#ch10fn13))] 除了安全和机器学习可解释性应用之外，在公平性和鲁棒性方面还有许多实际用途。
- en: ^(13)
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([13](#ch10fn13))
- en: ''
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Black-Box Adversarial Attacks with Limited Queries and Information,” by
    Andrew Ilyas et al., 2018, [https://arxiv.org/abs/1804.08598](https://arxiv.org/abs/1804.08598).
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Andrew Ilyas 等人于 2018 年发表的“带有有限查询和信息限制的黑盒对抗攻击”，[https://arxiv.org/abs/1804.08598](https://arxiv.org/abs/1804.08598)。
- en: Furthermore, adversarial examples are an excellent way of solidifying your own
    understanding of deep learning and GANs. Adversarial examples take advantage of
    the difficulty in training classifiers in general and the relative ease of fooling
    the classifier in *one particular case*. The classifier has to make predictions
    for many images, and crafting a special offset to fool the classifier exactly
    right is easy because of the many degrees of freedom. As a result, we can easily
    get adversarial noise that completely changes the label of a picture without changing
    the image perceptibly.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对抗样本是巩固你对深度学习和 GANs 理解的极好方式。对抗样本利用了训练分类器的一般困难以及欺骗特定分类器的相对容易。分类器需要对许多图像进行预测，而制作一个特殊的偏移量来精确欺骗分类器是容易的，因为有很多自由度。因此，我们可以轻松地得到对抗噪声，它完全改变了图片的标签，而不会在感知上改变图像。
- en: Adversarial examples can be found in many domains and many areas of AI, not
    just deep learning or computer vision. But as you saw in the code, creating the
    ones in computer vision is not challenging. Defenses against these examples exist,
    and you saw one using GANs, but adversarial examples are far from being solved
    completely.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本可以在许多领域和AI的许多领域中找到，而不仅仅是深度学习或计算机视觉。但正如你在代码中看到的，在计算机视觉中创建这些样本并不具有挑战性。存在针对这些样本的防御措施，你看到了一个使用GANs的例子，但对抗样本远未完全解决。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Adversarial examples, which come from abusing the dimensionality of the problem
    space, are an important aspect of machine learning because they show us why GANs
    work and why some classifiers can be easily broken.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗样本，它们来自滥用问题空间的维度，是机器学习的一个重要方面，因为它们展示了GANs为什么能工作，以及为什么某些分类器容易被破坏。
- en: We can easily generate our own adversarial examples with real images and noise.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以轻松地使用真实图像和噪声生成自己的对抗样本。
- en: Few meaningful attack vectors can be used with adversarial examples.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对抗样本中，可以使用的有效攻击向量很少。
- en: Applications of adversarial examples include cybersecurity and machine learning
    fairness, and we can defend against them by using GANs.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗样本的应用包括网络安全和机器学习公平性，我们可以通过使用GANs来防御它们。
- en: Chapter 11\. Practical applications of GANs
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章：GANs的实际应用
- en: '*This chapter covers*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Use of GANs in medicine
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs在医学领域的应用
- en: Use of GANs in fashion
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs在时尚领域的应用
- en: As captivating as generating handwritten digits and turning apples into oranges
    may be, GANs can be used for a lot more. This chapter explores some of the practical
    applications of GANs. It is only fitting that this chapter focuses on areas where
    GANs have been harnessed for practical use. After all, one of our main goals with
    this book is to give you the knowledge and tools necessary to not only understand
    what has been accomplished with GANs to date, but also to empower you to find
    new applications of your choosing. There is no better place to start that journey
    than taking a look at several successful examples of just that.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 就像生成手写数字和将苹果变成橘子一样吸引人，生成对抗网络（GANs）可以用于更多领域。本章探讨了GANs的一些实际应用。本章专注于GANs在实际应用中被充分利用的领域，这是非常合适的。毕竟，我们编写这本书的主要目标之一是提供必要的知识和工具，不仅让你理解到目前为止GANs所取得的成就，还让你能够找到你选择的新的应用。没有比研究几个成功的例子更好的开始这段旅程的地方了。
- en: 'You have already seen several innovative use cases of GANs. [Chapter 6](../Text/kindle_split_016.xhtml#ch06)
    showed how Progressive GANs can create not only photorealistic renditions of human
    faces, but also samples of, arguably, much greater practical importance: medical
    mammograms. [Chapter 9](../Text/kindle_split_019.xhtml#ch09) showed how the CycleGAN
    can create realistic simulated virtual environments by translating clips from
    a video game into movie-like scenes, which can then be used to train self-driving
    cars.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了几个GANs的创新应用案例。[第6章](../Text/kindle_split_016.xhtml#ch06)展示了渐进式GANs不仅可以创建逼真的人类面部渲染，还可以创建具有更大实际重要性的样本：医学乳腺X光片。[第9章](../Text/kindle_split_019.xhtml#ch09)展示了CycleGAN如何通过将视频游戏中的剪辑转换为类似电影的场景来创建逼真的模拟虚拟环境，这些场景可以用来训练自动驾驶汽车。
- en: 'This chapter reviews GAN applications in greater detail. We will walk through
    what motivated these applications, what makes them uniquely suited to benefit
    from the advances made possible by GANs, and how their creators went about implementing
    them. Specifically, we will look at GAN applications in medicine and fashion.
    We chose these two fields based on the following criteria:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本章更详细地回顾了GANs的应用。我们将探讨是什么激发了这些应用，是什么使它们特别适合从GANs带来的进步中受益，以及它们的创造者是如何实施它们的。具体来说，我们将探讨GANs在医学和时尚领域的应用。我们选择这两个领域基于以下标准：
- en: They showcase not only academic but also, and primarily, the business value
    of GANs. They represent how the academic advances achieved by GAN researchers
    can be applied to solve real-world problems.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不仅展示了GANs的学术价值，而且更重要的是，展示了商业价值。它们代表了GAN研究人员通过GANs取得的学术进步如何应用于解决现实世界问题。
- en: They use GAN models that are understandable with the tools and techniques discussed
    in this book. Instead of introducing new concepts, we will look at how the models
    we implemented can be applied to uses other than the MNIST.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们使用的是本书中讨论的工具和技术可以理解的GAN模型。我们不会引入新的概念，而是将探讨我们实现的模型如何应用于除了MNIST以外的其他用途。
- en: They are understandable without the need for specialized domain expertise. For
    example, GAN applications in chemistry and physics tend to be hard to comprehend
    for anyone without a strong background in the given field.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们易于理解，无需专门的领域专业知识。例如，GAN在化学和物理学中的应用通常对没有该领域强大背景的人来说难以理解。
- en: Moreover, the chosen fields and the examples we selected serve to illustrate
    the versatility of GANs. In medicine, we show how GANs can be useful in situations
    with limited data. In fashion, we present the other extreme and explore GAN applications
    in scenarios where extensive datasets are available. Even if you have no interest
    in medicine or fashion, the tools and approaches that you will learn about in
    this chapter are applicable to countless other use cases.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们选择的领域和例子旨在展示GAN的通用性。在医学领域，我们展示了GAN在数据有限的情况下如何有用。在时尚领域，我们展示了另一个极端，并探讨了在大量数据集可用的情况下的GAN应用。即使你对医学或时尚没有兴趣，你将在本章中学到的工具和方法也适用于无数其他用例。
- en: Sadly, as is all too often the case, the practical applications we will review
    are virtually impossible to reproduce in a coding tutorial because of the proprietary
    or otherwise hard-to-obtain nature of the training data. Instead of a full coding
    tutorial like the ones throughout this book, we can provide only a detailed explanation
    of the GAN models and the implementation choices behind them. Accordingly, by
    the end of this chapter, you should be fully equipped to implement any of the
    applications in this chapter by making only small modifications to the GAN models
    we implemented earlier and feeding them a dataset for the given use case or one
    similar to it. With that, let’s dive in.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，正如经常发生的那样，我们将要回顾的实际应用几乎无法在编码教程中重现，这是因为训练数据是专有的或难以获得的。因此，我们无法提供像本书中其他部分那样的完整编码教程，而只能提供GAN模型及其背后实现选择的详细解释。相应地，到本章结束时，你应该已经完全准备好通过仅对之前实现的GAN模型进行少量修改，并为其提供特定用例或类似用例的数据集，来实施本章中的任何应用。有了这些，让我们深入探讨。
- en: 11.1\. GANs in medicine
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1. GAN在医学中的应用
- en: This section presents applications of GANs in medicine. Namely, we look at how
    to use GAN-produced synthetic data to enlarge a training dataset to help improve
    diagnostic accuracy.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了GAN在医学中的应用。具体来说，我们探讨如何使用GAN生成的合成数据来扩大训练数据集，以帮助提高诊断准确性。
- en: 11.1.1\. Using GANs to improve diagnostic accuracy
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.1. 使用GAN提高诊断准确性
- en: Machine learning applications in medicine face a range of challenges that lend
    the field well to benefiting from GANs. Perhaps most important, it is challenging
    to procure training datasets large enough for supervised machine learning algorithms
    because of difficulties involved in collecting medical data.^([[1](#ch11fn01)])
    Obtaining samples of medical conditions tends to be prohibitively expensive and
    impractical.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 医学中的机器学习应用面临一系列挑战，这些挑战使得该领域非常适合从GAN中受益。也许最重要的是，由于收集医疗数据所涉及的困难，获取足够大的训练数据集以供监督机器学习算法使用是具有挑战性的。[^[[1](#ch11fn01)]]
    获取医疗状况的样本往往既昂贵又不切实际。
- en: ¹
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Synthetic Data Augmentation Using GAN for Improved Liver Lesion Classification,”
    by Maayan Frid-Adar et al., 2018, [http://mng.bz/rPBg](http://mng.bz/rPBg).
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Maayan Frid-Adar等人于2018年发表的《使用GAN进行合成数据增强以改善肝脏病变分类》，[http://mng.bz/rPBg](http://mng.bz/rPBg)。
- en: Unlike datasets of handwritten letters for optical character recognition (OCR)
    or footage of roads for self-driving cars, which anyone can procure, examples
    of medical conditions are harder to come by, and they often require specialized
    equipment to collect. Not to mention the all-important considerations of patient
    privacy that limit how medical data can be collected and used.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与光学字符识别（OCR）的手写字母数据集或自动驾驶汽车的路面视频数据集不同，任何人都可以获取这些数据集，而医疗状况的例子则更难获得，并且通常需要专用设备来收集。更不用说患者隐私这一至关重要的考虑因素，它限制了医疗数据的收集和使用方式。
- en: In addition to difficulties in obtaining medical datasets, it is also challenging
    to properly label this data, a process that often requires annotations by people
    with expert knowledge of a given condition.^([[2](#ch11fn02)]) As a result, many
    medical applications have been unable to benefit from advances in deep learning
    and AI.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 除了获取医疗数据集的困难之外，正确标记这些数据也是一个挑战，这个过程通常需要具有特定状况专业知识的人进行标注。[^[[2](#ch11fn02)]] 因此，许多医学应用未能从深度学习和人工智能的进步中受益。
- en: ²
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: Many techniques have been developed to help address the problem of small labeled
    datasets. In [chapter 7](../Text/kindle_split_017.xhtml#ch07), you learned how
    GANs can be used to enhance the performance of classification algorithms in a
    semi-supervised setting. You saw how the SGAN achieved superior accuracy while
    using only a tiny subset of labels for training. This, however, addresses only
    half of the problem medical researchers face. Semi-supervised learning helps in
    situations in which we have a large dataset, but only a small portion of it is
    labeled. In many medical applications, having labels for a tiny portion of the
    dataset is only part of the problem—this small portion is often the only data
    we have! In other words, we do not have the luxury of thousands of additional
    samples from the same domain just waiting to be labeled or used in a semi-supervised
    setting.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了许多技术来帮助解决小标签数据集的问题。在[第7章](../Text/kindle_split_017.xhtml#ch07)中，你学习了如何使用GANs在半监督设置中提高分类算法的性能。你看到了SGAN如何仅使用极小部分标签进行训练就实现了优越的准确率。然而，这仅解决了医疗研究人员面临问题的一半。半监督学习有助于我们在拥有大量数据集但只有一小部分被标记的情况下。在许多医学应用中，拥有数据集一小部分标签只是问题的一部分——这小部分数据往往是唯一的数据！换句话说，我们没有成千上万的额外样本可供标记或用于半监督设置中的奢侈。
- en: Medical researchers strive to overcome the challenge of insufficient datasets
    by using data-augmentation techniques. For images, these include small tweaks
    and transformations such as scaling (zooming in and out), translations (moving
    left/right and up/down), and rotations.^([[3](#ch11fn03)]) These strategies allow
    a single example to be used to create many others, thereby expanding the dataset
    size. [Figure 11.1](#ch11fig01) shows examples of data augmentations commonly
    used in computer vision.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 医学研究人员通过使用数据增强技术来克服数据集不足的挑战。对于图像，这些包括缩放（放大和缩小）、平移（左右和上下移动）以及旋转等小的调整和变换。[图11.1](#ch11fig01)展示了在计算机视觉中常用的数据增强示例。
- en: ³
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-231
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: Figure 11.1\. Techniques used to enlarge a dataset by altering existing data
    include scaling (zooming in and out), translations (moving left/right and up/down),
    and rotations. Although effective at increasing dataset sizes, classic data augmentation
    techniques bring only limited additional data diversity.
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1\. 通过改变现有数据来扩大数据集的技术包括缩放（放大和缩小）、平移（左右和上下移动）以及旋转。尽管这些技术在增加数据集大小方面非常有效，但经典的数据增强技术只能带来有限的数据多样性。
- en: '![](../Images/11fig01_alt.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11fig01_alt.jpg)'
- en: '(Source: “Data Augmentation: How to Use Deep Learning When You Have Limited
    Data,” by Bharath Raj, 2018, [http://mng.bz/dxPD](http://mng.bz/dxPD).)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“数据增强：当数据有限时如何使用深度学习”，作者Bharath Raj，2018年，[http://mng.bz/dxPD](http://mng.bz/dxPD)。）
- en: As you may imagine, standard data augmentation has many limitations. For one,
    small modifications yield examples that do not diverge far from the original image.
    As a result, the additional examples do not add much variety to help the algorithm
    learn to generalize.^([[4](#ch11fn04)]) In the case of handwritten digits, for
    example, we want to see the number 6 rendered in different writing styles, not
    just permutations of the same underlying image.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所想象，标准数据增强有许多局限性。首先，小的修改只会产生与原始图像差异不大的示例。因此，这些额外的示例并没有增加很多多样性，无法帮助算法学习泛化。[例如，在处理手写数字时，我们希望看到数字6以不同的书写风格呈现，而不仅仅是同一基本图像的排列组合。]^([[4](#ch11fn04)])
- en: ⁴
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: In the case of medical diagnostics, we want different examples of the same underlying
    pathology. Enriching a dataset with synthetic examples, such as those produced
    by GANs, has the potential to further enrich the available data beyond traditional
    augmentation techniques. That is precisely what the Israeli researchers Maayan
    Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan set
    out to investigate.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学诊断的情况下，我们希望看到同一基本病理的不同示例。通过使用如GANs生成的合成示例来丰富数据集，有可能在传统增强技术之外进一步丰富可用数据。这正是以色列研究人员Maayan
    Frid-Adar、Eyal Klang、Michal Amitai、Jacob Goldberger和Hayit Greenspan着手研究的问题。
- en: Encouraged by GANs’ ability to synthesize high-quality images in virtually any
    domain, Frid-Adar and his colleagues decided to explore the use of GANs for medical
    data augmentation. They chose to focus on improving the classification of liver
    lesions. One of their primary motivations for focusing on the liver is that this
    organ is one of the three most common sites for metastatic cancer, with over 745,000
    deaths caused by liver cancer in 2012 alone.^([[5](#ch11fn05)]) Accordingly, tools
    and machine learning models that would help doctors diagnose at-risk patients
    have the potential to save lives and improve outcomes for countless patients.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 受到GAN在几乎任何领域都能合成高质量图像的能力的鼓舞，Frid-Adar及其同事决定探索GAN在医学数据增强中的应用。他们选择专注于提高肝脏病变的分类。他们专注于肝脏的一个主要动机是，这个器官是三种最常见的转移性癌症发生地之一，仅在2012年就有超过74.5万人因肝癌而死亡。[5](#ch11fn05)因此，有助于医生诊断高风险患者的工具和机器学习模型有可能拯救生命并改善无数患者的预后。
- en: ⁵
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-242
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Cancer Incidence and Mortality Worldwide: Sources, Methods, and Major
    Patterns in GLOBOCAN 2012,” by J. Ferlay et al., 2015, *International Journal
    of Cancer*, [https://www.ncbi.nlm.nih.gov/pubmed/25220842](https://www.ncbi.nlm.nih.gov/pubmed/25220842).'
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见J. Ferlay等人撰写的“全球癌症发病率与死亡率：GLOBOCAN 2012的数据来源、方法和主要模式”，2015年，*国际癌症杂志*，[https://www.ncbi.nlm.nih.gov/pubmed/25220842](https://www.ncbi.nlm.nih.gov/pubmed/25220842)。
- en: 11.1.2\. Methodology
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.2. 方法论
- en: 'Frid-Adar and his team found themselves in a catch-22 situation: their goal
    was to train a GAN to augment a small dataset, but GANs themselves need a lot
    of data to train. In other words, they wanted to use GANs to create a large dataset,
    but they needed a large dataset to train the GAN in the first place.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Frid-Adar及其团队发现自己陷入了一个两难境地：他们的目标是训练一个GAN来增强一个小数据集，但GAN本身需要大量的数据来训练。换句话说，他们想利用GAN来创建一个大数据集，但首先他们需要一个大数据集来训练GAN。
- en: Their solution was ingenious. First, they used standard data-augmentation techniques
    to create a larger dataset. Second, they used this dataset to train a GAN to create
    synthetic examples. Third, they used the augmented dataset from step 1 along with
    the GAN-produced synthetic examples from step 2 to train a liver lesion classifier.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的解决方案非常巧妙。首先，他们使用标准的数据增强技术来创建一个更大的数据集。其次，他们使用这个数据集来训练一个GAN以创建合成示例。第三，他们使用步骤1中增强的数据集以及步骤2中由GAN产生的合成示例来训练一个肝脏病变分类器。
- en: The GAN model the researchers used was a variation on the Deep Convolutional
    GAN (DCGAN) covered in [chapter 4](../Text/kindle_split_013.xhtml#ch04). Attesting
    to the applicability of GANs across a wide array of datasets and scenarios, Frid-Adar
    et al. had to make only minor tweaks and customizations to make the DCGAN work
    for their use case. As evidenced by [figure 11.2](#ch11fig02), the only parts
    of the model that needed adjustment were the dimensions of the hidden layers and
    the dimensions of the output from the Generator and input into the Discriminator
    network.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员使用的GAN模型是第4章中介绍的深度卷积GAN（DCGAN）的一个变体。[第4章](../Text/kindle_split_013.xhtml#ch04)。为了证明GAN在广泛的数据集和场景中的适用性，Frid-Adar等人只需进行一些小的调整和定制，就能使DCGAN适用于他们的用例。正如[图11.2](#ch11fig02)所示，需要调整的模型部分只有隐藏层的维度以及生成器输出的维度和判别器网络的输入维度。
- en: Figure 11.2\. The DCGAN model architecture employed by Frid-Adar et al. to generate
    synthetic images of liver lesions to augment their dataset, aiming to improve
    classification accuracy. The model architecture is similar to the DCGAN in [chapter
    4](../Text/kindle_split_013.xhtml#ch04), underscoring the applicability of GANs
    across a wide array of datasets and use cases. (Note that the figure shows only
    the GAN flow for fake examples.)
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2。Frid-Adar等人采用的DCGAN模型架构，用于生成肝脏病变的合成图像以增强他们的数据集，旨在提高分类精度。该模型架构与第4章中的DCGAN相似，强调了GAN在广泛的数据集和用例中的适用性。（注意，该图只显示了伪造示例的GAN流程。）
- en: '![](../Images/11fig02_alt.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11fig02_alt.jpg)'
- en: '(Source: Frid-Adar et al., 2018, [http://mng.bz/rPBg](http://mng.bz/rPBg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Frid-Adar等人，2018年，[http://mng.bz/rPBg](http://mng.bz/rPBg)）
- en: Instead of 28 × 28 × 1-sized images like those in the MNIST dataset, this GAN
    deals with images that are 64 × 64 × 1\. As noted in their paper, Frid-Adar et
    al. also used 5 × 5 convolutional kernels—but then again, that is also only a
    small change to the network hyperparameters. Except for the image size, which
    is given by the training data, all these adjustments were in all likelihood determined
    by trial and error. The researchers kept tweaking the parameters until the model
    produced satisfactory images.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与MNIST数据集中的28 × 28 × 1大小的图像不同，这个GAN处理的是64 × 64 × 1的图像。正如他们论文中提到的，Frid-Adar等人还使用了5
    × 5的卷积核——然而，这仅仅是网络超参数的一个小改动。除了由训练数据给出的图像大小之外，所有这些调整很可能都是通过试错确定的。研究人员不断调整参数，直到模型生成令人满意的图像。
- en: Before we review how well the approach devised by Frid-Adar and his team worked,
    let’s pause for a moment and appreciate how far your understanding of GANs has
    progressed. As early as [chapter 4](../Text/kindle_split_013.xhtml#ch04) in this
    book, you had already learned enough about GANs to apply them to a real-world
    scenario, discussed in a paper presented at the 2018 International Symposium on
    Biomedical Imaging.^([[6](#ch11fn06)])
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们回顾Frid-Adar及其团队设计的方法效果如何之前，让我们暂停一下，来欣赏一下你对GANs的理解已经进步到了何种程度。早在本书的[第4章](../Text/kindle_split_013.xhtml#ch04)中，你就已经学到了足够多的关于GANs的知识，可以将其应用于一个真实世界的场景，该场景在2018年国际生物医学成像研讨会上的一篇论文中有所讨论。（[6](#ch11fn06)）
- en: ⁶
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-254
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Frid-Adar et al., 2018, [http://mng.bz/rPBg](http://mng.bz/rPBg).
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 见Frid-Adar等，2018，[http://mng.bz/rPBg](http://mng.bz/rPBg)。
- en: 11.1.3\. Results
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.3. 结果
- en: Using DCGAN for data augmentation, Frid-Adar and his team achieved a significant
    improvement in classification accuracy compared to the baseline (standard data
    augmentation only).^([[7](#ch11fn07)]) Their results are summarized in [figure
    11.3](#ch11fig03), which shows the classification accuracy (y-axis) as the number
    of training examples (x-axis) increases.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DCGAN进行数据增强，Frid-Adar及其团队与仅使用标准数据增强的基线相比，实现了分类准确率的显著提升。[7](#ch11fn07)他们的结果总结在[图11.3](#ch11fig03)中，该图显示了随着训练样本数量（x轴）增加，分类准确率（y轴）的变化。
- en: ⁷
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: 'Figure 11.3\. This chart shows classification accuracy as new examples are
    added using two dataset augmentation strategies: standard/classic data augmentation;
    and augmentation using synthetic examples produced by DCGAN. Using standard augmentation
    (dotted line), the classification performance peaks at around 80%. Using GAN-created
    examples (dashed line) boosts the accuracy to over 85%.'
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3。此图表显示了使用两种数据集增强策略添加新示例时的分类准确率：标准/经典数据增强；以及使用由DCGAN生成的合成示例进行增强。使用标准增强（点线），分类性能在约80%时达到峰值。使用由GAN创建的示例（虚线）将准确率提升到超过85%。
- en: '![](../Images/11fig03_alt.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11fig03_alt.jpg)'
- en: '(Source: Frid-Adar et al., 2018, [http://mng.bz/rPBg](http://mng.bz/rPBg).)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Frid-Adar等，2018，[http://mng.bz/rPBg](http://mng.bz/rPBg)）
- en: The dotted line depicts classification performance for classic data augmentation.
    The performance improves as the quantity of new (augmented) training examples
    increases; however, the improvement plateaus around the accuracy of 80%, beyond
    which additional examples fail to yield improvement.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 点线图展示了经典数据增强的分类性能。随着新（增强）训练样本数量的增加，性能得到提升；然而，当准确率达到80%左右时，提升趋于平缓，超出这个范围后，额外的样本无法带来进一步的提升。
- en: The dashed line shows the additional increase in accuracy achieved by augmenting
    the dataset using GAN-produced synthetic examples. Starting from the point beyond
    which additional classically augmented examples stopped improving accuracy, Frid-Adar
    et al. added synthetic data produced by their DCGAN. The classification performance
    improved from around 80% to over 85%, demonstrating the usefulness of GANs.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 虚线显示了通过使用由GAN生成的合成示例增强数据集所获得的额外准确率提升。从额外经典增强示例停止提升准确率的点开始，Frid-Adar等人添加了由他们的DCGAN生成的合成数据。分类性能从大约80%提升到超过85%，证明了GANs的有用性。
- en: Improved classification of liver lesions is only one of many data-constrained
    use cases in medicine that can benefit from data augmentation through GAN-produced
    synthetic examples. For example, a team of British researchers led by Christopher
    Bowles from the Imperial College London harnessed GANs (in particular, the Progressive
    GANs discussed in [chapter 6](../Text/kindle_split_016.xhtml#ch06)) to boost performance
    on brain segmentation tasks.^([[8](#ch11fn08)]) Crucially, an improvement in performance
    can unlock a model’s usability in practice, especially in fields like medicine,
    where accuracy may mean the difference between life and death.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 改善肝脏病变的分类只是医学中许多数据受限的应用场景之一，这些场景可以通过 GANs 产生的合成示例进行数据增强而受益。例如，由伦敦帝国理工学院 Christopher
    Bowles 领导的一组英国研究人员利用 GANs（特别是第 6 章中讨论的渐进式 GANs）来提高脑部分割任务的表现。^([[8](#ch11fn08)])
    关键的是，性能的提高可以解锁模型在实际应用中的可用性，特别是在医学等领域，准确性可能意味着生死之别。
- en: ⁸
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “GAN Augmentation: Augmenting Training Data Using Generative Adversarial
    Networks,” by Christopher Bowles et al., 2018, [https://arxiv.org/abs/1810.10863](https://arxiv.org/abs/1810.10863).'
  id: totrans-269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参见 Christopher Bowles 等人于 2018 年发表的“GAN Augmentation: Augmenting Training Data
    Using Generative Adversarial Networks”，[https://arxiv.org/abs/1810.10863](https://arxiv.org/abs/1810.10863).'
- en: 'Let’s switch gears and explore applications of GANs in a field with much lower
    stakes and a whole different set of considerations and challenges: fashion.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转换一下话题，探索 GANs 在一个风险较低且具有完全不同考虑和挑战领域的应用：时尚。
- en: 11.2\. GANs in fashion
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 时尚领域的 GANs
- en: Unlike medicine, for which data is hard to obtain, researchers in fashion are
    fortunate to have huge datasets at their disposal. Sites like Instagram and Pinterest
    have countless images of outfits and clothing items, and retail giants like Amazon
    and eBay have data on millions of purchases of everything from socks to dresses.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据难以获得的医学不同，时尚研究人员幸运地拥有大量可用的数据集。像 Instagram 和 Pinterest 这样的网站上有无数套装和服装的图片，而像亚马逊和
    eBay 这样的零售巨头拥有从袜子到连衣裙的数百万件商品的购买数据。
- en: In addition to data availability, many other characteristics make fashion well-suited
    to AI applications. Fashion tastes vary greatly from customer to customer, and
    the ability to personalize content has the potential to unlock significant business
    benefits. In addition, fashion trends change frequently, and it is vital for brands
    and retailers to react quickly and adapt to customers’ shifting preferences.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据可用性之外，许多其他特性使时尚非常适合 AI 应用。时尚品味因顾客而异，个性化内容的能力有可能解锁重大的商业利益。此外，时尚趋势经常变化，对于品牌和零售商来说，快速反应并适应顾客不断变化的偏好至关重要。
- en: In this section, we explore some of the innovative uses of GANs in fashion.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 GANs 在时尚领域的一些创新应用。
- en: 11.2.1\. Using GANs to design fashion
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1\. 使用 GANs 设计时尚
- en: From drone deliveries to cashier-less grocery stores, Amazon is no stranger
    to headline news about its futuristic endeavors. In 2017, Amazon earned another
    one, this time about the company’s ambition to develop an AI fashion designer
    by using no other technique than GANs.^([[9](#ch11fn09)]) The story, published
    in *MIT Technology Review*, is unfortunately short on details besides the mention
    of using GANs to design new products matching a particular style.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从无人机配送到无收银员杂货店，亚马逊对关于其未来探索的头条新闻并不陌生。2017 年，亚马逊又获得了一个，这次是关于公司利用 GANs（生成对抗网络）开发
    AI 时尚设计师的雄心。这个故事发表在《MIT Technology Review》上，不幸的是，除了提到使用 GANs 设计符合特定风格的新产品外，细节很少。^([[9](#ch11fn09)])
- en: ⁹
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-278
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Amazon Has Developed an AI Fashion Designer,” by Will Knight, 2017, *MIT
    Technology Review*, [http://mng.bz/VPqX](http://mng.bz/VPqX).
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Will Knight 于 2017 年发表的“Amazon Has Developed an AI Fashion Designer”，《MIT
    Technology Review》，[http://mng.bz/VPqX](http://mng.bz/VPqX).
- en: 'Luckily, researchers from Adobe and the University of California, San Diego,
    published a paper in which they set out to accomplish the same goal.^([[10](#ch11fn10)])
    Their approach can give us a hint about what goes on behind the secretive veil
    of Amazon’s AI research labs seeking to reinvent fashion. Using a dataset of hundreds
    of thousands of users, items, and reviews scraped from Amazon, lead author Wang-Cheng
    Kang and his collaborators trained two separate models: one that recommends fashion
    and the other that creates it.^([[11](#ch11fn11)])'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Adobe公司和加州圣地亚哥大学的学者们发表了一篇论文，其中他们着手实现相同的目标.^([[10](#ch11fn10)])他们的方法可以给我们一些线索，了解亚马逊AI研究实验室在秘密的面纱背后正在进行的时尚革命。他们使用从亚马逊爬取的数十万个用户、物品和评论的数据集，主要作者王成康和他的合作者训练了两个不同的模型：一个用于推荐时尚，另一个用于创造时尚.^([[11](#ch11fn11)])
- en: ^(10)
  id: totrans-281
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “This AI Learns Your Fashion Sense and Invents Your Next Outfit,” by Jackie
    Snow, 2017, *MIT Technology Review*, [http://mng.bz/xlJ8](http://mng.bz/xlJ8).
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅2017年Jackie Snow在《麻省理工学院技术评论》上发表的“This AI Learns Your Fashion Sense and Invents
    Your Next Outfit”，[http://mng.bz/xlJ8](http://mng.bz/xlJ8)。
- en: ^(11)
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Visually-Aware Fashion Recommendation and Design with Generative Image
    Models,” by Wang-Cheng Kang et al., 2017, [https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231).
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Wang-Cheng Kang等人2017年发表的“Visually-Aware Fashion Recommendation and Design
    with Generative Image Models”，[https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231)。
- en: 'For our purposes, we can treat the recommendation model as a black box. The
    only thing we need to know about the model is what it does: for any person-item
    pair, it returns a preference score; the greater the score, the better match the
    item is for the person’s tastes. Nothing too unusual.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们可以将推荐模型视为一个黑盒。我们唯一需要了解关于模型的信息是它做什么：对于任何一个人-物品对，它返回一个偏好分数；分数越高，物品与个人口味的匹配度越好。没有什么特别不寻常的。
- en: 'The latter model is a lot more novel and interesting—not only because it uses
    GANs, but also thanks to the two creative applications Kang and his colleagues
    devised:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 后者模型更加新颖和有趣——不仅因为它使用了生成对抗网络（GANs），还因为康和他的同事们设计了两个富有创意的应用：
- en: Creating new fashion items matching the fashion taste of a given individual
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建符合特定个人时尚品味的全新时尚物品
- en: Suggesting personalized alterations to existing items based on an individual’s
    fashion preferences.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据个人的时尚偏好对现有物品提出个性化修改建议。
- en: In this section, we explore how Kang and his team achieved these goals.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨康和他的团队是如何实现这些目标的。
- en: 11.2.2\. Methodology
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2. 方法论
- en: 'Let’s start with the model. Kang and his colleagues use a Conditional GAN (CGAN),
    with a product’s category as the conditioning label. Their dataset has six categories:
    tops (men’s and women’s), bottoms (men’s and women’s), and shoes (men’s and women’s).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从模型开始。康和他的同事们使用了一个条件生成对抗网络（CGAN），以产品的类别作为条件标签。他们的数据集包含六个类别：上衣（男性和女性），下装（男性和女性），和鞋子（男性和女性）。
- en: Recall that in [chapter 8](../Text/kindle_split_018.xhtml#ch08), we used MNIST
    labels to teach a CGAN to produce any handwritten digit we wanted. In a similar
    fashion (pun intended), Kang et al. use the category labels to train their CGAN
    to generate fashion items belonging to a specified category. Even though we are
    now dealing with shirts and pants instead of threes and fours, the CGAN model
    setup is almost identical to the one we implemented in [chapter 8](../Text/kindle_split_018.xhtml#ch08).
    The Generator uses random noise *z* and conditioning information (label/category
    *c*) to synthesize an image, and the Discriminator outputs a probability that
    a particular image-category pair is real rather than fake. [Figure 11.4](#ch11fig04)
    details the network architecture Kang et al. used.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在[第8章](../Text/kindle_split_018.xhtml#ch08)中，我们使用MNIST标签来教一个CGAN生成我们想要的任何手写数字。以类似的方式（有意为之），康等人使用类别标签来训练他们的CGAN生成属于指定类别的时尚物品。尽管我们现在处理的是衬衫和裤子而不是三和四，但CGAN模型设置几乎与我们第8章中实现的相同。生成器使用随机噪声*z*和条件信息（标签/类别*c*）来合成图像，而判别器输出一个特定图像-类别对是真实还是虚假的概率。[图11.4](#ch11fig04)详细说明了康等人使用的网络架构。
- en: Figure 11.4\. The architectures of the CGAN Generator and the Discriminator
    networks that Kang etal. use in their study. The label c represents the category
    of clothing. The researchers use it as the conditioning label to guide the Generator
    to synthesize an image matching the given category, and the Discriminator to identify
    real image-category pairs.
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4. Kang等人研究中使用的CGAN生成器和判别器网络的架构。标签c代表服装类别。研究人员将其用作条件标签，以引导生成器合成与给定类别匹配的图像，并使判别器识别真实图像-类别对。
- en: '![](../Images/11fig04_alt.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11fig04_alt.jpg)'
- en: '(Source: Kang et al., 2017, [https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231).)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Kang等人，2017，[https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231)。）
- en: 'Each box represents a layer; *fc* stands for *fully connected layer*; *st*
    denotes *strides* for the convolutional kernel whose dimensions (width × height)
    are given as the first two numbers in the conv/deconv layers; and *deconv* and
    *conv* denote what kind of layer is used: a regular convolution or a transposed
    convolution, respectively. The number directly after the *conv* or *deconv* sets
    the depth of the layer or, equivalently, the number of convolutional filters used.
    *BN* tells us that batch normalization was used on the output of the given layer.
    Also, notice that Kang et al. chose to use least squares loss instead of cross-entropy
    loss.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 每个框代表一层；*fc*代表*全连接层*；*st*表示卷积核的步长，其尺寸（宽度×高度）作为卷积/反卷积层的第一个两个数字给出；*deconv*和*conv*表示使用的层类型：分别是常规卷积或转置卷积。*conv*或*deconv*后面的数字设置层的深度或等价地，使用的卷积核数量。*BN*告诉我们，在给定层的输出上使用了批归一化。注意，Kang等人选择使用最小二乘损失而不是交叉熵损失。
- en: 'Equipped with a CGAN capable of producing realistic clothing items for each
    of the top-level categories in their dataset, Kang and his colleagues tested it
    on two applications with significant practical potential: creating new personalized
    items and making personalized alterations to existing items.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了能够为数据集中每个顶级类别生成逼真服装物品的CGAN，Kang和他的同事们对其进行了两个具有重大实际潜力的应用的测试：创建新的个性化物品和对现有物品进行个性化修改。
- en: 11.2.3\. Creating new items matching individual preferences
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.3. 创建符合个人偏好的新项目
- en: 'To ensure that the produced images are customized to an individual’s fashion
    taste, Kang and his colleagues came up with an ingenious approach. They started
    off with the following insight: given that their recommendation model assigns
    scores to *existing* items based on how much a person would like the given item,
    the ability to generate *new* items maximizing this preference score would likely
    yield items matching the person’s style and taste.^([[12](#ch11fn12)])'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保产生的图像符合个人的时尚品味，Kang和他的同事们提出了一种巧妙的方法。他们从以下洞察开始：鉴于他们的推荐模型根据一个人对给定物品的喜好程度为*现有*物品分配分数，生成*新*物品以最大化这种偏好分数的能力可能会产生符合个人风格和品味的物品。[12](#ch11fn12)
- en: ^(12)
  id: totrans-302
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（12）
- en: ''
  id: totrans-303
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-304
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: Borrowing a term from economics and choice theory,^([[13](#ch11fn13)]) Kang
    et al. call this process *preference maximization*. What is unique about Kang
    et al.’s approach is that their universe of possible items is not limited to the
    corpus of training data or even the entire Amazon catalog. Thanks to their CGAN,
    they can fine-tune the generation of new items to virtually infinite granularity.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 借用经济学和选择理论中的一个术语，[13](#ch11fn13) Kang等人称这个过程为*偏好最大化*。Kang等人的方法独特之处在于，他们可能的物品宇宙不仅限于训练数据集的语料库，甚至不是整个亚马逊目录。多亏了他们的CGAN，他们可以将新物品的生成微调到几乎无限的粒度。
- en: ^(13)
  id: totrans-306
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（13）
- en: ''
  id: totrans-307
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Introduction to Choice Theory,” by Jonathan Levin and Paul Milgrom, 2004,
    [http://mng.bz/AN2p](http://mng.bz/AN2p).
  id: totrans-308
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 见“选择理论导论”，作者：Jonathan Levin和Paul Milgrom，2004年，[http://mng.bz/AN2p](http://mng.bz/AN2p)。
- en: The next problem Kang and his colleagues had to solve was ensuring that the
    CGAN Generator would produce a fashion item maximizing individual preference.
    After all, their CGAN was trained to produce realistic-looking images for only
    a given category, *not* a given person. One possible option would be to keep generating
    images and check their preference score until we happen upon one whose score is
    sufficiently high. However, given the virtually infinite variations of the images
    that can be generated, this approach would be extremely inefficient and time-consuming.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 下一问题康和他的同事们需要解决的是确保CGAN生成器能够生成最大化个人偏好的时尚单品。毕竟，他们的CGAN被训练来只为给定类别生成看起来逼真的图像，*而不是*为特定个人。一个可能的选择是持续生成图像并检查它们的偏好分数，直到我们偶然发现一个分数足够高的图像。然而，考虑到可以生成的图像的几乎无限变化，这种方法将非常低效且耗时。
- en: 'Instead, Kang and his team solved the issue by framing it as an optimization
    problem: in particular, constraint maximization. The constraint (the boundary
    within which their algorithm had to operate) is the size of the latent space,
    given by the size of the vector *z*. Kang et al. used the standard size (100-dimensional
    vector) with each number in [–1, 1] range. To make the values differentiable so
    that they can be used in an optimization algorithm, the authors set each element
    in the vector *z* to the *tanh* function, initialized randomly.^([[14](#ch11fn14)])'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，康和他的团队通过将其构造成一个优化问题来解决这个问题：特别是约束最大化。约束（他们的算法必须操作的边界）是潜在空间的大小，由向量*z*的大小给出。Kang等人使用了标准大小（100维向量），每个数字在[–1,
    1]范围内。为了使这些值可微分，以便它们可以在优化算法中使用，作者将向量*z*中的每个元素设置为*tanh*函数，随机初始化。^([14](#ch11fn14)）
- en: ^(14)
  id: totrans-311
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([14](#ch11fn14))
- en: ''
  id: totrans-312
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Kang et al., 2017, [https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231).
  id: totrans-313
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Kang等人，2017年，[https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231)。
- en: The researchers then employed gradient ascent. *Gradient ascent* is just like
    gradient descent, except that instead of *minimizing* a cost function by iteratively
    moving in the direction of the steepest *decrease*, we are *maximizing* a reward
    function (in this case, the score given by the recommendation model) by iteratively
    moving in the direction of the steepest *increase*.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员随后采用了梯度上升法。*梯度上升*与梯度下降类似，只不过我们不是通过迭代地向最陡的*下降*方向移动来*最小化*成本函数，而是通过迭代地向最陡的*增加*方向移动来*最大化*奖励函数（在这种情况下，是推荐模型给出的分数）。
- en: Kang et al.’s results are shown in [figure 11.5](#ch11fig05), which compares
    the top three images from the dataset with the top three generated images for
    six different individuals. Attesting to the ingenuity of Kang et al.’s solution,
    the examples they produced have higher preference scores, suggesting that they
    are a better match for the shoppers’ style and preferences.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Kang等人展示的结果见[图11.5](#ch11fig05)，该图比较了数据集中排名前三的图像与六个不同个人生成的排名前三的图像。Kang等人解决方案的独创性得到了证实，他们产生的示例具有更高的偏好分数，这表明它们与购物者的风格和偏好更匹配。
- en: Figure 11.5\. In the results Kang et al. present in their paper, every image
    is annotated with its preference score. Each row shows results for a different
    shopper and product category (men’s and women’s tops, men’s and women’s bottoms,
    and men’s and women’s shoes).
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.5。在Kang等人论文中呈现的结果中，每张图像都标注了其偏好分数。每一行显示了一个不同购物者和产品类别（男性和女性上衣、男性和女性下装、男性和女性鞋类）的结果。
- en: '![](../Images/11fig05_alt.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5](../Images/11fig05_alt.jpg)'
- en: '(Source: Kang et al., 2017, [https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231).)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Kang等人，2017年，[https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231)）
- en: The three columns on the left show the items from the dataset with the highest
    scores; the three columns on the right show generated items with the highest scores.
    Based on the preference score, the generated images are a better match for the
    shoppers’ preferences.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的三列显示了数据集中得分最高的物品；右侧的三列显示了得分最高的生成物品。根据偏好分数，生成的图像与购物者的偏好更匹配。
- en: Kang and his team didn’t stop there. In addition to creating new items, they
    explored whether the model they developed could be used to make changes to existing
    items, tailored to an individual’s style. Given the highly subjective nature of
    fashion shopping, having the ability to alter a garment until it is “just right”
    has significant potential business benefits. Let’s see how Kang et al. went about
    solving this challenge.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 康和他的团队并没有止步于此。除了创造新物品外，他们还探索了他们开发的模型是否可以用来对现有物品进行修改，以适应个人的风格。鉴于时尚购物的主观性很强，能够调整服装直到“恰到好处”，具有显著的商业潜力。让我们看看康等人是如何解决这个挑战的。
- en: 11.2.4\. Adjusting existing items to better match individual preferences
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.4\. 调整现有物品以更好地匹配个人偏好
- en: Recall that the numbers in the latent space (represented by the input vector
    *z*) have real-world meaning, and that vectors that are mathematically close to
    one another (as measured by their distance in the high-dimensional space they
    occupy) tend to produce images that are similar in terms of content and style.
    Accordingly, as Kang et al. point out, in order to generate variations of some
    image *A*, all we need to do is to find the latent vector *zA* that the Generator
    would use to create the image. Then, we could produce images from neighboring
    vectors to generate similar images.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，潜在空间中的数字（由输入向量*z*表示）具有现实意义，并且数学上彼此接近的向量（通过它们在占据的高维空间中的距离来衡量）往往会产生在内容和风格上相似的内容。因此，正如康等人所指出的，为了生成某些图像*A*的变体，我们只需要找到生成器用来创建该图像的潜在向量*zA*。然后，我们可以从邻近的向量生成图像，以生成相似的图像。
- en: To make it a little less abstract, let’s look at a concrete example using our
    favorite dataset, the MNIST. Consider an input vector *z’* that, when fed into
    the Generator, produces an image of the number 9\. If we then feed the vector
    *z”* that is, mathematically speaking, very close to *z’* in the 100-dimensional
    latent space the vectors occupy, then *z”* will produce another, slightly different,
    image of the number 8\. This is illustrated in [figure 11.6](#ch11fig06). You
    saw a little bit of this back in [chapter 2](../Text/kindle_split_011.xhtml#ch02).
    In the context of variational autoencoders, the intermediate/compressed representation
    works just like *z* does in the world of GANs.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让它不那么抽象，让我们用一个具体的例子来看，使用我们最喜欢的数据集MNIST。考虑一个输入向量*z’*，当将其输入生成器时，会产生数字9的图像。如果我们然后输入向量*z”*，从数学上讲，它在100维潜在空间中与*z’*非常接近，那么*z”*将产生另一个稍微不同的数字8的图像。这如图11.6所示。你曾在[第2章](../Text/kindle_split_011.xhtml#ch02)中看到过一些这样的例子。在变分自编码器的背景下，中间/压缩表示与GAN世界中的*z*起的作用一样。
- en: Figure 11.6\. Variations on the digit 9 obtained by moving around in the latent
    space (image reproduced from [chapter 2](../Text/kindle_split_011.xhtml#ch02)).
    Nearby vectors produce variations on the same digit. For example, notice that
    as we move from left to right in the first row, the numeral 9 starts off being
    slightly right-slanted but eventually turns fully upright. Also notice that as
    we move far enough away, the number 9 morphs into another, visually similar digit.
    Progressive variations like these apply equally to more complex datasets, where
    the variations tend to be more nuanced.
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.6\. 在潜在空间中移动得到的数字9的变体（图片来自[第2章](../Text/kindle_split_011.xhtml#ch02)）。邻近的向量会产生相同数字的变体。例如，注意当我们从第一行的左边移动到右边时，数字9最初是略微右倾的，但最终完全直立。也请注意，当我们足够远离时，数字9会变成另一个视觉上相似的数字。这种渐进的变体同样适用于更复杂的数据集，其中变体往往更加微妙。
- en: '![](../Images/11fig06_alt.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11fig06_alt.jpg)'
- en: Of course, in fashion, things are more nuanced. After all, a photo of a dress
    is incomparably more complex than a grayscale image of a numeral. Moving in the
    latent space around a vector producing, say, a T-shirt, can produce a T-shirt
    in different colors, patterns, and styles (V-neck as opposed to crew-neck, for
    example). It all depends on the types of encodings and meanings the Generator
    has internalized during training. The best way to find out is to try.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在时尚领域，事情更加微妙。毕竟，一件裙子的照片比数字的灰度图像复杂得多。在生成T恤的向量周围在潜在空间中移动，可以产生不同颜色、图案和风格的T恤（例如V领与圆领相比）。这完全取决于生成器在训练期间内化的编码和意义类型。最好的办法是尝试一下。
- en: 'This brings us to the next challenge Kang and his team had to overcome. In
    order for the preceding approach to work, we need the vector *z* for the image
    we want to alter. This would be straightforward if we wanted to modify a synthetic
    image: we can just record the vector *z* each time we generate an image so that
    we can refer to it later. What complicates the situation in our scenario is that
    we want to modify a *real* image.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们来到了康和他的团队必须克服的下一个挑战。为了使前面的方法有效，我们需要想要修改的图像的向量 *z*。如果我们想要修改一个合成图像，这将很简单：我们每次生成图像时都可以记录向量
    *z*，以便以后可以引用它。在我们的场景中，使情况复杂化的是，我们想要修改一个*真实*图像。
- en: By definition, a real image cannot have been produced by the Generator, so there
    is no vector *z*. The best we can do is to find latent space representation of
    a generated image as close as possible to the one we seek to modify. Put differently,
    we have to find a vector *z* that the Generator uses to synthesize an image similar
    to the real image, and use it as a proxy for the hypothetical *z* that would have
    produced the real image.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，真实图像不可能是由生成器产生的，因此没有向量 *z*。我们能做到的最好的事情是找到与我们要修改的图像尽可能接近的生成图像的潜在空间表示。换句话说，我们必须找到一个生成器用于合成与真实图像相似的图像的向量
    *z*，并将其用作假设的*z*的代理，该*z*本应产生真实图像。
- en: That is precisely what Kang et al. did. Just as before, they start by formulating
    the scenario as an optimization problem. They define a loss function in terms
    of the so-called *reconstruction loss* (a measure of the difference between two
    images; the greater the loss, the more different a given pair of images is from
    one another).^([[15](#ch11fn15)]) Having formulated the problem in this way, Kang
    et al. then iteratively find the closest possible generated image for any real
    image by using gradient descent (minimizing the reconstruction loss). Once we
    have a fake image that is similar to the real image (and hence also the vector
    *z* used to produce it), we can modify it through the latent space manipulations.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是康等人所做的事情。正如之前一样，他们首先将场景制定为一个优化问题。他们定义了一个损失函数，称为所谓的*重建损失*（两个图像之间差异的度量；损失越大，给定的一对图像之间的差异就越大）。^([[15](#ch11fn15)])
    以这种方式制定问题后，康等人随后通过梯度下降（最小化重建损失）迭代地找到与任何真实图像最接近的可能生成的图像。一旦我们得到一个与真实图像相似（因此也是用于生成它的向量
    *z*）的假图像，我们就可以通过潜在空间操作来修改它。
- en: ^(15)
  id: totrans-330
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(15)
- en: ''
  id: totrans-331
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ibid.
  id: totrans-332
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同上。
- en: 'This is where the approach Kang and his colleagues devised shows its full potential.
    We can move around the latent space to points that generate images similar to
    the one we want to modify, while also optimizing for the preferences of the given
    user. We can see this process in [figure 11.7](#ch11fig07): as we move from left
    to right in each row, the shirts and pants get progressively more personalized.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是康和他的同事们设计的这种方法充分展示其潜力的地方。我们可以在潜在空间中移动到生成我们想要修改的图像相似点的位置，同时优化给定用户的偏好。我们可以在[图11.7](#ch11fig07)中看到这个过程：当我们从每一行的左边移动到右边时，衬衫和裤子逐渐变得更加个性化。
- en: 'Figure 11.7\. The personalization process for six shoppers (three male and
    three female) using the same starting image: polo shirt for males and a pair of
    pants for women.'
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.7\. 使用相同起始图像对六位购物者（三位男性和三位女性）进行个性化过程：男性的圆领衫和女性的裤子。
- en: '![](../Images/11fig07_alt.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11fig07_alt.jpg)'
- en: '(Source: Kang et al., 2017, [https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231).)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：康等人，2017，[https://arxiv.org/abs/1711.02231](https://arxiv.org/abs/1711.02231)。)
- en: For instance, the person of the first row was looking for a more colorful option
    and, as Kang et al. observed, the person in row 5 seems to prefer brighter colors
    and a more distressed look; and the last person, it appears, prefers skirts over
    jeans. This is hyperpersonalization at its finest. No wonder Amazon took notice.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第一行的人正在寻找更鲜艳的选项，正如康等人所观察到的，第五行的人似乎更喜欢更亮的颜色和更破旧的外观；而最后一个人，看起来更喜欢裙子而不是牛仔裤。这是超个性化最极致的体现。难怪亚马逊注意到了这一点。
- en: The leftmost photo shows the real product from the training dataset; the second
    photo from the left shows a generated image closest to the real photo that was
    used as a starting point for the personalization process. Each image is annotated
    with its preference score. As we move from left to right, the item is progressively
    optimized for the given individual. As evidenced by the increasing scores, the
    personalization process improves the likelihood that the item matches the given
    shopper’s style and taste.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 最左侧的照片显示了训练数据集中的真实产品；左侧第二张照片显示了与用作个性化过程起点的真实照片最接近的生成图像。每张图像都标注了其偏好分数。随着我们从左到右移动，该物品逐渐针对给定的个人进行优化。正如不断上升的分数所示，个性化过程提高了该物品与给定购物者的风格和品味相匹配的可能性。
- en: 11.3\. Conclusion
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3. 结论
- en: The applications covered in this chapter only scratch the surface of what is
    possible with GANs. Countless other use cases exist in medicine and fashion alone,
    not to mention other fields. What is certain is that GANs have expanded far beyond
    academia, with myriad applications leveraging their ability to synthesize realistic
    data.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的应用仅是GANs可能实现的应用的冰山一角。仅医学和时尚领域就有无数其他用例，更不用说其他领域。可以肯定的是，GANs已经远远超出了学术领域，利用它们合成真实数据的能力，在众多应用中发挥着作用。
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Because of the versatility of GANs, they can be harnessed for a wide array of
    nonacademic applications and easily repurposed to use cases beyond the MNIST.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于GANs的通用性，它们可以被用于广泛的非学术应用，并且可以轻松地重新用于MNIST之外的用例。
- en: In medicine, GANs produce synthetic examples that can improve classification
    accuracy beyond what is possible with standard dataset augmentation strategies.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在医学领域，GANs产生的合成示例可以提高分类精度，这超出了标准数据集增强策略所能达到的。
- en: In fashion, GANs can be used to create new items and alter existing items to
    better match someone’s personal style. This is accomplished by generating images
    that maximize preference score provided by a recommendation algorithm.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时尚领域，GANs可以用来创建新物品和修改现有物品，以更好地符合某人的个人风格。这是通过生成最大化推荐算法提供的偏好分数的图像来实现的。
- en: Chapter 12\. Looking ahead
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12章. 展望未来
- en: '*This chapter covers*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: The ethics of generative models
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型的伦理
- en: 'Three recent improvements that we expect to be dominant in the years to come:'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预计在未来几年中将占主导地位的三个最近改进：
- en: Relativistic GAN (RGAN)
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对论生成对抗网络（RGAN）
- en: Self-Attention GAN (SAGAN)
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力生成对抗网络（SAGAN）
- en: BigGAN
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigGAN
- en: Further reading for three more cutting-edge techniques
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三种更前沿技术的进一步阅读
- en: A summary of the key themes and takeaways from this book
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书关键主题和主要收获的总结
- en: In this final chapter, we want to give you a brief overview of our thoughts
    about the ethics of GANs. Then we will talk about some important innovations that
    we expect to be even more important in the future. This chapter includes high-level
    ideas that we expect to define the future of GANs, but it does not feature any
    code. We want you to be prepared for the GANtastic journey ahead—even for advances
    that are yet to be published at the time of writing. Lastly, we will wrap up and
    say our teary-eyed goodbyes.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们想简要概述一下我们对GANs伦理的看法。然后我们将讨论一些我们认为在未来将变得更加重要的创新。本章包含了一些高级理念，我们预计这些理念将定义GANs的未来，但它不包含任何代码。我们希望你能为即将到来的GAN之旅做好准备——即使是写作时还未发表的进步。最后，我们将总结并表达我们依依不舍的告别。
- en: 12.1\. Ethics
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1. 伦理
- en: The world is beginning to realize that AI ethics—GANs included—is an important
    issue. Some institutions have decided to not release their expensive, pretrained
    models for fear of misuse as a tool for generating fake news.^([[1](#ch12fn01)])
    Numerous articles describe the ways in which GANs specifically may have potential
    malicious uses.^([[2](#ch12fn02)])
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 世界开始意识到，AI伦理——包括GANs——是一个重要问题。一些机构已经决定不发布它们昂贵的预训练模型，以防它们被误用作生成虚假新闻的工具。[1](#ch12fn01)
    数篇文章描述了GANs可能具有潜在恶意用途的方式。[2](#ch12fn02)
- en: ¹
  id: totrans-357
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-358
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “An AI That Writes Convincing Prose Risks Mass-Producing Fake News,” by
    Will Knight, *MIT Technology Review*, 2019, [http://mng.bz/RPGj](http://mng.bz/RPGj).
  id: totrans-359
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Will Knight在2019年《麻省理工学院技术评论》上发表的“An AI That Writes Convincing Prose Risks
    Mass-Producing Fake News”，[http://mng.bz/RPGj](http://mng.bz/RPGj)。
- en: ²
  id: totrans-360
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-361
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Inside the World of AI that Forges Beautiful Art and Terrifying Deepfakes,”
    by Karen Hao, *MIT Technology Review*, 2019, [http://mng.bz/2JA8](http://mng.bz/2JA8).
    See also “AI Gets Creative Thanks to GANs Innovations,” by Jakub Langr, *Forbes*,
    2019, [http://mng.bz/1w71](http://mng.bz/1w71).
  id: totrans-362
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Karen Hao在2019年发表于《麻省理工学院技术评论》上的文章“AI世界内部：创造美丽艺术和恐怖深度伪造”，[http://mng.bz/2JA8](http://mng.bz/2JA8)。另请参阅Jakub
    Langr在2019年发表于《福布斯》上的文章“AI因GANs创新而变得富有创造力”，[http://mng.bz/1w71](http://mng.bz/1w71)。
- en: We all understand that misinformation can be a huge problem and that GANs with
    photorealistic, synthetic images could pose a danger. Imagine synthesizing videos
    of a world leader saying they are about to launch a military strike on another
    country. Will the correcting information spread quickly enough to soothe the panic
    that will follow?
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都明白错误信息可能是一个大问题，并且具有逼真合成图像的GANs可能构成危险。想象一下合成一位世界领导人的视频，声称他们即将对另一个国家发动军事打击。纠正信息能否迅速传播以平息随之而来的恐慌？
- en: This is not a book about AI ethics, so we touch on this topic only briefly.
    But we strongly believe that it is important for all of us to think about the
    ethics of what we are doing and about the risks and unintended consequences that
    our work could have. Given that AI is such a scalable technology, it is vital
    to think through whether we are helping to create a world we want to live in.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一本关于AI伦理的书，所以我们只是简要地触及这个话题。但我们坚信，对于我们所有人来说，思考我们所做的事情的伦理、我们工作的风险和意外后果是非常重要的。鉴于AI是一项如此可扩展的技术，思考我们是否在帮助创造一个我们想要生活的世界至关重要。
- en: We urge you to think about your principles and to go through at least one of
    the more evolved ethical frameworks. We are not going to discuss which one is
    better than the other—after all, humans have generally not yet agreed on a moral
    framework on much more mundane things—but please put the book down and read at
    least one of these if you have not already.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们敦促您思考您的原则，并至少通过一个更成熟的伦理框架。我们不会讨论哪一个比另一个更好——毕竟，人类在许多更为平凡的事情上还没有就道德框架达成一致——但如果你还没有读过，请放下这本书并至少阅读其中之一。
- en: '|  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You can read about Google’s AI principles at [https://ai.google/principles](https://ai.google/principles).
    The Institute for Ethical AI & ML details its principles at [https://ethical.institute/principles.html](https://ethical.institute/principles.html).
    See also “IBM’s Rometty Lays Out AI Considerations, Ethical Principles,” by Larry
    Dignan, 2017, ZDNet, [http://mng.bz/ZeZm](http://mng.bz/ZeZm).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://ai.google/principles](https://ai.google/principles)上了解谷歌的AI原则。道德AI与机器学习研究所详细介绍了其原则，请参阅[https://ethical.institute/principles.html](https://ethical.institute/principles.html)。另请参阅Larry
    Dignan于2017年发表在ZDNet上的文章“IBM的Rometty阐述AI考虑因素和伦理原则”，[http://mng.bz/ZeZm](http://mng.bz/ZeZm)。
- en: '|  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: For example, the technology known as *DeepFakes*—although not originally based
    on GANs—has been cited by many as a source for concern.^([[3](#ch12fn03)]) DeepFakes—a
    portmanteau of *deep learning* and *fake imagery*—has already proven controversial
    by generating fake political videos and synthetic involuntary pornographic content.
    Soon, this technology may be at a point where it would be impossible to tell whether
    the video or image is authentic. Given GANs’ ability to synthesize new images,
    they may soon dominate this domain.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，被称为*深度伪造*的技术——尽管最初并非基于GANs——被许多人视为一个令人担忧的来源。[^([3](#ch12fn03))] 深度伪造——由*深度学习*和*伪造图像*组合而成——已经通过生成虚假政治视频和合成色情内容而证明具有争议性。很快，这项技术可能达到一个点，以至于无法判断视频或图像是否真实。鉴于GANs合成新图像的能力，它们可能很快就会主导这个领域。
- en: ³
  id: totrans-371
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-372
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “The Liar’s Dividend, and Other Challenges of Deep-Fake News,” by Paul Chadwick,
    *The Guardian*, 2018, [http://mng.bz/6wN5](http://mng.bz/6wN5). See also “If You
    Thought Fake News Was a Problem, Wait for DeepFakes,” by Roula Khalaf, 2018, *Financial
    Times*, [http://mng.bz/PO8Y](http://mng.bz/PO8Y).
  id: totrans-373
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Paul Chadwick在2018年发表于《卫报》上的文章“谎言的回报，以及其他深度伪造新闻的挑战”，[http://mng.bz/6wN5](http://mng.bz/6wN5)。另请参阅Roula
    Khalaf在2018年发表于《金融时报》上的文章“如果你认为假新闻是个问题，那你就等着深度伪造吧”，[http://mng.bz/PO8Y](http://mng.bz/PO8Y)。
- en: To say that everyone should think about the consequences of their research and
    code seems insufficient, but the reality is that there is no silver bullet. We
    should consider these implications, even if the initial focus was entirely ethical,
    regardless of whether we are working in research or industry. We also do not want
    to give you a dull lecture nor unsubstantiated media-grabbing forecast, but this
    is a problem we care deeply about.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 说每个人都应该考虑他们研究和代码的后果似乎还不够，但现实是，没有一劳永逸的解决方案。即使我们的初始关注完全是伦理方面的，无论我们是在研究还是工业界工作，我们都应该考虑这些影响。我们也不想给你们一个枯燥的讲座或未经证实的媒体炒作预测，但这是我们非常关心的问题。
- en: AI ethics is a real problem *already*, and we have presented three real problems
    here—AI-generated fake news, synthesized political proclamations, and involuntary
    pornography. But many more problems exist, such as Amazon using an AI-hiring tool
    showing negative bias against women.^([[4](#ch12fn04)]) But the practical landscape
    is complicated—some suggest that GANs have a tendency to favor images of women
    in face-generation. Yet another angle is that GANs also have a potential to help
    AI be more ethical—by synthesizing the underrepresented class in, for example,
    face-recognition problems in a semi-supervised setup, thereby improving the quality
    of classification in less-represented communities.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能伦理是一个真实存在的问题 *已经存在了*，我们在这里提出了三个真实的问题——AI生成的虚假新闻、合成的政治宣言和强制性的色情内容。但还有很多其他问题存在，比如亚马逊使用一个显示对女性存在负面偏见的AI招聘工具。[^[[4](#ch12fn04)]]
    但实际情况很复杂——有些人认为GANs倾向于在面部生成中偏爱女性形象。另一个角度是，GANs也有潜力帮助AI更加道德——通过在半监督设置中合成代表性不足的类别，例如在面部识别问题中，从而提高在代表性不足的社区中的分类质量。
- en: ⁴
  id: totrans-376
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-377
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women,”
    by Jeffrey Dastin, 2018, Reuters, [http://mng.bz/Jz8K](http://mng.bz/Jz8K).
  id: totrans-378
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 见“亚马逊取消了对女性存在偏见的秘密AI招聘工具”，作者Jeffrey Dastin，2018年，路透社，[http://mng.bz/Jz8K](http://mng.bz/Jz8K)。
- en: We are writing this book partially to make everyone more aware of the possibilities
    and possible misuses of GANs. We are excited by the future academic and practical
    applications of GANs and the ongoing research, but we are also aware that some
    applications may have negative uses. Because it is impossible to “uninvent” a
    technology, we have to be aware of its capabilities. By no means are we saying
    that the world would be better off if GANs did not exist—but GANs are just a tool,
    and as we all know, tools can be misused.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们写这本书的部分原因是为了让每个人更加意识到GANs的可能性和可能的滥用。我们对GANs未来的学术和实际应用以及正在进行的研究感到兴奋，但我们也意识到一些应用可能具有负面用途。由于技术无法“重新发明”，我们必须了解其能力。我们绝不是说，如果没有GANs，世界会变得更好——GANs只是一个工具，正如我们所知，工具可以被滥用。
- en: We feel morally compelled to talk about the promises and dangers of this technology,
    as otherwise misusing it becomes easier by a narrow group of the initiated. Although
    this book is not written for the general public, we hope that this is one stepping
    stone toward broader awareness—beyond the mostly academic circles that have dominated
    the field of GANs for now. Equally, much of the public outreach we are doing—we
    hope—is contributing to greater knowledge and discussions about this topic.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感到道德上有义务讨论这项技术的承诺和危险，否则，一小部分知情者更容易滥用它。尽管这本书不是为普通大众所写，但我们希望这能成为迈向更广泛意识的一个台阶——超越至今仍由GANs领域占主导地位的学术圈。同样，我们进行的许多公众宣传——我们希望——正在为这一主题的知识和讨论做出贡献。
- en: As more people are aware of this technology, even the existing malicious actors
    will no longer be able to catch anyone by surprise. We are hoping that GANs will
    never be a source of malicious acts, but that may be too idealistic. The next
    best thing is for knowledge of GANs to be available to everyone—not just academics
    and really invested malicious parties. We also hope (and all evidence thus far
    seems to point to this reality) that GANs will overall contribute positively to
    art, science, and engineering. Furthermore, people are also working on DeepFake
    detection, incorporating ideas from GANs and adversarial examples, but we have
    to be cautious, because any classifier that can detect these with any degree of
    accuracy will lend all the more credibility to an example that will manage to
    fool it.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的人了解这项技术，甚至现有的恶意行为者也不再能够出其不意地抓住任何人。我们希望GANs永远不会成为恶意行为的来源，但这可能过于理想化。最好的办法是让每个人都能了解GANs的知识——而不仅仅是学者和真正投入的恶意分子。我们还希望（迄今为止的所有证据似乎都指向这一现实），GANs总体上将对艺术、科学和工程产生积极贡献。此外，人们也在研究DeepFake检测，结合了GANs和对抗性样本的想法，但我们必须谨慎，因为任何能够以任何程度的准确性检测这些样本的分类器都将使能够欺骗它的示例更加可信。
- en: In many ways, we are also hoping to start a more thorough conversation without
    any grandstanding—this is an invitation to connect with us through our book forums
    or our Twitter accounts. We are aware that we need a diverse range of perspectives
    to keep checking our moral framework. We also are aware that these things will
    evolve over time, especially as use cases become clearer. Indeed, some people—such
    as Benedict Evans of a16z—argue that to regulate or talk about the ethics of AI
    does not make any more sense than to talk about the ethics of databases. What
    matters is the use case, not the technology.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，我们也希望开始一场更加深入的对话，而不需要任何夸张的表现——这是通过我们的书籍论坛或我们的Twitter账户与我们联系的一种邀请。我们意识到，我们需要各种不同的观点来不断检验我们的道德框架。我们也意识到，这些事情会随着时间的推移而发展，特别是在用例变得更加清晰的情况下。确实，有些人——比如来自a16z的Benedict
    Evans——认为，谈论AI的监管或伦理与谈论数据库的伦理一样没有意义。重要的是用例，而不是技术。
- en: 12.2\. GAN innovations
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2. GAN创新
- en: 'Speaking of use cases, we are aware that GANs are an ever-evolving field. In
    this section, we want to quickly update you on things that are not as robust in
    the community as some of the topics in prior chapters, but things we expect to
    be significant in the future. In the spirit of keeping this practical, we have
    picked out three GAN innovations that all have an interesting practical application:
    either a practical paper (RGAN), GitHub project (SAGAN), or artistic application
    (BigGAN).'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 说到用例，我们知道GANs是一个不断发展的领域。在本节中，我们想快速更新您关于社区中不如前几章中某些主题稳健的一些事情，但我们预计这些事情在未来将非常重要。为了保持实用性，我们挑选出了三个具有有趣实际应用的GAN创新：一篇实用论文（RGAN）、GitHub项目（SAGAN）或艺术应用（BigGAN）。
- en: 12.2.1\. Relativistic GAN
  id: totrans-385
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.1. 相对论生成对抗网络
- en: Not often do we get to see an update so simple and elegant that it could have
    been in the original paper, yet powerful enough to beat many of the state-of-the-art
    algorithms. *Relativistic GAN (RGAN)* is one such example. The core idea of the
    RGAN is that in addition to the original GAN (specifically, the NS-GAN that you
    may recall from [chapter 5](../Text/kindle_split_015.xhtml#ch05)), we add an extra
    term to the Generator—forcing it to make the generated data seem more real than
    the real data.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有机会看到如此简单而优雅的更新，它几乎可以出现在原始论文中，同时足够强大，足以击败许多最先进的算法。*相对论生成对抗网络（RGAN）*就是这样一个例子。RGAN的核心思想是，除了原始的GAN（特别是你可能从[第5章](../Text/kindle_split_015.xhtml#ch05)中回忆起的NS-GAN）之外，我们还在生成器中添加了一个额外的项——迫使它使生成的数据看起来比真实数据更真实。
- en: In other words, the Generator should, in addition to making fake data seem more
    real, make real data seem comparatively less real, thereby also increasing the
    stability of the training. But of course, the only data the Generator has control
    over is the synthetic data, so the Generator can achieve this only comparatively.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，生成器除了使假数据看起来更真实之外，还应该使真实数据看起来相对不那么真实，从而也增加了训练的稳定性。但当然，生成器唯一能够控制的数据是合成数据，因此生成器只能相对地实现这一点。
- en: 'The RGAN’s author describes it as being a generalized version of the WGAN,
    which we discussed previously. Let’s start with the simplified loss function from
    [table 5.1](../Text/kindle_split_015.xhtml#ch05table01) in [chapter 5](../Text/kindle_split_015.xhtml#ch05):'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: RGAN的作者将其描述为WGAN（我们之前讨论过的）的通用版本。让我们从[第5章](../Text/kindle_split_015.xhtml#ch05)中的[表5.1](../Text/kindle_split_015.xhtml#ch05table01)中的简化损失函数开始。
- en: equation 12.1\.
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式12.1。
- en: '![](../Images/12equ01.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12equ01.jpg)'
- en: equation 12.2\.
  id: totrans-391
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式12.2。
- en: '![](../Images/12equ02.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12equ02.jpg)'
- en: Recall that [equation 12.1](#ch12equ01) describes the loss function for the
    Discriminator—where we measure the difference between the real data (*D*(*x*))
    and the generated ones (*D*(*G*(*z*))). [Equation 12.2](#ch12equ02) then describes
    the loss function of the Generator, where we are trying to make the Discriminator
    believe that the samples it is seeing are real.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，[方程式12.1](#ch12equ01)描述了判别器的损失函数——我们测量真实数据(*D*(*x*))和生成数据(*D*(*G*(*z*)))之间的差异。[方程式12.2](#ch12equ02)然后描述了生成器的损失函数，其中我们试图让判别器相信它看到的样本是真实的。
- en: To go to our closest predecessor, remember that the WGAN is trying to minimize
    the amount of probability mass we would have to move to get the generated distribution
    to look like the real one. In this sense, the RGAN has many similarities (for
    example, the Discriminator is frequently called the *critic*, and the WGAN is
    presented as a special case of the RGAN in this paper). Ultimately, both measure
    the current state of play as a single number—remember the earth mover’s distance?
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 要回到我们的直接前辈，记住WGAN试图最小化我们需要移动的概率质量，以使生成的分布看起来像真实分布。在这方面，RGAN有很多相似之处（例如，判别器经常被称为*评论员*，而WGAN在本论文中被呈现为RGAN的特殊情况）。最终，两者都将当前状态作为单个数字来衡量——还记得地球迁移距离吗？
- en: The innovation of the RGAN is that we no longer get the previous unhelpful dynamic
    of the Generator always playing catch-up. In other words, the Generator is trying
    to generate data that looks more realistic than the real data so that it is not
    always on the defensive. As a result, *D*(*x*) can be interpreted as the probability
    that the real data is more realistic than the generated data.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: RGAN的创新之处在于，我们不再得到生成器总是处于追赶状态的前所未有的动态。换句话说，生成器正在尝试生成比真实数据更真实的数据，这样它就不再总是处于防守状态。因此，*D*(*x*)可以被解释为真实数据比生成数据更真实概率。
- en: Before we delve into the difference on a high level, we will introduce a slightly
    different notation, as to approximate the notation used by the paper, but simplify.
    In [equations 12.3](#ch12equ03) and [12.4](#ch12equ04), *C*(*x*) acts as a critic
    similar to a WGAN setup,^([[5](#ch12fn05)]) and you may think of it as a Discriminator.
    Furthermore, *a*() is defined as log(sigmoid()). In the paper, *G*(*z*) is replaced
    by *x[f]* for fake samples, and *x* gets subscript *r* to indicate real samples,
    but we will follow the simpler notation from the earlier chapters.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨高级别差异之前，我们将引入一种略微不同的符号，以近似论文中使用的符号，但简化。在[方程式12.3](#ch12equ03)和[12.4](#ch12equ04)中，*C*(*x*)充当类似于WGAN设置的评论员，^([[5](#ch12fn05)])您可能将其视为判别器。此外，*a*()定义为log(sigmoid())。在论文中，*G*(*z*)被替换为*x[f]*表示假样本，而*x*通过下标*r*表示真实样本，但我们将遵循早期章节中更简单的符号。
- en: ⁵
  id: totrans-397
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-398
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because we are skipping over some details, we want to equip you with the high-level
    idea and keep the notation consistent so that you can fill in the blanks yourself.
  id: totrans-399
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于我们跳过了一些细节，我们希望向您提供高级概念，并保持符号的一致性，以便您可以自己填补空白。
- en: equation 12.3\.
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式12.3。
- en: '![](../Images/12equ03.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12equ03.jpg)'
- en: equation 12.4\.
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式12.4。
- en: '![](../Images/12equ04.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12equ04.jpg)'
- en: 'Importantly, in these equations, we see only one key difference in the Generator:
    the real data now adds into the loss function. This seemingly simple trick aligns
    the incentives of the Generator to not be at a permanent disadvantage. To understand
    this and two other perspectives in an idealized setting, let’s plot the different
    Discriminator outputs as in [figure 12.1](#ch12fig01).'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，在这些方程中，我们只看到生成器中的一个关键差异：真实数据现在被添加到损失函数中。这个看似简单的技巧使生成器的激励与永久劣势保持一致。为了理解这一点以及在其他理想化设置中的两个其他视角，让我们绘制不同的判别器输出，如[图12.1](#ch12fig01)所示。
- en: Figure 12.1\. Under divergence minimization (a), the Generator is always playing
    catch-up with the Discriminator (because divergence is always ≥ 0). In (b), we
    see what “good” NS-GAN training looks like. Again, the Generator cannot win. In
    (c), we can see that now the generator can win, but more importantly, the Generator
    always has something to strive for (and therefore recover useful gradient), no
    matter the stage of training.
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1\. 在发散最小化（a）中，生成器总是追赶判别器（因为发散总是≥0）。在（b）中，我们看到“良好”的NS-GAN训练是什么样的。再次，生成器不能获胜。在（c）中，我们可以看到现在生成器可以获胜，但更重要的是，生成器在训练的任何阶段都有东西可以追求（因此恢复有用的梯度）。
- en: '![](../Images/12fig01_alt.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12fig01_alt.jpg)'
- en: '(Source: “The Relativistic Discriminator: A Key Element Missing from Standard
    GAN,” by Alexia Jolicoeur-Martineau, 2018, [http://arxiv.org/abs/1807.00734](http://arxiv.org/abs/1807.00734).)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：“相对判别器：标准 GAN 缺失的关键元素”，作者：Alexia Jolicoeur-Martineau，2018年，[http://arxiv.org/abs/1807.00734](http://arxiv.org/abs/1807.00734).)
- en: You may be wondering, why should just adding this term be noteworthy? Well,
    this simple addition makes the training significantly more stable at a little
    extra computational cost. This is important, especially when you remember the
    “Are GANs Created Equal?” paper from [chapter 5](../Text/kindle_split_015.xhtml#ch05),
    where the authors argue that all the major GAN architectures considered so far
    have an only limited improvement over the original GAN when adjusted for the extra
    processing requirements. This is because many new GAN architectures are better
    only at huge computational cost, which makes them less useful, but the RGAN has
    potential to change GAN architectures across the board.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想，为什么仅仅添加这个术语就值得关注？嗯，这个简单的添加使得训练在略微增加的计算成本下变得更加稳定。这很重要，特别是当你想起“GANs Created
    Equal？”这篇论文来自[第5章](../Text/kindle_split_015.xhtml#ch05)，其中作者们争论说，到目前为止考虑的所有主要GAN架构在调整额外的处理需求后，对原始GAN的改进仅是有限的。这是因为许多新的GAN架构仅在巨大的计算成本下表现更好，这使得它们不太有用，但RGAN有潜力改变整个GAN架构。
- en: Always be aware of this trick, because even though a method may take fewer update
    steps, if each step takes two times longer because of the extra computation, is
    it really worth it? The peer review process at most conferences is not immune
    to this weakness, so you have to be careful.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 总是要意识到这个技巧，因为即使一个方法可能需要更少的更新步骤，但如果每个步骤因为额外的计算而需要两倍的时间，这真的值得吗？大多数会议的同行评审过程并不免疫于这种弱点，所以你必须小心。
- en: Application
  id: totrans-410
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 应用
- en: Your next question may be, why should this matter in practice? In less than
    a year, this paper has gathered more than 50 citations^([[6](#ch12fn06)])—which
    is a lot for a new paper from a previously unknown author. Moreover, people have
    already written papers using the RGAN to, for example, achieve state-of-the-art
    speech (that is, best performance ever achieved) enhancement, beating other GAN-based
    and non-GAN-based methods.^([[7](#ch12fn07)])
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 您接下来的问题可能是，这在实践中为什么很重要？不到一年时间，这篇论文已经收集了超过50次引用^([[6](#ch12fn06)])——对于一个之前未知的作者的新论文来说，这是一个相当大的数字。此外，人们已经使用RGAN撰写了论文，例如，实现了最先进的语音（即，迄今为止的最佳性能）增强，击败了其他基于GAN和非GAN的方法.^([[7](#ch12fn07)])
- en: ⁶
  id: totrans-412
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-413
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The following link names all the papers that cite the RGAN paper: [http://mng.bz/omGj](http://mng.bz/omGj).'
  id: totrans-414
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以下链接列出了所有引用RGAN论文的论文：[http://mng.bz/omGj](http://mng.bz/omGj)。
- en: ⁷
  id: totrans-415
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-416
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “SERGAN: Speech Enhancement Using Relativistic Generative Adversarial Networks
    with Gradient Penalty,” by Deepak Baby and Sarah Verhulst, 2019, IEEE-ICASSP,
    [https://ieeexplore.ieee.org/document/8683799](https://ieeexplore.ieee.org/document/8683799).'
  id: totrans-417
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参见Deepak Baby和Sarah Verhulst于2019年发表的IEEE-ICASSP论文“SERGAN: 使用梯度惩罚的相对生成对抗网络进行语音增强”，[https://ieeexplore.ieee.org/document/8683799](https://ieeexplore.ieee.org/document/8683799)。'
- en: As you are reading this, the paper should be available, so feel free to take
    a look. Explaining this paper, with all the background necessary, however, is
    beyond the scope of this book.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 当您阅读此内容时，这篇论文应该已经可用，所以请随意查看。然而，解释这篇论文，包括所有必要的背景知识，超出了本书的范围。
- en: 12.2.2\. Self-Attention GAN
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.2\. 自注意力GAN
- en: 'The next innovation we believe is going to change the landscape is the *Self-Attention
    GAN (SAGAN)*. Attention is based on a very human idea of how we look at the world—through
    small patches of focus at a time.^([[8](#ch12fn08)]) A GAN’s attention works similarly:
    your mind is consciously able to focus on only a small part of, say, a table,
    but your brain is able to stitch the whole table together through quick, minor
    eye movements called *saccades* while still focusing on only a subset of the image
    at a time.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信下一个即将改变格局的创新是*自注意力生成对抗网络（SAGAN）*。注意力基于一个非常人性化的关于我们如何观察世界——一次关注一小块的想法。[8](#ch12fn08)
    GAN的注意力工作方式类似：你的意识能够专注于，比如说，桌子的一小部分，但你的大脑能够通过快速、微小的眼动，称为*眼跳（saccades）*，将整个桌子拼接在一起，同时仍然一次只关注图像的子集。
- en: ⁸
  id: totrans-421
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-422
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See *The Mind Is Flat: The Illusion of Mental Depth and the Improvised Mind*
    by Nick Chater (Penguin, 2018).'
  id: totrans-423
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '请参阅尼克·查特（Nick Chater）所著的《The Mind Is Flat: The Illusion of Mental Depth and
    the Improvised Mind》（企鹅出版社，2018年）。'
- en: The computer equivalent has been used in many fields, including natural language
    processing (NLP) and computer vision. Attention can help us solve, for example,
    the problem of convolutional neural networks (CNNs) ignoring much of the picture.
    As we know, CNNs rely on a small receptive field—as determined by the size of
    the convolution. However, as you may recall from [chapter 5](../Text/kindle_split_015.xhtml#ch05),
    in GANs, the size of the receptive field is likely to cause problems (such as
    cows with multiple heads or bodies), and the GAN will not consider them strange.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机等效方法已在许多领域得到应用，包括自然语言处理（NLP）和计算机视觉。注意力可以帮助我们解决，例如，卷积神经网络（CNNs）忽略图片大部分内容的问题。众所周知，CNNs依赖于一个小的感受野——由卷积的大小决定。然而，如您在[第5章](../Text/kindle_split_015.xhtml#ch05)中可能回忆的那样，在生成对抗网络（GANs）中，感受野的大小可能会引起问题（如多头或多身体的情况），而GAN不会认为这是奇怪的。
- en: This is because when generating or evaluating that subset of the image, we may
    see that a leg is present in one field, but we do not see that other legs are
    already present in another one. This could be because the convolution ignores
    the structure of the object or because legs or leg rotations are represented by
    different, higher-level neurons that do not talk to each other. Our seasoned data
    scientists will remember that is what Hinton’s CapsuleNets were attempting to
    solve, but they never really took off. For everyone else, the short story is that
    no one can say with absolute certainty why attention fixes this, but a good way
    to think about it is that we can now create *feature detectors* with a flexible
    receptive field (shape) to really focus on several key aspects of a given picture
    (see [figure 12.2](#ch12fig02)).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在生成或评估图像的该子集时，我们可能会看到在一个区域中有一个腿，但我们没有看到在另一个区域中已经存在其他腿。这可能是由于卷积忽略了物体的结构，或者是因为腿或腿的旋转由不同的高级神经元表示，这些神经元之间没有交流。我们经验丰富的数据科学家会记得这正是Hinton的CapsuleNets试图解决的问题，但它们从未真正起飞。对其他人来说，简而言之，没有人能绝对确定为什么注意力可以解决这个问题，但一种好的思考方式是我们现在可以创建具有灵活感受野（形状）的*特征检测器*，真正关注给定图片的几个关键方面（参见[图12.2](#ch12fig02)）。
- en: Figure 12.2\. The output pixel (2 × 2 patch) ignores anything except the small
    highlighted region. Attention helps us solve that.
  id: totrans-426
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2\. 输出像素（2 × 2块）忽略除了小的高亮区域之外的所有内容。注意力帮助我们解决这个问题。
- en: '![](../Images/12fig02_alt.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12fig02_alt.jpg)'
- en: '(Source: “Convolution Arithmetic,” by vdmoulin, 2016, [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic).)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“Convolution Arithmetic”，作者：vdmoulin，2016年，[https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)）
- en: Recall that this is especially a problem when our images are, say, 512 × 512,
    but the largest commonly used convolution sizes are 7, so that is loads of ignored
    features! Even in higher-level nodes, the neural network may not be appropriately
    checking for, for example, a head in the right place. As a result, as long as
    the cow has a cow head next to a cow body, the network does not care about any
    other head, as long as it has at least one. But the structure is wrong.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，当我们的图像是512 × 512时，这尤其是一个问题，但最大的常用卷积大小是7，这意味着有大量的被忽略的特征！即使在高级节点中，神经网络也可能没有适当地检查，例如，头部是否在正确的位置。因此，只要牛头在牛身体旁边，网络就不关心任何其他头部，只要至少有一个。但结构是错误的。
- en: These higher-level representations are harder to reason about, and so even researchers
    disagree as to exactly why this happens, but empirically, the network does not
    seem to pick it up. Attention allows us to pick out the relevant regions—whatever
    the shape or size—and consider them appropriately. To see the types of regions
    that attention can flexibly focus on, consider [figure 12.3](#ch12fig03).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这些高级表示更难推理，因此即使是研究人员也不同意为什么会发生这种情况，但根据经验，网络似乎并没有注意到这一点。注意力使我们能够挑选出相关的区域——无论形状或大小——并相应地考虑它们。要了解注意力可以灵活关注的区域类型，请考虑
    [图 12.3](#ch12fig03)。
- en: Figure 12.3\. Here, we can see the regions of the image that the attention mechanism
    pays most attention to, given a representative query location. We can see that
    the attention mechanism generally cares about regions of different shapes and
    sizes, which is a good sign, given that we want it to pick out the regions of
    the image that indicate the kind of object it is.
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.3\. 在这里，我们可以看到在给定的代表性查询位置下，注意力机制最关注的图像区域。我们可以看到，注意力机制通常关注不同形状和大小的区域，这是一个好兆头，因为我们希望它能挑选出图像中表明其类型的区域。
- en: '![](../Images/12fig03_alt.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12fig03_alt.jpg)'
- en: '(Source: “Self-Attention Generative Adversarial Networks,” by Han Zhang, 2018,
    [http://arxiv.org/abs/1805.08318](http://arxiv.org/abs/1805.08318).)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：“Self-Attention Generative Adversarial Networks”，Han Zhang，2018，[http://arxiv.org/abs/1805.08318](http://arxiv.org/abs/1805.08318)。)
- en: Application
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 应用
- en: '*DeOldify* ([https://github.com/jantic/DeOldify](https://github.com/jantic/DeOldify))
    is one of the popular applications of the SAGAN that was made by Jason Antic,
    a student of Jeremy Howard’s fast.ai course. DeOldify uses the SAGAN to colorize
    old images and drawings to an amazing level of accuracy. As you can see in [figure
    12.4](#ch12fig04), you can turn famous historic photographs and paintings into
    fully colorized versions.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '*DeOldify* ([https://github.com/jantic/DeOldify](https://github.com/jantic/DeOldify))
    是由 Jeremy Howard 的 fast.ai 课程学生 Jason Antic 开发的一种基于 SAGAN 的流行应用。DeOldify 使用 SAGAN
    将旧图像和绘画着色到令人难以置信的精确程度。如图 12.4 所示，你可以将著名的历史照片和画作转换为全彩版本。'
- en: Figure 12.4\. Deadwood, South Dakota, 1877\. The image on the right has been
    colorized . . . for a black-and-white book. Trust us. If you do not believe us,
    check out the online liveBook on Manning’s website to see for yourself!
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.4\. 南达科他州 Deadwood，1877 年。右侧的图像已被着色……用于黑白书籍。请相信我们。如果您不相信我们，请查看 Manning
    网站上的在线 liveBook，亲自看看！
- en: '![](../Images/12fig04_alt.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12fig04_alt.jpg)'
- en: 12.2.3\. BigGAN
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.3\. BigGAN
- en: 'Another architecture that has taken the world by storm is *BigGAN*.^([[9](#ch12fn09)])
    BigGAN has achieved highly realistic 512 × 512 images on all 1,000 classes of
    ImageNet—a feat previously deemed almost impossible with the current generation
    of GANs. BigGAN achieved three times the previous best inception score. In brief,
    BigGAN builds on the SAGAN and spectral normalization and has further innovated
    in five directions:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个震撼世界的架构是 *BigGAN*.^([[9](#ch12fn09)]) BigGAN 在 ImageNet 的所有 1,000 个类别上实现了高度逼真的
    512 × 512 图像——这是当前一代 GAN 所认为几乎不可能完成的壮举。BigGAN 实现了之前最佳 inception 分数的三倍。简而言之，BigGAN
    建立在 SAGAN 和频谱归一化之上，并在五个方向上进一步创新：
- en: ⁹
  id: totrans-440
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-441
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Large Scale GAN Training for High Fidelity Natural Image Synthesis,” by
    Andrew Brock et al., 2019, [https://arxiv.org/pdf/1809.11096.pdf](https://arxiv.org/pdf/1809.11096.pdf).
  id: totrans-442
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Andrew Brock 等人于 2019 年发表的“Large Scale GAN Training for High Fidelity Natural
    Image Synthesis”，[https://arxiv.org/pdf/1809.11096.pdf](https://arxiv.org/pdf/1809.11096.pdf)。
- en: Scaling up GANs to previously unbelievable computational scale. The BigGAN authors
    trained with eight times the batch size, which was part of their success—giving
    already a 46% boost. Theoretically, the resources required to train a BigGAN add
    up to $59,000 worth of compute.^([[10](#ch12fn10)])
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 GAN 扩展到之前难以置信的计算规模。BigGAN 的作者使用了八倍的批量大小进行训练，这是他们成功的一部分——已经提供了 46% 的提升。理论上，训练
    BigGAN 所需的资源总计达到 59,000 美元的计算能力.^([[10](#ch12fn10)])
- en: ^(10)
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Mario Klingemann’s Twitter post at [http://mng.bz/wll2](http://mng.bz/wll2).
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Mario Klingemann 的 Twitter 帖子 [http://mng.bz/wll2](http://mng.bz/wll2)。
- en: BigGAN’s architecture has 1.5 times the number of channels (feature maps) in
    each layer relative to the SAGAN architecture. This may be due to the complexity
    of the dataset used.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 SAGAN 架构相比，BigGAN 的每个层的通道数（特征图）数量是 1.5 倍。这可能是因为数据集的复杂性。
- en: Improving the stability of the Generator and the Discriminator through controlling
    the adversarial process, which leads to overall better results. The underlying
    mathematics are unfortunately beyond the scope of this book, but if you’re interested,
    we recommend starting with understanding spectral normalization. For those who
    are not, take solace in the fact that even the authors themselves abandon this
    strategy in later parts of training and let the mode collapse because of computational
    costs.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过控制对抗过程来提高生成器和判别器的稳定性，从而带来整体更好的结果。不幸的是，这种基础的数学超出了本书的范围，但如果您对此感兴趣，我们建议从理解谱归一化开始。对于那些不感兴趣的人，您可以安慰自己，即使是作者在训练的后期部分也放弃了这种策略，并让模式崩溃，因为计算成本过高。
- en: Introducing a *truncation trick* to give us a way of controlling the trade-off
    between variety and fidelity. The truncation trick achieves better equality results
    if we sample closer to the middle of the distribution (truncate it). It makes
    sense that this would yield better samples, as this is where BigGAN has the “most
    experience.”
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入一种*截断技巧*来给我们一种控制多样性和保真度之间权衡的方法。如果我们从分布的中间部分采样（截断它），截断技巧可以达到更好的平等结果。这很有道理，因为这是BigGAN“经验最丰富”的地方。
- en: The authors introduce a further three theoretical advancements. According to
    the authors’ own performance table, however, these seem to have only a marginal
    effect on the scores and frequently lead to less stability. They are useful for
    computational efficiency, but we will not discuss them.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者介绍了另外三个理论进步。然而，根据作者自己的性能表，这些似乎只对分数有轻微的影响，并且经常导致稳定性降低。它们对计算效率很有用，但我们将不讨论它们。
- en: Application
  id: totrans-451
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 应用
- en: One fascinating artistic application of BigGAN is the Ganbreeder app, which
    was made possible thanks to the pretrained models and Joel Simon’s hard work.
    Ganbreeder is an interactive web-based (free!) way to explore the latent space
    of BigGAN. It has been used in numerous artistic applications as a way to come
    up with new images.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: BigGAN一个令人着迷的艺术应用是Ganbreeder应用程序，这得益于预训练模型和Joel Simon的辛勤工作。Ganbreeder是一个基于网络的交互式（免费！）方式来探索BigGAN的潜在空间。它已被用于众多艺术应用中，作为产生新图像的一种方式。
- en: You can either explore the adjacent latent space or use a linear interpolation
    between two samples of the two images to create new images. [Figure 12.5](#ch12fig05)
    shows an example of creating Ganbreeder offspring.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择探索相邻的潜在空间，或者使用两个图像样本之间的线性插值来创建新的图像。[图12.5](#ch12fig05)展示了创建Ganbreeder后代的示例。
- en: Figure 12.5\. Every time you click the Make Children button, Ganbreeder gives
    you a selection of mutated images in the nearby latent space, producing the three
    images below. You may start from your own sample or someone else’s—thereby making
    it a collaborative exercise. This is what the Crossbreed section is for, where
    you can select another interesting sample from other parts of the space and mix
    the two samples. Lastly, in Edit-Genes, you can edit parameters (such as Castle
    and Stone Wall, in this case) and add more or less of that feature into the picture.
  id: totrans-454
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.5。每次您点击“制作孩子”按钮时，Ganbreeder都会在附近的潜在空间中为您提供一组变异图像，产生下面的三幅图像。您可以从自己的样本或他人的样本开始——因此使其成为一种协作练习。这就是交叉混合部分的作用，您可以从空间的其它部分选择另一个有趣的样本，并将两个样本混合。最后，在编辑-基因中，您可以编辑参数（例如，在本例中的城堡和石墙）并添加更多或更少的这种特征到图片中。
- en: '![](../Images/12fig05_alt.jpg)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12fig05_alt.jpg)'
- en: '(Source: Ganbreeder, [http://mng.bz/nv28](http://mng.bz/nv28).)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Ganbreeder，[http://mng.bz/nv28](http://mng.bz/nv28)。）
- en: BigGAN is further notable because DeepMind has given us all this compute for
    free and uploaded pretrained models onto TensorFlow Hub—a machine learning code
    repository that we used in [chapter 6](../Text/kindle_split_016.xhtml#ch06).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: BigGAN另一个值得注意的地方是DeepMind免费为我们提供了所有这些计算资源，并将预训练模型上传到了TensorFlow Hub——一个我们用于[第6章](../Text/kindle_split_016.xhtml#ch06)的机器学习代码仓库。
- en: 12.3\. Further reading
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3. 进一步阅读
- en: 'We wanted to cover many other topics that seem to be gaining popularity in
    the works of academics and practitioners, but we did not have the space. Here,
    we will list at least three of them for interested readers. We hope we have equipped
    you with all that you need to understand these papers. We picked just three, as
    we expect this section to be changing quickly:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本想涵盖许多其他在学者和实践者作品中似乎越来越受欢迎的话题，但我们没有足够的空间。在这里，我们将至少列出三个供感兴趣的读者参考。我们希望我们已经为您提供了理解这些论文所需的一切。我们只挑选了三个，因为我们预计这一部分会很快发生变化：
- en: '*Style GAN* ([http://arxiv.org/abs/1812.04948](http://arxiv.org/abs/1812.04948))
    merges ideas from GANs and “traditional” style transfer to give users much more
    control over the output they generate. This Conditional GAN from NVIDIA has managed
    to produce stunning full-HD results with several levels of control—from finer
    details to overall image. This work builds on [chapter 6](../Text/kindle_split_016.xhtml#ch06),
    so you may want to reread it before delving into this paper.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Style GAN* ([http://arxiv.org/abs/1812.04948](http://arxiv.org/abs/1812.04948))
    将 GAN 和“传统”风格迁移的理念相结合，使用户对生成的输出有更多的控制。这款来自 NVIDIA 的条件 GAN 已经能够通过几个级别的控制——从更细致的细节到整体图像——产生令人惊叹的全高清结果。这项工作建立在[第
    6 章](../Text/kindle_split_016.xhtml#ch06)的基础上，因此在你深入研究这篇论文之前，你可能想要重新阅读它。'
- en: '*Spectral normalization* ([http://arxiv.org/abs/1802.05957](http://arxiv.org/abs/1802.05957))
    is a complex regularization technique and requires somewhat advanced linear algebra.
    For now, just remember the use case—stabilizing training by normalizing the weights
    in a network to satisfy a particular property, which is even formally required
    in WGAN (touched on in [chapter 5](../Text/kindle_split_015.xhtml#ch05)). Spectral
    normalization acts somewhat similarly to gradient penalties.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*谱归一化* ([http://arxiv.org/abs/1802.05957](http://arxiv.org/abs/1802.05957))
    是一种复杂的正则化技术，需要一定的先进线性代数知识。现在，只需记住其用例——通过在网络上归一化权重以满足特定属性来稳定训练，这在 WGAN 中也是形式上要求的（在第
    5 章[http://arxiv.org/abs/1802.05957](http://arxiv.org/abs/1802.05957)中有所涉及）。谱归一化在某种程度上与梯度惩罚相似。'
- en: '*SPADE*, aka *GauGAN* ([https://arxiv.org/pdf/1903.07291.pdf](https://arxiv.org/pdf/1903.07291.pdf))
    is cutting-edge work published in 2019 to synthesize photorealistic images based
    solely on a semantic map of the image, as you may recall from the start of [chapter
    9](../Text/kindle_split_019.xhtml#ch09). The images can be up to 512 × 256 in
    resolution, but knowing NVIDIA, this may increase before the end of the year.
    This may be the most challenging technique of the three, but also one that has
    gathered the most media attention—probably because of how impressive the tech
    demo is!'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SPADE*，也称为 *GauGAN* ([https://arxiv.org/pdf/1903.07291.pdf](https://arxiv.org/pdf/1903.07291.pdf))，是
    2019 年发表的一项前沿工作，它根据图像的语义图合成逼真的图像，正如你从[第 9 章](../Text/kindle_split_019.xhtml#ch09)的开头所回忆的那样。图像的分辨率可以达到
    512 × 256，但鉴于 NVIDIA 的能力，这可能在年底前增加。这可能是三种技术中最具挑战性的，但也是最受媒体关注的——可能是因为技术演示的令人印象深刻！'
- en: There is so much going on in the world of GANs that it may be impossible to
    stay up-to-date all the time. However, we hope that in terms of both ethical frameworks
    and the latest interesting papers, we have given you the resources needed to look
    at the problems in this ever-evolving space. Indeed, that is our hope, even when
    it comes to the innovations behind the GANs presented in this chapter. We do not
    know whether all of these will become part of the routine bag of tricks that people
    use, but we think that they might. We also hope that this will be true for the
    most recent innovations listed in this section.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GAN 的世界中，发生的事情如此之多，以至于可能无法始终跟上最新动态。然而，我们希望，无论是在伦理框架还是最新有趣论文方面，我们都已经为你提供了所需的资源，以便观察这个不断发展的空间中的问题。确实，这是我们的希望，即使是对本章中介绍的
    GAN 背后的创新也是如此。我们不知道这些是否会成为人们日常技巧包的一部分，但我们认为它们可能会。我们也希望这一点适用于本节中列出的最新创新。
- en: 12.4\. Looking back and closing thoughts
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4. 回顾与总结
- en: We hope that the cutting-edge techniques we’ve discussed will give you enough
    subject material to continue exploring GANs even as our book comes to an end.
    Before we send you off, however, it is worth looking back and recapping all that
    you have learned.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们讨论的尖端技术能够给你足够的素材，让你在本书结束时仍能继续探索 GAN。然而，在我们让你离开之前，回顾并总结你所学的内容是值得的。
- en: We started off with a basic explanation of what GANs are and how they work ([chapter
    1](../Text/kindle_split_010.xhtml#ch01)) and implemented a simple version of this
    system ([chapter 3](../Text/kindle_split_012.xhtml#ch03)). We introduced you to
    generative models in an easier setting with autoencoders ([chapter 2](../Text/kindle_split_011.xhtml#ch02)).
    We covered the theory of GANs ([chapters 3](../Text/kindle_split_012.xhtml#ch03)
    and [5](../Text/kindle_split_015.xhtml#ch05)) as well as their shortcomings and
    some of the ways to overcome them ([chapter 5](../Text/kindle_split_015.xhtml#ch05)).
    This provided the foundation and tools for the later, advanced chapters.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从对GANs是什么以及它们如何工作的基本解释开始（[第1章](../Text/kindle_split_010.xhtml#ch01)）并实现了这个系统的简单版本（[第3章](../Text/kindle_split_012.xhtml#ch03)）。我们在一个更容易的环境中向您介绍了生成模型，即自编码器（[第2章](../Text/kindle_split_011.xhtml#ch02)）。我们涵盖了GANs的理论（[第3章](../Text/kindle_split_012.xhtml#ch03)和[第5章](../Text/kindle_split_015.xhtml#ch05)）以及它们的不足之处和克服这些不足的一些方法（[第5章](../Text/kindle_split_015.xhtml#ch05)）。这为后续的、更高级的章节提供了基础和工具。
- en: 'We implemented several of the most canonical and influential GAN variants—Deep
    Convolutional GAN ([chapter 4](../Text/kindle_split_013.xhtml#ch04)) and Conditional
    GAN ([chapter 8](../Text/kindle_split_018.xhtml#ch08))—as well as a few of the
    most advanced and complex ones—Progressive GANs ([chapter 6](../Text/kindle_split_016.xhtml#ch06))
    and CycleGANs ([chapter 9](../Text/kindle_split_019.xhtml#ch09)). We also implemented
    Semi-Supervised GANs ([chapter 8](../Text/kindle_split_018.xhtml#ch08)), a GAN
    variant designed to tackle one of the most severe shortcomings in machine learning:
    the lack of large, labeled datasets. We also explored several of the many practical
    and innovative applications of GANs ([chapter 11](../Text/kindle_split_022.xhtml#ch11)),
    and presented adversarial examples ([chapter 10](../Text/kindle_split_021.xhtml#ch10)),
    which are a challenge for all of machine learning.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了几个最经典和最有影响力的GAN变体——深度卷积GAN（[第4章](../Text/kindle_split_013.xhtml#ch04)）和条件GAN（[第8章](../Text/kindle_split_018.xhtml#ch08)）——以及一些最先进和复杂的变体——渐进式GANs（[第6章](../Text/kindle_split_016.xhtml#ch06)）和CycleGANs（[第9章](../Text/kindle_split_019.xhtml#ch09)）。我们还实现了半监督GANs（[第8章](../Text/kindle_split_018.xhtml#ch08)），这是一种旨在解决机器学习中最严重不足之一的GAN变体：缺乏大量标记的数据集。我们还探讨了GANs的许多实用和创新应用（[第11章](../Text/kindle_split_022.xhtml#ch11)），并展示了对抗性示例（[第10章](../Text/kindle_split_021.xhtml#ch10)），这对所有机器学习都是一个挑战。
- en: Along the way, you expanded your theoretical and practical toolbox. From inception
    score and Fréchet inception distance ([chapter 5](../Text/kindle_split_015.xhtml#ch05))
    to pixel-wise feature normalization ([chapter 6](../Text/kindle_split_016.xhtml#ch06)),
    batch normalization ([chapter 4](../Text/kindle_split_013.xhtml#ch04)), and dropout
    ([chapter 7](../Text/kindle_split_017.xhtml#ch07)), you learned about concepts
    and techniques that will serve you well for GANs and beyond.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，您扩展了您的理论和实践工具箱。从 inception score 和 Fréchet inception distance ([第5章](../Text/kindle_split_015.xhtml#ch05))
    到像素级特征归一化 ([第6章](../Text/kindle_split_016.xhtml#ch06))，批量归一化 ([第4章](../Text/kindle_split_013.xhtml#ch04))，以及dropout
    ([第7章](../Text/kindle_split_017.xhtml#ch07)），您学习了关于概念和技术，这些将在GANs及其它领域为您服务得很好。
- en: 'As we look back, it is worth highlighting a few themes that came up time and
    time again as we explored GANs:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾过去，当我们探索生成对抗网络（GANs）时，有几个主题反复出现，值得我们强调：
- en: GANs are tremendously versatile, in terms of both practical use cases and resilience
    against theoretical requirements and constraints. This was perhaps most apparent
    in the case of CycleGAN in [chapter 9](../Text/kindle_split_019.xhtml#ch09). This
    technique not only is unconstrained by the need for paired data that burdened
    its predecessors, but also can translate between examples in virtually any domain,
    from apples and oranges to horses and zebras. The versatility of GANs was also
    evident in [chapter 6](../Text/kindle_split_016.xhtml#ch06), where you saw that
    Progressive GANs can learn to generate equally well images as disparate as human
    faces and medical mammograms, and in [chapter 7](../Text/kindle_split_017.xhtml#ch07),
    where we needed to make only a handful of adjustments to turn the Discriminator
    into a multiclass classifier.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs在实用用例和抵御理论要求及约束方面的适应性都非常强。这一点在第9章中CycleGAN的案例中可能最为明显。这项技术不仅不受其前辈需要成对数据的需求的限制，而且几乎可以在任何领域之间进行转换，从苹果和橙子到马和斑马。GANs的适应性在第6章中也得到了体现，您在那里看到渐进式GANs可以学习生成与人类面部和医学乳腺X光片一样不同的图像，在第7章中，我们只需进行少量调整，就可以将判别器转变为多类分类器。
- en: GANs are as much an art as they are a science. The beauty and the curse of GANs—and,
    indeed, deep learning in general—is that our understanding of what makes them
    work so well in practice is limited. Few known mathematical guarantees exist,
    and most achievements are experimental only. This makes GANs susceptible to many
    training pitfalls, such as mode collapse, which you may recall from our discussion
    in [chapter 5](../Text/kindle_split_015.xhtml#ch05). Fortunately, researchers
    have found many tips and tricks that greatly mitigate these challenges—everything
    from input preprocessing to our choice of optimizer and activation functions—many
    of which you learned about and even saw firsthand in code tutorials thought the
    book. Indeed, as the GAN variants covered in this chapter show, the techniques
    to improve GANs continue to evolve.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs既是艺术也是科学。GANs的美丽和诅咒——实际上，深度学习总体上也是如此——在于我们对它们在实践中为何如此有效理解有限。已知数学保证很少，大多数成就仅限于实验。这使得GANs容易受到许多训练陷阱的影响，例如模式崩溃，您可能还记得我们在第5章中的讨论。[第5章](../Text/kindle_split_015.xhtml#ch05)。幸运的是，研究人员已经找到了许多技巧和窍门，这些技巧和窍门极大地减轻了这些挑战——从输入预处理到我们选择的优化器和激活函数——其中许多您在本书中已经了解，甚至在代码教程中亲自看到了。确实，正如本章介绍的GAN变体所显示的，提高GANs的技术仍在不断发展。
- en: In addition to difficulties in training, it is crucial to keep in mind that
    even techniques as powerful and versatile as GANs have other important limitations.
    GANs have been hailed by many as the technique that gave machines the gift of
    creativity. This is true to a degree—in a few short years, GANs have become the
    undisputed state-of-the-art technique in synthesizing fake data; however, they
    fall short of what human creativity can do.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练中的困难之外，还必须牢记，即使像GANs这样强大而多功能的技巧也有其他重要的局限性。GANs被许多人誉为赋予机器创造力的技术。这在一定程度上是正确的——在短短几年内，GANs已经成为合成假数据的无可争议的最先进技术；然而，它们在人类创造力方面还有所不足。
- en: Indeed, as we showed time and time again throughout this book, GANs can mimic
    the features of almost any existing dataset and come up with examples that look
    as though they came from that dataset. However, by their very nature, GANs will
    not stray far from the training data. For instance, if we have a training dataset
    of classical art masterpieces, the examples our GAN will produce will look more
    like Michelangelo than Jackson Pollock. Until a new AI paradigm comes along that
    gives machines true autonomy, it will be ultimately up to the (human) researcher
    to guide the GAN to the desired end goal.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，正如我们在本书中一次又一次地展示的那样，GANs可以模仿几乎所有现有数据集的特征，并提出看起来似乎来自该数据集的例子。然而，由于它们的本质，GANs不会远离训练数据。例如，如果我们有一个经典艺术大师作品的训练数据集，我们的GAN产生的例子将看起来更像米开朗基罗而不是杰克逊·波洛克。除非出现一种新的AI范式，赋予机器真正的自主性，否则最终将由（人类）研究人员引导GAN达到预期的最终目标。
- en: As you experiment with GANs and their applications, bear in mind not only the
    practical techniques, tips, and tricks covered throughout this book, but also
    the ethical considerations discussed in this chapter. With that, we wish you all
    the best in the GANtastic journey ahead.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 当你实验GAN及其应用时，请记住，不仅包括在这本书中涵盖的实用技术、技巧和窍门，还包括本章讨论的伦理考量。带着这些，我们祝愿你们在GAN之旅中一切顺利。
- en: —Jakub and Vladimir
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: —雅库布和弗拉基米尔
- en: Summary
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: We touched on AI and GAN ethics and discussed the moral frameworks, need for
    awareness, and openness of discussion.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了人工智能和生成对抗网络（GAN）的伦理问题，并探讨了道德框架、意识需求和讨论的开放性。
- en: 'We equipped you with the innovations we believe will drive the future of GANs,
    and we gave you the high-level idea behind the following:'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提供了我们认为将推动GAN未来发展的创新，并给出了以下高级概念的背景想法：
- en: Relativistic GAN, which now ensures that the Generator considers the relative
    likelihood of real and generated data
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对论GAN，它现在确保生成器考虑真实数据和生成数据的相对可能性
- en: SAGAN, with attention mechanisms that act similarly to human perception
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAGAN，其注意力机制与人类感知相似
- en: BigGAN, which allowed us to generate all 1,000 ImageNet classes of unprecedented
    quality
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigGAN，它使我们能够生成前所未有的1,000个ImageNet类别
- en: 'We highlighted two key recurring themes of our book: (1) the versatility of
    GANs and (2) the necessity for experimentation because, much like the rest of
    deep learning, GANs are as much an art as they are a science.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们强调了本书的两个关键重复主题：（1）GAN的通用性以及（2）实验的必要性，因为，与深度学习的其他部分一样，GAN既是艺术也是科学。
