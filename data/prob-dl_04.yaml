- en: 3 Principles of curve fitting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 曲线拟合的3个原则
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How to fit a parametric model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何拟合参数模型
- en: What a loss function is and how to use it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数是什么以及如何使用它
- en: Linear regression, the mother of all neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归，所有神经网络之母
- en: Gradient descent as a tool to optimize a loss function
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降作为优化损失函数的工具
- en: Implementing gradient descent with different frameworks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同框架实现梯度下降
- en: '![](../Images/3-unnumb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb.png)'
- en: 'DL models became famous because they outperformed traditional machine learning
    (ML) methods in a broad variety of relevant tasks such as computer vision and
    natural language processing. From the previous chapter, you already know that
    a critical success factor of DL models is their deep hierarchical architecture.
    DL models have millions of tunable parameters, and you might wonder how to tune
    these so that the models behave optimally. The solution is astonishingly simple.
    It’s already used in many methods in traditional ML: you first define a loss function
    that describes how badly a model performs on the training data and then tune the
    parameters of the model to minimize the loss. This procedure is called fitting.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型之所以闻名，是因为它们在计算机视觉和自然语言处理等广泛的任务中优于传统的机器学习（ML）方法。从上一章，你已经知道深度学习模型的关键成功因素是其深层的层次结构。深度学习模型拥有数百万可调整的参数，你可能想知道如何调整这些参数以使模型表现最优。解决方案惊人地简单。它已经在许多传统的机器学习方法中得到了应用：你首先定义一个损失函数，该函数描述了模型在训练数据上的表现有多糟糕，然后调整模型的参数以最小化损失。这个过程被称为拟合。
- en: 'In ML models that have simple loss functions, it’s often possible to provide
    a formula that lets you compute the optimal parameter values from the data. Yet,
    this isn’t the case for complex models. For complex models, it took several decades
    to develop sophisticated optimization procedures. DL almost exclusively uses an
    approach to determine the parameters: this is called gradient descent. In this
    chapter, you’ll see that gradient descent is an astonishingly simple technique.
    It’s still kind of a miracle that this technique works so well in DL, while more
    advanced procedures for optimization fail.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有简单损失函数的机器学习模型中，通常可以提供一个公式，让你从数据中计算出最优的参数值。然而，对于复杂模型来说并非如此。对于复杂模型，开发复杂的优化过程花费了几十年时间。深度学习几乎完全使用一种确定参数的方法：这被称为梯度下降。在本章中，你会发现梯度下降是一种惊人的简单技术。这在深度学习中仍然是一种奇迹，因为这种技术在深度学习中效果如此之好，而更高级的优化过程却失败了。
- en: All the components needed for DL--a model with weights and a loss function used
    to fit the weights to the (training) data--are already present in much simpler
    models like linear regression. In order to give you a clear picture of the gradient
    descent method, we demonstrate step by step how it works for fitting a simple
    linear regression model. This, in fact, is the smallest neural network (NN) possible.
    We introduce the loss function, which is most often used to fit a linear regression
    model, and show how to determine the parameter values that minimize this loss
    function. Think of linear regression as the “hello world” of machine and deep
    learning. Let’s have a look at linear regression through the eyes of DL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所有深度学习所需的组件——一个具有权重和损失函数的模型，用于将权重拟合到（训练）数据中——在像线性回归这样的更简单模型中已经存在。为了给你一个清晰的梯度下降方法图景，我们逐步演示了它是如何用于拟合一个简单的线性回归模型的。实际上，这是可能的最小神经网络（NN）。我们介绍了损失函数，这是最常用于拟合线性回归模型的函数，并展示了如何确定最小化这个损失函数的参数值。将线性回归视为机器和深度学习的“Hello
    world”。让我们从深度学习的角度来审视线性回归。
- en: 3.1 “Hello world” in curve fitting
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 曲线拟合中的“Hello world”
- en: Let’s look at a simple model in the form of linear regression. Imagine you’re
    a novice medical assistant, and you work together with a gynecologist who asks
    you to notify her if a patient shows an unusual systolic blood pressure (SBP)
    during routine checks. For this purpose, she provides you with a table that shows
    a normal range of SBP values for each age category. According to this table, it’s
    normal that the SBP increases with age. This raises your curiosity. You wonder
    whether the data is consistent with actual data measured during past routine checks.
    To follow up on this, you want to look at some other data. Fortunately, you’re
    allowed to use the data from your patients for internal analysis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的线性回归模型。想象一下，你是一名初级医疗助理，你和一位妇科医生一起工作，她要求你在常规检查中如果患者出现异常的收缩压（SBP），就通知她。为此，她提供给你一张表格，显示每个年龄段的正常SBP值范围。根据这张表格，SBP随年龄增加是正常的。这引起了你的好奇心。你想知道数据是否与过去常规检查中测量的实际数据一致。为了跟进这一点，你希望查看一些其他数据。幸运的是，你被允许使用患者的数据进行内部分析。
- en: As a first step, you randomly select a set of 33 patients, who during at least
    one visit were diagnosed as healthy by the doctor. From each of the selected patients,
    you note information on age and blood pressure recorded during one of the routine
    checks, where the patient was diagnosed as healthy. To get an impression of the
    data, you can produce a scatter plot where you graph the SBP value versus the
    age (see figure 3.1).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，你随机选择一组33名患者，他们在至少一次就诊中被医生诊断为健康。从每位选定的患者中，你记录下在常规检查中记录的年龄和血压信息，其中患者被诊断为健康。为了对数据有一个印象，你可以制作一个散点图，其中绘制收缩压（SBP）值与年龄的关系（见图3.1）。
- en: '![](../Images/3-1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-1.png)'
- en: Figure 3.1 Scatter plot showing the systolic blood pressure (SBP) versus age
    (for women)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 显示收缩压（SBP）与年龄（女性）关系的散点图
- en: From figure 3.1, you can see that most SBP values are in the range 100 to 220\.
    The scatter plot reveals that, similar to the table your boss gave you, there
    is indeed a trend indicating that for healthy women, blood pressure increases
    with age. It also makes sense that the table gives a range of normal blood pressures
    for each age because the relationship between age and SBP is far from deterministic.
    (Two healthy women with similar ages can have quite different blood pressure values.)
    Information in the table thus seems consistent with your observations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从图3.1中，你可以看到大多数SBP值都在100到220之间。散点图显示，与你的上司给你的表格一样，确实存在一种趋势，表明对于健康的女性来说，血压随着年龄的增长而增加。表格给出每个年龄的正常血压范围也是合理的，因为年龄和SBP之间的关系远非确定性。（两个年龄相似的健康女性可以有相当不同的血压值。）因此，表格中的信息似乎与你的观察结果一致。
- en: You now want to go a step further and look for a model that describes how the
    blood pressure depends on age. Independent from individual variations in blood
    pressure, it seems that, on average, the SBP values somehow increase linearly
    with the age of the women. You can manually draw a straight line through the points
    to end up with something similar to the line in figure 3.1\. We can describe such
    a straight line by the linear model
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你希望更进一步，寻找一个描述血压如何随年龄变化的模型。独立于血压的个体差异，似乎平均而言，SBP值以某种方式与女性的年龄呈线性增加。你可以手动通过这些点画一条直线，得到类似于图3.1中的线条。我们可以用线性模型来描述这样的直线
- en: '*y* = *a* ⋅ *x* + *b*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *a* ⋅ *x* + *b*'
- en: where a represents the slope of the line and *b* is the intercept with the y-axis.
    Another way (which is more DL-like) is to look at the model of the graph shown
    in figure 3.2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，a代表直线的斜率，*b*是y轴的截距。另一种方式（更类似于深度学习）是查看图3.2中所示图形的模型。
- en: '![](../Images/3-2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-2.png)'
- en: Figure 3.2 Linear regression model (*y* = *a* · *x* + *b*) as a fully connected
    neural network (fcNN) or a computational graph
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 线性回归模型（*y* = *a* · *x* + *b*）作为一个全连接神经网络（fcNN）或计算图
- en: 'This graph is probably the smallest fully connected network (fcNN) you can
    think of. At the same time, it’s a part of the more complicated one that was shown
    in figure 1.2 in chapter 1\. Moreover, it graphically represents the computational
    steps that you need to take to compute a value for *y* when you have a value for
    *x* and you know the parameter values a and *b* :'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是你能想到的最小的全连接网络（fcNN）。同时，它也是第一章中图1.2所示更复杂网络的一部分。此外，它图形化地表示了当你有x的值并且知道参数值a和*b*时，你需要采取的计算步骤来计算y的值：
- en: Take *x* and multiply it by *a*(*a* ⋅ *x*)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将*x*乘以*a*(*a* ⋅ *x*)
- en: Take 1 and multiply it by *b*(*b* ⋅ 1)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将1乘以*b*(*b* ⋅ 1)
- en: Add both results to get *y* = *a* ⋅ *x* + *b*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将两个结果相加得到*y* = *a* ⋅ *x* + *b*
- en: Regardless of whether you interpret the model as a line equation or as an fcNN,
    you can only use the model to compute an estimate for the mean *y* based on a
    given value of *x* when the values for a and *b* are fixed. Therefore, a and *b*
    are called the parameters of the model. Because the parameters enter the model
    linearly, we call the model a linear model, and because there’s only one input
    feature *x*, we call it a simple linear model. How can you get appropriate values
    for the parameters a (slope) and *b* (intercept) of the linear model that best
    describes the data?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你将模型解释为线方程还是作为fcNN，只有当a和b的值固定时，你才能使用模型根据给定的x值计算*y*的均值估计。因此，a和b被称为模型的参数。因为参数以线性方式进入模型，所以我们称该模型为线性模型，并且因为只有一个输入特征*x*，我们称它为简单线性模型。你如何得到描述数据的线性模型中参数a（斜率）和*b*（截距）的最佳适当值？
- en: 3.1.1  Fitting a linear regression model based on a loss function
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 基于损失函数拟合线性回归模型
- en: You can use your gut feeling to manually draw a straight line through the points
    in the scatter plot. If you do, you might end up with a line like that shown in
    figure 3.1, which has a slope of *a* = 1 and an intercept of *b* = 100\. You can
    now use this model to make predictions, say, for example, for a new patient who
    is 31 years old.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用你的直觉手动在散点图中的点之间画一条直线。如果你这样做，你可能会得到图3.1中显示的类似直线，斜率*a* = 1，截距*b* = 100。现在你可以使用这个模型进行预测，比如，例如，对于一个31岁的新的病人。
- en: If you use the network or equation representation of the model, you multiply
    31 with 1 and add 100, resulting in a predicted mean SBP value of 131.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用模型的网络或方程表示，将31乘以1并加上100，得到预测的平均SBP值为131。
- en: If you use the graphical representation of the fitted line in figure 3.1, you
    go from *x* = 31 vertically up until you reach the fitted line and then horizontally
    left until you reach the y-axis.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用图3.1中拟合线的图形表示，你从*x* = 31垂直向上直到达到拟合线，然后水平向左直到达到y轴。
- en: From here, you can read the fitted value of approximately ![](../Images/equation_3-04.png).
    Fitted values are often indicated with a hat on top (see figure 3.3\. and table
    3.1). For convenience, we often drop these hats, and we frequently neglect the
    hat when reporting estimated or fitted parameter values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，你可以读取拟合值大约为![](../Images/equation_3-04.png)。拟合值通常用顶部的帽子表示（参见图3.3和表3.1）。为了方便起见，我们经常省略这些帽子，并且在报告估计或拟合参数值时经常忽略帽子。
- en: If you ask different persons for an eyeball analysis and let them draw a straight
    line through the points in figure 3.1, you’ll probably get similar but not identical
    linear models. To decide how good the different suggested models fit the data
    and to eventually find the best fitting model, you need a clear and quantitative
    criterion.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要求不同的人进行目测分析，并让他们在图3.1中的点之间画一条直线，你可能会得到相似但不完全相同的线性模型。为了决定不同的建议模型如何拟合数据，并最终找到最佳拟合模型，你需要一个清晰和量化的标准。
- en: '![](../Images/3-3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-3.png)'
- en: Figure 3.3 Measured data points (Dots) and linear model (straight line). The
    differences between the data points and the linear model (vertical lines) are
    called residuals. Do you see the “hat” on the right?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 测量数据点（点状）和线性模型（直线）。数据点和线性模型（垂直线）之间的差异称为残差。你看到右边的“帽子”了吗？
- en: In figure 3.3, we show the real data points, *y**[i]*(Dots), with the differences
    from the values predicted by the model (the many vertical lines). Statisticians
    call these differences between observed values and estimated values residuals.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.3中，我们展示了实际数据点，*y**[i]*（点状），以及与模型预测值（许多垂直线）之间的差异。统计学家将这些观察值和估计值之间的差异称为残差。
- en: Table 3.1 Data and derived quantities for the first 5 entries in figure 3.3,
    calculated with slope = 1 and intercept = 100
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 图3.3前5个条目的数据和导出量，计算时斜率为1，截距为100
- en: '| *x*  | *y*  | *Ŷ = a · x + b* | Residual | Squared residual |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| *x*  | *y*  | *Ŷ = a · x + b* | Residual | Squared residual |'
- en: '| 22 | 131 | 122 | 009 | 081 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 131 | 122 | 009 | 081 |'
- en: '| 41 | 139 | 141 | 0-2 | 004 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 41 | 139 | 141 | 0-2 | 004 |'
- en: '| 52 | 128 | 152 | -24 | 576 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 52 | 128 | 152 | -24 | 576 |'
- en: '| 23 | 128 | 123 | 005 | 025 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 23 | 128 | 123 | 005 | 025 |'
- en: '| 41 | 171 | 141 | 030 | 900 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 41 | 171 | 141 | 030 | 900 |'
- en: One famous rule, first used by Gauss in the 1790s and first published by Legendre
    in 1805, is the squared error criterion. You may remember this from introductory
    statistics courses. Simply choose a and *b* so that the sum of the squared errors
    is minimal. Why squared and not absolute or cubed values? We come to that point
    later in chapter 4\. This is often called the residual sum of squared errors or
    RSS for short. Let’s calculate the RSS for the 5 data points in figure 3.3, assuming
    *a* = 1 and *b* = 100.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的规则，最早由高斯在1790年代使用，最早由勒让德在1805年发表，是平方误差准则。你可能从入门统计学课程中记得这个。简单地选择 a 和 *b*，使得平方误差之和最小。为什么是平方而不是绝对或立方值？我们将在第4章中稍后讨论这一点。这通常被称为残差平方和（RSS）或简称
    RSS。让我们计算图3.3中5个数据点的RSS，假设 *a* = 1 和 *b* = 100。
- en: 'Summing up all terms in the column Squared Residual results in an RSS of 1,586\.
    In general the RSS is computed as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将“平方残差”列中的所有项相加，得到RSS为1,586。一般来说，RSS的计算如下：
- en: '![](../Images/equation_3-05.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![方程式3-05](../Images/equation_3-05.png)'
- en: The RSS quantifies the deviation between the observed target values *y**[i]*
    and the modeled values yˆi . It’s a criterion that measures how poorly a model
    fits the data. Therefore, it’s clear that the best fitting model has the smallest
    RSS. The RSS is 0 if the model perfectly fits all data points; otherwise, it’s
    larger than 0.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RSS衡量的是观测到的目标值 *y**[i]* 和模型值 yˆi 之间的偏差。它是一个衡量模型拟合数据好坏的准则。因此，很明显，最佳拟合模型具有最小的RSS。如果模型完美地拟合所有数据点，RSS为0；否则，它大于0。
- en: You saw from the plot in the blood pressure example (figure 3.1) that RSS can’t
    be 0 because a straight line won’t go through all points. But what would be an
    acceptable RSS? That is almost impossible to tell because the RSS increases with
    each additional data point used for the fit (which doesn’t lie directly on the
    regression line).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从血压示例中的图表（图3.1）中看到，RSS不能为0，因为直线不会穿过所有点。但什么是可接受的RSS？这几乎是不可能的，因为RSS随着用于拟合的额外数据点的增加而增加（这些点不直接位于回归线上）。
- en: But the fit doesn’t get worse when you use more data. If you divide RSS by the
    number of data points, you get the mean squared deviation of a modeled value yˆi
    from the observed value *y**[i]* . This quantity does not systematically increase
    (or decrease) with the number of data points. This is called the mean squared
    error (MSE). Because the number of data points n is constant, the MSE is just
    the RSS divided by n and has for the same model the minimum value as the RSS (see
    equation 3.1). The computed RSS as 1,586 transforms to an MSE of 1,586 / 5, which
    equals 317.2\. You get this value for the MSE if each modeled blood pressure deviates
    from the observed blood pressure value by √(1586/5)=17.8 .
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用更多数据时，拟合不会变得更差。如果你将RSS除以数据点的数量，你得到的是模型值 yˆi 与观测值 *y**[i]* 的平均平方偏差。这个量不会系统地随着数据点的数量增加（或减少）。这被称为平均平方误差（MSE）。因为数据点的数量
    n 是常数，MSE只是RSS除以 n，对于同一模型，MSE的最小值与RSS相同（见方程式3.1）。计算出的RSS为1,586，转换为MSE为1,586 /
    5，等于317.2。如果你得到的MSE值是每个模型血压值与观测血压值偏差的√(1586/5)=17.8。
- en: 'Usually you don’t have the same error at each data point. Because the errors
    contribute squared to the loss, some large errors need to be balanced by many
    small errors. The MSE doesn’t depend on the sample size of the data and has a
    nicer interpretation than the RSS, so it’s often preferred. We finally look at
    our first loss function with pride--the MSE:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常你不会在每个数据点上犯相同的错误。因为误差对损失的贡献是平方的，一些大的误差需要通过许多小的误差来平衡。均方误差（MSE）不依赖于数据的样本大小，并且比均方和（RSS）有更好的解释，所以它通常更受欢迎。我们最后以自豪的心情来看我们的第一个损失函数——均方误差（MSE）：
- en: Equation 3.3
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式3.3
- en: '![](../Images/equation_3-07.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![方程式3-07](../Images/equation_3-07.png)'
- en: A close look at the formula reveals that this loss function measures the mean
    of the squared deviation of the observable target values, *y**[i]* , and the modeled
    values, yˆi . The model parameters a and *b* are the variables of the loss function
    for which you want to find the values that minimize the loss. Besides the parameters,
    the loss function depends also on the data *x* and *y*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察公式可以发现，这个损失函数衡量的是可观测的目标值 *y**[i]* 和模型值 yˆi 的平方偏差的平均值。模型参数 a 和 *b* 是损失函数的变量，你想要找到使损失最小化的值。除了参数之外，损失函数还依赖于数据
    *x* 和 *y*。
- en: 'Now let’s look at some code (see listing 3.1). For determining the value of
    the loss for the whole data set (not only 5 points), you need to fix the model
    parameters to some values (here *a* = 1 and *b* = 100) and use the formula in
    equation 3.1 to compute the MSE loss. (In DL, we make heavy use of linear algebra
    to speed up the computations.) You can write the squared residual vector in Python
    as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些代码（参见列表 3.1）。为了确定整个数据集（而不仅仅是 5 个点）的损失值，你需要将模型参数固定到某些值（这里 *a* = 1 和
    *b* = 100），并使用方程 3.1 中的公式来计算 MSE 损失。（在深度学习中，我们大量使用线性代数来加速计算。）你可以在 Python 中如下编写平方残差向量：
- en: (*y* − (*a* ⋅ *x* + *b*))² =(*y* − *a* ⋅ *x* − *b*)²
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (*y* − (*a* ⋅ *x* + *b*))² =(*y* − *a* ⋅ *x* − *b*)²
- en: This expression corresponds to the last column in table 3.1 as a vector.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式对应于表 3.1 的最后一列，作为一个向量。
- en: Note that we dropped the row number i. The quantities *y* and *x* are now vectors;
    consequently, *y* − *a* ⋅ *x* is also a vector. If you look closely at the expression,
    you might wonder about the compatibility of the dimensions. If *y* − *a* ⋅ *x*
    is a vector, how can you add (or subtract) a scalar *b* to (or from) it? This
    is called broadcasting, and basically what happens is that *b* is also transformed
    into a vector having the same length as *y* with all elements having the value
    b. See section 2.3.2 in François Chollet’s Deep Learning with Python (Manning,
    2017) at [http://mng.bz/6Qde](http://mng.bz/6Qde) . The following listing shows
    how you can compute the MSE in Python.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们省略了行号 i。现在，*y* 和 *x* 是向量；因此，*y* − *a* ⋅ *x* 也是一个向量。如果你仔细观察这个表达式，可能会对维度的兼容性感到疑惑。如果
    *y* − *a* ⋅ *x* 是一个向量，你怎么能向（或从）它添加（或减去）一个标量 *b* 呢？这被称为广播，基本上发生的事情是 *b* 也会被转换成一个与
    *y* 长度相同的向量，所有元素都具有值 b。参见 François Chollet 的《Python 深度学习》（Manning, 2017）第 2.3.2
    节，[http://mng.bz/6Qde](http://mng.bz/6Qde) 。下面的列表显示了如何在 Python 中计算均方误差（MSE）。
- en: Listing 3.1 Calculation of the MSE in Python with NumPy
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 使用 NumPy 在 Python 中计算 MSE
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Column 3 in table 3.1 (note the vector notation and broadcasting)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 表 3.1 的第 3 列（注意向量表示和广播）
- en: ❷ Column 4 in table 3.1
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 表 3.1 的第 4 列
- en: ❸ MSE calculated from column 5 in table 3.1 (created, summed, and divided by
    the number of data points in the data set).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从表 3.1 的第 5 列计算出的 MSE（在数据集的数据点数量上创建、求和并除以数据点的数量）。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open the notebook [http://mng.bz/oPOZ](http://mng.bz/oPOZ)
    and step through the code to see how you can simulate some data, fit a linear
    regression model to it, and determine the MSE for a linear model with the specified
    parameter values. You will soon reach the first exercises in the notebook, indicated
    by a pen icon. In this exercise, we ask you to experiment with different parameter
    values. The goal is to manually find the values that yield the smallest MSE for
    the parameters *a* and *b*. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 打开笔记本 [http://mng.bz/oPOZ](http://mng.bz/oPOZ)
    并逐步执行代码，以了解如何模拟一些数据，将其拟合到线性回归模型中，并确定具有指定参数值的线性模型的 MSE。你很快就会到达笔记本中的第一个练习，由笔形图标指示。在这个练习中，我们要求你尝试不同的参数值。目标是手动找到产生最小的
    MSE 的 *a* 和 *b* 参数值。|'
- en: As described previously, you can use the data from 33 women with known age and
    measured blood pressure in the notebook. With this information, you can easily
    compute the loss for a proposed model with *a* = 1 and *b* = 100.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以使用笔记本中 33 位已知年龄和测量血压的妇女的数据。有了这些信息，你可以轻松地计算具有 *a* = 1 和 *b* = 100 的建议模型的损失。
- en: Let’s find the best-fitting linear model! In figure 3.1, you saw that in this
    example, you can’t find a straight line that runs through all data points. Therefore,
    it won’t be possible to reduce the loss to zero. Instead, you want to find the
    parameter values for a and *b* that minimize the MSE.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找到最佳拟合线性模型！在图 3.1 中，你看到在这个例子中，你找不到穿过所有数据点的直线。因此，不可能将损失减少到零。相反，你想要找到使 MSE
    最小的 *a* 和 *b* 参数值。
- en: Closed form solution for the optimal parameter estimates in a simple linear
    regression model
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归模型中最佳参数估计的闭式解
- en: 'Now, you’ll see a derivation of a formula that lets you directly compute the
    optimal parameter values from the training data. Finding the minimum of the loss
    function requires finding the variable values for which the first derivative gets
    0\. This requires solving the following two conditions for *a* and *b*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将看到一个公式的推导，该公式允许你直接从训练数据中计算最优参数值。找到损失函数的最小值需要找到使一阶导数为 0 的变量值。这需要解决以下两个关于
    *a* 和 *b* 的条件：
- en: '![](../Images/equation_3-10sb.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 3-10sb](../Images/equation_3-10sb.png)'
- en: 'You can do the calculation as an exercise resulting in a linear equation system
    with two equations and two unknowns: parameter a (slope) and *b* (intercept).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将计算作为练习来做，这将导致一个包含两个方程和两个未知数（参数 a（斜率）和 *b*（截距））的线性方程组。
- en: '![](../Images/equation_3-11s.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 3-11s](../Images/equation_3-11s.png)'
- en: As soon as you have at least two data points with different values for *x**[i]*
    , you can determine the solution with a formula that directly yields the parameter
    values after plugging in the values of the training data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你至少有两个具有不同 *x**[i]* 值的数据点，你就可以通过将训练数据的值代入公式来直接确定解，从而得到参数值。
- en: This is called a closed-form solution. In the previous formula, the hat above
    a and *b* indicates that these values are estimated from the data. This result
    is simple. It allows you to directly compute the optimal parameter estimates for
    â and bˆ, which minimize the MSE. All you need for that is the data (x and *y*).
    In our blood pressure example, this yields the value *â* = 1.1 for the slope and
    *b̂* =87,67 for the intercept.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为闭式解。在之前的公式中，a 和 *b* 上方的帽子表示这些值是从数据中估计出来的。这个结果很简单。它允许你直接计算最优参数估计值 â 和 bˆ，以最小化均方误差（MSE）。为此，你只需要数据（x
    和 *y*）。在我们的血压示例中，这给出了斜率的值 *â* = 1.1 和截距的值 *b̂* = 87.67。
- en: For this example, it’s quite easy to derive the formula that lets you determine
    the optimal parameter values (see sidebar). This is only possible because the
    model has a simple loss function, and we worked with quite few data. If you work
    with complex models like NNs, it isn’t possible to directly compute the parameters
    that optimize the loss from the training data (like we did in the sidebar). Another
    reason that hinders direct calculations for the parameters are huge data sets.
    With DL, you usually have too many data points and no closed-form solution. In
    such cases, you need to use an iterative approach. For this purpose, we use gradient
    descent, which we explain in the next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，推导出确定最优参数值的公式相当简单（见侧边栏）。这仅因为模型具有简单的损失函数，并且我们处理的数据量很少。如果你处理的是复杂的模型，如神经网络（NNs），则无法直接从训练数据中计算优化损失的参数（如我们在侧边栏中所做的那样）。阻碍参数直接计算的另一个原因是数据集很大。在深度学习中，通常数据点太多，没有闭式解。在这种情况下，你需要使用迭代方法。为此，我们使用梯度下降，我们将在下一节中解释。
- en: 3.2 Gradient descent method
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 梯度下降法
- en: Gradient descent is the most often used iterative optimization method in ML
    and for almost all optimizations in DL. You’ll learn how this method works, first
    with one parameter and then with two. Be assured that the same method also works
    to optimize the millions of parameters in DL models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是机器学习中最常用的迭代优化方法，也是深度学习（DL）中几乎所有优化的常用方法。你将学习这种方法是如何工作的，首先是一个参数，然后是两个参数。请放心，同样的方法也可以用来优化深度学习模型中的数百万个参数。
- en: 3.2.1 Loss with one free model parameter
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 具有一个自由模型参数的损失
- en: 'To give you a clear picture of gradient descent, we want to start with the
    particular case where you only have to optimize one model parameter. For that,
    we’ll stay with our blood pressure example and assume that you somehow know that
    the optimal value for the intercept is *b* = 87.6\. In order to minimize the loss
    function, you only need to find the value of the second parameter, a. In this
    case, the loss, which we discussed previously, looks as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你对梯度下降有一个清晰的了解，我们想从这样一个特殊情况开始，即你只需要优化一个模型参数。为此，我们将继续使用我们的血压示例，并假设你以某种方式知道截距的最优值为
    *b* = 87.6。为了最小化损失函数，你只需要找到第二个参数 a 的值。在这种情况下，我们之前讨论的损失函数如下所示：
- en: '![](../Images/equation_3-14.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 3-14](../Images/equation_3-14.png)'
- en: Based on this equation, it’s easy to compute the loss for *x* and *y* from the
    training data for each proposed parameter a.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个方程，很容易从训练数据中计算每个提议的参数 a 对应的损失 *x* 和 *y*。
- en: Intuition of gradient descent
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的直觉
- en: Figure 3.4 plots the relationship between the loss and the value of the parameter
    a. In the figure, you can see that the minimum of the loss achieved for the value
    a is slightly larger than 1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 绘制了损失与参数 a 的值之间的关系。在图中，你可以看到对于 a 的值，损失的最低点略大于 1。
- en: '![](../Images/3-4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 3-4](../Images/3-4.png)'
- en: Figure 3.4 A plot of the loss (see equation 3.1) versus the free regression
    model parameter a, with *b* fixed to the optimal value. At the position *a* =
    0, the tangent is plotted as a dashed line; at *a* = 2, the tangent is plotted
    as a dashed line.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：损失（见方程3.1）与自由回归模型参数 a 的关系图，其中 *b* 固定为最优值。在 *a* = 0 的位置，切线以虚线绘制；在 *a* =
    2 的位置，切线以虚线绘制。
- en: How can you systematically find the value of the parameter a for where the loss
    is minimal? It might be helpful for you to imagine that the loss function is a
    1D landscape in which a blind wanderer from the 1D world wants to go to the minimum.
    If the landscape is bowl-shaped like the loss in figure 3.4, this is quite an
    easy task. Even if the wanderer is blind and can only explore the local environment,
    it’s quite clear that walking in the direction that points downwards is needed.
    But what does this mean with respect to (w.r.t.) the value a? That depends on
    the local slope. If the local slope is negative as in the position of the wanderer
    in figure 3.4 (see the negative slope of the dashed tangent at the position *a*
    = 0), a step in the direction of a larger value a leads towards the minimum. At
    a position with positive local slope (see the positive slope of the dotted tangent
    at position *a* = 2), a step in the direction of a smaller value a leads towards
    the minimum.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何系统地找到使损失最小的参数 a 的值？想象一下，损失函数是一个一维景观，其中来自一维世界的盲探索者想要到达最小值，这可能对你有所帮助。如果景观像图3.4中的损失一样呈碗状，这将是一项相当容易的任务。即使探索者是盲目的，只能探索局部环境，也很清楚需要朝向下方指示的方向行走。但是，这与值
    a 有什么关系呢？这取决于局部斜率。如果在图3.4中探索者的位置局部斜率为负（见 *a* = 0 处虚线切线的负斜率），则朝向更大值 a 的方向迈步会指向最小值。在具有正局部斜率的位置（见
    *a* = 2 处虚线切线的正斜率），朝向更小值 a 的方向迈步会指向最小值。
- en: The derivative of the function gives the slope of the tangent. Because we’re
    looking at the loss in dependency of a, this derivative is called gradient and
    denoted by *grad**[a]*(*Loss*). Its sign (positive or negative) indicates the
    direction where the loss function is increasing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的导数给出了切线的斜率。因为我们关注的是依赖性损失，所以这个导数被称为梯度，表示为 *grad**[a]*(*Loss*)。其符号（正或负）指示损失函数增加的方向。
- en: You can see in figure 3.4 that the curve is flatter closer to the minimum and
    gets steeper for points further away from that. The steeper the slope, the larger
    the absolute value of the gradient. If your wanderer is a mathematician, you should
    provide the advice to take a step in a direction that is given by the negative
    sign of the *grad**[a]*(*Loss*) function. The step size would be proportional
    to the absolute value of the gradient. The wanderer should take large steps if
    farther from the minimum, where the slope is steep. And to avoid overstepping,
    taking smaller steps is needed when getting closer to the minimum, where the slope
    becomes flatter. Only at the minimum is the slope zero.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图3.4中看到，曲线在接近最小值时更平缓，而在远离该点的点处变得更陡峭。斜率越陡，梯度的绝对值就越大。如果你的探索者是一位数学家，你应该提供这样的建议：在
    *grad**[a]*(*Loss*) 函数的负号指示的方向上迈步。步长将与梯度的绝对值成比例。当探索者远离最小值，斜率较陡时，应该迈大步。为了避免越界，当接近最小值，斜率变得平缓时，需要采取更小的步长。只有在最小值处，斜率才为零。
- en: 'How do you choose the proportional factor, which is called the learning rate
    : η (eta)? This might sound picky, but you’ll soon see that getting the learning
    rate (η) right is probably the most critical ingredient for a successful optimization
    using gradient descent.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何选择比例因子，即学习率 η（eta）？这听起来可能有些挑剔，但很快你就会看到，正确选择学习率（η）可能是使用梯度下降进行成功优化的最关键因素。
- en: The example of the blind wandering mathematician should help you develop a feel
    for how to systematically tune the parameter a to get to the minimum of the loss
    function. The wordy description presented here can be transformed into crisp mathematical
    instructions on how to iteratively change the parameter value, which in gradient
    descent is called the update rule.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 盲目漫游的数学家的例子应该能帮助你培养如何系统地调整参数 a 以到达损失函数最小值的感觉。这里提供的冗长描述可以转化为关于如何迭代改变参数值的清晰数学指令，这在梯度下降中被称为更新规则。
- en: The update rule in gradient descent
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降中的更新规则
- en: 'Here’s the parameter update formula according to the gradient descent optimization
    procedure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是根据梯度下降优化过程得到的参数更新公式：
- en: '*a**[t+]*[1] − *a**[t]* − *η* ⋅ *grad**[a]*(*Loss*) Equation 3.2'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*a**[t+]*[1] − *a**[t]* − *η* ⋅ *grad**[a]*(*Loss*) 方程3.2'
- en: 'This update rule summarizes the iterative procedure of gradient descent: you
    first start with a random guess a0 for the parameter value a (for example, *a*[0]
    = 0). Then you determine the value of the gradient of the loss function w.r.t.
    the parameter a. A negative sign in *grad**[a]*(*Loss*) indicates the direction
    in which parameter a needs to be changed in order to go in the direction of decreasing
    loss. (you’ll see in section 3.4 how to compute gradients.) The update step size
    is proportional to the absolute value of the gradient and to the learning rate,
    η (eta). Equation 3.2 ensures that you take a step in the direction of decreasing
    loss. You repeat this stepwise updating of parameter a until convergence. Using
    the found model for the parameter a leads to the model that fits the data best.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更新规则总结了梯度下降的迭代过程：你首先对参数值a（例如，*a*[0] = 0）进行随机猜测。然后确定损失函数关于参数a的梯度值。*grad**[a]*(*Loss*)中的负号表示参数a需要改变的方向，以便朝着减小损失的方向前进。（你将在第3.4节中看到如何计算梯度。）更新步长与梯度的绝对值和学习率η（eta）成正比。方程3.2确保你朝着减小损失的方向迈出一步。你重复这一步对参数a的逐步更新，直到收敛。使用找到的参数a模型会导致最适合数据的模型。
- en: How large a learning rate, η (eta), in the update formula (equation 3.2) should
    you choose? You might be tempted to use a large learning rate in order to reach
    the minimum much sooner.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新公式（方程3.2）中，你应该选择多大的学习率η（eta）？你可能倾向于使用大的学习率，以便更快地达到最小值。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Again, open the notebook
    [http://mng.bz/oPOZ](http://mng.bz/oPOZ) and continue to work through the code
    after exercise 1\. This lets you see how you can determine the values for the
    slope and the intercept via a closed formula and how you can implement the gradient
    descent method to tune one parameter when the second parameter is fixed. At the
    end of the notebook, you will find a second exercise, where your task is to study
    the impact of the learning rate on the convergence. Do it. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 再次打开笔记本[http://mng.bz/oPOZ](http://mng.bz/oPOZ)，在练习1之后继续编写代码。这让你看到如何通过封闭公式确定斜率和截距的值，以及如何实现梯度下降方法来调整一个参数，当第二个参数固定时。在笔记本的末尾，你会找到一个第二个练习，你的任务是研究学习率对收敛的影响。做一下。|'
- en: What do you observe? You probably see that with large learning rates, the loss
    function gets larger and larger instead of decreasing. It finally reaches NaN
    or infinity in a few iterations. What happened?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你观察到了什么？你可能看到，随着学习率的增大，损失函数越来越大，而不是减小。它最终在几次迭代后达到NaN或无穷大。发生了什么？
- en: Take a look at figure 3.5, which shows the dependency of the loss to parameter
    a, the slope in our blood pressure example. (The intercept was fixed at its optimal
    value of 87.6.) The only task remaining is to find the optimal value for parameter
    a. This is done via gradient descent. In figure 3.5, you can see the development
    of the value a when starting from a first guess of *a*[1] = 0,5 , using three
    different learning rates.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下图3.5，它显示了损失与参数a的依赖关系，即我们血压示例中的斜率。（截距固定在其最优值87.6。）剩下的唯一任务是找到参数a的最优值。这是通过梯度下降来完成的。在图3.5中，你可以看到从第一次猜测*a*[1]
    = 0.5出发，使用三种不同的学习率时a值的发展情况。
- en: '![](../Images/3-5.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图3-5](../Images/3-5.png)'
- en: Figure 3.5 Plots of the loss from equation 3.1 versus the free regression model
    parameter a. It shows the results of 5 steps of gradient descent, starting with
    *a* = 0.5 with different learning rates. With a learning rate of 0.00010, the
    minimum is approximately reached in 5 steps without overshooting. With a learning
    rate of 0.00030, the minimum is also reached after approximately 5 steps but overshooting
    the position of the minimum twice. With a learning rate of 0.00045, the updates
    for parameter a always overshoot the position of the minimum. In this case, the
    updated values for a are more and more apart from the minimum. The corresponding
    loss grows without bounds.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5展示了方程3.1的损失与自由回归模型参数a的曲线图。它显示了以*a* = 0.5为起始点，不同学习率下的5步梯度下降的结果。当学习率为0.00010时，最小值在5步内大致达到，没有超过最小值。当学习率为0.00030时，最小值在大约5步后达到，但两次超过了最小值的位置。当学习率为0.00045时，参数a的更新总是超过最小值的位置。在这种情况下，a的更新值越来越远离最小值。相应的损失无限制地增长。
- en: In figure 3.5, you can see that the learning rate is a critical hyperparameter.
    If you choose a value too small, a lot of update steps are needed to find the
    optimal model parameter. However, if the learning rate is too large (see figure
    3.5, right panel), it’s impossible to converge to the position of the optimal
    parameter value a for which the loss is minimal. If the learning rate is too large,
    the loss increases with each update. This leads to numerical problems, resulting
    in NaNs or in infinity. When you observe that the loss gets infinity or NaN, there’s
    a saying that goes, “Keep calm and lower your learning rate.” The next time you
    see that your loss in the training set gets higher and higher instead of lower
    and lower, try decreasing the learning rate (Dividing it by 10 is a good guess
    to begin with).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.5中，你可以看到学习率是一个关键的超参数。如果你选择一个过小的值，需要很多更新步骤来找到最优模型参数。然而，如果学习率过大（见图3.5，右面板），则无法收敛到损失最小的最优参数值a的位置。如果学习率过大，损失会随着每次更新而增加。这会导致数值问题，导致NaN或无穷大。当你观察到损失变为无穷大或NaN时，有句话叫做，“保持冷静，降低学习率。”下次当你看到训练集中的损失越来越高而不是越来越低时，尝试降低学习率（最初将其除以10是一个好的猜测）。
- en: 3.2.2 Loss with two free model parameters
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 具有两个自由模型参数的损失
- en: Now the somewhat artificial condition of parameter *b* that’s fixed at its optimal
    value is lifted, and the loss function is simultaneously optimized for the two
    parameters *a* and *b*. We’ve already computed the optimal values *a* = 1.1 ,
    *b* = 87.67 . In figure 3.6, you see the loss function of the blood pressure example
    for the different values *a* and *b*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将参数*b*固定在其最优值的某种人为条件移除，同时优化两个参数*a*和*b*的损失函数。我们已经计算出了最优值 *a* = 1.1 ， *b* =
    87.67 。在图3.6中，你可以看到血压例子中不同值 *a* 和 *b* 的损失函数。
- en: '![](../Images/3-6.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图3-6](../Images/3-6.png)'
- en: Figure 3.6 The loss function for the blood pressure data for the different values
    *a* and *b*. You can see that the loss function is shaped more like a ravine than
    a bowl. The isolines at the bottom indicate positions of equal loss values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 血压数据不同值 *a* 和 *b* 的损失函数。你可以看到损失函数的形状更像是峡谷而不是碗。底部的等值线表示损失值相等的位置。
- en: If you recall the parabolic shape of the 1D loss function in figure 3.4, it
    probably comes as a surprise to you that the 2D loss surface isn’t shaped like
    a bowl, but rather looks like a ravine. It turns out that in most examples of
    linear regression, the loss surface looks more like a ravine or, to be more precise,
    an elongated bowl. Although these loss surfaces do have a minimum, it’s flat in
    some directions. Ravine-like loss surfaces are hard to optimize because these
    need many optimization steps to reach the minimum (see later in this chapter for
    a more detailed example).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回忆一下图3.4中1D损失函数的抛物线形状，你可能会对2D损失表面不是碗形，而更像峡谷感到惊讶。实际上，在大多数线性回归的例子中，损失表面看起来更像峡谷，或者更准确地说，是一个拉长的碗。尽管这些损失表面确实有最小值，但在某些方向上是平的。类似峡谷的损失表面很难优化，因为这些需要许多优化步骤才能达到最小值（本章后面将给出一个更详细的例子）。
- en: Let’s explain the optimization for the two variables a and *b* with a simulated
    example of linear regression that has a well-behaved loss with a bowl shape and
    not a ravine. You return to the ravine-like shape of the loss function for the
    blood pressure example later.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个线性回归的模拟例子来解释变量a和*b*的优化，这个例子有一个形状良好的碗形损失函数，而不是峡谷形。稍后，你会在血压例子中看到损失函数类似峡谷的形状。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open the notebook [http://mng.bz/nPp5](http://mng.bz/nPp5)
    and work through it until the section with the title “The gradients.” You’ll see
    how to simulate data for a simple linear regression fit where the loss function
    has a bowl-like shape. Figure 3.7 is produced with the code of this notebook and
    shows the loss surface of the simulated data set. Indeed, it’s shaped like a bowl.
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 打开笔记本 [http://mng.bz/nPp5](http://mng.bz/nPp5)
    并完成它，直到标题为“梯度”的部分。你将看到如何模拟一个简单线性回归拟合的数据，其中损失函数具有碗形形状。图3.7是用这个笔记本的代码生成的，显示了模拟数据集的损失表面。实际上，它看起来像是一个碗。'
- en: '![](../Images/3-7.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图3-7](../Images/3-7.png)'
- en: Figure 3.7 The loss function for the simulated data for the different values
    *a* and *b*. You can see that the loss function is shaped more like a bowl. The
    contour lines at the bottom indicate positions of equal loss values. The large
    dot indicates the position of the start value (a and *b* = 3). The right side
    shows a zoom of that region.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 对于模拟数据的不同值*a*和*b*的损失函数。你可以看到损失函数的形状更像一个碗。底部的等高线表示损失值相等的位置。大点表示起始值的位置（a和*b*
    = 3）。右侧显示了该区域的放大图。
- en: The optimization procedure via gradient descent is similar to the case where
    one variable remains fixed. Let’s start with the initial values of *a* = 3 and
    *b* = 3; the loss function at that position is 6.15\. The first step in the optimization
    is to find which direction shows the steepest descent and then take a small step
    downhill in that direction.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过梯度下降的优化过程与一个变量保持固定的情况相似。让我们从*a* = 3和*b* = 3的初始值开始；该位置的损失函数是6.15。优化的第一步是找到哪个方向显示最陡下降，然后在该方向上向下迈一小步。
- en: If you zoom very close to the initial starting point, you’ll see that the bowl
    looks like a plate; this is true for any point of the bowl. If asked to take a
    small step downhill, starting from *a* = 3 and *b* = 3, you’ll probably agree
    that it should be in the direction towards the center. How can you calculate this
    direction? Let’s use a marble to determine this direction for us.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你非常接近初始起始点，你会看到碗看起来像一个盘子；这对于碗上的任何点都是正确的。如果你被要求从*a* = 3和*b* = 3开始向下迈一小步，你可能同意应该朝向中心的方向。你如何计算这个方向？让我们用一个弹珠来确定这个方向。
- en: 'We place the marble at the upper right corner (*a* = 3, *b* = 3). The marble
    can only roll in the direction of the resultant force acting on it. As you might
    recall from high school physics, the resultant force is based on the forces acting
    in different directions (see figure 3.8). As the force in a certain direction
    gets stronger, the steeper the slope in that direction. We know that the slope
    in the direction a is grad_a. Hence, the force on the marble in direction a is
    proportional to (the operator ∝) the negative slope or gradient (as in the 1D
    example) in that direction:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将弹珠放在右上角（*a* = 3，*b* = 3）。弹珠只能沿着作用在其上的合力方向滚动。你可能还记得高中物理，合力是基于不同方向上作用的力（见图3.8）。当某个方向上的力变强时，该方向上的斜率就越陡。我们知道方向a的斜率是grad_a。因此，弹珠在方向a上的力与该方向上的负斜率或梯度（如在1D示例中）成正比：
- en: '*f**[a]* ∝ − *grad**[a]*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*f**[a]* ∝ − *grad**[a]*'
- en: For the physics nerds, the force is given by the mass of the marble times the
    acceleration of gravity (*ɡ* ≈ 9.81 *m* /*s*²) times grad_a. Similarly, the force
    in the direction *b* is
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于物理爱好者，力由弹珠的质量乘以重力加速度（*ɡ* ≈ 9.81 *m* / *s*²）乘以grad_a给出。同样，方向*b*上的力是
- en: '*f**[b]* ∝ − *grad**[b]*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*f**[b]* ∝ − *grad**[b]*'
- en: 'The total force on the marble is given by this equation:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 弹珠上的总力由这个方程给出：
- en: '![](../Images/equation_3-23.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-23.png)'
- en: 'If you now release the marble, it’ll roll a step in the direction of the steepest
    descent, which is usually not along one single axis a or *b* (see figure 3.8).
    The learning rate η times the gradient gives the step size. For the concrete example,
    this results in a new coordinate with the update formula:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在释放弹珠，它将沿着最陡下降的方向滚动一步，这通常不是沿着单个轴a或*b*（见图3.8）。学习率η乘以梯度给出步长。对于具体例子，这导致一个新的坐标，其更新公式为：
- en: '*a**[t+1]* = *a**[t]* - *η* ⋅ *grad**[a]*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*a**[t+1]* = *a**[t]* - *η* ⋅ *grad**[a]*'
- en: '*b**[t+1]* = *b**[t]* - *η* ⋅ *grad**[b]* Equation 3.3'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*b**[t+1]* = *b**[t]* - *η* ⋅ *grad**[b]* 方程式3.3'
- en: '![](../Images/3-8.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-8.png)'
- en: Figure 3.8 The forces dragging on the marble and the resultant direction of
    the steepest descent
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 拖拽弹珠的力和最陡下降方向的结果方向
- en: 'This shows that for determining the new position in the 2D parameter space,
    you need to calculate the 1D derivation twice. First, you keep *b* fixed at 3
    and just vary a. In more mathematical terms, “You calculate the partial derivative
    of the loss function w.r.t. a.” You keep the parameter *b* fixed and do the derivation
    of the loss function (MSE) w.r.t. a to get:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，为了确定2D参数空间中的新位置，你需要计算一阶导数两次。首先，你将*b*固定在3，只改变a。用更数学的话来说，“你计算损失函数关于a的偏导数。”你保持参数*b*不变，对损失函数（均方误差）关于a求导，得到：
- en: '![](../Images/equation_3-26.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-26.png)'
- en: 'This is the same formula as in the 1D case (see equation 3.1). You may have
    noticed that we’ve swapped the observed *y* and the fitted value ax + *b* in the
    squared error. However, this neither changes the value of the loss nor of the
    gradient, but it facilitates the notation and differentiation. To determine the
    partial derivative of the loss w.r.t. b, you can keep the parameter a fixed and
    do the derivation w.r.t. *b* :'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这与一维情况下的公式相同（见方程式3.1）。你可能已经注意到，我们在平方误差中交换了观测到的*y*和拟合值ax + *b*。然而，这既没有改变损失值，也没有改变梯度值，但它简化了符号和微分。为了确定损失对b的偏导数，你可以保持参数a不变，并对*b*进行微分：
- en: '![](../Images/equation_3-27.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![方程式3-27](../Images/equation_3-27.png)'
- en: Now you can plug these numbers into the update formula (see equation 3.3) to
    get the new values for *a* and *b*. You reach the minimum of the loss function
    if you choose a learning rate that’s small enough. If you have a bowl-like structure,
    you reach the minimum quite fast. But for ravine-like landscapes with a flat minimum,
    you finally reach the minimum value with more effort--it just takes longer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将这些数字代入更新公式（见方程式3.3），以获得*a*和*b*的新值。如果你选择足够小的学习率，你将达到损失函数的最小值。如果你有一个类似碗的结构，你将很快达到最小值。但对于具有平坦最小值的峡谷状景观，你最终需要付出更多努力才能达到最小值——这需要更长的时间。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open again the notebook
    [http://mng.bz/nPp5](http://mng.bz/nPp5) and continue to work through it from
    the section “The gradients” onward. You’ll see how to compute the gradient of
    the loss w.r.t parameters via the gradient formula and how to update the parameter
    values via the update formula. To perform the gradient descent method manually,
    you repeat the update step several times until you arrive close to the minimum
    of the loss function. The provided code uses the data that was simulated in the
    beginning of the notebook. At the end of the notebook, you reach an exercise indicated
    by the pen symbol, where your task is to perform the manual gradient method for
    the blood pressure data and compare the number of needed steps with the case of
    simulated data. You’ll see that you need many more steps in the blood pressure
    case. The loss function has a ravine-like shape rather than a bowl-like shape
    as in the case of the simulated data. |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 再次打开笔记本 [http://mng.bz/nPp5](http://mng.bz/nPp5)
    并从“梯度”部分继续工作。你会看到如何通过梯度公式计算损失相对于参数的梯度，以及如何通过更新公式更新参数值。要手动执行梯度下降法，你需要重复更新步骤，直到你接近损失函数的最小值。提供的代码使用笔记本开头模拟的数据。在笔记本的末尾，你将达到由笔符号指示的练习，你的任务是手动执行血压数据的梯度方法，并比较所需步骤数与模拟数据的情况。你会发现血压情况下需要更多的步骤。损失函数具有类似峡谷的形状，而不是模拟数据中的类似碗的形状。
    |'
- en: Note that while the direction of the steepest descent is probed by using the
    marble, a real marble would follow a different path. This is due to the fact that
    a real marble gathers some momentum and, thus, doesn’t follow the steepest descent
    when moving. There are some advanced methods that include the idea of momentum
    in gradient descent, but a standard gradient descent has no momentum. This is
    often misunderstood in optimization.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然使用弹珠探测最速下降的方向，但真实的弹珠会遵循不同的路径。这是因为真实的弹珠会聚集一些动量，因此在移动时不会遵循最速下降。有一些高级方法包括在梯度下降中引入动量的概念，但标准梯度下降没有动量。这在优化中常常被误解。
- en: 'Similar to the simple 1D example, the learning rate is a critical parameter.
    Hopefully you’ve noticed this in the exercise and set it correctly. Another question
    is whether you always reach the minimum regardless of the starting values. Recall
    what you’re doing: you start at a certain position, calculate the local direction
    of the steepest descent, and then go one step in that direction. The length of
    the step depends on the steepness and the learning rate. The steeper the descent,
    the larger the gradient, which corresponds to a larger step. Closer to a minimum,
    the curve gets flatter and the gradient smaller. This means that you should take
    a small step to avoid overshooting. Figure 3.9 shows this quite remarkably for
    a more complex landscape.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的1D示例类似，学习率是一个关键参数。希望你在练习中已经注意到了这一点，并正确设置了它。另一个问题是，无论起始值如何，你是否总能达到最小值。回想一下你在做什么：你从一个特定的位置开始，计算局部最陡下降的方向，然后朝那个方向迈出一步。步长取决于陡峭程度和学习率。下降越陡峭，梯度越大，对应的步长也越大。接近最小值时，曲线变得越平坦，梯度越小。这意味着你应该采取小步以避免超调。图3.9非常显著地展示了这一点，对于更复杂的景观。
- en: '![](../Images/3-9.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-9.png)'
- en: Figure 3.9 A wanderer undertaking gradient descent. At a certain position, he
    seeks the direction of the steepest descent. Note that the wanderer can’t see
    the whole landscape; he just knows his altitude and the local direction of the
    steepest descent. He then goes down a bit (Depending on the learning rate and
    the steepness) in the direction of the steepest descent. Do you believe he’ll
    reach the valley? Figure inspired by the Stanford Deep Learning course (cs231n).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 一个在梯度下降中徘徊的人。在某个位置，他寻找最陡下降的方向。请注意，这个徘徊者看不到整个景观；他只知道自己的高度和局部最陡下降的方向。然后他沿着最陡下降的方向稍微下降一点（取决于学习率和陡峭程度）。你相信他会到达山谷吗？这幅图灵感来源于斯坦福深度学习课程（cs231n）。
- en: Concerning your simple linear regression problem, if you look at figure 3.7,
    you see that the loss function is shaped like a bowl, which has only one minimum.
    We call that a convex problem. It’s obvious and can be shown that regardless of
    the starting values, the gradient descent (with a sufficiently small learning
    rate) always finds the (single) minimum in convex problems.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 关于你的简单线性回归问题，如果你看图3.7，你会看到损失函数的形状像一个碗，只有一个最小值。我们称之为凸问题。很明显，并且可以证明，无论起始值如何，只要学习率足够小，梯度下降（在凸问题中）总能找到（单个）最小值。
- en: A DL model is much more flexible than a linear regression model. The loss function
    looks much more complicated than a higher dimensional bowl. It rather resembles
    the landscape with the wanderer shown in figure 3.9, which has several local minima
    (like the lake). It’s thus quite a miracle that the simple gradient descent algorithm
    works fine to fit these complex DL models. Some recent research shows that usually
    all minima in the loss landscape of a DL model are equally good and, therefore,
    getting stuck in local minima isn’t really a problem.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个深度学习（DL）模型比线性回归模型要灵活得多。损失函数看起来比一个高维的碗要复杂得多。它更像是图3.9中展示的带有徘徊者的景观，有几个局部最小值（就像湖泊）。因此，简单的梯度下降算法能够很好地拟合这些复杂的深度学习模型，这真是一个奇迹。一些最近的研究表明，通常深度学习模型损失景观中的所有最小值都是同样好的，因此陷入局部最小值并不是真正的问题。
- en: 3.3 Special DL sauce
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 特殊深度学习技巧
- en: 'Simple linear regression already includes almost all ingredients necessary
    for training a DL model. There are only three more special tricks you need to
    know:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的线性回归已经包含了训练深度学习模型所需的所有基本成分。你只需要再了解三个特殊的技巧：
- en: Using mini-batch gradient descent to calculate the loss function in a typical
    DL setting that has millions of data points
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小批量梯度下降在具有数百万数据点的典型深度学习设置中计算损失函数
- en: Using variants of the stochastic gradient descent (SGD) to speed up the learning
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机梯度下降（SGD）的变体来加速学习
- en: Automating the differentiation process
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化微分过程
- en: The first two tricks are easy; the third one is a bit involved. But don’t worry
    we will go slow. Let’s start with the easy ones first.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个技巧很简单；第三个稍微复杂一些。但别担心，我们会慢慢来。让我们先从简单的开始。
- en: 3.3.1 Mini-batch gradient descent
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 小批量梯度下降
- en: 'The first trick is called mini-batch gradient descent. It lets you calculate
    the loss function in a typical DL setting with millions of data points. As you
    can see, the loss function depends on all data points:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个技巧被称为小批量梯度下降。它允许你在具有数百万数据点的典型深度学习设置中计算损失函数。正如你所见，损失函数依赖于所有数据点：
- en: '![](../Images/equation_3-28.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 3-28](../Images/equation_3-28.png)'
- en: The same applies for the gradients. In DL, you typically do the calculations
    on a graphics processing unit (GPU). These devices have limited memory, often
    too small to fit all the data on it. A solution is to use a randomly chosen subset
    (a mini-batch) of the data points for approximately calculating the loss function
    and its gradient. The gradient calculated when using a mini-batch is sometimes
    higher and sometimes lower than the gradient you get when using all data points.
    This is why mini-batch gradient descent is also referred to as stochastic gradient
    descent (SGD).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 同样适用于梯度。在深度学习中，你通常在图形处理单元（GPU）上执行计算。这些设备内存有限，通常太小，无法容纳所有数据。一种解决方案是使用随机选择的数据点子集（一个迷你批次）来近似计算损失函数及其梯度。使用迷你批次计算出的梯度有时高于使用所有数据点时得到的梯度，有时则低于。这就是为什么迷你批次梯度下降也被称为随机梯度下降（SGD）。
- en: On an average, the gradients calculated with this mini-batch procedure don’t
    systematically deviate from those calculated using all data points. Statisticians
    call this an unbiased estimate of the gradient. When using this unbiased estimate,
    the values of the parameters are updated as before. You could even just use a
    mini-batch of size 1 and update the weights after each training instance. In fact,
    some people restrict the name SGD to cases where only a single data point is used
    at a time to calculate the gradients and refer to the methods described as mini-batch
    gradient descent instead.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，使用这种迷你批次程序计算出的梯度与使用所有数据点计算出的梯度没有系统地偏离。统计学家称这为无偏估计的梯度。当使用这种无偏估计时，参数的值会像以前一样更新。你甚至可以使用大小为
    1 的迷你批次，并在每个训练实例之后更新权重。事实上，有些人将 SGD 的名称限制在每次只使用一个数据点来计算梯度的情况下，并将描述的方法称为迷你批次梯度下降。
- en: Deep NNs have up to many millions of parameters, and it’s known that the shape
    of the loss landscape is far from looking like a smooth bowl (it’s a non-convex
    problem). For non-convex problems, the gradient descent procedure might get stuck
    in a local minima, like the wanderer in figure 3.9 who ends up getting wet in
    the lake instead of reaching the global minimum at the bottom of the valley. Nevertheless,
    SGD works great to fit DL models.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络可能有数百万个参数，而且已知损失景观的形状远非看起来像是一个光滑的碗（它是一个非凸问题）。对于非凸问题，梯度下降过程可能会陷入局部最小值，就像图
    3.9 中的漫游者最终在湖里弄湿了，而不是达到山谷底部的全局最小值。尽管如此，SGD 在拟合深度学习模型方面效果很好。
- en: So far, it hasn’t been completely understood why the simple gradient descent
    procedure works so well in DL. It has been suggested that one of the reasons is
    that you don’t need to reach the absolute global minimum but that many other minimas
    are fine as well. It has even been suggested that the gradient descent in DL tends
    to find solutions that generalize well to unseen novel data, better than the global
    minimum would do. DL is still in its infancy; it works, but we don’t know why.
    Mini-batch gradient descent is one of the main ingredients to the secret sauce
    of DL.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，还没有完全理解为什么简单的梯度下降程序在深度学习中效果如此之好。有人提出，其中一个原因是，你不需要达到绝对的全局最小值，但许多其他最小值也足够好。甚至有人提出，深度学习中的梯度下降倾向于找到对未见的新数据泛化良好的解决方案，比全局最小值做得更好。深度学习仍然处于起步阶段；它有效，但我们不知道为什么。迷你批次梯度下降是深度学习秘方的主要成分之一。
- en: 3.3.2 Using SGD variants to speed up the learning
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 使用 SGD 变体加速学习
- en: The second trick is a bit less spectacular and deals with variants of the SGD
    to speed up the learning. If you did the exercise with the blood pressure data
    and not the simulated one, you might wonder whether it’s really necessary to have
    100,000 iterations for an easy problem like linear regression. There are more
    advanced methods of optimization that also just calculate the local gradient,
    no further derivative is used, but do some more or less clever tricks. These apply
    the heuristics of using the values of previous iterations to speed up performance.
    You can think about this with reference to the marble, where the momentum depends
    on the direction of the steepest gradients in the previous steps.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个技巧稍微不那么引人注目，它处理 SGD 的变体以加速学习。如果你使用的是血压数据而不是模拟数据来做练习，你可能会想知道对于像线性回归这样的简单问题，是否真的需要
    100,000 次迭代。有一些更高级的优化方法也只计算局部梯度，不使用进一步的导数，但进行了一些更或更聪明的技巧。这些方法应用了使用先前迭代值的启发式方法来加速性能。你可以参考弹珠，其中动量取决于先前步骤中最陡峭梯度的方向。
- en: 'Two prominent algorithms are RMSProb and Adam. They are included in all DL
    frameworks like TensorFlow, Keras, PyTorch, and others. These methods aren’t fundamentally
    different from SGD but are of great use in speeding up the learning process, which
    is achieved by taking the last couple of updates into account. But for understanding
    the principles of DL, you don’t need to know the details of these variants of
    SGD. We, therefore, just provide a reference that explains these techniques excellently:
    [https://distill.pub/2017/momentum/](https://distill.pub/2017/momentum/) .'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 两种突出的算法是RMSProb和Adam。它们包含在所有深度学习框架中，如TensorFlow、Keras、PyTorch等。这些方法与SGD在本质上没有区别，但在加速学习过程中非常有用，这是通过考虑最后几次更新来实现的。但为了理解深度学习的原理，你不需要知道这些SGD变体的细节。因此，我们只提供一个参考，它出色地解释了这些技术：[https://distill.pub/2017/momentum/](https://distill.pub/2017/momentum/)。
- en: 3.3.3 Automatic differentiation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 自动微分
- en: 'The third trick is automatic differentiation. While it’s possible to calculate
    the gradient for the linear regression analytical by hand, this gets practically
    impossible for DL models. Luckily, there exist several approaches to do the dull
    procedure of differentiation automatically. The underlying method is nicely explained
    in Chollet’s book, Deep Learning with Python :'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个技巧是自动微分。虽然可以手动计算线性回归的梯度，但对于深度学习模型来说，这实际上是不可能的。幸运的是，存在几种自动进行微分的方法。这种方法在Chollet的《Python深度学习》一书中得到了很好的解释：
- en: In practice, a neural network function consists of many tensor operations chained
    together . . . , each of which has a simple, known derivative.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，一个神经网络函数由许多连在一起的张量操作组成……，每个操作都有一个简单、已知的导数。
- en: 'Calculus tells us that such a chain of functions can be derived using the following
    identity, called the chain rule:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分告诉我们，可以使用以下恒等式来推导这样的函数链，这个恒等式被称为链式法则：
- en: '*(f* ’(*g*(*x*)))’ = *f* ''(*g*(*x*)) ⋅ *g* ''(*x*)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*(f* ’(*g*(*x*)))’ = *f* ''(*g*(*x*)) ⋅ *g* ''(*x*)'
- en: When applying the chain rule to the computation of the gradient values of a
    neural network we get an algorithm called Backpropagation(also sometimes called
    reverse-mode differentiation). Backpropagation starts with the final loss value
    and works backwards from the output layer to the input layer, applying on the
    way the chain rule iteratively to compute the gradient of the loss with respect
    to each of the model parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当将链式法则应用于计算神经网络梯度值时，我们得到一个称为反向传播（有时也称为逆模式微分）的算法。反向传播从最终的损失值开始，从输出层反向工作到输入层，在途中迭代地应用链式法则来计算损失相对于每个模型参数的梯度。
- en: You can use these gradients to compute the updated parameter value by using
    the update rule. Note that the update formula, introduced earlier in this chapter
    (see equation 3.2), is valid for each parameter independently, regardless of how
    many parameters a model has. Basically, all you need is the gradient of the functions
    within each layer and the chain rule to glue these together. Modern DL frameworks
    know how to automatically apply the chain rule. These also know the gradient of
    the basic functions used within a NN. You usually don’t have to care about those
    details when fitting a DL model. Let’s still do it to get an idea what’s going
    on under the hood.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这些梯度通过使用更新规则来计算更新后的参数值。请注意，本章前面介绍过的更新公式（见方程式3.2）对每个参数都是有效的，无论模型有多少个参数。基本上，你需要的是每一层中函数的梯度以及链式法则来将这些梯度粘合在一起。现代深度学习框架知道如何自动应用链式法则。它们也知道神经网络中使用的基函数的梯度。在拟合深度学习模型时，你通常不需要关心这些细节。但让我们仍然这样做，以便了解底层发生了什么。
- en: 3.4 Backpropagation in DL frameworks
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 深度学习框架中的反向传播
- en: In most parts of the book, you use the high-level library Keras. This library
    abstracts from the nasty details and lets you quickly build complex models. But
    as every architect should know how to lay bricks and the limits in building mechanics,
    a DL practitioner should understand the underlying principles. So let’s get our
    hands dirty!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，你使用高级库Keras。这个库抽象掉了讨厌的细节，让你可以快速构建复杂模型。但正如每个建筑师都应该知道如何砌砖以及建筑力学的限制一样，深度学习实践者应该理解底层原理。所以，让我们动手实践吧！
- en: 'DL libraries can be grouped according to how they handle automatic differentiation.
    The two approaches for tackling the calculation of the gradients needed for gradient
    descent are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习库可以根据它们处理自动微分的方式分组。解决梯度下降所需梯度计算的两种方法如下：
- en: Static graph frameworks
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态图框架
- en: Dynamic graph frameworks
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态图形框架
- en: 'Static graph frameworks like Theano were the first frameworks used in DL. But
    these are a bit clumsy to work with and were currently replaced or augmented with
    dynamic frameworks such as PyTorch. We start with the description of the static
    framework because it gives you a good idea what is going on under the hood. TensorFlow
    v2.x can handle both approaches. However, in TensorFlow v2.x, static graphs are
    hidden quite a bit, and you usually do not encounter these. Therefore, we now
    switch back to TensorFlow v1.x in the accompanying notebook [http://mng.bz/vxmp](http://mng.bz/vxmp)
    . Note that this notebook does not run in the provided Docker container. We suggest
    that you run it in Colab. If you want to see the computational graph in TensorFlow
    v2.x, you can use the notebook: [http://mng.bz/4AlR](http://mng.bz/4AlR) . However,
    as said, the computational graph is a bit more hidden in TensorFlow v2.0.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Theano 的静态图形框架是深度学习中最先使用的框架。但它们使用起来有点笨拙，目前已被 PyTorch 等动态框架所取代或增强。我们首先描述静态框架，因为它能给您一个很好的内部工作原理的印象。TensorFlow
    v2.x 可以处理这两种方法。然而，在 TensorFlow v2.x 中，静态图形被隐藏得相当多，您通常不会遇到这些。因此，我们现在在配套的笔记本 [http://mng.bz/vxmp](http://mng.bz/vxmp)
    中切换回 TensorFlow v1.x。请注意，这个笔记本在提供的 Docker 容器中无法运行。我们建议您在 Colab 中运行它。如果您想在 TensorFlow
    v2.x 中查看计算图，可以使用笔记本：[http://mng.bz/4AlR](http://mng.bz/4AlR)。然而，正如所说，计算图在 TensorFlow
    v2.0 中隐藏得更多一些。
- en: 3.4.1 Static graph frameworks
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 静态图形框架
- en: Static graph frameworks use a two-step procedure. In the first step, the user
    defines the computations, like multiply a by *x*, add b, and call this y_hat_,
    and so on. Under the hood this results in a structure called a computational graph.
    The first steps in listing 3.2 show how the construction is done, but the code
    isn’t executed yet. The code describes the construction phase of a static graph
    for the linear regression problem using TensorFlow v1.x (see also the notebook
    [http://mng.bz/vxmp](http://mng.bz/vxmp)).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 静态图形框架使用两步程序。在第一步中，用户定义计算，例如乘以 *x*，加 b，并调用这个 y_hat_，等等。在底层，这会产生一个名为计算图的结构的结构。列表
    3.2 的第一步显示了构建是如何进行的，但代码尚未执行。代码描述了使用 TensorFlow v1.x 构建线性回归问题的静态图形的构建阶段（也请参阅笔记本
    [http://mng.bz/vxmp](http://mng.bz/vxmp)）。
- en: Listing 3.2 Construction of the computational graph in TensorFlow
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 TensorFlow 中计算图的构建
- en: '[PRE1]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Constructs a new graph from scratch
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从头开始构建一个新的图形
- en: ❷ Variables with initial values can be optimized later. We name them so that
    they look nicer in the graph.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 具有初始值的变量可以在以后进行优化。我们这样命名它们，以便在图形中看起来更美观。
- en: ❸ Constants that are fixed tensors, which hold the data values
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 固定的张量常数，它们持有数据值
- en: ❹ Symbolic operations create new nodes in the computational graph.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 符号操作在计算图中创建新的节点。
- en: ❺ Writes the graph for visualization
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为可视化编写图形
- en: Listing 3.2 defines the building of the static computational graph for the linear
    regression problem in TensorFlow. After the graph is defined, it can be written
    to disk. This is done in the last two lines of the listing.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 定义了 TensorFlow 中线性回归问题的静态计算图的构建。在定义了图形之后，它可以被写入磁盘。这是在列表的最后两行中完成的。
- en: NOTE Google provides a component called TensorBoard, where you can visualize
    the computational graph. To learn more about TensorBoard, you can refer to Chollet’s
    book at [http://mng.bz/QyJ6](http://mng.bz/QyJ6) .
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Google 提供了一个名为 TensorBoard 的组件，您可以在其中可视化计算图。要了解更多关于 TensorBoard 的信息，您可以参考
    Chollet 的书籍，链接为 [http://mng.bz/QyJ6](http://mng.bz/QyJ6)。
- en: Figure 3.10 displays the output of the computational graph. In addition, we
    added some comments (shown with arrows). If you want to reproduce it, just follow
    the steps in the notebook [http://mng.bz/vxmp](http://mng.bz/vxmp) .
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 显示了计算图的输出。此外，我们还添加了一些注释（用箭头表示）。如果您想重现它，只需遵循笔记本 [http://mng.bz/vxmp](http://mng.bz/vxmp)
    中的步骤。
- en: Say namaste to the computational graph, meditate, and feel the tensors flowing!
    TensorFlow lays out computational graphs from bottom to top. Let’s start at the
    lower left corner with the variable a_ (named a_var in the graph). This corresponds
    to line 2 of listing 3.2\. This variable is multiplied by the constant 1D tensor
    (vector) x_ (named x_const in the figure), which holds the 33 values. This is
    done in the node Mul of the computational graph. This is defined in the tiny `a_*x_`
    part of line 6 of listing 3.2.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 向计算图致敬，冥想，感受张量的流动！TensorFlow从下到上布局计算图。让我们从左下角的变量a_（在图中命名为a_var）开始。这对应于列表3.2的第2行。这个变量乘以常数一维张量（向量）x_（在图中命名为x_const），它包含33个值。这是在计算图的Mul节点中完成的。这是在列表3.2的第6行的`a_*x_`小部分中定义的。
- en: '![](../Images/3-10.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-10.png)'
- en: Figure 3.10 The static graph built using listing 3.2 as displayed in TensorBoard
    with some additional annotations (indicated by arrows).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 使用列表3.2构建的静态图，在TensorBoard中显示，并带有一些额外的注释（由箭头指示）。
- en: In general, in figure 3.10, the edges are the flowing tensors, and the nodes
    are operations like multiplication. After the multiplication, the result of a
    times *x* is still a 1D tensor holding 33 values. Going up the graph, *b* is added,
    *y* is subtracted, and finally the expression is squared. When entering the Mean
    node, we still have a 1D tensor holding 33 values, which are then summed and divided
    by 33.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在图3.10中，边是流动的张量，节点是像乘法这样的操作。乘法之后，a乘以x的结果仍然是一个包含33个值的1维张量。沿着图向上，b被添加，y被减去，最后表达式被平方。当进入平均节点时，我们仍然有一个包含33个值的1维张量，然后这些值被求和并除以33。
- en: A minor detail in figure 3.10 is the constant Const entering from the left side.
    Usually the tensors in DL are more complex than 1D tensors; for example, 2D tensors
    (matrices). The Mean node then needs to know whether to average over the rows
    or the columns.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10中的一个细节是来自左侧的常数Const。通常，深度学习中的张量比一维张量更复杂；例如，二维张量（矩阵）。然后平均节点需要知道是按行还是按列平均。
- en: After you stepped through the computational graph, let’s now flow the numbers
    *a* = 0 and *b* = 139 through the graph (139 is the mean of the blood pressure,
    and the slope *a* = 0 implies that the model predicts this mean for each woman
    regardless of her age). For this, we need a materialization/instantiation of the
    graph--in TensorFlow lingo, a session. The next listing shows this.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在你遍历了计算图之后，现在让我们让数值*a* = 0和*b* = 139通过图流动（139是血压的平均值，斜率*a* = 0意味着模型预测每个女性的这个平均值，而不考虑她的年龄）。为此，我们需要图的实例化/具体化——在TensorFlow术语中，会话。下一个列表显示了这一点。
- en: Listing 3.3 Let the tensors flow, a feed-forward pass
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.3 让张量流动，前向传播
- en: '[PRE2]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Starts a session (grabs memory and other resources)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 开始会话（获取内存和其他资源）
- en: ❷ Lets the variables *a* = 0 and *b* = 139 flow through the graph and stores
    the numerical value in res_val
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 让变量*a* = 0和*b* = 139通过图流动，并将数值存储在res_val中
- en: ❸ Prints 673.4545
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印673.4545
- en: ❹ At the end, always close session.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 最后，始终关闭会话。
- en: 'Now that the computation graph is defined, it’s easy for TensorFlow to calculate
    the gradients. The loss here is the MSE (see equation 3.1), which is calculated
    at the very top of the graph. You have seen in the computational graph shown in
    figure 3.10 that the MSE can be computed by performing a couple of basic operations:
    addition, subtraction, multiplication, and squaring. For the gradient descent
    update rule, you need the gradient of the MSE w.r.t. *a* and *b*. The chain rule
    from calculus guarantees that this can be calculated by going back through the
    graph. Every node you pass on your way back to a (or b) contributes an additional
    factor. This factor is the derivative of the output of the node w.r.t to its input.
    It’s called the local gradient.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在计算图已经定义，TensorFlow计算梯度变得很容易。这里的损失是均方误差（MSE）（见方程3.1），它在图的顶部计算。你在图3.10中显示的计算图中看到，MSE可以通过执行几个基本操作来计算：加法、减法、乘法和平方。对于梯度下降更新规则，你需要MSE相对于*a*和*b*的梯度。微积分中的链式法则保证可以通过沿着图回溯来计算。你回溯到a（或b）的过程中经过的每个节点都会贡献一个额外的因子。这个因子是节点输出相对于其输入的导数。它被称为局部梯度。
- en: 'You’ll now see how this is done, step by step, for a concrete example. This
    example shows that when you lower the MSE via the gradient update rule, the better
    the fit. Additionally, you can calculate the gradients in two ways: directly via
    equations 3.3 and 3.4 and step-by-step via backpropagation in the computational
    graph.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将逐步看到如何通过一个具体的例子来完成这个过程。这个例子表明，当您通过梯度更新规则降低均方误差（MSE）时，拟合效果会更好。此外，您可以通过两种方式计算梯度：直接通过方程3.3和3.4，以及通过计算图中的反向传播逐步计算。
- en: We demonstrate this fitting procedure with our blood pressure example via SGD.
    To keep the discussion simple, we use a mini-batch size of 1 (pick a single training
    example) and do one update step manually. We start with the mean blood pressure
    139 and the slope 0 (*b* = 139 and slope *a* = 0); see the solid line in figure
    3.11\. For the first round of SGD, we randomly pick a patient, say number 15\.
    This patient is *x* = 58 years old and has a blood pressure of *y* = 153(see the
    only filled data point in figure 3.11). The initial model predicts for that age
    a blood pressure of 139\. The resulting residuum is visualized by the vertical
    line between the data point and the model prediction. The current loss for this
    data point is the square of the residuum. We now update the model parameters a
    and *b* to lower the loss for the selected patient. To do so, we use the update
    rule (see equation 3.2) for which we need to compute the gradients of the loss
    w.r.t. the two model parameters *a* and *b*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用随机梯度下降法（SGD）的血压示例来说明这个拟合过程。为了使讨论简单，我们使用批大小为1（选择一个训练示例）并手动进行一步更新。我们从平均血压139和斜率0（*b*
    = 139和斜率 *a* = 0）开始；请参见图3.11中的实线。对于SGD的第一轮，我们随机选择一个患者，比如编号15。这位患者是 *x* = 58岁，血压为
    *y* = 153（请参见图3.11中的唯一一个填充数据点）。初始模型预测该年龄的血压为139。通过数据点和模型预测之间的垂直线来可视化剩余量。当前数据点的损失是剩余量的平方。我们现在更新模型参数
    *a* 和 *b* 以降低所选患者的损失。为此，我们使用更新规则（见方程3.2），我们需要计算损失相对于两个模型参数 *a* 和 *b* 的梯度。
- en: 'Let’s calculate the gradients in two ways. First, you use the formulae for
    the gradients that you get with *n* = 1 , *a* = 0 , *b* = 139 , *x* = 58, and
    *y* = 153 :'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以两种方式计算梯度。首先，您使用公式计算梯度，这些梯度是在 *n* = 1，*a* = 0，*b* = 139，*x* = 58，和 *y* =
    153 的情况下得到的：
- en: '![](../Images/equation_3-28b.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-28b.png)'
- en: and
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/equation_3-28c.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-28c.png)'
- en: '![](../Images/3-11.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-11.png)'
- en: Figure 3.11 The blood pressure data set where we use only one data point in
    the first round of SGD fitting. This picked data point is visualized as a filled
    dot. The initial linear model is represented by the solid line and has a slope
    *a* = 0 and intercept *b* = 139\. The dashed line represents the linear model
    after the first update and has a slope *a* = 0.3 and an intercept slightly larger
    than *b* = 139.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11展示了我们只在SGD拟合的第一轮中使用一个数据点的血压数据集。这个选择的数据点以填充点的方式可视化。初始线性模型由实线表示，斜率 *a* =
    0，截距 *b* = 139。虚线表示第一次更新后的线性模型，斜率 *a* = 0.3，截距略大于 *b* = 139。
- en: 'Knowing that these values of the gradients of the MSE loss w.r.t. to the parameters
    a and *b* are both negative tells you that you need to increase the parameter
    values to lower the loss in order to get closer to the picked data point. You
    can check that this makes sense by looking at figure 3.11\. The modeled value
    on the line should go up at the position *x* = 58 to get closer to the observed
    value *y* = 158 . This can be achieved in two ways: by increasing the intercept
    and by increasing the slope. We updated the values of a and *b* by using the formula
    in equation 3.2 with a learning rate *η* = 0.0002 to get:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 知道这些关于均方误差损失相对于参数 *a* 和 *b* 的梯度值都是负值，这告诉您需要增加参数值以降低损失，以便更接近选择的数据点。您可以通过查看图3.11来验证这一点。模型值在
    *x* = 58的位置应该上升以接近观察到的值 *y* = 158。这可以通过两种方式实现：通过增加截距和增加斜率。我们通过使用方程3.2中的公式，学习率
    *η* = 0.0002来更新 *a* 和 *b* 的值，得到：
- en: '*b**[t+1]* = *b**[t]* − *η* ⋅ *grad**[b]* = 139 − 0.0002 ⋅ ( −28) = 139.0056'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*b**[t+1]* = *b**[t]* − *η* ⋅ *grad**[b]* = 139 − 0.0002 ⋅ ( −28) = 139.0056'
- en: and
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '*a**[t]*[+1] = *a**[t]* − *η* ⋅ *grad**[b]* = 0 − 0.0002 ⋅ ( −1624) = 0.3248'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*a**[t]*[+1] = *a**[t]* − *η* ⋅ *grad**[b]* = 0 − 0.0002 ⋅ ( −1624) = 0.3248'
- en: The dashed line in figure 3.11 shows the resulting linear model with updated
    values *a* and *b*. Now let’s update the parameter values the TensorFlow way by
    using the computational graph and then check to see if we get the same results.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11中的虚线显示了具有更新值 *a* 和 *b* 的结果线性模型。现在让我们通过使用计算图以TensorFlow的方式更新参数值，然后检查我们是否得到相同的结果。
- en: '![](../Images/3-12.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-12.png)'
- en: Figure 3.12 The forward and backward pass for the concrete blood pressure example
    with one data point (x = 58, *y* = 153) and the initial parameter values *a* =
    0 and *b* = 139\. The left side of the graph shows the flowing values in the forward
    pass; the right side shows the flowing values of the gradients during the backward
    pass.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 对于具有一个数据点（x = 58, *y* = 153）和初始参数值 *a* = 0 和 *b* = 139的混凝土血压示例的前向和反向传播。图的左侧显示了前向传播中的流动值；右侧显示了反向传播中梯度的流动值。
- en: 'You first start to calculate the loss in what’s called the forward pass. The
    intermediate results are written on the left side of figure 3.12\. You do the
    computation step by step and give the intermediate term names to keep track of
    those. (you’ll need them later in the backward pass.) You start on the lower left
    side by multiplying a by *x* : *ax* = *a* ⋅ *x* = 0 ⋅ 58 = 0 . Then you add *b*
    = 139 to it and get 139\. Working your way further up the graph, you subtract
    *y* = 153 , yielding *r* = −14 . Then you square that to get *s* = 196 .'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先开始计算所谓的正向传播中的损失。中间结果写在图3.12的左侧。你逐步进行计算，并给中间项命名以跟踪它们。（你将在反向传播中需要它们。）你从左下角开始，将
    *a* 乘以 *x*：*ax* = *a* ⋅ *x* = 0 ⋅ 58 = 0。然后你加上 *b* = 139，得到 139。继续向上图工作，你减去 *y*
    = 153，得到 *r* = −14。然后你平方它以得到 *s* = 196。
- en: In this case (*n* = 1), the mean operation does nothing. It’s just the identity
    (written with I in figure 3.12), and the final loss is 196.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下（*n* = 1），平均操作不起作用。它只是恒等式（在图3.12中用I表示），最终损失是196。
- en: 'Let’s do the backward path to calculate the partial derivatives of the loss
    w.r.t of the parameters. You want to keep track of the intermediate quantities
    (s, r, abx, and ax) shown in figure 3.12\. Hence, you build the partial derivatives
    of the MSE loss according to those intermediate quantities you pass on your way
    back to the parameters *a* and *b*. To determine the partial derivative of the
    loss function MSE w.r.t. b, you simply walk down the graph from MSE to *b* and
    multiply the local gradients as you encounter those on your way back. The local
    gradients are the derivatives of the outcomes of a certain operation w.r.t. its
    incoming value:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过反向路径来计算损失函数相对于参数的偏导数。你需要跟踪图3.12中显示的中间量（s, r, abx, 和 ax）。因此，你根据你在返回参数 *a*
    和 *b* 的过程中传递的这些中间量来构建MSE损失的偏导数。为了确定损失函数MSE相对于 *b* 的偏导数，你只需沿着图从MSE走到 *b* 并在返回的过程中乘以局部梯度。局部梯度是某个操作的结果相对于其输入值的导数：
- en: '![](../Images/equation_3-47a.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-47a.png)'
- en: 'Let’s verify that this is indeed the gradient of MSE w.r.t to b. We treat symbols
    like ∂s as variables (shocking real mathematicians) and cancel these out on the
    right side of the formula:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证这确实是MSE相对于 *b* 的梯度。我们将像 ∂s 这样的符号视为变量（这会让真正的数学家感到震惊），并在公式的右侧取消这些变量：
- en: '![](../Images/equation_3-47b.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-47b.png)'
- en: You can see that the terms in the product of partial derivatives are the local
    gradients. To calculate the local gradients, you need the derivatives for the
    basic operations that are shown in figure 3.13.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到偏导数乘积中的项是局部梯度。为了计算局部梯度，你需要基本操作（如图3.13所示）的导数。
- en: '![](../Images/3-13.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-13.png)'
- en: Figure 3.13  Local gradients for the backward pass in the linear regression
    example. The circles contain the operations you need, which are square (^), addition
    (+), subtraction (-), and multiplication (·).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13  线性回归示例中反向传播的局部梯度。圆圈包含你需要进行的操作，它们是平方 (^)，加法 (+)，减法 (-) 和乘法 (·)。
- en: 'Let’s do the calculation with the current values (*x* = 58 , *y* = 153 , *a*
    = 0 , and *b* = 139) and determine the terms needed to compute the partial derivative
    of the MSE w.r.t. *b* :'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用当前值（*x* = 58, *y* = 153, *a* = 0, 和 *b* = 139）进行计算，并确定计算MSE相对于 *b* 的偏导数所需的项：
- en: '![](../Images/equation_3-48.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-48.png)'
- en: 'You start on the top of the computational graph with:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你从计算图的顶部开始：
- en: '![](../Images/equation_3-49.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-49.png)'
- en: The next local gradients are
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个局部梯度是
- en: '![](../Images/equation_3-50b.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/equation_3-50b.png)'
- en: 'This yields the gradient of the loss along the *b* axis:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了沿*b*轴的损失梯度：
- en: '![](../Images/equation_3-51b.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![方程式3-51b](../Images/equation_3-51b.png)'
- en: Multiplying the local gradients together gives you the same value as the closed
    formula. Similarly, you can calculate the gradient w.r.t. to a. You don’t need
    to walk down the graph from scratch, however, but can take a shortcut, starting
    at the value of -28 and multiplying it with *∂ax / ∂a* = 58 to yield -1,624\.
    This is the expected value for the gradient of the MSE w.r.t. a. This static graph
    approach is used under the hood when TensorFlow (or any other DL framework using
    static graphs, like Theano) calculates the gradients.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 将局部梯度相乘，得到的值与封闭公式相同。同样，您可以计算关于a的梯度。然而，您不需要从头开始沿着图走，而是可以走捷径，从-28的值开始，乘以*∂ax /
    ∂a* = 58，得到-1,624。这是关于a的MSE梯度的预期值。当TensorFlow（或任何使用静态图的深度学习框架，如Theano）计算梯度时，在底层使用这种静态图方法。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open the notebook [http://mng.bz/XPJ9](http://mng.bz/XPJ9)
    and verify the numerical values of the local gradients. Note that we calculated
    all the intermediate values and gradients just for demonstration of the backpropagation
    procedure. If you want to use gradient descent to get the fitted parameter values,
    you use a simpler code like the example in the notebook [http://mng.bz/vxmp](http://mng.bz/vxmp)
    . |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ![电脑图标](../Images/computer-icon.png) | 实践时间 打开笔记本 [http://mng.bz/XPJ9](http://mng.bz/XPJ9)
    并验证局部梯度的数值。请注意，我们仅为了演示反向传播过程，计算了所有中间值和梯度。如果您想使用梯度下降法获取拟合的参数值，您可以使用笔记本中的示例代码，如[http://mng.bz/vxmp](http://mng.bz/vxmp)所示。|'
- en: Usually you don’t need to apply the gradients in the gradient descent update
    formula by hand. Instead, you can use an optimizer like `tf.train.GradientDescentOptimizer()`
    for that. Listing 3.4 shows the optimizer for the graph. Calling it once will
    do a single gradient descent step with a given learning rate. This is shown at
    the very end of the notebook [http://mng.bz/vxmp](http://mng.bz/vxmp) .
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通常您不需要手动在梯度下降更新公式中应用梯度。相反，您可以使用`tf.train.GradientDescentOptimizer()`之类的优化器。列表3.4显示了图的优化器。调用它一次将执行一次给定学习率的梯度下降步骤。这显示在笔记本[http://mng.bz/vxmp](http://mng.bz/vxmp)的末尾。
- en: Listing 3.4 Fitting the computational graph in TensorFlow
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 TensorFlow中计算图的拟合
- en: '[PRE3]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Adds an additional operation to the graph to optimize the MSE
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向图中添加一个额外的操作以优化MSE
- en: ❷ Closes the session and frees all allocated resources on exit
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在退出时关闭会话并释放所有分配的资源
- en: ❸ Initializes all variables
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化所有变量
- en: ❹ Sets the number of gradient descent steps to 80,000
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置梯度下降步骤数为80,000
- en: ❺ Runs the train_op and mse_, a_, b_
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 运行train_op和mse_、a_、b_
- en: ❻ Restricts printing to every 5,000 records
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 限制打印每5,000条记录
- en: As you can see in the listing, that’s quite some code. For that reason, higher-level
    frameworks were developed to work in conjunction with TensorFlow; Keras is one
    such framework to work on top of TensorFlow (and other) DL libraries. Keras is
    also included in the TensorFlow distribution, so you don’t have to install anything
    to use it. Chollet’s book, Deep Learning with Python, covers Keras in detail.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表所示，这需要相当多的代码。因此，开发了更高级的框架与TensorFlow一起工作；Keras就是这样一种框架，它可以在TensorFlow（和其他）深度学习库之上工作。Keras也包含在TensorFlow发行版中，因此您不需要安装任何东西就可以使用它。Chollet的《Python深度学习》一书详细介绍了Keras。
- en: You can view linear regression as a simple NN, having a dense layer with no
    applied activation function. (Keras uses the word linear to mean no activation.)
    The second line in listing 3.4, the Dense layer, holds the instruction for a linear
    combination of *x* weighted with the parameter a and the bias parameter *b* to
    determine the output ax + *b* of the only node in the output layer. Further, you
    can build the whole graph and the optimizer with four lines of Keras code as shown
    in the following listing.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将线性回归视为一个简单的NN，它有一个没有应用激活函数的密集层。（Keras使用“线性”一词表示没有激活。）列表3.4的第二行，Dense层，包含了使用参数a和偏置参数*b*对*x*进行加权线性组合的指令，以确定输出层中唯一节点的输出ax
    + *b*。此外，您可以使用四行Keras代码构建整个图和优化器，如下所示。
- en: Listing 3.5 Construction of the computational graph in Keras
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 Keras中计算图的构建
- en: '[PRE4]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Starts building the model
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 开始构建模型
- en: ❷ Adds a single dense layer with no activation function
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加一个没有激活函数的单个密集层
- en: Adding a dense layer with no activation function in this listing is linear regression
    (see also listing 2.1 and figure 3.2). The following listing shows the training
    in Keras for fitting the computational graph.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中添加一个没有激活函数的密集层是线性回归（也参见列表 2.1 和图 3.2）。下面的列表展示了 Keras 中用于拟合计算图的训练过程。
- en: Listing 3.6 Fitting the computational graph in Keras
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 在 Keras 中拟合计算图
- en: '[PRE5]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open the notebook [http://mng.bz/yyEp](http://mng.bz/yyEp)
    to see the complete code of listings 3.5 and 3.6\. Go ahead and play with these.
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/computer-icon.png) | 实践时间 打开笔记本 [http://mng.bz/yyEp](http://mng.bz/yyEp)
    以查看列表 3.5 和 3.6 的完整代码。大胆尝试并玩转这些代码。|'
- en: 3.4.2 Dynamic graph frameworks
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 动态图框架
- en: The main issue with static libraries is that due to the two-step procedure (first
    building the graph and then executing it), debugging is quite cumbersome. In dynamic
    graph frameworks, the graph is defined and evaluated on the fly. You therefore
    have access to the real numerical values at every point in the code. This has
    tremendous advantages when it comes to debugging. Moreover, static graphs have
    the disadvantage that you can’t include conditionals and loops in these in order
    to react dynamically for different input. Chainer and Torch were two of the first
    frameworks that allowed such dynamical computations. The downside of Torch is
    that the host language is Lua, a programming language not commonly used. In 2017,
    Torch switched from Lua to PyTorch, and many DL practitioners started using that
    framework. As a reaction, TensorFlow now also includes the possibility of a dynamic
    graph, called eager execution.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 静态库的主要问题是由于两步程序（首先构建图然后执行它），调试相当繁琐。在动态图框架中，图是即时定义和评估的。因此，你可以在代码的每个点上访问真实的数值。这在调试时具有巨大的优势。此外，静态图的一个缺点是，你不能包含条件语句和循环，以便对不同的输入动态响应。Chainer
    和 Torch 是最早允许这种动态计算的框架之一。Torch 的缺点是宿主语言是 Lua，这是一种不太常用的编程语言。2017 年，Torch 从 Lua
    转换为 PyTorch，许多深度学习从业者开始使用这个框架。作为回应，TensorFlow 现在也包含了动态图的可能性，称为 eager 执行。
- en: Listing 3.7 displays the TensorFlow code for the linear regression problem using
    eager execution (see also the notebook [http://mng.bz/MdJQ](http://mng.bz/MdJQ)).
    The framework doesn’t need to build a static graph anymore. You can stop at any
    point and each tensor has a value associated with it. TensorFlow in eager mode
    still needs to calculate the gradients for the individual operations but does
    this in parallel to executing the code. TensorFlow internally stores the intermediate
    values needed for calculation of the gradients in an entity called `tape` .
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 显示了使用 eager 执行的线性回归问题的 TensorFlow 代码（也参见笔记本 [http://mng.bz/MdJQ](http://mng.bz/MdJQ)）。框架不再需要构建静态图。你可以在任何点停止，每个张量都有一个与之关联的值。在
    eager 模式下，TensorFlow 仍然需要计算单个操作的梯度，但这是在执行代码的同时并行的。TensorFlow 在内部将用于计算梯度的中间值存储在一个名为
    `tape` 的实体中。
- en: Listing 3.1 Linear regression with `TF.eager`
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 使用 `TF.eager` 的线性回归
- en: '[PRE6]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Sets a and *b* as variables so that you can optimize them later
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 a 和 *b* 设置为变量，以便以后可以优化它们
- en: ❷ Sets the learning rate
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置学习率
- en: ❸ Records all information needed to calculate the gradients in this scope
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 记录在此范围内计算梯度所需的所有信息
- en: ❹ Calculates the loss w.r.t. *a* and *b*
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算相对于 *a* 和 *b* 的损失
- en: ❺ Applies the update formula 3.3 and assigns a to a new value
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 应用更新公式 3.3 并将 a 赋予新的值
- en: ❻ Code for printing omitted
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印代码被省略
- en: ❼ Applies the update formula 3.3 and assigns *b* to a new value
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 应用更新公式 3.3 并将 *b* 赋予新的值
- en: As you can see, there’s no separation between the construction and execution
    of the code. The line `with` `tf.GradientTape()` `as` `tape:` tells TensorFlow
    to keep track of all differentiations using a so-called tape mechanism. Because
    it takes some time to store the intermediate values, you only want this to take
    place when you really need it. This is great for debugging and developing complex
    networks. However, there is a price to pay. Especially if you use many small operations,
    the eager approach can get quite slow. But there is a solution. If you put all
    the relevant code in a function and decorate this function with the `@tf.function`
    decorator, the code in the function gets compiled into a graph and then runs much
    faster.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，代码的构建和执行之间没有分离。`with tf.GradientTape() as tape:` 这行代码告诉 TensorFlow 使用所谓的带子机制跟踪所有微分。由于存储中间值需要一些时间，你只希望在真正需要的时候才这样做。这对于调试和开发复杂的网络来说是非常好的。然而，这也需要付出代价。特别是如果你使用了很多小操作，即时方法可能会变得相当慢。但是有一个解决方案。如果你把所有相关的代码放在一个函数中，并用
    `@tf.function` 装饰器装饰这个函数，那么函数中的代码就会被编译成一个图，然后运行得更快。
- en: 'With TensorFlow v2.0, you have the best of both worlds: eager execution during
    development and a graph-based framework in production. The following notebook
    contains an example of how to use `@tf.function` ; further details can be found
    at [https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function)
    .'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow v2.0 中，你拥有了开发时的即时执行和在生产中的基于图框架的最佳结合。以下笔记本包含了一个如何使用 `@tf.function`
    的示例；更多细节可以在 [https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function)
    找到。
- en: In addition to the notebooks discussed, we also provide a notebook using the
    `autograd` library in Python to automatically calculate the gradient; see the
    notebook [http://mng.bz/aR5j](http://mng.bz/aR5j) . In the next chapter, we’ll
    really start our journey and encounter the first principle with which loss functions
    can be derived--the principle of maximum likelihood (MaxLike).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 除了讨论过的笔记本外，我们还提供了一个使用 Python 中的 `autograd` 库来自动计算梯度的笔记本；请参阅笔记本 [http://mng.bz/aR5j](http://mng.bz/aR5j)
    。在下一章中，我们将真正开始我们的旅程，并遇到第一个可以从中推导出损失函数的原理——最大似然原理（MaxLike）。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Linear regression is the mother of all parametric models and is one of the smallest
    neural networks (NNs) you can think of.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归是所有参数模型的母亲，也是你可以想到的最小的神经网络（NNs）之一。
- en: You can fit a parametric model by minimizing a loss function that quantifies
    the deviation between the model and the data.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过最小化一个损失函数来拟合参数模型，该损失函数量化了模型与数据之间的偏差。
- en: The mean square error (MSE) is an appropriate loss function for regression models.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差（MSE）是回归模型的一个合适的损失函数。
- en: Gradient descent is a method for finding the parameter values that minimize
    the loss function. Gradient descent is a simple, general method that you can use
    for all kinds of parametric models, as long as the loss function is differentiable.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降是一种寻找最小化损失函数的参数值的方法。梯度下降是一种简单、通用的方法，只要损失函数是可微的，你就可以用它来处理所有类型的参数模型。
- en: With gradient descent, each parameter is updated iteratively and independently
    from the other parameters. This requires determining the gradient of the loss
    function with regard to each parameter. In addition, you need to define a learning
    rate.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降，每个参数都是迭代和独立地从其他参数中更新的。这需要确定损失函数相对于每个参数的梯度。此外，你还需要定义一个学习率。
- en: Getting the learning rate right (not too low or too high) is most important
    for successful fitting. With DL, you can perform a stochastic version of gradient
    descent (SG*D*) that estimates the gradients based on a random subset of the data
    (a mini-batch) instead of using all the data.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整学习率（既不要太低也不要太高）对于成功拟合至关重要。在使用深度学习（DL）时，你可以执行梯度下降的随机版本（SG*D*），它基于数据的一个随机子集（小批量）来估计梯度，而不是使用所有数据。
- en: DL frameworks like TensorFlow or Keras use backpropagation to determine the
    required gradients.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（DL）框架，如 TensorFlow 或 Keras，使用反向传播来确定所需的梯度。
- en: The optimizers used in DL are variants of SGD that accelerate the learning process
    by additionally taking advantage of past gradients.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习中使用的优化器是 SGD 的变体，通过利用过去的梯度来加速学习过程。
